Practical Foundations for Programming Languages  This text develops a comprehensive theory of programming languages based on type sys- tems and structural operational semantics. Language concepts are precisely deﬁned by their static and dynamic semantics, presenting the essential tools both intuitively and rigorously while relying on only elementary mathematics. These tools are used to analyze and prove properties of languages and provide the framework for combining and comparing language features. The broad range of concepts includes fundamental data types such as sums and products, polymorphic and abstract types, dynamic typing, dynamic dispatch, subtyping and reﬁnement types, symbols and dynamic classiﬁcation, parallelism and cost semantics, and concurrency and distribution. The methods are directly applicable to language imple- mentation, to the development of logics for reasoning about programs, and to the formal veriﬁcation language properties such as type safety.  This thoroughly revised second edition includes exercises at the end of nearly every  chapter and a new chapter on type reﬁnements.  Robert Harper is a professor in the Computer Science Department at Carnegie Mellon University. His main research interest is in the application of type theory to the design and implementation of programming languages and to the mechanization of their meta- theory. Harper is a recipient of the Allen Newell Medal for Research Excellence and the Herbert A. Simon Award for Teaching Excellence, and is an Association for Computing Machinery Fellow.    Practical Foundations for Programming Languages  Second Edition  Robert Harper  Carnegie Mellon University   32 Avenue of the Americas, New York, NY 10013  Cambridge University Press is part of the University of Cambridge.  It furthers the University’s mission by disseminating knowledge in the pursuit of education, learning, and research at the highest international levels of excellence.  Information on this title: www.cambridge.org 9781107150300  www.cambridge.org    Robert Harper 2016  This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written  permission of Cambridge University Press.  First published 2016  Printed in the United States of America  A catalog record for this publication is available from the British Library.  Library of Congress Cataloging in Publication Data  Names: Harper, Robert, 1957–  Title: Practical foundations for programming languages   Robert Harper,  Carnegie Mellon University.  Description: Second edition.  New York NY : Cambridge University Press,  2016.  Includes bibliographical references and index.  Identiﬁers: LCCN 2015045380  ISBN 9781107150300  alk. paper  Subjects: LCSH: Programming languages  Electronic computers   Classiﬁcation: LCC QA76.7 .H377 2016  DDC 005.13–dc23  LC record available at http:  lccn.loc.gov 2015045380  ISBN 978-1-107-15030-0 Hardback  Cambridge University Press has no responsibility for the persistence or accuracy of URLs for external or third-party Internet Web sites referred to in this publication  and does not guarantee that any content on such Web sites is, or will remain,  accurate or appropriate.   Contents  Preface to the Second Edition Preface to the First Edition  page xv xvii  Part I  Judgments and Rules  1 Abstract Syntax  1.1 Abstract Syntax Trees 1.2 Abstract Binding Trees 1.3 Notes  2 Inductive Deﬁnitions  Judgments Inference Rules  2.1 2.2 2.3 Derivations 2.4 Rule Induction 2.5 2.6 Deﬁning Functions by Rules 2.7 Notes  3 Hypothetical and General Judgments  3.1 Hypothetical Judgments 3.2 Hypothetical Inductive Deﬁnitions 3.3 General Judgments 3.4 Generic Inductive Deﬁnitions 3.5 Notes  Iterated and Simultaneous Inductive Deﬁnitions  Part II Statics and Dynamics  4 Statics  4.1 Syntax 4.2 Type System 4.3 Structural Properties 4.4 Notes  3 3 6 10  12 12 12 14 15 17 18 19  21 21 24 26 27 28  33 33 34 35 37   vi  Contents  5 Dynamics  5.1 Transition Systems 5.2 Structural Dynamics 5.3 Contextual Dynamics 5.4 Equational Dynamics 5.5 Notes  6 Type Safety  6.1 Preservation 6.2 Progress 6.3 Run-Time Errors 6.4 Notes  7 Evaluation Dynamics  7.1 Evaluation Dynamics 7.2 Relating Structural and Evaluation Dynamics 7.3 Type Safety, Revisited 7.4 Cost Dynamics 7.5 Notes  Part III Total Functions  8 Function Deﬁnitions and Values  8.1 First-Order Functions 8.2 Higher-Order Functions 8.3 Evaluation Dynamics and Deﬁnitional Equality 8.4 Dynamic Scope 8.5 Notes  9 System T of Higher-Order Recursion  9.1 Statics 9.2 Dynamics 9.3 Deﬁnability 9.4 Undeﬁnability 9.5 Notes  Part IV Finite Data Types  10 Product Types  10.1 Nullary and Binary Products 10.2 Finite Products 10.3 Primitive Mutual Recursion 10.4 Notes  39 39 40 42 44 46  48 48 49 50 52  53 53 54 55 56 57  61 61 62 65 66 67  69 69 70 71 73 75  79 79 81 82 83   vii  Contents  11 Sum Types  11.1 Nullary and Binary Sums 11.2 Finite Sums 11.3 Applications of Sum Types 11.4 Notes  Part V Types and Propositions  12 Constructive Logic  12.1 Constructive Semantics 12.2 Constructive Logic 12.3 Proof Dynamics 12.4 Propositions as Types 12.5 Notes  13 Classical Logic  13.1 Classical Logic 13.2 Deriving Elimination Forms 13.3 Proof Dynamics 13.4 Law of the Excluded Middle 13.5 The Double-Negation Translation 13.6 Notes  14 Generic Programming  14.1 Introduction 14.2 Polynomial Type Operators 14.3 Positive Type Operators 14.4 Notes  15 Inductive and Coinductive Types  15.1 Motivating Examples 15.2 Statics 15.3 Dynamics 15.4 Solving Type Equations 15.5 Notes  16 System F of Polymorphic Types  16.1 Polymorphic Abstraction 16.2 Polymorphic Deﬁnability 16.3 Parametricity Overview 16.4 Notes  Part VI  Inﬁnite Data Types  Part VII Variable Types  85 85 86 88 91  95 95 96 100 101 101  104 105 109 110 111 113 114  119 119 119 122 123  125 125 128 130 131 132  137 137 140 142 144   Part VIII Partiality and Recursive Types  viii  Contents  17 Abstract Types  17.1 Existential Types 17.2 Data Abstraction 17.3 Deﬁnability of Existential Types 17.4 Representation Independence 17.5 Notes  18 Higher Kinds  18.1 Constructors and Kinds 18.2 Constructor Equality 18.3 Expressions and Types 18.4 Notes  19 System PCF of Recursive Functions  19.1 Statics 19.2 Dynamics 19.3 Deﬁnability 19.4 Finite and Inﬁnite Data Structures 19.5 Totality and Partiality 19.6 Notes  20 System FPC of Recursive Types  20.1 Solving Type Equations 20.2 Inductive and Coinductive Types 20.3 Self-Reference 20.4 The Origin of State 20.5 Notes  21 The Untyped λ-Calculus  21.1 The λ-Calculus 21.2 Deﬁnability 21.3 Scott’s Theorem 21.4 Untyped Means Uni-Typed 21.5 Notes  22 Dynamic Typing  22.1 Dynamically Typed PCF 22.2 Variations and Extensions 22.3 Critique of Dynamic Typing 22.4 Notes  Part IX Dynamic Types  146 146 149 150 151 153  154 155 156 157 158  161 162 163 165 167 167 169  171 171 172 174 176 177  181 181 182 184 186 187  189 189 192 194 195   ix  Contents  23 Hybrid Typing  23.1 A Hybrid Language 23.2 Dynamic as Static Typing 23.3 Optimization of Dynamic Typing 23.4 Static versus Dynamic Typing 23.5 Notes  Part X Subtyping  24 Structural Subtyping  24.1 Subsumption 24.2 Varieties of Subtyping 24.3 Variance 24.4 Dynamics and Safety 24.5 Notes  25 Behavioral Typing  25.1 Statics 25.2 Boolean Blindness 25.3 Reﬁnement Safety 25.4 Notes  26 Classes and Methods  26.1 The Dispatch Matrix 26.2 Class-Based Organization 26.3 Method-Based Organization 26.4 Self-Reference 26.5 Notes  27 Inheritance  27.1 Class and Method Extension 27.2 Class-Based Inheritance 27.3 Method-Based Inheritance 27.4 Notes  28 Control Stacks  28.1 Machine Deﬁnition 28.2 Safety 28.3 Correctness of the K Machine 28.4 Notes  Part XI Dynamic Dispatch  Part XII Control Flow  198 198 200 201 203 204  207 207 208 211 215 216  219 220 226 228 229  235 235 238 239 240 242  245 245 246 248 249  253 253 255 256 259   x  Contents  29 Exceptions 29.1 Failures 29.2 Exceptions 29.3 Exception Values 29.4 Notes  30 Continuations 30.1 Overview 30.2 Continuation Dynamics 30.3 Coroutines from Continuations 30.4 Notes  Part XIII Symbolic Data  31 Symbols  31.1 Symbol Declaration 31.2 Symbol References 31.3 Notes  32 Fluid Binding  32.1 Statics 32.2 Dynamics 32.3 Type Safety 32.4 Some Subtleties 32.5 Fluid References 32.6 Notes  33 Dynamic Classiﬁcation  33.1 Dynamic Classes 33.2 Class References 33.3 Deﬁnability of Dynamic Classes 33.4 Applications of Dynamic Classiﬁcation 33.5 Notes  Part XIV Mutable State  34 Modernized Algol  34.1 Basic Commands 34.2 Some Programming Idioms 34.3 Typed Commands and Typed Assignables 34.4 Notes  35 Assignable References  35.1 Capabilities 35.2 Scoped Assignables  260 260 262 263 264  266 266 268 269 272  277 277 280 282  284 284 285 286 287 288 289  291 291 293 294 295 296  301 301 306 307 310  313 313 314   Part XV Parallelism  xi  Contents  35.3 Free Assignables 35.4 Safety 35.5 Benign Effects 35.6 Notes  36 Lazy Evaluation  36.1 PCF By-Need 36.2 Safety of PCF By-Need 36.3 FPC By-Need 36.4 Suspension Types 36.5 Notes  37 Nested Parallelism  37.1 Binary Fork-Join 37.2 Cost Dynamics 37.3 Multiple Fork-Join 37.4 Bounded Implementations 37.5 Scheduling 37.6 Notes  38 Futures and Speculations  38.1 Futures 38.2 Speculations 38.3 Parallel Dynamics 38.4 Pipelining with Futures 38.5 Notes  39 Process Calculus  39.1 Actions and Events 39.2 Interaction 39.3 Replication 39.4 Allocating Channels 39.5 Communication 39.6 Channel Passing 39.7 Universality 39.8 Notes  40 Concurrent Algol  40.1 Concurrent Algol 40.2 Broadcast Communication 40.3 Selective Communication  Part XVI Concurrency and Distribution  316 318 320 321  323 323 326 328 329 331  335 335 338 341 342 346 348  350 350 351 352 354 356  359 359 361 363 364 366 369 371 372  375 375 378 380   xii  Contents  40.4 Free Assignables as Processes 40.5 Notes  41 Distributed Algol  41.1 Statics 41.2 Dynamics 41.3 Safety 41.4 Notes  Part XVII Modularity  42 Modularity and Linking  42.1 Simple Units and Linking 42.2 Initialization and Effects 42.3 Notes  43 Singleton Kinds and Subkinding  43.1 Overview 43.2 Singletons 43.3 Dependent Kinds 43.4 Higher Singletons 43.5 Notes  44 Type Abstractions and Type Classes  44.1 Type Abstraction 44.2 Type Classes 44.3 A Module Language 44.4 First- and Second-Class 44.5 Notes  45 Hierarchy and Parameterization  45.1 Hierarchy 45.2 Abstraction 45.3 Hierarchy and Abstraction 45.4 Applicative Functors 45.5 Notes  Part XVIII Equational Reasoning  46 Equality for System T  46.1 Observational Equivalence 46.2 Logical Equivalence 46.3 Logical and Observational Equivalence Coincide 46.4 Some Laws of Equality 46.5 Notes  382 383  385 385 388 390 391  395 395 396 398  399 399 400 402 405 407  409 410 412 414 418 419  422 422 425 427 429 431  435 435 439 440 443 444   xiii  Contents  47 Equality for System PCF  47.1 Observational Equivalence 47.2 Logical Equivalence 47.3 Logical and Observational Equivalence Coincide 47.4 Compactness 47.5 Lazy Natural Numbers 47.6 Notes  48 Parametricity 48.1 Overview 48.2 Observational Equivalence 48.3 Logical Equivalence 48.4 Parametricity Properties 48.5 Representation Independence, Revisited 48.6 Notes  49 Process Equivalence 49.1 Process Calculus 49.2 Strong Equivalence 49.3 Weak Equivalence 49.4 Notes  A Background on Finite Sets  Bibliography Index  Part XIX Appendices  445 445 446 446 449 452 453  454 454 455 456 461 464 465  467 467 469 472 473  477  479 487    Preface to the Second Edition  Writing the second edition to a textbook incurs the same risk as building the second version of a software system. It is difﬁcult to make substantive improvements, while avoiding the temptation to overburden and undermine the foundation on which one is building. With the hope of avoiding the second system effect, I have sought to make corrections, revisions, expansions, and deletions that improve the coherence of the development, remove some topics that distract from the main themes, add new topics that were omitted from the ﬁrst edition, and include exercises for almost every chapter.  The revision removes a number of typographical errors, corrects a few material errors  especially the formulation of the parallel abstract machine and of concurrency in Algol , and improves the writing throughout. Some chapters have been deleted  general pattern matching and polarization, restricted forms of polymorphism , some have been completely rewritten  the chapter on higher kinds , some have been substantially revised  general and parametric inductive deﬁnitions, concurrent and distributed Algol , several have been reorganized  to better distinguish partial from total type theories , and a new chapter has been added  on type reﬁnements . Titular attributions on several chapters have been removed, not to diminish credit, but to avoid confusion between the present and the original formulations of several topics. A new system of  pronounceable!  language names has been introduced throughout. The exercises generally seek to expand on the ideas in the main text, and their solutions often involve signiﬁcant technical ideas that merit study. Routine exercises of the kind one might include in a homework assignment are deliberately few.  My purpose in writing this book is to establish a comprehensive framework for formu- lating and analyzing a broad range of ideas in programming languages. If language design and programming methodology are to advance from a trade-craft to a rigorous discipline, it is essential that we ﬁrst get the deﬁnitions right. Then, and only then, can there be mean- ingful analysis and consolidation of ideas. My hope is that I have helped to build such a foundation.  I am grateful to Stephen Brookes, Evan Cavallo, Karl Crary, Jon Sterling, James R. Wilcox and Todd Wilson for their help in critiquing drafts of this edition and for their suggestions for modiﬁcation and revision. I thank my department head, Frank Pfenning, for his support of my work on the completion of this edition. Thanks also to my editors, Ada Brunstein and Lauren Cowles, for their guidance and assistance. And thanks to Andrew Shulaev for corrections to the draft.  Neither the author nor the publisher make any warranty, express or implied, that the deﬁnitions, theorems, and proofs contained in this volume are free of error, or are consistent with any particular standard of merchantability, or that they will meet requirements for any particular application. They should not be relied on for solving a problem whose incorrect   xvi  Preface to the Second Edition  solution could result in injury to a person or loss of property. If you do use this material in such a manner, it is at your own risk. The author and publisher disclaim all liability for direct or consequential damage resulting from its use.  Pittsburgh July 2015   Preface to the First Edition  Types are the central organizing principle of the theory of programming languages. Lan- guage features are manifestations of type structure. The syntax of a language is governed by the constructs that deﬁne its types, and its semantics is determined by the interactions among those constructs. The soundness of a language design—the absence of ill-deﬁned programs—follows naturally.  The purpose of this book is to explain this remark. A variety of programming language features are analyzed in the unifying framework of type theory. A language feature is deﬁned by its statics, the rules governing the use of the feature in a program, and its dynamics, the rules deﬁning how programs using this feature are to be executed. The concept of safety emerges as the coherence of the statics and the dynamics of a language.  In this way, we establish a foundation for the study of programming languages. But why these particular methods? The main justiﬁcation is provided by the book itself. The methods we use are both precise and intuitive, providing a uniform framework for explaining programming language concepts. Importantly, these methods scale to a wide range of programming language concepts, supporting rigorous analysis of their properties. Although it would require another book in itself to justify this assertion, these methods are also practical in that they are directly applicable to implementation and uniquely effective as a basis for mechanized reasoning. No other framework offers as much.  Being a consolidation and distillation of decades of research, this book does not provide an exhaustive account of the history of the ideas that inform it. Sufﬁce it to say that much of the development is not original but rather is largely a reformulation of what has gone before. The notes at the end of each chapter signpost the major developments but are not intended as a complete guide to the literature. For further information and alternative perspectives, the reader is referred to such excellent sources as Constable  1986, 1998 , Girard  1989 , Martin-L¨of  1984 , Mitchell  1996 , Pierce  2002, 2004 , and Reynolds  1998 .  The book is divided into parts that are, in the main, independent of one another. Parts I and II, however, provide the foundation for the rest of the book and must therefore be considered prior to all other parts. On ﬁrst reading, it may be best to skim Part I, and begin in earnest with Part II, returning to Part I for clariﬁcation of the logical framework in which the rest of the book is cast.  Numerous people have read and commented on earlier editions of this book and have suggested corrections and improvements to it. I am particularly grateful to Umut Acar, Jesper Louis Andersen, Carlo Angiuli, Andrew Appel, Stephanie Balzer, Eric Bergstrom, Guy E. Blelloch, Iliano Cervesato, Lin Chase, Karl Crary, Rowan Davies, Derek Dreyer, Dan Licata, Zhong Shao, Rob Simmons, and Todd Wilson for their extensive efforts in   xviii  Preface to the First Edition  reading and criticizing the book. I also thank the following people for their suggestions: Joseph Abrahamson, Arbob Ahmad, Zena Ariola, Eric Bergstrome, William Byrd, Alejan- dro Cabrera, Luis Caires, Luca Cardelli, Manuel Chakravarty, Richard C. Cobbe, James Cooper, Yi Dai, Daniel Dantas, Anupam Datta, Jake Donham, Bill Duff, Matthias Felleisen, Kathleen Fisher, Dan Friedman, Peter Gammie, Maia Ginsburg, Byron Hawkins, Kevin Hely, Kuen-Bang Hou  Favonia , Justin Hsu, Wojciech Jedynak, Cao Jing, Salil Joshi, Gabriele Keller, Scott Kilpatrick, Danielle Kramer, Dan Kreysa, Akiva Leffert, Ruy Ley- Wild, Karen Liu, Dave MacQueen, Chris Martens, Greg Morrisett, Stefan Muller, Tom Murphy, Aleksandar Nanevski, Georg Neis, David Neville, Adrian Trejo Nu˜nez, Cyrus Omar, Doug Perkins, Frank Pfenning, Jean Pichon, Benjamin Pierce, Andrew M. Pitts, Gordon Plotkin, David Renshaw, John Reynolds, Andreas Rossberg, Carter Schonwald, Dale Schumacher, Dana Scott, Shayak Sen, Pawel Sobocinski, Kristina Sojakova, Daniel Spoonhower, Paulo Tanimoto, Joe Tassarotti, Peter Thiemann, Bernardo Toninho, Michael Tschantz, Kami Vaniea, Carsten Varming, David Walker, Dan Wang, Jack Wileden, Sergei Winitzki, Roger Wolff, Omer Zach, Luke Zarko, and Yu Zhang. I am very grateful to the students of 15-312 and 15-814 at Carnegie Mellon who have provided the impetus for the preparation of this book and who have endured the many revisions to it over the last ten years.  I thank the Max Planck Institute for Software Systems for its hospitality and support. I also thank Espresso a Mano in Pittsburgh, CB2 Cafe in Cambridge, and Thonet Cafe in Saarbr¨ucken for providing a steady supply of coffee and a conducive atmosphere for writing.  This material is, in part, based on work supported by the National Science Foundation under Grant Nos. 0702381 and 0716469. Any opinions, ﬁndings, and conclusions or rec- ommendations expressed in this material are those of the author s  and do not necessarily reﬂect the views of the National Science Foundation.  Robert Harper Pittsburgh March 2012   P A R T I  Judgments and Rules    1  Abstract Syntax  Programming languages express computations in a form comprehensible to both people and machines. The syntax of a language speciﬁes how various sorts of phrases  expressions, commands, declarations, and so forth  may be combined to form programs. But what are these phrases? What is a program made of?  The informal concept of syntax involves several distinct concepts. The surface, or con- crete, syntax is concerned with how phrases are entered and displayed on a computer. The surface syntax is usually thought of as given by strings of characters from some alphabet  say, ASCII or Unicode . The structural, orabstract, syntax is concerned with the structure of phrases, speciﬁcally how they are composed from other phrases. At this level, a phrase is a tree, called an abstract syntax tree, whose nodes are operators that combine several phrases to form another phrase. The binding structure of syntax is concerned with the introduction and use of identiﬁers: how they are declared, and how declared identiﬁers can be used. At this level, phrases are abstract binding trees, which enrich abstract syntax trees with the concepts of binding and scope.  We will not concern ourselves in this book with concrete syntax but will instead consider pieces of syntax to be ﬁnite trees augmented with a means of expressing the binding and scope of identiﬁers within a syntax tree. To prepare the ground for the rest of the book, we deﬁne in this chapter what is a “piece of syntax” in two stages. First, we deﬁne abstract syntax trees, or ast’s, which capture the hierarchical structure of a piece of syntax, while avoiding commitment to their concrete representation as a string. Second, we augment abstract syntax trees with the means of specifying the binding  declaration  and scope  range of signiﬁcance  of an identiﬁer. Such enriched forms of abstract syntax are called abstract binding trees, or abt’s for short.  Several functions and relations on abt’s are deﬁned that give precise meaning to the informal ideas of binding and scope of identiﬁers. The concepts are infamously difﬁcult to deﬁne properly and are the mother lode of bugs for language implementors. Consequently, precise deﬁnitions are essential, but they are also fairly technical and take some getting used to. It is probably best to skim this chapter on ﬁrst reading to get the main ideas, and return to it for clariﬁcation as necessary.  1.1 Abstract Syntax Trees  An abstract syntax tree, or ast for short, is an ordered tree whose leaves are variables, and whose interior nodes are operators whose arguments are its children. Ast’s are classiﬁed   4  Abstract Syntax  into a variety of sorts corresponding to different forms of syntax. A variable stands for an unspeciﬁed, or generic, piece of syntax of a speciﬁed sort. Ast’s can be combined by an operator, which has an arity specifying the sort of the operator and the number and sorts of its arguments. An operator of sort s and arity s1, . . . , sn combines n ≥ 0 ast’s of sort s1, . . . , sn, respectively, into a compound ast of sort s.  The concept of a variable is central and therefore deserves special emphasis. A variable is an unknown object drawn from some domain. The unknown can become known by substitution of a particular object for all occurrences of a variable in a formula, thereby specializing a general formula to a particular instance. For example, in school algebra variables range over real numbers, and we may form polynomials, such as x2 + 2 x + 1, that can be specialized by substitution of, say, 7 for x to obtain 72 +  2× 7 + 1, which can be simpliﬁed according to the laws of arithmetic to obtain 64, which is  7 + 1 2.  Abstract syntax trees are classiﬁed by sorts that divide ast’s into syntactic categories. For example, familiar programming languages often have a syntactic distinction between expressions and commands; these are two sorts of abstract syntax trees. Variables in abstract syntax trees range over sorts in the sense that only ast’s of the speciﬁed sort of the variable can be plugged in for that variable. Thus, it would make no sense to replace an expression variable by a command, nor a command variable by an expression, the two being different sorts of things. But the core idea carries over from school mathematics, namely that a variable is an unknown, or a place-holder, whose meaning is given by substitution.  As an example, consider a language of arithmetic expressions built from numbers, addition, and multiplication. The abstract syntax of such a language consists of a single sort Exp generated by these operators: 1. An operator num[n] of sort Exp for each n ∈ N. 2. Two operators, plus and times, of sort Exp, each with two arguments of sort Exp. The expression 2 +  3 × x , which involves a variable, x, would be represented by the ast  plus num[2]; times num[3]; x    of sort Exp, under the assumption that x is also of this sort. Because, say, num[4], is an ast of sort Exp, we may plug it in for x in the above ast to obtain the ast  plus num[2]; times num[3]; num[4]  ,  which is written informally as 2 +  3 × 4 . We may, of course, plug in more complex ast’s of sort Exp for x to obtain other ast’s as result. The tree structure of ast’s provides a very useful principle of reasoning, called structural induction. Suppose that we wish to prove that some property P a  holds for all ast’s a of a given sort. To show this, it is enough to consider all the ways in which a can be generated and show that the property holds in each case under the assumption that it holds for its constituent ast’s  if any . So, in the case of the sort Exp just described, we must show 1. The property holds for any variable x of sort Exp: prove that P x . 2. The property holds for any number, num[n]: for every n ∈ N, prove that P num[n] .   5  1.1 Abstract Syntax Trees  3. Assuming that the property holds for a1 and a2, prove that it holds for plus a1; a2  and  times a1; a2 : if P a1  and P a2 , then P plus a1; a2   and P times a1; a2  .  Because these cases exhaust all possibilities for the formation of a, we are assured that P a  holds for any ast a of sort Exp.  It is common to apply the principle of structural induction in a form that takes account of the interpretation of variables as place-holders for ast’s of the appropriate sort. Informally, it is often useful to prove a property of an ast involving variables in a form that is conditional on the property holding for the variables. Doing so anticipates that the variables will be replaced with ast’s that ought to have the property assumed for them, so that the result of the replacement will have the property as well. This amounts to applying the principle of structural induction to properties P a  of the form “if a involves variables x1, . . . , xk, and Q holds of each xi, then Q holds of a,” so that a proof of P a  for all ast’s a by structural induction is just a proof that Q a  holds for all ast’s a under the assumption that Q holds for its variables. When there are no variables, there are no assumptions, and the proof of P is a proof that Q holds for all closed ast’s. On the other hand, if x is a variable in a, and we replace it by an ast b for which Q holds, then Q will hold for the result of replacing x by b in a. For the sake of precision, we now give precise deﬁnitions of these concepts. Let S be a ﬁnite set of sorts. For a given set S of sorts, an arity has the form  s1, . . . , sn s, which speciﬁes the sort s ∈ S of an operator taking n ≥ 0 arguments, each of sort si ∈ S. Let O = {Oα } be an arity-indexed family of disjoint sets of operators Oα of arity α. Ifo is an operator of arity  s1, . . . , sn s, we say that o has sort s and has n arguments of sorts s1, . . . , sn. Fix a set S of sorts and an arity-indexed family O of sets of operators of each arity. Let X = {Xs }s∈S be a sort-indexed family of disjoint ﬁnite sets Xs of variables x of sort s. When X is clear from context, we say that a variable x is of sort s if x ∈ Xs, and we say that x is fresh for X , or justfresh when X is understood, if x  ∈ Xs for any sort s. If x is fresh for X and s is a sort, then X , x is the family of sets of variables obtained by adding x to Xs. The notation is ambiguous in that the sort s is not explicitly stated but determined from context. The family A[X ] = {A[X ]s }s∈S of abstract syntax trees, orast’s, of sort s is the smallest family satisfying the following conditions: 1. A variable of sort s is an ast of sort s: ifx ∈ Xs, then x ∈ A[X ]s. 2. Operators combine ast’s: if o is an operator of arity  s1, . . . , sn s, and if a1 ∈ A[X ]s1,  . . . , an ∈ A[X ]sn, then o a1; . . . ;an  ∈ A[X ]s.  It follows from this deﬁnition that the principle of structural induction can be used to prove that some property P holds of every ast. To show P a  holds for every a ∈ A[X ], it is enough to show: 1. If x ∈ Xs, then Ps x . 2. If o has arity  s1, . . . , sn s and Ps1 a1  and . . . and Psn an , then Ps o a1; . . . ;an  .   6  Abstract Syntax  For example, it is easy to prove by structural induction that A[X ] ⊆ A[Y] whenever X ⊆ Y.  cid:5 , and b ∈ A[X ]s, then Variables are given meaning by substitution. If a ∈ A[X , x]s [b x]a ∈ A[X ]s  cid:5  is the result of substituting b for every occurrence of x in a. The ast a is called the target, and x is called the subject, of the substitution. Substitution is deﬁned by the following equations: 1. [b x]x = b and [b x]y = y if x  cid:6 = y. 2. [b x]o a1; . . . ;an  = o [b x]a1; . . . ;[b x]an .  For example, we may check that  [num[2] x]plus x; num[3]  = plus num[2]; num[3] .  We may prove by structural induction that substitution on ast’s is well-deﬁned. Theorem 1.1. If a ∈ A[X , x], then for every b ∈ A[X ] there exists a unique c ∈ A[X ] such that [b x]a = c Proof By structural induction on a. If a = x, then c = b by deﬁnition; otherwise, if a = y  cid:6 = x, then c = y, also by deﬁnition. Otherwise, a = o a1, . . . , an , and we have by induction unique c1, . . . , cn such that [b x]a1 = c1 and . . . [b x]an = cn, and so c is c = o c1; . . . ;cn , by deﬁnition of substitution.  1.2 Abstract Binding Trees  Abstract binding trees, orabt’s, enrich ast’s with the means to introduce new variables and symbols, called a binding, with a speciﬁed range of signiﬁcance, called its scope. The scope of a binding is an abt within which the bound identiﬁer can be used, either as a place-holder  in the case of a variable declaration  or as the index of some operator  in the case of a symbol declaration . Thus, the set of active identiﬁers can be larger within a subtree of an abt than it is within the surrounding tree. Moreover, different subtrees may introduce identiﬁers with disjoint scopes. The crucial principle is that any use of an identiﬁer should be understood as a reference, or abstract pointer, to its binding. One consequence is that the choice of identiﬁers is immaterial, so long as we can always associate a unique binding with each use of an identiﬁer.  As a motivating example, consider the expression let x be a1 in a2, which introduces a variable x for use within the expression a2 to stand for the expression a1. The variable x is bound by the let expression for use within a2; any use ofx within a1 refers to a different variable that happens to have the same name. For example, in the expression let x be 7 in x + x occurrences of x in the addition refer to the variable introduced by the let. On the other hand, in the expression let x be x ∗ x in x + x, occurrences of x within the multiplication refer to a different variable than those occurring within the addition. The   7  1.2 Abstract Binding Trees  latter occurrences refer to the binding introduced by the let, whereas the former refer to some outer binding not displayed here. The names of bound variables are immaterial insofar as they determine the same binding. So, for example, let x be x ∗ x in x + x could just as well have been written let y be x ∗ x in y + y, without changing its meaning. In the former case, the variable x is bound within the addition, and in the latter, it is the variable y, but the “pointer structure” remains the same. On the other hand, the expression let x be y ∗ y in x + x has a different meaning to these two expressions, because now the variable y within the multiplication refers to a different surrounding variable. Renaming of bound variables is constrained to the extent that it must not alter the reference structure of the expression. For example, the expression  has a different meaning than the expression  let x be 2 in let y be 3 in x + x  let y be 2 in let y be 3 in y + y,  because the y in the expression y + y in the second case refers to the inner declaration, not the outer one as before.  The concept of an ast can be enriched to account for binding and scope of a variable. These enriched ast’s are called abstract binding trees, orabt’s for short. Abt’s generalize ast’s by allowing an operator to bind any ﬁnite number  possibly zero  of variables in each argument. An argument to an operator is called an abstractor and has the form x1, . . . , xk.a. The sequence of variables x1, . . . , xk are bound within the abt a.  When k is zero, we elide the distinction between .a and a itself.  Written in the form of an abt, the expression let x be a1 in a2 has the form let a1; x.a2 , which more clearly speciﬁes that the variable x is bound within a2, and not within a1. We often write  cid:8 x to stand for a ﬁnite sequence x1, . . . , xn of distinct variables and write  cid:8 x.a to mean x1, . . . , xn.a. To account for binding, operators are assigned generalized arities of the form  υ1, . . . , υn s, which speciﬁes operators of sort s with n arguments of valence υ1, . . . , υn. In general a valence υ has the form s1, . . . , sk.s, which speciﬁes the sort of an argument as well as the number and sorts of the variables bound within it. We say that a sequence  cid:8 x of variables is of sort  cid:8 s to mean that the two sequences have the same length k and that the variable xi is of sort si for each 1 ≤ i ≤ k. Thus, to specify that the operator let has arity  Exp, Exp.Exp Exp indicates that it is of sort Exp whose ﬁrst argument is of sort Exp and binds no variables and whose second argument is also of sort Exp and within which is bound one variable of sort Exp. The informal expression let x be 2 + 2 in x × x may then be written as the abt  let plus num[2]; num[2] ; x.times x; x    in which the operator let has two arguments, the ﬁrst of which is an expression, and the second of which is an abstractor that binds one expression variable. Fix a setS of sorts and a familyO of disjoint sets of operators indexed by their generalized arities. For a given family of disjoint sets of variables X , the family of abstract binding   8  Abstract Syntax  trees, orabt’s B[X ], is deﬁned similarly to A[X ], except that X is not ﬁxed throughout the deﬁnition but rather changes as we enter the scopes of abstractors.  This simple idea is surprisingly hard to make precise. A ﬁrst attempt at the deﬁnition is  as the least family of sets closed under the following conditions: 1. If x ∈ Xs, then x ∈ B[X ]s. 2. For each operator o of arity   cid:8 s1.s1, . . . , cid:8 sn.sn s, if a1 ∈ B[X ,  cid:8 x1]s1, . . . , and an ∈  B[X ,  cid:8 xn]sn, then o  cid:8 x1.a1; . . . ; cid:8 xn.an  ∈ B[X ]s.  The bound variables are adjoined to the set of active variables within each argument, with the sort of each variable determined by the valence of the operator.  This deﬁnition is almost correct but fails to properly account for renaming of bound vari- ables. An abt of the form let a1; x.let a2; x.a3   is ill-formed according to this deﬁnition, because the ﬁrst binding adds x to X , which implies that the second cannot also add x to X , x, because it is not fresh for X , x. The solution is to ensure that each of the arguments is well-formed regardless of the choice of bound variable names, which is achieved using  cid:5  is fresh for X . We write cid:2 ρ a  for the result of replacing each fresh renamings, which are bijections between sequences of variables. Speciﬁcally, a fresh renaming  relative to X   of a ﬁnite sequence of variables  cid:8 x is a bijection ρ :  cid:8 x ↔  cid:8 x  cid:5  between  cid:8 x and  cid:8 x occurrence of xi in a by ρ xi , its fresh counterpart.   cid:5 , where  cid:8 x  This is achieved by altering the second clause of the deﬁnition of abt’s using fresh  renamings as follows:  For each operator o of arity   cid:8 s1.s1, . . . , cid:8 sn.sn s, if for each 1 ≤ i ≤ n and each fresh renaming ρi :  cid:8 xi ↔  cid:8 x  i], then o  cid:8 x1.a1; . . . ; cid:8 xn.an  ∈ B[X ]s.  cid:5  The renaming cid:2 ρi ai  of each ai ensures that collisions cannot occur and that the abt is valid  i, we have cid:2 ρi ai  ∈ B[X ,  cid:8 x   cid:5   for almost all renamings of any bound variables that occur within it. The principle of structural induction extends to abt’s and is called structural induction modulo fresh renaming. It states that to show that P[X ] a  holds for every a ∈ B[X ], it is enough to show the following: 1. if x ∈ Xs, then P[X ]s x . 2. For every o of arity   cid:8 s1.s1, . . . , cid:8 sn.sn s, if for each 1 ≤ i ≤ n, P[X ,  cid:8 x i  ∈ X , then P[X ]s o  cid:8 x1.a1; . . . ; cid:8 xn.an  .  cid:5   i]si   cid:2 ρi ai   holds  for every ρi :  cid:8 xi ↔  cid:8 x  i with  cid:8 x  cid:5    cid:5   The second condition ensures that the inductive hypothesis holds for all fresh choices of bound variable names, and not just the ones actually given in the abt. As an example let us deﬁne the judgment x ∈ a, where a ∈ B[X , x], to mean that x occurs free in a. Informally, this means that x is bound somewhere outside of a, rather than within a itself. If x is bound within a, then those occurrences of x are different from those occurring outside the binding. The following deﬁnition ensures that this is the case:   9  1.2 Abstract Binding Trees  1. x ∈ x. ρ :  cid:8 xi ↔  cid:8 zi we have x ∈ cid:2 ρ ai . 2. x ∈ o  cid:8 x1.a1; . . . ; cid:8 xn.an  if there exists 1 ≤ i ≤ n such that for every fresh renaming  The ﬁrst condition states that x is free in x but not free in y for any variable y other than x. The second condition states that if x is free in some argument, independently of the choice of bound variable names in that argument, then it is free in the overall abt. The relation a =α b of α-equivalence  so-called for historical reasons  means that a and b are identical up to the choice of bound variable names. The α-equivalence relation is the strongest congruence containing the following two conditions:  1. x =α x. 2. o  cid:8 x1.a1; . . . ; cid:8 xn.an  =α o  cid:8 x  n  if for every 1 ≤ i ≤ n, cid:2 ρi ai  =α 1; . . . ; cid:8 x  cid:5   cid:5   cid:5  n.a all fresh renamings ρi :  cid:8 xi ↔  cid:8 zi and ρ i :  cid:8 x  cid:5   ↔  cid:8 zi.   cid:5  1.a   cid:5  i   cid:2    cid:5  i a ρ   cid:5  i  for  The idea is that we rename  cid:8 xi and  cid:8 x and a  i are α-equivalent. If a =α b, then a and b are α-variants of each other.  cid:5    cid:5  i consistently, avoiding confusion, and check that ai  Some care is required in the deﬁnition of substitution of an abt b of sort s for free occurrences of a variable x of sort s in some abt a of some sort, written [b x]a. Substitution is partially deﬁned by the following conditions:  1. [b x]x = b, and [b x]y = y if x  cid:6 = y. 2. [b x]o  cid:8 x1.a1; . . . ; cid:8 xn.an  = o  cid:8 x1.a  that  cid:8 xi  ∈ b, and we set a   cid:5  i  1; . . . ; cid:8 xn.a  cid:5   n , where, for each 1 ≤ i ≤ n, we require  cid:5   = [b x]ai if x  ∈  cid:8 xi, and a  = ai otherwise.   cid:5  i  The deﬁnition of [b x]a is quite delicate and merits careful consideration.  One trouble spot for substitution is to notice that if x is bound by an abstractor within a, then x does not occur free within the abstractor and hence is unchanged by substitution. For example, [b x]let a1; x.a2  = let [b x]a1; x.a2 , there being no free occurrences of x in x.a2. Another trouble spot is the capture of a free variable of b during substitution. For example, if y ∈ b and x  cid:6 = y, then [b x]let a1; y.a2  is undeﬁned, rather than being let [b x]a1; y.[b x]a2 , as one might at ﬁrst suspect. For example, provided that x  cid:6 = y, [y x]let num[0]; y.plus x; y   is undeﬁned, not let num[0]; y.plus y; y  , which confuses two different variables named y.  Although capture avoidance is an essential characteristic of substitution, it is, in a sense, merely a technical nuisance. If the names of bound variables have no signiﬁcance, then capture can always be avoided by ﬁrst renaming the bound variables in a to avoid any free variables in b. In the foregoing example, if we rename the bound variable y to  cid:5  to obtain a  cid:5  is deﬁned and is equal to y  cid:5   . The price for avoiding capture in this way is that substitution  cid:5  let num[0]; y is only determined up to α-equivalence, and so we may no longer think of substitution as a function but only as a proper relation.   cid:5   cid:2  let num[0]; y .plus b; y   cid:5   , then [b x]a  .plus x; y   cid:5    10  Abstract Syntax  To restore the functional character of substitution, it is sufﬁcient to adopt the identiﬁcation  convention, which is stated as follows:  Abstract binding trees are always identiﬁed up to α-equivalence.  That is, α-equivalent abt’s are regarded as identical. Substitution can be extended to α- equivalence classes of abt’s to avoid capture by choosing representatives of the equivalence classes of b and a in such a way that substitution is deﬁned, then forming the equiv- alence class of the result. Any two choices of representatives for which substitution is deﬁned gives α-equivalent results, so that substitution becomes a well-deﬁned total func- tion. We will adopt the identiﬁcation convention for abt’s throughout this book.  It will often be necessary to consider languages whose abstract syntax cannot be speciﬁed by a ﬁxed set of operators but rather requires that the available operators be sensitive to the context in which they occur. For our purposes, it will sufﬁce to consider a set of symbolic parameters, or symbols, that index families of operators so that as the set of symbols varies, so does the set of operators. An indexed operator o is a family of operators indexed by symbols u, so that o[u] is an operator when u is an available symbol. If U is a ﬁnite set of symbols, thenB[U;X ] is the sort-indexed family of abt’s that are generated by operators and variables as before, admitting all indexed operator instances by symbols u ∈ U. Whereas a variable is a place-holder that stands for an unknown abt of its sort, a symbol does not stand for anything, and is not, itself, an abt. The only signiﬁcance of symbol is whether it  cid:5 ] are the is the same as or differs from another symbol; the operator instances o[u] and o[u same exactly when u is u   cid:5  and are the same symbol.  The set of symbols is extended by introducing a new, or fresh, symbol within a scope using the abstractor u.a, which binds the symbol u within the abt a. An abstracted symbol is “new” in the same sense as for an abstracted variable: the name of the bound symbol can be varied at will provided that no conﬂicts arise. This renaming property ensures that an abstracted symbol is distinct from all others in scope. The only difference between symbols and variables is that the only operation on symbols is renaming; there is no notion of substitution for a symbol.  Finally, a word about notation: to help improve the readability we often “group” and “stage” the arguments to an operator, using round brackets and braces to show grouping, and generally regarding stages to progress from right to left. All arguments in a group are consid- ered to occur at the same stage, though their order is signiﬁcant, and successive groups are considered to occur in sequential stages. Staging and grouping is often a helpful mnemonic device, but has no fundamental signiﬁcance. For example, the abt o{a1; a2} a3; x.a4  is the same as the abt o a1; a2; a3; x.a4 , as would be any other order-preserving grouping or staging of its arguments.  1.3 Notes  The concept of abstract syntax has its origins in the pioneering work of Church, Turing, and G¨odel, who ﬁrst considered writing programs that act on representations of programs.   11  Exercises  Originally, programs were represented by natural numbers, using encodings, now called G¨odel-numberings, based on the prime factorization theorem. Any standard text on mathe- matical logic, such as Kleene  1952 , has a thorough account of such representations. The Lisp language  McCarthy, 1965; Allen, 1978  introduced a much more practical and direct representation of syntax as symbolic expressions. These ideas were developed further in the language ML  Gordon et al., 1979 , which featured a type system capable of expressing abstract syntax trees. The AUTOMATH project  Nederpelt et al., 1994  introduced the idea of using Church’s λ notation  Church, 1941  to account for the binding and scope of variables. These ideas were developed further in LF  Harper et al., 1993 .  The concept of abstract binding trees presented here was inspired by the system of notation developed in the NuPRL Project, which is described in Constable  1986  and from Martin-L¨of’s system of arities, which is described in Nordstrom et al.  1990 . Their enrichment with symbol binders is inﬂuenced by Pitts and Stark  1993 .  Exercises  1.1. Prove by structural induction on abstract syntax trees that if X ⊆ Y, then A[X ] ⊆ A[Y]. 1.2. Prove by structural induction modulo renaming on abstract binding trees that ifX ⊆ Y, then B[X ] ⊆ B[Y]. 1.3. Show that if a =α a [b x]a =α [b  cid:5 .   cid:5  and both [b x]a and [b   cid:5  are deﬁned, then   cid:5  and b =α b   x]a   x]a   cid:5    cid:5   1.4. Bound variables can be seen as the formal analogs of pronouns in natural languages. The binding occurrence of a variable at an abstractor ﬁxes a “fresh” pronoun for use within its body that refers unambiguously to that variable  in contrast to English, in which the referent of a pronoun can often be ambiguous . This observation suggests an alternative representation of abt’s, called abstract binding graphs, or abg’s for short, as directed graphs constructed as follows:  a  Free variables are atomic nodes with no outgoing edges.  b  Operators with n arguments are n-ary nodes, with one outgoing edge directed at  each of their children.   c  Abstractors are nodes with one edge directed to the scope of the abstracted variable.  d  Bound variables are back edges directed at the abstractor that introduced it. Notice that ast’s, thought of as abt’s with no abstractors, are acyclic directed graphs  more precisely, variadic trees , whereas general abt’s can be cyclic. Draw a few examples of abg’s corresponding to the example abt’s given in this chapter. Give a precise deﬁnition of the sort-indexed family G[X ] of abstract binding graphs. What representation would you use for bound variables  back edges ?   2  Inductive Deﬁnitions  Inductive deﬁnitions are an indispensable tool in the study of programming languages. In this chapter we will develop the basic framework of inductive deﬁnitions and give some examples of their use. An inductive deﬁnition consists of a set of rules for deriving judgments, or assertions, of a variety of forms. Judgments are statements about one or more abstract binding trees of some sort. The rules specify necessary and sufﬁcient conditions for the validity of a judgment, and hence fully determine its meaning.  2.1 Judgments  We start with the notion of a judgment, or assertion, about an abstract binding tree. We shall make use of many forms of judgment, including examples such as these:  n nat  n1 + n2 = n  τ type e : τ e ⇓ v  n is a natural number n is the sum of n1 and n2 τ is a type expression e has type τ expression e has value v  A judgment states that one or more abstract binding trees have a property or stand in some relation to one another. The property or relation itself is called a judgment form, and the judgment that an object or objects have that property or stand in that relation is said to be an instance of that judgment form. A judgment form is also called a predicate, and the objects constituting an instance are its subjects. We write a J or J a, for the judgment asserting that J holds of the abt a. Correspondingly, we sometimes notate the judgment form J by − J, or J −, using a dash to indicate the absence of an argument to J. When it is not important to stress the subject of the judgment, we write J to stand for an unspeciﬁed judgment, that is, an instance of some judgment form. For particular judgment forms, we freely use preﬁx, inﬁx, or mix-ﬁx notation, as illustrated by the above examples, in order to enhance readability.  2.2 Inference Rules  An inductive deﬁnition of a judgment form consists of a collection of rules of the form  J1  . . . J  Jk   2.1    13  2.2 Inference Rules  in which J and J1, . . . , Jk are all judgments of the form being deﬁned. The judgments above the horizontal line are called the premises of the rule, and the judgment below the line is called its conclusion. If a rule has no premises  that is, when k is zero , the rule is called an axiom; otherwise, it is called a proper rule.  An inference rule can be read as stating that the premises are sufﬁcient for the conclusion: to show J , it is enough to show J1, . . . , Jk. When k is zero, a rule states that its conclusion holds unconditionally. Bear in mind that there may be, in general, many rules with the same conclusion, each specifying sufﬁcient conditions for the conclusion. Consequently, if the conclusion of a rule holds, then it is not necessary that the premises hold, for it might have been derived by another rule. For example, the following rules form an inductive deﬁnition of the judgment form− nat:  These rules specify that a nat holds whenever either a is zero, ora is succ b  where b nat for some b. Taking these rules to be exhaustive, it follows that a nat iff a is a natural number. Similarly, form − tree:  the following rules constitute an inductive deﬁnition of the judgment  These rules specify that a tree holds if either a is empty, ora is node a1;a2 , where a1 tree and a2 tree. Taking these to be exhaustive, these rules state that a is a binary tree, which is to say it is either empty, or a node consisting of two children, each of which is also a binary tree.  The judgment form a is b expresses the equality of two abt’s a and b such that a nat  and b nat is inductively deﬁned by the following rules:  zero nat  a nat  succ a  nat  empty tree  a1 tree a2 tree node a1;a2  tree  zero is zero  a is b  succ a  is succ b    2.2a    2.2b    2.3a    2.3b    2.4a    2.4b   In each of the preceding examples, we have made use of a notational convention for specifying an inﬁnite family of rules by a ﬁnite number of patterns, or rule schemes. For example, rule  2.2b  is a rule scheme that determines one rule, called an instance of the rule scheme, for each choice of object a in the rule. We will rely on context to determine whether a rule is stated for a speciﬁc object a or is instead intended as a rule scheme specifying a rule for each choice of objects in the rule.   14  Inductive Deﬁnitions  A collection of rules is considered to deﬁne the strongest judgment form that is closed under, or respects, those rules. To be closed under the rules simply means that the rules are sufﬁcient to show the validity of a judgment: J holds if there is a way to obtain it using the given rules. To be the strongest judgment form closed under the rules means that the rules are also necessary: J holds only if there is a way to obtain it by applying the rules. The sufﬁciency of the rules means that we may show that J holds by deriving it by composing rules. Their necessity means that we may reason about it using rule induction.  2.3 Derivations  To show that an inductively deﬁned judgment holds, it is enough to exhibit a derivation of it. A derivation of a judgment is a ﬁnite composition of rules, starting with axioms and ending with that judgment. It can be thought of as a tree in which each node is a rule whose children are derivations of its premises. We sometimes say that a derivation of J is evidence for the validity of an inductively deﬁned judgment J .  We usually depict derivations as trees with the conclusion at the bottom, and with the children of a node corresponding to a rule appearing above it as evidence for the premises of that rule. Thus, if  is an inference rule and  1, . . . ,  k are derivations of its premises, then   cid:2    cid:2   J1   cid:2   1  . . . J  . . .  J  Jk   cid:2   k  is a derivation of its conclusion. In particular, if k = 0, then the node has no children.  For example, this is a derivation of succ succ succ zero    nat:  Similarly, here is a derivation of node node empty;empty ;empty  tree:  zero nat  succ zero  nat  succ succ zero   nat  succ succ succ zero    nat  .  empty tree empty tree node empty;empty  tree empty tree node node empty;empty ;empty  tree  .   2.5    2.6   To show that an inductively deﬁned judgment is derivable, we need only ﬁnd a deriva- tion for it. There are two main methods for ﬁnding derivations, called forward chaining, or bottom-up construction, and backward chaining, or top-down construction. Forward   15  2.4 Rule Induction  chaining starts with the axioms and works forward towards the desired conclusion, whereas backward chaining starts with the desired conclusion and works backwards towards the axioms.  More precisely, forward chaining search maintains a set of derivable judgments and continually extends this set by adding to it the conclusion of any rule all of whose premises are in that set. Initially, the set is empty; the process terminates when the desired judgment occurs in the set. Assuming that all rules are considered at every stage, forward chaining will eventually ﬁnd a derivation of any derivable judgment, but it is impossible  in general  to decide algorithmically when to stop extending the set and conclude that the desired judgment is not derivable. We may go on and on adding more judgments to the derivable set without ever achieving the intended goal. It is a matter of understanding the global properties of the rules to determine that a given judgment is not derivable.  Forward chaining is undirected in the sense that it does not take account of the end goal when deciding how to proceed at each step. In contrast, backward chaining is goal-directed. Backward chaining search maintains a queue of current goals, judgments whose derivations are to be sought. Initially, this set consists solely of the judgment we wish to derive. At each stage, we remove a judgment from the queue and consider all rules whose conclusion is that judgment. For each such rule, we add the premises of that rule to the back of the queue, and continue. If there is more than one such rule, this process must be repeated, with the same starting queue, for each candidate rule. The process terminates whenever the queue is empty, all goals having been achieved; any pending consideration of candidate rules along the way can be discarded. As with forward chaining, backward chaining will eventually ﬁnd a derivation of any derivable judgment, but there is, in general, no algorithmic method for determining in general whether the current goal is derivable. If it is not, we may futilely add more and more judgments to the goal set, never reaching a point at which all goals have been satisﬁed.  2.4 Rule Induction  Because an inductive deﬁnition speciﬁes the strongest judgment form closed under a collection of rules, we may reason about them by rule induction. The principle of rule induction states that to show that a property a P holds whenever a J is derivable, it is enough to show that P is closed under, or respects, the rules deﬁning the judgment form J. More precisely, the property P respects the rule  a1 J  ak J  . . . a J  if P a  holds whenever P a1 , . . . ,P ak  do. The assumptions P a1 , . . . ,P ak  are called the inductive hypotheses, and P a  is called the inductive conclusion of the inference.  The principle of rule induction is simply the expression of the deﬁnition of an inductively deﬁned judgment form as the strongest judgment form closed under the rules comprising the deﬁnition. Thus, the judgment form deﬁned by a set of rules is both  a  closed under   16  Inductive Deﬁnitions  those rules, and  b  sufﬁcient for any other property also closed under those rules. The former means that a derivation is evidence for the validity of a judgment; the latter means that we may reason about an inductively deﬁned judgment form by rule induction. When specialized to rules  2.2 , the principle of rule induction states that to show P a   whenever a nat, it is enough to show:  1. P zero . 2. for every a, if P a , then P succ a  .  The sufﬁciency of these conditions is the familiar principle of mathematical induction.  Similarly, rule induction for rules  2.3  states that to show P a  whenever a tree, it is  enough to show  1. P empty . 2. for every a1 and a2, ifP  a1 , and if P a2 , then P node a1;a2  .  The sufﬁciency of these conditions is called the principle of tree induction.  We may also show by rule induction that the predecessor of a natural number is also a natural number. Although this may seem self-evident, the point of the example is to show how to derive this from ﬁrst principles.  Lemma 2.1. If succ a  nat, then a nat.  It sufﬁces to show that the property P a  stating that a nat and that a = succ b   Proof implies b nat is closed under rules  2.2 .  Rule  2.2a  Clearly zero nat, and the second condition holds vacuously, because zero  is not of the form succ − .  Rule  2.2b  Inductively, we know that a nat and that if a is of the form succ b , then b nat. We are to show that succ a  nat, which is immediate, and that if succ a  is of the form succ b , then b nat, and we have b nat by the inductive hypothesis.  Using rule induction, we may show that equality, as deﬁned by rules  2.4  is reﬂexive.  Lemma 2.2. If a nat, then a is a.  Proof By rule induction on rules  2.2 :  Rule  2.2a  Applying rule  2.4a  we obtain zero is zero. Rule  2.2b  Assume that a is a. It follows that succ a  is succ a  by an application of  rule  2.4b .  Similarly, we may show that the successor operation is injective.   17  2.5 Iterated and Simultaneous Inductive Deﬁnitions  Lemma 2.3. If succ a1  is succ a2 , then a1 is a2.  Proof Similar to the proof of Lemma 2.1.  2.5 Iterated and Simultaneous Inductive Deﬁnitions  Inductive deﬁnitions are often iterated, meaning that one inductive deﬁnition builds on top of another. In an iterated inductive deﬁnition, the premises of a rule  may be instances of either a previously deﬁned judgment form, or the judgment form being deﬁned. For example, the following rules deﬁne the judgment form − list, which states that a is a list of natural numbers:  The ﬁrst premise of rule  2.7b  is an instance of the judgment form a nat, which was deﬁned previously, whereas the premise b list is an instance of the judgment form being deﬁned by these rules.  Frequently two or more judgments are deﬁned at once by a simultaneous inductive deﬁnition. A simultaneous inductive deﬁnition consists of a set of rules for deriving instances of several different judgment forms, any of which may appear as the premise of any rule. Because the rules deﬁning each judgment form may involve any of the others, none of the judgment forms can be taken to be deﬁned prior to the others. Instead, we must understand that all of the judgment forms are being deﬁned at once by the entire collection of rules. The judgment forms deﬁned by these rules are, as before, the strongest judgment forms that are closed under the rules. Therefore, the principle of proof by rule induction continues to apply, albeit in a form that requires us to prove a property of each of the deﬁned judgment forms simultaneously.  For example, consider the following rules, which constitute a simultaneous inductive deﬁnition of the judgments a even, stating that a is an even natural number, and a odd, stating that a is an odd natural number:  J1  . . . J  Jk  nil list  a nat b list cons a;b  list  zero even  b odd  succ b  even  a even  succ a  odd   2.7a    2.7b    2.8a    2.8b    2.8c    18  Inductive Deﬁnitions  The principle of rule induction for these rules states that to show simultaneously that  P a  whenever a even and Q b  whenever b odd, it is enough to show the following: 1. P zero ; 2. if Q b , then P succ b  ; 3. if P a , then Q succ a  .  As an example, we may use simultaneous rule induction to prove that  1  if a even, then either a is zero or a is succ b  with b odd, and  2  if a odd, then a is succ b  with b even. We deﬁne P a  to hold iff a is zero or a is succ b  for some b with b odd, and deﬁne Q b  to hold iff b is succ a  for somea with a even. The desired result follows by rule induction, because we can prove the following facts: 1. P zero , which holds because zero is zero. 2. If Q b , then succ b  is succ b  cid:5   for some b 3. If P a , then succ a  issucc a   cid:5  with Q b  cid:5  with P a   cid:5  to be b and apply the   cid:5  to be a and apply the  inductive assumption.   cid:5   for some a   cid:5  . Take b   cid:5  . Take a  inductive assumption.  2.6 Deﬁning Functions by Rules  A common use of inductive deﬁnitions is to deﬁne a function by giving an inductive deﬁnition of its graph relating inputs to outputs, and then showing that the relation uniquely determines the outputs for given inputs. For example, we may deﬁne the addition function on natural numbers as the relation sum a;b;c , with the intended meaning that c is the sum of a and b, as follows:  b nat  sum zero;b;b   sum a;b;c   sum succ a ;b;succ c     2.9a    2.9b   The rules deﬁne a ternary  three-place  relation sum a;b;c  among natural numbers a, b, and c. We may show that c is determined by a and b in this relation.  Theorem 2.4. For every a nat and b nat, there exists a unique c nat such that sum a;b;c .  Proof The proof decomposes into two parts:  1.  Existence  If a nat and b nat, then there exists c nat such that sum a;b;c . 2.  Uniqueness  If sum a;b;c , and sum a;b;c   cid:5  , then c is c   cid:5 .   19  Exercises  For existence, let P a  be the proposition if b nat then there exists c nat such that sum a;b;c . We prove that if a nat then P a  by rule induction on rules  2.2 . We have two cases to consider:  sum zero;b;c  by rule  2.9a .  Rule  2.2a  We are to showP  zero . Assuming b nat and taking c to be b, we obtain Rule  2.2b  Assuming P a , we are to showP  succ a  . That is, we assume that if nat, then nat. Then by  cid:5  to be succ c , and applying  b nat then there exists c such that sum a;b;c  and are to show that if b  cid:5  . To this end, suppose that b  cid:5 ;c  cid:5  such that sum succ a ;b there exists c  cid:5 ;c . Taking c induction there exists c such that sum a;b  cid:5  , as required.  cid:5 ;c rule  2.9b , we obtain sum succ a ;b   cid:5    cid:5   For uniqueness, we prove that if sum a;b;c1 , then if sum a;b;c2 , then c1 is c2 by rule induction based on rules  2.9 .  Rule  2.9a  We have a is zero and c1 is b. By an inner induction on the same rules, we  may show that if sum zero;b;c2 , then c2 is b. By Lemma 2.2, we obtain b is b.  Rule  2.9b  We have that a is succ a   cid:5   and c1 is succ c  inner induction on the same rules, we may show that if sum a;b;c2 , then c2 issucc c where sum a   cid:5  2 . By the outer inductive hypothesis, c   cid:5  2 and so c1 is c2.   cid:5 ;b;c   cid:5  1 , where sum a  cid:5  1 is c   cid:5 ;b;c   cid:5  1 . By an  cid:5  2   Aczel  1977  provides a thorough account of the theory of inductive deﬁnitions on which the present account is based. A signiﬁcant difference is that we consider inductive deﬁnitions of judgments over abt’s as deﬁned in Chapter 1, rather than with natural numbers. The emphasis on judgments is inspired by Martin-L¨of’s logic of judgments  Martin-L¨of, 1983, 1987 .  2.7 Notes  Exercises  2.1. Give an inductive deﬁnition of the judgment max m;n;p , where m nat, n nat, and p nat, with the meaning that p is the larger of m and n. Prove that every m and n are related to a unique p by this judgment.  2.2. Consider the following rules, which deﬁne the judgment hgt t;n  stating that the binary  tree t has height n.  hgt empty;zero   hgt t1;n1  hgt t2;n2  max n1;n2;n   hgt node t1;t2 ;succ n     2.10a    2.10b   Prove that the judgment hgt deﬁnes a function from trees to natural numbers.   20  Inductive Deﬁnitions  2.3. Given an inductive deﬁnition of ordered variadic trees whose nodes have a ﬁnite, but variable, number of children with a speciﬁed left-to-right ordering among them. Your solution should consist of a simultaneous deﬁnition of two judgments, t tree, stating that t is a variadic tree, and f forest, stating that f is a “forest”  ﬁnite sequence  of variadic trees.  2.4. Give an inductive deﬁnition of the height of a variadic tree of the kind deﬁned in Exercise 2.3. Your deﬁnition should make use of an auxiliary judgment deﬁning the height of a forest of variadic trees and will be deﬁned simultaneously with the height of a variadic tree. Show that the two judgments so deﬁned each deﬁne a function.  2.5. Give an inductive deﬁnition of the binary natural numbers, which are either zero, twice a binary number, or one more than twice a binary number. The size of such a representation is logarithmic, rather than linear, in the natural number it represents.  2.6. Give an inductive deﬁnition of addition of binary natural numbers as deﬁned in Exer- cise 2.5. Hint: Proceed by analyzing both arguments to the addition, and make use of an auxiliary function to compute the successor of a binary number. Hint: Alternatively, deﬁne both the sum and the sum-plus-one of two binary numbers mutually recursively.   3  Hypothetical and General  Judgments  A hypothetical judgment expresses an entailment between one or more hypotheses and a conclusion. We will consider two notions of entailment, called derivability and admissibil- ity. Both express a form of entailment, but they differ in that derivability is stable under extension with new rules, admissibility is not. A general judgment expresses the universal- ity, or genericity, of a judgment. There are two forms of general judgment, the generic and the parametric. The generic judgment expresses generality with respect to all substitution instances for variables in a judgment. The parametric judgment expresses generality with respect to renamings of symbols.  3.1 Hypothetical Judgments  The hypothetical judgment codiﬁes the rules for expressing the validity of a conclusion conditional on the validity of one or more hypotheses. There are two forms of hypothetical judgment that differ according to the sense in which the conclusion is conditional on the hypotheses. One is stable under extension with more rules, and the other is not.  3.1.1 Derivability  For a given set R of rules, we deﬁne the derivability judgment, written J1, . . . , Jk  cid:12 R K, where each Ji and K are basic judgments, to mean that we may derive K from the expansion R ∪ { J1, . . . , Jk } of the rules R with the axioms  J1  . . .  .  Jk  We treat the hypotheses, or antecedents, of the judgment, J1, . . . , Jk as “temporary axioms,” and derive the conclusion, or consequent, by composing rules in R. Thus, evidence for a hypothetical judgment consists of a derivation of the conclusion from the hypotheses using the rules in R. We use capital Greek letters, usually  cid:7  or  cid:8 , to stand for a ﬁnite set of basic judgments, and write R ∪  cid:7  for the expansion of R with an axiom corresponding to each judgment in  cid:7 . The judgment  cid:7   cid:12 R K means that K is derivable from rules R ∪  cid:7 , and the judgment  cid:12 R  cid:7  means that  cid:12 R J for each J in  cid:7 . An equivalent way of deﬁning J1, . . . , Jn  cid:12 R J is    3.2    3.3    3.4   22  Hypothetical and General Judgments  to say that the rule   3.1  is derivable from R, which means that there is a derivation of J composed of the rules in R augmented by treating J1, . . . , Jn as axioms. For example, consider the derivability judgment  J1  . . . J  Jn  a nat  cid:12  2.2  succ succ a   nat  relative to rules  2.2 . This judgment is valid for any choice of object a, as shown by the derivation  a nat  succ a  nat  succ succ a   nat  a nat  succ succ a   nat  which composes rules  2.2 , starting with a nat as an axiom, and ending with succ succ a   nat. Equivalently, the validity of  3.2  may also be expressed by stating that the rule  is derivable from rules  2.2 .  It follows directly from the deﬁnition of derivability that it is stable under extension with  new rules. Theorem 3.1  Stability . If  cid:7   cid:12 R J , then  cid:7   cid:12 R∪R cid:5  J . Proof Any derivation of J from R ∪  cid:7  is also a derivation from  R ∪ R cid:5   ∪  cid:7 , because any rule in R is also a rule inR ∪ R cid:5 .  Derivability enjoys a number of structural properties that follow from its deﬁnition,  independently of the rules R in question.  justiﬁes itself as conclusion.  Reﬂexivity Every judgment is a consequence of itself:  cid:7 , J  cid:12 R J . Each hypothesis Weakening If  cid:7   cid:12 R J , then  cid:7 , K  cid:12 R J . Entailment is not inﬂuenced by un-exercised Transitivity If  cid:7 , K  cid:12 R J and  cid:7   cid:12 R K, then  cid:7   cid:12 R J . If we replace an axiom by a  options.  derivation of it, the result is a derivation of its consequent without that hypothesis.  Reﬂexivity follows directly from the meaning of derivability. Weakening follows directly from the deﬁnition of derivability. Transitivity is proved by rule induction on the ﬁrst premise.   23  3.1 Hypothetical Judgments  3.1.2 Admissibility  Admissibility, written  cid:7  =R J , is a weaker form of hypothetical judgment stating that  cid:12 R  cid:7  implies  cid:12 R J . That is, the conclusion J is derivable from rules R when the assumptions  cid:7  are all derivable from rules R. In particular if any of the hypotheses are not derivable relative to R, then the judgment is vacuously true. An equivalent way to deﬁne the judgment J1, . . . , Jn =R J is to state that the rule J1  Jn   3.5  is admissible relative to the rules in R. Given any derivations of J1, . . . , Jn using the rules in R, we may build a derivation of J using the rules in R.  . . . J  For example, the admissibility judgment  succ a  even = 2.8  a odd  is valid, because any derivation of succ a  even from rules  2.2  must contain a sub- derivation of a odd from the same rules, which justiﬁes the conclusion. This fact can be proved by induction on rules  2.8 . That judgment  3.6  is valid may also be expressed by saying that the rule  is admissible relative to rules  2.8 .  In contrast to derivability the admissibility judgment is not stable under extension to the  rules. For example, if we enrich rules  2.8  with the axiom  succ a  even  a odd  succ zero  even  ,   3.6    3.7    3.8   then rule  3.6  is inadmissible, because there is no composition of rules deriving zero odd. Admissibility is as sensitive to which rules are absent from an inductive deﬁnition as it is to which rules are present in it.  The structural properties of derivability ensure that derivability is stronger than admissi-  bility. Theorem 3.2. If  cid:7   cid:12 R J , then  cid:7  =R J . Proof Repeated application of the transitivity of derivability shows that if  cid:7   cid:12 R J and  cid:12 R  cid:7 , then  cid:12 R J .  To see that the converse fails, note that  succ zero  even  cid:6  cid:12  2.8  zero odd,  because there is no derivation of the right-hand side when the left-hand side is added as an axiom to rules  2.8 . Yet the corresponding admissibility judgment  succ zero  even = 2.8  zero odd   24  Hypothetical and General Judgments  is valid, because the hypothesis is false: there is no derivation of succ zero  even from rules  2.8 . Even so, the derivability  succ zero  even  cid:12  2.8  succ succ zero   odd  is valid, because we may derive the right-hand side from the left-hand side by composing rules  2.8 .  Evidence for admissibility can be thought of as a mathematical function transforming derivations  cid:3 1, . . . ,  cid:3 n of the hypotheses into a derivation  cid:3  of the consequent. Therefore, the admissibility judgment enjoys the same structural properties as derivability and hence is a form of hypothetical judgment:  Reﬂexivity If J is derivable from the original rules, then J is derivable from the original  rules: J =R J .  Weakening If J is derivable from the original rules assuming that each of the judgments in  cid:7  are derivable from these rules, then J must also be derivable assuming that  cid:7  and K are derivable from the original rules: if  cid:7  =R J , then  cid:7 , K =R J . Transitivity If  cid:7 , K =R J and  cid:7  =R K, then  cid:7  =R J . If the judgments in  cid:7  are derivable, so is K, by assumption, and hence so are the judgments in  cid:7 , K, and hence so is J .  Theorem 3.3. The admissibility judgment  cid:7  =R J enjoys the structural properties of entailment.  Proof Follows immediately from the deﬁnition of admissibility as stating that if the hypotheses are derivable relative to R, then so is the conclusion. If a rule r is admissible with respect to a rule set R, then  cid:12 R,r J is equivalent to  cid:12 R J . For if  cid:12 R J , then obviously  cid:12 R,r J , by simply disregarding r. Conversely, if  cid:12 R,r J , then we may replace any use of r by its expansion in terms of the rules in R. It follows by rule induction on R, r that every derivation from the expanded set of rules R, r can be transformed into a derivation from R alone. Consequently, if we wish to prove a property of the judgments derivable from R, r, when r is admissible with respect to R, it sufﬁces show that the property is closed under rules R alone, because its admissibility states that the consequences of rule r are implicit in those of rules R.  3.2 Hypothetical Inductive Deﬁnitions  It is useful to enrich the concept of an inductive deﬁnition to allow rules with derivability judgments as premises and conclusions. Doing so lets us introduce local hypotheses that apply only in the derivation of a particular premise, and also allows us to constrain inferences based on the global hypotheses in effect at the point where the rule is applied.   25  form:  3.2 Hypothetical Inductive Deﬁnitions  A hypothetical inductive deﬁnition consists of a set of hypothetical rules of the following   cid:7   cid:7 1  cid:12  J1  . . .  cid:7   cid:7 n  cid:12  Jn  cid:7   cid:12  J  .   3.9   The hypotheses  cid:7  are the global hypotheses of the rule, and the hypotheses  cid:7 i are the local hypotheses of the ith premise of the rule. Informally, this rule states that J is a derivable consequence of  cid:7  when each Ji is a derivable consequence of  cid:7 , augmented with the hypotheses  cid:7 i. Thus, one way to show that J is derivable from  cid:7  is to show, in turn, that each Ji is derivable from  cid:7   cid:7 i. The derivation of each premise involves a “context switch” in which we extend the global hypotheses with the local hypotheses of that premise, establishing a new set of global hypotheses for use within that derivation.  We require that all rules in a hypothetical inductive deﬁnition be uniform in the sense that they are applicable in all global contexts. Uniformity ensures that a rule can be presented in implicit, or local form,   cid:7 1  cid:12  J1  . . .  cid:7 n  cid:12  Jn  ,  J  in which the global context has been suppressed with the understanding that the rule applies for any choice of global hypotheses. A hypothetical inductive deﬁnition is to be regarded as an ordinary inductive deﬁnition of a formal derivability judgment  cid:7   cid:12  J consisting of a ﬁnite set of basic judgments  cid:7  and a basic judgment J . A set of hypothetical rules R deﬁnes the strongest formal derivability judgment that is structural and closed under uniform rules R. Structurality means that the formal derivability judgment must be closed under the following rules:   cid:7 , J  cid:12  J  cid:7   cid:12  J  cid:7 , K  cid:12  J   cid:7   cid:12  K  cid:7 , K  cid:12  J   cid:7   cid:12  J  These rules ensure that formal derivability behaves like a hypothetical judgment. We write  cid:7   cid:12 R J to mean that  cid:7   cid:12  J is derivable from rules R. The principle of hypothetical rule induction is just the principle of rule induction applied to the formal hypothetical judgment. So to show that P  cid:7   cid:12  J   when  cid:7   cid:12 R J , it is enough to show that P is closed under the rules of R and under the structural rules.1 Thus, for each rule of the form  3.9 , whether structural or in R, we must show that  if P  cid:7   cid:7 1  cid:12  J1  and . . . and P  cid:7   cid:7 n  cid:12  Jn , then P  cid:7   cid:12  J  .  But this is just a restatement of the principle of rule induction given in Chapter 2, specialized to the formal derivability judgment  cid:7   cid:12  J .  In practice, we usually dispense with the structural rules by the method described in Section 3.1.2. By proving that the structural rules are admissible, any proof by rule induction   3.10    3.11a    3.11b    3.11c    26  Hypothetical and General Judgments  may restrict attention to the rules in R alone. If all rules of a hypothetical inductive deﬁnition are uniform, the structural rules  3.11b  and  3.11c  are clearly admissible. Usually, rule  3.11a  must be postulated explicitly as a rule, rather than shown to be admissible on the basis of the other rules.  3.3 General Judgments  General judgments codify the rules for handling variables in a judgment. As in mathematics in general, a variable is treated as an unknown, ranging over a speciﬁed set of objects. A generic judgment states that a judgment holds for any choice of objects replacing designated variables in the judgment. Another form of general judgment codiﬁes the handling of symbolic parameters. A parametric judgment expresses generality over any choice of fresh renamings of designated symbols of a judgment. To keep track of the active variables and symbols in a derivation, we write  cid:7   cid:12 U;X R J to say that J is derivable from  cid:7  according to rules R, with objects consisting of abt’s over symbols U and variables X .  The concept of uniformity of a rule must be extended to require that rules be closed under renaming and substitution for variables and closed under renaming for parameters. More precisely, if R is a set of rules containing a free variable x of sort s, then it must also contain all possible substitution instances of abt’s a of sort s for x, including those that contain other free variables. Similarly, if R contains rules with a parameter u, then it must  cid:5  of the same sort. contain all instances of that rule obtained by renaming u of a sort to any u Uniformity rules out stating a rule for a variable, without also stating it for all instances of that variable. It also rules out stating a rule for a parameter without stating it for all possible renamings of that parameter.  Generic derivability judgment is deﬁned by  Y   cid:7   cid:12 X R J  iff  cid:7   cid:12 X Y  R J,  where Y ∩ X = ∅. Evidence for generic derivability consists of a generic derivation  cid:3  involving the variables X Y. So long as the rules are uniform, the choice of Y does not matter, in a sense to be explained shortly. For example, the generic derivation  cid:3 ,  x nat  succ x  nat  succ succ x   nat  ,  is evidence for the judgment  x  x nat  cid:12 X   2.2  succ succ x   nat  provided x  ∈ X . Any other choice of x would work just as well, as long as all rules are uniform.   27  3.4 Generic Inductive Deﬁnitions  The generic derivability judgment enjoys the following structural properties governing  the behavior of variables, provided that R is uniform.  Proliferation If Y   cid:7   cid:12 X Renaming If Y, y   cid:7   cid:12 X Substitution If Y, y   cid:7   cid:12 X  R J , then Y, y   cid:7   cid:12 X R J , then Y, y R J and a ∈ B[X Y], then Y  [a y] cid:7   cid:12 X  R J .  cid:5   [y ↔ y  R [y ↔ y   cid:5 ] cid:7   cid:12 X   cid:5 ]J for any y R [a y]J .   cid:5    ∈ X Y.  Proliferation is guaranteed by the interpretation of rule schemes as ranging over all expan- sions of the universe. Renaming is built into the meaning of the generic judgment. It is left implicit in the principle of substitution that the substituting abt is of the same sort as the substituted variable.  Parametric derivability is deﬁned analogously to generic derivability, albeit by general-  izing over symbols, rather than variables. Parametric derivability is deﬁned by  V  cid:16  Y   cid:7   cid:12 U;X R J  iff Y   cid:7   cid:12 U V;X  R  J,  where V ∩U = ∅. Evidence for parametric derivability consists of a derivation  cid:3  involving the symbols V. Uniformity of R ensures that any choice of parameter names is as good as any other; derivability is stable under renaming.  3.4 Generic Inductive Deﬁnitions  A generic inductive deﬁnition admits generic hypothetical judgments in the premises of rules, with the effect of augmenting the variables, as well as the rules, within those premises. A generic rule has the form  Y Y1   cid:7   cid:7 1  cid:12  J1  . . . Y Yn   cid:7   cid:7 n  cid:12  Jn   3.12  The variables Y are the global variables of the inference, and, for each 1 ≤ i ≤ n, the variables Yi are the local variables of the ith premise. In most cases, a rule is stated for all choices of global variables and global hypotheses. Such rules can be given in implicit form,  Y   cid:7   cid:12  J  .  Y1   cid:7 1  cid:12  J1  . . . Yn   cid:7 n  cid:12  Jn  .  J   3.13   A generic inductive deﬁnition is just an ordinary inductive deﬁnition of a family of formal generic judgments of the form Y   cid:7   cid:12  J . Formal generic judgments are identiﬁed up to Y cid:5   cid:2 ρ  cid:7    cid:12  cid:2 ρ J   for any renaming ρ : Y ↔ Y cid:5 . IfR is a collection of generic rules, we renaming of variables, so that the latter judgment is treated as identical to the judgment write Y   cid:7   cid:12 R J to mean that the formal generic judgment Y   cid:7   cid:12  J is derivable from rules R. When specialized to a set of generic rules, the principle of rule induction states that to show P Y   cid:7   cid:12  J   when Y   cid:7   cid:12 R J , it is enough to show that P is closed under the rules   28  Hypothetical and General Judgments  R. Speciﬁcally, for each rule in R of the form  3.12 , we must show that  if P Y Y1   cid:7   cid:7 1  cid:12  J1  . . . P Y Yn   cid:7   cid:7 n  cid:12  Jn  then P Y   cid:7   cid:12  J  .  By the identiﬁcation convention  stated in Chapter 1 , the property P must respect renam- ings of the variables in a formal generic judgment.  To ensure that the formal generic judgment behaves like a generic judgment, we must  always ensure that the following structural rules are admissible:  Y   cid:7 , J  cid:12  J Y   cid:7   cid:12  J Y   cid:7 , J  cid:5   cid:12  J Y   cid:7   cid:12  J Y, x   cid:7   cid:12  J  Y, x   cid:5   [x ↔ x   cid:5 ] cid:7   cid:12  [x ↔ x   cid:5 ]J  Y, x   cid:7   cid:12  J   cid:5   Y   cid:7   cid:12  J Y   cid:7 , J  cid:12  J  Y   cid:7   cid:12  J   cid:5  Y, x   cid:7   cid:12  J a ∈ B[Y] Y  [a x] cid:7   cid:12  [a x]J   3.14a    3.14b    3.14c    3.14d    3.14e    3.14f   The admissibility of rule  3.14a  is, in practice, ensured by explicitly including it. The admissibility of rules  3.14b  and  3.14c  is assured if each of the generic rules is uniform, because we may assimilate the added variable x to the global variables, and the added hypothesis J , to the global hypotheses. The admissibility of rule  3.14d  is ensured by the identiﬁcation convention for the formal generic judgment. Rule  3.14f  must be veriﬁed explicitly for each inductive deﬁnition. The concept of a generic inductive deﬁnition extends to parametric judgments as well. Brieﬂy, rules are deﬁned on formal parametric judgments of the form V  cid:16  Y   cid:7   cid:12  J , with symbols V, as well as variables, Y. Such formal judgments are identiﬁed up to renaming of its variables and its symbols to ensure that the meaning is independent of the choice of variable and symbol names.  3.5 Notes  The concepts of entailment and generality are fundamental to logic and programming lan- guages. The formulation given here builds on Martin-L¨of  1983, 1987  and Avron  1991 . Hypothetical and general reasoning are consolidated into a single concept in the AU- TOMATH languages  Nederpelt et al., 1994  and in the LF Logical Framework  Harper et al., 1993 . These systems allow arbitrarily nested combinations of hypothetical and   29  Exercises  general judgments, whereas the present account considers only general hypothetical judg- ments over basic judgment forms. On the other hand, we consider here symbols, as well as variables, which are not present in these previous accounts. Parametric judgments are required for specifying languages that admit the dynamic creation of “new” objects  see Chapter 34 .  3.1. Combinators are inductively deﬁned by the rule set C given as follows:  Give an inductive deﬁnition of the length of a combinator deﬁned as the number of occurrences of S and K within it.  3.2. The general judgment  x1, . . . , xn  x1 comb, . . . , xn comb  cid:12 C A comb  states that A is a combinator that may involve the variables x1, . . . , xn. Prove that if x  x comb  cid:12 C a2 comb and a1 comb, then [a1 x]a2 comb by induction on the derivation of the ﬁrst hypothesis of the implication. 3.3. Conversion, or equivalence, of combinators is expressed by the judgment A ≡ B deﬁned by the rule set E extending C as follows:2  Exercises  s comb  k comb  a1 comb a2 comb  ap a1;a2  comb  a ≡ a a comb a2 ≡ a1 a1 ≡ a2 a1 ≡ a3  cid:5  1  a1 ≡ a2 a1 ≡ a a1 a2 ≡ a  a2 ≡ a3 a2 ≡ a  cid:5   cid:5  1 a 2 k a1 a2 ≡ a1  a1 comb a2 comb   cid:5  2   3.15a    3.15b    3.15c    3.16a    3.16b    3.16c    3.16d    3.16e    3.16f   The no-doubt mysterious motivation for the last two equations will become clearer in a moment. For now, show that  a1 comb a2 comb a3 comb s a1 a2 a3 ≡  a1 a3   a2 a3   x  x comb  cid:12 C∪E s k kx ≡ x.   30  Hypothetical and General Judgments  3.4. Show that if x  x comb  cid:12 C a comb, then there is a combinator a   cid:5 , written [x] a and  called bracket abstraction, such that  Consequently, by Exercise 3.2, if a  Hint: Inductively deﬁne the judgment   cid:5   x ≡ a.  x  x comb  cid:12 C∪E a comb, then  cid:5  cid:5  ≡ [a  cid:5  cid:5    [x] a  a   cid:5  cid:5    x]a.  x  x comb  cid:12  absx a is a   cid:5   ,  where x  x comb  cid:12  a comb. Then argue that it deﬁnes a  cid:5  as a binary function of x and a. The motivation for the conversion axioms governing k and s should become clear while developing the proof of the desired equivalence.  3.5. Prove that bracket abstraction, as deﬁned in Exercise 3.4, isnon-compositional by  exhibiting a and b such that a comb and  x y  x comb y comb  cid:12 C b comb  such that [a y] [x] b   cid:6 = [x]  [a y]b . Hint: Consider the case that b is y.  Suggest a modiﬁcation to the deﬁnition of bracket abstraction that is compositional  by showing under the same conditions given above that [a y] [x] b  = [x]  [a y]b .  3.6. Consider the set B[X ] of abt’s generated by the operators ap, with arity  Exp, Exp Exp, and λ, with arity  Exp.Exp Exp, and possibly involving variables in X , all of which are of sort Exp. Give an inductive deﬁnition of the judgment b closed, which speciﬁes that b has no free occurrences of the variables in X . Hint: it is essential to give an inductive deﬁnition of the hypothetical, general judgment  x1, . . . , xn  x1 closed, . . . , xn closed  cid:12  b closed  in order to account for the binding of a variable by the λ operator. The hypothesis that a variable is closed seems self-contradictory in that a variable obviously occurs free in itself. Explain why this is not the case by examining carefully the meaning of the hypothetical and general judgments.  Notes  1 Writing P  cid:7   cid:12  J   is a mild abuse of notation in which the turnstile is used to separate the two arguments to P for the sake of readability. 2 The combinator ap a1;a2  is written a1 a2 for short, left-associatively when used in succession.   P A R T II  Statics and Dynamics    4  Statics  Most programming languages exhibit a phase distinction between the static and dynamic phases of processing. The static phase consists of parsing and type checking to ensure that the program is well-formed; the dynamic phase consists of execution of well-formed programs. A language is said to be safe exactly when well-formed programs are well- behaved when executed.  The static phase is speciﬁed by a statics comprising a set of rules for deriving typing judgments stating that an expression is well-formed of a certain type. Types mediate the interaction between the constituent parts of a program by “predicting” some aspects of the execution behavior of the parts so that we may ensure they ﬁt together properly at run-time. Type safety tells us that these predictions are correct; if not, the statics is considered to be improperly deﬁned, and the language is deemed unsafe for execution.  In this chapter, we present the statics of a simple expression language, E, as an illustration  of the method that we will employ throughout this book.  4.1 Syntax  When deﬁning a language we shall be primarily concerned with its abstract syntax, speciﬁed by a collection of operators and their arities. The abstract syntax provides a systematic, unambiguous account of the hierarchical and binding structure of the language and is considered the ofﬁcial presentation of the language. However, for the sake of clarity, it is also useful to specify minimal concrete syntax conventions, without going through the trouble to set up a fully precise grammar for it.  We will accomplish both of these purposes with a syntax chart, whose meaning is best illustrated by example. The following chart summarizes the abstract and concrete syntax of E.  Typ τ  Exp e  ::= num str ::= x  num str x n "s" e1 + e2 e1 ∗ e2 e1 ^ e2 e let x be e1 in e2  numbers strings variable numeral literal addition multiplication concatenation length deﬁnition  num[n] str[s] plus e1; e2  times e1; e2  cat e1; e2  len e  let e1; x.e2    34  Statics  This chart deﬁnes two sorts, Typ, ranged over by τ, and Exp, ranged over by e. The chart deﬁnes a set of operators and their arities. For example, it speciﬁes that the operator let has arity  Exp, Exp.Exp Exp, which speciﬁes that it has two arguments of sort Exp, and binds a variable of sort Exp in the second argument.  4.2 Type System  The role of a type system is to impose constraints on the formations of phrases that are sensitive to the context in which they occur. For example, whether the expression plus x; num[n]  is sensible depends on whether the variable x is restricted to have type num in the surrounding context of the expression. This example is, in fact, illustrative of the general case, in that the only information required about the context of an expression is the type of the variables within whose scope the expression lies. Consequently, the statics of E consists of an inductive deﬁnition of generic hypothetical judgments of the form   cid:8 x   cid:7   cid:12  e : τ,  where  cid:8 x is a ﬁnite set of variables, and  cid:7  is a typing context consisting of hypotheses of the form x : τ, one for each x ∈  cid:8 x. We rely on typographical conventions to determine the set of variables, using the letters x and y to stand for them. We write x  ∈ dom  cid:7   to say that there is no assumption in  cid:7  of the form x : τ for any type τ, in which case we say that the variable x is fresh for  cid:7 .  The rules deﬁning the statics of E are as follows:  cid:7 , x : τ  cid:12  x : τ  cid:7   cid:12  str[s] : str  cid:7   cid:12  num[n] : num   cid:7   cid:12  e1 : num  cid:7   cid:12  e2 : num   cid:7   cid:12  plus e1; e2  :num   cid:7   cid:12  e1 : num  cid:7   cid:12  e2 : num  cid:7   cid:12  times e1; e2  : num  cid:7   cid:12  e1 : str  cid:7   cid:12  e2 : str   cid:7   cid:12  cat e1; e2  :str  cid:7   cid:12  len e  : num   cid:7   cid:12  e : str   cid:7   cid:12  e1 : τ1  cid:7 , x : τ1  cid:12  e2 : τ2   cid:7   cid:12  let e1; x.e2  :τ 2   4.1a    4.1b    4.1c    4.1d    4.1e    4.1f    4.1g    4.1h   In rule  4.1h , we tacitly assume that the variable x is not already declared in  cid:7 . This condition may always be met by choosing a suitable representative of the α-equivalence class of the let expression.   35  4.3 Structural Properties  It is easy to check that every expression has at most one type by induction on typing,  which is rule induction applied to rules  4.1 .  Lemma 4.1  Unicity of Typing . For every typing context  cid:7  and expression e, there exists at most one τ such that  cid:7   cid:12  e : τ .  Proof By rule induction on rules  4.1 , making use of the fact that variables have at most one type in any typing context.  The typing rules are syntax-directed in the sense that there is exactly one rule for each form of expression. Consequently, it is easy to give necessary conditions for typing an expression that invert the sufﬁcient conditions expressed by the corresponding typing rule. Lemma 4.2  Inversion for Typing . Suppose that  cid:7   cid:12  e : τ . If e = plus e1; e2 , then τ = num,  cid:7   cid:12  e1 : num, and  cid:7   cid:12  e2 : num, and similarly for the other constructs of the language.  Proof These may all be proved by induction on the derivation of the typing judgment  cid:7   cid:12  e : τ.  In richer languages such inversion principles are more difﬁcult to state and to prove.  4.3 Structural Properties   cid:5  : τ   cid:5  : τ   cid:5 , then  cid:7 , x : τ  cid:12  e   cid:5  for any x  ∈ dom  cid:7   and  The statics enjoys the structural properties of the generic hypothetical judgment. Lemma 4.3  Weakening . If  cid:7   cid:12  e any type τ . Proof By induction on the derivation of  cid:7   cid:12  e rule  4.1h . We have that e assume z is chosen such that z  ∈ dom  cid:7   and z  cid:6 = x. By induction, we have 1.  cid:7 , x : τ  cid:12  e1 : τ1, 2.  cid:7 , x : τ, z : τ1  cid:12  e2 : τ   cid:5 . We will give one case here, for  cid:5  = let e1; z.e2 , where by the conventions on variables we may   cid:5  : τ   cid:5 ,  from which the result follows by rule  4.1h .  Lemma 4.4  Substitution . If  cid:7 , x : τ  cid:12  e   cid:5  : τ   cid:5  and  cid:7   cid:12  e : τ , then  cid:7   cid:12  [e x]e   cid:5  : τ   cid:5 .   36  Statics  Proof By induction on the derivation of  cid:7 , x : τ  cid:12  e rule  4.1h . As in the preceding case, e and z  ∈ dom  cid:7  . We have by induction and Lemma 4.3 that 1.  cid:7   cid:12  [e x]e1 : τ1, 2.  cid:7 , z : τ1  cid:12  [e x]e2 : τ   cid:5 .   cid:5 . We again consider only  cid:5  = let e1; z.e2 , where z is chosen so that z  cid:6 = x   cid:5  : τ  By the choice of z, we have  It follows by rule  4.1h  that  cid:7   cid:12  [e x]let e1; z.e2  : τ  [e x]let e1; z.e2  = let [e x]e1; z.[e x]e2 .  cid:5 , as desired.  From a programming point of view, Lemma 4.3 allows us to use an expression in any context that binds its free variables: if e is well-typed in a context  cid:7 , then we may “import” it into any context that includes the assumptions  cid:7 . In other words, introducing new variables beyond those required by an expression e does not invalidate e itself; it remains well-formed, with the same type.1 More importantly, Lemma 4.4 expresses the  cid:5  important concepts of modularity and linking. We may think of the expressions e and e  cid:5  is a client of the implementation e. The as two components of a larger system in which e client declares a variable specifying the type of the implementation and is type checked knowing only this information. The implementation must be of the speciﬁed type to satisfy the assumptions of the client. If so, then we may link them to form the composite system  cid:5 . This implementation may itself be the client of another component, represented by [e x]e a variable y that is replaced by that component during linking. When all such variables have been implemented, the result is a closed expression that is ready for execution  evaluation . The converse of Lemma 4.4 is called decomposition. It states that any  large  expression can be decomposed into a client and implementor by introducing a variable to mediate their interaction. Lemma 4.5  Decomposition . If  cid:7   cid:12  [e x]e we have  cid:7 , x : τ  cid:12  e   cid:5 , then for every type τ such that  cid:7   cid:12  e : τ ,   cid:5  : τ   cid:5  : τ   cid:5 .  Proof The typing of [e x]e   cid:5  depends only on the type of e wherever it occurs, if at all.  Lemma 4.5 tells us that any sub-expression can be isolated as a separate module of a larger system. This property is especially useful when the variable x occurs more than once  cid:5 , because then one copy of e sufﬁces for all occurrences of x in e in e The statics of E given by rules  4.1  exempliﬁes a recurrent pattern. The constructs of a language are classiﬁed into one of two forms, the introduction and the elimination. The introduction forms for a type determine the values, or canonical forms, of that type. The elimination forms determine how to manipulate the values of a type to form a computation of another  possibly the same  type. In the language E, the introduction forms for the type num are the numerals, and those for the type str are the literals. The elimination forms for   cid:5 .   37  Exercises  the type num are addition and multiplication, and those for the type str are concatenation and length.  The importance of this classiﬁcation will become clear once we have deﬁned the dynam- ics of the language in Chapter 5. Then we will see that the elimination forms are inverse to the introduction forms in that they “take apart” what the introduction forms have “put together.” The coherence of the statics and dynamics of a language expresses the concept of type safety, the subject of Chapter 6.  The concept of the static semantics of a programming language was historically slow to develop, perhaps because the earliest languages had relatively few features and only very weak type systems. The concept of a static semantics in the sense considered here was introduced in the deﬁnition of the Standard ML programming language  Milner et al., 1997 , building on earlier work by Church and others on the typed λ-calculus  Barendregt, 1992 . The concept of introduction and elimination, and the associated inversion principle, was introduced by Gentzen in his pioneering work on natural deduction  Gentzen, 1969 . These principles were applied to the structure of programming languages by Martin-L¨of  1984, 1980 .  4.4 Notes  Exercises  4.1. It is sometimes useful to give the typing judgment  cid:7   cid:12  e : τ an “operational” reading that speciﬁes more precisely the ﬂow of information required to derive a typing judgment  or determine that it is not derivable . The analytic mode corresponds to the context, expression, and type being given, with the goal to determine whether the typing judgment is derivable. The synthetic mode corresponds to the context and expression being given, with the goal to ﬁnd the unique type τ, if any, possessed by the expression in that context. These two readings can be made explicit as judgments of the form e ↓ τ, corresponding to the analytic mode, and e ↑ τ, corresponding to the synthetic mode.  Give a simultaneous inductive deﬁnition of these two judgments according to the  following guidelines:  a  Variables are introduced in synthetic form.  b  If we can synthesize a unique type for an expression, then we can analyze it with  respect to a given type by checking type equality.   c  Deﬁnitions need care, because the type of the deﬁned expression is not given, even  when the type of the result is given. There is room for variation; the point of the exercise is to explore the possibilities.   38  Statics  4.2. One way to limit the range of possibilities in the solution to Exercise 4.1 is to restrict and extend the syntax of the language so that every expression is either synthetic or analytic according to the following suggestions:  a  Variables are analytic.  b  Introduction forms are analytic, elimination forms are synthetic.  c  An analytic expression can be made synthetic by introducing a type cast of the form cast{τ} e  specifying that e must check against the speciﬁed type τ, which is synthesized for the whole expression.   d  The deﬁning expression of a deﬁnition must be synthetic, but the scope of the  deﬁnition can be either synthetic or analytic.  Reformulate your solution to Exercise 4.1 to take account of these guidelines.  Note  1 This point may seem so obvious that it is not worthy of mention, but, surprisingly, there are useful type systems that lack this property. Because they do not validate the structural principle of weakening, they are called substructural type systems.   5  Dynamics  The dynamics of a language describes how programs are executed. The most important way to deﬁne the dynamics of a language is by the method of structural dynamics, which deﬁnes a transition system that inductively speciﬁes the step-by-step process of executing a program. Another method for presenting dynamics, called contextual dynamics, is a variation of structural dynamics in which the transition rules are speciﬁed in a slightly different way. An equational dynamics presents the dynamics of a language by a collection of rules deﬁning when one program is deﬁnitionally equivalent to another.  5.1 Transition Systems  A transition system is speciﬁed by the following four forms of judgment:  1. s state, asserting that s is a state of the transition system. 2. s ﬁnal, where s state, asserting that s is a ﬁnal state. 3. s initial, where s state, asserting that s is an initial state. 4. s  cid:20 −→ s   cid:5 , where s state and s   cid:5   state, asserting that state s may transition to state s   cid:5 .   cid:5   state such that s  cid:20 −→ s  In practice, we always arrange things so that no transition is possible from a ﬁnal state: if  cid:5 . A state from which no transition is s ﬁnal, then there is no s possible is stuck. Whereas all ﬁnal states are, by convention, stuck, there may be stuck states in a transition system that are not ﬁnal. A transition system is deterministic iff for every  cid:5 ; otherwise, it is non-deterministic. state s there exists at most one state s A transition sequence is a sequence of states s0, . . . , sn such that s0 initial, and si  cid:20 −→ si+1 for every 0 ≤ i < n. A transition sequence is maximal iff there is no s such that sn  cid:20 −→ s, and it is complete iff it is maximal and sn ﬁnal. Thus, every complete transition sequence is maximal, but maximal sequences are not necessarily complete. The judgment s ↓ means that there is a complete transition sequence starting from s, which is to say that there exists ﬁnal such that s  cid:20 −→∗  cid:5  s The iteration of transition judgment s  cid:20 −→∗  cid:5  is inductively deﬁned by the following   cid:5  such that s  cid:20 −→ s   cid:5 .  s  s  rules:  s  cid:20 −→∗  cid:5  s  cid:20 −→∗  s  s  cid:5   cid:20 −→∗ s   cid:5  cid:5    cid:5  cid:5   s  s  cid:20 −→ s   5.1a    5.1b    40  Dynamics  When applied to the deﬁnition of iterated transition, the principle of rule induction states  cid:5 , it is enough to show these two properties   cid:5   holds when s  cid:20 −→∗  s  that to show that P  s, s of P :  1. P  s, s . 2. if s  cid:20 −→ s   cid:5  and P  s   cid:5   , s   cid:5  cid:5  , then P  s, s   cid:5  cid:5  .  The ﬁrst requirement is to show that P is reﬂexive. The second is to show that P is closed under head expansion, or closed under inverse evaluation. Using this principle, it is easy to prove that  cid:20 −→∗ is reﬂexive and transitive.  cid:5 , where n ≥ 0, is inductively deﬁned  The n-times iterated transition judgment s  cid:20 −→n s  by the following rules:  s  cid:20 −→0 s  cid:5   s  cid:20 −→ s   cid:5   cid:20 −→n s   cid:5  cid:5   s  s  cid:20 −→n+1 s   cid:5  cid:5   Theorem 5.1. For all states s and s   cid:5 , s  cid:20 −→∗   cid:5  iff s  cid:20 −→k s   cid:5  for some k ≥ 0.  s  Proof From left to right, by induction on the deﬁnition of multi-step transition. From right to left, by mathematical induction on k ≥ 0.  5.2 Structural Dynamics  A structural dynamics for the language E is given by a transition system whose states are closed expressions. All states are initial. The ﬁnal states are the  closed  values, which represent the completed computations. The judgment e val, which states that e is a value, is inductively deﬁned by the following rules:  num[n] val  The transition judgment e  cid:20 −→ e  str[s] val   5.3b   cid:5  between states is inductively deﬁned by the following  rules:  n1 + n2 = n  plus num[n1]; num[n2]   cid:20 −→ num[n]  e1  cid:20 −→ e   cid:5  1   cid:5  1; e2   plus e1; e2   cid:20 −→ plus e e2  cid:20 −→ e  cid:5  2  e1 val  plus e1; e2   cid:20 −→ plus e1; e  cid:5  2    5.2a    5.2b    5.3a    5.4a    5.4b    5.4c    41  5.2 Structural Dynamics  s1 ˆ s2 = s str  cat str[s1]; str[s2]   cid:20 −→ str[s]  e1  cid:20 −→ e   cid:5  1   cid:5  1; e2   cat e1; e2   cid:20 −→ cat e e2  cid:20 −→ e  cid:5  2  e1 val  cat e1; e2   cid:20 −→ cat e1; e  cid:5  2   e1  cid:20 −→ e   cid:5  1  let e1; x.e2   cid:20 −→ let e   cid:5  1; x.e2    cid:4    cid:3   [e1 val]  let e1; x.e2   cid:20 −→ [e1 x]e2   5.4d    5.4e    5.4f    5.4g    5.4h   We have omitted rules for multiplication and computing the length of a string, which follow a similar pattern. Rules  5.4a ,  5.4d , and  5.4h  are instruction transitions, because they correspond to the primitive steps of evaluation. The remaining rules are search transitions that determine the order of execution of instructions.  The bracketed rule  5.4g  and bracketed premise on rule  5.4h  are included for a by-value interpretation of let and omitted for a by-name interpretation. The by-value interpretation evaluates an expression before binding it to the deﬁned variable, whereas the by-name interpretation binds it in unevaluated form. The by-value interpretation saves work if the deﬁned variable is used more than once but wastes work if it is not used at all. Conversely, the by-name interpretation saves work if the deﬁned variable is not used and wastes work if it is used more than once.  A derivation sequence in a structural dynamics has a two-dimensional structure, with the number of steps in the sequence being its “width” and the derivation tree for each step being its “height.” For example, consider the following evaluation sequence:  let plus num[1]; num[2] ; x.plus plus x; num[3] ; num[4]     cid:20 −→ let num[3]; x.plus plus x; num[3] ; num[4]    cid:20 −→ plus plus num[3]; num[3] ; num[4]   cid:20 −→ plus num[6]; num[4]   cid:20 −→ num[10]  Each step in this sequence of transitions is justiﬁed by a derivation according to rules  5.4 . For example, the third transition in the preceding example is justiﬁed by the following derivation:  plus num[3]; num[3]   cid:20 −→ num[6]   5.4a   plus plus num[3]; num[3] ; num[4]   cid:20 −→ plus num[6]; num[4]    5.4b   The other steps are similarly justiﬁed by composing rules. The principle of rule induction for the structural dynamics of E states that to show P e  cid:20 −→ e  cid:5 , it is enough to show that P is closed under rules  5.4 . For example, we may show by rule induction that the structural dynamics of E is determinate,   cid:5   when e  cid:20 −→ e   42  Dynamics  which means that an expression may transition to at most one other expression. The proof requires a simple lemma relating transition to values. Lemma 5.2  Finality of Values . For no expression e do we have both e val, and e  cid:20 −→ e for some e   cid:5 .   cid:5   Proof By rule induction on rules  5.3  and  5.4 .  Lemma 5.3  Determinacy . If e  cid:20 −→ e  cid:5  and e  cid:20 −→ e  cid:5  cid:5  are α-equivalent. Proof By rule induction on the premises e  cid:20 −→ e  cid:5  cid:5 , carried out either simultaneously or in either order. The primitive operators, such as addition, are assumed to have a unique value when applied to values.   cid:5  cid:5 , then e  cid:5  and e  cid:5  and e  cid:20 −→ e  Rules  5.4  exemplify the inversion principle of language design, which states that the elimination forms are inverse to the introduction forms of a language. The search rules determine the principal arguments of each elimination form, and the instruction rules specify how to evaluate an elimination form when all of its principal arguments are in introduction form. For example, rules  5.4  specify that both arguments of addition are principal and specify how to evaluate an addition once its principal arguments are evaluated to numerals. The inversion principle is central to ensuring that a programming language is properly deﬁned, the exact statement of which is given in Chapter 6.  5.3 Contextual Dynamics  A variant of structural dynamics, called contextual dynamics, is sometimes useful. There is no fundamental difference between contextual and structural dynamics, but rather one of style. The main idea is to isolate instruction steps as a special form of judgment, called instruction transition, and to formalize the process of locating the next instruction using a device called an evaluation context. The judgment e val deﬁning whether an expression is a value, remains unchanged. The instruction transition judgment e1 → e2 for E is deﬁned by the following rules, together with similar rules for multiplication of numbers and the length of a string.  m + n is p nat  plus num[m]; num[n]  → num[p]  s ˆ t = u str  cat str[s]; str[t]  → str[u]  let e1; x.e2  → [e1 x]e2   5.5a    5.5b    5.5c    43  5.3 Contextual Dynamics  The judgment E ectxt determines the location of the next instruction to execute in a larger expression. The position of the next instruction step is speciﬁed by a “hole,” written ◦, into which the next instruction is placed, as we shall detail shortly.  The rules for multiplication and length are omitted for concision, as they are handled similarly.   The ﬁrst rule for evaluation contexts speciﬁes that the next instruction may occur “here,” at the occurrence of the hole. The remaining rules correspond one-for-one to the search rules of the structural dynamics. For example, rule  5.6c  states that in an expression plus e1; e2 , if the ﬁrst argument, e1, is a value, then the next instruction step, if any, lies at or within the second argument, e2.  An evaluation context is a template that is instantiated by replacing the hole with an  cid:5  is the result instruction to be executed. The judgment e of ﬁlling the hole in the evaluation context E with the expression e. It is inductively deﬁned by the following rules:   cid:5  = E{e} states that the expression e  ◦ ectxt E1 ectxt  plus E1; e2  ectxt e1 val E2 ectxt plus e1;E2  ectxt  e = ◦{e} e1 = E1{e}  plus e1; e2  = plus E1; e2 {e}  e2 = E2{e}  e1 val  plus e1; e2  = plus e1;E2 {e}   5.6a    5.6b    5.6c    5.7a    5.7b    5.7c   There is one rule for each form of evaluation context. Filling the hole with e results in e; otherwise, we proceed inductively over the structure of the evaluation context.  Finally, the contextual dynamics for E is deﬁned by a single rule:  e = E{e0}  e0 → e  cid:5  0 e  cid:20 −→ e  cid:5    cid:5  = E{e  e  }   cid:5  0   5.8   cid:5  consists of  1  decomposing e into an evaluation context Thus, a transition from e to e and an instruction,  2  execution of that instruction, and  3  replacing the instruction by the result of its execution in the same spot within e to obtain e The structural and contextual dynamics deﬁne the same transition relation. For the sake of the proof, let us write e  cid:20 −→s e  cid:5  for the transition relation deﬁned by the structural dynamics  rules  5.4  , and e  cid:20 −→c e  cid:5  for the transition relation deﬁned by the contextual dynamics  rules  5.8  . Theorem 5.4. e  cid:20 −→s e   cid:5  if, and only if, e  cid:20 −→c e   cid:5 .   cid:5 .   44  Dynamics  e   cid:5  0   cid:5  0   cid:5  = E{e  Proof From left to right, proceed by rule induction on rules  5.4 . It is enough in each case to exhibit an evaluation context E such that e = E{e0}, e }, and e0 → e  cid:5  0. For example, for rule  5.4a , take E = ◦, and note that e → e  cid:5 . For rule  5.4b , we have by induction that there exists an evaluation context E1 such that e1 = E1{e0}, e =  cid:5  0. Take E = plus E1; e2 , and note that e = plus E1; e2 {e0} and }, and e0 → e E1{e 1  cid:5  } with e0 → e  cid:5  = plus E1; e2 {e  cid:5   cid:5  0. From right to left, note that if e  cid:20 −→c e  cid:5 , then there exists an evaluation context E 0 }, and e0 → e  cid:5  = E{e such that e = E{e0}, e  cid:5   cid:5  0. We prove by induction on rules  5.7  that e  cid:20 −→s e 0  cid:5 . Hence, }, = E1{e  cid:5 . For rule  5.7b , we have that E = plus E1; e2 , e1 = E1{e0}, e e  cid:20 −→s e  cid:5  and e1  cid:20 −→s e 0  cid:5   cid:5  1; e2 , and therefore by rule  5.4b , 1. Therefore, e is plus e1; e2 , e e  cid:20 −→s e  cid:5 .   cid:5 . For example, for rule  5.7a , e0 is e, e   cid:5 , and e → e  cid:5  1   cid:5  is plus e   cid:5  0 is e  Because the two transition judgments coincide, contextual dynamics can be considered an alternative presentation of a structural dynamics. It has two advantages over structural dynamics, one relatively superﬁcial, one rather less so. The superﬁcial advantage stems from writing rule  5.8  in the simpler form  e0 → e   cid:5  0  E{e0}  cid:20 −→ E{e  } .   cid:5  0   5.9   This formulation is superﬁcially simpler in that it does not make explicit how an expression is decomposed into an evaluation context and a reducible expression. The deeper advantage of contextual dynamics is that all transitions are between complete programs. One need never consider a transition between expressions of any type other than the observable type, which simpliﬁes certain arguments, such as the proof of Lemma 47.16.  5.4 Equational Dynamics  Another formulation of the dynamics of a language regards computation as a form of equational deduction, much in the style of elementary algebra. For example, in algebra, we may show that the polynomials x2 + 2 x + 1 and  x + 1 2 are equivalent by a simple process of calculation and re-organization using the familiar laws of addition and multiplication. The same laws are enough to determine the value of any polynomial, given the values of its variables. So, for example, we may plug in 2 for x in the polynomial x2 + 2 x + 1 and calculate that 22 + 2 × 2 + 1 = 9, which is indeed  2 + 1 2. We thus obtain a model of computation in which the value of a polynomial for a given value of its variable is determined by substitution and simpliﬁcation. Very similar ideas give rise to the concept of deﬁnitional, or computational, equivalence of expressions in E, which we write as X   cid:7   cid:12  e ≡ e  cid:5  : τ, where  cid:7  consists of one assumption of the form x : τ for each x ∈ X . We only consider deﬁnitional equality of well-typed expressions, so that when considering the judgment  cid:7   cid:12  e ≡ e  cid:5  : τ, we tacitly   45  5.4 Equational Dynamics  assume that  cid:7   cid:12  e : τ and  cid:7   cid:12  e variables X when they can be determined from the forms of the assumptions  cid:7 .   cid:5  : τ. Here, as usual, we omit explicit mention of the  Deﬁnitional equality of expressions in E under the by-name interpretation of let is  inductively deﬁned by the following rules:   cid:7   cid:12  e ≡ e   cid:5  cid:5  : τ   cid:5  ≡ e   cid:7   cid:12  e ≡ e : τ  cid:5  ≡ e : τ  cid:7   cid:12  e  cid:7   cid:12  e ≡ e  cid:5  : τ  cid:5  : τ  cid:7   cid:12  e  cid:7   cid:12  e ≡ e  cid:5  cid:5  : τ 1 : num  cid:7   cid:12  e2 ≡ e  cid:5   cid:5  2 : num  cid:5   cid:5  2  : num 1; e 1 : str  cid:7   cid:12  e2 ≡ e  cid:5   cid:5  2 : str  cid:5   cid:5  2  :str 1; e 1 : τ1  cid:7 , x : τ1  cid:12  e2 ≡ e  cid:5   cid:5  2 : τ2  cid:5  2  :τ 2   cid:5  1; x.e   cid:7   cid:12  e1 ≡ e  cid:7   cid:12  plus e1; e2  ≡ plus e  cid:7   cid:12  e1 ≡ e  cid:7   cid:12  cat e1; e2  ≡ cat e   cid:7   cid:12  e1 ≡ e  cid:7   cid:12  let e1; x.e2  ≡ let e n1 + n2 is n nat   cid:7   cid:12  plus num[n1]; num[n2]  ≡ num[n] : num  s1 ˆ s2 = s str   cid:7   cid:12  cat str[s1]; str[s2]  ≡ str[s] :str   cid:7   cid:12  let e1; x.e2  ≡ [e1 x]e2 : τ   5.10a    5.10b    5.10c    5.10d    5.10e    5.10f    5.10g    5.10h    5.10i   Rules  5.10a  through  5.10c  state that deﬁnitional equality is an equivalence relation. Rules  5.10d  through  5.10f  state that it is a congruence relation, which means that it is compatible with all expression-forming constructs in the language. Rules  5.10g  through  5.10i  specify the meanings of the primitive constructs of E. We say that rules  5.10  deﬁne the strongest congruence closed under rules  5.10g ,  5.10h , and  5.10i .  Rules  5.10  sufﬁce to calculate the value of an expression by a deduction similar to that  used in high school algebra. For example, we may derive the equation  let x be 1 + 2 in x + 3 + 4 ≡ 10 : num  by applying rules  5.10 . Here, as in general, there may be many different ways to derive the same equation, but we need ﬁnd only one derivation in order to carry out an evaluation. Deﬁnitional equality is rather weak in that many equivalences that we might intuitively think are true are not derivable from rules  5.10 . A prototypical example is the putative equivalence  x1 : num, x2 : num  cid:12  x1 + x2 ≡ x2 + x1 : num,   5.11    46  Dynamics  which, intuitively, expresses the commutativity of addition. Although we shall not prove this here, this equivalence is not derivable from rules  5.10 . And yet we may derive all of its closed instances,  n1 + n2 ≡ n2 + n1 : num,   5.12   where n1 nat and n2 nat are particular numbers.  The “gap” between a general law, such as Equation  5.11 , and all of its instances, given by Equation  5.12 , may be ﬁlled by enriching the notion of equivalence to include a principle of proof by mathematical induction. Such a notion of equivalence is sometimes called semantic equivalence, because it expresses relationships that hold by virtue of the dynamics of the expressions involved.  Semantic equivalence is developed rigorously for a related language in Chapter 46.  Theorem 5.5. For the expression language E, the relation e ≡ e e0 val such that e  cid:20 −→∗   cid:5  : τ holds iff there exists   cid:5   cid:20 −→∗  e0 and e  e0.  Proof The proof from right to left is direct, because every transition step is a valid equation. The converse follows from the following, more general, proposition, which is proved by induction on rules  5.10 : if x1 : τ1, . . . , xn : τn  cid:12  e ≡ e  cid:5  : τ, then when  cid:5  e1 : τ1, e i evaluate to a common value vi, then there exists e0 val such that  n : τn, if for each 1 ≤ i ≤ n the expressions ei and e  cid:5  [e1, . . . , en x1, . . . , xn]e  cid:20 −→∗   cid:5  1 : τ1, . . . , en : τn, e  e0  and   cid:5  1, . . . , e   cid:5  n x1, . . . , xn]e  [e   cid:5   cid:20 −→∗  e0.  5.5 Notes  The use of transition systems to specify the behavior of programs goes back to the early work of Church and Turing on computability. Turing’s approach emphasized the concept of an abstract machine consisting of a ﬁnite program together with unbounded memory. Computation proceeds by changing the memory in accordance with the instructions in the program. Much early work on the operational semantics of programming languages, such as the SECD machine  Landin, 1965 , emphasized machine models. Church’s approach emphasized the language for expressing computations and deﬁned execution in terms of the programs themselves, rather than in terms of auxiliary concepts such as memories or tapes. Plotkin’s elegant formulation of structural operational semantics  Plotkin, 1981 , which we use heavily throughout this book, was inspired by Church’s and Landin’s ideas  Plotkin, 2004 . Contextual semantics, which was introduced by Felleisen and Hieb  1992 , may be seen as an alternative formulation of structural semantics in which “search rules” are replaced by “context matching.” Computation viewed as equational deduction goes back to the early work of Herbrand, G¨odel, and Church.   47  Exercises  Exercises  5.1. Prove that if s  cid:20 −→∗ 5.2. Complete the proof of Theorem 5.1 along the lines suggested there. 5.3. Complete the proof of Theorem 5.5 along the lines suggested there.   cid:5  cid:5 , then s  cid:20 −→∗   cid:5  and s   cid:5   cid:20 −→∗   cid:5  cid:5 .  s  s  s   6  Type Safety  Most programming languages are safe  or, type safe, or strongly typed . Informally, this means that certain kinds of mismatches cannot arise during execution. For example, type safety for E states that it will never arise that a number is added to a string, or that two numbers are concatenated, neither of which is meaningful.  In general, type safety expresses the coherence between the statics and the dynamics. The statics may be seen as predicting that the value of an expression will have a certain form so that the dynamics of that expression is well-deﬁned. Consequently, evaluation cannot “get stuck” in a state for which no transition is possible, corresponding in implementation terms to the absence of “illegal instruction” errors at execution time. Safety is proved by showing that each step of transition preserves typability and by showing that typable states are well-deﬁned. Consequently, evaluation can never “go off into the weeds” and, hence, can never encounter an illegal instruction.  Type safety for the language E is stated precisely as follows:  Theorem 6.1  Type Safety . 1. If e : τ and e  cid:20 −→ e 2. If e : τ , then either e val, or there exists e   cid:5 , then e   cid:5  : τ .   cid:5  such that e  cid:20 −→ e   cid:5 .  The ﬁrst part, called preservation, says that the steps of evaluation preserve typing; the second, called progress, ensures that well-typed expressions are either values or can be further evaluated. Safety is the conjunction of preservation and progress.  cid:5  such that e  cid:20 −→ e  cid:5 . It follows from the safety theorem that a stuck state is necessarily ill-typed. Or, putting it the other way around, that well-typed states do not get stuck.  We say that an expression e is stuck iff it is not a value, yet there is no e  6.1 Preservation  The preservation theorem for E deﬁned in Chapters 4 and 5 is proved by rule induction on the transition system  rules  5.4  . Theorem 6.2  Preservation . If e : τ and e  cid:20 −→ e   cid:5 , then e   cid:5  : τ .   49  6.2 Progress  Proof We will give the proof in two cases, leaving the rest to the reader. Consider rule  5.4b ,  e1  cid:20 −→ e   cid:5  1  plus e1; e2   cid:20 −→ plus e  .   cid:5  1; e2   Assume that plus e1; e2  : τ. By inversion for typing, we have that τ = num, e1 : num, and  cid:5  e2 : num. By induction, we have that e 1; e2  : num. The case for concatenation is handled similarly.   cid:5  1 : num, and hence plus e  Now consider rule  5.4h ,  let e1; x.e2   cid:20 −→ [e1 x]e2  .  Assume that let e1; x.e2  :τ 2. By the inversion Lemma 4.2, e1 : τ1 for some τ1 such that x : τ1  cid:12  e2 : τ2. By the substitution Lemma 4.4 [e1 x]e2 : τ2, as desired. It is easy to check that the primitive operations are all type-preserving; for example, if a nat and b nat and a + b is c nat, then c nat.  The proof of preservation is naturally structured as an induction on the transition judg- ment, because the argument hinges on examining all possible transitions from a given expression. In some cases, we may manage to carry out a proof by structural induction on e, or by an induction on typing, but experience shows that this often leads to awkward arguments, or, sometimes, cannot be made to work at all.  6.2 Progress  The progress theorem captures the idea that well-typed programs cannot “get stuck.” The proof depends crucially on the following lemma, which characterizes the values of each type.  Lemma 6.3  Canonical Forms . If e val and e : τ , then 1. If τ = num, then e = num[n] for some number n. 2. If τ = str, then e = str[s] for some string s.  Proof By induction on rules  4.1  and  5.3 .  Progress is proved by rule induction on rules  4.1  deﬁning the statics of the language.  cid:5  such that e  cid:20 −→ e  cid:5 .  Theorem 6.4  Progress . If e : τ , then either e val, or there exists e   50  Type Safety  Proof The proof proceeds by induction on the typing derivation. We will consider only one case, for rule  4.1d ,  e1 : num e2 : num plus e1; e2  : num  ,  where the context is empty because we are considering only closed terms. By induction, we have that either e1 val, or there exists e latter case, it follows that plus e1; e2   cid:20 −→ plus e 2 such that e2  cid:20 −→ e  cid:5  also have by induction that either e2 val, or there exists e case, we have that plus e1; e2   cid:20 −→ plus e1; e the Canonical Forms Lemma 6.3, e1 = num[n1] and e2 = num[n2], and hence   cid:5  1. In the  cid:5  1; e2 , as required. In the former, we  cid:5  2. In the latter  cid:5  2 , as required. In the former, we have, by  1 such that e1  cid:20 −→ e  cid:5   plus num[n1]; num[n2]   cid:20 −→ num[n1 + n2].  Because the typing rules for expressions are syntax-directed, the progress theorem could equally well be proved by induction on the structure of e, appealing to the inversion theorem at each step to characterize the types of the parts of e. But this approach breaks down when the typing rules are not syntax-directed, that is, when there is more than one rule for a given expression form. Such rules present no difﬁculties, so long as the proof proceeds by induction on the typing rules and not on the structure of the expression.  Summing up, the combination of preservation and progress together constitute the proof of safety. The progress theorem ensures that well-typed expressions do not “get stuck” in an ill-deﬁned state, and the preservation theorem ensures that if a step is taken, the result remains well-typed  with the same type . Thus, the two parts work together to ensure that the statics and dynamics are coherent and that no ill-deﬁned states can ever be encountered while evaluating a well-typed expression.  6.3 Run-Time Errors  e1 : num e2 : num div e1; e2  :num  .  Suppose that we wish to extend E with, say, a quotient operation that is undeﬁned for a zero divisor. The natural typing rule for quotients is given by the following rule:  But the expression div num[3]; num[0]  is well-typed, yet stuck! We have two options to correct this situation:  1. Enhance the type system, so that no well-typed program may divide by zero. 2. Add dynamic checks, so that division by zero signals an error as the outcome of evalu-  ation.  Either option is, in principle, practical, but the most common approach is the second. The ﬁrst requires that the type checker prove that an expression be non-zero before permitting   51  6.3 Run-Time Errors  it to be used in the denominator of a quotient. It is difﬁcult to do this without ruling out too many programs as ill-formed. We cannot predict statically whether an expression will be non-zero when evaluated, so the second approach is most often used in practice.  The overall idea is to distinguish checked from unchecked errors. An unchecked error is one that is ruled out by the type system. No run-time checking is performed to ensure that such an error does not occur, because the type system rules out the possibility of it arising. For example, the dynamics need not check, when performing an addition, that its two arguments are, in fact, numbers, as opposed to strings, because the type system ensures that this is the case. On the other hand, the dynamics for quotient must check for a zero divisor, because the type system does not rule out the possibility.  One approach to modeling checked errors is to give an inductive deﬁnition of the judg- ment e err stating that the expression e incurs a checked run-time error, such as division by zero. Here are some representative rules that would be present in a full inductive deﬁnition of this judgment:  e1 val  div e1; num[0]  err  e1 err  div e1; e2  err  e1 val e2 err div e1; e2  err   cid:7   cid:12  error : τ  error err  Rule  6.1a  signals an error condition for division by zero. The other rules propagate this error upwards: if an evaluated sub-expression is a checked error, then so is the overall expression.  Once the error judgment is available, we may also consider an expression, error, which  forcibly induces an error, with the following static and dynamic semantics:  The preservation theorem is not affected by checked errors. However, the statement  and  proof  of progress is modiﬁed to account for checked errors.  Theorem 6.5  Progress With Error . If e : τ , then either e err, or e val, or there exists e such that e  cid:20 −→ e   cid:5 .  Proof The proof is by induction on typing, and proceeds similarly to the proof given earlier, except that there are now three cases to consider at each point in the proof.   6.1a    6.1b    6.1c    6.2a    6.2b    cid:5    52  Type Safety  6.4 Notes  The concept of type safety was ﬁrst formulated by Milner  1978 , who invented the slogan “well-typed programs do not go wrong.” Whereas Milner relied on an explicit notion of “going wrong” to express the concept of a type error, Wright and Felleisen  1994  observed that we can instead show that ill-deﬁned states cannot arise in a well-typed program, giving rise to the slogan “well-typed programs do not get stuck.” However, their formulation relied on an analysis showing that no stuck state is well-typed. The progress theorem, which relies on the characterization of canonical forms in the style of Martin-L¨of  1980 , eliminates this analysis.  Exercises  6.1. Complete the proof of Theorem 6.2 in full detail. 6.2. Complete the proof of Theorem 6.4 in full detail. 6.3. Give several cases of the proof of Theorem 6.5 to illustrate how checked errors are  handled in type safety proofs.   7  Evaluation Dynamics  In Chapter 5, we deﬁned evaluation of expressions in E using a structural dynamics. Structural dynamics is very useful for proving safety, but for some purposes, such as writing a user manual, another formulation, called evaluation dynamics, is preferable. An evaluation dynamics is a relation between a phrase and its value that is deﬁned without detailing the step-by-step process of evaluation. A cost dynamics enriches an evaluation dynamics with a cost measure specifying the resource usage of evaluation. A prime example is time, measured as the number of transition steps required to evaluate an expression according to its structural dynamics.  7.1 Evaluation Dynamics  An evaluation dynamics consists of an inductive deﬁnition of the evaluation judgment e ⇓ v stating that the closed expression e evaluates to the value v. The evaluation dynamics of E is deﬁned by the following rules:  num[n] ⇓ num[n] str[s] ⇓ str[s] e2 ⇓ num[n2] n1 + n2 is n nat  plus e1; e2  ⇓ num[n]  e1 ⇓ num[n1]  e1 ⇓ str[s1]  e2 ⇓ str[s2]  s1 ˆ s2 = s str  cat e1; e2  ⇓ str[s] e ⇓ str[s] s = n nat  len e  ⇓ num[n] [e1 x]e2 ⇓ v2 let e1; x.e2  ⇓ v2   7.1a    7.1b    7.1c    7.1d    7.1e    7.1f   The value of a let expression is determined by substitution of the binding into the body. The rules are not syntax-directed, because the premise of rule  7.1f  is not a sub-expression of the expression in the conclusion of that rule.   54  Evaluation Dynamics  Rule  7.1f  speciﬁes a by-name interpretation of deﬁnitions. For a by-value interpretation,  the following rule should be used instead: e1 ⇓ v1  [v1 x]e2 ⇓ v2  let e1; x.e2  ⇓ v2   7.2   Because the evaluation judgment is inductively deﬁned, we prove properties of it by rule induction. Speciﬁcally, to show that the property P e ⇓ v  holds, it is enough to show that P is closed under rules  7.1 : 1. Show that P num[n] ⇓ num[n] . 2. Show that P str[s] ⇓ str[s] . 3. Show that P plus e1; e2  ⇓ num[n] , 4. Show that P cat e1; e2  ⇓ str[s] , 5. Show that P let e1; x.e2  ⇓ v2 , if P [e1 x]e2 ⇓ v2 .  if P e1 ⇓ num[n1] , P e2 ⇓ num[n2] , and if P e1 ⇓ str[s1] , P e2 ⇓ str[s2] ,  n1 + n2 is n nat. s1 ˆ s2 = s str.  and  This induction principle is not the same as structural induction on e itself, because the evaluation rules are not syntax-directed. Lemma 7.1. If e ⇓ v, then v val.  Proof By induction on rules  7.1 . All cases except rule  7.1f  are immediate. For the latter case, the result follows directly by an appeal to the inductive hypothesis for the premise of the evaluation rule.  7.2 Relating Structural and Evaluation Dynamics  We have given two different forms of dynamics for E. It is natural to ask whether they are equivalent, but to do so ﬁrst requires that we consider carefully what we mean by equivalence. The structural dynamics describes a step-by-step process of execution, whereas the evaluation dynamics suppresses the intermediate states, focusing attention on the initial and ﬁnal states alone. This remark suggests that the right correspondence is between complete execution sequences in the structural dynamics and the evaluation judgment in the evaluation dynamics. Theorem 7.2. For all closed expressions e and values v, e  cid:20 −→∗  v iff e ⇓ v.  How might we prove such a theorem? We will consider each direction separately. We  consider the easier case ﬁrst. Lemma 7.3. If e ⇓ v, then e  cid:20 −→∗  v.   55  7.3 Type Safety, Revisited  Proof By induction on the deﬁnition of the evaluation judgment. For example, suppose that plus e1; e2  ⇓ num[n] by the rule for evaluating additions. By induction, we know that e1  cid:20 −→∗  num[n1] and e2  cid:20 −→∗  plus e1; e2   num[n2]. We reason as follows: plus num[n1]; e2  plus num[n1]; num[n2]    cid:20 −→∗  cid:20 −→∗  cid:20 −→ num[n1 + n2]  Therefore, plus e1; e2   cid:20 −→∗ similarly.  num[n1 + n2], as required. The other cases are handled  For the converse, recall from Chapter 5 the deﬁnitions of multi-step evaluation and complete evaluation. Because v ⇓ v when v val, it sufﬁces to show that evaluation is closed under converse evaluation:1 Lemma 7.4. If e  cid:20 −→ e   cid:5  ⇓ v, then e ⇓ v.   cid:5  and e  Proof By induction on the deﬁnition of the transition judgment. For example, suppose that plus e1; e2   cid:20 −→ plus e 1; e2  ⇓ v, so  cid:5  ⇓ num[n1], and e2 ⇓ num[n2], and n1 + n2 is n nat, and v is num[n]. By induction  cid:5  that e e1 ⇓ num[n1], and hence plus e1; e2  ⇓ num[n], as required. 1   cid:5  1. Suppose further that plus e  1; e2 , where e1  cid:20 −→ e  cid:5   7.3 Type Safety, Revisited  Type safety is deﬁned in Chapter 6 as preservation and progress  Theorem 6.1 . These concepts are meaningful when applied to a dynamics given by a transition system, as we shall do throughout this book. But what if we had instead given the dynamics as an evaluation relation? How is type safety proved in that case?  The answer, unfortunately, is that we cannot. Although there is an analog of the preserva- tion property for an evaluation dynamics, there is no clear analog of the progress property. Preservation may be stated as saying that if e ⇓ v and e : τ, then v : τ. It can be readily proved by induction on the evaluation rules. But what is the analog of progress? We might be tempted to phrase progress as saying that if e : τ, then e ⇓ v for some v. Although this property is true for E, it demands much more than just progress—it requires that every expression evaluate to a value! If E were extended to admit operations that may result in an error  as discussed in Section 6.3 , or to admit non-terminating expressions, then this property would fail, even though progress would remain valid.  One possible attitude towards this situation is to conclude that type safety cannot be properly discussed in the context of an evaluation dynamics, but only by reference to a structural dynamics. Another point of view is to instrument the dynamics with explicit checks for dynamic type errors, and to show that any expression with a dynamic type fault must be statically ill-typed. Re-stated in the contrapositive, this means that a statically well-typed program cannot incur a dynamic type error. A difﬁculty with this point of view   56  Evaluation Dynamics  is that we must explicitly account for a form of error solely to prove that it cannot arise! Nevertheless, a semblance of type safety can be established using evaluation dynamics.  We deﬁne a judgment e err stating that the expression e goes wrong when executed. The exact deﬁnition of “going wrong” is given by a set of rules, but the intention is that it should cover all situations that correspond to type errors. The following rules are representative of the general case:  plus str[s]; e2  err  e1 val  plus e1; str[s]  err   7.3a    7.3b   These rules explicitly check for the misapplication of addition to a string; similar rules govern each of the primitive constructs of the language.  Theorem 7.5. If e err, then there is no τ such that e : τ .  Proof By rule induction on rules  7.3 . For example, for rule  7.3a , we note that str[s] : str, and hence plus str[s]; e2  is ill-typed. Corollary 7.6. If e : τ , then ¬ e err .  Apart from the inconvenience of having to deﬁne the judgment e err only to show that it is irrelevant for well-typed programs, this approach suffers a very signiﬁcant methodological weakness. If we should omit one or more rules deﬁning the judgment e err, the proof of Theorem 7.5 remains valid; there is nothing to ensure that we have included sufﬁciently many checks for run-time type errors. We can prove that the ones we deﬁne cannot arise in a well-typed program, but we cannot prove that we have covered all possible cases. By contrast the structural dynamics does not specify any behavior for ill-typed expressions. Consequently, any ill-typed expression will “get stuck” without our explicit intervention, and the progress theorem rules out all such cases. Moreover, the transition system cor- responds more closely to implementation—a compiler need not make any provisions for checking for run-time type errors. Instead, it relies on the statics to ensure that these can- not arise, and assigns no meaning to any ill-typed program. Therefore, execution is more efﬁcient, and the language deﬁnition is simpler.  7.4 Cost Dynamics  A structural dynamics provides a natural notion of time complexity for programs, namely the number of steps required to reach a ﬁnal state. An evaluation dynamics, however, does not provide such a direct notion of time. Because the individual steps required to complete an evaluation are suppressed, we cannot directly read off the number of steps required to evaluate to a value. Instead, we must augment the evaluation relation with a cost measure, resulting in a cost dynamics.   Exercises  Evaluation judgments have the form e ⇓k v, with the meaning that e evaluates to v in k  57  steps.  num[n] ⇓0 num[n]  e2 ⇓k2 num[n2] e1 ⇓k1 num[n1] plus e1; e2  ⇓k1+k2+1 num[n1 + n2]  str[s] ⇓0 str[s] e2 ⇓k2 s2 e1 ⇓k1 s1  cat e1; e2  ⇓k1+k2+1 str[s1 ˆ s2]  [e1 x]e2 ⇓k2 v2  let e1; x.e2  ⇓k2+1 v2   7.4a    7.4b    7.4c    7.4d    7.4e   For a by-value interpretation of let, rule  7.4e  is replaced by the following rule:  [v1 x]e2 ⇓k2 v2 e1 ⇓k1 v1 let e1; x.e2  ⇓k1+k2+1 v2   7.5  Theorem 7.7. For any closed expression e and closed value v of the same type, e ⇓k v iff e  cid:20 −→k v.  Proof From left to right, proceed by rule induction on the deﬁnition of the cost dynamics. From right to left, proceed by induction on k, with an inner rule induction on the deﬁnition of the structural dynamics.  The structural similarity between evaluation dynamics and typing rules was ﬁrst developed in The Deﬁnition of Standard ML  Milner et al., 1997 . The advantage of evaluation semantics is its directness; its disadvantage is that it is not well-suited to proving properties such as type safety. Robin Milner introduced the apt phrase “going wrong” as a description of a type error. Cost dynamics was introduced by Blelloch and Greiner  1996  in a study of parallel computation  see Chapter 37 .  7.5 Notes  Exercises  7.1. Show that evaluation is deterministic: if e ⇓ v1 and e ⇓ v2, then v1 = v2. 7.2. Complete the proof of Lemma 7.3. 7.3. Complete the proof of Lemma 7.4. Then show that if e  cid:20 −→∗   cid:5  with e  e   cid:5   val, then e ⇓ e   cid:5 .   58  Evaluation Dynamics  7.4. Augment the evaluation dynamics with checked errors, along the lines sketched in Chapter 5, using e err to say that e incurs a checked  or an unchecked  error. What remains unsatisfactory about the type safety proof? Can you think of a better alterna- tive?  7.5. Consider generic hypothetical judgments of the form  x1 ⇓ v1, . . . , xn ⇓ vn  cid:12  e ⇓ v  where v1 val, . . . , vn val, and v val. The hypotheses, written  cid:8 , are called the en- vironment of the evaluation; they provide the values of the free variables in e. The hypothetical judgment  cid:8   cid:12  e ⇓ v is called an environmental evaluation dynamics.  Give a hypothetical inductive deﬁnition of the environmental evaluation dynamics  without making any use of substitution. In particular, you should include the rule  deﬁning the evaluation of a free variable. Show that x1 ⇓ v1, . . . , xn ⇓ vn  cid:12  e ⇓ v iff [v1, . . . , vn x1, . . . , xn]e ⇓ v  using the by-value form of evaluation .   cid:8 , x ⇓ v  cid:12  x ⇓ v  Note  1 Converse evaluation is also known as head expansion.   P A R T III  Total Functions    8  Function Deﬁnitions and Values  In the language E, we may perform calculations such as the doubling of a given expression, but we cannot express doubling as a concept in itself. To capture the pattern of doubling a number, we abstract away from the particular number being doubled using a variable to stand for a ﬁxed, but unspeciﬁed, number, to express the doubling of an arbitrary number. Any particular instance of doubling may then be obtained by substituting a numeric expression for that variable. In general, an expression may involve many distinct variables, necessitating that we specify which of several possible variables is varying in a particular context, giving rise to a function of that variable.  In this chapter, we will consider two extensions of E with functions. The ﬁrst, and perhaps most obvious, extension is by adding function deﬁnitions to the language. A function is deﬁned by binding a name to an abt with a bound variable that serves as the argument of that function. A function is applied by substituting a particular expression  of suitable type  for the bound variable, obtaining an expression.  The domain and range of deﬁned functions are limited to the types nat and str, because these are the only types of expression. Such functions are called ﬁrst-order functions, in contrast to higher-order functions, which permit functions as arguments and results of other functions. Because the domain and range of a function are types, this requires that we introduce function types whose elements are functions. Consequently, we may form functions of higher type, those whose domain and range may themselves be function types.  8.1 First-Order Functions  The language ED extends E with function deﬁnitions and function applications as described by the following grammar:  Exp e  ::= apply{f} e   fun{τ1; τ2} x1.e2; f.e  fun f  x1 : τ1  : τ2 = e2 in e  f  e   application deﬁnition  The expression fun{τ1; τ2} x1.e2; f.e  binds the function name f within e to the pattern x1.e2, which has argument x1 and deﬁnition e2. The domain and range of the function are, respectively, the types τ1 and τ2. The expression apply{f} e  instantiates the binding of f with the argument e.   62  Function Deﬁnitions and Values  The statics of ED deﬁnes two forms of judgment:   8.1a   1. Expression typing, e : τ, stating that e has type τ; 2. Function typing, f  τ1  :τ 2, stating that f is a function with argument type τ1 and result  type τ2.  The judgment f  τ1  :τ 2 is called the function header of f ; it speciﬁes the domain type and the range type of a function.  The statics of ED is deﬁned by the following rules:   cid:7 , x1 : τ1  cid:12  e2 : τ2  cid:7 , f  τ1  : τ2  cid:12  e : τ   cid:7   cid:12  fun{τ1; τ2} x1.e2; f.e  : τ  cid:7   cid:12  f  τ1  : τ2  cid:7   cid:12  e : τ1  cid:7   cid:12  apply{f} e  : τ2  Function substitution, written [[x.e f ]]e   8.1b   cid:5   cid:5 , is deﬁned by induction on the structure of e much like ordinary substitution. However, a function name f does not stand for an expres- sion and can only occur in an application of the form apply{f} e . Function substitution is deﬁned by the following rule:  [[x.e f ]]apply{f} e At application sites to f with argument e that binds x to the result of expanding any further applications to f within e Lemma 8.1. If  cid:7 , f  τ1  :τ 2  cid:12  e : τ and  cid:7 , x1 : τ1  cid:12  e2 : τ2, then  cid:7   cid:12  [[x1.e2 f ]]e : τ .   8.2   cid:5   = let [[x.e f ]]e  cid:5 , function substitution yields a let expression   cid:5 ; x.e    cid:5 .  Proof By rule induction on the ﬁrst premise, similarly to the proof of Lemma 4.4.  The dynamics of ED is deﬁned using function substitution:  fun{τ1; τ2} x1.e2; f.e   cid:20 −→ [[x1.e2 f ]]e   8.3   Because function substitution replaces all applications of f by appropriate let expressions, there is no need to give a rule for application expressions  essentially, they behave like variables that are replaced during evaluation, and not like a primitive operation of the language .  The safety of ED may, with some effort, be derived from the safety theorem for higher-  order functions, which we discuss next.  8.2 Higher-Order Functions  The similarity between variable deﬁnitions and function deﬁnitions in ED is striking. Is it possible to combine them? The gap that must be bridged is the segregation of functions   63  8.2 Higher-Order Functions  from expressions. A function name f is bound to an abstractor x.e specifying a pattern that is instantiated when f is applied. To reduce function deﬁnitions to ordinary deﬁnitions, we reify the abstractor into a form of expression, called a λ-abstraction, written lam{τ1} x.e . Applications generalize to ap e1; e2 , where e1 is an expression denoting a function, and not just a function name. λ-abstraction and application are the introduction and elimination forms for the function type arr τ1; τ2 , which classiﬁes functions with domain τ1 and range τ2.  The language EF enriches E with function types, as speciﬁed by the following  grammar:  Typ τ Exp e  τ1 → τ2 ::= arr τ1; τ2  ::= lam{τ} x.e  λ  x : τ   e  ap e1; e2   e1 e2   function abstraction application  In EF functions are ﬁrst-class in that they are a form of expression that can be used like any other. In particular, functions may be passed as arguments to, and returned as results from, other functions. For this reason, ﬁrst-class functions are said to be higher-order, rather than ﬁrst-order.  The statics of EF is given by extending rules  4.1  with the following rules:   cid:7 , x : τ1  cid:12  e : τ2   cid:7   cid:12  lam{τ1} x.e  :arr τ 1; τ2   cid:7   cid:12  e1 : arr τ2; τ   cid:7   cid:12  e2 : τ2   cid:7   cid:12  ap e1; e2  : τ  Lemma 8.2  Inversion . Suppose that  cid:7   cid:12  e : τ . 1. If e = lam{τ1} x.e2 , then τ = arr τ1; τ2  and  cid:7 , x : τ1  cid:12  e2 : τ2. 2. If e = ap e1; e2 , then there exists τ2 such that  cid:7   cid:12  e1 : arr τ2; τ  and  cid:7   cid:12  e2 : τ2.  Proof The proof proceeds by rule induction on the typing rules. Observe that for each rule, exactly one case applies and that the premises of the rule provide the required result. Lemma 8.3  Substitution . If  cid:7 , x : τ  cid:12  e   cid:5 , and  cid:7   cid:12  e : τ , then  cid:7   cid:12  [e x]e   cid:5  : τ   cid:5  : τ   cid:5 .  Proof By rule induction on the derivation of the ﬁrst judgment.  The dynamics of EF extends that of E with the following rules:   8.4a    8.4b    8.5a    8.5b   lam{τ} x.e  val  e1  cid:20 −→ e   cid:5  1  ap e1; e2   cid:20 −→ ap e   cid:5  1; e2    64  Function Deﬁnitions and Values   cid:3    cid:4   e1 val  e2  cid:20 −→ e  cid:5  2 ap e1; e2   cid:20 −→ ap e1; e  cid:5  2   ap lam{τ2} x.e1 ; e2   cid:20 −→ [e2 x]e1  [e2 val]   8.5c    8.5d   The bracketed rule and premise are included for a call-by-value interpretation of function application and excluded for a call-by-name interpretation.1  When functions are ﬁrst class,  there is no need for function declarations: sim- ply replace the function declaration fun f  x1 : τ1  :τ 2 = e2 in e by the deﬁnition let λ  x : τ1  e2 be f in e, and replace second-class function application f  e  by the ﬁrst- class function application f  e . Because λ-abstractions are values, it makes no difference whether the deﬁnition is evaluated by-value or by-name for this replacement to make sense. However, using ordinary deﬁnitions, we may, for example, give a name to a partially applied function, as in the following example:  let k be λ  x1 : num  λ  x2 : num  x1 in let kz be k 0  in kz 3  + kz 5 .  Without ﬁrst-class functions, we cannot even form the function k, which returns a function as result when applied to its ﬁrst argument. Theorem 8.4  Preservation . If e : τ and e  cid:20 −→ e   cid:5 , then e   cid:5  : τ .  Proof The proof is by induction on rules  8.5 , which deﬁne the dynamics of the language.  Consider rule  8.5d ,  ap lam{τ2} x.e1 ; e2   cid:20 −→ [e2 x]e1  .  Suppose that ap lam{τ2} x.e1 ; e2  :τ 1. By Lemma 8.2, we have e2 : τ2 and x : τ2  cid:12  e1 : τ1, so by Lemma 8.3, [e2 x]e1 : τ1.  The other rules governing application are handled similarly. Lemma 8.5  Canonical Forms . If e : arr τ1; τ2  and e val, then e = λ  x : τ1  e2 for some variable x and expression e2 such that x : τ1  cid:12  e2 : τ2.  Proof By induction on the typing rules, using the assumption e val.  Theorem 8.6  Progress . If e : τ , then either e val, or there exists e   cid:5  such that e  cid:20 −→ e   cid:5 .  Proof The proof is by induction on rules  8.4 . Note that because we consider only closed terms, there are no hypotheses on typing derivations. Consider rule  8.4b   under the by-name interpretation . By induction either e1 val or e1  cid:20 −→ e  cid:5  1; e2 . In the former case, we  1. In the latter case, we have ap e1; e2   cid:20 −→ ap e  cid:5    65  8.3 Evaluation Dynamics and Deﬁnitional Equality  have by Lemma 8.5 that e1 = lam{τ2} x.e  for somex and e. But then ap e1; e2   cid:20 −→ [e2 x]e.  8.3 Evaluation Dynamics and Deﬁnitional Equality  An inductive deﬁnition of the evaluation judgment e ⇓ v for EF is given by the following rules:  lam{τ} x.e  ⇓ lam{τ} x.e  e1 ⇓ lam{τ} x.e  [e2 x]e ⇓ v  ap e1; e2  ⇓ v  It is easy to check that if e ⇓ v, then v val, and that if e val, then e ⇓ e. Theorem 8.7. e ⇓ v iff e  cid:20 −→∗  v and v val.  In the forward direction, we proceed by rule induction on rules  8.6 , following  Proof along similar lines as the proof of Theorem 7.2.  In the reverse direction, we proceed by rule induction on rules  5.1 . The proof relies on an analog of Lemma 7.4, which states that evaluation is closed under converse execution, which is proved by induction on rules  8.5 .  Deﬁnitional equality for the call-by-name dynamics of EF is deﬁned by extension of  rules  5.10 .   cid:7   cid:12  ap lam{τ} x.e2 ; e1  ≡ [e1 x]e2 : τ2  cid:7   cid:12  e1 ≡ e  cid:5  2 : τ2  1 : τ2 → τ  cid:7   cid:12  e2 ≡ e  cid:5   cid:5  2  :τ   cid:7   cid:12  ap e1; e2  ≡ ap e  cid:5  1; e  cid:7 , x : τ1  cid:12  e2 ≡ e  cid:5  2 : τ2  cid:7   cid:12  lam{τ1} x.e2  ≡ lam{τ1} x.e 2  :τ 1 → τ2  cid:5   Deﬁnitional equality for call-by-value requires a bit more machinery. The main idea is to restrict rule  8.7a  to require that the argument be a value. In addition, values must be expanded to include variables, because in call-by-value, the argument variable of a function stands for the value of its argument. The call-by-value deﬁnitional equality judgment takes the form   cid:7   cid:12  e1 ≡ e2 : τ,  where  cid:7  consists of paired hypotheses x : τ, x val stating, for each variable x in scope, its type and that it is a value. We write  cid:7   cid:12  e val to show that e is a value under these hypotheses, so that x : τ, x val  cid:12  x val.   8.6a    8.6b    8.7a    8.7b    8.7c    66  Function Deﬁnitions and Values  8.4 Dynamic Scope  The dynamics of function application given by rules  8.5  is deﬁned only for expressions without free variables. When a function is applied, the argument is substituted for the argument variable, ensuring that the result remains closed. Moreover, because substitution of closed expressions can never incur capture, the scopes of variables are not disturbed by the dynamics, ensuring that the principles of binding and scope described in Chapter 1 are respected. This treatment of variables is called static scoping, or static binding, to contrast it with an alternative approach that we now describe.  Another approach, called dynamic scoping, or dynamic binding, is sometimes advocated as an alternative to static binding. The crucial difference is that with dynamic scoping the principle of identiﬁcation of abt’s up to renaming of bound variables is denied. Conse- quently, capture-avoiding substitution is not available. Instead, evaluation is deﬁned for open terms, with the bindings of free variables provided by an environment mapping vari- able names to  possibly open  values. The binding of a variable is determined as late as possible, at the point where the variable is evaluated, rather than where it is bound. If the environment does not provide a binding for a variable, evaluation is aborted with a run-time error.  For ﬁrst-order functions, dynamic and static scoping coincide, but in the higher-order case, the two approaches diverge. For example, there is no difference between static and dynamic scope when it comes to evaluation of an expression such as  λ  x : num  x + 7  42 . Whether 42 is substituted for x in the body of the function before evaluation, or the body is evaluated in the presence of the binding of x to 42, the outcome is the same.  In the higher-order case, the equivalence of static and dynamic scope breaks down. For  example, consider the expression  e  cid:2   λ  x : num  λ  y : num  x + y  42 .  With static scoping e evaluates to the closed value v  cid:2  λ  y : num  42 + y, which, if applied, would add 42 to its argument. It makes no difference how the bound variable x is chosen, the outcome will always be the same. With dynamic scoping, e evaluates to the open  cid:5   cid:2  λ  y : num  x + y in which the variable x occurs free. When this expression is value v evaluated, the variable x is bound to 42, but this is irrelevant because the binding is not  cid:5  needed to evaluate the λ-abstraction. The binding of x is not retrieved until such time as v is applied to an argument, at which point the binding for x in force at that time is retrieved, and not the one in force at the point where e is evaluated.  Therein lies the difference. For example, consider the expression   cid:5   cid:2   λ  f : num → num   λ  x : num  f  0   7   e .  e   cid:5  is 7, whereas its value is 42 under When evaluated using dynamic scope, the value of e static scope. The discrepancy can be traced to the re-binding of x to 7 before the value of e, namely v   cid:5 , is applied to 0, altering the outcome.  Dynamic scope violates the basic principle that variables are given meaning by capture- avoiding substitution as deﬁned in Chapter 1. Violating this principle has at least two   67  Exercises  undesirable consequences. One is that the names of bound variables matter, in contrast to static scope which obeys the identiﬁcation principle. For example, had the innermost  cid:5  bound the variable y, rather than x, then its value would have been 42, λ-abstraction of e rather than 7. Thus, one component of a program may be sensitive to the names of bound variables chosen in another component, a clear violation of modular decomposition.  Another problem is that dynamic scope is not, in general, type-safe. For example, consider  the expression   cid:5   cid:2   λ  f : num → num   λ  x : str  f  "zero"   7   e .  e  Under dynamic scoping this expression gets stuck attempting to evaluate x+y with x bound to the string “zero,” and no further progress can be made. For this reason dynamic scope is only ever advocated for so-called dynamically typed languages, which replace static consistency checks by dynamic consistency checks to ensure a weak form of progress. Compile-time errors are thereby transformed into run-time errors.   For more on dynamic typing, see Chapter 22, and for more on dynamic scope, see  Chapter 32.   Nearly all programming languages provide some form of function deﬁnition mechanism of the kind illustrated here. The main point of the present account is to demonstrate that a more natural, and more powerful, approach is to separate the generic concept of a deﬁnition from the speciﬁc concept of a function. Function types codify the general notion in a systematic way that encompasses function deﬁnitions as a special case, and moreover, admits passing functions as arguments and returning them as results without special provision. The essential contribution of Church’s λ-calculus  Church, 1941  was to take functions as primary, and to show that nothing more is needed to get a fully expressive programming language.  8.5 Notes  Exercises  8.1. Formulate an environmental evaluation dynamics  see Exercise 7.5  forED . Hint:  Introduce a new form of judgment for evaluation of function identiﬁers.  8.2. Consider an environmental dynamics for EF, which includes higher-order functions. What difﬁculties arise? Can you think of a way to evade these difﬁculties? Hint: One approach is to “substitute away” all free variables in a λ-abstraction at the point at which it is evaluated. The second is to “freeze” the values of each of the free variables in a λ-abstraction, and to “thaw” them when such a function is applied. What problems arise in each case?   68  Function Deﬁnitions and Values  Note  1 Although the term “call-by-value” is accurately descriptive, the origin of the term “call-by-name”  remains shrouded in mystery.   9  System T of Higher-Order Recursion  System T, well-known as G¨odel’s T, is the combination of function types with the type of natural numbers. In contrast to E, which equips the naturals with some arbitrarily chosen arithmetic operations, the language T provides a general mechanism, called primitive recursion, from which these primitives may be deﬁned. Primitive recursion captures the essential inductive character of the natural numbers, and hence may be seen as an intrinsic termination proof for each program in the language. Consequently, we may only deﬁne total functions in the language, those that always return a value for each argument. In essence, every program in T “comes equipped” with a proof of its termination. Although this may seem like a shield against inﬁnite loops, it is also a weapon that can be used to show that some programs cannot be written in T. To do so would demand a master termination proof for every possible program in the language, something that we shall prove does not exist.  9.1 Statics  The syntax of T is given by the following grammar:  Typ τ  Exp e  arr τ1; τ2   nat τ1 → τ2  ::= nat ::= x z s e  rec{e0; x.y.e1} e  rec e {z  cid:9 → e0  s x  with y  cid:9 → e1} lam{τ} x.e  ap e1; e2   naturals function variable zero successor  recursion abstraction application  λ  x : τ  e e1 e2   x z s e   We write n for the expression s . . . s z  , in which the successor is applied n ≥ 0 times to zero. The expression rec{e0; x.y.e1} e  is called the recursor. It represents the e-fold iteration of the transformation x.y.e1 starting from e0. The bound variable x represents the predecessor and the bound variable y represents the result of the x-fold iteration. The “with” clause in the concrete syntax for the recursor binds the variable y to the result of the recursive call, as will become clear shortly. Sometimes the iterator, iter{e0; y.e1} e , is considered as an alternative to the recursor. It has essentially the same meaning as the recursor, except that only the result of the recursive   70  System T of Higher-Order Recursion  call is bound to y in e1, and no binding is made for the predecessor. Clearly, the iterator is a special case of the recursor, because we can always ignore the predecessor binding. Conversely, the recursor is deﬁnable from the iterator, provided that we have product types  Chapter 10  at our disposal. To deﬁne the recursor from the iterator, we simultaneously compute the predecessor while iterating the speciﬁed computation.  The statics of T is given by the following typing rules:   cid:7 , x : τ  cid:12  x : τ   cid:7   cid:12  z : nat  cid:7   cid:12  e : nat  cid:7   cid:12  s e  : nat   cid:7   cid:12  e : nat  cid:7   cid:12  e0 : τ  cid:7 , x : nat, y : τ  cid:12  e1 : τ   cid:7   cid:12  rec{e0; x.y.e1} e  : τ   cid:7 , x : τ1  cid:12  e : τ2   cid:7   cid:12  lam{τ1} x.e  : arr τ1; τ2   cid:7   cid:12  e1 : arr τ2; τ   cid:7   cid:12  e2 : τ2   cid:7   cid:12  ap e1; e2  :τ  As usual, admissibility of the structural rule of substitution is crucially important.  Lemma 9.1. If  cid:7   cid:12  e : τ and  cid:7 , x : τ  cid:12  e   cid:5  : τ   cid:5 , then  cid:7   cid:12  [e x]e   cid:5  : τ   cid:5 .  The closed values of T are deﬁned by the following rules:  The premise of rule  9.2b  is included for an eager interpretation of successor, and excluded for a lazy interpretation.  The transition rules for the dynamics of T are as follows:  9.2 Dynamics  z val  [e val] s e  val  lam{τ} x.e  val  cid:3   cid:4   e  cid:20 −→ e  cid:5  s e   cid:20 −→ s e   cid:5     9.1a    9.1b    9.1c    9.1d    9.1e    9.1f    9.2a    9.2b    9.2c    9.3a    71  9.3 Deﬁnability  e1  cid:20 −→ e   cid:5  1   cid:3  ap e1; e2   cid:20 −→ ap e  cid:5  1; e2  e2  cid:20 −→ e  cid:5  2 ap e1; e2   cid:20 −→ ap e1; e  cid:5  2   e1 val   cid:4   [e2 val]  ap lam{τ} x.e ; e2   cid:20 −→ [e2 x]e  e  cid:20 −→ e   cid:5   rec{e0; x.y.e1} e   cid:20 −→ rec{e0; x.y.e1} e   cid:5    rec{e0; x.y.e1} z   cid:20 −→ e0   9.3b    9.3c    9.3d    9.3e    9.3f    9.3g   rec{e0; x.y.e1} s e    cid:20 −→ [e, rec{e0; x.y.e1} e  x, y]e1  s e  val  The bracketed rules and premises are included for an eager successor and call-by-value application, and omitted for a lazy successor and call-by-name application. Rules  9.3f  and  9.3g  specify the behavior of the recursor on z and s e . In the former case, the recursor reduces to e0, and in the latter case, the variable x is bound to the predecessor e and y is bound to the  unevaluated  recursion on e. If the value ofy is not required in the rest of the computation, the recursive call is not evaluated.  Lemma 9.2  Canonical Forms . If e : τ and e val, then 1. If τ = nat, then e = s e 2. If τ = τ1 → τ2, then e = λ  x : τ1  e2 for some e2. Theorem 9.3  Safety . 1. If e : τ and e  cid:20 −→ e  cid:5 , then e 2. If e : τ , then either e val or e  cid:20 −→ e   cid:5   for some e   cid:5  for some e   cid:5 .   cid:5 .   cid:5  : τ .  9.3 Deﬁnability  A mathematical function f : N → N on the natural numbers is deﬁnable in T iff there exists an expression ef of type nat → nat such that for every n ∈ N,  ef  n  ≡ f  n  : nat.   9.4  That is, the numeric function f : N → N is deﬁnable iff there is an expression ef of type nat → nat such that, when applied to the numeral representing the argument n ∈ N, the application is deﬁnitionally equal to the numeral corresponding to f  n  ∈ N.   72  System T of Higher-Order Recursion  Deﬁnitional equality for T, written  cid:7   cid:12  e ≡ e   cid:5  : τ, is the strongest congruence containing  these axioms:   cid:7 , x : τ1  cid:12  e2 : τ2  cid:7   cid:12  e1 : τ1   cid:7   cid:12  ap lam{τ1} x.e2 ; e1  ≡ [e1 x]e2 : τ2   cid:7   cid:12  e0 : τ  cid:7 , x : τ  cid:12  e1 : τ  cid:7   cid:12  rec{e0; x.y.e1} z  ≡ e0 : τ  cid:7   cid:12  e0 : τ  cid:7 , x : τ  cid:12  e1 : τ   9.5a    9.5b    cid:7   cid:12  rec{e0; x.y.e1} s e   ≡ [e, rec{e0; x.y.e1} e  x, y]e1 : τ   9.5c  For example, the doubling function, d n  = 2 × n, is deﬁnable in T by the expression ed : nat → nat given by  λ  x : nat  rec x {z  cid:9 → z  s u  with v  cid:9 → s s v  }.  To check that this deﬁnes the doubling function, we proceed by induction on n ∈ N. For the basis, it is easy to check that  ed 0  ≡ 0 : nat.  For the induction, assume that  ed n  ≡ d n  :nat . Then calculate using the rules of deﬁnitional equality:  ed n + 1  ≡ s s ed n    ≡ s s 2 × n   = 2 ×  n + 1  = d n + 1 .  A 0, n  = n + 1 A m + 1, 0  = A m, 1   A m + 1, n + 1  = A m, A m + 1, n  .  As another example, consider the following function, called Ackermann’s function, de-  ﬁned by the following equations:  The Ackermann function grows very quickly. For example, A 4, 2  ≈ 265,536, which is often cited as being larger than the number of atoms in the universe! Yet we can show that the Ackermann function is total by a lexicographic induction on the pair of arguments  m, n . On each recursive call, either m decreases, or else m remains the same, and n decreases, so inductively the recursive calls are well-deﬁned, and hence so is A m, n . A ﬁrst-order primitive recursive function is a function of type nat → nat that is deﬁned using the recursor, but without using any higher-order functions. Ackermann’s function is deﬁned so that it is not ﬁrst-order primitive recursive but is higher-order primitive recursive. The key to showing that it is deﬁnable in T is to note that A m + 1, n  iterates n times   73  9.4 Undeﬁnability  the function A m,− , starting with A m, 1 . As an auxiliary, let us deﬁne the higher-order function  it :  nat → nat  → nat → nat → nat  to be the λ-abstraction  λ  f : nat → nat  λ  n : nat  rec n{z  cid:9 → id  s    with g  cid:9 → f ◦ g},  where id = λ  x : nat  x is the identity, and f ◦ g = λ  x : nat  f  g x   is the composition of f and g. It is easy to check that  where the latter expression is the n-fold composition of f starting with m. We may then deﬁne the Ackermann function  it f   n  m  ≡ f  n  m  : nat,  ea : nat → nat → nat  to be the expression  λ  m : nat  rec m{z  cid:9 → s  s    with f  cid:9 → λ  n : nat  it f   n  f  1  }.  It is instructive to check that the following equivalences are valid:  ea 0  n  ≡ s n   ea m + 1  0  ≡ ea m  1   ea m + 1  n + 1  ≡ ea m  ea s m   n  .   9.6   9.7   9.8   That is, the Ackermann function is deﬁnable in T.  9.4 Undeﬁnability  It is impossible to deﬁne an inﬁnite loop in T. Theorem 9.4. If e : τ , then there exists v val such that e ≡ v : τ .  Proof See Corollary 46.15.  Consequently, values of function type in T behave like mathematical functions: if e : τ1 → τ2 and e1 : τ1, then e e1  evaluates to a value of type τ2. Moreover, if e : nat, then there exists a natural number n such that e ≡ n : nat.  Using this, we can show, using a technique called diagonalization, that there are functions on the natural numbers that are not deﬁnable in T. We make use of a technique, called G¨odel-numbering, that assigns a unique natural number to each closed expression of T. By assigning a unique number to each expression, we may manipulate expressions as data values in T so that T is able to compute with its own programs.1   74  System T of Higher-Order Recursion  The essence of G¨odel-numbering is captured by the following simple construction on abstract syntax trees.  The generalization to abstract binding trees is slightly more difﬁcult, the main complication being to ensure that all α-equivalent expressions are assigned the same G¨odel number.  Recall that a general ast a has the form o a1, . . . , ak , where o is an operator of arity k. Enumerate the operators so that every operator has an index i ∈ N, and let m be the index of o in this enumeration. Deﬁne the G¨odel number  cid:4 a cid:5  of a to be the number  2m 3n1 5n2 . . . pnk k ,  where pk is the kth prime number  so that p0 = 2, p1 = 3, and so on , and n1, . . . , nk are the G¨odel numbers of a1, . . . , ak, respectively. This procedure assigns a natural number to each ast. Conversely, given a natural number, n, we may apply the prime factorization theorem to “parse” n as a unique abstract syntax tree.  If the factorization is not of the right form, which can only be because the arity of the operator does not match the number of factors, then n does not code any ast.  Now, using this representation, we may deﬁne a  mathematical  function funiv : N → N → N such that, for any e : nat → nat, funiv  cid:4 e cid:5   m  = n iff e m  ≡ n : nat.2 The determinacy of the dynamics, together with Theorem 9.4, ensure that funiv is a well- deﬁned function. It is called the universal function for T because it speciﬁes the behavior of any expression e of type nat → nat. Using the universal function, let us deﬁne an auxiliary mathematical function, called the diagonal function δ : N → N, by the equation δ m  = funiv m  m . The δ function is chosen so that δ  cid:4 e cid:5   = n iff e  cid:4 e cid:5   ≡ n : nat.  The motivation for its deﬁnition will become clear in a moment.   The function funiv is not deﬁnable in T. Suppose that it were deﬁnable by the expression  euniv, then the diagonal function δ would be deﬁnable by the expression  But in that case we would have the equations  Now let e cid:8  be the function expression  so that we may deduce  eδ = λ  m : nat  euniv m  m .  eδ  cid:4 e cid:5   ≡ euniv  cid:4 e cid:5    cid:4 e cid:5    ≡ e  cid:4 e cid:5  .  λ  x : nat  s eδ x  ,  e cid:8   cid:4 e cid:8  cid:5   ≡ s eδ  cid:4 e cid:8  cid:5    ≡ s e cid:8   cid:4 e cid:8  cid:5   .  But the termination theorem implies that there exists n such that e cid:8   cid:4 e cid:8  cid:5   ≡ n, and hence we have n ≡ s n , which is impossible. We say that a language L is universal if it is possible to write an interpreter for L in L itself. It is intuitively clear that funiv is computable in the sense that we can deﬁne it in   75  Exercises  some sufﬁciently powerful programming language. But the preceding argument shows that T is not up to the task; it is not a universal programming language. Examination of the foregoing proof reveals an inescapable trade-off: by insisting that all expressions terminate, we necessarily lose universality—there are computable functions that are not deﬁnable in the language.  System T was introduced by G¨odel  1958  in his study of the consistency of arithmetic  G¨odel, 1980 . G¨odel showed how to “compile” proofs in arithmetic into well-typed terms of system T, and to reduce the consistency problem for arithmetic to the termination of programs in T. It was perhaps the ﬁrst programming language whose design was directly inﬂuenced by the veriﬁcation  of termination  of its programs.  9.5 Notes  Exercises  9.1. Prove Lemma 9.2. 9.2. Prove Theorem 9.3. 9.3. Attempt to prove that if e : nat is closed, then there exists n such that e  cid:20 −→∗  n under  the eager dynamics. Where does the proof break down?   cid:5   val such that e  cid:20 −→∗  9.4. Attempt to prove termination for all well-typed closed terms: if e : τ, then there exists  cid:5 . You are free to appeal to Lemma 9.2 and Theorem 9.3 as e necessary. Where does the attempt break down? Can you think of a stronger inductive hypothesis that might evade the difﬁculty?  e  9.5. Deﬁne a closed term e of type τ in T to be hereditarily terminating at type τ by induction on the structure of τ as follows:  a  If τ = nat, then e is hereditarily terminating at type τ iff e is terminating  that is,  b  If τ = τ1 → τ2, then e is hereditarily terminating iff when e1 is hereditarily  iff e  cid:20 −→∗  n for some n.   terminating at type τ1, then e e1  is hereditarily terminating at type τ2.  Attempt to prove hereditary termination for well-typed terms: if e : τ, then e is hereditarily terminating at type τ. The stronger inductive hypothesis bypasses the difﬁculty that arose in Exercise 9.4 but introduces another obstacle. What is the complication? Can you think of an even stronger inductive hypothesis that would sufﬁce for the proof?  cid:5   cid:20 −→ e, then e is also hereditarily terminating at type τ.  The need for this result will become clear in the solution to Exercise 9.5.   9.6. Show that if e is hereditarily terminating at type τ, e   cid:5  : τ, and e   76  System T of Higher-Order Recursion  9.7. Deﬁne an open, well-typed term  to be open hereditarily terminating iff every substitution instance  x1 : τ1, . . . , xn : τn  cid:12  e : τ  [e1, . . . , en x1, . . . , xn]e  is closed hereditarily terminating at type τ when each ei is closed hereditarily termi- nating at type τi for each 1 ≤ i ≤ n. Derive Exercise9.3 from this result.  Notes  1 The same technique lies at the heart of the proof of G¨odel’s celebrated incompleteness theorem. The undeﬁnability of certain functions on the natural numbers within T may be seen as a form of incompleteness like that considered by G¨odel.  2 The value of funiv k  m  may be chosen arbitrarily to be zero when k is not the code of any  expression e.   P A R T IV  Finite Data Types    10  Product Types  The binary product of two types consists of ordered pairs of values, one from each type in the order speciﬁed. The associated elimination forms are projections, which select the ﬁrst and second component of a pair. The nullary product, or unit, type consists solely of the unique “null tuple” of no values and has no associated elimination form. The product type admits both a lazy and an eager dynamics. According to the lazy dynamics, a pair is a value without regard to whether its components are values; they are not evaluated until  if ever  they are accessed and used in another computation. According to the eager dynamics, a pair is a value only if its components are values; they are evaluated when the pair is created. More generally, we may consider the ﬁnite product,  cid:24 τi cid:25 i∈I , indexed by a ﬁnite set of indices I. The elements of the ﬁnite product type are I -indexed tuples whose ith component is an element of the type τi, for each i ∈ I. The components are accessed by I -indexed projection operations, generalizing the binary case. Special cases of the ﬁnite product include n-tuples, indexed by sets of the form I = { 0, . . . , n − 1}, and labeled tuples, or records, indexed by ﬁnite sets of symbols. Similarly to binary products, ﬁnite products admit both an eager and a lazy interpretation.  10.1 Nullary and Binary Products  The abstract syntax of products is given by the following grammar:  Typ τ  Exp e  ::= unit ::= triv  prod τ1; τ2   pair e1; e2  pr[l] e  pr[r] e   unit τ1 × τ2  cid:24  cid:25   cid:24 e1, e2 cid:25  e · l e · r  nullary product binary product null tuple ordered pair left projection right projection  There is no elimination form for the unit type, there being nothing to extract from the null tuple.  The statics of product types is given by the following rules.   cid:7   cid:12   cid:24  cid:25 : unit   10.1a    80  Product Types  The dynamics of product types is deﬁned by the following rules:   cid:7   cid:12  e1 : τ1  cid:7   cid:12  e2 : τ2  cid:7   cid:12   cid:24 e1, e2 cid:25  : τ1 × τ2   cid:7   cid:12  e : τ1 × τ2  cid:7   cid:12  e · l : τ1  cid:7   cid:12  e : τ1 × τ2  cid:7   cid:12  e · r : τ2   cid:24  cid:25  val   cid:3   cid:3   [e2 val]  [e1 val]   cid:24 e1, e2 cid:25  val e1  cid:20 −→ e  cid:5  1  cid:24 e1, e2 cid:25   cid:20 −→  cid:24 e  1, e2 cid:25   cid:5  e2  cid:20 −→ e  cid:5  e1 val 2  cid:24 e1, e2 cid:25   cid:20 −→  cid:24 e1, e  cid:5  2   cid:25    cid:4   cid:4   e  cid:20 −→ e e · l  cid:20 −→ e e  cid:20 −→ e e · r  cid:20 −→ e   cid:5   cid:5  · l  cid:5   cid:5  · r  [e1 val] [e2 val]  cid:24 e1, e2 cid:25  ·l  cid:20 −→ e1  [e1 val] [e2 val]  cid:24 e1, e2 cid:25  ·r  cid:20 −→ e2   10.1b    10.1c    10.1d    10.2a    10.2b    10.2c    10.2d    10.2e    10.2f    10.2g    10.2h   The bracketed rules and premises are omitted for a lazy dynamics and included for an eager dynamics of pairing.  The safety theorem applies to both the eager and the lazy dynamics, with the proof  proceeding along similar lines in each case. Theorem 10.1  Safety . 1. If e : τ and e  cid:20 −→ e 2. If e : τ then either e val or there exists e   cid:5 , then e   cid:5  : τ .  cid:5  such that e  cid:20 −→ e   cid:5 .  Proof Preservation is proved by induction on transition deﬁned by rules  10.2 . Progress is proved by induction on typing deﬁned by rules  10.1 .   81  10.2 Finite Products  10.2 Finite Products  The syntax of ﬁnite product types is given by the following grammar:  Typ τ Exp e  ::= prod {i  cid:9 → τi}i∈I   ::= tpl {i  cid:9 → ei}i∈I    pr[i] e    cid:24 τi cid:25 i∈I  cid:24 ei cid:25 i∈I e · i  product tuple projection   cid:5   The variable I stands for a ﬁnite index set over which products are formed. The type prod {i  cid:9 → τi}i∈I  , or i∈I τi for short, is the type of I -tuples of expressions ei of type τi, one for each i ∈ I. AnI -tuple has the form tpl {i  cid:9 → ei}i∈I  , or  cid:24 ei cid:25 i∈I for short, and for each i ∈ I the ith projection from an I-tuple e is written pr[i] e , or e · i for short. When I = { i1, . . . , in }, theI -tuple type may be written in the form  where we make explicit the association of a type to each index i ∈ I. Similarly, we may write   cid:24 i1  cid:9 → τ1, . . . , in  cid:9 → τn cid:25    cid:24 i1  cid:9 → e1, . . . , in  cid:9 → en cid:25   for the I-tuple whose ith component is ei. Finite products generalize empty and binary products by choosing I to be empty or the two-element set { l, r}, respectively. In practice, I is often chosen to be a ﬁnite set of symbols that serve as labels for the components of the tuple to enhance readability.  The statics of ﬁnite products is given by the following rules: . . .  cid:7   cid:12  en : τn   cid:7   cid:12  e1 : τ1   cid:7   cid:12   cid:24 i1  cid:9 → e1, . . . , in  cid:9 → en cid:25  :  cid:24 i1  cid:9 → τ1, . . . , in  cid:9 → τn cid:25    10.3a    cid:7   cid:12  e :  cid:24 i1  cid:9 → τ1, . . . , in  cid:9 → τn cid:25   cid:7   cid:12  e · ik : τk   10.3b  In rule  10.3b , the index ik ∈ I is a particular element of the index set I, whereas in rule  10.3a , the indices i1, . . . , in range over the entire index set I. The dynamics of ﬁnite products is given by the following rules:   1 ≤ k ≤ n    cid:9   ⎡⎢⎣  [e1 val  . . .  en val]   cid:24 i1  cid:9 → e1, . . . , in  cid:9 → en cid:25  val  cid:5  ej−1 val j−1 e = en  cid:5   cid:5  j+1 e j  = e1  cid:5  e 1 = ej+1   cid:5  e n  . . .  . . .  = ej−1   cid:10   ⎤⎥⎦  e1 val  . . .  ej  cid:20 −→ e   cid:24 i1  cid:9 → e1, . . . , in  cid:9 → en cid:25   cid:20 −→  cid:24 i1  cid:9 → e  1, . . . , in  cid:9 → e  cid:5    cid:5  n   cid:25   e  cid:20 −→ e e · i  cid:20 −→ e   cid:5   cid:5  · i  [ cid:24 i1  cid:9 → e1, . . . , in  cid:9 → en cid:25  val]   cid:24 i1  cid:9 → e1, . . . , in  cid:9 → en cid:25  ·i k  cid:20 −→ ek   10.4a    10.4b    10.4c    10.4d    82  Product Types  As formulated, rule  10.4b  speciﬁes that the components of a tuple are evaluated in some sequential order, without specifying the order in which the components are considered. It is not hard, but a bit technically complicated, to impose an evaluation order by imposing a total ordering on the index set and evaluating components according to this ordering.  Theorem 10.2  Safety . If e : τ , then either e val or there exists e e  cid:20 −→ e   cid:5 .   cid:5  such that e   cid:5  : τ and  Proof The safety theorem is decomposed into progress and preservation lemmas, which are proved as in Section 10.1.  10.3 Primitive Mutual Recursion  Using products we may simplify the primitive recursion construct of T so that only the recursive result on the predecessor, and not the predecessor itself, is passed to the successor  cid:5  · r, branch. Writing this as iter{e0; x.e1} e , we may deﬁne rec{e0; x.y.e1} e  to bee where e’ is the expression  iter{ cid:24 z, e0 cid:25 ; x   cid:5   . cid:24 s x   cid:5  · l , [x   cid:5  · r x]e1 cid:25 } e .  The idea is to compute inductively both the number n and the result of the recursive call on n, from which we can compute both n+ 1 and the result of another recursion using e1. The base case is computed directly as the pair of zero and e0. It is easy to check that the statics and dynamics of the recursor are preserved by this deﬁnition.  We may also use product types to implement mutual primitive recursion, in which we deﬁne two functions simultaneously by primitive recursion. For example, consider the following recursion equations deﬁning two mathematical functions on the natural numbers:  e 0  = 1 o 0  = 0  e n + 1  = o n  o n + 1  = e n   Intuitively, e n  is non-zero if and only if n is even, and o n  is non-zero if and only if n is odd.  To deﬁne these functions in T enriched with products, we ﬁrst deﬁne an auxiliary function  eeo of type  nat →  nat × nat   that computes both results simultaneously by swapping back and forth on recursive calls:  λ  n : nat × nat  iter n{z  cid:9 →  cid:24 1, 0 cid:25   s b   cid:9 →  cid:24 b · r, b · l cid:25 }.   83  Exercises  We may then deﬁne eev and eod as follows:  eev  cid:2  λ  n : nat  eeo n  · l eod  cid:2  λ  n : nat  eeo n  · r.  10.4 Notes  Product types are the most basic form of structured data. All languages have some form of product type but often in a form that is combined with other, separable, concepts. Common manifestations of products include  1  functions with “multiple arguments” or “multiple results”;  2  “objects” represented as tuples of mutually recursive functions;  3  “structures,” which are tuples with mutable components. There are many papers on ﬁnite product types, which include record types as a special case. Pierce  2002  provides a thorough account of record types and their subtyping properties  for which, see Chapter 24 . Allen et al.  2006  analyze many of the key ideas in the framework of dependent type theory.  Exercises   cid:5   10.1. A database schema may be thought of as a ﬁnite product type  i∈I τ, in which the columns, or attributes, are labeled by the indices I whose values are restricted to atomic types, such as nat and str. A value of a schema type is called a tuple, or instance, of that schema. A database may be thought of as a ﬁnite sequence of such tuples, called the rows of the database. Give a representation of a database using function, product, and natural numbers types, and deﬁne the project operation that  cid:5  ⊆ I by restricting sends a database with columns I to a database with columns I each row to the speciﬁed columns.  10.2. Rather than choose between a lazy and an eager dynamics for products, we can instead distinguish two forms of product type, called the positive and the negative. The statics of the negative product is given by rules  10.1 , and the dynamics is lazy. The statics of the positive product, written τ1 ⊗ τ2, is given by the following rules:  10.5a    cid:7   cid:12  e1 : τ1  cid:7   cid:12  e2 : τ2  cid:7   cid:12  fuse e1; e2  :τ 1 ⊗ τ2   cid:7   cid:12  e0 : τ1 ⊗ τ2  cid:7  x1 : τ1 x2 : τ2  cid:12  e : τ   cid:7   cid:12  split e0; x1, x2.e  :τ   10.5b   The dynamics of fuse, the introduction form for the positive pair, is eager, essentially because the elimination form, split, extracts both components simultaneously.  Show that the negative product is deﬁnable in terms of the positive product using the unit and function types to express the lazy semantics of negative pairing. Show that the positive product is deﬁnable in terms of the negative product, provided that   84  Product Types  we have at our disposal a let expression with a by-value dynamics so that we may enforce eager evaluation of positive pairs.  10.3. Specializing Exercise 10.2 to nullary products, we obtain a positive and a negative unit type. The negative unit type is given by rules  10.1 , with no elimination forms and one introduction form. Give the statics and dynamics for a positive unit type, and show that the positive and negative unit types are inter-deﬁnable without any further assumptions.   11  Sum Types  Most data structures involve alternatives such as the distinction between a leaf and an interior node in a tree, or a choice in the outermost form of a piece of abstract syntax. Importantly, the choice determines the structure of the value. For example, nodes have children, but leaves do not, and so forth. These concepts are expressed by sum types, speciﬁcally the binary sum, which offers a choice of two things, and the nullary sum, which offers a choice of no things. Finite sums generalize nullary and binary sums to allow an arbitrary number of cases indexed by a ﬁnite index set. As with products, sums come in both eager and lazy variants, differing in how values of sum type are deﬁned.  11.1 Nullary and Binary Sums  The abstract syntax of sums is given by the following grammar:  Typ τ  Exp e  void τ1 + τ2 abort e  l · e r · e  ::= void sum τ1; τ2  ::= abort{τ} e  in[l]{τ1; τ2} e  in[r]{τ1; τ2} e  case e; x1.e1; x2.e2  case e {l · x1  cid:9 → e1  r · x2  cid:9 → e2}  nullary sum binary sum abort left injection right injection case analysis The nullary sum represents a choice of zero alternatives, and hence admits no introduction form. The elimination form, abort e , aborts the computation in the event that e evaluates to a value, which it cannot do. The elements of the binary sum type are labeled to show whether they are drawn from the left or the right summand, either l · e or r · e. A value of the sum type is eliminated by case analysis.  The statics of sum types is given by the following rules.   cid:7   cid:12  e : void  cid:7   cid:12  abort e  : τ  cid:7   cid:12  e : τ1  cid:7   cid:12  e : τ2   cid:7   cid:12  l · e : τ1 + τ2  cid:7   cid:12  r · e : τ1 + τ2   cid:7   cid:12  e : τ1 + τ2  cid:7 , x1 : τ1  cid:12  e1 : τ  cid:7 , x2 : τ2  cid:12  e2 : τ   cid:7   cid:12  case e {l · x1  cid:9 → e1  r · x2  cid:9 → e2} : τ   11.1a    11.1b    11.1c    11.1d    86  Sum Types  For the sake of readability, in rules  11.1b  and  11.1c  we have written l· e and r· e in place of the abstract syntax in[l]{τ1; τ2} e  and in[r]{τ1; τ2} e , which includes the types τ1 and τ2 explicitly. In rule  11.1d , both branches of the case analysis must have the same type. Because a type expresses a static “prediction” on the form of the value of an expression, and because an expression of sum type could evaluate to either form at run-time, we must insist that both branches yield the same type.  The dynamics of sums is given by the following rules: abort e   cid:20 −→ abort e  e  cid:20 −→ e   cid:5    cid:5     cid:3   cid:3   [e val] l · e val [e val] r · e val e  cid:20 −→ e   cid:5   l · e  cid:20 −→ l · e   cid:5   e  cid:20 −→ e   cid:5   r · e  cid:20 −→ r · e e  cid:20 −→ e   cid:5    cid:5    cid:4   cid:4    11.2a    11.2b    11.2c    11.2d    11.2e    11.2g    11.2h   case e {l · x1  cid:9 → e1  r · x2  cid:9 → e2}  cid:20 −→ case e   cid:5  {l · x1  cid:9 → e1  r · x2  cid:9 → e2}   11.2f   case l · e {l · x1  cid:9 → e1  r · x2  cid:9 → e2}  cid:20 −→ [e x1]e1  case r · e {l · x1  cid:9 → e1  r · x2  cid:9 → e2}  cid:20 −→ [e x2]e2  [e val]  [e val]  The bracketed premises and rules are included for an eager dynamics and excluded for a lazy dynamics.  The coherence of the statics and dynamics is stated and proved as usual.  Theorem 11.1  Safety . 1. If e : τ and e  cid:20 −→ e 2. If e : τ , then either e val or e  cid:20 −→ e  cid:5  for some e   cid:5 .   cid:5 , then e   cid:5  : τ .  Proof The proof proceeds by induction on rules  11.2  for preservation, and by induction on rules  11.1  for progress.  11.2 Finite Sums  Just as we may generalize nullary and binary products to ﬁnite products, so may we also generalize nullary and binary sums to ﬁnite sums. The syntax for ﬁnite sums is given by   87  11.2 Finite Sums  the following grammar:  Typ τ Exp e  ::= sum {i  cid:9 → τi}i∈I   ::= in[i]{ cid:8 τ} e   [τi]i∈I i · e  case e;{i  cid:9 → xi.ei}i∈I   case e {i · xi  cid:9 → ei} i∈I   cid:14  The variable I stands for a ﬁnite index set over which sums are formed. The notation  cid:8 τ stands for a ﬁnite function {i  cid:9 → τi}i∈I for some index set I. The type sum {i  cid:9 → τi}i∈I  , i∈I τi for short, is the type of I-classiﬁed values of the form in[i]{I} ei , or i · ei for or short, where i ∈ I and ei is an expression of type τi. AnI -classiﬁed value is analyzed by an I-way case analysis of the form case e;{i  cid:9 → xi.ei}i∈I  . When I = { i1, . . . , in }, the type of I-classiﬁed values may be written  sum injection case analysis  [i1  cid:9 → τ1, . . . , in  cid:9 → τn]  specifying the type associated with each class li ∈ I. Correspondingly, the I-way case analysis has the form  case e {i1 · x1  cid:9 → e1  . . .  in · xn  cid:9 → en}.  Finite sums generalize empty and binary sums by choosing I to be empty or the two- element set { l, r}, respectively. In practice I is often chosen to be a ﬁnite set of symbols that serve as names for the classes so as to enhance readability.  The statics of ﬁnite sums is deﬁned by the following rules:   cid:7   cid:12  e : τk   1 ≤ k ≤ n    cid:7   cid:12  ik · e : [i1  cid:9 → τ1, . . . , in  cid:9 → τn]   cid:7   cid:12  e : [i1  cid:9 → τ1, . . . , in  cid:9 → τn]  cid:7 , x1 : τ1  cid:12  e1 : τ   cid:7   cid:12  case e {i1 · x1  cid:9 → e1  . . .  in · xn  cid:9 → en} : τ  . . .  cid:7 , xn : τn  cid:12  en : τ  These rules generalize the statics for nullary and binary sums given in Section 11.1.  The dynamics of ﬁnite sums is deﬁned by the following rules:   cid:3   [e val] i · e val e  cid:20 −→ e   cid:4    cid:5    cid:5  i · e  cid:20 −→ i · e e  cid:20 −→ e  cid:20 −→ case e i∈I i · e val case i · e {i · xi  cid:9 → ei} i∈I   cid:5    cid:20 −→ [e xi]ei  case e {i · xi  cid:9 → ei}   cid:5  {i · xi  cid:9 → ei}  i∈I  These again generalize the dynamics of binary sums given in Section 11.1.  Theorem 11.2  Safety . If e : τ , then either e val or there exists e   cid:5  : τ such that e  cid:20 −→ e   cid:5 .  Proof The proof is like that for the binary case, as described in Section 11.1.   11.3a    11.3b    11.4a    11.4b    11.4c    11.4d    88  Sum Types  11.3 Applications of Sum Types  Sum types have many uses, several of which we outline here. More interesting examples arise once we also have induction and recursive types, which are introduced in Parts VI and Part VIII.  11.3.1 Void and Unit  It is instructive to compare the types unit and void, which are often confused with one another. The type unit has exactly one element,  cid:24  cid:25 , whereas the type void has no elements at all. Consequently, if e : unit, then if e evaluates to a value, that value is  cid:24  cid:25 —in other words, e has no interesting value. On the other hand, if e : void, then e must not yield a value; if it were to have a value, it would have to be a value of typevoid, of which there are none. Thus, what is called the void type in many languages is really the type unit because it indicates that an expression has no interesting value, not that it has no value at all!  11.3.2 Booleans  Perhaps the simplest example of a sum type is the familiar type of Booleans, whose syntax is given by the following grammar: ::= bool ::= true false if e; e1; e2  if e then e1 else e2 The expression if e; e1; e2  branches on the value of e : bool.  booleans truth falsity conditional  bool true false  Typ τ Exp e  The statics of Booleans is given by the following typing rules:   cid:7   cid:12  true : bool   cid:7   cid:12  false : bool   cid:7   cid:12  e : bool  cid:7   cid:12  e1 : τ  cid:7   cid:12  e2 : τ   cid:7   cid:12  if e then e1 else e2 : τ  The dynamics is given by the following value and transition rules:   11.5a    11.5b    11.5c    11.6a    11.6b   true val  false val   89  11.3 Applications of Sum Types  if true then e1 else e2  cid:20 −→ e1  if false then e1 else e2  cid:20 −→ e2   cid:5  if e then e1 else e2  cid:20 −→ if e  e  cid:20 −→ e   cid:5   then e1 else e2   11.6c    11.6d    11.6e   The type bool is deﬁnable in terms of binary sums and nullary products:  bool = unit + unit true = l ·  cid:24  cid:25  false = r ·  cid:24  cid:25    11.7a   11.7b   11.7c   11.7d  In Equation  11.7d , the variables x1 and x2 are chosen arbitrarily such that x1  ∈ e1 and x2  ∈ e2. It is a simple matter to check that the readily-deﬁned statics and dynamics of the type bool are engendered by these deﬁnitions.  if e then e1 else e2 = case e {l · x1  cid:9 → e1  r · x2  cid:9 → e2}  11.3.3 Enumerations  More generally, sum types can be used to deﬁne ﬁnite enumeration types, those whose values are one of an explicitly given ﬁnite set, and whose elimination form is a case analysis on the elements of that set. For example, the type suit, whose elements are ♣, ♦, ♥, and ♠, has as elimination form the case analysis  case e {♣  cid:9 → e0  ♦  cid:9 → e1  ♥ cid:9 → e2  ♠ cid:9 → e3},  which distinguishes among the four suits. Such ﬁnite enumerations are easily representable as sums. For example, we may deﬁne suit = [unit] ∈I , where I = {♣,♦,♥,♠} and the type family is constant over this set. The case analysis form for a labeled sum is almost literally the desired case analysis for the given enumeration, the only difference being the binding for the uninteresting value associated with each summand, which we may ignore. Other examples of enumeration types abound. For example, most languages have a type char of characters, which is a large enumeration type containing all possible Unicode  or other such standard classiﬁcation  characters. Each character is assigned a code  such as UTF-8  used for interchange among programs. The type char is equipped with operations such as chcode n  that yield the char associated to the code n, and codech c  that yield the code of character c. Using the linear ordering on codes we may deﬁne a total ordering of characters, called the collating sequence determined by that code.   90  Sum Types  11.3.4 Options  Another use of sums is to deﬁne the option types, which have the following syntax:  Typ τ Exp e  ::= opt τ  ::= null  just e  ifnull{τ}{e1; x.e2} e  which e {null  cid:9 → e1  just x   cid:9 → e2}  τ opt null just e   option nothing something  null test  The type opt τ  represents the type of “optional” values of type τ. The introduction forms are null, corresponding to “no value,” and just e , corresponding to a speciﬁed value of type τ. The elimination form discriminates between the two possibilities.  The option type is deﬁnable from sums and nullary products according to the following  equations:1  τ opt = unit + τ null = l ·  cid:24  cid:25  just e  = r · e   11.8a   11.8b   11.8c   11.8d   which e {null  cid:9 → e1  just x2   cid:9 → e2} =case e {l ·   cid:9 → e1  r · x2  cid:9 → e2}  We leave it to the reader to check the statics and dynamics implied by these deﬁnitions.  The option type is the key to understanding a common misconception, the null pointer fallacy. This fallacy arises from two related errors. The ﬁrst error is to deem values of certain types to be mysterious entities called pointers. This terminology arises from suppositions about how these values might be represented at run-time, rather than on their semantic role in the language. The second error compounds the ﬁrst. A particular value of a pointer type is distinguished as the null pointer, which, unlike the other elements of that type, does not stand for a value of that type at all, but rather rejects all attempts to use it. To help avoid such failures, such languages usually include a function, say null : τ → bool, that yields true if its argument is null, and false otherwise. Such a test allows the programmer to take steps to avoid using null as a value of the type it purports to inhabit. Consequently, programs are riddled with conditionals of the form  if null e  then . . . error . . . else . . . proceed . . . .   11.9   Despite this, “null pointer” exceptions at run-time are rampant, in part because it is quite easy to overlook the need for such a test, and in part because detection of a null pointer leaves little recourse other than abortion of the program.  The underlying problem is the failure to distinguish the type τ from the type τ opt. Rather than think of the elements of type τ as pointers, and thereby have to worry about the null pointer, we instead distinguish between a genuine value of type τ and an optional value of type τ. An optional value of type τ may or may not be present, but, if it is, the underlying value is truly a value of type τ  and cannot be null . The elimination form for the option type,  which e {null  cid:9 → eerror  just x   cid:9 → eok},   11.10    91  Exercises  propagates the information that e is present into the non-null branch by binding a genuine value of type τ to the variable x. The case analysis effects a change of type from “optional value of type τ” to “genuine value of type τ,” so that within the non-null branch no further null checks, explicit or implicit, are necessary. Note that such a change of type is not achieved by the simple Boolean-valued test exempliﬁed by expression  11.9 ; the advantage of option types is precisely that they do so.  Heterogeneous data structures are ubiquitous. Sums codify heterogeneity, yet few languages support them in the form given here. The best approximation in commercial languages is the concept of a class in object-oriented programming. A class is an injection into a sum type, and dispatch is case analysis on the class of the data object.  See Chapter 26 for more on this correspondence.  The absence of sums is the origin of C.A.R. Hoare’s self- described “billion dollar mistake,” the null pointer  Hoare, 2009 . Bad language designs put the burden of managing “null” values entirely at run-time, instead of making the possibility or the impossibility of “null” apparent at compile time.  11.4 Notes  Exercises  11.1. Complete the deﬁnition of a ﬁnite enumeration type sketched in Section 11.3.3.  Derive enumeration types from ﬁnite sum types. 11.2. The essence of Hoare’s mistake is the misidentiﬁcation of the type τ opt with the type bool × τ. Values of the latter type are pairs consisting of a boolean “ﬂag” and a value of type τ. The idea is that the ﬂag indicates whether the associated value is “present.” When the ﬂag is true, the second component is present, and, when the ﬂag is false, the second component is absent. Analyze Hoare’s mistake by attempting to deﬁne τ opt to be the type bool × τ  by ﬁlling in the following chart:  null  cid:2  ? just e   cid:2  ? which e {null  cid:9 → e1  just x   cid:9 → e2}  cid:2  ?  Argue that even if we adopt Hoare’s convention of admitting a “null” value of every type, the chart cannot be properly ﬁlled.  11.3. Databases have a version of the “null pointer” problem that arises when not every tuple provides a value for every attribute  such as a person’s middle name . More generally, many commercial databases are limited to a single atomic type for each attribute, presenting problems when the value of that attribute may have several   92  Sum Types  types  for example, one may have different sorts of postal codes depending on the country . Consider how to address these problems using the methods discussed in Exercise 10.1. Suggest how to handle null values and heterogeneous values that avoids some of the complications that arise in traditional formulations of databases.  11.4. A combinational circuit is an open expression of type  x1 : bool, . . . , xn : bool  cid:12  e : bool,  which computes a boolean value from n boolean inputs. Deﬁne a nor and a nand gate as boolean circuits with two inputs and one output. There is no reason to restrict to a single output. For example, deﬁne an half-adder that takes two boolean inputs, but produces two boolean outputs, the sum and the carry outputs of the half-adder. Then deﬁne a full-adder that takes three inputs, the addends and an incoming carry, and produces two outputs, the sum and the outgoing carry. Deﬁne the type nybble to be the product bool × bool × bool × bool. Deﬁne the combinational circuit nybble-adder that takes two nybbles as input and produces a nybble and a carry-out bit as output.  11.5. A signal is a time-varying sequence of booleans, representing the status of the signal at each time instant. An RS latch is a fundamental digital circuit with two input signals and two output signals. Deﬁne the type signal of signals to be the function type nat → bool of inﬁnite sequences of booleans. Deﬁne an RS latch as a function of type   signal × signal  →  signal × signal .  Note  1 We often write an underscore in place of a bound variable that is not used within its scope.   P A R T V  Types and Propositions    12  Constructive Logic  Constructive logic codiﬁes the principles of mathematical reasoning as it is actually prac- ticed. In mathematics a proposition is judged true exactly when it has a proof, and is judged false exactly when it has a refutation. Because there are, and always will be, unsolved problems, we cannot expect in general that a proposition is either true or false, for in most cases, we have neither a proof nor a refutation of it. Constructive logic can be described as logic as if people matter, as distinct from classical logic, which can be described as the logic of the mind of god.  From a constructive viewpoint, a proposition is true when it has a proof. What is a proof is a social construct, an agreement among people as to what is a valid argument. The rules of logic codify a set of principles of reasoning that may be used in a valid proof. The valid forms of proof are determined by the outermost structure of the proposition whose truth is asserted. For example, a proof of a conjunction consists of a proof of each conjunct, and a proof of an implication transforms a proof of its antecedent to a proof of its consequent. When spelled out in full, the forms of proof are seen to correspond exactly to the forms of expression of a programming language. To each proposition is associated the type of its proofs; a proof is then an expression of the associated type. This association between programs and proofs induces a dynamics on proofs. In this way, proofs in constructive logic have computational content, which is to say that they are interpreted as executable programs of the associated type. Conversely, programs have mathematical content as proofs of the proposition associated to their type.  The uniﬁcation of logic and programming is called the propositions as types principle. It is a central organizing principle of the theory of programming languages. Propositions are identiﬁed with types, and proofs are identiﬁed with programs. A programming technique corresponds to a method of proof; a proof technique corresponds to a method of program- ming. Viewing types as behavioral speciﬁcations of programs, propositions are problem statements whose proofs are solutions that implement the speciﬁcation.  12.1 Constructive Semantics  Constructive logic is concerned with two judgments, namely φ prop, stating that φ expresses a proposition, and φ true, stating that φ is a true proposition. What distinguishes constructive from non-constructive logic is that a proposition is not conceived of as merely a truth value, but instead as a problem statement whose solution, if it has one, is given by a proof. A   96  Constructive Logic  proposition is true exactly when it has a proof, in keeping with ordinary mathematical practice. In practice, there is no other criterion of truth than the existence of a proof.  Identifying truth with proof has important, and possibly surprising, consequences. The most important consequence is that we cannot say, in general, that a proposition is either true or false. If for a proposition to be true means to have a proof of it, what does it mean for a proposition to be false? It means that we have a refutation of it, showing that it cannot be proved. That is, a proposition is false if we can show that the assumption that it is true  has a proof  contradicts known facts. In this sense, constructive logic is a logic of positive, or afﬁrmative, information—we must have explicit evidence in the form of a proof to afﬁrm the truth or falsity of a proposition.  In light of this, it is clear that not every proposition is either true or false. For if φ expresses ?= NP problem, then we have neither a proof an unsolved problem, such as the famous P nor a refutation of it  the mere absence of a proof not being a refutation . Such a problem is undecided, precisely because it has not been solved. Because there will always be unsolved problems  there being inﬁnitely many propositions, but only ﬁnitely many proofs at a given point in time , we cannot say that every proposition is decidable, that is, either true or false.  Of course, some propositions are decidable, and hence are either true or false. For example, if φ expresses an inequality between natural numbers, then φ is decidable, because we can always work out, for given natural numbers m and n, whether m ≤ n or m  cid:6 ≤ n—we can either prove or refute the given inequality. This argument does not extend to the real numbers. To get an idea of why not, consider the representation of a real number by its decimal expansion. At any ﬁnite time, we will have explored only a ﬁnite initial segment of the expansion, which is not enough to decide if it is, say, less than 1. For if we have calculated the expansion to be 0.99 . . . 9, we cannot decide at any time, short of inﬁnity, whether or not the number is 1.  The constructive attitude is simply to accept the situation as inevitable, and make our peace with that. When faced with a problem, we have no choice but to roll up our sleeves and try to prove it or refute it. There is no guarantee of success! Life is hard, but we muddle through somehow.  12.2 Constructive Logic  The judgments φ prop and φ true of constructive logic are rarely of interest by themselves, but rather in the context of a hypothetical judgment of the form  φ1 true, . . . , φn true  cid:12  φ true.  This judgment says that the proposition φ is true  has a proof , under the assumptions that each of φ1, . . . , φn are also true  have proofs . Of course, when n = 0 this is just the same as the judgment φ true.   97  12.2 Constructive Logic  The structural properties of the hypothetical judgment, when specialized to constructive  logic, deﬁne what we mean by reasoning under hypotheses:   cid:7   cid:12  φ1 true  cid:7 , φ1 true  cid:12  φ2 true   cid:7 , φ true  cid:12  φ true   cid:7   cid:12  φ2 true  cid:7   cid:12  φ2 true   cid:7 , φ1 true  cid:12  φ2 true   cid:7 , φ1 true, φ1 true  cid:12  φ2 true   cid:7 , φ1 true  cid:12  φ2 true   cid:7 1, φ2 true, φ1 true,  cid:7 2  cid:12  φ true  cid:7 1, φ1 true, φ2 true,  cid:7 2  cid:12  φ true   12.1a    12.1b    12.1c    12.1d    12.1e   The last two rules are implicit in that we regard  cid:7  as a set of hypotheses, so that two “copies” are as good as one, and the order of hypotheses does not matter.  12.2.1 Provability  The syntax of propositional logic is given by the following grammar:   cid:31  ⊥  Prop φ ::=  cid:31  ⊥ ∧ φ1; φ2  φ1 ∧ φ2 ∨ φ1; φ2  φ1 ∨ φ2 ⊃ φ1; φ2  φ1 ⊃ φ2  truth falsity conjunction disjunction implication  The connectives of propositional logic are given meaning by rules that deﬁne  a  what constitutes a “direct” proof of a proposition formed from that connective, and  b  how to exploit the existence of such a proof in an “indirect” proof of another proposition. These are called the introduction and elimination rules for the connective. The principle of conservation of proof states that these rules are inverse to one another—the elimination rule cannot extract more information  in the form of a proof  than was put into it by the introduction rule, and the introduction rules can reconstruct a proof from the information extracted by the elimination rules.  Truth Our ﬁrst proposition is trivially true. No information goes into proving it, and so no information can be obtained from it.   cid:7   cid:12   cid:31 true   no elimination rule    12.2a    12.2b    98  Constructive Logic  Conjunction Conjunction expresses the truth of both of its conjuncts.   cid:7   cid:12  φ1 true  cid:7   cid:12  φ2 true   cid:7   cid:12  φ1 ∧ φ2 true  cid:7   cid:12  φ1 ∧ φ2 true   cid:7   cid:12  φ1 true   cid:7   cid:12  φ1 ∧ φ2 true   cid:7   cid:12  φ2 true   cid:7 , φ1 true  cid:12  φ2 true  cid:7   cid:12  φ1 ⊃ φ2 true   cid:7   cid:12  φ1 ⊃ φ2 true  cid:7   cid:12  φ1 true   cid:7   cid:12  φ2 true   cid:7   cid:12  ⊥ true  cid:7   cid:12  φ true   cid:7   cid:12  φ1 true   cid:7   cid:12  φ1 ∨ φ2 true   cid:7   cid:12  φ2 true   cid:7   cid:12  φ1 ∨ φ2 true  Implication Implication expresses the truth of a proposition under an assumption.  Falsehood Falsehood expresses the trivially false  refutable  proposition.   no introduction rule   Disjunction Disjunction expresses the truth of either  or both  of two propositions.   12.3a    12.3b    12.3c    12.4a    12.4b    12.5a    12.5b    12.6a    12.6b    12.6c    cid:7   cid:12  φ1 ∨ φ2 true  cid:7 , φ1 true  cid:12  φ true  cid:7 , φ2 true  cid:12  φ true   cid:7   cid:12  φ true  Negation The negation, ¬φ, of a proposition φ is deﬁned as the implication φ ⊃⊥. As a result, ¬φ true if φ true  cid:12  ⊥ true, which is to say that the truth of φ is refutable in that we may derive a proof of falsehood from any purported proof of φ. Because constructive truth is deﬁned to be the existence of a proof, the implied semantics of negation is rather strong. In particular, a problem φ is open exactly when we can neither afﬁrm nor refute it. In contrast, the classical conception of truth assigns a ﬁxed truth value to each proposition so that every proposition is either true or false.   99  12.2 Constructive Logic  12.2.2 Proof Terms  The key to the propositions-as-types principle is to make explicit the forms of proof. The basic judgment φ true, which states that φ has a proof, is replaced by the judgment p : φ, stating that p is a proof of φ.  Sometimes p is called a “proof term,” but we will simply call p a “proof.”  The hypothetical judgment is modiﬁed correspondingly, with variables standing for the presumed, but unknown, proofs:  x1 : φ1, . . . , xn : φn  cid:12  p : φ.  We again let  cid:7  range over such hypothesis lists, subject to the restriction that no variable occurs more than once.  The syntax of proof terms is given by the following grammar: Prf p ::= true-I   cid:24  cid:25   cid:24 p1, p2 cid:25  p · l p · r λ  x  p p1 p2  abort p  l · p r · p  and-I p1; p2  and-E[l] p  and-E[r] p  imp-I x.p  imp-E p1; p2  false-E p  or-I[l] p  or-I[r] p  or-E p; x1.p1; x2.p2  case p {l · x1  cid:9 → p1  r · x2  cid:9 → p2}  truth intro conj. intro conj. elim conj. elim impl. intro impl. elim false elim disj. intro disj. intro disj. elim  The concrete syntax of proof terms is chosen to stress the correspondence between propo- sitions and types discussed in Section 12.4 below.  The rules of constructive propositional logic can be restated using proof terms as  follows.   cid:7   cid:12   cid:24  cid:25 :  cid:31    cid:7   cid:12  p1 : φ1  cid:7   cid:12  p2 : φ2  cid:7   cid:12   cid:24 p1, p2 cid:25  : φ1 ∧ φ2   cid:7   cid:12  p1 : φ1 ∧ φ2  cid:7   cid:12  p1 · l : φ1  cid:7   cid:12  p1 : φ1 ∧ φ2  cid:7   cid:12  p1 · r : φ2  cid:7 , x : φ1  cid:12  p2 : φ2  cid:7   cid:12  λ  x  p2 : φ1 ⊃ φ2   cid:7   cid:12  p : φ1 ⊃ φ2  cid:7   cid:12  p1 : φ1   cid:7   cid:12  p p1  :φ 2   12.7a    12.7b    12.7c    12.7d    12.7e    12.7f    100  Constructive Logic   cid:7   cid:12  p : ⊥   cid:7   cid:12  abort p  :φ  cid:7   cid:12  p1 : φ1   cid:7   cid:12  l · p1 : φ1 ∨ φ2   cid:7   cid:12  p2 : φ2   cid:7   cid:12  r · p2 : φ1 ∨ φ2   cid:7   cid:12  p : φ1 ∨ φ2  cid:7 , x1 : φ1  cid:12  p1 : φ  cid:7 , x2 : φ2  cid:12  p2 : φ   cid:7   cid:12  case p {l · x1  cid:9 → p1  r · x2  cid:9 → p2} : φ  12.3 Proof Dynamics  Proof terms in constructive logic are given a dynamics by Gentzen’s Principle. It states that the elimination forms are inverse to the introduction forms. One aspect of Gentzen’s Prin- ciple is the principle of conservation of proof, which states that the information introduced into a proof of a proposition can be extracted without loss by elimination. For example, we may state that conjunction elimination is post-inverse to conjunction introduction by the deﬁnitional equations:  Another aspect of Gentzen’s Principle is that principle of reversibility of proof, which states that every proof can be reconstructed from the information that can be extracted from it by elimination. In the case of conjunction this can be stated by the deﬁnitional equation  Similar equivalences can be stated for the other connectives. For example, the conserva-  tion and reversibility principles for implication are given by these rules:   cid:7   cid:12  p1 : φ1  cid:7   cid:12  p2 : φ2  cid:7   cid:12   cid:24 p1, p2 cid:25  ·l ≡ p1 : φ1  cid:7   cid:12  p1 : φ1  cid:7   cid:12  p2 : φ2  cid:7   cid:12   cid:24 p1, p2 cid:25  ·r ≡ p2 : φ2   cid:7   cid:12  p1 : φ1  cid:7   cid:12  p2 : φ2   cid:7   cid:12   cid:24 p · l, p · r cid:25  ≡p : φ1 ∧ φ2   cid:7 , x : φ1  cid:12  p2 : φ2  cid:7   cid:12  p2 : φ2  cid:7   cid:12   λ  x  p2  p1  ≡ [p1 x]p2 : φ2   cid:7   cid:12  p : φ1 ⊃ φ2   cid:7   cid:12  λ  x   p x   ≡ p : φ1 ⊃ φ2  The corresponding rules for disjunction and falsehood are given as follows:   cid:7   cid:12  p : φ1 ∨ φ2  cid:7 , x1 : φ1  cid:12  p1 : ψ  cid:7 , x2 : φ2  cid:12  p2 : ψ  cid:7   cid:12  case l · p {l · x1  cid:9 → p1  r · x2  cid:9 → p2} ≡[p x 1]p1 : ψ  cid:7   cid:12  p : φ1 ∨ φ2  cid:7 , x1 : φ1  cid:12  p1 : ψ  cid:7 , x2 : φ2  cid:12  p2 : ψ  cid:7   cid:12  case r · p {l · x1  cid:9 → p1  r · x2  cid:9 → p2} ≡[p x 2]p2 : ψ   12.7g    12.7h    12.7i    12.7j    12.8a    12.8b    12.9    12.10a    12.10b    12.11a    12.11b    101  12.5 Notes   cid:7   cid:12  p : φ1 ∨ φ2  cid:7 , x : φ1 ∨ φ2  cid:12  q : ψ   cid:7   cid:12  [p x]q ≡ case p {l · x1  cid:9 → [l · x1 x]q  r · x2  cid:9 → [r · x2 x]q} : ψ   cid:7   cid:12  p : ⊥  cid:7 , x : ⊥  cid:12 q : ψ  cid:7   cid:12  [p x]q ≡ abort p  :ψ  12.4 Propositions as Types   12.11c    12.11d   Reviewing the statics and dynamics of proofs in constructive logic reveals a striking similar- ity to the statics and dynamics of expressions of various types. For example, the introduction rule for conjunction speciﬁes that a proof of a conjunction consists of a pair of proofs, one for each conjunct, and the elimination rule inverts this, allowing us to extract a proof of each conjunct from any proof of a conjunction. There is an obvious analogy with the static semantics of product types, whose introduction form is a pair and whose elimination forms are projections. Gentzen’s Principle extends the analogy to the dynamics as well, so that the elimination forms for conjunction amount to projections that extract the appropriate components from an ordered pair.  The following chart summarizes the correspondence between propositions and types and  between proofs and programs:  Prop  cid:31  ⊥ φ1 ∧ φ2 φ1 ⊃ φ2 φ1 ∨ φ2  Type unit void τ1 × τ2 τ1 → τ2 τ1 + τ2  12.5 Notes  The correspondence between propositions and types is a cornerstone of the theory of programming languages. It exposes a deep connection between computation and deduction, and serves as a framework for the analysis of language constructs and reasoning principles by relating them to one another.  The propositions as types principle has its origins in the semantics of intuitionistic logic developed by Brouwer, according to which the truth of a proposition is witnessed by a construction providing computable evidence for it. The forms of evidence are determined by the form of the proposition, so that evidence for an implication is a computable function transforming evidence for the hypothesis into evidence for the conclusion. An explicit formulation of this semantics was introduced by Heyting, and further developed by several people, including de Bruijn, Curry, Gentzen, Girard, Howard, Kolmogorov, Martin-L¨of, and Tait. The propositions-as-types correspondence is sometimes called the Curry-Howard   102  Constructive Logic  Isomorphism, but this terminology neglects the crucial contributions of the others just mentioned. Moreover, the correspondence is not, in general, an isomorphism; rather, it expresses Brouwer’s Dictum that the concept of proof is best explained by the more general concept of construction  program .  Exercises  12.1. The law of the excluded middle  LEM  is the statement that every proposition φ is decidable in the sense that φ ∨ ¬φ true. Constructively, the law of the excluded middle states that, for every proposition φ, we either have a proof of φ or a refutation of φ  proof of its negation . Because this is manifestly not the case in general, one may suspect that the law of the excluded middle is not constructively valid. This is so, but not in the sense that the law is refuted, but rather in the sense that it is not afﬁrmed. First, any proposition φ for which we have a proof or a refutation is already decided, and so is decidable. Second, there are broad classes of propositions for which we can, on demand, produce a proof or a refutation. For example, it is decidable whether or not two integers are equal. Third, and most important, there are, and always will be, propositions φ whose status is unresolved: it may turn out that φ is true, or it may turn out that φ is false. For all these reasons, constructive logic does not refute the decidability propositions: ¬¬ φ ∨ ¬φ  true for any proposition φ. Prove it using the rules given in this chapter. 12.2. The proposition ¬¬φ is no stronger than φ: prove φ ⊃ ¬¬φ true. Thelaw of double- negation elimination  DNE  states that  ¬¬φ  ⊃ φ true for every proposition φ. It follows immediately from Exercise 12.1 that DNE entails LEM; prove the converse. 12.3. Deﬁne the relation φ ≤ ψ to mean that φ true  cid:12  ψ true according to the rules of constructive logic given above. With respect to this relation, show the following facts:  a  It is a pre-order, which is say that it is reﬂexive and transitive.  b  φ ∧ ψ is the meet, or greatest lower bound, of φ and ψ, and  cid:31  is the top, or  c  Show that φ ∨ ψ is the join, or least upper bound, of φ and ψ, and that ⊥ is the bottom, or least, element.  d  Show that φ ⊃ ψ is an exponential, or pseudo-complement, in the sense that it is the largest ρ such that φ ∧ ρ ≤ ψ.  The exponential φ ⊃ ψ is sometimes written ψ φ.   greatest, element.  Altogether these facts state that entailment in constructive propositional logic forms a Heyting algebra. Show that a general Heyting algebra  that is, an ordering with the above structure  is distributive in the sense that  φ ∧  ψ1 ∨ ψ2  ≡  φ ∧ ψ1  ∨  φ ∧ ψ2  φ ∨  ψ1 ∧ ψ2  ≡  φ ∨ ψ1  ∧  φ ∨ ψ2 ,  where φ ≡ ψ means φ ≤ ψ and ψ ≤ φ.   103  Exercises  12.4. In any Heyting algebra, we have φ ∧ ¬φ ≤⊥, which is to say that the negation is inconsistent with the negated. But ¬φ is not necessarily the complement of φ in the sense that φ ∨ ¬φ ≤  cid:31 . A Boolean algebra is a Heyting algebra in which negation is always the complement of the negated:  cid:31  ≤ φ ∨ ¬φ for every φ. Check that the two-element Boolean algebra for which meets, joins, and exponentials are given by the classical truth tables  deﬁning φ ⊃ ψ as  ¬φ  ∨ ψ is Boolean algebra. Conclude that it is consistent to adjoin LEM to constructive logic, which is to say that classical logic is a special case of constructive logic in which we assume that every proposition is decidable. Being a Heyting algebra, every Boolean algebra is clearly distributive. Show that every Boolean algebra also satisﬁes the de Morgan duality laws:  ¬ φ ∨ ψ  ≡ ¬φ ∧ ¬ψ ¬ φ ∧ ψ  ≡ ¬φ ∨ ¬ψ.  The ﬁrst of these is valid in any Heyting algebra; the second only in a Boolean algebra.   13  Classical Logic  In constructive logic, a proposition is true exactly when it has a proof, a derivation of it from axioms and assumptions, and is false exactly when it has a refutation, a derivation of a contradiction from the assumption that it is true. Constructive logic is a logic of positive evidence. To afﬁrm or deny a proposition requires a proof, either of the proposition itself, or of a contradiction, under the assumption that it has a proof. We are not always able to afﬁrm or deny a proposition. An open problem is one for which we have neither a proof nor a refutation—constructively speaking, it is neither true nor false.  In contrast, classical logic  the one we learned in school  is a logic of perfect information where every proposition is either true or false. We may say that classical logic corresponds to “god’s view” of the world—there are no open problems, rather all propositions are either true or false. Put another way, to assert that every proposition is either true or false is to weaken the notion of truth to encompass all that is not false, dually to the constructively  and classically  valid interpretation of falsity as all that is not true. The symmetry between truth and falsity is appealing, but there is a price to pay for this: the meanings of the logical connectives are weaker in the classical case than in the constructive.  The law of the excluded middle provides a prime example. Constructively, this principle is not universally valid, as we have seen in Exercise 12.1. Classically, however, it is valid, because every proposition is either false or not false, and being not false is the same as being true. Nevertheless, classical logic is consistent with constructive logic in that constructive logic does not refute classical logic. As we have seen, constructive logic proves that the law of the excluded middle is positively not refuted  its double negation is constructively true . Consequently, constructive logic is stronger  more expressive  than classical logic, because it can express more distinctions  namely, between afﬁrmation and irrefutability , and because it is consistent with classical logic.  Proofs in constructive logic have computational content: they can be executed as pro- grams, and their behavior is described by their type. Proofs in classical logic also have computational content, but in a weaker sense than in constructive logic. Rather than posi- tively afﬁrm a proposition, a proof in classical logic is a computation that cannot be refuted. Computationally, a refutation consists of a continuation, or control stack, that takes a proof of a proposition and derives a contradiction from it. So a proof of a proposition in classical logic is a computation that, when given a refutation of that proposition derives a contra- diction, witnessing the impossibility of refuting it. In this sense, the law of the excluded middle has a proof, precisely because it is irrefutable.   105  13.1 Classical Logic  13.1 Classical Logic  In constructive logic, a connective is deﬁned by giving its introduction and elimination rules. In classical logic, a connective is deﬁned by giving its truth and falsity conditions. Its truth rules correspond to introduction, and its falsity rules to elimination. The symmetry between truth and falsity is expressed by the principle of indirect proof. To show that φ true it is enough to show that φ false entails a contradiction, and, conversely, to show that φ false it is enough to show that φ true leads to a contradiction. Although the second of these is constructively valid, the ﬁrst is fundamentally classical, expressing the principle of indirect proof.  13.1.1 Provability and Refutability  There are three basic judgment forms in classical logic:  1. φ true, stating that the proposition φ is provable; 2. φ false, stating that the proposition φ is refutable; 3. , stating that a contradiction has been derived.  These are extended to hypothetical judgments in which we admit both provability and refutability assumptions:  φ1 false, . . . , φm false ψ1 true, . . . , ψn true  cid:12  J.  The hypotheses are divided into two zones, one for falsity assumptions,  cid:8 , and one for truth assumptions,  cid:7 .  The rules of classical logic are organized around the symmetry between truth and falsity,  which is mediated by the contradiction judgment.  The hypothetical judgment is reﬂexive:  The remaining rules are stated so that the structural properties of weakening, contraction, and transitivity are admissible.  A contradiction arises when a proposition is judged both true and false. A proposition is  true if its falsity is absurd, and is false if its truth is absurd.  cid:8   cid:7   cid:12  φ false  cid:8   cid:7   cid:12  φ true   cid:8 , φ false  cid:7   cid:12  φ false  cid:8   cid:7 , φ true  cid:12  φ true   cid:8   cid:7   cid:12     cid:8 , φ false  cid:7   cid:12    cid:8   cid:7   cid:12  φ true  cid:8   cid:7 , φ true  cid:12    cid:8   cid:7   cid:12  φ false   13.1a    13.1b    13.1c    13.1d    13.1e    106  Classical Logic  Truth is trivially true and cannot be refuted.  A conjunction is true if both conjuncts are true and is false if either conjunct is false.   cid:8   cid:7   cid:12  φ1 true  cid:8   cid:7   cid:12  φ2 true   cid:8   cid:7   cid:12   cid:31 true   cid:8   cid:7   cid:12  φ1 ∧ φ2 true  cid:8   cid:7   cid:12  φ1 false   cid:8   cid:7   cid:12  φ1 ∧ φ2 false   cid:8   cid:7   cid:12  φ2 false   cid:8   cid:7   cid:12  φ1 ∧ φ2 false   cid:8   cid:7   cid:12  ⊥false   cid:8   cid:7   cid:12  φ1 true   cid:8   cid:7   cid:12  φ1 ∨ φ2 true   cid:8   cid:7   cid:12  φ2 true   cid:8   cid:7   cid:12  φ1 ∨ φ2 true   cid:8   cid:7   cid:12  φ false  cid:8   cid:7   cid:12  ¬φ true  cid:8   cid:7   cid:12  φ true  cid:8   cid:7   cid:12  ¬φ false  Falsity is trivially false and cannot be proved.  A disjunction is true if either disjunct is true and is false if both disjuncts are false.   cid:8   cid:7   cid:12  φ1 false  cid:8   cid:7   cid:12  φ2 false   cid:8   cid:7   cid:12  φ1 ∨ φ2 false  Negation inverts the sense of each judgment:  An implication is true if its conclusion is true when the assumption is true and is false if  its conclusion is false yet its assumption is true.   cid:8   cid:7 , φ1 true  cid:12  φ2 true  cid:8   cid:7   cid:12  φ1 ⊃ φ2 true   cid:8   cid:7   cid:12  φ1 true  cid:8   cid:7   cid:12  φ2 false   cid:8   cid:7   cid:12  φ1 ⊃ φ2 false  13.1.2 Proofs and Refutations  To explain the dynamics of classical proofs, we ﬁrst introduce an explicit syntax for proofs and refutations. We will deﬁne three hypothetical judgments for classical logic with explicit derivations: 1.  cid:8   cid:7   cid:12  p : φ, stating that p is a proof of φ;   13.1f    13.1g    13.1h    13.1i    13.1j    13.1k    13.1l    13.1m    13.1n    13.1o    13.1p    13.1q    107  13.1 Classical Logic  2.  cid:8   cid:7   cid:12  k ÷ φ, stating that k is a refutation of φ; 3.  cid:8   cid:7   cid:12  k  p, stating that k and p are contradictory.  The falsity assumptions  cid:8  are given by a context of the form  u1 ÷ φ1, . . . , um ÷ φm,  where m ≥ 0, in which the variables u1, . . . , un stand for refutations. The truth assumptions  cid:7  are given by a context of the form  where n ≥ 0, in which the variables x1, . . . , xn stand for proofs. The syntax of proofs and refutations is given by the following grammar:  x1 : ψ1, . . . , xn : ψn,  Prf p ::= true-T  Ref  k  ::= false-F   cid:24  cid:25   cid:24 p1, p2 cid:25  l · p r · p not k  λ  x  p  and-T p1; p2  or-T[l] p  or-T[r] p  not-T k  imp-T x.p  ccr u. k  p   ccr u. k  p    abort fst ; k snd ; k case k1; k2  not p  ap p  ; k  and-F[l] k  and-F[r] k  or-F k1; k2  not-F p  imp-F p; k  ccp x. k  p   ccp x. k  p    truth conjunction disjunction left disjunction right negation implication contradiction falsehood conjunction left conjunction right disjunction negation implication contradiction  Proofs serve as evidence for truth judgments, and refutations serve as evidence for false judgments. Contradictions are witnessed by the juxtaposition of a proof and a refutation.  A contradiction arises when a proposition is both true and false:  Truth and falsity are deﬁned symmetrically in terms of contradiction:   cid:8   cid:7   cid:12  k ÷ φ  cid:8   cid:7   cid:12  p : φ   cid:8   cid:7   cid:12  k  p   cid:8 , u ÷ φ  cid:7   cid:12  k  p   cid:8   cid:7   cid:12  ccr u. k  p   : φ   cid:8   cid:7 , x : φ  cid:12  k  p   cid:8   cid:7   cid:12  ccp x. k  p   ÷ φ   cid:8 , u ÷ φ  cid:7   cid:12  u ÷ φ  cid:8   cid:7 , x : φ  cid:12  x : φ  Reﬂexivity corresponds to the use of a variable hypothesis:  The other structure properties are admissible.   13.2a    13.2b    13.2c    13.2d    13.2e    108  Classical Logic  Truth is trivially true and cannot be refuted.  A conjunction is true if both conjuncts are true and is false if either conjunct is false.  Falsity is trivially false and cannot be proved.  A disjunction is true if either disjunct is true and is false if both disjuncts are false.   cid:8   cid:7   cid:12   cid:24  cid:25 :  cid:31    cid:8   cid:7   cid:12  p1 : φ1  cid:8   cid:7   cid:12  p2 : φ2   cid:8   cid:7   cid:12   cid:24 p1, p2 cid:25  : φ1 ∧ φ2   cid:8   cid:7   cid:12  k1 ÷ φ1   cid:8   cid:7   cid:12  fst ; k1 ÷ φ1 ∧ φ2   cid:8   cid:7   cid:12  k2 ÷ φ2   cid:8   cid:7   cid:12  snd ; k2 ÷ φ1 ∧ φ2   cid:8   cid:7   cid:12  abort ÷ ⊥   cid:8   cid:7   cid:12  p1 : φ1   cid:8   cid:7   cid:12  l · p1 : φ1 ∨ φ2   cid:8   cid:7   cid:12  p2 : φ2   cid:8   cid:7   cid:12  r · p2 : φ1 ∨ φ2   cid:8   cid:7   cid:12  k1 ÷ φ1  cid:8   cid:7   cid:12  k2 ÷ φ2  cid:8   cid:7   cid:12  case k1; k2  ÷ φ1 ∨ φ2   cid:8   cid:7   cid:12  k ÷ φ  cid:8   cid:7   cid:12  not k  :¬φ   cid:8   cid:7   cid:12  p : φ   cid:8   cid:7   cid:12  not p  ÷ ¬φ   cid:8   cid:7 , x : φ1  cid:12  p2 : φ2  cid:8   cid:7   cid:12  λ  x  p2 : φ1 ⊃ φ2   cid:8   cid:7   cid:12  p1 : φ1  cid:8   cid:7   cid:12  k2 ÷ φ2  cid:8   cid:7   cid:12  ap p1  ;k 2 ÷ φ1 ⊃ φ2  Negation inverts the sense of each judgment:  An implication is true if its conclusion is true when the assumption is true and is false if  its conclusion is false, yet its assumption is true.   13.2f    13.2g    13.2h    13.2i    13.2j    13.2k    13.2l    13.2m    13.2n    13.2o    13.2p    13.2q    109  13.2 Deriving Elimination Forms  13.2 Deriving Elimination Forms  The price of achieving a symmetry between truth and falsity in classical logic is that we must very often rely on the principle of indirect proof: to show that a proposition is true, we often must derive a contradiction from the assumption of its falsity. For example, a proof of   φ ∧  ψ ∧ θ   ⊃  θ ∧ φ   λ  w  ccr u. k  w  ,  in classical logic has the form  where k is the refutation  fst ; ccp x. snd ; ccp y. snd ; ccp z. u   cid:24 z, x cid:25     y    w  .  And yet in constructive logic this proposition has a direct proof that avoids the circumlo- cutions of proof by contradiction:  λ  w  cid:24 w · r · r, w · l cid:25 .  But this proof cannot be expressed  as is  in classical logic, because classical logic lacks the elimination forms of constructive logic.  However, we may package the use of indirect proof into a slightly more palatable form  by deriving the elimination rules of constructive logic. For example, the rule   cid:8   cid:7   cid:12  φ ∧ ψ true  cid:8   cid:7   cid:12  φ true  is derivable in classical logic:   cid:8 , φ false  cid:7   cid:12  φ false  cid:8 , φ false  cid:7   cid:12  φ ∧ ψ false   cid:8   cid:7   cid:12  φ ∧ ψ true   cid:8 , φ false  cid:7   cid:12  φ ∧ ψ true   cid:8 , φ false  cid:7   cid:12    cid:8   cid:7   cid:12  φ true  The other elimination forms are derivable similarly, in each case relying on indirect proof to construct a proof of the truth of a proposition from a derivation of a contradiction from the assumption of its falsity.  The derivations of the elimination forms of constructive logic are most easily exhibited  using proof and refutation expressions, as follows:  abort p  = ccr u. abort  p   p · l = ccr u. fst ; u  p   p · r = ccr u. snd ; u  p   p1 p2  = ccr u. ap p2  ; u  p1    case p1 {l · x  cid:9 → p2  r · y  cid:9 → p} = ccr u. case ccp x. u  p2  ; ccp y. u  p     p1      13.3    13.4    13.5a   13.5b   13.5c   13.5d   13.5e   13.5f    13.5g   13.5h   110  Classical Logic  The expected elimination rules are valid for these deﬁnitions. For example, the rule  is derivable using the deﬁnition of p1 p2  given above. By suppressing proof terms, we may derive the corresponding provability rule   cid:8   cid:7   cid:12  p1 : φ ⊃ ψ  cid:8   cid:7   cid:12  p2 : φ   cid:8   cid:7   cid:12  p1 p2  :ψ   cid:8   cid:7   cid:12  φ ⊃ ψ true  cid:8   cid:7   cid:12  φ true   cid:8   cid:7   cid:12  ψ true  .  13.3 Proof Dynamics  The dynamics of classical logic arises from the simpliﬁcation of the contradiction between a proof and a refutation of a proposition. To make this explicit, we will deﬁne a transition system whose states are contradictions k  p consisting of a proof p and a refutation k of the same proposition. The steps of the computation consist of simpliﬁcations of the contradictory state based on the form of p and k.  The truth and falsity rules for the connectives play off one another in a pleasing way:  fst ; k   cid:24 p1, p2 cid:25   cid:20 −→ k  p1 snd ; k   cid:24 p1, p2 cid:25   cid:20 −→ k  p2 case k1; k2  l · p1  cid:20 −→ k1  p1 case k1; k2  r · p2  cid:20 −→ k2  p2 not p  not k    cid:20 −→ k  p  ap p1  ; k  λ  x  p2  cid:20 −→ k  [p1 x]p2 The rules of indirect proof give rise to the following transitions:  ccp x. k1  p1    p2  cid:20 −→ [p2 x]k1  [p2 x]p1 k1  ccr u. k2  p2    cid:20 −→ [k1 u]k2  [k1 u]p2  The ﬁrst of these deﬁnes the behavior of the refutation of φ that proceeds by contradicting the assumption that φ is true. Such a refutation is activated by presenting it with a proof of φ, which is then substituted for the assumption in the new state. Thus, “ccp” stands for “call with current proof.” The second transition deﬁnes the behavior of the proof of φ that proceeds by contradicting the assumption that φ is false. Such a proof is activated by presenting it with a refutation of φ, which is then substituted for the assumption in the new state. Thus, “ccr” stands for “call with current refutation.”  Rules  13.5g  to  13.5h  overlap in that there are two transitions for a state of the form  ccp x. k1  p1    ccr u. k2  p2  ,  one to the state [p x]k1  [p x]p1, where p is ccr u. k2  p2  , and one to the state [k u]k2  [k u]p2, where k is ccp x. k1  p1  . The dynamics of classical logic is   111  13.4 Law of the Excluded Middle  non-deterministic. To avoid this one may impose a priority ordering among the two cases, preferring one transition over the other when there is a choice. Preferring the ﬁrst corre- sponds to a “lazy” dynamics for proofs, because we pass the unevaluated proof p to the refutation on the left, which is thereby activated. Preferring the second corresponds to an “eager” dynamics for proofs, in which we pass the unevaluated refutation k to the proof, which is thereby activated.  All proofs in classical logic proceed by contradicting the assumption that it is false. In terms of the classical logic machine the initial and ﬁnal states of a computation are as follows:  haltφ  p initial  p canonical  haltφ  p ﬁnal   13.6a    13.6b   where p is a proof of φ, and haltφ is the assumed refutation of φ. The judgment p canonical states that p is a canonical proof, which holds of any proof other than an indirect proof. Execution consists of driving a general proof to a canonical proof, under the assumption that the theorem is false. Theorem 13.1  Preservation . If k ÷ φ, p : φ, and k  p  cid:20 −→ k such that k   cid:5 , then there exists φ   cid:5  and p   cid:5  ÷ φ   cid:5   p   cid:5  : φ   cid:5 .   cid:5   Proof By rule induction on the dynamics of classical logic. Theorem 13.2  Progress . If k ÷ φ and p : φ, then either k  p ﬁnal or k  p  cid:20 −→ k   cid:5   p   cid:5 .  Proof By rule induction on the statics of classical logic.  13.4 Law of the Excluded Middle  The law of the excluded middle is derivable in classical logic:  φ ∨ ¬φ false, φ true  cid:12  φ true  φ ∨ ¬φ false, φ true  cid:12  φ ∨ ¬φ true φ ∨ ¬φ false, φ true  cid:12  φ ∨ ¬φ false  φ ∨ ¬φ false, φ true  cid:12   φ ∨ ¬φ false  cid:12  φ false φ ∨ ¬φ false  cid:12  ¬φ true φ ∨ ¬φ false  cid:12  φ ∨ ¬φ true  φ ∨ ¬φ false  cid:12  φ ∨ ¬φ false  φ ∨ ¬φ false  cid:12   φ ∨ ¬φ true   Classical Logic  112  φ ∨ ¬φ:  When written out using explicit proofs and refutations, we obtain the proof term p0 :  ccr u. u  r · not ccp x. u  l · x     .  To understand the computational meaning of this proof, let us juxtapose it with a refutation k ÷ φ ∨ ¬φ and simplify it using the dynamics given in Section 13.3. The ﬁrst step is the transition  k  ccr u. u  r · not ccp x. u  l · x       k  r · not ccp x. k  l · x   ,  wherein we have replicated k so that it occurs in two places in the result state. By virtue of its type, the refutation k must have the form case k1; k2 , where k1 ÷ φ and k2 ÷ ¬φ. Continuing the reduction, we obtain:  case k1; k2  r · not ccp x. case k1; k2  l · x     k2  not ccp x. case k1; k2  l · x   .  By virtue of its type k2 must have the form not p2 , where p2 : φ, and hence the transition proceeds as follows:  not p2  not  ccp x. case k1; k2  l · x      cid:20 −→   cid:20 −→   cid:20 −→  ccp x. case k1; k2  l · x    p2. Observe that p2 is a valid proof of φ. Proceeding, we obtain ccp x. case k1; k2  l · x    p2  case k1; k2  l · p2   cid:20 −→   cid:20 −→ k1  p2  The ﬁrst of these two steps is the crux of the matter: the refutation, k = case k1; k2 , which was replicated at the outset of the derivation, is re-used, but with a different argument. At the ﬁrst use, the refutation k which is provided by the context of use of the law of the excluded middle, is presented with a proof r · p1 of φ ∨ ¬φ. That is, the proof behaves as though the right disjunct of the law is true, which is to say that φ is false. If the context is such that it inspects this proof, it can only be by providing the proof p2 of φ that refutes the claim that φ is false. Should this occur, the proof of the law of the excluded middle “backtracks” the context, providing instead the proof l · p2 to k, which then passes p2 to k1 without further incident. The proof of the law of the excluded middle boldly asserts   113  13.5 The Double-Negation Translation  ¬φ true, regardless of the form of φ. Then, if caught in its lie by the context providing a proof of φ, it “changes its mind” and asserts φ to the original context k after all. No further reversion is possible, because the context has itself provided a proof p2 of φ.  The law of the excluded middle illustrates that classical proofs are interactions between proofs and refutations, which is to say interactions between a proof and the context in which it is used. In programming terms, this corresponds to an abstract machine with an explicit control stack, or continuation, representing the context of evaluation of an expression. That expression may access the context  stack, continuation  to backtrack so as to maintain the perfect symmetry between truth and falsity. The penalty is that a closed proof of a disjunction no longer need show which disjunct it proves, for as we have just seen, it may, on further inspection, “change its mind.”  13.5 The Double-Negation Translation  One consequence of the greater expressiveness of constructive logic is that classical proofs may be translated systematically into constructive proofs of a classically equivalent propo- sition. Therefore, by systematically reorganizing the classical proof, we may, without changing its meaning from a classical perspective, turn it into a constructive proof of a con- structively weaker proposition. Consequently, there is no loss in adhering to constructive proofs, because every classical proof is a constructive proof of a constructively weaker, but classically equivalent, proposition. Moreover, it proves that classical logic is weaker  less expressive  than constructive logic, contrary to a na¨ıve interpretation which would say that the added reasoning principles, such as the law of the excluded middle, afforded by classical logic makes it stronger. In programming language terms adding a “feature” does not necessarily strengthen  improve the expressive power  of your language; on the contrary, it may weaken it. ∗ of propositions that interprets classical into constructive  We will deﬁne a translation φ  logic according to the following correspondences:  Classical  cid:8   cid:7   cid:12  φ true  cid:8   cid:7   cid:12  φ false  cid:8   cid:7   cid:12    ∗  Constructive ∗  cid:12  ¬¬φ ¬ cid:8  ∗  cid:12  ¬φ ¬ cid:8  ∗ ∗  cid:12  ⊥ true ¬ cid:8    cid:7   cid:7   cid:7   ∗ ∗ ∗  true  true  truth falsity contradiction  Classical truth is weakened to constructive irrefutability, but classical falsehood is construc- tive refutability, and classical contradiction is constructive falsehood. Falsity assumptions are negated after translation to express their falsehood; truth assumptions are merely trans- lated as is. Because the double negations are classically cancelable, the translation will be easily seen to yield a classically equivalent proposition. But because ¬¬φ is constructively weaker than φ, we also see that a proof in classical logic is translated to a constructive proof of a weaker statement.   114  Classical Logic  There are many choices for the translation; here is one that makes the proof of the  correspondence between classical and constructive logic especially simple:   cid:31 ∗ =  cid:31  ∗ = φ ∗ 1 ⊥∗ =⊥ ∗ = φ ∗ 1 ∗ = φ ∗ 1 ∗ = ¬φ   φ1 ∧ φ2    φ1 ∨ φ2   φ1 ⊃ φ2   ¬φ   ∧ φ  ∗ 2  ∗ 2  ∨ φ ⊃ ¬¬φ ∗  ∗ 2  13.6 Notes  One may show by induction on the rules of classical logic that the correspondences sum- marized above hold, using constructively valid entailments such as ¬¬φ true¬¬ψ true  cid:12  ¬¬ φ ∧ ψ  true.  The computational interpretation of classical logic was ﬁrst explored by Grifﬁn  1990  and Murthy  1991 . The present account is inﬂuenced by Wadler  2003 , transposed by Nanevski from sequent calculus to natural deduction using multiple forms of judgment. The terminology is inspired by Lakatos  1976 , an insightful and inspiring analysis of the discovery of proofs and refutations of conjectures in mathematics. Versions of the double-negation translation were originally given by G¨odel and Gentzen. The computational content of the double-negation translation was ﬁrst elucidated by Murthy  1991 , who established the important relationship with continuation passing.  Exercises  13.1. If the continuation type expresses negation, the types shown to be inhabited in Exercise 30.2, when interpreted under the proposition-as-types interpretation, look suspiciously like the following propositions:  a  φ ∨ ¬φ.  b   ¬¬φ  ⊃ φ.  c   ¬φ2 ⊃ ¬φ1  ⊃  φ1 ⊃ φ2 .  d  ¬ φ1 ∨ φ2  ⊃  ¬φ1 ∧ ¬φ2 . None of these propositions is true, in general, in constructive logic. Show that each of these propositions is true in classical logic by exhibiting a proof term for each.  The ﬁrst one is done for you in Section 13.4; you need only do the other three.  Compare the proof term you get for each with the inhabitant of the corresponding type that you gave in your solution to Exercise 30.2.   115  Exercises  13.2. Complete the proof of the double-negation interpretation sketched in Section 13.5, providing explicit proof terms for clarity. Because  φ ∨ ¬φ  ∗, the double-negation translation applied to the proof of LEM  for φ  given in Section 13.4 ∗  in constructive logic. How yields a proof of the double negation of LEM  for φ does the translated proof compare to the one you derived by hand in Exercise 12.1?  ∗ ∨ ¬φ  ∗ = φ    P A R T VI  Infinite Data Types    14  Generic Programming  14.1 Introduction  To get a ﬂavor of the concept, consider a function f of type ρ → ρ  Many programs are instances of a pattern in a particular situation. Sometimes types de- termine the pattern by a technique called  type  generic programming. For example, in Chapter 9, recursion over the natural numbers is introduced in an ad hoc way. As we shall see, the pattern of recursion on values of an inductive type is expressed as a generic program.  cid:5 , which transforms  cid:5 . For example, f might be the doubling function on values of type ρ into values of type ρ  t]τ natural numbers. We wish to extend f to a transformation from type [ρ t]τ to type [ρ by applying f to various spots in the input, where a value of type ρ occurs to obtain a value  cid:5 , leaving the rest of the data structure alone. For example, τ might be bool× t, in of type ρ which case f could be extended to a function of type bool × ρ → bool × ρ  cid:5  that sends the pairs  cid:24 a, b cid:25  to the pair  cid:24 a, f  b  cid:25 .   cid:5   The foregoing example glosses over an ambiguity arising from the many-one nature of substitution. A type can have the form [ρ t]τ in many different ways, according to how many occurrences of t there are within τ. Given f as above, it is not clear how to extend it  t]τ. To resolve the ambiguity, we must be given a template to a function from [ρ t]τ to [ρ that marks the occurrences of t in τ at which f is applied. Such a template is known as a type operator, t.τ, which is an abstractor binding a type variable t within a type τ. Given such an abstractor, we may unambiguously extend f to instances of τ given by substitution for t in τ.   cid:5   The power of generic programming depends on the type operators that are allowed. The simplest case is that of a polynomial type operator, one constructed from sum and product of types, including their nullary forms. These are extended to positive type operators, which also allow certain forms of function types.  14.2 Polynomial Type Operators  A type operator is a type equipped with a designated variable whose occurrences mark the spots in the type where a transformation is applied. A type operator is an abstractor t.τ such that t type  cid:12  τ type. An example of a type operator is the abstractor  t.unit +  bool × t    120  Generic Programming  in which occurrences of t mark the spots in which a transformation is applied. An instance of the type operator t.τ is obtained by substituting a type ρ for the variable t within the type τ. The polynomial type operators are those constructed from the type variable t the types void and unit, and the product and sum type constructors τ1 × τ2 and τ1 + τ2. More precisely, the judgment t.τ poly is inductively deﬁned by the following rules:  t.t poly  t.unit poly  t.τ1 poly  t.τ2 poly  t.τ1 × τ2 poly  t.void poly  t.τ1 poly  t.τ2 poly  t.τ1 + τ2 poly   14.1a    14.1b    14.1c    14.1d    14.1e   Exercise 14.1 asks for a proof that polynomial type operators are closed under substitution. Polynomial type operators are templates describing the structure of a data structure with slots for values of a particular type. For example, the type operator t.t ×  nat + t  speciﬁes all types ρ ×  nat + ρ  for any choice of type ρ. Thus a polynomial type operator designates points of interest in a data structure that have a common type. As we shall see shortly, this allows us to specify a program that applies a given function to all values lying at points of interest in a compound data structure to obtain a new one with the results of the applications at those points. Because substitution is not injective, one cannot recover the type operator from its instances. For example, if ρ were nat, then the instance would be nat ×  nat + nat ; it is impossible to know which occurrences of nat are in designated spots unless we are given the pattern by the type operator.  The generic extension of a polynomial type operator is a form of expression with the  following syntax  Exp e  ::= map{t.τ} x.e   cid:5   e  map{t.τ} x.e   cid:5   e   generic extension.  Its statics is given as follows:  t.τ poly  cid:7 , x : ρ  cid:12  e  cid:5  : ρ  cid:7   cid:12  map{t.τ} x.e  cid:5   e  : [ρ   cid:5    cid:7   cid:12  e : [ρ t]τ   cid:5    cid:5  speciﬁes a mapping from [ρ t]τ to [ρ   t]τ  cid:5  speciﬁes a mapping that sends x : ρ to e   14.2   cid:5 . The generic extension The abstractor x.e  t]τ. The latter mapping replaces of t.τ along x.e values v of type ρ occurring at spots corresponding to occurrences of t in τ by the  cid:5  at the same spot. The type operator t.τ is a template in transformed value [v x]e which certain spots, marked by occurrences of t, show where to apply the transformation x.e   cid:5  to a value of type [ρ t]τ to obtain a value of type [ρ   cid:5  of type ρ   cid:5  : ρ   t]τ.   cid:5    cid:5    121  14.2 Polynomial Type Operators  The following dynamics makes precise the concept of the generic extension of a poly-  nomial type operator.  map{t.t} x.e   cid:5   e   cid:20 −→ [e x]e   cid:5   map{t.unit} x.e   cid:5   e   cid:20 −→ e  map{t.τ1 × τ2} x.e   cid:5     e    cid:20 −→   cid:24 map{t.τ1} x.e   cid:5     e · l , map{t.τ2} x.e   cid:5     e · r  cid:25   map{t.void} x.e   cid:5   e   cid:20 −→ abort e    14.3a    14.3b    14.3c    14.3d    14.3e   map{t.τ1 + τ2} x.e   cid:5     e   case e {l · x1  cid:9 → l · map{t.τ1} x.e   cid:5    cid:20 −→   x1   r · x2  cid:9 → r · map{t.τ2} x.e   cid:5     x2 }   cid:5  to e itself, because the operator t.t speciﬁes Rule  14.3a  applies the transformation x.e that the transformation is performed directly. Rule  14.3b  states that the empty tuple is transformed to itself. Rule  14.3c  states that to transform e according to the operator t.τ1 × τ2, the ﬁrst component of e is transformed according to t.τ1 and the second component of e is transformed according to t.τ2. Rule  14.3d  states that the transformation of a value of type void aborts, because there are no such values. Rule  14.3e  states that to transform e according to t.τ1 + τ2, case analyze e and reconstruct it after transforming the injected value according to t.τ1 or t.τ2. Consider the type operator t.τ given by t.unit +  bool × t . Let x.e be the abstractor  x.s x , which increments a natural number. Using rules  14.3  we may derive that  map{t.τ} x.e  r ·  cid:24 true, n cid:25    cid:20 −→∗  r ·  cid:24 true, n + 1 cid:25 .  The natural number in the second component of the pair is incremented, because the type variable t occurs in that spot in the type operator t.τ. Theorem 14.1  Preservation . If map{t.τ} x.e  cid:5   e  : τ  cid:5  cid:5  : τ   cid:5  and map{t.τ} x.e   cid:5   e   cid:20 −→ e   cid:5  cid:5 , then   cid:5 .  e  Proof By inversion of rule  14.2 , we have 1. t type  cid:12  τ type; 2. x : ρ  cid:12  e   cid:5  for some ρ and ρ   cid:5  : ρ   cid:5 ;   122  Generic Programming  3. e : [ρ t]τ; 4. τ   cid:5  is [ρ   cid:5    t]τ.  The proof proceeds by cases on rules  14.3 . For example, consider rule  14.3c . It follows from inversion that map{t.τ1} x.e  t]τ1, and similarly that map{t.τ2} x.e   cid:5   e · l   t]τ2. It is easy to check that   cid:5   e · r  : [ρ  [ρ  :   cid:5    cid:5    cid:24 map{t.τ1} x.e  t] τ1 × τ2 , as required.   cid:5   has type [ρ   cid:5     e · l , map{t.τ2} x.e   cid:5     e · r  cid:25   14.3 Positive Type Operators  The positive type operators extend the polynomial type operators to admit restricted forms of function type. Speciﬁcally, t.τ1 → τ2 is a positive type operator, if  1  t does not occur in τ1, and  2  t.τ2 is a positive type operator. In general, any occurrences of a type variable t in the domain of a function type are negative occurrences, whereas any occurrences of t within the range of a function type, or within a product or sum type, are positive occurrences.1 A positive type operator is one for which only positive occurrences of the type variable t are allowed. Positive type operators, like polynomial type operators, are closed under substitution.  We deﬁne the judgment t.τ pos, which states that the abstractor t.τ is a positive type  operator by the following rules:  t.t pos  t.unit pos  t.τ1 pos  t.τ2 pos  t.τ1 × τ2 pos  t.void pos  t.τ1 pos  t.τ2 pos  t.τ1 + τ2 pos  τ1 type t.τ2 pos t.τ1 → τ2 pos   14.4a    14.4b    14.4c    14.4d    14.4e    14.4f   In rule  14.4f , the type variable t is excluded from the domain of the function type by demanding that it be well-formed without regard to t.   123  Exercises  The generic extension of a positive type operator is deﬁned similarly to that of a polyno-  mial type operator, with the following dynamics on function types:  map  +{t.τ1 → τ2} x.e   14.5  Because t is not allowed to occur within the domain type, the type of the result is τ1 →  t]τ2, assuming that e is of type τ1 → [ρ t]τ2. It is easy to verify preservation for the  cid:5  [ρ generic extension of a positive type operator.   cid:5   e   cid:20 −→ λ  x1 : τ1  map  +{t.τ2} x.e   cid:5   e x1    It is instructive to consider what goes wrong if we try to extend the generic extension to an arbitrary type operator, without any positivity restriction. Consider the type operator t.τ1 → τ2, without restriction on t, and suppose that x : ρ  cid:12  e  cid:5  : ρ  cid:5 . The generic extension map{t.τ1 → τ2} x.e  t]τ1 → [ρ  cid:5   t]τ2, given that e has type [ρ t]τ1 → [ρ t]τ2. The extension should yield a function of the form   cid:5   e  should have type [ρ   cid:5    cid:5   λ  x1 : [ρ   t]τ1  . . . e . . . x1     in which we apply e to a transformation ofx 1 and then transform the result. The trouble is that we are given, inductively, that map{t.τ1} x.e  cid:5   −  transforms values of type [ρ t]τ1  t]τ1, but we need to go the other way around to make x1 suitable as into values of type [ρ an argument for e.   cid:5   The generic extension of a type operator is an example of the concept of a functor in category theory  MacLane, 1998 . Generic programming is essentially functorial programming, exploiting the functorial action of polynomial type operators  Hinze and Jeuring, 2003 .  14.4 Notes  Exercises   cid:5    cid:5    cid:5    cid:5 ]τ  14.1. Prove that if t.τ poly and t 14.2. Show that the generic extension of a constant type operator is essentially the identity in that it sends each closed value to itself. More precisely, show that, for each value e of type τ, the expression  poly, then t.[τ t  poly.  .τ  map{ .τ} x.e  cid:5    e   cid:5 . For simplicity, assume an eager dy- evaluates to e, regardless of the choice of e namics for products and sums, and consider only polynomial type operators. What complications arise when extending this observation to positive type operators?  14.3. Consider Exercises 10.1 and 11.3 in which a database schema is represented by a ﬁnite product type indexed by the attributes of the schema, and a database with that schema is a ﬁnite sequence of instances of tuples of that type. Show that any database   124  Generic Programming  transformation that applies a function to one or more of the columns of each row of a database can be programmed in two steps using generic programming according to the following plan:  a  Specify a type operator whose type variable shows which columns are trans- formed. All speciﬁed columns must be of the same type in order for the transfor- mation to make sense.   b  Specify the transformation on the type of column that will be applied to each  tuple of the database to obtain an updated database.   c  Form the generic extension of the type operator with the given transformation,  and apply it to the given database.  For speciﬁcity, consider a schema whose attributes I include the attributes first and last, both of type str. Let c : str → str be a function that capitalizes strings according to some convention. Use generic programming to capitalize the first and last attributes of each row of a given database with the speciﬁed schema. 14.4. Whereas t occurs negatively in the type t → bool, and does not occur only positively  t → bool  → bool, we may say that t does occur non-negatively in the latter type. This example illustrates that occurrences of t in the domain of a function are negative, and that occurrences in the domain of the domain of a function are non-negative. Every positive occurrence counts as non-negative, but not every non-negative occurrence is positive.2 Give a simultaneous induction deﬁnition of the negative and non-negative type operators. Check that the type operator t. t → bool  → bool is non-negative according to your deﬁnition.  14.5. Using the deﬁnitions of negative and non-negative type operators requested in Exer- cise 14.4, give the deﬁnition of the generic extension of a non-negative type operator.  cid:5   by  cid:5   e  and map Speciﬁcally, deﬁne simultaneously map induction on the structure of τ with the statics give by these rules:  cid:7   cid:12  e : [ρ t]τ  −{t.τ} x.e  e  −−{t.τ} x.e   cid:5    cid:7   cid:12  map  t.τ non-neg  cid:7 , x : ρ  cid:12  e −−{t.τ} x.e  cid:5  : ρ −{t.τ} x.e  t.τ neg  cid:7 , x : ρ  cid:12  e   cid:7   cid:12  map   cid:5  : ρ  cid:5   e  : [ρ  t]τ  cid:7   cid:12  e : [ρ  cid:5   cid:5   e  : [ρ t]τ   cid:5    cid:5    t]τ   14.6a    14.6b   Note well the reversal of the types of e and the overall type in these two rules. Calculate the generic extension of the type operator t. t → bool  → bool.  Notes  1 The origin of this terminology is that a function type τ1 → τ2 is analogous to the implication φ1 ⊃ φ2, which is classically equivalent to ¬φ1 ∨ φ2, so that occurrences in the domain are under the negation.  2 Often what we have called “positive” is called “strictly positive,” and what we have called “non-  negative” is called “positive.”   15  Inductive and Coinductive Types  The inductive and the coinductive types are two important forms of recursive type. Inductive types correspond to least, or initial, solutions of certain type equations, and coin- ductive types correspond to their greatest, or ﬁnal, solutions. Intuitively, the elements of an inductive type are those that are given by a ﬁnite composition of its introduction forms. Consequently, if we specify the behavior of a function on each of the introduction forms of an inductive type, then its behavior is deﬁned for all values of that type. Such a function is a recursor, orcatamorphism . Dually, the elements of a coinductive type are those that behave properly in response to a ﬁnite composition of its elimination forms. Consequently, if we specify the behavior of an element on each elimination form, then we have fully speciﬁed a value of that type. Such an element is a generator, or anamorphism.  15.1 Motivating Examples  The most important example of an inductive type is the type of natural numbers as formalized in Chapter 9. The type nat is the least type containing z and closed under s − . The minimality condition is expressed by the existence of the iterator, iter e {z  cid:9 → e0  s x   cid:9 → e1}, which transforms a natural number into a value of type τ, given its value for zero, and a transformation from its value on a number to its value on the successor of that number. This operation is well-deﬁned precisely because there are no other natural numbers.  With a view towards deriving the type nat as a special case of an inductive type, it is useful to combine zero and successor into a single introduction form, and to correspondingly combine the basis and inductive step of the iterator. The following rules specify the statics of this reformulation:   cid:7   cid:12  e : unit + nat  cid:7   cid:12  foldnat e  : nat   cid:7 , x : unit + τ  cid:12  e1 : τ  cid:7   cid:12  e2 : nat   cid:7   cid:12  recnat x.e1; e2  : τ   15.1a    15.1b   The expression foldnat e  is the unique introduction form of the type nat. Using this, the expression z is foldnat l ·  cid:24  cid:25  , and s e  is foldnat r · e . The recursor, recnat x.e1; e2 , takes as argument the abstractor x.e1 that combines the basis and inductive step into a single computation that, given a value of type unit + τ, yields a value of type τ. Intuitively, if   126  Inductive and Coinductive Types  x is replaced by the value l ·  cid:24  cid:25 , then e1 computes the base case of the recursion, and if x is replaced by the value r · e, then e1 computes the inductive step from the result e of the recursive call.  The dynamics of the combined representation of natural numbers is given by the following  rules:  foldnat e  val  e2  cid:20 −→ e   cid:5  2  recnat x.e1; e2   cid:20 −→ recnat x.e1; e  cid:5  2   recnat x.e1; foldnat e2     cid:20 −→  [map{t.unit + t} y.recnat x.e1; y   e2  x]e1   15.2a    15.2b    15.2c   Rule  15.2c  uses  polynomial  generic extension  see Chapter 14  to apply the recursor to the predecessor, if any, of a natural number. If we expand the deﬁnition of the generic extension in place, we obtain this rule:  recnat x.e1; foldnat e2    cid:9 → l ·  cid:24  cid:25   r · y  cid:9 → r · recnat x.e1; y } x]e1   cid:20 −→  [case e2 {l ·  Exercise 15.2 asks for a derivation of the iterator, as deﬁned in Chapter 9, from the recursor just given.  An illustrative example of a coinductive type is the type of streams of natural numbers. A stream is an inﬁnite sequence of natural numbers such that an element of the stream can be computed only after computing all preceding elements in that stream. That is, the computations of successive elements of the stream are sequentially dependent in that the computation of one element inﬂuences the computation of the next. In this sense, the introduction form for streams is dual to the elimination form for natural numbers.  A stream is given by its behavior under the elimination forms for the stream type: hd e  returns the next, or head, element of the stream, and tl e  returns the tail of the stream, the stream resulting when the head element is removed. A stream is introduced by a generator, the dual of a recursor, that deﬁnes the head and the tail of the stream in terms of the current state of the stream, which is represented by a value of some type. The statics of streams is given by the following rules:   cid:7   cid:12  e : stream  cid:7   cid:12  hd e  : nat  cid:7   cid:12  e : stream  cid:7   cid:12  tl e  : stream   15.3a    15.3b    127  15.1 Motivating Examples   cid:7   cid:12  e : τ  cid:7 , x : τ  cid:12  e1 : nat  cid:7 , x : τ  cid:12  e2 : τ   cid:7   cid:12  strgen x is e in   : stream   15.3c   In rule  15.3c , the current state of the stream is given by the expression e of some type τ, and the head and tail of the stream are determined by the expressions e1 and e2, respectively, as a function of the current state.  The notation for the generator is chosen to emphasize that every stream has both a head and a tail.   The dynamics of streams is given by the following rules:  strgen x is e in   val  hd strgen x is e in     cid:20 −→ [e x]e1  e  cid:20 −→ e   cid:5   hd e   cid:20 −→ hd e   cid:5    e  cid:20 −→ e   cid:5   tl e   cid:20 −→ tl e   cid:5    tl strgen x is e in      cid:20 −→  strgen x is [e x]e2 in    Rules  15.4c  and  15.4e  express the dependency of the head and tail of the stream on its current state. Observe that the tail is obtained by applying the generator to the new state determined by e2 from the current state.  To derive streams as a special case of a coinductive type, we combine the head and the tail into a single elimination form, and reorganize the generator correspondingly. Thus, we consider the following statics:   cid:7   cid:12  e : stream   cid:7   cid:12  unfoldstream e  : nat × stream  cid:7 , x : τ  cid:12  e1 : nat × τ  cid:7   cid:12  e2 : τ  cid:7   cid:12  genstream x.e1; e2  : stream  Rule  15.5a  states that a stream may be unfolded into a pair consisting of its head, a natural number, and its tail, another stream. The head hd e  and tail tl e  of a stream e are the projections unfoldstream e  · l and unfoldstream e  · r, respectively. Rule  15.5b  states that a stream is generated from the state element e2 by an expression e1 that yields the head element and the next state as a function of the current state. The dynamics of streams is given by the following rules:  genstream x.e1; e2  val   15.4a    15.4b    15.4c    15.4d    15.4e    15.5a    15.5b    15.6a    128  Inductive and Coinductive Types  e  cid:20 −→ e   cid:5   unfoldstream e   cid:20 −→ unfoldstream e   cid:5    unfoldstream genstream x.e1; e2     cid:20 −→  map{t.nat × t} y.genstream x.e1; y   [e2 x]e1    15.6b    15.6c   Rule  15.6c  uses generic extension to generate a new stream whose state is the second com- ponent of [e2 x]e1. Expanding the generic extension we obtain the following reformulation of this rule:  unfoldstream genstream x.e1; e2     cid:20 −→   cid:24  [e2 x]e1  · l, genstream x.e1;  [e2 x]e1  · r  cid:25   Exercise 15.3 asks for a derivation of strgen x is e in   from the coinductive generation form.  15.2 Statics  15.2.1 Types  We may now give a general account of inductive and coinductive types, which are deﬁned in terms of positive type operators. We will consider a variant of T, which we will call M, with natural numbers replaced by functions, products, sums, and a rich class of inductive and coinductive types.  The syntax of inductive and coinductive types involves type variables, which are, of course, variables ranging over types. The abstract syntax of inductive and coinductive types is given by the following grammar:  Typ τ  ::= t  t  ind t.τ  μ t.τ  coi t.τ  ν t.τ   self-reference inductive coinductive  Type formation judgments have the form  t1 type, . . . , tn type  cid:12  τ type,  where t1, . . . , tn are type names. We let  cid:8  range over ﬁnite sets of hypotheses of the form t type, where t is a type name. The type formation judgment is inductively deﬁned by the   129  15.2 Statics  following rules:   cid:8 , t type  cid:12  t type  cid:8   cid:12  unit type   cid:8   cid:12  τ1 type  cid:8   cid:12  τ2 type  cid:8   cid:12  prod τ1; τ2  type   cid:8   cid:12  void type   cid:8   cid:12  τ1 type  cid:8   cid:12  τ2 type   cid:8   cid:12  sum τ1; τ2  type   cid:8   cid:12  τ1 type  cid:8   cid:12  τ2 type   cid:8   cid:12  arr τ1; τ2  type   cid:8 , t type  cid:12  τ type  cid:8   cid:12  t.τ pos   cid:8   cid:12  ind t.τ  type   cid:8 , t type  cid:12  τ type  cid:8   cid:12  t.τ pos   cid:8   cid:12  coi t.τ  type  15.2.2 Expressions  The abstract syntax of M is given by the following grammar:  Exp e  ::= fold{t.τ} e   foldt.τ  e   rec{t.τ} x.e1; e2  rec x.e1; e2  unfold{t.τ} e  unfoldt.τ  e  gen{t.τ} x.e1; e2  gen x.e1; e2   constructor recursor destructor generator  The subscripts on the concrete syntax forms are often omitted when they are clear from context.  The statics for M is given by the following typing rules:   cid:7   cid:12  e : [ind t.τ  t]τ   cid:7   cid:12  fold{t.τ} e  : ind t.τ   t]τ  cid:12  e1 : τ  cid:5   cid:7   cid:12  rec{t.τ} x.e1; e2  :τ   cid:5    cid:5    cid:7   cid:12  e2 : ind t.τ    cid:7 , x : [τ   cid:7   cid:12  e : coi t.τ    cid:7   cid:12  unfold{t.τ} e  : [coi t.τ  t]τ  cid:7   cid:12  e2 : τ2  cid:7 , x : τ2  cid:12  e1 : [τ2 t]τ  cid:7   cid:12  gen{t.τ} x.e1; e2  : coi t.τ    15.7a    15.7b    15.7c    15.7d    15.7e    15.7f    15.7g    15.7h    15.8a    15.8b    15.8c    15.8d    130  Inductive and Coinductive Types  15.3 Dynamics  The dynamics of M is given in terms of the positive generic extension operation described in Chapter 14. The following rules specify a lazy dynamics for M:  fold{t.τ} e  val  e2  cid:20 −→ e   cid:5  2  rec{t.τ} x.e1; e2   cid:20 −→ rec{t.τ} x.e1; e  cid:5  2   rec{t.τ} x.e1; fold{t.τ} e2     cid:20 −→  +{t.τ} y.rec{t.τ} x.e1; y   e2  x]e1  [map  gen{t.τ} x.e1; e2  val  e  cid:20 −→ e   cid:5   unfold{t.τ} e   cid:20 −→ unfold{t.τ} e   cid:5    unfold{t.τ} gen{t.τ} x.e1; e2     cid:20 −→  +{t.τ} y.gen{t.τ} x.e1; y   [e2 x]e1   map   15.9a    15.9b    15.9c    15.9d    15.9e    15.9f   Rule  15.9c  states that to evaluate the recursor on a value of recursive type, we inductively apply the recursor as guided by the type operator to the value, and then apply the inductive step to the result. Rule  15.9f  is simply the dual of this rule for coinductive types. Lemma 15.1. If e : τ and e  cid:20 −→ e   cid:5 , then e   cid:5  : τ .  Proof By rule induction on rules  15.9 .  Lemma 15.2. If e : τ , then either e val or there exists e   cid:5  such that e  cid:20 −→ e   cid:5 .  Proof By rule induction on rules  15.8 .  Although a proof of this fact lies beyond our current reach, all programs in M terminate.  cid:5 .  Theorem 15.3  Termination for M . If e : τ , then there exists e  val such that e  cid:20 −→∗  e   cid:5    131  15.4 Solving Type Equations  It may, at ﬁrst, seem surprising that a language with inﬁnite data structures, such as streams, can enjoy such a termination property. But bear in mind that inﬁnite data structures, such as streams, are represented as in a continuing state of creation, and not as a completed whole.  15.4 Solving Type Equations  For a positive type operator t.τ, we may say that the inductive type μ t.τ  and the coinduc- tive type ν t.τ  are both solutions  up to isomorphism  of the type equation t  ∼= τ:  μ t.τ  ∼= [μ t.τ  t]τ ν t.τ  ∼= [ν t.τ   t]τ.  Intuitively speaking, this means that every value of an inductive type is the folding of a value of the unfolding of the inductive type, and that, similarly, every value of the unfolding of a coinductive type is the unfolding of a value of the coinductive type itself. It is a good exercise to deﬁne functions back and forth between the isomorphic types and to convince yourself informally that they are mutually inverse to one another. Whereas both are solutions to the same type equation, they are not isomorphic to each other. To see why, consider the inductive type nat  cid:2  μ t.unit + t  and the coinductive type conat  cid:2  ν t.unit + t . Informally, nat is the smallest  most restrictive  type containing zero, given by fold l ·  cid:24  cid:25  , and closed under formation of the successor of any other e of type nat, given by fold r · e . Dually, conat is the largest  most permissive  type of expressions e for which the unfolding, unfold. e , is either zero, given by l ·  cid:24  cid:25 , or to the successor of some other e   cid:5  of type conat, given by r · e   cid:5 .  Because nat is deﬁned by the composition of its introduction forms and sum injections, it is clear that only ﬁnite natural numbers can be constructed in ﬁnite time. Because conat is deﬁned by the composition of its elimination forms  unfoldings plus case analyses , it is clear that a co-natural number can only be explored to ﬁnite depth in ﬁnite time—essentially we can only examine some ﬁnite number of predecessors of a given co-natural number in a terminating program. Consequently, 1. there is a function i : nat → conat embedding every ﬁnite natural number into the  type of possibly inﬁnite natural numbers; and  2. there is an “actually inﬁnite” co-natural number ω that is essentially an inﬁnite compo-  sition of successors.  Deﬁning the embedding of nat into conat is the subject of Exercise 15.1. The inﬁnite co-natural number ω is deﬁned as follows:  ω  cid:2  gen x.r · x; cid:24  cid:25  .   132  Inductive and Coinductive Types  r · ω, which means that ω is its own predecessor. One may check that unfold. ω   cid:20 −→∗ The co-natural number ω is larger than any ﬁnite natural number in that any ﬁnite number of predecessors of ω is non-zero.  Summing up, the mere fact of being a solution to a type equation does not uniquely characterize a type: there can be many different solutions to the same type equation, the natural and the co-natural numbers being good examples of the discrepancy. However, we will show in Part VIII that type equations have unique solutions  up to isomorphism  and that the restriction to polynomial type operators is no longer required. The price we pay for the additional expressive power is that programs are no longer guaranteed to terminate.  The language M is named after Mendler, on whose work the present treatment is based  Mendler, 1987 . Mendler’s work is grounded in category theory, speciﬁcally the concept of an algebra for a functor  MacLane, 1998; Taylor, 1999 . The functorial action of a type constructor  described in Chapter 14  plays a central role. Inductive types are initial algebras and coinductive types are ﬁnal coalgebras for the functor given by a  polynomial or positive  type operator.  15.5 Notes  Exercises  l ·  cid:24  cid:25 .  15.1. Deﬁne a function i : nat → conat that sends every natural number to “itself” in the sense that every ﬁnite natural number is sent to its correlate as a co-natural number.  a  unfold. i z    cid:20 −→∗  b  unfold. i s n     cid:20 −→∗  recursor for the inductive type of natural numbers given in Section 15.1.  15.2. Derive the iterator, iter e {z  cid:9 → e0  s x   cid:9 → e1}, described in Chapter 9 from the 15.3. Derive the stream generator, strgen x is e in   from the gen- 15.4. Consider the type seq  cid:2  nat → nat of inﬁnite sequences of natural numbers. Every  erator for the coinductive stream type given in Section 15.1.  r · i n .  stream can be turned into a sequence by the following function:  λ  stream : s  λ  n : nat  hd iter n{z  cid:9 → s  s x   cid:9 → tl x } .  Show that every sequence can be turned into a stream whose nth element is the nth element of the given sequence.  15.5. The type of lists of natural numbers is deﬁned by the following introduction and  elimination forms:   cid:7   cid:12  nil : natlist   15.10a    133  Note   cid:7   cid:12  e1 : nat  cid:7   cid:12  e2 : natlist   cid:7   cid:12  cons e1; e2  :natlist   cid:7   cid:12  e : natlist  cid:7   cid:12  e0 : τ  cid:7  x : nat y : τ  cid:12  e1 : τ   cid:7   cid:12  rec e {nil  cid:9 → e0  cons x; y   cid:9 → e1} : τ  The associated dynamics, whether eager or lazy, can be derived from that of the recursor for the type nat given in Chapter 9. Give a deﬁnition of natlist as an inductive type, including the deﬁnitions of its associated introduction and elimination forms. Check that they validate the expected dynamics.  15.6. Consider the type itree of possibly inﬁnite binary trees with the following intro-  duction and elimination forms:   cid:7   cid:12  e : itree   cid:7   cid:12  view e  :  itree × itree  opt  cid:7   cid:12  e : τ  cid:7  x : τ  cid:12  e  cid:5  :  τ × τ  opt  cid:7   cid:12  itgen x is e in e  cid:5  : itree   15.10b    15.10c    15.11a    15.11b   Because a possibly inﬁnite tree must be in a state of continual generation, viewing a tree exposes only its top-level structure, an optional pair of possibly inﬁnite trees.1 If the view is null, the tree is empty, and if it is just e1 e2, then it is non-empty, with children given by e1 and e2. To generate an inﬁnite tree, choose a type τ of its state  cid:5  that, when of generation, and provide its current state e and a state transformation e applied to the current state, announces whether or not generation is complete, and, if not, provides the state for each of the children.  a  Give a precise dynamics for the itree operations as just described informally.  Hint: use generic programming!   b  Reformulate the type itree as a coinductive type, and derive the statics and  dynamics of its introduction and elimination forms.  15.7. Exercise 11.5 asked you to deﬁne an RS latch as a signal transducer, in which signals are expressed explicitly as functions of time. Here you are asked again to deﬁne an RS latch as a signal transducer, but this time with signals expressed as streams of booleans. Under such a representation, time is implicitly represented by the successive elements of the stream. Deﬁne an RS latch as a transducer of signals consisting of pairs of booleans.  Note  1 See Chapter 11 for the deﬁnition of option types.    P A R T VII  Variable Types    16  System F of Polymorphic Types  The languages we have considered so far are all monomorphic in that every expression has a unique type, given the types of its free variables, if it has a type at all. Yet it is often the case that essentially the same behavior is required, albeit at several different types. For example, in T there is a distinct identity function for each type τ, namely λ  x : τ  x, even though the behavior is the same for each choice of τ. Similarly, there is a distinct composition operator for each triple of types, namely  ◦τ1,τ2,τ3  = λ  f : τ2 → τ3  λ  g : τ1 → τ2  λ  x : τ1  f  g x  .  Each choice of the three types requires a different program, even though they all have the same behavior when executed.  Obviously, it would be useful to capture the pattern once and for all, and to instantiate this pattern each time we need it. The expression patterns codify generic  type-independent  behaviors that are shared by all instances of the pattern. Such generic expressions are polymorphic. In this chapter, we will study the language F, which was introduced by Girard under the name System F and by Reynolds under the name polymorphic typed λ-calculus. Although motivated by a simple practical problem  how to avoid writing redundant code , the concept of polymorphism is central to an impressive variety of seemingly disparate concepts, including the concept of data abstraction  the subject of Chapter 17 , and the deﬁnability of product, sum, inductive, and coinductive types considered in the preceding chapters.  Only general recursive types extend the expressive power of the language.   16.1 Polymorphic Abstraction  The language F is a variant of T in which we eliminate the type of natural numbers, but add, in compensation, polymorphic types:1  Typ τ  ::= t  Exp e  ::= x  t  τ1 → τ2 ∀ t.τ   arr τ1; τ2  all t.τ  lam{τ} x.e  λ  x : τ  e ap e1; e2  Lam t.e  App{τ} e   e1 e2   cid:16  t  e e[τ]  x  variable function polymorphic  abstraction application type abstraction type application   138  System F of Polymorphic Types  A type abstraction Lam t.e  deﬁnes a generic, or polymorphic, function with type variable t standing for an unspeciﬁed type within e. A type application, or instantiation App{τ} e , applies a polymorphic function to a speciﬁed type, which is plugged in for the type variable to obtain the result. The universal type, all t.τ , classiﬁes polymorphic functions.  The statics of F consists of two judgment forms, the type formation judgment,  and the typing judgment,   cid:8   cid:12  τ type,   cid:8   cid:7   cid:12  e : τ.  The hypotheses  cid:8  have the form t type, where t is a variable of sort Typ, and the hypotheses  cid:7  have the form x : τ, where x is a variable of sort Exp.  The rules deﬁning the type formation judgment are as follows:   cid:8 , t type  cid:12  t type   cid:8   cid:12  τ1 type  cid:8   cid:12  τ2 type   cid:8   cid:12  arr τ1; τ2  type  cid:8 , t type  cid:12  τ type  cid:8   cid:12  all t.τ  type The rules deﬁning the typing judgment are as follows:   cid:8   cid:7 , x : τ  cid:12  x : τ   cid:8   cid:12  τ1 type  cid:8   cid:7 , x : τ1  cid:12  e : τ2  cid:8   cid:7   cid:12  lam{τ1} x.e  :arr  τ1; τ2   cid:8   cid:7   cid:12  e1 : arr τ2; τ   cid:8   cid:7   cid:12  e2 : τ2   cid:8   cid:7   cid:12  ap e1; e2  :τ  cid:8 , t type  cid:7   cid:12  e : τ   cid:8   cid:7   cid:12  Lam t.e  :all t.τ     cid:8   cid:7   cid:12  e : all t.τ   cid:5    cid:8   cid:12  τ type   16.2e  Lemma 16.1  Regularity . If  cid:8   cid:7   cid:12  e : τ , and if  cid:8   cid:12  τi type for each assumption xi : τi in  cid:7 , then  cid:8   cid:12  τ type.   cid:8   cid:7   cid:12  App{τ} e  : [τ t]τ   cid:5   Proof By induction on rules  16.2 .  The statics admits the structural rules for a general hypothetical judgment. In particular, we have the following critical substitution property for type formation and expression typing. Lemma 16.2  Substitution . 1. If  cid:8 , t type  cid:12  τ  type and  cid:8   cid:12  τ type, then  cid:8   cid:12    cid:5    cid:5   [τ t]τ  type.   16.1a    16.1b    16.1c    16.2a    16.2b    16.2c    16.2d    139  16.1 Polymorphic Abstraction  2. If  cid:8 , t type  cid:7   cid:12  e  cid:5  : τ 3. If  cid:8   cid:7 , x : τ  cid:12  e  cid:5  : τ   cid:5  and  cid:8   cid:12  τ type, then  cid:8  [τ t] cid:7   cid:12  [τ t]e  cid:5  and  cid:8   cid:7   cid:12  e : τ , then  cid:8   cid:7   cid:12  [e x]e  cid:5  : τ  cid:5 .   cid:5  : [τ t]τ   cid:5 .  The second part of the lemma requires substitution into the context  cid:7  as well as into the  term and its type, because the type variable t may occur freely in any of these positions.  Returning to the motivating examples from the introduction, the polymorphic identity  function, I, is written  it has the polymorphic type   cid:16  t  λ  x : t  x;  ∀ t.t → t .  Instances of the polymorphic identity are written I[τ], where τ is some type, and have the type τ → τ.  Similarly, the polymorphic composition function, C, is written   cid:16  t1   cid:16  t2   cid:16  t3  λ  f : t2 → t3  λ  g : t1 → t2  λ  x : t1  f  g x  .  The function C has the polymorphic type  ∀ t1.∀ t2.∀ t3. t2 → t3  →  t1 → t2  →  t1 → t3    .  Instances of C are obtained by applying it to a triple of types, written C[τ1][τ2][τ3]. Each such instance has the type   τ2 → τ3  →  τ1 → τ2  →  τ1 → τ3 .  The dynamics of F is given as follows:  Dynamics  lam{τ} x.e  val  Lam t.e  val  [e2 val]   cid:5  1  e1  cid:20 −→ e  ap lam{τ1} x.e ; e2   cid:20 −→ [e2 x]e  cid:3   cid:4  ap e1; e2   cid:20 −→ ap e  cid:5  1; e2  e2  cid:20 −→ e  cid:5  2 ap e1; e2   cid:20 −→ ap e1; e  cid:5  2   e1 val   16.3a    16.3b    16.3c    16.3d    16.3e    140  System F of Polymorphic Types  App{τ} Lam t.e    cid:20 −→ [τ t]e  e  cid:20 −→ e   cid:5   App{τ} e   cid:20 −→ App{τ} e   cid:5     16.3f    16.3g   The bracketed premises and rule are included for a call-by-value interpretation and omitted for a call-by-name interpretation of F.  It is a simple matter to prove safety for F, using familiar methods.  Lemma 16.3  Canonical Forms . Suppose that e : τ and e val, then 1. If τ = arr τ1; τ2 , then e = lam{τ1} x.e2  with x : τ1  cid:12  e2 : τ2. 2. If τ = all t.τ   cid:5  , then e = Lam t.e   cid:5   with t type  cid:12  e   cid:5  : τ   cid:5 .  Proof By rule induction on the statics.  Theorem 16.4  Preservation . If e : τ and e  cid:20 −→ e   cid:5 , then e   cid:5  : τ .  Proof By rule induction on the dynamics.  Theorem 16.5  Progress . If e : τ , then either e val or there exists e   cid:5  such that e  cid:20 −→ e   cid:5 .  Proof By rule induction on the statics.  16.2 Polymorphic Deﬁnability  The language F is astonishingly expressive. Not only are all  lazy  ﬁnite products and sums deﬁnable in the language, but so are all  lazy  inductive and coinductive types. Their deﬁnability is most naturally expressed using deﬁnitional equality, which is the least congruence containing the following two axioms:   cid:8   cid:7 , x : τ1  cid:12  e2 : τ2  cid:8   cid:7   cid:12  e1 : τ1  cid:8   cid:7   cid:12   λ  x : τ  e2  e1  ≡ [e1 x]e2 : τ2   cid:8 , t type  cid:7   cid:12  e : τ  cid:8   cid:12  ρ type  cid:8   cid:7   cid:12   cid:16  t  e[ρ] ≡ [ρ t]e : [ρ t]τ   16.4a    16.4b   In addition, there are rules omitted here specifying that deﬁnitional equality is a congruence relation  that is, an equivalence relation respected by all expression-forming operations .   141  16.2 Polymorphic Deﬁnability  The nullary product, or unit, type is deﬁnable in F as follows:  16.2.1 Products and Sums  unit  cid:2  ∀ r.r → r    cid:24  cid:25   cid:2   cid:16  r  λ  x : r  x  The identity function plays the role of the null tuple, because it is the only closed value of this type.  Binary products are deﬁnable in F by using encoding tricks similar to those described in  Chapter 21 for the untyped λ-calculus:  τ1 × τ2  cid:2  ∀ r. τ1 → τ2 → r  → r   cid:24 e1, e2 cid:25   cid:2   cid:16  r  λ  x : τ1 → τ2 → r  x e1  e2   e · l  cid:2  e[τ1] λ  x : τ1  λ  y : τ2  x  e · r  cid:2  e[τ2] λ  x : τ1  λ  y : τ2  y   The statics given in Chapter 10 is derivable according to these deﬁnitions. Moreover, the following deﬁnitional equalities are derivable in F from these deﬁnitions:  The nullary sum, or void, type is deﬁnable in F:   cid:24 e1, e2 cid:25  · l ≡ e1 : τ1   cid:24 e1, e2 cid:25  ·r ≡ e2 : τ2.  void  cid:2  ∀ r.r  abort{ρ} e   cid:2  e[ρ]  Binary sums are also deﬁnable in F:  τ1 + τ2  cid:2  ∀ r. τ1 → r  →  τ2 → r  → r  l · e  cid:2   cid:16  r  λ  x : τ1 → r  λ  y : τ2 → r  x e  r · e  cid:2   cid:16  r  λ  x : τ1 → r  λ  y : τ2 → r  y e   case e {l · x1  cid:9 → e1  r · x2  cid:9 → e2}  cid:2  e[ρ] λ  x1 : τ1  e1  λ  x2 : τ2  e2   and  and  provided that the types make sense. It is easy to check that the following equivalences are derivable in F:  case l · d1 {l · x1  cid:9 → e1  r · x2  cid:9 → e2} ≡ [d1 x1]e1 : ρ  case r · d2 {l · x1  cid:9 → e1  r · x2  cid:9 → e2} ≡[d 2 x2]e2 : ρ.  Thus, the dynamic behavior speciﬁed in Chapter 11 is correctly implemented by these deﬁnitions.   142  System F of Polymorphic Types  As we remarked above, the natural numbers  under a lazy interpretation  are also deﬁnable in F. The key is the iterator, whose typing rule we recall here for reference:  16.2.2 Natural Numbers  e0 : nat e1 : τ  iter{e1; x.e2} e0  :τ  x : τ  cid:12  e2 : τ  .  nat → ∀ t.t →  t → t  → t .  Because the result type τ is arbitrary, this means that if we have an iterator, then we can use it to deﬁne a function of type  This function, when applied to an argument n, yields a polymorphic function that, for any result type, t, given the initial result for z and a transformation from the result for x into the result for s x , yields the result of iterating the transformation n times, starting with the initial result.  Because the only operation we can perform on a natural number is to iterate up to it, we may simply identify a natural number, n, with the polymorphic iterate-up-to-n function just described. Thus, we may deﬁne the type of natural numbers in F by the following equations:  nat  cid:2  ∀ t.t →  t → t  → t   z  cid:2   cid:16  t  λ  z : t  λ  s : t → t  z  s e   cid:2   cid:16  t  λ  z : t  λ  s : t → t  s e[t] z  s    iter{e1; x.e2} e0   cid:2  e0[τ] e1  λ  x : τ  e2   It is easy to check that the statics and dynamics of the natural numbers type given in Chapter 9 are derivable in F under these deﬁnitions. The representations of the numerals in F are called the polymorphic Church numerals.  The encodability of the natural numbers shows that F is at least as expressive as T. But is it more expressive? Yes! It is possible to show that the evaluation function for T is deﬁnable in F, even though it is not deﬁnable in T itself. However, the same diagonal argument given in Chapter 9 applies here, showing that the evaluation function for F is not deﬁnable in F. We may enrich F a bit more to deﬁne the evaluator for F, but as long as all programs in the enriched language terminate, we will once again have an undeﬁnable function, the evaluation function for that extension.  16.3 Parametricity Overview  A remarkable property of F is that polymorphic types severely constrain the behavior of their elements. We may prove useful theorems about an expression knowing only its type—that is, without ever looking at the code. For example, if i is any expression of type ∀ t.t → t , then it is the identity function. Informally, when i is applied to a type, τ, and   143  16.3 Parametricity Overview  an argument of type τ, it returns a value of type τ. But because τ is not speciﬁed until i is called, the function has no choice but to return its argument, which is to say that it is essentially the identity function. Similarly, if b is any expression of type ∀ t.t → t → t , then b is equivalent to either  cid:16  t  λ  x : t  λ  y : t  x or  cid:16  t  λ  x : t  λ  y : t  y. Intuitively, when b is applied to two arguments of a given type, the only value it can return is one of the givens.  Properties of a program in F that can be proved knowing only its type are called para- metricity properties. The facts about the functions i and b stated above are examples of parametricity properties. Such properties are sometimes called “free theorems,” because they come from typing “for free,” without any knowledge of the code itself. It bears repeat- ing that in F we prove non-trivial behavioral properties of programs without ever examining the program text. The key to this incredible fact is that we are able to prove a deep property, called parametricity, about the language F, that then applies to every program written in F. One may say that the type system “pre-veriﬁes” programs with respect to a broad range of useful properties, eliminating the need to prove those properties about every program sep- arately. The parametricity theorem for F explains the remarkable experience that if a piece of code type checks, then it “just works.” Parametricity narrows the space of well-typed programs sufﬁciently that the opportunities for programmer error are reduced to almost nothing.  So how does the parametricity theorem work? Without getting into too many technical details  but see Chapter 48 for a full treatment , we can give a brief summary of the main idea. Any function i : ∀ t.t → t  inF enjoys the following property:  For any type τ and any property P of the type τ, then ifP holds of x : τ, then P holds of i[τ] x .  To show that for any type τ, and any x of type τ, the expression i[τ] x  is equivalent to x, it sufﬁces to ﬁx x0 : τ, and consider the property Px0 that holds of y : τ iff y is equivalent to x0. Obviously, P holds of x0 itself, and hence by the above-displayed property of i, it sends any argument satisfying Px0 to a result satisfying Px0, which is to say that it sends x0 to x0. Because x0 is an arbitrary element of τ, it follows thati [τ] is the identity function, λ  x : τ  x, on the type τ, and because τ is itself arbitrary, i is the polymorphic identity function,  cid:16  t  λ  x : t  x.  A similar argument sufﬁces to show that the function b, deﬁned above, is either  cid:16  t  λ  x : t  λ  y : t  x or  cid:16  t  λ  x : t  λ  y : t  y. By virtue of its type, the function b enjoys the parametricity property  For any type τ and any property P of τ, ifP holds of x : τ and of y : τ, then P holds of b[τ] x  y .  Choose an arbitrary type τ and two arbitrary elements x0 and y0 of type τ. Deﬁne Qx0,y0 to hold of z : τ iff either z is equivalent to x0 or z is equivalent to y0. Clearly Qx0,y0 holds of both x0 and y0 themselves, so by the quoted parametricity property of b, it follows that Qx0,y0 holds of b[τ] x0  y0 , which is to say that it is equivalent to either x0 or y0. Since τ, x0, and y0 are arbitrary, it follows that b is equivalent to either  cid:16  t  λ  x : t  λ  y : t  x or  cid:16  t  λ  x : t  λ  y : t  y.   144  System F of Polymorphic Types  The parametricity theorem for F implies even stronger properties of functions such as i and b considered above. For example, the function i of type ∀ t.t → t  also satisﬁes the following condition:  If τ and τ x : τ and x   cid:5  are any two types, and R is a binary relation between τ and τ  cid:5  : τ   cid:5 , then R relates i[τ] x  to i[τ   cid:5 , ifR relates x to x   cid:5 ] x   cid:5  .   cid:5 , then for any   cid:5  iff x  Using this property, we may again prove that i is equivalent to the polymorphic identity function. Speciﬁcally, if τ is any type and g : τ → τ is any function on that type, then it follows from the type of i alone that i[τ] g x   is equivalent to g i[τ] x   for any x : τ. To prove this, simply choose R to the be graph of the function g, the relation Rg that holds of  cid:5  is equivalent to g x . The parametricity property of i, when specialized to x and x Rg, states that if x  cid:5   is equivalent to g i[τ] x  , which is to say that i[τ] g x   is equivalent to g i[τ] x  . To show that i is equivalent to the identity function, choose x0 : τ arbitrarily, and consider the constant function g0 on τ that always returns x0. Because x0 is equivalent to g0 x0 , it follows that i[τ] x0  is equivalent to x0, which is to say that i behaves like the polymorphic identity function.   cid:5  is equivalent to g x , then i[τ] x  System F was introduced by Girard  1972  in the context of proof theory and by Reynolds  1974  in the context of programming languages. The concept of parametricity was origi- nally isolated by Strachey but was not fully developed until the work of Reynolds  1983 . The phrase “free theorems” for parametricity theorems was introduced by Wadler  1989 .  16.4 Notes  Exercises  16.1. Give polymorphic deﬁnitions and types to the s and k combinators deﬁned in Exer-  cise 3.1.  16.2. Deﬁne in F the type bool of Church booleans. Deﬁne the type bool, and deﬁne true and false of this type, and the conditional if e then e0 else e1, where e is of this type.  16.3. Deﬁne in F the inductive type of lists of natural numbers as deﬁned in Chapter 15. Hint: Deﬁne the representation in terms of the recursor  elimination form  for lists, following the pattern for deﬁning the type of natural numbers.  16.4. Deﬁne in F an arbitrary inductive type, μ t.τ . Hint: generalize your answer to  Exercise 16.3.  16.5. Deﬁne the type t list as in Exercise 16.3, with the element type, t, unspeciﬁed. Deﬁne the ﬁnite set of elements of a list l to be those x given by the head of some number of tails of l. Now suppose that f : ∀ t.t list → t list  is an arbitrary   145  Note  function of the stated type. Show that the elements of f [τ] l  are a subset of those of l. Thus, f may only permute, replicate, or drop elements from its input list to obtain its output list.  Note  1 Girard’s original version of System F included the natural numbers as a basic type.   17  Abstract Types  Data abstraction is perhaps the most important technique for structuring programs. The main idea is to introduce an interface that serves as a contract between the client and the implementor of an abstract type. The interface speciﬁes what the client may rely on for its own work, and, simultaneously, what the implementor must provide to satisfy the contract. The interface serves to isolate the client from the implementor so that each may be developed in isolation from the other. In particular, one implementation can be replaced by another without affecting the behavior of the client, provided that the two implementations meet the same interface and that each simulates the other with respect to the operations of the interface. This property is called representation independence for an abstract type.  Data abstraction is formalized by extending the language F with existential types. Inter- faces are existential types that provide a collection of operations acting on an unspeciﬁed, or abstract, type. Implementations are packages, the introduction form for existential types, and clients are uses of the corresponding elimination form. It is remarkable that the pro- gramming concept of data abstraction is captured so naturally and directly by the logical concept of existential type quantiﬁcation. Existential types are closely connected with uni- versal types, and hence are often treated together. The superﬁcial reason is that both are forms of type quantiﬁcation, and hence both require the machinery of type variables. The deeper reason is that existential types are deﬁnable from universals—surprisingly, data abstraction is actually just a form of polymorphism! Consequently, representation indepen- dence is an application of the parametricity properties of polymorphic functions discussed in Chapter 16.  17.1 Existential Types  The syntax of FE extends F with the following constructs:  Typ τ Exp e  ::= some t.τ  ::= pack{t.τ}{ρ} e   ∃ t.τ  pack ρ with e as∃ t.τ   open{t.τ}{ρ} e1; t, x.e2  open e1 as t with x:τ in e2  interface implementation client  The introduction form ∃ t.τ  is apackage of the form pack ρ with e as∃ t.τ , where ρ is a type and e is an expression of type [ρ t]τ. The type ρ is the representation type of the   147  17.1 Existential Types  package, and the expression e is the implementation of the package. The elimination form is the expression open e1 as t with x:τ in e2, which opens the package e1 for use within the client e2 by binding its representation type to t and its implementation to x for use within e2. Crucially, the typing rules ensure that the client is type-correct independently of the actual representation type used by the implementor, so that it can be varied without affecting the type correctness of the client.  The abstract syntax of the open construct speciﬁes that the type variable t and the expression variable x are bound within the client. They may be renamed at will by α- equivalence without affecting the meaning of the construct, provided, of course, that the names do not conﬂict with any others in scope. In other words the type t is a “new” type, one that is distinct from all other types, when it is introduced. This principle is sometimes called generativity of abstract types: the use of an abstract type by a client “generates” a “new” type within that client. This behavior relies on the identiﬁcation covnention stated in Chapter 1.  The statics of FE is given by these rules:  17.1.1 Statics   cid:8 , t type  cid:12  τ type  cid:8   cid:12  some t.τ  type   17.1a    17.1b    17.1c    cid:8   cid:12  ρ type  cid:8 , t type  cid:12  τ type  cid:8   cid:7   cid:12  e : [ρ t]τ   cid:8   cid:7   cid:12  pack{t.τ}{ρ} e  : some t.τ    cid:8   cid:7   cid:12  e1 : some t.τ   cid:8 , t type  cid:7 , x : τ  cid:12  e2 : τ2  cid:8   cid:12  τ2 type   cid:8   cid:7   cid:12  open{t.τ}{τ2} e1; t, x.e2  :τ 2  Rule  17.1c  is complex, so study it carefully! There are two important things to notice:  1. The type of the client, τ2, must not involve the abstract type t. This restriction prevents the client from attempting to export a value of the abstract type outside of the scope of its deﬁnition.  2. The body of the client, e2, is type checked without knowledge of the representation type,  t. The client is, in effect, polymorphic in the type variable t.  Lemma 17.1  Regularity . Suppose that  cid:8   cid:7   cid:12  e : τ . If cid:8   cid:12  τi type for each xi : τi in  cid:7 , then  cid:8   cid:12  τ type.  Proof By induction on rules  17.1 , using substitution for expressions and types.   148  Abstract Types  17.1.2 Dynamics  The dynamics of FE is deﬁned by the following rules  including the bracketed material for an eager interpretation, and omitting it for a lazy interpretation :  [e val]  pack{t.τ}{ρ} e  val   cid:3   e  cid:20 −→ e   cid:5   pack{t.τ}{ρ} e   cid:20 −→ pack{t.τ}{ρ} e   cid:5     cid:4   open{t.τ}{τ2} e1; t, x.e2   cid:20 −→ open{t.τ}{τ2} e   cid:5  1; t, x.e2   e1  cid:20 −→ e   cid:5  1  open{t.τ}{τ2} pack{t.τ}{ρ} e ; t, x.e2   cid:20 −→ [ρ, e t, x]e2  [e val]   17.2a    17.2b    17.2c    17.2d   It is important to see that, according to these rules, there are no abstract types at run- time! The representation type is propagated to the client by substitution when the package is opened, thereby eliminating the abstraction boundary between the client and the im- plementor. Thus, data abstraction is a compile-time discipline that leaves no traces of its presence at execution time.  17.1.3 Safety  Safety of FE is stated and proved by decomposing it into progress and preservation. Theorem 17.2  Preservation . If e : τ and e  cid:20 −→ e Proof By rule induction on e  cid:20 −→ e variables.   cid:5 , using substitution for both expression- and type   cid:5 , then e   cid:5  : τ .  Lemma 17.3  Canonical Forms . If e : some t.τ  and e val, then e = pack{t.τ}{ρ} e some type ρ and some e   cid:5  such that e   cid:5  : [ρ t]τ .   cid:5   for  Proof By rule induction on the statics, using the deﬁnition of closed values.  Theorem 17.4  Progress . If e : τ , then either e val or there exists e   cid:5  such that e  cid:20 −→ e   cid:5 .  Proof By rule induction on e : τ, using the canonical forms lemma.   149  17.2 Data Abstraction  17.2 Data Abstraction  To illustrate the use of FE, we consider an abstract type of queues of natural numbers supporting three operations:  1. Forming the empty queue. 2. Inserting an element at the tail of the queue. 3. Removing the head of the queue, which is assumed non-empty.  This is clearly a bare-bones interface but sufﬁces to illustrate the main ideas of data abstraction. Queue elements are natural numbers, but nothing depends on this choice.  The crucial property of this description is that nowhere do we specify what queues actually are, only what we can do with them. The behavior of a queue is expressed by the existential type ∃ t.τ , which serves as the interface of the queue abstraction: ∃ t. cid:24 emp  cid:9 → t, ins  cid:9 → nat × t → t, rem  cid:9 → t →  nat × t  opt cid:25  .  The representation type t of queues is abstract—all that is known about it is that it supports the operations emp, ins, and rem, with the given types.  An implementation of queues consists of a package specifying the representation type, together with the implementation of the associated operations in terms of that representation. Internally to the implementation, the representation of queues is known and relied upon by the operations. Here is a very simple implementation el in which queues are represented as lists:  pack natlist with cid:24 emp  cid:9 → nil, ins  cid:9 → ei, rem  cid:9 → er cid:25  as∃ t.τ ,  where  and  ei : nat × natlist → natlist = λ  x : nat × natlist  . . . ,  er : natlist → nat × natlist = λ  x : natlist  . . . .  The elided body of ei conses the ﬁrst component of x, the element, onto the second component of x, the queue, and the elided body of er reverses its argument, and returns the head element paired with the reversal of the tail. Both of these operations “know” that queues are represented as values of type natlist and are programmed accordingly. It is also possible to give another implementation ep of the same interface ∃ t.τ , but in which queues are represented as pairs of lists, consisting of the “back half” of the queue paired with the reversal of the “front half.” This two-part representation avoids the need for reversals on each call and, as a result, achieves amortized constant-time behavior: pack natlist × natlist with cid:24 emp  cid:9 →  cid:24 nil, nil cid:25 , ins  cid:9 → ei, rem  cid:9 → er cid:25  as∃ t.τ . In this case, ei has type  nat ×  natlist × natlist  →  natlist × natlist ,   150  Abstract Types  and er has type   natlist × natlist  → nat ×  natlist × natlist .  These operations “know” that queues are represented as values of type natlist×natlist and are implemented accordingly.  The important point is that the same client type checks regardless of which implemen- tation of queues we choose, because the representation type is hidden, or held abstract, from the client during type checking. Consequently, it cannot rely on whether it is natlist or natlist × natlist or some other type. That is, the client is independent of the representation of the abstract type.  17.3 Deﬁnability of Existential Types  The language FE is not a proper extension of F, because existential types  under a lazy dynamics  are deﬁnable in terms of universal types. Why should this be possible? Note that the client of an abstract type is polymorphic in the representation type. The typing rule for  open e1 as t with x:τ in e2 : τ2,  where e1 : ∃ t.τ , speciﬁes that e2 : τ2 under the assumptions t type and x : τ. In essence, the client is a polymorphic function of type  ∀ t.τ → τ2 ,  where t may occur in τ  the type of the operations , but not in τ2  the type of the result .  This suggests the following encoding of existential types:  ∃ t.τ   cid:2  ∀ u.∀ t.τ → u  → u   pack ρ with e as∃ t.τ   cid:2   cid:16  u  λ  x : ∀ t.τ → u   x[ρ] e   open e1 as t with x:τ in e2  cid:2  e1[τ2]  cid:16  t  λ  x : τ  e2   An existential is encoded as a polymorphic function taking the overall result type u as argument, followed by a polymorphic function representing the client with result type u, and yielding a value of type u as overall result. Consequently, the open construct simply packages the client as such a polymorphic function, instantiates the existential at the result type, τ2, and applies it to the polymorphic client.  The translation therefore depends on knowing the overall result type τ2 of the open construct.  Finally, a package consisting of a representation type ρ and an implementation e is a polymorphic function that, when given the result type u and the client x, instantiates x with ρ and passes to it the imple- mentation e.   151  17.4 Representation Independence  17.4 Representation Independence  An important consequence of parametricity is that it ensures that clients are insensitive to the representations of abstract types. More precisely, there is a criterion, bisimilarity, for relating two implementations of an abstract type such that the behavior of a client is unaffected by swapping one implementation by another that is bisimilar to it. This principle leads to a simple method for proving the correctness of candidate implementation of an abstract type, which is to show that it is bisimilar to an obviously correct reference implementation of it. Because the candidate and the reference implementations are bisimilar, no client may distinguish them from one another, and hence if the client behaves properly with the reference implementation, then it must also behave properly with the candidate.  To derive the deﬁnition of bisimilarity of implementations, it is helpful to examine the deﬁnition of existential types in terms of universals given in Section 17.3. It is immediately clear that the client of an abstract type is polymorphic in the representation of the abstract type. A client c of an abstract type ∃ t.τ  has type ∀ t.τ → τ2 , where t does not occur free in τ2  but may, of course, occur in τ . Applying the parametricity property described informally in Chapter 16  and developed rigorously in Chapter 48 , this says that if R is a bisimulation relation between any two implementations of the abstract type, then the client behaves identically on them. The fact that t does not occur in the result type ensures that the behavior of the client is independent of the choice of relation between the implementations, provided that this relation is preserved by the operations that implement it. ∃ t.τ , where τ is the labeled tuple type  Explaining what is a bisimulation is best done by example. Consider the existential type   cid:24 emp  cid:9 → t, ins  cid:9 → nat × t → t, rem  cid:9 → t →  nat × t  opt cid:25 .  This speciﬁes an abstract type of queues. The operations emp, ins, and rem specify, respectively, the empty queue, an insert operation, and a remove operation. For the sake of simplicity, the element type is the type of natural numbers. The result of removal is an optional pair, according to whether the queue is empty or not.  cid:5  are any two closed types, and if R is a relation Theorem 48.12 ensures that if ρ and ρ between expressions of these two types, then if the implementations e : [ρ x]τ and  cid:5 . It remains to deﬁne when  cid:5  : [ρ e two implementations respect the relation R. Let   x]τ respect R, then c[ρ]e behaves the same as c[ρ   cid:5 ]e   cid:5   and  e  cid:2   cid:24 emp  cid:9 → em, ins  cid:9 → ei, rem  cid:9 → er cid:25    cid:5   cid:2   cid:24 emp  cid:9 → e  e  m, ins  cid:9 → e  cid:5   i, rem  cid:9 → e  cid:5    cid:5  r   cid:25 .  For these implementations to respect R means that the following three conditions hold:  1. The empty queues are related: R em, e 2. Inserting the same element on each of two related queues yields related queues: if d : τ  and R q, q   cid:5  , then R ei d  q , e   cid:5  i d  q   cid:5  m .  cid:5   .   152  Abstract Types  3. If two queues are related, then either they are both empty, or their front elements are the  same and their back elements are related: if R q, q  a  er q  ∼= null ∼= e  cid:5  , or  cid:5  r q  b  er q  ∼= just  cid:24 d, r cid:25   and e   cid:5   ∼= just  cid:24 d  , r   cid:5    cid:5  r q   cid:5  , then either ∼= d  cid:5  cid:25  , with d   cid:5  and R r, r   cid:5  .   cid:5  are bisimilar. The terminology If such a relation R exists, then the implementations e and e stems from the requirement that the operations of the abstract type preserve the relation: if it holds before an operation is performed, then it must also hold afterwards, and the relation must hold for the initial state of the queue. Thus, each implementation simulates the other up to the relationship speciﬁed by R.  To see how this works in practice, let us consider informally two implementations of the abstract type of queues deﬁned earlier. For the reference implementation, we choose ρ to be the type natlist, and deﬁne the empty queue to be the empty list, deﬁne insert to add the given element to the head of the list, and deﬁne remove to remove the last element of the list. The code is as follows:  t  cid:2  natlist  For the candidate implementation, we choose ρ  emp  cid:2  nil ins  cid:2  λ  x : nat  λ  q : t  cons x; q  rem  cid:2  λ  q : t  case rev q {nil  cid:9 → null  cons f ; qr   cid:9 → just  cid:24 f, rev qr  cid:25  }. Removing an element takes time linear in the length of the list, because of the reversal.  cid:5  to be the type natlist × natlist of pairs of lists  cid:24 b, f cid:25  in which b is the “back half” of the queue, and f is the reversal of the “front half” of the queue. For this representation, we deﬁne the empty queue to be a pair of empty lists, deﬁne insert to extend the back with that element at the head, and deﬁne remove based on whether the front is empty. If it is non-empty, the head element is removed from it and returned along with the pair consisting of the back and the tail of the front. If it is empty, and the back is not, then we reverse the back, remove the head element, and return the pair consisting of the empty list and the tail of the now-reversed back. The code is as follows:  t  cid:2  natlist × natlist  emp  cid:2   cid:24 nil, nil cid:25  ins  cid:2  λ  x : nat  λ   cid:24 bs, f s cid:25  : t  cid:24 cons x; bs , f s cid:25  rem  cid:2  λ   cid:24 bs, f s cid:25  : t  case f s {nil  cid:9 → e  cons f ; f s  e  cid:2  case rev bs {nil  cid:9 → null  cons b; bs   cid:5    cid:5      cid:9 →  cid:24 bs, f s     cid:9 → just  cid:24 b, cid:24 nil, bs   cid:5  cid:25 }, where  cid:5  cid:25  cid:25  }.  The cost of the occasional reversal is amortized across the sequence of inserts and removes to show that each operation in a sequence costs unit time overall.  To show that the candidate implementation is correct, we show that it is bisimilar to the reference implementation. To do so, we specify a relation R between the types natlist and natlist × natlist such that the two implementations satisfy the three simulation conditions given earlier. The required relation states that R l, cid:24 b, f cid:25   iff the list l is the list   153  Exercises  app b  rev f   , where app is the evident append function on lists. That is, thinking of l as the reference representation of the queue, the candidate must ensure that the elements of b followed by the elements of f in reverse order form precisely the list l. It is easy to check that the implementations just described preserve this relation. Having done so, we are assured that the client c behaves the same regardless of whether we use the reference or the candidate. Because the reference implementation is obviously correct  albeit inefﬁcient , the candidate must also be correct in that the behavior of any client is not affected by using it instead of the reference.  The connection between abstract types in programming languages and existential types in logic was made by Mitchell and Plotkin  1988 . Closely related ideas were already present in Reynolds  1974 , but the connection with existential types was not explicitly drawn there. The present formulation of representation independence follows closely Mitchell  1986 .  17.5 Notes  Exercises  17.1. Show that the statics and dynamics of existential types are correctly simulated using  the interpretation given in Section 17.3.  17.2. Deﬁne in FE of the coinductive type of streams of natural numbers as deﬁned in Chapter 15. Hint: Deﬁne the representation in terms of the generator  introduction form  for streams.  17.3. Deﬁne in FE an arbitrary coinductive type ν t.τ . Hint: generalize your answer to  Exercise 17.2.  17.4. Representation independence for abstract types is a corollary of the parametricity the- orem for polymorphic types, using the interpretation of FE in F given in Section 17.3. Recast the proof of equivalence of the two implementations of queues given in Sec- tion 17.4 as an instance of parametricity as deﬁned informally in Chapter 16.   18  Higher Kinds  The concept of type quantiﬁcation naturally leads to the consideration of quantiﬁcation over type constructors, such as list, which are functions mapping types to types. For example, the abstract type of queues of natural numbers considered in Section 17.4 could be generalized to an abstract type constructor of queues that does not ﬁx the element type. In the notation that we shall develop in this chapter, such an abstraction is expressed by the existential type ∃ q :: T → T.σ , where σ is the labeled tuple type  cid:24 emp  cid:9 → ∀ t :: T.t, ins  cid:9 → ∀ t :: T.t × q[t] → q[t], rem  cid:9 → ∀ t :: T.q[t] →  t × q[t]  opt cid:25 . The existential type quantiﬁes over the kind T → T of type constructors, which map types to types. The operations are polymorphic, or generic, in the type of the elements of the queue. Their types involve instances of the abstract queue constructor q[t] representing the abstract type of queues whose elements are of type t. The client instantiates the polymorphic quantiﬁer to specify the element type; the implementations are parametric in this choice  in that their behavior is the same in any case . A package of the existential type given above consists of a representation type constructor and an implementation of the operations in terms of this choice. Possible representations include the constructor λ  u :: T  u list and the constructor λ  u :: T  u list × u list, both of which have kind T → T. It is easy to check that the implementations of the queue operations given in Section 17.4 carry over to the more general case, almost without change, because they do not rely on the type of the elements of the queue. The language Fω enriches the language F with universal and existential quantiﬁcation over kinds, such as T → T, used in the queues example. The extension accounts for deﬁnitional equality of constructors. For example, an implementation of the existential given in the preceding paragraph have to give implementations for the operations in terms of the choice of representation for q. If, say, q is the constructor λ  u :: T  u list, then the ins operation takes a type argument specifying the element type t and a queue of type  λ  u :: T  u list [t], which should simplify to t list by substitution of t for u in the body of the λ-abstraction. Deﬁnitional equality of constructors deﬁnes the permissible rules of simpliﬁcation and thereby deﬁnes when two types are equal. Equal types should be interchangeable as classiﬁers, meaning that if e is of type τ and τ is deﬁnitionally equal  cid:5 . In the queues example, any expression of type t list to τ should also be of the unsimpliﬁed type to which it is deﬁnitionally equal.   cid:5 , then e should also have type τ   155  18.1 Constructors and Kinds  18.1 Constructors and Kinds  The syntax of kinds of Fω is given by the following grammar:  Kind κ  ::= Type Unit Prod κ1; κ2  Arr κ1; κ2   T 1 κ1 × κ2 κ1 → κ2  types nullary product binary product function  The kinds consist of the kind of types T and the unit kind Unit and are closed under formation of product and function kinds.  Con c  ::= u  The syntax of constructors of Fω is deﬁned by this grammar: variable function constructor universal quantiﬁer existential quantiﬁer ﬁrst projection second projection application null tuple pair abstraction  arr all{κ} some{κ} proj[l] c  proj[r] c  app c1; c2  unit pair c1; c2  lam u.c   u → ∀κ ∃κ c · l c · r c1[c2]  cid:24  cid:25   cid:24 c1,c2 cid:25  λ  u  c  The syntax of constructors follows the syntax of kinds in that there are introduction and elimination forms for all kinds. The constants →, ∀κ, and ∃κ are the introduction forms for the kind T; there are no elimination forms, because types are only used to classify expressions. We use the meta-variable τ for constructors of kind T, and write τ1 → τ2 for the application →[τ1][τ2], ∀ u :: κ.τ for ∀κ[λ  u :: κ  τ ], and similarly for the existential quantiﬁer.  The statics of constructors and kinds of Fω is speciﬁed by the judgment   cid:8   cid:12  c :: κ  which states that the constructor c is well-formed with kind κ. The hypotheses  cid:8  consist of a ﬁnite set of assumptions  where n ≥ 0, specifying the kinds of the active constructor variables.  The statics of constructors is deﬁned by the following rules:  u1 :: κ1, . . . , un :: κn,   cid:8 , u :: κ  cid:12  u :: κ  cid:8   cid:12  → :: T → T → T  cid:8   cid:12  ∀κ ::  κ → T  → T  cid:8   cid:12  ∃κ ::  κ → T  → T   18.1a    18.1b    18.1c    18.1d    156  The kinds of the three constants specify that they can be used to build constructors of kind T, the kind of types, which, as usual, classify expressions.  The rules of deﬁnitional equality for Fω deﬁne when two constructors, in particular two types, are interchangeable by differing only by simpliﬁcations that can be performed to obtain one from the other. The judgment  states that c1 and c2 are deﬁnitionally equal constructors of kind κ. When κ is the kind T, the constructors c1 and c2 are deﬁnitionally equal types.  Deﬁnitional equality of constructors is deﬁned by these rules:   cid:8   cid:12  c1 :: κ2 → κ  cid:8   cid:12  c2 :: κ2  Higher Kinds   cid:8   cid:12  c :: κ1 × κ2  cid:8   cid:12  c · l :: κ1  cid:8   cid:12  c :: κ1 × κ2  cid:8   cid:12  c · r :: κ2   cid:8   cid:12  c1[c2] :: κ  cid:8   cid:12   cid:24  cid:25  :: 1   cid:8   cid:12  c1 :: κ1  cid:8   cid:12  c2 :: κ2  cid:8   cid:12   cid:24 c1,c2 cid:25  :: κ1 × κ2  cid:8 , u :: κ1  cid:12  c2 :: κ2  cid:8   cid:12  λ  u  c2 :: κ1 → κ2  18.2 Constructor Equality   cid:8   cid:12  c1 ≡ c2 :: κ   cid:8   cid:12  c :: κ  cid:8   cid:12  c ≡ c :: κ  cid:8   cid:12  c ≡ c  cid:5  :: κ  cid:8   cid:12  c  cid:5  ≡ c :: κ  cid:5  :: κ  cid:8   cid:12  c  cid:8   cid:12  c ≡ c  cid:5  cid:5  :: κ   cid:8   cid:12  c ≡ c   cid:5  ≡ c   cid:5  cid:5  :: κ   cid:8   cid:12  c ≡ c  cid:8   cid:12  c · l ≡ c  cid:8   cid:12  c ≡ c  cid:8   cid:12  c · r ≡ c   cid:5  :: κ1 × κ2  cid:5  · l :: κ1  cid:5  :: κ1 × κ2  cid:5  · r :: κ2 1 :: κ1  cid:8   cid:12  c2 ≡ c  cid:5    cid:8   cid:12  c1 ≡ c   cid:8   cid:12   cid:24 c1,c2 cid:25  ≡  cid:24 c   cid:5   cid:5  1,c 2   cid:25  :: κ1 × κ2   cid:5  2 :: κ2   18.1e    18.1f    18.1g    18.1h    18.1i    18.1j    18.2a    18.2b    18.2c    18.2d    18.2e    18.2f    157  18.3 Expressions and Types   cid:8   cid:12  c1 :: κ1  cid:8   cid:12  c2 :: κ2  cid:8   cid:12   cid:24 c1,c2 cid:25  ·l ≡ c1 :: κ1  cid:8   cid:12  c1 :: κ1  cid:8   cid:12  c2 :: κ2  cid:8   cid:12   cid:24 c1,c2 cid:25  ·r ≡ c2 :: κ2   cid:5  2 :: κ2   cid:8   cid:12  c1 ≡ c  1 :: κ2 → κ  cid:8   cid:12  c2 ≡ c  cid:5   cid:8   cid:12  c1[c2] ≡ c  cid:5   cid:5  2] :: κ 1[c  cid:8 , u :: κ  cid:12  c2 ≡ c  cid:5  2 :: κ2  cid:8   cid:12  λ  u :: κ  c2 ≡ λ  u :: κ  c  2 :: κ → κ2  cid:5    cid:8 , u :: κ1  cid:12  c2 :: κ2  cid:8   cid:12  c1 :: κ1  cid:8   cid:12   λ  u :: κ  c2 [c1] ≡ [c1 u]c2 :: κ2  18.3 Expressions and Types   18.2g    18.2h    18.2i    18.2j    18.2k   In short, deﬁnitional equality of constructors is the strongest congruence containing the rules  18.2g ,  18.2h , and  18.2k .  The statics of expressions of Fω is deﬁned using two judgment forms:   cid:8   cid:12  τ type  cid:8   cid:7   cid:12  e : τ  type formation expression formation  Here, as before,  cid:7  is a ﬁnite set of hypotheses of the form  x1 : τ1, . . . , xk : τk  such that  cid:8   cid:12  τi type for each 1 ≤ i ≤ k. The types of Fω are the constructors of kind T:  cid:8   cid:12  τ :: T  cid:8   cid:12  τ type  .   18.3    18.4   This being the only rule for introducing types, the only types are the constructors of kind T.  Deﬁnitionally equal types classify the same expressions:  cid:8   cid:7   cid:12  e : τ1  cid:8   cid:12  τ1 ≡ τ2 :: T  .   cid:7   cid:12  e : τ2  This rule ensures that in situations such as that described in the introduction to this chapter, typing is inﬂuenced by simpliﬁcation of types.  The language Fω extends F to permit universal quantiﬁcation over arbitrary kinds; the language FEω extends Fω with existential quantiﬁcation over arbitrary kinds. The statics of the quantiﬁers in FEω is deﬁned by the following rules:  cid:8   cid:7   cid:12   cid:16  u :: κ  e : ∀ u :: κ.τ   cid:8 , u :: κ  cid:7   cid:12  e : τ   18.5a    158  Higher Kinds   cid:8   cid:7   cid:12  e : ∀ u :: κ.τ  cid:8   cid:12  c :: κ   cid:8   cid:7   cid:12  e[c] : [c u]τ   cid:8   cid:12  c :: κ  cid:8 , u :: κ  cid:12  τ type  cid:8   cid:7   cid:12  e : [c u]τ   cid:8   cid:7   cid:12  pack c with e as∃ u :: κ.τ : ∃ u :: κ.τ   cid:8   cid:7   cid:12  e1 : ∃ u :: κ.τ  cid:8 , u :: κ  cid:7 , x : τ  cid:12  e2 : τ2  cid:8   cid:12  τ2 type   cid:8   cid:7   cid:12  open e1 as u :: κ with x : τ in e2 : τ2  The dynamics of FEω is the subject of Exercise 18.2.   18.5b    18.5c    18.5d   The language Fω given here is standard, apart from details of notation. The rule of invariance of typing under deﬁnitional equality of types demands that a type checking algorithm must include as a subroutine an algorithm for checking deﬁnitional equality. Numerous methods for checking such equivalences are given in the literature, all of which proceed by various means to simplify both sides of an equation, and check whether the results are the same. Another approach, pioneered by Watkins et al.  2008  in another setting, is to avoid deﬁnitional equality by maintaining constructors in simpliﬁed form. The discussion in the introduction shows that substitution of a simpliﬁed constructor into a simpliﬁed constructor is not necessarily simpliﬁed. The burden is then shifted to deﬁning a form of simplifying substitution whose result is always in simpliﬁed form.  18.4 Notes  Exercises  18.1. Adapt the two implementations of queues given in Chapter 17 to match the signature  of queue constructors given in the introduction,  ∃ q :: T → T. cid:24 emp  cid:9 → ∀ t :: T.t, ins  cid:9 → ∀ t :: T.t × q[t] → q[t],  rem  cid:9 → ∀ t :: T.q[t] →  t × q[t]  opt cid:25 .  Consider the role played by deﬁnitional equality in ensuring that both implementa- tions have this type.  18.2. Give an equational dynamics for FEω. What role does deﬁnitional equality of con- structors play in it? Formulate a transition dynamics for FEω extended with a type of observable results, say, nat. What role does deﬁnitional equality play in the transition dynamics?   P A R T VIII  Partiality and Recursive  Types    19  System PCF of Recursive Functions  We introduced the language T as a basis for discussing total computations, those for which the type system guarantees termination. The language M generalizes T to admit inductive and coinductive types, while preserving totality. In this chapter, we introduce PCF as a basis for discussing partial computations, those that may not terminate when evaluated, even when they are well-typed. At ﬁrst blush, this may seem like a disadvantage, but as we shall see in Chapter 20, it admits greater expressive power than is possible in T.  The source of partiality in PCF is the concept of general recursion, which permits the solution of equations between expressions. The price for admitting solutions to all such equations is that computations may not terminate—the solution to some equations might be undeﬁned  divergent . In PCF, the programmer must make sure that a computation terminates; the type system does not guarantee it. The advantage is that the termination proof need not be embedded into the code itself, resulting in shorter programs.  For example, consider the equations  f  0   cid:2  1  f  n + 1   cid:2   n + 1  × f  n .  Intuitively, these equations deﬁne the factorial function. They form a system of simultaneous equations in the unknown f , which ranges over functions on the natural numbers. The function we seek is a solution to these equations—a speciﬁc function f : N → N such that the above conditions are satisﬁed.  A solution to such a system of equations is a ﬁxed point of an associated functional   higher-order function . To see this, let us re-write these equations in another form:  f  n   cid:2   1 n × f  n   cid:5    if n = 0 if n = n   cid:5  + 1.   cid:9    cid:9   Re-writing yet again, we seek f given by  n  cid:20 →  1 n × f  n  if n = 0 if n = n Now deﬁne the functional F by the equation F  f   = f if n = 0 if n = n  1 n × f  n  n  cid:20 →   cid:9    cid:5     cid:5     cid:5  + 1.  cid:5 , where f   cid:5  + 1.   cid:5  is given by   162  System PCF of Recursive Functions   cid:5  is expressed in terms of f , the argument to the functional Note well that the condition on f  cid:5  itself! The function f we seek is a ﬁxed point of F , a function F , and not in terms of f f : N → N such that f = F  f  . In other words e is deﬁned to be ﬁx F  , where ﬁx is a higher-order operator on functionals F that computes a ﬁxed point for it.  Why should an operator such as F have a ﬁxed point? The key is that functions in PCF are partial, which means that they may diverge on some  or even all  inputs. Consequently, a ﬁxed point of a functional F is the limit of a series of approximations of the desired solution obtained by iterating F . Let us say that a partial function φ on the natural numbers, is an approximation to a total function f if φ m  = n implies that f  m  = n. Let ⊥: N  cid:19  N be the totally undeﬁned partial function—⊥ n  is undeﬁned for every n ∈ N. This is the “worst” approximation to the desired solution f of the recursion equations given above.  cid:5  = F  φ . The partial function Given any approximation φ of f , we may “improve” it to φ  cid:5  is deﬁned on 0 and on m + 1 for every m ≥ 0 on which φ is deﬁned. Continuing, φ  cid:5  cid:5  = F  φ  cid:5 , and hence a further improvement on φ. φ If we start with ⊥ as the initial approximation to f , then pass to the limit   cid:5   = F  F  φ   is an improvement on φ  F  i  ⊥ ,  lim i≥0  we will obtain the least approximation to f that is deﬁned for every m ∈ N, and hence is the function f itself. Turning this around, if the limit exists, it is the solution we seek.  Because this construction works for any functional F , we conclude that all such operators have ﬁxed points, and hence that all systems of equations such as the one given above have solutions. The solution is given by general recursion, but there is no guarantee that it is a total function  deﬁned on all elements of its domain . For the above example, it happens to be true, because we can prove by induction that this is so, but in general, the solution is a partial function that may diverge on some inputs. It is our task as programmers to ensure that the functions deﬁned by general recursion are total, or at least that we have a grasp of those inputs for which it is well-deﬁned.  19.1 Statics  The syntax of PCF is given by the following grammar:  Typ τ  Exp e  parr τ1; τ2   nat τ1  cid:19  τ2 x z s e   ::= nat ::= x z s e  ifz{e0; x.e1} e  ifz e {z  cid:9 → e0  s x   cid:9 → e1} lam{τ} x.e  ap e1; e2  fix{τ} x.e   λ  x : τ  e e1 e2  fix x : τ is e  naturals partial function variable zero successor zero test abstraction application recursion   163  19.2 Dynamics  The expression fix{τ} x.e  is general recursion; it is discussed in more detail below. The expression ifz{e0; x.e1} e  branches according to whether e evaluates to z, binding the predecessor to x in the case that it is not.  The statics of PCF is inductively deﬁned by the following rules:   cid:7 , x : τ  cid:12  x : τ   cid:7   cid:12  z : nat  cid:7   cid:12  e : nat  cid:7   cid:12  s e  : nat   cid:7   cid:12  e : nat  cid:7   cid:12  e0 : τ  cid:7 , x : nat  cid:12  e1 : τ   cid:7   cid:12  ifz{e0; x.e1} e  : τ   cid:7 , x : τ1  cid:12  e : τ2   cid:7   cid:12  lam{τ1} x.e  :parr  τ1; τ2   cid:7   cid:12  e1 : parr τ2; τ    cid:7   cid:12  e2 : τ2   cid:7   cid:12  ap e1; e2  : τ  cid:7 , x : τ  cid:12  e : τ  cid:7   cid:12  fix{τ} x.e  :τ  Rule  19.1g  reﬂects the self-referential nature of general recursion. To show that fix{τ} x.e  has type τ, we assume that it is the case by assigning that type to the variable x, which stands for the recursive expression itself, and checking that the body, e, has type τ under this very assumption.  The structural rules, including in particular substitution, are admissible for the static  semantics. Lemma 19.1. If  cid:7 , x : τ  cid:12  e   cid:5  : τ   cid:5 ,  cid:7   cid:12  e : τ , then  cid:7   cid:12  [e x]e   cid:5  : τ   cid:5 .  The dynamic semantics of PCF is deﬁned by the judgments e val, specifying the closed values, and e  cid:20 −→ e   cid:5 , specifying the steps of evaluation. The judgment e val is deﬁned by the following rules:  19.2 Dynamics  z val  [e val] s e  val   19.1a    19.1b    19.1c    19.1d    19.1e    19.1f    19.1g    19.2a    19.2b    164  System PCF of Recursive Functions  The bracketed premise on rule  19.2b  is included for the eager interpretation of the successor operation, and omitted for the lazy interpretation.  See Chapter 36 for a further discussion of laziness.   The transition judgment e  cid:20 −→ e   cid:5  is deﬁned by the following rules:  lam{τ} x.e  val   cid:3    cid:4   e  cid:20 −→ e  cid:5  s e   cid:20 −→ s e  cid:5   e  cid:20 −→ e   cid:5    ifz{e0; x.e1} e   cid:20 −→ ifz{e0; x.e1} e   cid:5    ifz{e0; x.e1} z   cid:20 −→ e0  s e  val   cid:5  1  e1  cid:20 −→ e  ifz{e0; x.e1} s e    cid:20 −→ [e x]e1  cid:3   cid:4  ap e1; e2   cid:20 −→ ap e  cid:5  1; e2  e2  cid:20 −→ e  cid:5  2 ap e1; e2   cid:20 −→ ap e1; e  cid:5  2   e1 val  [e2 val]  ap lam{τ} x.e ; e2   cid:20 −→ [e2 x]e  fix{τ} x.e   cid:20 −→ [fix{τ} x.e  x]e   19.2c    19.3a    19.3b    19.3c    19.3d    19.3e    19.3f    19.3g    19.3h   The bracketed rule  19.3a  is included for an eager interpretation of the successor and omitted otherwise. Bracketed rule  19.3f  and the bracketed premise on rule  19.3g  are included for a call-by-value interpretation, and omitted for a call-by-name interpretation, of function application. Rule  19.3h  implements self-reference by substituting the recursive expression itself for the variable x in its body; this is called unwinding the recursion.  Theorem 19.2  Safety .  1. If e : τ and e  cid:20 −→ e 2. If e : τ , then either e val or there exists e   cid:5 , then e   cid:5  : τ .   cid:5  such that e  cid:20 −→ e   cid:5 .  Proof The proof of preservation is by induction on the derivation of the transition judg- ment. Consider rule  19.3h . Suppose that fix{τ} x.e  :τ . By inversion and substitution we have [fix{τ} x.e  x]e : τ, as required. The proof of progress proceeds by induction on the derivation of the typing judgment. For example, for rule  19.1g  the result follows because we may make progress by unwinding the recursion.   165  19.3 Deﬁnability  It is easy to check that if e val, then e is irreducible in that there is no e   cid:5  such that e  cid:20 −→ e  cid:5 . The safety theorem implies the converse, that an irreducible expression is a value, provided that it is closed and well-typed. Deﬁnitional equality for the call-by-name variant of PCF, written  cid:7   cid:12  e1 ≡ e2 : τ, is the strongest congruence containing the following axioms:   cid:7   cid:12  ifz{e0; x.e1} z  ≡ e0 : τ   cid:7   cid:12  ifz{e0; x.e1} s e   ≡ [e x]e1 : τ   cid:7   cid:12  fix{τ} x.e  ≡ [fix{τ} x.e  x]e : τ   cid:7   cid:12  ap lam{τ1} x.e2 ; e1  ≡ [e1 x]e2 : τ   19.4a    19.4b    19.4c    19.4d   These rules sufﬁce to calculate the value of any closed expression of type nat: ife : nat, then e ≡ n : nat iff e  cid:20 −→∗  n.  19.3 Deﬁnability  Let us write fun x y:τ1 :τ2 is e for a recursive function within whose body, e : τ2, are bound two variables, y : τ1 standing for the argument and x : τ1  cid:19  τ2 standing for the function itself. The dynamic semantics of this construct is given by the axiom   fun x y:τ1 :τ2 is e  e1   cid:20 −→ [fun x y:τ1 :τ2 is e, e1 x, y]e  .  That is, to apply a recursive function, we substitute the recursive function itself for x and the argument for y in its body.  Recursive functions are deﬁned in PCF using recursive functions, writing  fix x : τ1  cid:19  τ2 is λ  y : τ1  e  for fun x y:τ1 :τ2 is e. We may easily check that the static and dynamic semantics of recursive functions are derivable from this deﬁnition.  The primitive recursion construct of T is deﬁned in PCF using recursive functions by  taking the expression  to stand for the application e  rec e {z  cid:9 → e0  s x  with y  cid:9 → e1}  cid:5  e , where e   cid:5  is the general recursive function fun f  u:nat :τ is ifz u{z  cid:9 → e0  s x   cid:9 → [f  x  y]e1}.   166  System PCF of Recursive Functions  The static and dynamic semantics of primitive recursion are derivable in PCF using this expansion.  In general, functions deﬁnable in PCF are partial in that they may be undeﬁned for some arguments. A partial  mathematical  function, φ : N  cid:19  N, is deﬁnable in PCF iff there is an expression eφ : nat  cid:19  nat such that φ m  = n iff eφ m  ≡ n : nat. So, for example, if φ is the totally undeﬁned function, then eφ is any function that loops without returning when it is applied.  It is informative to classify those partial functions φ that are deﬁnable in PCF. The partial recursive functions are deﬁned to be the primitive recursive functions extended with the minimization operation: given φ m, n , deﬁne ψ n  to be the least m ≥ 0 such that , n  is deﬁned and non-zero, and  2  φ m, n  = 0. If no such m exists,  1  for m then ψ n  is undeﬁned.  < m, φ m   cid:5    cid:5   Theorem 19.3. A partial function φ on the natural numbers is deﬁnable in PCF iff it is partial recursive.  Proof sketch Minimization is deﬁnable in PCF, so it is at least as powerful as the set of partial recursive functions. Conversely, we may, with some tedium, deﬁne an evaluator for expressions of PCF as a partial recursive function, using G¨odel-numbering to represent expressions as numbers. Therefore, PCF does not exceed the power of the set of partial recursive functions.  Church’s Law states that the partial recursive functions coincide with the set of effectively computable functions on the natural numbers—those that can be carried out by a program written in any programming language that is or will ever be deﬁned.1 Therefore, PCF is as powerful as any other programming language with respect to the set of deﬁnable functions on the natural numbers.  The universal function φuniv for PCF is the partial function on the natural numbers  deﬁned by  φuniv  cid:4 e cid:5   m  = n iff e m  ≡ n : nat.  In contrast to T, the universal function φuniv for PCF is partial  might be undeﬁned for some inputs . It is, in essence, an interpreter that, given the code  cid:4 e cid:5  of a closed expression of type nat  cid:19  nat, simulates the dynamic semantics to calculate the result, if any, of applying it to the m, obtaining n. Because this process may fail to terminate, the universal function is not deﬁned for all inputs.  By Church’s Law, the universal function is deﬁnable in PCF. In contrast, we proved in Chapter 9 that the analogous function is not deﬁnable in T using the technique of diagonalization. It is instructive to examine why that argument does not apply in the present setting. As in Section 9.4, we may derive the equivalence  e cid:8   cid:4 e cid:8  cid:5   ≡ s e cid:8   cid:4 e cid:8  cid:5      167  19.5 Totality and Partiality  for PCF. But now, instead of concluding that the universal function, euniv, does not exist as we did for T, we instead conclude for PCF that euniv diverges on the code for e cid:8  applied to its own code.  19.4 Finite and Inﬁnite Data Structures  Finite data types  products and sums , including their use in pattern matching and generic programming, carry over verbatim to PCF. However, the distinction between the eager and lazy dynamics for these constructs becomes more important. Rather than being a matter of preference, the decision to use an eager or lazy dynamics affects the meaning of a program: the “same” types mean different things in a lazy dynamics than in an eager dynamics. For example, the elements of a product type in an eager language are pairs of values of the component types. In a lazy language, they are instead pairs of unevaluated, possibly divergent, computations of the component types, a very different thing indeed. And similarly for sums.  The situation grows more acute for inﬁnite types such as the type nat of “natural numbers.” The scare quotes are warranted, because the “same” type has a very different meaning under an eager dynamics than under a lazy dynamics. In the former case, the type nat is, indeed, the authentic type of natural numbers—the least type containing zero and closed under successor. The principle of mathematical induction is valid for reasoning about the type nat in an eager dynamics. It corresponds to the inductive type nat deﬁned in Chapter 15.  On the other hand, under a lazy dynamics the type nat is no longer the type of natural  numbers at all. For example, it includes the value  ω  cid:2  fix x : nat is s x ,  which has itself as predecessor! It is, intuitively, an “inﬁnite stack of successors,” growing without end. It is clearly not a natural number  it is larger than all of them , so the principle of mathematical induction does not apply. In a lazy setting, nat could be renamed lnat to remind us of the distinction; it corresponds to the type conat deﬁned in Chapter 15.  19.5 Totality and Partiality  The advantage of a total programming language such as T is that it ensures, by type checking, that every program terminates, and that every function is total. There is no way to have a well-typed program that goes into an inﬁnite loop. This prohibition may seem appealing, until one considers that the upper bound on the time to termination may be large, so large that it might as well diverge for all practical purposes. But let us grant for the moment that it is a virtue of T that it precludes divergence. Why, then, bother with a language such as PCF that does not rule out divergence? After all, inﬁnite loops are   168  System PCF of Recursive Functions  invariably bugs, so why not rule them out by type checking? The notion seems appealing until one tries to write a program in a language such as T.  Consider the computation of the greatest common divisor  gcd  of two natural numbers. It can be programmed in PCF by solving the following equations using general recursion:  gcd m, 0  = m gcd 0, n  = n gcd m, n  = gcd m − n, n  gcd m, n  = gcd m, n − m   if m > n if m < n  The type of gcd deﬁned this way is  nat × nat   cid:19  nat, which suggests that it may not terminate for some inputs. But we may prove by induction on the sum of the pair of arguments that it is, in fact, a total function.  Now consider programming this function in T. It is, in fact, programmable using only primitive recursion, but the code to do it is rather painful  try it! . One way to see the problem is that in T the only form of looping is one that reduces a natural number by one on each recursive call; it is not  directly  possible to make a recursive call on a smaller number other than the immediate predecessor. In fact, one may code up more general patterns of terminating recursion using only primitive recursion as a primitive, but if you check the details, you will see that doing so comes at a price in performance and program complexity. Program complexity can be mitigated by building libraries that codify standard patterns of reasoning whose cost of development should be amortized over all programs, not just one in particular. But there is still the problem of performance. Indeed, the encoding of more general forms of recursion into primitive recursion means that, deep within the encoding, there must be a “timer” that goes down by ones to ensure that the program terminates. The result will be that programs written with such libraries will be slower than necessary.  But, one may argue, T is simply not a serious language. A more serious total pro- gramming language would admit sophisticated patterns of control without performance penalty. Indeed, one could easily envision representing the natural numbers in binary, rather than unary, and allowing recursive calls by halving to get logarithmic complexity. Such a formulation is possible, as would be quite a number of analogous ideas that avoid the awkwardness of programming in T. Could we not then have a practical language that rules out divergence?  We can, but at a cost. We have already seen one limitation of total programming lan- guages: they are not universal. You cannot write an interpreter for T within T, and this limitation extends to any total language whatever. If this does not seem important, then consider the Blum Size Theorem  BST , which places another limitation on total languages. Fix any total language L that permits writing functions on the natural numbers. Pick any blowup factor, say 22n. The BST states that there is a total function on the natural numbers that is programmable in L, but whose shortest program in L is larger by the given blowup factor than its shortest program in PCF!  The underlying idea of the proof is that in a total language the proof of termination of a program must be baked into the code itself, whereas in a partial language the termination proof is an external veriﬁcation condition left to the programmer. There are, and always   169  Exercises  will be, programs whose termination proof is rather complicated to express, if you ﬁx in advance the means of proving it total.  In T it was primitive recursion, but one can be more ambitious, yet still get caught by the BST.  But if you leave room for ingenuity, then programs can be short, because they do not have to embed the proof of their termination in their own running code.  The solution to recursion equations described here is based on Kleene’s ﬁxed point theorem for complete partial orders, specialized to the approximation ordering of partial functions. The language PCF is derived from Plotkin  1977  as a laboratory for the study of semantics of programming languages. Many authors have used PCF as the subject of study of many problems in semantics. It has thereby become the E. coli of programming languages.  19.6 Notes  Exercises  19.1. Consider the problem considered in Section 10.3 of how to deﬁne the mutually recursive “even” and “odd” functions. There we gave a solution in terms of primitive recursion. You are, instead, to give a solution in terms of general recursion. Hint: consider that a pair of mutually recursive functions is a recursive pair of functions. 19.2. Show that minimization, as explained before the statement of Theorem 19.3, is  deﬁnable in PCF.  19.3. Consider the partial function φhalts such that if e : nat  cid:19  nat, then φhalts  cid:4 e cid:5   evaluates to zero iff e  cid:4 e cid:5   converges, and evaluates to one otherwise. Prove that φhalts is not deﬁnable in PCF. 19.4. Suppose that we changed the speciﬁcation of minimization given prior to Theo- rem 19.3 so that ψ n  is the least m such that φ m, n  = 0 and is undeﬁned if no such m exists. Is this “simpliﬁed” form of minimization deﬁnable in PCF?  19.5. Suppose that we wished to deﬁne, in the lazy variant of PCF, a version of the parallel or function speciﬁed a function of two arguments that returns z if either of its arguments is z, and s z  otherwise. That is, we wish to ﬁnd an expression e satisfying the following properties:  e e1  e2   cid:20 −→∗ e e1  e2   cid:20 −→∗ e e1  e2   cid:20 −→∗  z  z if e1  cid:20 −→∗ z if e2  cid:20 −→∗ z s z  otherwise  Thus, e deﬁnes a total function of its two arguments, even if one of the arguments diverges. Clearly, such a function cannot be deﬁned in the call-by-value variant of   170  System PCF of Recursive Functions  PCF, but can it be deﬁned in the call-by-name variant? If so, show how; if not, prove that it cannot be, and suggest an extension of PCF that would allow it to be deﬁned. 19.6. We appealed to Church’s Law to argue that the universal function for PCF is deﬁnable in PCF. See what is behind this claim by considering two aspects of the problem:  1  G¨odel-numbering, the representation of abstract syntax by a number;  2  evalua- tion, the process of interpreting a function on its inputs. Part  1  is a technical issue arising from the limited data structures available in PCF. Part  2  is the heart of the matter; explore its implementation in terms of a solution to Part  1 .  Note  1 See Chapter 21 for further discussion of Church’s Law.   20  System FPC of Recursive Types  In this chapter, we study FPC, a language with products, sums, partial fucntions, and ∼= τ where there is no recursive types. Recursive types are solutions to type equations t restriction on where t may occur in τ. Equivalently, a recursive type is a ﬁxed point up to isomorphism of the associated unrestricted type operator t.τ. By removing the restrictions ∼= t  cid:19  t, on the type operator, we may consider the solution of a type equation such as t which describes a type that is isomorphic to the type of partial functions deﬁned on itself. If types were sets, such an equation could not be solved, because there are more partial functions on a set than there are elements of that set. But types are not sets: they classify computable functions, not arbitrary functions. With types we may solve such “dubious” type equations, even though we cannot expect to do so with sets. The penalty is that we must admit non-termination. For one thing, type equations involving functions have solutions only if the functions involved are partial.  A beneﬁt of working in the setting of partial functions is that type equations have unique solutions  up to isomorphism . Therefore, it makes sense, as we shall do in this chapter, to speak of the solution to a type equation. But what about the distinct solutions to a type equation given in Chapter 15? These turn out to coincide for any ﬁxed dynamics but give rise to different solutions according to whether the dynamics is eager or lazy  as illustrated in Section 19.4 for the special case of the natural numbers . Under a lazy dynamics  where all constructs are evaluated lazily , recursive types have a coinductive ﬂavor, and the inductive analogs are inaccessible. Under an eager dynamics  where all constructs are evaluated eagerly , recursive types have an inductive ﬂavor. But the coinductive analogs are accessible as well, using function types to selectively impose laziness. It follows that the eager dynamics is more expressive than the lazy dynamics, because it is impossible to go the other way around  one cannot deﬁne inductive types in a lazy language .  20.1 Solving Type Equations  The language FPC has products, sums, and partial functions inherited from the preceding development, extended with the new concept of recursive types. The syntax of recursive types is deﬁned as follows:  Typ τ  Exp e  ::= t t rec t is τ ::= fold{t.τ} e  fold e   rec t.τ   unfold e   unfold e   self-reference recursive type fold unfold   172  System FPC of Recursive Types  The subscript on the concrete syntax of fold is often omitted when it is clear from context. Recursive types have the same general form as the inductive and coinductive types discussed in Chapter 15, but without restriction on the type operator involved. Recursive type are formed according to the rule:  The statics of folding and unfolding is given by the following rules:   cid:8 , t type  cid:12  τ type  cid:8   cid:12  rec t.τ  type   cid:7   cid:12  e : [rec t.τ  t]τ   cid:7   cid:12  fold{t.τ} e  : rec t.τ    cid:7   cid:12  e : rec t.τ    cid:7   cid:12  unfold e  : [rec t.τ  t]τ   cid:3    cid:4   [e val]  fold{t.τ} e  val  e  cid:20 −→ e   cid:5   fold{t.τ} e   cid:20 −→ fold{t.τ} e   cid:5    e  cid:20 −→ e   cid:5   unfold e   cid:20 −→ unfold e   cid:5    fold{t.τ} e  val  unfold fold{t.τ} e    cid:20 −→ e   20.1    20.2a    20.2b    20.3a    20.3b    20.3c    20.3d   The dynamics of folding and unfolding is given by these rules:  The bracketed premise and rule are included for an eager interpretation of the introduction form, and omitted for a lazy interpretation. As mentioned in the introduction, the choice of eager or lazy dynamics affects the meaning of recursive types. Theorem 20.1  Safety . 1. If e : τ and e  cid:20 −→ e  cid:5  : τ . 2. If e : τ , then either e val, or there exists e   cid:5  such that e  cid:20 −→ e   cid:5 , then e   cid:5 .  20.2 Inductive and Coinductive Types  Recursive types may be used to represent inductive types such as the natural numbers. Using an eager dynamics for FPC, the recursive type  satisﬁes the type equation  ρ = rec t is [z  cid:9 → unit, s  cid:9 → t]  ∼= [z  cid:9 → unit, s  cid:9 → ρ],  ρ   173  20.2 Inductive and Coinductive Types  and is isomorphic to the type of eager natural numbers. The introduction and elimination forms are deﬁned on ρ by the following equations:1 z  cid:2  fold z ·  cid:24  cid:25   s e   cid:2  fold s · e   ifz e {z  cid:9 → e0  s x   cid:9 → e1}  cid:2  case unfold e {z ·   cid:9 → e0  s · x  cid:9 → e1}.  It is a good exercise to check that the eager dynamics of natural numbers in PCF is correctly simulated by these deﬁnitions.  On the other hand, under a lazy dynamics for FPC, the same recursive type ρ   cid:5 ,  satisﬁes the same type equation,  rec t is [z  cid:9 → unit, s  cid:9 → t],   cid:5  ∼= [z  cid:9 → unit, s  cid:9 → ρ   cid:5   ],  ρ  but is not the type of natural numbers! Rather, it is the type lnat of lazy natural numbers  cid:5  contains the “inﬁnite number” introduced in Section 19.4. As discussed there, the type ρ ω, which is of course not a natural number.  Similarly, using an eager dynamics for FPC, the type natlist of lists of natural numbers  is deﬁned by the recursive type  rec t is [n  cid:9 → unit, c  cid:9 → nat × t],  which satisﬁes the type equation  natlist ∼= [n  cid:9 → unit, c  cid:9 → nat × natlist]. The list introduction operations are given by the following equations:  nil  cid:2  fold n ·  cid:24  cid:25    cons e1; e2   cid:2  fold c ·  cid:24 e1, e2 cid:25  .  A conditional list elimination form is given by the following equation: case e {nil  cid:9 → e0  cons x; y   cid:9 → e1}  cid:2  case unfold e {n ·  cid:9 → e0  c ·  cid:24 x, y cid:25   cid:9 → e1}, where we have used pattern-matching syntax to bind the components of a pair for the sake of clarity.  Now consider the same recursive type, but in the context of a lazy dynamics for FPC.  What type is it? If all constructs are lazy, then a value of the recursive type  rec t is [n  cid:9 → unit, c  cid:9 → nat × t],  has the form fold e , where e is an unevaluated computation of the sum type, whose values are injections of unevaluated computations of either the unit type or of the product type nat × t. And the latter consists of pairs of an unevaluated computation of a  lazy!  natural number, and an unevaluated computation of another value of this type. In particular, this type contains inﬁnite lists whose tails go on without end, as well as ﬁnite lists that   174  System FPC of Recursive Types  eventually reach an end. The type is, in fact, a version of the type of inﬁnite streams deﬁned in Chapter 15, rather than a type of ﬁnite lists as is the case under an eager dynamics.  It is common in textbooks to depict data structures using “box-and-pointer” diagrams. These work well in the eager setting, provided that no functions are involved. For example, an eager list of eager natural numbers may be depicted using this notation. We may think of fold as an abstract pointer to a tagged cell consisting of either  a  the tag n with no associated data, or  b  the tag c attached to a pair consisting of an authentic natural number and another list, which is an abstract pointer of the same type. But this notation does not scale well to types involving functions, or to languages with a lazy dynamics. For example, the recursive type of “lists” in lazy FPC cannot be depicted using boxes and pointers, because of the unevaluated computations occurring in values of this type. It is a mistake to limit one’s conception of data structures to those that can be drawn on the blackboard using boxes and pointers or similar informal notations. There is no substitute for a programming language to express data structures fully and accurately.  It is deceiving that the “same” recursive type can have two different meanings according to whether the underlying dynamics is eager or lazy. For example, it is common for lazy languages to use the name “list” for the recursive type of streams, or the name “nat” for the type of lazy natural numbers. This terminology is misleading, considering that such languages do not  and can not  have a proper type of ﬁnite lists or a type of natural numbers. Caveat emptor!  20.3 Self-Reference  In the general recursive expression fix{τ} x.e , the variable x stands for the expression itself. Self-reference is effected by the unrolling transition  fix{τ} x.e   cid:20 −→ [fix{τ} x.e  x]e,  which substitutes the expression itself for x in its body during execution. It is useful to think of x as an implicit argument to e that is instantiated to itself when the expression is used. In many well-known languages this implicit argument has a special name, such as this or self, to emphasize its self-referential interpretation.  Using this intuition as a guide, we may derive general recursion from recursive types. This derivation shows that general recursion may, like other language features, be seen as a manifestation of type structure, instead of as an ad hoc language feature. The derivation isolates a type of self-referential expressions given by the following grammar:  Typ τ Exp e  ::= self τ  ::= self{τ} x.e  self x is e unroll e   unroll e   τ self  self-referential type self-referential expression unroll self-reference  The statics of these constructs is given by the following rules:  cid:7   cid:12  self{τ} x.e  :self  τ    cid:7 , x : self τ   cid:12  e : τ   20.4a    The dynamics is given by the following rule for unrolling the self-reference:  175  20.3 Self-Reference   cid:7   cid:12  e : self τ   cid:7   cid:12  unroll e  : τ  self{τ} x.e  val  e  cid:20 −→ e   cid:5   unroll e   cid:20 −→ unroll e   cid:5     20.4b    20.5a    20.5b    20.5c   unroll self{τ} x.e    cid:20 −→ [self{τ} x.e  x]e  The main difference, compared to general recursion, is that we distinguish a type of self- referential expressions, instead of having self-reference at every type. However, as we shall see, the self-referential type sufﬁces to implement general recursion, so the difference is a matter of taste.  The type self τ  is deﬁnable from recursive types. As suggested earlier, the key is to consider a self-referential expression of type τ to depend on the expression itself. That is, we seek to deﬁne the type self τ  so that it satisﬁes the isomorphism  self τ  ∼= self τ   cid:19  τ.  We seek a ﬁxed point of the type operator t.t  cid:19  τ, where t  ∈ τ is a type variable standing for the type in question. The required ﬁxed point is just the recursive type  rec t.t  cid:19  τ ,  which we take as the deﬁnition of self τ .  The self-referential expression self{τ} x.e  is the expression  fold λ  x : self τ   e .  We may check that rule  20.4a  is derivable according to this deﬁnition. The expression unroll e  is correspondingly the expression  unfold e  e .  It is easy to check that rule  20.4b  is derivable from this deﬁnition. Moreover, we may check that  unroll self{τ} y.e    cid:20 −→∗  [self{τ} y.e  y]e.  This completes the derivation of the type self τ  of self-referential expressions of type τ. The self-referential type self τ  can be used to deﬁne general recursion for any type. We may deﬁne fix{τ} x.e  to stand for the expression  unroll self{τ} y.[unroll y  x]e     176  System FPC of Recursive Types  where the recursion at each occurrence of x is unrolled within e. It is easy to check that this veriﬁes the statics of general recursion given in Chapter 19. Moreover, it also validates the dynamics, as shown by the following derivation:  fix{τ} x.e  = unroll self{τ} y.[unroll y  x]e    [unroll self{τ} y.[unroll y  x]e   x]e   cid:20 −→∗ = [fix{τ} x.e  x]e.  It follows that recursive types can be used to deﬁne a non-terminating expression of every type, fix{τ} x.x .  20.4 The Origin of State  The concept of state in a computation—which will be discussed in Part XIV—has its origins in the concept of recursion, or self-reference, which, as we have just seen, arises from the concept of recursive types. For example, the concept of a ﬂip-ﬂop or a latch is a circuit built from combinational logic elements  typically, nor or nand gates  that have the characteristic that they maintain an alterable state over time. An RS latch, for example, maintains its output at the logical level of zero or one in response to a signal on the R or S inputs, respectively, after a brief settling delay. This behavior is achieved using feedback, which is just a form of self-reference, or recursion: the output of the gate feeds back into its input so as to convey the current state of the gate to the logic that determines its next state. We can implement an RS latch using recursive types. The idea is to use self-reference to model the passage of time, with the current output being computed from its input and its previous outputs. Speciﬁcally, an RS latch is a value of type τrsl given by  rec t is  cid:24 X  cid:9 → bool, Q  cid:9 → bool, N  cid:9 → t cid:25 .  The X and Q components of the latch represent its current outputs  of which Q represents the current state of the latch , and the N component represents the next state of the latch. If e is of type τrsl, then we deﬁne e @ X to mean unfold e  · X, and deﬁne e @ Q and e @ N similarly. The expressions e @ X and e @ Q evaluate to the boolean outputs of the latch e, and e @ N evaluates to another latch representing its evolution over time based on these inputs.  For given values r and s, a new latch is computed from an old latch by the recursive  function rsl deﬁned as follows:2  fix rsl is λ  l : τrsl  ersl,  where ersl is the expression  fix this is fold  cid:24 X  cid:9 → enor  cid:24 s, l @ Q cid:25  , Q  cid:9 → enor  cid:24 r, l @ X cid:25  , N  cid:9 → rsl this  cid:25  ,  where enor is the obvious binary function on booleans. The outputs of the latch are computed in terms of the r and s inputs and the outputs of the previous state of the latch. To get the   177  Exercises  construction started, we deﬁne an initial state of the latch in which the outputs are arbitrarily set to false, and whose next state is determined by applying the recursive function rsl to that state:  fix this is fold  cid:24 X  cid:9 → false, Q  cid:9 → false, N  cid:9 → rsl this  cid:25  .  Selection of the N component causes the outputs to be recalculated based on their current values. Notice the role of self-reference in maintaining the state of the latch.  The systematic study of recursive types in programming was initiated by Scott  1976, 1982  to give a mathematical model of the untyped λ-calculus. The derivation of recursion from recursive types is an application of Scott’s theory. The category-theoretic view of recursive types was developed by Wand  1979  and Smyth and Plotkin  1982 . Implementing state using self-reference is fundamental to digital logic  Ward and Halstead, 1990 . The example given in Section 20.4 is inspired by Cook  2009  and Abadi and Cardelli  1996 . The account of signals as streams  explored in the exercises  is inspired by the pioneering work of Kahn  MacQueen, 2009 . The language name FPC is taken from Gunter  1992 .  20.5 Notes  Exercises  20.1. Show that the recursive type D  cid:2  rec t is t  cid:19  t is non-trivial by interpreting the sk-combinators deﬁned in Exercise 3.1 into it. Speciﬁcally, deﬁne elements k : D and s : D and a  left-associative  “application” function x : D y : D  cid:12  x · y : D  such that  a  k · x · y  cid:20 −→∗  b  s · x · y · z  cid:20 −→∗  x · z  ·  y · z .  x;  20.2. Recursive types admit the structure of both inductive and coinductive types. Consider  cid:5  and the associated inductive and coinductive types the recursive type τ  cid:2  rec t is τ  cid:5  . Complete the following chart consistently with the statics of μ t.τ inductive and coinductive types on the left-hand side and with the statics of recursive types on the right:   cid:5   and ν t.τ  foldt.t opt e   cid:2  fold e  rec x.e  ; e   cid:2  ?   cid:5   unfoldt.t opt e   cid:2  unfold e   gen x.e   cid:5   ; e   cid:2  ?   178  System FPC of Recursive Types  Check that the statics is derivable under these deﬁnitions. Hint: you will need to use general recursion on the right to ﬁll in the missing cases. You may also ﬁnd it useful to use generic programming.  Now consider the dynamics of these deﬁnitions, under both an eager and a lazy  interpretation. What happens in each case?  20.3. Deﬁne the type signal of signals to be the coinductive type of inﬁnite streams of booleans  bits . Deﬁne a signal transducer to be a function of type signal  cid:19  signal. Combinational logic gates, such as the nor gate, can be deﬁned as signal transducers. Give a coinductive deﬁnition of the type signal, and deﬁne nor as a signal transducer. Be sure to take account of the underlying dynamics of PCF.  The passage from combinational to digital logic  circuit elements that maintain state  relies on self-reference. For example, an RS latch can be built from nor two nor gates in this way. Deﬁne an RS latch using general recursion and two of the nor gates just deﬁned.  20.4. The type τrsl given in Section 20.4 above is the type of streams of pairs of booleans. Give another formulation of an RS latch as a value of type τrsl, but this time using the coinductive interpretation of the recursive type proposed in Exercise 20.2  using the lazy dynamics for FPC . Expand and simplify this deﬁnition using your solution to Exercise 20.2, and compare it with the formulation given in Section 20.4. Hint: the internal state of the stream is a pair of booleans corresponding to the X and Q outputs of the latch.  Notes  1 The “underscore” stands for a variable that does not occur free in e0. 2 For convenience, we assume that fold is evaluated lazily.   P A R T IX  Dynamic Types    21  The Untyped λ-Calculus  In this chapter, we study the premier example of a uni-typed programming language, the  untyped  λ-calculus. This formalism was introduced by Church in the 1930s as a universal language of computable functions. It is distinctive for its austere elegance. The λ-calculus has but one “feature,” the higher-order function. Everything is a function, hence every expression may be applied to an argument, which must itself be a function, with the result also being a function. To borrow a turn of phrase, in the λ-calculus it’s functions all the way down.  21.1 The λ-Calculus  The abstract syntax of the untyped λ-calculus, called  cid:2 , is given by the following grammar:  Exp u ::= x  λ x.u  ap u1; u2  u1 u2   variable  x λ  x  u λ-abstraction  application  The statics of  cid:2  is deﬁned by general hypothetical the form x1 ok, . . . , xn ok  cid:12  u ok, stating that u is a well-formed expression involving the vari- ables x1, . . . , xn.  As usual, we omit explicit mention of the variables when they can be determined from the form of the hypotheses.  This relation is inductively deﬁned by the following rules:  judgments of   cid:7 , x ok  cid:12  x ok   cid:7   cid:12  u1 ok  cid:7   cid:12  u2 ok   cid:7   cid:12  u1 u2  ok  cid:7 , x ok  cid:12  u ok  cid:7   cid:12  λ  x  u ok equality for  cid:2  is a judgment of the form  cid:7   cid:12  u ≡ u n ≥ 0, and u and u deﬁned by the following rules:  The dynamics of  cid:2  is given equationally, rather than via a transition system. Deﬁnitional  cid:5 , where  cid:7  = x1 ok, . . . , xn ok for some  cid:5  are terms having at most the variables x1, . . . , xn free. It is inductively   21.1a    21.1b    21.1c    21.2a    cid:7 , u ok  cid:12  u ≡ u   182  The Untyped λ-Calculus   21.2b    21.2c    21.2d    21.2e    cid:7   cid:12  u ≡ u  cid:5   cid:7   cid:12  u  cid:5  ≡ u  cid:7   cid:12  u  cid:7   cid:12  u ≡ u  cid:5   cid:7   cid:12  u ≡ u  cid:5  cid:5    cid:5  ≡ u  cid:5  cid:5    cid:7   cid:12  u1 ≡ u 1  cid:7   cid:12  u2 ≡ u  cid:5   cid:5  2  cid:5  2    cid:7   cid:12  u1 u2  ≡ u  cid:5  1 u  cid:7 , x ok  cid:12  u ≡ u  cid:5   cid:7   cid:12  λ  x  u ≡ λ  x  u   cid:5   21.2 Deﬁnability   cid:7 , x ok  cid:12  u2 ok  cid:7   cid:12  u1 ok  cid:7   cid:12   λ  x  u2  u1  ≡ [u1 x]u2   21.2f   cid:5  when the variables involved need not be emphasized or are clear  We often write just u ≡ u from context.  Interest in the untyped λ-calculus stems from its surprising expressiveness. It is a Turing- complete language in the sense that it has the same capability to express computations on the natural numbers as does any other known programming language. Church’s Law states that any conceivable notion of computable function on the natural numbers is equivalent to the λ-calculus. This assertion is true for all known means of deﬁning computable functions on the natural numbers. The force of Church’s Law is that it postulates that all future notions of computation will be equivalent in expressive power  measured by deﬁnability of functions on the natural numbers  to the λ-calculus. Church’s Law is therefore a scientiﬁc law in the same sense as, say, Newton’s Law of Universal Gravitation, which predicts the outcome of all future measurements of the acceleration in a gravitational ﬁeld.1  We will sketch a proof that the untyped λ-calculus is as powerful as the language PCF described in Chapter 19. The main idea is to show that the PCF primitives for manipulating the natural numbers are deﬁnable in the untyped λ-calculus. In particular, we must show that the natural numbers are deﬁnable as λ-terms in such a way that case analysis, which discriminates between zero and non-zero numbers, is deﬁnable. The principal difﬁculty is with computing the predecessor of a number, which requires a bit of cleverness. Finally, we show how to represent general recursion, completing the proof.  The ﬁrst task is to represent the natural numbers as certain λ-terms, called the Church  numerals.  It follows that  0  cid:2  λ  b  λ  s  b  n + 1  cid:2  λ  b  λ  s  s n b  s    n u1  u2  ≡ u2 . . .  u2 u1   ,   21.3a   21.3b    183  21.2 Deﬁnability  the n-fold application of u2 to u1. That is, n iterates its second argument  the induction step  n times, starting with its ﬁrst argument  the basis .  Using this deﬁnition, it is not difﬁcult to deﬁne the basic functions of arithmetic. For example, successor, addition, and multiplication are deﬁned by the following untyped λ-terms:  succ  cid:2  λ  x  λ  b  λ  s  s x b  s   plus  cid:2  λ  x  λ  y  y x  succ  times  cid:2  λ  x  λ  y  y 0  plus x     21.4   21.5   21.6  It is easy to check that succ n  ≡ n + 1, and that similar correctness conditions hold for the representations of addition and multiplication. To deﬁne ifz{u0; x.u1} u  requires a bit of ingenuity. The key is to deﬁne the “cut-off predecessor,” pred, such that  pred 0  ≡ 0 pred n + 1  ≡ n.   21.7   21.8   To compute the predecessor using Church numerals, we must show how to compute the result for n + 1 in terms of its value for n. At ﬁrst glance, this seems simple—just take the successor—until we consider the base case, in which we deﬁne the predecessor of 0 to be 0. This formulation invalidates the obvious strategy of taking successors at inductive steps, and necessitates some other approach.  What to do? A useful intuition is to think of the computation in terms of a pair of “shift registers” satisfying the invariant that on the nth iteration the registers contain the predecessor of n and n itself, respectively. Given the result for n, namely the pair  n− 1, n , we pass to the result for n + 1 by shifting left and incrementing to obtain  n, n + 1 . For the base case, we initialize the registers with  0, 0 , reﬂecting the stipulation that the predecessor of zero be zero. To compute the predecessor of n, we compute the pair  n−1, n  by this method, and return the ﬁrst component.  To make this precise, we must ﬁrst deﬁne a Church-style representation of ordered pairs.   cid:24 u1, u2 cid:25   cid:2  λ  f   f  u1  u2  u · l  cid:2  u λ  x  λ  y  x  u · r  cid:2  u λ  x  λ  y  y    21.9   21.10   21.11  It is easy to check that under this encoding  cid:24 u1, u2 cid:25  ·l ≡ u1, and that a similar equivalence holds for the second projection. We may now deﬁne the required representation, up, of the predecessor function:   cid:5  u p up  cid:2  λ  x  u   cid:2  λ  x  x  cid:24 0, 0 cid:25   λ  y  cid:24 y · r, succ  y · r  cid:25     21.12   21.13  It is easy to check that this gives us the required behavior. Finally, deﬁne ifz{u0; x.u1} u  to be the untyped term  p x  · l  cid:5   u u0  λ     [up u  x]u1 .   184  The Untyped λ-Calculus  This deﬁnition gives us all the apparatus of PCF, apart from general recursion. But general recursion is also deﬁnable in  cid:2  using a ﬁxed point combinator. There are many choices of ﬁxed point combinator, of which the best known is the Y combinator:  Y  cid:2  λ  F    λ  f   F  f  f     λ  f   F  f  f    .  It is easy to check that  Y F   ≡ F  Y F   .  Using the Y combinator, we may deﬁne general recursion by writing Y λ  x  u , where x stands for the recursive expression itself.  Although it is clear that Y as just deﬁned computes a ﬁxed point of its argument, it is probably less clear why it works or how we might have invented it in the ﬁrst place. The main idea is quite simple. If a function is recursive, it is given an extra ﬁrst argument, which is arranged at call sites to be the function itself. Whenever we wish to call a self-referential function with an argument, we apply the function ﬁrst to itself and then to its argument; this protocol is imposed on both the “external” calls to the function and on the “internal” calls that the function may make to itself. For this reason, the ﬁrst argument is often called this or self, to remind you that it will be, by convention, bound to the function itself.  With this in mind, it is easy to see how to derive the deﬁnition of Y. If F is the function  cid:5  = λ  f   F  f  f    is a variant of F in whose ﬁxed point we seek, then the function F which the self-application convention has been imposed internally by substituting for each  cid:5   ≡ F  F  cid:5   , so occurrence of f in F  f   the self-application f  f  . Now check that F  cid:5 , we have derived that F that the desired ﬁxed point of F is   cid:5  F  cid:5   is the desired ﬁxed point of F . Expanding the deﬁnition of F   cid:5  F   cid:5  F  λ  f   F  f  f    λ  f   F  f  f    .  To ﬁnish the derivation, we need only note that nothing depends on the particular choice of F , which means that we can compute a ﬁxed point for F uniformly in F . That is, we may deﬁne a single function, the term Y as deﬁned above, that computes the ﬁxed point of any F .  21.3 Scott’s Theorem  Scott’s Theorem states that deﬁnitional equality for the untyped λ-calculus is undecidable: there is no algorithm to determine whether two untyped terms are deﬁnitionally equal. The proof uses the concept of inseparability. Any two properties, A0 and A1, ofλ-terms are inseparable if there is no decidable property, B, such that A0 u implies that B u holds, and A1 u implies that B u does not hold. We say that a property, A, of untyped terms is behavioral iff whenever u ≡ u   cid:5 , then A u iff A u  cid:5 .   185  21.3 Scott’s Theorem  The proof of Scott’s Theorem decomposes into two parts:  1. For any untyped λ-term u, we may ﬁnd an untyped term v such that u  cid:4 v cid:5   ≡ v, where  cid:4 v cid:5  is the G¨odel number of v, and  cid:4 v cid:5  is its representation as a Church numeral.  See Chapter 9 for a discussion of G¨odel-numbering.  2. Any two non-trivial2 behavioral properties A0 and A1 of untyped terms are inseparable. Lemma 21.1. For any u, there exists v such that u  cid:4 v cid:5   ≡ v.  Proof Sketch The proof relies on the deﬁnability of the following two operations in the untyped λ-calculus: 1. ap  cid:4 u1 cid:5    cid:4 u2 cid:5   ≡  cid:4 u1 u2  cid:5 . 2. nm n  ≡  cid:4 n cid:5 .  Intuitively, the ﬁrst takes the representations of two untyped terms and builds the represen- tation of the application of one to the other. The second takes a numeral for n, and yields the representation of the Church numeral n. Given these, we may ﬁnd the required term v by deﬁning v  cid:2  w  cid:4 w cid:5  , where w  cid:2  λ  x  u ap x  nm x   . We have  v = w  cid:4 w cid:5   ≡ u ap  cid:4 w cid:5   nm  cid:4 w cid:5     ≡ u  cid:4 w  cid:4 w cid:5   cid:5   ≡ u  cid:4 v cid:5  .  The deﬁnition is very similar to that of Y u , except that u takes as input the representation of a term, and we ﬁnd a v such that, when applied to the representation of v, the term u yields v itself. Lemma 21.2. Suppose that A0 and A1 are two non-trivial behavioral properties of untyped terms. Then there is no untyped term w such that 1. For every u, either w  cid:4 u cid:5   ≡ 0 or w  cid:4 u cid:5   ≡ 1. 2. If A0 u, then w  cid:4 u cid:5   ≡ 0. 3. If A1 u, then w  cid:4 u cid:5   ≡ 1.  Proof Suppose there is such an untyped term w. Let v be the untyped term  λ  x  ifz{u1; .u0} w x  ,  where u0 and u1 are chosen such that A0 u0 and A1 u1.  Such a choice must exist by non- triviality of the properties.  By Lemma 21.1 there is an untyped term t such that v  cid:4 t cid:5   ≡ t. If w  cid:4 t cid:5   ≡ 0, then t ≡ v  cid:4 t cid:5   ≡ u1, and so A1 t, because A1 is behavioral and A1 u1.   186  The Untyped λ-Calculus  But then w  cid:4 t cid:5   ≡ 1 by the deﬁning properties of w, which is a contradiction. Similarly, if w  cid:4 t cid:5   ≡ 1, then A0 t, and hence w  cid:4 t cid:5   ≡ 0, again a contradiction.  Corollary 21.3. There is no algorithm to decide whether u ≡ u Proof For ﬁxed u, the property Eu u of untyped terms. So it is inseparable from its negation, and hence is undecidable.   cid:5  ≡ u is a non-trivial behavioral property   cid:5  deﬁned by u   cid:5 .  21.4 Untyped Means Uni-Typed  The untyped λ-calculus can be faithfully embedded in a typed language with recursive types. Thus, every untyped λ-term has a representation as a typed expression in such a way that execution of the representation of a λ-term corresponds to execution of the term itself. This embedding is not a matter of writing an interpreter for the λ-calculus in FPC, but rather a direct representation of untyped λ-terms as typed expressions in a language with recursive types.  The key observation is that the untyped λ-calculus is really the uni-typed λ-calculus. It is not the absence of types that gives it its power, but rather that it has only one type, the recursive type  D  cid:2  rec t is t  cid:19  t.  A value of type D is of the form fold e  where e is a value of type D  cid:19  D—a function whose domain and range are both D. Any such function can be regarded as a value of type D by “folding”, and any value of type D can be turned into a function by “unfolding”. As usual, a recursive type is a solution to a type equation, which in the present case is the equation  ∼= D  cid:19  D.  D  This isomorphism speciﬁes that D is a type that is isomorphic to the space of partial functions on D itself, which is impossible if types are just sets.  This isomorphism leads to the following translation, of  cid:2  into FPC:  x λ  x  u u1 u2   †  cid:2  x †  cid:2  fold λ  x : D  u † † †  cid:2  unfold u 2   † 1  u      21.14a   21.14b   21.14c   Note that the embedding of a λ-abstraction is a value, and that the embedding of an application exposes the function being applied by unfolding the recursive type. And so   187  we have  Exercises  λ  x  u1 u2   † 1   u  † 2   † = unfold fold λ  x : D  u ≡ λ  x : D  u † 1 u ≡ [u † 1 =  [u2 x]u1   † 2 x]u  † 2   †  .  The last step, stating that the embedding commutes with substitution, is proved by induction on the structure of u1. Thus β-reduction is implemented by evaluation of the embedded terms.  Thus, we see that the canonical untyped language,  cid:2 , which by dint of terminology stands in opposition to typed languages, turns out to be but a typed language after all. Rather than eliminating types, an untyped language consolidates an inﬁnite collection of types into a single recursive type. Doing so renders static type checking trivial, at the cost of incurring dynamic overhead to coerce values to and from the recursive type. In Chapter 22, we will take this a step further by admitting many different types of data values  not just functions , each of which is a component of a “master” recursive type. This generalization shows that so-called dynamically typed languages are, in fact, statically typed. Thus, this traditional distinction cannot be considered an opposition, because dynamic languages are but particular forms of static languages in which undue emphasis is placed on a single recursive type.  The untyped λ-calculus was introduced by Church  1941  as a formalization of the informal concept of a computable function. Unlike the well-known machine models, such as the Turing machine or the random access machine, the λ-calculus codiﬁes mathematical and programming practice. Barendregt  1984  is the deﬁnitive reference for all aspects of the untyped λ-calculus; the proof of Scott’s theorem is adapted from Barendregt’s account. Scott  1980a  gave the ﬁrst model of the untyped λ-calculus in terms of an elegant theory of recursive types. This construction underlies Scott’s apt description of the λ-calculus as “uni-typed,” rather than “untyped.” The idea to characterize Church’s Law as such was communicated to the author, independently of each other, by Robert L. Constable and Mark Lillibridge.  21.5 Notes  Exercises  21.1. Deﬁne an encoding of ﬁnite products as deﬁned in Chapter 10 in  cid:2 . 21.2. Deﬁne the factorial function in  cid:2  two ways, one without using Y, and one using Y.  In both cases, show that your solution, u, has the property that u n  ≡ n!.   188  The Untyped λ-Calculus  21.3. Deﬁne the “Church booleans” in  cid:2  by deﬁning terms true and false such that   a  true u1  u2  ≡ u1.  b  false u1  u2  ≡ u2. What is the encoding of if u then u1 else u2?  21.4. Deﬁne an encoding of ﬁnite sums as deﬁned in Chapter 11 in  cid:2 . 21.5. Deﬁne an encoding of ﬁnite lists of natural numbers as deﬁned in Chapter 15 in  cid:2 . 21.6. Deﬁne an encoding of the inﬁnite streams of natural numbers as deﬁned in Chapter 15  in  cid:2 .  21.7. Show that  cid:2  can be “compiled” to sk-combinators using bracket abstraction  see ∗ from  cid:2  into sk combinators such that  Exercises 3.4 and 3.5. Deﬁne a translation u  ∗ by induction on the structure of u, using the compositional form Hint: Deﬁne u of bracket abstraction considered in Exercise 3.5. Show that the translation is itself compositional in that it commutes with substitution:  if u1 ≡ u2, then u ∗ 1  ≡ u ∗ 2.   [u2 x]u1   ∗ = [u  ∗  ∗ 2 x]u  .  Then proceed by rule induction on rules  21.2  to show the required correctness condition.  Notes  1 It is debatable whether there are any scientiﬁc laws in Computer Science. In the opinion of the author Church’s Law, which is usually called Church’s Thesis, is a strong candidate for being a scientiﬁc law.  2 A property of untyped terms is trivial if it either holds for all untyped terms or never holds for any  untyped term.   22  Dynamic Typing  We saw in Chapter 21 that an untyped language is a uni-typed language in which “untyped” terms are just terms of single recursive type. Because all expressions of  cid:2  are well-typed, type safety ensures that no misinterpretation of a value is possible. When spelled out for  cid:2 , type safety follows from there being exactly one class of values, that of functions on values. No application can get stuck, because every value is a function that may be applied to an argument.  This safety property breaks down once more than one class of value is admitted. For example, if the natural numbers are added as a primitive to  cid:2 , then it is possible to incur a run-time error by attempting to apply a number to an argument. One way to manage this is to embrace the possibility, treating class mismatches as checked errors, and weakening the progress theorem as outlined in Chapter 6. Such languages are called dynamic languages because an error such as the one described is postponed to run-time, rather than precluded at compile time by type checking. Languages of the latter sort are called static languages. Dynamic languages are often considered in opposition to static languages, but the op- position is illusory. Just as the untyped λ-calculus is uni-typed, so dynamic languages are but special cases of static languages in which there is only one recursive type  albeit with multiple classes of value .  22.1 Dynamically Typed PCF  To illustrate dynamic typing, we formulate a dynamically typed version of PCF, called DPCF. The abstract syntax of DPCF is given by the following grammar:  Exp d  ::= x  num[n] zero succ d  ifz{d0; x.d1} d  ifz d {zero  cid:9 → d0  succ x   cid:9 → d1}  x n zero succ d   variable numeral zero successor  fun x.d  ap d1; d2  fix x.d   λ  x  d d1 d2  fix x is d  zero test abstraction application recursion   190  Dynamic Typing  There are two classes of values in DPCF, the numbers, which have the form num[n], and the functions, which have the form fun x.d . The expressions zero and succ d  are not themselves values, but rather are constructors that evaluate to values. General recursion is deﬁnable using a ﬁxed point combinator but is taken as primitive here to simplify the analysis of the dynamics in Section 22.3.  As usual, the abstract syntax of DPCF is what matters, but we use the concrete syntax to improve readability. However, notational conveniences can obscure important details, such as the tagging of values with their class and the checking of these tags at run-time. For example, the concrete syntax for a number, n, suggests a “bare” representation, the abstract syntax reveals that the number is labeled with the class num to distinguish it from a function. Correspondingly, the concrete syntax for a function is λ  x  d, but its abstract syntax, fun x.d , shows that it also sports a class label. The class labels are required to ensure safety by run-time checking, and must not be overlooked when comparing static with dynamic languages.  The statics of DPCF is like that of  cid:2 ; it merely checks that there are no free variables in  the expression. The judgment  x1 ok, . . . xn ok  cid:12  d ok  states that d is a well-formed expression with free variables among those in the hypotheses. If the assumptions are empty, then we write just d ok to mean that d is a closed expression of DPCF.  The dynamics of DPCF must check for errors that would never arise in a language such as PCF. For example, evaluation of a function application must ensure that the value being applied is indeed a function, signaling an error if it is not. Similarly, the conditional branch must ensure that its principal argument is a number, signaling an error if it is not. To account for these possibilities, the dynamics is given by several judgment forms, as summarized in the following chart:   cid:5   d val d  cid:20 −→ d d err d is num n d isnt num d is fun x.d d isnt fun   cid:5   d is a  closed  value d evaluates in one step to d d incurs a run-time error d is of class num with value n d is not of class num d is of class fun with body x.d d is not of class fun  The last four judgment forms implement dynamic class checking. They are only relevant when d is already a value. The afﬁrmative class-checking judgments have a second argument that represents the underlying structure of a value; this argument is not itself an expression of DPCF.  The value judgment d val states that d is a evaluated  closed  expression:  num[n] val  fun x.d  val   22.1a    22.1b    191  22.1 Dynamically Typed PCF  The afﬁrmative class-checking judgments are deﬁned by the following rules:  The negative class-checking judgments are correspondingly deﬁned by these rules:  The transition judgment d  cid:20 −→ d  ously by the following rules:   22.3b  fun x.d  isnt num  cid:5  and the error judgment d err are deﬁned simultane-  num[n] is num n  fun x.d  is fun x.d  num[n] isnt fun  zero  cid:20 −→ num[z]  d  cid:20 −→ d   cid:5   succ d   cid:20 −→ succ d   cid:5    d err  succ d  err  d is num n  succ d   cid:20 −→ num[s n ]  d isnt num succ d  err d  cid:20 −→ d  cid:5   ifz{d0; x.d1} d   cid:20 −→ ifz{d0; x.d1} d   cid:5    d err  ifz{d0; x.d1} d  err  d is num 0  ifz{d0; x.d1} d   cid:20 −→ d0  d is num n + 1  ifz{d0; x.d1} d   cid:20 −→ [num[n] x]d1  d isnt num  ifz{d0; x.d1} d  err  d1  cid:20 −→ d   cid:5  1  ap d1; d2   cid:20 −→ ap d   cid:5  1; d2   d1 err  ap d1; d2  err  d1 is fun x.d  ap d1; d2   cid:20 −→ [d2 x]d   22.2a    22.2b    22.3a    22.4a    22.4b    22.4c    22.4d    22.4e    22.4f    22.4g    22.4h    22.4i    22.4j    22.4k    22.4l    22.4m    192  Dynamic Typing  d1 isnt fun ap d1; d2  err  fix x.d   cid:20 −→ [fix x.d  x]d   22.4n    22.4o   Rule  22.4i  labels the predecessor with the class num to maintain the invariant that variables are bound to expressions of DPCF.  Lemma 22.1  Class Checking . If d val, then  1. either d is num n for some n, ord isnt num; 2. either d is fun x.d   cid:5  for some x and d   cid:5 , or d isnt fun.  Proof By inspection of the rules deﬁning the class-checking judgments.   cid:5  such that   cid:5 .  Theorem 22.2  Progress . If d ok, then either d val, or d err, or there exists d d  cid:20 −→ d Proof By induction on the structure of d. For example, if d = succ d val, or d by induction either d have by rule  22.4b  that succ d by rule  22.4c  that succ d  cid:5  d The other cases are handled similarly.   cid:5  , then we have  cid:20 −→ d  cid:5  cid:5 . In the last case, we  cid:5  cid:5  , and in the second-to-last case, we have is num n or  cid:5   err.  val, then by Lemma 22.1, either d  cid:5    cid:20 −→ num[s n ], and in the latter succ d  isnt num. In the former case succ d   cid:5  err, or d  cid:5    cid:20 −→ succ d   cid:5  cid:5  for some d   cid:5   err. Ifd   cid:5    cid:5    cid:5    cid:5   Lemma 22.3  Exclusivity . For any d in DPCF, exactly one of the following holds: d val, or d err, or d  cid:20 −→ d   cid:5  for some d   cid:5 .  Proof By induction on the structure of d, making reference to rules  22.4 .  22.2 Variations and Extensions  The dynamic language DPCF deﬁned in Section 22.1 parallels the static language PCF deﬁned in Chapter 19. One discrepancy, however, is in the treatment of natural numbers. Whereas in PCF the zero and successor operations are introduction forms for the type nat, in DPCF they are elimination forms that act on specially deﬁned numerals. The present formulation uses only a single class of numbers.  One could instead treat zero and succ d  as values of separate classes and introduce the obvious class-checking judgments for them. When written in this style, the dynamics of the conditional branch is given as follows:  d  cid:20 −→ d   cid:5   ifz{d0; x.d1} d   cid:20 −→ ifz{d0; x.d1} d   cid:5     22.5a    193  22.2 Variations and Extensions  d is zero  ifz{d0; x.d1} d   cid:20 −→ d0  d is succ d  ifz{d0; x.d1} d   cid:20 −→ [d   cid:5    x]d1   cid:5   d isnt zero  ifz{d0; x.d1} d  err  d isnt succ   22.5b    22.5c    22.5d   Notice that the predecessor of a value of the successor class need not be a number, whereas in the previous formulation this possibility does not arise.  DPCF can be extended with structured data similarly. A classic example is to consider  a classnil , consisting of a “null” value, and a class cons, consisting of pairs of values.  Exp d  ::= nil  cons d1; d2  ifnil d; d0; x, y.d1  ifnil d {nil  cid:9 → d0  cons x; y   cid:9 → d1}  null nil cons d1; d2  pair  conditional  The expression ifnil d; d0; x, y.d1  distinguishes the null value from a pair, and signals an error on any other class of value.  Lists  ﬁnite sequences  can be encoded using null and pairing. For example, the list  consisting of three zeros can berepresented by the value  cons zero; cons zero; cons zero; nil   .  But what to make of the following value?  cons zero; cons zero; cons zero; λ  x  x     It is not a list, because it does not end with nil, but it is a permissible value in the enriched language.  A difﬁculty with encoding lists using null and pair emerges when deﬁning functions on them. For example, here is a deﬁnition of the function append that concatenates two lists:  fix a is λ  x  λ  y  ifnil x; y; x1, x2.cons x1; a x2  y     Nothing prevents us from applying this function to any two values, regardless of whether they are lists. If the ﬁrst argument is not a list, then execution aborts with an error. But because the function does not traverse its second argument, it can be any value at all. For example, we may apply append with a list and a function to obtain the “list” that ends with a λ given above.  It might be argued that the conditional branch that distinguishes null from a pair is inappropriate in DPCF, because there are more than just these two classes in the language. One approach that avoids this criticism is to abandon pattern matching on the class of data, replacing it by a general conditional branch that distinguishes null from all other values, and adding to the language predicates1 that test the class of a value and destructors that invert the constructors of each class.   194  Dynamic Typing  We could instead reformulate null and and pairing as follows:  Exp d  ::= cond d; d0; d1  cond d; d0; d1   nil? d  cons? d  car d  cdr d   nil? d  cons? d  car d  cdr d   conditional nil test pair test ﬁrst projection second projection  The conditional cond d; d0; d1  distinguishes d between nil and all other values. If d is not nil, the conditional evaluates to d0, and otherwise evaluates to d1. In other words, the value nil represents boolean falsehood, and all other values represent boolean truth. The predicates nil? d  and cons? d  test the class of their argument, yielding nil if the argument is not of the speciﬁed class, and yielding some non-nil if so. The destructors car d  and cdr d  decompose cons d1; d2  into d1 and d2, respectively.2  Written in this form, the function append is given by the expression  fix a is λ  x  λ  y  cond x; cons car x ; a cdr x   y  ; y .  The behavior of this formulation of append is no different from the earlier one; the only difference is that instead of dispatching on whether a value is either null or a pair, we instead allow discrimination on any predicate of the value, which includes such checks as special cases.  An alternative, which is not widely used, is to enhance, and not restrict, the conditional branch so that it includes cases for each possible class of value in the language. So in a language with numbers, functions, null, and pairing, the conditional would have four branches. The fourth branch, for pairing, would deconstruct the pair into its constituent parts. The difﬁculty with this approach is that in realistic languages there are many classes of data, and such a conditional would be rather unwieldy. Moreover, even once we have dispatched on the class of a value, it is nevertheless necessary for the primitive operations associated with that class to admit run-time checks. For example, we may determine that a value d is of the numeric class, but there is no way to propagate this information into the branch of the conditional that then adds d to some other number. The addition operation must still check the class of d, recover the underlying number, and create a new value of numeric class. It is an inherent limitation of dynamic languages that they do not allow values other than classiﬁed values.  22.3 Critique of Dynamic Typing  The safety theorem for DPCF is an advantage of dynamic over static typing. Unlike static languages, which rule out some candidate programs as ill-typed, every piece of abstract syntax in DPCF is well-formed, and hence, by Theorem 22.2, has a well-deﬁned dynamics  albeit one with checked errors . But this convenience is also a disadvantage, because errors that could be ruled out at compile time by type checking are not signaled until run-time.   195  22.4 Notes  Consider, for example, the addition function in DPCF, whose speciﬁcation is that, when  passed two values of class num, returns their sum, which is also of class num:3  fun x.fix p.fun y.ifz{x; y   cid:5   .succ p y   cid:5     }   .  The addition function may, deceptively, be written in concrete syntax as follows:  λ  x  fix p is λ  y  ifz y {zero  cid:9 → x  succ y   cid:5      cid:9 → succ p y   cid:5     }.  It is deceptive, because it obscures the class tags on values, and the operations that check the validity of those tags. Let us now examine the costs of these operations in a bit more detail.  First, note that the body of the ﬁxed point expression is labeled with class fun. The dynamics of the ﬁxed point construct binds p to this function. Consequently, the dynamic class check incurred by the application of p in the recursive call is guaranteed to succeed. But DPCF offers no means of suppressing the redundant check, because it cannot express the invariant that p is always bound to a value of class fun.  Second, note that the result of applying the inner λ-abstraction is either x, the argument of the outer λ-abstraction, or the successor of a recursive call to the function itself. The successor operation checks that its argument is of class num, even though this condition is guaranteed to hold for all but the base case, which returns the given x, which can be of any class at all. In principle, we can check that x is of class num once, and note that it is otherwise a loop invariant that the result of applying the inner function is of this class. However, DPCF gives us no way to express this invariant; the repeated, redundant tag checks imposed by the successor operation cannot be avoided.  Third, the argument y to the inner function is either the original argument to the addition function, or is the predecessor of some earlier recursive call. But as long as the original call is to a value of class num, then the dynamics of the conditional will ensure that all recursive calls have this class. And again there is no way to express this invariant in DPCF, and hence, there is no way to avoid the class check imposed by the conditional branch.  Classiﬁcation is not free—storage is required for the class label, and it takes time to detach the class from a value each time it is used and to attach a class to a value when it is created. Although the overhead of classiﬁcation is not asymptotically signiﬁcant  it slows down the program only by a constant factor , it is nevertheless non-negligible, and should be eliminated when possible. But this is impossible within DPCF, because it cannot enforce the restrictions required to express the required invariants. For that we need a static type system.  22.4 Notes  The earliest dynamically typed language is Lisp  McCarthy, 1965 , which continues to inﬂuence language design a half century after its invention. Dynamic PCF is the core of Lisp, but with a proper treatment of variable binding, correcting what McCarthy himself   196  Dynamic Typing  has described as an error in the original design. Informal discussions of dynamic lan- guages are often complicated by the elision of the dynamic checks that are made explicit here. Although the surface syntax of dynamic PCF is almost the same as that for PCF, minus the type annotations, the underlying dynamics is different. It is for this reason that static PCF cannot be seen as a restriction of dynamic PCF by the imposition of a type system.  Exercises  22.1. Surface syntax can be deceiving; even simple arithmetic expressions do not have the same meaning in DPCF that they do in PCF. To see why, deﬁne the addition function, plus, in DPCF, and examine the dynamics of evaluating expressions such as plus 5  7 . Even though this expression might be written as “5+ 7” in both static and dynamic languages, they have different meanings.  22.2. Give a precise dynamics to the data structuring primitives described informally in Section 22.2. What class restrictions should cons impose on its arguments? Check the dynamics of the append function when called with two lists as arguments.  22.3. To avoid the difﬁculties with the representation of lists using cons and nil, introduce a class of lists that are constructed using revised versions of nil and cons that operate on the class of lists. Revisit the dynamics of the append function when redeﬁned using the class of lists.  22.4. Allowing multiple arguments to, and multiple results from, functions is a notorious source of trouble in dynamic languages. The restriction to a single type makes it impossible even to distinguish n things from m things, let alone express more subtle properties of a program. Numerous workarounds have been proposed. Explore the problem yourself by enriching DPCF with multi-argument and multi-result functions. Be sure to consider these questions:  a  If a function is deﬁned with n parameters, what should happen if it is called with  more or fewer than n arguments?   b  What happens if one were to admit functions with a varying number of arguments? How would you refer to these arguments within the body of such a function? How does this relate to pattern matching?   c  What if one wished to admit keyword parameter passing by giving names to the arguments, and allowing them to be passed in any order by associating them with their names?   d  What notation would you suggest for functions returning multiple results? For example, a division function might return the quotient and the remainder. How might one notate this in the function body? How would a caller access the results individually or collectively?   e  How would one deﬁne the composition of two functions when either or both can  take multiple arguments or return multiple results?   197  Notes  Notes  1 Predicates evaluate to the null value to mean that a condition is false, and some non-null value to  mean that it is true.  2 The terminology for the projections is archaic, but well-established. It is said that car originally stood for “contents of the address register” and that cdr stood for “contents of the data register,” referring to the details of the original implementation of Lisp.  3 This speciﬁcation imposes no restrictions on the behavior of addition on arguments that are not classiﬁed as numbers, but we could make the further demand that the function abort when applied to arguments that are not classiﬁed by num.   23  Hybrid Typing  A hybrid language is one that combines static and dynamic typing by enriching a statically typed language with a distinguished type dyn of dynamic values. The dynamically typed language considered in Chapter 22 can be embedded into the hybrid language by viewing a dynamically typed program as a statically typed program of type dyn. Static and dynamic types are not opposed to one another, but may coexist harmoniously. The ad hoc device of adding the type dyn to a static language is unnecessary in a language with recursive types, wherein it is deﬁnable as a particular recursive type. Thus, one may say that dynamic typing is a mode of use of static typing, reconciling an apparent opposition between them.  Consider the language HPCF, which extends PCF with the following constructs:  23.1 A Hybrid Language  Typ τ Exp e  Cls  l  ::= dyn ::= new[l] e  cast[l] e  inst[l] e   ::= num fun  dyn l ! e e @ l l ? e num fun  dynamic construct destruct discriminate number function  The type dyn is the type of dynamically classiﬁed values. The constructor attaches a classiﬁer to a value of a type associated to that classifer, the destructor recovers the value classiﬁed with the given classiﬁer, and the discriminator tests the class of a classiﬁed value.  The statics of HPCF extends that of PCF with the following rules:   cid:7   cid:12  e : nat   cid:7   cid:12  new[num] e  : dyn  cid:7   cid:12  e : dyn  cid:19  dyn  cid:7   cid:12  new[fun] e  : dyn   cid:7   cid:12  e : dyn   cid:7   cid:12  cast[num] e  : nat   cid:7   cid:12  e : dyn   cid:7   cid:12  cast[fun] e  : dyn  cid:19  dyn   23.1a    23.1b    23.1c    23.1d    199  23.1 A Hybrid Language  The statics ensures that classiﬁers are attached to values of the right type, namely natural numbers for num, and functions on classiﬁed values for fun.  The dynamics of HPCF extends that of PCF with the following rules:   cid:7   cid:12  e : dyn   cid:7   cid:12  inst[num] e  : bool   cid:7   cid:12  e : dyn   cid:7   cid:12  inst[fun] e  : bool  e val  new[l] e  val e  cid:20 −→ e   cid:5   new[l] e   cid:20 −→ new[l] e   cid:5    e  cid:20 −→ e   cid:5   cast[l] e   cid:20 −→ cast[l] e   cid:5    new[l] e  val  cast[l] new[l] e    cid:20 −→ e l  cid:6 = l  cid:5   cid:5 ] e   err  cid:5    cid:5 ] e  val new[l cast[l] new[l e  cid:20 −→ e  inst[l] e   cid:20 −→ inst[l] e   cid:5    new[l] e  val  inst[l] new[l] e    cid:20 −→ true  new[l] e  val  cid:5 ] new[l] e    cid:20 −→ false  inst[l  l  cid:6 = l   cid:5    23.1e    23.1f    23.2a    23.2b    23.2c    23.2d    23.2e    23.2f    23.2g    23.2h   Casting compares the class of the object to the required class, returning the underlying object if these coincide, and signaling an error otherwise.1 Lemma 23.1  Canonical Forms . If e : dyn and e val, then e = new[l] e and some e   cid:5  : nat, and if l = fun, then e  val. Ifl = num, then e   cid:5   for some class l   cid:5  : dyn  cid:19  dyn.   cid:5   Proof By rule induction on the statics of HPCF.  Theorem 23.2  Safety . The language HPCF is safe: 1. If e : τ and e  cid:20 −→ e 2. If e : τ , then either e val, or e err, or e  cid:20 −→ e   cid:5 , then e   cid:5  : τ .   cid:5  for some e   cid:5 .   200  Hybrid Typing  Proof Preservation is proved by rule induction on the dynamics, and progress is proved by rule induction on the statics, making use of the canonical forms lemma. The opportunities for run-time errors are the same as those for DPCF—a well-typed cast might fail at run-time if the class of the cast does not match the class of the value.  In a language such as FPC  Chapter 20  with recursive types, there is no need to add  dyn as a primitive type. Instead, it is deﬁned to be type  rec t is [num  cid:9 → nat, fun  cid:9 → t  cid:19  t].  The introduction and elimination forms for this deﬁnition of dyn are deﬁnable as follows:2  new[num] e   cid:2  fold num · e  new[fun] e   cid:2  fold fun · e  cast[num] e   cid:2  case unfold e {num · x  cid:9 → x  fun · x  cid:9 → error} cast[fun] e   cid:2  case unfold e {num · x  cid:9 → error  fun · x  cid:9 → x}.   23.3    23.4   23.5   23.6   23.7   These deﬁnition simply decompose the class operations for dyn into recursive unfoldings and case analyses on values of a sum type.  23.2 Dynamic as Static Typing  The language DPCF of Chapter 22 can be embedded into HPCF by a simple translation that makes explicit the class checking in the dynamics of DPCF. Speciﬁcally, we may † of expressions of DPCF into expressions of HPCF according to the deﬁne a translation d following static correctness criterion: Theorem 23.3. If x1 ok, . . . , xn ok  cid:12  d ok according to the statics of DPCF, then x1 : dyn, . . . , xn : dyn  cid:12  d  † : dyn in HPCF.  The proof of Theorem 23.3 is given by induction on the structure of d based on the  following translation:  x num[n]  zero succ d   †  cid:2  x †  cid:2  new[num] n  †  cid:2  new[num] z  †  cid:2  new[num] s cast[num] d  †   fun x.d    ap d1; d2    †  cid:2  new[fun] λ  x : dyn  d † †  cid:2  cast[fun] d 1  d fix x.d   cid:2  fix{dyn} x.d †    † 2   †     ifz{d0; x.d1} d   cid:2  ifz{d      † † 0; x.[new[num] x  x]d 1  } cast[num] d  †      A rigorous proof of correctness of this translation requires methods like those in Chapter 47.   201  23.3 Optimization of Dynamic Typing  23.3 Optimization of Dynamic Typing  The language HPCF combines static and dynamic typing by enriching PCF with the type dyn of classiﬁed values. It is, for this reason, called a hybrid language. Unlike a purely dynamic type system, a hybrid type system can express invariants that are crucial to the optimization of programs in HPCF.  Consider the addition function in DPCF given in Section 22.3, which we transcribe here  for convenience:  λ  x  fix p is λ  y  ifz y {zero  cid:9 → x  succ y   cid:5      cid:9 → succ p y   cid:5     }.  It is a value of type dyn in HPCF given as follows:  fun ! λ  x : dyn  fix p : dyn is fun ! λ  y : dyn  ex,p,y ,   23.8   within which the fragment  stands for the expression  x : dyn, p : dyn, y : dyn  cid:12  ex,p,y : dyn  ifz  y @ num {zero  cid:9 → x  succ y   cid:5      cid:9 → num !  s  p @ fun  num ! y   cid:5     @ num  }.  The embedding into HPCF makes explicit the run-time checks that are implicit in the dynamics of DPCF.  Careful examination of the embedded formulation of addition reveals a great deal of redundancy and overhead that can be eliminated in the statically typed version. Eliminating this redundancy requires a static type discipline, because the intermediate computations involve values of a type other than dyn. This transformation shows that the freedoms offered by dynamic languages accruing from the absence of types are, instead, limitations on their expressive power arising from the restriction to a single type.  The ﬁrst redundancy arises from the use of recursion in a dynamic language. In the above example, we use recursion to deﬁne the inner loop p of the computation. The value p is, by deﬁnition, a λ-abstraction, which is explicitly tagged as a function. Yet the call to p within the loop checks at run-time whether p is in fact a function before applying it. Because p is an internally deﬁned function, all of its call sites are under the control of the addition function, which means that there is no need for such pessimism at calls to p, provided that we change its type to dyn  cid:19  dyn, which directly expresses the invariant that p is a function acting on dynamic values.  Performing this transformation, we obtain the following reformulation of the addition  function that eliminates this redundancy:  fun ! λ  x : dyn  fun ! fix p : dyn  cid:19  dyn is λ  y : dyn  e   cid:5  x,p,y ,  where e   cid:5  x,p,y is the expression ifz  y @ num {zero  cid:9 → x  succ y   cid:5      cid:9 → num !  s p num ! y   cid:5     @ num  }.  We have “hoisted” the function class label out of the loop and suppressed the cast inside the loop. Correspondingly, the type of p has changed to dyn  cid:19  dyn.   202  Hybrid Typing  Next, note that the variable y of type dyn is cast to a number on each iteration of the loop before it is tested for zero. Because this function is recursive, the bindings of y arise in one of two ways: at the initial call to the addition function, and on each recursive call. But the recursive call is made on the predecessor of y, which is a true natural number that is labeled with num at the call site, only to be removed by the class check at the conditional on the next iteration. This observation suggests that we hoist the check on y outside of the loop, and avoid labeling the argument to the recursive call. Doing so changes the type of the function, however, from dyn  cid:19  dyn to nat  cid:19  dyn. Consequently, further changes are required to ensure that the entire function remains well-typed.  Before doing so, let us make another observation. The result of the recursive call is checked to ensure that it has class num, and, if so, the underlying value is incremented and labeled with class num. If the result of the recursive call came from an earlier use of this branch of the conditional, then obviously the class check is redundant, because we know that it must have class num. But what if the result came from the other branch of the conditional? In that case, the function returns x, which need not be of class num because it is provided by the caller of the function. However, we may reasonably insist that it is an error to call addition with a non-numeric argument. This restriction can be enforced by replacing x in the zero branch of the conditional by x @ num.  cid:5  cid:5  x deﬁned as follows: Combining these optimizations we obtain the inner loop e fix p : nat  cid:19  nat is λ  y : nat  ifz y {zero  cid:9 → x @ num  succ y    cid:9 → s p y    }.   cid:5    cid:5   It has the type nat  cid:19  nat, and runs without class checks when applied to a natural number. Finally, recall that the goal is to deﬁne a version of addition that works on values of type dyn. Thus, we need a value of type dyn  cid:19  dyn, but what we have at hand is a function of type nat  cid:19  nat. It can be converted to the needed form by pre-composing with a cast to num and post-composing with a coercion to num:  fun ! λ  x : dyn  fun ! λ  y : dyn  num !  e   cid:5  cid:5  x y @ num  .   cid:5  cid:5  The innermost λ-abstraction converts the function e x from type nat cid:19 nat to type dyn cid:19 dyn by composing it with a class check that ensures that y is a natural number at the initial call site, and applies a label to the result to restore it to type dyn.  The outcome of these transformations is that the inner loop of the computation runs at “full speed,” without any manipulation of tags on functions or numbers. But the outermost form of addition remains; it is a value of type dyn encapsulating a curried function that takes two arguments of type dyn. Doing so preserves the correctness of all calls to addition, which pass and return values of type dyn, while optimizing its execution during the computation. Of course, we could strip the class tags from the addition function, changing its type from dyn to the more descriptive dyn  cid:19  dyn  cid:19  dyn, but this imposes the obligation on the caller to treat addition not as a value of type dyn, but rather as a function that must be applied to two successive values of type dyn whose class is num. As long as the call sites to addition are under programmer control, there is no obstacle to effecting this transformation. It is only when there are external call sites, not directly under programmer control, that there is any need to package addition as a value of type dyn. Applying this principle generally, we see that dynamic typing is only of marginal utility—it is used only at the margins of a system   203  23.4 Static versus Dynamic Typing  where uncontrolled calls arise. Internally to a system there is no beneﬁt, and considerable drawback, to restricting attention to the type dyn.  23.4 Static versus Dynamic Typing  There have been many attempts by advocates of dynamic typing to distinguish dynamic from static languages. It is useful to consider the supposed distinctions from the present viewpoint.  1. Dynamic languages associate types with values, whereas static languages associate types to variables. Dynamic languages associate classes, not types, to values by tagging them with identiﬁers such as num and fun. This form of classiﬁcation amounts to a use of recursive sum types within a statically typed language, and hence cannot be seen as a distinguishing feature of dynamic languages. Moreover, static languages assign types to expressions, not just variables. Because dynamic languages are just particular static languages  with a single type , the same can be said of dynamic languages.  2. Dynamic languages check types at run-time, whereas static language check types at compile time. Dynamic languages are just as surely statically typed as static languages, albeit for a degenerate type system with only one type. As we have seen, dynamic languages do perform class checks at run-time, but so too do static languages that admit sum types. The difference is only the extent to which we must use classiﬁcation: always in a dynamic language, only as necessary in a static language.  3. Dynamic languages support heterogeneous collections, whereas static languages sup- port homogeneous collections. The purpose of sum types is to support heterogeneity, so that any static language with sums admits heterogeneous data structures. A typical example is a list such as  cons num[1]; cons fun x.x ; nil     written in abstract syntax for emphasis . It is sometimes said that such a list is not representable in a static language, because of the disparate nature of its components. Whether in a static or a dynamic language, lists are type homogeneous, but can be class heterogeneous. All elements of the above list are of type dyn; the ﬁrst is of class num, and the second is of class fun.  Thus, the seeming opposition between static and dynamic typing is an illusion. The question is not whether to have static typing, but rather how best to embrace it. Conﬁning one’s attention to a single recursive type seems pointlessly restrictive. Indeed, many so- called untyped languages have implicit concessions to there being more than one type. The classic example is the ubiquitous concept of “multi-argument functions,” which are a concession to the existence of products of the type of values  with pattern matching . It is then a short path to considering “multi-result functions,” and other ad hoc language features that amount to admitting a richer and richer static type discipline.   204  Hybrid Typing  23.5 Notes  Viewing dynamic languages as static languages with recursive types was ﬁrst proposed by Dana Scott  Scott, 1980b , who also suggested glossing “untyped” as “uni-typed.” Most modern statically typed languages, such as Java or Haskell or OCaml or SML, include a type similar to dyn, or admit recursive types with which to deﬁne it. For this reason one might expect that the opposition between dynamic and static typing would fade away, but industrial and academic trends suggest otherwise.  23.1. Consider the extensions to DPCF described in Section 22.2 to admit null and pairing and their associated operations. Extend the statics and dynamics of HPCF to account for these extensions, and give a translation of the null and pairing operations described informally in Chapter 22 in terms of this extension to HPCF.  23.2. Continue the interpretation of the null and pairing operations in HPCF given in Exercise 23.1 to provide an interpretation in FPC. Speciﬁcally, deﬁne the expanded dyn as a recursive type and give a direct implementation of the null and pairing primitives in terms of this recursive type.  23.3. Consider the append function deﬁned in Chapter 22 using nil and cons to represent  lists:  fix a is λ  x  λ  y  cond x; cons car x ; a cdr x   y  ; y .  Rewrite append in HPCF using the deﬁnitions given in Exercise 23.1. Then optimize the implementation to eliminate unnecessary overhead while ensuring that append still has type dyn.  Exercises  Notes  1 The judgment e err signals a checked error that is to be treated as described in Section 6.3. 2 The expression error aborts the computation with an error; this can be accomplished using  exceptions, which are the subject of Chapter 29.   P A R T X  Subtyping    24  Structural Subtyping  A subtype relation is a pre-order  reﬂexive and transitive relation  on types that validates the subsumption principle:   cid:5  is a subtype of τ, then a value of type τ   cid:5  may be provided when a value of type τ is  if τ required.  The subsumption principle relaxes the strictures of a type system to allow values of one type to be treated as values of another.  Experience shows that the subsumption principle, although useful as a general guide, can be tricky to apply correctly in practice. The key to getting it right is the principle of introduction and elimination. To see whether a candidate subtyping relationship is sensible, it sufﬁces to consider whether every introduction form of the subtype can be safely manipulated by every elimination form of the supertype. A subtyping principle makes sense only if it passes this test; the proof of the type safety theorem for a given subtyping relation ensures that this is the case.  A good way to get a subtyping principle wrong is to think of a type merely as a set of values  generated by introduction forms  and to consider whether every value of the subtype can also be considered to be a value of the supertype. The intuition behind this approach is to think of subtyping as akin to the subset relation in ordinary mathematics. But, as we shall see, this can lead to serious errors, because it fails to take account of the elimination forms that are applicable to the supertype. It is not enough to think only of the introduction forms; subtyping is a matter of behavior, not containment.  24.1 Subsumption  A subtyping judgment has the form τ we demand that the following structural rules of subtyping be admissible:  <: τ, and states that τ   cid:5  is a subtype of τ. At the least   cid:5   τ <: τ   cid:5  cid:5   τ  <: τ  cid:5  cid:5  τ   cid:5    cid:5  τ <: τ  <: τ   24.1a    24.1b   In practice, we either tacitly include these rules as primitive or prove that they are admissible for a given set of subtyping rules.   208  Structural Subtyping  The point of a subtyping relation is to enlarge the set of well-typed programs, which is  accomplished by the subsumption rule:   cid:7   cid:12  e : τ   cid:5    cid:5   cid:7   cid:12  e : τ  τ  <: τ   24.2   In contrast to most other typing rules, the rule of subsumption is not syntax-directed, because it does not constrain the form of e. That is, the subsumption rule can be applied to any form of expression. In particular, to show that e : τ, we have two choices: either apply the rule appropriate to the particular form of e, or apply the subsumption rule, checking that e : τ   cid:5  and τ  <: τ.   cid:5   24.2 Varieties of Subtyping  In this section, we will informally explore several different forms of subtyping in the context of extensions of the language FPC introduced in Chapter 20.  Numeric Types  We begin with an informal discussion of numeric types such as are common in many programming languages. Our mathematical experience suggests subtyping relationships among numeric types. For example, in a language with types int, rat, and real, repre- senting the integers, the rationals, and the reals, it is tempting to postulate the subtyping relationships  by analogy with the set containments  int <: rat <: real  Z ⊆ Q ⊆ R.  But are these subtyping relationships sensible? The answer depends on the represen- tations and interpretations of these types. Even in mathematics, the containments just mentioned are usually not true—or are true only in a rough sense. For example, the set of rational numbers can be considered to consist of ordered pairs  m, n , with n  cid:6 = 0 and gcd m, n  = 1, representing the ratio m n. The setZ of integers can be isomorphically embedded within Q by identifying n ∈ Z with the ratio n 1. Similarly, the real numbers are often represented as convergent sequences of rationals, so that strictly speaking the rationals are not a subset of the reals, but rather can be embedded in them by choosing a canonical representative  a particular convergent sequence  of each rational.  For mathematical purposes, it is entirely reasonable to overlook ﬁne distinctions such as that between Z and its embedding within Q. Ignoring the difference is justiﬁed because the operations on rationals restrict to the embedding in the expected way: if we add two integers thought of as rationals in the canonical way, then the result is the rational associated with their sum. And similarly for the other operations, provided that we take some care   209  24.2 Varieties of Subtyping  in deﬁning them to ensure that it all works out properly. For the purposes of computing, however, we must also take account of algorithmic efﬁciency and the ﬁniteness of machine representations. For example, what are often called “real numbers” in a programming language are, of course, ﬂoating point numbers, a ﬁnite subset of the rational numbers. Not every rational can be exactly represented as a ﬂoating point number, nor does ﬂoating point arithmetic restrict to rational arithmetic, even when its arguments are exactly represented as ﬂoating point numbers.  Product Types  Product types give rise to a form of subtyping based on the subsumption principle. The only elimination form applicable to a value of product type is a projection. Under mild assumptions about the dynamics of projections, we may consider one product type to be a subtype of another by considering whether the projections applicable to the supertype can be validly applied to values of the subtype. Consider a context in which a value of type τ =  cid:24 τj cid:25 j∈J is required. The statics of ﬁnite products  rules  10.3   ensures that the only operation we may perform on a value of type τ, other than to bind it to a variable, is to take the jth projection from it for some j ∈ J  cid:5 . For the projection e · j to to obtain a value of type τj . Now suppose that e is of type τ  cid:25 i∈I such that j ∈ I. Moreover, for the be well-formed, then τ = τj . Because j ∈ J is arbitrary,  cid:5  projection to be of type τj , it is enough to require that τ j we arrive at the following subtyping rule for ﬁnite product types:   cid:5  i   cid:5  is a ﬁnite product type  cid:24 τ  cid:5  J ⊆ I i∈I τi <:   cid:5   .  j∈J τj   24.3   This rule suﬁces for the required subtyping, but not necessary; we will consider a more liberal form of this rule in Section 24.3. The justiﬁcation for rule  24.3  is that we may evaluate e · i regardless of the actual form of e, provided only that it has a ﬁeld indexed by i ∈ I.  Sum Types   cid:14   By an argument dual to the one given for ﬁnite product types, we may derive a related j∈J τj is required, the statics of subtyping rule for ﬁnite sum types. If a value of type sums  rules  11.3   ensures that the only non-trivial operation that we may perform on  cid:5  i instead, no that value is a J -indexed case analysis. If we provide a value of type difﬁculty will arise so long as I ⊆ J and each τ  cid:5  i is equal to τi. If the containment is strict,  cid:14  some cases cannot arise, but this does not disrupt safety. I ⊆ J i∈I τi <:   cid:14    cid:14   j∈J τj   24.4   i∈I τ  .  Note well the reversal of the containment as compared to rule  24.3 .   210  Structural Subtyping  Dynamic Types  A popular form of subtyping is associated with the type dyn introduced in Chapter 23. The type dyn provides no information about the class of a value of this type. One might argue that it is whole the point of dynamic typing to suppress this information statically, making it available only dynamically. On the other hand, it is not much trouble to introduce subtypes of dyn that specify the class of a value, relying on subsumption to “forget” the class when it cannot be determined statically.  Working in the context of Chapter 23 this amounts to introduce two new types, dyn[num]  and dyn[fun], governed by the following two subtyping axioms:  Of course, in a richer language with more classes of dynamic values, one would corre- spondingly introduce more such subtypes of dyn, one for each additional class. As a matter of notation, the type dyn is frequently spelled object, and its class-speciﬁc subtypes dyn[num] and dyn[fun], are often written as num and fun, respectively. But doing so in- vites confusion between the separate concepts of class and type, as discussed in detail in Chapters 22 and 23.  The class-speciﬁc subtypes of dyn come into play by reformulating the typing rules for  introducing values of type dyn to note the class of the created value:  dyn[num] <: dyn  dyn[fun] <: dyn   cid:7   cid:12  e : nat   cid:7   cid:12  new[num] e  : dyn[num]   cid:7   cid:12  e : dyn  cid:19  dyn   cid:7   cid:12  new[fun] e  : dyn[fun]   24.5a    24.5b    24.6a    24.6b   Thus, in this formulation, classiﬁed values “start life” with class-speciﬁc types, because in those cases it is statically apparent what is the class of the introduced value. Subsumption is used to weaken the type to dyn in those cases where no static prediction can be made—for example, when the branches of a conditional evaluate to dynamic values of different classes it is necessary to weaken the type of the branches to dyn.  The advantage of such a subtyping mechanism is that we can express more precise types, such as the type dyn[num]  cid:19  dyn[num] of functions mapping a value of type dyn with class num to another such value. This typing is more precise than, say, dyn  cid:19  dyn, which merely classiﬁes functions that act on dynamically typed values. In this way, weak invariants can be expressed and enforced, but only insofar as it is possible to track the classes of the values involved in a computation. Subtyping is not nearly a powerful enough mechanism for practical situations, rendering the additional speciﬁcity not worth the effort of including it.  A more powerful approach is developed in Chapter 25.    211  24.3 Variance  24.3 Variance  In addition to basic subtyping principles such as those considered in Section 24.2, it is also important to consider the effect of subtyping on type constructors. A type constructor covariant in an argument if subtyping in that argument is preserved by the constructor. It is contravariant if subtyping in that argument is reversed by the constructor. It is invariant in an argument if subtyping for the constructed type is not affected by subtyping in that argument.   cid:5  Finite product types are covariant in each ﬁeld. For if e is of type e · j is to be of type τj , then it sufﬁces to require that j ∈ I and τ  Product and Sum Types   cid:5  i , and the projection  i∈I τ  cid:5  j <: τj .  It is implicit in this rule that the dynamics of projection cannot be sensitive to the precise type of any of the ﬁelds of a value of ﬁnite product type.  Finite sum types are also covariant, because each branch of a case analysis on a value of the supertype expects a value of the corresponding summand, for which it sufﬁces to provide a value of the corresponding subtype summand:   cid:5   ∀i ∈ I  τ  cid:5  i <: i∈I τ   cid:5    cid:5  i <: τi i∈I τi   cid:14   ∀i ∈ I  τ  cid:5  i <: i∈I τ   cid:14    cid:5  i <: τi i∈I τi  Partial Function Types   cid:5  2 <: τ2 τ  cid:5  2 <: τ1  cid:19  τ2  τ1  cid:19  τ   24.7    24.8    24.9   The variance of the function type constructors is a bit more subtle. We will work here with the partial function types, but the same considerations apply to the total function type. Let  cid:5  2. us consider ﬁrst the variance of the function type in its range. Suppose that e : τ1  cid:19  τ Then if e1 : τ1, then e e1  : τ   cid:5  2 <: τ2, then e e1  :τ 2 as well.   cid:5  2, and if τ   cid:5  2 also delivers a value of type τ2, provided that  Every function that delivers a value of type τ  cid:5  2 <: τ2. Thus, the function type constructor is covariant in its range. τ Now let us consider the variance of the function type in its domain. Suppose again that e : τ1  cid:19  τ2. Then e can be applied to any value of type τ1 to obtain a value of type τ2.  cid:5  1 of τ1, Hence, by the subsumption principle, it can be applied to any value of a subtype τ and it will still deliver a value of type τ2. Consequently, we may just as well think of e as   212  Structural Subtyping  having type τ   cid:5  1  cid:19  τ2.  The function type is contravariant in its domain position. Note well the reversal of the subtyping relation in the premise as compared to the conclusion of the rule!  Combining these rules we obtain the following general principle of contra- and covariance  for function types:   24.10    24.11   Beware of the reversal of the ordering in the domain!   cid:5  1 <: τ1 τ  cid:5  τ1  cid:19  τ2 <: τ 1  cid:19  τ2   cid:5   cid:5  2 <: τ2 1 <: τ1 τ τ  cid:5   cid:5  2 <: τ τ1  cid:19  τ 1  cid:19  τ2  Recursive Types  The language FPC has a partial function types, which behave the same under subtyping as total function types, sums and products, which behave as described above, and recursive types, which introduce some subtleties that have been the source of error in language design. To gain some intuition, consider the type of labeled binary trees with natural numbers at each node,  rec t is [empty  cid:9 → unit, binode  cid:9 →  cid:24 data  cid:9 → nat, lft  cid:9 → t, rht  cid:9 → t cid:25 ],  and the type of “bare” binary trees, without data attached to the nodes,  rec t is [empty  cid:9 → unit, binode  cid:9 →  cid:24 lft  cid:9 → t, rht  cid:9 → t cid:25 ].  Is either a subtype of the other? Intuitively, we might expect the type of labeled binary trees to be a subtype of the type of bare binary trees, because any use of a bare binary tree can simply ignore the presence of the label.  Now consider the type of bare “two-three” trees with two sorts of nodes, those with two  children, and those with three:  rec t is [empty  cid:9 → unit, binode  cid:9 → τ2, trinode  cid:9 → τ3],  where  τ2  cid:2   cid:24 lft  cid:9 → t, rht  cid:9 → t cid:25 , and τ3  cid:2   cid:24 lft  cid:9 → t, mid  cid:9 → t, rht  cid:9 → t cid:25 .  What subtype relationships should hold between this type and the preceding two tree types? Intuitively the type of bare two-three trees should be a supertype of the type of bare binary trees, because any use of a two-three tree proceeds by three-way case analysis, which covers both forms of binary tree.  To capture the pattern illustrated by these examples, we need a subtyping rule for recursive  types. It is tempting to consider the following rule: <: τ  t type  cid:12  τ   cid:5   rec t is τ  <: rec t is τ   cid:5   ??   24.12    213  24.3 Variance  That is, to check whether one recursive type is a subtype of the other, we simply compare their bodies, with the bound variable treated as an argument. Notice that by reﬂexivity of subtyping, we have t <: t, and hence we may use this fact in the derivation of τ  <: τ.   cid:5   Rule  24.12  validates the intuitively plausible subtyping between labeled binary tree and bare binary trees just described. Deriving this requires checking that the subtyping relationship   cid:24 data  cid:9 → nat, lft  cid:9 → t, rht  cid:9 → t cid:25  <:  cid:24 lft  cid:9 → t, rht  cid:9 → t cid:25 ,  holds generically in t, which is evidently the case.  Unfortunately, Rule  24.12  also underwrites incorrect subtyping relationships, as well  as some correct ones. As an example of what goes wrong, consider the recursive types  and   cid:5  = rec t is  cid:24 a  cid:9 → t  cid:19  nat, b  cid:9 → t  cid:19  int cid:25   τ  τ = rec t is  cid:24 a  cid:9 → t  cid:19  int, b  cid:9 → t  cid:19  int cid:25 .  We assume for the sake of the example that nat <: int, so that by using rule  24.12  we may derive τ   cid:5  <: τ, which is incorrect. Let e : τ   4, b  cid:9 → λ  x : τ fold  cid:24 a  cid:9 → λ  x : τ   cid:5  be the expression  cid:5     q  unfold x  · a  x   cid:25  ,   cid:5    cid:5   <: τ, it follows that  where q : nat  cid:19  nat is the discrete square root function. Because τ e : τ as well, and hence  unfold e  :  cid:24 a  cid:9 → τ  cid:19  int, b  cid:9 → τ  cid:19  int cid:25 .  Now let e   cid:5  : τ be the expression  fold  cid:24 a  cid:9 → λ  x : τ  -4, b  cid:9 → λ  x : τ   0 cid:25  .   cid:5  is that the a method returns a negative number; the b method   The important point about e is of no signiﬁcance.  To ﬁnish the proof, observe that    cid:20 −→∗   unfold e  · b  e   cid:5   q -4 ,  which is a stuck state. We have derived a well-typed program that “gets stuck,” refuting type safety!  Rule  24.12  is therefore incorrect. But what has gone wrong? The error lies in the choice of a single variable to stand for both recursive types, which does not correctly model self- reference. In effect, we are treating two distinct recursive types as if they were equal while checking their bodies for a subtyping relationship. But this is clearly wrong! It fails to take account of the self-referential nature of recursive types. On the left side, the bound variable stands for the subtype, whereas on the right the bound variable stands for the super-type. Confusing them leads to the unsoundness just illustrated.  As is often the case with self-reference, the solution is to assume what we are trying to prove, and check that this assumption can be maintained by examining the bodies of the recursive types. To do so, we use hypothetical judgments of the form  cid:8   cid:12  τ <: τ, where  cid:8  consists of hypotheses t type and t <: τ that declares a fresh type variable t that is not   cid:5    214  Structural Subtyping  otherwise declared in  cid:8 . Using such hypothetical judgments, we may state the correct rule for subtyping recursive types as follows:   cid:8   cid:12  rec t  cid:5   cid:5    cid:8 , t type, t  type, t   cid:5    cid:5   <: t  cid:12  τ   cid:5   <: τ  cid:8 , t  cid:5    cid:5   is τ   cid:5    cid:5   type  cid:12  τ <: rec t is τ  type  cid:8 , t type  cid:12  τ type  .   24.13   cid:5  and That is, to check whether rec t <: τ under this assumption. t stand for the corresponding recursive types, and check that τ It is instructive to check that the unsound subtyping example given above is not derivable using this rule: the subtyping assumption is at odds with the contravariance of the function type in its domain.  <: rec t is τ, we assume that t  <: t, because t  is τ   cid:5    cid:5   Consider extending FPC with the universal and existential quantiﬁed types discussed in Chapters 16 and 17. The variance principles for the quantiﬁers state that they are uniformly covariant in the quantiﬁed types:  Quantiﬁed Types   cid:8 , t type  cid:12  τ  cid:8   cid:12  ∀ t.τ  cid:8 , t type  cid:12  τ  cid:8   cid:12  ∃ t.τ   cid:5    cid:5   <: τ  cid:5   <: ∀ t.τ   <: τ  cid:5   <: ∃ t.τ   Consequently, we may derive the principle of substitution: Lemma 24.1. If  cid:8 , t type  cid:12  τ1 <: τ2, and  cid:8   cid:12  τ type, then  cid:8   cid:12  [τ t]τ1 <: [τ t]τ2.  Proof By induction on the subtyping derivation.  It is easy to check that the above variance principles for the quantiﬁers are consistent  cid:5   consists <: τ, <: [ρ t]τ, and hence e is also an implementation of  with the principle of subsumption. For example, a package of the subtype ∃ t.τ of a representation type ρ and an implementation e of type [ρ t]τ we have by substitution that [ρ t]τ type [ρ t]τ. So the package is also of the supertype.   cid:5 . But if t type  cid:12  τ   cid:5    cid:5   It is natural to extend subtyping to the quantiﬁers by allowing quantiﬁcation over all  subtypes of a speciﬁed type; this is called bounded quantiﬁcation.   24.14a    24.14b    24.15a    24.15b    24.15c    cid:8 , t type, t <: τ  cid:12  t <: τ   cid:8   cid:12  τ :: T  cid:8   cid:12  τ <: τ  cid:8   cid:12  τ  cid:5  <: τ  cid:8   cid:12  τ <: τ   cid:5  cid:5    cid:5   <: τ   cid:8   cid:12  τ   cid:5  cid:5    215  24.4 Dynamics and Safety   cid:8   cid:12  τ   cid:5   cid:5  1 <: τ1  cid:8 , t type, t <: τ 1  cid:8   cid:12  ∀ t <: τ1.τ2 <: ∀ t <: τ   cid:5  2   cid:12  τ2 <: τ  cid:5   cid:5  1.τ 2   cid:8   cid:12  τ1 <: τ  1  cid:8 , t type, t <: τ1  cid:12  τ2 <: τ  cid:5    cid:5  2   cid:8   cid:12  ∃ t <: τ1.τ2 <: ∃ t <: τ   cid:5   cid:5  1.τ 2   24.15d    24.15e   Rule  24.15d  states that the universal quantiﬁer is contravariant in its bound, whereas rule  24.15e  states that the existential quantiﬁer is covariant in its bound.  24.4 Dynamics and Safety  There is a subtle assumption in the deﬁnition of product subtyping in Section 24.2, namely that the same projection operation from an I-tuple applies also to a J -tuple, provided J ⊇ I. But this need not be the case. One could represent I-tuples differently from J -tuples at will, so that the meaning of the projection at position i ∈ I ⊆ J is different in the two cases. Nothing rules out this possibility, yet product subtyping relies on it not being the case. From this point of view, product subtyping is not well-justiﬁed, but one may instead consider that subtyping limits possible implementations to ensure that it makes sense. Similar considerations apply to sum types. An J -way case analysis need not be applicable to an I-way value of sum type, even when I ⊆ J and all the types in common agree. For example, one might represent values of a sum type with a “small” index set in a way that is not applicable for a “large” index set. In that case, the “large” case analysis would not make sense on a value of “small” sum type. Here again we may consider either that subtyping is not justiﬁed, or that it imposes limitations on the implementation that are not otherwise forced.  These considerations merit careful consideration of the safety of languages with subtyp- ing. As an illustrative case we consider the safety of FPC enriched with product subtyping. The main concern is that the subsumption rule obscures the “true” type of a value, compli- cating the canonical forms lemma. Moreover, we assume that the same projection makes sense for a wider tuple than a narrower one, provided that it is within range.  Lemma 24.2  Structurality .  1. The tuple subtyping relation is reﬂexive and transitive. 2. The typing judgment  cid:7   cid:12  e : τ is closed under weakening and substitution.  Proof  1. Reﬂexivity is proved by induction on the structure of types. Transitivity is proved by <: τ to obtain a derivation   cid:5  and τ  <: τ   cid:5  cid:5    cid:5   induction on the derivations of the judgments τ of τ  <: τ.   cid:5  cid:5   2. By induction on rules  10.3 , augmented by rule  24.2 .   216  Structural Subtyping   cid:5  Lemma 24.3  Inversion .  cid:5  1. If e · j : τ , then e : i∈I τi, j ∈ I , and τj <: τ . 2. If  cid:24 ei cid:25 i∈I : τ , then  cid:5  i∈I τ 3. If τ j∈J τj , then τ  cid:5  4. If i <:  j∈J τj , then J ⊆ I and τ   cid:5  i <: τ where ei : τ   cid:5  = cid:5   <: i∈I τ   cid:5    cid:5   i∈I τ   cid:5   i for each i ∈ I .  cid:5    cid:5  i for some I and some types τ j <: τj for each j ∈ J .  cid:5   i for i ∈ I .  cid:5   Proof By induction on the subtyping and typing rules, paying special attention to rule  24.2 . Theorem 24.4  Preservation . If e : τ and e  cid:20 −→ e  cid:5  Proof By induction on rules  10.4 . For example, consider rule  10.4d , so that e =  cid:5   cid:5  j∈J τj , with k ∈ J and τk <: τ.  cid:5  = ek. By Lemma 24.3, we have  cid:24 ei cid:25 i∈I :  cid:24 ei cid:25 i∈I · k and e By another application of Lemma 24.3 for each i ∈ I, there exists τ  cid:5   cid:5  i and i such that ei : τ j <: τj for each j ∈ J . j∈J τj . By Lemma 24.3 again, we have J ⊆ I and τ  cid:5  i∈I τ   cid:5 , then e   cid:5  i <:   cid:5  : τ .  But then ek : τk, as desired. The remaining cases are similar.  Lemma 24.5  Canonical Forms . If e val and e : where J ⊆ I , and ej : τj for each j ∈ J .  j∈J τj , then e is of the form  cid:24 ei cid:25 i∈I ,   cid:5   Proof By induction on rules  10.3  augmented by rule  24.2 .  Theorem 24.6  Progress . If e : τ , then either e val or there exists e   cid:5  such that e  cid:20 −→ e   cid:5 .  Proof By induction on rules  10.3  augmented by rule  24.2 . The rule of subsumption is handled by appeal to the inductive hypothesis on the premise of the rule. Rule  10.4d  follows from Lemma 24.5.  24.5 Notes  Subtyping is perhaps the most widely misunderstood concept in programming languages. Subtyping is principally a convenience, akin to type inference, that makes some programs simpler to write. But the subsumption rule cuts both ways. Inasmuch as it allows the  cid:5  is a subtype of τ, it also weakens the meaning of implicit passage from τ a type assertion e : τ to mean that e has some type contained in the type τ. Subsumption precludes expressing the requirement that e has exactly the type τ, or that two expressions jointly have the same type. And it is just this weakness that creates so many of the difﬁculties with subtyping.   cid:5  to τ when τ   217  Exercises  Much has been written about subtyping, often in relation to object-oriented programming. Standard ML  Milner et al., 1997  is one of the ﬁrst languages to make use of subtyping, in two forms, called enrichment and realization. The former corresponds to product subtyping, and the latter to the “forgetful” subtyping associated with type deﬁnitions  see Chapter 43 . The ﬁrst systematic studies of subtyping include those by Mitchell  1984 , Reynolds  1980 , and Cardelli  1988 . Pierce  2002  give a thorough account of subtyping, especially of recursive and polymorphic types and proves that subtyping for bounded impredicative universal quantiﬁcation is undecidable.  Exercises  24.1. Check the variance of the type   unit  cid:19  τ  ×  τ  cid:19  unit .  When viewed as a constructor with argument τ, is it covariant or contravariant? Give a precise proof or counterexample in each case.  24.2. Consider the two recursive types,  ρ1  cid:2  rec t is  cid:24 eq  cid:9 →  t  cid:19  bool  cid:25 ,  and  ρ2  cid:2  rec t is  cid:24 eq  cid:9 →  t  cid:19  bool , f  cid:9 → bool cid:25 .  It is clear that ρ1 could not be a subtype of ρ2, because, viewed as a product after unrolling, a value of the former type lacks a component that a value of the latter has. But is ρ2 a subtype of ρ1? If so, prove it by exhibiting a derivation of this fact using the rules given in Section 24.3. If not, give a counterexample showing that the suggested subtyping would violate type safety.  24.3. Another approach to the dynamics of subtyping that ensures safety, but gives sub- sumption dynamic signiﬁcance, associates a witness, called a coercion, to each subtyping relation, and inserts a coercion wherever subsumption is used. More precisely,  cid:5  that  a  Assign to each valid subtyping τ <: τ  cid:5  is  cid:5 , applying subsumption to e : τ inserts an application of   b  Interpret the subsumption rule as implicit coercion. Speciﬁcally, when τ <: τ   cid:5  a coercion function χ : τ  cid:19  τ  transforms a value of type τ into a value of type τ   cid:5 .  witnessed by χ : τ  cid:19  τ χ to obtain χ e  : τ   cid:5 .  Formulate this idea precisely for the case of a subtype relation generated by “width” subtyping for products, and the variance principles for product, sum, and function types. Your solution should make clear that it evades the tacit projection assumption mentioned above.   218  Structural Subtyping  But there may be more than one coercion χ : τ  cid:19  τ   cid:5  corresponding to the sub-  cid:5 . The meaning of a program would then depend on which coercion is typing τ <: τ chosen when subsumption is used. If there is exactly one coercion for each subtyping relation, it is said to be coherent. Is your coercion interpretation of product subtyping coherent?  A proper treatment of coherence requires expression equivalence, which is discussed in Chapter 47.    25  Behavioral Typing  In Chapter 23, we demonstrated that dynamic typing is but a mode of use of static typing, one in which dynamically typed values are of type dyn, a particular recursive sum type. A value of type dyn is always of the form new[c] e , where c is its class and e is its underlying value. Importantly, the class c determines the type of the underlying value of a dynamic value. The type system of the hybrid language is rather weak in that every dynamically classiﬁed value has the same type, and there is no mention of the class in its type. To correct this shortcoming it is common to enrich the type system of the hybrid language to capture such information, for example, as described in Section 24.2.  In such a situation, subtyping is used to resolve a fundamental tension between structure and behavior in the design of type systems. On the one hand, types determine the structure of a programming language and, on the other, serve a behavioral speciﬁcations of expressions written in that language. Subtyping attempts to resolve this tension, unsuccessfully, by allowing certain forms of retyping. Although subtyping works reasonably well for small examples, things get far more complicated when we wish to specify the deep structure of a value, say, that it is of a class c and its underlying value is of another class d whose underlying value is a natural number. There is no limit to the degree of speciﬁcity one may wish in such descriptions, which gives rise to endless variations on type systems to accommodate various special situations.  Another resolution of the tension between structure and behavior in typing is to separate these aspects by distinguishing types from type reﬁnements. Type reﬁnements specify the execution behavior of an expression of a particular type using speciﬁcations that capture whatever properties are of interest, limited only by the difﬁculty of proving that a program satisﬁes the speciﬁcation given by a reﬁnement.  Certain limited forms of behavioral speciﬁcations can express many useful properties of programs while remaining mechanically checkable. These include the fundamental behavioral properties determined by the type itself but can be extended to include sharper conditions than just these structural properties. In this chapter, we will consider a particular notion of reﬁnement tailored to the hybrid language of Chapter 23. It is based on two basic principles:  1. Type constructors, such as product, sum, and function space, act on reﬁnements of their component types to induce a reﬁnement on the compound type formed by those constructors.   220  Behavioral Typing  2. It is useful to track the class of a value of type dyn and to assign multiple reﬁnements to  specify distinct behaviors of expressions involving values of dynamic type.  We will formulate a system of reﬁnements based on these principles that ensures that a well-reﬁned program cannot incur a run-time error arising from the attempt to cast a value to a class other than its own.  25.1 Statics  We will develop a system of reﬁnements for the extension with sums and products of the language HPCF deﬁned in Chapter 23 in which there are but two classes of values of type dyn, namely num and fun.1 The syntax of reﬁnements φ is given by the following grammar:  Ref φ ::= true{τ}   cid:31 τ  and{τ} φ1;φ2  φ1 ∧τ φ2 num ! φ new[num] φ  fun ! φ new[fun] φ  φ1 × φ2 prod φ1; φ2  φ1 + φ2 sum φ1; φ2  parr φ1; φ2  φ1  cid:19  φ2  truth conjunction dynamic number dynamic function product sum function  Informally, a reﬁnement is a predicate specifying a property of the values of some type. Equivalently, one may think of a reﬁnement as a subset of the values of a type, those that satisfy the speciﬁed property. To expose the dependence of reﬁnements on types, the syntax of truth and conjunction is parameterized by the type whose values they govern. In most cases, the underlying type is clear from context, in which case it is omitted. Note that the syntax of the product, sum, and function reﬁnements is exactly the same as the syntax of the types they govern, but they are reﬁnements, not types. The judgment φ   τ means that φ is a reﬁnement of the type τ. It is deﬁned by the  following rules:   cid:31   τ  φ1   τ φ2   τ φ1 ∧ φ2   τ φ   nat  num ! φ   dyn φ   dyn  cid:19  dyn fun ! φ   dyn φ1   τ1 φ2   τ2 φ1 × φ2   τ1 × τ2   25.1a    25.1b    25.1c    25.1d    25.1e    221  25.1 Statics  φ1   τ1 φ2   τ2 φ1 + φ2   τ1 + τ2 φ1   τ1 φ2   τ2 φ1  cid:19  φ2   τ1  cid:19  τ2  It is easy to see that each reﬁnement reﬁnes a unique type, the underlying type of that reﬁnement. The concrete syntax num ! φ and fun ! φ is both concise and commonplace, but, beware, it tends to obscure the critical distinctions between types, classes, and reﬁnements. The reﬁnement satisfaction judgment, e ∈τ φ, where e : τ and φ   τ, states that the well-typed expression e exhibits the behavior speciﬁed by φ. The hypothetical form,  x1 ∈τ1 φ1, . . . , xn ∈τn φn  cid:12  e ∈τ φ,  constrains the expressions that may be substituted for a variable to satisfy its associated reﬁnement type  which could be the trivial reﬁnement  cid:31 , that imposes no constraints . We write  cid:22  for such a ﬁnite sequence of reﬁnement assumptions on variables, called a reﬁnement context. Each such  cid:22  determines a typing context  cid:7  given by x1 : τ1, . . . , xn : τn, specifying only the types of the variables involved. We often write  cid:22  cid:7  to state that  cid:7  is the unique typing context determined by  cid:22  in this way. The deﬁnition of the reﬁnement satisfaction judgment makes use of an auxiliary judg- ment, φ1 ≤τ φ2, where φ1   τ and φ2   τ, which we shall often just write as φ1 ≤ φ2 when τ is clear from context. This judgment is called reﬁnement entailment. It states that the reﬁnement φ1 is at least as strong as, or no weaker than, the reﬁnement φ2. Informally, this means that if e : τ satisﬁes φ1, then it must also satisfy φ2. According to this interpretation, reﬁnement entailment is reﬂexive and transitive. The reﬁnement  cid:31 , which holds of any well-typed expression, is greater than  entailed by  any other reﬁnement, and the conjunc- tion of two reﬁnements, φ1 ∧ φ2 is the meet  greatest lower bound  of φ1 and φ2. Because no value can be of two different classes, the conjunction of num ! φ1 and fun ! φ2 entails any reﬁnement all. Finally, reﬁnement entailment satisﬁes the same variance principles given for subtyping in Section 24.3.   25.1f    25.1g    25.2a    25.2b    25.2c    25.2d    25.2e    25.2f   φ1 ≤τ φ2 φ2 ≤τ φ3  φ   τ φ ≤τ φ  φ1 ≤τ φ3 φ   τ φ ≤τ  cid:31   φ1   τ φ2   τ φ1 ∧ φ2 ≤τ φ1 φ1   τ φ2   τ φ1 ∧ φ2 ≤τ φ2 φ ≤τ φ1 φ ≤τ φ2  φ ≤τ φ1 ∧ φ2   222  Behavioral Typing  num ! φ1 ∧ fun ! φ2 ≤dyn φ  φ ≤nat φ   cid:5   num ! φ ≤dyn num ! φ   cid:5   φ ≤dyn cid:19 dyn φ   cid:5    cid:5    cid:5  2   cid:5  1  fun ! φ ≤dyn fun ! φ φ1 ≤τ1 φ 1 φ2 ≤τ2 φ  cid:5   cid:5  2 φ1 × φ2 ≤τ1×τ2 φ × φ 1 φ2 ≤τ2 φ φ1 ≤τ1 φ  cid:5   cid:5  2 φ1 + φ2 ≤τ1+τ2 φ + φ ≤τ1 φ1 φ2 ≤τ2 φ  cid:5   cid:5  2 1  cid:5  1  cid:19  φ  φ1  cid:19  φ2 ≤τ1 cid:19 τ2 φ   cid:5  2   cid:5  1  φ   cid:5  2   cid:22  cid:7 , x ∈τ φ  cid:12  x ∈τ φ  cid:22   cid:12  e ∈τ φ  cid:5  ≤τ φ  φ   cid:5    cid:22   cid:12  e ∈τ φ   cid:22   cid:12  e ∈τ φ1  cid:22   cid:12  e ∈τ φ2   cid:22   cid:12  e ∈τ φ1 ∧ φ2  cid:22 , x ∈τ φ  cid:12  e ∈τ φ  cid:22   cid:12  fix x : τ is e ∈τ φ   25.2g    25.2h    25.2i    25.2j    25.2k    25.2l    25.3a    25.3b    25.3c    25.3d   For the sake of brevity, we usually omit the type subscripts from reﬁnements and the reﬁnement entailment relation. We are now in a position to deﬁne the reﬁnement satisfaction judgment,  cid:22  cid:7   cid:12  e ∈τ φ, in which we assume that  cid:7   cid:12  e : τ. When such a satisfaction judgment holds, we say that e is well-reﬁned, a property that can be stated only for expressions that are well-typed. The goal is to ensure that well-reﬁned expressions do not incur  checked  run-time errors. In  cid:6 = c. The the present setting reﬁnements rule out casting a value of class c to a classc formulation of satisfaction, though simple-minded, will involve many important ideas.   cid:5   To simplify the exposition, it is best to present the rules in groups, rather than all at once. The ﬁrst group consists of the rules that pertain to expressions independently of their types.  Rule  25.3a  expresses the obvious principle that if a variable is assumed to satisfy a reﬁnement φ, then of course it does satisfy that reﬁnement. As usual, the principle of substitution is admissible. It states that if a variable is assumed to satisfy φ, then we may substitute for it any expression that does satisfy that reﬁnement, and the resulting instance will continue to satisfy the same reﬁnement it had before the substitution.  Rule  25.3b  is analogous to the subsumption principle given in Chapter 24, although here it has a subtly different meaning. Speciﬁcally, if an expression e satisﬁes a reﬁnement   223  25.1 Statics   cid:5  logically entails φ.   cid:5  is stronger than some reﬁnement φ  as determined by rules  25.2  , then e must  cid:5 , and φ φ also satisfy the reﬁnement φ. This inference is simply a matter of logic: the judgment  cid:5  ≤ φ states that φ φ Rule  25.3c  expresses the logical meaning of conjunction. If an expression e satisﬁes both φ1 and φ2, then it also satisﬁes φ1 ∧ φ2. Rule  25.3b  ensures that the converse holds as well, noting that by rules  25.2d  and  25.2e  a conjunction is stronger than either of its conjuncts. Similarly, the same rule ensures that if e satisﬁes φ, then it also satisﬁes  cid:31 , but we do not postulate that every well-typed expression satisﬁes  cid:31 , for that would defeat the goal of ensuring that well-reﬁned expressions do not incur a run-time fault.  Rule  25.3d  states that reﬁnements are closed under general recursion  formation of ﬁxed points . To show that fix x : τ is e satisﬁes a reﬁnement φ, it sufﬁces to show that e satisﬁes φ, under the assumption that x, which stands for the recursive expression itself, satisﬁes φ. It is thus obvious that non-terminating expressions, such as fix x :τ is x, satisfy any reﬁnement at all. In particular, such a divergent expression does not incur a run-time error, the guarantee we are after with the present system of reﬁnements.  The second group concerns the type dyn of classiﬁed values.   cid:22   cid:12  e ∈nat φ   cid:22   cid:12  num ! e ∈dyn num ! φ   cid:22   cid:12  e ∈dyn cid:19 dyn φ   cid:22   cid:12  fun ! e ∈dyn fun ! φ   cid:22   cid:12  e ∈dyn num ! φ  cid:22   cid:12  e @ num ∈nat φ  cid:22   cid:12  e ∈dyn cid:19 dyn fun ! φ  cid:22   cid:12  e @ fun ∈dyn cid:19 dyn φ   cid:22   cid:12  e ∈dyn  cid:31    cid:22   cid:12  num ? e ∈bool  cid:31    cid:22   cid:12  e ∈dyn  cid:31    cid:22   cid:12  fun ? e ∈bool  cid:31    25.4a    25.4b    25.4c    25.4d    25.4e    25.4f   Rules  25.4a  and  25.4b  state that a newly created value of class c satisﬁes the reﬁnement of the type dyn stating this fact, provided that the underlying value satisﬁes the given reﬁnement φ of the type associated to that class  nat for num and dyn  cid:19  dyn for fun .  Rules  25.4c  and  25.4d  state that a value of type dyn may only be safely cast to a class c if the value is known statically to be of this class. This condition is stated in the premises of these rules, which require the class of the cast value to be known and suitable for the cast. The result of a well-reﬁned cast satisﬁes the reﬁnement given to its underlying value. It is important to realize that in the quest to avoid run-time faults, it is not possible to cast a value whose only known reﬁnement is  cid:31 , which imposes no restrictions on it. This limitation is burdensome, because in many situations it is not possible to determine statically what is the class of a value. We will return to this critical point shortly.   224  Behavioral Typing  Rules  25.4e  and  25.4f  compute a boolean based on the class of its argument. We shall  have more to say about this in Section 25.2.  The third group of rules govern nullary and binary product types.  Rule  25.5a  states the obvious: the null-tuple is well-reﬁned by the trivial reﬁnement. Because unit contains only one element, little else can be said about it. Rule  25.5b  states that a pair satisﬁes a product of reﬁnements if each component satisﬁes the corresponding reﬁnement. Rules  25.5c  and  25.5d  state the converse.  The fourth group of rules govern nullary and binary sum types.   cid:22   cid:12   cid:24  cid:25  ∈unit  cid:31    cid:22   cid:12  e1 ∈τ1 φ1  cid:22   cid:12  e2 ∈τ2 φ2  cid:22   cid:12   cid:24 e1, e2 cid:25  ∈τ1×τ2 φ1 × φ2   cid:22   cid:12  e ∈τ1×τ2 φ1 × φ2  cid:22   cid:12  e · l ∈τ1 φ1  cid:22   cid:12  e ∈τ1×τ2 φ1 × φ2  cid:22   cid:12  e · r ∈τ1 φ1   cid:22   cid:12  e ∈void φ  cid:5   cid:22   cid:12  e ∈void φ  cid:22   cid:12  e1 ∈τ1 φ1   cid:22   cid:12  l · e1 ∈τ1+τ2 φ1 + φ2   cid:22   cid:12  e2 ∈τ2 φ2   cid:22   cid:12  r · e2 ∈τ1+τ2 φ1 + φ2   25.5a    25.5b    25.5c    25.5d    25.6a    25.6b    25.6c    25.6d    cid:22   cid:12  e ∈τ1+τ2 φ1 + φ2  cid:22 , x1 ∈τ1 φ1  cid:12  e1 ∈τ φ  cid:22 , x2 ∈τ2 φ2  cid:12  e2 ∈τ φ   cid:22   cid:12  case e {l · x1  cid:9 → e1  r · x2  cid:9 → e2} ∈τ φ  Rule  25.6a  states that if an expression of type void satisﬁes some reﬁnement  and hence is error-free , then it satisﬁes every reﬁnement  of type void , because there are no values of this type, and hence, being error-free, must diverge. Rules  25.6b  and  25.6c  are similarly motivated. If e1 satisﬁes φ1, then l · e1 satisﬁes φ1 + φ2 for any reﬁnement φ2, precisely because the latter reﬁnement is irrelevant to the injection. Similarly, the right injection is independent of the reﬁnement of the left summand. Rule  25.6d  is in some respects the most interesting rule of all, one that we shall have occasion to revise shortly. The salient feature of this rule is that it propagates reﬁnement information about the injected value into the corresponding branch by stating an assumption about the bound variable of each branch. But it does not propagate any information into the branches about what is known about e in each branch, namely that in the ﬁrst branch e must be of the form l · e1, and in the second branch, e must be of the form r · e2. The failure to propagate this information may seem harmless, but it is, in fact, quite restrictive. To see why, consider the special case of the type bool, which is deﬁned in Section 11.3.2 to be unit + unit. The conditional expression if e then e1 else e2 is    25.7a    25.7b    25.8a    25.8b    25.8c   225  25.1 Statics  deﬁned to be a case analysis in which there is no associated data to pass into the branches of the conditional. So, within the then branch e1 it is not known statically that e is in fact true, nor is it known within the else branch e2 that e is in fact false.  The ﬁfth group of rules governs the function type.   cid:22 , x ∈τ1 φ1  cid:12  e2 ∈τ2 φ2   cid:22   cid:12  λ  x : τ1  e2 ∈τ1 cid:19 τ2 φ1  cid:19  φ2  cid:22   cid:12  e1 ∈τ2 cid:19 τ φ2  cid:19  φ e2 ∈τ2 φ2   cid:22   cid:12  e1 e2  ∈τ φ  Rule  25.7a  states that a λ-abstraction satisﬁes a function reﬁnement if its body satisﬁes the range reﬁnement, under the assumption that its argument satisﬁes the domain reﬁnement. This is only to be expected.  Rule  25.7b  states the converse. If an expression of function type satisﬁes a function reﬁnement, and it is applied to an argument satisfying the domain reﬁnement, then the application must satisfy the range reﬁnement. The last group of rules govern the type nat:   cid:22   cid:12  z ∈nat  cid:31   cid:22   cid:12  e ∈nat  cid:31   cid:22   cid:12  s e  ∈nat  cid:31    cid:22   cid:12  e ∈nat  cid:31   cid:22   cid:12  e0 ∈τ φ  cid:22 , x ∈nat  cid:31   cid:12 e 1 ∈τ φ   cid:22   cid:12  ifz e {z  cid:9 → e0  s x   cid:9 → e1} ∈τ φ  These rules are completely unambitious: they merely restate the typing rules as reﬁnement rules that impose no requirements and make no guarantees of any properties of the natural numbers. One could envision adding reﬁnements of the natural numbers, for instance stating that a natural number is known to be zero or non-zero, for example.  To get a feel for the foregoing rules, it is useful to consider some simple examples. First,  we may combine rules  25.3b  and  25.4a  to derive the judgment  num ! n ∈dyn  cid:31   for any natural number n. That is, we may “forget” the class of a value by applying subsumption and appealing to rule  25.2c . Second, such reasoning is essential in stating reﬁnement satisfaction for a boolean conditional  or, more generally, any case analysis . For example, the following judgment is directly derivable without subsumption:  cid:22 , x ∈bool  cid:31   cid:12  if x then  num ! z  else  num ! s z   ∈dyn num !  cid:31 .  But the following judgment is only derivable because we may weaken knowledge of the class of a value in each branch to a common reﬁnement, in this case the weakest reﬁnement of all:   cid:22 , x ∈bool  cid:31   cid:12 if x then  num ! z  else  fun !  λ  y : dyn  y   ∈dyn  cid:31 .   226  Behavioral Typing  In general conditionals attenuate the information we have about a value, except in those cases where the same information is known about both branches. Conditionals are the main source of lossiness in checking reﬁnement satisfaction.  Conjunction reﬁnements are used to express multiple properties of a single expression.  For example, the identity function on the type dyn satisﬁes the conjunctive reﬁnement   num !  cid:31   cid:19  num !  cid:31   ∧  fun !  cid:31   cid:19  fun !  cid:31  .  The occurrences of  cid:31  in the ﬁrst conjunct reﬁne the type nat, whereas the occurrences in the second conjunct reﬁne the type dyn  cid:19  dyn. It is a good exercise to check that λ  x : dyn  x satisﬁes the above reﬁnement of the type dyn  cid:19  dyn.  25.2 Boolean Blindness  Let us consider a very simple example that exposes a serious disease suffered by many programming languages, a condition called boolean blindness. Suppose that x is a variable of type dyn with reﬁnement  cid:31 , and consider the expression  if  num ? x  then x @ num else z.  Although it is clear that this expression has type nat, it is nevertheless ill-reﬁned  satisﬁes no reﬁnement , even though it does not incur a run-time error. Speciﬁcally, within the then branch, we as programmers know that x is a value of class num, but this fact is not propagated into the then branch by rules  25.6   of which the boolean conditional is a special case . Consequently, rule  25.4c  does not apply  formally we do not know enough about x to cast it safely , so the expression is ill-reﬁned. The branches of the conditional are “blind” both to the outcome and to the meaning of the boolean value computed by the test whether x is of class num.  Boolean blindness is endemic among programming languages. The difﬁculty is that a boolean carries exactly one bit of information, which is not sufﬁcient to capture the meaning of that bit. A boolean, which is  literally  a bit of data, ought to be distinguished from a proposition, which expresses a fact. Taken by itself, a boolean conveys no information other than its value, whereas a proposition expresses the reasoning that goes into ensuring that a piece of code is correct. In terms of the above example knowing that num ? x evaluates dynamically to the boolean true is not connected statically with the fact that the class of x is num or not. That information lives elsewhere, in the speciﬁcation of the class test primitive. The question is how to connect the boolean value returned by the class test with relevant facts about whether the class of x is num or not.  Because the purpose of a type reﬁnement system is precisely to capture such facts in a form that can be stated and veriﬁed, one may suspect that the difﬁculty with the foregoing example is that the system of reﬁnements under consideration is too weak to capture the property that we need to ensure that run-time faults do not occur. The example shows that something more is needed if type reﬁnement is to be useful, so we ﬁrst consider what might   227  25.2 Boolean Blindness  be involved in enriching the deﬁnition of reﬁnement satisfaction to ensure that the proper connections are made. The matter boils down to two issues:  1. Propagating that num ? x returned true into the then branch, and that it returned false  into the else branch.  2. Connecting facts about the return value of a test to facts about the value being tested.  These are, in the present context, distinct aspects of the problem. Let us consider them in turn. The upshot of the discussion will be to uncover a design ﬂaw in casting and to suggest an alternative formulation that does not suffer from boolean blindness. To address the problem as stated, we ﬁrst need to enrich the language of reﬁnements to include true   bool and false   bool, stating that a boolean is either true or false, respectively. That way we can hope to express facts about boolean-valued expressions as reﬁnement judgments. But what is the reﬁnement rule for the boolean conditional? As a ﬁrst guess one might consider something like the following:   cid:22 , e ∈bool true  cid:12  e1 ∈τ φ  cid:22 , e ∈bool false  cid:12  e2 ∈τ φ   cid:22   cid:12  if e then e1 else e2 ∈τ φ   25.9   Such a rule is a bit unusual in that it introduces a hypothesis about an expression, rather than a variable, but let us ignore that for the time being and press on. Having re-formulated the reﬁnement rule for the conditional, we immediately run into another problem: how to deduce that x ∈dyn num !  cid:31 , which is required for casting, from the assumption that num ? x ∈bool true? One way to achieve this is by modifying the reﬁne- ment rule for the conditional to account for the special case in which the boolean expression is literally num ? e for some e. If it is, then we propagate into the then branch the additional assumption e ∈dyn num !  cid:31 , but if it is not, then no fact about the class of e is propagated into the else branch.2 This change is enough to ensure that the example under consideration is well-reﬁned. But what if the boolean on which we are conditioning is not literally num?e, but merely implies the test would return true? How then are we to make the connection between such expressions and relevant facts about the class of e? Clearly, there is no end to the special cases we could consider to restore vision to reﬁnements, but there is no unique best solution. All is not lost, however! The foregoing analysis suggests that the fault lies not in our reﬁnements, but in our language design. The sole source of run-time errors is the “naked cast” that attempts to extract the underlying natural number from a value of type dyn—and signals a run-time error if it cannot do so because the class of the value is not num. The boolean-valued test for the class of a value seems to provide a way to avoid these errors. But, as we have just seen, the true cause of the problem is the attempt to separate the test from the cast. We may, instead, combine these into a single form, say  ifofcl[num] e;x0.e0;e1 ,  that tests whether the class of e is num, and, if so, passes the underlying number to e0 by substituting it for x0, and otherwise evaluates e1. No run-time error is possible, and hence no reﬁnements are needed to ensure that it cannot occur.   228  Behavioral Typing  But does this not mean that type reﬁnements are pointless? Not at all. It simply means that in some cases veriﬁcation methods, such as type reﬁnements, are needed solely to remedy a language design ﬂaw, and not provide a useful tool for programmers to help express and verify the correctness of their programs. We may still wish to express invariants such  cid:5 , as the property that a particular function maps values of class c into values of class c simply for the purpose of stating a programmer’s intention. Or we may enrich the system of reﬁnements to track properties such as whether a number is even or odd, and to state conditions such as that a given function maps evens to odds and odds to evens. There is no limit, in principle, to the variations and extensions to help ensure that programs behave as expected.  25.3 Reﬁnement Safety  The judgment  cid:22  cid:7   cid:12  a ∈τ φ presupposes that  cid:7   cid:12  e : τ, so by adapting the proofs given earlier, we may type preservation and progress for well-typed terms.  Theorem 25.1  Type Safety . Suppose that e : τ for closed e. 1. If e  cid:20 −→ e 2. Either e err, ore val, or there exists e   cid:5  such that e  cid:20 −→ e   cid:5 , then e   cid:5  : τ .   cid:5 .  The proof of progress requires a canonical forms lemma, which for the type dyn is stated  as follows: Lemma 25.2  Canonical Forms . Suppose that e : dyn and e val. Then either e = num ! e or e = fun ! e   cid:5  for some e   cid:5 .   cid:5    cid:5  would also be a value under an eager dynamics but need not be under a The expression e lazy dynamics. The proof of Lemma 25.2 proceeds as usual, by analyzing the typing rules for values.  The goal of the reﬁnement system introduced in Section 25.1 is to ensure that errors cannot arise in a well-reﬁned program. To show this, we ﬁrst show that the dynamics preserves reﬁnements. Lemma 25.3. Suppose that e val and e ∈dyn φ. If φ ≤ num ! φ  cid:5  ∈   cid:5 , then e = num ! e   cid:5 , and if φ ≤ fun ! φ   cid:5 , then e = fun ! e   cid:5  ∈dyn cid:19 dyn φ   cid:5 , where e   cid:5 , where   cid:5 .  e  nat φ  Proof The proof requires Lemma 25.2 to characterize the possible values of dyn, and an analysis of the reﬁnement satisfaction rules. The lemma accommodates rule  25.3b , which appeals to the transitivity of reﬁnement entailment.   229  25.4 Notes  Theorem 25.4  Reﬁnement Preservation . If e ∈τ φ and e  cid:20 −→ e   cid:5  ∈τ φ.  cid:5  ∈τ φ we proceed Proof We know by the preceding theorem that e by induction on the deﬁnition of reﬁnement satisfaction given in Section 25.1. The type- independent group, rules  25.3 , are all easily handled, apart from the rule for ﬁxed points, which requires an appeal to a substitution lemma, just as in Theorem 19.2. The remaining groups are all handled easily, bearing in mind that an incorrect expression cannot make a transition.   cid:5 , then e  cid:5  : τ. To show that e  Theorem 25.5  Reﬁnement Error-Freedom . If e ∈τ φ, then ¬ e err .  Proof By induction on the deﬁnition of reﬁnement satisfaction. The only interesting cases are rules  25.4c  and  25.4d , which are handled by appeal to Lemma 25.3 in the case that the cast expression is a value.  Corollary 25.6  Reﬁnement Safety . If e ∈τ φ, then either e val or there exists e  cid:5  ∈τ φ and e  cid:20 −→ e   cid:5 . In particular, ¬ e err .  e   cid:5  such that  Proof By Theorems 25.1, 25.4, and 25.5.  25.4 Notes  The distinction between types and reﬁnements is fundamental; yet, the two are often conﬂated. Types determine the structure of a programming language, including its statics and dynamics; reﬁnements specify the behavior of well-typed programs. In full generality, the satisfaction judgment e ∈τ φ need not be decidable, whereas it is sensible to insist that the typing judgment e : τ be decidable. The reﬁnement system presented in this chapter is decidable, but one may consider many notions of reﬁnement that are not. For example, one may postulate that if e ∈τ φ and that e  cid:5  is indistinguishable from e by any program in the  cid:5  ∈τ φ. In contrast such a move is not sensible in a type system, because language,3 then e the dynamics is derived from the statics by the inversion principle. Therefore, reﬁnement is necessarily posterior to typing.  The syntactic formulation of type reﬁnements considered in this chapter was originally given by Freeman and Pfenning  1991  and extended by Davies and Pfenning  2000 , Davies  2005 , Xi and Pfenning  1998 , Dunﬁeld and Pfenning  2003 , and Mandelbaum et al.  2003 . A more general semantic formulation of type reﬁnement was given explicitly by Denney  1998  in the style of the realizability interpretation of type theory on which NuPRL  Constable, 1986  is based.  See the survey by van Oosten  2002  for the history of the realizability interpretations of constructive logic.    230  Behavioral Typing  Exercises  25.1. Show that if φ1 ≤ φ 25.2. Show that φ ≤ φ  1 and φ2 ≤ φ 2, then φ1 ∧ φ2 ≤ φ  cid:5   cid:5   cid:5  cid:5 , if φ  cid:5  iff for every φ  ∧ φ  cid:5  cid:5  ≤ φ, then φ   cid:5  1   cid:5  2.  cid:5  cid:5  ≤ φ   cid:5 .  This property of  entailment is an instance of the more general Yoneda Lemma in category theory.   25.3. Extend the system of reﬁnements to recursive types by introducing a reﬁnement fold φ  that classiﬁes values of recursive type rec t is τ in terms of a reﬁnement of the unfolding of that recursive type.  25.4. Consider the following two forms of reﬁnement for sum types, summand reﬁnements:  φ1   τ1  l · φ1   τ1 + τ2  φ2   τ2   25.10a   r · φ2   τ1 + τ2   25.10b  Informally, l·φ1 classiﬁes expressions of type τ1+τ2 that lie within the left summand and whose underlying value satisﬁes φ1, and similarly for r · φ2.  a  State entailment rules governing summand reﬁnements.  b  State reﬁnement rules assigning summand reﬁnements to the introduction forms  of a sum type.   c  Give rules for the case analysis construct using summand reﬁnements to allow  unreachable branches to be disregarded during reﬁnement checking.   d  Modify rule  25.6d  so that the information “learned” by examining the value of e at execution time is propagated into the appropriate branch of the case analysis. Check the importance of this extension to the prospects for a cure for Boolean blindness.  25.5. Using the preceding exercise, derive the reﬁnements num ! φ and fun ! φ from the other reﬁnement rules, including the reﬁnement fold φ  considered in Exercise 25.3. 25.6. Show that the addition function  23.8 , a value of type dyn, satisﬁes the reﬁnement  fun !  num !  cid:31   cid:19  fun !  num !  cid:31   cid:19  num !  cid:31   ,  stating that  a  It is itself a value of class fun.  b  The so-classiﬁed function maps a value of class num to a result, if any, of class  fun.  num.   c  The so-classiﬁed function maps a value of class num to a result, if any, of class  This description exposes the hidden complexity in the superﬁcial simplicity of a uni-typed language.  25.7. Revisit the optimization process of the addition function carried out in Section 23.3 in view of your answer to Exercise 25.6. Show that the validity of the optimizations is guaranteed by the satisfaction of the stated type reﬁnement for addition.   231  Notes  Notes  1 Of course, in a richer language, there would be more classes than just these two, each with an  associated type of the underlying data that it classiﬁes.  2 For the special case of there being exactly two classes, we could propagate that the class of e is  fun into the else branch, but this approach does not generalize.  3 See Chapter 47 for a precise deﬁnition and development of this concept.    P A R T XI  Dynamic Dispatch    26  Classes and Methods  It often arises that the values of a type are partitioned into a variety of classes, each classifying data with distinct internal structure. A simple example is provided by the type of points in the plane, which are classiﬁed according to whether they are represented in cartesian or polar form. Both are represented by a pair of real numbers, but in the cartesian case, these are the x and y coordinates of the point, whereas in the polar case, these are its distance r from the origin and its angle θ with the polar axis. A classiﬁed value is an object, or instance, of its class. The class determines the type of the classiﬁed data, the instance type of the class; the classiﬁed data itself is the instance data of the object.  Methods are functions that act on classiﬁed values. The behavior of a method is deter- mined by the class of its argument. The method dispatches on the class of the argument.1 Because the selection is made at run-time, it is called dynamic dispatch. For example, the squared distance of a point from the origin is calculated differently according to whether the point is represented in cartesian or polar form. In the former case, the required distance is x2 + y2, whereas in the latter it is simply r2. Similarly, the quadrant of a cartesian point can be determined by examining the sign of its x and y coordinates, and the quadrant of a polar point can be calculated by taking the integral part of the angle θ divided by π 2.  Dynamic dispatch is often described in terms of a particular implementation strategy, which we will call the class-based organization. In this organization, each object is repre- sented by a vector of methods specialized to the class of that object. We may equivalently use a method-based organization in which each method branches on the class of an ob- ject to determine its behavior. Regardless of the organization used, the fundamental idea is that  a  objects are classiﬁed and  b  methods dispatch on the class of an object. The class-based and method-based organizations are interchangeable and, in fact, related by a natural duality between sum and product types. We explain this symmetry by focusing ﬁrst on the behavior of each method on each object, which is given by a dispatch matrix. From this, we derive both a class-based and a method-based organization in such a way that their equivalence is obvious.  26.1 The Dispatch Matrix  Because each method acts by dispatch on the class of its argument, we may envision the entire system of classes and methods as a dispatch matrix edm whose rows are classes, whose columns are methods, and whose  c, d -entry deﬁnes the behavior of method d   236  Classes and Methods  acting on an argument of class c, expressed as a function of the instance data of the object. Thus, the dispatch matrix has a type of the form   cid:15    cid:15   c∈C  d∈D   τ c  cid:19  ρd ,  where C is the set of class names, D is the set of method names, τ c is the instance type associated with class c, and ρd is the result type of method d. The instance type is the same for all methods acting on a given class, and the result type is the same for all classes acted on by a given method.  As an illustrative example, let us consider the type of points in the plane classiﬁed into two classes, cart and pol, that corresponds to the cartesian and polar representations. The instance data for a cartesian point has the type  τ cart =  cid:24 x  cid:9 → float, y  cid:9 → float cid:25 ,  and the instance data for a polar point has the type  τ pol =  cid:24 r  cid:9 → float, th  cid:9 → float cid:25 .  Consider two methods acting on points, dist and quad, which compute, respectively, the squared distance of a point from the origin and the quadrant of a point. The squared distance method is given by the tuple edist =  cid:24 cart  cid:9 → ecart  dist, pol  cid:9 → epol   cid:25  of type  dist   cid:24 cart  cid:9 → τ cart  cid:19  ρdist, pol  cid:9 → τ pol  cid:19  ρdist cid:25 ,  where ρdist = float is the result type,  = λ  u : τ cart   u · x 2 +  u · y 2  ecart dist  is the squared distance computation for a cartesian point, and  = λ  v : τ pol   v · r 2  epol dist  is the squared distance computation for a polar point. Similarly, the quadrant method is given by the tuple equad =  cid:24 cart  cid:9 → ecart  quad, pol  cid:9 → epol   cid:25  of type  quad   cid:24 cart  cid:9 → τ cart  cid:19  ρquad, pol  cid:9 → τ pol  cid:19  ρquad cid:25 ,  where ρquad = [I, II, III, IV] is the type of quadrants, and ecart quad are expressions that compute the quadrant of a point in rectangular and polar forms, respectively. Now let C = { cart, pol} and let D = { dist, quad}, and deﬁne the dispatch matrix  quad and epol  edm to be the value of type  c∈C such that, for each class c and method d,  d∈D   cid:15    cid:15    τ c  cid:19  ρd   edm · c · d  cid:20 −→∗  ec d .  That is, the entry in the dispatch matrix edm for class c and method d deﬁnes the behavior of that method acting on an object of that class.   237  26.1 The Dispatch Matrix  Dynamic dispatch is an abstraction given by the following components:   An abstract type tobj of objects, which are classiﬁed by the classes on which the methods act.   An operation new[c] e  of type tobj that creates an object of the class c with instance data given by the expression e of type τ c.   An operation e ⇐ d of type ρd that invokes method d on the object given by the expression e of type tobj.  These operations must satisfy the deﬁning characteristic of dynamic dispatch,   new[c] e  ⇐ d  cid:20 −→∗  d e , ec  which states that invoking method d on an object of class c with instance data e amounts to applying ec d, the code in the dispatch matrix for class c and method d to the instance data e. In other words, dynamic dispatch is an abstract type with interface given by the existential  type  ∃ tobj. cid:24 new  cid:9 →  τ c  cid:19  tobj, snd  cid:9 →  tobj  cid:19  ρd cid:25  .   26.1    cid:15   d∈D   cid:15   c∈C  There are two main ways to implement this abstract type. The class-based organization, deﬁnes objects as tuples of methods and creates objects by specializing the methods to the given instance data. The method-based organization creates objects by tagging the instance data with the class and deﬁnes methods by examining the class of the object. These two organizations are isomorphic to one another and hence can be interchanged at will. Nevertheless, many languages favor one representation over the other, asymmetrizing an inherently symmetric situation.  The abstract type  26.1  calls attention to shortcoming of dynamic dispatch, namely that a message can be sent to exactly one object at a time. This viewpoint seems natural in certain cases, such as discrete event simulation in the language Simula-67. But often it is essential to act on several classes of object at once. For example, the multiplication of a vector by a scalar combines the elements of a ﬁeld and a commutative monoid; there is no natural way to associate scalar multiplication with either the ﬁeld or the monoid, nor any way to anticipate that particular combination. Moreover, the multiplication is not performed by checking at run-time that one has a scalar and a vector in hand, for there is nothing inherent in a scalar or a vector that marks them as such. The right tool for handling such situations is a module system  Chapters 44 and 45 , not dynamic dispatch. The two mechanisms serve different purposes and complement each other.  The same example serves to refute a widely held fallacy, namely that the values of an abstract type cannot be heterogeneous. It is sometimes said that an abstract type of complex numbers must commit to a single representation, say, rectangular, and cannot accommodate multiple representations. This is a fallacy. Although it is true that an abstract type deﬁnes a single type, it is wrong to say that only one representation of objects is possible. The abstract type can be implemented as a sum, and the operations may correspondingly dispatch on   238  Classes and Methods  the summand to compute the result. Dynamic dispatch is a mode of use of data abstraction, and therefore cannot be opposed to it.  26.2 Class-Based Organization  The class-based organization starts with the observation that the dispatch matrix can be reorganized to “factor out” the instance data for each method acting on that class to obtain the class vector ecv of type   cid:15   c∈C   cid:15   d∈D  τcv  cid:2    τ c  cid:19     ρd  .  An object has the type ρ = cid:5   Each entry of the class vector consists of a constructor that determines the result of each of the methods when acting on given instance data.  methods. For example, in the case of points in the plane, the type ρ is the product type  d∈D ρd consisting of the product of the result types of the   cid:24 dist  cid:9 → ρdist, quad  cid:9 → ρquad cid:25 .  Each component speciﬁes the result of the methods acting on that object. The message send operation e ⇐ d is just the projection e · d. So, in the case of points in the plane, e ⇐ dist is the projection e · dist, and similarly e ⇐ quad is the projection e · quad.  The class-based organization combines the implementation of each class into a class vector ecv a tuple of type τcv, consisting of the constructor of type τ c  cid:19  ρ for each class c ∈ C. The class vector is deﬁned by ecv =  cid:24 c  cid:9 → ec cid:25 c∈C, where for each c ∈ C the expression ec is  λ  u : τ c  cid:24 d  cid:9 → edm · c · d u  cid:25 d∈D.  For example, the constructor for the class cart is the function ecart given by the  expression  λ  u : τ cart  cid:24 dist  cid:9 → edm · cart · dist u , quad  cid:9 → edm · cart · quad u  cid:25 .  Similarly, the constructor for the class pol is the function epol given by the expression  λ  u : τ pol  cid:24 dist  cid:9 → edm · pol · dist u , quad  cid:9 → edm · pol · quad u  cid:25 .  The class vector ecv in this case is the tuple  cid:24 cart  cid:9 → ecart, pol  cid:9 → epol cid:25  of type  cid:24 cart  cid:9 → τ cart  cid:19  ρ, pol  cid:9 → τ pol  cid:19  ρ cid:25 .  An object of a class is obtained by applying the constructor for that class to the instance  data:  new[c] e   cid:2  ecv · c e .  For example, a cartesian point is obtained by writing new[cart]  cid:24 x  cid:9 → x0, y  cid:9 → y0 cid:25  , which is deﬁned by the expression  ecv · cart  cid:24 x  cid:9 → x0, y  cid:9 → y0 cid:25  .   239  26.3 Method-Based Organization  Similarly, a polar point is obtained by writing new[pol] r  cid:9 → r0, th  cid:9 → θ0 , which is deﬁned by the expression  ecv · pol  cid:24 r  cid:9 → r0, th  cid:9 → θ0 cid:25  .  It is easy to check for this organization of points that, for each class c and method d, we may derive   new[c] e  ⇐ d  cid:20 −→∗  cid:20 −→∗   ecv · c e   · d edm · c · d e .  That is, the message send evokes the behavior of the given method on the instance data of the given object.  26.3 Method-Based Organization  The method-based organization starts with the transpose of the dispatch matrix, which has the type   cid:15    cid:15   d∈D  c∈C   τ c  cid:19  ρd .  By observing that each row of the transposed dispatch matrix determines a method, we obtain the method vector emv of type τmv  cid:2   τ c   cid:19  ρd .      cid:15    cid:16   Each entry of the method vector consists of a dispatcher that determines the result as a function of the instance data associated with a given object.  An object is a value of type τ = cid:14   c∈C τ c, the sum over the classes of the instance types.  d∈D  c∈C  For example, the type of points in the plane is the sum type [cart  cid:9 → τ cart, pol  cid:9 → τ pol].  Each point is labeled with its class, specifying its representation as having either cartesian or polar form.  An object of a class c is just the instance data labeled with its class to form an element  of the object type:  new[c] e   cid:2  c · e.  For example, a cartesian point with coordinates x0 and y0 is given by the expression  new[cart]  cid:24 x  cid:9 → x0, y  cid:9 → y0 cid:25    cid:2  cart ·  cid:24 x  cid:9 → x0, y  cid:9 → y0 cid:25 .  Similarly, a polar point with distance r0 and angle θ0 is given by the expression  new[pol]  cid:24 r  cid:9 → r0, th  cid:9 → θ0 cid:25    cid:2  pol ·  cid:24 r  cid:9 → r0, th  cid:9 → θ0 cid:25 .   240  Classes and Methods  The method-based organization consolidates the implementation of each method into the method vector emv of type τmv deﬁned by  cid:24 d  cid:9 → ed cid:25 d∈D, where for each d ∈ D the expression ed : τ  cid:19  ρd is  λ  this : τ  case this{c · u  cid:9 → edm · c · d u }  c∈C .  Each entry in the method vector is a dispatch function that deﬁnes the action of that method on each class of object.  In the case of points in the plane, the method vector has the product type   cid:24 dist  cid:9 → τ  cid:19  ρdist, quad  cid:9 → τ  cid:19  ρquad cid:25 .  The dispatch function for the dist method has the form λ  this : τ  case this{cart · u  cid:9 → edm · cart · dist u   pol · v  cid:9 → edm · pol · dist v }, and the dispatch function for the quad method has the similar form λ  this : τ  case this{cart · u  cid:9 → edm · cart · quad u   pol · v  cid:9 → edm · pol · quad v }. The message send operation e ⇐ d applies the dispatch function for method d to the  object e:  Thus, we have, for each class c and method d  e ⇐ d  cid:2  emv · d e .   new[c] e  ⇐ d  cid:20 −→∗  cid:20 −→∗  emv · d c · e  edm · c · d e   The result is, of course, the same as for the class-based organization.  26.4 Self-Reference  It is often useful to allow methods to create new objects or to send messages to objects. It is not possible to do so using the simple dispatch matrix described in Section 26.1, for the simple reason that there is no provision for self-reference within its entries. This deﬁciency may be remedied by changing the type of the entries of the dispatch matrix to account for sending messages and creating objects, as follows:  ∀ tobj.τcv  cid:19  τmv  cid:19  τ c  cid:19  ρd .   cid:15    cid:15   c∈C  d∈D  The type variable tobj represents the abstract object type.2 The types τcv and τmv, are, respectively, the type of the class and method vectors, deﬁned in terms of the abstract type of objects tobj. They are deﬁned by the equations   cid:15   c∈C  τcv  cid:2    τ c  cid:19  tobj    241  and  26.4 Self-Reference   cid:15   d∈D  τmv  cid:2    tobj  cid:19  ρd .  The component of the class vector corresponding to a class c is a constructor that builds a value of the abstract object type tobj from the instance data for c. The component of the method vector corresponding to a method d is a dispatcher that yields a result of type ρd when applied to a value of the abstract object type tobj.  In accordance with the revised type of the dispatch matrix, the behavior associated to  class c and method d has the form   cid:16  tobj  λ  cv : τcv  λ  mv : τmv  λ  u : τ c  ec d .   cid:5  e  d, an object of class c  The arguments cv and mv are used to create new objects and to send messages to objects.  cid:5  is created by writing Within the expression ec cv · c  cid:5  , which selects the appropriate constructor from the class vector cv and applies it  cid:5  may well be the class c itself; this is one form of self- to the given instance data. The class c  cid:5  e  cid:5  . d a method d reference within ec  cid:5  may well be the method d itself; this is another aspect of self-reference The method d d. within ec   cid:5  with instance data e   cid:5  by writing mv · d  d. Similarly, within ec   cid:5  is invoked on e  To account for self-reference in the method-based organization, the method vector emv will be deﬁned to have the self-referential type [τ tobj]τmv self in which the object type c∈C τ c. The method vector is τ is, as before, the sum of the instance types of the classes, deﬁned by the following equation: emv  cid:2  self mv is cid:24 d  cid:9 → λ  this : τ  case this{c · u  cid:9 → edm · c · d[τ] e where  mv  u }  cid:5    cid:14    cid:5  cv  e  c∈C   cid:25 d∈D,   cid:5  e cv   cid:2   cid:24 c  cid:9 → λ  u : τ c  c · u cid:25 c∈C : [τ tobj]τcv  and   cid:5  e mv   cid:2  unroll mv  : [τ tobj]τmv.  Object creation is deﬁned by the equation  new[c] e   cid:2  c · e : τ  and message send is deﬁned by the equation  e ⇐ d  cid:2  unroll emv  · d e  : ρd .  To account for self-reference in the class-based organization, the class vector ecv will be deﬁned to have the type [ρ tobj]τcv self in which the object type ρ is, as before, the d∈D ρd. The class vector is deﬁned by the product of the result types of the methods   cid:5    242  Classes and Methods  following equation:  ecv  cid:2  self cv is cid:24 c  cid:9 → λ  u : τ c  cid:24 d  cid:9 → edm · c · d[ρ] e   cid:5  cid:5  cv  e  mv  u  cid:25 d∈D cid:25 c∈C ,  cid:5  cid:5   where  and   cid:5  cid:5  e cv   cid:2  unroll cv  : [ρ tobj]τcv   cid:5  cid:5  e mv   cid:2   cid:24 d  cid:9 → λ  this : ρ  this · d cid:25 d∈D : [ρ tobj]τmv.  Object creation is deﬁned by the equation  new[c] e   cid:2  unroll ecv  · c e  : ρ,  and message send is deﬁned by the equation  e ⇐ d  cid:2  e · d : ρd .  The symmetries between the two organizations are striking. They are a reﬂection of the  fundamental symmetries between sum and product types.  The term “object-oriented” means many things to many people, but certainly dynamic dispatch, the action of methods on instances of classes, is one of its central concepts. These characteristic features emerge from the more general concepts of sum, product, and function types, which are useful, alone and in combination, in a wider variety of circumstances. A bias towards either a class- or method-based organization seems misplaced in view of the inherent symmetries of the situation. The dynamic dispatch abstraction given by the type  26.1  admits either form of implementation, as demonstrated in Sections 26.2 and 26.3. The literature on object-oriented programming, of which dynamic dispatch is a signiﬁcant aspect, is extensive. Abadi and Cardelli  1996  and Pierce  2002  give a thorough account of much of this work.  26.5 Notes  Exercises  26.1. Consider the possibility that some methods may only be deﬁned on instances of some classes, so that a message send operation may result in a “not understood” error at run-time. Use the type τ opt deﬁned in Section 11.3.4 to rework the dispatch matrix to account for “not understood” errors. Reformulate the class- and method-based im- plementations of dynamic dispatch using the revised dispatch matrix representation. Proceed in two stages. In the ﬁrst stage, ignore the possibility of self-reference so   243  Exercises  that the behavior associated to a method on an instance of a particular class cannot incur a “not understood” error. In the second stage, use your solution to the ﬁrst stage to further rework the dispatch matrix and the implementations of dynamic dispatch to account for the behavior of a method to include incurring a “not understood” error.  26.2. Type reﬁnements can be used to ensure the absence of speciﬁed “not understood” errors that may otherwise arise in the context of Exercise 26.1. To do so, begin by specifying, for each c ∈ C, a subset Dc ⊆ D of methods that must be well- deﬁned on instances of class c. This deﬁnition determines, for each d ∈ D, the set Cd  cid:2  { c ∈ C  d ∈ Dc } of classes on which method d must be well-deﬁned. Using summand reﬁnements for the type τ opt, deﬁne the type reﬁnement   cid:15   c∈C   cid:15     d∈Dc   cid:15   d∈D\Dc  φdm  cid:2   just  cid:31 τ c  cid:19   cid:31 ρd    ×     cid:31  τ c cid:19 ρd   opt ,  which reﬁnes the type of dispatch matrix to within a permutation of its columns.3 It speciﬁes that if d ∈ Dc, then the dispatch matrix entry for class c and method d must be present and imposes no restriction on any other entry. Assume that edm ∈τdm φdm, as expected. Assume a method-based organization in which the object type tobj is the sum over all classes of their instance types.  a  Deﬁne the reﬁnements inst[c] and admits[d] of tobj stating that e ∈ tobj is an instance of class c ∈ C and that e admits method d ∈ D, respectively. Show that if d ∈ Dc, then inst[c] ≤ admits[d], which is to say that any instance of class c admits any method d for that class.  b  Deﬁne φcv   τcv and φmv   τmv in terms of inst[c] and admits[d] so that ecv ∈τcv φcv and emv ∈τmv φmv. Remember to use the class and method vectors derived in Exercise 26.1.  c  Referring to the deﬁnitions of object creation and message send and of the class- and method vectors derived in Exercise 26.1, conclude that if message d ∈ Dc is sent to an instance of c ∈ C, then no “not understood” error can arise at run-time in a well-reﬁned program.  26.3. Using self-reference, set up a dispatch matrix in which two methods may call one another mutually recursively when invoked on an instance of a class. Speciﬁcally, let num be a class of numbers with instance type τ num = nat, and let ev and od be two methods with result type ρev = ρod = bool. Deﬁne the dispatch entries for methods ev and od for the class num so that they determine, by laborious mutual recursion, whether the instance datum is an even or an odd natural number.  26.4. Generalize the account of self-reference to admit constructors whose arguments may involve objects and methods whose results may involve objects. Speciﬁcally, allow the abstract object type tobj to occur in the instance type τ c of a class c or in the result type ρd of a method d. Rework the development in Section 26.4 to account for this generalization. Hint: Use recursive types as described in Chapter 20.   244  Classes and Methods  Notes  1 More generally, we may dispatch on the class of multiple arguments simultaneously. We concentrate  on single dispatch for the sake of simplicity.  2 The variable tobj is chosen not to occur in any τ c or ρd. This restriction can be relaxed; see  3 Working up to such a permutation is a notational convenience and can be avoided at the expense  Exercise 26.4.  of some clarity in the presentation.   27  Inheritance  In this chapter, we build on Chapter 26 and consider the process of deﬁning the dispatch matrix that determines the behavior of each method on each class. A common strategy is to build the dispatch matrix incrementally by adding new classes or methods to an existing dispatch matrix. To add a class requires that we deﬁne the behavior of each method on objects of that class, and to deﬁne a method requires that we deﬁne the behavior of that method on objects of the classes. The deﬁnition of these behaviors can be given by any means available in the language. However, it is often suggested that a useful means of deﬁning a new class is to inherit the behavior of another class on some methods, and to override its behavior on others, resulting in an amalgam of the old and new behaviors. The new class is often called a subclass of the old class, which is then called the superclass. Similarly, a new method can be deﬁned by inheriting the behavior of another method on some classes, and overriding the behavior on others. By analogy, we may call the new method a sub-method of a given super-method. For the sake of clarity, we restrict attention to the non-self-referential case in the following development.  27.1 Class and Method Extension  We begin by extending a given dispatch matrix, edm, of type   cid:15    cid:15    τ c → ρd   with a new class c type  ∗  c∈C  d∈D  ∈ C and a new method d  cid:15    cid:15    τ c → ρd ,  ∗  ∗  c∈C  d∈D ∗ = D ∪ { d ∗ }.  where C  ∗ = C ∪ { c To add a new class c  ∗ } and D ∗ to the dispatch matrix, we must specify the following information:1  1. The instance type τ c 2. The behavior ec ∗ → ρd.  type τ c  ∗  ∗  of the new class c  ∗.  d of each method d ∈ D on an object of the new class c  ∗, a function of  ∗   ∈ D to obtain a new dispatch matrix e  ∗ dm of   246  Inheritance  These data determine a new dispatch matrix e satisﬁed: 1. For each c ∈ C and d ∈ D, the behavior e 2. For each d ∈ D, the behavior e  ∗ dm  · c  ∗ dm  ∗ · d is given by ec ∗ d .  ∗ dm such that the following conditions are  · c· d is the same as the behavior edm · c· d.  ∗ as a subclass of some class c ∈ C means to deﬁne the behavior ec  ∗ To deﬁne c d to be ec for some  perhaps many  d ∈ D. It is sensible to inherit a method d in this way only if the subtype relationship  d  τ c → ρd <: τ c  ∗ → ρd  ∗  is valid, which will be the case if τ c inherited behavior can be invoked on the instance data of the new class.  <: τ c. This subtyping condition ensures that the ∗ to the dispatch matrix, we must specify the following  Similarly, to add a new method d  information:  1. The result type ρd 2. The behavior ec type τ c → ρd d ∗.  ∗ of the new method d  ∗ of the new method d  ∗. ∗ on an object of each class c ∈ C, a function of  These data determine a new dispatch matrix e satisﬁed: 1. For each c ∈ C and d ∈ D, the behavior e 2. The behavior e ∗.  ∗ is given by ec  · c · d  ∗ dm  ∗ dm  d  ∗ dm such that the following conditions are  · c · d is the same as edm · c · d.  ∗ as a sub-method of some d ∈ D means to deﬁne the behavior ec  To deﬁne d ∗ to be ec for some  perhaps many  classes c ∈ C. This deﬁnition is only sensible if the subtype d relationship  d  τ c → ρd <: τ c → ρd ∗. This subtyping relationship ensures that the result of  ∗  holds, which is the case if ρd <: ρd the old behavior sufﬁces for the new behavior.  We will now consider how inheritance relates to the method- and class-based organiza-  tions of dynamic dispatch considered in Chapter 26.  27.2 Class-Based Inheritance  Recall that the class-based organization given in Chapter 26 consists of a class vector ecv of type   cid:15   c∈C  τcv  cid:2    τ c → ρ ,    cid:5    cid:15   c∈C  ∗  ∗ τ cv   cid:2    τ c → ρ .  τcv ×  τ c  ∗ → ρ ,  247  27.2 Class-Based Inheritance  where the object type ρ is the ﬁnite product type tuple of constructors that specialize the methods to a given object of each class.  d∈D ρd. The class vector consists of a ∗ as described in Section 27.1. The new  Let us consider the effect of adding a new class c  class vector e  ∗ cv has type  There is an isomorphism, written    †, between τ  ∗ cv and the type  which can be used to deﬁne the new class vector e   cid:24 d  cid:9 → ec   cid:24 ecv, λ  u : τ c  ∗  ∗ cv as follows: d  u  cid:25 d∈D cid:25 †  .  ∗  This deﬁnition makes clear that the old class vector ecv is reused intact in the new class vector, which extends the old class vector with a new constructor.  Although the object type ρ is the same both before and after the extension with the new ∗ may differ arbitrarily from that of any other class, the behavior of an object of class c ∗ object, even that of the superclass from which it inherits its behavior. So, knowing that c inherits from c tells us nothing about the behavior of its objects, but only about the means by which the class is deﬁned. Inheritance carries no semantic signiﬁcance but is only a record of the history of how a class is deﬁned. ∗ as described in Section 27.1.  Now let us consider the effect of adding a new method d  The new class vector e  ∗ cv has type   cid:15   c∈C  ∗ τ cv   cid:2    τ c → ρ  ∗   ,   cid:5   ∗ is the product type  ∗, where ρ is the old object type. Using this the new class vector e  ∗ ∗ ρd. There is an isomorphism, written    ‡, between ρ ∗ cv is  d∈D  where ρ and the type ρ × ρd deﬁned by   cid:24 c  cid:9 → λ  u : τ c  cid:24  cid:24 d  cid:9 →   ecv · c  u   · d cid:25 d∈D, ec  ∗ u  cid:25 ‡ cid:25 c∈C .  d  By this construction, the new object type ρ  Observe that each constructor must be re-deﬁned to account for the new method, but the deﬁnition makes use of the old class vector for the deﬁnitions of the old methods. ∗ is a subtype of the old object type ρ. Consequently, any objects with the new method can be used in situations expecting an object without the new method, as might be expected. To avoid redeﬁning old classes when a new method is introduced, we may restrict inheritance so that new methods are only added to new subclasses. Subclasses may then have more methods than super-classes, and objects of the subclass can be provided when an object of the superclass is required.   248  Inheritance  27.3 Method-Based Inheritance  The method-based organization is dual to that of the class-based organization. Recall that the method-based organization given in Chapter 26 consists of a method vector emv of type   cid:15   cid:14   d∈D   cid:15   d∈D  ∗  τmv  cid:2   τ → ρd ,  ∗ mv  τ   cid:2   τ → ρd .  where the instance type τ is the sum type of functions that dispatch on the class of the object to determine their behavior.  c∈C τ c. The method vector consists of a tuple ∗ as described in Section 27.1. The  Let us consider the effect of adding a new method d  new method vector e  ∗ mv has type  There is an isomorphism, written    ‡, between τ  ∗ mv and the type  Using this isomorphism, the new method vector e  ∗ .  τmv ×  τ → ρd ∗ mv is deﬁned as ∗ u }   cid:24 emv, λ  this : τ   case this{c · u  cid:9 → ec  d   cid:25 ‡  .  c∈C  The old method vector is re-used intact, extended with a dispatch function for the new method.  ∗  ρ added method is ignored.  The object type does not change under the extension with a new method, but because <: ρ, there is no difﬁculty using a new object in a context expecting an old object—the ∗ as described in Section 27.1.  Finally, let us consider the effect of adding a new class c  The new method vector, e  ∗ mv, has the type   cid:15   d∈D  ∗ τ mv   cid:2    cid:14   ∗ → ρd ,  τ  ∗ is the new object type  where τ There is an isomorphism, written    †, between τ use to deﬁne the new method vector e   cid:24 d  cid:9 → λ  this : τ  ∗    case this  ∗ τ c, which is a super-type of the old object type τ. , which we may  ∗ and the sum type τ + τ c  c∈C ∗ mv as follows: † {l · u  cid:9 →  emv · d  u   r · u  cid:9 → ec  d  u } cid:25 d∈D.  ∗  ∗  Every method must be redeﬁned to account for the new class, but the old method vector is reused.   249  Note  27.4 Notes  Abadi and Cardelli  1996  and Pierce  2002  provide thorough accounts of the interaction of inheritance and subtyping. Liskov and Wing  1994  discuss it from a behavioral perspec- tive. They propose to require that subclasses respect the behavior of the superclass when inheritance is used.  Exercises  27.1. Consider the case of extending a dispatch matrix with self-reference by a new class ∗ in which a method d is inherited from an existing class c. What requirements c ensure that such an inheritance is properly deﬁned? What happens if we extend a ∗ that inherits its behavior on self-referential dispatch matrix with a new method, d class c from another method d? 27.2. Consider the example of two mutually recursive methods given in Exercise 26.3. Suppose that num∗ is a new class with instance type τ num∗ <: τ num that inherits the ev method from num, but deﬁnes its own version of the od method. What happens when message ev is sent to an instance of num∗? Will the revised od method ever be invoked?  27.3. Method specialization consists of deﬁning a new class by inheriting methods from another class or classes, while redeﬁning some of the methods that the inherited methods might invoke. The behavior of the inherited methods on instances of the new class is altered to the extent that they invoke a method that is specialized to the new class. Reconsider Exercise 26.3 in light of Exercise 27.2, seeking to ensure that the specialization of od is invoked when the inherited method ev is invoked on instances of the new class.  a  Redeﬁne the class num along the following lines. The instance data of num is an object admitting methods ev and od. The class num admits these methods and simply hands them off to the instance object.   b  The classes zero or of succ admit both the ev and od methods and are deﬁned  c  Deﬁne a subclass succ∗ of succ that overrides the od method. Show that ev on  using message send to effect mutual recursion as necessary. an instance of succ∗ correctly invokes the overridden od method.  Note  1 The extension with a new method will be considered separately for the sake of clarity.    P A R T XII  Control Flow    28  Control Stacks  Structural dynamics is convenient for proving properties of languages, such as a type safety theorem, but is less convenient as a guide for implementation. A structural dynamics deﬁnes a transition relation using rules that determine where to apply the next instruction without spelling out how to ﬁnd where the instruction lies within an expression. To make this process explicit, we introduce a mechanism, called a control stack, that records the work that remains to be done after an instruction is executed. Using a stack eliminates the need for premises on the transition rules so that the transition system deﬁnes an abstract machine whose steps are determined by information explicit in its state, much as a concrete computer does.  In this chapter, we develop an abstract machine K for evaluating expressions in PCF. The machine makes explicit the context in which primitive instruction steps are executed, and the process by which the results are propagated to determine the next step of execution. We prove that K and PCF are equivalent in the sense that both achieve the same outcomes for the same expressions.  28.1 Machine Deﬁnition  A state s of the stack machine K for PCF consists of a control stack k and a closed expression e. States take one of two forms: 1. An evaluation state of the form k * e corresponds to the evaluation of a closed expression 2. A return state of the form k , e, where e val, corresponds to the evaluation of a stack k  e on a control stack k.  on a closed value e.  As an aid to memory, note that the separator “points to” the focal entity of the state, the expression in an evaluation state and the stack in a return state.  The control stack represents the context of evaluation. It records the “current location” of evaluation, the context into which the value of the current expression is returned. Formally, a control stack is a list of frames:   cid:24  stack  f frame  k stack  k;f stack   28.1a    28.1b    254  Control Stacks  The frames of theK machine are inductively deﬁned by the following rules:  s −  frame  ifz{e0; x.e1} −  frame  ap −; e2  frame  k * z  cid:20 −→ k , z  k * s e   cid:20 −→ k;s −  * e  k;s −  , e  cid:20 −→ k , s e   The frames correspond to search rules in the dynamics of PCF. Thus, instead of relying on the structure of the transition derivation to keep a record of pending computations, we make an explicit record of them in the form of a frame on the control stack.  The transition judgment between states of the PCF machine is inductively deﬁned by a set of inference rules. We begin with the rules for natural numbers, using an eager semantics for the successor.  To evaluate z, we simply return it. To evaluate s e , we push a frame on the stack to record  cid:5   to the stack. the pending successor and evaluate e; when that returns with e   cid:5 , we return s e  Next, we consider the rules for case analysis.  k * ifz{e0; x.e1} e   cid:20 −→ k;ifz{e0; x.e1} −  * e  k;ifz{e0; x.e1} −  , z  cid:20 −→ k * e0  k;ifz{e0; x.e1} −  , s e   cid:20 −→ k * [e x]e1  The test expression is evaluated, recording the pending case analysis on the stack. Once the value of the test expression is determined, the zero or non-zero branch of the condition is evaluated, substituting the predecessor in the latter case.  Finally, we give the rules for functions, which are evaluated by-name, and the rule for  general recursion.  k * lam{τ} x.e   cid:20 −→ k , lam{τ} x.e   k * ap e1; e2   cid:20 −→ k;ap −; e2  * e1   28.2a    28.2b    28.2c    28.3a    28.3b    28.3c    28.4a    28.4b    28.4c    28.5a    28.5b    255  28.2 Safety  k;ap −; e2  , lam{τ} x.e   cid:20 −→ k * [e2 x]e  k * fix{τ} x.e   cid:20 −→ k * [fix{τ} x.e  x]e  It is important that evaluation of a general recursion requires no stack space.  The initial and ﬁnal states of the K machine are deﬁned by the following rules:   cid:24  * e initial  cid:24  , e ﬁnal  e val  28.2 Safety  k ,: τ   cid:24  ,: τ f : τ  cid:6  τ   cid:5  k;f ,: τ   cid:5   s −  :nat  cid:6  nat x : nat  cid:12  e1 : τ e0 : τ ifz{e0; x.e1} −  :nat  cid:6  τ ap −; e2  :parr  τ2; τ   cid:6  τ  e2 : τ2  k ,: τ e : τ k * e ok e : τ k , e ok  k ,: τ  e val  To deﬁne and prove safety for the PCF machine requires that we introduce a new typing judgment, k ,: τ, which states that the stack k expects a value of type τ. This judgment is inductively deﬁned by the following rules:  This deﬁnition makes use of an auxiliary judgment, f : τ  cid:6  τ transforms a value of type τ to a value of type τ   cid:5 .   28.7b   cid:5 , stating that a frame f  The states of the PCF machine are well-formed if their stack and expression components  match:  We leave the proof of safety of the PCF machine as an exercise.  Theorem 28.1  Safety . 1. If s ok and s  cid:20 −→ s 2. If s ok, then either s ﬁnal or there exists s   cid:5 , then s   cid:5   ok.   cid:5  such that s  cid:20 −→ s   cid:5 .   28.5c    28.5d    28.6a    28.6b    28.7a    28.8a    28.8b    28.8c    28.9a    28.9b    256  Control Stacks  28.3 Correctness of the K Machine  Does evaluation of an expression e using the K machine yield the same result as does the structural dynamics of PCF? The answer to this question can be derived from the following facts.  Completeness If e  cid:20 −→∗ Soundness If  cid:24  * e  cid:20 −→∗  e   cid:5    cid:5 , where e  cid:24  , e  val, then  cid:24  * e  cid:20 −→∗ val.   cid:5  with e   cid:5 , then e  cid:20 −→∗  e   cid:5    cid:24  , e   cid:5 .  To prove completeness a plausible ﬁrst step is to consider a proof by induction on the deﬁnition of multi-step transition, which reduces the theorem to the following two lemmas: 1. If e val, then  cid:24  * e  cid:20 −→∗ 2. If e  cid:20 −→ e   cid:5 , then, for every v val, if  cid:24  * e   cid:24  , v, then  cid:24  * e  cid:20 −→∗   cid:24  , v.   cid:5   cid:20 −→∗   cid:24  , e.  The ﬁrst can be proved easily by induction on the structure of e. The second requires an inductive analysis of the derivation of e  cid:20 −→ e  cid:5  that gives rise to two complications. The ﬁrst complication is that we cannot restrict attention to the empty stack, for if e is, say, ap e1; e2 , then the ﬁrst step of the K machine is   cid:24  * ap e1; e2   cid:20 −→  cid:24 ;ap −; e2  * e1.  To handle such situations, we consider the evaluation of e1 on any stack, not just the empty stack. k , v. Speciﬁcally, we prove that if e  cid:20 −→ e 1; e2 , with e1  cid:20 −→ e Reconsider the case e = ap e1; e2 , e  cid:5   cid:5  1. We are given that k , v, and we are to show that k * ap e1; e2   cid:20 −→∗ k , v. It is easy to k * ap e show that the ﬁrst step of the former derivation is  k , v, then k * e  cid:20 −→∗   cid:5  and k * e  cid:5  = ap e  1; e2   cid:20 −→∗  cid:5    cid:20 −→∗   cid:5   1; e2   cid:20 −→ k;ap −; e2  * e  cid:5   cid:5  1. We would like to apply induction to the derivation of e1  cid:20 −→ e  cid:5  value v1 such that e 1  v1, which is not at hand.  k * ap e   cid:20 −→∗   cid:5  1, but to do so, we need a   cid:5  iff e  cid:20 −→∗  We therefore consider the value of each sub-expression of an expression. This information is given by the evaluation dynamics described in Chapter 7, which has the property that e ⇓ e Lemma 28.2. If e ⇓ v, then for every k stack, k * e  cid:20 −→∗   cid:5  and e  k , v.  val.  e   cid:5   v.  The desired result follows by the analog of Theorem 7.2 for PCF, which states that e ⇓ v  iff e  cid:20 −→∗ To prove soundness, we note that it is awkward to reason inductively about a multi-  cid:24  , v. The intermediate steps could involve alternations step transition from  cid:24  * e  cid:20 −→∗ of evaluation and return states. Instead, we consider a K machine state to encode an   257  28.3 Correctness of the K Machine  expression, and show that the machine transitions are simulated by the transitions of the structural dynamics. To do so, we deﬁne a judgment, s  cid:7  e, stating that state s “unravels to” expression e. It will turn out that for initial states, s =  cid:24  * e, and ﬁnal states, s =  cid:24  , e, we have s  cid:7  e. Then we show that if s  cid:20 −→∗ val and e  cid:20 −→∗ 1. If s  cid:7  e and s ﬁnal, then e val. 2. If s  cid:20 −→ s  cid:5 , and e   cid:5 . For this, it is enough to show the following two facts:  v, where v val, then e  cid:20 −→∗  ﬁnal, s  cid:7  e, and s   cid:5 , s  cid:7  e, s   cid:5 , where s   cid:5 , then e   cid:5   cid:20 −→∗   cid:5   cid:7  e   cid:5   cid:7  e  v.  e  s   cid:5    cid:5   The ﬁrst is quite simple, we need only note that the unraveling of a ﬁnal state is a value. For the second, it is enough to prove the following lemma.  cid:5 , then e  cid:20 −→∗ Lemma 28.3. If s  cid:20 −→ s Corollary 28.4. e  cid:20 −→∗   cid:5 , s  cid:7  e, and s n iff  cid:24  * e  cid:20 −→∗   cid:5   cid:7  e  cid:24  , n.   cid:5 .  e  28.3.1 Completeness  Proof of Lemma 28.2 The proof is by induction on an evaluation dynamics for PCF.  Consider the evaluation rule  e1 ⇓ lam{τ2} x.e   [e2 x]e ⇓ v   28.10  For an arbitrary control stack k, we are to show that k * ap e1; e2   cid:20 −→∗ k , v. Applying both of the inductive hypotheses in succession, interleaved with steps of the K machine, we obtain  ap e1; e2  ⇓ v  k * ap e1; e2   cid:20 −→ k;ap −; e2  * e1  k;ap −; e2  , lam{τ2} x.e    cid:20 −→∗  cid:20 −→ k * [e2 x]e  cid:20 −→∗  k , v.  The other cases of the proof are handled similarly.  28.3.2 Soundness  The judgment s  cid:7  e judgment k *, e = e   cid:5 , where s is either k * e or k , e, is deﬁned in terms of the auxiliary  cid:5  by the following rules:  k *, e = e k * e  cid:7  e k *, e = e k , e  cid:7  e   cid:5   cid:5    cid:5   cid:5    28.11a    28.11b    258  Control Stacks  In words, to unravel a state, we wrap the stack around the expression to form a complete program. The unraveling relation is inductively deﬁned by the following rules:   cid:24  *, e = e k *, s e  = e  cid:5  k;s −  *, e = e   cid:5   k *, ifz{e0; x.e1} e  = e  cid:5  k;ifz{e0; x.e1} −  *, e = e   cid:5   k *, ap e1; e2  = e k;ap −; e2  *, e1 = e   28.12a    28.12b    28.12c    28.12d   These judgments both deﬁne total functions.  Lemma 28.5. The judgment s  cid:7  e relates every state s to a unique expression e, and the judgment k *, e = e   cid:5  relates every stack k and expression e to a unique expression e  cid:5  such that k *, e = e  cid:5 .  We are therefore justiﬁed in writing k *, e for the unique e The following lemma is crucial. It states that unraveling preserves the transition relation.   cid:5 .   cid:5  = d   cid:5 , k *, e = d, k *, e   cid:5 , then d  cid:20 −→ d Lemma 28.6. If e  cid:20 −→ e  cid:5 . Proof The proof is by rule induction on the transition e  cid:20 −→ e  cid:5 . The inductive cases, where the transition rule has a premise, follow easily by induction. The base cases, where the transition is an axiom, are proved by an inductive analysis of the stack k. For an example of an inductive case, suppose that e = ap e1; e2 , e e1  cid:20 −→ e k;ap −; e2  *, e1 = d and k;ap −; e2  *, e For an example of a base case, suppose that e = ap lam{τ2} x.e ; e2  and e with e  cid:20 −→ e that d  cid:20 −→ d result follows immediately. Consider, say, the stack k = k rules  28.12  that k dynamics ap e; c2   cid:20 −→ ap e as desired.   cid:5  1; e2 , and  cid:5 . It follows from rules  28.12  that  cid:5 , as desired.  cid:5  = [e2 x]e  cid:5  directly. Assume that k *, e = d and k *, e  cid:5 ; we are to show  cid:5 . We proceed by an inner induction on the structure of k. If k =  cid:24 , the  cid:5 ;ap −; c2 . It follows from  cid:5 . But by the structural  cid:5 ; c2 , so by the inner inductive hypothesis we have d  cid:20 −→ d  cid:5 ,   cid:5  = d 1. We havek *, e = d and k *, e  cid:5  = d  cid:5 . So by induction d  cid:20 −→ d   cid:5  *, ap e; c2  = d and k   cid:5 ; c2  = d   cid:5  *, ap e   cid:5  = ap e   cid:5  = d   cid:5  1  We may now complete the proof of Lemma 28.3.  Proof of Lemma 28.3 The proof is by case analysis on the transitions of the K machine. In each case, after unraveling, the transition will correspond to zero or one transitions of the PCF structural dynamics.  cid:5  iff k;s − *, e = Suppose that s = k * s e  and s  cid:5 , from which the result follows immediately.   cid:5  = k;s −  * e. Note that k*, s e  = e  e   259  Exercises  Suppose that s = k;ap lam{τ} x.e1 ;−  , e2 and s k;ap lam{τ} x.e1 ;−  *, e2 = e k *, ap lam{τ} x.e1 ; e2  = e   cid:5  and let e   cid:5  = k * [e2 x]e1. Let e  cid:5  cid:5  be such that k *, [e2 x]e1 = e   cid:5 . The result follows from Lemma 28.6.   cid:5  be such that  cid:5  cid:5 . Observe that  The abstract machine considered here is typical of a wide class of machines that make control ﬂow explicit in the state. The prototype is the SECD machine  Landin, 1965 , which is a linearization of a structural operational semantics  Plotkin, 1981 . The advantage of a machine model is that the explicit treatment of control is needed for languages that allow the control state to be manipulated  see Chapter 30 for a prime example . The disadvantage is that the control state of the computation must be made explicit, necessitating rules for manipulating it that are left implicit in a structural dynamics.  28.4 Notes  Exercises  28.1. Give the proof of Theorem 28.1 for conditional expressions. 28.2. Formulate a call-by-value variant of the PCF machine. 28.3. Analyze the worst-case asymptotic complexity of executing each instruction of the  28.4. Reﬁne the proof of Lemma 28.2 by bounding the number of machine steps taken for  K machine.  each step of the PCF dynamics.   29  Exceptions  Exceptions effect a non-local transfer of control from the point at which the exception is raised to an enclosing handler for that exception. This transfer interrupts the normal ﬂow of control in a program in response to unusual conditions. For example, exceptions can be used to signal an error condition, or to signal the need for special handling in unusual circumstances. We could use conditionals to check for and process errors or unusual conditions, but using exceptions is often more convenient, particularly because the transfer to the handler is conceptually direct and immediate, rather than indirect via explicit checks. In this chapter, we will consider two extensions of PCF with exceptions. The ﬁrst, FPCF, enriches PCF with the simplest form of exception, called a failure, with no associated data. A failure can be intercepted and turned into a success  or another failure!  by transferring control to another expression. The second, XPCF, enriches PCF with exceptions, with associated data that is passed to an exception handler that intercepts it. The handler may analyze the associated data to determine how to recover from the exceptional condition. A key choice is to decide on the type of the data associated to an exception.  29.1 Failures  The syntax of FPCF is deﬁned by the following extension of the grammar of PCF:  Exp e  ::= fail  fail  catch e1; e2  catch e1 ow e2  signal a failure catch a failure  The expression fail aborts the current evaluation, and the expression catch e1; e2  catches any failure in e1 by evaluating e2 instead. Either e1 or e2 may themselves abort, or they may diverge or return a value as usual in PCF.  The statics of FPCF is given by these rules:   cid:7   cid:12  fail : τ   cid:7   cid:12  e1 : τ  cid:7   cid:12  e2 : τ  cid:7   cid:12  catch e1; e2  :τ   29.1a    29.1b   A failure can have any type, because it never returns. The two expressions in a catch expression must have the same type, because either might determine the value of that expression.   261  29.1 Failures  The dynamics of FPCF is given using a technique called stack unwinding. Evaluation of a catch pushes a frame of the form catch −; e  onto the control stack that awaits the arrival of a failure. Evaluation of a fail expression pops frames from the control stack until it reaches a frame of the form catch −; e , at which point the frame is removed from the stack and the expression e is evaluated. Failure propagation is expressed by a state of the form k  cid:8  , which extends the two forms of state considered in Chapter 28 to express failure propagation.  The FPCF machine extends the PCF machine with the following additional rules:  k * fail  cid:20 −→ k  cid:8   k * catch e1; e2   cid:20 −→ k;catch −; e2  * e1  k;catch −; e2  , v  cid:20 −→ k , v  k;catch −; e2   cid:8   cid:20 −→ k * e2   f  cid:6 = catch −; e   k;f  cid:8   cid:20 −→ k  cid:8   Evaluating fail propagates a failure up the stack. The act of failing itself, fail, will, of course, give rise to a failure. Evaluating catch e1; e2  consists of pushing the handler on the control stack and evaluating e1. If a value reaches to the handler, the handler is removed and the value is passed to the surrounding frame. If a failure reaches the handler, the stored expression is evaluated with the handler removed from the control stack. Failures propagate through all frames other than the catch frame.  The initial and ﬁnal states of the FPCF machine are deﬁned by the following rules:   cid:24  initial  e val   cid:24  , e ﬁnal   cid:24   cid:8  ﬁnal  The deﬁnition of stack typing given in Chapter 28 can be extended to account for the new forms of frame so that safety can be proved in the same way as before. The only difference is that the statement of progress must be weakened to take account of failure: a well-typed expression is either a value, or may take a step, or may signal failure. Theorem 29.1  Safety for FPCF . 1. If s ok and s  cid:20 −→ s 2. If s ok, then either s ﬁnal or there exists s   cid:5  such that s  cid:20 −→ s   cid:5 , then s  cid:5 .  ok.   cid:5    29.2a    29.2b    29.2c    29.2d    29.2e    29.3a    29.3b    29.3c    262  Exceptions  29.2 Exceptions  The language XPCF enriches FPCF with exceptions, failures to which a value is attached. The syntax of XPCF extends that of PCF with the following forms of expression:  Exp e  ::= raise e   try e1; x.e2  try e1 ow x  cid:9 → e2  raise e   raise an exception handle an exception  The argument to raise is evaluated to determine the value passed to the handler. The expression try e1; x.e2  binds a variable x in the handler e2. The associated value of the exception is bound to that variable within e2, should an exception be raised when e1 is evaluated.  The statics of exceptions extends the statics of failures to account for the type of the  value carried with the exception:   cid:7   cid:12  e : τexn   cid:7   cid:12  raise e  : τ   cid:7   cid:12  e1 : τ  cid:7 , x : τexn  cid:12  e2 : τ   cid:7   cid:12  try e1; x.e2  :τ  The type τexn is some ﬁxed, but as yet unspeciﬁed, type of exception values.  The choice of τexn is discussed in Section 29.3.  The dynamics of XPCF is similar to that of FPCF, except that the failure state k  cid:8  is replaced by the exception state k  cid:8  e which passes an exception value e to the stack k. There is only one notion of exception, but the associated value can be used to identify the source of the exception. We use a by-value interpretation to avoid the problem of imprecise exceptions that arises under a by-name interpretation. The stack frames of the PCF machine are extended to include raise −  and try −; x.e2 . These are used in the following rules:  k * raise e   cid:20 −→ k;raise −  * e  k;raise −  , e  cid:20 −→ k  cid:8  e  k * try e1; x.e2   cid:20 −→ k;try −; x.e2  * e1  k;try −; x.e2  , e  cid:20 −→ k , e  k;try −; x.e2   cid:8  e  cid:20 −→ k * [e x]e2   f  cid:6 = try −; x.e2   k;f  cid:8  e  cid:20 −→ k  cid:8  e   29.4a    29.4b    29.5a    29.5b    29.5c    29.5d    29.5e    29.5f    263  29.3 Exception Values  The main difference compared to rules  29.2  is that an exception passes a values to the stack, whereas a failure does not.  The initial and ﬁnal states of the XPCF machine are deﬁned by the following rules:   cid:24  * e initial  e val   cid:24  , e ﬁnal   cid:24   cid:8  e ﬁnal   29.6a    29.6b    29.6c   Theorem 29.2  Safety for XPCF . 1. If s ok and s  cid:20 −→ s 2. If s ok, then either s ﬁnal or there exists s   cid:5  such that s  cid:20 −→ s   cid:5 , then s  cid:5 .   cid:5   ok.  29.3 Exception Values  The statics of XPCF is parameterized by the type τexn of values associated to exceptions. The choice of τexn is important because it determines how the source of an exception is identiﬁed in a program. If τexn is the one-element type unit, then exceptions degenerate to failures, which are unable to identify their source. Thus, τexn must have more than one value to be useful.  This fact suggests that τexn should be a ﬁnite sum. The classes of the sum identify the sources of exceptions, and the classiﬁed value carries information about the particular instance. For example, τexn might be a sum type of the form  [div  cid:9 → unit, fnf  cid:9 → string, . . .].  Here the class div might represent an arithmetic fault, with no associated data, and the class fnf might represent a “ﬁle not found” error, with associated data being the name of the ﬁle that was not found.  Using a sum means that an exception handler can dispatch on the class of the exception  value to identify its source and cause. For example, we might write  handle e1 ow x  cid:9 → match x { div  cid:24  cid:25   cid:9 → ediv  fnf s  cid:9 → efnf }  to handle the exceptions speciﬁed by the above sum type. Because the exception and its associated data are coupled in a sum type, there is no possibility of misinterpreting the data associated to one exception as being that of another.   264  Exceptions  The disadvantage of choosing a ﬁnite sum for τexn is that it speciﬁes a closed world of possible exception sources. All sources must be identiﬁed for the entire program, which impedes modular development and evolution. A more modular approach admits an open world of exception sources that can be introduced as the program evolves and even as it exe- cutes. A generalization of ﬁnite sums, called dynamic classiﬁcation, deﬁned in Chapter 33, is required for an open world.  See that Chapter for further discussion.   When τexn is a type of classiﬁed values, its classes are often called exceptions, so that one may speak of “the fnf exception” in the above example. This terminology is harmless, and all but unavoidable, but it invites confusion between two separate ideas:  1. Exceptions as a control mechanism that allows the course of evaluation to be altered by  raising and handling exceptions.  2. Exceptions as a data value associated with such a deviation of control that allows the  source of the deviation to be identiﬁed.  As a control mechanism, exceptions can be eliminated using explicit exception passing. A computation of type τ that may raise an exception is interpreted as an exception-free computation of type τ + τexn; see Exercise 29.5 for more on this method.  Various forms of exceptions were considered in Lisp  Steele, 1990 . The original formula- tion of ML  Gordon et al., 1979  as a metalanguage for mechanized logic used failures to implement backtracking proof search. Most modern languages now have exceptions, but differ in the forms of data that may be associated with them.  29.4 Notes  Exercises  29.1. Prove Theorem 29.2. Are any properties of τexn required for the proof? 29.2. Give an evaluation dynamics for XPCF using the following judgment forms:    Normal evaluation: e ⇓ v, where e : τ, v : τ, and v val.   Exceptional evaluation: e ⇑ v, where e : τ, and v : τexn, and v val. The ﬁrst states that e evaluates normally to value v, the second that e raises an exception with value v.  29.3. Give a structural operational dynamics to XPCF by inductively deﬁning the following  judgment forms:   e  cid:20 −→ e   e val, stating that expression e is a value.   cid:5 , stating that expression e transitions to expression e   cid:5 ;   265  Exercises  Ensure that e ⇓ v iff e  cid:20 −→∗ cases.  v, and e ⇑ v iff e  cid:20 −→∗  raise v , where v val in both  29.4. The closed world assumption on exceptions amounts to choosing the type of exception values to be a ﬁnite sum type shared by the entire program. Under such an assumption, it is possible to track exceptions by placing an upper bound on the possible classes of an exception value.  Type reﬁnements  deﬁned in Chapter 25  can be used for exception tracking in a  closed-world setting. Deﬁne ﬁnite sum reﬁnements by the rule   cid:5  ⊆ X  ∀x ∈ X   cid:5   φx   τx  X  [φx]x∈X   cid:5    [τx]x∈X  .  In particular, the reﬁnement ∅ is the vacuous sum reﬁnement [] satisﬁed by no value. Entailment of ﬁnite sum reﬁnements is deﬁned by the rule  cid:5   φx ≤ φ  ∀x ∈ X  cid:5   cid:5  ≤ [φ x  cid:5  x]x∈X  So, in particular, ∅ ≤ φ for all sum reﬁnements φ of τexn. Entailment weakens knowledge of the class of a value of sum type, which is crucial to their application to exception tracking.   cid:5  ⊆ X  [φx]x∈X  X   cid:5  cid:5    cid:5  cid:5   The goal of this exercise is to develop a system of type reﬁnements for the modal formulation of exceptions in MPCF using sum reﬁnements to perform exception  a  Deﬁne the command reﬁnement judgment m ∈τ φ ow χ, where m ∼·· τ, φ   τ, tracking. and χ   τexn, to mean that if m returns e, then e ∈τ φ, and if m raises e, then e ∈τexn χ.  b  Deﬁne satisfaction and entailment for the expression reﬁnement cmd φ; χ    cmd τ , where φ   τ and χ   τexn. This reﬁnement classiﬁes encapsulated commands that satisfy the stated value and exception reﬁnements in the sense of the preceding problem.  command m ∼·· τ of MPCF is translated to a pure expression cid:2 m of type cid:2 τ + τexn 29.5. Show that exceptions in MPCF can be eliminated by a translation into PCF enriched with sum types by what is called the exception-passing style transformation. Each whose value is either l·e, where e : τ, for normal return, or r·e, where e : τexn, for an  cid:2 e that replaces occurrences of cmd m  by cid:2 m. The corresponding type translation, cid:2 τ, replaces cmd τ  by cid:2 τ + τexn. Deﬁne the command translation from MPCF to PCF exceptional return. The command translation is extended to an expression translation  enriched with sums, and show that it has the required type and correctly simulates the behavior of exceptions.   30  Continuations  The semantics of many control constructs  such as exceptions and coroutines  can be expressed in terms of reiﬁed control stacks, a representation of a control stack as a value that can be reactivated at any time, even if control has long since returned past the point of reiﬁcation. Reiﬁed control stacks of this kind are called continuations; they are values that can be passed and returned at will in a computation. Continuations never “expire”, and it is always sensible to reinstate a continuation without compromising safety. Thus continuations support unlimited “time travel” — we can go back to a previous step of the computation, then return to some point in its future.  Why are continuations useful? Fundamentally, they are representations of the control state of a computation at a given time. Using continuations we can “checkpoint” the control state of a program, save it in a data structure, and return to it later. In fact this is precisely what is necessary to implement threads  concurrently executing programs  — the thread scheduler suspends a program for later execution from where it left off.  30.1 Overview  We will consider the extension KPCF of PCF with the type cont τ  of continuations accepting values of type τ. The introduction form for cont τ  isletcc{τ } x.e , which binds the current continuation  that is, the current control stack  to the variable x, and evaluates the expression e. The corresponding elimination form is throw{τ} e1; e2 , which restores the value given by e1 to the control stack given by e2.  To illustrate the use of these primitives, consider the problem of multiplying the ﬁrst n elements of an inﬁnite sequence q of natural numbers, where q is represented by a function of type nat  cid:19  nat. If zero occurs among the ﬁrst n elements, we would like to effect an “early return” with the value zero, without further multiplication. This problem can be solved using exceptions, but we will solve it with continuations to show how they are used.  Here is the solution in PCF, without short-cutting:  fix ms is  λ q : nat  cid:19  nat.  λ n : nat.  z  cid:9 → s z   case n {  s n’   cid:9 →  q z  ×  ms  q ◦ succ  n’  }   267  30.1 Overview  The recursive call composes q with the successor function to shift the sequence by one step.  Here is the solution in KPCF, with short-cutting:  λ q : nat  cid:19  nat.  λ n : nat.  letcc ret : nat cont in  let ms be  fix ms is  λ q : nat  cid:19  nat.  λ n : nat.  case n { z  cid:9 → s z   s n’   cid:9 → case q z {  s n’’   cid:9 →  q z  ×  ms  q ◦ succ  n’  }  z  cid:9 → throw z to ret  }  in  ms q n  The letcc binds the return point of the function to the variable ret for use within the main loop of the computation. If an element is zero, control is thrown to ret, effecting an early return with the value zero.  To take another example, given that k has type τ cont and f has type τ of type return a continuation k  cid:5  throws the value of f  v k   cid:5   cid:19  τ, return k  cid:5  to  cid:5  of type τ  cid:5   tok . Thus, we seek to deﬁne a function compose of type  cont such that throwing a value v   cid:5  of type τ   cid:5    cid:5    cid:5    τ   cid:19  τ   cid:19  τ cont  cid:19  τ  cont.   cid:5   The continuation we seek is the one in effect at the point of the ellipsis in the expression  cid:5 , applies f to it, throw f  ...  to k. It is the continuation that, when given a value v and throws the result to k. We can seize this continuation using letcc by writing  throw f letcc x:τ  cont in ...  to k   cid:5   The desired continuation is bound to x, but how can we return it as the result of compose? We use the same idea as for short-circuit multiplication, writing  letcc ret:τ  cont cont in   cid:5   throw  f  letcc r in throw r to ret   to k  as the body of compose. Note that the type of ret is τ cont cont, that of a continuation that expects to be thrown a continuation!   268  Continuations  30.2 Continuation Dynamics  The syntax of KPCF is as follows: ::= cont τ  ::= letcc{τ} x.e   Type Expr  τ e  τ cont letcc x in e throw{τ} e1; e2  throw e1 to e2 cont k   cont k   continuation mark goto continuation  The expression cont k  is a reiﬁed control stack, which arises during evaluation.  The statics of KPCF is deﬁned by the following rules:  cid:7 , x : cont τ   cid:12  e : τ  cid:7   cid:12  letcc{τ} x.e  :τ   cid:7   cid:12  e1 : τ1  cid:7   cid:12  e2 : cont τ1    cid:7   cid:12  throw{τ} e1; e2  :τ  The result type of a throw expression is arbitrary because it does not return to the point of the call.  The statics of continuation values is given by the following rule:  A continuation value cont k  has type cont τ  exactly if it is a stack accepting values of type τ.  To deﬁne the dynamics of KPCF, we extend the PCF machine with two forms of stack  frame:  k : τ   cid:7   cid:12  cont k  :cont  τ   throw{τ} −; e2  frame  e1 val  throw{τ} e1;−  frame  k stack  cont k  val  Every reiﬁed control stack is a value:  The transition rules of the PCF machine governing continuations are as follows:  k * cont k   cid:20 −→ k , cont k   k * letcc{τ} x.e   cid:20 −→ k * [cont k  x]e  k * throw{τ} e1; e2   cid:20 −→ k;throw{τ} −; e2  * e1   30.1a    30.1b    30.2    30.3a    30.3b    30.4    30.5a    30.5b    30.5c     30.5d    30.5e    30.6a    30.6b   269  30.3 Coroutines from Continuations  k;throw{τ} −; e2  , e1  cid:20 −→ k;throw{τ} e1;−  * e2  e1 val  k;throw{τ} e;−  , cont k   cid:5    cid:20 −→ k   cid:5  , e  e val  Evaluation of a letcc expression duplicates the control stack; evaluation of a throw expression destroys the current control stack.  The safety of KPCF is proved by extending the safety proof for the K machine given in  Chapter 28.  We need only add typing rules for the two new forms of frame, which are as follows:  throw{τ  e2 : cont τ   cid:5 } −; e2  : τ  cid:6  τ   cid:5   throw{τ  e1 : τ  cid:5 } e1;−  : cont τ   cid:6  τ  e1 val   cid:5   The rest of the deﬁnitions remain as in Chapter 28. Lemma 30.1  Canonical Forms . If e : cont τ  and e val, then e = cont k  for some k such that k : τ . Theorem 30.2  Safety . 1. If s ok and s  cid:20 −→ s 2. If s ok, then either s ﬁnal or there exists s   cid:5  such that s  cid:20 −→ s   cid:5 , then s  ok.   cid:5 .   cid:5   30.3 Coroutines from Continuations  The distinction between a routine and a subroutine is the distinction between a manager and a worker. The routine calls the subroutine to do some work, and the subroutine returns to the routine when its work is done. The relationship is asymmetric in that there is a distinction between the caller, the main routine, and the callee, the subroutine. It is useful to consider a symmetric situation in which two routines each call the other to do some work. Such a pair of routines are called coroutines; their relationship to one another is symmetric, not hierarchical.  A subroutine is implemented by having the caller pass to the callee a continuation representing the work to be done once the subroutine ﬁnishes. When it does, it throws the return value to that continuation, without the possibility of return. A coroutine is implemented by having two routines each call each other as subroutines by providing a continuation when control is ceded from one to the other. The only tricky part is how the entire process gets started.  Consider the type of each routine of the pair. A routine is a continuation accepting two arguments, data to be passed to the routine when it is resumed and a continuation to be resumed when the routine has ﬁnished its task. The datum represents the state of the   270  Continuations  computation, and the continuation is a coroutine that accepts arguments of the same form. Thus, the type of a coroutine must satisfy the type isomorphism  τ coro ∼=  τ × τ coro  cont.  Therefore, we deﬁne τ coro to be the recursive type  τ coro  cid:2  rec t is  τ × t  cont.  Up to isomorphism, the type τ coro is the type of continuations that accept a value of type τ, representing the state of the coroutine, and the partner coroutine, a value of the same type.  cid:5  by evaluating the expression  cid:5  cid:25  , where s is the current state of the computation. Doing so creates a new resume  cid:24 s, r coroutine whose entry point is the return point  calling site  of resume. Therefore, the type of resume is  A coroutine r passes control to another coroutine r  τ × τ coro  cid:19  τ × τ coro.  The deﬁnition of resume is as follows:  λ   cid:24 s, r   cid:5  cid:25  : τ × τ coro  letcc k in throw cid:24 s, fold k  cid:25  to unfold r   cid:5    .  When applied, resume seizes the current continuation and passes the state, s, and the seized continuation  packaged as a coroutine  to the called coroutine.  Because the state is explicitly passed from one routine to the other, a coroutine is a state transformation function that, when activated with the current state, determines the next state of the computation. A system of coroutines is created by establishing a joint exit point to which the result of the system is thrown and creating a pair of coroutines that transform the state and pass control to the partner routine. If either routine wishes to terminate the computation, it does so by throwing a result value to their common exit point. Thus, a coroutine is a function of type   cid:5    τ  ,τ  rout  cid:2  τ   cid:5   cont  cid:19  τ  cid:19  τ ,  where τ   cid:5  is the result type and τ is the state type of the system of coroutines.  To set up a system of coroutines we deﬁne a function run that, given two routines, creates  cid:5 .  cid:5  that, when applied to the initial state, computes a result of type τ a function of type τ  cid:19  τ The computation consists of a cooperating pair of routines that share a common exit point. The deﬁnition of run begins as follows:  λ   cid:24 r1, r2 cid:25   λ  s0  letcc x0 in let r   cid:5  1 be r1 x0  in let r   cid:5  2 be r2 x0  in . . .  Given two routines, run establishes their common exit point and passes this continua- tion to both routines. By throwing to this continuation, either routine may terminate the   271  30.3 Coroutines from Continuations  computation with a result of type τ   cid:5 . The body of the run function continues as follows:  rep r   cid:5  2  letcc k in rep r  1   cid:24 s0, fold k  cid:25     cid:5   The auxiliary function rep creates an inﬁnite loop that transforms the state and passes control to the other routine:  λ  t  fix l is λ   cid:24 s, r cid:25   l resume  cid:24 t s , r cid:25   .  The system is initialized by starting routine r1 with the initial state, and arranging that, when it cedes control to its partner, it starts routine r2 with the resulting state. At that point, the system is bootstrapped: each routine will resume the other on each iteration of the loop. A good example of coroutining is the interleaving of input and output in a computation. This is done by coroutining between a producer routine and a consumer routine. The producer emits the next element of the input, if any, and passes control to the consumer, removing that element from the input. The consumer processes the next data item, and returns control to the producer, with the result of processing attached to the output. For simplicity input and output are modeled as lists of type τi list and τo list, respectively, which are passed back and forth between the routines. The routines exchange messages according to the following protocol. The message OK  cid:24 i, o cid:25   is sent from the consumer to producer to acknowledge receipt of the previous message, and to pass back the current state of the input and output channels. The message EMIT  cid:24 e, cid:24 i, o cid:25  cid:25  , where e is a value of type τi opt, is sent from the producer to the consumer to emit the next value  if any  from the input, and to pass the current state of the input and output channels to the consumer.  Here is an implementation of the producer consumer coroutines. The type τ of the state  maintained by the routines is the labeled sum type  [OK  cid:9 → τi list × τo list, EMIT  cid:9 → τi opt ×  τi list × τo list ].  The above type speciﬁes the message protocol between the producer and the consumer described in the preceding paragraph.  The producer P is deﬁned by the expression  where the ﬁrst branch b1 is  and the second branch b2 is  and the third branch b3 is  λ  x0  λ  msg  case msg{b1  b2  b3},  OK ·  cid:24 nil, os cid:25   cid:9 → EMIT ·  cid:24 null, cid:24 nil, os cid:25  cid:25   OK ·  cid:24 cons i; is , os cid:25   cid:9 → EMIT ·  cid:24 just i , cid:24 is, os cid:25  cid:25 ,  EMIT ·   cid:9 → error.  In words, if the input is exhausted, the producer emits the value null, along with the current channel state. Otherwise, it emits just i , where i is the ﬁrst remaining input and removes   272  Continuations  that element from the passed channel state. The producer cannot see an EMIT message and signals an error if it should occur.  The consumer C is deﬁned by the expression  where the ﬁrst branch b  λ  x0  λ  msg  case msg{b   cid:5  1   b   cid:5  2   b   cid:5  3  },   cid:5  1 is EMIT ·  cid:24 null, cid:24  , os cid:25  cid:25   cid:9 → throw os to x0,  the second branch b   cid:5  2 is EMIT ·  cid:24 just i , cid:24 is, os cid:25  cid:25   cid:9 → OK ·  cid:24 is, cons f  i ; os  cid:25 ,  and the third branch b   cid:5  3 is  OK ·   cid:9 → error.  The consumer dispatches on the emitted datum. If it is absent, the output channel state is passed to x0 as the overall value of the computation. If it is present, the function f  unspeciﬁed here  of type τi  cid:19  τo is applied to transform the input to the output, and the result is added to the output channel. If the message OK is received, the consumer signals an error, as the producer never produces such a message. The initial state s0 has the form OK ·  cid:24 is, os cid:25 , where is and os are the initial input and output channel state, respectively. The computation is created by the expression  run  cid:24 P , C cid:25   s0 ,  which sets up the coroutines as described earlier.  Although it is relatively easy to visualize and implement coroutines involving only two partners, it is more complex and less useful to consider a similar pattern of control among n ≥ 2 participants. In such cases, it is more common to structure the interaction as a collection of n routines, each of which is a coroutine of a central scheduler. When a routine resumes its partner, it passes control to the scheduler, which determines which routine to execute next, again as a coroutine of itself. When structured as coroutines of a scheduler, the individual routines are called threads. A thread yields control by resuming its partner, the scheduler, which then determines which thread to execute next as a coroutine of itself. This pattern of control is called cooperative multi-threading, because it is based on voluntary yields, rather than forced suspensions by a scheduler.  30.4 Notes  Continuations are a ubiquitous notion in programming languages. Reynolds  1993  provides an excellent account of the multiple discoveries of continuations. The formulation given here is inspired by Felleisen and Hieb  1992 , who pioneered the development of linguistic theories of control and state.   273  Exercises  Exercises  30.1. Type safety for KPCF follows almost directly from Theorem 28.1. Isolate the key  30.2. Exhibit a closed KPCF expression of each of the following types:  observations required to extend the proof to include continuation types.  a  τ +  τ cont .  b  τ cont cont → τ.  c   τ2 cont → τ1 cont  →  τ1 → τ2 .  d   τ1 + τ2  cont →  τ1 cont × τ2 cont . Hint: you will need to use letcc and throw.  30.3. The type stream of inﬁnite streams of natural numbers deﬁned in Chapter 15 can be implemented using continuations. Deﬁne stream to be the recursive type satisfying the isomorphism  stream ∼=  nat × stream  cont cont.  To examine the front of the stream, throw to it a continuation expecting a natural number and another stream. When passed such a continuation, the stream throws to it the next number in the stream, paired with another stream  that is, another continuation  representing the stream of numbers following that number. Deﬁne the introduction and elimination forms for streams deﬁned in Chapter 15 using this representation.    P A R T XIII  Symbolic Data    31  Symbols  A symbol is an atomic datum with no internal structure. Whereas a variable is given meaning by substitution, a symbol is given meaning by a family of operations indexed by symbols. A symbol is just a name, or index, for a family of operations. Many different interpretations may be given to symbols according to the operations we choose to consider, giving rise to concepts such as ﬂuid binding, dynamic classiﬁcation, mutable storage, and communication channels. A type is associated to each symbol whose interpretation depends on the particular application. For example, in the case of mutable storage, the type of a symbol constrains the contents of the cell named by that symbol to values of that type.  In this chapter, we consider two constructs for computing with symbols. The ﬁrst is a means of declaring new symbols for use within a speciﬁed scope. The expression new a ∼ ρ in e introduces a “new” symbol a with associated type ρ for use within e. The declared symbol a is “new” in the sense that it is bound by the declaration within e and so may be renamed at will to ensure that it differs from any ﬁnite set of active symbols. Whereas the statics determines the scope of a declared symbol, its range of signiﬁcance, or extent, is determined by the dynamics. There are two different dynamic interpretations of symbols, the scoped and the free  short for scope-free  dynamics. The scoped dynamics limits the extent of the symbol to its scope; the lifetime of the symbol is restricted to the evaluation of its scope. Alternatively, under the free dynamics the extent of a symbol exceeds its scope, extending to the entire computation of which it is a part. We may say that in the free dynamics a symbol “escapes its scope,” but it is more accurate to say that its scope widens to encompass the rest of the computation.  The second construct associated with symbols is the concept of a symbol reference, an expression whose purpose is to refer to a particular symbol. Symbol references are values of a type ρ sym and are written ’a for some symbol a with associated type ρ. The elimination form for the type ρ sym is a conditional branch that determines whether a symbol reference refers to a statically speciﬁed symbol. The statics of the elimination form ensures that, in the positive case, the type associated to the referenced symbol is manifested, whereas in the negative case, no type information can be gleaned from the test.  31.1 Symbol Declaration  We will consider here an extension SPCF of PCF with the means to allocate new symbols. This capability will be used in later chapters that use symbols for other purposes. Here   278  Symbols  we will only be concerned with symbol allocation, and the introduction and elimination of symbols as values of a type of plain symbols.  The syntax for symbol declaration in SPCF is given by the following grammar:  Exp e  ::= new{τ} a.e  new a ∼ τ in e  generation  The statics of symbol declaration makes use of a signature, or symbol context, that associates a type to each of a ﬁnite set of symbols. We use the letter  cid:25  to range over signatures, which are ﬁnite sets of pairs a ∼ τ, where a is a symbol and τ is a type. The typing judgment  cid:7   cid:12  cid:25  e : τ is parameterized by a signature  cid:25  associating types to symbols. In effect, there is an inﬁnite family of typing judgments, one for each choice of  cid:25 . The expression new a ∼ τ in e shifts from one instance of the family to another by adding a new symbol to  cid:25 .  The statics of symbol declaration makes use of a judgment, τ mobile, whose deﬁnition depends on whether the dynamics is scoped. In a scoped dynamics, mobility is deﬁned so that the computed value of a mobile type cannot depend on any symbol. By constraining the scope of a declaration to have mobile type, we can, under this interpretation, ensure that the extent of a symbol is conﬁned to its scope. In a free dynamics, every type is deemed mobile, because the dynamics ensures that the scope of a symbol is widened to accommodate the possibility that the value returned from the scope of a declaration may depend on the declared symbol. The term “mobile” reﬂects the informal idea that symbols may or may not be “moved” from the scope of their declaration according to the dynamics given to them. A free dynamics allows symbols to be moved freely, whereas a scoped dynamics limits their range of motion.  The statics of symbol declaration itself is given by the following rule:   cid:7   cid:12  cid:25 ,a∼ρ e : τ  cid:7   cid:12  cid:25  new{ρ} a.e  :τ  τ mobile   31.1   As mentioned, the condition on τ ensures that the returned value does not escape its scope, if any.  The scoped dynamics of symbol declaration is given by a transition judgment of the form e  cid:20 −→  cid:5  indexed by a signature  cid:25  specifying the active symbols of the transition. Either e  cid:25  or e  e  cid:5  may involve the symbols declared in  cid:25 , but no others.  31.1.1 Scoped Dynamics  e  cid:20 −−−→  cid:25 ,a∼ρ new{ρ} a.e   cid:20 −→   cid:5   e   cid:25   new{ρ} a.e   cid:5    e val cid:25   new{ρ} a.e   cid:20 −→  e   cid:25    31.2a    31.2b    279  31.1 Symbol Declaration  Rule  31.2a  speciﬁes that evaluation takes place within the scope of the declaration of a symbol. Rule  31.2b  speciﬁes that the declared symbol is “forgotten” once its scope has been evaluated.  The deﬁnition of the judgment τ mobile must be chosen to ensure that the following  mobility condition is satisﬁed:  If τ mobile,  cid:12  cid:25 ,a∼ρ e : τ, and e val cid:25 ,a∼ρ, then cid:12   cid:25  e : τ and e val cid:25 .  For example, in the presence of symbol references  see Section 31.2 below , a function type cannot be deemed mobile, because a function may contain a reference to a local symbol. The type nat may only be deemed mobile if the successor is evaluated eagerly, for otherwise a symbol reference may occur within a value of this type, invalidating the condition. Theorem 31.1  Preservation . If  cid:12  cid:25  e : τ and e  cid:20 −→   cid:5 , then  cid:12  cid:25  e   cid:5  : τ .  e  Proof By induction on the dynamics of symbol declaration. Rule  31.2a  follows by induction, applying rule  31.1 . Rule  31.2b  follows from the condition on mobility. Theorem 31.2  Progress . If  cid:12  cid:25  e : τ , then either e  cid:20 −→   cid:5 , or e val cid:25 .  e   cid:25    cid:25   Proof There is only one rule to consider, rule  31.1 . By induction, we have either e  cid:20 −−−→  cid:5 , in which case rule  31.2a  applies, or e val cid:25 ,a∼ρ, in which case by the  cid:25 ,a∼ρ mobility condition we have e val cid:25 , and hence rule  31.2b  applies.  e  31.1.2 Scope-Free Dynamics  The scope-free dynamics of symbols is deﬁned by a transition system between states of the form ν  cid:25  { e }, where  cid:25  is a signature and e is an expression over this signature. The  cid:5  } states that evaluation of e relative to symbols  cid:25  results judgment ν  cid:25  { e }  cid:20 −→ ν  cid:25  in the expression e   cid:5  in the extension  cid:25    cid:5  of  cid:25 .   cid:5  { e  ν  cid:25  { new{ρ} a.e }  cid:20 −→ ν  cid:25 , a ∼ ρ { e }   31.3   Rule  31.3  speciﬁes that symbol generation enriches the signature with the newly intro- duced symbol by extending the signature for all future transitions.  All other rules of the dynamics are changed to account for the allocated symbols. For example, the dynamics of function application cannot be inherited from Chapter 19 but is reformulated as follows:  ν  cid:25  { e1 }  cid:20 −→ ν  cid:25  ν  cid:25  { e1 e2 }  cid:20 −→ ν  cid:25    cid:5  { e  cid:5  { e  }  cid:5  1 1 e2 }  cid:5   ν  cid:25  { λ  x : τ  e e2 }  cid:20 −→ ν  cid:25  { [e2 x]e }   31.4a    31.4b    280  Symbols  These rules shufﬂe around the signature to account for symbol declarations within the constituent expressions of the application. Similar rules are required for all other constructs of SPCF. Theorem 31.3  Preservation . If ν  cid:25  { e }  cid:20 −→ ν  cid:25   cid:12  cid:25    cid:5  } and  cid:12  cid:25  e : τ , then  cid:25    cid:5  ⊇  cid:25  and   cid:5  : τ .   cid:5  { e   cid:5  e  Proof There is only one rule to consider, rule  31.3 , which is handled by inversion of rule  31.1 . Theorem 31.4  Progress . If  cid:12  cid:25  e : τ , then either e val cid:25  or ν  cid:25  { e }  cid:20 −→ ν  cid:25  some  cid:25    cid:5  and e   cid:5  } for   cid:5  { e   cid:5 .  Proof  Immediate, by rule  31.3 .  31.2 Symbol References  Symbols are not themselves values, but they may be used to form values. One useful example is provided by the type τ sym of symbol references. A value of this type has the form ’a, where a is a symbol in the signature. To compute with a reference, we may branch according to whether it is a reference to a speciﬁed symbol. The syntax of symbol references is given by the following grammar:  ::= sym τ   Typ τ Exp e  quote[a] is[a]{t.τ} e; e1; e2  if e is a then e1 ow e2  τ sym ’a  symbols reference comparison  The expression quote[a] is a reference to the symbol a, a value of type sym τ . The expression is[a]{t.τ} e; e1; e2  compares the value of e, which is a reference to some symbol b, with the given symbol a. If b is a, the expression evaluates to e1, and otherwise to e2.  31.2.1 Statics  The typing rules for symbol references are as follows:   cid:7   cid:12  cid:25 ,a∼ρ quote[a] : sym ρ    cid:7   cid:12  cid:25 ,a∼ρ e : sym ρ   cid:5    cid:7   cid:12  cid:25 ,a∼ρ e1 : [ρ t]τ  cid:7   cid:12  cid:25 ,a∼ρ e2 : [ρ   cid:5    t]τ   cid:7   cid:12  cid:25 ,a∼ρ is[a]{t.τ} e; e1; e2  : [ρ   cid:5    t]τ   31.5a    31.5b   Rule  31.5a  is the introduction rule for the type sym ρ . It states that if a is a symbol with associated type ρ, then quote[a] is an expression of type sym ρ . Rule  31.5b  is the   281  31.2 Symbol References  elimination rule for the type sym ρ . The type associated to the given symbol a need not be the same as the type of the symbol referred to by the expression e. If e evaluates to a reference to a, then these types will coincide, but if it refers to another symbol, b  cid:6 = a, then these types may well differ.  With this in mind, consider rule  31.5b . A priori there is a discrepancy between the type  cid:5  of the symbol referred to by e. This discrepancy is mediated by ρ of a and the type ρ the type operator t.τ.1 Regardless of the outcome of the comparison, the overall type of  cid:5   t]τ. If e evaluates to the symbol a, then we “learn” that the types ρ the expression is [ρ and ρ coincide, because the speciﬁed and referenced symbol coincide. This coincidence is  cid:6 = a, then the reﬂected by the type [ρ t]τ for e1. If e evaluates to some other symbol, a comparison evaluates to e2, which is required to have type [ρ  t]τ; no further information about the type of the symbol is acquired in this branch.   cid:5    cid:5    cid:5   The  scoped  dynamics of symbol references is given by the following rules:  31.2.2 Dynamics  quote[a] val cid:25 ,a∼ρ  is[a]{t.τ} quote[a]; e1; e2   cid:20 −−−→  cid:25 ,a∼ρ  e1  is[a]{t.τ} quote[a   cid:5     a  cid:6 = a  cid:5 ]; e1; e2   cid:20 −−−−−−−→  cid:5 ∼ρ   cid:25 ,a∼ρ,a   cid:5  e2  e  cid:20 −−−→  cid:25 ,a∼ρ is[a]{t.τ} e; e1; e2   cid:20 −−−→  cid:25 ,a∼ρ  e   cid:5  is[a]{t.τ} e   cid:5 ; e1; e2    31.6a    31.6b    31.6c    31.6d   Rules  31.6b  and  31.6c  specify that is[a]{t.τ} e; e1; e2  branches according to whether the value of e is a reference to the symbol a.  31.2.3 Safety  To ensure that the mobility condition is satisﬁed, it is important that symbol reference types not be deemed mobile. Theorem 31.5  Preservation . If  cid:12  cid:25  e : τ and e  cid:20 −→   cid:5 , then  cid:12  cid:25  e   cid:5  : τ .  e   cid:25   Proof By rule induction on rules  31.6 . The most interesting case is rule  31.6b . When  cid:5  must be the same, because each symbol has the comparison is positive, the types ρ and ρ   282  Symbols  at most one associated type. Therefore, e1, which has type [ρ as required. Lemma 31.6  Canonical Forms . If  cid:12  cid:25  e : sym ρ  and e val cid:25 , then e = quote[a] for some a such that  cid:25  =  cid:25    t]τ, also has type [ρ t]τ,  , a ∼ ρ.   cid:5    cid:5   Proof By rule induction on rules  31.5 , taking account of the deﬁnition of values. Theorem 31.7  Progress . Suppose that  cid:12  cid:25  e : τ . Then either e val cid:25 , or there exists e such that e  cid:20 −→   cid:5 .  e   cid:5    cid:25   Proof By rule induction on rules  31.5 . For example, consider rule  31.5b , in which we have that is[a]{t.τ} e; e1; e2  has some type τ and that e : sym ρ  for someρ . By induction either rule  31.6d  applies, or else we have that e val cid:25 , in which case we are assured by Lemma 31.6 that e is quote[a] for some symbol b of type ρ declared in  cid:25 . But then progress is assured by rules  31.6b  and  31.6c , because equality of symbols is decidable  either a is b or it is not .  The concept of a symbol in a programming language was considered by McCarthy in the original formulation of Lisp  McCarthy, 1965 . Unfortunately, symbols were not clearly distinguished from variables, leading to unexpected behaviors  see Chapter 32 . The present account of symbols was inﬂuenced by Pitts and Stark  1993  on the declaration of names in the π-calculus  Milner, 1999 . The associated type of a symbol may be used for applications that associate information with the symbol, such as its ﬂuid binding  see Chapter 32  or its string representation  its “print name” in Lisp jargon .  31.3 Notes  Exercises  31.1. The elimination form for symbol references given in Section 31.2 is “one-sided” in the sense that one may compare a reference to an unknown symbol to a known symbol with a known type. An alternative elimination form provides an equality test on symbol references. Formulate such a variation. 31.2. A list of type  τ sym × τ   list is called an association list. Using your solution to Exercise 31.1 deﬁne a function find that sends an association list to a mapping of type τ sym  cid:19  τ opt.  31.3. It would be more efﬁcient to represent an association list by a balanced tree associating values to symbols, but to do so would require a total ordering on symbols  at least   283  Note  among the symbols with the same associated type . What obstacles arise when introducing a linear ordering on symbols?  31.4. In Lisp a symbolic expression, or s-expression, or sexpr, may be thought of as a value  of the recursive type  sexpr  cid:2  rec s is [sym  cid:9 → sym s  ;nil  cid:9 → unit ; cons  cid:9 → s × s].  It is customary to write cons e0; e1  forfold cons ·  cid:24 e0, e1 cid:25  , where e0 : sexpr and e1 : sexpr, and to write nil for fold nil ·  cid:24  cid:25  . The list notation  e0, . . . , en−1  is then used as shorthand for the s-expression  cons e0; . . . cons en−1; nil  . . . .  Because lists involving symbols arise often, it is customary to extend the quotation notation from symbols to general s-expressions so that one need not quote each symbol contained within it. Give the deﬁnition of this extension, and work out its meaning for the special case of lists described above.  31.5. Considering symbol allocation to be an effect, give a modal formulation of SPCF along the lines of MPCF described in Chapter 29. Consider both a scoped and a scope-free extent for symbols.  Note  1 See Chapter 14 for a discussion of type operators.   32  Fluid Binding  In this chapter, we return to the concept of dynamic scoping of variables that was criticized in Chapter 8. There it was observed that dynamic scoping is problematic for at least two reasons. One is that renaming of bound variables is not respected; another is that dynamic scope is not type safe. These violations of the expected behavior of variables is intolerable, because they are at variance with mathematical practice and because they compromise modularity.  It is possible, however, to recover a type-safe analog of dynamic scoping by divorcing it from the concept of a variable, and instead introducing a new mechanism, called ﬂuid binding. Fluid binding associates to a symbol  and not a variable  a value of a speciﬁed type within a speciﬁed scope. The identiﬁcation principle for bound variables is retained, type safety is not compromised, yet some of the beneﬁts of dynamic scoping are preserved.  32.1 Statics  To account for ﬂuid binding, we enrich SPCF deﬁned in Chapter 31 with these constructs to obtain FSPCF:  Exp e  ::= put[a] e1; e2  put e1 for a in e2  get[a]  get a  binding retrieval  The expression get[a] evaluates to the value of the current binding of a, if it has one, and is stuck otherwise. The expression put[a] e1; e2  binds the symbol a to the value e1 for the duration of the evaluation of e2, at which point the binding of a reverts to what it was prior to the execution. The symbol a is not bound by the put expression but is instead a parameter of it.  The statics of FSPCF is deﬁned by judgments of the form   cid:7   cid:12  cid:25  e : τ,  much as in Chapter 31, except that here the signature associates a type to each symbol, instead of just declaring the symbol to be in scope. Thus,  cid:25  is here deﬁned to be a ﬁnite set of declarations of the form a ∼ τ such that no symbol is declared more than once in the same signature. Note that the association of a type to a symbol is not a typing assumption. In particular, the signature  cid:25  enjoys no structural properties and cannot be considered as a form of hypothesis as deﬁned in Chapter 3.   285  32.2 Dynamics  The following rules govern the new expression forms:  cid:7   cid:12  cid:25 ,a∼τ get[a] : τ   cid:7   cid:12  cid:25 ,a∼τ1 e1 : τ1  cid:7   cid:12  cid:25 ,a∼τ1 e2 : τ2   cid:7   cid:12  cid:25 ,a∼τ1 put[a] e1; e2  :τ 2  Rule  32.1b  speciﬁes that the symbol a is a parameter of the expression that must be declared in  cid:25 .  32.2 Dynamics  The dynamics of FSPCF relies on a stack-like allocation of symbols in SPCF and maintains an association of values to symbols that tracks this stack-like allocation discipline. To do  cid:5 , where  cid:25  is as in the so, we deﬁne a family of transition judgments of the form e statics, and μ is a ﬁnite function mapping some subset of the symbols declared in  cid:25  to  cid:5  ⊗ a  cid:9 → e values of the right type. If μ is deﬁned for some symbol a, then it has the form μ  cid:5  and value e. If μ is undeﬁned for some symbol a, we may regard it as having for some μ  cid:5  ⊗ a  cid:9 →  . We will write a  cid:9 → to stand for either a  cid:9 →   or a  cid:9 → e for some the form μ expression e.  μ cid:20 −→  e   cid:25   The dynamics of FSPCF is deﬁned by the following rules:  e  get[a] μ⊗a cid:9 →e  cid:20 −−−−→  cid:25 ,a∼τ  cid:20 −−−→  cid:25 ,a∼τ  cid:20 −−−→  cid:25 ,a∼τ   cid:5  e 1  e1  μ  μ  put[a] e1; e2   put[a] e   cid:5  1; e2   e1 val cid:25 ,a∼τ  e2 put[a] e1; e2  μ⊗a cid:9 →  cid:20 −−−−→  cid:25 ,a∼τ   cid:5  e 2  μ⊗a cid:9 →e1  cid:20 −−−−−→  cid:25 ,a∼τ put[a] e1; e   cid:5  2   e1 val cid:25 ,a∼τ put[a] e1; e2   e2 val cid:25 ,a∼τ  cid:20 −−−→  cid:25 ,a∼τ  e2  μ  Rule  32.2a  speciﬁes that get[a] evaluates to the current binding of a, if any. Rule  32.2b  speciﬁes that the binding for the symbol a is evaluated before the binding is created. Rule  32.2c  evaluates e2 in an environment where the symbol a is bound to the value e1, regardless of whether or not a is already bound in the environment. Rule  32.2d  eliminates the ﬂuid binding for a once evaluation of the extent of the binding has completed.   32.1a    32.1b    32.2a    32.2b    32.2c    32.2d    286  Fluid Binding  According to the dynamics of FSPCF given by rules  32.2 , there is no transition of the e if μ a  =  . The judgment e unbound cid:25  states that execution of e will  form get[a] μ cid:20 −→ lead to such a “stuck” state and is inductively deﬁned by the following rules:   cid:25   In a larger language, it would also be necessary to include error propagation rules of the sort discussed in Chapter 6.  We ﬁrst deﬁne the auxiliary judgment μ :  cid:25  by the following rules:  μ a  =    get[a] unboundμ  e1 unboundμ  put[a] e1; e2  unboundμ  e1 val cid:25  e2 unboundμ put[a] e1; e2  unboundμ  32.3 Type Safety  ∅ : ∅   cid:12  cid:25  e : τ μ :  cid:25  μ ⊗ a  cid:9 → e :  cid:25 , a ∼ τ  μ :  cid:25   μ ⊗ a  cid:9 →   :  cid:25 , a ∼ τ   32.3a    32.3b    32.3c    32.4a    32.4b    32.4c   These rules specify that, if a symbol is bound to a value, then that value must be of the type associated to the symbol by  cid:25 . No demand is made in the case that the symbol is unbound  equivalently, bound to a “black hole” . μ cid:20 −→   cid:5 , where μ :  cid:25  and  cid:12  cid:25  e : τ , then  cid:12  cid:25  e  Theorem 32.1  Preservation . If e   cid:5  : τ .  e   cid:25   Proof By rule induction on rules  32.2 . Rule  32.2a  is handled by the deﬁnition of μ :  cid:25 . Rule  32.2b  follows by induction. Rule  32.2d  is handled by inversion of rules  32.1 . Finally, rule  32.2c  is handled by inversion of rules  32.1  and induction. Theorem 32.2  Progress . If  cid:12  cid:25  e : τ and μ :  cid:25 , then either e val cid:25 , or e unboundμ, or there exists e Proof By induction on rules  32.1 . For rule  32.1a , we have  cid:25   cid:12  a ∼ τ from the premise of the rule, and hence, because μ :  cid:25 , we have either μ a  =   or μ a  = e for some e such that  cid:12  cid:25  e : τ. In the former case, we have e unboundμ, and in the latter, we have get[a] μ cid:20 −→ e. For rule  32.1b , we have by induction that either e1 val cid:25  or e1 unboundμ, or   cid:5  such that e  μ cid:20 −→   cid:5 .  e   cid:25    cid:25    287  32.4 Some Subtleties  μ cid:20 −→   cid:25    cid:5  1. In the latter two cases, we may apply rule  32.2b  or rule  32.3b , respectively. If e e1 e1 val cid:25 , we apply induction to obtain that either e2 val cid:25 , in which case rule  32.2d  applies;  cid:5  e2 unboundμ, in which case rule  32.3c  applies; or e2 2, in which case rule  32.2c  e applies.  μ cid:20 −→   cid:25   32.4 Some Subtleties  The value of put e1 for a in e2 is the value of e2, calculated in a context where a is bound to the value of e1. If e2 is of a basic type, such as nat, then the reversion of the binding of a cannot inﬂuence the meaning of the result.1  But what if the type of put e1 for a in e2 is a function type, so that the returned value is a λ-abstraction? The body of the returned λ may refer to the binding of a, which is reverted upon return from the put. For example, consider the expression put 17 for a in λ  x : nat  x + get a,   32.5   which has type nat  cid:19  nat, given that a is a symbol of type nat. Let us assume, for the sake of discussion, that a is unbound at the point at which this expression is evaluated. Evaluating the put binds a to the number 17 and returns the function λ  x : nat  x + get a. But because a is reverted to its unbound state upon exiting the put, applying this function to an argument will result in an error, unless a binding for a is given. Thus, if f is bound to the result of evaluating  32.5 , then the expression  put 21 for a in f  7    32.6    32.7   will evaluate to 28, whereas evaluation of f  7  in the absence of a surrounding binding for a will incur an error.  Contrast this with the similar expression  let y be 17 in λ  x : nat  x + y,  where we have replaced the ﬂuid-bound symbol a by a statically bound variable y. This expression evaluates to λ  x : nat  x + 17, which adds 17 to its argument when applied. There is no possibility of an unbound symbol arising at execution time, because variables are interpreted by substitution.  One way to think about this situation is to consider that ﬂuid-bound symbols serve as an alternative to passing extra arguments to a function to specialize its value when it is called. To see this, let e stand for the value of expression  32.5 , a λ-abstraction whose body is dependent on the binding of the symbol a. To use this function safely, it is necessary that the programmer provide a binding for a prior to calling it. For example, the expression  evaluates to 16, and the expression  put 7 for a in  e 9    put 8 for a in  e 9     288  Fluid Binding  evaluates to 17. Writing just e 9 , without a surrounding binding for a, results in a run-time error attempting to retrieve the binding of the unbound symbol a.  This behavior can be simulated by adding an argument to the function value that will be bound to the current binding of the symbol a at the point where the function is called. Instead of using ﬂuid binding, we would provide an extra argument at each call site, writing  and  respectively, where e  e  cid:5  is the λ-abstraction   8  9 ,   7  9   e   cid:5    cid:5   λ  y : nat  λ  x : nat  x + y.  Adding arguments can be cumbersome, though, especially when several call sites provide the same binding for a. Using ﬂuid binding, we may write put 7 for a in cid:24 e 8 , e 9  cid:25 ,  whereas using an extra argument we must write  cid:5    cid:5    cid:24 e   7  8 , e   7  9  cid:25 .  However, such redundancy can be reduced by factoring out the common part, writing  let f be e   cid:5    7  in cid:24 f  8 , f  9  cid:25 .  The awkwardness of this simulation is usually taken as an argument in favor of including ﬂuid binding in a language. The drawback, which is often perceived as an advantage, is that nothing in the type of a function reveals its dependency on the binding of a symbol. It is therefore quite easy to forget that such a binding is required, leading to run-time failures that might better be caught at compile time.  32.5 Fluid References  The get and put operations for ﬂuid binding are indexed by a symbol that must be given as part of the syntax of the operator. It is sometimes useful to defer until run-time the choice of ﬂuid on which a get or put acts. References to ﬂuids allow the name of the ﬂuid to be a value. References come equipped with analogs of the get and put primitives, but for a dynamically determined symbol.  We may extend FSPCF with ﬂuid references by adding the following syntax:  Typ τ Exp e  ::= fluid τ  ::= fl[a]  τ fluid & a getfl e  getfl e  putfl e; e1; e2  putfl e is e1 in e2  ﬂuid reference retrieval binding   289  32.6 Notes  The expression fl[a] is the symbol a considered as a value of type fluid τ . The ex- pressions getfl e  and putfl e; e1; e2  are analogs of the get and put operations for ﬂuid-bound symbols.  The statics of these constructs is given by the following rules:   cid:7   cid:12  cid:25 ,a∼τ fl[a] : fluid τ    cid:7   cid:12  cid:25  e : fluid τ   cid:7   cid:12  cid:25  getfl e  : τ   cid:7   cid:12  cid:25  e : fluid τ   cid:7   cid:12  cid:25  e1 : τ  cid:7   cid:12  cid:25  e2 : τ2   cid:7   cid:12  cid:25  putfl e; e1; e2  : τ2  Because we are using a scoped dynamics, references to ﬂuids cannot be deemed mobile.  The dynamics of references consists of resolving the referent and deferring to the under-  lying primitives acting on symbols.   32.8a    32.8b    32.8c    32.9a    32.9b    32.9c    32.9d    32.9e   fl[a] val cid:25 ,a∼τ   cid:5   e  e  μ cid:20 −→ getfl e  μ cid:20 −→   cid:25    cid:25   getfl e   cid:5    getfl fl[a]  μ cid:20 −→  get[a]   cid:25    cid:5   e  μ cid:20 −→ putfl e; e1; e2  μ cid:20 −→  e   cid:25    cid:25   putfl e   cid:5 ; e1; e2   putfl fl[a]; e1; e2  μ cid:20 −→   cid:25   put[a] e1; e2   32.6 Notes  Dynamic binding arose in early dialects of Lisp from not distinguishing variables from symbols. When separated, variables retain their substitutive meaning, and symbols give rise to a separate concept of ﬂuid binding. Allen  1978  discusses the implementation of ﬂuid binding. The present formulation here draws on Nanevski  2003 .   290  Fluid Binding  Exercises  32.1. Deep binding is an implementation of ﬂuid binding where the value associated to a symbol is stored on the control stack as part of a put frame, and is retrieved by ﬁnding the most recent such association. Deﬁne a stack machine for FSPCF that implements deep binding by extending the FPCF machine. Be sure to consider new as well as put and get. Attempting to get the binding for an unbound symbol signals a failure; otherwise, its most recent binding is returned. Where do the issues discussed in Section 32.4 arise? Hint: you will need to introduce an auxiliary judgment k ≥ k  cid:5  ? a,  cid:5 , returning its value  or which searches for the binding of the symbol a on the stack k failure  to the stack k.  32.2. Shallow binding is an implementation of ﬂuid binding that maintains a mapping sending each active symbol to a stack of values, the topmost being the active binding for that symbol. Deﬁne a stack machine for FSPCF that maintains such a mapping to facilitate access to the binding of a symbol. Hint: use evaluation states of the form k  cid:16  μ * e, where μ is a mapping each symbol a allocated on k to a stack of values, the topmost element of which, if any, is the current binding of a. Use similar forms of return and fail states, and ensure that the mapping invariant is maintained.  32.3. Exception handlers can be implemented by combining ﬂuid binding with continua- tions  Chapter 30 . Reserve a single ﬂuid-bound symbol hdlr that is always bound to the active exception handler, which is represented by a continuation accepting a value of type τexn. Raising an exception consists of throwing an exception value to this continuation. When entering the scope of a handler, a continuation representing the “otherwise” clause is put as the binding of hdlr. Give a precise formulation of exception handling based on this summary. Hint: it is important to ensure that the current handler is maintained for both normal and exceptional returns.  Note  1 As long as the successor is evaluated eagerly; if not, the following examples are adaptable to  situations where the value of e2 is a lazily evaluated number.   33  Dynamic Classiﬁcation  In Chapters 11 and 26, we investigated the use of sums for classifying values of disparate type. Every value of a classiﬁed type is labeled with a symbol that determines the type of the instance data. A classiﬁed value is decomposed by pattern matching against a known class, which reveals the type of the instance data. Under this representation, the possible classes of an object are determined statically by its type. However, it is sometimes useful to allow the possible classes of data value to be determined dynamically.  Dynamic generation of classes has many applications, most of which derive from the guarantee that a newly allocated class is distinct from all others that have been or ever will be generated. In this regard a dynamic class is a “secret” whose disclosure can be used to limit the ﬂow of information in a program. In particular, a dynamically classiﬁed value is opaque unless its identity has been disclosed by its creator. Thus, dynamic classiﬁcation can be used to ensure that an exception reaches only its intended handler, or that a message on a communication channel reaches only the intended recipient.  33.1 Dynamic Classes  A dynamic class is a symbol is generated at run-time. A classiﬁed value consists of a symbol of type τ together with a value of that type. To compute with a classiﬁed value, it is compared with a known class. If the value is of this class, the underlying instance data are passed to the positive branch; otherwise, the negative branch is taken, where it is matched against other known classes.  33.1.1 Statics  The syntax of dynamic classiﬁcation is given by the following grammar:  Typ τ Exp e  ::= clsfd ::= in[a] e   clsfd a · e  isin[a] e; x.e1; e2  match e as a · x  cid:9 → e1 ow  cid:9 → e2  classiﬁed instance comparison  The expression in[a] e  is a classiﬁed value with class a and underlying value e. The expression isin[a] e; x.e1; e2  checks whether the class of the value given by e is a. If so, the classiﬁed value is passed to e1; if not, the expression e2 is evaluated instead.    33.1a    33.1b    33.2a    33.2b    33.2c    33.2d    33.2e   292  Dynamic Classiﬁcation  The statics of dynamic classiﬁcation is deﬁned by these rules:   cid:7   cid:12  cid:25 ,a∼τ e : τ   cid:7   cid:12  cid:25 ,a∼τ in[a] e  :clsfd   cid:7   cid:12  cid:25 ,a∼τ e : clsfd  cid:7 , x : τ  cid:12  cid:25 ,a∼τ e1 : τ   cid:5   cid:7   cid:12  cid:25 ,a∼τ isin[a] e; x.e1; e2  :τ   cid:7   cid:12  cid:25 ,a∼τ e2 : τ  cid:5    cid:5   The typing judgment is indexed by a signature associating a type to each symbol. Here the type governs the instance data associated to each symbol.  33.1.2 Dynamics  To maximize the ﬂexibility in using dynamic classiﬁcation, we will consider a free dynamics for symbol generation. Within this framework, the dynamics of classiﬁcation is given by the following rules:  e val cid:25   in[a] e  val cid:25  ν  cid:25  { e }  cid:20 −→ ν  cid:25  ν  cid:25  { in[a] e }  cid:20 −→ ν  cid:25    cid:5  }   cid:5  { e  cid:5  { in[a] e   cid:5  }  ν  cid:25  { isin[a] in[a] e ; x.e1; e2 }  cid:20 −→ ν  cid:25  { [e x]e1 }  e val cid:25    cid:5  ν  cid:25  { isin[a] in[a  e  val cid:25   cid:5 ] e   cid:5     a  cid:6 = a  cid:5  ; x.e1; e2 }  cid:20 −→ ν  cid:25  { e2 }  ν  cid:25  { e }  cid:20 −→ ν  cid:25  ν  cid:25  { isin[a] e; x.e1; e2 }  cid:20 −→ ν  cid:25    cid:5  }   cid:5  { e  cid:5  { isin[a] e   cid:5 ; x.e1; e2 }  Throughout, if the states involved are well-formed, then there will be a declaration a ∼ τ for some type τ in  cid:25 .  The dynamics of the elimination form for the type clsfd relies on disequality of names  speciﬁcally, rule  33.2d  . Because disequality is not preserved under substitution, it is not sensible to consider any language construct whose dynamics relies on such a substitution. To see what goes wrong, consider the expression  match b ·  cid:24  cid:25  as a ·   cid:9 → true ow  cid:9 → match b ·  cid:24  cid:25  as b ·   cid:9 → false ow  cid:9 → true.  This expression evaluates to false, because the outer conditional is on the class a, which is a priori different from b. However, if we substitute b for a in this expression, we obtain  match b ·  cid:24  cid:25  as b ·   cid:9 → true ow  cid:9 → match b ·  cid:24  cid:25  as b ·   cid:9 → false ow  cid:9 → true,  which evaluate to true, because now the outer conditional governs the evaluation.   293  33.2 Class References  33.1.3 Safety  Theorem 33.1  Safety . 1. If  cid:12  cid:25  e : τ and ν  cid:25  { e }  cid:20 −→ ν  cid:25  2. If  cid:12  cid:25  e : τ , then either e val cid:25  or ν  cid:25  { e }  cid:20 −→ ν  cid:25    cid:5  }, then  cid:25    cid:5  { e   cid:5  ⊇  cid:25  and  cid:12  cid:25    cid:5  : τ .  cid:5  } for some e   cid:5  e   cid:5  { e   cid:5  and  cid:25    cid:5 .  Proof Similar to the safety proofs given in Chapters 11 and 31.  33.2 Class References  The type cls τ  has as values references to classes.  Typ τ Exp e  ::= cls τ  ::= cls[a]  τ cls & a mk e1; e2   mk e1; e2  isof e0; e1; x.e2; e3  isof e0; e1; x.e2; e3   class reference reference instance dispatch  The statics of these constructs is given by the following rules:   cid:7   cid:12  cid:25 ,a∼τ cls[a] :cls  τ   cid:7   cid:12  cid:25  e1 : cls τ   cid:7   cid:12  cid:25  e2 : τ   cid:7   cid:12  cid:25  mk e1; e2  :clsfd   cid:7   cid:12  cid:25  e3 : τ   cid:5    cid:5    cid:5    cid:7   cid:12  cid:25  e0 : cls τ   cid:7   cid:12  cid:25  e1 : clsfd  cid:7 , x : τ  cid:12  cid:25  e2 : τ   cid:7   cid:12  cid:25  isof e0; e1; x.e2; e3  :τ The corresponding dynamics is given by these rules:  cid:5  { e }  cid:5  1  cid:5  { mk e 1; e2 }  cid:5  ν  cid:25  { e2 }  cid:20 −→ ν  cid:25  }  cid:5  { e  cid:5  2  cid:5  { mk e1; e 2 }  cid:5   ν  cid:25  { e1 }  cid:20 −→ ν  cid:25  ν  cid:25  { mk e1; e2 }  cid:20 −→ ν  cid:25   ν  cid:25  { mk e1; e2 }  cid:20 −→ ν  cid:25   e1 val cid:25   ν  cid:25  { mk cls[a]; e }  cid:20 −→ ν  cid:25  { in[a] e }  e val cid:25   ν  cid:25  { e0 }  cid:20 −→ ν  cid:25  ν  cid:25  { isof e0; e1; x.e2; e3 }  cid:20 −→ ν  cid:25   }   cid:5  0   cid:5  { e  cid:5  { isof e  0; e1; x.e2; e3 }  cid:5   ν  cid:25  { isof cls[a]; e1; x.e2; e3 }  cid:20 −→ ν  cid:25  { isin[a] e1; x.e2; e3 }   33.3a    33.3b    33.3c    33.4a    33.4b    33.4c    33.4d    33.4e    294  Dynamic Classiﬁcation  Rules  33.4d  and  33.4e  specify that the ﬁrst argument is evaluated to determine the target class, which is then used to check whether the second argument, a classiﬁed data value, is of the target class. This formulation is a two-stage process in which e0 determines the pattern against which to match the classiﬁed value of e1.  33.3 Deﬁnability of Dynamic Classes  The type clsfd can be deﬁned in terms of symbolic references, product types, and exis- tential types by the type expression  clsfd  cid:2  ∃ t.t sym × t .  The introduction form in[a] e , in which a is a symbol with associated type is τ and e is an expression of type τ, is deﬁned to be the package  pack τ with cid:24 ’a, e cid:25  as∃ t.t sym × t .   33.5   cid:5 , and where the type associated to The elimination form isin[a] e; x.e1; e2 , of some type τ a is τ, is deﬁned in terms of symbol comparison  see Chapter 31 , together with existential and product elimination, and function types. By rule  33.1b , the type of e is clsfd, which  cid:5 , and is now the existential type  33.5 . Similarly, the branches both have the overall type τ within e1 the variable x has type τ. The elimination form for the type clsfd is deﬁned to be  open e as t with cid:24 x, y cid:25 :t sym × t in  ebody y  ,  where ebody is an expression to be deﬁned shortly. It opens the package e which is an element of the type  33.5 , decomposing it into a type t a symbol reference x of type t sym, and an  cid:5  so associated value y of type t. The expression ebody will turn out to have the type t  cid:19  τ that the application to y will be type correct.  The expression ebody compares the symbolic reference x to the symbol a of type τ,  yielding a value of type t  cid:19  τ   cid:5 . The expression ebody is is[a]{u.u  cid:19  τ  cid:5 } x; e  cid:5  1; e   cid:5  2 ,   cid:5   = τ  cid:19  τ   cid:5   = t  cid:19  τ   cid:5 . The expression e  where, as speciﬁed by rule  31.5b , e [t u] u  cid:19  τ associated to the symbol a, because the comparison is positive. On the other hand, e not “learn” anything about the type t.   cid:5  2 has type  cid:5  1 “knows” that the abstract type t is τ, the type  cid:5  2 does   cid:5  1 has type [τ u] u  cid:19  τ   cid:5  2. In the case of a positive comparison, we wish to pass the classiﬁed value to the expression e1 by substitution for the variable x. We therefore deﬁne e  It remains to choose the expressions e   cid:5  1 to be the expression   cid:5  1 and e   cid:5 , and e   cid:5  In the case of a negative comparison, no value is propagated to e2. We therefore deﬁne e 2 to be the expression  λ  x : τ  e1 : τ  cid:19  τ  λ   : t  e2 : t  cid:19  τ   cid:5   .   cid:5   .   295  33.4 Applications of Dynamic Classiﬁcation  We may then check that the statics and dynamics given in Section 33.1 are derivable under these deﬁnitions.  33.4 Applications of Dynamic Classiﬁcation  Dynamic classiﬁcation has a number of interesting applications in programming. The most obvious is to generalize dynamic dispatch  Chapter 26  to support computation over a dynamically extensible type of heterogeneous values. Introducing a new class requires introducing a new row in the dispatch matrix deﬁning the behavior of the methods on the newly deﬁned class. To allow for this, the rows of the matrix must be indexed by class references, rather than by classes, so that it is accessible without knowing statically the class.  Another application is to use dynamic classiﬁcation as a form of “perfect encryption” that ensures that classiﬁed values can neither be constructed nor deconstructed without knowing the class in question. Abstract encryption of this form can be used to ensure privacy of communication among the parties in a computation. One example of such a scenario is in channel-based communication, as will be considered in Chapter 40. Another, less obvious, application is to ensure that an exception value may only be received by the intended handler, and no other.  33.4.1 Classifying Secrets  Dynamic classiﬁcation can be used to enforce conﬁdentiality and integrity of data values in a program. A value of type clsfd may only be constructed by sealing it with some class a and may only be deconstructed by a case analysis that includes a branch for a. By controlling which parties in a multi-party interaction have access to the classiﬁer a we may control how classiﬁed values are created  ensuring their integrity  and how they are inspected  ensuring their conﬁdentiality . Any party that lacks access to a cannot decipher a value classiﬁed by a, nor may it create a classiﬁed value with this class. Because classes are dynamically generated symbols, they offer an absolute conﬁdentiality guarantee among parties in a computation.1  Consider the following simple protocol for controlling the integrity and conﬁdentiality of data in a program. A fresh symbol a is introduced, and we return a pair of functions of type   τ  cid:19  clsfd  ×  clsfd  cid:19  τ opt ,  called the constructor and destructor functions for that class, which is accomplished by writing  new a ∼ τ in   cid:24  λ  x : τ  a · x,  λ  x : clsfd  match x as a · y  cid:9 → just y  ow  cid:9 → null  cid:25 .   296  Dynamic Classiﬁcation  The ﬁrst function creates a value classiﬁed by a, and the second function recovers the instance data of a value classiﬁed by a. Outside of the scope of the declaration, the symbol a is an unguessable secret.  To enforce the integrity of a value of type τ, it sufﬁces to ensure that only trusted parties have access to the constructor. To enforce the conﬁdentiality of a value of type τ it sufﬁces to ensure that only trusted parties have access to the destructor. Ensuring the integrity of a value amounts to associating an invariant to it that is maintained by the trusted parties that may create an instance of that class. Ensuring the conﬁdentiality of a value amounts to propagating the invariant to parties that may decipher it.  33.4.2 Exception Values  Exception handling is a communication between two agents, one that may raise an excep- tion, and one that may handle it. We wish to ensure that an exception can be caught only by a designated handler, without fear that any intervening handler may intercept it. This secrecy property can be ensured by using dynamic class allocation. A new class is declared, with the ability to create an instance given only to the raising agent and the ability to match an instance given only to the handler. The exception value cannot be intercepted by any other handler, because no other handler is capable of matching it. This property is crucial to “black box” composition of programs from components. Without dynamic classiﬁcation one can never be sure that alien code cannot intercept an exception intended for a handler within one’s own code, or vice versa.  With this in mind, let us now reconsider the choice of the type τexn of exception val- ues speciﬁed in Chapter 29. There we distinguished the closed-world assumption, which amounts to deﬁning τexn to be a ﬁnite sum type known to the whole program, from the open- world assumption, which is realized by deﬁning τexn to be the type clsfd of dynamically classiﬁed values. This choice supports modularity and evolution by allowing fresh excep- tions  classes  to be allocated at will, avoiding the need for an up-front agreement on the forms of exception. Another perspective is that dynamic classiﬁcation treats an exception as a shared secret between the handler and the raiser of that exception. When an exception value is raised, it can only be intercepted and analyzed by a handler that can match the value against the speciﬁed class. It is only by using dynamic classiﬁcation that one can gain control over the ﬂow of information in a program that uses exceptions. Without it, an unintended handler can intercept an exception that was not intended for it, disrupting the logic of the program.  33.5 Notes  Dynamic classiﬁcation appears in Standard ML  Milner et al., 1997  as the type exn. The usefulness of the type exn is obscured by its too-close association with the exception mechanism. The π-calculus  Milner, 1999  popularized using “name generation” and   297  Exercises  “channel passing” to control the connectivity and information ﬂow in a process network. In Chapter 40, we shall make explicit that this aspect of the π-calculus is an application of dynamic classiﬁcation.  Exercises  33.1. Consider the following open-world named exception mechanism, which is typical of  the exception mechanism found in many languages.  exception a of τ in e raise a with e  declare an exception a of type τ in e raise exception a with value e  cid:5  handle exceptions a1, . . . , an  try e ow a1 x1   cid:9 → e1  . . .  an xn   cid:9 → en  x  cid:9 → e Exceptions are declared by name, specifying the type of their associated values. Each execution of an exception declaration generates a fresh exception. An exception is raised by specifying the exception name and a value to associate with it. The handler intercepts any ﬁnite number of named exceptions, passing their associated values to handlers and otherwise propagates the exception to the default handler.  The following rules deﬁne the statics of these constructs:   cid:7   cid:12  cid:25 ,a∼τ e : τ   cid:5    cid:7   cid:12  cid:25  exception a of τ in e : τ   cid:5    cid:25   cid:12  a ∼ τ  cid:7   cid:12  cid:25  e : τ  cid:7   cid:12  cid:25  raise a with e : τ . . .  cid:25   cid:12  an   cid:5   ∼ τn  cid:7   cid:12  cid:25  e : τ   cid:5    cid:25   cid:12  a1 ∼ τ1   33.6a    33.6b    33.6c    cid:7 , x1 : τ1  cid:12  cid:25  e1 : τ   cid:5   . . .  cid:7 , xn : τn  cid:12  en : τ   cid:5    cid:7   cid:12  cid:25  try e ow a1 x1   cid:9 → e1  . . .  an xn   cid:9 → en  x  cid:9 → e   cid:7 , x : τexn  cid:12  e  cid:5   cid:5   cid:5  : τ   cid:5   : τ  Give an implementation of named exceptions in terms of dynamic classiﬁcation and general value-passing exceptions  Chapter 29 .  33.2. Show that dynamic classiﬁcation with dynamic classes can be implemented in the combination of FPC and FEω enriched with references to free assignables, and with no modal separation  so as to permit benign effects . Speciﬁcally, provide a package of the following higher-kind existential type:  τ  cid:2  ∃ clsfd :: T.∃ class :: T → T. cid:24 new  cid:9 → τnew, mk  cid:9 → τmk, isof  cid:9 → τisof cid:25 .  where  τnew  cid:2  ∀ t.cls[t]  τmk  cid:2  ∀ t. cls[t] × t  → clsfd  τisof  cid:2  ∀ t.∀ u. cls[t] × clsfd ×  t → u  × u  → u  .  These operations correspond to the mechanisms of dynamic classiﬁcation described earlier in this chapter. Hint: Deﬁne cls[t] to be t opt ref, and deﬁne clsfd so that a   298  Dynamic Classiﬁcation  classiﬁed value is represented by an encapsulated assignment to its class. Creating a new class allocates a reference, creating a classiﬁed value creates an encapsulated assignment, and testing for a class is implemented by assigning to the target class, then running the classiﬁed value, and seeing whether the contents of the target class has changed.  33.3. Open-world named exceptions obstruct exception tracking  as described in Exer-  cise 29.4 .  a  Show that it is not computable to track the exact set of exception names that  might be raised by an expression.   b  Show that it is impossible to ﬁnitely bound set of exceptions that can be thrown by an expression. Hint: Show that there are expressions for which any such upper bound is inaccurate.  33.4. Exercise 33.3 may seem disappointing, until you realize that whereas positive ex- ception tracking is impossible under the open-world assumption, negative exception tracking is not only possible, but more desirable. It is often more useful to know that a speciﬁed exception cannot be raised than it is to know that it can. Negative exception tracking can be expressed using exclusion reﬁnements of the form X, where X is a ﬁnite set of dynamic classes. Informally, a value satisﬁes such a reﬁnement only if its class is not among those in the set X. Deﬁne a system of exclusion reﬁnements by deﬁning entailment and satisfaction for them. Be sure to state reﬁnement rules for class allocation and for the introduction and elimination forms for the type clsfd.  Note  1 Of course, this guarantee is for programs written in conformance with the statics given here. If the abstraction imposed by the type system is violated, no guarantees of conﬁdentiality can be made.   P A R T XIV  Mutable State    34  Modernized Algol  Modernized Algol, orMA , is an imperative, block-structured programming language based on the classic language Algol. MA extends PCF with a new syntactic sort of commands that act on assignables by retrieving and altering their contents. Assignables are introduced by declaring them for use within a speciﬁed scope; this is the essence of block structure. Commands are combined by sequencing and are iterated using recursion.  MA maintains a careful separation between pure expressions, whose meaning does not depend on any assignables, and impure commands, whose meaning is given in terms of assignables. The segregation of pure from impure ensures that the evaluation order for expressions is not constrained by the presence of assignables in the language, so that they can be manipulated just as in PCF. Commands, on the other hand, have a constrained execution order, because the execution of one may affect the meaning of another.  A distinctive feature of MA is that it adheres to the stack discipline, which means that assignables are allocated on entry to the scope of their declaration, and deallocated on exit, using a conventional stack discipline. Stack allocation avoids the need for more complex forms of storage management, at the cost of reducing the expressive power of the language.  34.1 Basic Commands  The syntax of the language MA of modernized Algol distinguishes pure expressions from impure commands. The expressions include those of PCF  as described in Chapter 19 , augmented with one construct, and the commands are those of a simple imperative program- ming language based on assignment. The language maintains a sharp distinction between variables and assignables. Variables are introduced by λ-abstraction and are given mean- ing by substitution. Assignables are introduced by a declaration and are given meaning by assignment and retrieval of their contents, which is, for the time being, restricted to natural numbers. Expressions evaluate to values, and have no effect on assignables. Com- mands are executed for their effect on assignables, and return a value. Composition of commands not only sequences their execution order, but also passes the value returned by the ﬁrst to the second before it is executed. The returned value of a command is, for the time being, restricted to the natural numbers.  But see Section 34.3 for the general case.    302  Modernized Algol  The syntax of MA is given by the following grammar, from which we have omitted  repetition of the expression syntax of PCF for the sake of brevity.  ::= cmd Typ ::= cmd m  Exp Cmd m ::= ret e   τ e  cmd cmd m ret e  command encapsulation return sequence  bnd e; x.m  bnd x ← e ; m dcl e; a.m  dcl a := e in m new assignable get[a] set[a] e   fetch assign  @ a a := e  The expression cmd m  consists of the unevaluated command m thought of as a value of type cmd. The command ret e  returns the value of the expression e without having any effect on the assignables. The command bnd e; x.m  evaluates e to an encapsulated command, then this command is executed for its effects on assignables, with its value substituted for x in m. The command dcl e; a.m  introduces a new assignable, a, for use within the command m whose initial contents is given by the expression e. The command get[a] returns the current contents of the assignable a and the command set[a] e  changes the contents of the assignable a to the value of e, and returns that value.  34.1.1 Statics  The statics of MA consists of two forms of judgment: 1. Expression typing:  cid:7   cid:12  cid:25  e : τ. 2. Command formation:  cid:7   cid:12  cid:25  m ok.  The context  cid:7  speciﬁes the types of variables, as usual, and the signature  cid:25  consists of a ﬁnite set of assignables. As with other uses of symbols, the signature cannot be interpreted as a form of typing hypothesis  it enjoys no structural properties of entailment , but must be considered as an index of a family of judgments, one for each choice of  cid:25 .  The statics of MA is inductively deﬁned by the following rules:   cid:7   cid:12  cid:25  m ok   cid:7   cid:12  cid:25  cmd m  : cmd   cid:7   cid:12  cid:25  e : nat  cid:7   cid:12  cid:25  ret e  ok   cid:7   cid:12  cid:25  e : cmd  cid:7 , x : nat  cid:12  cid:25  m ok   cid:7   cid:12  cid:25  bnd e; x.m  ok   cid:7   cid:12  cid:25  e : nat  cid:7   cid:12  cid:25 ,a m ok   cid:7   cid:12  cid:25  dcl e; a.m  ok   34.1a    34.1b    34.1c    34.1d    303  34.1 Basic Commands   cid:7   cid:12  cid:25 ,a get[a] ok  cid:7   cid:12  cid:25 ,a e : nat   cid:7   cid:12  cid:25 ,a set[a] e  ok   34.1e    34.1f   Rule  34.1a  is the introduction rule for the type cmd, and rule  34.1c  is the corresponding elimination form. Rule  34.1d  introduces a new assignable for use within a speciﬁed command. The name a of the assignable is bound by the declaration, and so may be renamed to satisfy the implicit constraint that it not already occur in  cid:25 . Rule  34.1e  states that the command to retrieve the contents of an assignable a returns a natural number. Rule  34.1f  states that we may assign a natural number to an assignable.  34.1.2 Dynamics  The dynamics of MA is deﬁned in terms of a memory μ a ﬁnite function assigning a numeral to each of a ﬁnite set of assignables.  The dynamics of expressions consists of these two judgment forms:  1. e val cid:25 , stating that e is a value relative to  cid:25 . 2. e  cid:20 −→   cid:5 , stating that the expression e steps to the expression e  e   cid:5 .   cid:25   These judgments are inductively deﬁned by the following rules, together with the rules deﬁning the dynamics of PCF  see Chapter 19 . It is important, however, that the successor operation be given an eager, instead of lazy, dynamics so that a closed value of type nat is a numeral  for reasons that will be explained in Section 34.3 .  cmd m  val cid:25    34.2a   Rule  34.2a  states that an encapsulated command is a value.  The dynamics of commands is deﬁned in terms of states m  cid:16  μ, where μ is a memory mapping assignables to values, and m is a command. There are two judgments governing such states: 1. m  cid:16  μ ﬁnal cid:25 . The state m  cid:16  μ is complete. 2. m  cid:16  μ  cid:20 −→   cid:5 . The state m  cid:16  μ steps to the state m   cid:5 ; the set of active assignables   cid:5   cid:16  μ   cid:5   cid:16  μ  m   cid:25   is given by the signature  cid:25 .  These judgments are inductively deﬁned by the following rules:  e val cid:25   ret e   cid:16  μ ﬁnal cid:25   e  cid:20 −→ ret e   cid:16  μ  cid:20 −→   cid:25    cid:5   e   cid:25   ret e   cid:5    cid:16  μ   34.3a    34.3b    304  Modernized Algol  e  cid:20 −→ bnd e; x.m   cid:16  μ  cid:20 −→   cid:25    cid:5   e   cid:25   bnd e   cid:5 ; x.m   cid:16  μ  bnd cmd ret e  ; x.m   cid:16  μ  cid:20 −→  [e x]m  cid:16  μ  e val cid:25   m1  cid:16  μ  cid:20 −→ bnd cmd m1 ; x.m2   cid:16  μ  cid:20 −→   cid:25    cid:25    cid:5  1  m   cid:16  μ  cid:5   bnd cmd m   cid:25   1 ; x.m2   cid:16  μ  cid:5   cid:5   get[a]  cid:16  μ ⊗ a  cid:9 → e  cid:20 −−→  ret e   cid:16  μ ⊗ a  cid:9 → e   cid:25 ,a  e  cid:20 −−→ set[a] e   cid:16  μ  cid:20 −−→   cid:25 ,a   cid:5   e   cid:25 ,a  set[a] e   cid:5    cid:16  μ  e val cid:25 ,a set[a] e   cid:16  μ ⊗ a  cid:9 →  cid:20 −−→  ret e   cid:16  μ ⊗ a  cid:9 → e   cid:25 ,a  e  cid:20 −→ dcl e; a.m   cid:16  μ  cid:20 −→   cid:25    cid:5   e   cid:25   dcl e   cid:5 ; a.m   cid:16  μ  e val cid:25  m  cid:16  μ ⊗ a  cid:9 → e  cid:20 −−→  cid:25 ,a dcl e  dcl e; a.m   cid:16  μ  cid:20 −→   cid:5   cid:16  μ m  cid:5 ; a.m   cid:5  ⊗ a  cid:9 → e  cid:5    cid:16  μ  cid:5    cid:5    cid:25    cid:5   e val cid:25  dcl e; a.ret e  val cid:25 ,a  cid:5     cid:16  μ  cid:20 −→  e  ret e   cid:5    cid:16  μ   cid:25    34.3c    34.3d    34.3e    34.3f    34.3g    34.3h    34.3i    34.3j    34.3k   Rule  34.3a  speciﬁes that a ret command is ﬁnal if its argument is a value. Rules  34.3c  to  34.3e  specify the dynamics of sequential composition. The expression e must, by virtue of the type system, evaluate to an encapsulated command, which is executed to ﬁnd its return value, which is then substituted into the command m before executing it.  Rules  34.3i  to  34.3k  deﬁne the concept of block structure in a programming language. Declarations adhere to the stack discipline in that an assignable is allocated during evaluation of the body of the declaration, and deallocated after evaluation of the body is complete. Therefore, the lifetime of an assignable can be identiﬁed with its scope, and hence we may visualize the dynamic lifetimes of assignables as being nested inside one another, in the same way as their static scopes are nested inside one another. The stack-like behavior of assignables is a characteristic feature of what are known as Algol-like languages.   305  34.1 Basic Commands  The judgment m  cid:16  μ ok cid:25  is deﬁned by the rule  34.1.3 Safety   cid:12  cid:25  m ok μ :  cid:25   m  cid:16  μ ok cid:25   where the auxiliary judgment μ :  cid:25  is deﬁned by the rule  ∀a ∈  cid:25  ∃e μ a  = e and e val∅ and  cid:12 ∅ e : nat  μ :  cid:25   That is, the memory must bind a number to each assignable in  cid:25 .  Theorem 34.1  Preservation .   cid:5  and  cid:12  cid:25  e : τ , then  cid:12  cid:25  e   cid:5  : τ .  1. If e  cid:20 −→ 2. If m  cid:16  μ  cid:20 −→  e   cid:25    cid:25    cid:5   cid:16  μ   cid:5 , with  cid:12  cid:25  m ok and μ :  cid:25 , then  cid:12  cid:25  m   cid:5   m  ok and μ   cid:5  :  cid:25 .   34.4    34.5   Proof Simultaneously, by induction on rules  34.2  and  34.3 . Consider rule  34.3j . Assume that  cid:12  cid:25  dcl e; a.m  ok and μ :  cid:25 . By inversion of typing we have  cid:12  cid:25  e : nat and  cid:12  cid:25 ,a m ok. Because e val cid:25  and μ :  cid:25 , we have μ ⊗ a  cid:9 → e :  cid:25 , a. By induction, we have  cid:12  cid:25 ,a m  cid:5  :  cid:25 , a, from which the result follows immediately. Consider rule  34.3k . Assume that  cid:12  cid:25  dcl e; a.ret e  cid:5    ok and μ :  cid:25 . By inversion we have  cid:12  cid:25  e : nat, and  cid:12  cid:25 ,a ret e  cid:5   ok, and so  cid:12  cid:25 ,a e  cid:5  val cid:25 ,a, and e is a numeral, and we also have  cid:12  cid:25  e  cid:5  : nat, as required.   cid:5  : nat. But because e   cid:5  ⊗ a  cid:9 → e  ok and μ   cid:5    cid:5   Theorem 34.2  Progress .  1. If  cid:12  cid:25  e : τ , then either e val cid:25 , or there exists e  cid:5  such that e  cid:20 −→ 2. If  cid:12  cid:25  m ok and μ :  cid:25 , then either m  cid:16  μ ﬁnal cid:25  or m  cid:16  μ  cid:20 −→   cid:25   e   cid:5 .  cid:5   cid:16  μ m   cid:5  for some μ   cid:5  and   cid:25    cid:5 .  m  Proof Simultaneously, by induction on rules  34.1 . Consider rule  34.1d . By the ﬁrst inductive hypothesis we have either e  cid:20 −→  cid:5  or e val cid:25 . In the former case, rule  34.3i  applies. In the latter, we have by the second inductive hypothesis,  e   cid:25   m  cid:16  μ ⊗ a  cid:9 → e ﬁnal cid:25 ,a  or m  cid:16  μ ⊗ a  cid:9 → e  cid:20 −−→   cid:5   cid:16  μ   cid:5  ⊗ a  cid:9 → e   cid:5   .  m   cid:25 ,a  In the former case, we apply rule  34.3k , and in the latter, rule  34.3j .   306  Modernized Algol  34.2 Some Programming Idioms  The language MA is designed to expose the elegant interplay between the execution of an expression for its value and the execution of a command for its effect on assignables. In this section we show how to derive several standard idioms of imperative programming in MA. We deﬁne the sequential composition of commands, written {x ← m1 ; m2}, to stand for the command bnd x ← cmd  m1  ; m2. Binary composition readily generalizes to an n-ary form by deﬁning  to stand for the iterated composition  {x1 ← m1 ; . . . xn−1 ← mn−1 ; mn},  {x1 ← m1 ; . . .{xn−1 ← mn−1 ; mn}}.  We sometimes write just {m1 ; m2} for the composition { ← m1 ; m2} where the returned value from m1 is ignored; this generalizes in the obvious way to an n-ary form. result. By deﬁnition, do e stands for the command bnd x ← e ; ret x.  A related idiom, the command do e, executes an encapsulated command and returns its  The conditional command if  m  m1 else m2 executes either m1 or m2 according to  whether the result of executing m is zero or not:  {x ← m ; do  ifz x {z  cid:9 → cmd m1  s     cid:9 → cmd m2} }.  The returned value of the conditional is the value returned by the selected command.  The while loop command while  m1  m2 repeatedly executes the command m2 while  the command m1 yields a non-zero number. It is deﬁned as follows:  do  fix loop : cmd is cmd  if  m1 {ret z} else {m2 ; do loop}  .  This command runs the self-referential encapsulated command that, when executed, ﬁrst executes m1, branching on the result. If the result is zero, the loop returns zero  arbitrarily . If the result is non-zero, the command m2 is executed and the loop is repeated.  A procedure is a function of type τ  cid:19  cmd that takes an argument of some type τ and yields an unexecuted command as result. Many procedures have the form λ  x : τ   cmd m, which we abbreviate to proc  x : τ  m. Aprocedure call is the composition of a function application with the activation of the resulting command. If e1 is a procedure and e2 is its argument, then the procedure call call e1 e2  is deﬁned to be the command do  e1 e2  , which immediately runs the result of applying e1 to e2.   307  34.3 Typed Commands and Typed Assignables  As an example, here is a procedure of type nat  cid:19  cmd that returns the factorial of its  argument:  proc  x:nat  { dcl r := 1 in dcl a := x in { while   @ a  { y ← @ r ; z ← @ a ; r :=  x-z+1 × y ; a := z-1 } ; x ← @ r ; ret x  }  }  The loop maintains the invariant that the contents of r is the factorial of x minus the contents of a. Initialization makes this invariant true, and it is preserved by each iteration of the loop, so that upon completion of the loop the assignable a contains 0 and r contains the factorial of x, as required.  34.3 Typed Commands and Typed Assignables  So far we have restricted the type of the returned value of a command, and the contents of an assignable, to be nat. Can this restriction be relaxed, while adhering to the stack discipline?  The key to admitting returned and assignable values of other types may be uncovered by a close examination of the proof of Theorem 34.1. For the proof to go through, it is crucial that values of type nat, the type of assignables and return values, cannot contain an assignable, for otherwise the embedded assignable would escape the scope of its declaration. This property is self-evidently true for eagerly evaluated natural numbers but fails when they are evaluated lazily. Thus, the safety of MA hinges on the evaluation order for the successor operation, in contrast to most other situations where either interpretation is also safe.  When extending MA to admit assignables and returned values of other types, it is necessary to pay close attention to whether assignables can be embedded in a value of a candidate type. For example, if return values of procedure type are allowed, then the following command violates safety:  dcl a := z in {ret  proc  x : nat {a := x} }.  This command, when executed, allocates a new assignable a and returns a procedure that, when called, assigns its argument to a. But this makes no sense, because the assignable a   308  Modernized Algol  is deallocated when the body of the declaration returns, but the returned value still refers to it. If the returned procedure is called, execution will get stuck in the attempt to assign to a. A similar example shows that admitting assignables of procedure type is also unsound. For example, suppose that b is an assignable whose contents are of type nat  cid:19  cmd, and consider the command  dcl a := z in {b := proc  x : nat  cmd a := x  ; ret z}.  We assign to b a procedure that uses a locally declared assignable a and then leaves the scope of the declaration. If we then call the procedure stored in b, execution will get stuck attempting to assign to the non-existent assignable a.  To admit declarations that return values other than nat and to admit assignables with contents of types other than nat, we must rework the statics of MA to record the returned type of a command and to record the type of the contents of each assignable. First, we generalize the ﬁnite set  cid:25  of active assignables to assign a mobile type to each active assignable so that  cid:25  has the form of a ﬁnite set of assumptions of the form a ∼ τ, where a is an assignable. Second, we replace the judgment  cid:7   cid:12  cid:25  m ok by the more general form  cid:7   cid:12  cid:25  m ∼·· τ, stating that m is a well-formed command returning a value of type τ. Third,  the type cmd is generalized to cmd τ , which is written in examples as τ cmd, to specify the return type of the encapsulated command.  The statics given in Section 34.1.1 is generalized to admit typed commands and typed  assignables as follows:   cid:7   cid:12  cid:25  e : cmd τ   cid:7 , x : τ  cid:12  cid:25  m ∼·· τ   cid:5    cid:7   cid:12  cid:25  e : τ   cid:5    cid:5   τ  mobile   cid:7   cid:12  cid:25  m ∼·· τ  cid:7   cid:12  cid:25  e : τ   cid:7   cid:12  cid:25  cmd m  :cmd τ    cid:7   cid:12  cid:25  ret e  ∼·· τ  cid:7   cid:12  cid:25  bnd e; x.m  ∼·· τ τ mobile  cid:7   cid:12  cid:25 ,a∼τ m ∼·· τ  cid:7   cid:12  cid:25  dcl e; a.m  ∼·· τ  cid:7   cid:12  cid:25 ,a∼τ get[a] ∼·· τ  cid:7   cid:12  cid:25 ,a∼τ set[a] e  ∼·· τ   cid:7   cid:12  cid:25 ,a∼τ e : τ   cid:5    cid:5    34.6a    34.6b    34.6c    34.6d    34.6e    34.6f   Apart from the generalization to track returned types and content types, the most im- portant change is that in rule  34.6d  both the type of a declared assignable and the return type of the declaration is required to be mobile. The deﬁnition of the judgment τ mobile is guided by the following mobility condition:  if τ mobile,  cid:12  cid:25  e : τ and e val cid:25 , then  cid:12 ∅ e : τ and e val∅. That is, a value of mobile type may not depend on any active assignables.   34.7    309  34.3 Typed Commands and Typed Assignables  As long as the successor operation is evaluated eagerly, the type nat is mobile:   34.8    34.9    34.10   Similarly, a product of mobile types may safely be deemed mobile, if pairs are evaluated eagerly:  And the same goes for sums, if the injections are evaluated eagerly:  nat mobile  τ1 mobile  τ2 mobile  τ1 × τ2 mobile  τ1 mobile  τ1 + τ2 mobile  τ2 mobile  In each of these cases, laziness defeats mobility, because values may contain suspended computations that depend on an assignable. For example, if the successor operation for the natural numbers were evaluated lazily, then s e  would be a value for any expression e including one that refers to an assignable a.  Because the body of a procedure may involve an assignable, no procedure type is mobile, nor is any command type. What about function types other than procedure types? We may think they are mobile, because a pure expression cannot depend on an assignable. Although this is the case, the mobility condition need not hold. For example, consider the following value of type nat  cid:19  nat:  λ  x : nat   λ   : τ cmd  z  cmd{@ a} .  Although the assignable a is not actually needed to compute the result, it nevertheless occurs in the value, violating the mobility condition.  The mobility restriction on the statics of declarations ensures that the type associated to an assignable is always mobile. We may therefore assume, without loss of generality, that the types associated to the assignables in the signature  cid:25  are mobile.  Theorem 34.3  Preservation for Typed Commands .  1. If e  cid:20 −→ 2. If m  cid:16  μ  cid:20 −→  e   cid:25    cid:25    cid:5  and  cid:12  cid:25  e : τ , then  cid:12  cid:25  e   cid:5  : τ .   cid:5   cid:16  μ  m   cid:5 , with  cid:12  cid:25  m ∼·· τ and μ :  cid:25 , then  cid:12  cid:25  m   cid:5  ∼·· τ and μ   cid:5  :  cid:25 .  Theorem 34.4  Progress for Typed Commands .  1. If  cid:12  cid:25  e : τ , then either e val cid:25 , or there exists e  cid:5  such that e  cid:20 −→ 2. If  cid:12  cid:25  m ∼·· τ and μ :  cid:25 , then either m  cid:16  μ ﬁnal cid:25  or m  cid:16  μ  cid:20 −→   cid:25   e   cid:5 .  cid:5   cid:16  μ  m   cid:5  for some μ   cid:5  and   cid:25    cid:5 .  m  The proofs of Theorems 34.3 and 34.4 follows very closely the proof of Theorems 34.1 and 34.2. The main difference is that we appeal to the mobility condition to ensure that returned values and stored values are independent of the active assignables.   310  Modernized Algol  34.4 Notes  Modernized Algol is a derivative of Reynolds’s Idealized Algol  Reynolds, 1981 . In contrast to Reynolds’s formulation, Modernized Algol maintains a separation between computations that depend on the memory and those that do not, and does not rely on call-by-name for function application, but rather has a type of encapsulated commands that can be used where call-by-name would otherwise be required. The modal distinction between expressions and commands was present in the original formulation of Algol 60 but is developed here in light of the concept of monadic effects introduced by Moggi  1989 . Its role in functional programming was emphasized by Wadler  1992 . The modal separation in MA is adapted directly from Pfenning and Davies  2001 , which stresses the connection to lax modal logic.  What are called assignables here are invariably called variables elsewhere. The distinc- tion between variables and assignables is blurred in languages that allow assignables as forms of expression.  Indeed, Reynolds himself1 regards this as a deﬁning feature of Algol, in opposition to the formulation given here.  In MA, we choose to make the distinction between variables, which are given meaning by substitution, and assignables, which are given meaning by mutation. Drawing this distinction requires new terminology; the term assignable seems apt for the imperative programming concept.  The concept of mobility of a type was introduced in the ML5 language for distributed computing  Murphy et al., 2004 , with the similar meaning that a value of a mobile type cannot depend on local resources. Here the mobility restriction is used to ensure that the language adheres to the stack discipline.  Exercises  34.1. Originally, Algol had both scalar assignables, whose contents are atomic values, and array assignables, which is a ﬁnite sequence of scalar assignables. Like scalar assignables, array assignables are stack-allocated. Extend MA with array assignables, ensuring that the language remains type safe, but allowing that computation may abort if a non-existent array element is accessed.  34.2. Consider carefully the behavior of assignable declarations within recursive proce-  dures, as in the following expression  fix p is λ  p : τ  dcl a := e in cmd m   of type τ  cid:19  ρ cmd for some ρ. Because p is recursive, the body m of the procedure may call itself during its execution, causing the same declaration to be executed more than once. Explain the dynamics of getting and setting a in such a situation.  34.3. Originally, Algol considered assignables as expressions that stand for their contents in memory. Thus, if a is an assignable containing a number, one could write expressions   311  Exercises  such as a + a that would evaluate to twice the contents of a. Moreover, one could write commands such as a := a + a to double the contents of a. These conventions encouraged programmers to think of assignables as variables, quite the opposite of their separation in MA. This convention, combined with an over-emphasis on concrete syntax, led to a conundrum about the different roles of a in the above assignment command: its meaning on the left of the assignment is different from its meaning on the right. These came to be called the left-, or l-value, and the right-, or r-value of the assignable a, corresponding to its position in the assignment statement. When viewed as abstract syntax, though, there is no ambiguity to be explained: the assignment operator is indexed by its target assignable, instead of taking as argument an expression that happens to be an assignable, so that the command is set[a] a + a , not set a; a + a .  This still leaves the puzzle of how to regard assignables as forms of expression. As a ﬁrst cut, reformulate the dynamics of MA to account for this. Reformulate the  cid:5  and e  cid:16  μ ﬁnal dynamics of expressions in terms of the judgments e  cid:16  μ  cid:20 −→ that allow evaluation of e to depend on the contents of the memory. Each use of an assignable as an expression should require one access to the memory. Then prove memory invariance:: if e  cid:16  μ  cid:20 −→   cid:5 , then μ   cid:5  = μ.   cid:5   cid:16  μ   cid:5   cid:16  μ  e  e   cid:25   A natural generalization is to allow any sequence of commands to be considered as an expression, if they are all passive in the sense that no assignments are allowed. Write do{m}, where m is a passive command, for a passive block whose evaluation consists of executing the command m on the current memory, using its return value as the value of the expression. Observe that memory invariance holds for passive blocks. The use of an assignable a as an expression may now be rendered as the passive block do{@ a}. More complex uses of assignables as expressions admit several different interpretations using passive blocks. For example, an expression such as a + a might be rendered in one of two ways:  a  do{@ a} + do{@ a}, or  b  let x be do{@ a} in x + x. The latter formulation accesses a only once, but uses its value twice. Comment on there being two different interpretations of a + a.   cid:25   34.4. Recursive procedures in Algol are declared using a command of the form  proc p x : τ  :ρ is m in m   cid:5 , which is governed by the typing rule  cid:7 , p : τ  cid:19  ρ cmd, x : τ  cid:12  cid:25  m ∼·· ρ  cid:7 , p : τ  cid:19  ρ cmd  cid:12  cid:25  m  cid:5  ∼·· τ   cid:5    cid:7   cid:12  cid:25  proc p x : τ  :ρ is m in m   cid:5  ∼·· τ   cid:5   .   34.11   From the present viewpoint, it is peculiar to insist on declaring procedures at all, because they are simply values of procedure type, and even more peculiar to insist that they be conﬁned for use within a command. One justiﬁcation for this limitation, though, is that Algol included a peculiar feature, called an own variable2 that was declared for use within the procedure, but whose state persisted across calls to the procedure. One application would be to a procedure that generated pseudo-random   312  Modernized Algol  numbers based on a stored seed that inﬂuenced the behavior of successive calls to it. Give a formulation in MA of the extended declaration  proc p x : τ   :ρ is {own a := e in m} in m  cid:5   where a is declared as an “own” of the procedure p. Contrast the meaning of the foregoing declaration with the following one:  proc p x : τ  : ρ is {dcl a := e in m} in m  cid:5   .  34.5. A natural generalization of own assignables is to allow the creation of many such scenarios for a single procedure  or mutually recursive collection of procedures , with each instance creating its own persistent state. This ability motivated the concept of a class in Simula-67 as a collection of procedures, possibly mutually recursive, that shared common persistent state. Each instance of a class is called an object of that class; calls to its constituent procedures mutate the private persistent state. Formulate this 1967 precursor of imperative object-oriented programming in the context of MA. 34.6. There are several ways to formulate an abstract machine for MA that accounts for both the control stack, which sequences execution  as described in Chapter 28 for PCF , and the data stack, which records the contents of the assignables. A consolidated stack combines these two separate concepts into one, whereas separated stacks keeps the memory separate from the control stack, much as we have done in the structural dynamics given by rules  34.3 . In either case, the storage required for an assignable is deallocated when exiting the scope of that assignable, a key beneﬁt of the stack discipline for assignables in MA.   cid:5  and e val , and a stack machine dynamics for commands.  With a modal separation between expressions and commands, it is natural to use a structural dynamics for expressions  given by the transition and value judgments, e  cid:20 −→ e  a  Formulate a consolidated stack machine where both assignables and stack frames are recorded on the same stack. Consider states k * cid:25  m, where  cid:12  cid:25  k ,: τ and  cid:12  cid:25  m ∼·· τ, and k , cid:25  e, where  cid:12  cid:25  k ,: τ and  cid:12  cid:25  e : τ. Comment on the  implementation methods required for a consolidated stack.  b  Formulate a separated stack machine where the memory is maintained separately from the control stack. Consider states of the form μ  cid:16  k * cid:25  m, where μ :  cid:25 ,  cid:12  cid:25  k ,: τ, and  cid:12  cid:25  m ∼·· τ, and of the form μ  cid:16  k , cid:25  e, where  cid:12  cid:25  k ,: τ,  cid:12  cid:25  e : τ, and e val.  Notes  1 Personal communication, 2012. 2 That is to say, an own assignable.   35  Assignable References  A reference to an assignable a is a value, written &a, ofreference type that determines the assignable a. A reference to an assignable provides the capability to get or set the contents of that assignable, even if the assignable itself is not in scope when it is used. Two references can be compared for equality to test whether they govern the same underlying assignable. If two references are equal, then setting one will affect the result of getting the other; if they are not equal, then setting one cannot inﬂuence the result of getting from the other. Two references that govern the same underlying assignable are aliases. Aliasing complicates reasoning about programs that use references, because any two references may refer to the assignable.  Reference types are compatible with both a scoped and a scope-free allocation of assignables. When assignables are scoped, the range of signiﬁcance of a reference type is limited to the scope of the assignable to which it refers. Reference types are therefore immobile, so that they cannot be returned from the body of a declaration, nor stored in an assignable. Although ensuring adherence to the stack discipline, this restriction precludes using references to create mutable data structures, those whose structure can be altered during execution. Mutable data structures have a number of applications in programming, including improving efﬁciency  often at the expense of expressiveness  and allowing cyclic  self-referential  structures to be created. Supporting mutability requires that assignables be given a scope-free dynamics, so that their lifetime persists beyond the scope of their declaration. Consequently, all types are mobile, so that a value of any type may be stored in an assignable or returned from a command.  35.1 Capabilities  The commands get[a] and set[a] e  in MA operate on statically speciﬁed assignable a. Even to write these commands requires that the assignable a be in scope where the command occurs. But suppose that we wish to deﬁne a procedure that, say, updates an assignable to double its previous value, and returns the previous value. We can write such a procedure for any given assignable, a, but what if we wish to write a generic procedure that works for any given assignable?  One way to do this is give the procedure the capability to get and set the contents of some caller-speciﬁed assignable. Such a capability is a pair consisting of a getter and a setter for that assignable. The getter for an assignable a is a command that, when executed, returns   314  Assignable References  the contents of a. The setter for an assignable a is a procedure that, when applied to a value of suitable type, assigns that value to a. Thus, a capability for an assignable a containing a value of type τ is a value of type  τ cap  cid:2  τ cmd ×  nat  cid:19  nat cmd .  A capability for getting and setting an assignable a containing a value of type τ is given by the pair   cid:24 cmd  @ a , proc  x : τ  a := x cid:25   of type τ cap. Because a capability type is a product of a command type and a procedure type, no capability type is mobile. Thus, a capability cannot be returned from a command, nor stored into an assignable. This is as it should be, for otherwise we would violate the stack discipline for allocating assignables.  The proposed generic doubling procedure is programmed using capabilities as follows: proc   cid:24 get, set cid:25  : nat cmd ×  nat  cid:19  nat cmd  {x ← do get ; y ← do  set x + x   ; ret x}. The procedure is called with the capability to access an assignable a. When executed, it invokes the getter to obtain the contents of a, and then invokes the setter to assign to a, returning the previous value. Observe that the assignable a need not be accessible by this procedure; the capability given by the caller comprises the commands required to get and set a.  35.2 Scoped Assignables  A weakness of using a capability to give indirect access to an assignable is that there is no guarantee that a given getter setter pair are in fact the capability for a particular assignable. For example, we might pair the getter for a with the setter for b, leading to unexpected behavior. There is nothing in the type system that prevents creating such mismatched pairs. To avoid this, we introduce the concept of a reference to an assignable. A reference is a value from which we may obtain the capability to get and set a particular assignable. Moreover, two references can be tested for equality to see whether they act on the same assignable.1 The reference type ref τ  has as values references to assignables of type τ. The introduction and elimination forms for this type are given by the following syntax chart:  ::= ref τ  ::= ref[a]  Typ Exp Cmd m ::= getref e   τ e  setref e1; e2   τ ref &a ∗ e e1 ∗= e2  assignable reference contents update  The statics of reference types is deﬁned by the following rules:   cid:7   cid:12  cid:25 ,a∼τ ref[a] : ref τ    35.1a     35.1b    35.1c    35.2a    35.2b    35.2c    35.2d    35.2e   315  35.2 Scoped Assignables   cid:7   cid:12  cid:25  e : ref τ   cid:7   cid:12  cid:25  getref e  ∼·· τ  cid:7   cid:12  cid:25  e1 : ref τ   cid:7   cid:12  cid:25  e2 : τ  cid:7   cid:12  cid:25  setref e1; e2  ∼·· τ  Rule  35.1a  speciﬁes that the name of any active assignable is an expression of type ref τ . The dynamics of reference types defers to the corresponding operations on assignables,  and does not alter the underlying dynamics of assignables:  ref[a] val cid:25 ,a∼τ  e  cid:20 −→ getref e   cid:16  μ  cid:20 −→   cid:25    cid:5   e   cid:25   getref e   cid:5    cid:16  μ  get[a]  cid:16  μ  getref ref[a]   cid:16  μ  cid:20 −−−→  cid:25 ,a∼τ  cid:5  e 1 setref e  e1  cid:20 −→ setref e1; e2   cid:16  μ  cid:20 −→   cid:25    cid:25   1; e2   cid:16  μ  cid:5   setref ref[a]; e   cid:16  μ  cid:20 −−−→  cid:25 ,a∼τ  set[a] e   cid:16  μ  A reference to an assignable is a value. The getref and setref operations on references defer to the corresponding operations on assignables once the referent has been resolved. Because references give rise to capabilities, the reference type is immobile. As a result, references cannot be stored in assignables or returned from commands. The immobility of references ensures safety, as can be seen by extending the safety proof given in Chapter 34. As an example of using references, the generic doubling procedure discussed in the  preceding section is programmed using references as follows:  proc  r : nat ref {x ← ∗ r ; r ∗= x + x ; ret x}.  Because the argument is a reference, rather than a capability, there is no possibility that the getter and setter refer to different assignables.  The ability to pass references to procedures comes at a price, because any two references might refer to the same assignable  if they have the same type . Consider a procedure that, when given two references x and y, adds twice the contents of y to the contents of x. One way to write this code creates no complications:  λ  x : nat ref  λ  y : nat ref  cmd{x   cid:5  ← ∗ x ; y   cid:5  ← ∗ y ; x ∗= x   cid:5  + y   cid:5  + y   cid:5 }.  Even if x and y refer to the same assignable, the effect will be to set the contents of the assignable referenced by x to the sum of its original contents and twice the contents of the assignable referenced by y.   316  Assignable References  But now consider the following seemingly equivalent implementation of this procedure:  λ  x : nat ref  λ  y : nat ref  cmd{x += y ; x += y},  where x += y is the command {x   cid:5  ← ∗ x ; y   cid:5  ← ∗ y ; x ∗= x   cid:5  + y   cid:5 }  that adds the contents of y to the contents of x. The second implementation works right, as long as x and y do not refer to the same assignable. If they do refer to a common assignable a, with contents n, the result is that a is to set 4 × n, instead of the intended 3 × n. The second get of y is affected by the ﬁrst set of x.  In this case, it is clear how to avoid the problem: use the ﬁrst implementation, rather than the second. But the difﬁculty is not in ﬁxing the problem once it has been discovered, but in noticing the problem in the ﬁrst place. Wherever references  or capabilities  are used, the problems of interference lurk. Avoiding them requires very careful consideration of all possible aliasing relationships among all of the references in play. The problem is that the number of possible aliasing relationships among n references grows combinatorially in n.  35.3 Free Assignables  Although it is interesting to note that references and capabilities are compatible with the stack discipline, for references to be useful requires that this restriction be relaxed. With immobile references it is impossible to build data structures containing references, or to return references from procedures. To allow this, we must arrange that the lifetime of an assignable extend beyond its scope. In other words, we must give up stack allocation for heap allocation. Assignables that persist beyond their scope of declaration are called scope- free, or justfree, assignables. When all assignables are free, every type is mobile and so any value, including a reference, may be used in a data structure.  Supporting free assignables amounts to changing the dynamics so that allocation of  assignables persists across transitions. We use transition judgments of the form  ν  cid:25  { m  cid:16  μ}  cid:20 −→ ν  cid:25    cid:5  { m   cid:5   cid:16  μ   cid:5  }.  Execution of a command may allocate new assignables, may alter the contents of existing assignables, and may give rise to a new command to be executed at the next step. The rules deﬁning the dynamics of free assignables are as follows: ν  cid:25  { ret e   cid:16  μ} ﬁnal   35.3a   e val cid:25   e  cid:20 −→  e  e  cid:20 −→  e   cid:5    cid:5   ν  cid:25  { ret e   cid:16  μ}  cid:20 −→ ν  cid:25  { ret e   cid:5    cid:16  μ}   cid:25   ν  cid:25  { bnd e; x.m   cid:16  μ}  cid:20 −→ ν  cid:25  { bnd e   cid:5 ; x.m   cid:16  μ}   cid:25    35.3b    35.3c    317  35.3 Free Assignables  ν  cid:25  { bnd cmd ret e  ; x.m   cid:16  μ}  cid:20 −→ ν  cid:25  { [e x]m  cid:16  μ}  e val cid:25   ν  cid:25  { m1  cid:16  μ}  cid:20 −→ ν  cid:25  ν  cid:25  { bnd cmd m1 ; x.m2   cid:16  μ}  cid:20 −→ ν  cid:25    cid:5  1   cid:5  { m  cid:5  }  cid:16  μ  cid:5  { bnd cmd m  1 ; x.m2   cid:16  μ  cid:5  }  cid:5   ν  cid:25 , a ∼ τ { get[a]  cid:16  μ ⊗ a  cid:9 → e }  cid:20 −→ ν  cid:25 , a ∼ τ { ret e   cid:16  μ ⊗ a  cid:9 → e }  ν  cid:25  { set[a] e   cid:16  μ}  cid:20 −→ ν  cid:25  { set[a] e   cid:5    cid:16  μ}   cid:25   e  cid:20 −→   cid:5   e  e val cid:25 ,a∼τ  e  cid:20 −→   cid:5   e  ν  cid:25 , a ∼ τ { set[a] e   cid:16  μ ⊗ a  cid:9 → }  cid:20 −→ ν  cid:25 , a ∼ τ { ret e   cid:16  μ ⊗ a  cid:9 → e }   35.3h   The language RMA extends MA with references to free assignables. Its dynamics is  similar to that of references to scoped assignables given earlier.  ν  cid:25  { dcl e; a.m   cid:16  μ}  cid:20 −→ ν  cid:25  { dcl e   cid:5 ; a.m   cid:16  μ}   cid:25   ν  cid:25  { dcl e; a.m   cid:16  μ}  cid:20 −→ ν  cid:25 , a ∼ τ { m  cid:16  μ ⊗ a  cid:9 → e }  e val cid:25   ν  cid:25  { getref e   cid:16  μ}  cid:20 −→ ν  cid:25  { getref e   cid:5    cid:16  μ}   cid:25   ν  cid:25  { getref ref[a]   cid:16  μ}  cid:20 −→ ν  cid:25  { get[a]  cid:16  μ}  e  cid:20 −→   cid:5   e  e1  cid:20 −→   cid:5  e 1  ν  cid:25  { setref e1; e2   cid:16  μ}  cid:20 −→ ν  cid:25  { setref e   cid:25   1; e2   cid:16  μ}  cid:5   ν  cid:25  { setref ref[a]; e2   cid:16  μ}  cid:20 −→ ν  cid:25  { set[a] e2   cid:16  μ}  The expressions cannot alter or extend the memory, only commands may do so.  As an example of using RMA, consider the command newref[τ] e  deﬁned by  This command allocates a fresh assignable and returns a reference to it. Its static and dynamics are derived from the foregoing rules as follows:  dcl a := e in ret  &a .   cid:7   cid:12  cid:25  e : τ   cid:7   cid:12  cid:25  newref[τ] e  ∼·· ref τ    35.3d    35.3e    35.3f    35.3g    35.3i    35.3j    35.4a    35.4b    35.4c    35.4d    35.5    35.6    318  Assignable References  e  cid:20 −→   cid:5   e  ν  cid:25  { newref[τ] e   cid:16  μ}  cid:20 −→ ν  cid:25  { newref[τ] e   cid:5    cid:16  μ}   cid:25   ν  cid:25  { newref[τ] e   cid:16  μ}  cid:20 −→ ν  cid:25 , a ∼ τ { ret ref[a]   cid:16  μ ⊗ a  cid:9 → e }  e val cid:25   Oftentimes, the command newref[τ] e  is taken as primitive, and the declaration command is omitted. In that case, all assignables are accessed by reference, and no direct access to assignables is provided.   35.7a    35.7b   35.4 Safety   cid:12  cid:25  m ∼·· τ  cid:12  cid:25  μ :  cid:25  ν  cid:25  { m  cid:16  μ} ok  Although the proof of safety for references to scoped assignables presents few difﬁculties, the safety for free assignables is tricky. The main difﬁculty is to account for cyclic depen- dencies within data structures. The contents of one assignable may contain a reference to itself, or a reference to another assignable that contains a reference to it, and so forth. For example, consider the following procedure e of type nat  cid:19  nat cmd:  proc  x : nat {if  x  ret  1  else {f ← @ a ; y ← f  x − 1  ; ret  x ∗ y }}.   cid:5  ⊗ a  cid:9 → e in which the contents of a contains, via the Let μ be a memory of the form μ body of the procedure, a reference to a itself. Indeed, if the procedure e is called with a non-zero argument, it will “call itself” by indirect reference through a.  Cyclic dependencies complicate the deﬁnition of the judgment μ :  cid:25 . It is deﬁned by  the following rule:   35.8    35.9   The ﬁrst premise of the rule states that the command m is well-formed relative to  cid:25 . The second premise states that the memory μ conforms to  cid:25 , relative to all of  cid:25  so that cyclic dependencies are permitted. The judgment  cid:12  cid:25    cid:5  μ :  cid:25  is deﬁned as follows:  ∀a ∼ τ ∈  cid:25  ∃e μ a  = e and  cid:12  cid:25    cid:5  e : τ   cid:12  cid:25    cid:5  μ :  cid:25   Theorem 35.1  Preservation .  1. If  cid:12  cid:25  e : τ and e  cid:20 −→ 2. If ν  cid:25  { m  cid:16  μ} ok and ν  cid:25  { m  cid:16  μ}  cid:20 −→ ν  cid:25    cid:5 , then  cid:12  cid:25  e   cid:5  : τ .  e   cid:25    cid:5  { m   cid:5   cid:16  μ   cid:5  }, then ν  cid:25    cid:5  { m   cid:5   cid:16  μ   cid:5  } ok.  Proof Simultaneously, by induction on transition. We prove the following stronger form of the second statement:   cid:5  }, where cid:12   cid:25  m ∼·· τ,  cid:12  cid:25  μ :  cid:25 , then cid:25    cid:5  extends  cid:25 ,   cid:5   cid:16  μ  cid:5  { m If ν  cid:25  { m  cid:16  μ}  cid:20 −→ ν  cid:25   cid:5  ∼·· τ, and  cid:12  cid:25  and  cid:12  cid:25   cid:5  :  cid:25   cid:5 .  cid:5  μ   cid:5  m   319  35.4 Safety  Consider the transition  ν  cid:25  { dcl e; a.m   cid:16  μ}  cid:20 −→ ν  cid:25 , a ∼ ρ { m  cid:16  μ ⊗ a  cid:9 → e }  where e val cid:25 . By assumption and inversion of rule  34.6d , we have  cid:12  cid:25  e : ρ,  cid:12  cid:25 ,a∼ρ m ∼·· τ, and  cid:12  cid:25  μ :  cid:25 . But because extension of  cid:25  with a fresh assignable does not affect typing, we also have  cid:12  cid:25 ,a∼ρ μ :  cid:25  and  cid:12  cid:25 ,a∼ρ e : ρ, from which it follows by rule  35.9  that  cid:12  cid:25 ,a∼ρ μ ⊗ a  cid:9 → e :  cid:25 , a ∼ ρ. The other cases follow a similar pattern and are left as an exercise for the reader.  Theorem 35.2  Progress .  1. If  cid:12  cid:25  e : τ , then either e val cid:25  or there exists e 2. If ν  cid:25  { m  cid:16  μ} ok then either ν  cid:25  { m  cid:16  μ} ﬁnal or ν  cid:25  { m  cid:16  μ}  cid:20 −→ ν  cid:25    cid:5  such that e  cid:20 −→   cid:5 .  e   cid:25    cid:5  { m   cid:5   cid:16  μ  cid:5  }  for some  cid:25    cid:5 , μ   cid:5 , and m   cid:5 .  Proof Simultaneously, by induction on typing. For the second statement, we prove the stronger form  If  cid:12  cid:25  m ∼·· τ and  cid:12  cid:25  μ :  cid:25 , then either ν  cid:25  { m  cid:16  μ} ﬁnal, or ν  cid:25  { m  cid:16  μ}  cid:20 −→   cid:5  { m   cid:5   cid:16  μ   cid:5  } for some  cid:25    cid:5 , μ   cid:5 , andm   cid:5 .  ν  cid:25   Consider the typing rule   cid:7   cid:12  cid:25  e : ρ  cid:7   cid:12  cid:25 ,a∼ρ m ∼·· τ   cid:7   cid:12  cid:25  dcl e; a.m  ∼·· τ  We have by the ﬁrst inductive hypothesis that either e val cid:25  or e  cid:20 −→ latter case, we have by rule  35.3i    cid:25    cid:5  for some e   cid:5 . In the  e  ν  cid:25  { dcl e; a.m   cid:16  μ}  cid:20 −→ ν  cid:25  { dcl e   cid:5   ; a.m   cid:16  μ}.  In the former case, we have by rule  35.3j  that  ν  cid:25  { dcl e; a.m   cid:16  μ}  cid:20 −→ ν  cid:25 , a ∼ ρ { m  cid:16  μ ⊗ a  cid:9 → e }.  Now consider the typing rule   cid:7   cid:12  cid:25 ,a∼τ get[a] ∼·· τ  By assumption  cid:12  cid:25 ,a∼τ μ :  cid:25 , a ∼ τ, and hence there exists e val cid:25 ,a∼τ such that μ =  cid:5  ⊗ a  cid:9 → e and  cid:12  cid:25 ,a∼τ e : τ. By rule  35.3f   μ  ν  cid:25 , a ∼ τ { get[a]  cid:16  μ   cid:5  ⊗ a  cid:9 → e }  cid:20 −→ ν  cid:25 , a ∼ τ { ret e   cid:16  μ   cid:5  ⊗ a  cid:9 → e },  as required. The other cases are handled similarly.   320  Assignable References  35.5 Benign Eﬀects  The modal separation between commands and expressions ensures that the meaning of an expression does not depend on the  ever-changing  contents of assignables. Although this is helpful in many, perhaps most, situations, it also precludes programming techniques that use storage effects to implement purely functional behavior. A prime example is memoization. Externally, a suspended computation behaves exactly like the underlying computation; internally, an assignable is associated with the computation that stores the result of any evaluation of the computation for future use. Other examples are self-adjusting data structures, which use state to improve their efﬁciency without changing their functional behavior. For example, a splay tree is a binary search tree that uses mutation internally to re-balance the tree as elements are inserted, deleted, and retrieved, so that lookup takes time proportional to the logarithm of the number of elements.  These are examples of benign storage effects, uses of mutation in a data structure to improve efﬁciency without disrupting its functional behavior. One class of examples are self-adjusting data structures that reorganize themselves during one use to improve efﬁciency of later uses. Another class of examples are memoized, or lazy, data structures, which are discussed in Chapter 36. Benign effects such as these are impossible to implement if a strict separation between expressions and commands is maintained. For example, a self- adjusting tree involves mutation but is a value just like any other, and this cannot be achieved in MA. Although several special-case techniques are known, the most general solution is to do away with the modal distinction, coalescing expressions and commands into a single syntactic category. The penalty is that the type system no longer ensures that an expression of type τ denotes a value of that type; it might also have storage effects during its evaluation. The beneﬁt is that one may freely use benign effects, but it is up to the programmer to ensure that they truly are benign.  The language RPCF extends PCF with references to free assignables. The following  rules deﬁne the statics of the distinctive features of RPCF:  cid:7   cid:12  cid:25  e1 : τ1  cid:7   cid:12  cid:25 ,a∼τ1 e2 : τ2  cid:7   cid:12  cid:25  dcl e1; a.e2  :τ 2   cid:7   cid:12  cid:25 ,a∼τ get[a] :τ  cid:7   cid:12  cid:25 ,a∼τ e : τ   cid:7   cid:12  cid:25 ,a∼τ set[a] e  : τ   35.10a    35.10b    35.10c   Correspondingly, the dynamics of RPCF is given by transitions of the form  ν  cid:25  { e  cid:16  μ}  cid:20 −→ ν  cid:25    cid:5  { e   cid:5   cid:16  μ   cid:5  },  where e is an expression and not a command. The rules deﬁning the dynamics are very similar to those for RMA, but with commands and expressions integrated into a single category.   321  35.6 Notes  To illustrate the concept of a benign effect, consider the technique of back-patching to implement recursion. Here is an implementation of the factorial function that uses an assignable to implement recursive calls:  dcl a := λn:nat.0 in  { f ← a := λn:nat.ifz n, 1, n ; ret f   }   cid:5   .n× @a  n  cid:5       This declaration returns a function of type nat cid:19 nat that is obtained by  a  allocating a free assignable initialized arbitrarily with a function of this type,  b  deﬁning a λ-abstraction in which each “recursive call” consists of retrieving and applying the function stored in that assignable,  c  assigning this function to the assignable, and  d  returning that function. The result is a function on the natural numbers, even though it uses state in its implementation. Backpatching is not expressible in RMA, because it relies on assignment. Let us attempt  to recode the previous example in RMA: dcl a := proc n:nat {ret 0} in  { f ← a := . . . ; ret f   },  where the elided procedure assigned to a is given by proc n:nat  {if  ret n   {ret 1 } else {f←@a; x←f n-1 ; ret n×x }}.  The difﬁculty is that what we have is a command, not an expression. Moreover, the result of the command is of the procedure type nat  cid:19   nat cmd  and not of the function type nat  cid:19  nat. Consequently, we cannot use the factorial procedure in an expression but have to execute it as a command using code such as this:  { f ← fact ; x ← f n ; ret x  }.  35.6 Notes  Reynolds  1981  uses capabilities to provide indirect access to assignables; references are just an abstract form of capability. References are often permitted only for free assignables, but with mobility restrictions one may also have references to scoped assignables. The proof of safety of free references outlined here follows those given by Wright and Felleisen  1994  and Harper  1994 .  Benign effects are central to the distinction between Haskell, which provides an Algol- like separation between commands and expressions, and ML, which integrates evaluation   322  Assignable References  with execution. The choice between them is classic trade-off, with neither superior to the other in all respects.  Exercises  35.1. Consider scoped array assignables as described in Exercise 34.1. Extend the treatment  of array assignables in Exercise 34.1, to account for array assignable references.  35.2. References to scope-free assignables are often used to implement recursive data structures such as mutable lists and trees. Examine such data structures in the context of RMA enriched with sum, product, and recursive types.  Give six different types that could be considered a type of linked lists, according  to the following characteristics:  a  A mutable list may only be updated in toto by replacing it with another  im-  mutable  list.   b  A mutable list can be altered in one of two ways, to make it empty, or to change both its head and tail element simultaneously. The tail element is any other such mutable list, so circularities may arise.   c  A mutable list is, permanently, either empty or non-empty. If not, both its head   d  A mutable list is, permanently, either empty or non-empty. If not, its tail, but not  and tail can be modiﬁed simultaneously.  its head, can be set to another such list.   e  A mutable list is, permanently, either empty or non-empty. If not, either its head  or its tail elements can be modiﬁed independently.   f  A mutable list can be altered to become either empty or non-empty. If it is non-  empty, either it head, or its tail, can be modiﬁed independently of one another.  Discuss the merits and deﬁciencies of each representation.  Note  1 The getter and setter do not sufﬁce to deﬁne equality, because not all types admit a test for equality. When they do, and when there are at least two distinct values of their type, we can determine whether they are aliases by assigning to one and checking whether the contents of the other is changed.   36  Lazy Evaluation  Lazy evaluation comprises a variety of methods to defer evaluation of an expression until it is required, and to share the results of any such evaluation among all uses of a deferred computation. Laziness is not merely an implementation device, but it also affects the meaning of a program.  One form of laziness is the by-need evaluation strategy for function application. Recall from Chapter 8 that the by-name evaluation order passes the argument to a function in unevaluated form so that it is only evaluated if it is actually used. But because the argument is replicated by substitution, it might be evaluated more than once. By-need evaluation ensures that the argument to a function is evaluated at most once, by ensuring that all copies of an argument share the result of evaluating any one copy.  Another form of laziness is the concept of a lazy data structure. As we have seen in Chapters 10, 11, and 20, we may choose to defer evaluation of the components of a data structure until they are actually required, and not when the data structure is created. But if a component is required more than once, then the same computation will, without further provision, be repeated on each use. To avoid this, the deferred portions of a data structure are shared so an access to one will propagate its result to all occurrences of the same computation.  Yet another form of laziness arises from the concept of general recursion considered in Chapter 19. Recall that the dynamics of general recursion is given by unrolling, which replicates the recursive computation on each use. It would be preferable to share the results of such computation across unrollings. A lazy implementation of recursion avoids such replications by sharing those results.  Traditionally, languages are biased towards either eager or lazy evaluation. Eager lan- guages use a by-value dynamics for function applications, and evaluate the components of data structures when they are created. Lazy languages adopt the opposite strategy, pre- ferring a by-name dynamics for functions, and a lazy dynamics for data structures. The overhead of laziness is reduced by managing sharing to avoid redundancy. Experience has shown, however, that the distinction is better drawn at the level of types. It is important to have both lazy and eager types, so that the programmer controls the use of laziness, rather than having it enforced by the language dynamics.  36.1 PCF By-Need  We begin by considering a lazy variant of PCF, called LPCF, in which functions are called by name, and the successor operator is evaluated lazily. Under a lazy interpretation variables   324  Lazy Evaluation  are bound to unevaluated expressions, and the argument to the successor left unevaluated: any successor is a value, regardless of whether the predecessor is or not. By-name function application replicates the unevaluated argument by substitution, which means that there can arise many copies of the same expression, each evaluated separately, if at all. By-need evaluation uses a device called memoization to share all such copies of an argument and to ensure that if it is evaluated at all, its value is stored so that all other uses of it will avoid re-computation. Computations are named during evaluation, and are accessed by a level of indirection using this name to index the memo table, which records the expression and, if it is every evaluated, its value. The dynamics of LPCF is based on a transition system with states of the form ν  cid:25  { e  cid:16  μ}, where  cid:25  is a ﬁnite set of hypotheses a1 ∼ τ1, . . . , an ∼ τn associating types to symbols, e is an expression that can involve the symbols in  cid:25 , and μ maps each symbol declared in  cid:25  to either an expression or a special symbol,  , called the black hole.  The role of the black hole is explained below.  As a notational convenience, we use a bit of legerdemain with the concrete syntax similar to that used in Chapter 34. Speciﬁcally, the concrete syntax for the expression via a , which fetches the contents of the assignable a, is @ a.  The dynamics of LPCF is given by he following two forms of judgment:  1. e val cid:25 , stating that e is a value that can involve the symbols in  cid:25 . 2. ν  cid:25  { e  cid:16  μ}  cid:20 −→ ν  cid:25    cid:5  }, stating that one step of evaluation of the expression  cid:5  e relative to memo table μ with the symbols declared in  cid:25  results in the expression e relative to the memo table μ   cid:5  with symbols declared in  cid:25    cid:5   cid:16  μ   cid:5  { e   cid:5 .  The dynamics is deﬁned so that the active symbols grow during evaluation. The memo table may be altered destructively during execution to show progress in the evaluation of the expression associated with a symbol.  The judgment e val cid:25  expressing that e is a closed value is deﬁned by the following rules:  z val cid:25   s @ a  val cid:25 ,a∼nat  λ  x : τ  e val cid:25    36.1a    36.1b    36.1c   Rules  36.1a  through  36.1c  specify that z is a value, any expression of the form s @ a , where a is a symbol, is a value, and that any λ-abstraction, possibly containing symbols, is a value. It is important that symbols themselves are not values, rather they stand for  possibly unevaluated  expressions as speciﬁed by the memo table. The expression @ a, which is short for via a , is not a value. Rather, it is accessed to obtain, and possibly update, the binding of the symbol a in memory.   325  36.1 PCF By-Need  The initial and ﬁnal states of evaluation are deﬁned as follows:  ν ∅{ e  cid:16  ∅} initial  e val cid:25   ν  cid:25  { e  cid:16  μ} ﬁnal  Rule  36.2a  speciﬁes that an initial state consists of an expression evaluated relative to an empty memo table. Rule  36.2b  speciﬁes that a ﬁnal state has the form ν  cid:25  { e  cid:16  μ}, where e is a value relative to  cid:25 .  The transition judgment for the dynamics of LPCF is deﬁned by the following rules:  e val cid:25 ,a∼τ  ν  cid:25 , a ∼ τ { @ a  cid:16  μ ⊗ a  cid:9 → e }  cid:20 −→ ν  cid:25 , a ∼ τ { e  cid:16  μ ⊗ a  cid:9 → e }  cid:5  ⊗ a  cid:9 →  } ν  cid:25 , a ∼ τ { e  cid:16  μ ⊗ a  cid:9 →  }  cid:20 −→ ν  cid:25  ν  cid:25 , a ∼ τ { @ a  cid:16  μ ⊗ a  cid:9 → e }  cid:20 −→ ν  cid:25   cid:5  ⊗ a  cid:9 → e  , a ∼ τ { e  cid:5   cid:16  μ , a ∼ τ { @ a  cid:16  μ   cid:5   cid:5    cid:5  }  ν  cid:25  { s e   cid:16  μ}  cid:20 −→ ν  cid:25 , a ∼ nat{ s @ a   cid:16  μ ⊗ a  cid:9 → e }  ν  cid:25  { e  cid:16  μ}  cid:20 −→ ν  cid:25  ν  cid:25  { ifz e {z  cid:9 → e0  s x   cid:9 → e1}  cid:16  μ}  cid:20 −→ ν  cid:25    cid:5   cid:16  μ  cid:5  { e  cid:5  }  cid:5  {z  cid:9 → e0  s x   cid:9 → e1}  cid:16  μ  cid:5  { ifz e  cid:5  }  36.3d   ν  cid:25  { ifz z{z  cid:9 → e0  s x   cid:9 → e1}  cid:16 μ }  cid:20 −→ ν  cid:25  { e0  cid:16  μ}  ⎧⎪⎨⎪⎩ ν  cid:25 , a ∼ nat{ ifz s @ a {z  cid:9 → e0  s x   cid:9 → e1}  cid:16 μ ⊗ a  cid:9 → e } ⎫⎪⎬⎪⎭   cid:20 −→  ν  cid:25 , a ∼ nat{ [@ a x]e1  cid:16  μ ⊗ a  cid:9 → e }  cid:16  μ  cid:5  }  cid:5  1 1 e2   cid:16  μ  cid:5  }  cid:5   ν  cid:25  { e1  cid:16  μ}  cid:20 −→ ν  cid:25  ν  cid:25  { e1 e2   cid:16  μ}  cid:20 −→ ν  cid:25    cid:5  { e  cid:5  { e  ⎧⎪⎨⎪⎩  ν  cid:25  {  λ  x : τ  e  e2   cid:16  μ}   cid:20 −→  ν  cid:25 , a ∼ τ { [@ a x]e  cid:16  μ ⊗ a  cid:9 → e2 }  ⎫⎪⎬⎪⎭  ν  cid:25  { fix x : τ is e  cid:16  μ}  cid:20 −→ ν  cid:25 , a ∼ τ { @ a  cid:16  μ ⊗ a  cid:9 → [@ a x]e }   36.3i    36.2a    36.2b    36.3a    36.3b    36.3c    36.3e    36.3f    36.3g    36.3h    326  Lazy Evaluation  Rule  36.3a  governs a symbol whose associated expression is a value; the value of the symbol is the value associated to that symbol in the memo table. Rule  36.3b  speciﬁes that if the expression associated to a symbol is not a value, then it is evaluated “in place” until such time as rule  36.3a  applies. This is achieved by switching the focus of evaluation to the associated expression, while at the same time associating the black hole to that symbol. The black hole represents the absence of a value for that symbol, so that any attempt to use it during evaluation of its associated expression cannot make progress. The black hole signals a circular dependency that, if not caught using a black hole, would initiate an inﬁnite regress.  Rule  36.3c  speciﬁes that evaluation of s e  allocates a fresh symbol a for the expression e, and yields the value s @ a . The value of e is not determined until such time as the predecessor is required in a later computation, implementing a lazy dynamics for the successor. Rule  36.3f , which governs a conditional branch on a successor, substitutes @ a for the variable x when computing the predecessor of a non-zero number, ensuring that all occurrences of x share the same predecessor computation.  Rule  36.3g  speciﬁes that the value of the function position of an application must be determined before the application can be executed. Rule  36.3h  speciﬁes that to evaluate an application of a λ-abstraction we allocate a fresh symbol a for the argument, and substitute @ a for the argument variable of the function. The argument is evaluated only if it is needed in the later computation, and then that value is shared among all occurrences of the argument variable in the body of the function. General recursion is implemented by rule  36.3i . Recall from Chapter 19 that the expression fix x:τ is e stands for the solution of the recursion equation x = e. Rule  36.3i  computes this solution by associating a fresh symbol a with the body e substituting @ a for x within e to effect the self-reference. It is this substitution that permits a named expression to depend on its own name. For example, the expression fix x:τ is x associates the expression a to a in the memo table, and returns @ a. The next step of evaluation is stuck, because it seeks to evaluate @ a with a bound to the black hole. In contrast, an expression such  cid:5   e does not get stuck, because the self-reference is “hidden” as fix f : τ within the λ-abstraction, and hence need not be evaluated to determine the value of the binding.   cid:19  τ is λ  x : τ   cid:5   36.2 Safety of PCF By-Need  We write  cid:7   cid:12  cid:25  e : τ to mean that e has type τ under the assumptions  cid:7 , treating symbols declared in  cid:25  as expressions of their associated type. The rules are as in Chapter 19, extended with the following rule for symbols:   cid:7   cid:12  cid:25 ,a∼τ @ a : τ   36.4   This rule states that the demand for the binding of a symbol, @ a, is a form of expression. It is a “delayed substitution” that lazily replaces a demand for a by its binding.   327  36.2 Safety of PCF By-Need  The judgment ν  cid:25  { e  cid:16  μ} ok is deﬁned by the following rules:   cid:12  cid:25  e : τ  cid:12  cid:25  μ :  cid:25  ν  cid:25  { e  cid:16  μ} ok  ∀a ∼ τ ∈  cid:25  μ a  = e  cid:6 =   =⇒ cid:12  cid:25    cid:5  e : τ   cid:12  cid:25    cid:5  μ :  cid:25    36.5a    36.5b   Rule  36.5b  permits self-reference through the memo table by allowing the expression associated to a symbol a to contain occurrences of @ a. A symbol that is bound to the “black hole” is considered to be of any type. Theorem 36.1  Preservation . If ν  cid:25  { e  cid:16  μ}  cid:20 −→ ν  cid:25  then ν  cid:25  Proof We prove by induction on rules  36.3  that if ν  cid:25  { e  cid:16  μ}  cid:20 −→ ν  cid:25   cid:12  cid:25  μ :  cid:25  and  cid:12  cid:25  e : τ, then  cid:25   cid:5  :  cid:25  Consider rule  36.3b , for which we have e = e ⊗ a  cid:9 → e  cid:5  μ 0   cid:5   cid:16  μ  cid:5  and  cid:12  cid:25   cid:5  = @ a, μ = μ0 ⊗ a  cid:9 → e0, μ   cid:5  } and ν  cid:25  { e  cid:16  μ} ok,   cid:5  ⊇  cid:25  and  cid:12  cid:25    cid:5  } and  cid:5  =   cid:5  } ok.   cid:5   cid:16  μ   cid:5   cid:16  μ   cid:5  : τ.   cid:5  { e   cid:5  { e   cid:5  { e   cid:5  μ   cid:5  e   cid:5  0, and ν  cid:25 , a ∼ τ { e0  cid:16  μ0 ⊗ a  cid:9 →  }  cid:20 −→ ν  cid:25    cid:5   , a ∼ τ { e   cid:5  0   cid:16  μ  cid:5  0  ⊗ a  cid:9 →  }.  Assume that  cid:12  cid:25 ,a∼τ μ :  cid:25 , a ∼ τ. It follows that  cid:12  cid:25 ,a∼τ e0 : τ and  cid:12  cid:25 ,a∼τ μ0 :  cid:25 , and hence that  We have by induction that  cid:25   But then   cid:5   ,a∼τ e   cid:5  0 : τ   cid:12  cid:25 ,a∼τ μ0 ⊗ a  cid:9 →   :  cid:25 , a ∼ τ .  cid:5  ⊇  cid:25  and  cid:12  cid:25   cid:12  cid:25   ,a∼τ μ0 ⊗ a  cid:9 →   :  cid:25 , a ∼ τ .  cid:12  cid:25    cid:5  ,a∼τ μ  , a ∼ τ ,   cid:5  and  :  cid:25    cid:5    cid:5    cid:5   which sufﬁces for the result.  Consider rule  36.3g , so that e is the application e1 e2  and  cid:5  }.  ν  cid:25  { e1  cid:16  μ}  cid:20 −→ ν  cid:25    cid:16  μ   cid:5  { e   cid:5  1  Suppose that  cid:12  cid:25  μ :  cid:25  and  cid:12  cid:25  e : τ. By inversion of typing  cid:12  cid:25  e1 : τ2  cid:19  τ for some type τ2 such that  cid:12  cid:25  e2 : τ2. By induction  cid:25   cid:5  1 : τ2  cid:19  τ. By  cid:5  e2 : τ2, so that  cid:12  cid:25  weakening we have  cid:12  cid:25    cid:5  1 e2  :τ , which is enough for the result.   cid:5  ⊇  cid:25  and  cid:12  cid:25   cid:5  e   cid:5  and  cid:12  cid:25    cid:5  :  cid:25    cid:5  μ   cid:5  e  The statement of the progress theorem allows for the occurrence of a black hole, rep- resenting a checkable form of non-termination. The judgment ν  cid:25  { e  cid:16  μ} loops, stating that e diverges by virtue of encountering the black hole, is deﬁned by the following rules:  ν  cid:25 , a ∼ τ { @ a  cid:16  μ ⊗ a  cid:9 →  } loops ν  cid:25 , a ∼ τ { e  cid:16  μ ⊗ a  cid:9 →  } loops ν  cid:25 , a ∼ τ { @ a  cid:16  μ ⊗ a  cid:9 → e } loops   36.6a    36.6b    328  Lazy Evaluation  ν  cid:25  { e  cid:16  μ} loops  ν  cid:25  { ifz e {z  cid:9 → e0  s x   cid:9 → e1}  cid:16 μ } loops  ν  cid:25  { e1  cid:16  μ} loops ν  cid:25  { e1 e2   cid:16  μ} loops   36.6c    36.6d   There are other ways of forming an inﬁnite loop. The looping judgment simply codiﬁes those cases in which the looping behavior is a self-dependency, which is mediated by a black hole.   cid:5  }.   cid:5  { e   cid:5   cid:16  μ   cid:5  and e  If ν  cid:25  { e  cid:16  μ} ok,  then either ν  cid:25  { e  cid:16  μ} ﬁnal, or   cid:5  such that ν  cid:25  { e  cid:16  μ}  cid:20 −→ ν  cid:25   Theorem 36.2  Progress . ν  cid:25  { e  cid:16  μ} loops, or there exists μ Proof We proceed by induction on the derivations of  cid:12  cid:25  e : τ and  cid:12  cid:25  μ :  cid:25  implicit in the derivation of ν  cid:25  { e  cid:16  μ} ok. Consider rule  19.1a , where the symbol a is declared in  cid:25 . Thus,  cid:25  =  cid:25 0, a ∼ τ and  cid:12  cid:25  μ :  cid:25 . It follows that μ = μ0 ⊗ a  cid:9 → e0 with  cid:12  cid:25  μ0 :  cid:25 0 and  cid:12  cid:25  e0 : τ. Note that  cid:12  cid:25  μ0 ⊗ a  cid:9 →   :  cid:25 . Applying induction to the derivation of  cid:12  cid:25  e0 : τ, we consider three cases: 1. ν  cid:25  { e0  cid:16  μ ⊗ a  cid:9 →  } ﬁnal. By inversion of rule  36.2b  we have e0 val cid:25 , and hence by rule  36.3a  we obtain ν  cid:25  { @ a  cid:16  μ}  cid:20 −→ ν  cid:25  { e0  cid:16  μ}. 2. ν  cid:25  { e0  cid:16  μ0 ⊗ a  cid:9 →  } rule loops. ν  cid:25  { @ a  cid:16  μ} loops. 3. ν  cid:25  { e0  cid:16  μ0 ⊗ a  cid:9 →  }  cid:20 −→ ν  cid:25   applying obtain ⊗ a  cid:9 →  }. By applying rule  36.3b  we   36.6b  we  By  cid:16  μ  cid:5  0   cid:5  { e   cid:5  0  obtain  ν  cid:25  { @ a  cid:16  μ ⊗ a  cid:9 → e0 }  cid:20 −→ ν  cid:25    cid:5  { @ a  cid:16  μ   cid:5  ⊗ a  cid:9 → e  }.   cid:5  0  The language LFPC is FPC but with a by-need dynamics. For example, the dynamics of product types in LFPC is given by the following rules:  36.3 FPC By-Need   cid:24 @ a1, @ a2 cid:25  val cid:25 ,a1∼τ1,a2∼τ2  ν  cid:25  { cid:24 e1, e2 cid:25   cid:16 μ }   cid:20 −→  ⎧⎪⎨⎪⎩  ν  cid:25 , a1 ∼ τ1, a2 ∼ τ2 { cid:24 @ a1, @ a2 cid:25   cid:16  μ ⊗ a1  cid:9 → e1 ⊗ a2  cid:9 → e2 }  ν  cid:25  { e  cid:16  μ}  cid:20 −→ ν  cid:25  ν  cid:25  { e · l  cid:16  μ}  cid:20 −→ ν  cid:25    cid:5  { e  cid:5  { e   cid:5   cid:16  μ  cid:5  }  cid:5  · l  cid:16  μ  cid:5  }  ⎫⎪⎬⎪⎭   36.7a    36.7b    36.7c    329  36.4 Suspension Types  ν  cid:25  { e  cid:16  μ} loops ν  cid:25  { e · l  cid:16  μ} loops  ⎧⎪⎨⎪⎩ ν  cid:25 , a1 ∼ τ1, a2 ∼ τ2 { cid:24 @ a1, @ a2 cid:25  ·l  cid:16  μ} ⎫⎪⎬⎪⎭   cid:20 −→  ν  cid:25 , a1 ∼ τ1, a2 ∼ τ2 { @ a1  cid:16  μ}  cid:5   cid:16  μ  cid:5  } ν  cid:25  { e  cid:16  μ}  cid:20 −→ ν  cid:25  ν  cid:25  { e · r  cid:16  μ}  cid:20 −→ ν  cid:25   cid:5  · r  cid:16  μ  cid:5  }   cid:5  { e  cid:5  { e ν  cid:25  { e  cid:16  μ} loops ν  cid:25  { e · r  cid:16  μ} loops  ⎧⎪⎨⎪⎩ ν  cid:25 , a1 ∼ τ1, a2 ∼ τ2 { cid:24 @ a1, @ a2 cid:25  ·r  cid:16  μ} ⎫⎪⎬⎪⎭  ν  cid:25 , a1 ∼ τ1, a2 ∼ τ2 { @ a2  cid:16  μ}   cid:20 −→   36.7d    36.7e    36.7f    36.7g    36.7h   A pair is considered a value only if its arguments are symbols  rule  36.7a  , which are introduced when the pair is created  rule  36.7b  . The ﬁrst and second projections evaluate to one or the other symbol in the pair, inducing a demand for the value of that component  rules  36.7e  and  36.7h  .  Similar ideas can be used to give a by-need dynamics to sums and recursive types.  36.4 Suspension Types  The dynamics of LFPC outlined in the previous section imposes a by-need interpretation on every type. A more ﬂexible approach is to isolate the machinery of by-need evaluation by introducing a type τ susp of memoized computations, called suspensions, of a value of type τ to an eager variant of FPC. Doing so allows the programmer to choose the extent to which a by-need dynamics is imposed.  Informally, the type τ susp has as introduction form susp x : τ is e representing a sus- pended, self-referential, computation, e, of type τ. It has as elimination form the operation force e  that evaluates the suspended computation presented by e, records the value in a memo table, and returns that value as result. Using suspension types, we can construct lazy types at will. For example, the type of lazy pairs with components of type τ1 and τ2 is expressible as the type  and the type of by-need functions with domain τ1 and range τ2 is expressible as the type  τ1 susp × τ2 susp  τ1 susp  cid:19  τ2.   330  Lazy Evaluation  We may also express more complex combinations of eagerness and laziness, such as the type of “lazy lists” consisting of computations that, when forced, evaluate either to the empty list, or a non-empty list consisting of a natural number and another lazy list:  Contrast this preceding type with this one:  rec t is  unit +  nat × t   susp.  rec t is  unit +  nat × t susp  .  Values of the latter type are the empty list and a pair consisting of a natural number and a computation of another such value.  The language SFPC extends FPC with a type of suspensions:  Typ τ Exp e  ::= susp τ  ::= susp{τ} x.e  susp x : τ is e  τ susp  force e  lcell[a]  force e  lcell[a]  suspension delay force indirection  Suspended computations are potentially self-referential; the bound variable x refers to the suspension itself. The expression lcell[a] is a reference to the suspension named a. The statics of SFPC is given using a judgment of the form  cid:7   cid:12  cid:25  e : τ, where  cid:25  assigns types to the names of suspensions. It is deﬁned by the following rules:   cid:7 , x : susp τ   cid:12  cid:25  e : τ   cid:7   cid:12  cid:25  susp{τ} x.e  :susp  τ    cid:7   cid:12  cid:25  e : susp τ   cid:7   cid:12  cid:25  force e  : τ   cid:7   cid:12  cid:25 ,a∼τ lcell[a] : susp τ   Rule  36.8a  checks that the expression, e, has type τ under the assumption that x, which stands for the suspension itself, has type susp τ .  The dynamics of SFPC is eager, with memoization conﬁned to the suspension type as  described by the following rules:  ⎧⎪⎨⎪⎩  lcell[a] val cid:25 ,a∼τ  ν  cid:25  { susp{τ} x.e   cid:16  μ}   cid:20 −→  ⎫⎪⎬⎪⎭  ν  cid:25 , a ∼ τ { lcell[a]  cid:16  μ ⊗ a  cid:9 → [lcell[a] x]e }  ν  cid:25  { e  cid:16  μ}  cid:20 −→ ν  cid:25  ν  cid:25  { force e   cid:16  μ}  cid:20 −→ ν  cid:25    cid:5  { e  cid:5   cid:16  μ  cid:5  }  cid:5  { force e   cid:5    cid:16  μ  cid:5  }   36.8a    36.8b    36.8c    36.9a    36.9b    36.9c    331  Exercises  ⎧⎪⎨⎪⎩ ν  cid:25 , a ∼ τ { force lcell[a]   cid:16  μ ⊗ a  cid:9 → e }  e val cid:25 ,a∼τ   cid:20 −→  ⎫⎪⎬⎪⎭  ν  cid:25 , a ∼ τ { e  cid:16  μ ⊗ a  cid:9 → e } ν  cid:25 , a ∼ τ { e  cid:16  μ ⊗ a  cid:9 →  }   cid:20 −→  cid:5   cid:16  μ   cid:5   ν  cid:25   , a ∼ τ { e  ⎧⎪⎨⎪⎩ ν  cid:25 , a ∼ τ { force lcell[a]   cid:16  μ ⊗ a  cid:9 → e }   cid:5  ⊗ a  cid:9 →  }  , a ∼ τ { force lcell[a]   cid:16  μ   cid:5  ⊗ a  cid:9 → e   cid:20 −→  ν  cid:25    cid:5    cid:5  }  ⎫⎪⎬⎪⎭   36.9d    36.9e   Rule  36.9a  speciﬁes that a reference to a suspension is a value. Rule  36.9b  speciﬁes that evaluation of a delayed computation consists of allocating a fresh symbol for it in the memo table, and returning a reference to that suspension. Rules  36.9c  to  36.9e  specify that demanding the value of a suspension forces evaluation of the suspended computation, which is then stored in the memo table and returned as the result.  The by-need dynamics given here is inspired by Ariola and Felleisen  1997  but with the difference that by-need cells are regarded as assignables, rather than variables. Doing so maintains the principle that variables are given meaning by substitution. In contrast, by-need cells are a form of assignable to which at most one assignment is ever done.  36.5 Notes  Exercises  36.1. Recall from Chapter 20 that, under a lazy interpretation, the recursive type  rec t is [z  cid:9 → unit, s  cid:9 → t]  contains the “inﬁnite number” ω  cid:2  fix x : nat is s x . Contrast the behavior of ω under the by-need interpretation given in this chapter with that by-name interpretation given in Chapters 19 and 20.  36.2. In LFPC the putative recursive type of “lists” of natural numbers, rec t is [nil  cid:9 → unit, cons  cid:9 → nat × t],  is, rather, the type of ﬁnite or inﬁnite streams of natural numbers. To prove this, exhibit the stream of all natural numbers as an element of this type.   332  Lazy Evaluation  36.3. Complete the deﬁnition of LFPC by giving the by-need dynamics for unit, void,  36.4. LFPC can be interpreted into SFPC. Complete the following chart deﬁning the  sum, and recursive types.  interpretation, cid:2 τ, of the type τ:  unit  cid:2  . . . τ1 × τ2  cid:2  . . . void  cid:2  . . . τ1 + τ2  cid:2  . . . rec t is τ  cid:2  . . . .  Hint: Characterize the values of the lazy types in the left column, and express those values as eager types in the right column, using suspensions where necessary.   P A R T XV  Parallelism    37  Nested Parallelism  Parallel computation seeks to reduce the running times of programs by allowing many com- putations to be carried out simultaneously. For example, if we wish to add two numbers, each given by a complex computation, we may consider evaluating the addends simul- taneously, then computing their sum. The ability to exploit parallelism is limited by the dependencies among parts of a program. Obviously, if one computation depends on the result of another, then we have no choice but to execute them sequentially so that we may propagate the result of the ﬁrst to the second. Consequently, the fewer dependencies among sub-computations, the greater the opportunities for parallelism.  In this chapter, we discuss the language PPCF, which is the extension of PCF with nested parallelism. Nested parallelism has a hierarchical structure arising from forking two  or more  parallel computations, then joining these computations to combine their results before proceeding. Nested parallelism is also known as fork-join parallelism. We will consider two forms of dynamics for nested parallelism. The ﬁrst is a structural dynamics in which a single transition on a compound expression may involve multiple transitions on its constituent expressions. The second is a cost dynamics  introduced in Chapter 7  that focuses attention on the sequential and parallel complexity  also known as the work and the depth, or span  of a parallel program by associating a series-parallel graph with each computation.  37.1 Binary Fork-Join  The syntax of PPCF extends that of PCF with the following construct: ::= par e1; e2; x1.x2.e  par x1 = e1 and x2 = e2 in e  Exp e  parallel let  The variables x1 and x2 are bound only within e, and not within e1 or e2, which ensures that they are not mutually dependent and hence can be evaluated simultaneously. The variable bindings represent a fork of two parallel computations e1 and e2, and the body e represents their join.  The static of PPCF enriches that of PCF with the following rule for parallel let:   cid:7   cid:12  e1 : τ1  cid:7   cid:12  e2 : τ2  cid:7 , x1 : τ1, x2 : τ2  cid:12  e : τ   cid:7   cid:12  par e1; e2; x1.x2.e  : τ   37.1    336  Nested Parallelism  The sequential structural dynamics of PPCF is deﬁned by a transition judgment of the form e  cid:20 →seq e   cid:5  deﬁned by these rules:  The parallel structural dynamics of PPCF is given by a transition judgment of the form e  cid:20 →par e   cid:5 , deﬁned as follows:  e1  cid:20 →seq e   cid:5  1  par e1; e2; x1.x2.e   cid:20 →seq par e e2  cid:20 →seq e  cid:5  2  par e1; e2; x1.x2.e   cid:20 →seq par e1; e  e1 val   cid:5  1; e2; x1.x2.e    cid:5  2; x1.x2.e   par e1; e2; x1.x2.e   cid:20 →seq [e1, e2 x1, x2]e  e1 val  e2 val  e1  cid:20 →par e  e2  cid:20 →par e  cid:5  2 par e1; e2; x1.x2.e   cid:20 →par par e  cid:5  1; e   cid:5  1   cid:5  2; x1.x2.e   e1  cid:20 →par e   cid:5  1  e2 val par e1; e2; x1.x2.e   cid:20 →par par e  cid:5  1; e2; x1.x2.e  e2  cid:20 →par e  cid:5  2  par e1; e2; x1.x2.e   cid:20 →par par e1; e   cid:5  2; x1.x2.e   e1 val  par e1; e2; x1.x2.e   cid:20 →par [e1, e2 x1, x2]e  e1 val  e2 val   37.2a    37.2b    37.2c    37.3a    37.3b    37.3c    37.3d   The parallel dynamics abstracts away from any limitations on processing capacity; such limitations are considered in Section 37.4.  The implicit parallelism theorem states that the sequential and the parallel dynamics coincide. Consequently, we need never be concerned with the semantics of a parallel pro- gram  its meaning is given by the sequential dynamics , but only with its efﬁciency. As a practical matter, this means that a program can be developed on a sequential platform, even if it is meant to run on a parallel platform, because the behavior is not affected by whether we execute it using a sequential or a parallel dynamics. Because the sequential dynamics is deterministic  every expression has at most one value , the implicit parallelism theorem implies that the parallel dynamics is also deterministic. For this reason, the implicit paral- lelism theorem is also known as the deterministic parallelism theorem. This terminology emphasizes the distinction between deterministic parallelism, the subject of this chapter, from non-deterministic concurrency, the subject of Chapters 39 and 40. e ⇓ v in the style of Chapter 7, and showing that e ⇓ v  A proof of the implicit parallelism theorem can be given by giving an evaluation dynamics  e  cid:20 →∗  e  cid:20 →∗  iff  iff  seq v  par v   337  37.1 Binary Fork-Join   where v is a closed expression such that v val . The most important rule of the evaluation dynamics is for the evaluation of a parallel let:  e1 ⇓ v1  e2 ⇓ v2 par e1; e2; x1.x2.e  ⇓ v  [v1, v2 x1, x2]e ⇓ v   37.4   The other rules are easily derived from the structural dynamics of PCF as in Chapter 7.  It is possible to show that the sequential dynamics of PPCF agrees with its evaluation  dynamics by extending the proof of Theorem 7.2. Lemma 37.1. For all v val, e  cid:20 →∗  seq v if, and only if, e ⇓ v.  Proof and e2  cid:20 →∗  It sufﬁces to show that if e  cid:20 →seq e seq v2 and [v1, v2 x1, x2]e  cid:20 →∗   cid:5  and e seq v, then   cid:5  ⇓ v, then e ⇓ v, and that if e1  cid:20 →∗  seq v1  par x1 = e1 and x2 = e2 in e  cid:20 →∗  seq v.  By a similar argument, we may show that the parallel dynamics also agrees with the  evaluation dynamics, and hence with the sequential dynamics. Lemma 37.2. For all v val, e  cid:20 →∗  par v if, and only if, e ⇓ v.  Proof and e2  cid:20 →∗  It sufﬁces to show that if e  cid:20 →par e par v2 and [v1, v2 x1, x2]e  cid:20 →∗   cid:5  and e par v, then   cid:5  ⇓ v, then e ⇓ v, and that if e1  cid:20 →∗  par v1  par x1 = e1 and x2 = e2 in e  cid:20 →∗  par v.  The proof of the ﬁrst is by induction on the parallel dynamics. The proof of the second proceeds by simultaneous induction on the derivations of e1  cid:20 →∗ par v2. If e1 = v1 with v1 val and e2 = v2 with v2 val, then the result follows immediately from the third premise. If e2 = v2 but e1  cid:20 →par e par v1, then by induction we have that par v, and hence the result follows by an application of par x1 = e rule  37.3b . The symmetric case follows similarly by an application of rule  37.3c , and in the case that both e1 and e2 transition, the result follows by induction and rule  37.3a .  1 and x2 = v2 in e  cid:20 →∗  cid:5   par v1 and e2  cid:20 →∗   cid:20 →∗   cid:5  1  Theorem 37.3  Implicit Parallelism . The sequential and parallel dynamics coincide: for all v val, e  cid:20 →∗  seq v iff e  cid:20 →∗  par v.  Proof By Lemmas 37.1 and 37.2.  The implicit parallelism theorem states that parallelism does not affect the semantics of a program, only the efﬁciency of its execution. Correctness is not affected by parallelism, only efﬁciency.   338  Nested Parallelism  37.2 Cost Dynamics  In this section, we deﬁne a parallel cost dynamics that assigns a cost graph to the evaluation of a PPCF expression. Cost graphs are deﬁned by the following grammar:  Cost  c  ::= 0 1 c1 ⊗ c2 c1 ⊕ c2  zero cost unit cost parallel combination sequential combination  A cost graph is a series-parallel ordered directed acyclic graph, with a designated source node and sink node. For 0 the graph consists of one node and no edges, with the source and sink both being the node itself. For 1 the graph consists of two nodes and one edge directed from the source to the sink. For c1⊗ c2, if g1 and g2 are the graphs of c1 and c2, respectively, then the graph has two extra nodes, a source node with two edges to the source nodes of g1 and g2, and a sink node, with edges from the sink nodes of g1 and g2 to it. The children of the source are ordered according to the sequential evaluation order. Finally, for c1 ⊕ c2, where g1 and g2 are the graphs of c1 and c2, the graph has as source node the source of g1, as sink node the sink of g2, and an edge from the sink of g1 to the source of g2.  The intuition behind a cost graph is that nodes represent subcomputations of an overall computation, and edges represent sequentiality constraints, stating that one computation depends on the result of another, and hence cannot be started before the one on which it depends completes. The product of two graphs represents parallelism opportunities in which there are no sequentiality constraints between the two computations. The assignment of source and sink nodes reﬂects the overhead of forking two parallel computations and joining them after they have both completed. At the structural level, we note that only the root has no ancestors, and only the ﬁnal node of the cost graph has no descendents. Interior nodes may have one or two descendents, the former representing a sequential dependency, and the latter representing a fork point. Such nodes may have one or two ancestors, the former corresponding to a sequential dependency and the latter representing a join point. We associate with each cost graph two numeric measures, the work, wk c , and the depth,  dp c . The work is deﬁned by the following equations:  0 1 wk c1  + wk c2  wk c1  + wk c2  The depth is deﬁned by the following equations:  wk c  =  if c = 0 if c = 1 if c = c1 ⊗ c2 if c = c1 ⊕ c2  dp c  =  0 1 max dp c1 , dp c2   dp c1  + dp c2   if c = 0 if c = 1 if c = c1 ⊗ c2 if c = c1 ⊕ c2  ⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩ ⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩   37.5    37.6    339  37.2 Cost Dynamics  Informally, the work of a cost graph determines the total number of computation steps represented by the cost graph, and thus corresponds to the sequential complexity of the computation. The depth of the cost graph determines the critical path length, the length of the longest dependency chain within the computation, which imposes a lower bound on the parallel complexity of a computation. The critical path length is a lower bound on the number of steps required to complete the computation. In Chapter 7 we introduced cost dynamics to assign time complexity to computations. The proof of Theorem 7.7 shows that e ⇓k v iff e  cid:20 −→k v. That is, the step complexity of an evaluation of e to a value v is just the number of transitions required to derive e  cid:20 −→∗ v. Here we use cost graphs as the measure of complexity, then relate these cost graphs to the structural dynamics given in Section 37.1. The judgment e ⇓c v, where e is a closed expression, v is a closed value, and c is a cost graph speciﬁes the cost dynamics. By deﬁnition we arrange that e ⇓0 e when e val. The cost assignment for let is given by the following rule:  e1 ⇓c1 v1  e2 ⇓c2 v2  [v1, v2 x1, x2]e ⇓c v  par e1; e2; x1.x2.e  ⇓ c1⊗c2 ⊕1⊕c v   37.7   The cost assignment speciﬁes that, under ideal conditions, e1 and e2 are evaluated in parallel, and that their results are passed to e. The cost of fork and join is implicit in the parallel combination of costs, and assign unit cost to the substitution because we expect it to be implemented by a constant-time mechanism for updating an environment. The cost dynamics of other language constructs is speciﬁed in a similar way, using only sequential combination to isolate the source of parallelism to the let construct.  Two simple facts about the cost dynamics are important to keep in mind. First, the cost  assignment does not inﬂuence the outcome. Lemma 37.4. e ⇓ v iff e ⇓c v for some c.  Proof From right to left, erase the cost assignments to construct an evaluation derivation. From left to right, decorate the evaluation derivations with costs as determined by the rules deﬁning the cost dynamics.  Second, the cost of evaluating an expression is uniquely determined.  Lemma 37.5. If e ⇓c v and e ⇓c  cid:5 . v, then c is c Proof By induction on the derivation of e ⇓c v.   cid:5   The link between the cost dynamics and the structural dynamics is given by the following theorem, which states that the work cost is the sequential complexity, and the depth cost is the parallel complexity, of the computation. Theorem 37.6. If e ⇓c v, then e  cid:20 →w Conversely, if e  cid:20 →w e  cid:20 →d  par v, where w = wk c  and d = dp c . seq v, then there exists c such that e ⇓c v with wk c  = w, and if   cid:5 , then there exists c  seq v and e  cid:20 →d   cid:5  such that e ⇓c   cid:5  with dp c   cid:5   = d.  v   cid:5   par v   340  Nested Parallelism  Proof The ﬁrst part is proved by induction on the derivation of e ⇓c v, the interesting case being rule  37.7 . By induction, we have e1  cid:20 →w1 seq v2, and [v1, v2 x1, x2]e  cid:20 →w v, where w1 = wk c1 , w2 = wk c2 , and w = wk c . By pasting together derivations, we get a derivation  seq v1, e2  cid:20 →w2  seq  par e1; e2; x1.x2.e   cid:20 →w1  cid:20 →w2  cid:20 →seq [v1, v2 x1, x2]e  cid:20 →w  seq par v1; e2; x1.x2.e  seq par v1; v2; x1.x2.e   seq v.  Noting that wk  c1 ⊗ c2  ⊕ 1 ⊕ c  = w1 + w2 + 1 + w completes the proof. Similarly, we have by induction that e1  cid:20 →d1 par v, where d1 = dp c1 , d2 = dp c2 , and d = dp c . Assume, without loss of generality, that d1 ≤ d2  otherwise simply swap the roles of d1 and d2 in what follows . We may paste together derivations as follows:  par v2, and [v1, v2 x1, x2]e  cid:20 →d  par v1, e2  cid:20 →d2  par e1; e2; x1.x2.e   cid:20 →d1   cid:5  2; x1.x2.e   par v1; v2; x1.x2.e   par par v1; e  par   cid:20 →d2−d1  cid:20 →par [v1, v2 x1, x2]e  cid:20 →d  par v.   cid:5    cid:5    cid:5    cid:5  ⇓c   cid:5  ⇓c   cid:5  ⇓c   cid:5  with e   cid:5  with e  cid:5  such that e   cid:5 , and a simple calculation shows that wk c  = wk c  cid:5  for e  v, then e ⇓c v  cid:5  +1.  cid:5 , where v. But then e ⇓c v, where  cid:5   + 1, as required.  cid:5  as above, and hence e ⇓c v for some c such that dp c  =  Calculating dp  c1 ⊗ c2  ⊕ 1 ⊕ c  = max d1, d2  + 1 + d completes the proof. Turning to the second part, it sufﬁces to show that if e  cid:20 →seq e  cid:5  +1, and if e  cid:20 →par e v, then e ⇓c v with dp c  = dp c with wk c  = wk c Suppose that e = par e1; e2; x1.x2.e0  with e1 val and e2 val. Then e  cid:20 →seq e e = [e1, e2 x1, x2]e0 and there exists c c =  0 ⊗ 0  ⊕ 1 ⊕ c Similarly, e  cid:20 →par e  cid:5   + 1, as required. dp c Suppose that e = par e1; e2; x1.x2.e0  and e  cid:20 →seq e  cid:5   cid:5 , where e 1; e2; x1.x2.e0  and e1  cid:20 →seq e  cid:5  1 v1, 2  ⊕ 1 ⊕ c ⊗ c e2 ⇓c  cid:5   cid:5   cid:5  0. By induction there exists c1 such that wk c1  = 1 + wk c 1  and e1 ⇓c1 v1. But then e ⇓c v, with c = 1  cid:5   c1 ⊗ c By a similar argument, suppose that e = par e1; e2; x1.x2.e0  and e  cid:20 →par e  cid:5  =  cid:5   cid:5   cid:5  1 v1, 1; e v. Then by inversion e par e ⇓c 2 v2, [v1, v2 x1, x2]e0 ⇓c0 v. But then e ⇓c v, where c =  c1 ⊗ c2  ⊕ 1⊕ c0, e1 ⇓c1 v1 1  cid:5   cid:5  e 2 , and [v1, v2 x1, x2]e0 ⇓c0 v. with dp c1  = 1 + dp c 2  cid:5  Calculating, we get  2 v2, and [v1, v2 x1, x2]e0 ⇓c 2  ⊕ 1 ⊕ c  cid:5   cid:5  0. 2; x1.x2.e0  and e1  cid:20 →par e  cid:5    cid:5  ⇓c 1 , e2 ⇓c2 v2 with dp c2  = 1 + dp c  cid:5    cid:5  1. From the assumption that e  cid:5  0 v, with c   cid:5  v, we have by inversion that e 1   cid:5 , where e ⇓c  1, e2  cid:20 →par e  cid:5    cid:5  ⇓c  cid:5  =  c   cid:5  = par e   cid:5  2, and e  ⇓c   cid:5    cid:5    cid:5   1  + 1, dp c dp c  = max dp c  cid:5  = max dp c  cid:5  1 , dp c ⊗ c 2  ⊕ 1 ⊕ c0  + 1 = dp  c  cid:5   cid:5  1   + 1, = dp c  cid:5   2  + 1  + 1 + dp c0   cid:5   2   + 1 + 1 + dp c0   cid:5   which completes the proof.   341  37.3 Multiple Fork-Join  Corollary 37.7. If e  cid:20 →w wk c  = w and dp c  = d.  seq v and e  cid:20 →d  par v   cid:5 , then v is v   cid:5  and e ⇓c v for some c such that  37.3 Multiple Fork-Join  So far we have conﬁned attention to binary fork join parallelism induced by the parallel let construct. A generalizaton, called data parallelism, allows the simultaneous creation of any number of tasks that compute on the components of a data structure. The main example is a sequence of values of a speciﬁed type. The primitive operations on sequences are a natural source of unbounded parallelism. For example, we may consider a parallel map construct that applies a given function to every element of a sequence simultaneously, forming a sequence of the results.  We will consider here a simple language of sequence operations to illustrate the main  ideas.  Typ τ Exp e  ::= seq τ  ::= seq e0, . . . ,en−1   len e  sub e1; e2  tab x.e1; e2  map x.e1; e2  cat e1; e2   τ seq [e0, . . . ,en−1] e e1[e2] tab x.e1; e2  [e1  x ∈ e2] cat e1; e2   sequence sequence size element tabulate map concatenate  The expression seq e0, . . . ,en−1  evaluates to an n-sequence whose elements are given by the expressions e0, . . . , en−1. The operation len e  returns the number of elements in the sequence given by e. The operation sub e1; e2  retrieves the element of the sequence given by e1 at the index given by e2. The tabulate operation, tab x.e1; e2 , yields the sequence of length given by e2 whose ith element is given by [i x]e1. The operation map x.e1; e2  computes the sequence whose ith element is given by [e x]e1, where e is the ith element of the sequence given by e2. The operation cat e1; e2  concatenates two sequences of the same type.  The statics of these operations is given by the following typing rules:   cid:7   cid:12  e0 : τ . . .  cid:7   cid:12  en−1 : τ  cid:7   cid:12  seq e0, . . . ,en−1  :seq  τ    cid:7   cid:12  e : seq τ   cid:7   cid:12  len e  : nat   cid:7   cid:12  e1 : seq τ   cid:7   cid:12  e2 : nat   cid:7   cid:12  sub e1; e2  :τ   cid:7 , x : nat  cid:12  e1 : τ  cid:7   cid:12  e2 : nat   cid:7   cid:12  tab x.e1; e2  :seq τ     37.8a    37.8b    37.8c    37.8d    342  Nested Parallelism   cid:7   cid:12  e2 : seq τ   cid:7 , x : τ  cid:12  e1 : τ   cid:7   cid:12  map x.e1; e2  : seq τ   cid:5     cid:5    cid:7   cid:12  e1 : seq τ   cid:7   cid:12  e2 : seq τ    cid:7   cid:12  cat e1; e2  : seq τ   The cost dynamics of these constructs is deﬁned by the following rules:  e2 ⇓c num[n]  e0 ⇓c0 v0   cid:24  en−1 ⇓cn−1 vn−1 seq e0, . . . ,en−1  ⇓ n−1 i=0 ci seq v0, . . . ,vn−1   . . .  e ⇓c seq v0, . . . ,vn−1  len e  ⇓c⊕1 num[n]  e2 ⇓c2 num[i] sub e1; e2  ⇓c1⊕c2⊕1 vi   0 ≤ i < n   e1 ⇓c1 seq v0, . . . ,vn−1  tab x.e1; e2  ⇓c⊕ cid:24  [num[0] x]e1 ⇓c0 v0 [v0 x]e1 ⇓c0 v map x.e1; e2  ⇓c⊕ cid:24  e1 ⇓c1 seq v0, . . . , vm−1  cat e1; e2  ⇓c1⊕c2⊕ cid:24   [num[n − 1] x]e1 ⇓cn−1 vn−1  . . .  n−1 i=0 ci seq v0, . . . ,vn−1   e2 ⇓c seq v0, . . . ,vn−1   cid:5  0  . . .  [vn−1 x]e1 ⇓cn−1 v  cid:5  n−1    cid:5  0, . . . ,v  n−1 i=0 ci seq v   cid:5  n−1  e2 ⇓c2 seq v  m+n i=0 1 seq v0, . . . , vm−1, v   cid:5  0, . . . , v   cid:5  n−1   cid:5   cid:5  n−1  0, . . . , v   37.8e    37.8f    37.9a    37.9b    37.9c    37.9d    37.9e    37.9f   The cost dynamics for sequence operations is validated by introducing a sequential and  parallel cost dynamics and extending the proof of Theorem 37.6 to cover this extension.  37.4 Bounded Implementations  Theorem 37.6 states that the cost dynamics accurately models the dynamics of the parallel let construct, whether executed sequentially or in parallel. The theorem validates the cost dynamics from the point of view of the dynamics of the language, and permits us to draw conclusions about the asymptotic complexity of a parallel program that abstracts away from the limitations imposed by a concrete implementation. Chief among these is the restriction to a ﬁxed number, p > 0, of processors on which to schedule the workload. Besides limiting the available parallelism this also imposes some synchronization overhead that must be taken into account. A bounded implementation is one for which we may establish an asymptotic bound on the execution time once these overheads are taken into account.  A bounded implementation must take account of the limitations and capabilities of the hardware on which the program is run. Because we are only interested in asymptotic upper bounds, it is convenient to formulate an abstract machine model, and to show that the   343  37.4 Bounded Implementations  primitives of the language can be implemented on this model with guaranteed time  and space  bounds. One example of such a model is the shared-memory multiprocessor, or SMP, model. The basic assumption of the SMP model is that there are some ﬁxed p > 0 processors coordinated by an interconnect that permits constant-time access to any object in memory shared by all p processors.1 An SMP is assumed to provide a constant-time synchronization primitive with which to control simultaneous access to a memory cell. There are a variety of such primitives, any of which are enough to provide a parallel fetch- and-add instruction that allows each processor to get the current contents of a memory cell and update it by adding a ﬁxed constant in a single atomic operation—the interconnect serializes any simultaneous accesses by more than one processor.  Building a bounded implementation of parallelism involves two major tasks. First, we must show that the primitives of the language can be implemented efﬁciently on the abstract machine model. Second, we must show how to schedule the workload across the processors to minimize execution time by maximizing parallelism. When working with a low-level machine model such as an SMP, both tasks involve a fair bit of technical detail to show how to use low-level machine instructions, including a synchronization primitive, to implement the language primitives and to schedule the workload. Collecting these together, we may then give an asymptotic bound on the time complexity of the implementation that relates the abstract cost of the computation to cost of implementing the workload on a p-way multiprocessor. The prototypical result of this kind is Brent’s Theorem. Theorem 37.8. If e ⇓c v with wk c  = w and dp c  = d, then e can be evaluated on a p-processor SMP in time O max w p, d  .  The theorem tells us that we can never execute a program in fewer steps than its depth d and that, at best, we can divide the work up evenly into w p rounds of execution by the p processors. Note that if p = 1 then the theorem establishes an upper bound of O w  steps, the sequential complexity of the computation. Moreover, if the work is proportional to the depth, then we are unable to exploit parallelism, and the overall time is proportional to the work alone. Theorem 37.8 motivates consideration of a useful ﬁgure of merit, the parallelizability ratio, which is the ratio w d of work to depth. If w d 1 p, then the program is paral- lelizable, because then w p 1 d, and we may therefore reduce running time by using p processors at each step. If the parallelizability ratio is a constant, then d will dominate w p, and we will have little opportunity to exploit parallelism to reduce running time. It is not known, in general, whether a problem admits a parallelizable solution. The best we can say, on present knowledge, is that there are algorithms for some problems that have a high degree of parallelizability, and there are problems for which no such algorithm is known. It is a difﬁcult problem in complexity theory to analyze which problems are parallelizable, and which are not.  Proving Brent’s Theorem for an SMP would take us much too far aﬁeld for the present purposes. Instead, we shall prove a Brent-type Theorem for an abstract machine, the P machine. The machine is unrealistic in that it is deﬁned at a very high level of abstraction. But it is designed to match well the cost semantics given earlier in this chapter. In particular,   344  Nested Parallelism  there are mechanisms that account for both sequential and parallel dependencies in a computation.  At the highest level, the state of the P machine consists of a global task graph whose structure corresponds to a “diagonal cut” through the cost graph of the overall computation. Nodes immediately above the cut are eligible to be executed, higher ancestors having already been completed, and whose immediate descendents are waiting for their ancestors to complete. Further descendents in the full task graph are tasks yet to be created, once the immediate descendents are ﬁnished. The P machine discards completed tasks, and future tasks beyond the immediate dependents are only created as execution proceeds. Thus, it is only those nodes next to the cut line through the cost graph that are represented in the P machine state. The global state of the P machine is a conﬁguration of the form ν  cid:25  { μ}, where  cid:25  is degenerated to just a ﬁnite set of  pairwise distinct  task names and μ is a ﬁnite mapping the task names in  cid:25  to local states, representing the state of an individual task. A local state is either a closed PCF expression, or one of two special join points that implement the sequential and parallel dependencies of a task on one or two ancestors, respectively.2 Thus, when expanded out, a global state has the form  ν a1, . . . , an { a1  cid:9 → s1 ⊗ . . . ⊗ an  cid:9 → sn },   cid:5  { μ  where n ≥ 1, and each si is a local state. The ordering of the tasks in a state, like the order of declarations in the signature, is not signiﬁcant.  cid:5  }. There are two A P machine state transition has the form ν  cid:25  { μ}  cid:20 −→ ν  cid:25  forms of such transitions, the global and the local. A global step selects as many tasks as are available, up to a pre-speciﬁed parameter p > 0, which represents the number of processors available at each round.  Such a scheduler is greedy in the sense that it never fails to execute an available task, up to the speciﬁed limit for each round.  A task is ﬁnished if it consists of a closed PCF value, or is a join point whose dependents are not yet ﬁnished; otherwise, a task is available, or ready. A ready task is always capable of taking a local step consisting of either a step of PCF, expressed in the setting of the P machine, or a synchronization step that manages the join-point logic. Because the P machine employs a greedy scheduler, it must complete execution in time proportional to max w p, d  steps by doing up to p steps of work at a time, insofar as it is possible within the limits of the depth of the computation. We thus get a Brent-type Theorem for the abstract machine that illustrates more sophisticated Brent-type Theorems for other models, such as the PRAM, that are used in the analysis of parallel algorithms.  The local transitions of the P machine corresponding to the steps of PCF itself are illustrated by the following example rules for application; the others follow a similar pattern.3  ν a { a  cid:9 → e1 e2 }  cid:20 −→loc ν a a1 { a  cid:9 → join[a1] x1.x1 e2   ⊗ a1  cid:9 → e1 }   37.10a   ¬ e1 val   ν a { a  cid:9 → e1 e2 }  cid:20 −→loc ν a a2 { a  cid:9 → join[a2] x2.e1 x2   ⊗ a2  cid:9 → e2 }  e1 val   37.10b    345  37.4 Bounded Implementations  ν a1 a2 { a1  cid:9 → join[a2] x2.e1 x2   ⊗ a2  cid:9 → e2 }  cid:20 −→loc ν a1 { a1  cid:9 → [e2 x2]e1 }  37.10c   e1 val  e2 val  ν a { a  cid:9 →  λ  x : τ2  e  e2 }  cid:20 −→loc ν a { a  cid:9 → [e2 x]e }  e2 val   37.10d   Rules  37.10a  and  37.10b  create create tasks for the evaluation of the function and argument of an expression. Rule  37.10c  propagates the result of evaluation of the function or argument of an application to the appropriate application expression. This rule mediates between the ﬁrst two rules and rule  37.10d , which effects a β-reduction in-place.  The local transitions of the P machine corresponding to binary fork and join are as  ν a { a  cid:9 → par e1; e2; x1.x2.e }  follows:⎧⎪⎨⎪⎩ ⎫⎪⎬⎪⎭ ⎧⎪⎨⎪⎩ ν a1, a2, a { a1  cid:9 → e1 ⊗ a2  cid:9 → e2 ⊗ a  cid:9 → join[a1; a2] x1; x2.e } ⎫⎪⎬⎪⎭  ν a1, a2, a { a1  cid:9 → e1 ⊗ a2  cid:9 → e2 ⊗ a  cid:9 → join[a1; a2] x1; x2.e }   cid:20 −→loc   cid:20 −→loc  e1 val  e2 val  ν a { a  cid:9 → [e1, e2 x1, x2]e }   37.11a    37.11b   Rule  37.11a  creates two parallel tasks on which the executing task depends. The expression join[a1; a2] x1; x2.e  is blocked on tasks a1 and a2, so that no local step applies to it. Rule  37.11b  synchronizes a task with the tasks on which it depends once their execution has completed; those tasks are no longer required, and are eliminated from the state. many as p ≥ 1 processors.  Each global transition is the simultaneous execution of one step of computation on as  ν  cid:25 1 a1 { μ1 ⊗ a1  cid:9 → s1 }  cid:20 −→loc ν  cid:25   ⊗ a1  cid:9 → s   cid:5  1  }  . . .  ν  cid:25 n an { μn ⊗ an  cid:9 → sn }  cid:20 −→loc ν  cid:25   ⎧⎪⎨⎪⎩ ν  cid:25 0  cid:25 1 a1 . . .  cid:25 n an { μ0 ⊗ μ1 ⊗ a1  cid:9 → s1 ⊗ . . . ⊗ μn ⊗ an  cid:9 → sn }  ⊗ an  cid:9 → s   cid:20 −→glo n an { μ0 ⊗ μ ⊗ a1  cid:9 → s  cid:5   cid:5  1  ⊗ . . . ⊗ μ  cid:5  n   cid:5  1 a1 . . .  cid:25   ⊗ an  cid:9 → s  ν  cid:25 0  cid:25    cid:5  n   cid:5  n   cid:5  1  }  }  ⎫⎪⎬⎪⎭  1 a1 { μ  cid:5   cid:5  1 n an { μ  cid:5   cid:5  n   37.12   At each global step, some number 1 ≤ n ≤ p of ready tasks are scheduled for execution, where n is maximal among the number of ready tasks. Because no two distinct tasks may depend on the same task, we may partition the n tasks so that each scheduled task is grouped with the tasks on which it depends as necessary for any local join step. Any local fork step adds two fresh tasks to the state resulting from the global transition; any local join step eliminates two tasks whose execution has completed. A subtle point is that it is implicit in our name binding conventions that the names of any created tasks are globally unique, even though they are locally created. In implementation terms, this requires a synchronization   346  Nested Parallelism  step among the processors to ensure that task names are not accidentally reused among the parallel tasks.  The proof of a Brent-type Theorem for the P machine is now obvious. We need only ensure that the parameter n of rule  37.12  is chosen as large as possible at each step, limited only by the parameter p and the number of ready tasks. A scheduler with this property is greedy; it never allows a processor to go idle if work remains to be done. Consequently, if there are always p available tasks at each global step, then the evaluation will complete in w p steps, where w is the work complexity of the program. If, at some stage, fewer than p tasks are available, then performance degrades according to the sequential dependencies among the sub-computations. In the limiting case, the P machine must take at least d steps, where d is the depth of the computation.  37.5 Scheduling  The global transition relation of the P machine deﬁned in Section 37.4 affords wide latitude in the choice of tasks that are advanced by taking a local transition. Doing so abstracts from implementation details that are irrelevant to the proof of the Brent-type Theorem given later in that section, the only requirement being that the number of tasks chosen be as large as possible up to the speciﬁed bound p, representing the number of available processors. When taking into account factors not considered here, it is necessary to specify the scheduling policy more precisely—for example, different scheduling policies may have asymptotically different space requirements. The overall idea is to consider scheduling a computation on p processors as a p-way parallel traversal of its cost graph, visiting up to p nodes at a time in an order consistent with the dependency ordering. In this section, we will consider one such traversal, p-way parallel depth-ﬁrst-search, or p-DFS, which specializes to the familiar depth-ﬁrst traversal in the case that p = 1.  Recall that the depth ﬁrst-search of a directed graph maintain a stack of unvisited nodes, which is initialized with the start node. At each round, a node is popped from the stack and visited, and then its unvisited children are pushed on the stack  in reverse order in the case of ordered graphs , completing that round. The traversal terminates when the stack is empty. When viewed as a scheduling strategy, visiting a node of a cost graph consists of scheduling the work associated with that node on a processor. The job of such as scheduler is to do the work of the computation in depth-ﬁrst order, visiting the children of a node from left to right, consistently with the sequential dynamics  which would, in particular, treat a parallel binding as two sequential bindings . Notice that because a cost graph is directed acyclic, there are no “back edges” arising from the traversal, and because it is series-parallel in structure, there are no “cross edges.” Thus, all children of a node are unvisited, and no task is considered more than once.  Although evocative, viewing scheduling as graph traversal invites one to imagine that the cost graph is given explicitly as a data structure, which is not at all the case. Instead, the graph is created dynamically as the sub-computations are executed. At each round, the computation associated with a node may complete  when it has achieved its value , continue   347  37.5 Scheduling   when more work is yet to be done , or fork  when it generates parallel sub-computations with a speciﬁed join point . Once a computation has completed and its value has been passed to the associated join point, its node in the cost graph is discarded. Furthermore, the children of a node only come into existence as a result of its execution, according to whether it completes  no children , continues  one child , or forks  two children . Thus, one may envision that the cost graph “exists” as a cut through the abstract cost graph representing pending tasks that have not yet been activated by the traversal.  A parallel depth-ﬁrst search works much the same way, except that as many as p nodes are visited at each round, constrained only by the presence of unvisited  yet-to-be-scheduled  nodes. One might naively think that this simply means popping up to p nodes from the stack on each round, visiting them all simultaneously, and pushing their dependents on the stack in reverse order, just as for conventional depth-ﬁrst search. But a moment’s thought reveals that this is not correct. Because the cost graphs are ordered, the visited nodes form a sequence determined by the left-to-right ordering of the children of a node. If a node completes, it has no children and is removed from its position in the sequence in the next round. If a node continues, it has one child that occupies the same relative position as its parent in the next round. And if a node forks two children, they are inserted into the sequence after the predecessor, and immediately prior to that node, related to each other by the left-to-right ordering of the children. The task associated to the visited node itself becomes the join point of the immediately preceding pair of tasks, with which it will synchronize when they complete. Thus, the visited sequence of k ≤ p nodes becomes, on the next round, anywhere from 0  if all nodes completes  to 3×k nodes  if each node forks . These are placed into consideration, in the speciﬁed order, for the next round to ensure that they are processed in depth-ﬁrst order. Importantly, the data structure maintaining the unvisited nodes of the graph is not a simple pushdown stack, because of the “in-place” replacement of each visited node by zero, one, or two nodes in between its predecessor and successor in the sequential ordering of the visited nodes.  Consider a variant of the P machine in which the order of the tasks is signiﬁcant. A task is ﬁnished if it is a value, blocked if it is a join, and ready otherwise. Local transitions remain the same as in Section 37.4, bearing in mind that the ordering is signiﬁcant. A global transition, however, consists of making a local transition on each of the ﬁrst k ≤ p ready tasks.4 After this selection, the global state is depicted as follows:  ν  cid:25 0 a1  cid:25 1 . . . ak  cid:25 k  cid:25  { μ0 ⊗ a1  cid:9 → e1 ⊗ μ1 ⊗ . . . ak  cid:9 → ek ⊗ μ}  where each μi consists of ﬁnished or blocked tasks, and each ei is ready. A schedule is greedy If k < p only when no task in μ is ready.  After a local transition is made on each of the k selected tasks, the resulting global state  has the form  ν  cid:25 0 cid:25    cid:5  1 a1  cid:25 1 . . .  cid:25   k ak  cid:25 k  cid:25  { μ0 ⊗ μ  cid:5   cid:5  1  ⊗ a1  cid:9 → e   cid:5  1  ⊗ μ1 ⊗ . . . μ  cid:5  k  ⊗ ak  cid:9 → e   cid:5  k  ⊗ μ}  i represents the newly created task s  of the local transition on task ai  cid:9 → ei,  cid:5  where each μ  cid:5  i is the expression resulting from the transition on that task. Next, all possible and each e   348  Nested Parallelism  synchronizations are made by replacing each occurrence of an adjacent triple of the form  ai,1  cid:9 → e1 ⊗ ai,2  cid:9 → e2 ⊗ ai  cid:9 → join[ai,1; ai,2] x1; x2.e    with e1 and e2 ﬁnished  by the task ai  cid:9 →[e1, e2 x1, x2]e. Doing so propagates the values of tasks ai,1 and ai,2 to the join point, enabling the computation to continue. The two ﬁnished tasks are removed from the state, and the join point is no longer blocked.  Parallelism is a high-level programming concept that increases efﬁciency by carrying out multiple computations simultaneously when they are mutually independent. Parallelism does not change the meaning of a program, but only how fast it is executed. The cost semantics speciﬁes the number of steps required to execute a program sequentially and with maximal parallelism. A bounded implementation provides a bound on the number of steps when the number of processors is limited, limiting the degree of parallelism that can be realized. This formulation of parallelism was introduced by Blelloch  1990 . The concept of a cost semantics and the idea of a bounded implementation studied here are derived from Blelloch and Greiner  1995, 1996 .  37.6 Notes  Exercises  37.1. Consider extending PPCF with exceptions, as described in Chapter 29, under the assumption that τexn has at least two exception values. Give a sequential and a parallel structural dynamics to parallel let in such a way that determinacy continues to hold. 37.2. Give a matching cost semantics to PPCF extended with exceptions  described in  Exercise 37.1  by inductively deﬁning the following two judgments:  a  e ⇓c v, stating that e evaluates to value v with cost c;  b  e ⇑c v, stating that e raises the value v with cost c. The analog of Theorem 37.6 remains valid for the dynamics. In particular, if e ⇑c v, then both e  cid:20 →w par raise v , where d = dp c , and conversely.  seq raise v , where w = wk c , and e  cid:20 →d  37.3. Extend the P machine to admit exceptions to match your solution to Exercise 37.2. Argue that the revised machine supports a Brent-type validation of the cost semantics. 37.4. Another way to express the dynamics of PPCF enriched with exceptions is by  cid:5  , rewriting par e1; e2; x1.x2.e  into another such parallel binding, par e which implements the correct dynamics to ensure determinacy. Hint: Extend XPCF with sums  Chapter 11 , using them to record the outcome of each parallel sub-  cid:5  2 derived from e2 , and then check the outcomes computation  e  cid:5  derived from e  in such a way to ensure determinacy.  e   cid:5  1 derived from e1, and e   cid:5  2; x   cid:5  1; e   cid:5  1.x   cid:5  2.e   349  Notes  Notes  1 A slightly weaker assumption is that each access may require up to lg p time to account for the overhead of synchronization, but we shall neglect this reﬁnement in the present, simpliﬁed account. 2 The use of join points for each sequential dependency is proﬂigate but aligns the machine with the cost semantics. Realistically, individual tasks manage sequential dependencies without synchro- nization, by using local control stacks as in Chapter 28.  3 Here and elsewhere typing information is omitted from  cid:25 , because it is not relevant to the dynamics. 4 Thus, the local transition given by rule  37.11b  is never applicable; the dynamics of joins will be  described shortly.   38  Futures and Speculations  A future is a computation that is performed before it is value is needed. Like a suspension, a future represents a value that is to be determined later. Unlike a suspension, a future is always evaluated, regardless of whether its value is required. In a sequential setting, futures are of little interest; a future of type τ is just an expression of type τ. In a parallel setting, however, futures are of interest because they provide a means of initiating a parallel computation whose result is not needed until later, by which time it will have been completed.  The prototypical example of the use of futures is to implementing pipelining, a method for overlapping the stages of a multistage computation to the fullest extent possible. Pipelining minimizes the latency caused by one stage waiting for a previous stage to complete by allowing the two stages to execute in parallel until an explicit dependency arises. Ideally, the computation of the result of an earlier stage is ﬁnished by the time a later stage needs it. At worst, the later stage is delayed until the earlier stage completes, incurring what is known as a pipeline stall.  A speculation is a delayed computation whose result might be needed for the overall computation to ﬁnish. The dynamics for speculations executes suspended computations in parallel with the main thread of computation, without regard to whether the value of the speculation is needed by the main thread. If the value of the speculation is needed, then such a dynamics pays off, but if not, the effort to compute it is wasted.  Futures are work efﬁcient in that the overall work done by a computation involving futures is no more than the work done by a sequential execution. Speculations, in contrast, are work inefﬁcient in that speculative execution might be in vain—the overall computation may involve more steps than the work needed to compute the result. For this reason, speculation is a risky strategy for exploiting parallelism. It can make use of available resources, but perhaps only at the expense of doing more work than necessary!  38.1 Futures  The syntax of futures is given by the following grammar:  Typ τ Exp e  ::= fut τ  τ fut ::= fut e  fut e  fsyn e  fsyn e  fcell[a] fcell[a]  future future synchronize indirection   351  38.2 Speculations  The type τ fut is the type of futures of type τ. Futures are introduced by the expression fut e , which schedules e for evaluation and returns a reference to it. Futures are eliminated by the expression fsyn e , which synchronizes with the future referred to by e, returning its value. Indirect references to future values are represented by fcell[a], indicating a future value to be stored ata .  The statics of futures is given by the following rules:  38.1.1 Statics   cid:7   cid:12  e : τ   cid:7   cid:12  fut e  : fut τ    cid:7   cid:12  e : fut τ   cid:7   cid:12  fsyn e  : τ  These rules are unsurprising, because futures add no new capabilities to the language beyond providing an opportunity for parallel evaluation.  38.1.2 Sequential Dynamics  The sequential dynamics of futures is easily deﬁned. Futures are evaluated eagerly; syn- chronization returns the value of the future.  e val  fut e  val e  cid:20 −→ e  cid:5   fut e   cid:20 −→ fut e   cid:5    e  cid:20 −→ e   cid:5   fsyn e   cid:20 −→ fsyn e   cid:5    e val  fsyn fut e    cid:20 −→ e  38.2 Speculations  Under a sequential dynamics futures have little purpose: they introduce a pointless level  of indirection.  The syntax of  non-recursive  speculations is given by the following grammar:1  Typ τ Exp e  ::= spec τ  τ spec ::= spec e  spec e  ssyn e  ssyn e  scell[a] scell[a]  speculation speculate synchronize indirection   38.1a    38.1b    38.2a    38.2b    38.2c    38.2d    352  Futures and Speculations  The type τ spec is the type of speculations of type τ. The introduction form spec e  creates a computation that can be speculatively evaluated, and the elimination form ssyn e  synchronizes with a speculation. A reference to the result of a speculative computation stored at a is written scell[a].  The statics of speculations is given by the following rules:  cid:7   cid:12  spec e  : spec τ    cid:7   cid:12  e : τ  38.2.1 Statics   cid:7   cid:12  e : spec τ   cid:7   cid:12  ssyn e  : τ  Thus, the statics for speculations as given by rules  38.3  is equivalent to the statics for futures given by rules  38.1 .  38.2.2 Sequential Dynamics  The deﬁnition of the sequential dynamics of speculations is like that of futures, except that speculations are values.  spec e  val e  cid:20 −→ e   cid:5   ssyn e   cid:20 −→ ssyn e   cid:5    ssyn spec e    cid:20 −→ e  38.3 Parallel Dynamics  Under a sequential dynamics speculations are simply a re-formulation of suspensions.  Futures are only interesting insofar as they admit a parallel dynamics that allows the computation of the future to go ahead concurrently with some other computation. In this section, we give a parallel dynamics of futures and speculation in which the creation, execution, and synchronization of tasks is made explicit. The parallel dynamics of futures and speculations is identical, except for the termination condition. Whereas futures require that all tasks are completed before termination, speculations may be abandoned before they are completed. For the sake of concision we will give the parallel dynamics of futures, remarking only where alterations are made for the parallel dynamics of speculations.   38.3a    38.3b    38.4a    38.4b    38.4c    353  38.3 Parallel Dynamics  The parallel dynamics of futures relies on a modest extension to the language given in Section 38.1 to introduce names for tasks. Let  cid:25  be a ﬁnite mapping assigning types to names. As mentioned earlier, the expression fcell[a] is a value referring to the outcome of task a. The statics of this expression is given by the following rule:2   cid:7   cid:12  cid:25 ,a∼τ fcell[a] :fut  τ    38.5   Rules  38.1  carry over in the obvious way with  cid:25  recording the types of the task names. States of the parallel dynamics have the form ν  cid:25  { e  cid:16  μ}, where e is the focus of evaluation, and μ records the active parallel futures  or speculations . Formally, μ is a ﬁnite mapping assigning expressions to the task names declared in  cid:25 . A state is well-formed according to the following rule:  cid:12  cid:25  e : τ   ∀a ∈ dom  cid:25     cid:12  cid:25  μ a  :  cid:25  a    38.6   ν  cid:25  { e  cid:16  μ} ok  As discussed in Chapter 35, this rule admits self-referential and mutually referential futures. A more reﬁned condition could as well be given that avoids circularities; we leave this as an exercise for the reader.  The parallel dynamics is divided into two phases, the local phase, which deﬁnes the basic steps of evaluation of an expression, and the global phase, which executes all possible local steps in parallel. The local dynamics of futures is deﬁned by the following rules:3  fcell[a] val cid:25 ,a∼τ  ν  cid:25  { fut e   cid:16  μ}  cid:20 −→loc ν  cid:25 , a ∼ τ { fcell[a]  cid:16  μ ⊗ a  cid:9 → e }   cid:5  }  cid:5  { e  cid:5   cid:16  μ  cid:5    cid:16  μ  cid:5  { fsyn e  cid:5  }  ν  cid:25  { e  cid:16  μ}  cid:20 −→loc ν  cid:25  ν  cid:25  { fsyn e   cid:16  μ}  cid:20 −→loc ν  cid:25  val cid:25 ,a∼τ  ⎧⎪⎨⎪⎩ ν  cid:25 , a ∼ τ { fsyn fcell[a]   cid:16  μ ⊗ a  cid:9 → e   cid:20 −→loc  cid:5   cid:16  μ ⊗ a  cid:9 → e  ν  cid:25 , a ∼ τ { e  ⎫⎪⎬⎪⎭   cid:5  }   cid:5  }  e   cid:5    38.7a    38.7b    38.7c    38.7d   Rule  38.7b  activates a future named a executing the expression e and returns a reference to it. Rule  38.7d  synchronizes with a future whose value has been determined. Note that a local transition always has the form  ν  cid:25  { e  cid:16  μ}  cid:20 −→loc ν  cid:25   cid:25    cid:5  { e   cid:5   cid:16  μ ⊗ μ  cid:5  }   cid:5  is either empty or declares the type of a single symbol, and μ   cid:5  is either empty or  where  cid:25  of the form a  cid:9 → e   cid:5  for some expression e   cid:5 .  A global step of the parallel dynamics consists of at most one local step for the focal expression and one local step for each of up to p futures, where p > 0 is a ﬁxed parameter   354  Futures and Speculations  representing the number of processors.  μ  μ = μ0 ⊗ a1  cid:9 → e1 ⊗ . . . ⊗ an  cid:9 → en ⊗ . . . ⊗ an  cid:9 → e  cid:5  cid:5  = μ0 ⊗ a1  cid:9 → e  cid:5  n ν  cid:25  { e  cid:16  μ}  cid:20 −→0,1  cid:5  }  cid:5   cid:16  μ ⊗ μ { e  cid:5   cid:5  i i  ν  cid:25  { ei  cid:16  μ}  cid:20 −→loc ν  cid:25   cid:25   loc ν  cid:25   cid:25    cid:5  { e   cid:5  1   ∀1 ≤ i ≤ n ≤ p   ν  cid:25  { e  cid:16  μ}   cid:20 −→glo  cid:5   cid:16  μ   cid:5  cid:5  ⊗ μ  { e  ν  cid:25   cid:25    cid:25    cid:5    cid:5  1 . . .  cid:25    cid:5  n   cid:5  ⊗ μ  cid:5  1  ⊗ . . . ⊗ μ  cid:5  n  }  ⎧⎪⎨⎪⎩  ⎫⎪⎬⎪⎭   cid:16  μ ⊗ μ  cid:5  i  }   38.8a   Rule  38.8a  allows the focus expression to take either zero or one step because it might be blocked awaiting the completion of evaluation of a parallel future  or synchronizing with a speculation . The futures allocated by the local steps of execution are consolidated in the result of the global step. We assume without loss of generality that the names of the new futures in each local step are pairwise disjoint so that the combination makes sense. In implementation terms, satisfying this disjointness assumption means that the processors must synchronize their access to memory.  The initial state of a computation, for futures or speculations, is deﬁned by the rule  For futures, a state is ﬁnal only if the focus and all parallel futures have completed evaluation:  ν ∅{ e  cid:16  ∅}initial  e val cid:25  μ val cid:25  ν  cid:25  { e  cid:16  μ} ﬁnal   ∀a ∈ dom  cid:25    μ a  val cid:25   μ val cid:25   e val cid:25   ν  cid:25  { e  cid:16  μ} ﬁnal   38.9    38.10a    38.10b    38.11   For speculations, a state is ﬁnal only if the focus is a value, regardless of whether any other speculations have completed:  All futures must terminate to ensure that the work performed in parallel matches that performed sequentially; no future is created whose value is not needed according to the sequential semantics. In contrast, speculations can be abandoned when their values are not needed.  38.4 Pipelining with Futures  Pipelining is an interesting example of the use of parallel futures. Consider a situation in which a producer builds a list whose elements represent units of work, and a consumer traverses the work list and acts on each element of that list. The elements of the work list   355  38.4 Pipelining with Futures  can be thought of as “instructions” to the consumer, which maps a function over that list to carry out those instructions. An obvious sequential implementation ﬁrst builds the work list, then traverses it to perform the work indicated by the list. This strategy works well provided that the elements of the list can be produced quickly, but if each element needs a lot of computation, it would be preferable to overlap production of the next list element with execution of the previous unit of work, which can be programmed using futures. Let flist be the recursive type rec t is unit +  nat × t fut , whose elements are nil, deﬁned to be fold l ·  cid:24  cid:25  , and cons e1,e2 , deﬁned to be fold r ·  cid:24 e1, fut e2  cid:25  . The producer is a recursive function that generates a value of type flist: fix produce :  nat → nat opt  → nat → flist is  λ f. λ i.  null  cid:9 → nil  case f i  {  just x  cid:9 → cons x, fut  produce f  i+1    }  On each iteration the producer generates a parallel future to produce the tail. The future continues to execute after the producer returns so that its evaluation overlaps with subsequent computation.  The consumer folds an operation over the work list as follows:  fix consume :   nat×nat →nat  → nat → flist → nat is  λ g. λ a. λ xs.  case xs { nil  cid:9 → a  cons  x, xs   cid:9 → consume g  g  x, a    fsyn xs  }  The consumer synchronizes with the tail of the work list just at the point where it makes a recursive call and hence needs the head element of the tail to continue processing. At this point, the consumer will block, if necessary, to await computation of the tail before continuing the recursion.  Speculations arise naturally in lazy languages. But although they provide opportunities for parallelism, they are not, in general, work efﬁcient: a speculation might be evaluated even though its value is never needed. An alternative is to combine suspensions  see Chapter 36  with futures so that the programmer may specify which suspensions ought to be evaluated in parallel. The notion of a spark is designed to achieve this. A spark evaluates a computation in parallel only for its effect on suspensions that are likely to be needed later. Speciﬁcally, we may deﬁne  spark e1; e2   cid:2  letfut be force e1  in e2,  where e1 : τ1 susp and e2 : τ2.4 The expression force e1  is evaluated in parallel, forcing the evaluation of e1, in hopes that it will have completed evaluation before its value is needed by e2.   356  Futures and Speculations  As an example, consider the type strm of streams of numbers deﬁned by the recursive type rec t is  unit +  nat × t   susp. Elements of this type are suspended computations that, when forced, either signals the end of stream, or produces a number and another such stream. Suppose that s is such a stream, and assume that we know, for reasons of its construction, that it is ﬁnite. We wish to compute map f   s  for some function f , and to overlap this computation with the production of the stream elements. We will make use of a function mapforce that forces successive elements of the input stream, but yields no useful output. The computation  letfut be map force  s  in map f   s   forces the elements of the stream in parallel with the computation of map f   s , with the intention that all suspensions in s are forced before their values are needed by the main computation.  Futures were introduced by Friedman and Wise  1976 , and featured in the MultiLisp language  Halstead, 1985  for parallel programming. A similar concept is proposed by Arvind et al.  1986  under the name “I-structures.” The formulation given here is derived from Greiner and Blelloch  1999 . Sparks were introduced by Trinder et al.  1998 .  38.1. Use futures to deﬁne letfut x be e1 in e2, a parallel let in which e2 is evaluated in  parallel with e1 up to the point that e2 needs the value of x.  38.2. Use futures to encode binary nested parallelism by giving a deﬁnition of  par e1; e2; x1.x2.e . Hint: Only one future is needed if you are careful.  38.5 Notes  Exercises  Notes  1 We conﬁne ourselves to the non-recursive case to ease the comparison with futures. 2 A similar rule applies to scell[a] in the case of speculations. 3 These rules are augmented by a reformulation of the dynamics of the other constructs of the  language phrased in terms of the present notion of state.  4 The expression evaluates e1 simultaneously with e2, up to the point that the value of x is needed.  Its deﬁnition in terms of futures is the subject of Exercise 38.1.   P A R T XVI  Concurrency and  Distribution    39  Process Calculus  So far we have studied the statics and dynamics of programs in isolation, without regard to their interaction with each other or the world. But to extend this analysis to even the most rudimentary forms of input and output requires that we consider external agents that interact with the program. After all, the purpose of a computer is, ultimately, to interact with a person!  To extend our investigations to interactive systems, we develop a small language, called PiC, which is derived from a variety of similar formalisms, called process calculi, that give an abstract formulation of interaction among independent agents. The development will be carried out in stages, starting with simple action models, then extending to interacting concurrent processes, and ﬁnally to synchronous and asynchronous communication. The calculus consists of two main syntactic categories, processes and events. The basic form of process is one that awaits the arrival of an event. Processes are formed by concurrent composition, replication, and declaration of a channel. The basic forms of event are signaling on a channel and querying a channel; these are later generalized to sending and receiving data on a channel. Events are formed from send and receive events by ﬁnite non-deterministic choice.  39.1 Actions and Events  Concurrent interaction is based on events, which specify the actions that a process can take. Two processes interact by taking two complementary actions, a signal and a query on a channel. The processes synchronize when one signals on a channel that the other is querying, after which they continue to interact with other processes.  To begin with, we will focus on sequential processes, which simply await the arrival of  one of several possible actions, known as an event.  Proc P ::= await E  Evt  E ::= null  $ E 0  or E1; E2  E1 + E2 que[a] P   sig[a] P    ?a;P !a;P  synchronize null choice query signal  The variable a ranges over symbols serving as channels that mediate communication among the processes.   360  Process Calculus  We will not distinguish between events that differ only up to structural congruence,  which is deﬁned to be the strongest equivalence relation closed under these rules:  E ≡ E  cid:5  $ E ≡ $ E   cid:5   1 E2 ≡ E  cid:5   cid:5  2 + E  cid:5  2   cid:5  1  E1 ≡ E E1 + E2 ≡ E P ≡ P  cid:5  ?a;P ≡ ?a;P  cid:5  !a;P ≡ !a;P  P ≡ P   cid:5    cid:5   E + 0 ≡ E  E1 + E2 ≡ E2 + E1   39.1a    39.1b    39.1c    39.1d    39.1e    39.1f    39.1g   E1 +  E2 + E3  ≡  E1 + E2  + E3  Imposing structural congruence on sequential processes enables us to think of an event as having the form  !a;P1 + ··· + ?a;Q1 + ···  consisting of a sum of signal and query events, with the sum of no events being the null event 0.  An illustrative example of Milner’s is a simple vending machine that may take in a 2p coin, then optionally either allow a request for a cup of tea, or take another 2p coin, then allow a request for a cup of coffee.  V = $  ?2p;$  !tea;V + ?2p;$  !cof;V       39.2   As the example indicates, we allow recursive deﬁnitions of processes, with the understand- ing that a deﬁned identiﬁer may always be replaced with its deﬁnition wherever it occurs.  Later we will show how to avoid reliance on recursive deﬁnitions.   Because the computation occurring within a process is suppressed, sequential processes have no dynamics on their own, but only through their interaction with other processes. For the vending machine to work, there must be another process  you  who initiates the events expected by the machine, causing both your state  the coins in your pocket  and its state  as just described  to change as a result.   361  39.2 Interaction  39.2 Interaction  Processes become interesting when they are allowed to interact with one another to achieve a common goal. To account for interaction, we enrich the language of processes with concurrent composition:  Proc P ::= await E   $ E 1  stop conc P1; P2  P1 ⊗ P2  synchronize inert composition  The process 1 represents the inert process, and the process P1⊗P2 represents the concurrent composition of P1 and P2. We may identify 1 with $ 0, the process that awaits the event that will never occur, but we prefer to treat the inert process as a primitive concept.  We will identify processes up to structural congruence, the strongest equivalence relation  closed under these rules:  P ⊗ 1 ≡ P  P1 ⊗ P2 ≡ P2 ⊗ P1   39.3a    39.3b    39.3c    39.3d   P1 ⊗  P2 ⊗ P3  ≡  P1 ⊗ P2  ⊗ P3  1 P2 ≡ P  cid:5   cid:5  2 ⊗ P  cid:5  2 Up to structural congruence every process has the form $ E1 ⊗ . . . ⊗ $ En  P1 ≡ P P1 ⊗ P2 ≡ P   cid:5  1  for some n ≥ 0, it being understood that when n = 0 this stands for the null process 1.   cid:5  states that the process P evolves to the process P α cid:20 −→ P  Interaction between processes consists of synchronization of two complementary actions. The dynamics of interaction is deﬁned by two forms of judgment. The transition judgment P  cid:20 −→ P  cid:5  as a result of a single step  cid:5 , where α is an action, states of computation. The family of transition judgments, P  cid:5  as long as the action α is permissible in that the process P may evolve to the process P the context in which the transition occurs. As a notational convenience, we often regard the unlabeled transition to be the labeled transition corresponding to the special silent action.  The possible actions are given by the following grammar:  Act α ::= que[a] sig[a] sil  a ? query signal a ! silent ε   362  Process Calculus  The query action a ? and the signal action a ! are complementary, and the silent action ε, is self-complementary. We deﬁne the complementary action to α to be the action α given by the equations a ? = a !, a ! = a ?, and ε = ε.  $  !a;P + E  a !   cid:20 −→ P   cid:20 −→ P $  ?a;P + E  a ?  cid:5  1  cid:5  1  α cid:20 −→ P α cid:20 −→ P  P1 P1 ⊗ P2 α cid:20 −→ P   cid:5  1 P2 P1 P1 ⊗ P2  cid:20 −→ P  ⊗ P2 α cid:20 −→ P  cid:5  2 ⊗ P  cid:5  1   cid:5  2   39.4a    39.4b    39.4c    39.4d   Rules  39.4a  and  39.4b  specify that any of the events on which a process is syn- chronizing may occur. Rule  39.4d  synchronizes two processes that take complementary actions.  As an example, let us consider the vending machine V , given by Equation  39.2 ,  interacting with the user process U deﬁned as follows:  U = $ !2p;$ !2p;$ ?cof;1.  Here is a trace of the interaction between V and U:  V ⊗ U  cid:20 −→ $  !tea;V + ?2p;$ !cof;V   ⊗ $ !2p;$ ?cof;1   cid:20 −→ $ !cof;V ⊗ $ ?cof;1  cid:20 −→ V  These steps are justiﬁed by the following pairs of labeled transitions:  2p !   cid:20 −→ U  cid:20 −−→ V  2p ?   cid:5  = $ !2p;$ ?cof;1  cid:5  = $  !tea;V + ?2p;$ !cof;V    U  V   cid:20 −→ U  cid:5  2p !  cid:20 −−→ V  cid:5  2p ?   cid:5  cid:5  = $ ?cof;1  cid:5  cid:5  = $ !cof;V  U  V   cid:20 −−→ 1  cid:5  cid:5  cof ?  cid:20 −−→ V  cid:5  cid:5  cof !  U  V  We have suppressed uses of structural congruence in the foregoing derivation to avoid clutter, but it is important to see its role in managing the non-deterministic choice of events by a process.   363  39.3 Replication  39.3 Replication  Some presentations of process calculi forego reliance on deﬁning equations for processes in favor of areplication construct, which we write as ∗ P . This process stands for as many concurrently executing copies of P as needed. Implicit replication can be expressed by the structural congruence  Understood as a principle of structural congruence, this rule hides the steps of process creation and gives no hint as to how often it should be applied. We could alternatively build replication into the dynamics to model the details of replication more closely:  ∗ P ≡ P ⊗ ∗ P .  ∗ P  cid:20 −→ P ⊗ ∗ P .   39.5    39.6   Because there is no constraint on the use of this rule, it can at any time create a new copy of the replicated process P . It is also possible to tie its use to send and receive events so that replication is causal, rather than spontaneous.  So far we have used recursive process deﬁnitions to deﬁne processes that interact repeat- edly according to some protocol. Rather than take recursive deﬁnition as a primitive notion, we may instead use replication to model repetition. We do so by introducing an “activator” process that is used to cause the replication. Consider the recursive deﬁnition X = P  X , where P is a process expression that may refer to itself as X. Such a self-referential process can be simulated by deﬁning the activator process  A = ∗ $  ?a;P  $  !a;1   ,  in which we have replaced occurrences of X within P by an initiator process that signals the event a to the activator. Note that the activator A is structurally congruent to the process  cid:5  ⊗ A, where A A   cid:5  is the process  $  ?a;P  $  !a;1   .  To start process P , we concurrently compose the activator A with an initiator process, $  !a;1 . Note that  A ⊗ $  !a;1   cid:20 −→ A ⊗ P  $ !a;1 ,  which starts the process P while maintaining a running copy of the activator, A.  As an example, let us consider Milner’s vending machine, written using replication,  instead of recursive process deﬁnition: V0 = $  !v;1  V1 = ∗ $  ?v;V2  V2 = $  ?2p;$  !tea;V0 + ?2p;$  !cof;V0      39.7   39.8   39.9   The process V1 is a replicated server that awaits a signal on channel v to create another instance of the vending machine. The recursive calls are replaced by signals along v to   364  Process Calculus  re-start the machine. The original machine V is simulated by the concurrent composition V0 ⊗ V1. This example motivates replacing spontaneous replication by replicated synchronization,  which is deﬁned by the following rules:  ∗ $  !a;P + E  a !   cid:20 −→ P ⊗ ∗ $  !a;P + E    39.10a   ∗ $  ?a;P + E  a ?   39.10b  The process ∗ $  E  is to be regarded not as a composition of replication and synchroniza- tion, but as the inseparable combination of these two constructs. The advantage is that the replication occurs only as needed, precisely when a synchronization with another process is possible, avoiding the need “guess” when replication is needed.   cid:20 −→ P ⊗ ∗ $  ?a;P + E   39.4 Allocating Channels  It is often useful  particularly once we have introduced inter-process communication  to introduce new channels within a process, and not assume that all channels of interaction are given a priori. To allow for this, we enrich the syntax of processes with channel declaration:  Proc P ::= new a.P    ν a.P new channel  The channel a is bound within the process P . To simplify notation, we sometimes write ν a1, . . . , ak.P for the iterated declaration ν a1.. . . ν ak.P .  We then extend structural congruence with the following rules:   cid:5   P =α P P ≡ P  cid:5  P ≡ P   cid:5   a  ∈ P2  ν a.P ≡ ν a.P   cid:5    ν a.P1  ⊗ P2 ≡ ν a. P1 ⊗ P2   ν a.ν b.P ≡ ν b.ν a.P   a  ∈ P   ν a.P ≡ P   39.11a    39.11b    39.11c    39.11d    39.11e   Rule  39.11c , called scope extrusion, will be especially important in Section 39.6. Rule  39.11e  states that channels are de-allocated once they are no longer in use.   365  39.4 Allocating Channels  To account for the scopes of channels, we extend the statics of PiC with a signature  cid:25  comprising a ﬁnite set of active channels. The judgment  cid:12  cid:25  P proc states that a process P is well-formed relative to the channels declared in the signature  cid:25 .   39.12d  The foregoing rules make use of an auxiliary judgment,  cid:12  cid:25  E event, stating that E is a well-formed event relative to  cid:25 .  The judgment  cid:12  cid:25  α action states that α is a well-formed action relative to  cid:25 :  The dynamics of the current fragment of PiC is correspondingly generalized to keep  cid:5   cid:5  states that P transitions to P track of the set of active channels. The judgment P with action α relative to channels  cid:25 . The dynamics of this extension is obtained by indexing the transitions by the signature, and adding a rule for channel declaration.  α cid:20 −→  P   cid:25    cid:12  cid:25  1 proc   cid:12  cid:25  P1 proc  cid:12  cid:25  P2 proc   cid:12  cid:25  P1 ⊗ P2 proc   cid:12  cid:25  E event  cid:12  cid:25  $ E proc  cid:12  cid:25 ,a P proc  cid:12  cid:25  ν a.P proc   cid:12  cid:25  0 event  cid:12  cid:25 ,a P proc  cid:12  cid:25 ,a ?a;P event  cid:12  cid:25 ,a P proc  cid:12  cid:25 ,a !a;P event   cid:12  cid:25  E1 event  cid:12  cid:25  E2 event   cid:12  cid:25  E1 + E2 event   cid:12  cid:25 ,a a ? action   cid:12  cid:25 ,a a ! action   cid:12  cid:25  ε action  $  !a;P + E    cid:20 −−→  a !   cid:25 ,a  P   cid:20 −−→ $  ?a;P + E  a ?  P   cid:25 ,a   39.12a    39.12b    39.12c    39.13a    39.13b    39.13c    39.13d    39.14a    39.14b    39.14c    39.15a    39.15b    366  Process Calculus  P1 P1 ⊗ P2  α cid:20 −→ α cid:20 −→   cid:25    cid:25    cid:5  P 1  cid:5  P 1  ⊗ P2  α cid:20 −→ ⊗ P   cid:5  P 2  cid:5  2   cid:25    cid:25   P  α cid:20 −→   cid:5  P1 1 P2 P1 ⊗ P2  cid:20 −→ α cid:20 −−→   cid:25    cid:5  P 1  P  P   cid:25 ,a   cid:5   cid:12  cid:25  α action α cid:20 −→  ν a.P   cid:5   ν a.P   cid:25    39.15c    39.15d    39.15e   Rule  39.15e  ensures that no process may interact with ν a.P along the channel a by using the identiﬁcation convention to choose a  ∈  cid:25 .  Consider again the deﬁnition of the vending machine using replication instead of re- cursion. The channel v used to initialize the machine is private to the machine itself. The process V = ν v. V0 ⊗ V1  declares a new channel v for use by V0 and V1, which are deﬁned essentially as before. The interaction of the user process U with V begins as follows:   ν v. V0 ⊗ V1   ⊗ U  cid:20 −→   ν v.V2  ⊗ U ≡ ν v. V2 ⊗ U .   cid:25   The interaction continues within the scope of the declaration, which ensures that v does not occur within U.  39.5 Communication  Synchronization coordinates the execution of two processes that take the complementary actions of signaling and querying a common channel. Synchronous communication gen- eralizes synchronization to pass a data value betwen two synchronizing processes, one of which is the sender of the value and the other its receiver. The type of the data is immaterial to the communication.  To account for interprocess communication, we enrich the language of processes to include variables, as well aschannels , in the formalism. Variables range, as always, over types, and are given meaning by substitution. Channels, on the other hand, are assigned types that classify the data carried on that channel and are given meaning by send and receive events that generalize the signal and query events considered in Section 39.2. The abstract syntax of communication events is given by the following grammar:  Evt E ::= snd[a] e; P   rcv[a] x.P    ! a e ; P   ? a x.P    send receive  The event rcv[a] x.P   represents the receipt of a value x on the channel a, passing x to the process P . The variable x is bound within P . The event snd[a] e; P   represents the transmission of e on a and continuing with P .   367  39.5 Communication  We modify the syntax of declarations to account for the type of value sent on a channel.  Proc P ::= new{τ} a.P    ν a ∼ τ .P typed channel  The process new[τ] a.P   introduces a new channel a with associated type τ for use within the process P . The channel a is bound within P . The statics is extended to account for the type of a channel. The judgment  cid:7   cid:12  cid:25  P proc states that P is a well-formed process involving the channels declared in  cid:25  and the variables declared in  cid:7 . It is inductively deﬁned by the following rules, wherein we assume that the typing judgment  cid:7   cid:12  cid:25  e : τ is given separately.   39.16d  Rules  39.16  make use of the auxiliary judgment  cid:7   cid:12  cid:25  E event, stating that E is a well-formed event relative to  cid:7  and  cid:25 , which is deﬁned as follows:   cid:7   cid:12  cid:25  1 proc   cid:7   cid:12  cid:25  P1 proc  cid:7   cid:12  cid:25  P2 proc   cid:7   cid:12  cid:25  P1 ⊗ P2 proc  cid:7   cid:12  cid:25 ,a∼τ P proc  cid:7   cid:12  cid:25  ν a ∼ τ .P proc   cid:7   cid:12  cid:25  E event  cid:7   cid:12  cid:25  $ E proc   cid:7   cid:12  cid:25  0 event   cid:7   cid:12  cid:25  E1 event  cid:7   cid:12  cid:25  E2 event   cid:7   cid:12  cid:25  E1 + E2 event  cid:7 , x : τ  cid:12  cid:25 ,a∼τ P proc  cid:7   cid:12  cid:25 ,a∼τ ? a x.P   event   cid:7   cid:12  cid:25 ,a∼τ e : τ  cid:7   cid:12  cid:25 ,a∼τ P proc   cid:7   cid:12  cid:25 ,a∼τ ! a e ; P   event   39.16a    39.16b    39.16c    39.17a    39.17b    39.17c    39.17d   Rule  39.17d  makes use of a typing judgment for expressions that ensures that the type of a channel is respected by communication.  The dynamics of communication extends that of synchronization by enriching send and  receive actions with the value sent or received.  Act α ::= rcv[a] e  snd[a] e  sil  a ? e a ! e ε  receive send silent  Complementarity is deﬁned as before, by switching the orientation of an action: a ? e = a!e, a ! e = a ? e, and ε = ε.   368  Process Calculus  The statics ensures that the expression associated with these actions is a value of a type  suitable for the channel:  The dynamics is deﬁned by replacing the synchronization rules  39.15a  and  39.15b   with the following communication rules:   cid:12  cid:25 ,a∼τ e : τ  e val cid:25 ,a∼τ   cid:12  cid:25 ,a∼τ a ! e action   cid:12  cid:25 ,a∼τ e : τ  e val cid:25 ,a∼τ  cid:12  cid:25 ,a∼τ a ? e action   cid:12  cid:25  ε action  e  cid:20 −−−→  cid:25 ,a∼τ $  ! a e ; P   + E   cid:20 −−−→  cid:25 ,a∼τ   cid:5   e  $  ! a e   cid:5  ; P   + E   e val cid:25 ,a∼τ $  ! a e ; P   + E    cid:20 −−−→ a!e  cid:25 ,a∼τ  P  $  ? a x.P   + E   e val cid:25 ,a∼τ  cid:20 −−−→ a?e  cid:25 ,a∼τ  [e x]P   39.18a    39.18b    39.18c    39.19a    39.19b    39.19c   Rule  39.19c  is non-deterministic in that it “guesses” the value e to be received along channel a. Rules  39.19  make reference to the dynamics of expressions, which is left unspeciﬁed because nothing depends on it.  Using synchronous communication, both the sender and the receiver of a message are blocked until the interaction is completed. Therefore the sender must be notiﬁed whenever a message is received, which means that there must be an implicit reply channel from receiver to sender that carries the notiﬁcation. This means that synchronous communication can be decomposed into a simpler asynchronous send operation, which sends a message on a channel without waiting for its receipt, together with channel passing to send an acknowledgment channel along with the message data.  Asynchronous communication is deﬁned by removing the synchronous send event from the process calculus and adding a new form of process that simply sends a message on a channel. The syntax of asynchronous send is as follows:  Proc P ::= asnd[a] e   ! a e   send  The process asnd[a] e  sends the message e on channel a and then terminates immediately. Without the synchronous send event, every event is, up to structural congruence, a choice of zero or more read events. The statics of asynchronous send is given by the following rule:   cid:7   cid:12  cid:25 ,a∼τ e : τ   cid:7   cid:12  cid:25 ,a∼τ ! a e  proc   39.20    369  39.6 Channel Passing  The dynamics is given similarly:  e val cid:25   cid:20 −→ ! a e  a!e  1   cid:25    39.21   The rule for communication remains unchanged. A pending asynchronous send is essen- tially a buffer holding the value to be sent once a receiver is available.  39.6 Channel Passing  An interesting case of interprocess communication arises when one process passes channel reference, a form of value, to another along a common channel. The receiving process need not have any direct access to the channel referred to by the reference. It merely operates on it using send and receive operations that act on channel references instead of ﬁxed channels. Doing so allows for new patterns of communication to be established among processes. For example, two processes, P and Q, may share a channel a along which they may send and receive messages. If the scope of a is conﬁned to these processes, then no other process R may communicate on that channel; it is, in effect, a private channel between P and Q.  The following process expression illustrates such a situation:   ν a ∼ τ . P ⊗ Q   ⊗ R.  The process R is excluded from the scope of the channel a, which however includes both P and Q. The processes P and Q may communicate with each other on channel a, butR has no access to this channel. If P and Q wish to allow R to communicate along a, they may do so by sending a reference to a to R along some channel b known to all three processes. Thus, we have the following situation:  ν b ∼ τ chan.  ν a ∼ τ . P ⊗ Q   ⊗ R .  Assuming that P initiates the inclusion of R into its communication with Q along a, it has  cid:5   . The the form $  ! b & a ; P system of processes therefore has the form   cid:5   . The process R correspondingly takes the form $  ? b x.R  ν b ∼ τ chan. ν a ∼ τ . $  ! b & a ; P   cid:5      ⊗ Q  ⊗ $  ? b x.R   cid:5      .  Sending a reference to a to R would seem to violate the scope of a. The communication of the reference would seem to escape the scope of the referenced channel, which would be nonsensical. It is here that the concept of scope extrusion, introduced in Section 39.4 comes into play:  ν b ∼ τ chan.ν a ∼ τ . $  ! b & a ; P   cid:5      ⊗ Q ⊗ $  ? b x.R   cid:5      .   370  Process Calculus  The scope of a expands to encompass R, preparing the ground for communication between P and R, resulting in  ν b ∼ τ chan.ν a ∼ τ . P   cid:5  ⊗ Q ⊗ [& a x]R   cid:5    .  The reference to the channel a is substituted for the variable x within R  The process R may now communicate with P and Q by sending and receiving messages along the channel reference substituted for the variable x. For this, we use dynamic forms of send and receive in which the channel on which to communicate is determined by evaluation of an expression. For example, to send a message e of type τ along the channel referred to by x, the process R   cid:5 .  Similarly, to receive along the referenced channel, the process R   cid:5  would have the form   cid:5  would have the form  cid:5  cid:5    .  $  !!  x ; e ; R  $  ??  x ; y.R    .   cid:5  cid:5   In both cases, the dynamic communication forms evolve to the static communication forms once the referenced channel has been determined.  The syntax of channel reference types is given by the following grammar:  Typ τ Exp e  ::= chan τ  ::= chref[a]  Evt E ::= sndref e1; e2; P   rcvref e; x.P    τ chan & a !!  e1 ; e2 ; P   ??  e ; x.P    channel type reference send receive  The events sndref e1; e2; P   and rcvref e; x.P   are dynamic versions of the events snd[a] e; P   and rcv[a] x.P   in which the channel reference is determined dynamically by evaluation of an expression.  The statics of channel references is given by the following rules:  Because channel references are forms of expression, events must be evaluated to deter-  mine the channel to which they refer.   cid:7   cid:12  cid:25 ,a∼τ & a : τ chan   cid:7   cid:12  cid:25  e1 : τ chan  cid:7   cid:12  cid:25  e2 : τ  cid:7   cid:12  cid:25  P proc   cid:7   cid:12  cid:25  !!  e1 ; e2 ; P   event   cid:7   cid:12  cid:25  e : τ chan  cid:7 , x : τ  cid:12  cid:25  P proc   cid:7   cid:12  cid:25  ??  e ; x.P   event  E  cid:20 −−−→  cid:25 ,a∼τ $  E   cid:20 −−−→  cid:25 ,a∼τ   cid:5   E   cid:5    $  E  e val cid:25 ,a∼τ $  !!  & a ; e ; P   + E   cid:20 −−−→  cid:25 ,a∼τ  $  ! a e ; P   + E    39.22a    39.22b    39.22c    39.23a    39.23b    371  39.7 Universality  e val cid:25 ,a∼τ $  ??  & a ; x.P   + E   cid:20 −−−→  cid:25 ,a∼τ  $  ? a x.P   + E    39.23c   Events must similarly be evaluated; see Chapter 40 for guidance on how to formulate such a dynamics.  39.7 Universality  The process calculus PiC developed in this chapter is universal in the sense that the untyped λ-calculus can be encoded within it. Consequently, via this encoding, the same functions on the natural numbers are deﬁnable in PiC as are deﬁnable in  cid:2  and hence, by Church’s Law, any known programming language. This claim is remarkable because PiC has so few capabilities that one might suspect that it is too weak to be a useful programming language. The key to seeing that PiC is universal is to note that communication allows processes to send and receive values of an arbitrary type. So long as recursive and channel reference types are available, then it is a purely technical matter to show that  cid:2  is encodable within it. After all, what makes  cid:2  universal is that its one type is a recursive type  see Chapter 21 , so it is natural to guess that with messages of recursive type available then PiC would be universal. And indeed it is.  To prove universality it sufﬁces to give an encoding of the untyped λ-calculus under a call-by-name dynamics into PiC. To motivate the translation, consider a call-by-name stack machine for evaluating λ-terms. A stack is a composition of frames, each of which have the form − e2  corresponding to the evaluation of the function part of an application. A stack is represented in PiC by a reference to a channel that expects an expression  the function to apply  and another channel reference  the stack on which to evaluate the result of the application . A λ-term is represented by a reference to a channel that expects a stack on which the expression is evaluated.  Let κ be the type of continuations. It should be isomorphic to the type of references to channels that carry a pair of values, an argument, whose type is a reference to a channel carrying a continuation, and another continuation to which to deliver the result of the application. Thus, we seek to have the following type isomorphism:  ∼=  κ chan × κ  chan.  κ  The solution is a recursive type, as described in Chapter 20. Thus, just as for  cid:2  itself, the key to the universality of PiC is the use of the recursive type κ.  We now give the translation of  cid:2  into PiC. For the sake of the induction, the translation of a  cid:2  expression u is given relative to a variable of type κ, representing the continuation to which the result will be sent. The representation is given by the following equations:  x @ k  cid:2  !!  x ; k   λ  x  u @ k  cid:2  $ ??  unfold k  ;  cid:24 x, k u1 u2  @ k  cid:2    cid:5  cid:25 .u @ k   cid:5      ν a1 ∼ κ chan × κ. u1 @ fold & a1   ⊗ ν a ∼ κ.∗ $ ? a k2.u2 @ k2  ⊗ ! a1  cid:24 & a, k cid:25     372  Process Calculus  We use pattern matching on pairs for the sake of readability. Only asynchronous sends are needed.  The use of static and dynamic communication operations in the translation merits close consideration. The call site of a λ-term is determined dynamically; we cannot predict at translation time the continuation of the term. In particular, the binding of a variable can be used at several call sites, corresponding to uses of that variable. On the other hand, the channel associated to an argument is determined statically. The server associated to the variable listens on a statically determined channel for a continuation, which is determined dynamically.  As a check on the correctness of the representation, consider the following derivation:  λ  x  x  y  @ k  cid:20 −→∗  ν a1 ∼ τ . $ ? a1  cid:24 x, k   cid:5  cid:25 .!!  x ; k   cid:5       ⊗ ν a ∼ κ.∗ $ ? a k2.!!  y ; k2   ⊗ ! a1  cid:24 & a, k cid:25     cid:20 −→∗  cid:20 −→∗  ν a ∼ κ.∗ $ ? a k2.!!  y ; k2   ⊗ ! a k  ν a ∼ κ.∗ $ ? a k2.!!  y ; k2   ⊗ !!  y ; k   Apart from the idle server process listening on channel a, this is just the translation y @ k.  Using the methods to be developed in detail in Chapter 49, we may show that the result of the computation step is “bisimilar” to the translation of y @ k, and hence equivalent to it for all purposes.   39.8 Notes  Process calculi as models of concurrency and interaction were introduced and extensively developed by Hoare  1978  and Milner  1999 . Milner’s original formulation, CCS, was introduced to model pure synchronization, whereas Hoare’s, CSP, included value-passing. CCS was extended to become the π-calculus  Milner, 1999 , which includes channel- passing. Dozens upon dozens of variations and extensions of CSP, CCS, and the π-calculus have been considered in the literature and continue to be a subject of intensive study.  See Engberg and Nielsen  2000  for an account of some of the critical developments in the area.   The process calculus considered here is derived from the π-calculus as presented in Milner  1999 . The overall line of development, and the vending machine example and the λ-calculus encoding, are adapted from Milner  1999 . The distinction drawn here between static and dynamic events  that is, those that are given syntactically versus those that arise by evaluation  ﬂows naturally from the distinction between variables and channels. It is possible to formulate PiC using only channel references, suppressing any mention of channels themselves. The present formulation coheres with the formulation of assignables and assignable references in Chapters 34 and 35. The concept of dynamic events is taken one step further in Concurrent ML  Reppy, 1999 , wherein events are values of an event type  see also Chapter 40 .   373  Exercises  Exercises  39.1. Booleans can be represented in the process calculus similarly to the way in they are represented in  cid:2   Chapter 21 , called the Milner booleans. Speciﬁcally, a boolean can be represented by a channel carrying a pair of channel references that are signaled  sent a trivial value  to indicate whether the boolean is true or false. Give a deﬁnition of processes corresponding to truth, falsehood, and conditional branch between two processes, each parameterized by a channel a representing the boolean. 39.2. Deﬁne the sequential composition P ; Q of processes P and Q in PiC. Hint: Deﬁne an auxiliary translation P * p, where p is a channel value, such that P * p behaves like P , but sends the unit value on p just before termination.  39.3. Consider again an RS latch, which was the subject of Exercises 11.4, 15.7, and 20.3. Implement an RS latch as a process L i, o  that takes a pair of booleans as input on channel i, representing the R and S inputs to the latch, and outputs a pair of booleans on channel o, representing the outputs Q and Z, with Q being the output of interest. Consider the following input processes:  I i   cid:2  ∗ ! i  cid:24 false, false cid:25    Ireset i   cid:2  ! i  cid:24 true, false cid:25   ; Ireset Iset i   cid:2  ! i  cid:24 true, false cid:25   ; Iset  The ﬁrst quiesces the inputs by holding both R and S at false forever. The second asserts the R input  only , and then quiesces; the third asserts the S input  only , and then quiesces. Show that the process L i, o  ⊗ Ireset i  evolves to a process capable of taking the action o ! cid:24 false, false cid:25  and is then forever capable of evolving to a process taking the same action. Similarly, show that L i, o  ⊗ Iset i  evolves to a process capable of taking the action o ! cid:24 true, false cid:25 , and is then forever capable of evolving to a process taking the same action. 39.4. Some versions of process calculus to note have a distinction between events and pro- cesses. They instead consider the non-deterministic choice P1 + P2 of two processes, which is deﬁned by the following silent transition rules:  P1 + P2  cid:20 −→  P1  P1 + P2  cid:20 −→  P2   cid:25    cid:25    39.24a    39.24b   Thus, P1 + P2 may spontaneously evolve into either P1 or P2, without interact- ing with any other process. Show that non-deterministic choice is deﬁnable in the asynchronous process calculus in such a way that the given transitions are possible.   374  Process Calculus  39.5. In the asynchronous process, calculus events are ﬁnite sums of inputs, called an input  choice, of the form  The behavior of the process  ? a1 x1.P1  + ··· + ? an xn.Pn .  P  cid:2  $  ? a1 x1.P1  + ··· + ? an xn.Pn    is tantalizingly close to that of the concurrent composition of processes receiving on a single channel,  Q  cid:2  $ ? a1 x1.P1  ⊗ . . . ⊗ $ ? an xn.Pn .  The processes P and Q are similar in that both may synchronize with a concurrently executing sender on any of the speciﬁed receive channels. They are different in that P abandons the other receives once one has synchronized with a sender, whereas Q leaves the other choices available for further synchronization. So a receive-only choice can be deﬁned in terms of the concurrent composition of single-choice receives by arranging that the other choices are deactivated once one has been chosen. Show that this is the case. Hint: Associate a Milner boolean  Exercise 39.1  with each choice group that limits synchronization to at most one sender.  39.6. The polyadic π-calculus is a process calculus in which all channels are constrained  to carry values of the recursive type π satisfying the isomorphism  Thus, a message value has the form   cid:16   n∈N  ∼=  π   cid:26  cid:27    cid:28   cid:25  π chan × . . . × π chan  cid:25   cid:28  n ·  cid:24 & a1, . . . , & an   cid:26  cid:27    cid:25   n  .  n  in which the tag n indicates the size of the tuple of channel references associated with it. Show that the encoding of  cid:2  given in Section 39.7 can be given using only channels of type π, proving the universality of the polyadic π-calculus.   40  Concurrent Algol  In this chapter, we integrate concurrency into the framework of Modernized Algol described in Chapter 34. The resulting language, called Concurrent Algol, or CA, illustrates the integration of the mechanisms of the process calculus described in Chapter 39 into a practical programming language. To avoid distracting complications, we drop assignables from Modernized Algol entirely.  There is no loss of generality, however, because free assignables are deﬁnable in Concurrent Algol using processes as cells.   The process calculus described in Chapter 39 is intended as a self-standing model of concurrent computation. When viewed in the context of a programming language, however, it is possible to streamline the machinery to take full advantage of types that are in any case required for other purposes. In particular the concept of a channel, which features prominently in Chapter 39, is identiﬁed with the concept of a dynamic class as described in Chapter 33. More precisely, we take broadcast communication of dynamically classiﬁed values as the basic synchronization mechanism of the language. Being dynamically classi- ﬁed, messages consist of a payload tagged with a class, or channel. The type of the channel determines the type of the payload. Importantly, only those processes that have access to the channel may decode the message; all others must treat it as inscrutable data that can be passed around but not examined. In this way, we can model not only the mechanisms described in Chapter 39 but also formulate an abstract account of encryption and decryption in a network using the methods described in Chapter 39.  Concurrent Algol features a modal separation between commands and expressions like in Modernized Algol. It is also possible to combine these two levels  so as to allow benign concurrency effects , but we do not develop this approach in detail here.  40.1 Concurrent Algol  The syntax of CA is obtained by removing assignables from MA, and adding a syntactic level of processes to represent the global state of a program:  τ e  ::= cmd τ  commands τ cmd Typ ::= cmd m  command cmd m Exp Cmd m ::= ret e return ret e bnd x ← e ; m sequence Proc p ::= stop 1 run m  run m  conc p1; p2  p1 ⊗ p2 ν a ∼ τ .p new[τ] a.p   idle atomic concurrent new channel  bnd e; x.m    376  Concurrent Algol  The process run m  is an atomic process executing the command m. The other forms of process are adapted from Chapter 39. If  cid:25  has the form a1 ∼ τ1, . . . , an ∼ τn, then we sometimes write ν  cid:25 {p} for the iterated form ν a1 ∼ τ1.. . . ν an The statics of CA is given by these judgments:  ∼ τn.p.   cid:7   cid:12  cid:25  e : τ  cid:7   cid:12  cid:25  m ∼·· τ expression typing command typing  cid:7   cid:12  cid:25  p proc process formation  cid:7   cid:12  cid:25  α action action formation  The expression and command typing judgments are essentially those of MA, augmented with the constructs described below.  Process formation is deﬁned by the following rules:  Processes are identiﬁed up to structural congruence, as described in Chapter 39.  Action formation is deﬁned by the following rules:   cid:12  cid:25  1 proc  cid:12  cid:25  m ∼·· τ  cid:12  cid:25  run m  proc   cid:12  cid:25  p1 proc  cid:12  cid:25  p2 proc   cid:12  cid:25  p1 ⊗ p2 proc  cid:12  cid:25 ,a∼τ p proc  cid:12  cid:25  ν a ∼ τ .p proc   cid:12  cid:25  ε action   cid:12  cid:25  e : clsfd e val cid:25    cid:12  cid:25  e ! action   cid:12  cid:25  e : clsfd e val cid:25    cid:12  cid:25  e ? action   cid:25   p  cid:5  while undertaking action α. α=⇒ run m  α cid:20 −→   cid:5  { m  cid:5  ⊗ p }  cid:5 {run m   cid:5   ⊗ p}  ν  cid:25   ν  cid:25   m   cid:25    cid:25   e val cid:25   run ret e  ε cid:20 −→  1   cid:25    40.1a    40.1b    40.1c    40.1d    40.2a    40.2b    40.2c    40.3a    40.3b   Messages are values of the type clsfd deﬁned in Chapter 33. α cid:20 −→  The dynamics of CA is deﬁned by transitions between processes, which represent the  cid:5  states that the process p  state of the computation. More precisely, the judgment p evolves in one step to the process p   377  40.1 Concurrent Algol   cid:25   p  p1 p1 ⊗ p2 α cid:20 −→ p1 p1 ⊗ p2  cid:20 −−−→  cid:25 ,a∼τ ν a ∼ τ .p  p  α  p  α cid:20 −→ α cid:20 −→   cid:25    cid:25    cid:5  p 1  cid:5  p 1   cid:5  1 p2 ε cid:20 −→   cid:5  p 1   cid:25   ⊗ p2 α cid:20 −→ ⊗ p   cid:5  p 2  cid:5  2   cid:25    cid:5   cid:12  cid:25  α action α cid:20 −→  cid:5  ν a ∼ τ .p   cid:25   Rule  40.3a  states that a step of execution of the atomic process run m  consists of a  cid:5  of symbols or step of execution of the command m, which may allocate some set  cid:25  create a concurrent process p. This rule implements scope extrusion for classes  channels  by expanding the scope of the channel declaration to the context in which the command m occurs. Rule  40.3b  states that a completed command evolves to the inert  stopped  process; processes are executed solely for their effect, and not for their value.  Executing a command in CA may, in addition to evolving to another command, allocate  a new channel or may spawn a new process. More precisely, the judgment1  α=⇒   cid:25   m   cid:5  { m   cid:5  ⊗ p   cid:5  }  ν  cid:25    cid:5   cid:5  while creating new channels  cid:25  states that the command m transitions to the command m  cid:5 . The action α speciﬁes the interactions of which m is capable when and new processes p executed. As a notational convenience, we drop mention of the new channels or processes when either are trivial.  The following rules deﬁne the execution of the basic forms of command inherited from  MA:   cid:5   e  e  cid:20 −→ ε=⇒   cid:25    cid:5   ret e α=⇒ α=⇒   cid:25    cid:25   ν  cid:25   ν  cid:25    cid:25   ret e  cid:5  { m ⊗ p  cid:5 {bnd x ← cmd m   cid:5  }   cid:5  1  1 ; m2 ⊗ p  cid:5    cid:5 }  m1 bnd x ← cmd m1 ; m2  e val cid:25   bnd x ← cmd  ret e  ; m2  ε=⇒   cid:25   [e x]m2  bnd x ← e1 ; m2  e1  cid:20 −→ ε=⇒   cid:25    cid:25    cid:5  e 1 bnd x ← e   cid:5  1 ; m2  These rules are supplemented by rules governing communication and synchronization among processes in the next two sections.   40.3c    40.3d    40.3e    40.4a    40.4b    40.4c    40.4d    378  Concurrent Algol  40.2 Broadcast Communication  In this section, we consider a very general form of process synchronization called broadcast. Processes emit and accept messages of type clsfd, the type of dynamically classiﬁed values considered in Chapter 33. A message consists of a channel, which is its class, and a payload, which is a value of the type associated with the channel  class . Recipients may pattern match against a message to determine whether it is of a given class, and, if so, recover the associated payload. No process that lacks access to the class of a message may recover the payload of that message.  See Section 33.4.1 for a discussion of how to enforce conﬁdentiality and integrity restrictions using dynamic classiﬁcation.   The syntax of the commands pertinent to broadcast communication is given by the  following grammar:  Cmd m ::= spawn e  spawn e  emit e  emit e  acc acc newch{τ} newch  spawn emit message accept message new channel  The command spawn e  spawns a process that executes the encapsulated command given by e. The commands emit e  and acc emit and accept messages, which are classiﬁed values whose class is the channel on which the message is sent. The command newch[τ] returns a reference to a fresh class carrying values of type τ.  The statics of broadcast communication is given by the following rules:   cid:7   cid:12  cid:25  e : cmd unit   cid:7   cid:12  cid:25  spawn e  ∼·· unit  cid:7   cid:12  cid:25  emit e  ∼·· unit   cid:7   cid:12  cid:25  e : clsfd   cid:7   cid:12  cid:25  acc ∼·· clsfd   cid:7   cid:12  cid:25  newch{τ} ∼·· cls τ   Execution of these commands is deﬁned as follows:  ret cid:24  cid:25  ⊗ run m    cid:25   spawn cmd m   ε=⇒ e  cid:20 −→ spawn e  ε=⇒   cid:25    cid:25    cid:5   e  spawn e   cid:5     40.5a    40.5b    40.5c    40.5d    40.6a    40.6b    379  40.2 Broadcast Communication   cid:25   e val cid:25  emit e  e !=⇒ e  cid:20 −→ emit e  ε=⇒  e   cid:25    cid:25   ret cid:24  cid:25   cid:5   emit e   cid:5    e val cid:25  acc e ?=⇒   cid:25   ret e   40.6c    40.6d    40.6e    40.6f   newch{τ} ε=⇒  ν a ∼ τ { ret  & a }   cid:25   Rule  40.6c  speciﬁes that emit e  has the effect of emitting the message e. Correspond- ingly, rule  40.6e  speciﬁes that acc may accept  any  message that is being sent.  As usual, the preservation theorem for CA ensures that well-typed programs remain well-typed during execution. The proof of preservation requires a lemma about command execution.   cid:5  { m   cid:5  ⊗ p  ν  cid:25    cid:5  },  cid:12  cid:25  m ∼·· τ , then  cid:12  cid:25  α action,  cid:12  cid:25   cid:25    cid:5  ∼·· τ , and   cid:5  m  Lemma 40.1. If m  cid:12  cid:25   cid:25   proc.   cid:5  p   cid:5   α=⇒   cid:25   Proof By induction on rules  40.4 .  With this in hand, the proof of preservation goes along familiar lines. Theorem 40.2  Preservation . If  cid:12  cid:25  p proc and p  cid:20 −→   cid:5 , then  cid:12  cid:25  p  p   cid:5   proc.   cid:25   Proof By induction on transition, appealing to Lemma 40.1 for the crucial steps.  Typing does not, however, guarantee progress with respect to unlabeled transition, for the simple reason that there may be no other process with which to communicate. By extending progress to labeled transitions, we may state that this is the only way for process execution to get stuck. But some care must be taken to account for allocating new channels. Theorem 40.3  Progress . If  cid:12  cid:25  p proc, then either p ≡ 1, orp ≡ ν  cid:25   cid:5    cid:5 } such that   cid:5 {p   cid:5  cid:5  for some  cid:12  cid:25   cid:25    cid:5  cid:5  and some  cid:12  cid:25   cid:25    cid:5  p   cid:5  α action.  p  α cid:20 −−→   cid:25   cid:25    cid:5  p  Proof By induction on rules  40.1  and  40.5 .  The progress theorem says that no process can get stuck for any reason other than the inability to communicate with another process. For example, a process that receives on a channel for which there is no sender is “stuck,” but this does not violate Theorem 40.3.   380  Concurrent Algol  40.3 Selective Communication  Broadcast communication provides no means of restricting acceptance to messages of a particular class  that is, of messages on a particular channel . Using broadcast communica- tion, we may restrict attention to a particular channel a of type τ by running the following command:  fix loop : τ cmd is{x ← acc ; match x as a · y  cid:9 → ret y ow  cid:9 → emit x  ;do loop}  This command is always capable of receiving a broadcast message. When one arrives, it is examined to see whether it is classiﬁed by a. If so, the underlying classiﬁed value is returned; otherwise, the message is re-broadcast so that another process may consider it. Polling consists of repeatedly executing the above command until a message of channel a is successfully accepted, if ever it is.  Polling is evidently impractical in most situations. An alternative is to change the lan- guage to allow for selective communication. Rather than accept any broadcast message, we may conﬁne attention to messages sent only on certain channels. The type event τ  of events consists of a ﬁnite choice of accepts, all of whose payloads are of type τ.  τ e  Typ Exp  ::= event τ  ::= rcv[a] never{τ} or e1; e2  wrap e1; x.e2  Cmd m ::= sync e   τ event ? a never e1 or e2 e1 as x in e2 sync e   events selective read null choice post-composition synchronize  Events in CA are similar to those of the asynchronous process calculus described in Chap- ter 39. The chief difference is that post-composition is considered as a general operation on events, instead of one tied to the receive event itself.  The statics of event expressions is given by the following rules:   cid:25   cid:12  a ∼ τ   cid:7   cid:12  cid:25  rcv[a] : event τ    cid:7   cid:12  cid:25  never{τ} : event τ    cid:7   cid:12  cid:25  e1 : event τ   cid:7   cid:12  cid:25  e2 : event τ    cid:7   cid:12  cid:25  or e1; e2  :event  τ    cid:7   cid:12  cid:25  e1 : event τ1   cid:7 , x : τ1  cid:12  cid:25  e2 : τ2   cid:7   cid:12  cid:25  wrap e1; x.e2  : event τ2    40.7a    40.7b    40.7c    40.7d    381  40.3 Selective Communication  The corresponding dynamics is deﬁned by these rules:   cid:25   cid:12  a ∼ τ rcv[a] val cid:25   never{τ} val cid:25   e1 val cid:25  e2 val cid:25  or e1; e2  val cid:25   e1  cid:20 −→ or e1; e2   cid:20 −→   cid:25    cid:5  e 1 or e   cid:25    cid:5  1; e2   e1 val cid:25   or e1; e2   cid:20 −→   cid:25   e2  cid:20 −→  cid:5  e 2 or e1; e   cid:25    cid:5  2   e1  cid:20 −→ wrap e1; x.e2   cid:20 −→   cid:25    cid:5  e 1 wrap e   cid:25    cid:5  1; x.e   cid:5  2   e1 val cid:25   wrap e1; x.e2  val cid:25    cid:7   cid:12  cid:25  e : event τ   cid:7   cid:12  cid:25  sync e  ∼·· τ   cid:5   e  e  cid:20 −→ sync e  ε=⇒   cid:25    cid:25   sync e   cid:5    e val cid:25   cid:12  cid:25  e : τ  cid:25   cid:12  a ∼ τ sync rcv[a]  a·e ?==⇒ ret e    cid:25   sync e1  α=⇒  m1 sync or e1; e2   α=⇒   cid:25   m1   cid:25    40.8a    40.8b    40.8c    40.8d    40.8e    40.8f    40.8g    40.9a    40.10a    40.10b    40.10c   Event values are identiﬁed up to structural congruence as described in Chapter 39.  The statics of the synchronization command is given by the following rule:  The type of the event determines the type of value returned by the synchronization command.  Execution of a synchronization command depends on the event.   382  Concurrent Algol  sync e2  α=⇒  m2 sync or e1; e2   α=⇒   cid:25   m2   cid:25   sync e1  α=⇒  m1   cid:25    40.10d    40.10e   sync wrap e1; x.e2   α=⇒   cid:25   bnd cmd m1 ; x.ret e2    Rule  40.10b  states that an acceptance on a channel a may synchronize only with messages classiﬁed by a. When combined with structural congruence, rules  40.10c  and  40.10d  state that either event between two choices may engender an action. Rule  40.10e  yields the command that performs the command m1 resulting from the action α taken by the event e1, then returns e2 with x bound to the return value of m1.  Selective communication and dynamic events can be used together to implement a communication protocol in which a channel reference is passed on a channel in order to establish a communication path with the recipient. Let a be a channel carrying values of type cls τ , and let b be a channel carrying values of type τ, so that & b can be passed as a message along channel a. A process that wishes to accept a channel reference on a and then accept on that channel has the form  {x ← sync ? a  ; y ← sync ?? x  ;. . . }.  The event ? a speciﬁes a selective receipt on channel a. Once the value x is accepted, the event ?? x speciﬁes a selective receipt on the channel referenced by x. So, if & b is sent along a, then the event ?? & b evaluates to ? b, which accepts selectively on channel b, even though the receiving process may have no direct access to the channel b itself.  40.4 Free Assignables as Processes  Scope-free assignables are deﬁnable in CA by associating to each assignable a server process that sets and gets the contents of the assignable. To each assignable a of type τ is associated a server that selectively accepts a message on channel a with one of two forms: 1. get ·  & b , where b is a channel of type τ. This message requests that the contents of a 2. set ·   cid:24 e, & b cid:25  , where e is a value of type τ, and b is a channel of type τ. This message requests that the contents of a be set to e, and that the new contents be transmitted on channel b.  be sent on channel b.  In other words, a is a channel of type τsrvr given by  [get  cid:9 → τ cls, set  cid:9 → τ × τ cls].  The server selectively accepts on channel a, then dispatches on the class of the message to satisfy the request.   383  40.5 Notes  The server associated with the assignable a of type τ maintains the contents of a using recursion. When called with the current contents of the assignable, the server selectively accepts on channel a, dispatching on the associated request, and calling itself recursively with the  updated, if necessary  contents: λ  u : τsrvr cls  fix srvr : τ  cid:19  void cmd is λ  x : τ  cmd{y ← sync ?? u  ; e 40.12 }.  40.11  The server is a procedure that takes an argument of type τ, the current contents of the assignable, and yields a command that never terminates, because it restarts the server loop after each request. The server selectively accepts a message on channel a, and dispatches on it as follows:  case y {get · z  cid:9 → e 40.13   set ·  cid:24 x   cid:5   , z cid:25   cid:9 → e 40.14 }.  A request to get the contents of the assignable a is served as follows:  { ← emit mk z; x   ; do srvr x }  A request to set the contents of the assignable a is served as follows:  { ← emit mk z; x   cid:5      ; do srvr x   cid:5    }   40.12    40.13    40.14   The type τ ref is deﬁned to be τsrvr cls, the type of channels  classes  to servers providing a cell containing a value of type τ. A new free assignable is created by the command ref e0, which is deﬁned to be  {x ← newch ; ← spawn e 40.11  x  e0   ; ret x}.   40.15   A channel carrying a value of type τsrvr is allocated to serve as the name of the assignable, and a new server is spawned that accepts requests on that channel, with initial value e0 of type τ0. The commands ∗ e0 and e0 ∗= e1 send a message to the server to get and set the contents of an assignable. The code for ∗ e0 is as follows:  {x ← newch ; ← emit mk e0; get · x   ; sync ??  x  }   40.16   A channel is allocated for the return value, the server is contacted with a get message specifying this channel, and the result of receiving on this channel is returned. Similarly, the code for e0 ∗= e1 is as follows:  {x ← newch ; ← emit mk e0; set ·  cid:24 e1, x cid:25    ; sync ??  x  }   40.17   40.5 Notes  Concurrent Algol is a synthesis of process calculus and Modernized Algol; is essentially an “Algol-like” formulation of Concurrent ML  Reppy, 1999 . The design is inﬂuenced by Parallel Algol  Brookes, 2002 . Much work on concurrent interaction takes communication channels as a basic concept, but see Linda  Gelernter, 1985  for an account similar to the one suggested here.   384  Concurrent Algol  Exercises  40.1. In Section 40.2 channels are allocated using the command newch, which returns a channel reference. Alternatively one may extend CA with a means of declaring channels just as assignables are declared in MA. Formulate the syntax, statics, and dynamics of such a construct, and derive newch using this extension.  40.2. Extend selective communication  Section 40.3  to account for channel references, which give rise to a new form of event. Give the syntax, statics, and semantics of this extension.  40.3. Adapt the implementation of an RS latch given in Exercise 39.3 to CA.  Note  1 The right-hand side of this judgment is a triple consisting of  cid:25    cid:5 , m   cid:5 , and p   cid:5 , not a process expression  comprising these parts.   41  Distributed Algol  A distributed computation is one that takes place at many sites, each of which controls some resources at that site. For example, the sites might be nodes on a network, and a resource might be a device or sensor at that site, or a database controlled by that site. Only programs that execute at a particular site may access the resources situated at that site. Consequently, command execution always takes place at a particular site, called the locus of execution. Access to resources at a remote site from a local site is achieved by moving the locus of execution to the remote site, running code to access the local resource, and returning a value to the local site.  In this chapter, we consider the language DA, which extends Concurrent Algol with a spatial type system that mediates access to resources on a network. The type safety theorem ensures that all accesses to a resource controlled by a site are through a program executing at that site, even though references to local resources can be freely passed around to other sites on the network. The main idea is that channels and events are located at a particular site, and that synchronization on an event can only occur at the proper site for that event. Issues of concurrency, which are temporal, are thereby separated from those of distribution, which are spatial.  The concept of location in DA is sufﬁciently abstract that it admits another useful inter- pretation that can be useful in computer security settings. The “location” of a computation can be considered to be the principal on whose behalf the computation is executing. From this point of view, a local resource is one that is accessible to a particular principal, and a mobile computation is one that can be executed by any principal. Movement from one location to another may then be interpreted as executing a piece of code on behalf of another principal, returning its result to the principal that initiated the transfer.  41.1 Statics  The statics of DA is inspired by the possible worlds interpretation of modal logic. Under that interpretation the truth of a proposition is considered relative to a world, which determines the state of affairs described by that proposition. A proposition may be true in one world, and false in another. For example, one may use possible worlds to model counter-factual reasoning, where one postulates that certain facts that happen to be true in this, the actual, world, might be otherwise in some other, possible, world. For instance, in the actual world   386  Distributed Algol  you, the reader, are reading this book, but in a possible world you may never have taken up the study of programming languages at all. Of course not everything is possible: there is no possible world in which 2 + 2 is other than 4, for example. Moreover, once a commitment has been made to one counter-factual, others are ruled out. We say that one world is accessible from another when the ﬁrst is a sensible counter-factual relative to the ﬁrst. So, for example, one may consider that relative to a possible world in which you are the king, there is no further possible world in which someone else is also the king  there being only one sovereign .  In DA, we shall interpret possible worlds as sites on a network, with accessibility between worlds expressing network connectivity. We postulate that every site is connected to itself  reﬂexivity ; that if one site is reachable from another, then the second is also reachable from the ﬁrst  symmetry ; and that if a site is reachable from a reachable site, then this site is itself reachable from the ﬁrst  transitivity . From the point of view of modal logics, the type system of DA is derived from the logic S5, for which accessibility is an equivalence relation.  The syntax of DA derives from that of CA. The following grammar summarizes the  important changes:  τ  ::= cmd[w] τ  Typ Cmd m ::= at[w] m   event[w] τ   τ cmd[w] τ event[w] at w do m  commands events change site  The command and event types are indexed by the site w at which they make sense. The command at[w] m  changes the locus of execution from one site to another.  A signature  cid:25  in DA consists of a ﬁnite set of declarations of the form a ∼ τ @ w, where τ is a type and w is a site. Such a declaration speciﬁes that a is a channel at site w carrying a payload of type τ. We may think of a signature  cid:25  as a family of signatures  cid:25 w one for each world w, containing the declarations of the channels at that world. Partitioning channels in this way corresponds to the idea that channels are located at a particular site. They may be handled passively at other sites, but their only active role is at the site at which they are declared.  The statics of DA is given by the following judgment forms:   cid:7   cid:12  cid:25  e : τ  cid:7   cid:12  cid:25  m ∼·· τ @ w expression typing command typing  cid:7   cid:12  cid:25  p proc @ w process formation  cid:7   cid:12  cid:25  α action @ w action formation  The expression typing judgment is independent of the site, expressing the requirement that the values of a type be meaningful at any site. On the other hand, commands can only be executed at a particular site, because their meaning depends on the resources at a site. Processes are similarly conﬁned to execution at a site. Actions are site-speciﬁc; there is no inter-site synchronization.   387  rules:  The expressions of the command and event types of DA are deﬁned by the following  Rule  41.1a  states that the type of an encapsulated command records the site at which the command is executed. Rules  41.1b  to  41.1e  specify that events are attached to a site because channels are. Communication among processes is conﬁned to a site; there is no inter-site synchronization.  The statics of the commands of DA is given by the following rules:  41.1 Statics   cid:7   cid:12  cid:25  m ∼·· τ @ w   cid:7   cid:12  cid:25  cmd m  :cmd[w ] τ    cid:7   cid:12  cid:25  never[τ] : event[w] τ    cid:25   cid:12  a ∼ τ @ w   cid:7   cid:12  cid:25  rcv[a] :event [w] τ    cid:7   cid:12  cid:25  e1 : event[w] τ   cid:7   cid:12  cid:25  e2 : event[w] τ    cid:7   cid:12  cid:25  or e1; e2  :event [w] τ    cid:7   cid:12  cid:25  e1 : event[w] τ1   cid:7 , x : τ1  cid:12  cid:25  e2 : τ2   cid:7   cid:12  cid:25  wrap e1; x.e2  : event[w] τ2    cid:7   cid:12  cid:25  e1 : τ1cmd @ w  cid:7 , x : τ1  cid:12  cid:25  m2 ∼·· τ2 @ w   cid:7   cid:12  cid:25  e : τ   cid:7   cid:12  cid:25  ret e  ∼·· τ @ w   cid:7   cid:12  cid:25  bnd e1; x.m2  ∼·· τ2 @ w  cid:7   cid:12  cid:25  e : cmd[w] unit   cid:7   cid:12  cid:25  spawn e  ∼·· unit @ w  cid:7   cid:12  cid:25  e : τ  cid:25   cid:12  a ∼ τ @ w  cid:7   cid:12  cid:25  snd[a] e  ∼·· unit @ w  cid:7   cid:12  cid:25  e : event[w] τ   cid:7   cid:12  cid:25  sync e  ∼·· τ @ w  cid:7   cid:12  cid:25  m  cid:5  @ w  cid:5   ∼·· τ  cid:7   cid:12  cid:25  at[w   cid:5  ∼·· τ  cid:5 ] m   cid:5   cid:5  @ w   41.1a    41.1b    41.1c    41.1d    41.1e    41.2a    41.2b    41.2c    41.2d    41.2e    41.2f   Rule  41.2a  states that an expression may be returned at any site, because its meaning is independent of the site. Rule  41.2b  ensures that the sequential composition of commands is allowed only within a site, and not across sites. Rule  41.2e  states that the sync command returns a value of the same type as that of the event, and can be executed only at the site to which the given event pertains. Rule  41.2d  states that a message can be sent along a channel available at the site from which it is sent. Finally, rule  41.2f  states that to execute  cid:5  requires that the command pertain to that site. The returned value a command at a site w is then passed to the original site.   388  Distributed Algol  Process formation is deﬁned as follows:   cid:12  cid:25  p1 proc @ w  cid:12  cid:25  p2 proc @ w   cid:12  cid:25  1 proc @ w  cid:12  cid:25  m ∼·· unit @ w  cid:12  cid:25  run m  proc @ w   cid:12  cid:25  p1 ⊗ p2 proc @ w  cid:12  cid:25 ,a∼τ @w p proc @ w  cid:12  cid:25  ν a ∼ τ .p proc @ w   cid:12  cid:25  e : τ   cid:12  cid:25  e : τ   cid:12  cid:25  ε action @ w e val cid:25   cid:25   cid:12  a ∼ τ @ w   cid:12  cid:25  a · e ! action @ w  e val cid:25   cid:25   cid:12  a ∼ τ @ w   cid:12  cid:25  a · e ? action @ w  These rules state that processes are sited. In particular, an atomic process consists of a command suitable for the site at which the process is run, and a new channel is allocated at the site of the process that allocates it.  Action formation is deﬁned as follows:  Messages are values of type clsfd and are meaningful only at the site at which the channel is allocated. Locality of actions corresponds to conﬁnement of communication to a single site.  41.2 Dynamics   cid:20 −−−→  α @ w   cid:5   p  p   cid:25   The dynamics of DA is a labeled transition judgment between processes at a site. Thus, the judgment  states that at site w the process p steps to the process p deﬁned by the following rules:   cid:5 , engendering the action α. It is  m  α @ w===⇒  cid:20 −−−→ run m  α @ w   cid:25    cid:25    cid:5  { m  cid:5  ⊗ p }  cid:5 {run m  ν  cid:25   ν  cid:25    cid:5   ⊗ p}  e val cid:25    cid:20 −−−→ run ret e  ε @ w  1   cid:25    41.3a    41.3b    41.3c    41.3d    41.4a    41.4b    41.4c    41.5a    41.5b    389  41.2 Dynamics   cid:25   α @ w  p1 p1 ⊗ p2  cid:20 −−−→ p1 p1 ⊗ p2  cid:20 −−−−−→  cid:25 ,a∼τ @w ν a ∼ τ .p  α @ w  p  p  α @ w   cid:20 −−−→  cid:20 −−−→  α @ w   cid:25    cid:25    cid:5  p 1  cid:5  p 1   cid:5  p 1 p2  cid:20 −−−→  ε @ w   cid:25   ⊗ p2  cid:20 −−−→ ⊗ p  cid:5  p 1  α @ w   cid:25    cid:5  p 2  cid:5  2   cid:5   cid:12  cid:25  α action @ w  cid:20 −−−→  ν a ∼ τ .p  α @ w   cid:5    cid:25   These rules are like rules  40.3 , but for the sensitivity to the site at which execution takes place. The site comes into play in rules  41.5a  and  41.5e .  Rule  41.5a  makes use of the command execution judgment  α @ w===⇒   cid:25   m   cid:5  { m   cid:5  ⊗ p },  ν  cid:25   which states that the command m when executed at site w may engender the action α and  cid:5 , and a new process p.  The result of the transition is in the process create new channels,  cid:25  not a process expression, but a triple comprising the newly allocated channels, the newly created processes, and a new command.   Command execution is deﬁned by the following rules:  spawn cmd m   ε @ w==⇒  ret  cid:24  cid:25   ⊗ run m    cid:25   e val cid:25   cid:12  cid:25  e : τ  cid:25   cid:12  a ∼ τ @ w  snd[a] e  a·e ! @ w====⇒  ret cid:24  cid:25   e val cid:25   cid:12  cid:25  e : τ  cid:25   cid:12  a ∼ τ @ w   cid:25   ret e    cid:25   sync rcv[a]  a·e ? @ w ====⇒ sync e1  α @ w===⇒  m1 sync or e1; e2   α @ w===⇒   cid:25   m1  m2   cid:25    cid:25   sync e2  α @ w===⇒  m2 sync or e1; e2   α @ w===⇒   cid:25   sync e1  α @ w===⇒  m1  sync wrap e1; x.e2   α @ w===⇒   cid:25    cid:25  bnd cmd m1 ; x.ret e2     41.5c    41.5d    41.5e    41.6a    41.6b    41.6c    41.6d    41.6e    41.6f    390  Distributed Algol  m  α @ w   cid:5 ===⇒  cid:5 ] m  α @ w===⇒   cid:25    cid:5  }  cid:5  ⊗ p  cid:5  { m  cid:5  { at[w  cid:5 ] m  ν  cid:25   ν  cid:25    cid:25   at[w   cid:5   ⊗ p   cid:5  }  e val cid:25    cid:5 ] ret e   ε @ w==⇒  at[w  ret e    cid:25    41.6g    41.6h   Rule  41.6a  states that new processes created at a site stay at that site—the new process executes the given command at the current site. Rule  41.6b  speciﬁes that a send generates an event speciﬁc to the site at which it occurs. Rules  41.6c  to  41.6f  specify that receive events occur only for channels allocated at the execution site. Rules  41.6g  and  41.6h   cid:5 , and state that the command at[w returning the result to the site w.   cid:5 ] m  is executed at site w by executing m at site w  41.3 Safety  The safety theorem for DA ensures that synchronization on a channel may only occur at the site on which the channel resides, even though channel references may be propagated from one site to another during a computation. By the time the reference is resolved and synchronization is attempted, the computation will be executing at the right site.  Lemma 41.1  Execution . If m α action @ w,  cid:12  cid:25   cid:25    cid:5  ∼·· τ @ w, and  cid:12  cid:25   cid:25    cid:5  m  ν  cid:25    cid:25   α @ w===⇒   cid:5  { m  cid:5   cid:5  p   cid:5  ⊗ p proc @ w.   cid:5  }, and  cid:12  cid:25  m ∼·· τ @ w, then  cid:12  cid:25   Proof By induction on rules  41.6 .  Theorem 41.2  Preservation . If p   cid:20 −−−→  α   cid:25  @ w   cid:5  and  cid:12  cid:25  p proc @ w, then  cid:12  cid:25  p   cid:5   p  proc @ w.  Proof By induction on the statics of DA, appealing to Lemma 41.1 for atomic processes.  The progress theorem states that the only impediment to execution of a well-typed  program is synchronizing on an event that never occurs. Theorem 41.3  Progress . If  cid:12  cid:25  p proc @ w, then either p ≡ 1 or there exists α and p such that p   cid:20 −−−→   cid:5 .  p  α   cid:5    cid:25  @ w  Proof By induction on the dynamics of DA.   391  Exercises  41.4 Notes  The use of a spatial modality to express locality and mobility constraints in a distributed program was introduced in the experimental language ML5  Murphy et al., 2004 . Some languages for distributed computing consolidate concurrency with distribution by allowing cross-site interaction. The idea of DA is to separate temporal from spatial considerations, limiting synchronization to a single site, but allowing movement of the locus of execution from one site to another.  Exercises  41.1. The deﬁnition of DA given in this chapter has no means of allocating new channels, or sending and receiving on them. Remedy this shortcoming by adding a command to create channel references. Give the statics and dynamics of this extension, and of any associated extensions needed to account for it. Hint: the type of channel references, chan[w] τ , should be indexed by the site of the channel to which the reference refers.  41.2. Given a channel reference e : chan[w   cid:5 ] τ , it is sensible to send a message asyn-  cid:5  : τ. It is also chronously from site w along this channel by providing a payload e possible to implement a synchronous remote send  also known as a remote procedure  cid:5 ] τ   and returns  cid:5  : τ on a remote channel e : chan[w call  that sends a message e  cid:5  in response to the message. Implement both of these capabili- a result of type τ ties in DA. Hint: Implement synchronous communication using a reply channel as described in Chapter 39.    P A R T XVII  Modularity    42  Modularity and Linking  Modularity is the most important technique for controlling the complexity of programs. Programs are decomposed into separate components with precisely speciﬁed, and tightly controlled, interactions. The pathways for interaction among components determine de- pendencies that constrain the process by which the components are integrated, or linked, to form a complete system. Different systems may use the same components, and a single system may use multiple instances of a single component. Sharing of components amor- tizes the cost of their development across systems and helps limit errors by limiting coding effort.  Modularity is not limited to programming languages. In mathematics, the proof of a theorem is decomposed into a collection of deﬁnitions and lemmas. References among the lemmas determine a dependency relation that constrains their integration to form a complete proof of the main theorem. Of course, one person’s theorem is another person’s lemma; there is no intrinsic limit on the depth and complexity of the hierarchies of results in mathematics. Mathematical structures are themselves composed of separable parts, for example, a ring comprises a group and a monoid structure on the same underlying set.  Modularity arises from the structural properties of the hypothetical and general judg- ments. Dependencies among components are expressed by free variables whose typing assumptions state the presumed properties of the component. Linking amounts to substitu- tion to discharge the hypothesis.  42.1 Simple Units and Linking  Decomposing a program into units amounts to exploiting the transitivity of the hypothetical judgment  see Chapter 3 . The decomposition may be described as an interaction between two parties, the client and the implementor, mediated by an agreed-upon contract, an inter- face. The client assumes that the implementor upholds the contract, and the implementor guarantees that the contract will be upheld. The assumption made by the client amounts to a declaration of its dependence on the implementor discharged by linking the two parties accordng to their agreed-upon contract.  The interface that mediates the interaction between a client and an implementor is a type. Linking is the implementation of the composite structural rules of substitution   396  Modularity and Linking  and transitivity:   cid:7   cid:12  eimpl : τintf  cid:7 , x : τintf  cid:12  eclient : τclient   cid:7   cid:12  [eimpl x]eclient : τclient   42.1   The type τintf is the interface type. It deﬁnes the operations provided by the implementor eimpl and relied upon by the client eclient. The free variable x expresses the dependency of eclient on eimpl. That is, the client accesses the implementation by using the variable x.  The interface type τintf is the contract between the client and the implementor. It deter- mines the properties of the implementation on which the client may depend and, at the same time, determines the obligations that the implementor must fulﬁll. The simplest form of interface type is a ﬁnite product type of the form  cid:24 f1  cid:9 → τ1, . . . , fn  cid:9 → τn cid:25 , specifying a component with components fi of type τi. Such a type is an application program interface, or API, because it determines the operations that the client  application  may expect from the implementor. A more advanced form of interface is one that deﬁnes an abstract type of the form ∃ t. cid:24 f1  cid:9 → τ1, . . . , fn  cid:9 → τn cid:25  , which deﬁnes an abstract type t representing the internal state of an “abstract machine” whose “instruction set” consists of the operations f1, . . . , fn whose types may involve t. Being abstract, the type t is not revealed to the client but is known only to the implementor.1  Conceptually, linking is just substitution, but practically this can be implemented in many ways. One method is separate compilation. The expressions eclient and eimpl, the source modules, are translated  compiled  into another, lower-level, language, resulting in object modules. Linking consists of performing the required substitution at the level of the object language in such a way that the result corresponds to translating [eimpl x]eclient. Another method, separate checking, shifts the requirement for translation to the linker. The client and implementor units are checked for type correctness with respect to the interface but are not translated into lower-level form. Linking then consists of translating the composite program as a whole, often resulting in a more efﬁcient outcome than would be possible when compiling separately.  The foregoing are all forms of static linking because the program is composed before it is executed. Another method, dynamic linking, defers program composition until run-time, so that a component is loaded only if it is actually required during execution. This might seem to involve executing programs with free variables, but it does not. Each client implemented by a stub that forwards accesses to a stored implementation  typically, in an ambient ﬁle system . The difﬁculty with dynamic linking is that it refers to components by name  say, a path in a ﬁle system , and the binding of that name may change at any time, wreaking havoc on program behavior.  42.2 Initialization and Eﬀects  Linking resolves the dependencies among the components of a program by substitution. This view is valid so long as the components are given by pure expressions, those that evaluate to a value without inducing any effects. For in such cases, there is no problem with   397  42.2 Initialization and Eﬀects  the replication, or complete omission, of a component arising from repeated, or absent, uses of a variable representing it. But what if the expression deﬁning the implementation of a component has an effect when evaluated? At a minimum replication of the component implies replication of its effects. Worse, effects introduce implicit dependencies among components that are not apparent from their types. For example, if each of two components mutates a shared assignable, the order in which they are linked with a client program affects the behavior of the whole.  This may raise doubts about the treatment of linking as substitution, but on closer in- spection it becomes clear that implicit dependencies are naturally expressed by the modal distinction between expressions and commands introduced in Chapter 34. Speciﬁcally, a component that may have an effect when executed does not have type τintf of implemen- tations of the interface type, but rather the type τintf cmd of encapsulated commands that, when executed, have effects and yield implementations. Being encapsulated, a value of this type is itself free of effects, but it may have effects when evaluated.  The distinction between the types τintf and τintf cmd is mediated by the sequencing com- mand introduced in Chapter 34. For the sake of generality, let us assume that the client is itself an encapsulated command of type τclient cmd, so that it may itself have effects when executed, and may serve as a component of a yet larger system. Assuming that the client refers to the encapsulated implementation by the variable x, the command  bnd x ← x ; do eclient  ﬁrst determines the implementation of the interface by running the encapsulated command x then running the client code with the result bound to x. The implicit dependencies of the client on the implementor are made explicit by the sequencing command, which ensures that the implementor’s effects occur prior to those of the client, precisely because the client depends on the implementor for its execution.  More generally, to manage such interactions in a large program it is common to isolate an initialization procedure whose role is to stage the effects engendered by the various components according to some policy or convention. Rather than attempt to survey all possible policies, let us just note that the upshot of such conventions is that the initialization procedure is a command of the form  {x1 ← x1 ; . . . xn ← xn ; mmain},  where x1, . . . , xn represent the components of the system and mmain is the main  startup  routine. After linking the initialization procedure has the form {x1 ← e1 ; . . . xn ← en ; mmain},  where e1, . . . , en are the encapsulated implementations of the linked components. When the initialization procedure is executed, it results in the substitution  [v1, . . . , vn x1, . . . , xn]mmain,  where the expressions v1, . . . , vn represent the values resulting from executing e1, . . . , en, respectively, and the implicit effects have occurred in the order speciﬁed by the initializer.   398  Modularity and Linking  42.3 Notes  The relationship between the structural properties of entailment and the practical problem of separate development was implicit in much early work on programming languages but became explicit once the correspondence between propositions and types was developed. There are many indications of this correspondence in sources such as Proofs and Types  Girard, 1989  and Intuitionistic Type Theory  Martin-L¨of, 1984 , but it was ﬁrst made explicit by Cardelli  1997 .  Note  1 See Chapters 17 and 48 for a discussion of type abstraction.   43  Singleton Kinds and Subkinding  The expression let e1 : τ be x in e2 is a form of abbreviation mechanism by which we may bind e1 to the variable x for use within e2. In the presence of function types, this expression is deﬁnable as the application  λ  x : τ  e2  e1 , which accomplishes the same thing. It is natural to consider an analogous form of let expression, which binds a type to a type variable within a scope. Using def t is τ in e to bind the type variable t to τ within the expression e, we may write expressions such as  def t is nat × nat in λ  x : t  s x · l ,  which introduces a type abbreviation within an expression. To ensure that this expression is well-typed, the type variable t is to be synonymous with the type nat × nat, for otherwise the body of the λ-abstraction is not type correct.  Following the pattern of the expression-level let, we might guess that def t is τ in e abbreviates the polymorphic instantiation  cid:16  t  e[τ], which binds t to τ within e. Doing so captures the dynamics of type abbreviation, but it fails to adhere to the intended statics. The difﬁculty is that, according to this interpretation of type deﬁnitions, the expression e is type-checked in the absence of any knowledge of the binding of t, rather than in the knowledge that t is synonymous with τ. Thus, in the above example, the expression s x · l  would fail to type check, unless the binding of t were exposed.  Interpreting type deﬁnition in terms of type abstraction and type application fails. One solution is to consider type abbreviation to be a primitive notion with the following statics:   cid:7   cid:12  [τ t]e : τ   cid:5    cid:7   cid:12  def t is τ in e : τ   cid:5    43.1   This formulation would solve the problem of type abbreviation, but in an ad hoc way. Is there a more general solution?  There is, by introducing singleton kinds, which classify type constructors by revealing their identity. Singletons n the type deﬁnition problem but play a crucial role in the design of module systems  as described in Chapters 44 and 45.   43.1 Overview  The central organizing principle of type theory is compositionality. To ensure that a program can be decomposed into separable parts, we ensure that the composition of a program from   400  Singleton Kinds and Subkinding  constituent parts is mediated by the types of those parts. Put in other terms, the only thing that one part of a program “knows” about another is its type. For example, the formation rule for addition of natural numbers depends only on the type of its arguments  both have type nat , and not on their speciﬁc form or value. But in the case of a type abbreviation of the form def t is τ in e, the principle of compositionality dictates that the only thing that e “knows” about the type variable t is its kind, namely T, and not its binding, namely τ. The proposed representation of type abbreviation as the combination of type abstraction and type application meets this requirement, but it does not have the intended meaning!  We could, as suggested in the introduction, abandon the core principles of type theory, and introduce type abbreviations as a primitive notion. But there is no need to do so. Instead, we need a kind for t that captures its identity; such a kind is called a singleton kind. Informally, the kind S τ  is the kind of types that are deﬁnitionally equal to τ. That is, up to deﬁnitional equality, this kind has only one inhabitant, namely τ. Consequently, if u :: S τ  is a variable of singleton kind, then within its scope, the variable u is synonymous with τ. Thus, we may represent def t is τ in e by  cid:16  t :: S τ   e[τ], which correctly propagates the identity of t, namely τ, toe during type checking.  The formalization of singleton kinds requires some more machinery at the constructor and kind level. First, we capture the idea that a constructor of singleton kind is a fortiori a constructor of kind T and hence is a type. Otherwise, a variable u of singleton kind cannot be used as a type, even though it is explicitly deﬁned to be one! To avoid this problem, we introduce a subkinding relation κ1 <:: κ2. The fundamental axiom of subkinding is S τ  <:: T, stating that every constructor of singleton kind is a type. Second, we account for constructors occurring within kinds. A singleton kind is a dependent kind in that its meaning depends on a constructor. Put another way, S τ  is afamily of kinds indexed by constructors of kind T. Products and functions are generalized to dependent products and dependent functions of families. The dependent product kind,  cid:25  u :: κ1.κ2, classiﬁes pairs  cid:24 c1,c2 cid:25  such that c1 :: κ1, as might be expected, and c2 :: [c1 u]κ2, in which the kind of the second component is sensitive to the ﬁrst component itself, and not just its kind. The dependent function kind,  cid:27  u :: κ1.κ2, classiﬁes functions that, when applied to a constructor c1 :: κ1, results in a constructor of kind [c1 u]κ2. Note that the kind of the result is sensitive to the argument, and not just to its kind.  Third, it is useful to consider singletons not just of kind T, but also of higher kinds. To support this, we introduce higher singletons, written S c :: κ , where κ is a kind and c is a constructor of kind κ. These are deﬁnable in terms of the basic form of singleton kinds using dependent function and product kinds.  43.2 Singletons  A singleton kind has the form S c , where c is a constructor. The singleton classiﬁes all constructors that are equivalent to the constructor c. For the time being, we consider singleton kinds in the context of the language Fω described in Chapter 18, which includes a kind of types and is closed under product and function kinds. In Section 43.3, we will   401  43.2 Singletons  enrich the language of kinds in a way that will ensure that the product and function kinds of Fω are deﬁnable.  The statics of singletons uses the following judgment forms:   cid:8   cid:12  κ kind  cid:8   cid:12  κ1 ≡ κ2  cid:8   cid:12  c :: κ  cid:8   cid:12  c1 ≡ c2 :: κ  cid:8   cid:12  κ1 <:: κ2  kind formation kind equivalence constructor formation constructor equivalence subkinding  These judgments are deﬁned simultaneously by a collection of rules, including the follow- ing:   cid:8   cid:12  c :: Type  cid:8   cid:12  S c  kind  cid:8   cid:12  c :: Type  cid:8   cid:12  c :: S c   cid:8   cid:12  c :: S d    cid:8   cid:12  c ≡ d :: Type   cid:8   cid:12  c :: κ1  cid:8   cid:12  κ1 <:: κ2   cid:8   cid:12  c :: κ2  cid:8   cid:12  c :: Type  cid:8   cid:12  S c  <:: Type  cid:8   cid:12  c ≡ d :: Type  cid:8   cid:12  S c  ≡ S d   cid:8   cid:12  κ1 ≡ κ2  cid:8   cid:12  κ1 <:: κ2   cid:8   cid:12  κ1 <:: κ2  cid:8   cid:12  κ2 <:: κ3   cid:8   cid:12  κ1 <:: κ3   43.2a    43.2b    43.2c    43.2d    43.2e    43.2f    43.2g    43.2h   Omitted for brevity are rules stating that constructor and kind equivalence are reﬂexive, symmetric, transitive, and preserved by kind and constructor formation.  Rule  43.2b  expresses the principle of “self-recognition,” which states that every con- structor c of kind Type also has the kind S c . By rule  43.2c , any constructor of kind S c  is deﬁnitionally equal to c. Consequently, self-recognition expresses the reﬂexivity of con- structor equivalence. Rule  43.2e  is just the subsumption principle re-stated at the level of constructors and kinds. Rule  43.2f  states that the singleton kind respects equivalence of its constructors, so that equivalent constructors determine the same singletons. Rules  43.2g  and  43.2h  state that the subkinding relation is a pre-order that respects kind equivalence. To see these rules in action, let us consider a few illustrative examples. First, consider the behavior of variables of singleton kind. Suppose that  cid:8   cid:12  u :: S c  is such a variable. Then, by rule  43.2c , we may deduce that  cid:8   cid:12  u ≡ c :: T. Thus, declaring u with a singleton kind deﬁnes it to be the constructor speciﬁed by its kind.   402  Singleton Kinds and Subkinding  Taking this a step further, the existential type ∃ u :: S c .τ is the type of packages whose representation type is  equivalent to  c—it is an abstract type whose identity is revealed by assigning it a singleton kind. By the general principles of equivalence, we have that the type ∃ u :: S c .τ is equivalent to the type ∃ :: S c .[c u]τ, wherein we have propagated the equivalence of u and c into the type τ. On the other hand, we may also “forget” the deﬁnition of u, because the subtyping  is derivable using the following variance rule for existential types over a kind:  ∃ u :: S c .τ <: ∃ u :: T.τ   cid:8   cid:12  κ1 <:: κ2  cid:8 , u :: κ1  cid:12  τ1 <: τ2   cid:8   cid:12  ∃ u :: κ1.τ1 <: ∃ u :: κ2.τ2  Similarly, we may derive the subtyping  ∀ u :: T.τ <: ∀ u :: S c .τ from the following variance rule for universals over a kind:   cid:8   cid:12  κ2 <:: κ1  cid:8 , u :: κ2  cid:12  τ1 <: τ2   cid:8   cid:12  ∀ u :: κ1.τ1 <: ∀ u :: κ2.τ2  Informally, the displayed subtyping states that a polymorphic function that may be applied to any type is one that may only be applied to a particular type c.  These examples show that singleton kinds express the idea of a scoped deﬁnition of a type variable in a way that is not tied to an ad hoc deﬁnition mechanism but arises naturally from general principles of binding and scope. We will see in Chapters 44 and 45 more advanced uses of singletons to manage the interaction among program modules.   43.3    43.4   43.3 Dependent Kinds  Although it is perfectly possible to add singleton kinds to the framework of higher kinds introduced in Chapter 18, to do so would be to short-change the expressiveness of the language. Using higher kinds, we can express the kind of constructors that, when applied to a type, yield a speciﬁc type, say int, as result, namely T → S int . But we cannot express the kind of constructors that, when applied to a type, yield that very type as result, for there is no way for the result kind to refer to the argument of the function. Similarly, using product kinds we can express the kind of pairs whose ﬁrst component is int and whose second component is an arbitrary type, namely S int × T. But we cannot express the kind of pairs whose second component is equivalent to its ﬁrst component, for there is no way for the kind of the second component to make reference to the ﬁrst component itself.  To express such concepts requires that product and function kinds be generalized so that the kind of the second component of a pair may mention the ﬁrst component of that pair, or the kind of the result of a function may mention the argument to which it is applied. Such   403  43.3 Dependent Kinds  kinds are called dependent kinds because they involve kinds that mention, or depend upon, constructors  of kind T . The syntax of dependent kinds is given by the following grammar:  Kind κ  ::= S c   S c   Con  c  ::= u   cid:25  κ1; u.κ2   cid:25  u :: κ1.κ2  cid:27  κ1; u.κ2   cid:27  u :: κ1.κ2  u   cid:24 c1,c2 cid:25  c · l c · r  pair c1; c2  proj[l] c  proj[r] c  lam{κ} u.c  λ  u :: κ  c app c1; c2   c1[c2]  singleton dependent product dependent function variable pair ﬁrst projection second projection abstraction application  As a notational convenience, when there is no dependency in a kind we write κ1 × κ2 for  cid:25  :: κ1.κ2, and κ1→ κ2 for  cid:27  :: κ1.κ2, where the “blank” stands for an irrelevant variable. The dependent product kind  cid:25  u :: κ1.κ2 classiﬁes pairs  cid:24 c1,c2 cid:25  of constructors in which c1 has kind κ1 and c2 has kind [c1 u]κ2. For example, the kind  cid:25  u :: T.S u  classiﬁes pairs  cid:24 c,c cid:25 , where c is a constructor of kind T. More generally, this kind classiﬁes pairs of the form  cid:24 c1,c2 cid:25  where c1 and c2 are equivalent but not necessarily identical, constructors. The dependent function kind  cid:27  u :: κ1.κ2 classiﬁes constructors c that, when applied to a constructor c1 of kind κ1, yields a constructor of kind [c1 u]κ2. For example, the kind  cid:27  u :: T.S u  classiﬁes constructors that, when applied to a constructor c, yields a construc- tor equivalent to c; a constructor of this kind is essentially the identity function. We may, of course, combine these to form kinds such as   cid:27  u :: T × T.S u · r  × S u · l ,  which classiﬁes functions that swap the components of a pair of types.  Such examples suggest that the behavior of a constructor may be pinned down precisely using dependent kinds. We shall see in Section 43.4 that this is the case.   The formation, introduction, and elimination rules for the product kind are as follows:   cid:8   cid:12  κ1 kind  cid:8 , u :: κ1  cid:12  κ2 kind   cid:8   cid:12   cid:25  u :: κ1.κ2 kind   cid:8   cid:12  c1 :: κ1  cid:8   cid:12  c2 :: [c1 u]κ2   cid:8   cid:12   cid:24 c1,c2 cid:25  ::  cid:25  u :: κ1.κ2   cid:8   cid:12  c ::  cid:25  u :: κ1.κ2   cid:8   cid:12  c · l :: κ1   cid:8   cid:12  c ::  cid:25  u :: κ1.κ2  cid:8   cid:12  c · r :: [c · l u]κ2   43.5a    43.5b    43.5c    43.5d   In rule  43.5a , note that the variable u may occur in the kind κ2 by appearing in a singleton kind. Correspondingly, rules  43.5b ,  43.5c , and  43.5d  substitute a constructor for this variable.   404  Singleton Kinds and Subkinding  The following equivalence axioms govern the constructors associated with the dependent  product kind:   cid:8   cid:12  c1 :: κ1  cid:8   cid:12  c2 :: κ2  cid:8   cid:12   cid:24 c1,c2 cid:25  · l ≡ c1 :: κ1  cid:8   cid:12  c1 :: κ1  cid:8   cid:12  c2 :: κ2  cid:8   cid:12   cid:24 c1,c2 cid:25  · r ≡ c2 :: κ2  The subkinding rule for the dependent product kind speciﬁes that it is covariant in both  positions:   cid:8   cid:12  κ1 <:: κ   cid:8   cid:12   cid:25  u :: κ1.κ2 <::  cid:25  u :: κ  1  cid:8 , u :: κ1  cid:12  κ2 <:: κ  cid:5   cid:5  2   cid:5  1.κ   cid:5  2  The congruence rule for equivalence of dependent product kinds is formally similar:  1  cid:8 , u :: κ1  cid:12  κ2 ≡ κ  cid:5   cid:5   cid:5  1.κ 2 Notable consequences of these rules include the subkindings   cid:8   cid:12  κ1 ≡ κ  cid:8   cid:12   cid:25  u :: κ1.κ2 ≡  cid:25  u :: κ   cid:5  2  and  and the equivalence   cid:25  u :: S int .S u  <::  cid:25  u :: T.S u    cid:25  u :: T.S u  <:: T × T,   cid:25  u :: S int .S u  ≡ S int  × S int .  Subkinding is used to “forget” information about the identity of the components of a pair, and equivalence is used to propagate such information within a kind.  The formation, introduction, and elimination rules for dependent function kinds are as  follows:   cid:8   cid:12  κ1 kind  cid:8 , u :: κ1  cid:12  κ2 kind   cid:8   cid:12   cid:27  u :: κ1.κ2 kind  cid:8 , u :: κ1  cid:12  c :: κ2   cid:8   cid:12  λ  u :: κ1  c ::  cid:27  u :: κ1.κ2  cid:8   cid:12  c ::  cid:27  u :: κ1.κ2  cid:8   cid:12  c1 :: κ1   cid:8   cid:12  c[c1] :: [c1 u]κ2  Rule  43.9b  speciﬁes that the result kind of a λ-abstraction depends uniformly on the argument u. Correspondingly, rule  43.9c  speciﬁes that the kind of an application is obtained by substitution of the argument into the result kind of the function itself.  The following rule of equivalence governs the constructors associated with the dependent  product kind:   cid:8 , u :: κ1  cid:12  c :: κ2  cid:8   cid:12  c1 :: κ1  cid:8   cid:12   λ  u :: κ1  c [c1] ≡ [c1 u]c :: κ2   43.6a    43.6b    43.7    43.8    43.9a    43.9b    43.9c    43.10    405  43.4 Higher Singletons  The subkinding rule for the dependent function kind speciﬁes that it is contravariant in  its domain and covariant in its range:  The equivalence rule is similar, except that the symmetry of equivalence obviates a choice of variance:   cid:8   cid:12  κ   cid:5  1 <:: κ1  cid:8 , u :: κ   cid:8   cid:12   cid:27  u :: κ1.κ2 <::  cid:27  u :: κ   cid:12  κ2 <:: κ  cid:5  2   cid:5  1.κ   cid:5  1   cid:5  2   cid:8   cid:12  κ1 ≡ κ  cid:8   cid:12   cid:27  u :: κ1.κ2 ≡  cid:27  u :: κ  1  cid:8 , u :: κ1  cid:12  κ2 ≡ κ  cid:5   cid:5   cid:5  1.κ 2   cid:5  2   cid:27  u :: T.S int  <::  cid:27  u :: S int .T,   43.11    43.12   Rule  43.11  gives rise to the subkinding  which illustrates the co- and contravariance of the dependent function kind. In particular, a function that takes any type and delivers the type int is also a function that takes the type int and delivers a type. Rule  43.12  gives rise to the equivalence  cid:27  u :: S int .S u  ≡ S int  → S int ,  which propagates information about the argument into the range kind. Combining these two rules we may derive the subkinding   cid:27  u :: T.S u  <:: S int  → S int .  Intuitively, a constructor function that yields its argument is, in particular, a constructor function that may only be applied to int and yields int. Formally, by contravariance we have the subkinding   cid:27  u :: T.S u  <::  cid:27  u :: S int .S u ,  and by sharing propagation we may derive the indicated superkind.  43.4 Higher Singletons  Although singletons are restricted to constructors of kind T, we may use dependent product and function kinds to deﬁne singletons of every kind. Speciﬁcally, we wish to deﬁne the kind S c :: κ , where c is of kind κ, that classiﬁes constructors equivalent to c. When κ = T, this is, of course, just S c ; the problem is to deﬁne singletons for the higher kinds  cid:25  u :: κ1.κ2 and  cid:27  u :: κ1.κ2. Suppose that c :: κ1 × κ2. The singleton kind S c :: κ1 × κ2  classiﬁes constructors equiv- alent to c. If we assume, inductively, that singletons are deﬁned for κ1 and κ2, then we need only note that c is equivalent to  cid:24 c · l,c · r cid:25 . For then, the singleton S c :: κ1 × κ2  can be deﬁned to be S c · l :: κ1  × S c · r :: κ2 . Similarly, suppose that c :: κ1 → κ2. Us- ing the equivalence of c and λ  u :: κ1 → κ2  c[u], we may deﬁne S c :: κ1 → κ2  to be  cid:27  u :: κ1.S c[u] :: κ2 .   406  Singleton Kinds and Subkinding  In general, the kind S c :: κ  is deﬁned by induction on the structure of κ by the following  kind equivalences:   cid:8   cid:12  c :: S c   cid:5     cid:8   cid:12  S c :: S c   cid:5    ≡ S c    cid:8   cid:12  c ::  cid:25  u :: κ1.κ2   cid:8   cid:12  S c ::  cid:25  u :: κ1.κ2  ≡  cid:25  u :: S c · l :: κ1 .S c · r :: κ2    cid:8   cid:12  c ::  cid:27  u :: κ1.κ2   cid:8   cid:12  S c ::  cid:27  u :: κ1.κ2  ≡  cid:27  u :: κ1.S c[u] ::κ 2   The sensibility of these equations relies on rule  43.2c  together with the following princi- ples of constructor equivalence, called extensionality principles:   cid:8   cid:12  c ::  cid:25  u :: κ1.κ2   cid:8   cid:12  c ≡  cid:24 c · l,c · r cid:25  ::  cid:25  u :: κ1.κ2   cid:8   cid:12  c ::  cid:27  u :: κ1.κ2   cid:8   cid:12  c ≡ λ  u :: κ1  c[u] ::  cid:27  u :: κ1.κ2   43.14b   cid:5 , and Rule  43.2c  states that the only constructors of kind S c rules  43.14a  and  43.14b  state that the only members of the dependent product and function types are, respectively, pairs and λ-abstractions of the right kinds.   cid:5   are those equivalent to c  Finally, the following self-recognition rules are required to ensure that rule  43.2b   extends to higher kinds.   43.13a    43.13b    43.13c    43.14a    43.15a    43.15b    cid:8   cid:12  c · l :: κ1  cid:8   cid:12  c · r :: [c · l u]κ2   cid:8   cid:12  c ::  cid:25  u :: κ1.κ2  cid:8 , u :: κ1  cid:12  c[u] :: κ2  cid:8   cid:12  c ::  cid:27  u :: κ1.κ2  An illustrative case arises when u is a constructor variable of kind  cid:25  v :: T.S v . We may derive that u · l :: S u · l  using rule  43.2b . We may also derive u · r :: S u · l  using rule  43.5d . Therefore, by rule  43.15a , we may derive u ::  cid:25  v :: S u · l .S u · l , which is a subkind of  cid:25  v :: T.S v . This more precise kind is a correct kinding for u, because the ﬁrst component of u is u · l, and the second component of u is equivalent to the ﬁrst component, and hence is also u· l. But without rule  43.15a , it is impossible to derive this fact.  The point of introducing higher singletons is to ensure that every constructor can be classiﬁed by a kind that determines it up to deﬁnitional equality. Viewed as extending singleton types, we would expect that higher singletons enjoy similar properties. Theorem 43.1. If  cid:8   cid:12  c :: κ, then  cid:8   cid:12  S c :: κ  <:: κ and  cid:8   cid:12  c :: S c :: κ .  The proof of this theorem is beyond the scope of this text.   407  Exercises  43.5 Notes  Singleton kinds were introduced by Stone and Harper  2006  to isolate the concept of type sharing that arises in the ML module system  Milner et al., 1997; Harper and Lillibridge, 1994; Leroy, 1994 . The meta-theory of singleton kinds is surprisingly intricate. The main source of complexity arises from constructor-indexed families of kinds. If u :: κ  cid:12  c  cid:5  :: κ  cid:5 ,  cid:5  and and if c1 :: κ and c2 : κ are distinct but equivalent, then so are the instances [c1 u]κ  cid:5 . Managing kind equivalence raises signiﬁcant technical difﬁculties in the proofs. [c2 u]κ  43.1. Show that rules  43.5c  and  43.5d  are inter-derivable with the following two rules:   43.16a    43.16b    43.17   43.2. Show that rule  43.9c  is inter-derivable with the rule   cid:8   cid:12  c :: κ1 → κ2  cid:8   cid:12  c1 :: κ1   cid:8   cid:12  c[c1] :: κ2  .  43.3. It is useful to modify a kind κ by imposing on κ a deﬁnition of one of its components. A component of a kind is speciﬁed by a simple path consisting of a ﬁnite, possibly empty, sequence of symbols l and r thought of as a tree address within a kind. The path projection c · p of a constructor c of kind κ by a path p is inductively deﬁned by these equations:  Exercises   cid:8   cid:12  c :: κ1 × κ2  cid:8   cid:12  c · l :: κ1  cid:8   cid:12  c :: κ1 × κ2  cid:8   cid:12  c · r :: κ2  .  c · ε  cid:2  c  c ·  l p   cid:2   c · l  · p c ·  r p   cid:2   c · r  · p  If  cid:8 , u :: κ  cid:12  c :: κr, then the patched kind κ{r := c} is the kind κ1 × S c :: κr . It has the property that   cid:8 , u :: κ{r := c}  cid:12 u · r ≡ c :: κr.  Deﬁne  cid:8   cid:12  κ{p := c} kind, where  cid:8   cid:12  κ kind,  cid:8 , u :: κ  cid:12  u · p :: κp, and  cid:8   cid:12  c :: κc, to be such that  cid:8   cid:12  κ{p := c} <:: κ and  cid:8 , u :: κ{p := c}  cid:12 u · p ≡ c :: κc. 43.4. Patching is used to constrain a component of a kind to be equivalent to a speciﬁed constructor. A sharing speciﬁcation imposed on a kind ensures that a deﬁnitional equality holds of any constructor of that kind. Informally, the kind u::κ  u · p ≡ u · q is a subkind κ   cid:5  of κ such that   cid:8 , u :: κ   cid:5   cid:12  u · p ≡ u · q :: κ   cid:5  cid:5   .   43.18    408  Singleton Kinds and Subkinding  For example, the kind u :: T × T   u · l ≡ u · r classiﬁers pairs of types whose left and right components are deﬁnitionally equal. Suppose that  cid:8   cid:12  κ kind is a well-formed kind and that  cid:8 , u :: κ  cid:12  u · p :: κp and  cid:8 , u :: κ  cid:12  u · q :: κq are well-formed paths. Deﬁne  cid:8   cid:12  u :: κ   p ≡ q kind  cid:5  speciﬁed by Equation  43.18 . Hint: Make use of the answer to to be the kind κ Exercise 43.3.   44  Type Abstractions and Type Classes  An interface is a contract that speciﬁes the rights of a client and the responsibilities of an implementor. Being a speciﬁcation of behavior, an interface is a type. In principle, any type may serve as an interface, but in practice it is usual to structure code into modules consisting of separable and reusable components. An interface speciﬁes the behavior of a module expected by a client and imposed on the implementor. It is the fulcrum balancing the tension between separation and integration. As a rule, a module ought to have a well- deﬁned behavior that can be understood separately, but it is equally important that it be easy to combine modules to form an integrated whole.  A fundamental question is, what is the type of a module? That is, what form should an interface take? One long-standing idea is that an interface is a labeled tuple of functions and procedures with speciﬁed types. The types of the ﬁelds of the tuple are often called function headers, because they summarize the call and return types of each function. Using interfaces of this form is called procedural abstraction, because it limits the dependencies between modules to a speciﬁed set of procedures. We may think of the ﬁelds of the tuple as being the instruction set of a virtual machine. The client makes use of these instructions in its code, and the implementor agrees to provide their implementations.  The problem with procedural abstraction is that it does not provide as much insulation as one might like. For example, a module that implements a dictionary must expose in the types of its operations the exact representation of the tree as, say, a recursive type  or, in more rudimentary languages, a pointer to a structure that itself may contain such pointers . Yet the client ought not depend on this representation: the purpose of abstraction is to get rid of pointers. The solution, as discussed in Chapter 17, is to extend the abstract machine metaphor to allow the internal state of the machine to be hidden from the client. In the case of a dictionary, the representation of the dictionary as a binary search tree is hidden by existential quantiﬁcation. This concept is called type abstraction, because the type of the underlying data  state of the abstract machine  is hidden.  Type abstraction is a powerful method for limiting the dependencies among the modules that constitute a program. It is very useful in many circumstances but is not universally applicable. It is often useful to expose, rather than to obscure, type information across a module boundary. A typical example is the implementation of a dictionary, which is a mapping from keys to values. To use, say, a binary search tree to implement a dictionary, we require that the key type admit a total ordering with which keys can be compared. The dictionary abstraction does not depend on the exact type of the keys but only requires that the key type be constrained to provide a comparison operation. A type class is a speciﬁcation of   410  Type Abstractions and Type Classes  such a requirement. The class of comparable types, for example, speciﬁes a type t together with an operation leq of type  t × t  → bool with which to compare them. Superﬁcially, such a speciﬁcation looks like a type abstraction, because it speciﬁes a type and one or more operations on it, but with the important difference that the type t is not hidden from the client. For if it were, the client would only be able to compare keys using leq but would have no means of obtaining keys to compare. A type class, in contrast to a type abstraction, is not intended to be an exhaustive speciﬁcation of the operations on a type, but rather a constraint on its behavior expressed by demanding that certain operations, such as comparison, be available, without limiting the other operations that might be deﬁned on it.  Type abstractions and type classes are the extremal cases of a general concept of module type that we shall discuss in detail in this chapter. The crucial idea is the controlled revelation of type information across module boundaries. Type abstractions are opaque; type classes are transparent. These are both instances of translucency, which arises from combining existential types  Chapter 17 , subtyping  Chapter 24 , and singleton kinds and subkinding  Chapter 43 . Unlike in Chapter 17, however, we will distinguish the types of modules, which are called signatures, from the types of ordinary values. The distinction is not essential, but it will be helpful to keep the two concepts separate at the outset, deferring discussion of how to ease the segregation once the basic concepts are in place.  44.1 Type Abstraction  Type abstraction is captured by a form of existential type quantiﬁcation similar to that described in Chapter 17. For example, a dictionary with keys of type τkey and values of type τval implements the signature σdict deﬁned by  cid:3 t :: T ; τdict cid:4 , where τdict is the labeled tuple type   cid:24 emp  cid:9 → t , ins  cid:9 → τkey × τval × t → t , fnd  cid:9 → τkey × t → τval opt cid:25 .  The type variable t occurring in τdict and bound by σdict is the abstract type of dictionaries on which are deﬁned three operations emp, ins, and fnd with the speciﬁed types. The type τval is immaterial to the discussion, because the dictionary operations impose no restrictions on the values that are associated to keys. However, it is important that the type τkey be some ﬁxed type, such as str, equipped with a suite of operations, such as comparison. Observe that the signature σdict merely speciﬁes that a dictionary is a value of some type that admits the operations emp, ins, and fnd with the types given by τdict. An implementation of the signature σdict is a structure Mdict of the form  cid:3 ρdict ; edict cid:4 , where ρdict is some concrete representation of dictionaries, and edict is a labeled tuple of type [ρdict t]τdict of the general form   cid:24 emp  cid:9 → . . . , ins  cid:9 → . . . , fnd  cid:9 → . . . cid:25 .   411  44.1 Type Abstraction  The elided parts implement the dictionary operations in terms of the chosen representation type ρdict, making use of the comparison operation that we assume is available of values of type τkey. For example, the type ρdict might be a recursive type deﬁning a balanced binary search tree, such as a red-black tree. The dictionary operations work on the underlying representation of the dictionary as such a tree, just as would a package of existential type  see Chapter 17 . The supposition about τkey is temporary and is lifted in Section 44.2.  To ensure that the representation of the dictionary is hidden from a client, the structure  Mdict is sealed with the signature σdict to obtain the module  Mdict  cid:9  σdict.  The effect of sealing is to ensure that the only information about Mdict that propagates to the client is given by σdict. In particular, because σdict only speciﬁes that the type t have kind T, no information about the choice of t as ρdict in Mdict is made available to the client. A module is a two-phase object consisting of a static part and a dynamic part. The static part is a constructor of a speciﬁed kind; the dynamic part is a value of a speciﬁed type. There are two elimination forms that extract the static and dynamic parts of a module. These are, respectively, a form of constructor and a form of expression. More precisely, the constructor M · s stands for the static part of M, and the expression M · d stands for its dynamic part. According to the inversion principle, if a module M has introduction form, then M · s should be equivalent to the static part of M. So, for example, Mdict · s should be equivalent to ρdict. But consider the static part of a sealed module, which has the form  Mdict  cid:9  σdict  · s. Because sealing hides the representation of an abstract type, this constructor should not be dict is another implementation of σdict, should  Mdict  cid:9  σdict  · s be  cid:5  equivalent to ρdict. If M  cid:9  σdict ·s? To ensure reﬂexivity of type equivalence, this equation should  cid:5  equivalent to  M  cid:5  are equivalent modules. But this violates representation independence dict hold when M and M for abstract types by making equivalence of abstract types sensitive to their implementation. It would seem, then, that there is a contradiction between two very fundamental concepts, type equivalence and representation independence. The way out of this conundrum is to disallow reference to the static part of a sealed module: the type expression M  cid:9  σ · s is deemed ill-formed. More generally, the formation of M · s is disallowed unless M is a module value, whose static part is always manifest. An explicit structure is a module value, as is any module variable  provided that module variables are bound by-value .  One effect of this restriction is that sealed modules must be bound to a variable before they are used. Because module variables are bound by-value, doing so has the effect of imposing abstraction at the binding site. In fact, we may think of sealing as a kind of computational effect that “occurs” at the binding site, much as the bind operation in Algol discussed in Chapter 34 engenders the effects induced by an encapsulated command. As a consequence two bindings of the same sealed module result in two abstract types. The type system willfully ignores the identity of the two occurrences of the same module in order to ensure that their representations can be changed independently of one another without disrupting the behavior of any client code  because the client cannot rely on their identity, it must regard them as different .   412  Type Abstractions and Type Classes  44.2 Type Classes  Type abstraction is an essential tool for limiting dependencies among modules in a program. The signature of a type abstraction determines all that is known about a module by a client; no other uses of the values of an abstract type are permissible. A complementary tool is to use a signature to partially specify the capabilities of a module. Such a signature is a type class, or aview; an instance of the type class is an implementation of it. Because the signature of a type class only constrains the minimum capabilities of an unknown module, there must be some other means of working with values of that type. The way to achieve this is to expose, rather than to hide, the identity of the static part of a module. In this sense, type classes are the “opposite” of type abstractions, but we shall see below that there is a smooth progression between them, mediated by a subsignature judgment.  Let us consider the implementation of dictionaries as a client of the implementation of its keys. To implement a dictionary using a binary search tree, the only requirement is that keys come equipped with a total ordering given by a comparison operation. This requirement can be expressed by a signature σord given by   cid:3 t :: T ;  cid:24 leq  cid:9 →  t × t  → bool cid:25  cid:4  .  Because a given type can be ordered in many ways, it is essential that the ordering be packaged with the type to determine a type of keys.  The implementation of dictionaries as binary search trees takes the form  X : σord  cid:12  M X  bstdict : σ X  dict.   cid:5    cid:6   Here σ X  t :: T ; τ X dict  , whose body, τ X  dict, is the tuple type  dict is the signature  cid:24 emp  cid:9 → t , ins  cid:9 → X · s × τval × t → t , fnd  cid:9 → X · s × t → τval opt cid:25 , bstdict is a structure  not given explicitly here  that implements the dictionary op- and M X erations using binary search trees.1 Within M X bstdict, the static and dynamic parts of the module X are accessed by writing X· s and X· d, respectively. In particular, the comparison operation on keys is accessed by the projection X · d · leq.  The declared signature of the module variable X expresses a constraint on the capabilities of a key type by specifying an upper bound on its signature in the subsignature ordering. So any module bound to X must provide a type of keys and a comparison operation on that type, but nothing else is assumed of it. Because this is all we know about the unknown module X, the dictionary implementation is constrained to rely only on these speciﬁed capabilities, and no others. When linking with a module deﬁning X, the implementation need not be sealed with this signature but must instead have a signature that is no larger than it in the subsignature relation. Indeed, the signature σord is useless for sealing, as is easily seen by example. Suppose that Mnatord : σord is an instance of the class of ordered types under the usual ordering. If we seal Mnatord with σord by writing  Mnatord  cid:9  σord,   413  44.2 Type Classes  the resulting module is useless, because we would then have no way to create values of the key type.  We see, then, that a type class is a description  or view  of a pre-existing type and is not a means of introducing a new type. Rather than obscure the identity of the static part of Mnatord, we wish to propagate its identity as nat while specifying a comparison with which to order them. Type identity propagation is achieved using singleton kinds  as described in Chapter 43 . Speciﬁcally, the most precise, or principal, signature of a structure is the one that exposes its static part using a singleton kind. In the case of the module Mnatord, the principal signature is the signature σnatord given by   cid:3 t :: S nat  ;leq  cid:9 →  t × t  → bool cid:4  ,  which, by the rules of equivalence  deﬁned formally in Section 44.3 , is equivalent to the signature   cid:3  :: S nat  ; leq  cid:9 →  nat × nat  → bool cid:4  .  The derivation of such an equivalence is called equivalence propagation, because it propa- gates the identity of the type t into its scope.  The dictionary implementation M X  bstdict expects a module X with signature σord, but the module Mnatord provides the signature σnatord. Applying the rules of subkinding given in Chapter 43, together with the covariance principle for signatures, we obtain the subsignature relationship  σnatord <: σord.  By the subsumption principle, a module of signature σnatord may be provided when a module of signature σord is required. Therefore, Mnatord may be linked to X in M X  Combining subtyping with sealing provides a smooth gradation between type classes  bstdict.  and type abstractions. The principal signature for M X  cid:5   bst  ; cid:24 emp  cid:9 → t , ins  cid:9 → X · s × τval × t → t , fnd  cid:9 → X · s × t → τval opt cid:25  cid:6   bstdict is the signature ρX  dict given by  t :: S τ X  ,  bst is the type of binary search trees with keys given by the module X of signature  where τ X σord. This signature is a subsignature of σ X  dict given earlier, so that the sealed module  M X  bstdict   cid:9  σ X  dict  is well-formed and has type σ X abstraction.  dict, which hides the representation type of the dictionary  After linking X to Mnatord, the signature of the dictionary is specialized by propagating the identity of the static part of Mnatord using the subsignature judgment. As remarked earlier, the dictionary implementation satisﬁes the typing  But because σnatord <: σord, we have, by contravariance, that  X : σord  cid:12  M X  bstdict : σ X  dict.  X : σnatord  cid:12  M X  bstdict : σ X  dict.   414  Type Abstractions and Type Classes  is also a valid typing judgment. If X : σnatord, then X · s is equivalent to nat, because it has kind S nat , so that the typing  X : σnatord  cid:12  M X  bstdict : σnatdict  is also valid. The closed signature σnatdict is given explicitly by   cid:3 t :: T ;  cid:24 emp  cid:9 → t , ins  cid:9 → nat × τval × t → t , fnd  cid:9 → nat × t → τval opt cid:25  cid:4  .  The representation of dictionaries is hidden, but the representation of keys as natural numbers is not. The dependency on X has been eliminated by replacing all occurrences of X · s within σ X dict by the type nat. Having derived this typing we may link X with Mnatord as described in Chapter 42 to obtain a composite module, Mnatdict, of signature σnatdict, in which keys are natural numbers ordered as speciﬁed by Mnatord.  It is convenient to exploit subtyping for labeled tuple types to avoid creating an ad hoc module specifying the standard ordering on the natural numbers. Instead we can extract the required module directly from the implementation of the abstract type of numbers using subsumption. As an illustration, let Xnat be a module variable of signature σnat, which has the form   cid:3 t :: T ;  cid:24 zero  cid:9 → t , succ  cid:9 → t → t , leq  cid:9 →  t × t  → bool , . . . cid:25  cid:4   The ﬁelds of the tuple provide all and only the operations that are available on the abstract type of natural numbers. Among them is the comparison operation leq, which is required by the dictionary module. Applying the subtyping rules for labeled tuples given in Chapter 24, together with the covariance of signatures, we obtain the subsignature relationship  σnat <: σord,  so that by subsumption the variable Xnat may be linked to the variable X postulated by the dictionary implementation. Subtyping takes care of extracting the required leq ﬁeld from the abstract type of natural numbers, demonstrating that the natural numbers are an instance of the class of ordered types. Of course, this approach only works if we wish to order the natural numbers in the natural way provided by the abstract type. If, instead, we wish to use another ordering, then we must construct instances of σord “by hand” to deﬁne the appropriate ordering.  44.3 A Module Language  The module language Mod formalizes the ideas outlined in the preceding section. The syntax is divided into ﬁve levels: expressions classiﬁed by types, constructors classiﬁed by kinds, and modules classiﬁed by signatures. The expression and type level consists of various language mechanisms described earlier in this book, including at least product, sum, and partial function types. The constructor and kind level is as described in Chapters 18   415  44.3 A Module Language  and 43, with singleton and dependent kinds. The following grammar summarizes the syntax of modules.  ::= sig{κ} t.τ   σ  Sig Mod M ::= X  str c;e  seal{σ} M  let{σ} M1; X.M2   Con Exp  c e  ::= stat M  ::= dyn M   X   cid:3 t :: κ ; τ cid:4   cid:3 c ; e cid:4  M  cid:9  σ  let X be M1 in M2  : σ M · s M · d  signature variable structure seal deﬁnition static part dynamic part  The statics of Mod consists of the following forms of judgment:   cid:7   cid:12  σ sig  cid:7   cid:12  σ1 ≡ σ2  cid:7   cid:12  σ1 <: σ2  cid:7   cid:12  M : σ  cid:7   cid:12  M val  cid:7   cid:12  e val  well-formed signature equivalent signatures subsignature well-formed module module value expression value  X : σ, X val u :: κ x : τ, x val  module value variable constructor variable expression value variable  Rather than segregate hypotheses into zones, we instead admit the following three forms of hypothesis groups:  It is important that module and expression variables are always regarded as values to ensure that type abstraction is properly enforced. Correspondingly, each module and expression variable appears in  cid:7  paired with the hypothesis that it is a value. As a notational con- venience, we will not explicitly state the value hypotheses associated with module and expression variables, under the convention that all such variables implicitly come paired with such an assumption.  The following rules deﬁne the formation, equivalence, and subsignature judgments.   cid:7   cid:12  κ kind  cid:7 , u :: κ  cid:12  τ type   cid:7   cid:12   cid:3 u :: κ ; τ cid:4  sig   cid:7   cid:12  κ1 ≡ κ2  cid:7 , u :: κ1  cid:12  τ1 ≡ τ2  cid:7   cid:12   cid:3 u :: κ1 ; τ1 cid:4  ≡  cid:3 u :: κ2 ; τ2 cid:4   cid:7   cid:12  κ1 <:: κ2  cid:7 , u :: κ1  cid:12  τ1 <: τ2  cid:7   cid:12   cid:3 u :: κ1 ; τ1 cid:4  <:  cid:3 u :: κ2 ; τ2 cid:4    44.1a    44.1b    44.1c   Most important, signatures are covariant in both the kind and type positions: subkinding and subtyping are preserved by the formation of a signature. It follows from rule  44.1b  that   cid:3 u :: S c  ;τ  cid:4  ≡  cid:3  :: S c  ; [c u]τ cid:4    416  Type Abstractions and Type Classes  and, further, it follows from rule  44.1c  that   cid:3  :: S c  ; [c u]τ cid:4  <:  cid:3  :: T ; [c u]τ cid:4   and so  It is also the case that   cid:3 u :: S c  ;τ  cid:4  <:  cid:3  :: T ; [c u]τ cid:4 .   cid:3 u :: S c  ;τ  cid:4  <:  cid:3 u :: T ; τ cid:4 .  But the two supersignatures of  cid:3 u :: S c  ;τ  cid:4  are incomparable with respect to the subsig- nature judgment.  The statics of expressions of Mod is given by the following rules:   cid:7 , X : σ  cid:12  X : σ   cid:7   cid:12  c :: κ  cid:7   cid:12  e : [c u]τ  cid:7   cid:12   cid:3 c ; e cid:4  :  cid:3 u :: κ ; τ cid:4   cid:7   cid:12  σ sig  cid:7   cid:12  M : σ   cid:7   cid:12  M  cid:9  σ : σ   cid:7   cid:12  σ sig  cid:7   cid:12  M1 : σ1  cid:7 , X : σ1  cid:12  M2 : σ   cid:7   cid:12   let X be M1 in M2  : σ : σ   cid:7   cid:12  M : σ  cid:7   cid:12  σ <: σ   cid:5    cid:7   cid:12  M : σ   cid:5    44.2a    44.2b    44.2c    44.2d    44.2e   In rule  44.2b , it is always possible to choose κ to be the most speciﬁc kind of c in the subkind ordering, which uniquely determines c up to constructor equivalence. For such a choice, the signature  cid:3 u :: κ ; τ cid:4  is equivalent to  cid:3  :: κ ; [c u]τ cid:4 , which propagates the identity of the static part of the module expression into the type of its dynamic part. Rule  44.2c  is used together with the subsumption  rule  44.2e   to ensure that M has the speciﬁed signature.  The need for a signature annotation on a module deﬁnition is a manifestation of the avoidance problem. Rule  44.2d  would be perfectly sensible were the signature σ omitted from the syntax of the deﬁnition. However, omitting this information greatly complicates type checking. If σ were omitted from the syntax of the deﬁnition, the type checker would be required to ﬁnd a signature σ for the body of the deﬁnition that avoids the module variable X. Inductively, we may suppose that we have found a signature σ1 for the module M1, and a signature σ2 for the module M2, under the assumption that X has signature σ1. To ﬁnd a signature for an unadorned deﬁnition, we must ﬁnd a supersignature σ of σ2 that avoids X. To ensure that all possible choices of σ are accounted for, we seek the least  most precise  such signature with respect to the subsignature relation; this is called the principal signature of a module. The problem is that there may not be a least supersignature of a given signature that avoids a speciﬁed variable.  Consider the example above of a signature with two incomparable supersignatures. The example can be chosen so that the supersignatures   417  44.3 A Module Language  avoid a variable X that occurs in the subsignature.  Consequently, modules do not have principal signatures, a signiﬁcant complication for type checking. To avoid this problem, we insist that the avoiding supersignature σ be given by the programmer so that the type checker is not required to ﬁnd one. Modules give rise to a new form of constructor expression, M · s, and a new form of value expression, M · d. These operations, respectively, extract the static and dynamic parts of the module M. Their formation rules are as follows:  Rule  44.3a  requires that the module expression M be a value according to the following rules:   cid:7   cid:12  M val  cid:7   cid:12  M :  cid:3 u :: κ ; τ cid:4    cid:7   cid:12  M · s :: κ   cid:7   cid:12  M :  cid:3  :: κ ; τ cid:4    cid:7   cid:12  M · d : τ   cid:7 , X : σ, X val  cid:12  X val   cid:7   cid:12  e val  cid:7   cid:12   cid:3 c ; e cid:4  val   44.3a    44.3b    44.4a    44.4b    It is not strictly necessary to insist that the dynamic part of a structure be a value for the structure to itself be a value.   Rule  44.3a  speciﬁes that only structure values have well-deﬁned static parts, and hence precludes reference to the static part of a sealed structure, which is not a value. This property ensures representation independence for abstract types, as discussed in Section 44.1. For if M · s were admissible when M is a sealed module, it would be a type whose identity depends on the underlying implementation, in violation of the abstraction principle. Module variables are, on the other hand, values, so that if X :  cid:3 t :: T ; τ cid:4  is a module variable, then X · s is a well-formed type. What this means in practice is that sealed modules must be bound to variables before they can be used. It is for this reason that we include deﬁnitions among module expressions.  Rule  44.3b  requires that the signature of the module, M, be non-dependent, so that the result type, τ, does not depend on the static part of the module. This independence may not always be the case. For example, if M is a sealed module, say, N  cid:9  cid:3 t :: T ; t cid:4  for some module N, then projection M · d is ill-formed. For if it were well-formed, its type would be M · s, which would violate representation independence for abstract types. But if M is a module value, then it is always possible to derive a non-dependent signature for it, provided that we include the following rule of self-recognition:   cid:7   cid:12  M :  cid:3 u :: κ ; τ cid:4   cid:7   cid:12  M val  cid:7   cid:12  M :  cid:3 u :: S M · s :: κ  ; τ cid:4    44.5   This rule propagates the identity of the static part of a module value into its signature. The dependency of the type of the dynamic part on the static part is then eliminable by sharing propagation.   418  Type Abstractions and Type Classes  The following rule of constructor equivalence states that a type projection from a module  value is eliminable:   cid:7   cid:12   cid:3 c ; e cid:4  :  cid:3 t :: κ ; τ cid:4   cid:7   cid:12   cid:3 c ; e cid:4  val   cid:7   cid:12   cid:3 c ; e cid:4  · s ≡ c :: κ  The requirement that the expression e be a value, which is implicit in the second premise of the rule, is not strictly necessary but does no harm. A consequence is that apparent dependencies of closed constructors  or kinds  on modules may always be eliminated. In particular, the identity of the constructor  cid:3 c ; e cid:4 · s is independent of e, as would be expected if representation independence is to be assured. The dynamics of modules is given as follows: e  cid:20 −→ e   cid:5    44.6    44.7a    44.7b    cid:3 c ; e cid:4   cid:20 −→  cid:3 c ; e   cid:5  cid:4   e val   cid:3 c ; e cid:4  · d  cid:20 −→ e  There is no need to evaluate constructors at run-time, because the dynamics of expressions does not depend on their types. It is not difﬁcult to prove type safety for this dynamics relative to the foregoing statics.  44.4 First- and Second-Class  It is common to draw a distinction between ﬁrst-class and second-class modules based on whether signatures are types, and hence whether modules are just a form of expression like any other. When modules are ﬁrst-class, their values can depend on the state of the world at run-time. When modules are second-class signatures are a separate form of classiﬁer from types, and module expressions may not be used in the same way as ordinary expressions. For example, it may not be possible to compute a module based on the phase of the moon.  Superﬁcially, it seems as though ﬁrst-class modules are uniformly superior to second- class modules, because you can do more with them. But on closer examination, we see that the “less is more” principle applies here as well, much as in the distinction between dynamic and static languages discussed in Chapters 22 and 23. In particular, if modules are ﬁrst-class, then one must adopt a “pessimistic” attitude towards expressions that compute them, precisely because they represent fully general, even state-dependent, computations. One consequence is that it is difﬁcult, or even impossible, to track the identity of the static part of a module during type checking. A general module expression need not have a well- deﬁned static component, precluding its use in type expressions. Second-class modules, on the other hand, can be permissive with the use of the static components of modules in types, precisely because the range of possible computations is reduced. In this respect, second-class modules are more powerful than ﬁrst-class, despite initial impressions. More   419  44.5 Notes  importantly, a second-class module system can always be enriched to allow ﬁrst-class modules, without requiring that they be ﬁrst-class. Thus, we have the best of both worlds: the ﬂexibility of ﬁrst-class modules and the precision of second-class modules. In short, you pay for only what you use: if you use ﬁrst-class capabilities, you should expect to pay a cost, but if you do not, you should not be taxed on the unrealized gain.  First-class modules are added to Mod in the following way. First, enrich the type system with existential types, as described in Chapter 17, so that “ﬁrst-class modules” are just packages of existential type. A second-class module M of signature  cid:3 t :: κ ; τ cid:4  is made ﬁrst-class by forming the package pack M · s with M · d as∃ t.τ  of type ∃ t :: κ.τ consisting of the static and dynamic parts of M. Second, to allow packages to act like modules, we introduce the module expression open e that opens the contents of a package as a module:   cid:7   cid:12  e : ∃ t :: κ.τ   cid:7   cid:12  open e :  cid:3 t :: κ ; τ cid:4    44.8   Because the package e is an arbitrary expression of existential type, the module expression open e may not be regarded as a value, and hence does not have a well-deﬁned static part. Instead, we must generally bind it to a variable before it is used, mimicking the composite behavior of the existential elimination form given in Chapter 17.  44.5 Notes  The use of dependent types to express modularity was ﬁrst proposed by MacQueen  1986 . Later studies extended this proposal to model the phase distinction between compile- and run-time  Harper et al., 1990  and to account for type abstraction as well as type classes  Harper and Lillibridge, 1994; Leroy, 1994 . The avoidance problem was ﬁrst isolated by Castagna and Pierce  1994  and by Harper and Lillibridge  1994 . It has come to play a central role in subsequent work on modules, such as Lillibridge  1997  and Dreyer  2005 . The self-recognition rule was introduced by Harper and Lillibridge  1994  and by Leroy  1994 . That rule was later identiﬁed as a manifestation of higher- order singletons  Stone and Harper, 2006 . A consolidation of these ideas is used as the foundation for a mechanization of the meta-theory of modules  Lee et al., 2007 . A thorough summary of the main issues in module system design is given in Dreyer  2005 .  The presentation given here focuses attention on the type structure required to support modularity. An alternative formulation uses elaboration, a translation of modularity con- structs into more primitive notions, such as polymorphism and higher-order functions. The Deﬁnition of Standard ML  Milner et al., 1997  pioneered the elaboration approach. Building on earlier work of Russo, a more rigorous type-theoretic formulation was given by Rossberg et al.  2010 . The advantage of the elaboration-based approach is that it can make do with a simpler type theory as the target language but at the expense of making the explanation of modularity more complex.   420  Type Abstractions and Type Classes  Exercises  44.1. Consider the type abstraction σset of ﬁnite sets of elements of type τelt given by the  following equations:  σset  cid:2   cid:3 t :: T ; τset cid:4  τset  cid:2   cid:24 emp  cid:9 → t , ins  cid:9 → τelt × t → t , mem  cid:9 → τelt × t → bool cid:25 .  Deﬁne an implementation   cid:7 , D : σdict  cid:12  Mset : σset  of ﬁnite sets of elements in terms of a dictionary whose key and value types are chosen appropriately.  44.2. Fix an ordered type τnod of nodes, and consider the type abstraction σgrph of ﬁnite  graphs given by the following equations: σgrph  cid:2   cid:3 tgrph :: T ;  cid:3 tedg :: S τedg  ; τgrph cid:4  cid:4  τedg  cid:2  τnod × τnod τgrph  cid:2   cid:24 emp  cid:9 → tgrph , ins  cid:9 → τedg × tgrph → tgrph , mem  cid:9 → τedg × tgrph → bool cid:25 . The signature σgrph is translucent, with both opaque and transparent type components: graphs themselves are abstract, but edges are pairs of nodes.  Deﬁne an implementation  N : σord, S : σnodset, D : σnodsetdict  cid:12  Mgrph : σgrph  in terms of an implementation of nodes, sets of nodes, and a dictionary mapping nodes to sets of nodes. Represent the graph by a dictionary assigning to each node the set of nodes incident upon it. Deﬁne the node type τnod to be the type N · s, and choose the signatures of the set and dictionary abstractions appropriately in terms of this choice of node type.  44.3. Deﬁne signature modiﬁcation, a variant of kind modiﬁcation deﬁned in Exercise 43.3, in which a deﬁnition of a constructor component can be imposed on a signature. Let P stand for a composition of static and dynamic projections of the form · d . . . · d· s, so that X· P stands for X · d . . . · d· s. Assume that  cid:7   cid:12  σ sig,  cid:7 , X : σ  cid:12  X · P :: κ, and  cid:7   cid:12  c :: κ. Deﬁne signature σ{P := c} such that  cid:7   cid:12  σ{P := c} <: σ and  cid:7 , X : σ{P := c}  cid:12  X · P ≡ c :: κ.  44.4. The signature σgrph is a subsignature  instance  of the type class σgrphcls  cid:2   cid:3 tgrph :: T ;  cid:3 tedg :: T ; τgrph cid:4  cid:4   in which the deﬁnition of tedg has been made explicit as the product of two nodes. Check that  cid:7   cid:12  σgrph ≡ σgrphcls{ · d · s := τnod × τnod}, so that the former can be deﬁned as the latter.   421  Note  Note  1 Here and elsewhere in this chapter and the next, the superscript X serves as a reminder that the  module variable X may occur free in the annotated module or signature.   45  Hierarchy and Parameterization  To be adequately expressive, it is essential that a module system support module hierarchies. Hierarchical structure arises naturally in programming, both as an organizational device for partitioning of a large program into manageable pieces, and as a localization device that allows one type abstraction or type class to be layered on top of another. In such a scenario, the lower layer plays an auxiliary role relative to the upper layer, and we may think of the upper layer as being abstracted over the lower in the sense that any implementation of the lower layer induces an instance of the upper layer corresponding to that instance. The pattern of dependency of one abstraction on another is captured by an abstraction mechanism that allows the implementation of one abstraction to be considered a function of the implementation of another. Hierarchies and abstraction work in tandem to offer an expressive language for organizing programs.  45.1 Hierarchy  It is common in modular programming to layer a type class or a type abstraction on top of a type class. For example, the class of equality types, which are those that admit a boolean equivalence test, is described by the signature σeq deﬁned as follows:   cid:3 t :: T ;  cid:24 eq  cid:9 →  t × t  → bool cid:25  cid:4  .  Instances of this class consist of a type together with a binary equality operation deﬁned on it. Such instances are modules with a subsignature of σeq; the signature σnateq given by   cid:3 t :: S nat  ;  cid:24 eq  cid:9 →  t × t  → bool cid:25  cid:4  is one example. A module value of this signature has the form   cid:3 nat ;  cid:24 eq  cid:9 → . . . cid:25  cid:4  ,  where the elided expression implements an equivalence relation on the natural numbers. All other instance values of the class σeq have a similar form, differing in the choice of type, and or the choice of comparison operation.  The class of ordered types are an extension of the class of equality types with a binary operation for the  strict  comparison of two elements of that type. One way to formulate this is as the signature   cid:3 t :: T ;  cid:24 eq  cid:9 →  t × t  → bool,lt  cid:9 →  t × t  → bool cid:25  cid:4  ,   423  45.1 Hierarchy  which is a subsignature of σeq according to the rules of subtyping given in Chapter 24. This relationship amounts to the requirement that every ordered type is a fortiori an equality type.  This situation is well and good, but it would be even better if there were a way to incrementally extend the equality type class to the ordered type class without having to rewrite the signature as we have done in the foregoing example. Instead, we would like to layer the comparison aspect on top of an equality type class to obtain the ordered type class. For this, we use a hierarchical signature σeqord of the form   cid:16   X : σeq . σ X  ord.  In this signature, we write σ X  ord for the signature   cid:3 t :: S X · s  ; cid:24 lt  cid:9 →  t × t  → bool cid:25  cid:4  ,  which refers to the static part of X, namely the type on which the equality relation is deﬁned. The notation σ X ord emphasizes that this signature has a free module variable X occurring within it, and hence is only meaningful in a context in which X has been declared. A value of the signature σeqord is a pair of modules,  cid:24 Meq ; Mord cid:25 , in which Meq comprises a type equipped with an equality relation on it, and the second comprises a type equipped with an ordering relation on it. Crucially, the second type is constrained by the singleton ord to be the same as the ﬁrst type. Such a constraint is a sharing speciﬁcation. kind in σ X The process of drawing out of the consequences of a sharing speciﬁcation is called sharing propagation.  Sharing propagation is achieved by combining subkinding  as described in Chapter 43  with subtyping for signatures. For example, a particular ordering Mnatord of the natural  numbers is a module with signature cid:16   X : σnateq . σ X  ord.  By covariance of the hierarchical signature, this signature is a subsignature of σeqord, so that by subsumption, we may regard Mnatord as a module of the latter signature. The static part of the subsignature is a singleton, so we may apply the rules of sharing propagation given in Chapter 43 to show that the subsignature is equivalent to the signature   cid:16   X : σnateq . σnatord,  where σnatord is the closed signature   cid:3 t :: S nat  ;  cid:24 lt  cid:9 →  t × t  → bool cid:25  cid:4  .  Notice that sharing propagation has replaced the type X · s in the signature with nat, eliminating the dependency on the module variable X. After another round of sharing propagation, this signature is equivalent to the signature ρnatord given by   cid:3  :: S nat  ; cid:24 lt  cid:9 →  nat × nat  → bool cid:25  cid:4  .  Here we have replaced both occurrences of t in the type of the comparison operation with nat as a consequence of the kind of t. The net effect is to propagate the identity of the static part of Mnatord to the signature of the second component of Mnatord.   424  Hierarchy and Parameterization  Although its value is a pair, which seems symmetric, a module of signature σeqord is asymmetric in that the signature of the second component is dependent on the ﬁrst component itself. The dependence is displayed by the occurrence of the module variable X in the signature σord. Thus, for  cid:24 Meq ; Mord cid:25  to be a well-formed module of signature σeqord, the ﬁrst component Meq must have signature σeq, which is meaningful independently of the other component of the pair. On the other hand, the second component, Mord, must have eq, with the understanding that X stands for the module Meq. In general, this signature σ X signature is not meaningful independently of Meq itself, and hence it may not be possible to handle Mord independently of Meq. Turning this the other way around, if M is any module of signature σeqord, then it is always sensible to project it onto its ﬁrst coordinate to obtain a module M · 1 of signature σeq. But it is not always sensible to project it onto its second coordinate, because it may not be possible to give a signature to M · 2 in the case that the dependency on the ﬁrst component cannot be resolved statically. This problem can arise if the M · 1 is a sealed module, whose static part cannot be formed in order to ensure representation independence. ord on the module variable X cannot be In such a situation, the dependence of the signature σ X eliminated, and so no signature can be given to the second projection. For this reason, the ﬁrst component of a module hierarchy is a submodule of the hierarchy, whereas the second component may or may not be a submodule of it. Put in other terms, the second component of a hierarchy is “projectible” exactly when the dependence of its signature on the ﬁrst component is eliminable by sharing propagation. That is, we may know enough about the ﬁrst component statically to ensure that an independent type for the second component can be given. In that case, the second component can be considered to be a submodule of the pair; otherwise, the second is inseparable from the ﬁrst, and therefore cannot be projected from the pair. Consider a module Mnatord of signature σnatord, which, we noted earlier, is a subsignature of σeqord. The ﬁrst projection Mnatord · 1 is a well-formed module of closed signature σeq and, hence, is a submodule of Mnatord. The situation is less clear for the second projection, Mnatord · 2, because its signature, σ X  cid:16  ord, depends on the ﬁrst component via the variable X. However, we noted above that the signature σnatord is equivalent to the signature  : σnateq . ρnatord  in which the dependency on X is eliminated by sharing propagation. This, too, is a valid signature for Mnatord, and hence, the second projection Mnatord · 2 is a well-formed module of closed signature ρnatord. Otherwise, if the only signature available for Mnatord were σeqord, then the second projection would be ill-formed—the second component would not be separable from the ﬁrst, and hence could not be considered a submodule of the pair.  The hierarchical dependency of the signature of the second component of a pair on the ﬁrst component gives rise to a useful alternative interpretation of a hierarchical module signature as describing a family of modules given by the second component thought of as being indexed by the ﬁrst component. In the case at hand, the collection of modules of the signature σeqord gives rise to a family of modules of signature σ X ord, where X ranges over σeq. That is, to each choice, Meq, of signature σeq, we associate the collection of choices Mord coherent with the ﬁrst choice in accordance with the sharing constraint in σ X ord, taking X to   425  45.2 Abstraction  be Mord. This collection is the ﬁber over Meq, and the collection of modules of signature σeqord is ﬁbered over σeq  by the ﬁrst projection .  The preceding example illustrates the layering of one type class on top of another. It is also useful to layer a type abstraction over a type class. A good example is given by a dictionary abstraction in which the type of keys is an instance of the class of ordered types but is otherwise unspeciﬁed. The signature σkeydict of such a dictionary is given as follows:   cid:16   K : σeqord . σ K  dict,  where σeqord is the signature of ordered equality types  in either of the two forms discussed above , and σ K  dict is the signature of dictionaries of some type τ given as follows:   cid:3 t :: T ;  cid:24 emp  cid:9 → t, ins  cid:9 → K · s × τ × t → t, fnd  cid:9 → K · s × t → τ opt cid:25  cid:4  .  The ins and fnd operations make use of the type K · s of keys given by the submodule of the dictionary module. We may think of σkeydict as specifying a family of dictionary modules, one for each choice of the ordered type of keys. Regardless of the interpretation, an implementation of the signature σkeydict consists of a two-level hierarchy of the form  cid:24 M1 ; M2 cid:25 , where M1 speciﬁes the key type and its ordering, and M2 implements the dictionary for keys of this type in terms of this ordering.  45.2 Abstraction  The signature σkeydict describes a family of dictionary modules indexed by a module of ordered keys. Such modules evaluate to pairs consisting of the ordered type of keys together with the dictionary per se, specialized to that choice of keys. Although it is possible that the code of the dictionary operations differs for each choice of keys, it is more often the case that the same implementation can be used for all choices of keys, the only difference being that references to, say, X · lt refers to a different function for each choice of key module X.  Such a uniform implementation of dictionaries is given by an abstracted module, or functor. A functor is a module expressed as a function of an unknown module of speciﬁed signature. The uniform dictionary module would be expressed as a functor abstracted over the module implementing keys, which is to say as a λ-abstraction  Mdictfun  cid:2  λ Z : σeqord . M Z  keydict.  Here M Z keydict is the generic implementation of dictionaries in terms of an unspeciﬁed module Z of signature σeqord. The signature of Z expresses the requirement that the dictionary implementation relies on the keys being an ordered type but makes no other requirement on it.  A functor is a form of module and hence has a signature as well, a functor signature.  The signature σdictfun of the functor M Z  keydict has the form   cid:15   Z : σeqord . ρZ  keydict,   426  Hierarchy and Parameterization  which speciﬁes that its domain is the signature, σeqord, of ordered types, and whose range keydict that depends on the module Z. Using a mild extension of the notation is a signature ρZ introduced in Exercise 44.3, we may deﬁne  ρkeydict  cid:2  σkeydict{ · 1 · 1 · s := Z · 1 · s}.  This deﬁnition ensures that the required sharing constraint between the key type in the result of the functor and the key type given as its argument.  The dictionary functor Mdictfun deﬁnes a generic implementation of dictionaries in terms of an ordered type of keys. An instance of the dictionary for a speciﬁc choice of keys is obtained by applying, or instantiating, it with a module of its domain signature σeqord. For example, because Mnatord, the type of natural numbers ordered in the usual way, is such a module, we may form the instance Mdictfun  Mnatord  to obtain a dictionary with numeric keys. By choosing other modules of signature σeqord, we may obtain corresponding instances of the dictionary functor. More generally, if M is any module of signature σdictfun, then it is a functor that we may apply it to any module Mkey of signature σeqord to obtain the instance M  Mkey .  But what is the signature of such an instance, and how may it be deduced? Recall that the result signature of σdictfun is dependent on the argument itself and not just its signature. It is therefore not immediately clear what signature to assign to the instance; the dependency on the argument must be resolved in order to obtain a signature that makes sense independently of the argument. The situation is broadly similar to the problem of computing the signature of the second component of a hierarchical module, and similar methods are used to resolve the dependencies, namely to exploit subtyping for signatures specialize the result signature according to the argument.  Let us consider an illustrative example. Note that by contravariance of subtyping for functor signatures, we may weaken a functor signature by strengthening its domain sig- nature. In the case of the signature σdictfun of the dictionary functor, we may obtain a supersignature σnatdictfun by strengthening its domain to require that the key type be the type of natural numbers:   cid:15   Z : σnatord . ρZ  keydict.  Fixing Z to be a module variable of the specialized signature σnatord, the range signature keydict is given by the modiﬁcation ρZ  σkeydict{ · 1 · 1 · s := Z · 1 · s}.  σkeydict{ · 1 · 1 · s := nat},  By sharing propagation, this is equivalent to the closed signature, ρnatdict, given by  because we may derive the equivalence of Z · 1 · s and nat once the signature of Z is specialized to σnatord.  Now by subsumption if M is a module of signature σdictfun, then M is also a module of  the supersignature   cid:15   Z : σnatord . ρZ  keydict.   427  45.3 Hierarchy and Abstraction  We have just shown that the latter signature is equivalent to the non-dependent functor signature   cid:15   : σnatord . ρnatdict.  The range is now given independently of the argument, so we may deduce that if Mnatkey has signature σnatord, then the application M  Mnatkey  has the signature ρnatdict.  The crucial point is that the dependence of the range signature on the domain signature is eliminated by propagating knowledge about the type components of the argument itself. Absent this knowledge, the functor application cannot be regarded as well-formed, much as the second projection from a hierarchy cannot be admitted if the dependency of its signature on the ﬁrst component cannot be eliminated. If the argument to the functor is a value, then it is always possible to ﬁnd a signature for it that maximizes the propagation of type sharing information so that the dependency of the range on the argument can always be eliminated.  45.3 Hierarchy and Abstraction  In this section, we sketch the extension of the module language introduced in Chapter 44 to account for module hierarchies and module abstraction.  The syntax of Mod is enriched with the following clauses:  σ  Sig  ::= sub σ1;X.σ2  fun σ1;X.σ2  Mod M ::= sub M1;M2    cid:14   cid:5   cid:24 M1 ; M2 cid:25  M · 1 M · 2  X : σ1 . σ2 X : σ1 . σ2  hierarchy functor hierarchy ﬁrst component second component  fst M  snd M  fun{σ} X.M  λ X : σ . M functor instance app M1;M2  M1  M2   The syntax of signatures is extended to include hierarchies and functors, and the syntax of modules is correspondingly extended with introduction and elimination forms for these signatures.  The judgment M projectible states that the module, M, isprojectible in the sense that its constituent types may be referenced by compositions of projections, including the static part of a structure. This judgment is inductively deﬁned by the following rules:   cid:7 , X : σ  cid:12  x projectible   cid:7   cid:12  M1 projectible  cid:7   cid:12  M2 projectible   cid:7   cid:12   cid:24 M1 ; M2 cid:25  projectible   45.1a    45.1b    428  Hierarchy and Parameterization   cid:7   cid:12  M projectible  cid:7   cid:12  M · 1 projectible  cid:7   cid:12  M projectible  cid:7   cid:12  M · 2 projectible  All module variables are considered projectible, even though this condition is only relevant for hierarchies of basic structures. Because the purpose of sealing is to hide the representa- tion of an abstract type, no sealed module is considered projectible. Furthermore, no functor is projectible, because there is no concept of projection for a functor. More importantly, no functor instance is projectible, which ensures that any two instances of the same functor deﬁne distinct abstract types. Functors are therefore generative.  See Section 45.4 for a discussion of an alternative treatment of functors.   The signature formation judgment is extended to include these rules:  Signature equivalence is deﬁned to be compatible with the two new forms of signature:  The subsignature judgment is augmented with the following rules:   cid:7   cid:12  σ1 sig  cid:7 , X : σ1  cid:12  σ2 sig  X : σ1 . σ2 sig   cid:7   cid:12  σ1 sig  cid:7 , X : σ1  cid:12  σ2 sig  X : σ1 . σ2 sig   cid:7   cid:12  cid:14   cid:7   cid:12  cid:5  X : σ1 . σ2 ≡ cid:14  X : σ1 . σ2 ≡ cid:5   cid:14   cid:5   X : σ1 . σ2 <:  X : σ1 . σ2 <:   cid:7   cid:12  σ1 ≡ σ   cid:7   cid:12  σ1 ≡ σ   cid:7   cid:12  cid:14   cid:7   cid:12  cid:5   cid:7   cid:12  cid:14   cid:7   cid:12  cid:5    cid:7   cid:12  σ   cid:5   cid:5  1 <: σ1  cid:7 , X : σ 1   cid:7   cid:12  σ1 <: σ  1  cid:7 , X : σ1  cid:12  σ2 ≡ σ  cid:5   cid:5  2  cid:5  . σ 2 1  cid:7 , X : σ1  cid:12  σ2 ≡ σ  cid:5   cid:5  . σ 2   cid:5  X : σ 1  X : σ   cid:5  1   cid:5  2  1  cid:7 , X : σ1  cid:12  σ2 <: σ  cid:5   cid:5  X : σ . σ 2  cid:12  σ2 <: σ  cid:5   cid:5  X : σ . σ 1 2   cid:5  1   cid:5  2   cid:5  2   cid:14    cid:7   cid:12  M1 : σ1  cid:7   cid:12  M2 : σ2  cid:14   cid:7   cid:12   cid:24 M1 ; M2 cid:25  : : σ1 . σ2  cid:14    cid:7   cid:12  M · 1 : σ1   cid:7   cid:12  M :  X : σ1 . σ2   cid:7   cid:12  M :  : σ1 . σ2   cid:7   cid:12  M · 2 : σ2  Rule  45.4a  speciﬁes that the hierarchical signature is covariant in both positions, whereas rule  45.4b  speciﬁes that the functor signature is contravariant in its domain and covariant in its range.  The statics of module expressions is extended by the following rules:   45.1c    45.1d    45.2a    45.2b    45.3a    45.3b    45.4a    45.4b    45.5a    45.5b    45.5c    429  45.4 Applicative Functors   cid:5   cid:7 , X : σ1  cid:12  M2 : σ2  cid:5   cid:7   cid:12  M1  M2  :σ   cid:7   cid:12  λ X : σ1 . M2 :  cid:7   cid:12  M1 :  X : σ1 . σ2 : σ2 . σ  cid:7   cid:12  M2 : σ2   45.5d    45.5e   Rule  45.5a  states that an explicit module hierarchy is given a signature in which there is no dependency of the signature of the second component on the ﬁrst component  indicated here by the underscore in place of the module variable . A dependent signature can be given to a hierarchy by sealing, which makes it into a non-value, even if the components are values. Rule  45.5b  states that the ﬁrst projection is deﬁned for general hierarchical signatures. On the other hand, rule  45.5c  restricts the second projection to non-dependent hierarchies, as discussed in the preceding section. Similarly, rule  45.5e  restricts instanti- ation to functors whose types are non-dependent, forcing any dependencies to be resolved using the subsignature relation and sharing propagation before application.  The self-recognition rules given in Chapter 44 are extended to account for the formation  of hierarchical module value by the following rules:   cid:14   cid:14  X : σ1 . σ2  cid:7   cid:12  M · 1 : σ  cid:7   cid:12  M projectible  cid:7   cid:12  M :  cid:14   cid:7   cid:12  M : X : σ  cid:14   cid:7   cid:12  M projectible  cid:7   cid:12  M :  cid:7   cid:12  M :   cid:5  1 : σ1 . σ2  cid:7   cid:12  M · 2 : σ  cid:5  : σ1 . σ 2  . σ2   cid:5  2   cid:5  1   45.6a    45.6b   Rules  45.6a  and  45.6b  allow the specialization of the signature of a hierarchical module value to express that its constructor components are equivalent to their projections from the module itself.  45.4 Applicative Functors  In the module language just described, functors are regarded as generative in the sense that any two instances, even with arguments, are considered to “generate” distinct abstract types. Generativity is achieved by treating a functor application M  M1  to be non-projectible, so that if it deﬁnes an abstract type in the result, that type cannot be referenced without ﬁrst binding the application to a variable. Any two such bindings are necessarily to distinct variables X and Y , and so the abstract types X · s and Y · s are distinct, regardless of their bindings.  The justiﬁcation for this design decision merits careful consideration. By treating functors as generative, we are ensuring that a client of the functor cannot in any way rely on the implementation of that functor. That is, we are extending the principle of representation independence for abstract types to functors in a natural way. A consequence of this policy is that the module language is compatible with extensions such as a conditional module that branches on an arbitrary dynamic condition that might even depend on external conditions   430  Hierarchy and Parameterization  such as the phase of the moon! A functor with such an implementation must be considered generative, because the abstract types arising from any instance cannot be regarded as well-deﬁned until the moment when the application is evaluated, which amounts to the point at which it is bound to a variable. By regarding all functors as generative we are, in effect, maximizing opportunities to exploit changes of representation without disrupting the behavior of clients of the functor, a bedrock principle of modular decomposition.  But because the module language considered in the preceding section does not include anything so powerful as a conditional module, we might consider that the restriction to generative functors is too severe and can be usefully relaxed. One such alternative is the concept of an applicative functor. An applicative functor is one for which instances by values are regarded as projectible:1  M projectible M1 val  M  M1  projectible   45.7   It is important that, because of this rule, applicative functors are not compatible with conditional modules. Thus, a module language based on applicative functors is inherently restricted as compared to one based on generative functors. The beneﬁt of considering a functor instance to be projectible is that we may form types such as  M  M1   · s, which projects the static part of the instance. But this raises the question. When are two such type expressions equivalent? The difﬁculty is that the answer to this question depends on the functor argument. For suppose that F is an applicative functor variable. Under what conditions should  F  M1  · s and  F  M2   · s be regarded as the same type? In the case of generative functors, we did not face this question, because the instances are not projectible, but for applicative functors the question cannot be dodged, but must be addressed. We will return to this point in a moment, after considering one further complication that raises a similar issue.  The difﬁculty is that the body of an applicative functor cannot be sealed to impose abstraction, and, according to the rules given in the preceding section, no sealed module is projectible. Because sealing is the only means of imposing abstraction, we must relax this condition and allow sealed projectible modules to be projectible:  M projectible M  cid:9  σ projectible   45.8  Thus, we may form type expressions of the form  M  cid:9  σ   · s, which project the static part of a sealed module. And once again we are faced with the issue that the equivalence of two such types must involve the equivalence of the sealed modules themselves, in seeming violation of representation independence.  Summarizing, if we are to treat functors as applicative, then some compromise of the principle of representation independence for abstract types is required. We must deﬁne equivalence for the static parts of sealed modules, and doing so requires at least checking whether the underlying modules are identical. Because the underlying modules have both static and dynamic parts, this means comparing their executable code for equivalence   431  Exercises  during type checking. More signiﬁcantly, because the formation of a client may depend on the equivalence of two modules, we cannot change the representation of a sealed module without fear of disrupting the typing or behavior of the client. But such a dependency undermines the very purpose of having a module system in the ﬁrst place!  Module hierarchies and functors in the form discussed here were introduced by Milner et al.  1997 , which also employed the reading of a module hierarchy as an indexed family of modules. The theory of hierarchies and functors was ﬁrst studied by Harper and Lillibridge  1994  and Leroy  1994 , building on earlier work by Mitchell and Plotkin  1988  on existential types. The concept of an applicative functor was introduced by Leroy  1995  and is central to the module system of O’Caml  2012 .  45.5 Notes  Exercises   cid:16   45.1. Consider the following signature σorddict of dictionaries equipped with their type of   cid:16   cid:2   cid:3 t :: T ;  cid:24 emp  cid:9 → t, ins  cid:9 → K · s × V · s × t → t, fnd  cid:9 → K · s × t → V · s opt cid:25  cid:4   ordered keys and their type of values: σorddict  cid:2  σ K,V dict σtyp  cid:2   cid:3 t :: T ; unit cid:4  .  V : σtyp . σ K,V dict  K : σord .  Deﬁne a functor Morddictfun that implements a dictionary in terms of an ordered type of keys and a type of values. It should have signature   cid:15    cid:16   σorddictfun  cid:2    cid:24 K ; V  cid:25  :  : σord . σtyp . σ K,V dict ,  where   cid:2  σorddict{ · 1 · s := K · s}{ · 2 · 1 · s := V · s}.  σ K,V dict  45.2. Deﬁne a signature σordset of ﬁnite sets that comes equipped with its own ordered type of elements. Deﬁne the signature σsetfun of a functor that implements this set abstraction in terms of an instance of the class of ordered types. Be sure to propagate type sharing information! Give a functor Msetfun with signature σsetfun implementing the ﬁnite set abstraction in terms of a given ordered element type. Hint: use the dictionary functor Mdictfun from Exercise 45.1.  45.3. Deﬁne a signature σordgrph of graphs that comes equipped with an ordered type of nodes. Deﬁne the signature of a functor that implements graphs in terms of an ordered type of nodes. Deﬁne a functor with this signature that implements graphs in terms   432  Hierarchy and Parameterization  of an ordered types of nodes. Hint: use the functor Mdictfun from Exercise 45.1 and the functor Msetfun of Exercise 45.2.  Note  1 We may also consider functor abstractions to be projectible, but because all variables are projectible.   P A R T XVIII  Equational Reasoning    46  Equality for System T  The beauty of functional programming is that equality of expressions in a functional language corresponds follows the familiar patterns of mathematical reasoning. For example, in the language T of Chapter 9 in which we can express addition as the function plus, the expressions  and  λ  x : nat  λ  y : nat  plus x  y   λ  x : nat  λ  y : nat  plus y  x   are equal. In other words, the addition function as programmed in T is commutative.  Commutativity of addition may seem self-evident, but why is it true? What does it mean for two expressions to be equal? These two expressions are not deﬁnitionally equal; their equality requires proof and is not merely a matter of calculation. Yet the two expressions are interchangeable because they given the same result when applied to the same number. In general, two functions are logically equivalent if they give equal results for equal arguments. Because this is all that matters about a function, we may expect that logically equivalent functions are interchangeable in any program. Thinking of the programs in which these functions occur as observations of their behavior, these functions are said to be observationally equivalent. The main result of this chapter is that observational and logical equivalence coincide for a variant of T in which the successor is evaluated eagerly, so that a value of type nat is a numeral.  46.1 Observational Equivalence  When are two expressions equal? Whenever we cannot tell them apart! It may seem tautological to say so, but it is not, because it all depends on what we consider to be a means of telling expressions apart. What “experiment” are we permitted to perform on expressions in order to distinguish them? What counts as an observation that, if different for two expressions, is a sure sign that they are different?  If we permit ourselves to consider the syntactic details of the expressions, then very few expressions could be considered equal. For example, if it is signiﬁcant that an ex- pression contains, say, more than one function application, or that it has an occurrence   436  Equality for System T  of λ-abstraction, then very few expressions would come out as equivalent. But such considerations seem silly, because they conﬂict with the intuition that the signiﬁcance of an expression lies in its contribution to the outcome of a computation, and not to the process of obtaining that outcome. In short, if two expressions make the same contribution to the outcome of a complete program, then they ought to be regarded as equal.  We must ﬁx what we mean by a complete program. Two considerations inform the deﬁnition. First, the dynamics of T is deﬁned only for expressions without free variables, so a complete program should clearly be a closed expression. Second, the outcome of a computation should be observable, so that it is evident whether the outcome of two computations differs or not. We deﬁne a complete program to be a closed expression of type nat and deﬁne the observable behavior of the program to be the numeral to which it evaluates.  An experiment on, or observation about, an expression is any means of using that expression within a complete program. We deﬁne an expression context to be an expression with a “hole” in it serving as a place-holder for another expression. The hole is permitted to occur anywhere, including within the scope of a binder. The bound variables within whose scope the hole lies are exposed to capture by the expression context. A program context is a closed expression context of type nat—that is, it is a complete program with a hole in it. The meta-variable C stands for any expression context. Replacement is the process of ﬁlling a hole in an expression context C with an expression e which is written C{e}. Importantly, the free variables of e that are exposed by C are captured by replacement  which is why replacement is not a form of substitution, which is deﬁned so as to avoid capture . If C is a program context, then C{e} is a complete program iff all free variables of e are captured by the replacement. For example, if C = λ  x : nat ◦, and e = x + x, then  C{e} = λ  x : nat  x + x.  The free occurrences of x in e are captured by the λ-abstraction as a result of the replacement of the hole in C by e. We sometimes write C{◦} to emphasize the occurrence of the hole in C. Expression contexts are closed under composition in that, if C1 and C2 are expression contexts, then so is  C{◦}  cid:2  C1{C2{◦}},  and we have C{e} =C 1{C2{e}}. Thetrivial, or hole,” written ◦, for which◦{e } =e .  identity, expression context is the “bare  The statics of expressions of T is extended to expression contexts by deﬁning the typing  judgment  C :   cid:7  * τ   cid:6    cid:7    cid:5  * τ   cid:5      so that if  cid:7   cid:12  e : τ, then  cid:7   cid:5 . This judgment is inductively deﬁned by a collection of rules derived from the statics of T  see rules  9.1  . Some representative rules are   cid:5   cid:12  C{e} : τ    46.1a    46.1b    46.1c    46.1d    46.1e    46.1f    46.1g   437  46.1 Observational Equivalence  as follows:  ◦ :   cid:7  * τ   cid:6    cid:7  * τ  C :   cid:7  * τ   cid:6    cid:7   cid:5  * nat  s C  :   cid:7  * τ   cid:6    cid:7   cid:5  * nat   cid:5    cid:5    cid:5   cid:12  e0 : τ  C :   cid:7  * τ   cid:6    cid:7    cid:5  * nat   cid:7   , x : nat, y : τ recC {z  cid:9 → e0  s x  with y  cid:9 → e1} :   cid:7  * τ   cid:6    cid:7    cid:7    cid:7    cid:5   cid:12  e : nat C0 :   cid:7  * τ   cid:6    cid:7   , x : nat, y : τ rec e {z  cid:9 → C0  s x  with y  cid:9 → e1} :   cid:7  * τ   cid:6    cid:7    cid:5    cid:7    cid:5  * τ   cid:5    cid:5    cid:5    cid:5     cid:5   cid:12  e1 : τ  cid:5  * τ  cid:5   cid:12  e1 : τ  cid:5  * τ   cid:7    cid:5   cid:12  e : nat  cid:7    cid:5   cid:12  e0 : τ   cid:5  C1 :   cid:7  * τ   cid:6    cid:7    cid:5   rec e {z  cid:9 → e0  s x  with y  cid:9 → C1} :   cid:7  * τ   cid:6    cid:7    cid:5    cid:5  * τ , x : nat, y : τ  cid:5  * τ  cid:5     cid:5    C2 :   cid:7  * τ   cid:6    cid:7    cid:5   , x : τ1 * τ2   λ  x : τ1 C2 :   cid:7  * τ   cid:6    cid:7   cid:5  * τ2 → τ C1 :   cid:7  * τ   cid:6    cid:7  C1 e2  :   cid:7  * τ   cid:6    cid:7    cid:5  * τ1 → τ2   cid:5   cid:12  e2 : τ2  cid:5    cid:7   cid:5  * τ  cid:5    cid:5  C2 :   cid:7  * τ   cid:6    cid:7   cid:5    e1 C2  :   cid:7  * τ   cid:6    cid:7    cid:5   cid:12  e1 : τ2 → τ   cid:5  * τ2    cid:5  * τ   cid:7   Lemma 46.1. If C :   cid:7  * τ   cid:6    cid:7    cid:5  * τ   cid:5  , then  cid:7    cid:5  ⊆  cid:7 , and if  cid:7   cid:12  e : τ , then  cid:7    46.1h   cid:5   cid:12  C{e} : τ  cid:5 .  Contexts are closed under composition, with the trivial context acting as an identity for  it. Lemma 46.2. If C :   cid:7  * τ   cid:6    cid:7    cid:7  * τ   cid:6    cid:7    cid:5  cid:5  * τ   cid:5  cid:5  . If C  cid:5  cid:5  * τ   cid:6    cid:7   cid:5   Lemma 46.3.   cid:7 , x : τ  : , x : τ    cid:7  * τ   cid:6    cid:7   cid:5  .   cid:5  cid:5  * τ  Proof By induction on rules  46.1 .   cid:5  * τ   cid:5  , and C cid:5  :   cid:7    cid:5  * τ   cid:5    cid:6    cid:7    cid:5  cid:5  * τ   cid:5  cid:5  , then C cid:5 {C{◦}} :   cid:5  * τ   cid:5   and x   ∈ dom  cid:7  ,  then C  :  A complete program is a closed expression of type nat.   cid:5 , areKleene equal, written e 2 e Deﬁnition 46.4. Two complete programs, e and e  cid:5   cid:20 −→∗ there exists n ≥ 0 such that e  cid:20 −→∗ n.  n and e   cid:5 , iff  Kleene equality is obviously reﬂexive and symmetric; transitivity follows from determi- nacy of evaluation. Closure under converse evaluation follows similarly. It is immediate from the deﬁnition that 0  cid:6 2 1.   438  Equality for System T  Deﬁnition 46.5. Suppose that  cid:7   cid:12  e : τ and  cid:7   cid:12  e same type. Two such expressions are observationally equivalent, written  cid:7   cid:12  e C{e} 2C {e   cid:5 } for every program context C :   cid:7  * τ   cid:6   ∅ *nat .   cid:5  : τ are two expressions of the  cid:5  : τ , iff  ∼= e  In other words, for all possible experiments, the outcome of an experiment on e is the same  cid:5 , which is an equivalence relation. For the sake of brevity, we often as the outcome on e  cid:5  for ∅  cid:12 e write e A family of equivalence relations  cid:7   cid:12  e1 E e2 : τ is a congruence iff it is preserved by all contexts. That is,  ∼=τ e   cid:5  : τ.  ∼= e  if  cid:7   cid:12  e E e   cid:5   cid:12  C{e} E C{e : τ, then  cid:7  for every expression context C :   cid:7  * τ   cid:6    cid:7   cid:5  * τ iff ∅  cid:12  e E e   cid:5  : nat implies e 2 e   cid:5 .   cid:5    cid:5 } : τ   cid:5    cid:5  . Such a family of relations is consistent  Theorem 46.6. Observational equivalence is the coarsest consistent congruence on expressions.   cid:5 }.  Proof Consistency follows from the deﬁnition by noting that the trivial context is a program context. Observational equivalence is clearly an equivalence relation. To show that it is a congruence, we need only observe that type-correct composition of a program context with an arbitrary expression context is again a program context. Finally, it is the coarsest such equivalence relation, for if  cid:7   cid:12  e E e  cid:5  : τ for some consistent congruence E,  cid:5 } : nat, and hence by and if C :   cid:7  * τ   cid:6   ∅ *nat  , then by congruence ∅  cid:12 C {e} E C{e consistency C{e} 2 C{e A closing substitution γ for the typing context  cid:7  = x1 : τ1, . . . , xn : τn is a ﬁnite function assigning closed expressions e1 : τ1, . . . , en : τn to x1, . . . , xn, respectively. We write ˆγ  e  for the substitution [e1, . . . , en x1, . . . , xn]e, and write γ :  cid:7  to mean that if x : τ occurs in ∼= cid:7  γ  cid:7 , then there exists a closed expression e such that γ  x  = e and e : τ. We write γ  cid:5 ,  cid:5  x  for each x declared in  cid:7 . where γ :  cid:7  and γ ∼= cid:7  γ Lemma 46.7. If  cid:7   cid:12  e ˆγ  e  ∼=τ  cid:5  e  and ˆγ  e Proof Let C :  ∅ *τ    cid:6   ∅ *nat   be a program context; we are to show that C{ ˆγ  e } 2  cid:5  }. Because C has no free variables, this is equivalent to showing that ˆγ  C{e}  2 C{ ˆγ  e  cid:5 } . Let D be the context ˆγ  C{e   cid:2 γ  cid:5  : τ and γ :  cid:7 , then ˆγ  e  ∼=τ ˆγ  e  cid:5  e   cid:5  :  cid:7 , to express that γ  x  ∼= cid:7  x  γ   cid:5  . Moreover, if γ  ∼= e  cid:5   ∼=τ   cid:5 , then   cid:2 γ   cid:5  .  λ  x1 : τ1  . . . λ  xn : τn C{◦} e1  . . . en ,  where  cid:7  = x1 : τ1, . . . , xn : τn and γ  x1  = e1, . . . , γ  xn  = en. By Lemma 46.3 we have C :   cid:7  * τ   cid:6    cid:7  * nat , from which it follows that D :   cid:7  * τ   cid:6   ∅ *nat  .  cid:5 }. But by construction D{e} 2 ˆγ  C{e} , Because  cid:7   cid:12  e  cid:5 } . Because C is arbitrary, it follows that and D{e ˆγ  e  ∼=τ ˆγ  e  cid:5  .   cid:5  : τ, we have D{e} 2 D{e  cid:5 } , so ˆγ  C{e}  2 ˆγ  C{e  ∼= e  cid:5 } 2 ˆγ  C{e   439  46.2 Logical Equivalence   cid:5  e  ∼=τ  and hence cid:2 γ  and D{e observational equivalence, we have D{e} 2D  cid:5 {e   cid:2 γ Deﬁning D cid:5  like D, but based on γ  cid:5  . Now if γ  cid:5  e  cid:5 }. It follows that D{e} ∼=nat D cid:5 {e  cid:5 } ∼=nat D cid:5 {e   cid:5 , rather than γ , we may also show that D cid:5 {e} 2D  cid:5 {e  cid:5 },  cid:5 , then by congruence we haveD{e} ∼=nat D cid:5 {e}, ∼= cid:7  γ  cid:5 }, and so, by consistency of   cid:5 }, which is to say that ˆγ  e  ∼=τ Theorem 46.6 licenses the principle of proof by coinduction: to show that  cid:7   cid:12  e   cid:5  : τ, it is enough to exhibit a consistent congruence E such that  cid:7   cid:12  e E e  cid:5  : τ. It can be difﬁcult to construct such a relation. In the next section, we will provide a general method for doing so that exploits types.   cid:5  .  cid:5  e ∼= e   cid:2 γ  46.2 Logical Equivalence  The key to simplifying reasoning about observational equivalence is to exploit types. Informally, we may classify the uses of expressions of a type into two broad categories, the passive and the active uses. The passive uses are those that manipulate expressions without inspecting them. For example, we may pass an expression of type τ to a function that simply returns it. The active uses are those that operate on the expression itself; these are the elimination forms associated with the type of that expression. For the purposes of distinguishing two expressions, it is only the active uses that matter; the passive uses manipulate expressions at arm’s length, affording no opportunities to distinguish one from another.  Logical equivalence is therefore deﬁned as follows. Deﬁnition 46.8. Logical equivalence is a family of relations e ∼τ e expressions of type τ . It is deﬁned by induction on τ as follows:   cid:5  between closed   cid:5   e ∼ nat e e ∼τ1→τ2 e   cid:5    cid:5   e 2 e if e1 ∼τ1 e  iff  iff  1, then e e1  ∼τ2 e  cid:5    cid:5  e   cid:5  1   The deﬁnition of logical equivalence at type nat licenses the following principle of proof   cid:5 , it is enough to show that   cid:5   whenever e ∼nat e  by nat-induction. To show that E  e, e 1. E  0, 0 , and 2. if E  n, n , then E  n + 1, n + 1 . This assertion is justiﬁed by mathematical induction on n ≥ 0, where e  cid:20 −→∗  cid:5   cid:20 −→∗ Lemma 46.9. Logical equivalence is symmetric and transitive: if e ∼τ e and if e ∼τ e  n by the deﬁnition of Kleene equivalence.   cid:5  cid:5 , then e ∼τ e   cid:5 , then e   cid:5  ∼τ e   cid:5  and e   cid:5  cid:5 .  e   cid:5  ∼τ e,  n and   440  Equality for System T  e   cid:5  1  Proof Simultaneously, by induction on the structure of τ. If τ = nat, the result is immediate. If τ = τ1 → τ2, then we may assume that logical equivalence is symmetric and transitive at types τ1 and τ2. For symmetry, assume that e ∼τ e  cid:5 ; we wish to show  cid:5  ∼τ e. Assume that e ∼τ1 e1; it sufﬁces to show that e 1  ∼τ2 e e1 . By induction we  cid:5   cid:5  e 1. Therefore, by assumption e e1  ∼τ2 e have that e1 ∼τ1 e  cid:5   cid:5   cid:5  e 1 , and hence by induction 1  ∼τ2 e e1 . For transitivity, assume that e ∼τ e  cid:5  ∼τ e  cid:5   cid:5  and e  cid:5  e  cid:5  cid:5 ; we are to show 1; it is enough to show that e e1  ∼τ e e ∼τ e  cid:5  cid:5   cid:5  cid:5   cid:5  cid:5  e 1 . By symmetry and transitivity, we have e1 ∼τ1 e1, so by assumption e e1  ∼τ2 e  cid:5  e1 . We also have by  cid:5  e1  ∼τ2 e  cid:5  cid:5   cid:5  cid:5  e 1 , which sufﬁces for assumption e the result.   cid:5  cid:5  1 . By transitivity, we have e   cid:5  cid:5 . Suppose that e1 ∼τ1 e   cid:5  e1  ∼τ2 e   cid:5  cid:5  e  e  Logical equivalence is extended to open terms by substitution of related closed terms to  cid:5  to hold obtain related results. If γ and γ  cid:5  x  for every variable, x, such that  cid:7   cid:12  x : τ. Open logical equivalence, iff γ  x  ∼ cid:7  x  γ  cid:5  : τ, is deﬁned to mean that ˆγ  e  ∼τ written  cid:7   cid:12  e ∼ e   cid:5  are two substitutions for  cid:7 , we deﬁne γ ∼ cid:7  γ  cid:5   whenever γ ∼ cid:7  γ  cid:5 .   cid:2 γ   cid:5  e  Lemma 46.10. Open logical equivalence is symmetric and transitive.  Proof Follows from Lemma 46.9 and the deﬁnition of open logical equivalence.  At this point, we are “two thirds of the way” to justifying the use of the name “open  logical equivalence.” The remaining third, reﬂexivity, is established in the next section.  46.3 Logical and Observational Equivalence Coincide   cid:5 .   cid:5   cid:20 −→ e   cid:5 , then e ∼τ d  In this section, we prove the coincidence of observational and logical equivalence.  cid:5 . If d  cid:20 −→ e, then d ∼τ e Lemma 46.11  Converse Evaluation . Suppose that e ∼τ e if d Proof By induction on the structure of τ. If τ = nat, then the result follows from the closure of Kleene equivalence under converse evaluation. If τ = τ1 → τ2, then suppose that e ∼τ e  cid:5   cid:5  e 1 . It follows from the assumption that e e1  ∼τ2 e 1 . Noting that d e1   cid:20 −→ e e1 , the result  cid:5  follows by induction. Lemma 46.12  Consistency . If e ∼   cid:5 , and d  cid:20 −→ e. To show that d ∼τ e  1 and show d e1  ∼τ2 e  cid:5    cid:5 , we assume e1 ∼τ1 e   cid:5 , then e 2 e   cid:5 , and   cid:5  e   cid:5 .  nat e  Immediate, from Deﬁnition 46.8.  Proof Theorem 46.13  Reﬂexivity . If  cid:7   cid:12  e : τ , then  cid:7   cid:12  e ∼ e : τ .    cid:2 γ  441  46.3 Logical and Observational Equivalence Coincide   cid:5 , then ˆγ  e  ∼τ  Proof We are to show that, if  cid:7   cid:12  e : τ and γ ∼ cid:7  γ  cid:5  e . The proof proceeds by induction on typing derivations; we consider two representative cases. Consider the case of rule  8.4a , in which τ = τ1 → τ2 and e = λ  x : τ1  e2. We are to show that  Assume that e1 ∼τ1 e  cid:5  e2 . Let γ2 = γ ⊗ x  cid:9 → e1 and γ  cid:5   cid:5  [e Therefore, by induction, we have ˆγ2 e2  ∼τ2 2 Now consider the case of rule  9.1d , for which we are to show that  λ  x : τ1  ˆγ  e2  ∼τ1→τ2 λ  x : τ1  cid:2 γ 1; by Lemma 46.11, it is enough to show that [e1 x] ˆγ  e2  ∼τ2  cid:5  1, and observe that γ2 ∼ cid:7 ,x:τ1 γ  cid:5  ⊗ x  cid:9 → e = γ  cid:5   cid:5  2.  cid:5  ˆγ 2 e2 , from which the result follows easily.  cid:5  e0 ; x.y. cid:2 γ   cid:5  e1 }  cid:2 γ  1 x] cid:2 γ   cid:5  e2 .   cid:5  e  .  By the induction hypothesis applied to the ﬁrst premise of rule  9.1d , we have  We proceed by nat-induction. It sufﬁces to show that   cid:5  e .  ˆγ  e  ∼nat  rec{ ˆγ  e0 ; x.y. ˆγ  e1 }  ˆγ  e   ∼τ rec{ cid:2 γ  cid:2 γ rec{ ˆγ  e0 ; x.y. ˆγ  e1 } z  ∼τ rec{ cid:2 γ rec{ ˆγ  e0 ; x.y. ˆγ  e1 } s n   ∼τ rec{ cid:2 γ rec{ ˆγ  e0 ; x.y. ˆγ  e1 } n  ∼τ rec{ cid:2 γ  and that  assuming   cid:5  e0 ; x.y. cid:2 γ  cid:5  e0 ; x.y. cid:2 γ  cid:5  e0 ; x.y. cid:2 γ   cid:5  e1 } z ,   cid:5  e1 } s n  ,   cid:5  e1 } n .   46.2    46.3    46.4  To show  46.2 , by Lemma 46.11 it is enough to show that ˆγ  e0  ∼τ  cid:5  e0 . This condition is assured by the outer inductive hypothesis applied to the second premise of rule  9.1d .   cid:2 γ  To show  46.3 , deﬁne  δ = γ ⊗ x  cid:9 → n ⊗ y  cid:9 → rec{ ˆγ  e0 ; x.y. ˆγ  e1 } n   and   cid:5  ⊗ x  cid:9 → n ⊗ y  cid:9 → rec{ cid:2 γ   cid:5  e0 ; x.y. cid:2 γ   cid:5  = γ By  46.4 , we have δ ∼ applied to the third premise of rule  9.1d , and Lemma 46.11, the required follows.   cid:5 . Consequently, by the outer inductive hypothesis   cid:5  e1 } n .   cid:7 ,x:nat,y:τ δ  δ  Corollary 46.14  Equivalence . Open logical equivalence is an equivalence relation. Corollary 46.15  Termination . If e : nat, then e  cid:20 −→∗ Lemma 46.16  Congruence . If C0 :   cid:7  * τ   cid:6    cid:7 0 * τ0 , and  cid:7   cid:12  e ∼ e  cid:7 0  cid:12  C0{e} ∼ C0{e   cid:5  for some e   cid:5 } : τ0.  val.  e   cid:5    cid:5   : τ , then   442  Equality for System T  Proof By induction on the derivation of the typing of C0. We consider a represen- tative case in which C0 = λ  x : τ1 C2 so that C0   cid:7  * τ   cid:6    cid:7 0 * τ1 → τ2  and C2 :   cid:7  * τ   cid:6    cid:7 0, x : τ1 * τ2 . Assuming  cid:7   cid:12  e ∼ e   cid:5  : τ, we are to show that  :   cid:7 0  cid:12  C0{e} ∼C 0{e   cid:5 } : τ1 → τ2,  which is to say  We know, by induction, that   cid:7 0  cid:12  λ  x : τ1 C2{e} ∼ λ  x : τ1 C2{e   cid:5 } : τ1 → τ2.   cid:7 0, x : τ1  cid:12  C2{e} ∼C 2{e   cid:5 } : τ2.  Suppose that γ0 ∼ cid:7 0 γ 0, and that e1 ∼τ1 e  cid:5  observe that γ1 ∼ cid:7 0,x:τ1 γ  = γ  cid:5  1. By Deﬁnition 46.8, it is enough to show that  1. Let γ1 = γ0 ⊗ x  cid:9 → e1, γ  cid:5    cid:5  1   cid:5  0  ⊗ x  cid:9 → e   cid:5  1, and  ˆγ1 C2{e}  ∼τ2 which follows from the inductive hypothesis.  1 C2{e  cid:5  ˆγ   cid:5 } ,  Theorem 46.17. If  cid:7   cid:12  e ∼ e   cid:5  : τ , then  cid:7   cid:12  e  ∼= e   cid:5  : τ .  Proof By Lemmas 46.12 and 46.16, and Theorem 46.6.  Corollary 46.18. If e : nat, then e Proof By Theorem 46.13, we have e ∼nat e. Hence, for some n ≥ 0, we have e ∼nat n, and so by Theorem 46.17, e  ∼=nat n.  ∼=nat n, for somen ≥ 0.   cid:5  : τ , if e  ∼=τ e   cid:5 , then e ∼τ e   cid:5 .  Lemma 46.19. For closed expressions e : τ and e Proof We proceed by induction on the structure of τ. Ifτ = nat, consider the empty  cid:5 . If τ = τ1 → τ2, then we are to show that  cid:5 , and hence e ∼nat e context to obtain e 2 e ∼=τ1 e whenever e1 ∼τ1 e  cid:5   cid:5   cid:5  e 1, and, 1 . By Theorem 46.17 we have e1 hence, by congruence of observational equivalence, it follows that e e1  ∼=τ2 e  cid:5   cid:5  e 1 , from which the result follows by induction.  1, we have e e1  ∼τ2 e  cid:5   ∼= e  cid:5  : τ , then  cid:7   cid:12  e ∼ e Theorem 46.20. If  cid:7   cid:12  e  cid:5  : τ . ∼= e  cid:5  : τ, and that γ ∼ cid:7  γ Proof Assume that  cid:7   cid:12  e  cid:5 , so by Lemma 46.7 ˆγ  e  ∼=τ  cid:5  e ∼= e Corollary 46.21.  cid:7   cid:12  e   cid:5  : τ iff  cid:7   cid:12  e ∼ e   cid:5  : τ .   cid:2 γ  γ   cid:5 . By Theorem 46.17, we have γ   cid:5  . Therefore, by Lemma 46.19, ˆγ  e  ∼τ ˆγ  e   cid:5  .  ∼= cid:7    443  46.4 Some Laws of Equality  Deﬁnitional equality is sufﬁcient for observational equivalence:  Theorem 46.22. If  cid:7   cid:12  e ≡ e   cid:5  : τ , then  cid:7   cid:12  e ∼ e   cid:5  : τ , and hence  cid:7   cid:12  e  ∼= e   cid:5  : τ .  Proof By an argument similar to that used in the proof of Theorem 46.13 and Lemma 46.16, then appealing to Theorem 46.17. Corollary 46.23. If e ≡ e Proof By Theorem 46.22 we have e ∼nat e   cid:5  : nat, then there exists n ≥ 0 such that e  cid:20 −→∗   cid:5  and hence e 2 e   cid:5   cid:20 −→∗  n and e  n.   cid:5 .  46.4 Some Laws of Equality  In this section, we summarize some useful principles of observational equivalence for T. For the most part, these are laws of logical equivalence and then transferred to observational equivalence by appeal to Corollary 46.21. The laws are presented as inference rules with the meaning that if all of the premises are true judgments about observational equivalence, then so are the conclusions. In other words, each rule is admissible as a principle of observational equivalence.  Logical equivalence is indeed an equivalence relation: it is reﬂexive, symmetric, and transitive.  46.4.1 General Laws   cid:7   cid:12  e   cid:5  ∼= e   cid:5  cid:5  : τ  ∼= e : τ  cid:7   cid:12  e  cid:5  ∼= e : τ  cid:7   cid:12  e ∼= e  cid:7   cid:12  e  cid:5  : τ ∼= e  cid:5  : τ  cid:7   cid:12  e ∼= e  cid:7   cid:12  e  cid:5  cid:5  : τ   cid:7   cid:12  e ≡ e ∼= e  cid:7   cid:12  e   cid:5  : τ  cid:5  : τ  Reﬂexivity is an instance of a more general principle, that all deﬁnitional equalities are  observational equivalences.  Observational equivalence is a congruence: we may replace equals by equals anywhere  in an expression.   cid:7   cid:12  e  ∼= e  C :   cid:7  * τ   cid:6    cid:7    cid:5  : τ  cid:5   cid:12  C{e} ∼= C{e  cid:7    cid:5 } : τ   cid:5    cid:5  * τ   cid:5     46.5a    46.5b    46.5c    46.6a    46.7a    444  Equality for System T  Equivalence is stable under substitution for free variables, and substituting equivalent  expressions in an expression gives equivalent results. ∼= e  cid:7   cid:12  e : τ  cid:7 , x : τ  cid:12  e2  cid:5  2 : τ ∼= [e x]e  cid:5   cid:5  2 : τ ∼= e 1 : τ  cid:7 , x : τ  cid:12  e2  cid:5   cid:5   cid:5   cid:5  1 x]e 2 : τ   cid:7   cid:12  [e x]e2 ∼= e  cid:7   cid:12  [e1 x]e2   cid:7   cid:12  e1  ∼= [e   cid:5    cid:5    cid:5  2 : τ  46.4.2 Equality Laws  Two functions are equal if they are equal on all arguments.  cid:5  x  : τ2   cid:7 , x : τ1  cid:12  e x  ∼= e   cid:7   cid:12  e  ∼= e   cid:5  : τ1 → τ2  Consequently, every expression of function type is equal to a λ-abstraction:   cid:7   cid:12  e  ∼= λ  x : τ1  e x  :τ 1 → τ2  46.4.3 Induction Law  An equation involving a free variable x of type nat can be proved by induction on x.   cid:7   cid:12  [n x]e  ∼= [n x]e  cid:7 , x : nat  cid:12  e  ∼= e   cid:5  : τ  for every n ∈ N    46.11a  To apply the induction rule, we proceed by mathematical induction on n ∈ N, which   cid:5  : τ  reduces to showing: 1.  cid:7   cid:12  [z x]e 2.  cid:7   cid:12  [s n  x]e  ∼= [z x]e  ∼= [s n  x]e   cid:5  : τ, and   cid:5  : τ, if  cid:7   cid:12  [n x]e  ∼= [n x]e   cid:5  : τ.   46.8a    46.8b    46.9    46.10   46.5 Notes  The method of logical relations interprets types as relations  here, equivalence relations  by associating with each type constructor a relational action that transforms the relation interpreting its arguments to the relation interpreting the constructed type. Logical relations  Statman, 1985  are a fundamental tool in proof theory and provide the foundation for the semantics of the NuPRL type theory  Constable, 1986; Allen, 1987; Harper, 1992 . The use of logical relations to characterize observational equivalence is an adaptation of the NuPRL semantics to the simpler setting of G¨odel’s System T.   47  Equality for System PCF  In this chapter, we develop the theory of observational equivalence for PCF, with an eager interpretation of the type of natural numbers. The development proceeds along lines similar to those in Chapter 46 but is complicated by the presence of general recursion. The proof depends on the concept of an admissible relation, one that admits the principle of proof by ﬁxed point induction.  47.1 Observational Equivalence  The deﬁnition of observational equivalence, along with the auxiliary notion of Kleene equivalence, are deﬁned similarly to Chapter 46 but modiﬁed to account for the possibility of non-termination. The collection of well-formed PCF contexts is inductively deﬁned in a manner directly  cid:5  * τ analogous to that in Chapter 46. Speciﬁcally, we deﬁne the judgmentC :   cid:7  * τ   cid:6    cid:7   cid:5   by rules similar to rules  46.1 , modiﬁed for PCF.  We leave the precise deﬁnition as an exercise for the reader.  When  cid:7  and  cid:7    cid:5  are empty, we write just C : τ  cid:6  τ   cid:5 .  A complete program is a closed expression of type nat.  Deﬁnition 47.1. We say that two complete programs, e and e e 2 e   cid:5 , iff for every n ≥ 0, e  cid:20 −→∗   cid:5   cid:20 −→∗  n iff e  n.   cid:5 , are Kleene equal, written  Kleene equality is clearly an equivalence relation and is closed under converse evaluation.  Moreover, 0  cid:6 2 1 and, if e and e   cid:5 . Observational equivalence is deﬁned just as it is in Chapter 46.   cid:5  are both divergent, then e 2 e  Deﬁnition 47.2. We say that  cid:7   cid:12  e : τ and  cid:7   cid:12  e equivalent iff for every program context C :   cid:7  * τ   cid:6   ∅ *nat , C{e} 2 C{e   cid:5  : τ are observationally, or contextually,   cid:5 }.  Theorem 47.3. Observational equivalence is the coarsest consistent congruence.  Proof See the proof of Theorem 46.6. Lemma 47.4  Substitution and Functionality . If  cid:7   cid:12  e  cid:5  e  and ˆγ  e ˆγ  e   cid:5  . Moreover, if γ   cid:5 , then ˆγ  e  ∼=τ  ∼= cid:7  γ  ˆγ  ∼= e  cid:5   ∼=τ   cid:5  : τ and γ :  cid:7 , then ˆγ  e  ∼=τ ˆγ   cid:5  e   cid:5  .   446  Equality for System PCF  Proof See Lemma 46.7.  47.2 Logical Equivalence  Deﬁnition 47.5. Logical equivalence, e ∼τ e deﬁned by induction on τ as follows:   cid:5 , between closed expressions of type τ is   cid:5   e ∼ nat e e ∼τ1 cid:19 τ2 e   cid:5    cid:5   e 2 e e1 ∼τ1 e  iff  iff  1 implies e e1  ∼τ2 e  cid:5    cid:5  e   cid:5  1   Formally, logical equivalence is deﬁned as in Chapter 46, except that the deﬁnition of Kleene equivalence is altered to account for non-termination. Logical equivalence is extended to open terms by substitution. Speciﬁcally, we deﬁne  cid:7   cid:12  e ∼ e  cid:5  : τ to mean that ˆγ  e  ∼τ By the same argument as given in the proof of Lemma 46.9, logical equivalence is   cid:5   whenever γ ∼ cid:7  γ   cid:2 γ   cid:5  e   cid:5 .  symmetric and transitive, as is its open extension.   cid:5  : τ are both divergent, then e ∼τ e  Lemma 47.6  Strictness . If e : τ and e Proof By induction on the structure of τ. If τ = nat, then the result follows immediately from the deﬁnition of Kleene equivalence. If τ = τ1  cid:19  τ2, then e e1  and e  cid:5  1  diverge, so by induction e e1  ∼τ2 e Lemma 47.7  Converse Evaluation . Suppose that e ∼τ e if d   cid:5 . Ifd  cid:20 −→ e, then d ∼τ e   cid:5  1 , as required.   cid:5 , then e ∼τ d   cid:5   cid:20 −→ e   cid:5 , and   cid:5  e   cid:5  e   cid:5 .   cid:5 .  47.3 Logical and Observational Equivalence Coincide  The proof of coincidence of logical and observational equivalence relies on the concept of bounded recursion, which we deﬁne by induction on m ≥ 0 as follows:  fix0 x : τ is e  cid:2  fix x : τ is x  fixm+1 x : τ is e  cid:2  [fixm x : τ is e x]e  When m = 0, bounded recursion is deﬁned to be a divergent expression of type τ. When m > 0, bounded recursion is deﬁned by unrolling the recursion m times by iterated substitution. Intuitively, the bounded recursive expression fixm x : τ is e is as good as fix x : τ is e for up to m unrollings, after which it is divergent.   447  47.3 Logical and Observational Equivalence Coincide  It is easy to check that the follow rule is derivable for each m ≥ 0:   cid:7 , x : τ  cid:12  e : τ   47.1a  The proof is by induction on m ≥ 0, and amounts to an iteration of the substitution lemma for the statics of PCF.   cid:7   cid:12  fixm{τ} x.e  : τ  .  The key property of bounded recursion is the principle of ﬁxed point induction, which permits reasoning about a recursive computation by induction on the number of unrollings required to reach a value. The proof relies on compactness, which will be stated and proved in Section 47.4 below. Theorem 47.8  Fixed Point Induction . Suppose that x : τ  cid:12  e : τ . If  cid:5    ∀m ≥ 0  fixm x : τ is e ∼τ fixm x : τ is e  ,   cid:5 .  then fix x : τ is e ∼τ fix x : τ is e Proof Deﬁne an applicative context A to be either a hole, ◦, or an application of the form A e , where A is an applicative context. The typing judgment for applicative contexts, A : τ0  cid:6  τ , is a special case of the general typing judgment for contexts. Deﬁne logical equivalence of applicative contexts, A ∼ A cid:5  : τ0  cid:6  τ, by induction on the structure of A as follows: 1. ◦ ∼ ◦: τ0  cid:6  τ0; 2. if A ∼ A cid:5  : τ0  cid:6  τ2  cid:19  τ and e2 ∼τ2 e We prove by induction on the structure of τ, if A ∼ A cid:5  : τ0  cid:6  τ and  2, then A e2  ∼ A cid:5  e  cid:5    cid:5  2  : τ0  cid:6  τ.  for every m ≥ 0, A{fixm x : τ0 is e} ∼τ A cid:5 {fixm x : τ0 is e   cid:5 },  then  A{fix x : τ0 is e} ∼τ A cid:5 {fix x : τ0 is e   cid:5 }.  Choosing A = A cid:5  = ◦ with τ0 = τ completes the proof. If τ = nat, then assume that A ∼ A cid:5  : τ0  cid:6  nat and  47.2 . By Deﬁnition 47.5, we are to show  A{fix x : τ0 is e} 2A  cid:5 {fix x : τ0 is e   cid:5 }.  By Corollary 47.17 there exists m ≥ 0 such that   47.2    47.3   By  47.2  we have  A{fix x : τ0 is e} 2A {fixm x : τ0 is e}.  A{fixm x : τ0 is e} 2A  cid:5 {fixm x : τ0 is e   cid:5 }.   448  Equality for System PCF  By Corollary 47.17  A cid:5 {fixm x : τ0 is e   cid:5 } 2A  cid:5 {fix x : τ0 is e   cid:5 }.  whenever e1 ∼τ1 e m ≥ 0  The result follows by transitivity of Kleene equivalence. If τ = τ1  cid:19  τ2, then by Deﬁnition 47.5, it is enough to show A cid:5 {fix x : τ0 is e = A cid:5  e A cid:5   A{fix x : τ0 is e} e1  ∼τ2 1. LetA 2 = A e1  and A cid:5   cid:5  A2{fixm x : τ0 is e} ∼τ2 2 : τ0  cid:6  τ2, we have by induction A2{fix x : τ0 is e} ∼τ2  Noting that A2 ∼ A cid:5   {fix x : τ0 is e  A cid:5   2  2  2   cid:5 },   cid:5 } e  cid:5  1   as required. Lemma 47.9  Reﬂexivity . If  cid:7   cid:12  e : τ , then  cid:7   cid:12  e ∼ e : τ .   cid:5  1 . It follows from  47.2  that for every {fixm x : τ0 is e   cid:5 }.  Proof The proof proceeds along the same lines as the proof of Theorem 46.13. The main difference is the treatment of general recursion, which is proved by ﬁxed point induction. Consider rule  19.1g . Assuming γ ∼ cid:7  γ  By Theorem 47.8, it is enough to show that, for every m ≥ 0,   cid:5 , we are to show that  cid:5  e .  fix x : τ is ˆγ  e  ∼τ fix x : τ is cid:2 γ fixm x : τ is ˆγ  e  ∼τ fixm x : τ is cid:2 γ   cid:5  e .  We proceed by an inner induction on m. When m = 0, the result is immediate, because both sides of the desired equivalence diverge. Assuming the result for m, and applying Lemma 47.7, it is enough to show that ˆγ  e1  ∼τ   cid:2 γ  cid:5  e  x] cid:2 γ  e   cid:5  e1 , where e1 = [fixm x : τ is ˆγ  e  x] ˆγ  e , and  cid:5  1  = [fixm x : τ is cid:2 γ fixm x : τ is ˆγ  e  ∼τ fixm x : τ is cid:2 γ [fixm x : τ is ˆγ  e  x] ˆγ  e  ∼τ [fixm x : τ is cid:2 γ   cid:5  e .   cid:5  e ,   cid:5  e  x] cid:2 γ   cid:5  e .  But this follows directly from the inner and outer inductive hypotheses. For by the outer inductive hypothesis, if   47.4   47.5   then  But the hypothesis holds by the inner inductive hypothesis, from which the result follows. To handle the conditional ifz e {z  cid:9 → e0  s x   cid:9 → e1}, we proceed by cases on whether e diverges, in which case the conditional is divergent and therefore self-related by Lemma 47.6, or e converges, in which case we can proceed by an inner mathematical induction on its value, appealing to the inductive hypotheses governing the branches of the conditional to complete the argument.   449  47.4 Compactness  Symmetry and transitivity of eager logical equivalence are easily established by induc- tion on types, noting that Kleene equivalence is symmetric and transitive. Eager logical equivalence is therefore an equivalence relation. Lemma 47.10  Congruence . If C0 :   cid:7  * τ   cid:6    cid:7 0 * τ0 , and  cid:7   cid:12  e ∼ e  cid:7 0  cid:12  C0{e} ∼ C0{e Proof By induction on the derivation of the typing of C0, following along similar lines to the proof of Lemma 47.9.   cid:5 } : τ0.  : τ , then   cid:5   Logical equivalence is consistent, by deﬁnition. Consequently, it is contained in obser-  vational equivalence. Theorem 47.11. If  cid:7   cid:12  e ∼ e   cid:5  : τ , then  cid:7   cid:12  e  ∼= e   cid:5  : τ .  Proof By consistency and congruence of logical equivalence.  ∼=τ e   cid:5 , then e ∼τ e   cid:5 .  1. We are to show that e e1  ∼τ2 e  cid:5   Lemma 47.12. If e Proof By induction on the structure of τ. If τ = nat, then the result is immediate, because the empty expression context is a program context. If τ = τ1  cid:19  τ2, then suppose that e1 ∼τ1 e  cid:5  1, and hence by Lemma 47.4 e e1  ∼=τ2 e Theorem 47.13. If  cid:7   cid:12  e Proof Assume that  cid:7   cid:12  e ∼= cid:7  γ  ∼=τ1 e  cid:5  1 , from which the result follows by induction.  ∼= e  cid:5  : τ , then  cid:7   cid:12  e ∼ e ∼= e  cid:5  : τ. Suppose that γ ∼ cid:7  γ   cid:5 , and so by Lemma 47.4, we have   cid:5 . By Theorem 47.11, we have   cid:5  1 . By Theorem 47.11 e1   cid:5  : τ .   cid:5  e   cid:5  e  γ  ˆγ  e  ∼=τ   cid:5    cid:5  e  ˆγ   .  ˆγ  e  ∼τ   cid:5    cid:5  e  ˆγ   .  Therefore, by Lemma 47.12, we have  Corollary 47.14.  cid:7   cid:12  e  ∼= e   cid:5  : τ iff  cid:7   cid:12  e ∼ e   cid:5  : τ .  47.4 Compactness  The principle of ﬁxed point induction is derived from a critical property of PCF, called compactness. This property states that only ﬁnitely many unwindings of a ﬁxed point expression are needed in a complete evaluation of a program. Although intuitively obvious   450  Equality for System PCF   one cannot complete inﬁnitely many recursive calls in a ﬁnite computation , it is rather tricky to state and prove rigorously.  The proof of compactness  Theorem 47.16  makes use of the stack machine for PCF deﬁned in Chapter 28, augmented with the following transitions for bounded recursive expressions:  k * fix0 x : τ is e  cid:20 −→ k * fix0 x : τ is e  k * fixm+1 x : τ is e  cid:20 −→ k * [fixm x : τ is e x]e   47.6a    47.6b   It is not difﬁcult to extend the proof of Corollary 28.4 to account for bounded recursion.  To get a feel for what is involved in the compactness proof, consider ﬁrst the factorial  function f in PCF:  fix f : nat  cid:19  nat is λ  x : nat  ifz x {z  cid:9 → s z   s x   cid:5      cid:9 → x ∗ f  x   cid:5    }.  Obviously evaluation of f  n  requires n recursive calls to the function itself. That is, for a given input n we may place a bound m on the recursion that is sufﬁcient to ensure termination of the computation. This property can be expressed formally using the m-bounded form of general recursion,  fixm f : nat  cid:19  nat is λ  x : nat  ifz x {z  cid:9 → s z   s x   }.    cid:9 → x ∗ f  x  cid:5  Call this expression f  m . It follows from the deﬁnition of f that if f  n   cid:20 −→∗ f  m  n   cid:20 −→∗  p for some m ≥ 0  in fact, m = n sufﬁces .   cid:5   p, then  When considering expressions of higher type, we cannot expect to get the same result from the bounded recursion as from the unbounded. For example, consider the addition function a of type τ = nat  cid:19   nat  cid:19  nat , given by the expression  fix p : τ is λ  x : nat  ifz x {z  cid:9 → id  s x   cid:5      cid:9 → s ◦  p x   cid:5     },   cid:5  ◦ e = λ  x : τ  e  where id = λ  y : nat  y is the identity, e  cid:5  e x   is composition, and s = λ  x : nat  s x  is the successor function. The application a n  terminates after three transitions, regardless of the value of n, resulting in a λ-abstraction. When n is positive, the result contains a residual copy of a itself, which is applied to n − 1 as a recursive call. The m-bounded version of a, written a m , is also such that a m  n  terminates in three steps, provided that m > 0. But the result is not the same, because the residuals of a appear as a m−1 , rather than as a itself. Turning now to the proof of compactness, it is helpful to introduce some notation. Suppose that x : τ  cid:12  ex : τ for some arbitrary abstractor x.ex. Let f  ω  = fix x : τ is ex, and let f  m  = fixm x : τ is ex. Observe that f  ω  : τ and f  m  : τ for any m ≥ 0. The following technical lemma governing the stack machine allows the bound on occur- rences of a recursive expression to be raised without affecting the outcome of evaluation. Lemma 47.15. For every m ≥ 0, if [f  m  y]k * [f  m  y]e  cid:20 −→∗  cid:24  , n, then [f  m+1  y]k * [f  m+1  y]e  cid:20 −→∗ Proof By induction on m ≥ 0, and then induction on transition.   cid:24  , n.   451  47.4 Compactness  Theorem 47.16  Compactness . Suppose that y : τ  cid:12  e : nat where y  ∈ f  ω . If [f  ω  y]e  cid:20 −→∗  n, then there exists m ≥ 0 such that [f  m  y]e  cid:20 −→∗  n.  Proof We prove simultaneously the stronger statements that if  cid:24  , n,  [f  ω  y]k * [f  ω  y]e  cid:20 −→∗  then for some m ≥ 0,  and if  then for some m ≥ 0,  [f  m  y]k * [f  m  y]e  cid:20 −→∗   cid:24  , n,  [f  ω  y]k , [f  ω  y]e  cid:20 −→∗   cid:24  , n  [f  m  y]k , [f  m  y]e  cid:20 −→∗   cid:24  , n.   Note that if [f  ω  y]e val, then [f  m  y]e val for all m ≥ 0.  The result then follows by the correctness of the stack machine  Corollary 28.4 .  We proceed by induction on transition. Suppose that the initial state is  [f  ω  y]k * f  ω ,  which arises when e = y, and the transition sequence is as follows: [f  ω  y]k * f  ω   cid:20 −→ [f  ω  y]k * [f  ω  x]ex  cid:20 −→∗   cid:24  , n.  Noting that [f  ω  x]ex = [f  ω  y][y x]ex, we have by induction that there exists m ≥ 0 such that  By Lemma 47.15,  [f  m  y]k * [f  m  x]ex  cid:20 −→∗   cid:24  , n.  [f  m+1  y]k * [f  m  x]ex  cid:20 −→∗   cid:24  , n  and we need only recall that  [f  m+1  y]k * f  m+1  = [f  m+1  y]k * [f  m  x]ex  to complete the proof. If, on the other hand, the initial step is an unrolling, but e  cid:6 = y, then we have for some z  ∈ f  ω  and z  cid:6 = y  [f  ω  y]k * fix z : τ is dω  cid:20 −→ [f  ω  y]k * [fix z : τ is dω z]dω  cid:20 −→∗   cid:24  , n.  where dω = [f  ω  y]d. By induction there exists m ≥ 0 such that  [f  m  y]k * [fix z : τ is dm z]dm  cid:20 −→∗ where dm = [f  m  y]d. But then by Lemma 47.15 we have   cid:24  , n,  [f  m+1  y]k * [fix z : τ is dm+1 z]dm+1  cid:20 −→∗ where dm+1 = [f  m+1  y]d, from which the result follows directly.   cid:24  , n,   452  Equality for System PCF  Corollary 47.17. There exists m ≥ 0 such that [f  ω  y]e 2 [f  m  y]e.  If [f  ω  y]e diverges, then it sufﬁces to take m to be zero. Otherwise, apply  Proof Theorem 47.16 to obtain m, and note that the required Kleene equivalence follows.  47.5 Lazy Natural Numbers  Recall from Chapter 19 that, if the successor is evaluated lazily, then the type nat changes its meaning to that of the lazy natural numbers, which we shall write lnat for emphasis. This type contains an “inﬁnite number” ω, which is essentially an endless stack of successors. To account for the lazy successor, the deﬁnition of logical equivalence must be reformu- lated. Rather than being deﬁned inductively as the strongest relation closed under speciﬁed conditions, it is now deﬁned coinductively as the weakest relation consistent with two anal- ogous conditions. We may then show that two expressions are related using the principle of proof by coinduction.  The deﬁnition of Kleene equivalence must be altered to account for the lazily evaluated successor operation. To account for ω, two computations are compared based solely on the outermost form of their values, if any. We deﬁne e 2 e z, then  cid:5   cid:20 −→∗ s e1 , then e Corollary 47.17 can be proved for the co-natural numbers by essentially the same argu-   cid:5  to hold iff  a  if e  cid:20 −→∗  cid:5   cid:20 −→∗  z, and vice versa; and  b  if e  cid:20 −→∗   cid:5  1 , and vice versa.  s e  e   cid:5   cid:20 −→∗  z, and vice versa.   cid:5  : lnat, then  1  with e1 E e  cid:5   z, then e s e1 , then e  s e  cid:5 , then e 2 e   cid:5  1 : lnat, and vice versa.  ment as before. The deﬁnition of logical equivalence at type lnat is deﬁned to be the weakest equiva- lence relation E between closed terms of type lnat satisfying the following consistency conditions: if e E e 1. If e  cid:20 −→∗  cid:5   cid:20 −→∗ 2. If e  cid:20 −→∗ It is immediate that if e ∼lnat e also strict in that if e and e The principle of proof by coinduction states that to show e ∼lnat e a relation, E, such that 1. e E e  cid:5  : lnat, and 2. E satisﬁes the above consistency conditions. If these requirements hold, then E is contained in logical equivalence at type lnat, and hence e ∼lnat e As an application of coinduction, let us consider the proof of Theorem 47.8. The overall argument remains as before, but the proof for the type lnat must be altered as   cid:5 , and so logical equivalence is consistent. It is  cid:5  are both divergent expressions of type lnat, then e ∼lnat e  cid:5 .  cid:5 , it sufﬁces to exhibit   cid:5 , as required.   453  47.6 Notes   cid:5  = follows. Suppose that A ∼ A cid:5  : τ0  cid:6  lnat, and let a = A{fix x : τ0 is e} and a A cid:5 {fix x : τ0 is e  cid:5  m  = A cid:5 {fixm x : τ0 is e  cid:5 }, assume that   cid:5 }. Writing a m  = A{fixm x : τ0 is e} and a  cid:5  m   for every m ≥ 0, a m  ∼lnat a  .  We are to show that  a ∼lnat a   cid:5   .   cid:5    cid:5  n   cid:5    s d  d undeﬁned = pn a  if pn d   cid:20 −→∗ otherwise  Deﬁne the functions pn for n ≥ 0 on closed terms of type lnat by the following equations:   cid:5  . Correspondingly, let a m   cid:5  m  . Deﬁne E to be the strongest relation such that an E a   cid:9  p0 d  = d p n+1  d  = For n ≥ 0, let an = pn a  and a = pn a m   and  m  = pn a  cid:5   cid:5  n : lnat for all a n ≥ 0. We will show that the relation E satisﬁes the consistency conditions, and so it n is contained in logical equivalence. Because a E a  cid:5  : lnat  by construction , the result follows immediately. n : lnat for some n ≥ 0. We have by To show that E is consistent, suppose that an E a  cid:5  Corollary 47.17 an 2 a m   m , and  cid:5  n   for so by Corollary 47.17 again, a  cid:5  n  m , and so there exists b some b m   cid:5   cid:5  n such that a n : lnat n by construction, as required.  n , for somem ≥ 0, and hence, by the assumption, an 2 a  s bn , then a m   cid:5  , and we have bn E b  n. Now if an  cid:20 −→∗  cid:5   m  such that a  n . But bn = pn+1 a  and b  cid:5   n , and hence there exists b   m   cid:20 −→∗  cid:5  = pn+1 a n   m  2 a  cid:5  n   cid:20 −→∗   cid:20 −→∗  s b m    cid:5  b n  s b   cid:5  n   cid:5  n  n  n  47.6 Notes  The use of logical relations to characterize observational equivalence for PCF is inspired by the treatment of partiality in type theory by Constable and Smith  1987  and by the studies of observational equivalence by Pitts  2000 . Although the technical details differ, the proof of compactness here is inspired by Pitts’s structurally inductive characterization of termination using an abstract machine. It is critical to restrict attention to transition systems whose states are complete programs  closed expressions of observable type . Structural operational semantics usually does not fulﬁll this requirement, thereby requiring a considerably more complex argument than given here.   48  Parametricity  The main motivation for polymorphism is to enable more programs to be written—those that are “generic” in one or more types, such as the composition function given in Chapter 16. If a program does not depend on the choice of types, we can code it using polymorphism. Moreover, if we wish to insist that a program cannot depend on a choice of types, we demand that it be polymorphic. Thus, polymorphism can be used both to expand the collection of programs we may write and to limit the collection of programs that are permissible in a given context.  The restrictions imposed by polymorphic typing give rise to the experience that in a polymorphic functional language, if the types are correct, then the program is correct. Roughly speaking, if a function has a polymorphic type, then the strictures of type genericity cut down the set of programs with that type. Thus, if you have written a program with this type, it is more likely to be the one you intended!  The technical foundation for these remarks is called parametricity. The goal of this  chapter is to give an account of parametricity for F under a call-by-name interpretation.  48.1 Overview  We will begin with an informal discussion of parametricity based on a “seat of the pants” understanding of the set of well-formed programs of a type. Suppose that a function value f has the type ∀ t.t → t . What function could it be? When instantiated at a type τ it should evaluate to a function g of type τ → τ that, when  cid:5  of type τ. Because f is polymorphic, further applied to a value v of type τ returns a value v  cid:5  must be v. In other words, g must be the identity function at g cannot depend on v, sov type τ, and f must therefore be the polymorphic identity. Suppose that f is a function of type ∀ t.t . What function could it be? A moment’s thought reveals that it cannot exist at all. For it must, when instantiated at a type τ, return a value of that type. But not every type has a value  including this one , so this is an impossible assignment. The only conclusion is that ∀ t.t  is an empty type. Let N be the type of polymorphic Church numerals introduced in Chapter 16, namely ∀ t.t →  t → t  → t . What are the values of this type? Given any type τ, and values z : τ and s : τ → τ, the expression  f [τ] z  s    455  48.2 Observational Equivalence  must yield a value of type τ. Moreover, it must behave uniformly with respect to the choice of τ. What values could it yield? The only way to build a value of type τ is by using the element z and the function s passed to it. A moment’s thought reveals that the application must amount to the n-fold composition  That is, the elements of N are in one-to-one correspondence with the natural numbers.  s s . . . s z  . . .  .  48.2 Observational Equivalence  The deﬁnition of observational equivalence given in Chapters 46 and 47 is based on identifying a type of answers that are observable outcomes of complete programs. Values of function type are not regarded as answers, but are treated as “black boxes” with no internal structure, only input-output behavior. In F, however, there are no  closed  base types. Every type is either a function type or a polymorphic type, and hence no types suitable to serve as observable answers.  Kleene equality is deﬁned for complete programs by requiring that e 2 e  One way to manage this difﬁculty is to augment F with a base type of answers to serve as the observable outcomes of a computation. The only requirement is that this type have two elements that can be immediately distinguished from each other by evaluation. We may achieve this by enriching F with a base type 2 containing two constants, tt and ff, that serve as possible answers for a complete computation. A complete program is a closed expression of type 2.  cid:5  iff either  cid:20 −→∗ ff. This relation is an  a  e  cid:20 −→∗ tt and e equivalence, and it is immediate that tt  cid:6 2 ff, because these are two distinct constants. As before, we say that a type-indexed family of equivalence relations between closed expressions of the same type is consistent if it implies Kleene equality at the answer type 2. To deﬁne observational equivalence, we must ﬁrst deﬁne the concept of an expression context for F as an expression with a “hole” in it. More precisely, we may give an inductive deﬁnition of the judgment   cid:20 −→∗ tt; or  b  e  cid:20 −→∗ ff and e   cid:5    cid:5   C :   cid:8 ;  cid:7  * τ   cid:6    cid:8   cid:5    cid:5  * τ   cid:5    ,  ;  cid:7   which states that C is an expression context that, when ﬁlled with an expression  cid:8 ;  cid:7   cid:12  e : τ  cid:5 .  We leave the precise deﬁnition of this judgment, yields an expression  cid:8  and the veriﬁcation of its properties, as an exercise for the reader.    cid:5   cid:12  C{e} : τ   cid:5 ;  cid:7   Deﬁnition 48.1. Two expressions of the same type are observationally equivalent, written  cid:8 ;  cid:7   cid:12  e   cid:5 } whenever C :   cid:8 ;  cid:7  * τ   cid:6   ∅;∅ * 2 .   cid:5  : τ , iff C{e} 2C {e  ∼= e  Lemma 48.2. Observational equivalence is the coarsest consistent congruence.  Proof Essentially the same as the the proof of Theorem 46.6.   456  Parametricity  Lemma 48.3. ∼= e 1. If  cid:8 , t;  cid:7   cid:12  e 2. If ∅;  cid:7 , x : τ0  cid:12  e  ∼=τ0 d  d   cid:5 , then ∅;  cid:7   cid:12  [d x]e   cid:5  : τ and τ0 type, then  cid:8 ; [τ0 t] cid:7   cid:12  [τ0 t]e ∼= e  x]e : τ and ∅;  cid:7   cid:12  [d x]e   cid:5  : τ and d : τ0, then ∅;  cid:7   cid:12  [d x]e  ∼= [d  ∼= [τ0 t]e ∼= [d x]e  cid:5  ∼= [d  cid:5    cid:5  : [τ0 t]τ .  cid:5  : τ .   cid:5  : τ . Moreover, if  x]e   cid:5   Proof 1. Let C :   cid:8 ; [τ0 t] cid:7  * [τ0 t]τ   cid:6   ∅ *2  be a program context. We are to show  that  C{[τ0 t]e} 2C {[τ0 t]e   cid:5 }.  Because C is closed, this is equivalent to  [τ0 t]C{e} 2 [τ0 t]C{e Let C cid:5  be the context  cid:16  t C{◦}[τ0], and observe that   cid:5 }.  C cid:5   :   cid:8 , t;  cid:7  * τ   cid:6   ∅ * 2 .  Therefore, from the assumption,  C cid:5 {e} 2C  cid:5 {e  cid:5 } 2[τ 0 t]C{e 2. By an argument similar to that for Lemma 46.7.  But C cid:5 {e} 2[τ 0 t]C{e}, and C cid:5 {e   cid:5 }.  cid:5 }, from which the result follows.  48.3 Logical Equivalence  In this section, we introduce a form of logical equivalence that captures the informal concept of parametricity, and also provides a characterization of observational equivalence. This characterization will permit us to derive properties of observational equivalence of polymorphic programs of the kind suggested earlier. The deﬁnition of logical equivalence for F is somewhat more complex than for T. The main idea is to deﬁne logical equivalence for a polymorphic type ∀ t.τ  to satisfy a very strong condition that captures the essence of parametricity. As a ﬁrst approximation, we  cid:5  of this type should be logically equivalent if they might say that two expressions e and e are logically equivalent for “all possible” interpretations of the type t. More precisely, we  cid:5 [ρ] at type [ρ t]τ, for any choice of type ρ. But this might require that e[ρ] be related to e runs into two problems, one technical, the other conceptual. The same device will be used to solve both problems.  The technical problem stems from impredicativity. In Chapter 46, logical equivalence is deﬁned by induction on the structure of types. But when polymorphism is impredicative, the type [ρ t]τ might well be larger than ∀ t.τ . At the very least, we would have to justify the deﬁnition of logical equivalence on some other grounds, but no criterion appears to be available. The conceptual problem is that, even if we could make sense of the deﬁnition   457  48.3 Logical Equivalence  of logical equivalence, it would be too restrictive. For such a deﬁnition amounts to saying that the unknown type t is interpreted as logical equivalence at whatever type it is when instantiated. To obtain useful parametricity results, we shall ask for much more than this.  cid:5  by types ρ and ρ  cid:5 , and What we shall do is to consider separately instances of e and e  cid:5 . We treat the type variable t as standing for any relation  of some form  between ρ and ρ may suspect that this is asking too much: perhaps logical equivalence is the empty relation. Surprisingly, this is not the case, and indeed it is this very feature of the deﬁnition that we shall exploit to derive parametricity results about the language.  To manage both of these problems, we will consider a generalization of logical equiv- alence that is parameterized by a relational interpretation of the free type variables of its classiﬁer. The parameters determine a separate binding for each free type variable in the classiﬁer for each side of the equation, with the discrepancy being mediated by a speciﬁed relation between them. Thus, related expressions need not have the same type, with the differences between them mediated by the given relation.  We will restrict attention to a certain collection of “admissible” binary relations between closed expressions. The conditions are imposed to ensure that logical equivalence and observational equivalence coincide.  Deﬁnition 48.4  Admissibility . A relation R between expressions of types ρ and ρ admissible, written R : ρ ↔ ρ   cid:5 , iff it satisﬁes two requirements:   cid:5  is  1. Respect for observational equivalence: if R e, e   cid:5  . R d, d  cid:5   cid:20 −→ e  d  2. Closure under converse evaluation: if R e, e   cid:5 , then R e, d   cid:5  .   cid:5   and d  ∼=ρ e and d   cid:5  ∼=ρ  cid:5  e  cid:5  , then if d  cid:20 −→ e, then R d, e   cid:5 , then   cid:5   and if  Closure under converse evaluation is a consequence of respect for observational equiva- lence, but we are not yet in a position to establish this fact. The judgment δ :  cid:8  states that δ is a type substitution that assigns a closed type to each type variable t ∈  cid:8 . A type substitution δ induces a substitution function ˆδ on types given by the equation  ˆδ τ  = [δ t1 , . . . , δ tn  t1, . . . , tn]τ,  Let δ and δ  and similarly for expressions. Substitution is extended to contexts point-wise by deﬁning ˆδ  cid:7   x  = ˆδ  cid:7  x   for each x ∈ dom  cid:7  .  cid:5  be two type substitutions of closed types to the type variables in  cid:8 . An  cid:5  is an assignment of an admissible  cid:5  states that η is an  admissible relation assignment η between δ and δ relation η t  : δ t  ↔ δ admissible relation assignment between δ and δ Logical equivalence is deﬁned in terms of its generalization, called parametric logical equivalence, written e ∼τ e   cid:5  t  to each t ∈  cid:8 . The judgment η : δ ↔ δ   cid:5 ], deﬁned as follows.   cid:5  [η : δ ↔ δ   cid:5 .   458  Parametricity   cid:5  [η : δ ↔ δ   cid:5 ] is   cid:5  [η : δ ↔ δ  cid:5 ]  cid:5  [η : δ ↔ δ  cid:5 ]  Deﬁnition 48.5  Parametric Logical Equivalence . The relation e ∼τ e deﬁned by induction on the structure of τ by the following conditions: e ∼t e e ∼2 e e ∼τ1→τ2 e e ∼∀ t.τ   e   cid:5   iff η t  e, e e 2 e  cid:5  iff e1 ∼τ1 e 1 [η : δ ↔ δ  cid:5  iff e e1  ∼τ2 e 1  [η : δ ↔ δ  cid:5   cid:5  e , and every admissible R : ρ ↔ ρ  cid:5  for every ρ, ρ  cid:5 ] [η ⊗ t  cid:9 → R : δ ⊗ t  cid:9 → ρ ↔ δ e[ρ] ∼τ e  cid:5 [ρ   cid:5  [η : δ ↔ δ  cid:5 ]  cid:5  [η : δ ↔ δ  cid:5 ]   cid:5 ] implies  cid:5 ]  iff   cid:5   ,   cid:5  ⊗ t  cid:9 → ρ   cid:5 ]  Logical equivalence is deﬁned in terms of parametric logical equivalence by considering all possible interpretations of its free type- and expression variables. An expression substi- tution γ for a context  cid:7 , written γ :  cid:7 , is a substitution of a closed expression γ  x  : cid:7   x  to each variable x ∈ dom  cid:7  . An expression substitution γ :  cid:7  induces a substitution function, ˆγ , deﬁned by the equation  ˆγ  e  = [γ  x1 , . . . , γ  xn  x1, . . . , xn]e,   cid:5   cid:2 δ  cid:2 γ   cid:5  [η :  cid:5  x  [η :   cid:5 ], then ˆγ  ˆδ e   ∼τ   cid:5   = dom  cid:7  , and γ  x  ∼ cid:7  x  γ   cid:5 ] is deﬁned to hold iff dom γ   = dom γ  cid:5 ] for every variable x in their common domain.  where the domain of  cid:7  consists of the variables x1, . . . , xn. The relation γ ∼ cid:7  γ δ ↔ δ δ ↔ δ Deﬁnition 48.6  Logical Equivalence . The expressions  cid:8 ;  cid:7   cid:12  e : τ and  cid:8 ;  cid:7   cid:12  e  cid:5  : τ are logically equivalent, written  cid:8 ;  cid:7   cid:12  e ∼ e  cid:5  : τ iff, for every assignment δ and δ  cid:5  of closed types to type variables in  cid:8 , and every admissible relation assignment η : δ ↔ δ  cid:5 ,  cid:5  [η : δ ↔ δ  cid:5    [η : δ ↔ δ if γ ∼ cid:7  γ  cid:5 ].  cid:5  e  cid:5  iff e ∼τ e  cid:5 , and τ are closed, this deﬁnition states that e ∼τ e  When e, e that logical equivalence is indeed a special case of its generalization. Lemma 48.7  Closure under Converse Evaluation . Suppose that e ∼τ e d  cid:20 −→ e, then d ∼τ e Proof By induction on the structure of τ. When τ = t, the result holds by the deﬁnition of admissibility. Otherwise, the result follows by induction, making use of the deﬁnition of the transition relation for applications and type applications. Lemma 48.8  Respect for Observational Equivalence . Suppose that e ∼τ e If d   cid:5  [∅ : ∅ ↔ ∅], so   cid:5 , then e ∼τ d   cid:5  [η : δ ↔ δ   cid:5  [η : δ ↔ δ   cid:5  [η : δ ↔ δ   cid:5 , then d ∼τ d  ∼=ˆδ τ   e and d   cid:5 , and if d   cid:5   cid:20 −→ e   cid:5  ∼= cid:2 δ   cid:5 ]. If   cid:5  τ   e   cid:5 ].   cid:5 ].   cid:5 .  Proof By induction on the structure of τ, relying on the deﬁnition of admissibility, and the congruence property of observational equivalence. For example, if τ = ∀ t.τ2 , then we are to show that for every admissible R : ρ ↔ ρ   cid:5 ,  d[ρ] ∼τ2 d   cid:5    cid:5   ] [η ⊗ t  cid:9 → R : δ ⊗ t  cid:9 → ρ ↔ δ   cid:5  ⊗ t  cid:9 → ρ   cid:5   ].  [ρ   459  48.3 Logical Equivalence  Because observational equivalence is a congruence, we have d[ρ] ∼=[ρ t]ˆδ τ2  e[ρ], and  cid:5 [ρ   cid:5 [ρ]. It follows that  d   cid:5    cid:5 ] ∼=[ρ   t] cid:2 δ   cid:5  τ2  e e[ρ] ∼τ2 e   cid:5    cid:5   ] [η ⊗ t  cid:9 → R : δ ⊗ t  cid:9 → ρ ↔ δ   cid:5  ⊗ t  cid:9 → ρ   cid:5   ],  [ρ  from which the result follows by induction. Corollary 48.9. The relation e ∼τ e  types ˆδ τ  and cid:2 δ   cid:5  τ .   cid:5  [η : δ ↔ δ  Proof By Lemmas 48.7 and 48.8. Corollary 48.10. If  cid:8 ;  cid:7   cid:12  e ∼ e  cid:8 ;  cid:7   cid:12  d ∼ d   cid:5  : τ .   cid:5 ] is an admissible relation between closed   cid:5  : τ , and  cid:8 ;  cid:7   cid:12  d  ∼= e : τ and  cid:8 ;  cid:7   cid:12  d   cid:5  ∼= e   cid:5  : τ , then  Proof By Lemma 48.3 and Corollary 48.9.  Lemma 48.11  Compositionality . Let R : ˆδ ρ  ↔ cid:2 δ   cid:5   holds iff d ∼ρ d   cid:5  ρ  be the relational interpretation  cid:5  [η : δ ↔ δ  cid:5  [η :   cid:5 ]. Then e ∼[ρ t]τ e  of some type ρ, which is to say R d, d δ ↔ δ   cid:5 ] if, and only if, e ∼τ e   cid:5   [η ⊗ t  cid:9 → R : δ ⊗ t  cid:9 → ˆδ ρ  ↔ δ   cid:5  ⊗ t  cid:9 → cid:2 δ   cid:5  ρ ].  Proof By induction on the structure of τ. When τ = t, the result is immediate from the  cid:5   cid:6 = t, the result follows from Deﬁnition 48.5. When choice of the relation R. When τ = t τ = τ1 → τ2, the result follows by induction, using Deﬁnition 48.5. Similarly, when or τ = ∀ u.τ1 , the result follows by induction, noting that we may assume, without loss of generality, that u  cid:6 = t and u  ∈ ρ.  Despite the strong conditions on polymorphic types, logical equivalence is not too restrictive—every expression satisﬁes its constraints. This result is often called the para- metricity theorem or the abstraction theorem: Theorem 48.12  Parametricity . If  cid:8 ;  cid:7   cid:12  e : τ , then  cid:8 ;  cid:7   cid:12  e ∼ e : τ .  Proof By rule induction on the statics of F given by rules  16.2 .  We consider two representative cases here.  Rule  16.2d  Suppose δ :  cid:8 , δ  we have that for all ρ, ρ   cid:5  :  cid:8 , η : δ ↔ δ   cid:5 , and γ ∼ cid:7  γ   cid:5 , and admissible R : ρ ↔ ρ   cid:5 ,   cid:5  [η : δ ↔ δ   cid:5 ]. By induction  [ρ t] ˆγ  ˆδ e   ∼τ [ρ   cid:5    t] cid:2 γ  cid:5   cid:2 δ  where η∗ = η ⊗ t  cid:9 → R, δ∗ = δ ⊗ t  cid:9 → ρ, and δ  cid:16  t  ˆγ  ˆδ e  [ρ]  cid:20 −→∗   cid:5  e   [η∗ : δ∗ ↔ δ  cid:5  ∗], ∗ = δ  cid:5  ⊗ t  cid:9 → ρ  cid:5 . Because  cid:5  [ρ t] ˆγ  ˆδ e     460  and  Parametricity   cid:16  t  cid:2 γ  cid:5   cid:2 δ   cid:5  e  [ρ   cid:5   ]  cid:20 −→∗  [ρ   t] cid:2 γ  cid:5   cid:2 δ   cid:5    cid:5  e  ,   cid:5   cid:2 δ  cid:2 γ   cid:5   cid:2 δ  cid:2 γ  the result follows by Lemma 48.7.  Rule  16.2e  Suppose δ :  cid:8 , δ   cid:5  :  cid:8 , η : δ ↔ δ   cid:5 , and γ ∼ cid:7  γ   cid:5  [η : δ ↔ δ   cid:5 ]. By induction  we have   cid:5  e   [η : δ ↔ δ  ˆγ  ˆδ e   ∼∀ t.τ   ]  cid:5  ρ . Deﬁne the relation R : ˆρ ↔ ˆρ  cid:5 ]. By Corollary 48.9, this relation is admissible.  Let ˆρ = ˆδ ρ  and ˆρ  cid:5  [η : δ ↔ δ By the deﬁnition of logical equivalence at polymorphic types, we obtain  cid:5  ⊗ t  cid:9 → ˆρ   cid:5 ] [η ⊗ t  cid:9 → R : δ ⊗ t  cid:9 → ˆρ ↔ δ   cid:5  = cid:2 δ  cid:5   cid:2 δ  cid:2 γ  ˆγ  ˆδ e  [ ˆρ] ∼τ   cid:5  by R d, d   cid:5  e  [ ˆρ  d   cid:5    cid:5 ].   cid:5   iffd ∼ρ  By Lemma 48.11  But  and similarly  ˆγ  ˆδ e  [ ˆρ] ∼[ρ t]τ   cid:5  e  [ ˆρ   cid:5 ] [η : δ ↔ δ   cid:5   ]  ˆγ  ˆδ e  [ ˆρ] = ˆγ  ˆδ e  [ˆδ ρ ] = ˆγ  ˆδ e[ρ]  ,   cid:5   cid:2 δ  cid:2 γ   cid:5  e  [ ˆρ   cid:5 ] = cid:2 γ  cid:5   cid:2 δ  cid:5  e  [ cid:2 δ  cid:5   cid:2 δ = cid:2 γ   cid:5  e[ρ]  ,   cid:5  ρ ]   48.1   48.2    48.3   48.4   from which the result follows. ∼= e   cid:5  : τ.   cid:5  : τ .   cid:5  : τ , then  cid:8 ;  cid:7   cid:12  e ∼ e  Corollary 48.13. If  cid:8 ;  cid:7   cid:12  e Proof By Theorem 48.12  cid:8 ;  cid:7   cid:12  e ∼ e : τ, and hence by Corollary 48.10,  cid:8 ;  cid:7   cid:12  e ∼ e Lemma 48.14  Congruence . If  cid:8 ;  cid:7   cid:12  e ∼ e  cid:5 ;  cid:7   cid:8  Proof By induction on the structure of C, following along very similar lines to the proof of Theorem 48.12.   cid:5  : τ and C :   cid:8 ;  cid:7  * τ   cid:6    cid:8    cid:5   cid:12  C{e} ∼C {e   cid:5  , then   cid:5 } : τ   cid:5  * τ   cid:5 ;  cid:7    cid:5 .  Lemma 48.15  Consistency . Logical equivalence is consistent.  Proof Follows from the deﬁnition of logical equivalence. ∼= e Corollary 48.16. If  cid:8 ;  cid:7   cid:12  e ∼ e   cid:5  : τ , then  cid:8 ;  cid:7   cid:12  e   cid:5  : τ .   461  48.4 Parametricity Properties  Proof By Lemma 48.15, logical equivalence is consistent, and by Lemma 48.14, it is a congruence, and hence is contained in observational equivalence.  Corollary 48.17. Logical and observational equivalence coincide.  Proof By Corollaries 48.13 and 48.16. ∼=τ e. Therefore, if If d : τ and d  cid:20 −→ e, then d ∼τ e, and hence by Corollary 48.16, d a relation respects observational equivalence, it must also be closed under converse evalu- ation. The second condition on admissibility is superﬂuous, now that we have established the coincidence of logical and observational equivalence.  Corollary 48.18  Extensionality .  ∼=τ1→τ2 e ∼=∀ t.τ   e   cid:5  iff for all e1 : τ1, e e1  ∼=τ2 e  cid:5  iff for all ρ, e[ρ] ∼=[ρ t]τ e  cid:5 [ρ].   cid:5  e1 .  1. e 2. e   cid:5    cid:5  e  1  ∼=τ2 e  1. We are to show that e e1  ∼τ2 e  cid:5    cid:5 . To this end, suppose that e1 ∼τ1 e  Proof The forward direction is immediate in both cases, because observational equiva- lence is a congruence, by deﬁnition. The backward direction is proved similarly in both cases, by appeal to Theorem 48.12. In the ﬁrst case, by Corollary 48.17 it sufﬁces to show that e ∼τ1→τ2 e  cid:5   cid:5  e 1 . 1 . By parametricity, we have e ∼τ1→τ2 e, and  cid:5  By the assumption, we have e e hence e e1  ∼τ2 e e  cid:5  1 . The result then follows by Lemma 48.8. In the second case, by  cid:5 . Suppose that R : ρ ↔ ρ Corollary 48.17 it is sufﬁcient to show that e ∼∀ t.τ   e  cid:5  for  cid:5 ], where some closed types ρ and ρ η t  = R, δ t  = ρ, and δ  cid:5 ]. By  cid:5 [ρ parametricity e ∼∀ t.τ   e, and hence e[ρ] ∼τ e  cid:5 ]. The result then follows by Lemma 48.8. Lemma 48.19  Identity Extension . Let η : δ ↔ δ be such that η t  is observational equivalence at type δ t  for each t ∈ dom δ . Then e ∼τ e   cid:5 . It sufﬁces to show that e[ρ] ∼τ e  cid:5  t  = ρ   cid:5 [ρ  cid:5 . By the assumption, we have e[ρ   cid:5 ] [η : δ ↔ δ  cid:5 ] ∼=[ρ  t]τ e   cid:5  [η : δ ↔ δ] iff e   cid:5 ] [η : δ ↔ δ  ∼=ˆδ τ   e   cid:5 [ρ   cid:5 .   cid:5   Proof The backward direction follows from Theorem 48.12 and respect for observational equivalence. The forward direction is proved by induction on the structure of τ, appealing to Corollary 48.18 to establish observational equivalence at function and polymorphic types.  48.4 Parametricity Properties  The parametricity theorem enables us to deduce properties of expressions of F that hold solely because of their type. The stringencies of parametricity ensure that a polymorphic   462  Parametricity  type has very few inhabitants. For example, we may prove that every expression of type ∀ t.t → t  behaves like the identity function. Theorem 48.20. Let e : ∀ t.t → t  be arbitrary, and let id be  cid:16  t  λ  x : t  x. Then ∼=∀ t.t→t  id. Proof By Corollary 48.17 it is sufﬁcient to show that e ∼∀ t.t→t  id. Let ρ and ρ  cid:5  be arbitrary closed types, let R : ρ ↔ ρ  cid:5   cid:5  be an admissible relation, and suppose that e0 R e 0. We are to show  e  e[ρ] e0  R id[ρ] e   cid:5  0 ,  e[ρ] e0  R e   cid:5  0.  which, given the deﬁnition of id and closure under converse evaluation, is to say  It sufﬁces to show that e[ρ] e0  ∼=ρ e0, for then the result follows by the admissibility of R  cid:5  0. and the assumption e0 R e By Theorem 48.12, we have e ∼∀ t.t→t  e. Let the relation S : ρ ↔ ρ be deﬁned by ∼=ρ e0 and d  cid:5  ∼=ρ e0. This relation is clearly admissible, and we have e0 S e0.   cid:5  iff d  d S d It follows that  e[ρ] e0  S e[ρ] e0 , and so, by the deﬁnition of the relation S, e[ρ] e0  ∼=ρ e0.  In Chapter 16, we showed that product, sum, and natural numbers types are all deﬁnable in F. The proof of deﬁnability in each case consisted of showing that the type and its associated introduction and elimination forms are encodable in F. The encodings are correct in the  weak  sense that the dynamics of these constructs as given in the earlier chapters is derivable from the dynamics of F via these deﬁnitions. By taking advantage of parametricity, we may extend these results to obtain a strong correspondence between these types and their encodings.  As a ﬁrst example, let us consider the representation of the unit type, unit, in F, as  deﬁned in Chapter 16 by the following equations:  unit = ∀ r.r → r    cid:24  cid:25  =  cid:16  r  λ  x : r  x  It is easy to see that  cid:24  cid:25  : unit according to these deﬁnitions. But this says that the type unit is inhabited  has an element . What we would like to know is that, up to observational equivalence, the expression  cid:24  cid:25  is the only element of that type. But this is the content of Theorem 48.20. We say that the type unit is strongly deﬁnable within F.   463  48.4 Parametricity Properties  Continuing in this vein, let us examine the deﬁnition of the binary product type in F, also  given in Chapter 16:  τ1 × τ2 = ∀ r. τ1 → τ2 → r  → r   cid:24 e1, e2 cid:25  = cid:16   r  λ  x : τ1 → τ2 → r  x e1  e2   e · l = e[τ1] λ  x : τ1  λ  y : τ2  x  e · r = e[τ2] λ  x : τ1  λ  y : τ2  y   It is easy to check that  cid:24 e1, e2 cid:25  ·l ∼=τ1 e1 and  cid:24 e1, e2 cid:25  ·r ∼=τ2 e2 by a direct calculation. We wish to show that the ordered pair, as deﬁned above, is the unique such expression, and hence that Cartesian products are strongly deﬁnable in F. We will make use of a lemma governing the behavior of the elements of the product type whose proof relies on Theorem 48.12. Lemma 48.21. If e : τ1 × τ2, then e   cid:24 e1, e2 cid:25  for some e1 : τ1 and e2 : τ2.  ∼=τ1×τ2   cid:5  be arbitrary closed types, and let R : ρ ↔ ρ  Proof Expanding the deﬁnitions of pairing and the product type, and applying Corol-  cid:5  be an admissible lary 48.17, we let ρ and ρ relation between them. Suppose further that h ∼τ1→τ2→t h  cid:5  t  = ρ   cid:5   and each is undeﬁned on t   cid:5   cid:6 = t . We are to show  [η : δ ↔ δ  ],   cid:5    cid:5   where η t  = R, δ t  = ρ, and δ that for some e1 : τ1 and e2 : τ2,  which is to say  e[ρ] h  ∼t h   cid:5    e1  e2  [η : δ ↔ δ   cid:5   ],  e[ρ] h  R h   e1  e2 .   cid:5   Now by Theorem 48.12 we have e ∼τ1×τ2 e. Deﬁne the relation S : ρ ↔ ρ the following conditions are satisﬁed:   cid:5  by d S d   cid:5  iff  ∼=ρ h d1  d2  for somed 1 : τ1 and d2 : τ2; 1. d  cid:5  ∼=ρ  cid:5  2. d 2 : τ2; 3. d R d   cid:5  2  for somed   cid:5  1 : τ1 and d   cid:5  1  d   cid:5  h  cid:5 .   cid:5  d  This relation is clearly admissible. Noting that h ∼τ1→τ2→t h  cid:5   cid:5   is undeﬁned for t   cid:5  t  = S and η   cid:5  t  where η and hence  e[ρ] h  R h   d   cid:5    cid:5  1  d   cid:5  2 ,  as required.   cid:5   : δ ↔ δ  cid:5  [η  cid:6 = t, we conclude that e[ρ] h  S e[ρ  cid:5   ],   cid:5 ] h   cid:5  ,   464  Parametricity  Now suppose that e : τ1 × τ2 is such that e · l ∼=τ1 e1 and e · r ∼=τ2 e2. We wish to  cid:24 e · l, e · r cid:25  by  cid:24 e1, e2 cid:25 . From Lemma 48.21 it follows that e show that e  cid:24 e1, e2 cid:25 . congruence and direct calculation. Hence, by congruence we have e  ∼=τ1×τ2 ∼=τ1×τ2  ∼=τ1×τ2  By a similar line of reasoning, we may show that the Church encoding of the natural numbers given in Chapter 16 strongly deﬁnes the natural numbers in that the following properties hold: 1. iter z{z  cid:9 → e0  s x   cid:9 → e1} ∼=ρ e0. 2. iter s e {z  cid:9 → e0  s x   cid:9 → e1} ∼=ρ [iter e {z  cid:9 → e0  s x   cid:9 → e1} x]e1. 3. Suppose that x : nat  cid:12  r x  : ρ. If  a  r z  ∼=ρ e0, and  b  r s e   ∼=ρ [r e  x]e1, then for every e : nat, r e  ∼=ρ iter e {z  cid:9 → e0  s x   cid:9 → e1}.  The ﬁrst two equations, which constitute weak deﬁnability, are easily established by cal- culation, using the deﬁnitions given in Chapter 16. The third property, the unicity of the iterator, is proved using parametricity by showing that every closed expression of type nat is observationally equivalent to a numeral n. We then argue for unicity of the iterator by mathematical induction on n ≥ 0.  ∼=nat z, or there exists e   cid:5    cid:5  . Consequently, there exists n ≥ 0 such that e  Lemma 48.22. If e : nat, then either e ∼=nat s e e Proof By Theorem 48.12, we have e ∼nat e. Deﬁne the relation R : nat ↔ nat to be ∼=nat s d1  the strongest relation such that d R d  cid:5 , then s e  R s e  cid:5  . and d Letting zero = z and succ = λ  x : nat  s x , we have   cid:5  1. It is easy to see that z R z, and if e R e  ∼=nat z and d   cid:5  ∼=nat z, ord   cid:5  ∼=nat s d   cid:5  1  and d1 R d  ∼=nat n.   cid:5  iff either d  : nat such that  e[nat] zero  succ  R e[nat] zero  succ .  The result follows by the induction principle arising from the deﬁnition of R as the strongest relation satisfying its deﬁning conditions.  48.5 Representation Independence, Revisited  In Section 17.4, we discussed the property of representation independence for abstract types. If two implementations of an abstract type are “similar,” then the client behavior is not affected by replacing one for the other. The crux of the matter is the deﬁnition of similarity of two implementations. Informally, two implementations of an abstract type are similar if there is a relation R between their representation types that is preserved by the operations of the type. The relation R may be thought of as expressing the “equivalence”   465  48.6 Notes  of the two representations; checking that each operation preserves R amounts to checking that the result of performing that operation on equivalent representations yields equivalent results.  As an example, we argued in Section 17.4 that two implementations of a queue abstraction are similar. The two representations of queues are related by a relation R such that q R  b, f   iff q is b followed by the reversal of f . When then argued that the operations preserve this relationship, and then claimed, without proof, that the behavior of the client would not be disrupted by changing one implementation to the other.  The proof of this claim relies on parametricity, as may be seen by considering the deﬁnability of existential types in F given in Section 17.3. According to that deﬁnition, the client, e, of an abstract type ∃ t.τ  is a polymorphic function of type ∀ t.τ → τ2 , where τ2, the result type of the computation, does not involve the type variable t. Being polymorphic, the client enjoys the parametricity property given by Theorem 48.12. Speciﬁcally, suppose that ρ1 and ρ2 are two closed representation types and that R : ρ1 ↔ ρ2 is an admissible relation between them. For example, in the case of the queue abstraction, ρ1 is the type of lists of elements of the queue, ρ2 is the type of a pair of lists of elements, and R is the relation given above. Suppose further that e1 : [ρ1 t]τ and e2 : [ρ2 t]τ are two implementations of the operations such that  e1 ∼τ e2 [η : δ1 ↔ δ2],   48.5  where η t  = R, δ1 t  = ρ1, and δ2 t  = ρ2. In the case of the queues example, the expression e1 is the implementation of the queue operations in terms of lists, and the e2 is the implementation in terms of pairs of lists described earlier. Condition  48.5  states that the two implementations are similar in that they preserve the relation R between the representation types. By Theorem 48.12, it follows that the client e satisﬁes  But because τ2 is a closed type  in particular, does not involve t , this is equivalent to  But then by Lemma 48.19 we have  That is, the client behavior is not affected by the change of representation.  e ∼τ2 e [η : δ1 ↔ δ2].  e ∼τ2 e [∅ : ∅ ↔ ∅].  e[ρ1] e1  ∼=τ2 e[ρ2] e2 .  48.6 Notes  The concept of parametricity is latent in the proof of normalization for System F  Girard, 1972 . Reynolds  1983 , though technically ﬂawed due to its reliance on a  non-existent  set-theoretic model of polymorphism, emphasizes the centrality of logical equivalence for characterizing equality of polymorphic programs. The application of parametricity   466  Parametricity  to representation independence was suggested by Reynolds and developed for existen- tial types by Mitchell  1986  and Pitts  1998 . The extension of System F with a “pos- itive”  inductively deﬁned  observable type appears to be needed to even deﬁne ob- servational equivalence, but this point seems not to have been made elsewhere in the literature.   49  Process Equivalence  As the name implies, a process is an ongoing computation that may interact with other processes by sending and receiving messages. From this point of view, a concurrent com- putation has no deﬁnite “ﬁnal outcome” but rather affords an opportunity for interaction that may well continue indeﬁnitely. The notion of equivalence of processes must therefore be based on their potential for interaction, rather than on the “answer” that they may compute. Let P and Q be such that  cid:12  cid:25  P proc and  cid:12  cid:25  Q proc. We say that P and Q are equivalent, written P ≈ cid:25  Q, iff there is a bisimulation R such that P R cid:25  Q. A family of relations R = {R cid:25  } cid:25  is a bisimulation iff whenever P may evolve to P  cid:5  taking the action α, then  cid:5  R cid:25  Q  cid:5 , and, Q may also evolve to some process Q  cid:5  taking the same conversely, if Q may evolve to Q  cid:5 . This correspondence captures the idea that the two processes afford action, and P the same opportunities for interaction in that they each simulate each other’s behavior with respect to their ability to interact with their environment.   cid:5  taking the same action such that P  cid:5  taking action α, then P may evolve to P   cid:5  R cid:25  Q  49.1 Process Calculus  We will consider a process calculus that consolidates the main ideas explored in Chapters 39 and 40. We assume as given an ambient language of expressions that includes the type clsfd of classiﬁed values  see Chapter 33 . Channels are treated as dynamically generated classes with which to build messages, as described in Chapter 40.  The syntax of the process calculus is given by the following grammar:  Proc P ::= stop  Evt  E ::= null  1  conc P1; P2  P1 ⊗ P2 await E  new[τ] a.P   emit e   inert composition synchronize  $ E ν a ∼ τ .P allocation ! e broadcast 0 null E1 + E2 choice acceptance ?  x.P    or E1; E2  acc x.P    The statics is given by the judgments  cid:7   cid:12  cid:25  P proc and  cid:7   cid:12  cid:25  E event deﬁned by the following rules. We assume as given a judgment  cid:7   cid:12  cid:25  e : τ for τ a type including the type   468  Process Equivalence  clsfd of classiﬁed values.  The dynamics is given by the judgments P  We assume as given the judgments e  cid:20 −→ are identiﬁed up to structural congruence, as described in Chapter 39.  e   cid:25    cid:25    cid:25   P  P , deﬁned as in Chapter 39.  cid:5  and e val cid:25  for expressions. Processes and events   cid:7   cid:12  cid:25  1 proc   cid:7   cid:12  cid:25  P1 proc  cid:7   cid:12  cid:25  P2 proc   cid:7   cid:12  cid:25  P1 ⊗ P2 proc   cid:7   cid:12  cid:25  E event  cid:7   cid:12  cid:25  $ E proc  cid:7   cid:12  cid:25 ,a∼τ P proc  cid:7   cid:12  cid:25  ν a ∼ τ .P proc  cid:7   cid:12  cid:25  e : clsfd  cid:7   cid:12  cid:25  ! e proc   cid:7   cid:12  cid:25  0 event   cid:7   cid:12  cid:25  E1 event  cid:7   cid:12  cid:25  E2 event   cid:7   cid:12  cid:25  E1 + E2 event  cid:7 , x : clsfd  cid:12  cid:25  P proc  cid:7   cid:12  cid:25  ?  x.P   event  α cid:20 −→   cid:5  and E  α=⇒  P1 P1 ⊗ P2 α cid:20 −→ P1 P1 ⊗ P2  P   cid:25   α cid:20 −→ α cid:20 −→   cid:25    cid:25    cid:5  P 1  cid:5  P 1   cid:5  1 P2 ε cid:20 −→   cid:5  P 1   cid:25   ⊗ P2 α cid:20 −→ ⊗ P   cid:5  P 2  cid:5  2   cid:25   α=⇒ P α cid:20 −→   cid:25    cid:25   P  E  $ E  P  α  P   cid:5   cid:12  cid:25  α action α cid:20 −→  cid:5    cid:20 −−−→  cid:25 ,a∼τ ν a ∼ τ .P ν a ∼ τ .P e val cid:25   cid:12  cid:25  e : clsfd   cid:25   ! e   cid:20 −→  e !   cid:25   1   49.1a    49.1b    49.1c    49.1d    49.1e    49.1f    49.1g    49.1h    49.2a    49.2b    49.2c    49.2d    49.2e    469  49.2 Strong Equivalence  E1  α=⇒ E1 + E2   cid:25   P α=⇒   cid:25   P  e val cid:25  ?  x.P   e ?=⇒   cid:25   [e x]P   49.2f    49.2g   Assuming that substitution is valid for expressions, it is also valid for processes and  events.  Lemma 49.1. 1. If  cid:7 , x : τ  cid:12  cid:25  P proc and  cid:7   cid:12  cid:25  e : τ , then  cid:7   cid:12  cid:25  [e x]P proc. 2. If  cid:7 , x : τ  cid:12  cid:25  E event and  cid:7   cid:12  cid:25  e : τ , then  cid:7   cid:12  cid:25  [e x]E event.  Transitions preserve well-formedness of processes and events.  Lemma 49.2. 1. If  cid:12  cid:25  P proc and P 2. If  cid:12  cid:25  E event and E   cid:5   α cid:20 −→ P α=⇒   cid:5 , then  cid:12  cid:25  P proc. P , then  cid:12  cid:25  P proc.   cid:25    cid:25   49.2 Strong Equivalence  Bisimilarity makes precise the informal idea that two processes are equivalent if they each can take the same actions and, in doing so, evolve into equivalent processes. A process relation P is a family {P cid:25  } of binary relations between processes P and Q such that  cid:12  cid:25  P proc and  cid:12  cid:25  Q proc, and an event relation E is a family {E cid:25  } of binary relations between events E and F such that  cid:12  cid:25  E event and  cid:12  cid:25  F event. A strong  bisimulation is a pair  P,E  consisting of a process relation P and an event relation E satisfying the following conditions: 1. If P P cid:25  Q, then   a  if P   b  if Q   cid:5 , then there exists Q  cid:5 , then there exists P   cid:5  such that Q  cid:5  such that P  P  Q  2. If E E cid:25  F , then   cid:25    a  if E  P , then there exists Q such that F   b  if F  Q, then there exists P such that E  α cid:20 −→ α cid:20 −→   cid:25   α=⇒ α=⇒   cid:25    cid:25   α cid:20 −→ α cid:20 −→   cid:25    cid:25   P   cid:5  with P Q  cid:5  with P   cid:5  P cid:25  Q  cid:5 , and  cid:5  P cid:25  Q  cid:5 . Q with P P cid:25  Q, and P with P P cid:25  Q.  α=⇒ α=⇒   cid:25    cid:25    470  Process Equivalence  The qualiﬁer “strong” refers to the fact that the action α in the conditions on being a bisimulation include the silent action ε.  In Section 49.3, we discuss another notion of bisimulation in which the silent actions are treated specially.   Strong  equivalence is the pair  ≈, ≈  of process and event relations such that P ≈ cid:25  Q and E ≈ cid:25  F iff there exists a strong bisimulation  P,E  such that P P cid:25  Q, and E E cid:25  F . Lemma 49.3. Strong equivalence is a strong bisimulation.  Proof Follows immediately from the deﬁnition.  The deﬁnition of strong equivalence gives rise to the principle of proof by coinduction. To show that P ≈ cid:25  Q, it is enough to give a bisimulation  P,E  such that P P cid:25  Q  and similarly for events . An instance of coinduction that arises fairly often is to choose  P,E  to be  ≈∪P0, ≈∪E0  for some P0 and E0 such that P P0 Q, and show that this expansion is a bisimulation. Because strong equivalence is itself a bisimulation, this reduces to show  cid:5  cid:5  ≈ cid:25  Q  cid:5  cid:5  that if P  cid:5 , and similarly for event transitions . or P This proof method amounts to assuming what we are trying to prove and showing that this assumption is tenable. The proof that the expanded relation is a bisimulation may make use of the assumptions P0 and E0; in this sense “circular reasoning” is a perfectly valid method of proof.   cid:5  cid:5   and analogously for transitions from Q   cid:5  cid:5  such that either P   cid:5  cid:5  for some Q   cid:5  cid:5 , then Q   cid:5  cid:5  P0 Q   cid:5  P0 Q   cid:5  and P   cid:5  α cid:20 −→   cid:5  α cid:20 −→  Q  P   cid:25    cid:25   Lemma 49.4. Strong equivalence is an equivalence relation.  Proof For reﬂexivity and symmetry, it sufﬁces to note that the identity relation is a bisim- ulation, as is the converse of a bisimulation. For transitivity, we need that the composition of two bisimulations is again a bisimulation, which follows directly from the deﬁnition.  It remains to verify that strong equivalence is a congruence, which means that each of the process- and event-forming constructs respects strong equivalence. To show this we require the open extension of strong equivalence to processes and events with free variables. The relation  cid:7   cid:12  cid:25  P ≈ Q is deﬁned for processes P and Q such that  cid:7   cid:12  cid:25  P proc and  cid:7   cid:12  cid:25  Q proc to mean that ˆγ  P   ≈ cid:25  ˆγ  Q  for every substitution, γ , of closed values of appropriate type for the variables  cid:7 . Lemma 49.5. If  cid:7 , x : clsfd  cid:12  cid:25  P ≈ Q, then  cid:7   cid:12  cid:25  ?  x.P   ≈ ?  x.Q . Proof Fix a closing substitution γ for  cid:7 , and let ˆP = ˆγ  P   and ˆQ = ˆγ  Q . By assumption, we have x : clsfd  cid:12  cid:25  ˆP ≈ ˆQ. We are to show that ?  x. ˆP   ≈ cid:25  ?  x. ˆQ . The proof is by coinduction, taking P = ≈ and E = ≈ ∪ E0, where  E0 = {  ?  x.P   cid:5    , ?  x.Q   cid:5       x : clsfd  cid:12  cid:25  P   cid:5  ≈ Q   cid:5  }.  Clearly, ?  x. ˆP   E0 ?  x. ˆQ . Suppose that ?  x.P  cid:5  cid:5  = [v x]P if ?  x.P   cid:5  cid:5 , then α = v ? and P   cid:5   α=⇒  P   cid:5   E0 ?  x.Q  cid:5  . By inspection of rules  49.2 ,  cid:5  for some v val cid:25  such that  cid:12  cid:25  v : clsfd.   cid:25    471  49.2 Strong Equivalence   cid:25    cid:5  ≈ cid:25  [v x]Q  [v x]Q  cid:5  E0 [v x]Q   cid:5 , and we have that [v x]P   cid:5  by the deﬁnition of E0,  cid:5 , as required. The symmetric case follows symmetrically,   cid:5   v ?=⇒ But ?  x.Q and hence [v x]P completing the proof. Lemma 49.6. If  cid:7   cid:12  cid:25 ,a∼τ P ≈ Q, then  cid:7   cid:12  cid:25  ν a ∼ τ .P ≈ ν a ∼ τ .Q. Proof Fix a closing value substitution γ for  cid:7 , and let ˆP = ˆγ  P   and ˆQ = ˆγ  Q . ˆQ, we are to show that ν a ∼ τ . ˆP ≈ cid:25  ν a ∼ τ . ˆQ. The proof is by Assuming that ˆP ≈ cid:25 ,a∼τ coinduction, taking P = ≈ ∪ P0 and E = ≈, where  cid:5  , ν a ∼ τ .Q  P0 = {  ν a ∼ τ .P   cid:5  ≈ cid:25 ,a∼τ Q     P   cid:5  }.   cid:5  P0 ν a ∼ τ .Q  Clearly, ν a ∼ τ . ˆP P0 ν a ∼ τ . ˆQ. Suppose that ν a ∼ τ .P α cid:20 −→ ν a ∼ τ .P  cid:5  cid:5  = ν a ∼ τ .P  cid:5  ≈ cid:25 ,a∼τ Q P  cid:5  cid:5  and by deﬁnition of P0 we have P we have that ν a ∼ τ .Q The symmetric case is proved symmetrically, completing the proof.   cid:5 , and that  cid:5  cid:5 . By inspection of rules  49.2 , we see that  cid:12  cid:25  α action and that P  cid:5  cid:5  cid:5 . But by deﬁnition of P0, we have  cid:5  cid:5  cid:5  for some P  cid:5  cid:5  = ν a ∼ τ .Q  cid:5 , and hence Q  cid:5   cid:5 ,  cid:20 −−−→  cid:5  cid:5 , as required.  cid:25 ,a∼τ   cid:5  cid:5  cid:5  such that P  cid:20 −−−→  cid:25 ,a∼τ Q   cid:5  cid:5  cid:5 . Letting Q  cid:5  cid:5  P0 Q   cid:20 −−−→  cid:25 ,a∼τ  cid:5  cid:5  cid:5  with P   cid:5  cid:5  cid:5  ≈ cid:25 ,a∼τ Q  Q  P  P   cid:25   α  α  α   cid:5    cid:5    cid:5    cid:5   Lemmas 49.5 and 49.6 capture two different cases of binding, the former of variables, and the latter of classes. The hypothesis of Lemma 49.5 relates all substitutions for the variable x in the recipient processes, whereas the hypothesis of Lemma 49.6 relates the constituent processes schematically in the class name, a. This makes all the difference, for if we were to consider all substitution instances of a class name by another class name, then a class would no longer be “new” within its scope, because we could identify it with an “old” class by substitution. On the other hand, we must consider substitution instances for variables, because the meaning of a variable is given in such terms. This shows that classes and variables must be distinct concepts.  See Chapter 33 for an example of what goes wrong when the two concepts are confused.  Lemma 49.7. If  cid:7   cid:12  cid:25  P1 ≈ Q1 and  cid:7   cid:12  cid:25  P2 ≈ Q2, then  cid:7   cid:12  cid:25  P1 ⊗ P2 ≈ Q1 ⊗ Q2. Proof Let γ be a closing value substitution for  cid:7 , and let ˆPi = ˆγ  Pi  and ˆQi = ˆγ  Qi  for i = 1, 2. The proof is by coinduction, considering the relation P = ≈∪P0 and E = ≈, where  ⊗ P  cid:5  1 P0 Q  cid:5  1  ⊗ Q 2   P  cid:5   cid:5   cid:5   cid:5   cid:5   cid:5  1 and P ≈ cid:25  Q 2, Q 2 1 1 α cid:20 −→ ⊗ Q ⊗ P  cid:5   cid:5   cid:5  2, and that P 1 2  P0 = {  P ≈ cid:25  Q ⊗ P  cid:5   cid:5  cid:5 . There are two cases to Suppose that P 1  cid:5  cid:5  = P  cid:5  cid:5  2 with consider, the interesting one being rule  49.2b . In this case, we have P  cid:5   cid:5  cid:5   cid:5  cid:5  2 with 1 and Q Q 2  cid:5  cid:5  P0 Q  cid:5  cid:5 , as required.   cid:5  P 1  cid:5  cid:5  P 1 The symmetric case is handled symmetrically, and rule  49.2a  is handled similarly.  2 . By deﬁnition of P0, we have that Q  cid:5  cid:5   α cid:20 −→  cid:5  cid:5   cid:5  1 and P P 2  cid:5  cid:5   cid:5  cid:5   cid:25  1 and P ≈ cid:25  Q 2  Q ⊗ Q  cid:5  cid:5  2, we have that P   cid:5  cid:5  2. Letting Q  α cid:20 −→  cid:25  ≈ cid:25  Q  ⊗ P  cid:5  cid:5  1 α cid:20 −→   cid:5  cid:5  = Q  cid:5  cid:5  1  α cid:20 −→  }.   cid:5  2   cid:5  2   cid:5  1  P  P   cid:25    cid:25    cid:25    472  Process Equivalence  Lemma 49.8. If  cid:7   cid:12  cid:25  E1 ≈ F1 and  cid:7   cid:12  cid:25  E2 ≈ F2, then  cid:7   cid:12  cid:25  E1 + E2 ≈ F1 + F2.  Proof Follows immediately from rules  49.2  and the deﬁnition of bisimulation. Lemma 49.9. If  cid:7   cid:12  cid:25  E ≈ F , then  cid:7   cid:12  cid:25  $ E ≈ $ F .  Proof Follows immediately from rules  49.2  and the deﬁnition of bisimulation. Lemma 49.10. If  cid:7   cid:12  cid:25  d  ∼= e : clsfd, then  cid:7   cid:12  cid:25  ! d ≈ ! e.  Proof The process calculus introduces no new observations on expressions, so that d and e remain indistinguishable as actions.  Theorem 49.11. Strong equivalence is a congruence.  Proof Follows immediately from the preceding lemmas, which cover each case separately.  49.3 Weak Equivalence  Strong equivalence expresses the idea that two processes are equivalent if they simulate each other step-by-step. Every action taken by one process is matched by a corresponding action taken by the other. This seems natural for the non-trivial actions e ! and e ? but is arguably overly restrictive for the silent action ε. Silent actions correspond to the actual steps of computation, whereas the send and receive actions express the potential to interact with another process. Silent steps are therefore of a very different ﬂavor than the other forms of action, and therefore might usefully be treated differently from them. Weak equivalence seeks to do just that.  Silent actions arise within the process calculus itself  when two processes communicate , but they play an even more important role when the dynamics of expressions is considered explicitly  as in Chapter 40 . For then each step e  cid:20 −→  cid:5  of evaluation of an expression corresponds to a silent transition for any process in which it is embedded. In particular,  cid:5 . We may also consider atomic processes of the form run m  ! e consisting of a command to be executed in accordance with the rules of some underlying dynamics. Here again we would expect that each step of command execution induces a silent transition from one atomic process to another.   cid:5  whenever e  cid:20 −→  ε cid:20 −→  ! e  e  e   cid:25    cid:25    cid:25   From the point of view of equivalence, it therefore seems sensible to allow that a silent action by one process may be mimicked by one or more silent actions by an- other. For example, there is little to be gained by distinguishing, say, run ret 3+4  from run ret  1+2 + 2+2   merely because the latter takes more steps to compute the same value than the former! The purpose of weak equivalence is precisely to disregard such   473  49.4 Notes  trivial distinctions by allowing a transition to be matched by a matching transition, possibly preceded by any number of silent transitions. A weak bisimulation is a pair  P,E  consisting of a process relation P and an event relation E satisfying the following conditions: 1. If P P cid:25  Q, then   cid:5 , where α  cid:6 = ε, then there exists Q ∗   cid:5 , and if P   cid:5 , then Q  P   cid:5 , where α  cid:6 = ε, then there exists P ∗   cid:5 , and if Q   cid:5 , then P  Q   cid:5  cid:5  and Q  cid:5  with P Q  cid:5  cid:5  and P  cid:5  with P   cid:5  such that Q  cid:5  P cid:25  Q  cid:5  such that P  cid:5  P cid:25  Q   cid:5 ;   cid:5 ;  P  ε cid:20 −→   cid:25   ε cid:20 −→   cid:25   ε cid:20 −→   cid:25   ε cid:20 −→   cid:25   ∗  ε cid:20 −→   cid:25    cid:5  cid:5  α cid:20 −→   cid:25   Q  Q  ∗  ε cid:20 −→   cid:25    cid:5  cid:5  α cid:20 −→   cid:25   P  P   cid:5    cid:5   P   a  if P   b  if Q  α cid:20 −→  cid:25   cid:5  P cid:25  Q with P α cid:20 −→  cid:5  P cid:25  Q 2. If E E cid:25  F , then α=⇒ α=⇒   cid:25  with P   a  if E   b  if F  Q   cid:25    cid:25   P , then there exists Q such that F  Q, then there exists P such that E  Q with P P cid:25  Q, and P with P P cid:25  Q.  α=⇒ α=⇒   cid:25    cid:25    The conditions on the event relation are the same as for strong bisimilarity because there are, in this calculus, no silent actions on events.  Weak equivalence is the pair  ∼, ∼  of process and event relations deﬁned by P ∼ cid:25  Q and E ∼ cid:25  F iff there exists a weak bisimulation  P,E  such that P P cid:25  Q, and E E cid:25  F . The open extension of weak equivalence, written  cid:7   cid:12  cid:25  P ∼ Q and  cid:7   cid:12  cid:25  E ∼ F , is deﬁned exactly as is the open extension of strong equivalence.  Theorem 49.12. Weak equivalence is an equivalence relation and a congruence.  Proof The proof proceeds along similar lines to that of Theorem 49.11.  49.4 Notes  The literature on process equivalence is extensive. Numerous variations have been con- sidered for an equally numerous array of formalisms. Milner recounts the history and development of the concept of bisimilarity in his monograph on the π-calculus  Milner, 1999 , crediting David Park with its original conception  Park, 1981 . The development in this chapter is inspired by Milner, and by a proof of congruence of strong bisimilarity given by Bernardo Toninho for the process calculus considered in Chapter 39.    P A R T XIX  Appendices    A  Background on Finite Sets  We make frequent use of the concepts of a ﬁnite set of discrete objects and of ﬁnite functions between them. A set X is discrete iff equality of its elements is decidable: for every x, y ∈ X, either x = y ∈ X or x  cid:6 = y ∈ X. This condition is to be understood constructively as stating that we may effectively determine whether any two elements of the set X are equal. Perhaps the most basic example of a discrete set is the set N of natural ∼= N between X and the set numbers. A set X is countable iff there is a bijection f : X ∼= { 0, . . . , n − 1}, where of natural numbers, and it is ﬁnite iff there is a bijection, f : X n ∈ N, between it and some inital segment of the natural numbers. This condition is again to be understood constructively in terms of computable mappings, so that countable and ﬁnite sets are computably enumerable and, in the ﬁnite case, have a computable size. Given countable sets, U and V , aﬁnite function is a computable partial function φ : U → V between them. The domain dom φ  of φ is the set { u ∈ U  φ u  ↓}, of objects u ∈ U such that φ u  = v for some v ∈ V . Two ﬁnite functions, φ and ψ, between U and V are disjoint iff dom φ  ∩ dom ψ  = ∅. Theempty ﬁnite function, ∅, between U and V is the totally undeﬁned partial function between them. If u ∈ U and v ∈ V , then the ﬁnite function, u  cid:9 → v, between U and V sends u to v, and is undeﬁned otherwise; its domain is therefore the singleton set { u}. In some situations, we write u ∼ v for the ﬁnite function u  cid:9 → v. If φ and ψ are two disjoint ﬁnite functions from U to V , then φ ⊗ ψ is the ﬁnite function  from U to V deﬁned by the equation  ⎧⎪⎪⎨⎪⎪⎩φ u   ψ v  undeﬁned  if u ∈ dom φ  if v ∈ dom ψ  otherwise   φ ⊗ ψ  u  =  If u1, . . . , un ∈ U are pairwise distinct, and v1, . . . , vn ∈ V , then we sometimes write u1  cid:9 →v1, . . . , un  cid:9 →vn, or u1∼v1, . . . , un ∼vn, for the ﬁnite function u1  cid:9 →v1⊗. . .⊗un  cid:9 →vn.    Bibliography  Mart´ın Abadi and Luca Cardelli. A Theory of Objects. Springer-Verlag, 1996. Peter Aczel. An introduction to inductive deﬁnitions. In Jon Barwise, editor, Handbook of  Mathematical Logic, chapter C.7, pages 783–818. North-Holland, 1977. John Allen. Anatomy of LISP. Computer Science Series. McGraw-Hill, 1978. S. F. Allen, M. Bickford, R. L. Constable, R. Eaton, C. Kreitz, L. Lorigo, and E. Moran. Innovations in computational type theory using Nuprl. Journal of Applied Logic, 4 4 :428–469, 2006. ISSN 1570-8683. doi: 10.1016 j.jal.2005.10.005.  Stuart Allen. A non-type-theoretic deﬁnition of Martin-L¨of’s types. In LICS, pages 215–  Zena M. Ariola and Matthias Felleisen. The call-by-need lambda calculus. J. Funct.  221, 1987.  Program., 7 3 :265–301, 1997.  Arvind, Rishiyur S. Nikhil, and Keshav Pingali. I-structures: Data structures for parallel In Joseph H. Fasel and Robert M. Keller, editors, Graph Reduction, computing. volume 279 of Lecture Notes in Computer Science, pages 336–369. Springer, 1986. ISBN 3-540-18420-1.  Arnon Avron. Simple consequence relations. Information and Computation, 92:105–139,  1991.  Henk Barendregt. The Lambda Calculus, Its Syntax and Semantics, volume 103 of Studies  in Logic and the Foundations of Mathematics. North-Holland, 1984.  Henk Barendregt. Lambda calculi with types. In S. Abramsky, D. M. Gabbay, and T. S. E. Maibaum, editors, Handbook of Logic in Computer Science, volume 2, Computational Structures. Oxford University Press, 1992.  Yves Bertot, G´erard Huet, Jean-Jacques L´evy, and Gordon Plotkin, editors. From Semantics to Computer Science: Essays in Honor of Gilles Kahn. Cambridge University Press, 2009.  Guy E. Blelloch. Vector Models for Data-Parallel Computing. MIT Press, 1990. ISBN  Guy E. Blelloch and John Greiner. Parallelism in sequential functional languages.  In  0-262-02313-X.  FPCA, pages 226–237, 1995.  Guy E. Blelloch and John Greiner. A provable time and space efﬁcient implementation of  NESL. In ICFP, pages 213–225, 1996.  Manuel Blum. On the size of machines. Information and Control, 11 3 :257–265, Septem-  ber 1967.  Stephen D. Brookes. The essence of parallel algol. Inf. Comput., 179 1 :118–149, 2002.   480  Bibliography  Samuel R. Buss, editor. Handbook of Proof Theory. Elsevier, 1998. Luca Cardelli. Structural subtyping and the notion of power type. In Proc. ACM Symposium  on Principles of Programming Languages, pages 70–79, 1988.  Luca Cardelli. Program fragments, linking, and modularization. In Proc. ACM Symposium  on Principles of Programming Languages, pages 266–277, 1997.  Giuseppe Castagna and Benjamin C. Pierce. Decidable bounded quantiﬁcation. In Proc.  ACM Symposium on Principles of Programming Languages, pages 151–162, 1994. Alonzo Church. The Calculi of Lambda-Conversion. Princeton University Press, 1941. Robert L. Constable. Implementing Mathematics with the Nuprl Proof Development System.  Robert L. Constable. Types in logic, mathematics, and programming.  In Buss  1998 ,  Robert L. Constable and Scott F. Smith. Partial objects in constructive type theory. In LICS,  pages 183–193. IEEE Computer Society, 1987.  William R. Cook. On understanding data abstraction, revisited. In OOPSLA, pages 557–  Prentice-Hall, 1986.  chapter X.  572, 2009.  Rowan Davies. Practical Reﬁnement-Type Checking. PhD thesis, Carnegie Mellon Univer- sity School of Computer Science, May 2005. Available as Technical Report CMU– CS–05–110.  Rowan Davies and Frank Pfenning.  In Martin Odersky and Philip Wadler, editors, ICFP, pages 198–208. ACM, 2000. ISBN 1-58113-202-6.  Intersection types and computational effects.  Ewen Denney. Reﬁnement types for speciﬁcation. In David Gries and Willem P. de Roever, editors, PROCOMET, volume 125 of IFIP Conference Proceedings, pages 148–166. Chapman & Hall, 1998. ISBN 0-412-83760-9.  Derek Dreyer. Understanding and Evolving the ML Module System. PhD thesis, Carnegie  Mellon University, Pittsburgh, PA, May 2005.  Joshua Dunﬁeld and Frank Pfenning. Type assignment for intersections and unions in call- by-value languages. In Andrew D. Gordon, editor, FoSSaCS, volume 2620 of Lecture Notes in Computer Science, pages 250–266. Springer, 2003. ISBN 3-540-00897-7.  Uffe Engberg and Mogens Nielsen. A calculus of communicating systems with label passing—ten years after. In Gordon D. Plotkin, Colin Stirling, and Mads Tofte, editors, Proof, Language, and Interaction, Essays in Honour of Robin Milner, pages 599–622. The MIT Press, 2000.  Matthias Felleisen and Robert Hieb. The revised report on the syntactic theories of sequen-  tial control and state. TCS: Theoretical Computer Science, 103, 1992.  Tim Freeman and Frank Pfenning. Reﬁnement types for ml. In David S. Wise, editor,  PLDI, pages 268–277. ACM, 1991. ISBN 0-89791-428-7.  Daniel Friedman and David Wise. The impact of applicative programming on multipro-  cessing. In International Conference on Parallel Processing, 1976.  David Gelernter. Generative communication in Linda. ACM Trans. Program. Lang. Syst.,  7 1 :80–112, 1985.   481  Bibliography  Gerhard Gentzen.  Investigations  In M. E. Szabo, edi- tor, The Collected Papers of Gerhard Gentzen, pages 68–213. North-Holland, 1969.  into logical deduction.  J.-Y. Girard.  Interpretation fonctionelle et elimination des coupures de l’arithmetique  d’ordre superieur. These d’etat, Universite Paris VII, 1972.  Jean-Yves Girard. Proofs and Types. Cambridge University Press, 1989. Translated by  Paul Taylor and Yves Lafont.  Kurt G¨odel. On a hitherto unexploited extension of the ﬁnitary standpoint. Journal of Philosophical Logic, 9:133–142, 1980. Translated by Wilfrid Hodges and Bruce Watson.  G¨odel Von Kurt. ¨Uber eine bisher noch nicht ben¨utzte erweiterung des ﬁniten standpunktes.  dialectica, 12 3-4 :280–287, 1958.  Michael J. Gordon, Arthur J. Milner, and Christopher P. Wadsworth. Edinburgh LCF,  volume 78 of Lecture Notes in Computer Science. Springer-Verlag, 1979.  John Greiner and Guy E. Blelloch. A provably time-efﬁcient parallel implementation of  full speculation. ACM Trans. Program. Lang. Syst., 21 2 :240–285, 1999.  Timothy Grifﬁn. A formulae-as-types notion of control. In Proc. ACM Symposium on  Principles of Programming Languages, pages 47–58, 1990.  Carl Gunter. Semantics of Programming Languages. Foundations of Computing Series.  Robert H. Halstead, Jr. Multilisp: A language for concurrent symbolic computation. ACM  Trans. Program. Lang. Syst., 7 4 :501–538, 1985.  Robert Harper. Constructing type systems over an operational semantics. J. Symb. Comput.,  Robert Harper. A simpliﬁed account of polymorphic references. Inf. Process. Lett., 51 4 :  MIT Press, 1992.  14 1 :71–84, 1992.  201–206, 1994.  Robert Harper, Furio Honsell, and Gordon Plotkin. A framework for deﬁning logics. Journal  of the Association for Computing Machinery, 40:194–204, 1993.  Robert Harper and Mark Lillibridge. A type-theoretic approach to higher-order modules with sharing. In Proc. ACM Symposium on Principles of Programming Languages, pages 123–137, 1994.  Robert Harper, John C. Mitchell, and Eugenio Moggi. Higher-order modules and the phase In Proc. ACM Symposium on Principles of Programming Languages,  distinction. pages 341–354, 1990.  Ralf Hinze and Johan Jeuring. Generic Haskell: Practice and theory.  In Roland Carl Backhouse and Jeremy Gibbons, editors, Generic Programming, volume 2793 of Lecture Notes in Computer Science, pages 1–56. Springer, 2003. ISBN 3-540-20194- 7.  C. A. R. Hoare. Communicating sequential processes. Commun. ACM, 21 8 :666–677,  Tony Hoare. Null references: The billion dollar mistake. Presentation at QCon 2009, August  1978.  2009.   482  Bibliography  S. C. Kleene. Introduction to Metamathematics. Van Nostrand, 1952. Imre Lakatos. Proofs and Refutations: The Logic of Mathematical Discovery. Cambridge  P. J. Landin. A correspondence between Algol 60 and Church’s lambda notation. CACM,  University Press, 1976.  8:89–101; 158–165, 1965.  Daniel K. Lee, Karl Crary, and Robert Harper. Towards a mechanized metatheory of standard ml. In Proc. ACM Symposium on Principles of Programming Languages, pages 173–184, 2007.  Xavier Leroy. Manifest types, modules, and separate compilation. In Proc. ACM Symposium  on Principles of Programming Languages, pages 109–122, 1994.  Xavier Leroy. Applicative functors and fully transparent higher-order modules. In Proc.  ACM Symposium on Principles of Programming Languages, pages 142–153, 1995.  Mark Lillibridge. Translucent Sums: A Foundation for Higher-Order Module Systems. PhD thesis, Carnegie Mellon University School of Computer Science, Pittsburgh, PA, May 1997.  Barbara Liskov and Jeannette M. Wing. A behavioral notion of subtyping. ACM Trans.  Program. Lang. Syst., 16 6 :1811–1841, 1994.  Saunders MacLane. Categories for the Working Mathematician. Graduate Texts in Math-  ematics. Springer-Verlag, second edition, 1998.  David B. MacQueen. Using dependent types to express modular structure. In Proc. ACM  Symposium on Principles of Programming Languges, pages 277–286, 1986.  David B. MacQueen. Kahn networks at the dawn of functional programming. In Bertot  et al.  2009 , chapter 5.  Yitzhak Mandelbaum, David Walker, and Robert Harper. An effective theory of type reﬁnements. In Runciman and Shivers  2003 , pages 213–225. ISBN 1-58113-756-7. Per Martin-L¨of. Constructive mathematics and computer programming. In Logic, Method-  ology and Philosophy of Science IV, pages 153–175. North-Holland, 1980.  Per Martin-L¨of. On the meanings of the logical constants and the justiﬁcations of the  logical laws. Unpublished Lecture Notes, 1983.  Per Martin-L¨of. Intuitionistic Type Theory. Studies in Proof Theory. Bibliopolis, Naples,  Per Martin-L¨of. Truth of a proposition, evidence of a judgement, validity of a proof.  Italy, 1984.  Synthese, 73 3 :407–420, 1987.  John McCarthy. LISP 1.5 Programmer’s Manual. MIT Press, 1965. N. P. Mendler. Recursive types and type constraints in second-order lambda calculus. In  LICS, pages 30–36, 1987.  Robin Milner. A theory of type polymorphism in programming. JCSS, 17:348–375, 1978. Robin Milner. Communicating and mobile systems—the Pi-calculus. Cambridge University  Press, 1999. ISBN 978-0-521-65869-0.  Robin Milner, Mads Tofte, Robert Harper, and David MacQueen. The Deﬁnition of Standard  ML  Revised . MIT Press, 1997.   483  Bibliography  John C. Mitchell. Coercion and type inference. In Proc. ACM Symposium on Principles of  Programming Languages, pages 175–185, 1984.  John C. Mitchell. Representation independence and data abstraction.  In Proc. ACM  Symposium on Principles of Programming Languages, pages 263–276, 1986.  John C. Mitchell. Foundations for Programming Languages. MIT Press, 1996. John C. Mitchell and Gordon D. Plotkin. Abstract types have existential type. ACM Trans.  Program. Lang. Syst., 10 3 :470–502, 1988.  Eugenio Moggi. Computational lambda-calculus and monads. In LICS, pages 14–23. IEEE  Computer Society, 1989. ISBN 0-8186-1954-6.  Tom Murphy VII, Karl Crary, Robert Harper, and Frank Pfenning. A symmetric modal  lambda calculus for distributed computing. In LICS, pages 286–295, 2004.  Chetan R. Murthy. An evaluation semantics for classical proofs. In LICS, pages 96–107.  IEEE Computer Society, 1991.  Aleksandar Nanevski. From dynamic binding to state via modal possibility. In PPDP,  pages 207–218. ACM, 2003. ISBN 1-58113-705-2.  R. P. Nederpelt, J. H. Geuvers, and R. C. de Vrijer, editors. Selected Papers on Automath, volume 133 of Studies in Logic and the Foundations of Mathematics. North-Holland, 1994.  B. Nordstrom, K. Petersson, and J. M. Smith. Programming in Martin-L¨of’s Type Theory. Oxford University Press, 1990. URL http:  www.cs.chalmers.se Cs Research Logic  book.  OCaml. Ocaml, 2012. URL http:  caml.inria.fr ocaml . David Michael Ritchie Park. Concurrency and automata on inﬁnite sequences.  In Pe- ter Deussen, editor, Theoretical Computer Science, volume 104 of Lecture Notes in Computer Science, pages 167–183. Springer, 1981. ISBN 3-540-10576-X.  Frank Pfenning and Rowan Davies. A judgmental reconstruction of modal logic. Mathe-  matical Structures in Computer Science, 11 4 :511–540, 2001.  Benjamin C. Pierce. Types and Programming Languages. MIT Press, 2002. Benjamin C. Pierce. Advanced Topics in Types and Programming Languages. MIT Press,  2004.  Andrew M. Pitts. Existential types: Logical relations and operational equivalence.  In Kim Guldstrand Larsen, Sven Skyum, and Glynn Winskel, editors, ICALP, volume 1443 of Lecture Notes in Computer Science, pages 309–326. Springer, 1998. ISBN 3-540-64781-3.  Andrew M. Pitts. Operational semantics and program equivalence. In Gilles Barthe, Peter Dybjer, Luis Pinto, and Jo˜ao Saraiva, editors, APPSEM, volume 2395 of Lecture Notes in Computer Science, pages 378–412. Springer, 2000. ISBN 3-540-44044-5.  Andrew M. Pitts and Ian D. B. Stark. Observable properties of higher order functions that dynamically create local names, or what’s new? In Andrzej M. Borzyszkowski and Stefan Sokolowski, editors, MFCS, volume 711 of Lecture Notes in Computer Science, pages 122–141. Springer, 1993. ISBN 3-540-57182-5.   484  Bibliography  G. D. Plotkin. A structural approach to operational semantics. Technical Report DAIMI  FN-19, Aarhus University Computer Science Department, 1981.  Gordon D. Plotkin. LCF considered as a programming language. Theor. Comput. Sci., 5   3 :223–255, 1977.  Gordon D. Plotkin. The origins of structural operational semantics. J. of Logic and  Algebraic Programming, 60:3–15, 2004.  John H. Reppy. Concurrent Programming in ML. Cambridge University Press, 1999. J. C. Reynolds. Types, abstraction, and parametric polymorphism. In Information Process-  ing ’83, pages 513–523. North-Holland, 1983.  John C. Reynolds. Towards a theory of type structure. In Bernard Robinet, editor, Sym- posium on Programming, volume 19 of Lecture Notes in Computer Science, pages 408–423. Springer, 1974. ISBN 3-540-06859-7.  John C. Reynolds. Using category theory to design implicit conversions and generic oper- ators. In Neil D. Jones, editor, Semantics-Directed Compiler Generation, volume 94 of Lecture Notes in Computer Science, pages 211–258. Springer, 1980. ISBN 3-540- 10250-7.  John C. Reynolds. The essence of Algol.  In Proceedings of the 1981 International  Symposium on Algorithmic Languages, pages 345–372. North-Holland, 1981.  John C. Reynolds. The discoveries of continuations. Lisp and Symbolic Computation, 6  John C. Reynolds. Theories of Programming Languages. Cambridge University Press,   3-4 :233–248, 1993.  1998.  Andreas Rossberg, Claudio V. Russo, and Derek Dreyer. F-ing modules.  In Andrew Kennedy and Nick Benton, editors, TLDI, pages 89–102. ACM, 2010. ISBN 978-1- 60558-891-9.  Colin Runciman and Olin Shivers, editors. Proceedings of the Eighth ACM SIGPLAN International Conference on Functional Programming, ICFP 2003, Uppsala, Sweden, August 25-29, 2003, 2003. ACM. ISBN 1-58113-756-7.  Dana Scott. Lambda calculus: Some models, some philosophy. In J. Barwise, H. J. Keisler, and K. Kunen, editors, The Kleene Symposium, pages 223–265. North Holland, 1980a.  Dana S. Scott. Data types as lattices. SIAM J. Comput., 5 3 :522–587, 1976. Dana S. Scott. Relating theories of the lambda calculus. To HB Curry: Essays on combi-  natory logic, lambda calculus and formalism, pages 403–450, 1980b.  Dana S. Scott. Domains for denotational semantics. In Mogens Nielsen and Erik Meineche Schmidt, editors, ICALP, volume 140 of Lecture Notes in Computer Science, pages 577–613. Springer, 1982. ISBN 3-540-11576-5.  Michael B. Smyth and Gordon D. Plotkin. The category-theoretic solution of recursive  domain equations. SIAM J. Comput., 11 4 :761–783, 1982.  Richard Statman. Logical relations and the typed lambda-calculus.  Information and  Control, 65 2 3 :85–97, 1985.  Guy L. Steele. Common Lisp: The Language. Digital Press, 2nd edition edition, 1990.   485  Bibliography  Christopher A. Stone and Robert Harper. Extensional equivalence and singleton types.  ACM Trans. Comput. Log., 7 4 :676–722, 2006.  Paul Taylor. Practical Foundations of Mathematics. Cambridge Studies in Advanced  Mathematics. Cambridge University Press, 1999.  P. W. Trinder, K. Hammond, H.-W. Loidl, and S. L. Peyton Jones. Algorithm + strategy =  parallelism. Journal of Functional Programming, 8:23–60, 1998.  Jaap van Oosten. Realizability: A historical essay. Mathematical Structures in Computer  Science, 12 3 :239–263, 2002.  Philip Wadler. Theorems for free! In FPCA, pages 347–359, 1989. Philip Wadler. Comprehending monads. Mathematical Structures in Computer Science, 2  Philip Wadler. Call-by-value is dual to call-by-name. In Runciman and Shivers  2003 ,  pages 189–201. ISBN 1-58113-756-7.  Mitchell Wand. Fixed-point constructions in order-enriched categories. Theor. Comput.   4 :461–493, 1992.  Sci., 8:13–30, 1979.  Stephen A. Ward and Robert H. Halstead. Computation structures. MIT Electrical Engi- neering and Computer Science Series. MIT Press, 1990. ISBN 978-0-262-23139-8. Kevin Watkins, Iliano Cervesato, Frank Pfenning, and David Walker. Specifying properties of concurrent computations in clf. Electr. Notes Theor. Comput. Sci., 199:67–87, 2008. Andrew K. Wright and Matthias Felleisen. A syntactic approach to type soundness. Inf.  Comput., 115 1 :38–94, 1994.  Hongwei Xi and Frank Pfenning. Eliminating array bound checking through dependent types. In Jack W. Davidson, Keith D. Cooper, and A. Michael Berman, editors, PLDI, pages 249–257. ACM, 1998. ISBN 0-89791-987-4.    Index  call-by-need, see laziness capabilities, 313 channel types, see Concurrent Algol class types, 293  deﬁnability, 294 dynamics, 293 statics, 293  classes, see dynamic dispatch classical logic, 104  classical laws, 114 contradiction, 105, 106, 107 derivability of elimination forms,  109  double-negation translation, 113, 115 dynamics, 110 excluded middle, 111 judgments, 105 proof, 106  conjunction, 108 disjunction, 108 implication, 108 negation, 108 truth, 107 variable, 107 provability, 105  conjunction, 106 disjunction, 106 hypothesis, 105 implication, 106 negation, 106 truth, 106  refutability, 105  conjunction, 106 disjunction, 106 falsehood, 106 hypothesis, 105 implication, 106 negation, 106 refutation, 106  conjunction, 108 disjunction, 108 falsehood, 108 implication, 108 negation, 108 variable, 107  safety, 111  FPC, see recursive types M, see inductive types, coinductive types  cid:2 , see untyped λ-calculus F, see universal types Fω, see higher kinds MA, see Modernized Algol PCF, see Plotkin’s PCF PPCF, see parallelism T, see G¨odel’s T  abstract binding tree, 3, 6  abstractor, 7 valence, 7  α-equivalence, 9 bound variable, 8 capture, 9 free variable, 8 graph representation, 11 operator, 7 arity, 7 parameter, 10  structural induction, 8 substitution, 9 weakening, 11  abstract binding trees  closed, 30  abstract syntax tree, 3, 5  operator, 3 arity, 3 index, 10 parameter, 10  structural induction, 5 substitution, 6 variable, 3 weakening, 11  abstract types, see existential types, see also signatures abstracted modules, see signatures abt, see abstract binding tree assignables, see Modernized Algol ast, see abstract syntax tree  back-patching, see references benign effects, see references bidirectional typing, 37 boolean blindness, see type reﬁnements boolean type, 88   488  Index  classiﬁed type, 291 applications, 295 conﬁdentiality, 295 dynamics, 292 exception values, 296 integrity, 295 named exceptions, 297 safety, 293 statics, 291  coinductive types dynamics, 130 inﬁnite trees, 133 statics, 129 streams, 126  combinators  sk basis, 29 bracket abstraction, 30 conversion, 29 substitution, 29  command types, see Modernized Algol compactness, see equality Concurrent Algol, 375  broadcast communication, 378  dynamics, 378 safety, 379 statics, 378  class declaration, 384 deﬁnability of free assignables, 382 dynamics, 376 RS latch, 384 selective communication, 380  dynamics, 381 statics, 380, 381  statics, 376  constructive logic, 95  Boolean algebra, 103 conservation of proof, 100 decidable proposition, 102 double-negation elimination, 102 Gentzen’s Principle, 100 Heyting algebra, 102 judgment forms, 96 law of the excluded middle, 102 proof, 99  conjunction, 99 disjunction, 100 falsehood, 99 implication, 99 truth, 99  proofs-as-programs, 101 propositions-as-types, 101 provability, 97  conjunction, 98 disjunction, 98 falsehood, 98 implication, 98  negation, 98 truth, 97  reversibility of proof, 100 semantics, 95  continuation types, 266  classical logic, 273 coroutines, 269 dynamics, 268 safety, 269 statics, 268 streams, 273 syntax, 268  continuations types  safety, 273  contravariance, see subtyping covariance, see subtyping  deﬁnitional equality, see equality Distributed Algol, 385  dynamics, 388 safety, 390 statics, 385  dynamic binding, see ﬂuids dynamic classiﬁcation, see classiﬁed type dynamic dispatch, 235 class-based, 236, 238  class vector, 238 instance, 238 message send, 238 object type, 238 self-reference, 241 dispatch matrix, 235 message not understood, 242, 243 method-based, 236  message send, 240 method vector, 239, 240 object type, 239 self-reference, 241  self-reference, 240, 243  instances, 243 results, 243  dynamic types, 189  as static types, 200 class dispatch, 194 cons, 193 critique, 194 destructors, 193 dynamics, 190 lists, 196 multiple arguments, 196 multiple results, 196 nil, 193 numeric classes, 192 pairs, 196 predicates, 193 safety, 192   489  Index  statics, 190 subtyping, 210  dynamic typing  vs. static typing, 203  dynamics, 33, 39  checked errors, 51 contextual, 42 cost, 56 deﬁnitional equality, 44 determinacy, 42 environmental evaluation, 58 equational, 44 equivalence theorem, 43 evaluation, 53  equivalence to transition, 54  evaluation context, 42 induction on transition, 40 inversion principle, 42 structural, 40 transition system, 39 unchecked errors, 51  dynamics types  arithmetic, 196  enumeration types, 89 equality, 435  coinduction, 439, 452 compactness, 447, 449, 451 congruence, 438 contexts, 436, 445 deﬁnitional, 44, 140, 165, 181, 400, 435 equational laws, 443 ﬁxed point induction, 447 Kleene equality, 445 Kleene equivalence, 437 logical equivalence, 435, 439, 440, 446  closed, 439, 440  observation, 436 observational equivalence, 435, 437, 440, 445  event types, see Concurrent Algol exceptions, 260, 262  dynamics, 262 evaluation dynamics, 264 exception type, 263, 297 safety, 263, 264 statics, 262 structural dynamics, 264 syntax, 262 type reﬁnements, 265  existential types  representation independence, 464  existential types, 146  coinductive types, 153 deﬁnability from universals, 150 dynamics, 148 modeling data abstraction, 149  parametricity, 153 representation independence, 151, 153 safety, 148 statics, 147 streams, 153 subtyping, 214  failures, 260, see also exceptions  dynamics, 260 failure-passing style, 265 safety, 261 statics, 260  ﬁnite function, 477 ﬁxed point induction, see equality ﬂuid binding, see ﬂuids ﬂuid reference types, 288  dynamics, 289 statics, 289  ﬂuids, 284  deep binding, 290 dynamics, 285 exception handling, 290 safety, 286 shallow binding, 290 statics, 284 subtleties, 287  function types  deﬁnitions, 61 dynamic binding, 66 ﬁrst order, 61  dynamics, 62 safety, 62 statics, 62  ﬁrst-class functions, 62 higher order, 62 dynamics, 63 safety, 64 statics, 63  second-class functions, 61 static binding, 66 subtyping, 211  functors, see signatures future types, 350 future let, 356 parallel dynamics, 352 parallel let, 356 pipelining, 354 sequential dynamics, 351 sparks, 355 statics, 351  futures, see future types  G¨odel’s T, 69  canonical forms, 75 deﬁnability, 71 deﬁnitional equality, 71   490  Index  G¨odel’s T  cont.   dynamics, 70 hereditary termination, 75 iterator, 69 recursor, 69 safety, 71, 75 statics, 70 termination, 75 undeﬁnability, 73  general judgment, 21, 26 generic derivability, 26  proliferation, 26 structurality, 26 substitution, 26  parametric derivability, 27  general recursion, 163 generic inductive deﬁnition, 27 formal generic judgment, 27 rule, 27 rule induction, 27 structurality, 28  Girard’s System F, see universal types  higher kinds, 154  constructor equality, 156 constructor statics, 155 constructors, 155 expression statics, 157 expressions, 157  hybrid types, 198  as recursive types, 200 dynamics, 199 optimization of dynamic types, 201 safety, 199 statics, 198  hypothetical inductive deﬁnition, 24  formal derivability, 25 rule, 24 rule induction, 25 uniformity of rules, 25 hypothetical judgment, 21  admissibility, 23 reﬂexivity, 24 structurality, 24 transitivity, 24 weakening, 24  derivability, 21 reﬂexivity, 22 stability, 22 structurality, 22 transitivity, 22 weakening, 22  inductive deﬁnition, 12  admissible rule, 23 backward chaining, 14  derivable rule, 21 derivation, 14 forward chaining, 14 function, 18 iterated, 17 rule, 12  axiom, 12 conclusion, 12 premise, 12  rule induction, 13, 15 rule scheme, 13 instance, 13  simultaneous, 17  inductive types  dynamics, 130 lists, 132 natural numbers, 125 statics, 129  inheritance, 245  class extension, 245 class-based, 246 method extension, 246 method specialization, 249 method-based, 248 self-reference, 249 simple method override, 249 subclass, 245 sub-method, 245 superclass, 245 super-method, 245  interface, see separate compilation  judgment, 12 judgment form, 12  predicate, 12 subject, 12  kinds  kinds  Kleene equality, see equality  laziness, 323  by-need general recursion, 326 data structures, 332 lists, 331 parallel or, 169 recursion, 331 safety, 326 suspension interpretation, 332 suspension types, 329  linking, see separate compilation logical equivalence, see equality  methods, see dynamic dispatch mobile types, 308  dependent, see singleton kinds,  cid:25  kinds,  cid:27    491  Index  mobility condition, 308 rules, 308  Modernized Algol, 301  arrays, 310 assignables, 301, 314 block structure, 304 classes and objects, 312 command types, 308 commands, 301, 307 control stack, 312 data stack, 312 expressions, 301 free assignables, 316 free dynamics, 316 idioms  conditionals, 306 iteration, 306 procedures, 306 sequential composition, 306  multiple declaration instances, 310 own assignables, 311 passive commands, 310 recursive procedures, 310 separated and consolidated stacks, 312 scoped dynamics, 303 scoped safety, 305 stack discipline, 304 stack machine, 312 statics, 302, 308  modules, see signatures mutual primitive recursion, 82  null, see option types  objects, see dynamic dispatch observational equivalence, see equality option types, 90  parallelism, 335  binary fork-join, 335 Brent’s Theorem, 343 cost dynamics, 338, 348 cost dynamics vs. transition dynamics,  339  cost graphs, 338 exceptions, 348 implicit parallelism theorem, 337 multiple fork-join, 341 parallel complexity, 339 parallel dynamics, 336 parallelizability, 343 provably efﬁcient implementation, 342 sequence types, 341  cost dynamics, 342 statics, 341  sequential complexity, 339  sequential dynamics, 335 statics, 335 structural dynamics, 348 task dynamics, 343, 348 work vs. depth, 339  parametricity, see equality phase distinction, 33, see also signatures  cid:27  kinds, 400, 402, 403 elimination rules, 404 equivalence, 404 formation rules, 404 introduction rules, 404 subkinding, 405 Plotkin’s PCF, 161  Blum size theorem, 168 bounded recursion, 446 deﬁnability, 166 deﬁnitional equality, 165 dynamics, 163 eager natural numbers, 167 eagerness and laziness, 167 halting problem, 169 induction, 167 mutual recursion, 169 safety, 164 statics, 162 totality and partiality, 167  polarity, 83 polymorphic types, see universal types primitive recursion, 82 process calculus, 359, 467  actions, 359 asynchronous communication, 368 bisimilarity, 467 channel types, 369  dynamics, 370 statics, 370  channels, 364, 366 coinduction, see strong and weak bisimilarity concurrent composition, 361 dynamics, 362, 365, 367, 468 equivalence, seebisimilarity467 events, 359 input choice, 374 Milner booleans, 373 polyadic π-calculus, 374 process choice, 373 replication, 363 RS latch, 373 sequential composition, 373 statics, 364, 367, 467 strong bisimilarity, 469, 470 strong bisimulation, 469  structural congruence, 359, 361 synchronization, 361 synchronous communication, 366   492  Index  process calculus  cont.   syntax, 467 universality, 371 weak bisimilarity, 472  coinduction, 473 weak bisimulation, 473  process equivalence, see process calculus product types, 79 dynamics, 80 ﬁnite, 81 safety, 80 statics, 79 subtyping, 209, 211  recursive types, see also type recursion  dynamics, 172 eager data structures, 172 eager lists, 173 eager natural numbers, 172 lazy data structures, 172 lazy lists, 173 lazy natural numbers, 173 RS latch, 178 self-reference, 174 signals, 178 statics, 171 subtyping, 212, 217  reference types, 313  aliasing, 315 free dynamics, 317 safety, 315, 318 scoped dynamics, 315 statics, 314  references  arrays, 322 back-patching, 320 benign effects, 320 mutable data structures, 322  reﬁnement types, see type reﬁnements reﬁnements, see type reﬁnements representation independence, see existential  representation independence, see also  types  parametricity  Reynolds’s Algol, see Modernized Algol  safety  evaluation, 55, 56  scoped assignables, see Modernized Algol self types, 174  deriving general recursion, 175 as recursive types, 175 self-reference, 174 unrolling, 174  separate compilation, 395  initialization, 396  interface, 395 linking, 395 units, 395   cid:25  kinds, 400, 402, 403 elimination rules, 403 equivalence, 403 formation rules, 403 introduction rules, 403 subkinding, 404  signatures, 409  abstracted modules, see functors abstraction, 425 applicative functor, 430 ascription, see sealing411 avoidance problem, 416 dictionary functor, 431 dynamic part, 411 dynamics, 418 ﬁrst- vs. second-class, 418 functors, 425, 426 generative functor, 428 graph abstraction, 420 graph class, 420 graph functor, 431 hierarchies, 422, 423 instances, 412 opacity, 410 principal signature, 413 revelation, 410 sealing, 411 self-recognition, 417, 429 set abstraction, 420 set functor, 431 sharing propagation, 423 sharing speciﬁcation, 423 signature modiﬁcation, 420 static part, 411 statics, 415, 427 structures, 410 submodule, 424 subsignature, 412, 413, 414 syntax, 414, 427 translucency, 410 transparency, 410 type abstractions, 409, 410 type classes, 409, 412 views, 412  singleton kinds, 400  constructor equivalence, 401 function kinds, 407 higher singletons, 400, 405 kind equivalence, 401 kind formation, 401 kind modiﬁcation, 407 product kinds, 407 sharing speciﬁcation, 407   493  Index  subkinding, 401 as type deﬁnitions, 402  sparks, see future types speculation types, 351  parallel dynamics, 352 sequential dynamics, 352 statics, 352  speculations, see speculation types stack machine, 253 correctness, 256  completeness, 257 soundness, 257 unraveling, 257  dynamics, 254 frame, 253 safety, 255 stack, 253 state, 253  state, 176  from recursion, 176 RS latch, 176  statics, 33  canonical forms, 36 decomposition, 36 induction on typing, 34 introduction and elimination, 36 structurality, 35 substitution, 35 type system, 34 unicity, 34 weakening, 35  structural subtyping, see subtyping subkinding, 400  cid:27  kinds, 405  cid:25  kinds, 404 singleton kinds, 401  submodules, see signatures subtyping, 207  behavioral vs. structural, 219 bounded quantiﬁcation, 214 class types, 210 coercion, 217 coherence, 217 dynamic types, 210 dynamics, 215 function types, 211 numeric types, 208 product types, 209, 211 quantiﬁed types, 214 recursive types, 212, 217 safety, 215 subsumption, 207 sum types, 209, 211 variance, 211, 217  sum types, 85  dynamics, 86  ﬁnite, 86 statics, 85 subtyping, 209, 211  suspension types, see laziness symbol reference, see symbol types symbol types, 280  association lists, 282 dynamics, 281 equality, 282 safety, 281 statics, 280 symbolic expressions, 283  symbols, 277  mobility, 279 modal separation, 283 safety, 279, 280 scope-free dynamics, 279 scoped dynamics, 278 statics, 278 total ordering, 282  syntax, 3  abstract, 3 binding, 3 chart, 33 concrete, 3 structural, 3 surface, 3  System F, see universal types System Fω, see higher kinds  type abstractions, see also existential types type classes, see signatures type constructors, see higher kinds type operators, 119 composition, 123 database programming, 123 generic extension, 120, 122  polynomial, 120 positive, 122  identity, 123 non-negative, 124  generic extension, 124  polynomial, 119 positive, 122  type recursion, see recursive types type reﬁnements, 219  addition reﬁnement, 230 boolean blindness, 226 booleans vs. propositions, 226 conjunction covariance, 230 dynamic type, 223, 230 entailment, 221 error-freedom, 229 examples, 225 exceptions, see exceptions type reﬁnements function types, 225   494  Index  type reﬁnements  cont.   general satisfaction, 222 natural numbers, 225 optimization of dynamic typing, 230 pre-order, 221 preservation, 228 product types, 224 recursive types, 230 reﬁned canonical forms, 228 reﬁnements vs. types, 219 safety, 228 safety theorem, 229 satisfaction, 221, 222 sum types, 224, 230 summand reﬁnements, 230 syntax, 220 types vs. reﬁnements, 229 underlying type, 220 Yoneda Lemma, 230  type safety, 48  canonical forms, 49 checked errors, 51 errors, 52 preservation, 48, 52 progress, 49, 52  uni-typed λ-calculus, 186  as untyped, 186  unit  dynamics, 80 statics, 79 unit type, 79  vs. void type, 88  units, see separate compilation universal types, 137  sk combinators, 144 abstraction theorem, 459 admissible relation, 457 Church numerals, 142 deﬁnability, 140 booleans, 144 inductive types, 144  lists, 144 natural numbers, 142 products, 141 sums, 141  deﬁnitional equality, 140 dynamics, 139 equivalence candidate, see admissible relation function extensionality, 461 Kleene equality, 455 logical equivalence, 456  closed, 457 compositionality, 459 open, 458  observational equivalence, 455 parametricity, 142, 144, 454, see equality, 461,  464  parametricity theorem, 459 rank-restricted Church numerals, 145 safety, 140 statics, 138 subtyping, 214  untyped λ-calculus, 181  Y combinator, 184 as uni-typed, 186 booleans, 187 bracket abstraction, 188 Church numerals, 182 deﬁnability, 182 deﬁnitional equality, 181 dynamics, 181 lists, 188 products, 187 Scott’s Theorem, 184 statics, 181 streams, 188 sums, 188  variance, see subtyping void type, 85  vs. unit type, 88 dynamics, 86 statics, 85

@highlight

This text develops a comprehensive theory of programming languages based on type systems and structural operational semantics. Language concepts are precisely defined by their static and dynamic semantics, presenting the essential tools both intuitively and rigorously while relying on only elementary mathematics. These tools are used to analyze and prove properties of languages and provide the framework for combining and comparing language features. The broad range of concepts includes fundamental data types such as sums and products, polymorphic and abstract types, dynamic typing, dynamic dispatch, subtyping and refinement types, symbols and dynamic classification, parallelism and cost semantics, and concurrency and distribution. The methods are directly applicable to language implementation, to the development of logics for reasoning about programs, and to the formal verification language properties such as type safety. This thoroughly revised second edition includes exercises at the end of nearly every chapter and a new chapter on type refinements