Instructor cid:146 s Manual by Thomas H. Cormen  Clara Lee Erica Lin  to Accompany Introduction to Algorithms Second Edition by Thomas H. Cormen Charles E. Leiserson Ronald L. Rivest Clifford Stein  The MIT Press Cambridge, Massachusetts London, England  McGraw-Hill Book Company Boston New York  Burr Ridge, IL San Francisco  Dubuque, IA  St. Louis  Montr·eal  Madison, WI Toronto   Instructor cid:146 s Manual by Thomas H. Cormen, Clara Lee, and Erica Lin to Accompany Introduction to Algorithms, Second Edition by Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein  Published by The MIT Press and McGraw-Hill Higher Education, an imprint of The McGraw-Hill Companies, Inc., 1221 Avenue of the Americas, New York, NY 10020. Copyright c cid:1  2002 by The Massachusetts Institute of Technology and The McGraw-Hill Companies, Inc. All rights reserved.  No part of this publication may be reproduced or distributed in any form or by any means, or stored in a database or retrieval system, without the prior written consent of The MIT Press or The McGraw-Hill Companies, Inc., in- cluding, but not limited to, network or other electronic storage or transmission, or broadcast for distance learning.   Contents  Chapter 5: Probabilistic Analysis and Randomized Algorithms  R-1  Revision History Preface P-1 Chapter 2: Getting Started  Lecture Notes Solutions  2-16  2-1  Chapter 3: Growth of Functions  Lecture Notes Solutions 3-7  3-1  Chapter 4: Recurrences  Lecture Notes Solutions 4-8  4-1  Lecture Notes Solutions 5-8  5-1  Chapter 6: Heapsort 6-1  Lecture Notes Solutions  6-10  Chapter 7: Quicksort 7-1  Lecture Notes Solutions 7-9  Chapter 8: Sorting in Linear Time  Lecture Notes Solutions 8-9  8-1  Chapter 9: Medians and Order Statistics  Lecture Notes Solutions 9-9  9-1  Chapter 11: Hash Tables  Lecture Notes Solutions  11-16  11-1  Chapter 12: Binary Search Trees  Lecture Notes Solutions  12-12  12-1  Chapter 13: Red-Black Trees  Lecture Notes Solutions  13-13  13-1  Chapter 14: Augmenting Data Structures  Lecture Notes Solutions  14-9  14-1   iv  Contents  Chapter 15: Dynamic Programming  Lecture Notes Solutions  15-19  15-1  Chapter 16: Greedy Algorithms  Lecture Notes Solutions  16-9  16-1  Chapter 17: Amortized Analysis  Lecture Notes Solutions  17-14  17-1  Chapter 21: Data Structures for Disjoint Sets  Chapter 22: Elementary Graph Algorithms  Lecture Notes Solutions  21-6  21-1  Lecture Notes Solutions  22-12  22-1  Lecture Notes Solutions  23-8  23-1  Lecture Notes Solutions  24-13  24-1  Lecture Notes Solutions  25-8  25-1  Chapter 23: Minimum Spanning Trees  Chapter 24: Single-Source Shortest Paths  Chapter 25: All-Pairs Shortest Paths  Chapter 26: Maximum Flow  Lecture Notes Solutions  26-15  26-1  Chapter 27: Sorting Networks  27-1  Lecture Notes Solutions I-1  27-8  Index   Revision History  Revisions are listed by date rather than being numbered. Because this revision history is part of each revision, the affected chapters always include the front matter in addition to those listed below.    18 January 2005. Corrected an error in the transpose-symmetry properties. Affected chapters: Chapter 3. 2 April 2004. Added solutions to Exercises 5.4-6, 11.3-5, 12.4-1, 16.4-2, 16.4-3, 21.3-4, 26.4-2, 26.4-3, and 26.4-6 and to Problems 12-3 and 17-4. Made minor changes in the solutions to Problems 11-2 and 17-2. Affected chapters: Chapters 5, 11, 12, 16, 17, 21, and 26; index. 7 January 2004. Corrected two minor typographical errors in the lecture notes for the expected height of a randomly built binary search tree. Affected chap- ters: Chapter 12. 23 July 2003. Updated the solution to Exercise 22.3-4 b  to adjust for a correc- tion in the text. Affected chapters: Chapter 22; index. 23 June 2003. Added the link to the website for the clrscode package to the preface. 2 June 2003. Added the solution to Problem 24-6. Corrected solutions to Ex- ercise 23.2-7 and Problem 26-4. Affected chapters: Chapters 23, 24, and 26; index. 20 May 2003. Added solutions to Exercises 24.4-10 and 26.1-7. Affected chapters: Chapters 24 and 26; index. 2 May 2003. Added solutions to Exercises 21.4-4, 21.4-5, 21.4-6, 22.1-6, and 22.3-4. Corrected a minor typographical error in the Chapter 22 notes on page 22-6. Affected chapters: Chapters 21 and 22; index. 28 April 2003. Added the solution to Exercise 16.1-2, corrected an error in the  cid:222 rst adjacency matrix example in the Chapter 22 notes, and made a minor change to the accounting method analysis for dynamic tables in the Chapter 17 notes. Affected chapters: Chapters 16, 17, and 22; index. 10 April 2003. Corrected an error in the solution to Exercise 11.3-3. Affected chapters: Chapter 11. 3 April 2003. Reversed the order of Exercises 14.2-3 and 14.3-3. Affected chapters: Chapter 13, index. 2 April 2003. Corrected an error in the substitution method for recurrences on page 4-4. Affected chapters: Chapter 4.                                    R-2  Revision History                 31 March 2003. Corrected a minor typographical error in the Chapter 8 notes on page 8-3. Affected chapters: Chapter 8. 14 January 2003. Changed the exposition of indicator random variables in the Chapter 5 notes to correct for an error in the text. Affected pages: 5-4 through 5-6.  The only content changes are on page 5-4; in pages 5-5 and 5-6 only pagination changes.  Affected chapters: Chapter 5. 14 January 2003. Corrected an error in the pseudocode for the solution to Ex- ercise 2.2-2 on page 2-16. Affected chapters: Chapter 2. 7 October 2002. Corrected a typographical error in EUCLIDEAN-TSP on page 15-23. Affected chapters: Chapter 15. 1 August 2002. Initial release.   Preface  This document is an instructor cid:146 s manual to accompany Introduction to Algorithms, Second Edition, by Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. It is intended for use in a course on algorithms. You might also  cid:222 nd some of the material herein to be useful for a CS 2-style course in data structures. Unlike the instructor cid:146 s manual for the  cid:222 rst edition of the text cid:151 which was organized around the undergraduate algorithms course taught by Charles Leiserson at MIT in Spring 1991 cid:151 we have chosen to organize the manual for the second edition according to chapters of the text. That is, for most chapters we have provided a set of lecture notes and a set of exercise and problem solutions pertaining to the chapter. This organization allows you to decide how to best use the material in the manual in your own course. We have not included lecture notes and solutions for every chapter, nor have we included solutions for every exercise and problem within the chapters that we have selected. We felt that Chapter 1 is too nontechnical to include here, and Chap- ter 10 consists of background material that often falls outside algorithms and data- structures courses. We have also omitted the chapters that are not covered in the courses that we teach: Chapters 18 cid:150 20 and 28 cid:150 35, as well as Appendices A cid:150 C; future editions of this manual may include some of these chapters. There are two reasons that we have not included solutions to all exercises and problems in the selected chapters. First, writing up all these solutions would take a long time, and we felt it more important to release this manual in as timely a fashion as possible. Second, if we were to include all solutions, this manual would be longer than the text itself! We have numbered the pages in this manual using the format CC-PP, where CC is a chapter number of the text and PP is the page number within that chapter cid:146 s lecture notes and solutions. The PP numbers restart from 1 at the beginning of each chapter cid:146 s lecture notes. We chose this form of page numbering so that if we add or change solutions to exercises and problems, the only pages whose numbering is affected are those for the solutions for that chapter. Moreover, if we add material for currently uncovered chapters, the numbers of the existing pages will remain unchanged.  The lecture notes  The lecture notes are based on three sources:   P-2  Preface    Some are from the  cid:222 rst-edition manual, and so they correspond to Charles Leis-  erson cid:146 s lectures in MIT cid:146 s undergraduate algorithms course, 6.046.    Some are from Tom Cormen cid:146 s lectures in Dartmouth College cid:146 s undergraduate  algorithms course, CS 25.    Some are written just for this manual. You will  cid:222 nd that the lecture notes are more informal than the text, as is appro- priate for a lecture situation. In some places, we have simpli cid:222 ed the material for lecture presentation or even omitted certain considerations. Some sections of the text cid:151 usually starred cid:151 are omitted from the lecture notes.  We have included lec- ture notes for one starred section: 12.4, on randomly built binary search trees, which we cover in an optional CS 25 lecture.  In several places in the lecture notes, we have included  cid:147 asides cid:148  to the instruc- tor. The asides are typeset in a slanted font and are enclosed in square brack- ets. [Hereisanaside.] Some of the asides suggest leaving certain material on the board, since you will be coming back to it later. If you are projecting a presenta- tion rather than writing on a blackboard or whiteboard, you might want to mark slides containing this material so that you can easily come back to them later in the lecture. We have chosen not to indicate how long it takes to cover material, as the time nec- essary to cover a topic depends on the instructor, the students, the class schedule, and other variables. There are two differences in how we write pseudocode in the lecture notes and the text:   Lines are not numbered in the lecture notes. We  cid:222 nd them inconvenient to  number when writing pseudocode on the board.    We avoid using the length attribute of an array.  Instead, we pass the array length as a parameter to the procedure. This change makes the pseudocode more concise, as well as matching better with the description of what it does.  We have also minimized the use of shading in  cid:222 gures within lecture notes, since drawing a  cid:222 gure with shading on a blackboard or whiteboard is dif cid:222 cult.  The solutions  The solutions are based on the same sources as the lecture notes. They are written a bit more formally than the lecture notes, though a bit less formally than the text. We do not number lines of pseudocode, but we do use the length attribute  on the assumption that you will want your students to write pseudocode as it appears in the text . The index lists all the exercises and problems for which this manual provides solu- tions, along with the number of the page on which each solution starts. Asides appear in a handful of places throughout the solutions. Also, we are less reluctant to use shading in  cid:222 gures within solutions, since these  cid:222 gures are more likely to be reproduced than to be drawn on a board.   Preface  Source  cid:222 les  P-3  For several reasons, we are unable to publish or transmit source  cid:222 les for this man- ual. We apologize for this inconvenience. In June 2003, we made available a clrscode package for LATEX 2ε. It enables you to typeset pseudocode in the same way that we do. You can  cid:222 nd this package at http:  www.cs.dartmouth.edu ˜thc clrscode . That site also includes documentation.  Reporting errors and suggestions  Undoubtedly, instructors will  cid:222 nd errors in this manual. Please report errors by sending email to clrs-manual-bugs@mhhe.com If you have a suggestion for an improvement to this manual, please feel free to submit it via email to clrs-manual-suggestions@mhhe.com As usual, if you  cid:222 nd an error in the text itself, please verify that it has not already been posted on the errata web page before you submit it. You can use the MIT Press web site for the text, http:  mitpress.mit.edu algorithms , to locate the errata web page and to submit an error report. We thank you in advance for your assistance in correcting errors in both this manual and the text.  Acknowledgments  This manual borrows heavily from the  cid:222 rst-edition manual, which was written by Julie Sussman, P.P.A. Julie did such a superb job on the  cid:222 rst-edition manual,  cid:222 nd- ing numerous errors in the  cid:222 rst-edition text in the process, that we were thrilled to have her serve as technical copyeditor for the second-edition text. Charles Leiser- son also put in large amounts of time working with Julie on the  cid:222 rst-edition manual. The other three Introduction to Algorithms authors cid:151 Charles Leiserson, Ron Rivest, and Cliff Stein cid:151 provided helpful comments and suggestions for solutions to exercises and problems. Some of the solutions are modi cid:222 cations of those written over the years by teaching assistants for algorithms courses at MIT and Dartmouth. At this point, we do not know which TAs wrote which solutions, and so we simply thank them collectively. We also thank McGraw-Hill and our editors, Betsy Jones and Melinda Dougharty, for moral and  cid:222 nancial support. Thanks also to our MIT Press editor, Bob Prior, and to David Jones of The MIT Press for help with TEX macros. Wayne Cripps, John Konkle, and Tim Tregubov provided computer support at Dartmouth, and the MIT sysadmins were Greg Shomo and Matt McKinnon. Phillip Meek of McGraw- Hill helped us hook this manual into their web site.  THOMAS H. CORMEN CLARA LEE ERICA LIN Hanover, New Hampshire July 2002    Lecture Notes for Chapter 2: Getting Started  Chapter 2 overview  Insertion sort  Goals:   Start using frameworks for describing and analyzing algorithms.   Examine two algorithms for sorting: insertion sort and merge sort.   See how to describe algorithms in pseudocode.   Begin using asymptotic notation to express running-time analysis.   Learn the technique of  cid:147 divide and conquer cid:148  in the context of merge sort.  The sorting problem Input: A sequence of n numbers  cid:2 a1, a2, . . . , an cid:3 . Output: A permutation  reordering   cid:2 a  cid:4   cid:4  , . . . , a n 1   cid:4  , a 2   cid:4  that a 1  ≤ a   cid:4  2  ≤ ··· ≤ a  cid:4  n.   cid:3  of the input sequence such  The sequences are typically stored in arrays. We also refer to the numbers as keys. Along with each key may be additional information, known as satellite data. [You might want to clarify that  cid:147 satellite data cid:148 doesnotnecessarilycomefromasatellite!] We will see several ways to solve the sorting problem. Each way will be expressed as an algorithm: a well-de cid:222 ned computational procedure that takes some value, or set of values, as input and produces some value, or set of values, as output.  Expressing algorithms  We express algorithms in whatever way is the clearest and most concise. English is sometimes the best way. When issues of control need to be made perfectly clear, we often use pseudocode.   2-2  Lecture Notes for Chapter 2: Getting Started    Pseudocode is similar to C, C++, Pascal, and Java. If you know any of these  languages, you should be able to understand pseudocode.    Pseudocode is designed for expressing algorithms to humans. Software en- gineering issues of data abstraction, modularity, and error handling are often ignored.    We sometimes embed English statements into pseudocode. Therefore, unlike for  cid:147 real cid:148  programming languages, we cannot create a compiler that translates pseudocode to machine code.  Insertion sort  A good algorithm for sorting a small number of elements. It works the way you might sort a hand of playing cards:   Start with an empty left hand and the cards face down on the table.   Then remove one card at a time from the table, and insert it into the correct    To  cid:222 nd the correct position for a card, compare it with each of the cards already  position in the left hand.  in the hand, from right to left.    At all times, the cards held in the left hand are sorted, and these cards were  originally the top cards of the pile on the table.  Pseudocode: We use a procedure INSERTION-SORT.   Takes as parameters an array A[1 . . n] and the length n of the array.   As in Pascal, we use  cid:147 . . cid:148  to denote a range within an array.    [We usually use 1-origin indexing, as we do here. There are a few places in later chapters where we use 0-origin indexing instead. If you are translating pseudocode to C, C++, or Java, which use 0-origin indexing, you need to be careful to get the indices right. One option is to adjust all index calculations intheC,C++,orJavacodetocompensate. Aneasieroptionis,whenusingan array A[1 . . n],toallocatethearraytobeoneentrylonger cid:151 A[0 . . n] cid:151 andjust don cid:146 tusetheentryatindex 0.] [In the lecture notes, we indicate array lengths by parameters rather than by usingthe length attributethatisusedinthebook. Thatsavesusalineofpseu- docodeeachtime. Thesolutionscontinuetousethe length attribute.]    The array A is sorted in place: the numbers are rearranged within the array,  with at most a constant number outside the array at any time.      Lecture Notes for Chapter 2: Getting Started  2-3  INSERTION-SORT A  cost for j ← 2 to n c1 do key ← A[ j] c2  cid:1  Insert A[ j] into the sorted sequence A[1 . . j − 1]. 0 i ← j − 1 c4 while i > 0 and A[i] > key c5 c6 c7 c8  i ← i − 1 A[i + 1] ← key  do A[i + 1] ← A[i]  times n n − 1  cid:1  n − 1  cid:1  n − 1  cid:1  n j=2 t j n j=2 n j=2 n − 1   t j − 1   t j − 1   [Leavethisontheboard, butshowonly thepseudocode fornow. We cid:146 llputinthe  cid:147 cost cid:148 and cid:147 times cid:148 columnslater.]  Example:  j 2 2  2 4  1 5  1 2  3 4  3 5  4 6  4 6  5 1  j 5 1  6 3  6 3  j 3 4  3 4  1 2  1 1  2 5  2 2  4 6  4 5  5 1  5 6  6 3  j 6 3  1 2  1 1  2 4  2 2  3 5  3 3  j 4 6  4 4  5 1  5 5  6 3  6 6  [Readthis cid:222 gurerowbyrow. Eachpartshowswhathappensforaparticularitera- tionwiththevalueof j indicated. j indexesthe cid:147 currentcard cid:148 beinginsertedinto thehand. Elementstotheleftof A[ j] thataregreaterthan A[ j] moveoneposition totheright,and A[ j] movesintotheevacuated position. Theheavy vertical lines separatethepartofthearrayinwhichaniterationworks cid:151 A[1 . . j] cid:151 fromthepart ofthearraythatisunaffected bythisiteration cid:151 A[ j + 1 . . n]. Thelastpartofthe  cid:222 gureshowsthe cid:222 nalsortedarray.]  Correctness  We often use a loop invariant to help us understand why an algorithm gives the correct answer. Here cid:146 s the loop invariant for INSERTION-SORT:  Loop invariant: At the start of each iteration of the  cid:147 outer cid:148  for loop cid:151 the loop indexed by j cid:151 the subarray A[1 . . j − 1] consists of the elements orig- inally in A[1 . . j − 1] but in sorted order.  To use a loop invariant to prove correctness, we must show three things about it:  Initialization: It is true prior to the  cid:222 rst iteration of the loop. Maintenance: If it is true before an iteration of the loop, it remains true before the  next iteration.  Termination: When the loop terminates, the invariant cid:151 usually along with the reason that the loop terminated cid:151 gives us a useful property that helps show that the algorithm is correct.  Using loop invariants is like mathematical induction:   2-4  Lecture Notes for Chapter 2: Getting Started    To prove that a property holds, you prove a base case and an inductive step.   Showing that the invariant holds before the  cid:222 rst iteration is like the base case.   Showing that the invariant holds from iteration to iteration is like the inductive  step.    The termination part differs from the usual use of mathematical induction, in which the inductive step is used in cid:222 nitely. We stop the  cid:147 induction cid:148  when the loop terminates.    We can show the three parts in any order.  For insertion sort: Initialization: Just before the  cid:222 rst iteration, j = 2. The subarray A[1 . . j − 1] is the single element A[1], which is the element originally in A[1], and it is trivially sorted.  Maintenance: To be precise, we would need to state and prove a loop invariant for the  cid:147 inner cid:148  while loop. Rather than getting bogged down in another loop invariant, we instead note that the body of the inner while loop works by moving A[ j − 1], A[ j − 2], A[ j − 3], and so on, by one position to the right until the proper position for key  which has the value that started out in A[ j]  is found. At that point, the value of key is placed into this position. Termination: The outer for loop ends when j > n; this occurs when j = n + 1. Therefore, j − 1 = n. Plugging n in for j − 1 in the loop invariant, the subarray A[1 . . n] consists of the elements originally in A[1 . . n] but in sorted order. In other words, the entire array is sorted!  Pseudocode conventions  Indentation indicates block structure. Saves space and writing time.  [Coveringmost,butnotall,here. Seebookpages19 cid:150 20forallconventions.]     Looping constructs are like in C, C++, Pascal, and Java. We assume that the loop variable in a for loop is still de cid:222 ned when the loop exits  unlike in Pascal .  cid:147  cid:1  cid:148  indicates that the remainder of the line is a comment.      Variables are local, unless otherwise speci cid:222 ed.   We often use objects, which have attributes  equivalently,  cid:222 elds . For an at-  This would be the equivalent of  tribute attr of object x, we write attr[x]. x. attr in Java or x-> attr in C++.    Objects are treated as references, like in Java. If x and y denote objects, then the assignment y ← x makes x and y reference the same object. It does not cause attributes of one object to be copied to another.    Parameters are passed by value, as in Java and C  and the default mechanism in Pascal and C++ . When an object is passed by value, it is actually a reference  or pointer  that is passed; changes to the reference itself are not seen by the caller, but changes to the object cid:146 s attributes are.    The boolean operators  cid:147 and cid:148  and  cid:147 or cid:148  are short-circuiting: if after evaluating the left-hand operand, we know the result of the expression, then we don cid:146 t evaluate the right-hand operand.  If x is FALSE in  cid:147 x and y cid:148  then we don cid:146 t evaluate y. If x is TRUE in  cid:147 x or y cid:148  then we don cid:146 t evaluate y.    Lecture Notes for Chapter 2: Getting Started  2-5  Analyzing algorithms  We want to predict the resources that the algorithm requires. Usually, running time. In order to predict resource requirements, we need a computational model.  Random-access machine  RAM  model         Instructions are executed one after another. No concurrent operations. It cid:146 s too tedious to de cid:222 ne each of the instructions and their associated time costs. Instead, we recognize that we cid:146 ll use instructions commonly found in real com- puters:   Arithmetic: add, subtract, multiply, divide, remainder,  cid:223 oor, ceiling . Also,  shift left shift right  good for multiplying dividing by 2k .    Data movement: load, store, copy.   Control: conditional unconditional branch, subroutine call and return. Each of these instructions takes a constant amount of time.  The RAM model uses integer and  cid:223 oating-point types.   We don cid:146 t worry about precision, although it is crucial in certain numerical ap-  plications.   There is a limit on the word size: when working with inputs of size n, assume that integers are represented by c lg n bits for some constant c ≥ 1.  lg n is a very frequently used shorthand for log2 n.    c ≥ 1 ⇒ we can hold the value of n ⇒ we can index the individual elements.   c is a constant ⇒ the word size cannot grow arbitrarily.  How do we analyze an algorithm cid:146 s running time?  The time taken by an algorithm depends on the input.   Sorting 1000 numbers takes longer than sorting 3 numbers.   A given sorting algorithm may even take differing amounts of time on two  inputs of the same size.    For example, we cid:146 ll see that insertion sort takes less time to sort n elements when  they are already sorted than when they are in reverse sorted order.  Input size: Depends on the problem being studied.   Usually, the number of items in the input. Like the size n of the array being  sorted.    But could be something else. If multiplying two integers, could be the total  number of bits in the two integers.    Could be described by more than one number. For example, graph algorithm running times are usually expressed in terms of the number of vertices and the number of edges in the input graph.   2-6  Lecture Notes for Chapter 2: Getting Started  Running time: On a particular input, it is the number of primitive operations  steps  executed.   Want to de cid:222 ne steps to be machine-independent.   Figure that each line of pseudocode requires a constant amount of time.   One line may take a different amount of time than another, but each execution  of line i takes the same amount of time ci .    This is assuming that the line consists only of primitive operations.        If the line is a subroutine call, then the actual call takes constant time, but the execution of the subroutine being called might not. If the line speci cid:222 es operations other than primitive ones, then it might take more than constant time. Example:  cid:147 sort the points by x-coordinate. cid:148   Analysis of insertion sort  [Now add statement costs and number of times executed to INSERTION-SORT pseudocode.]   Assume that the ith line takes time ci, which is a constant.  Since the third line   For j = 2, 3, . . . , n, let t j be the number of times that the while loop test is   Note that when a for or while loop exits in the usual way cid:151 due to the test in the  is a comment, it takes no time.   executed for that value of j.  loop header cid:151 the test is executed one time more than the loop body.   cost of statement  ·  number of times statement is executed  .  The running time of the algorithm is   cid:2  n cid:2  Let T  n  = running time of INSERTION-SORT. T  n  = c1n + c2 n − 1  + c4 n − 1  + c5  all statements  n cid:2   j=2  t j + c6   t j − 1   j=2  n cid:2   j=2  + c7   t j − 1  + c8 n − 1  .  The running time depends on the values of tj . These vary according to the input.  Best case: The array is already sorted.   Always  cid:222 nd that A[i] ≤ key upon the  cid:222 rst time the while loop test is run  when  i = j − 1 .   All t j are 1.   Running time is  T  n  = c1n + c2 n − 1  + c4 n − 1  + c5 n − 1  + c8 n − 1   =  c1 + c2 + c4 + c5 + c8 n −  c2 + c4 + c5 + c8  .    Can express T  n  as an + b for constants a and b  that depend on the statement  costs ci   ⇒ T  n  is a linear function of n.   Lecture Notes for Chapter 2: Getting Started  2-7  t j = n cid:2     Since the while loop exits because i reaches 0, there cid:146 s one additional test after  Worst case: The array is in reverse sorted order.   Always  cid:222 nd that A[i] > key in while loop test.   Have to compare key with all elements to the left of the jth position ⇒ compare with j − 1 elements. n cid:2  n cid:2   t j − 1  = n cid:2  the j − 1 tests ⇒ tj = j. n cid:2  j=1 n n + 1  n cid:2   j is known as an arithmetic series, and equation  A.1  shows that it equals   cid:3  n cid:2     j − 1 .   cid:4   j and  j=2  j=2  j=2  j=2  .        j =  j=2  j  j=1  − 1, it equals  n n + 1   2  − 1.  2   Since  [The parentheses around the summation are not strictly necessary. They are there for clarity, but it might be a good idea to remind the students that the meaningoftheexpressionwouldbethesameevenwithouttheparentheses.]  j=2    j − 1  = n−1 cid:2  n cid:2   cid:5  k=1 n n + 1   cid:6   cid:5  T  n  = c1n + c2 n − 1  + c4 n − 1  + c5 n n − 1  c1 + c2 + c4 + c5 2    Letting k = j − 1, we see that   Running time is  cid:6  n n − 1   cid:8    cid:7   cid:7  + c7 + c6 n2 + c5 2 −  c2 + c4 + c5 + c8  .  k = n n − 1   cid:6  − 1 + c8 n − 1  − c6 2  2 + c7 2  + c6 2   cid:5   =  2  2  2  .   cid:8   − c7 2  + c8  n    Can express T  n  as an2 + bn + c for constants a, b, c  that again depend on  statement costs  ⇒ T  n  is a quadratic function of n.  Worst-case and average-case analysis  We usually concentrate on  cid:222 nding the worst-case running time: the longest run- ning time for any input of size n.  Reasons:   The worst-case running time gives a guaranteed upper bound on the running  time for any input.    For some algorithms, the worst case occurs often. For example, when search- ing, the worst case often occurs when the item being searched for is not present, and searches for absent items may be frequent.    Why not analyze the average case? Because it cid:146 s often about as bad as the worst  case.   2-8  Lecture Notes for Chapter 2: Getting Started  Example: Suppose that we randomly choose n numbers as the input to inser- tion sort. On average, the key in A[ j] is less than half the elements in A[1 . . j − 1] and it cid:146 s greater than the other half. ⇒ On average, the while loop has to look halfway through the sorted subarray A[1 . . j − 1] to decide where to drop key. ⇒ t j = j 2. Although the average-case running time is approximately half of the worst-case running time, it cid:146 s still a quadratic function of n.  Order of growth  Another abstraction to ease analysis and focus on the important features. Look only at the leading term of the formula for running time.   Drop lower-order terms.    Ignore the constant coef cid:222 cient in the leading term.  Example: For insertion sort, we already abstracted away the actual statement costs to conclude that the worst-case running time is an2 + bn + c. Drop lower-order terms ⇒ an2. Ignore constant coef cid:222 cient ⇒ n2. But we cannot say that the worst-case running time T  n  equals n2. It grows like n2. But it doesn cid:146 t equal n2. We say that the running time is  cid:2  n2  to capture the notion that the order of growth is n2. We usually consider one algorithm to be more ef cid:222 cient than another if its worst- case running time has a smaller order of growth.  There are many ways to design algorithms. For example, insertion sort is incremental: having sorted A[1 . . j − 1], place A[ j] correctly, so that A[1 . . j] is sorted.  Divide and conquer  Another common approach. Divide the problem into a number of subproblems. Conquer the subproblems by solving them recursively.  Base case: If the subproblems are small enough, just solve them by brute force. [Itwould beagood ideatomakesurethatyour students arecomfortable with recursion. Iftheyarenot,thentheywillhaveahardtimeunderstanding divide andconquer.]  Combine the subproblem solutions to give a solution to the original problem.  Designing algorithms   Lecture Notes for Chapter 2: Getting Started  2-9  Merge sort  A sorting algorithm based on divide and conquer. Its worst-case running time has a lower order of growth than insertion sort. Because we are dealing with subproblems, we state each subproblem as sorting a subarray A[ p . . r]. Initially, p = 1 and r = n, but these values change as we recurse through subproblems. To sort A[ p . . r]: Divide by splitting into two subarrays A[p . . q] and A[q + 1 . . r], where q is the Conquer by recursively sorting the two subarrays A[p . . q] and A[q + 1 . . r]. Combine by merging the two sorted subarrays A[p . . q] and A[q + 1 . . r] to pro- duce a single sorted subarray A[p . . r]. To accomplish this step, we cid:146 ll de cid:222 ne a procedure MERGE A, p, q, r  .  halfway point of A[p . . r].  The recursion bottoms out when the subarray has just 1 element, so that it cid:146 s trivially sorted.  MERGE-SORT A, p, r   if p < r then q ←  cid:9   p + r   2 cid:10   MERGE-SORT A, p, q  MERGE-SORT A, q + 1, r   MERGE A, p, q, r     cid:1  Check for base case  cid:1  Divide  cid:1  Conquer  cid:1  Conquer  cid:1  Combine  Initial call: MERGE-SORT A, 1, n  [Itis astounding howoften students forget howeasy itisto compute the halfway point of p and r as their average   p + r   2. We of course have to take the  cid:223 oor toensurethatwegetanintegerindex q. Butitiscommontoseestudentsperform calculationslike p +  r − p  2,orevenmoreelaborateexpressions,forgettingthe easywaytocomputeanaverage.] Example: Bottom-up view for n = 8: [Heavy lines demarcate subarrays used in subproblems.]  sorted array 3 6 5 2  5 4  4 3  7 6  8 7  1 1  2 2  2  4  5  7  1  2  3  6  2  5  4  7  1  3  2  6  5 1  2 2  1 5  7 4  4 3 6 3 initial array  2 7  6 8  merge  merge  merge   2-10  Lecture Notes for Chapter 2: Getting Started  [Examples when n is a power of 2 are most straightforward, but students might alsowantanexamplewhen n isnotapowerof2.] Bottom-up view for n = 11:  sorted array  1 1  2 2  3 2  4 3  5 4  6 4  7 5  8 6  9 6  10 11 7 7  1  2  4  4  6  7  2  3  5  6  7  2  4  7  1  4  6  3  5  7  2  6  4  7  2  1  6  4  3  7  5  2  6  4 1  7 2  2 3  6 4  1 5  4 6  7 7  3 8  5 9  6 2 10 11  initial array  merge  merge  merge  merge  [Here, atthenext-to-last levelofrecursion, someofthe subproblems have only 1 element. Therecursionbottomsoutonthesesingle-elementsubproblems.]  Merging  What remains is the MERGE procedure.  Input: Array A and indices p, q, r such that    p ≤ q < r.   Subarray A[p . . q] is sorted and subarray A[q + 1 . . r] is sorted. By the  restrictions on p, q, r, neither subarray is empty.  Output: The two subarrays are merged into a single sorted subarray in A[p . . r]. We implement it so that it takes  cid:2  n  time, where n = r − p + 1 = the number of elements being merged. What is n? Until now, n has stood for the size of the original problem. But now we cid:146 re using it as the size of a subproblem. We will use this technique when we analyze recursive algorithms. Although we may denote the original problem size by n, in general n will be the size of a given subproblem.  Idea behind linear-time merging: Think of two piles of cards.   Each pile is sorted and placed face-up on a table with the smallest cards on top.   We will merge these into a single sorted pile, face-down on the table.   A basic step:    Choose the smaller of the two top cards.   Lecture Notes for Chapter 2: Getting Started  2-11    Remove it from its pile, thereby exposing a new top card.   Place the chosen card face-down onto the output pile.    Repeatedly perform basic steps until one input pile is empty.   Once one input pile empties, just take the remaining input pile and place it  face-down onto the output pile.    Each basic step should take constant time, since we check just the two top cards.   There are ≤ n basic steps, since each basic step removes one card from the  input piles, and we started with n cards in the input piles.    Therefore, this procedure should take  cid:2  n  time. We don cid:146 t actually need to check whether a pile is empty before each basic step.   Put on the bottom of each input pile a special sentinel card.   It contains a special value that we use to simplify the code.   We use ∞, since that cid:146 s guaranteed to  cid:147 lose cid:148  to any other value.   The only way that ∞ cannot lose is when both piles have ∞ exposed as their  top cards.    But when that happens, all the nonsentinel cards have already been placed into the output pile.   We know in advance that there are exactly r − p + 1 nonsentinel cards ⇒ stop once we have performed r − p + 1 basic steps. Never a need to check for sentinels, since they cid:146 ll always lose.    Rather than even counting basic steps, just  cid:222 ll up the output array from index p  up through and including index r.  Pseudocode: MERGE A, p, q, r   n1 ← q − p + 1 n2 ← r − q create arrays L[1 . . n1 + 1] and R[1 . . n2 + 1] for i ← 1 to n1 for j ← 1 to n2 L[n1 + 1] ← ∞ R[n2 + 1] ← ∞ i ← 1 j ← 1 for k ← p to r  do L[i] ← A[p + i − 1] do R[ j] ← A[q + j]  do if L[i] ≤ R[ j]  then A[k] ← L[i] else A[k] ← R[ j]  i ← i + 1 j ← j + 1  [The book uses a loop invariant to establish that MERGE works correctly. In a lecturesituation,itisprobablybettertouseanexampletoshowthattheprocedure workscorrectly.]   2-12  Lecture Notes for Chapter 2: Getting Started  Example: A call of MERGE 9, 12, 16   8  cid:133   2 4  8  cid:133   2 4 i  8  cid:133   2 4 i  8  cid:133   2 4  8  cid:133   2 4  9 2 k 3 5  9 1  3 5  9 1  3 5  9 1  3 5  9 1  3 5  A  1 2 i  A  1 2  A  1 2  A  1 2  A  1 2  L  L  L  L  L  10 11 12 13 14 15 16 4 6  2  1  7  5  3  17  cid:133   4 7  5 ∞  2 2  3 3  4 6  5 ∞  R  1 1 j  10 11 12 13 14 15 16 2 6  2  1  7  3  17  cid:133   5 k 5 ∞  4 7  1 1  R  3 3  4 6  5 ∞  10 11 12 13 14 15 16 2 6  2  2  3  3  17  cid:133   4 7  5 ∞  1 1  2 2  3 3  5 ∞  1 k  R  10 11 12 13 14 15 16 2 6  2  3  4  5  17  cid:133   5 ∞  1 1  R  3 3  5 ∞  4 6 j  4 6 j  10 11 12 13 14 15 16 2 7  4  2  3  5  6  1 1  2 2  3 3  R  5 ∞ i  17  cid:133  k 4 6  5 ∞ j  4 7 i  4 7  2 2 j  3 k 2 2  8  cid:133   2 4  8  cid:133   2 4 i  8  cid:133   2 4  8  cid:133   2 4  9 1  3 5  9 1  3 5  9 1  3 5 i  9 1  3 5  A  1 2 i  A  1 2  A  1 2  A  1 2  L  L  L  L  7  5  3  1  2  10 11 12 13 14 15 16 4 6 k 4 7  5 ∞  1 1  3 3  R  17  cid:133   4 6  5 ∞  2 2 j  10 11 12 13 14 15 16 2 6  2  1  2  3  17  cid:133   7 k  4 7  5 ∞  1 1  2 2  R  4 6  5 ∞  3 3 j  10 11 12 13 14 15 16 2 6  3  4  3  2  17  cid:133   4 7  5 ∞  R  2 2  3 3  5 ∞  4 6 j  2 k 1 1  6  5  4  3  2  10 11 12 13 14 15 16 2 6 k 3 3  5 ∞  1 1  2 2  R  17  cid:133   4 6  4 7 i  5 ∞ j  [Read this  cid:222 gure row by row. The  cid:222 rst part shows the arrays at the start of the  cid:147 for k ← p tor cid:148 loop,where A[p . . q] iscopiedinto L[1 . . n1] and A[q+1 . . r] is copiedinto R[1 . . n2].Succeedingpartsshowthesituationatthestartofsuccessive iterations. Entriesin A withslashes havehadtheirvaluescopied toeither L or R andhavenothadavaluecopiedbackinyet. Entriesin L and R withslasheshave been copied back into A. Thelast part shows that the subarrays are merged back into A[p . . r],whichisnowsorted,andthatonlythesentinels ∞ areexposedin thearrays L and R.] Running time: The  cid:222 rst two for loops take  cid:2  n1 + n2  =  cid:2  n  time. The last for loop makes n iterations, each taking constant time, for  cid:2  n  time. Total time:  cid:2  n .   Lecture Notes for Chapter 2: Getting Started  2-13  Analyzing divide-and-conquer algorithms     Use a recurrence equation  more commonly, a recurrence  to describe the running time of a divide-and-conquer algorithm. Let T  n  = running time on a problem of size n.  If the problem size is small enough  say, n ≤ c for some constant c , we have a base case. The brute-force solution takes constant time:  cid:2  1 . original.  In merge sort, a = b = 2.     Otherwise, suppose that we divide into a subproblems, each 1 b the size of the    Let the time to divide a size-n problem be D n .   There are a subproblems to solve, each of size n b ⇒ each subproblem takes  T  n b  time to solve ⇒ we spend aT  n b  time solving subproblems.   cid:9     Let the time to combine solutions be C n .   We get the recurrence  T  n  =  if n ≤ c ,  cid:2  1  aT  n b  + D n  + C n  otherwise .  Analyzing merge sort For simplicity, assume that n is a power of 2 ⇒ each divide step yields two sub- problems, both of size exactly n 2. The base case occurs when n = 1. When n ≥ 2, time for merge sort steps: Divide: Just compute q as the average of p and r ⇒ D n  =  cid:2  1 . Conquer: Recursively solve 2 subproblems, each of size n 2 ⇒ 2T  n 2 . Combine: MERGE on an n-element subarray takes  cid:2  n  time ⇒ C n  =  cid:2  n . Since D n  =  cid:2  1  and C n  =  cid:2  n , summed together they give a function that is linear in n:  cid:2  n  ⇒ recurrence for merge sort running time is T  n  =   cid:9   cid:2  1  2T  n 2  +  cid:2  n   if n = 1 , if n > 1 .  Solving the merge-sort recurrence: By the master theorem in Chapter 4, we can show that this recurrence has the solution T  n  =  cid:2  n lg n . [Reminder: lg n standsforlog2 n.] Compared to insertion sort   cid:2  n2  worst-case time , merge sort is faster. Trading a factor of n for a factor of lg n is a good deal. On small inputs, insertion sort may be faster. But for large enough inputs, merge sort will always be faster, because its running time grows more slowly than inser- tion sort cid:146 s. We can understand how to solve the merge-sort recurrence without the master the- orem.   2-14  Lecture Notes for Chapter 2: Getting Started    Let c be a constant that describes the running time for the base case and also is the time per array element for the divide and conquer steps. [Ofcourse, we cannotnecessarilyusethesameconstantforboth. It cid:146 snotworthgoingintothis detailatthispoint.]    We rewrite the recurrence as   cid:9  if n = 1 , c 2T  n 2  + cn if n > 1 .  T  n  =    Draw a recursion tree, which shows successive expansions of the recurrence.   For the original problem, we have a cost of cn, plus the two subproblems, each  costing T  n 2 :  cn  T n 2   T n 2     For each of the size-n 2 subproblems, we have a cost of cn 2, plus two sub-  problems, each costing T  n 4 :  cn  cn 2  cn 2  T n 4   T n 4   T n 4   T n 4     Continue expanding until the problem sizes get down to 1: cn  cn  cn 2  cn 2  lg n  cn 4  cn 4  cn 4  cn 4  cn  cn   cid:133   cn  c  c  c  c  c   cid:133   c  c  n  Total: cn lg n + cn   Lecture Notes for Chapter 2: Getting Started  2-15    Each level has cost cn.    The top level has cost cn.   The next level down has 2 subproblems, each contributing cost cn 2.   The next level has 4 subproblems, each contributing cost cn 4.   Each time we go down one level, the number of subproblems doubles but the  cost per subproblem halves ⇒ cost per level stays the same.    There are lg n + 1 levels  height is lg n .    Use induction.   Base case: n = 1 ⇒ 1 level, and lg 1 + 1 = 0 + 1 = 1.     Inductive hypothesis is that a tree for a problem size of 2i has lg 2i+1 = i+1 levels. size up after 2i is 2i+1. i + 2 levels.    Because we assume that the problem size is a power of 2, the next problem   A tree for a problem size of 2i+1 has one more level than the size-2i tree ⇒   Since lg 2i+1 + 1 = i + 2, we cid:146 re done with the inductive argument. total cost is cn lg n + cn. Ignore low-order term of cn and constant coef cid:222 cient c ⇒  cid:2  n lg n .    Total cost is sum of costs at each level. Have lg n + 1 levels, each costing cn ⇒      Solutions for Chapter 2: Getting Started  Solution to Exercise 2.2-2  SELECTION-SORT A  n ← length[A] for j ← 1 to n − 1 do smallest ← j  for i ← j + 1 to n  do if A[i] < A[smallest] then smallest ← i exchange A[ j] ↔ A[smallest]  The algorithm maintains the loop invariant that at the start of each iteration of the outer for loop, the subarray A[1 . . j − 1] consists of the j − 1 smallest elements in the array A[1 . . n], and this subarray is in sorted order. After the  cid:222 rst n − 1 elements, the subarray A[1 . . n − 1] contains the smallest n − 1 elements, sorted, and therefore element A[n] must be the largest element. The running time of the algorithm is  cid:2  n2  for all cases.  Solution to Exercise 2.2-4  Modify the algorithm so it tests whether the input satis cid:222 es some special-case con- dition and, if it does, output a pre-computed answer. The best-case running time is generally not a good measure of an algorithm.  Solution to Exercise 2.3-3  The base case is when n = 2, and we have n lg n = 2 lg 2 = 2 · 1 = 2.   Solutions for Chapter 2: Getting Started  2-17  For the inductive step, our inductive hypothesis is that T  n 2  =  n 2  lg n 2 . Then T  n  = 2T  n 2  + n = 2 n 2  lg n 2  + n = n lg n − 1  + n = n lg n − n + n = n lg n ,  which completes the inductive proof for exact powers of 2.  Solution to Exercise 2.3-4   cid:9   Since it takes  cid:2  n  time in the worst case to insert A[n] into the sorted array A[1 . . n − 1], we get the recurrence T  n  = The solution to this recurrence is T  n  =  cid:2  n2 .   cid:2  1  T  n − 1  +  cid:2  n   if n = 1 , if n > 1 .  Solution to Exercise 2.3-5  Procedure BINARY-SEARCH takes a sorted array A, a value v, and a range [low . . high] of the array, in which we search for the value v. The procedure com- pares v to the array entry at the midpoint of the range and decides to eliminate half the range from further consideration. We give both iterative and recursive versions, each of which returns either an index i such that A[i] = v, or NIL if no entry of A[low . . high] contains the value v. The initial call to either version should have the parameters A, v, 1, n.  ITERATIVE-BINARY-SEARCH A, v, low, high  while low ≤ high  do mid ←  cid:9  low+ high  2 cid:10   if v = A[mid] then return mid if v > A[mid] then low ← mid +1 else high ← mid −1  return NIL   2-18  Solutions for Chapter 2: Getting Started  RECURSIVE-BINARY-SEARCH A, v, low, high  if low > high then return NIL mid ←  cid:9  low+ high  2 cid:10  if v = A[mid] then return mid if v > A[mid] then return RECURSIVE-BINARY-SEARCH A, v, mid +1, high  else return RECURSIVE-BINARY-SEARCH A, v, low, mid −1  Both procedures terminate the search unsuccessfully when the range is empty  i.e., low > high  and terminate it successfully if the value v has been found. Based on the comparison of v to the middle element in the searched range, the search continues with the range halved. The recurrence for these procedures is therefore T  n  = T  n 2  +  cid:2  1 , whose solution is T  n  =  cid:2  lg n .  Solution to Exercise 2.3-6  The while loop of lines 5 cid:150 7 of procedure INSERTION-SORT scans backward through the sorted array A[1 . . j − 1] to  cid:222 nd the appropriate place for A[ j]. The hitch is that the loop not only searches for the proper place for A[ j], but that it also moves each of the array elements that are bigger than A[ j] one position to the right  line 6 . These movements can take as much as  cid:2   j   time, which occurs when all the j − 1 elements preceding A[ j] are larger than A[ j]. We can use binary search to improve the running time of the search to  cid:2  lg j  , but binary search will have no effect on the running time of moving the elements. Therefore, binary search alone cannot improve the worst-case running time of INSERTION-SORT to  cid:2  n lg n .  Solution to Exercise 2.3-7  The following algorithm solves the problem:  same for S  .   cid:4    cid:4  = {z : z = x − y for some y ∈ S}.  1. Sort the elements in S. 2. Form the set S  cid:4  3. Sort the elements in S 4. If any value in S appears more than once, remove all but one instance. Do the  .   cid:4  5. Merge the two sorted sets S and S 6. There exist two elements in S whose sum is exactly x if and only if the same  .  value appears in consecutive positions in the merged output.  To justify the claim in step 4,  cid:222 rst observe that if any value appears twice in the merged output, it must appear in consecutive positions. Thus, we can restate the condition in step 5 as there exist two elements in S whose sum is exactly x if and only if the same value appears twice in the merged output.   Solution to Problem 2-1  Solutions for Chapter 2: Getting Started  2-19   cid:4    cid:4  . Because w appeared in S  Suppose that some value w appears twice. Then w appeared once in S and once , there exists some y ∈ S such that w = x − y, or in S x = w + y. Since w ∈ S, the elements w and y are in S and sum to x. Conversely, suppose that there are values w, y ∈ S such that w + y = x. Then, since x − y = w, the value w appears in S  cid:4  , and so it will appear twice in the merged output. Steps 1 and 3 require O n lg n  steps. Steps 2, 4, 5, and 6 require O n  steps. Thus the overall running time is O n lg n .   cid:4  . Thus, w is in both S and S  [ItmaybebettertoassignthisproblemaftercoveringasymptoticnotationinSec- tion3.1;otherwisepart c maybetoodif cid:222 cult.]  a. Insertion sort takes  cid:2  k2  time per k-element list in the worst case. Therefore, sorting n k lists of k elements each takes  cid:2  k2n k  =  cid:2  nk  worst-case time. b. Just extending the 2-list merge to merge all the lists at once would take  cid:2  n ·  n k   =  cid:2  n2 k  time  n from copying each element once into the result list, n k from examining n k lists at each step to select next item for result list . To achieve  cid:2  n lg n k  -time merging, we merge the lists pairwise, then merge the resulting lists pairwise, and so on, until there cid:146 s just one list. The pairwise merging requires  cid:2  n  work at each level, since we are still working on n el- ements, even if they are partitioned among sublists. The number of levels, starting with n k lists  with k elements each  and  cid:222 nishing with 1 list  with n elements , is  cid:14 lg n k  cid:15 . Therefore, the total running time for the merging is  cid:2  n lg n k  . c. The modi cid:222 ed algorithm has the same asymptotic running time as standard merge sort when  cid:2  nk + n lg n k   =  cid:2  n lg n . The largest asymptotic value of k as a function of n that satis cid:222 es this condition is k =  cid:2  lg n . To see why,  cid:222 rst observe that k cannot be more than  cid:2  lg n   i.e., it can cid:146 t have a higher-order term than lg n , for otherwise the left-hand expression wouldn cid:146 t be  cid:2  n lg n   because it would have a higher-order term than n lg n . So all we need to do is verify that k =  cid:2  lg n  works, which we can do by plugging k = lg n into  cid:2  nk + n lg n k   =  cid:2  nk + n lg n − n lg k  to get  cid:2  n lg n + n lg n − n lg lg n  =  cid:2  2n lg n − n lg lg n  , which, by taking just the high-order term and ignoring the constant coef cid:222 cient, equals  cid:2  n lg n .  d. In practice, k should be the largest list length on which insertion sort is faster  than merge sort.   2-20  Solutions for Chapter 2: Getting Started  Solution to Problem 2-2   cid:4  a. We need to show that the elements of A  form a permutation of the elements  of A.  b.  Loop invariant: At the start of each iteration of the for loop of lines 2 cid:150 4, A[ j] = min{A[k] : j ≤ k ≤ n} and the subarray A[ j . . n] is a permuta- tion of the values that were in A[ j . . n] at the time that the loop started.  Initialization: Initially,  j = n, and the subarray A[ j . . n] consists of single  element A[n]. The loop invariant trivially holds.  Maintenance: Consider an iteration for a given value of j. By the loop in- variant, A[ j] is the smallest value in A[ j . . n]. Lines 3 cid:150 4 exchange A[ j] and A[ j − 1] if A[ j] is less than A[ j − 1], and so A[ j − 1] will be the smallest value in A[ j − 1 . . n] afterward. Since the only change to the sub- array A[ j − 1 . . n] is this possible exchange, and the subarray A[ j . . n] is a permutation of the values that were in A[ j . . n] at the time that the loop started, we see that A[ j − 1 . . n] is a permutation of the values that were in A[ j − 1 . . n] at the time that the loop started. Decrementing j for the next iteration maintains the invariant. Termination: The loop terminates when j reaches i. By the statement of the loop invariant, A[i] = min{A[k] : i ≤ k ≤ n} and A[i . . n] is a permutation of the values that were in A[i . . n] at the time that the loop started.  c.  Loop invariant: At the start of each iteration of the for loop of lines 1 cid:150 4, the subarray A[1 . . i − 1] consists of the i − 1 smallest values originally in A[1 . . n], in sorted order, and A[i . . n] consists of the n − i + 1 remaining values originally in A[1 . . n].  Initialization: Before the  cid:222 rst iteration of the loop, i = 1. The subarray  A[1 . . i − 1] is empty, and so the loop invariant vacuously holds. Maintenance: Consider an iteration for a given value of i. By the loop invari- ant, A[1 . . i − 1] consists of the i smallest values in A[1 . . n], in sorted order. Part  b  showed that after executing the for loop of lines 2 cid:150 4, A[i] is the smallest value in A[i . . n], and so A[1 . . i] is now the i smallest values orig- inally in A[1 . . n], in sorted order. Moreover, since the for loop of lines 2 cid:150 4 permutes A[i . . n], the subarray A[i + 1 . . n] consists of the n − i remaining values originally in A[1 . . n]. Termination: The for loop of lines 1 cid:150 4 terminates when i = n + 1, so that i − 1 = n. By the statement of the loop invariant, A[1 . . i − 1] is the entire array A[1 . . n], and it consists of the original array A[1 . . n], in sorted order.  Note: We have received requests to change the upper bound of the outer for loop of lines 1 cid:150 4 to length[A] − 1. That change would also result in a correct algorithm. The loop would terminate when i = n, so that according to the loop invariant, A[1 . . n − 1] would consist of the n − 1 smallest values originally in A[1 . . n], in sorted order, and A[n] would contain the remaining element, which must be the largest in A[1 . . n]. Therefore, A[1 . . n] would be sorted.   Solution to Problem 2-4  Solutions for Chapter 2: Getting Started  2-21  In the original pseudocode, the last iteration of the outer for loop results in no iterations of the inner for loop of lines 1 cid:150 4. With the upper bound for i set to length[A]− 1, the last iteration of outer loop would result in one iteration of the inner loop. Either bound, length[A] or length[A]−1, yields a correct algorithm. d. The running time depends on the number of iterations of the for loop of lines 2 cid:150 4. For a given value of i, this loop makes n − i iterations, and i takes n cid:2  on the values 1, 2, . . . , n. The total number of iterations, therefore, is   n − i   = n cid:2   i=1  i  i=1  n − n cid:2  i=1 = n2 − n n + 1  2 − n = n2 − n2 2 2 − n = n2 2 2  .  Thus, the running time of bubblesort is  cid:2  n2  in all cases. The worst-case running time is the same as that of insertion sort.  n 2   cid:10    cid:11  = n n − 1  2.  a. The inversions are  1, 5 ,  2, 5 ,  3, 4 ,  3, 5 ,  4, 5 .  Remember that inver- sions are speci cid:222 ed by indices rather than by the values in the array.  b. The array with elements from {1, 2, . . . , n} with the most inversions is  cid:2 n, n − 1, n − 2, . . . , 2, 1 cid:3 . For all 1 ≤ i < j ≤ n, there is an inversion  i, j  . The number of such inversions is c. Suppose that the array A starts out with an inversion  k, j  . Then k < j and A[k] > A[ j]. At the time that the outer for loop of lines 1 cid:150 8 sets key ← A[ j], the value that started in A[k] is still somewhere to the left of A[ j]. That is, it cid:146 s in A[i], where 1 ≤ i < j, and so the inversion has become  i, j  . Some iteration of the while loop of lines 5 cid:150 7 moves A[i] one position to the right. Line 8 will eventually drop key to the left of this element, thus eliminating the inversion. Because line 5 moves only elements that are less than key, it moves only elements that correspond to inversions. In other words, each iteration of the while loop of lines 5 cid:150 7 corresponds to the elimination of one inversion.  d. We follow the hint and modify merge sort to count the number of inversions in   cid:2  n lg n  time. To start, let us de cid:222 ne a merge-inversion as a situation within the execution of merge sort in which the MERGE procedure, after copying A[p . . q] to L and A[q + 1 . . r] to R, has values x in L and y in R such that x > y. Consider an inversion  i, j  , and let x = A[i] and y = A[ j], so that i   y. We claim that if we were to run merge sort, there would be exactly one merge- inversion involving x and y. To see why, observe that the only way in which ar- ray elements change their positions is within the MERGE procedure. Moreover,   2-22  Solutions for Chapter 2: Getting Started  since MERGE keeps elements within L in the same relative order to each other, and correspondingly for R, the only way in which two elements can change their ordering relative to each other is for the greater one to appear in L and the lesser one to appear in R. Thus, there is at least one merge-inversion involving x and y. To see that there is exactly one such merge-inversion, observe that after any call of MERGE that involves both x and y, they are in the same sorted subarray and will therefore both appear in L or both appear in R in any given call thereafter. Thus, we have proven the claim. We have shown that every inversion implies one merge-inversion. In fact, the correspondence between inversions and merge-inversions is one-to-one. Sup- pose we have a merge-inversion involving values x and y, where x originally was A[i] and y was originally A[ j]. Since we have a merge-inversion, x > y. And since x is in L and y is in R, x must be within a subarray preceding the subarray containing y. Therefore x started out in a position i preceding y cid:146 s original position j, and so  i, j   is an inversion. Having shown a one-to-one correspondence between inversions and merge- inversions, it suf cid:222 ces for us to count merge-inversions. Consider a merge-inversion involving y in R. Let z be the smallest value in L that is greater than y. At some point during the merging process, z and y will be the  cid:147 exposed cid:148  values in L and R, i.e., we will have z = L[i] and y = R[ j] in line 13 of MERGE. At that time, there will be merge-inversions involving y and L[i], L[i + 1], L[i + 2], . . . , L[n1], and these n1 − i + 1 merge-inversions will be the only ones involving y. Therefore, we need to detect the  cid:222 rst time that z and y become exposed during the MERGE procedure and add the value of n1 − i + 1 at that time to our total count of merge-inversions. The following pseudocode, modeled on merge sort, works as we have just de- scribed. It also sorts the array A.  COUNT-INVERSIONS A, p, r   inversions ← 0 if p < r then q ←  cid:9   p + r   2 cid:10   inversions ← inversions +COUNT-INVERSIONS A, p, q  inversions ← inversions +COUNT-INVERSIONS A, q + 1, r   inversions ← inversions +MERGE-INVERSIONS  A, p, q, r    return inversions   Solutions for Chapter 2: Getting Started  2-23  do L[i] ← A[p + i − 1] do R[ j] ← A[q + j]  MERGE-INVERSIONS A, p, q, r   n1 ← q − p + 1 n2 ← r − q create arrays L[1 . . n1 + 1] and R[1 . . n2 + 1] for i ← 1 to n1 for j ← 1 to n2 L[n1 + 1] ← ∞ R[n2 + 1] ← ∞ i ← 1 j ← 1 inversions ← 0 counted ← FALSE for k ← p to r  do if counted = FALSE and R[ j] < L[i]  counted ← TRUE  then inversions ← inversions +n1 − i + 1 if L[i] ≤ R[ j] then A[k] ← L[i] else A[k] ← R[ j]  i ← i + 1 j ← j + 1 counted ← FALSE  return inversions  The initial call is COUNT-INVERSIONS A, 1, n . In MERGE-INVERSIONS, the boolean variable counted indicates whether we have counted the merge-inversions involving R[ j]. We count them the  cid:222 rst time that both R[ j] is exposed and a value greater than R[ j] becomes exposed in the L array. We set counted to FALSE upon each time that a new value becomes exposed in R. We don cid:146 t have to worry about merge-inversions involving the sentinel ∞ in R, since no value in L will be greater than ∞. Since we have added only a constant amount of additional work to each pro- cedure call and to each iteration of the last for loop of the merging procedure, the total running time of the above pseudocode is the same as for merge sort:  cid:2  n lg n .    Lecture Notes for Chapter 3: Growth of Functions    A way to describe behavior of functions in the limit. We cid:146 re studying asymptotic    Describe growth of functions.   Focus on what cid:146 s important by abstracting away low-order terms and constant    How we indicate running times of algorithms.   A way to compare  cid:147 sizes cid:148  of functions:  Chapter 3 overview  ef cid:222 ciency.  factors.  O ≈ ≤  cid:3  ≈ ≥  cid:2  ≈ = o ≈ < ω ≈ >  Asymptotic notation  O-notation O g n   = { f  n  : there exist positive constants c and n0 such that  0 ≤ f  n  ≤ cg n  for all n ≥ n0} .  cg n   f n   n  n0  g n  is an asymptotic upper bound for f  n . If f  n  ∈ O g n  , we write f  n  = O g n    will precisely explain this soon .   3-2  Lecture Notes for Chapter 3: Growth of Functions  Example: 2n2 = O n3 , with c = 1 and n0 = 2. Examples of functions in O n2 : n2 n2 + n n2 + 1000n 1000n2 + 1000n Also, n n 1000 n1.99999 n2  lg lg lg n   cid:1 -notation  cid:3  g n   = { f  n  : there exist positive constants c and n0 such that  0 ≤ cg n  ≤ f  n  for all n ≥ n0} .  f n   cg n   n  n0  √  g n  is an asymptotic lower bound for f  n .  n =  cid:3  lg n , with c = 1 and n0 = 16.  Example: Examples of functions in  cid:3  n2 : n2 n2 + n n2 − n 1000n2 + 1000n 1000n2 − 1000n Also, n3 n2.00001 n2 lg lg lg n 22n   cid:2 -notation  cid:2  g n   = { f  n  : there exist positive constants c1, c2, and n0 such that  0 ≤ c1g n  ≤ f  n  ≤ c2g n  for all n ≥ n0} .   Lecture Notes for Chapter 3: Growth of Functions  3-3  c2g n   f n   c1g n   n  n0  g n  is an asymptotically tight bound for f  n . Example: n2 2 − 2n =  cid:2  n2 , with c1 = 1 4, c2 = 1 2, and n0 = 8.  Theorem f  n  =  cid:2  g n   if and only if f = O g n   and f =  cid:3  g n   .  Leading constants and low-order terms don cid:146 t matter.  Asymptotic notation in equations  When on right-hand side: O n2  stands for some anonymous function in the set O n2 . 2n2+3n+1 = 2n2+ cid:2  n  means 2n2+3n+1 = 2n2+ f  n  for some f  n  ∈  cid:2  n . In particular, f  n  = 3n + 1. By the way, we interpret  of anonymous functions as =  of times the asymptotic n cid:2  notation appears: OK: 1 anonymous function i=1 O 1  + O 2  + ··· + O n  not OK: n hidden constants  O i    ⇒ no clean interpretation  When on left-hand side: No matter how the anonymous functions are chosen on the left-hand side, there is a way to choose the anonymous functions on the right- hand side to make the equation valid. Interpret 2n2 +  cid:2  n  =  cid:2  n2  as meaning for all functions f  n  ∈  cid:2  n , there exists a function g n  ∈  cid:2  n2  such that 2n2 + f  n  = g n . Can chain together: 2n2 + 3n + 1 = 2n2 +  cid:2  n   =  cid:2  n2  .  Interpretation:   First equation: There exists f  n  ∈  cid:2  n  such that 2n2+ 3n+ 1 = 2n2+ f  n .   Second equation: For all g n  ∈  cid:2  n   such as the f  n  used to make the  cid:222 rst  equation hold , there exists h n  ∈  cid:2  n2  such that 2n2 + g n  = h n .   3-4  Lecture Notes for Chapter 3: Growth of Functions  o-notation o g n   = { f  n  : for all constants c > 0, there exists a constant  n0 > 0 such that 0 ≤ f  n  < cg n  for all n ≥ n0} .  lim n→∞  f  n  g n   = 0.  Another view, probably easier to use: n1.9999 = o n2  n2  lg n = o n2  n2  cid:19 = o n2   just like 2  cid:19 < 2  n2 1000  cid:19 = o n2   ω-notation ω g n   = { f  n  : for all constants c > 0, there exists a constant  n0 > 0 such that 0 ≤ cg n  < f  n  for all n ≥ n0} .  Another view, again, probably easier to use: n2.0001 = ω n2  n2 lg n = ω n2  n2  cid:19 = ω n2   lim n→∞  f  n  g n   = ∞.  Comparisons of functions  Relational properties:  Transitivity:  Re cid:223 exivity:  f  n  =  cid:2  g n   and g n  =  cid:2  h n   ⇒ f  n  =  cid:2  h n  . Same for O,  cid:3 , o, and ω. f  n  =  cid:2   f  n  . Same for O and  cid:3 . f  n  =  cid:2  g n   if and only if g n  =  cid:2   f  n  . f  n  = O g n   if and only if g n  =  cid:3   f  n  . f  n  = o g n   if and only if g n  = ω  f  n  .  Transpose symmetry:  Symmetry:  Comparisons:      f  n  is asymptotically smaller than g n  if f  n  = o g n  . f  n  is asymptotically larger than g n  if f  n  = ω g n  .  No trichotomy. Although intuitively, we can liken O to ≤,  cid:3  to ≥, etc., unlike real numbers, where a   b, we might not be able to compare functions. Example: n1+sin n and n, since 1 + sin n oscillates between 0 and 2.   Lecture Notes for Chapter 3: Growth of Functions  3-5  Standard notations and common functions  [You probably do not want to use lecture time going over all the de cid:222 nitions and properties given in Section 3.2, but it might be worth spending a few minutes of lecturetimeonsomeofthefollowing.]  Monotonicity           f  n  is monotonically increasing if m ≤ n ⇒ f  m  ≤ f  n . f  n  is monotonically decreasing if m ≥ n ⇒ f  m  ≥ f  n . f  n  is strictly increasing if m < n ⇒ f  m  < f  n . f  n  is strictly decreasing if m > n ⇒ f  m  > f  n .  Exponentials  Useful identities: −1 = 1 a , a  am  n = amn , aman = am+n . Can relate rates of growth of polynomials and exponentials: for all real constants a and b such that a > 1,  = 0 ,  nb an  lim n→∞ which implies that nb = o an . A suprisingly useful inequality: for all real x, ex ≥ 1 + x . As x gets closer to 0, ex gets closer to 1 + x.  Logarithms  Notations:  lg n = log2 n  binary logarithm  , ln n = loge n  natural logarithm  , lgk n =  lg n k  exponentiation  , lg lg n = lg lg n   composition  . Logarithm functions apply only to the next term in the formula, so that lg n + k means  lg n  + k, and not lg n + k . In the expression logb a:    If we hold b constant, then the expression is strictly increasing as a increases.   3-6  Lecture Notes for Chapter 3: Growth of Functions     If we hold a constant, then the expression is strictly decreasing as b increases.  Useful identities for all real a > 0, b > 0, c > 0, and n, and where logarithm bases are not 1:  Changing the base of a logarithm from one constant to another only changes the value by a constant factor, so we usually don cid:146 t worry about logarithm bases in asymptotic notation. Convention is to use lg within asymptotic notation, unless the base actually matters. Just as polynomials grow more slowly than exponentials, logarithms grow more  = 0, substitute lg n for n and 2a for a:  nb an  a = blogb a ,  ab  = logc a + logc b , logc logb an = n logb a , logb a = logc a logc b  1 a  = − logb a , logb a = loga b alogb c = clogb a .  1  ,  ,  logb  slowly than polynomials. In lim n→∞ = 0 ,  lim n→∞ implying that lgb n = o na .  = lim n→∞  lgb n  2a lg n  lgb n na  Factorials n! = 1 · 2 · 3 · n. Special case: 0! = 1. Can use Stirling cid:146 s approximation, n! = √ to derive that lg n!  =  cid:2  n lg n .  1 +  cid:2    cid:6  cid:6    cid:5    cid:5    cid:7    cid:8   2πn  1 n  n e  ,  n   Solutions for Chapter 3: Growth of Functions  Solution to Exercise 3.1-1   cid:9   First, let cid:146 s clarify what the function max  f  n , g n   is. Let cid:146 s de cid:222 ne the function h n  = max  f  n , g n  . Then if f  n  ≥ g n  , h n  = if f  n  < g n  .  f  n  g n   Since f  n  and g n  are asymptotically nonnegative, there exists n0 such that f  n  ≥ 0 and g n  ≥ 0 for all n ≥ n0. Thus for n ≥ n0, f  n  + g n  ≥ f  n  ≥ 0 and f  n +g n  ≥ g n  ≥ 0. Since for any particular n, h n  is either f  n  or g n , we have f  n  + g n  ≥ h n  ≥ 0, which shows that h n  = max  f  n , g n   ≤ c2  f  n  + g n   for all n ≥ n0  with c2 = 1 in the de cid:222 nition of  cid:2  . Similarly, since for any particular n, h n  is the larger of f  n  and g n , we have for all n ≥ n0, 0 ≤ f  n  ≤ h n  and 0 ≤ g n  ≤ h n . Adding these two inequalities yields 0 ≤ f  n  + g n  ≤ 2h n , or equivalently 0 ≤   f  n  + g n   2 ≤ h n , which shows that h n  = max  f  n , g n   ≥ c1  f  n + g n   for all n ≥ n0  with c1 = 1 2 in the de cid:222 nition of  cid:2  .  Solution to Exercise 3.1-2  To show that  n + a b =  cid:2  nb , we want to  cid:222 nd constants c1, c2, n0 > 0 such that 0 ≤ c1nb ≤  n + a b ≤ c2nb for all n ≥ n0. Note that n + a ≤ n + a  ≤ 2n  when a ≤ n ,  ≥ 1 2  n  when a ≤ 1  2 n .  and n + a ≥ n − a  Thus, when n ≥ 2a, n ≤ n + a ≤ 2n . 0 ≤ 1 2   3-8  Solutions for Chapter 3: Growth of Functions  Since b > 0, the inequality still holds when all parts are raised to the power b: 0 ≤  n   cid:5   cid:5   1 2   cid:6  b ≤  n + a b ≤  2n b ,  cid:6   b  1 2  nb ≤  n + a b ≤ 2bnb .  0 ≤ Thus, c1 =  1 2 b, c2 = 2b, and n0 = 2a satisfy the de cid:222 nition.  Solution to Exercise 3.1-3  Let the running time be T  n . T  n  ≥ O n2  means that T  n  ≥ f  n  for some function f  n  in the set O n2 . This statement holds for any running time T  n , since the function g n  = 0 for all n is in O n2 , and running times are always nonnegative. Thus, the statement tells us nothing about the running time.  Solution to Exercise 3.1-4  2n+1 = O 2n  , but 22n  cid:19 = O 2n . To show that 2n+1 = O 2n  , we must  cid:222 nd constants c, n0 > 0 such that 0 ≤ 2n+1 ≤ c · 2n for all n ≥ n0 . Since 2n+1 = 2 · 2n for all n, we can satisfy the de cid:222 nition with c = 2 and n0 = 1. To show that 22n  cid:19 = O 2n , assume there exist constants c, n0 > 0 such that 0 ≤ 22n ≤ c · 2n for all n ≥ n0 . Then 22n = 2n · 2n ≤ c · 2n ⇒ 2n ≤ c. But no constant is greater than all 2n, and so the assumption leads to a contradiction.  Solution to Exercise 3.1-8   cid:3  g n, m   = { f  n, m  : there exist positive constants c, n0, and m0  such that 0 ≤ cg n, m  ≤ f  n, m  for all n ≥ n0 and m ≥ m0} .   cid:2  g n, m   = { f  n, m  : there exist positive constants c1, c2, n0, and m0 such that 0 ≤ c1g n, m  ≤ f  n, m  ≤ c2g n, m  for all n ≥ n0 and m ≥ m0} .   Solutions for Chapter 3: Growth of Functions  3-9  Solution to Exercise 3.2-4   cid:14 lg n cid:15 ! is not polynomially bounded, but cid:14 lg lg n cid:15 ! is. Proving that a function f  n  is polynomially bounded is equivalent to proving that lg  f  n   = O lg n  for the following reasons.     If f is polynomially bounded, then there exist constants c, k, n0 such that for all n ≥ n0, f  n  ≤ cnk. Hence, lg  f  n   ≤ kc lg n, which, since c and k are constants, means that lg  f  n   = O lg n .    Similarly, if lg  f  n   = O lg n , then f is polynomially bounded. In the following proofs, we will make use of the following two facts:  lg n!  =  cid:2  n lg n   by equation  3.18  .  1. 2.  cid:14 lg n cid:15  =  cid:2  lg n , because     cid:14 lg n cid:15  ≥ lg n    cid:14 lg n cid:15  < lg n + 1 ≤ 2 lg n for all n ≥ 2  lg  cid:14 lg n cid:15 !  =  cid:2   cid:14 lg n cid:15  lg cid:14 lg n cid:15    =  cid:2  lg n lg lg n  = ω lg n  .  lg  cid:14 lg lg n cid:15 !  =  cid:2   cid:14 lg lg n cid:15  lg cid:14 lg lg n cid:15    =  cid:2  lg lg n lg lg lg n  = o  lg lg n 2  = o lg2 lg n   = o lg n  .  Therefore, lg  cid:14 lg n cid:15 !   cid:19 = O lg n , and so  cid:14 lg n cid:15 ! is not polynomially bounded.  The last step above follows from the property that any polylogarithmic function grows more slowly than any positive polynomial function, i.e., that for constants a, b > 0, we have lgb n = o na . Substitute lg n for n, 2 for b, and 1 for a, giving lg2 lg n  = o lg n . Therefore, lg  cid:14 lg lg n cid:15 !  = O lg n , and so  cid:14 lg lg n cid:15 ! is polynomially bounded.  Solution to Problem 3-3  a. Here is the ordering, where functions on the same line are in the same equiva-  lence class, and those higher on the page are  cid:3  of those below them:   3-10  Solutions for Chapter 3: Growth of Functions  22n+1 22n  n + 1 ! n! en n · 2n 2n  3 2 n  lg n lg n = nlg lg n  lg n ! n3 n2 = 4lg n n lg n and lg n!  n = 2lg n √ 2 lg n = √ √ 2 ln n cid:12  lg2 n  2 lg n  n      lg n ln ln n ∗ 2lg ∗ lg n  ∗ lg lg lg n1  lg n = 2  and 1  n n and lg ∗ n  see justi cid:222 cation 7 see justi cid:222 cation 1  see identity 1 see justi cid:222 cations 2, 8  see identity 2 see justi cid:222 cation 6 see identity 3 see identity 6, justi cid:222 cation 3 see identity 5, justi cid:222 cation 4  see justi cid:222 cation 5  see identity 7  see identity 4  Much of the ranking is based on the following properties:   Exponential functions grow faster than polynomial functions, which grow  faster than polylogarithmic functions.    The base of a logarithm doesn cid:146 t matter asymptotically, but the base of an  exponential and the degree of a polynomial do matter.  We have the following identities: 1.  lg n lg n = nlg lg n because alogb c = clogb a. 2. 4lg n = n2 because alogb c = clogb a. 3. 2lg n = n. 4. 2 = n1  lg n by raising identity 3 to the power 1  lg n. √  cid:10 √ 2 lg n = n 2  lg n by raising identity 4 to the power 5. 2 √ n because 2 ∗ lg n  =  lg n  − 1. ∗   cid:11 lg n = 2 1 2  lg n = 2lg   cid:11 lg n = √   cid:10 √  6. 7. lg  √  2   cid:12  n = √  2 lg n.  n.  The following justi cid:222 cations explain some of the rankings: 1. en = 2n e 2 n = ω n2n , since  e 2 n = ω n . 2.  lg n ! = ω n3  by taking logs: approximation, lg n3  = 3 lg n. lg lg n = ω 3 .  lg lg n ! =  cid:2  lg n lg lg n  by Stirling cid:146 s   Solutions for Chapter 3: Growth of Functions  2 lg n  3-11   cid:10   2 lg n .  √ 2  2 lg n =  2 lg n = cid:12   √ 2 lg n =  1 2  lg n, lg 2   cid:11   cid:12   cid:12  √ √ 2 lg n = ω 3.   by taking logs: lg  2 lg n.  1 2  lg n = ω   cid:12  √ √ 2 lg n = ω lg2 n  by taking logs: lg 2 4. 2 2 lg n = ω 2 lg lg n . 5. ln ln n = ω 2lg ∗ n  by taking logs: lg 2lg 6. lg n!  =  cid:2  n lg n   equation  3.18  . 7. n! =  cid:2  nn+1 2e −n  by dropping constants and low-order terms in equa- 8.  lg n ! =  cid:2   lg n lg n+1 2e − lg n   by substituting lg n for n in the previous  2 lg n, lg lg2 n = 2 lg lg n. n. lg ln ln n = ω lg ∗  tion  3.17 . justi cid:222 cation.  lg n ! =  cid:2   lg n lg n+1 2n  − lg e  because alogb c = clogb a.  n = lg  n .  ∗  ∗  b. The following f  n  is nonnegative, and for all functions gi  n  in part  a , f  n    cid:9  22n+2 0  is neither O gi  n   nor  cid:3  gi  n  . f  n  =  if n is even , if n is odd .    Lecture Notes for Chapter 4: Recurrences  Chapter 4 overview  A recurrence is a function is de cid:222 ned in terms of      one or more base cases, and itself, with smaller arguments.   cid:9   cid:9   cid:9   cid:9   if n = 1 , 1 T  n − 1  + 1 if n > 1 .  if n = 1 , 1 2T  n 2  + n if n ≥ 1 .        Examples: T  n  = Solution: T  n  = n. T  n  = Solution: T  n  = n lg n + n. T  n  = Solution: T  n  = lg lg n. T  n  = Solution: T  n  =  cid:2  n lg n .  0 T    √        if n = 2 , n  + 1 if n > 2 .  if n = 1 , 1 T  n 3  + T  2n 3  + n if n > 1 .  [The notes for this chapter are fairly brief because we teach recurrences in much greaterdetailinaseparatediscretemathcourse.] Many technical issues:   Floors and ceilings  [Floors andceilings caneasilyberemovedanddon cid:146 t affectthesolution tothe recurrence. Theyarebetterlefttoadiscretemathcourse.]    Exact vs. asymptotic functions   Boundary conditions In algorithm analysis, we usually express both the recurrence and its solution using asymptotic notation.   4-2  Lecture Notes for Chapter 4: Recurrences    Example: T  n  = 2T  n 2  +  cid:2  n , with solution T  n  =  cid:2  n lg n .   The boundary conditions are usually expressed as  cid:147 T  n  = O 1  for suf cid:222 -  ciently small n. cid:148     When we desire an exact, rather than an asymptotic, solution, we need to deal     with boundary conditions. In practice, we just use asymptotics most of the time, and we ignore boundary conditions.  [In my course, there are only two acceptable ways of solving recurrences: the substitution method andthemastermethod. Unlesstherecursion treeiscarefully accounted for,Idonotacceptitasaproofofasolution, thoughIcertainly accept a recursion tree as a way to generate a guess for substitution method. You may choosetoallowrecursiontreesasproofsinyourcourse,inwhichcasesomeofthe substitutionproofsinthesolutionsforthischapterbecomerecursiontrees. I also never use the iteration method, which had appeared in the  cid:222 rst edition of Introduction to Algorithms. I  cid:222 nd that it is too easy to make an error in paren- thesization, and that recursion trees give a better intuitive idea than iterating the recurrenceofhowtherecurrenceprogresses.]  Substitution method   cid:9   1. Guess the solution. 2. Use induction to  cid:222 nd the constants and show that the solution works.  2. Induction:  if n = 1 , 1 2T  n 2  + n if n > 1 .  Example: T  n  = 1. Guess: T  n  = n lg n + n. [Here, we have a recurrence with an exact func- tion,rather than asymptotic notation, andthesolution isalso exactrather than asymptotic. We cid:146 llhavetocheckboundaryconditionsandthebasecase.] Basis: n = 1 ⇒ n lg n + n = 1 = T  n  Inductive step: Inductive hypothesis is that T  k  = k lg k + k for all k < n. We cid:146 ll use this inductive hypothesis for T  n 2 .  cid:8   cid:7  + n T  n  = 2T n 2 + n = 2 n n lg 2 2 2 + n + n = n lg n 2 = n lg n − lg 2  + n + n = n lg n − n + n + n = n lg n + n .   by inductive hypothesis   + n   cid:7    cid:8    Lecture Notes for Chapter 4: Recurrences  4-3  Generally, we use asymptotic notation:   We would write T  n  = 2T  n 2  +  cid:2  n .   We assume T  n  = O 1  for suf cid:222 ciently small n.   We express the solution by asymptotic notation: T  n  =  cid:2  n lg n .   We don cid:146 t worry about boundary cases, nor do we show base cases in the substi-  tution proof.   T  n  is always constant for any constant n.   Since we are ultimately interested in an asymptotic solution to a recurrence,  it will always be possible to choose base cases that work.    When we want an asymptotic solution to a recurrence, we don cid:146 t worry about  the base cases in our proofs.    When we want an exact solution, then we have to deal with base cases.  For the substitution method:   Name the constant in the additive term.   Show the upper  O  and lower   cid:3   bounds separately. Might need to use dif-  ferent constants for each.  Example: T  n  = 2T  n 2 + cid:2  n . If we want to show an upper bound of T  n  = 2T  n 2  + O n , we write T  n  ≤ 2T  n 2  + cn for some positive constant c. 1. Upper bound:  Guess: T  n  ≤ dn lg n for some positive constant d. We are given c in the recurrence, and we get to choose d as any positive constant. It cid:146 s OK for d to depend on c.  cid:8   cid:7  Substitution: T  n  ≤ 2T  n 2  + cn = 2 n n lg d 2 2 + cn = dn lg n 2 = dn lg n − dn + cn ≤ dn lg n  + cn  if −dn + cn ≤ 0 , d ≥ c  2. Lower bound: Write T  n  ≥ 2T  n 2  + cn for some positive constant c.  Therefore, T  n  = O n lg n . Guess: T  n  ≥ dn lg n for some positive constant d.  cid:8   cid:7  Substitution: T  n  ≥ 2T  n 2  + cn = 2 n n lg d 2 2 + cn = dn lg n 2 = dn lg n − dn + cn ≥ dn lg n  + cn  if −dn + cn ≥ 0 , d ≤ c   4-4  Lecture Notes for Chapter 4: Recurrences  Therefore, T  n  =  cid:3  n lg n .  Therefore, T  n  =  cid:2  n lg n . [Forthisparticularrecurrence,wecanused = c for boththeupper-boundandlower-boundproofs. Thatwon cid:146 talwaysbethecase.]  Make sure you show the same exact form when doing a substitution proof. Consider the recurrence T  n  = 8T  n 2  +  cid:2  n2  . For an upper bound: T  n  ≤ 8T  n 2  + cn2 . Guess: T  n  ≤ dn3. T  n  ≤ 8d n 2 3 + cn2 = 8d n3 8  + cn2 = dn3 + cn2  cid:19 ≤ dn3  doesn cid:146 t work!  Remedy: Subtract off a lower-order term. Guess: T  n  ≤ dn3 − d  cid:4  n2. T  n  ≤ 8 d n 2 3 − d  cid:4  n 2 2  + cn2 = 8d n3 8  − 8d  cid:4  n2 4  + cn2 = dn3 − 2d n2 + cn2  cid:4  n2 + cn2 = dn3 − d n2 − d  cid:4   cid:4  ≤ dn3 − d if −d  cid:4   cid:4  n2  n2 + cn2 ≤ 0 ,  cid:4  ≥ c  d  Be careful when using asymptotic notation. The false proof for the recurrence T  n  = 4T  n 4  + n, that T  n  = O n : T  n  ≤ 4 c n 4   + n  ≤ cn + n = O n   wrong! Because we haven cid:146 t proven the exact form of our inductive hypothesis  which is that T  n  ≤ cn , this proof is false.  Recursion trees  Use to generate a guess. Then verify by substitution method. Example: T  n  = T  n 3 +T  2n 3 + cid:2  n . For upper bound, rewrite as T  n  ≤ T  n 3  + T  2n 3  + cn; for lower bound, as T  n  ≥ T  n 3  + T  2n 3  + cn. By summing across each level, the recursion tree shows the cost at each level of recursion  minus the costs of recursive calls, which appear in subtrees :   Lecture Notes for Chapter 4: Recurrences  4-5  cn   cid:133   c n 3   c 2n 3   c n 9   c 2n 9   c 2n 9   c 4n 9   cn  cn  cn  to 1.  rightmost branch peters out after log3 2 n levels  leftmost branch peters out after log3 n levels   There are log3 n full levels, and after log3 2 n levels, the problem size is down   Each level contributes ≤ cn.   Lower bound guess: ≥ dn log3 n =  cid:3  n lg n  for some positive constant d.   Upper bound guess: ≤ dn log3 2 n = O n lg n  for some positive constant d.   Then prove by substitution. 1. Upper bound:  Guess: T  n  ≤ dn lg n. Substitution: T  n  ≤ T  n 3  + T  2n 3  + cn ≤ d n 3  lg n 3  + d 2n 3  lg 2n 3  + cn =  d n 3  lg n − d n 3  lg 3  +  d 2n 3  lg n − d 2n 3  lg 3 2   + cn = dn lg n − d  n 3  lg 3 +  2n 3  lg 3 2   + cn = dn lg n − d  n 3  lg 3 +  2n 3  lg 3 −  2n 3  lg 2  + cn = dn lg n − dn lg 3 − 2 3  + cn ≤ dn lg n  if −dn lg 3 − 2 3  + cn ≤ 0 ,  d ≥  c  lg 3 − 2 3  .  2. Lower bound:  Therefore, T  n  = O n lg n . Note: Make sure that the symbolic constants used in the recurrence  e.g., c  and the guess  e.g., d  are different. Guess: T  n  ≥ dn lg n. Substitution: Same as for the upper bound, but replacing ≤ by ≥. End up needing 0 < d ≤ Therefore, T  n  =  cid:3  n lg n .  lg 3 − 2 3  c  .  Since T  n  = O n lg n  and T  n  =  cid:3  n lg n , we conclude that T  n  =  cid:2  n lg n .   4-6  Lecture Notes for Chapter 4: Recurrences  Master method  Used for many divide-and-conquer recurrences of the form T  n  = aT  n b  + f  n  , where a ≥ 1, b > 1, and f  n  > 0. Based on the master theorem  Theorem 4.1 . Compare nlogb a vs. f  n : Case 1:  f  n  = O nlogb a− cid:6   for some constant  cid:6  > 0.    f  n  is polynomially smaller than nlogb a.  Solution: T  n  =  cid:2  nlogb a .  Intuitively: cost is dominated by leaves.  f  n  =  cid:2  nlogb a lgk n , where k ≥ 0.  Case 2:  [ThisformulationofCase2ismoregeneralthaninTheorem4.1,anditisgiven inExercise4.4-2.]   f  n  is within a polylog factor of nlogb a, but not smaller.  Solution: T  n  =  cid:2  nlogb a lgk+1 n .  Intuitively: cost is nlogb a lgk n at each level, and there are  cid:2  lg n  levels.  Simple case: k = 0 ⇒ f  n  =  cid:2  nlogb a  ⇒ T  n  =  cid:2  nlogb a lg n . f  n  =  cid:3  nlogb a+ cid:6   for some constant  cid:6  > 0 and f  n  satis cid:222 es the regu- larity condition a f  n b  ≤ c f  n  for some constant c < 1 and all suf cid:222 ciently large n.   f  n  is polynomially greater than nlogb a.  Solution: T  n  =  cid:2   f  n  .  Intuitively: cost is dominated by root.   Case 3:  What cid:146 s with the Case 3 regularity condition?   Generally not a problem.    It always holds whenever f  n  = nk and f  n  =  cid:3  nlogb a+ cid:6   for constant  cid:6  > 0. [Proving this makes a nice homework exercise. See below.] So you don cid:146 t need to check it when f  n  is a polynomial. [Here cid:146 s a proof that the regularity condition holds when f  n  = nk and f  n  =  cid:3  nlogb a+ cid:6   forconstant  cid:6  > 0. Since f  n  =  cid:3  nlogb a+ cid:6   and f  n  = nk, we have that k > logb a. Using a base of b and treating both sides as exponents, we have bk > blogb a = a, and so a bk < 1. Since a, b,and k areconstants, ifwelet c = a bk,then c isaconstant strictlylessthan 1. Wehavethata f  n b  = a n b k =  a bk  nk = c f  n ,andso theregularityconditionissatis cid:222 ed.]  Examples:    T  n  = 5T  n 2  +  cid:2  n2  nlog2 5 vs. n2 Since log2 5 −  cid:6  = 2 for some constant  cid:6  > 0, use Case 1 ⇒ T  n  =  cid:2  nlg 5    Lecture Notes for Chapter 4: Recurrences  4-7           T  n  = 27T  n 3  +  cid:2  n3 lg n  nlog3 27 = n3 vs. n3 lg n Use Case 2 with k = 1 ⇒ T  n  =  cid:2  n3 lg2 n  T  n  = 5T  n 2  +  cid:2  n3  nlog2 5 vs. n3 Now lg 5 +  cid:6  = 3 for some constant  cid:6  > 0 Check regularity condition  don cid:146 t really need to since f  n  is a polynomial : a f  n b  = 5 n 2 3 = 5n3 8 ≤ cn3 for c = 5 8 < 1 Use Case 3 ⇒ T  n  =  cid:2  n3  T  n  = 27T  n 3  +  cid:2  n3  lg n  nlog3 27 = n3 vs. n3  lg n = n3 lg Cannot use the master method.  −1 n  cid:19 =  cid:2  n3 lgk n  for any k ≥ 0.  [Wedon cid:146 tprovethemastertheoreminouralgorithmscourse. Wesometimesprove asimpli cid:222 edversionforrecurrencesoftheform T  n  = aT  n b + nc.Section4.4 ofthetexthasthefullproofofthemastertheorem.]   Solutions for Chapter 4: Recurrences  Solution to Exercise 4.2-2  The shortest path from the root to a leaf in the recursion tree is n →  1 3 n →  1 3 2n → ··· → 1. Since  1 3 kn = 1 when k = log3 n, the height of the part of the tree in which every node has two children is log3 n. Since the values at each of these levels of the tree add up to n, the solution to the recurrence is at least n log3 n =  cid:3  n lg n .  Solution to Exercise 4.2-5  T  n  = T  αn  + T   1 − α n  + n We saw the solution to the recurrence T  n  = T  n 3  + T  2n 3  + cn in the text. This recurrence can be similarly solved. Without loss of generality, let α ≥ 1−α, so that 0 < 1−α ≤ 1 2 and 1 2 ≤ α < 1.  cαn  c 1 − α n  log1  1−α  n  cα2n  cα 1 − α n  cα 1 − α n  c 1 − α 2n  log1 α n  cn   cid:133   cn  cn  cn   cid:133  Total: O n lg n   The recursion tree is full for log1  1−α  n levels, each contributing cn, so we guess  cid:3  n log1  1−α  n  =  cid:3  n lg n . It has log1 α n levels, each contributing ≤ cn, so we guess O n log1 α n  = O n lg n .   Solutions for Chapter 4: Recurrences  4-9  Now we show that T  n  =  cid:2  n lg n  by substitution. To prove the upper bound, we need to show that T  n  ≤ dn lg n for a suitable constant d > 0. T  n  = T  αn  + T   1 − α n  + cn  ≤ dαn lg αn  + d 1 − α n lg  1 − α n  + cn = dαn lg α + dαn lg n + d 1 − α n lg 1 − α  + d 1 − α n lg n + cn = dn lg n + dn α lg α +  1 − α  lg 1 − α   + cn ≤ dn lg n ,  if dn α lg α +  1 − α  lg 1 − α   + cn ≤ 0. This condition is equivalent to d α lg α +  1 − α  lg 1 − α   ≤ −c . Since 1 2 ≤ α < 1 and 0 < 1− α ≤ 1 2, we have that lg α < 0 and lg 1− α  < 0. Thus, α lg α +  1 − α  lg 1 − α  < 0, so that when we multiply both sides of the inequality by this factor, we need to reverse the inequality: d ≥ or d ≥  α lg α +  1 − α  lg 1 − α   −c  −α lg α + − 1 − α  lg 1 − α   c  .  The fraction on the right-hand side is a positive constant, and so it suf cid:222 ces to pick any value of d that is greater than or equal to this fraction. To prove the lower bound, we need to show that T  n  ≥ dn lg n for a suitable constant d > 0. We can use the same proof as for the upper bound, substituting ≥ for ≤, and we get the requirement that 0 < d ≤ −α lg α −  1 − α  lg 1 − α  Therefore, T  n  =  cid:2  n lg n .  c  .  Note: In parts  a ,  b , and  d  below, we are applying case 3 of the master theorem, which requires the regularity condition that a f  n b  ≤ c f  n  for some constant f  n  has the form nk. The regularity condition is c < 1. In each of these parts, satis cid:222 ed because a f  n b  = ank bk =  a bk nk =  a bk  f  n , and in each of the cases below, a bk is a constant strictly less than 1. a. T  n  = 2T  n 2  + n3 =  cid:2  n3 . This is a divide-and-conquer recurrence with a = 2, b = 2, f  n  = n3, and nlogb a = nlog2 2 = n. Since n3 =  cid:3  nlog2 2+2  and a bk = 2 23 = 1 4 < 1, case 3 of the master theorem applies, and T  n  =  cid:2  n3 . b. T  n  = T  9n 10  + n =  cid:2  n . This is a divide-and-conquer recurrence with f  n  = n, and nlogb a = nlog10 9 1 = n0 = 1. Since n = a = 1, b = 10 9,  cid:3  nlog10 9 1+1  and a bk = 1  10 9 1 = 9 10 < 1, case 3 of the master theorem applies, and T  n  =  cid:2  n .  Solution to Problem 4-1   4-10  Solutions for Chapter 4: Recurrences  c. T  n  = 16T  n 4  + n2 =  cid:2  n2 lg n . This is another divide-and-conquer recurrence with a = 16, b = 4, f  n  = n2, and nlogb a = nlog4 16 = n2. Since n2 =  cid:2  nlog4 16 , case 2 of the master theorem applies, and T  n  =  cid:2  n2 lg n . d. T  n  = 7T  n 3  + n2 =  cid:2  n2 . This is a divide-and-conquer recurrence with a = 7, b = 3, f  n  = n2, and nlogb a = nlog3 7. Since 1 < log3 7 < 2, we have that n2 =  cid:3  nlog3 7+ cid:6   for some constant  cid:6  > 0. We also have a bk = 7 32 = 7 9 < 1, so that case 3 of the master theorem applies, and T  n  =  cid:2  n2 . e. T  n  = 7T  n 2  + n2 = O nlg 7 . This is a divide-and-conquer recurrence with a = 7, b = 2, f  n  = n2, and nlogb a = nlog2 7. Since 2 < lg 7 < 3, we have that n2 = O nlog2 7− cid:6   for some constant  cid:6  > 0. Thus, case 1 of the master theorem applies, and T  n  =  cid:2  nlg 7 . √ f. T  n  = 2T  n 4  + √ n =  cid:2   n, and nlogb a = nlog4 2 = √ n lg n . This is another divide-and-conquer recurrence with a = 2, b = 4, n. n =  cid:2  nlog4 2 , case 2 of the master theorem applies, and T  n  = √ Since n lg n .  cid:2    f  n  = √  √  g. T  n  = T  n − 1  + n  Using the recursion tree shown below, we get a guess of T  n  =  cid:2  n2 .  n  n  n-1  n-2  2  1  n  n-1  n-2  ...  2  1   cid:2  n2   First, we prove the T  n  =  cid:3  n2  part by induction. The inductive hypothesis is T  n  ≥ cn2 for some constant c > 0. T  n  = T  n − 1  + n ≥ c n − 1 2 + n = cn2 − 2cn + c + n ≥ cn2  if −2cn + n + c ≥ 0 or, equivalently, n 1 − 2c  + c ≥ 0. This condition holds when n ≥ 0 and 0 < c ≤ 1 2. For the upper bound, T  n  = O n2 , we use the inductive hypothesis that T  n  ≤ cn2 for some constant c > 0. By a similar derivation, we get that   Solution to Problem 4-4  Solutions for Chapter 4: Recurrences  4-11  T  n  ≤ cn2 if −2cn + n + c ≤ 0 or, equivalently, n 1 − 2c  + c ≤ 0. This condition holds for c = 1 and n ≥ 1. Thus, T  n  =  cid:3  n2  and T  n  = O n2 , so we conclude that T  n  =  cid:2  n2 .  h. T  n  = T    n  + 1  √  The easy way to do this is with a change of variables, as on page 66 of the text. Let m = lg n and S m  = T  2m  . T  2m   = T  2m 2  + 1, so S m  = S m 2  + 1. Using the master theorem, nlogb a = nlog2 1 = n0 = 1 and f  n  = 1. Since 1 =  cid:2  1 , case 2 applies and S m  =  cid:2  lg m . There- fore, T  n  =  cid:2  lg lg n .  [Thisproblemissolvedonlyforpartsa,c,e,f,g,h,andi.] a. T  n  = 3T  n 2  + n lg n  We have f  n  = n lg n and nlogb a = nlg 3 ≈ n1.585. Since n lg n = O nlg 3− cid:6    for any 0 <  cid:6  ≤ 0.58, by case 1 of the master theorem, we have T  n  =  cid:2  nlg 3 .  √ c. T  n  = 4T  n 2  + n2 √ n n = n5 2 and nlogb a = nlog2 4 = nlg 2. Since n5 2 = We have f  n  = n2  cid:3  nlg 2+3 2 , we look at the regularity condition in case 3 of the master theorem. √ n 2 = n5 2  2 ≤ cn5 2 for 1  2 ≤ c < 1. We have a f  n b  = 4 n 2 2 √ Case 3 applies, and we have T  n  =  cid:2  n2 n .  √  √  e. T  n  = 2T  n 2  + n  lg n  We can get a guess by means of a recursion tree:  n 2  lg n 2   n 2  lg n 2   lg n  n 4  lg n 4   n 4  lg n 4   n 4  lg n 4   n 4  lg n 4   n lg n   cid:133   n lg n  n  lg n − 1  n  lg n − 2  lg n−1 cid:2   i=0   cid:133  =  cid:2  n lg lg n   n  lg n − i  We get the sum on each level by observing that at depth i, we have 2i nodes, each with a numerator of n 2i and a denominator of lg n 2i   = lg n − i, so that the cost at depth i is   4-12  Solutions for Chapter 4: Recurrences  .  n  =  2i · n 2i lg n − i lg n−1 cid:2   lg n − i lg n cid:2  The sum for all levels is lg n cid:2   lg n − i  = n  i=1  i=0  n  n i  1 i  i=1  = n = n ·  cid:2  lg lg n  =  cid:2  n lg lg n  .   by equation  A.7 , the harmonic series   We can use this analysis as a guess that T  n  =  cid:2  n lg lg n . If we were to do a straight substitution proof, it would be rather involved. Instead, we will show by substitution that T  n  ≤ n 1 + H cid:9 lg n cid:10   and T  n  ≥ n · H cid:14 lg n cid:15 , where Hk is the kth harmonic number: Hk = 1 1 + 1 2 + 1 3 + ··· + 1 k. We also de cid:222 ne H0 = 0. Since Hk =  cid:2  lg k , we have that H cid:9 lg n cid:10  =  cid:2  lg cid:9 lg n cid:10   =  cid:2  lg lg n  and H cid:14 lg n cid:15  =  cid:2  lg cid:14 lg n cid:15   =  cid:2  lg lg n . Thus, we will have that T  n  =  cid:2  n lg lg n . The base case for the proof is for n = 1, and we use T  1  = 1. Here, lg n = 0, so that lg n =  cid:9 lg n cid:10  =  cid:14 lg n cid:15 . Since H0 = 0, we have T  1  = 1 ≤ 1 1 + H0  and T  1  = 1 ≥ 0 = 1 · H0. For the upper bound of T  n  ≤ n 1 + H cid:9 lg n cid:10  , we have T  n  = 2T  n 2  + n  lg n  ≤ 2  n 2  1 + H cid:9 lg n 2  cid:10    + n  lg n = n 1 + H cid:9 lg n−1 cid:10   + n  lg n = n 1 + H cid:9 lg n cid:10 −1 + 1  lg n  ≤ n 1 + H cid:9 lg n cid:10 −1 + 1  cid:9 lg n cid:10   = n 1 + H cid:9 lg n cid:10   ,  where the last line follows from the identity Hk = Hk−1 + 1 k. The upper bound of T  n  ≥ n · H cid:14 lg n cid:15  is similar: T  n  = 2T  n 2  + n  lg n  ≥ 2  n 2  · H cid:14 lg n 2  cid:15   + n  lg n = n · H cid:14 lg n−1 cid:15  + n  lg n = n ·  H cid:14 lg n cid:15 −1 + 1  lg n  ≥ n ·  H cid:14 lg n cid:15 −1 + 1  cid:14 lg n cid:15   = n · H cid:14 lg n cid:15  .  Thus, T  n  =  cid:2  n lg lg n .  f. T  n  = T  n 2  + T  n 4  + T  n 8  + n  Using the recursion tree shown below, we get a guess of T  n  =  cid:2  n .   Solutions for Chapter 4: Recurrences  4-13  n  n 4  n 8  n 2  n 8  n 4  n 16  n 8  n 16  n 32  n 16  n 32  n 64  log4 n  log8 n  n  1 4  + 1  64     n  n  4+2+1  8    = 7 8 n  8  32  64  16  = 7  + 3  + 2 + 2 = n 16+16+12+4+1 = n 49 2n ...  cid:8 i  cid:7   64  8  n =  cid:2  n   7 8  log n cid:2   i=1  We use the substitution method to prove that T  n  = O n . Our inductive hypothesis is that T  n  ≤ cn for some constant c > 0. We have T  n  = T  n 2  + T  n 4  + T  n 8  + n  ≤ cn 2 + cn 4 + cn 8 + n = 7cn 8 + n =  1 + 7c 8 n ≤ cn Therefore, T  n  = O n . Showing that T  n  =  cid:3  n  is easy: T  n  = T  n 2  + T  n 4  + T  n 8  + n ≥ n . Since T  n  = O n  and T  n  =  cid:3  n , we have that T  n  =  cid:2  n .  if c ≥ 8 .  g. T  n  = T  n − 1  + 1 n  This recurrence corresponds to the harmonic series, so that T  n  = Hn, where Hn = 1 1+1 2+1 3+···+1 n. For the base case, we have T  1  = 1 = H1. For the inductive step, we assume that T  n − 1  = Hn−1, and we have T  n  = T  n − 1  + 1 n  = Hn−1 + 1 n = Hn .  Since Hn =  cid:2  lg n  by equation  A.7 , we have that T  n  =  cid:2  lg n .  h. T  n  = T  n − 1  + lg n  We guess that T  n  =  cid:2  n lg n . To prove the upper bound, we will show that T  n  = O n lg n . Our inductive hypothesis is that T  n  ≤ cn lg n for some constant c. We have   4-14  Solutions for Chapter 4: Recurrences  T  n  = T  n − 1  + lg n  ≤ c n − 1  lg n − 1  + lg n = cn lg n − 1  − c lg n − 1  + lg n ≤ cn lg n − 1  − c lg n 2  + lg n = cn lg n − 1  − c lg n + c + lg n < cn lg n − c lg n + c + lg n ≤ cn lg n , if −c lg n + c + lg n ≤ 0. Equivalently, −c lg n + c + lg n ≤ 0   since lg n − 1  ≥ lg n 2  for n ≥ 2   c ≤  c − 1  lg n lg n ≥ c  c − 1  . This works for c = 2 and all n ≥ 4. To prove the lower bound, we will show that T  n  =  cid:3  n lg n . Our inductive hypothesis is that T  n  ≥ cn lg n + dn for constants c and d. We have T  n  = T  n − 1  + lg n  ≥ c n − 1  lg n − 1  + d n − 1  + lg n = cn lg n − 1  − c lg n − 1  + dn − d + lg n ≥ cn lg n 2  − c lg n − 1  + dn − d + lg n  since lg n − 1  ≥ lg n 2  for n ≥ 2  = cn lg n − cn − c lg n − 1  + dn − d + lg n ≥ cn lg n ,  if −cn − c lg n − 1  + dn − d + lg n ≥ 0. Since −cn − c lg n − 1  + dn − d + lg n > −cn − c lg n − 1  + dn − d + lg n − 1  , it suf cid:222 ces to  cid:222 nd conditions in which −cn−c lg n−1 +dn−d+lg n−1  ≥ 0. Equivalently, −cn − c lg n − 1  + dn − d + lg n − 1  ≥ 0 This works for c = 1, d = 2, and all n ≥ 2. Since T  n  = O n lg n  and T  n  =  cid:3  n lg n , we conclude that T  n  =  cid:2  n lg n .   d − c n ≥  c − 1  lg n − 1  + d .  i. T  n  = T  n − 2  + 2 lg n  We guess that T  n  =  cid:2  n lg n . We show the upper bound of T  n  = O n lg n  by means of the inductive hypothesis T  n  ≤ cn lg n for some con- stant c > 0. We have T  n  = T  n − 2  + 2 lg n  ≤ c n − 2  lg n − 2  + 2 lg n ≤ c n − 2  lg n + 2 lg n =  cn − 2c + 2  lg n   Solutions for Chapter 4: Recurrences  4-15  = cn lg n +  2 − 2c  lg n ≤ cn lg n if c > 1 .  Therefore, T  n  = O n lg n . For the lower bound of T  n  =  cid:3  n lg n , we cid:146 ll show that T  n  ≥ cn lg n+dn, for constants c, d > 0 to be chosen. We assume that n ≥ 4, which implies that 1. lg n − 2  ≥ lg n 2 , 2. n 2 ≥ lg n, and 3. n 2 ≥ 2.  We cid:146 ll use these inequalities as we go along.  We have T  n  ≥ c n − 2  lg n − 2  + d n − 2  + 2 lg n  = cn lg n − 2  − 2c lg n − 2  + dn − 2d + 2 lg n > cn lg n − 2  − 2c lg n + dn − 2d + 2 lg n  since − lg n < − lg n − 2   = cn lg n − 2  − 2 c − 1  lg n + dn − 2d ≥ cn lg n 2  − 2 c − 1  lg n + dn − 2d = cn lg n − cn − 2 c − 1  lg n + dn − 2d ≥ cn lg n ,   by inequality  1  above   if −cn−2 c−1  lg n+dn−2d ≥ 0 or, equivalently, dn ≥ cn+2 c−1  lg n+2d. Pick any constant c > 1 2, and then pick any constant d such that d ≥ 2 2c − 1  .  The requirement that c > 1 2 means that d is positive.  Then d 2 ≥ 2c − 1 = c +  c − 1  , and adding d 2 to both sides, we have d ≥ c +  c − 1  + d 2 . Multiplying by n yields dn ≥ cn +  c − 1 n + dn 2 , and then both multiplying and dividing the middle term by 2 gives dn ≥ cn + 2 c − 1 n 2 + dn 2 . Using inequalities  2  and  3  above, we get dn ≥ cn + 2 c − 1  lg n + 2d , which is what we needed to show. Thus T  n  =  cid:3  n lg n . Since T  n  = O n lg n  and T  n  =  cid:3  n lg n , we conclude that T  n  =  cid:2  n lg n .    Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  The hiring problem  [This chapter introduces probabilistic analysis and randomized algorithms. It as- sumesthatthestudentisfamiliarwiththebasicprobabilitymaterialinAppendixC. Theprimarygoalsofthesenotesareto   explain the difference between probabilistic analysis and randomized algo-    presentthetechniqueofindicatorrandomvariables,and   giveanotherexampleoftheanalysisofarandomizedalgorithm permutingan  rithms,  arrayinplace .  Thesenotesomitthetechniqueofpermutinganarraybysorting,andtheyomitthe starredSection5.4.]  Scenario:   You are using an employment agency to hire a new of cid:222 ce assistant.   The agency sends you one candidate each day.   You interview the candidate and must immediately decide whether or not to hire that person. But if you hire, you must also  cid:222 re your current of cid:222 ce assis- tant cid:151 even if it cid:146 s someone you have recently hired.    Cost to interview is ci per candidate  interview fee paid to agency .   Cost to hire is ch per candidate  includes cost to  cid:222 re current of cid:222 ce assistant +  hiring fee paid to agency .    Assume that ch > ci .   You are committed to having hired, at all times, the best candidate seen so far. Meaning that whenever you interview a candidate who is better than your current of cid:222 ce assistant, you must  cid:222 re the current of cid:222 ce assistant and hire the candidate. Since you must have someone hired at all times, you will always hire the  cid:222 rst candidate that you interview.  Goal: Determine what the price of this strategy will be.   5-2  Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  Pseudocode to model this scenario: Assumes that the candidates are numbered 1 to n and that after interviewing each candidate, we can determine if it cid:146 s better than the current of cid:222 ce assistant. Uses a dummy candidate 0 that is worse than all others, so that the  cid:222 rst candidate is always hired.  HIRE-ASSISTANT n  best ← 0 for i ← 1 to n  do interview candidate i   cid:1  candidate 0 is a least-quali cid:222 ed dummy candidate  if candidate i is better than candidate best then best ← i  hire candidate i  Cost: If n candidates, and we hire m of them, the cost is O nci + mch  .   Have to pay nci to interview, no matter how many we hire.   So we focus on analyzing the hiring cost mch.   mch varies with each run cid:151 it depends on the order in which we interview the  candidates.    This is a model of a common paradigm: we need to  cid:222 nd the maximum or minimum in a sequence by examining each element and maintaining a current  cid:147 winner. cid:148  The variable m denotes how many times we change our notion of which element is currently winning.  Worst-case analysis  In the worst case, we hire all n candidates. This happens if each one is better than all who came before. In other words, if the candidates appear in increasing order of quality. If we hire all n, then the cost is O nci + nch  = O nch    since ch > ci  .  Probabilistic analysis  In general, we have no control over the order in which candidates appear. We could assume that they come in a random order:   Assign a rank to each candidate: rank i   is a unique integer in the range 1 to n.   The ordered list  cid:2 rank 1 , rank 2 , . . . , rank n  cid:3  is a permutation of the candi-  date numbers  cid:2 1, 2, . . . , n cid:3 .    The list of ranks is equally likely to be any one of the n! permutations.   Equivalently, the ranks form a uniform random permutation: each of the pos-  sible n! permutations appears with equal probability.  Essential idea of probabilistic analysis: We must use knowledge of, or make as- sumptions about, the distribution of inputs.   The expectation is over this distribution.   This technique requires that we can make a reasonable characterization of the  input distribution.   Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-3  Randomized algorithms  We might not know the distribution of inputs, or we might not be able to model it computationally. Instead, we use randomization within the algorithm in order to impose a distribu- tion on the inputs.  For the hiring problem: Change the scenario:   The employment agency sends us a list of all n candidates in advance.   On each day, we randomly choose a candidate from the list to interview  but  considering only those we have not yet interviewed . Instead of relying on the candidates being presented to us in a random order, we take control of the process and enforce a random order.  What makes an algorithm randomized: An algorithm is randomized if its behav- ior is determined in part by values produced by a random-number generator.   RANDOM a, b  returns an integer r, where a ≤ r ≤ b and each of the b− a + 1  possible values of r is equally likely. In practice, RANDOM is implemented by a pseudorandom-number generator, which is a deterministic method returning numbers that  cid:147 look cid:148  random and pass statistical tests.        A simple yet powerful technique for computing the expected value of a random variable. Helpful in situations in which there may be dependence. Given a sample space and an event A, we de cid:222 ne the indicator random variable I{A} =  1 if A occurs , 0 if A does not occur .   cid:9   Lemma For an event A, let X A = I{A}. Then E [X A] = Pr{A}. Proof Letting A be the complement of A, we have E [X A] = E [I{A}] = 1 · Pr{A} + 0 · Pr{A} = Pr{A} .   de cid:222 nition of expected value    lemma   Indicator random variables   5-4  Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  Simple example: Determine the expected number of heads when we  cid:223 ip a fair coin one time.   Sample space is {H, T}.   Pr{H} = Pr{T} = 1 2.   De cid:222 ne indicator random variable XH = I{H}. X H counts the number of heads   Since Pr{H} = 1 2, lemma says that E [X H ] = 1 2. Slightly more complicated example: Determine the expected number of heads in n coin  cid:223 ips.   Let X be a random variable for the number of heads in n  cid:223 ips.  in one  cid:223 ip.  k=0 k·Pr{X = k}. In fact, this is what the book does  n  in equation  C.36 . Instead, we cid:146 ll use indicator random variables.    For i = 1, 2, . . . , n, de cid:222 ne Xi = I{the ith  cid:223 ip results in event H}.  cid:1    Lemma says that E [Xi] = Pr{H} = 1 2 for i = 1, 2, . . . , n.   Expected number of heads is E[X] = E [   Problem: We want E [  n i=1 Xi].  n i=1 Xi.   cid:1   n i=1 Xi ]. We have only the individual expectations E [X1] , E [X2] , . . . , E [Xn].    Solution: Linearity of expectation says that the expectation of the sum equals    Could compute E [X] = cid:1    Then X = cid:1      the sum of the expectations. Thus, E [X] = E  Xi   cid:14    cid:13  n cid:2  = n cid:2  = n cid:2   i=1  i=1  1 2 i=1 = n 2 .  E [Xi]    Linearity of expectation applies even when there is dependence among the ran- dom variables. [Not an issue in this example, but it can be a great help. The hat-checkproblemofExercise5.2-4isaproblemwithlotsofdependence. See thesolutiononpage5-10ofthismanual.]  Analysis of the hiring problem  Assume that the candidates arrive in a random order. Let X be a random variable that equals the number of times we hire a new of cid:222 ce assistant. De cid:222 ne indicator random variables X1, X2, . . . , Xn, where Xi = I{candidate i is hired} .   Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-5  Useful properties:   X = X1 + X2 + ··· + Xn.   Lemma ⇒ E [Xi ] = Pr{candidate i is hired}. We need to compute Pr{candidate i is hired}.   Candidate i is hired if and only if candidate i is better than each of candidates 1, 2, . . . , i − 1.   Assumption that the candidates arrive in random order ⇒ candidates 1, 2, . . . , i arrive in random order ⇒ any one of these  cid:222 rst i candidates is equally likely to be the best one so far.    Thus, Pr{candidate i is the best so far} = 1 i.   Which implies E [Xi] = 1 i. Now compute E [X]: E [X] = E   cid:14   Xi   cid:13  n cid:2  = n cid:2  = n cid:2   i=1  i=1  1 i  E [Xi ]  i=1  = ln n + O 1    equation  A.7 : the sum is a harmonic series  .  Thus, the expected hiring cost is O ch ln n , which is much better than the worst- case cost of O nch .  Randomized algorithms  Instead of assuming a distribution of the inputs, we impose a distribution.  The hiring problem  For the hiring problem, the algorithm is deterministic:   For any given input, the number of times we hire a new of cid:222 ce assistant will  always be the same.    The number of times we hire a new of cid:222 ce assistant depends only on the input.   In fact, it depends only on the ordering of the candidates cid:146  ranks that it is given.   Some rank orderings will always produce a high hiring cost. Example:  cid:2 1, 2, 3, 4, 5, 6 cid:3 , where each candidate is hired.    Some will always produce a low hiring cost. Example: any ordering in which the best candidate is the  cid:222 rst one interviewed. Then only the best candidate is hired.    Some may be in between.   5-6  Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  Instead of always interviewing the candidates in the order presented, what if we  cid:222 rst randomly permuted this order?   The randomization is now in the algorithm, not in the input distribution.   Given a particular input, we can no longer say what its hiring cost will be. Each     time we run the algorithm, we can get a different hiring cost. In other words, each time we run the algorithm, the execution depends on the random choices made.    No particular input always elicits worst-case behavior.   Bad behavior occurs only if we get  cid:147 unlucky cid:148  numbers from the random-  number generator.  Pseudocode for randomized hiring problem:  RANDOMIZED-HIRE-ASSISTANT n  randomly permute the list of candidates HIRE-ASSISTANT  n   Lemma The expected hiring cost of RANDOMIZED-HIRE-ASSISTANT is O ch ln n .  Proof After permuting the input array, we have a situation identical to the proba- bilistic analysis of deterministic HIRE-ASSISTANT.  Randomly permuting an array  [Thebookconsiderstwomethodsofrandomlypermutingann-elementarray. The  cid:222 rstmethodassignsarandompriorityintherange1ton3 toeachpositionandthen reorders the array elements into increasing priority order. We omit this method from these notes. The second method is better: it works in place  unlike the priority-basedmethod ,itrunsinlineartimewithoutrequiringsorting,anditneeds fewer random bits  n random numbers in therange 1to n rather than the range 1 to n3 . Wepresentandanalyzethesecondmethodinthesenotes.]  Goal: Produce a uniform random permutation  each of the n! permutations is equally likely to be produced . Non-goal: Show that for each element A[i], the probability that A[i] moves to position j is 1 n.  See Exercise 5.3-4, whose solution is on page 5-13 of this manual.  The following procedure permutes the array A[1 . . n] in place  i.e., no auxiliary array is required .  RANDOMIZE-IN-PLACE A, n  for i ← 1 to n  do swap A[i] ↔ A[RANDOM i, n ]   Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-7  In iteration i, choose A[i] randomly from A[i . . n].  Idea:     Will never alter A[i] after iteration i. Time: O 1  per iteration ⇒ O n  total.  Correctness: Given a set of n elements, a k-permutation is a sequence containing k of the n elements. There are n!  n − k ! possible k-permutations.  Lemma RANDOMIZE-IN-PLACE computes a uniform random permutation.  Proof Use a loop invariant:  Loop invariant: Just prior to the ith iteration of the for loop, for each possible  i − 1 -permutation, subarray A[1 . . i − 1] contains this  i − 1 - permutation with probability  n − i + 1 ! n!.  Initialization: Just before  cid:222 rst iteration, i = 1. Loop invariant says that for each possible 0-permutation, subarray A[1 . . 0] contains this 0-permutation with probability n! n! = 1. A[1 . . 0] is an empty subarray, and a 0-permutation has no elements. So, A[1 . . 0] contains any 0-permutation with probability 1. Maintenance: Assume that just prior to the ith iteration, each possible  i − 1 - permutation appears in A[1 . . i − 1] with probability  n− i + 1 ! n!. Will show that after the ith iteration, each possible i-permutation appears in A[1 . . i] with probability  n − i  ! n!. Incrementing i for the next iteration then maintains the invariant. Consider a particular i-permutation π =  cid:2 x1, x2, . . . , xi cid:3 .  i − 1 -permutation π cid:4  =  cid:2 x1, x2, . . . , xi−1 cid:3 , followed by xi . Let E1 be the event that the algorithm actually puts π cid:4  loop invariant, Pr{E1} =  n − i + 1 ! n!. Let E2 be the event that the ith iteration puts xi into A[i]. We get the i-permutation π in A[1 . . i] if and only if both E1 and E2 occur ⇒ the probability that the algorithm produces π in A[1 . . i] is Pr{E2 ∩ E1}. Equation  C.14  ⇒ Pr{E2 ∩ E1} = Pr{E2  E1} Pr{E1}. The algorithm chooses xi randomly from the n − i + 1 possibilities in A[i . . n] ⇒ Pr{E2  E1} = 1  n − i + 1 . Thus, Pr{E2 ∩ E1} = Pr{E2  E1} Pr{E1}  It consists of an into A[1 . . i − 1]. By the  ·  n − i + 1 !  n!  1  = n − i + 1 =  n − i  !  .  n!  Termination: At termination, i = n + 1, so we conclude that A[1 . . n] is a given  lemma   n-permutation with probability  n − n ! n! = 1 n!.   Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  Solution to Exercise 5.1-3  To get an unbiased random bit, given only calls to BIASED-RANDOM, call BIASED-RANDOM twice. Repeatedly do so until the two calls return different values, and when this occurs, return the  cid:222 rst of the two bits:  UNBIASED-RANDOM while TRUE  do  x ← BIASED-RANDOM y ← BIASED-RANDOM if x  cid:19 = y then return x  To see that UNBIASED-RANDOM returns 0 and 1 each with probability 1 2, ob- serve that the probability that a given iteration returns 0 is Pr{x = 0 and y = 1} =  1 − p  p , and the probability that a given iteration returns 1 is Pr{x = 1 and y = 0} = p 1 − p  .  We rely on the bits returned by BIASED-RANDOM being independent.  Thus, the probability that a given iteration returns 0 equals the probability that it returns 1. Since there is no other way for UNBIASED-RANDOM to return a value, it returns 0 and 1 each with probability 1 2. Assuming that each iteration takes O 1  time, the expected running time of UNBIASED-RANDOM is linear in the expected number of iterations. We can view each iteration as a Bernoulli trial, where  cid:147 success cid:148  means that the iteration returns a value. The probability of success equals the probability that 0 is returned plus the probability that 1 is returned, or 2p 1 − p . The number of trials until a success occurs is given by the geometric distribution, and by equation  C.31 , the expected number of trials for this scenario is 1  2 p 1 − p  . Thus, the expected running time of UNBIASED-RANDOM is  cid:2  1  2 p 1 − p  .   Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-9  Solution to Exercise 5.2-1  Since HIRE-ASSISTANT always hires candidate 1, it hires exactly once if and only if no candidates other than candidate 1 are hired. This event occurs when candi- date 1 is the best candidate of the n, which occurs with probability 1 n. HIRE-ASSISTANT hires n times if each candidate is better than all those who were interviewed  and hired  before. This event occurs precisely when the list of ranks given to the algorithm is  cid:2 1, 2, . . . , n cid:3 , which occurs with probability 1 n!.  Solution to Exercise 5.2-2  We make three observations:  1. Candidate 1 is always hired. 2. The best candidate, i.e., the one whose rank is n, is always hired. 3. If the best candidate is candidate 1, then that is the only candidate hired. Therefore, in order for HIRE-ASSISTANT to hire exactly twice, candidate 1 must have rank i ≤ n− 1 and all candidates whose ranks are i + 1, i + 2, . . . , n− 1 must be interviewed after the candidate whose rank is n.  When i = n − 1, this second condition vacuously holds.  Let Ei be the event in which candidate 1 has rank i; clearly, Pr{Ei} = 1 n for any given value of i. Letting j denote the position in the interview order of the best candidate, let F be the event in which candidates 2, 3, . . . , j − 1 have ranks strictly less than the rank of candidate 1. Given that event Ei has occurred, event F occurs when the best candidate is the  cid:222 rst one interviewed out of the n − i candidates whose ranks are i + 1, i + 2, . . . , n. Thus, Pr{F  Ei} = 1  n − i  . Our  cid:222 nal event is A, which occurs when HIRE-ASSISTANT hires exactly twice. Noting that the events E1, E2, . . . , En are disjoint, we have A = F ∩  E1 ∪ E2 ∪ ··· ∪ En−1   =  F ∩ E1  ∪  F ∩ E2  ∪ ··· ∪  F ∩ En−1  .  and  Pr{A} = n−1 cid:2   i=1  Pr{F ∩ Ei} .  By equation  C.14 , Pr{F ∩ Ei} = Pr{F  Ei} Pr{Ei}  =  1 n − i  · 1 n  ,   5-10  Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  · 1 n  and so  Pr{A} = n−1 cid:2  1 n−1 cid:2  n − i  cid:5  1 n − i i=1 1 n − 1 · Hn−1 ,  i=1 = 1 n = 1 n = 1 n   cid:6   + 1 n − 2  + ··· + 1 1  where Hn−1 is the nth harmonic number.  Solution to Exercise 5.2-4  Another way to think of the hat-check problem is that we want to determine the  A  cid:222 xed point of a expected number of  cid:222 xed points in a random permutation. permutation π is a value i for which π i   = i.  One could enumerate all n! permutations, count the total number of  cid:222 xed points, and divide by n! to determine the average number of  cid:222 xed points per permutation. This would be a painstaking process, and the answer would turn out to be 1. We can use indicator random variables, however, to arrive at the same answer much more easily. De cid:222 ne a random variable X that equals the number of customers that get back their own hat, so that we want to compute E[X]. For i = 1, 2, . . . , n, de cid:222 ne the indicator random variable Xi = I{customer i gets back his own hat} . Then X = X1 + X2 + ··· + Xn. Since the ordering of hats is random, each customer has a probability of 1 n of get- ting back his own hat. In other words, Pr{Xi = 1} = 1 n, which, by Lemma 5.1, implies that E [Xi] = 1 n.  cid:14  Thus, E [X] = E  Xi  E [Xi ]   linearity of expectation    cid:13  n cid:2  = n cid:2  = n cid:2   i=1  i=1  1 n  i=1 = 1 ,  and so we expect that exactly 1 customer gets back his own hat. Note that this is a situation in which the indicator random variables are not inde- pendent. For example, if n = 2 and X1 = 1, then X2 must also equal 1. Con- versely, if n = 2 and X1 = 0, then X2 must also equal 0. Despite the dependence,   Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-11  Pr{Xi = 1} = 1 n for all i, and linearity of expectation holds. Thus, we can use the technique of indicator random variables even in the presence of dependence.  Solution to Exercise 5.2-5  Let Xi j be an indicator random variable for the event where the pair A[i], A[ j] for i   A[ j]. More precisely, we de cid:222 ne Xi j = I{A[i] > A[ j]} for 1 ≤ i < j ≤ n. We have Pr{Xi j = 1} = 1 2, because given two distinct random numbers, the probability that the  cid:222 rst is bigger than the second is 1 2. By Lemma 5.1, E [Xi j ] = 1 2. Let X be the the random variable denoting the total number of inverted pairs in the array, so that  X = n−1 cid:2   n cid:2   i=1  j=i+1  Xi j .  We want the expected number of inverted pairs, so we take the expectation of both sides of the above equation to obtain E [X] = E  .  Xi j  We use linearity of expectation to get E [X] = E  Xi j   cid:14    cid:14   E [Xi j ]  j=i+1  i=1  i=1  j=i+1   cid:13  n cid:2  n−1 cid:2   cid:13  n−1 cid:2  n cid:2  = n−1 cid:2  n cid:2  n cid:2  = n−1 cid:2   cid:6   cid:5  = = n n − 1  = n n − 1   j=i+1 1 2  i=1 n 2  j=i+1  i=1  2  .  · 1 2  1 2  Thus the expected number of inverted pairs is n n − 1  4.  4  Solution to Exercise 5.3-1  Here cid:146 s the rewritten procedure:   5-12  Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  RANDOMIZE-IN-PLACE A  n ← length[A] swap A[1] ↔ A[RANDOM 1, n ] for i ← 2 to n  do swap A[i] ↔ A[RANDOM i, n ]  The loop invariant becomes  Loop invariant: Just prior to the iteration of the for loop for each value of i = 2, . . . , n, for each possible  i−1 -permutation, the subarray A[1 . . i−1] contains this  i − 1 -permutation with probability  n − i + 1 ! n!.  The maintenance and termination parts remain the same. The initialization part is for the subarray A[1 . . 1], which contains any 1-permutation with probability  n − 1 ! n! = 1 n.  Solution to Exercise 5.3-2  Although PERMUTE-WITHOUT-IDENTITY will not produce the identity permuta- tion, there are other permutations that it fails to produce. For example, consider its operation when n = 3, when it should be able to produce the n! − 1 = 5 non- identity permutations. The for loop iterates for i = 1 and i = 2. When i = 1, the call to RANDOM returns one of two possible values  either 2 or 3 , and when i = 2, the call to RANDOM returns just one value  3 . Thus, there are only 2 · 1 = 2 pos- sible permutations that PERMUTE-WITHOUT-IDENTITY can produce, rather than the 5 that are required.  Solution to Exercise 5.3-3  The PERMUTE-WITH-ALL procedure does not produce a uniform random per- mutation. Consider the permutations it produces when n = 3. There are 3 calls to RANDOM, each of which returns one of 3 values, and so there are 27 possible outcomes of calling PERMUTE-WITH-ALL. Since there are 3! = 6 permutations, if PERMUTE-WITH-ALL did produce a uniform random permutation, then each permutation would occur 1 6 of the time. That would mean that each permutation would have to occur an integer number m times, where m 27 = 1 6. No integer m satis cid:222 es this condition. In fact, if we were to work out the possible permutations of  cid:2 1, 2, 3 cid:3  and how often they occur with PERMUTE-WITH-ALL, we would get the following probabilities: permutation  cid:2 1, 2, 3 cid:3   cid:2 1, 3, 2 cid:3   cid:2 2, 1, 3 cid:3   cid:2 2, 3, 1 cid:3   cid:2 3, 1, 2 cid:3   cid:2 3, 2, 1 cid:3   4 27 5 27 5 27 5 27 4 27 4 27  probability   Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-13  Although these probabilities add to 1, none are equal to 1 6.  Solution to Exercise 5.3-4  PERMUTE-BY-CYCLIC chooses offset as a random integer in the range 1 ≤ offset ≤ n, and then it performs a cyclic rotation of the array. That is, B[  i + offset −1  mod n  + 1] ← A[i] for i = 1, 2, . . . , n.  The subtraction and addition of 1 in the index calculation is due to the 1-origin indexing. If we had used 0-origin indexing instead, the index calculation would have simplied to B[ i + offset  mod n] ← A[i] for i = 0, 1, . . . , n − 1.  Thus, once offset is determined, so is the entire permutation. Since each value of offset occurs with probability 1 n, each element A[i] has a probability of ending up in position B[ j] with probability 1 n. This procedure does not produce a uniform random permutation, however, since it can produce only n different permutations. Thus, n permutations occur with probability 1 n, and the remaining n! − n permutations occur with probability 0.  Solution to Exercise 5.4-6  First we determine the expected number of empty bins. We de cid:222 ne a random vari- able X to be the number of empty bins, so that we want to compute E[X]. Next, for i = 1, 2, . . . , n, we de cid:222 ne the indicator random variable Yi = I{bin i is empty}. Thus,  X = n cid:2   i=1  and so E [X] = E   cid:14   Yi  Yi ,   cid:13  n cid:2  = n cid:2  = n cid:2   i=1  i=1  i=1   by linearity of expectation   E [Yi] Pr{bin i is empty}   by Lemma 5.1  .  Let us focus on a speci cid:222 c bin, say bin i. We view a toss as a success if it misses bin i and as a failure if it lands in bin i. We have n independent Bernoulli trials, each with probability of success 1 − 1 n. In order for bin i to be empty, we need n successes in n trials. Using a binomial distribution, therefore, we have that Pr{bin i is empty} = =   cid:6  cid:5   cid:6  1 − 1 n   cid:5    cid:6    cid:6   1 n  0  n  n  .   cid:5   cid:5  n n 1 − 1 n   5-14  Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms   cid:6   Thus,   cid:5  E [X] = n cid:2   cid:6   cid:5  1 − 1 n 1 − 1 n n  i=1 = n  n  .  E [X] = n cid:2   i=1  By equation  3.13 , as n approaches ∞, the quantity  1 − 1 n n approaches 1 e, and so E [X] approaches n e. Now we determine the expected number of bins with exactly one ball. We re- de cid:222 ne X to be number of bins with exactly one ball, and we rede cid:222 ne Yi to be I{bin i gets exactly one ball}. As before, we  cid:222 nd that  Again focusing on bin i, we need exactly n−1 successes in n independent Bernoulli trials, and so Pr{bin i gets exactly one ball} =   cid:6    cid:6    cid:5   n−1  1  1 n  Pr{bin i gets exactly one ball} .  cid:6  cid:5   cid:6  1 − 1 n n−1 1 n   cid:5   cid:5  n n − 1  cid:5  = n · 1 − 1 = n   cid:6  1 − 1 n n−1  ,   cid:6   and so   cid:5   cid:6  1 − 1 n n−1  E [X] = n cid:2   cid:5  i=1 = n 1 − 1  cid:6   cid:5  n Because n−1 = n  1 − 1 1 − 1 1 − 1 n  n  n−1  .   n  ,  n as n approaches ∞, we  cid:222 nd that E[X] approaches n e 1 − 1 n  e n − 1   =  n2  n  .  Solution to Problem 5-1  a. To determine the expected value represented by the counter after n INCREMENT operations, we de cid:222 ne some random variables:   For j = 1, 2, . . . , n, let X j denote the increase in the value represented by the counter due to the jth INCREMENT operation.   Let Vn be the value represented by the counter after n INCREMENT opera-  tions.   Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-15  Then Vn = X1 + X2 + ··· + Xn. We want to compute E [Vn]. By linearity of expectation, E [Vn] = E [X1 + X2 + ··· + Xn] = E [X1] + E [X2] + ··· + E [Xn] . We shall show that E [X j ] = 1 for j = 1, 2, . . . , n, which will prove that E [Vn] = n. We actually show that E [X j ] = 1 in two ways, the second more rigorous than the  cid:222 rst:  1. Suppose that at the start of the jth INCREMENT operation, the counter holds the value i, which represents ni. If the counter increases due to this INCRE- MENT operation, then the value it represents increases by ni+1 − ni. The counter increases with probability 1  ni+1 − ni  , and so E [X j ] =  0 · Pr{counter does not increase}   cid:5  +   ni+1 − ni   · Pr{counter increases}  1 −   cid:5   ni+1 − ni   ·   cid:6  cid:6    cid:6   +  1  1  ni+1 − ni  ni+1 − ni   cid:5  0 · = = 1 ,  and so E [X j ] = 1 regardless of the value held by the counter.  2. Let C j be the random variable denoting the value held in the counter at the start of the jth INCREMENT operation. Since we can ignore values of Cj greater than 2b − 1, we use a formula for conditional expectation: E [X j ] = E [E [X j  C j ]]  = 2b−1 cid:2   i=0  E [X j  C j = i] · Pr{C j = i} .  To compute E [X j  C j = i], we note that   Pr{X j = 0  C j = i} = 1 − 1  ni+1 − ni  ,   Pr{X j = ni+1 − ni  C j = i} = 1  ni+1 − ni  , and   Pr{X j = k  C j = i} = 0 for all other k.  cid:2  Thus,  cid:6  cid:6   cid:5   cid:5   cid:5  k · Pr{X j = k  C j = i} E [X j  C j = i] =  ni+1 − ni   · + = 1 − 0 · = 1 . Therefore, noting that Pr{C j = i} = 1 ,  ni+1 − ni  1  k  2b−1 cid:2  E [X j ] = 2b−1 cid:2   i=0 we have  i=0 = 1 .  1 · Pr{C j = i}   cid:6   1  ni+1 − ni   5-16  Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  Why is the second way more rigorous than the  cid:222 rst? Both ways condition on the value held in the counter, but only the second way incorporates the conditioning into the expression for E[X j ]. b. De cid:222 ning Vn and X j as in part  a , we want to compute Var[Vn], where ni = 100i. The X j are pairwise independent, and so by equation  C.28 , Var[Vn] = Var [X1] + Var [X2] + ··· + Var [Xn]. Since ni = 100i, we see that ni+1 − ni = 100 i + 1 − 100i = 100. Therefore, with probability 99 100, the increase in the value represented by the counter  cid:15  due to the jth INCREMENT operation is 0, and with probability 1 100, the  cid:5  cid:5  value represented increases by 100. Thus, by equation  C.26 , Var [X j ] = E = = 100 − 1 = 99 .   cid:16  − E2 [X j ]  cid:6   cid:5   X 2 j 02 · 99 100  1002 · 1 100   cid:6  cid:6   − 12  +  Summing up the variances of the X j gives Var [Vn] = 99n.   Lecture Notes for Chapter 6: Heapsort  Chapter 6 overview  Heapsort    O n lg n  worst case cid:151 like merge sort.   Sorts in place cid:151 like insertion sort.   Combines the best of both algorithms.  Heaps  Heap data structure  To understand heapsort, we cid:146 ll cover heaps and heap operations, and then we cid:146 ll take a look at priority queues.    Heap A  not garbage-collected storage  is a nearly complete binary tree.    Height of node =  of edges on a longest simple path from the node down to   Height of heap = height of root =  cid:2  lg n .  a leaf.    A heap can be stored as an array A.    Root of tree is A[1].   Parent of A[i] = A[ cid:9 i  2 cid:10 ].   Left child of A[i] = A[2i].   Right child of A[i] = A[2i + 1].   Computing is fast with binary representation implementation.  [Inbook,havelength andheap-size attributes. Here,webypasstheseattributesand useparametervaluesinstead.]   6-2  Lecture Notes for Chapter 6: Heapsort  Example: of a max-heap. [Arcsaboveandbelowthearrayontherightgobetween parentsandchildren. Thereisnosigni cid:222 cancetowhetheranarcisdrawnaboveor belowthearray.]  4 8  5 7  6 9  7 3  1  2  3  4  16 14 10 8  5  7  6  9  7  3  8  2  9  4  10  1  1 16  2 14  3 10  8 2  9 4  10 1  Heap property    For max-heaps  largest element at root , max-heap property: for all nodes i,  excluding the root, A[PARENT i  ] ≥ A[i]. excluding the root, A[PARENT i  ] ≤ A[i].    For min-heaps  smallest element at root , min-heap property: for all nodes i,  By induction and transitivity of ≤, the max-heap property guarantees that the max- imum element of a max-heap is at the root. Similar argument for min-heaps. The heapsort algorithm we cid:146 ll show uses max-heaps. Note: In general, heaps can be k-ary tree instead of binary.  Maintaining the heap property  MAX-HEAPIFY is important for manipulating max-heaps. It is used to maintain the max-heap property.   Before MAX-HEAPIFY, A[i] may be smaller than its children.   Assume left and right subtrees of i are max-heaps.   After MAX-HEAPIFY, subtree rooted at i is a max-heap.  MAX-HEAPIFY A, i, n  l ← LEFT i   r ← RIGHT i   if l ≤ n and A[l] > A[i] then largest ← l else largest ← i if r ≤ n and A[r] > A[largest] then largest ← r if largest  cid:19 = i then exchange A[i] ↔ A[largest] MAX-HEAPIFY A, largest, n    Lecture Notes for Chapter 6: Heapsort  6-3  [Parameter n replacesattribute heap-size[A].] The way MAX-HEAPIFY works:   Compare A[i], A[LEFT i  ], and A[RIGHT i  ].    If necessary, swap A[i] with the larger of the two children to preserve heap property.    Continue this process of comparing and swapping down the heap, until subtree rooted at i is max-heap. If we hit a leaf, then the subtree rooted at the leaf is trivially a max-heap.  Run MAX-HEAPIFY on the following heap example.  3 10  5 7  6 9  7 3  2 14  4 4  i 8 2  9 8  10 1  1 16   b   4 14  i  9 8  10 1  2 4  2 14  4 8  i  9 4  10 1  8 2  8 2  1 16   a   1 16   c   5 7  5 7  6 9  6 9  3 10  3 10  7 3  7 3    Node 2 violates the max-heap property.   Compare node 2 with its children, and then swap it with the larger of the two    Continue down the tree, swapping until the value is properly placed at the root  of a subtree that is a max-heap. In this case, the max-heap is a leaf.  children.  Time: O lg n .  Correctness: [Instead of book cid:146 s formal analysis with recurrence, just come up with O lg n  intuitively.] Heap is almost-complete binary tree, hence must pro- cess O lg n  levels, with constant work at each level  comparing 3 items and maybe swapping 2 .  Building a heap  The following procedure, given an unordered array, will produce a max-heap.   6-4  Lecture Notes for Chapter 6: Heapsort  BUILD-MAX-HEAP A, n  for i ←  cid:9 n 2 cid:10  downto 1  do MAX-HEAPIFY A, i, n   [Parameter n replacesbothattributes length[A] and heap-size[A].]  Example: Building a max-heap from the following unsorted array results in the  cid:222 rst heap example.     MAX-HEAPIFY is applied to subtrees rooted at nodes  in order : 16, 2, 3, 1, 4.  i starts off as 5.  1 4  2 1  3 3  4 2  6  7  5 9 16 9 10 14 8  8  10 7  A  1 4  3 3  1 16  2 14  3 10  5 16  6 9  7 10  4 8  5 7  6 9  7 3  8 2  9 4  10 1  2 1  i 10 7  4 2  8 14  9 8  Correctness  Loop invariant: At start of every iteration of for loop, each node i + 1, i + 2, . . . , n is root of a max-heap. Initialization: By Exercise 6.1-7, we know that each node cid:9 n 2 cid:10  + 1,  cid:9 n 2 cid:10  + 2, . . . , n is a leaf, which is the root of a trivial max-heap. Since i = cid:9 n 2 cid:10  before the  cid:222 rst iteration of the for loop, the invariant is initially true. Maintenance: Children of node i are indexed higher than i, so by the loop invari- ant, they are both roots of max-heaps. Correctly assuming that i+1, i+2, . . . , n are all roots of max-heaps, MAX-HEAPIFY makes node i a max-heap root. Decrementing i reestablishes the loop invariant at each iteration. Termination: When i = 0, the loop terminates. By the loop invariant, each node,  notably node 1, is the root of a max-heap.  Analysis    Simple bound: O n  calls to MAX-HEAPIFY, each of which takes O lg n  time ⇒ O n lg n .  Note: A good approach to analysis in general is to start by proving easy bound, then try to tighten it.     Tighter analysis: Observation: Time to run MAX-HEAPIFY is linear in the height of the node it cid:146 s run on, and most nodes have small heights. Have ≤  cid:14 n 2h+1 cid:15  nodes of height h  see Exercise 6.3-3 , and height of heap is cid:9 lg n cid:10   Exercise 6.1-2 .   The heapsort algorithm  Lecture Notes for Chapter 6: Heapsort  6-5   cid:3    cid:4    cid:18   n 2h+1   cid:9 lg n cid:10  cid:2   O h  = O  The time required by MAX-HEAPIFY when called on a node of height h is O h , so the total cost of BUILD-MAX-HEAP is   cid:17   cid:9 lg n cid:10  cid:2   cid:1 ∞ h=0 Evaluate the last summation by substituting x = 1 2 in the formula  A.8  ∞ cid:2   h 2h  h=0  n     .  k=0 kx k  , which yields h  1 − 1 2 2 2h  1 2  = = 2 .  h=0  Thus, the running time of BUILD-MAX-HEAP is O n . Building a min-heap from an unordered array can be done by calling MIN- HEAPIFY instead of MAX-HEAPIFY, also taking linear time.  Given an input array, the heapsort algorithm acts as follows:   Builds a max-heap from the array.   Starting with the root  the maximum element , the algorithm places the maxi- mum element into the correct place in the array by swapping it with the element in the last position in the array.  cid:147 Discard cid:148  this last node  knowing that it is in its correct place  by decreasing the heap size, and calling MAX-HEAPIFY on the new  possibly incorrectly-placed  root.    Repeat this  cid:147 discarding cid:148  process until only one node  the smallest element      remains, and therefore is in the correct place in the array.  HEAPSORT A, n  BUILD-MAX-HEAP A, n  for i ← n downto 2  do exchange A[1] ↔ A[i]  MAX-HEAPIFY A, 1, i − 1   [Parametern replaces length[A],andparametervaluei − 1 in MAX-HEAPIFY call replacesdecrementingofheap-size[A].]  Example: Sort an example heap on the board. [Nodes with heavy outline are no longerintheheap.]   6-6  Lecture Notes for Chapter 6: Heapsort  2  3  1  i  7  4   b   2  1  3  i  4  7   d   A  1 2  3 4  7  3  1  3  7   a   3   c   1   e   2  7  7  4  2  1  i  4  i  2  4  Analysis    BUILD-MAX-HEAP: O n  for loop: n − 1 times     exchange elements: O 1    MAX-HEAPIFY: O lg n   Total time: O n lg n . Though heapsort is a great algorithm, a well-implemented quicksort usually beats it in practice.  Heap implementation of priority queue  Heaps ef cid:222 ciently implement priority queues. These notes will deal with max- priority queues implemented with max-heaps. Min-priority queues are imple- mented with min-heaps similarly. A heap gives a good compromise between fast insertion but slow extraction and vice versa. Both operations take O lg n  time.  Priority queue    Maintains a dynamic set S of elements.   Each set element has a key cid:151 an associated value.   Max-priority queue supports dynamic-set operations:  INSERT S, x : inserts element x into set S.      MAXIMUM S : returns element of S with largest key.   Lecture Notes for Chapter 6: Heapsort  6-7    EXTRACT-MAX S : removes and returns element of S with largest key.    INCREASE-KEY S, x, k : increases value of element x cid:146 s key to k. Assume k ≥ x cid:146 s current key value.    Example max-priority queue application: schedule jobs on shared computer.   Min-priority queue supports similar operations: INSERT S, x : inserts element x into set S.      MINIMUM S : returns element of S with smallest key.   EXTRACT-MIN S : removes and returns element of S with smallest key.   DECREASE-KEY S, x, k : decreases value of element x cid:146 s key to k. Assume  k ≤ x cid:146 s current key value.    Example min-priority queue application: event-driven simulator.  Note: Actual implementations often have a handle in each heap element that allows access to an object in the application, and objects in the application often have a handle  likely an array index  to access the heap element. Will examine how to implement max-priority queue operations.  Finding the maximum element  Getting the maximum element is easy: it cid:146 s the root.  HEAP-MAXIMUM A  return A[1]  Time:  cid:2  1 .  Extracting max element  Given the array A:   Make sure heap is not empty.   Make a copy of the maximum element  the root .   Make the last node in the tree the new root.   Re-heapify the heap, with one fewer node.   Return the copy of the maximum element.  HEAP-EXTRACT-MAX A, n  if n < 1 then error  cid:147 heap under cid:223 ow cid:148  max ← A[1] A[1] ← A[n] MAX-HEAPIFY A, 1, n − 1   cid:1  remakes heap return max [Parametern replaces heap-size[A],andparametervalue n − 1 in MAX-HEAPIFY callreplacesdecrementingofheap-size[A].]   6-8  Lecture Notes for Chapter 6: Heapsort  Analysis: constant time assignments plus time for MAX-HEAPIFY.  Time: O lg n .  Example: Run HEAP-EXTRACT-MAX on  cid:222 rst heap example.   Take 16 out of node 1.   Move 1 from node 10 to node 1.   Erase node 10.   MAX-HEAPIFY from the root to preserve max-heap property.   Note that successive extractions will remove items in reverse sorted order.  Increasing key value  Given set S, element x, and new key value k:   Make sure k ≥ x cid:146 s current key.   Update x cid:146 s key value to k.   Traverse the tree upward comparing x to its parent and swapping keys if neces-  sary, until x cid:146 s key is smaller than its parent cid:146 s key.  HEAP-INCREASE-KEY A, i, key  if key < A[i] then error  cid:147 new key is smaller than current key cid:148  A[i] ← key while i > 1 and A[PARENT i  ] < A[i] do exchange A[i] ↔ A[PARENT i  ]  i ← PARENT i    Analysis: Upward path from node i has length O lg n  in an n-element heap.  Example: Increase key of node 9 in  cid:222 rst heap example to have value 15. Exchange keys of nodes 4 and 9, then of nodes 2 and 4.  Time: O lg n .  Inserting into the heap  Given a key k to insert into the heap:      Insert a new node in the very last position in the tree with key −∞. Increase the −∞ key to k using the HEAP-INCREASE-KEY procedure de cid:222 ned above.  MAX-HEAP-INSERT A, key, n  A[n + 1] ← −∞ HEAP-INCREASE-KEY A, n + 1, key  [Parameter n replaces heap-size[A],anduseofvalue n + 1 replacesincrementing of heap-size[A].]   Lecture Notes for Chapter 6: Heapsort  6-9  Analysis: constant time assignments + time for HEAP-INCREASE-KEY.  Time: O lg n .  Min-priority queue operations are implemented similarly with min-heaps.   Solutions for Chapter 6: Heapsort  Solution to Exercise 6.1-1  Solution to Exercise 6.1-2  Solution to Exercise 6.1-3  Since a heap is an almost-complete binary tree  complete at all levels except pos- sibly the lowest , it has at most 2h+1 − 1 elements  if it is complete  and at least 2h − 1+ 1 = 2h elements  if the lowest level has just 1 element and the other levels are complete .  Given an n-element heap of height h, we know from Exercise 6.1-1 that 2h ≤ n ≤ 2h+1 − 1 < 2h+1 . Thus, h ≤ lg n < h + 1. Since h is an integer, h = cid:9 lg n cid:10   by de cid:222 nition of  cid:9   cid:10  .  Assume the claim is false cid:151 i.e., that there is a subtree whose root is not the largest element in the subtree. Then the maximum element is somewhere else in the sub- tree, possibly even at more than one location. Let m be the index at which the maximum appears  the lowest such index if the maximum appears more than once . Since the maximum is not at the root of the subtree, node m has a parent. Since the parent of a node has a lower index than the node, and m was chosen to be the smallest index of the maximum value, A[PARENT m ] < A[m]. But by the max- heap property, we must have A[PARENT m ] ≥ A[m]. So our assumption is false, and the claim is true.  Solution to Exercise 6.2-6  If you put a value at the root that is less than every value in the left and right subtrees, then MAX-HEAPIFY will be called recursively until a leaf is reached. To   Solution to Exercise 6.3-3  Solutions for Chapter 6: Heapsort  6-11  make the recursive calls traverse the longest path to a leaf, choose values that make MAX-HEAPIFY always recurse on the left child. It follows the left branch when the left child is ≥ the right child, so putting 0 at the root and 1 at all the other nodes, for example, will accomplish that. With such values, MAX-HEAPIFY will be called h times  where h is the heap height, which is the number of edges in the longest path from the root to a leaf , so its running time will be  cid:2  h   since each call does  cid:2  1  work , which is  cid:2  lg n . Since we have a case in which MAX-HEAPIFY cid:146 s running time is  cid:2  lg n , its worst-case running time is  cid:3  lg n .  Let H be the height of the heap. Two subtleties to beware of:   Be careful not to confuse the height of a node  longest distance from a leaf      with its depth  distance from the root . If the heap is not a complete binary tree  bottom level is not full , then the nodes at a given level  depth  don cid:146 t all have the same height. For example, although all nodes at depth H have height 0, nodes at depth H − 1 can have either height 0 or height 1. For a complete binary tree, it cid:146 s easy to show that there are  cid:14 n 2h+1 cid:15  nodes of height h. But the proof for an incomplete tree is tricky and is not derived from the proof for a complete tree.  Proof By induction on h. Basis: Show that it cid:146 s true for h = 0  i.e., that  of leaves ≤ cid:14 n 2h+1 cid:15  =  cid:14 n 2 cid:15  . In fact, we cid:146 ll show that the  of leaves = cid:14 n 2 cid:15 . The tree leaves  nodes at height 0  are at depths H and H − 1. They consist of       all nodes at depth H, and the nodes at depth H − 1 that are not parents of depth-H nodes.  Let x be the number of nodes at depth H cid:151 that is, the number of nodes in the bottom  possibly incomplete  level. Note that n − x is odd, because the n − x nodes above the bottom level form a complete binary tree, and a complete binary tree has an odd number of nodes  1 less than a power of 2 . Thus if n is odd, x is even, and if n is even, x is odd. To prove the base case, we must consider separately the case in which n is even  x is odd  and the case in which n is odd  x is even . Here are two ways to do this: The  cid:222 rst requires more cleverness, and the second requires more algebraic manipulation.  1. First method of proving the base case:     If n is odd, then x is even, so all nodes have siblings cid:151 i.e., all internal nodes have 2 children. Thus  see Exercise B.5-3 ,  of internal nodes =  of leaves − 1.   6-12  Solutions for Chapter 6: Heapsort        So, n =  of nodes =  of leaves +  of internal nodes = 2·  of leaves − 1. Thus,  of leaves =  n + 1  2 =  cid:14 n 2 cid:15 .  The latter equality holds because n is odd.  If n is even, then x is odd, and some leaf doesn cid:146 t have a sibling. If we gave it a sibling, we would have n + 1 nodes, where n + 1 is odd, so the case we analyzed above would apply. Observe that we would also increase the number of leaves by 1, since we added a node to a parent that already had a child. By the odd-node case above,  of leaves + 1 =  cid:14  n + 1  2 cid:15  =  cid:14 n 2 cid:15  + 1.  The latter equality holds because n is even.   In either case,  of leaves =  cid:14 n 2 cid:15 .  2. Second method of proving the base case:  Note that at any depth d < H there are 2d nodes, because all such tree levels are complete. If x is even, there are x 2 nodes at depth H − 1 that are parents of depth H   nodes, hence 2H−1−x 2 nodes at depth H−1 that are not parents of depth-H nodes. Thus, total  of height-0 nodes = x + 2H−1 − x 2  = 2H−1 + x 2 =  2H + x  2 =  cid:14  2H + x − 1  2 cid:15  =  cid:14 n 2 cid:15  .   because x is even   n = 2H + x − 1 because the complete tree down to depth H − 1 has 2H − 1 nodes and depth H has x nodes.  If x is odd, by an argument similar to the even case, we see that  of height-0 nodes = x + 2H−1 −  x + 1  2  = 2H−1 +  x − 1  2 =  2H + x − 1  2 = n 2 =  cid:14 n 2 cid:15    because x odd ⇒ n even  .   cid:4   formed by removing the leaves of T . It has n  Inductive step: Show that if it cid:146 s true for height h − 1, it cid:146 s true for h. Let nh be the number of nodes at height h in the n-node tree T .  cid:4  = n − n0 nodes. Consider the tree T We know from the base case that n0 =  cid:14 n 2 cid:15 , so n  cid:4  = n−n0 = n− cid:14 n 2 cid:15  =  cid:9 n 2 cid:10 . Note that the nodes at height h in T would be at height h − 1 if the leaves of the tree were removed cid:151 that is, they are at height h − 1 in T  cid:4  h−1 denote the number of nodes at height h − 1 in T nh = n  cid:4  h−1: By induction, we can bound n nh = n   cid:4  2h cid:15  =  cid:14  cid:9 n 2 cid:10   2h cid:15  ≤  cid:14  n 2  2h cid:15  =  cid:14 n 2h+1 cid:15  .  . Letting n  , we have  ≤  cid:14 n   cid:4  h−1  .   cid:4    cid:4    cid:4  h−1   Solutions for Chapter 6: Heapsort  6-13  Solution to Exercise 6.4-1  13  20  13  17  13  8  7  17  2  7  4  2  8  7  4  2  5  4  i  20  25  8  i 25  5  25   a   13   d   5   g   8  4  5  2  20   b   8   e   4   h   7  2  2  7  4  i  17  2  4  13  i  17  2  i  8  13  17  20  25  20  25  20  25  5  4  i  7  8  13  17  7  8  13  17  7  8  13  17  20 25  20  25  20  25  5  i  i  4  A  2 4  5 7  8 13 17 20 25  5  5  5  17   c   7   f   2   i    5  12  8 8  7  5  12  8 8  7  4  0  6  2  1  -∞  4  0  6  2  1  5  12  7  5  12  9  7  4  0  6  2  1  8  4  0  6  2  1  8  13  13  9  i  10  i 10  15   b   15   d   6-14  Solutions for Chapter 6: Heapsort  Solution to Exercise 6.5-2  13  13  9  9  i 10  Solution to Problem 6-1  15   a   15   c   1  1  a. The procedures BUILD-MAX-HEAP and BUILD-MAX-HEAP  do not always create the same heap when run on the same input array. Consider the following counterexample. Input array A:   cid:4   A  1  2 3  BUILD-MAX-HEAP A :  2  3  BUILD-MAX-HEAP  1   cid:4  A :  3  2  2  1  A  3 2  1  3  A  3 1  2  -∞  -∞  1  2  b. An upper bound of O n lg n  time follows immediately from there being n − 1 calls to MAX-HEAP-INSERT, each taking O lg n  time. For a lower bound of   Solutions for Chapter 6: Heapsort  6-15   cid:3  n lg n , consider the case in which the input array is given in strictly increas- n cid:2  ing order. Each call to MAX-HEAP-INSERT causes HEAP-INCREASE-KEY to go all the way up to the root. Since the depth of node i is cid:9 lg i cid:10 , the total time is   cid:2   cid:9 lg i cid:10   ≥   cid:2   cid:9 lg cid:14 n 2 cid:15  cid:10    i=1  n cid:2  n cid:2  n cid:2   i= cid:14 n 2 cid:15   i= cid:14 n 2 cid:15   ≥   cid:2   cid:9 lg n 2  cid:10     cid:2   cid:9 lg n − 1 cid:10    i= cid:14 n 2 cid:15   = ≥ n 2 ·  cid:2  lg n  =  cid:3  n lg n  .  In the worst case, therefore, BUILD-MAX-HEAP build an n-element heap.   cid:4   requires  cid:2  n lg n  time to  Solution to Problem 6-2  a. A d-ary heap can be represented in a 1-dimensional array as follows. The root is kept in A[1], its d children are kept in order in A[2] through A[d + 1], their children are kept in order in A[d + 2] through A[d2 + d + 1], and so on. The following two procedures map a node with index i to its parent and to its jth child  for 1 ≤ j ≤ d , respectively.  D-ARY-PARENT i   return  cid:9  i − 2  d + 1 cid:10   D-ARY-CHILD i, j   return d i − 1  + j + 1  To convince yourself that these procedures really work, verify that D-ARY-PARENT D-ARY-CHILD i, j    = i , for any 1 ≤ j ≤ d. Notice that the binary heap procedures are a special case of the above procedures when d = 2.  cid:2  logd n  =  cid:2  lg n  lg d .  b. Since each node has d children, the height of a d-ary heap with n nodes is  c. The procedure HEAP-EXTRACT-MAX given in the text for binary heaps works  cid:222 ne for d-ary heaps too. The change needed to support d-ary heaps is in MAX- HEAPIFY, which must compare the argument node to all d children instead of just 2 children. The running time of HEAP-EXTRACT-MAX is still the running time for MAX-HEAPIFY, but that now takes worst-case time proportional to the product of the height of the heap by the number of children examined at each  node  at most d , namely  cid:2  d logd n  =  cid:2  d lg n  lg d .   6-16  Solutions for Chapter 6: Heapsort  d. The procedure MAX-HEAP-INSERT given in the text for binary heaps works  cid:222 ne for d-ary heaps too. The worst-case running time is still  cid:2  h , where h is the height of the heap.  Since only parent pointers are followed, the number of children a node has is irrelevant.  For a d-ary heap, this is  cid:2  logd n  =  cid:2  lg n  lg d .  e. D-ARY-HEAP-INCREASE-KEY can be implemented as a slight modi cid:222 cation of MAX-HEAP-INSERT  only the  cid:222 rst couple lines are different . Increas- ing an element may make it larger than its parent, in which case it must be moved higher up in the tree. This can be done just as for insertion, travers- ing a path from the increased node toward the root. In the worst case, the entire height of the tree must be traversed, so the worst-case running time is   cid:2  h  =  cid:2  logd n  =  cid:2  lg n  lg d . D-ARY-HEAP-INCREASE-KEY A, i, k  A[i] ← max A[i], k  while i > 1 and A[PARENT i  ] < A[i] do exchange A[i] ↔ A[PARENT i  ]  i ← PARENT i     Lecture Notes for Chapter 7: Quicksort  Chapter 7 overview  [The treatment in the second edition differs from that of the  cid:222 rstedition. Weuse adifferent partitioning method cid:151 known as cid:147 Lomuto partitioning cid:148  cid:151 in thesecond edition,ratherthanthe cid:147 Hoarepartitioning cid:148 usedinthe cid:222 rstedition. UsingLomuto partitioning helps simplify theanalysis, which uses indicator random variables in thesecondedition.]  Quicksort    Worst-case running time:  cid:2  n2 .   Expected running time:  cid:2  n lg n .   Constants hidden in  cid:2  n lg n  are small.   Sorts in place.  Description of quicksort  Quicksort is based on the three-step process of divide-and-conquer.   To sort the subarray A[p . . r]:  Divide: Partition A[p . . r], into two  possibly empty  subarrays A[p . . q − 1] and A[q + 1 . . r], such that each element in the  cid:222 rst subarray A[p . . q − 1] is ≤ A[q] and A[q] is ≤ each element in the second subarray A[q + 1 . . r].  Conquer: Sort the two subarrays by recursive calls to QUICKSORT. Combine: No work is needed to combine the subarrays, because they are sorted  in place.    Perform the divide step by a procedure PARTITION, which returns the index q  that marks the position separating the subarrays.   7-2  Lecture Notes for Chapter 7: Quicksort  QUICKSORT A, p, r   if p < r then q ← PARTITION A, p, r   QUICKSORT A, p, q − 1  QUICKSORT A, q + 1, r   Initial call is QUICKSORT A, 1, n .  Partitioning  Partition subarray A[ p . . r] by the following procedure:  PARTITION A, p, r   x ← A[r] i ← p − 1 for j ← p to r − 1 do if A[ j] ≤ x  then i ← i + 1 exchange A[i + 1] ↔ A[r] return i + 1  exchange A[i] ↔ A[ j]     PARTITION always selects the last element A[r] in the subarray A[p . . r] as the pivot cid:151 the element around which to partition.    As the procedure executes, the array is partitioned into four regions, some of  which may be empty:  Loop invariant: 1. All entries in A[p . . i] are ≤ pivot. 2. All entries in A[i + 1 . . j − 1] are > pivot. 3. A[r] = pivot.  It cid:146 s not needed as part of the loop invariant, but the fourth region is A[ j . . r−1], whose entries have not yet been examined, and so we don cid:146 t know how they compare to the pivot.  Example: On an 8-element subarray.   Lecture Notes for Chapter 7: Quicksort  7-3  i  i  p,j 8  p 8  p,i 1  p,i 1  p 1  p 1  p 1  p 1  p 1  1  j 1  8  8  i 4  6  4  0  3  9  6  j 6  6  4  0  3  9  0  3  9  3  9  0  j 0  6  8  3  9  i 0  4  j 3  6  4  0  6  8  9  j 9  4  0  6  8  9  4  0  5  8  9  4  j 4  8  i 3  i 3  i 3  r 5  r 5  r 5  r 5  r 5  r 5  r 5  r 5  r 6  pivot not yet examined  A[r]:  A[j .. r cid:150 1]:  A[i+1 .. j cid:150 1]:  known to be > pivot A[p .. i]:  known to be ≤ pivot  [Theindex j disappearsbecauseitisnolongerneededoncetheforloopisexited.]  Correctness: Use the loop invariant to prove correctness of PARTITION:  Initialization: Before the loop starts, all the conditions of the loop invariant are satis cid:222 ed, because r is the pivot and the subarrays A[p . . i] and A[i + 1 . . j − 1] are empty. Maintenance: While the loop is running, if A[ j] ≤ pivot, then A[ j] and A[i + 1] are swapped and then i and j are incremented. If A[ j] > pivot, then increment only j. Termination: When the loop terminates, j = r, so all elements in A are parti- tioned into one of the three cases: A[p . . i] ≤ pivot, A[i + 1 . . r − 1] > pivot, and A[r] = pivot.  The last two lines of PARTITION move the pivot element from the end of the array to between the two subarrays. This is done by swapping the pivot and the  cid:222 rst element of the second subarray, i.e., by swapping A[i + 1] and A[r].  Time for partitioning:  cid:2  n  to partition an n-element subarray.   7-4  Lecture Notes for Chapter 7: Quicksort  Performance of quicksort  The running time of quicksort depends on the partitioning of the subarrays:   If the subarrays are balanced, then quicksort can run as fast as mergesort.   If they are unbalanced, then quicksort can run as slowly as insertion sort.  Worst case   Occurs when the subarrays are completely unbalanced.   Have 0 elements in one subarray and n − 1 elements in the other subarray.   Get the recurrence  T  n  = T  n − 1  + T  0  +  cid:2  n   = T  n − 1  +  cid:2  n  =  cid:2  n2  .    Same running time as insertion sort.    In fact, the worst-case running time occurs when quicksort takes a sorted array as input, but insertion sort runs in O n  time in this case.  Best case   Occurs when the subarrays are completely balanced every time.   Each subarray has ≤ n 2 elements.   Get the recurrence  T  n  = 2T  n 2  +  cid:2  n   =  cid:2  n lg n  .  Balanced partitioning   Quicksort cid:146 s average running time is much closer to the best case than to the      Get the recurrence  worst case. Imagine that PARTITION always produces a 9-to-1 split. T  n  ≤ T  9n 10  + T  n 10  +  cid:2  n   = O n lg n  .     It cid:146 s like the one for T  n  = T  n 3  + T  2n 3  + O n  in Section 4.2.  Intuition: look at the recursion tree.     Except that here the constants are different; we get log10 n full levels and   As long as it cid:146 s a constant, the base of the log doesn cid:146 t matter in asymptotic  log10 9 n levels that are nonempty.    Any split of constant proportionality will yield a recursion tree of depth  notation.   cid:2  lg n .   Lecture Notes for Chapter 7: Quicksort  7-5  Intuition for the average case    Splits in the recursion tree will not always be constant.   There will usually be a mix of good and bad splits throughout the recursion  tree.    To see that this doesn cid:146 t affect the asymptotic running time of quicksort, assume  that levels alternate between best-case and worst-case splits.  n  0  n cid:150 1   n cid:150 1  2  cid:150  1   n cid:150 1  2  Θ n   n  Θ n    n cid:150 1  2   n cid:150 1  2    The extra level in the left-hand  cid:222 gure only adds to the constant hidden in the   cid:2 -notation.    There are still the same number of subarrays to sort, and only twice as much  work was done to get to that point.    Both  cid:222 gures result in O n lg n  time, though the constant for the  cid:222 gure on the  left is higher than that of the  cid:222 gure on the right.    We have assumed that all input permutations are equally likely.   This is not always true.   To correct this, we add randomization to quicksort.   We could randomly permute the input array.     Don cid:146 t always use A[r] as the pivot. Instead, randomly pick an element from the  Instead, we use random sampling, or picking one element at random.  subarray that is being sorted.  We add this randomization by not always using A[r] as the pivot, but instead ran- domly picking an element from the subarray that is being sorted.  RANDOMIZED-PARTITION A, p, r   i ← RANDOM  p, r   exchange A[r] ↔ A[i] return PARTITION A, p, r    Randomly selecting the pivot element will, on average, cause the split of the input array to be reasonably well balanced.  Randomized version of quicksort   7-6  Lecture Notes for Chapter 7: Quicksort  RANDOMIZED-QUICKSORT A, p, r   if p < r then q ← RANDOMIZED-PARTITION A, p, r   RANDOMIZED-QUICKSORT A, p, q − 1  RANDOMIZED-QUICKSORT A, q + 1, r    Randomization of quicksort stops any speci cid:222 c type of array from causing worst- case behavior. For example, an already-sorted array causes worst-case behavior in non-randomized QUICKSORT, but not in RANDOMIZED-QUICKSORT.  Analysis of quicksort  We will analyze    the worst-case running time of QUICKSORT and RANDOMIZED-QUICKSORT  the same , and the expected  average-case  running time of RANDOMIZED-QUICKSORT.     Worst-case analysis  T  n  = max 0≤q≤n−1 from 0 to n − 1.  We will prove that a worst-case split at every level produces a worst-case running time of O n2 .   Recurrence for the worst-case running time of QUICKSORT:   T  q  + T  n − q − 1   +  cid:2  n  .    Because PARTITION produces two subproblems, totaling size n − 1, q ranges   Guess: T  n  ≤ cn2, for some c.   Substituting our guess into the above recurrence:  cq2 + c n − q − 1 2  +  cid:2  n   q2 +  n − q − 1 2  +  cid:2  n  .  T  n  ≤  max 0≤q≤n−1 = c · max 0≤q≤n−1    The maximum value of  q2 +  n − q − 1 2  occurs when q is either 0 or n − 1.   Second derivative with respect to q is positive.  This means that max 0≤q≤n−1   q2 +  n − q − 1 2  ≤  n − 1 2  = n2 − 2n + 1 .    Therefore,  T  n  ≤ cn2 − c 2n − 1  +  cid:2  n   ≤ cn2  if c 2n − 1  ≥  cid:2  n  .    Pick c so that c 2n − 1  dominates  cid:2  n .   Therefore, the worst-case running time of quicksort is O n2 .   Can also show that the recurrence cid:146 s solution is  cid:3  n2 . Thus, the worst-case  running time is  cid:2  n2 .   Lecture Notes for Chapter 7: Quicksort  7-7  Average-case analysis  number of comparisons that are performed in its for loop.  PARTITION removes the pivot element from future consideration each time.    The dominant cost of the algorithm is partitioning.     Thus, PARTITION is called at most n times.   QUICKSORT recurses on the partitions.   The amount of work that each call to PARTITION does is a constant plus the   Let X = the total number of comparisons performed in all calls to PARTITION.   Therefore, the total work done over the entire execution is O n + X  . We will now compute a bound on the overall number of comparisons. For ease of analysis:   Rename the elements of A as z1, z2, . . . , zn, with zi being the ith smallest ele-   De cid:222 ne the set Zi j = {zi , zi+1, . . . , z j} to be the set of elements between zi  ment.  and z j , inclusive.  Each pair of elements is compared at most once, because elements are compared only to the pivot element, and then the pivot element is never in any later call to PARTITION. Let Xi j = I{zi is compared to z j}.  Considering whether zi is compared to z j at any time during the entire quicksort algorithm, not just during one call of PARTITION.  Since each pair is compared at most once, the total number of comparisons per- formed by the algorithm is  Take expectations of both sides, use Lemma 5.1 and linearity of expectation: E [X] = E  Xi j  n cid:2   X = n−1 cid:2   i=1   cid:14   Xi j .  j=i+1   cid:13  n cid:2  n−1 cid:2  n cid:2  = n−1 cid:2  = n−1 cid:2  n cid:2   j=i+1  i=1  i=1  j=i+1  i=1  j=i+1  E [Xi j ]  Pr{zi is compared to z j} .  Now all we have to do is  cid:222 nd the probability that two elements are compared.   Think about when two elements are not compared.    For example, numbers in separate partitions will not be compared. In the previous example,  cid:2 8, 1, 6, 4, 0, 3, 9, 5 cid:3  and the pivot is 5, so that none   of the set {1, 4, 0, 3} will ever be compared to any of the set{8, 6, 9}.   7-8  Lecture Notes for Chapter 7: Quicksort    Once a pivot x is chosen such that zi < x < z j , then zi and z j will never be     compared at any later time. If either zi or z j is chosen before any other element of Zi j , then it will be compared to all the elements of Zi j , except itself.  the  cid:222 rst element chosen.    The probability that zi is compared to z j is the probability that either zi or z j is   There are j−i+1 elements, and pivots are chosen randomly and independently. Thus, the probability that any particular one of them is the  cid:222 rst one chosen is 1   j − i + 1 .  Therefore, Pr{zi is compared to z j} = Pr{zi or z j is the  cid:222 rst pivot chosen from Zi j} + Pr{z j is the  cid:222 rst pivot chosen from Zi j} 1  = Pr{zi is the  cid:222 rst pivot chosen from Zi j}  1  = =  +  .  j − i + 1 j − i + 1  2  j − i + 1  [Thesecondlinefollowsbecausethetwoeventsaremutuallyexclusive.] Substituting into the equation for E[X]:  Evaluate by using a change in variables  k = j − i  and the bound on the harmonic series in equation  A.7 :  n cid:2   j=i+1  i=1  E [X] = n−1 cid:2  E [X] = n−1 cid:2  = n−1 cid:2  n−1 cid:2  = n−1 cid:2   i=1  i=1  i=1  <  2  .  2  j − i + 1 n cid:2  n−i cid:2  n cid:2   j − i + 1 2 k + 1 2 k  j=i+1  k=1  k=1  O lg n  = O n lg n  .  i=1  So the expected running time of quicksort, using RANDOMIZED-PARTITION, is O n lg n .   Solutions for Chapter 7: Quicksort  Solution to Exercise 7.2-3  PARTITION does a  cid:147 worst-case partitioning cid:148  when the elements are in decreasing order. It reduces the size of the subarray under consideration by only 1 at each step, which we cid:146 ve seen has running time  cid:2  n2 . In particular, PARTITION, given a subarray A[p . . r] of distinct elements in de- creasing order, produces an empty partition in A[p . . q − 1], puts the pivot  orig- inally in A[r]  into A[p], and produces a partition A[p + 1 . . r] with only one fewer element than A[p . . r]. The recurrence for QUICKSORT becomes T  n  = T  n − 1  +  cid:2  n , which has the solution T  n  =  cid:2  n2 .  Solution to Exercise 7.2-5  The minimum depth follows a path that always takes the smaller part of the par- tition cid:151 i.e., that multiplies the number of elements by α. One iteration reduces the number of elements from n to αn, and i iterations reduces the number of ele- ments to αin. At a leaf, there is just one remaining element, and so at a minimum- depth leaf of depth m, we have αmn = 1. Thus, αm = 1 n. Taking logs, we get m lg α = − lg n, or m = − lg n  lg α. Similarly, maximum depth corresponds to always taking the larger part of the par- tition, i.e., keeping a fraction 1 − α of the elements each time. The maximum depth M is reached when there is one element left, that is, when  1 − α Mn = 1. Thus, M = − lg n  lg 1 − α . All these equations are approximate because we are ignoring  cid:223 oors and ceilings.  Solution to Exercise 7.3-1  We may be interested in the worst-case performance, but in that case, the random- ization is irrelevant: it won cid:146 t improve the worst case. What randomization can do is make the chance of encountering a worst-case scenario small.   7-10  Solutions for Chapter 7: Quicksort  Solution to Exercise 7.4-2  To show that quicksort cid:146 s best-case running time is  cid:3  n lg n , we use a technique similar to the one used in Section 7.4.1 to show that its worst-case running time is O n2 . Let T  n  be the best-case time for the procedure QUICKSORT on an input of size n. We have the recurrence T  n  = min 1≤q≤n−1 We guess that T  n  ≥ cn lg n for some constant c. Substituting this guess into the recurrence, we obtain T  n  ≥   T  q  + T  n − q − 1   +  cid:2  n  .   cq lg q + c n − q − 1  lg n − q − 1   +  cid:2  n   q lg q +  n − q − 1  lg n − q − 1   +  cid:2  n  .  min 1≤q≤n−1 = c · min 1≤q≤n−1  As we cid:146 ll show below, the expression q lg q +  n − q − 1  lg n − q − 1  achieves a minimum over the range 1 ≤ q ≤ n−1 when q = n−q−1, or q =  n−1  2, since the  cid:222 rst derivative of the expression with respect to q is 0 when q =  n − 1  2 and the second derivative of the expression is positive.  It doesn cid:146 t matter that q is not an integer when n is even, since we cid:146 re just trying to determine the minimum value of a function, knowing that when we constrain q to integer values, the function cid:146 s value will be no lower.  Choosing q =  n − 1  2 gives us the bound  cid:5   q lg q +  n − q − 1  lg n − q − 1  min 1≤q≤n−1 n − n − 1   cid:6  − 1   cid:6  − 1  n − n − 1   cid:5   lg  2  2  2  ≥ n − 1 lg =  n − 1  lg n − 1 2  n − 1 + 2 n − 1 2 +  cid:2  n   .  Continuing with our bounding of T  n , we obtain, for n ≥ 2, T  n  ≥ c n − 1  lg  = c n − 1  lg n − 1  − c n − 1  +  cid:2  n  = cn lg n − 1  − c lg n − 1  − c n − 1  +  cid:2  n  ≥ cn lg n 2  − c lg n − 1  − c n − 1  +  cid:2  n  = cn lg n − cn − c lg n − 1  − cn + c +  cid:2  n  = cn lg n −  2cn + c lg n − 1  − c  +  cid:2  n  ≥ cn lg n ,   since n ≥ 2   since we can pick the constant c small enough so that the  cid:2  n  term dominates the quantity 2cn + c lg n − 1  − c. Thus, the best-case running time of quicksort is  cid:3  n lg n . Letting f  q  = q lg q +  n − q − 1  lg n − q − 1 , we now show how to  cid:222 nd the minimum value of this function in the range 1 ≤ q ≤ n − 1. We need to  cid:222 nd the value of q for which the derivative of f with respect to q is 0. We rewrite this function as   Solutions for Chapter 7: Quicksort  7-11  f  q  = q ln q +  n − q − 1  ln n − q − 1   ,  ln 2   cid:5   and so  cid:4  q  = d dq  f  q ln q +  n − q − 1  ln n − q − 1    cid:6   ln 2  = ln q + 1 − ln n − q − 1  − 1 = ln q − ln n − q − 1   ln 2  .  ln 2   cid:4  q  is 0 when q = n − q − 1, or when q =  n − 1  2. To verify The derivative f that q =  n − 1  2 is indeed a minimum  not a maximum or an in cid:223 ection point , we need to check that the second derivative of f is positive at q =  n − 1  2:  cid:4  cid:4  q  f  cid:5    cid:5  ln q − ln n − q − 1   cid:5   cid:6   cid:6   cid:5   ln 2 1   cid:6    cid:6   +   cid:4  cid:4   f  n − 1 2  n − q − 1 + 2 n − 1  1 q 2 n − 1 4 n − 1  ·   since n ≥ 2  .  = d dq = 1 ln 2 = 1 ln 2 = 1 ln 2  > 0   cid:4    cid:4    cid:4   a. QUICKSORT  does exactly what QUICKSORT does; hence it sorts correctly.  QUICKSORT and QUICKSORT do the same partitioning, and then each calls itself with arguments A, p, q − 1. QUICKSORT then calls itself again, with instead sets p ← q + 1 and performs arguments A, q + 1, r. QUICKSORT another iteration of its while loop. This executes the same operations as calling itself with A, q + 1, r, because in both cases, the  cid:222 rst and third arguments  A and r  have the same values as before, and p has the old value of q + 1.   cid:4   b. The stack depth of QUICKSORT  will be  cid:2  n  on an n-element input array if  cid:4  there are  cid:2  n  recursive calls to QUICKSORT . This happens if every call to PARTITION A, p, r   returns q = r. The sequence of recursive calls in this scenario is QUICKSORT QUICKSORT QUICKSORT   cid:4  A, 1, n  ,  cid:4  A, 1, n − 1  ,  cid:4  A, 1, n − 2  , ...  cid:4  A, 1, 1  .  QUICKSORT   cid:4  Any array that is already sorted in increasing order will cause QUICKSORT behave this way.  to  Solution to Problem 7-4   7-12  Solutions for Chapter 7: Quicksort   cid:4    cid:4   calls QUICKSORT  c. The problem demonstrated by the scenario in part  b  is that each invocation of again with almost the same range. To avoid QUICKSORT  cid:4  such behavior, we must change QUICKSORT so that the recursive call is on a  cid:4  checks smaller interval of the array. The following variation of QUICKSORT which of the two subarrays returned from PARTITION is smaller and recurses on the smaller subarray, which is at most half the size of the current array. Since the array size is reduced by at least half on each recursive call, the number of recursive calls, and hence the stack depth, is  cid:2  lg n  in the worst case. Note that this method works no matter how partitioning is performed  as long as the PARTITION procedure has the same functionality as the procedure given in Section 7.1 .   cid:4  cid:4  A, p, r    QUICKSORT while p < r  do  cid:1  Partition and sort the small subarray  cid:222 rst  q ← PARTITION A, p, r   if q − p < r − q then QUICKSORT p ← q + 1 else QUICKSORT r ← q − 1   cid:4  cid:4  A, p, q − 1   cid:4  cid:4  A, q + 1, r    The expected running time is not affected, because exactly the same work is done as before: the same partitions are produced, and the same subarrays are sorted.   Lecture Notes for Chapter 8: Sorting in Linear Time  Chapter 8 overview  How fast can we sort?  Comparison sorting  We will prove a lower bound, then beat it by playing a different game.    The only operation that may be used to gain order information about a sequence  is comparison of pairs of elements.    All sorts seen so far are comparison sorts: insertion sort, selection sort, merge  sort, quicksort, heapsort, treesort.  Lower bounds for sorting  Lower bounds     cid:3  n  to examine all the input.   All sorts seen so far are  cid:3  n lg n .   We cid:146 ll show that  cid:3  n lg n  is a lower bound for comparison sorts.  Decision tree    Abstraction of any comparison sort.   Represents comparisons made by    a speci cid:222 c sorting algorithm   on inputs of a given size.    Abstracts away everything else: control and data movement.   We cid:146 re counting only comparisons.   8-2  Lecture Notes for Chapter 8: Sorting in Linear Time  For insertion sort on 3 elements:  compare A[1] to A[2]  A[1] ≤ A[2]  ≤  1:2  A[1] > A[2]  swap in array   ≤  〈1,2,3〉  2:3  A[1] ≤ A[2] A[2] > A[3]  >  1:3  A[1] > A[2] A[1] > A[3]  >  1:3  ≤  >  〈2,1,3〉  2:3  ≤  >  〈1,3,2〉  〈3,1,2〉  〈2,3,1〉  〈3,2,1〉  A[1] ≤ A[2] ≤ A[3]  >  ≤  [Each internal node is labeled by indices of array elements from their original positions. Each leaf is labeled by the permutation of orders that the algorithm determines.] How many leaves on the decision tree? There are ≥ n! leaves, because every permutation appears at least once. For any comparison sort,     View the tree as if the algorithm splits in two at each node, based on the infor-  1 tree for each n.  mation it has determined up to that point.    The tree models all possible execution traces. What is the length of the longest path from root to leaf?   Depends on the algorithm   Insertion sort:  cid:2  n2    Merge sort:  cid:2  n lg n   Lemma Any binary tree of height h has ≤ 2h leaves. In other words:      l =  of leaves, h = height,   Then l ≤ 2h.   We cid:146 ll prove this lemma later.  Why is this useful?  Theorem Any decision tree that sorts n elements has height  cid:3  n lg n .   Lecture Notes for Chapter 8: Sorting in Linear Time  8-3  Proof    l ≥ n!    By lemma, n! ≤ l ≤ 2h or 2h ≥ n!   Take logs: h ≥ lg n!    Use Stirling cid:146 s approximation: n! >  n e n  by equation  3.16    h ≥ lg n e n = n lg n e  = n lg n − n lg e =  cid:3  n lg n  .  Now to prove the lemma:   theorem   Proof By induction on h. Basis: h = 0. Tree is just one node, which is a leaf. 2h = 1. Inductive step: Assume true for height = h − 1. Extend tree of height h − 1 by making as many new leaves as possible. Each leaf becomes parent to two new leaves.  of leaves for height h = 2 ·   of leaves for height h − 1   = 2 · 2h−1 = 2h .   ind. hypothesis   lemma   Corollary Heapsort and merge sort are asymptotically optimal comparison sorts.  Sorting in linear time  Non-comparison sorts.  Counting sort Depends on a key assumption: numbers to be sorted are integers in{0, 1, . . . , k}. Input: A[1 . . n], where A[ j] ∈ {0, 1, . . . , k} for j = 1, 2, . . . , n. Array A and  values n and k are given as parameters.  Output: B[1 . . n], sorted. B is assumed to be already allocated and is given as a  parameter.  Auxiliary storage: C[0 . . k]   8-4  Lecture Notes for Chapter 8: Sorting in Linear Time  COUNTING-SORT A, B, n, k  for i ← 0 to k do C[i] ← 0 for j ← 1 to n do C[A[ j]] ← C[A[ j]] + 1 for i ← 1 to k do C[i] ← C[i] + C[i − 1] for j ← n downto 1 do B[C[A[ j]]] ← A[ j]  C[A[ j]] ← C[A[ j]] − 1  Do an example for A = 21, 51, 31, 01, 22, 32, 02, 33 Counting sort is stable  keys with same value appear in same order in output as they did in input  because of how the last loop works. Analysis:  cid:2  n + k , which is  cid:2  n  if k = O n . How big a k is practical?   Good for sorting 32-bit values? No.        16-bit? Probably not. 8-bit? Maybe, depending on n. 4-bit? Probably  unless n is really small .  Counting sort will be used in radix sort.  Radix sort  How IBM made its money. Punch card readers for census tabulation in early 1900 cid:146 s. Card sorters, worked on one column at a time. It cid:146 s the algorithm for using the machine that extends the technique to multi-column sorting. The human operator was part of the algorithm! Key idea: Sort least signi cid:222 cant digits  cid:222 rst. To sort d digits:  RADIX-SORT A, d  for i ← 1 to d  do use a stable sort to sort array A on digit i  Example:  326 453 608 835 751 435 704 690  690 751 453 704 835 435 326 608  sorted  704 608 326 835 435 751 453 690  326 435 453 608 690 704 751 835   Lecture Notes for Chapter 8: Sorting in Linear Time  8-5  Induction on number of passes  i in pseudocode .  Correctness:     Assume digits 1, 2, . . . , i − 1 are sorted.   Show that a stable sort on digit i leaves digits 1, . . . , i sorted:        If 2 digits in position i are different, ordering by position i is correct, and positions 1, . . . , i − 1 are irrelevant. If 2 digits in position i are equal, numbers are already in the right order  by inductive hypothesis . The stable sort on digit i leaves them in the right order.  This argument shows why it cid:146 s so important to use a stable sort for intermediate sort.     Analysis: Assume that we use counting sort as the intermediate sort.    cid:2  n + k  per pass  digits in range 0, . . . , k     cid:2  d n + k   total  d passes If k = O n , time =  cid:2  dn .     How to break each key into digits?      n words. b bits word.    Break into r-bit digits. Have d =  cid:14 b r cid:15 .   Use counting sort, k = 2r − 1.  r        cid:2   b lg n   n + 2r   .  Example: 32-bit words, 8-bit digits. b = 32, r = 8, d =  cid:14 32 8 cid:15  = 4, k = 28 − 1 = 255.   Time =  cid:2   b  cid:10  How to choose r? Balance b r and n + 2r . Choosing r ≈ lg n gives us  n + n  If we choose r   b  lg n, and n + 2r term doesn cid:146 t improve. If we choose r > lg n, then n + 2r term gets big. Example: r = 2 lg n ⇒ 2r = 22 lg n =  2lg n 2 = n2.   cid:11  =  cid:2  bn  lg n .  So, to sort 216 32-bit numbers, use r = lg 216 = 16 bits.  cid:14 b r cid:15  = 2 passes. Compare radix sort to merge sort and quicksort:   1 million  220  32-bit integers.   Radix sort:  cid:14 32 20 cid:15  = 2 passes.   Merge sort quicksort: lg n = 20 passes.   Remember, though, that each radix sort  cid:147 pass cid:148  is really 2 passes cid:151 one to take  census, and one to move data.  How does radix sort violate the ground rules for a comparison sort?   Using counting sort allows us to gain information about keys by means other  than directly comparing 2 keys.    Used keys as array indices.   8-6  Lecture Notes for Chapter 8: Sorting in Linear Time  Bucket sort  Assumes the input is generated by a random process that distributes elements uni- formly over [0, 1 .  Idea:   Divide [0, 1  into n equal-sized buckets.   Distribute the n input values into the buckets.   Sort each bucket.   Then go through buckets in order, listing elements in each one. Input: A[1 . . n], where 0 ≤ A[i] < 1 for all i. Auxiliary array: B[0 . . n − 1] of linked lists, each list initially empty.  do insert A[i] into list B[ cid:9 n · A[i] cid:10 ]  BUCKET-SORT A, n  for i ← 1 to n for i ← 0 to n − 1 concatenate lists B[0], B[1], . . . , B[n − 1] together in order return the concatenated lists  do sort list B[i] with insertion sort  Correctness: Consider A[i], A[ j]. Assume without loss of generality that A[i] ≤ A[ j]. Then  cid:9 n · A[i] cid:10  ≤  cid:9 n · A[ j] cid:10 . So A[i] is placed into the same bucket as A[ j] or into a bucket with a lower index.      If same bucket, insertion sort  cid:222 xes up. If earlier bucket, concatenation of lists  cid:222 xes up.  Analysis:   Relies on no bucket getting too many values.   All lines of algorithm except insertion sorting take  cid:2  n  altogether.    Intuitively, if each bucket gets a constant number of elements, it takes O 1  time to sort each bucket ⇒ O n  sort time for all buckets.    We  cid:147 expect cid:148  each bucket to have few elements, since the average is 1 element  per bucket.    But we need to do a careful analysis. De cid:222 ne a random variable:    ni = the number of elements placed in bucket B[i].  Because insertion sort runs in quadratic time, bucket sort time is  T  n  =  cid:2  n  + n−1 cid:2   O n2 i    .  i=0   Lecture Notes for Chapter 8: Sorting in Linear Time  8-7  E [O n2 i   ]   linearity of expectation   O E [n2  i ]   i=0   E [a X] = aE [X]    cid:14   Take expectations of both sides: E [T  n ] = E  O n2 i      cid:13   cid:2  n  + n−1 cid:2  =  cid:2  n  + n−1 cid:2  =  cid:2  n  + n−1 cid:2   i=0  i=0  Claim E [n2  i ] = 2 −  1 n  for i = 0, . . . , n − 1.  Proof of claim De cid:222 ne indicator random variables:   Xi j = I{A[ j] falls in bucket i}   Pr{A[ j] falls in bucket i} = 1 n  ni = n cid:2   Xi j     Then  i ] = E  E [n2  2  Xi j  X 2 i j  j=1  j=1   cid:14    cid:4   + 2  = E  n cid:2  n cid:2    cid:14  n−1 cid:2  n−1 cid:2   cid:16  + 2  cid:6    cid:13  cid:3  n cid:2   cid:13  n cid:2  = n cid:2   cid:15   cid:16  = 02 · Pr{A[ j] doesn cid:146 t fall in bucket i} + 12 · Pr{A[ j] falls in bucket i}  cid:5  1 − 1 n   linearity of expectation   + 1 · 1 n  E [Xi j Xik ]  k= j+1  k= j+1  Xi j Xik  j=1  j=1  j=1  j=1  X 2 i j  E   cid:15   E  X 2 i j  E [Xi j Xik] for j  cid:19 = k: Since j  cid:19 = k, Xi j and Xik are independent random variables ⇒ E [Xi j Xik] = E [Xi j ] E [Xik]  = 0 · = 1 n  · 1 n  = 1 n = 1 n2  Therefore:  i ] = n cid:2   j=1  E [n2  n−1 cid:2   n cid:2   j=1  k= j+1  1 n2  + 2  1 n   8-8  Lecture Notes for Chapter 8: Sorting in Linear Time  n 2  1 n2   cid:5  + 2   cid:6  = n · 1 n = 1 + 2 · n n − 1  = 1 + n − 1 n = 1 + 1 − 1 n = 2 − 1 n  2  · 1 n2  Therefore:  E [T  n ] =  cid:2  n  + n−1 cid:2   O 2 − 1 n   i=0 =  cid:2  n  + O n  =  cid:2  n    claim     Again, not a comparison sort. Used a function of key values to index into an    This is a probabilistic analysis cid:151 we used probability to analyze an algorithm  whose running time depends on the distribution of inputs.    Different from a randomized algorithm, where we use randomization to impose  array.  a distribution.    With bucket sort, if the input isn cid:146 t drawn from a uniform distribution on [0, 1 ,  all bets are off  performance-wise, but the algorithm is still correct .   Solutions for Chapter 8: Sorting in Linear Time  Solution to Exercise 8.1-3  Solution to Exercise 8.1-4  If the sort runs in linear time for m input permutations, then the height h of the portion of the decision tree consisting of the m corresponding leaves and their ancestors is linear. Use the same argument as in the proof of Theorem 8.1 to show that this is impos- sible for m = n! 2, n! n, or n! 2n. We have 2h ≥ m, which gives us h ≥ lg m. For all the possible m cid:146 s given here, lg m =  cid:3  n lg n , hence h =  cid:3  n lg n . In particular,  lg  lg  lg  n! 2 n! n n! 2n  = lg n! − 1 ≥ n lg n − n lg e − 1 = lg n! − lg n ≥ n lg n − n lg e − lg n = lg n! − n ≥ n lg n − n lg e − n  Let S be a sequence of n elements divided into n k subsequences each of length k where all of the elements in any subsequence are larger than all of the elements of a preceding subsequence and smaller than all of the elements of a succeeding subsequence.  Claim Any comparison-based sorting algorithm to sort s must take  cid:3  n lg k  time in the worst case.  Proof First notice that, as pointed out in the hint, we cannot prove the lower bound by multiplying together the lower bounds for sorting each subsequence. That would only prove that there is no faster algorithm that sorts the subsequences independently. This was not what we are asked to prove; we cannot introduce any extra assumptions.   8-10  Solutions for Chapter 8: Sorting in Linear Time  Now, consider the decision tree of height h for any comparison sort for S. Since the elements of each subsequence can be in any order, any of the k! permutations correspond to the  cid:222 nal sorted order of a subsequence. And, since there are n k such subsequences, each of which can be in any order, there are  k! n k permutations of S that could correspond to the sorting of some input order. Thus, any decision tree for sorting S must have at least  k! n k leaves. Since a binary tree of height h has no more than 2h leaves, we must have 2h ≥  k! n k or h ≥ lg  k! n k . We therefore obtain h ≥ lg  k! n k  =  n k  lg k!  ≥  n k  lg  k 2 k 2  =  n 2  lg k 2  .  The third line comes from k! having its k 2 largest terms being at least k 2 each.  We implicitly assume here that k is even. We could adjust with  cid:223 oors and ceilings if k were odd.  Since there exists at least one path in any decision tree for sorting S that has length at least  n 2  lg k 2 , the worst-case running time of any comparison-based sorting algorithm for S is  cid:3  n lg k .  Solution to Exercise 8.2-3  [ThefollowingsolutionalsoanswersExercise8.2-2.] Notice that the correctness argument in the text does not depend on the order in which A is processed. The algorithm is correct no matter what order is used! But the modi cid:222 ed algorithm is not stable. As before, in the  cid:222 nal for loop an element equal to one taken from A earlier is placed before the earlier one  i.e., at a lower index position  in the output arrray B. The original algorithm was stable because an element taken from A later started out with a lower index than one taken earlier. But in the modi cid:222 ed algorithm, an element taken from A later started out with a higher index than one taken earlier. In particular, the algorithm still places the elements with value k in positions C[k − 1] + 1 through C[k], but in the reverse order of their appearance in A.  Solution to Exercise 8.2-4  Compute the C array as is done in counting sort. The number of integers in the range [a . . b] is C[b] − C[a − 1], where we interpret C[−1] as 0.   Solutions for Chapter 8: Sorting in Linear Time  8-11  Solution to Exercise 8.3-2  Solution to Exercise 8.3-3  Insertion sort is stable. When inserting A[ j] into the sorted sequence A[1 . . . j−1], we do it the following way: compare A[ j] to A[i], starting with i = j−1 and going down to i = 1. Continue at long as A[ j] < A[i]. Merge sort as de cid:222 ned is stable, because when two elements compared are equal, the tie is broken by taking the element from array L which keeps them in the original order. Heapsort and quicksort are not stable. One scheme that makes a sorting algorithm stable is to store the index of each element  the element cid:146 s place in the original ordering  with the element. When comparing two elements, compare them by their values and break ties by their indices. Additional space requirements: For n elements, their indices are 1 . . . n. Each can be written in lg n bits, so together they take O n lg n  additional space. Additional time requirements: The worst case is when all elements are equal. The asymptotic time does not change because we add a constant amount of work to each comparison.  Basis: If d = 1, there cid:146 s only one digit, so sorting on that digit sorts the array. Inductive step: Assuming that radix sort works for d − 1 digits, we cid:146 ll show that it works for d digits. Radix sort sorts separately on each digit, starting from digit 1. Thus, radix sort of d digits, which sorts on digits 1, . . . , d is equivalent to radix sort of the low-order d − 1 digits followed by a sort on digit d. By our induction hypothesis, the sort of the low-order d − 1 digits works, so just before the sort on digit d, the elements are in order according to their low-order d − 1 digits. The sort on digit d will order the elements by their dth digit. Consider two ele- ments, a and b, with dth digits ad and bd respectively.    If ad < bd, the sort will put a before b, which is correct, since a < b regardless of the low-order digits. If ad > bd, the sort will put a after b, which is correct, since a > b regardless of the low-order digits. If ad = bd, the sort will leave a and b in the same order they were in, because it is stable. But that order is already correct, since the correct order of a and b is determined by the low-order d − 1 digits when their dth digits are equal, and the elements are already sorted by their low-order d − 1 digits.        If the intermediate sort were not stable, it might rearrange elements whose dth digits were equal cid:151 elements that were in the right order after the sort on their lower-order digits.   8-12  Solutions for Chapter 8: Sorting in Linear Time  Solution to Exercise 8.3-4  Treat the numbers as 2-digit numbers in radix n. Each digit ranges from 0 to n − 1. Sort these 2-digit numbers with radix sort. There are 2 calls to counting sort, each taking  cid:2  n + n  =  cid:2  n  time, so that the total time is  cid:2  n .  Solution to Exercise 8.4-2  The worst-case running time for the bucket-sort algorithm occurs when the assump- tion of uniformly distributed input does not hold. If, for example, all the input ends up in the  cid:222 rst bucket, then in the insertion sort phase it needs to sort all the input, which takes O n2  time. A simple change that will preserve the linear expected running time and make the worst-case running time O n lg n  is to use a worst-case O n lg n -time algorithm like merge sort instead of insertion sort when sorting the buckets.  Solution to Problem 8-1  a. For a comparison algorithm A to sort, no two input permutations can reach the same leaf of the decision tree, so there must be at least n! leaves reached in TA, one for each possible input permutation. Since A is a deterministic algorithm, it must always reach the same leaf when given a particular permutation as input, so at most n! leaves are reached  one for each permutation . Therefore exactly n! leaves are reached, one for each input permutation. These n! leaves will each have probability 1 n!, since each of the n! possible permutations is the input with the probability 1 n!. Any remaining leaves will have probability 0, since they are not reached for any input. Without loss of generality, we can assume for the rest of this problem that paths leading only to 0-probability leaves aren cid:146 t in the tree, since they cannot affect the running time of the sort. That is, we can assume that TA consists of only the n! leaves labeled 1 n! and their ancestors.  b.  If k > 1, then the root of T is not a leaf. This implies that all of T  cid:146 s leaves are leaves in LT and RT . Since every leaf at depth h in LT or RT has depth h + 1 in T , D T   must be the sum of D LT  , D RT  , and k, the total number of leaves. To prove this last assertion, let dT  x  = depth of node x in tree T . Then, D T   = =   cid:2   cid:2    cid:2   x∈leaves T    dT  x  dT  x  +  dT  x   x∈leaves LT    x∈leaves RT     Solutions for Chapter 8: Sorting in Linear Time  8-13   cid:2   cid:2   x∈leaves LT     cid:2   dLT  x  + 1  + dLT  x  +  = = = D LT   + D RT   + k .  x∈leaves LT    x∈leaves RT     cid:2  dRT  x  +  x∈leaves RT     cid:2   dRT  x  + 1   1  x∈leaves T    c. To show that d k  = min1≤i≤k−1 {d i   + d k − i   + k} we will show separately  that d k  ≤ min 1≤i≤k−1  {d i   + d k − i   + k}  {d i   + d k − i   + k} .  and d k  ≥ min 1≤i≤k−1   To show that d k  ≤ min1≤i≤k−1 {d i   + d k − i   + k}, we need only show that d k  ≤ d i   + d k − i   + k, for i = 1, 2, . . . , k − 1. For any i from 1 to k − 1 we can  cid:222 nd trees RT with i leaves and LT with k − i leaves such that D RT   = d i   and D LT   = d k − i  . Construct T such that RT and LT are the right and left subtrees of T  cid:146 s root respectively. Then d k  ≤ D T    = D RT   + D LT   + k = d i   + d k − i   + k   by de cid:222 nition of d as min D T   value   by part  b    by choice of RT and LT   .    To show that d k  ≥ min1≤i≤k−1 {d i   + d k − i   + k}, we need only show that d k  ≥ d i   + d k − i   + k, for some i in {1, 2, . . . , k − 1}. Take the tree T with k leaves such that D T   = d k , let RT and LT be T  cid:146 s right and left subtree, respecitvely, and let i be the number of leaves in RT . Then k − i is the number of leaves in LT and d k  = D T    = D RT   + D LT   + k ≥ d i   + d k − i   + k   by choice of T    by part  b    by de cid:222 ntion of d as min D T   value  . Neither i nor k − i can be 0  and hence 1 ≤ i ≤ k − 1 , since if one of these were 0, either RT or LT would contain all k leaves of T , and that k-leaf subtree would have a D equal to D T   − k  by part  b  , contradicting the choice of T as the k-leaf tree with the minimum D. d. Let fk i   = i lg i +  k − i   lg k − i  . To  cid:222 nd the value of i that minimizes fk,   cid:222 nd the i for which the derivative of fk with respect to i is 0:  i   = d di  i ln i +  k − i   ln k − i    ln 2   cid:4  k  f   cid:6    cid:5   = ln i + 1 − ln k − i   − 1 = ln i − ln k − i    ln 2  ln 2  is 0 at i = k 2. To verify this is indeed a minimum  not a maximum , check that the second derivative of fk is positive at i = k 2:  cid:4  cid:4  f k  ln i − ln k − i     cid:5    cid:6    i   = d di  ln 2   8-14  Solutions for Chapter 8: Sorting in Linear Time   cid:5    cid:6   cid:6  + 1 k − i + 2 k  1 i   cid:5   .   cid:4  cid:4  k  f  = 1 ln 2  k 2  = 1 ln 2 = 1 ln 2 > 0  2 k · 4 k  since k > 1 .  min 1≤i≤k−1 min 1≤i≤k−1 min 1≤i≤k−1  Now we use substitution to prove d k  =  cid:3  k lg k . The base case of the induction is satis cid:222 ed because d 1  ≥ 0 = c · 1 · lg 1 for any constant c. For the inductive step we assume that d i   ≥ ci lg i for 1 ≤ i ≤ k − 1, where c is some constant to be determined. d k  = ≥  cid:7  = = c k 2 = ck lg = c k lg k − k  + k = ck lg k +  k − ck  if c ≤ 1 , ≥ ck lg k  {d i   + d k − i   + k} {c i lg i +  k − i   lg k − i    + k}  cid:8  cid:8   cid:8   cid:7  {c fk i   + k}  cid:8  k − k k 2 2 + k k 2  k − k 2  + k   cid:7    cid:7   lg  lg  and so d k  =  cid:3  k lg k .  e. Using the result of part  d  and the fact that TA  as modi cid:222 ed in our solution to  part  a   has n! leaves, we can conclude that D TA  ≥ d n!  =  cid:3  n! lg n!   . D TA  is the sum of the decision-tree path lengths for sorting all input per- mutations, and the path lengths are proportional to the run time. Since the n! permutations have equal probability 1 n!, the expected time to sort n random elements  1 input permutation  is the total time for all permutations divided by n!:  cid:3  n! lg n!    =  cid:3  lg n!   =  cid:3  n lg n  .  n!  f. We will show how to modify a randomized decision tree  algorithm  to de cid:222 ne a deterministic decision tree  algorithm  that is at least as good as the randomized one in terms of the average number of comparisons. At each randomized node, pick the child with the smallest subtree  the subtree with the smallest average number of comparisons on a path to a leaf . Delete all the other children of the randomized node and splice out the randomized node itself. The deterministic algorithm corresponding to this modi cid:222 ed tree still works, be- cause the randomized algorithm worked no matter which path was taken from each randomized node.   Solution to Problem 8-3  Solutions for Chapter 8: Sorting in Linear Time  8-15  The average number of comparisons for the modi cid:222 ed algorithm is no larger than the average number for the original randomized tree, since we discarded the higher-average subtrees in each case. In particular, each time we splice out a randomized node, we leave the overall average less than or equal to what it was, because    the same set of input permutations reaches the modi cid:222 ed subtree as before, but those inputs are handled in less than or equal to average time than before, and the rest of the tree is unmodi cid:222 ed.     The randomized algorithm thus takes at least as much time on average as the corresponding deterministic one.  We cid:146 ve shown that the expected running time for a deterministic comparison sort is  cid:3  n lg n , hence the expected time for a randomized comparison sort is also  cid:3  n lg n .   a. The usual, unadorned radix sort algorithm will not solve this problem in the required time bound. The number of passes, d, would have to be the number of digits in the largest integer. Suppose that there are m integers; we always have m ≤ n. In the worst case, we would have one integer with n 2 digits and n 2 integers with one digit each. We assume that the range of a single digit is constant. Therefore, we would have d = n 2 and m = n 2 + 1, and so the running time would be  cid:2  dm  =  cid:2  n2 . Let us assume without loss of generality that all the integers are positive and have no leading zeros.  If there are negative integers or 0, deal with the positive numbers, negative numbers, and 0 separately.  Under this assumption, we can observe that integers with more digits are always greater than integers with fewer digits. Thus, we can  cid:222 rst sort the integers by number of digits  using counting sort , and then use radix sort to sort each group of integers with the same length. Noting that each integer has between 1 and n digits, let mi be the number of integers with i digits, for i = 1, 2, . . . , n. Since there are n digits altogether, we have It takes O n  time to compute how many digits all the integers have and, once the numbers of digits have been computed, it takes O m + n  = O n  time n cid:2  to group the integers by number of digits. To sort the group with mi digits by radix sort takes  cid:2  i · mi   time. The time to sort all groups, therefore, is   cid:1   n  i=1 i · mi = n.  cid:3   cid:4  n cid:2  i=1 =  cid:2  n  .  i · mi   cid:2  i · mi   =  cid:2   i=1  b. One way to solve this problem is by a radix sort from right to left. Since the strings have varying lengths, however, we have to pad out all strings that are shorter than the longest string. The padding is on the right end of the string, and it cid:146 s with a special character that is lexicographically less than any other character  e.g., in C, the character ’\0’ with ASCII value 0 . Of course, we   8-16  Solutions for Chapter 8: Sorting in Linear Time  don cid:146 t have to actually change any string; if we want to know the jth character of a string whose length is k, then if j > k, the jth character is the pad character. Unfortunately, this scheme does not always run in the required time bound. Suppose that there are m strings and that the longest string has d characters. In the worst case, one string has n 2 characters and, before padding, n 2 strings have one character each. As in part  a , we would have d = n 2 and m = n 2 + 1. We still have to examine the pad characters in each pass of radix sort, even if we don cid:146 t actually create them in the strings. Assuming that the range of a single character is constant, the running time of radix sort would be  cid:2  dm  =  cid:2  n2 . To solve the problem in O n  time, we use the property that, if the  cid:222 rst letter of string x is lexicographically less that the  cid:222 rst letter of string y, then x is lexicographically less than y, regardless of the lengths of the two strings. We take advantage of this property by sorting the strings on the  cid:222 rst letter, using counting sort. We take an empty string as a special case and put it  cid:222 rst. We gather together all strings with the same  cid:222 rst letter as a group. Then we recurse, within each group, based on each string with the  cid:222 rst letter removed. The correctness of this algorithm is straightforward. Analyzing the running time is a bit trickier. Let us count the number of times that each string is sorted by a call of counting sort. Suppose that the ith string, si , has length li. Then si is sorted by at most li + 1 counting sorts.  The  cid:147 +1 cid:148  is because it may have to be sorted as an empty string at some point; for example, ab and a end up in the same group in the  cid:222 rst pass and are then ordered based on b and the empty string in the second pass. The string a is sorted its length, 1, time plus one more time.  A call of counting sort on t strings takes  cid:2  t  time  remembering that the number of different characters on which we are sorting is a constant.  Thus, the total time for all calls of counting sort is   cid:4    cid:3  m cid:2    cid:4   li + 1   O  i=1   cid:3  m cid:2  = O i=1 = O n + m  = O n  ,  where the second line follows from m ≤ n.  li + m  cid:1  i=1 li = n, and the last line is because  m  Solution to Problem 8-4  a. Compare each red jug with each blue jug. Since there are n red jugs and n blue  jugs, that will take  cid:2  n2  comparisons in the worst case.  b. To solve the problem, an algorithm has to perform a series of comparisons until it has enough information to determine the matching. We can view the computation of the algorithm in terms of a decision tree. Every internal node is labeled with two jugs  one red, one blue  which we compare, and has three outgoing edges  red jug smaller, same size, or larger than the blue jug . The leaves are labeled with a unique matching of jugs.   Solutions for Chapter 8: Sorting in Linear Time  8-17  The height of the decision tree is equal to the worst-case number of comparisons the algorithm has to make to determine the matching. To bound that size, let us  cid:222 rst compute the number of possible matchings for n red and n blue jugs. If we label the red jugs from 1 to n and we also label the blue jugs from 1 to n before starting the comparisons, every outcome of the algorithm can be represented as a set { i, π i    : 1 ≤ i ≤ n and π is a permutation on{1, . . . , n}} , which contains the pairs of red jugs   cid:222 rst component  and blue jugs  second component  that are matched up. Since every permutation π corresponds to a different outcome, there must be exactly n! different results. Now we can bound the height h of our decision tree. Every tree with a branch- ing factor of 3  every inner node has at most three children  has at most 3h leaves. Since the decison tree must have at least n! children, it follows that 3h ≥ n! ≥  n e n ⇒ h ≥ n log3 n − n log3 e =  cid:3  n lg n  . So any algorithm solving the problem must use  cid:3  n lg n  comparisons.  c. Assume that the red jugs are labeled with numbers 1, 2, . . . , n and so are the blue jugs. The numbers are arbitrary and do not correspond to the volumes of jugs, but are just used to refer to the jugs in the algorithm description. Moreover, the output of the algorithm will consist of n distinct pairs  i, j  , where the red jug i and the blue jug j have the same volume. The procedure MATCH-JUGS takes as input two sets representing jugs to be matched: R ⊆ {1, . . . , n}, representing red jugs, and B ⊆ {1, . . . , n}, rep- resenting blue jugs. We will call the procedure only with inputs that can be matched; one necessary condition is thatR = B. MATCH-JUGS R, B  if R = 0 then return if R = 1 then let R = {r} and B = {b}   cid:1  Sets contain just one jug each   cid:1  Sets are empty  output  cid:147  r, b  cid:148  return  else r ← a randomly chosen jug in R  compare r to every jug of B B< ← the set of jugs in B that are smaller than r B> ← the set of jugs in B that are larger than r b ← the one jug in B with the same size as r compare b to every jug of R −{r} R< ← the set of jugs in R that are smaller than b R> ← the set of jugs in R that are larger than b output  cid:147  r, b  cid:148  MATCH-JUGS R<, B<  MATCH-JUGS R>, B>   Correctness can be seen as follows  remember that R = B in each call . Once we pick r randomly from R, there will be a matching among the jugs in   8-18  Solutions for Chapter 8: Sorting in Linear Time  volume smaller than r  which are in the sets R< and B< , and likewise between the jugs larger than r  which are in R> and B> . Termination is also easy to see: since R  < R in every recursive step, the size of the  cid:222 rst parameter reduces with every recursive call. It eventually must reach 0 or 1, in which case the recursion terminates. What about the running time? The analysis of the expected number of com- parisons is similar to that of the quicksort algorithm in Section 7.4.2. Let us order the jugs as r1, . . . , rn and b1, . . . , bn where ri < ri+1 and bi < bi+1 for i = 1, . . . , n, and ri = bi . Our analysis uses indicator random variables Xi j = I{red jug ri is compared to blue jug bj} . As in quicksort, a given pair ri and b j is compared at most once. When we compare ri to every jug in B, jug ri will not be put in either R . When we compare bi to every jug in R − {ri}, jug bi is not put into either B . The total number of comparisons is  n cid:2   i=1  X = n−1 cid:2  E [X] = n−1 cid:2   j=i+1  Xi j .  n cid:2   i=1  j=i+1  To calculate the expected value of X, we follow the quicksort analysis to arrive at  Pr{ri is compared to bj} .  As in the quicksort analysis, once we choose a jug rk such that ri < rk < b j , we will put ri in R , and so ri and b j will never be compared again. Let us denote Ri j = {ri , . . . , r j}. Then jugs ri and b j will be compared if and only if the  cid:222 rst jug in Ri j to be chosen is either ri or r j . Still following the quicksort analysis, until a jug from Ri j is chosen, the entire set Ri j is together. Any jug in Ri j is equally likely to be  cid:222 rst one chosen. Since Ri j = j − i + 1, the probability of any given jug being the  cid:222 rst one chosen in Ri j is 1   j−i+1 . The remainder of the analysis is the same as the quicksort analysis, and we arrive at the solution of O n lg n  comparisons. Just like in quicksort, in the worst case we always choose the largest  or small- est  jug to partition the sets, which reduces the set sizes by only 1. The running time then obeys the recurrence T  n  = T  n − 1  +  cid:2  n , and the number of comparisons we make in the worst case is T  n  =  cid:2  n2 .   Lecture Notes for Chapter 9: Medians and Order Statistics  Chapter 9 overview     ith order statistic is the ith smallest element of a set of n elements.    The minimum is the  cid:222 rst order statistic  i = 1 .   The maximum is the nth order statistic  i = n .   A median is the  cid:147 halfway point cid:148  of the set.   When n is odd, the median is unique, at i =  n + 1  2.   When n is even, there are two medians:   The lower median, at i = n 2, and   The upper median, at i = n 2 + 1.   We mean lower median when we use the phrase  cid:147 the median. cid:148   The selection problem: Input: A set A of n distinct numbers and a number i, with 1 ≤ i ≤ n. Output: The element x ∈ A that is larger than exactly i − 1 other elements in A.  In other words, the ith smallest element of A.  The selection problem can be solved in O n lg n  time.   Sort the numbers using an O n lg n -time algorithm, such as heapsort or merge  sort.    Then return the ith element in the sorted array. There are faster algorithms, however.   First, we cid:146 ll look at the problem of selecting the minimum and maximum of a    Then, we cid:146 ll look at a simple general selection algorithm with a time bound of  set of elements.  O n  in the average case.    Finally, we cid:146 ll look at a more complicated general selection algorithm with a  time bound of O n  in the worst case.   9-2  Lecture Notes for Chapter 9: Medians and Order Statistics  Minimum and maximum  We can easily obtain an upper bound of n−1 comparisons for  cid:222 nding the minimum of a set of n elements.   Examine each element in turn and keep track of the smallest one.   This is the best we can do, because each element, except the minimum, must be  compared to a smaller element at least once.  The following pseudocode  cid:222 nds the minimum element in array A[1 . . n]:  MINIMUM A, n  min ← A[1] for i ← 2 to n  do if min > A[i]  then min ← A[i]  return min  The maximum can be found in exactly the same way by replacing the > with < in the above algorithm.  Simultaneous minimum and maximum  Some applications need both the minimum and maximum of a set of elements.   For example, a graphics program may need to scale a set of  x, y  data to  cid:222 t onto a rectangular display. To do so, the program must  cid:222 rst  cid:222 nd the minimum and maximum of each coordinate.  A simple algorithm to  cid:222 nd the minimum and maximum is to  cid:222 nd each one indepen- dently. There will be n − 1 comparisons for the minimum and n − 1 comparisons for the maximum, for a total of 2n − 2 comparisons. This will result in  cid:2  n  time. In fact, at most 3 cid:9 n 2 cid:10  comparisons are needed to  cid:222 nd both the minimum and maximum:   Maintain the minimum and maximum of elements seen so far.   Don cid:146 t compare each element to the minimum and maximum separately.   Process elements in pairs.   Compare the elements of a pair to each other.   Then compare the larger element to the maximum so far, and compare the  smaller element to the minimum so far.  This leads to only 3 comparisons for every 2 elements. Setting up the initial values for the min and max depends on whether n is odd or even.    If n is even, compare the  cid:222 rst two elements and assign the larger to max and the smaller to min. Then process the rest of the elements in pairs. If n is odd, set both min and max to the  cid:222 rst element. Then process the rest of the elements in pairs.      Selection in expected linear time  Lecture Notes for Chapter 9: Medians and Order Statistics  9-3  Analysis of the total number of comparisons        2  If n is even, we do 1 initial comparison and then 3 n − 2  2 more comparisons.  of comparisons = 3 n − 2  + 1 = 3n − 6 + 1 2 − 3 + 1 = 3n 2 − 2 . = 3n 2  If n is odd, we do 3 n − 1  2 = 3 cid:9 n 2 cid:10  comparisons.  In either case, the maximum number of comparisons is ≤ 3 cid:9 n 2 cid:10 .  Selection of the ith smallest element of the array A can be done in  cid:2  n  time. The function RANDOMIZED-SELECT uses RANDOMIZED-PARTITION from the quicksort algorithm in Chapter 7. RANDOMIZED-SELECT differs from quicksort because it recurses on one side of the partition only.   cid:1  pivot value is the answer  RANDOMIZED-SELECT  A, p, r, i   if p = r then return A[p] q ← RANDOMIZED-PARTITION A, p, r   k ← q − p + 1 if i = k then return A[q] elseif i < k then return RANDOMIZED-SELECT  A, p, q − 1, i   else return RANDOMIZED-SELECT A, q + 1, r, i − k  After the call to RANDOMIZED-PARTITION, the array is partitioned into two sub- arrays A[ p . . q − 1] and A[q + 1 . . r], along with a pivot element A[q].   The elements of subarray A[p . . q − 1] are all ≤ A[q].   The elements of subarray A[q + 1 . . r] are all > A[q].   The pivot element is the kth element of the subarray A[p . . r], where k =  q − p + 1. If the pivot element is the ith smallest element  i.e., i = k , return A[q].     Otherwise, recurse on the subarray containing the ith smallest element.  If i < k, this subarray is A[p . . q − 1], and we want the ith smallest element. If i > k, this subarray is A[q + 1 . . r] and, since there are k elements in A[p . . r] that precede A[q + 1 . . r], we want the  i − k th smallest element of this subarray.        9-4  Lecture Notes for Chapter 9: Medians and Order Statistics  Analysis  Worst-case running time:  cid:2  n2 , because we could be extremely unlucky and always recurse on a subarray that is only 1 element smaller than the previous sub- array.  pivot.  Expected running time: RANDOMIZED-SELECT works well on average. Because it is randomized, no particular input brings out the worst-case behavior consis- tently. The running time of RANDOMIZED-SELECT is a random variable that we denote by T  n . We obtain an upper bound on E[T  n ] as follows:   RANDOMIZED-PARTITION is equally likely to return any element of A as the   For each k such that 1 ≤ k ≤ n, the subarray A[p . . q] has k elements  all ≤ pivot  with probability 1 n. [Note that we cid:146 re nowconsidering a subarray that includesthepivot,alongwithelementslessthanthepivot.]   For k = 1, 2, . . . , n, de cid:222 ne indicator random variable Xk = I{subarray A[p . . q] has exactly k elements} . that E [Xk] = 1 n.   When we call RANDOMIZED-SELECT, we don cid:146 t know if it will terminate im- mediately with the correct answer, recurse on A[p . . q − 1], or recurse on A[q + 1 . . r]. It depends on whether the ith smallest element is less than, equal to, or greater than the pivot element A[q].    Since Pr{subarray A[p . . q] has exactly k elements} = 1 n, Lemma 5.1 says  and that the ith smallest element is always in the larger subarray. and Xk = 0 for all other k.    To obtain an upper bound, we assume that T  n  is monotonically increasing   For a given call of RANDOMIZED-SELECT, Xk = 1 for exactly one value of k,   When Xk = 1, the two subarrays have sizes k − 1 and n − k.   For a subproblem of size n, RANDOMIZED-PARTITION takes O n  time. [Ac- tually,ittakes cid:2  n  time,but O n  suf cid:222 ces,sincewe cid:146 reobtainingonlyanupper boundontheexpectedrunningtime.]    Therefore, we have the recurrence  k=1  T  n  ≤ n cid:2  = n cid:2   cid:13  n cid:2  = n cid:2   E [T  n ] ≤ E  k=1  k=1  Xk ·  T  max k − 1, n − k   + O n   Xk · T  max k − 1, n − k   + O n  .  cid:14  Xk · T  max k − 1, n − k   + O n  k=1 E [Xk · T  max k − 1, n − k  ] + O n     Taking expected values gives   linearity of expectation    = n cid:2  = n cid:2   k=1  k=1  1 n  Lecture Notes for Chapter 9: Medians and Order Statistics  9-5  E [Xk] · E [T  max k − 1, n − k  ] + O n   equation  C.23   · E [T  max k − 1, n − k  ] + O n  .    We rely on Xk and T  max k − 1, n − k   being independent random variables   Looking at the expression max k − 1, n − k , we have  in order to apply equation  C.23 .   cid:9  k − 1 if k >  cid:14 n 2 cid:15  , if k ≤  cid:14 n 2 cid:15  . n − k  max k − 1, n − k  =        If n is even, each term from T   cid:14 n 2 cid:15   up to T  n − 1  appears exactly twice in the summation. If n is odd, these terms appear twice and T   cid:9 n 2 cid:10   appears once.  n−1 cid:2     Either way, E [T  n ] ≤ 2 n  E [T  k ] + O n  .   Solve this recurrence by substitution:  k= cid:9 n 2 cid:10   of the recurrence.    Guess that T  n  ≤ cn for some constant c that satis cid:222 es the initial conditions   Assume that T  n  = O 1  for n < some constant. We cid:146 ll pick this constant   Also pick a constant a such that the function described by the O n  term is  later.    Using this guess and constants c and a, we have  k  2   cid:4   k= cid:9 n 2 cid:10   + an  bounded from above by an for all n > 0.  E [T  n ] ≤ 2 n = 2c n = 2c n ≤ 2c n = 2c n = c n = c ≤ 3cn 4 = cn −  n−1 cid:2   cid:3  n−1 cid:2   cid:5  k − k=1  n − 1 n  cid:5   n − 1 n  cid:5  2 n2 − n  cid:5   cid:5   ck + an  cid:9 n 2 cid:10 −1 cid:2   cid:6  k=1 −   cid:9 n 2 cid:10  − 1  cid:9 n 2 cid:10   cid:6  + an −  n 2 − 2  n 2 − 1   cid:6  + an − n2 4 − 3n 2 + 2  cid:6  + an 2  cid:6  − 2 + n 3n2 4 2 + 1 − 2 3n 2 4 n + c + an 2 − c cn 2 4  2 + an + an  cid:8   − an   cid:7   2  2  .   9-6  Lecture Notes for Chapter 9: Medians and Order Statistics    To complete this proof, we choose c such that  cn 4 − c 2 − an ≥ 0 cn 4 − an ≥ c 2 n c 4 − a  ≥ c 2 c 2 c 4 − a 2c c − 4a  n ≥ n ≥  .  E [T  n ] = O n .    Thus, as long as we assume that T  n  = O 1  for n < 2c  c − 4a , we have  Therefore, we can determine any order statistic in linear time on average.  Selection in worst-case linear time  We can  cid:222 nd the ith smallest element in O n  time in the worst case. We cid:146 ll describe a procedure SELECT that does so. SELECT recursively partitions the input array.     Will use the deterministic procedure PARTITION, but with a small modi cid:222 ca- tion. Instead of assuming that the last element of the subarray is the pivot, the modi cid:222 ed PARTITION procedure is told which element to use as the pivot.  Idea: Guarantee a good split when the array is partitioned.  SELECT works on an array of n > 1 elements. It executes the following steps: 1. Divide the n elements into groups of 5. Get cid:14 n 5 cid:15  groups:  cid:9 n 5 cid:10  groups with exactly 5 elements and, if 5 does not divide n, one group with the remaining n mod 5 elements.  2. Find the median of each of the cid:14 n 5 cid:15  groups:    Run insertion sort on each group. Takes O 1  time per group since each  group has ≤ 5 elements.  3. Find the median x of the  cid:14 n 5 cid:15  medians by a recursive call to SELECT.    Then just pick the median from each group, in O 1  time.  cid:14 n 5 cid:15  is even, then follow our convention and  cid:222 nd the lower median.    If  4. Using the modi cid:222 ed version of PARTITION that takes the pivot element as input, partition the input array around x. Let x be the kth element of the array after partitioning, so that there are k − 1 elements on the low side of the partition and n − k elements on the high side. 5. Now there are three possibilities:  If i = k, just return x. If i < k, return the ith smallest element on the low side of the partition by making a recursive call to SELECT. If i > k, return the  i −k th smallest element on the high side of the partition by making a recursive call to SELECT.           Lecture Notes for Chapter 9: Medians and Order Statistics  9-7  Start by getting a lower bound on the number of elements that are greater than the partitioning element x:  Analysis  x  [Each group is a column. Each white circle is the median of a group, as found instep2. Arrowsgofromlarger elements tosmallerelements, basedonwhatwe knowafterstep4.Elementsintheregiononthelowerrightareknowntobegreater than x.]   At least half of the medians found in step 2 are ≥ x.   Look at the groups containing these medians that are ≥ x. All of them con- tribute 3 elements that are > x  the median of the group and the 2 elements in the group greater than the group cid:146 s median , except for 2 of the groups: the group containing x  which has only 2 elements > x  and the group with < 5 elements. − 2 groups with 3 ele-    Forget about these 2 groups. That leaves ≥   cid:18  cid:20    cid:19    cid:17   1 2  n 5   cid:17    cid:5  cid:19   ments known to be > x.   cid:18  cid:20    cid:6  − 2    Thus, we know that at least ≥ 3n 10  n 5  1 2  3  − 6  elements are > x.  Symmetrically, the number of elements that are < x is at least 3n 10 − 6. Therefore, when we call SELECT recursively in step 5, it cid:146 s on ≤ 7n 10 + 6 ele- ments. Develop a recurrence for the worst-case running time of SELECT:   Steps 1, 2, and 4 each take O n  time:    Step 1: making groups of 5 elements takes O n  time.   Step 2: sorting  cid:14 n 5 cid:15  groups in O 1  time each.   Step 4: partitioning the n-element array around x takes O n  time.    Step 3 takes time T   cid:14 n 5 cid:15  .   Step 5 takes time ≤ T  7n 10 + 6 , assuming that T  n  is monotonically in-  creasing.   9-8  Lecture Notes for Chapter 9: Medians and Order Statistics    Assume that T  n  = O 1  for small enough n. We cid:146 ll use n < 140 as  cid:147 small  enough. cid:148  Why 140? We cid:146 ll see why later.    Thus, we get the recurrence   cid:9   T  n  ≤  O 1  T   cid:14 n 5 cid:15   + T  7n 10 + 6  + O n   if n < 140 , if n ≥ 140 .    Substitute the inductive hypothesis in the right-hand side of the recurrence:    Pick a constant a such that the function described by the O n  term in the  Solve this recurrence by substitution:     Assume that c is large enough that T  n  ≤ cn for all n < 140. So we are  Inductive hypothesis: T  n  ≤ cn for some constant c and all n > 0. concerned only with the case n ≥ 140. recurrence is ≤ an for all n > 0. T  n  ≤ c cid:14 n 5 cid:15  + c 7n 10 + 6  + an ≤ cn 5 + c + 7cn 10 + 6c + an = 9cn 10 + 7c + an = cn +  −cn 10 + 7c + an  .   This last quantity is ≤ cn if −cn 10 + 7c + an ≤ 0 cn 10 − 7c ≥ an cn − 70c ≥ 10an c n − 70  ≥ 10an  c ≥ 10a n  n − 70   .  which in turn gives us the condition we need to show that T  n  ≤ cn.    Because we assumed that n ≥ 140, we have n  n − 70  ≤ 2.   Thus, 20a ≥ 10a n  n−70  , so choosing c ≥ 20a gives c ≥ 10a n  n−70  ,   We conclude that T  n  = O n , so that SELECT runs in linear time in all cases.   Why 140? We could have used any integer strictly greater than 70.   Observe that for n > 70, the fraction n  n − 70  decreases as n increases.   We picked n ≥ 140 so that the fraction would be ≤ 2, which is an easy constant to work with.   We could have picked, say, n ≥ 71, so that for all n ≥ 71, the fraction would be ≤ 71  71 − 70  = 71. Then we would have had 20a ≥ 710a, so we cid:146 d have needed to choose c ≥ 710a.  Notice that SELECT and RANDOMIZED-SELECT determine information about the relative order of elements only by comparing elements.   Sorting requires  cid:3  n lg n  time in the comparison model.   Sorting algorithms that run in linear time need to make assumptions about their  input.  input.    Linear-time selection algorithms do not require any assumptions about their    Linear-time selection algorithms solve the selection problem without sorting  and therefore are not subject to the  cid:3  n lg n  lower bound.   Solutions for Chapter 9: Medians and Order Statistics  Solution to Exercise 9.1-1  The smallest of n numbers can be found with n − 1 comparisons by conducting a tournament as follows: Compare all the numbers in pairs. Only the smaller of each pair could possibly be the smallest of all n, so the problem has been reduced to that of  cid:222 nding the smallest of cid:14 n 2 cid:15  numbers. Compare those numbers in pairs, and so on, until there cid:146 s just one number left, which is the answer. To see that this algorithm does exactly n − 1 comparisons, notice that each number except the smallest loses exactly once. To show this more formally, draw a binary tree of the comparisons the algorithm does. The n numbers are the leaves, and each number that came out smaller in a comparison is the parent of the two numbers that were compared. Each non-leaf node of the tree represents a comparison, and there are n − 1 internal nodes in an n-leaf full binary tree  see Exercise  B.5-3  , so exactly n − 1 comparisons are made. In the search for the smallest number, the second smallest number must have come out smallest in every comparison made with it until it was eventually compared with the smallest. So the second smallest is among the elements that were com- pared with the smallest during the tournament. To  cid:222 nd it, conduct another tourna- ment  as above  to  cid:222 nd the smallest of these numbers. At most cid:14 lg n cid:15   the height of the tree of comparisons  elements were compared with the smallest, so  cid:222 nding the smallest of these takes  cid:14 lg n cid:15  − 1 comparisons in the worst case. The total number of comparisons made in the two tournaments was n − 1 +  cid:14 lg n cid:15  − 1 = n +  cid:14 lg n cid:15  − 2 in the worst case.  Solution to Exercise 9.3-1  For groups of 7, the algorithm still works in linear time. The number of elements greater than x  and similarly, the number less than x  is at least   cid:5  cid:19    cid:17   1 2  n 7   cid:18  cid:20    cid:6  − 2  4  ≥ 2n 7  − 8 ,   9-10  Solutions for Chapter 9: Medians and Order Statistics   cid:17    cid:18  cid:20    cid:5  cid:19    cid:6  − 2  and the recurrence becomes T  n  ≤ T   cid:14 n 7 cid:15   + T  5n 7 + 8  + O n  , which can be shown to be O n  by substitution, as for the groups of 5 case in the text. For groups of 3, however, the algorithm no longer works in linear time. The number of elements greater than x, and the number of elements less than x, is at least  2  1 2  n 3  ≥ n 3  − 4 , and the recurrence becomes T  n  ≤ T   cid:14 n 3 cid:15   + T  2n 3 + 4  + O n  , which does not have a linear solution. We can prove that the worst-case time for groups of 3 is  cid:3  n lg n . We do so by  cid:21   cid:22  cid:22  deriving a recurrence for a particular case that takes  cid:3  n lg n  time. In counting up the number of elements greater than x  and similarly, the num- ber less than x , consider the particular case in which there are exactly  cid:11  + 1  the −1 discounts x cid:146 s group, as usual, and the +1 is con-  cid:10  cid:21  groups with medians ≥ x and in which the  cid:147 leftover cid:148  group does contribute 2 elements greater than x. Then the number of elements greater than x is exactly 2 tributed by x cid:146 s group  = 2 cid:14 n 6 cid:15  − 1, and the recursive step for elements ≤ x has n −  2 cid:14 n 6 cid:15  − 1  ≥ n −  2 n 6 + 1  − 1  = 2n 3 − 1 elements. Observe also that the O n  term in the recurrence is really  cid:2  n , since the partitioning in step 4 takes  cid:2  n   not just O n   time. Thus, we get the recurrence T  n  ≥ T   cid:14 n 3 cid:15   + T  2n 3 − 1  +  cid:2  n  ≥ T  n 3  + T  2n 3 − 1  +  cid:2  n  , from which you can show that T  n  ≥ cn lg n by substitution. You can also see that T  n  is nonlinear by noticing that each level of the recursion tree sums to n. [Infact,anyoddgroupsize≥ 5 worksinlineartime.]   cid:22  cid:22  − 1   cid:21    cid:21   n 3  n 3  1 2  1 2  Solution to Exercise 9.3-3  A modi cid:222 cation to quicksort that allows it to run in O n lg n  time in the worst case uses the deterministic PARTITION algorithm that was modi cid:222 ed to take an element to partition around as an input parameter. SELECT takes an array A, the bounds p and r of the subarray in A, and the rank i of an order statistic, and in time linear in the size of the subarray A[p . . r] it returns the ith smallest element in A[p . . r].  BEST-CASE-QUICKSORT  A, p, r   if p < r then i ←  cid:9  r − p + 1  2 cid:10  x ← SELECT A, p, r, i   q ← PARTITION x  BEST-CASE-QUICKSORT A, p, q − 1  BEST-CASE-QUICKSORT A, q + 1, r     Solutions for Chapter 9: Medians and Order Statistics  9-11  For an n-element array, the largest subarray that BEST-CASE-QUICKSORT recurses on has n 2 elements. This situation occurs when n = r − p + 1 is even; then the subarray A[q +1 . . r] has n 2 elements, and the subarray A[p . . q −1] has n 2−1 elements. Because BEST-CASE-QUICKSORT always recurses on subarrays that are at most half the size of the original array, the recurrence for the worst-case running time is T  n  ≤ 2T  n 2  +  cid:2  n  = O n lg n .  Solution to Exercise 9.3-5  We assume that are given a procedure MEDIAN that takes as parameters an ar- ray A and subarray indices p and r, and returns the value of the median element of A[ p . . r] in O n  time in the worst case.  cid:4  Given MEDIAN, here is a linear-time algorithm SELECT for  cid:222 nding the ith small- est element in A[p . . r]. This algorithm uses the deterministic PARTITION algo- rithm that was modi cid:222 ed to take an element to partition around as an input parame- ter.   cid:4  A, p, r, i    SELECT if p = r then return A[p] x ← MEDIAN A, p, r   q ← PARTITION x  k ← q − p + 1 if i = k then return A[q]  elseif i < k  then return SELECT   cid:4  A, p, q − 1, i    cid:4  A, q + 1, r, i − k   else return SELECT Because x is the median of A[p . . r], each of the subarrays A[p . . q − 1] and A[q + 1 . . r] has at most half the number of elements of A[p . . r]. The recurrence is T  n  ≤ T  n 2  + O n  = O n .  cid:4  for the worst-case running time of SELECT  Solution to Exercise 9.3-8  Let cid:146 s start out by supposing that the median  the lower median, since we know we have an even number of elements  is in X. Let cid:146 s call the median value m, and let cid:146 s suppose that it cid:146 s in X[k]. Then k elements of X are less than or equal to m and n − k elements of X are greater than or equal to m. We know that in the two arrays combined, there must be n elements less than or equal to m and n elements greater than or equal to m, and so there must be n − k elements of Y that are less than or equal to m and n −  n − k  = k elements of Y that are greater than or equal to m.   9-12  Solutions for Chapter 9: Medians and Order Statistics  ], where k  Thus, we can check that X[k] is the lower median by checking whether Y [n− k] ≤ X[k] ≤ Y [n − k + 1]. A boundary case occurs for k = n. Then n − k = 0, and there is no array entry Y [0]; we only need to check that X[n] ≤ Y [1]. Now, if the median is in X but is not in X[k], then the above condition will not  cid:4   cid:4  < k, then X[k] is above the median, and hold. If the median is in X[k Y [n − k + 1] < X[k]. Conversely, if the median is in X[k  cid:4  cid:4  > k, then  cid:4  cid:4  X[k] is below the median, and X[k] < Y [n − k]. Thus, we can use a binary search to determine whether there is an X[k] such that either k < n and Y [n−k] ≤ X[k] ≤ Y [n−k+1] or k = n and X[k] ≤ Y [n−k+1]; if we  cid:222 nd such an X[k], then it is the median. Otherwise, we know that the median is in Y , and we use a binary search to  cid:222 nd a Y [k] such that either k < n and X[n − k] ≤ Y [k] ≤ X[n − k + 1] or k = n and Y [k] ≤ X[n − k + 1]; such a Y [k] is the median. Since each binary search takes O lg n  time, we spend a total of O lg n  time. Here cid:146 s how we write the algorithm in pseudocode:  ], where k  TWO-ARRAY-MEDIAN X, Y   n ← length[X] median ← FIND-MEDIAN X, Y, n, 1, n  if median = NOT-FOUND then median ← FIND-MEDIAN Y, X, n, 1, n  return median   cid:1  n also equals length[Y ]  FIND-MEDIAN A, B, n, low, high  if low > high then return NOT-FOUND else k ←  cid:9  low+ high  2 cid:10   if k = n and A[n] ≤ B[1] then return A[n] elseif k < n and B[n − k] ≤ A[k] ≤ B[n − k + 1] then return A[k] elseif A[k] > B[n − k + 1] then return FIND-MEDIAN A, B, n, low, k − 1  else return FIND-MEDIAN A, B, n, k + 1, high   Solution to Exercise 9.3-9  In order to  cid:222 nd the optimal placement for Professor Olay cid:146 s pipeline, we need only  cid:222 nd the median s  of the y-coordinates of his oil wells, as the following proof explains.  Claim The optimal y-coordinate for Professor Olay cid:146 s east-west oil pipeline is as follows:   If n is even, then on either the oil well whose y-coordinate is the lower median or the one whose y-coordinate is the upper median, or anywhere between them. If n is odd, then on the oil well whose y-coordinate is the median.      Solutions for Chapter 9: Medians and Order Statistics  9-13   cid:4   will denote the sum after moving the pipeline.  Proof We examine various cases. In each case, we will start out with the pipeline at a particular y-coordinate and see what happens when we move it. We cid:146 ll denote by s the sum of the north-south spurs with the pipeline at the starting location, and s We start with the case in which n is even. Let us start with the pipeline somewhere on or between the two oil wells whose y-coordinates are the lower and upper me- dians. If we move the pipeline by a vertical distance d without crossing either of the median wells, then n 2 of the wells become d farther from the pipeline and  cid:4  = s + dn 2 − dn 2 = s; thus, all locations on or n 2 become d closer, and so s between the two medians are equally good. Now suppose that the pipeline goes through the oil well whose y-coordinate is the upper median. What happens when we increase the y-coordinate of the pipeline by d > 0 units, so that it moves above the oil well that achieves the upper median? All oil wells whose y-coordinates are at or below the upper median become d units farther from the pipeline, and there are at least n 2 + 1 such oil wells  the upper median, and every well at or below the lower median . There are at most n 2 − 1 oil wells whose y-coordinates are above the upper median, and each of these oil wells becomes at most d units closer to the pipeline when it moves up. Thus, we  cid:4  ≥ s + d n 2 + 1  − d n 2 − 1  = s + 2d > s.  cid:4  have a lower bound on s We conclude that moving the pipeline up from the oil well at the upper median increases the total spur length. A symmetric argument shows that if we start with the pipeline going through the oil well whose y-coordinate is the lower median and move it down, then the total spur length increases. We see, therefore, that when n is even, an optimal placement of the pipeline is anywhere on or between the two medians. Now we consider the case when n is odd. We start with the pipeline going through the oil well whose y-coordinate is the median, and we consider what happens when we move it up by d > 0 units. All oil wells at or below the median become d units farther from the pipeline, and there are at least  n + 1  2 such wells  the one at the median and the  n − 1  2 at or below the median. There are at most  n − 1  2 oil wells above the median, and each of these becomes at most d units closer to the  cid:4  ≥ s + d n + 1  2 − d n − 1  2 =  cid:4  pipeline. We get a lower bound on s s + d > s, and we conclude that moving the pipeline up from the oil well at the median increases the total spur length. A symmetric argument shows that moving the pipeline down from the median also increases the total spur length, and so the  claim  optimal placement of the pipeline is on the median.  of s  of s  Since we know we are looking for the median, we can use the linear-time median-  cid:222 nding algorithm.  Solution to Problem 9-1  We assume that the numbers start out in an array.  a. Sort the numbers using merge sort or heapsort, which take  cid:2  n lg n  worst-case time.  Don cid:146 t use quicksort or insertion sort, which can take  cid:2  n2  time.  Put   9-14  Solutions for Chapter 9: Medians and Order Statistics  b.  the i largest elements  directly accessible in the sorted array  into the output array, taking  cid:2  i   time. Total worst-case running time:  cid:2  n lg n + i   =  cid:2  n lg n   because i ≤ n . Implement the priority queue as a heap. Build the heap using BUILD-HEAP, which takes  cid:2  n  time, then call HEAP-EXTRACT-MAX i times to get the i largest elements, in  cid:2  i lg n  worst-case time, and store them in reverse order of extraction in the output array. The worst-case extraction time is  cid:2  i lg n  because i extractions from a heap with O n  elements takes i · O lg n  = O i lg n    time, and   half of the i extractions are from a heap with ≥ n 2 elements, so those i  2 extractions take  i  2  cid:3  lg n 2   =  cid:3  i lg n  time in the worst case. Total worst-case running time:  cid:2  n + i lg n .  c. Use the SELECT algorithm of Section 9.3 to  cid:222 nd the ith largest number in  cid:2  n  time. Partition around that number in  cid:2  n  time. Sort the i largest numbers in  cid:2  i lg i   worst-case time  with merge sort or heapsort . Total worst-case running time:  cid:2  n + i lg i  .  Note that method  c  is always asymptotically at least as good as the other two methods, and that method  b  is asymptotically at least as good as  a .  Com- paring  c  to  b  is easy, but it is less obvious how to compare  c  and  b  to  a .  c  and  b  are asymptotically at least as good as  a  because n, i lg i, and i lg n are all O n lg n . The sum of two things that are O n lg n  is also O n lg n .   Solution to Problem 9-2  a. The median x of the elements x1, x2, . . . , xn, is an element x = xk satisfying {xi : 1 ≤ i ≤ n and xi   x} ≤ n 2.  cid:2  If each element xi is assigned a weight wi = 1 n, then we get   cid:2   wi =  xi <x  1 n   cid:2   xi <x  1  xi <x  ·  = 1 n = 1 n ≤ 1 n = 1 2  and cid:2   xi >x  wi =  ,   cid:2   xi >x  1 n  · {xi : 1 ≤ i ≤ n and xi < x} · n 2   Solutions for Chapter 9: Medians and Order Statistics  9-15   cid:2   xi >x  1  ·  ,  = 1 n = 1 n ≤ 1 n = 1 2  · {xi : 1 ≤ i ≤ n and xi > x} · n 2  which proves that x is also the weighted median of x1, x2, . . . , xn with weights wi = 1 n, for i = 1, 2, . . . , n.  b. We  cid:222 rst sort the n elements into increasing order by xi values. Then we scan the array of sorted xi  cid:146 s, starting with the smallest element and accumulating weights as we scan, until the total exceeds 1 2. The last element, say xk, whose weight caused the total to exceed 1 2, is the weighted median. Notice that the total weight of all elements smaller than xk is less than 1 2, because xk was the  cid:222 rst element that caused the total weight to exceed 1 2. Similarly, the total weight of all elements larger than xk is also less than 1 2, because the total weight of all the other elements exceeds 1 2. The sorting phase can be done in O n lg n  worst-case time  using merge sort or heapsort , and the scanning phase takes O n  time. The total running time in the worst case, therefore, is O n lg n .  c. We  cid:222 nd the weighted median in  cid:2  n  worst-case time using the  cid:2  n  worst- case median algorithm in Section 9.3.  Although the  cid:222 rst paragraph of the section only claims an O n  upper bound, it is easy to see that the more precise running time of  cid:2  n  applies as well, since steps 1, 2, and 4 of SELECT actually take  cid:2  n  time.  The weighted-median algorithm works as follows. If n ≤ 2, we just return the brute-force solution. Otherwise, we proceed as follows. We  cid:222 nd the actual median xk of the n elements and then partition around it. We then compute the total weights of the two halves. If the weights of the two halves are each strictly less than 1 2, then the weighted median is xk. Otherwise, the weighted median should be in the half with total weight exceeding 1 2. The total weight of the  cid:147 light cid:148  half is lumped into the weight of xk, and the search continues within the half that weighs more than 1 2. Here cid:146 s pseudocode, which takes as input a set X = {x1, x2, . . . , xn}:   9-16  Solutions for Chapter 9: Medians and Order Statistics  WEIGHTED-MEDIAN X   if n = 1 then return x1 elseif n = 2 then if w1 ≥ w2  else  then return x1 else return x2  cid:222 nd the median xk of X = {x1, x2, . . . , xn} partition the set X around xk  compute WL = cid:1   wi and WG = cid:1   xi <xk  if WL < 1 2 and WG < 1 2  then return xk elseif WL > 1 2 then wk ← wk + WG  else wk ← wk + WL   cid:4  ← {xi ∈ X : xi ≤ xk} X return WEIGHTED-MEDIAN X  cid:4  ← {xi ∈ X : xi ≥ xk} X return WEIGHTED-MEDIAN X   cid:4     cid:4    wi  xi >xk  The recurrence for the worst-case running time of WEIGHTED-MEDIAN is T  n  = T  n 2+ 1 +  cid:2  n , since there is at most one recursive call on half the number of elements, plus the median element xk, and all the work preceding the recursive call takes  cid:2  n  time. The solution of the recurrence is T  n  =  cid:2  n . d. Let the n points be denoted by their coordinates x1, x2, . . . , xn, let the corre- sponding weights be w1, w2, . . . , wn, and let x = xk be the weighted median. wi p − xi; we want to  cid:222 nd a point p such that f   p  is minimum. Let y be any point  real number  other than x. We show the optimality of the weighted median x by showing that f  y − f  x  ≥ 0. We examine separately the cases in which y > x and x > y. For any x and y, we have  For any point p, let f   p  = cid:1  f  y  − f  x  = n cid:2  = n cid:2   wi y − xi − n cid:2   wi  y − xi − x − xi  .  wi x − xi  n i=1  i=1  i=1  i=1  When y > x, we bound the quantity y − xi − x − xi from below by exam- ining three cases: 1. x < y ≤ xi : Here, x − y+y − xi = x − xi and x − y = y− x, which imply that y − xi − x − xi = −x − y = x − y. 2. x < xi ≤ y: Here, y − xi ≥ 0 and xi − x ≤ y − x, which imply that y − xi − x − xi ≥ − y − x  = x − y. 3. xi ≤ x < y: Here, x − xi+y − x = y − xi and y − x = y− x, which imply that y − xi − x − xi = y − x = y − x.   Solutions for Chapter 9: Medians and Order Statistics  9-17  Separating out the  cid:222 rst two cases, in which x < xi , from the third case, in which x ≥ xi , we get  f  y  − f  x  = n cid:2   cid:2   i=1  ≥   cid:2  wi  y − xi − x − xi   cid:4  wi  y − x  wi  x − y  +  cid:2   cid:1   x≥xi wi −   cid:3  cid:2   cid:1   x≥xi  x <xi  wi  .  x <xi   cid:1  =  y − x   x <xi  xi <x  x≥xi  wi < 1 2 implies that  wi ≥ 1 2. This fact, The property that wi ≤ 1 2, yields that f  y  − f  x  ≥ 0. combined with y − x > 0 and When x > y, we again bound the quantity y − xi − x − xi from below by examining three cases: 1. xi ≤ y < x: Here, y − xi+x − y = x − xi and x − y = x − y, which 2. y ≤ xi < x: Here, y − xi ≥ 0 and x − xi ≤ x − y, which imply that 3. y < x ≤ xi . Here, x − y+x − xi = y − xi and x − y = x − y, which  imply that y − xi − x − xi = −x − y = y − x. y − xi − x − xi ≥ − x − y  = y − x. imply that y − xi − x − xi = x − y = x − y. Separating out the  cid:222 rst two cases, in which x > xi , from the third case, in which x ≤ xi , we get  f  y  − f  x  = n cid:2   cid:2   i=1  ≥   cid:2  wi  y − xi − x − xi   cid:4  wi  x − y  wi  y − x  +  cid:2   cid:1   x≤xi wi −  wi ≤ 1 2 implies that   cid:3  cid:2   cid:1   x≤xi  x >xi  wi  .  x >xi   cid:1  =  x − y   x≤xi  x >xi  xi >x  wi  x − xi + y − yi  .  wi > 1 2. This fact, The property that wi   0. combined with x − y > 0 and e. We are given n 2-dimensional points p1, p2, . . . , pn, where each pi is a pair of real numbers pi =  xi , yi  , and positive weights w1, w2, . . . , wn. The goal is to  cid:222 nd a point p =  x, y  that minimizes the sum  f  x, y  = n cid:2  wi x − xi, and h y  =  cid:1   cid:1  f  x, y , as the sum of We can express the cost function of the two variables, f  x, y  = g x  + h y , where g x  = two functions of one variable each: wi y − yi. The goal of  cid:222 nding a point n n i=1 i=1 p =  x, y  that minimizes the value of f  x, y  can be achieved by treating each dimension independently, because g does not depend on y and h does not depend on x. Thus, min x ,y  f  x, y  = min   g x  + h y    i=1  x ,y   9-18  Solutions for Chapter 9: Medians and Order Statistics   cid:8   cid:8   g x  + h y     cid:7   cid:7  min g x  + min g x  + min  h y  .  h y   y  y  y  x  = min = min = min  x  x  Consequently,  cid:222 nding the best location in 2 dimensions can be done by  cid:222 nding the weighted median xk of the x-coordinates and then  cid:222 nding the weighted median y j of the y-coordinates. The point  xk, y j   is an optimal solution for the 2-dimensional post-of cid:222 ce location problem.  Solution to Problem 9-3  a. Our algorithm relies on a particular property of SELECT: that not only does it return the ith smallest element, but that it also partitions the input array so that the  cid:222 rst i positions contain the i smallest elements  though not necessarily in sorted order . To see that SELECT has this property, observe that there are only two ways in which returns a value: when n = 1, and when immediately after partitioning in step 4, it  cid:222 nds that there are exactly i elements on the low side of the partition. Taking the hint from the book, here is our modi cid:222 ed algorithm to select the ith smallest element of n elements. Whenever it is called with i ≥ n 2, it just calls SELECT and returns its result; in this case, Ui  n  = T  n . When i < n 2, our modi cid:222 ed algorithm works as follows. Assume that the input is in a subarray A[p + 1 . . p + n], and let m =  cid:9 n 2 cid:10 . In the initial call, p = 1. 1. Divide the input as follows. If n is even, divide the input into two parts: A[p + 1 . . p + m] and A[p + m + 1 . . p + n]. If n is odd, divide the input into three parts: A[p + 1 . . p + m], A[p + m + 1 . . p + n − 1], and A[p + n] as a leftover piece. 2. Compare A[p + i] and A[p + i + m] for i = 1, 2, . . . , m, putting the smaller of the the two elements into A[p + i + m] and the larger into A[p + i]. 3. Recursively  cid:222 nd the ith smallest element in A[p+m+1 . . p+n], but with an additional action performed by the partitioning procedure: whenever it ex- changes A[ j] and A[k]  where p+m+1 ≤ j, k ≤ p+2m , it also exchanges A[ j−m] and A[k−m]. The idea is that after recursively  cid:222 nding the ith small- est element in A[p+m+1 . . p+n], the subarray A[p + m + 1 . . p+m+i] contains the i smallest elements that had been in A[p + m + 1 . . p + n] and the subarray A[p + 1 . . p + i] contains their larger counterparts, as found in step 1. The ith smallest element of A[p + 1 . . p + n] must be either one of the i smallest, as placed into A[p + m + 1 . . p + m + i], or it must be one of the larger counterparts, as placed into A[p + 1 . . p + i]. 4. Collect the subarrays A[p + 1 . . p + i] and A[p + m + 1 . . p + m + i] into a single array B[1 . . 2i], call SELECT to  cid:222 nd the ith smallest element of B, and return the result of this call to SELECT.  The number of comparisons in each step is as follows:   Solutions for Chapter 9: Medians and Order Statistics  9-19  1. No comparisons. 2. m =  cid:9 n 2 cid:10  comparisons. 3. Since we recurse on A[p + m + 1 . . p + n], which has  cid:14 n 2 cid:15  elements, the  number of comparisons is Ui   cid:14 n 2 cid:15  .  isons is T  2i  .  4. Since we call SELECT on an array with 2i elements, the number of compar- Thus, when i < n 2, the total number of comparisons is cid:9 n 2 cid:10  + Ui   cid:14 n 2 cid:15   + T  2i  . b. We show by substitution that if i < n 2, then Ui  n  = n+O T  2i   lg n i   . In particular, we shall show that Ui  n  ≤ n + cT  2i   lg n i   − d lg lg n T  2i   = n + cT  2i   lg n − cT  2i   lg i − d lg lg n T  2i   for some positive constant c, some positive constant d to be chosen later, and n ≥ 4. We have Ui  n  =  cid:9 n 2 cid:10  + Ui   cid:14 n 2 cid:15   + T  2i   − d lg lg cid:14 n 2 cid:15  T  2i    ≤  cid:9 n 2 cid:10  +  cid:14 n 2 cid:15  + cT  2i   lg cid:14 n 2 cid:15  − cT  2i   lg i = n + cT  2i   lg cid:14 n 2 cid:15  − cT  2i   lg i − d lg lg cid:14 n 2 cid:15  T  2i   ≤ n + cT  2i   lg n 2 + 1  − cT  2i   lg i − d lg lg n 2  T  2i   = n + cT  2i   lg n 2 + 1  − cT  2i   lg i − d lg lg n − 1  T  2i   ≤ n + cT  2i   lg n − cT  2i   lg i − d lg lg n T  2i     cid:6   cid:6  cid:6  ≤ d lg ≤ d lg  if cT  2i   lg n 2 + 1  − d lg lg n − 1  T  2i   ≤ cT  2i   lg n − d lg lg n T  2i  . Simple algebraic manipulations gives the following sequence of equivalent con- ditions: cT  2i   lg n 2 + 1  − d lg lg n − 1  T  2i   ≤ cT  2i   lg n − d lg lg n T  2i   c lg n 2 + 1  − d lg lg n − 1   ≤ c lg n − d lg lg n   cid:5  c lg n 2 + 1  − lg n  ≤ d lg lg n − 1  − lg lg n   cid:5  c c Observe that 1 2+1 n decreases as n increases, but  lg n−1   lg n increases as n increases. When n = 4, we have 1 2+1 n = 3 4 and  lg n−1   lg n = 1 2. Thus, we just need to choose d such that c lg 3 4  ≤ d lg 1 2  or, equivalently, c lg 3 4  ≤ −d. Multiplying both sides by −1, we get d ≤ −c lg 3 4  = c lg 4 3 . Thus, any value of d that is at most c lg 4 3  suf cid:222 ces. c. When i is a constant, T  2i   = O 1  and lg n i   = lg n−lg i = O lg n . Thus,  lg n − 1 lg n lg n − 1 lg n  n 2 + 1  cid:5   n + 1 n  1 2  lg  lg  when i is a constant less than n 2, we have that Ui  n  = n + O T  2i   lg n i    = n + O O 1  · O lg n   = n + O lg n  .  have Ui  n  = n + O T  2i   lg n i     d. Suppose that i = n k for k ≥ 2. Then i ≤ n 2. If k > 2, then i < n 2, and we   9-20  Solutions for Chapter 9: Medians and Order Statistics  = n + O T  2n k  lg n  n k   = n + O T  2n k  lg k  .  If k = 2, then n = 2i and lg k = 1. We have Ui  n  = T  n   = n +  T  n  − n  ≤ n +  T  2i   − n  = n +  T  2n k  − n  = n +  T  2n k  lg k − n  = n + O T  2n k  lg k  .   Lecture Notes for Chapter 11: Hash Tables  Chapter 11 overview  Many applications require a dynamic set that supports only the dictionary opera- tions INSERT, SEARCH, and DELETE. Example: a symbol table in a compiler. A hash table is effective for implementing a dictionary.   The expected time to search for an element in a hash table is O 1 , under some  reasonable assumptions.    Worst-case search time is  cid:2  n , however. A hash table is a generalization of an ordinary array.   With an ordinary array, we store the element whose key is k in position k of the  array.    Given a key k, we  cid:222 nd the element whose key is k by just looking in the kth  position of the array. This is called direct addressing.    Direct addressing is applicable when we can afford to allocate an array with  one position for every possible key.  We use a hash table when we do not want to  or cannot  allocate an array with one position per possible key.   Use a hash table when the number of keys actually stored is small relative to  the number of possible keys.    A hash table is an array, but it typically uses a size proportional to the number  of keys to be stored  rather than the number of possible keys .    Given a key k, don cid:146 t just use k as the index into the array.    Instead, compute a function of k, and use that value to index into the array. We call this function a hash function.  Issues that we cid:146 ll explore in hash tables:   How to compute hash functions. We cid:146 ll look at the multiplication and division  methods.    What to do when the hash function maps multiple keys to the same table entry.  We cid:146 ll look at chaining and open addressing.   11-2  Lecture Notes for Chapter 11: Hash Tables  Direct-address tables  Scenario:   Maintain a dynamic set.   Each element has a key drawn from a universe U = {0, 1, . . . , m − 1} where  m isn cid:146 t too large.    No two elements have the same key. Represent by a direct-address table, or array, T [0 . . . m − 1]:   Each slot, or position, corresponds to a key in U.     Otherwise, T [k] is empty, represented by NIL.  If there cid:146 s an element x with key k, then T [k] contains a pointer to x.  U   universe of keys  6  0  1  9  4  K   actual keys   2  5  7  3      8  T  key  satellite data  0  1  2  3  4  5  6  7  8  9  2 3  5  8  Dictionary operations are trivial and take O 1  time each:  DIRECT-ADDRESS-SEARCH T, k  return T [k]  DIRECT-ADDRESS-INSERT T, x  T [key[x]] ← x  DIRECT-ADDRESS-DELETE  T, x  T [key[x]] ← NIL  Hash tables  The problem with direct addressing is if the universe U is large, storing a table of size U may be impractical or impossible. Often, the set K of keys actually stored is small, compared to U, so that most of the space allocated for T is wasted.   Lecture Notes for Chapter 11: Hash Tables  11-3  direct-address table.    When K is much smaller than U, a hash table requires much less space than a   Can reduce storage requirements to  cid:2  K .   Can still get O 1  search time, but in the average case, not the worst case.  Idea: Instead of storing an element with key k in slot k, use a function h and store the element in slot h k .   We call h a hash function.     We say that k hashes to slot h k .  h : U → {0, 1, . . . , m − 1}, so that h k  is a legal slot number in T .  Collisions: When two or more keys hash to the same slot.   Can happen when there are more possible keys than slots  U > m .   For a given set K of keys with K ≤ m, may or may not happen. De cid:222 nitely  happens if K > m.    Therefore, must be prepared to handle collisions in all cases.   Use two methods: chaining and open addressing.   Chaining is usually better than open addressing. We cid:146 ll examine both.  Collision resolution by chaining  Put all elements that hash to the same slot into a linked list.  T  U   universe of keys   K   actual keys   k5     k4  k2  k7    k8    k3  k1  k6  k1  k5  k3 k8  k4  k2  k6  k7  [This cid:222 gureshowssingly linked lists. Ifwewanttodelete elements, it cid:146 sbetter to usedoublylinkedlists.]   Slot j contains a pointer to the head of the list of all stored elements that hash  to j [ortothesentinelifusingacircular,doublylinkedlistwithasentinel] , If there are no such elements, slot j contains NIL.      11-4  Lecture Notes for Chapter 11: Hash Tables  How to implement dictionary operations with chaining:    Insertion:  CHAINED-HASH-INSERT T, x  insert x at the head of list T [h key[x] ]   Worst-case running time is O 1 .   Assumes that the element being inserted isn cid:146 t already in the list.    It would take an additional search to check if it was already inserted.    Search:    Deletion:  CHAINED-HASH-SEARCH T, k  search for an element with key k in list T [h k ]  Running time is proportional to the length of the list of elements in slot h k .  CHAINED-HASH-DELETE  T, x  delete x from the list T [h key[x] ]   Given pointer x to the element to delete, so no search is needed to  cid:222 nd this  element.    Worst-case running time is O 1  time if the lists are doubly linked.    If the lists are singly linked, then deletion takes as long as searching, be- cause we must  cid:222 nd x cid:146 s predecessor in its list in order to correctly update next pointers.  Analysis of hashing with chaining  Given a key, how long does it take to  cid:222 nd an element with that key, or to determine that there is no element with that key?   Analysis is in terms of the load factor α = n m:    n =  of elements in the table.   m =  of slots in the table =  of  possibly empty  linked lists.   Load factor is average number of elements per linked list.   Can have α   1. ⇒ worst-case time to search is  cid:2  n , plus time to compute hash function.    Worst case is when all n keys hash to the same slot ⇒ get a single list of length n    Average case depends on how well the hash function distributes the keys among  the slots.  We focus on average-case performance of hashing with chaining.   Assume simple uniform hashing: any given element is equally likely to hash  into any of the m slots.   Lecture Notes for Chapter 11: Hash Tables  11-5  n = n0 + n1 + ··· + nm−1.  j = 0, 1, . . . , m − 1, denote the length of list T [ j] by nj . Then    For   Average value of n j is E [n j ] = α = n m.   Assume that we can compute the hash function in O 1  time, so that the time required to search for the element with key k depends on the length nh k  of the list T [h k ].  We consider two cases:      If the hash table contains no element with key k, then the search is unsuccessful. If the hash table does contain an element with key k, then the search is success- ful.  [Inthetheoremstatementsthatfollow,weomittheassumptionsthatwe cid:146 reresolv- ingcollisionsbychainingandthatsimpleuniformhashingapplies.]  Unsuccessful search:  Theorem An unsuccessful search takes expected time  cid:2  1 + α . Proof Simple uniform hashing ⇒ any key not already in the table is equally likely to hash to any of the m slots. To search unsuccessfully for any key k, need to search to the end of the list T [h k ]. This list has expected length E[nh k ] = α. Therefore, the expected number of elements examined in an unsuccessful search is α. Adding in the time to compute the hash function,  cid:2  1 + α .  the total time required is  Successful search:   The expected time for a successful search is also  cid:2  1 + α .   The circumstances are slightly different from an unsuccessful search.   The probability that each list is searched is proportional to the number of ele-  ments it contains.  Theorem A successful search takes expected time  cid:2  1 + α . Proof Assume that the element x being searched for is equally likely to be any of the n elements stored in the table. The number of elements examined during a successful search for x is 1 more than the number of elements that appear before x in x cid:146 s list. These are the elements inserted after x was inserted  because we insert at the head of the list . So we need to  cid:222 nd the average, over the n elements x in the table, of how many elements were inserted into x cid:146 s list after x was inserted. For i = 1, 2, . . . , n, let xi be the ith element inserted into the table, and let ki = key[xi ].   11-6  Lecture Notes for Chapter 11: Hash Tables  For all i and j, de cid:222 ne indicator random variable Xi j = I{h ki   = h k j  }. Simple uniform hashing ⇒ Pr{h ki   = h k j  } = 1 m ⇒ E [Xi j ] = 1 m  by  cid:13  Lemma 5.1 . Expected number of elements examined in a successful search is   cid:3   n cid:2   i=1  E  1 n   linearity of expectation    cid:4    cid:4   cid:6   Xi j  i=1   cid:4   j=i+1  j=i+1  E [Xi j ]   cid:3   cid:3   = 1 n = 1 n  1 + n cid:2  n cid:2  n cid:2    cid:4  cid:14  1 + n cid:2  1 + n cid:2  1 n cid:2  j=i+1 i=1 m  cid:3   n − i   = 1 + 1 n − n cid:2  n cid:2  nm  cid:5  = 1 + 1 i=1 i=1 nm n2 − n n + 1  = 1 + 1 nm = 1 + n − 1 2m − α = 1 + α 2n 2  i=1  2  i  .   equation  A.1    Adding in the time for computing the hash function, we get that the expected total time for a successful search is  cid:2  2 + α 2 − α 2n  =  cid:2  1 + α .  Alternative analysis, using indicator random variables even more: For each slot l and for each pair of keys ki and k j , de cid:222 ne the indicator random variable Xi jl = I{the search is for xi , h ki   = l, and h k j   = l}. Xi jl = 1 when keys ki and k j collide at slot l and when we are searching for xi . Simple uniform hashing ⇒ Pr{h ki   = l} = 1 m and Pr{h k j   = l} = 1 m. Also have Pr{the search is for xi} = 1 n. These events are all independent ⇒ Pr{Xi jl = 1} = 1 nm2 ⇒ E [Xi jl ] = 1 nm2  by Lemma 5.1 . De cid:222 ne, for each element x j , the indicator random variable Y j = I{x j appears in a list prior to the element being searched for} . Y j = 1 if and only if there is some slot l that has both elements xi and x j in its list, and also i < j  so that xi appears after x j in the list . Therefore,  m−1 cid:2   Y j = j−1 cid:2  prior to the element being searched for: Z = cid:1   Xi jl .  i=1  l=0  One  cid:222 nal random variable: Z, which counts how many elements appear in the list n j=1 Y j . We must count the element   Lecture Notes for Chapter 11: Hash Tables  11-7  being searched for as well as all those preceding it in its list ⇒ compute E[Z + 1]: E [Z + 1] = E   cid:14   Xi jl   linearity of expectation   E [Xi jl ]   linearity of expectation   Y j  l=0  l=0  i=1  i=1  j=1  j=1  j=1   cid:13   = 1 + E   cid:14  1 + n cid:2   cid:13  n cid:2  j−1 cid:2  m−1 cid:2  m−1 cid:2  j−1 cid:2  = 1 + n cid:2  = 1 + n cid:2  m−1 cid:2  j−1 cid:2   cid:5   cid:6  = 1 + = 1 + n n − 1  = 1 + n − 1 2m − 1 = 1 + n 2m 2m − α = 1 + α . 2n 2  i=1 l=0 · m · 1 nm2 · 1 nm  j=1 n 2  2  1  nm2  Adding in the time for computing the hash function, we get that the expected total time for a successful search is  cid:2  2 + α 2 − α 2n  =  cid:2  1 + α . Interpretation: If n = O m , then α = n m = O m  m = O 1 , which means that searching takes constant time on average. Since insertion takes O 1  worst-case time and deletion takes O 1  worst-case time when the lists are doubly linked, all dictionary operations take O 1  time on average.  Hash functions  We discuss some issues regarding hash-function design and present schemes for hash function creation.  What makes a good hash function?       Ideally, the hash function satis cid:222 es the assumption of simple uniform hashing. In practice, it cid:146 s not possible to satisfy this assumption, since we don cid:146 t know in advance the probability distribution that keys are drawn from, and the keys may not be drawn independently.    Often use heuristics, based on the domain of the keys, to create a hash function  that performs well.   11-8  Lecture Notes for Chapter 11: Hash Tables  Keys as natural numbers    Hash functions assume that the keys are natural numbers.   When they cid:146 re not, have to interpret them as natural numbers.   Example: Interpret a character string as an integer expressed in some radix  notation. Suppose the string is CLRS:   ASCII values: C = 67, L = 76, R = 82, S = 83.   There are 128 basic ASCII values.   So interpret CLRS as  67 · 1283  +  76 · 1282  +  82 · 1281  +  83 · 1280  =  141,764,947.  Division method h k  = k mod m . Example: m = 20 and k = 91 ⇒ h k  = 11. Advantage: Fast, since requires just one division operation. Disadvantage: Have to avoid certain values of m:   Powers of 2 are bad. If m = 2p for integer p, then h k  is just the least signi cid:222 -     cant p bits of k. If k is a character string interpreted in radix 2p  as in CLRS example , then m = 2p − 1 is bad: permuting characters in a string does not change its hash value  Exercise 11.3-3 .  Good choice for m: A prime not too close to an exact power of 2.  Multiplication method  1. Choose constant A in the range 0 < A < 1. 2. Multiply key k by A. 3. Extract the fractional part of k A. 4. Multiply the fractional part by m. 5. Take the  cid:223 oor of the result. Put another way, h k  =  cid:9 m  k A mod 1  cid:10 , where k A mod 1 = k A −  cid:9 k A cid:10  = fractional part of k A. Disadvantage: Slower than division method. Advantage: Value of m is not critical.  Relatively  easy implementation:   Choose m = 2p for some integer p.   Let the word size of the machine be w bits.   Assume that k  cid:222 ts into a single word.  k takes w bits.    Let s be an integer in the range 0 < s < 2w.  s takes w bits.    Lecture Notes for Chapter 11: Hash Tables  11-9    Restrict A to be of the form s 2w.  w bits  k  s = A · 2w  r0 extract p bits   cid:215   r1  binary point  h k        Multiply k by s.   Since we cid:146 re multiplying two w-bit words, the result is 2w bits, r12w+r0, where r1 is the high-order word of the product and r0 is the low-order word. r1 holds the integer part of k A   cid:9 k A cid:10   and r0 holds the fractional part of k A  k A mod 1 = k A −  cid:9 k A cid:10  . Think of the  cid:147 binary point cid:148   analog of decimal point, but for binary representation  as being between r1 and r0. Since we don cid:146 t care about the integer part of k A, we can forget about r1 and just use r0.   Since we want  cid:9 m  k A mod 1  cid:10 , we could get that value by shifting r0 to the left by p = lg m bits and then taking the p bits that were shifted to the left of the binary point.    We don cid:146 t need to shift. The p bits that would have been shifted to the left of the binary point are the p most signi cid:222 cant bits of r0. So we can just take these bits after having formed r0 by multiplying k by s.   Example: m = 8  implies p = 3 , w = 5, k = 21. Must have 0 < s < 25; choose s = 13 ⇒ A = 13 32.   Using just the formula to compute h k : k A = 21 · 13 32 = 273 32 = 817 ⇒ k A mod 1 = 17 32 ⇒ m  k A mod 1  = 8 · 17 32 = 17 4 = 41 ⇒  cid:9 m  k A mod 1  cid:10  = 4, so that h k  = 4.   Using the implementation: ks = 21 · 13 = 273 = 8 · 25 + 17 ⇒ r1 = 8, r0 = 17. Written in w = 5 bits, r0 = 10001. Take the p = 3 most signi cid:222 cant bits of r0, get 100 in binary, or 4 in decimal, so that h k  = 4.  32  4  How to choose A:   The multiplication method works with any legal value of A.   But it works better with some values than with others, depending on the keys √   Knuth suggests using A ≈   5 − 1  2.  being hashed.  Universal hashing  [We just touch on universal hashing in these notes. See the book for a full treat- ment.] Suppose that a malicious adversary, who gets to choose the keys to be hashed, has seen your hashing program and knows the hash function in advance. Then he could choose keys that all hash to the same slot, giving worst-case behavior.   11-10  Lecture Notes for Chapter 11: Hash Tables  One way to defeat the adversary is to use a different hash function each time. You choose one at random at the beginning of your program. Unless the adversary knows how you cid:146 ll be randomly choosing which hash function to use, he cannot intentionally defeat you. Just because we choose a hash function randomly, that doesn cid:146 t mean it cid:146 s a good hash function. What we want is to randomly choose a single hash function from a set of good candidates. Consider a  cid:222 nite collection H of hash functions that map a universe U of keys into the range {0, 1, . . . , m − 1}. H is universal if for each pair of keys k, l ∈ U, where k  cid:19 = l, the number of hash functions h ∈ H for which h k  = h l  is ≤H  m. Put another way, H is universal if, with a hash function h chosen randomly from H, the probability of a collision between two different keys is no more than than 1 m chance of just choosing two slots randomly and independently. Why are universal hash functions good?   They give good hashing behavior:  Theorem Using chaining and universal hashing on key k:       If k is not in the table, the expected length E[nh k ] of the list that k hashes to is ≤ α. If k is in the table, the expected length E[nh k ] of the list that holds k is ≤ 1 + α. Corollary Using chaining and universal hashing, the expected time for each SEARCH op- eration is O 1 .    They are easy to design. [Seebookfordetailsofbehavioranddesignofauniversalclassofhashfunctions.]  Open addressing  An alternative to chaining for handling collisions.  Idea:   Store all keys in the hash table itself.   Each slot contains either a key or NIL.   To search for key k:    Compute h k  and examine slot h k . Examining a slot is known as a probe.   If slot h k  contains key k, the search is successful. If this slot contains NIL, the search is unsuccessful.    There cid:146 s a third possibility: slot h k  contains a key that is not k. We compute the index of some other slot, based on k and on which probe  count from 0: 0th, 1st, 2nd, etc.  we cid:146 re on.   Lecture Notes for Chapter 11: Hash Tables  11-11    Keep probing until we either  cid:222 nd key k  successful search  or we  cid:222 nd a slot  holding NIL  unsuccessful search .    We need the sequence of slots probed to be a permutation of the slot numbers  cid:2 0, 1, . . . , m − 1 cid:3   so that we examine all slots if we have to, and so that we don cid:146 t examine any slot more than once .   cid:26    Thus, the hash function is h : U ×{0, 1, . . . , m − 1} probe number   The requirement that the sequence of slots be a permutation of  cid:2 0, 1, . . . , m − 1 cid:3  is equivalent to requiring that the probe sequence  cid:2 h k, 0 , h k, 1 , . . . , h k, m − 1  cid:3  be a permutation of  cid:2 0, 1, . . . , m − 1 cid:3 .   cid:26   cid:23  → {0, 1, . . . , m − 1}  slot number   cid:24  cid:25    cid:24  cid:25    cid:23     To insert, act as though we cid:146 re searching, and insert at the  cid:222 rst NIL slot we  cid:222 nd.  .  HASH-SEARCH returns the index of a slot containing key k, or NIL if the search is unsuccessful.  Pseudocode for searching:  HASH-SEARCH T, k  i ← 0 repeat j ← h k, i   if T [ j] = k then return j i ← i + 1  until T [ j] = NIL or i = m return NIL  Pseudocode for insertion:  HASH-INSERT T, k  i ← 0 repeat j ← h k, i   if T [ j] = NIL then T [ j] ← k return j else i ← i + 1  until i = m error  cid:147 hash table over cid:223 ow cid:148   HASH-INSERT returns the number of the slot that gets key k, or it  cid:223 ags a  cid:147 hash table over cid:223 ow cid:148  error if there is no empty slot in which to put key k.  Deletion: Cannot just put NIL into the slot containing the key we want to delete.   Suppose we want to delete key k in slot j.  cid:4    And suppose that sometime after inserting key k, we were inserting key k  , and  during this insertion we had probed slot j  which contained key k .    And suppose we then deleted key k by storing NIL into slot j.  cid:4    And then we search for key k  .   11-12  Lecture Notes for Chapter 11: Hash Tables    During the search, we would probe slot j before probing the slot into which   cid:4   key k  was eventually stored.   cid:4    Thus, the search would be unsuccessful, even though key k Solution: Use a special value DELETED instead of NIL when marking a slot as empty during deletion.   Search should treat DELETED as though the slot holds a key that does not match  is in the table.     the one being searched for. Insertion should treat DELETED as though the slot were empty, so that it can be reused.  The disadvantage of using DELETED is that now search time is no longer dependent on the load factor α.  How to compute probe sequences  The ideal situation is uniform hashing: each key is equally likely to have any of the m! permutations of  cid:2 0, 1, . . . , m − 1 cid:3  as its probe sequence.  This generalizes simple uniform hashing for a hash function that produces a whole probe sequence rather than just a single number.  It cid:146 s hard to implement true uniform hashing, so we approximate it with techniques that at least guarantee that the probe sequence is a permutation of  cid:2 0, 1, . . . , m−1 cid:3 . None of these techniques can produce all m! probe sequences. They will make use of auxiliary hash functions, which map U → {0, 1, . . . , m − 1}.  , the probe sequence starts at  cid:4  k  and continues sequentially through the table, wrapping after slot m − 1   cid:4  Linear probing: Given auxiliary hash function h slot h to slot 0. Given key k and probe number i  0 ≤ i < m , h k, i   =  h The initial probe determines the entire sequence ⇒ only m possible sequences. Linear probing suffers from primary clustering: long runs of occupied sequences build up. And long runs tend to get longer, since an empty slot preceded by i full slots gets  cid:222 lled next with probability  i + 1  m. Result is that the average search and insertion times increase.   cid:4  k  + i   mod m.   cid:4  k . Un- Quadratic probing: As in linear probing, the probe sequence starts at h like linear probing, it jumps around in the table according to a quadratic function  cid:4  k  + c1i + c2i 2  mod m, where c1, c2  cid:19 = 0 are of the probe number: h k, i   =  h constants. Must constrain c1, c2, and m in order to ensure that we get a full permutation of  cid:2 0, 1, . . . , m − 1 cid:3 .  Problem 11-3 explores one way to implement quadratic probing.   cid:4  Can get secondary clustering: if two distinct keys have the same h they have the same probe sequence.  value, then   Lecture Notes for Chapter 11: Hash Tables  11-13  Double hashing: Use two auxiliary hash functions, h1 and h2. h1 gives the initial probe, and h2 gives the remaining probes: h k, i   =  h1 k  + ih2 k   mod m. Must have h2 k  be relatively prime to m  no factors in common other than 1  in order to guarantee that the probe sequence is a full permutation of  cid:2 0, 1, . . . , m−1 cid:3 .   Could choose m to be a power of 2 and h2 to always produce an odd number  > 1.    Could let m be prime and have 1 < h2 k  < m.  cid:2  m2  different probe sequences, since each possible combination of h1 k  and h2 k  gives a different probe sequence.  Analysis of open-address hashing  Assumptions:   Analysis is in terms of load factor α. We will assume that the table never  completely  cid:222 lls, so we always have 0 ≤ n < m ⇒ 0 ≤ α < 1.    Assume uniform hashing.   No deletion.    In a successful search, each key is equally likely to be searched for.  Theorem The expected number of probes in an unsuccessful search is at most 1  1 − α . Proof Since the search is unsuccessful, every probe is to an occupied slot, except for the last probe, which is to an empty slot. De cid:222 ne random variable X =  of probes made in an unsuccessful search. De cid:222 ne events Ai , for i = 1, 2, . . ., to be the event that there is an ith probe and that it cid:146 s to an occupied slot. X ≥ i if and only if probes 1, 2, . . . , i − 1 are made and are to occupied slots ⇒ Pr{X ≥ i} = Pr{A1 ∩ A2 ∩ ··· ∩ Ai−1}. By Exercise C.2-6, Pr{A1 ∩ A2 ∩ ··· ∩ Ai−1} = Pr{A1} · Pr{A2  A1} · Pr{A3  A1 ∩ A2}···  Pr{Ai−1  A1 ∩ A2 ∩ ··· ∩ Ai−2} .  Claim Pr{A j  A1 ∩ A2 ∩ ··· ∩ A j−1} =  n − j + 1   m − j + 1 . Boundary case: j = 1 ⇒ Pr{A1} = n m. Proof For the boundary case j = 1, there are n stored keys and m slots, so the probability that the  cid:222 rst probe is to an occupied slot is n m. Given that j−1 probes were made, all to occupied slots, the assumption of uniform hashing says that the probe sequence is a permutation of  cid:2 0, 1, . . . , m − 1 cid:3 , which in turn implies that the next probe is to a slot that we have not yet probed. There are m − j + 1 slots remaining, n − j + 1 of which are occupied. Thus, the probability that the jth probe is to an occupied slot is  n − j + 1   m − j + 1 .  claim   Using this claim,   11-14  Lecture Notes for Chapter 11: Hash Tables   cid:23  Pr{X ≥ i} = n m  ··· n − i + 2  cid:26  m − i + 2  · n − 2  cid:24  cid:25  m − 2 i − 1 factors  .  n < m ⇒  n − j    m − j   ≤ n m for j ≥ 0, which implies Pr{X ≥ i} ≤  · n − 1 m − 1  cid:8   cid:7   i−1 = αi−1 .  n m  By equation  C.24 ,  E [X] = ∞ cid:2  ≤ ∞ cid:2  = ∞ cid:2   i=1  i=1  Pr{X ≥ i}  αi−1  αi  i=0 1 1 − α  =   equation  A.6   .   theorem   Interpretation: If α is constant, an unsuccessful search takes O 1  time.    If α = 0.5, then an unsuccessful search takes an average of 1  1 − 0.5  = 2 probes. If α = 0.9, takes an average of 1  1 − 0.9  = 10 probes.     Corollary The expected number of probes to insert is at most 1  1 − α . Proof Since there is no deletion, insertion uses the same probe sequence as an unsuccessful search.  Theorem The expected number of probes in a successful search is at most  1 α  ln  1 1 − α  .  Proof A successful search for key k follows the same probe sequence as when key k was inserted. By the previous corollary, if k was the  i + 1 st key inserted, then α equaled i  m at the time. Thus, the expected number of probes made in a search for k is at most 1  1 − i  m  = m  m − i  . That was assuming that k was the  i + 1 st key inserted. We need to average over n−1 cid:2  n−1 cid:2  all n keys: 1 m m − i n  1 m − i  i=0  = m n = 1  α  i=0  Hm − Hm−n  ,   where Hi = cid:1   Lecture Notes for Chapter 11: Hash Tables  11-15  i j=1 1 j is the ith harmonic number.  m cid:2   α  1 k   cid:27   k=m−n+1  Simplify by using the technique of bounding a summation by an integral:  Hm − Hm−n  = 1 1 α ≤ 1 = 1 = 1   inequality  A.12     1 x  dx  ln  α  α  m m−n m m − n 1 1 − α  ln  α   theorem    Solutions for Chapter 11: Hash Tables  Solution to Exercise 11.1-4   cid:4   We denote the huge array by T and, taking the hint from the book, we also have a stack implemented by an array S. The size of S equals the number of keys actually stored, so that S should be allocated at the dictionary cid:146 s maximum size. The stack has an attribute top[S], so that only entries S[1 . . top[S]] are valid. The idea of this scheme is that entries of T and S validate each other. If key k is actually stored in T , then T [k] contains the index, say j, of a valid entry in S, and S[ j] contains the value k. Let us call this situation, in which 1 ≤ T [k] ≤ top[S], S[T [k]] = k, and T [S[ j]] = j, a validating cycle. Assuming that we also need to store pointers to objects in our direct-address table, we can store them in an array that is parallel to either T or S. Since S is smaller  cid:4  , allocated to be the same size as S, for these pointers. than T , we cid:146 ll use an array S Thus, if the dictionary contains an object x with key k, then there is a validating cycle and S The operations on the dictionary work as follows:    Initialization: Simply set top[S] = 0, so that there are no valid entries in the stack. SEARCH: Given key k, we check whether we have a validating cycle, i.e., whether 1 ≤ T [k] ≤ top[S] and S[T [k]] = k. [T [k]], and otherwise we return NIL. INSERT: To insert object x with key k, assuming that this object is not already [top[S]] ← x, in the dictionary, we increment top[S], set S[top[S]] ← k, set S  cid:4  and set T [k] ← top[S].   cid:4  If so, we return S  [T [k]] points to x.    DELETE: To delete object x with key k, assuming that this object is in the dictionary, we need to break the validating cycle. The trick is to also ensure that we don cid:146 t leave a  cid:147 hole cid:148  in the stack, and we solve this problem by moving the top entry of the stack into the position that we are vacating cid:151 and then  cid:222 xing up that entry cid:146 s validating cycle. That is, we execute the following sequence of assignments:         Solutions for Chapter 11: Hash Tables  11-17   cid:4   S[T [k]] ← S[top[S]] [T [k]] ← S  cid:4  [top[S]] S T [S[T [k]]] ← T [k] T [k] ← 0 top[S] ← top[S] − 1  Solution to Exercise 11.2-1  Each of these operations cid:151 initialization, SEARCH, INSERT, and DELETE cid:151 takes O 1  time.  Xkl   cid:29   Y = cid:1   k cid:19 =l Xkl. The expected number of collisions is  For each pair of keys k, l, where k  cid:19 = l, de cid:222 ne the indicator random variable Xkl = I{h k  = h l }. Since we assume simple uniform hashing, Pr{Xkl = 1} = Pr{h k  = h l } = 1 m, and so E [Xkl] = 1 m.  cid:28  cid:2  Now de cid:222 ne the random variable Y to be the total number of collisions, so that  cid:2  E [Y ] = E  cid:5   cid:6  = = n n − 1  = n n − 1    linearity of expectation   k cid:19 =l E [Xkl]  k cid:19 =l n 2  · 1 m  1 m  =  2  .  2m  The  cid:223 ag in each slot will indicate whether the slot is free.   A free slot is in the free list, a doubly linked list of all free slots in the table.  The slot thus contains two pointers.    A used slot contains an element and a pointer  possibly NIL  to the next element that hashes to this slot.  Of course, that pointer points to another slot in the table.   Operations     Insertion:    If the element hashes to a free slot, just remove the slot from the free list and store the element there  with a NIL pointer . The free list must be doubly linked in order for this deletion to run in O 1  time.  Solution to Exercise 11.2-4   11-18  Solutions for Chapter 11: Hash Tables        If the element hashes to a used slot j, check whether the element x already there  cid:147 belongs cid:148  there  its key also hashes to slot j .        If so, add the new element to the chain of elements in this slot. To do so, allocate a free slot  e.g., take the head of the free list  for the new element and put this new slot at the head of the list pointed to by the hashed-to slot   j . If not, E is part of another slot cid:146 s chain. Move it to a new slot by allo- cating one from the free list, copying the old slot cid:146 s   j cid:146 s  contents  ele- ment x and pointer  to the new slot, and updating the pointer in the slot that pointed to j to point to the new slot. Then insert the new element in the now-empty slot as usual. To update the pointer to j, it is necessary to  cid:222 nd it by searching the chain of elements starting in the slot x hashes to.    Deletion: Let j be the slot the element x to be deleted hashes to.     If x is the only element in j   j doesn cid:146 t point to any other entries , just free the slot, returning it to the head of the free list. If x is in j but there cid:146 s a pointer to a chain of other elements, move the  cid:222 rst pointed-to entry to slot j and free the slot it was in. If x is found by following a pointer from j, just free x cid:146 s slot and splice it out of the chain  i.e., update the slot that pointed to x to point to x cid:146 s successor .   Searching: Check the slot the key hashes to, and if that is not the desired     element, follow the chain of pointers from the slot.  All the operations take expected O 1  times for the same reason they do with the version in the book: The expected time to search the chains is O 1 + α  regardless of where the chains are stored, and the fact that all the elements are stored in the table means that α ≤ 1. If the free list were singly linked, then operations that involved removing an arbitrary slot from the free list would not run in O 1  time.  Solution to Exercise 11.3-3  First, we observe that we can generate any permutation by a sequence of inter- changes of pairs of characters. One can prove this property formally, but infor- mally, consider that both heapsort and quicksort work by interchanging pairs of elements and that they have to be able to produce any permutation of their input array. Thus, it suf cid:222 ces to show that if string x can be derived from string y by interchanging a single pair of characters, then x and y hash to the same value. Let us denote the ith character in x by xi , and similarly for y. The interpreta- mod  2p − 1 . n−1 tion of x in radix 2p is n−1 i=0 yi2ip  i=0 xi 2ip, and so h x  =  cid:10  cid:1   cid:1   cid:11   Similarly, h y  = cid:10  cid:1   mod  2p − 1 .  n−1 i=0 xi 2ip   cid:11   Suppose that x and y are identical strings of n characters except that the characters in positions a and b are interchanged: xa = yb and ya = xb. Without loss of generality, let a > b. We have   Solutions for Chapter 11: Hash Tables  11-19   cid:3   n−1 cid:2    cid:4    cid:3  n−1 cid:2    cid:4   i=0  i=0  yi2ip  xi 2ip  mod  2p − 1  .  mod  2p − 1  −  h x  − h y  = Since 0 ≤ h x , h y  < 2p − 1, we have that − 2p − 1  < h x  − h y  < 2p − 1. If we show that  h x  − h y   mod  2p − 1  = 0, then h x  = h y . Since the sums in the hash functions are the same except for indices a and b, we have  h x  − h y   mod  2p − 1  =   xa2ap + xb2bp  −  ya2ap + yb2bp   mod  2p − 1  =   xa2ap + xb2bp  −  xb2ap + xa2bp   mod  2p − 1  =   xa − xb 2ap −  xa − xb 2bp  mod  2p − 1  =   xa − xb  2ap − 2bp   mod  2p − 1  =   xa − xb 2bp 2 a−b  p − 1   mod  2p − 1  . a−b−1 cid:2  By equation  A.5 , 2pi = 2 a−b  p − 1 and multiplying both sides by 2p − 1, we get 2 a−b  p − 1 = cid:10  cid:1  2p − 1  cid:3  a−b−1 cid:2  Thus,  h x  − h y   mod  2p − 1  = = 0 , since one of the factors is 2p − 1. We have shown that  h x  − h y   mod  2p − 1  = 0, and so h x  = h y .   cid:4   2p − 1   mod  2p − 1    xa − xb 2bp   2p − 1 .  a−b−1 i=0   cid:4    cid:3   i=0  i=0   cid:11   2pi  2pi  ,  to j ∈ B. We have u = cid:1   cid:10   Let b = B and u = U. We start by showing that the total number of collisions is minimized by a hash function that maps u b elements of U to each of the b values in B. For a given hash function, let uj be the number of elements that map j∈B u j . We also have that the number of collisions for a given value of j ∈ B is   cid:11  = u j  u j − 1  2.  u j 2  Lemma The total number of collisions is minimized when uj = u b for each j ∈ B. Proof If u j ≤ u b, let us call j underloaded, and if uj ≥ u b, let us call j  cid:19 = u b for at least overloaded. Consider an unbalanced situation in which uj one value j ∈ B. We can think of converting a balanced situation in which all u j equal u b into the unbalanced situation by repeatedly moving an element that maps to an underloaded value to map instead to an overloaded value.  If you think  Solution to Exercise 11.3-5   11-20  Solutions for Chapter 11: Hash Tables  of the values of B as representing buckets, we are repeatedly moving elements from buckets containing at most u b elements to buckets containing at least u b elements.  We now show that each such move increases the number of collisions, so that all the moves together must increase the number of collisions. Suppose that we move an element from an underloaded value j to an overloaded value k, and we leave all other elements alone. Because j is underloaded and k is overloaded, u j ≤ u b ≤ uk. Considering just the collisions for values j and k, we have u j  u j − 1  2 + uk  uk − 1  2 collisions before the move and  u j − 1  u j − 2  2 +  uk + 1 uk 2 collisions afterward. We wish to show that u j  u j − 1  2+ uk uk − 1  2 <  u j − 1  u j − 2  2+  uk + 1 uk 2. We have the following sequence of equivalent inequalities: u j < uk + 1 2u j < 2uk + 2 −uk < uk − 2u j + 2 − uk < u2  − 3u j + 2 + u2  − u j + u2  u j  u j − 1  + uk uk − 1  <  u j − 1  u j − 2  +  uk + 1 uk  u j  u j − 1  2 + uk  uk − 1  2 <  u j − 1  u j − 2  2 +  uk + 1 uk 2 . Thus, each move increases the number of collisions. We conclude that the number of collisions is minimized when u j = u b for each j ∈ B.  + uk  u2 j  k  k  j  By the above lemma, for any hash function, the total number of collisions must be at least b u b  u b − 1  2. The number of pairs of distinct elements is u u − 1  2. Thus, the number of collisions per pair of distinct elements must be at least b u b  u b − 1  2  u 2  u u − 1  2   cid:10    cid:11  =  = u b − 1 u − 1 u b − 1  >  = 1 b  u − 1 u  .  Thus, the bound  cid:6  on the probability of a collision for any pair of distinct elements can be no less than 1 b − 1 u = 1 B − 1 U.  Solution to Problem 11-1  a. Since we assume uniform hashing, we can use the same observation as is used in Corollary 11.7: that inserting a key entails an unsuccessful search followed by placing the key into the  cid:222 rst empty slot found. As in the proof of Theorem 11.6, if we let X be the random variable denoting the number of probes in an unsuc- cessful search, then Pr{X ≥ i} ≤ αi−1. Since n ≤ m 2, we have α ≤ 1 2. Let- ting i = k + 1, we have Pr{X > k} = Pr{X ≥ k + 1} ≤  1 2  k+1 −1 = 2 −k.   Solutions for Chapter 11: Hash Tables  11-21  −2 = 1 n2.  b. Substituting k = 2 lg n into the statement of part  a  yields that the probability −2 lg n =  that the ith insertion requires more than k = 2 lg n probes is at most 2  2lg n  −2 = n c. Let the event A be X > 2 lg n, and for i = 1, 2, . . . , n, let the event Ai be Xi > 2 lg n. In part  b , we showed that Pr{Ai} ≤ 1 n2 for i = 1, 2, . . . , n. From how we de cid:222 ned these events, A = A1 ∪ A2 ∪ ··· ∪ An. Using Boole cid:146 s inequality,  C.18 , we have Pr{A} ≤ Pr{A1} + Pr{A2} + ··· + Pr{An}  d. We use the de cid:222 nition of expectation and break the sum into two parts:  ≤ n · 1 n2 = 1 n .  E [X] = n cid:2  k · Pr{X = k}  cid:14 2 lg n cid:15  cid:2   cid:14 2 lg n cid:15  cid:2   k=1  k=1  =  k · Pr{X = k} +  n cid:2   k= cid:14 2 lg n cid:15 +1  k · Pr{X = k} n cid:2  n cid:2   ≤  k=1  =  cid:14 2 lg n cid:15   k= cid:14 2 lg n cid:15 +1   cid:14 2 lg n cid:15  · Pr{X = k} +  cid:14 2 lg n cid:15  cid:2   n · Pr{X = k} Pr{X = k} .  cid:1  cid:14 2 lg n cid:15  k=1 Pr{X = k} = k= cid:14 2 lg n cid:15 +1 Pr{X = k} ≤ Pr{X > 2 lg n} ≤ 1 n,  Pr{X = k} + n  cid:1   k= cid:14 2 lg n cid:15 +1  k=1 Since X takes on exactly one value, we have that Pr{X ≤  cid:14 2 lg n cid:15 } ≤ 1 and by part  c . Therefore, E [X] ≤  cid:14 2 lg n cid:15  · 1 + n ·  1 n  =  cid:14 2 lg n cid:15  + 1 = O lg n  .  n  Solution to Problem 11-2  a. A particular key is hashed to a particular slot with probability 1 n. Suppose we select a speci cid:222 c set of k keys. The probability that these k keys are inserted into the slot in question and that all other keys are inserted elsewhere is   cid:5    cid:6   k  1 n  n−k   cid:6   cid:5   cid:11   cid:10  1 − 1 n  cid:6   cid:5   cid:5  1 − 1 n  1 n  n k  Since there are Qk =  k  .   cid:6   n−k   cid:5    cid:6   n k  .  ways to choose our k keys, we get   11-22  Solutions for Chapter 11: Hash Tables   cid:8    cid:31   = k   cid:30  cid:7   b. For i = 1, 2, . . . , n, let Xi be a random variable denoting the number of keys that hash to slot i, and let Ai be the event that Xi = k, i.e., that exactly k keys hash to slot i. From part  a , we have Pr{A} = Qk. Then, Pk = Pr{M = k} max Xi 1≤i≤n  = Pr = Pr{there exists i such that Xi = k and that Xi ≤ k for i = 1, 2, . . . , n} ≤ Pr{there exists i such that Xi = k} = Pr{A1 ∪ A2 ∪ ··· ∪ An} ≤ Pr{A1} + Pr{A2} + ··· + Pr{An} = n Qk .   by inequality  C.18    c. We start by showing two facts.  First, 1 − 1 n < 1, which implies  1 − 1 n n−k < 1. Second, n!  n−k ! = n· n−1 · n−2 ···  n−k+1  < nk. Using these facts, along with the simpli cid:222 cation k! >  k e k of equation  3.17 , we have Qk =   cid:6    cid:6    cid:5   n−k  n!  k! n − k !  k   cid:5  1 − 1 1 n n nkk! n − k ! 1 k! ek kk  n!  <  <  <    1 − 1 n n−k < 1   n!  n − k ! < nk    k! >  k e k  .  d. Notice that when n = 2, lg lg n = 0, so to be precise, we need to assume that  n ≥ 3. In part  c , we showed that Qk < ek kk for any k; in particular, this inequality k0 < 1 n3 or, equivalently, that holds for k0. Thus, it suf cid:222 ces to show that ek0  k0 n3 < k0 Taking logarithms of both sides gives an equivalent condition: 3 lg n < k0 lg k0 − lg e   k0  ek0.   lg c + lg lg n − lg lg lg n − lg e  .  = c lg n lg lg n Dividing both sides by lg n gives the condition  cid:5   lg c + lg lg n − lg lg lg n − lg e  3 < 1 + lg c − lg e  lg lg n   cid:6   c  .  − lg lg lg n lg lg n  lg lg n  Let x be the last expression in parentheses: x =  1 + lg c − lg e  − lg lg lg n lg lg n  lg lg n  .  = c  cid:5    cid:6   We need to show that there exists a constant c > 1 such that 3 < cx. Noting that limn→∞ x = 1, we see that there exists n0 such that x ≥ 1 2 for all n ≥ n0. Thus, any constant c > 6 works for n ≥ n0.   Solutions for Chapter 11: Hash Tables  11-23  We handle smaller values of n cid:151 in particular, 3 ≤ n < n0 cid:151 as follows. Since n is constrained to be an integer, there are a  cid:222 nite number of n in the range 3 ≤ n < n0. We can evaluate the expression x for each such value of n and determine a value of c for which 3 < cx for all values of n. The  cid:222 nal value of c that we use is the larger of   6, which works for all n ≥ n0, and   max3≤n<n0 3 ≤ n < n0.  {c : 3 < cx}, i.e., the largest value of c that we chose for the range  < 1 n3, as desired. ≤ n Qk0  Thus, we have shown that Qk0 To see that Pk < 1 n2 for k ≥ k0, we observe that by part  b , Pk ≤ n Qk for   k0, we all k. Choosing k = k0 gives Pk0 will show that we can pick the constant c such that Qk < 1 n3 for all k ≥ k0, and thus conclude that Pk < 1 n2 for all k ≥ k0. To pick c as required, we let c be large enough that k0 > 3 > e. Then e k < 1 for all k ≥ k0, and so ek kk decreases as k increases. Thus, Qk < ek kk ≤ ek0  kk0 < 1 n3  for k ≥ k0.  e. The expectation of M is  k=0  k · Pr{M = k}  E [M] = n cid:2  = k0 cid:2  k · Pr{M = k} + n cid:2  ≤ k0 cid:2  k0 · Pr{M = k} + n cid:2  n cid:2  k0 cid:2   k=k0+1  k=0  k · Pr{M = k}  n · Pr{M = k}  k=k0+1  k=0 ≤ k0 = k0 · Pr{M ≤ k0} + n · Pr{M > k0} ,  Pr{M = k} + n  k=k0+1  k=0  Pr{M = k}  which is what we needed to show, since k0 = c lg n  lg lg n. To show that E [M] = O lg n  lg lg n , note that Pr{M ≤ k0} ≤ 1 and Pr{M > k0} =  Pr{M = k}  n cid:2  n cid:2  n cid:2   k=k0+1  Pk  k=k0+1  =  <  1 n2 k=k0+1 < n ·  1 n2  = 1 n .   by part  d     11-24  Solutions for Chapter 11: Hash Tables  We conclude that E [M] ≤ k0 · 1 + n ·  1 n  = k0 + 1 = O lg n  lg lg n  .  Solution to Problem 11-3  a. From how the probe-sequence computation is speci cid:222 ed, it is easy to see that the probe sequence is  cid:2 h k , h k  + 1, h k  + 1 + 2, h k  + 1 + 2 + 3, . . . , h k  + 1 + 2 + 3 + ··· + i , . . . cid:3 , where all the arithmetic is modulo m. Starting i cid:2  the probe numbers from 0, the ith probe is offset  modulo m  from h k  by  j = i  i + 1   = 1 2  i 2 + 1 2  i .  2   cid:5   j=0 Thus, we can write the probe sequence as  cid:4  k, i   =  mod m ,   cid:6   i 2  h  h k  + 1 2  i + 1 2  b. Let h   cid:4  k, i    cid:19 = h   cid:4  k, i   = h  cid:4  k, i   = h  which demonstrates that this scheme is a special case of quadratic probing.  cid:4  k, i   denote the ith probe of our scheme. We saw in part  a  that  cid:4  k, i   =  h k  + i  i + 1  2  mod m. To show that our algorithm examines h every table position in the worst case, we show that for a given key, each of the  cid:222 rst m probes hashes to a distinct value. That is, for any key k and for any probe numbers i and j such that 0 ≤ i < j < m, we have h  cid:4  k, j  .  cid:4  k, j   yields a contradiction. We do so by showing that h Let us assume that there exists a key k and probe numbers i and j satsifying 0 ≤ i < j < m for which h  cid:4  k, j  . Then h k  + i  i + 1  2 ≡ h k  + j   j + 1  2  mod m  , which in turn implies that i  i + 1  2 ≡ j   j + 1  2  mod m  , or j   j + 1  2 − i  i + 1  2 ≡ 0  mod m  . Since j   j + 1  2 − i  i + 1  2 =   j − i    j + i + 1  2, we have   j − i    j + i + 1  2 ≡ 0  mod m  . The factors j−i and j+i+1 must have different parities, i.e., j−i is even if and only if j+i+1 is odd.  Work out the various cases in which i and j are even and odd.  Since   j−i    j+i+1  2 ≡ 0  mod m , we have   j−i    j+i+1  2 = rm for some integer r or, equivalently,   j − i    j + i + 1  = r · 2m. Using the assumption that m is a power of 2, let m = 2p for some nonnegative integer p, so that now we have   j − i    j + i + 1  = r · 2p+1. Because exactly one of   Solutions for Chapter 11: Hash Tables  11-25  the factors j − i and j + i + 1 is even, 2p+1 must divide one of the factors. It cannot be j − i, since j − i < m < 2p+1. But it also cannot be j + i + 1, since j + i + 1 ≤  m − 1  +  m − 2  + 1 = 2m − 2 < 2p+1. Thus we have derived the contradiction that 2p+1 divides neither of the factors j − i and j + i + 1. We conclude that h   cid:4  k, i    cid:19 = h   cid:4  k, j  .    Lecture Notes for Chapter 12: Binary Search Trees  Chapter 12 overview  Search trees  Binary search trees    Data structures that support many dynamic-set operations.   Can be used as both a dictionary and as a priority queue.   Basic operations take time proportional to the height of the tree.    For complete binary tree with n nodes: worst case  cid:2  lg n .   For linear chain of n nodes: worst case  cid:2  n .    Different types of search trees include binary search trees, red-black trees  cov-  ered in Chapter 13 , and B-trees  covered in Chapter 18 .  We will cover binary search trees, tree walks, and operations on binary search trees.  Binary search trees are an important data structure for dynamic sets.   Accomplish many dynamic-set operations in O h  time, where h = height of  tree.    As in Section 10.4, we represent a binary tree by a linked data structure in which  each node is an object. root[T ] points to the root of tree T .      Each node contains the  cid:222 elds    key  and possibly other satellite data .      left: points to left child. right: points to right child.    p: points to parent. p[root[T ]] = NIL.    Stored keys must satisfy the binary-search-tree property.     If y is in left subtree of x, then key[y] ≤ key[x].   12-2  Lecture Notes for Chapter 12: Binary Search Trees     If y is in right subtree of x, then key[y] ≥ key[x].  Draw sample tree. [ThisisFigure12.1 a  fromthetext, using A, B, D, F, H, K inplace of2,3,5, 5,7,8,withalphabetic comparisons. It cid:146 sOKtohaveduplicate keys,thoughthere arenoneinthisexample. Showthatthebinary-search-tree propertyholds.]  F  B  H  A  D  K  The binary-search-tree property allows us to print keys in a binary search tree in order, recursively, using an algorithm called an inorder tree walk. Elements are printed in monotonically increasing order. How INORDER-TREE-WALK works:   Check to make sure that x is not NIL.   Recursively, print the keys of the nodes in x cid:146 s left subtree.   Print x cid:146 s key.   Recursively, print the keys of the nodes in x cid:146 s right subtree.  INORDER-TREE-WALK x  if x  cid:19 = NIL then INORDER-TREE-WALK left[x]   print key[x] INORDER-TREE-WALK right[x]   Example: Do the inorder tree walk on the example above, getting the output AB DF H K .  Correctness: Follows by induction directly from the binary-search-tree property.  Time: Intuitively, the walk takes  cid:2  n  time for a tree with n nodes, because we visit and print each node once. [Bookhasformalproof.]  Querying a binary search tree  Searching  TREE-SEARCH x, k  if x = NIL or k = key[x] then return x  if k < key[x]  then return TREE-SEARCH left[x], k  else return TREE-SEARCH right[x], k   Initial call is TREE-SEARCH root[T ], k .   Lecture Notes for Chapter 12: Binary Search Trees  12-3  Example: Search for values D and C in the example tree from above.  Time: The algorithm recurses, visiting nodes on a downward path from the root. Thus, running time is O h , where h is the height of the tree. [The text also gives an iterative version of TREE-SEARCH, which is more ef cid:222 - cientonmostcomputers. Theaboverecursive procedure ismorestraightforward, however.]  Minimum and maximum  The binary-search-tree property guarantees that      the minimum key of a binary search tree is located at the leftmost node, and the maximum key of a binary search tree is located at the rightmost node.  Traverse the appropriate pointers  left or right  until NIL is reached.  TREE-MINIMUM x  while left[x]  cid:19 = NIL do x ← left[x]  return x  TREE-MAXIMUM x  while right[x]  cid:19 = NIL do x ← right[x]  return x  Time: Both procedures visit nodes that form a downward path from the root to a leaf. Both procedures run in O h  time, where h is the height of the tree.  Successor and predecessor  Assuming that all keys are distinct, the successor of a node x is the node y such that key[y] is the smallest key > key[x].  We can  cid:222 nd x cid:146 s successor based entirely on the tree structure. No key comparisons are necessary.  If x has the largest key in the binary search tree, then we say that x cid:146 s successor is NIL. There are two cases:  1. If node x has a non-empty right subtree, then x cid:146 s successor is the minimum in  x cid:146 s right subtree.  2. If node x has an empty right subtree, notice that:    As long as we move to the left up the tree  move up through right children ,  we cid:146 re visiting smaller keys.    x cid:146 s successor y is the node that x is the predecessor of  x is the maximum in  y cid:146 s left subtree .   12-4  Lecture Notes for Chapter 12: Binary Search Trees  TREE-PREDECESSOR is symmetric to TREE-SUCCESSOR.  TREE-SUCCESSOR x  if right[x]  cid:19 = NIL then return TREE-MINIMUM right[x]  y ← p[x] while y  cid:19 = NIL and x = right[y]  do x ← y  y ← p[y]  return y  Example:  15  6  18  3  7  17  20  2  4  13  9    Find the successor of the node with key value 15.  Answer: Key value 17    Find the successor of the node with key value 6.  Answer: Key value 7    Find the successor of the node with key value 4.  Answer: Key value 6    Find the predecessor of the node with key value 6.  Answer: Key value 4   Time: For both the TREE-SUCCESSOR and TREE-PREDECESSOR procedures, in both cases, we visit nodes on a path down the tree or up the tree. Thus, running time is O h , where h is the height of the tree.  Insertion and deletion  Insertion and deletion allows the dynamic set represented by a binary search tree to change. The binary-search-tree property must hold after the change. Insertion is more straightforward than deletion.   Lecture Notes for Chapter 12: Binary Search Trees  12-5  Insertion  TREE-INSERT T, z  y ← NIL x ← root[T ] while x  cid:19 = NIL do y ← x  if key[z] < key[x] then x ← left[x] else x ← right[x]  p[z] ← y if y = NIL then root[T ] ← z else if key[z] < key[y] then left[y] ← z else right[y] ← z   cid:1  Tree T was empty    To insert value v into the binary search tree, the procedure is given node z, with  key[z] = v, left[z] = NIL, and right[z] = NIL.    Beginning at root of the tree, trace a downward path, maintaining two pointers.    Pointer x: traces the downward path.   Pointer y:  cid:147 trailing pointer cid:148  to keep track of parent of x.    Traverse the tree downward by comparing the value of node at x with v, and  move to the left or right child accordingly.    When x is NIL, it is at the correct position for node z.   Compare z cid:146 s value with y cid:146 s value, and insert z at either y cid:146 s left or right, appro-  priately.  Example: Run TREE-INSERT C  on the  cid:222 rst sample binary search tree. Result:  F  B  H  A  D  K  C  Time: Same as TREE-SEARCH. On a tree of height h, procedure takes O h  time. TREE-INSERT can be used with INORDER-TREE-WALK to sort a given set of num- bers.  See Exercise 12.3-3.    12-6  Lecture Notes for Chapter 12: Binary Search Trees  Deletion  TREE-DELETE is broken into three cases.  Case 1: z has no children.    Delete z by making the parent of z point to NIL, instead of to z.  Case 2: z has one child.  Case 3: z has two children.    Delete z by making the parent of z point to z cid:146 s child, instead of to z.     z cid:146 s successor y has either no children or one child. node cid:151 with no left child cid:151 in z cid:146 s right subtree.     Delete y from the tree  via Case 1 or 2 .   Replace z cid:146 s key and satellite data with y cid:146 s.   y is the minimum  TREE-DELETE  T, z   cid:1  Determine which node y to splice out: either z or z cid:146 s successor. if left[z] = NIL or right[z] = NIL then y ← z else y ← TREE-SUCCESSOR z   cid:1  x is set to a non-NIL child of y, or to NIL if y has no children. if left[y]  cid:19 = NIL then x ← left[y] else x ← right[y]  cid:1  y is removed from the tree by manipulating pointers of p[y] and x. if x  cid:19 = NIL then p[x] ← p[y] if p[y] = NIL then root[T ] ← x else if y = left[p[y]]  then left[p[y]] ← x else right[p[y]] ← x   cid:1  If it was z cid:146 s successor that was spliced out, copy its data into z. if y  cid:19 = z then key[z] ← key[y]  copy y cid:146 s satellite data into z  return y  Example: We can demonstrate on the above sample tree.   For Case 1, delete K .   For Case 2, delete H.   For Case 3, delete B, swapping it with C.  Time: O h , on a tree of height h.   Expected height of a randomly built binary search tree  Lecture Notes for Chapter 12: Binary Search Trees  12-7  Minimizing running time  We cid:146 ve been analyzing running time in terms of h  the height of the binary search tree , instead of n  the number of nodes in the tree .   Problem: Worst case for binary search tree is  cid:2  n  cid:151 no better than linked list.   Solution: Guarantee small height  balanced tree  cid:151 h = O lg n . In later chapters, by varying the properties of binary search trees, we will be able to analyze running time in terms of n.   Method: Restructure the tree if necessary. Nothing special is required for querying, but there may be extra work when changing the structure of the tree  inserting or deleting .  Red-black trees are a special class of binary trees that avoids the worst-case behav- ior of O n  like  cid:147 plain cid:148  binary search trees. Red-black trees are covered in detail in Chapter 13.  [These are notes on a starred section in the book. I covered this material in an optionallecture.] Given a set of n distinct keys. Insert them in random order into an initially empty binary search tree.   Each of the n! permutations is equally likely.   Different from assuming that every binary search tree on n keys is equally likely. Try it for n = 3. Will get 5 different binary search trees. When we look at the binary search trees resulting from each of the 3! input permutations, 4 trees will appear once and 1 tree will appear twice. [Thisgives theidea forthe solution toExercise12.4-3.]    Forget about deleting keys. We will show that the expected height of a randomly built binary search tree is O lg n .  Random variables  De cid:222 ne the following random variables:   Xn = height of a randomly built binary search tree on n keys.   Yn = 2Xn = exponential height.   Rn = rank of the root within the set of n keys used to build the binary search     tree.   Equally likely to be any element of{1, 2, . . . , n}.  If Rn = i, then   Left subtree is a randomly-built binary search tree on i − 1 keys.   Right subtree is a randomly-built binary search tree on n − i keys.   12-8  Lecture Notes for Chapter 12: Binary Search Trees  Foreshadowing  We will need to relate E [Yn] to E [Xn]. We cid:146 ll use Jensen cid:146 s inequality: E [ f  X  ] ≥ f  E [X]  , provided      the expectations exist and are  cid:222 nite, and f  x  is convex: for all x, y and all 0 ≤ λ ≤ 1 f  λx +  1 − λ y  ≤ λ f  x  +  1 − λ  f  y  .  [leaveonboard]  f y   λf x  +  1 cid:150 λ f y   f x   f λx +  1 cid:150 λ y   x  λx +  1 cid:150 λ y  y  Convex ≡  cid:147 curves upward cid:148   We cid:146 ll use Jensen cid:146 s inequality for f  x  = 2x.  Since 2x curves upward, it cid:146 s convex.  Formula for Yn Think about Yn, if we know that Rn = i:  i cid:150 1 nodes  n cid:150 i nodes  Height of root is 1 more than the maximum height of its children:   Lecture Notes for Chapter 12: Binary Search Trees  12-9  Yn = 2 · max Yi−1, Yn−i   . Base cases:   Y1 = 1  expected height of a 1-node tree is 20 = 1 .   De cid:222 ne Y0 = 0. De cid:222 ne indicator random variables Zn,1, Zn,2, . . . , Zn,n: Zn,i = I{Rn = i} . Rn is equally likely to be any element of{1, 2, . . . , n} ⇒ Pr{Rn = i} = 1 n ⇒ E [Zn,i ] = 1 n  [leaveonboard]   since E [I{A}] = Pr{A}   Consider a given n-node binary search tree  which could be a subtree . Exactly one Zn,i is 1, and all others are 0. Hence,  Yn = n cid:2   Zn,i ·  2 · max Yi−1, Yn−i    .  i=1  [leaveonboard] [Recall: Yn = 2 · max Yi−1, Yn−i   wasassumingthat Rn = i.]  Bounding E [Yn] We will show that E [Yn] is polynomial in n, which will imply that E[Xn] = O lg n .  Claim Zn,i is independent of Yi−1 and Yn−i. Justi cid:222 cation If we choose the root such that Rn = i, the left subtree contains i − 1 nodes, and it cid:146 s like any other randomly built binary search tree with i − 1 nodes. Other than the number of nodes, the left subtree cid:146 s structure has nothing to do with it being the left subtree of the root. Hence, Yi−1 and Zn,i are independent. Similarly, Yn−i and Zn,i are independent.   claim   Fact If X and Y are nonnegative random variables, then E[max X, Y  ] ≤ E [X]+E [Y ]. [Leaveonboard. ThisisExerciseC.3-4fromthetext.]   cid:14   Thus, E [Yn] = E   cid:13  n cid:2  = n cid:2  = n cid:2   i=1  i=1  Zn,i  2 · max Yi−1, Yn−i    i=1 E [Zn,i ·  2 · max Yi−1, Yn−i   ] E [Zn,i ] · E [2 · max Yi−1, Yn−i  ]   linearity of expectation    independence    12-10  Lecture Notes for Chapter 12: Binary Search Trees  1 n  = n cid:2  n cid:2  n cid:2   i=1 = 2 n ≤ 2 n  i=1  i=1  · E [2 · max Yi−1, Yn−i  ]  E [max Yi−1, Yn−i  ]  E [Yi−1] + E [Yn−i ]    E [Zn,i] = 1 n   E [a X] = a E [X]    earlier fact  .  Observe that the last summation is  E [Y0] + E [Yn−1]  +  E [Y1] + E [Yn−2]  +  E [Y2] + E [Yn−3]   +··· +  E [Yn−1] + E [Y0]  = 2  E [Yi] ,  n−1 cid:2   i=0  and so we get the recurrence E [Yn] ≤ 4 n  E [Yi] .  n−1 cid:2   i=0  Solving the recurrence  [leaveonboard]   cid:6    cid:5   cid:6   .  n + 3  cid:5  3  We will show that for all integers n > 0, this recurrence has the solution E [Yn] ≤ 1  cid:5  4 n−1 cid:2  i + 3 3   cid:6  n + 3 4  Lemma  =  .  i=0 [ThislemmasolvesExercise12.4-1.]  n k   cid:5   cid:6   cid:6   cid:5    cid:6   n 4  Proof Use Pascal cid:146 s identity  Exercise C.1-7 :   cid:5    cid:6  Also using the simple identity n + 3 4  3 3   cid:5    cid:5   cid:6   cid:5   cid:6  = 1 = 4 4 n + 2  cid:5   cid:6   cid:5  4 n + 1 n + 1  cid:5   cid:5   cid:6   cid:6  4 3 n + 1  cid:6   cid:5   cid:5   cid:6  3 n + 1  cid:5   cid:6   cid:5   cid:6  3 n + 1  cid:6  3  + +  + +  n 3 n 3  +  n 3  .  + + +  = = =   cid:5   cid:5   cid:5   cid:5   cid:5  = n−1 cid:2    cid:6  n + 2  cid:6  3 n + 2  cid:6  3 n + 2  cid:6  3 n + 2  cid:6  + 3 n + 2  cid:5  + 3 i + 3 3  ... = =  i=0   cid:5   cid:5    cid:6   cid:6   4 3 4 3  + +   cid:5   cid:5    cid:6   cid:6   4 4 3 3  + ··· + + ··· +   cid:6   =   cid:5    cid:6  n − 1 k − 1   cid:5   cid:6  n − 1 k  .  +  , we have   lemma    Lecture Notes for Chapter 12: Binary Search Trees  12-11  for all i < n. Then   inductive hypothesis    cid:6    cid:5  · 4 = 1 . i + 3 3   from before    cid:5    cid:6   i=0  E [Yi]  = 1 4  1 + 3 3  We solve the recurrence by induction on n. Basis: n = 1. 1 = Y1 = E [Y1] ≤ 1 4 Inductive step: Assume that E [Yi] ≤ 1 n−1 cid:2  4 E [Yn] ≤ 4  cid:5   cid:6  n−1 cid:2  n i + 3 ≤ 4  cid:5   cid:6  n−1 cid:2  3 n i + 3  cid:5   cid:6  = 1 i=0 3 n n + 3 = 1 4 n  n + 3 ! · = 1 4!  n − 1 ! n  cid:5   cid:6  ·  n + 3 ! = 1 4 3! n! n + 3 = 1 3 4   lemma   i=0  1 4  .  Thus, we cid:146 ve proven that E[Yn] ≤ 1 4   cid:5    cid:6   .  n + 3 3  Bounding E [Xn]  With our bound on E [Yn], we use Jensen cid:146 s inequality to bound E[Xn]: 2E[Xn] ≤ E [2Xn ] = E [Yn] .  cid:5   cid:6  Thus, n + 3 2E[Xn] ≤ 1 4 3 ·  n + 3  n + 2  n + 1  = 1 4 = O n3  .  6  Taking logs of both sides gives E[Xn] = O lg n . Done!   Solutions for Chapter 12: Binary Search Trees  Solution to Exercise 12.1-2  In a heap, a node cid:146 s key is ≥ both of its children cid:146 s keys. In a binary search tree, a node cid:146 s key is ≥ its left child cid:146 s key, but ≤ its right child cid:146 s key. The heap property, unlike the binary-searth-tree property, doesn cid:146 t help print the nodes in sorted order because it doesn cid:146 t tell which subtree of a node contains the element to print before that node. In a heap, the largest element smaller than the node could be in either subtree. Note that if the heap property could be used to print the keys in sorted order in O n  time, we would have an O n -time algorithm for sorting, because building the heap takes only O n  time. But we know  Chapter 8  that a comparison sort must take  cid:3  n lg n  time.  Solution to Exercise 12.2-5  Let x be a node with two children. In an inorder tree walk, the nodes in x cid:146 s left subtree immediately precede x and the nodes in x cid:146 s right subtree immediately fol- low x. Thus, x cid:146 s predecessor is in its left subtree, and its successor is in its right subtree. Let s be x cid:146 s successor. Then s cannot have a left child, for a left child of s would come between x and s in the inorder walk.  It cid:146 s after x because it cid:146 s in x cid:146 s right subtree, and it cid:146 s before s because it cid:146 s in s cid:146 s left subtree.  If any node were to come between x and s in an inorder walk, then s would not be x cid:146 s successor, as we had supposed. Symmetrically, x cid:146 s predecessor has no right child.  Solution to Exercise 12.2-7  Note that a call to TREE-MINIMUM followed by n − 1 calls to TREE-SUCCESSOR performs exactly the same inorder walk of the tree as does the procedure INORDER- INORDER-TREE-WALK prints the TREE-MINIMUM  cid:222 rst, and by TREE-WALK.   Solutions for Chapter 12: Binary Search Trees  12-13  de cid:222 nition, the TREE-SUCCESSOR of a node is the next node in the sorted order determined by an inorder tree walk. This algorithm runs in  cid:2  n  time because:      It requires  cid:3  n  time to do the n procedure calls. It traverses each of the n − 1 tree edges at most twice, which takes O n  time. To see that each edge is traversed at most twice  once going down the tree and once going up , consider the edge between any node u and either of its children, node v. By starting at the root, we must traverse  u, v  downward from u to v, before traversing it upward from v to u. The only time the tree is traversed downward is in code of TREE-MINIMUM, and the only time the tree is traversed upward is in code of TREE-SUCCESSOR when we look for the successor of a node that has no right subtree. Suppose that v is u cid:146 s left child.   Before printing u, we must print all the nodes in its left subtree, which is rooted  at v, guaranteeing the downward traversal of edge  u, v .    After all nodes in u cid:146 s left subtree are printed, u must be printed next. Procedure TREE-SUCCESSOR traverses an upward path to u from the maximum element  which has no right subtree  in the subtree rooted at v. This path clearly includes edge  u, v , and since all nodes in u cid:146 s left subtree are printed, edge  u, v  is never traversed again.  Now suppose that v is u cid:146 s right child.   After u is printed, TREE-SUCCESSOR u  is called. To get to the minimum element in u cid:146 s right subtree  whose root is v , the edge  u, v  must be traversed downward.    After all values in u cid:146 s right subtree are printed, TREE-SUCCESSOR is called on the maximum element  again, which has no right subtree  in the subtree rooted at v. TREE-SUCCESSOR traverses a path up the tree to an element after u, since u was already printed. Edge  u, v  must be traversed upward on this path, and since all nodes in u cid:146 s right subtree have been printed, edge  u, v  is never traversed again.  Hence, no edge is traversed twice in the same direction. Therefore, this algorithm runs in  cid:2  n  time.  Solution to Exercise 12.3-3  Here cid:146 s the algorithm:  TREE-SORT A  let T be an empty binary search tree for i ← 1 to n  do TREE-INSERT T, A[i]   INORDER-TREE-WALK root[T ]    12-14  Solutions for Chapter 12: Binary Search Trees  Solution to Exercise 12.4-1  Worst case:  cid:2  n2  cid:151 occurs when a linear chain of nodes results from the repeated TREE-INSERT operations. Best case:  cid:2  n lg n  cid:151 occurs when a binary tree of height  cid:2  lg n  results from the repeated TREE-INSERT operations.  We will answer the second part  cid:222 rst. We shall show that if the average depth of a n lg n . Then we will answer the node is  cid:2  lg n , then the height of the tree is O   cid:222 rst part by exhibiting that this bound is tight: there is a binary search tree with average node depth  cid:2  lg n  and height  cid:2     cid:12   cid:12  n lg n  = ω lg n .   cid:12   Lemma If the average depth of a node in an n-node binary search tree is  cid:2  lg n , then the height of the tree is O   n lg n .   cid:3  cid:2   Proof Suppose that an n-node binary search tree has average depth  cid:2  lg n  and height h. Then there exists a path from the root to a node at depth h, and the depths of the nodes on this path are 0, 1, . . . , h. Let P be the set of nodes on this path and Q be all other nodes. Then the average depth of a node is 1 n  depth x  +   cid:2   depth y   depth x    cid:4   x∈P  y∈Q   cid:12  n lg n , so that h =  For the purpose of contradiction, suppose that h is not O  ω  1 n  · ω n lg n    cid:12  n lg n . Then we have ·  cid:2  h2  = 1  cid:12  n = ω lg n  ,  which contradicts the assumption that the average depth is  cid:2  lg n . Thus, the height is O   n lg n .  Here is an example of an n-node binary search tree with average node depth  cid:2  lg n  but height ω lg n :   cid:2  h cid:2   x∈P  d  d=0 ·  cid:2  h2  .  ≥ 1 n = 1 n = 1 n  n − cid:12   n lg n  nodes   cid:12   n lg n nodes   12-15   cid:12   nodes protrude from below as a single chain. This tree has height  n lg n nodes are a complete binary tree, and the other  n lg n  Solutions for Chapter 12: Binary Search Trees  In this tree, n − cid:12  n lg n   + cid:12   cid:2  lg n − cid:12  an upper bound on the depth of each of the n − cid:12  binary tree part and O lg n + cid:12   cid:12    cid:12  n lg n =  cid:2   = ω lg n  .  n lg n    cid:12  n lg n  lg n + cid:12   the bounded from above by · O  1 n  n lg n  +  n − cid:12   To compute an upper bound on the average depth of a node, we use O lg n  as n lg n nodes in the complete n lg n  as an upper bound on the depth of each of n lg n nodes in the protruding chain. Thus, the average depth of a node is  n lg n  lg n  = 1 n  · O n lg n   To bound the average depth of a node from below, observe that the bottommost n lg n  nodes, and each of these  level of the complete binary tree part has  cid:2  n − cid:12  ·  cid:2   n − cid:12   nodes has depth  cid:2  lg n . Thus, the average node depth is at least 1 n  n lg n  lg n  = 1 n  = O lg n  .  ·  cid:3  n lg n   =  cid:3  lg n  .  Because the average node depth is both O lg n  and  cid:3  lg n , it is  cid:2  lg n .  We cid:146 ll go one better than showing that the function 2x is convex. Instead, we cid:146 ll show that the function cx is convex, for any positive constant c. According to the de cid:222 nition of convexity on page 1109 of the text, a function f  x  is convex if for all x and y and for all 0 ≤ λ ≤ 1, we have f  λx +  1− λ y  ≤ λ f  x +  1− λ  f  y . Thus, we need to show that for all 0 ≤ λ ≤ 1, we have cλx+ 1−λ y ≤ λcx+ 1−λ cy. We start by proving the following lemma.  Lemma For any real numbers a and b and any positive real number c, ca ≥ cb +  a − b cb ln c . Proof We  cid:222 rst show that for all real r, we have cr ≥ 1+ r ln c. By equation  3.11  from the text, we have ex ≥ 1+ x for all real x. Let x = r ln c, so that ex = er ln c =  eln c r = cr . Then we have cr = er ln c ≥ 1 + r ln c. Substituting a − b for r in the above inequality, we have ca−b ≥ 1 +  a − b  ln c. Multiplying both sides by cb gives ca ≥ cb +  a − b cb ln c.  lemma  Now we can show that cλx+ 1−λ y ≤ λcx +  1 − λ cy for all 0 ≤ λ ≤ 1. For convenience, let z = λx +  1 − λ y. In the inequality given by the lemma, substitute x for a and z for b, giving cx ≥ cz +  x − z cz ln c .  Solution to Exercise 12.4-4   12-16  Solutions for Chapter 12: Binary Search Trees  Also substitute y for a and z for b, giving cy ≥ cz +  y − z cz ln c . If we multiply the  cid:222 rst inequality by λ and the second by 1 − λ and then add the resulting inequalities, we get λcx +  1 − λ cy ≥ λ cz +  x − z cz ln c  +  1 − λ  cz +  y − z cz ln c  = λcz + λxcz ln c − λzcz ln c +  1 − λ cz +  1 − λ ycz ln c −  1 − λ zcz ln c =  λ +  1 − λ  cz +  λx +  1 − λ y cz ln c −  λ +  1 − λ  zcz ln c = cz + zcz ln c − zcz ln c = cz = cλx+ 1−λ y , as we wished to show.  Solution to Problem 12-2  To sort the strings of S, we  cid:222 rst insert them into a radix tree, and then use a preorder tree walk to extract them in lexicographically sorted order. The tree walk outputs strings only for nodes that indicate the existence of a string  i.e., those that are lightly shaded in Figure 12.5 of the text .  Correctness: The preorder ordering is the correct order because:   Any node cid:146 s string is a pre cid:222 x of all its descendants cid:146  strings and hence belongs  before them in the sorted order  rule 2 .    A node cid:146 s left descendants belong before its right descendants because the corre- sponding strings are identical up to that parent node, and in the next position the left subtree cid:146 s strings have 0 whereas the right subtree cid:146 s strings have 1  rule 1 .  Time:  cid:2  n .    Insertion takes  cid:2  n  time, since the insertion of each string takes time propor- tional to its length  traversing a path through the tree whose length is the length of the string , and the sum of all the string lengths is n.    The preorder tree walk takes O n  time. It is just like INORDER-TREE-WALK  it prints the current node and calls itself recursively on the left and right sub- trees , so it takes time proportional to the number of nodes in the tree. The number of nodes is at most 1 plus the sum  n  of the lengths of the binary strings in the tree, because a length-i string corresponds to a path through the root and i other nodes, but a single node may be shared among many string paths.   Solutions for Chapter 12: Binary Search Trees  12-17  Solution to Problem 12-3   cid:1   ties by n gives the desired equation.  x∈T d x, T  . Dividing both quanti- a. The total path length P T   is de cid:222 ned as b. For any node x in TL, we have d x, TL   = d x, T   − 1, since the distance to the root of TL is one less than the distance to the root of T . Similarly, for any node x in TR, we have d x, TR   = d x, T   − 1. Thus, if T has n nodes, we have P T   = P TL   + P TR  + n − 1 , since each of the n nodes of T  except the root  is in either TL or TR.  c.  If T is a randomly built binary search tree, then the root is equally likely to be any of the n elements in the tree, since the root is the  cid:222 rst element inserted. It follows that the number of nodes in subtree TL is equally likely to be any integer in the set {0, 1, . . . , n − 1}. The de cid:222 nition of P n  as the average total path length of a randomly built binary search tree, along with part  b , gives us the recurrence P n  = 1 n   P i   + P n − i − 1  + n − 1  .  n−1 cid:2   d. Since P 0  = 0, and since for k = 1, 2, . . . , n − 1, each term P k  in the summation appears once as P i   and once as P n − i − 1 , we can rewrite the equation from part  c  as P n  = 2 n  P k  +  cid:2  n  .  n−1 cid:2   i=0  k=1  e. Observe that if, in the recurrence  7.6  in part  c  of Problem 7-2, we replace E [T  · ] by P ·  and we replace q by k, we get almost the same recurrence as in part  d  of Problem 12-3. The remaining difference is that in Problem 12-3 d , the summation starts at 1 rather than 2. Observe, however, that a binary tree with just one node has a total path length of 0, so that P 1  = 0. Thus, we can rewrite the recurrence in Problem 12-3 d  as P n  = 2 n  P k  +  cid:2  n   n−1 cid:2   k=2  and use the same technique as was used in Problem 7-2 to solve it. We start by solving part  d  of Problem 7-2: showing that  k=2 Following the hint in Problem 7-2 d , we split the summation into two parts:  k lg k ≤ 1 2  k lg k =  n2 .  n2 lg n − 1 8  cid:14 n 2 cid:15 −1 cid:2   k lg k + n−1 cid:2   k=2  k lg k .  k= cid:14 n 2 cid:15   n−1 cid:2  n−1 cid:2   k=2   12-18  Solutions for Chapter 12: Binary Search Trees  The lg k in the  cid:222 rst summation on the right is less than lg n 2  = lg n − 1, and n−1 cid:2  the lg k in the second summation is less than lg n. Thus,  n−1 cid:2   k=2  k lg k <  lg n − 1  n−1 cid:2    cid:14 n 2 cid:15 −1 cid:2   cid:14 n 2 cid:15 −1 cid:2   k=2  k  k= cid:14 n 2 cid:15   k + lg n  cid:7    cid:8  − 1  n 2  n 2  k −  = lg n ≤ 1 2 ≤ 1 2  k k=2 k=2 n n − 1  lg n − 1 2 n2 lg n − 1 8  n2  k=2  n−1 cid:2   P k  +  cid:2  n   if n ≥ 2. Now we show that the recurrence P n  = 2 n has the solution P n  = O n lg n . We use the substitution method. Assume inductively that P n  ≤ an lg n + b for some positive constants a and b to be determined. We can pick a and b suf cid:222 ciently large so that an lg n + b ≥ P 1 . Then, for n > 1, we have by substitution P n  = 2 n ≤ 2 n = 2a n ≤ 2a n ≤ an lg n − a 4 = an lg n + b + ≤ an lg n + b ,  n−1 cid:2  P k  +  cid:2  n  n−1 cid:2   ak lg k + b  +  cid:2  n  n−1 cid:2   cid:5   k lg k + 2b n n2 lg n − 1 + 2b 8 n  cid:7  n + 2b +  cid:2  n   cid:2  n  + b − a 4   n − 2  +  cid:2  n   cid:8    cid:6   n − 2  +  cid:2  n   k=2 1 2  k=2  k=2  n2  n  4 n dominates  cid:2  n  + b. Thus,  since we can choose a large enough so that a P n  = O n lg n .  f. We draw an analogy between inserting an element into a subtree of a binary search tree and sorting a subarray in quicksort. Observe that once an element x is chosen as the root of a subtree T , all elements that will be inserted after x into T will be compared to x. Similarly, observe that once an element y is chosen as the pivot in a subarray S, all other elements in S will be compared to y. Therefore, the quicksort implementation in which the comparisons are the same as those made when inserting into a binary search tree is simply to consider the pivots in the same order as the order in which the elements are inserted into the tree.   Lecture Notes for Chapter 13: Red-Black Trees  Chapter 13 overview  Red-black trees  Red-black trees    A variation of binary search trees.   Balanced: height is O lg n , where n is the number of nodes.   Operations will take O lg n  time in the worst case.  [Thesenotes areabitsimpler than the treatment inthebook, tomakethem more amenable to a lecture situation. Our students  cid:222 rst see red-black trees in a course that precedes our algorithms course. This set of lecture notes is intended as a refresher for the students, bearing in mind that some time may have passed since theylastsawred-blacktrees. Theproceduresinthischapterareratherlongsequencesofpseudocode. Youmight wanttomakearrangementstoprojectthemratherthanspendingtimewritingthem onaboard.]  A red-black tree is a binary search tree + 1 bit per node: an attribute color, which is either red or black. All leaves are empty  nil  and colored black.   We use a single sentinel, nil[T ], for all the leaves of red-black tree T .     The root cid:146 s parent is also nil[T ]. All other attributes of binary search trees are inherited by red-black trees  key, left, right, and p . We don cid:146 t care about the key in nil[T ].  color[nil[T ]] is black.  Red-black properties  [Leavetheseupontheboard.]   13-2  Lecture Notes for Chapter 13: Red-Black Trees  1. Every node is either red or black. 2. The root is black. 3. Every leaf  nil[T ]  is black. 4. If a node is red, then both its children are black.  Hence no two reds in a row  on a simple path from the root to a leaf.   5. For each node, all paths from the node to descendant leaves contain the same  number of black nodes.  Example:  26  h = 4 bh = 2  17  h = 1 bh = 1  41  h = 3 bh = 2  30  h = 2 bh = 1  47  h = 2 bh = 1  38  h = 1 bh = 1  50  h = 1 bh = 1  nil[T]  [Nodeswithboldoutlineindicateblacknodes. Don cid:146 taddheightsandblack-heights yet. Wewon cid:146 tbotherwithdrawing nil[T ] anymore.]  Height of a red-black tree    Height of a node is the number of edges in a longest path to a leaf.   Black-height of a node x: bh x  is the number of black nodes  including nil[T ]  on the path from x to leaf, not counting x. By property 5, black-height is well de cid:222 ned.  [Nowlabeltheexampletreewithheight h and bh values.]  Claim Any node with height h has black-height ≥ h 2. Proof By property 4, ≤ h 2 nodes on the path from the node to a leaf are red. Hence ≥ h 2 are black.  claim   Claim The subtree rooted at any node x contains ≥ 2bh x   − 1 internal nodes.   Lecture Notes for Chapter 13: Red-Black Trees  13-3  Proof By induction on height of x. Basis: Height of x = 0 ⇒ x is a leaf ⇒ bh x  = 0. The subtree rooted at x has 0 internal nodes. 20 − 1 = 0. Inductive step: Let the height of x be h and bh x  = b. Any child of x has height h − 1 and black-height either b  if the child is red  or b − 1  if the child is black . By the inductive hypothesis, each child has ≥ 2bh x  −1 − 1 internal nodes. Thus, the subtree rooted at x contains ≥ 2 ·  2bh x  −1 − 1  + 1 = 2bh x   − 1 internal nodes.  The +1 is for x itself.   claim   Lemma A red-black tree with n internal nodes has height ≤ 2 lg n + 1 . Proof Let h and b be the height and black-height of the root, respectively. By the above two claims, n ≥ 2b − 1 ≥ 2h 2 − 1 . Adding 1 to both sides and then taking logs gives lg n + 1  ≥ h 2, which implies that h ≤ 2 lg n + 1 .  theorem   Operations on red-black trees  The non-modifying binary-search-tree operations MINIMUM, MAXIMUM, SUC- CESSOR, PREDECESSOR, and SEARCH run in O height  time. Thus, they take O lg n  time on red-black trees. Insertion and deletion are not so easy. If we insert, what color to make the new node?   Red? Might violate property 4.   Black? Might violate property 5. If we delete, thus removing a node, what color was the node that was removed?   Red? OK, since we won cid:146 t have changed any black-heights, nor will we have created two red nodes in a row. Also, cannot cause a violation of property 2, since if the removed node was red, it could not have been the root.    Black? Could cause there to be two reds in a row  violating property 4 , and can also cause a violation of property 5. Could also cause a violation of prop- erty 2, if the removed node was the root and its child cid:151 which becomes the new root cid:151 was red.  Rotations    The basic tree-restructuring operation.   Needed to maintain red-black trees as balanced binary search trees.   Changes the local pointer structure.  Only pointers are changed.    13-4  Lecture Notes for Chapter 13: Red-Black Trees    Won cid:146 t upset the binary-search-tree property.   Have both left rotation and right rotation. They are inverses of each other.   A rotation takes a red-black-tree and a node within the tree.  LEFT-ROTATE T, x   x  α  y  RIGHT-ROTATE T, y   β  γ  α  x  γ  y  β  LEFT-ROTATE T, x  y ← right[x] right[x] ← left[y] if left[y]  cid:19 = nil[T ] then p[left[y]] ← x p[y] ← p[x] if p[x] = nil[T ] then root[T ] ← y else if x = left[p[x]]  then left[p[x]] ← y else right[p[x]] ← y   cid:1  Put x on y cid:146 s left.  left[y] ← x p[x] ← y   cid:1  Set y.  cid:1  Turn y cid:146 s left subtree into x cid:146 s right subtree.   cid:1  Link x cid:146 s parent to y.  [Inthe cid:222 rsttwoprintings ofthesecondedition, thisprocedure contains abugthat iscorrected above andinthethirdandsubsequent printings . Thebugisthatthe assignmentinline4 p[left[y]] ← x shouldbeperformedonlywhen y cid:146 sleftchild is not the sentinel  which is tested in line 3 . The  cid:222 rst two printings omitted this test.] The pseudocode for LEFT-ROTATE assumes that      right[x]  cid:19 = nil[T ], and root cid:146 s parent is nil[T ].  Pseudocode for RIGHT-ROTATE is symmetric: exchange left and right everywhere.  Example: [Use to demonstrate that rotation maintains inorder ordering of keys. Nodecolorsomitted.]   Lecture Notes for Chapter 13: Red-Black Trees  13-5  LEFT-ROTATE T, x   11  x  9  18  y  14  19  17  22  7  7  4  4  x  11  19  9  14  22  18  y  17    Before rotation: keys of x cid:146 s left subtree ≤ 11 ≤ keys of y cid:146 s left subtree ≤ 18 ≤  keys of y cid:146 s right subtree.    Rotation makes y cid:146 s left subtree into x cid:146 s right subtree.   After rotation: keys of x cid:146 s left subtree ≤ 11 ≤ keys of x cid:146 s right subtree ≤ 18 ≤  keys of y cid:146 s right subtree.  Time: O 1  for both LEFT-ROTATE and RIGHT-ROTATE, since a constant number of pointers are modi cid:222 ed.  Notes:   Rotation is a very basic operation, also used in AVL trees and splay trees.   Some books talk of rotating on an edge rather than on a node.  Insertion  Start by doing regular binary-search-tree insertion:   13-6  Lecture Notes for Chapter 13: Red-Black Trees  RB-INSERT T, z  y ← nil[T ] x ← root[T ] while x  cid:19 = nil[T ] do y ← x  if key[z] < key[x] then x ← left[x] else x ← right[x]  p[z] ← y if y = nil[T ] then root[T ] ← z else if key[z] < key[y] then left[y] ← z else right[y] ← z  left[z] ← nil[T ] right[z] ← nil[T ] color[z] ← RED RB-INSERT-FIXUP  T, z     RB-INSERT ends by coloring the new node z red.   Then it calls RB-INSERT-FIXUP because we could have violated a red-black  property.  Which property might be violated?  1. OK. 2. If z is the root, then there cid:146 s a violation. Otherwise, OK. 3. OK. 4. If p[z] is red, there cid:146 s a violation: both z and p[z] are red. 5. OK.  Remove the violation by calling RB-INSERT-FIXUP:   Lecture Notes for Chapter 13: Red-Black Trees  13-7  RB-INSERT-FIXUP T, z  while color[ p[z]] = RED  do if p[z] = left[p[p[z]]]  then y ← right[p[p[z]]] if color[y] = RED then color[p[z]] ← BLACK color[y] ← BLACK color[p[p[z]]] ← RED z ← p[p[z]] else if z = right[p[z]] then z ← p[z] color[p[z]] ← BLACK color[p[p[z]]] ← RED RIGHT-ROTATE T, p[p[z]]   LEFT-ROTATE T, z   else  same as then clause  color[root[T ]] ← BLACK  with  cid:147 right cid:148  and  cid:147 left cid:148  exchanged   Loop invariant: At the start of each iteration of the while loop,  a. z is red. b. There is at most one red-black violation:    Property 2: z is a red root, or   Property 4: z and p[z] are both red.   cid:1  Case 1  cid:1  Case 1  cid:1  Case 1  cid:1  Case 1   cid:1  Case 2  cid:1  Case 2  cid:1  Case 3  cid:1  Case 3  cid:1  Case 3  [Thebookhasathirdpartoftheloopinvariant,butweomititforlecture.]  Initialization: We cid:146 ve already seen why the loop invariant holds initially. Termination: The loop terminates because p[z] is black. Hence, property 4 is  OK. Only property 2 might be violated, and the last line  cid:222 xes it.  Maintenance: We drop out when z is the root  since then p[z] is the sentinel nil[T ], which is black . When we start the loop body, the only violation is of property 4. There are 6 cases, 3 of which are symmetric to the other 3. The cases are not mutually exclusive. We cid:146 ll consider cases in which p[z] is a left child. Let y be z cid:146 s uncle  p[z] cid:146 s sibling .   13-8  Lecture Notes for Chapter 13: Red-Black Trees  Case 1: y is red  A  B  C  C  D  y  δ  ε  D  y  δ  ε  α  B  z  α  B  δ  ε  β  γ  If z is a right child  β  γ  new z  C  new z  C  A  B  D  D  z  A  γ  A  γ  δ  ε  α  β  If z is a left child  α  β  and there are no other violations of property 4.    p[p[z]]  z cid:146 s grandparent  must be black, since z and p[z] are both red   Make p[z] and y black ⇒ now z and p[z] are not both red. But prop-   Make p[p[z]] red ⇒ restores property 5.   The next iteration has p[p[z]] as the new z  i.e., z moves up 2 levels .  erty 5 might now be violated.  Case 2: y is black, z is a right child  C  C  B  A  δ  y  δ  y  z A  C  α  B  z  z  A  γ  α  β  γ  δ  B  β  Case 3  β  γ  Case 2  α    Left rotate around p[z] ⇒ now z is a left child, and both z and p[z] are  red.    Takes us immediately to case 3.  Case 3: y is black, z is a left child    Make p[z] black and p[p[z]] red.   Then right rotate on p[p[z]].   No longer have 2 reds in a row.   p[z] is now black ⇒ no more iterations.  Analysis  O lg n  time to get through RB-INSERT up to the call of RB-INSERT-FIXUP. Within RB-INSERT-FIXUP:   Lecture Notes for Chapter 13: Red-Black Trees  13-9    Each iteration takes O 1  time.   Each iteration is either the last one or it moves z up 2 levels.   O lg n  levels ⇒ O lg n  time.   Also note that there are at most 2 rotations overall. Thus, insertion into a red-black tree takes O lg n  time.  Deletion  Start by doing regular binary-search-tree deletion:  RB-DELETE T, z  if left[z] = nil[T ] or right[z] = nil[T ] then y ← z else y ← TREE-SUCCESSOR z  if left[y]  cid:19 = nil[T ] then x ← left[y] else x ← right[y] p[x] ← p[y] if p[y] = nil[T ] then root[T ] ← x else if y = left[p[y]]  then left[p[y]] ← x else right[p[y]] ← x  if y  cid:19 = z then key[z] ← key[y] if color[y] = BLACK then RB-DELETE-FIXUP  T, x   copy y cid:146 s satellite data into z  return y      y is the node that was actually spliced out. x is either      y cid:146 s sole non-sentinel child before y was spliced out, or the sentinel, if y had no children.  In both cases, p[x] is now the node that was previously y cid:146 s parent.  If y is black, we could have violations of red-black properties:  1. OK. 2. If y is the root and x is red, then the root has become red. 3. OK. 4. Violation if p[y] and x are both red. 5. Any path containing y now has 1 fewer black node.    Correct by giving x an  cid:147 extra black. cid:148    13-10  Lecture Notes for Chapter 13: Red-Black Trees    Add 1 to count of black nodes on paths containing x.   Now property 5 is OK, but property 1 is not.   x is either doubly black  if color[x] = BLACK  or red & black  if color[x] =   The attribute color[x] is still either RED or BLACK. No new values for color  RED .     attribute. In other words, the extra blackness on a node is by virtue of x pointing to the node.  Remove the violations by calling RB-DELETE-FIXUP:  RB-DELETE-FIXUP  T, x  while x  cid:19 = root[T ] and color[x] = BLACK  do if x = left[ p[x]]  then w ← right[p[x]] if color[w] = RED then color[w] ← BLACK color[p[x]] ← RED LEFT-ROTATE  T, p[x]  w ← right[p[x]] if color[left[w]] = BLACK and color[right[w]] = BLACK then color[w] ← RED else if color[right[w]] = BLACK  x ← p[x] then color[left[w]] ← BLACK  color[w] ← RED RIGHT-ROTATE T, w  w ← right[p[x]] color[w] ← color[p[x]] color[p[x]] ← BLACK color[right[w]] ← BLACK LEFT-ROTATE  T, p[x]  x ← root[T ]   cid:1  Case 1  cid:1  Case 1  cid:1  Case 1  cid:1  Case 1   cid:1  Case 2  cid:1  Case 2   cid:1  Case 3  cid:1  Case 3  cid:1  Case 3  cid:1  Case 3  cid:1  Case 4  cid:1  Case 4  cid:1  Case 4  cid:1  Case 4  cid:1  Case 4  else  same as then clause with  cid:147 right cid:148  and  cid:147 left cid:148  exchanged   color[x] ← BLACK  Idea: Move the extra black up the tree until x points to a red & black node ⇒ turn it into a black node,   x points to the root ⇒ just remove the extra black, or     we can do certain rotations and recolorings and  cid:222 nish. Within the while loop:     w is x cid:146 s sibling.   w cannot be nil[T ], since that would violate property 5 at p[x]. There are 8 cases, 4 of which are symmetric to the other 4. As with insertion, the cases are not mutually exclusive. We cid:146 ll look at cases in which x is a left child.  x always points to a nonroot doubly black node.   Lecture Notes for Chapter 13: Red-Black Trees  13-11  Case 1: w is red  B  Case 1  D  x  A  D  w  B  E  α  β  C  E  x  A  new w  C  ε  ζ  γ  γ  ε  δ  β  α  ζ   w must have black children.   Make w black and p[x] red.   Then left rotate on p[x].   New sibling of x was a child of w before rotation ⇒ must be black.   Go immediately to case 2, 3, or 4.  δ  Case 2: w is black and both of w cid:146 s children are black  B  c  Case 2  x  A  D  w  new x  B  c  A  D  α  β  C  E  α  β  C  E  γ  δ  ε  ζ  γ  δ  ε  ζ  [Nodewithgrayoutlineisofunknowncolor,denotedbyc.]   Take 1 black off x  ⇒ singly black  and off w  ⇒ red .   Move that black to p[x].   Do the next iteration with p[x] as the new x.    If entered this case from case 1, then p[x] was red ⇒ new x is red & black ⇒ color attribute of new x is RED ⇒ loop terminates. Then new x is made black in the last line.  Case 3: w is black, w cid:146 s left child is red, and w cid:146 s right child is black  B  c  Case 3  B  c  x  A  D  w  x  A  C  new w  α  β  C  E  α  β  γ  D  γ  δ  ε  ζ  δ  E  ε  ζ    Make w red and w cid:146 s left child black.   Then right rotate on w.   New sibling w of x is black with a red right child ⇒ case 4.   13-12  Lecture Notes for Chapter 13: Red-Black Trees  Case 4: w is black, w cid:146 s left child is black, and w cid:146 s right child is red  B  c  Case 4  D  c  c′  δ  x  A  D  w  B  E  α  β  C  E  A  C  ε  ζ  c′  δ  γ  ε  ζ  α  β  γ  new x = root[T]   cid:4  [Nowtherearetwonodesofunknowncolors,denotedby c and c   Make w be p[x] cid:146 s color  c .   Make p[x] black and w cid:146 s right child black.   Then left rotate on p[x].   Remove the extra black on x  ⇒ x is now singly black  without violating  .]  any red-black properties.    All done. Setting x to root causes the loop to terminate.  Analysis  O lg n  time to get through RB-DELETE up to the call of RB-DELETE-FIXUP. Within RB-DELETE-FIXUP:   Case 2 is the only case in which more iterations occur.    x moves up 1 level.   Hence, O lg n  iterations.    Each of cases 1, 3, and 4 has 1 rotation ⇒ ≤ 3 rotations in all.   Hence, O lg n  time. [InChapter14,we cid:146 llseeatheoremthatreliesonred-blacktreeoperationscausing at most a constant number of rotations. This is where red-black trees enjoy an advantage overAVLtrees: intheworstcase,anoperation onan n-nodeAVLtree causes  cid:3  lg n  rotations.]   Solutions for Chapter 13: Red-Black Trees  Solution to Exercise 13.1-3  Solution to Exercise 13.1-4  Solution to Exercise 13.1-5  If we color the root of a relaxed red-black tree black but make no other changes, the resulting tree is a red-black tree. Not even any black-heights change.  After absorbing each red node into its black parent, the degree of each node black node is        2, if both children were already black, 3, if one child was black and one was red, or 4, if both children were red.  All leaves of the resulting tree have the same depth.  In the longest path, at least every other node is black. In the shortest path, at most every node is black. Since the two paths contain equal numbers of black nodes, the length of the longest path is at most twice the length of the shortest path. We can say this more precisely, as follows: Since every path contains bh x  black nodes, even the shortest path from x to a descendant leaf has length at least bh x . By de cid:222 nition, the longest path from x to a descendant leaf has length height x . Since the longest path has bh x  black nodes and at least half the nodes on the longest path are black  by property 4 , bh x  ≥ height x  2, so length of longest path = height x  ≤ 2 · bh x  ≤ twice length of shortest path .   13-14  Solutions for Chapter 13: Red-Black Trees  Solution to Exercise 13.2-4  Since the exercise asks about binary search trees rather than the more speci cid:222 c red- black trees, we assume here that leaves are full- cid:223 edged nodes, and we ignore the sentinels. Taking the book cid:146 s hint, we start by showing that with at most n − 1 right rotations, we can convert any binary search tree into one that is just a right-going chain. The idea is simple. Let us de cid:222 ne the right spine as the root and all descendants of the root that are reachable by following only right pointers from the root. A binary search tree that is just a right-going chain has all n nodes in the right spine. As long as the tree is not just a right spine, repeatedly  cid:222 nd some node y on the right spine that has a non-leaf left child x and then perform a right rotation on y:  RIGHT-ROTATE T, y   x  γ  α  y  x  β  γ  y  β  α   In the above  cid:222 gure, note that any of α, β, and γ can be an empty subtree.  Observe that this right rotation adds x to the right spine, and no other nodes leave the right spine. Thus, this right rotation increases the number of nodes in the right spine by 1. Any binary search tree starts out with at least one node cid:151 the root cid:151 in the right spine. Moreover, if there are any nodes not on the right spine, then at least one such node has a parent on the right spine. Thus, at most n − 1 right rotations are needed to put all nodes in the right spine, so that the tree consists of a single right-going chain. If we knew the sequence of right rotations that transforms an arbitrary binary search , then we could perform this sequence in tree T to a single right-going chain T  cid:4  reverse cid:151 turning each right rotation into its inverse left rotation cid:151 to transform T back into T . Therefore, here is how we can transform any binary search tree T1 into any other binary search tree T2. Let T be the unique right-going chain consisting of the nodes of T1  which is the same as the nodes of T2 . Let r =  cid:2 r1, r2, . . . , rk cid:3  be a k cid:4  cid:3   cid:4  =  cid:2 r  cid:4   cid:4  , . . . , r sequence of right rotations that transforms T1 to T 1 be a sequence of right rotations that transforms T2 to T . We know that there exist  cid:4  ≤ n − 1. For each right rotation r  cid:4   cid:4  i be the i , let l sequences r and r corresponding inverse left rotation. Then the sequence  cid:2 r1, r2, . . . , rk , l  cid:4   cid:4 −1  cid:4  , l , k . . . , l   cid:3  transforms T1 to T2 in at most 2n − 2 rotations.  , and let r  with k, k  , r  , l   cid:4  2   cid:4  k   cid:4    cid:4    cid:4    cid:4    cid:4    cid:4  2   cid:4  1  Solution to Exercise 13.3-3  In Figure 13.5, nodes A, B, and D have black-height k+1 in all cases, because each of their subtrees has black-height k and a black root. Node C has black-height k+1   Solutions for Chapter 13: Red-Black Trees  13-15  on the left  because its red children have black-height k+ 1  and black-height k+ 2 on the right  because its black children have black-height k + 1 .   a   k+1  A  y  D  k+1  k+1  A  D  k+1  α  z  B  k+1  δ  ε  α  B  k+1  δ  ε  k+1  C  β  γ  k+1  C  k+2  C  β  γ  k+2  C   b   k+1  B  y  D  k+1  k+1  B  k+1  A  z  γ  δ  ε  k+1  A  γ  D  k+1  δ  ε  α  β  α  β  In Figure 13.6, nodes A, B, and C have black-height k + 1 in all cases. At left and in the middle, each of A cid:146 s and B cid:146 s subtrees has black-height k and a black root, while C has one such subtree and a red child with black-height k + 1. At the right, each of A cid:146 s and C cid:146 s subtrees has black-height k and a black root, while B cid:146 s red children each have black-height k + 1.  k+1  C  k+1  C  B  k+1  k+1  A  δ  y  k+1  B  δ  y  k+1  A  C  k+1  α  z  B  k+1  k+1  A  z  γ  α  β  γ  δ  β  γ  Case 2  α  β  Case 3  Property 5 is preserved by the transformations. We have shown above that the black-height is well-de cid:222 ned within the subtrees pictured, so property 5 is preserved within those subtrees. Property 5 is preserved for the tree containing the subtrees pictured, because every path through these subtrees to a leaf contributes k+2 black nodes.  Solution to Exercise 13.3-4  Colors are set to red only in cases 1 and 3, and in both situations, it is p[p[z]] that is reddened. If p[p[z]] is the sentinel, then p[z] is the root. By part  b  of the loop invariant and line 1 of RB-INSERT-FIXUP, if p[z] is the root, then we have dropped out of the loop. The only subtlety is in case 2, where we set z ← p[z] before coloring p[p[z]] red. Because we rotate before the recoloring, the identity of p[p[z]] is the same before and after case 2, so there cid:146 s no problem.   13-16  Solutions for Chapter 13: Red-Black Trees  Solution to Exercise 13.4-6  Case 1 occurs only if x cid:146 s sibling w is red. If p[x] were red, then there would be two reds in a row, namely p[x]  which is also p[w]  and w, and we would have had these two reds in a row even before calling RB-DELETE.  Solution to Exercise 13.4-7  No, the red-black tree will not necessarily be the same. Here are two examples: one in which the tree cid:146 s shape changes, and one in which the shape remains the same but the node colors change.  3  3  2  2  insert 1  delete 1  2  3  1  2  3  4  2  3  3  4  2  insert 1  delete 1  4  1  Solution to Problem 13-1  a. When inserting key k, all nodes on the path from the root to the added node  a new leaf  must change, since the need for a new child pointer propagates up from the new node to all of its ancestors. When deleting a node, let y be the node actually removed and z be the node given to the delete procedure.    If y has at most one child, it will be removed or spliced out  see Figure 12.4, parts  a  and  b , where y and z are the same node . All ancestors of y will be changed.  As with insertion, the need for a new child pointer propagates up from the removed node.  If z has two children, y is its successor; it is y that will be spliced out and moved to z cid:146 s position  see Figure 12.4 c  . Therefore all ancestors of both z and y must be changed.  Actually, this is just all ancestors of z, since z is an ancestor of y in this case.      In either case, y cid:146 s children  if any  are unchanged, because we have assumed that there is no parent  cid:222 eld.   Solutions for Chapter 13: Red-Black Trees  13-17  b. We assume that we can call two procedures:    MAKE-NEW-NODE k  creates a new node whose key  cid:222 eld has value k and  with left and right  cid:222 elds NIL, and it returns a pointer to the new node.    COPY-NODE x  creates a new node whose key, left, and right  cid:222 elds have the  same values as those of node x, and it returns a pointer to the new node.  Here are two ways to write PERSISTENT-TREE-INSERT. The  cid:222 rst is a version of TREE-INSERT, modi cid:222 ed to create new nodes along the path to where the new node will go, and to not use parent  cid:222 elds. It returns the root of the new tree.  PERSISTENT-TREE-INSERT  T, k  z ← MAKE-NEW-NODE k  new-root ← COPY-NODE root[T ]  y ← NIL x ← new-root while x  cid:19 = NIL do y ← x  if key[z] < key[x] then x ← COPY-NODE left[x]  left[y] ← x else x ← COPY-NODE right[x]  right[y] ← x  if y = NIL then new-root ← z else if key[z] < key[y] then left[y] ← z else right[y] ← z  return new-root  The second is a rather elegant recursive procedure. It must be called with root[T ] instead of T as its  cid:222 rst argument  because the recursive calls pass a node for this argument , and it returns the root of the new tree.  PERSISTENT-TREE-INSERT  r, k  if r = NIL then x ← MAKE-NEW-NODE k  else x ← COPY-NODE r    if k < key[r] then left[x] ← PERSISTENT-TREE-INSERT left[r], k  else right[x] ← PERSISTENT-TREE-INSERT  right[r], k   return x  c. Like TREE-INSERT, PERSISTENT-TREE-INSERT does a constant amount of work at each node along the path from the root to the new node. Since the length of the path is at most h, it takes O h  time. Since it allocates a new node  a constant amount of space  for each ancestor of the inserted node, it also needs O h  space.   13-18  Solutions for Chapter 13: Red-Black Trees  d. If there were parent  cid:222 elds, then because of the new root, every node of the tree would have to be copied when a new node is inserted. To see why, observe that the children of the root would change to point to the new root, then their children would change to point to them, and so on. Since there are n nodes, this change would cause insertion to create  cid:3  n  new nodes and to take  cid:3  n  time.  e. From parts  a  and  c , we know that insertion into a persistent binary search tree of height h, like insertion into an ordinary binary search tree, takes worst- case time O h . A red-black tree has h = O lg n , so insertion into an ordinary red-black tree takes O lg n  time. We need to show that if the red-black tree is persistent, insertion can still be done in O lg n  time. To do this, we will need to show two things:   How to still  cid:222 nd the parent pointers we need in O 1  time without using a parent  cid:222 eld. We cannot use a parent  cid:222 eld because a persistent tree with parent  cid:222 elds uses  cid:3  n  time for insertion  by part  d  .    That the additional node changes made during red-black tree operations  by rotation and recoloring  don cid:146 t cause more than O lg n  additional nodes to change.  Each parent pointer needed during insertion can be found in O 1  time without having a parent  cid:222 eld as follows: To insert into a red-black tree, we call RB-INSERT, which in turn calls RB- INSERT-FIXUP. Make the same changes to RB-INSERT as we made to TREE- INSERT for persistence. Additionally, as RB-INSERT walks down the tree to  cid:222 nd the place to insert the new node, have it build a stack of the nodes it tra- verses and pass this stack to RB-INSERT-FIXUP. RB-INSERT-FIXUP needs parent pointers to walk back up the same path, and at any given time it needs parent pointers only to  cid:222 nd the parent and grandparent of the node it is working on. As RB-INSERT-FIXUP moves up the stack of parents, it needs only parent pointers that are at known locations a constant distance away in the stack. Thus, the parent information can be found in O 1  time, just as if it were stored in a parent  cid:222 eld. Rotation and recoloring change nodes as follows:   RB-INSERT-FIXUP performs at most 2 rotations, and each rotation changes the child pointers in 3 nodes  the node around which we rotate, that node cid:146 s parent, and one of the children of the node around which we rotate . Thus, at most 6 nodes are directly modi cid:222 ed by rotation during RB-INSERT-FIXUP. In a persistent tree, all ancestors of a changed node are copied, so RB-INSERT- FIXUP cid:146 s rotations take O lg n  time to change nodes due to rotation.  Ac- tually, the changed nodes in this case share a single O lg n -length path of ancestors.     RB-INSERT-FIXUP recolors some of the inserted node cid:146 s ancestors, which are being changed anyway in persistent insertion, and some children of an- cestors  the  cid:147 uncles cid:148  referred to in the algorithm description . There are at most O lg n  ancestors, hence at most O lg n  color changes of uncles. Recoloring uncles doesn cid:146 t cause any additional node changes due to persis- tence, because the ancestors of the uncles are the same nodes  ancestors of   Solutions for Chapter 13: Red-Black Trees  13-19  the inserted node  that are being changed anyway due to persistence. Thus, recoloring does not affect the O lg n  running time, even with persistence.  We could show similarly that deletion in a persistent tree also takes worst-case time O h .   We already saw in part  a  that O h  nodes change.   We could write a persistent RB-DELETE procedure that runs in O h  time, analogous to the changes we made for persistence in insertion. But to do so without using parent pointers we need to walk down the tree to the node to be deleted, to build up a stack of parents as discussed above for insertion. This is a little tricky if the set cid:146 s keys are not distinct, because in order to  cid:222 nd the path to the node to delete cid:151 a particular node with a given key cid:151 we have to make some changes to how we store things in the tree, so that duplicate keys can be distinguished. The easiest way is to have each key take a second part that is unique, and to use this second part as a tiebreaker when comparing keys.  Then the problem of showing that deletion needs only O lg n  time in a persis- tent red-black tree is the same as for insertion.   As for insertion, we can show that the parents needed by RB-DELETE- FIXUP can be found in O 1  time  using the same technique as for insertion .   Also, RB-DELETE-FIXUP performs at most 3 rotations, which as discussed above for insertion requires O lg n  time to change nodes due to persistence. It also does O lg n  color changes, which  as for insertion  take only O lg n  time to change ancestors due to persistence, because the number of copied nodes is O lg n .    Lecture Notes for Chapter 14: Augmenting Data Structures  Chapter 14 overview  Dynamic order statistics  We cid:146 ll be looking at methods for designing algorithms. In some cases, the design will be intermixed with analysis. In other cases, the analysis is easy, and it cid:146 s the design that cid:146 s harder.  Augmenting data structures       It cid:146 s unusual to have to design an all-new data structure from scratch. It cid:146 s more common to take a data structure that you know and store additional information in it.    With the new information, the data structure can support new operations.   But. . . you have to  cid:222 gure out how to correctly maintain the new information  without loss of ef cid:222 ciency.  We cid:146 ll look at a couple of situations in which we augment red-black trees.  We want to support the usual dynamic-set operations from R-B trees, plus:   OS-SELECT x, i  : return pointer to node containing the ith smallest key of the    OS-RANK T, x : return the rank of x in the linear order determined by an  subtree rooted at x.  inorder walk of T .  Augment by storing in each node x: size[x] =  of nodes in subtree rooted at x .  Includes x itself.      Does not include leaves  sentinels . De cid:222 ne for sentinel size[nil[T ]] = 0. Then size[x] = size[left[x]] + size[right[x]] + 1.   14-2  Lecture Notes for Chapter 14: Augmenting Data Structures  i=5 r=2  R  M C  8 5  M A  8 1  B  i=5 r=6  B  M M  8 8  B  i=3 r=2  M F  8 3  R  M D  8 1  R  i=1 r=1  M H  8 1  B  M P  8 2  R  M Q  8 1  [Example above: Ignore colors,butlegalcoloring shownwith cid:147 R cid:148 and cid:147 B cid:148 nota- tions. Valuesofi andr arefortheexamplebelow.] Note: OK for keys to not be distinct. Rank is de cid:222 ned with respect to position in inorder walk. So if we changed D to C, rank of original C is 2, rank of D changed to C is 3.  OS-SELECT x, i   r ← size[left[x]]+1 if i = r then return x  elseif i < r then return OS-SELECT left[x], i   else return OS-SELECT right[x], i − r   Initial call: OS-SELECT root[T ], i   Try OS-SELECT root[T ], 5 . [Valuesshownin cid:222 gureabove. Returnsnodewhose keyisH.] Correctness: r = rank of x within subtree rooted at x.  If i = r, then we want x. If i < r, then ith smallest element is in x cid:146 s left subtree, and we want the ith smallest element in the subtree. If i > r, then ith smallest element is in x cid:146 s right subtree, but subtract off the r elements in x cid:146 s subtree that precede those in x cid:146 s right subtree.            Like the randomized SELECT algorithm!  Analysis: Each recursive call goes down one level. Since R-B tree has O lg n  levels, have O lg n  calls ⇒ O lg n  time. OS-RANK T, x  r ← size[left[x]] + 1 y ← x while y  cid:19 = root[T ]  do if y = right[p[y]]  then r ← r + size[left[p[y]]] + 1 y ← p[y]  return r   Lecture Notes for Chapter 14: Augmenting Data Structures  14-3  Demo: Node D. Why does this work?  Loop invariant: At start of each iteration of while loop, r = rank of key[x] in subtree rooted at y.  Initialization: Initially, r = rank of key[x] in subtree rooted at x, and y = x. Termination: Loop terminates when y = root[T ] ⇒ subtree rooted at y is entire tree. Therefore, r = rank of key[x] in entire tree. Maintenance: At end of each iteration, set y ← p[y]. So, show that if r = rank of key[x] in subtree rooted at y at start of loop body, then r = rank of key[x] in subtree rooted at p[y] at end of loop body.  y  x  [r = ofnodesinsubtreerootedat y preceding x ininorderwalk] Must add nodes in y cid:146 s sibling cid:146 s subtree.    If y is a left child, its sibling cid:146 s subtree follows all nodes in y cid:146 s subtree ⇒ don cid:146 t change r. If y is a right child, all nodes in y cid:146 s sibling cid:146 s subtree precede all nodes in y cid:146 s subtree ⇒ add size of y cid:146 s sibling cid:146 s subtree, plus 1 for p[y], into r.     p[y]  left[p[y]]  y  Analysis: y goes up one level in each iteration ⇒ O lg n  time.  Maintaining subtree sizes    Need to maintain size[x]  cid:222 elds during insert and delete operations.   Need to maintain them ef cid:222 ciently. Otherwise, might have to recompute them  all, at a cost of  cid:3  n .  Will see how to maintain without increasing O lg n  time for insert and delete.  Insert:   During pass downward, we know that the new node will be a descendant of each node we visit, and only of these nodes. Therefore, increment size  cid:222 eld of each node visited.   14-4  Lecture Notes for Chapter 14: Augmenting Data Structures    Then there cid:146 s the  cid:222 xup pass:    Goes up the tree.   Changes colors O lg n  times.   Performs ≤ 2 rotations.    Color changes don cid:146 t affect subtree sizes.   Rotations do!   But we can determine new sizes based on old sizes and sizes of children.  M C  8 5  x  LEFT-ROTATE T, x   y  M F  8 5  M A  8 1  M F  8 3  y  M D  8 1  M H  8 1  x  M C  8 3  M A  8 1  M D  8 1  M H  8 1  size[y] ← size[x] size[x] ← size[left[x]] + size[right[x]] + 1    Similar for right rotation.   Therefore, can update in O 1  time per rotation ⇒ O 1  time spent updating  size  cid:222 elds during  cid:222 xup.    Therefore, O lg n  to insert.  Delete: Also 2 phases:  1. Splice out some node y. 2. Fixup. After splicing out y, traverse a path y → root, decrementing size in each node on path. O lg n  time. During  cid:222 xup, like insertion, only color changes and rotations.   ≤ 3 rotations ⇒ O 1  time spent updating size  cid:222 elds during  cid:222 xup.   Therefore, O lg n  to delete. Done!  Methodology for augmenting a data structure  1. Choose an underlying data structure. 2. Determine additional information to maintain.   Lecture Notes for Chapter 14: Augmenting Data Structures  14-5  3. Verify that we can maintain additional information for existing data structure  operations.  4. Develop new operations.  Don cid:146 t need to do these steps in strict order! Usually do a little of each, in parallel. How did we do them for OS trees?  1. R-B tree. 2. size[x]. 3. Showed how to maintain size during insert and delete. 4. Developed OS-SELECT and OS-RANK.  Red-black trees are particularly amenable to augmentation.  Theorem Augment a R-B tree with  cid:222 eld f , where f [x] depends only on information in x, left[x], and right[x]  including f [left[x]] and f [right[x]] . Then can maintain values of f in all nodes during insert and delete without affecting O lg n  perfor- mance.  Proof Since f [x] depends only on x and its children, when we alter information in x, changes propagate only upward  to p[x], p[p[x]], . . . , root . Height = O lg n  ⇒ O lg n  updates, at O 1  each.  Insertion: Insert a node as child of existing node. Even if can cid:146 t update f on way down, can go up from inserted node to update f . During  cid:222 xup, only changes come from color changes  no effect on f   and rotations. Each rotation affects f of ≤ 3 nodes  x,y, and parent , and can recompute each in O 1  time. Then, if necessary, propagate changes up the tree. Therefore, O lg n  time per rotation. Since ≤ 2 rotations, O lg n  time to update f during  cid:222 xup.  Delete: Same idea. After splicing out a node, go up from there to update f . Fixup has ≤ 3 rotations. O lg n  per rotation ⇒ O lg n  to update f during  cid:222 xup.   theorem   For some attributes, can get away with O 1  per rotation. Example: size  cid:222 eld.  Interval trees  Maintain a set of intervals. For instance, time intervals.  low[i] = 7  high[i] = 10  i=[7,10]  7  10  11  5  4  17  19  8  15  18  21  23  [leaveonboard]   14-6  Lecture Notes for Chapter 14: Augmenting Data Structures  Operations         INTERVAL-INSERT  T, x : int[x] already  cid:222 lled in. INTERVAL-DELETE T, x  INTERVAL-SEARCH T, i  : return pointer to a node x in T such that int[x] over- laps interval i. Any overlapping node in T is OK. Return pointer to sentinel nil[T ] if no overlapping node in T .  Interval i has low[i], high[i]. i and j overlap if and only if low[i] ≤ high[ j] and low[ j] ≤ high[i].  Go through examples of proper inclusion, overlap without proper inclusion, no overlap.  Another way: i and j don cid:146 t overlap if and only if: low[i] > high[ j] or low[ j] > high[i]. [leavethisonboard] Recall the 4-part methodology.  For interval trees  1. Use R-B trees.    Each node x contains interval int[x].   Key is low endpoint  low[int[x]] .    Inorder walk would list intervals sorted by low endpoint.  2. Each node x contains  max[x] = max endpoint value in subtree rooted at x .  int  max  M [21,23] 23 8  M [17,19]  23 8  M [15,18] 18 8  M [4,8] 8  8  M [5,11] 18 8  M [7,10] 10 8  [leaveonboard] max[x] = max  ⎧⎨⎩high[int[x]] ,  max[left[x]] , max[right[x]]  Could max[left[x]] > max[right[x]]? Sure. Position in tree is determined only by low endpoints, not high endpoints.  3. Maintaining the information.   Lecture Notes for Chapter 14: Augmenting Data Structures  14-7    This is easy cid:151 max[x] depends only on:         information in x: high[int[x]] information in left[x]: max[left[x]] information in right[x]: max[right[x]]    Apply the theorem.    In fact, can update max on way down during insertion, and in O 1  time per rotation.  4. Developing new operations.  INTERVAL-SEARCH T, i   x ← root[T ] while x  cid:19 = nil[T ] and i does not overlap int[x]  do if left[x]  cid:19 = nil[T ] and max[left[x]] ≥ low[i]  then x ← left[x] else x ← right[x]  return x  Examples: Search for [14, 16] and [12, 14].  Time: O lg n .  Correctness: Key idea: need check only 1 of node cid:146 s 2 children.  Theorem If search goes right, then either:   There is an overlap in right subtree, or   There is no overlap in either subtree. If search goes left, then either:   There is an overlap in left subtree, or   There is no overlap in either subtree.  Proof If search goes right:      If there is an overlap in right subtree, done. If there is no overlap in right, show there is no overlap in left. Went right because    left[x] = nil[T ] ⇒ no overlap in left. OR    max[left[x]] < low[i] ⇒ no overlap in left.  i  max[left[x]] = highest endpoint in left   14-8  Lecture Notes for Chapter 14: Augmenting Data Structures  If search goes left:      If there is an overlap in left subtree, done. If there is no overlap in left, show there is no overlap in right.   Went left because:  low[i] ≤ max[left[x]]  = high[ j] for some j in left subtree .    Since there is no overlap in left, i and j don cid:146 t overlap.   Refer back to: no overlap if  low[i] > high[ j] or low[ j] > high[i] .  low[ j]   cid:23   cid:24  cid:25   cid:26  ≤ low[k] in right    Since low[i] ≤ high[ j], must have low[ j] > high[i].   Now consider any interval k in right subtree.  cid:23   cid:24  cid:25   cid:26    Because keys are low endpoint,   Therefore, high[i] < low[ j] ≤ low[k].   Therefore, high[i] < low[k].   Therefore, i and k do not overlap.  in left  .   theorem    Solutions for Chapter 14: Augmenting Data Structures  Solution to Exercise 14.1-5  Given an element x in an n-node order-statistic tree T and a natural number i, the following procedure retrieves the ith successor of x in the linear order of T :  OS-SUCCESSOR T, x, i   r ← OS-RANK T, x  s ← r + i return OS-SELECT root[T ], s   Since OS-RANK and OS-SELECT each take O lg n  time, so does the procedure OS-SUCCESSOR.  Solution to Exercise 14.1-6  When inserting node z, we search down the tree for the proper place for z. For each node x on this path, add 1 to rank[x] if y is inserted within x cid:146 s left subtree, and leave rank[x] unchanged if y is inserted within x cid:146 s right subtree. Similarly when deleting, subtract 1 from rank[x] whenever the spliced-out node y had been in x cid:146 s left subtree. We also need to handle the rotations that occur during the  cid:222 xup procedures for insertion and deletion. Consider a left rotation on node x, where the pre-rotation right child of x is y  so that x becomes y cid:146 s left child after the left rotation . We leave rank[x] unchanged, and letting r = rank[y] before the rotation, we set rank[y] ← r + rank[x]. Right rotations are handled in an analogous manner.  Solution to Exercise 14.1-7  Let A[1 . . n] be the array of n distinct numbers. One way to count the inversions is to add up, for each element, the number of larger elements that precede it in the array:   14-10  Solutions for Chapter 14: Augmenting Data Structures   of inversions = n cid:2   Inv  j   ,  j=1  where Inv  j   = {i : i   A[ j]}. Note that Inv  j   is related to A[ j] cid:146 s rank in the subarray A[1 . . j] because the elements in Inv  j   are the reason that A[ j] is not positioned according to its rank. Let r   j   be the rank of A[ j] in A[1 . . j]. Then j = r   j   + Inv  j  , so we can compute Inv  j   = j − r   j   by inserting A[1], . . . , A[n] into an order-statistic tree and using OS-RANK to  cid:222 nd the rank of each A[ j] in the tree immediately after it is inserted into the tree.  This OS-RANK value is r   j  .  Insertion and OS-RANK each take O lg n  time, and so the total time for n ele- ments is O n lg n .  Solution to Exercise 14.2-2  Yes, by Theorem 14.1, because the black-height of a node can be computed from the information at the node and its two children. Actually, the black-height can be computed from just one child cid:146 s information: the black-height of a node is the black-height of a red child, or the black height of a black child plus one. The second child does not need to be checked because of property 5 of red-black trees. Within the RB-INSERT-FIXUP and RB-DELETE-FIXUP procedures are color changes, each of which potentially cause O lg n  black-height changes. Let us show that the color changes of the  cid:222 xup procedures cause only local black-height changes and thus are constant-time operations. Assume that the black-height of each node x is kept in the  cid:222 eld bh[x]. For RB-INSERT-FIXUP, there are 3 cases to examine.  Case 1: z cid:146 s uncle is red.   a   k+1  A  y  D  k+1  k+1  A  D  k+1  α  z  B  k+1  δ  ε  α  B  k+1  δ  ε  k+1  C  β  γ  k+1  C  k+2  C  β  γ  k+2  C   b   k+1  B  y  D  k+1  k+1  B  k+1  A  z  γ  δ  ε  k+1  A  γ  D  k+1  δ  ε  α  β  α  β   Solutions for Chapter 14: Augmenting Data Structures  14-11    Before color changes, suppose that all subtrees α, β, γ , δ,  cid:6  have the same black-height k with a black root, so that nodes A, B, C, and D have black- heights of k + 1.   After color changes, the only node whose black-height changed is node C. To  cid:222 x that, add bh[p[p[z]]] = bh[p[p[z]]] + 1 after line 7 in RB-INSERT- FIXUP.    Since the number of black nodes between p[p[z]] and z remains the same,  nodes above p[p[z]] are not affected by the color change.  Case 2: z cid:146 s uncle y is black, and z is a right child. Case 3: z  cid:146 s uncle y is black, and z is a left child.   cid:4   k+1  C  k+1  C  B  k+1  k+1  A  δ  y  k+1  B  δ  y  k+1  A  C  k+1  α  z  B  k+1  k+1  A  z  γ  α  β  γ  δ  β  γ  Case 2  α  β  Case 3    With subtrees α, β, γ , δ,  cid:6  of black-height k, we see that even with color changes and rotations, the black-heights of nodes A, B, and C remain the same  k + 1 .  Thus, RB-INSERT-FIXUP maintains its original O lg n  time. For RB-DELETE-FIXUP, there are 4 cases to examine.  Case 1: x cid:146 s sibling w is red.  B  Case 1  D  x  A  D  w  B  E  α  β  C  E  x  A  new w  C  ε  ζ  γ  δ  ε  ζ  α  β  γ  δ    Even though case 1 changes colors of nodes and does a rotation, black-  heights are not changed.    Case 1 changes the structure of the tree, but waits for cases 2, 3, and 4 to  deal with the  cid:147 extra black cid:148  on x.  Case 2: x cid:146 s sibling w is black, and both of w cid:146 s children are black.  B  c  Case 2  x  A  D  w  new x  B  c  A  D  α  β  C  E  α  β  C  E  γ  δ  ε  ζ  γ  δ  ε  ζ   14-12  Solutions for Chapter 14: Augmenting Data Structures    w is colored red, and x cid:146 s  cid:147 extra cid:148  black is moved up to p[x].   Now we can add bh[p[x]] = bh[x] after line 10 in RB-DELETE-FIXUP.   This is a constant-time update. Then, keep looping to deal with the extra  black on p[x].  Case 3: x cid:146 s sibling w is black, w cid:146 s left child is red, and w cid:146 s right child is black.  B  c  Case 3  B  c  x  A  D  w  x  A  C  new w  α  β  C  E  α  β  γ  D  γ  δ  ε  ζ  δ  E  ζ   Regardless of the color changes and rotation of this case, the black-heights  ε  don cid:146 t change.    Case 3 just sets up the structure of the tree, so it can fall correctly into case 4.  Case 4: x cid:146 s sibling w is black, and w cid:146 s right child is red.  B  c  Case 4  x  A  D  w  D  c  B  E  α  β  C  c′  E  A  C  c′  ε  ζ  γ  δ  new x = root[T]   Nodes A, C, and E keep the same subtrees, so their black-heights don cid:146 t  α  β  ζ  δ  ε  γ    Add these two constant-time assignments in RB-DELETE-FIXUP after  change.  line 20: bh[p[x]] = bh[x] + 1. bh[p[p[x]]] = bh[p[x]] + 1.    The extra black is taken care of. Loop terminates.  Thus, RB-DELETE-FIXUP maintains its original O lg n  time. Therefore, we conclude that black-heights of nodes can be maintained as  cid:222 elds in red-black trees without affecting the asymptotic performance of red-black tree operations.  Solution to Exercise 14.2-3  No, because the depth of a node depends on the depth of its parent. When the depth of a node changes, the depths of all nodes below it in the tree must be updated. Updating the root node causes n − 1 other nodes to be updated, which would mean that operations on the tree that change node depths might not run in O n lg n  time.   Solutions for Chapter 14: Augmenting Data Structures  14-13  Solution to Exercise 14.3-3  As it travels down the tree, INTERVAL-SEARCH  cid:222 rst checks whether current node x overlaps the query interval i and, if it does not, goes down to either the left or right child. If node x overlaps i, and some node in the right subtree overlaps i, but no node in the left subtree overlaps i, then because the keys are low endpoints, this order of checking   cid:222 rst x, then one child  will return the overlapping interval with the minimum low endpoint. On the other hand, if there is an interval that overlaps i in the left subtree of x, then checking x before the left subtree might cause the procedure to return an interval whose low endpoint is not the minimum of those that overlap i. Therefore, if there is a possibility that the left subtree might contain an interval that overlaps i, we need to check the left subtree  cid:222 rst. If there is no overlap in the left subtree but node x overlaps i, then we return x. We check the right subtree under the same conditions as in INTERVAL-SEARCH: the left subtree cannot contain an interval that overlaps i, and node x does not overlap i, either. Because we might search the left subtree  cid:222 rst, it is easier to write the pseudocode to use a recursive procedure MIN-INTERVAL-SEARCH-FROM T, x, i  , which returns the node overlapping i with the minimum low endpoint in the subtree rooted at x, or nil[T ] if there is no such node.  MIN-INTERVAL-SEARCH T, i   return MIN-INTERVAL-SEARCH-FROM T, root[T ], i    MIN-INTERVAL-SEARCH-FROM T, x, i   if left[x]  cid:19 = nil[T ] and max[left[x]] ≥ low[i] then y ← MIN-INTERVAL-SEARCH-FROM T, left[x], i    if y  cid:19 = nil[T ] then return y  elseif i overlaps int[x]  then return x  else return nil[T ]  elseif i overlaps int[x]  then return x  else return MIN-INTERVAL-SEARCH-FROM T, right[x], i    The call MIN-INTERVAL-SEARCH T, i   takes O lg n  time, since each recursive call of MIN-INTERVAL-SEARCH-FROM goes one node lower in the tree, and the height of the tree is O lg n .  Solution to Exercise 14.3-6  1. Underlying data structure:  A red-black tree in which the numbers in the set are stored simply as the keys of the nodes.   14-14  Solutions for Chapter 14: Augmenting Data Structures  SEARCH is then just the ordinary TREE-SEARCH for binary search trees, which runs in O lg n  time on red-black trees.  2. Additional information:  The red-black tree is augmented by the following  cid:222 elds in each node x:   min-gap[x] contains the minimum gap in the subtree rooted at x. It has the magnitude of the difference of the two closest numbers in the subtree rooted at x. If x is a leaf  its children are all nil[T ] , let min-gap[x] = ∞.    min-val[x] contains the minimum value  key  in the subtree rooted at x.   max-val[x] contains the maximum value  key  in the subtree rooted at x.  3. Maintaining the information:  The three  cid:222 elds added to the tree can each be computed from information in the node and its children. Hence by Theorem 14.1, they can be maintained during insertion and deletion without affecting the O lg n  running time: min-val[x] =  if there cid:146 s a left subtree , otherwise ,  min-val[left[x]] key[x]   cid:9   cid:9   if there cid:146 s a right subtree , otherwise ,  max-val[x] =  min-gap[x] = min  max-val[right[x]] key[x]  ⎧⎪⎪⎨⎪⎪⎩min-gap[left[x]]  min-gap[right[x]] key[x] − max-val[left[x]] min-val[right[x]] − key[x]   ∞ if no left subtree  ,  ∞ if no right subtree  ,  ∞ if no left subtree  ,  ∞ if no right subtree  .  In fact, the reason for de cid:222 ning the min-val and max-val  cid:222 elds is to make it possible to compute min-gap from information at the node and its children.  4. New operation:  MIN-GAP simply returns the min-gap stored at the tree root. Thus, its running time is O 1 . Note that in addition  not asked for in the exercise , it is possible to  cid:222 nd the two closest numbers in O lg n  time. Starting from the root, look for where the minimum gap  the one stored at the root  came from. At each node x, simulate the computation of min-gap[x] to  cid:222 gure out where min-gap[x] came from. If it came from a subtree cid:146 s min-gap  cid:222 eld, continue the search in that subtree. If it came from a computation with x cid:146 s key, then x and that other number are the closest numbers.  Solution to Exercise 14.3-7  General idea: Move a sweep line from left to right, while maintaining the set of rectangles currently intersected by the line in an interval tree. The interval tree will organize all rectangles whose x interval includes the current position of the sweep line, and it will be based on the y intervals of the rectangles, so that any overlapping y intervals in the interval tree correspond to overlapping rectangles.   Solution to Problem 14-1  Solutions for Chapter 14: Augmenting Data Structures  14-15  Details:  1. Sort the rectangles by their x-coordinates.  Actually, each rectangle must ap- pear twice in the sorted list cid:151 once for its left x-coordinate and once for its right x-coordinate.   2. Scan the sorted list  from lowest to highest x-coordinate .    When an x-coordinate of a left edge is found, check whether the rectangle cid:146 s y-coordinate interval overlaps an interval in the tree, and insert the rectangle  keyed on its y-coordinate interval  into the tree.    When an x-coordinate of a right edge is found, delete the rectangle from the  interval tree.  The interval tree always contains the set of  cid:147 open cid:148  rectangles intersected by the sweep line. If an overlap is ever found in the interval tree, there are overlapping rectangles.  Time: O n lg n    O n lg n  to sort the rectangles  we can use merge sort or heap sort .   O n lg n  for interval-tree operations  insert, delete, and check for overlap .  a. Assume for the purpose of contradiction that there is no point of maximum overlap in an endpoint of a segment. The maximum overlap point p is in the interior of m segments. Actually, p is in the interior of the intersection of those  cid:4  of the intersection of the m m segments. Now look at one of the endpoints p  cid:4  has the same overlap as p because it is in the same intersec- segments. Point p  cid:4   cid:4  is also a point of maximum overlap. Moreover, p tion of m segments, and so p is in the endpoint of a segment  otherwise the intersection would not end there , which contradicts our assumption that there is no point of maximum overlap in an endpoint of a segment. Thus, there is always a point of maximum overlap which is an endpoint of one of the segments.  b. Keep a balanced binary tree of the endpoints. That is, to insert an interval, we insert its endpoints separately. With each left endpoint e, associate a value p[e] = +1  increasing the overlap by 1 . With each right endpoint e associate a value p[e] = −1  decreasing the overlap by 1 . When multiple endpoints have the same value, insert all the left endpoints with that value before inserting any of the right endpoints with that value. Here cid:146 s some intuition. Let e1, e2, . . . , en be the sorted sequence of endpoints corresponding to our intervals. Let s i, j   denote the sum p[ei ] + p[ei+1] + ··· + p[e j ] for 1 ≤ i ≤ j ≤ n. We wish to  cid:222 nd an i maximizing s 1, i  . Each node x stores three new attributes. Suppose that the subtree rooted at x includes the endpoints el[x], . . . , er[x]. We store v[x] = s l[x], r[x] , the sum of the values of all nodes in x cid:146 s subtree. We also store m[x], the maximum value   14-16  Solutions for Chapter 14: Augmenting Data Structures  ⎧⎨⎩m[left[x]]  obtained by the expression s l[x], i   for any i in{l[x], l[x] + 1, . . . , r[x]}. Fi- nally, we store o[x] as the value of i for which m[x] achieves its maximum. For the sentinel, we de cid:222 ne v[nil[T ]] = m[nil[T ]] = 0. We can compute these attributes in a bottom-up fashion to satisfy the require- ments of Theorem 14.1: v[x] = v[left[x]] + p[x] + v[right[x]] , m[x] = max   max is in x cid:146 s left subtree  , v[left[x]] + p[x]  max is at x  , v[left[x]] + p[x] + m[right[x]]  max is in x cid:146 s right subtree  . The computation of v[x] is straightforward. The computation of m[x] bears further explanation. Recall that it is the maximum value of the sum of the p values for the nodes in x cid:146 s subtree, starting at l[x], which is the leftmost endpoint in x cid:146 s subtree and ending at any node i in x cid:146 s subtree. The value of i that maximizes this sum is either a node in x cid:146 s left subtree, x itself, or a node in x cid:146 s right subtree. If i is a node in x cid:146 s left subtree, then m[left[x]] represents a sum starting at l[x], and hence m[x] = m[left[x]]. If i is x itself, then m[x] represents the sum of all p values in x cid:146 s left subtree plus p[x], so that m[x] = v[left[x]] + p[x]. Finally, if i is in x cid:146 s right subtree, then m[x] represents the sum of all p values in x cid:146 s left subtree, plus p[x], plus the sum of some set of p values in x cid:146 s right subtree. Moreover, the values taken from x cid:146 s right subtree must start from the leftmost endpoint in the right subtree. To maximize this sum, we need to maximize the sum from the right subtree, and that value is precisely m[right[x]]. Hence, in this case, m[x] = v[left[x]] + p[x] + m[right[x]]. Once we understand how to compute m[x], it is straightforward to compute o[x] from the information in x and its two children. Thus, we can implement the operations as follows:      INTERVAL-INSERT: insert two nodes, one for each endpoint of the interval. INTERVAL-DELETE: delete the two nodes representing the interval end- points.    FIND-POM: return the interval whose endpoint is represented by o[root[T ]]. Because of how we have de cid:222 ned the new attributes, Theorem 14.1 says that each operation runs in O lg n  time. In fact, FIND-POM takes only O 1  time.  Solution to Problem 14-2  a. We use a circular list in which each element has two  cid:222 elds, key and next. At the beginning, we initialize the list to contain the keys 1, 2, . . . , n in that order. This initialization takes O n  time, since there is only a constant amount of work per element  i.e., setting its key and its next  cid:222 elds . We make the list circular by letting the next  cid:222 eld of the last element point to the  cid:222 rst element. We then start scanning the list from the beginning. We output and then delete every mth element, until the list becomes empty. The output sequence is the   Solutions for Chapter 14: Augmenting Data Structures  14-17   n, m -Josephus permutation. This process takes O m  time per element, for a total time of O mn . Since m is a constant, we get O mn  = O n  time, as required.  b. We can use an order-statistic tree, straight out of Section 14.1. Why? Suppose that we are at a particular spot in the permutation, and let cid:146 s say that it cid:146 s the jth largest remaining person. Suppose that there are k ≤ n people remaining. Then we will remove person j, decrement k to re cid:223 ect having removed this person, and then go on to the   j +m−1 th largest remaining person  subtract 1 because we have just removed the jth largest . But that assumes that j + m ≤ k. If not, then we use a little modular arithmetic, as shown below. In detail, we use an order-statistic tree T , and we call the procedures OS- INSERT, OS-DELETE, OS-RANK, and OS-SELECT:  JOSEPHUS n, m  initialize T to be empty for j ← 1 to n  do create a node x with key[x] = j  OS-INSERT T, x   k ← n j ← m while k > 2  do x ← OS-SELECT root[T ], j    print key[x] OS-DELETE T, x  k ← k − 1 j ←    j + m − 2  mod k  + 1  print key[OS-SELECT root[T ], 1 ]  JOSEPHUS n, m  initialize T to be empty for j ← 1 to n  do create a node x with key[x] = j  OS-INSERT T, x   j ← 1 for k ← n downto 1  do j ←    j + m − 2  mod k  + 1 x ← OS-SELECT root[T ], j   print key[x] OS-DELETE T, x   The above procedure is easier to understand. Here cid:146 s a streamlined version:  Either way, it takes O n lg n  time to build up the order-statistic tree T , and then we make O n  calls to the order-statistic-tree procedures, each of which takes O lg n  time. Thus, the total time is O n lg n .    Lecture Notes for Chapter 15: Dynamic Programming  Dynamic Programming    Not a speci cid:222 c algorithm, but a technique  like divide-and-conquer .   Developed back in the day when  cid:147 programming cid:148  meant  cid:147 tabular method cid:148   like  linear programming . Doesn cid:146 t really refer to computer programming.    Used for optimization problems:    Find a solution with the optimal value.   Minimization or maximization.  We cid:146 ll see both.   Four-step method  1. Characterize the structure of an optimal solution. 2. Recursively de cid:222 ne the value of an optimal solution. 3. Compute the value of an optimal solution in a bottom-up fashion. 4. Construct an optimal solution from computed information.  Assembly-line scheduling  A simple dynamic-programming example. Actually, solvable by a graph algorithm that we cid:146 ll see later in the course. But a good warm-up for dynamic programming. [Newinthesecondeditionofthebook.]   15-2  Lecture Notes for Chapter 15: Dynamic Programming  enter  2  e1  e2  4  S1,1  a1,1  7  S1,2  a1,2  9  S1,3  a1,3  3  S1,4  a1,4  4  S1,5  a1,5  8  2 t1,1  3 t1,2  1 t1,3  3 t1,4  2  t2,1  1  t2,2  2  t2,3  2  t2,4  8  a2,1  S2,1  5  a2,2  S2,2  6  a2,3  S2,3  4  a2,4  S2,4  a2,5  5 S2,5  3  x1  x2  6  exit  Automobile factory with two assembly lines.   Each line has n stations: S1,1, . . . , S1,n and S2,1, . . . , S2,n.   Corresponding stations S1, j and S2, j perform the same function but can take  different amounts of time a1, j and a2, j .    Entry times e1 and e2.   Exit times x1 and x2.   After going through a station, can either       stay on same line; no cost, or transfer to other line; cost after Si, j is ti, j .   j = 1, . . . , n−1. No ti,n, because the assembly line is done after Si,n.   Problem: Given all these costs  time = cost , what stations should be chosen from line 1 and from line 2 for fastest way through factory? Try all possibilities?   Each candidate is fully speci cid:222 ed by which stations from line 1 are included.  Looking for a subset of line 1 stations.    Line 1 has n stations.      2n subsets. Infeasible when n is large.  Structure of an optimal solution  Think about fastest way from entry through S1, j.      If j = 1, easy: just determine how long it takes to get through S1,1. If j ≥ 2, have two choices of how to get to S1, j:   Through S1, j−1, then directly to S1, j.   Through S2, j−1, then transfer over to S1, j.  Suppose fastest way is through S1, j−1.   Lecture Notes for Chapter 15: Dynamic Programming  15-3  Key observation: We must have taken a fastest way from entry through S1, j−1 in this solution. If there were a faster way through S1, j−1, we would use it instead to come up with a faster way through S1, j. Now suppose a fastest way is through S2, j−1. Again, we must have taken a fastest way through S2, j−1. Otherwise use some faster way through S2, j−1 to give a faster way through S1, j Generally: An optimal solution to a problem  fastest way through S1, j  contains within it an optimal solution to subproblems  fastest way through S1, j−1 or S2, j−1 . This is optimal substructure. Use optimal substructure to construct optimal solution to problem from optimal solutions to subproblems. Fastest way through S1, j is either      fastest way through S1, j−1 then directly through S1, j, or fastest way through S2, j−1, transfer from line 2 to line 1, then through S1, j.  Symmetrically: Fastest way through S2, j is either      fastest way through S2, j−1 then directly through S2, j, or fastest way through S1, j−1, transfer from line 1 to line 2, then through S2, j.  Therefore, to solve problems of  cid:222 nding a fastest way through S1, j and S2, j, solve subproblems of  cid:222 nding a fastest way through S1, j−1 and S2, j−1.  .  ∗  = min  f1[n] + x1, f2[n] + x2   Recursive solution Let fi[ j] = fastest time to get through Si, j , i = 1, 2 and j = 1, . . . , n. Goal: fastest time to get all the way through = f ∗ f f1[1] = e1 + a1,1 f2[1] = e2 + a2,1 For j = 2, . . . , n: f1[ j] = min  f1[ j − 1] + a1, j , f2[ j − 1] + t2, j−1 + a1, j   f2[ j] = min  f2[ j − 1] + a2, j , f1[ j − 1] + t1, j−1 + a2, j   fi[ j] gives the value of an optimal solution. What if we want to construct an optimal solution? li [ j] = line   1 or 2  whose station j − 1 is used in fastest way through Si, j .     In other words Sli [ j], j−1 precedes Si, j .   De cid:222 ned for i = 1, 2 and j = 2, . . . , n. ∗ = line  whose station n is used.  l      15-4  Lecture Notes for Chapter 15: Dynamic Programming  For example:  j f1[j] f2[j]  1 9 12  2 18 16  3 20 22  4 24 25  5 32 30  f * = 35  j l1[j] l2[j]  2 1 1  5 1 2  3 2 2  4 1 1  l* = 1  Go through optimal way given by l values.  Shaded path in earlier  cid:222 gure.   Compute an optimal solution  Could just write a recursive algorithm based on above recurrences.   Let ri   j   =  of references made to fi[ j].  r1 n  = r2 n  = 1. r1  j   = r2  j   = r1  j + 1  + r2  j + 1  for j = 1, . . . , n − 1.       Claim ri   j   = 2n− j . Proof Induction on j, down from n. Basis: j = n. 2n− j = 20 = 1 = ri  n . Inductive step: Assume ri   j + 1  = 2n−  j+1 . Then ri   j   = ri   j + 1  + r2  j + 1  = 2n−  j+1  + 2n−  j+1  = 2n−  j+1 +1 = 2n− j .  Therefore, f1[1] alone is referenced 2n−1 times! So top down isn cid:146 t a good way to compute fi[ j]. Observation: So compute in order of increasing j.  fi[ j] depends only on f1[ j − 1] and f2[ j − 1]  for j ≥ 2 .   claim    Lecture Notes for Chapter 15: Dynamic Programming  15-5  FASTEST-WAY a, t, e, x, n  f1[1] ← e1 + a1,1 f2[1] ← e2 + a2,1 for j ← 2 to n  do if f1[ j − 1] + a1, j ≤ f2[ j − 1] + t2, j−1 + a1, j  l1[ j] ← 1 l1[ j] ← 2  then f1[ j] ← f1[ j − 1] + a1, j else f1[ j] ← f2[ j − 1] + t2, j−1 + a1, j if f2[ j − 1] + a2, j ≤ f1[ j − 1] + t1, j−1 + a2, j then f2[ j] ← f2[ j − 1] + a2, j else f2[ j] ← f1[ j − 1] + t1, j−1 + a2, j  l2[ j] ← 2 l2[ j] ← 1 if f1[n] + x1 ≤ f2[n] + x2 ∗ = f1[n] + x1 then f ∗ = 1 l ∗ = f2[n] + x2 else f ∗ = 2 l  Go through example.  Constructing an optimal solution  ∗  PRINT-STATIONS l, n  i ← l print  cid:147 line  cid:148  i  cid:147 , station  cid:148  n for j ← n downto 2  do i ← li [ j]  print  cid:147 line  cid:148  i  cid:147 , station  cid:148  j − 1  Go through example. Time =  cid:2  n   Longest common subsequence  Problem: Given 2 sequences, X =  cid:2 x1, . . . , xm cid:3  and Y =  cid:2 y1, . . . , yn cid:3 . Find a subsequence common to both whose length is longest. A subsequence doesn cid:146 t have to be consecutive, but it has to be in order. [To come up with examples of longest common subsequences, search the dictio- naryforallwordsthatcontainthewordyouarelookingforasasubsequence. On a UNIX system, for example, to  cid:222 nd all the words with pineas a subsequence, usethecommandgrep ’.*p.*i.*n.*e.*’ dict,wheredictisyourlo- caldictionary. Thencheckifthatwordisactuallyalongestcommonsubsequence. Working C code for  cid:222 nding a longest commmon subsequence of two strings ap- pearsathttp:  www.cs.dartmouth.edu  cid:152 thc code lcs.c]   15-6  Lecture Notes for Chapter 15: Dynamic Programming  Examples: [Theexamplesareofdifferenttypesoftrees.]  s p r i n g t i m e  h o r s e b a c k  p i o n e e r  s n o w f l a k e  m a e l s t r o m  h e r o i c a l l y  b e c a l m  s c h o l a r l y  Brute-force algorithm: For every subsequence of X, check whether it cid:146 s a subsequence of Y . Time:  cid:2  n2m .     Each subsequence takes  cid:2  n  time to check: scan Y for  cid:222 rst letter, from there  2m subsequences of X to check.  scan for second, and so on.  Optimal substructure  Notation: Xi = pre cid:222 x  cid:2 x1, . . . , xi cid:3  Yi = pre cid:222 x  cid:2 y1, . . . , yi cid:3   Theorem Let Z =  cid:2 z1, . . . , zk cid:3  be any LCS of X and Y . 1. If xm = yn, then zk = xm = yn and Zk−1 is an LCS of Xm−1 and Yn−1. 2. If xm  cid:19 = yn, then zk  cid:19 = xm ⇒ Z is an LCS of Xm−1 and Y . 3. If xm  cid:19 = yn, then zk  cid:19 = yn ⇒ Z is an LCS of X and Yn−1. Proof 1. First show that zk = xm = yn. Suppose not. Then make a subsequence Z   cid:4  =  cid:2 z1, . . . , zk, xm cid:3 . It cid:146 s a common subsequence of X and Y and has length k + 1 is a longer common subsequence than Z ⇒ contradicts Z being an LCS. ⇒ Z Now show Zk−1 is an LCS of Xm−1 and Yn−1. Clearly, it cid:146 s a common subse- quence. Now suppose there exists a common subsequence W of Xm−1 and Yn−1 that cid:146 s longer than Zk−1 ⇒ length of W ≥ k. Make subsequence W by append- is common subsequence of X and Y , has length ≥ k + 1 ⇒ ing xm to W . W contradicts Z being an LCS. 2. If zk  cid:19 = xm, then Z is a common subsequence of Xm−1 and Y . Suppose there exists a subsequence W of Xm−1 and Y with length > k. Then W is a common subsequence of X and Y ⇒ contradicts Z being an LCS.   cid:4    cid:4    cid:4    Lecture Notes for Chapter 15: Dynamic Programming  3. Symmetric to 2.  15-7   theorem   Therefore, an LCS of two sequences contains as a pre cid:222 x an LCS of pre cid:222 xes of the sequences.  Recursive formulation De cid:222 ne c[i, j] = length of LCS of Xi and Y j . We want c[m, n]. if i = 0 or j = 0 , if i, j > 0 and xi = y j , c[i, j] =  cid:19 = y j . if i, j > 0 and xi  c[i − 1, j − 1] + 1 max c[i − 1, j], c[i, j − 1]   ⎧⎨⎩0  Again, we could write a recursive algorithm based on this formulation. Try with bozo, bat.  4,3  3,3  3,3  2,3  3,2  3,2  4,1  1,3  2,2  2,2  3,1  2,2  3,1  3,1  4,0  0,3  1,2  1,2  2,1  1,2  2,1  2,1  3,0    Lots of repeated subproblems.    Instead of recomputing, store in a table.  Compute length of optimal solution  LCS-LENGTH X, Y, m, n  for i ← 1 to m for j ← 0 to n for i ← 1 to m  do c[i, 0] ← 0 do c[0, j] ← 0 do for j ← 1 to n do if xi = y j  then c[i, j] ← c[i − 1, j − 1] + 1 b[i, j] ←  cid:147  cid:25  cid:148  else if c[i − 1, j] ≥ c[i, j − 1] then c[i, j] ← c[i − 1, j] else c[i, j] ← c[i, j − 1]  b[i, j] ←  cid:147 ↑ cid:148  b[i, j] ←  cid:147 ← cid:148   return c and b   15-8  Lecture Notes for Chapter 15: Dynamic Programming  PRINT-LCS b, X, i, j   if i = 0 or j = 0 then return if b[i, j] =  cid:147  cid:25  cid:148  then PRINT-LCS b, X, i − 1, j − 1  elseif b[i, j] =  cid:147 ↑ cid:148  then PRINT-LCS b, X, i − 1, j   else PRINT-LCS b, X, i, j − 1   print xi  Initial call is PRINT-LCS b, X, m, n . b[i, j] points to table entry whose subproblem we used in solving LCS of Xi and Y j .   When b[i, j] =  cid:25 , we have extended LCS by one character. So longest com- mon subsequence = entries with  cid:25  in them.       s  p  a  n  k  i  n  g  Demonstration: show only c[i, j]:  pma  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  0  0  0  1  1  1  1  1  1  u  0  0  1  1  1  1  1  1  1  t  0  0  1  1  1  1  1  1  1  0  0  1  1  1  1  1  1  1  p  t  0  0  1  2  2  2  2  2  2  a  0  0  1  2  2  2  2  2  2  a  i  0  0  1  2  2  2  3  3  3  i  o  0  0  1  2  2  2  3  3  3  n  0  0  1  2  3  3  3  4  4  n  Time:  cid:2  mn   Optimal binary search trees  ··· < kn .  [Alsonewinthesecondedition.]   Given sequence K =  cid:2 k1, k2, . . . , kn cid:3  of n distinct keys, sorted  k1 < k2 <   Want to build a binary search tree from the keys.   For ki , have probability pi that a search is for ki.   Want BST with minimum expected search cost.   Lecture Notes for Chapter 15: Dynamic Programming  15-9    Actual cost =  of items examined.  For key ki, cost = depthT   ki   + 1, where depthT   ki   = depth of ki in BST T .  E [search cost in T]  i=1  = n cid:2  = n cid:2  = 1 + n cid:2   i=1  i=1   depthT  depthT   ki   + 1  · pi   ki   · pi + n cid:2   i=1  ki   · pi  depthT  pi   since probabilities sum to 1    ∗   [SimilartooptimalBSTprobleminthebook,butsimpli cid:222 edhere: weassumethat all searches are successful. Book has probabilities of searches between keys in tree.]  i pi  1 .25  2 .2  3 .05  4 .2  5 .3  Example:  k1  k4  k2  k3  i 1 2 3 4 5  depthT 1 0 2 1 2  k5   ki    depthT   ki   · pi .25 0 .1 .2 .6 1.15  Therefore, E [search cost] = 2.15.   15-10  Lecture Notes for Chapter 15: Dynamic Programming  k1  k5  k2  k3  k4  i 1 2 3 4 5  depthT 1 0 3 2 1   ki    depthT   ki   · pi .25 0 .15 .4 .3 1.10  Therefore, E [search cost] = 2.10, which turns out to be optimal.  Observations:   Optimal BST might not have smallest height.   Optimal BST might not have highest-probability key at root. Build by exhaustive checking?   Construct each n-node BST.   For each, put in keys.   Then compute expected search cost.   But there are  cid:3  4n n3 2  different BSTs with n nodes.  Optimal substructure  Consider any subtree of a BST. It contains keys in a contiguous range ki , . . . , k j for some 1 ≤ i ≤ j ≤ n.  T  T’  If T is an optimal BST and T contains subtree T must be an optimal BST for keys ki , . . . , k j .  Proof Cut and paste.   cid:4   with keys ki , . . . , k j , then T   cid:4   Use optimal substructure to construct an optimal solution to the problem from op- timal solutions to subproblems:   Lecture Notes for Chapter 15: Dynamic Programming  15-11    Given keys ki , . . . , k j  the problem .   One of them, kr , where i ≤ r ≤ j, must be the root.   Left subtree of kr contains ki , . . . , kr−1.   Right subtree of kr contains kr+1, . . . , k j .  kr  ki  kr cid:150 1  kr+1  kj     If   we examine all candidate roots kr , for i ≤ r ≤ j, and   we determine all optimal BSTs containing ki , . . . , kr−1 and containing  kr+1, . . . , k j ,  then we cid:146 re guaranteed to  cid:222 nd an optimal BST for ki , . . . , k j .  Recursive solution  Subproblem domain:   Find optimal BST for ki , . . . , k j , where i ≥ 1, j ≤ n, j ≥ i − 1.   When j = i − 1, the tree is empty. De cid:222 ne e[i, j] = expected search cost of optimal BST for ki , . . . , k j . If j = i − 1, then e[i, j] = 0. If j ≥ i,   Select a root kr , for some i ≤ r ≤ j.   Make an optimal BST with ki , . . . , kr−1 as the left subtree.   Make an optimal BST with kr+1, . . . , k j as the right subtree.   Note: when r = i, left subtree is ki , . . . , ki−1; when r = j, right subtree is  k j+1, . . . , k j .  When a subtree becomes a subtree of a node:   Depth of every node in subtree goes up by 1.   Expected search cost increases by  w i, j   = j cid:2   pl  l=i   refer to equation  ∗   .  If kr is the root of an optimal BST for ki , . . . , k j : e[i, j] = pr +  e[i, r − 1] + w i, r − 1   +  e[r + 1, j] + w r + 1, j    .   15-12  Lecture Notes for Chapter 15: Dynamic Programming  But w i, j   = w i, r − 1  + pr + w r + 1, j  . Therefore, e[i, j] = e[i, r − 1] + e[r + 1, j] + w i, j  . This equation assumes that we already know which key is kr . We don cid:146 t. Try all candidates, and pick the best one: e[i, j] =  {e[i, r − 1] + e[r + 1, j] + w i, j  }  $  0 min i≤r≤ j  if j = i − 1 , if i ≤ j .  Could write a recursive algorithm. . .  Computing an optimal solution   cid:23  cid:24  cid:25  cid:26    cid:24  cid:25   cid:26   cid:23  As  cid:147 usual, cid:148  we cid:146 ll store the values in a table: e[ 1 . . n + 1 e[n + 1, n] can store   Will use only entries e[i, j], where j ≥ i − 1.   Will also compute  , 0 . . n ] can store e[1, 0]  root[i, j] = root of subtree with keys ki , . . . , k j , for 1 ≤ i ≤ j ≤ n .  One other table. . . don cid:146 t recompute w i, j   from scratch every time we need it.  Would take  cid:2   j − i   additions.  Instead:   Table w[1 . . n + 1, 0 . . n]   w[i, i − 1] = 0 for 1 ≤ i ≤ n   w[i, j] = w[i, j − 1] + p j for 1 ≤ i ≤ j ≤ n Can compute all  cid:2  n2  values in O 1  time each.  OPTIMAL-BST  p, q, n  for i ← 1 to n + 1 do e[i, i − 1] ← 0 w[i, i − 1] ← 0  for l ← 1 to n  do for i ← 1 to n − l + 1 do j ← i + l − 1 e[i, j] ← ∞ w[i, j] ← w[i, j − 1] + p j for r ← i to j  do t ← e[i, r − 1] + e[r + 1, j] + w[i, j]  if t < e[i, j] then e[i, j] ← t  root[i, j] ← r  return e and root   Lecture Notes for Chapter 15: Dynamic Programming  15-13  First for loop initializes e, w entries for subtrees with 0 keys. Main for loop:      Iteration for l works on subtrees with l keys. Idea: compute in order of subtree sizes, smaller  1 key  to larger  n keys .  For example at beginning:  0 0  0 0  1 1  e 1 2 3 4 5 6  w 1 2 3 4 5 6  i  i  root 1 2 3 4 5  i  j  2 .65 .2 0  3 .8 .3 .05 0  1 .25 0  pi  4  1.25 .75 .3 .2 0  5  2.10 1.35 .85 .7 .3 0  1 .25 0  j  2 .45 .2 0  4 3 .5 .7 .25 .45 .05 .25 0 .2 0  5 1.0 .75 .55 .5 .3 0  2 1 2  j 3 1 2 3  4 2 2 4 4  5 2 4 5 5 5  Time: O n3 : for loops nested 3 deep, each loop index takes on ≤ n values. Can also show  cid:3  n3 . Therefore,  cid:2  n3 .  Construct an optimal solution  CONSTRUCT-OPTIMAL-BST root  r ← root[1, n] print  cid:147 k cid:148 r  cid:147 is the root cid:148  CONSTRUCT-OPT-SUBTREE 1, r − 1, r,  cid:147 left cid:148 , root  CONSTRUCT-OPT-SUBTREE r + 1, n, r,  cid:147 right cid:148 , root  CONSTRUCT-OPT-SUBTREE  i, j, r, dir, root  if i ≤ j then t ← root[i, j]  print  cid:147 k cid:148 t  cid:147 is cid:148  dir  cid:147 child of k cid:148 r CONSTRUCT-OPT-SUBTREE  i, t − 1, t,  cid:147 left cid:148 , root  CONSTRUCT-OPT-SUBTREE  t + 1, j, t,  cid:147 right cid:148 , root    15-14  Lecture Notes for Chapter 15: Dynamic Programming  Elements of dynamic programming  Mentioned already:      optimal substructure overlapping subproblems  Optimal substructure    Show that a solution to a problem consists of making a choice, which leaves  one or subproblems to solve.    Suppose that you are given this last choice that leads to an optimal solution. [We  cid:222 nd that students often have trouble understanding the relationship between optimal substructure and determining which choice is made in an optimal so- lution. Onewaythathelpsthemunderstand optimalsubstructure istoimagine that cid:147 God cid:148 tellsyouwhatwasthelastchoicemadeinanoptimalsolution.]    Given this choice, determine which subproblems arise and how to characterize  the resulting space of subproblems.    Show that the solutions to the subproblems used within the optimal solution  must themselves be optimal. Usually use cut-and-paste:   Suppose that one of the subproblem solutions is not optimal.   Cut it out.   Paste in an optimal solution.   Get a better solution to the original problem. Contradicts optimality of prob-  lem solution.  That was optimal substructure. Need to ensure that you consider a wide enough range of choices and subproblems that you get them all. [ cid:147 God cid:148  is too busy to tell you what that last choice really was.] Try all the choices, solve all the subproblems resulting from each choice, and pick the choice whose solution, along with subproblem solutions, is best. How to characterize the space of subproblems?   Keep the space as simple as possible.   Expand it as necessary.  Examples:  Assembly-line scheduling    Space of subproblems was fastest way from factory entry through stations  S1, j and S2, j.    No need to try a more general space of subproblems.  Optimal binary search trees  keys k1, k2, . . . , k j .    Suppose we had tried to constrain space of subproblems to subtrees with   Lecture Notes for Chapter 15: Dynamic Programming  15-15    An optimal BST would have root kr , for some 1 ≤ r ≤ j.   Get subproblems k1, . . . , kr−1 and kr+1, . . . , k j .   Unless we could guarantee that r = j, so that subproblem with kr+1, . . . , k j   Thus, needed to allow the subproblems to vary at  cid:147 both ends, cid:148  i.e., allow  is empty, then this subproblem is not of the form k1, k2, . . . , k j .  both i and j to vary.  Optimal substructure varies across problem domains:  1. How many subproblems are used in an optimal solution. 2. How many choices in determining which subproblem s  to use.   Assembly-line scheduling:    1 subproblem   2 choices  for Si, j use either S1, j−1 or S2, j−1     Longest common subsequence:    1 subproblem   Either    1 choice  if xi = y j , LCS of X i−1 and Y j−1 , or   2 choices  if xi   cid:19 = y j , LCS of X i−1 and Y , and LCS of X and Y j−1     Optimal binary search tree:    2 subproblems  ki , . . . , kr−1 and kr+1, . . . , k j      j − i + 1 choices for kr in ki , . . . , k j . Once we determine optimal solutions to subproblems, we choose from among the j − i + 1 candidates for kr . Informally, running time depends on   of subproblems overall  ×   of choices .   Assembly-line scheduling:  cid:2  n  subproblems, 2 choices for each   Longest common subsequence:  cid:2  mn  subproblems, ≤ 2 choices for each    Optimal binary search tree:  cid:2  n2  subproblems, O n  choices for each  ⇒  cid:2  n  running time. ⇒  cid:2  mn  running time. ⇒ O n3  running time.  Dynamic programming uses optimal substructure bottom up.   First  cid:222 nd optimal solutions to subproblems.   Then choose which to use in optimal solution to the problem. When we look at greedy algorithms, we cid:146 ll see that they work top down:  cid:222 rst make a choice that looks best, then solve the resulting subproblem. Don cid:146 t be fooled into thinking optimal substructure applies to all optimization prob- lems. It doesn cid:146 t. Here are two problems that look similar. directed graph G =  V, E .  In both, we cid:146 re given an unweighted,   15-16  Lecture Notes for Chapter 15: Dynamic Programming    V is a set of vertices.   E is a set of edges.  And we ask about  cid:222 nding a path  sequence of connected edges  from vertex u to vertex v.   Shortest path:  cid:222 nd path u  cid:2  v with fewest edges. Must be simple  no cycles ,  since removing a cycle from a path gives a path with fewer edges.    Longest simple path:  cid:222 nd simple path u  cid:2  v with most edges. If didn cid:146 t require  simple, could repeatedly traverse a cycle to make an arbitrarily long path.  Shortest path has optimal substructure.  p1  p2  u  v    Suppose p is shortest path u  cid:2  v.   Let w be any vertex on p.   Let p1 be the portion of p, u  cid:2  w.   Then p1 is a shortest path u  cid:2  w.  cid:4  1, u  cid:2  w. Cut out p1, replace it Proof Suppose there exists a shorter path p  with p   cid:4  1, get path u   cid:4  p 1 cid:2  w p2 cid:2  v with fewer edges than p.  Therefore, can  cid:222 nd shortest path u  cid:2  v by considering all intermediate vertices w, then  cid:222 nding shortest paths u  cid:2  w and w  cid:2  v. Same argument applies to p2. Does longest path have optimal substructure?      It seems like it should. It does not.  w  p  r  t  q  s  Consider q → r → t = longest path q  cid:2  t. Are its subpaths longest paths? No!   Subpath q  cid:2  r is q → r.   Longest simple path q  cid:2  r is q → s → t → r.   Subpath r  cid:2  t is r → t.   Longest simple path r  cid:2  t is r → q → s → t.   Lecture Notes for Chapter 15: Dynamic Programming  15-17  Not only isn cid:146 t there optimal substructure, but we can cid:146 t even assemble a legal solu- tion from solutions to subproblems. Combine longest simple paths: q → s → t → r → q → s → t Not simple! In fact, this problem is NP-complete  so it probably has no optimal substructure to  cid:222 nd.  What cid:146 s the big difference between shortest path and longest path?   Shortest path has independent subproblems.   Solution to one subproblem does not affect solution to another subproblem of  the same problem.    Longest simple path: subproblems are not independent.   Consider subproblems of longest simple paths q  cid:2  r and r  cid:2  t.   Longest simple path q  cid:2  r uses s and t.   Cannot use s and t to solve longest simple path r  cid:2  t, since if we do, the path  isn cid:146 t simple.    But we have to use t to  cid:222 nd longest simple path r  cid:2  t!   Using resources  vertices  to solve one subproblem renders them unavailable to  solve the other subproblem. [Forshortest paths, ifwelook atashortest path u p1 cid:2  w p2 cid:2  v,novertex other than w canappearin p1 and p2. Otherwise,wehaveacycle.]  Independent subproblems in our examples:   Assembly line and longest common subsequence   1 subproblem ⇒ automatically independent.    Optimal binary search tree    ki , . . . , kr−1 and kr+1, . . . , k j ⇒ independent.  Overlapping subproblems  These occur when a recursive algorithm revisits the same problem over and over. Good divide-and-conquer algorithms usually generate a brand new problem at each stage of recursion. Example: merge sort  1..8  1..4  5..8  1..2  3..4  5..6  7..8  1..1  2..2  3..3  4..4  5..5  6..6  7..7  8..8   15-18  Lecture Notes for Chapter 15: Dynamic Programming  Won cid:146 t go through exercise of showing repeated subproblems. Book has a good example for matrix-chain multiplication. Alternative approach: memoization     Make a table indexed by subproblem.   When solving a subproblem:   cid:147 Store, don cid:146 t recompute. cid:148   If answer is there, use it.    Lookup in table.     Else, compute answer, then store it. In dynamic programming, we go one step further. We determine in what order we cid:146 d want to access the table, and  cid:222 ll it in that way.      Solutions for Chapter 15: Dynamic Programming  Solution to Exercise 15.1-5  Solution to Exercise 15.2-4  If l1[ j] = 2, then the fastest way to go through station j on line 1 is by changing lines from station j − 1 on line 2. This means that f2[ j − 1] + t1, j−1 + a1, j < f1[ j − 1] + a1, j . Dropping a1, j from both sides of the equation yields f2[ j − 1] + t1, j−1 < f1[ j − 1]. If l2[ j] = 1, then the fastest way to go through station j on line 2 is by changing lines from station j − 1 on line 1. This means that f1[ j − 1] + t2, j−1 + a2, j < f2[ j − 1] + a2, j . Dropping a2, j from both sides of the equation yields f1[ j − 1] + t2, j−1 < f2[ j − 1]. We can derive a contradiction by combining the two equations as follows: f2[ j − 1] + t1, j−1 < f1[ j − 1] and f1[ j − 1] + t2, j−1 < f2[ j − 1] yields f2[ j − 1] + t1, j−1 + t2, j−1 < f2[ j − 1]. Since all transfer costs are nonnega- tive, the resulting inequality cannot hold. We conclude that we cannot have the situation where l1[ j] = 2 and l2[ j] = 1.  Each time the l-loop executes, the i-loop executes n − l + 1 times. Each time the i-loop executes, the k-loop executes j − i = l − 1 times, each time referencing n cid:2  n cid:2  m twice. Thus the total number of times that an entry of m is referenced while computing other entries is   n − l + 1  l − 1 2. Thus,  i=1  j=i  n l=2   cid:1  R i, j   = n cid:2   n − l + 1  l − 1 2 n−1 cid:2   n − l l n−1 cid:2  n−1 cid:2  nl − 2 l=1 n n − 1 n  l=2 = 2  l=1  = 2 = 2  l=1 − 2  2  6  l2  n − 1 n 2n − 1    15-20  Solutions for Chapter 15: Dynamic Programming  = n3 − n2 − 2n3 − 3n2 + n = n3 − n  3  .  3  Solution to Exercise 15.3-1  Running RECURSIVE-MATRIX-CHAIN is asymptotically more ef cid:222 cient than enu- merating all the ways of parenthesizing the product and computing the number of multiplications for each. Consider the treatment of subproblems by the two approaches.   For each possible place to split the matrix chain, the enumeration approach  cid:222 nds all ways to parenthesize the left half,  cid:222 nds all ways to parenthesize the right half, and looks at all possible combinations of the left half with the right half. The amount of work to look at each combination of left- and right-half subproblem results is thus the product of the number of ways to do the left half and the number of ways to do the right half.    For each possible place to split the matrix chain, RECURSIVE-MATRIX-CHAIN  cid:222 nds the best way to parenthesize the left half,  cid:222 nds the best way to parenthesize the right half, and combines just those two results. Thus the amount of work to combine the left- and right-half subproblem results is O 1 .  Section 15.2 argued that the running time for enumeration is  cid:3  4n n3 2 . We will show that the running time for RECURSIVE-MATRIX-CHAIN is O n3n−1 . To get an upper bound on the running time of RECURSIVE-MATRIX-CHAIN, we cid:146 ll use the same approach used in Section 15.2 to get a lower bound: Derive a recur- rence of the form T  n  ≤ . . . and solve it by substitution. For the lower-bound recurrence, the book assumed that the execution of lines 1 cid:150 2 and 6 cid:150 7 each take at least unit time. For the upper-bound recurrence, we cid:146 ll assume those pairs of lines each take at most constant time c. Thus, we have the recurrence  T  n  ≤   T  k  + T  n − k  + c   if n = 1 , if n ≥ 2 .  T  i   + cn .  This is just like the book cid:146 s ≥ recurrence except that it has c instead of 1, and so we can be rewrite it as T  n  ≤ 2 We shall prove that T  n  = O n3n−1  using the substitution method.  Note: Any upper bound on T  n  that is o 4n n3 2  will suf cid:222 ce. You might prefer to prove one that is easier to think up, such as T  n  = O 3.5n  .  Speci cid:222 cally, we shall show that T  n  ≤ cn3n−1 for all n ≥ 1. The basis is easy, since T  1  ≤ c = c · 1 · 31−1.  i=1  ⎧⎪⎨⎪⎩c c + n−1 cid:2  n−1 cid:2   k=1   Solutions for Chapter 15: Dynamic Programming  15-21  Inductively, for n ≥ 2 we have T  n  ≤ 2  i=1  n−1 cid:2  n−1 cid:2   cid:3   cid:5    cid:4   ≤ 2  T  i   + cn ci3i−1 + cn n−1 cid:2   cid:5  i3i−1 + n n3n−1  cid:5  3 − 1  2 i=1 2 ·  i=1  cid:6  ≤ c · + 1 − 3n  cid:6  = c ·  3 − 1 2 1 − 3n + n = cn3n−1 + c · 2  2n + 1 − 3n  = cn3n−1 + c 2 ≤ cn3n−1 for all c > 0, n ≥ 1 .   cid:6   + n   see below   + 1 − x n  x − 1 2  Running RECURSIVE-MATRIX-CHAIN takes O n3n−1  time, and enumerating all parenthesizations takes  cid:3  4n n3 2  time, and so RECURSIVE-MATRIX-CHAIN is more ef cid:222 cient than enumeration. n−1 cid:2  Note: The above substitution uses the fact that i x i−1 = nx n−1 x − 1 f  x  = n−1 cid:2  n−1 cid:2   i=1 This equation can be derived from equation  A.5  by taking the derivative. Let  x i = x n − 1 x − 1  − 1 .  Then  i=1  .  i x i−1 = f   cid:4  x  = nx n−1 x − 1  + 1 − x n  x − 1 2  .  i=1  When computing a particular row of the c table, no rows before the previous row are needed. Thus only two rows cid:151 2·length[Y ] entries cid:151 need to be kept in memory at a time.  Note: Each row of c actually has length[Y ] + 1 entries, but we don cid:146 t need to store the column of 0 cid:146 s cid:151 instead we can make the program  cid:147 know cid:148  that those entries are 0.  With this idea, we need only 2· min m, n  entries if we always call LCS-LENGTH with the shorter sequence as the Y argument. We can thus do away with the c table as follows:   Use two arrays of length min m, n , previous-row and current-row, to hold the  appropriate rows of c. Initialize previous-row to all 0 and compute current-row from left to right.     Solution to Exercise 15.4-4   15-22  Solutions for Chapter 15: Dynamic Programming  current-row into previous-row and compute the new current-row.    When current-row is  cid:222 lled, if there are still more rows to compute, copy Actually only a little more than one row cid:146 s worth of c entries cid:151 min m, n  + 1 en- tries cid:151 are needed during the computation. The only entries needed in the table when it is time to compute c[i, j] are c[i, k] for k ≤ j − 1  i.e., earlier entries in the current row, which will be needed to compute the next row ; and c[i − 1, k] for k ≥ j − 1  i.e., entries in the previous row that are still needed to compute the rest of the current row . This is one entry for each k from 1 to min m, n  except that there are two entries with k = j − 1, hence the additional entry needed besides the one row cid:146 s worth of entries. We can thus do away with the c table as follows:   Use an array a of length min m, n  + 1 to hold the appropriate entries of c. At the time c[i, j] is to be computed, a will hold the following entries:   a[k] = c[i, k] for 1 ≤ k < j − 1  i.e., earlier entries in the current  cid:147 row cid:148  ,   a[k] = c[i − 1, k] for k ≥ j − 1  i.e., entries in the previous  cid:147 row cid:148  ,   a[0] = c[i, j − 1]  i.e., the previous entry computed, which couldn cid:146 t be put into the  cid:147 right cid:148  place in a without erasing the still-needed c[i − 1, j − 1] .     Initialize a to all 0 and compute the entries from left to right.   Note that the 3 values needed to compute c[i, j] for j > 1 are in a[0] =   When c[i, j] has been computed, move a[0]  c[i, j − 1]  to its  cid:147 correct cid:148   c[i, j − 1], a[ j − 1] = c[i − 1, j − 1], and a[ j] = c[i − 1, j]. place, a[ j − 1], and put c[i, j] in a[0].  Taking the book cid:146 s hint, we sort the points by x-coordinate, left to right, in O n lg n  time. Let the sorted points be, left to right,  cid:2 p1, p2, p3, . . . , pn cid:3 . Therefore, p1 is the leftmost point, and pn is the rightmost. We de cid:222 ne as our subproblems paths of the following form, which we call bitonic paths. A bitonic path Pi, j , where i ≤ j, includes all points p1, p2, . . . , p j ; it starts at some point pi , goes strictly left to point p1, and then goes strictly right to point p j . By  cid:147 going strictly left, cid:148  we mean that each point in the path has a lower x- coordinate than the previous point. Looked at another way, the indices of the sorted points form a strictly decreasing sequence. Likewise,  cid:147 going strictly right cid:148  means that the indices of the sorted points form a strictly increasing sequence. Moreover, Pi, j contains all the points p1, p2, p3, . . . , p j . Note that p j is the rightmost point in Pi, j and is on the rightgoing subpath. The leftgoing subpath may be degenerate, consisting of just p1. Let us denote the euclidean distance between any two points pi and p j by pi p j. And let us denote by b[i, j], for 1 ≤ i ≤ j ≤ n, the length of the shortest bitonic path Pi, j . Since the leftgoing subpath may be degenerate, we can easily compute all values b[1, j]. The only value of b[i, i] that we will need is b[n, n], which is  Solution to Problem 15-1   Solutions for Chapter 15: Dynamic Programming  15-23  the length of the shortest bitonic tour. We have the following formulation of b[i, j] for 1 ≤ i ≤ j ≤ n:  b[1, 2] =  p1 p2 , b[i, j] = b[i, j − 1] +p j−1 p j  b[ j − 1, j] =  {b[k, j − 1] + pk p j} .  min 1≤k< j−1  for i < j − 1 ,  Why are these formulas correct? Any bitonic path ending at p2 has p2 as its right- most point, so it consists only of p1 and p2. Its length, therefore, is p1 p2. Now consider a shortest bitonic path Pi, j . The point p j−1 is somewhere on this path. If it is on the rightgoing subpath, then it immediately preceeds pj on this subpath. Otherwise, it is on the leftgoing subpath, and it must be the rightmost point on this subpath, so i = j − 1. In the  cid:222 rst case, the subpath from pi to p j−1 must be a shortest bitonic path Pi, j−1, for otherwise we could use a cut-and-paste argument to come up with a shorter bitonic path than Pi, j .  This is part of our opti- mal substructure.  The length of Pi, j , therefore, is given by b[i, j − 1] +p j−1 p j. In the second case, pj has an immediate predecessor pk, where k < j − 1, on the rightgoing subpath. Optimal substructure again applies: the subpath from pk to p j−1 must be a shortest bitonic path Pk, j−1, for otherwise we could use cut-and- paste to come up with a shorter bitonic path than Pi, j .  We have implicitly relied on paths having the same length regardless of which direction we traverse them.  The length of Pi, j , therefore, is given by min1≤k≤ j−1 {b[k, j − 1] + pk p j}. We need to compute b[n, n]. In an optimal bitonic tour, one of the points adjacent to pn must be pn−1, and so we have b[n, n] = b[n − 1, n] +pn−1 pn . To reconstruct the points on the shortest bitonic tour, we de cid:222 ne r[i, j] to be the immediate predecessor of pj on the shortest bitonic path Pi, j . The pseudocode below shows how we compute b[i, j] and r[i, j]:  EUCLIDEAN-TSP  p  sort the points so that  cid:2 p1, p2, p3, . . . , pn cid:3  are in order of increasing x-coordinate b[1, 2] ← p1 p2 for j ← 3 to n  do for i ← 1 to j − 2  do b[i, j] ← b[i, j − 1] +p j−1 p j  r[i, j] ← j − 1  b[ j − 1, j] ← ∞ for k ← 1 to j − 2  do q ← b[k, j − 1] + pk p j if q < b[ j − 1, j] then b[ j − 1, j] ← q r[ j − 1, j] ← k  b[n, n] ← b[n − 1, n] +pn−1 pn return b and r  We print out the tour we found by starting at pn, then a leftgoing subpath that includes pn−1, from right to left, until we hit p1. Then we print right-to-left the remaining subpath, which does not include pn−1. For the example in Figure 15.9 b    15-24  Solutions for Chapter 15: Dynamic Programming  on page 365, we wish to print the sequence p7, p6, p4, p3, p1, p2, p5. Our code is recursive. The right-to-left subpath is printed as we go deeper into the recursion, and the left-to-right subpath is printed as we back out.  PRINT-TOUR r, n  print pn print pn−1 k ← r[n − 1, n] PRINT-PATH r, k, n − 1  print pk  PRINT-PATH r, i, j   if i < j then k ← r[i, j]  print pk if k > 1  else k ← r[ j, i]  if k > 1  then PRINT-PATH r, i, k   then PRINT-PATH r, k, j   print pk  Solution to Problem 15-2  The relative values of the parameters i and j in each call of PRINT-PATH indicate which subpath we cid:146 re working on. If i < j, we cid:146 re on the right-to-left subpath, and if i > j, we cid:146 re on the left-to-right subpath. The time to run EUCLIDEAN-TSP is O n2  since the outer loop on j iterates n − 2 times and the inner loops on i and k each run at most n − 2 times. The sorting step at the beginning takes O n lg n  time, which the loop times dominate. The time to run PRINT-TOUR is O n , since each point is printed just once.  Note: we will assume that no word is longer than will  cid:222 t into a line, i.e., li ≤ M for all i. First, we cid:146 ll make some de cid:222 nitions so that we can state the problem more uniformly. Special cases about the last line and worries about whether a sequence of words  cid:222 ts in a line will be handled in these de cid:222 nitions, so that we can forget about them when framing our overall strategy.    De cid:222 ne extras[i, j] = M− j+i− cid:1   j k=i lk to be the number of extra spaces at the end of a line containing words i through j. Note that extras may be negative.   Now de cid:222 ne the cost of including a line containing words i through j in the sum  ⎧⎨⎩∞  we want to minimize: lc[i, j] =  if extras[i, j] < 0  i.e., words i, . . . , j don cid:146 t  cid:222 t  , if j = n and extras[i, j] ≥ 0  last line costs 0  ,  0  extras[i, j] 3 otherwise .   Solutions for Chapter 15: Dynamic Programming  15-25  By making the line cost in cid:222 nite when the words don cid:146 t  cid:222 t on it, we prevent such an arrangement from being part of a minimal sum, and by making the cost 0 for the last line  if the words  cid:222 t , we prevent the arrangement of the last line from in cid:223 uencing the sum being minimized.  We want to minimize the sum of lc over all lines of the paragraph. Our subproblems are how to optimally arrange words 1, . . . , j, where j = 1, . . . , n. Consider an optimal arrangement of words 1, . . . , j. Suppose we know that the last line, which ends in word j, begins with word i. The preceding lines, therefore, contain words 1, . . . , i − 1. In fact, they must contain an optimal arrangement of words 1, . . . , i − 1.  Insert your favorite cut-and-paste argument here.  Let c[ j] be the cost of an optimal arrangement of words 1, . . . , j. If we know that the last line contains words i, . . . , j, then c[ j] = c[i − 1]+ lc[i, j]. As a base case, when we cid:146 re computing c[1], we need c[0]. If we set c[0] = 0, then c[1] = lc[1, 1], which is what we want. But of course we have to  cid:222 gure out which word begins the last line for the sub- problem of words 1, . . . , j. So we try all possibilities for word i, and we pick the one that gives the lowest cost. Here, i ranges from 1 to j. Thus, we can de cid:222 ne c[ j] recursively by c[ j] =   c[i − 1] + lc[i, j]   if j = 0 , if j > 0 .  $  0 min 1≤i≤ j  Note that the way we de cid:222 ned lc ensures that    all choices made will  cid:222 t on the line  since an arrangement with lc = ∞ cannot be chosen as the minimum , and the cost of putting words i, . . . , j on the last line will not be 0 unless this really is the last line of the paragraph   j = n  or words i . . . j  cid:222 ll the entire line.     We can compute a table of c values from left to right, since each value depends only on earlier values. To keep track of what words go on what lines, we can keep a parallel p table that points to where each c value came from. When c[ j] is computed, if c[ j] is based on the value of c[k − 1], set p[ j] = k. Then after c[n] is computed, we can trace the pointers to see where to break the lines. The last line starts at word p[n] and goes through word n. The previous line starts at word p[p[n]] and goes through word p[n] − 1, etc. In pseudocode, here cid:146 s how we construct the tables:   15-26  Solutions for Chapter 15: Dynamic Programming  PRINT-NEATLY l, n, M   cid:1  Compute extras[i, j] for 1 ≤ i ≤ j ≤ n. for i ← 1 to n  do extras[i, i] ← M − li for j ← i + 1 to n   cid:1  Compute lc[i, j] for 1 ≤ i ≤ j ≤ n. for i ← 1 to n  do for j ← i to n  do extras[i, j] ← extras[i, j − 1] − lj − 1  do if extras[i, j] < 0  then lc[i, j] ← ∞ elseif j = n and extras[i, j] ≥ 0 then lc[i, j] ← 0 else lc[i, j] ←  extras[i, j] 3   cid:1  Compute c[ j] and p[ j] for 1 ≤ j ≤ n. c[0] ← 0 for j ← 1 to n  do c[ j] ← ∞  for i ← 1 to j  do if c[i − 1] + lc[i, j] < c[ j]  then c[ j] ← c[i − 1] + lc[i, j]  p[ j] ← i  return c and p  Quite clearly, both the time and space are  cid:2  n2 . In fact, we can do a bit better: we can get both the time and space down to  cid:2  n M . The key observation is that at most  cid:14 M 2 cid:15  words can  cid:222 t on a line.  Each word is at least one character long, and there cid:146 s a space between words.  Since a line with words i, . . . , j contains j − i + 1 words, if j − i + 1 >  cid:14 M 2 cid:15  then we know that lc[i, j] = ∞. We need only compute and store extras[i, j] and lc[i, j] for j − i + 1 ≤  cid:14 M 2 cid:15 . And the inner for loop header in the computation of c[ j] and p[ j] can run from max 1, j − cid:14 M 2 cid:15  + 1  to j. We can reduce the space even further to  cid:2  n . We do so by not storing the lc and extras tables, and instead computing the value of lc[i, j] as needed in the last loop. The idea is that we could compute lc[i, j] in O 1  time if we knew the value of extras[i, j]. And if we scan for the minimum value in descending order of i, we can compute that as extras[i, j] = extras[i + 1, j] − li − 1.  Initially, extras[ j, j] = M − l j .  This improvement reduces the space to  cid:2  n , since now the only tables we store are c and p. Here cid:146 s how we print which words are on which line. The printed output of GIVE-LINES  p, j   is a sequence of triples  k, i, j  , indicating that words i, . . . , j are printed on line k. The return value is the line number k.   Solutions for Chapter 15: Dynamic Programming  15-27  GIVE-LINES  p, j   i ← p[ j] if i = 1 then k ← 1 else k ← GIVE-LINES  p, i − 1  + 1 print  k, i, j   return k  The initial call is GIVE-LINES  p, n . Since the value of j decreases in each recur- sive call, GIVE-LINES takes a total of O n  time.  Solution to Problem 15-3     a. Dynamic programming is the ticket. This problem is slightly similar to the longest-common-subsequence problem. In fact, we cid:146 ll de cid:222 ne the notational con- veniences Xi and Y j in the similar manner as we did for the LCS problem: Xi = x[1 . . i] and Y j = y[1 . . j]. Our subproblems will be determining an optimal sequence of operations that converts Xi to Y j , for 0 ≤ i ≤ m and 0 ≤ j ≤ n. We cid:146 ll call this the  cid:147 Xi → Y j problem. cid:148  The original problem is the Xm → Yn problem. Let cid:146 s suppose for the moment that we know what was the last operation used to convert Xi to Y j. There are six possibilities. We denote by c[i, j] the cost of an optimal solution to the Xi → Y j problem. If the last operation was a copy, then we must have had x[i] = y[ j]. The sub- problem that remains is converting Xi−1 to Y j−1. And an optimal solution to the Xi → Y j problem must include an optimal solution to the Xi−1 → Y j−1 problem. The cut-and-paste argument applies. Thus, assuming that the last operation was a copy, we have c[i, j] = c[i − 1, j − 1] + cost copy . If it was a replace, then we must have had x[i]  cid:19 = y[ j].  Here, we assume that we cannot replace a character with itself. It is a straightforward modi cid:222 cation if we allow replacement of a character with itself.  We have the same optimal substructure argument as for copy, and assuming that the last operation was a replace, we have c[i, j] = c[i − 1, j − 1] + cost replace . If it was a twiddle, then we must have had x[i] = y[ j − 1] and x[i − 1] = y[ j], along with the implicit assumption that i, j ≥ 2. Now our subproblem is Xi−2 → Y j−2 and, assuming that the last operation was a twiddle, we have c[i, j] = c[i − 2, j − 2] + cost twiddle . If it was a delete, then we have no restrictions on x or y. Since we can view delete as removing a character from Xi and leaving Yj alone, our subproblem is Xi−1 → Y j . Assuming that the last operation was a delete, we have c[i, j] = c[i − 1, j] + cost delete . If it was an insert, then we have no restrictions on x or y. Our subproblem is Xi → Y j−1. Assuming that the last operation was an insert, we have c[i, j] = c[i, j − 1] + cost insert .               15-28  Solutions for Chapter 15: Dynamic Programming     If it was a kill, then we had to have completed converting Xm to Yn, so that the current problem must be the Xm → Yn problem. In other words, we must have i = m and j = n. If we think of a kill as a multiple delete, we can get any Xi → Yn, where 0 ≤ i < m, as a subproblem. We pick the best one, and so assuming that the last operation was a kill, we have c[m, n] = min 0≤i<m  {c[i, n]} + cost kill  .  We have not handled the base cases, in which i = 0 or j = 0. These are easy. X0 and Y0 are the empty strings. We convert an empty string into Yj by a sequence of j inserts, so that c[0, j] = j · cost insert . Similarly, we convert Xi into Y0 by a sequence of i deletes, so that c[i, 0] = i · cost delete . When i = j = 0, either formula gives us c[0, 0] = 0, which makes sense, since there cid:146 s no cost to convert the empty string to the empty string. For i, j > 0, our recursive formulation for c[i, j] applies the above formulas in the situations in which they hold:  ⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩  c[i, j] = min  c[i − 1, j − 1] + cost copy  c[i − 1, j − 1] + cost replace  c[i − 2, j − 2] + cost twiddle   c[i − 1, j] + cost delete  c[i, j] = c[i, j − 1] + cost insert  {c[i, n]} + cost kill  min 0≤i<m  if x[i] = y[ j] , if x[i]  cid:19 = y[ j] , if i, j ≥ 2, x[i] = y[ j − 1], and x[i − 1] = y[ j] , always , always , if i = m and j = n .  Like we did for LCS, our pseudocode  cid:222 lls in the table in row-major order, i.e., row-by-row from top to bottom, and left to right within each row. Column- major order  column-by-column from left to right, and top to bottom within each column  would also work. Along with the c[i, j] table, we  cid:222 ll in the table op[i, j], holding which operation was used.   Solutions for Chapter 15: Dynamic Programming  15-29  EDIT-DISTANCE x, y, m, n  for i ← 0 to m  do c[i, 0] ← i · cost delete   op[i, 0] ← DELETE  for j ← 0 to n  do c[0, j] ← j · cost insert   op[0, j] ← INSERT  for i ← 1 to m  do for j ← 1 to n  do c[i, j] ← ∞ if x[i] = y[ j] then c[i, j] ← c[i − 1, j − 1] + cost copy  if x[i]  cid:19 = y[ j] and c[i − 1, j − 1] + cost replace  < c[i, j] then c[i, j] ← c[i − 1, j − 1] + cost replace  if i ≥ 2 and j ≥ 2 and x[i] = y[ j − 1] and  op[i, j] ← COPY  op[i, j] ← REPLACE by y[ j]  x[i − 1] = y[ j] and c[i − 2, j − 2] + cost twiddle  < c[i, j] then c[i, j] ← c[i − 2, j − 2] + cost twiddle  op[i, j] ← TWIDDLE if c[i − 1, j] + cost delete  < c[i, j] then c[i, j] ← c[i − 1, j] + cost delete  if c[i, j − 1] + cost insert  < c[i, j] then c[i, j] ← c[i, j − 1] + cost insert   op[i, j] ← DELETE  op[i, j] ← INSERT y[ j]   for i ← 0 to m − 1  do if c[i, n] + cost kill  < c[m, n]  then c[m, n] ← c[i, n] + cost kill   op[m, n] ← KILL i  return c and op  The time and space are both  cid:2  mn . If we store a KILL operation in op[m, n], we also include the index i after which we killed, to help us reconstruct the optimal sequence of operations.  We don cid:146 t need to store y[i] in the op table for replace or insert operations.  To reconstruct this sequence, we use the op table returned by EDIT-DISTANCE. The procedure OP-SEQUENCE op, i, j   reconstructs the optimal operation se- quence that we found to transform Xi into Y j . The base case is when i = j = 0. The  cid:222 rst call is OP-SEQUENCE op, m, n .   15-30  Solutions for Chapter 15: Dynamic Programming  OP-SEQUENCE op, i, j   if i = 0 and j = 0 then return if op[i, j] = COPY or op[i, j] = REPLACE  cid:4  ← i − 1 then i  cid:4  ← j − 1 j elseif op[i, j] = TWIDDLE  cid:4  ← i − 2 then i  cid:4  ← j − 2 j elseif op[i, j] = DELETE  cid:4  ← i − 1 then i  cid:4  ← j j elseif op[i, j] = INSERT  cid:4  ← i then i  cid:4  ← j − 1 j let op[i, j] = KILLk  cid:4  ← k i  cid:4  ← j j  else  OP-SEQUENCE op, i print op[i, j]   cid:4 , j   cid:4     cid:1  Don cid:146 t care yet what character is inserted.   cid:1  Must be KILL, and must have i = m and j = n.  This procedure determines which subproblem we used, recurses on it, and then prints its own last operation.  b. The DNA-alignment problem is just the edit-distance problem, with  = −1 , cost copy  cost replace  = +1 , cost delete  = +2 , = +2 , cost insert  and the twiddle and kill operations are not permitted. The score that we are trying to maximize in the DNA-alignment problem is precisely the negative of the cost we are trying to minimize in the edit-distance problem. The negative cost of copy is not an impediment, since we can only apply the copy operation when the characters are equal.  Solution to Problem 15-6  Denote each square by the pair  i, j  , where i is the row number, j is the column number, and 1 ≤ i, j ≤ n. Our goal is to  cid:222 nd a most pro cid:222 table way from any square in row 1 to any square in row n. Once we do so, we can look up all the most pro cid:222 table ways to get to any square in row n and pick the best one. A subproblem is the most pro cid:222 table way to get from some square in row 1 to a particular square  i, j  . We have optimal substructure as follows. Consider a subproblem for  i, j  , where i > 1, and consider the most pro cid:222 table way to  i, j  .   Solutions for Chapter 15: Dynamic Programming  15-31   cid:4   dollars, where d   cid:4   that earns d  cid:4   on our way to  i, j  , earning d  Because of how we de cid:222 ne legal moves, it must be through square  i − 1, j  cid:4  , where  cid:4  = j − 1, j, or j + 1. Then the way that we got to  i − 1, j  cid:4   within the most j pro cid:222 table way to  i, j   must itself be a most pro cid:222 table way to  i −1, j  cid:4  . The usual cut-and-paste argument applies. Suppose that in our most pro cid:222 table way to  i, j  ,  cid:4  , we earn a pro cid:222 t of d dollars to get to  i − 1, j which goes through  i − 1, j  cid:4  , and then earn p  i − 1, j  cid:4  ,  i, j    dollars getting from  i − 1, j  cid:4   to  i, j  ; thus, we earn d + p  i − 1, j  cid:4  ,  i, j    dollars getting to  i, j  . Now suppose that there cid:146 s a way to  i − 1, j  cid:4  > d. Then we would use that to get to  i − 1, j  cid:4  ,  i, j    > d + p  i − 1, j  cid:4  ,  i, j   , and thus contradicting the optimality of our way to  i, j  . We also have overlapping subproblems. We need the most pro cid:222 table way to  i, j   to  cid:222 nd the most pro cid:222 table way to  i + 1, j − 1 , to  i + 1, j  , and to  i + 1, j + 1 . So we cid:146 ll need to directly refer to the most pro cid:222 table way to  i, j   up to three times, and if we were to implement this algorithm recursively, we cid:146 d be solving each sub- problem many times. Let d[i, j] be the pro cid:222 t we earn in the most pro cid:222 table way to  i, j  . Then we have that d[1, j] = 0 for all j = 1, 2, . . . , n. For i = 2, 3, . . . , n, we have if j > 1 , d[i, j] = max always , if j < n .  ⎧⎨⎩d[i − 1, j − 1] + p  i − 1, j − 1 ,  i, j     d[i − 1, j] + p  i − 1, j  ,  i, j    d[i − 1, j + 1] + p  i − 1, j + 1 ,  i, j      cid:4  + p  i − 1, j  To keep track of how we got to  i, j   most pro cid:222 tably, we let w[i, j] be the value of j used to achieve the maximum value of d[i, j]. These values are de cid:222 ned for 2 ≤ i ≤ n and 1 ≤ j ≤ n. Thus, we can run the following procedure:  CHECKERBOARD n, p  for j ← 1 to n for i ← 2 to n  do d[1, j] ← 0 do for j ← 1 to n  do d[i, j] ← −∞  w[i, j] ← j − 1  if j > 1 then d[i, j] ← d[i − 1, j − 1] + p  i − 1, j − 1 ,  i, j    if d[i − 1, j] + p  i − 1, j  ,  i, j    > d[i, j] then d[i, j] ← d[i − 1, j] + p  i − 1, j  ,  i, j    if j   d[i, j] then d[i, j] ← d[i − 1, j + 1] + p  i − 1, j + 1 ,  i, j     w[i, j] ← j  w[i, j] ← j + 1  return d and w  Once we  cid:222 ll in the d[i, j] table, the pro cid:222 t earned by the most pro cid:222 table way to any square along the top row is max1≤ j≤n {d[n, j]}. To actually compute the set of moves, we use the usual recursive backtracking method. This procedure prints the squares visited, from row 1 to row n:   15-32  Solutions for Chapter 15: Dynamic Programming  PRINT-MOVES w, i, j   if i > 1 then PRINT-MOVES w, i − 1, w[i, j]  print  cid:147   cid:148  i  cid:147 , cid:148  j  cid:147   cid:148  Letting t = max1≤ j≤n {d[n, j]}, the initial call is PRINT-MOVES w, n, t . The time to run CHECKERBOARD is clearly  cid:2  n2 . Once we have computed the d and w tables, PRINT-MOVES runs in  cid:2  n  time, which we can see by observing that i = n in the initial call and i decreases by 1 in each recursive call.   Lecture Notes for Chapter 16: Greedy Algorithms  Chapter 16 Introduction  Similar to dynamic programming. Used for optimization problems.  Idea: When we have a choice to make, make the one that looks best right now. Make a locally optimal choice in hope of getting a globally optimal solution. Greedy algorithms don cid:146 t always yield an optimal solution. But sometimes they do. We cid:146 ll see a problem for which they do. Then we cid:146 ll look at some general characteristics of when greedy algorithms give optimal solutions. [WedonotcoverHuffmancodesormatroidsinthesenotes.]  Activity selection  n activities require exclusive use of a common resource. For example, scheduling the use of a classroom. Set of activities S = {a1, . . . , an}. ai needs resource during period [si , fi  , which is a half-open interval, where si = start time and fi =  cid:222 nish time. Goal: Select the largest possible set of nonoverlapping  mutually compatible  ac- tivities.  Note: Could have many other objectives:   Schedule room for longest time.   Maximize income rental fees.  Example: S sorted by  cid:222 nish time: [Leaveonboard]  i si fi  1 2 3 4 5 8 9 1 2 4 1 5 11 13 3 5 7 8 9 10 11 14 16  7 9  6 8   16-2  Lecture Notes for Chapter 16: Greedy Algorithms  a5  a4  a2  a1  a3  a6  a8  a7  a9  0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  Maximum-size mutually compatible set: {a1, a3, a6, a8}. Not unique: also {a2, a5, a7, a9}.  Optimal substructure of activity selection Si j = {ak ∈ S : fi ≤ sk < fk ≤ s j}  = activities that start after ai  cid:222 nishes and  cid:222 nish before aj starts .  [Leaveonboard]  . . .  fi  sk  fk  sj  ai  ak  aj  . . .  Activities in Si j are compatible with   all activities that  cid:222 nish by fi, and   all activities that start no earlier than sj .  To represent the entire problem, add  cid:222 ctitious activities: a0 = [−∞, 0  an+1 = [∞,  cid:147 ∞ + 1 cid:148   We don cid:146 t care about −∞ in a0 or  cid:147 ∞ + 1 cid:148  in an+1. Then S = S0,n+1. Range for Si j is 0 ≤ i, j ≤ n + 1. Assume that activities are sorted by monotonically increasing  cid:222 nish time: f0 ≤ f1 ≤ f2 ≤ ··· ≤ fn < fn+1 . Then i ≥ j ⇒ Si j = ∅.  [Leaveonboard]     If there exists ak ∈ Si j : fi ≤ sk < fk ≤ s j < f j ⇒ fi < f j .   But i ≥ j ⇒ fi ≥ f j . Contradiction. So only need to worry about Si j with 0 ≤ i < j ≤ n + 1. All other Si j are ∅. Suppose that a solution to Si j includes ak. Have 2 subproblems:      Sik  start after ai  cid:222 nishes,  cid:222 nish before ak starts  Skj  start after ak  cid:222 nishes,  cid:222 nish before aj starts    Lecture Notes for Chapter 16: Greedy Algorithms  16-3  Solution to Si j is  solution to Sik  ∪ {ak} ∪  solution to Skj  . Since ak is in neither subproblem, and the subproblems are disjoint, solution to S = solution to Sik + 1 + solution to Skj . If an optimal solution to Si j includes ak, then the solutions to Sik and Skj used within this solution must be optimal as well. Use the usual cut-and-paste argument. Let Ai j = optimal solution to Si j . So Ai j = Aik ∪ {ak} ∪ Akj [leaveonboard] , assuming:  Si j is nonempty, and      we know ak.  Recursive solution to activity selection c[i, j] = size of maximum-size subset of mutually compatible activities in Si j .     i ≥ j ⇒ Si j = ∅ ⇒ c[i, j] = 0.  cid:19 = ∅, suppose we know that ak is in the subset. Then  If Si j c[i, j] = c[i, k] + 1 + c[k, j] . But of course we don cid:146 t know which k to use, and so if Si j = ∅ ,  cid:19 = ∅ . if Si j  {c[i, k] + c[k, j] + 1}  c[i, j] =  ⎧⎨⎩0  max i<k< j ak∈Si j  [The  cid:222 rst two printings of the book omit the requirement that ak ∈ Si j from this max computation. Thiserrorwascorrectedinthethirdprinting.] Why this range of k? Because Si j = {ak ∈ S : fi ≤ sk < fk ≤ s j} ⇒ ak can cid:146 t be ai or a j . Also need to ensure that ak is actually in Si j , since i < k < j is not suf cid:222 cient on its own to ensure this. From here, we could continue treating this like a dynamic-programming problem. We can simplify our lives, however.   cid:19 = ∅, and let am be the activity in Si j with the earliest  cid:222 nish time:  Theorem Let Si j min{ fk : ak ∈ Si j}. Then: 1. am is used in some maximum-size subset of mutually compatible activities 2. Sim = ∅, so that choosing am leaves Sm j as the only nonempty subproblem.  fm =  of Si j .   16-4  Lecture Notes for Chapter 16: Greedy Algorithms  Proof 2. Suppose there is some ak ∈ Sim. Then fi ≤ sk < fk ≤ sm < fm ⇒ fk < fm. Then ak ∈ Si j and it has an earlier  cid:222 nish time than fm, which contradicts our choice of am. Therefore, there is no ak ∈ Sim ⇒ Sim = ∅.  1. Let Ai j be a maximum-size subset of mutually compatible activities in Si j .  Order activites in Ai j in monotonically increasing order of  cid:222 nish time. Let ak be the  cid:222 rst activity in Ai j . If ak = am, done  am is used in a maximum-size subset .  cid:4  Otherwise, construct A i j  = Ai j − {ak} ∪ {am}  replace ak by am .  Claim  cid:4  i j are disjoint. Activities in A Proof Activities in Ai j are disjoint, ak is the  cid:222 rst activity in Ai j fm ≤ fk  so am doesn cid:146 t overlap anything else in A  cid:4  i j  .  %% = Ai j and Ai j is a maximum-size subset, so is A  %%A  Since   cid:4  i j .   cid:4  i j  to  cid:222 nish,  claim    theorem   This is great:   of subproblems in optimal solution  of choices to consider  before theorem j − i − 1  2  after theorem  1 1  Now we can solve top down:   To solve a problem Si j ,    Choose am ∈ Si j with earliest  cid:222 nish time: the greedy choice.   Then solve Sm j.  What are the subproblems?   Original problem is S0,n+1.   Suppose our  cid:222 rst choice is am1.   Then next subproblem is Sm1,n+1.   Suppose next choice is am2.   Next subproblem is Sm2,n+1.   And so on. Each subproblem is Smi ,n+1, i.e., the last activities to  cid:222 nish. And the subproblems chosen have  cid:222 nish times that increase. Therefore, we can consider each activity just once, in monotonically increasing order of  cid:222 nish time.   Lecture Notes for Chapter 16: Greedy Algorithms  16-5  Easy recursive algorithm: Assumes activites already sorted by monotonically in- creasing  cid:222 nish time.  If not, then sort in O n lg n  time.  Return an optimal solu- tion for Si,n+1: [The cid:222 rsttwoprintingshadaprocedurethatpurportedtoreturnanoptimalsolution for Si j,where j > i. Thisprocedurehadanerror: itworkedonlywhen j = n + 1. Itturnsoutthatitwascalledonlywith j = n + 1,however. Toavoidthisproblem altogether,theprocedurewaschangedtothefollowinginthethirdprinting.]  REC-ACTIVITY-SELECTOR s, f, i, n  m ← i + 1 while m ≤ n and sm < fi do m ← m + 1 if m ≤ n then return {am} ∪ REC-ACTIVITY-SELECTOR s, f, m, n  else return ∅   cid:1  Find  cid:222 rst activity in Si,n+1.  Initial call: REC-ACTIVITY-SELECTOR s, f, 0, n .     Idea: The while loop checks ai+1, ai+2, . . . , an until it  cid:222 nds an activity am that is compatible with ai  need sm ≥ fi . If the loop terminates because am is found  m ≤ n , then recursively solve Sm,n+1, and return this solution, along with am. If the loop never  cid:222 nds a compatible am  m > n , then just return empty set.     Go through example given earlier. Should get{a1, a4, a8, a11}. Time:  cid:2  n  cid:151 each activity examined exactly once. Can make this iterative. It cid:146 s already almost tail recursive.  GREEDY-ACTIVITY-SELECTOR s, f, n  A ← {a1} i ← 1 for m ← 2 to n do if sm ≥ fi  then A ← A ∪ {am}  i ← m   cid:1  ai is most recent addition to A  return A Go through example given earlier. Should again get{a1, a4, a8, a11}. Time:  cid:2  n .  Greedy strategy  The choice that seems best at the moment is the one we go with. What did we do for activity selection?   16-6  Lecture Notes for Chapter 16: Greedy Algorithms  1. Determine the optimal substructure. 2. Develop a recursive solution. 3. Prove that at any stage of recursion, one of the optimal choices is the greedy  choice. Therefore, it cid:146 s always safe to make the greedy choice.  4. Show that all but one of the subproblems resulting from the greedy choice are  empty.  5. Develop a recursive greedy algorithm. 6. Convert it to an iterative algorithm.  At  cid:222 rst, it looked like dynamic programming. Typically, we streamline these steps. Develop the substructure with an eye toward   making the greedy choice,    leaving just one subproblem. For activity selection, we showed that the greedy choice implied that in Si j , only i varied, and j was  cid:222 xed at n + 1. We could have started out with a greedy algorithm in mind:   De cid:222 ne Si = {ak ∈ S : fi ≤ sk}.   Then show that the greedy choice cid:151  cid:222 rst am to  cid:222 nish in Si cid:151 combined with  optimal solution to Sm ⇒ optimal solution to Si.  Typical streamlined steps:  1. Cast the optimization problem as one in which we make a choice and are left  with one subproblem to solve.  2. Prove that there cid:146 s always an optimal solution that makes the greedy choice, so 3. Show that greedy choice and optimal solution to subproblem ⇒ optimal solu-  that the greedy choice is always safe.  tion to the problem.  No general way to tell if a greedy algorithm is optimal, but two key ingredients are  1. greedy-choice property and 2. optimal substructure.  Greedy-choice property  A globally optimal solution can be arrived at by making a locally optimal  greedy  choice.  Dynamic programming:   Make a choice at each step.   Choice depends on knowing optimal solutions to subproblems. Solve subprob-  lems  cid:222 rst.    Solve bottom-up.   Lecture Notes for Chapter 16: Greedy Algorithms  16-7  Greedy:   Make a choice at each step.   Make the choice before solving the subproblems.   Solve top-down. Typically show the greedy-choice property by what we did for activity selection:   Look at a globally optimal solution.   If it includes the greedy choice, done.   Else, modify it to include the greedy choice, yielding another solution that cid:146 s  just as good.  Can get ef cid:222 ciency gains from greedy-choice property.   Preprocess input to put it into greedy order.   Or, if dynamic data, use a priority queue.  Optimal substructure Just show that optimal solution to subproblem and greedy choice ⇒ optimal solu- tion to problem.  Greedy vs. dynamic programming  The knapsack problem is a good example of the difference.  0-1 knapsack problem:      n items. Item i is worth $vi , weighs wi pounds.    Find a most valuable subset of items with total weight ≤ W .   Have to either take an item or not take it cid:151 can cid:146 t take part of it.  Fractional knapsack problem: Like the 0-1 knapsack problem, but can take frac- tion of an item. Both have optimal substructure. But the fractional knapsack problem has the greedy-choice property, and the 0-1 knapsack problem does not. To solve the fractional problem, rank items by value weight: vi  wi. Let vi  wi ≥ vi+1 wi+1 for all i. FRACTIONAL-KNAPSACK v, w, W   load ← 0 i ← 1 while load < W and i ≤ n do if wi ≤ W − load  then take all of item i else take  W − load  wi of item i add what was taken to load i ← i + 1   16-8  Lecture Notes for Chapter 16: Greedy Algorithms  Time: O n lg n  to sort, O n  thereafter. Greedy doesn cid:146 t work for the 0-1 knapsack problem. Might get empty space, which lowers the average value per pound of the items taken.  2  20 5  1 3 i vi 60 100 120 wi 10 30 vi  wi 6 4 W = 50. Greedy solution:   Take items 1 and 2.    value = 160, weight = 30.  Have 20 pounds of capacity left over. Optimal solution:   Take items 2 and 3.    value = 220, weight = 50.  No leftover capacity.   Solutions for Chapter 16: Greedy Algorithms  Solution to Exercise 16.1-2   cid:4  = {a  The proposed approach cid:151 selecting the last activity to start that is compatible with all previously selected activities cid:151 is really the greedy algorithm but starting from the end rather than the beginning. Another way to look at it is as follows. We are given a set S = {a1, a2, . . . , an} of activities, where ai = [si , fi  , and we propose to  cid:222 nd an optimal solution by selecting the last activity to start that is compatible with all previously selected = [ fi , si  .  cid:4   cid:4  , . . . , a , a activities. Instead, let us create a set S i is ai in reverse. Clearly, a subset of {ai1 } ⊆ S is mutually  cid:4  n 2 , ai2 , . . . , aik That is, a compatible if and only if the corresponding subset{a  cid:4   cid:4   cid:4  , a is also i2 i1 mutually compatible. Thus, an optimal solution for S maps directly to an optimal solution for S The proposed approach of selecting the last activity to start that is compatible with all previously selected activities, when run on S, gives the same answer as the greedy algorithm from the text cid:151 selecting the  cid:222 rst activity to  cid:222 nish that is com-  cid:4  . The solution that patible with all previously selected activities cid:151 when run on S the proposed approach  cid:222 nds for S corresponds to the solution that the text cid:146 s greedy algorithm  cid:222 nds for S   cid:4  i } ⊆ S  , and so it is optimal.  }, where a   cid:4  , . . . , a ik  and vice versa.   cid:4  1   cid:4    cid:4   Solution to Exercise 16.1-3  Let S be the set of n activities. The  cid:147 obvious cid:148  solution of using GREEDY-ACTIVITY-SELECTOR to  cid:222 nd a maxi- mum-size set S1 of compatible activities from S for the  cid:222 rst lecture hall, then using it again to  cid:222 nd a maximum-size set S2 of compatible activities from S − S1 for the second hall,  and so on until all the activities are assigned , requires  cid:2  n2  time in the worst case. There is a better algorithm, however, whose asymptotic time is just the time needed to sort the activities by time cid:151 O n lg n  time for arbitrary times, or possibly as fast as O n  if the times are small integers. The general idea is to go through the activities in order of start time, assigning each to any hall that is available at that time. To do this, move through the set   16-10  Solutions for Chapter 16: Greedy Algorithms  of events consisting of activities starting and activities  cid:222 nishing, in order of event time. Maintain two lists of lecture halls: Halls that are busy at the current event- time t  because they have been assigned an activity i that started at si ≤ t but fi > t  and halls that are free at time t.  As in the activity- won cid:146 t  cid:222 nish until selection problem in Section 16.1, we are assuming that activity time intervals are half open cid:151 i.e., that if si ≥ f j, then activities i and j are compatible.  When t is the start time of some activity, assign that activity to a free hall and move the hall from the free list to the busy list. When t is the  cid:222 nish time of some activity, move the activity cid:146 s hall from the busy list to the free list.  The activity is certainly in some hall, because the event times are processed in order and the activity must have started before its  cid:222 nish time t, hence must have been assigned to a hall.  To avoid using more halls than necessary, always pick a hall that has already had an activity assigned to it, if possible, before picking a never-used hall.  This can be done by always working at the front of the free-halls list cid:151 putting freed halls onto the front of the list and taking halls from the front of the list cid:151 so that a new hall doesn cid:146 t come to the front and get chosen if there are previously-used halls.  This guarantees that the algorithm uses as few lecture halls as possible: The algo- rithm will terminate with a schedule requiring m ≤ n lecture halls. Let activity i be the  cid:222 rst activity scheduled in lecture hall m. The reason that i was put in the mth lecture hall is that the  cid:222 rst m − 1 lecture halls were busy at time si . So at this time there are m activities occurring simultaneously. Therefore any schedule must use at least m lecture halls, so the schedule returned by the algorithm is optimal. Run time:   Sort the 2n activity-starts activity-ends events.  In the sorted order, an activity- ending event should precede an activity-starting event that is at the same time.  O n lg n  time for arbitrary times, possibly O n  if the times are restricted  e.g., to small integers .    Process the events in O n  time: Scan the 2n events, doing O 1  work for each  moving a hall from one list to the other and possibly associating an activity with it .  Total: O n + time to sort  [The idea of this algorithm is related to the rectangle-overlap algorithm in Exer- cise14.3-7.]  Solution to Exercise 16.1-4    For the approach of selecting the activity of least duration from those that are  compatible with previously selected activities:  2 2 4 2  1 0 3 3  i si fi duration This approach selects just{a2}, but the optimal solution selects{a1, a3}.  3 3 6 3   Solution to Exercise 16.2-2  Solutions for Chapter 16: Greedy Algorithms  16-11    For the approach of always selecting the compatible activity that overlaps the  fewest other remaining activities:  i si fi  of overlapping activities  1 0 2 3  2 1 3 4  3 1 3 4  4 1 3 4  5 2 4 4  6 3 5 2  7 4 6 4  8 5 7 4  9 5 7 4  10 5 7 4  11 6 8 3  This approach  cid:222 rst selects a6, and after that choice it can select only two other activities  one of a1, a2, a3, a4 and one of a8, a9, a10, a11 . An optimal solution is {a1, a5, a7, a11}.    For the approach of always selecting the compatible remaining activity with the earliest start time, just add one more activity with the interval [0, 14  to the example in Section 16.1. It will be the  cid:222 rst activity selected, and no other activities are compatible with it.  The solution is based on the optimal-substructure observation in the text: Let i be the highest-numbered item in an optimal solution S for W pounds and items  cid:4  = S − {i} must be an optimal solution for W − wi pounds 1, . . . , n. Then S and items 1, . . . , i − 1, and the value of the solution S is vi plus the value of the  cid:4  subproblem solution S We can express this relationship in the following formula: De cid:222 ne c[i, w] to be the value of the solution for items 1, . . . , i and maximum weight w. Then  .  c[i, w] =  c[i − 1, w] max vi + c[i − 1, w − wi], c[i − 1, w]   if i = 0 or w = 0 , if wi > w , if i > 0 and w ≥ wi .  ⎧⎨⎩0  The last case says that the value of a solution for i items either includes item i, in which case it is vi plus a subproblem solution for i − 1 items and the weight excluding wi, or doesn cid:146 t include item i, in which case it is a subproblem solution for i − 1 items and the same weight. That is, if the thief picks item i, he takes vi value, and he can choose from items 1, . . . , i − 1 up to the weight limit w − wi, and get c[i − 1, w − wi] additional value. On the other hand, if he decides not to take item i, he can choose from items 1, . . . , i − 1 up to the weight limit w, and get c[i − 1, w] value. The better of these two choices should be made. The algorithm takes as inputs the maximum weight W , the number of items n, and the two sequences v =  cid:2 v1, v2, . . . , vn cid:3  and w =  cid:2 w1, w2, . . . , wn cid:3 . It stores the c[i, j] values in a table c[0 . . n, 0 . . W ] whose entries are computed in row-major order.  That is, the  cid:222 rst row of c is  cid:222 lled in from left to right, then the second row, and so on.  At the end of the computation, c[n, W ] contains the maximum value the thief can take.   16-12  Solutions for Chapter 16: Greedy Algorithms  DYNAMIC-0-1-KNAPSACK v, w, n, W   for w ← 0 to W for i ← 1 to n  do c[0, w] ← 0 do c[i, 0] ← 0  for w ← 1 to W do if wi ≤ w  then if vi + c[i − 1, w − wi ] > c[i − 1, w] then c[i, w] ← vi + c[i − 1, w − wi] else c[i, w] ← c[i − 1, w]  else c[i, w] ← c[i − 1, w]  The set of items to take can be deduced from the c table by starting at c[n, W ] and tracing where the optimal values came from. If c[i, w] = c[i − 1, w], then item i is not part of the solution, and we continue tracing with c[i − 1, w]. Otherwise item i is part of the solution, and we continue tracing with c[i − 1, w − wi ]. The above algorithm takes  cid:2  nW   time total:    cid:2  nW   to  cid:222 ll in the c table:  n + 1 ·  W + 1  entries, each requiring  cid:2  1  time    O n  time to trace the solution  since it starts in row n of the table and moves  to compute.  up one row at each step .  The optimal strategy is the obvious greedy one. Starting will a full tank of gas, Professor Midas should go to the farthest gas station he can get to within n miles of Newark. Fill up there. Then go to the farthest gas station he can get to within n miles of where he  cid:222 lled up, and  cid:222 ll up there, and so on. Looked at another way, at each gas station, Professor Midas should check whether he can make it to the next gas station without stopping at this one. If he can, skip this one. If he cannot, then  cid:222 ll up. Professor Midas doesn cid:146 t need to know how much gas he has or how far the next station is to implement this approach, since at each  cid:222 llup, he can determine which is the next station at which he cid:146 ll need to stop. This problem has optimal substructure. Suppose there are m possible gas stations. Consider an optimal solution with s stations and whose  cid:222 rst stop is at the kth gas station. Then the rest of the optimal solution must be an optimal solution to the subproblem of the remaining m − k stations. Otherwise, if there were a better solution to the subproblem, i.e., one with fewer than s − 1 stops, we could use it to come up with a solution with fewer than s stops for the full problem, contradicting our supposition of optimality. This problem also has the greedy-choice property. Suppose there are k gas stations beyond the start that are within n miles of the start. The greedy solution chooses the kth station as its  cid:222 rst stop. No station beyond the kth works as a  cid:222 rst stop, since Professor Midas runs out of gas  cid:222 rst. If a solution chooses a station j < k as  Solution to Exercise 16.2-4   Solution to Exercise 16.2-6  Solutions for Chapter 16: Greedy Algorithms  16-13  its  cid:222 rst stop, then Professor Midas could choose the kth station instead, having at least as much gas when he leaves the kth station as if he cid:146 d chosen the jth station. Therefore, he would get at least as far without  cid:222 lling up again if he had chosen the kth station. If there are m gas stations on the map, Midas needs to inspect each one just once. The running time is O m .  i∈G respectively.    Use a linear-time median algorithm to calculate the median m of the vi  wi ra- tios. Next, partition the items into three sets: G = {i : vi  wi > m}, E = {i : vi  wi = m}, and L = {i : vi  wi < m}; this step takes linear time. Compute wi, the total weight of the items in sets G and E,  wi and WE = cid:1   WG = cid:1   i∈E     If WG > W , then do not yet take any items in set G, and instead recurse on the set of items G and knapsack capacity W .   Otherwise  WG ≤ W  , take all items in set G, and take as much of the items in set E as will  cid:222 t in the remaining capacity W − WG. If WG + WE ≥ W  i.e., there is no capacity left after taking all the items in set G and all the items in set E that  cid:222 t in the remaining capacity W − WG , then we are done.   Otherwise  WG + WE < W  , then after taking all the items in sets G and E, recurse on the set of items L and knapsack capacity W − WG − WE .  To analyze this algorithm, note that each recursive call takes linear time, exclusive of the time for a recursive call that it may make. When there is a recursive call, there is just one, and it cid:146 s for a problem of at most half the size. Thus, the running time is given by the recurrence T  n  ≤ T  n 2  +  cid:2  n , whose solution is T  n  = O n .  Solution to Exercise 16.2-7  bi a j  b j ≥ ai  Sort A and B into monotonically decreasing order. Here cid:146 s a proof that this method yields an optimal solution. Consider any indices i b j . We want to show that and j such that i < j, and consider the terms ai bi , i.e., it is no worse to include these terms in the payoff than to include ai bi . Since A and B are sorted into monotonically decreasing that ai order and i < j, we have ai ≥ a j and bi ≥ b j . Since ai and a j are positive and bi − b j is nonnegative, we have ai bi−b j . Multiplying both sides by b j a j ai Since the order of multiplication doesn cid:146 t matter, sorting A and B into monotoni- cally increasing order works as well.  bi−b j ≥ a j  b j ≥ ai  b j yields ai  b j and a j  bi and a j  b j a j  b j a j  bi a j  bi .   16-14  Solutions for Chapter 16: Greedy Algorithms  Solution to Exercise 16.4-2  We need to show three things to prove that  S, I  is a matroid:  1. S is  cid:222 nite. That cid:146 s because S is the set of of m columns of matrix T . 2. I is hereditary. That cid:146 s because if B ∈ I, then the columns in B are linearly in- dependent. If A ⊆ B, then the columns of A must also be linearly independent, and so A ∈ I. 3.  S, I  satis cid:222 es the exchange property. To see why, let us suppose that A, B ∈ I and A < B. We will use the following properties of matrices:   The rank of a matrix is the number of columns in a maximal set of linearly independent columns  see page 731 of the text . The rank is also equal to the dimension of the column space of the matrix. If the column space of matrix B is a subspace of the column space of ma- trix A, then rank B  ≤ rank A .     Because the columns in A are linearly independent, if we take just these columns as a matrix A, we have that rank A  = A. Similarly, if we take the columns of B as a matrix B, we have rank B  = B. Since A < B, we have rank A  < rank B . We shall show that there is some column b ∈ B that is not a linear combination of the columns in A, and so A∪{b} is linearly independent. The proof proceeds by contradiction. Assume that each column in B is a linear combination of the columns of A. That means that any vector that is a linear combination of the columns of B is also a linear combination of the columns of A, and so, treating the columns of A and B as matrices, the column space of B is a subspace of the column space of A. By the second property above, we have rank B  ≤ rank A . But we have already shown that rank A  < rank B , a contradiction. Therefore, some column in B is not a linear combination of the columns of A, and  S, I  satis cid:222 es the exchange property.  Solution to Exercise 16.4-3  We need to show three things to prove that  S, I cid:4   is a matroid: 1. S is  cid:222 nite. We are given that. 2. I cid:4    cid:4  ⊆ B and A , there is some maximal set B ∈ I such that B ⊆ S − B  cid:4  implies that  cid:4  ⊆ S − A S − B  cid:4  . Thus, there exists a maximal  cid:4  ∈ I cid:4  set B ∈ I such that B ⊆ S − A  cid:4  , proving that A 3.  S, I cid:4   satis cid:222 es the exchange property. We start with two preliminary facts   cid:4  ∈ I cid:4  , and so B ⊆ S − B  is hereditary. Suppose that B  . Since B  cid:4  ⊆ B   cid:4  ⊆ S − A  cid:4    cid:4  ∈ I cid:4   cid:4   . But A  .   cid:4   about sets. The proofs of these facts are omitted.   Solutions for Chapter 16: Greedy Algorithms  16-15  Fact 1: X − Y = X − X ∩ Y. Fact 2: Let S be the universe of elements. If X − Y ⊆ Z and Z ⊆ S − Y , then X ∩ Z = X − X ∩ Y.  cid:4  ∈ I cid:4  To show that  S, I cid:4   satis cid:222 es the exchange property, let us assume that A , , and that A  cid:4 . We need to show that there exists some x ∈  cid:4  < B  cid:4  ∈ I cid:4  B such that A∪{x} ∈ I cid:4   cid:4 − A  cid:4  , there are maximal . Because A B sets A ⊆ S − A  cid:4   cid:4  − A, so that X consists of elements in B but not in De cid:222 ne the set X = B  cid:4  A If X is nonempty, then let x be any element of X. By how we de cid:222 ned set X, we . Since x  cid:19 ∈ A, we also have and x  cid:19 ∈ A know that x ∈ B  cid:4   cid:4   cid:4  − {x} = S −  A ∪ {x} , and so A ∪ {x} ∈ I cid:4  that A ⊆ S − A  cid:4 , we have If X is empty, the situation is more complicated. BecauseA  cid:4  − A that B   cid:4  ∈ I cid:4  such that A ∈ I and B ∈ I.   cid:4   cid:19 = ∅, and so X being empty means that B  and B ⊆ S − B  , so that x ∈ B  .  cid:4  < B  cid:4  ⊆ A.   cid:4  − A   cid:4  − A   cid:4  − A   cid:4  ∈ I cid:4   and B  or A.   cid:4    cid:4   such that  A − B   cid:4   ∪ {y} ∈ I.   cid:4  ⊆ B and B ∈ I, and so B − A   cid:4  ⊆ A and A ∈ I, we have that A− B   cid:4  ∈ If we show  cid:4 , the assumption that  S, I  is a matroid proves the   cid:4  ∈ I.   cid:4 . We claim that B ∩ A  cid:4  ∩  S − B  cid:4  = A  cid:4  ∩  S − B  cid:4   = A  cid:4  = A  cid:4  − B  cid:4  ∩ B  , we can apply Fact 2 to conclude  cid:4 . To , and so  cid:4  . Applying Fact 1, we  cid:4 , and hence B ∩ A  cid:4  ≤   cid:4   and B ⊆ S − B  cid:4  − B  cid:4  ∩ A   cid:4  ≤ A   cid:4  − B  cid:4    cid:4  ⊆ A and A ⊆ S − A  cid:4   Claim There is an element y ∈ B − A  cid:4  Proof First, observe that because A− B I. Similarly, B − A  cid:4  < B − A that A − B existence of y.  cid:4  − A Because B  cid:4  ∩ A = B  cid:4  − B that B see why, observe that A  cid:4  ⊆  S − B  cid:4   ∩ A B ∩ A  cid:4  = A see that A  cid:4  − B  cid:4 .  cid:4  − B A  cid:4  ∩ A Now, we haveA  cid:4   cid:4  < B  cid:4  − B  cid:4  < B  cid:4  − B A  cid:4  ∩ A  cid:4  − B  cid:4  < B B ∩ A  cid:4  ∩ A  cid:4  < B B ∩ A  cid:4  > A − B B − B ∩ A  cid:4  > A − B  cid:4  B − A   cid:4  ∩ A  cid:4  − B  cid:4  = A  cid:4  − A   cid:4   cid:4    cid:4  − B  cid:4  − B   cid:4  ∩ A  cid:4  ∩ A  cid:4  ∩ A   by assumption   subtracting same quantity   cid:4  ≤ A  B ∩ A  cid:4    cid:4  ∩ A  cid:4  ∩ A = B  B  cid:4    cid:4  ∩ A  A = B   claim   Fact 1   cid:4   ∪ {y} ∈ I. Now we know there is an element y ∈ B − A  cid:4  Moreover, we claim that y  cid:19 ∈ A. To see why, we know that by the exchange . In order for y to be in A, it would have to be in A ∩ B property, y  cid:19 ∈ A − B  cid:4  . . Therefore y  cid:19 ∈ A. But y ∈ B, which means that y  cid:19 ∈ B to A− B We keep applying the exchange property, adding elements in B − A  cid:4   cid:4  , maintaining that the set we get is in I. Continue adding these elements until we get a set, say C, such that C = A. Once C = A, there is some element x ∈ A that we have not added into C. We know this because the element y that we  cid:222 rst added into C was not in A, and so some element of A must be left over.  , and hence y  cid:19 ∈ A ∩ B  such that  A − B   cid:4    cid:4    cid:4    16-16  Solutions for Chapter 16: Greedy Algorithms  The set C is maximal, because it has the same cardinality as A, which is maxi- mal, and C ∈ I. Since C started with all elements in A− B  cid:4  and we added only elements in B − A  cid:4   cid:4  , at no time did C receive an element in A . Because we also  cid:4  ∪ {x} , which never added x to C, we have that C ⊆ S − A , as we needed to show. proves that A   cid:4  − {x} = S −  A   cid:4  ∪ {x} ∈ I cid:4   Solution to Problem 16-1  Before we go into the various parts of this problem, let us  cid:222 rst prove once and for all that the coin-changing problem has optimal substructure. Suppose we have an optimal solution for a problem of making change for n cents, and we know that this optimal solution uses a coin whose value is c cents; let this optimal solution use k coins. We claim that this optimal solution for the problem of n cents must contain within it an optimal solution for the problem of n− c cents. We use the usual cut-and-paste argument. Clearly, there are k − 1 coins in the solution to the n − c cents problem used within our optimal solution to the n cents problem. If we had a solution to the n − c cents problem that used fewer than k − 1 coins, then we could use this solution to produce a solution to the n cents problem that uses fewer than k coins, which contradicts the optimality of our solution.  a. A greedy algorithm to make change using quarters, dimes, nickels, and pennies works as follows:   Give q =  cid:9 n 25 cid:10  quarters. That leaves nq = n mod 25 cents to make change.   Then give d =  cid:9 nq  10 cid:10  dimes. That leaves nd = nq mod 10 cents to make change.   Then give k =  cid:9 nd  5 cid:10  nickels. That leaves nk = nd mod 5 cents to make change.   Finally, give p = nk pennies. An equivalent formulation is the following. The problem we wish to solve is making change for n cents. If n = 0, the optimal solution is to give no coins. If n > 0, determine the largest coin whose value is less than or equal to n. Let this coin have value c. Give one such coin, and then recursively solve the subproblem of making change for n − c cents. To prove that this algorithm yields an optimal solution, we  cid:222 rst need to show that the greedy-choice property holds, that is, that some optimal solution to making change for n cents includes one coin of value c, where c is the largest coin value such that c ≤ n. Consider some optimal solution. If this optimal solution includes a coin of value c, then we are done. Otherwise, this optimal solution does not include a coin of value c. We have four cases to consider: If 1 ≤ n < 5, then c = 1. A solution may consist only of pennies, and so it   must contain the greedy choice. If 5 ≤ n < 10, then c = 5. By supposition, this optimal solution does not contain a nickel, and so it consists of only pennies. Replace  cid:222 ve pennies by one nickel to give a solution with four fewer coins.      Solutions for Chapter 16: Greedy Algorithms  16-17        If 10 ≤ n < 25, then c = 10. By supposition, this optimal solution does not contain a dime, and so it contains only nickels and pennies. Some subset of the nickels and pennies in this solution adds up to 10 cents, and so we can replace these nickels and pennies by a dime to give a solution with  between 1 and 9  fewer coins. If 25 ≤ n, then c = 25. By supposition, this optimal solution does not contain a quarter, and so it contains only dimes, nickels, and pennies. If it contains three dimes, we can replace these three dimes by a quarter and a nickel, giving a solution with one fewer coin. If it contains at most two dimes, then some subset of the dimes, nickels, and pennies adds up to 25 cents, and so we can replace these coins by one quarter to give a solution with fewer coins.  Thus, we have shown that there is always an optimal solution that includes the greedy choice, and that we can combine the greedy choice with an optimal solu- tion to the remaining subproblem to produce an optimal solution to our original problem. Therefore, the greedy algorithm produces an optimal solution. For the algorithm that chooses one coin at a time and then recurses on sub- problems, the running time is  cid:2  k , where k is the number of coins used in an optimal solution. Since k ≤ n, the running time is O n . For our  cid:222 rst descrip- tion of the algorithm, we perform a constant number of calculations  since there are only 4 coin types , and the running time is O 1 .  b. When the coin denominations are c0, c1, . . . , ck, the greedy algorithm to make j = change for n cents works by  cid:222 nding the denomination cj such that max{0 ≤ i ≤ k : ci ≤ n}, giving one coin of denomination cj , and recurs- ing on the subproblem of making change for n − cj cents.  An equivalent, but more ef cid:222 cient, algorithm is to give  cid:9 n ck cid:10  coins of denomination ck and  cid:9  n mod ci+1  ci cid:10  coins of denomination ci for i = 0, 1, . . . , k − 1.  To show that the greedy algorithm produces an optimal solution, we start by proving the following lemma:  Lemma For i = 0, 1, . . . , k, let ai be the number of coins of denomination ci used in an optimal solution to the problem of making change for n cents. Then for i = 0, 1, . . . , k − 1, we have ai < c. Proof If ai ≥ c for some 0 ≤ i < k, then we can improve the solution by using one more coin of denomination ci+1 and c fewer coins of denomination ci . The amount for which we make change remains the same, but we use c − 1 > 0  lemma  fewer coins.  To show that the greedy solution is optimal, we show that any non-greedy so- lution is not optimal. As above, let j = max{0 ≤ i ≤ k : ci ≤ n}, so that the greedy solution uses at least one coin of denomination cj . Consider a non- greedy solution, which must use no coins of denomination cj or higher. Let the non-greedy solution use ai coins of denomination ci, for i = 0, 1, . . . , j − 1; i=0 ai ci ≥ c j . j−1 thus we have  i=0 ai ci = n. Since n ≥ c j , we have that j−1   cid:1    cid:1    16-18  Solutions for Chapter 16: Greedy Algorithms  j−1 cid:2  Now suppose that the non-greedy solution is optimal. By the above lemma, ai ≤ c − 1 for i = 0, 1, . . . , j − 1. Thus,  aici ≤ j−1 cid:2   i=0  i=0   c − 1 ci j−1 cid:2  ci i=0 c j − 1 c − 1  =  c − 1  =  c − 1  = c j − 1 < c j ,   cid:1   i=0 ai ci ≥ c j . We conclude that j−1  which contradicts our earlier assertion that the non-greedy solution is not optimal. Since any algorithm that does not produce the greedy solution fails to be opti- mal, only the greedy algorithm produces the optimal solution. The problem did not ask for the running time, but for the more ef cid:222 cient greedy- algorithm formulation, it is easy to see that the running time is O k , since we have to perform at most k each of the division,  cid:223 oor, and mod operations. c. With actual U.S. coins, we can use coins of denomination 1, 10, and 25. When n = 30 cents, the greedy solution gives one quarter and  cid:222 ve pennies, for a total of six coins. The non-greedy solution of three dimes is better. The smallest integer numbers we can use are 1, 3, and 4. When n = 6 cents, the greedy solution gives one 4-cent coin and two 1-cent coins, for a total of three coins. The non-greedy solution of two 3-cent coins is better.  d. Since we have optimal substructure, dynamic programming might apply. And  indeed it does. Let us de cid:222 ne c[ j] to be the minimum number of coins we need to make change for j cents. Let the coin denominations be d1, d2, . . . , dk. Since one of the coins is a penny, there is a way to make change for any amount j ≥ 1. Because of the optimal substructure, if we knew that an optimal solution for the problem of making change for j cents used a coin of denomination di , we would have c[ j] = 1 + c[ j − di ]. As base cases, we have that c[ j] = 0 for all j ≤ 0. To develop a recursive formulation, we have to check all denominations, giving c[ j] =  $  if j ≤ 0 , if j > 1 .  {c[ j − di ]}  0 1 + min 1≤i≤k  We can compute the c[ j] values in order of increasing j by using a table. The following procedure does so, producing a table c[1 . . n]. It avoids even exam- ining c[ j] for j ≤ 0 by ensuring that j ≥ di before looking up c[ j − di ]. The procedure also produces a table denom[1 . . n], where denom[ j] is the denomi- nation of a coin used in an optimal solution to the problem of making change for j cents.   Solutions for Chapter 16: Greedy Algorithms  16-19  COMPUTE-CHANGE n, d, k  for j ← 1 to n  do c[ j] ← ∞  for i ← 1 to k  do if j ≥ di and 1 + c[ j − di ] < c[ j]  then c[ j] ← 1 + c[ j − di ]  denom[ j] ← di  return c and denom  This procedure obviously runs in O nk  time. We use the following procedure to output the coins used in the optimal solution computed by COMPUTE-CHANGE:  GIVE-CHANGE  j, denom  if j > 0  then give one coin of denomination denom[ j] GIVE-CHANGE  j − denom[ j], denom   The initial call is GIVE-CHANGE n, denom . Since the value of the  cid:222 rst pa- rameter decreases in each recursive call, this procedure runs in O n  time.    Lecture Notes for Chapter 17: Amortized Analysis  Chapter 17 overview  Amortized analysis    Analyze a sequence of operations on a data structure.   Goal: Show that although some individual operations may be expensive, on  average the cost per operation is small.  Average in this context does not mean that we cid:146 re averaging over a distribution of inputs.   No probability is involved.   We cid:146 re talking about average cost in the worst case.  Organization  We cid:146 ll look at 3 methods:        aggregate analysis accounting method potential method  Using 3 examples:        stack with multipop operation binary counter dynamic tables  later on   Aggregate analysis  Stack operations       PUSH S, x : O 1  each ⇒ O n  for any sequence of n operations. POP S : O 1  each ⇒ O n  for any sequence of n operations.   17-2  Lecture Notes for Chapter 17: Amortized Analysis    MULTIPOP S, k   while S is not empty and k > 0  do POP S   k ← k − 1  Running time of MULTIPOP:   Linear in  of POP operations.   Let each PUSH POP cost 1.     Therefore, total cost = min s, k . Sequence of n PUSH, POP, MULTIPOP operations:   Worst-case cost of MULTIPOP is O n .   Have n operations.   Therefore, worst-case cost of sequence is O n2 .   of iterations of while loop is min s, k , where s =  of objects on stack.  Observation    Each object can be popped only once per time that it cid:146 s pushed.   Have ≤ n PUSHes ⇒ ≤ n POPs, including those in MULTIPOP.   Therefore, total cost = O n .   Average over the n operations ⇒ O 1  per operation on average. Again, notice no probability.   Showed worst-case O n  cost for sequence.   Therefore, O 1  per operation on average. This technique is called aggregate analysis.  Binary counter     k-bit binary counter A[0 . . k − 1] of bits, where A[0] is the least signi cid:222 cant bit and A[k − 1] is the most signi cid:222 cant bit.    Counts upward from 0.  k−1 cid:2   A[i] · 2i.    Value of counter is     To increment, add 1  mod 2k :  Initially, counter value is 0, so A[0 . . k − 1] = 0.  i=0  INCREMENT A, k  i ← 0 while i < k and A[i] = 1  do A[i] ← 0 i ← i + 1 if i < k then A[i] ← 1   Lecture Notes for Chapter 17: Amortized Analysis  17-3  Cost of INCREMENT =  cid:2   of bits  cid:223 ipped  .  Analysis: Each call could  cid:223 ip k bits, so n INCREMENTs takes O nk  time.  Example: k = 3 [Underlinedbits cid:223 ip. Showcostslater.]  counter value  0 1 2 3 4 5 6 7 0 ...  A  2 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0  ...  cost 0 1 3 4 7 8 10 11 14  15  Observation  Not every bit  cid:223 ips every time. [Showcostsfromabove.] bit 0 1 2  every time 1 2 the time 1 4 the time   cid:223 ips how often  1 2i the time  ...  ...  i i ≥ k  times in n INCREMENTs  n  cid:9 n 2 cid:10   cid:9 n 4 cid:10   cid:9 n 2i cid:10   0  never  Therefore, total  of  cid:223 ips = k−1 cid:2   cid:9 n 2i cid:10  ∞ cid:2   cid:5  < n = n = 2n .  i=0 1 − 1 2  1 2i  i=0  1   cid:6   Therefore, n INCREMENTs costs O n . Average cost per operation = O 1 .   17-4  Lecture Notes for Chapter 17: Amortized Analysis  Accounting method  Assign different charges to different operations.   Some are charged more than actual cost.   Some are charged less. Amortized cost = amount we charge. When amortized cost > actual cost, store the difference on speci cid:222 c objects in the data structure as credit. Use credit later to pay for operations whose actual cost > amortized cost. Differs from aggregate analysis:      In the accounting method, different operations can have different costs. In aggregate analysis, all operations have same cost.  Need credit to never go negative.   Otherwise, have a sequence of operations for which the amortized cost is not  ci for all sequences of n operations.  an upper bound on actual cost.    Amortized cost would tell us nothing. &ci = amortized cost of ith operation . Let ci = actual cost of ith operation , &ci ≥ n cid:2  Total credit stored = n cid:2   &ci − n cid:2   Then require  n cid:2   i=1  i=1  ci ≥ cid:23  cid:24  cid:25  cid:26   i=1  i=1  0 .  had better be  Stack  actual cost  operation PUSH POP MULTIPOP min k, s   1 1  2 0 0  amortized cost  $1 pays for the PUSH. $1 is prepayment for it being popped by either POP or MULTIPOP.  Intuition: When pushing an object, pay $2.       Since each object has $1, which is credit, the credit can never go negative.   Therefore, total amortized cost, = O n , is an upper bound on total actual cost.   Lecture Notes for Chapter 17: Amortized Analysis  17-5  Binary counter  $1 pays for setting a bit to 1. $1 is prepayment for  cid:223 ipping it back to 0.  Charge $2 to set a bit to 1.       Have $1 of credit for every 1 in the counter.   Therefore, credit ≥ 0. Amortized cost of INCREMENT:   Cost of resetting bits to 0 is paid by credit.   At most 1 bit is set to 1.   Therefore, amortized cost ≤ $2.   For n operations, amortized cost = O n .  Potential method  Like the accounting method, but think of the credit as potential stored with the entire data structure.   Accounting method stores credit with speci cid:222 c objects.   Potential method stores potential in the data structure as a whole.   Can release potential to pay for future operations.   Most  cid:223 exible of the amortized analysis methods. Let Di = data structure after ith operation , D0 = initial data structure , &ci = amortized cost of ith operation . ci = actual cost of ith operation , Potential function  cid:12  : Di → R &ci = ci +  cid:12  Di   −  cid:12  Di−1   cid:12  Di   is the potential associated with data structure Di.  cid:23   cid:24  cid:25   cid:26  . = ci +  cid:13  cid:12  Di   Total amortized cost = n cid:2  &ci = n cid:2   ci +  cid:12  Di   −  cid:12  Di−1   = n cid:2   i=1  telescoping sum: every term other than D0 and Dn  is added once and subtracted once  ci +  cid:12  Dn  −  cid:12  D0  .  increase in potential due to ith operation  i=1  i=1   17-6  Lecture Notes for Chapter 17: Amortized Analysis  If we require that  cid:12  Di   ≥  cid:12  D0  for all i, then the amortized cost is always an upper bound on actual cost. In practice:  cid:12  D0  = 0,  cid:12  Di   ≥ 0 for all i.  Stack  cid:12  =  of objects in stack   =  of $1 bills in accounting method   actual cost  D0 = empty stack ⇒  cid:12  D0  = 0. Since  of objects in stack is always ≥ 0,  cid:12  Di   ≥ 0 =  cid:12  D0  for all i. operation PUSH  amortized cost 1 + 1 = 2 1 − 1 = 0 POP  cid:4  = 0  cid:4  − k MULTIPOP k Therefore, amortized cost of a sequence of n operations = O n .   s + 1  − s = 1 where s =  of objects initially  s − 1  − s = −1  cid:4   − s = −k  s − k   cid:4  = min k, s    cid:13  cid:12   1  1  k   cid:4   Binary counter  cid:12  = bi =  of 1 cid:146 s after ith INCREMENT Suppose ith operation resets ti bits to 0. ci ≤ ti + 1  resets ti bits, sets ≤ 1 bit to 1         If bi = 0, the ith operation reset all k bits and didn cid:146 t set one, so bi−1 = ti = k ⇒ bi = bi−1 − ti . If bi > 0, the ith operation reset ti bits, set one, so bi = bi−1 − ti + 1.    Either way, bi ≤ bi−1 − ti + 1.   Therefore,  cid:13  cid:12  Di   ≤  bi−1 − ti + 1  − bi−1 &ci = ci +  cid:13  cid:12  Di   = 1 − ti . ≤  ti + 1  +  1 − ti   = 2 .  If counter starts at 0,  cid:12  D0  = 0. Therefore, amortized cost of n operations = O n .  Dynamic tables  A nice use of amortized analysis.   Lecture Notes for Chapter 17: Amortized Analysis  17-7    Have a table cid:151 maybe a hash table.   Don cid:146 t know in advance how many objects will be stored in it.   When it  cid:222 lls, must reallocate with a larger size, copying all objects into the new,  larger table.    When it gets suf cid:222 ciently small, might want to reallocate with a smaller size.  Details of table organization not important.  Scenario  Goals  1. O 1  amortized time per operation. 2. Unused space always ≤ constant fraction of allocated space. Load factor α = num size, where num =  items stored, size = allocated size. If size = 0, then num = 0. Call α = 1. Never allow α > 1. Keep α > a constant fraction ⇒ goal  2 .  Table expansion  Consider only insertion.   When the table becomes full, double its size and reinsert all existing items.   Guarantees that α ≥ 1 2.   Each time we actually insert an item into the table, it cid:146 s an elementary insertion.  TABLE-INSERT T, x  if size[T ] = 0 then allocate table[T ] with 1 slot size[T ] ← 1 if num[T ] = size[T ] then allocate new-table with 2 · size[T ] slots insert all items in table[T ] into new-table free table[T ] table[T ] ← new-table size[T ] ← 2 · size[T ]   cid:1  expand?  insert x into table[T ] num[T ] ← num[T ] + 1 Initially, num[T ] = size[T ] = 0.   cid:1  num[T ] elem insertions   cid:1  1 elem insertion   17-8  Lecture Notes for Chapter 17: Amortized Analysis       Running time: Charge 1 per elementary insertion. Count only elementary inser- tions, since all other costs together are constant per call. ci = actual cost of ith operation  If not full, ci = 1. If full, have i − 1 items in the table at the start of the ith operation. Have to copy all i − 1 existing items, then insert ith item ⇒ ci = i.  cid:9  Total cost = n cid:2   n operations ⇒ ci = O n  ⇒ O n2  time for n operations. Of course, we don cid:146 t always expand: ci =  if i − 1 is exact power of 2 ,  i 1 otherwise .  ci  2 j  i=1 ≤ n +   cid:9 lg n cid:10  cid:2  j=0  cid:9 lg n cid:10 +1 − 1 = n + 2 2 − 1 < n + 2n = 3n  Therefore, aggregate analysis says amortized cost per operation = 3.  Accounting method  Charge $3 per insertion of x.        $1 pays for x cid:146 s insertion. $1 pays for x to be moved in the future. $1 pays for some other item to be moved.  Suppose we cid:146 ve just expanded, size = m before next expansion, size = 2m after next expansion.   Assume that the expansion used up all the credit, so that there cid:146 s no credit stored  after the expansion.    Will expand again after another m insertions.   Each insertion will put $1 on one of the m items that were in the table just after  expansion and will put $1 on the item inserted.    Have $2m of credit by next expansion, when there are 2m items to move. Just  enough to pay for the expansion, with no credit left over!   Lecture Notes for Chapter 17: Amortized Analysis  17-9  Potential method  cid:12  T   = 2 · num[T ] − size[T ]         Initially, num = size = 0 ⇒  cid:12  = 0. Just after expansion, size = 2 · num ⇒  cid:12  = 0. Just before expansion, size = num ⇒  cid:12  = num ⇒ have enough potential to pay for moving all items.    Need  cid:12  ≥ 0, always.  Always have size ≥ num  ≥ 1 2 · num ≥ size ≥ 0 .   cid:12   2  · size ⇒ ⇒     Amortized cost of ith operation: numi = num after ith operation , sizei = size after ith operation ,  cid:12 i =  cid:12  after ith operation . If no expansion: sizei = sizei−1 , numi = numi−1 +1 , ci = 1 . &ci = ci +  cid:12 i −  cid:12 i−1 Then we have = 1 +  2 · numi − sizei   −  2 · numi−1 − sizei−1  = 1 +  2 · numi − sizei   −  2 numi −1  − sizei   = 1 + 2 = 3 . If expansion: sizei = 2 · sizei−1 , sizei−1 = numi−1 = numi −1 , ci = numi−1 +1 = numi . &ci = ci +  cid:12 i +  cid:12 i−1 = numi +  2 · numi − sizei   −  2 · numi−1 − sizei−1  = numi +  2 · numi −2 numi −1   −  2 numi −1  −  numi −1   = numi + 2 −  numi −1  = 3 .  Then we have      17-10  Lecture Notes for Chapter 17: Amortized Analysis  sizei  numi  Φi  32  24  16  8  0  0  8  16  24  i  32  Expansion and contraction  When α drops too low, contract the table.   Allocate a new, smaller one.   Copy all items. Still want   α bounded from below by a constant, amortized cost per operation = O 1 .    Measure cost in terms of elementary insertions and deletions.  α would become > 1 .  so that after deletion α would become < 1 2 .   cid:147 Obvious strategy cid:148 :   Double size when inserting into a full table  when α = 1, so that after insertion   Halve size when deletion would make table less than half full  when α = 1 2,   Then always have 1 2 ≤ α ≤ 1.   Suppose we  cid:222 ll table. Then insert ⇒ double 2 deletes ⇒ halve 2 inserts ⇒ double 2 deletes ⇒ halve  ···  Not performing enough operations after expansion or contraction to pay for the next one.  Simple solution:   Double as before: when inserting with α = 1 ⇒ after doubling, α = 1 2.   Halve size when deleting with α = 1 4 ⇒ after halving, α = 1 2.   Thus, immediately after either expansion or contraction, have α = 1 2.   Always have 1 4 ≤ α ≤ 1.   Lecture Notes for Chapter 17: Amortized Analysis  17-11  Intuition:   Want to make sure that we perform enough operations between consecutive  expansions contractions to pay for the change in table size.    Need to delete half the items before contraction.   Need to double number of items before expansion.   Either way, number of operations between expansions contractions is at least a   cid:9  2 · num[T ] − size[T ] size[T ] 2 − num[T ]  constant fraction of number of items copied. if α ≥ 1 2 , if α < 1 2 .  2  2  · size ⇒ 2 · num ≥ size ⇒  cid:12  ≥ 0. · size ⇒  cid:12  ≥ 0.   cid:12  T   = T empty ⇒  cid:12  = 0. α ≥ 1 2 ⇒ num ≥ 1 α < 1 2 ⇒ num < 1 Intuition:  cid:12  measures how far from α = 1 2 we are.   α = 1 2 ⇒  cid:12  = 2 · num−2 · num = 0.   α = 1 ⇒  cid:12  = 2 · num− num = num.   α = 1 4 ⇒  cid:12  = size  2 − num = 4 · num  2 − num = num.   Therefore, when we double or halve, have enough potential to pay for moving   Potential increases linearly between α = 1 2 and α = 1, and it also increases  all num items. linearly between α = 1 2 and α = 1 4.    Since α has different distances to go to get to 1 or 1 4, starting from 1 2, rate  of increase of  cid:12  differs.   For α to go from 1 2 to 1, num increases from size  2 to size, for a total increase of size  2.  cid:12  increases from 0 to size. Thus,  cid:12  needs to increase by 2 for each item inserted. That cid:146 s why there cid:146 s a coef cid:222 cient of 2 on the num[T ] term in the formula for  cid:12  when α ≥ 1 2.    For α to go from 1 2 to 1 4, num decreases from size  2 to size  4, for a total decrease of size  4.  cid:12  increases from 0 to size  4. Thus,  cid:12  needs to increase by 1 for each item deleted. That cid:146 s why there cid:146 s a coef cid:222 cient of −1 on the num[T ] term in the formula for  cid:12  when α < 1 2.  Amortized costs: more cases    insert, delete     size does doesn cid:146 t change    α ≥ 1 2, α < 1 2  use αi , since α can vary a lot    αi−1 ≥ 1 2, same analysis as before ⇒&ci = 3.   αi−1 < 1 2 ⇒ no expansion  only occurs when αi−1 = 1 .  Insert:   17-12  Lecture Notes for Chapter 17: Amortized Analysis        If αi−1 < 1 2 and αi < 1 2:  &ci = ci +  cid:12 i +  cid:12 i−1 = 1 +  sizei  2 − numi   −  sizei−1  2 − numi−1  = 1 +  sizei  2 − numi   −  sizei  2 −  numi −1   = 0 . &ci = 1 +  2 · numi − sizei   −  sizei−1  2 − numi−1  If αi−1 < 1 2 and αi ≥ 1 2: = 1 +  2 numi−1 +1  − sizei−1  −  sizei−1  2 − numi−1  = 3 · numi−1 −3 2 = 3 · αi−1 sizei−1 −3 2 · sizei−1 +3 < 3 2 = 3 .  · sizei−1 −3 2  · sizei−1 +3  · sizei−1 +3  Therefore, amortized cost of insert is < 3.  Delete:    If αi−1 < 1 2, then αi < 1 2.    If no contraction:  &ci = 1 +  sizei  2 − numi   −  sizei−1  2 − numi−1  = 1 +  sizei  2 − numi   −  sizei  2 −  numi +1   = 2 . &ci =  numi +1   cid:23   cid:24  cid:25   cid:26   +  sizei  2 − numi   −  sizei−1  2 − numi−1  move + delete [sizei  2 = sizei−1  4 = numi−1 = numi +1]  If contraction:        =  numi +1  +   numi +1  − numi   −   2 · numi +2  −  numi +1   = 1 . If αi−1 ≥ 1 2, then no contraction. &ci = 1 +  2 · numi − sizei   −  2 · numi−1 − sizei−1  If αi ≥ 1 2: = 1 +  2 · numi − sizei   −  2 · numi +2 − sizei   = −1 .        If αi < 1 2, since αi−1 ≥ 1 2, have numi = numi−1 −1 ≥ 1 2  · sizei−1 −1 = 1 2  · sizei −1 .   Lecture Notes for Chapter 17: Amortized Analysis  17-13  Thus,&ci = 1 +  sizei  2 − numi   −  2 · numi−1 − sizei−1  = 1 +  sizei  2 − numi   −  2 · numi +2 − sizei   = −1 + 3 2 ≤ −1 + 3 2 = 2 .   cid:5  · sizei −3 · numi · sizei −3   cid:6  · sizei −1  1 2  Therefore, amortized cost of delete is ≤ 2.   Solutions for Chapter 17: Amortized Analysis  Solution to Exercise 17.1-3   cid:9   Let ci = cost of ith operation. ci =  i 1 otherwise .  if i is an exact power of 2 ,  Operation Cost  1 2 3 4 5 6 7 8 9 10 ...  1 2 1 4 1 1 1 8 1 1 ...  n cid:2   n operations cost  ci ≤ n + lg n cid:2   i=1  j=0  2 j = n +  2n − 1  < 3n .  cid:1   Note: Ignoring  cid:223 oor in upper bound of Average cost of operation = Total cost  operations By aggregate analysis, the amortized cost per operation = O 1 .  2 j .   < 3  Solution to Exercise 17.2-1  [We assume that the only way in which COPY is invoked is automatically, after everysequenceof k PUSH and POP operations.]   Solutions for Chapter 17: Amortized Analysis  17-15  Charge $2 for each PUSH and POP operation and $0 for each COPY. When we call PUSH, we use $1 to pay for the operation, and we store the other $1 on the item pushed. When we call POP, we again use $1 to pay for the operation, and we store the other $1 in the stack itself. Because the stack size never exceeds k, the actual cost of a COPY operation is at most $k, which is paid by the $k found in the items in the stack and the stack itself. Since there are k PUSH and POP operations between two consecutive COPY operations, there are $k of credit stored, either on individual items  from PUSH operations  or in the stack itself  from POP operations  by the time a COPY occurs. Since the amortized cost of each operation is O 1  and the amount of credit never goes negative, the total cost of n operations is O n .  Solution to Exercise 17.2-2   cid:9   Let ci = cost of ith operation. ci = Charge each operation $3  amortized cost&ci  .  i 1 otherwise .  if i is an exact power of 2 ,       If i is not an exact power of 2, pay $1, and store $2 as credit. If i is an exact power of 2, pay $i, using stored credit.  Operation Cost Actual cost Credit remaining  1 2 3 4 5 6 7 8 9 10 ...  3 3 3 3 3 3 3 3 3 3 ...  1 2 1 4 1 1 1 8 1 1 ...  2 3 5 4 6 8 10 5 7 9 ...  Since the amortized cost is $3 per operation,  We know from Exercise 17.1-3 that  ci < 3n.  n cid:2   i=1  &ci = 3n.  n cid:2   i=1  n cid:2   i=1  &ci ≥ n cid:2   i=1  Then we have  ci ⇒ credit = amortized cost − actual cost ≥ 0.  Since the amortized cost of each operation is O 1 , and the amount of credit never goes negative, the total cost of n operations is O n .   17-16  Solutions for Chapter 17: Amortized Analysis  Solution to Exercise 17.2-3  We introduce a new  cid:222 eld max[A] to hold the index of the high-order 1 in A. Ini- tially, max[A] is set to −1, since the low-order bit of A is at index 0, and there are initially no 1 cid:146 s in A. The value of max[A] is updated as appropriate when the counter is incremented or reset, and we use this value to limit how much of A must be looked at to reset it. By controlling the cost of RESET in this way, we can limit it to an amount that can be covered by credit from earlier INCREMENTs.  INCREMENT A  i ← 0 while i < length[A] and A[i] = 1  do A[i] ← 0 i ← i + 1 if i < length[A] then A[i] ← 1   cid:1  Additions to book cid:146 s INCREMENT start here if i > max[A] then max[A] ← i else max[A] ← −1 RESET A  for i ← 0 to max[A] do A[i] ← 0 max[A] ← −1  As for the counter in the book, we assume that it costs $1 to  cid:223 ip a bit. In addition, we assume it costs $1 to update max[A]. Setting and resetting of bits by INCREMENT will work exactly as for the original counter in the book: $1 will pay to set one bit to 1; $1 will be placed on the bit that is set to 1 as credit; the credit on each 1 bit will pay to reset the bit during incrementing. In addition, we cid:146 ll use $1 to pay to update max, and if max increases, we cid:146 ll place an additional $1 of credit on the new high-order 1.  If max doesn cid:146 t increase, we can just waste that $1 cid:151 it won cid:146 t be needed.  Since RESET manipulates bits at positions only up to max[A], and since each bit up to there must have become the high-order 1 at some time before the high-order 1 got up to max[A], every bit seen by RESET has $1 of credit on it. So the zeroing of bits of A by RESET can be completely paid for by the credit stored on the bits. We just need $1 to pay for resetting max. Thus charging $4 for each INCREMENT and $1 for each RESET is suf cid:222 cient, so the sequence of n INCREMENT and RESET operations takes O n  time.   Solutions for Chapter 17: Amortized Analysis  17-17  Solution to Exercise 17.3-3  Let Di be the heap after the ith operation, and let Di consist of ni elements. Also, let k be a constant such that each INSERT or EXTRACT-MIN operation takes at most k ln n time, where n = max ni−1, ni  .  We don cid:146 t want to worry about taking the log of 0, and at least one of ni−1 and ni is at least 1. We cid:146 ll see later why we use the natural log.  De cid:222 ne  cid:12  Di   =   cid:9   if ni = 0 , if ni > 0 .  0 kni ln ni  This function exhibits the characteristics we like in a potential function: if we start with an empty heap, then  cid:12  D0  = 0, and we always maintain that  cid:12  Di   ≥ 0. Before proving that we achieve the desired amortized times, we show that if n ≥ 2, then n ln n n−1 n n − 1   cid:6   n ln   since 1 + x ≤ ex for all real x    cid:5  ≤ 2. We have  cid:6   cid:5  = n ln 1 + 1 n − 1  cid:10   cid:11 n 1 + 1 = ln n − 1 ≤ ln n−1 1 e = ln e n−1 n = n n − 1 ≤ 2 ,  n  n−1 = n  assuming that n ≥ 2.  The equation ln e n n−1 is why we use the natural log.  If the ith operation is an INSERT, then ni = ni−1 + 1. If the ith operation inserts &ci = ci +  cid:12  Di   −  cid:12  Di−1  into an empty heap, then ni = 1, ni−1 = 0, and the amortized cost is ≤ k ln 1 + k · 1 ln 1 − 0 = 0 . If the ith operation inserts into a nonempty heap, then ni = ni−1 + 1, and the &ci = ci +  cid:12  Di   −  cid:12  Di−1  amortized cost is ≤ k ln ni + kni ln ni − kni−1 ln ni−1 = k ln ni + kni ln ni − k ni − 1  ln ni − 1  = k ln ni + kni ln ni − kni ln ni − 1  + k ln ni − 1  < 2k ln ni + kni ln ≤ 2k ln ni + 2k = O lg ni   .  If the ith operation is an EXTRACT-MIN, then ni = ni−1 − 1. If the ith operation extracts the one and only heap item, then ni = 0, ni−1 = 1, and the amortized cost  ni ni − 1   17-18  Solutions for Chapter 17: Amortized Analysis  is&ci = ci +  cid:12  Di   −  cid:12  Di−1  ≤ k ln 1 + 0 − k · 1 ln 1 = 0 . If the ith operation extracts from a heap with more than 1 item, then ni = ni−1 − 1 &ci = ci +  cid:12  Di   −  cid:12  Di−1  and ni−1 ≥ 2, and the amortized cost is ≤ k ln ni−1 + kni ln ni − kni−1 ln ni−1 = k ln ni−1 + k ni−1 − 1  ln ni−1 − 1  − kni−1 ln ni−1 = k ln ni−1 + kni−1 ln ni−1 − 1  − k ln ni−1 − 1  − kni−1 ln ni−1 = k ln  ni−1 − 1 ni−1  + kni−1 ln + kni−1 ln 1   since ni−1 ≥ 2   ni−1 ni−1 − 1 ni−1 ni−1 − 1 ni−1 ni−1 − 1  < k ln = k ln ≤ k ln 2 = O 1  .  cid:2   cid:3   x∈Di  k di  x  + 1   cid:2    cid:4   di  x   ,  x∈Di  = k  ni +  A slightly different potential function cid:151 which may be easier to work with cid:151 is as follows. For each node x in the heap, let di  x  be the depth of x in Di. De cid:222 ne  cid:12  Di   =  where k is de cid:222 ned as before. Initially, the heap has no items, which means that the sum is over an empty set, and so  cid:12  D0  = 0. We always have  cid:12  Di   ≥ 0, as required. Observe that after an INSERT, the sum changes only by an amount equal to the depth of the new last node of the heap, which is  cid:9 lg ni cid:10 . Thus, the change in potential due to an INSERT is k 1 +  cid:9 lg ni cid:10  , and so the amortized cost is O lg ni   + O lg ni   = O lg ni   = O lg n . After an EXTRACT-MIN, the sum changes by the negative of the depth of the old last node in the heap, and so the potential decreases by k 1 + cid:9 lg ni−1 cid:10  . The amortized cost is at most k lg ni−1 − k 1 +  cid:9 lg ni−1 cid:10   = O 1 .  Solution to Problem 17-2  a. The SEARCH operation can be performed by searching each of the individually sorted arrays. Since all the individual arrays are sorted, searching one of them using a binary search algorithm takes O lg m  time, where m is the size of the array. In an unsuccessful search, the time is  cid:2  lg m . In the worst case, we may   Solutions for Chapter 17: Amortized Analysis  17-19  assume that all the arrays A0, A1, . . . , Ak−1 are full, k =  cid:14 lg n + 1  cid:15 , and we perform an unsuccessful search. The total time taken is T  n  =  cid:2  lg 2k−1 + lg 2k−2 + ··· + lg 21 + lg 20   =  cid:2   k − 1  +  k − 2  + ··· + 1 + 0  =  cid:2  k k − 1  2  =  cid:2   cid:14 lg n + 1  cid:15    cid:14 lg n + 1  cid:15  − 1  2  =  cid:2   lg2 n  .  Thus, the worst-case running time is  cid:2  lg2 n .  b. We create a new sorted array of size 1 containing the new element to be inserted. If array A0  which has size 1  is empty, then we replace A0 with the new sorted array. Otherwise, we merge sort the two arrays into another sorted array of size 2. If A1 is empty, then we replace A1 with the new array; otherwise we merge sort the arrays as before and continue. Since array Ai is of size 2i , if we merge sort two arrays of size 2i each, we obtain one of size 2i+1, which is the size of Ai+1. Thus, this method will result in another list of arrays in the same structure that we had before. Let us analyze its worst-case running time. We will assume that merge sort takes 2m time to merge two sorted lists of size m each. If all the arrays A0, A1, . . . , Ak−2 are full, then the running time to  cid:222 ll array Ak−1 would be T  n  = 2  20 + 21 + ··· + 2k−2   = 2 2k−1 − 1  = 2k − 2 =  cid:2  n  .  Therefore, the worst-case time to insert an element into this data structure is  cid:2  n . However, let us now analyze the amortized running time. Using the aggregate method, we compute the total cost of a sequence of n inserts, starting with the empty data structure. Let r be the position of the rightmost 0 in the binary representation  cid:2 nk−1, nk−2, . . . , n0 cid:3  of n, so that n j = 1 for j = 0, 1, . . . , r − 1. r−1 cid:2  The cost of an insertion when n items have already been inserted is j=0 Furthermore, r = 0 half the time, r = 1 a quarter of the time, and so on. There are at most  cid:14 n 2r cid:15  insertions for each value of r. The total cost of the n operations is therefore bounded by  2 · 2 j = O 2r   .   cid:3  cid:14 lg n+1  cid:15  cid:2    cid:7  cid:17    cid:4    cid:18  cid:8   O  r=0  n 2r  = O n lg n  .  2r  The amortized cost per INSERT operation, therefore is O lg n . We can also use the accounting method to analyze the running time. We can charge $k to insert an element. $1 pays for the insertion, and we put $ k − 1  on the inserted item to pay for it being involved in merges later on. Each time it is merged, it moves to a higher-indexed array, i.e., from Ai to Ai+1. It can   17-20  Solutions for Chapter 17: Amortized Analysis  move to a higher-indexed array at most k − 1 times, and so the $ k − 1  on the item suf cid:222 ces to pay for all the times it will ever be involved in merges. Since k =  cid:2  lg n , we have an amortized cost of  cid:2  lg n  per insertion.  c. DELETE x  will be implemented as follows:  1. Find the smallest j for which the array Aj with 2 j elements is full. Let y be  the last element of Aj .  2. Let x be in the array Ai . If necessary,  cid:222 nd which array this is by using the  search procedure. 3. Remove x from Ai and put y into Ai . Then move y to its correct place in Ai . 4. Divide A j  which now has 2j − 1 elements left : The  cid:222 rst element goes into array A0, the next 2 elements go into array A1, the next 4 elements go into array A2, and so forth. Mark array Aj as empty. The new arrays are created already sorted. The cost of DELETE is  cid:2  n  in the worst case, where i = k − 1 and j = k − 2:  cid:2  lg n  to  cid:222 nd A j ,  cid:2  lg2 n  to  cid:222 nd Ai ,  cid:2  2i   =  cid:2  n  to put y in its correct place in array Ai , and  cid:2  2 j   =  cid:2  n  to divide array Aj . The following sequence of n operations, where n 3 is a power of 2, yields an amortized cost that is no better: perform n 3 INSERT operations, followed by n 3 pairs of DELETE and INSERT. It costs O n lg n  to do the  cid:222 rst n 3 INSERT operations. This creates a single full array. Each subsequent DELETE INSERT pair costs  cid:2  n  for the DELETE to divide the full array and another  cid:2  n  for the INSERT to recombine it. The total is then  cid:2  n2 , or  cid:2  n  per operation.  a. For RB-INSERT, consider a complete red-black tree in which the colors alter- nate between levels. That is, the root is black, the children of the root are red, the grandchildren of the root are black, the great-grandchildren of the root are red, and so on. When a node is inserted as a red child of one of the red leaves, then case 1 of RB-INSERT-FIXUP occurs  lg n + 1   2 times, so that there are  cid:3  lg n  color changes to  cid:222 x the colors of nodes on the path from the inserted node to the root. For RB-DELETE, consider a complete red-black tree in which all nodes are black. If a leaf is deleted, then the double blackness will be pushed all the way up to the root, with a color change at each level  case 2 of RB-DELETE-FIXUP , for a total of  cid:3  lg n  color changes.  b. All cases except for case 1 of RB-INSERT-FIXUP and case 2 of RB-DELETE-  FIXUP are terminating.  c. Case 1 of RB-INSERT-FIXUP reduces the number of red nodes by 1. As Fig- ure 13.5 shows, node z cid:146 s parent and uncle change from red to black, and z cid:146 s grandparent changes from black to red. Hence,  cid:12  T   cid:4   =  cid:12  T   − 1.  d. Lines 1 cid:150 16 of RB-INSERT cause one node insertion and a unit increase in po- tential. The nonterminating case of RB-INSERT-FIXUP  Case 1  makes three  Solution to Problem 17-4   Solutions for Chapter 17: Amortized Analysis  17-21  color changes and decreases the potential by 1. The terminating cases of RB- INSERT-FIXUP  cases 2 and 3  cause one rotation each and do not affect the potential.  Although case 3 makes color changes, the potential does not change. As Figure 13.6 shows, node z cid:146 s parent changes from red to black, and z cid:146 s grand- parent changes from black to red.   e. The number of structural modi cid:222 cations and amount of potential change result- ing from lines 1 cid:150 16 of RB-INSERT and from the terminating cases of RB- INSERT-FIXUP are O 1 , and so the amortized number of structural modi cid:222 ca- tions of these parts is O 1 . The nonterminating case of RB-INSERT-FIXUP may repeat O lg n  times, but its amortized number of structural modi cid:222 cations is 0, since by our assumption the unit decrease in the potential pays for the structural modi cid:222 cations needed. Therefore, the amortized number of structural modi cid:222 cations performed by RB-INSERT is O 1 .  f. From Figure 13.5, we see that case 1 of RB-INSERT-FIXUP makes the follow-  ing changes to the tree:   Changes a black node with two red children  node C  to a red node, resulting  in a potential change of −2.    Changes a red node  node A in part  a  and node B in part  b   to a black  node with one red child, resulting in no potential change.  in a potential change of 1.    Changes a red node  node D  to a black node with no red children, resulting The total change in potential is −1, which pays for the structural modi cid:222 cations performed, and thus the amortized number of structural modi cid:222 cations in case 1  the nonterminating case  is 0. The terminating cases of RB-INSERT-FIXUP cause O 1  structural changes. Because w v  is based solely on node colors and the number of color changes caused by terminating cases is O 1 , the change in potential in terminating cases is O 1 . Hence, the amortized number of structural modi cid:222 cations in the terminating cases is O 1 . The overall amortized number of structural modi cid:222 cations in RB-INSERT, therefore, is O 1 .  g. Figure 13.7 shows that case 2 of RB-DELETE-FIXUP makes the following       changes to the tree:   Changes a black node with no red children  node D  to a red node, resulting  in a potential change of −1. If B is red, then it loses a black child, with no effect on potential. If B is black, then it goes from having no red children to having one red child, resulting in a potential change of −1. The total change in potential is either −1 or −2, depending on the color of B. In either case, one unit of potential pays for the structural modi cid:222 cations per- formed, and thus the amortized number of structural modi cid:222 cations in case 2  the nonterminating case  is at most 0. The terminating cases of RB-DELETE cause O 1  structural changes. Because w v  is based solely on node col- ors and the number of color changes caused by terminating cases is O 1 , the change in potential in terminating cases is O 1 . Hence, the amortized number of structural changes in the terminating cases is O 1 . The overall amortized number of structural modi cid:222 cations in RB-DELETE-FIXUP, therefore, is O 1 .   17-22  Solutions for Chapter 17: Amortized Analysis  h. Since the amortized number structural modi cid:222 cation in each operation is O 1 , the actual number of structural modi cid:222 cations for any sequence of m RB- INSERT and RB-DELETE operations on an initially empty red-black tree is O m  in the worst case.   Lecture Notes for Chapter 21: Data Structures for Disjoint Sets  Chapter 21 overview  Disjoint-set data structures    Also known as  cid:147 union  cid:222 nd. cid:148    Maintain collection S = {S1, . . . , Sk} of disjoint dynamic  changing over time    Each set is identi cid:222 ed by a representative, which is some member of the set.  sets.  Doesn cid:146 t matter which member is the representative, as long as if we ask for the representative twice without modifying the set, we get the same answer both times.  [We do not include notes for the proof of running time of the disjoint-set forest implementation,whichiscoveredinSection21.4.]  Operations    MAKE-SET x : make a new set Si = {x}, and add Si to S.   UNION x, y : if x ∈ Sx , y ∈ Sy, then S ← S − Sx − Sy ∪ {Sx ∪ Sy}.  of one of Sx and Sy.    Representative of new set is any member of Sx ∪ Sy, often the representative   Destroys Sx and Sy  since sets must be disjoint . FIND-SET x : return representative of set containing x.     Analysis in terms of:    n =  of elements =  of MAKE-SET operations,    m = total  of operations.   21-2  Lecture Notes for Chapter 21: Data Structures for Disjoint Sets  Analysis:   Since MAKE-SET counts toward total  of operations, m ≥ n.   Can have at most n − 1 UNION operations, since after n − 1 UNIONs, only 1    Assume that the  cid:222 rst n operations are MAKE-SET  helpful for analysis, usually  set remains.  not really necessary .  Application: dynamic connected components. For a graph G =  V, E , vertices u, v are in same connected component if and only if there cid:146 s a path between them.   Connected components partition vertices into equivalence classes.  CONNECTED-COMPONENTS  V, E  for each vertex v ∈ V do MAKE-SET v  for each edge  u, v  ∈ E do if FIND-SET u   cid:19 = FIND-SET v   then UNION u, v   SAME-COMPONENT u, v  if FIND-SET u  = FIND-SET v  then return TRUE else return FALSE  Note: If actually implementing connected components,      each vertex needs a handle to its object in the disjoint-set data structure, each object in the disjoint-set data structure needs a handle to its vertex.  Linked list representation    Each set is a singly linked list.   Each list node has  cid:222 elds for  the set member      pointer to the representative   next    List has head  pointer to representative  and tail.  MAKE-SET: create a singleton list. FIND-SET: return pointer to representative. UNION: a couple of ways to do it.  1. UNION x, y : append x cid:146 s list onto end of y cid:146 s list. Use y cid:146 s tail pointer to  cid:222 nd  the end.   Lecture Notes for Chapter 21: Data Structures for Disjoint Sets  21-3    Need to update the representative pointer for every node on x cid:146 s list.    If appending a large list onto a small list, it can take a while.  Operation UNION x1, x2  UNION x2, x3  UNION x3, x4  UNION x4, x5   ...  UNION xn−1, xn    objects updated  1 2 3 4 ... n − 1  cid:2  n2  total  Amortized time per operation =  cid:2  n .  2. Weighted-union heuristic: Always append the smaller list to the larger list. A single union can still take  cid:3  n  time, e.g., if both sets have n 2 members.  Theorem With weighted union, a sequence of m operations on n elements takes O m + n lg n  time. Sketch of proof Each MAKE-SET and FIND-SET still takes O 1 . How many times can each object cid:146 s representative pointer be updated? It must be in the smaller set each time. times updated  size of resulting set  1 2 3 ... k ... lg n  ≥ 2 ≥ 4 ≥ 8 ... ≥ 2k ... ≥ n  Therefore, each representative is updated ≤ lg n times.   theorem   Seems pretty good, but we can do much better.  Disjoint-set forest  Forest of trees.   1 tree per set. Root is representative.   Each node points only to its parent.   21-4  Lecture Notes for Chapter 21: Data Structures for Disjoint Sets  c  e  h  b  f  d  g  UNION e,g   c  f  e  d  g  h  b    MAKE-SET: make a single-node tree.   UNION: make one root a child of the other.    FIND-SET: follow pointers to the root.  Not so good cid:151 could get a linear chain of nodes.  Great heuristics    Union by rank: make the root of the smaller tree  fewer nodes  a child of the  root of the larger tree.   Don cid:146 t actually use size.   Use rank, which is an upper bound on height of node.   Make the root with the smaller rank into a child of the root with the larger   Path compression: Find path = nodes visited during FIND-SET on the trip to  rank.  the root. Make all nodes on the  cid:222 nd path direct children of root.  d  a  b  c  d  c  b  a  MAKE-SET x  p[x] ← x rank[x] ← 0 UNION x, y  LINK FIND-SET x , FIND-SET y     Lecture Notes for Chapter 21: Data Structures for Disjoint Sets  21-5   cid:1  If equal ranks, choose y as parent and increment its rank. if rank[x] = rank[y] then rank[y] ← rank[y] + 1  LINK x, y  if rank[x] > rank[y] then p[y] ← x else p[x] ← y  FIND-SET x  if x  cid:19 = p[x] then p[x] ← FIND-SET  p[x]  return p[x]  FIND-SET makes a pass up to  cid:222 nd the root, and a pass down as recursion unwinds to update each node on  cid:222 nd path to point directly to root.  If use both union by rank and path compression, O m α n  .  Running time  n 0 cid:150 2 3 4 cid:150 7  8 cid:150 2047  α n   0 1 2 3 4  2048 cid:150 A4 1  What cid:146 s A4 1 ? See Section 21.4, if you dare. It cid:146 s  cid:28  1080 ≈  of atoms in observ- able universe. This bound is tight cid:151 there is a sequence of operations that takes  cid:3  m α n   time.   Solutions for Chapter 21: Data Structures for Disjoint Sets  Solution to Exercise 21.2-3  We want to show that we can assign O 1  charges to MAKE-SET and FIND-SET and an O lg n  charge to UNION such that the charges for a sequence of these operations are enough to cover the cost of the sequence cid:151 O m+ n lg n , according to the theorem. When talking about the charge for each kind of operation, it is helpful to also be able to talk about the number of each kind of operation. Consider the usual sequence of m MAKE-SET, UNION, and FIND-SET operations, n of which are MAKE-SET operations, and let l < n be the number of UNION operations.  Recall the discussion in Section 21.1 about there being at most n − 1 UNION operations.  Then there are n MAKE-SET operations, l UNION operations, and m − n − l FIND-SET operations. The theorem didn cid:146 t separately name the number l of UNIONs; rather, it bounded the number by n. If you go through the proof of the theorem with l UNIONs, you get the time bound O m−l+l lg l  = O m+l lg l  for the sequence of operations. That is, the actual time taken by the sequence of operations is at most c m + l lg l , for some constant c. Thus, we want to assign operation charges such that  MAKE-SET charge  + FIND-SET charge  + UNION charge  ≥ c m + l lg l  , so that the amortized costs give an upper bound on the actual costs.  cid:4  The following assignments work, where c   MAKE-SET: c  cid:4    FIND-SET: c   UNION: c Substituting into the above sum, we get n + c  cid:4  c  is some constant ≥ c:   m − n − l  l   cid:4  m − n − l  + c   cid:4  lg n + 1   · n · ·   cid:4    cid:4    cid:4  lg n + 1 l = c m + c  cid:4  l lg n  cid:4  m + l lg n  = c > c m + l lg l  .   Solutions for Chapter 21: Data Structures for Disjoint Sets  21-7  Solution to Exercise 21.2-5  Solution to Exercise 21.3-3  Let cid:146 s call the two lists A and B, and suppose that the representative of the new list will be the representative of A. Rather than appending B to the end of A, instead splice B into A right after the  cid:222 rst element of A. We have to traverse B to update representative pointers anyway, so we can just make the last element of B point to the second element of A.  You need to  cid:222 nd a sequence of m operations on n elements that takes  cid:3  m lg n  time. Start with n MAKE-SETs to create singleton sets{x1} ,{x2} , . . . ,{xn}. Next perform the n − 1 UNION operations shown below to create a single set whose tree has depth lg n.  n 4 of these  n 2 of these  UNION x1, x2  UNION x3, x4  UNION x5, x6  ... UNION xn−1, xn   UNION x2, x4  UNION x6, x8  UNION x10, x12  ... UNION xn−2, xn   UNION x4, x8  UNION x12, x16  UNION x20, x24  ... UNION xn−4, xn   ... UNION xn 2, xn  Finally, perform m − 2n + 1 FIND-SET operations on the deepest element in the tree. Each of these FIND-SET operations takes  cid:3  lg n  time. Letting m ≥ 3n, we have more than m 3 FIND-SET operations, so that the total cost is  cid:3  m lg n .  n 8 of these  1 of these  Solution to Exercise 21.3-4  With the path-compression heuristic, the sequence of m MAKE-SET, FIND-SET, and LINK operations, where all the LINK operations take place before any of the   21-8  Solutions for Chapter 21: Data Structures for Disjoint Sets  FIND-SET operations, runs in O m  time. The key observation is that once a node x appears on a  cid:222 nd path, x will be either a root or a child of a root at all times thereafter. We use the accounting method to obtain the O m  time bound. We charge a MAKE-SET operation two dollars. One dollar pays for the MAKE-SET, and one dollar remains on the node x that is created. The latter pays for the  cid:222 rst time that x appears on a  cid:222 nd path and is turned into a child of a root. We charge one dollar for a LINK operation. This dollar pays for the actual linking of one node to another. We charge one dollar for a FIND-SET. This dollar pays for visiting the root and its child, and for the path compression of these two nodes, during the FIND-SET. All other nodes on the  cid:222 nd path use their stored dollar to pay for their visitation and path compression. As mentioned, after the FIND-SET, all nodes on the  cid:222 nd path become children of a root  except for the root itself , and so whenever they are visited during a subsequent FIND-SET, the FIND-SET operation itself will pay for them. Since we charge each operation either one or two dollars, a sequence of m opera- tions is charged at most 2m dollars, and so the total time is O m . Observe that nothing in the above argument requires union by rank. Therefore, we get an O m  time bound regardless of whether we use union by rank.  Solution to Exercise 21.4-4  Clearly, each MAKE-SET and LINK operation takes O 1  time. Because the rank of a node is an upper bound on its height, each  cid:222 nd path has length O lg n , which in turn implies that each FIND-SET takes O lg n  time. Thus, any sequence of m MAKE-SET, LINK, and FIND-SET operations on n elements takes O m lg n  time. It is easy to prove an analogue of Lemma 21.7 to show that if we convert a MAKE-SET, UNION, and FIND-SET operations into a sequence of sequence of m m MAKE-SET, LINK, and FIND-SET operations that take O m lg n  time, then the lg n  sequence of m time.  MAKE-SET, UNION, and FIND-SET operations takes O m   cid:4    cid:4    cid:4   Solution to Exercise 21.4-5  Professor Dante is mistaken. Take the following scenario. Let n = 16, and make 16 separate singleton sets using MAKE-SET. Then do 8 UNION operations to link the sets into 8 pairs, where each pair has a root with rank 0 and a child with rank 1. Now do 4 UNIONs to link pairs of these trees, so that there are 4 trees, each with a root of rank 2, children of the root of ranks 1 and 0, and a node of rank 0 that is the child of the rank-1 node. Now link pairs of these trees together, so that there are two resulting trees, each with a root of rank 3 and each containing a path from a leaf to the root with ranks 0, 1, and 3. Finally, link these two trees together, so that   Solutions for Chapter 21: Data Structures for Disjoint Sets  21-9  there is a path from a leaf to the root with ranks 0, 1, 3, and 4. Let x and y be the nodes on this path with ranks 1 and 3, respectively. Since A1 1  = 3, level x  = 1, and since A0 3  = 4, level y  = 0. Yet y follows x on the  cid:222 nd path.  Solution to Exercise 21.4-6  First, α cid:4  22047 − 1  = min{k : Ak  1  ≥ 2047} = 3, and 22047 − 1  cid:28  1080. Second, we need that 0 ≤ level x  ≤ α cid:4  n  for all nonroots x with rank[x] ≥ 1. With this de cid:222 nition of α cid:4  n , we have Aα cid:4  n  rank[x]  ≥ Aα cid:4  n  1  ≥ lg n + 1  > lg n ≥ rank  p[x] . The rest of the proof goes through with α cid:4  n  replacing α n .  Solution to Problem 21-1  a. For the input sequence  4, 8, E, 3, E, 9, 2, 6, E, E, E, 1, 7, E, 5 ,  the values in the extracted array would be 4, 3, 2, 6, 8, 1. The following table shows the situation after the ith iteration of the for loop when we use OFF-LINE-MINIMUM on the same input.  For this input, n = 9 and m cid:151 the number of extractions cid:151 is 6 .  K3 {9, 2, 6} {9, 2, 6}  K2 {3} {3} {3}  K1 {4, 8} {4, 8} {4, 8} {4, 8}  K4 {} {}  {9, 2, 6} {9, 2, 6, 3} {9, 2, 6, 3, 4, 8} {9, 2, 6, 3, 4, 8}  K5 {} {} {} {} {} {}  K6 {1, 7}  {9, 2, 6, 3, 4, 8} {9, 2, 6, 3, 4, 8}  i  0 1 2 3 4 5 6 7 8  K7 {5} {5, 1, 7} {5, 1, 7} {5, 1, 7} {5, 1, 7} {5, 1, 7} {5, 1, 7} {5, 1, 7}  {5, 1, 7, 9, 2, 6, 3, 4, 8}  extracted  1 2 3 4 5 6  1 1 2 1 3 2 1 4 3 2 1 4 3 2 1 4 3 2 6 4 3 2 6 1 4 3 2 6 8 1  Because j = m + 1 in the iterations for i = 5 and i = 7, no changes occur in these iterations. b. We want to show that the array extracted returned by OFF-LINE-MINIMUM is correct, meaning that for i = 1, 2, . . . , m, extracted[ j] is the key returned by the jth EXTRACT-MIN call. We start with n INSERT operations and m EXTRACT-MIN operations. The smallest of all the elements will be extracted in the  cid:222 rst EXTRACT-MIN after its insertion. So we  cid:222 nd j such that the minimum element is in Kj , and put the minimum element in extracted[ j], which corresponds to the EXTRACT-MIN after the minimum element insertion. Now we reduce to a similar problem with n − 1 INSERT operations and m − 1 the INSERT operations are EXTRACT-MIN operations in the following way:   21-10  Solutions for Chapter 21: Data Structures for Disjoint Sets  the same but without the insertion of the smallest that was extracted, and the EXTRACT-MIN operations are the same but without the extraction that ex- tracted the smallest element. Conceptually, we unite Ij and I j+1, removing the extraction between them and also removing the insertion of the minimum element from Ij ∪ I j+1. Uniting I j and I j+1 is accomplished by line 6. We need to determine which set is Kl, rather than just using K j+1 unconditionally, because Kj+1 may have been destroyed when it was united into a higher-indexed set by a previous execution of line 6. Because we process extractions in increasing order of the minimum value found, the remaining iterations of the for loop correspond to solving the re- duced problem. There are two other points worth making. First, if the smallest remaining el- ement had been inserted after the last EXTRACT-MIN  i.e., j = m + 1 , then no changes occur, because this element is not extracted. Second, there may be smaller elements within the K j sets than the the one we are currently looking for. These elements do not affect the result, because they correspond to ele- ments that were already extracted, and their effect on the algorithm cid:146 s execution is over.  c. To implement this algorithm, we place each element in a disjoint-set forest. Each root has a pointer to its Ki set, and each Ki set has a pointer to the root of the tree representing it. All the valid sets Ki are in a linked list. Before OFF-LINE-MINIMUM, there is initialization that builds the initial sets Ki according to the Ii sequences.   Line 2   cid:147 determine j such that i ∈ Kj  cid:148   turns into j ← FIND-SET i  .   Line 5   cid:147 let l be the smallest value greater than j for which set Kl exists cid:148   turns into Kl ← next[K j ].   Line 6   cid:147 Kl ← K j ∪ Kl, destroying K j  cid:148   turns into l ← LINK  j, l  and remove K j from the linked list.  To analyze the running time, we note that there are n elements and that we have the following disjoint-set operations:   n MAKE-SET operations   at most n − 1 UNION operations before starting   n FIND-SET operations   at most n LINK operations Thus the number m of overall operations is O n . The total running time is O m α n   = O n α n  . [The cid:147 tightbound cid:148 wordingthatthisquestionusesdoesnotrefertoan cid:147 asymp- toticallytight cid:148 bound. Instead,thequestionismerelyaskingforaboundthatis nottoo cid:147 loose. cid:148 ]   Solutions for Chapter 21: Data Structures for Disjoint Sets  21-11  Solution to Problem 21-2  a. Denote the number of nodes by n, and let n =  m + 1  3, so that m = 3n − 1. First, perform the n operations MAKE-TREE v1 , MAKE-TREE v2 , . . . , MAKE-TREE vn . Then perform the sequence of n − 1 GRAFT operations GRAFT v1, v2 , GRAFT v2, v3 , . . . , GRAFT vn−1, vn ; this sequence produces a single disjoint-set tree that is a linear chain of n nodes with vn at the root and v1 as the only leaf. Then perform FIND-DEPTH v1  repeatedly, n times. The total number of operations is n +  n − 1  + n = 3n − 1 = m. Each MAKE-TREE and GRAFT operation takes O 1  time. Each FIND-DEPTH operation has to follow an n-node  cid:222 nd path, and so each of the n FIND-DEPTH operations takes  cid:2  n  time. The total time is n ·  cid:2  n  +  2n − 1  · O 1  =  cid:2  n2  =  cid:2  m2 .  b. MAKE-TREE is like MAKE-SET, except that it also sets the d value to 0:  MAKE-TREE v  p[v] ← v rank[v] ← 0 d[v] ← 0 It is correct to set d[v] to 0, because the depth of the node in the single-node disjoint-set tree is 0, and the sum of the depths on the  cid:222 nd path for v consists only of d[v].  c. FIND-DEPTH will call a procedure FIND-ROOT:  FIND-ROOT v  if p[v]  cid:19 = p[p[v]] then y ← p[v]  p[v] ← FIND-ROOT y  d[v] ← d[v] + d[y]  return p[v]  FIND-DEPTH v  FIND-ROOT v  if v = p[v] then return d[v] else return d[v] + d[p[v]]   cid:1  No need to save the return value.  FIND-ROOT performs path compression and updates pseudodistances along the  cid:222 nd path from v. It is similar to FIND-SET on page 508, but with three changes. First, when v is either the root or a child of a root  one of these conditions holds if and only if p[v] = p[p[v]]  in the disjoint-set forest, we don cid:146 t have to re- curse; instead, we just return p[v]. Second, when we do recurse, we save the pointer p[v] into a new variable y. Third, when we recurse, we update d[v] by adding into it the d values of all nodes on the  cid:222 nd path that are no longer proper   21-12  Solutions for Chapter 21: Data Structures for Disjoint Sets  ancestors of v after path compression; these nodes are precisely the proper an- cestors of v other than the root. Thus, as long as v does not start out the FIND- ROOT call as either the root or a child of the root, we add d[y] into d[v]. Note that d[y] has been updated prior to updating d[v], if y is also neither the root nor a child of the root. FIND-DEPTH  cid:222 rst calls FIND-ROOT to perform path compression and update pseudodistances. Afterward, the  cid:222 nd path from v consists of either just v  if v is a root  or just v and p[v]  if v is not a root, in which case it is a child of the root after path compression . In the former case, the depth of v is just d[v], and in the latter case, the depth is d[v] + d[p[v]].  d. Our procedure for GRAFT is a combination of UNION and LINK:   cid:4   GRAFT r, v   cid:4  ← FIND-ROOT r   r v cid:4  ← FIND-ROOT v  z ← FIND-DEPTH v  ] > rank[v cid:4  if rank[r ] ] ← r  cid:4  then p[v cid:4  ] + z + 1 ] ← d[r  cid:4   cid:4  d[r ] ← d[v cid:4  ] − d[r  cid:4  d[v cid:4  ] ] ← v cid:4   cid:4  else p[r ] + z + 1 − d[v cid:4  ] ← d[r  cid:4   cid:4  d[r  cid:4  if rank[r ] ← rank[v cid:4  then rank[v cid:4   ] = rank[v cid:4   ]  ] ] + 1   cid:4   .  and v cid:4   This procedure works as follows. First, we call FIND-ROOT on r and v in , respectively, of their trees in the disjoint-set order to  cid:222 nd the roots r forest. As we saw in part  c , these FIND-ROOT calls also perform path com- pression and update pseudodistances on the  cid:222 nd paths from r and v. We then call FIND-DEPTH v , saving the depth of v in the variable z.  Since we have just compressed v cid:146 s  cid:222 nd path, this call of FIND-DEPTH takes O 1  time.  Next,  cid:4  or v cid:4    of smaller rank a we emulate the action of LINK, by making the root  r a child of v cid:4   cid:4  child of the root of larger rank; in case of a tie, we make r If v cid:4  has the smaller rank, then all nodes in r cid:146 s tree will have their depths in- creased by the depth of v plus 1  because r is to become a child of v . Altering the psuedodistance of the root of a disjoint-set tree changes the computed depth of all nodes in that tree, and so adding z + 1 to d[r  cid:4  ] accomplishes this update for all nodes in r cid:146 s disjoint-set tree. Since v cid:4  in the disjoint-set forest, we have just increased the computed depth of all nodes in the disjoint-set tree rooted at v cid:4  ]. These computed depths should not  cid:4  ], so that the sum have changed, however. Thus, we subtract off d[r ] + d[r d[v cid:4   cid:4  equals d[v cid:4  a child  cid:4  . of r On the other hand, if r becomes a child of v cid:4  in the disjoint-set forest afterward, and we can leave d[v cid:4  a child of v cid:4  update d[r node in r cid:146 s disjoint-set tree is increased by z + 1. We add z + 1 to d[r  cid:4    cid:4   cid:4  has the smaller rank, or if the ranks are equal, then r in the disjoint-set forest. In this case, v cid:4  remains a root ] alone. We have to , the depth of each ], but we  ], however, so that after making r  ] before making v cid:4   ] after making v cid:4   will become a child of r  ] from d[v cid:4   a child of r  by d[r   cid:4    cid:4    cid:4    cid:4    cid:4    Solutions for Chapter 21: Data Structures for Disjoint Sets  21-13  also subtract out d[v cid:4  ranks of r procedure.  and v cid:4    cid:4   ], since we have just made r  are equal, we increment the rank of v cid:4   . Finally, if the , as is done in the LINK   cid:4   a child of v cid:4   e. The asymptotic running times of MAKE-TREE, FIND-DEPTH, and GRAFT are equivalent to those of MAKE-SET, FIND-SET, and UNION, respectively. Thus, a sequence of m operations, n of which are MAKE-TREE operations, takes  cid:2  m α n   time in the worst case.    Lecture Notes for Chapter 22: Elementary Graph Algorithms  Graph representation  Given graph G =  V, E .   May be either directed or undirected.   Two common ways to represent for algorithms:  1. Adjacency lists. 2. Adjacency matrix.  When expressing the running time of an algorithm, it cid:146 s often in terms of bothV and E. In asymptotic notation cid:151 and only in asymptotic notation cid:151 we cid:146 ll drop the cardinality. Example: O V + E . [TheintroductiontoPartVItalksmoreaboutthis.]  Adjacency lists Array Adj of V lists, one per vertex. Vertex u cid:146 s list has all vertices v such that  u, v  ∈ E.  Works for both directed and undirected graphs.   Example: For an undirected graph:  1  5  2  4  3  Adj  1 2 3 4 5  2 1 2 2 4  5 5 4 5 1  3  4  3 2  If edges have weights, can put the weights in the lists. Weight: w : E → R We cid:146 ll use weights later on for spanning trees and shortest paths. Space:  cid:2  V + E . Time: to list all vertices adjacent to u:  cid:2  degree u  . Time: to determine if  u, v  ∈ E: O degree u  .   22-2  Lecture Notes for Chapter 22: Elementary Graph Algorithms  Example: For a directed graph:  1  3  2  4  Adj  1 2 3 4  2 4 1  4  2  3  Same asymptotic space and time.  Adjacency matrix  cid:9  V × V matrix A =  ai j   1 if  i, j   ∈ E , ai j = 0 otherwise .  1 0 1 0 0 1  2 1 0 1 1 1  3 0 1 0 1 0  4 0 1 1 0 1  5 1 1 0 1 0  1 2 3 4 5  1 0 0 1 0  2 1 0 1 0  3 0 0 0 1  4 0 1 0 1  1 2 3 4  Space:  cid:2  V 2 . Time: to list all vertices adjacent to u:  cid:2  V  . Time: to determine if  u, v  ∈ E:  cid:2  1 . Can store weights instead of bits for weighted graph. We cid:146 ll use both representations in these lecture notes.  Breadth- cid:222 rst search  Input: Graph G =  V, E , either directed or undirected, and source vertex s ∈ V . Output: d[v] = distance  smallest  of edges  from s to v, for all v ∈ V .  In book, also π[v] = u such that  u, v  is last edge on shortest path s  cid:2  v.   u is v cid:146 s predecessor.    set of edges { π[v], v  : v  cid:19 = s} forms a tree.  Later, we cid:146 ll see a generalization of breadth- cid:222 rst search, with edge weights. For now, we cid:146 ll keep it simple.   Compute only d[v], not π[v]. [Seebookfor π[v].]   Omitting colors of vertices. [Usedinbooktoreasonaboutthealgorithm. We cid:146 ll  skipthemhere.]   Lecture Notes for Chapter 22: Elementary Graph Algorithms  22-3  Idea: Send a wave out from s.   First hits all vertices 1 edge from s.   From there, hits all vertices 2 edges from s.   Etc. Use FIFO queue Q to maintain wavefront.    v ∈ Q if and only if wave has hit v but has not come out of v yet.  BFS V, E, s  for each u ∈ V − {s} do d[u] ← ∞ d[s] ← 0 Q ← ∅ ENQUEUE Q, s  while Q  cid:19 = ∅  do u ← DEQUEUE Q  for each v ∈ Adj[u] do if d[v] = ∞  then d[v] ← d[u] + 1 ENQUEUE Q, v   s  a  b  0  1  3  1 c  e  2  f  g  h  3  2  3  i  3  Example: directed graph [undirectedexampleinbook] .  Can show that Q consists of vertices with d values.  i + 1 i + 1 . . .  i + 1  i  i  i  . . .  i   Only 1 or 2 values.    If 2, differ by 1 and all smallest are  cid:222 rst.  Since each vertex gets a  cid:222 nite d value at most once, values assigned to vertices are monotonically increasing over time. Actual proof of correctness is a bit trickier. See book. BFS may not reach all vertices. Time = O V + E .   O V   because every vertex enqueued at most once.   O E  because every vertex dequeued at most once and we examine  u, v  only when u is dequeued. Therefore, every edge examined at most once if directed, at most twice if undirected.   22-4  Lecture Notes for Chapter 22: Elementary Graph Algorithms  Depth- cid:222 rst search  Input: G =  V, E , directed or undirected. No source vertex given! Output: 2 timestamps on each vertex:    d[v] = discovery time f [v] =  cid:222 nishing time     These will be useful for other algorithms later on. Can also compute π[v]. [Seebook.]  Will methodically explore every edge.   Start over from different vertices as necessary. As soon as we discover a vertex, explore from it.   Unlike BFS, which puts a vertex on a queue so that we explore from it later. As DFS progresses, every vertex has a color:   WHITE = undiscovered   GRAY = discovered, but not  cid:222 nished  not done exploring from it   BLACK =  cid:222 nished  have found everything reachable from it      Discovery and  cid:222 nish times:   Unique integers from 1 to 2V.   For all v, d[v] < f [v]. In other words, 1 ≤ d[v] < f [v] ≤ 2V.  Pseudocode: Uses a global timestamp time.  do color[u] ← WHITE  DFS V, E  for each u ∈ V time ← 0 for each u ∈ V  do if color[u] = WHITE then DFS-VISIT u   DFS-VISIT u  color[u] ← GRAY time ← time+1 d[u] ← time for each v ∈ Adj[u]  color[u] ← BLACK time ← time+1 f [u] ← time   cid:1  discover u   cid:1   cid:222 nish u  do if color[v] = WHITE then DFS-VISIT v    cid:1  explore  u, v    Lecture Notes for Chapter 22: Elementary Graph Algorithms  22-5  Example: [Gothroughthisexample,addinginthed and f valuesasthey cid:146 recom- puted. Showcolorsastheychange. Don cid:146 tputintheedgetypesyet.]  2  7  T  T  1  12  B  3  4  T  d  f  T  C  9  10  C  C  8  11  C  65  C  13  16  T  14  15  T  F  C  Time =  cid:2  V + E .   Similar to BFS analysis.    cid:2 , not just O, since guaranteed to examine every vertex and edge. DFS forms a depth- cid:222 rst forest comprised of > 1 depth- cid:222 rst trees. Each tree is made of edges  u, v  such that u is gray and v is white when  u, v  is explored.  Theorem  Parenthesis theorem  [Proofomitted.] For all u, v, exactly one of the following holds:  1. d[u] < f [u] < d[v] < f [v] or d[v] < f [v] < d[u] < f [u] and neither of u  and v is a descendant of the other.  2. d[u] < d[v] < f [v] < f [u] and v is a descendant of u. 3. d[v] < d[u] < f [u] < f [v] and u is a descendant of v.  So d[u] < d[v] < f [u] < f [v] cannot happen. Like parentheses:   OK:   Not OK:    [ ]   [   ]        [ ]   [   ]  [     ]  Corollary v is a proper descendant of u if and only if d[u] < d[v] < f [v] < f [u].  Theorem  White-path theorem  [Proofomitted.] v is a descendant of u if and only if at time d[u], there is a path u  cid:2  v consisting of only white vertices.  Except for u, which was just colored gray.    22-6  Lecture Notes for Chapter 22: Elementary Graph Algorithms  Classi cid:222 cation of edges    Tree edge: in the depth- cid:222 rst forest. Found by exploring  u, v .   Back edge:  u, v , where u is a descendant of v.   Forward edge:  u, v , where v is a descendant of u, but not a tree edge.   Cross edge: any other edge. Can go between vertices in same depth- cid:222 rst tree  or in different depth- cid:222 rst trees.  [Nowlabeltheexamplefromabovewithedgetypes.] In an undirected graph, there may be some ambiguity since  u, v  and  v, u  are the same edge. Classify by the  cid:222 rst type above that matches.  Theorem [Proofomitted.] In DFS of an undirected graph, we get only tree and back edges. No forward or cross edges.  Topological sort  Directed acyclic graph  dag   a > b and b > c ⇒ a > c.  A directed graph with no cycles. Good for modeling processes and structures that have a partial order:     But may have a and b such that neither a > b nor b > c. Can always make a total order  either a > b or b > a for all a  cid:19 = b  from a partial order. In fact, that cid:146 s what a topological sort will do.  Example: dag of dependencies for putting on goalie equipment: [Leaveonboard, butshowwithoutdiscoveryand cid:222 nishtimes. Willputtheminlater.]  25 26  socks  15 24  shorts  7 14  T-shirt  1 6  batting glove  16 23  hose  8 13  chest pad  17 22  pants  18 21  skates  19 20  leg pads  9 12  sweater  10 11  mask  catch glove  2 5  3 4  blocker   Lecture Notes for Chapter 22: Elementary Graph Algorithms  22-7  Lemma A directed graph G is acyclic if and only if a DFS of G yields no back edges. Proof ⇒ : Show that back edge ⇒ cycle. Suppose there is a back edge  u, v . Then v is ancestor of u in depth- cid:222 rst forest.  B  v  T  T  T  u  Therefore, there is a path v  cid:2  u, so v  cid:2  u → v is a cycle. ⇐ : Show that cycle ⇒ back edge. Suppose G contains cycle c. Let v be the  cid:222 rst vertex discovered in c, and let  u, v  be the preceding edge in c. At time d[v], vertices of c form a white path v  cid:2  u  since v is the  cid:222 rst vertex discovered in c . By white-path theorem, u is descendant of v in depth- cid:222 rst forest. Therefore,  u, v  is a back edge.  lemma  Topological sort of a dag: a linear ordering of vertices such that if  u, v  ∈ E, then u appears somewhere before v.  Not like sorting numbers.   TOPOLOGICAL-SORT V, E  call DFS V, E  to compute  cid:222 nishing times f [v] for all v ∈ V output vertices in order of decreasing  cid:222 nish times  Don cid:146 t need to sort by  cid:222 nish times.   Can just output vertices as they cid:146 re  cid:222 nished and understand that we want the  reverse of this list.    Or put them onto the front of a linked list as they cid:146 re  cid:222 nished. When done, the  list contains vertices in topologically sorted order.  Time:  cid:2  V + E . Do example. [Nowwritediscoveryand cid:222 nishtimesingoalieequipmentexample.]   22-8  Lecture Notes for Chapter 22: Elementary Graph Algorithms  socks shorts hose pants skates leg pads t-shirt chest pad sweater  Order: 26 24 23 22 21 20 14 13 12 11 mask 6 5 4 Correctness: Just need to show if  u, v  ∈ E, then f [v] < f [u]. When we explore  u, v , what are the colors of u and v?      u is gray. Is v gray, too?   No, because then v would be ancestor of u.  batting glove catch glove blocker  ⇒  u, v  is a back edge. ⇒ contradiction of previous lemma  dag has no back edges .  Is v white?   Then becomes descendant of u.  By parenthesis theorem, d[u] < d[v] < f [v] < f [u].        Is v black?   Then v is already  cid:222 nished.  Since we cid:146 re exploring  u, v , we have not yet  cid:222 nished u. Therefore, f [v] < f [u].  Strongly connected components  Given directed graph G =  V, E . A strongly connected component  SCC  of G is a maximal set of vertices C ⊆ V such that for all u, v ∈ C, both u  cid:2  v and v  cid:2  u.  Example: [JustshowSCC cid:146 sat cid:222 rst. DoDFSalittlelater.]  14 19  15 16  17 18  13 20  3 4  2 5  1 12  10 11  6 9  7 8   Lecture Notes for Chapter 22: Elementary Graph Algorithms  22-9  Algorithm uses GT = transpose of G.   GT =  V, E T , E T = { u, v  :  v, u  ∈ E}.   GT is G with all edges reversed. Can create GT in  cid:2  V + E  time if using adjacency lists. Observation: G and GT have the same SCC cid:146 s.  u and v are reachable from each other in G if and only if reachable from each other in GT.   Component graph   GSCC =  V SCC, E SCC .   V SCC has one vertex for each SCC in G.   E SCC has an edge if there cid:146 s an edge between the corresponding SCC cid:146 s in G. For our example:   cid:4    cid:4   be distinct SCC cid:146 s in G, let u, v ∈ C,  cid:4  , and suppose there is a path u  cid:2  u in G. Then there cannot also be a  Lemma GSCC is a dag. More formally, let C and C  cid:4 , v cid:4  ∈ C u path v cid:4   cid:2  v in G. Proof Suppose there is a path v cid:4   cid:2  v in G. Then there are paths u  cid:2  u v cid:4   cid:2  v  cid:2  u in G. Therefore, u and v cid:4  not in separate SCC cid:146 s.  and are reachable from each other, so they are  lemma    cid:4   cid:2  v cid:4   SCC G  call DFS G  to compute  cid:222 nishing times f [u] for all u compute GT call DFS GT , but in the main loop, consider vertices in order of decreasing f [u]   as computed in  cid:222 rst DFS   output the vertices in each tree of the depth- cid:222 rst forest formed in second DFS  as a separate SCC  Example: 1. Do DFS 2. GT 3. DFS  roots blackened   Time:  cid:2  V + E . How can this possibly work?   22-10  Lecture Notes for Chapter 22: Elementary Graph Algorithms  Idea: By considering vertices in second DFS in decreasing order of  cid:222 nishing times from  cid:222 rst DFS, we are visiting vertices of the component graph in topological sort order. To prove that it works,  cid:222 rst deal with 2 notational issues:   Will be discussing d[u] and f [u]. These always refer to  cid:222 rst DFS.   Extend notation for d and f to sets of vertices U ⊆ V :    d U   = minu∈U {d[u]}  earliest discovery time  f  U   = maxu∈U { f [u]}  latest  cid:222 nishing time      Lemma Let C and C such that u ∈ C and v ∈ C   cid:4    cid:4   .  be distinct SCC cid:146 s in G =  V, E . Suppose there is an edge  u, v  ∈ E  C  u  C′  v  Then f  C  > f  C   cid:4  .   cid:4    cid:4      .  are descendants of x in  Proof Two cases, depending on which SCC had the  cid:222 rst discovered vertex during the  cid:222 rst DFS.     cid:4  , let x be the  cid:222 rst vertex discovered in C. At time d[x], all are white. Thus, there exist paths of white vertices from x  If d C  < d C vertices in C and C to all vertices in C and C  cid:4  By the white-path theorem, all vertices in C and C depth- cid:222 rst tree. By the parenthesis theorem, f [x] = f  C  > f  C  cid:4  . If d C  > d C vertices in C all vertices in C At time d[y], all vertices in C are white.  cid:4  By earlier lemma, since there is an edge  u, v , we cannot have a path from C to C. So no vertex in C is reachable from y. Therefore, at time f [y], all vertices in C are still white. Therefore, for all w ∈ C, f [w] > f [y], which implies that f  C  > f  C  cid:4  .   cid:4  , let y be the  cid:222 rst vertex discovered in C  cid:4  are white and there is a white path from y to each vertex in C become descendants of y. Again, f [y] = f  C  cid:4   cid:4  .  . At time d[y], all  cid:4  ⇒   cid:4    lemma    cid:4   Corollary be distinct SCC cid:146 s in G =  V, E . Suppose there is an edge Let C and C  u, v  ∈ E T, where u ∈ C and v ∈ C Proof  u, v  ∈ ET ⇒  v, u  ∈ E. Since SCC cid:146 s of G and GT are the same, f  C  corollary   . Then f  C  < f  C   cid:4   > f  C .   cid:4  .   cid:4    Lecture Notes for Chapter 22: Elementary Graph Algorithms  22-11  Corollary Let C and C  cid:4  Then there cannot be an edge from C to C   cid:4   be distinct SCC cid:146 s in G =  V, E , and suppose that f  C  > f  C  cid:4  .  in GT.  Proof It cid:146 s the contrapositive of the previous corollary.   cid:4    cid:4   for all C  Now we have the intuition to understand why the SCC procedure works. When we do the second DFS, on GT, start with SCC C such that f  C  is maximum. The second DFS starts from some x ∈ C, and it visits all vertices in C. Corollary  cid:19 = C, there are no edges from C to C  cid:4  says that since f  C  > f  C in GT. Therefore, DFS will visit only vertices in C. Which means that the depth- cid:222 rst tree rooted at x contains exactly the vertices of C.  cid:4   cid:4   is maximum The next root chosen in the second DFS is in SCC C  cid:4  , but the only edges out over all SCC cid:146 s other than C. DFS visits all vertices in C of C  cid:4  Therefore, the only tree edges will be to vertices in C We can continue the process. Each time we choose a root for the second DFS, it can reach only      vertices in its SCC cid:151 get tree edges to these, vertices in SCC cid:146 s already visited in second DFS cid:151 get no tree edges to these.  go to C, which we cid:146 ve already visited.  such that f  C  .   cid:4   We are visiting vertices of  GT SCC in reverse of topologically sorted order. [Thebookhasaformalproof.]   Solutions for Chapter 22: Elementary Graph Algorithms  Solution to Exercise 22.1-6  We start by observing that if ai j = 1, so that  i, j   ∈ E, then vertex i cannot be a universal sink, for it has an outgoing edge. Thus, if row i contains a 1, then vertex i cannot be a universal sink. This observation also means that if there is a self-loop  i, i  , then vertex i is not a universal sink. Now suppose that ai j = 0, so that  i, j    cid:19 ∈ E, and also that i  cid:19 = j. Then vertex j cannot be a universal sink, for either its in-degree must be strictly less than V − 1 or it has a self-loop. Thus if column j contains a 0 in any position other than the diagonal entry   j, j  , then vertex j cannot be a universal sink. Using the above observations, the following procedure returns TRUE if vertex k is a universal sink, and FALSE otherwise. It takes as input aV × V adjacency matrix A =  ai j  . IS-SINK A, k  let A be V × V for j ← 1 to V do if akj = 1 for i ← 1 to V   cid:1  Check for an off-diagonal 0 in column k   cid:1  Check for a 1 in row k  then return FALSE do if aik = 0 and i  cid:19 = k then return FALSE  return TRUE  Because this procedure runs in O V   time, we may call it only O 1  times in order to achieve our O V  -time bound for determining whether directed graph G contains a universal sink. Observe also that a directed graph can have at most one universal sink. This prop- erty holds because if vertex j is a universal sink, then we would have  i, j   ∈ E for all i  cid:19 = j and so no other vertex i could be a universal sink. The following procedure takes an adjacency matrix A as input and returns either a message that there is no universal sink or a message containing the identity of the universal sink. It works by eliminating all but one vertex as a potential universal sink and then checking the remaining candidate vertex by a single call to IS-SINK.   Solutions for Chapter 22: Elementary Graph Algorithms  22-13  UNIVERSAL-SINK A  let A be V × V i ← j ← 1 while i ≤ V and j ≤ V then i ← i + 1 else j ← j + 1  do if ai j = 1  s ← 0 if i > V then return  cid:147 there is no universal sink cid:148  elseif IS-SINK A, i   = FALSE then return  cid:147 there is no universal sink cid:148   else return i  cid:147 is a universal sink cid:148   UNIVERSAL-SINK walks through the adjacency matrix, starting at the upper left corner and always moving either right or down by one position, depending on whether the current entry ai j it is examining is 0 or 1. It stops once either i or j exceeds V. To understand why UNIVERSAL-SINK works, we need to show that after the while loop terminates, the only vertex that might be a universal sink is vertex i. The call to IS-SINK then determines whether vertex i is indeed a universal sink. Let us  cid:222 x i and j to be values of these variables at the termination of the while loop. We claim that every vertex k such that 1 ≤ k < i cannot be a universal sink. That is because the way that i achieved its  cid:222 nal value at loop termination was by  cid:222 nding a 1 in each row k for which 1 ≤ k < i. As we observed above, any vertex k whose row contains a 1 cannot be a universal sink. If i > V at loop termination, then we have eliminated all vertices from consid- eration, and so there is no universal sink. If, on the other hand, i ≤ V at loop termination, we need to show that every vertex k such that i < k ≤ V cannot be a universal sink. If i ≤ V at loop termination, then the while loop terminated because j > V. That means that we found a 0 in every column. Recall our earlier observation that if column k contains a 0 in an off-diagonal position, then vertex k cannot be a universal sink. Since we found a 0 in every column, we found a 0 in every column k such that i < k ≤ V. Moreover, we never examined any matrix entries in rows greater than i, and so we never examined the diagonal entry in any column k such that i < k ≤ V. Therefore, all the 0s that we found in columns k such that i < k ≤ V were off-diagonal. We conclude that every vertex k such that i < k ≤ V cannot be a universal sink. Thus, we have shown that every vertex less than i and every vertex greater than i cannot be a universal sink. The only remaining possibility is that vertex i might be a universal sink, and the call to IS-SINK checks whether it is. To see that UNIVERSAL-SINK runs in O V   time, observe that either i or j is incremented in each iteration of the while loop. Thus, the while loop makes at most 2V − 1 iterations. Each iteration takes O 1  time, for a total while loop time of O V   and, combined with the O V  -time call to IS-SINK, we get a total running time of O V  .   22-14  Solutions for Chapter 22: Elementary Graph Algorithms  Solution to Exercise 22.1-7   cid:2   e∈E   cid:2   e∈E  B B T  i, j   =  =  biebT ej  bieb j e        If i = j, then bieb j e = 1  it is 1 · 1 or  −1  ·  −1   whenever e enters or leaves vertex i, and 0 otherwise. If i  cid:19 = j, then bieb j e = −1 when e =  i, j   or e =   j, i  , and 0 otherwise.  Thus, B B T  i, j   =   cid:9  degree of i = in-degree + out-degree −  of edges connecting i and j    if i = j , if i  cid:19 = j .  Solution to Exercise 22.2-4  The correctness proof for the BFS algorithm shows that d[u] = δ s, u , and the algorithm doesn cid:146 t assume that the adjacency lists are in any particular order. In Figure 22.3, if t precedes x in Adj[w], we can get the breadth- cid:222 rst tree shown in the  cid:222 gure. But if x precedes t in Adj[w] and u precedes y in Adj[x], we can get edge  x, u  in the breadth- cid:222 rst tree.  Solution to Exercise 22.2-5  The edges in Eπ are shaded in the following graph:  s  u  v  w  x  To see that Eπ cannot be a breadth- cid:222 rst tree, let cid:146 s suppose that Adj[s] contains u before v. BFS adds edges  s, u  and  s, v  to the breadth- cid:222 rst tree. Since u is enqueued before v, BFS then adds edges  u, w  and  u, x .  The order of w and x in Adj[u] doesn cid:146 t matter.  Symmetrically, if Adj[s] contains v before u, then BFS adds edges  s, v  and  s, u  to the breadth- cid:222 rst tree, v is enqueued before u, and BFS adds edges  v, w  and  v, x .  Again, the order of w and x in Adj[v] doesn cid:146 t matter.  BFS will never put both edges  u, w  and  v, x  into the breadth- cid:222 rst tree. In fact, it will also never put both edges  u, x  and  v, w  into the breadth- cid:222 rst tree.  Solution to Exercise 22.2-6  Create a graph G where each vertex represents a wrestler and each edge represents a rivalry. The graph will contain n vertices and r edges.   Solution to Exercise 22.3-4  Solutions for Chapter 22: Elementary Graph Algorithms  22-15  Perform as many BFS cid:146 s as needed to visit all vertices. Assign all wrestlers whose distance is even to be good guys and all wrestlers whose distance is odd to be bad guys. Then check each edge to verify that it goes between a good guy and a bad guy. This solution would take O n + r   time for the BFS, O n  time to designate each wrestler as a good guy or bad guy, and O r   time to check edges, which is O n + r   time overall.  a. Edge  u, v  is a tree edge or forward edge if and only if v is a descendant of u in the depth- cid:222 rst forest.  If  u, v  is a back edge, then u is a descendant of v, and if  u, v  is a cross edge, then neither of u or v is a descendant of the other.  By Corollary 22.8, therefore,  u, v  is a tree edge or forward edge if and only if d[u] < d[v] < f [v] < f [u]. b. First, suppose that  u, v  is a back edge. A self-loop is by de cid:222 nition a back edge. If  u, v  is a self-loop, then clearly d[v] = d[u] < f [u] = f [v]. If  u, v  is not a self-loop, then u is a descendant of v in the depth- cid:222 rst forest, and by Corollary 22.8, d[v] < d[u] < f [u] < f [v]. Now, suppose that d[v] ≤ d[u] < f [u] ≤ f [v]. If u and v are the same vertex, then d[v] = d[u] < f [u] = f [v], and  u, v  is a self-loop and hence a back edge. If u and v are distinct, then d[v] < d[u] < f [u] < f [v]. By Theorem 22.7, interval [d[u], f [u]] is contained entirely within the interval [d[v], f [v]], and u is a descendant of v in a depth- cid:222 rst tree. Thus,  u, v  is a back edge.  c. First, suppose that  u, v  is a cross edge. Since neither u nor v is an ancestor of the other, Theorem 22.7 says that the intervals [d[u], f [u]] and [d[v], f [v]] are entirely disjoint. Thus, we must have either d[u] < f [u] < d[v] < f [v] or d[v] < f [v] < d[u] < f [u]. We claim that we cannot have d[u] < d[v] if  u, v  is a cross edge. Why? If d[u] < d[v], then v is white at time d[u]. By Theorem 22.9, v is a descendant of u, which contradicts  u, v  being a cross edge. Thus, we must have d[v] < f [v] < d[u] < f [u]. Now suppose that d[v] < f [v] < d[u] < f [u]. By Theorem 22.7, neither u nor v is a descendant of the other, which means that  u, v  must be a cross edge.  Solution to Exercise 22.3-7  Let us consider the example graph and depth- cid:222 rst search below.  d w 1 2 u v 4  f 6 3 5  w  u  v   22-16  Solutions for Chapter 22: Elementary Graph Algorithms  Clearly, there is a path from u to v in G. The bold edges are in the depth- cid:222 rst forest produced. We can see that d[u] < d[v] in the depth- cid:222 rst search but v is not a descendant of u in the forest.  Solution to Exercise 22.3-8  Let us consider the example graph and depth- cid:222 rst search below.  w  u  v  Clearly, there is a path from u to v in G. The bold edges of G are in the depth- cid:222 rst forest produced by the search. However, d[v] > f [u] and the conjecture is false.  Solution to Exercise 22.3-10  Let us consider the example graph and depth- cid:222 rst search below.  w  u  v  Clearly u has both incoming and outgoing edges in G but a depth- cid:222 rst search of G produced a depth- cid:222 rst forest where u is in a tree by itself.  Solution to Exercise 22.3-11  Compare the following pseudocode to the pseudocode of DFS on page 541 of the book. Changes were made in order to assign the desired cc label to vertices.  d w 1 2 u v 4  d w 1 3 u v 5  f 6 3 5  f 2 4 6  DFS G  for each vertex u ∈ V [G] do color[u] ← WHITE  π[u] ← NIL  time ← 0 counter ← 0 for each vertex u ∈ V [G] do if color[u] = WHITE  then counter ← counter +1 DFS-VISIT u, counter    Solutions for Chapter 22: Elementary Graph Algorithms  22-17   cid:1  Label the vertex.  DFS-VISIT u, counter  color[u] ← GRAY cc[u] ← counter time ← time+1 d[u] ← time for each v ∈ Adj[u]  do if color[v] = WHITE  then π[v] ← u  DFS-VISIT v, counter   color[u] ← BLACK f [u] ← time ← time+1 This DFS increments a counter each time DFS-VISIT is called to grow a new tree in the DFS forest. Every vertex visited  and added to the tree  by DFS-VISIT is labeled with that same counter value. Thus cc[u] = cc[v] if and only if u and v are visited in the same call to DFS-VISIT from DFS, and the  cid:222 nal value of the counter is the number of calls that were made to DFS-VISIT by DFS. Also, since every vertex is visited eventually, every vertex is labeled. Thus all we need to show is that the vertices visited by each call to DFS-VISIT from DFS are exactly the vertices in one connected component of G.   All vertices in a connected component are visited by one call to DFS-VISIT  from DFS: Let u be the  cid:222 rst vertex in component C visited by DFS-VISIT. Since a vertex becomes non-white only when it is visited, all vertices in C are white when DFS-VISIT is called for u. Thus, by the white-path theorem, all vertices in C become descendants of u in the forest, which means that all vertices in C are visited  by recursive calls to DFS-VISIT  before DFS-VISIT returns to DFS.   All vertices visited by one call to DFS-VISIT from DFS are in the same con-  nected component: If two vertices are visited in the same call to DFS-VISIT from DFS, they in the same connected component, because vertices are visited only by following paths in G  by following edges found in adjacency lists, starting from some vertex .  Solution to Exercise 22.4-3  An undirected graph is acyclic  i.e., a forest  if and only if a DFS yields no back edges.      If there cid:146 s a back edge, there cid:146 s a cycle. If there cid:146 s no back edge, then by Theorem 22.10, there are only tree edges. Hence, the graph is acyclic.  Thus, we can run DFS: if we  cid:222 nd a back edge, there cid:146 s a cycle.   Time: O V  .  Not O V + E !   If we ever see V distinct edges, we must have seen a back edge because  by Theorem B.2 on p. 1085  in an acyclic  undirected  forest,E ≤ V − 1.   22-18  Solutions for Chapter 22: Elementary Graph Algorithms  Solution to Exercise 22.4-5  TOPOLOGICAL-SORT G   cid:1  Initialize in-degree,  cid:2  V   time for each vertex u ∈ V  cid:1  Compute in-degree,  cid:2  V + E  time for each vertex u ∈ V  do in-degree[u] ← 0  do for each v ∈ Adj[u]  do in-degree[v] ← in-degree[v] + 1   cid:1  Initialize Queue,  cid:2  V   time Q ← ∅ for each vertex u ∈ V  do if in-degree[u] = 0  then ENQUEUE Q, u    cid:1  while loop takes O V + E  time while Q  cid:19 = ∅  do u ← DEQUEUE Q   output u  cid:1  for loop executes O E  times total for each v ∈ Adj[u]  do in-degree[v] ← in-degree[v] − 1  if in-degree[v] = 0 then ENQUEUE Q, v    cid:1  Check for cycles, O V   time for each vertex u ∈ V  do if in-degree[u]  cid:19 = 0  then report that there cid:146 s a cycle   cid:1  Another way to check for cycles would be to count the vertices  cid:1  that are output and report a cycle if that number is <V.  To  cid:222 nd and output vertices of in-degree 0, we  cid:222 rst compute all vertices cid:146  in-degrees by making a pass through all the edges  by scanning the adjacency lists of all the vertices  and incrementing the in-degree of each vertex an edge enters.   This takes  cid:2  V + E  time  V adjacency lists accessed, E edges total found  in those lists,  cid:2  1  work for each edge .  We keep the vertices with in-degree 0 in a FIFO queue, so that they can be en- queued and dequeued in O 1  time.  The order in which vertices in the queue are processed doesn cid:146 t matter, so any kind of queue works.     Initializing the queue takes one pass over the vertices doing  cid:2  1  work, for total time  cid:2  V  .  As we process each vertex from the queue, we effectively remove its outgoing edges from the graph by decrementing the in-degree of each vertex one of those edges enters, and we enqueue any vertex whose in-degree goes to 0. There cid:146 s   Solutions for Chapter 22: Elementary Graph Algorithms  22-19  no need to actually remove the edges from the adjacency list, because that ad- jacency list will never be processed again by the algorithm: Each vertex is en- queued dequeued at most once because it is enqueued only if it starts out with in-degree 0 or if its in-degree becomes 0 after being decremented  and never incre- mented  some number of times.   The processing of a vertex from the queue happens O V   times because no vertex can be enqueued more than once. The per-vertex work  dequeue and output  takes O 1  time, for a total of O V   time.    Because the adjacency list of each vertex is scanned only when the vertex is dequeued, the adjacency list of each vertex is scanned at most once. Since the sum of the lengths of all the adjacency lists is  cid:2  E , at most O E  time is spent in total scanning adjacency lists. For each edge in an adjacency list,  cid:2  1  work is done, for a total of O E  time.  Thus the total time taken by the algorithm is O V + E . The algorithm outputs vertices in the right order  u before v for every edge  u, v   because v will not be output until its in-degree becomes 0, which happens only when every edge  u, v  leading into v has been  cid:147 removed cid:148  due to the processing  including output  of u. If there are no cycles, all vertices are output.   Proof: Assume that some vertex v0 is not output. v0 cannot start out with in- degree 0  or it would be output , so there are edges into v0. Since v0 cid:146 s in-degree never becomes 0, at least one edge  v1, v0  is never removed, which means that at least one other vertex v1 was not output. Similarly, v1 not output means that some vertex v2 such that  v2, v1  ∈ E was not output, and so on. Since the number of vertices is  cid:222 nite, this path  ··· → v2 → v1 → v0  is  cid:222 nite, so we must have vi = v j for some i and j in this sequence, which means there is a cycle.  If there are cycles, not all vertices will be output, because some in-degrees never become 0.   Proof: Assume that a vertex in a cycle is output  its in-degree becomes 0 . Let v be the  cid:222 rst vertex in its cycle to be output, and let u be v cid:146 s predecessor in the cycle. In order for v cid:146 s in-degree to become 0, the edge  u, v  must have been  cid:147 removed, cid:148  which happens only when u is processed. But this cannot have happened, because v is the  cid:222 rst vertex in its cycle to be processed. Thus no vertices in cycles are output.  Solution to Exercise 22.5-5  We have at our disposal an O V + E -time algorithm that computes strongly con- nected components. Let us assume that the output of this algorithm is a mapping scc[u], giving the number of the strongly connected component containing ver- tex u, for each vertex u. Without loss of generality, assume that scc[u] is an integer in the set {1, 2, . . . ,V}.   22-20  Solutions for Chapter 22: Elementary Graph Algorithms  Construct the multiset  a set that can contain the same object more than once  T = {scc[u] : u ∈ V}, and sort it by using counting sort. Since the values we are sorting are integers in the range 1 toV, the time to sort is O V  . Go through the sorted multiset T and every time we  cid:222 nd an element x that is distinct from the one before it, add x to V SCC.  Consider the  cid:222 rst element of the sorted set as  cid:147 distinct from the one before it. cid:148   It takes O V   time to construct VSCC. Construct the set of ordered pairs S = { x, y  : there is an edge  u, v  ∈ E, x = scc[u], and y = scc[v]} . We can easily construct this set in  cid:2  E  time by going through all edges in E and looking up scc[u] and scc[v] for each edge  u, v  ∈ E. Having constructed S, remove all elements of the form  x, x . Alternatively, when we construct S, do not put an element in S when we  cid:222 nd an edge  u, v  for which scc[u] = scc[v]. S now has at most E elements. Now sort the elements of S using radix sort. Sort on one component at a time. The order does not matter. In other words, we are performing two passes of counting sort. The time to do so is O V + E , since the values we are sorting on are integers in the range 1 to V. Finally, go through the sorted set S, and every time we  cid:222 nd an element  x, y  that is distinct from the element before it  again considering the  cid:222 rst element of the sorted set as distinct from the one before it , add  x, y  to ESCC. Sorting and then adding  x, y  only if it is distinct from the element before it ensures that we add  x, y  at most once. It takes O E  time to go through S in this way, once S has been sorted. The total time is O V + E .  The basic idea is to replace the edges within each SCC by one simple, directed cycle and then remove redundant edges between SCC cid:146 s. Since there must be at least k edges within an SCC that has k vertices, a single directed cycle of k edges gives the k-vertex SCC with the fewest possible edges. The algorithm works as follows: 1. Identify all SCC cid:146 s of G. Time:  cid:2  V + E , using the SCC algorithm in Sec- 2. Form the component graph GSCC. Time: O V + E , by Exercise 22.5-5. 3. Start with E  cid:4  4. For each SCC of G, let the vertices in the SCC be v1, v2, . . . , vk, and add to E the directed edges  v1, v2 ,  v2, v3 , . . . ,  vk−1, vk ,  vk , v1 . These edges form a simple, directed cycle that includes all vertices of the SCC. Time for all SCC cid:146 s: O V  .   cid:4  = ∅. Time: O 1 .  tion 22.5.  5. For each edge  u, v  in the component graph GSCC, select any vertex x in u cid:146 s  cid:4  SCC and any vertex y in v cid:146 s SCC, and add the directed edge  x, y  to E . Time: O E .  Thus, the total time is  cid:2  V + E .  Solution to Exercise 22.5-6   Solutions for Chapter 22: Elementary Graph Algorithms  22-21  Solution to Exercise 22.5-7  To determine if G =  V, E  is semiconnected, do the following: 1. Call STRONGLY-CONNECTED-COMPONENTS. 2. Form the component graph.  By Exercise 22.5-5, you may assume that this  takes O V + E  time.  3. Topologically sort the component graph.  Recall that it cid:146 s a dag.  Assuming that there are k SCC cid:146 s, the topological sort gives a linear ordering  cid:2 v1, v2, . . . , vk cid:3  of the vertices. 4. Verify that the sequence of vertices  cid:2 v1, v2, . . . , vk cid:3  given by topological sort forms a linear chain in the component graph. That is, verify that the edges  v1, v2 ,  v2, v3 , . . . ,  vk−1, vk   exist in the component graph. If the vertices form a linear chain, then the original graph is semiconnected; otherwise it is not.  Because we know that all vertices in each SCC are mutually reachable from each other, it suf cid:222 ces to show that the component graph is semiconnected if and only if it contains a linear chain. We must also show that if there cid:146 s a linear chain in the component graph, it cid:146 s the one returned by topological sort. We cid:146 ll  cid:222 rst show that if there cid:146 s a linear chain in the component graph, then it cid:146 s the one returned by topological sort. In fact, this is trivial. A topological sort has to respect every edge in the graph. So if there cid:146 s a linear chain, a topological sort must give us the vertices in order. Now we cid:146 ll show that the component graph is semiconnected if and only if it con- tains a linear chain. First, suppose that the component graph contains a linear chain. Then for every pair of vertices u, v in the component graph, there is a path between them. If u precedes v in the linear chain, then there cid:146 s a path u  cid:2  v. Otherwise, v precedes u, and there cid:146 s a path v  cid:2  u. Conversely, suppose that the component graph does not contain a linear chain. Then in the list returned by topological sort, there are two consecutive vertices vi and vi+1, but the edge  vi , vi+1  is not in the component graph. Any edges out of vi are to vertices vj , where j > i + 1, and so there is no path from vi to vi+1 in the component graph. And since vi+1 follows vi in the topological sort, there cannot be any paths at all from vi+1 to vi . Thus, the component graph is not semiconnected. Running time of each step: 1.  cid:2  V + E . 2. O V + E . 3. Since the component graph has at most V vertices and at most E edges, O V + E . 4. Also O V + E . We just check the adjacency list of each vertex vi in the component graph to verify that there cid:146 s an edge  vi , vi+1 . We cid:146 ll go through each adjacency list once.  Thus, the total running time is  cid:2  V + E .   22-22  Solutions for Chapter 22: Elementary Graph Algorithms  Solution to Problem 22-1  a. 1. Suppose  u, v  is a back edge or a forward edge in a BFS of an undirected graph. Then one of u and v, say u, is a proper ancestor of the other  v  in the breadth- cid:222 rst tree. Since we explore all edges of u before exploring any edges of any of u cid:146 s descendants, we must explore the edge  u, v  at the time we explore u. But then  u, v  must be a tree edge. 2. In BFS, an edge  u, v  is a tree edge when we set π[v] ← u. But we only do so when we set d[v] ← d[u] + 1. Since neither d[u] nor d[v] ever changes thereafter, we have d[v] = d[u] + 1 when BFS completes.  3. Consider a cross edge  u, v  where, without loss of generality, u is visited before v. At the time we visit u, vertex v must already be on the queue, for otherwise  u, v  would be a tree edge. Because v is on the queue, we have d[v] ≤ d[u] + 1 by Lemma 22.3. By Corollary 22.4, we have d[v] ≥ d[u]. Thus, either d[v] = d[u] or d[v] = d[u] + 1.  b. 1. Suppose  u, v  is a forward edge. Then we would have explored it while  visiting u, and it would have been a tree edge.  2. Same as for undirected graphs. 3. For any edge  u, v , whether or not it cid:146 s a cross edge, we cannot have d[v] > d[u] + 1, since we visit v at the latest when we explore edge  u, v . Thus, d[v] ≤ d[u] + 1. 4. Clearly, d[v] ≥ 0 for all vertices v. For a back edge  u, v , v is an ancestor of u in the breadth- cid:222 rst tree, which means that d[v] ≤ d[u].  Note that since self-loops are considered to be back edges, we could have u = v.   a. An Euler tour is a single cycle that traverses each edge of G exactly once, but it might not be a simple cycle. An Euler tour can be decomposed into a set of edge-disjoint simple cycles, however. If G has an Euler tour, therefore, we can look at the simple cycles that, together, form the tour. In each simple cycle, each vertex in the cycle has one entering edge and one leaving edge. In each simple cycle, therefore, each vertex v has in-degree v  = out-degree v , where the degrees are either 1  if v is on the simple cycle  or 0  if v is not on the simple cycle . Adding the in- and out- degrees over all edges proves that if G has an Euler tour, then in-degree v  = out-degree v  for all vertices v. We prove the converse cid:151 that if in-degree v  = out-degree v  for all vertices v, then G has an Euler tour cid:151 in two different ways. One proof is nonconstructive, and the other proof will help us design the algorithm for part  b . First, we claim that if in-degree v  = out-degree v  for all vertices v, then we can pick any vertex u for which in-degree u  = out-degree u  ≥ 1 and create a cycle  not necessarily simple  that contains u. To prove this claim, let us start  Solution to Problem 22-3   Solutions for Chapter 22: Elementary Graph Algorithms  22-23   cid:4   of edges, we get the graph G  by placing vertex u on the cycle, and choose any leaving edge of u, say  u, v . Now we put v on the cycle. Since in-degree v  = out-degree v  ≥ 1, we can pick some leaving edge of v and continue visiting edges and vertices. Each time we pick an edge, we can remove it from further consideration. At each vertex other than u, at the time we visit an entering edge, there must be an unvisited leaving edge, since in-degree v  = out-degree v  for all vertices v. The only vertex for which there might not be an unvisited leaving edge is u, since we started the cycle by visiting one of u cid:146 s leaving edges. Since there cid:146 s always a leaving edge we can visit from all vertices other than u, eventually the cycle must return to u, thus proving the claim. The nonconstructive proof proves the contrapositive cid:151 that if G does not have an Euler tour, then in-degree v   cid:19 = out-degree v  for some vertex v cid:151 by con- tradiction. Choose a graph G =  V, E  that does not have an Euler tour but has at least one edge and for which in-degree v  = out-degree v  for all ver- tices v, and let G have the fewest edges of any such graph. By the above claim, G contains a cycle. Let C be a cycle of G with the greatest number of edges, and let VC be the set of vertices visited by cycle C. By our assumption, C is  cid:4  = E − C is nonempty. If we use not an Euler tour, and so the set of edges E  cid:4  =  V, E  cid:4  ; the set V of vertices and the set E this graph has in-degree v  = out-degree v  for all vertices v, since we have removed one entering edge and one leaving edge for each vertex on cycle C. also has Consider any component G in-degree v  = out-degree v  for all vertices v. Since E  cid:4   cid:1  E, it follows  cid:4  . Because the from how we chose G that G  cid:4  cid:4  ∪ VC and, original graph G is connected, there must be some vertex x ∈ V without loss of generality, consider x to be the  cid:222 rst and last vertex on both C . But then the cycle C formed by  cid:222 rst traversing C and then travers- and C  cid:4  is a cycle of G with more edges than C, contradicting our choice of C. ing C We conclude that C must have been an Euler tour. The constructive proof uses the same ideas. Let us start at a vertex u and, via random traversal of edges, create a cycle. We know that once we take any edge entering a vertex v  cid:19 = u, we can  cid:222 nd an edge leaving v that we have not yet taken. Eventually, we get back to vertex u, and if there are still edges leaving u that we have not taken, we can continue the cycle. Eventually, we get back to vertex u and there are no untaken edges leaving u. If we have visited every edge in the graph G, we are done. Otherwise, since G is connected, there must be some unvisited edge leaving a vertex, say v, on the cycle. We can traverse a new cycle starting at v, visiting only previously unvisited edges, and we can splice this cycle into the cycle we already know. That is, if the original cycle is  cid:2 u, . . . , v, w, . . . , u cid:3 , and the new cycle is  cid:2 v, x, . . . , v cid:3 , then we can create the cycle  cid:2 u, . . . , v, x, . . . , v, w, . . . , u cid:3 . We continue this process of  cid:222 nding a vertex with an unvisited leaving edge on a visited cycle, visiting a cycle starting and ending at this vertex, and splicing in the newly visited cycle, until we have visited every edge.   cid:4  cid:4  ⊆ E must have an Euler tour, say C   cid:4  cid:4  =  V  cid:4  cid:4   , and observe that G   cid:4  cid:4   of G   cid:4  cid:4 , E   cid:4  cid:4    cid:4  cid:4    cid:4    cid:4   b. The algorithm is based on the idea in the constructive proof above.  We assume that G is represented by adjacency lists, and we work with a copy of the adjacency lists, so that as we visit each edge, we can remove it from   22-24  Solutions for Chapter 22: Elementary Graph Algorithms  its adjacency list. The singly linked form of adjacency list will suf cid:222 ce. The output of this algorithm is a doubly linked list T of vertices which, read in list order, will give an Euler tour. The algorithm constructs T by  cid:222 nding cycles  also represented by doubly linked lists  and splicing them into T . By using doubly linked lists for cycles and the Euler tour, splicing a cycle into the Euler tour takes constant time. We also maintain a singly linked list L in which each list element consists of two parts:  1. a vertex v, and 2. a pointer to some appearance of v in T .  Initially, L contains one vertex, which may be any vertex of G. Here is the algorithm:  EULER-TOUR G  T ← empty list L ←  any vertex v, NIL  while L is not empty C ← VISIT v  if location-in-T = NIL then T ← C else splice C into T just before location-in-T  do remove  v, location-in-T   from L  return T  VISIT v  C ← empty sequence of vertices u ← v while out-degree u  > 0  do let w be the  cid:222 rst vertex in Adj[u]  remove w from Adj[u], decrementing out-degree u  add u onto the end of C if out-degree u  > 0 then add  u, u cid:146 s location in C  to L u ← w  return C  The use of NIL in the initial assignment to L ensures that the  cid:222 rst cycle C returned by VISIT becomes the current version of the Euler tour T . All cycles returned by VISIT thereafter are spliced into T . We assume that whenever an empty cycle is returned by VISIT, splicing it into T leaves T unchanged. Each time EULER-TOUR removes a vertex v from the list L, it calls VISIT v  to  cid:222 nd a cycle C, possibly empty and possibly not simple, that starts and ends at v; the cycle C is represented by a list that starts with v and ends with the last vertex on the cycle before the cycle ends at v. EULER-TOUR then splices this cycle C into the Euler tour T just before some appearance of v in T . When VISIT is at a vertex u, it looks for some vertex w such that the edge  u, w  has not yet been visited. Removing w from Adj[u] ensures that we will never   Solutions for Chapter 22: Elementary Graph Algorithms  22-25  visit  u, w  again. VISIT adds u onto the cycle C that it constructs. If, after removing edge  u, w , vertex u still has any leaving edges, then u, along with its location in C, is added to L. The cycle construction continues from w, and it ceases once a vertex with no unvisited leaving edges is found. Using the argument from part  a , at that point, this vertex must close up a cycle. At that point, therefore, the cycle C is returned. It is possible that a vertex u has unvisited leaving edges at the time it is added to list L in VISIT, but that by the time that u is removed from L in EULER-TOUR, all of its leaving edges have been visited. In this case, the while loop of VISIT executes 0 iterations, and VISIT returns an empty cycle. Once the list L is empty, every edge has been visited. The resulting cycle T is then an Euler tour. To see that EULER-TOUR takes O E  time, observe that because we remove each edge from its adjacency list as it is visited, no edge is visited more than once. Since each edge is visited at some time, the number of times that a vertex is added to L, and thus removed from L, is at mostE. Thus, the while loop in EULER-TOUR executes at most E iterations. The while loop in VISIT executes one iteration per edge in the graph, and so it executes at most E iterations as well. Since adding vertex u to the doubly linked list C takes constant time and splicing C into T takes constant time, the entire algorithm takes O E  time. Here is a variation on EULER-TOUR, which may be a bit simpler to reason about. It maintains a pointer u to a vertex on the Euler tour, with the invariant that all vertices on the Euler tour behind u have already had all entering and leaving edges added to the tour. This variation calls the same procedure VISIT as above.   cid:4  G   EULER-TOUR v ← any vertex T ← VISIT v  mark v cid:146 s position as the starting vertex in T u ← next[v] while u cid:146 s position in T  cid:19 = v cid:146 s position in T  do C ← VISIT u   splice C into T , just before u cid:146 s position  cid:1  If C was empty, T has not changed.  cid:1  If C was nonempty, then it began with u u ← next[next[prev[u]]]  cid:1  If C was empty, u now points to the next vertex on T  cid:1  If C was nonempty, u now points to the next vertex on C   which has been spliced into T    return T Whenever we return from calling VISIT u , we know that out-degree u  = 0, which means that we have visited all edges entering or leaving vertex u. Since VISIT adds each edge it visits to the cycle C, which is then added to the Euler tour T , when we return from a call to VISIT u , all edges entering or leaving vertex u have been added to the tour. When we advance the pointer u in the   22-26  Solutions for Chapter 22: Elementary Graph Algorithms  while loop, we need to ensure that it is advanced according to the current tour T , which may have just had a cycle C spliced into it. That cid:146 s why we advance u by the expression next[next[prev[u]]], rather than just simply next[u]. Since the graph G is connected, every edge will eventually be visited and added to the tour T . As before, each edge is visited exactly once, so that at completion, T will consist of exactly E edges. Once a vertex u has had VISIT called on it, any future call of VISIT u  will take O 1  time, and so the total time for all calls to VISIT is O E .  Solution to Problem 22-4  Compute GT in the usual way, so that GT is G with its edges reversed. Then do a depth- cid:222 rst search on GT, but in the main loop of DFS, consider the vertices in order of increasing values of L v . If vertex u is in the depth- cid:222 rst tree with root v, then min u  = v. Clearly, this algorithm takes O V + E  time. To show correctness,  cid:222 rst note that if u is in the depth- cid:222 rst tree rooted at v in GT, then there is a path v  cid:2  u in GT, and so there is a path u  cid:2  v in G. Thus, the minimum vertex label of all vertices reachable from u is at most L v , or in other words, L v  ≥ min{L w  : w ∈ R u }. Now suppose that L v  > min{L w  : w ∈ R u }, so that there is a vertex w ∈ R u  such that L w  < L v . At the time d[v] that we started the depth-  cid:222 rst search from v, we would have already discovered w, so that d[w] < d[v]. By the parenthesis theorem, either the intervals [d[v], f [v]], and [d[w], f [w]] are disjoint and neither v nor w is a descendant of the other, or we have the ordering d[w] < d[v] < f [v] < f [w] and v is a descendant of w. The latter case cannot occur, since v is a root in the depth- cid:222 rst forest  which means that v cannot be a descendant of any other vertex . In the former case, since d[w] < d[v], we must have d[w] < f [w] < d[v] < f [v]. In this case, since u is reachable from w in GT, we would have discovered u by the time f [w], so that d[u] < f [w]. Since we discovered u during a search that started at v, we have d[v] ≤ d[u]. Thus, d[v] ≤ d[u] < f [w] < d[v], which is a contradiction. We conclude that no such vertex w can exist.   Lecture Notes for Chapter 23: Minimum Spanning Trees  Chapter 23 overview  Problem    A town has a set of houses and a set of roads.   A road connects 2 and only 2 houses.   A road connecting houses u and v has a repair cost w u, v .   Goal: Repair enough  and no more  roads such that  1. everyone stays connected: can reach every house from all other houses, and 2. total repair cost is minimum.  Model as a graph:   Undirected graph G =  V, E .   Weight w u, v  on each edge  u, v  ∈ E.   Find T ⊆ E such that  1. T connects all vertices  T is a spanning tree , and 2. w T   =  w u, v  is minimized.   cid:2    u,v ∈T  A spanning tree whose weight is minimum over all spanning trees is called a min- imum spanning tree, or MST. Example of such a graph [edgesinMSTareshaded] :  a  10  12  b  9  c  3  8  e  1  7  3  d  f  8  5  6  g  h  9  2  i  11  In this example, there is more than one MST. Replace edge  e, f   by  c, e . Get a different spanning tree with the same weight.   23-2  Lecture Notes for Chapter 23: Minimum Spanning Trees  Growing a minimum spanning tree  Some properties of an MST:        It has V − 1 edges. It has no cycles. It might not be unique.  Building up the solution  Initially, A has no edges.    We will build a set A of edges.     As we add edges to A, maintain a loop invariant: Loop invariant: A is a subset of some MST.    Add only edges that maintain the invariant. If A is a subset of some MST, an edge  u, v  is safe for A if and only if A ∪ { u, v } is also a subset of some MST. So we will add only safe edges.  Generic MST algorithm  GENERIC-MST G, w  A ← ∅ while A is not a spanning tree  do  cid:222 nd an edge  u, v  that is safe for A  A ← A ∪ { u, v }  return A  Use the loop invariant to show that this generic algorithm works.  Initialization: The empty set trivially satis cid:222 es the loop invariant. Maintenance: Since we add only safe edges, A remains a subset of some MST. Termination: All edges added to A are in an MST, so when we stop, A is a span-  ning tree that is also an MST.  Finding a safe edge  How do we  cid:222 nd safe edges? Let cid:146 s look at the example. Edge  c, f   has the lowest weight of any edge in the graph. Is it safe for A = ∅? Intuitively: Let S ⊂ V be any set of vertices that includes c but not f  so that f is in V − S . In any MST, there has to be one edge  at least  that connects S with V − S. Why not choose the edge with minimum weight?  Which would be  c, f   in this case.  Some de cid:222 nitions: Let S ⊂ V and A ⊆ E.   Lecture Notes for Chapter 23: Minimum Spanning Trees  23-3    A cut  S, V − S  is a partition of vertices into disjoint sets V and S − V .   Edge  u, v  ∈ E crosses cut  S, V − S  if one endpoint is in S and the other is  in V − S.    A cut respects A if and only if no edge in A crosses the cut.   An edge is a light edge crossing a cut if and only if its weight is minimum over all edges crossing the cut. For a given cut, there can be > 1 light edge crossing it.  Theorem Let A be a subset of some MST,  S, V − S  be a cut that respects A, and  u, v  be a light edge crossing  S, V − S . Then  u, v  is safe for A. Proof Let T be an MST that includes A. If T contains  u, v , done.  cid:4  So now assume that T does not contain  u, v . We cid:146 ll construct a different MST T that includes A ∪{ u, v }. Recall: a tree has unique path between each pair of vertices. Since T is an MST, it contains a unique path p between u and v. Path p must cross the cut  S, V − S  at least once. Let  x, y  be an edge of p that crosses the cut. From how we chose  u, v , must have w u, v  ≤ w x, y .  u  v  S  x  y  V cid:150 S  [Exceptforthedashededge  u, v ,alledgesshownarein T. A issomesubsetof theedgesof T,but A cannotcontainanyedgesthatcrossthecut  S, V − S ,since thiscutrespects A. Shadededgesarethepath p.] Since the cut respects A, edge  x, y  is not in A. To form T   Remove  x, y . Breaks T into two components.   Add  u, v . Reconnects.  from T :   cid:4    23-4  Lecture Notes for Chapter 23: Minimum Spanning Trees   cid:4    cid:4  = T − { x, y } ∪ { u, v }. is a spanning tree.  cid:4   = w T   − w x, y  + w u, v  ≤ w T   ,  So T T w T since w u, v  ≤ w x, y . Since T must be an MST. MST, then T Need to show that  u, v  is safe for A:   A ⊆ T and  x, y   cid:19 ∈ A ⇒ A ⊆ T  cid:4    A ∪ { u, v } ⊆ T   Since T  .  .   cid:4    cid:4    cid:4    cid:4   is an MST,  u, v  is safe for A.  is a spanning tree, w T   cid:4   ≤ w T  , and T is an   theorem   So, in GENERIC-MST:   A is a forest containing connected components. Initially, each component is a  single vertex.    Any safe edge merges two of these components into one. Each component is a tree.   Since an MST has exactly V − 1 edges, the for loop iterates V − 1 times. Equivalently, after addingV−1 safe edges, we cid:146 re down to just one component.  Corollary If C =  VC , EC   is a connected component in the forest GA =  V, A  and  u, v  is a light edge connecting C to some other component in GA  i.e.,  u, v  is a light edge crossing the cut  VC , V − VC   , then  u, v  is safe for A. Proof Set S = VC in the theorem.   corollary   This naturally leads to the algorithm called Kruskal cid:146 s algorithm to solve the minimum-spanning-tree problem.  Kruskal cid:146 s algorithm  G =  V, E  is a connected, undirected, weighted graph. w : E → R.   Starts with each vertex being its own component.   Repeatedly merges two components into one by choosing the light edge that  connects them  i.e., the light edge crossing the cut between them .   Scans the set of edges in monotonically increasing order by weight.   Uses a disjoint-set data structure to determine whether an edge connects ver-  tices in different components.   Lecture Notes for Chapter 23: Minimum Spanning Trees  23-5  KRUSKAL V, E, w  A ← ∅ for each vertex v ∈ V do MAKE-SET v   sort E into nondecreasing order by weight w for each  u, v  taken from the sorted list do if FIND-SET u   cid:19 = FIND-SET v   then A ← A ∪ { u, v }  UNION u, v   return A  : : : : : : : : : : : :  safe safe safe reject safe safe reject safe safe reject reject safe  Run through the above example to see how Kruskal cid:146 s algorithm works on it:  c, f    g, i    e, f    c, e   d, h    f, h   e, d   b, d   d, g   b, c   g, h   a, b  At this point, we have only one component, so all other edges will be rejected. [We could add a test to the main loop of KRUSKAL to stop onceV − 1 edges have beenaddedto A.] Get the shaded edges shown in the  cid:222 gure. Suppose we had examined  c, e  before  e, f  . Then would have found  c, e  safe and would have rejected  e, f  .  Analysis  O 1  V MAKE-SETs O E lg E   Initialize A: First for loop: Sort E: Second for loop: O E  FIND-SETs and UNIONs   Assuming the implementation of disjoint-set data structure, already seen in  Chapter 21, that uses union by rank and path compression: O  V + E  α V    + O E lg E  .    Since G is connected, E ≥ V − 1 ⇒ O E α V    + O E lg E .   α V  = O lg V   = O lg E .   Therefore, total time is O E lg E .    E ≤ V2 ⇒ lgE = O 2 lg V   = O lg V  .   23-6  Lecture Notes for Chapter 23: Minimum Spanning Trees    Therefore, O E lg V   time.  If edges are already sorted, O E α V   , which is  almost linear.   Prim cid:146 s algorithm    Builds one tree, so A is always a tree.   Starts from an arbitrary  cid:147 root cid:148  r.   At each step,  cid:222 nd a light edge crossing cut  VA, V − VA , where VA = vertices  that A is incident on. Add this edge to A.  VA  light edge  [Edgesof A areshaded.]  How to  cid:222 nd the light edge quickly? Use a priority queue Q:   Each object is a vertex in V − VA.   Key of v is minimum weight of any edge  u, v , where u ∈ VA.   Then the vertex returned by EXTRACT-MIN is v such that there exists u ∈ VA   Key of v is ∞ if v is not adjacent to any vertices in VA. The edges of A will form a rooted tree with root r:    and  u, v  is light edge crossing  VA, V − VA .  r is given as an input to the algorithm, but it can be any vertex. π[v] = NIL if v = r or v has no parent.    Each vertex knows its parent in the tree by the attribute π[v] = parent of v.   As algorithm progresses, A = { v, π[v]  : v ∈ V − {r} − Q}.   At termination, VA = V ⇒ Q = ∅, so MST is A = { v, π[v]  : v ∈ V − {r}}.   Lecture Notes for Chapter 23: Minimum Spanning Trees  23-7  PRIM V, E, w, r   Q ← ∅ for each u ∈ V  do key[u] ← ∞ π[u] ← NIL INSERT Q, u   DECREASE-KEY Q, r, 0   cid:1  key[r] ← 0 while Q  cid:19 = ∅  do u ← EXTRACT-MIN Q   for each v ∈ Adj[u]  do if v ∈ Q and w u, v  < key[v]  then π[v] ← u  DECREASE-KEY Q, v, w u, v    Do example from previous graph. [Letastudentpicktheroot.]  Depends on how the priority queue is implemented:   Suppose Q is a binary heap.  Initialize Q and  cid:222 rst for loop: O V lg V   Decrease key of r: while loop:  Analysis  Total:  O lg V   V EXTRACT-MIN calls ⇒ O V lg V   ≤ E DECREASE-KEY calls ⇒ O E lg V   O E lg V      Suppose we could do DECREASE-KEY in O 1  amortized time.  Then ≤ E DECREASE-KEY calls take O E  time altogether ⇒ total time becomes O V lg V + E . In fact, there is a way to do DECREASE-KEY in O 1  amortized time: Fi- bonacci heaps, in Chapter 20.   Solutions for Chapter 23: Minimum Spanning Trees  Solution to Exercise 23.1-1  Solution to Exercise 23.1-4  Solution to Exercise 23.1-6  Theorem 23.1 shows this. Let A be the empty set and S be any set containing u but not v.  A triangle whose edge weights are all equal is a graph in which every edge is a light edge crossing some cut. But the triangle is cyclic, so it is not a minimum spanning tree.   cid:4    cid:4   , which means that T and T  Suppose that for every cut of G, there is a unique light edge crossing the cut. Let , of G. We will show that every us consider two minimum spanning trees, T and T  cid:4  are the same tree and hence edge of T is also in T there is a unique minimum spanning tree. Consider any edge  u, v  ∈ T . If we remove  u, v  from T , then T becomes disconnected, resulting in a cut  S, V − S . The edge  u, v  is a light edge crossing the cut  S, V − S   by Exercise 23.1-3 . Now consider the edge  x, y  ∈ T that crosses  S, V − S . It, too, is a light edge crossing this cut. Since the light edge crossing  S, V − S  is unique, the edges  u, v  and  x, y  are the same edge. Thus,  u, v  ∈ T Here cid:146 s a counterexample for the converse:  . Since we chose  u, v  arbitrarily, every edge in T is also in T  .   cid:4    cid:4    cid:4   x  1  1  y  z   Solutions for Chapter 23: Minimum Spanning Trees  23-9  Here, the graph is its own minimum spanning tree, and so the minimum spanning tree is unique. Consider the cut  {x} ,{y, z} . Both of the edges  x, y  and  x, z  are light edges crossing the cut, and they are both light edges.  Solution to Exercise 23.1-10  Let w T   = cid:1    x ,y ∈T , so that w T   ≤ w T  cid:4  , then w cid:4  T , then w cid:4  T  w x, y . We have w cid:4  T   = w T   − k. Consider any other  cid:4   = w T  cid:4   = w T   cid:4   ≥ w T   > w cid:4  T  .  cid:4   − k ≥ w T   − k = w cid:4  T  .   cid:4  .   cid:4  , and so T is a minimum spanning tree for weight   cid:4   cid:4   spanning tree T If  x, y   cid:19 ∈ T If  x, y  ∈ T Either way, w cid:4  T   ≤ w cid:4  T function w cid:4   .  Solution to Exercise 23.2-4  We know that Kruskal cid:146 s algorithm takes O V   time for initialization, O E lg E  time to sort the edges, and O E α V    time for the disjoint-set operations, for a total running time of O V + E lg E + E α V    = O E lg E . If we knew that all of the edge weights in the graph were integers in the range from 1 to V, then we could sort the edges in O V + E  time using counting sort. Since the graph is connected, V = O E , and so the sorting time is re- duced to O E . This would yield a total running time of O V + E + E α V    = O E α V   , again since V = O E , and since E = O E α V   . The time to process the edges, not the time to sort them, is now the dominant term. Knowl- edge about the weights won cid:146 t help speed up any other part of the algorithm, since nothing besides the sort uses the weight values. If the edge weights were integers in the range from 1 to W for some constant W , then we could again use counting sort to sort the edges more quickly. This time, sorting would take O E + W   = O E  time, since W is a constant. As in the  cid:222 rst part, we get a total running time of O E α V   .  Solution to Exercise 23.2-5  The time taken by Prim cid:146 s algorithm is determined by the speed of the queue oper- ations. With the queue implemented as a Fibonacci heap, it takes O E + V lg V   time. Since the keys in the priority queue are edge weights, it might be possible to im- plement the queue even more ef cid:222 ciently when there are restrictions on the possible edge weights. We can improve the running time of Prim cid:146 s algorithm if W is a constant by imple- menting the queue as an array Q[0 . . W + 1]  using the W + 1 slot for key= ∞ ,   23-10  Solutions for Chapter 23: Minimum Spanning Trees  where each slot holds a doubly linked list of vertices with that weight as their key. Then EXTRACT-MIN takes only O W   = O 1  time  just scan for the  cid:222 rst nonempty slot , and DECREASE-KEY takes only O 1  time  just remove the ver- tex from the list it cid:146 s in and insert it at the front of the list indexed by the new key . This gives a total running time of O E , which is the best possible asymptotic time  since  cid:3  E  edges must be processed . However, if the range of edge weights is 1 to V, then EXTRACT-MIN takes  cid:2  V   time with this data structure. So the total time spent doing EXTRACT-MIN is  cid:2  V 2 , slowing the algorithm to  cid:2  E + V 2  =  cid:2  V 2 . In this case, it is better to keep the Fibonacci-heap priority queue, which gave the  cid:2  E + V lg V   time. There are data structures not in the text that yield better running times:   The van Emde Boas data structure  mentioned in the chapter notes for Chapter 6 and the introduction to Part V  gives an upper bound of O E + V lg lg V   time for Prim cid:146 s algorithm.    A redistributive heap  used in the single-source shortest-paths algorithm of Ahuja, Mehlhorn, Orlin, and Tarjan, and mentioned in the chapter notes for Chapter 24  gives an upper bound of O  for Prim cid:146 s algorithm.  E + V  lg V   cid:12    cid:11    cid:10   Solution to Exercise 23.2-7  We start with the following lemma.  Lemma Let T be a minimum spanning tree of G =  V, E , and consider a graph G  cid:4 , E  V  cid:4  the edges of G that are not in T . Then there is a minimum spanning tree of G includes no edges in T .   cid:4   for which G is a subgraph, i.e., V ⊆ V   cid:4  = . Let T = E − T be that  and E ⊆ E   cid:4    cid:4   Proof By Exercise 23.2-1, there is a way to order the edges of E so that Kruskal cid:146 s algorithm, when run on G, produces the minimum spanning tree T . We will show  cid:4  that that Kruskal cid:146 s algorithm, run on G includes no edges in T . We assume that the edges in E are considered in the same  cid:4  . We  cid:222 rst state and relative order when Kruskal cid:146 s algorithm is run on G and on G prove the following claim.  , produces a minimum spanning tree T   cid:4   Claim For any pair of vertices u, v ∈ V , if these vertices are in the same set after Kruskal cid:146 s algorithm run on G considers any edge  x, y  ∈ E, then they are in the same set  cid:4  after Kruskal cid:146 s algorithm run on G Proof of claim Let us order the edges of E by nondecreasing weight as  cid:2  x1, y1 ,  x2, y2 , . . . ,  xk, yk  cid:3 , where k = E. This sequence gives the order in which the  cid:4  . edges of E are considered by Kruskal cid:146 s algorithm, whether it is run on G or on G We will use induction, with the inductive hypothesis that if u and v are in the same set after Kruskal cid:146 s algorithm run on G considers an edge  xi , yi  , then they are in  considers  x, y .   Solutions for Chapter 23: Minimum Spanning Trees  23-11      cid:4  the same set after Kruskal cid:146 s algorithm run on G considers the same edge. We use induction on i. Basis: For the basis, i = 0. Kruskal cid:146 s algorithm run on G has not considered any edges, and so all vertices are in different sets. The inductive hypothesis holds trivially. Inductive step: We assume that any vertices that are in the same set after Kruskal cid:146 s algorithm run on G has considered edges  cid:2  x1, y1 ,  x2, y2 , . . . ,  xi−1, yi−1  cid:3   cid:4  has considered the same are in the same set after Kruskal cid:146 s algorithm run on G  cid:4  , after it considers  xi−1, yi−1 , it may edges. When Kruskal cid:146 s algorithm runs on G  cid:4 −E may consider some edges in E cause UNION operations to occur, but sets are never divided. Hence, any vertices  cid:4  considers  xi−1, yi−1  that are in the same set after Kruskal cid:146 s algorithm run on G are still in the same set when  xi , yi   is considered. When Kruskal cid:146 s algorithm run on G considers  xi , yi  , either xi and yi are found to be in the same set or they are not.     cid:4 −E before considering  xi , yi  . The edges in E  If Kruskal cid:146 s algorithm run on G  cid:222 nds xi and yi to be in the same set, then no UNION operation occurs. The sets of vertices remain the same, and so the inductive hypothesis continues to hold after considering  xi , yi  . If Kruskal cid:146 s algorithm run on G  cid:222 nds xi and yi to be in different sets, then the  cid:4  operation UNION xi , yi   will occur. Kruskal cid:146 s algorithm run on G will  cid:222 nd that either xi and yi are in the same set or they are not. By the inductive hypoth- esis, when edge  xi , yi   is considered, all vertices in xi  cid:146 s set when Kruskal cid:146 s  cid:4  algorithm runs on G are in xi  cid:146 s set when Kruskal cid:146 s algorithm runs on G , and  cid:4  the same holds for yi. Regardless of whether Kruskal cid:146 s algorithm run on G  cid:222 nds xi and yi to already be in the same set, their sets are united after consider- ing  xi , yi  , and so the inductive hypothesis continues to hold after considering  xi , yi  .  claim  With the claim in hand, we suppose that some edge  u, v  ∈ T is placed into T  cid:4  . That means that Kruskal cid:146 s algorithm run on G found u and v to be in the same set  since  u, v  ∈ T   but Kruskal cid:146 s algorithm run on G  cid:4  found u and v to be in different sets  since  u, v  is placed into T  . This fact contradicts the claim, and we  cid:4  . Thus, by running Kruskal cid:146 s algorithm conclude that no edge in T is placed into T that on G and G  lemma  includes no edges in T .  cid:4   be the graph G =  V, E  with We use this lemma as follows. Let G the one new vertex and its incident edges added. Suppose that we have a minimum  cid:4  by creating spanning tree T for G. We compute a minimum spanning tree for G consists of the edges of T and the edges in the graph G  cid:4  − E  i.e., the edges added to G that made G  cid:4   , and then  cid:222 nding a minimum E  cid:4  . By the lemma, there is a minimum spanning tree for G spanning tree T that includes no edges of E − T . In other words, G  cid:4  has a minimum spanning tree  cid:4  − E; these edges comprise exactly the set E  cid:4  cid:4  . that includes only edges in T and E is also a minimum spanning tree Thus, the the minimum spanning tree T of G   cid:4  , we demonstrate that there exists a minimum spanning tree of G   cid:4  cid:4  , where E   cid:4  cid:4  =  V   cid:4  =  V   cid:4 , E  cid:4  cid:4    cid:4 , E  for G  of G   cid:4  cid:4    cid:4  cid:4   .   cid:4    cid:4    cid:4    cid:4    cid:4    23-12  Solutions for Chapter 23: Minimum Spanning Trees   cid:4   Even though the proof of the lemma uses Kruskal cid:146 s algorithm, we are not required . We can  cid:222 nd a minimum spanning tree by any to use this algorithm to  cid:222 nd T means we choose. Let us use Prim cid:146 s algorithm with a Fibonacci-heap priority queue. Since V contains the V − 1  cid:4  = V + 1 and E  cid:4  cid:4  ≤ 2V − 1  E edges of T and at most V edges in E  cid:4  − E , it takes O V   time to construct G  cid:4  cid:4  , and the run of Prim cid:146 s algorithm with a Fibonacci-heap priority queue takes time  cid:4   = O V lg V  . Thus, if we are given a minimum spanning tree O E  cid:4  of G, we can compute a minimum spanning tree of G  in O V lg V   time.   cid:4  cid:4  + V  lg V   cid:4  cid:4    cid:4   Solution to Problem 23-1  a. To see that the minimum spanning tree is unique, observe that since the graph is connected and all edge weights are distinct, then there is a unique light edge crossing every cut. By Exercise 23.1-6, the minimum spanning tree is unique. To see that the second-best minimum spanning tree need not be unique, here is a weighted, undirected graph with a unique minimum spanning tree of weight 7 and two second-best minimum spanning trees of weight 8:  1  1  1  3  5  3  5  3  5  2  4  minimum  spanning tree  2  4  second-best minimum  spanning tree  2  4  second-best minimum  spanning tree  b. Since any spanning tree has exactly V − 1 edges, any second-best minimum spanning tree must have at least one edge that is not in the  best  minimum spanning tree. If a second-best minimum spanning tree has exactly one edge, say  x, y , that is not in the minimum spanning tree, then it has the same set of edges as the minimum spanning tree, except that  x, y  replaces some edge, say  cid:4  = T −{ u, v }∪{ x, y },  u, v , of the minimum spanning tree. In this case, T as we wished to show. Thus, all we need to show is that by replacing two or more edges of the min- imum spanning tree, we cannot obtain a second-best minimum spanning tree. Let T be the minimum spanning tree of G, and suppose that there exists a that differs from T by two or more second-best minimum spanning tree T edges. There are at least two edges in T − T , and let  u, v  be the edge in T − T , we would get  cid:4  − T  since otherwise, T a cycle c. This cycle contains some edge  x, y  in T would contain a cycle . We claim that w x, y  > w u, v . We prove this claim by contradiction, so let us assume that w x, y  < w u, v .  Recall the assumption that edge weights are distinct, so that we do not have to concern ourselves with w x, y  = w u, v .  If we add  x, y  to T , we get a cycle c  cid:4  , which contains  with minimum weight. If we were to add  u, v  to T   cid:4    cid:4    cid:4    cid:4    Solutions for Chapter 23: Minimum Spanning Trees  23-13   cid:4 , v cid:4   in T −T   cid:4    cid:4    cid:4    cid:4    cid:4    cid:4  cid:4    since otherwise, T  , the set of edges T   cid:4   but is not T . Hence, T  some edge  u would contain a cycle . There-  cid:4  cid:4  = T − { u  cid:4 , v cid:4  } ∪ { x, y } forms a spanning tree, and fore, the set of edges T  cid:4 , v cid:4   < w x, y , since otherwise T we must also have w u would be a span-  cid:4 , v cid:4   < w x, y  < w u, v , ning tree with weight less than w T  . Thus, w u which contradicts our choice of  u, v  as the edge in T −T  cid:4  of minimum weight. Since the edges  u, v  and  x, y  would be on a common cycle c if we were  cid:4  − { x, y } ∪ { u, v } is a spanning tree, to add  u, v  to T  cid:4  . Moreover, it differs from T  because it differs and its weight is less than w T by only one edge . Thus, we have formed a spanning tree whose weight from T is less than w T was not a second-best minimum spanning tree. c. We can  cid:222 ll in max[u, v] for all u, v ∈ V in O V 2  time by simply doing a search from each vertex u, having restricted the edges visited to those of the spanning tree T . It doesn cid:146 t matter what kind of search we do: breadth- cid:222 rst, depth- cid:222 rst, or any other kind. We cid:146 ll give pseudocode for both breadth- cid:222 rst and depth- cid:222 rst approaches. Each approach differs from the pseudocode given in Chapter 22 in that we don cid:146 t need to compute d or f values, and we cid:146 ll use the max table itself to record whether a vertex has been visited in a given search. In particular, max[u, v] = NIL if and only if u = v or we have not yet visited vertex v in a search from vertex u. Note also that since we cid:146 re visiting via edges in a spanning tree of an undirected graph, we are guaranteed that the search from each vertex u cid:151 whether breadth- cid:222 rst or depth- cid:222 rst cid:151 will visit all vertices. There will be no need to  cid:147 restart cid:148  the search as is done in the DFS procedure of Section 22.3. Our pseudocode assumes that the adjacency list of each vertex consists only of edges in the spanning tree T . Here cid:146 s the breadth- cid:222 rst search approach:  BFS-FILL-MAX T, w  for each vertex u ∈ V  do for each vertex v ∈ V  do max[u, v] ← NIL  Q ← ∅ ENQUEUE Q, u  while Q  cid:19 = ∅  do x ← DEQUEUE Q  for each v ∈ Adj[x]  do if max[u, v] = NIL and v  cid:19 = u  then if x = u or w x, v  > max[u, x] then max[u, v] ←  x, v  else max[u, v] ← max[u, x] ENQUEUE Q, v   return max  Here cid:146 s the depth- cid:222 rst search approach:   23-14  Solutions for Chapter 23: Minimum Spanning Trees  DFS-FILL-MAX T, w  for each vertex u ∈ V  do for each vertex v ∈ V  do max[u, v] ← NIL  DFS-FILL-MAX-VISIT  u, u, max   return max  DFS-FILL-MAX-VISIT  u, x, max  for each vertex v ∈ Adj[x] do if max[u, v] = NIL and v  cid:19 = u  then if x = u or w x, v  > max[u, x] then max[u, v] ←  x, v  else max[u, v] ← max[u, x] DFS-FILL-MAX-VISIT  u, v, max   For either approach, we are  cid:222 lling in V rows of the max table. Since the number of edges in the spanning tree isV − 1, each row takes O V   time to  cid:222 ll in. Thus, the total time to  cid:222 ll in the max table is O V2 .   cid:4   d. In part  b , we established that we can  cid:222 nd a second-best minimum spanning tree by replacing just one edge of the minimum spanning tree T by some edge  u, v  not in T . As we know, if we create spanning tree T by replacing  cid:4   = w T  − w x, y  + w u, v . edge  x, y  ∈ T by edge  u, v   cid:19 ∈ T , then w T For a given edge  u, v , the edge  x, y  ∈ T that minimizes w T  cid:4   is the edge of maximum weight on the unique path between u and v in T . If we have already computed the max table from part  c  based on T , then the identity of this edge is precisely what is stored in max[u, v]. All we have to do is determine an edge  u, v   cid:19 ∈ T for which w max[u, v]  − w u, v  is minimum. Thus, our algorithm to  cid:222 nd a second-best minimum spanning tree goes as fol- lows: 1. Compute the minimum spanning tree T . Time: O E+ V lg V  , using Prim cid:146 s algorithm with a Fibonacci-heap implementation of the priority queue. Since E < V2, this running time is O V 2 .  Time: O V 2 .  2. Given the minimum spanning tree T , compute the max table, as in part  c . 3. Find an edge  u, v   cid:19 ∈ T that minimizes w max[u, v]  − w u, v . Time:  cid:4  = T −{max[u, v]}∪{ u, v }  4. Having found an edge  u, v  in step 3, return T  O E , which is O V 2 .  as a second-best minimum spanning tree.  The total time is O V 2 .   Lecture Notes for Chapter 24: Single-Source Shortest Paths  Shortest paths  How to  cid:222 nd the shortest route between two points on a map.  Input:    Directed graph G =  V, E    Weight function w : E → R Weight of path p =  cid:2 v0, v1, . . . , vk cid:3   = k cid:2   i=1  w vi−1, vi    ’   cid:9   = sum of edge weights on path p . Shortest-path weight u to v: δ u, v  = Shortest path u to v is any path p such that w  p  = δ u, v .  if there exists a path u  cid:2  v , otherwise .  w  p  : u p cid:2  v  min ∞     Example: shortest paths from s [d valuesappearinsidevertices. Shadededgesshowshortestpaths.]  s  0  2  1  2  7  s  0  2  1  2  7  3  5  t 3  5 y  6  4  3  6  x 9  11 z  3  5  t 3  5 y  6  4  3  6  x 9  11 z  This example shows that the shortest path might not be unique. It also shows that when we look at shortest paths from one vertex to all other vertices, the shortest paths are organized as a tree. Can think of weights as representing any measure that   24-2  Lecture Notes for Chapter 24: Single-Source Shortest Paths  accumulates linearly along a path,      we want to minimize. Examples: time, cost, penalties, loss. Generalization of breadth- cid:222 rst search to weighted graphs.  Variants   Single-source: Find shortest paths from a given source vertex s ∈ V to every  vertex v ∈ V .    Single-destination: Find shortest paths to a given destination vertex.   Single-pair: Find shortest path from u to v. No way known that cid:146 s better in   All-pairs: Find shortest path from u to v for all u, v ∈ V . We cid:146 ll see algorithms  worst case than solving single-source.  for all-pairs in the next chapter.  Negative-weight edges  OK, as long as no negative-weight cycles are reachable from the source.    If we have a negative-weight cycle, we can just keep going around it, and get w s, v  = −∞ for all v on the cycle.    But OK if the negative-weight cycle is not reachable from the source.   Some algorithms work only if there are no negative-weight edges in the graph.  We cid:146 ll be clear when they cid:146 re allowed and not allowed.  Optimal substructure  Lemma Any subpath of a shortest path is a shortest path.  Proof Cut-and-paste.  pux  u  pxy  y  x  pyv  v  Suppose this path p is a shortest path from u to v. Then δ u, v  = w  p  = w  pux   + w  px y  + w  pyv . Now suppose there exists a shorter path x  cid:4  Then w  p x y Construct p    < w  px y .  cid:4  :   cid:4  p x y cid:2  y.  pux  u  p’xy  y  x  pyv  v   Lecture Notes for Chapter 24: Single-Source Shortest Paths  24-3  Then w  p   cid:4   = w  pux   + w  p    + w  pyv  < w  pux   + w  px y  + w  pyv  = w  p  .   cid:4  x y  So p wasn cid:146 t a shortest path after all!   lemma   Cycles  them.  Shortest paths can cid:146 t contain cycles:   Already ruled out negative-weight cycles.   Positive-weight ⇒ we can get a shorter path by omitting the cycle.   Zero-weight: no reason to use them ⇒ assume that our solutions won cid:146 t use  Output of single-source shortest-path algorithm For each vertex v ∈ V :     d[v] = δ s, v .     Initially, d[v] = ∞.    Reduces as algorithms progress. But always maintain d[v] ≥ δ s, v .   Call d[v] a shortest-path estimate.    π[v] = predecessor of v on a shortest path from s.  If no predecessor, π[v] = NIL.      π induces a tree cid:151 shortest-path tree.   We won cid:146 t prove properties of π in lecture cid:151 see text.  All the shortest-paths algorithms start with INIT-SINGLE-SOURCE.  Initialization  INIT-SINGLE-SOURCE  V, s  for each v ∈ V  do d[v] ← ∞ π[v] ← NIL  d[s] ← 0  Relaxing an edge  u, v   Can we improve the shortest-path estimate for v by going through u and taking  u, v ?   24-4  Lecture Notes for Chapter 24: Single-Source Shortest Paths  RELAX u, v, w  if d[v] > d[u] + w u, v  then d[v] ← d[u] + w u, v   π[v] ← u  u 4  4  3  RELAX  v 10  7  4  4  3  6 RELAX  6  For all the single-source shortest-paths algorithms we cid:146 ll look at,      start by calling INIT-SINGLE-SOURCE, then relax edges.  The algorithms differ in the order and how many times they relax each edge.  Shortest-paths properties  Based on calling INIT-SINGLE-SOURCE once and then calling RELAX zero or more times.  Triangle inequality For all  u, v  ∈ E, we have δ s, v  ≤ δ s, u  + w u, v . Proof Weight of shortest path s  cid:2  v is ≤ weight of any path s  cid:2  v. Path s  cid:2  u → v is a path s  cid:2  v, and if we use a shortest path s  cid:2  u, its weight is δ s, u  + w u, v .  Upper-bound property Always have d[v] ≥ δ s, v  for all v. Once d[v] = δ s, v , it never changes. Proof Initially true. Suppose there exists a vertex such that d[v] < δ s, v . Without loss of generality, v is  cid:222 rst vertex for which this happens. Let u be the vertex that causes d[v] to change. Then d[v] = d[u] + w u, v . So,  d[v] < δ s, v   ≤ δ s, u  + w u, v  ≤ d[u] + w u, v  ⇒ d[v] < d[u] + w u, v  .   triangle inequality   v is  cid:222 rst violation    Lecture Notes for Chapter 24: Single-Source Shortest Paths  24-5  Contradicts d[v] = d[u] + w u, v . Once d[v] reaches δ s, v , it never goes lower. It never goes up, since relaxations only lower shortest-path estimates.  No-path property If δ s, v  = ∞, then d[v] = ∞ always. Proof d[v] ≥ δ s, v  = ∞ ⇒ d[v] = ∞.  Convergence property If s  cid:2  u → v is a shortest path, d[u] = δ s, u , and we call RELAX u, v, w , then d[v] = δ s, v  afterward. Proof After relaxation: d[v] ≤ d[u] + w u, v  = δ s, u  + w u, v  = δ s, v    lemma cid:151 optimal substructure    RELAX code   Since d[v] ≥ δ s, v , must have d[v] = δ s, v .  Path relaxation property Let p =  cid:2 v0, v1, . . . , vk cid:3  be a shortest path from s = v0 to vk. If we relax, in order,  v0, v1 ,  v1, v2 , . . . ,  vk−1, vk , even intermixed with other relaxations, then d[vk] = δ s, vk  . Proof Induction to show that d[vi ] = δ s, vi   after  vi−1, vi   is relaxed. Basis: i = 0. Initially, d[v0] = 0 = δ s, v0  = δ s, s . Inductive step: Assume d[vi−1] = δ s, vi−1 . Relax  vi−1, vi  . By convergence property, d[vi ] = δ s, vi   afterward and d[vi ] never changes.  The Bellman-Ford algorithm    Allows negative-weight edges.   Computes d[v] and π[v] for all v ∈ V .   Returns TRUE if no negative-weight cycles reachable from s, FALSE otherwise.   24-6  Lecture Notes for Chapter 24: Single-Source Shortest Paths  BELLMAN-FORD V, E, w, s  INIT-SINGLE-SOURCE  V, s  for i ← 1 to V − 1  do for each edge  u, v  ∈ E do RELAX u, v, w  do if d[v] > d[u] + w u, v   for each edge  u, v  ∈ E  then return FALSE  return TRUE Core: The  cid:222 rst for loop relaxes all edgesV − 1 times. Time:  cid:2  V E .  Example:  s  0  4  r  cid:150 1   cid:150 1  2  3  1  2  2 z  5   cid:150 2 y  1  x   cid:150 3  Values you get on each pass and how quickly it converges depends on order of relaxation. But guaranteed to converge after V − 1 passes, assuming no negative-weight cycles.  Proof Use path-relaxation property. Let v be reachable from s, and let p =  cid:2 v0, v1, . . . , vk cid:3  be a shortest path from s to v, where v0 = s and vk = v. Since p is acyclic, it has ≤ V − 1 edges, so k ≤ V − 1. Each iteration of the for loop relaxes all edges:   First iteration relaxes  v0, v1 .   Second iteration relaxes  v1, v2 .   kth iteration relaxes  vk−1, vk . By the path-relaxation property, d[v] = d[vk] = δ s, vk   = δ s, v .  How about the TRUE FALSE return value?   Suppose there is no negative-weight cycle reachable from s.  At termination, for all  u, v  ∈ E, d[v] = δ s, v   ≤ δ s, u  + w u, v  = d[u] + w u, v  .  So BELLMAN-FORD returns TRUE.   triangle inequality    Lecture Notes for Chapter 24: Single-Source Shortest Paths  24-7    Now suppose there exists negative-weight cycle c =  cid:2 v0, v1, . . . , vk cid:3 , where  k cid:2   v0 = vk, reachable from s.  vi−1, vi   < 0 .  Then  i=1  d[vi ] ≤ k cid:2  = k cid:2   i=1  i=1  i=1  0 ≤ k cid:2   i=1  w vi−1, vi   .  Suppose  for contradiction  that BELLMAN-FORD returns TRUE. Then d[vi ] ≤ d[vi−1] + w vi−1, vi   for i = 1, 2, . . . , k. k cid:2  Sum around c:   d[vi−1] + w vi−1, vi     d[vi−1] + k cid:2   w vi−1, vi    i=1   cid:1   Each vertex appears once in each summation  k i=1 d[vi ] and  This contradicts c being a negative-weight cycle!   cid:1  i=1 d[vi−1] ⇒  k  Single-source shortest paths in a directed acyclic graph  Since a dag, we cid:146 re guaranteed no negative-weight cycles.  DAG-SHORTEST-PATHS  V, E, w, s  topologically sort the vertices INIT-SINGLE-SOURCE  V, s  for each vertex u, taken in topologically sorted order  do for each vertex v ∈ Adj[u] do RELAX u, v, w   Example:  s 0  2  7  6  t 2  Time:  cid:2  V + E .  1  y 5   cid:150 2  z 3  x  6  4   cid:150 1  2  Correctness: Because we process vertices in topologically sorted order, edges of any path must be relaxed in order of appearance in the path. ⇒ Edges on any shortest path are relaxed in order. ⇒ By path-relaxation property, correct.   24-8  Lecture Notes for Chapter 24: Single-Source Shortest Paths  Dijkstra cid:146 s algorithm  Instead of a FIFO queue, uses a priority queue.  No negative-weight edges. Essentially a weighted version of breadth- cid:222 rst search.     Keys are shortest-path weights  d[v] . Have two sets of vertices:    S = vertices whose  cid:222 nal shortest-path weights are determined,    Q = priority queue = V − S.   cid:1  i.e., insert all vertices into Q  DIJKSTRA V, E, w, s  INIT-SINGLE-SOURCE  V, s  S ← ∅ Q ← V while Q  cid:19 = ∅  do u ← EXTRACT-MIN Q  S ← S ∪ {u} for each vertex v ∈ Adj[u] do RELAX u, v, w     Looks a lot like Prim cid:146 s algorithm, but computing d[v], and using shortest-path    Dijkstra cid:146 s algorithm can be viewed as greedy, since it always chooses the  cid:147 light-  weights as keys. est cid:148    cid:147 closest cid:148 ?  vertex in V − S to add to S.  Example:  10  5  x 8  5 y  2  1  s  0  3  4  6  z  Order of adding to S: s, y, z, x.  Correctness:  Loop invariant: At the start of each iteration of the while loop, d[v] = δ s, v  for all v ∈ S.  Initialization: Initially, S = ∅, so trivially true. Termination: At end, Q = ∅ ⇒ S = V ⇒ d[v] = δ s, v  for all v ∈ V .   Lecture Notes for Chapter 24: Single-Source Shortest Paths  24-9  Maintenance: Need to show that d[u] = δ s, u  when u is added to S in each iteration. Suppose there exists u such that d[u]  cid:19 = δ s, u . Without loss of generality, let u be the  cid:222 rst vertex for which d[u]  cid:19 = δ s, u  when u is added to S. Observations:   u  cid:19 = s, since d[s] = δ s, s  = 0.   Therefore, s ∈ S, so S  cid:19 = ∅.   There must be some path s  cid:2  u, since otherwise d[u] = δ s, u  = ∞ by  no-path property.  So, there cid:146 s a path s  cid:2  u. This means there cid:146 s a shortest path s p cid:2  u. Just before u is added to S, path p connects a vertex in S  i.e., s  to a vertex in V − S  i.e., u . Let y be  cid:222 rst vertex along p that cid:146 s in V − S, and let x ∈ S be y cid:146 s predecessor.  S  s  p1  x  y  p2  u  Decompose p into s p1 cid:2  x → y p2 cid:2  u.  Could have x = s or y = u, so that p1 or p2 may have no edges.   Claim d[y] = δ s, y  when u is added to S. Proof x ∈ S and u is the  cid:222 rst vertex such that d[u]  cid:19 = δ s, u  when u is added to S ⇒ d[x] = δ s, x  when x is added to S. Relaxed  x, y  at that time, so by the convergence property, d[y] = δ s, y .  claim  Now can get a contradiction to d[u]  cid:19 = δ s, u : y is on shortest path s  cid:2  u, and all edge weights are nonnegative ⇒ δ s, y  ≤ δ s, u  ⇒ d[y] = δ s, y  ≤ δ s, u  ≤ d[u]   upper-bound property  .  Also, both y and u were in Q when we chose u, so d[u] ≤ d[y] ⇒ d[u] = d[y] . Therefore, d[y] = δ s, y  = δ s, u  = d[u]. Contradicts assumption that d[u]  cid:19 = δ s, u . Hence, Dijkstra cid:146 s algorithm is correct.   24-10  Lecture Notes for Chapter 24: Single-Source Shortest Paths  Analysis: Like Prim cid:146 s algorithm, depends on implementation of priority queue.      If binary heap, each operation takes O lg V   time ⇒ O E lg V  . If a Fibonacci heap:   Each EXTRACT-MIN takes O 1  amortized time.   There are O V   other operations, taking O lg V   amortized time each.   Therefore, time is O V lg V + E .  Difference constraints  Given a set of inequalities of the form xj − xi ≤ bk.       x cid:146 s are variables, 1 ≤ i, j ≤ n, b cid:146 s are constants, 1 ≤ k ≤ m.  Want to  cid:222 nd a set of values for the x cid:146 s that satisfy all m inequalities, or determine that no such values exist. Call such a set of values a feasible solution.  Example: x1 − x2 ≤ 5 x1 − x3 ≤ 6 x2 − x4 ≤ −1 x3 − x4 ≤ −2 x4 − x1 ≤ −3 Solution: x =  0,−4,−5,−3  Also: x =  5, 1, 0, 2  = [above solution] + 5  Lemma If x is a feasible solution, then so is x + d for any constant d. Proof x is a feasible solution ⇒ xj − xi ≤ bk for all i, j, k ⇒  x j + d  −  xi + d  ≤ bk.   lemma   Constraint graph G =  V, E , weighted, directed.   V = {v0, v1, v2, . . . , vn}: one vertex per variable + v0   E = { vi , v j   : x j − xi ≤ bk is a constraint} ∪ { v0, v1 ,  v0, v2 , . . . ,  v0, vn }   w v0, v j   = 0 for all j   w vi , v j   = bk if x j − xi ≤ bk   Lecture Notes for Chapter 24: Single-Source Shortest Paths  24-11  v0  0  0  0  0  0   cid:150 3  v1 0   cid:150 3 v4  5   cid:150 1  6   cid:150 2  v2  cid:150 4   cid:150 5 v3  Theorem Given a system of difference constraints, let G =  V, E  be the corresponding constraint graph.  1. If G has no negative-weight cycles, then x =  δ v0, v1 , δ v0, v2 , . . . , δ v0, vn   is a feasible solution.  2. If G has a negative-weight cycle, then there is no feasible solution.  Proof 1. Show no negative-weight cycles ⇒ feasible solution. Need to show that x j − xi ≤ bk for all constraints. Use x j = δ v0, v j   xi = δ v0, vi   bk = w vi , v j   . By the triangle inequality, δ v0, v j   ≤ δ v0, vi   + w vi , v j   x j − xi ≤ bk . Therefore, feasible. Without loss of generality, let a negative-weight cycle be c =  cid:2 v1, v2, . . . , vk cid:3 , where v1 = vk.  v0 can cid:146 t be on c, since v0 has no entering edges.  c corresponds to the constraints  2. Show negative-weight cycles ⇒ no feasible solution.  x j ≤ xi + bk  x2 − x1 ≤ w v1, v2  , x3 − x2 ≤ w v2, v3  ,  ...  xk−1 − xk−2 ≤ w vk−2, vk−1  , xk − xk−1 ≤ w vk−1, vk  . [The last twoinequalities above are incorrect inthe  cid:222 rstthree printings ofthe book. Theywerecorrectedinthefourthprinting.] If x is a solution satisfying these inequalities, it must satisfy their sum. So add them up.   24-12  Lecture Notes for Chapter 24: Single-Source Shortest Paths  Each xi is added once and subtracted once.  v1 = vk ⇒ x1 = xk.  We get 0 ≤ w c . But w c  < 0, since c is a negative-weight cycle. Contradiction ⇒ no such feasible solution x exists.   theorem   How to  cid:222 nd a feasible solution  1. Form constraint graph.    n + 1 vertices.   m + n edges.    cid:2  m + n  time.  2. Run BELLMAN-FORD from v0.    O  n + 1  m + n   = O n2 + nm  time.  3. If BELLMAN-FORD returns FALSE ⇒ no feasible solution.  If BELLMAN-FORD returns TRUE ⇒ set xi = δ v0, vi   for all i.   Solutions for Chapter 24: Single-Source Shortest Paths  Solution to Exercise 24.1-3  The proof of the convergence property shows that for every vertex v, the shortest- path estimate d[v] has attained its  cid:222 nal value after length  any shortest-weight path to v  iterations of BELLMAN-FORD. Thus after m passes, BELLMAN-FORD can terminate. We don cid:146 t know m in advance, so we can cid:146 t make the algorithm loop exactly m times and then terminate. But if we just make the algorithm stop when nothing changes any more, it will stop after m+1 iterations  i.e., after one iteration that makes no changes .  BELLMAN-FORD- M+1   G, w, s  INITIALIZE-SINGLE-SOURCE G, s  changes ← TRUE while changes = TRUE  do changes ← FALSE  for each edge  u, v  ∈ E[G] do RELAX-M u, v, w   RELAX-M u, v, w  if d[v] > d[u] + w u, v  then d[v] ← d[u] + w u, v   π[v] ← u changes ← TRUE  The test for a negative-weight cycle  based on there being a d that would change if another relaxation step was done  has been removed above, because this version of the algorithm will never get out of the while loop unless all d cid:146 s stop changing.  Solution to Exercise 24.2-3  We cid:146 ll give two ways to transform a PERT chart G =  V, E  with weights on  cid:4   with weights on edges. In each way, we cid:146 ll vertices to a PERT chart G have that V the same   cid:4  ≤ V + E. We can then run on G   cid:4  ≤ 2V and E   cid:4  =  V   cid:4 , E   cid:4    24-14  Solutions for Chapter 24: Single-Source Shortest Paths   cid:4    cid:4    cid:4    cid:4    cid:4   in V  in E  in E  and v cid:4  cid:4    cid:4  cid:4 , v cid:4   ∈ E  . In other words, if  u, v  ∈ E, then  u  . , and all edges in E that leave v will . All such edges for all vertices v ∈ V , and these  cid:4  = 2V, equals the vertex weight  algorithm to  cid:222 nd a longest path through a dag as is given in Section 24.2 of the text. In the  cid:222 rst way, we transform each vertex v ∈ V into two vertices v cid:4  All edges in E that enter v will enter v cid:4  leave v cid:4  cid:4  have weight 0. We also put edges  v cid:4 , v cid:4  cid:4   into E edges are given the weight of the corresponding vertex v in G. Thus,V  cid:4  = V + E, and the edge weight of each path in G E  cid:4  of the corresponding path in G. In the second way, we leave vertices in V alone, but we add one new source vertex s also includes an to V edge  s, v  for every vertex v ∈ V that has in-degree 0 in G. Thus, the only vertex is the with in-degree 0 in G  cid:4  weight of vertex v in G. In other words, the weight of each entering edge in G is the weight of the vertex it enters in G. In effect, we have  cid:147 pushed back cid:148  the weight  cid:4  ≤ V + E of each vertex onto the edges that enter it. Here,V  since no more than V vertices have in-degree 0 in G , and again the edge weight of each path in G  is the new source s. The weight of edge  u, v  ∈ E  cid:4    cid:4  = V ∪ {s}. All edges of E are in E  equals the vertex weight of the corresponding path in G.   cid:4  = V + 1, E  , so that V  , and E   cid:4    cid:4    cid:4    cid:4    cid:4   Solution to Exercise 24.3-3  Solution to Exercise 24.3-4  Yes, the algorithm still works. Let u be the leftover vertex that does not get ex- If u is not reachable from s, then d[u] = tracted from the priority queue Q. δ s, u  = ∞. If u is reachable from s, there is a shortest path p = s  cid:2  x → u. When the node x was extracted, d[x] = δ s, x  and then the edge  x, u  was re- laxed; thus, d[u] = δ s, u .  To  cid:222 nd the most reliable path between s and t, run Dijkstra cid:146 s algorithm with edge weights w u, v  = − lg r  u, v  to  cid:222 nd shortest paths from s in O E+V lg V   time. The most reliable path is the shortest path from s to t, and that path cid:146 s reliability is the product of the reliabilities of its edges. Here cid:146 s why this method works. Because the probabilities are independent, the probability that a path will not fail is the product of the probabilities that its edges will not fail. We want to  cid:222 nd a path s p cid:2  t such that  u,v ∈ p r  u, v  is maximized.  u,v ∈ p lg r  u, v , which This is equivalent to maximizing lg  − lg r  u, v .  Note: r  u, v  can be 0, is in turn equivalent to minimizing and lg 0 is unde cid:222 ned. So in this algorithm, de cid:222 ne lg 0 = −∞.  Thus if we assign weights w u, v  = − lg r  u, v , we have a shortest-path problem. Since lg 1 = 0, lg x < 0 for 0 < x < 1, and we have de cid:222 ned lg 0 = −∞, all the weights w are nonnegative, and we can use Dijkstra cid:146 s algorithm to  cid:222 nd the shortest paths from s in O E + V lg V   time.     u,v ∈ p r  u, v   = cid:1      cid:1    u,v ∈ p   Solutions for Chapter 24: Single-Source Shortest Paths  24-15  Alternate answer  You can also work with the original probabilities by running a modi cid:222 ed version of Dijkstra cid:146 s algorithm that maximizes the product of reliabilities along a path instead of minimizing the sum of weights along a path. In Dijkstra cid:146 s algorithm, use the reliabilities as edge weights and substitute   max  and EXTRACT-MAX  for min  and EXTRACT-MIN  in relaxation and the   × for + in relaxation,  queue, 1  identity for ×  for 0  identity for +  and −∞  identity for min  for ∞  iden- tity for max .     For example, the following is used instead of the usual RELAX procedure:  RELAX-RELIABILITY u, v, r   if d[v] < d[u] · r  u, v  then d[v] ← d[u] · r  u, v   π[v] ← u  This algorithm is isomorphic to the one above: It performs the same operations except that it is working with the original probabilities instead of the transformed ones.  Observe that if a shortest-path estimate is not ∞, then it cid:146 s at most  V − 1 W . Why? In order to have d[v] < ∞, we must have relaxed an edge  u, v  with d[u] < ∞. By induction, we can show that if we relax  u, v , then d[v] is at most the number of edges on a path from s to v times the maximum edge weight. Since any acyclic path has at most V − 1 edges and the maximum edge weight is W , we see that d[v] ≤  V− 1 W . Note also that d[v] must also be an integer, unless it is ∞. We also observe that in Dijkstra cid:146 s algorithm, the values returned by the EXTRACT- MIN calls are monotonically increasing over time. Why? After we do our initial V INSERT operations, we never do another. The only other way that a key value can change is by a DECREASE-KEY operation. Since edge weights are nonneg- ative, when we relax an edge  u, v , we have that d[u] ≤ d[v]. Since u is the minimum vertex that we just extracted, we know that any other vertex we extract later has a key value that is at least d[u]. When keys are known to be integers in the range 0 to k and the key values extracted are monotonically increasing over time, we can implement a min-priority queue so that any sequence of m INSERT, EXTRACT-MIN, and DECREASE-KEY operations takes O m + k  time. Here cid:146 s how. We use an array, say A[0 . . k], where A[ j] is a linked list of each element whose key is j. Think of A[ j] as a bucket for all elements with key j. We implement each bucket by a circular, doubly linked list with a sentinel, so that we can insert into or delete from each bucket in O 1  time. We perform the min-priority queue operations as follows:  Solution to Exercise 24.3-6   24-16  Solutions for Chapter 24: Single-Source Shortest Paths     INSERT: To insert an element with key j, just insert it into the linked list in A[ j]. Time: O 1  per INSERT.    EXTRACT-MIN: We maintain an index min of the value of the smallest key extracted. Initially, min is 0. To  cid:222 nd the smallest key, look in A[min] and, if this list is nonempty, use any element in it, removing the element from the list and returning it to the caller. Otherwise, we rely on the monotonicity property  and that there is no INCREASE-KEY operation  and increment min until we either  cid:222 nd a list A[min] that is nonempty  using any element in A[min] as before  or we run off the end of the array A  in which case the min-priority queue is empty . Since there are at most m INSERT operations, there are at most m elements in the min-priority queue. We increment min at most k times, and we remove and return some element at most m times. Thus, the total time over all EXTRACT- MIN operations is O m + k .   DECREASE-KEY: To decrease the key of an element from j to i,  cid:222 rst check whether i ≤ j,  cid:223 agging an error if not. Otherwise, we remove the element from its list A[ j] in O 1  time and insert it into the list A[i] in O 1  time. Time: O 1  per DECREASE-KEY.  To apply this kind of min-priority queue to Dijkstra cid:146 s algorithm, we need to let k =  V − 1 W , and we also need a separate list for keys with value ∞. The num- ber of operations m is O V + E   since there areV INSERT and V EXTRACT- MIN operations and at mostE DECREASE-KEY operations , and so the total time is O V + E + V W   = O V W + E .  First, observe that at any time, there are at most W + 2 distinct key values in the priority queue. Why? A key value is either ∞ or it is not. Consider what happens whenever a key value d[v] becomes  cid:222 nite. It must have occurred due to the relaxation of an edge  u, v . At that time, u was being placed into S, and d[u] ≤ d[y] for all vertices y ∈ V − S. After relaxing edge  u, v , we have d[v] ≤ d[u] + W . Since any other vertex y ∈ V − S with d[y] < ∞ also had its estimate changed by a relaxation of some edge x with d[x] ≤ d[u], we must have d[y] ≤ d[x]+ W ≤ d[u]+ W . Thus, at the time that we are relaxing edges from a vertex u, we must have, for all vertices v ∈ V − S, that d[u] ≤ d[v] ≤ d[u] + W or d[v] = ∞. Since shortest-path estimates are integer values  except for ∞ , at any given moment we have at most W + 2 different ones: d[u], d[u] + 1, d[u] + 2, . . . , d[u] + W and ∞. Therefore, we can maintain the min-priorty queue as a binary min-heap in which each node points to a doubly linked list of all vertices with a given key value. There are at most W + 2 nodes in the heap, and so EXTRACT-MIN runs in O lg W   time. To perform DECREASE-KEY, we need to be able to  cid:222 nd the heap node corresponding to a given key in O lg W   time. We can do so in O 1  time as follows. First, keep a pointer inf to the node containing all the ∞ keys. Second, maintain an array loc[0 . . W ], where loc[i] points to the unique heap entry whose  Solution to Exercise 24.3-7   Solution to Exercise 24.4-4  Solutions for Chapter 24: Single-Source Shortest Paths  24-17  key value is congruent to i  mod  W + 1  . As keys move around in the heap, we can update this array in O 1  time per movement. Alternatively, instead of using a binary min-heap, we could use a red-black tree. Now INSERT, DELETE, MINIMUM, and SEARCH cid:151 from which we can construct the priority-queue operations cid:151 each run in O lg W   time.  xs ≤ 0 , −xs ≤ 0 ,  δ s  = 0  Let δ u  be the shortest-path weight from s to u. Then we want to  cid:222 nd δ t . δ must satisfy δ v  − δ u  ≤ w u, v  for all  u, v  ∈ E where w u, v  is the weight of edge  u, v . Thus xv = δ v  is a solution to xv − xu ≤ w u, v  . To turn this into a set of inequalities of the required form, replace xs = 0 by xs ≤ 0 and −xs ≤ 0  i.e., xs ≥ 0 . The constraints are now  xs = 0   Lemma 24.10   xv − xu ≤ w u, v  , which still has xv = δ v  as a solution. However, δ isn cid:146 t the only solution to this set of inequalities.  For example, if all edge weights are nonnegative, all xi = 0 is a solution.  To force xt = δ t  as required by the shortest-path problem, add the requirement to maximize  the ob- jective function  xt . This is correct because   max xt   ≥ δ t  because xt = δ t  is part of one solution to the set of inequali-   max xt   ≤ δ t  can be demonstrated by a technique similar to the proof of  ties,   cid:2   cid:2    u,v ∈ p  w u, v  .  Theorem 24.9: Let p be a shortest path from s to t. Then by de cid:222 nition, δ t  = But for each edge  u, v  we have the inequality xv − xu ≤ w u, v , so w u, v  ≥ δ t  = But xs = 0, so xt ≤ δ t .   xv − xu   = xt − xs .   cid:2    u,v ∈ p   u,v ∈ p  Note: Maximizing xt subject to the above inequalities solves the single-pair shortest-path problem when t is reachable from s and there are no negative-weight cycles. But if there cid:146 s a negative-weight cycle, the inequalities have no feasible so- lution  as demonstrated in the proof of Theorem 24.9 ; and if t is not reachable from s, then xt is unbounded.   24-18  Solutions for Chapter 24: Single-Source Shortest Paths  Solution to Exercise 24.4-7  Observe that after the  cid:222 rst pass, all d values are at most 0, and that relaxing edges  v0, vi   will never again change a d value. Therefore, we can eliminate v0 by running the Bellman-Ford algorithm on the constraint graph without the v0 node but initializing all shortest path estimates to 0 instead of ∞.  Solution to Exercise 24.4-10  To allow for single-variable constraints, we add the variable x0 and let it correspond to the source vertex v0 of the constraint graph. The idea is that, if there are no negative-weight cycles containing v0, we will  cid:222 nd that δ v0, v0  = 0. In this case, we set x0 = 0, and so we can treat any single-variable constraint using xi as if it were a 2-variable constraint with x0 as the other variable. Speci cid:222 cally, we treat the constraint xi ≤ bk as if it were xi − x0 ≤ bk, and we add the edge  v0, vi   with weight bk to the constraint graph. We treat the constraint −xi ≤ bk as if it were x0 − xi ≤ bk, and we add the edge  vi , v0  with weight bk to the constraint graph. Once we  cid:222 nd shortest-path weights from v0, we set xi = δ v0, vi   for all i = 0, 1, . . . , n; that is, we do as before but also include x0 as one of the variables that we set to a shortest-path weight. Since v0 is the source vertex, either x0 = 0 or x0 < 0. If δ v0, v0  = 0, so that x0 = 0, then setting xi = δ v0, vi   for all i = 0, 1, . . . , n gives a feasible solution for the system. The only new constraints beyond those in the text are those involving x0. For constraints xi ≤ bk, we use xi − x0 ≤ bk. By the triangle inequality, δ v0, vi   ≤ δ v0, v0  + w v0, vi   = bk, and so xi ≤ bk. For constraints −xi ≤ bk, we use x0 − xi ≤ bk. By the triangle inequality, 0 = δ v0, v0  ≤ δ v0, vi   + w vi , v0 ; thus, 0 ≤ xi + bk or, equivalently, −xi ≤ bk. If δ v0, v0  < 0, so that x0 < 0, then there is a negative-weight cycle containing v0. The portion of the proof of Theorem 24.9 that deals with negative-weight cycles carries through but with v0 on the negative-weight cycle, and we see that there is no feasible solution.  Solution to Exercise 24.5-4  Whenever RELAX sets π for some vertex, it also reduces the vertex cid:146 s d value. Thus if π[s] gets set to a non-NIL value, d[s] is reduced from its initial value of 0 to a negative number. But d[s] is the weight of some path from s to s, which is a cycle including s. Thus, there is a negative-weight cycle.   Solutions for Chapter 24: Single-Source Shortest Paths  24-19  Solution to Exercise 24.5-7  Solution to Exercise 24.5-8  Suppose we have a shortest-paths tree Gπ . Relax edges in Gπ according to the order in which a BFS would visit them. Then we are guaranteed that the edges along each shortest path are relaxed in order. By the path-relaxation property, we would then have d[v] = δ s, v  for all v ∈ V . Since Gπ contains at most V − 1 edges, we need to relax only V − 1 edges to get d[v] = δ s, v  for all v ∈ V .  Suppose that there is a negative-weight cycle c =  cid:2 v0, v1, . . . , vk cid:3 , where v0 = vk, that is reachable from the source vertex s; thus, w c  < 0. With- out loss of generality, c is simple. There must be an acyclic path from s to some vertex of c that uses no other vertices in c. Without loss of generality let this vertex of c be v0, and let this path from s to v0 be p =  cid:2 u0, u1, . . . , ul cid:3 , where u0 = s and ul = v0 = vk.  It may be the case that ul = s, in which case path p has no edges.  After the call to INITIALIZE-SINGLE-SOURCE sets d[v] = ∞ for all v ∈ V − {s}, perform the following sequence of re- laxations. First, relax every edge in path p, in order. Then relax every edge in cycle c, in order, and repeatedly relax the cycle. That is, we relax the edges  u0, u1 ,  u1, u2 , . . . ,  ul−1, v0 ,  v0, v1 ,  v1, v2 , . . . ,  vk−1, v0 ,  v0, v1 ,  v1, v2 , . . . ,  vk−1, v0 ,  v0, v1 ,  v1, v2 , . . . ,  vk−1, v0 , . . .. We claim that every edge relaxation in this sequence reduces a shortest-path es- timate. Clearly, the  cid:222 rst time we relax an edge  ui−1, ui   or  v j−1, v j  , for i = 1, 2, . . . , l and j = 1, 2, . . . , k − 1  note that we have not yet relaxed the last edge of cycle c , we reduce d[ui ] or d[v j ] from ∞ to a  cid:222 nite value. Now consider the relaxation of any edge  vj−1, v j   after this opening sequence of re- laxations. We use induction on the number of edge relaxations to show that this relaxation reduces d[vj ]. Basis: The next edge relaxed after the opening sequence is  vk−1, vk . Before re- laxation, d[vk] = w  p , and after relaxation, d[vk] = w  p  + w c  < w  p , since w c  < 0. Inductive step: Consider the relaxation of edge  vj−1, v j  . Since c is a sim- ple cycle, the last time d[vj ] was updated was by a relaxation of this same edge. By the inductive hypothesis, d[vj−1] has just been reduced. Thus, d[v j−1] + w v j−1, v j   < d[v j ], and so the relaxation will reduce the value of d[v j ].  Solution to Problem 24-1  a. Assume for contradiction that G f is not acyclic; thus G f has a cycle. A cycle must have at least one edge  u, v  in which u has higher index than v. This   24-20  Solutions for Chapter 24: Single-Source Shortest Paths  edge is not in E f  by the de cid:222 nition of E f  , in contradition to the assumption that G f has a cycle. Thus G f is acyclic.  cid:2 v1, v2, . . . , vV cid:3  is a topological sort for G f , because from the de cid:222 nition of E f we know that all edges are directed from smaller indices to larger indices. The proof for Eb is similar. b. For all vertices v ∈ V , we know that either δ s, v  = ∞ or δ s, v  is  cid:222 nite. If δ s, v  = ∞, then d[v] will be ∞. Thus, we need to consider only the case where d[v] is  cid:222 nite. There must be some shortest path from s to v. Let p =  cid:2 v0, v1, . . . , vk−1, vk cid:3  be that path, where v0 = s and vk = v. Let us now consider how many times there is a change in direction in p, that is, a situation in which  vi−1, vi   ∈ E f and  vi , vi+1  ∈ Eb or vice versa. There can be at most V − 1 edges in p, so there can be at mostV − 2 changes in direction. Any portion of the path where there is no change in direction is computed with the correct d values in the  cid:222 rst or second half of a single pass once the node that begins the no-change-in-direction sequence has the correct d value, because the edges are relaxed in the order of the direction of the sequence. Each change in direction requires a half pass in the new direction of the path. The following table shows the maximum number of passes needed depending on the parity of V − 1 and the direction of the  cid:222 rst edge: V − 1  cid:222 rst edge direction passes  V − 1  2 even  V − 1  2 + 1 even V  2 odd V  2 odd In any case, the maximum number of passes that we will need is cid:14 V  2 cid:15 . c. This scheme does not affect the asymptotic running time of the algorithm be- cause even though we perform only cid:14 V  2 cid:15  passes instead of V − 1 passes, it is still O V   passes. Each pass still takes  cid:2  E  time, so the running time remains O V E .  forward backward forward backward  Solution to Problem 24-2  a. Consider boxes with dimensions x =  x1, . . . , xd  , y =  y1, . . . , yd  , and z =  z1, . . . , zd  . Suppose there exists a permutation π such that xπ i  < yi for i = 1, . . . , d and there exists a permutation π cid:4  such that yπ cid:4  i  < zi for i = 1, . . . , d, so that x nests inside y and y nests inside z. Construct a , where π cid:4  cid:4  i   = π cid:4  π i   . Then for i = 1, . . . , d, we have permutation π cid:4  cid:4  xπ cid:4  cid:4  i  = xπ cid:4  π i   < yπ cid:4  i  < zi, and so x nests inside z.  b. Sort the dimensions of each box from longest to shortest. A box X with sorted dimensions  x1, x2, . . . , xd   nests inside a box Y with sorted dimensions  y1, y2, . . . , yd   if and only if xi < yi for i = 1, 2, . . . , d. The sorting can be done in O d lg d  time, and the test for nesting can be done in O d  time, and so the algorithm runs in O d lg d  time. This algorithm works because a   Solution to Problem 24-3  Solutions for Chapter 24: Single-Source Shortest Paths  24-21   cid:11    cid:10   d-dimensional box can be oriented so that every permutation of its dimensions is possible.  Experiment with a 3-dimensional box if you are unsure of this . c. Construct a dag G =  V, E , where each vertex vi corresponds to box Bi, and  vi , v j   ∈ E if and only if box Bi nests inside box Bj. Graph G is indeed a dag, because nesting is transitive and antire cid:223 exive. The time to construct the dag is O dn2 + dn lg d , from comparing each of the pairs of boxes after sorting the dimensions of each. Add a supersource vertex s and a supersink vertex t to G, and add edges  s, vi   for all vertices vi with in-degree 0 and  vj , t  for all vertices vj with out-  cid:4  degree 0. Call the resulting dag G  cid:4  .  Section 24.2 discusses how to  cid:222 nd a Find a longest path from s to t in G longest path in a dag.  This path corresponds to a longest sequence of nesting has n + 2 vertices and boxes. The time to  cid:222 nd a longest path is O n2 , since G O n2  edges. Overall, this algorithm runs in O dn2 + dn lg d  time.  . The time to do so is O n .  n 2   cid:4   a. We can use the Bellman-Ford algorithm on a suitable weighted, directed graph G =  V, E , which we form as follows. There is one vertex in V for each currency, and for each pair of currencies ci and c j , there are directed edges   vi , v j   and  v j , vi  .  Thus, V = n and E = cid:10    cid:11   .   n 2  To determine edge weights, we start by observing that R[i1, i2] · R[i2, i3]··· R[ik−1, ik] · R[ik , i1] > 1 if and only if  1  ·  1  ···  1  ·  1  < 1 .  R[i2, i3]  R[i1, i2] Taking logs of both sides of the inequality above, we express this condition as  R[ik, i1]  R[ik−1, ik]  1  lg  + lg  1  + lg  R[i1, i2]  R[i2, i3]  1  R[ik−1, ik]  + ··· + lg  1  R[ik, i1]  < 0 .  Therefore, if we de cid:222 ne the weight of edge  vi , v j   as w vi , v j   = lg  1  R[i, j]  = − lg R[i, j] ,  then we want to  cid:222 nd whether there exists a negative-weight cycle in G with these edge weights. We can determine whether there exists a negative-weight cycle in G by adding an extra vertex v0 with 0-weight edges  v0, vi   for all vi ∈ V , running BELLMAN-FORD from v0, and using the boolean result of BELLMAN-FORD  which is TRUE if there are no negative-weight cycles and FALSE if there is a   24-22  Solutions for Chapter 24: Single-Source Shortest Paths  Solution to Problem 24-4  negative-weight cycle  to guide our answer. That is, we invert the boolean result of BELLMAN-FORD. This method works because adding the new vertex v0 with 0-weight edges from v0 to all other vertices cannot introduce any new cycles, yet it ensures that all negative-weight cycles are reachable from v0. It takes  cid:2  n2  time to create G, which has  cid:2  n2  edges. Then it takes O n3  time to run BELLMAN-FORD. Thus, the total time is O n3 . Another way to determine whether a negative-weight cycle exists is to create G and, without adding v0 and its incident edges, run either of the all-pairs shortest- paths algorithms. If the resulting shortest-path distance matrix has any negative values on the diagonal, then there is a negative-weight cycle.  b. Assuming that we ran BELLMAN-FORD to solve part  a , we only need to  cid:222 nd the vertices of a negative-weight cycle. We can do so as follows. First, relax all the edges once more. Since there is a negative-weight cycle, the d value of some vertex u will change. We just need to repeatedly follow the π values until we get back to u. In other words, we can use the recursive method given by the PRINT-PATH procedure of Section 22.2, but stop it when it returns to vertex u. The running time is O n3  to run BELLMAN-FORD, plus O n  to print the vertices of the cycle, for a total of O n3  time.  a. Since all weights are nonnegative, use Dijkstra cid:146 s algorithm.  Implement the priority queue as an array Q[0 . .E + 1], where Q[i] is a list of vertices v for which d[v] = i. Initialize d[v] for v  cid:19 = s toE + 1 instead of to ∞, so that all vertices have a place in Q.  Any initial d[v] > δ s, v  works in the algorithm, since d[v] decreases until it reaches δ s, v .  The V EXTRACT-MINs can be done in O E  total time, and decreasing a d value during relaxation can be done in O 1  time, for a total running time of O E .   When d[v] decreases, just add v to the front of the list in Q[d[v]].   EXTRACT-MIN removes the head of the list in the  cid:222 rst nonempty slot of Q. To do EXTRACT-MIN without scanning all of Q, keep track of the smallest i for which Q[i] is not empty. The key point is that when d[v] decreases due to relaxation of edge  u, v , d[v] remains ≥ d[u], so it never moves to an earlier slot of Q than the one that had u, the previous minimum. Thus EXTRACT-MIN can always scan upward in the array, taking a total of O E  time for all EXTRACT-MINs. b. For all  u, v  ∈ E, we have w1 u, v  ∈ {0, 1}, so δ1 s, v  ≤ V− 1 ≤ E. Use part  a  to get the O E  time bound. c. To show that wi  u, v  = 2wi−1 u, v  or wi  u, v  = 2wi−1 u, v  + 1, observe that the i bits of wi  u, v  consist of the i − 1 bits of wi−1 u, v  followed by one   Solutions for Chapter 24: Single-Source Shortest Paths  24-23  more bit. If that low-order bit is 0, then wi  u, v  = 2wi−1 u, v ; if it is 1, then wi  u, v  = 2wi−1 u, v  + 1. Notice the following two properties of shortest paths:  1. If all edge weights are multiplied by a factor of c, then all shortest-path  weights are multiplied by c. 2. If all edge weights are increased by at most c, then all shortest-path weights are increased by at most c V − 1 , since all shortest paths have at most V − 1 edges.  The lowest possible value for wi  u, v  is 2wi−1 u, v , so by the  cid:222 rst observa- tion, the lowest possible value for δi  s, v  is 2δi−1 s, v . The highest possible value for wi  u, v  is 2wi−1 u, v  + 1. Therefore, us- ing the two observations together, the highest possible value for δi  s, v  is 2δi−1 s, v  + V − 1. &wi  u, v  = wi  u, v  + 2δi−1 s, u  − 2δi−1 s, v  ≥ 2wi−1 u, v  + 2δi−1 s, u  − 2δi−1 s, v  ≥ 0 .  d. We have  The third line follows from  cancel for every intermediate vertex t on the path. Thus,  The second line follows from part  c . e. Observe that if we compute&wi   p  for any path p : u  cid:2  v, the terms δi−1 s, t  Lemma 24.10: δi−1 s, v  ≤ δi−1 s, u  + wi−1 u, v . &wi   p  = wi   p  + 2δi−1 s, u  − 2δi−1 s, v  . therefore the same paths will be of minimum wi weight and of minimum &wi in equation  25.10  within the proof of  This will be shown in detail Lemma 25.1.  The δi−1 terms depend only on u, v, and s, but not on the path p; &δi  s, v  = δi  s, v  + 2δi−1 s, s  − 2δi−1 s, v  = δi  s, v  − 2δi−1 s, v  . weight between u and v. Letting u = s, we get Rewriting this result as δi  s, v  =&δi  s, v  + 2δi−1 s, v  and combining it with δi  s, v  ≤ 2δi−1 s, v +V−1  from part  c   gives us&δi  s, v  ≤ V−1 ≤ E. 1. Compute the weights&wi  u, v  in O E  time, as shown in part  d . f. To compute δi  s, v  from δi−1 s, v  for all v ∈ V in O E  time: 2. By part  e ,&δi  s, v  ≤ E, so use part  a  to compute all&δi  s, v  in O E  3. Compute all δi  s, v  from&δi  s, v  and δi−1 s, v  as shown in part  e , in  time.  O V   time.  To compute all δ s, v  in O E lg W   time: 1. Compute δ1 s, v  for all v ∈ V . As shown in part  b , this takes O E  time. 2. For each i = 2, 3, . . . , k, compute all δi  s, v  from δi−1 s, v  in O E  time as shown above. This procedure computes δ s, v  = δk  u, v  in time O Ek  = O E lg W  .   24-24  Solutions for Chapter 24: Single-Source Shortest Paths  Solution to Problem 24-6  Observe that a bitonic sequence can increase, then decrease, then increase, or it can decrease, then increase, then decrease. That is, there can be at most two changes of direction in a bitonic sequence. Any sequence that increases, then decreases, then increases, then decreases has a bitonic sequence as a subsequence. Now, let us suppose that we had an even stronger condition than the bitonic prop- erty given in the problem: for each vertex v ∈ V , the weights of the edges along any shortest path from s to v are increasing. Then we could call INITIALIZE- SINGLE-SOURCE and then just relax all edges one time, going in increasing order of weight. Then the edges along every shortest path would be relaxed in order of their appearance on the path.  We rely on the uniqueness of edge weights to en- sure that the ordering is correct. [Notethattheuniqueness assumption wasadded in the  cid:222 fth printing of the text.]   The path-relaxation property  Lemma 24.15  would guarantee that we would have computed correct shortest paths from s to each vertex. If we weaken the condition so that the weights of the edges along any shortest path increase and then decrease, we could relax all edges one time, in increasing order of weight, and then one more time, in decreasing order of weight. That order, along with uniqueness of edge weights, would ensure that we had relaxed the edges of every shortest path in order, and again the path-relaxation property would guarantee that we would have computed correct shortest paths. To make sure that we handle all bitonic sequences, we do as suggested above. That is, we perform four passes, relaxing each edge once in each pass. The  cid:222 rst and third passes relax edges in increasing order of weight, and the second and fourth passes in decreasing order. Again, by the path-relaxation property and the uniqueness of edge weights, we have computed correct shortest paths. The total time is O V + E lg V  , as follows. The time to sort E edges by weight is O E lg E  = O E lg V    since E = O V 2  . INITIALIZE-SINGLE-SOURCE takes O V   time. Each of the four passes takes O E  time. Thus, the total time is O E lg V + V + E  = O V + E lg V  .   Lecture Notes for Chapter 25: All-Pairs Shortest Paths  Chapter 25 overview  Given a directed graph G =  V, E , weight function w : E → R,V = n. Goal: create an n × n matrix of shortest-path distances δ u, v . Could run BELLMAN-FORD once from each vertex:   O V 2 E  cid:151 which is O V 4  if the graph is dense  E =  cid:2  V 2  . If no negative-weight edges, could run Dijkstra cid:146 s algorithm once from each vertex:   O V E lg V   with binary heap cid:151 O V 3 lg V   if dense,   O V 2 lg V + V E  with Fibonacci heap cid:151 O V 3  if dense. We cid:146 ll see how to do in O V 3  in all cases, with no fancy data structure.  Shortest paths and matrix multiplication  ⎧⎨⎩0  Assume that G is given as adjacency matrix of weights: W =  wi j  , with vertices numbered 1 to n. wi j = Output is matrix D =  di j  , where di j = δ i, j  . Won cid:146 t worry about predeces- sors cid:151 see book. Will use dynamic programming at  cid:222 rst.  if i = j , if i  cid:19 = j,  i, j   ∈ E , if i  cid:19 = j,  i, j    ∈ E .  weight of  i, j   ∞  Optimal substructure: Recall: subpaths of shortest paths are shortest paths.  = weight of shortest path i  cid:2  j that contains ≤ m  Recursive solution: Let l m  i j edges.    m = 0⇒ there is a shortest path i  cid:2  j with ≤ m edges if and only if i = j ⇒ l  0    cid:9  if i = j , 0 ∞ if i  cid:19 = j .  =  i j   25-2  Lecture Notes for Chapter 25: All-Pairs Shortest Paths    m ≥ 1 ⇒ l  m   i j    cid:8   = min   cid:7  l  m−1  ’ l  m−1   ’ l  m−1     k is all possible predecessors of j  = min 1≤k≤n since w j j = 0 for all j .  , min 1≤k≤n + wkj  + wkj  ik  ik  i j    Observe that when m = 1, must have l 1   = wi j .  i j  ’ l  0  ik + wi j  Conceptually, when the path is restricted to at most 1 edge, the weight of the shortest path i  cid:2  j must be wi j . And the math works out, too: l  1  i j  + wkj     ii   l  0  ii  is the only non-∞ among l 0  ik    = min 1≤k≤n = l  0  = wi j . All simple shortest paths contain ≤ n − 1 edges ⇒ δ i, j   = l  n−1  Compute a solution bottom-up: Compute L 1 , L  2 , . . . , L  n−1 . Start with L  1  = W , since l 1  Go from L  m−1  to L  m :  = l  n+1   = wi j .  = l  n   = . . .  i j  i j  i j  i j  ← min l   cid:4  i j  , lik + wkj     cid:4   EXTEND L , W, n  create L for i ← 1 to n  , an n × n matrix  do for j ← 1 to n ← ∞  cid:4  i j   cid:4  for k ← 1 to n i j  do l  do l   cid:4   return L  Compute each L m :  SLOW-APSP W, n  L  1  ← W for m ← 2 to n − 1 return L  n−1   Time:   EXTEND:  cid:2  n3 .    SLOW-APSP:  cid:2  n4 .  do L  m  ← EXTEND L  m−1 , W, n    Lecture Notes for Chapter 25: All-Pairs Shortest Paths  25-3  Observation: EXTEND is like matrix multiplication: L → A W → B  cid:4  → C L min → + + → · ∞ → 0 create C, an n × n matrix for i ← 1 to n  do for j ← 1 to n do ci j ← 0  for k ← 1 to n  do ci j ← ci j + aik · bkj  So, we can view EXTEND as just like matrix multiplication! Why do we care? Because our goal is to compute L n−1  as fast as we can. Don cid:146 t need to compute all the intermediate L 1 , L  2 , L  3 , . . . , L  n−2 . Suppose we had a matrix A and we wanted to compute An−1  like calling EXTEND n − 1 times . Could compute A, A2, A4, A8, . . . If we knew Am = An−1 for all m ≥ n − 1, could just  cid:222 nish with Ar , where r is the smallest power of 2 that cid:146 s ≥ n − 1.  r = 2 FASTER-APSP W, n  L  1  ← W m ← 1 while m < n − 1 m ← 2m  do L  2m  ← EXTEND L  m , L  m , n    cid:14 lg n−1  cid:15      return L  m  OK to overshoot, since products don cid:146 t change after L n−1 .  Time:  cid:2  n3 lg n .  Floyd-Warshall algorithm  A different dynamic-programming approach. For path p =  cid:2 v1, v2, . . . , vl cid:3 , an intermediate vertex is any vertex of p other than v1 or vl. = shortest-path weight of any path i  cid:2  j with all intermediate vertices in Let d  k  {1, 2, . . . , k}. i j Consider a shortest path i p cid:2  j with all intermediate vertices in{1, 2, . . . , k}:   25-4  Lecture Notes for Chapter 25: All-Pairs Shortest Paths        If k is not an intermediate vertex, then all intermediate vertices of p are in {1, 2, . . . , k − 1}. If k is an intermediate vertex:  p1  i  p2  k  j  all intermediate vertices in {1, 2, ..., k cid:150 1}   cid:9   =   cid:11   d  k  i j  Recursive formulation   cid:10  wi j d  k−1  min Want D n  = cid:10   cid:11  = wi j because can cid:146 t have intermediate vertices ⇒ ≤ 1 edge.  , since all vertices numbered ≤ n.  if k = 0 , if k ≥ 1 .  + d  k−1    Have d 0  i j  , d  k−1   kj  ik  i j  d  n  i j  Compute bottom-up  Compute in increasing order of k:  FLOYD-WARSHALL W, n  D 0  ← W for k ← 1 to n  do for i ← 1 to n  do for j ← 1 to n  do d  k  i j  ← min   cid:10  d  k−1   i j  return D n    cid:11   , d  k−1   ik  + d  k−1   kj  Can drop superscripts.  See Exercise 25.2-4 in text.   Time:  cid:2  n3 .  ∗ =  V, E  Transitive closure Given G =  V, E , directed. Compute G   E Could assign weight of 1 to each edge, then run FLOYD-WARSHALL.    ∗ = { i, j   : there is a path i  cid:2  j in G}.  ∗ .  If di j < n, then there is a path i  cid:2  j.    Otherwise, di j = ∞ and there is no path.   Lecture Notes for Chapter 25: All-Pairs Shortest Paths  25-5     Simpler way: Substitute other values and operators in FLOYD-WARSHALL.   Use unweighted adjacency matrix   min → ∨  OR    + → ∧  AND    cid:9  1 if there is path i  cid:2  j with all intermediate vertices in{1, 2, . . . , k} ,  cid:9  0 otherwise . 0 if i  cid:19 = j and  i, j    ∈ E ,  cid:11  1 if i = j or  i, j   ∈ E . .  = = t  k−1   ∧ t  k−1   ∨ cid:10   t  k−1   t  0  i j  t  k  i j  =        i j  ik  kj  t  k  i j  TRANSITIVE-CLOSURE  E, n  for i ← 1 to n  do for j ← 1 to n  do if i = j or  i, j   ∈ E[G]  then t  0  i j else t  0  i j  ← 1 ← 0  for k ← 1 to n  do for i ← 1 to n  do for j ← 1 to n  ∨ cid:10   return T  n   do t  k  i j  ← t  k−1   i j  t  k−1   ik  ∧ t  k−1   kj   cid:11   Time:  cid:2  n3 , but simpler operations than FLOYD-WARSHALL.  Idea: If the graph is sparse, it pays to run Dijkstra cid:146 s algorithm once from each vertex. If we use a Fibonacci heap for the priority queue, the running time is down to O V 2 lg V + V E , which is better than FLOYD-WARSHALL cid:146 s  cid:2  V 3  time if E = o V 2 . But Dijkstra cid:146 s algorithm requires that all edge weights be nonnegative. Donald Johnson  cid:222 gured out how to make an equivalent graph that does have all edge weights ≥ 0. Compute a new weight function&w such that path u  cid:2  v using&w. 1. For all u, v ∈ V , p is a shortest path u  cid:2  v using w if and only if p is a shortest 2. For all  u, v  ∈ E,&w u, v  ≥ 0.  Reweighting  Johnson cid:146 s algorithm   25-6  Lecture Notes for Chapter 25: All-Pairs Shortest Paths  Property  1  says that it suf cid:222 ces to  cid:222 nd shortest paths with&w. Property  2  says we How to come up with&w?  can do so by running Dijkstra cid:146 s algorithm from each vertex.  Lemma shows it cid:146 s easy to get property  1 :  Also, G has a negative-weight cycle with w if and only if G has a negative-weight  Lemma  Reweighting doesn cid:146 t change shortest paths  Given a directed, weighted graph G =  V, E , w : E → R. Let h be any function &w u, v  = w u, v  + h u  − h v  . such that h : V → R. For all  u, v  ∈ E, de cid:222 ne Let p =  cid:2 v0, v1, . . . , vk cid:3  be any path v0  cid:2  vk. with&w.  Formally, w  p  = δ v0, vk  if and only if&w =&δ v0, vk  , where&δ is the shortest-path weight with&w.  Then, p is a shortest path v0  cid:2  vk with w if and only if p is a shortest path v0  cid:2  vk cycle with&w. Proof First, we cid:146 ll show that&w  p  = w  p  + h v0  − h vk : &w  p  = k cid:2  = k cid:2  = k cid:2   &w vi−1, vi    w vi−1, vi   + h vi−1  − h vi     w vi−1, vi   + h v0  − h vk    sum telescopes   i=1  i=1  = w  p  + h v0  − h vk  .  p cid:2  vk has &w  p  = w  p  + h v0  − h vk . Since h v0   Therefore, any path v0 and h vk  don cid:146 t depend on the path from v0 to vk, if one path v0  cid:2  vk is shorter  than another with w, it cid:146 s also shorter with&w. negative-weight cycle with&w: Now show there exists a negative-weight cycle with w if and only if there exists a   Let cycle c =  cid:2 v0, v1, . . . , vk cid:3 , where v0 = vk.   Then&w c  = w c  + h v0  − h vk  = w c  weight cycle with&w. Therefore, c has a negative-weight cycle with w if and only if it has a negative-  lemma  such that when we compute&w u, v  = w u, v  + h u  − h v , it cid:146 s ≥ 0. So, now to get property  2 , we just need to come up with a function h : V → R   since v0 = vk  .  i=1  Do what we did for difference constraints:   G   cid:4  =  V   cid:4 , E   cid:4     Lecture Notes for Chapter 25: All-Pairs Shortest Paths  25-7  a negative-weight cycle if and only if G does.   cid:4  has the same set of cycles as G. In particular, G  has   cid:4  = V ∪ {s}, where s is a new vertex.   V  cid:4  = E ∪ { s, v  : v ∈ V}.   E   w s, v  = 0 for all v ∈ V .   cid:4    Since no edges enter s, G De cid:222 ne h v  = δ s, v  for all v ∈ V . Claim&w u, v  = w u, v  + h u  − h v  ≥ 0. Proof By the triangle inequality, δ s, v  ≤ δ s, u  + w u, v  h v  ≤ h u  + w u, v  . Therefore, w u, v  + h u  − h v  ≥ 0.   claim   Johnson cid:146 s algorithm   cid:4   form G run BELLMAN-FORD on G if BELLMAN-FORD returns FALSE   cid:4   to compute δ s, v  for all v ∈ V  then G has a negative-weight cycle else  compute&w u, v  = w u, v  + δ s, u  − δ s, v  for all  u, v  ∈ E do run Dijkstra cid:146 s algorithm from u using weight function&w for each vertex u ∈ V for each vertex v ∈ V  to compute&δ u, v  for all v ∈ V duv = &δ u, v  + δ s, v  − δ s, u   cid:26   cid:23  then&w  p  = w  p  + h u  − h v   do  cid:1  Compute entry duv in matrix D  because if p is a path u  cid:2  v,   cid:24  cid:25    cid:4   .  Time:    cid:2  V + E  to compute G    cid:2  E  to compute&w.   O V E  to run BELLMAN-FORD.   O V 2 lg V + V E  to run Dijkstra cid:146 s algorithmV times  using Fibonacci heap .    cid:2  V 2  to compute D matrix. Total: O V 2 lg V + V E .   Solutions for Chapter 25: All-Pairs Shortest Paths  The matrix L  0  corresponds to the identity matrix  Solution to Exercise 25.1-3  ⎛⎜⎜⎜⎜⎜⎝  I =  ⎞⎟⎟⎟⎟⎟⎠  1 0 0 ··· 0 0 1 0 ··· 0 0 0 1 ··· 0 ... ... 0 0 0 ··· 1  . . .  ...  ...  of regular matrix multiplication. Substitute 0  the identity for +  for ∞  the iden- tity for min , and 1  the identity for ·  for 0  the identity for + .  Solution to Exercise 25.1-5  i j  The all-pairs shortest-paths algorithm in Section 25.1 computes L  n−1  = W n−1 = L  0  · W n−1 = δ i, j   and L  0  is the identity matrix. That is, the entry in the where l  n−1  ith row and jth column of the matrix  cid:147 product cid:148  is the shortest-path distance from vertex i to vertex j, and row i of the product is the solution to the single-source shortest-paths problem for vertex i. Notice that in a matrix  cid:147 product cid:148  C = A · B, the ith row of C is the ith row of A  cid:147 multiplied cid:148  by B. Since all we want is the ith row of C, we never need more than the ith row of A. · W n−1, Thus the solution to the single-source shortest-paths from vertex i is L 0  where L  0  is the ith row of L 0  cid:151 a vector whose ith entry is 0 and whose other entries are ∞. i Doing the above  cid:147 multiplications cid:148  starting from the left is essentially the same as the BELLMAN-FORD algorithm. The vector corresponds to the d values in BELLMAN-FORD cid:151 the shortest-path estimates from the source to each vertex.   The vector is initially 0 for the source and ∞ for all other vertices, the same as  i  the values set up for d by INITIALIZE-SINGLE-SOURCE.   Solution to Exercise 25.1-10  Solutions for Chapter 25: All-Pairs Shortest Paths  25-9    Each  cid:147 multiplication cid:148  of the current vector by W relaxes all edges just as BELLMAN-FORD does. That is, a distance estimate in the row, say the distance to v, is updated to a smaller estimate, if any, formed by adding some w u, v  to the current estimate of the distance to u.    The relaxation multiplication is done n − 1 times.  Run SLOW-ALL-PAIRS-SHORTEST-PATHS on the graph. Look at the diagonal el- ements of L m . Return the  cid:222 rst value of m for which one  or more  of the diagonal   is negative. If m reaches n + 1, then stop and declare that there are elements  l m  no negative-weight cycles. ∗ Let the number of edges in a minimum-length negative-weight cycle be m ∗ = ∞ if the graph has no negative-weight cycles. m  , where  ii  ∗   ∗ ≤ n and some value of i, we Correctness: Let cid:146 s assume that for some value m ∗  cid:222 nd that l m < 0. Then the graph has a cycle with m edges that goes from vertex i ii to itself, and this cycle has negative weight  stored in l m  . This is the minimum- ii length negative-weight cycle because SLOW-ALL-PAIRS-SHORTEST-PATHS com- putes all paths of 1 edge, then all paths of 2 edges, and so on, and all cycles shorter edges were checked before and did not have negative weight. Now as- than m sume that for all m ≤ n, there is no negative l m  element. This means there is no negative-weight cycle in the graph, because all cycles have length at most n. Time: O n4 . More precisely,  cid:2  n3 · min n, m  ∗  .  ∗   ∗  ii  Faster solution  Run FASTER-ALL-PAIRS-SHORTEST-PATHS on the graph until the  cid:222 rst time that the matrix L m  has one or more negative values on the diagonal, or until we have computed L m  for some m > n. If we  cid:222 nd any negative entries on the diagonal, we know that the minimum-length negative-weight cycle has more than m 2 edges ∗ in the range and at most m edges. We just need to binary search for the value of m ∗ ≤ m. The key observation is that on our way to computing L m , we m 2 < m computed L 1 , L  2 , L  4 , L  8 , . . . , L  m 2 , and these matrices suf cid:222 ce to compute every matrix we cid:146 ll need. Here cid:146 s pseudocode:   25-10  Solutions for Chapter 25: All-Pairs Shortest Paths  FIND-MIN-LENGTH-NEG-WEIGHT-CYCLE  W   n ← rows[W ] L  1  ← W m ← 1 while m ≤ n and no diagonal entries of L m  are negative do L  2m  ← EXTEND-SHORTEST-PATHS  L  m , L  m    m ← 2m  if m > n and no diagonal entries of L m  are negative then return  cid:147 no negative-weight cycles cid:148  elseif m ≤ 2 then return m  else  low ← m 2 high ← m d ← m 4 while d ≥ 1  do s ← low+d  L  s  ← EXTEND-SHORTEST-PATHS L  low , L  d   if L  s  has any negative entries on the diagonal then high ← s else low ← s d ← d 2  return high  Correctness: If, after the  cid:222 rst while loop, m > n and no diagonal entries of L m  are negative, then there is no negative-weight cycle. Otherwise, if m ≤ 2, then either m = 1 or m = 2, and L m  is the  cid:222 rst matrix with a negative entry on the diagonal. Thus, the correct value to return is m. If m > 2, then we maintain an interval bracketed by the values low and high, such ∗ ≤ high. We use the following ∗ that the correct value m loop invariant:  is in the range low < m  Loop invariant: At the start of each iteration of the  cid:147 while d ≥ 1 cid:148  loop, 1. d = 2p for some integer p ≥ −1, 2. d =  high − low  2, ∗ ≤ high. 3. low < m  Initialization: Initially, m is an integer power of 2 and m > 2. Since d = m 4, we have that d is an integer power of 2 and d > 1 2, so that d = 2p for some integer p ≥ 0. We also have  high − low  2 =  m −  m 2   2 = m 4 = d. Finally, L  m  has a negative entry on the diagonal and L m 2  does not. Since low = m 2 and high = m, we have that low < m  ∗ ≤ high.   cid:4   Maintenance: We use high, low, and d to denote variable values in a given it-  cid:4  to denote the same variable values in the next eration, and high iteration. Thus, we wish to show that d = 2p for some integer p ≥ −1 im-  cid:4  ≥ −1, that d =  high − low  2 implies plies d ∗ ≤ high  cid:4  =  high  cid:4  . d  for some integer p  cid:4   2, and that low < m  ∗ ≤ high implies low   cid:4  = 2p   cid:4  − low   cid:4  < m  , and d  , low   cid:4    cid:4    Solutions for Chapter 25: All-Pairs Shortest Paths  25-11   cid:4    cid:4    cid:4  < m  , note that d   cid:4  = 2p   cid:4  ≥ −1.   cid:4   2 =   low+d  − low  2 = d 2 = d   cid:4  = d 2, and so d = 2p−1. The condition that To see that d d ≥ 1 implies that p ≥ 0, and so p Within each iteration, s is set to low+d, and one of the following actions oc- curs:    is set to s and  cid:4   2 =  cid:4  − low . Since L  s  has a negative  cid:4  = s and low  cid:4  = low,   cid:4  If L  s  has any negative entries on the diagonal, then high  cid:4  is set to d 2. Upon entering the next iteration,  high d  s − low ∗ ≤ s. Because high diagonal entry, we know that m ∗ ≤ high  cid:4  . we have that low  cid:4  If L  s  has no negative entries on the diagonal, then low  cid:4  d  high d 2 = d Because low  is set to s, and  cid:4  − low  cid:4   2 = is set to d 2. Upon entering the next iteration,  high  cid:4  −s  2 =  high − low+d   2 =  high− low  2 − d 2 = d − d 2 = ∗ > s. Termination: At termination, d < 1. Since d = 2p for some integer p ≥ −1, we must have p = −1, so that d = 1 2. By the second part of the loop invariant, if we multiply both sides by 2, we get that high− low = 2d = 1. ∗ ≤ high. Since By the third part of the loop invariant, we know that low < m high − low = 2d = 1 and m is high, which the procedure returns.  . Since L  s  has no negative diagonal entries, we know that m ∗ ≤ high .  ∗ > low, the only possible value for m ∗   cid:4  = high, we have that low   cid:4  = s and high   cid:4  < m      cid:4    cid:4   Time: If there is no negative-weight cycle, the  cid:222 rst while loop iterates  cid:2  lg n  times, and the total time is  cid:2  n3 lg n . Now suppose that there is a negative-weight cycle. We claim that each time we call EXTEND-SHORTEST-PATHS  L  low , L  d  , we have already computed L low  and L  d . Initially, since low = m 2, we had already computed L low  in the  cid:222 rst while loop. In succeeding iterations of the second while loop, the only way that low changes is when it gets the value of s, and we have just computed L s . As for L  d , observe that d takes on the values m 4, m 8, m 16, . . . , 1, and again, we computed all of these L matrices in the  cid:222 rst while loop. Thus, the claim is proven. Each of ∗  times. Since we have already computed the the two while loops iterates  cid:2  lg m parameters to each call of EXTEND-SHORTEST-PATHS, each iteration is dominated by the  cid:2  n3 -time call to EXTEND-SHORTEST-PATHS. Thus, the total time is  cid:2  n3 lg m In general, therefore, the running time is  cid:2  n3 lg min n, m  ∗  .  ∗ .  Space: The slower algorithm needs to keep only three matrices at any time, and so its space requirement is  cid:2  n3 . This faster algorithm needs to main- tain  cid:2  lg min n, m increases to ∗  .  cid:2  n3 lg min n, m  ∗   matrices, and so the space requirement  Solution to Exercise 25.2-4  With the superscripts, the computation is d k  . If, i j having dropped the superscripts, we were to compute and store dik or dkj before  kj  ik  i j  ← min  d  k−1   , d  k−1   + d  k−1    cid:10    cid:11    25-12  Solutions for Chapter 25: All-Pairs Shortest Paths   cid:10   cid:10   cid:10    cid:11   cid:11   i j  i j  i j  kj  kj  d  k−1  d  k−1  d  k−1   , d  k  ik , d  k−1  ik , d  k  ik  + d  k−1   cid:11  + d  k  + d  k   ← min ← min ← min  using these values to compute di j , we might be computing one of the following: d  k  i j d  k  i j d  k  i j In any of these scenarios, we cid:146 re computing the weight of a shortest path from i to j with all intermediate vertices in {1, 2, . . . , k}. If we use d k  ik , rather than d k−1  , in the computation, then we cid:146 re using a subpath from i to k with all intermediate vertices in {1, 2, . . . , k}. But k cannot be an intermediate vertex on a shortest path from i to k, since otherwise there would be a cycle on this shortest path. Thus, d  k  . Hence, we ik can drop the superscripts in the computation.  . A similar argument applies to show that d k  kj  = d  k−1   = d  k−1   kj  kj  ik  ik  Solution to Exercise 25.2-6  Here are two ways to detect negative-weight cycles:  1. Check the main-diagonal entries of the result matrix for a negative value. There           < 0 for some vertex i:  is a path weight from i to itself; so if it is negative, there is a path from i  is a negative weight cycle if and only if d n  ii   d  n  ii to itself  i.e., a cycle , with negative weight. If there is a negative-weight cycle, consider the one with the fewest vertices. If it has just one vertex, then some wii < 0, so dii starts out negative, and since d values are never increased, it is also negative when the algorithm terminates. If it has at least two vertices, let k be the highest-numbered vertex in the cycle, and let i be some other vertex in the cycle. d k−1  have correct shortest-path weights, because they are not based on negative- weight cycles.  Neither d k−1  can include k as an intermediate vertex, and i and k are on the negative-weight cycle with the fewest vertices.  Since i  cid:2  k  cid:2  i is a negative-weight cycle, the sum of those two weights is negative, so d k  ii will be set to a negative value. Since d values are never increased, it is also negative when the algorithm terminates.  and d k−1   nor d k−1   In fact, it suf cid:222 ces to check whether d n−1  < 0 for some vertex i. Here cid:146 s why. A negative-weight cycle containing vertex i either contains vertex n or it does not. If it does not, then clearly d n−1  < 0. If the negative-weight cycle contains vertex n, then consider d n−1  ii . This value must be negative, since the cycle, starting and ending at vertex n, does not include vertex n as an intermediate vertex.  nn  ik  ik  ki  ki  ii  2. Alternatively, one could just run the normal FLOYD-WARSHALL algorithm one extra iteration to see if any of the d values change. If there are negative cycles, then some shortest-path cost will be cheaper. If there are no such cycles, then no d values will change because the algorithm gives the correct shortest paths.   Solutions for Chapter 25: All-Pairs Shortest Paths  25-13  Solution to Exercise 25.3-4  It changes shortest paths. Consider the following graph. V = {s, x, y, z}, and So we cid:146 d add 10 to every weight to make&w. With w, the shortest path from s to y there are 4 edges: w s, x  = 2, w x, y  = 2, w s, y  = 5, and w s, z  = −10. is s → x → y, with weight 4. With &w, the shortest path from s to y is s → y, with weight 15.  The path s → x → y has weight 24.  The problem is that by just adding the same amount to every edge, you penalize paths with more edges, even if their weights are low.  Solution to Exercise 25.3-6  In this solution, we assume that ∞ − ∞ is unde cid:222 ned; in particular, it cid:146 s not 0. Let G =  V, E , where V = {s, u}, E = { u, s }, and w u, s  = 0. There &w u, s  = 0 + ∞ − 0 = ∞. We compute&δ u, s  = ∞, and so we compute is only one edge, and it enters s. When we run Bellman-Ford from s, we get h s  = δ s, s  = 0 and h u  = δ s, u  = ∞. When we reweight, we get dus = ∞ + 0 − ∞  cid:19 = 0. Since δ u, s  = 0, we get an incorrect answer. If the graph G is strongly connected, then we get h v  = δ s, v  < ∞ for all edges  u, v  ∈ E, and so&w u, v  = w u, v +h u −h v  ≥ 0. Moreover, all edge vertices v ∈ V . Thus, the triangle inequality says that h v  ≤ h u +w u, v  for all weights&w u, v  used in Lemma 25.1 are  cid:222 nite, and so the lemma holds. Therefore, does not change shortest paths, and that all edge weights&w u, v  are nonnegative. Again relying on G being strongly connected, we get that&δ u, v  < ∞ for all edges  u, v  ∈ E, which means that duv =&δ u, v  + h v  − h u  is  cid:222 nite and  the conditions we need in order to use Johnson cid:146 s algorithm hold: that reweighting  correct.  Solution to Problem 25-1  a. Let T be the V × V matrix representing the transitive closure, such that   cid:9   T [i, j] is 1 if there is a path from i to j, and 0 if not. Initialize T  when there are no edges in G  as follows: T [i, j] =  1 if i = j , 0 otherwise .  T can be updated as follows when an edge  u, v  is added to G:  TRANSITIVE-CLOSURE-UPDATE  u, v  for i ← 1 to V  do for j ← 1 to V  do if T [i, u] = 1 and T [v, j] = 1  then T [i, j] ← 1   25-14  Solutions for Chapter 25: All-Pairs Shortest Paths    This says that the effect of adding edge  u, v  is to create a path  via the new edge   from every vertex that could already reach u to every vertex that could already be reached from v.   Note that the procedure sets T [u, v] ← 1, because of the initial values T [u, u] = T [v, v] = 1.    This takes  cid:2  V 2  time because of the two nested loops. b. Consider inserting the edge  vn, v1  into the straight-line graph v1 → v2 → ··· → vn, where n = V. Before this edge is inserted, only n n + 1  2 entries in T are 1  the entries on and above the main diagonal . After the edge is inserted, the graph is a cycle in which every vertex can reach every other vertex, so all n2 entries in T are 1. Hence n2 −  n n + 1  2  =  cid:2  n2  =  cid:2  V 2  entries must be changed in T , so any algorithm to update the transitive closure must take  cid:3  V2  time on this graph.  c. The algorithm in part  a  would take  cid:2  V4  time to insert all possible  cid:2  V2  edges, so we need a more ef cid:222 cient algorithm in order for any sequence of in- sertions to take only O V 3  total time. To improve the algorithm, notice that the loop over is pointless when T [i, v] = 1. That is, if there is already a path i  cid:2  v, then adding the edge  u, v  can cid:146 t make any new vertices reachable from i. The loop to set T [i, j] to 1 for j such that there cid:146 s a path v  cid:2  j is just setting entries that are already 1. Eliminate this redundant processing as follows:  j  TRANSITIVE-CLOSURE-UPDATE u, v  for i ← 1 to V do if T [i, u] = 1 and T [i, v] = 0 do if T [v, j] = 1  then for j ← 1 to V  then T [i, j] ← 1  We show that this procedure takes O V3  time to update the transitive closure for any sequence of n insertions:   There can cid:146 t be more than V2 edges in G, so n ≤ V2.   Summed over n insertions, time for the  cid:222 rst two lines is O nV   = O V3 .   The last three lines, which take  cid:2  V   time, are executed only O V2  times for n insertions. To see this, notice that the last three lines are executed only when T [i, v] = 0, and in that case, the last line sets T [i, v] ← 1. Thus, the number of 0 entries in T is reduced by at least 1 each time the last three lines run. Since there are only V2 entries in T , these lines can run at mostV2 times.    Hence the total running time over n insertions is O V3 .   Lecture Notes for Chapter 26: Maximum Flow  Chapter 26 overview  Network  cid:223 ow  Use a graph to model material that  cid:223 ows through conduits. Each edge represents one conduit, and has a capacity, which is an upper bound on the  cid:223 ow rate = units time. Can think of edges as pipes of different sizes. But  cid:223 ows don cid:146 t have to be of liquids. Book has an example where a  cid:223 ow is how many trucks per day can ship hockey pucks between cities. Want to compute max rate that we can ship material from a designated source to a designated sink.  Flow networks  G =  V, E  directed. Each edge  u, v  has a capacity c u, v  ≥ 0. If  u, v   cid:19 ∈ E, then c u, v  = 0. Source vertex s, sink vertex t, assume s  cid:2  v  cid:2  t for all v ∈ V . Example: [Edgesarelabeledwithcapacities.]  s  3  1  2  w  3  y  2  1  3  x  z  3  2  t  3  2  [Inthesenotes,wede cid:222 nepositive cid:223 ow cid:222 rstbecauseit cid:146 smoreintuitivetostudents than the  cid:223 ow formulation in the book. We cid:146 ll migrate towards  cid:223 ow in the book soon. We cid:146 ll call it  cid:147 net  cid:223 ow cid:148  at  cid:222 rst in the lecture notes. Net  cid:223 ow tends to be mathematicallyneatertoworkwiththanpositive cid:223 ow.]   26-2  Lecture Notes for Chapter 26: Maximum Flow  Positive  cid:223 ow: A function p : V × V → R satisfying  cid:2   cid:2    Capacity constraint: For all u, v ∈ V, 0 ≤ p u, v  ≤ c u, v ,  cid:24  cid:25   cid:23   cid:23    Flow conservation: For all u ∈ V −{s, t}, v∈V v∈V  cid:223 ow out of u  cid:223 ow into u p v, u  = 0.  p u, v  −  Equivalently,   cid:2    cid:2    cid:24  cid:25   p u, v   p v, u    cid:26    cid:26   =  .  v∈V  [Add cid:223 owstopreviousexample. Edgesherearelabeledas cid:223 ow capacity.]  v∈V  2 3  1   2  s  1   3  0 1  2 2  1 3  w  y  2 2  1 1  2 3  x  z  1 2  2 3  t    Note that all positive  cid:223 ows are ≤ capacities.   Verify  cid:223 ow conservation by adding up  cid:223 ows at a couple of vertices.   Note that all positive  cid:223 ows = 0 is legitimate.  Cancellation with positive  cid:223 ows       Without loss of generality, can say positive  cid:223 ow goes either from u to v or from v to u, but not both.  Because if not true, can transform by cancellation to be true.  In the above example, we can  cid:147 cancel cid:148  1 unit of  cid:223 ow in each direction between x and z. 1 unit x → z 2 units z → x In both cases,  cid:147 net  cid:223 ow cid:148  is 1 unit z → x.      Capacity constraint is still satis cid:222 ed  because  cid:223 ows only decrease .   Flow conservation is still satis cid:222 ed   cid:223 ow in and  cid:223 ow out are both reduced by the  ⇒ 0 units x → z 1 unit z → x  same amount .  Here cid:146 s a concept similar to positive  cid:223 ow: Net  cid:223 ow: A function f : V × V → R satisfying   Capacity constraint: For all u, v ∈ V, f  u, v  ≤ c u, v ,  cid:2    Skew symmetry: For all u, v ∈ V, f  u, v  = − f  v, u ,   Flow conservation: For all u ∈ V −{s, t} ,  cid:2   cid:24  cid:25   cid:23   Another way to think of  cid:223 ow conservation:   cid:2   cid:24  cid:25   v∈V : f  v,u >0  v∈V : f  u,v >0  f  u, v   f  v, u   v∈V   cid:26    cid:23    cid:26   =  .  f  u, v  = 0.  total positive  cid:223 ow entering u  total positive  cid:223 ow leaving u   Lecture Notes for Chapter 26: Maximum Flow  26-3   cid:147  cid:223 ow in =  cid:223 ow out cid:148  The differences between positive  cid:223 ow p and net  cid:223 ow f :      p u, v  ≥ 0, f satis cid:222 es skew symmetry.  Equivalence of positive  cid:223 ow and net  cid:223 ow de cid:222 nitions  De cid:222 ne net  cid:223 ow in terms of positive  cid:223 ow:   De cid:222 ne f  u, v  = p u, v  − p v, u .   Argue, given de cid:222 nition of p, that this de cid:222 nition of f satis cid:222 es capacity con-  straint and  cid:223 ow conservation. Capacity constraint: p u, v  ≤ c u, v  and p v, u  ≥ 0 ⇒ p u, v  − p v, u  ≤ c u, v  .  cid:2  Flow conservation:   cid:2   cid:2  f  u, v  = = v∈V = 0 .  v∈V  v∈V   cid:2     p u, v  − p v, u   p u, v  −  p v, u   v∈V    Skew symmetry is trivially satis cid:222 ed by this de cid:222 nition of f  u, v :  f  u, v  = p u, v  − p v, u   = −  p v, u  − p u, v   = − f  v, u  .  cid:9   De cid:222 ne positive  cid:223 ow in terms of net  cid:223 ow:   De cid:222 ne  p u, v  =  f  u, v  0  if f  u, v  > 0 , if f  u, v  ≤ 0 .  straint and  cid:223 ow conservation. Capacity constraint:    If f  u, v  > 0: f  u, v  ≤ c u, v  ⇒ 0 ≤ p u, v  ≤ c u, v . If f  u, v  ≤ 0: 0 = p u, v  ≤ c u, v .       Argue, given de cid:222 nition of f , that this de cid:222 nition of p satis cid:222 es capacity con-   26-4  Lecture Notes for Chapter 26: Maximum Flow   cid:2   v∈V  Flow conservation:  p u, v  −   cid:2   cid:3   cid:2   p v, u   v∈V   cid:2    cid:4    cid:4    cid:2   cid:2   cid:2   p v, u   v∈V: f  u,v ≤0 p u, v   v∈V: f  v,u ≥0  p v, u   p v, u   v∈V : f  u,v >0  v∈V: f  u,v ≤0  p u, v   =  =  v∈V : f  u,v >0  v∈V: f  u,v ≤0  p u, v  +   cid:3   cid:2   cid:2  p v, u  + v∈V : f  u,v >0  cid:2  p u, v  + p v, u  −  cid:2  p u, v  + 0 − 0 −  cid:2  f  u, v  − f  u, v  −  v∈V : f  v,u <0  −  −   cid:2   cid:2   cid:2   cid:2   v∈V : f  u,v >0  v∈V : f  u,v >0  v∈V : f  u,v >0 f  u, v   = =  cid:2  = = v∈V = 0 .  v∈V : f  v,u ≥0 f  v, u   − f  u, v    v∈V : f  v,u ≥0  v∈V : f  u,v ≤0  [We cid:146 llusenet cid:223 ow,insteadofpositive cid:223 ow,fortherestofourdiscussion,inorder to cut the number of summations down by half. From now on, we cid:146 ll just call it  cid:147  cid:223 ow cid:148 ratherthan cid:147 net cid:223 ow. cid:148 ] Value of  cid:223 ow f =  f  =  f  s, v  = total  cid:223 ow out of source.   cid:2   Consider the example below. [The cancellation possible in the previous example hasbeenmadehere. Also,showingonly cid:223 owsthatarepositive.]  v∈V  x  z  2 3  1   2  s  1 3  1 3  2  t  1   3  1  2 2  w  y  2 2  1 1  2 3  Value of  cid:223 ow f =  f  = 3.  Cancellation with  cid:223 ow  If we have edges  u, v  and  v, u , skew symmetry makes it so that at most one of these edges has positive  cid:223 ow. Say f  u, v  = 5. If we  cid:147 ship cid:148  2 units v → u, we lower f  u, v  to 3. The 2 units v → u cancel 2 of the u → v units. Due to cancellation, a  cid:223 ow only gives us this  cid:147 net cid:148  effect. We cannot reconstruct actual shipments from a  cid:223 ow.   Lecture Notes for Chapter 26: Maximum Flow  26-5  same as  8 units u → v 3 units v → u  5 units u → v 0 units v → u We could add another 3 units of  cid:223 ow u → v and another 3 units v → u, maintain- ing  cid:223 ow conservation. The  cid:223 ow from u to v would remain f  u, v  = 5, and f  v, u  = −5.  Maximum- cid:223 ow problem: Given G, s, t, and c,  cid:222 nd a  cid:223 ow whose value is maxi- mum.  Implicit summation notation   cid:2    cid:2   We work with functions, like f , that take pairs of vertices as arguments. Extend to take sets of vertices, with interpretation of summing over all vertices in the set.  y∈Y  x∈X  f  x, y  .  Example: If X and Y are sets of vertices, f  X, Y   = Therefore, can express  cid:223 ow conservation as f  u, V   = 0, for all u ∈ V −{s, t}. Notation: Omit braces in sets with implicit summations. f  s, V − s  = f  s, V  . Here, f  s, V − s  really means f  s, V −{s} . Example:  Lemma For any  cid:223 ow f in G =  V, E : 1. For all X ⊆ V , f  X, X   = 0, 2. For all X, Y ⊆ V , f  X, Y   = − f  Y, X  , 3. For all X, Y, Z ⊆ V such that X ∩ Y = ∅, f  X ∪ Y, Z   = f  X, Z   + f  Y, Z   and f  Z , X ∪ Y   = f  Z , X   + f  Z , Y  .  Proof  2.  [Leaveonboard.]  x∈X   cid:2   cid:2   cid:2   cid:2  f  X, Y   =  cid:2   cid:2  = = = − f  Y, X    x∈X  x∈X  y∈Y  y∈Y  y∈Y  f  x, y  − f  y, x  − f  y, x   1.  f  X, X   = − f  X, X    ⇒ f  X, X   = 0   part  2     skew symmetry    26-6  Lecture Notes for Chapter 26: Maximum Flow  f  X ∪ Y, Z   =  3.   cid:2   cid:2   cid:3  cid:2   cid:2  + = = f  X, Z   + f  Y, Z    v∈X∪Y  f  v, z   f  v, z    cid:4   v∈X  z∈Z  z∈Z   cid:3  cid:2   z∈Z   cid:2   v∈Y   cid:4   f  v, z    X ∩ Y = ∅   Other part is symmetric.   lemma   Example of using this lemma: Lemma  f  = f  V, t . Proof First, show that f  V, V − s − t  = 0: f  u, V   = 0 for all u ∈ V − {s, t} ⇒ f  V − s − t, V   = 0  add up f  u, V   for all u ∈ V −{s, t}  ⇒ f  V, V − s − t  = 0  by lemma, part  2  . Thus,  f  = f  s, V    = f  V, V   − f  V − s, V   = − f  V − s, V   = f  V, V − s  = f  V, t  + f  V, V − s − t  = f  V, t    de cid:222 nition of  f    lemma, part  3    lemma, part  1    lemma, part  2    lemma, part  3    from above    lemma   Cuts A cut  S, T   of  cid:223 ow network G =  V, E  is a partition of V into S and T = V − S such that s ∈ S and t ∈ T .   Similar to cut used in minimum spanning trees, except that here the graph is  directed, and we require s ∈ S and t ∈ T .  For  cid:223 ow f , the net  cid:223 ow across cut  S, T   is f  S, T  . Capacity of cut  S, T   is c S, T  . A minimum cut of G is a cut whose capacity is minimum over all cuts of G. For our example: [Leaveonboard.]  G  s  1   3  1  2 2  w  y  2 2  1 1  2 3  x  z  2 3  1   2  1 3  1 3 2  t   Lecture Notes for Chapter 26: Maximum Flow  26-7  Consider the cut S = {s, w, y} , T = {x, z, t}. f  S, T   = f  w, x  + f  y, x  + f  y, z   = 2 + −1 + 2 = 3 .  c S, T   = c w, x  + c y, z   = 2 + 3 = 5 .  Note the difference between capacity and  cid:223 ow.   Flow obeys skew symmetry, so f  y, x  = − f  x, y  = −1.   Capacity does not: c y, x  = 0, but c x, y  = 1. So include  cid:223 ows going both ways across the cut, but capacity going only S to T . Now consider the cut S = {s, w, x, y} , T = {z, t}. f  S, T   = f  x, z  + f  x, t  + f  y, z   c S, T   = c x, z  + c x, t  + c y, z   = −1 + 2 + 2 = 3 .  = 2 + 3 + 3 = 8 .  Same  cid:223 ow as previous cut, higher capacity.  Lemma For any cut  S, T  , f  S, T   =  f . [Leaveonboard.] Proof First, show that f  S − s, V   = 0: S − {s} ⊆ V − {s, t} .  cid:2  Therefore,  cid:2  f  S − s, V   = = = 0 .  u∈S−{s}  u∈S−{s}  f  u, V    0  So, f  S, T   = f  S, V   − f  S, S     cid:223 ow conservation and S −{s} ⊆ V − {s, t}   = f  S, V   = f  s, V   + f  S − s, V   = f  s, V   =  f    lemma, part  3 , S ∪ T = V, S ∩ T = ∅   lemma, part  1    lemma, part  3     f  S − s, V   = 0    lemma    26-8  Lecture Notes for Chapter 26: Maximum Flow  Corollary The value of any  cid:223 ow ≤ capacity of any cut. [Leaveonboard.]   lemma    cid:2   cid:2  Proof Let  S, T   be any cut, f be any  cid:223 ow.  f  = f  S, T    cid:2   cid:2  = v∈T ≤ v∈T = c S, T   .  f  u, v   c u, v   u∈S  u∈S   capacity constraints   Therefore, maximum  cid:223 ow ≤ capacity of minimum cut. Will see a little later that this is in fact an equality.  The Ford-Fulkerson method   corollary   Residual network Given a  cid:223 ow f in network G =  V, E . Consider a pair of vertices u, v ∈ V . How much additional  cid:223 ow can we push directly from u to v? That cid:146 s the residual capacity, c f  u, v  = c u, v  − f  u, v    since f  u, v  ≤ c u, v   .  ≥ 0  Residual network: G f =  V, E f  , E f = { u, v  ∈ V × V : c f  u, v  > 0} . Each edge of the residual network can admit a positive  cid:223 ow. For our example:  Gf  s  2  1  3  w  y  2  1  1 2  x  z  1  2  1  1  1  2  2  3  t  Every edge  u, v  ∈ E f corresponds to an edge  u, v  ∈ E or  v, u  ∈ E  or both . Therefore, E f  ≤ 2E. Given  cid:223 ows f1 and f2, the  cid:223 ow sum f1 + f2 is de cid:222 ned by   f1 + f2  u, v  = f1 u, v  + f2 u, v  for all u, v ∈ V .   Lecture Notes for Chapter 26: Maximum Flow  Lemma Given a  cid:223 ow network G, a  cid:223 ow f be any  cid:223 ow in G f . Then the  cid:223 ow sum f + f  f + f   cid:4  =  f  +  f   cid:4 .  in G, and the residual network G f , let  f is a  cid:223 ow in G with value   cid:4   26-9   cid:4   [Seebookforproof.]  Augmenting path  A path s  cid:2  t in G f .   Admits more  cid:223 ow along each edge.   Like a sequence of pipes through which we can squirt more  cid:223 ow from s to t. How much more  cid:223 ow can we push from s to t along augmenting path p? c f   p  = min{c f  u, v  :  u, v  is on p} . For our example, consider the augmenting path p =  cid:2 s, w, y, z, x, t cid:3 . Minimum residual capacity is 1. After we push 1 additional unit along p: [Continue from G left on board from before.]  G  s  Gf  s  2   3  1 2 2  1  2  3  w  3  y  w  3  y  2 2  1 1  3 3  2  1  3  2  2 3  1  4  x  z  x  z  3 3  1   2  3  1  1  t  t  Observe that G f now has no augmenting path. Why? No edges cross the cut  {s, w} ,{x, y, z, t}  in the forward direction in G f . So no path can get from s to t. Claim that the  cid:223 ow shown in G is a maximum  cid:223 ow.  ⎧⎨⎩c f   p   Lemma Given  cid:223 ow network G,  cid:223 ow f in G, residual network G f . Let p be an augmenting path in G f . De cid:222 ne f p : V × V → R: f p u, v  = Then f p is a  cid:223 ow in G f with value  f p = c f   p  > 0.  if  u, v  is on p , if  v, u  is on p , otherwise .  −c f   p  0   26-10  Lecture Notes for Chapter 26: Maximum Flow  Corollary Given  cid:223 ow network G,  cid:223 ow f in G, and an augmenting path p in G f , de cid:222 ne f p as in lemma, and de cid:222 ne f is a  cid:223 ow in G with value  f   cid:4  =  f  + c f   p  >  f .   cid:4  = f + f p. Then f  : V × V → R by f   cid:4    cid:4   Theorem  Max- cid:223 ow min-cut theorem  The following are equivalent:  1. 2. 3.  f is a maximum  cid:223 ow. f admits no augmenting path.  f  = c S, T   for some cut  S, T  .  Proof  1  ⇒  2 : If f admits an augmenting path p, then  by above corollary  would get a  cid:223 ow with value  f  + c f   p  >  f , so f wasn cid:146 t a max  cid:223 ow to start with.  2  ⇒  3 : Suppose f admits no augmenting path. De cid:222 ne S = {v ∈ V : there exists a path s  cid:2  v in G f } , T = V − S . Must have t ∈ T ; otherwise there is an augmenting path. Therefore,  S, T   is a cut. For each u ∈ S and v ∈ T , must have f  u, v  = c u, v , since otherwise  u, v  ∈ E f and then v ∈ S. By lemma   f  S, T   =  f  ,  f  = f  S, T   = c S, T  .  3  ⇒  1 : By corollary,  f  ≤ c S, T  .  f  = c S, T   ⇒ f is a max  cid:223 ow.   theorem   Ford-Fulkerson algorithm  FORD-FULKERSON V, E, s, t  for all  u, v  ∈ E do f [u, v] ← f [v, u] ← 0  while there is an augmenting path p in G f  do augment f by c f   p   Subtle difference between f [u, v] and f  u, v : f  u, v  is a function, de cid:222 ned on all u, v ∈ V .     f [u, v] is a value computed by algorithm.      f [u, v] = f  u, v  where  u, v  ∈ E or  v, u  ∈ E. f [u, v] is unde cid:222 ned if neither  u, v  ∈ E nor  v, u  ∈ E.   Lecture Notes for Chapter 26: Maximum Flow  26-11  ∗  , then need ≤  f  ∗ iterations ⇒ time is O E  f  Analysis: If capacities are all integer, then each augmenting path raises f  by ≥ 1. If max  cid:223 ow is f [Handwaving cid:151 seebookforbetterexplanation.] Note that this running time is not polynomial in input size. It depends on f which is not a function ofV and E. If capacities are rational, can scale them to integers. If irrational, FORD-FULKERSON might never terminate!  ∗ .  ∗,  Edmonds-Karp algorithm  Do FORD-FULKERSON, but compute augmenting paths by BFS of G f . Augment- ing paths are shortest paths s  cid:2  t in G f , with all edge weights = 1. Edmonds-Karp runs in O V E2  time. To prove, need to look at distances to vertices in Gf . Let δ f  u, v  = shortest path distance u to v in G f , with unit edge weights.  Lemma For all v ∈ V − {s, t}, δ f  s, v  increases monotonically with each  cid:223 ow augmenta- tion. Proof Suppose there exists v ∈ V −{s, t} such that there is a  cid:223 ow augmentation that causes δ f  s, v  to decrease. Will derive a contradiction. Let f be the  cid:223 ow before the  cid:222 rst augmentation that causes a shortest-path distance be the  cid:223 ow afterward. to decrease, f Let v be a vertex with minimum δ f  cid:4   s, v  < δ f  s, v . augmentation, so δ f Let a shortest path s to v in G f  cid:4   s, u  + 1.  Or δ f  cid:4  s, u  = δ f δ f  cid:4   s, v  and how we chose v, we have δ f  cid:4   s, u  < δ f Since δ f   cid:4   s, v  whose distance was decreased by the  cid:4   s, v  =   cid:4  be s  cid:2  u → v, so  u, v  ∈ E f  cid:4   s, v  − 1.    cid:4   s, u  ≥ δ f  s, u .   cid:4  and δ f   cid:4   Claim  u, v   cid:19 ∈ E f . Proof If  u, v  ∈ E f , then δ f  s, v  ≤ δ f  s, u  + 1  cid:4   s, u  + 1 ≤ δ f = δ f  cid:4   s, v  ,   triangle inequality   contradicting δ f  cid:4   s, v  < δ f  s, v . How can  u, v   cid:19 ∈ E f and  u, v  ∈ E f The augmentation must increase  cid:223 ow v to u. Since Edmonds-Karp augments along shortest paths, the shortest path s to u in Gf has v → u as its last edge.   claim    cid:4 ?   26-12  Lecture Notes for Chapter 26: Maximum Flow  Therefore, δ f  s, v  = δ f  s, u  − 1  cid:4  s, u  − 1  cid:4  s, v  − 2 ,  ≤ δ f = δ f  contradicting δ f  cid:4   s, v  < δ f  s, v . Therefore, v cannot exist.   lemma   Theorem Edmonds-Karp performs O V E  augmentations. Proof Suppose p is an augmenting path and cf  u, v  = c f   p . Then call  u, v  a critical edge in G f , and it disappears from the residual network after an augmen- tation along p. ≥ 1 edge on any augmenting path is critical. Will show that each of the E edges can become critical ≤ V  2 − 1 times. Consider u, v ∈ V such that either  u, v  ∈ E or  v, u  ∈ E or both. Since aug- menting paths are shortest paths, when  u, v  becomes critical  cid:222 rst time, δf  s, v  = δ f  s, u  + 1. Augment  cid:223 ow, so that  u, v  disppears from the residual network. This edge cannot reappear in the residual network until  cid:223 ow from u to v decreases, which happens only if  v, u  is on an augmenting path in G f  cid:4  : δ f  cid:4   s, u  = δ f  cid:4   s, v  + 1.   f is  cid:223 ow when this occurs.  By lemma, δ f  s, v  ≤ δ f  cid:4   s, v  ⇒ δ f  cid:4   s, u  = δ f  cid:4  s, v  + 1 ≥ δ f  s, v  + 1 = δ f  s, u  + 2 .   cid:4   Therefore, from the time  u, v  becomes critical to the next time, distance of u from s increases by ≥ 2. Initially, distance to u is ≥ 0, and augmenting path can cid:146 t have s, u, and t as intermediate vertices. Therefore, until u becomes unreachable from source, its distance is ≤V − 2 ⇒ u can become critical ≤  V − 2  2 = V  2 − 1 times. Since O E  pairs of vertices can have an edge between them in residual graph, total  of critical edges is O V E . Since each augmenting path has ≥ 1 critical edge, have O V E  augmentations.  theorem  Use BFS to  cid:222 nd each augmenting path in O E  time ⇒ O V E2  time. Can get better bounds. Push-relabel algorithms in Sections 26.4 cid:150 26.5 give O V3 . Can do even better.   Lecture Notes for Chapter 26: Maximum Flow  26-13  Maximum bipartite matching  Example of a problem that can be solved by turning it into a  cid:223 ow problem. G =  V, E   undirected  is bipartite if we can partition V = L ∪ R such that all edges in E go between L and R.  L  R  L  R  matching  maximum matching  A matching is a subset of edges M ⊆ E such that for all v ∈ V , ≤ 1 edge of M is incident on v.  Vertex v is matched if an edge of M is incident on it; otherwise unmatched .  Maximum matching: a matching of maximum cardinality. matching if M ≥ M   cid:4  for all matchings M  .    cid:4    M is a maximum  Problem: Given a bipartite graph  with the partition ,  cid:222 nd a maximum matching.  Application: Matching planes to routes.    L = set of planes.   R = set of routes.  u, v  ∈ E if plane u can  cid:223 y route v.      Want maximum  of routes to be served by planes. Given G, de cid:222 ne  cid:223 ow network G   V   E   cid:4  = V ∪ {s, t}.  cid:4  = { s, u  : u ∈ L}   cid:4  =  V  ∪ { u, v  : u ∈ L , v ∈ R,  u, v  ∈ E} ∪ { v, t  : v ∈ R} . c u, v  = 1 for all  u, v  ∈ E   cid:4 , E   cid:4  .  .      cid:4    26-14  Lecture Notes for Chapter 26: Maximum Flow  s  t   cid:4  = E + V ≤ 3E.  Each vertex in V has ≥ 1 incident edge ⇒E ≥ V  2. Therefore, E ≤ E Therefore, E Find a max  cid:223 ow in G Use edges that carry  cid:223 ow of 1 in matching. Book proves this gives maximum matching.   cid:4  =  cid:2  E .   cid:4   . Book shows that it will have integer values for all  u, v .   Solutions for Chapter 26: Maximum Flow  Solution to Exercise 26.1-4  We want to prove the following lemma.  Lemma For any  cid:223 ow f in G =  V, E : 1. For all X ⊆ V, f  X, X   = 0, 2. For all X, Y ⊆ V, f  X, Y   = − f  Y, X  , 3. For all X, Y, Z ⊆ V such that X ∩ Y = ∅, f  X ∪ Y, Z   = f  X, Z   + f  Y, Z   and f  Z , X ∪ Y   = f  Z , X   + f  Z , Y  .  Proof 2.  x∈X   cid:2   cid:2   cid:2   cid:2  f  X, Y   =  cid:2   cid:2  = = = − f  Y, X    x∈X  x∈X  y∈Y  y∈Y  y∈Y  f  x, y  − f  y, x  − f  y, x   1.  3.  f  X ∪ Y, Z   =  f  X, X   = − f  X, X   ⇒ f  X, X   = 0  cid:2   cid:2   cid:3  cid:2   cid:2  + = = f  X, Z   + f  Y, Z    v∈X∪Y  f  v, z   f  v, z    cid:4   v∈X  z∈Z  z∈Z   skew symmetry    cid:4    cid:3  cid:2   z∈Z   cid:2   v∈Y  f  v, z    X ∩ Y = ∅   The other part is symmetric.   lemma    26-16  Solutions for Chapter 26: Maximum Flow  Solution to Exercise 26.1-6   skew symmetry   The  cid:223 ow sum f1 + f2 satis cid:222 es skew symmetry and  cid:223 ow conservation, but might violate the capacity constraint. We give proofs for skew symmetry and  cid:223 ow conservation and an example that shows a violation of the capacity constraint. Let f  u, v  =   f1 + f2  u, v . For skew symmetry: f  u, v  = f1 u, v  + f2 u, v  = − f1 v, u  − f2 v, u  = −  f1 v, u  + f2 v, u   = − f  v, u  .  cid:2   cid:2  f  u, v  = = v∈V = 0 + 0 = 0 .   cid:2  For  cid:223 ow conservation, let u ∈ V −{s, t}:  cid:2    f1 u, v  + f2 u, v   v∈V f1 u, v  +  For the capacity constraint, let V = {s, t}, E = { s, t }, and c s, t  = 1. Let f1 s, t  = f2 s, t  = 1. Then f1 and f2 obey the capacity constraint, but   f1 + f2  u, v  = 2, which violates the capacity constraint.    cid:223 ow conservation   f2 u, v   v∈V  v∈V  To see that the  cid:223 ows form a convex set, we show that if f1 and f2 are  cid:223 ows, then so is α f1 +  1 − α  f2 for all α such that 0 ≤ α ≤ 1. For the capacity constraint,  cid:222 rst observe that α ≤ 1 implies that 1 − α ≥ 0. Thus, for any u, v ∈ V , we have α f1 u, v  +  1 − α  f2 u, v  ≥ 0 · f1 u, v  + 0 ·  1 − α  f2 u, v  Since f1 u, v  ≤ c u, v  and f2 u, v  ≤ c u, v , we also have α f1 u, v  +  1 − α  f2 u, v  ≤ αc u, v  +  1 − α c u, v   = 0 .  =  α +  1 − α  c u, v  = c u, v  .  For skew symmetry, we have f1 u, v  = − f1 v, u  and f2 u, v  = − f2 v, u  for any u, v ∈ V . Thus, we have α f1 u, v  +  1 − α  f2 u, v  = −α f1 v, u  −  1 − α  f2 v, u   = − α f1 v, u  +  1 − α  f2 v, u   .  Solution to Exercise 26.1-7   Solutions for Chapter 26: Maximum Flow   cid:1   cid:2   α f1 u, v  +  1 − α  f2 u, v   = α   cid:1  v∈V f2 u, v  = 0 for any u ∈ V − {s, t}. Thus, f2 u, v   For  cid:223 ow conservation, observe that since f1 and f2 obey  cid:223 ow conservation, we have  v∈V f1 u, v  = 0 and  f1 u, v  +  1 − α    cid:2    cid:2   26-17  v∈V  v∈V = α · 0 +  1 − α  · 0 = 0 .  v∈V  Solution to Exercise 26.1-9  Create a vertex for each corner, and if there is a street between corners u and v, create directed edges  u, v  and  v, u . Set the capacity of each edge to 1. Let the source be corner on which the professor cid:146 s house sits, and let the sink be the corner on which the school is located. We wish to  cid:222 nd a  cid:223 ow of value 2 that also has the property that f  u, v  is an integer for all vertices u and v. Such a  cid:223 ow represents two edge-disjoint paths from the house to the school.  Solution to Exercise 26.2-4  c f  u, v  + c f  v, u  = c u, v  − f  u, v  + c v, u  − f  v, u    by de cid:222 nition  = c u, v  + c v, u   by skew symmetry: f  u, v  = − f  v, u    Solution to Exercise 26.2-9  For any two vertices u and v in G, you can de cid:222 ne a  cid:223 ow network Guv consisting of the directed version of G with all edge capacities set to 1, s = u, and t = v.  Guv has O V   vertices cid:151 actually, V cid:151 and O E  edges, as required. We want all capacities to be 1 so that the number of edges crossing a cut equals the capacity of the cut.  Let fuv denote a maximum  cid:223 ow in Guv.  fuv. We cid:146 ll We claim that for any u ∈ V , the edge connectivity k equals min v∈V−{u} show below that this claim holds. Assuming that it holds, we can  cid:222 nd k as follows:  EDGE-CONNECTIVITY G  select any vertex u ∈ V for each vertex v ∈ V −{u}  cid:1  V − 1 iterations  do set up the  cid:223 ow network Guv as described above   cid:222 nd the maximum  cid:223 ow fuv on Guv  return the minimum of the V − 1 max- cid:223 ow values: min v∈V−{u}   fuv   26-18  Solutions for Chapter 26: Maximum Flow   fuv:  The claim follows from the max- cid:223 ow min-cut theorem and how we chose capac- ities so that the capacity of a cut is the number of edges crossing it. We prove  fuv, for any u ∈ V by showing separately that k is at least this that k = min v∈V−{u} minimum and that k is at most this minimum.   Proof that k ≥ min v∈V−{u}  fuv. Suppose we remove only m − 1 edges from G. For Let m = min v∈V−{u} any vertex v, by the max- cid:223 ow min-cut theorem, u and v are still connected.  The max  cid:223 ow from u to v is at least m, hence any cut separating u from v has capacity at least m, which means at least m edges cross any such cut. Thus at least one edge is left crossing the cut when we remove m−1 edges.  Thus every node is connected to u, which implies that the graph is still connected. So at  fuv. least m edges must be removed to disconnect the graph cid:151 i.e., k ≥ min v∈V−{u}    Proof that k ≤ min v∈V−{u}   fuv:  Consider a vertex v with the minimum fuv. By the max- cid:223 ow min-cut theorem, there is a cut of capacity fuv separating u and v. Since all edge capacities are 1, exactly  fuv edges cross this cut. If these edges are removed, there is no path  fuv. from u to v, and so our graph becomes disconnected. Hence k ≤ min v∈V−{u}    Thus, the claim that k = min v∈V−{u}   fuv, for any u ∈ V is true.  From the time  u, v  is a critical edge until it is again a critical edge, δ s, u  increases by at least 2, as shown in Theorem 26.9. Similarly, you can show that δ v, t  also increases by at least 2. Thus the length of the augmenting path s  cid:2  u → v  cid:2  t increases by at least 4 between times  u, v  is critical. Since the length of an augmenting path cannot exceedV−1,  u, v  can be critical < V  4 times. Edmonds-Karp terminates when there are no more augmenting paths, so it must terminate when there are no more critical edges, which takes at most   edges · max  times each edge critical  < E f   V  4  iterations. In general, E f ≤ 2 E, so the number of iterations is at most  actually, fewer than  EV  2. But if we assume that G always has edges in both directions  i.e.,  u, v  ∈ E if and only if  v, u  ∈ E , then E f ≤ E, and the number of itera- tions is at most EV  4.  Solution to Exercise 26.2-10  Solution to Exercise 26.3-3   cid:4  By de cid:222 nition, an augmenting path is a simple path s  cid:2  t in the residual graph G f . Since G has no edges between vertices in L and no edges between vertices in R,  cid:4   cid:4  and hence neither does G f . Also, the only edges neither does the  cid:223 ow network G   Solutions for Chapter 26: Maximum Flow  26-19  can go   cid:4  f can also go from R to L.   cid:4  involving s or t connect s to L and R to t. Note that although edges in G only from L to R, edges in G Thus any augmenting path must go s → L → R → ··· → L → R → t , crossing back and forth between L and R at most as many times as it can do so without using a vertex twice. It contains s, t, and equal numbers of dis- tinct vertices from L and R cid:151 at most 2 + 2 · min L ,R  vertices in all. The length of an augmenting path  i.e., its number of edges  is thus bounded above by 2 · min L ,R  + 1.  Solution to Exercise 26.4-2  Each time we call RELABEL u , we examine all edges  u, v  ∈ E f . Since the number of relabel operations is at most 2V − 1 per vertex, edge  u, v  will be examined during relabel operations at most 4V − 2 = O V   times  at most 2V − 1 times during calls to RELABEL u  and at most 2V − 1 times during calls to RELABEL v  . Summing up over all the possible residual edges, of which there are at most 2E = O E , we see that the total time spent relabeling vertices is O V E .  Solution to Exercise 26.4-3  push-relabel algorithm, in O V   time. First,  cid:222 nd a height&h such that 0 <&h < V We can  cid:222 nd a minimum cut, given a maximum  cid:223 ow found in G =  V, E  by a and there is no vertex whose height equals&h at termination of the algorithm. there are V − 1 possible values for&h, we know that for at least one number in Since h[s] = V and h[t] = 0, we need consider only V − 2 vertices. Since 1, 2, . . . ,V − 1, there will be no vertex of that height. Hence,&h is well de cid:222 ned, Let S = {u ∈ V : h[u] >&h} and T = {v ∈ V : h[v]   and it is easy to  cid:222 nd in O V   time by using a simple boolean array indexed by heights 1, 2, . . . ,V − 1. &h, we have s ∈ S, and because h[t] = 0 <&h, we have t ∈ T , as required for a cut. We need to show that f  u, v  = c u, v , i.e., that  u, v   cid:19 ∈ E f for all u ∈ S and v ∈ T . Once we do that, we have that f  S, T   = c S, T  , and by Corollary 26.6,  S, T   is a minimum cut. Suppose for the purpose of contradiction that there exist vertices u ∈ S and v ∈ T  Lemma 26.17 , we have that h[u] ≤ h[v]+ 1. But we also have h[v] <&h < h[u], such that  u, v  ∈ E f . Because h is always maintained as a height function and because all values are integer, h[v] ≤ h[u] − 2. Thus, we have h[u] ≤ h[v] + 1 ≤ h[u] − 2 + 1 = h[u] − 1, which gives the contradiction that 0 ≤ −1. Thus,  S, T   is a minimum cut.   26-20  Solutions for Chapter 26: Maximum Flow  Solution to Exercise 26.4-6  Solution to Problem 26-2  If we set h[s] = V − 2, we have to change our de cid:222 nition of a height function to allow h[s] = V− 2, rather than h[s] = V. The only change we need to make to the proof of correctness is to update the proof of Lemma 26.18. The original proof derives the contradiction that h[s] ≤ k < V, which is at odds with h[s] = V. When h[s] = V − 2, there is no contradiction. As in the original proof, let us suppose that we have a simple augmenting path  cid:2 v0, v1, . . . , vk cid:3 , where v0 = s and vk = t, so that k < V. How could  s, v1  be a residual edge? It had been saturated in INITIALIZE-PREFLOW, which means that we had to have pushed some  cid:223 ow from v1 to s. In order for that to have happened, we must have had h[v1] = h[s] + 1. If we set h[s] = V − 2, that means that h[v1] was V− 1 at the time. Since then, h[v1] did not decrease, and so we have h[v1] ≥ V − 1. Working backwards over our augmenting path, we have h[vk−i ] ≤ h[t] + i for i = 0, 1, . . . , k. As before, because the augmenting path is simple, k < V. Letting i = k − 1, we have h[v1] ≤ h[t] + k − 1 < 0 + V − 1. We now have the contradiction that h[v1] ≥ V − 1 and h[v1] < V − 1, which shows that Lemma 26.18 still holds. Nothing in the analysis changes asymptotically.   cid:4    cid:4  as suggested, with directed edges. Make G  a. The idea is to use a maximum- cid:223 ow algorithm to  cid:222 nd a maximum bipartite matching that selects the edges to use in a minimum path cover. We must show how to formulate the max- cid:223 ow problem and how to construct the path cover from the resulting matching, and we must prove that the algorithm indeed  cid:222 nds a minimum path cover. into a  cid:223 ow network De cid:222 ne G  cid:4  is the with source x0 and sink y0 by de cid:222 ning all edge capacities to be 1. G in which L = {x1, . . . xn},  cid:4  cid:4   cid:223 ow network corresponding to a bipartite graph G R = {y1, . . . yn}, and the edges are the  undirected version of the  subset of E  cid:4  that doesn cid:146 t involve x0 or y0.  cid:4  cid:4  is that every vertex i in G is The relationship of G to the bipartite graph G . Edge  i, j   in G corresponds to represented by two vertices, xi and yi, in G edge  xi , y j   in G means that an edge in G leaves i and enters j. xi tells us about edges leaving i and yi tells us about edges entering i.  cid:4  cid:4  The edges in a bipartite matching in G because:    can be used in a path cover of G,  . That is, an edge  xi , y j   in G  In a bipartite In a bipartite matching, no vertex is used more than once. , the fact that no xi is used more than once means that at most matching in G one edge in the matching leaves any vertex i in G, and similarly the fact that   cid:4  cid:4    cid:4  cid:4    cid:4  cid:4    cid:4  cid:4    Solutions for Chapter 26: Maximum Flow  26-21  no yi is used more than once means that at most one edge in the matching enters any vertex i in G. In a path cover, no vertex appears in more than one path, hence at most one path edge enters each vertex and at most one path edge leaves each vertex.     We can construct a path cover P from any bipartite matching M  not just a maximum matching  by moving from some xi to the matching yj  if any , then from x j to its matching yk, and so on, as follows: 1. Start a new path containing a vertex i that has not yet been placed in a path. 2. If xi is unmatched, the path can cid:146 t go any farther; just add it to P. 3. If xi is matched to some yj , add j to the current path. If j has already been placed in a path  i.e., though we cid:146 ve just entered j by processing yj, we cid:146 ve already built a path that leaves j by processing xj  , combine this path with that one and go back to step 1. Otherwise go to step 2 to process xj .  This algorithm constructs a path cover because:   Every vertex is put into some path, because we keep picking an unused vertex  from which to start a path until there are no unused vertices.    No vertex is put into two paths, because every xi is matched to at most one yj , and vice versa. That is, at most one candidate edge leaves each vertex and at most one candidate edge enters each vertex. The normal path-building starts at or enters a vertex and then leaves it, building a single path. If we ever enter a vertex that was left earlier, it must have been the start of another path, since there are no cycles, and we combine those paths so that the vertex is entered and left on a single path.  p∈P    vertices in p    every vertex is on exactly 1 path   Every edge in M is used in some path because we visit every xi , and we incor- porate the single edge, if any, from each visited xi . Thus there is a one-to-one correspondence between edges in the matching and edges in the constructed path cover. We now show that the path cover P constructed above has the fewest possible  cid:2  paths when the matching is maximum. Let f be the  cid:223 ow corresonding to the bipartite matching M.  cid:2  V =  cid:2  = = p∈P = P +   edges in M  = P +  f    by 1-to-1 correspondence   Lemma 26.10  .   1 +  edges in p  1 +  Thus for the  cid:222 xed set V in our graph G,P  the number of paths  is minimized when the  cid:223 ow f is maximized. Thus the overall algorithm is as follows:   Use FORD-FULKERSON to  cid:222 nd a maximum  cid:223 ow in G  , hence a maximum    edges in p    cid:2   p∈P  p∈P   cid:4    cid:4  cid:4  bipartite matching M in G  .    Construct the path cover as described above.   26-22  Solutions for Chapter 26: Maximum Flow  Time: O V E  total   O V + E  to set up G   O V E  to  cid:222 nd the maximum bipartite matching   O E  to trace the paths, because each edge ∈ M is traversed only once and   cid:4   there are O E  edges in M.  b. The algorithm does not work if there are cycles.  Consider a graph G with 4 vertices, consisting of a directed triangle and an edge pointing to the triangle: E = { 1, 2 ,  2, 3 ,  3, 1 ,  4, 1 } G can be covered with a single path: 4 → 1 → 2 → 3, but our algorithm might  cid:222 nd only a 2-path cover.  cid:4  In the bipartite graph G  , the edges  xi , y j   are   x1, y2 ,  x2, y3 ,  x3, y1 ,  x4, y1  .  There are 4 edges from an xi to a y j , but 2 of them lead to y1, so a maximum  cid:4  is 3 . In bipartite matching can have only 3 edges  and the maximum  cid:223 ow in G fact, there are 2 possible maximum matchings. It is always possible to match x1 → y2 and x2 → y3, but then either x3 → y1 or x4 → y1 can be chosen, but not both. The maximum- cid:223 ow found by one of our max- cid:223 ow algorithms could  cid:222 nd the  cid:223 ow corresponding to either of these matchings, since both are maximal. But one of the matchings doesn cid:146 t contain an edge to or from vertex 4, so given that matching, our path algorithm is forced to produce 2 paths, one of which contains just the vertex 4.  a. Just execute one iteration of the Ford-Fulkerson algorithm. The edge  u, v  in E with increased capacity ensures that the edge  u, v  is in the residual graph. So look for an augmenting path and update the  cid:223 ow if a path is found. Time: O V + E  = O E  if we  cid:222 nd the augmenting path with either depth-  cid:222 rst or breadth- cid:222 rst search. To see that only one iteration is needed, consider separately the cases in which  u, v  is or is not an edge that crosses a minimum cut. If  u, v  does not cross a minimum cut, then increasing its capacity does not change the capacity of any minimum cut, and hence the value of the maximum  cid:223 ow does not change. If  u, v  does cross a minimum cut, then increasing its capacity by 1 increases the capacity of that minimum cut by 1, and hence possibly the value of the maxi- mum  cid:223 ow by 1. In this case, there is either no augmenting path  in which case there was some other minimum cut that  u, v  does not cross , or the augment- ing path increases  cid:223 ow by 1. No matter what, one iteration of Ford-Fulkerson suf cid:222 ces.  Solution to Problem 26-4   Solutions for Chapter 26: Maximum Flow  26-23  b. Let f be the maximum  cid:223 ow before reducing c u, v .   cid:4    cid:4   f   cid:4  x, y  = f  x, y  for all x, y ∈ V , except that f  If f  u, v  = 0, we don cid:146 t need to do anything. If f  u, v  > 0, we will need to update the maximum  cid:223 ow. Assume from now on that f  u, v  > 0, which in turn implies that f  u, v  ≥ 1.  cid:4  u, v  = f  u, v − 1. De cid:222 ne f obeys all capacity contraints, even after c u, v  has been reduced, Although f it is not a legal  cid:223 ow, as it violates skew symmetry and  cid:223 ow conservation at u and v. has one more unit of  cid:223 ow entering u than leaving u, and it has one more unit of  cid:223 ow leaving v than entering v. The idea is to try to reroute this unit of  cid:223 ow so that it goes out of u and into v via some other path. If that is not possible, we must reduce the  cid:223 ow from s to u and from v to t by one unit. Look for an augmenting path from u to v  note: not from s to t .      If there is such a path, augment the  cid:223 ow along that path. If there is no such path, reduce the  cid:223 ow from s to u by augmenting the  cid:223 ow from u to s. That is,  cid:222 nd an augmenting path u  cid:2  s and augment the  cid:223 ow along that path.  There de cid:222 nitely is such a path, because there is  cid:223 ow from s to u.  Similarly, reduce the  cid:223 ow from v to t by  cid:222 nding an augmenting path t  cid:2  v and augmenting the  cid:223 ow along that path.  Time: O V + E  = O E  if we  cid:222 nd the paths with either DFS or BFS.  a. The capacity of a cut is de cid:222 ned to be the sum of the capacities of the edges crossing it. Since the number of such edges is at mostE, and the capacity of each edge is at most C, the capacity of any cut of G is at most CE.  b. The capacity of an augmenting path is the minimum capacity of any edge on the path, so we are looking for an augmenting path whose edges all have capacity at least K . Do a breadth- cid:222 rst search or depth- cid:222 rst-search as usual to  cid:222 nd the path, considering only edges with residual capacity at least K .  Treat lower-capacity edges as though they don cid:146 t exist.  This search takes O V + E  = O E  time.  Note that V = O E  in a  cid:223 ow network.   c. MAX-FLOW-BY-SCALING uses the Ford-Fulkerson method. It repeatedly aug- ments the  cid:223 ow along an augmenting path until there are no augmenting paths of capacity greater ≥ 1. Since all the capacities are integers, and the capac- ity of an augmenting path is positive, this means that there are no augmenting paths whatsoever in the residual graph. Thus, by the max- cid:223 ow min-cut theorem, MAX-FLOW-BY-SCALING returns a maximum  cid:223 ow.   The  cid:222 rst time line 4 is executed, the capacity of any edge in Gf equals its capacity in G, and by part  a  the capacity of a minimum cut of G is at most C E. Initially K = 2  cid:9 lg C cid:10 +1 > 2lg C = C. So the capacity of a minimum cut of G f is initially less than 2K E.  , hence 2K = 2 · 2   cid:9 lg C cid:10  = 2   cid:9 lg C cid:10   d.  Solution to Problem 26-5   26-24  Solutions for Chapter 26: Maximum Flow   cid:4   e. By part  d , when line 4 is reached, the capacity of a minimum cut of Gf is at   cid:4  < K E  in G f has value  f    The other times line 4 is executed, K has just been halved, so the capacity of a cut of G f is at most 2K E at line 4 if and only if that capacity was at most K E when the while loop of lines 5 cid:150 6 last terminated. So we want to show that when line 7 is reached, the capacity of a minimum cut of Gf is most K E. Let G f be the residual network when line 7 is reached. There is no augmenting path of capacity ≥ K in G f ⇒ max  cid:223 ow f ⇒ min cut in G f has capacity < K E most 2K E, and thus the maximum  cid:223 ow in G f is at most 2K E. By an extension of Lemma 26.2, the value of the maximum  cid:223 ow in G equals the value of the current  cid:223 ow in G plus the value of the maximum  cid:223 ow in Gf .  cid:4  in G f induces a  Lemma 26.2 shows that, given a  cid:223 ow f in G, every  cid:223 ow f in G; the reverse claim, that every  cid:223 ow f + f  cid:223 ow f + f  cid:4  in G induces a in G f , is proved in a similar manner. Together these claims provide the  cid:223 ow f necessary correspondence between a maximum  cid:223 ow in G and a maximum  cid:223 ow in G f .  Therefore, the maximum  cid:223 ow in G is at most 2K E more than the current  cid:223 ow in G. Every time the inner while loop  cid:222 nds an augmenting path of capacity at least K , the  cid:223 ow in G increases by ≥ K . Since the  cid:223 ow cannot increase by more than 2K E, the loop executes at most  2K E  K = 2E times.   cid:4    cid:4   f. The time complexity is dominated by the loop of lines 4 cid:150 7.  The lines out- side the loop take O E  time.  The outer while loop executes O lg C  times, since K is initially O C  and is halved on each iteration, until K < 1. By part  e , the inner while loop executes O E  times for each value of K ; and by part  b , each iteration takes O E  time. Thus, the total time is O E2 lg C .   Lecture Notes for Chapter 27: Sorting Networks  An example of parallel algorithms. We cid:146 ll see how, if we allow a certain kind of parallelism, we can sort in O lg2 n   cid:147 time. cid:148  Along the way, we cid:146 ll see the 0-1 principle, which is a great way to prove the cor- rectness of any comparison-based sorting algorithm.  Chapter 27 overview  Sorting networks  Comparison networks  Comparator  x  y  min x,y       max x,y   Works in O 1  time.  Comparison network  input  output  9  5  2  6  5  9  2  6  2  5  6  9  2  5  6  9  wire  Wires go straight, left to right. Each comparator has inputs outputs on some pair of wires. Claim that this comparison network will sort any set of 4 input values:   27-2  Lecture Notes for Chapter 27: Sorting Networks    After leftmost comparators, minimum is on either wire 1  from top  or 3,  maximum is on either wire 2 or 4.    After next 2 comparators, minimum is on wire 1, maximum on wire 4.   Last comparator gets correct values onto wires 2 and 3. Running time = depth = longest path of comparators.  3 in previous example.    Think of dag of comparators that depend on each other. Depth = longest path  through dag  counting vertices, not edges .    Depth  cid:19 = max  of comparators attached to a single wire.     In the above example, that is 2.  Selection sorter  To  cid:222 nd max of 5 values:  Can repeat, decreasing  of values:  Depth: D n  = D n − 1  + 2  D 2  = 1  ⇒ D n  = 2n − 3 =  cid:2  n  .  If view depth as  cid:147 time, cid:148  parallelism gets us a faster method than any sequential comparison sort! Can view the same network as insertion sort:  [This material answers Exercise 27.1-6, showing that the network in Figure 27.3 doescorrectlysortandshowingitsrelationshiptoinsertionsort.]   Lecture Notes for Chapter 27: Sorting Networks  27-3  Zero-one principle  How can we test if a comparison network sorts?   We could try all n! permutations of input.   But we need to test only 2n permutations. This is many fewer than all n! per-  mutations.  Theorem  0-1 principle  If a comparison network with n inputs sorts all 2n sequences of 0 cid:146 s and 1 cid:146 s, then it sorts all sequences of arbitrary numbers.  Note: In practice, we don cid:146 t even have to reason about  cid:147 all 2n sequences cid:148  cid:151 instead, we look at the patterns of 0 cid:146 s and 1 cid:146 s cid:151 we cid:146 ll see later how.  Lemma If a comparison network transforms a =  cid:2 a1, a2, . . . , an cid:3  into b =  cid:2 b1, b2, . . . , bn cid:3  , then for any monotonically increasing function f , it transforms f  a  =  cid:2  f  a1 , f  a2 , . . . , f  an  cid:3  into f  b  =  cid:2  f  b1 , f  b2 , . . . , f  bm   cid:3  . Sketch of proof  x  y  f x   f y   min x,y        max x,y   min f x , f y   = f min x,y    max f x , f y   = f max x,y    since f is monotonically increasing  Then use induction on comparator depth.   lemma   Proof  of 0-1 principle  Suppose that the principle is not true, so that an n-input comparison network sorts all 0-1 sequences, but there is a sequence  cid:2 a1, a2, . . . , an cid:3  such that ai < a j but ai comes after aj in the output. De cid:222 ne the monotonically increasing function f  x  = By the lemma, if we give the input  cid:2  f  a1 , f  a2 , . . . , f  an  cid:3 , then the output will have f  ai   after f  a j  :   cid:9  0 if x ≤ ai , 1 if x > ai .  f aj  = 1  f ai  = 0  But that cid:146 s a 0-1 sequence that is sorted incorrectly, a contradiction.   theorem    27-4  Lecture Notes for Chapter 27: Sorting Networks  A bitonic sorting network  Constructing a sorting network  Step 1: Construct a  cid:147 bitonic sorter. cid:148  It sorts any bitonic sequence. A sequence is bitonic if it monotonically increases, then monotonically decreases, or it can be circularly shifted to become so.  Examples:   cid:2 1, 3, 7, 4, 2 cid:3   cid:2 6, 8, 3, 1, 2, 4 cid:3   cid:2 8, 7, 2, 1, 3, 5 cid:3  Any sequence of 1 or 2 numbers  For 0-1 sequences cid:151 which we can focus on cid:151 bitonic sequences have the form  0i  0  1j  1  0k  0  1i  1  0j  0  1k  1  Half-cleaner:  0 0 1 1 1 0 0 0  Depth = 1.  0 0 0 0 1 0 1 1  clean  bitonic  bitonic  bitonic  0 0 1 1 1 1 1 0  0 0 1 0 1 1 1 1  bitonic  clean  Lemma If the input to a half-cleaner is a bitonic 0-1 sequence, then for the output:        both the top and bottom half are bitonic, every element in the top half is ≤ every element in the bottom half, and at least one of the halves is clean cid:151 all 0 cid:146 s or all 1 cid:146 s.  Skipping proof cid:151 see book  not dif cid:222 cult at all .   Lecture Notes for Chapter 27: Sorting Networks  27-5  Bitonic sorter:  n  bitonic  half-cleaner  sorted  bitonic sorter  bitonic sorter  n 2  sorted  ≤ n 2  sorted  n 2  n 2  bitonic  0 0 0 0 1 0 1 1  0 0 0 0 1 0 1 1  0 0 0 0 0 1 1 1  0 0 1 1 1 0 0 0  bitonic  sorted  Depth: D n  = D n 2  + 1  D 2  = 1  ⇒ D n  = lg n .  Step 2: Construct a merging network. It merges 2 sorted sequences. Adapt a half-cleaner. Idea: Given 2 sorted sequences, reverse the second one, then concatenate with the  cid:222 rst one ⇒ get a bitonic sequence. Example: X = 0011  XY R = 00111110 So, we can merge X and Y by doing a bitonic sort on X and Y R. How to reverse Y ? Don cid:146 t! Instead, reverse the bottom half of the connections of the  cid:222 rst half-cleaner:  = 0111 Y Y R = 1110  bitonic   X,  sorted  Y,  sorted  0 0 1 1 0 1 1 1  0 0 1 0 1 1 1 1  bitonic  ≤  clean   27-6  Lecture Notes for Chapter 27: Sorting Networks  Full merging network:  0 1 1 1 0 0 1 1  0 1 0 1 0 1 0 0  X,  sorted  Y,  sorted  0 0 1 0 1 1 1 1  0 0 1 0 1 1 1 1  0 0 0 1 1 1 1 1  sorted  Depth is same as bitonic sorter: lg n.  Step 3: Construct a sorting network. Recursive merging cid:151 like merge sort, bottom-up:  n 2  n 2  sorter  sorter  n 2  n 2  sorted  2  2  2  2  merger  merger  merger  merger  4  4  merger  merger  1 0 1 0 1 0 0 0  0 0 1 1 0 0 0 1  merger  n  sorted  merger  8  sorted  0 0 0 0 0 1 1 1  mergers mergers  merger  Depth: D n  = D n 2  + lg n  D 2  = 1  ⇒ D n  =  cid:2  lg2 n    Exercise 4.4-2  .   Lecture Notes for Chapter 27: Sorting Networks  27-7  Use 0-1 principle to prove that this sorts all inputs.  Can we do better? Yes cid:151 the AKS network has depth O lg n .   Huge constant cid:151 over 1000.   Really hard to construct.   Highly impractical cid:151 of theoretical interest only.   Solutions for Chapter 27: Sorting Networks  Solution to Exercise 27.1-4  Solution to Exercise 27.1-5  Solution to Exercise 27.1-7  Consider any input element x. After 1 level of the network, x can be in at most 2 different places, in at most 4 places after 2 levels, and so forth. Thus we need at least lg n depth to be able to move x to the right place, which could be any of the n  = 2lg n  outputs.  Simulation of any sorting network on a serial machine is a comparison sort, hence there are  cid:3  n lg n  comparisons comparators. Intuitively, since the depth is  cid:3  lg n  and we can perform at most n 2 comparisons at each depth of the net- work, this  cid:3  n lg n  bound makes sense.  We take advantage of the comparators appearing in sorted order within the network in the following pseudocode. for i ← 1 to n do d[i] ← 0 do d[i] ← d[ j] ← max d[i], d[ j]  + 1  for each comparator  i, j   in the list of comparators  return max1≤i≤n d[i]  This algorithm implicitly  cid:222 nds the longest path in a dag of the comparators  in which an edge connects each comparator to the comparators that need its outputs . Even though we don cid:146 t explicitly construct the dag, the above sort produces a topo- logical sort of the dag. The  cid:222 rst for loop takes  cid:2  n  time, the second for loop takes  cid:2  c  time, and com- puting the maximum d[i] value in the return statement takes  cid:2  n  time, for a total of  cid:2  n + c  = O n + c  time.   Solutions for Chapter 27: Sorting Networks  27-9  Solution to Exercise 27.2-2   cid:9  0 if x ≤ k , 1 if x > k .  In both parts of the proof, we will be using a set{ f1, f2, . . . , fn−1} of monotoni- cally increasing functions, where fk x  = For convenience, let us also de cid:222 ne the sequences s1, s2, . . . , sn−1, where si is the sequence consisting of n − i 1 cid:146 s followed by i 0 cid:146 s. ⇒ : Assume that the sequence  cid:2 n,n−1, . . . ,1 cid:3  is correctly sorted by the given com- parison network. Then by Lemma 27.1, we know that applying any monotonically increasing function to the sequence s =  cid:2 n, n − 1, . . . , 1 cid:3  produces a sequence that is also correctly sorted by the given comparison network. For k = 1, 2, . . . , n − 1, when we apply the monotonically increasing function fk to the sequence s, the resulting sequence is sk, which is correctly sorted by the comparison network. ⇐ : Now assume that the comparison network fails to correctly sort the input sequence  cid:2 n, n − 1, . . . , 1 cid:3 . Then there are elements i and j in this sequence for which i < j but i appears after j in the output sequence. Consider the input sequence  cid:2  fi  n , fi  n − 1 , . . . , fi  1  cid:3 , which is the same as the sequence si. By Lemma 27.1, the network produces an output sequence in which fi  i   appears after fi   j  . But fi  i   = 0 and fi   j   = 1, and so the network fails to sort the input sequence si.  $ 0 2C n 2  + n 2  SORTER[n] consists of  n 4  lg2 n +  n 4  lg n =  cid:2  n lg2 n  comparators. To see this result, we  cid:222 rst note that MERGER[n] consists of  n 2  lg n comparators, since it has lg n levels, each with n 2 comparators. If we denote the number of comparators in SORTER[n] by C n , we have the re- currence C n  = We prove that C n  =  n 4  lg2 n +  n 4  lg n by induction on k. Basis: When k = 0, we have n = 1. Then  n 4  lg2 n +  n 4  lg n = 0 = C n . Inductive step: Assume that the inductive hypothesis holds for k − 1, so that C n 2  =  n 8  lg2 n 2  +  n 8  lg n 2  =  n 8  lg n − 1 2 +  n 8  lg n − 1 . We have  lg n if n = 2k and k ≥ 1 .  if n = 1 ,  Solution to Exercise 27.5-1   27-10  Solutions for Chapter 27: Sorting Networks  C n  = 2C n 2  + n 2   cid:7  lg n  lg n − 1 2 + n n 8 8 lg n + n lg2 n − n 4 2 lg2 n + n 4  lg n .  = 2 = n 4 = n 4   cid:8   lg n − 1  + n lg n 2 lg n − n + n + n 2 4 4  lg n  Solution to Exercise 27.5-2   cid:9   if n = 1 ,  0 D n 2  + lg n if n = 2k and k ≥ 1 ,  We show by substitution that the recurrence for the depth of SORTER[n], D n  = has the solution D n  =  lg n  lg n + 1  2. Basis: When k = 0, we have n = 1. Then  lg n  lg n + 1  2 = 0 = D 1 . Inductive step: Assume that the inductive hypothesis holds for k − 1, so that D n 2  =  lg n 2   lg n 2  + 1  2 =  lg n − 1  lg n  2. We have D n  = D n 2  + lg n =  lg n − 1  lg n  = lg2 n − lg n + lg n = lg2 n + lg n =  lg n  lg n + 1   + lg n  2  2  2  .  2   Index  This index covers exercises and problems from the textbook that are solved in this manual. The  cid:222 rst page in the manual that has the solution is listed here.  Exercise 2.2-2, 2-16 Exercise 2.2-4, 2-16 Exercise 2.3-3, 2-16 Exercise 2.3-4, 2-17 Exercise 2.3-5, 2-17 Exercise 2.3-6, 2-18 Exercise 2.3-7, 2-18 Exercise 3.1-1, 3-7 Exercise 3.1-2, 3-7 Exercise 3.1-3, 3-8 Exercise 3.1-4, 3-8 Exercise 3.1-8, 3-8 Exercise 3.2-4, 3-9 Exercise 4.2-2, 4-8 Exercise 4.2-5, 4-8 Exercise 5.1-3, 5-8 Exercise 5.2-1, 5-9 Exercise 5.2-2, 5-9 Exercise 5.2-4, 5-10 Exercise 5.2-5, 5-11 Exercise 5.3-1, 5-11 Exercise 5.3-2, 5-12 Exercise 5.3-3, 5-12 Exercise 5.3-4, 5-13 Exercise 5.4-6, 5-13 Exercise 6.1-1, 6-10 Exercise 6.1-2, 6-10 Exercise 6.1-3, 6-10 Exercise 6.2-6, 6-10 Exercise 6.3-3, 6-11 Exercise 6.4-1, 6-13 Exercise 6.5-2, 6-14 Exercise 7.2-3, 7-9 Exercise 7.2-5, 7-9  Exercise 7.3-1, 7-9 Exercise 7.4-2, 7-10 Exercise 8.1-3, 8-9 Exercise 8.1-4, 8-9 Exercise 8.2-2, 8-10 Exercise 8.2-3, 8-10 Exercise 8.2-4, 8-10 Exercise 8.3-2, 8-11 Exercise 8.3-3, 8-11 Exercise 8.3-4, 8-12 Exercise 8.4-2, 8-12 Exercise 9.1-1, 9-9 Exercise 9.3-1, 9-9 Exercise 9.3-3, 9-10 Exercise 9.3-5, 9-11 Exercise 9.3-8, 9-11 Exercise 9.3-9, 9-12 Exercise 11.1-4, 11-16 Exercise 11.2-1, 11-17 Exercise 11.2-4, 11-17 Exercise 11.3-3, 11-18 Exercise 11.3-5, 11-19 Exercise 12.1-2, 12-12 Exercise 12.2-5, 12-12 Exercise 12.2-7, 12-12 Exercise 12.3-3, 12-13 Exercise 12.4-1, 12-10, 12-14 Exercise 12.4-3, 12-7 Exercise 12.4-4, 12-15 Exercise 13.1-3, 13-13 Exercise 13.1-4, 13-13 Exercise 13.1-5, 13-13 Exercise 13.2-4, 13-14 Exercise 13.3-3, 13-14   I-2  Index  Exercise 13.3-4, 13-15 Exercise 13.4-6, 13-16 Exercise 13.4-7, 13-16 Exercise 14.1-5, 14-9 Exercise 14.1-6, 14-9 Exercise 14.1-7, 14-9 Exercise 14.2-2, 14-10 Exercise 14.2-3, 14-12 Exercise 14.3-3, 14-13 Exercise 14.3-6, 14-13 Exercise 14.3-7, 14-14 Exercise 15.1-5, 15-19 Exercise 15.2-4, 15-19 Exercise 15.3-1, 15-20 Exercise 15.4-4, 15-21 Exercise 16.1-2, 16-9 Exercise 16.1-3, 16-9 Exercise 16.1-4, 16-10 Exercise 16.2-2, 16-11 Exercise 16.2-4, 16-12 Exercise 16.2-6, 16-13 Exercise 16.2-7, 16-13 Exercise 16.4-2, 16-14 Exercise 16.4-3, 16-14 Exercise 17.1-3, 17-14 Exercise 17.2-1, 17-14 Exercise 17.2-2, 17-15 Exercise 17.2-3, 17-16 Exercise 17.3-3, 17-17 Exercise 21.2-3, 21-6 Exercise 21.2-5, 21-7 Exercise 21.3-3, 21-7 Exercise 21.3-4, 21-7 Exercise 21.4-4, 21-8 Exercise 21.4-5, 21-8 Exercise 21.4-6, 21-9 Exercise 22.1-6, 22-12 Exercise 22.1-7, 22-14 Exercise 22.2-4, 22-14 Exercise 22.2-5, 22-14 Exercise 22.2-6, 22-14 Exercise 22.3-4, 22-15 Exercise 22.3-7, 22-15 Exercise 22.3-8, 22-16 Exercise 22.3-10, 22-16 Exercise 22.3-11, 22-16 Exercise 22.4-3, 22-17 Exercise 22.4-5, 22-18  Exercise 22.5-5, 22-19 Exercise 22.5-6, 22-20 Exercise 22.5-7, 22-21 Exercise 23.1-1, 23-8 Exercise 23.1-4, 23-8 Exercise 23.1-6, 23-8 Exercise 23.1-10, 23-9 Exercise 23.2-4, 23-9 Exercise 23.2-5, 23-9 Exercise 23.2-7, 23-10 Exercise 24.1-3, 24-13 Exercise 24.2-3, 24-13 Exercise 24.3-3, 24-14 Exercise 24.3-4, 24-14 Exercise 24.3-6, 24-15 Exercise 24.3-7, 24-16 Exercise 24.4-4, 24-17 Exercise 24.4-7, 24-18 Exercise 24.4-10, 24-18 Exercise 24.5-4, 24-18 Exercise 24.5-7, 24-19 Exercise 24.5-8, 24-19 Exercise 25.1-3, 25-8 Exercise 25.1-5, 25-8 Exercise 25.1-10, 25-9 Exercise 25.2-4, 25-11 Exercise 25.2-6, 25-12 Exercise 25.3-4, 25-13 Exercise 25.3-6, 25-13 Exercise 26.1-4, 26-15 Exercise 26.1-6, 26-16 Exercise 26.1-7, 26-16 Exercise 26.1-9, 26-17 Exercise 26.2-4, 26-17 Exercise 26.2-9, 26-17 Exercise 26.2-10, 26-18 Exercise 26.3-3, 26-18 Exercise 26.4-2, 26-19 Exercise 26.4-3, 26-19 Exercise 26.4-6, 26-20 Exercise 27.1-4, 27-8 Exercise 27.1-5, 27-8 Exercise 27.1-6, 27-2 Exercise 27.1-7, 27-8 Exercise 27.2-2, 27-9 Exercise 27.5-1, 27-9 Exercise 27.5-2, 27-10   I-3  Index  Problem 2-1, 2-19 Problem 2-2, 2-20 Problem 2-4, 2-21 Problem 3-3, 3-9 Problem 4-1, 4-9 Problem 4-4, 4-11 Problem 5-1, 5-14 Problem 6-1, 6-14 Problem 6-2, 6-15 Problem 7-4, 7-11 Problem 8-1, 8-12 Problem 8-3, 8-15 Problem 8-4, 8-16 Problem 9-1, 9-13 Problem 9-2, 9-14 Problem 9-3, 9-18 Problem 11-1, 11-20 Problem 11-2, 11-21 Problem 11-3, 11-24 Problem 12-2, 12-16 Problem 12-3, 12-17 Problem 13-1, 13-16 Problem 14-1, 14-15 Problem 14-2, 14-16 Problem 15-1, 15-22 Problem 15-2, 15-24 Problem 15-3, 15-27 Problem 15-6, 15-30 Problem 16-1, 16-16 Problem 17-2, 17-18 Problem 17-4, 17-20 Problem 21-1, 21-9 Problem 21-2, 21-11 Problem 22-1, 22-22 Problem 22-3, 22-22 Problem 22-4, 22-26 Problem 23-1, 23-12 Problem 24-1, 24-19 Problem 24-2, 24-20 Problem 24-3, 24-21 Problem 24-4, 24-22 Problem 24-6, 24-24 Problem 25-1, 25-13 Problem 26-2, 26-20 Problem 26-4, 26-22 Problem 26-5, 26-23

@highlight

The updated new edition of the classic Introduction to Algorithms is intended primarily for use in undergraduate or graduate courses in algorithms or data structures. Like the first edition, this text can also be used for self-study by technical professionals since it discusses engineering issues in algorithm design as well as the mathematical aspects. In its new edition, Introduction to Algorithms continues to provide a comprehensive introduction to the modern study of algorithms. The revision has been updated to reflect changes in the years since the book's original publication. New chapters on the role of algorithms in computing and on probabilistic analysis and randomized algorithms have been included. Sections throughout the book have been rewritten for increased clarity, and material has been added wherever a fuller explanation has seemed useful or new information warrants expanded coverage. As in the classic first edition, this new edition of Introduction to Algorithms presents a rich variety of algorithms and covers them in considerable depth while making their design and analysis accessible to all levels of readers. Further, the algorithms are presented in pseudocode to make the book easily accessible to students from all programming language backgrounds. Each chapter presents an algorithm, a design technique, an application area, or a related topic. The chapters are not dependent on one another, so the instructor can organize his or her use of the book in the way that best suits the course's needs. Additionally, the new edition offers a 25% increase over the first edition in the number of problems, giving the book 155 problems and over 900 exercises that reinforce the concepts the students are learning.