Deep Learning for Computer Vision with  Python  Practitioner Bundle  Dr. Adrian Rosebrock  1st Edition  1.1.0    Copyright c cid:13  2017 Adrian Rosebrock, PyImageSearch.com  PUBLISHED BY PYIMAGESEARCH  PYIMAGESEARCH.COM The contents of this book, unless otherwise indicated, are Copyright c cid:13 2017 Adrian Rosebrock, PyimageSearch.com. All rights reserved. Books like this are made possible by the time invested by the authors. If you received this book and did not purchase it, please consider making future books possible by buying a copy at https:  www.pyimagesearch.com deep-learning-computer-vision- python-book  today.  First printing, September 2017   To my father, Joe; my wife, Trisha;  and the family beagles, Josie and Jemma. Without their constant love and support,  this book would not be possible.    Contents  1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11  2 Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 13 What Is Data Augmentation? 2.1 14 Visualizing Data Augmentation 2.2 Comparing Training With and Without Data Augmentation 17 2.3 2.3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 The Flowers-17 Dataset 2.3.2 Aspect-aware Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Flowers-17: No Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.3.3 2.3.4 Flowers-17: With Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 29 Summary 2.4  3 Networks as Feature Extractors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.1 32 Extracting Features with a Pre-trained CNN 3.1.1 What Is HDF5? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.1.2 Writing Features to an HDF5 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 The Feature Extraction Process 3.2 37 Extracting Features From Animals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 3.2.1 Extracting Features From CALTECH-101 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 3.2.2 3.2.3 Extracting Features From Flowers-17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 Training a Classiﬁer on Extracted Features 3.3 43 Results on Animals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.3.1 3.3.2 Results on CALTECH-101 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 Results on Flowers-17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.3.3 3.4 Summary 46   4 Understanding rank-1 & rank-5 Accuracies . . . . . . . . . . . . . . . . . . . . 49 4.1 Ranked Accuracy 49 4.1.1 Measuring rank-1 and rank-5 Accuracies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 Implementing Ranked Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4.1.2 Ranked Accuracy on Flowers-17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 4.1.3 Ranked Accuracy on CALTECH-101 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 4.1.4 4.2 Summary 54  5 Fine-tuning Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 Transfer Learning and Fine-tuning 57 5.1 5.1.1 Indexes and Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 5.1.2 Network Surgery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 5.1.3 Fine-tuning, from Start to Finish . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 69 Summary 5.2  6 6.1 6.1.1 6.1.2 Constructing an Ensemble of CNNs 6.1.3 6.2  Improving Accuracy with Network Ensembles . . . . . . . . . . . . . . . . . . 71 Ensemble Methods 71 Jensen’s Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 Evaluating an Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 80 Summary  7 Advanced Optimization Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 7.1 83 Adaptive Learning Rate Methods 7.1.1 Adagrad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 7.1.2 Adadelta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 7.1.3 RMSprop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 7.1.4 Adam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 7.1.5 Nadam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 7.2 86 Choosing an Optimization Method Three Methods You Should Learn how to Drive: SGD, Adam, and RMSprop . . 86 7.2.1 7.3 Summary 87  8 8.1 8.2 8.3  9 9.1 9.2 9.2.1 9.3 9.4  Optimal Pathway to Apply Deep Learning . . . . . . . . . . . . . . . . . . . . . 89 89 A Recipe for Training Transfer Learning or Train from Scratch 93 94 Summary  Working with HDF5 and Large Datasets . . . . . . . . . . . . . . . . . . . . . . . . . 95 95 Downloading Kaggle: Dogs vs. Cats Creating a Conﬁguration File 96 Your First Conﬁguration File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 98 Building the Dataset Summary 102   10 Competing in Kaggle: Dogs vs. Cats . . . . . . . . . . . . . . . . . . . . . . . . . . 103 10.1 Additional Image Preprocessors 103 10.1.1 Mean Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 10.1.2 Patch Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 10.1.3 Crop Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 10.2 HDF5 Dataset Generators 109 112 10.3 117 10.4 10.5 120 10.6 Obtaining a Top-5 Spot on the Kaggle Leaderboard 123 10.6.1 Extracting Features Using ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 10.6.2 Training a Logistic Regression Classiﬁer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 128 10.7  Implementing AlexNet Training AlexNet on Kaggle: Dogs vs. Cats Evaluating AlexNet  Summary  The Inception Module  and its Variants   11 GoogLeNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 11.1 132 11.1.1 Inception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 11.1.2 Miniception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 11.2 MiniGoogLeNet on CIFAR-10 134 11.2.1 Implementing MiniGoogLeNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 11.2.2 Training and Evaluating MiniGoogLeNet on CIFAR-10 . . . . . . . . . . . . . . . . . . . 140 11.2.3 MiniGoogLeNet: Experiment 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 11.2.4 MiniGoogLeNet: Experiment 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 11.2.5 MiniGoogLeNet: Experiment 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 11.3 146 11.3.1 Downloading Tiny ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 11.3.2 The Tiny ImageNet Directory Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148 11.3.3 Building the Tiny ImageNet Dataset 11.4 DeeperGoogLeNet on Tiny ImageNet 153 11.4.1 Implementing DeeperGoogLeNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 11.4.2 Training DeeperGoogLeNet on Tiny ImageNet . . . . . . . . . . . . . . . . . . . . . . . . 161 11.4.3 Creating the Training Script . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 11.4.4 Creating the Evaluation Script . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163 11.4.5 DeeperGoogLeNet Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 11.5 168  The Tiny ImageNet Challenge  Summary  12 ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171 12.1 ResNet and the Residual Module 171 12.1.1 Going Deeper: Residual Modules and Bottlenecks . . . . . . . . . . . . . . . . . . . . . 172 12.1.2 Rethinking the Residual Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174 175 12.2 12.3 180 12.3.1 Training ResNet on CIFAR-10 With the ctrl + c Method . . . . . . . . . . . . . . . . . . 181 12.3.2 ResNet on CIFAR-10: Experiment 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185  Implementing ResNet ResNet on CIFAR-10   Training ResNet on CIFAR-10 with Learning Rate Decay ResNet on Tiny ImageNet  188 12.4 12.5 192 12.5.1 Updating the ResNet Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 12.5.2 Training ResNet on Tiny ImageNet With the ctrl + c Method . . . . . . . . . . . . . . 194 12.5.3 Training ResNet on Tiny ImageNet with Learning Rate Decay . . . . . . . . . . . . . 198 12.6 202  Summary   Companion Website  Amazon Machine Image  AMI   Thank you for picking up a copy of Deep Learning for Computer Vision with Python! To accompany this book I have created a companion website which includes:    Up-to-date installation instructions on how to conﬁgure your development environment   Instructions on how to use the pre-conﬁgured Ubuntu VirtualBox virtual machine and   Supplementary material that I could not ﬁt inside this book Additionally, you can use the “Issues” feature inside the companion website to report any bugs, typos, or problems you encounter when working through the book. I don’t expect many problems; however, this is a brand new book so myself and other readers would appreciate reporting any issues you run into. From there, I can keep the book updated and bug free.  To create your companion website account, just use this link: http:  pyimg.co fnkxk Take a second to create your account now so you’ll have access to the supplementary materials  as you work through the book.    1. Introduction  Welcome to the Practitioner Bundle of Deep Learning for Computer Vision with Python! This volume is meant to be the next logical step in your deep learning for computer vision education after completing the Starter Bundle.  At this point, you should have a strong understanding of the fundamentals of parameterized learning, neural networks, and Convolutional Neural Networks  CNNs . You should also feel relatively comfortable using the Keras library and the Python programming language to train your own custom deep learning networks.  The purpose of the Practitioner Bundle is to build on your knowledge gained from the Starter Bundle and introduce more advanced algorithms, concepts, and tricks of the trade — these tech- niques will be covered in three distinct parts of the book.  The ﬁrst part will focus on methods that are used to boost your classiﬁcation accuracy in one way or another. One way to increase your classiﬁcation accuracy is to apply transfer learning methods such as ﬁne-tuning or treating your network as a feature extractor.  We’ll also explore ensemble methods  i.e., training multiple networks and combining the results  and how these methods can give you a nice classiﬁcation boost with little extra effort. Regularization methods such as data augmentation are used to generate additional training data – in nearly all situations, data augmentation improves your model’s ability to generalize. More advanced optimization algorithms such as Adam [1], RMSprop [2], and others can also be used on some datasets to help you obtain lower loss. After we review these techniques, we’ll look at the optimal pathway to apply these methods to ensure you obtain the maximum amount of beneﬁt with the least amount of effort.  We then move on to the second part of the Practitioner Bundle which focuses on larger datasets and more exotic network architectures. Thus far we have only worked with datasets that have ﬁt into the main memory of our system – but what if our dataset is too large to ﬁt into RAM? What do we do then? We’ll address this question in Chapter 9 when we work with HDF5.  Given that we’ll be working with larger datasets, we’ll also be able to discuss more advanced network architectures using AlexNet, GoogLeNet, ResNet, and deeper variants of VGGNet. These network architectures will be applied to more challenging datasets and competitions, including the   12  Chapter 1. Introduction  Kaggle: Dogs vs. Cats recognition challenge [3] as well as the cs231n Tiny ImageNet challenge [4], the exact same task Stanford CNN students compete in. As we’ll ﬁnd out, we’ll be able to obtain a top-25 position on the Kaggle Dogs vs. Cats leaderboard and top the cs231n challenge for our technique type.  The ﬁnal part of this book covers applications of deep learning for computer vision outside of image classiﬁcation, including basic object detection, deep dreaming and neural style, Generative Adversarial Networks  GANs , and Image Super Resolution. Again, the techniques covered in this volume are meant to be much more advanced than the Starter Bundle – this is where you’ll start to separate yourself from a deep learning novice and transform into a true deep learning practitioner. To start your transformation to deep learning expert, just ﬂip the page.   2. Data Augmentation  According to Goodfellow et al., regularization is “any modiﬁcation we make to a learning algo- rithm that is intended to reduce its generalization error, but not its training error” [5]. In short, regularization seeks to reduce our testing error perhaps at the expense of increasing training error slightly.  We’ve already looked at different forms of regularization in Chapter 9 of the Starter Bundle; however, these were parameterized forms of regularization, requiring us to update our loss update function. In fact, there exist other types of regularization that either:  1. Modify the network architecture itself. 2. Augment the data passed into the network for training. Dropout is a great example of modifying a network architecture by achieving greater general- izability. Here we insert a layer that randomly disconnects nodes from the previous layer to the next layer, thereby ensuring that no single node is responsible for learning how to represent a given class.  In the remainder of this chapter, we’ll be discussing another type of regularization called data augmentation. This method purposely perturbs training examples, changing their appearance slightly, before passing them into the network for training. The end result is that a network consistently sees “new” training data points generated from the original training data, partially alleviating the need for us to gather more training data  though in general, gathering more training data will rarely hurt your algorithm .  2.1 What Is Data Augmentation?  Data augmentation encompasses a wide range of techniques used to generate new training samples from the original ones by applying random jitters and perturbations such that the classes labels are not changed. Our goal when applying data augmentation is to increase the generalizability of the model. Given that our network is constantly seeing new, slightly modiﬁed versions of the input data points, it’s able to learn more robust features. At testing time, we do not apply data augmentation and evaluate our trained network – in most cases, you’ll see an increase in testing accuracy, perhaps at the expense at a slight dip in training accuracy.   14  Chapter 2. Data Augmentation  Figure 2.1: Left: A sample of 250 data points that follow a normal distribution exactly. Right: Adding a small amount of random “jitter” to the distribution. This type of data augmentation can increase the generalizability of our networks.  Let’s consider the Figure 2.1  left  of a normal distribution with zero mean and unit variance. Training a machine learning model on this data may result in us modeling the distribution exactly – however, in real-world applications, data rarely follows such a neat distribution.  Instead, to increase the generalizability of our classiﬁer, we may ﬁrst randomly jitter points along the distribution by adding some values ε drawn from a random distribution  right . Our plot still follows an approximately normal distribution, but it’s not a perfect distribution as on the left. A model trained on this data is more likely to generalize to example data points not included in the training set.  In the context of computer vision, data augmentation lends itself naturally. For example, we can obtain additional training data from the original images by apply simple geometric transforms such as random:  1. Translations 2. Rotations 3. Changes in scale 4. Shearing 5. Horizontal  and in some cases, vertical  ﬂips Applying a  small  amount of these transformations to an input image will change its appearance slightly, but it does not change the class label – thereby making data augmentation a very natural, easy method to apply to deep learning for computer vision tasks. More advanced techniques for data augmentation applied to computer vision include random perturbation of colors in a given color space [6] and nonlinear geometric distortions [7].  2.2 Visualizing Data Augmentation  The best way to understand data augmentation applied to computer tasks is to simply visualize a given input being augmented and distorted. To accomplish this visualization, let’s build a simple Python script that uses the built-in power of Keras to perform data augmentation. Create a new ﬁle, name it augmentation_demo.py. and insert the following code:   1  2  3  4  5  6  8  9  10  11  12  13  14  15  16  18  19  20  21  22  23  25  26  27  28  29  30  2.2 Visualizing Data Augmentation  15   import the necessary packages from keras.preprocessing.image import ImageDataGenerator from keras.preprocessing.image import img_to_array from keras.preprocessing.image import load_img import numpy as np import argparse  Lines 2-6 import our required Python packages. Take note of Line 2 where we import the ImageDataGenerator class from Keras – this code will be used for data augmentation and includes all relevant methods to help us transform our input image.  Next, we parse our command line arguments:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-i", "--image", required=True,  help="path to the input image"   ap.add_argument "-o", "--output", required=True,  help="path to output directory to store augmentation examples"   ap.add_argument "-p", "--prefix", type=str, default="image",  help="output filename prefix"   args = vars ap.parse_args     visualize the results.  Our script requires three command line arguments, each detailed below:   --image: This is the path to the input image that we want to apply data augmentation to and   --output: After applying data augmentation to a given image, we would like to store the   --prefix: A string that will be prepended to the output image ﬁlename. Now that our command line arguments are parsed, let’s load our input image, convert it to a Keras-compatible array, and add an extra dimension to the image, just as we would do if we were preparing our image for classiﬁcation:  result on disk so we can inspect it – this switch controls the output directory.   load the input image, convert it to a NumPy array, and then  reshape it to have an extra dimension print "[INFO] loading example image..."  image = load_img args["image"]  image = img_to_array image  image = np.expand_dims image, axis=0   We are now ready to initialize our ImageDataGenerator:   construct the image generator for data augmentation then  initialize the total number of images generated thus far aug = ImageDataGenerator rotation_range=30, width_shift_range=0.1,  height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode="nearest"   total = 0  The ImageDataGenerator class has a number of parameters, too many to enumerate in this book. For a full review of the parameters, please refer to the ofﬁcial Keras documentation  http:  pyimg.co j8ad8 .   16  Chapter 2. Data Augmentation  Instead, we’ll be focusing on the augmentation parameters you will most likely use in your own applications. The rotation_range parameter controls the degree range of the random rotations. Here we’ll allow our input image to be randomly rotated ±30 degrees. Both the width_shift_range and height_shift_range are used for horizontal and vertical shifts, re- spectively. The parameter value is a fraction of the given dimension, in this case, 10%.  The shear_range controls the angle in counterclockwise direction as radians in which our image will allowed to be sheared. We then have the zoom_range, a ﬂoating point value that allows the image to be “zoomed in” or “zoomed out” according to the following uniform distribution of values: [1 - zoom_range, 1 + zoom_range].  Finally, the horizontal_flip boolean controls whether or not a given input is allowed to be ﬂipped horizontally during the training process. For most computer vision applications a horizontal ﬂip of an image does not change the resulting class label – but there are applications where a horizontal  or vertical  ﬂip does change the semantic meaning of the image. Take care when applying this type of data augmentation as our goal is to slightly modify the input image, thereby generating a new training sample, without changing the class label itself. For a more detailed review of image transformations, please refer to Module 1 in PyImageSearch Gurus  [8], PyImageSearch Gurus  as well as Szeliski [9].  Once ImageDataGenerator is initialized, we can actually generate new training examples:  32  33  34  35  36  37  38  39  40  41  42  43  44   construct the actual Python generator print "[INFO] generating images..."  imageGen = aug.flow image, batch_size=1, save_to_dir=args["output"],  save_prefix=args["prefix"], save_format="jpg"    loop over examples from our image data augmentation generator for image in imageGen:   increment our counter total += 1   if we have reached 10 examples, break from the loop if total == 10:  break  Lines 34 and 35 initialize a Python generator used to construct our augmented images. We’ll pass in our input image, a batch_size of 1  since we are only augmenting one image , along with a few additional parameters to specify the output image ﬁle paths, the preﬁx for each ﬁle path, and the image ﬁle format. Line 38 then starts looping over each image in the imageGen generator. Internally, imageGen is automatically generating a new training sample each time one is requested via the loop. We then increment the total number of data augmentation examples written to disk and stop the script from executing once we’ve reached ten examples.  To visualize data augmentation in action, we’ll be using Figure 2.2  left , an image of Jemma, my family beagle. To generate new training example images of Jemma, just execute the following command:  $ python augmentation_demo.py --image jemma.png --output output  After the script executes you should see ten images in the output directory:  $ ls output  image_0_1227.jpg image_0_2358.jpg  image_0_4205.jpg  image_0_4770.jpg   2.3 Comparing Training With and Without Data Augmentation  17  Figure 2.2: Left: The input image we are going to apply data augmentation to. Right: A montage of data augmentation examples. Notice how each image has been randomly rotated, sheared, zoomed, and horizontally ﬂipped.  image_0_1933.jpg image_0_2914.jpg image_0_9197.jpg image_0_953.jpg  image_0_4657.jpg  image_0_6934.jpg  I have constructed a montage of each of these images so you can visualize them in Figure 2.2  right . Notice how each image has been randomly rotated, sheared, zoomed, and horizontally ﬂipped. In each case the image retains the original class label: dog; however, each image has been modiﬁed slightly, thereby giving our neural network new patterns to learn from when training. Since the input images will constantly be changing  while the class labels remain the same , it’s common to see our training accuracy decrease when compared to training without data augmentation.  However, as we’ll ﬁnd out later in this chapter, data augmentation can help dramatically reduce overﬁtting, all the while ensuring that our model generalizes better to new input samples. Further- more, when working with datasets where we have too few examples to apply deep learning, we can utilize data augmentation to generate additional training data, thereby reducing the amount of hand-labeled data required to train a deep learning network.  2.3 Comparing Training With and Without Data Augmentation  In the ﬁrst part of this section, we’ll discuss the Flowers-17 dataset, a very small dataset  in terms of deep learning for computer vision tasks , and how data augmentation can help us artiﬁcially increase the size of this dataset by generating additional training samples. From there we’ll perform two experiments:  1. Train MiniVGGNet on Flowers-17 without data augmentation. 2. Train MiniVGGNet on Flowers-17 with data augmentation. As we’ll ﬁnd out, applying data augmentation dramatically reduces overﬁtting and allows  MiniVGGNet to obtain substantially higher classiﬁcation accuracy.  2.3.1 The Flowers-17 Dataset  The Flowers-17 dataset [10] is a ﬁne-grained classiﬁcation challenge where our task is to recognize 17 distinct species of ﬂowers. The image dataset is quite small, having only 80 images per class for a total of 1,360 images. A general rule of thumb when applying deep learning to computer vision tasks is to have 1,000-5,000 examples per class, so we are certainly at a huge deﬁcit here.  We call the Flowers-17 a ﬁne-grained classiﬁcation task because all categories are very similar  i.e., species of ﬂower . In fact, we can think of each of these categories as subcategories. The categories are certainly different, but share a signiﬁcant amount of common structure  e.x., petals,   18  Chapter 2. Data Augmentation  Figure 2.3: A sample of ﬁve  out of the seventeen total  classes in the Flowers-17 dataset where each class represents a speciﬁc ﬂower species.  stamen, pistil, etc. . Fine-grained classiﬁcation tasks tend to be the most challenging for deep learning practitioners as it implies that our machine learning models need to learn extremely discrim- inating features to distinguish between classes that are very similar. This ﬁne-grained classiﬁcation task becomes even more problematic given our limited training data.  2.3.2 Aspect-aware Preprocessing  Up until this point, we have only preprocessed images by resizing them to a ﬁxed size, ignoring the aspect ratio. In some situations, especially for basic benchmark datasets, doing so is acceptable. However, for more challenging datasets we should still seek to resize to a ﬁxed size, but  maintain the aspect ratio. To visualize this action, consider Figure 2.4. On the left, we have an input image that we need to resize to a ﬁxed width and height. Ignoring the aspect ratio, we resize the image to 256 × 256 pixels  middle , effectively squishing and distorting the image such that it meets our desired dimensions. A better approach would be to take into account the aspect ratio of the image  right  where we ﬁrst resize along the shorter dimension such that the width is 256 pixels and then crop the image along the height, such that the height is 256 pixels.  While we have effectively discarded part of the image during the crop, we have also maintained the original aspect ratio of the image. Maintaining a consistent aspect ratio allows our Convolutional Neural Network to learn more discriminative, consistent features. This is a common technique that we’ll be applying when working with more advanced datasets throughout the rest of the Practitioner Bundle and ImageNet Bundle.  To see how aspect-aware preprocessing is implemented, let’s update our pyimagesearch  project structure to include a AspectAwarePreprocessor:  --- pyimagesearch        --- __init__.py --- callbacks --- nn --- preprocessing    --- __init__.py --- aspectawarepreprocessor.py   2.3 Comparing Training With and Without Data Augmentation  19  Figure 2.4: Left: The original input image  410× 310 . Middle: Resizing the image to 256× 256 pixels, ignoring the aspect ratio. Notice how the image now appears squished and distorted. Right: Resizing the image to 256× 256 while maintaining the aspect ratio.        --- utils  --- imagetoarraypreprocessor.py --- simplepreprocessor.py  Notice how we have added a new ﬁle named aspectawarepreprocessor.py inside the preprocessing sub-module – this location is where our new preprocessor will be live. Open up aspectawarepreprocessor.py and insert the following code:   import the necessary packages import imutils import cv2  class AspectAwarePreprocessor:  def __init__ self, width, height, inter=cv2.INTER_AREA :   store the target image width, height, and interpolation  method used when resizing self.width = width self.height = height self.inter = inter  Just as in our SimplePreprocessor, our constructor requires two parameters  the desired width and height of the target output image  along with the interpolation method used when resizing the image. We can then deﬁne the preprocess function below:  def preprocess self, image :   grab the dimensions of the image and then initialize  the deltas to use when cropping  h, w  = image.shape[:2] dW = 0 dH = 0  1  2  3  4  5  6  7  8  9  10  11  13  14  15  16  17  18   20  Chapter 2. Data Augmentation  The preprocess function accepts a single argument, the image that we wish to preprocess. Line 16 grabs the width and height of the input image, while Lines 17 and 18 determine the delta offsets we’ll be using when cropping along the larger dimension. Again, our aspect-aware preprocessor is a two step algorithm:  1. Step 1: Determine the shortest dimension and resize along it. 2. Step 2: Crop the image along the largest dimension to obtain the target width and height. The following code block handles checking if the width is smaller than the height, and if so,  resizes along the width:  20  21  22  23  24  25  26  27  29  30  31  32  33  34  35  37  38  39  40  41  42  43  44  45  46  47   if the width is smaller than the height, then resize  along the width  i.e., the smaller dimension  and then  update the deltas to crop the height to the desired  dimension if w < h:  image = imutils.resize image, width=self.width,  inter=self.inter   dH = int  image.shape[0] - self.height    2.0   Otherwise, if the height is smaller than the width, then we resize along the height:   otherwise, the height is smaller than the width so  resize along the height and then update the deltas  to crop along the width else:  image = imutils.resize image, height=self.height,  inter=self.inter   dW = int  image.shape[1] - self.width    2.0   Now that our image is resized, we need to re-grab the width and height and use the deltas to  crop the center of the image:   now that our images have been resized, we need to  re-grab the width and height, followed by performing  the crop  h, w  = image.shape[:2] image = image[dH:h - dH, dW:w - dW]   finally, resize the image to the provided spatial  dimensions to ensure our output image is always a fixed  size return cv2.resize image,  self.width, self.height ,  interpolation=self.inter   When cropping  due to rounding errors , our image target image dimensions may be off by ± one pixel; therefore, we make a call to cv2.resize to ensure our output image has the desired width and height. The preprocessed image is then returned to the calling function. Now that we’ve implemented our AspectAwarePreprocessor, let’s put it to work when training the MiniVGGNet architecture on the Flowers-17 dataset.   2.3 Comparing Training With and Without Data Augmentation  21  2.3.3 Flowers-17: No Data Augmentation  To start, let’s establish a baseline using no data augmentation when training the MiniVGGNet architecture  Chapter 15, Starter Bundle  on the Flowers-17 dataset. Open up a new ﬁle, name it minivggnet_flowers17.py, and we’ll get to work:   import the necessary packages from sklearn.preprocessing import LabelBinarizer from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from pyimagesearch.preprocessing import ImageToArrayPreprocessor from pyimagesearch.preprocessing import AspectAwarePreprocessor from pyimagesearch.datasets import SimpleDatasetLoader from pyimagesearch.nn.conv import MiniVGGNet from keras.optimizers import SGD from imutils import paths import matplotlib.pyplot as plt import numpy as np import argparse import os  Lines 2-14 import our required Python packages. Most of these imports you’ve seen before,  but I want to draw your attention to:  1. Line 6: Here we import our newly deﬁned AspectAwarePreprocessor. 2. Line 7: Despite using a separate image preprocessor, we’ll still be able to use SimpleDatasetLoader  to load our dataset from disk.  3. Line 8: We’ll be training the MiniVGGNet architecture on our dataset. Next, we parse our command line arguments:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-d", "--dataset", required=True,  help="path to input dataset"   args = vars ap.parse_args     We only need a single switch here, --dataset, which is the path to our Flowers-17 dataset  directory on disk.  Let’s go ahead and extract the class labels from our input images:   grab the list of images that we’ll be describing, then extract  the class label names from the image paths print "[INFO] loading images..."  imagePaths = list paths.list_images args["dataset"]   classNames = [pt.split os.path.sep [-2] for pt in imagePaths] classNames = [str x  for x in np.unique classNames ]  Our Flowers-17 dataset has the following directory structure:  flowers17 {species} {image}  An example of an image in the dataset follows:  1  2  3  4  5  6  7  8  9  10  11  12  13  14  16  17  18  19  20  22  23  24  25  26  27   22  Chapter 2. Data Augmentation  flowers17 bluebell image_0241.jpg  Therefore, to extract the class labels, we can simply extract the second to last index after splitting on the path separator  Line 26  yielding the text bluebell. If you struggle to see how this path and label extraction works, I would suggest opening a Python shell and playing around with ﬁle paths and path separators. In particular, notice how you can split a string based on the operating system’s path separator and then use Python indexing to extract various parts of the array. Line 27 then determines the unique set of class labels  in this case, 17 total classes  from the image paths.  Given our imagePaths, we can load the Flowers-17 dataset from disk:   initialize the image preprocessors aap = AspectAwarePreprocessor 64, 64  iap = ImageToArrayPreprocessor     load the dataset from disk then scale the raw pixel intensities  to the range [0, 1] sdl = SimpleDatasetLoader preprocessors=[aap, iap]   data, labels  = sdl.load imagePaths, verbose=500  data = data.astype "float"    255.0  Line 30 initializes our AspectAwarePreprocessor such that every image it processes will be 64× 64 pixels. The ImageToArrayPreprocessor is then initialized on Line 31, allowing us to convert images to Keras-compatible arrays. We then instantiate the SimpleDatasetLoader using these two preprocessors, respectively  Line 35 .  The data and corresponding labels are loaded from disk on Line 36. All images in the data  array are then normalized to the range [0,1] by dividing the raw pixel intensities by 255.  Now that our data is loaded we can perform a training and testing split  75 percent for training,  25 percent for testing  along with one-hot encoding our labels:   partition the data into training and testing splits using 75% of  the data for training and the remaining 25% for testing  trainX, testX, trainY, testY  = train_test_split data, labels,  test_size=0.25, random_state=42    convert the labels from integers to vectors trainY = LabelBinarizer  .fit_transform trainY  testY = LabelBinarizer  .fit_transform testY   To train our ﬂower classiﬁer we’ll be using the MiniVGGNet architecture along with the SGD  optimizer:   initialize the optimizer and model print "[INFO] compiling model..."  opt = SGD lr=0.05  model = MiniVGGNet.build width=64, height=64, depth=3,  model.compile loss="categorical_crossentropy", optimizer=opt,  classes=len classNames    metrics=["accuracy"]   29  30  31  32  33  34  35  36  37  39  40  41  42  43  44  45  46  48  49  50  51  52  53  54   55  56  57  58  59  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  2.3 Comparing Training With and Without Data Augmentation  23   train the network print "[INFO] training network..."  H = model.fit trainX, trainY, validation_data= testX, testY ,  batch_size=32, epochs=100, verbose=1   The MiniVGGNet architecture will accept images with spatial dimensions 64× 64× 3  64 pixels wide, 64 pixels tall, and 3 channels . The total number of classes is len classNames  which, in this case, equals seventeen, one for each of the categories in the Flowers-17 dataset.  We’ll train MiniVGGNet using SGD with an initial learning rate of α = 0.05. We’ll purposely leave out learning rate decay so we can demonstrate the affect data augmentation has in the next section. Lines 58 and 59 train MiniVGGNet for a total of 100 epochs.  We then evaluate our network and plot our loss and accuracy over time:   evaluate the network print "[INFO] evaluating network..."  predictions = model.predict testX, batch_size=32  print classification_report testY.argmax axis=1 ,  predictions.argmax axis=1 , target_names=classNames     plot the training loss and accuracy plt.style.use "ggplot"  plt.figure   plt.plot np.arange 0, 100 , H.history["loss"], label="train_loss"  plt.plot np.arange 0, 100 , H.history["val_loss"], label="val_loss"  plt.plot np.arange 0, 100 , H.history["acc"], label="train_acc"  plt.plot np.arange 0, 100 , H.history["val_acc"], label="val_acc"  plt.title "Training Loss and Accuracy"  plt.xlabel "Epoch "  plt.ylabel "Loss Accuracy"  plt.legend   plt.show    To obtain a baseline accuracy on Flowers-17 using MiniVGGNet, just execute the following  command:  $ python minivggnet_flowers17.py --dataset .. flowers17 images [INFO] loading images... [INFO] processed 500 1360 [INFO] processed 1000 1360 [INFO] compiling model... [INFO] training network... Train on 1020 samples, validate on 340 samples Epoch 1 100 ... Epoch 100 100 3s - loss: 0.0030 - acc: 1.0000 - val_loss: 1.7683 - val_acc: 0.6206 [INFO] evaluating network...  precision  recall  f1-score  support  bluebell buttercup coltsfoot  0.48 0.67 0.53  0.67 0.60 0.40  0.56 0.63 0.46  18 20 20   24  Chapter 2. Data Augmentation  cowslip crocus daffodil daisy dandelion fritillary iris lilyvalley pansy snowdrop sunflower tigerlily tulip windflower  avg   total  0.35 0.62 0.43 0.74 0.61 0.72 0.80 0.59 0.82 0.64 0.95 0.88 0.18 0.67  0.64  0.44 0.50 0.33 0.85 0.83 0.82 0.76 0.67 0.74 0.39 0.91 0.74 0.25 0.62  0.62  0.39 0.56 0.38 0.79 0.70 0.77 0.78 0.62 0.78 0.49 0.93 0.80 0.21 0.65  0.62  18 20 27 20 23 22 21 15 19 23 23 19 16 16  340  As we can see from the output, we are able to obtain 64 percent classiﬁcation accuracy, which is fairly reasonable given our limited amount of training data. However, what is concerning is our loss and accuracy plot  Figure  2.5 . As the plot demonstrates, our network quickly starts overﬁtting past epoch 20. The reason for this behavior is because we only have 1,020 training examples with 60 images per class  the other images are used for testing . Keep in mind that we should ideally have anywhere between 1,000-5,000 examples per class when training a Convolutional Neural Network.  Figure 2.5: Learning plot for MiniVGGNet applied to the Flowers-17 dataset without data augmen- tation. Notice how overﬁtting starts to occur past epoch 25 as our validation loss increases.  Furthermore, training accuracy skyrockets past 95% in the ﬁrst few epochs, eventually obtaining 100% accuracy in the later epochs – this output is a clear case of overﬁtting. Due to the lack of substantial training data, MiniVGGNet is modeling the underlying patterns in the training data   2.3 Comparing Training With and Without Data Augmentation  25  too closely and is unable to generalize to the test data. To combat the overﬁtting, we can apply regularization techniques – in the context of this chapter, our regularization method will be data augmentation. In practice, you would also include other forms of regularization  weight decay, dropout, etc.  to further reduce the effects of overﬁtting.  2.3.4 Flowers-17: With Data Augmentation  In this example we are going to apply the example same training process as the previous section only with one addition: we’ll be applying data augmentation. To see how data augmentation can increase our classiﬁcation accuracy while preventing overﬁtting, open up a new ﬁle, name it minivggnet_flowers17_data_aug.py, and let’s get to work:   import the necessary packages from sklearn.preprocessing import LabelBinarizer from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from pyimagesearch.preprocessing import ImageToArrayPreprocessor from pyimagesearch.preprocessing import AspectAwarePreprocessor from pyimagesearch.datasets import SimpleDatasetLoader from pyimagesearch.nn.conv import MiniVGGNet from keras.preprocessing.image import ImageDataGenerator from keras.optimizers import SGD from imutils import paths import matplotlib.pyplot as plt import numpy as np import argparse import os  Our imports are the same as in minivggnet_flowers17.py, with the exception of Line  9 where we import the ImageDataGenerator class used for data augmentation.  Next, let’s parse our command line arguments and extract the class names from the image paths:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-d", "--dataset", required=True,  help="path to input dataset"   args = vars ap.parse_args      grab the list of images that we’ll be describing, then extract  the class label names from the image paths print "[INFO] loading images..."  imagePaths = list paths.list_images args["dataset"]   classNames = [pt.split os.path.sep [-2] for pt in imagePaths] classNames = [str x  for x in np.unique classNames ]  As well as load our dataset from disk, construct our training testing splits, and encode our  labels:   initialize the image preprocessors aap = AspectAwarePreprocessor 64, 64  iap = ImageToArrayPreprocessor    1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  17  18  19  20  21  22  23  24  25  26  27  28  30  31  32  33   26  Chapter 2. Data Augmentation   load the dataset from disk then scale the raw pixel intensities  to the range [0, 1] sdl = SimpleDatasetLoader preprocessors=[aap, iap]   data, labels  = sdl.load imagePaths, verbose=500  data = data.astype "float"    255.0   partition the data into training and testing splits using 75% of  the data for training and the remaining 25% for testing  trainX, testX, trainY, testY  = train_test_split data, labels,  test_size=0.25, random_state=42    convert the labels from integers to vectors trainY = LabelBinarizer  .fit_transform trainY  testY = LabelBinarizer  .fit_transform testY   Our next code block is very important as it initializes our ImageDataGenerator:   construct the image generator for data augmentation aug = ImageDataGenerator rotation_range=30, width_shift_range=0.1,  height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode="nearest"   Here we’ll allow images to be: 1. Randomly rotated ±30 degrees 2. Horizontally and vertically shifted by a factor of 0.2 3. Sheared by 0.2 4. Zoomed by uniformly sampling in the range [0.8, 1.2] 5. Randomly horizontally ﬂipped Depending on your exact dataset you’ll want to tweak these data augmentation values. It’s typical to see rotation ranges between [10,30] depending on your application. Horizontal and vertical shifts normally fall in the range [0.1,0.2]  same goes for zoom values . Unless horizontally ﬂipping your image changes the class label, you should always include horizontal ﬂipping as well.  Just as in the previous experiment we’ll train MiniVGGNet using the SGD optimizer:   initialize the optimizer and model print "[INFO] compiling model..."  opt = SGD lr=0.05  model = MiniVGGNet.build width=64, height=64, depth=3,  model.compile loss="categorical_crossentropy", optimizer=opt,  classes=len classNames    metrics=["accuracy"]   However, the code used to train our network has to change slightly as we are now using an  image generator:   train the network print "[INFO] training network..."  H = model.fit_generator aug.flow trainX, trainY, batch_size=32 ,  validation_data= testX, testY , steps_per_epoch=len trainX     32, epochs=100, verbose=1   34  35  36  37  38  39  40  41  42  43  44  45  46  47  49  50  51  52  54  55  56  57  58  59  60  62  63  64  65  66   2.3 Comparing Training With and Without Data Augmentation  27  Instead of calling the .fit method of model, we now need to call .fit_generator. The ﬁrst parameter to .fit_generator is aug.flow, our data augmentation function used to generate new training samples from the training data. The aug.flow requires us to pass in our training data and corresponding labels. We also need to supply the batch size so the generator can construct appropriate batches when training the network.  We then supply the validation_data as a 2-tuple of  testX, testY  – this data is used for validation at the end of every epoch. The steps_per_epoch parameter controls the number of batches per epoch – we can programmatically determine the proper steps_per_epoch value by dividing the total number of training samples by our batch size and casting it to an integer. Finally, epochs controls the total number of epochs our network should be trained for, in this case, 100 epochs.  After training our network we’ll evaluate it and plot the corresponding accuracy loss plot:  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85   evaluate the network print "[INFO] evaluating network..."  predictions = model.predict testX, batch_size=32  print classification_report testY.argmax axis=1 ,  predictions.argmax axis=1 , target_names=classNames     plot the training loss and accuracy plt.style.use "ggplot"  plt.figure   plt.plot np.arange 0, 100 , H.history["loss"], label="train_loss"  plt.plot np.arange 0, 100 , H.history["val_loss"], label="val_loss"  plt.plot np.arange 0, 100 , H.history["acc"], label="train_acc"  plt.plot np.arange 0, 100 , H.history["val_acc"], label="val_acc"  plt.title "Training Loss and Accuracy"  plt.xlabel "Epoch "  plt.ylabel "Loss Accuracy"  plt.legend   plt.show    Notice how we do not apply data augmentation to the validation data. We only apply data  To train MiniVGGNet on Flowers-17 with data augmentation, just execute the following  augmentation to the training set.  command:  $ python minivggnet_flowers17_data_aug.py --dataset .. flowers17 images [INFO] loading images... [INFO] processed 500 1360 [INFO] processed 1000 1360 [INFO] compiling model... [INFO] training network... Epoch 1 100 3s - loss: 3.4073 - acc: 0.2108 - val_loss: 3.0306 - val_acc: 0.1882 ... Epoch 100 100 3s - loss: 0.2769 - acc: 0.9078 - val_loss: 1.3560 - val_acc: 0.6794 [INFO] evaluating network...  precision  recall  f1-score  support  bluebell  0.67  0.44  0.53  18   28  Chapter 2. Data Augmentation  buttercup coltsfoot cowslip crocus daffodil daisy dandelion fritillary iris lilyvalley pansy snowdrop sunflower tigerlily tulip windflower  avg   total  0.59 0.56 0.45 0.82 0.67 1.00 0.63 0.94 0.78 0.44 1.00 0.54 1.00 0.80 0.22 0.72  0.71  0.80 0.50 0.50 0.45 0.30 0.95 0.96 0.77 0.86 0.73 0.74 0.65 0.96 0.84 0.25 0.81  0.68  0.68 0.53 0.47 0.58 0.41 0.97 0.76 0.85 0.82 0.55 0.85 0.59 0.98 0.82 0.24 0.76  0.68  20 20 18 20 27 20 23 22 21 15 19 23 23 19 16 16  340  Immediately after the network ﬁnishes training, you’ll notice an increase of accuracy from 64% to 71%, a 10.9% improvement from our previous run. However, accuracy isn’t everything – the real question is whether data augmentation has helped prevent overﬁtting. To answer that question, we’ll need to examine the loss and accuracy plot in Figure 2.6.  Figure 2.6: Applying MiniVGGNet to Flowers-17 with data augmentation. Overﬁtting is still a concern; however, we are able to obtain substantially higher classiﬁcation accuracy and lower loss.  While there is still overﬁtting occurring, the effect is signiﬁcantly dampened by using data augmentation. Again, keep in mind that these two experiments are identical – the only changes we made were whether or not data augmentation was applied. As a regularizer, you can also see   2.4 Summary  29  data augmentation having an impact. We were able to increase our validation accuracy, thereby improving the generalizability of our model, despite having lowering training accuracy.  Further accuracy can be obtained by decaying the learning rate over time. Learning rate was speciﬁcally left out of this chapter so we could focus solely on the impact data augmentation as a regularizer has when training Convolutional Neural Networks.  2.4 Summary  Data augmentation is a type of regularization technique that operates on the training data. As the name suggests, data augmentation randomly jitters our training data by applying a series of random translations, rotations, shears, and ﬂips. Applying these simple transformations does not change the class label of the input image; however, each augmented image can be considered a “new” image that the training algorithm has not seen before. Therefore, our training algorithm is being constantly presented with new training samples, allowing it to learn more robust and discriminative patterns. As our results demonstrated, applying data augmentation increased our classiﬁcation accuracy while helping mitigate the effects of overﬁtting. Furthermore, data augmentation also allowed us to train a Convolutional Neural Network on only 60 samples per class, far below the suggested 1,000-5,000 samples per class.  While it’s always better to gather “natural” training samples, in a pinch, data augmentation can be used to overcome small dataset limitations. When it comes to your own experiments, you should apply data augmentation to nearly every experiment you run. There is a slight performance hit you must take due to the fact that the CPU is now responsible for randomly transforming your inputs; however, this performance hit is mitigated by using threading and augmenting your data in the background before it is passed to the thread responsible for training your network.  Again, in nearly all remaining chapters inside the Practitioner Bundle and ImageNet Bun- dle, we’ll be using data augmentation. Take the time to familiarize yourself with this technique now as it will help you obtain better performing deep learning models  using less data  quicker.    3. Networks as Feature Extractors  Over the new few chapters, we’ll be discussing the concept of transfer learning, the ability to use a pre-trained model as a “shortcut” to learn patterns from data it was not originally trained on.  Consider a traditional machine learning scenario where we are given two classiﬁcation chal- lenges. In the ﬁrst challenge, our goal is to train a Convolutional Neural Network to recognize dogs vs. cats in an image  as we’ll do in Chapter 10 .  Then, in the second project, we are tasked with recognizing three separate species of bears: grizzly bears, polar bears, and giant pandas. Using standard practices in machine learning, neural networks, and deep learning, we would treat these these challenges as two separate problems. First, we would gather a sufﬁcient labeled dataset of dogs and cats, followed by training a model on the dataset. We would then repeat the process a second time, only this time, gathering images of our bear breeds, and then training a model on top of the labeled bear dataset.  Transfer learning proposes a different training paradigm – what if we could use an existing pre- trained classiﬁer and use it as a starting point for a new classiﬁcation task? In context of the proposed challenges above, we would ﬁrst train a Convolutional Neural Network to recognize dogs versus cats. Then, we would use the same CNN trained on dog and cat data to be used to distinguish between bear classes, even though no bear data was mixed with the dog and cat data. Does this sound too good to be true? It’s actually not. Deep neural networks trained on large-scale datasets such as ImageNet have demonstrated to be excellent at the task of transfer learning. These networks learn a set of rich, discriminating features to recognize 1,000 separate object classes. It makes sense that these ﬁlters can be reused for classiﬁcation tasks other than what the CNN was originally trained on.  In general, there are two types of transfer learning when applied to deep learning for computer  vision:  1. Treating networks as arbitrary feature extractors. 2. Removing the fully-connected layers of an existing network, placing new FC layer set on top of the CNN, and ﬁne-tuning these weights  and optionally previous layers  to recognize object classes.  In this chapter, we’ll be focusing primarily on the ﬁrst method of transfer learning, treating   32  Chapter 3. Networks as Feature Extractors  networks as feature extractors. We’ll then discuss how to ﬁne-tune the weights of a network to a speciﬁc classiﬁcation task in Chapter 5.  3.1 Extracting Features with a Pre-trained CNN  Up until this point, we have treated Convolutional Neural Networks as end-to-end image classiﬁers:  1. We input an image to the network. 2. The image forward propagates through the network. 3. We obtain the ﬁnal classiﬁcation probabilities from the end of the network. However, there is no “rule” that says we must allow the image to forward propagate through the entire network. Instead, we can stop the propagation at an arbitrary layer, such as an activation or pooling layer, extract the values from the network at this time, and then use them as feature vectors. For example, let’s consider the VGG16 network architecture by Simonyan and Zisserman [11]  Figure 3.1, left .  Figure 3.1: Left: The original VGG16 network architecture that outputs probabilities for each of the 1,000 ImageNet class labels. Right: Removing the FC layers from VGG16 and instead returning the output of the ﬁnal POOL layer. This output will serve as our extracted features.  Along with the layers in the network, we have also included the input and output shapes of the   3.1 Extracting Features with a Pre-trained CNN  33  volumes for each layer. When treating networks as a feature extractor, we essentially “chop off” the network at an arbitrary point  normally prior to the fully-connected layers, but it really depends on your particular dataset . Now the last layer in our network is a max pooling layer  Figure 3.1, right  which will have the output shape of 7× 7× 512 implying there are 512 ﬁlters each of size 7× 7. If we were to forward propagate an image through this network with its FC head removed, we would be left with 512, 7× 7 activations that have either activated or not based on the image contents. Therefore, we can actually take these 7× 7× 512 = 25,088 values and treat them as a feature vector that quantiﬁes the contents of an image.  If we repeat this process for an entire dataset of images  including datasets that VGG16 was not trained on , we’ll be left with a design matrix of N images, each with 25,088 columns used to quantify their contents  i.e., feature vectors . Given our feature vectors, we can train an off-the-shelf machine learning model such a Linear SVM, Logistic Regression classiﬁer, or Random Forest on top of these features to obtain a classiﬁer that recognizes new classes of images.  Keep in mind that the CNN itself is not capable of recognizing these new classes – instead, we are using the CNN as an intermediary feature extractor. The downstream machine learning classiﬁer will take care of learning the underlying patterns of the features extracted from the CNN. Later in this chapter, I’ll be demonstrating how you can use pre-trained CNNs  speciﬁcally VGG16  and the Keras library to obtain > 95% classiﬁcation accuracy on image datasets such as Animals, CALTECH-101, and Flowers-17. Neither of these datasets contain images that VGG16 was trained on, but by applying transfer learning, we are able to build super accurate image classiﬁers with little effort. The trick is extracting these features and storing them in an efﬁcient manner. To accomplish this task, we’ll need HDF5.  HDF5 is binary data format created by the HDF5 group [12] to store gigantic numerical datasets on disk  far too large to store in memory  while facilitating easy access and computation on the rows of the datasets. Data in HDF5 is stored hierarchically, similar to how a ﬁle system stores data. Data is ﬁrst deﬁned in groups, where a group is a container-like structure which can hold datasets and other groups. Once a group has been deﬁned, a dataset can be created within the group. A dataset can be thought of as a multi-dimensional array  i.e., a NumPy array  of a homogeneous data type  integer, ﬂoat, unicode, etc. . An example of an HDF5 ﬁle containing a group with multiple datasets is displayed in Figure 3.2.  HDF5 is written in C; however, by using the h5py module  h5py.org , we can gain access to the underlying C API using the Python programming language. What makes h5py so awesome is the ease of interaction with data. We can store huge amounts of data in our HDF5 dataset and manipulate the data in a NumPy-like fashion. For example, we can use standard Python syntax to access and slice rows from multi-terabyte datasets stored on disk as if they were simple NumPy arrays loaded into memory. Thanks to specialized data structures, these slices and row accesses are lighting quick. When using HDF5 with h5py, you can think of your data as a gigantic NumPy array that is too large to ﬁt into main memory but can still be accessed and manipulated just the same. Perhaps best of all, the HDF5 format is standardized, meaning that datasets stored in HDF5 format are inherently portable and can be accessed by other developers using different programming languages such as C, MATLAB, and Java.  In the rest of this chapter, we’ll be writing a custom Python class that allows us to efﬁciently  accept input data and write it to an HDF5 dataset. This class will then serve two purposes:  1. Facilitate a method for us to apply transfer learning by taking our extracted features from  VGG16 and writing them to an HDF5 dataset in an efﬁcient manner.  2. Allow us to generate HDF5 datasets from raw images to facilitate faster training  Chapter 9 .  3.1.1 What Is HDF5?   34  Chapter 3. Networks as Feature Extractors  Figure 3.2: An example of a HDF5 ﬁle with three datasets. The ﬁrst dataset contains the label_names for CALTECH-101. We then have labels, which maps the each image to its corresponding class label. Finally, the features dataset contains the image quantiﬁcations ex- tracted by the CNN.  If you do not already have HDF5 and h5py installed on your system, please see the supplemen-  tary material for Chapter 6 of the Starter Bundle for instructions to conﬁgure your system.  3.1.2 Writing Features to an HDF5 Dataset  Before we can even think about treating VGG16  or any other CNN  as a feature extractor, we ﬁrst need to develop a bit of infrastructure. In particular, we need to deﬁne a Python class named HDF5DatasetWriter, which as the name suggests, is responsible for taking an input set of NumPy arrays  whether features, raw images, etc.  and writing them to HDF5 format. To do so, create a new sub-module in the pyimagesearch package named io and then place a ﬁle named hdf5datasetwriter.py inside of io:  --- pyimagesearch          --- __init__.py --- callbacks --- io   --- nn --- preprocessing --- utils  --- __init__.py --- hdf5datasetwriter.py   3.1 Extracting Features with a Pre-trained CNN  35  From there, open up hdf5datasetwriter.py and we’ll get to work:  1  2  3   import the necessary packages import h5py import os  We’ll start off easy with our imports. We only need two Python packages to build the func- tionality inside this class – the built-in os module and h5py so we have access to the HDF5 bindings.  From there, let’s deﬁne the constructor:  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  class HDF5DatasetWriter:  def __init__ self, dims, outputPath, dataKey="images",  bufSize=1000 :  check to see if the output path exists, and if so, raise  an exception if os.path.exists outputPath :  raise ValueError "The supplied ‘outputPath‘ already "  "exists and cannot be overwritten. Manually delete " "the file before continuing.", outputPath    open the HDF5 database for writing and create two datasets:  one to store the images features and another to store the  class labels self.db = h5py.File outputPath, "w"  self.data = self.db.create_dataset dataKey, dims,  self.labels = self.db.create_dataset "labels",  dims[0], ,  dtype="float"   dtype="int"    store the buffer size, then initialize the buffer itself  along with the index into the datasets self.bufSize = bufSize self.buffer = {"data": [], "labels": []} self.idx = 0  The constructor to HDF5DatasetWriter accepts four parameters, two of which are optional. The dims parameter controls the dimension or shape of the data we will be storing in the dataset. Think of dims as the .shape of a NumPy array. If we were storing the  ﬂattened  raw pixel intensities of the 28× 28 = 784 MNIST dataset, then dims= 70000, 784  as there are 70,000 examples in MNIST, each with a dimensionality of 784. If we wanted to store the raw CIFAR-10 images, then we would set dims= 60000, 32, 32, 3  as there are 60,000 total images in the CIFAR-10 dataset, each represented by a 32× 32× 3 RGB image. In the context of transfer learning and feature extraction, we’ll be using the VGG16 architecture and taking the outputs after the ﬁnal POOL layer. The output of the ﬁnal POOL layer is 512× 7× 7 which, when ﬂattened, yields a feature vector of length 25,088. Therefore, when using VGG16 for feature extraction, we’ll set dims= N, 25088  where N is the total number of images in our dataset.  The next parameter to the HDF5DatasetWriter constructor is the outputPath – this is the path to where our output HDF5 ﬁle will be stored on disk. The optional dataKey is the name of the dataset that will store the data our algorithm will learn from. We default this value to "images",   36  Chapter 3. Networks as Feature Extractors  since in most cases we’ll be storing raw images in HDF5 format. However, for this example, when we instantiate the HDF5DatasetWriter we’ll set dataKey="features" to indicate that we are storing features extracted from a CNN in the ﬁle.  Finally, bufSize controls the size of our in-memory buffer, which we default to 1,000 feature  vectors images. Once we reach bufSize, we’ll ﬂush the buffer to the HDF5 dataset.  Lines 10-13 then make a check to see if outputPath already exists. If it does, we raise an  error to the end user  as we don’t want to overwrite an existing database .  Line 18 opens the HDF5 ﬁle for writing using the supplied outputPath. Lines 19 and 20 create a dataset with the dataKey name and the supplied dims – this is where we will store our raw images extracted features. Lines 21 and 22 create a second dataset, this one to store the  integer  class labels for each record in the dataset Lines 25-28 then initialize our buffers.  Next, let’s review the add method used to add data to our buffer:  def add self, rows, labels :   add the rows and labels to the buffer self.buffer["data"].extend rows  self.buffer["labels"].extend labels    check to see if the buffer needs to be flushed to disk if len self.buffer["data"]  >= self.bufSize:  self.flush    The add method requires two parameters: the rows that we’ll be adding to the dataset, along with their corresponding class labels. Both the rows and labels are added to their respective buffers on Lines 32 and 33. If the buffer ﬁlls up, we call the flush method to write the buffers to ﬁle and reset them.  Speaking of the flush method, let’s deﬁne the function now:  def flush self :   write the buffers to disk then reset the buffer i = self.idx + len self.buffer["data"]  self.data[self.idx:i] = self.buffer["data"] self.labels[self.idx:i] = self.buffer["labels"] self.idx = i self.buffer = {"data": [], "labels": []}  If we think of our HDF5 dataset as a big NumPy array, then we need to keep track of the current index into the next available row where we can store data  without overwriting existing data  – Line 41 determines the next available row in the matrix. Lines 42 and 43 then apply NumPy array slicing to store the data and labels in the buffers. Line 45 then resets the buffers.  We’ll also deﬁne a handy utility function named storeClassLabels which, if called, will  store the raw string names of the class labels in a separate dataset:  def storeClassLabels self, classLabels :   create a dataset to store the actual class label names,  then store the class labels dt = h5py.special_dtype vlen=unicode  labelSet = self.db.create_dataset "label_names",   len classLabels , , dtype=dt   labelSet[:] = classLabels  30  31  32  33  34  35  36  37  39  40  41  42  43  44  45  47  48  49  50  51  52  53   3.2 The Feature Extraction Process  37  Finally, our last function close will be used to write any data left in the buffers to HDF5 as  well as close the dataset:  def close self :  55  56  57  58  59  60  61  62   check to see if there are any other entries in the buffer  that need to be flushed to disk if len self.buffer["data"]  > 0:  self.flush     close the dataset self.db.close    As you can see, the HDF5DatasetWriter doesn’t have much to do with machine learning or deep learning at all – it’s simply a class used to help us store data in HDF5 format. As you continue in your deep learning career, you’ll notice that much of the initial labor when setting up a new problem is getting the data into a format you can work with. Once you have your data in a format that’s straightforward to manipulate, it becomes substantially easier to apply machine learning and deep learning techniques to your data.  All that said, since the HDF5DatasetWriter class is a utility class non-speciﬁc to deep learning and computer vision, I’ve kept the explanation of the class shorter than the other code examples in this book. If you ﬁnd yourself struggling to understand this class I would suggest you:  1. Finish reading the rest of this chapter so you can understand how we use it in context of  feature extraction.  2. Take the time to educate yourself on some basic Python programming paradigms – I provide  a list of Python programming sources I recommend here: http:  pyimg.co ida57.  3. Take apart this class and implement by hand, piece-by-piece, until you understand what is  going on under the hood.  Now that our HDF5DatasetWriter is implemented, we can move on to actually extracting  features using pre-trained Convolutional Neural Networks.  3.2 The Feature Extraction Process  Let’s deﬁne a Python script that can be used to extract features from an arbitrary image dataset  provided the input dataset follows a speciﬁc directory structure . Open up a new ﬁle, name it extract_features.py, and we’ll get to work:  1  2  3  4  5  6  7  8  9  10  11  12  13   import the necessary packages from keras.applications import VGG16 from keras.applications import imagenet_utils from keras.preprocessing.image import img_to_array from keras.preprocessing.image import load_img from sklearn.preprocessing import LabelEncoder from pyimagesearch.io import HDF5DatasetWriter from imutils import paths import numpy as np import progressbar import argparse import random import os   38  Chapter 3. Networks as Feature Extractors  Lines 2-13 import our required Python packages. Notice how on Line 2 we import the Keras implementation of the pre-trained VGG16 network – this is the architecture we’ll be using as our feature extractor. The LabelEncoder class on Line 6 will be used to convert our class labels from strings to integers. We also import our HDF5DatasetWriter on Line 7 so we can write the features extracted from our CNN to a HDF5 dataset.  One import you haven’t seen yet is progressbar on Line 10. This package has nothing to do with deep learning, but I like to use it for long running tasks as it displays a nicely formatted progress bar to your terminal, as well as provides approximate timings as to when your script will ﬁnish executing:  1  Extracting Features  30%    ETA: 0:00:18  If you do not already have progressbar installed on your system, you can install it via:  $ pip install progressbar  Otherwise, you can simply comment out all lines that use progressbar  the package is only  used for fun, after all .  Let’s move on to our command line arguments:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-d", "--dataset", required=True,  help="path to input dataset"   ap.add_argument "-o", "--output", required=True,  help="path to output HDF5 file"   ap.add_argument "-b", "--batch-size", type=int, default=32,  help="batch size of images to be passed through network"  ap.add_argument "-s", "--buffer-size", type=int, default=1000,  help="size of feature extraction buffer"   args = vars ap.parse_args     Our extract_features.py script will require two command line arguments, followed by two optional ones. The --dataset switch controls the path to our input directory of images that we wish to extract features from. The --output switch determines the path to our output HDF5 data ﬁle.  We can then supply a --batch-size – this is the number of images in a batch that will be passed through VGG16 at a time. A value of 32 is reasonable here, but you can increase it if your machine has sufﬁcient memory. The --buffer-size switch controls the number of extracted features we’ll store in memory before writing the buffer for our HDF5 dataset. Again, if your machine has sufﬁcient memory, you can increase the buffer size.  The next step is to grab our image paths from disk, shufﬂe them, and encode the labels:   store the batch size in a convenience variable bs = args["batch_size"]   grab the list of images that we’ll be describing then randomly  shuffle them to allow for easy training and testing splits via  array slicing during training time  15  16  17  18  19  20  21  22  23  24  25  27  28  29  30  31  32   33  34  35  36  37  38  39  40  41  43  44  45  46  47  48  49  50  51  3.2 The Feature Extraction Process  39  print "[INFO] loading images..."  imagePaths = list paths.list_images args["dataset"]   random.shuffle imagePaths    extract the class labels from the image paths then encode the  labels labels = [p.split os.path.sep [-2] for p in imagePaths] le = LabelEncoder   labels = le.fit_transform labels   On Line 34 we grab our imagePaths, the ﬁlenames for all images in our dataset. We then purposely shufﬂe them on Line 35. Why do we bother with the shufﬂing? Well, keep in mind that in previous examples in this book we computed a training and testing split prior to training our classiﬁer. However, since we’ll be working with datasets too large to ﬁt into memory, we won’t be able to perform this shufﬂe in memory – therefore, we shufﬂe the image paths before we extract the features. Then, at training time, we can compute the 75 percent index into the HDF5 dataset and use that index as the end of the training data and the start of the testing data  this point will become more clear in Section 3.3 below .  Line 39 then extracts the class label names from our ﬁle paths, assuming our ﬁle paths have the  directory structure:  dataset_name {class_label} example.jpg  Provided that our dataset does follow this directory structure  as all examples in this book do , Line 39 splits the path into an array based on the path separator  ‘ ’ on Unix machines and ‘\’ on Windows , and then grabs the second-to-last entry in the array – this operation yields the class label of the particular image. Given the labels, we then encode them as integers on Lines 40 and 41  we’ll perform one-hot encoding during the training process .  We can now load the VGG16 network weights and instantiate our HDF5DatasetWriter:   load the VGG16 network print "[INFO] loading network..."  model = VGG16 weights="imagenet", include_top=False    initialize the HDF5 dataset writer, then store the class label  names in the dataset dataset = HDF5DatasetWriter  len imagePaths , 512 * 7 * 7 ,  args["output"], dataKey="features", bufSize=args["buffer_size"]   dataset.storeClassLabels le.classes_   Line 45 we load the pre-trained VGG16 network from disk; however, notice how we have included the parameter include_top=False – supplying this value indicates that the ﬁnal fully- connected layers should not be included in the architecture. Therefore, when forward propagating an image through the network, we’ll obtain the feature values after the ﬁnal POOL layer rather than the probabilities produced by the softmax classiﬁer in the FC layers.  Lines 49 and 50 instantiates the HDF5DatasetWriter. The ﬁrst parameter is our dimensions of the dataset, where there will be len imagePaths  total images, each with a feature vector of size 512× 7× 7 = 25,088. Line 51 then stores the string names of the class labels according to the label encoder.  Now it’s time to perform the actual feature extraction:   40  Chapter 3. Networks as Feature Extractors   initialize the progress bar widgets = ["Extracting Features: ", progressbar.Percentage  , " ",  progressbar.Bar  , " ", progressbar.ETA  ]  pbar = progressbar.ProgressBar maxval=len imagePaths ,  widgets=widgets .start     loop over the images in patches for i in np.arange 0, len imagePaths , bs :   extract the batch of images and labels, then initialize the  list of actual images that will be passed through the network  for feature extraction batchPaths = imagePaths[i:i + bs] batchLabels = labels[i:i + bs] batchImages = []  Lines 54-57 initialize our progress bar so we can visualize and estimate how long the feature extraction process is going to take. Again, using progressbar is optional, so feel free to comment these lines out.  On Line 60 we start looping over our imagePaths in batches of --batch-size. Lines 64 and 65 extract the image paths and labels for the corresponding batch, while Line 66 initializes a list to store the images about to be loaded and fed into VGG16.  Preparing an image for feature extraction is exactly the same as preparing an image for  classiﬁcation via a CNN:   loop over the images and labels in the current batch for  j, imagePath  in enumerate batchPaths :   load the input image using the Keras helper utility  while ensuring the image is resized to 224x224 pixels image = load_img imagePath, target_size= 224, 224   image = img_to_array image    preprocess the image by  1  expanding the dimensions and   2  subtracting the mean RGB pixel intensity from the  ImageNet dataset image = np.expand_dims image, axis=0  image = imagenet_utils.preprocess_input image    add the image to the batch batchImages.append image   On Line 69 we loop over each image path in the batch. Each image is loaded from disk and converted to a Keras-compatible array  Lines 72 and 73 . We then preprocess the image on Lines 78 and 79, followed by adding it to batchImages  Line 82 .  To obtain our feature vectors for the images in batchImages, all we need to do is call the  .predict method of model:   pass the images through the network and use the outputs as  our actual features batchImages = np.vstack batchImages  features = model.predict batchImages, batch_size=bs   53  54  55  56  57  58  59  60  61  62  63  64  65  66  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  84  85  86  87  88   3.2 The Feature Extraction Process  41  89  90  91  92  93  94  95   reshape the features so that each image is represented by  a flattened feature vector of the ‘MaxPooling2D‘ outputs features = features.reshape  features.shape[0], 512 * 7 * 7     add the features and labels to our HDF5 dataset dataset.add features, batchLabels  pbar.update i   We use the .vstack method of NumPy on Lines 86 to “vertically stack” our images such that  they have the shape  N, 224, 224, 3  where N is the size of the batch.  Passing batchImages through our network yields our actual feature vectors – remember, we chopped off the fully-connected layers at the head of VGG16, so now we are left with the values after the ﬁnal max pooling operation  Line 87 . However, the output of the POOL has the shape  N, 512, 7, 7 , implying there are 512 ﬁlters, each of size 7× 7. To treat these values as a feature vector, we need to ﬂatten them into an array with shape  N, 25088 , which is exactly what Line 91 accomplishes. Line 94 adds our features and batchLabels to our HDF5 dataset.  Our ﬁnal code block handles closing our HDF5 dataset:  97  98  99   close the dataset dataset.close   pbar.finish    CNN from various datasets.  In the remainder of this section, we are going to practice extracting features using a pre-trained  3.2.1 Extracting Features From Animals  The ﬁrst dataset we are going to extract features from using VGG16 is our “Animals” dataset. This dataset consists of 3,000 images, of three classes: dogs, cats, and pandas. To utilize VGG16 to extract features from these images, simply execute the following command:  $ python extract_features.py --dataset .. datasets animals images \  --output .. datasets animals hdf5 features.hdf5  [INFO] loading images... [INFO] loading network... Extracting Features: 100%  Time: 0:00:35  Using my Titan X GPU, I was able to extract features from all 3,000 images in approximately 35 seconds. After the script executes, take a look inside your animals hdf5 directory and you’ll ﬁnd a ﬁle named features.hdf5:  $ ls .. datasets animals hdf5  features.hdf5  To investigate the .hdf5 ﬁle for the Animals dataset, ﬁre up a Python shell:  $ python >>> import h5py >>> p = ".. datasets animals hdf5 features.hdf5" >>> db = h5py.File p  >>> list db.keys    [u’features’, u’label_names’, u’labels’]   42  Chapter 3. Networks as Feature Extractors  Notice how our HDF5 ﬁle has three datasets: features, label_names, and labels. The features dataset is where our actual extracted features are stored. You can examine the shape of this dataset using the following commands:  >>> db["features"].shape  3000, 25088  >>> db["labels"].shape  3000,  >>> db["label_names"].shape  3,   Notice how the .shape is  3000, 25088  – this result implies that each of the 3,000 images in our Animals dataset is quantiﬁed via feature vector with length 25,088  i.e., the values inside VGG16 after the ﬁnal POOL operation . Later in this chapter, we’ll learn how we can train a classiﬁer on these features.  3.2.2 Extracting Features From CALTECH-101  Just as we extracted features from the Animals dataset, we can do the same with CALTECH-101:  $ python extract_features.py --dataset .. datasets caltech-101 images \  --output .. datasets caltech-101 hdf5 features.hdf5  [INFO] loading images... [INFO] loading network... Extracting Features: 100%  Time: 0:01:27  We now have a ﬁle named features.hdf5 in the caltech-101 hdf5 directory:  $ ls .. datasets caltech-101 hdf5  features.hdf5  Examining this ﬁle you’ll see that each of the 8,677 images is represented by a 25,088-dim  feature vector.  3.2.3 Extracting Features From Flowers-17  Finally, let’s apply CNN feature extraction to the Flowers-17 dataset:  $ python extract_features.py --dataset .. datasets flowers17 images \  --output .. datasets flowers17 hdf5 features.hdf5  [INFO] loading images... [INFO] loading network... Extracting Features: 100%  Time: 0:00:19  Examining the features.hdf5 ﬁle for Flowers-17 you’ll see that each of the 1,360 images in  the dataset is quantiﬁed via a 25,088-dim feature vector.   3.3 Training a Classiﬁer on Extracted Features  43  3.3 Training a Classiﬁer on Extracted Features  Now that we’ve used a pre-trained CNN to extract features from a handful of datasets, let’s see how discriminative these features really are, especially given that the VGG16 was trained on ImageNet and not Animals, CALTECH-101, or Flowers-17.  Take second now and venture a guess on how good of a job a simple linear model might do at using these features to classify an image – would you guess better than 60% classiﬁcation accuracy? Does 70% seem unreasonable? Surely 80% is unlikely? And 90% classiﬁcation accuracy would be unfathomable, right?  Let’s ﬁnd out for ourselves. Open up a new ﬁle, name it train_model.py, and insert the  following code:   import the necessary packages from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV from sklearn.metrics import classification_report import argparse import pickle import h5py  Lines 2-7 import our required Python packages. The GridSearchCV class will be used to help us turn the parameters to our LogisticRegression classiﬁer. We’ll be using pickle to serialize our LogisticRegression model to disk after training. Finally, h5py will be used so we can interface with our HDF5 dataset of features.  We can now parse our command line arguments:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-d", "--db", required=True,  help="path HDF5 database"   ap.add_argument "-m", "--model", required=True,  help="path to output model"   ap.add_argument "-j", "--jobs", type=int, default=-1,  help=" of jobs to run when tuning hyperparameters"   args = vars ap.parse_args     Our script requires two command line arguments, followed by a third optional one: 1. --db: The path to our HDF5 dataset containing our extracted features and class labels. 2. --model: Here we supply the path to our output Logistic Regression classiﬁer. 3. --jobs: An optional integer used to specify the number of concurrent jobs when running a  grid search to tune our hyperparameters to the Logistic Regression model.  Let’s open our HDF5 dataset and determine where our training testing split will be:  19  20  21  22  23   open the HDF5 database for reading then determine the index of  the training and testing split, provided that this data was  already shuffled *prior* to writing it to disk db = h5py.File args["db"], "r"  i = int db["labels"].shape[0] * 0.75   1  2  3  4  5  6  7  9  10  11  12  13  14  15  16  17   44  Chapter 3. Networks as Feature Extractors  As I mentioned earlier in this chapter, we purposely shufﬂed our image paths prior to writing the associated images feature vectors to the HDF5 dataset – the reason why becomes clear on Lines 22 and 23.  Given that our dataset is too large to ﬁt into memory, we need an efﬁcient method to determine our training and testing split. Since we know how many entries there are in the HDF5 dataset  and we know we want to use 75% of the data for training and 25% for evaluation , we can simply compute the 75% index i into the database. Any data before the index i is considered training data – anything after i is testing data.  Given our training and testing splits, let’s train our Logistic Regression classiﬁer:  25  26  27  28  29  30  31  32  33  34  35  36  37  38   define the set of parameters that we want to tune then start a  grid search where we evaluate our model for each value of C print "[INFO] tuning hyperparameters..."  params = {"C": [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]} model = GridSearchCV LogisticRegression  , params, cv=3,  n_jobs=args["jobs"]   model.fit db["features"][:i], db["labels"][:i]  print "[INFO] best hyperparameters: {}".format model.best_params_     evaluate the model print "[INFO] evaluating..."  preds = model.predict db["features"][i:]  print classification_report db["labels"][i:], preds,  target_names=db["label_names"]    Lines 28-31 run a grid search over the parameter C, the strictness of the Logistic Regression classiﬁer to determine what the optimal value is. A full detailed review of Logistic Regression is outside the scope of this book, so please see Andrew Ng’s notes for a thorough review of the Logistic Regression classiﬁer [13].  Take note of how we indicate the training data and training labels via array slices:  Again, any data before index i is part of our training set. Once the best hyperparameters are  found, we then evaluate the classiﬁer on the testing data  Lines 36-38 .  Notice here that our testing data and testing labels are accessed via the array slices:  db["features"][:i] db["labels"][:i]  db["features"][i:] db["labels"][i:]  Anything after the index i is part of our testing set. Even though our HDF5 dataset resides on disk  and is too large to ﬁt into memory , we can still treat it as if it was a NumPy array, which is one of the huge advantages of using HDF5 and h5py together for deep learning and machine learning tasks.  Finally, we save our LogisticRegression model to disk and close the database:  40  41   serialize the model to disk print "[INFO] saving model..."    3.3 Training a Classiﬁer on Extracted Features  45  f = open args["model"], "wb"  f.write pickle.dumps model.best_estimator_   f.close    42  43  44  45  46  47   close the database db.close    Also notice how there is no speciﬁc code related to either of the Animals, CALTECH-101, or Flowers-17 datasets – as long as our input dataset of images conforms to the directory structure detailed in Section 3.2 above, we can use both extract_features.py and train_model.py to rapidly build robust image classiﬁers based on features extracted from CNNs. How robust, you ask? Let’s let the results do the talking.  3.3.1 Results on Animals  To train a Logistic Regression classiﬁer on the features extracted via the VGG16 network on the Animals dataset, simply execute the following command:  $ python train_model.py --db .. datasets animals hdf5 features.hdf5 \  --model animals.cpickle  [INFO] tuning hyperparameters... [INFO] best hyperparameters: {’C’: 0.1} [INFO] evaluating...  precision  recall  f1-score  support  cats dogs panda  avg   total  0.96 0.98 0.98  0.98  0.98 0.95 1.00  0.98  0.97 0.97 0.99  0.98  252 253 245  750  Notice here that we are able to reach 98% classiﬁcation accuracy! This number is a massive im-  provement from our previous best of 71% in Chapter 12 of the Starter Bundle.  3.3.2 Results on CALTECH-101  These incredible results continue to the CALTECH-101 dataset as well. Execute this command to evaluate the performance of VGG16 features on CALTECH-101:  $ python train_model.py \  --db .. datasets caltech-101 hdf5 features.hdf5 \ --model caltech101.cpickle [INFO] tuning hyperparameters... [INFO] best hyperparameters: {’C’: 1000.0} [INFO] evaluating...  precision  recall  f1-score  support  Faces Faces_easy Leopards Motorbikes  ...  windsor_chair  1.00 0.98 1.00 1.00  0.92  0.98 1.00 1.00 1.00  0.92  0.99 0.99 1.00 1.00  0.92  114 104 44 197  13   46  Chapter 3. Networks as Feature Extractors  wrench yin_yang  avg   total  0.88 1.00  0.96  0.78 1.00  0.96  0.82 1.00  0.96  9 11  2170  This time we are able to obtain 96% classiﬁcation accuracy on 101 separate object categories  with minimal effort!  3.3.3 Results on Flowers-17  Finally, let’s apply the VGG16 features to the Flowers-17 dataset, where previously we struggled to break 71 percent accuracy, even when using data augmentation:  $ python train_model.py \  --db .. datasets flowers17 hdf5 features.hdf5 \ --model flowers17.cpickle  [INFO] tuning hyperparameters... [INFO] best hyperparameters: {’C’: 0.1} [INFO] evaluating...  precision  recall  f1-score  support  bluebell buttercup coltsfoot cowslip crocus daffodil daisy dandelion fritillary iris lilyvalley pansy snowdrop sunflower tigerlily tulip windflower  avg   total  1.00 0.90 1.00 0.67 0.94 0.94 1.00 1.00 1.00 1.00 0.73 0.95 0.95 0.96 1.00 0.76 1.00  0.93  1.00 0.78 1.00 0.95 1.00 0.77 0.95 1.00 0.96 0.94 0.94 1.00 0.72 1.00 1.00 0.84 0.94  0.92  1.00 0.84 1.00 0.78 0.97 0.85 0.97 1.00 0.98 0.97 0.82 0.98 0.82 0.98 1.00 0.80 0.97  0.92  25 23 20 19 16 22 20 18 23 16 17 20 29 24 12 19 17  340  This time we reach 93% classiﬁcation accuracy, much higher than the 71% before. Clearly, the networks such as VGG are capable of performing transfer learning, encoding their discriminative features into output activations that we can use to train our own custom image classiﬁers.  3.4 Summary  In this chapter, we started to explore transfer learning, the concept of using a pre-trained Convo- lutional Neural Network to classify class labels outside of what it was originally trained on. In general, there are two methods to perform transfer learning when applied to deep learning and computer vision:  1. Treat networks as feature extractors, forward propagating the image until a given layer, and  then taking these activations and treating them as feature vectors.   3.4 Summary  47  2. Fine-tuning networks by adding a brand-new set of fully-connected layers to the head of the network and tuning these FC layers to recognize new classes  while still using the same underlying CONV ﬁlters .  We focused strictly on the feature extraction component of transfer learning in this chapter, demonstrating that deep CNNs such as VGG, Inception, and ResNet are capable of acting as powerful feature extraction machines, even more powerful than hand-designed algorithms such as HOG [14], SIFT [15], and Local Binary Patterns [16], just to name a few. Whenever approaching a new problem with deep learning and Convolutional Neural Networks, always consider if applying feature extraction will obtain reasonable accuracy – if so, you can skip the network training process entirely, saving you a ton of time, effort, and headache.  We’ll go through my optimal pathway to apply deep learning techniques such as feature extraction, ﬁne-tuning, and training from scratch in Chapter 8. Until then, let’s continue studying transfer learning.    4. Understanding rank-1 & rank-5 Accuracies  Before we get too far in our discussion of advanced deep learning topics  such as transfer learning , let’s ﬁrst take a step back and discuss the concept of rank-1, rank-5, and rank-N accuracy. When reading deep learning literature, especially in the computer vision and image classiﬁcation space, you’ll likely encounter the concept of ranked accuracy. For example, nearly all papers that present machine learning methods evaluated on the ImageNet dataset present their results in terms of both rank-1 and rank-5 accuracy  we’ll ﬁnd out why both rank-1 and rank-5 accuracy are reported later in this chapter .  What exactly is rank-1 and rank-5 accuracy? And how do they differ from the traditional accuracy  i.e., precision ? In this chapter, we’ll discuss ranked accuracy, learn how to implement it, and then apply it to machine learning models trained on the Flowers-17 and CALTECH-101 datasets.  4.1 Ranked Accuracy  Figure 4.1: Left: An input image of a frog that our neural network will try to classify. Right: An input image of a car.  Ranked accuracy is best explained in terms of an example. Let’s suppose we are are evaluating a neural network trained on the CIFAR-10 dataset which includes ten classes: airplane, automobile,   50  Chapter 4. Understanding rank-1 & rank-5 Accuracies  Class Label Probability Airplane Automobile Bird Cat Deer Dog Frog Horse Ship Truck  0.0% 0.0% 2.1% 0.03% 0.01% 0.56% 97.3% 0.0% 0.0% 0.0%  Class Label Probability Airplane Automobile Bird Cat Deer Dog Frog Horse Ship Truck  1.1% 38.7% 0.0% 0.5% 0.0% 0.4% 0.11% 1.4% 2.39% 55.4%  Table 4.1: Left: Class label probabilities returned by our neural network for Figure 4.1  left . Right: Class label probabilities returned by our network for Figure 4.1  right .  bird, cat, deer, dog, frog, horse, ship, and truck. Given the following input image  Figure 4.1, left  we ask our neural network to compute the probabilities for each class label – the neural network then returns the class label probabilities listed in Table 4.1  left .  The class label with the largest probability is frog  97.3%  which is indeed the correct prediction.  If we were to repeat this process of:  1. Step 1: Computing the class label probabilities for each input image in the dataset. 2. Step 2: Determining if the ground-truth label is equal to the predicted class label with the  largest probability.  3. Step 3: Tallying the number of times where Step 2 is true. We would arrive at our rank-1 accuracy. Rank-1 accuracy is, therefore, the percentage of predictions where the top prediction matches the ground-truth label – this is the “standard” type of accuracy we are used to computing: take the total number of correct predictions and divide it by the number of data points in the dataset.  We can then extend this concept to rank-5 accuracy. Instead of caring only about the number  one prediction, we care about the top-5 predictions. Our evaluation process now becomes:  1. Step 1: Compute the class label probabilities for each input image in the dataset. 2. Step 2: Sort the predicted class label probabilities in descending order, such that labels  with higher probability are placed at the front of the list.  3. Step 3: Determine if the ground-truth label exists in the top-5 predicted labels from Step  2.  4. Step 4: Tally the number of times where Step 3 is true. Rank-5 is simply an extension to rank-1 accuracy: instead of caring about only the 1 prediction from the classiﬁer, we’ll take into account the top-5 predictions from the network. For example, let’s again consider an input image that is to be categorized into a CIFAR-10 category based on an arbitrary neural network  Figure 4.1, right . After being passed through our network, we obtain the class label probabilities detailed in Table 4.1  right .  Our image is clearly of a car; however, our network has reported truck as the top prediction – this would be considered an incorrect prediction for rank-1 accuracy. But if we examine the top-5 predictions by the network, we see that automobile is actually the number two prediction, which would be accurate when computing rank-5 accuracy. This approach can easily be extended to arbitrary rank-N accuracy as well; however, we normally only compute rank-1 and rank-5 accuracies – which raises the question, why bother computing rank-5 accuracy at all?  For the CIFAR-10 dataset, computing the rank-5 accuracy is a bit silly, but for large, challenging   4.1 Ranked Accuracy  51  datasets, especially for ﬁne-grained classiﬁcation, it’s often helpful to look at the top-5 predictions from a given CNN. Perhaps the best example of why we compute rank-1 and rank-5 accuracy can be found in Szegedy et al. [17] where we can see a Siberian husky on the left and an Eskimo dog on the right  Figure 4.2 . Most humans would fail to recognize the difference between the two animals; however, both of these classes are valid labels in the ImageNet dataset.  Figure 4.2: Left: Siberian husky. Right: Eskimo dog.  When working with large datasets that cover many class labels with similar characteristics, we often examine the rank-5 accuracy as an extension to the rank-1 accuracy to see how our network is performing. In an ideal world our rank-1 accuracy would increase at the same rate as our rank-5 accuracy, but on challenging datasets, this is not always the case.  Therefore, we examine the rank-5 accuracy as well to ensure that our network is still “learning” in later epochs. It may be the case where rank-1 accuracy stagnates towards the end of training, but rank-5 accuracy continues to improve as our network learns more discriminating features  but not discriminative enough to overtake the top 1 predictions . Finally, depending on the image classiﬁcation challenge  ImageNet being the canonical example , you are required to report both your rank-1 and rank-5 accuracies together.  4.1.1 Measuring rank-1 and rank-5 Accuracies  Computing rank-1 and rank-5 accuracy can be accomplished by building a simple utility function. Inside our pyimagesearch module we’ll add this functionality to the utils sub-module by adding a ﬁle named ranked.py:  --- pyimagesearch           --- __init__.py --- callbacks --- io --- nn --- preprocessing --- utils     --- __init__.py --- captchahelper.py --- ranked.py  Open up ranked.py and we’ll deﬁne the rank5_accuracy function:  1  2  3  4   import the necessary packages import numpy as np  def rank5_accuracy preds, labels :   52  5  6  7  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  25  26  27  28  29  30  Chapter 4. Understanding rank-1 & rank-5 Accuracies   initialize the rank-1 and rank-5 accuracies rank1 = 0 rank5 = 0  with each class label T .  Line 4 deﬁnes our rank5_accuracy function. This method accepts two parameters:   preds: An N × T matrix where N, the number of rows, contains the probabilities associated   labels: The ground-truth labels for the images in the dataset. We then initialize the rank-1 and rank-5 accuracies on Lines 6 and 7, respectively. Let’s go ahead and compute the rank-1 and rank-5 accuracies:   loop over the predictions and ground-truth labels for  p, gt  in zip preds, labels :   sort the probabilities by their index in descending  order so that the more confident guesses are at the  front of the list p = np.argsort p [::-1]   check if the ground-truth label is in the top-5  predictions if gt in p[:5]: rank5 += 1   check to see if the ground-truth is the 1 prediction if gt == p[0]: rank1 += 1  On Line 10 we start looping over the predictions and ground-truth class labels for each example in the dataset. Line 14 sorts the probabilities of the predictions p in descending order, such that the indices of the largest probabilities are placed at the front of the list. If the ground-truth label exists in the top-5 predictions, we increment our rank5 variable  Lines 18 and 19 . If the ground-truth label is equal to the number one position, we increment our rank1 variable  Lines 22 and 23 .  Our ﬁnal code block handles converting rank1 and rank5 to percentages by dividing by the  total number of labels:   compute the final rank-1 and rank-5 accuracies rank1  = float len labels   rank5  = float len labels     return a tuple of the rank-1 and rank-5 accuracies return  rank1, rank5   Line 30 returns a 2-tuple of the rank-1 and rank-5 accuracies to the calling function.  4.1.2 Implementing Ranked Accuracy  To demonstrate how to compute rank-1 and rank-5 accuracy for a dataset, let’s go back to Chapter 3 where we used a pre-trained Convolutional Neural Network on the ImageNet dataset as a feature extractor. Based on these extracted features we trained a Logistic Regression classiﬁer on the data and evaluated the model. We’ll now extend our accuracy reports to include rank-5 accuracy as well. While we are computing rank-1 and rank-5 accuracy for our Logistic Regression model, keep in mind that both rank-1 and rank-5 accuracy can be computed for any machine learning, neural   4.1 Ranked Accuracy  53  network, or deep learning model – it is common to run into both of these metrics outside of the deep learning community. With all that said, open up a new ﬁle, name it rank_accuracy.py, and insert the following code:  1  2  3  4  5   import the necessary packages from pyimagesearch.utils.ranked import rank5_accuracy import argparse import pickle import h5py  Lines 2-5 import our required Python packages. We’ll be using our newly deﬁned rank5_accuracy  function to compute the rank-1 and rank-5 accuracies of our predictions, respectively. The pickle package is used to load our pre-trained scikit-learn classiﬁer from disk. Finally, h5py will be used to interface with our HDF5 database of features extracted from our CNN in Chapter 3.  The next step is to parse our command line arguments:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-d", "--db", required=True,  help="path HDF5 database"   ap.add_argument "-m", "--model", required=True,  help="path to pre-trained model"   args = vars ap.parse_args     Our script will require two arguments: --db, which is the path to our HDF5 database of  extracted features sand --model, the path to our pre-trained Logistic Regression classiﬁer.  The next code block handles loading the pre-trained model from disk as well as determining the index of the training and testing split into the HDF5 dataset, assuming that 75% of the data was used for training and 25% for testing:   load the pre-trained model print "[INFO] loading pre-trained model..."  model = pickle.loads open args["model"], "rb" .read      open the HDF5 database for reading then determine the index of  the training and testing split, provided that this data was  already shuffled *prior* to writing it to disk db = h5py.File args["db"], "r"  i = int db["labels"].shape[0] * 0.75   Finally, let’s compute our rank-1 and rank-5 accuracies:   make predictions on the testing set then compute the rank-1  and rank-5 accuracies print "[INFO] predicting..."  preds = model.predict_proba db["features"][i:]   rank1, rank5  = rank5_accuracy preds, db["labels"][i:]    display the rank-1 and rank-5 accuracies print "[INFO] rank-1: {:.2f}%".format rank1 * 100    7  8  9  10  11  12  13  15  16  17  18  19  20  21  22  23  25  26  27  28  29  30  31  32   Chapter 4. Understanding rank-1 & rank-5 Accuracies  print "[INFO] rank-5: {:.2f}%".format rank5 * 100    54  33  34  35  36   close the database db.close    Line 28 computes the probabilities for each class label for every data point in the testing set. Based on the predicted probabilities and the ground-truth labels of the testing data, we can compute the ranked accuracies on Line 29. Lines 32 and 33 then display the rank-1 and rank-5 to our terminal, respectively.  Please take note that we have coded this example such that it will work with any example from Chapter 3 where we extracted features from a CNN and then trained a scikit-learn model on top of the features. Later in the Practitioner Bundle and ImageNet Bundle, we’ll compute the rank-1 and rank-5 accuracies for Convolutional Neural Networks trained from scratch as well.  4.1.3 Ranked Accuracy on Flowers-17  To start, let’s compute the rank-1 and rank-5 accuracy for the Flowers-17 dataset:  $ python rank_accuracy.py --db .. datasets flowers17 hdf5 features.hdf5 \  --model .. chapter03-feature_extraction flowers17.cpickle  [INFO] loading pre-trained model... [INFO] predicting... [INFO] rank-1: 92.06% [INFO] rank-5: 99.41%  On the Flowers-17 dataset, we obtain 92.06% rank-1 accuracy using a Logistic Regression classiﬁer trained on features extracted from the VGG16 architecture. Examining the rank-5 accuracy we see that our classiﬁer is nearly perfect, obtaining 99.41% rank-5 accuracy.  4.1.4 Ranked Accuracy on CALTECH-101  Let’s try another example, this one on the larger CALTECH-101 dataset:  $ python rank_accuracy.py --db .. datasets caltech101 hdf5 features.hdf5 \  --model .. chapter03-feature_extraction caltech101.cpickle  [INFO] loading pre-trained model... [INFO] predicting... [INFO] rank-1: 95.58% [INFO] rank-5: 99.45%  Here we obtain 95.58% rank-1 accuracy and 99.45% rank-5 accuracy, a substantial improve- ment from previous computer vision and machine learning techniques that struggled to break 60% classiﬁcation accuracy.  4.2 Summary  In this chapter, we reviewed the concept of rank-1 and rank-5 accuracy. Rank-1 accuracy is the number of times our ground-truth label equals our class label with the largest probability. Rank-5 accuracy extends on rank-1 accuracy, allowing it to be a bit more “lenient” – here we compute rank-5 accuracy as the number of times our ground-truth label appears in the top-5 predicted class labels with the largest probability.   4.2 Summary  55  We typically report rank-5 accuracy on large, challenging datasets such as ImageNet where it is often hard for even humans to correctly label the image. In this case, we’ll consider a prediction for our model to be “correct” if the ground-truth label simply exists in its top-5 predictions. As we discussed in Chapter 9 of the Starter Bundle, a network that is truly generalizing well will produce contextually similar predictions in its top-5 probabilities.  Finally, keep in mind that rank-1 and rank-5 accuracy are not speciﬁc to deep learning and  image classiﬁcation – you will often see these metrics in other classiﬁcation tasks as well.    5. Fine-tuning Networks  In Chapter 3 we learned how to treat a pre-trained Convolutional Neural Network as feature extractor. Using this feature extractor, we forward propagated our dataset of images through the network, extracted the activations at a given layer, and saved the values to disk. A standard machine learning classiﬁer  in this case, Logistic Regression  was then trained on top of the CNN features, exactly as we would do if we were using hand-engineered features such as SIFT [15], HOG [14], LBPs [16], etc. This CNN feature extractor approach, called transfer learning, obtained remarkable accuracy, far higher than any of our previous experiments on the Animals, CALTECH-101, or Flowers-17 dataset.  But there is another type of transfer learning, one that can actually outperform the feature extraction method if you have sufﬁcient data. This method is called ﬁne-tuning and requires us to perform “network surgery”. First, we take a scalpel and cut off the ﬁnal set of fully-connected layers  i.e., the “head” of the network  from a pre-trained Convolutional Neural Network, such as VGG, ResNet, or Inception. We then replace the head with a new set of fully-connected layers with random initializations. From there all layers below the head are frozen so their weights cannot be updated  i.e., the backward pass in backpropagation does not reach them .  We then train the network using a very small learning rate so the new set of FC layers can start to learn patterns from the previously learned CONV layers earlier in the network. Optionally, we may unfreeze the rest of the network and continue training. Applying ﬁne-tuning allows us to apply pre-trained networks to recognize classes that they were not originally trained on; furthermore, this method can lead to higher accuracy than feature extraction.  In the remainder of this chapter we’ll discuss the ﬁne-tuning in more detail, including network surgery. We’ll end by providing an example of applying ﬁne-tuning to the Flowers-17 dataset and outperforming all other approaches we’ve tried in this book thus far.  5.1 Transfer Learning and Fine-tuning  Fine-tuning is a type of transfer learning. We apply ﬁne-tuning to deep learning models that have already been trained on a given dataset. Typically, these networks are state-of-the-art architectures such as VGG, ResNet, and Inception that have been trained on the ImageNet dataset.   58  Chapter 5. Fine-tuning Networks  As we found out in Chapter 3 on feature extraction, these networks contain rich, discriminative ﬁlters that can be used on datasets and class labels outside the ones they have already been trained on. However, instead of simply applying feature extraction, we are going to perform network surgery and modify the actual architecture so we can re-train parts of the network.  If this sounds like something out of a bad horror movie; don’t worry, there won’t be any blood and gore – but we will have some fun and learn a lot with our experiments. To understand how ﬁne- tuning works, consider Figure 5.1  left  where we have the layers of the VGG16 network. As we know, the ﬁnal set of layers  i.e., the “head”  are our fully-connected layers along with our softmax classiﬁer. When performing ﬁne-tuning, we actually remove the head from the network, just as in feature extraction  middle . However, unlike feature extraction, when we perform ﬁne-tuning we actually build a new fully-connected head and place it on top of the original architecture  right .  Figure 5.1: Left: The original VGG16 network architecture. Middle: Removing the FC layers from VGG16 and treating the ﬁnal POOL layer as a feature extractor. Right: Removing the original FC layers and replacing them with a brand new FC head. These new FC layers can then be ﬁne-tuned to the speciﬁc dataset  the old FC layers are no longer used .  In most cases your new FC head will have fewer parameters than the original one; however, that really depends on your particular dataset. The new FC head is randomly initialized  just like any other layer in a new network  and connected to the body of the original network, and we are ready to train.  However, there is a problem – our CONV layers have already learned rich, discriminating ﬁlters while our FC layers are brand new and totally random. If we allow the gradient to backpropagate from these random values all the way through the body of our network, we risk destroying these   5.1 Transfer Learning and Fine-tuning  59  powerful features. To circumvent this, we instead let our FC head “warm up” by  ironically  “freezing” all layers in the body of the network  I told you the cadaver analogy works well here  as in Figure 5.2  left .  Figure 5.2: Left: When we start the ﬁne-tuning process we freeze all CONV layers in the network and only allow the gradient to backpropagate through the FC layers. Doing this allows our network to “warm up”. Right: After the FC layers have had a chance to warm up we may choose to unfreeze all layers in the network and allow each of them to be ﬁne-tuned as well.  Training data is forward propagated through the network as we normally would; however, the backpropagation is stopped after the FC layers, which allows these layers to start to learn patterns from the highly discriminative CONV layers. In some cases, we may never unfreeze the body of the network as our new FC head may obtain sufﬁcient accuracy. However, for some datasets it is often advantageous to allow the original CONV layers to be modiﬁed during the ﬁne-tuning process as well  Figure 5.2, right .  After the FC head has started to learn patterns in our dataset, pause training, unfreeze the body, and then continue the training, but with a very small learning rate – we do not want to deviate our CONV ﬁlters dramatically. Training is then allowed to continue until sufﬁcient accuracy is obtained. Fine-tuning is a super powerful method to obtain image classiﬁers from pre-trained CNNs on custom datasets, even more powerful than feature extraction in most cases. The downside is that ﬁne-tuning can require a bit more work and your choice in FC head parameters does play a big part in network accuracy – you can’t rely strictly on regularization techniques here as your network has already been pre-trained and you can’t deviate from the regularization already being performed by the network.  Secondly, for small datasets, it can be challenging to get your network to start “learning” from   60  Chapter 5. Fine-tuning Networks  a “cold” FC start, which is why we freeze the body of the network ﬁrst. Even still, getting past the warm-up stage can be a bit of a challenge and might require you to use optimizers other than SGD  covered in Chapter 7 . While ﬁne-tuning does require a bit more effort, if it is done correctly, you’ll nearly always enjoy higher accuracy.  5.1.1 Indexes and Layers  Prior to performing network surgery, we need to know the layer name and index of every layer in a given deep learning model. We need this information as we’ll be required to “freeze” and “unfreeze” certain layers in a pre-trained CNN. Without knowing the layer names and indexes ahead of time, we would be “cutting blindly”, an out-of-control surgeon with no game plan. If we instead take a few minutes to examine the network architecture and implementation, we can better prepare for our surgery.  Let’s go ahead and take a look at the layer names and indexes in VGG16. Open up a new ﬁle,  name it inspect_model.py, and insert the following code:  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20   import the necessary packages from keras.applications import VGG16 import argparse   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-i", "--include-top", type=int, default=1,  help="whether or not to include top of CNN"   args = vars ap.parse_args      load the VGG16 network print "[INFO] loading network..."  model = VGG16 weights="imagenet",  include_top=args["include_top"] > 0   print "[INFO] showing layers..."    loop over the layers in the network and display them to the  console for  i, layer  in enumerate model.layers :  print "[INFO] {}\t{}".format i, layer.__class__.__name__    Lines 2 imports our VGG16 Keras implementation, the network we’ll be examining and prepping for surgery. Lines 6-9 parse our command line arguments. A single switch is needed here --include-top, which is used to indicate if the head of the network should be included in the model summary.  Lines 23 and 24 load VGG16 with pre-trained ImageNet weights from disk – the head of the  network is optionally included. Finally, Lines 19 and 20 allow us to investigate our model.  For each layer in the network, we print its corresponding index, i. Given this information, we’ll know the index of where the FC head starts  and where to replace it with our new FC head .  To investigate the VGG16 architecture, just execute the following command:  $ python inspect_model.py [INFO] showing layers... [INFO] 0 InputLayer Conv2D [INFO] 1 [INFO] 2 Conv2D   5.1 Transfer Learning and Fine-tuning  61  Here we can see that Layers 20-22 are our fully-connected layers. To verify, let’s re-execute the inspect_model.py script, this time supplying the switch --include-top -1 which will leave off the FC head:  [INFO] 3 [INFO] 4 [INFO] 5 [INFO] 6 [INFO] 7 [INFO] 8 [INFO] 9 [INFO] 10 [INFO] 11 [INFO] 12 [INFO] 13 [INFO] 14 [INFO] 15 [INFO] 16 [INFO] 17 [INFO] 18 [INFO] 19 [INFO] 20 [INFO] 21 [INFO] 22  MaxPooling2D Conv2D Conv2D MaxPooling2D Conv2D Conv2D Conv2D MaxPooling2D Conv2D Conv2D Conv2D MaxPooling2D Conv2D Conv2D Conv2D MaxPooling2D Flatten Dense Dense Dense  $ python inspect_model.py --include-top -1 [INFO] showing layers... [INFO] 0 InputLayer Conv2D [INFO] 1 Conv2D [INFO] 2 MaxPooling2D [INFO] 3 [INFO] 4 Conv2D Conv2D [INFO] 5 MaxPooling2D [INFO] 6 Conv2D [INFO] 7 [INFO] 8 Conv2D Conv2D [INFO] 9 MaxPooling2D [INFO] 10 Conv2D [INFO] 11 Conv2D [INFO] 12 [INFO] 13 Conv2D MaxPooling2D [INFO] 14 Conv2D [INFO] 15 Conv2D [INFO] 16 [INFO] 17 Conv2D MaxPooling2D [INFO] 18  Notice how the ﬁnal layer in the network is now the POOL layer  just like in Chapter 3 on feature  extraction . This body of the network will serve as a starting point for ﬁne-tuning.  5.1.2 Network Surgery  Before we can replace the head of a pre-trained CNN, we need something to replace it with – therefore, we need to deﬁne our own fully-connected head of the network. To start, create a new ﬁle named fcheadnet.py in the nn.conv sub-module of pyimagesearch:   62  Chapter 5. Fine-tuning Networks  --- pyimagesearch               --- __init__.py --- callbacks --- io --- nn        --- preprocessing --- utils  --- __init__.py --- conv       --- __init__.py --- lenet.py --- minivggnet.py --- fcheadnet.py --- shallownet.py  Then, open up fcheadnet.py and insert the following code:  1  2  3  4   import the necessary packages from keras.layers.core import Dropout from keras.layers.core import Flatten from keras.layers.core import Dense  Lines 2-4 import our required Python packages. As you can see, these three packages are  typically only used for fully-connected networks  the exception being the dropout layer .  Next, let’s deﬁne the FCHeadNet class:  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  class FCHeadNet:  @staticmethod def build baseModel, classes, D :   initialize the head model that will be placed on top of  the base, then add a FC layer headModel = baseModel.output headModel = Flatten name="flatten"  headModel  headModel = Dense D, activation="relu"  headModel  headModel = Dropout 0.5  headModel    add a softmax layer headModel = Dense classes, activation="softmax"  headModel    return the model return headModel  Just as in our previous network implementations, we deﬁne the build method responsible for constructing the actual network architecture. This method requires three parameters: the baseModel  the body of the network , the total number of classes in our dataset, and ﬁnally D, the number of nodes in the fully-connected layer.  Line 11 initializes the headModel which is responsible for connecting our network with the rest of the body, baseModel.output. From there Lines 12-17 build a very simple fully-connected architecture of:  INPUT => FC => RELU => DO => FC => SOFTMAX   5.1 Transfer Learning and Fine-tuning  63  Again, this fully-connected head is very simplistic compared to the original head from VGG16 which consists of two sets of 4,096 FC layers. However, for most ﬁne-tuning problems you are not seeking to replicate the original head of the network, but rather simplify it so it is easier to ﬁne-tune – the fewer parameters in the head, the more likely we’ll be to correctly tune the network to a new classiﬁcation task. Finally, Line 20 returns the newly constructed FC head to the calling function. As we’ll see in the next section, we’ll be replacing the head of VGG16 with our newly deﬁned  FCHeadNet via network surgery.  5.1.3 Fine-tuning, from Start to Finish  It is now time to apply ﬁne-tuning from start to ﬁnish. Open up a new ﬁle, name it finetune_flowers17.py, and insert the following code:  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18   import the necessary packages from sklearn.preprocessing import LabelBinarizer from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from pyimagesearch.preprocessing import ImageToArrayPreprocessor from pyimagesearch.preprocessing import AspectAwarePreprocessor from pyimagesearch.datasets import SimpleDatasetLoader from pyimagesearch.nn.conv import FCHeadNet from keras.preprocessing.image import ImageDataGenerator from keras.optimizers import RMSprop from keras.optimizers import SGD from keras.applications import VGG16 from keras.layers import Input from keras.models import Model from imutils import paths import numpy as np import argparse import os  Lines 2-18 require importing our Python packages, more packages that we have seen before in our previous examples  although many of them we are already familiar with . Lines 5-7 import our image preprocessors along with a our dataset load. Line 8 imports our newly deﬁned FCHeadNet to replace the head of VGG16  Line 12 . Importing the ImageDataGenerator class on Line 9 implies that we’ll be applying data augmentation to our dataset.  Lines 10 and 11 import our optimizers required for our network to actually learn patterns from the input data. We’re already quite familiar with SGD, but we haven’t yet covered RMSprop – we’ll save a discussion on advanced optimization techniques until Chapter 7, but for the time being simply understand that RMSprop is frequently used in situations where we need to quickly obtain reasonable performance  as is the case when we are trying to “warm up” a set of FC layers .  Lines 13 and 14 import two classes required when applying ﬁne-tuning with Keras – Input  and Model. We’ll need both of these when performing network surgery.  Let’s go ahead and parse our command line arguments:  20  21  22  23  24   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-d", "--dataset", required=True,  help="path to input dataset"   ap.add_argument "-m", "--model", required=True,   64  25  26  help="path to output model"   args = vars ap.parse_args     Chapter 5. Fine-tuning Networks  We’ll require two command line arguments for our script, --dataset, the path to the input directory containing the Flowers-17 dataset, and --model, the path to our output serialized weights after training.  We can also initialize ImageDataGenerator, responsible for performing data augmentation  when training our network:   construct the image generator for data augmentation aug = ImageDataGenerator rotation_range=30, width_shift_range=0.1,  height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode="nearest"   As I mentioned in Chapter 2, in nearly all cases you should be applying data augmentation as it rarely hurts accuracy and often helps increase it and avoid overﬁtting. The same is especially true for ﬁne-tuning when we might not have enough data to train a deep CNN from scratch.  The next code block handles grabbing the imagePaths from disk along with parsing the  classNames from the ﬁle paths:   grab the list of images that we’ll be describing, then extract  the class label names from the image paths print "[INFO] loading images..."  imagePaths = list paths.list_images args["dataset"]   classNames = [pt.split os.path.sep [-2] for pt in imagePaths] classNames = [str x  for x in np.unique classNames ]  Again, we make the assumption that our input dataset has the following directory structure:  dataset_name {class_name} example.jpg  Therefore, we can use the path separator to easily  and conveniently  extract the class label  from the ﬁle path.  We are now ready to load our image dataset from disk:   initialize the image preprocessors aap = AspectAwarePreprocessor 224, 224  iap = ImageToArrayPreprocessor     load the dataset from disk then scale the raw pixel intensities to  the range [0, 1] sdl = SimpleDatasetLoader preprocessors=[aap, iap]   data, labels  = sdl.load imagePaths, verbose=500  data = data.astype "float"    255.0  Lines 41 and 42 initialize our image preprocessors. We’ll be resizing all input images to 224× 224 pixels  maintaining the original aspect ratio of the image , the required input size for the VGG16 network. Lines 46 and 47 then apply our image preprocessors to load the data and labels from disk.  Next, let’s create our training and testing splits  75% of the data for training, 25% for testing   and one-hot encode the labels:  28  29  30  31  33  34  35  36  37  38  40  41  42  43  44  45  46  47  48   5.1 Transfer Learning and Fine-tuning  65   partition the data into training and testing splits using 75% of  the data for training and the remaining 25% for testing  trainX, testX, trainY, testY  = train_test_split data, labels,  test_size=0.25, random_state=42    convert the labels from integers to vectors trainY = LabelBinarizer  .fit_transform trainY  testY = LabelBinarizer  .fit_transform testY   Here comes the fun part – performing network surgery:   load the VGG16 network, ensuring the head FC layer sets are left  off baseModel = VGG16 weights="imagenet", include_top=False,  input_tensor=Input shape= 224, 224, 3      initialize the new head of the network, a set of FC layers  followed by a softmax classifier headModel = FCHeadNet.build baseModel, len classNames , 256    place the head FC model on top of the base model -- this will  become the actual model we will train model = Model inputs=baseModel.input, outputs=headModel   Lines 61 and 62 load the VGG16 architecture from disk using the supplied, pre-trained ImageNet weights. We purposely leave off the head of VGG16 as we’ll be replacing it with our own FCHeadNet. We also want to explicitly deﬁne the input_tensor to be 224× 224× 3 pixels  again, assuming channeling ordering  otherwise we’ll run into errors when trying to train our network as the shapes of the volumes will not match up.  Line 66 instantiates the FCHeadNet using the baseModel body as input, len classNames  as the total number of class labels  17 in the case of Flowers-17 , along with 256 nodes in the FC layer.  The actual “surgery” is performed on Line 70 where we construct a new model using the body of VGG16  baseModel.input  as the input and the headModel as the output. However, we’re not ready to train our network yet – keep in mind that earlier in this chapter, I mentioned we need to freeze the weights in the body so they are not updated during the backpropagation phase.  We can accomplish this freezing by setting the .trainable parameter to False for every layer  in baseModel:   loop over all layers in the base model and freeze them so they  will *not* be updated during the training process for layer in baseModel.layers:  layer.trainable = False  Now that we’ve connected the head to the body and frozen the layers in the body, we can warm  up the new head of the network:   compile our model  this needs to be done after our setting our  layers to being non-trainable print "[INFO] compiling model..."   50  51  52  53  54  55  56  57  59  60  61  62  63  64  65  66  67  68  69  70  72  73  74  75  77  78  79   66  80  81  82  83  84  85  86  87  88  89  90  91  93  94  95  96  97  99  100  101  102  Chapter 5. Fine-tuning Networks  opt = RMSprop lr=0.001  model.compile loss="categorical_crossentropy", optimizer=opt,  metrics=["accuracy"]    train the head of the network for a few epochs  all other  layers are frozen  -- this will allow the new FC layers to  start to become initialized with actual "learned" values  versus pure random print "[INFO] training head..."  model.fit_generator aug.flow trainX, trainY, batch_size=32 ,  validation_data= testX, testY , epochs=25, steps_per_epoch=len trainX     32, verbose=1   Line 80 initializes the RMSprop optimizer, an algorithm we’ll discuss more in Chapter 7. Notice how we are using a small learning rate of 1e− 3 to warm up the FC head. When applying ﬁne-tuning you’ll nearly always use a learning rate that is one, if not multiple, orders of magnitude smaller than the original learning rate used to train the network.  Lines 88-91 then train our new FC head using our data augmentation method. Again, keep in mind that while each image is being fully forward propagated, the gradients are only being partially backpropagated – the backpropagation ends after the FC layers, as our goal here is to only “warm-up” the head and not change the weights in the body of the network. Here we allow the warm-up phase to train for 25 epochs. Typically you’ll allow your own FC head to warmup for 10-30 epochs, depending on your dataset.  After the warm-up phase, we’ll pause to evaluate network performance on the testing set:   evaluate the network after initialization print "[INFO] evaluating after initialization..."  predictions = model.predict testX, batch_size=32  print classification_report testY.argmax axis=1 ,  predictions.argmax axis=1 , target_names=classNames    The above code will allow us to compare the effects of ﬁne-tuning before and after allowing  the head to warm up.  Now that our FC layers have been partly trained and initialized, let’s unfreeze some of the CONV  layers in the body and make them trainable:   now that the head FC layers have been trained initialized, lets  unfreeze the final set of CONV layers and make them trainable for layer in baseModel.layers[15:]:  layer.trainable = True  Making a given layer in the body trainable again is an example of setting the parameter .trainable to True for the given layer. In some cases you’ll want to allow the entire body to be trainable; however, for deeper architectures with many parameters such as VGG, I suggest only unfreezing the top CONV layers and then continuing training. If classiﬁcation accuracy continues to improve  without overﬁtting , you may want to consider unfreezing more layers in the body.  At this point we should have a warm start to training, so we’ll switch over to SGD  again with  a small learning rate  and continue training:   104  105  106  107  108  109  110  111  112  113  114  115  116  118  119  120  121  122  123  124  125  126  5.1 Transfer Learning and Fine-tuning  67   for the changes to the model to take affect we need to recompile  the model, this time using SGD with a *very* small learning rate print "[INFO] re-compiling model..."  opt = SGD lr=0.001  model.compile loss="categorical_crossentropy", optimizer=opt,  metrics=["accuracy"]    train the model again, this time fine-tuning *both* the final set  of CONV layers along with our set of FC layers print "[INFO] fine-tuning model..."  model.fit_generator aug.flow trainX, trainY, batch_size=32 ,  validation_data= testX, testY , epochs=100, steps_per_epoch=len trainX     32, verbose=1   This time we permit our network to train over 100 epochs, allowing the CONV ﬁlters to adapt to  the underlying patterns in the training data.  Finally, we can evaluate our ﬁne-tuned network as well as serialize the weights to disk:   evaluate the network on the fine-tuned model print "[INFO] evaluating after fine-tuning..."  predictions = model.predict testX, batch_size=32  print classification_report testY.argmax axis=1 ,  predictions.argmax axis=1 , target_names=classNames     save the model to disk print "[INFO] serializing model..."  model.save args["model"]   To perform network surgery and ﬁne-tune VGG16 on the Flowers-17 dataset, just execute the  following command:  $ python finetune_flowers17.py --dataset .. datasets flowers17 images \  --model flowers17.model  [INFO] loading images... [INFO] processed 500 1360 [INFO] processed 1000 1360 [INFO] compiling model... [INFO] training head... Epoch 1 25 10s - loss: 4.8957 - acc: 0.1510 - val_loss: 2.1650 - val_acc: 0.3618 ... Epoch 10 25 10s - loss: 1.1318 - acc: 0.6245 - val_loss: 0.5132 - val_acc: 0.8441 ... Epoch 23 25 10s - loss: 0.7203 - acc: 0.7598 - val_loss: 0.4679 - val_acc: 0.8529 Epoch 24 25 10s - loss: 0.7355 - acc: 0.7520 - val_loss: 0.4268 - val_acc: 0.8853 Epoch 25 25 10s - loss: 0.7504 - acc: 0.7598 - val_loss: 0.3981 - val_acc: 0.8971 [INFO] evaluating after initialization... f1-score  precision  support  recall   68  Chapter 5. Fine-tuning Networks  bluebell buttercup coltsfoot cowslip crocus daffodil daisy dandelion fritillary iris lilyvalley pansy snowdrop sunflower tigerlily tulip windflower  avg   total  0.75 0.94 0.94 0.70 1.00 0.87 0.90 0.96 1.00 1.00 0.93 0.83 0.88 1.00 0.90 0.86 0.83  0.90  1.00 0.85 0.85 0.78 0.80 0.96 0.95 0.96 0.86 0.95 0.93 1.00 0.96 0.96 1.00 0.38 0.94  0.90  0.86 0.89 0.89 0.74 0.89 0.91 0.93 0.96 0.93 0.98 0.93 0.90 0.92 0.98 0.95 0.52 0.88  0.89  18 20 20 18 20 27 20 23 22 21 15 19 23 23 19 16 16  340  Notice how our initial accuracy is extremely low for the ﬁrst epoch  ≈ 36%  during the warm up phase. This result is due to the fact that the FC layers in our new head are randomly initialized and still trying to learn the patterns from the previously trained CONV ﬁlters. However, accuracy quickly rises – by epoch 10 we are above 80% classiﬁcation accuracy, and by the end of epoch 25 we have reached almost 90% accuracy.  Now that our FCHeadNet has obtained a warm start, we switch over to SGD and unfreeze the ﬁrst set of CONV layers in the body, allowing the network to train for another 100 epochs. Accuracy continues to improve, all the way to 95% classiﬁcation accuracy, higher than the 93% we obtained using feature extraction:  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  ... [INFO] re-compiling model... [INFO] fine-tuning model... Epoch 1 100 12s - loss: 0.5127 - acc: 0.8147 - val_loss: 0.3640 - val_acc: 0.8912 ... Epoch 99 100 12s - loss: 0.1746 - acc: 0.9373 - val_loss: 0.2286 - val_acc: 0.9265 Epoch 100 100 12s - loss: 0.1845 - acc: 0.9402 - val_loss: 0.2019 - val_acc: 0.9412 [INFO] evaluating after fine-tuning...  precision  recall  f1-score  support  bluebell buttercup coltsfoot cowslip crocus daffodil daisy dandelion fritillary iris lilyvalley  0.94 0.95 1.00 0.85 0.90 1.00 1.00 0.96 1.00 1.00 1.00  0.94 1.00 0.90 0.94 0.90 0.78 0.95 1.00 0.95 0.95 0.93  0.94 0.98 0.95 0.89 0.90 0.88 0.97 0.98 0.98 0.98 0.97  18 20 20 18 20 27 20 23 22 21 15   5.2 Summary  25  26  27  28  29  30  31  32  pansy snowdrop sunflower tigerlily tulip windflower  avg   total  1.00 0.92 0.96 0.90 0.70 0.94  0.95  1.00 0.96 1.00 1.00 0.88 0.94  0.94  1.00 0.94 0.98 0.95 0.78 0.94  0.94  19 23 23 19 16 16  340  69  Additional accuracy can be obtained by performing more aggressive data augmentation and continually unfreezing more and more CONV blocks in VGG16. While ﬁne-tuning is certainly more work than feature extraction, it also enables us to tune and modify the weights in our CNN to a particular dataset – something that feature extraction does not allow. Thus, when given enough training data, consider applying ﬁne-tuning as you’ll likely obtain higher classiﬁcation accuracy than simple feature extraction alone.  5.2 Summary  In this chapter, we discussed the second type of transfer learning, ﬁne-tuning. Fine-tuning works by replacing the fully-connected head of a network with a new, randomly initialized head. The layers in the body of the original network are frozen while we train the new FC layers.  Once our network starts to obtain reasonable accuracy, implying that the FC layers have started to learn patterns from both  1  the underlying training data and  2  the previously trained CONV ﬁlters earlier in the network, we unfreeze part  if not all  of the body – training is then allowed to continue.  Applying ﬁne-tuning is an extremely powerful technique as we do not have to train an en- tire network from scratch. Instead, we can leverage pre-existing network architectures, such as state-of-the-art models trained on the ImageNet dataset which consist of a rich, discriminative set of ﬁlters. Using these ﬁlters, we can “jump start” our learning, allowing us to perform network surgery, which ultimately leads to a higher accuracy transfer learning model with less effort  and headache  than training from scratch.  For more practical examples of transfer learning and ﬁne-tuning, be sure to refer to the ImageNet  Bundle where I demonstrate how to:  1. Recognize the make and model of a vehicle. 2. Automatically identify and correct image orientation.    6. Improving Accuracy with Network Ensembles  In this chapter, we’ll explore the concept of ensemble methods, the process of taking multiple clas- siﬁers and aggregating them into one big meta-classiﬁer. By averaging multiple machine learning models together, we can outperform  i.e., achieve higher accuracy  by using just a single model chosen at random. In fact, nearly all state-of-the-art publications you read that compete in the ImageNet challenge report their best ﬁndings over ensembles of Convolutional Neural Networks. We’ll start with this chapter with a discussion on Jensen’s Inequality – the theory ensemble methods hinge on. From there I’ll demonstrate how to train multiple CNNs from a single script and evaluate their performance. We’ll then combine these CNNs into a single meta-classiﬁer and notice an increase in accuracy.  6.1 Ensemble Methods  The term “ensemble methods” generally refers to training a “large” number of models  where the exact value of “large” depends on the classiﬁcation task  and then combining their output predictions via voting or averaging to yield an increase in classiﬁcation accuracy. In fact, ensemble methods are hardly speciﬁc to deep learning and Convolutional Neural Networks. We’ve been using ensemble methods for years. Techniques such as AdaBoost [18] and Random Forests [19] are the quintessential examples of ensemble methods.  In Random Forests, we train multiple Decision Trees [20, 21] and use our forest to make predictions. As you can see from Figure 6.2  left , our Random Forest consists of multiple decision trees aggregated together. Each decision tree “votes” on what it thinks the ﬁnal classiﬁcation should be. These votes are tabulated by the meta-classiﬁer, and the category with the most votes is chosen as the ﬁnal classiﬁcation.  The same concept can be applied to deep learning and Convolutional Neural Networks. Here we train multiple networks and then ask each network to return the probabilities for each class label given an input data point  Figure 6.2, left . These probabilities are averaged together, and the ﬁnal classiﬁcation is obtained. To understand why averaging predictions over multiple models works, we ﬁrst need to discuss Jensen’s Inequality. We’ll then provide Python and Keras code to implement an ensemble of CNNs and see for ourselves that classiﬁcation accuracy does indeed increase.   72  Chapter 6. Improving Accuracy with Network Ensembles  Figure 6.1: A Random Forest consists of multiple decision trees. The outputs of each decision tree are averaged together to obtain the ﬁnal classiﬁcation. Image reproduced from Nguyen et al. [22]  6.1.1 Jensen’s Inequality  In the most general terms, an ensemble is a ﬁnite collection of models that can be used to obtain better average predictive accuracy than using a single model in the ensemble collection. The seminal work of Dietterich [23] details the theory of why ensemble methods can typically obtain higher accuracy than a single model alone.  Dietterich’s work hinges on Jensen’s Inequality, which is known as the “diversity” or the “ambiguity decomposition” in machine learning literature. The formal deﬁnition of Jensen’s Inequality states that the convex combined  average  ensemble will have error less than or equal to the average error of the individual models. It may be that one individual model has a lower error than the average of all models, but since there is no criterion that we can use to “select” this model, we can be assured that the average of all models will perform no worse than selecting any single model at random. In short, we can only get better by averaging our predictions together; we don’t have to fear making our classiﬁer worse.  For those of us who enjoy visual examples, perhaps Jensen’s Inequality and the concept of model averaging is best explained by asking you to look at this jar of candies andguess how many candies are inside  Figure 6.2, right .  How many candies would you guess? 100? 200? 500? Your guess might be extremely above or below the actual number of candies in the jar. It could be very close. Or if you’re very lucky, you might guess the exact number of candies.  However, there is a little trick to this game – and it’s based on Jensen’s Inequality. If you were to ask me how many candies are in the jar, I would go around to you and everyone else who purchased a copy of Deep Learning for Computer Vision with Python and ask each of them what they thought the candy count is. I would then take all of these guesses and average them together – and I would use this average as my ﬁnal prediction.  Now, it may be that a handful of you are really good guessers and can beat the average; however, I don’t have any criterion to determine which of you are really good guessers. Since I cannot tell who are the best guessers, I’ll instead take the average of everyone I ask – and thereby I’m guaranteed to do no worse  on average  than selecting any one of your guesses at random. I may not win the candy guessing game each time we play, but I’ll always be in the running; and that, in essence, is Jensen’s Inequality.  The difference between randomly guessing candy counts and deep learning models is that   6.1 Ensemble Methods  73  Figure 6.2: Left: An ensemble of neural networks consists of multiple networks. When classifying an input image the data point is passed to each network where it classiﬁes the image independently of all other networks. The classiﬁcations across networks are then averaged to obtain the ﬁnal prediction. Right: Ensemble methods are possible due to Jensen’s Inequality. By averaging guesses as to the number of candies in the jar, we can better approximate the true number of candies.  we assume our CNNs are performing well and are good guessers  i.e., not randomly guessing . Therefore, if we average the results of these predictors together, we’ll often see a rise in our classiﬁcation accuracy. This improvement is exactly why you see state-of-the-art publications on deep learning train multiple models and then report their best accuracies over these ensembles.  6.1.2 Constructing an Ensemble of CNNs  The ﬁrst step in building an ensemble of CNNs is to train each individual CNN. At this point in Deep Learning for Computer Vision with Python we’ve seen many examples of training a single CNN – but how do we train multiple networks? In general, we have two options:  1. Run the script we use to train a single network multiple times, changing the path to the output  serialized model weights to be unique for each run.  2. Create a separate Python script that uses for loop to train N networks and outputs the  serialized model at the end of each iteration.  Both methods are perfectly acceptable to train a simple ensemble of CNNs. Since we’re quite comfortable running a single command to generate a single output CNN, let’s try the second option where a single script is responsible for training multiple networks. Open up a new ﬁle, name it train_models.py, and insert the following code:  1  2  3  4  5  6  7  8  9  10   set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use "Agg"    import the necessary packages from sklearn.preprocessing import LabelBinarizer from sklearn.metrics import classification_report from pyimagesearch.nn.conv import MiniVGGNet from keras.preprocessing.image import ImageDataGenerator from keras.optimizers import SGD   74  Chapter 6. Improving Accuracy with Network Ensembles  11  12  13  14  15  from keras.datasets import cifar10 import matplotlib.pyplot as plt import numpy as np import argparse import os  Lines 2 and 3 import the matplotlib package and then set the backend such that we can save plots to disk. Lines 6-15 then import our remaining Python packages. All of these packages we have used before, but I’ll call out the important ones below:    Line 8: We’ll be training multiple MiniVGGNet models to form our ensemble.   Line 9: We’ll be using the ImageDataGenerator class to apply data augmentation when   Lines 10 and 11: Our MiniVGGNet models will be trained on the CIFAR-10 dataset using  training our network.  The train_models.py script will require two command line arguments followed by an addi-  the SGD optimizer.  tional optional one:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-o", "--output", required=True,  help="path to output directory"   ap.add_argument "-m", "--models", required=True,  help="path to output models directory"   ap.add_argument "-n", "--num-models", type=int, default=5,  help=" of models to train"   args = vars ap.parse_args     The --output argument will serve as the base output directory where we’ll save classiﬁcation reports along with loss accuracy plots for each of the networks we will train. We then have the --models switch which controls the path to the output directory where we will be storing our serialized network weights.  Finally, the --num-models argument indicates the number of networks in our ensemble. We default this value to 5 networks. While traditional ensemble methods such as Random Forests typically consist of > 30 Decision Trees  and in many cases > 100 , we normally only see 5-10 Convolutional Neural Networks in an ensemble – the reason is due to the fact that CNNs are much more time-consuming and computationally expensive to train.  Our next code block handles loading the CIFAR-10 dataset from disk, scaling the pixel in- tensities to the range [0,1], and one-hot encoding our class labels so we can apply categorical cross-entropy as our loss function:   load the training and testing data, then scale it into the  range [0, 1]   trainX, trainY ,  testX, testY   = cifar10.load_data   trainX = trainX.astype "float"    255.0 testX = testX.astype "float"    255.0   convert the labels from integers to vectors lb = LabelBinarizer   trainY = lb.fit_transform trainY  testY = lb.transform testY   17  18  19  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37   38  39  40  42  43  44  45  47  48  49  50  51  52  53  54  55  56  57  59  60  61  62  63  64  65  66  6.1 Ensemble Methods  75   initialize the label names for the CIFAR-10 dataset labelNames = ["airplane", "automobile", "bird", "cat", "deer",  "dog", "frog", "horse", "ship", "truck"]  We also need to initialize our ImageDataGenerator so we can apply data augmentation to the  CIFAR-10 training data:   construct the image generator for data augmentation aug = ImageDataGenerator rotation_range=10, width_shift_range=0.1,  height_shift_range=0.1, horizontal_flip=True, fill_mode="nearest"   Here we’ll allow images to be randomly rotated 10 degrees, shifted by a factor of 0.1, and  randomly horizontally ﬂipped.  We are now ready to train each individual MiniVGGNet model in the ensemble:   loop over the number of models to train for i in np.arange 0, args["num_models"] :  initialize the optimizer and model print "[INFO] training model {} {}".format i + 1,  args["num_models"]    opt = SGD lr=0.01, decay=0.01   40, momentum=0.9,  model = MiniVGGNet.build width=32, height=32, depth=3,  nesterov=True   classes=10   model.compile loss="categorical_crossentropy", optimizer=opt,  metrics=["accuracy"]   On Line 48 we start looping over the number of --num-models to train. Line 52 initializes the SGD optimizer using a learning rate of α = 0.01, a momentum of γ = 0.9, and a standard Keras learning rate decay of the learning rate divided by total number of epochs  Chapter 16, Starter Bundle . We’ll also indicate that Nesterov accelerations should be used. Lines 54-57 then instantiate the individual MiniVGGNet model and compile it.  Next, let’s train the network and serialize it to disk:   train the network H = model.fit_generator aug.flow trainX, trainY, batch_size=64 ,  validation_data= testX, testY , epochs=40, steps_per_epoch=len trainX     64, verbose=1    save the model to disk p = [args["models"], "model_{}.model".format i ] model.save os.path.sep.join p    Lines 60-62 train our MiniVGGNet model using the fit_generator method. We use fit_generator  because we need the .flow method of the ImageDataGenerator to apply data augmentation. The network will be trained for a total of 64 epochs using batch sizes of 64. The steps_per_epoch parameter controls the number of batches per epoch, which is simply the number of training samples divided by our batch size.   76  Chapter 6. Improving Accuracy with Network Ensembles  After the network ﬁnishes training, we construct a unique output path for it and save the weights to disk  Lines 65 and 66 . Let’s also save a classification_report to disk for each network as well so we can review performance once the script ﬁnishes executing:  68  69  70  71  72  73  74  75  76  77  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96   evaluate the network predictions = model.predict testX, batch_size=64  report = classification_report testY.argmax axis=1 ,  predictions.argmax axis=1 , target_names=labelNames    save the classification report to file p = [args["output"], "model_{}.txt".format i ] f = open os.path.sep.join p , "w"  f.write report  f.close    The same goes for plotting our loss and accuracy over time:   plot the training loss and accuracy p = [args["output"], "model_{}.png".format i ] plt.style.use "ggplot"  plt.figure   plt.plot np.arange 0, 40 , H.history["loss"],  plt.plot np.arange 0, 40 , H.history["val_loss"],  plt.plot np.arange 0, 40 , H.history["acc"],  plt.plot np.arange 0, 40 , H.history["val_acc"],  label="train_loss"   label="val_loss"   label="train_acc"   label="val_acc"   plt.title "Training Loss and Accuracy for model {}".format i   plt.xlabel "Epoch "  plt.ylabel "Loss Accuracy"  plt.legend   plt.savefig os.path.sep.join p   plt.close    It’s important to note that we would never jump straight to training an ensemble – we would ﬁrst run a series of experiments to determine which combination of architecture, optimizer, and hyperparameters yields the highest accuracy on a given dataset.  Once you’ve reached this optimal set of combination, we would then switch over to training multiple models to form an ensemble. Training an ensemble as your very ﬁrst experiment is consid- ered premature optimization as you don’t know what combination of architecture, optimizer, and hyperparameters will work best for your given dataset.  With that said, we know from Chapter 15 of the Starter Bundle that MiniVGGNet trained with SGD gives a reasonable classiﬁcation accuracy of 83% – by applying ensemble methods we hope to increase this accuracy.  To train our set of MiniVGGNet models, just execute the following command:  $ python train_models.py --output output --models models [INFO] training model 1 5 [INFO] training model 2 5   6.1 Ensemble Methods  [INFO] training model 3 5 [INFO] training model 4 5 [INFO] training model 5 5  77  Since we are now training ﬁve networks rather than one, it will take 5x as long for this script to  run. Once it executes, take a look at your output directory:  $ ls output  model_0.png model_1.png model_0.txt model_1.txt  model_2.png model_3.png model_2.txt model_3.txt  model_4.png model_4.txt  Here you will see the output classiﬁcation reports and training curves for each of the networks.  Using grep we can easily extract the classiﬁcation accuracy of each network:  $ grep ’avg   total’ output *.txt output model_0.txt:avg   total output model_1.txt:avg   total output model_2.txt:avg   total output model_3.txt:avg   total output model_4.txt:avg   total  0.83 0.83 0.83 0.82 0.83  0.83 0.83 0.83 0.82 0.83  0.83 0.83 0.83 0.82 0.83  10000 10000 10000 10000 10000  Four of the ﬁve networks obtain 83% classiﬁcation accuracy while the remaining network reaches only 82% accuracy. Furthermore, looking at all ﬁve training plots  Figure 6.3  we can see that each set of learning curves looks somewhat similar, although each also looks unique, demonstrating that each MiniVGGNet model “learned” in a different manner.  Now that we’ve trained our ﬁve individual ensembles, it’s time to combine their predictions and  see if our classiﬁcation accuracy increases.  6.1.3 Evaluating an Ensemble  To construct and evaluate our ensemble of CNNs, create a separate ﬁle named test_ensemble.py and insert the following code:  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15   import the necessary packages from sklearn.preprocessing import LabelBinarizer from sklearn.metrics import classification_report from keras.models import load_model from keras.datasets import cifar10 import numpy as np import argparse import glob import os   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-m", "--models", required=True,  help="path to models directory"   args = vars ap.parse_args     Lines 2-9 import our required Python packages while Lines 12-15 parse our command line arguments. We only need a single switch with here, --models, the path to where our serialized network weights are stored on disk.   78  Chapter 6. Improving Accuracy with Network Ensembles  Figure 6.3: Training and validation plots for each of the ﬁve networks in our ensemble.  From there we can load the CIFAR-10 dataset, keeping only the testing set since we are only  evaluating  and not training  our networks:  17  18  19  20  21  22  23  24  25  26  27   load the testing data, then scale it into the range [0, 1]  testX, testY  = cifar10.load_data  [1] testX = testX.astype "float"    255.0   initialize the label names for the CIFAR-10 dataset labelNames = ["airplane", "automobile", "bird", "cat", "deer",  "dog", "frog", "horse", "ship", "truck"]   convert the labels from integers to vectors lb = LabelBinarizer   testY = lb.fit_transform testY    6.1 Ensemble Methods  79  We now need to gather the paths to our pre-trained MiniVGGNet networks, which is easy  enough using the glob module built into Python:  29  30  31  32  33   construct the path used to collect the models then initialize the  models list modelPaths = os.path.sep.join [args["models"], "*.model"]  modelPaths = list glob.glob modelPaths   models = []  Line 31 constructs a wildcard path  notice the asterisk “*” in the ﬁle path  to all .model ﬁles in the --models directory. Using glob.glob on Line 32 we can automatically ﬁnd all ﬁle paths inside --models that end with the .model ﬁle extension. After executing Line 32 our modelPaths list now contains the following entries:  [’models model_0.model’, ’models model_1.model’, ’models model_2.model’,  ’models model_3.model’, ’models model_4.model’]  Line 33 then initializes a list of models which will store the deserialized MiniVGGNet networks  loaded from disk.  Let’s go ahead and load each model from disk now:   loop over the model paths, loading the model, and adding it to  the list of models for  i, modelPath  in enumerate modelPaths :  print "[INFO] loading model {} {}".format i + 1,  len modelPaths     models.append load_model modelPath    On Line 37 we loop over each of the individual modelPath ﬁle paths. We then load the  serialized network via load_model and append it to the models list.  Finally, we are ready to evaluate our ensemble:   initialize the list of predictions print "[INFO] evaluating ensemble..."  predictions = []   loop over the models for model in models:   use the current model to make predictions on the testing data,  then store these predictions in the aggregate predictions list predictions.append model.predict testX, batch_size=64     average the probabilities across all model predictions, then show  a classification report predictions = np.average predictions, axis=0  print classification_report testY.argmax axis=1 ,  predictions.argmax axis=1 , target_names=labelNames    On Line 44 we initialize our list of predictions. Each model in the models list will produce ten probabilities  one for each class label in the CIFAR-10 dataset  for every data point in the testing  35  36  37  38  39  40  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56   80  Chapter 6. Improving Accuracy with Network Ensembles  set. Given that there are 10,000 data points in the CIFAR-10 dataset, each model will produce an array of size 10,000× 10 – each row corresponds to a given data point and each column the corresponding probability.  To accumulate these predictions, we loop over each individual model on Line 47. We then call .predict on the testing data and update the predictions list with the probabilities produced by the respective model. After we have looped over the ﬁve models in our ensemble and updated the predictions list, our predictions array now has the shape  5, 10000, 10 , implying that there are ﬁve models, each of which produced 10 class label probabilities for each of the 10,000 testing data points. Line 54 then averages the probabilities for each testing data point across all ﬁve models.  To see this for ourselves, we can investigate the shape of our predictions array which is now  10000, 10  implying that probabilities for each of the ﬁve models have been averaged together. The averaging is why we call this method an ensemble – we are taking the output of multiple, independent models and averaging them together to obtain our ﬁnal output. According to Jensen’s Inequality, applying ensemble methods should perform no worse  on average  than selecting one of the individual models at random.  Finally, Lines 55 and 56 display a classiﬁcation report for our ensemble predictions. To determine if our ensemble of MiniVGGNet models increased classiﬁcation accuracy, execute the following command:  $ python test_ensemble.py --models models [INFO] loading model 1 5 [INFO] loading model 2 5 [INFO] loading model 3 5 [INFO] loading model 4 5 [INFO] loading model 5 5 [INFO] evaluating ensemble...  precision  recall  f1-score  support  airplane automobile bird cat deer dog frog horse ship truck  avg   total  0.89 0.93 0.80 0.72 0.80 0.77 0.86 0.91 0.89 0.87  0.84  0.82 0.93 0.74 0.67 0.86 0.77 0.91 0.87 0.94 0.93  0.84  0.85 0.93 0.77 0.69 0.83 0.77 0.88 0.89 0.92 0.90  0.84  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000  10000  Looking at the output classiﬁcation report we can see that we increased our accuracy from 83% to 84%, simply by combining the output of multiple networks, even though these networks were trained on the same dataset using the exact same hyperparameters. In general, you can expect an increase of 1-5% accuracy when applying ensembles of Convolutional Neural Networks depending on your dataset.  6.2 Summary  In this chapter, we reviewed the ensembles machine learning technique and how training multiple, independent models followed by averaging the results together can increase classiﬁcation accuracy.   6.2 Summary  81  The theoretical justiﬁcation for ensemble methods can be found by reviewing Jensen’s Inequality which states that on average, we are better off averaging the results of multiple models together rather than picking one at random.  In fact, the top results you see reported by state-of-the-art papers  including Inception [17], ResNet [24], etc.  are the average over multiple models  typically 3-5, depending on how long the authors had to train their networks before their publication was due . Depending on your dataset, you can normally expect a 1-5% increase in accuracy.  While ensembles may be a simple method to improve classiﬁcation accuracy they are also a computationally expensive one – rather than training a single network, we are now responsible for training N of them. Training a CNN is already a time-consuming operation, so a method that scales linearly may not be practical in some situations.  To alleviate the computational burden of training multiple models, Huang et al. [25] propose the idea of using cyclic learning rate schedules to train multiple models during a single training process in their 2017 paper, Snapshot Ensembles: Train 1, get M for free.  This method works by starting training with a high learning rate, quickly lowering it, saving the model weights, and then resetting the learning rate to its original value without re-initializing network weights. This action enables the network to theoretically extend coverage to areas of local minima  or at least areas of low loss  multiple times during the training process. Snapshot Ensembles are outside the scope of this book but are worth investigating if you need to boost your classiﬁcation accuracy but cannot afford to train multiple models.    7. Advanced Optimization Methods  So far in this book, we have only studied and used Stochastic Gradient Descent  SGD  to optimize our networks – but there are other optimization methods that are used in deep learning. Speciﬁcally, these more advanced optimization techniques seek to either:  1. Reduce the amount of time  i.e., number of epochs  to obtain reasonable classiﬁcation  2. Make the network more “well-behaved” for a larger range of hyperparameters other than the  accuracy.  learning rate.  3. Ideally, obtain higher classiﬁcation accuracy than what is possible with SGD. With the latest incarnation of deep learning, there has been an explosion of new optimization techniques, each seeking to improve on SGD and provide the concept of adaptive learning rates. As we know, SGD modiﬁes all parameters in a network equally in proportion to a given learning rate. However, given that the learning rate of a network is  1  the most important hyperparameter to tune and  2  a hard, tedious hyperparameter to set correctly, deep learning researchers have postulated that it’s possible to adaptively tune the learning rate  and in some cases, per parameter  as the network trains.  In this chapter, we’ll review adaptive learning rate methods. I’ll also provide suggestions on  which optimization algorithms you should be using in your own projects.  7.1 Adaptive Learning Rate Methods  In order to understand each of the optimization algorithms in this section, we are going to examine them in terms of pseudocode – speciﬁcally the update step. Much of this chapter has been inspired by the excellent overview of optimization methods by Karpathy [26] and Ruder [27]. We’ll extend  and in some cases, simplify  their explanations of these methods to make the content more digestible.  To get started, let’s take a look at an algorithm we are already familiar with – the update phase  of vanilla SGD:  W += -lr * dW   84  Chapter 7. Advanced Optimization Methods  Here we have three values: 1. W: Our weight matrix. 2. lr: The learning rate. 3. dW: The gradient of W. Our learning rate here is ﬁxed and, provided it is small enough, we know our loss will decrease during training. We’ve also seen extensions to SGD which incorporate momentum and Nesterov acceleration in Chapter 7. Given this notation, let’s explore common adaptive learning rate optimizers you will encounter in your deep learning career.  7.1.1 Adagrad  The ﬁrst adaptive learning rate method we are going to explore is Adagrad, ﬁrst introduced by Duchi et al [28]. Adagrad adapts the learning rate to the network parameters. Larger updates are performed on parameters that change infrequently while smaller updates are done on parameters that change frequently.  Below we can see a pseudocode representation of the Adagrad update:  cache +=  dW ** 2  W += -lr * dW    np.sqrt cache  + eps   The ﬁrst parameter you’ll notice here is the cache – this variable maintains the per-parameter sum of squared gradients and is updated at every mini-batch in the training process. By examining the cache, we can see which parameters are updated frequently and which ones are updated infrequently.  We can then divide the lr * dx by the square-root of the cache  adding in an epsilon value for smoothing and preventing division by zero errors . Scaling the update by all previous sum of square gradients allows us to adaptively update the parameters in our network.  Weights that have frequently updated large gradients in the cache will scale the size of the update down, effectively lowering the learning rate for the parameter. On the other hand, weights that have infrequent updates smaller gradients in the cache will scale up the size of the update, effectively raising the learning rate for the speciﬁc parameter.  The primary beneﬁt of Adagrad is that we no longer have to manually tune the learning rate – most implementations of the Adagrad algorithm leave the initial learning rate at 0.01 and allow the adaptive nature of the algorithm to tune the learning rate on a per-parameter basis.  However, the weakness of Adagrad can be seen by examining the cache. At each mini-batch, the squared gradients are accumulated in the denominator. Since the gradients are squared  and are therefore always positive , this accumulation keeps growing and growing during the training process. As we know, dividing a small number  the gradient  by a very large number  the cache  will result in an update that is inﬁnitesimally small, too small for the network to actually learn anything in later epochs  This phenomena occurs for even small, infrequently updated parameters as positive values in the cache grows monotonically, which is why we rarely see Adagrad used to train  modern  deep learning neural networks. However, it is important to review so we can understand the extensions to the Adagrad algorithm.  7.1.2 Adadelta  The Adadelta algorithm was proposed by Zeiler in their 2012 paper, ADADELTA: An Adaptive Learning Rate Method [29]. Adadelta can be seen as an extension to Adagrad that seeks to reduce the monotonically decreasing learning rate caused by the cache.   7.1 Adaptive Learning Rate Methods  85  In the Adagrad algorithm, we update our cache with all of the previously squared gradients. However, Adadelta restricts this cache update by only accumulating a small number of past gradients – when actually implemented, this operation amounts to computing a decaying average of all past squared gradients.  Adadelta can thus be seen as an improvement to Adagrad; however, the very closely related  RMSprop algorithm  which also performs cache decay  is often preferred to Adadelta.  7.1.3 RMSprop  Developed independently of Adadelta, the RMSprop algorithm is an  unpublished  optimization algorithm shown in the slides of Geoffrey Hinton’s Coursera class [2]. Similar to Adadelta, RMSprop attempts to rectify the negative effects of a globally accumulated cache by converting the cache into an exponentially weighted moving average.  Let’s take a look at the RMSprop pseudocode update:  cache = decay_rate * cache +  1 - decay_rate  *  dW ** 2  W += -lr * dW    np.sqrt cache  + eps   The ﬁrst aspect of RMSprop you’ll notice is that the actual update to the weight matrix W is identical to that of Adagrad – what matters here is how the cache is updated. The decay_rate, often deﬁned as ρ, is a hyperparameter typically set to 0.9. Here we can see that previous entries in the cache will be weighted substantially smaller than new updates. This “moving average” aspect of RMSprop allows the cache to “leak out" old squared gradients and replace them with newer, “fresher” ones.  Again, the actual update to W is identical to that of Adagrad – the crux of the algorithm hinges on exponentially decaying the cache, enabling us to avoid monotonically decreasing learning rates during the training process. In practice, RMSprop tends to be more effective than both Adagrad and Adadelta when applied to training a variety of deep learning networks [5]. Furthermore, RMSprop tends to converge signiﬁcantly faster than SGD.  Outside of SGD, RMSprop has arguably been the second most used optimization algorithm in recent deep learning literature; however, the next optimization method we are about to discuss, Adam, is now being used more than RMSprop.  7.1.4 Adam  The Adam  Adaptive Moment Estimation  optimization algorithm, proposed by Kingma and Ba in their 2014 paper, Adam: A Method for Stochastic Optimization [1] is essentially RMSprop only with momentum added to it:  m = beta1 * m +  1 - beta1  * dW v = beta2 * v +  1 - beta2  *  dW ** 2  x += -lr * m    np.sqrt v  + eps   R Again, I want to draw special attention to these pseudocode updates as they were derived and  popularized by Karpathy’s excellent optimization method notes [26].  The values of both m and v are similar to SGD momentum, relying on their respective previous values from time t − 1. The value m represents the ﬁrst moment  mean  of the gradients while v is the second moment  variance .   86  Chapter 7. Advanced Optimization Methods  The actual update to W is near identical to RMSprop, only now we are using the “smoothed” version  due to computing the mean  of m rather than the raw gradient dW – using the mean tends to lead to more desirable updates as we can smooth over noisy updates in the raw dW values. Typically beta1 is set to 0.9 while beta2 is set to 0.999 – these values are rarely  if ever  changed when using the Adam optimizer.  In practice, Adam tends to work better than RMSprop in many situations. For more details on  the Adam optimization algorithm, please see Kingma and Ba [1].  7.1.5 Nadam  Just like Adam is RMSprop with momentum, Nadam is RMSprop with Nesterov acceleration. Nadam was proposed by Timothy Dozat, a Ph.D. student at Stanford University [30]. We typically don’t see Nadam used “in the wild”, but is important to understand that this variation of Adam does exist.  7.2 Choosing an Optimization Method  Given the choices between all of these optimization algorithms, which one should you choose? Unfortunately, the answer is highly inconclusive – the work of Schaul et al. in 2014, Unit tests for Stochastic Optimization [31], attempted to benchmark many of these optimization methods and found that while adaptive learning rate algorithms performed favorably, there was no clear winner. Deep learning optimization algorithms  and how to choose them  is still an open area of research, and will likely continue to be for many years. Therefore, instead of exhaustively trying every optimization algorithm you can ﬁnd, throwing each at your dataset and noting what sticks, it’s better to master two or three optimization algorithms. Often, the success of a deep learning project is a combination of the optimization algorithm  and associated parameters  along with how adept the researcher is at “driving” the algorithm.  7.2.1 Three Methods You Should Learn how to Drive: SGD, Adam, and RMSprop  “The choice of which algorithm to use, at this point, seems to depend largely on the user’s familiarity with the algorithm  for ease of hyperparameter tuning .” – Goodfellow et al. [5]  Given the success of adaptive learning rate algorithms such as RMSprop and Adam, you might be tempted to simply ignore SGD and treat it like an archaic tool. After all, “better" methods exist, right?  However, implying ignoring SGD would be a big mistake. Take a look at any recent state-of- the-art deep learning publication on challenging image classiﬁcation datasets such as ImageNet: AlexNet [6], VGGNet [11], SqueezeNet [32], Inception [17], ResNet [33] – all of these state-of- the-art architectures were trained using SGD.  But why is this? We can clearly see the beneﬁts in algorithms that apply adaptive learning rates such as RMSprop and Adam – networks can converge faster. However, the speed of convergence, while important, is not the most important factor – hyperparameters still win out. If you cannot tune the hyperparameters to a given optimizer  and associated model , your network will never obtain reasonable accuracy.  While SGD certainly converges slower than adaptive learning rate algorithms, it’s also a more studied algorithm. Researchers are more familiar with SGD and have spent years using it to train networks.  For example, consider a professional race car driver who has been driving the same make and model of a race car for ﬁve years. Then, one day, the driver’s sponsor changes and they are forced   7.3 Summary  87  to drive a new vehicle. The driver has no time to try out the new race car, and they are forced to start racing with no experience in the car. Will the driver perform as well in their ﬁrst few races? Most likely not – the driver is not familiar with the vehicle and its intricacies  but still might perform reasonably as the driver is a professional after all .  The same goes for deep learning architectures and optimization algorithms. The more experi- ments we perform with a given architecture and optimization algorithm, the more we learn about the intricacies of the training process. Given that SGD has been the cornerstone of training neural networks for nearly 60 years, it’s no wonder that this algorithm is still consistently used today – the rate of convergence simply doesn’t matter  as much  when compared to the performance  accuracy  of the model.  Simply put: If we can obtain higher accuracy on a given dataset using SGD, we’ll likely use SGD even if it takes 1.5x longer to train than when using Adam or RMSprop simply because we understand the hyperparameters better. Currently, the most used deep learning optimization algorithms are:  1. SGD 2. RMSprop 3. Adam I would recommend that you master SGD ﬁrst and apply it to every architecture and dataset you encounter. In some cases. it will perform great, and, in others, it will perform poorly. The goal here is for you to expose yourself to as many deep learning problems as possible using a speciﬁc optimization algorithm and learn how to tune the associated hyperparameters. Remember, deep learning is part science and part art – mastering an optimization algorithm is absolutely an art that requires much practice. From there, move on to either RMSprop or Adam.  I personally recommend studying Adam prior to RMSprop as, in my experience, Adam tends  to outperform RMSprop in most situations.  7.3 Summary  In this chapter, we discussed adaptive learning rate optimization algorithms that can be used in place of SGD. Choosing an optimization algorithm to train a deep neural network is highly dependent on your familiarity with:  1. The dataset 2. The model architecture 3. The optimization algorithm  and associated hyperparameters  Instead of exhaustively running experiments to try every optimization algorithm you can ﬁnd, it’s instead better to master two or three techniques and how to tune their hyperparameters. Becoming an expert at these techniques will enable you to apply new model architectures to datasets you haven’t worked with before with much more ease.  My personal recommendation is to spend a lot of time early in your deep learning career mastering how to use SGD; speciﬁcally, SGD with momentum. Once you feel comfortable applying SGD to a variety of architectures and datasets, move on to Adam and RMSprop.  Finally, keep in mind that the speed of model rate convergence is secondary to loss and accuracy – choose an optimization algorithm that you can  conﬁdently  tune the hyperparameters to, resulting in a reasonably performing network.    8. Optimal Pathway to Apply Deep Learning  In Chapter 10 of the Starter Bundle, we examined a recipe to train a neural network. The four ingredients to the recipe included:  1. Your dataset 2. A loss function 3. A neural network architecture 4. An optimization method Using this recipe, we can train any type of deep learning model. However, what this recipe does not cover is the optimal way to combine these ingredients together, as well as which parts of the recipe you need to ﬁddle with if you aren’t obtaining your desired results.  As you’ll ﬁnd out in your deep learning career, arguably the hardest aspect of deep learning is examining your accuracy loss curve and making the decision on what to do next. If your training error is too high, what do you do? What happens if your validation error is also high? How do you adjust your recipe when your validation error matches your training error. . . but then your testing set error is high?  Inside this chapter, I’ll discuss the optimal way to apply deep learning techniques, starting with rules of thumb you can use to adjust your recipe for training. I’ll then provide a decision process that you can use when deciding if you should train your deep learning model from scratch or apply transfer learning. By the end of this chapter, you’ll have a strong understanding of rules of thumb that expert deep learning practitioners use when training their own networks.  8.1 A Recipe for Training  The following section is heavily inspired by Andrew Ng’s excellent tutorial at NIPS 2016 titled, Nuts and Bolts of Building Deep Learning Applications [34]. In this talk, Ng discussed how we can get deep learning methods to work in our own products, businesses, and academic research. Arguably the most important takeaway from Ng’s talk follows  summarized by Malisiewicz [35] :  R  “Most issues in applied deep learning come from training data testing data mismatch. In some scenarios this issue just doesn’t come up, but you’d be surprised how often applied ma-   90  Chapter 8. Optimal Pathway to Apply Deep Learning  chine learning projects use training data  which is easy to collect and annotate  that is different from the target application.” – Andrew Ng  summarized by Malisiewicz   What both Ng and Malisiewicz are saying here is that you should take excruciating care to make sure your training data is representative of your validation and testing sets. Yes, obtaining, annotating, and labeling a dataset is extremely time consuming and even in some cases, very expensive. And yes, deep learning methods do tend to generalize well in certain situations. However, you cannot expect any machine learning model trained on data that is not representative to succeed.  For example, suppose we are tasked with the responsibility of building a deep learning system responsible for recognizing the make and model of a vehicle from a camera mounted to our car as we drive down the road  Figure 8.1, left .  Figure 8.1: Left: Cars on a highway that we wish to identify using deep learning. Right: Example “product shot” images of what our network was actually trained on.  The ﬁrst step is to gather our training data. To speed up the data gathering process, we decide to scrape websites that have both photos of cars and their make and model listed on the webpage – great examples of such websites in include Autotrader.com, eBay, CarMax, etc. For each of these websites we can build a simple spider that crawls the website, ﬁnds individual product listings,  i.e., the “car pages” that list the speciﬁcations of the vehicle , and then download the images and make + model information.  This method is quite simplistic, and outside the time it takes us to develop the spider, it won’t take us long to accumulate a reasonably large labeled dataset. We then split this dataset into two: training and validation, and proceed to train a given deep learning architecture to a high accuracy  > 90% .  However, when we apply our newly trained model to example images, such as in Figure 8.1  left , we ﬁnd that results are terrible – we are lucky to obtain 5 percent accuracy when deployed in the real-world. Why is this?  The reason is that we took the easy way out. We didn’t stop to consider that the product shots of cars listed on Autotrader, CarMax, and eBay  Figure 8.1, right  are not representative of the vehicles our deep learning vision system will be seeing mounted to the dash of our car. While our deep learning system may be great at identifying the make and model of a vehicle in a product shot, it will fail to recognize the make an model of a car from either a frontal or rear view, as is common when driving.  There is no shortcut to building your own image dataset. If you expect a deep learning system to obtain high accuracy in a given real-world situation, then make sure this deep learning system was trained on images representative of where it will be deployed – otherwise you will be very disappointed in its performance.   8.1 A Recipe for Training  91  Assuming we have gathered sufﬁcient training data that is representative of the classiﬁcation task we are trying to solve, Andrew Ng has provided with a four step process to aid us in our training [34].  Figure 8.2: Slide 13 of Andrew Ng’s talk [34]. Here Ng proposes four separate data splits when training a deep learning model.  Based on Figure 8.2 we can see that Ng is proposing four sets of data splits when training a  deep learning model:  1. Training 2. Training-validation  which Ng refers to as “development”  3. Validation 4. Testing We’ve already seen training, validation, and testing splits before – but what is this new “training- validation” set? Ng recommends that we take all of our data and split it into 60% for training and the remaining 40% for testing. We then split the testing data into two parts: one for validation and the other for true testing  i.e., the data we never touch until we are ready to evaluate the performance of our network . From our training set, we then take a small chunk of it and add it to our “training-validation set”. The training set will help us determine the bias of our model while the training-validation set will help determine variance.  If our training error is too high, as in Figure 8.3  top-left  below, then we should consider deepening our current architecture by adding in more layers and neurons. We should also consider training for longer  i.e., more epochs  while simultaneously tweaking our learning rate – using a smaller learning rate may enable you to train for longer while helping prevent overﬁtting. Finally, if after many experiments using our current architecture and varying learning rates does not prove useful, then we likely need to try an entirely different model architecture.  Moving on to the second item in the ﬂow chart, if our training-validation error is high  Figure 8.3, top-right , then we should examine the regularization parameters in our network. Are we applying dropout layers inside the network architecture? Is data augmentation being used to help generate new training samples? What about the actual loss update function itself – is a regularization penalty being included? Examine these questions in the context of your own deep learning experiments and start adding in regularization.  You should also consider gathering more training data  again, taking care that this training data is representative of where the model will be deployed  at this point – in nearly all cases having more training data is never a bad thing. It is likely that your model does not have enough training data to learn the underlying patterns in your example images. Finally, after exhausting these options,   92  Chapter 8. Optimal Pathway to Apply Deep Learning  Figure 8.3: The four stages of Andrew Ng’s machine learning recipe. Top-right: Our training error is high, implying that we need a more powerful model to represent the underlying patterns in the data. Top-left: Our training error has decreased, but our training-validation error is high. This implies we should obtain more data or apply strong regularization. Bottom-left: If both training and training-validation error are low, but validation error is high we should examine our training data and ensure it mimics our validation and testing sets properly. Bottom-right: If training, training-validation, and validation error are all low but testing error is high then we need to gather more training + validation data.  you’ll once again want to consider using a different network architecture.  Continuing through the ﬂowchart in Figure 8.3  bottom-left , if our training-validation error is low, but our validation set error is high, we need to examine our training data with a closer eye. Are we absolutely, positively sure that our training images are similar to our validation images?  Be honest with yourself – you cannot expect a deep learning model trained on images not representative of the images they’ll see in a validation or testing setting to perform well. If you make the hard realization that this is indeed the case, go back to the dataset collection phase and spend the time gathering more data. Without data representative of where your deep learning model will be deployed, you will not obtain high accuracy results. You should also again inspect your regularization parameters – are you regularizing strong enough? Finally, you may once again need to consider a new model architecture.  Finally, we move on to the last step in the ﬂow chart – is our testing error high? At this point, we’ve overﬁt our model to the training and validation data  Figure 8.3 bottom-right . We need to go back and gather more data for the validation set to help us identify when this overﬁtting   8.2 Transfer Learning or Train from Scratch  93  is starting to occur. Using this methodology proposed by Andrew Ng, we can more easily make  correct  decisions regarding updating our model dataset when our experiments don’t turn out as we expected.  8.2 Transfer Learning or Train from Scratch  The following section is inspired by the excellent “Transfer Learning” lesson of Stanford’s cs231n class [36]. I’ve also included my own anecdotal experiences to aid in your own experiments. Given the success of transfer learning in Chapter 3 on feature extraction and Chapter 5 on ﬁne-tuning, you may wonder when you should be applying transfer learning and when you should be training a model from scratch.  To make this decision, you need to consider two important factors: 1. The size of your dataset. 2. The similarity of your dataset to the dataset the pre-trained CNN was trained on  which is  typically ImageNet .  Based on these factors we can construct a chart to help us make a decision on whether or not we need to apply transfer learning or train from scratch  Figure 8.4 . Let’s review each of the four possibilities below.  Figure 8.4: A table you can use to determine if you should train our network from scratch or transfer learning. Figure inspired by Greg Chu from Deep Learning Sandbox [37].  Your Dataset is Small and Similar to the Original Dataset Since your dataset is small, you likely don’t have enough training examples to train a CNN from scratch  again, keep in mind that you should ideally have 1,000-5,000 examples per class you want to recognize . Furthermore, given the lack of training data, it’s likely not a good idea to attempt ﬁne-tuning as we’ll likely end up overﬁtting.  Instead, since your image dataset is similar to what the pre-trained network was trained on, you should treat the network as a feature extractor and train a simple machine learning classiﬁer on top of these features. You should extract features from layers deeper in the architecture as these features are more rich and representative of the patterns learned from the original dataset.  Your Dataset is Large and Similar to the Original Dataset With a large dataset, we should have enough examples to apply ﬁne-tuning without overﬁtting. You may be tempted to train your own model from scratch here as well – this is an experiment worth running. However, since your dataset is similar to the original dataset the network was already trained on, the ﬁlters inside the network are likely already discriminative enough to obtain a reasonable classiﬁer. Therefore, apply ﬁne-tuning in this case.  Your Dataset is Small and Different than the Original Dataset Again, given a small dataset, we likely won’t obtain a high accuracy deep learning model by training from scratch. Instead, we should again apply feature extraction and train a standard   94  Chapter 8. Optimal Pathway to Apply Deep Learning  machine learning model on top of them – but since our data is different from the original dataset, we should use lower level layers in the network as our feature extractors.  Keep in mind that the deeper we go into the network architecture, the more rich and discrimi- native the features are speciﬁc to the dataset it was trained on. By extracting features from lower layers in the network, we can still leverage these ﬁlters, but without the abstraction caused by the deeper layers.  Your New Dataset is Large and Different than Original Dataset In this case, we have two options. Given that we have sufﬁcient training data, we can likely train our own custom network from scratch. However, the pre-trained weights from models trained on dataset such as ImageNet make for excellent initializations, even if the datasets are unrelated. We should therefore perform two sets of experiments:  1. In the ﬁrst set of experiments, attempt to ﬁne-tune a pre-trained network to your dataset and  evaluate the performance.  2. Then in the second set of experiments, train a brand new model from scratch and evaluate. Exactly which method performs best is entirely dependent on your dataset and classiﬁcation problem. However, I would recommend trying to ﬁne-tune ﬁrst as this method will allow you to establish a baseline to beat when you move on to your second set of experiments and train your network from scratch.  8.3 Summary  In this chapter, we explored the optimal pathway to apply deep learning techniques when training your own custom networks. When gathering your training data, keep in mind there are no shortcuts – take the time to ensure that data you use to train your model is representative of the images your network will see when deployed in a real-world application.  There is an old computer science anecdote that states “Garbage in, garbage out”. If your input data does not represent examples of data points your model will see after being trained, you’re essentially falling into this garbage in, garbage out trap. That isn’t to say your data is “garbage”. Instead, remind yourself of this anecdote when performing your own experiments and realize that it’s not possible for your deep learning model to perform well on data points it was never trained to recognize in the ﬁrst place.  We also reviewed when you should consider transfer learning versus training your own network from scratch. With small datasets, you should consider feature extraction. For larger datasets, consider ﬁne-tuning ﬁrst  to establish a baseline  and then move on to training a model from scratch.   9. Working with HDF5 and Large Datasets  So far in this book, we’ve only worked with datasets that can ﬁt into the main memory of our machines. For small datasets this is a reasonable assumption – we simply load each individual image, preprocess it, and allow it to be fed through our network. However, for large scale deep learning datasets  e.x., ImageNet , we need to create data generators that access only a portion of the dataset at a time  i.e., a mini-batch , then allow the batch to be passed through the network.  Luckily, Keras ships with methods that allow you to use the raw ﬁle paths on disk as inputs to a training process. You do not have to store the entire dataset in memory – simply supply the image paths to the Keras data generator and your images will be loaded in batches and fed through the network.  However, this method is terribly inefﬁcient. Each and every image residing on your disk requires an I O operation which introduces latency into your training pipeline. Training deep learning networks is already slow enough – we would do well to avoid the I O bottleneck as much as possible.  A more elegant solution would be to generate an HDF5 dataset for your raw images just as we did in Chapter 3 on transfer learning and feature extraction, only this time we are storing the images themselves rather than extracted features. Not only is HDF5 capable of storing massive datasets, but it’s optimized for I O operations, especially for extracting batches  called “slices”  from the ﬁle. As we’ll see throughout the remainder of this book, taking the extra step to pack the raw images residing on disk into an HDF5 ﬁle allows us to construct a deep learning framework that can be used to rapidly build datasets and train deep learning networks on top of them.  In the remainder of this chapter, I’ll demonstrate how to construct an HDF5 dataset for the Kaggle Dogs vs. Cats competition [3]. Then, in the next chapter, we’ll use this HDF5 dataset to train the seminal AlexNet architecture [6], eventually resulting in a top-25 position on the leaderboard in the subsequent chapter.  9.1 Downloading Kaggle: Dogs vs. Cats  To download the Kaggle: Dogs vs. Cats dataset you’ll ﬁrst need to create an account on kaggle.com. From there, head to the Dogs vs. Cats homepage  http:  pyimg.co xb5lb .   96  Chapter 9. Working with HDF5 and Large Datasets  You’ll need need to download train.zip. Do not download test1.zip. The images inside test1.zip are only used for computing predictions and submitting to the Kaggle evaluation server. Since we need the class labels to construct our own training and testing splits we only need train.zip. Submitting your own predicted results is outside the scope of this book but can easily be accomplished by writing your predictions on test1.zip following the ﬁle format outlined in sampleSubmission.csv.  After train.zip has been downloaded, unarchive it and you’ll ﬁnd a directory named train – this directory contains our actual images. The labels themselves can be derived from examining the ﬁle names. I have included a sample of the ﬁle names below:  kaggle_dogs_vs_cats train cat.11866.jpg ... kaggle_dogs_vs_cats train dog.11046.jpg  As I recommended in the Starter Bundle, I’ll be using the following data structure for this  project:  --- hdf5 --- train  --- kaggle_dogs_vs_cats    --- datasets    --- dogs_vs_cats --- config  --- build_dogs_vs_cats.py   --- ...  Notice how I’ll be storing the train directory containing our example images in a folder dedi- cated exclusively to the Kaggle: Dogs vs. Cats competition. From there, I have the dogs_vs_cats directory which is where we’ll be storing the code for this project.  Now that we have downloaded the Dogs vs. Cats dataset and examined our directory structure,  let’s create our conﬁguration ﬁle.  9.2 Creating a Conﬁguration File  Now that we are starting to build more advanced projects and deep learning methods, I like to create a special config Python module for each of my projects. For example, here is the directory structure for the Kaggle Dogs vs. Cats project:  --- __init__.py --- dogs_vs_cats_config.py  --- dogs_vs_cats --- config      --- build_dogs_vs_cats.py   --- crop_accuracy.py --- extract_features.py  --- train_alexnet.py  --- train_model.py   --- output       --- __init__.py --- alexnet_dogs_vs_cats.model   9.2 Creating a Conﬁguration File  97          --- dogs_vs_cats_features.hdf5 --- dogs_vs_cats_mean.json --- dogs_vs_cats.pickle  You can ignore the actual Python scripts for now as we’ll be reviewing them in the next chapter, but take a look at the directory named config. Inside of config you’ll ﬁnd a single Python ﬁle named dogs_vs_cats_config.py – I use this ﬁle to store all relevant conﬁgurations for the project, including:  1. The paths to the input images. 2. The total number of class labels. 3. Information on the training, validation, and testing splits. 4. The paths to the HDF5 datasets. 5. Paths to output models, plots, logs, etc. Using a Python ﬁle rather than a JSON ﬁle allows me to include snippets of Python code and makes the conﬁguration ﬁle more efﬁcient to work with  a great example being manipulating ﬁle paths using the os.path module . I would suggest you get into the habit of using Python-based conﬁguration ﬁles for your own deep learning projects as it will greatly improve your productivity and allow you to control most of the parameters in your project through a single ﬁle.  9.2.1 Your First Conﬁguration File  Let’s go ahead and take a look at my conﬁguration ﬁle  dogs_vs_cats_config.py  for the Kaggle Dogs vs. Cats dataset:   define the paths to the images directory IMAGES_PATH = ".. datasets kaggle_dogs_vs_cats train"   since we do not have validation data or access to the testing  labels we need to take a number of images from the training  data and use them instead NUM_CLASSES = 2 NUM_VAL_IMAGES = 1250 * NUM_CLASSES NUM_TEST_IMAGES = 1250 * NUM_CLASSES   define the path to the output training, validation, and testing  HDF5 files TRAIN_HDF5 = ".. datasets kaggle_dogs_vs_cats hdf5 train.hdf5" VAL_HDF5 = ".. datasets kaggle_dogs_vs_cats hdf5 val.hdf5" TEST_HDF5 = ".. datasets kaggle_dogs_vs_cats hdf5 test.hdf5"  On Line 2 I deﬁne the path to the directory containing the dog and cat images – these are the images that we’ll be packing into a HDF5 dataset later in this chapter. Lines 7-9 deﬁne the total number of class labels  two: one for dog, another for cat  along with the number of validation and testing images  2,500 for each . We can then specify the path to our output HDF5 ﬁles for the training, validation, and testing splits, respectively on Lines 13-15.  The second half of the conﬁguration ﬁle deﬁnes the path to the output serialized weights, the  dataset mean, and a general “output” path to store plots, classiﬁcation reports, logs, etc.:   path to the output model file MODEL_PATH = "output alexnet_dogs_vs_cats.model"  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  17  18  19   Chapter 9. Working with HDF5 and Large Datasets   define the path to the dataset mean DATASET_MEAN = "output dogs_vs_cats_mean.json"   define the path to the output directory used for storing plots,  classification reports, etc. OUTPUT_PATH = "output"  The DATASET_MEAN ﬁle will be used to store the average red, green, and blue pixel intensity values across the entire  training  dataset. When we train our network, we’ll subtract the mean RGB values from every pixel in the image  the same goes for testing and evaluation as well . This method, called mean subtraction, is a type of data normalization technique and is more often used than scaling pixel intensities to the range [0,1] as it’s shown to be more effective on large datasets and deeper neural networks.  9.3 Building the Dataset  Now that our conﬁguration ﬁle has been deﬁned, let’s move on to actually building our HDF5 datasets. Open up a new ﬁle, name it build_dogs_vs_cats.py, and insert the following code:   import the necessary packages from config import dogs_vs_cats_config as config from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from pyimagesearch.preprocessing import AspectAwarePreprocessor from pyimagesearch.io import HDF5DatasetWriter from imutils import paths import numpy as np import progressbar import json import cv2 import os  Lines 2-12 import our required Python packages. I like to import our project conﬁguration ﬁle as the ﬁrst import in the project  Line 2 . This method is a matter of taste, so feel free to place the import wherever you like in the ﬁle. I also rename dogs_vs_cats_config as simply config to make it less verbose when writing code.  From there, the rest of the imports you have encountered before in previous chapters; how- ever, I would like to draw your attention to the HDF5DatasetWriter on Line 6, the very same HDF5DatasetWriter we deﬁned in Chapter 3 – this class will be used to pack our raw images on disk into a single, serialized ﬁle.  We’ll also again be using the progressbar module, a simple utility library I like to use when measuring the approximate time a given task is taking. This module is totally irrelevant to deep learning, but again, I ﬁnd it convenient to use as for large datasets it may take several hours to pack a dataset of images into HDF5 format.  Next, let’s grab the paths to the images in the Kaggle Dogs vs. Cats dataset:   grab the paths to the images trainPaths = list paths.list_images config.IMAGES_PATH   trainLabels = [p.split os.path.sep [2].split "." [0]  for p in trainPaths]  98  20  21  22  23  24  25  1  2  3  4  5  6  7  8  9  10  11  12  14  15  16  17   9.3 Building the Dataset  99  18  19  le = LabelEncoder   trainLabels = le.fit_transform trainLabels   The Dogs vs. Cats dataset has the following example directory structure:  kaggle_dogs_vs_cats train cat.11866.jpg ... kaggle_dogs_vs_cats train dog.11046.jpg  Notice how the name of the class is built-into the actual ﬁlename. Therefore, we need to extract the ﬁle component of the ﬁle path, split on the . separator, and extract the class name – in fact, that is exactly what Lines 16 and 17 do.  Given the paths to all images in the dataset, they loop over them individually and extract the labels from the ﬁle paths. If you ﬁnd these lines of code confusing, I would suggest taking a second now to manually play with the code, speciﬁcally the os.path.sep variable and the .split function used on the ﬁle path string to further see how these utilities are used to manipulate ﬁle paths.  Lines 18 and 19 then encode the class labels. For the Kaggle Dogs vs. Cats project we’ll need  three splits: a training split, a validation split, and a testing split. Our next code block handles generating each of these splits:   perform stratified sampling from the training set to build the  testing split from the training data split = train_test_split trainPaths, trainLabels,  test_size=config.NUM_TEST_IMAGES, stratify=trainLabels, random_state=42    trainPaths, testPaths, trainLabels, testLabels  = split   perform another stratified sampling, this time to build the  validation data split = train_test_split trainPaths, trainLabels,  test_size=config.NUM_VAL_IMAGES, stratify=trainLabels, random_state=42    trainPaths, valPaths, trainLabels, valLabels  = split  On Lines 23-26 we take our input images and labels and use them to construct the training and testing split. However, we need to perform another split on Lines 30-33 to create the validation set. The validation set is  almost always  taken from the training data. The size of the testing and validation splits are controlled via the NUM_TEST_IMAGES and NUM_VAL_IMAGES, each of which are deﬁned in our config ﬁle.  Now that we have our training, testing, and validation splits, let’s create a simple list that will  allow us to loop over them and efﬁciently write the images in each dataset to our HDF5 ﬁle:   construct a list pairing the training, validation, and testing  image paths along with their corresponding labels and output HDF5  files datasets = [   "train", trainPaths, trainLabels, config.TRAIN_HDF5 ,  "val", valPaths, valLabels, config.VAL_HDF5 ,  "test", testPaths, testLabels, config.TEST_HDF5 ]  21  22  23  24  25  26  27  28  29  30  31  32  33  35  36  37  38  39  40  41   100  Chapter 9. Working with HDF5 and Large Datasets   initialize the image preprocessor and the lists of RGB channel  averages aap = AspectAwarePreprocessor 256, 256   R, G, B  =  [], [], []   On Line 38 we deﬁne a datasets list that includes our training, validation, and testing  variables. Each entry in the list is a 4-tuple consisting of:  1. The name of the split  i.e., training, testing, or validation . 2. The respective image paths for the split. 3. The labels for the split. 4. The path to the output HDF5 ﬁle for the split. We then initialize our AspectAwarePreprocessor on Line 45 used to resize images to 256× 256 pixels  keeping the aspect ratio of the image in mind  prior to being written to HDF5. We’ll also initialize three lists on Line 46 – R, G, and B, used to store the average pixel intensities for each channel.  Finally, we are ready to build our HDF5 datasets:   loop over the dataset tuples for  dType, paths, labels, outputPath  in datasets:   create HDF5 writer print "[INFO] building {}...".format outputPath   writer = HDF5DatasetWriter  len paths , 256, 256, 3 , outputPath    initialize the progress bar widgets = ["Building Dataset: ", progressbar.Percentage  , " ",  progressbar.Bar  , " ", progressbar.ETA  ]  pbar = progressbar.ProgressBar maxval=len paths ,  widgets=widgets .start    On Line 49 we start looping over each of the 4-tuple values in the datasets list. For each data split, we instantiate the HDF5DatasetWriter on Line 52. Here the dimensions of the output dataset will be the  len paths , 256, 256, 3 , implying there are len paths  total images, each of them with a width of 256 pixels, a height of 256 pixels, and 3 channels.  Lines 54-58 then initialize our progress bar so we can easily monitor the process of the dataset generation. Again, this code block  along with the rest of the progressbar function calls  is entirely optional, so feel free to comment them out if you so wish. Next, let’s write each image in a given data split to the writer:   loop over the image paths for  i,  path, label   in enumerate zip paths, labels  :   load the image and process it image = cv2.imread path  image = aap.preprocess image    if we are building the training dataset, then compute the  mean of each channel in the image, then update the  respective lists if dType == "train":   b, g, r  = cv2.mean image [:3] R.append r   42  43  44  45  46  48  49  50  51  52  53  54  55  56  57  58  60  61  62  63  64  65  66  67  68  69  70  71   9.3 Building the Dataset  101  72  73  74  75  76  77  78  79  80  81  G.append g  B.append b    add the image and label  to the HDF5 dataset writer.add [image], [label]  pbar.update i    close the HDF5 writer pbar.finish   writer.close    On Line 61 we start looping over each individual image and corresponding class label in the data split. Lines 63 and 64 load the image from disk and then apply our aspect-aware preprocessor to resize the image to 256× 256 pixels.  We make a check on Line 69 to see if we are examining the train data split and, if so, we compute the average of the Red, Green, and Blue channels  Line 70  and update their respective lists on Lines 71-73. Computing the average of the RGB channels is only done for the training set and is a requirement if we wish to apply mean subtraction normalization.  Line 76 adds the corresponding image and label to our HDF5DatasetWriter. Once all images in the data split have been serialized to the HDF5 dataset, we close the writer on Line 81.  The ﬁnal step is to serialize our RGB averages to disk:  83  84  85  86  87  88  89   construct a dictionary of averages, then serialize the means to a  JSON file print "[INFO] serializing means..."  D = {"R": np.mean R , "G": np.mean G , "B": np.mean B } f = open config.DATASET_MEAN, "w"  f.write json.dumps D   f.close    Line 86 constructs a Python dictionary of the average RGB values over all images in the training set. Keep in mind that each individual R, G, and B contains the average of channel for each image in the dataset. Computing the mean of this list gives us the average pixel intensity value for all images in the list. This dictionary is then serialized to disk in JSON format on Lines 87-88.  Let’s go ahead and serialize the Kaggle Dogs vs. Cats dataset to HDF5 format. Open up a  terminal and then issue the following command:  $ python build_dogs_vs_cats.py [INFO] building kaggle_dogs_vs_cats hdf5 train.hdf5... Building Dataset: 100%  Time: 0:02:39 [INFO] building kaggle_dogs_vs_cats hdf5 val.hdf5... Building Dataset: 100%  Time: 0:00:20 [INFO] building kaggle_dogs_vs_cats hdf5 test.hdf5... Building Dataset: 100%  Time: 0:00:19  As you can see from my output, an HDF5 ﬁle was created for each of the training, testing, and validation splits. The training split generation took the longest to generate as this split contained the most data  2m39s . The testing and validation splits took substantially less time  ≈ 20s  due the fact that there is less data in these splits.  We can see each of these output ﬁles on our disk by listing the contents of the hdf5 directory:   102  Chapter 9. Working with HDF5 and Large Datasets  $ ls -l .. datasets kaggle_dogs_vs_cats hdf5  total 38400220 3932182144 Apr -rw-rw-r-- 1 adrian adrian -rw-rw-r-- 1 adrian adrian 31457442144 Apr -rw-rw-r-- 1 adrian adrian 3932182144 Apr  7 18:00 test.hdf5 7 17:59 train.hdf5 7 18:00 val.hdf5  Looking at these ﬁle sizes you might be a bit surprised. The raw Kaggle Dogs vs. Cats images residing on disk are only 595MB – why are the .hdf5 ﬁles so large? The train.hdf5 ﬁle alone is 31.45GB while the test.hdf5 and val.hdf5 ﬁles are almost 4GB. Why?  Well, keep in mind that raw image ﬁle formats such as JPEG and PNG apply data compression algorithms to keep image ﬁle sizes small. However, we have effectively removed any type of com- pression and are storing the images as raw NumPy arrays  i.e., bitmaps . This lack of compression dramatically inﬂates our storage costs, but will also help speed up our training time as we won’t have to waste processor time decoding the image – we can instead access the image directly from the HDF5 dataset, preprocess it, and pass it through our network.  Let’s also take a look at our RGB mean ﬁle:  $ cat output dogs_vs_cats_mean.json {"B": 106.13178224639893, "R": 124.96761639328003, "G": 115.97504255599975}  Here we can see that the red channel has an average pixel intensity of 124.96 across all images in the dataset. The blue channel has an average of 106.13 and the green channel an average of 115.97. We’ll be constructing a new image preprocessor to normalize our images by subtracting these RGB averages from the input images prior to passing them through our network. This mean normalization helps “center” the data around the zero mean. Typically, this normalization enables our network to learn faster and is also why we use this type of normalization  rather than [0,1] scaling  on larger, more challenging datasets.  9.4 Summary  In this chapter, we learned how to serialize raw images into an HDF5 dataset suitable for training a deep neural network. The reason we serialized the raw images into an HDF5 ﬁle rather than simply accessing mini-batches of image paths on disk when training is due to I O latency – for each image on disk we would have to perform an I O operation to read the image. This subtle optimization doesn’t seem like a big deal, but I O latency is a huge problem in a deep learning pipeline – the training process is already slow enough, and if we make it hard for our networks to access our data, we are only further shooting ourselves in the foot.  Conversely, if we serialize all images into an efﬁciently packed HDF5 ﬁle, we can leverage very fast array slices to extract our mini-batches, thereby dramatically reducing I O latency and helping speed up the training process. Whenever you are using the Keras library and working with a dataset too large to ﬁt into memory, be sure you consider serializing your dataset into HDF5 format ﬁrst – as we’ll ﬁnd out in the next chapter, it makes training your network an easier  and more efﬁcient  task.   10. Competing in Kaggle: Dogs vs. Cats  In our previous chapter, we learned how to work with HDF5 and datasets too large to ﬁt into memory. To do so, we deﬁned a Python utility script that can be used to take an input dataset of images and serialize them into a highly efﬁcient HDF5 dataset. Representing a dataset of images in an HDF5 dataset allows us to avoid issues of I O latency, thereby speeding up the training process. For example, if we deﬁned a dataset generator that loaded images sequentially from disk, we would need N read operations, one for each image. However, by placing our dataset of images into an HDF5 dataset, we can instead load batches of images using a single read. This action dramatically reduces the number of I O calls and allows us to work with very large image datasets. In this chapter, we are going to extend our work and learn how to deﬁne an image generator for HDF5 datasets suitable for training Convolutional Neural Networks with Keras. This generator will open the HDF5 dataset, yield batches of images and associated training labels for the network to be trained on, and proceed to do so until our model reaches sufﬁciently low loss high accuracy.  To accomplish this process, we’ll ﬁrst explore three new image pre-processors designed to increase classiﬁcation accuracy – mean subtraction, patch extraction, and cropping  also called 10-cropping or over-sampling . Once we’ve deﬁned our new set of pre-processors, we’ll move on deﬁning the actual HDF5 dataset generator.  From there, we’ll implement the seminal AlexNet architecture from Krizhevsky et al.’s 2012 paper, ImageNet Classiﬁcation with Deep Convolutional Neural Networks [6]. This implementation of AlexNet will then be trained on the Kaggle Dogs vs. Cats challenge. Given the trained model, we’ll evaluate its performance on the testing set, followed by using over-sampling methods to boost classiﬁcation accuracy further. As our results will demonstrate, our network architecture + cropping methods will enable us to obtain a position in the top-25 leaderboard of the Kaggle Dogs vs. Cats challenge.  10.1 Additional Image Preprocessors  In this section we’ll implement two new image pre-preprocessors:  1. A mean subtraction pre-processor designed to subtract the mean Red, Green, and Blue pixel  intensities across a dataset from an input image  which is a form of data normalization .   104  Chapter 10. Competing in Kaggle: Dogs vs. Cats 2. A patch preprocessor used to randomly extract M × N pixel regions from an image during  training.  3. An over-sampling pre-processor used at testing time to sample ﬁve regions of an input image  the four corners + center area  along with their corresponding horizontal ﬂips  for a total of 10 crops .  Using over-sampling, we can boost our classiﬁcation accuracy by passing the 10 crops through  our CNN and then averaging across the 10 predictions.  10.1.1 Mean Preprocessing  Let’s get started with the mean pre-processor. In Chapter 9 we learned how to convert an image dataset to HDF5 format – part of this conversion involved computing the average Red, Green, and Blue pixel intensities across all images in the entire dataset. Now that we have these averages, we are going to perform a pixel-wise subtraction of these values from our input images as a form of data normalization. Given an input image I and its R, G, B channels, we can perform mean subtraction via:   R = R− µR   G = G− µG   B = B− µB Where µR, µG, and µB are computed when the image dataset is converted to HDF5 format. Figure 10.1 includes a visualization of subtracting the mean RGB values from an input image – notice how the subtraction is done pixel-wise.  Figure 10.1: An example of applying mean subtraction to an input image  left  by subtracting R = 124.96, G = 115.97, B = 106.13 pixel-wise, resulting in the output image  right . Mean subtraction is used to reduce the affects of lighting variations during classiﬁcation.  To make this concept more concrete, let’s go ahead and implement our MeanPreprocessor  class:  --- pyimagesearch            --- __init__.py --- callbacks --- nn --- preprocessing      --- utils  --- __init__.py --- aspectawarepreprocessor.py --- imagetoarraypreprocessor.py --- meanpreprocessor.py --- simplepreprocessor.py   10.1 Additional Image Preprocessors  105  Notice how I have placed a new ﬁle named meanpreprocessor.py in the preprocessing sub-module of pyimagesearch – this location is where our MeanPreprocessor class will live. Let’s go ahead and implement this class now:   import the necessary packages import cv2  class MeanPreprocessor:  def __init__ self, rMean, gMean, bMean :   store the Red, Green, and Blue channel averages across a  training set self.rMean = rMean self.gMean = gMean self.bMean = bMean  Line 5 deﬁnes the constructor to the MeanPreprocessor, which requires three arguments – the respective Red, Green, and Blue averages computed across the entire dataset. These values are then stored on Lines 8-10.  Next, let’s deﬁne the preprocess method, a required function for every pre-processor we  intend to apply to our image processing pipeline:  def preprocess self, image :   split the image into its respective Red, Green, and Blue  channels  B, G, R  = cv2.split image.astype "float32"     subtract the means for each channel R -= self.rMean G -= self.gMean B -= self.bMean   merge the channels back together and return the image return cv2.merge [B, G, R]   1  2  3  4  5  6  7  8  9  10  12  13  14  15  16  17  18  19  20  21  22  23  Line 15 uses the cv2.split function to split our input image into its respective RGB com- ponents. Keep in mind that OpenCV represents images in BGR order rather than RGB  [38], http:  pyimg.co ppao , hence why our return tuple has the signature  B, G, R  rather than  R, G, B . We’ll also ensure that these channels are of a ﬂoating point data type as OpenCV images are typically represented as unsigned 8-bit integers  in which case we can’t have negative values, and modulo arithmetic would be performed instead .  Lines 17-20 perform the mean subtraction itself, subtracting the respective mean RGB values from the RGB channels of the input image. Line 23 then merges the normalized channels back together and returns the resulting image to the calling function.  10.1.2 Patch Preprocessing  The PatchPreprocessor is responsible for randomly sampling M× N regions of an image during the training process. We apply patch preprocessing when the spatial dimensions of our input images are larger than what the CNN expects – this is a common technique to help reduce overﬁtting, and is, therefore, a form of regularization. Instead of using the entire image during training, we instead crop a random portion of it and pass it to the network  see Figure 10.2 for an example of crop preprocessing .   106  Chapter 10. Competing in Kaggle: Dogs vs. Cats  Figure 10.2: Left: Our original 256× 256 input image. Right: Randomly cropping a 227× 227 region from the image.  Applying this cropping implies that a network never sees the exact same image  unless by random happenstance , similar to data augmentation. As you know from our previous chapter, we constructed an HDF5 dataset of Kaggle Dogs vs. Cats images where each image is 256×256 pixels. However, the AlexNet architecture that we’ll be implementing later in this chapter can only accept images of size 227× 227 pixels. So, what are we to do? Apply a SimplePreprocessor to resize our each of the 256× 256 pixels down to 227 × 227? No, that would be wasteful, especially since this is an excellent opportunity to perform data augmentation by randomly cropping a 227× 227 region from the 256× 256 image during training – in fact, this process is exactly how Krizhevsky et al. trains AlexNet on the ImageNet dataset.  The PatchPreprocessor, just like all other image pre-processors, will be sorted in the  preprocessing sub-module of pyimagesearch:  --- pyimagesearch             --- __init__.py --- callbacks --- nn --- preprocessing       --- utils  --- __init__.py --- aspectawarepreprocessor.py --- imagetoarraypreprocessor.py --- meanpreprocessor.py --- patchpreprocessor.py --- simplepreprocessor.py  Open up the patchpreprocessor.py ﬁle and let’s deﬁne the PatchPreprocessor class:  1  2  3  4  5  6  7  8   import the necessary packages from sklearn.feature_extraction.image import extract_patches_2d  class PatchPreprocessor:  def __init__ self, width, height :   store the target width and height of the image self.width = width self.height = height   10.1 Additional Image Preprocessors  107  Line 5 deﬁnes the construct to PatchPreprocessor – we simply need to supply the target  width and height of the cropped image.  We can then deﬁne the preprocess function:  10  11  12  13  14  def preprocess self, image :   extract a random crop from the image with the target width  and height return extract_patches_2d image,  self.height, self.width ,  max_patches=1 [0]  Extracting a random patches of size self.width x self.height is easy using the extract_patches_2d  function from the scikit-learn library. Given an input image, this function randomly extracts a patch from image. Here we supply max_patches=1, indicating that we only need a single random patch from the input image.  The PatchPreprocessor class doesn’t seem like much, but it’s actually a very effective method to avoid overﬁtting by applying yet another layer of data augmentation. We’ll be using the PatchPreprocessor when training AlexNet. The next pre-processor, CropPreprocessor, will be used when evaluating our trained network.  10.1.3 Crop Preprocessing  Next, we need to deﬁne a CropPreprocessor responsible for computing the 10-crops for over- sampling. During the evaluating phase of our CNN, we’ll crop the four corners of the input image + the center region and then take their corresponding horizontal ﬂips, for a total of ten samples per input image  Figure 10.3 .  Figure 10.3: Left: The original 256× 256 input image. Right: Applying the 10-crop prepro- cessor to extract ten 227× 227 crops of the image including the center, four corners, and their corresponding horizontal mirrors.  These ten samples will be passed through the CNN, and then the probabilities averaged. Applying this over-sampling method tends to include 1-2 percent increases in classiﬁcation accuracy  and in some cases, even higher .  The CropPreprocessor class will also live in the preprocessing sub-module of pyimagesearch:  --- pyimagesearch     --- __init__.py --- callbacks --- nn   Chapter 10. Competing in Kaggle: Dogs vs. Cats  108            --- preprocessing        --- utils  --- __init__.py --- aspectawarepreprocessor.py --- croppreprocessor.py --- imagetoarraypreprocessor.py --- meanpreprocessor.py --- patchpreprocessor.py --- simplepreprocessor.py  Open up the croppreprocessor.py ﬁle and let’s deﬁne it:   import the necessary packages import numpy as np import cv2  class CropPreprocessor:  1  2  3  4  5  6  7  8  9  10  11  12  13  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  def __init__ self, width, height, horiz=True, inter=cv2.INTER_AREA :   store the target image width, height, whether or not  horizontal flips should be included, along with the  interpolation method used when resizing self.width = width self.height = height self.horiz = horiz self.inter = inter  Line 6 deﬁnes the constructor to to CropPreprocessor. The only required arguments are the target width and height of each cropped region. We can also optionally specify whether horizontal ﬂipping should be applied  defaults to True  along with the interpolation algorithm OpenCV will use for resizing. These arguments are all stored inside the class for use within the preprocess method.  Speaking of which, let’s deﬁne the preprocess method now:  def preprocess self, image :   initialize the list of crops crops = []   grab the width and height of the image then use these  dimensions to define the corners of the image based  h, w  = image.shape[:2] coords = [  [0, 0, self.width, self.height], [w - self.width, 0, w, self.height], [w - self.width, h - self.height, w, h], [0, h - self.height, self.width, h]]   compute the center crop of the image as well dW = int 0.5 *  w - self.width   dH = int 0.5 *  h - self.height   coords.append [dW, dH, w - dW, h - dH]   The preprocess method requires only a single argument – the image which we are going to apply over-sampling. We grab the width and height of the input image on Line 21, which then   10.2 HDF5 Dataset Generators  109  allows us to compute the  x,y -coordinates of the four corners  top-left, top-right, bottom-right, bottom-left, respectively  on Lines 22-26. The center crop of the image is then computed on Lines 29 and 30, then added to the list of coords on Line 31.  We are now ready to extract each of the crops:   loop over the coordinates, extract each of the crops,  and resize each of them to a fixed size for  startX, startY, endX, endY  in coords: crop = image[startY:endY, startX:endX] crop = cv2.resize crop,  self.width, self.height ,  interpolation=self.inter   crops.append crop   On Line 35 we loop over each of the starting and ending  x,y -coordinates of the rectangular crops. Line 36 extracts the crop via NumPy array slicing which we then resize on Line 37 to ensure the target width and height dimensions are met. The crop is the added to the crops list. In the case that horizontal mirrors are to be computed, we can ﬂip each of the ﬁve original  crops, leaving us with ten crops overall:   check to see if the horizontal flips should be taken if self.horiz:   compute the horizontal mirror flips for each crop mirrors = [cv2.flip c, 1  for c in crops] crops.extend mirrors    return the set of crops return np.array crops   33  34  35  36  37  38  39  41  42  43  44  45  46  47  48  The array of crops is then returned to the calling function on Line 48. Using both the MeanPreprocessor for normalization and the CropPreprocessor for oversampling, we’ll be able to obtain higher classiﬁcation accuracy than is otherwise possible.  10.2 HDF5 Dataset Generators  Before we can implement the AlexNet architecture and train it on the Kaggle Dogs vs. Cats dataset, we ﬁrst need to deﬁne a class responsible for yielding batches of images and labels from our HDF5 dataset. Chapter 9 discussed how to convert a set of images residing on disk into an HDF5 dataset – but how do we get them back out again?  The answer is to deﬁne an HDF5DatasetGenerator class in the io sub-module of pyimagesearch:  --- pyimagesearch           --- __init__.py --- callbacks --- io    --- nn --- preprocessing --- utils  --- __init__.py --- hdf5datasetgenerator.py --- hdf5datasetwriter.py   110  Chapter 10. Competing in Kaggle: Dogs vs. Cats  Previously, all of our image datasets could be loaded into memory so we could rely on Keras generator utilities to yield our batches of images and corresponding labels. However, now that our datasets are too large to ﬁt into memory, we need to handle implementing this generator ourselves.  Go ahead and open the hdf5datasetgenerator.py ﬁle and we’ll get to work:  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21   import the necessary packages from keras.utils import np_utils import numpy as np import h5py  class HDF5DatasetGenerator:  def __init__ self, dbPath, batchSize, preprocessors=None,  aug=None, binarize=True, classes=2 :  store the batch size, preprocessors, and data augmentor,  whether or not the labels should be binarized, along with  the total number of classes self.batchSize = batchSize self.preprocessors = preprocessors self.aug = aug self.binarize = binarize self.classes = classes   open the HDF5 database for reading and determine the total  number of entries in the database self.db = h5py.File dbPath  self.numImages = self.db["labels"].shape[0]  ImageToArrayPreprocessor, etc. .  On Line 7 we deﬁne the constructor to our HDF5DatasetGenerator. This class accepts a number of arguments, two of which are required and the rest optional. I have detailed each of the arguments below:   dbPath: The path to our HDF5 dataset that stores our images and corresponding class labels.   batchSize: The size of mini-batches to yield when training our network.   preprocessors: The list of image preprocessors we are going to apply  i.e., MeanPreprocessor,   aug: Defaulting to None, we could also supply a Keras ImageDataGenerator to apply data   binarize: Typically we will store class labels as single integers inside our HDF5 dataset; however, as we know, if we are applying categorical cross-entropy or binary cross-entropy as our loss function, we ﬁrst need to binarize the labels as one-hot encoded vectors – this switch indicates whether or not this binarization needs to take place  which defaults to True .   classes: The number of unique class labels in our dataset. This value is required to  augmentation directly inside our HDF5DatasetGenerator.  accurately construct our one-hot encoded vectors during the binarization phase.  These variables are stored on Lines 12-16 so we can access them from the rest of the class. Line 20 opens a ﬁle pointer to our HDF5 dataset ﬁle Line 21 creates a convenience variable used to access the total number of data points in the dataset.  Next, we need to deﬁne a generator function, which as the name suggests, is responsible for yielding batches of images and class labels to the Keras .fit_generator function when training a network:  23  24  def generator self, passes=np.inf :   initialize the epoch count   25  26  27  28  29  30  31  32  33  34  36  37  38  39  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  10.2 HDF5 Dataset Generators  111  epochs = 0   keep looping infinitely -- the model will stop once we have  reach the desired number of epochs while epochs < passes:   loop over the HDF5 dataset for i in np.arange 0, self.numImages, self.batchSize :   extract the images and labels from the HDF dataset images = self.db["images"][i: i + self.batchSize] labels = self.db["labels"][i: i + self.batchSize]  Line 23 deﬁnes the generator function which can accept an optional argument, passes. Think of the passes value as the total number of epochs – in most cases, we don’t want our generator to be concerned with the total number of epochs; our training methodology  ﬁxed number of epochs, early stopping, etc.  should be responsible for that. However, in certain situations, it’s often helpful to provide this information to the generator.  On Line 29 we start looping over the number of desired epochs – by default, this loop will run  indeﬁnitely until either:  1. Keras reaches training termination criteria. 2. We explicitly stop the training process  i.e., ctrl + c . Line 31 starts looping over each batch of data points in the dataset. We extract the images and  labels of size batchSize from our HDF5 dataset on Lines 33 and 34.  Next, let’s check to see if the labels should be one-hot encoded:   check to see if the labels should be binarized if self.binarize:  labels = np_utils.to_categorical labels,  self.classes   We can then also see if any image preprocessors should be applied:   check to see if our preprocessors are not None if self.preprocessors is not None:   initialize the list of processed images procImages = []   loop over the images for image in images:   loop over the preprocessors and apply each  to the image for p in self.preprocessors:  image = p.preprocess image    update the list of processed images procImages.append image    update the images array to be the processed  images images = np.array procImages   Provided the preprocessors is not None  Line 42 , we loop over each of the images in the batch and apply each of the preprocessors by calling the preprocess method on the individual image. Doing this enables us to chain together multiple image pre-processors.   60  61  62  63  65  66  67  68  69  70  71  72  73  112  Chapter 10. Competing in Kaggle: Dogs vs. Cats  For example, our ﬁrst pre-processor may resize the image to a ﬁxed size via our SimplePreprocessor  class. From there we may perform mean subtraction via the MeanPreprocessor. And after that, we’ll need to convert the image to a Keras-compatible array using the ImageToArrayPreprocessor. At this point it should be clear why we deﬁned all of our pre-processing classes with a preprocess method – it allows us to chain our pre-processors together inside the data generator. The prepro- cessed images are then converted back to a NumPy array on Line 58.  Provided we supplied an instance of aug, an ImageDataGenerator class used for data aug-  mentation, we’ll also want to apply data augmentation to the images as well:   if the data augmenator exists, apply it if self.aug is not None:   images, labels  = next self.aug.flow images,  labels, batch_size=self.batchSize    Finally, we can yield a 2-tuple of the batch of images and labels to the calling Keras generator:   yield a tuple of images and labels yield  images, labels    increment the total number of epochs epochs += 1  def close self :   close the database self.db.close    Line 69 increments our total number of epochs after all mini-batches in the dataset have been processed. The close method on Lines 71-73 is simply responsible for closing the pointer to the HDF5 dataset.  Admittedly, implementing the HDF5DatasetGenerator may not “feel” like we’re doing any deep learning. After all, isn’t this just a class responsible for yielding batches of data from a ﬁle? Technically, yes, that is correct. However, keep in mind that practical deep learning is more than just deﬁning a model architecture, initializing an optimizer, and applying it to a dataset.  In reality, we need extra tools to help facilitate our ability to work with datasets, espe- cially datasets that are too large to ﬁt into memory. As we’ll see throughout the rest of this book, our HDF5DatasetGenerator will come in handy a number of times – and when you start creating your own deep learning applications experiments, you’ll feel quite lucky to have it in your repertoire.  10.3 Implementing AlexNet  Let’s now move on to implement the seminal AlexNet architecture by Krizhevsky et al. A table summarizing the AlexNet architecture can be seen in Table 10.1. Notice how our input images are assumed to be 227× 227× 3 pixels – this is actually the correct input size for AlexNet. As mentioned in Chapter 9, in the original publication, Krizhevsky et al. reported the input spatial dimensions to be 224× 224× 3; however, since we know 224× 224 cannot possible by tiled with an 11× 1 kernel, we assume there was likely a typo in the publication, and 224× 224 should actually be 227× 227. The ﬁrst block of AlexNet applies 96, 11× 11 kernels with a stride of 4× 4, followed by a RELU activation and max pooling with a pool size of 3× 3 and strides of 2× 2, resulting in an output volume of size 55× 55.   10.3 Implementing AlexNet  113  Filter Size   Stride 11× 11 4× 4,K = 96  3× 3 2× 2 5× 5,K = 256  3× 3 2× 2 3× 3,K = 384  3× 3,K = 384  3× 3,K = 256  3× 3 2× 2  Output Size Layer Type INPUT IMAGE 227× 227× 3 57× 57× 96 CONV 57× 57× 96 ACT 57× 57× 96 BN 16× 16× 96 POOL 28× 28× 96 DROPOUT 28× 28× 256 CONV 28× 28× 256 ACT 28× 28× 256 BN 13× 13× 256 POOL 13× 13× 256 DROPOUT 13× 13× 384 CONV 13× 13× 384 ACT 13× 13× 384 BN 13× 13× 384 CONV 13× 13× 384 ACT 13× 13× 384 BN 13× 13× 256 CONV 13× 13× 256 ACT 13× 13× 256 BN 13× 13× 256 POOL 6× 6× 256 DROPOUT 4096 FC 4096 ACT BN 4096 4096 DROPOUT 4096 FC 4096 ACT BN 4096 4096 DROPOUT 1000 FC SOFTMAX 1000  Table 10.1: A table summary of the AlexNet architecture. Output volume sizes are included for each layer, along with convolutional ﬁlter size pool size when relevant.   114  Chapter 10. Competing in Kaggle: Dogs vs. Cats We then apply a second CONV => RELU => POOL layer this, this time using 256, 5× 5 ﬁlters with 1×1 strides. After applying max pooling again with a pool size of 3×3 and strides of 2×2 we are left with a 13× 13 volume. Next, we apply  CONV => RELU  * 3 => POOL. The ﬁrst two CONV layers learn 384, 3× 3 ﬁlters while the ﬁnal CONV learns 256, 3× 3 ﬁlters.  After another max pooling operation, we reach our two FC layers, each with 4096 nodes and  RELU activations in between. The ﬁnal layer in the network is our softmax classiﬁer.  When AlexNet was ﬁrst introduced we did not have techniques such as batch normalization – in our implementation ,we are going to include batch normalization after the activation, as is standard for the majority of image classiﬁcation tasks using Convolutional Neural Networks. We’ll also include a very small amount of dropout after each POOL operation to further help reduce overﬁtting. To implement AlexNet, let’s create a new ﬁle named alexnet.py in the conv sub-module of  nn in pyimagesearch:  --- __init__.py --- callbacks --- io --- nn      --- pyimagesearch         ...    --- preprocessing --- utils  --- __init__.py --- conv    --- __init__.py --- alexnet.py  From there, open up alexnet.py, and we’ll implement this seminal architecture:   import the necessary packages from keras.models import Sequential from keras.layers.normalization import BatchNormalization from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D from keras.layers.core import Activation from keras.layers.core import Flatten from keras.layers.core import Dropout from keras.layers.core import Dense from keras.regularizers import l2 from keras import backend as K  Lines 2-11 import our required Keras classes – we have used all of these layers before in previ- ous chapters of this book so I’m going to skip explicitly describing each of them. The only import I do want to draw your attention to is Line 10 where we import the l2 function – this method will be responsible for applying L2 weight decay to the weight layers in the network.  Now that our imports are taken care of, let’s start the deﬁnition of AlexNet:  class AlexNet:  @staticmethod def build width, height, depth, classes, reg=0.0002 :   initialize the model along with the input shape to be  1  2  3  4  5  6  7  8  9  10  11  13  14  15  16   10.3 Implementing AlexNet  115  17  18  19  20  21  22  23  24  25  26  28  29  30  31  32  33  34  35  37  38  39  40  41  42  43   "channels last" and the channels dimension itself model = Sequential   inputShape =  height, width, depth  chanDim = -1   if we are using "channels first", update the input shape  and channels dimension if K.image_data_format   == "channels_first":  inputShape =  depth, height, width  chanDim = 1  Line 15 deﬁnes the build method of AlexNet. Just like in all previous examples in this book, the build method is required for constructing the actual network architecture and returning it to the calling function. This method accepts four arguments: the width, height, and depth of the input images, followed by the total number of class labels in the dataset. An optional parameter, reg, controls the amount of L2 regularization we’ll be applying to the network. For larger, deeper networks, applying regularization is critical to reducing overﬁtting while increasing accuracy on the validation and testing sets.  Line 18 initializes the model itself along with the inputShape and channel dimension assum- ing we are using “channels last” ordering. If we are instead using “channels ﬁrst” ordering, we update inputShape and chanDim  Lines 24-26 .  Let’s now deﬁne the ﬁrst CONV => RELU => POOL layer set in the network:   Block 1: first CONV => RELU => POOL layer set model.add Conv2D 96,  11, 11 , strides= 4, 4 ,  input_shape=inputShape, padding="same", kernel_regularizer=l2 reg     model.add Activation "relu"   model.add BatchNormalization axis=chanDim   model.add MaxPooling2D pool_size= 3, 3 , strides= 2, 2    model.add Dropout 0.25    Our ﬁrst CONV layer will learn 96 ﬁlters, each of size 11× 11  Lines 28 and 29 , using a stride of 4× 4. By applying the kernel_regularizer parameter to the Conv2D class, we can apply our L2 weight regularization parameter – this regularization will be applied to all CONV and FC layers in the network.  A ReLU activation is applied after our CONV, followed by a BatchNormalization  Lines 32 and 33 . The MaxPooling2D is then applied to reduce our spatial dimensions  Line 34 . We’ll also apply dropout with a small probability  25 percent  to help reduce overﬁtting  Lines 35 . 256 ﬁlters, each of size 5× 5:  The following code block deﬁnes another CONV => RELU => POOL layer set, this time learning   Block 2: second CONV => RELU => POOL layer set model.add Conv2D 256,  5, 5 , padding="same",  kernel_regularizer=l2 reg     model.add Activation "relu"   model.add BatchNormalization axis=chanDim   model.add MaxPooling2D pool_size= 3, 3 , strides= 2, 2    model.add Dropout 0.25    Deeper, richer features are learned in the third block of AlexNet where we stack multiple CONV  => RELU together prior to applying a POOL operation:   45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78  79  116  Chapter 10. Competing in Kaggle: Dogs vs. Cats   Block 3: CONV => RELU => CONV => RELU => CONV => RELU model.add Conv2D 384,  3, 3 , padding="same",  kernel_regularizer=l2 reg     model.add Activation "relu"   model.add BatchNormalization axis=chanDim   model.add Conv2D 384,  3, 3 , padding="same",  kernel_regularizer=l2 reg     model.add Activation "relu"   model.add BatchNormalization axis=chanDim   model.add Conv2D 256,  3, 3 , padding="same",  kernel_regularizer=l2 reg     model.add Activation "relu"   model.add BatchNormalization axis=chanDim   model.add MaxPooling2D pool_size= 3, 3 , strides= 2, 2    model.add Dropout 0.25    The ﬁrst two CONV ﬁlters learn 384, 3× 3 ﬁlters while the third CONV learns 256, 3× 3 ﬁlters. Again, stacking multiple CONV =>RELU layers on to of each other prior to applying a destructive POOL layer enables our network to learn richer, and potentially more discriminating features.  From there we collapse our multi-dimensional representation down into a standard feedforward  network using two fully-connected layers  4096 nodes each :   Block 4: first set of FC => RELU layers model.add Flatten    model.add Dense 4096, kernel_regularizer=l2 reg    model.add Activation "relu"   model.add BatchNormalization    model.add Dropout 0.5     Block 5: second set of FC => RELU layers model.add Dense 4096, kernel_regularizer=l2 reg    model.add Activation "relu"   model.add BatchNormalization    model.add Dropout 0.5    Batch normalization is applied after each activation in the FC layer sets, just as in the CONV layers above. Dropout, with a larger probability of 50 percent, is applied after every FC layer set, as is standard with the vast majority of CNNs.  Finally, we deﬁne the softmax classiﬁer using the desired number of classes and return the  resulting model to the calling function:   softmax classifier model.add Dense classes, kernel_regularizer=l2 reg    model.add Activation "softmax"     return the constructed network architecture return model  As you can see, implementing AlexNet is a fairly straightforward process, especially when you have the “blueprint” of the architecture presented in Table 10.1 above. Whenever implementing architectures from publications, try to see if they provide such a table as it makes implementation   10.4 Training AlexNet on Kaggle: Dogs vs. Cats  117  much easier. For your own network architectures, use Chapter 19 of the Starter Bundle on visualizing network architectures to aid you in ensuring your input volume and output volume sizes are what you expect.  10.4 Training AlexNet on Kaggle: Dogs vs. Cats  Now that the AlexNet architecture has been deﬁned, let’s apply it to the Kaggle Dogs vs. Cats challenge. Open up a new ﬁle, name it train_alexnet.py, and insert the following code:   import the necessary packages  set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use "Agg"    import the necessary packages from config import dogs_vs_cats_config as config from pyimagesearch.preprocessing import ImageToArrayPreprocessor from pyimagesearch.preprocessing import SimplePreprocessor from pyimagesearch.preprocessing import PatchPreprocessor from pyimagesearch.preprocessing import MeanPreprocessor from pyimagesearch.callbacks import TrainingMonitor from pyimagesearch.io import HDF5DatasetGenerator from pyimagesearch.nn.conv import AlexNet from keras.preprocessing.image import ImageDataGenerator from keras.optimizers import Adam import json import os  Lines 3 and 4 import matplotlib, while ensuring the backend is set such that we can save ﬁgures and plots to disk as our network trains. We then implement our pre-processors on Lines 8-11. The HDF5DatasetGenerator is then imported on Line 13 so we can access batches of training data from our serialized HDF5 dataset. AlexNet is also implement don Line 14.  Our next code block handles initializing our data augmentation generator via the ImageDataGenerator  class:   construct the training image generator for data augmentation aug = ImageDataGenerator rotation_range=20, zoom_range=0.15,  width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode="nearest"   Let’s take the time now to initialize each of our image pre-processors:   load the RGB means for the training set means = json.loads open config.DATASET_MEAN .read      initialize the image preprocessors sp = SimplePreprocessor 227, 227  pp = PatchPreprocessor 227, 227  mp = MeanPreprocessor means["R"], means["G"], means["B"]  iap = ImageToArrayPreprocessor    1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  20  21  22  23  25  26  27  28  29  30  31  32   118  Chapter 10. Competing in Kaggle: Dogs vs. Cats  On Line 26 we load the serialized RGB means from disk – these are the means for each of the respective Red, Green, and Blue channels across our training dataset. These values will later be passed into a MeanPreprocessor for mean subtraction normalization. Line 29 instantiates a SimplePreprocessor used to resize an input image down to 227× 227 pixels. This pre-processor will be used in the validation data generator as our input images are 256× 256 pixels; however, AlexNet is intended to handle only 227× 227 images  hence why we need to resize the image during validation . Line 30 instantiates a PatchPreprocessor – this pre-processor will randomly sample 227× 227 regions from the 256× 256 input images during training time, serving as a second form of data augmentation.  We then initialize the MeanPreprocessor on Line 31 using our respective, Red, Green, and Blue averages. Finally, the ImageToArrayPreprocessor  Line 32  is used to convert images to Keras-compatible arrays.  Given our pre-processors, let’s deﬁne the HDF5DatasteGenerator for both the training and  validation data:   initialize the training and validation dataset generators trainGen = HDF5DatasetGenerator config.TRAIN_HDF5, 128, aug=aug,  preprocessors=[pp, mp, iap], classes=2   valGen = HDF5DatasetGenerator config.VAL_HDF5, 128,  preprocessors=[sp, mp, iap], classes=2   Lines 35 and 36 create our training dataset generator. Here we supply the path to our training HDF5 ﬁle, indicating that we should use batch sizes of 128 images, data augmentation, and three pre-processors: patch, mean, and image to array, respectively.  Lines 37 and 38 are responsible for instantiating the testing generator. This time we’ll supply the path to the validation HDF5 ﬁle, use a batch size of 128, no data augmentation, and a simple pre-processor rather than a patch pre-processor  since data augmentation is not applied to validation data .  Finally, we are ready to initialize the Adam optimizer and AlexNet architecture:   initialize the optimizer print "[INFO] compiling model..."  opt = Adam lr=1e-3  model = AlexNet.build width=227, height=227, depth=3,  model.compile loss="binary_crossentropy", optimizer=opt,  classes=2, reg=0.0002   metrics=["accuracy"]    construct the set of callbacks path = os.path.sep.join [config.OUTPUT_PATH, "{}.png".format   os.getpid   ]   callbacks = [TrainingMonitor path ]  On Line 42 we instantiate the Adam optimizer using the default learning rate of 0.001. The  reason I choose Adam for this experiment  rather than SGD  is two-fold:  1. I wanted to give you exposure to using the more advanced optimizers we covered in Chapter  7.  2. Adam performs better on this classiﬁcation task than SGD  which I know from the multiple  previous experiments I ran before publishing this book .  34  35  36  37  38  40  41  42  43  44  45  46  47  48  49  50  51   10.4 Training AlexNet on Kaggle: Dogs vs. Cats  119  We then initialize AlexNet on Lines 43 and 44, indicating that each input image will have a width of 227 pixels, a height of 227 pixels, 3 channels, and the dataset itself will have two classes  one for dogs, and another for cats . We’ll also apply a small regularization penalty of 0.0002 to help combat overﬁtting and increase the ability of our model to generalize to the testing set.  We’ll use binary cross-entropy rather than categorical cross-entropy  Lines 45 and 46  as this is only a two-class classiﬁcation problem. We’ll also deﬁne a TrainingMonitor callback on Line 51 so we can monitor the performance of our network as it trains.  Speaking of training the network, let’s do that now:  53  54  55  56  57  58  59  60  61  63  64  65  66  67  68  69   train the network model.fit_generator   trainGen.generator  , steps_per_epoch=trainGen.numImages    128, validation_data=valGen.generator  , validation_steps=valGen.numImages    128, epochs=75, max_queue_size=128 * 2, callbacks=callbacks, verbose=1   To train AlexNet on the Kaggle Dogs vs. Cats dataset using our HDF5DatasetGenerator, we need to use the fit_generator method of the model. First, we pass in trainGen.generator  , the HDF5 generator used to construct mini-batches of training data  Line 55 . To determine the number of batches per epoch, we divide the total number of images in the training set by our batch size  Line 56 . We do the same on Lines 57 and 58 for the validation data. Finally, we’ll indicate that AlexNet is to be trained for 75 epochs.  The last step is to simply serialize our model to ﬁle after training, along with closing each of  the training and testing HDF5 datasets, respectively:   save the model to file print "[INFO] serializing model..."  model.save config.MODEL_PATH, overwrite=True    close the HDF5 datasets trainGen.close   valGen.close    To train AlexNet on the Kaggle Dogs vs. Cats dataset, execute the following command:  $ python train_alexnet.py Epoch 73 75 415s - loss: 0.4862 - acc: 0.9126 - val_loss: 0.6826 - val_acc: 0.8602 Epoch 74 75 408s - loss: 0.4865 - acc: 0.9166 - val_loss: 0.6894 - val_acc: 0.8721 Epoch 75 75 401s - loss: 0.4813 - acc: 0.9166 - val_loss: 0.4195 - val_acc: 0.9297 [INFO] serializing model...  A plot of the training and validation loss accuracy over the 75 epochs can be seen in Figure 10.4. Overall we can see that the training and accuracy plots correlate well with each other, although we could help stabilize variations in validation loss towards the end of the 75 epoch cycle by applying   120  Chapter 10. Competing in Kaggle: Dogs vs. Cats  Figure 10.4: Training AlexNet on the Kaggle Dogs vs. Cats competition where we obtain 92.97% classiﬁcation accuracy on our validation set. Our learning curve is stable with changes in training accuracy loss being reﬂected in the respective validation split.  a bit of learning rate decay. Examining the classiﬁcation report of AlexNet on the Dogs vs. Cats dataset, we see our obtained obtained 92.97% on the validation set.  In the next section, we’ll evaluate AlexNet on the testing set using both the standard method and over-sampling method. As our results will demonstrate, using over-sampling can increase your classiﬁcation from 1-3% depending on your dataset and network architecture.  10.5 Evaluating AlexNet  To evaluate AlexNet on the testing set using both our standard method and over-sampling technique, let’s create a new ﬁle named crop_accuracy.py:  --- dogs_vs_cats --- config  --- build_dogs_vs_cats.py   --- crop_accuracy.py --- extract_features.py  --- train_alexnet.py  --- train_model.py   --- output  From there, open crop_accuracy.py and insert the following code:  1  2   import the necessary packages from config import dogs_vs_cats_config as config   3  4  5  6  7  8  9  10  11  12  14  15  16  17  18  19  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38  10.5 Evaluating AlexNet  121  from pyimagesearch.preprocessing import ImageToArrayPreprocessor from pyimagesearch.preprocessing import SimplePreprocessor from pyimagesearch.preprocessing import MeanPreprocessor from pyimagesearch.preprocessing import CropPreprocessor from pyimagesearch.io import HDF5DatasetGenerator from pyimagesearch.utils.ranked import rank5_accuracy from keras.models import load_model import numpy as np import progressbar import json  Lines 2-12 import our required Python packages. Line 2 imports our Python conﬁguration ﬁle for the Dogs vs. Cats challenge. We’ll also import our image preprocessors on Lines 3-6, including the ImageToArrayPreprocessor, SimplePreprocessor, MeanPreprocessor, and CropPreprocessor. The HDF5DatasetGenerator is required so we can access the testing set of our dataset and obtain predictions on this data using our pre-trained model.  Now that our imports are complete, let’s load the RGB means from disk, initialize our image  pre-preprocessors, and load the pre-trained AlexNet network:   load the RGB means for the training set means = json.loads open config.DATASET_MEAN .read      initialize the image preprocessors sp = SimplePreprocessor 227, 227  mp = MeanPreprocessor means["R"], means["G"], means["B"]  cp = CropPreprocessor 227, 227  iap = ImageToArrayPreprocessor     load the pretrained network print "[INFO] loading model..."  model = load_model config.MODEL_PATH   Before we apply over-sampling and 10-cropping, let’s ﬁrst obtain a baseline on the testing set  using only the original testing image as input to our network:   initialize the testing dataset generator, then make predictions on  the testing data print "[INFO] predicting on test data  no crops ..."  testGen = HDF5DatasetGenerator config.TEST_HDF5, 64,  preprocessors=[sp, mp, iap], classes=2   predictions = model.predict_generator testGen.generator  , steps=testGen.numImages    64, max_queue_size=64 * 2    compute the rank-1 and rank-5 accuracies  rank1, _  = rank5_accuracy predictions, testGen.db["labels"]  print "[INFO] rank-1: {:.2f}%".format rank1 * 100   testGen.close    Lines 30 and 31 initialize the HDF5DatasetGenerator to access the testing dataset in batches of 64 images. Since we are obtaining a baseline, we’ll use only the SimplePreprocessor to resize the 256× 256 input images down to 227× 227 pixels, followed by mean normalization and   122  Chapter 10. Competing in Kaggle: Dogs vs. Cats  converting the batch to a Keras-compatible array of images. Lines 32 and 33 then use the generator to evaluate AlexNet on the dataset.  Given our predictions, we can compute our accuracy on the test set  Lines 36-38 . Notice here how we only care about the rank1 accuracy, which is because the Dogs vs. Cats is a 2-class dataset – computing the rank-5 accuracy for a 2-class dataset would trivially report 100 percent classiﬁcation accuracy.  Now that we have a baseline for the standard evaluation technique, let’s move on to over-  sampling:   re-initialize the testing set generator, this time excluding the  ‘SimplePreprocessor‘ testGen = HDF5DatasetGenerator config.TEST_HDF5, 64,  preprocessors=[mp], classes=2   predictions = []   initialize the progress bar widgets = ["Evaluating: ", progressbar.Percentage  , " ",  progressbar.Bar  , " ", progressbar.ETA  ]  pbar = progressbar.ProgressBar maxval=testGen.numImages    64,  widgets=widgets .start    On Lines 42 and 43 we re-initialize the HDF5DatasetGenerator, this time instructing it to use just the MeanPreprocessor – we’ll apply both over-sampling and Keras-array conversion later in the pipeline. Lines 47-50 also initialize progressbar widgets to our screen if we are interested in having the evaluating progress displayed to our screen.  Given the re-instantiated testGen, we are now ready to apply the 10-cropping technique:   loop over a single pass of the test data for  i,  images, labels   in enumerate testGen.generator passes=1  :   loop over each of the individual images for image in images:   apply the crop preprocessor to the image to generate 10  separate crops, then convert them from images to arrays crops = cp.preprocess image  crops = np.array [iap.preprocess c  for c in crops],  dtype="float32"    make predictions on the crops and then average them  together to obtain the final prediction pred = model.predict crops  predictions.append pred.mean axis=0     update the progress bar pbar.update i   On Line 53 we start looping over every batch of images in the testing generator. Typically an HDF5DatasetGenerator is set to loop forever until we explicitly tell it to stop  normally by setting a maximum number of iterations via Keras when training ; however, since we are now evaluating, we can supply passes=1 to indicate the testing data only needs to be looped over once. Then, for each image in the images batch  Line 55 , we apply the 10-crop pre-processor on Line 58, which converts the image into an array of ten 227× 227 images. These 227× 227 crops were extracted from the original 256× 256 batch based on the:  40  41  42  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68   10.6 Obtaining a Top-5 Spot on the Kaggle Leaderboard  123    Top-left corner   Top-right corner   Bottom-right corner   Bottom-left corner   Corresponding horizontal ﬂips Once we have the crops, we pass them through the model on Line 64 for prediction. The ﬁnal  prediction  Line 65  is the average of the probabilities across all ten crops.  Our ﬁnal code block handles displaying the accuracy of the over-sampling method:  70  71  72  73  74  75   compute the rank-1 accuracy pbar.finish   print "[INFO] predicting on test data  with crops ..."   rank1, _  = rank5_accuracy predictions, testGen.db["labels"]  print "[INFO] rank-1: {:.2f}%".format rank1 * 100   testGen.close    To evaluate AlexNet on the Kaggle Dog vs. Cats dataset, just execute the following command:  $ python crop_accuracy.py [INFO] loading model... [INFO] predicting on test data  no crops ... [INFO] rank-1: 92.60% Evaluating: 100%  Time: 0:01:12 [INFO] predicting on test data  with crops ... [INFO] rank-1: 94.00%  As our results demonstrate, we reach 92.60% accuracy on the testing set. However, by applying the 10-crop over-sampling method, we are able to boost classiﬁcation accuracy to 94.00%, an increase of 1.4%, which this was all accomplished simply by taking multiple crops of the input image and averaging the results. This straightforward, uncomplicated trick is an easy way to eke out an extra few percentage points when evaluating your network.  10.6 Obtaining a Top-5 Spot on the Kaggle Leaderboard  Of course, if you were to look at the Kaggle Dogs vs. Cats leaderboard, you would notice that to even break into the top-25 position we would need 96.69% accuracy, which our current method is not capable of reaching. So, what’s the solution?  The answer is transfer learning, speciﬁcally transfer learning via feature extraction. While the ImageNet dataset consists of 1,000 object categories, a good portion of those include both dog species and cat species. Therefore, a network trained on ImageNet could not only tell you if an image was of a dog or a cat, but what particular breed the animal is as well. Given that a network trained on ImageNet must be able to discriminate between such ﬁne-grained animals, it’s natural to hypothesize that the features extracted from a pre-trained network would likely lend itself well to claiming a top spot on the Kaggle Dogs vs. Cats leaderboard.  To test this hypothesis, let’s ﬁrst extract features from the pre-trained ResNet architecture and  then train a Logistic Regression classiﬁer on top of these features.  10.6.1 Extracting Features Using ResNet  The transfer learning via feature extraction technique we’ll be using in this section is heavily based on Chapter 3. I’ll review the entire contents of extract_features.py as a matter of completeness;   Chapter 10. Competing in Kaggle: Dogs vs. Cats  however, please refer to Chapter 3 if you require further knowledge on feature extraction using CNNs.  To get started, open up a new ﬁle, name it extract_features.py, and insert the following  124  code:   import the necessary packages from keras.applications import ResNet50 from keras.applications import imagenet_utils from keras.preprocessing.image import img_to_array from keras.preprocessing.image import load_img from sklearn.preprocessing import LabelEncoder from pyimagesearch.io import HDF5DatasetWriter from imutils import paths import numpy as np import progressbar import argparse import random import os  Lines 2-13 import our required Python packages. We import the ResNet50 class on Line 2 so we can access the pre-trained ResNet architecture. We’ll also use the HDF5DatasetWriter on Line 7 so we can write the extracted features to an efﬁciently HDF5 ﬁle format.  From there, let’s parse our command line arguments:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-d", "--dataset", required=True,  help="path to input dataset"   ap.add_argument "-o", "--output", required=True,  help="path to output HDF5 file"   ap.add_argument "-b", "--batch-size", type=int, default=16,  help="batch size of images to be passed through network"  ap.add_argument "-s", "--buffer-size", type=int, default=1000,  help="size of feature extraction buffer"   args = vars ap.parse_args      store the batch size in a convenience variable bs = args["batch_size"]  We only need two required command line arguments here, --dataset, which is the path to the input dataset of Dogs vs. Cats images, along with --output, the path to the output HDF5 ﬁle containing the features extracted via ResNet.  Next, let’s grab the paths to the Dogs vs. Cats images residing on disk and then use the ﬁle  paths to extract the label names:   grab the list of images that we’ll be describing then randomly  shuffle them to allow for easy training and testing splits via  array slicing during training time print "[INFO] loading images..."  imagePaths = list paths.list_images args["dataset"]   random.shuffle imagePaths   1  2  3  4  5  6  7  8  9  10  11  12  13  15  16  17  18  19  20  21  22  23  24  25  26  27  28  30  31  32  33  34  35   10.6 Obtaining a Top-5 Spot on the Kaggle Leaderboard  125   extract the class labels from the image paths then encode the  labels labels = [p.split os.path.sep [-1].split "." [0] for p in imagePaths] le = LabelEncoder   labels = le.fit_transform labels   Now we can load our pre-trained ResNet50 weights from disk  excluding the FC layers :   load the ResNet50 network print "[INFO] loading network..."  model = ResNet50 weights="imagenet", include_top=False   In order to store the features extracted from ResNet50 to disk, we need to instantiate a  HDF5DatasetWriter object:   initialize the HDF5 dataset writer, then store the class label  names in the dataset dataset = HDF5DatasetWriter  len imagePaths , 2048 ,  args["output"], dataKey="features", bufSize=args["buffer_size"]   dataset.storeClassLabels le.classes_   The ﬁnal average pooling layer of ResNet50 is 2048-d, hence why we supply a value of 2048  as the dimensionality to our HDF5datasetWriter.  We’ll also initialize a progressbar so we can keep track of the feature extraction process:   initialize the progress bar widgets = ["Extracting Features: ", progressbar.Percentage  , " ",  progressbar.Bar  , " ", progressbar.ETA  ]  pbar = progressbar.ProgressBar maxval=len imagePaths ,  widgets=widgets .start    Extracting features from a dataset using a CNN is the same as it was in Chapter 3. First, we  loop over the imagePaths in batches:   loop over the images in batches for i in np.arange 0, len imagePaths , bs :   extract the batch of images and labels, then initialize the  list of actual images that will be passed through the network  for feature extraction batchPaths = imagePaths[i:i + bs] batchLabels = labels[i:i + bs] batchImages = []  Followed by pre-processing each image:   loop over the images and labels in the current batch for  j, imagePath  in enumerate batchPaths :   load the input image using the Keras helper utility  36  37  38  39  40  41  43  44  45  47  48  49  50  51  53  54  55  56  57  59  60  61  62  63  64  65  66  68  69  70   71  72  73  74  75  76  77  78  79  80  81  82  84  85  86  87  88  89  90  91  93  94  95  96  97  98  99  126  Chapter 10. Competing in Kaggle: Dogs vs. Cats   while ensuring the image is resized to 224x224 pixels image = load_img imagePath, target_size= 224, 224   image = img_to_array image    preprocess the image by  1  expanding the dimensions and   2  subtracting the mean RGB pixel intensity from the  ImageNet dataset image = np.expand_dims image, axis=0  image = imagenet_utils.preprocess_input image    add the image to the batch batchImages.append image   And then passing the batchImages through the network architecture, enabling us to extract  features from the ﬁnal POOL layer of ResNet50:   pass the images through the network and use the outputs as  our actual features batchImages = np.vstack batchImages  features = model.predict batchImages, batch_size=bs    reshape the features so that each image is represented by  a flattened feature vector of the ‘MaxPooling2D‘ outputs features = features.reshape  features.shape[0], 2048    These extracted features are then added to our dataset:   add the features and labels to our HDF5 dataset dataset.add features, batchLabels  pbar.update i    close the dataset dataset.close   pbar.finish    following command:  To utilize ResNet to extract features from the Dogs vs. Cats dataset, simply execute the  $ python extract_features.py --dataset .. datasets kaggle_dogs_vs_cats train \  --output .. datasets kaggle_dogs_vs_cats hdf5 features.hdf5  [INFO] loading images... [INFO] loading network... Extracting Features: 100%  Time: 0:06:18  After the command ﬁnishes executing, you should now have a ﬁle named dogs_vs_cats_features.hdf5  in your output directory:  $ ls -l output dogs_vs_cats_features.hdf5 -rw-rw-r-- adrian 409806272 Jun  3 07:17 output dogs_vs_cats_features.hdf5  Given these features, we can train a Logistic Regression classier on top of them to  ideally   obtain a top-5 spot on the Kaggle Dogs vs. Cats leaderboard.   10.6 Obtaining a Top-5 Spot on the Kaggle Leaderboard  127  10.6.2 Training a Logistic Regression Classiﬁer  To train our Logistic Regression classiﬁer, open up a new ﬁle and name it train_model.py. From there, we can get started:  Lines 2-8 import our required Python packages. We’ll then parse our command line arguments:   import the necessary packages from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV from sklearn.metrics import classification_report from sklearn.metrics import accuracy_score import argparse import pickle import h5py   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-d", "--db", required=True,  help="path HDF5 database"   ap.add_argument "-m", "--model", required=True,  help="path to output model"   ap.add_argument "-j", "--jobs", type=int, default=-1,  help=" of jobs to run when tuning hyperparameters"   args = vars ap.parse_args     We only need two switches here, the path to the input HDF5 --db, along with the path to the  output Logistic Regression --model after training is complete.  Next, let’s open the HDF5 dataset for reading and determine the training and testing split – 75%  of the data for training and 25% for testing:   open the HDF5 database for reading then determine the index of  the training and testing split, provided that this data was  already shuffled *prior* to writing it to disk db = h5py.File args["db"], "r"  i = int db["labels"].shape[0] * 0.75   Given this feature split, we’ll perform a grid search over the C hyperparameter of the LogisticRegression  classiﬁer:   define the set of parameters that we want to tune then start a  grid search where we evaluate our model for each value of C print "[INFO] tuning hyperparameters..."  params = {"C": [0.0001, 0.001, 0.01, 0.1, 1.0]} model = GridSearchCV LogisticRegression  , params, cv=3,  n_jobs=args["jobs"]   model.fit db["features"][:i], db["labels"][:i]  print "[INFO] best hyperparameters: {}".format model.best_params_    Once we’ve found the best choice of C, we can generate a classiﬁcation report for the testing  set:  1  2  3  4  5  6  7  8  10  11  12  13  14  15  16  17  18  20  21  22  23  24  26  27  28  29  30  31  32  33   35  36  37  38  39  40  41  42  43  45  46  47  48  49  50  51  52  128  Chapter 10. Competing in Kaggle: Dogs vs. Cats   generate a classification report for the model print "[INFO] evaluating..."  preds = model.predict db["features"][i:]  print classification_report db["labels"][i:], preds,  target_names=db["label_names"]     compute the raw accuracy with extra precision acc = accuracy_score db["labels"][i:], preds  print "[INFO] score: {}".format acc    And ﬁnally, the trained model can be serialized to disk for later use, if we so wish:  To train our model on the ResNet50 features, simply execute the following command:  python train_model.py --db .. datasets kaggle_dogs_vs_cats hdf5 features.hdf5 \   serialize the model to disk print "[INFO] saving model..."  f = open args["model"], "wb"  f.write pickle.dumps model.best_estimator_   f.close     close the database db.close    --model dogs_vs_cats.pickle [INFO] tuning hyperparameters... [INFO] best hyperparameters: {’C’: 0.001} [INFO] evaluating...  precision  recall  f1-score  support  cat dog  avg   total  0.99 0.98  0.99  0.98 0.99  0.99  0.99 0.99  0.99  3160 3090  6250  [INFO] score: 0.98688 [INFO] saving model...  As you can see from the output, our approach of using transfer learning via feature extraction yields an impressive accuracy of 98.69%, enough for us to claim the 2 spot on the Kaggle Dogs vs. Cats leaderboard.  10.7 Summary  In this chapter we took a deep dive into the Kaggle Dogs vs. Cats dataset and studied to methods to obtain > 90% classiﬁcation accuracy on it:  1. Training AlexNet from scratch. 2. Applying transfer learning via ResNet. The AlexNet architecture is a seminal work ﬁrst introduced by Krizhevsky et al. in 2012 [6]. Using our implementation of AlexNet, we reached 94 percent classiﬁcation accuracy. This is a very respectable accuracy, especially for a network trained from scratch. Further accuracy can likely be obtained by:   10.7 Summary  129  1. Obtaining more training data. 2. Applying more aggressive data augmentation. 3. Deepening the network. However, the 94 percent we obtained is not even enough for us to break our way into the top-25 leaderboard, let alone the top-5. Thus, to obtain our top-5 placement, we relied on transfer learning via feature extraction, speciﬁcally, the ResNet50 architecture trained on the ImageNet dataset. Since ImageNet contains many examples of both dog and cat breeds, applying a pre-trained network to this task is a natural, easy method to ensure we obtain higher accuracy with less effort. As our results demonstrated, we were able to obtain 98.69% classiﬁcation accuracy, high enough to claim the second position on the Kaggle Dogs vs. Cats leaderboard.    11. GoogLeNet  In this chapter, we will study the GoogLeNet architecture, introduced by Szegedy et al. in their 2014 paper, Going Deeper With Convolutions [17]. This paper is important for two reasons. First, the model architecture is tiny compared to AlexNet and VGGNet  ≈ 28MB for the weights themselves . The authors are able to obtain such a dramatic drop in network architecture size  while still increasing the depth of the overall network  by removing fully-connected layers and instead using global average pooling. Most of the weights in a CNN can be found in the dense FC layers – if these layers can be removed, the memory savings are massive.  Secondly, the Szegedy et al. paper makes usage of a network in network or micro-architecture when  constructing the overall macro-architecture. Up to this point, we have seen only sequential neural networks where the output of one network feeds directly into the next. We are now going to see micro-architectures, small building blocks that are used inside the rest of the architecture, where the output from one layer can split into a number of various paths and be rejoined later.  Speciﬁcally, Szegedy et al. contributed the Inception module to the deep learning community, a building block that ﬁts into a Convolutional Neural Network enabling it to learn CONV layers with multiple ﬁlter sizes, turning the module into a multi-level feature extractor.  Micro-architectures such as Inception have inspired other important variants including the Residual module in ResNet [24] and the Fire module in SqueezeNet [32]. We’ll be discussing the Inception module  and its variants  later in this chapter. Once we’ve examined the Inception module and ensure we know how it works, we’ll then implement a smaller version of GoogLeNet called “MiniGoogLeNet” – we’ll train this architecture on the CIFAR-10 dataset and obtain higher accuracy than in any of our previous chapters.  From there, we’ll move on to the more difﬁcult cs231n Tiny ImageNet Challenge [4]. This challenge is offered to students enrolled in Stanford’s cs231n Convolutional Neural Networks for Visual Recognition class [39] as part of their ﬁnal project. It means to give them a taste of the challenges associated with large scale deep learning on modern architectures, without being as time-consuming or taxing to work with as the entire ImageNet dataset.  By training GoogLeNet from scratch on Tiny ImageNet, we’ll demonstrate how to obtain a top ranking position on the Tiny ImageNet leaderboard. And in our next chapter, we’ll utilize ResNet   132  Chapter 11. GoogLeNet  to claim the top position from models trained from scratch.  Let’s go ahead and get this chapter started by discussing the Inception module.  11.1 The Inception Module  and its Variants   Modern state-of-the-art Convolutional Neural Networks utilize micro-architectures, also called network-in-network modules, originally proposed by Lin et al. [40]. I personally prefer the term micro-architecture as it better describes these modules as building blocks in context of the overall macro-architecture  i.e., what you actually build and train .  Micro-architectures are small building blocks designed by deep learning practitioners to enable networks to learn  1  faster and  2  more efﬁciently, all while increasing network depth. These micro-architecture building blocks are stacked, along with conventional layer types such as CONV, POOL, etc., to form the overall macro-architecture.  In 2014, Szegedy et al. introduced the Inception module. The general idea behind the Inception  module is two-fold:  1. It can be hard to decide the size of the ﬁlter you need to learn at a given CONV layers. Should they be 5 × 5 ﬁlters? What about 3 × 3 ﬁlters? Should we learn local features using 1× 1 ﬁlters? Instead, why not learn them all and let the model decide? Inside the Inception module, we learn all three 5×5, 3×3, and 1×1 ﬁlters  computing them in parallel  concatenating the resulting feature maps along the channel dimension. The next layer in the GoogLeNet architecture  which could be another Inception module  receives these concatenated, mixed ﬁlters and performs the same process. Taken as a whole, this process enables GoogLeNet to learn both local features via smaller convolutions and abstracted features with larger convolutions – we don’t have to sacriﬁce our level of abstraction at the expense of smaller features. 2. By learning multiple ﬁlter sizes, we can turn the module into a multi-level feature extractor. The 5 × 5 ﬁlters have a larger receptive size and can learn more abstract features. The 1× 1 ﬁlters are by deﬁnition local. The 3× 3 ﬁlters sit as a balance in between.  11.1.1 Inception  Now that we’ve discussed the motivation behind the Inception module, let’s look at the actual module itself in Figure 11.1.  R An activation function  ReLU  is implicitly applied after every CONV layer. To save space, this activation function was not included in the network diagram above. When we implement GoogLeNet, you will see how this activation is used in the Inception module.  Speciﬁcally take note of how the Inception module branches into four distinct paths from the input layer. The ﬁrst branch in the Inception module simply learns a series of 1× 1 local features from the input. The second batch ﬁrst applies 1× 1 convolution, not only as a form of learning local features, but instead as dimensionality reduction. Larger convolutions  i.e., 3× 3 and 5× 5  by deﬁnition take more computation to perform. Therefore, if we can reduce the dimensionality of the inputs to these larger ﬁlters by applying 1× 1 convolutions, we can reduce the amount of computation required by our network. Therefore, the number of ﬁlters learned in the 1× 1 CONV in the second branch will always be smaller than the number of 3× 3 ﬁlters learned directly afterward. The third branch applies the same logic as the second branch, only this time with the goal of learning 5× 5 ﬁlters. We once again reduce dimensionality via 1× 1 convolutions, then feed the output into the 5× 5 ﬁlters.   11.1 The Inception Module  and its Variants   133  Figure 11.1: The original Inception module used in GoogLeNet. The Inception module acts as a “multi-level feature extractor” by computing 1× 1, 3× 3, and 5× 5 convolutions within the same module of the network. Figure from Szegedy et al., 2014 [17].  The fourth and ﬁnal branch of the Inception module performs 3× 3 max pooling with a stride of 1× 1 – this branch is commonly referred to as the pool projection branch. Historically, models that perform pooling have demonstrated an ability to obtain higher accuracy, although we now know through the work of Springenberg et al. in their 2014 paper, Striving for Simplicity: The All Convolutional Net [41] that this isn’t necessarily true, and that POOL layers can be replaced with CONV layers for reducing volume size.  In the case of Szegedy et al., this POOL layer was added simply due to the fact that it was thought that they were needed for CNNs to perform reasonably. The output of the POOL is then fed into another series of 1× 1 convolutions to learn local features.  Finally, all four branches of the Inception module converge where they are concatenated together along the channel dimension. Special care is taken during the implementation  via zero padding  to ensure the output of each branch has the same volume size, thereby allowing the outputs to be concatenated. The output of the Inception module is then fed into the next layer in the network. In practice, we often stack multiple Inception modules on top of each other before performing a pooling operation to reduce volume size.  11.1.2 Miniception  Of course, the original Inception module was designed for GoogLeNet such that it could be trained on the ImageNet dataset  where each input image is assumed to be 224× 224× 3  and obtain state-of-the-art accuracy. For smaller datasets  with smaller image spatial dimensions  where fewer network parameters are required, we can simplify the Inception module.  I ﬁrst became aware of the “Miniception” module from a tweet by @ericjang11  https:  twitter.com ericjang11   and @pluskid  https:  twitter.com pluskid  where they beautifully visualize a smaller variant of Inception used when training the CIFAR-10 dataset  Figure 11.2; credit to @ericjang11 and @pluskid .  After doing a bit of research, it turns out that this graphic was from Zhang et al.’s 2017 publication, Understanding Deep Learning Requires Re-Thinking Generalization [42]. The top row of the ﬁgure describes three modules used in their MiniGoogLeNet implementation:   Left: A convolution module responsible for performing convolution, batch normalization,  and activation.   134  Chapter 11. GoogLeNet  Figure 11.2: The Miniception architecture consists of building blocks including a convolution module, Inception module, and Downsample module. These modules are put together to form the overall architecture.    Middle: The Miniception module which performs two sets of convolutions, one for 1× 1 ﬁlters and the other for 3× 3 ﬁlters, then concatenates the results. No dimensionality reduction is performed before the 3× 3 ﬁlter as  1  the input volumes will be smaller already  since we’ll be using the CIFAR-10 dataset  and  2  to reduce the number of parameters in the network.   Right: A downsample module which applies both a convolution and max pooling to reduce  dimensionality, then concatenates across the ﬁlter dimension.  These building blocks are then used to build the MiniGoogLeNet architecture on the bottom row. You’ll notice here that the authors placed the batch normalization before the activation  presumably because this is what Szegedy et al. did as well , in contrast to what is now recommended when implementing CNNs.  In this book I have stuck with the implementation of the original author’s work, placing the batch normalization before activation in order to replicate results. In your own experiments, consider swapping this order.  In our next section, we’ll implement the MiniGoogLeNet architecture and apply it to the CIFAR-10 dataset. From there, we’ll be ready to implement the full Inception module and tackle the cs231n Tiny ImageNet challenge.  11.2 MiniGoogLeNet on CIFAR-10  In this section, we are going to implement the MiniGoogLeNet architecture using the Miniception module. We’ll then train MiniGoogLeNet on the CIFAR-10 dataset. As our results will demonstrate,   11.2 MiniGoogLeNet on CIFAR-10  135  this architecture will obtain > 90% accuracy on CIFAR-10, far better than all of our previous attempts.  11.2.1 Implementing MiniGoogLeNet  To get started, let’s ﬁrst create a ﬁle named minigooglenet.py inside the conv module of pyimagesearch.nn – this is where our implementation of the MiniGoogLeNet class will live:  --- pyimagesearch                 --- __init__.py --- callbacks --- io --- nn          --- preprocessing --- utils  --- __init__.py --- conv         --- __init__.py --- alexnet.py --- lenet.py --- minigooglenet.py --- minivggnet.py --- fcheadnet.py --- shallownet.py  From there, open up minigooglenet.py and insert the following code:  1  2  3  4  5  6  7  8  9  10  11  12  13   import the necessary packages from keras.layers.normalization import BatchNormalization from keras.layers.convolutional import Conv2D from keras.layers.convolutional import AveragePooling2D from keras.layers.convolutional import MaxPooling2D from keras.layers.core import Activation from keras.layers.core import Dropout from keras.layers.core import Dense from keras.layers import Flatten from keras.layers import Input from keras.models import Model from keras.layers import concatenate from keras import backend as K  Lines 2-13 import our required Python packages. Rather than importing the Sequential class where the output of one layer feeds directly into the next, we’ll instead need to use the Model class  Line 11 . Using Model rather than Sequential will allow us to create a network graph with splits and forks like in the Inception module. Another import you have not yet seen is the concatenate function on Line 12. As the name suggests, this function takes a set of inputs and concatenates them along a given axis, which in this case will be the channel dimension.  We’ll be implementing the exact version of MiniGoogLeNet as detailed in Figure 11.2 above,  so let’s start off with the conv_module:  15  16  class MiniGoogLeNet:  @staticmethod   Chapter 11. GoogLeNet  136  17  18  19  20  21  22  23  24  def conv_module x, K, kX, kY, stride, chanDim, padding="same" :   define a CONV => BN => RELU pattern x = Conv2D K,  kX, kY , strides=stride, padding=padding  x  x = BatchNormalization axis=chanDim  x  x = Activation "relu"  x    return the block return x  normalization, and then ﬁnally an activation. The parameters to the method are detailed below:  The conv_module function is responsible for applying a convolution, followed by a batch   x: The input layer to the function.   K: The number of ﬁlters our CONV layer is going to learn.   kX and kY: The size of each of the K ﬁlters that will be learned.   stride: The stride of the CONV layer.   chanDim: The channel dimension, which is derived from either “channels last” or “channels   padding: The type of padding to be applied to the CONV layer. On Line 19 we create the convolutional layer. The actual parameters to Conv2D are identical to examples in previous architectures such as AlexNet and VGGNet, but what changes here is how we supply the input to a given layer.  ﬁrst” ordering.  Since we are using a Model rather than a Sequential to deﬁne the network architecture, we cannot call model.add as this would imply that the output from one layer follows sequentially into the next layer. Instead, we supply the input layer in parenthesis at the end of the function call, which is called a Functional API. Each layer instance in a Model is callable on a tensor and also returns a tensor. Therefore, we can supply the inputs to a given layer by calling it as a function once the object is instantiated.  A template for constructing layers in this manner can be seen below:  output = Layer parameters  input   Take a second to familiarize yourself with this new style of adding layers to a network as we’ll  be using it whenever we deﬁne networks that are non-sequential.  The output of the Conv2D layer is then passed into the BatchNormalization layer on Line 20. The output of BatchNormalization then goes through a ReLU activation  Line 21 . If we were to construct a ﬁgure to help us visualize the conv_module it would look like Figure 11.3.  Figure 11.3: The conv_module of the MiniGoogLeNet architecture. This module includes no branching and is a simple CONV => BN => ACT.  First the convolution is applied, then a batch normalization, followed by an activation. Note that this module did not perform any branching. That is going to change with the deﬁnition of the inception_module below:   11.2 MiniGoogLeNet on CIFAR-10  137  26  27  28  29  30  31  32  33  34  35  36  37  @staticmethod def inception_module x, numK1x1, numK3x3, chanDim :   define two CONV modules, then concatenate across the  channel dimension conv_1x1 = MiniGoogLeNet.conv_module x, numK1x1, 1, 1,   1, 1 , chanDim    1, 1 , chanDim   conv_3x3 = MiniGoogLeNet.conv_module x, numK3x3, 3, 3,  x = concatenate [conv_1x1, conv_3x3], axis=chanDim    return the block return x  Our Mininception module will perform two sets of convolutions – a 1×1 CONV and a 3×3 CONV. These two convolutions will be performed in parallel and the resulting features concatenated across the channel dimension. Lines 30 and 31 use the handy conv_module we just deﬁned to learn numK1x1 ﬁlters  1× 1 . Lines 32 and 33 then apply conv_module again to learn numK3x3 ﬁlters  3× 3 . By using the conv_module function we are able to reuse code and not have to bloat our MiniGoogLeNet class by inserting many blocks of CONV => BN => RELU blocks – this stacking is taken care of concisely via conv_module. Notice how both the input to the 1× 1 and 3× 3 Conv2D class is x, the input to the layer. When using the Sequential class, this type of layer structure was not possible. But by using the Model class, we can now have multiple layers accept the same input. Once we have both conv_1x1 and conv_3x3, we concatenate them across the channel dimension.  Figure 11.4: The  mini -inception_module consists of two branches. The ﬁrst branch is a CONV layer responsible for learning 1× 1 ﬁlters. The second branch is another CONV layer that learns 3× 3 ﬁlters. The ﬁlter responses are then concatenated along the channel dimension.  To visualize the “Mini"-Inception module, take a look at Figure 11.4. Our 1× 1 and 3× 3 CONV layers take a given input and apply their respective convolutions. The output of both convolutions is then concatenated  Line 33 . We are allowed to concatenate the layer outputs because the output volume size for both convolutions is identical due to padding="same".  Next comes the downsample_module, which as the name suggests, is responsible for reducing  the spatial dimensions of an input volume:  39  40  @staticmethod def downsample_module x, K, chanDim :   Chapter 11. GoogLeNet  138  41  42  43  44  45  46  47  48  49   define the CONV module and POOL, then concatenate  across the channel dimensions conv_3x3 = MiniGoogLeNet.conv_module x, K, 3, 3,  2, 2 ,  chanDim, padding="valid"   pool = MaxPooling2D  3, 3 , strides= 2, 2   x  x = concatenate [conv_3x3, pool], axis=chanDim    return the block return x  This method requires us to pass in an input x, the number of ﬁlters K our convolutional layer will learn, along with the chanDim for batch normalization and channel concatenation. The ﬁrst branch of the downsample_module learns a set of K, 3× 3 ﬁlters using a stride of 2× 2, thereby decreasing the output volume size  Lines 43 and 44 . We apply max pooling on Line 45  the second branch , again with window size of 3× 3 and stride of 2× 2 to reduce volume size. The conv_3x3 and pool outputs are then concatenated  Line 46  and returned to the calling function.  Figure 11.5: The downsample_module is responsible for reducing the spatial dimensions of our input volume. The ﬁrst branch learns a set of ﬁlters with 2× 2 stride to reduce the output volume. The second branch also reduces the spatial dimensions, this time by applying max pooling. The output of the downsample_module is concatenated along the channel dimension.  We can visualize the downsample_module in Figure 11.5. As the ﬁgure demonstrates, a con-  volution and max pooling operation are applied to the same input and then concatenated.  We are now ready to put all the pieces together:  51  52  53  54  55  56  57  58  59  60  61  62  @staticmethod def build width, height, depth, classes :   initialize the input shape to be "channels last" and the  channels dimension itself inputShape =  height, width, depth  chanDim = -1   if we are using "channels first", update the input shape  and channels dimension if K.image_data_format   == "channels_first":  inputShape =  depth, height, width  chanDim = 1   11.2 MiniGoogLeNet on CIFAR-10  139  Line 52 deﬁnes the build method to our network, as is standard for all other examples in this book. Our build method accepts an input width, height, depth, and total number of classes that will be learned. Lines 55 and 56 initialize our inputShape and chanDim assuming we are using “channels last” ordering. If we are instead using “channels ﬁrst” ordering, Lines 60-62 update these variables, respectively.  Let’s deﬁne the model Input along with the ﬁrst conv_module:   define the model input and first CONV module inputs = Input shape=inputShape  x = MiniGoogLeNet.conv_module inputs, 96, 3, 3,  1, 1 ,  chanDim   The call to Input on Line 65 initializes the architecture – all inputs to the network will start at this layer which simply “holds” the input data  all networks need to have an input, after all . The ﬁrst CONV => BN => RELU is applied on Lines 66 and 67 where we learn 96, 3× 3 ﬁlters.  From there, we stack two Inception modules followed by a downsample module:   two Inception modules followed by a downsample module x = MiniGoogLeNet.inception_module x, 32, 32, chanDim  x = MiniGoogLeNet.inception_module x, 32, 48, chanDim  x = MiniGoogLeNet.downsample_module x, 80, chanDim   When concatenated, this module outputs a volume with K = 32 + 32 = 64 ﬁlters.  The ﬁrst Inception module  Line 70  learns 32 ﬁlters for both the 1× 1 and 3× 3 CONV layers. The second Inception module  Line 71  learns 32, 1× 1 ﬁlters and 48, 3× 3 ﬁlters. Again, when concatenated, we see that the output volume size is K = 32 + 48 = 80. The downsample module reduces our input volume sizes but keeps the same number of ﬁlters learned at 80.  Next, let’s stack four Inception modules on top of each other before applying a downsample,  allowing GoogLeNet to learn deeper, richer features:   four Inception modules followed by a downsample module x = MiniGoogLeNet.inception_module x, 112, 48, chanDim  x = MiniGoogLeNet.inception_module x, 96, 64, chanDim  x = MiniGoogLeNet.inception_module x, 80, 80, chanDim  x = MiniGoogLeNet.inception_module x, 48, 96, chanDim  x = MiniGoogLeNet.downsample_module x, 96, chanDim   Notice how in some layers we learn more 1× 1 ﬁlters than 3× 3 ﬁlters, while other Inception modules learn more 3× 3 ﬁlters than 1× 1. This type of alternating pattern is done on purpose and was justiﬁed by Szegedy et al. after running many experiments. When we implement the deeper variant of GoogLeNet later in this chapter, we’ll also see this pattern as well.  Continuing our implementation of Figure 11.2 by Zhang et al., we’ll now apply two more  inception modules followed by a global pool and dropout:   two Inception modules followed by global POOL and dropout x = MiniGoogLeNet.inception_module x, 176, 160, chanDim  x = MiniGoogLeNet.inception_module x, 176, 160, chanDim  x = AveragePooling2D  7, 7   x  x = Dropout 0.5  x   64  65  66  67  69  70  71  72  74  75  76  77  78  79  81  82  83  84  85   87  88  89  90  91  92  93  94  95  96  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  140  Chapter 11. GoogLeNet The output volume size after Line 83 is 7× 7× 336. Applying an average pooling of 7× 7 reduces the volume size to 1× 1× 336 and thereby alleviates the need to apply many dense fully-connected layers – instead, we simply average over the spatial outputs of the convolution. Dropout is applied with a probability of 50 percent on Line 85 to help reduce overﬁtting.  Finally, we add in our softmax classiﬁer based on the number of classes we wish to learn:   softmax classifier x = Flatten   x  x = Dense classes  x  x = Activation "softmax"  x    create the model model = Model inputs, x, name="googlenet"    return the constructed network architecture return model  The actual Model is then instantiated on Line 93 where we pass in the inputs, the layers  x, which includes the built-in branching , and optionally a name for the network. The constructed architecture is returned to the calling function on Line 96.  11.2.2 Training and Evaluating MiniGoogLeNet on CIFAR-10  Now that MiniGoogLeNet is implemented, let’s train it on the CIFAR-10 dataset and see if we can beat our previous best of 84 percent. Open up a new ﬁle, name it googlenet_cifar10.py, and insert the following code:   set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use "Agg"    import the necessary packages from sklearn.preprocessing import LabelBinarizer from pyimagesearch.nn.conv import MiniGoogLeNet from pyimagesearch.callbacks import TrainingMonitor from keras.preprocessing.image import ImageDataGenerator from keras.callbacks import LearningRateScheduler from keras.optimizers import SGD from keras.datasets import cifar10 import numpy as np import argparse import os  Lines 2 and 3 conﬁgure matplotlib so we can save ﬁgures and plots to disk in the background. We then import the rest of our required packages on Lines 6-15. Line 7 imports our implementation of MiniGoogLeNet.  Also notice how we are importing the LearningRateScheduler class on Line 10, which implies that we’ll be deﬁning a speciﬁc learning rate for our optimizer to follow when training the network. Speciﬁcally, we’ll be deﬁning a polynomial decay learning rate schedule. A polynomial learning rate scheduler will follow the equation:  α = α0 ∗  1− e emax p   11.1    11.2 MiniGoogLeNet on CIFAR-10  141  Where α0 is the initial learning rate, e is the current epoch number, emax is the maximum number of epochs we are going to perform, and p is the power of the polynomial. Applying this equation yields the learning rate α for the current epoch.  Given the maximum number of epochs, the learning rate will decay to zero. This learning rate scheduler can also be made linear by setting the power to 1.0 – which is often done – and, in fact, what we are going to do in this example. I have included a number of example polynomial learning rate schedules using a maximum of 70 epochs, an initial learning rate of 5e− 3, and varying powers in Figure 11.6. Notice how as the power increases, the faster the learning rate drops. Using a power of 1.0 turns the curve into a linear decay.  Figure 11.6: Plots of polynomial learning rate decays for varying values of the power, p. Notice how as the power increases the sharper the decay. Setting p = 1.0 turns a polynomial decay into a linear decay.  Let’s go ahead and implement this learning rate schedule function below:  17  18  19  20  21  22  23  24  25  26  27  28  29   define the total number of epochs to train for along with the  initial learning rate NUM_EPOCHS = 70 INIT_LR = 5e-3  def poly_decay epoch :   initialize the maximum number of epochs, base learning rate,  and power of the polynomial maxEpochs = NUM_EPOCHS baseLR = INIT_LR power = 1.0   compute the new learning rate based on polynomial decay   142  30  31  32  33  35  36  37  38  39  40  41  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  Chapter 11. GoogLeNet  alpha = baseLR *  1 -  epoch   float maxEpochs    ** power   return the new learning rate return alpha  Per our discussion of learning rate schedulers in Chapter 16 of the Starter Bundle, you know that a learning rate scheduling function can only accept a single argument, the current epoch. We then initialize the maxEpochs the network is allowed to train for  so we can decay the learning rate to zero , the base learning rate, as well as the power of the polynomial.  Computing the new learning rate based on the polynomial decay is handled on Line 30 – the output of this equation will match our graphs exactly based on the parameters supplied. The new learning rate is returned to the calling function on Line 33 so that the optimizer can update its internal learning rate. Again, for more information on learning rate schedulers, please see Chapter 16 of the Starter Bundle.  Now that we have deﬁned our learning rate deﬁned, we can parse our command line arguments:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-m", "--model", required=True,  help="path to output model"   ap.add_argument "-o", "--output", required=True,  help="path to output directory  logs, plots, etc. "   args = vars ap.parse_args     Our script requires two arguments, --model, the path to the output ﬁle where MiniGoogLeNet  will be serialized after training, along with --output, where we will store any plots, logs, etc.  The next step is to load the CIFAR-10 data from disk, perform pixel-wise mean subtraction,  and then one-hot encode the labels:   load the training and testing data, converting the images from  integers to floats print "[INFO] loading CIFAR-10 data..."    trainX, trainY ,  testX, testY   = cifar10.load_data   trainX = trainX.astype "float"  testX = testX.astype "float"    apply mean subtraction to the data mean = np.mean trainX, axis=0  trainX -= mean testX -= mean   convert the labels from integers to vectors lb = LabelBinarizer   trainY = lb.fit_transform trainY  testY = lb.transform testY   To help combat overﬁtting and enable our model to obtain higher classiﬁcation accuracy, we’ll  apply data augmentation:  60  61   construct the image generator for data augmentation aug = ImageDataGenerator width_shift_range=0.1,   11.2 MiniGoogLeNet on CIFAR-10  143  height_shift_range=0.1, horizontal_flip=True, fill_mode="nearest"   We’ll also construct a set of callbacks to monitor training progress as well as call our  LearningRateScheduler:   construct the set of callbacks figPath = os.path.sep.join [args["output"], "{}.png".format   jsonPath = os.path.sep.join [args["output"], "{}.json".format   os.getpid   ]   os.getpid   ]   callbacks = [TrainingMonitor figPath, jsonPath=jsonPath ,  LearningRateScheduler poly_decay ]  Finally, we are ready to train our network:   initialize the optimizer and model print "[INFO] compiling model..."  opt = SGD lr=INIT_LR, momentum=0.9  model = MiniGoogLeNet.build width=32, height=32, depth=3, classes=10  model.compile loss="categorical_crossentropy", optimizer=opt,  metrics=["accuracy"]    train the network print "[INFO] training network..."  model.fit_generator aug.flow trainX, trainY, batch_size=64 ,  validation_data= testX, testY , steps_per_epoch=len trainX     64, epochs=NUM_EPOCHS, callbacks=callbacks, verbose=1    save the network to disk print "[INFO] serializing network..."  model.save args["model"]   62  63  65  66  67  68  69  70  71  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  Line 75 initializes the SGD optimizer with an INIT_LR initial learning rate. This learning rate will be updated via the LearningRateScheduler once training starts. The MiniGoogLeNet architecture itself will accept input images with a width of 32 pixels, height of 32 pixels, depth of 3 channels, and a total of 10 class labels. Lines 82-84 kick off the training process using mini-batch sizes of 64, training for a total of NUM_EPOCHS. Once training is complete, Line 88 serializes our model to disk.  11.2.3 MiniGoogLeNet: Experiment 1  At this point in the Practitioner Bundle, it becomes important for you to understand the actual mindset, process, and set of experiments you’ll need to perform to obtain a high accuracy model on a given dataset.  In previous chapters in both this bundle and the Starter Bundle, you were just getting your feet wet – and it was enough to simply see a piece of code, understand what it does, execute it, and look at the output. That was a great starting point to developing an understanding of deep learning.  However, now that we are working with more advanced architectures and challenging problems, you need to understand the process behind performing experiments, examining the results, and then updating the parameters. I provide a gentle introduction to this scientiﬁc method in this chapter   144  Chapter 11. GoogLeNet  and the following chapter on ResNet. If you are interested in mastering this ability to perform experiments, examine the results, and make intelligent postulations as to what the next best course of action is, then please refer to the more advanced lessons in the ImageNet Bundle. In my ﬁrst experiment with GoogLeNet, I started with an initial learning rate of 1e− 3 with the SGD optimizer. The learning rate was then decayed linearly over the course of 70 epochs. Why 70 epochs? Two reasons:  1. A priori knowledge. After reading hundreds of deep learning papers, blog posts, tutorials and not to mention, performing your own experiments, you’ll start to notice a pattern for some datasets. In my case, I knew from previous experiments in my career with the CIFAR- 10 dataset that anywhere between 50-100 epochs is typically all that is required to train CIFAR-10. The deeper the network architecture is  with sufﬁcient regularization , along with a decreasing learning rate, will normally allow us to train our network for longer. Therefore, I choose 70 epochs for my ﬁrst experiment. After the experiment was ﬁnished running, I could examine the learning plot and decide if more less epochs should be used  as it turns out, 70 epochs was spot on .  2. Inevitable overﬁtting. Secondly, we know from previous experiments in this book that we will eventually overﬁt when working with CIFAR-10. It’s inevitable; even with strong regularization and data augmentation, it’s still going to happen. Therefore, I decided on 70 epochs rather than risking 80-100 epochs where the effects of overﬁtting would become more pronounced.  I then started training using the following command:  $ python googlenet_cifar10.py --output output \ --model output minigooglenet_cifar10.hdf5  Using our initial learning rate of 1e − 3 with a linear decay over 70 epochs, we obtained 87.95% classiﬁcation accuracy  Figure 11.7, top-left . Furthermore, looking at our plot, while there is a gap between the training and validation loss, the gap stays  relatively  proportional past epoch 40. The same can be said of the training and validation accuracy curves as well. Looking at this graph, I became concerned that we did not train hard enough and that we might be able to obtain higher classiﬁcation accuracy with a 1e− 2 learning rate.  11.2.4 MiniGoogLeNet: Experiment 2  In our second MiniGoogLeNet experiment, I swapped out the initial SGD learning rate of 1e− 3 for a larger 1e− 2. This learning rate was linearly decayed over the course of 70 epochs. I once again trained the network and gathered the results:  $ python googlenet_cifar10.py --output output \ --model output minigooglenet_cifar10.hdf5  At the end of the 70th epoch, we are obtaining 91.79% accuracy on the validation set  Figure 11.7, top-right , certainly better than our previous experiment – but we shouldn’t get too excited yet. Looking at the loss for the training curve, we can see that it falls entirely to zero past epoch 60. Furthermore, the classiﬁcation accuracy for the training curve is fully saturated at 100%.  While we have increased our validation accuracy, we have done so at the expense of overﬁtting – the gap between validation loss and training loss is huge past epoch 20. Instead, we would do better to re-train our network with a 5e− 3 learning rate, falling square in the middle of 1e− 2 and 1e− 3. We might obtain slightly less validation accuracy, but we’ll ideally be able to reduce the effects of overﬁtting.   11.2 MiniGoogLeNet on CIFAR-10  145  Figure 11.7: Top-left: Learning curves for Experiment 1. Top-right: Plots for Experiment 2. Bottom: Learning curves for Experiment 3. Our ﬁnal experiment is a good balance between the ﬁrst two, obtaining 90.81% accuracy, higher than all of our previous CIFAR-10 experiments.  R  For what it’s worth, if you ﬁnd that your network has totally saturated loss  0.0  and accuracy  100%  for the training set, be sure to pay close attention to your validation curves. If you see larges of gaps between the validation and training plot, you have surely overﬁt. Go back to your experiment and play with the parameters, introduce more regularization, and adjust the learning rate. Saturations such as those displayed in this experiment are indicative of a model that will not generalize well.  11.2.5 MiniGoogLeNet: Experiment 3  In this experiment, I adjusted my learning rate to 5e−3. MiniGoogLeNet was trained for 70 epochs using the SGD optimizer and a linear learning rate decay:  $ python googlenet_cifar10.py --output output \ --model output minigooglenet_cifar10.hdf5  As the output shows, we obtained 90.81% classiﬁcation accuracy on the validation set  Figure 11.7, bottom  – lower than the previous experiment, but higher than the ﬁrst experiment. We are deﬁnitely overﬁtting in this experiment  by approximately an order of magnitude , but we can accept this as an inevitability when working with CIFAR-10.   146  Chapter 11. GoogLeNet  What’s more important here is that we kept our training loss and accuracy saturation levels as low as possible – the training loss did not fall completely to zero, and the training accuracy did not reach 100%. We can also see a reasonable gap maintained between training and validation accuracy, even in the later epochs.  At this point I would consider this experiment to be an initial success  with the caveat that more experiments should be done to reduce overﬁtting  – we have successfully trained MiniGoogLeNet on CIFAR-10, reaching our goal of > 90% classiﬁcation, beating out all previous experiments on CIFAR-10.  Future revisions of this experiment should consider being aggressive with regularization. Speciﬁcally, we did not use any type of weight regularization here. Applying L2 weight decay would help combat our overﬁtting  as our experiments in the next section will demonstrate .  Moving forward, use this experiment as a baseline. We know there is overﬁtting and we’d like to reduce it while increasing classiﬁcation accuracy. In our next chapter on ResNet, we’ll see how we can accomplish both.  Now that we’ve explored MiniGoogLeNet applied to CIFAR-10, let’s move on to the more difﬁcult classiﬁcation task of the cs231n Tiny ImageNet challenge where we’ll be implementing a deeper variant of GoogLeNet, similar to the architecture used by Szgedy et al. in their original paper.  11.3 The Tiny ImageNet Challenge  Figure 11.8: A sample of images from Stanford’s Tiny ImageNet classiﬁcation challenge.  The Tiny ImageNet Visual Recognition Challenge  a sample of which can be seen in Figure 11.8  is part of the cs231n Stanford course on Convolutional Neural Networks for Visual Recognition [39]. As part of their ﬁnal project, students can compete in the classiﬁcation by either training a CNN from scratch or performing transfer learning via ﬁne-tuning  transfer learning via feature extraction is not allowed .  The Tiny ImageNet dataset is actually a subset of the full ImageNet dataset  hence why feature extraction cannot be used, as it would give the network an unfair advantage , consisting of 200 diverse classes, including everything from Egyptian cats to volleyballs to lemons. Given that there are 200 classes, guessing at random we would expect to be correct 1 200 = 0.5% of the time; therefore, our CNN needs to obtain at least 0.5% to demonstrate it has learned discriminative underlying patterns in the respective classes.  Each class includes 500 training images, 50 validation images, and 50 testing images. Ground- truth labels are only provided for the training and validation images. Since we do not have access to the Tiny ImageNet evaluation server, we will use part of the training set to form our own testing set so we can evaluate the performance of our classiﬁcation algorithms.  As readers of the ImageNet Bundle will discover  where we discuss how to train deep Convolu- tional Neural Networks on the full ImageNet dataset from scratch , the images in the ImageNet   11.3 The Tiny ImageNet Challenge  147  Large Scale Visual Recognition Challenge  ILSVRC  have varying widths and heights. Therefore, whenever we work with ILSVRC, we ﬁrst need to resize all images in the dataset to a ﬁxed width and height before we can train our network. To help students focus strictly on the deep learning and image classiﬁcation component  and not get caught up in image processing details , all images in the Tiny ImageNet dataset have been resized to 64× 64 pixels and center cropped.  In some ways, having the images resized makes Tiny ImageNet a bit more challenging than it’s bigger brother, ILSVRC. In ILSVRC we are free to apply any type of resizing, cropping, etc. operations that we see ﬁt. However, with Tiny ImageNet, much of the image has already been discarded for us. As we’ll ﬁnd out, obtaining a reasonable rank-1 and rank-5 accuracy on Tiny ImageNet isn’t as easy as one might think, making it a great, insightful dataset for budding deep learning practitioners to learn and practice on.  In the next few sections, you will learn how to obtain the Tiny ImageNet dataset, understand its  structure, and create HDF5 ﬁles for the training, validation, and testing images.  11.3.1 Downloading Tiny ImageNet  You can download the Tiny ImageNet dataset from ofﬁcial cs231n leaderboard page here:  https:  tiny-imagenet.herokuapp.com  Alternatively, I have created a mirror for the ﬁle here as well: http:  pyimg.co h28e4 The .zip ﬁle is ≈ 237MB, so make sure you have a reasonable internet connection before  attempting the download.  11.3.2 The Tiny ImageNet Directory Structure  After downloading and unpacking your tiny-imagenet-200.zip ﬁle, unarchive it, and you’ll ﬁnd the following directory structure:  --- tiny-imagenet-200       --- test --- train --- val --- wnids.txt --- words.txt  Inside the test directory are the testing images – we’ll be ignoring these images since we do not have access to the cs231n evaluation server  the labels are purposely left out from the download to ensure no one can “cheat” in the challenge .  We then have the train directory which contains subdirectories with strange names starting with the letter n followed by a series of numbers. These subdirectories are the WordNet [43] IDs called “synonym set” or “synsets” for short. Each WordNet ID maps to a speciﬁc word object. Every image inside a given WordNet subdirectory contains examples of that object.  We can lookup the human readable label for a WordNet ID by parsing the words.txt ﬁle, which is simply a tab separated ﬁle with the WordNet ID in the ﬁrst column and the human readable word object in the second column. The wnids.txt ﬁle lists out the 200 WordNet IDs  one per line  in the ImageNet dataset.  Finally, the val directory stores our validation set. Inside the val directory, you’ll ﬁnd an images subdirectory and a ﬁle named val_annotations.txt. The val_annotations.txt provides the WordNet IDs for every image in the val directory.  Therefore, before we can even get started training GoogLeNet on Tiny ImageNet, we ﬁrst need to write a script to parse these ﬁles and put them into HDF5 format. Keep in mind that being a deep   148  Chapter 11. GoogLeNet  learning practitioner isn’t about implementing Convolutional Neural Networks and training them from scratch. Part of being a deep learning practitioner involves using your programming skills to build simple scripts that can parse data.  The more general purpose programming skills you have, the better deep learning practitioner you can become – while other deep learning researchers are struggling to organize ﬁles on disk or understand how a dataset is structured, you’ll have already converted your entire dataset to a format suitable for training a CNN.  In the next section, I’ll teach you how to deﬁne your project conﬁguration ﬁle and create a single,  simple Python script that will convert the Tiny ImageNet dataset into an HDF5 representation.  11.3.3 Building the Tiny ImageNet Dataset  Let’s go ahead and deﬁne the project structure for Tiny ImageNet + GoogLeNet:  --- __init__.py --- tiny_imagenet_config.py  --- deepergooglenet           --- config   --- build_tiny_imagenet.py --- rank_accuracy.py --- train.py --- output     --- checkpoints  --- tiny-image-net-200-mean.json  We’ll create a config module where we’ll store any tiny_imagenet_config.py conﬁgura- tions. We then have the build_tiny_imagenet.py script which is responsible for taking Tiny ImageNet and converting it to HDF5. The train.py script will train GoogLeNet on the HDF5 version of Tiny ImageNet. Finally, we’ll use rank.py to compute the rank-1 and rank-5 accuracies for the testing set.  Let’s go ahead and take a look at tiny_imagenet_config.py:   import the necessary packages from os import path   define the paths to the training and validation directories TRAIN_IMAGES = ".. datasets tiny-imagenet-200 train" VAL_IMAGES = ".. datasets tiny-imagenet-200 val images"   define the path to the file that maps validation filenames to  their corresponding class labels VAL_MAPPINGS = ".. datasets tiny-imagenet-200 val val_annotations.txt"  Lines 5 and 6 deﬁne the paths to the Tiny ImageNet training and validation images, respectively. We then deﬁne the path to the validation ﬁle mappings which enables us to map the validation ﬁlenames to actual class labels  i.e., the WordNet IDs .  Speaking of WordNet IDs and human readable labels, let’s deﬁne the paths to those as well:   define the paths to the WordNet hierarchy files which are used  to generate our class labels WORDNET_IDS = ".. datasets tiny-imagenet-200 wnids.txt" WORD_LABELS = ".. datasets tiny-imagenet-200 words.txt"  1  2  3  4  5  6  7  8  9  10  12  13  14  15   11.3 The Tiny ImageNet Challenge  149  Given that we do not have access to the testing labels, we’ll need to take a portion of the training data and use it for validation  since our training data does have labels associated with each image :   since we do not have access to the testing data we need to  take a number of images from the training data and use it instead NUM_CLASSES = 200 NUM_TEST_IMAGES = 50 * NUM_CLASSES  The purpose of this section is to convert Tiny ImageNet to HDF5, therefore we need to supply  paths to the training, validation, and testing HDF5 ﬁles:   define the path to the output training, validation, and testing  HDF5 files TRAIN_HDF5 = ".. datasets tiny-imagenet-200 hdf5 train.hdf5" VAL_HDF5 = ".. datasets tiny-imagenet-200 hdf5 val.hdf5" TEST_HDF5 = ".. datasets tiny-imagenet-200 hdf5 test.hdf5"  When writing the images to disk, we’ll want to compute the RGB means for the training set, enabling us to perform mean normalization – after we have the means, they will need to be serialized to disk as a JSON ﬁle:  28  29   define the path to the dataset mean DATASET_MEAN = "output tiny-image-net-200-mean.json"  Finally, we’ll deﬁne the paths to our output model and training logs plots:   define the path to the output directory used for storing plots,  classification reports, etc. OUTPUT_PATH = "output" MODEL_PATH = path.sep.join [OUTPUT_PATH,  "checkpoints epoch_70.hdf5"]   FIG_PATH = path.sep.join [OUTPUT_PATH,  "deepergooglenet_tinyimagenet.png"]   JSON_PATH = path.sep.join [OUTPUT_PATH,  "deepergooglenet_tinyimagenet.json"]   As you can see, this conﬁguration ﬁle is fairly straightforward. We are mainly just deﬁning paths to input directories of images label mappings along with output ﬁles. However, taking the time to create this conﬁguration ﬁle makes our life much easier when actually building Tiny ImageNet and converting it to HDF5.  To see how this is true, let’s go ahead and examine build_tiny_imagenet.py:   import the necessary packages from config import tiny_imagenet_config as config from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from pyimagesearch.io import HDF5DatasetWriter from imutils import paths import numpy as np  17  18  19  20  22  23  24  25  26  31  32  33  34  35  36  37  38  39  1  2  3  4  5  6  7   150  8  9  10  11  import progressbar import json import cv2 import os  Chapter 11. GoogLeNet  Lines 2-11 import our required Python packages. On Line 2 we import our newly coded conﬁguration ﬁle so we can have access to the variables inside it. We’ll be using the LabelEncoder to encode the WordNet IDs as integers. The train_test_split function will be applied to construct our training and testing split. We’ll the use the HDF5DatasetWriter class to actually write the raw images to their respective HDF5 datasets.  Let’s go ahead and grab the paths to the training images, extract the class labels, and encode  them:  13  14  15  16  17  18   grab the paths to the training images, then extract the training  class labels and encode them trainPaths = list paths.list_images config.TRAIN_IMAGES   trainLabels = [p.split os.path.sep [-3] for p in trainPaths] le = LabelEncoder   trainLabels = le.fit_transform trainLabels   On Line 15 we grab a list of all image paths inside the TRAIN_IMAGES directory. Every path in  this list has the pattern:  tiny-imagenet-200 train {wordnet_id} {unique_filename}.JPG  Therefore, to extract the WordNet ID  i.e., class label , we simply need to split on the path separator and grab the third entry  Python is zero-indexed, so we supply a value of 2 here . Once we have all the trainLabels, we can initiate the LabelEncoder and convert all labels to unique integers  Lines 17 and 18 .  Since we do not have a testing split, we need to sample a set of images from the training set to  form one:  20  21  22  23  24  25   perform stratified sampling from the training set to construct a  a testing set split = train_test_split trainPaths, trainLabels,  test_size=config.NUM_TEST_IMAGES, stratify=trainLabels, random_state=42    trainPaths, testPaths, trainLabels, testLabels  = split  Here we supply the trainPaths and trainLabels, along with the test_size of NUM_TEST_IMAGES,  which is 50 images per class  for a total of 10,000 images . Our testing set is sampled from our training set, where we already have the class labels for the images, enabling us to evaluate the performance of our neural network when we are ready; however, we have not parsed the validation labels yet.  Parsing and encoding the validation labels is handled in the following code block:  27  28  29   load the validation filename => class from file and then use these  mappings to build the validation paths and label lists M = open config.VAL_MAPPINGS .read  .strip  .split "\n"    11.3 The Tiny ImageNet Challenge  151  30  31  32  M = [r.split "\t" [:2] for r in M] valPaths = [os.path.sep.join [config.VAL_IMAGES, m[0]]  for m in M] valLabels = le.transform [m[1] for m in M]   On Line 29 we load the entire contents of the VAL_MAPPINGS ﬁle  i.e., the tab separated ﬁle that maps validation image ﬁle names to their respective WordNet ID . For every line inside the M, we split it into two columns – the image ﬁlename and the WordNet ID  Line 30 . Based on the path to the validation images  VAL_IMAGES  along with the ﬁlenames in M, we can then construct the paths to the validation ﬁles  Line 31 . Similarly, we can transform the WordNet ID string to a unique class label integer on Line 32 by looping over the WordNet IDs in each row and applying the label encoder.  For readers who struggle to understand this section of code, I would suggest stopping here to spend a while executing each line and investigating the contents of every variable. We are making heavy use of Python list comprehensions here, which are natural, succinct methods to build lists with very little code. Again, this code block has nothing to do with deep learning – it’s simply parsing a ﬁle which is a general purpose programming problem. Take a few minutes and ensure you understand how we are able to parse the val_annotations.txt ﬁle using this code block.  Now that we have the paths to our training, validation, and testing images, we can deﬁne a datasets tuple that we’ll loop over and write the images and associated class labels for each set to HDF5, respectively:   construct a list pairing the training, validation, and testing  image paths along with their corresponding labels and output HDF5  files datasets = [   "train", trainPaths, trainLabels, config.TRAIN_HDF5 ,  "val", valPaths, valLabels, config.VAL_HDF5 ,  "test", testPaths, testLabels, config.TEST_HDF5 ]  We’ll also initialize our RGB averages as well:  42  43   initialize the lists of RGB channel averages  R, G, B  =  [], [], []   Finally, we are ready to build our HDF5 datasets for Tiny ImageNet:   loop over the dataset tuples for  dType, paths, labels, outputPath  in datasets:   create HDF5 writer print "[INFO] building {}...".format outputPath   writer = HDF5DatasetWriter  len paths , 64, 64, 3 , outputPath    initialize the progress bar widgets = ["Building Dataset: ", progressbar.Percentage  , " ",  progressbar.Bar  , " ", progressbar.ETA  ]  pbar = progressbar.ProgressBar maxval=len paths ,  widgets=widgets .start    34  35  36  37  38  39  40  45  46  47  48  49  50  51  52  53  54  55  On Line 46 we loop over the dataset type  dType , paths, labels, and outputPath in the datasets list. For each of these output HDF5 ﬁles we’ll create an HDF5DatasetWriter which will   Chapter 11. GoogLeNet 152 store a total of len paths  images, each of which is a 64× 64× 3 RGB image  Line 49 . Lines 52-55 simply initialize a progressbar so we can easily visualize the dataset creation process.  We now need to loop over each path and label pair in the respective set:  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  79  80  81  82  83  84  85   loop over the image paths for  i,  path, label   in enumerate zip paths, labels  :   load the image from disk image = cv2.imread path    if we are building the training dataset, then compute the  mean of each channel in the image, then update the  respective lists if dType == "train":   b, g, r  = cv2.mean image [:3] R.append r  G.append g  B.append b    add the image and label to the HDF5 dataset writer.add [image], [label]  pbar.update i    close the HDF5 writer pbar.finish   writer.close    For each image, we load it from disk on Line 60. If the image is a training image, we need to compute the RGB average of the image and update the respective lists  Lines 66-69 . Line 72 adds the image  which is already 64× 64× 3  and label to the HDF5 dataset while Line 77 closes the dataset.  The ﬁnal step is to compute the RGB averages across the entire dataset and write them to disk:   construct a dictionary of averages, then serialize the means to a  JSON file print "[INFO] serializing means..."  D = {"R": np.mean R , "G": np.mean G , "B": np.mean B } f = open config.DATASET_MEAN, "w"  f.write json.dumps D   f.close    To build the Tiny ImageNet dataset in HDF5 format, just execute the following command:  $ python build_tiny_imagenet.py [INFO] building .. datasets tiny-imagenet-200 hdf5 train.hdf5... Building Dataset: 100%  Time: 0:00:36 [INFO] building .. datasets tiny-imagenet-200 hdf5 val.hdf5... Building Dataset: 100%  Time: 0:00:04 [INFO] building .. datasets tiny-imagenet-200 hdf5 test.hdf5... Building Dataset: 100%  Time: 0:00:05 [INFO] serializing means...   11.4 DeeperGoogLeNet on Tiny ImageNet  153  After the script ﬁnishes executing, you’ll have three ﬁles in your hdf5 directory, train.hdf5, val.hdf5, and test.hdf5. You can investigate each of these ﬁles with the h5py library if you wish to validate that the datasets do indeed contain the images:  >>> import h5py >>> filenames = ["train.hdf5", "val.hdf5", "test.hdf5"] >>> for filename in filenames: ... ... ... ...  90000, 64, 64, 3   10000, 64, 64, 3   10000, 64, 64, 3   db = h5py.File filename, "r"  print db["images"].shape  db.close    We’ll be using these HDF5 dataset representations of ImageNet to train both GoogLeNet in  this chapter as well as ResNet in the following chapter.  11.4 DeeperGoogLeNet on Tiny ImageNet  Now that we have our HDF5 representation of the Tiny ImageNet dataset, we are ready to train GoogLeNet on it – but instead of using MiniGoogLeNet as in the previous section, we are going to use a deeper variant which more closely models the Szegedy et al. implementation. This deeper variation will use the original Inception module as detailed in Figure 11.1 earlier in this chapter, which will help you understand the original architecture and implement it on your own in the future. To get started, we’ll ﬁrst learn how to implement this deeper network architecture. We’ll then train DeeperGoogLeNet on the Tiny ImageNet dataset and evaluate the results in terms of rank-1 and rank-5 accuracy.  11.4.1 Implementing DeeperGoogLeNet  I have provided a ﬁgure  replicated and modiﬁed from Szegedy et al.  detailing our Deeper- GoogLeNet architecture in Figure 11.9. There are only two primary differences between our implementation and the full GoogLeNet architecture used by Szegedy et al. when training the network on the complete ImageNet dataset: 1. Instead of using 7× 7 ﬁlters with a stride of 2× 2 in the ﬁrst CONV layer, we use 5× 5 ﬁlters with a 1×1 stride. We use these due to the fact that our implementation of GoogLeNet is only able to accept 64× 64× 3 input images while the original implementation was constructed to accept 224× 224× 3 images. If we applied 7× 7 ﬁlters with a 2× 2 stride, we would reduce our input dimensions too quickly.  2. Our implementation is slightly shallower with two fewer Inception modules – in the original Szegedy et al. paper, two more Inception modules were added prior to the average pooling operation. This implementation of GoogLeNet will be more than enough for us to perform well on Tiny ImageNet and claim a spot on the cs231n Tiny ImageNet leaderboard. For readers who are interested in training the full GoogLeNet architecture from scratch on the entire ImageNet dataset  thereby replicating the performance of the Szegedy et al. experiments , please refer to Chapter 7 in the ImageNet Bundle.  To implement our DeeperGoogLeNet class, let’s create a ﬁle name deeergooglenet.py  inside the nn.conv sub-module of pyimagesearch:   154  Chapter 11. GoogLeNet  Figure 11.9: Our modiﬁed GoogLeNet architecture which we will call “DeeperGoogLeNet”. The DeeperGoogLNet architecture is identical to the original GoogLeNet architecture with two modiﬁcations:  1  5× 5 ﬁlters with a stride of 1× 1 are used in the ﬁrst CONV layer and  2  the ﬁnal two inception modules  5a and 5b  are left out.  --- pyimagesearch                  --- __init__.py --- callbacks --- io --- nn           --- preprocessing --- utils  --- __init__.py --- conv          --- __init__.py --- alexnet.py --- deepergooglenet.py --- lenet.py --- minigooglenet.py --- minivggnet.py --- fcheadnet.py --- shallownet.py  From there, we can start working on the implementation:  1  2  3  4  5  6  7   import the necessary packages from keras.layers.normalization import BatchNormalization from keras.layers.convolutional import Conv2D from keras.layers.convolutional import AveragePooling2D from keras.layers.convolutional import MaxPooling2D from keras.layers.core import Activation from keras.layers.core import Dropout   11.4 DeeperGoogLeNet on Tiny ImageNet  155  8  9  10  11  12  13  14  from keras.layers.core import Dense from keras.layers import Flatten from keras.layers import Input from keras.models import Model from keras.layers import concatenate from keras.regularizers import l2 from keras import backend as K  Lines 2-14 start by importing our required Python packages. Notice how we’ll be using the Input and Model classes as in our MiniGoogLeNet implementation so we can construct a graph rather than a sequential network – this graph construct is a requirement due to how the Inception module branches. Also take note of Line 13 where we import the l2 class, implying that we will allow L2 weight regularization in the network to help reduce overﬁtting.  As a matter of convenience  and to ensure our code doesn’t become bloated , let’s deﬁne a conv_module function that will be responsible for accepting an input layer, performing a CONV => BN => RELU, and then returning the output. Typically I would prefer to place the BN after the RELU, but since we are replicating the original work of Szegedy et al., let’s stick with the batch normalization prior to the activation. The implementation of conv_module can be seen below:  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  class DeeperGoogLeNet:  @staticmethod def conv_module x, K, kX, kY, stride, chanDim,  padding="same", reg=0.0005, name=None :  initialize the CONV, BN, and RELU layer names  convName, bnName, actName  =  None, None, None    if a layer name was supplied, prepend it if name is not None:  convName = name + "_conv" bnName = name + "_bn" actName = name + "_act"   define a CONV => BN => RELU pattern x = Conv2D K,  kX, kY , strides=stride, padding=padding,  kernel_regularizer=l2 reg , name=convName  x   x = BatchNormalization axis=chanDim, name=bnName  x  x = Activation "relu", name=actName  x    return the block return x  The conv_module method accepts a number of parameters, including:   x: The input to the network.   K: The number of ﬁlters the convolutional layer will learn.   kX and kY: The ﬁlter size for the convolutional layer.   stride: The stride  in pixels  for the convolution. Typically we’ll use a 1× 1 stride, but we   chanDim: This value controls the dimension  i.e., axis  of the image channel. It is automati- cally set later in this class based on whether we are using “channels_last” or “channels_ﬁrst” ordering.  could use a larger stride if we wished to reduce the output volume size.    padding: Here we can control the padding of the convolution layer.   reg: The L2 weight decay strength.   156  Chapter 11. GoogLeNet   name: Since this network is deeper than all others we have worked with in this book, we may wish to name the blocks of layers to help us  1  debug the network and  2  share explain the network to others.  Line 21 initializes the name of each of the convolution, batch normalization, and activation layers, respectively. Provided that the name parameter to conv_module is not None, we then update the names of our layers  Lines 24-27 .  Deﬁning the CONV => BN => RELU layer pattern is accomplished on Lines 30-33 – notice how the name of each layer is also included. Again, the primary beneﬁt of naming each layer is that we can visualize the name in the output, just as we did in Chapter 19 in the Starter Bundle. For example, using the plot_model on the conv_module would result in a chart similar to Figure 11.10.  Figure 11.10: A sample of the DeeperGoogLeNet architecture visualization which includes the actual names for each layer. Naming layers makes it easier to keep track of them in larger, deeper networks.  Notice how the names of each layer are included in the chart, which is especially helpful when you are working with deep CNNs and can easily get “lost” examining the massive charts. The output of the conv_module is then returned to the calling function on Line 36. Next, let’s deﬁne the inception_module, as detailed by Szgedy et al.  in their original  publication and displayed in Figure 11.1:  38  39  40  41  42  43  44  45  @staticmethod def inception_module x, num1x1, num3x3Reduce, num3x3, num5x5Reduce, num5x5, num1x1Proj, chanDim, stage, reg=0.0005 :  define the first branch of the Inception module which  consists of 1x1 convolutions first = DeeperGoogLeNet.conv_module x, num1x1, 1, 1,  1, 1 , chanDim, reg=reg, name=stage + "_first"   The Inception module includes four branches, the outputs of which are concatenated along the channel dimension. The first branch of in the Inception module simply performs a series of 1× 1 convolutions – these enable the Inception module to learn local features. The second branch of the Inception module ﬁrst performs dimensionality reduction via 1× 1 convolution followed by expanding with a 3× 3 convolution – we call these our num3x3Reduce   11.4 DeeperGoogLeNet on Tiny ImageNet  157  and num3x3 variables, respectively:   define the second branch of the Inception module which  consists of 1x1 and 3x3 convolutions second = DeeperGoogLeNet.conv_module x, num3x3Reduce, 1, 1,   1, 1 , chanDim, reg=reg, name=stage + "_second1"   second = DeeperGoogLeNet.conv_module second, num3x3, 3, 3,   1, 1 , chanDim, reg=reg, name=stage + "_second2"   Here we can see that the ﬁrst conv_module applies the 1× 1 convolutions to the input. The output of these 1× 1 convolutions are then passed into the second conv_module which performs a series of 3× 3 convolutions. The number of 1× 1 convolutions is always smaller than the number of 3× 3 convolutions, thereby serving as a form of dimensionality reduction. The third branch in Inception is identical to the second branch, only instead of performing a 1×1 reduce followed by a 3×3 expand, we are now going to use a 1×1 reduce and a 5×5 expand:   define the third branch of the Inception module which  are our 1x1 and 5x5 convolutions third = DeeperGoogLeNet.conv_module x, num5x5Reduce, 1, 1,   1, 1 , chanDim, reg=reg, name=stage + "_third1"   third = DeeperGoogLeNet.conv_module third, num5x5, 5, 5,   1, 1 , chanDim, reg=reg, name=stage + "_third2"   On Lines 56 and 57 we learn num5x5Reduce kernels, each of size 1× 1 based on the input to the inception_module. The output of the 1× 1 convolutions are then passed into a second conv_module which then learns a num5x5 ﬁlters, each of size 5 × 5. Again, the number of 1× 1 convolutions in this branch is always smaller than the number of 5× 5 ﬁlters. projection. Here we apply max pooling followed by a series of 1× 1 convolutions:  The fourth and ﬁnal branch of the Inception module is commonly referred to as the pool   define the fourth branch of the Inception module which  is the POOL projection fourth = MaxPooling2D  3, 3 , strides= 1, 1 , padding="same", name=stage + "_pool"  x   fourth = DeeperGoogLeNet.conv_module fourth, num1x1Proj,  1, 1,  1, 1 , chanDim, reg=reg, name=stage + "_fourth"   The rationale for this branch is partially scientiﬁc and partially anecdotal. In 2014, most  if not all  Convolutional Neural Networks that were obtaining state-of-the-art performance on the ImageNet dataset were applying max pooling. Therefore, it was believed that a CNN should apply max pooling. While GoogLeNet does apply max pooling outside of the Inception module, Szegedy et al. decided to include the pool projection branch as another form of max pooling.  Now that we have all four branches computed, we can concatenate their output along the  channel dimension and return the output to the calling function:   concatenate across the channel dimension x = concatenate [first, second, third, fourth], axis=chanDim,  name=stage + "_mixed"    return the block return x  47  48  49  50  51  52  54  55  56  57  58  59  61  62  63  64  65  66  68  69  70  71  72  73   158  Chapter 11. GoogLeNet  parameters:  If we were to call the plot_model function on the Inception module using the following   num1x1=64   num3x3Reduce=96   num3x3=128   num5x5Reduce=16   num5x5=32   num1x1Proj=32 The resulting graph would look like Figure 11.11. We can see in this visualization how the Inception module constructs four branches. The ﬁrst branch is responsible for learning local 1× 1 features. The second branch performs dimensionality reduction via a 1× 1 convolution, followed by learning a larger ﬁlter size of 3× 3. The third branch behaves similarly to the second branch, only learning 5× 5 ﬁlters rather than 3× 3 ﬁlters. Finally, the fourth branch applies max pooling.  Figure 11.11: The full Inception module proposed by Szegedy et al.  zoomed out to save space . The key takeaway here is that there are four distinct branches in the Inception module.  By learning all three 1× 1, 3× 3, and 5× 5 ﬁlters, the Inception module can learn both general  5× 5 and 3× 3  along with local  1× 1  features at the same time. The actual optimization process will automatically determine how to value these branches and layers, essentially giving us a “general purpose” module that will learn the best set of features  local, small convolutions  or higher-level abstracted features  larger convolutions  at a given time. The output of the Inception module is thus 256, which is the concatenation of all the 64 + 128 + 32 + 32 = 256 ﬁlters from each branch.  Now that the inception_module has been deﬁned, we can create the build method responsi-  ble for constructing the complete DeeperGoogLeNet architecture:  75  76  77  78  79  80  81  82  83  84  @staticmethod def build width, height, depth, classes, reg=0.0005 :   initialize the input shape to be "channels last" and the  channels dimension itself inputShape =  height, width, depth  chanDim = -1   if we are using "channels first", update the input shape  and channels dimension if K.image_data_format   == "channels_first":   85  86  88  89  90  91  92  93  94  95  96  97  98  99  100  102  103  104  105  106  107  108  11.4 DeeperGoogLeNet on Tiny ImageNet  159  inputShape =  depth, height, width  chanDim = 1  Our build method will accept the spatial input dimensions of our images, including the width, height, and depth. We’ll also be able to supply the number of class labels the network is to learn, along with an optional regularization term for L2 weight decay. Lines 77-86 then handle properly setting the inputShape and chanDim based on the “channels last” or “channels ﬁrst” conﬁguration in Keras.  Following Figure 11.9 above, our ﬁrst block of layers will perform a sequence of CONV =>  POOL =>  CONV * 2  => POOL:   define the model input, followed by a sequence of CONV =>  POOL =>  CONV * 2  => POOL layers inputs = Input shape=inputShape  x = DeeperGoogLeNet.conv_module inputs, 64, 5, 5,  1, 1 ,  chanDim, reg=reg, name="block1"   x = MaxPooling2D  3, 3 , strides= 2, 2 , padding="same",  name="pool1"  x   x = DeeperGoogLeNet.conv_module x, 64, 1, 1,  1, 1 ,  chanDim, reg=reg, name="block2"   x = DeeperGoogLeNet.conv_module x, 192, 3, 3,  1, 1 ,  chanDim, reg=reg, name="block3"   x = MaxPooling2D  3, 3 , strides= 2, 2 , padding="same",  name="pool2"  x   The ﬁrst CONV layer learns 64 5× 5 ﬁlters with a stride of 1× 1. We then apply max pooling with a window size of 3× 3 and stride of 2× 2 to reduce the volume size of the input. Lines 95-98 are responsible for performing a reduce and expand. First, 64 1× 1 ﬁlters are learned  Lines 95 and 96 . Then, 192 3× 3 ﬁlters are learned on Lines 97 and 98. This process is very similar to the Inception module  which will be applied in later layers of the network , only without the branching factor. Finally, another max pooling is performed on Lines 99 and 100.  Next, let’s apply two Inception modules  3a and 3b  followed by a max pooling:   apply two Inception modules followed by a POOL x = DeeperGoogLeNet.inception_module x, 64, 96, 128, 16,  32, 32, chanDim, "3a", reg=reg   x = DeeperGoogLeNet.inception_module x, 128, 128, 192, 32,  96, 64, chanDim, "3b", reg=reg   x = MaxPooling2D  3, 3 , strides= 2, 2 , padding="same",  name="pool3"  x   Looking at this code, you might wonder how we decided on the number of ﬁlters for each CONV layer. The answer is that all parameter values in this network were taken directly from the original Szegedy et al. paper on GoogLeNet where the authors performed a number of experiments to tune the parameters. In every case, you’ll notice a common pattern with all Inception modules: 1. The number of 1× 1 ﬁlters we learn in the ﬁrst branch of the Inception module will be less 2. The number of 1× 1 ﬁlters will always be smaller than the 3× 3 and 5× 5 convolutions they 3. The number of ﬁlters we learn in the 3× 3 branch will be more than the 5× 5 branch which  than or equal to the 1× 1 ﬁlters in the 3× 3  second  and 5× 5  third  branches.  feed into.  helps reduce the size of the network as well as improves the speed of training evaluation.   160  Chapter 11. GoogLeNet 4. The number of pool projection ﬁlters will always be smaller than the ﬁrst branch of 1×1 local  5. Regardless of branch type, the number of ﬁlters will increase  or at least remain the same  as  features.  we go deeper in the network.  While there are certainly more parameters to keep track of when implementing this network, we’re still following the same general rules of thumb as in previous CNNs – the deeper the network gets, the smaller the volume size is; therefore, the more ﬁlters we learn to compensate.  The network continues to become deeper, learning richer features as we now stack ﬁve Inception  modules  4a-4e  on top of each other before applying a POOL:   apply five Inception modules followed by POOL x = DeeperGoogLeNet.inception_module x, 192, 96, 208, 16,  48, 64, chanDim, "4a", reg=reg   x = DeeperGoogLeNet.inception_module x, 160, 112, 224, 24,  64, 64, chanDim, "4b", reg=reg   x = DeeperGoogLeNet.inception_module x, 128, 128, 256, 24,  64, 64, chanDim, "4c", reg=reg   x = DeeperGoogLeNet.inception_module x, 112, 144, 288, 32,  64, 64, chanDim, "4d", reg=reg   x = DeeperGoogLeNet.inception_module x, 256, 160, 320, 32,  128, 128, chanDim, "4e", reg=reg   x = MaxPooling2D  3, 3 , strides= 2, 2 , padding="same",  name="pool4"  x   After the ﬁnal POOL on Lines 121 and 122, our volume size is 4 x 4 x classes. To avoid the usage of computationally expensive fully-connected layers  not to mention, dramatically increased network size , we apply average pooling with a 4× 4 kernel to reduce the volume size to 1 x 1 x classes:   apply a POOL layer  average  followed by dropout x = AveragePooling2D  4, 4 , name="pool5"  x  x = Dropout 0.4, name="do"  x    softmax classifier x = Flatten name="flatten"  x  x = Dense classes, kernel_regularizer=l2 reg ,  name="labels"  x   x = Activation "softmax", name="softmax"  x    create the model model = Model inputs, x, name="googlenet"    return the constructed network architecture return model  Dropout is then applied with a probability of 40%. Typically we would use a 50% dropout rate,  but again, we are simply following the original implementation.  Lines 130 and 131 create the Dense layer for the total number of classes we wish to learn. A softmax classiﬁer is then applied after the fully-connected layer on Line 132. Finally, the actual Model is constructed based on the inputs and x, the actual computational network graph. This model is returned to the calling function on Line 138.  110  111  112  113  114  115  116  117  118  119  120  121  122  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138   11.4 DeeperGoogLeNet on Tiny ImageNet  161  11.4.2 Training DeeperGoogLeNet on Tiny ImageNet  Now that our DeeperGoogLeNet architecture is implemented, we need to create a Python script that will train the network on Tiny ImageNet. We’ll also need to create a second Python script that will be responsible for evaluating our model on the testing set by computing rank-1 and rank-5 accuracies.  Once we have completed both of these tasks, I’ll share three experiments I ran when gathering the results for this chapter. These experiments will form a “case study” and enable you to learn how to run an experiment, investigate the results, and make an educated guess on how to tune your hyperparameters to obtain a better performing network in your next experiment.  11.4.3 Creating the Training Script  Let’s go ahead and implement the training script – open up a new ﬁle, name it train.py, and insert the following code:   set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use "Agg"    import the necessary packages from config import tiny_imagenet_config as config from pyimagesearch.preprocessing import ImageToArrayPreprocessor from pyimagesearch.preprocessing import SimplePreprocessor from pyimagesearch.preprocessing import MeanPreprocessor from pyimagesearch.callbacks import EpochCheckpoint from pyimagesearch.callbacks import TrainingMonitor from pyimagesearch.io import HDF5DatasetGenerator from pyimagesearch.nn.conv import DeeperGoogLeNet from keras.preprocessing.image import ImageDataGenerator from keras.optimizers import Adam from keras.models import load_model import keras.backend as K import argparse import json  Lines 2 and 3 instruct the matplotlib library to use a backend such that we can save our loss and accuracy plots to disk. We then import the remainder of our required Python packages on Lines 6-19. Take a look at Line 6 where we import the conﬁguration ﬁle for the Tiny ImageNet experiment. We also import our implementation of DeeperGoogLeNet on Line 13. All the imports should feel relatively familiar to you now.  From there, we can parse our command line arguments:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-c", "--checkpoints", required=True,  help="path to output checkpoint directory"   ap.add_argument "-m", "--model", type=str,  help="path to *specific* model checkpoint to load"   ap.add_argument "-s", "--start-epoch", type=int, default=0,  help="epoch to restart training at"   args = vars ap.parse_args     1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  21  22  23  24  25  26  27  28  29   162  Chapter 11. GoogLeNet  We’ll be using the ctrl + c method to train our network, meaning that we’ll start the train- ing process, monitor how the training is going, then stop script if overﬁtting stagnation occurs, adjust any hyperparameters, and restart training. To start, we’ll ﬁrst need the --checkpoints switch, which is the path to the output directory that will store individual checkpoints for the DeeperGoogLeNet model. If we are restarting training, then we’ll need to supply the path to a speciﬁc --model that we are restarting training from. Similarly, we’ll also need to supply --start-epoch to obtain the integer value of the epoch we are restarting training from.  In order to obtain reasonable accuracy on the Tiny ImageNet dataset, we’ll need to apply data  augmentation to the training data:   construct the training image generator for data augmentation aug = ImageDataGenerator rotation_range=18, zoom_range=0.15,  width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode="nearest"    load the RGB means for the training set means = json.loads open config.DATASET_MEAN .read     We’ll also load our RGB means  Line 37  for mean subtraction and normalization. Let’s move on to instantiating both our image pre-processors as well as the training and  validation HDF5 dataset generators:   initialize the image preprocessors sp = SimplePreprocessor 64, 64  mp = MeanPreprocessor means["R"], means["G"], means["B"]  iap = ImageToArrayPreprocessor     initialize the training and validation dataset generators trainGen = HDF5DatasetGenerator config.TRAIN_HDF5, 64, aug=aug,  preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES   valGen = HDF5DatasetGenerator config.VAL_HDF5, 64,  preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES   Both the training and validation generator will apply: 1. A simple preprocessor to ensure the image is resized to 64× 64 pixels  which it already  should be, but we’ll include it here as a matter of completeness .  2. Mean subtraction to normalize the data. 3. An image to Keras-compatible array converter. We’ll be training our network in mini-batch sizes of 64. In the case that we are training  DeeperGoogLeNet from the ﬁrst epoch, wet must instantiate the network and optimizer:   if there is no specific model checkpoint supplied, then initialize  the network and compile the model if args["model"] is None:  print "[INFO] compiling model..."  model = DeeperGoogLeNet.build width=64, height=64, depth=3,  classes=config.NUM_CLASSES, reg=0.0002   opt = Adam 1e-3  model.compile loss="categorical_crossentropy", optimizer=opt,  metrics=["accuracy"]   31  32  33  34  35  36  37  39  40  41  42  43  44  45  46  47  48  50  51  52  53  54  55  56  57  58   11.4 DeeperGoogLeNet on Tiny ImageNet  163  Notice here how we are applying a L2 regularization strength of 0.0002 as well as the Adam  optimizer – we’ll ﬁnd out why in Section 11.4.5 below.  Otherwise, we must be restarting training from a speciﬁc epoch, so we’ll need to load the model  and adjust the learning rate:   otherwise, load the checkpoint from disk else:  print "[INFO] loading {}...".format args["model"]   model = load_model args["model"]    update the learning rate print "[INFO] old learning rate: {}".format   K.get_value model.optimizer.lr    K.set_value model.optimizer.lr, 1e-5  print "[INFO] new learning rate: {}".format   K.get_value model.optimizer.lr     We’ll create two callbacks, one to serialize the model weights to disk every ﬁve epochs and  another to create our loss accuracy plot over time:   construct the set of callbacks callbacks = [  EpochCheckpoint args["checkpoints"], every=5,  startAt=args["start_epoch"] ,  TrainingMonitor config.FIG_PATH, jsonPath=config.JSON_PATH,  startAt=args["start_epoch"] ]  Finally, we can train our network:   train the network model.fit_generator   trainGen.generator  , steps_per_epoch=trainGen.numImages    64, validation_data=valGen.generator  , validation_steps=valGen.numImages    64, epochs=10, max_queue_size=64 * 2, callbacks=callbacks, verbose=1    close the databases trainGen.close   valGen.close    60  61  62  63  64  65  66  67  68  69  70  72  73  74  75  76  77  79  80  81  82  83  84  85  86  87  88  89  90  91  The exact number of epochs we choose to train our network for will depend on how our loss accuracy plots look. We will make an informed decision regarding updating learning rates or applying early stopping based on model performance.  11.4.4 Creating the Evaluation Script  Once we are satisﬁed with our model performance on the training and validation set, we can move on to evaluating the network on the testing set. To do so, let’s create a new ﬁle named rank_accuracy.py:   164  Chapter 11. GoogLeNet   import the necessary packages from config import tiny_imagenet_config as config from pyimagesearch.preprocessing import ImageToArrayPreprocessor from pyimagesearch.preprocessing import SimplePreprocessor from pyimagesearch.preprocessing import MeanPreprocessor from pyimagesearch.utils.ranked import rank5_accuracy from pyimagesearch.io import HDF5DatasetGenerator from keras.models import load_model import json  To start, we’ll import our required Python packages. The rank5_accuracy function is imported  on Line 6 so we can compute both the rank-1 and rank-5 accuracy on the dataset.  From there, we load our RGB means, initialize our image pre-processors  in the same manner  as we did for testing , and then initialize the testing dataset generator:   load the RGB means for the training set means = json.loads open config.DATASET_MEAN .read      initialize the image preprocessors sp = SimplePreprocessor 64, 64  mp = MeanPreprocessor means["R"], means["G"], means["B"]  iap = ImageToArrayPreprocessor     initialize the testing dataset generator testGen = HDF5DatasetGenerator config.TEST_HDF5, 64,  preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES   The following code block handles loading the pre-trained model from disk via the MODEL_PATH  we supply in our conﬁguration ﬁle:   load the pre-trained network print "[INFO] loading model..."  model = load_model config.MODEL_PATH   You should set MODEL_PATH to be the ﬁnal epoch checkpoint after training is complete. Al- ternatively, you can set this variable to earlier epochs to obtain an understanding on how testing accuracy increases in later epochs.  Once the model is loaded we can make predictions on the testing data and display both the  rank-1 and rank-5 accuracies:   make predictions on the testing data print "[INFO] predicting on test data..."  predictions = model.predict_generator testGen.generator  , steps=testGen.numImages    64, max_queue_size=64 * 2    compute the rank-1 and rank-5 accuracies  rank1, rank5  = rank5_accuracy predictions, testGen.db["labels"]  print "[INFO] rank-1: {:.2f}%".format rank1 * 100   print "[INFO] rank-5: {:.2f}%".format rank5 * 100     close the database testGen.close    1  2  3  4  5  6  7  8  9  11  12  13  14  15  16  17  18  19  20  21  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38   11.4 DeeperGoogLeNet on Tiny ImageNet  165  Epoch 1− 25 26− 35 36− 65  Learning Rate 1e− 2 1e− 3 1e− 4  Table 11.1: Learning rate schedule used when training DeeperGoogLeNet on Tiny ImageNet for Experiment 1.  11.4.5 DeeperGoogLeNet Experiments  In the following sections I have included the results of four separate experiments I ran when training DeeperGoogLeNet on Tiny ImageNet. After each experiment I evaluated the results and then made an educated decision on how the hyperparameters and network architecture should be updated to increase accuracy.  Case studies like these are especially helpful to you as a budding deep learning practitioner. Not only do they demonstrate that deep learning is an iterative process requiring many experiments, but they also show which parameters you should be paying attention to and how to update them.  Finally, it’s worth noting that some of these experiments required changes to the code. Both the implementations of deepergooglenet.py and train.py are my ﬁnal implementations that obtained the best accuracy. I’ll note the changes I made in earlier experiments in case you want to replicate my  less accurate  results.  DeeperGoogLeNet: Experiment 1 Given that this was my ﬁrst time training a network on the Tiny ImageNet challenge, I wasn’t sure what the optimal depth should be for a given architecture on this dataset. While I knew Tiny ImageNet would be a challenging classiﬁcation task, I didn’t think Inception modules 4a-4e were required, so I removed them from our DeeperGoogLeNet implementation above, leading to a substantially more shallow network architecture. I decided to train DeeperGoogLenet using SGD with an initial learning rate of 1e− 2 and mo- I always use SGD in my ﬁrst mentum term of 0.9  no Nesterov acceleration was applied . experiment. Per my guidelines and rules of thumb in Chapter 7, you should ﬁrst try SGD to obtain a baseline, and then if need be, use more advanced optimization methods.  I started training using the following command:  $ python train.py --checkpoints output checkpoints  The learning rate schedule detailed in Table 11.1 was then used. This table implies that after epoch 25 I stopped training, lowered the learning rate to 1e− 3, then resumed training for another 10 epochs:  $ python train.py --checkpoints output checkpoints \  --model output checkpoints epoch_25.hdf5 --start-epoch 25  After epoch 35 I again stopped training, lowered the learning rate to 1e− 4, and then resumed  training for thirty more epochs:  $ python train.py --checkpoints output checkpoints \  --model output checkpoints epoch_35.hdf5 --start-epoch 35   166  Chapter 11. GoogLeNet  Training for an extra thirty epochs was excessive, to say the least; however, I wanted to get a feel for the level of overﬁtting to expect for a large number of epochs after the original learning rate had been dropped  as this was the ﬁrst time I had worked with GoogLeNet + Tiny ImageNet . You can see a plot of the loss accuracy over time for both the training and validation in Figure 11.12  top-left .  Figure 11.12: Top-left: Plots for Experiment 1. Top-right: Learning curves for Experiment 2. Bottom: Training validation plots for Experiment 3. The ﬁnal experiment obtains the best validation accuracy at 55.77%.  Starting at approximately epoch 15, there is a divergence in training and validation loss. By the time we get to epoch 25, the divergence is getting more signiﬁcant, so I lowered the learning rate by an order of magnitude; the result is a nice jump in accuracy and decrease in loss. The problem is that after this point, both training and validation learning essentially stagnate. Even lowering the learning rate to 1e− 4 at epoch 35 does not introduce an extra boost in accuracy.  At the end of epoch 40 learning has stagnated completely. Had I not wanted to see the effects of a low learning rate for a long period of time, I would have stopped training after epoch 45. In this case, I let the network train until epoch 65  with no change in loss accuracy  where I stopped training and examined the results, noting that the network was obtaining 52.25% rank-1 accuracy on the validation set. However, given how our network performance plateaued quickly after dropping the learning rate, I determined that there is clearly more work to be done.   11.4 DeeperGoogLeNet on Tiny ImageNet  167  Epoch 1− 20 21− 30 31− 40  Learning Rate 1e− 3 1e− 4 1e− 5  Epoch 1− 40 41− 60 61− 70  Learning Rate 1e− 3 1e− 4 1e− 5  Table 11.2: Left: The learning rate schedule used when training DeeperGoogLeNet on Tiny ImageNet in Experiment 2. Right: The learning rate schedule for Experiment 3.  DeeperGoogLeNet: Experiment 2 In my second experiment with DeeperGoogLeNet + Tiny ImageNet, I decided to switch out the SGD optimizer for Adam. This decision was made strictly because I wasn’t convinced that the network architecture needed to be deeper  yet . The Adam optimizer was used with the default initial learning rate of 1e− 3. I then used the learning rate schedule in Table 11.2  left  to lower the learning rate.  A plot of the learning curves can be seen below in Figure 11.12  top-right . This plot looks very similar to the top-left above. We initially start off strong, but validation loss diverges quickly past epoch 10, forcing me to lower the learning rate at epoch 20  or otherwise risk overﬁtting . As soon as the learning rate is reduced, learning plateaus and I am unable to increase accuracy, even reducing learning rate a second time. However, at the end of the 40th epoch, I noticed that my validation loss was lower than the previous experiment and my accuracy was higher.  By swapping out SGD for Adam, I was able to boost validation accuracy to 54.20% rank-1, an increase of nearly 2%. However, I still had the issue of learning stagnation as soon as the initial learning rate was lowered.  DeeperGoogLeNet: Experiment 3 Given the learning stagnation, I postulated that the network was not deep enough to model the underlying patterns in the Tiny ImageNet dataset. Therefore, I decided to enable the Inception modules 4a-4e, creating a much deeper network architecture capable of learning deeper, more discriminative features. The Adam optimizer with an initial learning rate of 1e− 3 was used to train the network. I left the L2 weight decay term at 0.0002. DeeperGoogLeNet was then trained according to Table 11.2  right .  You can see the plot in Figure 11.12  bottom . Immediately you’ll notice that using the deeper network architecture enabled me to train for longer without risking stagnation or severe overﬁtting. At the end of the 70th epoch, I was obtaining 55.77% rank-1 accuracy on the validation set.  At this point, I decided it was time to evaluate DeeperGoogLeNet on the testing set:  $ python rank_accuracy.py [INFO] loading model... [INFO] predicting on test data... [INFO] rank-1: 54.38% [INFO] rank-5: 78.96%  The evaluation script reported a rank-1 accuracy of 54.38%, or an error rate of 1− 0.5438 = 0.4562. It’s also interesting to note that our rank-5 accuracy is 78.96%, which is quite impressive for this challenge. Looking at the Tiny ImageNet leaderboard  http:  pyimg.co h5q0o  below, we can see this error rate is enough to claim the 7 position, a great start to journey to climb to the top of the leaderboard  Figure 11.13 .   168  Chapter 11. GoogLeNet  Figure 11.13: Our ﬁrst successful attempt at training DeeperGoogLeNet on the Tiny ImageNet dataset allows us to claim the 7 position on the leaderboard. We’ll be able to reach higher positions in our next chapter on ResNet.  R  Positions 1-4 on the Tiny ImageNet leaderboard were achieved by transfer learning via ﬁne-tuning on networks already trained on the full ImageNet dataset. Since we are training our networks from scratch, we are more concerned with claiming the 5 position, the highest position achieved without transfer learning. As we’ll ﬁnd out in the next chapter on ResNet, we’ll easily be able to claim this position. For more information on the techniques the cs231n students used to achieve their error rates, please see the Stanford cs231n project page [4].  For readers interested in trying to boost the accuracy of DeeperGoogLeNet further, I would  suggest the following experiments:  1. Change the conv_module to use CONV => RELU => BN instead of the original CONV => BN 2. Attempt using ELUs instead of ReLUs, which will likely lead to a small 0.5− 1% gain in  => RELU ordering.  In this chapter, we reviewed the work of Szgedy et al. [17] which introduced the now famous Inception module. The Inception module is an example of a micro-architecture, a building block that ﬁts into the overall macro-architecture of the network. Current state-of-the-art Convolutional Neural Networks tend to use some form of micro-architecture.  We then applied the Inception module to create two variants of GoogLeNet: 1. One for CIFAR-10. 2. And another for the more challenging Tiny ImageNet. When training on CIFAR-10, we obtained our best accuracy thus far of 90.81%  and improve-  ment from the previous best of 84% .  On the challenging Tiny ImageNet dataset we reached 54.38% rank-1 and 78.96% rank-5 accuracy on the testing set, enabling us to claim position 7 on the Tiny ImageNet leaderboard. Our goal is to climb the leaderboard to position 5  the highest position obtained when training a network from scratch, all higher positions applied ﬁne-tuning on networks pre-trained on the  accuracy.  11.5 Summary   11.5 Summary  169  ImageNet dataset, giving them an unfair advantage . To reach our goal of position 5, we’ll need to use the ResNet architecture detailed in the following chapter.    12. ResNet  In our previous chapter, we discussed the GoogLeNet architecture and the Inception module, a micro-architecture that acts as a building block in the overall macro-architecture. We are now going to discuss another network architecture that relies on micro-architectures – ResNet.  ResNet uses what’s called a residual module to train Convolutional Neural Networks to depths previously thought impossible. For example, in 2014, the VGG16 and VGG19 architectures were considered very deep [11]. However, with ResNet, we have successfully trained networks with > 100 layers on the challenging ImageNet dataset and over 1,000 layers on CIFAR-10 [24].  These depths are only made possible by using “smarter” weight initialization algorithms  such as Xavier Glorot [44] and MSRA He et al. [45]  along with identity mapping, a concept we’ll discuss later in this chapter. Given the depths of ResNet networks, perhaps it comes as no surprise that in ResNet took ﬁrst place in all three ILSVRC 2015 challenges  classiﬁcation, detection, and localization .  In this chapter, we are going to discuss the ResNet architecture, the residual module, along with updates to the residual module that have made it capable of obtaining higher classiﬁcation accuracy. From there we’ll implement and train variants of ResNet on the CIFAR-10 dataset and the Tiny ImageNet challenge – in each case, our ResNet implementations will outperform every experiment we have executed in this book.  12.1 ResNet and the Residual Module  First introduced by He et al. in their 2015 paper, Deep Residual Learning for Image Recognition [24], the ResNet architecture has become a seminal work, demonstrating that extremely deep networks can be trained using standard SGD and a reasonable initialization function. In order to train networks at depths greater than 50-100  and in some cases, 1,000  layers, ResNet relies on a micro-architecture called the residual module.  Another interesting component of ResNet is that pooling layers are used extremely sparingly. Building on the work of Springenberg et al. [41], ResNet does not strictly rely on max pooling operations to reduce volume size. Instead, convolutions with strides > 1 are used to not only learn   172  Chapter 12. ResNet  weights, but reduce the output volume spatial dimensions. In fact, there are only two occurrences of pooling being applied in the full implementation of the architecture:  1. The ﬁrst  and only  currency of max pooling happens early in the network to help reduce  2. The second pooling operation is actually an average pooling layer used in place of fully-  spatial dimensions.  connected layers, like in GoogLeNet.  Strictly speaking, there is only one max pooling layer – all other reductions in spatial dimensions  are handled by convolutional layers.  In this section, we’ll review the original residual module, along with the bottleneck residual module used to train deeper networks. From there, we’ll discuss extensions and updates to the original residual module by He et al. in their 2016 publication, Identity Mappings in Deep Residual Networks [33], that allow us to further increase classiﬁcation accuracy. Later in this chapter, we’ll implement ResNet from scratch using Keras.  12.1.1 Going Deeper: Residual Modules and Bottlenecks  The original residual module introduced by He et al. in 2015 relies on identity mappings, the process of taking the original input to the module and adding it to the output of a series of operations. A graphical depiction of this module can be seen in Figure 12.1  left . Notice how this module only has two branches, unlike the four branches in the Inception module of GoogLeNet. Furthermore, this module is highly simplistic.  Figure 12.1: Left: The original Residual module proposed by He et al. Right: The more commonly used bottleneck variant of the Residual module.  At the top of the module, we accept an input to the module  i.e., the previous layer in the network . The right branch is a “linear shortcut" – it connects the input to an addition operation at the bottom of the module. Then, on the left branch of the residual module, we apply a series of convolutions  all of which are 3× 3 , activations, and batch normalizations. This is a fairly standard pattern to follow when constructing Convolutional Neural Networks.  But what makes ResNet interesting is that He et al. suggested adding the original input to the output of the CONV, RELU and BN layers. We call this addition an identity mapping since the input  the identity  is added to the output of series of operations. It is also why the term “residual” is used. The “residual” input is added to the output of a series of layer operations. The connection between the input and the addition node is called the shortcut. Note that we are not referring to concatenation along the channel dimension as we have done in previous chapters. Instead, we are performing simple 1 + 1 = 2 addition at the bottom of the module between the two branches.  While traditional neural network layers can be seen as learning a function y = f  x , a residual layer attempts to approximate y via f  x  + id x  = f  x  + x where id x  is the identity function.   12.1 ResNet and the Residual Module  173  These residual layers start at the identity function and evolve to become more complex as the network learns. This type of residual learning framework allows us to train networks that are sub- stantially deeper than previously proposed network architectures.  Furthermore, since the input is included in every residual module, it turns out the network can learn faster and with larger learning rates. It is very common to see the base learning rates for ResNet implementations start at 1e− 1. For most architectures such as AlexNet or VGGNet, this high of a learning rate would almost guarantee the network would not converge. But since ResNet relies on residual modules via identity mappings, this higher learning rate is completely possible. In the same 2015 work, He et al. also included an extension to the original residual module called bottlenecks  Figure 12.1, right . Here we can see that the same identity mapping is taking place, only now the CONV layers in the left branch of the residual module have been updated:  1. We are utilizing three CONV layers rather than just two. 2. The ﬁrst and last CONV layers are 1× 1 convolutions. 3. The number of ﬁlters learned in the ﬁrst two CONV layers are 1 4 the number of ﬁlters learned  in the ﬁnal CONV.  To understand why we call this a “bottleneck”, consider the following ﬁgure where two residual  modules are stacked on top of each other, with one residual feeding into the next  Figure 12.2 .  Figure 12.2: An example of two stacked residual modules where one feeds into next. Both modules learn K = 32, 32, and 128 ﬁlters, respectively. Notice how the dimensionality is reduced during the ﬁrst two CONV layers then increased during the ﬁnal CONV layer.  The ﬁrst residual module accepts an input volume of size M × N × 64  the actual width and height are arbitrary for this example . The three CONV layers in the ﬁrst residual module learn K = 32, 32, and 128 ﬁlters, respectively. After applying the ﬁrst residual module our output volume size is M × N × 128 which is then fed into the second residual module.  In the second residual module, our number of ﬁlters learned by each of the three CONV layers stays the same at K = 32, 32, and 128, respectively. However, notice that 32 < 128, implying that we are actually reducing the volume size during the 1× 1 and 3× 3 CONV layers. This result has the beneﬁt of leaving the 3× 3 bottleneck layer with smaller input and output dimensions. The ﬁnal 1× 1 CONV then applies 4x the number of ﬁlters than the ﬁrst two CONV layers, thereby   174  Chapter 12. ResNet  increasing dimensionality once again, which is why we call this update to the residual module the “bottleneck” technique. When building our own residual modules, it’s common to supply pseudocode such as residual_module K=128  which implies that the ﬁnal CONV layer will learn 128 ﬁlters, while the ﬁrst two will learn 128 4 = 32 ﬁlters. This notation is often easier to work with as it’s understood that the bottleneck CONV layers will learn 1 4th the number of ﬁlters as the ﬁnal CONV layer.  When it comes to training ResNet, we typically use the bottleneck variant of the residual module rather than the original version, especially for ResNet implementations with > 50 layers.  12.1.2 Rethinking the Residual Module  In 2016, He et al. published a second paper on the residual module entitled Identity Mappings in Deep Residual Networks [33]. This publication described a comprehensive study, both theo- retically and empirically, on the ordering of convolutional, activation, and batch normalization layers within the residual module itself. Originally, the residual module  with bottleneck  looked like Figure 12.3  left .  Figure 12.3: Left: The original residual module with bottleneck. Right: Adapting the bottleneck module to use pre-activations.  The original residual module with bottleneck accepts an input  a ReLU activation map  and then applies a series of  CONV => BN => RELU  * 2 => CONV => BN before adding this output to the original input and applying a ﬁnal ReLU activation  which is then fed into the next residual module in the network . However, the He et al. 2016 study, it was found there was a more optimal layer ordering capable of obtaining higher accuracy – this method is called pre-activation.   12.2 Implementing ResNet  175  In the pre-activation version of the residual module, we remove the ReLU at the bottom of the module and re-order the batch normalization and activation such that they come before the convolution  Figure 12.3, right .  Now, instead of starting with a convolution, we apply a series of  BN => RELU => CONV  * 3  assuming the bottleneck is being used, of course . The output of the residual module is now the addition operation which is subsequently fed into the next residual module in the network  since residual modules are stacked on top of each other .  We call this layer ordering pre-activation as our ReLUs and batch normalization are placed before the convolutions, which is in contrast to the typical approach of applying ReLUs and batch normalizations after the convolutions. In our next section, we’ll implement ResNet from scratch using both bottlenecks and pre-activations.  12.2 Implementing ResNet  Now that we have reviewed the ResNet architecture, let’s go ahead and implement in Keras. For this speciﬁc implementation, we’ll be using the most recent incarnation of the residual module, including bottlenecks and pre-activations. To update your project structure, create a new ﬁle named resnet.py inside the nn.conv sub-module of pyimagesearch – that is where our ResNet implementation will live:  --- pyimagesearch                   --- __init__.py --- callbacks --- io --- nn            --- preprocessing --- utils  --- __init__.py --- conv           --- __init__.py --- alexnet.py --- deepergooglenet.py --- lenet.py --- minigooglenet.py --- minivggnet.py --- fcheadnet.py --- resnet.py --- shallownet.py  From there, open up resnet.py and insert the following code:  1  2  3  4  5  6  7  8  9  10  11   import the necessary packages from keras.layers.normalization import BatchNormalization from keras.layers.convolutional import Conv2D from keras.layers.convolutional import AveragePooling2D from keras.layers.convolutional import MaxPooling2D from keras.layers.convolutional import ZeroPadding2D from keras.layers.core import Activation from keras.layers.core import Dense from keras.layers import Flatten from keras.layers import Input from keras.models import Model   176  Chapter 12. ResNet  12  13  14  from keras.layers import add from keras.regularizers import l2 from keras import backend as K  We start off by importing our fairly standard set of classes and functions when building Convolutional Neural Networks. However, I would like to draw your attention to Line 12 where we import the add function. Inside the residual module, we’ll need to add together the outputs of two branches, which will be accomplished via this add method. We’ll also import the l2 function on Line 13 so that we can perform L2 weight decay. Regularization is extremely important when training ResNet since, due to the network’s depth, it is prone to overﬁtting.  Next, let’s move on to our residual_module:  class ResNet:  @staticmethod def residual_module data, K, stride, chanDim, red=False,  reg=0.0001, bnEps=2e-5, bnMom=0.9 :  This speciﬁc implementation of ResNet was inspired by both He et al. in their Caffe distribu- tion [46] as well as the mxnet implementation from Wei Wu [47], therefore we will follow their parameter choices as closely as possible. Looking at the residual_module we can see that the function accepts more parameters than any of our previous functions – let’s review each of them in detail.  The data parameter is simply the input to the residual module. The value K deﬁnes the number of ﬁlters that will be learned by the ﬁnal CONV in the bottleneck. The ﬁrst two CONV layers will learn K   4 ﬁlters, as per the He et al. paper. The stride controls the stride of the convolution. We’ll use this parameter to help us reduce the spatial dimensions of our volume without resorting to max pooling.  We then have the chanDim parameter which deﬁnes the axis which will perform batch normal- ization – this value is speciﬁed later in the build function based on whether we are using “channels last” or “channels ﬁrst” ordering.  Not all residual modules will be responsible for reducing the dimensions of our spatial volume – the red  i.e., “reduce”  boolean will control whether we are reducing spatial dimensions  True  or not  False .  We can then supply a regularization strength to all CONV layers in the residual module via reg. The bnEps parameter controls the ε responsible for avoiding “division by zero” errors when normalizing inputs. In Keras, ε defaults to 0.001; however, for our particular implementation, we’ll allow this value to be reduced signiﬁcantly. The bnMom controls the momentum for the moving average. This value normally defaults to 0.99 inside Keras, but He et al. as well as Wei Wu recommend decreasing the value to 0.9.  Now that the parameters of residual_module are deﬁned, let’s move on to the body of the  function:   the shortcut branch of the ResNet module should be  initialize as the input  identity  data shortcut = data   the first block of the ResNet module are the 1x1 CONVs bn1 = BatchNormalization axis=chanDim, epsilon=bnEps,  momentum=bnMom  data   act1 = Activation "relu"  bn1   16  17  18  19  20  21  22  23  24  25  26  27   28  29  31  32  33  34  35  36  37  39  40  41  42  43  44  45  47  48  49  50  51  12.2 Implementing ResNet  177  conv1 = Conv2D int K * 0.25 ,  1, 1 , use_bias=False,  kernel_regularizer=l2 reg   act1   On Line 22 we initialize the shortcut in the residual module, which is simply a reference to the input data. We will later add the shortcut to the output of our bottleneck + pre-activation branch. The ﬁrst pre-activation of the bottleneck branch can be seen in Lines 25-29. Here we apply a batch normalization layer, followed by ReLU activation, and then a 1× 1 convolution, using K 4 total ﬁlters. You’ll also notice that we are excluding the bias term from our CONV layers via use_bias=False. Why might we wish to purposely leave out the bias term? According to He et al., the biases are in the BN layers that immediately follow the convolutions [48], so there is no need to introduce a second bias term. of K 4, 3× 3 ﬁlters:  Next, we have our second CONV layer in the bottleneck, this one responsible for learning a total   the second block of the ResNet module are the 3x3 CONVs bn2 = BatchNormalization axis=chanDim, epsilon=bnEps,  momentum=bnMom  conv1   act2 = Activation "relu"  bn2  conv2 = Conv2D int K * 0.25 ,  3, 3 , strides=stride,  padding="same", use_bias=False, kernel_regularizer=l2 reg   act2   The ﬁnal block in the bottleneck learns K ﬁlters, each of which are 1× 1:   the third block of the ResNet module is another set of 1x1  CONVs bn3 = BatchNormalization axis=chanDim, epsilon=bnEps,  momentum=bnMom  conv2   act3 = Activation "relu"  bn3  conv3 = Conv2D K,  1, 1 , use_bias=False,  kernel_regularizer=l2 reg   act3   For more details on why we call this a “bottleneck” with “pre-activation”, please see Section  The next step is to see if we need to reduce spatial dimensions, thereby alleviating the need to  12.1 above.  apply max pooling:   if we are to reduce the spatial size, apply a CONV layer to  the shortcut if red:  shortcut = Conv2D K,  1, 1 , strides=stride,  use_bias=False, kernel_regularizer=l2 reg   act1   If we are instructed to reduce spatial dimensions, we’ll do so with a convolutional layer  applied  to the shortcut  with a stride > 1.  The output of the ﬁnal conv3 in the bottleneck is the added together with the shortcut, thus  serving as the output of the residual_module:   178  Chapter 12. ResNet   add together the shortcut and the final CONV x = add [conv3, shortcut]    return the addition as the output of the ResNet module return x  The residual_module will serve as our building block when creating deep residual networks.  Let’s move on to using this building block inside the build method:  @staticmethod def build width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset="cifar" :  Just as our residual_module requires more parameters than previous micro-architecture implementations, the same is true for our build function. The width, height, and depth classes all control the input spatial dimensions of the images in our dataset. The classes variable dictates how many overall classes our network should learn – these variables you have already seen.  What is interesting are the stages and filters parameters, both of which are lists. When constructing the ResNet architecture, we’ll be stacking a number of residual modules on top of each other  using the same number of ﬁlters for each stack , followed by reducing the spatial dimensions of the volume – this process is then continued until we are ready to apply our average pooling and softmax classiﬁer.  To make this point clear, let’s suppose that stages= 3, 4, 6  and filters= 64, 128, 256, 512 . The ﬁrst ﬁlter value, 64, will be applied to the only CONV layer not part of the residual module  i.e., ﬁrst convolutional layer in the network . We’ll then stack three residual modules on top of each other – each of these residual modules will learn K = 128 ﬁlters. The spatial dimensions of the volume will be reduced, and then we’ll move on to the second entry in stages where we’ll stack four residual modules on top of each other, each responsible for learning K = 256 ﬁlters. After these four residual modules, we’ll again reduce dimensionality and move on to the ﬁnal entry in the stages list, instructing us to stack six residual modules on top of each other, where each residual module will learn K = 512.  The beneﬁt of specifying both stages and filters in a list  rather than hardcoding them  is that we can easily leverage for loops to build the very deep network architectures without introducing code bloat – this point will become more clear later in our implementation.  Finally, we have the dataset parameter which is assumed to be a string. Depending on the dataset we are building ResNet for, we may want to apply more less convolutions and batch normalizations before we start stacking our residual modules. We’ll see why we might want to vary the number of convolutional layers in Section 12.5 below, but for the time being, you can safely ignore this parameter.  Next, let’s initialize our inputShape and chanDim based on whether we are using “channels  last”  Lines 64 and 65  or “channels ﬁrst”  Lines 69-71  ordering.   initialize the input shape to be "channels last" and the  channels dimension itself inputShape =  height, width, depth  chanDim = -1   if we are using "channels first", update the input shape  and channels dimension  53  54  55  56  57  59  60  61  62  63  64  65  66  67  68   69  70  71  73  74  75  76  77  78  79  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  12.2 Implementing ResNet  179  if K.image_data_format   == "channels_first":  inputShape =  depth, height, width  chanDim = 1  We are now ready to deﬁne the Input to our ResNet implementation:   set the input and apply BN inputs = Input shape=inputShape  x = BatchNormalization axis=chanDim, epsilon=bnEps,  momentum=bnMom  inputs    check if we are utilizing the CIFAR dataset if dataset == "cifar":   apply a single CONV layer x = Conv2D filters[0],  3, 3 , use_bias=False,  padding="same", kernel_regularizer=l2 reg   x   Unlike previous network architectures we have seen in this book  where the ﬁrst layer is typically a CONV , we see that ResNet uses a BN as the ﬁrst layer. The reasoning behind applying batch normalization to your input is an added level of normalization. In fact, performing batch normalization on the input itself can sometimes remove the need to apply mean normalization to the inputs. In either case, the BN on Lines 75 and 76 acts as an added level of normalization. From there, we apply a single CONV layer on Lines 81 and 82. This CONV layer will learn a total of filters[0], 3× 3 ﬁlters  keep in mind that filters is a list, so this value is speciﬁed via the build method when constructing the architecture .  You’ll also notice that I’ve made a check to see if we are using the CIFAR-10 dataset  Line 79 . Later in this chapter, we’ll be updating this if block to include an elif statement for Tiny ImageNet. Since the input dimensions to Tiny ImageNet are larger, we’ll apply a series of convolutions, batch normalizations, and max pooling  the only max pooling in the ResNet architecture  before we start stacking residual modules. However, for the time being, we are only using the CIFAR-10 dataset. Let’s go ahead and start stacking residual layers on top of each other, the cornerstone of the  ResNet architecture:   loop over the number of stages for i in range 0, len stages  :   initialize the stride, then apply a residual module  used to reduce the spatial size of the input volume stride =  1, 1  if i == 0 else  2, 2  x = ResNet.residual_module x, filters[i + 1], stride,  chanDim, red=True, bnEps=bnEps, bnMom=bnMom    loop over the number of layers in the stage for j in range 0, stages[i] - 1 :   apply a ResNet module x = ResNet.residual_module x, filters[i + 1],   1, 1 , chanDim, bnEps=bnEps, bnMom=bnMom   On Line 85 we start looping over the list of stages. Keep in mind that every entry in the stages list is an integer, indicating how many residual modules will be stacked on top of each other. Following the work of Springenberg et al., ResNet tries to reduce the usage of pooling as much as possible, relying on CONV layers to reduce the spatial dimensions of a volume.   180  Chapter 12. ResNet  To reduce volume size without pooling layers, we must set the stride of the convolution on Line 88. If this is the ﬁrst entry in the stage, we’ll set the stride to  1, 1 , indicating that no downsampling should be performed. However, for every subsequent stage we’ll apply a residual module with a stride of  2, 2 , which will allow us to decrease the volume size.  From there, we’ll loop over the number of layers in the current stage on Line 93  i.e., the number of residual modules that will be stacked on top of each other . The number of ﬁlters each residual module will learn is controlled by the corresponding entry in the filters list. The reason we use i + 1 as the index into filters is because the ﬁrst ﬁlter value was used on Lines 81 and 82. The rest of the ﬁlter values correspond to the number of ﬁlters in each stage. Once we have stacked stages[i] residual modules on top of each other, our for loop brings us back up to Lines 88-90 where we decrease the spatial dimensions of the volume and repeat the process.  At this point, our volume size has been reduced to 8 x 8 x classes  you can verify this for yourself by computing the input output volume sizes for each layer, or better yet, simply using the plot_model function from Chapter 19 of the Starter Bundle .  In order to avoid using dense fully-connected layers, we’ll instead apply average pooling to  reduce the volume size to 1 x 1 x classes:   apply BN => ACT => POOL x = BatchNormalization axis=chanDim, epsilon=bnEps,  momentum=bnMom  x   x = Activation "relu"  x  x = AveragePooling2D  8, 8   x   From there, we create a dense layer for the total number of classes we are going to learn,  followed by applying a softmax activation to obtain our ﬁnal output probabilities:   softmax classifier x = Flatten   x  x = Dense classes, kernel_regularizer=l2 reg   x  x = Activation "softmax"  x    create the model model = Model inputs, x, name="resnet"    return the constructed network architecture return model  98  99  100  101  102  104  105  106  107  108  109  110  111  112  113  The fully constructed ResNet model is then returned to the calling function on Line 113.  12.3 ResNet on CIFAR-10  Outside of training smaller variants on ResNet on the full ImageNet dataset, I had never attempted to train ResNet on CIFAR-10  or Stanford’s Tiny ImageNet challenge, as we’ll see in this section . Because of this fact, I have decided to treat this section and the next as candid case studies where I reveal my personal rules of thumb and best practices I have developed over years of training neural networks.  These best practices allow me to approach a new problem with an initial plan, iterate on it, and eventually arrive at a solution that obtains good accuracy. In the case of CIFAR-10, we’ll be able to replicate the performance of He et al. and claim a spot amongst other state-of-the-art approaches [49].   12.3 ResNet on CIFAR-10  181  12.3.1 Training ResNet on CIFAR-10 With the ctrl + c Method  Whenever I start a new set of experiments with either a network architecture I am unfamiliar with, a dataset I have never worked with, or both, I always begin with the ctrl + c method of training. Using this method, I can start training with an initial learning rate  and associated set of hyperparameters , monitor training, and quickly adjust the learning rate based the results as they come in. This method is especially helpful when I am totally unsure on the approximate number of epochs it will take for a given architecture to obtain reasonable accuracy or a speciﬁc dataset.  In the case of CIFAR-10, I have previous experience  as do you, after reading all the other chapters in this book , so I’m quite conﬁdent that it will take 60-100 epochs, but I’m not exactly sure since I’ve never trained ResNet on the CIFAR-10 before.  Therefore, our ﬁrst few experiments will rely on the ctrl + c method of training to narrow in on what hyperparameters we should be using. Once we are comfortable with our set of hyperpa- rameters, we’ll switch over to a speciﬁc learning rate decay schedule in hopes of milking every last bit of accuracy out of the training process.  To get started, open up a new ﬁle, name it resnet_cifar10.py, and insert the following code:   set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use "Agg"   1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20   import the necessary packages from sklearn.preprocessing import LabelBinarizer from pyimagesearch.nn.conv import ResNet from pyimagesearch.callbacks import EpochCheckpoint from pyimagesearch.callbacks import TrainingMonitor from keras.preprocessing.image import ImageDataGenerator from keras.optimizers import SGD from keras.datasets import cifar10 from keras.models import load_model import keras.backend as K import numpy as np import argparse import sys   set a high recursion limit so Theano doesn’t complain sys.setrecursionlimit 5000   We start by importing our required Python packages on Lines 6-17. Since we’ll be using the ctrl + c method to training, we’ll make sure to import the EpochCheckpoint class  Line 8  to serialize ResNet weights to disk during the training process, allowing us to stop and restart training from a speciﬁc checkpoint. Since this is our ﬁrst experiment with ResNet, we’ll be using the SGD optimizer  Line 11  – time will tell if we decide to switch and use a different optimizer  we’ll let our results dictate that .  On Line 20 I update the recursion limit for the Python programming language. I wrote this book with both TensorFlow and Theano in mind, so if you are using TensorFlow, you don’t have to worry about this line. However, if you are using Theano, you may encounter an error when instantiating the ResNet architecture that a maximum recursion level has been reached. This is a known “bug” with Theano and can be resolved simply by increasing the recursion limit of the Python programming language [50].  Next, let’s parse our command line arguments:   182  Chapter 12. ResNet   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-c", "--checkpoints", required=True,  help="path to output checkpoint directory"   ap.add_argument "-m", "--model", type=str,  help="path to *specific* model checkpoint to load"   ap.add_argument "-s", "--start-epoch", type=int, default=0,  help="epoch to restart training at"   args = vars ap.parse_args     Our script will require only the --checkpoints switch, the path to the directory where we will store the ResNet weights every N epochs. In the case that we need to restart training from a particular epoch, we can supply the --model path along with an integer indicating the speciﬁc epoch number.  The next step is to load the CIFAR-10 dataset from disk  pre-split into training and testing ,  perform mean subtraction, and one-hot encode the integer labels as vectors:   load the training and testing data, converting the images from  integers to floats print "[INFO] loading CIFAR-10 data..."    trainX, trainY ,  testX, testY   = cifar10.load_data   trainX = trainX.astype "float"  testX = testX.astype "float"    apply mean subtraction to the data mean = np.mean trainX, axis=0  trainX -= mean testX -= mean   convert the labels from integers to vectors lb = LabelBinarizer   trainY = lb.fit_transform trainY  testY = lb.transform testY   While we’re at it, let’s also initialize an ImageDataGenerator so we can apply data augmen-  tation to CIFAR-10:   construct the image generator for data augmentation aug = ImageDataGenerator width_shift_range=0.1,  height_shift_range=0.1, horizontal_flip=True, fill_mode="nearest"   In the case we are training ResNet from the very ﬁrst epoch, we need to instantiate the network  architecture:   if there is no specific model checkpoint supplied, then initialize  the network  ResNet-56  and compile the model if args["model"] is None:  print "[INFO] compiling model..."  opt = SGD lr=1e-1  model = ResNet.build 32, 32, 3, 10,  9, 9, 9 ,  22  23  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  49  50  51  52  54  55  56  57  58  59   12.3 ResNet on CIFAR-10  183   64, 64, 128, 256 , reg=0.0005   model.compile loss="categorical_crossentropy", optimizer=opt,  metrics=["accuracy"]   To start, take a look at the learning rate for our SGD optimizer on Line 58 – at 1e− 1 this learning rate is by far the largest we have used in this book  by an order of magnitude . The reason we are able to get away with such a high learning rate is due to the identity mappings built into the residual module. Learning rates this high would not  typically  work for networks such as AlexNet, VGG, etc.  We then instantiate our ResNet model on Lines 59 and 60. Here we can see that the network will accept input images with a width of 32 pixels, height of 32 pixels, and depth of 3  one for each of the RGB channels in the CIFAR-10 dataset . Since the CIFAR-10 dataset has ten classes, we’ll learn ten output labels.  The next parameter we need to supply is  9, 9, 9 , or the number of stages in our architecture. This tuple indicates that we will be learning three stages with each stage containing nine residual modules stacked on top of each other. In between each stage, we will apply an additional residual module to decrease the volume size.  The next parameter,  64, 64, 128, 256  is the number of ﬁlters that the CONV layers will learn. The ﬁrst CONV layer  before any residual model is applied  will learn K = 64 ﬁlters. The remaining entries, 64, 128, and 256 correspond to the number of ﬁlters each of the residual module stages will learn. For example, the ﬁrst nine residual modules will learn K = 64 ﬁlters. The second set of nine residual modules will learn K = 128 ﬁlters. And ﬁnally, the last set of nine residual modules will learn K = 256 ﬁlters. The last argument we’ll supply to ResNet is reg, or our L2 regularization strength for weight decay – this value is crucial as it will enable us to prevent overﬁtting.  In the case we start restarting training from a speciﬁc epoch, we need to load the network  weights from disk and update the learning rate:   otherwise, load the checkpoint from disk else:  print "[INFO] loading {}...".format args["model"]   model = load_model args["model"]    update the learning rate print "[INFO] old learning rate: {}".format   K.get_value model.optimizer.lr    K.set_value model.optimizer.lr, 1e-5  print "[INFO] new learning rate: {}".format   K.get_value model.optimizer.lr      construct the set of callbacks callbacks = [  EpochCheckpoint args["checkpoints"], every=5,  startAt=args["start_epoch"] ,  TrainingMonitor "output resnet56_cifar10.png", jsonPath="output resnet56_cifar10.json", startAt=args["start_epoch"] ]  Let’s also construct a set of callbacks so we can both  1  checkpoint ResNet weights every  ﬁve epochs and  2  monitor training:  60  61  62  64  65  66  67  68  69  70  71  72  73  74  76  77  78  79  80  81  82   Chapter 12. ResNet  Finally, we’ll train our network in batch sizes of 128:   train the network print "[INFO] training network..."  model.fit_generator   aug.flow trainX, trainY, batch_size=128 , validation_data= testX, testY , steps_per_epoch=len trainX     128, epochs=10, callbacks=callbacks, verbose=1   184  84  85  86  87  88  89  90  with it.  Now that our resnet_cifar10.py script is coded up, let’s move on to running experiments  ResNet on CIFAR-10: Experiment 1  Figure 12.4: Top-left: First 50 epochs when training ResNet on CIFAR-10 in Experiment 1. Top-right: Next 25 epochs. Bottom: Final 10 epochs.   12.3 ResNet on CIFAR-10  185  In my very ﬁrst experiment with CIFAR-10, I was worried about the number of ﬁlters in the network, especially regarding overﬁtting. Because of this concern, my initial ﬁlter list consisted of  16, 16, 32, 64  along with  9, 9, 9  stages of residual modules. I also applied a very small amount of L2 regularization with reg=0.0001 – I knew regularization would be needed, but I wasn’t sure on the correct amount  yet . ResNet was trained using SGD with a base learning rate of 1e− 1 and a momentum term of 0.9.  I started training using the following command:  $ python resnet_cifar10.py --checkpoints output checkpoints  Past epoch 50 I noticed training loss starting to slow as well as some volatility in the validation loss  and a growing gap between the two   Figure 12.4, top-left . I stopped training, lowered the learning rate to 1e− 2, and then continued training:  $ python resnet_cifar10.py --checkpoints output checkpoints \ --model output checkpoints epoch_50.hdf5 --start-epoch 50  The drop in learning rate proved very effective, stabilizing validation loss, but also overﬁtting on the training set start to creep in  in inevitability when working with CIFAR-10  around epoch 75  Figure 12.4, top-right . After epoch 75 I once again stopped training, lowered the learning rate to 1e− 3, and allowed ResNet to continue training for another 10 epochs:  $ python resnet_cifar10.py --checkpoints output checkpoints \ --model output checkpoints epoch_75.hdf5 --start-epoch 75  The ﬁnal plot is shown in Figure 12.4  bottom , where we reach 89.06% accuracy on the validation set. For our very ﬁrst experiment 89.06% is a good start; however, it’s not as high as the 90.81% achieved by GoogLeNet in Chapter 11. Furthermore, He et al. reported an accuracy of 93% with ResNet on CIFAR-10, so we clearly have some work to do.  Our previous experiment achieved a reasonable accuracy of 89.06% accuracy – but we need higher accuracy. Instead of increasing the depth of the network  by adding more stages , I decided to add more ﬁlters to each of the CONV layers. Thus, my filters list was updated to be  16, 64, 128, 256 .  Notice how the number of ﬁlters in all residual modules have doubled from the previous experiment  the number of ﬁlters in the ﬁrst CONV layer was left the same . SGD was once again used to train the network with a momentum term of 0.9. I also kept the regularization term at 0.0001.  In Figure 12.5  top-left  you can ﬁnd a plot of my ﬁrst 40 epochs: We can clearly see a gap between training loss and validation loss, but overall, validation accuracy is still keeping up with training accuracy. In an effort to improve the accuracy, I decided to lower the learning rate from 1e− 1 to 1e− 2 and train for another ﬁve epochs – the result was that learning stagnated entirely  top-right . Lowering learning rate again from 1e− 2 to 1e− 3 even caused overﬁtting through a slight rise in validation loss  bottom   Interestingly, the loss stagnated for both the training set and the validation set, not unlike previous experiments we’ve run with GoogLeNet. After the initial drop in learning rate, it appears that our network could not learn any more underlying patterns in the dataset. All that said, after the 50th epoch validation accuracy had increased to 90.10%, an improvement from our ﬁrst experiment.  12.3.2 ResNet on CIFAR-10: Experiment 2   186  Chapter 12. ResNet  Figure 12.5: Top-left: First 40 epochs when training ResNet on CIFAR-10 in Experiment 2. Top-right: Next 5 epochs. Bottom: Final 5 epochs.  ResNet on CIFAR-10: Experiment 3 At this point, I was starting to become more comfortable training ResNet on CIFAR-10. Clearly the increase of ﬁlters helped, but the stagnation in learning after the ﬁrst learning rate drop was still troubling. I was conﬁdent that a slow, linear decrease in learning rate would help combat this problem, but I wasn’t convinced that I had obtained a good set of hyperparameters to warrant switching over to learning rate decay.  Instead, I decided to increase the number of ﬁlters learned in the ﬁrst CONV layer to 64  up from 16 , turning the filters list into  64, 64, 128, 256 . The increase in ﬁlters helped in the second experiment, and there is no reason the ﬁrst CONV layer should miss out on these beneﬁts as well. The SGD optimizer was left alone with an initial learning rate of 1e− 1 and momentum of 0.9.  Furthermore, I also decided to dramatically increase regularization from 0.0001 to 0.0005. I had a suspicion that allowing the network to train for longer would result in higher validation accuracy – using a larger regularization term would likely enable me to train for longer.  I was also considering lowering my learning rate, but given that the network was making   12.3 ResNet on CIFAR-10  187  Figure 12.6: Top-left: First 80 epochs when training ResNet on CIFAR-10 in Experiment 3. Top-right: Next 10 epochs. Bottom: Final 10 epochs.  traction without a problem at 1e− 1, it hardly seemed worth it to lower to 1e− 2. Doing so might have stabilized training  i.e., less ﬂuctuation in validation loss accuracy , but would have ultimately led to lower accuracy after training completed. If my larger learning rate + larger regularization term suspicion turned out to be correct, then it would make sense to switch over to a learning rate decay to avoid stagnation after order of magnitude drops. But before I could make this switch, I ﬁrst needed to prove my hunch.  As my plot of the ﬁrst 80 epochs demonstrates  Figure 12.6, top-left , there certainly is overﬁtting as validation quickly diverges from training. However, what’s interesting here is that while overﬁtting is undoubtedly occurring  as training loss drops much faster than validation loss , we are able to train the network for longer without validation loss starting to increase.  After the 80th epoch, I stopped training, lowered the learning rate to 1e− 2, then trained for another 10 epochs  Figure 12.6, top-right . We see an initial drop and loss and increase in accuracy, but from there validation loss accuracy plateaus. Furthermore, we can start to see the validation loss increase, a sure sign of overﬁtting.   188  Chapter 12. ResNet  To validate that overﬁtting was indeed happening, I stopped training at epoch 90, lowered the learning rate to 1e-3, then trained for another 10 epochs  Figure 12.6, bottom . Sure enough, this is the telltale sign of overﬁtting: validation loss increasing while training loss decreases remains constant.  However, what’s very interesting is that after the 100th epoch we obtained 91.83% validation accuracy, higher than our third experiment. The downside is that we are overﬁt – we need a way to maintain this level of accuracy  and increase it  without overﬁtting. To do so, I decided to switch from ctrl + c training to learning rate decay.  12.4 Training ResNet on CIFAR-10 with Learning Rate Decay  At this point, it seems that we have gotten as far as we can using standard ctrl + c training. We’ve also been able to see that our most successful experiments occur when we can train for longer, in the range of 80-100 epochs. However, there are two major problems we need to overcome:  1. Whenever we drop the learning rate by an order of magnitude and restart training, we obtain  a nice bump in accuracy, but then we quickly plateau.  2. We are overﬁtting. To solve these problems, and boost accuracy further, a good experiment to try is linearly decreasing the learning rate over a large number of epochs, typically about the same as your longest ctrl + c experiments  if not slightly longer . To start this, let’s open up a new ﬁle, name it resnet_cifar10_decay.py, and insert the following code:   set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use "Agg"   1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19   import the necessary packages from sklearn.preprocessing import LabelBinarizer from pyimagesearch.nn.conv import ResNet from pyimagesearch.callbacks import TrainingMonitor from keras.preprocessing.image import ImageDataGenerator from keras.callbacks import LearningRateScheduler from keras.optimizers import SGD from keras.datasets import cifar10 import numpy as np import argparse import sys import os   set a high recursion limit so Theano doesn’t complain sys.setrecursionlimit 5000   Line 2 conﬁgures matplotlib so we can save plots in the background. We then import the remainder of our Python packages on Lines 6-16. Take a look at Line 10 where we import our LearningRateScheduler so we can deﬁne a custom learning rate decay for the training process. We then set a high system recursion limit in order to avoid any issues with the Theano backend  just in case you are using it .  The next step is to deﬁne the learning rate decay schedule:  21  22   define the total number of epochs to train for along with the  initial learning rate   12.4 Training ResNet on CIFAR-10 with Learning Rate Decay  189  NUM_EPOCHS = 100 INIT_LR = 1e-1  def poly_decay epoch :   initialize the maximum number of epochs, base learning rate,  and power of the polynomial maxEpochs = NUM_EPOCHS baseLR = INIT_LR power = 1.0   compute the new learning rate based on polynomial decay alpha = baseLR *  1 -  epoch   float maxEpochs    ** power   return the new learning rate return alpha  We’ll train our network for a total of 100 epochs with a base learning rate of 1e− 1. The poly_decay function will decay our 1e− 1 learning rate linearly over the course of 100 epochs. This is a linear decay due to the fact that we set power=1. For more information on learning rate schedules, see Chapter 16 of the Starter Bundle along with Chapter 11 of the Practitioner Bundle.  We then need to supply two command line arguments:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-m", "--model", required=True,  help="path to output model"   ap.add_argument "-o", "--output", required=True,  help="path to output directory  logs, plots, etc. "   args = vars ap.parse_args     The --model switch controls the path to our ﬁnal serialized model after training, while  --output is the base directory to where we will store any logs, plots, etc.  We can now load the CIFAR-10 dataset and mean normalize it:   load the training and testing data, converting the images from  integers to floats print "[INFO] loading CIFAR-10 data..."    trainX, trainY ,  testX, testY   = cifar10.load_data   trainX = trainX.astype "float"  testX = testX.astype "float"    apply mean subtraction to the data mean = np.mean trainX, axis=0  trainX -= mean testX -= mean  Encode the integer labels as vectors:   convert the labels from integers to vectors lb = LabelBinarizer   trainY = lb.fit_transform trainY  testY = lb.transform testY   23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  39  40  41  42  43  44  45  47  48  49  50  51  52  53  54  55  56  57  59  60  61  62   190  Chapter 12. ResNet  As well as initialize our ImageDataGenerator for data argumentation:   construct the image generator for data augmentation aug = ImageDataGenerator width_shift_range=0.1,  height_shift_range=0.1, horizontal_flip=True, fill_mode="nearest"   Our callbacks list will consist of both a TrainingMonitor along with a LearningRateScheduler  with the poly_decay function supplied as the only argument – this class will allow us to decay our learning rate as we train.   construct the set of callbacks figPath = os.path.sep.join [args["output"], "{}.png".format   jsonPath = os.path.sep.join [args["output"], "{}.json".format   os.getpid   ]   os.getpid   ]   callbacks = [TrainingMonitor figPath, jsonPath=jsonPath ,  LearningRateScheduler poly_decay ]  We’ll then instantiate ResNet with the best parameters we found from Section 12.6  three stacks of nine residual modules: 64 ﬁlters in the ﬁrst CONV layer before the residual modules, and 64, 128, and 256 ﬁlters for each stack of respective residual modules :   initialize the optimizer and model  ResNet-56  print "[INFO] compiling model..."  opt = SGD lr=INIT_LR, momentum=0.9  model = ResNet.build 32, 32, 3, 10,  9, 9, 9 ,   64, 64, 128, 256 , reg=0.0005   model.compile loss="categorical_crossentropy", optimizer=opt,  metrics=["accuracy"]   We’ll then train our network using learning rate decay:   train the network print "[INFO] training network..."  model.fit_generator   aug.flow trainX, trainY, batch_size=128 , validation_data= testX, testY , steps_per_epoch=len trainX     128, epochs=10, callbacks=callbacks, verbose=1    save the network to disk print "[INFO] serializing network..."  model.save args["model"]   The big question is – will our learning rate decay pay off? To ﬁnd out, proceed to the next  section.  64  65  66  67  69  70  71  72  73  74  75  77  78  79  80  81  82  83  85  86  87  88  89  90  91  92  93  94  95   12.4 Training ResNet on CIFAR-10 with Learning Rate Decay  191  ResNet on CIFAR-10: Experiment 4 As the code in the previous section indicates, we are going to use the SGD optimizer with a base learning rate of 1e− 1 and a momentum term of 0.9. We’ll train ResNet for a total of 100 epochs, linearly decreasing the linear rate from 1e− 1 down to zero. To train ResNet on CIFAR-10 with learning rate decay, I executed the following command:  $ python resnet_cifar10_decay.py --output output \  --model output resnet_cifar10.hdf5  Figure 12.7: Training ResNet on CIFAR-10 using learning rate decay beats out all previous experiments.  After training was complete, I took a look at the plot  Figure 12.7 . As in previous experiments, training and validation loss start to diverge early on, but more importantly, the gap remains approximately constant after the initial divergence. This result is important as it indicates that our overﬁtting is controlled. We have to accept that we will overﬁt when training on CIFAR-10, but we need to control this overﬁtting. By applying learning rate decay, we were able to successfully do so. The question is, did we obtain higher classification accuracy? To answer that,  take a look at the output of the last few epochs:  ... Epoch 98 100 247s - loss: 0.1563 - acc: 0.9985 - val_loss: 0.3987 - val_acc: 0.9351 Epoch 99 100 245s - loss: 0.1548 - acc: 0.9987 - val_loss: 0.3973 - val_acc: 0.9358 Epoch 100 100   192  Chapter 12. ResNet  244s - loss: 0.1538 - acc: 0.9990 - val_loss: 0.3978 - val_acc: 0.9358 [INFO] serializing network...  After the 100th epoch, ResNet is reaching 93.58% accuracy on our testing set. This result is substantially higher than our previous two experiments, and more importantly, it has allowed us to replicate the results from He et al. when training ResNet on CIFAR-10.  Taking a look at the CIFAR-10 leaderboard [49], we see that He et al. reached 93.57% accuracy [24], near identical to our result  Figure 12.8 . The red arrow indicates our accuracy, safely landing us in the top-10 leaderboard.  Figure 12.8: We have successfully replicated the work of He et al. when applying ResNet to CIFAR-10 down to 0.01%.  12.5 ResNet on Tiny ImageNet  In this section, we will train the ResNet architecture  with bottleneck and pre-activation  on Stanford’s cs231n Tiny ImageNet challenge. Similar to the ResNet + CIFAR-10 experiments earlier in this chapter, I have never trained ResNet on Tiny ImageNet before, so I’m going to apply my same exact experiment process:  1. Start with ctrl + c-based training to obtain a baseline. 2. If stagnation plateauing occurs after order of magnitude learning rate drops, then switch over  to learning rate decay.  Given that we’ve already applied a similar technique to CIFAR-10, we should be able to save ourselves some time noticing signs of overﬁtting and plateauing earlier. That said, let’s get started by reviewing directory structure for this project, which is near identical to the GoogLeNet and Tiny ImageNet challenge from the previous chapter:  --- resnet_tinyimagenet.py    --- config   --- __init__.py   12.5 ResNet on Tiny ImageNet  193  --- tiny_imagenet_config.py           --- rank_accuracy.py --- train.py --- train_decay.py --- output     --- checkpoints  --- tiny-image-net-200-mean.json  Here you can see we have created config Python module where we have stored a ﬁle named  tiny_imagenet_config.py.  This ﬁle was copied directly from the GoogLeNet chapter. I then updated Lines 31-39 to point to the output ResNet MODEL_PATH, FIG_PATH  learning plot , and JSON_PATH  serialized log of training history :   define the path to the output directory used for storing plots,  classification reports, etc. OUTPUT_PATH = "output" MODEL_PATH = path.sep.join [OUTPUT_PATH,  "resnet_tinyimagenet.hdf5"]   FIG_PATH = path.sep.join [OUTPUT_PATH,  "resnet56_tinyimagenet.png"]   JSON_PATH = path.sep.join [OUTPUT_PATH,  "resnet56_tinyimagenet.json"]   From there we have train.py which will be responsible for training ResNet using the standard ctrl + c method. In the case that we wish to apply learning rate decay, we’ll be able to use train_decay.py. Finally, rank_accuracy.py will be used to compute the rank-1 and rank-5 accuracy of ResNet on Tiny ImageNet.  12.5.1 Updating the ResNet Architecture  Earlier in this chapter we reviewed our implementation of the ResNet architecture in detail. Speciﬁcally, we noted the dataset parameter supplied to the build method, like so:  @staticmethod def build width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset="cifar" :  This value defaulted to cifar; however, since we are now working with Tiny ImageNet, we need to update our ResNet implementation to include an if elif block. Go ahead and open up your resnet.py ﬁle in the nn.conv sub-module of PyImageSearch and insert the following code:   check if we are utilizing the CIFAR dataset if dataset == "cifar":   apply a single CONV layer x = Conv2D filters[0],  3, 3 , use_bias=False,  padding="same", kernel_regularizer=l2 reg   x    check to see if we are using the Tiny ImageNet dataset elif dataset == "tiny_imagenet":   apply CONV => BN => ACT => POOL to reduce spatial size  31  32  33  34  35  36  37  38  39  59  60  61  78  79  80  81  82  83  84  85  86   194  87  88  89  90  91  92  93  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  Chapter 12. ResNet  x = Conv2D filters[0],  5, 5 , use_bias=False,  padding="same", kernel_regularizer=l2 reg   x  x = BatchNormalization axis=chanDim, epsilon=bnEps,  momentum=bnMom  x   x = Activation "relu"  x  x = ZeroPadding2D  1, 1   x  x = MaxPooling2D  3, 3 , strides= 2, 2   x   Lines 79-82 we have already reviewed before – these lines are where we apply a single 3× 3 CONV layer for CIFAR-10. However, we are now updating the architecture to check for Tiny ImageNet  Line 85 . Provided we are instantiating ResNet for Tiny ImageNet, we need to add in some additional layers. To start, we apply 5× 5 CONV layer to learn larger feature maps  in the full ImageNet dataset implementation, we’ll actually be learning 7× 7 ﬁlters . Next, we apply a batch normalization followed a ReLU activation. Max pooling, the only max pooling layer in the ResNet architecture, is applied on Line 93 using a size of 3× 3 and stride of 2× 2. Combined with the previous zero padding layer  Line 92 , pooling ensures that our output spatial volume size is 32× 32, the exact same spatial dimensions as the input images from CIFAR-10. Validating that the output volume size is 32× 32 ensures we can easily reuse the rest of the ResNet implementation without having to make any additional changes.  12.5.2 Training ResNet on Tiny ImageNet With the ctrl + c Method  Now that our ResNet implementation has been updated, let’s code up a Python script responsible for the actual training process. Open up a new ﬁle, name it train.py, and insert the following code:   set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use "Agg"    import the necessary packages from config import tiny_imagenet_config as config from pyimagesearch.preprocessing import ImageToArrayPreprocessor from pyimagesearch.preprocessing import SimplePreprocessor from pyimagesearch.preprocessing import MeanPreprocessor from pyimagesearch.callbacks import EpochCheckpoint from pyimagesearch.callbacks import TrainingMonitor from pyimagesearch.io import HDF5DatasetGenerator from pyimagesearch.nn.conv import ResNet from keras.preprocessing.image import ImageDataGenerator from keras.optimizers import SGD from keras.models import load_model import keras.backend as K import argparse import json import sys   set a high recursion limit so Theano doesn’t complain sys.setrecursionlimit 5000   Lines 2 and 3 conﬁgure matplotlib so we can save our ﬁgures and plots to disk during the training process. We then import the remainder of our Python packages on Lines 6-20. We have   12.5 ResNet on Tiny ImageNet  195  seen all of these imports before from Chapter 11 on GoogLeNet + Tiny ImageNet, only now we are importing ResNet  Line 13  rather than GoogLeNet. We’ll also update the maximum recursion limit just in case you are using the Theano backend.  Next comes our command line arguments:   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-c", "--checkpoints", required=True,  help="path to output checkpoint directory"   ap.add_argument "-m", "--model", type=str,  help="path to *specific* model checkpoint to load"   ap.add_argument "-s", "--start-epoch", type=int, default=0,  help="epoch to restart training at"   args = vars ap.parse_args     These command line switches can be used to start training from scratch or restart training from a speciﬁc epoch. For a more detailed review of each command line argument, please see Section 12.3.1.  Let’s also initialize our ImageDataGenerator that will be applied to the training set for data  augmentation:   construct the training image generator for data augmentation aug = ImageDataGenerator rotation_range=18, zoom_range=0.15,  width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode="nearest"    load the RGB means for the training set means = json.loads open config.DATASET_MEAN .read     Line 41 then loads our RGB means computed over the training set of mean subtraction. Our image pre-processors are fairly standard here, consisting of ensuring the image is resized to 64 × 64 pixels, performing mean normalization, and then converting the image to a Keras- compatible array:   initialize the image preprocessors sp = SimplePreprocessor 64, 64  mp = MeanPreprocessor means["R"], means["G"], means["B"]  iap = ImageToArrayPreprocessor     initialize the training and validation dataset generators trainGen = HDF5DatasetGenerator config.TRAIN_HDF5, 64, aug=aug,  preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES   valGen = HDF5DatasetGenerator config.VAL_HDF5, 64,  preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES   25  26  27  28  29  30  31  32  33  35  36  37  38  39  40  41  43  44  45  46  47  48  49  50  51  52  Based on the image pre-processors and data augmentation, we can then construct an HDF5DatasetGenerator  for both the training and validation datasets  Lines 49-52 . Batches of 64 images will be polled at a time for these generators and passed through the network.  If we are training ResNet from the very ﬁrst epoch, we need to instantiate the model and  compile it:   196  Chapter 12. ResNet   if there is no specific model checkpoint supplied, then initialize  the network and compile the model if args["model"] is None:  print "[INFO] compiling model..."  model = ResNet.build 64, 64, 3, config.NUM_CLASSES,  3, 4, 6 ,  64, 128, 256, 512 , reg=0.0005, dataset="tiny_imagenet"   opt = SGD lr=1e-1, momentum=0.9  model.compile loss="categorical_crossentropy", optimizer=opt,  metrics=["accuracy"]   Lines 58 and 59 initialize the ResNet model itself. The model will accept input images with 64× 64× 3 spatial dimensions and will learn a total of NUM_CLASSES, which in the case of Tiny ImageNet is 200. My choice for stages= 3, 4, 6  was inspired by the Deep Residual Learning for Image Recognition [24] paper where a similar version of residual module layer stacking was used for the full ImageNet dataset.  Given that Tiny ImageNet will require more discriminative ﬁlters than CIFAR-10, I also updated the filters list to learn more ﬁlters for each of the CONV layers:  64, 128, 256, 512 . This list of ﬁlters implies that the ﬁrst CONV layer  before any of the residual modules  will learn a total of 64 5× 5 ﬁlters. From there, three residual modules will be stacked on top of each other, each responsible for learning K = 128 ﬁlters. Dimensionality is then reduced, then four residual modules are stacked, this time learning K = 256 ﬁlters. Once again the spatial dimensions of the volume are reduced, then six residual modules are stacked, each module learning K = 512 ﬁlters.  We’ll also apply a regularization strength of 0.0005 as regularization seemed to aide us in training ResNet on CIFAR-10. To train the network, SGD will be used with a base learning rate of 1e− 1 and a momentum term of 0.9.  If we have stopped training, updated any hyperparameters  such as the learning rate , and wish  to restart training, the following code block will handle that process for us:   otherwise, load the checkpoint from disk else:  print "[INFO] loading {}...".format args["model"]   model = load_model args["model"]    update the learning rate print "[INFO] old learning rate: {}".format   K.get_value model.optimizer.lr    K.set_value model.optimizer.lr, 1e-5  print "[INFO] new learning rate: {}".format   K.get_value model.optimizer.lr     Our callbacks list will consist of checkpointing ResNet weights to disk every ﬁve epochs,  followed by plotting the training history:   construct the set of callbacks callbacks = [  EpochCheckpoint args["checkpoints"], every=5,  startAt=args["start_epoch"] ,  TrainingMonitor config.FIG_PATH, jsonPath=config.JSON_PATH,  startAt=args["start_epoch"] ]  54  55  56  57  58  59  60  61  62  64  65  66  67  68  69  70  71  72  73  74  76  77  78  79  80  81   12.5 ResNet on Tiny ImageNet  197  Finally, we’ll kick off the training process and train our network using mini-batches of size 64:  83  84  85  86  87  88  89  90  91  92  93  94  95   train the network model.fit_generator   trainGen.generator  , steps_per_epoch=trainGen.numImages    64, validation_data=valGen.generator  , validation_steps=valGen.numImages    64, epochs=50, max_queue_size=64 * 2, callbacks=callbacks, verbose=1    close the databases trainGen.close   valGen.close    epochs to a large number and adjust as needed.  ResNet on Tiny ImageNet: Experiment 1 To start training, I executed the following command:  $ python train.py --checkpoints output checkpoints  The exact number of epochs we’ll need to train ResNet is unknown at this point, so we’ll set  After monitoring training for the ﬁrst 25 epochs, it became clear that training loss was starting to stagnate a bit  Figure 12.9, top-left . To combat this stagnation, I stopped training, reduced my learning rate from 1e− 1 to 1e− 2, and resumed training:  $ python train.py --checkpoints output checkpoints \  --model output checkpoints epoch_25.hdf5 --start-epoch 25  Training continued from epochs 25-35 at this lower learning rate. We can immediately see the beneﬁt of lowering the learning rate by an order of magnitude – loss drops dramatically, and accuracy enjoys a nice bump.  Figure 12.9, top-right . However, after this initial bump, the training loss continued to drop at a much faster rate than the validation loss. I once again stopped training at epoch 35, lowered the learning rate from 1e− 2 to 1e− 3, and resumed training:  $ python train.py --checkpoints output checkpoints \  --model output checkpoints epoch_35.hdf5 --start-epoch 35  I only allowed ResNet to train for another 5 epochs as I started to notice clear signs of overﬁtting  Figure 12.9, bottom . Loss dips slightly upon the 1e− 3 change, then starts to increase, all the while training loss decreases at a faster rate – this is a telltale sign of overﬁtting. I stopped training altogether at this point, and noted the validation accuracy to be 53.14%.  To determine the accuracy on the testing set, I executed the following command  keeping in mind that the rank_accuracy.py script is identical to the one from Chapter 11 on GoogLeNet:   198  Chapter 12. ResNet  Figure 12.9: Top-left: First 25 epochs when training ResNet on Tiny ImageNet in Experiment 1. Top-right: Next 10 epochs. Bottom: Final 5 epochs. Notice the telltale sign of overﬁtting as validation loss starts to increase during the ﬁnal epochs.  $ python rank_accuracy.py [INFO] loading model... [INFO] predicting on test data... [INFO] rank-1: 53.10% [INFO] rank-5: 75.43%  As the output demonstrates, we are obtaining 53.10% rank-1 accuracy on the testing set. This ﬁrst experiment was not a bad one as we are already closing in on the GoogLeNet + Tiny ImageNet accuracy. Given the success of applying learning rate decay to Tiny ImageNet, I immediately decided to apply the same process to ResNet.  12.5.3 Training ResNet on Tiny ImageNet with Learning Rate Decay  To train ResNet using learning rate decay, open up a new ﬁle, name it train_decay.py, and insert the following code:   1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  12.5 ResNet on Tiny ImageNet  199   set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use "Agg"    import the necessary packages from config import tiny_imagenet_config as config from pyimagesearch.preprocessing import ImageToArrayPreprocessor from pyimagesearch.preprocessing import SimplePreprocessor from pyimagesearch.preprocessing import MeanPreprocessor from pyimagesearch.callbacks import TrainingMonitor from pyimagesearch.io import HDF5DatasetGenerator from pyimagesearch.nn.conv import ResNet from keras.preprocessing.image import ImageDataGenerator from keras.callbacks import LearningRateScheduler from keras.optimizers import SGD import argparse import json import sys import os   set a high recursion limit so Theano doesn’t complain sys.setrecursionlimit 5000   Lines 6-19 import our required Python packages. We’ll need to import the LearningRateScheduler  class on Line 14 so we can apply a learning rate schedule to the training process. The system recursion limit is then set on Line 22 just in case you are using the Theano backend.  Next, we deﬁne the actual function responsible for applying the learning rate decay:   define the total number of epochs to train for along with the  initial learning rate NUM_EPOCHS = 75 INIT_LR = 1e-1  def poly_decay epoch :   initialize the maximum number of epochs, base learning rate,  and power of the polynomial maxEpochs = NUM_EPOCHS baseLR = INIT_LR power = 1.0   compute the new learning rate based on polynomial decay alpha = baseLR *  1 -  epoch   float maxEpochs    ** power   return the new learning rate return alpha  Here we indicate that we’ll train for a maximum of 75 epochs  Line 26  with a base learning rate of 1e− 1  Line 27 . The poly_decay function is deﬁned on Line 29 which accepts a single parameter, the current epoch number. We set power=1.0 on Line 34 to turn the polynomial decay into a linear one  see Chapter 11 for more details . The new learning rate  based on the current epoch  is computed on Line 37 and then returned to the calling function on Line 40.  Let’s move on to the command line arguments:   200  Chapter 12. ResNet   construct the argument parse and parse the arguments ap = argparse.ArgumentParser   ap.add_argument "-m", "--model", required=True,  help="path to output model"   ap.add_argument "-o", "--output", required=True,  help="path to output directory  logs, plots, etc. "   args = vars ap.parse_args     Here we simply need to provide a path to our output serialized --model after training is  complete along with an --output path to store any plots logs.  We can now initialize our data augmentation class and load the RGB means from disk:   construct the training image generator for data augmentation aug = ImageDataGenerator rotation_range=18, zoom_range=0.15,  width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode="nearest"    load the RGB means for the training set means = json.loads open config.DATASET_MEAN .read     As well as create our training and validation HDF5DatasetGenerators:   initialize the image preprocessors sp = SimplePreprocessor 64, 64  mp = MeanPreprocessor means["R"], means["G"], means["B"]  iap = ImageToArrayPreprocessor     initialize the training and validation dataset generators trainGen = HDF5DatasetGenerator config.TRAIN_HDF5, 64, aug=aug,  preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES   valGen = HDF5DatasetGenerator config.VAL_HDF5, 64,  preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES   Our callbacks list will consist of a TrainingMonitor and a LearningRateScheduler:   construct the set of callbacks figPath = os.path.sep.join [args["output"], "{}.png".format   jsonPath = os.path.sep.join [args["output"], "{}.json".format   os.getpid   ]   os.getpid   ]   callbacks = [TrainingMonitor figPath, jsonPath=jsonPath ,  LearningRateScheduler poly_decay ]  Below we instantiate our ResNet architecture and SGD optimizer using the same parameters  as our ﬁrst experiment:   initialize the optimizer and model  ResNet-56  print "[INFO] compiling model..."  model = ResNet.build 64, 64, 3, config.NUM_CLASSES,  3, 4, 6 ,  64, 128, 256, 512 , reg=0.0005, dataset="tiny_imagenet"   42  43  44  45  46  47  48  50  51  52  53  54  55  56  58  59  60  61  62  63  64  65  66  67  69  70  71  72  73  74  75  77  78  79  80   12.5 ResNet on Tiny ImageNet  201  opt = SGD lr=INIT_LR, momentum=0.9  model.compile loss="categorical_crossentropy", optimizer=opt,  metrics=["accuracy"]   81  82  83  85  86  87  88  89  90  91  92  93  94  96  97  98  99  100  101  102  Finally, we can train our network in mini-batches of 64:   train the network print "[INFO] training network..."  model.fit_generator   trainGen.generator  , steps_per_epoch=trainGen.numImages    64, validation_data=valGen.generator  , validation_steps=valGen.numImages    64, epochs=NUM_EPOCHS, max_queue_size=64 * 2, callbacks=callbacks, verbose=1    save the network to disk print "[INFO] serializing network..."  model.save args["model"]    close the databases trainGen.close   valGen.close    The number of epochs we are going to train for is controlled by NUM_EPOCHS deﬁned earlier in this script. We’ll be linearly decreasing our learning rate from 1e-1 down to zero over the course of NUM_EPOCHS. And ﬁnally serialize the model to disk once training is complete:  ResNet on Tiny ImageNet: Experiment 2 In this experiment, I trained the ResNet architecture detailed above on the Tiny ImageNet dataset using the SGD optimizer, a base learning rate of 1e− 1, and a momentum term of 0.9. The learning rate was decayed linearly over the course of 75 epochs. To perform this experiment, I executed the following command:  $ python train_decay.py --model output resnet_tinyimagenet_decay.hdf5 \  --output output  The resulting learning plot can be found in Figure 12.11. Here we can see the dramatic effect that learning rate decay can have on a training process. While both training loss and validation loss diverge from each other, it’s not until epoch 60 where the gap widens past previous epochs. Furthermore, our validation loss continues to decrease as well. At the end of the 75th epoch, I obtained 58.32% rank-1 accuracy.  I then evaluated the network on the testing set using the following command:  $ python rank_accuracy.py [INFO] loading model... [INFO] predicting on test data... [INFO] rank-1: 58.03% [INFO] rank-5: 80.46%   202  Chapter 12. ResNet  Figure 12.10: Using ResNet we are able to reach the 5 position on the Tiny ImageNet leaderboard, beating out all other approaches that attempted to train a network from scratch. All results with lower error than our approach applied ﬁne-tuning feature extraction.  As the results demonstrated, we have reached 58.03% rank-1 and 80.46% rank-5 accuracy on the testing set, a substantial improvement over our previous chapter on GoogLeNet. This result leads to a test error of 1− 0.5803 = 0.4197, which easily takes position 5 on the Tiny ImageNet leaderboard  Figure 12.10 .  This is the best accuracy obtained on the Tiny ImageNet dataset that does not perform some form of transfer learning or ﬁne-tuning. Given that the accuracies for positions 1-4 were obtained from ﬁne-tuning a network that was already trained on the fully ImageNet dataset, it’s hard to compare a ﬁne-tuned network to one that was trained entirely from scratch. If we  fairly  compare our network that was trained from scratch to the other networks trained from scratch on the leaderboard, we can see that our ResNet has obtained the highest accuracy amongst the group.  12.6 Summary  In this chapter, we discussed the ResNet architecture in detail, including the residual module micro-architecture. The original residual module proposed by He et al. in their 2015 paper, Deep Residual Learning for Image Recognition [24] has gone through many revisions. Originally the module consisted of two CONV layers and an identity mapping “shortcut”. In the same paper, it was found that adding the “bottleneck” sequence of 1× 1, 3× 3, and 1× 1 CONV layers improved accuracy.  Then, in their 2016 study, Identity Mappings in Deep Residual Networks [33], the pre- activation residual module was introduced. We call this update “pre-activation” because we apply the activation and batch normalization before the convolution, going against the “conventional wisdom” when building Convolutional Neural Networks.  From there, we implemented the ResNet architecture using both bottleneck and pre-activation using the Keras framework. This implementation was then used to train ResNet on both the CIFAR-10 and Tiny ImageNet datasets. In CIFAR-10, we were able to replicate the results of He et al., obtaining 93.58% accuracy. Then, on Tiny ImageNet, reached 58.03% accuracy, the highest accuracy thus far from a network trained from scratch on Stanford’s cs231n Tiny ImageNet   12.6 Summary  203  Figure 12.11: Applying learning rate to ResNet when trained on Tiny ImageNet leads to 58.32% validation and 58.03% testing accuracy – signiﬁcantly higher than our previous experiment using ctrl + c training.  challenge.  Later in the ImageNet Bundle, we’ll investigate ResNet and train it on the complete ImageNet  dataset, once again replicating the work of He et al.    Bibliography  [1] Diederik P. Kingma and Jimmy Ba. “Adam: A Method for Stochastic Optimization”. In: CoRR abs 1412.6980  2014 . URL: http :     arxiv . org   abs   1412 . 6980  cited on pages 11, 85, 86 .  [2] Geoffrey Hinton. Neural Networks for Machine Learning. http:  www.cs.toronto.edu   ~tijmen csc321 slides lecture_slides_lec6.pdf  cited on pages 11, 85 .  [3] Kaggle Team. Kaggle: Dogs vs. Cats. https:  www.kaggle.com c dogs-vs-cats   cited on pages 12, 95 .  [4] Andrej Karpathy. Tiny ImageNet Challenge. http:  cs231n.stanford.edu project.  [5]  html  cited on pages 12, 131, 168 . Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. http :     www . deeplearningbook.org. MIT Press, 2016  cited on pages 13, 85, 86 .  [6] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. “ImageNet Classiﬁcation with Deep Convolutional Neural Networks”. In: Advances in Neural Information Processing Systems 25. Edited by F. Pereira et al. Curran Associates, Inc., 2012, pages 1097–1105. URL: http:  papers.nips.cc paper 4824- imagenet- classification- with- deep- convolutional-neural-networks.pdf  cited on pages 14, 86, 95, 103, 128 .  [7] Yann Lecun et al. “Gradient-based learning applied to document recognition”. In: Proceed-  ings of the IEEE. 1998, pages 2278–2324  cited on page 14 .  [8] Adrian Rosebrock. PyImageSearch Gurus. https:  www.pyimagesearch.com pyimagesearch-  gurus . 2016  cited on page 16 .  [9] Richard Szeliski. Computer Vision: Algorithms and Applications. 1st. New York, NY, USA: Springer-Verlag New York, Inc., 2010. ISBN: 1848829345, 9781848829343  cited on page 16 .   206  BIBLIOGRAPHY  [10] Maria-Elena Nilsback and Andrew Zisserman. “A Visual Vocabulary for Flower Classi- ﬁcation.” In: CVPR  2 . IEEE Computer Society, 2006, pages 1447–1454. URL: http:   dblp.uni-trier.de db conf cvpr cvpr2006-2.htmlNilsbackZ06  cited on page 17 .  [11] Karen Simonyan and Andrew Zisserman. “Very Deep Convolutional Networks for Large- Scale Image Recognition”. In: CoRR abs 1409.1556  2014 . URL: http:  arxiv.org  abs 1409.1556  cited on pages 32, 86, 171 .  [12] The HDF Group. Hierarchical data format version 5. http:  www.hdfgroup.org HDF5  [13] Andrew Ng. Machine Learning. https :     www . coursera . org   learn   machine -   cited on page 33 .  learning  cited on page 44 .  [14] Navneet Dalal and Bill Triggs. “Histograms of Oriented Gradients for Human Detection”. In: Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition  CVPR’05  - Volume 1 - Volume 01. CVPR ’05. Washington, DC, USA: IEEE Computer Society, 2005, pages 886–893. ISBN: 0-7695-2372-2. DOI: 10.1109 CVPR. 2005.177. URL: http:  dx.doi.org 10.1109 CVPR.2005.177  cited on pages 47, 57 .  [15] David G. Lowe. “Object Recognition from Local Scale-Invariant Features”. In: Proceedings of the International Conference on Computer Vision-Volume 2 - Volume 2. ICCV ’99. Washington, DC, USA: IEEE Computer Society, 1999, pages 1150–. ISBN: 0-7695-0164-8. URL: http:  dl.acm.org citation.cfm?id=850924.851523  cited on pages 47, 57 . [16] T. Ojala, M. Pietikainen, and T. Maenpaa. “Multiresolution gray-scale and rotation invari- ant texture classiﬁcation with local binary patterns”. In: Pattern Analysis and Machine Intelligence, IEEE Transactions on 24.7  2002 , pages 971–987  cited on pages 47, 57 .  [17] Christian Szegedy et al. “Going Deeper with Convolutions”. In: Computer Vision and Pattern Recognition  CVPR . 2015. URL: http:  arxiv.org abs 1409.4842  cited on pages 51, 81, 86, 131, 133, 168 .  [18] Yoav Freund and Robert E Schapire. “A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting”. In: J. Comput. Syst. Sci. 55.1  Aug. 1997 , pages 119–139. ISSN: 0022-0000. DOI: 10.1006 jcss.1997.1504. URL: http:  dx. doi.org 10.1006 jcss.1997.1504  cited on page 71 .  [19] Leo Breiman. “Random Forests”. In: Mach. Learn. 45.1  Oct. 2001 , pages 5–32. ISSN: 0885-6125. DOI: 10.1023 A:1010933404324. URL: http:  dx.doi.org 10.1023 A: 1010933404324  cited on page 71 .  [20] L. Breiman et al. Classiﬁcation and Regression Trees. Monterey, CA: Wadsworth and Brooks,  1984  cited on page 71 .  [21] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer Series in Statistics. New York, NY, USA: Springer New York Inc., 2001  cited on page 71 .  [22] Cuong Nguyen, Yong Wang, and Ha Nam Nguyen. “Random forest classiﬁer combined with feature selection for breast cancer diagnosis and prognostic”. In: Journal of Biomedical Science and Engineering  2013 . URL: http:  file.scirp.org Html 6-9101686_ 31887.htm  cited on page 72 .   BIBLIOGRAPHY  207  [23] Thomas G. Dietterich. “Ensemble Methods in Machine Learning”. In: Proceedings of the First International Workshop on Multiple Classiﬁer Systems. MCS ’00. London, UK, UK: Springer-Verlag, 2000, pages 1–15. ISBN: 3-540-67704-6. URL: http:  dl.acm.org  citation.cfm?id=648054.743935  cited on page 72 .  [24] Kaiming He et al. “Deep Residual Learning for Image Recognition”. In: CoRR abs 1512.03385  2015 . URL: http:  arxiv.org abs 1512.03385  cited on pages 81, 131, 171, 192, 196, 202 .  [25] Gao Huang et al. “Snapshot Ensembles: Train 1, get M for free”. In: CoRR abs 1704.00109   2017 . URL: http:  arxiv.org abs 1704.00109  cited on page 81 .  [26] Andrej Karpathy. Neural Networks  Part III . http:  cs231n.github.io neural-  networks-3   cited on pages 83, 85 .  [28]  [27] Sebastian Ruder. “An overview of gradient descent optimization algorithms”. In: CoRR abs 1609.04747  2016 . URL: http:  arxiv.org abs 1609.04747  cited on page 83 . John Duchi, Elad Hazan, and Yoram Singer. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization”. In: J. Mach. Learn. Res. 12  July 2011 , pages 2121– 2159. ISSN: 1532-4435. URL: http :     dl . acm . org   citation . cfm ? id = 1953048 . 2021068  cited on page 84 .  [29] Matthew D. Zeiler. “ADADELTA: An Adaptive Learning Rate Method”. In: CoRR abs 1212.5701   2012 . URL: http:  arxiv.org abs 1212.5701  cited on page 84 .  [30] Timothy Dozat. Incorporating Nesterov Momentum into Adam. http:  cs229.stanford.  edu proj2015 054_report.pdf  cited on page 86 .  [31] Tom Schaul, Ioannis Antonoglou, and David Silver. “Unit Tests for Stochastic Optimization”. In: CoRR abs 1312.6055  2013 . URL: http:  arxiv.org abs 1312.6055  cited on page 86 .  [32] Forrest N. Iandola et al. “SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size”. In: CoRR abs 1602.07360  2016 . URL: http:  arxiv.org abs  1602.07360  cited on pages 86, 131 .  [33] Kaiming He et al. “Identity Mappings in Deep Residual Networks”. In: CoRR abs 1603.05027   2016 . URL: http:  arxiv.org abs 1603.05027  cited on pages 86, 172, 174, 202 .  [34] Andrew Ng. Nuts and Bolts of Building Applications using Deep Learning. https:  nips.  cc Conferences 2016 Schedule?showEvent=6203. 2016  cited on pages 89, 91 .  [35] Tomasz Malisiewicz. Nuts and Bolts of Building Deep Learning Applications: Ng at NIPS2016. http:  www.computervisionblog.com 2016 12 nuts- and- bolts- of-building-deep.html  cited on page 89 .  [36] Andrej Karpathy. Transfer Learning. http:  cs231n.github.io transfer-learning    cited on page 93 .  [37] Greg Chu. How to use transfer learning and ﬁne-tuning in Keras and Tensorﬂow to build an  image recognition system and classify  almost  any object. https:  deeplearningsandbox. com   how - to - use - transfer - learning - and - fine - tuning - in - keras - and - tensorflow-to-build-an-image-recognition-94b0b02444f2  cited on page 93 .  [38] Adrian Rosebrock. Practical Python and OpenCV + Case Studies. PyImageSearch.com, 2016. URL: https:  www.pyimagesearch.com practical-python-opencv   cited on page 105 .  [39] Andrej Karpathy. CS231n: Convolutional Neural Networks for Visual Recognition. http:    cs231n.stanford.edu . 2016  cited on pages 131, 146 .   BIBLIOGRAPHY  208  [41]  [40] Min Lin, Qiang Chen, and Shuicheng Yan. “Network In Network”. In: CoRR abs 1312.4400   2013 . URL: http:  arxiv.org abs 1312.4400  cited on page 132 . Jost Tobias Springenberg et al. “Striving for Simplicity: The All Convolutional Net”. In: CoRR abs 1412.6806  2014 . URL: http :     arxiv . org   abs   1412 . 6806  cited on pages 133, 171 .  [42] Chiyuan Zhang et al. “Understanding deep learning requires rethinking generalization”. In: CoRR abs 1611.03530  2016 . URL: http:  arxiv.org abs 1611.03530  cited on page 133 .  [43] WordNet. About WordNet. http:  wordnet.princeton.edu. 2010  cited on page 147 . [44] Xavier Glorot and Yoshua Bengio. “Understanding the difﬁculty of training deep feedforward neural networks”. In: In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics  AISTATS’10 . Society for Artiﬁcial Intelligence and Statistics. 2010  cited on page 171 .  [45] Kaiming He et al. “Delving Deep into Rectiﬁers: Surpassing Human-Level Performance on ImageNet Classiﬁcation”. In: CoRR abs 1502.01852  2015 . URL: http:  arxiv.org  abs 1502.01852  cited on page 171 .  [46] Kaiming He. Deep Residual Networks. https :     github . com   KaimingHe   deep -  residual-networks  cited on page 176 .  [47] Wei Wu. ResNet. https:  github.com tornadomeet ResNet  cited on page 176 . [48] Kaiming He. ResNet: Should the convolution layers have biases? https:  github.com  KaimingHe deep-residual-networks issues 10issuecomment-194037195  cited on page 177 .  [49] Rodrigo Benenson. CIFAR-10: Who is the best in CIFAR-10? http:  rodrigob.github. io   are _ we _ there _ yet   build   classification _ datasets _ results . html  43494641522d3130  cited on pages 180, 192 .  [50] Theano Community. Theano: Max Recursion Limit. https:  github.com Theano   Theano issues 689  cited on page 181 .

@highlight

Welcome to the Practitioner Bundle of Deep Learning for Computer Vision with Python! This volume is meant to be the next logical step in your deep learning for computer vision education after completing the Starter Bundle. At this point, you should have a strong understanding of the fundamentals of parameterized learning, neural networks, and Convolutional Neural Networks (CNNs). You should also feel relatively comfortable using the Keras library and the Python programming language to train your own custom deep learning networks. The purpose of the Practitioner Bundle is to build on your knowledge gained from the Starter Bundle and introduce more advanced algorithms, concepts, and tricks of the trade—these techniques will be covered in three distinct parts of the book.