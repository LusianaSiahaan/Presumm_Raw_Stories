Intelligent Systems Reference Library  151  Leslie F. Sikos   Editor  AI in  Cybersecurity   Intelligent Systems Reference Library  Volume 151  Series editors  Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland e-mail: kacprzyk@ibspan.waw.pl  Lakhmi C. Jain, Faculty of Engineering and Information Technology, Centre for Artiﬁcial Intelligence, University of Technology, Sydney, NSW, Australia; Faculty of Science, Technology and Mathematics, University of Canberra, Canberra, ACT, Australia; KES International, Shoreham-by-Sea, UK e-mail: jainlakhmi@gmail.com; jainlc2002@yahoo.co.uk   integrated knowledge and current  The aim of this series is to publish a Reference Library, including novel advances and developments in all aspects of Intelligent Systems in an easily accessible and well structured form. The series includes reference works, handbooks, compendia, textbooks, well-structured monographs, dictionaries, and encyclopedias. It contains well information in the ﬁeld of Intelligent Systems. The series covers the theory, applications, and design methods of Intelligent Systems. Virtually all disciplines such as engineering, computer science, avionics, business, e-commerce, environment, healthcare, physics and life science are included. The list of topics spans all the areas of modern intelligent systems such as: Ambient intelligence, Computational neuroscience, Artiﬁcial life, Virtual society, Cognitive systems, DNA and immunity-based systems, e-Learning and teaching, Human-centred computing and Machine ethics, Intelligent data analysis, Knowledge-based paradigms, Knowledge management, agents, Intelligent decision making, Intelligent network security, Interactive entertainment, Learning paradigms, Recommender systems, Robotics and Mechatronics including human-machine teaming, Self-organizing and adaptive systems, Soft computing including Neural systems, Fuzzy systems, Evolutionary computing and the Fusion of these paradigms, Perception and Vision, Web intelligence and Multimedia.  intelligence, Computational  intelligence, Social  Intelligent control,  Intelligent  More information about this series at http:  www.springer.com series 8578   Leslie F. Sikos Editor  AI in Cybersecurity  123   Editor Leslie F. Sikos School of Information Technology  and Mathematical Sciences University of South Australia Adelaide, SA, Australia  ISSN 1868-4394 Intelligent Systems Reference Library ISBN 978-3-319-98841-2 https:  doi.org 10.1007 978-3-319-98842-9  ISSN 1868-4408   electronic   ISBN 978-3-319-98842-9   eBook   Library of Congress Control Number: 2018950921    Springer Nature Switzerland AG 2019 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, in this publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.  trademarks, service marks, etc.  This Springer imprint is published by the registered company Springer Nature Switzerland AG The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland   Foreword  Artiﬁcial intelligence  AI  is a relatively mature research topic, as evidenced by the top two AI conferences, the International Joint Conference on Artiﬁcial Intelligence and the AAAI Conference on Artiﬁcial Intelligence, which have been held since 1969 and 1980, respectively. Over the years, the advances in AI techniques have been phenomenal, and now AI is nearly ubiquitous. For example, the Guardian recently reported that AlphaZero AI beats champion chess program after teaching itself in 4 hours. Thus, a world in which everything is inﬂuenced, even to the extent of being controlled, by AI is no longer science ﬁction.  AI has applications not only in the commercial world, but also in battleﬁelds and military settings, although this might be controversial. This is why Google decided not to renew their US government contract to develop AI techniques that can be used for military applications, such as battleﬁelds  e.g., AI in Internet of military battleﬁeld things applications .  AI can also play an important role in cybersecurity, cyberthreat intelligence, and analytics to detect, contain, and mitigate advanced persistent threats  APTs , and ﬁght against and mitigate malicious cyberactivities  e.g., organized cybercrimes and state-sponsored cyberthreats . For example, AI techniques can be trained to automatically scan for unknown malware or zero-day exploitation, based on certain features and behavior, rather than speciﬁc signatures.  This is the focus of this book. In Chap. 2, for example, the authors explain the potential of formal knowledge representation of network semantics in addressing interoperability challenges due to heterogeneous network data obtained from a wide range of sources. This constitutes a signiﬁcant amount of data collected from or generated by, say, different security monitoring solutions. The formal knowledge representation of network semantics can be leveraged by intelligent and next-generation AI techniques and solutions to facilitate mining, interpreting, and extracting knowledge from structured  big  data to inform decision-making and cyberdefense strategies. Other cybersecurity applications of AI include the identi- ﬁcation of vulnerabilities and weaknesses in systems and devices, and the detection of suspicious behaviors and anomalies, as explained in this book.  v   vi  Foreword  AI techniques and tools can, however, be exploited for malicious purposes. For example, an adversary can also use AI techniques to identify and exploit vulner- abilities in systems and devices. One possible scenario is for an attacker  or group of attackers  to design AI techniques to identify and exploit vulnerabilities in, say, driverless vehicles and unmanned aerial vehicles  UAVs, also known as drones , in order to facilitate coordinated attacks at places of mass gatherings  e.g., ﬁnancial districts in cities during peak hours . Also, via coordinated attacks, AI techniques can exploit vulnerabilities in smart city infrastructures  e.g., intelligent transporta- tion systems  to maximize the impact of such attacks, with the aim of causing societal panic and unrest. Hence, there is also a need to defend against AI-based cyberphysical attacks.  In other words, more work needs to be done in several directions to answer these  open questions:  1. How to use AI techniques to facilitate forensic investigation? 2. How can we design AI techniques to facilitate the prediction of future cyber-  attacks or potential vulnerabilities  and their risk of successful exploitation ?  3. Can AI techniques help design new security solutions to overcome the pitfalls of human design  e.g., mitigate the “break-and-ﬁx” trend in cryptographic proto- cols  or even design new blockchain types?  In short, the authors provide a comprehensive treatment of AI in cybersecurity, making this book useful to anyone interested in advancing the ﬁeld of AI in cybersecurity.  Unquestionably, this is an exciting era, and the nexus between AI and cyber-  security will be increasingly important in the foreseeable future.  San Antonio, TX, USA  Kim-Kwang Raymond Choo, Ph.D.   Preface  Security breaches and compromised computer systems cause signiﬁcant losses for governments and enterprises alike. Attack mechanisms evolve in parallel with defense mechanisms, and detecting fraudulent payment gateways, protecting cloud services, and securely transferring ﬁles require next-generation techniques that are constantly improved. Artiﬁcial intelligence methods can be employed to ﬁght against cyberthreats and attacks, with the aim of preventing as many future cyber- attacks as possible or at least minimizing their impact. The constantly increasing volume of global cyberthreats and cyberattacks urges automated mechanisms that are capable of identifying vulnerabilities, threats, and malicious activities in a timely manner. Knowledge representation and reasoning, automated planning, and machine learning are just some of those areas through which machines can contribute to proactive, rather than reactive, cybersecurity measures. To this end, AI-powered cybersecurity applications are developed—see the AI-based security infrastructure Chronicle and the “enterprise immune system” Darktrace, for example.  levels  The increasing complexity of networks, operating system and wireless network vulnerabilities, and malware behavior poses a real challenge at both the national and international to security specialists. This book is a collection of state-of-the-art AI-powered security techniques and approaches. Chapter 1 intro- duces the reader to ontology engineering and its use in cybersecurity, cyberthreat intelligence, and cybersituational awareness applications. The formal deﬁnition of concepts, properties, relationships, and entities of network infrastructures in ontologies makes it possible to write machine-interpretable statements. These statements can be used for efﬁcient indexing, querying, and reasoning over expert knowledge. Chapter 2 details how to employ knowledge engineering in describing network topologies and trafﬁc ﬂow so that software agents can process them automatically, and perform network knowledge discovery. Network analysts use information derived from diverse sources that cannot be efﬁciently processed by software agents unless a uniform syntax is used, associated with well-understood such as model-theoretic semantics and meaning ﬁxed via interpretations,  vii   viii  Preface  Tarski-style interpretation. The formal representation of network knowledge requires not only crisp, but also fuzzy and probabilistic axioms, and metadata such as provenance. Reasoning over semantically enriched network knowledge enables automated mechanisms to generate non-trivial statements about correlations even experienced analysts would overlook, and automatically identify misconﬁgurations that could potentially lead to vulnerabilities.  Chapter 3 warns us that AI can not only assist, but might also pose a threat to cybersecurity, because hackers can also use AI methods, such as to attack machine learning systems to exploit software vulnerabilities and compromise program code. For example, they might introduce misleading data in machine learning algorithms  data poisoning , which can result in email campaigns that mark thousands of spam emails as “not spam” to skew the algorithm’s behavior, thereby achieving malicious emails to be considered solicited.  Software vulnerabilities mentioned in social media sites can be used by software vendors for prompt patching, but also by adversaries to exploit them before they are patched. Using their hands-on skills, the authors of Chap. 4 demonstrate the cor- relation between publishing and exploiting software vulnerabilities, and propose an approach to predict the likelihood of vulnerability exploitation based on online resources, including the Dark Web and the Deep Web, which can be used by vendors for prioritizing patches.  Chapter 5 describes AI methods suitable for detecting network attacks. It pre- sents a binary classiﬁer and optimization techniques to increase classiﬁcation accuracy, training speed, and the efﬁciency of distributed AI-powered computations for network attack detection. It describes models based on neural, fuzzy, and evolutionary computations, and various schemes for combining binary classiﬁers to enable training on different subsamples.  Chapter 6 discusses intrusion detection techniques that utilize machine learning and data mining to identify malicious connections. Common intrusion detection systems using fuzzy logic and artiﬁcial neural networks are critically reviewed and compared. Based on this comparison, the primary challenges and opportunities of handling cyberattacks using artiﬁcial neural networks are summarized.  Security is only as strong as the weakest link in a system, which can be easily demonstrated by the example of a careless or inexperienced user, who can com- promise the security of a ﬁle transfer or a login regardless of enterprise security measures. For example, software installations may not only result in malware infections, but also in data loss, privacy breach, etc. Artiﬁcial intelligence can be utilized in the analysis of the security of software installers, as seen in Chap. 7, which investigates the machine learning-based analysis of the Android application package ﬁle format used on Android mobile devices for the distribution and installation of mobile apps and middleware. The analysis of the APK ﬁle structure makes it possible to understand those properties that can be used by machine learning algorithms to identify potential malware targeting.   Preface  ix  Network analysts, defense scientists, students, and cybersecurity researchers can all beneﬁt from the range of AI approaches compiled in this book, which not only reviews the state of the art, but also suggests new directions in this rapidly growing research ﬁeld.  Adelaide, SA, Australia  Leslie F. Sikos, Ph.D.   Contents  1 OWL Ontologies in Cybersecurity: Conceptual Modeling  of Cyber-Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Leslie F. Sikos 1.1 Introduction to Knowledge Engineering in Cybersecurity . . . . . . . 1.2 Cybersecurity Taxonomies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 A Core Reference Ontology for Cybersecurity . . . . . . . . . . . . . . . 1.4 Upper Ontologies for Cybersecurity . . . . . . . . . . . . . . . . . . . . . . . 1.5 Domain Ontologies for Cybersecurity . . . . . . . . . . . . . . . . . . . . . 1.5.1 Intrusion Detection Ontologies . . . . . . . . . . . . . . . . . . . . . 1.5.2 Malware Classiﬁcation and Malware  Behavior Ontologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.5.3 Ontologies for Cyberthreat Intelligence . . . . . . . . . . . . . . . 1.5.4 The Ontology for Digital Forensics . . . . . . . . . . . . . . . . . . 1.5.5 Ontologies for Secure Operations and Processes . . . . . . . . 1.5.6 An Ontology for Describing Cyberattacks and Their  Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6 Networking Ontologies for Cybersecurity . . . . . . . . . . . . . . . . . . . 1.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  2 Knowledge Representation of Network Semantics  for Reasoning-Powered Cyber-Situational Awareness . . . . . . . . . . . . Leslie F. Sikos, Dean Philp, Catherine Howard, Shaun Voigt, Markus Stumptner, and Wolfgang Mayer 2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Communication Network Concepts . . . . . . . . . . . . . . . . . . . . . . . 2.3.1 Networks and Topologies . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.2 Network Interfaces and IP Addressing . . . . . . . . . . . . . . . .  1  1 4 6 6 8 8  8 9 10 11  11 12 14 15  19  19 20 25 25 26  xi   xii  Contents  2.3.3 Routers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.4 Autonomous Systems and Routing . . . . . . . . . . . . . . . . . .  2.4 Formal Knowledge Representation for Cyber-Situational  Awareness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.1 Representing Network Knowledge Using Ontology  Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.5 Representing Network Data Provenance . . . . . . . . . . . . . . . . . . . . 2.6 Representing Network Data Uncertainty . . . . . . . . . . . . . . . . . . . . 2.7 Representing Network Data Vagueness . . . . . . . . . . . . . . . . . . . . 2.8 Reasoning Support for Cyber-Situational Awareness . . . . . . . . . . . 2.9 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3 The Security of Machine Learning Systems . . . . . . . . . . . . . . . . . . .  Luis Muñoz-González and Emil C. Lupu 3.1 Machine Learning Algorithms Are Vulnerable . . . . . . . . . . . . . . . 3.2 Threat Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Threats by the Capability of the Attacker . . . . . . . . . . . . . 3.2.2 Threats by the Goal of the Attacker . . . . . . . . . . . . . . . . . 3.2.3 Threats by the Knowledge of the Attacker . . . . . . . . . . . . . 3.2.4 Threats by Attack Strategy . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Data Poisoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Poisoning Attack Scenarios . . . . . . . . . . . . . . . . . . . . . . . 3.3.2 Optimal Poisoning Attacks . . . . . . . . . . . . . . . . . . . . . . . . 3.3.3 Transferability of Poisoning Attacks . . . . . . . . . . . . . . . . . 3.3.4 Defense Against Poisoning Attacks . . . . . . . . . . . . . . . . . . 3.4 Attacks at Test Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 Evasion Attack Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.2 Computing Evasion Attacks . . . . . . . . . . . . . . . . . . . . . . . 3.4.3 Transferability of Evasion Attacks . . . . . . . . . . . . . . . . . . 3.4.4 Defense Against Evasion Attacks . . . . . . . . . . . . . . . . . . . 3.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  4 Patch Before Exploited: An Approach to Identify Targeted  Software Vulnerabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Mohammed Almukaynizi, Eric Nunes, Krishna Dharaiya, Manoj Senguttuvan, Jana Shakarian, and Paulo Shakarian 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.1 Supervised Learning Approaches . . . . . . . . . . . . . . . . . . . 4.3.2 Challenges of Exploit Prediction . . . . . . . . . . . . . . . . . . . .  26 28  29  30 34 37 39 41 42 42  47  47 48 49 50 51 52 54 55 57 63 65 67 69 70 73 75 76 77  81  81 85 86 86 87   Contents  xiii  4.4 Exploit Prediction Model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.1 Data Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.2 Feature Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Vulnerability and Exploit Analysis . . . . . . . . . . . . . . . . . . . . . . . 4.5.1 Likelihood of Exploitation . . . . . . . . . . . . . . . . . . . . . . . . 4.5.2 Time-Based Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.3 Vendor- Platform-Based Analysis . . . . . . . . . . . . . . . . . . . 4.5.4 Language-Based Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 4.6 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  89 90 92 95 95 96 97 98 99 4.6.1 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 100 4.6.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 4.7 Adversarial Data Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . 107 4.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 4.9 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111  5 Applying Artiﬁcial Intelligence Methods to Network Attack  Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 Alexander Branitskiy and Igor Kotenko 5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 5.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 5.3 Binary Classiﬁers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 5.3.1 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 5.3.2 Neuro-Fuzzy Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 5.3.3 Support Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . . 128 5.4 Training the Binary Classiﬁer for Detecting Network Attacks . . . . 132 5.4.1 Calculating and Preprocessing Network Parameters . . . . . . 132 5.4.2 Genetic Optimization of the Weights of the Binary  Classiﬁer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 5.4.3 An Algorithm for Network Attack Detection . . . . . . . . . . . 136 5.5 Schemes for Combining the Binary Classiﬁers . . . . . . . . . . . . . . . 137 5.5.1 Low-Level Schemes for Combining Detectors . . . . . . . . . . 137 5.5.2 Aggregating Compositions . . . . . . . . . . . . . . . . . . . . . . . . 140 5.5.3 Common Approach for Combining Detectors . . . . . . . . . . 142 5.6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 5.6.1 The Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 5.6.2 Experiment 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 5.6.3 Experiment 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 5.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147   xiv  Contents  6 Machine Learning Algorithms for Network Intrusion Detection . . . . 151  Jie Li, Yanpeng Qu, Fei Chao, Hubert P. H. Shum, Edmond S. L. Ho, and Longzhi Yang 6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 6.2 Network Intrusion Detection Systems . . . . . . . . . . . . . . . . . . . . . 153 6.2.1 Deployment Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 6.2.2 Detection Methodologies . . . . . . . . . . . . . . . . . . . . . . . . . 155 6.3 Machine Learning in Network Intrusion Detection . . . . . . . . . . . . 156 6.3.1 Fuzzy Inference Systems . . . . . . . . . . . . . . . . . . . . . . . . . 157 6.3.2 Artiﬁcial Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . 164 6.3.3 Deployment of ML-Based NIDSes . . . . . . . . . . . . . . . . . . 169 6.4 Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170 6.4.1 Evaluation Environment . . . . . . . . . . . . . . . . . . . . . . . . . . 170 6.4.2 Model Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170 6.4.3 Result Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 6.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174  7 Android Application Analysis Using Machine Learning  Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 Takeshi Takahashi and Tao Ban 7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 7.2 The Structure of Android Application Packages . . . . . . . . . . . . . . 183 7.2.1 Central Conﬁguration  AndroidManifest.xml  . . . . . . . . . . 183 7.2.2 Dalvik Bytecode  classes.dex  . . . . . . . . . . . . . . . . . . . . . . 184 7.3 Techniques for Identifying Android Malware . . . . . . . . . . . . . . . . 185 7.3.1 Blacklisting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 7.3.2 Parameterizing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 7.3.3 Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 7.4 Dataset Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 7.4.1 APK File Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 7.4.2 Application Metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 7.4.3 Label Assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188 7.4.4 Data Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 7.4.5 A Novel Dataset of Safe and Malicious APK Files . . . . . . 189 7.5 Detecting Malware Using SVM . . . . . . . . . . . . . . . . . . . . . . . . . . 191 7.5.1 SVM: A Brief Overview . . . . . . . . . . . . . . . . . . . . . . . . . 191 7.5.2 Feature Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 7.5.3 Hyperparameter Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . 194 7.5.4 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194 7.5.5 Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195   Contents  xv  7.6 Comparison with Parameterizing . . . . . . . . . . . . . . . . . . . . . . . . . 196 7.6.1 Extending DroidRisk . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197 7.6.2 DroidRisk Performance . . . . . . . . . . . . . . . . . . . . . . . . . . 197 7.7 Feature Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 7.7.1 Recursive Feature Elimination . . . . . . . . . . . . . . . . . . . . . 199 7.7.2 Ranking Criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200 7.7.3 Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200 7.8 Issues and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202 7.9 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204   About the Editor  that  require  Leslie F. Sikos, Ph.D., is a computer scientist special- izing in formal knowledge representation, ontology engineering, and automated reasoning applied to var- including cyberthreat intelligence and ious domains, network applications cybersituational awareness. He has worked in both academia and the enterprise, and acquired hands-on skills in datacenter and cloud infrastructures, cyberthreat management, and ﬁrewall conﬁguration. He holds professional certiﬁcates and is a member of industry-leading organizations, such as the ACM, the Association for Automated Reasoning, the IEEE Special Interest Group on Big Data for Cyber Security and Privacy, and the IEEE Computer Society Technical Committee on Security and Privacy.  xvii   Chapter 1 OWL Ontologies in Cybersecurity: Conceptual Modeling of Cyber-Knowledge  Leslie F. Sikos  Abstract Network vulnerability checking, automated cyberthreat intelligence, and real-time cybersituational awareness require task automation that beneﬁt from formally described conceptual models. Knowledge organization systems, includ- ing controlled vocabularies, taxonomies, and ontologies, can provide the network semantics needed to turn raw network data into valuable information for cyberse- curity specialists. The formal knowledge representation of cyberspace concepts and properties in the form of upper and domain ontologies that capture the semantics of network topologies and devices, information ﬂow, vulnerabilities, and cyberthreats can be used for application-speciﬁc, situation-aware querying and knowledge dis- covery via automated reasoning. The corresponding structured data can be used for network monitoring, cybersituational awareness, anomaly detection, vulnerability assessment, and cybersecurity countermeasures.  1.1 Introduction to Knowledge Engineering  in Cybersecurity  Formal knowledge representation is a ﬁeld of artiﬁcial intelligence, which captures the semantics  meaning  of concepts, properties, relationships, and individuals of speciﬁc knowledge domains, i.e., ﬁelds of interest or areas of concern, as struc- tured data. The Resource Description Framework  RDF 1 makes it possible to write machine-interpretable statements in the form of subject–predicate–object triples, called RDF triples [1]. These can be expressed using various syntaxes, among which  1https:  www.w3.org RDF  L. F. Sikos  B   University of South Australia, Adelaide, SA, Australia e-mail: leslie.sikos@unisa.edu.au    Springer Nature Switzerland AG 2019 L. F. Sikos  ed. , AI in Cybersecurity, Intelligent Systems Reference Library 151, https:  doi.org 10.1007 978-3-319-98842-9_1  1   2  L. F. Sikos  some of the most common ones are RDF XML,2 Turtle,3 N-Triples,4 JSON-LD,5 RDFa,6 and HTML5 Microdata.7 In the Turtle serialization format, for example, the natural language sentence “WannaCry is a ransomware” can be written as the triple :WannaCry rdf:type :Ransomware ., where type is the isA relation- ship from the RDF vocabulary  rdfV  at http:  www.w3.org 1999 02 22-rdf-syntax- nstype.8 Note that RDF triples are terminated by a period in Turtle, unless there are multiple statements with the same subject and predicate, but different objects, in which case the subject and the predicate are written just once, followed by the list of objects, separated by semicolons. Using the namespace mechanism, preﬁxes  such as rdf: in this example  can be abbreviated by deﬁning the preﬁx in the beginning of the Turtle ﬁle as shown in Listing 1.1.  Listing 1.1 Namespace preﬁx declaration @prefix rdf :   .  Such RDF triples are machine-interpretable, and using strict rules they can be used  to infer new statements based on explicitly stated ones via automated reasoning.  Machine-interpretable deﬁnitions are created on different levels of conceptual- ization and comprehensiveness, resulting in different types of knowledge organiza- tion systems  KOS . Controlled vocabularies organize terms bound to preselected schemes for subsequent retrieval in subject indexing, describe subject headings, etc.  see Deﬁnition 1.1 .  Deﬁnition 1.1  Controlled Vocabulary  A controlled vocabulary is a triple V =  NC, NR, NI   of countably inﬁnite sets of IRI9 symbols denoting atomic concepts  concept names or classes   NC , atomic roles  role names, properties, or predicates   NR , and individual names  objects   NI  , respectively, where NC, NR, and NI are pairwise disjoint sets.  Thesauri are reference works that list words grouped together according to sim- ilarity of meaning, such synonyms. Taxonomies are categorized words in hierarchy. Knowledge bases  KB  describe the terminology and individuals of a knowledge domain, including the relationships between concepts and individuals. Ontologies are formal conceptualizations of a knowledge domain with complex relationships and complex rules suitable for inferring new statements automatically. Datasets are collections of related machine-interpretable statements.  2https:  www.w3.org TR rdf-syntax-grammar  3https:  www.w3.org TR turtle  4https:  www.w3.org TR n-triples  5https:  www.w3.org TR json-ld  6https:  www.w3.org TR rdfa-primer  7https:  www.w3.org TR microdata  8Because this is a very common predicate, Turtle allows the abbreviation of rdf:type simply as a. 9Internationalized resource identiﬁer.   1 OWL Ontologies in Cybersecurity …  3  The RDF vocabulary, which is capable of expressing core relationships, was extended to be able to describe more sophisticated relationships between con- cepts and properties, such as taxonomical structures, resulting in the RDF Schema Language  RDFS .10 Vocabularies, taxonomies, thesauri, and simple ontologies are usually deﬁned in RDFS. Complex knowledge domains require even more repre- sentational capabilities, such as property cardinality constraints, domain and range restrictions, and enumerated classes, which led to the Web Ontology Language  OWL, purposefully abbreviated with the W and the O swapped ,11 a language spe- cially designed for creating web ontologies with a rich set of modeling constructors and addressing the ontology engineering limitations of RDFS.  Each OWL ontology consists of RDF triples that deﬁne concepts  classes , roles  properties and relationships , and individuals.12 The ontology ﬁle of an OWL ontology can be deﬁned by the ontology URI, the rdf:type predicate, and the Ontology concept from the OWL vocabulary  see Listing 1.2 .  Listing 1.2 An OWL ﬁle deﬁned as an ontology @prefix rdf :   . @prefix owl :   .   a owl : O n t o l o g y .  Classes can be declared using rdf:type and the Class concept from the OWL  vocabulary as shown in Listing 1.3.  Listing 1.3 An OWL class declaration : Malware a owl : Class .  Superclass-subclass relationships can be deﬁned using the subclassOf prop-  erty from the RDFS vocabulary as shown in Listing 1.4.  Listing 1.4 An OWL ﬁle deﬁned as an ontology : R a n s o m w a r e rdfs : s u b C l a s s O f : M a l w a r e .  In OWL, properties that deﬁne relationships between entities are declared using owl:ObjectProperty, and those that assign properties to property values or value ranges with owl:DatatypeProperty, as demonstrated in the examples in Listing 1.5.  Listing 1.5 An object property and a datatype property declaration : c o n n e c t e d T o a owl : O b j e c t P r o p e r t y . : h a s I P A d d r e s s a owl : D a t a t y p e P r o p e r t y .  Note that OWL concept names are typically written in PascalCase, in which words are created by concatenating capitalized words, and OWL property names are usually written in camelCase, i.e., compound words or phrases are written in a way that each word or abbreviation begins with a capital letter  medial capitalization .  Individuals can be declared using the namedIndividual term from the OWL  vocabulary as shown in Listing 1.6.  10https:  www.w3.org TR rdf-schema  11https:  www.w3.org OWL  12These may be complemented by SWRL rules, although doing so can result in undecidability.   4  L. F. Sikos  Listing 1.6 An entity declaration : W a n n a C r y a owl : n a m e d I n d i v i d u a l .  Individuals can also be declared as an instance of a class, as seen in our very ﬁrst example. In OWL, many other constructors are available as well, such as atomic and complex concept negation, concept intersection, universal restrictions, limited exis- tential quantiﬁcation, transitivity, role hierarchies, inverse roles, functional proper- ties, datatypes, nominals, and cardinality restrictions. In the second version of OWL, OWL 2, these are complemented by the union of a ﬁnite set of complex role inclu- sions and a role hierarchy, and unqualiﬁed cardinality restrictions are replaced by qualiﬁed cardinality restrictions.13  Formal grounding for OWL ontologies can be provided by description logics, many of which are decidable fragments of ﬁrst-order predicate logic and feature a different balance between expressivity and reasoning complexity by supporting different sets of mathematical constructors [2].  The uniform representation of expert cyber-knowledge enables important data val- idation tasks, such as automated data integrity checking, and inference mechanisms to make implicit statements explicit via automated reasoning.  The scope of an ontology is determined by granularity. Upper ontologies, also known as upper-level ontologies, top-level ontologies, or foundational ontologies, are generic ontologies applicable to various domains. Domain ontologies describe the vocabulary of a speciﬁc knowledge domain with a speciﬁc viewpoint and factual data. Core reference ontologies are standardized or de facto standard ontologies used by different user groups to integrate their different viewpoints about a knowledge domain by merging several domain ontologies.  1.2 Cybersecurity Taxonomies  The Taxonomy of Dependable and Secure Computing deﬁnes a concept hierarchy for attributes  availability, reliability, safety, conﬁdentiality, integrity, maintainability , threats  faults, errors, failures , and means  fault prevention, fault tolerance, fault removal, fault forecasting  [3]. The taxonomical structure details these concepts fur- ther. Faults are classiﬁed as development faults, physical faults, and interaction faults. Failures can be service failures, development failures, or dependability failures. Fault tolerance has subclasses such as error detection and recovery. Fault removal deﬁnes more speciﬁc classes, such as veriﬁcation, diagnosis, correction, and non-regression veriﬁcation. Fault forecasting is either ordinal evaluation or probabilistic evaluation, the latter of which can be modeling or operational testing. The fault classes are particularly detailed in a very deep class hierarchy.  Hansman and Hunt created a taxonomy of network and computer attacks to cat- egorize attack types [4]. It consists of four “dimensions,” which categorize attacks  13Providing examples for each constructor is beyond the scope of this chapter. For a detailed description, see https:  www.w3.org TR owl2-quick-reference    1 OWL Ontologies in Cybersecurity …  5  The attack taxonomy of Gao et al. categorizes attacks as follows [5]:  by attack classes, attack targets, vulnerabilities and exploits used by the attack, and the possibility whether the attack has a payload or effect beyond itself. To achieve this, the ﬁrst dimension deﬁnes concepts such as virus, worm, Trojan, buffer over- ﬂow, denial of service attack, network attack, physical attack, password attack, and information gathering attacks. The second dimension has both hardware and soft- ware categories, including computer, operating system, application, and network, all detailed to be able to express the highest level of granularity, such as operating system versions and editions, and networking protocols. The third dimension reuses de facto standard Common Vulnerabilities and Exposures  CVE 14 entries whenever possible. The fourth dimension describes possible payloads, such as a worm may have a Trojan payload.   Attack impact: conﬁdentiality, integrity, availability, authentication, authorization, auditing   Attack vector: DoS, human behavioral attack, information gathering, malformed input, malicious code, network attack, password attack, physical attack   Attack target: hardware  computer, network equipment, peripheral device , soft- ware  application, network, operating system , human  receiver, sender    Vulnerability: location, motivation, resource-speciﬁc weaknesses, weaknesses introduced during design, weaknesses introduced during implementation, weak- nesses in OWASP15 top ten   Defense: security network communication, security hardware, standard, emissions security, technology mistakenly used as countermeasure, backup, encryption, cryp- tography, message digest checksum, memory protection, trust management, access control, vulnerability scanner, source code scanner, login system, monitoring, hon- eypot, sanitizer, key management This taxonomy was designed to describe attack effect evaluation by capturing security property weights and evaluation indices, where the attack effect covers system performance changes both before and after attacks.  The taxonomy of cyberthreat intelligence information exchange of Burger et al. is aligned with OpenIOC,16 the Structured Threat Information Expression  STIX ,17 and the Incident Object Description Exchange Format  IODEF 18 [6]. In contrast to taxonomies with a class hierarchy, it adopts a layered model, which partially follows the ISO OSI model. The ﬁve layers from bottom to top are transport, session, indi- cators, intelligence, and 5W’s. The transport conceptual layer contains synchronous byte streams, asynchronous atomic messages, and raw byte streams. The session layer covers authenticated senders and receivers, and permissions on entire con- tent. The indicators layer deﬁnes patterns, behaviors, and permission on indicator.  14https:  cve.mitre.org 15https:  www.owasp.org 16http:  www.openioc.org 17https:  oasis-open.github.io cti-documentation stix intro 18https:  www.ietf.org rfc rfc5070.txt   6  L. F. Sikos  Intelligence covers three concepts: action, query, and target. The top layer contains the 5W’s and the H  Who, What, When, Where, Why, How .  1.3 A Core Reference Ontology for Cybersecurity  The Reference Ontology for Cybersecurity Operational Information is a cyberse- curity ontology that structures cybersecurity information and orchestrates industry speciﬁcations from the viewpoint of cybersecurity operations [7]. It is aimed at serving as the basis for cybersecurity information exchange on a global scale using standards from ISO IEC,19 OASIS,20 NIST,21 ITU-T,22 MITRE,23 the Open Grid Forum,24 and IEEE.25 The implementation of this ontology allows vulnerabilities to be described using de facto standard CVE identiﬁers and threats using Common Attack Pattern Enumeration and Classiﬁcation  CAPEC  identiﬁers.26 The Refer- ence Ontology for Cybersecurity Operational Information is aligned with standard guidelines such as ISO IEC 27032,27 ITU-T E.409,28 ITU-T X.1500,29 and IETF RFC 2350.30  1.4 Upper Ontologies for Cybersecurity  The Security Ontology  SO  was developed to be used for representing arbitrary information systems [8]. It can model assets such as data, network, and services, countermeasures such as ﬁrewalls and antivirus software, and risk assessment knowl- edge. The Security Ontology can be used to formalize security requirements, and supports the aggregation of, and reasoning about, interoperable security knowledge derived from diverse sources.  The Security Asset Vulnerability Ontology  SAVO  is an upper ontology for information security, and was specially designed to capture core concepts such as threat, risk, DoS attack, illegal access, and vulnerability, and the properties of these  19https:  www.iso.org 20https:  www.oasis-open.org 21https:  www.nist.gov 22https:  www.itu.int 23https:  www.mitre.org 24https:  www.ogf.org 25https:  www.ieee.org 26https:  capec.mitre.org 27https:  www.iso.org standard 44375.html 28https:  www.itu.int rec T-REC-E.409-200405-I en 29https:  www.itu.int rec T-REC-X.1500 en 30https:  www.ietf.org rfc rfc2350.txt   1 OWL Ontologies in Cybersecurity …  7  attacks  victimOf  DoS Attack  subclassOf  proceeds  Attack  performs  causes  Threat Agent  hasRelation  Precondition  precedes hasPrecondition  Asset Value  hasValue  Data  Annihilation  hasAsset  Peer  is  Information  Leakage  Illegal Access  is  is  Failure  Security Event  is  affects  Consequence  causes  resultsIn  affects  hasDefence  isDefence  mitigates  Asset  Risk  instanceOf  Quantitative RiskAnalysis  estimates  estimates  SystemData  Qualitative RiskAnalysis  ClientData  subclassOf  subclassOf  Component  Data  subclassOf  Service  subclassOf  subclassOf subclassOf  subclassOf  Resource  Software  subclassOf  subclassOf  CPU  Account  influences  resultsIn SASO  hasVulnerability  subclassOf  Memory  subclassOf  subclassOf  Storage  SystemAccount  isExploredWith  subclassOf  Defence  Exposure isDescription  isDescription  hasDescription  hasDescription  relatesTo exploits  Security Function  uses, assures, hasSecParams  uses  VulnName  VulnAttribute  hasName  VulnValue  hasAttribute  hasValue  Supplier  hasSupplier  develops  Vulnerability  hasPatch  isPatch  Patch  Threat  isPreconditionTo  hasPrecondition  endangers  Client Account  Fig. 1.1 Structure of the Security Asset Vulnerability Ontology [9]  concepts. It deﬁnes the relationship between threat, vulnerability, risk, exposure, attack, and other security concepts, and maps low-level and high-level security requirements and capabilities  see Fig. 1.1 .  The Security Algorithm-Standard Ontology  SASO , developed by the same authors as SAVO, encompasses security algorithms, standards, concepts, creden- tials, objectives, assurance levels. It consists of concepts such as security algo- rithm, security assurance, security credential, and security objective. The security ontology of Fenz et al. was created to capture the semantics of risk management, covering security concepts such as attributes, vulnerabilities, threats, controls, rat- ings, and alike [10]. The attribute type contains subtypes such as control type, threat origin, scale, etc. The control concept covers the control type, relation to established information security standards, implementation speciﬁcation, and mit- igation speciﬁcation. Vulnerability covers both administrative vulnerabilities and technical vulnerabilities. The deﬁnitions in the ontology are formally grounded with description logic axioms, and are aligned with standards such as ISO 2700131 and the NIST information security risk management guide [11]. Wali et al. gen- erated a cybersecurity ontology from textbook index terms via automated clas- siﬁcation using bootstrapping strategies [12]. The ontology is rather comprehen- sive, with a deep taxonomical structure. For the Countermeasure concept, for example, it deﬁnes subconcepts such as AccessControl, Backup, Checksum, Cryptography, EmissionsSecurity, HoneyPot, and KeyManagement.  31https:  www.iso.org isoiec-27001-information-security.html   8  L. F. Sikos  The ontology engineers considered terminological differences and alternate cate- gories by using two textbook indices, along with Wikipedia categories. Because the Wikipedia categories provided either no category information or too many ambigu- ous categories for index terms, however, the ontology is dominated by terminology obtained from textbooks. The Uniﬁed Cybersecurity Ontology  UCO 32 was designed to incorporate and integrate heterogeneous data and knowledge schemata from var- ious cybersecurity systems [13]. It is aligned with the most common cybersecurity standards, and can be used to complement other cybersecurity ontologies. The Uni- ﬁed Cybersecurity Ontology deﬁnes important concepts, such as means of attack, consequences, attacks, attackers, attack patterns, exploits, exploit targets, and indi- cators.  1.5 Domain Ontologies for Cybersecurity  There is an abundance of domain ontologies that provide speciﬁc cybersecurity con- cept and property deﬁnitions. All of these are different in terms of speciﬁcity, gran- ularity, and intended application.  1.5.1 Intrusion Detection Ontologies  He et al. introduced an ontology for intrusion detection, which captures semantics for attack signatures including, but not limited to, system status  connection status, CPU and memory usage , IP address, port, protocol, and router and ﬁrewall logs [14]. Vulnerability representations in this ontology are aligned with common CVE codes. The ontology can be used for ontology-based signature intrusion detection using string matching, and for multi-sensor cooperative detection.  1.5.2 Malware Classiﬁcation and Malware Behavior  Ontologies  The Cyber Ontology of MITRE is based on a malware ontology, enables integration across disparate data sources, and supports automated cyber-defense tasks [15]. The conceptual framework behind this ontology is Ingle’s diamond model of malicious activity, which includes actors, victims, infrastructure, and capabilities. The Ontology for Malware Behavior is an ontology based on a set of suspicious behaviors observed during malware infections on the victims’ systems [16]. It deﬁnes the following types of concepts:  32https:  github.com Ebiquity Uniﬁed-Cybersecurity-Ontology blob master uco_1_5_rdf.owl   1 OWL Ontologies in Cybersecurity …  9    Attack launch events, such as denial of service, email sending  spam, phising , scanning, exploit sending   Evasion, including anti-analysis  debugger checking, environment detection, removal of evidence , anti-defense  removal of registries, shutdown of defense mechanisms    Remote control, such as download code  known malware execution, other code execution, get command    Self-defense, including maintenance events, such as component checking, creating synchronization object, language checking, persistence   Stealing, including system information stealing  hostname, OS information, resource information  and user information stealing  credential, Internet banking data    Subversion, such as browser, memory writing, and operating system  The ontology can be used to describe malware actions with RDF statements. Swim- mer proposed malonto, a malware ontology to be used for hypothesis testing and root cause analysis via reasoning. It deﬁnes classes for viruses, worms, Trojans, exploits, droppers, and properties such as drops, hasPayload, hasInsituacy, has Locality, hasObfuscation, hasTransitivity, and hasPlurality. The Malware Ontology33 is a comprehensive ontology containing malware cate- gories, such as Trojan, exploit categories, such as XSS and SQLi, and about 12,000 malware names. This ontology was designed for threat intelligence teams to assist in completing Big Data tasks in a timely manner. It can be used to summarize malware targets, answer questions related to patterns, describe recently reported IPs, hashes, and ﬁle names, and aggregate and link technical information related to malware.  1.5.3 Ontologies for Cyberthreat Intelligence  There are ongoing community efforts to standardize cyberthreat information, as evi- denced by the STIX and the IODEF formats, and ontologies that deﬁne terms aligned with them [17]. In fact, several cyberthreat ontologies and based on STIX concepts, and some extend STIX [18], although there are threat ontologies that are more speciﬁc or are not aligned with de facto standards.  The Incidence Response and Threat Intelligence Ontology34 is, as its name sug- gests, an ontology for classifying and analyzing Internet entities with an emphasis on computer incident response and threat intelligence processing. It deﬁnes semantics for data formats used in this ﬁeld, making it unnecessary to re-interpret data in case it is not trivial how the data was produced.  Ekelhart et al. created an ontology based on the Taxonomy of Dependable and Secure Computing mentioned in Sect. 1.2 and concepts of the IT infrastructure  33https:  www.recordedfuture.com malware-ontology  34https:  raw.githubusercontent.com mswimmer IRTI-Ontology master irti.rdf   10  L. F. Sikos  domain [19]. It can be used to formally describe the allocation of relevant IT infras- tructure elements in a building, including ﬂoor and room numbers. Such descriptions can be used for deﬁning disasters, such as a physical threat. When used for simula- tion, the ontology can provide the basis for calculating IT costs, analyze the impact of a particular threat, potential countermeasures, and their beneﬁts.  The insider threat indicator ontology of Costa et al.35 deﬁnes concepts for describ- ing potential indicators of malicious insider activity, insider threat detection, pre- vention, and mitigation [20]. The ontology supports the creation, sharing, and analysis of indicators of insider threats with concepts such as AcceptAction, AccessAction, AccountAuthenticationInformation, Anomalous Event, Breach Action, Data ExfiltrationEvent, DataModificationEvent, EncryptAction, FailedAction, FraudulentAction, IllegitimateAction, and IntellectualProperty.  BackdoorSoftwareAsset,  ClassifiedInformation,  DataDeletionEvent,  BankAccountAsset,  The Ontology for Threat Intelligence is an ontology that implements the widely adopted intrusion kill chain of Lockheed-Martin [21], capable of describing seven attack stages, each of which is associated with controls through which intrusion can be disrupted, namely reconnaissance, weaponization, delivery, exploit, install, command and control, and actions on target [22]. The ontology is designed to help threat intelligence analysts effectively organize and search open source intelligence and threat indicators, thereby acquiring a better understanding of the threat envi- ronment. The Ontology for Threat Intelligence can potentially be used to describe various attacks, such as distributed denial of service  DDoS  attacks, which generate an incoming trafﬁc that ﬂoods the victim from multiple unique IP addresses, and industrial control system attacks, which may result in power outages in the energy sector.  1.5.4 The Ontology for Digital Forensics  The Ontology for Digital Forensics in IT Security Incidents deﬁnes all forensically relevant parts of computers for a particular point in time [23]. Each forensic object is associated with a timestamp and the forensic tool by which it was retrieved. In terms of hardware, these include concepts such as memory, HDD, network interface card  NIC . As for software, the ontology deﬁnes the kernel, resources, and process list concepts, which are used in relation deﬁnitions such as hasResource and processlist. The user concept is characterized by properties such as name and password, and one or more groups the user belongs to. The ontology also deﬁnes the process concept, which is associated with every program, and all processes are listed in the process list. By deﬁnition, each of them, except the initial one, has at least one thread and one parent process. Networking terms are deﬁned as NIC  35http:  resources.sei.cmu.edu asset_ﬁles TechnicalReport 2016_005_112_465537.owl   1 OWL Ontologies in Cybersecurity …  11  conﬁgurations, which include IP, gateway, and name server. The connections, which are wrapped by sockets, feature local and remote IP addresses. The port and protocol deﬁned for sockets can be used by processes to reference resources. The registry concept contain hives, each of which has a root key. Each key has a name and a state that represent the ﬂag where the key can be found. Keys may have optional subkeys and values. Values contain key-value pairs, a value type, and a state. For ﬁle systems, the ontology deﬁnes ﬁle system categories, data unit categories, ﬁle name categories, metadata categories, and application categories. The Memory concept has ﬁve subclasses: memory system architecture, metadata, metacode, data, and code. Metadata has more speciﬁc subclasses, namely, memory organization metadata and runtime organization metadata. The subclasses of the data concept are OS-speciﬁc data and application data, and the subclasses of the code concept are OS-speciﬁc code and application code.  1.5.5 Ontologies for Secure Operations and Processes  Oltramari et al. created a formally grounded ontology for secure operations [24]. Among other things, it can be used to describe the task of retrieving ﬁles securely and detect intrusions. It features concepts such as cyber-operation, cyber-operator, mission plan, cyber-exploitation, payload, etc., and the relationships between these concepts.  Maines et al. created an ontology for the cybersecurity requirements of busi- ness processes [25]. It deﬁnes concepts at four levels, the highest of which contains the key concepts: privacy, access control, availability, integrity, accountability, and attack harm detection and prevention. Privacy has two direct subclasses: conﬁden- tiality and user consent. Access control deﬁnes subclasses such as authentication, identiﬁcation, and authorization. Availability includes service, data, personnel, and hardware backups. Integrity covers data, hardware, personnel, and software integrity control. The accountability class has subclasses such as audit trail, digital forensics, and non-repudiation. Attack harm detection and prevention includes the following concepts: vulnerability assessment, honeypot, ﬁrewall, and intrusion detection and prevention system. These concepts are detailed further on the third and fourth level.  1.5.6 An Ontology for Describing Cyberattacks  and Their Impact  The Cyber Effects Simulation Ontology  CESO 36 is suitable for describing cyber- impacts in terms of not only qualitative outcomes, but also technical and human  36https:  github.com AustralianCentreforCyberSecurity Cyber-Simulation-Terrain blob master  v1-0 cyber ThreatSimulationOntology tso.ttl   12  L. F. Sikos  aspects [26]. It is aligned with STIX, and deﬁnes the classes campaign, course of action, exploit, exploit target, and threat actor, and their properties. The ontology was designed for modeling and simulating the effects of cyberattacks on organizations and military units, such as an artillery ﬁre mission in a land combat. To do so, it not only models virtual concepts, but also physical concepts and events.  1.6 Networking Ontologies for Cybersecurity  The formal representation of network infrastructures provides standardized data schemas, which may enable task automation via machine learning for vulnerabil- ity and exposure analysis [27]. These infrastructures include not only computers, but also Internet of Things  IoT  devices, which can be described efﬁciently using ontologies [28]. Ontology-based representations can also be used for infrastructure- less wireless networks, such as mobile ad hoc networks  MANET ,37 which can be described using the MANET Distributed Functions Ontology  MDFO  [29].  The ﬁrst network management ontologies date back to the early days of the Seman- tic Web  see [30], for example . The network ontology of De Paola et al. for computer network management deﬁnes concepts such as trafﬁc entity, event, database, man- agement tool, trafﬁc statistics, action, demand, actor, routing, abnormality, and net entity [31]. The Network Domain Ontology was designed for detailed network rep- resentations, where network devices are instances of network concepts, which can be described using properties such as MAC address, IP address, operating system, and the number of the ofﬁce in which the device is located [32].  The Network Ontology  NetOnto  is an ontology designed to describe exchanged BGP38 routes [33]. The OWL axioms of the ontology are complemented by BGP policies written as SWRL39 rules, which determine how routing information is shared between neighbors to control trafﬁc ﬂow across networks. These policies include con- textual information such as afﬁliations and route restrictions as well. Using NetOnto allows focusing on high-level policies and reconﬁgure networks automatically.  The ontology of Basile et al. deﬁnes concepts at three levels of granularity: ﬁrst-level  e.g., Switch, Router, Firewall , second-level  Workstation, Server,  e.g., SharedWorkstation, PublicServer , out of which the letter two can be automatically generated via reasoning over the ﬁrst [34].  PublicService,  third-level  concepts  etc. ,  and  The ﬁrewall ontology of Ghiran et al. deﬁnes concepts for ranges of ports, ranges of IP addresses, interfaces, ﬁrewall rules, protocols, and actions [35]. The ﬁrewall rules are further classiﬁed as correct rules, conﬂicting rules, and unclassiﬁed rules. Conﬂicting rules are deﬁned to be generalizations, redundant rules, shadowed rules,  37https:  tools.ietf.org html rfc2501 38Border Gateway Protocol. 39Semantic Web Rule Language.   1 OWL Ontologies in Cybersecurity …  13  or correlated rules. The ontology is best complemented by SWRL rules for advanced reasoning over ﬁrewall policies.  The SCADA Ontology of the INSPIRE project deﬁnes asset, vulnerability, treat, attack source, and safeguard concepts based on the now-withdrawn ISO IEC 13335- 1:2004 standard,40 and the attributes, relations, and interdependencies of these con- cepts [36]. It captures the semantics of the connections and dependencies among SCADA systems and their security aspects.  The Physical Devices Ontology deﬁnes concepts such as device, card, conﬁgura- tion, and slot, along with their object and datatype properties [37]. It can be used to describe devices, such as routers, and their conﬁguration.  The Measurement Ontology for IP Trafﬁc is an ETSI speciﬁcation for interfacing and data exchange with IP trafﬁc measurement devices [38]. It merges ontologies that deﬁne concepts such as connection, protocol, application and routing device, logistic and physical location, routing and queueing algorithm, and units for mea- suring network trafﬁc. It is aligned with standard information models for network measurements, such as IETF SNMP MIBs  RFC 3418 ,41 IPFIX  RFC 3955 ,42 and IPPM  RFC 6576 ,43 CAIDA DatCat,44 ITU M.3100,45 and DMTF CIM.46  The Ontology for the Router Conﬁguration Domain  ORCONF  can be used to describe the semantics of the command-line interface  CLI  of network routers [39]. More speciﬁcally, it is suitable for the description of all executable conﬁgu- ration statements of routers, i.e., valid sequences of commands and variables used in combination to represent a router conﬁguration operation  e.g., set → system → host_name→   . Other devices, such as switches, can be described similarly using the Ontology for Network Device Conﬁguration  ONDC , which can be used as part of a framework for populating vendor- and device-speciﬁc ontologies [40]. The probabilistic ontology of Laskey et al. was designed for large-scale IP address geolocation [41]. It integrates a factor graph model for IP geolocation with a domain ontology representing geolocation knowledge. Random variables in the factor graph model correspond to uncertain properties and relationships in the domain ontology. The model can be used to reason over arbitrary numbers of IP nodes, regions, and network topologies.  MonONTO is a domain ontology for network monitoring [42]. It deﬁnes concepts for the quality of service of advanced applications, network performance measure- ments, and user proﬁles. It can be used to monitor the performance of advanced Internet applications based on the use of an expert system. MonONTO can cap- ture the semantics of latencies, bandwidth throughput, link utilization, traceroutes, network losses, and delays. The developers of the ontology also created a set of  40https:  www.iso.org standard 39066.html 41https:  tools.ietf.org html rfc3418 42https:  www.ietf.org rfc rfc3955.txt 43https:  tools.ietf.org html rfc6576 44http:  www.datcat.org 45https:  www.itu.int rec T-REC-M.3100 en 46https:  www.dmtf.org standards cim   14  L. F. Sikos  inference rules that consist of network knowledge derived from the relationships between MonONTO concepts.  The IP Networks Topology and Communications Ontology47 is one of the most comprehensive ontologies that deﬁnes IPv4 and IPv6 network topology and con- nection concepts. It also deﬁnes custom datatypes for HTTP methods, IP addresses, protocols, and connection states.  The Communication Network Topology and Forwarding Ontology  CNTFO 48 is based on the Internet Protocol  IP  Ontology, the Open Shortest Path First  OSPF  Ontology, and the Border Gateway Protocol  BGP  Ontology [43]. It captures the semantics of routing information retrieved from OSPF links state advertisements and BGP update messages, router conﬁguration ﬁles, and open datasets. In addition, it covers fundamental administrative and networking concepts and their properties for describing autonomous systems, network topologies, and trafﬁc ﬂow. The prop- erty value ranges of the speciﬁc properties in the ontology are aligned with stan- dard deﬁnitions from IETF RFCs and deﬁned using restrictions on standard XSD datatypes. To make automatically generated data authoritative, the Communication Network Topology and Forwarding Ontology supports provenance data to be attached to cyber-knowledge both at the RDF triple and at the dataset level using standard PROV-O terms49 and custom, very speciﬁc provenance terms, such as importHost, importUser, and sourceType. These provenance-aware statements can be cap- tured using RDF quadruples, such as using the TriG50 serialization format [44].  1.7 Summary  Ontology engineering can provide solutions to cybersecurity challenges via support- ing automated data processing, which requires a common form of representation for expert cyber-knowledge. Considering the characteristics of RDF triples, using OWL ontologies for deﬁning the semantics of the concepts and roles used in RDF triples describing cyber-knowledge is trivial. This is witnessed by the proliferation of cyber- security ontologies ranging from upper ontologies to very speciﬁc domain ontologies, which can be used in combination to describe complex statements about IP infras- tructures, network topology, current network status, and potential cyberthreats alike. Based on these formal descriptions, automated tasks can be performed efﬁciently, and non-trivial, implicit statements can be made explicit, thereby facilitating knowledge discovery in the increasingly complex and highly dynamic cyberspace.  47https:  github.com twosixlabs icas-ontology blob master ontology ipnet.ttl 48http:  purl.org ontology network  49https:  www.w3.org TR prov-o  50https:  www.w3.org TR trig    1 OWL Ontologies in Cybersecurity …  15  References  1. Sikos LF  2015  Mastering structured data on the Semantic Web: from HTML5 Microdata to  Linked Open Data. Apress, New York. https:  doi.org 10.1007 978-1-4842-1049-9  2. Sikos LF  2017  Description logics in multimedia reasoning. Springer, Cham. https:  doi.org   10.1007 978-3-319-54066-5  3. Avizienis A, Laprie J-C, Randell B, Landwehr C  2004  Basic concepts and taxonomy of dependable and secure computing. IEEE Trans Depend Secur Comput 1 1 :11–33. https:  doi.org 10.1109 TDSC.2004.2  4. Hansman S, Hunt R  2005  A taxonomy of network and computer attacks. Comput Secur  24 1 :31–43. https:  doi.org 10.1016 j.cose.2004.06.011  5. Gao J, Zhang B, Chen X, Luo Z  2013  Ontology-based model of network and com- puter attacks for security assessment. J Shanghai Jiaotong Univ  Sci  18 5 :554–562. https:  doi.org 10.1007 s12204-013-1439-5  6. Burger EW, Goodman MD, Kampanakis P  2014  Taxonomy model for cyber threat intelligence information exchange technologies. In: Ahn G-J, Sander T  eds  Proceedings of the 2014 ACM Workshop on Information Sharing & Collaborative Security. ACM, New York, pp 51–60. https:  doi.org 10.1145 2663876.2663883  7. Takahashi T, Kadobayashi Y  2015  Reference ontology for cybersecurity operational infor-  mation. Comput J 58 10 :2297–2312. https:  doi.org 10.1093 comjnl bxu101  8. Tsoumas B, Papagiannakopoulos P, Dritsas S, Gritzalis D  2006  Security-by-ontology: a knowledge-centric approach. In: Fischer-Hübner S, Rannenberg K, Yngström L, Lind- skog S  eds  Security and privacy in dynamic environments. Springer, Boston, pp 99–110. https:  doi.org 10.1007 0-387-33406-8_9  9. Vorobiev A, Bekmamedova N  2007  An ontological approach applied to information security and trust. In: Cater-Steel A, Roberts L, Toleman M  eds  ACIS2007 Toowoomba 5–7 Decem- ber 2007: Delegate Handbook for the 18th Australasian Conference on Information Systems. University of Southern Queensland, Toowoomba, Australia. http:  aisel.aisnet.org acis2007  114   10. Fenz S, Ekelhart A  2009  Formalizing information security knowledge. In: Li W, Susilo W, Tupakula U, Safavi-Naini R, Varadharajan V  eds  Proceedings of the 4th International Symposium on Information, Computer, and Communications Security. ACM, New York, pp 183–194. https:  doi.org 10.1145 1533057.1533084  11. Stoneburner G, Goguen A, Feringa A  2002  Risk management guide for information technol- ogy systems. NIST Special Publication 800-30, National Institute of Standards and Technology  NIST , Gaithersburg, MD, USA  12. Wali A, Chun SA, Geller J  2013  A bootstrapping approach for developing a cyber-security ontology using textbook index terms. In: Guerrero JE  ed  Proceedings of the 2013 International Conference on Availability, Reliability, and Security. IEEE Computer Society, Washington, pp 569–576. https:  doi.org 10.1109 ARES.2013.75  13. Syed Z, Padia A, Mathews ML, Finin T, Joshi A  2016  UCO: a uniﬁed cybersecurity ontology. In: Wong W-K, Lowd D  eds  Proceedings of the Thirtieth AAAI Workshop on Artiﬁcial Intelligence for Cyber Security. AAAI Press, Palo Alto, CA, USA, pp 195–202. https:  www.aaai.org ocs index.php WS AAAIW16 paper download 12574 12365  14. He Y, Chen W, Yang M, Peng W  2004  Ontology-based cooperative intrusion detection system. In: Jin H, Gao GR, Xu Z, Chen H  eds  Network and parallel computing. Springer, Heidelberg, pp 419–426. https:  doi.org 10.1007 978-3-540-30141-7_59  15. Obrst L, Chase P, Markeloff R  2012  Developing an ontology of the cyber security domain. In: Costa PCG, Laskey KB  eds  Proceedings of the Seventh International Conference on Semantic Technologies for Intelligence, Defense, and Security. RWTH Aachen University, Aachen, pp 49–56. http:  ceur-ws.org Vol-966 STIDS2012_T06_ObrstEtAl_CyberOntology.pdf  16. Grégio A, Bonacin R, Nabuco O, Afonso VM, De Geus PL, Jino M  2014  Ontology for malware behavior: a core model proposal. In: Reddy SM  ed  Proceedings of the 2014 IEEE   16  L. F. Sikos  23rd International WETICE Conference. IEEE, New York, pp 453–458. https:  doi.org 10. 1109 WETICE.2014.72  17. Asgarli E, Burger E  2016  Semantic ontologies for cyber threat sharing standards. In: Pro- ceedings of the 2016 IEEE Symposium on Technologies for Homeland Security. IEEE, New York. https:  doi.org 10.1109 THS.2016.7568896  18. Ussath M, Jaeger D, Cheng F, Meinel C  2016  Pushing the limits of cyber threat intelligence: extending STIX to support complex patterns. In: Latiﬁ S  ed  Information technology: new generations. Springer, Cham, pp 213–225. https:  doi.org 10.1007 978-3-319-32467-8_20  19. Ekelhart A, Fenz S, Klemen M, Weippl E  2007  Security ontologies: improving quantita- tive risk analysis. In: Sprague RH  ed  Proceedings of the 40th Annual Hawaii International Conference on System Sciences. IEEE Computer Society, Los Alamitos, CA, USA. https:  doi.org 10.1109 HICSS.2007.478  20. Costa DL, Collins ML, Perl SJ, Albrethsen MJ, Silowash GJ, Spooner DL  2014  An ontology for insider threat indicators: development and applications. In: Laskey KB, Emmons I, Costa PCG  eds  Proceedings of the Ninth Conference on Semantic Technology for Intelligence, Defense, and Security. RWTH Aachen University, Aachen, pp 48–53. http:  ceur-ws.org Vol- 1304 STIDS2014_T07_CostaEtAl.pdf  21. Falk C  2016  An ontology for threat intelligence. In: Koch R, Rodosek G  eds  Proceedings of the 15th European Conference on Cyber Warfare and Security. Curran Associates, Red Hook, NY, USA  22. Hutchins EM, Cloppert MJ, Amin RM  2011  Intelligence-driven computer network defense informed by analysis of adversary campaigns and intrusion kill chains. In: Armistead EL  ed  Proceedings of the 6th International Conference on Information Warfare and Security. Academic Conferences and Publishing International, Sonning Common, UK, pp 113–125  23. Wolf JP  2013  An ontology for digital forensics in IT security incidents. M.Sc. thesis, Uni-  versity of Augsburg, Augsburg, Germany  24. Oltramari A, Cranor LF, Walls RJ, McDaniel P  2014  Building an ontology of cyber security. In: Laskey KB, Emmons I, Costa PCG  eds  Proceedings of the Ninth Conference on Semantic Technology for Intelligence, Defense, and Security. RWTH Aachen University, Aachen, pp 54–61. http:  ceur-ws.org Vol-1304 STIDS2014_T08_OltramariEtAl.pdf  25. Maines CL, Llewellyn-Jones D, Tang S, Zhou B  2015  A cyber security ontology for BPMN- security extensions. In: Wu Y, Min G, Georgalas N, Hu J, Atzori L, Jin X, Jarvis S, Liu L, Calvo RA  eds  Proceedings of the 2015 IEEE International Conference on Computer and Informa- tion Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing. IEEE, New York, pp 1756–1763. https:  doi.org 10.1109 CIT IUCC DASC PICOM.2015.265  26. Ormrod D, Turnbull B, O’Sullivan K  2015  System of systems cyber effects simulation ontol- ogy. In: Proceedings of the 2015 Winter Simulation Conference. IEEE, New York, pp 2475– 2486. https:  doi.org 10.1109 WSC.2015.7408358  27. Sicilia MA, García-Barriocanal E, Bermejo-Higuera J, Sánchez-Alonso S  2015  What are information security ontologies useful for? In: Garoufallou E, Hartley R, Gaitanou P  eds  Metadata and semantics research. Springer, Cham, pp 51–61. https:  doi.org 10.1007 978-3- 319-24129-6_5  28. Gaglio S, Lo Re G  eds   2014  Advances onto the Internet of Things: how ontologies make the Internet of Things meaningful. Springer, Cham. https:  doi.org 10.1007 978-3-319-03992-3 29. Orwat ME, Levin TE, Irvine CE  2008  An ontological approach to secure MANET manage- ment. In: Jakoubi S, Tjoa S, Weippl ER  eds  Proceedings of the Third International Conference on Availability, Reliability and Security. IEEE Computer Society, Los Alamitos, CA, USA, pp 787–794. https:  doi.org 10.1109 ARES.2008.183  30. De Vergara JEL, Villagra VA, Asensio JI, Berrocal J  2003  Ontologies: giving semantics to network management models. IEEE Netw 17 3 :15–21. https:  doi.org 10.1109 MNET.2003. 1201472  31. De Paola A, Gatani L, Lo Re G, Pizzitola A, Urso A  2003  A network ontology for computer network management. Technical report No 22. Institute for High Performance Computing and Networking, Palermo, Italy   1 OWL Ontologies in Cybersecurity …  17  32. Abar S, Iwaya Y, Abe T, Kinoshita T  2006  Exploiting domain ontologies and intelligent agents: an automated network management support paradigm. In: Chong I, Kawahara K  eds  Information networking. Advances in data communications and wireless networks. Springer, Heidelberg, pp 823–832. https:  doi.org 10.1007 11919568_82  33. Kodeswaran P, Kodeswaran SB, Joshi A, Perich F  2008  Utilizing semantic policies for man- aging BGP route dissemination. In: 2008 IEEE INFOCOM Workshops. IEEE, Piscataway, NJ, USA. https:  doi.org 10.1109 INFOCOM.2008.4544611  34. Basile C, Lioy A, Scozzi S, Vallini M  2009  Ontology-based policy translation. In: Herrero Á, Gastaldo P, Zunino R, Corchado E  eds  Computational intelligence in security for information systems. Springer, Heidelberg, pp 117–126. https:  doi.org 10.1007 978-3-642-04091-7_15 35. Ghiran AM, Silaghi GC, Tomai N  2009  Ontology-based tools for automating integration and validation of ﬁrewall rules. In: Abramowicz W  ed  Business information systems. Springer, Heidelberg, pp 37–48. https:  doi.org 10.1007 978-3-642-01190-0_4  36. Chora´s M, Flizikowski A, Kozik R, Hołubowicz W  2010  Decision aid tool and ontology-based reasoning for critical infrastructure vulnerabilities and threats analysis. In: Rome E, Bloom- ﬁeld R  eds  Critical information infrastructures security. Springer, Heidelberg, pp 98–110. https:  doi.org 10.1007 978-3-642-14379-3_9  37. Miksa K, Sabina P, Kasztelnik M  2010  Combining ontologies with domain speciﬁc languages: a case study from network conﬁguration software. In: Aßmann U, Bartho A, Wende C  eds  Reasoning web. Semantic technologies for software engineering. Springer, Heidelberg, pp 99–118. https:  doi.org 10.1007 978-3-642-15543-7_4  38. ETSI Industry Speciﬁcation Group  2013  Measurement ontology for IP trafﬁc  MOI ; require- ments for IP trafﬁc measurement ontologies development. ETSI, Valbonne. http:  www.etsi. org deliver etsi_gs MOI 001_099 003 01.01.01_60 gs_moi003v010101p.pdf  39. Martínez A, Yannuzzi M, Serral-Gracià R, Ramírez W  2014  Ontology-based information extraction from the conﬁguration command line of network routers. In: Prasath R, O’Reilly P, Kathirvalavakumar T  eds  Mining intelligence and knowledge exploration. Springer, Cham, pp 312–322. https:  doi.org 10.1007 978-3-319-13817-6_30  40. Martínez A, Yannuzzi M, López J, Serral-Gracià R, Ramírez W  2015  Applying information extraction for abstracting and automating CLI-based conﬁguration of network devices in het- erogeneous environments. In: Laalaoui Y, Bouguila N  eds  Artiﬁcial intelligence applications in information and communication technologies. Springer, Cham, pp 167–193. https:  doi.org  10.1007 978-3-319-19833-0_8  41. Laskey K, Chandekar S, Paris B-P  2015  A probabilistic ontology for large-scale IP geoloca- tion. In: Laskey KB, Emmons I, Costa PCG, Oltramari A  eds  Tenth Conference on Semantic Technology for Intelligence, Defense, and Security. RWTH Aachen University, Aachen, pp 18–25. http:  ceur-ws.org Vol-1523 STIDS_2015_T03_Laskey_etal.pdf  42. Moraes PS, Sampaio LN, Monteiro JAS, Portnoi M  2008  MonONTO: a domain ontology for network monitoring and recommendation for advanced Internet applications users. In: 2008 IEEE Network Operations and Management Symposium Workshops–NOMS 2008. IEEE, Piscataway, NJ, USA. https:  doi.org 10.1109 NOMSW.2007.21  43. Sikos LF, Stumptner M, Mayer W, Howard C, Voigt S, Philp D  2018  Representing net- work knowledge using provenance-aware formalisms for cyber-situational awareness. Procedia Comput Sci 126C: 29–38  44. Sikos LF, Stumptner M, Mayer W, Howard C, Voigt S, Philp D  2018  Automated reason- ing over provenance-aware communication network knowledge in support of cyber-situational awareness. In: Liu W, Giunchiglia F, Yang B  eds  Knowledge science, engineering and man- agement. Springer, Cham., pp. 132–143. https:  doi.org 10.1007 978-3-319-99247-1_12   Chapter 2 Knowledge Representation of Network Semantics for Reasoning-Powered Cyber-Situational Awareness  Leslie F. Sikos, Dean Philp, Catherine Howard, Shaun Voigt, Markus Stumptner, and Wolfgang Mayer  Abstract For network analysts, understanding how network devices are intercon- nected and how information ﬂows around the network is crucial to the cyber- situational awareness required for applications such as proactive network security monitoring. Many heterogeneous data sources are useful for these applications, including router conﬁguration ﬁles, routing messages, and open datasets. However, these datasets have interoperability issues, which can be overcome by using for- mal knowledge representation techniques for network semantics. Formal knowledge representation also enables automated reasoning over statements about network con- cepts, properties, entities, and relationships, thereby enabling knowledge discovery. This chapter describes formal knowledge representation formalisms to capture the semantics of communication network concepts, their properties, and the relationships between them, in addition to metadata such as data provenance. It also describes how the expressivity of these knowledge representation mechanisms can be increased to represent uncertainty and vagueness.  2.1 Introduction  Proactive network security monitoring highly depends on accurate, concise, quality network data. Traditional network vulnerability checking relies on labor-intensive processes that require expertise and are prone to errors.  While hackers’ attempts to compromise critical infrastructures are inevitable, intelligent systems provide mechanisms to reduce the impact of cyberattacks, and whenever possible, prevent attacks and manage vulnerability risks via real-time cyber-situational awareness. Transforming raw data into valuable information based L. F. Sikos  B  · M. Stumptner · W. Mayer  University of South Australia, Adelaide, Australia e-mail: leslie.sikos@unisa.edu.au  D. Philp · C. Howard · S. Voigt  B   Defence Science and Technology Group, Department of Defence, Australian Government, Adelaide, Australia e-mail: shaun.voigt@dst.defence.gov.au    Springer Nature Switzerland AG 2019 L. F. Sikos  ed. , AI in Cybersecurity, Intelligent Systems Reference Library 151, https:  doi.org 10.1007 978-3-319-98842-9_2  19   20  L. F. Sikos et al.  on the semantics of network devices and the trafﬁc ﬂow between them cannot rely solely on manual methods because of the data volume and diversity that char- acterize communication networks. Network analysts can beneﬁt from automated reasoning-based frameworks to gain improved cyber-situational awareness of the highly dynamic cyberspace [1]. Developing such frameworks requires data integra- tion via syntactic and semantic interoperability, which can be achieved by formal knowledge representation standards, such as the Resource Description Framework  RDF 1 [2].  This chapter describes formal knowledge representation formalisms to capture the semantics of communication network concepts, their properties, and the relation- ships between them. After introducing some basic communication network concepts  Sect. 2.3 , a detailed discussion explains how to represent these  Sect. 2.4 , along with metadata  Sect. 2.5 , including data provenance, and how to increase expressiv- ity to be able to represent uncertainty and vagueness  Sects. 2.6 and 2.7 .  2.2 Preliminaries  One technique to enable automated data processing for communication networks is to provide syntactic and semantic interoperability, which can be achieved through formal knowledge representation. Formal knowledge representation is a ﬁeld of artiﬁ- cial intelligence  AI , which is dedicated to representing information about a selected part of the world, called the knowledge domain  ﬁeld of interest or area of concern , in a form that allows software agents to solve complex tasks. Formally represented information can not only be indexed and queried very efﬁciently, but also enable reasoning algorithms to infer new statements, thereby facilitating knowledge discov- ery. From the representation perspective, there are two types of network data: expert knowledge and statements about entities  individuals  of real-world networks.  Expert knowledge represents networking concepts and relationships that can be codiﬁed as terminological deﬁnitions in ontologies as background knowledge, thereby describing ground truth about communication networks, such as IP addresses should not be reused within a public network. Ontology-based representations use a common data format that enables the information fusion needed to maximize semantic enrichment [3]. Owing to the complexity of communication networks, there is a variety of network ontologies, all of which capture the semantics of dif- ferent aspects of networking. Some notable network ontologies include the Physical Devices Ontology [4], the Network Domain Ontology [5], the Ontology for Network Device Conﬁguration  ONDC  [6], the Common Information Model Ontology  CIM  [7], the Ontology for the Router Conﬁguration Domain  ORCONF  [8], a probabilis- tic ontology for large-scale IP geolocation [9], the Measurement Ontology for IP Trafﬁc  MOI  [10], and the Network Ontology  NetOnto  [11]. However, not all net- work ontologies are suitable for representing network topology and network trafﬁc  1https:  www.w3.org RDF    2 Knowledge Representation of Network Semantics …  21  ﬂow because of their scope, conceptualization, and intended application [12]. This led to the development of the Communication Network Topology and Forwarding Ontology  CNTFO 2 [13].  Entities of real-world networks, which can be used to describe a particular network, can be characterized by properties that are either declared manually or extracted by software agents from routing messages3 sent between network devices, conﬁguration ﬁles of network devices, etc. Many data sources utilized for cyber-situational awareness, such as traceroute responses, routing messages, and router conﬁguration ﬁles, constitute unstructured data, which is often stored in proprietary formats. The corresponding information is human-readable only, and only experts can comprehend it. For example, software agents cannot efﬁ- ciently process the natural sentence the “IP address of ServerX is 103.254.136.21,” because to software agents this statement would be just a meaningless string. In contrast, the same statement written in a semistructured data format, such as XML,4 is machine-readable, because the name of the device and the IP address are annotated with separate tags  e.g.,  ServerX  and  103.254.136.21  , which allows software agents to extract the corresponding data. However, the semantics  meaning  of these properties are still not deﬁned. To overcome this limitation, the statement can be written as structured data using Semantic Web standards, such as RDF, and extended with annotations from knowledge organization systems  KOS , in particular controlled vocabularies, which collect terms, properties, relationships, and entities of a knowledge domain, and ontologies, which are formal conceptualizations of a knowledge domain with complex classes, relationships, and rules suitable for inferring new statements. By doing so, the data becomes machine-interpretable, unambigious, and interoperable, allowing a much wider range of automated tasks than unstructured or semistructured data. Structured data written according to best practices for publishing structured data is called Linked Data. Linked Data is often used to complement concept deﬁni- tions of ontologies to further detail the semantics of a knowledge representation and interlink related resources [14]. The Linked Data principles include dereferencable URIs for every resource over the HTTP protocol, knowledge representation using open standards, and links to related resources [15]. If published with an open license  e.g., PDDL,5 ODC-By,6 ODC-ODbL,7 CC0 1.0,8 CC-BY-SA,9 GFDL ,10 Linked Data becomes Linked Open Data  LOD . Those datasets that contain Linked Open Data are called LOD datasets.  2http:  purl.org ontology network  3Routing is the process of selecting network paths to carry network trafﬁc. 4https:  www.w3.org TR xml11  5https:  opendatacommons.org licenses pddl  6https:  opendatacommons.org licenses by  7https:  opendatacommons.org licenses odbl  8https:  creativecommons.org publicdomain zero 1.0  9https:  creativecommons.org licenses by-sa 4.0  10https:  www.gnu.org copyleft fdl.html   22  L. F. Sikos et al.  RDF allows machine-interpretable statements of the form subject-predicate-  object  see Deﬁnition 2.1 .  Deﬁnition 2.1  RDF Triple  Let S be a set of data sources, which is a subset of the set of International Resource Identiﬁers  IRIs   I , i.e., sets of strings of Unicode charac- ters of the form scheme:[  [user:password@]host[:port][ ]path [?query][fragment], formally S ⊂ I. Assume there are pairwise disjoint inﬁnite sets of  1. IRIs; 2. RDF literals  L , which are either  b.  a. self-denoting plain literals LP of the form " " @  , where   is a string and   is an optional language tag; or typed literals LT of the form " "^^ , where   is an IRI denoting a datatype according to a schema, such as the XML Schema, and   is an element of the lexical space corresponding to the datatype; and  3. blank nodes  B , i.e., unique but anonymous resources that are neither IRIs nor RDF literals. A triple  s, p, o  ∈  I ∪ B  × I ×  I ∪ L ∪ B  is called an RDF triple  or RDF  statement , where s is the subject, p is the predicate, and o is the object.  For example, the fact that routers are networking devices can be expressed as  shown in Example 2.1.  Example 2.1 A triple  Router–isA–NetworkingDevice To declare the deﬁnition of each triple element, use the deﬁnition of “Router” from DBpedia, the “isA” relationship from the RDF vocabulary  rdfV  and the deﬁnition of networking devices from DBpedia. This leads to the following RDF triple:   Subject: http:  dbpedia.org resource Router_ computing    Predicate: http:  www.w3.org 1999 02 22-rdf-syntax-nstype   Object: http:  dbpedia.org resource Networking_ device  Long and frequently used URLs can be abbreviated using the namespace mech- anism, which deﬁnes a preﬁx URL that can be concatenated with the triple element to obtain the full URL. Because RDF supports a wide range of serialization formats, RDF triples can be represented in a variety of ways. Using the Turtle serialization,11 for example, the previous triple would look like Listing 2.1.  11https:  www.w3.org TR turtle    2 Knowledge Representation of Network Semantics …  23  Listing 2.1 An RDF triple @prefix dbpedia:   . @prefix rdf:   .  dbpedia:Router_ computing  rdf:type dbpedia:Networking_device .  A set of RDF triples can be visualized as a directed, labeled graph, called an RDF graph, in which the set of nodes corresponds to the set of subjects and objects of RDF triples in the graph, and the edges represent the predicate relationships that hold between the subjects and the objects.  RDF triples complemented by context form RDF quadruples  see Deﬁnition 2.2 .  Deﬁnition 2.2  RDF Quadruple  A 4-tuple of the form  s, p, o, c , where s, p, o represents an RDF triple and c identiﬁes the context of the triple, is called an RDF quadruple  RDF quad .  For example, :host18 :hasInterface :I10.1.8.1 :traceroute101. is an RDF quad, which describes that host18 has the interface address 10.1.8.1, according to traceroute 101.  The context of each triple can identify the RDF graph to which the triple belongs   see Deﬁnition 2.3 .  Deﬁnition 2.3  Named Graph  If the context is an IRI that identiﬁes the name of the RDF graph the triple belongs to, an RDF dataset D can be deﬁned as a set of RDF graphs of the form g0, n1, g1,…, nm, gm, where each gi is a graph and g0 is the default graph of D. Each optional pair ni, gi is called a named graph, in which n1, …, nm ∈ I are unique graph names in D [16].  To achieve a favorable trade-off between expressivity and reasoning complexity, and to ensure decidability,12 knowledge representations can be formally grounded in description logics  DL  [17]. DL formalisms are typically implemented in RDF statements that utilize terms from ontologies written in the Web Ontology Language  OWL .13 The expressivity of each description logic is determined by the supported mathematical constructors, which is reﬂected by the letters in the DL names. For example, the core description logic ALC  Attributive Language with Complements  allows atomic and complex concept negation, concept intersection, universal restric- tions, and limited existential quantiﬁcation. The extension of ALC with transitive roles is S. S extended with role hierarchies  H , inverse roles  I , functional prop- erties  F , and datatypes  D  is called SHIF  D , which roughly corresponds to OWL Lite. Adding nominals  O  and cardinality restrictions  N   to SHIF  D  leads to SHOIN  D , the description logic behind OWL DL. SROIQ D  is the logical  12The decidability of a formalism ensures that an inference algorithm will not run into an inﬁnite loop. 13https:  www.w3.org OWL    24  L. F. Sikos et al.  underpinning of OWL 2 DL, which, beyond theSHOIN  D  constructors, allows the union of a ﬁnite set of complex role inclusions and a role hierarchy  R , and replaces the unqualiﬁed cardinality restrictions  N   with qualiﬁed cardinality restrictions  Q . OWL 2 DL-compliant knowledge bases, with a logical underpinning in the SROIQ D  description logic, are based on three ﬁnite and pairwise disjoint sets of atomic concepts  NC , atomic roles  NR , and individual names  NI  , where  NC ,  NR , and  NI   ∈ I. They may implement a wide range of constructors in concept expressions, including concept names, concept intersection, concept union, comple- ment, tautology, contradiction, existential and universal quantiﬁers, qualiﬁed at-least and at-most restrictions, local reﬂexivity, and individual names  see Deﬁnition 2.4 . Deﬁnition 2.4  SROIQ Concept Expression  The set of constructors allowed in SROIQ concept expressions can be deﬁned as C:: = NC   C  cid:5  D   C  cid:6  D ¬C cid:7  ⊥∃R.C∀R.C  cid:2  nR.C  cid:3  nR.C∃R.Self NI, where C represents concepts, R represents roles, and n is a nonnegative integer.  The role expressions support the universal role, atomic roles, and negated atomic  roles  see Deﬁnition 2.5 . Deﬁnition 2.5  SROIQ Role Expression  The permissible constructors of SROIQ role expressions can be deﬁned as R:: = U NR  NR  −.  Based on these sets, each axiom is either a concept inclusion, an individual asser-  tion, or a role assertion  see Deﬁnition 2.6 .  Deﬁnition 2.6  SROIQ Axiom  SROIQ axioms can be classiﬁed as follows:   general concept inclusions of the form C  cid:11  D and C ≡ D for concepts C and D  terminological knowledge, TBox ;   individual assertions of the form C a , R a, b , ¬R a, b , a ≈ b, a  cid:14 ≈ b, where a, b ∈ NI denotes individual names, C ∈ C denotes concept expressions, and R ∈ R denotes roles  assertional knowledge, ABox ;   role assertions of the form R  cid:11  S, R1 ◦ . . .◦ n  cid:11  S, Asymmetric R , Reﬂexive R , Irreﬂexive R , or Disjoint R, S  for roles R, Ri , and S  role box, RBox . The meaning of formally described network concepts and roles is deﬁned by their model-theoretic semantics, which are based on interpretations. These interpretations consist of a domain of discourse,  cid:2 , consisting of two disjoint sets and an interpreta- tion function. An object domain  cid:2 I covers speciﬁc abstract objects  individuals  and classes of abstract objects  concepts  as well as abstract roles. A concrete domain D is a pair   cid:2 D,  cid:3 D , where  cid:2 D is a set called the datatype domain, and  cid:3 D is a set of concrete predicates. Each predicate name P from  cid:3 D is associated with an arity n, and an n-ary predicate P D ⊆  cid:2 nD. Concrete domains integrate description logics with concrete sets, such as natural numbers  N , integers  Z , real numbers  R , complex numbers  C , strings, Boolean values, and date and time values, along with concrete roles deﬁned on these sets, including numerical comparisons, string   2 Knowledge Representation of Network Semantics …  25  , .I  comparisons, and comparisons with constants. A Tarski-style DL interpretation I   utilizes a concept interpretation function .IC to map concepts into sub- =   cid:2 I sets of the object domain, a role interpretation function .IR to map object roles into subsets of  cid:2 I ×  cid:2 I and datatype roles into subsets of  cid:2 I ×  cid:2 D, and an individual interpretation function .II to map individuals into elements of the object domain.  2.3 Communication Network Concepts  The following sections brieﬂy introduce some communication networking concepts and discuss how the semantics of these concepts and their properties and relation- ships can be captured using formal knowledge representation formalism. While core networking concepts can be described using expert knowledge, as evidenced by a large number of networking ontologies, the deﬁnition of networking properties is far less trivial. Firstly, not all properties are important for understanding how network devices are interconnected or how information ﬂows around the network; the rele- vant properties must be carefully selected. Secondly, not all networking properties are standardized, and even the ones that are may have proprietary implementations, as seen in the implementations of routing protocols in Cisco and Juniper routers. Thirdly, the deﬁnition of complex network properties requires custom datatypes, not all of which can be deﬁned by restricting standardized datatypes, and some of which are context-dependent. The following sections discuss some of the interdependencies and properties of core networking concepts from the formal representation point of view.  2.3.1 Networks and Topologies  Communication networks establish connections between network nodes to exchange data via cable or wireless media. These network nodes are either hosts, such as per- sonal computers, workstations, and servers, or network devices, such as modems, routers, switches, repeaters, hubs, bridges, gateways, VPN appliances, and ﬁrewalls. In knowledge representation, these network nodes can be described by concept deﬁ- nitions, which can be used in arbitrary statements. In a network, if multiple instances of the same node type are present, individual names typically contain consecutive numbers after the node type, such as Router1 and Router2. If the name of the actual network device is known, such as the name used in routing messages, the individual name will be identical to that name, e.g., Router10143R1.  The topology of a communication network is the arrangement of the various net- work elements, such as routers, computers, and links, within the network. There are two basic categories of computer network topologies, physical topologies and logical topologies. The physical topology of a network is the arrangement of the physical components of the network, including the location of devices and cables, while the   26  L. F. Sikos et al.  logical network topologies show how network devices logically communicate with each other via a communication protocol. Network topologies can be described for- mally by the nodes and the relationships between them, as for example, by using the connectedTo predicate between directly connected devices. Each node in a representation must have a unique identiﬁer, such as a public IP address or a name consisting of the node type and a number. Sophisticated statements about complex networks typically need a combination of background knowledge and individual assertions.  2.3.2 Network Interfaces and IP Addressing  The Internet Protocol  IP  is the fundamental network routing protocol used in communication networks. To participate in an IP network, each network element must have a properly conﬁgured interface to the IP network. A properly conﬁgured interface has two main parameters: an IP address and a subnet mask. IP addresses are 32 bits long, and are unique within the whole IP network. To make IP addresses and subnet masks easy to read, they are often written in dotted decimal format [26]  see Fig. 2.1 .  The subnet mask deﬁnes the Network ID and Host ID portions of an IP address. Property-value pairs can be used in formal representations to describe network interfaces. Network interfaces from different network elements are joined together into sub-networks  subnets . Different subnets are joined together to form a larger IP network. A network element that joins one or more subnets together is called a router.  When a router receives a packet that is not destined for one of its own interfaces, it uses a routing table to determine to which interface to forward that packet towards its destination. For each reachable destination, a routing table lists which connected  2.3.3 Routers  Fig. 2.1 IPv4 address and subnet mask   2 Knowledge Representation of Network Semantics …  27  network element is next along the path to the destination. When an IP packet arrives, a router uses this table to determine the interface14 on which to forward the packet based on its destination IP address.15 The next routers then repeat this process using their own routing table until the packet reaches its destination. For an IP network to function, the routing tables on all of the network elements need to be conﬁgured in a consistent manner with routing table entries that forward packets from any network element to any other network element.  Each router is conﬁgured using a router conﬁguration ﬁle. These ﬁles hold all the commands and parameters required to conﬁgure the router [18]. To demonstrate the types of data that can be used from router conﬁgurations to gain cyber-situational awareness, consider the fragment of a Cisco router conﬁguration ﬁle shown in Listing 2.2.  Listing 2.2 Part of a router conﬁguration ﬁle [19] no keepalive no cable proxy-arp cable helper-address 10.100.0.30 interface FastEthernet0 0 ip address 10.100.0.14 255.255.255.0 no ip directed-broadcast no ip mroute-cache !! interface Ethernet2 3 ip address 10.145.30.22 255.255.255.0 no ip directed-broadcast ! interface Cable5 0 ip address 172.1.71.1 255.255.255.0 secondary ip address 10.100.1.1 255.255.252.0 no ip directed-broadcast no ip route-cache no ip mroute-cache cable downstream annex B cable downstream modulation 64qam cable downstream interleave-depth 32 cable upstream 0 spectrum-group 1 cable upstream 0 modulation-profile 3 cable downstream frequency 531000000 cable upstream 0 frequency 28000000 cable upstream 0 power-level 0 no cable upstream 0 shutdown cable upstream 1 shutdown cable upstream 2 shutdown cable upstream 3 shutdown  14There is usually one-to-one mapping between interfaces and IP addresses. 15Within a subnet, each IP address is assumed to be unique.   28  L. F. Sikos et al.  2.3.4 Autonomous Systems and Routing  An autonomous system  AS  is a network, or collection of networks, managed and supervised by a single administrative entity or organization. Such an administrative entity can be an Internet service provider, an enterprise, a university, a company division, or a group of organizations. For the purpose of routing, each AS is assigned a globally unique AS number, called the Autonomous System Number  ASN , assigned by the Regional Internet Registry  RIR  of the country of registration  e.g., APNIC16 in the Asia Paciﬁc . For example, ASN 3356 identiﬁes Level 3 Communications, Inc., ASN 1299 corresponds to Telia Company AB, and ASN 174 represents Cogent Communications. ASNs can be used in individual names to make statements about ASes and their relationships with organizations, network devices, and network events. Different protocols are used to route trafﬁc between and within ASes. The routing of trafﬁc between ASes  which is referred to as inter-AS routing  is managed by Exte- rior Gateways Protocols  EGPs , such as BGP [20]. BGP is used to facilitate inter-AS relationships by exchanging routing and reachability information among ASes on the Internet. BGP is commonly classiﬁed as a path vector routing protocol. Autonomous system boundary routers  ASBRs  advertise network reachability information  that is, the networks which are reachable via each of their neighbors and how many hops away each network is . When a BGP session is initialized between routers, messages are sent to exchange routing information until the complete BGP routing table has been exchanged. BGP makes routing decisions based on AS paths, network policies, and rulesets conﬁgured by the network administrators. A BGP peer group is a set of BGP neighbors that shares the same outbound routing policy  although the inbound policies may differ .  The routing of trafﬁc within an individual AS  which is referred to as intra-AS routing  is managed by an Interior Gateway Protocol  IGP , such as the OSPF proto- col [21], the Intermediate System to Intermediate System  IS-IS  protocol [22], the Routing Information Protocol  RIP  [23] and the Enhanced Interior Gateway Rout- ing Protocol  EIGRP . Within an AS, multiple IGPs can be used simultaneously and multiple instances of the same IGP can be used on different network segments. Rout- ing tables can be conﬁgured manually or populated using dynamic routing protocols such as BGP, OSPF, IS-IS, and RIP.  OSPF, which is a link state routing protocol,17 is the most widely used interior gateway protocol on the Internet [24]. Each link has an associated cost metric, where the cost is often determined by factors such as the speed of the router’s interface, data throughput, link availability, and link reliability. Each participating router com- municates its own link states to every other participating router such that each router has an identical view of the entire OSPF network. Using the topology constructed from the advertised link state information, and Dijkstra’s shortest path ﬁrst algo- rithm [25], each router computes the shortest path tree for each destination network.  16https:  www.apnic.net 17In a link state routing protocol, each router constructs a map of the connectivity of the network in which it resides.   2 Knowledge Representation of Network Semantics …  29  This tree is then used to create the routing table, which is then used to determine the interface on which to forward the individual incoming packets. Because link state routing protocols, like OSPF, maintain information about the complete topology, this information can be exploited to support cyber-situational awareness [27].  Each OSPF router within a network is assigned a Router Identiﬁcation Number. This number, which is hereafter referred to as routerID, uniquely identiﬁes the router within a routing domain.18 The routerID is displayed using the dot-decimal notation, for example, 10.0.2.2. OSPF networks are often subdivided into OSPF areas to simplify administration or optimize trafﬁc ﬂow or resource utilization. These routing areas are identiﬁed by 32-bit numbers, which is expressed either in decimal or the octet-based dot-decimal notation typical to IPv4 addresses. The core or backbone area of an OSPF network is Area 0 or 0.0.0.0.  Routers can be classiﬁed according to their location and role in the network. The OSPF protocol itself is conﬁgured on a router’s interface, rather than the router as a whole. Those routers that have interfaces in multiple areas are called area border routers  ABRs . Routers that have an interface in the backbone area are called back- bone routers. Routers that have interfaces in only one area are called internal routers. Routers that exchange routing information with routing devices in non-OSPF net- works are called autonomous system boundary routers  ASBRs . ASBRs advertise externally learned routes throughout the OSPF AS. Depending on the location of an ASBR in the network, it can be an ABR, a backbone router, or an internal router. These different router types can be deﬁned as classes and used with the rdf:type predicate to make statements about the router type. Link State Advertisements  LSAs  are the basic communication mechanism of the OSPF routing protocol. There are eleven different types of LSAs. Some of the information included in these LSAs is useful for cyber-situational awareness.  2.4 Formal Knowledge Representation  for Cyber-Situational Awareness  The following sections demonstrate the formal representation of the communication network concepts introduced in the previous section.  18In computer networking, a routing domain is a collection of networked systems that operate common routing protocols and are under the control of a single administrative entity. A given AS may contain multiple routing domains. A routing domain can exist without being an Internet- participating AS.   30  L. F. Sikos et al.  2.4.1 Representing Network Knowledge Using Ontology  Deﬁnitions  To demonstrate how core networking concepts and properties can be deﬁned formally in ontologies, take a look at the description logic deﬁnition of the router conﬁguration properties shown in Listing 2.3.  Listing 2.3 Formal deﬁnition of some router conﬁguration terms with description logic axioms Area Interface Network NetworkElement OSPFSummaryRouteEntry Router Router  cid:11  NetworkElement RouteEntry connectedTo ∃connectedTo. cid:7   cid:11  Interface  cid:7   cid:11  ∀connectedTo.Network  cid:7   cid:11  cid:3 1areaId. cid:7  ∃areaId. cid:7   cid:11  Area  cid:7   cid:11  ∀areaId.unsignedInt ∃ip. cid:7   cid:11  Interface ipv4  cid:11  ip Dis ipv4, areaId  Dis ipv4, routerId  ∃ipv4. cid:7   cid:11  Interface  cid:7   cid:11  ∀ipv4.ipv4Type ∃routerId. cid:7   cid:11  RouteEntry  cid:6  Router  cid:6  OSPFSummaryRouteEntry  cid:7   cid:11  ∀routerId.ipv4Type ipv4Type ≡ string["  [0−1]?[0−9]?[0−9]2 [0−4][0−9]5[0−5]    \. {3} [0−1]?[0−9]?[0−9]2 [0−4][0−9]5[0−5]  "] These deﬁnitions have been implemented in CNTFO as shown in Listing 2.4.  Listing 2.4 Some router conﬁguration concepts and properties deﬁned in RDF  Turtle syntax  @prefix net:   . @prefix rdf:   . @prefix rdfs:   . @prefix owl:   . @prefix xsd:   .  :Area a owl:Class . :Interface a owl:Class . :Network a owl:Class . :NetworkElement a owl:Class . :OSPFSummaryRouteEntry a owl:Class . :Router a owl:Class ; rdfs:subclassOf :NetworkElement . :RouteEntry a owl:Class .  :connectedTo a owl:ObjectProperty ; rdfs:domain :Interface ;  rdfs:range :Network .   2 Knowledge Representation of Network Semantics …  31  :areaId a owl:DatatypeProperty , owl:FunctionalProperty ; rdfs:domain :Area ; rdfs:range xsd:unsignedInt . :ip a owl:DatatypeProperty ; rdfs:domain :Interface . :ipv4 a owl:DatatypeProperty ; rdfs:subPropertyOf :ip ; owl:propertyDisjointWith :areaId , :routerId ; rdfs:domain :Interface ; rdfs:range :ipv4Type . :routerId a owl:DatatypeProperty ; rdfs:domain [ a owl:Class ; owl:unionOf  :RouteEntry :Router :OSPFSummaryRouteEntry  ] ; rdfs:range net:ipv4Type .  :ipv4Type a rdfs:Datatype ; owl:onDatatype xsd:string ; owl:withRestrictions   [xsd:pattern  "  [0-1]?[0-9]?[0-9]2 [0-4][0-9]5[0-5]  \. {3}  [0-1]?[0-9]?[0-9]2 [0-4][0-9]5[0-5]  "]   .  These deﬁnitions can be used in arbitrary RDF statements, in which the domain, range, and datatype restrictions apply accordingly. Assume the fragment of a router conﬁguration ﬁle shown in Listing 2.5.  Listing 2.5 Some properties with their values in a router conﬁguration ﬁle ---- Router configuration file for RE20 hostname "RE20" interface FastEthernet0 0 ip address 10.100.0.13 255.255.255.252 router ospf 1 router-id 10.100.0.13  ---- Router configuration file for RE37 hostname "RE37" interface FastEthernet0 0 ip address 10.100.0.14 255.255.255.252 router ospf 1 router-id 10.100.0.14  These can be written in a description logic formalism as shown in Listing 2.6.  Listing 2.6 Formal description of Listing 2.5 with description logic axioms Router RE20  RE20  cid:5  ∃hostname.{RE20} RE20  cid:5  ∃routerId.{10.100.0.13} Interface RE20_FastEthernet0_0  RE20_FastEthernet0_0  cid:5  ∃ipv4.{10.100.0.13} Router RE37  RE20  cid:5  ∃hostname.{RE37} RE37  cid:5  ∃routerId.{10.100.0.14} Interface RE37_FastEthernet0_0  RE37_FastEthernet0_0  cid:5  ∃ipv4.{10.100.0.14} Network N10.100.0.12 30  N10.100.0.12 30  cid:5  ∃ipv4Subnet.{10.100.0.12 30}  This can be serialized in Turtle using the concept, role, and datatype deﬁnitions  from CNTFO as shown in Listing 2.7.   32  L. F. Sikos et al.  Listing 2.7 Turtle serialization of Listing 2.6  RE20 is a router and defines  hostname, interface, ipv4, and OSPF routerId :RE20 a net:Router . :RE20 net:hostname "RE20" . :I10.100.0.13 a net:Interface . :I10.100.0.13 net:ipv4 "10.100.0.13"^^net:ipv4Type . :RE20 net:hasInterface :I10.100.0.13 . :RE20 net:routerId "10.100.0.13"^^net:ipv4Type .   RE37 is a router and defines  hostname, interface, ipv4, and OSPF routerId :RE37 a net:Router . :RE37 net:hostname "RE37" . :I10.100.0.14 a net:Interface . :I10.100.0.14 net:ipv4 "10.100.0.14"^^net:ipv4Type . :RE37 net:hasInterface :I10.100.0.14 . :RE37 net:routerId "10.100.0.14"^^net:ipv4Type .   ipv4 addresses belong to subnets  subnets define networks; 255.255.255.252 is a  30 network  Both RE20 and RE27 interfaces have ipv4 addresses :N10.100.0.12_30 a net:Network . :N10.100.0.12_30 net:ipv4subnet "10.100.0.12 30"^^net:ipv4SubnetType . :I10.100.0.13 net:connectedTo :N10.100.0.12_30 . :I10.100.0.14 net:connectedTo :N10.100.0.12_30 .  This example shows how we can deﬁne the network connection between two  routers, which corresponds to the red link in Fig. 2.2.  We deﬁne the two routers, :RE20 and :RE37, with hostnames RE20 and RE37 using net:hostname. Interfaces :I10.100.0.13  for RE20  and :I10.100. 0.14  for RE37  are attached via the net:hasInterface predicate. Network interfaces have IPv4 addresses, deﬁned by the net:ipv4 datatype property of an interface. Because IPv4 addresses are deﬁned in dotted notation format, we restrict the strings “10.100.0.13” and “10.100.0.14” to net:ipv4Type. Since IPv4 networks are deﬁned in terms of IPv4 subnets, we also deﬁne the network :N10.100.0.12_30, with an IPv4 address range deﬁned by net:ipv4subnet. The net:ipv4SubnetType restriction ensures that the string “10.100.0.12 30” is valid. To complete our network connection between RE20 and RE37, we state that their respective interfaces are connected to the same subnet. Speciﬁ- cally, both :I10.100.0.13 and :I10.100.0.14 are net:connectedTo :N10.100.0.12_30.  In summary, using Turtle serialization, we have deﬁned the pattern Router- Interface-Network-Interface-Router. This pattern of network connectivity between routers was derived from the raw ﬁle in Listing 2.5, then with formal semantics in Listing 2.6, then in Turtle format in Listing 2.7. This way, we can formally describe the semantics of the full network of Fig. 2.2 using Turtle and CNTFO. Moreover, because we express network connectivity formally using CNTFO, we can deﬁne rules   2 Knowledge Representation of Network Semantics …  33  0 . 5 . 0 . 1  1  4  5  0 . 5 . 0 . 2  1  1  8  .  0  .  8  2  1  .  0  2  1  8  .  0  .  8  2  1  .  0  1  10.128.0.74 10.128.0.73  1 0.2 0 0.2 0 0.2 1 0.2 0 0.2 0 0.1 10.200.190.1 10.200.190.2  10.145.30.42 10.145.30.41  10.128.0.62  10.128.0.61  10.100.0.42  10.100.0.41  .  .  7 3 0 8 2 1 0 1  .  1  2  .  0  .  8  2  1  .  0  1  10.128.0.14  .  .  2 4 0 8 2 1 0 1  .  1  2  .  0  .  0  0  1  .  0  1  10.100.0.38  8 5 . 0 . 0 0 1 . 0 1  7  3  .  0  3  .  5  4  1  .  0  1  .  .  8 3 0 8 2 1 0 1  .  10.128.0.13  2  2  .  0  .  8  2  1  .  0  1  1 4 . 0 . 8 2 1 . 0 1  10.100.0.37  10.128.0.58  10.128.0.57  8  3  .  0  3  .  5  4  1  .  0  1  10.200.180.2 10.200.180.1  0.1  6  0.1  0  0.2  0.2  1  6  0.1  0  0.2  1  0 .2  5 0 .1 0 .1 5  0 0 .2 0 .1 0  1 0 .2  1  10.200.140.1  10.200.140.2  3  1 . 0 . 0  0  1 . 0  1  2  2  .  0  .  0  0  1  .  0  1  4  1 . 0 . 0  0  1 . 0  1  7 5 . 0 . 0 0 1 . 0 1  10.145.30.22 10.145.30.21  10.1.8.1  10.1.8.254  8 1 t s o h  10.1.7.1  10.1.7.254  10.1.6.1  10.1.6.254  10.1.4.1  10.1.4.254  10.1.3.1  10.1.3.254  k r o w t e n  l e v e l - e s i r p r e t n e n a  f o  y g o l o p o T  2 2  .  .  g i F   34  L. F. Sikos et al.  that facilitate information fusion and enable automated reasoning across multiple disparate network data sources. This automation is largely hidden from the network analysts, while the information it provides signiﬁcantly increases cyber-situational awareness.  binary  predicates  Complex routing policies may need an expressivity higher than that of OWL, such as SWRL19 rules. SWRL rules use unary predicates for describing classes and data types, predicates  see Deﬁnition 2.7 . Deﬁnition 2.7  Atom  An atom is an expression of the form P arg1, arg2, . . . , where P is a predicate symbol  classes, properties, or individuals  and arg1, arg2, . . . are the arguments of the expression  individuals, data values, or variables .  properties,  built-in  n-ary  and  for  SWRL rules contain an antecedent  body  and a consequent  head , both of which can be an atom or a positive  i.e., unnegated  conjunction of atoms  see Deﬁni- tion 2.8 . Deﬁnition 2.8  SWRL Rule  A rule R is given as B1 ∧ . . . ∧ Bm → H1 ∧ . . . ∧ Hn n  cid:2  0, m  cid:2  0 , where B1, . . . , Bm , H1, . . . Hn are atoms, B1 ∧ . . . ∧ Bm is called the body  premise or antecedent , and H1 ∧ . . . ∧ Hn is the head  conclusion or con- sequent .  For example, a BGP peer group can be deﬁned as a group of BGP neighbors  sharing the same outbound policies  see Listing 2.8  [28].  ComputerSystem  ?CS  ∧ Dedicated  ?CS,  cid:19 router cid:19   Listing 2.8 A SWRL rule ∧ RouterInAS  ?CS,AS400  → InBGPPeerGroup  ?CS,AS100_AS400   However, DL axioms should be preferred over rule-based formalisms whenever  possible to ensure decidability.  2.5 Representing Network Data Provenance  While Semantic Web standards support the level of task automation required for the automated processing of complex network data, capturing the provenance of RDF statements, which can make automatically generated network data authoritative and veriﬁable, is not trivial. You see, most mechanisms for capturing provenance in RDF have multiple issues. Reiﬁcation, which makes statements about RDF statements, has no formal semantics, uses blank nodes that cannot be identiﬁed globally, and leads to triple bloat, application-dependent interpretation, and difﬁcult-to-query represen- tations. N-ary relations also employ blank nodes to represent instances of relations,  19Semantic Web Rule Language.   2 Knowledge Representation of Network Semantics …  35  and the restrictions that deﬁne the corresponding classes require a large number of triples. Therefore, alternatives have been proposed over the years  some for captur- ing arbitrary metadata, not provenance speciﬁcally , all with different representa- tion prerequisites, provenance granularity and precision, and reasoning complexity. Approaches that represent provenance with triples by extending the standard RDF data model include RDF XML Source Declaration,20 Annotated RDF  aRDF  [29], Provenance Context Entity  PaCE  [30], singleton property [31], and RDF* [32]. At the description logic level, an approach called contextual annotation [33] was proposed. Quadruple-based approaches include N-Quads,21 named graphs [34] and their derivations, such as RDF triple coloring [35], RDF S graphsets [36], nano- is an approach that publications [37], as well as Annotated RDF Schema [38]. RDF employs quintuples to capture provenance [39]. At the implementation level, many graph databases have proprietary solutions, e.g., AllegroGraph provides sorted index groups and triple attributes that can be used for capturing data provenance [40].  +  Because named graphs constitute the most standard-compliant means of capturing RDF data provenance, the following examples will demonstrate the use of named graphs. By using two named graphs, the default  or assertion  graph can describe the RDF statements about network knowledge, and the provenance graph can detail the provenance of the network knowledge as shown in Listing 2.9.  Listing 2.9 Provenance-aware network knowledge representation using two named graphs  Network knowledge graph @ http:  example.com NetworkDataset1  @prefix :   . @prefix net:   . :Router10143R1 net:inAS :AS10143 .   Provenance graph @ http:  example.org provenance   @prefix prov:   .   prov:wasDerivedFrom   .  The next example, shown in Listing 2.10, utilizes multiple named graphs to capture the provenance of a BGP update message, an OSPF LSA, a router conﬁguration ﬁle, and an open dataset used for generating the statements about a router and the AS to which it belongs.  Listing 2.10 Multiple named graphs for representing network knowledge derived from diverse resources @prefix :   . @prefix net:   . @prefix prov:   .  :BGPUM { :Router10143R1 net:inAS :AS10143 . } :OSPFLSA { :Router10143R1 net:isASBR "true" . }  20https:  www.w3.org Submission rdfsource  21https:  www.w3.org TR n-quads    36  L. F. Sikos et al.  :R1CF { :Router10143R1 net:hostname "AS10143R1" . } :OD { :AS10143 net:asName "EXETEL-AS-AP" . }  :PROV { :BGPUM prov:wasDerivedFrom :BGP10143R1toAS1221R1 ;  :OSPFLSA prov:wasDerivedFrom :OSPF143R1143R2 ; :R1CF prov:wasDerivedFrom :AS10143R1ConfigFile ; :OD prov:wasDerivedFrom :CAIDAASDataset . }  :META { :PROV prov:generatedAtTime  "2017-11-02T12:37:00+09:30"^^xsd:dateTime . }  Because the various parsers utilized to generate the network data may run at different intervals, the timestamps of the corresponding RDF triples vary. To be able to query massive network datasets efﬁciently by creation date and time, the provenance for each node have to be distinguished by a unique identiﬁer, which can be used to capture the second layer of provenance, thereby segregating the timestamps from the network data and the rest of the provenance data  see Listing 2.11 .  Listing 2.11 Capturing dual-layer provenance with named graphs @prefix :   . @prefix net:   . @prefix prov:   . @prefix xsd:   .  :BGPUM { :R10.0.3.1 net:inAS :AS10143 . } :PROVBGP { :BGPUM prov:wasDerivedFrom :BGP_AS10143R1_AS1221R1 .  }  :METABGP { :PROVBGP prov:generatedAtTime  "2017-11-02T12:37:00+09:30"^^xsd:dateTime . }  :OSPFLSA { :R10.0.3.1 net:isASBR "true"^^xsd:boolean . } :PROVLSA { :OSPFLSA prov:wasDerivedFrom :OSPF_AS10143R1 . } :METALSA { :PROVLSA prov:generatedAtTime  "2017-11-02T12:37:00+09:30"^^xsd:dateTime . }  :R1CF { :R10.0.3.1 net:hostname "AS10143R1"^^xsd:string . } :PROVR1CF { :R1CF prov:wasDerivedFrom :AS10143R1_CF . } :METAR1CF { :PROVR1CF prov:generatedAtTime  "2017-11-02T12:37:00+09:30"^^xsd:dateTime . }  :OD { :AS10143 net:asName "EXETEL-AS-AP" . } :PROVOD { :OD prov:wasDerivedFrom :CAIDAASDataset . } :METAOD { :PROVOD prov:generatedAtTime  "2017-11-02T12:37:00+09:30"^^xsd:dateTime . }  To maximize interoperability, dataset-level provenance can be deﬁned using stan- dard terms from the Vocabulary of Interlinked Datasets  VoID 22 as shown in Listing 2.12. Listing 2.12 A dataset whose statements are based on a traceroute @prefix :   .  22https:  www.w3.org TR void    2 Knowledge Representation of Network Semantics …  37  @prefix prov:   . @prefix rdf:   . @prefix void:   .    a void:dataset ; prov:wasGeneratedBy :host18_lsa_traceroute_20171106 .  This makes it possible to describe datasets and interlink concepts with related LOD datasets efﬁciently [41], and execute federated queries on LOD datasets [42]. To segregate network knowledge from provenance data and provide provenance for different graphs, partitioned datasets can be declared with different provenance data using void:subset. In this context, the void:Dataset concept is used strictly for those network knowledge datasets that comply with the LOD dataset requirements.  2.6 Representing Network Data Uncertainty  Network data is inherently uncertain. Probabilistic description logics support the rep- resentation of terminological probabilistic knowledge about concepts and roles, and assertional probabilistic knowledge about instances of concepts and roles. For exam- ple, probabilistic axioms can be used to express the probability of an IP address being spoofed, or the probability that there are misconﬁgurations in a network segment that may create vulnerabilities. In the P-SROIQ probabilistic description logic [43], for example, the set of individuals I is divided into two disjoint sets, the set of classical individuals IC, and the ﬁnite set of probabilistic individuals IP, which are associated with every probabilistic individual in the probabilistic ABox. Assume a ﬁnite nonempty set C of basic classiﬁcation concepts, which are atomic or complex concepts C inSROIQ D  that are free of individuals from IP. Every basic classiﬁcation concept φ ∈ C is a classiﬁcation concept. If φ and ψ are classiﬁcation concepts, then¬φ and  φ  cid:5  ψ   are also classiﬁcation concepts. Note that every classiﬁcation concept is also a standard SROIQ concept and conversely, for every ﬁnite set S of concepts in SROIQ, there exists a ﬁnite set of basic classiﬁcation concepts such that their set of classiﬁcation concepts includes S. A conditional statement is an expression of the form  ψφ [l, u], where φ and ψ are classiﬁcation concepts, and l and u are real numbers from [0, 1]. A probabilistic interpretation Pr is a probability function on IC that maps every IC to a value in [0, 1], i.e., Pr: IC → [0, 1], such that the sum of all Pr I , where I ∈ IC, is 1. The probability of a classiﬁcation concept φ in a probabilistic interpretation Pr, Pr φ , is the sum of all Pr I  such that I= φ. cid:7  has a satisfying classical interpretation I =   cid:2 I    iff it has a satisfying probabilistic interpretation Pr.  , .I  Possibilistic relationships, such as the possibility that two network devices are identical, or that the trafﬁc between two network devices go through a particular coun- try, can be described with possibilistic description logic axioms. In the π-SROIQ possibilistic description logic [44], for example, possibilistic concepts represent pos-   38  L. F. Sikos et al.  I  I  I  −  −  , which associated with each concept C a function C  :  cid:2 I ×  cid:2 D → [0, 1], with each individual a an element in  cid:2 I   , and the universal role U according to the rule R → RA  R  sibilistic sets of individuals and can be constructed using the rule C, D →  cid:7   ⊥  A  C  cid:5  D  C  cid:6  D  ¬C  ∀R.C  ∃R.C  ∀T.d  ∃T.d   cid:2 nS.C   cid:3 nS.C   cid:2 nT.d   cid:3 nT.d  ∃S. Self,  oi , αi     A, α , where C, D are  possibly complex  concepts, A denotes atomic concepts, R denotes  possibly complex  abstract roles, S denotes simple roles, , and α ∈  0, 1]. Possi- T denotes concrete roles, d is a concrete predicate, n ∈ N + bilistic roles represent possibilistic relationships and are built from atomic roles  RA ,  U. inverse roles  R A possibilistic interpretation I =   cid:2 I , .I   with respect to the possibilistic concrete , disjoint with  cid:2 D,  the domain of I  and domain  cid:2 D consists of a nonempty set  cid:2 I :  cid:2 I → [0, 1], a function .I :  cid:2 I ×  cid:2 I → [0, 1], with each concrete role with each abstract role R a function R , with T a function T each concrete individual v and element in  cid:2 D, and to each n-ary concrete predicate D :  cid:2 nD → [0, 1]. The semantics of possibilistic concepts are d the interpretation d deﬁned as follows:  cid:7 I  x  = 1 ⊥I  x  = 0  cid:2   C  cid:5  D I  x  = min I  x , D C  cid:2   C  cid:6  D I  x  = max I  x , D C  ¬C I  x  = 1–C I  x   cid:4   cid:2  1 − R  ∀R.C I  x  = inf y∈ cid:2 I  ∃R.C I  x  = sup I  x, y , C R y∈ cid:2 I  cid:2  1 − T  ∀T .d I  x  = inf v∈ cid:2 D  cid:4   ∃T .d I  x  = sup I  x, v , dD v  T min v∈ cid:2 D  cid:4    cid:2  nS.C I  x  = sup i=1 min yi∈ cid:2 I  cid:4    cid:3  nS.C I  x  = inf yi∈ cid:2 I max n+1 i=1   cid:2  nT .d I  x  = sup i=1 minn vi∈ cid:2 D   cid:3  nT .d I  x  = inf max n+1 i=1 max vi∈ cid:2 D  ∃S.Sel f  I  x  = S I x,x   { o1, α1 , . . . ,  on, αn }  I  x  = sup  A, α I  x  = A where v represents concrete individuals.  I  x, yi  , C S  cid:2  1 − S I  x, vi  , dD vi   T  cid:2  1 − T  cid:4  αi  x = o I  x   cid:2  α, otherwise A   cid:3  I  x   cid:3  I  x  I  x, y , C I  y   I  x, yi  , 1 − C  cid:3  cid:5   I  x, vi  , 1 − dD vi     cid:5  I I  x  = 0 i  I  x, v , dD v   I  x  if A  I  y   cid:3  cid:5    cid:4  max  max  cid:2   min  cid:4   I  yi    I  yi    minn  max  min   cid:3  cid:5    cid:3  cid:5    cid:3  cid:5    cid:3  cid:5    cid:3  cid:5    cid:3  cid:5    cid:4    cid:4    cid:2    cid:2    cid:2   The semantics of possibilistic roles are deﬁned as follows:   R A I  x, y  = R I  x, y  − I  x, y  = R I  y, x  A  R I  x, y  = 1 U   2 Knowledge Representation of Network Semantics …  39  The semantics of possibilistic axioms are deﬁned as follows:   C a  I = C I  a I    R a, b  I = R I   I , b I  a  ¬R a, b  I = 1 − R I , b I  a I    T  a, v  I = T I , v D  I  a  ¬T  a, v  I = 1 − T I , v D  I  a  C  cid:11  D I = inf {C I  x  ⇒ D x∈ cid:2 I  R1 . . . Rn  cid:11  R I = sup{in f  R I  x1, x2 , . . . , R x1,xn+1∈ cid:2 I n  x, v  ⇒ T  T1  cid:11  T2 I = inf where a and b are abstract individuals.  x∈ cid:2 I ,v∈ cid:2 D  I  x }  I T 1  inf  I 1   cid:4    cid:4   the triples in Example 2.2.   xn, xn+1   ⇒ R  cid:5  I  x, v  2   x1, xn+1 } cid:5   I n  As a concrete example for representing uncertain network knowledge, consider  Example 2.2 Representing uncertain network knowledge  cid:21  connectedTo I10.204.20.14, N10.204.20.12_30  [0.85, 1] cid:22   cid:21 traverses T10798_36149, Australia , 0.91 cid:22   The ﬁrst triple expresses that the probability of connection between interface 10.204.20.14 and network 10.204.20.12 30 is at least 85%, reﬂecting the accuracy and completeness limitations of network diagnostic tools used to extract this infor- mation.  The second triple describes the possibility that the trafﬁc between an AS in South Africa  AS10798  and an AS in Hawaii  AS36149  will go through Australia is 91%.  2.7 Representing Network Data Vagueness  The previous formalisms are related to uncertainty theory, because their statements are either true or false to some probability or possibility. However, when representing cyber-knowledge, not all statements are true or false: many are true to a certain degree [45]. This can be expressed efﬁciently with fuzzy values taken from a truth space, such as [0, 1] or a complete lattice, which are useful, among other things, to capture imprecise information derived from probe data and vague concepts, such as secure connection and reliable network data. In the ŁSROIQ fuzzy description logic [46], for example, fuzzy concepts C and D can be built inductively according to the following syntax rule from atomic concepts  A , top concept   cid:7  , bottom concept  ⊥ , named individuals  oi  , and − roles R and S, where S is a simple role of the form RA  atomic role , R  negated role , or U  universal role : C, D → A   cid:7   ⊥  C  cid:5  D  C  cid:6  D  ¬C  ∀R.C  ∃R.C  α1 o1, . . . , αm  om    cid:2  mS.C     cid:3  nS.C   ∃S. Self, where n, m ∈ N, n  cid:2  0,  cid:14 = o j , 1  cid:3  i < j  cid:3  m. Fuzzy general concept inclusions are of the form m > 0, oi   40  L. F. Sikos et al.  I :  cid:2 I ×  cid:2 I → [0, 1], rather than subsets of  cid:2 I ×  cid:2 I  C  cid:11  D  cid:2  α or C  cid:11  D > β, which constrain the truth value of general concept inclu- sions. Fuzzy assertions can be inequality assertions of the form a  cid:14 = b, equality assertions of the form a = b, and constraints on the truth value of a concept or role assertion of the form ψ  cid:2  α, ψ > β, ψ  cid:3  β, or ψ < α, where ψ is of the form C a , R a, b , or ¬R a, b . Role axioms can be fuzzy role inclusion axioms of the form w  cid:11  R  cid:2  α or w  cid:11  R > β for a role chain w = R1 R2, . . . , Rn, or any stan- dard SROIQ role axioms of the form transitive R , disjoint S1, S2 , reﬂexive R , irrefexive R , symmetric R , or asymmetric S . A fuzzy interpretation function .I in , .I   maps each individual name a ∈ NI to an element a fuzzy interpretation I =   cid:2 I I ∈  cid:2 I , but in contrast to the typical Tarski-style interpretation of crisp DLs, maps a I :  cid:2 I → [0,1], rather each atomic concept A ∈ NC to a membership function A than subsets of the object domain, and each atomic role R ∈ NR to a membership . Given a t-norm function R ⊗, a t-conorm ⊕, a negation function  cid:25  and an implication function ⇒, the fuzzy interpretation is extended to complex concepts and roles as follows:  cid:7 I  x  = 1 ⊥I  x  = 0  C  cid:5  D I  x  = C I  x  ⊗ D  C  cid:6  D I  x  = C I  x  ⊕ D  ¬C I  x  =  cid:25 C I  x   cid:4   ∀R.C I  x  = inf I  y  y∈ cid:2 I  cid:5   ∃R.C I  x  = sup I  y  y∈ cid:2 I {α1 o1, . . . , αm  om}I  x  = sup{αix = o   cid:2  mS.C I  x  =  cid:6   cid:14 = yk  I  x, y  ⇒ C I  x, y  ⊗ C  I  x, yi   ⊗ C  sup y1,...,ym∈ cid:2 I  cid:5  cid:7   I  x  I  x   I  yi    m min i=1   cid:5  cid:7   ⊗  ⊗  I i  y j   cid:6   }  R  R   cid:4    cid:4    cid:5    cid:4   S   cid:6    cid:4   n+1 min i=1  I  x, yi   ⊗ C  S  I  yi     cid:7    cid:5   ⇒  1 cid:2  j <k cid:2 m   cid:3  nS.C I  x  =  cid:6   ⊗  y1,...,yn+1∈ cid:2 I  inf  cid:7   cid:5    cid:4   y j = yk  1 cid:2  j <k cid:2 n+1  ∃S.Self I  x  = S − I  x, y  = R  R I  x, y  = 1 U  I  x, x  I  y, x    C a  I = C I   I  a R a, b I = R I   I  a I , b  ¬R a, b  I =  cid:25 R I  a I , b I   I  x  ⇒ D {C  C  cid:11  D I = inf x∈ cid:2 I  I  x }  The fuzzy interpretation function is extended to fuzzy axioms as follows:   2 Knowledge Representation of Network Semantics …  41   R1 . . . Rn  cid:11  R I = inf I  x1, xn+1    cid:5   cid:9   R  x1,xn+1∈ cid:2 I   cid:8   sup  x2...xn∈ cid:2 I   cid:4   R  I 1   x1, x2  ⊗ . . . ⊗ R  I n   xn, xn+1   ⇒  Using fuzzy axioms, graded propositions can be expressed in the ABox, such as connection between workstation 18 and Node 254 is secure to a degree of 85%  see Example 2.3 .  Example 2.3 A fuzzy axiom expressing a graded proposition  A =  cid:10  cid:21 Connection WKS18_N254  cid:22 , cid:21 WKS18_N254  cid:5  ∃isSecure.{true}  cid:2  0.85 cid:22  cid:11   Such statements can be used, for example, to express the difference between connections established over HTTP and HTTPS, or a ﬁle transfer via FTP or an encrypted connection using SSL, TLS, SCP, or SFTP.  2.8 Reasoning Support for Cyber-Situational Awareness  The real strength of the formal representation of network knowledge is that it enables inference, thereby making implicit knowledge explicit. Reasoning over the network knowledge also make it possible to perform core tasks, e.g., check whether the knowl- edge represented in a knowledge base is meaningful by searching for contradictory statements  KB consistency , and determine concept satisﬁability, i.e., check whether a concept can ever have instances. Formally speaking, given knowledge base K as input, a decision procedure for knowledge base consistency returns “K is consistent” if there is an interpretation I such that I = K, otherwise it returns “K is inconsis- tent.” Given concept C and knowledge base K as input, a decision procedure for concept satisﬁability with reference to knowledge base K returns “C is satisﬁable with reference to K” if there is an interpretation I =   cid:2 I ,.I   and an element d ∈ I such that I = K and d ∈ C , otherwise it returns “C is unsatisﬁable with reference to K.”  I  Factors that determine the type of reasoning tasks that can be performed on a knowledge base include the level of semantic representation and mathematical for- malization, the knowledge base size, presence or absence of entities, and the functions available in the reasoner used for the task.  Reasoning tasks rely on different sets of reasoning rules, such as RDFS entail- ment,23 which gives semantics to the RDF and RDFS vocabularies; D-entailment,24  23https:  www.w3.org TR 2004 REC-rdf-mt-20040210 RDFSRules 24https:  www.w3.org TR 2004 REC-rdf-mt-20040210 D_entailment   42  L. F. Sikos et al.  which provides semantics to datatypes; and the OWL entailment,25 which adds semantics to the OWL vocabulary [47].  For example, based on the triple :Router10143R1 net:inAS :AS10143 and the domain deﬁnition of net:inAS in CNTFO  :inAS rdfs:domain :Router , it can be inferred using the prp-dom OWL entailment rule, i.e., if T ?p, rdfs:domain, ?c  and T ?x, ?p, ?y , then T ?x, rdf:type, ?c , that :Router10143R1 a net:Router, which was not explicitly stated before. Similarly, using the range deﬁnition of net:isAS  :inAS rdfs:range :AutonomousSystem  and the prp-rng OWL entailment rule, i.e., if T ?p, rdfs:range, ?c  and T ?x, ?p, ?y , then T ?y, rdf:type, ?c , that :AS10143 a net:AutonomousSystem, which originally was implicit knowledge.  2.9 Conclusions  This chapter described formal knowledge representation formalisms that can be used to capture the semantics of communication network concepts, their properties and the relationships between them, in addition to metadata such as data provenance. Using these state-of-the-art knowledge representation formalisms to reason over network data is a promising research direction for cyber-situational awareness. Semantic Web standards provide syntactic and semantic interoperability for network data derived from diverse sources, and machine interpretable deﬁnitions of networking concepts, their properties and relationships, and the entities that instantiate them. The discussed formalisms can employ crisp, fuzzy, probabilistic, or possibilistic logics to express different characteristics of the data, with preference given to those languages which have lower computational complexities, are decidable, and whose axioms can be implemented in OWL. Capturing the semantics of network nodes and routing mes- sages relevant to network topology and information ﬂow can be utilized in reasoning- based network knowledge discovery and ultimately network topology visualization. Expert systems that utilize formally described network semantics can help analysts correlate, combine, and understand network information, disambiguate misleading or conﬂicting network information, identify vulnerabilities, collaborate with other analysts, and see the larger picture.  References  1. Vishik C, Balduccini M  2015  Making sense of future cybersecurity technologies: using ontologies for multidisciplinary domain analysis. In: Reimer H, Pohlmann N, Schneider W  eds  ISSE 2015. Springer, Wiesbaden, pp 135–145. https:  doi.org 10.1007 978-3-658-10934-9_12 2. Sikos LF  2014  Web standards: mastering HTML5, CSS3, and XML, 2nd edn. Apress, New  York. https:  doi.org 10.1007 978-1-4842-0883-0  25https:  www.w3.org TR owl2-proﬁles Reasoning_in_OWL_2_RL_and_RDF_Graphs_using_ Rules   2 Knowledge Representation of Network Semantics …  43  3. Sikos LF  2017  Utilizing multimedia ontologies in video scene interpretation via information fusion and automated reasoning. In: Ganzha M, Maciaszek L, Paprzycki M  eds  Proceedings of the 2017 Federated Conference on Computer Science and Information Systems. IEEE, New York, pp 91–98. https:  doi.org 10.15439 2017F66  4. Miksa K, Sabina P, Kasztelnik M  2010  Combining ontologies with domain speciﬁc languages: a case study from network conﬁguration software. In: Amann U, Bartho A, Wende C  eds  Reasoning Web: semantic technologies for software engineering. Springer, Heidelberg, pp 99–118. https:  doi.org 10.1007 978-3-642-15543-7_4  5. Abar S, Iwaya Y, Abe T, Kinoshita T  2006  Exploiting domain ontologies and intelligent agents: an automated network management support paradigm. In: Chong I, Kawahara K  eds  Information networking: advances in data communications and wireless networks. Springer, Heidelberg, pp 823–832. https:  doi.org 10.1007 11919568_82  6. Martínez A, Yannuzzi M, López J, Serral-Gracià R, Ramarez W  2015  Applying information extraction for abstracting and automating CLI-based conﬁguration of network devices in het- erogeneous environments. In: Laalaoui Y, Bouguila N  eds  Artiﬁcial intelligence applications in information and communication technologies. Springer, Cham, pp 167–193. https:  doi.org  10.1007 978-3-319-19833-0_8  7. Quirolgico S, Assis P, Westerinen A, Baskey M, Stokes E  2004  Toward a formal common information model ontology. In: Bussler C, Hong S-k, Jun W, Kaschek R, Kinshuk, Krish- naswamy S, Loke SW, Oberle D, Richards D, Sharma A, Sure Y, Thalheim B  eds  Web information systems–WISE 2004 workshops. Springer, Heidelberg, pp 11–21. https:  doi.org  10.1007 978-3-540-30481-4_2  8. Martínez A, Yannuzzi M, Serral-Gracià R, Ramírez W  2014  Ontology-based information extraction from the conﬁguration command line of network routers. In: Prasath R, O’Reilly P, Kathirvalavakumar T  eds  Mining intelligence and knowledge exploration. Springer, Cham, pp 312–322. https:  doi.org 10.1007 978-3-319-13817-6_30  9. Laskey K, Chandekar S, Paris B-P  2015  A probabilistic ontology for large-scale IP geolo- cation. In: Laskey KB, Emmons I, Costa PCG, Oltramari A  eds  Proceedings of the Tenth Conference on Semantic Technology for Intelligence, Defense, and Security. RWTH Aachen University, Aachen, pp 18–25. http:  ceur-ws.org Vol-1523 STIDS_2015_T03_Laskey_etal. pdf  10. ETSI Industry Speciﬁcation Group  2012  Measurement ontology for IP trafﬁc  MOI ; requirements for IP trafﬁc measurement ontologies development. ETSI GS MOI 002 V1.1.1. http:  www.etsi.org deliver etsi_gs MOI 001_099 002 01.01.01_60 gs_MOI002v010101p. pdf  11. Kodeswaran P, Kodeswaran SB, Joshi A, Perich F  2008  Utilizing semantic policies for man- aging BGP route dissemination. In: IEEE INFOCOM workshops 2008. IEEE, New York, pp 184–187. https:  doi.org 10.1109 INFOCOM.2008.4544611  12. Voigt S, Howard C, Philp D, Penny C  2018  Representing and reasoning about logical network topologies. In: Croitoru M, Marquis P, Rudolph S, Stapleton G  eds  Graph structures for knowledge representation and reasoning. Springer, Cham, pp 73–83. https:  doi.org 10.1007  978-3-319-78102-0_4  13. Sikos LF, Stumptner M, Mayer W, Howard C, Voigt S, Philp D  2018  Representing net- work knowledge using provenance-aware formalisms for cyber-situational awareness. Procedia Comput Sci 126C:29–38  14. Sikos LF  2016  RDF-powered semantic video annotation tools with concept mapping to Linked Data for next-generation video indexing: a comprehensive review. Multim Tools Appl 76 12 :14437–14460. https:  doi.org 10.1007 s11042-016-3705-7  15. Bizer C, Heath T, Berners-Lee T  2009  Linked data—the story so far. Int J Semant Web Inform  Syst 5 3 :1–22. https:  doi.org 10.4018 jswis.2009081901  16. Carroll JJ, Bizer C, Hayes P, Stickler P  2005  Named graphs, provenance, and trust. In: Proceedings of the 14th International Conference on World Wide Web. ACM, New York, pp 613–622. https:  doi.org 10.1145 1060745.1060835   44  L. F. Sikos et al.  17. Sikos LF  2017  Description logics in multimedia reasoning. Springer, Cham. https:  doi.org   10.1007 978-3-319-54066-5  18. Alani MM  2017  Guide to Cisco routers conﬁguration: becoming a router geek. Springer,  Cham. https:  doi.org 10.1007 978-3-319-54630-8  19. Systems C  2009  Cisco uBR7200 series universal broadband router software conﬁguration  guide. Cisco Press, Indianapolis  org html rfc4271  20. Rekhter Y, Li T, Hares S  eds   2006  A border gateway protocol 4  BGP-4 . https:  tools.ietf.  21. Moy J  ed   1998  OSPF version 2. https:  tools.ietf.org html rfc2328 22. Callon R  ed   1990  Use of OSI IS-IS for routing in TCP IP and dual environments.  https:  tools.ietf.org html rfc1195  23. Hedrick C  ed   1988  Routing information protocol. https:  tools.ietf.org html rfc1058 24. Nakibly G, Gonikman D, Kirshon A, Boneh D  eds   2012  Persistent OSPF attacks. In: 19th Annual Network and Distributed System Security Conference, San Diego, CA, USA, 5–8 Feb 2012  25. Dijkstra EW  1959  A note on two problems in connexion with graphs. Numer Math 1 1 :269–  271. https:  doi.org 10.1007 BF01386390  26. Braden R  ed   1989  Requirements for internet hosts–application and support. https:  tools.  ietf.org html rfc1123  27. Sikos LF, Stumptner M, Mayer W, Howard C, Voigt S, Philp D  2018  Summarizing network information for cyber-situational awareness via cyber-knowledge integration. In: AOC 2018 Convention, Adelaide, Australia, 28–30 May 2018  28. Clemente FJG, Calero JMA, Bernabe JB, Perez JMM, Perez GM, Skarmeta AFG  2011  Semantic Web-based management of routing conﬁgurations. J Netw Syst Manag 19 2 :209– 229. https:  doi.org 10.1007 s10922-010-9169-6  29. Udrea O, Recupero DR, Subrahmanian VS  2010  Annotated RDF. ACM Trans Comput Logic  11, Article 10. https:  doi.org 10.1145 1656242.1656245  30. Sahoo SS, Bodenreider O, Hitzler P, Sheth A, Thirunarayan K  2010  Provenance context entity  PaCE : scalable provenance tracking for scientiﬁc RDF data. In: Gertz M, Ludascher B  eds  Scientiﬁc and statistical database management. Springer, Heidelberg, pp 461–470. https:  doi. org 10.1007 978-3-642-13818-8_32  31. Nguyen V, Bodenreider O, Sheth A  2014  Don’t like RDF reiﬁcation? Making statements about statements using singleton property. In: Chung C-W  ed  Proceedings of the 23rd Inter- national Conference on World Wide Web. ACM, New York, pp 759–770. https:  doi.org 10. 1145 2566486.2567973  32. Hartig O, Thompson B  2014  Foundations of an alternative approach to reiﬁcation in RDF.  arXiv:1406.3399  tion logics. arXiv:1709.04970  33. Zimmermann A, Gimenez-Garcea JM  2017  Integrating context of statements within descrip-  34. Watkins ER, Nicole DA  2006  Named graphs as a mechanism for reasoning about provenance. In: Zhou X, Li J, Shen HT, Kitsuregawa M, Zhang Y  eds  Frontiers of WWW research and development. Springer, Heidelberg, pp 943–948. https:  doi.org 10.1007 11610113_99  35. Flouris G, Fundulaki I, Pediaditis P, Theoharis Y, Christophides V  2009  Coloring RDF triples to capture provenance. In: Bernstein A, Karger DR, Heath T, Feigenbaum L, Maynard D, Motta E, Thirunarayan K  eds  The Semantic Web–ISWC 2009. Springer, Heidelberg, pp 196–212. https:  doi.org 10.1007 978-3-642-04930-9_13  36. Pediaditis P, Flouris G, Fundulaki I, Christophides V  2009  On explicit provenance manage- ment in RDF S graphs. In: Proceedings of the First Workshop on the Theory and Practice of Provenance, Article 4. USENIX Association, Berkeley  37. Groth P, Gibson A, Velterop J  2010  The anatomy of a nanopublication. Inform Serv Use  30 1–2 :51–56. https:  doi.org 10.3233 ISU-2010-0613  38. Straccia U, Lopes N, Lukácsy G, Polleres A  2010  A general framework for representing and reasoning with annotated semantic web data. In: Proceedings of the 24th AAAI Conference on Artiﬁcial Intelligence. AAAI Press, Menlo Park, CA, USA, pp 1437–1442. https:  www.aaai. org ocs index.php AAAI AAAI10 paper view 1590 2228   2 Knowledge Representation of Network Semantics …  45  39. Schüler B, Sizov S, Staab S, Tran DT  2008  Querying for meta knowledge. In: Proceedings of the 17th International Conference on World Wide Web. ACM, New York, pp 625–634. https:  doi.org 10.1145 1367497.1367582  40. Sikos LF  2015  Mastering structured data on the Semantic Web: from HTML5 Microdata to  Linked Open Data. Apress, New York. https:  doi.org 10.1007 978-1-4842-1049-9  41. Alexander K, Cyganiak R, Hausenblas M, Zhao J  2009  Describing linked datasets. In: Bizer C, Heath T, Berners-Lee T, Idehen K  eds  Proceedings of the WWW2009 Workshop on Linked Data on the Web. RWTH Aachen University, Aachen. http:  ceur-ws.org Vol-538 ldow2009_ paper20.pdf  42. Akar Z, Halaç TG, Ekinci EE, Dikenelli O  2012  Querying the Web of interlinked datasets using VoID descriptions. In: Bizer C, Heath T, Berners-Lee T, Hausenblas M  eds  Proceedings of the WWW2012 Workshop on Linked Data on the Web. RWTH Aachen University, Aachen. http:  ceur-ws.org Vol-937 ldow2012-paper-06.pdf  43. Klinov P, Parsia B  2013  Understanding a probabilistic description logic via connections to ﬁrst-order logic of probability. In: Bobillo F, Costa PCG, d’Amato C, Fanizzi N, Laskey KB, Laskey KJ, Lukasiewicz T, Nickles M, Pool M  eds  Uncertainty reasoning for the Semantic Web II. Springer, Heidelberg, pp 41–58. https:  doi.org 10.1007 978-3-642-35975-0_3 44. Bal-Bourai S, Mokhtari A  2016  π-SROIQ D : possibilistic description logic for uncer- tain geographic information. In: Fujita H, Ali M, Selamat A, Sasaki J, Kurematsu M  eds  Trends in applied knowledge-based systems and data science. Springer, Cham, pp 818–829. https:  doi.org 10.1007 978-3-319-42007-3_69  45. Sikos LF  2018  Handling uncertainty and vagueness in network knowledge representation for cyberthreat intelligence. In: Proceedings of the 2018 IEEE International Conference on Fuzzy Systems. Curran Associates, Red Hook, NY, USA 46. Bobillo F, Straccia U  2011  Reasoning with the ﬁnitely many-valued Łukasiewicz fuzzy description logic SROIQ. Inform Sci 181 4 :758–778. https:  doi.org 10.1016 j.ins.2010. 10.020  47. Sikos LF, Stumptner M, Mayer W, Howard C, Voigt S, Philp D  2018  Automated reason- ing over provenance-aware communication network knowledge in support of cyber-situational awareness. In: Liu W, Giunchiglia F, Yang B  eds  Knowledge science, engineering, and man- agement. Springer, Cham, pp 132–143. https:  doi.org 10.1007 978-3-319-99247-1_12   Chapter 3 The Security of Machine Learning Systems  Luis Muñoz-González and Emil C. Lupu  Abstract Machine learning lies at the core of many modern applications, extracting valuable information from data acquired from numerous sources. It has produced a disruptive change in society, providing new functionality, improved quality of life for users, e.g., through personalization, optimized use of resources, and the automation of many processes. However, machine learning systems can themselves be the targets of attackers, who might gain a signiﬁcant advantage by exploiting the vulnerabilities of learning algorithms. Such attacks have already been reported in the wild in different application domains. This chapter describes the mechanisms that allow attackers to compromise machine learning systems by injecting malicious data or exploiting the algorithms’ weaknesses and blind spots. Furthermore, mechanisms that can help mitigate the effect of such attacks are also explained, along with the challenges of designing more secure machine learning systems.  3.1 Machine Learning Algorithms Are Vulnerable  Advances in machine learning1 have produce a disruptive change in the society and the development of new technologies. In the Big Data2 era, an increasing number of services rely on AI and data-driven approaches that leverage the huge amount of data available from diverse sources, including people, devices, and sensors. Machine learning algorithms allow to extract valuable information from this overwhelming amount of data, providing powerful predictive capabilities. The use of machine learn- ing facilitates the automation of many tasks, and brings important beneﬁts in terms of new functionality, personalization, and optimization of resources.  1Machine learning is a ﬁeld of computer science that gives software tools the ability to pro- gressively improve their performance on a speciﬁc task without being explicitly programmed. 2Big data refers to extremely large datasets that, when analyzed, can reveal patterns, trends, and associations, but cannot be processed with traditional data processing tools due to data velocity, volume, value, variety, and veracity.  L. Muñoz-González  B  · E. C. Lupu  Imperial College London, London, UK e-mail: l.munoz@imperial.ac.uk    Springer Nature Switzerland AG 2019 L. F. Sikos  ed. , AI in Cybersecurity, Intelligent Systems Reference Library 151, https:  doi.org 10.1007 978-3-319-98842-9_3  47   48  L. Muñoz-González and E. C. Lupu  Machine learning has been successfully applied in many different application domains, including computer and system security. Thus machine learning is at the core of most non-signature-based detection systems, including, among other things, spam, malware, network intrusions, and fraudulent activities. In contrast to tradi- tional signature-based systems, machine learning has generalization capabilities, i.e., learning algorithms can produce predictions for samples they have not seen before. Despite the beneﬁts of machine learning technologies, learning algorithms can be abused, providing new opportunities to cyber-criminals to conduct illicit and highly proﬁtable activities. It has been shown that machine learning algorithms are vul- nerable and can be the objectives of attackers, who might gain a signiﬁcant beneﬁt by exploiting the vulnerabilities of these algorithms [1, 2]. In fact, machine learn- ing itself can be the weakest link in the security chain, and its vulnerabilities can be exploited by attackers to compromise entire infrastructures. Attackers can inject malicious data to poison the learning process or manipulate data at test time, exploit- ing the blind spots and weaknesses of the learning algorithm to evade detection.  Far from a merely theoretical hypothesis, these attacks have already been reported in real-world systems, such as antivirus engines, spam ﬁlters, and fake proﬁle and fake news detection [1]. These attacks have fostered the investigation of the security properties of machine learning. Therefore, there is a growing interest in adversarial machine learning [3], a research area that lies at the intersection of machine learning and cybersecurity, which aims to understand the vulnerabilities of existing machine learning algorithms and the development of new, more secure algorithms.  This chapter describes the vulnerabilities of machine learning systems, explaining the mechanisms that allow attackers to compromise the learning algorithms at training and test time, and are commonly referred to as poisoning and evasion attacks, respec- tively. First, the threat models describing the possible attack scenarios depending on the attacker’s capability, goal, knowledge, and strategy are formalized. Then, poi- soning attacks are described, demonstrating how attackers can manipulate machine learning systems by injecting malicious data into the training dataset used by the algorithm to learn the parameters. Some defensive techniques are provided that can help mitigate the effect of such attacks. Moreover, evasion attacks are also described, which are attacks produced at test time and aim to produce intentional errors or evade the system. Not only attack strategies are described, but also some defense strategies that can help reduce their success. However, defense against poisoning and evasion attacks still remains an open research problem.  3.2 Threat Model  Similar to other traditional security contexts, the ﬁrst step towards the analysis of the vulnerabilities and the security aspects of a system requires the deﬁnition of an adequate threat model. In this section, the framework originally proposed by Barreno et al. [4, 5] and Huang et al. [3] is described with the extensions of Biggio et al. [6] and Muñoz-González et al. [7], which encompasses different attack scenarios   3 The Security of Machine Learning Systems  49  against machine learning algorithms with different attack models. The framework characterizes the attacks according to the attacker’s goal, his capability to manipulate the data and inﬂuence the learning system, his familiarity with the algorithms, the data used by the defender, and the attacker’s strategy. These aspects allow the deﬁnition of optimal attack strategies as an optimization problem, whose solution provides the construction of attack samples.  This framework is valid for attacks both at training and test time, usually referred to as poisoning and evasion attacks [3, 6]. When the attack samples target deep learning algorithms, these samples are also known as adversarial  training and test  samples [7–11]. Although an important part of the related work in adversarial machine learning focuses on classiﬁcation tasks, this framework can also be used for the description of threats for other learning tasks and machine learning paradigms  such as unsupervised learning .3  3.2.1 Threats by the Capability of the Attacker  The capability of attackers to compromise a machine learning system can be deﬁned based on the inﬂuence the attackers have on the data used by the learning algorithm and on the presence of data manipulation constraints.  Attack Inﬂuence  According to the ﬁrst aspect, the attack inﬂuence can be causative, if the attacker can inject or manipulate the training data used by the learning algorithm, or exploratory, if the attacker cannot inﬂuence the training process, but can create malicious samples at test time  poisoning and evasion attacks .  Poisoning attacks occur when the data collected to train the learning algorithms is untrusted, i.e., data is collected and labeled from sensors, people, or devices that can be compromised or maliciously manipulated. In these applications, given the huge amount of data collected, manual data curation to eliminate malicious examples and outliers is often not feasible. Moreover, many applications require frequent retraining of the learning algorithms to adapt to changes in the underlying data distribution.  In exploratory attacks, the attacker can only manipulate data at test time. Even if the dataset used for training the learning algorithm is trusted, the attacker can probe the system to learn the weaknesses and blind spots to produce intentional errors in the learning system  evasion attacks . However, exploratory attacks can also include scenarios in which the objective of the attacker is to obtain information about the machine learning model used by the defender or the data used for training, which in either case means privacy violation.  3Unsupervised machine learning refers to machine learning tasks that infer a function to describe hidden structure from unlabeled data.   50  L. Muñoz-González and E. C. Lupu  Data Manipulation Constraints  Another aspect regarding the attacker’s capabilities is the potential presence of con- straints for data manipulation to carry out attacks. This strongly depends on the application domain. In computer vision applications, for example, the attacker might be able to manipulate every pixel in the image with the only constrain of providing valid values for each pixel. In contrast, if an attacker aims to evade a malware classiﬁ- cation system, the manipulation of the data must preserve the malicious functionality of the malware, limiting the degrees of freedom to carry out an attack.  In poisoning attacks, the attacker may not be in control of the labels assigned to the injected poisoning points. For example, in spam detection systems, the labels are usually assigned automatically, and then  sometimes  relabeled with user feed- back. However, an attacker can infer the labels that are likely to be assigned to the poisoning points. This way, the attacker attempts to control the maximum amount of perturbation added to the input data to have the attack points labeled as desired. This may also be important to generate poisoning samples that are more difﬁcult to detect with outlier detection and data preﬁltering, which can mitigate the effect of some attacks [12].  Typically, these constraints can be modeled in the deﬁnition of optimal attack strategies, and can be characterized by assuming that an initial set of attack samples D p is given and that the attack points are modiﬁed according to a space of possible modiﬁcations φ  D p , such as by constraining the norm of the input perturbation of each attack point.  3.2.2 Threats by the Goal of the Attacker  The goal of the attack can be described in terms of the desired security violation and attack speciﬁcity. In some tasks, such as in multi-class classiﬁcation scenarios, the attacker’s goal must also be described in terms of error speciﬁcity, i.e., the type of errors the attacker aims to produce in the system [7].  Security Violation  In our context, security violation refers to high-level security violations caused by attacks. Three different security violations can be distinguished against machine learning systems:   Integrity violation: the attack evades detection without compromising the nor- mal operation of the system. For example, in a spam ﬁlter application this can be achieved by generating spam emails that are misclassiﬁed as solicited emails  produce false negatives .   Availability violation: the attacker aims to compromise system functionality, as for example, by increasing the overall error rate in a classiﬁcation algorithm.   Privacy violation: the attacker obtains private information about the machine learn- ing system, the data used for training, or the users of the system.   3 The Security of Machine Learning Systems  51  Attack Speciﬁcity  Attack speciﬁcity deﬁnes how speciﬁc the attacker’s intention is, representing a con- tinuous spectrum of possibilities ranging from targeted to nondiscriminatory scenar- ios:   Targeted attack: the attacker aims to produce errors or to degrade system perfor- mance for a reduced set of samples.   Indiscriminate attack: the attacker aims to degrade the performance of the system or to produce errors in a nondiscriminatory fashion  for a broad set of samples .  Error Speciﬁcity  Error speciﬁcity disambiguates cases in which the nature of the errors can be different, as in multi-class classiﬁcation tasks [7]. Therefore, from the error speciﬁcity point of view, an attack can be one of the following:   Error-agnostic attack: the attacker aims to produce any kind of error in the system. For example, in a computer vision application, an error-agnostic attack is produced when an attacker modiﬁes an image so that the machine learning system misclas- siﬁes the object represented in that image, regardless of the  incorrect  predicted category.   Error-speciﬁc attack: the attacker aims to produce a speciﬁc type of error. In a computer vision application, for example, this is the case if the attacker manages the malicious image to be misclassiﬁed by the machine learning system as a speciﬁc  incorrect  class, say, if the attacker aims to misclassify the picture of a dog as a cat.  3.2.3 Threats by the Knowledge of the Attacker  Attacker may have various levels of knowledge about the targeted machine learning system, including the following:   The dataset used for training the learning algorithm, Dtr .   The set of features used by the learning algorithm, X .   The learning algorithm, M.   The objective function the learning algorithm aims to optimize, L.   The parameters of the machine learning algorithm, w. Thus, attacker knowledge can be described in terms of a vector θ =  Dtr ,X ,M, L, w  that encodes the aforementioned elements of the machine learning system. This representation encompasses many different attack scenarios, depending on the assumptions made on each component of θ. Typically, only two main settings are considered though, namely perfect and limited knowledge attacks.   52  L. Muñoz-González and E. C. Lupu  Perfect Knowledge Attacks  In the case of perfect knowledge attacks, the attacker is assumed to know everything about the targeted system. Although this may be unrealistic in most cases, perfect knowledge attacks allow to perform worst-case evaluations of the security of machine learning systems. This enables the estimation of upper bounds on the performance degradation of a system under attack. It can also be helpful for model selection, comparing the performance of different learning algorithms to different settings while taking into account the possibility of being attacked.  Limited Knowledge Attacks  Limited knowledge attacks include a broad range of possibilities, but the literature typically considers two cases: attacks with surrogate data and attacks with surrogate learners.   Limited knowledge attacks with surrogate data are attacks in which the attacker knows the feature representation X , the learning algorithm M, and the objective function optimized by the learning algorithm, L. These attacks also assume that the attacker does not have access to the training data Dtr , but that she has access to a surrogate dataset, ˆDtr , with similar characteristics and data distribution than Dtr . Then, the attacker estimates the parameters of the learning algorithm ˆw by optimizing the objective function L on the surrogate dataset ˆDtr .   In limited knowledge attacks with surrogate models, the attacker is assumed to know the training data Dtr and the feature set X  e.g., if the learning algorithm is trained on publicly available datasets , but not the learning algorithm and the optimized objective function L. In this case, the estimated parameter vector ˆw may also belong to a different vector space than that of the targeted system, as the model of the attacker can be different. These attacks also include cases in which, regardless whether the attacker knows the learning algorithm of the targeted system, it is not possible for the attacker to derive an optimal attack strategy. This happens if the corresponding optimization problem is intractable or difﬁcult to solve, in which case a surrogate model can help overcome this difﬁculty by carrying out a tractable but  possibly  less effective attack.  3.2.4 Threats by Attack Strategy  The attack strategy can be formulated as an optimization problem that considers the different aspects of the threat model. Thus, given the attacker’s knowledge θ and a set of malicious samples D p ∈ φ  D p  the attacker wants to produce, the attacker’s goal can be characterized in terms of an objective function A D p, θ   ∈ R, which measures attack efﬁciency with respect to the attack points D p. Based on these, the optimal attack strategy can be written as the following optimization problem:  D∗  p  ∈ arg maxD p∈φ  D p  A D p, θ     3.1    3 The Security of Machine Learning Systems  53  This high-level formulation includes both poisoning and evasion attacks, and can be applied to different learning tasks. However, the rest of the chapter focuses on classiﬁcation tasks of adversarial machine learning. Finally, Table 3.1 summarizes the different aspects of the described threat model.  Table 3.1 Threat model Attacker’s capability  Attacker’s goal  Attacker’s knowledge  Attack inﬂuence:   Causative attacks: the attacker can manipulate the training data and inﬂuence the learning algorithm   Exploratory attacks: the attacker can only manipulate data at test time Data manipulation constraints: depending on the application, the attacker may be limited to manipulate the features or the labels of the attack samples. The attacker may also consider additional constraints to avoid detection. Security violation:   Integrity violation: malicious activities do not compromise normal system operation   Availability violation: normal system operation is compromised   Privacy violation: the attacker obtains private information about the learning algorithm, the data or the users of the system Attack speciﬁcity:   Targeted attack: focused on a reduced number of target samples   Indiscriminate attack: targets a broad range of samples, e.g., by increasing the overall error rate of the learning system Error speciﬁcity:   Error-agnostic attack: the attacker aims to produce errors in the system regardless of the kind of error   Error-speciﬁc attack: the attacker aims to produce speciﬁc types of errors in the system Perfect knowledge: the attacker is assumed to know the dataset and the feature set used to train the learning algorithm, the model, the function optimized to train the algorithm, and its parameters Limited knowledge:   Surrogate data: the attacker does not know the training dataset used in the target system, but has access to a surrogate dataset with similar characteristics and data distribution   Surrogate model: the attacker knows the dataset used by the target system, but does not know anything about the target model and the function to be optimized. The attacker uses a surrogate model to craft the attack points.   54  L. Muñoz-González and E. C. Lupu  3.3 Data Poisoning  Data poisoning  also known as causative attacks  are considered among the most relevant and emerging security threats for data-driven technologies [13]. In these attacks, the attacker is assumed to have some control over a fraction of the training data used by the learning algorithm. The goal of the attacker is to subvert the entire learning process in a nondiscriminatory or targeted way, i.e., aiming to decrease the overall performance of the system or to produce particular kinds of errors. These attacks can also facilitate subsequent evasion of the system.  In many applications, data is collected in the wild from untrusted sources, such as sensors, devices, or information from humans. For example, many systems and online services rely on users’ data and feedback on their decisions to train and update the underlying learning algorithms. This threat is part of a more general problem related to the reliability of the large amount of data collected by these systems. Obviously, an attacker could provide information purposefully generated to gradually poison the system and compromise its performance over time. Even if the malicious data injected by the attacker is correctly classiﬁed or identiﬁed by the learning algorithm, its performance is still degraded when using this poisoned data to retrain the model. For instance, in the context of spam ﬁltering applications, machine learning algorithms usually classify an email as spam or ham  i.e., solicited email  based on, among other features, the words contained in the body and the header of the email. Attackers can send malicious emails by mixing spam and ham words. As a result, when the system is retrained to adapt to new forms of spam, some of the words previously considered by the system as indications of solicited email will now be considered as typical spam. In addition, some emails containing legitimate words will be incorrectly classiﬁed as spam.  In a classiﬁcation task, the attacker aims to modify the decision boundary4 learned by the machine learning algorithm to increase the overall classiﬁcation error or the error rate of some speciﬁc classes, or, alternatively, to produce misclassiﬁcation for speciﬁc sets of data points. Following the previous example for spam ﬁltering applications, the attacker may want to make the system unusable by having both spam and ham emails classiﬁed incorrectly, or by increasing the number of false negatives, i.e., spam classiﬁed as ham. The attacker can also target some speciﬁc emails that she wants to be misclassiﬁed as spam, for example targeting advertising emails from a competitor [14].  Figure 3.1 shows an example of a poisoning attack for a binary classiﬁer with a small neural network,5 where the system aims to correctly classify the blue and the red dots, assuming that the attacker can only inject malicious points  labeled as red . The decision boundary before poisoning is depicted with a blue line. Figure 3.1a shows that after injecting a single poisoning point  represented as a red star , the decision  4A decision boundary is a hypersurface that partitions the underlying vector space into multiple sets, one for each class. 5Artiﬁcial neural networks are computing systems inspired by the biological neural networks of brains.   3 The Security of Machine Learning Systems  55   a   2  1  0  -1  -2   b   2  1  0  -1  -2  -2  -1  0  1  2  -2  -1  0  1  2  Fig. 3.1 Example of data poisoning in a binary classiﬁcation problem injecting a one poisoning point and b ﬁve poisoning points. The blue line depicts the decision boundary when the system is not attacked. The decision boundary after poisoning is depicted in red. The poisoning points belong to the red class and are represented with stars  boundary  red line  changes noticeably. In Fig. 3.1b represents the signiﬁcant effect of the attack after adding ﬁve malicious points.  In this section, different poisoning attack scenarios against machine learning clas- siﬁers are reviewed, using the threat model described in Sect. 3.2. In addition, a for- mulation is described for optimal poisoning attack strategies, which allows us to model worst-case scenarios that can be applied to assess the security of machine learning classiﬁers when considering the possibility of data poisoning. The transfer- ability property of poisoning attacks is also explained, meaning that attacks against a particular learning algorithm are usually effective against other learning algorithms for the same task. Finally, mechanisms that can help to mitigate the effect of these attacks are described.  3.3.1 Poisoning Attack Scenarios  Following a treatment similar to [7], two main scenarios are described that cover most of the possible attacks against multi-class classiﬁcation systems, according to the type of errors the attacker aims to cause.  Error-Agnostic Poisoning Attacks  Error-agnostic poisoning attacks are the most common poisoning attacks [15–17], in which binary classiﬁcation tasks are typically considered only to cause a Denial of Service  DoS  attack. In these attacks, the attacker aims to produce errors in the system, but it does not matter what type of errors. From the security violation point of   56  L. Muñoz-González and E. C. Lupu  view, these are availability attacks, which can be targeted indiscriminately depending on whether they affect a speciﬁc set of data points or a single data point.  For multi-class classiﬁcation systems, error-agnostic attacks do not aim to cause speciﬁc errors, but only to produce misclassiﬁcation on the targeted data points. Using the formulation of the attack strategy in Eq. 3.1, an error-agnostic poisoning attack can be written as  D∗  p  ∈ arg maxD p∈φ  D p  A D p, θ   = L Dtarget , w D p     3.2   where the objective function the attacker aims to maximize is deﬁned in terms of a loss function6 L  similar to the one used by the targeted classiﬁer  evaluated in a set of points Dtarget , which are the targets of the attacker. Thus, Dtarget can be a reduced set of points  in the case of a targeted attack , or a representative set of points drawn from a similar data distribution than the one used by the victim  in the case of an indiscriminate attack . Loss L is also a function of the parameters of the learning algorithm, w, which depends on the set of poisoning points injected, D p, i.e., the dependency of L on D p is indirectly encoded through the parameters w. As described later in Sect. 3.3.2, this implicit dependency can be modeled as a bilevel optimization7 problem.  Therefore, in error-agnostic poisoning attacks the attacker wants to maximize the loss function for the targeted samples, regardless of the classiﬁcation errors that can be produced in the system. This scenario allows to model poisoning attacks in which the attacker can manipulate both the features and the labels of the malicious points. Error-Speciﬁc Poisoning Attacks In error-speciﬁc poisoning attacks, it is assumed that the attacker wants to cause speciﬁc misclassiﬁcation errors, which is relevant in multi-class classiﬁcation tasks [7]. According to the taxonomy described in Sect. 3.2, this kind of attack can produce both integrity and availability security violations, and, as in the previous case, it can be targeted or nondiscriminatory, depending on the set of data points targeted by the attacker.  The attack strategy for error-speciﬁc poisoning attacks can be formulated as D∗  ∈ arg maxD p∈φ  D p  A D p, θ   = arg maxD p∈φ  D p   − L D cid:4   , w D p    target  p   3.3   or, equivalently  D∗  ∈ arg minD p∈φ  D p  L D cid:4   , w D p    p   3.4  target contains the same data as Dtarget , but In this case, the set of target samples D cid:4  with labels chosen by the attacker. For example, if the attacker wants to misclassify a sample from class 1  in the dataset Dtarget   as if it was a sample from class 2,  target  6A loss function is a function that maps values of one or more variables onto a real number repre- senting the cost associated with those values. 7Bilevel optimization is an optimization that embeds  nests  a problem within another problem.   3 The Security of Machine Learning Systems  57  the attacker must label that sample in D cid:4  target as class 2. Then, the objective of the attacker is to ﬁnd the set of poisoning points, D p, that minimize the loss function on the relabeled samples in D cid:4  target . As in the previous case, the dependency of the objective function w.r.t. D p is indirect  through the parameters of the model w . This can also be modeled as a bilevel optimization problem.  For integrity violations and targeted attacks, some of the labels in D cid:4   target may actually be the same as the true labels, so that normal system operation is not com- promised, or only speciﬁc data points are affected. This can also help to make the attack less detectable.  Synthetic Example  To illustrate the previous scenarios, Fig. 3.2 shows a synthetic dataset for multi-class classiﬁcation with three classes, represented as blue, green, and red dots. Figure 3.2a suggests that when there is no attack, the multi-class logistic regression classiﬁer used in this example is capable of separating the three classes correctly  the decision regions are depicted with the same colors as the corresponding data points . For the attacks, it is assumed that the attacker can only inject data points labeled as green. Figure 3.2b shows the resulting decision regions after an error-agnostic poisoning attack, during which the attacker injected three malicious points. These points caused signiﬁcant changes in the decision boundary, compared to what can be learned when no attack is present  depicted with a blue line . The poisoning points are represented as green dots with a red border and are situated at the corners of a black rectangle that represents the attacker’s constraints to manipulate the features  i.e., φ  D p  in Eq. 3.2 . In this case, there are two overlapping poisoning points in the upper-left corner.  Figure 3.2c shows the effect of an error-speciﬁc poisoning attack, for which the attacker wants to have the red points misclassiﬁed as green. After injecting three poisoning points, the decision region for the red class have decreased accordingly. As a consequence, most of the red points will now be misclassiﬁed as green. As in Fig. 3.2c, the decision boundary learned for the clean dataset is represented with a blue line and the black rectangle represents the attacker’s constraints in Eqs. 3.3 and 3.4.  3.3.2 Optimal Poisoning Attacks  The optimal formulations of the poisoning attack strategies described previously can help to understand the vulnerabilities of machine learning classiﬁers and to assess the robustness of the algorithms against data poisoning in worst-case scenarios. As mentioned before, the two poisoning attack scenarios described in Eqs. 3.2 and 3.3 can be modeled as bilevel optimization problems.  In line with most related work in poisoning attacks [7, 15–17], the following  assumptions can be made to reduce the complexity of the problem:   58  L. Muñoz-González and E. C. Lupu  Fig. 3.2 Poisoning attack scenarios in a multi-class classiﬁcation problem with three classes. The objective of the error-agnostic attack in  b  is to increase the overall classiﬁcation error. The error- speciﬁc attack in  c  aims to misclassify red points as green. The poisoning points belong to the green class and are denoted with a red border. The blue line represents the decision boundary before the attack and the black rectangle represents the attacker’s constraints to generate the malicious data points    Consider the optimization of one poisoning point at a time.   Assume that the labels of the poisoning points are chosen initially by the attacker, and then kept ﬁxed during the optimization of the features of the poisoning points.  With these assumptions, optimal poisoning attacks can be simpliﬁed as  ∗ p  x  ∈ arg maxx p∈φ  x p  A D p, θ   = arg maxx p∈φ  x p  L Dtarget , w D p   , s.t. w  Ltr  Dtr ∪  x p, yp , w   ∗ ∈ arg min  w   3.5    3 The Security of Machine Learning Systems  59  where Dtr is the training dataset used by the learning algorithm and the pair  xp, yp  represents the features and the label of the poisoning point, respectively. Thus, the outer optimization problem in Eq. 3.5 represents the attacker’s problem, whereas the inner problem is related to the defender’s objective, in other words, to minimize the loss function, Ltr , on the  tainted  training dataset. Usually L and Ltr are expected to be the same  in perfect knowledge scenarios , but by using a different notation for the attacker’s and defender’s losses, more ﬂexible scenarios can be considered.  This bilevel optimization problem can be solved with a gradient ascent approach for some classes of loss functions and learning algorithms, for which the gradients can be computed. This is the case of, among other things, neural networks, deep learning architectures, and support vector machines  SVM . However, it is still not clear how to solve this problem for decision trees or random forests, to which a gradient-based approaches cannot be applied. Provided thatL andLtr , the loss for the attacker and the defender, are differentiable w.r.t. w and xp, the gradient ∇x p A can be computed, which is needed to solve the bilevel optimization problem, following the chain rule: ∇wL  A = ∇xp  ∇xp   3.6   L + ∂w ∂ xp  The main difﬁculty here is to compute the implicit derivative of the parameters w w.r.t. the poisoning point xp. To do so, the inner optimization problem in Eq. 3.5 can be replaced with its stationarity conditions, i.e., the Karush-Kuhn-Tucker  KKT  conditions [7, 15–17]:   3.7  This can only be done under some regularity conditions, requiring Ltr , the loss function of the learning algorithm, to be differentiable w.r.t. w and xp. This is the case of linear classiﬁers, such as Adaline, support vector machines, or logistic regression, neural networks, and deep learning architectures.  ∇wLtr  Dtr ∪  xp, yp , w  = 0  Following from the KKT conditions in Eq. 3.7, the implicit function theorem can  be applied to compute the desired implicit derivative:  = − ∇xp  ∇wLtr   ∇2  wLtr  −1  ∂w ∂ xp  By inserting Eqs. 3.8 into 3.6, the gradient required to solve the bilevel optimization problem can be written as  ∇xp  A = ∇xp  L −  ∇xp  ∇wLtr   ∇2  wLtr  −1∇wL  Then, following a gradient ascent strategy, the update equation to compute the poi- soning point is given by  xp =  cid:4 φ   cid:2   xp + η∇xp  A cid:3    3.8    3.9    3.10    60  L. Muñoz-González and E. C. Lupu  where η is the learning rate and  cid:4 φ is a projection operator to map the current update of the poisoning point onto the feasible domain φ that models the constraints of the attacker in the outer optimization problem.  Synthetic Example  Figure 3.3 shows an example of the optimal poisoning attack strategy on a synthetic example with a logistic regression classiﬁer. In this example, there are two classes, red and yellow dots, and the attacker aims to inject a poisoning point from the red class to maximize the overall classiﬁcation error. To do so, the attacker’s objective is evaluated in a separate  validation  dataset, Dtarget , with data points drawn from the same distribution as the training points shown in the ﬁgure. Cross-entropy was used as the loss function for the attacker’s objective, A, and for the logistic regression classiﬁer, Ltr .  In Fig. 3.3a, the trajectory of the poisoning point  yellow line  is shown when applying the gradient ascent strategy. The background color map depicts the attacker’s objective function A. The black rectangle represents φ, the constraints imposed by the attacker to control the values of the features for the poisoning points. The red star represents the poisoning point injected by the attacker as a result of the bilevel optimization problem in Eq. 3.5. As expected, the poisoning point is located at the point within the feasible domain for which the cross-entropy is maximized. Addi- tionally, the effect of the poisoning point on the decision boundary  solid red line  can be seen in comparison with the decision boundary before the attack  dashed red line .   a    b   Fig. 3.3 Optimal poisoning attack on a synthetic dataset. The attacker aims to maximize the clas- siﬁcation error by injecting one poisoning point  red . The black rectangle represents the attacker’s constraints. The decision boundary before and after the attack is represented with dashed and solid red lines, respectively. The trajectory of the gradient ascent strategy followed to solve the opti- mization problem in Eq. 3.5 is depicted in yellow, and the ﬁnal poisoning point is represented as a red star. The color map in  a  depicts the cross-entropy used to model the attacker’s objective A, evaluated in Dtarget . The color map in  b  represents the classiﬁcation error evaluated in the same dataset used for A   3 The Security of Machine Learning Systems  61  Figure 3.3b shows the same scenario, but the color map is represented with the clas- siﬁcation error evaluated in the validation dataset used by the attacker in A. Attackers cannot maximize the classiﬁcation error directly, because it is a non-differentiable function, hence the optimal attack strategy in Eq. 3.5 cannot be applied. Note, how- ever, that the color map for the classiﬁcation error is similar to the one depicted in Fig. 3.3a for cross-entropy. This shows that cross-entropy is good for maximizing the classiﬁcation error.  In Fig. 3.4, an example of a poisoning attack is shown in MNIST,8 a well-known benchmark for handwritten digit recognition in computer vision problems. To sim- plify the scenario, a binary classiﬁcation problem with digits 1 and 7 is considered. Similar to the previous example, the attacker aims to maximize the classiﬁcation error by maximizing the cross-entropy and targeting a logistic regression classiﬁer. In this case, the attacker aims to inject a poisoning point labeled 7 to maximize the classiﬁcation error. To do so, the attacker solves the bilevel optimization problem in Eq. 3.5 starting from an image representing 1  see Fig. 3.4a . The resulting poison- ing point, shown in Fig. 3.4b, have features related to both digits, 1 and 7, i.e., the shape of the poisoning point is designed in such a way that targets features that are important to distinguish the two digits. Reducing Computational Complexity Each update in Eq. 3.10 requires to solve the inner optimization problem to get the KKT conditions, i.e., the corresponding derivatives are evaluated for the values of w obtained after training the learning algorithm. On the other hand, computing and inverting the Hessian∇2 wLtr scales in time asO d3  and in memory asO d2 , being d the cardinality of w. Moreover, the computation of Eq. 3.9 requires solving one linear system per feature in the poisoning point, which is also computationally expensive, and in some cases, infeasible.  Following a similar approach to [18], the computational complexity of Eq. 3.9 can be reduced by applying a conjugate gradient descent to solve a simpler linear system, which is obtained by a trivial reorganization of the terms in the second part of Eq. 3.9. This can be achieved by ﬁrst solving the linear system given by  Fig. 3.4 Example of the optimal poisoning attack in MNIST dataset  using digits 1 and 7 . a Initial point chosen by the attacker. b Final poisoning point after solving the bilevel optimization problem for an optimal attack  8http:  yann.lecun.com exdb mnist    62  L. Muñoz-González and E. C. Lupu   ∇2  wLtr  v = ∇wL   3.11   and then compute the gradient as  ∇x p  A = ∇x p The computation of the matrices ∇x p following Hessian vector products [19]:  L −  ∇x p ∇wLtr and ∇2  ∇wLtr  v  3.12  wLtr can be avoided by using the   ∇x p  ∇wLtr  z = lim h→0 wLtr  z = lim  ∇2 h→0  1 h 1 h  Ltr  x p, w + hz  − ∇x p   cid:2 ∇x p Ltr  x p, w   cid:2 ∇wLtr  x p, w + hz  − ∇wLtr  x p, w   cid:3    cid:3    3.13   Although these numerical tricks allow for a more efﬁcient computation of the poisoning points, the inner optimization problem in Eq. 3.5 still has to be solved exactly for each iteration in the gradient ascent. This can be infeasible in large neural networks and deep learning systems, which require a lot of time to be trained. How- ever, as proposed in [7], this limitation can be overcome by employing back-gradient optimization, a technique ﬁrst proposed in the context of energy-based models [20], and later in deep learning architectures [35] for hyperparameter optimization, via solving bilevel optimization problems similar to the one in Eq. 3.5.  The underlying idea of this approach is to replace the inner optimization prob- lem with a reduced set of iterations performed by the learning algorithm to update parameters w. This requires updates to be smooth, similar to the case of gradient- based learning algorithms, such as neural networks and deep architectures. This way, it is possible to compute the gradients required in the outer optimization problem using wT , the parameters of the learning algorithm obtained from an incomplete optimization of the inner problem, truncated to T iterations [20].  In Algorithm 1, the standard gradient-descent approach is described, which is used to solve the incomplete inner optimization problem, limiting the number of iterations to T .  Algorithm 1: Gradient Descent 1 Input: initial parameters w0, learning rate η, Dtr , Ltr 1: for t = 0, . . . , T − 1 do gt = ∇wLtr  Dtr , wt   2: 3: wt+1 ← wt − η gt 4: end for  Output: trained parameters wT   3 The Security of Machine Learning Systems  63  putes ∇x p  In Algorithm 2, the back-gradient optimization procedure is described that com- A with a recurrent algorithm similar to gradient descent with T iterations.  Algorithm 2: Back-Gradient Descent Input: trained parameters wT , learning rate  inner problem  η, learning rate  outer problem  α, Dtr , Dtarget , poisoning point  x p, yp , attacker’s function L, learner’s loss function Ltr . initialize d x p ← 0, dw ← ∇wL Dtarget , wT   1: for t = T , . . . , 1 do d xp ← d xp − η dw∇xp 2: dw ← dw − η dw∇2 3: gt−1 = ∇wt Ltr  xp, wt   4: 5: wt−1 = wt + αgt−1 6: end for Output: ∇xp  ∇wLtr  xp, wt   wLtr  xp, wt    A = ∇xc  L + d xp  This represents a signiﬁcant computational improvement compared to traditional approaches, because it only requires to compute a reduced number of iterations for the learning algorithm. This allows poisoning attacks against large neural networks and deep learning systems [7].  3.3.3 Transferability of Poisoning Attacks  In adversarial machine learning, attack transferability is an important property, which can be deﬁned as how effective an attack that targets a speciﬁc learning algorithm is when applied to a different algorithm. Thus, transferability of attacks between learning algorithms allows to analyze up to what extent the algorithms share the same vulnerabilities, which can be valuable when proposing defensive mechanisms that can be effective to mitigate the effect of attacks for a broader range of algo- rithms. Following the threat model described in Sect. 3.2, transferability attacks can be deﬁned as partial knowledge attacks with surrogate models, i.e., attacks in which the attacker does not know the learning algorithm used by the defender.  Although transferability has been mainly analyzed in the context of evasion attacks [21]  see Sect. 3.4.2 , experimental evidence of this property has also been shown in the context of poisoning attacks [7]. More precisely, it has been shown that poisoning attack strategies targeting speciﬁc learning algorithms are quite effective against other algorithms as well, particularly if they are similar. For example, attacks between different linear classiﬁers are quite transferable.  To illustrate this, Fig. 3.5 shows a synthetic example in a binary classiﬁcation task, where the aim is to classify the red and the yellow dots with a linear classiﬁer. The attacker attempts to inject a red poisoning point to maximize the error of the   64  L. Muñoz-González and E. C. Lupu   a    b    c   Fig. 3.5 Transferability of poisoning attacks. Synthetic example with a linear classiﬁcation prob- lem using three different conﬁgurations: a Adaline minimizing the MSE; b Logistic regression minimizing the MSE; c Logistic regression minimizing cross-entropy. In all the plots, the color map shows the attacker’s loss A evaluated on a separate dataset Dtarget . The red line represents the decision boundaries. Note that the maximum of the attacker’s loss is located at the same position for the three plots, which means that the poisoning attacks are transferable between the three models  classiﬁer.Thus, in Fig. 3.5a an Adaline classiﬁer  sometimes also incorrectly referred as Perceptron  is used that minimizes the mean squared error  MSE 9 over the set of training points. The color map shows the MSE for the attacker’s loss function A, which in this case is also the MSE evaluated over a separate dataset Dtarget . The color map suggests that the maximum of this attacker’s function is achieved for poisoning points in the upper-right corner of the plot. Similarly, in Fig. 3.5b a similar color map is shown with the MSE evaluated on Dtarget when the defender is training a logistic regression classiﬁer that aims to minimize the MSE over the same training set that was used by the previous Adaline  9Mean square error is the average of the squares of errors. It is a measure of estimator quality, is always non-negative, and the closer its value to zero the better.   3 The Security of Machine Learning Systems  65  classiﬁer. Although the color map is slightly different, the maximum of the attacker’s objective function is located at the same position as in the previous case. A similar result is obtained in Fig. 3.5c, in which both the attacker and the defender use cross- entropy as the loss function. Therefore, if the optimal poisoning attack strategy is applied in the three scenarios, the resulting poisoning attack point will be identical. This example suggests that regardless of the loss function used by the attacker and the defender, and the type of linear classiﬁer applied, the attack points are trans- ferable. Therefore, even if the attacker does not have perfect knowledge about the learning algorithm used by the defender, the effect of the attack can still be signiﬁcant providing that the surrogate model used by the attacker is reasonably similar.  Transferability between linear and non-linear classiﬁers has also been empirically shown in [7], in which attacks carried out against linear classiﬁers were able to effectively compromise  non-linear  neural network architectures.  3.3.4 Defense Against Poisoning Attacks  Defensive techniques against data poisoning have been less explored in the literature compared to attack strategies, and it remains unclear how to effectively defend against some of these attacks.  Reject On Negative Impact  RONI  defense [14] was one of the ﬁrst algorithms proposed to mitigate the effect of data poisoning via detecting and discarding samples in the training dataset that have a negative impact on the classiﬁer’s performance. RONI requires the retraining of the algorithm using subsets of the training dataset, which, similar to Leave-One-Out validation procedures, uses all the training samples in the subset except one, which is considered for validation. Next, RONI measures the validation performance for each sample in each subset, and removes the points that have the most negative impact on the classiﬁer. Although this technique has been shown to be effective against some types of poisoning attacks [14], it is computa- tionally expensive, as it requires the learning algorithm to be retrained for each point in the training set  even if small subsets are used to speed up the computation . This might be infeasible for deep networks and for applications that require large train- ing datasets. Additionally, RONI may suffer from overﬁtting if the training subsets used by the algorithm are small compared to the number of features. In such cases, the validation performance may not be a reliable indicator to detect malicious data points.  Outlier detection and adequate data preﬁltering can also help to reduce the impact of poisoning attacks, especially in cases when the attacker’s constraints are not prop- erly deﬁned. For example, although the experimental results of the optimal poisoning attacks in [7, 15–17] show that the attacks considerably degrade the performance of machine learning classiﬁers, they do not consider any detectability constraint; in other words, they only check whether the generated attack points are valid points. The lack of detectability constraints in optimal attack strategies can lead to poisoning   66  L. Muñoz-González and E. C. Lupu  points that are quite different from the genuine ones [12], which can be detected and removed from the training dataset by applying outlier detection.  Relying on this argument, in the context of generative models, the authors in [22] proposed a two-step defense strategy for logistic regression classiﬁers. First, outlier detection is applied, followed by the training of the algorithm to solve an optimization problem based on the correlation between the classiﬁer and the labels. Although this approach is effective, its main shortcoming is that the defender is assumed to know a fraction of the poisoning examples in advance, and even though this is not available in practical applications, the performance of the algorithm is sensitive to this value. An outlier detection scheme to defend against data poisoning in classiﬁcation tasks is proposed in [12], with which it is possible to train outlier detectors for each class by relying on a small fraction of trusted data points. This strategy is effective to mitigate the effect of poisoning attacks in case the attacker does not model speciﬁc attack constraints, but its capabilities to detect more constrained attacks are limited. This is the case of label ﬂipping attacks, where the attacker can only manipulate the labels of the poisoning points. In this case, the previous algorithm is only capable to detect and remove the poisoning points far from the genuine points of the corresponding class, which is not always the case. Additionally, this technique requires to curate a small fraction of the training points, i.e., to check that some of the training points are labeled adequately, and that they are not malicious. This assumption might be reasonable in many application domains, although it can be limiting in some contexts. Relying on outlier detection, the authors in [23] provided an upper bound on the performance of binary classiﬁers under data poisoning, also assuming that some data preprocessing is performed before training. However, the bound was computed using a surrogate loss function rather than using the classiﬁcation error, so that the computed upper bound can be quite loose.  Other defense techniques have also been proposed against less aggressive strate- gies, such as label ﬂipping attacks. In [24], a defensive mechanism based on k-Nearest Neighbors is proposed to relabel possibly malicious data points based on the labels of their neighboring samples in the training set. However, this can result in limited performance for attacks in which subsets of poisoning points are close. In [25], inﬂu- ence functions from robust statistics have been proposed both to generate poisoning examples and as a countermeasure against label ﬂipping attacks. Base on these, it is possible to identify the most dangerous samples in the training set that would need manual inspection. Inﬂuence functions provide an estimate of the inﬂuence of each training sample on the performance of the machine learning system. In contrast to RONI, the computation of the inﬂuence functions do not require to retrain the algorithm for each training data point, only for some gradient calculations, thereby reducing the computational complexity signiﬁcantly. In other words, inﬂuence func- tions can be applied to mitigate the effect of label ﬂipping attacks in deep learning systems.   3 The Security of Machine Learning Systems  67  3.4 Attacks at Test Time  Evasion attacks are attacks produced at test time, in which the attacker aims to manipulate the input data to produce an error in the machine learning system. In contrast to data poisoning, evasion attacks do not alter the behavior of the system; instead, they exploit its blind spots and weaknesses to produce the desired errors. Similar to the case of poisoning, these attacks have been mainly analyzed in the con- text of machine learning classiﬁcation, and, similar to the previous section, machine learning classiﬁers are considered here as well.  Evasion attacks can exploit the following two vulnerabilities in the machine learn- ing system:   By leveraging regions of the feature space not supported by the training data  by generating attack points that are quite different from the points used to train the learning algorithm . In these regions, the algorithm can produce quite unexpected predictions. However, these attacks can be easily mitigated by adequate data pre- ﬁltering or by outlier detection. Consequently, points that are considered outliers are rejected by the system. For this reason, this section does not analyze these scenarios.   By exploiting the weaknesses of the learning algorithm  regions of the feature space for which the learned decision boundary differs from the true unknown decision boundary that separates the classes in an optimal way . This is because the number of data points used for training the algorithm is ﬁnite and or the learning algorithm has limited capacity to solve the classiﬁcation problem  for example when using a linear classiﬁer to solve a non-linear classiﬁcation problem or by using small neural networks with limited expressive power . In contrast, in noisy classiﬁcation problems in which the classes cannot be perfectly separated, the attacker can use regions with more overlapping between classes to facilitate the evasion.  In Fig. 3.6, a synthetic example is shown in a binary classiﬁcation task, which illus- trates the aforementioned weaknesses. The blue curve represents the true unknown decision boundary that could be obtained by having an inﬁnite number of train- ing points and a learning algorithm with enough capacity to solve the classiﬁcation problem. The red curve depicts the decision boundary learned by a machine learning classiﬁer on the training points shown in the ﬁgure. In this case, the algorithm is capable of correctly classifying the training points, which means that the algorithm is quite competent to solve the classiﬁcation task. However, when comparing this w.r.t. the true decision boundary, there are regions  shaded in gray  in which the learning algorithm differs  areas where the learning algorithm is incorrect . These adversarial regions can be exploited to carry out successful evasion attacks.  Even if the machine learning classiﬁer has enough expressive power to solve the classiﬁcation task, in some cases these adversarial regions produce scarce data, for example due to the probability density of the data being very low in these regions. Hence, the chance of having training data points in these areas is also very low. This problem gets more severe as the number of features used by the dataset increases:   68  L. Muñoz-González and E. C. Lupu  Fig. 3.6 Motivation of evasion attacks. In this binary classiﬁcation problem, the true  unknown  decision boundary that optimally separates the two classes is depicted with a blue curve. Because the training dataset, given by the red dots and the yellow stars, is ﬁnite, the learned decision boundary is represented with a red curve. Although the learned classiﬁer perfectly separates the training data, it differs from the true decision boundary. The gray areas represent the regions for which the machine learning classiﬁer will make a mistake, which can be exploited by attackers to carry out successful evasion attacks  learning becomes more challenging, and a larger dataset is needed to have a dataset that actually represents the underlying data distribution.  Evasion attacks have recently started targeting deep neural networks, and it turned out that they are very vulnerable to this kind of threat. The associated incidents conﬁrm the existence of adversarial examples, i.e., samples that can be misclassiﬁed by the deep learning system when adding imperceptible distortions to genuine images  which are correctly classiﬁed by the system . The existence of adversarial examples was ﬁrst noticed in the context of image classiﬁcation [9], where it was shown that undetectable modiﬁcations in an image can produce different predictions in deep learning classiﬁers. Then, [10, 26] provided a more comprehensive analysis of this threat, describing efﬁcient evasion attack strategies capable of deceiving deep neural networks.  From a practical perspective, evasion attacks are considered relevant threats and can hinder the penetration of machine learning technologies to some application domains. For instance, if deep networks are used to perform image analysis in self- driving cars, attackers can exploit the vulnerabilities of the learning algorithms to produce undesirable actions, as for example by  physically  manipulating trafﬁc signs on the road [27].  The rest of this section describes different evasion attack scenarios against machine learning classiﬁers according to the attacker’s goal. It is also explained how to compute evasion attacks by solving a constrained optimization problem, showing some examples on MNIST. Similar to the case of data poisoning, the transferabil- ity properties of evasion attacks are also described. Finally, some mechanisms that   3 The Security of Machine Learning Systems  69  can help to mitigate the effect of these attacks are described, although this area has challenges yet to be addressed.  3.4.1 Evasion Attack Scenarios  Using the threat model in Sect. 3.2 and following a treatment similar to what was used for poisoning attacks, evasion attack scenarios are described in multi-class classiﬁcation systems according to the type of errors that the attacker wants to produce [28]. More speciﬁc attack scenarios can also be derived from these.  Error-Agnostic Evasion Attacks In multi-class classiﬁcation, the predicted class c is often considered to be the class whose discriminant function for a given input sample x, fk  x  with k = 1, . . . , c, is maximum:  ∗  ∗ = F  x  = arg maxk=1,...,c fk  x   c   3.14   In error-agnostic evasion attacks, also known as indiscriminate evasion, the attacker aims to produce a misclassiﬁcation at test time, regardless of the class pre- dicted by the classiﬁer. For instance, in a face recognition system, an attacker may want to prevent the system from recognizing him, regardless of the incorrect identity predicted by the algorithm.  In this scenario, error-agnostic evasion attacks can be formulated as the following  optimization problem:  ∗ e  x  ∈ arg minxe∈φ  xe  fk  xe  − max j cid:9 =k s.t. d x, xe   cid:2  dmax  f j  xe ,   3.15   where fk  xe  denotes the discriminant function associated with the true class evalu- ated in xe, max j cid:9 =k f j  xe  is the discriminant function from the set of incorrect classes whose value is higher. The constraint of the optimization problem, d x, xe   cid:2  dmax , limits the maximum perturbation, dmax , allowed between the original sample  the one the attacker wants to modify to evade the system , x, and the attack point, xe, given a distance metric d. In the research literature, typically the  cid:7 2,  cid:7 1, and  cid:7 ∞ distances are used to constrain the perturbation of the adversarial examples. Addi- tionally, attack point xe is also restricted to the region of valid points determined by φ, which can also model attack scenarios in which the attacker can only modify some of the features used by the learning algorithm.  Error-Speciﬁc Evasion Attacks  In error-speciﬁc evasion attacks, the attacker aims to evade the system to produce a speciﬁc type of error. For example, in trafﬁc sign detection for self-driving cars, the   70  L. Muñoz-González and E. C. Lupu  attacker may want a stop sign to be misclassiﬁed as a yield  give way  sign. These attacks are usually called targeted evasion attacks in the literature.  These attacks can be formulated as the optimization problem given by   cid:4   ∗ e  x  ∈ arg minxe∈φ  xe  max j cid:9 =ke s.t. d x, xe   cid:2  dmax   cid:5   f j  xe   − fke   xe ,   3.16    xe  is the discriminant function of the target class ke, i.e., the class to which where fke the adversarial example should be assigned. Thus, max j cid:9 =ke f j  xe  is the discriminant function, different from the target class, with the highest value. The constraints are deﬁned the same way as the error-agnostic attacks formulated in Eq. 3.15. Synthetic Example Figure 3.7 shows three different attack scenarios on a synthetic example for a clas- siﬁcation problem with three classes, using a multi-class logistic regression as the machine learning classiﬁer. In Fig. 3.7a, an error-agnostic attack scenario can be observed, in which the attacker aims to produce a misclassiﬁcation of the green dot in the centre of the circle delimited by the dashed black line, which determines the maximum perturbation allowed for the attacker. In this example, the easiest way to evade the classiﬁer is to misclassify the point as red, because the decision boundary between the green and the red class is closer to the original point. The attack point is depicted in green with a red border. Note that attacks with smaller perturbation would also be able to evade the system.  In Fig. 3.7b, an error-speciﬁc evasion attack is shown, in which the attacker attempts to misclassify the green point as blue. In this case, even though the decision boundary between the green and the blue classes is larger than in the previous case, the error-speciﬁc attack is still successful. However, if the maximum level of per- mitted perturbation is decreased, as shown in Fig. 3.7c, only an error-agnostic attack would be successful, because the decision boundary between the green and the blue classes is beyond the maximum budget allowed for the attacker.  3.4.2 Computing Evasion Attacks  The optimization problems in Eqs. 3.15 and 3.16 can be solved using a gradient descent strategy. This involves the update equation to compute the evasion point as follows:  xe =  cid:4 φ,dmax   xe − η∇xe  A    3.17   where A = fk  xe  − max j cid:9 =k f j  xe  or A =  cid:2    xe  for error- generic and error-speciﬁc attacks, respectively. Similar to the case of poisoning attacks, at each iteration the updated point is projected onto the feasible domain through the projection operator  cid:4 φ,dmax , which considers the constraints deﬁned by  max j cid:9 =ke f j  xe    cid:3  − fke   3 The Security of Machine Learning Systems  71   a    b    c   Fig. 3.7 Evasion attack scenarios: a Error-agnostic evasion: the green dot is modiﬁed in a way that it is classiﬁed as red  the dashed black line depicts the attacker’s constraints . b Error-speciﬁc evasion: the attacker aims to misclassify the green dot as blue. c Evasion with a reduced budget: the selected green dot can only be misclassiﬁed as red  φ, i.e., the evasion point should be a valid point, and dmax , the maximum perturbation permitted according to the distance d ·,· .  Figure 3.8 shows the trajectory of the evasion point when applying gradient descent on the synthetic scenarios in Fig. 3.7a and b for both error-agnostic and error-speciﬁc attacks. The color maps depict the cost  A  the attacker tries to mini- mize. The blue regions correspond to lower A values. In the error-agnostic scenario in Fig. 3.8a, the blue regions encompass the areas close to the red and the blue points, i.e., both regions can be used to achieve evasion. However, given the initial position of the evasion point and the attacker’s constraints, the gradient descent algorithm ﬁnds it easier to go to areas near the red points, because the decision boundary is closer. Figure 3.8b shows that for the error-speciﬁc attack, during which the attacker aims to classify the green dot as blue, only the region close to the blue points have low values for cost function A.   72   a   L. Muñoz-González and E. C. Lupu   b   Fig. 3.8 Gradient descent approach to compute the evasion points in the scenario shown in Figs. 3.7a and b. The color map represents the cost the attacker aims to minimize for each scenario. The dashed black lines depict the attacker’s constraints, and the solid red lines show the trajectory of the points when applying gradient descent  Other strategies have been proposed in the literature to compute evasion attack points. For example, for large deep networks, in which computing the gradients can be computationally demanding, an approximate  suboptimal  solution can be produced in one iteration with the Fast Gradient Sign Method  FSGM  [26], with which the evasion attack point can be calculated as  xe = x + dmax sign ∇xLtr   x, y , w     3.18  where Ltr is the loss function minimized by the learning algorithm evaluated on the initial point x  with its initial label y . The formulation of this attack only consid- ers  cid:7 ∞ scenarios in which each feature in the evasion point can be incremented or decremented by no more than dmax .  A different formulation of the optimization problem for error-speciﬁc attacks, Eq. 3.16, is proposed in [10], for which the objective is deﬁned in terms of pertur- bation, and the score of the predicted evasion point is modeled as a constraint as follows:  δ∗ ∈ arg minδδ p  s.t. F  x + δ  = y  ∗   3.19   ∗  where the attacker aims to minimize the perturbation δ  according to some norm p , subject to the fact that the evasion point xe = x + δ should be classiﬁed as y , i.e., F  x + δ  = y . Additional constraints should also be considered to ensure that the evasion point is a valid point. This optimization problem was reformulated in [8] using Lagrangian multipliers to include the constraints in the optimization objective, and providing different scoring functions to model those constraints.  ∗   3 The Security of Machine Learning Systems  73  Fig. 3.9 Example of evasion attacks in the MNIST dataset. a Original image, correctly classiﬁed as an 8. b  cid:7 2 norm evasion attack, limiting the  cid:7 2 norm of the perturbation to 1.5. The digit is now classiﬁed as 6. c  cid:7 ∞ norm evasion attack, limiting the  cid:7 ∞ norm to 0.3. The digit is also classiﬁed as 6. d Perturbation introduced by the attacker in the  cid:7 2 norm attack  the scale is multiplied by a factor of 2 . e Perturbation introduced by the attacker in the  cid:7 ∞ norm attack  the scale is also multiplied by a factor of 2   Example in the MNIST Dataset  Similar to the case of poisoning attacks in Sect. 3.3, Fig. 3.9 shows two examples of evasion points in the MNIST dataset for attacks using  cid:7 2 and  cid:7 ∞ to model the attacker’s constraints. To be more precise, Fig. 3.9a–c show the original point before the attack, and the resulting evasion points for the  cid:7 2 and  cid:7 ∞ attacks, respectively. Before the attack, the image was correctly classiﬁed as 8, but after both attacks, the label of the corresponding images became 6. Figure 3.9d, e show the perturbations  augmented by a factor of 2  added to the original point in the attacks. Note that the digit represented in both Fig. 3.9b, c is 8, with an insigniﬁcant amount of background  adversarial  noise.  3.4.3 Transferability of Evasion Attacks  Section 3.3.3 introduced the concept of transferable attacks, which are attacks that can be used against multiple machine learning algorithms. In the context of eva-   74  L. Muñoz-González and E. C. Lupu  sion, attack transferability implies that an attacker can carry out an evasion attack on a surrogate model and transfer the attack points to the victim’s model success- fully with limited knowledge of the victim’s algorithm. This enables the attacker to achieve successful black box attacks while reducing the number of queries required, thereby hindering the detection of the attack. It has been shown empirically that eva- sion attacks are transferable between different families and conﬁguration of learning algorithms [21], especially for algorithms that share common characteristics. Attacks are also transferable in cases when the attacker uses surrogate datasets. As explained earlier, this is because some the regions that can be leveraged by the attacker to carry out the evasion attack correspond to regions in which the probability density of the underlying data distribution is low.  Figure 3.10 illustrates why evasion attacks are usually transferable between learn- ing algorithms. Similar to the example in Fig. 3.6, for a binary classiﬁcation problem, given a set of data points  red dots and yellow stars , two non-linear machine learning classiﬁers are trained on the same dataset. The corresponding decision boundaries are represented with green and red curves. Similar to Fig. 3.6, the blue curve repre- sents the optimal  unknown  decision boundary that could be obtained for an inﬁnite number of training points. As shown in Fig. 3.10, for the same training dataset, both non-linear classiﬁers may produce similar decision boundaries. Then, the gray shaded areas denote the regions in which attack points will produce an error with both learn- ing algorithms. In contrast, the yellow shaded areas represent regions for which the attack points would be only effective against one of the classiﬁers. Comparing the two sets of regions suggests that in most cases, attack points generated for one of the classiﬁers will also be effective for the other one.  Fig. 3.10 Example of transferability in evasion attacks. The blue curve depicts the true  unknown  decision boundary. The red and green curves represent the decision boundaries learned by two non- linear machine learning classiﬁers trained on the red dots and yellow stars in the ﬁgure. The shaded gray areas show regions in which evasion attacks are effective against the two learning algorithms. Yellow regions depict attacks that are only effective against one of the classiﬁers. The comparison between the gray and yellow surfaces shows that most of the attacks committed against one learning algorithm can also compromise the other one effectively   3 The Security of Machine Learning Systems  75  3.4.4 Defense Against Evasion Attacks  Two main families of defensive techniques against evasion attacks have been inves- tigated in the research community: the ones that aim to classify adversarial examples correctly, and the ones that attempt to detect adversarial examples. However, these defensive techniques are not effective to mitigate the effect of evasion attacks if the attacker targets a speciﬁc defense [31]. For some of these techniques, attackers just have to slightly increase the distortion of the evasion points to succeed in evading the system.  Adversarial retraining is perhaps one of the most effective ways to partially miti- gate the effect of evasion attacks. The idea behind adversarial retraining is to intro- duce adversarial examples in the training dataset  using the correct labels . It was initially proposed in [9], but the cost of computing large sets of adversarial exam- ples is prohibitive if standard gradient-based approaches are used, especially in deep learning systems. However, the computational complexity can be reduced by using the FGSM [26] in Eq. 3.18, which does not require an iterative search for the evasion point. Although this reduces the attack surface, adversarial retraining does not sys- tematically eliminate all possible adversarial regions. Variants of adversarial retrain- ing have also been proposed [29, 30], but they have proven to be inefﬁcient across datasets [31].  Other defense techniques use gradient masking to hide the successful attacks when computing the gradients required to solve the optimization problems in Eqs. 3.15 and 3.16. Papernot et al. proposed Defensive Distillation, a training procedure that aims to smooth the model’s decision surface in the adversarial directions that can be exploited by attackers [32]. However, as shown in Sect. 3.4.3, attack points are often transferable, meaning that attackers can generate adversarial examples using a surrogate model, and then transfer them to the victim’s model with high probability of success. Additionally, it is shown in [33] that Defensive Distillation is no more robust against evasion attacks than unprotected neural networks.  Dimensionality reduction can also be applied to reduce the attack surface and mitigate the effect of evasion attacks. The authors in [34] propose to use Principal Component Analysis  PCA  for reducing the number of effective components used by the classiﬁer. This means an inherent trade-off between the accuracy and the security of learning algorithms. You see, reducing the number of components can negatively affect the performance of the classiﬁer, but a large number of components can provide more ﬂexibility to the attacker to generate the adversarial examples. Experimental results in [34] show the effectiveness of dimensionality reduction to mitigate the effect of evasion attacks, although the authors in [31] state that this defense is not more secure than convolutional neural networks  CNNs . This can be argued, considering the experimental evaluation in [31], which compared the security of various machine learning algorithms  PCA applied to a standard deep   76  L. Muñoz-González and E. C. Lupu  neural network compared to a CNN , not proving that the defense proposed in [34] is not effective compared to algorithms in the same family.  A more comprehensive list and description of similar defense mechanisms and  their limitations can be found in [31].  3.5 Conclusion  This chapter has shown vulnerabilities that can be exploited to compromise machine learning systems. A threat model has been introduced that can be applied to model different attack scenarios according to the attacker’s capabilities, goal, knowledge, and strategy. There are two broad attack classiﬁcations by the attacker’s capability to manipulate the learning algorithm: poisoning and evasion attacks. During poisoning attacks, the attacker can manipulate the behavior of the learning algorithm by inject- ing misleading data into the training set. In contrast, evasion attacks aim to produce errors in the system at test time, leveraging the limitations and blind spots of the learning algorithms.Various attack scenarios have been demonstrated for both attack types depending on the goal of the attacker. Speciﬁcally, error-speciﬁc attacks aim to produce speciﬁc types of errors in the learning algorithm, while error-agnostic attacks simply aim to cause errors in the system, no matter what the type.  Even though various defense mechanisms have been proposed to mitigate both poisoning and evasion attacks, they are still limited, making defense against these threats an open research problem, which is especially true for evasion attacks, for which detection and classiﬁcation of adversarial examples are extremely difﬁcult.  Beyond the research challenge of better understanding the attacks and defense mechanisms against machine learning systems, a particularly important open ques- tion is how to systematically test the security of machine learning systems. In this sense, traditional machine learning design methodologies rely on performance indi- cators  such as accuracy or error rates  that are measured on a separate validation  or test  dataset, which has not been used to train the learning algorithm. Then, Occam’s razor principle is often applied for model selection: the simplest models with similar performance are preferred. This design methodology implies a trade-off between complexity and performance. However, when considering security aspects, i.e., the possible presence of an attacker targeting the learning algorithms, this design methodology is no longer a viable option.  In the case of poisoning attacks, worse-case analysis can be used to compare the robustness of the algorithms for different levels of data poisoning. Optimal attack strategies can help achieve this assessment, but currently they cannot be applied to families of algorithms for which gradients are not available, such as in the case of decision trees and random forests, as well as very large neural networks, for which the computation of the optimal poisoning points can be very expensive. For this reason, better attack strategies are needed, which can provide more capable models for the detectability constraints expected from skilled attackers who try to remain undetected.   3 The Security of Machine Learning Systems  77  Testing robustness against evasion attacks is even more challenging. Evaluating the performance of the algorithms on a test dataset drawn from the natural underlying data distribution does not provide any robustness indicator for evasion attacks, only an estimation of the upper bound of system performance. Therefore, measuring the robustness of machine learning systems against evasion attacks requires veriﬁcation instead to make sure that the system will behave as expected for a broad range of scenarios.  References  1. Muñoz González L, Lupu EC  2018  The secret of machine learning. ITNow 60 1 :38–39.  https:  doi.org 10.1093 itnow bwy018  2. McDaniel P, Papernot N, Celik ZB  2016  Machine learning in adversarial settings. IEEE Secur  Priv 14 3 :68–72. https:  doi.org 10.1109 MSP.2016.51  3. Huang L, Joseph AD, Nelson B, Rubinstein BI, Tygar J  2011  Adversarial machine learning. In: Chen Y, Cárdenas A.A, Greenstadt R, Rubinstein B  eds  Proceedings of the 4th ACM Workshop on Security and Artiﬁcial Intelligence. ACM, New York, pp 43–58. https:  doi.org  10.1145 2046684.2046692  4. Barreno M, Nelson B, Joseph AD, Tygar J  2010  The security of machine learning. Mach  Learn 81 2 :121–148. https:  doi.org 10.1007 s10994-010-5188-5  5. Barreno M, Nelson B, Sears R, Joseph AD, Tygar JD  2006  Can machine learning be secure? In: Lin, F-C, Lee, D-T, Lin B-S, Shieh S, Jajodia S  eds  Proceedings of the 2006 ACM Sym- posium on Information, Computer and Communications Security. ACM, New York, pp 16–25. https:  doi.org 10.1145 1128817.1128824  6. Biggio B, Fumera G, Roli F  2014  Security evaluation of pattern classiﬁers under attack. IEEE  T Knowl Data En 26 4 :984–996. https:  doi.org 10.1109 TKDE.2013.57  7. Muñoz-González L, Biggio B, Demontis A, Paudice A, Wongrassamee V, Lupu EC, Roli F  2017  Towards poisoning of deep learning algorithms with back-gradient optimization. In: Thuraisingham B, Biggio B, Freeman DM, Miller B, Sinha A  eds  Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security, pp 27–38. https:  doi.org 10.1145  3128572.3140451  8. Carlini N, Wagner D  2017  Towards evaluating the robustness of neural networks. In: 2017 IEEE Symposium on Security and Privacy. IEEE Computer Society, Los Alamitos, CA, USA, pp 39–57. https:  doi.org 10.1109 SP.2017.49  9. Szegedy C, Zaremba W, Sutskever I, Bruna J, Erhan D, Goodfellow I, Fergus R  2013  Intrigu-  ing properties of neural networks. arXiv:1312.6199  10. Papernot N, McDaniel P, Jha S, Fredrikson M, Celik ZB, Swami A  2016  The limitations of deep learning in adversarial settings. In: Proceedings of the 2016 IEEE European Symposium on Security and Privacy. IEEE Computer Society, Los Alamitos, CA, USA, pp 372–387. https:  doi.org 10.1109 EuroSP.2016.36  11. Papernot N, McDaniel P, Goodfellow I, Jha S, Celik ZB, Swami A  2017  Practical black- box attacks against machine learning. In: Karri R, Sinanoglu O, Sadeghi A-R, Yi X  eds  Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM, New York, pp 506–519. https:  doi.org 10.1145 3052973.3053009  12. Paudice A, Muñoz-González L, György A, Lupu EC  2018  Detection of adversarial training  examples in poisoning attacks through anomaly detection. arXiv:1802.03041  13. Joseph AD, Laskov P, Roli F, Tygar JD, Nelson B  eds.   2013  Machine learning methods for computer security. Dagstuhl Manif 3 1 :1–30. http:  drops.dagstuhl.de opus volltexte 2013  4356 pdf dagman-v003-i001-p001-12371.pdf   78  L. Muñoz-González and E. C. Lupu  14. Nelson B, Barreno M, Chi FJ, Joseph AD, Rubinstein BI, Saini U, Sutton CA, Tygar JD, Xia K  2008  Exploiting machine learning to subvert your spam ﬁlter. In: Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats, article no. 7. USENIX Association, Berkeley, CA, USA. https:  www.usenix.org legacy event leet08 tech  full_papers nelson nelson.pdf  15. Biggio B, Nelson B, Laskov P  2012  Poisoning attacks against support vector machines. In: Langford J, Pineau J  eds  Proceedings of the 29th International Conference on Machine Learning, pp 1807–1814. arXiv:1206.6389  16. Mei S, Zhu X  2015  Using machine teaching to identify optimal training-set attacks on machine learners. In: Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence. AAAI Press, Palo Alto, CA, USA, pp 2871–2877. https:  www.aaai.org ocs index.php AAAI  AAAI15 paper viewFile 9472 9954  17. Xiao H, Biggio B, Brown G, Fumera G, Eckert C, Roli F  2015  Is feature selection secure against training data poisoning? In: Bach F, Blei D  eds  Proceedings of the 32nd International Conference on Machine Learning, pp 1689–1698  18. Do CB, Foo CS, Ng AY  2007  Efﬁcient multiple hyperparameter learning for log-linear models. In: Proceedings of the 20th International Conference on Neural Information Processing Systems. Curran Associates, Red Hook, NY, USA, pp 377–384  19. Pearlmutter BA  1994  Fast exact multiplication by the Hessian. Neural Comput 6 1 :147–160.  https:  doi.org 10.1162 neco.1994.6.1.147  20. Domke J  2012  Generic methods for optimization-based modeling. In: Proceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics, pp 318–326. http:   proceedings.mlr.press v22 domke12 domke12.pdf  21. Papernot N, McDaniel, P, Goodfellow I  2016  Transferability in machine learning: from phe-  nomena to black-box attacks using adversarial samples. arXiv:1605.07277  22. Feng J, Xu H, Mannor S, Yan S  2014  Robust logistic regression and classiﬁcation. In: Ghahra- mani Z, Welling M, Cortes C, Lawrence ND, Weinberger KQ  eds  Proceedings of the 27th International Conference on Neural Information Processing Systems, vol 1. MIT Press, Cam- bridge, pp 253–261  23. Steinhardt J, Koh PWW, Liang PS  2017  Certiﬁed defenses for data poisoning attacks. In: Guyon I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R  eds  Advances in neural information processing systems 30  NIPS 2017 . Curran Associates, Red Hook, NY, USA, pp 3520–3532. http:  papers.nips.cc paper 6943-certiﬁed-defenses-for-data- poisoning-attacks.pdf  24. Paudice A, Muñoz-González L, Lupu EC  2018  Label sanitization against label ﬂipping poi-  soning attacks. arXiv:1803.00992  25. Koh PW, Liang P  2017  Understanding black-box predictions via inﬂuence functions. In: Proceedings of the 34th International Conference on Machine Learning, pp 1885–1894. arXiv:1703.04730v2  26. Goodfellow I, Shlens J, Szegedy C  2015  Explaining and harnessing adversarial examples.  arXiv:1412.6572  27. Evtimov I, Eykholt K, Fernandes E, Kohno T, Li B, Prakash A, Rahmati A, Song D  2017   Robust physical-world attacks on deep learning models. arXiv:1707.08945  28. Melis M, Demontis A, Biggio B, Brown G, Fumera G, Roli F  2017  Is deep learning safe for robot vision? Adversarial examples against the iCub Humanoid. In: ICCV Workshop on Vision in Practice on Autonomous Robots, Venice, Italy, 23 Oct 2017. arXiv:1708.06939  29. Grosse K, Manoharan P, Papernot N, Backes M, McDaniel P  2017  On the statistical detection  of adversarial examples. arXiv:1702.06280  30. Gong Z, Wang W, Ku WS  2017  Adversarial and clean data are not twins. arXiv:1704.04960 31. Carlini N, Wagner D  2017  Adversarial examples are not easily detected: bypassing ten detec- tion methods. In: Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Secu- rity. ACM, New York, pp. 3–14. https:  doi.org 10.1145 3128572.3140444  32. Papernot N, McDaniel P, Wu X, Jha S, Swami A  2016  Distillation as a defense to adversarial perturbations against deep neural networks. In: 2016 IEEE Symposium on Security and Privacy.   3 The Security of Machine Learning Systems  79  IEEE Computer Society, Los Alamitos, CA, USA, pp 582–597. https:  doi.org 10.1109 SP. 2016.41  33. Carlini N, Wagner D  2016  Defensive distillation is not robust to adversarial examples.  arXiv:1607.04311  34. Bhagoji AN, Cullina D, Mittal P  2017  Dimensionality reduction as a defense against evasion  attacks on machine learning classiﬁers. arXiv:1704.02654v2  35. Maclaurin D, Duvenaud D, Adams R  2015  Gradient-based hyperparameter optimization through reversible learning. In: Bach F, Blei D  eds  Proceedings of the 32nd International Conference on Machine Learning, pp 2113–2122   Chapter 4 Patch Before Exploited: An Approach to Identify Targeted Software Vulnerabilities  Mohammed Almukaynizi, Eric Nunes, Krishna Dharaiya, Manoj Senguttuvan, Jana Shakarian, and Paulo Shakarian  Abstract The number of software vulnerabilities discovered and publicly disclosed is increasing every year; however, only a small fraction of these vulnerabilities are exploited in real-world attacks. With limitations on time and skilled resources, orga- nizations often look at ways to identify threatened vulnerabilities for patch prioritiza- tion. In this chapter, an exploit prediction model is presented, which predicts whether a vulnerability will likely be exploited. Our proposed model leverages data from a variety of online data sources  white hat community, vulnerability research commu- nity, and dark web deep web  DW  websites  with vulnerability mentions. Compared to the standard scoring system  CVSS base score  and a benchmark model that lever- ages Twitter data in exploit prediction, our model outperforms the baseline models with an F1 measure of 0.40 on the minority class  266% improvement over CVSS base score  and also achieves high true positive rate and low false positive rate  90%, 13%, respectively , making it highly effective as an early predictor of exploits that could appear in the wild. A qualitative and a quantitative study are also conducted to investigate whether the likelihood of exploitation increases if a vulnerability is mentioned in each of the examined data sources. The proposed model is proven to be much more robust than adversarial examples—postings authored by adversaries in the attempt to induce the model to produce incorrect predictions. A discussion on the viability of the model is provided, showing cases where the classiﬁer achieves high performance, and other cases where the classiﬁer performs less efﬁciently.  4.1 Introduction  An increasing number of software vulnerabilities are discovered and publicly dis- closed every year. A vulnerability is a weakness in a software product that can be exploited by an attacker to compromise the conﬁdentiality, integrity, or availabil- ity of the system hosting that product and cause harm [1]. The National Institute  M. Almukaynizi  B  · E. Nunes · K. Dharaiya · M. Senguttuvan · J. Shakarian · P. Shakarian  Arizona State University, Tempe, AZ, USA e-mail: malmukay@asu.edu    Springer Nature Switzerland AG 2019 L. F. Sikos  ed. , AI in Cybersecurity, Intelligent Systems Reference Library 151, https:  doi.org 10.1007 978-3-319-98842-9_4  81   82  M. Almukaynizi et al.  of Standards and Technology  NIST 1 maintains a comprehensive list of publicly disclosed vulnerabilities in its National Vulnerability Database  NVD .2 The NVD also provides information regarding target software products  CPE ,3 severity rat- ing  CVSS 4 in terms of exploitability and impact, and the date a vulnerability was published.  In 2016 alone, at least 6,000 vulnerabilities were disclosed in the NVD. This has risen to over 14,500 vulnerabilities in 2017. Once the vulnerabilities are publicly disclosed, the likelihood of their exploitation increases [2]. With limited resources, organizations often look to prioritize which vulnerabilities to patch by assessing the impact they will have on the organization if exploited. An exploit is deﬁned as a piece of code that modiﬁes the functionality of a system using an existing vulnerability [3]. In this chapter, the exploits that have been used to target systems in real-world attacks are referred to as real-world exploits. In contrast, Proof-of-Concept  PoC  exploits are typically developed to verify the existence of a reported ﬂaw in order to reserve a CVE-ID or to illustrate how a vulnerability can be exploited. PoCs generally require additional functionalities to be weaponized and be useful by malicious hackers. Although the chances of detecting real-world exploits of a particular vulnerability if a PoC is already present are high, the presence of a PoC does not imply that it has been used in the wild.  To be on the safe side, standard risk assessment systems, such as Common Vulner- ability Scoring System  CVSS , Microsoft Exploitability Index,5 and Adobe Priority Rating6 report many vulnerabilities to be severe. The foregoing systems are broadly viewed as guidelines to supply vulnerability management teams with tools that help in patch prioritization. One commonality across those systems is that they rank vul- nerabilities based on historical attack patterns that are relevant to the technical details of vulnerabilities that are evaluated, rather than what hacker communities discuss and circulate in the underground forums and marketplaces. This does little to alleviate the problem, because the majority of ﬂagged vulnerabilities will not be attacked [4]. Furthermore, current methods of patch prioritization appear to fall short [4]. Verizon reported that over 99% of breaches are caused by exploits to known vulnerabilities. Cisco also reported that “the gap between the availability and the actual implementation of such patches is giving attackers an opportunity to launch exploits.”7 For some vulnerabilities, the time window to patch the system is very small. For instance, exploits targeting the Heartbleed8 bug in the OpenSSL9  1https:  www.nist.gov 2https:  nvd.nist.gov 3https:  nvd.nist.gov cpe.cfm 4https:  nvd.nist.gov vuln-metrics cvss 5https:  technet.microsoft.com en-us security cc998259.aspx 6https:  helpx.adobe.com security severity-ratings.html 7https:  www.cisco.com c dam m en_ca never-better assets ﬁles midyear-security-report-2016. pdf 8http:  heartbleed.com 9https:  www.openssl.org   4 Patch Before Exploited: An Approach …  83  cryptographic software library were detected in the wild 21 hours after the vulnera- bility was publicly disclosed [5]. Hence, organizations need to efﬁciently assess the likelihood that a vulnerability is going to be exploited in the wild, while keeping the false alarm rate low.  NIST provides the National Vulnerability Database  NVD , which contains a comprehensive list of vulnerabilities. Yet, only a small fraction  less than 3%  of these vulnerabilities are exploited in the wild [4, 6–9]—a result conﬁrmed in this chapter. In addition, a previous work has found that the CVSS score provided by NIST is not an effective predictor of vulnerabilities being exploited [4]. It has previously been proposed that other methods such as the use of social media [8, 10], dark web markets [11–13], and certain white hat10 websites like Contagio11 would be suitable alternatives. However, these approaches have their limitations. For instance, methodical concerns on the use of social media for exploit prediction were recently raised [14]; and data feeds for exploit and malware were limited to single sites and were only used for analysis to provide insights on economic factors of those sites [12, 13]. While other studies demonstrate the viability of data collection, they do not quantify the results of prediction [10, 11].  In this chapter, the potential of identifying software vulnerabilities that will likely be exploited in real-world cyberattacks is investigated. In this effort, cyberthreat intelligence feeds are used, which are aggregated from a variety of sources. This problem is directly related to patch prioritization. Previous works on online vulner- ability mentions either studied the correlation between such feeds and the existence of exploits in the wild [4, 15], or developed machine learning models to predict the availability of Proof-of-Concept exploits [6, 14, 16]. However, only a small fraction of vulnerabilities having PoCs are actually exploited in the wild.  After reviewing the literature, including studies on data gathered from dark web and deep web [4, 11, 17–19], conducting analyses on data feeds collected from vari- ous online sources  e.g., SecurityFocus,12 Talos ,13 and after over one hundred inter- views with professionals working for managed security service providers  MSSPs ,14 ﬁrms specializing in cyber-risk assessment, and security specialists working for man- aged  IT  service providers  MSPs , three data sources have been identiﬁed that can represent the current threat intelligence used for vulnerability prioritization:  1  Exploit-DB  EDB 15 contains information on Proof-of-Concept exploits for vulner- abilities provided by security researchers from various blogs and security reports,  2   10Ethical  white hat  hacker is a person who practices hacking activities against some computer network to identify its weaknesses and assess its security, rather than having malicious intent or seeking personal gain. 11http:  contagiodump.blogspot.com 12http:  www.securityfocus.com 13https:  www.talosintelligence.com vulnerability_reports 14An MSSP is a service provider that provides its clients with tools that continuously monitor and manage wide range of cybersecurity-related activities and operations, which may include threat intelligence, virus and spam blocking, and vulnerability and risk assessment. 15https:  www.exploit-db.com   84  M. Almukaynizi et al.  Zero Day Initiative  ZDI 16 is curated by the commercial ﬁrm TippingPoint and uses a variety of reported sources focusing on disclosures by various software vendors and their security researchers, and  3  a collection of information scraped from over 120 sites on DW sites from a system introduced in [20, 21] and currently maintained by the cybersecurity ﬁrm CYR3CON.17 The intuition behind each of these feeds was not only to utilize information that was aggregated over numerous related sources, but also to represent feeds commonly used by cybersecurity professionals.  This chapter focuses on vulnerabilities that have publicly been disclosed in 2015– 2016. The presented models employ supervised machine learning techniques using Symantec18 attack signatures as ground truth.   The utility of the proposed machine learning models in predicting exploits in the wild is demonstrated with true positive rate  TPR 19 of 90% while maintaining a false positive rate  FPR 20 of less than 15%. In addition, the proposed model is compared to a recent benchmark model that utilized online mentions for exploit prediction [8]. The proposed model achieves a signiﬁcantly higher precision while maintaining a recall under the assumptions made. The performance of variants of the presented model is examined, when both temporal mixing and the case when only a single source is used are considered. The robustness of the presented model against various adversarial data manipulation strategies is also discussed.   Using vulnerability mentions on EDB, ZDI, and DW, the increase in the vulner- ability exploitation likelihood over vulnerabilities only disclosed on the NVD is studied. Results are provided to demonstrate the likelihood of exploitation given the vulnerability mention on EDB  9% , ZDI  12%  and DW  14%  and is com- pared to the NVD  2.4% . The availability of such information relative to the time an exploit is found in the wild is also studied.   Exploited vulnerabilities are analyzed based on various other features derived from these data sources, such as the language used. Apparently, Russian language sites on the dark web discussing vulnerabilities are 19 times more likely to be exploited than random vulnerabilities, more likely than vulnerabilities described on websites in any of the other languages. Additionally, the probability of exploitation is investigated in terms of both data source and software vendor.  The rest of the chapter is organized as follows. State-of-the-art approaches are discussed in Sect. 4.2. Section 4.3 outlines some challenges related to the problem addressed in this chapter. Section 4.4 provides an overview of the presented exploit prediction model and describes the data sources used. Vulnerability analysis is dis- cussed in Sect. 4.5. In Sect. 4.6, experimental results are provided for predicting the  16http:  www.zerodayinitiative.com 17https:  www.cyr3con.ai 18https:  www.symantec.com 19TPR is a metric that measures the proportion of exploited vulnerabilities that are correctly pre- dicted from all exploited vulnerabilities. 20FPR is a metric that measures the proportion of non-exploited vulnerabilities that are incorrectly predicted as being exploited from the total number of all non-exploited vulnerabilities.   4 Patch Before Exploited: An Approach …  85  likelihood of vulnerability exploitation. The robustness of the presented machine learning model in comparison with adversarial data manipulation is demonstrated in Sect. 4.7. A discussion on the viability of the proposed model and the cost of misclassiﬁcation are provided in Sect. 4.8.  4.2 Related Work  Predicting cybersecurity events is one of those domains that have recently received a growing attention [6, 16, 22–24]. However, only little work in this line of research has been proposed so far compared to works proposed for detecting cyberthreats after they occur. Predicting whether a disclosed vulnerability will be exploited in the wild is important to organizations for prioritizing vulnerability patching. Previous studies attempted to address this problem using both a standard scoring system, in particular the Common Vulnerability Scoring System  CVSS , and machine learning techniques. An approach to predict the likelihood that a software contains a yet-to-be- discovered vulnerability was proposed by Zhang et al. [25]. In this study, the National Vulnerability Database  NVD  was leveraged to predict the time a next vulnerability will be discovered in a software. The results showed a poor predictive capability of the NVD data, because the approach only uses the Common Vulnerability Scoring System  CVSS  and the Common Platform Enumeration  CPE  as features, but not the NVD description. CVSS version 2.0 is a poor indicator of predicting whether a vulnerability will be exploited [4]. Apparently, deciding to patch a vulnerability because of a high CVSS score is equivalent to randomly guessing which vulnera- bility to patch. Moreover, integrating information about whether a PoC exploit is available is a good predictor of exploitation. In our approach, CVSS scores are used as indicators.  Our approach closely resembles previous work on using publicly disclosed vulner- ability information as features to train machine learning models to predict whether a given vulnerability will be exploited. Bozorgi et al. [16] proposed a model that engineered features from two online sources, namely the now-discontinued Open Source Vulnerability Database  OSVDB  and the NVD to predict whether PoCs will be developed for a particular vulnerability. In their data, 73% of the vulnerabilities were reported as exploited, which is signiﬁcantly higher than the ones reported in the literature  1.3%  [4, 7, 8]. The reason behind this is that the authors considered PoC as an indicator of whether the vulnerability will be exploited, which is not true in most cases. Using this assumption, and using a support vector machine classiﬁer, their approach predicts whether vulnerabilities will have PoCs available, a problem that is different from the one studied in this chapter. A similar technique used the NVD as the data source, and Exploit-DB as the ground truth, with 27% vulnerabil- ities marked as exploited  have PoC exploits  [6]. They achieve high accuracy on a balanced dataset. Our analysis aims to predict vulnerabilities that will be used in real-world attacks and not just the ones that have PoC exploits available.   86  M. Almukaynizi et al.  Building on the work on using publicly disclosed vulnerabilities, Sabottle et al. looked at predicting the exploitability based on vulnerabilities disclosed in Twitter21 [8]. They collected Twitter data that have references to CVE-IDs and use these tweets22 to compute features. A linear support vector machine  SVM  classiﬁer was trained for prediction. As the source of ground truth data, Symantec threat signatures were used to label positive samples. Compared to previous predictive studies, even though [8] maintained the class ratio of 1.3% vulnerabilities exploited, this study used a resampled and balanced dataset to report the results. Additionally, the temporal aspect  training data should precede testing  of the tweets was not maintained while performing the experiments. This temporal intermixing caused future events to inﬂuence the prediction of past events, a practice that is known to lead to unrealistic predictions [14]. In this chapter, the class imbalance and the temporal aspect are respected while reporting the results.  In the literature, the impact of adversarial interference for hackers aiming to poison and evade the machine learning models has been discussed. Hao et al. proposed a prediction model to predict the web domain abuse based on features derived from the behavior of users at the time of registration [26]. They studied different evading strategies attackers can use, and demonstrated that evading attempts are expensive to attackers, and their model’s reliance on different sets of features allows for limiting the decrease in false positive rate. Other researchers have also discussed the robustness of their models against adversarial attacks  e.g., [8, 22] . In this work, simulation experiments have been executed, which pointed out that the impact is very limited, as discussed in Sect. 4.7.  4.3 Preliminaries  To see how our proposed model works, some machine learning approaches and known challenges have to be discussed ﬁrst.  4.3.1 Supervised Learning Approaches  The machine learning approaches used in this study are the following:   Support Vector Machine  SVM . Introduced by Cortes and Vapnik, SVM attempts to ﬁnd a separating margin that maximizes the geometric distance between classes [27]  in the case of vulnerabilities, exploited and not exploited . The sepa- rating margin is called a hyperplane. When a separating plane cannot be found to distinguish between the two classes, the SVM cost function includes a regulariza-  21https:  twitter.com 22Twitter posts, called tweets, are limited to 280 characters.   4 Patch Before Exploited: An Approach …  87  d  i=1 Pr aic .  tion penalty and a slack variable for the misclassiﬁed samples. By varying these parameters, different trade-offs can be achieved between precision and recall.   Naïve Bayes Classiﬁer  NB . Naïve Bayes is a probabilistic classiﬁer, which uses Bayes theorem with independent attribute assumption. During training, the conditional probabilities of a sample of a given class having a certain attribute are computed. The prior probabilities for each class are also computed, i.e., the fraction of the training data belonging to each class. Naïve Bayes assumes that the attributes are statistically independent; therefore the likelihood for a sample S represented with a set of attributes a associated with a class c is given as Pr cS  = P c  ×  cid:2    Bayesian Network  BN . BN is a generalized form of NB such that not all features are assumed to be independent. Instead, variable dependencies are modeled in a graph leaned from the training data.   Decision Tree  DT . Decision tree is a hierarchical recursive partitioning algo- rithm. A decision tree is built by ﬁnding the best split attribute, i.e., the attribute that maximizes the information gain at each split of a node. In order to avoid overﬁtting, the terminating criteria is set to less than 5% of the total samples.   Logistic Regression  LOG-REG . Logistic regression classiﬁes samples by com- puting the odds ratio. The odds ratio gives the strength of association between the attributes and the class.  4.3.2 Challenges of Exploit Prediction  The methodological issues of exploit prediction [14], along with ﬁnding a balance between conducting an evaluation under real-world conditions and with an adequate sample size, pose three challenges:   Class Imbalance. As mentioned earlier, evidence of real-world exploits is found for only around 2.4% of the reported vulnerabilities. This skews the distribu- tion towards one class in the prediction problem  i.e., not exploited . In such cases, standard machine learning approaches favor the majority class, leading to poor performance on the minority class. Some of the prior work in predicting the likelihood of exploitation considers the existence of PoCs as an indicator of real-world exploitations, which substantially increases the number of exploited vulnerabilities in the studies adopting this assumption [6, 14, 16]. However, out of the PoC exploits that are identiﬁed, only a small fraction are ever used in real- world attacks [7]—a result conﬁrmed in this chapter  e.g., only about 4.5% of the vulnerabilities having PoCs were subsequently exploited in the wild . Other prior work uses class balancing techniques on both training and testing datasets and reports performance achieved using metrics like TPR, FPR, and accuracy23  23Note that these metrics are sensitive to the underlying class distribution and sensitive to the ratio of class rebalancing.   88  M. Almukaynizi et al.  [22, 23]. Resampling the data to balance both classes in the dataset leads to train- ing the classiﬁer on a data distribution that is highly different than the underlying distribution. The impact of this manipulation, whether positive or negative, can- not be observed when testing the same classiﬁer on a manipulated dataset, e.g., a testing set with the same rebalancing ratio. Hence, the prediction results of the proposed models in deployed, real-world settings are debatable. To conﬁrm the impact of the highly imbalanced dataset used on the machine learning models, oversampling techniques  in particular SMOTE [28]  are examined. Note that the testing dataset is not manipulated because of the aim to observe a performance that can be reproduced in the settings of a model running on real-world deployment  e.g., streaming predictions . Doing so, only a marginal improvement is observed for some classiﬁers, as reported in Sect. 4.6.2, while other classiﬁers have shown a slightly negative impact when they are trained on an oversampled dataset.   Evaluating Models on Temporal Data. Machine learning models are evaluated by training the model on one set of data, and then testing the model on another set that is assumed to be drawn from the same distribution. The data split can be done randomly or in a stratiﬁed manner, where the class ratio is maintained in both training and testing. Exploit prediction is a time-dependent prediction problem. Hence, splitting the data randomly violates the temporal aspect of the data—as events that happen in the future will now be used to predict events that happened in the past. Prior research efforts ignored this aspect while designing experiments [8]. In this work, this temporal mixing is avoided in most experiments. However, experiments with a very small sample size, in which this is not controlled, are included  this is because one of the used ground truth sources does not have date time information . It is explicitly noted when this is the case.   Limitations of Ground Truth. As mentioned, attack signatures reported by Symantec are used as the ground truth of the exploited vulnerabilities, similar to previous works [4, 8]. This ground truth is not comprehensive because the dis- tribution of the exploited vulnerabilities over software vendors is found to differ from that for overall vulnerabilities  i.e., vulnerabilities that affect Microsoft prod- ucts have a good coverage compared to products of other OS vendors . Although this source is limited in terms of coverage [8], it is still the most reliable source of exploited vulnerabilities because it reports attack signatures of exploits detected in the wild for known vulnerabilities. Other sources either report whether a software is malicious without proper mapping to the exploited CVE-ID  e.g., VirusTotal ,24 or rely on online blogs and social media sites to identify exploited vulnerabilities  e.g., SecurityFocus .25 In this chapter, Symantec data is used while taking into account the false negatives. To avoid overﬁtting the machine learning model on this not-so-representative ground truth, the software vendor is omitted from the set of examined features.  24https:  www.virustotal.com 25https:  www.securityfocus.com There are many examples where attack signatures are reported by Symantec, but not reported by SecurityFocus. Also, there are vulnerabilities SecurityFocus reports as exploited, and those exist in software whose vendors are well-covered by Symantec, yet Symantec does not report them.   4 Patch Before Exploited: An Approach …  89  4.4 Exploit Prediction Model  Using machine learning models to address exploit prediction has interesting security implications in terms of prioritizing which vulnerabilities need to be patched ﬁrst to minimize the risk of cyberattacks. Figure 4.1 gives an overview of the proposed exploit prediction model.  This model consists of the following phases:  1. Data Collection: Three data sources are used in addition to the NVD. These data sources are EDB  Exploit-DB , ZDI  Zero Day Initiative  and data mined from DW markets and forums, focusing on malicious hacking. Ground truth is assigned to the binary classiﬁcation problem studied in this chapter using Symantec attack signatures of exploits detected in the wild. The data sources are described in Sect. 4.4.1.  2. Feature Selection: Features are extracted from each of the data sources. The fea- tures include bag-of-words features for vulnerability description and discussions on the DW, binary features that check for the presence of PoC exploits in EDB, vulnerability disclosures in ZDI and DW. Additional features are also included from the NVD, including CVSS scores and CVSS vectors.  3. Prediction: Binary classiﬁcation is performed on the selected features to deter- mine whether the vulnerability will likely be exploited or not. To address this classiﬁcation problem, several standard supervised machine learning approaches are evaluated.  Symantec attack signatures   exploits detected in the wild   Ground truth  Feature  selection  Machine  learning  Exploit prediction  CVSS and   description features  Proof-of-concept   feature  Vulnerability disclosure   feature  Vulnerability description   language  NVD  Exploit-DB  Zero Day  Initiative  Dark Web  Fig. 4.1 Exploit prediction model   90  4.4.1 Data Sources  M. Almukaynizi et al.  Our analysis combines vulnerability and exploit information from multiple open source databases, namely NVD, EDB, ZDI, and DW. The DW database was obtained from an API maintained by CYR3CON. Our experiments cover vulnerabilities pub- lished in 2015–2016. Table 4.1 shows the vulnerabilities identiﬁed from each of the data sources between 2015 and 2016, as well as the number of vulnerabilities that were exploited in real-world attacks.  A brief overview of each of these data sources is provided in the following sections.  4.4.1.1 National Vulnerability Database  The National Vulnerability Database is a database of publicly disclosed vulnerabil- ities. Each vulnerability is identiﬁed using a unique CVE-ID. Our dataset contains 12,598 vulnerabilities. Figure 4.2 shows the disclosure of vulnerabilities per month. At the time of data collection in December 2016, there were only 30 vulnerabil- ities disclosed, hence the small bar at the end of 2016. For each vulnerability, the description, the CVSS score and vector, and the publication date were collected. Organizations often use the CVSS score to prioritize the vulnerabilities to patch. The  Table 4.1 Number of vulnerabilities  2015–2016  Database NVD EDB ZDI DW  Vulnerabilities 12,598 799 824 378  Exploited 306 74 95 52  % exploited 2.4 9.2 11.5 13.8  800  600  400  200  0  s e i t i l i b a r e n l u v   f o     01 02 03 04 05 06 07 08 09 10 11 12 01 02 03 04 05 06 07 08 09 10 11 12  2015  Month  2016  Fig. 4.2 Vulnerabilities disclosed per month   4 Patch Before Exploited: An Approach …  91  CVSS vector lists the components from which the score is computed. More details about CVSS components are provided in Sect. 4.4.2.  4.4.1.2 Exploit Database  The Exploit Database of the white hat community is an archive of PoC exploits main- tained by Offensive Security.26 PoC exploits for known vulnerabilities are reported with CVE-IDs of target vulnerabilities. Using the unique CVE-IDs from the NVD for the time period between 2015 and 2016, EDB was queried to ﬁnd out whether a PoC exploit is available. The availability date of PoCs has also been recorded. By querying EDB, veriﬁed PoCs have been found for 799 of the vulnerabilities studied.  4.4.1.3 Zero Day Initiative  The Zero Day Initiative maintains a database of vulnerabilities that are identiﬁed and reported by security researchers. Reported software ﬂaws are ﬁrst veriﬁed by ZDI before disclosure. Monetary incentives are provided to researchers who report valid vulnerabilities. Before ZDI publicly discloses a vulnerability, the software vendors of the target products are notiﬁed and allowed time to implement patches. The ZDI database has been queried for the examined vulnerabilities, and found 824 common CVE-IDs between the NVD and ZDI. The publication date has also been noted.  4.4.1.4 Dark Web and Deep Web  The data collection infrastructure maintained by CYR3CON was originally described in [20]. In this paper, the authors built a system that crawls sites on DW, both market- places and forums, to collect data relating to malicious hacking. They ﬁrst identify sites before developing scripts for automatic data collection. A site is put forward to script development after it has been determined whether the content is of inter- est  i.e., hacking-related  and is relatively stable. The population size of a site is observed, though not much decisive power is assigned to it. While a large population is an indicator for the age and stability of a site, a small population number can be associated with higher-value information  i.e., closed forums .  DW users advertise and sell their products on marketplaces. Hence, DW market- places provide a new avenue to gather information about vulnerabilities and exploits. Forums, on the other hand, feature discussions on kits of newly discovered vulner- abilities and exploits. Data related to malicious hacking is ﬁltered from the noise and added to a database using a machine learning approach with high precision and recall. Not all exploits or vulnerability items in the database have an associated CVE number. First, the database was queried to extract all items with CVE mentions. Some  26https:  www.offensive-security.com   92  M. Almukaynizi et al.  vulnerabilities are mentioned in DW by their Microsoft Security Bulletin Number27  e.g., MS16-006 . Every bulletin number was mapped to its corresponding CVE-ID, making ground truth assignment easy. These items can be both products sold on mar- kets and posts extracted from forums discussing topics related to malicious hacking. 378 unique CVE mentions have been found between 2015 and 2016 on more than 120 DW websites, much more than what a previous work discovered [4]. In addition, the posting date and descriptions associated with all the CVE mentions have also been queried, including product title and description, vendor information, the entire discussion with the CVE mention, post author, and the topic of the discussion.  4.4.1.5 Attack Signatures  Ground Truth   For ground truth, vulnerabilities that were exploited in the wild using Symantec antivirus attack signatures28 and Intrusion Detection Systems  IDS  attack signa- tures29 have been identiﬁed. The attack signatures are associated with the CVE-ID of the vulnerability that was exploited. These CVEs have been mapped to the CVEs mined from the NVD, EDB, ZDI, and DW. This ground truth indicates actual exploits that were used in the wild  not PoC exploits . For the NVD, around 2.4% of the dis- closed vulnerabilities were exploited, which is consistent with the literature. In addi- tion, for EDB, ZDI, and DW there is a signiﬁcant increase in exploited vulnerabilities to 9%, 12%, and 14%, respectively. This shows that it is more likely to identify a vulnerability that will be exploited in the future if it has a PoC available in EDB or mentions in ZDI or DW. For this research, no data is available about the volume and frequency of the attacks carried by exploits; hence all exploited vulnerabilities are considered equally important. This assumption has been adopted by previous works as well [4, 8]. Additionally, the exploitation date of a vulnerability is deﬁned as the date it was ﬁrst detected in the wild. Symantec IDS attack signatures are reported without recoding the dates when they were ﬁrst detected, but its anti-virus attack signatures are reported with their exploitation date. Between 2015 and 2016, 112 attack signatures have been reported without and 194 with their discovery date.  4.4.2 Feature Description  In this section, features from all the data sources discussed in the previous section are combined, including bag-of-words, numerical, categorical, and binary features  see Table 4.2 .  These features are discussed in the following sections in more detail.  27https:  technet.microsoft.com en-us security bulletins.aspx 28https:  www.symantec.com security-center a-z 29https:  www.symantec.com security_response attacksignatures    4 Patch Before Exploited: An Approach …  93  Table 4.2 Summary of features  Type Feature TF-IDF on bag of words NVD and DW description Numeric and categorical CVSS Categorical DW language Binary Presence of PoC Vulnerability mention on ZDI Binary Vulnerability mention on DW Binary  4.4.2.1 NVD and DW Description  The NVD description provides information on the vulnerability and what it allows hackers to do when they exploit it. DW description often provides rich context on what the discussion is about, and is often synthesized from forums rather than mar- ketplaces since items are described in fewer words. Patterns can be learned based on this textual content. The description of published vulnerabilities have been obtained from the NVD, and the DW database was queried for CVE mentions between 2015 and 2016. This description was appended to the NVD description with the corre- sponding CVE. Note that some of the descriptions on DW are in a foreign language as discussed in Sect. 4.5, which have been translated to English using the Google Translate API.30 Then the text features were vectorized using the frequency-inverse document frequency  TF-IDF  model learned from the training set and used it to vectorize the testing set. TF-IDF creates a vocabulary of all the words in the descrip- tion. The importance of a word feature increases with the number of times it occurs, but is normalized by the total number of words in the description. This eliminates common words from being important features. Our TF-IDF model was limited to the 1,000 most frequent words  using more word features has no beneﬁt in terms of performance, however, it would increase the computational cost .  4.4.2.2 CVSS  NVD provides a CVSS score and the CVSS vector from which the score can be computed, indicating the severity of each disclosed vulnerability. CVSS version 2.0 was used, rather than version 3.0, because the latter is only present for a fraction of the studied vulnerabilities. The CVSS vector lists the components from which the score is computed. The components of the vector include Access Complexity, Authentication, Conﬁdentiality, Integrity, and Availability. Access Complexity indicates how difﬁcult it is to exploit the vulnerability once the attacker has gained access. It is deﬁned in terms of three levels: High, Medium, and Low. Authentication indicates whether  30https:  cloud.google.com translate docs   94  M. Almukaynizi et al.  authentication is required by the attacker to exploit the vulnerability. It is a binary identiﬁer taking the values Required and Not Required. Conﬁdentiality, Integrity, and Availability indicate what loss the system would incur if the vulnerability is exploited. They take the values None, Partial, and Complete. All the CVSS vector features are categorical. These features are vectorized by building a vector with all possible categories. Then if that category is present, 1 is inserted, otherwise 0.  4.4.2.3 Language  DW feeds are posted in different languages. The four most frequently used languages in DW posts that describe vulnerabilities are English, Chinese, Russian, and Swedish. Since there is limited number of non-English postings, giving the model little chance to learn adequate representation for each language, text translation was used, as mentioned earlier. To this end, while translation can result in a loss of important information, the impact can be minimized by using the language as a feature. The analysis on the languages of DW feeds and their variation in the exploitation rate are detailed in Sect. 4.5.  4.4.2.4 Presence of Proof-of-Concept  The presence of PoC exploits in EDB increases the likelihood of vulnerability exploitation. It can be treated as a binary feature that indicates whether a PoC is present for a vulnerability or not.  4.4.2.5 Vulnerability Mention on ZDI  ZDI acts similar to the NVD, i.e., both disclose software vulnerabilities. Given that a vulnerability is disclosed on ZDI, its exploitation likelihood raises. Similar to the presence of PoCs, a binary feature can be used to denote whether a vulnerability was disclosed in ZDI before it is exploited.  4.4.2.6 Vulnerability Mention on the Dark Web or Deep Web  A binary feature can be used to indicate whether a vulnerability is mentioned on the Dark Web or the Deep Web.   4 Patch Before Exploited: An Approach …  95  4.5 Vulnerability and Exploit Analysis  To assess the importance of aggregating different data sources for early identiﬁ- cation of threatened vulnerabilities, ﬁrst the likelihood of exploitation is analyzed provided that a vulnerability is mentioned by each data source. Time-based analysis is then performed for the exploited vulnerabilities that have reported exploitation dates  n = 194  to show the difference in days between when vulnerabilities are exploited and when they are mentioned online. Our time-based analysis ignores the exploited vulnerabilities without reported dates, because no assumption can be made about their exploitation dates.  To identify the vendors of highly vulnerable software and systems, the ground truth has to be analyzed and compared to other sources. As previous works described, Symantec reports attack signatures for vulnerabilities of certain products [4, 8]. To show the variation in vendor coverage attained from various data sources, the distribution of the target software vendors are studied per data source. This analysis is based on the vendor mentions by CPE data from the NVD. Note that a vulnerability can appear in various software versions, including versions developed for different platforms, and therefore a single vulnerability can potentially be mapped to multiple software vendors. Finally, a language-based analysis is provided on Deep Web Dark Web data to reveal some socio-cultural factors present in the Deep Web and the Dark Web that seem to affect the likelihood of exploitation.  4.5.1 Likelihood of Exploitation  For each data source, Table 4.3 shows the vulnerability exploitation probability for the vulnerabilities mentioned in that data source. This analysis emphasizes the value of open data sources in supplementing the NVD data. As mentioned in Sect. 4.4.1, approximately 2.4% of the vulnerabilities in NVD are exploited in the wild. Hence, including other sources can increase the likelihood of correctly identifying the vul- nerabilities that will be exploited.  Table 4.3 Number of vulnerabilities, number of exploited vulnerabilities, fraction of exploited vulnerabilities that appeared in each source, and fraction of total vulnerabilities that appeared in each source. Results are reported for vulnerabilities and exploited vulnerabilities appeared in EDB, ZDI, DW  distinct CVEs , CVEs in ZDI or DW, and CVEs in any of the three sources  Number of vulnerabilities Number of exploited vulnerabilities Percentage of exploited vulnerabilities 21% Percentage of total vulnerabilities  EDB 799 74  ZDI 824 95 31%  ZDI DW EDB ZDI DW 1,180 140 46% 6.3% 6.5% 3.0% 9.3%  1,791 164 54% 14.2%  DW 378 52 17%   96  M. Almukaynizi et al.  4.5.2 Time-Based Analysis  Most software systems are attacked repeatedly using vulnerabilities after exploits to such vulnerabilities have been detected in the wild [29]. As a matter of fact, a long time may pass between the date the vulnerabilities are disclosed and the date they are patched by vulnerability management teams. Here, only the population of exploited vulnerabilities that are reported with their exploitation date are analyzed  194 vulnerabilities .  Figure 4.3 shows that for more than 93% of the vulnerabilities, they are disclosed by NIST before any real-world attacks are detected. In the other few cases, attacks were detected in the wild before NIST published the vulnerabilities  i.e., zero-day attacks . This could be caused by  1  the vulnerability information is sometimes leaked before the disclosure,  2  by the time NIST disclosed a vulnerability in the NVD, other sources have already validated and published it, then exploits rapidly started using it in real-world attacks, or  3  the attacker knew that what they were doing was successful and continued to exploit their targets until discovered [2]. Additionally, ZDI and NVD have limited variation on the vulnerability disclosure dates  median is 0 days . It is important to note that because ZDI disclosures come from the industry, reserved CVE numbers are shown earlier here than in other sources. In the case of EDB, almost all of the exploited vulnerabilities that have PoCs archived were found in the wild within the ﬁrst 100 days of the PoCs availability. Such a short time period between the availability of PoCs and actual real-world attacks indicates that having a template for exploits  in this case PoCs  makes it easy for hackers to conﬁgure and use in real-world attacks. Figure 4.4 shows the difference in days between the availability of PoCs and exploitation dates.  In the case of the DW database, more than 60% of the ﬁrst-time mentions to the exploited vulnerabilities are within 100 days before or after the exploitation dates.  Fig. 4.3 Day difference between the CVE ﬁrst published in the NVD and the Symantec attack signature date versus the fraction of exploited CVEs the NVD reported  cumulative    4 Patch Before Exploited: An Approach …  97  Fig. 4.4 Day difference between the date of availability of PoC on EDB and the Symantec attack signature date versus the fraction of exploited CVEs with PoCs EDB reported  cumulative   The remaining mentions are within the 18 months time frame after the vulnerability exploitation date  see Fig. 4.5 .  4.5.3 Vendor- Platform-Based Analysis  As noted, Symantec reports vulnerabilities that attack the systems and software con- ﬁgurations used by their customers. For the studied vulnerabilities, more than 84% and 36% of the exploited vulnerabilities reported by Symantec exist in products solely from, or run on, Microsoft and Adobe products, respectively; whereas less than 16% and 8% of vulnerabilities published in the NVD are related to Microsoft and Adobe, respectively. Figure 4.6 shows the percentage from the exploited vulnerabilities that  Fig. 4.5 Day difference between CVE ﬁrst mentioned in DW and Symantec attack signature date versus the fraction of exploited CVEs reported on DW  cumulative    98  M. Almukaynizi et al.  Fig. 4.6 Most exploited vendors in each data source  -  can affect each of the top ﬁve vendors in every data source. It is important to note that a vulnerability may affect more than a single vendor  e.g., CVE-2016-4272 exists in Adobe Flash Player,31 and it allows attackers to execute arbitrary code via unspeci- ﬁed vectors and can affect products from all ﬁve vendors . Additionally, the absence of vulnerabilities detected in other important systems and software vendors from Symantec’s dataset does not imply that they have not been exploited; rather, they are not detected by Symantec  i.e., false negatives . Furthermore, the presence of some operating systems  e.g., Linux  in the exploited vulnerabilities does not necessarily imply good coverage of Symantec data to these systems; however, other exploited products can run on these operating systems.  Furthermore, DW data appears to have more uniform vendor coverage. Only 30% and 6.2% of the vulnerabilities mentioned in DW during the time period of this study were related to Microsoft and Adobe, respectively. Additionally, ZDI favors products from these two vendors  57.8% in the case of Microsoft and 35.2% in the case of Adobe . This provides evidence that each data source cover vulnerabilities that target various sets of software vendors.  4.5.4 Language-Based Analysis  Interestingly, notable variations have been found on the exploitation likelihood w.r.t. the language used on Dark Web and Deep Web data feeds that reference CVEs. In DW feeds, four languages are detected with different vulnerability post and item distri- butions. Not surprisingly, English and Chinese have far more vulnerability mentions  242 and 112, respectively  than Russian and Swedish  13 and 11, respectively . How- ever, vulnerabilities mentioned in Chinese postings are characterized by the lowest  31https:  www.adobe.com products ﬂashplayer.html   4 Patch Before Exploited: An Approach …  99  Fig. 4.7 The number of exploited vulnerabilities mentioned by each language  left , and the number of vulnerabilities mentions in each language  right   exploitation rate. For example, of those vulnerabilities, only 12 are exploited  about 10% , while 32 of the vulnerability mentioned on English postings are exploited  about 13% . Although vulnerability mentions in Russian or Swedish postings are few, these vulnerabilities have a very high exploitation rate. For example, about 46% of the vulnerabilities mentioned in Russian were exploited  6 , and about 19% for vulnerabilities mentioned in Swedish  2 . Figure 4.7 shows the number of vulnera- bility mentions by each language as well as the number of exploited vulnerabilities mentioned by each language.  4.6 Experimental Setup  To evaluate our approach, a series of experiments was conducted with the presented models. First, our model was compared to a benchmark work presented in [8]. For our model, random forest gave the best F1 measure.32 Random forest is an ensemble method proposed by Breiman [30], which is based on the idea of generating mul- tiple predictors  decision trees in this case  to be used in combination to classify a new disclosed vulnerability. The strength of the random forest lies in introducing randomness to build each classiﬁer and using random, low-dimensional subspaces to classify the data at each node in a classiﬁer. A random forest that combines bag- ging [30] for each tree with random feature selection [31] at each node to split the data was used. The ﬁnal result was therefore an ensemble of decision trees, each having their own independent opinion on class labels  i.e., exploited or not exploited for a given disclosed vulnerability . Therefore, new vulnerabilities were classiﬁed independently by each tree, and the most suitable class label was assigned to them. Multiple decision trees may result in having multiple class labels for the same data  32The harmonic mean of precision and recall.   100  M. Almukaynizi et al.  sample; hence, a majority vote was taken and the class label with most votes was assigned to the vulnerability.  In [8], the authors temporally mix their samples, i.e., vulnerabilities exploited in the future are used to predict vulnerabilities exploited in the past, a practice discussed in [14]. Additionally, to account for the severe class imbalance, only vulnerabilities that occur in Microsoft or Adobe products were used in training and testing  477 vulnerabilities, 41 of which were exploited . Our model is compared to [8] under the same conditions.  For the second experiment, the training samples are restricted to the vulnerabili- ties published before any of the vulnerabilities in the testing samples were published. Also, only data feeds that are present before the exploitation date are used. This guar- antees that the experimental settings resemble the real world. Because no assump- tions can be made regarding the sequence of events for the exploited vulnerabilities reported by Symantec without the exploitation date  n = 112 , these vulnerabilities are removed from the experiments. The fraction of exploited vulnerabilities becomes 1.2%. The performance of our model is compared to the CVSS score.  Additionally, the goal for exploit prediction is to predict whether a disclosed vulnerability will likely be exploited in the future or not. Few vulnerabilities are exploited before they are published [2]. Prediction for such vulnerabilities does not add any value to the goal of the prediction task, considering that they had been already exploited by the time those vulnerabilities are revealed. That being said, knowing what vulnerabilities are exploited in the wild can help organizations with their cyber defense strategies, but this is out of the scope of this chapter.  4.6.1 Performance Evaluation  Our classiﬁers are evaluated based on two classes of metrics that have been used in previous work. The ﬁrst class is used to demonstrate the performance achieved on the minority class  in our case 1.2% . The metrics under this class are precision and recall. They are computed as shown in Table 4.4. Precision is deﬁned as the fraction of vulnerabilities that were exploited from all vulnerabilities predicted to be exploited by our model. It highlights the effect of mistakingly ﬂagging non-exploited vulnerabilities. Recall is deﬁned as the fraction of correctly predicted exploited vul- nerabilities from the total number of exploited vulnerabilities. It highlights the effect of unﬂagging important vulnerabilities that were used later in attacks. For highly imbalanced data, these metrics give us an intuition regarding how well the classi- ﬁer performed on the minority class  i.e., exploited vulnerabilities . The F1 measure summarizes precision and recall in a common metric. It may vary depending on the trade-off between precision and recall, which it turn depends on the application. If keeping the number of incorrectly ﬂagged vulnerabilities to a minimum is a prior- ity, then high precision is desired. To keep the number of undetected vulnerabilities that are later exploited to a minimum, high recall is desired. The Receiver Operat- ing Characteristics  ROC  curve and the Area Under Curve  AUC  of the classiﬁer   4 Patch Before Exploited: An Approach …  101  Table 4.4 Evaluation metrics. TP—true positive, FP—false positive, FN—false negative, TN—true negative  TPR  recall in case of binary classiﬁcation   Metric  Precision  F1  FPR  T P  T P  Formula T P + F P T P + F N 2 · precision · recall precision + recall F P F P + T N  are also described. ROC visualizes the performance of our classiﬁer by plotting the true positive rate against the false positive rate at various thresholds of the con- ﬁdence scores the classiﬁer outputs. In binary classiﬁcation problems, the overall TPR is always equivalent to recall for the positive class, while FPR is the number of non-exploited vulnerabilities that are incorrectly classiﬁed as exploited from all the samples that have not been exploited. ROC is a curve, thus, AUC is the area under ROC. The higher the AUC value, the more optimal the model  i.e., a classiﬁer with an AUC of 1 is a perfect classiﬁer .  4.6.2 Results  The performance of several standard supervised machine learning approaches were used and compared for exploit prediction. Parameters for all approaches were set in a way to provide the best performance. The scikit-learn33 Python package [32] was used.  4.6.2.1 Examining Classiﬁers  The temporal information is maintained for all the classiﬁers. The vulnerabilities are sorted by the date they were posted on NVD. The ﬁrst 70% are reserved for training, along with any features that are available by the end of the training period. The remaining vulnerabilities are used for testing.  Table 4.5 shows a comparison between the classiﬁers with respect to precision,  recall and F1 measure.  Random forest  RF  performs the best with an F1 measure of 0.4 as compared to Support Vector Machine’s 0.34, Bayesian Network’s 0.34, Logistic Regression’s 0.33, Decision Tree’s 0.25, and Naïve Bayes’s 0.27. Note that even though RF has the best F1 measure, it does not have the best recall—NB does. RF with high precision is chosen, which makes the model reliable compared to low precision, which results in a lot of false positives.  33http:  scikit-learn.org   102  Table 4.5 Precision, recall, and F1 measure for RF, SVM, LOG-REG, DT and NB to predict whether a vulnerability will likely be exploited  M. Almukaynizi et al.  Classiﬁer RF BN SVM LOG-REG DT NB  Precision 0.45 0.31 0.28 0.28 0.25 0.17  Recall 0.35 0.38 0.42 0.4 0.24 0.76  F1 measure 0.40 0.34 0.34 0.33 0.25 0.27  4.6.2.2 Benchmark Test  Our model is compared to a recent work that uses vulnerability mentions on Twitter to predict the likelihood of exploitation [8]. In this study, the authors used SVM as the classiﬁer, while our model works best with a random forest classiﬁer. Although our approach is expected to achieve better performance, a comparison with this work is important due to two reasons:  1  to the best of our knowledge, there is no existing work on predicting exploits in the wild using Deep Web Dark Web data, and  2  the comparison between all major approaches conﬁrmed that using feeds from social media is currently the best approach.  In [8], the authors restricted the training and evaluation of their classiﬁer to vulner- abilities targeting Microsoft and Adobe products, because Symantec does not have attack signatures for all the targeted platforms. They performed a 10-fold stratiﬁed cross-validation, where the data is partitioned into 10 parts while maintaining the class ratio in each part. They trained on 9 parts and tested on the remaining one. The experiment was repeated for all 10 parts. Hence, each sample was tested once.  For comparison, the same experiment is performed under highly similar assump- tions. All exploited vulnerabilities are used regardless of whether the date is reported by Symantec. In our case, there were 2,056 vulnerabilities targeting Microsoft and Adobe products. Out of these, 261 were exploited, which is consistent with previous works  by rate . A 10-fold stratiﬁed cross-validation is performed. The precision- recall curve of this model is depicted in Fig. 4.8.  The precision-recall curve shows the trade-off between precision and recall for different decision threshold. Since the F1 measure is not reported in [8], the reported precision-recall curve is used for comparison while maintaining the recall value con- stant. Table 4.6 shows the comparison between the two models in terms of precision for different values of recall.  For a threshold of 0.5, the F1 measure is 0.44 with precision 0.53 and recall 0.3. Maintaining the recall, the precision displayed in the graph in [8] is 0.3, which is noticeably lower than 0.4. To compare the precision, the same experiment is performed on different recall values. At each point, a higher precision is obtained than with the previous approach.   4 Patch Before Exploited: An Approach …  103  Fig. 4.8 Precision-recall curve for proposed features for Microsoft-Adobe vulnerabilities  RF   Table 4.6 Precision comparison between [8] and the proposed model while keeping the recall constant Recall 0.20 0.40 0.70 aNumbers derived from Fig. 6a from [8]  Precision  our work  0.41 0.40 0.29  Precisiona 0.30 0.18 0.10  4.6.2.3 Baseline Comparison  Bullough et al. argue that the problem of predicting the likelihood of exploitation is sensitive to the sequence of vulnerability-related events [14]. Temporally mixing such events leads to future events being used to predict past ones, resulting in inaccurate prediction results. To avoid the temporal mixing of events, time-based splits are created, as described in this section.  For baseline comparison, the CVSS version 2.0 base score is used to classify whether a vulnerability will likely be exploited based on the associated severity score. CVSS score has been used as a baseline in previous studies [4, 8]. CVSS tends to be overly cautious, i.e., it tends to assign high scores to many vulnerabilities, resulting in many false positives. Figure 4.9 shows the precision-recall curve for the CVSS score.  It is computed by varying the decision threshold  x-axis , based on which the class label of each vulnerability can be determined. CVSS gives high recall with very low precision, which is not desired for real-world patch prioritization tasks. The best F1 measure that could be obtained is 0.15. Figure 4.10 shows the performance comparison between our proposed RF model and the CVSS-based model that yields the highest F1 score.   104  M. Almukaynizi et al.  Fig. 4.9 Precision and recall for classiﬁcation based on CVSS base score version 2.0 threshold  Fig. 4.10 Precision-recall curve for classiﬁcation based on CVSS score threshold  RF   Our model outperforms the baseline with an F1 measure of 0.4, a precision of 0.45, and a recall of 0.35. Additionally, our classiﬁer shows very high TPR  90%  at low FPR  13% , with an AUC of 94%, as depicted in Fig. 4.11.  4.6.2.4 Evaluation with Individual Data Sources  Our study investigated how the prediction of vulnerabilities mentioned in a data source is affected by introducing the corresponding data source. This is important for understanding if the addition of a particular data source beneﬁts the vulnerabilities that have been mentioned in that data source. It turned out that time-based split used in the previous experiments leaves very few vulnerabilities mentioned in these data sources in the test set  ZDI: 18, DW: 4, EDB: 2 . Therefore, the numbers were incresed by  1  performing a 10-fold cross-validation without sorting the vulnerabilities, and  2  increasing the ground truth by considering the exploited vulnerabilities that did   4 Patch Before Exploited: An Approach …  105  Fig. 4.11 ROC curve for classiﬁcation based on a random forest classiﬁer  not have exploit date  these were removed from earlier experiments because it was not clear whether these were exploited before or after the vulnerability was exploited . Using these two techniques, there were 84 vulnerabilities mentioned in ZDI that have been exploited, 57 in EDB, and 32 in DW. The results  precision, recall, and F1  for the vulnerabilities mentioned in each data source were captured. Also, the prediction of these vulnerabilities are described by using only the NVD features. For the vulnerabilities mentioned in DW, only DW features provided along with the NVD features are considered. The model predicts 12 vulnerabilities as exploited with a precision of 0.67 and recall of 0.38. By only considering the NVD features, the model predicts 12 vulnerabilities as exploited with a precision of 0.23 and recall of 0.38. Consequently, using the DW features, the precision improved signiﬁcantly from 0.23 to 0.67. Table 4.7 shows the precision-recall with the corresponding F1 measure. DW information was thus able to correctly identify positive samples mentioned in DW with higher precision.  For ZDI, 84 vulnerabilities were mentioned. Using only NVD features yields to an F1 measure of 0.25  precision: 0.16, recall: 0.54  as compared to adding the ZDI feature with an F1 measure of 0.32  precision: 0.49, recall: 0.24 —a signiﬁ- cant improvement in precision. Table 4.7 also shows the precision-recall with the corresponding F1 measure for samples mentioned on ZDI.  Table 4.7 Precision, recall, and F1 measure for vulnerabilities mentioned on DW, ZDI, and EDB Source DW  Case NVD NVD + DW NVD NVD + ZDI NVD NVD + EDB  Precision 0.23 0.67 0.16 0.49 0.15 0.31  Recall 0.38 0.375 0.54 0.24 0.56 0.40  F1 measure 0.27 0.48 0.25 0.32 0.24 0.35  ZDI  EDB   106  Table 4.8 Performance improvement attained by applying SMOTE for the BN classiﬁer using different oversampling for the exploited samples  M. Almukaynizi et al.  Oversampling [%] 100 200 300 400  Precision Recall 0.37 0.40 0.41 0.31  0.42 0.44 0.40 0.40  F1 measure 0.39 0.42 0.40 0.35  Similar analysis was performed for the vulnerabilities that had PoCs available on EDB. For EDB, there were 57 vulnerabilities with PoCs. When using only NVD features, this time the model scored an F1 measure of 0.24  precision: 0.15, recall: 0.56 ; while adding the EDB feature boosted the F1 measure to 0.35  precision: 0.31, recall: 0.40 , which is a signiﬁcant improvement in precision, as shown in Table 4.7.  4.6.2.5 Feature Importance  To better explain our feature choices, an explanation of where our prediction power primarily derives from is due. The features that have the most contribution to the prediction performance are reported. A feature vector for a sample has 28 features computed from the non-textual data  summarized in Table 4.2  as well as the textual features—TF-IDF computed from the bag of words for the 1,000 words that have the highest frequency in the NVD description and DW. For each feature, the Mutual Information  MI  [33] is computed, which expresses how much a variable  here a feature xi   tells about another variable  here the class label y ∈ {exploited, not exploited} . The features that contribute the most from the non-textual data are {language_Russian = true, has_DW = true, has_PoC = false}. In addition, the features that contribute the most from the textual data are the words {demonstrate, launch, network, xen, zdi, binary, attempt}. All of these features received MI scores higher than 0.02.  4.6.2.6 Addressing Class Imbalance  The problem of class imbalance has gained lot of research interest [34]. Since our dataset is highly imbalanced, SMOTE [28] is used and the improvement is measured in terms of classiﬁcation performance. SMOTE oversamples the minority class by creating synthetic samples with features similar to those of the exploited vulnera- bilities. This data manipulation is only applied to the training set. Using SMOTE, no performance improvement is achieved for our RF classiﬁer. However, SMOTE introduces a considerable improvement with a Bayesian Network classiﬁer. Table 4.8 reports different oversampling ratios and the change in performance. The best over- sampling ratio is experimentally determined, i.e., high oversampling ratios lead to the model learning from a distribution that differs signiﬁcantly from the real distribution.   4 Patch Before Exploited: An Approach …  107  4.7 Adversarial Data Manipulation  The effects of adversarial data manipulation is studied here only on DW data. As for the presence of PoCs, only those PoCs that are veriﬁed by EDB are considered. Adversaries need to hack into EDB to add noise or remove PoCs from the EDB database. In this analysis, it is assumed that such an action cannot be taken by adversaries. Similarly, ZDI publishes only vulnerabilities that are veriﬁed by its researchers; hence there is a very small chance of manipulating these data sources. On the other hand, the public nature of DW marketplaces and forums gives an adversary the ability to poison the data used by the classiﬁer. They can achieve this by adding vulnerability discussions on these platforms with the intent of fooling the classiﬁer to make it produce high false positives. Previous works discuss how an adversary can inﬂuence a classiﬁer by manipulating the training data [35–37].  In our prediction model, the presence of the vulnerability in DW is used, along with the language of the market forum on which it was mentioned, and the vulnera- bility description, as features. An adversary could easily post discussions regarding vulnerabilities he does not intent to exploit, nor does he expect that they will be exploited. To study the inﬂuence of such noise on the performance of the model, experiments were conducted using the following two strategies:  1. Adversary adding random vulnerability discussions. In this strategy, the adversary initiates random vulnerability discussions on DW and reposts them with a different CVE number. So the CVE mentions on DW increases. For our experiment, two cases are considered with different amounts of noise added. In case 1, it is assumed that the noise is present in both the training and the testing data. Various fractions of noise are considered  5, 10, 20% of the total data sam- ples , randomly distributed in training and testing data. The experimental setup follows conditions discussed in Sect. 4.6. Vulnerabilities are ﬁrst sorted accord- ing to time, and the ﬁrst 70% are reserved for training and the remaining for testing. Figure 4.12a shows the ROC curve showing the false positive rate  FPR  versus the true positive rate  TPR . For different amount of noise introduced, our model still maintains a high TPR with low FPR and an AUC of at least 0.94, a performance similar to the experiment without adding noise. This shows that the model is highly robust against noise such that it learns good representation of the noise in the training dataset and can distinguish them in the testing dataset. For case 2, random vulnerability discussions found on DW are added with a different CVE number only to the test data and repeat the same experiment. Figure 4.12a shows the ROC plot. In the case, even though there is a slight increase in the FPR, the performance is still on par with the experiment without noise  AUC  cid:2  0.87 . Hence, noisy sample affect the prediction model slightly, if no noise was introduced in the training data.  2. An adversary adds a vulnerability discussion similar to the NVD description. In the previous strategy, the adversary adds vulnerability discussions randomly without taking into account the actual capability of the vulnerability. For instance,   108  M. Almukaynizi et al.  Fig. 4.12 ROC curves demonstrating the robustness of the random forest model against adversarial data manipulation  CVE-2016-335034 is a vulnerability in Microsoft Edge, as reported by the NVD. If the vulnerability is mentioned on DW as noise by an adversary but targeting Google Chrome, then it might be easy for the prediction model to detect it as seen in previous experiments. But, what if the adversary crafts the vulnerability discussion such that it is a copy of the NVD description or consistent with the NVD description? In this strategy, the adversary posts the NVD description with the CVE number on DW. For case 1, this noise is considered to be randomly distributed in both training and testing. Figure 4.12b shows the ROC curves for different levels of noise. The performance decreases as the number of noisy samples increases, but there is no signiﬁcant decline  AUC  cid:2  0.88 . The experiment was repeated by adding noise only to the test data for case 2. This experiment indicated that the biggest drop in performance results in an AUC of 0.78, when 20% of the samples are noise  see Fig. 4.12b . This conﬁrms that adding correct vulnerability discussions does affect the prediction model, except if a large number of such samples are added. Also, the effect can be countered by also adding such noisy samples to the training data for the model to learn from.  Note that an adversary would need to add a large number of noisy samples to lower the performance of the prediction model. Previous research on using data feeds like Twitter for exploit prediction mentions that an adversary can register a  34https:  nvd.nist.gov vuln detail CVE-2015-3350   4 Patch Before Exploited: An Approach …  109  large number of Twitter accounts and ﬂood Twitter with vulnerability mentions [8]. In DW markets and forums, creation of accounts needs veriﬁcation and, in some cases, technical skills. While fake accounts are often sold on the dark web, it is difﬁcult to purchase and maintain thousands of such fake accounts to post with them. Also, if someone posts a large volume of discussions with CVE mentions, he can be identiﬁed from his username or can be removed from the market forum if many of his posts get downvoted for being irrelevant. It is also important to note that such forums also function as a meritocracy [19], where users who contribute more are held in higher regard  which also makes it difﬁcult to ﬂood discussions with such information .  4.8 Discussion  Viability of the model and cost of misclassiﬁcation. The performance achieved by our model as a ﬁrst-line defense layer is very promising. Recall that a random forest classiﬁer outputs a conﬁdence score for every testing sample. A threshold can be set to identify the decision boundary. Note that all the reported results were based on hard-cut thresholds, such that all samples that are assigned conﬁdence scores greater than a threshold thr are predicted to be exploited. Relying solely on a hard- cut threshold may not be a good practice in real-world threat assessments; rather, thr should be varied in accordance to other variables within an organization such that different thresholds can be set to different vendors  i.e., thrven1, thrven2 , or information systems  i.e., thrsys1, thrsys2 . For instance, if an organization hosts an important website on an Apache server, and the availability of this site is of top priority for the organization, then any vulnerability of the Apache server should receive high attention and put forward to remediation plans regardless of other measures. Other vulnerabilities, tens of which are disclosed on a daily bases, may exist in many other systems within the organization.  Since it is very expensive to be responsive to that many security advisories  e.g., some patches may be unavailable, some systems may need to be taken ofﬂine to apply patches , assessing the likelihood of exploitation can help in quantifying the risk and planning mitigations. Risk is always thought of as a function of likelihood  exploitation  and impact. The cost of classifying negative samples as exploited is the effort made to have them ﬁxed. This mostly involves patching or other remedi- ations such as controlling access or blocking network ports. Similarly, the cost of misclassiﬁcation depends on the impact. For example, if two companies run the same database management system, and one hosts a database with data about all business transactions, while the other hosts a database with data that is of little value to the company, the resulting cost of a data breach would be signiﬁcantly different.  Model success and failure cases. The analysis of false negatives and false positives provides an understanding of why and in which case our model performs well and when is it inefﬁcient. The 10 exploited vulnerabilities  about 18% of the exploited   110  M. Almukaynizi et al.  samples in the testing dataset  that received the lowest conﬁdence scores seem to have common features. For example, 9 of these appear in Adobe products, namely Flash Player  ﬁve vulnerabilities  and Acrobat Reader  four vulnerabilities . Flash Player vulnerabilities seem to have very similar description in the NVD. The same holds for Acrobat Reader. They were assigned CVE-IDs on the same day  April 27, 2016 , and 7 out of these 9 were published on the same day as well  July 12, 2016 , and assigned a CVSS base score of 10.0  except for one, assigned 7.0 . The other vulnerability exist in Windows Azure Active Directory  CVSS score of 4.3 . Out of these 10 vulnerabilities, one had a veriﬁed PoC archived on EDB before it was detected in the wild, and another one had a ZDI mention, while none were mentioned in DW. The reason for misclassifying these vulnerabilities is the limited representation of these samples in the training dataset. Moreover, this observation signiﬁes the importance of avoiding experiments on time-intermixed data, a point discussed in Sect. 4.3.2.  False positive samples were also studied, which received high conﬁdence score— samples our model predicted as exploited while they were not. For our random forest classiﬁer, all the examined false positives appeared in Microsoft products, although vendor was not used as a feature. Our model is able to infer the vendor from other textual features. Assumingly, this level of overﬁtting is inevitable and marginal, and is caused mainly by the limitations of the ground truth. The model is highly generalizable though. There are examples of vulnerabilities from other vendors with conﬁdence scores close to the thr used by us; however, it cannot be assumed that these vulnerabilities are exploited.  4.9 Conclusion  This chapter proposed an approach that aggregates early signs of vulnerability exploitation from various online sources to predict the likelihood of exploitation, a problem directly related to patch prioritization. By conducting a series of exper- iments, the use of cyberthreat feeds from the data sources was demonstrated. Our machine learning model outperforms existing models that combine information from social media sites like Twitter for exploit prediction. More precisely, our results show that while maintaining a high true positive rate, a reasonably low false positive rate can be achieved in predicting exploits compared to previous severity scoring systems.  Acknowledgements Some of the authors were supported by the Ofﬁce of Naval Research  ONR  contract N00014-15-1-2742, the Ofﬁce of Naval Research  ONR  Neptune program and the ASU Global Security Initiative  GSI . Paulo Shakarian and Jana Shakarian are supported by the Ofﬁce of the Director of National Intelligence  ODNI  and the Intelligence Advanced Research Projects Activity  IARPA  via the Air Force Research Laboratory  AFRL  contract number FA8750-16- C-0112. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing   4 Patch Before Exploited: An Approach …  111  the ofﬁcial policies or endorsements, either expressed or implied, of ODNI, IARPA, AFRL, or the U.S. Government.  References  1. Pﬂeeger CP, Pﬂeeger SL, Margulies J  2015  Security in computing, 5th edn. Prentice Hall,  Upper Saddle River, NJ, USA  2. Bilge L, Dumitras T  2012  Before we knew it: an empirical study of zero-day attacks in the real world. In: Yu T, Danezis G, Gligor V  eds  Proceedings of the 2012 ACM Conference on Computer and Communications Security. ACM, New York, pp 833–844. https:  doi.org 10. 1145 2382196.2382284  3. Frei S, Schatzmann D, Plattner B, Trammell B  2010  Modeling the security ecosystem–The dynamics of  in security. In: Moore T, Pym D, Ioannidis C  eds  Economics of information security and privacy. Springer, Boston, pp 79–106. https:  doi.org 10.1007 978-1-4419-6967- 5_6  4. Allodi L, Massacci F  2014  Comparing vulnerability severity and exploits using case-control studies. ACM Trans Inform Syst Secur 17 1 , Article No. 1. https:  doi.org 10.1145 2630069 5. Durumeric Z, Kasten J, Adrian D, Halderman JA, Bailey M, Li F, Weaver N, Amann J, Beekman J, Payer M, Weaver N, Adrian D, Paxson V, Bailey M, Halderman JA  2014  The matter of Heartbleed. In: Williamson C, Akella A, Taft N  eds  Proceedings of the 2014 Conference on Internet Measurement Conference. ACM, New York, pp 475–488. https:  doi.org 10.1145  2663716.2663755  6. Edkrantz M, Said A  2015  Predicting cyber vulnerability exploits with machine learning. In: Thirteenth Scandinavian Conference on Artiﬁcial Intelligence, pp 48–57. https:  doi.org 10. 3233 978-1-61499-589-0-48  7. Nayak K, Marino D, Efstathopoulos P, Dumitra¸s T  2014  Some vulnerabilities are different than others. In: Stavrou A, Bos H, Portokalidis G  eds  Research in attacks, intrusions and defenses. Springer, Cham, pp 426–446. https:  doi.org 10.1007 978-3-319-11379-1_21  8. Sabottke C, Suciu O, Dumitras T  2015  Vulnerability disclosure in the age of social media: exploiting Twitter for predicting real-world exploits. In: Proceedings of the 24th USENIX Security Symposium. USENIX Association, Berkeley, CA, USA, pp 1041–1056. https:  www. usenix.org sites default ﬁles sec15_full_proceedings.pdf  9. Allodi L, Massacci F  2012  A preliminary analysis of vulnerability scores for attacks in wild: the EKITS and SYM datasets. In: Yu T, Christodorescu M  eds  Proceedings of the 2012 ACM Workshop on Building Analysis Datasets and Gathering Experience Returns for Security. ACM, New York, pp 17–24. https:  doi.org 10.1145 2382416.2382427  10. Mittal S, Das PK, Mulwad V, Joshi A, Finin T  2016  CyberTwitter: using Twitter to generate alerts for cybersecurity threats and vulnerabilities. In: Subrahmanian VS, Rokne J, Kimar R, Caverlee J, Tong H  eds  Proceedings of the 2016 IEEE ACM International Conference on Advances in Social Networks Analysis and Mining. IEEE Press, Piscataway, NJ, USA, pp 860–867  11. Marin E, Diab A, Shakarian P  2016  Product offerings in malicious hacker markets. In: Zhou L, Kaati L, Mao W, Wang GA  eds  Proceedings of the 2016 IEEE Conference on Intelligence and Security Informatics. The Printing House, Stoughton, WI, USA, pp 187–189. https:  doi. org 10.1109 ISI.2016.7745465  12. Samtani S, Chinn K, Larson C, Chen H  2016  AZSecure hacker assets portal: cyber threat intelligence and malware analysis. In: Zhou L, Kaati L, Mao W, Wang GA  eds  Proceedings of the 2016 IEEE Conference on Intelligence and Security Informatics. The Printing House, Stoughton, WI, USA, pp 19–24. https:  doi.org 10.1109 ISI.2016.7745437  13. Allodi L  2017  Economic factors of vulnerability trade and exploitation. In: Thuraisingham B, Evans D, Malkin T, Xu D  eds  Proceedings of the 2017 ACM SIGSAC Conference on   112  M. Almukaynizi et al.  Computer and Communications Security. ACM, New York, pp 1483–1499. https:  doi.org 10. 1145 3133956.3133960  14. Bullough BL, Yanchenko AK, Smith CL, Zipkin JR  2017  Predicting exploitation of disclosed software vulnerabilities using open-source data. In: Verma R, Thuraisingham B  eds  Proceed- ings of the 3rd ACM on International Workshop on Security and Privacy Analytics. ACM, New York, pp 45–53. https:  doi.org 10.1145 3041008.3041009  15. Allodi L, Shim W, Massacci F  2013  Quantitative assessment of risk reduction with cybercrime black market monitoring. In: 2013 IEEE Security and Privacy Workshops. IEEE Computer Society, Los Alamitos, CA, USA, pp 165–172. https:  doi.org 10.1109 SPW.2013.16  16. Bozorgi M, Saul LK, Savage S, Voelker GM  2010  Beyond heuristics: learning to classify vulnerabilities and predict exploits. In: Rao B, Krishnapuram B, Tomkins A, Yang Q  eds  Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, New York, pp 105–114. https:  doi.org 10.1145 1835804.1835821  17. Motoyama M, McCoy D, Levchenko K, Savage S, Voelker GM  2011  An analysis of under- ground forums. In: Thiran P, Willinger W  eds  Proceedings of the 2011 ACM SIGCOMM Conference on Internet Measurement. ACM, New York, pp 71–80. https:  doi.org 10.1145  2068816.2068824  18. Holt TJ, Lampke E  2010  Exploring stolen data markets online: products and market forces.  Crim Justice Stud 23 1 :33–50. https:  doi.org 10.1080 14786011003634415  19. Shakarian J, Gunn AT, Shakarian P  2016  Exploring malicious hacker forums. In: Jajodia S, Subrahmanian V, Swarup V, Wang C  eds  Cyber deception. Springer, Cham, pp 259–282. https:  doi.org 10.1007 978-3-319-32699-3_11  20. Nunes E, Diab A, Gunn A, Marin E, Mishra V, Paliath V, Robertson J, Shakarian J, Thart A, Shakarian P  2016  Darknet and deepnet mining for proactive cybersecurity threat intelligence. In: Chen H, Hariri S, Thuraisingham B, Zeng D  eds  Proceedings of the 2016 IEEE Conference on Intelligence and Security Informatics, pp 7–12. https:  doi.org 10.1109 ISI.2016.7745435 21. Robertson J, Diab A, Marin E, Nunes E, Paliath V, Shakarian J, Shakarian P  2017  Darkweb cyber threat intelligence mining. Cambridge University Press, New York. https:  doi.org 10. 1017 9781316888513  22. Liu Y, Sarabi A, Zhang J, Naghizadeh P, Karir M, Bailey M, Liu M  2015  Cloudy with a chance of breach: forecasting cyber security incidents. In: Proceedings of the 24th USENIX Security Symposium. USENIX Association, Berkeley, CA, USA, pp 1009–1024. https:  www. usenix.org sites default ﬁles sec15_full_proceedings.pdf  23. Soska N, Christin K  2014  Automatically detecting vulnerable websites before they turn mali- cious. In: Proceedings of the 23rd USENIX Security Symposium. USENIX Association, Berke- ley, CA, USA, pp 625–640. https:  www.usenix.org sites default ﬁles sec14_full_proceedings. pdf  24. Almukaynizi M, Nunes E, Dharaiya K, Senguttuvan M, Shakarian J, Shakarian P  2017  Proac- tive identiﬁcation of exploits in the wild through vulnerability mentions online. In: Sobiesk E, Bennett D, Maxwell P  eds  Proceedings of the 2017 International Conference on Cyber Con- ﬂict. Curran Associates, Red Hook, NY, USA, pp 82–88. https:  doi.org 10.1109 CYCONUS. 2017.8167501  25. Zhang S, Caragea D, Ou X  2011  An empirical study on using the national vulnerability database to predict software vulnerabilities. In: Hameurlain A, Liddle SW, Schewe KD, Zhou X  eds  Database and expert systems applications. Springer, Heidelberg, pp 217–231. https:   doi.org 10.1007 978-3-642-23088-2_15  26. Hao S, Kantchelian A, Miller B, Paxson V, Feamster N  2016  PREDATOR: proactive recog- nition and elimination of domain abuse at time-of-registration. In: Weippl E, Katzenbeisser S, Kruegel C, Myers A, Halevi S  eds  Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, New York, pp 1568-1579. https:  doi.org 10. 1145 2976749.2978317  27. Cortes C, Vapnik V  1995  Support-vector networks. Mach Learn 20 3 :273–297. https:  doi.  org 10.1023 A:1022627411411   4 Patch Before Exploited: An Approach …  113  28. Chawla NV, Bowyer KW, Hall LO, Kegelmeyer WP  2002  SMOTE: synthetic minority over-  sampling technique. J Artif Int Res 16 1 :321–357. https:  doi.org 10.1613 jair.953  29. Allodi L, Massacci F, Williams JM  2017  The work-averse cyber attacker model: theory and  evidence from two million attack signatures. https:  doi.org 10.2139 ssrn.2862299  30. Breiman L  2001  Random forests. Mach Learn 45 1 :5–32. https:  doi.org 10.1023 A:  31. Breiman L  1996  Bagging predictors. Mach Learn 24 2 :123–140. https:  doi.org 10.1007   1010933404324  BF00058655  32. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Pretten- hofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay É  2011  Scikit-learn: machine learning in Python. J Mach Learn Res 12:2825–2830 33. Guo D, Shamai S, Verdu S  2005  Mutual information and minimum mean-square error in Gaussian channels. IEEE Trans Inform Theory 51 4 :1261–1282. https:  doi.org 10.1109 TIT. 2005.844072  34. Galar M, Fernandez A, Barrenechea E, Bustince H, Herrera F  2012  A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approach. IEEE Trans Syst Man Cybern C 42 4 :463–484. https:  doi.org 10.1109 TSMCC.2011.2161285  35. Barreno M, Bartlett PL, Chi FJ, Joseph AD, Nelson B, Rubinstein BIP, Saini U, Tygar JD  2008  Open problems in the security of learning. In: Balfanz D, Staddon J  eds  Proceedings of the 1st ACM Workshop on AISec. ACM, New York, pp 19–26. https:  doi.org 10.1145  1456377.1456382  36. Barreno M, Nelson B, Joseph AD, Tygar J  2010  The security of machine learning. Mach  Learn 81 2 :121–148. https:  doi.org 10.1007 s10994-010-5188-5  37. Biggio B, Nelson B, Laskov P  2011  Support vector machines under adversarial label noise. In: Hsu C-N, Lee WS  eds  Proceedings of the 3rd Asian Conference on Machine Learning, pp 97–112. http:  www.jmlr.org proceedings papers v20 biggio11 biggio11.pdf   Chapter 5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  Alexander Branitskiy and Igor Kotenko  Abstract This chapter reveals the methods of artiﬁcial intelligence and their appli- cation for detecting network attacks. Particular attention is paid to the representation of models based on neural, fuzzy, and evolutionary computations. The main object is a binary classiﬁer, which is designed to match each input object to one of two sets of classes. Various schemes for combining binary classiﬁers are considered, which allows building models trained on different subsamples. Several optimizing techniques are proposed, both in terms of parallelization  for increasing the speed of training  and usage of aggregating compositions  for enhancing the classiﬁca- tion accuracy . Principal component analysis is also considered, which is aimed at reducing the dimensionality of the analyzed attack feature vectors. A sliding window method was developed and adopted to decrease the number of false positives. Finally, the model efﬁciency indicators obtained during the experiments using the multifold cross-validation are provided.  5.1 Introduction  Network attack detection is a difﬁcult task, and there are lots of methods for it in the modern literature. Some of these are based on such rapidly developing ﬁelds as artiﬁcial intelligence. These include neural networks, fuzzy logic, and genetic algorithms.  The construction of attack detection systems plays one of the key roles in ensur- ing the security of network nodes. Therefore it is important to use advanced tools, including artiﬁcial intelligence methods, and apply them to network attack detection problems. A. Branitskiy · I. Kotenko  B   St. Petersburg Institute for Informatics and Automation of the Russian Academy of Sciences, St. Petersburg, Russia e-mail: ivkote@comsec.spb.ru    Springer Nature Switzerland AG 2019 L. F. Sikos  ed. , AI in Cybersecurity, Intelligent Systems Reference Library 151, https:  doi.org 10.1007 978-3-319-98842-9_5  115   116  A. Branitskiy and I. Kotenko  From the input data interpretation point of view, the methods of network attack detection are divided into anomaly detection methods and misuse detection methods [1].  In the case of anomaly detection methods, the system should contain data on the normal behavior of the analyzed object  user, process, sequence of network packets —a normal behavior pattern. If there is a discrepancy between the observed parameters and the parameters of the normal behavior pattern, the system detects a network anomaly. Adding and modifying of such patterns can be performed both in manual mode and automatically; and some parameters of the normal behavior pattern may vary depending on the time of day. The drawback of anomaly detection systems is the presence of false positives, which can be caused by incompleteness of the available set of normal behavior patterns. In contrast, misuse detection methods identify only those illegal actions for which there is an exact representation in the form of attack templates. By attack pattern we mean some set of rules, e.g., pattern matching or signature search, which explicitly describes a speciﬁc attack. Applying such rules to the ﬁelds of the identiﬁed object gives an unambiguous answer about its belonging to this attack. The main problem of designing misuse detection systems is how to provide a mechanism for fast search based on speciﬁed signature rules. The approach discussed in this chapter is based on artiﬁcial intelligence methods, and uses parameters of normal and anomalous behavior as training data for binary classiﬁers.  The chapter is organized as follows. In Sect. 5.2, we consider some papers that concern the issue of network attack detection using hybrid approaches. Section 5.3 describes the models of binary classiﬁers and algorithms for their training. In Sect. 5.4, we introduce a sliding window method for calculating the network parameters, a genetic optimization of binary classiﬁer weights, and an algorithm for network attack detection using a binary classiﬁer. In Sect. 5.5, we propose to consider several combining schemes [2–4], which allows to construct multi-class models based on binary classiﬁers. The results of the experiments are presented in Sect. 5.6.  5.2 Related Work  In this section we present some papers devoted to the investigated problem.  In [5, 6] three classiﬁers were considered: decision tree, support vector machine  SVM , and a combination of these, where the hybrid classiﬁer consisted of two phases. First, the test data were passed to decision trees, which generated a node information as numbers of leaves. Then the test data with node information were processed by the SVM, which output is a classiﬁcation result. It is suggested that while using this approach, the additional information obtained from the decision tree will allow to enhance the effectiveness of the SVM. If these three classiﬁers give different results, the ultimate result is based on weight voting.   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  117  In [7] the set of three neural networks and SMV is considered. The output value of the hybrid classiﬁer is a weighted sum of output values of these classiﬁers. The weights are calculated on the basis of the mean square error  MSE  value.  In [8] the authors propose to use the output values of neural networks as the input values for the procedure of weight voting and majority voting. Using the test sample consisting of 6,890 instances, the classiﬁcation rate was achieved above 99%.  In [9] the two-level scheme of network attack detection is described. To this end, several adaptive neuro-fuzzy classiﬁers are combined together. Each of these classi- ﬁers is designed for detecting only one attack. The ﬁnal classiﬁcation is performed by a fuzzy module, which implements a system of Mamdani fuzzy inference with two membership functions. The module assignment works by determining how anoma- lous the network record is; the class of this record is a class of the ﬁrst-level fuzzy module with the greatest output value.  For solving the task of attack detection, in [10] it was proposed to use K radial basis neural networks. Each of these networks is trained on different disjoint subsets D1, . . . , DK of the original training dataset D. Such subsets are generated using the method of fuzzy clustering, according to which each element  cid:2 z ∈ D belongs to the i . Each subset Di  i = 1, . . . , K  is  cid:2 z region Di with a certain membership degree of u composed of those elements that have the greatest membership degree with respect to this subset among all other subsets. As the authors emphasize, due to such pre- liminary decomposition, the generalization ability of base classiﬁers is improved and time of their training is reduced, because only those objects are employed for their conﬁguration which are most densely grouped around the formed center of the training subset. To combine the output results  cid:2 y1, . . . ,  cid:2 yK of these classiﬁers, taking the vector  cid:2 z as an input argument, the multilayer neural network is used. Its input vector is represented as a set of elements obtained by applying a threshold function ·  cid:2 yi  i = 1, . . . , K . A similar approach has been  cid:2 z to each component of the vector u i used previously in [11], where feed-forward neural networks served as base classi- ﬁers and the input for an aggregating module was composed of the direct values of  cid:2 z vectors u 1  ·  cid:2 y1, . . . , u  In [12], the analysis of the records of network connections was performed by neuro-fuzzy models and SVM. The authors determined four main stages in the pro- posed approach. In the ﬁrst stage, the generation of training data is performed by a K-means clustering method. The second stage was training of neuro-fuzzy classi- ﬁers. In the third stage, the input vector was formed for SVM. The ﬁnal stage was attack detection using the latter classiﬁer.  ·  cid:2 yK .   cid:2 z k  In [13], an individual neural network with a single hidden layer1 was constructed for detecting each of the three types of DDoS attacks, which were conducted with the use of protocols TCP, UDP, and ICMP. The last layer of each of these neural networks consists of a single node, the output value of which is interpreted as the presence or absence of DDoS attacks of the appropriate type. The proposed approach  1A hidden layer is a layer hidden between the input and output layers, whose output is the input of another layer. Neural networks with more than one hidden layer are called deep neural networks, in which machine learning is called deep learning.   118  A. Branitskiy and I. Kotenko  was implemented as a module in the Snort, and tested on real trafﬁc network envi- ronments.  In [14], to detect DoS attacks, it was proposed to use an approach, which combines the method of normalized entropy for calculating the feature vectors and SVM for their analysis. In order to detect anomalies, six parameters were extracted from network trafﬁc, numerically expressed as the occurrence intensity of different values of selected ﬁelds within the packets during the 60 second window. In this approach, the network parameters are calculated using the method of normalized entropy, and they are used as the input for training and testing data based on SVM.  In [15], to detect DoS attacks and scanning hosts, the authors considered an approach based on consecutive application of a vector compression procedure and two fuzzy transformations. First, principal component analysis is applied to the input eight-dimensional feature vector of network connections, which allows to reduce its dimension to ﬁve components while preserving the relative total variance at a level of more than 90%. The next step is the training or testing of the neuro-fuzzy network, the output value of which is processed by the method of fuzzy clustering.  Our approach is based on these papers, but we suggest to apply the binary classi- ﬁers for constructing the multi-class model for recognizing different types of attacks. This approach is more ﬂexible, which allows to construct a variety of combining schemes without strict binding to the aggregating compositions.  The following sections discuss popular models for binary classiﬁcation.  5.3 Binary Classiﬁers  5.3.1 Neural Networks  This section describes neural networks in general and some training algorithms. By structure and function, an artiﬁcial neural network is an analogue to the human brain with computing nodes corresponding to neurons, and relations corresponding to synapses. The simulation of perturbing nervous impulse transmission from one neuron to another is reduced to establish corresponding connections between the elements of the considered computational structure. The presence and strength of relations between neurons can be set by specifying non-zero weight coefﬁcients whose values proportionally express the signiﬁcance of input signals. The stronger the input signal, the more multiplicative weight is assigned to the corresponding connection. After conﬁguring the similar structures described by at least two layers, they can perform a sufﬁciently accurate approximation of the training instances [16– 19].   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  119  A biological neuron consists of a nucleus, a soma  cell body , and multiple appendages [20]. Appendages of the ﬁrst type, known as axons, are represented in a single copy in each neuron and serves as an original transmitter of the nervous impulse generated by this neuron. Appendages of the second type, known as den- drites, receive the signals coming from the axons of adjacent cells. A region of nerve ﬁbers located at the junction of the axon and dendrite is called a synapse. Depending on the type, this element can cause agitation or inhibition of the transmitted signals that turns accordingly into ampliﬁcation or attenuation of relations between neurons when projected onto an artiﬁcial analogue of a neuron.   cid:4    cid:5   w 1  i cid:4 j   cid:4 n j=1   cid:3  w 1  i cid:4 j  i cid:4  = ϕ  i cid:4  =  cid:2  n j=1  The input layer of a neural network is a dummy layer that performs the pre- distribution of input signals before they are treated. The input vector for each node of the this layer2 is the scalar product of the synaptic weights vector and the input vector  cid:2 z =  z1, . . . , zn T . The signal on the input of the i th neuron of the ﬁrst hidden · zj + θ 1  layer consisting of N1 nodes is constructed as follows: z 1  , i cid:4   cid:4  = 1, . . . , N1, , are weights, specifying a conversion of the signal where i  cid:2 z on the input of the i  cid:4  th neuron, located in the ﬁrst hidden layer. The output of this neuron can be considered as value y 1  th + neuron located in the second hidden layer with N2 neurons: z 2   cid:4  cid:4  = 1, . . . , N2, i cid:4  cid:4  and y 2  θ 2  a conversion of the signals  cid:2 y 1  =  y 1  , . . . , y 1  N1 which is located in the second hidden layer, θ 2  i cid:4  cid:4  is an activation function. The resulting signal y 3  1 ϕ  th neuron of the ﬁrst hidden layer, and θ 1  i cid:4   cid:6   cid:4  cid:4  . Similarly, the input and output signals are set for each i · y 1   th neuron, th neuron, and ϕ = are weights on the input of the last layer   T on the input of the i is a bias of the i is constructed as follows: y 3  1  are weights, specifying  · y 2  is a bias of the output neuron  see Fig. 5.1 .   cid:6  , where  i cid:4  cid:4  =  cid:2   is a bias of the i  i cid:4  cid:4  = ϕ   cid:3  w 2  i cid:4  cid:4 j   cid:4 N2 j=1   cid:4 N1 j=1  + θ 3   , where i  neuron, and θ 3  1  Generally speaking, the following formula can be used to describe how the binary  w 3  1j  w 3  1j  w 2  i cid:4  cid:4 j   cid:5  cid:2   z 2  i cid:4  cid:4   z 1  i cid:4   N1 j=1  N2 j=1   cid:3    cid:6    cid:5    cid:4  cid:4    cid:4  cid:4   1  1   cid:4   j  j  neural network model functions:  Y   cid:2 z  = ϕ  ⎛  ⎝  N2 cid:9   i=1  · ϕ  w 3  1i   cid:10  N1 cid:9   j=1  · ϕ  w 2  ij   cid:10  n cid:9   k=1   cid:11    cid:11   · zk + θ 1   j  w 1  jk  + θ 2   i  + θ 3   1  ⎞  ⎠ .  The most common learning algorithm of multilayer neural networks is the back-  propagation3 algorithm  see Algorithm 3 .  2The ﬁrst hidden layer is intended to perform sequential operations of linear and nonlinear trans- formations. 3Backpropagation is a method for calculating a gradient needed in weight calculations.   120  A. Branitskiy and I. Kotenko  Fig. 5.1 Three-layer neural network  Algorithm 3: Algorithm for training multilayer neural networks 1. Set the neural network structure, i.e., choose activation function types and the  number of hidden layers and neurons located there.  the total mean square error ε.  2. Specify the maximum number of training epochs T and the minimum value of 3. Set the counter of current iterations to zero, i.e., t := 0, and initialize the weight coefﬁcients w K   by arbitrary values, where K denotes a layer number, i corre- sponds to a neuron position number in the Kth layer, j indicates a connection between the current neuron and the output signal of the jth neuron in the  K–1 th layer.  4. Perform steps 4a–4c for each vector  cid:2 xk  k = 1, . . . , M  .  ij  i  j  j  i  j  ij  ij   cid:6    cid:5   ϕ  j=1  w K    x K−1   and y K−1  for others K > 1 and y K−1   a. Perform a feed-forward propagation of the signals—calculate input sig- = nals for each ith neuron in the Kth layer according to the formula x K   · y K−1   cid:2 NK−1+1 , where NK−1 is the number of neurons in the = θ K   = = 1 for j = NK−1 + 1, y K−1   K − 1 th layer, w K   = xkj  original signal  for K = 1. b. Perform backpropagation of the error: calculate an augmentation of weight coefﬁcients of neurons by the formula  cid:2 w K   , succes- sively starting with the last and ending with the ﬁrst layer, where α is the correction proportionality coefﬁcient of weights and 0 < α  cid:2  1. If the Kth layer is output, then δ K   , otherwise δ K   i  , where uki denotes the desired  = ϕ cid:4   cid:5  · w K+1    cid:6  ·  cid:2 NK+1+1  = α · δ K     cid:6  ·  cid:5  uki  · y K−1   = ϕ cid:4   cid:5   − y K    δ K+1   x K   i  x K   i  j=1   cid:6   ij  j  i  j  i  j  i  j  ji   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  121  output of the neural network in the ith neuron of the output layer for the kth training vector. +  c. Adjust the weight coefﬁcients of neurons by the formula w K    := w K    ij  ij   cid:2 w K    .  ij  5. Increase the counter of current iterations, i.e., t := t + 1. 6. Terminate the algorithm if one of the conditions is satisﬁed, formally t  cid:3  T or is the total mean square  k=1 E   cid:2 xk   cid:2 ε, where E   cid:2 xk  = 1  ·  cid:2 NKall i=1  −y Kall     cid:5  uki   cid:6 2   cid:2   M  i  2  error of the neural network, which has the output value   cid:6 T and consists of three layers and a neuron on the output layer  Kall = = 1  if vector  cid:2 xk is passed to the distributed layer  with the desired output   cid:6 T  y Kall   NKall 3, NKall vector  cid:2 uk =  cid:5   uk1, . . . , ukNKall   ; otherwise jump to step 4.   cid:2 y Kall  =  cid:5   y Kall   1  ,. . .,  The above algorithm belongs to the general family of gradient descent algorithms in which a search of the minimum point is performed in the direction opposite to the gradient of the function to be optimized  e.g., MSE . Falling into a local minimum pit is a characteristic of such algorithms, meaning that these algorithms can barely modify the weight parameters, despite the presence of a deeper extremum compared to what has already been found. These issues are solved partially by means of various improvements of the backpropagation algorithm [20]. First a feed-forward propagation of the signals is performed, then an adjustmentment is calculated for each weight. Some of such modiﬁcations use a variable correction proportionality coefﬁcient of weights, which depends on maintaining or changing the derivative sign [21], taking into account the momentum factors for changing each individual weight [22], or considering the second derivatives [23, 24].   cid:5   Radial basis function networks are constructed differently than multilayer neural networks. The ﬁrst hidden layer of such networks is designed for the projection of  cid:6  cid:6 T the input vector cid:2 z into the new feature space4 . In such a space, the original n-dimensional vector is transformed into an N1- dimensional vector. Each component of a new vector reﬂects the degree of proximity of input vector cid:2 z =  z1, . . . , zn T and weight vector , which is assigned to the ith neuron of the ﬁrst hidden layer, where i = 1, . . . , N1. Here  cid:5  cid:2 z −  cid:2  is a radial basis func-   cid:5  cid:2 z −  cid:2    cid:5  cid:2 z −  cid:2    cid:6  = exp  , . . . , w 1  in  − cid:5  cid:2 z−  cid:2   = exp   cid:2  w 1   =  cid:5   −  cid:2   , . . . , Φ  w 1  i1  w 1  N1  w 1  1  w 1    cid:6 T  n j=1   cid:5 2   cid:3    cid:3    cid:17    cid:16    cid:15    cid:14    cid:6 2   cid:6    cid:5   i  zj−w 1  s  ij  w 1  s  i  i  tion. A very unique feature is that the weights of this layer are not changed during the training process; instead they are set static using one of the following approaches:  4In machine learning, a feature space is a vector space associated with feature vectors, i.e., n-dimensional vectors of numerical features that represent objects.   122  A. Branitskiy and I. Kotenko   1  random initialization,  2  initialization by randomly selected training vectors,  3  initialization using the clustering method. In the experiments, the third approach was used, namely the K-means method, in which the number of clusters was equal to the dimension of the hidden layer, N1. The method is aimed at constructing points  cid:3  cid:2 ¯vi , known as centroids, and arranged in such a way that the summary distance   cid:4 K i=1  from them to the points, located in one of the vicinities of points  , is minimal.   cid:3  cid:2 ¯vi   cid:4 K i=1  The model of the radial basis function network is represented as follows:  Y   cid:2 z  = ϕ   cid:18   N1 cid:9   i=1  · Φ   cid:5  cid:2 z −  cid:2   w 1   i   cid:6  + θ 2   1  w 2  1i   cid:19   .  The algorithm for training the radial basis function network is given in Algorithm 4.  Algorithm 4: Algorithm for training the radial basis function network  1. Set a neural network structure, i.e., choose the dimension of the hidden layer N1  and activation function type ϕ on the output layer.  2. Specify the maximum number of training epochs T and the minimum value of  the total MSE ε.   cid:3  cid:2 ¯vi   cid:4 N1 i=1  3. Calculate the centroids  tialize the weight coefﬁcients w 1  components of these centroids, i.e., w 1  ij  of clusters using the K-means method and ini- ij of the neurons on the ﬁrst hidden layer by the := ¯vij  i = 1, . . . , N1, j = 1, . . . , n . 4. Set the counter of current iterations to zero  t := 0 , and initialize the weight ij of the neurons on the second output layer by arbitrary values  coefﬁcients w 2   i = 1, . . . , N2, j = 1, . . . , N1 .  5. Perform steps 5a–5b for each vector  cid:2 xk  k = 1, . . . , M  .  i   cid:6    cid:5   = ϕ  · y 1   = θ 2   , where x 2  = Φ  N1+1 w 2  ; w 2  j=1 ij ij  cid:6  for j = 1, . . . , N1.  a. Perform a feed-forward propagation of the signals—calculate output signals for each ith neuron located in the second layer according to the formula = 1 for y 2  x 2  i i j = N1 + 1; y 1   =  cid:2   cid:5   cid:2 xk −  cid:2  w 1  b. Update the weight coefﬁcients of the output layer of the neural network = α · by the Widrow-Hoff rule w 2  ϕ cid:4   cid:5  ij , uki is the desired output of the neural network in the ith neuron on the output layer for the kth instance from the training sample.  [20], where  cid:2 w 2  ij  uki − y 2   +  cid:2 w 2    cid:6  · y 1   := w 2   and y 1    cid:6  ·  cid:5   x 2  i  ij  ij  j  j  i  j  i  j  j  6. Increase the counter of current iterations, i.e., t := t + 1.   M   cid:2   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection 123 7. Terminate the algorithm if one of the conditions is satisﬁed, formally t  cid:3  T or —a total MSE of the  k=1 E   cid:2 xk    cid:2  ε, where E   cid:2 xk   = 1  ·  cid:2 NKall i=1   cid:6 T , . . . , y Kall   neural network, which has the output value and consists of 2 layers and 1 neuron on the output layer  Kall = 2, NKall = 1 , if NKall the vector  cid:2 xk is passed to the distributed layer  with the desired output vector  cid:2 uk =  cid:5    ; otherwise jump to step 5.  uki − y Kall    cid:2 y Kall   =  cid:5   y Kall   1   cid:6 T   cid:6 2  uk1, . . . , ukNKall   cid:5   2  i  The model of the Jordan recurrent network is an extension of the multilayer neural network model described above [25], which introduces an input-output feedback into the context layer as follows:  Y   cid:2 z  = ϕ  ⎛  ⎝  N1 cid:9   i=1  · ϕ  w 2  1i   cid:10  n cid:9   j=1  · zj + w 1   i0  · z0 + θ 1   i  w 1  ij  + θ 2   1   cid:11   ⎞  ⎠ .  The output signal z0 resulting from the analysis of one of the previous vectors is stored for several cycles before processing the new vector complemented by the value of this signal. This allows to remember some history of alternation of images of anomalous and normal network connections. In principle, the rest of the func- tioning of such networks is similar to the training of multilayer neural networks  see Algorithm 5 .  Algorithm 5: Algorithm for training the recurrent neural network 1. Initialize the values, i.e., z0 := 0. 2. Perform steps 1–6 of the algorithm for training the multilayer neural network  and store the output value in variable z0 after each iteration.  5.3.2 Neuro-Fuzzy Networks  In this section, we discuss neuro-fuzzy networks and an algorithm for their training. An approach used in the construction of intelligent kernel for detecting the network anomalies is neuro-fuzzy networks [26]. This approach reﬂects the ability of the human mind to make decisions under conditions of uncertainty and ambiguity.  Typically, such systems consist of ﬁve functional blocks [27]. The ﬁrst block is a rule database, which includes a set of fuzzy implications  rules  in the form “if A then B,” where A is called the premise  antecedent , and B is called the conse- quence  succedent . Such rules are mainly different from the traditional production rules in a way that each of the statements included in A and B is assigned  attributed   124  A. Branitskiy and I. Kotenko  to  a certain number between 0 and 1 to indicate the conﬁdence degree of the premise and the consequence. The second block is a database that contains a set of mem- bership functions. Such functions set the input linguistic variables to the transition from their crisp values to fuzzy linguistic terms. For each of such terms, an individual membership function is constructed, whose output value characterizes the measure of matching the input variable to a corresponding fuzzy set  term . The most frequently used membership functions are continuous piecewise differentiable  triangular and trapezoidal  functions or smooth functions  family of bell-shaped functions  with the range [0, 1] or  0, 1]. The third block is a fuzziﬁcation block whose role is to apply the speciﬁed membership function to the input argument. Each of the conjuncts Ai, which is a part of premise A = A1 ∧ . . . ∧ An, and consequence B are represented as the fuzzy statements zi is γi and y is  cid:5 , respectively, where zi and y are the linguistic variables, and γi and  cid:5  are the linguistic terms  i = 1, . . . , n . The result of a fuzzi- ﬁcation process is the calculated values of these fuzzy statements. The fourth block is a fuzzy inference block, which contains a set of fuzzy implications already inte- grated into its kernel and provides a mechanism  e.g., rules modus ponens or modus tollens  for calculating consequence B using the input set of conjuncts in premise A. To calculate the full verity degree of the left-hand side, T-norms are used, the most common examples of which are the minimum and product operators. On the output of the fuzzy inference block, one or more fuzzy terms along with the corresponding values membership functions are formed for linguistic variable y. The ﬁfth block is the defuzziﬁcation block, which restores the quantitative value of the linguistic variable y by its fuzzy values. Namely, the data obtained through the fuzzy infer- ence block operation is converted into quantitative values using one of the following methods: center of area, centre of gravity, center of sums, or the maximum of the membership function.  In the above fuzzy inference system, consequence B in all rules “if A then B” had a type of the fuzzy statement y is  cid:5 , which is not dependent on the linguistic variables encountered as a part of premise A. The approach proposed by Takagi and Sugeno is aimed at eliminating this drawback, and is based on introducing into the right-hand side of each rule a certain functional dependence on elements of its left- hand side, i.e., y = f  z1, . . . , zn  [26]. In real-world situations, one often has to deal with models of this type, in particular, when a person or device is not able to estimate accurately the values of the input parameters, but at the same time the control effect can be explicitly calculated by the known formula.  Neuro-fuzzy networks, also known as adaptive neuro-fuzzy inference systems  ANFIS  [27], are extensions of the Takagi-Sugeno model, which possess an element of adaptive adjustment  training  of parameters. These networks consist of ﬁve layers, where the input signals undergo changes, propagating sequentially from the ﬁrst to the last layer. Each fuzzy rule in these networks is represented as an element belonging to a set of rules of the form   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  125   cid:5    cid:3  if  ∧ . . . ∧ zn is γ jn  z1 is γ j1  1 y = f  j  z1, . . . , zn  = p j   n  0   cid:6   then · z1 + ··· + p j   n  + p j   1  · zn   cid:4 Q j=1  .  Here Q denotes a cardinality of the fuzzy rule set, in which each variable z1, . . . , zn has exactly r fuzzy terms; j1, . . . , jn denote numbers of fuzzy terms, correspond- ing to linguistic variables z1, . . . , zn, in the fuzzy rule at number j  1  cid:2  j1  cid:2  r, . . . , 1  cid:2  jn  cid:2  r . As in classical fuzzy inference systems, the left-hand side of this rule is the conjunction of fuzzy statements, which expresses the degree of matching the input crisp value zi to a particular linguistic term γ ji   according to value μ  zi  are  cid:21 −1 or the Gaussian function the bell-shaped function μ μ   zi  = exp In Fig. 5.2, a neuro-fuzzy network is shown. The node elements of the ﬁrst layer in a neuro-fuzzy network act as a fuzziﬁcation of the input linguistic variable zi cid:4 , and the output vector of this layer are the values   zi  =  cid:20  , where i = 1, . . . , n and j = 1, . . . , Q.   zi , where the most frequently used membership functions μ  1 +  zi − cij   aij2·bij   cid:22 −   zi − cij   aij 2  γ ji   i  γ ji   i  γ ji   i  γ ji   i   cid:23   i  Fig. 5.2 Neuro-fuzzy network   126  A. Branitskiy and I. Kotenko  k   cid:4    cid:4    knn  k11  γ k1   1  = μ  = Y  1    zi cid:4   , where i  × . . . × Y  1   th fuzzy set  term  γ j i cid:4    z1  × . . . × μγ kn    γ j cid:4    i cid:4  of this variable to the j  cid:4  = 1, . . . , n, j  j cid:4 i cid:4  = : Y  1  of membership function μ  cid:4  = 1, . . . , r. The second layer performs the creation γ j cid:4    μ i cid:4  of premises of fuzzy rules with their association with using the T-norm operation  product ; the kth output value of this layer can be considered as the weight assigned  zn , where k = to the kth rule Y  2  1, . . . , Q. Let us emphasize that Q  cid:2  rn in the absence of any conﬂicting rules. The ratio of a rule weight and the total sum of the weights of all the rules is calculated in the elements of the third layer; the output of this layer is the value normalized to[0, 1]: Y  3  . The result of the consequence of each of the rules, taking into account k the relative degree of its fulﬁllment received on the third layer, is calculated on the fourth layer. The kth output value of this layer represents an additive portion of the · f  k  z1, . . . , zn  = Y  3  · kth rule in the entire output of the network, i.e., Y  4   cid:5  p k  . At the ﬁfth output layer, there is a single neuron 0 responsible for summing the input signals received from the fourth layer nodes, formally Y  5  =  cid:2 Q  · f  i  z1, . . . , zn  =  cid:2 Q  · z1 + ··· + p k   · zn =  cid:2 Q  = Y  2  k cid:2 Q i=1 Y  2   = Y  3   + p k   i=1 Y  2  i  cid:2 Q   cid:6   .  1  n  k  k  k  n  i=1 Y  4   i  i=1 Y  3   i  i  Thereby the ANFIS model is represented as Y   cid:2 z  =  ·f  i  z1,...,zn  i=1 Y  2   i   cid:5   Q cid:2  i=1  μ  γ i1   1  =   z1  × . . . × μγ in    n  p i  0   cid:6  ·  cid:5   + p i   zn   z1  × . . . × μγ in    1  n   zn   · z1 + ··· + p i   n  · zn   cid:6   .  Q cid:2  i=1  μ  γ i1   1  For training, the author of the neuro-fuzzy network in [27] proposed a hybrid rule. According to this rule, the conﬁgurable parameters are decomposed into two parts: some of them are trained using the gradient descent algorithm, and the rest are changed using the least square method. Algorithm 6 is such an algorithm.  Algorithm 6: Algorithm for training the neuro-fuzzy network  1. Set the neuro-fuzzy network structure, i.e., the number of linguistic terms r and  the type of membership functions μγ j   ;  i  the total MSE ε;  2. Specify the maximum number of training epochs T and the minimum value of 3. Set the counter of current iterations to zero, i.e., t := 0, and initialize the n by arbitrary values  i = 1, . . . , n, j =  , . . . , p j   parameters aij, bij, cij, p j  0 1, . . . , Q .  , p j  1  4. Calculate the coefﬁcients p j  0  , . . . , p j   cid:23 n j=1 substituting the elements of training sample formula, the following system of equations can be obtained:  n using the least square method. By into the model   cid:3  cid:2 xi =  cid:22    cid:4 M i=1  , p j  1  xij   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  127  ⎛  ⎜ ⎜ ⎜ ⎝   cid:26   e11 e11x11 ... e11x1n ... e1Q e1Qx11 ... e1Qx1n e21 e21x21 ... e21x2n ... e2Q e2Qx21 ... e2Qx2n ... eM 1 eM 1xM 1 ... eM 1xMn ... eMQ eMQxM 1 ... eMQxMn  ...  ...  ...  ...  ...  ...  ...  ...   cid:27  cid:28  H  =  ·  ⎞  ⎟ ⎟ ⎟ ⎠   cid:29   ⎞  ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠  ⎛  ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝  p 1  0 p 1  1 ... p 1  n ... p Q  0 p Q  1 ... p Q  n  cid:2 p   cid:26   cid:27  cid:28   cid:29   ⎞  ⎛  ⎜ ⎝  =   cid:26   cid:27  cid:28   cid:29   u1 ... uM  cid:2 u  ⎟ ⎠  , where eij = μ  γ   j1   1   xi1 ×...×μ  xi1 ×...×μ  γ   xin    jn   n  .   xin   γ kn   n  Q cid:2  k=1  μ γ   k1   1  Usually M  cid:7   n + 1  · Q, therefore this system of equations does not always have solutions, in which case it is recommended to search for a vector  cid:2 p instead that satisﬁes the following condition [28]:  ξ  cid:2 p  =  cid:5 H ·  cid:2 p −  cid:2 u cid:5 2 =  H ·  cid:2 p −  cid:2 u T ·  H ·  cid:2 p −  cid:2 u  →  min ξ  cid:2 p  =  H ·  cid:2 p −  cid:2 u T ·  H ·  cid:2 p −  cid:2 u  =  =  H ·  cid:2 p T ·  H ·  cid:2 p  −  H ·  cid:2 p T ·  cid:2 u −  cid:2 uT ·  H ·  cid:2 p + +  cid:2 uT ·  cid:2 u =  cid:2 pT · HT · H ·  cid:2 p − 2 ·  cid:2 uT · H ·  cid:2 p +  cid:5  cid:2 u cid:5 2 = 2 · HT · H ·  cid:2 p − 2 ·  cid:2 uT · H = 2 · HT · H ·  cid:2 p − 2 · HT ·  cid:2 u = =  cid:2 0HT · H ·  cid:2 p = HT ·  cid:2 u ⇒  cid:2 p =  cid:20    cid:21 −1 · HT ·  cid:2 u  HT · H  d ξ  cid:2 p  d cid:2 p  5. Calculate coefﬁcients aij, bij, cij using the backpropagation algorithm in batch  mode as follows:  ,  M   cid:2    cid:2   · ∂  aij := aij − αa M bij := bij − αb M cij := cij − αc M 0 < αa  cid:2  1, 0 < αb  cid:2  1, 0 < αc  cid:2  1.  k=1 E   cid:2 xk   ∂aij k=1 E   cid:2 xk   M ∂bij k=1 E   cid:2 xk   M ∂cij  · ∂  · ∂   cid:2   ,  ,   128  A. Branitskiy and I. Kotenko  M   cid:2   k=1 E   cid:2 xk    cid:2  ε, where E   cid:2 xk   = 1  6. Increase the counter of current iterations, i.e., t := t + 1. 7. Terminate the algorithm if one of the following conditions is satisﬁed: t  cid:3  T is an MSE of the neuro- or fuzzy network, which consists of 5 layers and 1 neuron on the output layer if vector  cid:2 xk is passed to  Kall = 5, NKall the distributed layer  with the desired output value uk ; otherwise jump to step 4.  ·  cid:5  uk − y Kall   = 1 , and has the output value y Kall     cid:6 2  2  k  k  In contrast to sequential mode, in the previous algorithm the weight adjustment was performed only after presenting the entire training sample to the ANFIS input accumulated during the epoch.  5.3.3 Support Vector Machines  Support vector machine  SVM  [29] is one of the most widely used approaches for solving the tasks of classiﬁcation [30], regression [31], and prediction [32]. SVM has a simple geometric analogy, which is associated with the assumption that the elements of various classes may be linearly separated by subspaces. The main idea is to construct a linear hyperplane, which ensures the necessary partition of classiﬁed points. It is trivial that there may be more than one such hyperplanes  if any ; therefore it is necessary to provide an optimization criterion that would make it possible to select the most suitable surface among all those that are suitable. A quite natural and reasonable criterion is that the margin between the points, nearest to this hyperplane and belonging to different classes, is maximized.  Figure 5.3 schematically demonstrates two elements of two classes, Cα and Cβ. These elements can be separated by several different hyperplanes, which are described by the set of equations  cid:2 wT ·  cid:2 z − b = 0, and differ from each other by the normal vector  cid:2 w  specifying the slope of a hyperplane  and bias parameter b  specify- ing the ascent or descent of a hyperplane . Denote the left-hand side of this equation as y  cid:2 z , i.e., y  cid:2 z  =  cid:2 wT ·  cid:2 z − b. When substituting a speciﬁc value  cid:2 z cid:4  into this func- tion, three different variants are possible. The ﬁrst variant, y  cid:2 z cid:4   > 0, corresponds to the case when vector  cid:2 z cid:4  is above the hyperplane. For the hyperplane H depicted in Fig. 5.3, we note that y  cid:2 z cid:4   =  cid:2 wT ·  cid:2 z cid:4  − b > 0 is equivalent to  cid:2 z cid:4  ∈ Cα. In the second variant, the inequality y  cid:2 z cid:4   < 0 means that vector  cid:2 z cid:4  is below hyperplane H and therefore  cid:2 z cid:4  ∈ Cβ. Finally, the third variant, y  cid:2 z cid:4   = 0, corresponds to the boundary case when vector  cid:2 z cid:4  is the decision of the equation, which means that it belongs to  the separating hyperplane. boundary separating hyperplanes, and are described by the equations  cid:2 wα 0 and  cid:2 wβ  Figure 5.3 shows several separating surfaces. Two of them, Hα and Hβ, refer to T ·  cid:2 z − bα = T ·  cid:2 z − bβ = 0, are parallel to the optimal hyperplane HO, are an equal   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  129  Fig. 5.3 Hyperplanes in the support vector machine  distance d from HO, and are the nearest to the elements of classes Cα and Cβ among  Such a hyperplane HO can be described using the equation  cid:2 wO  all the other hyperplanes. The nearest elements marked on the ﬁgure as  cid:2 α0,  cid:2 α1,  cid:2 β0,  cid:2 β1, which are called support vectors and these vectors affect parameters  cid:2 w and b, which are conﬁgured during the learning process. T ·  cid:2 z − bO = T =  wO1, . . . , wOn T , then  cid:2 wα =  cid:2 wO, bα = bO + ε,  cid:2 wβ =  cid:2 wO, bβ = 0, where  cid:2 wO bO − ε because of the parallelism of the planes  ε > 0 . Without loss of generality, it can be assumed that ε = 1  otherwise this can be achieved by dividing both parts of equations by ε . After simple transformations, the equations of the two separating hyperplanes, Hα and Hβ, take the form  cid:2 wO T ·  cid:2 z − bO = −1, while classes Cα and Cβ are presented as Cα =  cid:3  cid:2 z , Cβ =  cid:3  cid:2 z  cid:30   cid:2 wO The margin value d can be calculated by dividing the distance between the bound- ary hyperplanes, Hα and Hβ, by two, which is equal to dα + dβ. Figure 5.3 shows that the value of dα + dβ is the length of the projection of the difference of any pair of support vectors    cid:2 α1,  cid:2 β1  from different classes onto the normal of a hyper- plane  cid:2 wO, i.e., the scalar product of the normal  cid:2 wO cid:5   cid:2 wO cid:5  of the optimal hyperplane HO and the vector difference  cid:2 α1 −  cid:2 β1. Thus we obtain the following expression: 2 · d =  cid:5   cid:2 wO cid:5  ·  cid:5   cid:2 α1 −  cid:2 β1 = bO+1− bO−1  = 2 cid:5   cid:2 wO cid:5  . This yields  cid:2 wO to d = dα = dβ = 1 cid:5   cid:2 wO cid:5  . Consequently, the SVM model is described using the for-  T ·  cid:2 z − bO = 1 and  cid:2 wO  T ·  cid:2 z − bO  cid:2  −1  T ·  cid:2 z − bO  cid:3  1  T· cid:5   cid:2 α1−  cid:2 β1  cid:5   cid:2 wO cid:5   T·  cid:2 α1−  cid:2 wO  cid:5   cid:2 wO cid:5    cid:6  =  cid:2 wO  =  cid:2 wO   cid:30   cid:2 wO   cid:5   cid:2 wO cid:5   T·  cid:2 β1   cid:4    cid:4    cid:30   cid:30    cid:30   cid:30   .   cid:6   T  mula   130  A. Branitskiy and I. Kotenko  Y   cid:2 z  = sign   cid:5   cid:2 wO  T ·  cid:2 z − bO   cid:6  = sign  wOi · zi − bO   cid:19   .   cid:18   n cid:9   i=1  Now we consider the training algorithm of SVM subject to the existence of the linear hyperplanes Hα and Hβ, which correctly separate all training instances  see Algorithm 7 .  Algorithm 7: Algorithm for training the support vector machine 1. Prepare training data in the form {  cid:2 xi, ui }M  ui =  cid:31  cid:2 xi ∈ Cα    −  cid:31  cid:2 xi ∈ Cβ    =  i=1, where  cid:14   1, if  cid:2 xi ∈ Cα −1, if  cid:2 xi ∈ Cβ.  2. Calculate Lagrange multipliers λ O   i  2   cid:21   ·  λ1,...,λM  by solving the optimization task − 1 with the restrictions that  λi −→ max  T ·  cid:2 xj + M cid:2  i=1  λi · λj · ui · uj ·  cid:2 xi  M cid:2  M cid:2  j=1 i=1 λi · ui = 0 and λi  cid:3  0  i = 1, . . . , M  . M cid:2  i=1 To correctly classify the training object { cid:2 xi}M i=1, it is necessary and sufﬁcient that  cid:20   cid:2 wT ·  cid:2 xi − b and ui are simultaneously pos- signs of the values of expressions  cid:21  · ui is greater itive or negative. Therefore it holds that the sign of than zero  i = 1, . . . , M  . Taking into account the assumption of the possibility of the linear separation of the training instances, we note that the arguments of  cid:21  · ui  cid:3  1. both multipliers on the left side is not less than 1; thus Because the distance between the optimal separating hyperplane and the ele- ments to classify is inversely proportional to the norm of the normal vector to this hyperplane, the maximization of this margin is equal to the optimization criteria  cid:3    cid:2 w  = 1 . According to the Kuhn-Tucker theo- rem [33], the minimization of the quadratic functional  cid:3    cid:2 w  can be determined using the method of Lagrange multipliers as follows: L    cid:2 w, b, λ1, . . . , λM   = ·  cid:2 wT ·  cid:2 w − M cid:2  . The necessary i=1  ·  cid:2 wT ·  cid:2 w −→ min cid:2 w   cid:20   cid:2 wT ·  cid:2 xi − b   cid:20   cid:2 wT ·  cid:2 xi − b   cid:20   cid:2 wT ·  cid:2 xi − b  −→ min cid:2 w,b  ·  cid:5   cid:2 w cid:5 2 = 1  λi ·   cid:10    cid:11   1 2  2  2   cid:21  · ui − 1  cid:5   cid:2 wO, bO, λ O   , . . . , λ O  M L    cid:2 wO, bO, λ1, . . . , λM   = L  1   cid:6   max λ1,...,λM  cid:6  , i.e., the point satisfying  cid:6  =   cid:5   cid:2 wO, bO, λ O   , . . . , λ O  M  1  conditions for a saddle point  , . . . , λ O  M  1  L  , to exist are conditions of nulliﬁcation of partial min cid:2 w,b derivatives of the Lagrangian L    cid:2 w, b, λ1, . . . , λM   with respect to variables  cid:2 w, b:  the condition max λ1,...,λM   cid:5   cid:2 w, b, λ O    5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  131   cid:16   ∂L   cid:2 w,b,λ1,...,λM   ∂L   cid:2 w,b,λ1,...,λM    ∂  cid:2 w  ∂b  =  cid:2 0 = 0  ⎧ ⎪⎪⎨  ⎪⎪⎩  ⇒  λi · ui ·  cid:2 xi =  cid:2 0   cid:2 w − M cid:2  i=1 λi · ui = 0. M cid:2  i=1  The Lagrangian takes the following form: L    cid:2 w, b, λ1, . . . , λM   = 1 ·  cid:2 wT ·  cid:2 w − M cid:2  i=1 T ·  cid:2 xj − M cid:2  i=1  λi · λj · ui · uj ·  cid:2 xi  2  2  M cid:2  j=1 · M cid:2  i=1  λi = 1 · M cid:2  i=1 λi = − 1  M cid:2  M cid:2  i=1 j=1 + M cid:2  T ·  cid:2 xj + M cid:2  λi. The required values λ O  i=1 i=1  i = 1, . . . , M   are calculated by the Lagrangian maximization L  λ1, . . . , λi · ui = 0 and λi  cid:3  0 λM   ≡ L    cid:2 w, b, λ1, . . . , λM   with the restrictions  i = 1, . . . , M  . For this, one can apply sequential minimal optimization [34].  λi · λj · ui · uj ·  cid:2 xi  M cid:2  j=1  M i=1   cid:2   2  i  λi · ui ·  cid:2 wT ·  cid:2 xi + b · M cid:2  i=1 λi · λj · ui · uj ·  cid:2 xi  λi · ui+ T ·  cid:2 xj  i  λ O   3. Calculate the normal vector and free coefﬁcient in the optimal separating hyper- plane equation. Vector  cid:2 wO is represented as  cid:2 wO= M cid:2  ·ui· cid:2 xi. At the saddle i=1 point    cid:2 wO, bO , according to the complementary slackness condition, the fol-  cid:6  = 0, where i = 1, . . . , M . lowing identity holds: λ O   cid:23  ⊆ {1, . . . , M} , The  cid:2 xij vectors, for which non-zero λ O  i1, . . . , i %M are called support vectors, which satisfy the equation of one of the boundary separating hyperplanes  Hα or Hβ . Based on this, we can calculate the coef- ﬁcient bO =  cid:2 wO − uij , where  cid:2 xij is any of support vectors. Thereby the normal vector  cid:2 wO is represented as a linear combination of support vectors  cid:2 xij  j = 1, . . . , %M  .   cid:6 ·ui−1 exists  ij ∈  cid:22   T· cid:2 xi−bO  ·  cid:5  cid:5   cid:2 wO  T ·  cid:2 xij  ij  i  4. Clarify the SVM model as follows: ⎛  Y   cid:2 z  = sign  ⎝  ·  cid:2 xij  T ·  cid:2 z − bO  ⎠ ,  wij  %M cid:9   j=1  ⎞  where wij  cid:22  i1, . . . , i %M  = λ O   cid:23   · uij , and the summation operator is applied to the index subset  ij of the training sample, which corresponds to support vectors.  If the objects from different classes cannot be separated linearly, two methods may be used, both of which are aimed at decreasing the empirical risk on the elements of the training set. The ﬁrst approach is to apply special transformations, namely, kernels ϕ for transition to a new&n-dimensional space [35]. It is supposed that in a new space,   132  A. Branitskiy and I. Kotenko  which is a result of the mapping  cid:2 ϕ   cid:2 z  =  ϕ1   cid:2 z  , . . . , ϕ&n   cid:2 z  T , there is a hyperplane satisfying the previously speciﬁed criteria. The second approach is based on intro- ducing a penalty function, which allows us to ignore some of the incorrectly classiﬁed objects based either on the total quantity or on the total distance from the separat- ing hyperplane. The ﬁrst case means searching for such a separating hyperplane, Y   cid:2 xi   cid:12 = ui   . which provides a minimum value of the characteristic function   · dist  cid:2 xi, HO , where Y   cid:2 xi   cid:12 = ui In the second case, the objective function is dist ·,·  is the distance function between the indicated  vector, hyperplane  argument pairs within the given metrics.  M i=1  M i=1   cid:2    cid:2    cid:31    cid:31   5.4 Training the Binary Classiﬁer for Detecting Network  Attacks  The following sections discuss how to calculate network parameters and optimize the weights of the binary classiﬁer, and introduces an algorithm for detecting network attacks.  5.4.1 Calculating and Preprocessing Network Parameters  Our proposed approach for detecting network attacks using a binary classiﬁer consists of three steps. The ﬁrst step is to calculate the network parameters and preprocess them. The second step is genetic optimization. The third step is detecting network attacks using a binary classiﬁer.  We have developed a network analyzer, which allows us to construct 106 param- eters to describe the network connections between hosts. These parameters include connection duration, network service, the intensity of sending special packets, the number of active connections between concrete IP address pairs  one of the criteria of DoS attacks , the binary characteristic of changing the TCP5 window scale after estab- lishing an actual session, the current state of the TCP connection, different attributes of the presence of scanning packet at the levels TCP, UDP,6 ICMP,7 IP,8 etc.  To calculate the statistical parameters of network attacks, we have used the adapted sliding window method. This method basically splits a given time interval = [0, L] with length L, during which a continuous observation of a number of  cid:2  L   cid:4   parameters is carried out, into several smaller intervals  cid:2  L , . . . ,  cid:2  L δ· k−1  with  cid:4   cid:2  L, the beginning of each has an offset 0 < δ  cid:2  L  cid:4  0 relative identical length 0 < L  ,  cid:2  L δ   cid:4     cid:4    0  5Transmission Control Protocol. 6User Datagram Protocol. 7Internet Control Message Protocol. 8Internet Protocol.   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  133  Fig. 5.4 Sliding window method   cid:4   δ  ’   cid:4  δ·i  cid:2 L   cid:2   cid:2  L   0 , then k = 1 +  cid:13  L−L  k−1 i=0  cid:14 . During time intervals  cid:2  L  ⊆  cid:2  L   cid:4  δ·i  cid:2 L to the beginning of the previous interval  see Fig. 5.4 . If and 0 ’  cid:4    cid:4   , . . . ,  cid:2  L k i=0 δ· k−1 , the snapshots of the parameter values ω0, . . . ωk−1 are made and their average value  intensity  ¯ω within the time window with length L  cid:4  is calculated by the for- mula ¯ω = 1  cid:4  was chosen parameter L was ﬁve seconds. The length of the smoothing interval L to be 1 s. The offset δ was set to half a second. Supposedly, such approach eliminates sparse and occasional network bursts, and decreases the false positive rate.  ωi. In the experiments, we have used an interval whose value for  · k−1 cid:2  i=0  0  k  For preprocessing, we have used principal component analysis  PCA  [36], which  is described as a sequence of the steps described in Algorithm 8.  Algorithm 8: Principal component analysis  1. Calculate the mathematical expectation of the random vector, which in our case  is represented by the set of training elements   = 1  : ¯ cid:2 x =  ¯x1, . . . , ¯xn T = E   cid:3  cid:2 xi =  cid:22    { cid:2 xi}M i=1  xij  M   cid:23 n j=1   cid:4 M i=1   cid:10    cid:2 xi =  · M cid:2  i=1  1 M  · M cid:2  i=1  xi1  , . . . , 1 M   cid:11 T  xin  .  · M cid:2  i=1  2. Generate elements of an unbiased theoretical covariance matrix  cid:6  =   cid:20  σij   cid:21  i=1,...,n j=1,...,n  :  σij = 1  · M cid:9  k=1   xki − ¯xi  ·  cid:20   M − 1 i=1 and eigenvectors { cid:2 νi}n   cid:21   xkj − ¯xj  .  3. Calculate eigenvalues {λi}n  i=1 of the matrix  cid:6  as the roots of equations  for this purpose we have used the Jacobi rotation in relation to matrix  cid:6  :  4. Sort eigenvalues {λi}n  { cid:2 νi}n  i=1:  i=1 in decreasing order and corresponding eigenvectors   cid:14   det   cid:6  − λ · I  = 0   cid:6  − λ · I  ·  cid:2 ν =  cid:2 0.  λ1  cid:3  λ2  cid:3  . . . λn  cid:3  0.   134 5. Select the required number &n  cid:2  n of principal components as follows:  A. Branitskiy and I. Kotenko  &n = min{jς j   cid:3  ε}n j=1  ,  where ς j  =  cid:2 j expertly;  λi cid:2  λi  i=1 n i=1  is an informativeness measure, 0  cid:2  ε  cid:2  1 is a value selected  6. Center the input feature vector  cid:2 z as  cid:2 zc =  cid:2 z − ¯ cid:2 x. 7. Project the centered feature vector  cid:2 zc into the new coordinate system, described  by the orthonormalized vectors { cid:2 νi}&n i=1:   cid:2 y =  y1, . . . , y&n T =    cid:2 ν1, . . . ,  cid:2 ν&n T ·  cid:2 zc,  where yi =  cid:2 νi  T ·  cid:2 zc is called ith principal component of vector  cid:2 z.  Figure 5.5 demonstrates the dependence of the informativeness measure ς on the  In our experiments, we have compressed the dimensionality of the input vectors  number of principal components. to 33, which corresponds to ε = 0.99. 5.4.2 Genetic Optimization of the Weights of the Binary  Classiﬁer  To accelerate the training process, we use genetic operators such as crossover G1, mutation G2, and permutation G3. After several training epochs, the inner weights of the adaptive classiﬁers are tuned. To this end, we create two copies of the original classiﬁer in which some weight vectors have been modiﬁed. The rule for genetic optimization of weights within the classiﬁer is represented as follows:  Fig. 5.5 Informativeness measure in PCA   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  135   136  A. Branitskiy and I. Kotenko  Here we represent each weight vector of the neuron as a single chromosome, which consists of several genes. The quantity of genes within this chromosome is equal to the dimension of the layer before the position of the corresponding neuron  encoded by this chromosome . From step 4a, it can be seen that the bit content of genes persists inside descendants while applying the crossover: the boundary specifying the separation of the chromosome passes between adjacent genes without violating their integrity. At the same time, the operators of mutation and permutation  steps 4b and 4c  are applied only to the part of the selected gene corresponding to the mantissa of a 64-bit real number. Such restriction is due in order to avoid an explosive growth of the gene content. Step 6 is performed for each classiﬁer in a separate thread: after making the genetic corrections on the child classiﬁers, it is necessary to calculate the level of their suitability. First, we perform parallel training of the child classiﬁers along with their ancestor during several epochs. From these three classiﬁers, the one that possesses the greatest value of the ﬁtness function is −1. selected. Instead of the ﬁtness function, here we used the inverse of the MSE, E Subsequently the newly formed classiﬁer B will be used as the parent to generate other classiﬁers, B  , which will compete again with B.  and B   cid:4  cid:4    cid:4   5.4.3 An Algorithm for Network Attack Detection  The algorithm for classifying network attacks using the binary classiﬁer %Y can be represented as shown in Algorithm 10.   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  137  Algorithm 10: Network attack detection algorithm 1. Select a binary classiﬁer  detector  %Y and specify its parameters. = %C%i %C%j ⊆ C for training the detector %Y . 2. Choose the class sets %C˜i˜j =  cid:22  cid:20   cid:2 x%k 3. Prepare the training data ϒX  LS  %C˜i˜j  %k=1, where  , ¯c%k   cid:21  cid:23  %M  *  =  %M = X  LS  %C˜i˜j  cid:14  ¯c%k %i, if u%k %j, if u%k ⎧ ¯y%i = ⎨ ¯y%j = 1,  =  u%k  ⎩  is the cardinality of the training set for detector %Y , ∈ {−1, 0} = 1 +−1 0  are class labels, and  is the desired output.  , if ∃%C ∈ %C%i  cid:2 x%k if ∃%C ∈ %C%j  cid:2 x%k  ∈ %C ∈ %C  The parameter value ¯y%i is chosen to be −1 or 0, depending on a type of the classiﬁer %Y and the activation membership function on its output layer. 4. Train the binary classiﬁer %Y using data ϒX  LS  %C˜i˜j 5. Classify the input object  cid:2 z represented as a feature vector: if %Y   cid:2 z  < ¯h˜i˜j, then object  cid:2 z belongs to one of the classes %C%i; if %Y   cid:2 z   cid:3  ¯h˜i˜j, then object  cid:2 z belongs to one of the classes %C%j. Here ¯h˜i˜j can be interpreted as an activation threshold of detector %Y . More precisely, by denoting C0 as a class of normal connections, if we have %C%i = {C0} ∧ %Y   cid:2 z  < ¯h˜i˜j, then the detector %Y recognizes object  cid:2 z as  = ¯y%i  +¯y%j  .  2  normal.  5.5 Schemes for Combining the Binary Classiﬁers  This section discusses how to combine detectors using low-level schemes and aggre- gate compositions.  5.5.1 Low-Level Schemes for Combining Detectors  In this section we consider several schemes that combine the binary classiﬁers and are designed for associating an object with one of the  m + 1  class labels. : Rn → {0, 1}  k = 1, . . . , m  is trained on data {  cid:2 xl, [¯cl = k] }M l=1, and functioning the group of detectors F  i  is j described using the exclusionary principle:  In the one-against-all approach, each detector F  i  jk   138  A. Branitskiy and I. Kotenko    cid:2 z  =  F  i  j   cid:16 {0} k   cid:3    cid:30   cid:30   cid:30 F  i  jk    cid:2 z  = 1   cid:4 m k=1  if ∀k ∈ {1, . . . , m} F  i  otherwise.  jk    cid:2 z  = 0   cid:20  m+1 2   cid:21  =  m+1 ·m  In the one-against-one approach, each ’{  cid:2 xl, 1¯cl = k1  }M  is trained on the set of objects that belong to only two classes, k0 and k1, namely, using the set {  cid:2 xl, 0¯cl = k0  }M l=1, where 0  cid:2  k0 < k1  cid:2  m, and the group l=1 of detectors can be set using max-wins voting as follows: ¯c−1 cid:9   detectors F  i  jk0k1   cid:17     m cid:9    cid:16   2    F  i  j¯ck    cid:2 z  = 0    +    F  i  jk¯c    cid:2 z  = 1  .  =  F  i  j  arg max  ¯c∈{0,...,m}  k=¯c+1  k=0  One of the variations of the previous approaches for combining the detectors is the classiﬁcation binary tree. Formally speaking, this structure is recursively deﬁned as follows:  CBTμ =   cid:14  cid:17 F  i   jLμRμ  μ,  , CBTLμ  , CBTRμ   cid:18 , if μ  cid:3  2 if μ = 1.  Here μ = {0, . . . , m} is the original set of class labels, Lμ  cid:3  μ is a randomly generated or user-deﬁned subset of μ  Lμ < μ , Rμ = μ \ Lμ, CBTLμ is a left classiﬁcation binary subtree, CBTRμ is a right classiﬁcation binary subtree, and F  i  jLμRμ  cid:23 M l=1, is a node detector trained on elements i.e., the detector output is conﬁgured to be 0 if the input object  cid:2 xl has label ¯cl ∈ Lμ, and 1 if the input object Xl has label ¯cl ∈ Rμ. Therefore the group of detectors nested in each other is given via the recursive function φ i  , which speciﬁes sequential j fragmentation of the set μ as follows:   cid:30 ¯cl ∈ Rμ   cid:30 ¯cl ∈ Lμ    cid:2 xl, 0     cid:2 xl, 1    cid:23 M l=1  ’  cid:22    cid:22    cid:30    cid:30    μ, cid:2 z , ⎧ ⎪⎨  = φ i   j  F  i  j   μ, cid:2 z  =  φ i  j  μ, φ i  j φ i  j   cid:20   cid:20    cid:21   cid:21   Lμ, cid:2 z Rμ, cid:2 z  ⎪⎩  if μ = 1 , if μ  cid:3  2 ∧ F  i  , if μ  cid:3  2 ∧ F  i   jLμRμ  jLμRμ    cid:2 z  = 0   cid:2 z  = 1.  Using function φ i  j  , an unambiguous search can be performed on the class label of the input object. This is possible because if a disjoint partition of the set of class labels takes place at each step during the descent down the classiﬁcation tree, then only one possible class label remains after reaching the terminal detector. Therefore conﬂict cases of classiﬁcation through the classiﬁcation tree are not possible within group F  i  , although they may occur for the other two approaches considered above. j detec- tors into a linked dynamic structure, which can be given by the following formula:  Another approach is a directed acyclic graph that combines   cid:21  =  m+1 ·m  m+1 2   cid:20   2   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  139  ⎧ ⎨   cid:17 F  i   jμk0k1  , DAGμ\{k0}, DAGμ\{k1} cid:18 , if μ  cid:3  2, where k0 ∈ μ,  DAGμ =  ⎩  μ,  k1 ∈ μ if μ = 1.  As with the one-against-one approach, each node detector F  i   on the elements {  cid:2 xl, 0¯cl = k0  }M l=1 graph is performed using a recursive function ξ i  j element detachment of set μ:  ’{  cid:2 xl, 1¯cl = k1  }M  jμk0k1  is trained based l=1  k0 < k1 . Bypassing the , which speciﬁes an element-by-   μ, cid:2 z , ⎧ ⎪⎨  = ξ i   j  F  i  j   μ, cid:2 z  =  ξ i  j  if μ = 1  ⎪⎩   μ \ {k1} , cid:2 z  , if μ  cid:3  2 ∧ F  i   μ \ {k0} , cid:2 z  , if μ  cid:3  2 ∧ F  i   μ,   cid:2 z  = 0 ξ i    cid:2 z  = 1. j ξ i  j votes for the k0th class of the object  cid:2 z, i.e., F  i   jμk0k1  jμk0k1  If detector F  i     cid:2 z  = 0, then label k1 is removed from set μ as obviously false, otherwise label k0 is excluded. The process is repeated until set μ degenerates into a single-element set.  jμk0k1  jμk0k1  Table 5.1 shows the characteristics of the considered schemes for combining detectors in a multi-class model, which attempts to associate an input object with one or more  m + 1  class labels.  Only one, namely the classiﬁcation binary tree, has a variable number of detectors, which can be used for classifying objects. The minimum value is reached when detector F  i  jLμRμ is activated, which is located in the tree root and trained to recognize   cid:2 z  = 1 , i.e., when only one object among all the rest and F  i  Lμ = 1  Rμ = 1 . The maximum value is achieved when the tree is represented by a sequential list and the most remote detector is activated in it. In the case of a balanced tree, this indicator can be equal to  cid:13 log2   m + 1  cid:14  or  cid:19 log2    cid:2 z  = 0  F  i    m + 1  cid:20 .  jLμRμ  jLμRμ  Table 5.1 Characteristics of the schemes for combining detectors Scheme for combining detectors  Number of detectors to be trained  m  m+1 ·m  One-against-all One-against-one Classiﬁcation binary tree Directed acyclic graph  m+1 ·m  m  2  2  The minimum number of detectors involved in the classiﬁcation of objects m  m+1 ·m  The minimum number of detectors involved in the classiﬁcation of objects m  m+1 ·m  2  1  m  2  m  m   140  A. Branitskiy and I. Kotenko  5.5.2 Aggregating Compositions  In this section, we present four aggregating compositions. Under the notation  cid:22  F  j    cid:23 P j=1 we mean basic classiﬁers.  1. Majority voting can be represented as follows:  G  cid:2 z  =   cid:31   k ∈ F  j   cid:2 z      >  ⎧ ⎨ ⎩k   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   P cid:9   j=1  1 2  · m cid:9  i=0  P cid:9   j=1   cid:31   i ∈ F  j   cid:2 z      ⎫ ⎬  m  ⎭  .  k=0  As a result of applying function G to argument  cid:2 z, the set of labels is formed for those classes for which more than half of the votes was given. G  cid:2 z  =  2. An extension of the previous function is weighted voting, which is described as  =  ⎧ ⎨ ⎩k   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   P cid:9   j=1  ωj ·  cid:31   k ∈ F  j   cid:2 z     = max i∈{0,...,m}  P cid:9   j=1  ωj ·  cid:31   i ∈ F  j   cid:2 z      ⎫ ⎬  m  ⎭  .  k=0  The coefﬁcients ωj are chosen in such a way that  can be calculated, for example, with the following formula:  ωj = 1 is satisﬁed. These  P cid:2  j=1  ωj =   cid:2    cid:22   cid:2 xk  P i=1    cid:30    cid:30 ¯ck ∈ F  j    cid:2 xk    cid:22   cid:2 xk   cid:23 M k=1  cid:30 ¯ck ∈ F  i    cid:2 xk    cid:23 M k=1   cid:30   .  In this formula, each coefﬁcient ωj  j = 0, . . . , P  is a proportion of training vectors correctly  and potentially with conﬂicts  recognized by classiﬁer F  j  among all other training vectors, which are correctly recognized by classiﬁers F  1 , . . . , F  P . When substituting the values ωj = 1 P into the weighted voting formula, we obtain its speciﬁc analogue: simple voting  max-wins voting .  3. Stacking is represented as a composition of some function F with basic classi- ﬁers F  1 , . . . , F  P  and identical function ID and speciﬁed using the following formula9:  G  cid:2 z  = F   cid:20   F  1   cid:2 z , . . . , F  P   cid:2 z , cid:2 z   cid:21   .  In this formula, function F aggregates input vector  cid:2 z, and output results of func- tions F  1 , . . . , F  P .  4. The Fix and Hodges method is based on the idea of forming a competence area for each classiﬁer [37]. Within this area, the upper-level classiﬁer G fully trusts  9For the sake of simplicity, some features related to the training of classiﬁers F and F  1 , . . . , F  P  on various subsamples are omitted here.   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  141  the corresponding basic classiﬁer. The choice of solution  resulting in the class label  by classiﬁer G depends on how correctly a basic classiﬁer recognizes %M  cid:2  M trainings instances, which are located in the nearest vicinity of vector  cid:2 z. Consequently, function G is speciﬁed as follows:   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   %M cid:9   j=1  cid:26   G  cid:2 z  = P  l=1  F  l   cid:2 z    cid:31   F  l    cid:2 xij    =  cid:22 ¯cij   cid:23    = max  k∈{1,...,P} D %M  k   ,   cid:27  cid:28   l  D %M   cid:29   i1, . . . , i %M   cid:23  = arg min %I⊆{1,...,M}  %I= %M     cid:9   i∈%I  ρ   cid:2 xi, cid:2 z  .  In this formula, the labels of those classes are combined for which the basic classiﬁers have voted with the greatest number of correctly recognized training objects the distance from which to vector  cid:2 z is minimal in accordance with metric of the training sample {  cid:2 xi, ¯ci }M i=1 ρ. To this end, the index subset is calculated and vectors with numbers of such a subset satisfy the speciﬁed requirement. The modiﬁcations can be represented as the following formulae:   cid:22  i1, . . . , i %M   cid:23   G  cid:2 z  = P  l=1  F  l   cid:2 z   = max  ∗ k∈{1,...,P} D %M   k   ,   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   %M cid:9   j=1  cid:26    cid:23     cid:31     =  cid:22 ¯cij F  l    cid:2 xij , cid:2 z  cid:20   cid:2 xij   cid:21    cid:29   ρ  cid:27  cid:28  ∗  l  D %M  G  cid:2 z  = P  l=1  F  l   cid:2 z    cid:9   j∈{1,...,M} ρ  cid:2 xj , cid:2 z  cid:2 %r  cid:26    cid:31   F  l   cid:2 xj  =  cid:22 ¯cj   cid:23    = max  k∈{1,...,P} D  ∗∗ %r   k   .   cid:27  cid:28  ∗∗  l  D %r   cid:29   In the ﬁrst formula, the weight encouragement of classiﬁers is performed, which is directly proportional to the proximity of the correctly classiﬁed training object and test object  cid:2 z. In the second formula, the summation is carried out only on the instances located in the vicinity with speciﬁed radius%r from test object  cid:2 z.  ⎛  ⎜ ⎜ ⎜ ⎜ ⎜ ⎝   cid:22   ⎛  ⎜ ⎜ ⎜ ⎜ ⎜ ⎝  ⎛  ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   ⎞  ⎟ ⎟ ⎟ ⎟ ⎟ ⎠  ⎞  ⎟ ⎟ ⎟ ⎟ ⎟ ⎠  ⎞  ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠   142  A. Branitskiy and I. Kotenko  5.5.3 Common Approach for Combining Detectors  This section presents a novel approach for combining the detectors, which allows the construction of multilevel schemes. Figure 5.9 presents an example of combining the detectors  this was used during the experiments described in the next section .  and combined into a group F  k   The detectors are designated as F  k  ij  using one of the low-level schemes considered earlier in Sect. 5.5.1. Detectors within different groups are trained using various subsamples from an initial training set. As shown in the ﬁgure, the number of detectors within each group is equal to m, which corresponds to the case of the one-against-all scheme, and the number of groups for each kth basic classiﬁer is denoted as qk. The combination of groups into the basic classiﬁer F  k  is performed using the following hybrid rule represented as a mixture of majority voting and max-wins voting:   cid:4 qk i=1  F  k  i   cid:3   i  F  k   cid:2 z  =  ⎧  ⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎩  ¯c   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   cid:30   qk cid:9   j=1  cid:26    ¯c ∈ F  k   j       cid:2 z   >   cid:27  cid:28   cid:8 k  ¯c    cid:29   · qk ∧  cid:8 k  ¯c  = max  ¯c cid:4 ∈{0,...,m}  cid:8 k   cid:4  cid:21    cid:20 ¯c  1 2  m  ⎫  ⎪⎪⎪⎪⎪⎬ ⎪⎪⎪⎪⎪⎭  .  ¯c=0  It should be noted that due to the technique described in [4], it becomes pos- sible to construct the multilevel schemes without a strict binding to the aggre-  Fig. 5.9 Combining the detectors via the scheme one-against-all   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  143  gating composition. In such schemes the upper-level classiﬁer combines the basic classiﬁers, which is independent of the low-level scheme for combining the detec- tors  i.e., binary classiﬁers . Moreover, the implemented interpreter and algorithm of cascade learning of classiﬁers allow to perform the construction of the universal structure for storage and presentation of classiﬁers, namely, classiﬁer tree, and tune the node classiﬁers.  In the following sections, two experiments are described, along with the dataset that was used with them.  5.6 Experiments  5.6.1 The Dataset  This section describes an experimental dataset and a method of calculating perfor- mance indicators based on a 5-fold cross-validation.  For experiments, we have used the DARPA 1998 dataset.10 This dataset is widely used for investigating methods for network attack detection, however, it is subject to severe criticism due to its synthetic nature [38]. In particular, paper [39] outlines that classiﬁcation between normal and anomalous trafﬁc could be carried out using solely IP-header ﬁelds  e.g., TTL . We have extracted more than 100 parameters, but among them there were no such ﬁelds, because their values did not reﬂect the presence or absence of anomaly  for example packets within a TCP-session may be transmitted on different routes in real-world networks . The parameters considered in this research are mostly statistical, and the DARPA 1998 dataset is used for experiments because of convenience: the CSV-ﬁles containing the class labels are available along with pcap-ﬁles. Moreover, the outdated attack types were excluded  e.g., buffer overﬂow in mail service or attack teardrop realizing the old vulnerability inside the TCP IP stack during the defragmentation of packets , we have dealt with the anomalies that are characterized by some deviations in statistical parameters. This way, seven classes were selected: six anomalous classes, namely neptune, smurf  DoS , ipsweep, nmap, portsweep, and satan  Probe ; and a normal class.  While forming the dataset, we have used the binary network traces “Training Data Set 1998,” which were collected on the Wednesday of the ﬁrst week, on the Monday of the second week, on the Tuesday of the second week, on the Monday of the third week, on the Wednesday of the third week, on the Friday of the third week, on the Tuesday of the fourth week, and on the Wednesday of the fourth week of the experiment.  10https:  www.ll.mit.edu ideval data 1998data.html   144  A. Branitskiy and I. Kotenko  First, we introduce some performance indicators: GPR is the true positive rate, FPR is the false positive rate, GCR is the correct classiﬁcation rate, and ICR is the incorrect classiﬁcation rate. All of these indicators will be calculated on unique elements that were not used while training.  C  C  C  C  5 3   cid:15 3  containing ¯M  for which  ¯X  TS 1  {Cl} ≈ ··· ≈ ¯X  TS 5 {Cl}   cid:21  = 30 times using the sets  cross-validation [40]. The dataset ¯X  TS C split into ﬁve disjoint subsets ¯X  TS 1  The efﬁciency of the developed models was estimated with the help of a 5-fold ∗=53,733 unique records was ≈ ··· ≈  ¯X  TS 5 , . . . , ¯X  TS 5 C . ¯X  TS k C Furthermore, in each set there are elements of all seven classes such that a subsample corresponding to each certain lth class possesses approximately an equal :  ¯X  TS 1 size inside each set ¯X  TS k  k = 1, . . . , 5, l = 0, . . . , 6 . The training and testing samples were taken in the ratio 3 : 2. The training process of basic classiﬁers was performed 3 ×  cid:20  ∪ ¯X  TS cp , where a, b, c ∈ N ∧ 1  cid:2  a < b < c  cid:2  5. Depending on these sets, the C , where d , e ∈ N \ {a, b, c} ∧ 1  cid:2  testing set is constructed as d < e  cid:2  5, and on each of these sets the values of indicators of the ith basic classiﬁer  i = 1, . . . , 5  and jth aggregating compo- GPR BC  , FPR BC  i de p i de p  j = 1, . . . , 4 . Such partitioning of sition GPR AC  , FPR AC  ¯X  TS C was performed three times  p = 1, 2, 3 , and every time the content j de p j de p the set of its ten subsets was randomly generated. The minimal, maximal, and average val- ues of indicator GPR corresponding to the ith basic classiﬁer and the jth aggre- · gating composition can be deﬁned as follows:  , GCR BC  i de p , GCR AC  j de p  , ICR BC  i de p , ICR AC  j de p   cid:14  ¯X  TS dp   cid:14  ¯X  TS ap  ∪ ¯X  TS bp  ∪ ¯X  TS ep  p=1  p=1   cid:15 3   cid:2   C  C  C  C  ,  GPR BC  i de p  1 10  1 10  · min p=1,2,3  1 cid:2 d <e cid:2 5  · min p=1,2,3   cid:2   1 cid:2 d <e cid:2 5  GPR AC  j de p  ,  ,   cid:2    cid:2   1 30  GPR BC  i de p  1 cid:2 d <e cid:2 5  cid:2   max p=1,2,3 · max 1 p=1,2,3 10 Other indicators can be calculated analogously.  1 cid:2 d <e cid:2 5  cid:2   GPR AC  j de p  1 cid:2 d <e cid:2 5  1 cid:2 d <e cid:2 5  , 1 30  · 3 cid:2  p=1 · 3 cid:2  p=1  GPR BC  i de p  ;  1 10  GPR AC  j de p  .  5.6.2 Experiment 1  Figure 5.10 shows the performance indicators calculated using the 5-fold cross- validation for ﬁve basic classiﬁers and four aggregating compositions. These indi- cators correspond to the one-against-all scheme.  As a result of applying the Fix and Hodges method, the GCR indicator was increased by 0.707% on average in comparison with multilayer neural networks, which indicates the best result among the basic classiﬁers. The GPR indicator was increased slightly  by 0.021%  compared to the neuro-fuzzy networks.   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  145  Fig. 5.10 Performance indicators for ﬁve basic classiﬁers and four aggregating compositions  The GPR − FPR and GCR − ICR indicators representing a trade-off between correct detection and false positives, and between correct classiﬁcation and incorrect classiﬁcation are depicted in Fig. 5.10. We denoted the amplitude of the change of these differences by a vertical line. Note that the greatest average value of the GPR − FPR parameter belongs to the aggregating composition of the Fix and Hodges method, and the value of this parameter is 0.142% larger than in the case of support vector machines. At the same time, the GCR − ICR indicator of the Fix and Hodges method is the largest, and exceeds the similar indicator of multilayer neural networks by 1.276%.  Within any model, an attempt to increase one indicator  true positive rate or correct classiﬁcation rate , as a general rule, negatively affects other indicators  false positive rate or incorrect classiﬁcation rate . However, as it was shown above, using aggregating compositions allows the exploitation of the beneﬁts of strong models and eliminates the shortcomings of weak models. More precisely, it can be seen from Fig. 5.10 that the recurrent neural networks having the worst performance are not expected to make any positive contribution to the classiﬁer ensemble. More detailed consideration of this effect is given in the next experiment.  5.6.3 Experiment 2  In this section we present results of experiments and evaluation of computational fault tolerance of aggregating compositions.  Figure 5.11 shows the performance indicators calculated using the 5-fold cross-  validation and the introduction of two erroneous  “bad”  classiﬁers.   146  A. Branitskiy and I. Kotenko  Fig. 5.11 Performance indicators for ﬁve basic classiﬁers and four aggregating compositions with the introduction of two erroneous classiﬁers  As earlier, these indicators correspond to the one-against-all scheme. The exper- iment described here is aimed at checking the properties of computational fault tolerance in relation to aggregating compositions.  In this experiment, the radial basis function networks and recurrent neural net- works were replaced by two other classiﬁers: random and false classiﬁers. While the output of the ﬁrst classiﬁer is a random set of class labels, among which the correct label may exist, the second classiﬁer deliberately excludes this possibility: its indicator of correct classiﬁcation is always 0. The leading position in terms of GPR and GCR still belongs to the Fix and Hodges method. The values of these indicators exceed maximal values calculated for basic classiﬁers  neuro-fuzzy net- works and multilayer neural networks  by 0.017% and 0.67%, respectively. There- fore, compared to the previous experiment, these values have fallen insigniﬁcantly  by 0.021 − 0.017 = 0.004% and 0.707 − 0.67 = 0.037% . Similarly, we have cal- culated the loss caused by the introduction of “bad” classiﬁers for GPR − FPR and GCR − ICR, which is 0.142 − 0.137 = 0.005% and 1.276 − 1.203 = 0.073%, respectively. Based on this, it can be concluded that the proposed approach for com- bining the detectors using the Fix and Hodges method is computationally fault- tolerant. In the case of stacking, we have used a multilayer neural network as an aggregating composition. As one would expect, stacking is the most sensitive aggre- gating composition when introducing new classiﬁers. The average values of GPR and GCR have been decreased by 48.542 and 45.475%. This is due to the new clas- siﬁers  random and false classiﬁers  using the weights conﬁgured for radial basis function networks and recurrent neural networks.   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  147  5.7 Conclusion  In this chapter, models of binary classiﬁers applied to network attack detection have been presented. For optimizing the training process of such classiﬁers, a parallel genetic algorithm based on operators of crossover, mutation, and permutation has been introduced. For calculating the network parameters, we have described the sliding window method, which attempts to eliminate infrequent network bursts.  For constructing the multi-class models, several low-level schemes and aggregat- ing compositions have been considered to combine binary classiﬁers. The experi- ments demonstrated the effectiveness of the proposed approach in terms of correct classiﬁcation and true positive rates of network records.  Acknowledgements This research was supported by the Russian Science Foundation under grant number 18-11-00302.  References  1. Branitskiy A, Kotenko I  2016  Analysis and classiﬁcation of methods for network attack detection. In: SPIIRAS Proceedings, vol 45 2 , pp 207–244. https:  doi.org 10.15622 sp.45. 13  in Russian   2. Branitskiy A, Kotenko I  2015  Network attack detection based on combination of neural, immune and neuro-fuzzy classiﬁers. In: Plessl C, Baz DE, Cong G, Cardoso JMP, Veiga L, Rauber T  eds  Proceedings of the 18th IEEE International Conference on Computational Science and Engineering, IEEE Computer Society, Los Alamitos, CA, USA, pp 152–159. https:  doi.org 10.1109 CSE.2015.26  3. Branitskiy A, Kotenko I  2017  Hybridization of computational intelligence methods for attack detection in computer networks. J Comput Sci 23:145–156. https:  doi.org 10.1016 j.jocs. 2016.07.010  4. Branitskiy A, Kotenko I  2017  Network anomaly detection based on an ensemble of adaptive binary classiﬁers. Computer Network Security. In: Rak J, Bay J, Kotenko I, Popyack L, Skormin V, Szczypiorski K  eds  Computer network security, pp 143–157. Springer, Cham. https:  doi. org 10.1007 978-3-319-65127-9_12  5. Abraham A, Thomas J  2006  Distributed intrusion detection systems: a computational intel- ligence approach. In: Abbass HA, Essam D  eds  Applications of information systems to homeland security and defense. Idea Group, Hershey, PA, USA, pp 107–137. https:  doi.org  10.4018 978-1-59140-640-2.ch005  6. Peddabachigari S, Abraham A, Grosan C, Thomas J  2007  Modeling intrusion detection system using hybrid intelligent systems. J Netw Comput Appl 30 1 :114–132. https:  doi.org  10.1016 j.jnca.2005.06.003  7. Mukkamala S, Sung AH, Abraham A  2003  Intrusion detection using ensemble of soft comput- ing paradigms. In: Abraham A, Franke K, Köppen M  eds  Intelligent systems design and appli- cations. Springer, Heidelberg, pp 239–248. https:  doi.org 10.1007 978-3-540-44999-7_23  8. Mukkamala S, Sung AH, Abraham A  2005  Intrusion detection using an ensemble of intelligent paradigms. J Netw Comput Appl 28 2 :167–182. https:  doi.org 10.1016 j.jnca.2004.01.003 9. Toosi AN, Kahani M  2007  A new approach to intrusion detection based on an evolutionary soft computing model using neuro-fuzzy classiﬁers. Comput Commun 30 10 :2201–2212. https:  doi.org 10.1016 j.comcom.2007.05.002  10. Amini M, Rezaeenour J, Hadavandi E  2014  Effective intrusion detection with a neural network ensemble using fuzzy clustering and stacking combination method. J Comput Sec 1 4 :293–305   148  A. Branitskiy and I. Kotenko  11. Wang G, Hao J, Ma J, Huang L  2010  A new approach to intrusion detection using artiﬁcial neural networks and fuzzy clustering. Expert Syst Appl 37 9 :6225–6232. https:  doi.org 10. 1016 j.eswa.2010.02.102  12. Chandrasekhar AM, Raghuveer K  2013  Intrusion detection technique by using k-means, fuzzy neural network and SVM classiﬁers. In: Proceedings of the 2013 International Conference on Computer Communication and Informatics. Curran Associates, Red Hook, NY, USA. https:  doi.org 10.1109 ICCCI.2013.6466310  13. Saied A, Overill RE, Radzik T  2016  Detection of known and unknown DDoS attacks using artiﬁcial neural networks. Neurocomputing 172:385–393. https:  doi.org 10.1016 j.neucom. 2015.04.101  14. Agarwal B, Mittal N  2012  Hybrid approach for detection of anomaly network trafﬁc using data mining techniques. Proc Tech 6:996–1003. https:  doi.org 10.1016 j.protcy.2012.10.121 15. He H-T, Luo X-N, Liu B-L  2005  Detecting anomalous network trafﬁc with combined fuzzy- based approaches. In: Huang D-S, Zhang X-P, Huang G-B  eds  Advances in intelligent com- puting. Springer, Heidelberg, pp. 433–442. https:  doi.org 10.1007 11538356_45  16. Kolmogorov AN  1957  On the representation of continuous functions of several variables as superpositions of continuous functions of one variable and addition. In: Tikhomirov VM  ed  Selected works of A. N. Kolmogorov, pp. 383–387. https:  doi.org 10.1007 978-94-011- 3030-1_56  17. Cybenko G  1989  Approximation by superpositions of a sigmoidal function. Math Control  Signal 2 4 :303–314. https:  doi.org 10.1007 BF02551274  18. Hornik K, Stinchcombe M, White H  1989  Multilayer feedforward networks are universal  approximators. Neural Netw 2 5 :359–366. https:  doi.org 10.1016 0893-6080 89 90020-8  19. Funahashi K-I  1989  On the approximate realization of continuous mappings by neural net-  works. Neural Netw 2 3 :183–192. https:  doi.org 10.1016 0893-6080 89 90003-8  20. Haykin SS  2011  Neural networks and learning machines, 3rd edn. Pearson, Upper Saddle  River, NJ, USA  21. Riedmiller M, Braun H  1993  A direct adaptive method for faster backpropagation learning: the RPROP algorithm. In: Proceedings of IEEE International Conference on Neural Networks, vol 1. IEEE, New York, pp 586–591. https:  doi.org 10.1109 ICNN.1993.298623  22. Fahlman SE  1988  Faster-learning variations on back-propagation: an empirical study. In: Pro- ceedings of the 1988 connectionist models summer school. Morgan Kaufmann, San Francisco, pp 38–51  23. Levenberg K  1944  A method for the solution of certain non-linear problems in least squares.  Q Appl Math 2 2 :164–168. https:  doi.org 10.1090 qam 10666  24. Marquardt DW  1963  An algorithm for least-squares estimation of nonlinear parameters. J  Soc Ind Appl Math 11 2 :431–441. https:  doi.org 10.1137 0111030  25. Jordan ML  1986  Attractor dynamics and parallelism in a connectionist sequential machine. In: Proceedings of the eighth annual conference of the cognitive science society. Lawrence Erlbaum Associates, Hillsdale, NJ, USA, pp 531–546  26. Takagi T, Sugeno M  1985  Fuzzy identiﬁcation of systems and its applications to modeling and control. IEEE T Syst Man Cyb SMC-15 1 :116–132. https:  doi.org 10.1109 TSMC.1985. 6313399  27. Jang J-SR  1993  ANFIS: adaptive-network-based fuzzy inference system. IEEE T Syst Man  Cyb 23 3 :665–685. https:  doi.org 10.1109 21.256541  28. Strang G  2016  Introduction to linear algebra, 5th edn. Cambridge Press, Wellesley, MA, USA 29. Vapnik V  1995  The nature of statistical learning theory. Springer-Verlag, New York.  https:  doi.org 10.1007 978-1-4757-2440-0  30. Hsu CW, Lin CJ  2002  A comparison of methods for multiclass support vector machines.  IEEE T Neural Networ 13 2 :415–425. https:  doi.org 10.1109 72.991427  31. Drucker H, Burges CJC, Kaufman L, Smola A, Vapnik V  1997  Support vector regression machines. Advances in neural information processing systems 9. MIT Press, Cambridge, MA, USA, pp 155–161   5 Applying Artiﬁcial Intelligence Methods to Network Attack Detection  149  32. Müller KR, Smola AJ, Rätsch G, Schölkopf B, Kohlmorgen J, Vapnik V  1997  Predicting time series with support vector machines. In: Gerstner W, Germond A, Hasler M, Nicoud J-D  eds  Artiﬁcial neural networks – ICANN’97, pp 999–1004. https:  doi.org 10.1007 BFb0020283 33. Kuhn HW, Tucker AW  1951  Nonlinear programming. In: Neyman J  ed  Proceedings of 2nd Berkeley Symposium on Mathematical Statistics and Probabilistics. University of California Press, Berkeley, CA, USA, pp 481–492  34. Platt J  1998  Sequential minimal optimization: a fast algorithm for training support vector machines  1998 . https:  www.microsoft.com en-us research publication sequential- minimal-optimization-a-fast-algorithm-for-training-support-vector-machines  35. Shawe-Taylor J, Cristianini N  2004  Kernel methods for pattern analysis. Cambridge Univer-  sity Press, New York  36. Jolliffe IT  2011  Principal component analysis. In: Lovric M  ed  International encyclopedia of statistical science. Springer, Heidelberg. https:  doi.org 10.1007 978-3-642-04898-2_455 37. Fix E, Hodges J  1951  Discriminatory analysis. Nonparametric discrimination: consistency properties. Technical Report 4, USAF School of Aviation Medicine, Randolph Field, TX, USA 38. McHugh J  2000  Testing intrusion detection systems: a critique of the 1998 and 1999 DARPA intrusion detection system evaluations as performed by Lincoln laboratory. ACM T Inform Syst Se 3 4 :262–294. https:  doi.org 10.1145 382912.382923  39. Mahoney MV, Chan PK  2003  An analysis of the 1999 DARPA Lincoln Laboratory evaluation data for network anomaly detection. In: Vigna G, Kruegel C, Jonsson E  eds  Recent advances in intrusion detection. Springer, Heidelberg, pp 220–237. https:  doi.org 10.1007 978-3-540- 45248-5_13  40. Refaeilzadeh P, Tang L, Liu H  2009  Cross-validation. In: Liu L, Özsu MT  eds  Encyclopedia of database systems. Springer, Boston, MA, USA. https:  doi.org 10.1007 978-0-387-39940- 9_565   Chapter 6 Machine Learning Algorithms for Network Intrusion Detection  Jie Li, Yanpeng Qu, Fei Chao, Hubert P. H. Shum, Edmond S. L. Ho, and Longzhi Yang  Abstract Network intrusion is a growing threat with potentially severe impacts, which can be damaging in multiple ways to network infrastructures and digi- tal intellectual assets in the cyberspace. The approach most commonly employed to combat network intrusion is the development of attack detection systems via machine learning and data mining techniques. These systems can identify and dis- connect malicious network trafﬁc, thereby helping to protect networks. This chapter systematically reviews two groups of common intrusion detection systems using fuzzy logic and artiﬁcial neural networks, and evaluates them by utilizing the widely used KDD 99 benchmark dataset. Based on the ﬁndings, the key challenges and opportunities in addressing cyberattacks using artiﬁcial intelligence techniques are summarized and future work suggested.  6.1 Introduction  Cybersecurity can be assisted by a set of techniques that protect cyberspace and ensure the integrity, conﬁdentiality, and availability of networks, applications, and data. Cybersecurity techniques also have the potential to defend against and recover from any type of attack. More devices, namely, Internet of Things  IoT  devices, are connected to the cyberspace, and cybersecurity has become an elevated concern affecting governments, businesses, other organizations, and individuals. The scope of cybersecurity is broad, and can be grouped into ﬁve areas: critical infrastructure, network security, cloud security, application security, and IoT security. Network J. Li · H. P. H. Shum · E. S. L. Ho · L. Yang  B   Northumbria University, Newcastle upon Tyne, UK e-mail: longzhi.yang@northumbria.ac.uk  Y. Qu Dalian Maritime University, Dalian, People’s Republic of China e-mail: yanpengqu@dlmu.edu.cn  F. Chao Xiamen University, Xiamen, People’s Republic of China e-mail: fchao@xmu.edu.cn    Springer Nature Switzerland AG 2019 L. F. Sikos  ed. , AI in Cybersecurity, Intelligent Systems Reference Library 151, https:  doi.org 10.1007 978-3-319-98842-9_6  151   152  J. Li et al.  security is an important challenge in the ﬁeld of cybersecurity, because networks provide the means for the crucial access to others devices, and for connectivity between all the assets in cyberspace. Severe network attacks can lead to system damage, network paralysis, and data loss or leakage. Network intrusion detection systems  NIDS  attempt to identify unauthorized, illicit, and anomalous behavior based solely on network trafﬁc to support decision-making in network preventative actions by network administrators.  Traditional network intrusion detection systems are mainly developed using avail- able knowledge bases, which are comprised of the speciﬁc patterns or strings that correspond to already known network behaviors, i.e., normal trafﬁc and abnormal trafﬁc [1]. These patterns are used to check monitored network trafﬁc to recognize possible threats. Typically, the knowledge bases of such systems are deﬁned based on expert knowledge, and the patterns must be updated to ensure the coverage of new threats [2]. Therefore, the detection performance of traditional network intrusion detection systems depends highly on the quality of the knowledge base. From the theoretical point of view, network intrusion detection systems mainly aim to clas- sify the monitored trafﬁc as either “legitimate” or “malicious.” Therefore, machine learning approaches are appropriate to solve such problems; and they have recently been widely applied to help better manage network intrusion detection issues.  Machine learning  ML  is a ﬁeld of artiﬁcial intelligence, which refers to a set of techniques that give computer systems the ability to “learn.” Typically, machine learning algorithms, such as artiﬁcial neural networks, learn from data samples to cat- egorize or ﬁnd patterns in the data, and enable computer systems to make predictions on new or unseen data instances based on the discovered patterns [3]. Depending on the way of learning, machine learning can be further grouped into two main categories: supervised learning and unsupervised learning. Supervised learning dis- covers the patterns to map an input to an output based on the labeled input-output pairs of data samples [4]. The classiﬁcation problem is a typical supervised learning problem, which has been commonly used for solving NIDS problems, such as those reported in [5–8]. The goal of unsupervised learning is to ﬁnd a mapping that is able to describe a hidden structure from unlabeled data samples. It is a powerful tool for identifying structures when unlabeled data samples are given [4]. Thanks to the relax- ation of the requirement for labels of training data in unsupervised learning, various unsupervised learning approaches have also been widely applied for NIDS problems, such as the clustering-based NIDS [9] and self-organizing map-based NIDS [10].  This chapter focuses primarily on network intrusion detection systems, and partic- ularly how the machine learning and data mining techniques can help in developing network intrusion detection systems. The chapter ﬁrst systematically reviews intru- sion detection techniques from the perspective of both hardware deployment and software implementation. The two most commonly used NIDS development meth- ods and the three most commonly used detection methodologies are reviewed; these are followed by the investigation of applying machine learning and data mining techniques in the implementation of intrusion detection systems. Two representa- tive machine learning approaches, including fuzzy inference systems and artiﬁcial neural networks, are of particular interest in this chapter, because they are the machine   6 Machine Learning Algorithms for Network …  153  learning and data mining techniques most suitable for supporting intrusion detection systems. Traditionally, fuzzy inference systems are not classiﬁed as machine learning algorithms, however, the rule base generation mechanism follows the data mining principle; therefore, fuzzy inference systems with automatic rule base generation can also be considered machine learning. Finally, the intrusion detection systems developed upon these machine learning approaches are evaluated using the widely used KDD 99 benchmark dataset.  The remainder of this chapter is organized as follows. Section 6.2 introduces the hardware deployment methods of network intrusion detection systems and detection methodologies. Section 6.3 reviews the existing machine learning-based network intrusion detection systems using fuzzy inference systems and artiﬁcial neural net- works. The limitations and potential solutions of both techniques are also discussed in this section. Section 6.4 evaluates the studied systems using the well-known bench- mark dataset KDD 99. Section 6.5 concludes the chapter and sets directions for future work.  6.2 Network Intrusion Detection Systems  Network intrusion detection systems are software-based or hardware-based tools that are used to monitor network trafﬁc, i.e., to analyze them for signs of possible attacks or suspicious activities. Usually one or more network trafﬁc sensors are used to monitor network activity on one or more network segments. The system constantly performs analysis and watches for certain patterns of passing trafﬁc in a monitored network environment. If the detected trafﬁc patterns match the deﬁned signatures or policies in the knowledge base  e.g., based on a fuzzy rule base or a trained neural network , a security alert is generated.  6.2.1 Deployment Methods  There are multiple methods that can be adopted to deploy a NIDS in order to capture and monitor trafﬁc in a network environment, with passive deployment and in-line deployment being the most commonly used, as shown in Fig. 6.1a and b.  In the passive deployment method, the NIDS device is connected to a network switch, which is deployed between the main ﬁrewall and the internal network. The switch is usually conﬁgured with a port mirroring technology, such as the Mirror Port supported by HP and the Switched Port Analyzer  SPAN  supported by Cisco. These port mirroring technologies are able to copy all network trafﬁc, including incoming and outgoing trafﬁc, to a particular interface of the NIDS for the purpose of trafﬁc monitoring and analysis. This method usually requires a high-end network switch in order to enable the port mirroring technologies. There is a special case of passive deployment, which is the passive network TAP  Terminal Access Point  [11].   154  J. Li et al.  Fig. 6.1 Deployment methods for intrusion detection systems  In particular, a network TAP uses pairs of cables included in the original Ethernet cable, as illustrated in Fig. 6.1c, to send a copy of the original network trafﬁc to the NIDS.  The in-line deployment method deploys NIDS devices the same way as ﬁrewalls, which allows all trafﬁc to pass directly through the NIDS. Therefore, this deployment   6 Machine Learning Algorithms for Network …  155  method does not require any particularly high-end network device, which is an ideal solution for those environments in which port mirroring technologies are unavailable, such as a small branch ofﬁce with low-end networking equipment.  It is important to note that the deployment methods should be carefully selected while taking into account the network topology for optimal performance. For instance, in the example shown in Fig. 6.1a, the port mirroring method is not only able to monitor the outgoing trafﬁc between the internal network and the Internet, but also the internal trafﬁc between hosts A, B, and C. However, the network TAP and in-line deployment method are only able to monitor the outgoing trafﬁc that is generated between the internal network and the Internet. Therefore, the NIDS, which is deployed by either the network TAP or the in-line method, will not notice if there is suspicious trafﬁc between two client machines. In addition, because the port mirroring method uses a signal network interface to monitor the entire switch trafﬁc, trafﬁc congestion may occur if the switch backbone trafﬁc is beyond the capacity of the bandwidth of the monitored port. Therefore, it is a good strategy to deploy multiple NIDSes in complex network environments so that these blind spots can be eliminated.  6.2.2 Detection Methodologies  Generally speaking, intrusion detection methodologies can be grouped into three major categories: signature-based detection, anomaly-based detection, and speciﬁcation-based detection [12].  The signature-based NIDS, also called knowledge-based detection or misuse detection, refers to the detection of attacks or threats by looking for speciﬁc pat- terns or strings that correspond to already known attacks or threats. These speciﬁc patterns or strings are saved in a knowledge base, such as the byte sequences of the network trafﬁc, known malicious instruction sequences exploited by malware, the speciﬁc ports a host tries to access, etc. Signature-based detection is a process that compares known patterns against monitored network trafﬁc to recognize possible intrusions. Therefore, signature-based detection is able to effectively detect known threats in a network environment, and its knowledge bases are usually generated by experts. A good example for this type of detection is a large amount of failed login attempts that have been detected in a Telnet session.  Anomaly-based detection primarily focuses on normal trafﬁc behaviors rather than speciﬁc attack behaviors, which overcomes the limitation of signature-based detection that is only able to detect known attacks. This method is usually comprised of two processes: a training process and a detection process. In the train- ing phase, machine learning algorithms are usually adopted to develop a model of trustworthy activity based on the behavior of the network trafﬁc without attacks. In the detection phase, the developed trustworthy activity model is compared to the currently monitored trafﬁc behavior, and any deviations indicate a potential threat. The anomaly-based detection method is usually adopted to detect unknown attacks   156  J. Li et al.  [13–18]. However, the effectiveness of anomaly-based detection is greatly affected by the selected features the machine learning algorithms use. Unfortunately, the selection of the appropriate set of features has proven to be a big challenge. Also, the observed systems’ behavior constantly changes, which causes anomaly-based detection to produce a weak proﬁle accuracy.  Speciﬁcation-based detection is similar to the anomaly-based detection method as it also detects attacks as deviations from normal behavior. However, speciﬁcation- based approaches are based on manually developed speciﬁcations that characterize legitimate behaviors rather than relying on machine learning algorithms. Although this method is not characterized by the high rate of false alarms typical to anomaly- based detection methods, the development of detailed speciﬁcations can be time- consuming. Because they detect attacks as deviations from legitimate behaviors, speciﬁcation-based approaches are commonly used for unknown attack detection [19, 20]. In addition, multiple detection methodologies can be adopted jointly to provide a more extensive and accurate detection [21].  6.3 Machine Learning in Network Intrusion Detection  Machine learning and data mining techniques work by establishing an explicit or implicit model that enables the analyzed patterns to be categorized. In general, machine learning techniques are able to deal with three common problems: clas- siﬁcation, regression, and clustering. Network intrusion detection can be considered as a typical classiﬁcation problem. Therefore, a labeled training dataset is usually required for system modeling. A number of machine learning approaches have been used to solve network intrusion detection problems, and all of them consist of three general phases  as illustrated in Fig. 6.2 :   Preprocessing: the data instances that are collected from the network environment are structured, which can then be directly fed into the machine learning algorithm. The processes of feature extraction and feature selection are also applied in this phase.   Training: a machine learning algorithm is adopted to characterize the patterns of various types of data, and build a corresponding system model.   Detection: once the system model is built, the monitored trafﬁc data will be used as system input to be compared to the generated system model. If the pattern of the observation is matched with an existing threat, an alarm will be triggered.  Both supervised and unsupervised machine learning approaches have already been utilized to solve network intrusion detection problems. For instance, supervised learning-based classiﬁers have been successfully employed to detect unauthorized access, such as k-nearest neighbor  k-NN  [6], support vector machine  SVM  [22], decision tree [23], naïve Bayes network [7], random forests [5], and artiﬁcial neu- ral networks  ANN  [24]. In addition, unsupervised learning algorithms, including k-means clustering [25] and self-organized maps  SOM  [10], have also been applied   6 Machine Learning Algorithms for Network …  157  Fig. 6.2 ML-NIDS architecture  to deal with network intrusion detection problems, with good results. For vari- ous reasons, such as the imbalance of training datasets and the high cost of com- putational requirement, it is currently very difﬁcult to design a single machine learning approach that outperforms the existing ones. Therefore, hybrid machine learning approaches, such as clustering with classiﬁer [16, 26] and hierarchical clas- siﬁers [27], have attracted a lot of attention in recent years. In addition, some data mining approaches have also been successfully utilized to solve intrusion detection problems. For instance, data mining approaches are employed to generate a fuzzy rule base, and a fuzzy inference approach is then applied for threat detection [14]. This section examines the existing NIDSes utilizing two approaches, namely, fuzzy inference systems and artiﬁcial neural networks.  6.3.1 Fuzzy Inference Systems  Due to their great ability to deal with uncertainty, fuzzy inference systems  FIS  have been widely used for detecting potential network threats. Generally speaking, fuzzy inference systems are built upon fuzzy logic to map system inputs and outputs. A typical fuzzy inference system consists of two main parts: a rule base  or knowledge base  and an inference engine. A number of inference engines are well established, with the Mamdani inference [28] and the TSK inference [29] being the most widely used. Although fuzzy sets are used in both rule antecedents and rule consequences by the Mamdani fuzzy model, which is more intuitive and suitable for handling linguistic variables, a defuzziﬁcation progress is required to transfer the fuzzy outputs to crisp   158  J. Li et al.  outputs. In contrast, the TSK inference approach produces crisp outputs directly, as crisp polynomials are used as rule consequences.  For a fuzzy inference-based NIDS  FIS-NIDS , the important features, which are extracted from the network packets, are used in the pre-detector component to analyze events with the set of rules to determine whether any incoming events have intrusive patterns or not. The set of rules is called a fuzzy rule base, which can be either predeﬁned by expert knowledge  knowledge-driven , or extracted from labeled data instances  data-driven  [30, 31]. In contrast to knowledge-driven rule base generation approaches, which essentially limit the system’s applicability as expert knowledge is not always available in some areas, data-driven rule base generation methods are most commonly used for intelligent NIDSes. Several data-driven approaches have been proposed to generate a rule base for FIS-NIDS use, which are usually derived from complete and dense datasets [32, 33]. The generated rule bases are often optimized using a general optimization technique, such as genetic algorithms  GA , for optimal system performance. As the used datasets are dense and complete, the resulted rule bases are generally dense and complete, each of which covers the entire input domain, and accordingly the resulted fuzzy models often yield to great reasoning performance. However, these systems will suffer if only incomplete, imbalanced, and sparse datasets are available. In addition, these systems are usually signature-based NIDSes, which are only able to detect known network threats for which the intrusive patterns have been covered in the rule base.  In order to address the previous limitations, fuzzy interpolation has been used to develop NIDSes [18, 34]. Brieﬂy, fuzzy interpolation enhances conventional fuzzy inference systems to work with sparse fuzzy rule bases, by which some inputs or observations are not covered [35]. Using fuzzy  interpolation techniques, even if the trafﬁc patterns of the incoming event do not match with any of the patterns stored in the rule base, an approximated result can still be obtained by considering the similar patters expressed as rules in the current rule base. A number of fuzzy interpolation approaches have been proposed in literature [36–47], many of which have already been applied to solve real-world problems [48–51].  A data-driven fuzzy interpolation-based NIDS can be developed in four steps: 1  training dataset generation and preprocessing, 2  rule base initialization, 3  rule base optimization, and 4  intrusion detection by fuzzy interpolation [14, 52], as illustrated in Fig. 6.3. These key steps are detailed in the following sections.  6.3.1.1 Dataset Generation and Preprocessing  The training dataset can either be collected from a real-world network environment, or it can be developed from an existing dataset. Whichever method is selected, the important features, which are selected for system modeling, have to be identiﬁed. In general, a number of features can be monitored by networking tools for network analysis during data packet transmission over the network, but some of these features are redundant or noisy. Therefore, a well-thought manual feature selection process   6 Machine Learning Algorithms for Network …  159  Dataset generation and  preprocessing  Processed training   dataset  Rule base initialization  Optimization  Initialized rule base  Optimized rule base  Input  network traffic  Fuzzy interpolation  reasoning  Decision  Fig. 6.3 The framework of TSK− based NIDS  is often required for network attack detection [53]. This common practice is also applied here. In particular, four important features identiﬁed by experts are selected as NIDS signature for the proposed FIS-NIDS, which are listed in Table 6.1.  The establishment of the optimal number of features that should be retained in datasets by feature selection methods is always an argued point, because feature selection usually causes information loss from the original dataset. Several pieces of work in the area of feature selection have claimed that more attributes generally lead to better approximations [54–57]. This can be the case for perfect, entirely consis- tent, and noise-free data, with all features being independent. Generally speaking, feature relevancy and redundancy have to be considered by feature selection meth- ods before the application of machine learning approaches [58, 59]. The selected features should be highly relevant to the problem and non-redundant to be useful and efﬁcient [60]. In fact, a large volume of published results in relevant literature has  Table 6.1 Features used in the NIDS Feature Source bytes Destination bytes Count  Description The number of data bytes sent by the source IP host The number of data bytes sent by the destination IP host The number of connections to the same host as the current connection in the past 2 seconds  Dst_Host_Diff_Rate % of connections whose ports are different, among the past 100  connections with the same destination IP   160  J. Li et al.  demonstrated that smaller number of selected features can lead to much-improved modeling accuracy [61–66]. In addition, more attributes retained in datasets will also increase the computational complexity [60]. Therefore, it is necessary to consider as many features as possible under certain circumstances especially for noise-free and fully consistent datasets, but in others, a minimal subset of features satisfying some predeﬁned criteria is more appealing.  Once the features are determined for machine learning, datasets for a given net- work of a particular environment need to be collected for model training. This is typically implemented in stages based ﬁrst on an attack-free network, and then different types of attacks that need to be identiﬁed. In other words, data regard- ing normal network trafﬁc is collected ﬁrst from a threat-free network environment. Then, a number of attacks simulating the ﬁrst type of attack are artiﬁcially launched so that this type of attack is sufﬁciently covered by the dataset. This process is repeated for every other type of attacks until all the classes that need to be considered are fully covered by the dataset. The ﬁnal dataset covers all attack types and attack-free situations. In most cases, if an existing dataset is adopted for model training, the process of data collection may be skipped. However, ideally, the structure of the existing dataset should follow the structure explained above.  6.3.1.2 Rule Base Initialization Suppose that the training dataset  T   contains l  l  cid:2  1, l ∈ N  labeled classes, which covers l − 1 types of attacks and the normal situation. As illustrated in Fig. 6.4, the system ﬁrst divides the training dataset T into l sub-datasets T1, T2, . . . , Tl, each representing a type of attack or the normal trafﬁc  i.e., T = ∪l  Then, the K-means, one of the most widely used clustering algorithms, is employed to each sub-dataset to group the data points into k clusters based on their feature values. Note that the value of k in the K-means algorithm has to be predeﬁned to enable the application of the algorithm. The Elbow method [67], which determines the number of clusters based on the criteria that adding another cluster is not much better for modeling the dataset, has been employed for determining the value of k. Based on this, each determined cluster is expressed as a fuzzy rule that contributes to the TSK rule base.  s=1Ts .  In this work, a 0-order TSK fuzzy model is adopted. All data instances in each class share the class label  an integer number , which is utilized as the consequent of the corresponding TSK rule. The triangular membership function is utilized in the rule antecedents. The support of the triangular fuzzy set is expressed as the span of the cluster along this input dimension, and the core of the corresponding fuzzy set is set as the cluster center. The ﬁnal TSK fuzzy rule base is generated by combining all the extracted rules from all l sub-datasets, which is illustrated as follows:  Rs ts  : IF x1 is Asts THEN z = s,  1 and x2 is Asts  2 and x3 is Asts  3 and and x4 is Asts 4  ,   6.1    6 Machine Learning Algorithms for Network …  161  Fig. 6.4 Rule base generation  where s = {1, . . . , l} represents the sth sub dataset that indicates the sth type of network trafﬁc, ts = {1, . . . , ks} denotes the tth cluster in the sth sub-dataset. The number of rules in this rule base is equal to the sum of the numbers of clusters for all the sub-datasets  i.e., k1 + k2 + ··· + kl .  6.3.1.3 Rule Base Optimization  The generated initial rule base can be employed for intrusion detection, but with rel- atively poor performance. In order to increase the detection performance, a genetic algorithm  GA  is adopted here to ﬁne-tune the membership functions involved in the initial rule base. Assume that a given initial TSK rule base is comprised of n fuzzy rules of the form shown in Eq. 6.1. Suppose a chromosome, denoted as I, is used to represent a potential solution in the GA, which is coded to represent the parameters of all rules in the rule base, as shown in Fig. 6.5. Based on this, the initial population P = {I1, I2, . . . , IP} can be formed by taking the parameters of the initial rule base and its random variations. During the optimization process, the number of chro- mosomes is selected for offspring reproduction by applying the genetic operators of crossover and mutation. Speciﬁcally, the ﬁtness proportionate selection method, also known as the roulette wheel selection, is implemented in this work for chromosome   162  J. Li et al.  Fig. 6.5 Chromosome encoding  selection, and the signal point crossover and mutation operators are employed for reproduction. In addition, in order to make sure that the resultant fuzzy sets are valid , i = {1, 2, 3, 4} is enforced to the genes and convex, the constraint a1 ir during optimization. The selection and reproduction processes are iterated until the predeﬁned maximum number of iterations is reached, or until the system perfor- mance reaches a predeﬁned threshold. Optimized parameters and thus an optimized rule base can be obtained when the termination condition is satisﬁed.  < a3 ir  < a2 ir  6.3.1.4  Intrusion Detection by TSK-Interpolation:  Once the rule base is generated, the TSK+ fuzzy inference approach can be deployed to perform inferences for attack detection. In order to generate network intrusion alerts in real time, the system is deployed by one of the deployment methods intro- duced in Sect. 6.2.1, which keeps capturing network trafﬁc data for analysis. For each captured network packet, four important features, detailed in Table 6.1, are extracted and fed into the proposed system. From this input, the TSK+ fuzzy infer- ence approach will classify the types of network trafﬁc using the generated rule base. Assume that an optimized TSK fuzzy rule base is comprised of n rules as follows:  R1 : IF x1 is A1 Rn : IF x1 is An  . . .  1 and x2 is A1  2 and x3 is A1  3 and x4 is A1  1 and x2 is An  2 and x3 is An  3 and x4 is An  4 THEN z = Z1, 4 THEN z = Zn,   6.2    k ∈ {1, 2, 3, 4} and i ∈ {1, . . . , n}  represents a normal and convex trian- where Ai k gular fuzzy set in the rule antecedent denoted accordingly as  ai  , and Zi k1 is an integer number that indicates the type of network trafﬁc, whether it is normal trafﬁc or a particular type of attack. By taking a captured network packet as an exam- ple, the working procedure of the TSK+ fuzzy inference for intrusion detection can be summarized as the following steps:  , ai k2  , ai k3   6 Machine Learning Algorithms for Network …  163  ∗ 1  ∗ k  ∗ , x 4  ∗ , x 3  ∗ , x 2  ∗ , x k  =  x  1. Extract the four feature values from the network packet, and express them in the }, which will be used as the system input. Note that the form I = {x extracted feature values are normally crisp values. They have to be represented  , where k = {1, 2, 3, 4}, for future use. ∗ ∗ as fuzzy sets of the form A , x k k   between the inputs I = {A ∗ ∗ ∗ , Ai 2. Determine the matching degree S A , A 4  for each rule Ri, i = {1, . . . , n} using: 1 3 k k , Ai , Ai 3 ⎛ 3 cid:5  j=1  } and rule antecedents  Ai  − ai  ∗ , A 2  , Ai 2  ∗ A 4  x  ⎞  ∗ k    kj  ,  1  ∗ S A k  , Ai k    =  ⎜⎜⎜⎜⎜⎝1 −  ⎟⎟⎟⎟⎟⎠  · DF ,  3   6.3   where DF, termed as distance factor, is a function of the distance between the two fuzzy sets of interest, which is deﬁned as follows:  DF = 1 −  1  1 + e−sd+5  ,   6.4    6.5    6.6   where s  s > 0  is a sensitivity factor, and d represents the Euclidean distance between the two fuzzy sets. A smaller s value results in a similarity degree more sensitive to the distance of the two fuzzy sets.  3. Obtain the ﬁring degree of each rule by integrating the matching degrees of its  antecedents and the given input values as follows:   ∧ S A ∗ 3  , Ai 4 where ∧ is a t-norm usually implemented as a minimum operator.  αi = S A ∗ 1    ∧ S A ∗ 2    ∧ S A ∗ 4  , Ai 1  , Ai 2  , Ai 3    ,  4. Integrate the sub-consequences from all rules to get the ﬁnal output using the  following formula:   cid:9   z =  αi · Zi  cid:9  n i=1 n i=1 αi  .  5. Apply the round function on the ﬁnal output to obtain the integer number that  indicates the network trafﬁc type for the given network packet.  As discussed above, if an unknown network’s threat behavior or trafﬁc pattern has been captured, a result of “network security alert” can still be expected by considering all fuzzy rules in the rule base.   164  J. Li et al.  6.3.2 Artiﬁcial Neural Networks  An artiﬁcial neural network  ANN  is an information processing system inspired by biological nervous systems that constitute animal brains, which is one of the most widely used machine learning algorithms [68]. Typically, an ANN is composed of two main parts: a set of simple processing units, also known as nodes or artiﬁcial neurons, and the connections between these. These simple units or nodes are orga- nized in layers, which usually consist of the input, output, and hidden layers. The hidden layers are those between the input and the output layers. Once the set of pro- cessing units and their connections are determined, or an ANN is built, the training process adjusts the connection weights between the connected units to determine to what extent one unit will affect the others. ANNs are successfully employed in NIDSes, which usually fall into two categories: supervised training-based NIDSes and unsupervised training-based NIDSes [69]. As demonstrated in Fig. 6.2, both types of NIDSes essentially follow the architecture and three general steps of ML- NIDS as speciﬁed in the beginning of this section.  If the supervised learning approach is applied, the desired output or pattern for a given input is learned from a set of labeled data. A well-known supervised neural network architecture is the multilayer perception  MLP , which is based on the feed- forward and backpropagation algorithms with one or more layers between the input and the output layer [1]. In this type of ANN-NIDS, the number of nodes in the input layer is set to the number of features selected from the original trafﬁc ﬂow, and the number of nodes in the output layer is conﬁgured to be the number of desired output classes [16, 70–73]. The number of hidden layers and the number of nodes for each hidden layer vary, and are usually conﬁgured according to the situation. A feed-forward-based MLP with a signal hidden layer ANN NIDS model is illustrated in Fig. 6.6.  Obviously, the entire data ﬂow in the ANN is in one direction only: from the input layer, though the hidden layer, to the output layer  see Fig. 6.6 . Therefore, given a network trafﬁc package as the input, the corresponding network behavior can be predicted. The advantages of this model are its ability to represent both linear and non-linear relationships, and directly learn these relationships from the data by means of training. However, a number of research projects have reported that the training process of this type of ANN can be very time-consuming, which may pose a signiﬁcant adverse impact for NIDS system updating [1, 24].  Another group of ANN NIDSes is based on unsupervised training, in which the network adapts to different clusters without having a desired output. One of the most popular algorithms in this group is the self-organizing map  SOM , which transforms the input of arbitrary dimension into a low-dimensional  usually 1- or 2-dimensional  discrete map by using Kohonen’s unsupervised learning method [74]. The structure of a conventional self-organizing map is shown in Fig. 6.7a. A conventional SOM network model usually has two layers: an input layer and an output layer  also known as a competitive layer . Similar to the supervised training-based NIDS, the number of nodes in the input layer are usually set to the number of selected features of the   6 Machine Learning Algorithms for Network …  165  Fig. 6.6 Multilayer perception-based NIDS architecture  training dataset. The output layer consists of neurons organized in a lattice, usually a ﬁnite two-dimensional space. Each neuron has a speciﬁc topological position and is associated with a weight vector of the same dimension as the input vectors [75]. The training process adjusts the weight vectors of the neurons, thereby describing a mapping from a higher-dimensional input space to a lower-dimensional map space. As a result, the SOM eventually settles into a map of stable zones as a type of feature map of the input space. Based on these mappings, various trafﬁc behaviors can be identiﬁed. Figure 6.7b illustrates an example of a SOM output, which clearly shows the four classes that have eventually been predicted.  When comparing the performance  speed and conversion rate  between SOM and supervised learning-based NIDS systems, it becomes clear that SOM is more suitable for real-time intrusion detection [76–80].  Although both types of ANN-network intrusion detection systems are successfully employed in detecting intrusions in real-world network environments with promis- ing results, existing ANN-network intrusion detection systems have two main draw- backs: 1  lower detection precision for low-frequency attacks, and 2  weaker detec- tion stability, which limits the applicability of such systems [16]. The reason behind these is the uneven distribution of different attack types. For example, the number of training data instances for low-frequency attacks are very limited compared to com- mon attacks. As a consequence, it is not easy for the ANN to learn the characteristics of such low-frequency attacks [81].  To address these issues, a number of solutions have been proposed  e.g., [16, 82, 83] . Among these systems, a fuzzy clustering-based neural network NIDS approach  FC-ANN-NIDS  [16] can be a potential solution. Comparing to conventional ANN- NIDSes, in which data clustering techniques are typically not involved during the training process, FC-ANN-NIDSes adopt a fuzzy clustering technique to generate   166  J. Li et al.  Fig. 6.7 Self-organizing map-based NIDS architecture  different training sub-datasets. This is followed by the application of multiple ANNs in the training stage based on the divided sub-datasets. Finally, a fuzzy aggregation module is applied to combine the results of the ANNs, in an effort to eliminate their errors. The framework of FC-ANN-NIDS is illustrated in Fig. 6.8, which basically   6 Machine Learning Algorithms for Network …  167  Fig. 6.8 FC-ANN-NIDS framework  contains three major stages: clustering, ANN modeling, and fuzzy aggregation. The details of this method  or FC-ANN-NIDS  are presented in the rest of this section.  6.3.2.1 Clustering  Given a training dataset that contains l network behaviors, the fuzzy C-means clus- tering technique [84] is employed to group the data instances in clusters, which essentially divides the entire training dataset into n sub-datasets. Note that only the size and complexity of the original training dataset is reduced after data clustering, and the data instances in each divided sub-dataset may still cover all the l network behaviors. Each divided training dataset will be forwarded to the next stage for ANN training. Unfortunately, the value of n  the number of clusters  in the proposed system is determined under a practice theory. Therefore, more intelligent methods, such as the Elbow method [67] may be considered for determining the value of n.  6.3.2.2 ANN Training  A multi-layer perceptron model, illustrated in Fig. 6.6, is used in this study for mod- eling each sub-training dataset. As mentioned previously, the number of input nodes is set to match the number of selected features of the training dataset; and the number of nodes in the output layer is set to the number of network trafﬁc behaviors covered √ by the training dataset. The number of hidden nodes is then obtained by adopting the I + O + α,  α = {1, . . . , 10} , where I denotes the number of empirical formula:   168  J. Li et al.  input nodes, O represents the number of nodes in the output layer, and α is a random number [81]. During the training process, the signals, which combine both the input values and the weight values between the corresponding input node and the hidden node, are received by each node in the hidden layer. These signals are processed by a sigmoid activation function, and broadcasted to all the neurons in the output layer with a special weight value. In this study, the most widely used ﬁrst-order optimization algorithm, gradient descent, is employed for weight-updating during the backpropagation process. Once the entire training process is completed, multiple ANN models can be generated based on the different training sub-datasets. Note that each ANN model can be applied individually for network intrusion detection in real- world network environments. In order to reduce the detection errors, an aggregation module is applied to aggregate the results from different ANNs.  6.3.2.3 Aggregation  Although each ANN generated in the last stage can be deployed individually as an NIDS, some of them may have an unacceptably poor detection performance. In this study, another multi-layer perceptron model is applied for sub-result aggregation. In this stage, the number of nodes in both the input and the output layer is set to the number of network behaviors. Given the entire training dataset and the multiple trained ANN models with the corresponding training sub-datasets generated in the last stage, the modeling process in the aggregation stage can be summarized as follows:  Step 1:  Feed each data instance j in the original training dataset to every trained ANN model  ANN1, ANN2, . . . , ANNn . Denote the output of model ANNi,  i = {1, . . . , n}  from data instance j as o j i , then the outputs from all ANNs collectively as O j and O j = [o j  n]. , . . . , o j  Step 2:  Form the new input for the new ANN model based on the previous outputs.  1  The new input I j  new generated from data instance j is  New = [o j I j  1  · μ1, . . . , o j  n  · μn] ,   6.7   where μi represents the degree of membership of data instance j belonging to cluster i. Note that the degree of membership for each data instance regarding each cluster has been determined in the clustering using the fuzzy C-means clustering algorithm.  Step 3: Generate a new ANN model and train it using the newly formed inputs  generated in Step 2.  Once the entire model is built, the system can be deployed in real-world network environments for intrusion detection. Given an incoming network trafﬁc package, the system ﬁrst calculates the membership of the incoming data using the cluster centers obtained in the ﬁrst stage. Next, the ANN models and the aggregation model will   6 Machine Learning Algorithms for Network …  169  be applied to predict the ﬁnal result, which indicates whether the incoming trafﬁc poses a threat. Such hybrid ANN network intrusion detection solutions can increase detection performance, especially for low-frequency attacks. However, they may be costly in time because of the training processes for the large number of feed-forward neural networks.  6.3.3 Deployment of ML-Based NIDSes  Although the developed ML-based network intrusion detection systems are able to take the network package  input  to predict whether it is a normal network behavior, these systems still cannot be directly implemented in real-world network environ- ments for real-time detection. The reason behind this is that the generated ML-based models do not have packet sniffers, which are used to capture the network trafﬁc in real time. In order to achieve real-time detection, the developed ML-based network intrusion detection systems have to work with packet sniffers, such as Snort, Bro, or Spark. A packet sniffer  or network sniffer  is a network trafﬁc monitoring and analyzing tool that can sniff out the network data traversing the monitored network in real time. A number of ML-based network intrusion detection systems have been successfully integrated with packet sniffers and achieved good real-time detection  e.g., [34, 85] . The general framework of these systems is illustrated in Fig. 6.9. A packet sniffer, which can be implemented by either a passive or an in-line deploy- ment method as introduced in Sect. 6.2.1, continuously captures the network trafﬁc, and extracts the required information from the captured network packets to feed into the system model developed by machine learning techniques, thereby generating the ﬁnal decisions.  Fig. 6.9 The framework of ML-NIDS deployment   170  6.4 Experiment  J. Li et al.  A number of network intrusion detection systems developed by different machine learning approaches are evaluated in this section by applying them to the KDD 99 benchmark dataset.  6.4.1 Evaluation Environment  A well-known benchmark dataset, KDD 99, which has been utilized a number of times in recent research [14, 16, 18, 86], is used in this work to evaluate multiple machine learning-based network intrusion detection systems. The KDD 99 dataset is a popular benchmark for intrusion detection; it includes legitimate connections and a wide variety of intrusions simulated in a military network environment [87]. This dataset contains almost 5 million data instances with 42 attributes, including the “class” attribute, which indicates whether a given instance is a normal connection instance or one of the four types of attacks to be identiﬁed  i.e., normal, denial of service attacks, user-to-root attacks, remote-to-user attacks, and probes . An impor- tant feature of this dataset is that it is an imbalanced dataset, with most data instances belonging to the normal, denial of service attack, and probe categories. As is the case of low-frequency attacks, the classes of user-to-root attacks and remote-to-user attacks are only covered by a small number of data samples. Knowing the inherent issues associated with the dataset, such as the high duplication rate of 78% [87], data instance selection methods, such as the random selection method, are used to reduce the size of the dataset for machine learning. It is worth mentioning that the KDD 99 dataset has been succeeded by the NSL-KDD-99 dataset [87], which reduces the size to 125,937 data samples, while keeping all the features of the original dataset. Table 6.2 details the information about the number of data instances in the training and testing datasets that were used by different network intrusion detection systems, as discussed in Sect. 6.3.  6.4.2 Model Construction  This section details the model construction of the aforementioned six ML-based network intrusion detection approaches.  6.4.2.1 TSK+ Fuzzy Inference  As discussed in Sect. 6.3.1, this system brings four important features to the system model. During rule base initialization, the training dataset was divided into ﬁve sub-   6 Machine Learning Algorithms for Network …  171  datasets based on the ﬁve symbolic labels, which are represented by ﬁve integer numbers. The fuzzy model takes four inputs, and predicts a crisp number. According to the Elbow method, 46 TSK fuzzy rules have been generated, which constructed the initial rule base. The ﬁnal rule base has then been optimized using the GA. The objective function in this work is deﬁned as the root mean square error  RMSE , while the GA parameters are listed in Table 6.3.  6.4.2.2 Conventional Mamdani Fuzzy Inference  The conventional Mamdani fuzzy inference model is investigated in this section. The system uses 34 features for system modeling, which results in 34 inputs and one output Mamdani fuzzy model. Each input domain has been equally partitioned into four regions, described by four linguistic terms, namely, “very low,” “low,” “medium,” and “high;” and two fuzzy sets, “low” and “high,” are used to indicate normal and abnormal network trafﬁc, respectively. The fuzzy rules are obtained by a mapping mechanism based on the given training dataset. Given the input, which is a network trafﬁc package, the system ﬁrst fuzziﬁes the crisp value of the required features based on the mapping mechanism, then generates a fuzzy output based on  Dataset  Entire NSL-KDD-99 Entire NSL-KDD-99  Random part Random part Random part Random part  Table 6.2 Details of datasets for machine learning-based NIDSes Machine learning approach  Training  Testing  TSK+ [14]  Normal 67,343  Abnormal 58,630  Normal 9,711  Abnormal 9,083  67,343  Conventional fuzzy inference [33] FC-ANN [16] 3,000 5,922 MLP [24] SOM [10] 97,277 97,277 Hierarchical SOM [10]  Table 6.3 GA parameters  58,630  9,711  9,083  15,285 6,237 396,744 396,744  60,593 3,608 60,593 60,593  250,496 3,388 250,436 250,436  Parameters Population size Crossover rate Mutation rate Maximum iteration Termination threshold  Values 100.00 0.85 0.05 10,000.00 0.01   172  J. Li et al.  the generated rule base. Finally, the center of gravity method is employed to defuzzify the fuzzy output to a crisp one, which indicates whether the trafﬁc is normal or attack trafﬁc.  6.4.2.3 Fuzzy Clustering-Based ANN  Fuzzy clustering-based ANN uses all the 41 features to predict the ﬁve network behaviors. Note that the symbolic values contained in the dataset have been converted to continuous values. In the beginning, six training sub-datasets are obtained by using fuzzy C-means clustering. From there, six signal-hidden-layered neural network models are trained, each of which is referred to as the [41;18;5] structure. This means that each network takes 41 inputs, goes through 18 hidden nodes, and ﬁnally produces 5 outputs. In the aggregation progress, a new signal hidden-layered ANN model with the structure [5;13;5] is designed to aggregate all results from upper-level ANN models. The mean square error  MSE  is used as the ﬁtness function during system modeling, and the threshold of MSE is set to 0.001. Also, the learning rate and the momentum factor at both ANN model levels are set to 0.01 and 0.2, respectively.  6.4.2.4 Multilayer Perceptron  Expert knowledge has been used in this work to help select the most important features. In particular, 35 features, including ﬁve symbolic features and 30 numerical features, have been selected. Similar to the FC-ANN approach introduced above, the symbolic values were converted to numerical values. Because of the lack of data samples in U2R and R2U attacks, only three categories, namely, “normal,” “DoS,” and “probes,” were considered. As a result, 35 input nodes and three output nodes were used. In this experiment, a two hidden-layered MLP network model was implemented, constituting a four-layer MLP, whose structure is referred to as [35;35;35;3].  6.4.2.5 Hierarchical Self-Organizing Maps  A hierarchical self-organizing map architecture, which consists of two levels of SOM networks, each comprised of three layers, was used in this experiment. The ﬁrst layer was an input layer, with 20 input nodes  corresponding to 20 selected features . At the ﬁrst level of SOM, six SOM networks were deployed, each of which represented one of the basic TCP features, including “duration,” “protocol type,” “service,” “ﬂag,” “destination bytes,” and “source bytes.” During the training process, each training data sample was fed into each SOM network, thereby creating a number of mappings between inputs and six 6 × 6 grids on the second layer, which resulted in 36 × 6 = 216 neurons. After this, potential function clustering [84] was employed on each output layer of the ﬁrst SOM level to reduce the total neurons   6 Machine Learning Algorithms for Network …  173  from 36 to 6. As a consequence, the total number of neurons in the second layer was reduced to 36. These 36 neurons were used as inputs for the second SOM level to train a new SOM network that consists of a 20 × 20 grid of neurons, which indicates the mapping from the input space to the different network behaviors. The learning rate was set to 0.05, and the neighborhood function was conﬁgured as a Gaussian function.  6.4.2.6 Conventional Self-Organizing Maps  In this experiment, all the 41 features have been used for the intrusion detection system. During the training process, the learning rate was set to 0.05, and the Gaussian function was used as the neighborhood function. The developed system took 41 inputs to create a mapping between ﬁve categories of network behaviors into a 6 × 6 grid of neurons.  6.4.3 Result Comparisons  In order to enable a direct comparison between the different ML-NIDS approaches, a common measurement, the detection rate, is employed in this work. In particular, the detection rate can be deﬁned as follows:  Detection rate = Number of instances correctly detected  · 100  Total number of instances   6.8   The detection rates of the classiﬁcation results for each network trafﬁc category are summarized in Table 6.4.  The results show that all the approaches achieved a high detection performance in the normal, DoS, and probes category, which contain sufﬁcient data samples for training. Note that conventional ANN-based network intrusion detection systems, such as the MLP-based approach and the SOM-based approach, led to an extremely poor detection performance in the case of U2R and R2U. As discussed in Sect. 6.3.2,  Table 6.4 Performance comparison Approach TSK+ [14] Conventional fuzzy inference [33] FC-ANN [16] MLP [24] SOM [10] Hierarchical SOM [10]  Normal 93.10 82.93 91.32 89.20 98.50 92.40  DoS 97.84 90.42 96.70 90.90 96.80 96.50  U2R 65.38 19.05 76.92 N A 0.00 22.90  R2U 84.65 15.58 58.57 N A 0.15 11.30  Probes 85.69 37.08 80.00 90.30 63.40 72.80   174  J. Li et al.  this issue is caused by the lack of training data samples for both U2R and R2U. In this case, a future investigation may be required to identify how the detection threshold affects the detection performance. Obviously, similar to the modiﬁed version of the ANN approaches, the FC-ANN-based approach and the hierarchical SOM-based approach increased the detection rate. It is worth mentioning that the TSK− based intrusion detection system not only achieved the best detection performance in the normal, DoS, and probes classes, but also had an outstanding performance in the other two classes.  6.5 Conclusion  This chapter investigated how machine learning algorithms can be used to develop NIDSes. In particular, the chapter ﬁrst reviewed the existing intrusion detection techniques, including hardware deployments and software implementations. They are followed by the discussion of a number of machine learning algorithms and their applications in network intrusion detection. Finally, a well-known network secu- rity benchmark dataset, KDD 99, was employed for the evaluation of the reviewed machine learning-based network intrusion detection systems, with a critical anal- ysis of the results. Although the benchmark dataset, KDD 99, is still popular in recent research, it is relatively outdated and many of today’s network threats are not covered by the KDD 99 dataset. Therefore, future research may consider using alternate datasets  e.g., [88, 89] . In addition, as IoT continues to expand, the data being generated will continue to grow in volume and velocity. How conventional machine learning and artiﬁcial intelligence techniques can be expanded to deal with the continuously growing data is an interesting research direction.  References  1. Stampar M, Fertalj K  2015  Artiﬁcial intelligence in network intrusion detection. In: Biljanovic P, Butkovic Z, Skala K, Mikac B, Cicin-Sain M, Sruk V, Ribaric S, Gros S, Vrdoljak B, Mauher M, Sokolic A  eds  Proceedings of the 38th International Convention on Information and Communication Technology, Electronics and Microelectronics, pp 1318–1323. https:  doi. org 10.1109 MIPRO.2015.7160479  2. Sommer R, Paxson V  2010  Outside the closed world: on using machine learning for network intrusion detection. In: Proceedings of the 2010 IEEE Symposium on Security and Privacy. IEEE Computer Society, Los Alamitos, CA, USA, pp 305–316. https:  doi.org 10.1109 SP. 2010.25  3. Buczak AL, Guven E  2016  A survey of data mining and machine learning methods for cyber security intrusion detection. IEEE Commun Surv Tutor 18 2 :1153–1176. https:  doi.org 10. 1109 COMST.2015.2494502  4. Russell SJ, Norvig P  2009  Artiﬁcial intelligence: a modern approach, 3rd edn. Pearson, Essex 5. Farnaaz N, Jabbar M  2016  Random forest modeling for network intrusion detection system.  Procedia Comput Sci 89:213–217. https:  doi.org 10.1016 j.procs.2016.06.047   6 Machine Learning Algorithms for Network …  175  6. Ma Z, Kaban A  2013  K-nearest-neighbours with a novel similarity measure for intrusion detection. In: Jin Y, Thomas SA  eds  Proceedings of the 13th UK Workshop on Computational Intelligence. IEEE, New York, pp 266–271. https:  doi.org 10.1109 UKCI.2013.6651315  7. Mukherjee S, Sharma N  2012  Intrusion detection using Naïve Bayes classiﬁer with feature  reduction. Proc Tech 4:119–128. https:  doi.org 10.1016 j.protcy.2012.05.017  8. Thaseen IS, Kumar CA  2017  Intrusion detection model using fusion of chi-square feature selection and multi class SVM. J King Saud Univ Comput Inf Sci 29 4 :462–472. https:  doi. org 10.1016 j.jksuci.2015.12.004  9. Zhang C, Zhang G, Sun S  2009  A mixed unsupervised clustering-based intrusion detection model. In: Huang T, Li L, Zhao M  eds  Proceedings of the Third International Conference on Genetic and Evolutionary Computing. IEEE Computer Society, Los Alamitos, CA, USA, pp 426–428. https:  doi.org 10.1109 WGEC.2009.72  10. Kayacik HG, Zincir-Heywood AN, Heywood MI  2007  A hierarchical SOM-based intrusion detection system. Eng Appl Artif Intell 20 4 :439–451. https:  doi.org 10.1016 j.engappai. 2006.09.005  11. Garﬁnkel S  2002  Network forensics: tapping the Internet. https:  paulohm.com classes cc06   ﬁles Week6%20Network%20Forensics.pdf  12. Liao HJ, Lin CHR, Lin YC, Tung KY  2013  Intrusion detection system: a comprehensive  review. J Netw Comput Appl 36 1 :16–24. https:  doi.org 10.1016 j.jnca.2012.09.004  13. Bostani H, Sheikhan M  2017  Modiﬁcation of supervised OPF-based intrusion detection systems using unsupervised learning and social network concept. Pattern Recogn 62:56–72. https:  doi.org 10.1016 j.patcog.2016.08.027  14. Li J, Yang L, Qu Y, Sexton G  2018  An extended Takagi-Sugeno-Kang inference system  TSK+  with fuzzy interpolation and its rule base generation. Soft Comput 22 10 :3155–3170. https:  doi.org 10.1007 s00500-017-2925-8  15. Ramadas M, Ostermann S, Tjaden B  2003  Detecting anomalous network trafﬁc with self- organizing maps. In: Vigna G, Krügel C, Jonsson E  eds  Recent advances in intrusion detection. Springer, Heidelberg, pp 36–54. https:  doi.org 10.1007 978-3-540-45248-5_3  16. Wang G, Hao J, Ma J, Huang L  2010  A new approach to intrusion detection using artiﬁcial neural networks and fuzzy clustering. Expert Syst Appl 37 9 :6225–6232. https:  doi.org 10. 1016 j.eswa.2010.02.102  17. Wang W, Battiti R  2006  Identifying intrusions in computer networks with principal component analysis. In: Revell N, Wagner R, Pernul G, Takizawa M, Quirchmayr G, Tjoa AM  eds  Proceedings of the First International Conference on Availability, Reliability and Security. IEEE Computer Society, Los Alamitos, CA, USA. https:  doi.org 10.1109 ARES.2006.73  18. Yang L, Li J, Fehringer G, Barraclough P, Sexton G, Cao Y  2017  Intrusion detection system by fuzzy interpolation. In: Proceedings of the 2017 IEEE International Conference on Fuzzy Systems. https:  doi.org 10.1109 FUZZ-IEEE.2017.8015710  19. Sekar R, Gupta A, Frullo J, Shanbhag T, Tiwari A, Yang H, Zhou S  2002  Speciﬁcation-based anomaly detection: a new approach for detecting network intrusions. In: Proceedings of the 9th ACM Conference on Computer and Communications Security. ACM, New York, pp 265–274. https:  doi.org 10.1145 586110.586146  20. Tseng CY, Balasubramanyam P, Ko C, Limprasittiporn R, Rowe J, Levitt K  2003  A speciﬁcation-based intrusion detection system for AODV. In: Swarup V, Setia S  eds  Pro- ceedings of the 1st ACM Workshop on Security of ad hoc and Sensor Networks. ACM, New York, pp 125–134. https:  doi.org 10.1145 986858.986876  21. Bostani H, Sheikhan M  2017  Hybrid of anomaly-based and speciﬁcation-based IDS for Internet of Things using unsupervised OPF based on MapReduce approach. Comput Commun 98:52–71. https:  doi.org 10.1016 j.comcom.2016.12.001  22. Mukkamala S, Sung A  2003  Feature selection for intrusion detection with neural networks  and support vector machines. Trans Res Rec 1822:33–39. https:  doi.org 10.3141 1822-05  23. Kumar M, Hanumanthappa M, Kumar TVS  2012  Intrusion detection system using decision tree algorithm. In: Proceedings of the 14th IEEE International Conference on Communication Technology. IEEE, New York, pp 629–634. https:  doi.org 10.1109 ICCT.2012.6511281   176  J. Li et al.  24. Moradi M, Zulkernine M  2004  A neural network based system for intrusion detection and  classiﬁcation of attacks. http:  research.cs.queensu.ca ~moradi 148-04-MM-MZ.pdf  25. Ravale U, Marathe N, Padiya P  2015  Feature selection based hybrid anomaly intrusion detection system using K means and RBF kernel function. Procedia Comput Sci 45:428–435. https:  doi.org 10.1016 j.procs.2015.03.174  26. Liu G, Yi Z  2006  Intrusion detection using PCASOM neural networks. In: Wang J, Yi Z, Zurada JM, Lu BL, Yin H  eds  Advances in neural networks–ISNN 2006. Springer, Heidelberg, pp 240–245. https:  doi.org 10.1007 11760191_35  27. Chen Y, Abraham A, Yang B  2007  Hybrid ﬂexible neural-tree-based intrusion detection  systems. Int J Intell Syst 22 4 :337–352. https:  doi.org 10.1002 int.20203  28. Mamdani EH  1977  Application of fuzzy logic to approximate reasoning using linguistic syn- thesis. IEEE Trans Comput C-26 12 :1182–1191. https:  doi.org 10.1109 TC.1977.1674779 29. Takagi T, Sugeno M  1985  Fuzzy identiﬁcation of systems and its applications to modeling and control. IEEE Trans Syst Man Cybern SMC-15 1 :116–132. https:  doi.org 10.1109 TSMC. 1985.6313399  30. Li J, Shum HP, Fu X, Sexton G, Yang L  2016  Experience-based rule base generation and adaptation for fuzzy interpolation. In: Cordón O  ed  Proceedings of the 2016 IEEE Interna- tional Conference on Fuzzy Systems. IEEE, New York, pp 102–109. https:  doi.org 10.1109  FUZZ-IEEE.2016.7737674  31. Tan Y, Li J, Wonders M, Chao F, Shum HP, Yang L  2016  Towards sparse rule base generation for fuzzy rule interpolation. In: Cordón O  ed  Proceedings of the 2016 IEEE International Conference on Fuzzy Systems. IEEE, New York, pp 110–117. https:  doi.org 10.1109 FUZZ- IEEE.2016.7737675  32. Chaudhary A, Tiwari V, Kumar A  2014  Design an anomaly based fuzzy intrusion detection system for packet dropping attack in mobile ad hoc networks. In: Batra U  ed  Proceedings of the 2014 IEEE International Advance Computing Conference. IEEE, New York, pp 256–261. https:  doi.org 10.1109 IAdCC.2014.6779330  33. Shanmugavadivu R, Nagarajan N  2011  Network intrusion detection system using fuzzy logic.  Indian J Comput Sci Eng 2 1 :101–111  34. Naik N, Diao R, Shen Q  2017  Dynamic fuzzy rule interpolation and its application to intrusion  detection. IEEE Trans Fuzzy Syst https:  doi.org 10.1109 TFUZZ.2017.2755000  35. Kóczy TL, Hirota K  1993  Approximate reasoning by linear rule interpolation and general approximation. Int J Approx Reason 9 3 :197–225. https:  doi.org 10.1016 0888- 613X 93 90010-B  36. Huang Z, Shen Q  2006  Fuzzy interpolative reasoning via scale and move transformations.  IEEE Trans Fuzzy Syst 14 2 :340–359. https:  doi.org 10.1109 TFUZZ.2005.859324  37. Huang Z, Shen Q  2008  Fuzzy interpolation and extrapolation: a practical approach. IEEE  Trans Fuzzy Syst 16 1 :13–28. https:  doi.org 10.1109 TFUZZ.2007.902038  38. Li J, Yang L, Fu X, Chao F, Qu Y  2018  Interval Type-2 TSK+ fuzzy inference system. In: Proceedings of the 2018 IEEE International Conference on Fuzzy Systems. Curran Associates, Red Hook, NY, USA  39. Yang L, Shen Q  2010  Adaptive fuzzy interpolation and extrapolation with multiple-antecedent rules. In: Proceedings of the 2010 IEEE International Conference on Fuzzy Systems. Curran Associates, Red Hook, NY, USA. https:  doi.org 10.1109 FUZZY.2010.5584701  40. Naik N, Diao R, Quek C, Shen Q  2013  Towards dynamic fuzzy rule interpolation. In: Pro- ceedings of the 2013 IEEE International Conference on Fuzzy Systems. Curran Associates, Red Hook, NY, USA. https:  doi.org 10.1109 FUZZ-IEEE.2013.6622404  41. Naik N, Diao R, Shen Q  2014  Genetic algorithm-aided dynamic fuzzy rule interpolation. In: Proceedings of the 2014 IEEE International Conference on Fuzzy Systems. Curran Associates, Red Hook, NY, USA. https:  doi.org 10.1109 FUZZ-IEEE.2014.6891816  42. Shen Q, Yang L  2011  Generalisation of scale and move transformation-based fuzzy interpo- lation. J Adv Comput Intell Int Inf 15 3 :288–298. https:  doi.org 10.20965 jaciii.2011.p0288 43. Yang L, Chao F, Shen Q  2017  Generalised adaptive fuzzy rule interpolation. IEEE Trans  Fuzzy Syst 25 4 :839–853. https:  doi.org 10.1109 TFUZZ.2016.2582526   6 Machine Learning Algorithms for Network …  177  44. Yang L, Chen C, Jin N, Fu X, Shen Q  2014  Closed form fuzzy interpolation with interval type- 2 fuzzy sets. In: Proceedings of the 2014 IEEE International Conference on Fuzzy Systems. IEEE, pp 2184–2191. https:  doi.org 10.1109 FUZZ-IEEE.2014.6891643  45. Yang L, Shen Q  2011  Adaptive fuzzy interpolation. IEEE Trans Fuzzy Syst 19 6 :1107–1126.  https:  doi.org 10.1109 TFUZZ.2011.2161584  46. Yang L, Shen Q  2011  Adaptive fuzzy interpolation with uncertain observations and rule base. In: Lin C-T, Kuo Y-H  eds  Proceedings of the 2011 IEEE International Conference on Fuzzy Systems. IEEE, New York, pp 471–478. https:  doi.org 10.1109 FUZZY.2011.6007582  47. Yang L, Shen Q  2013  Closed form fuzzy interpolation. Fuzzy Sets Syst 225:1–22.  https:  doi.org 10.1016 j.fss.2013.04.001  48. Li J, Yang L, Fu X, Chao F, Qu Y  2017  Dynamic QoS solution for enterprise networks using TSK fuzzy interpolation. In: Proceedings of the 2017 IEEE International Conference on Fuzzy Systems. Curran Associates, Red Hook, NY, USA. https:  doi.org 10.1109 FUZZ-IEEE.2017. 8015711  49. Li J, Yang L, Shum HP, Sexton G, Tan Y  2015  Intelligent home heating controller using fuzzy rule interpolation. In: UK Workshop on Computational Intelligence, 7–9 September 2015, Exeter, UK  50. Naik N  2015  Fuzzy inference based intrusion detection system: FI-Snort. In: Wu Y, Min G, Georgalas N, Hu J, Atzori L, Jin X, Jarvis S, Liu L, Calvo RA  eds  Proceedings of the 2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing. IEEE Computer Society, Los Alamitos, CA, USA, pp 2062–2067. https:  doi.org 10.1109 CIT IUCC DASC PICOM.2015.306  51. Yang L, Li J, Hackney P, Chao F, Flanagan M  2017  Manual task completion time estimation for job shop scheduling using a fuzzy inference system. In: Wu Y, Min G, Georgalas N, Al-Dubi A, Jin X, Yang L, Ma J, Yang P  eds  Proceedings of the 2017 IEEE International Conference on Internet of Things  iThings  and IEEE Green Computing and Communications  GreenCom  and IEEE Cyber, Physical and Social Computing  CPSCom  and IEEE Smart Data  SmartData . IEEE Computer Society, Los Alamitos, CA, USA, pp 139–146. https:  doi. org 10.1109 iThings-GreenCom-CPSCom-SmartData.2017.26  52. Li J, Qu Y, Shum HPH, Yang L  2017  TSK inference with sparse rule bases. In: Angelov P, Gegov A, Jayne C, Shen Q  eds  Advances in computational intelligence systems. Springer, Cham, pp 107–123. https:  doi.org 10.1007 978-3-319-46562-3_8  53. Guha S, Yau SS, Buduru AB  2016  Attack detection in cloud infrastructures using artiﬁcial neu- ral network with genetic feature selection. In: Proceedings of the 14th International Conference on Dependable, Autonomic and Secure Computing, 14th International Conference on Pervasive Intelligence and Computing, 2nd International Conference on Big Data Intelligence and Com- puting and Cyber Science and Technology Congress. IEEE Computer Society, Los Alamitos, CA, USA, pp 414–419. https:  doi.org 10.1109 DASC-PICom-DataCom-CyberSciTec.2016. 32  54. Jensen R, Shen Q  2008  Computational intelligence and feature selection: rough and fuzzy  approaches. Wiley-IEEE Press, New York  55. Jensen R, Shen Q  2009  New approaches to fuzzy-rough feature selection. IEEE Trans Fuzzy  Syst 17 4 :824–838. https:  doi.org 10.1109 TFUZZ.2008.924209  56. Tsang EC, Chen D, Yeung DS, Wang XZ, Lee JW  2008  Attributes reduction using fuzzy rough sets. IEEE Trans Fuzzy Syst 16 5 :1130–1141. https:  doi.org 10.1109 TFUZZ.2006. 889960  57. Zuo Z, Li J, Anderson P, Yang L, Naik N  2018  Grooming detection using fuzzy-rough feature selection and text classiﬁcation. In: Proceedings of the 2018 IEEE International Conference on Fuzzy Systems. Curran Associates, Red Hook, NY, USA  58. Dash M, Liu H  1997  Feature selection for classiﬁcation. Intell. Data Anal 1 3 :131–156.  https:  doi.org 10.1016 S1088-467X 97 00008-5  59. Langley P  1994  Selection of relevant features in machine learning. In: Proceedings of the  AAAI Fall Symposium on Relevance. AAAI Press, Palo Alto, CA, USA, pp 245–271   178  J. Li et al.  60. Jensen R, Shen Q  2009  Are more features better? A response to attributes reduction using fuzzy rough sets. IEEE Trans Fuzzy Syst 17 6 :1456–1458. https:  doi.org 10.1109 TFUZZ. 2009.2026639  61. Guyon I, Elisseeff A  2003  An introduction to variable and feature selection. J Mach Learn  Res 3:1157–1182. http:  www.jmlr.org papers volume3 guyon03a guyon03a.pdf  62. Jensen R, Shen Q  2004  Semantics-preserving dimensionality reduction: rough and fuzzy- rough-based approaches. IEEE Trans Knowl Data Eng 16 12 :1457–1471. https:  doi.org 10. 1109 TKDE.2004.96  63. Parthaláin NM, Shen Q  2009  Exploring the boundary region of tolerance rough sets for feature  selection. Pattern Recogn 42 5 :655–667. https:  doi.org 10.1016 j.patcog.2008.08.029  64. Parthaláin NM, Shen Q, Jensen R  2010  A distance measure approach to exploring the rough set boundary region for attribute reduction. IEEE Trans Knowl Data Eng 22 3 :305–317. https:  doi.org 10.1109 TKDE.2009.119  65. Saeys Y, Inza I, Larrañaga P  2007  A review of feature selection techniques in bioinformatics.  Bioinformatics 23 19 :2507–2517. https:  doi.org 10.1093 bioinformatics btm344  66. Yu L, Liu H  2004  Efﬁcient feature selection via analysis of relevance and redundancy. J Mach  67. Thorndike RL  1953  Who belongs in the family? Psychometrika 18 4 :267–276. https:  doi.  Learn Res 5:1205–1224  org 10.1007 BF02289263  68. Anderson JA  1995  An introduction to neural networks. MIT Press, Cambridge, MA, USA 69. Planquart J-P  2001  Application of neural networks to intrusion detection. Sans Institute.  https:  www.sans.org reading-room whitepapers detection application-neural-networks- intrusion-detection-336  70. Cameron R, Zuo Z, Sexton G, Yang L  2017  A fall detection recognition system and an empirical study of gradient-based feature extraction approaches. In: Chao F, Schockaert S, Zhang Q  eds  Advances in computational intelligence systems. Springer, Cham, pp 276–289. https:  doi.org 10.1007 978-3-319-66939-7_24  71. Linda O, Vollmer T, Manic M  2009  Neural network based intrusion detection system for critical infrastructures. In: Proceedings of the 2009 International Joint Conference on Neural Networks. IEEE, Piscataway, NJ, USA, pp 1827–1834. https:  doi.org 10.1109 IJCNN.2009. 5178592  72. Subba B, Biswas S, Karmakar S  2016  A neural network based system for intrusion detec- tion and attack classiﬁcation. In: Proceedings of the Twenty-Second National Conference on Communication. IEEE, New York. https:  doi.org 10.1109 NCC.2016.7561088  73. Zuo Z, Yang L, Peng Y, Chao F, Qu Y  2018  Gaze-informed egocentric action recognition for memory aid systems. IEEE Access 6:12894–12904. https:  doi.org 10.1109 ACCESS.2018. 2808486  74. Beghdad R  2008  Critical study of neural networks in detecting intrusions. Comput Secur  27 5 :168–175. https:  doi.org 10.1016 j.cose.2008.06.001  75. Ouadfel S, Batouche M  2007  Antclust: an ant algorithm for swarm-based image clustering.  Inf Technol J 6 2 :196–201. https:  doi.org 10.3923 itj.2007.196.201  76. De la Hoz E, de la Hoz E, Ortiz A, Ortega J, Martínez-Álvarez A: Feature selection by multi-objective optimisation: application to network anomaly detection by hierarchical self- organising maps. Knowl Based Syst 71:322–338. https:  doi.org 10.1016 j.knosys.2014.08. 013  77. Labib K, Vemuri R  2002  NSOM: a real-time network-based intrusion detection system using  self-organizing maps. http:  web.cs.ucdavis.edu ~vemuri papers som-ids.pdf  78. Vasighi M, Amini H  2017  A directed batch growing approach to enhance the topology preservation of self-organizing map. Appl Soft Comput 55:424–435. https:  doi.org 10.1016  j.asoc.2017.02.015  79. Vokorokos L, Balaz A, Chovanec M  2006  Intrusion detection system using self organiz- ing map. Acta Electrotechnica et Informatica 6 1 . http:  www.aei.tuke.sk papers 2006 1  Vokorokos.pdf   6 Machine Learning Algorithms for Network …  179  80. Prabhakar SY, Parganiha P, Viswanatham VM, Nirmala M  2017  Comparison between genetic algorithm and self organizing map to detect botnet network trafﬁc. In: IOP conference series: materials science and engineering, vol 263. IOP Publishing, Bristol. https:  doi.org 10.1088  1757-899X 263 4 042103  81. Haykin S  2009  Neural networks and learning machines, 3rd edn. Prentice Hall, Upper Saddle  River, NJ, USA  82. Joo D, Hong T, Han I  2003  The neural network models for IDS based on the asymmet- ric costs of false negative errors and false positive errors. Expert Syst Appl 25 1 :69–75. https:  doi.org 10.1016 S0957-4174 03 00007-1  83. Patcha A, Park JM  2007  An overview of anomaly detection techniques: existing solutions and latest technological trends. Comput Netw 51 12 :3448–3470. https:  doi.org 10.1016 j. comnet.2007.02.001  84. Chiu SL  1994  Fuzzy model identiﬁcation based on cluster estimation. J Intell Fuzzy Syst  2 3 :267–278. https:  doi.org 10.3233 IFS-1994-2306  85. Mahoney MV  2003  A machine learning approach to detecting attacks by identifying anoma-  lies in network trafﬁc. Ph.D. thesis, Florida Institute of Technology, Melbourne, FL, USA  86. Elisa N, Yang L, Naik N  2018  Dendritic cell algorithm with optimised parameters using genetic algorithm. In: Proceedings of the 2018 IEEE Congress on Evolutionary Computation. Curran Associates, Red Hook, NY, USA  87. Tavallaee M, Bagheri E, Lu W, Ghorbani A  2009  A detailed analysis of the KDD Cup 99 data set. In: Wesolkowski S, Abbass H, Abielmona R  eds  Proceedings of the 2009 IEEE Symposium on Computational Intelligence for Security and Defense Applications. https:  doi. org 10.1109 CISDA.2009.5356528  88. Gharib A, Sharafaldin I, Lashkari AH, Ghorbani AA  2016  An evaluation framework for intrusion detection dataset. In: Joukov N, Kim H  eds  Proceedings of the 2016 International Conference on Information Science and Security. Curran Associates, Red Hook, NY, USA. https:  doi.org 10.1109 ICISSEC.2016.7885840  89. Sharafaldin I, Lashkari AH, Ghorbani AA  2018  Toward generating a new intrusion detection dataset and intrusion trafﬁc characterization. In: Mori P, Furnell S, Camp O  eds  Proceedings of the 4th International Conference on Information Systems Security and Privacy, vol 1, pp 108–116. https:  doi.org 10.5220 0006639801080116   Chapter 7 Android Application Analysis Using Machine Learning Techniques  Takeshi Takahashi and Tao Ban  Abstract The amount of malware that target Android terminals is growing. Malware applications are distributed to Android terminals in the form of Android Packages  APKs , similar to other Android applications. Analyzing APKs may thus help iden- tify malware. In this chapter, we describe how machine learning techniques can be used to identify Android malware. We begin by looking at the structure of an APK ﬁle and introduce techniques for identifying malware. We then describe how data can be collected and analyzed and then used to prepare a dataset. This is done by not only using permission requests and API calls, but also by using application clusters and descriptions as the source. To demonstrate the effectiveness of machine learning techniques for analyzing Android applications, we analyze the performance of support vector machine classiﬁcation on our dataset and compare it to that of a scheme that does not utilize machine learning. We also evaluate the effectiveness of the features used and further improve the classiﬁcation performance by removing irrelevant features. Finally, we address several issues and limitations on the use of machine learning techniques for analyzing Android applications.  7.1 Introduction  Android1 is one of the most widely used operating systems  OSes  for smartphones, with a global market share of 87.7% at the time of writing in terms of sales to end users [1]. Its open speciﬁcation facilitates the development of applications and their release on the Android application market. However, this makes it difﬁcult to manage Android OSes and applications in a centralized manner. The difﬁculty of management enables Android malware to be distributed without being discovered. The amount of Android malware is increasing, and various types of threats exist on the Android platform [2]. For example, Simplocker [3] and LockerPin [4] are two  1https:  www.android.com T. Takahashi  B  · T. Ban  National Institute of Information and Communications Technology, Tokyo, Japan e-mail: takeshi_takahashi@nict.go.jp   Springer Nature Switzerland AG 2019 L. F. Sikos  ed. , AI in Cybersecurity, Intelligent Systems Reference Library 151, https:  doi.org 10.1007 978-3-319-98842-9_7  181   182  T. Takahashi and T. Ban  ransomware applications for Android. Simplocker encrypts user ﬁles while Locker- Pin changes a device’s personal identiﬁcation number  PIN 2 lock. The new PIN is not known by the legitimate user, nor by the attacker, so the user will be unable to obtain the PIN even if the requested ransom is paid.  The impact of Android malware is not limited to smartphones. Although Android is currently used mainly on smartphone devices, it will be widely used by Inter- net of Things  IoT  devices as well. In fact, an Android-based OS for IoT, called “Android Things,” which was rebranded as “Brillo,” is already available.3 Conse- quently, Android malware will progressively affect more than just smartphones.  In this chapter, we introduce techniques for detecting Android malware and describe how machine learning techniques, in particular support vector machines  SVMs  [5, 6], can be used for analyzing Android applications.4 We also address dataset generation, which is essential for machine learning techniques, because their performance largely depends on the size and quality of the dataset. We use not only permission requests and API calls, but also application categories and descriptions as the data source. Using the generated dataset, we demonstrate the effectiveness of using an SVM for analyzing Android applications by measuring the classiﬁcation performance of an SVM and comparing it to that of a scheme that does not utilize machine learning. To improve the performance of the SVM further, we evaluate the effectiveness of the features used for analyzing Android applications and remove the non-contributing encoded features from the dataset. We then describe an experiment in which 94.15% classiﬁcation accuracy was achieved. We close with a discussion of several issues and limitations on the practical use of machine learning techniques in this ﬁeld.  The remainder of this chapter is structured as follows. Section 7.2 overviews the Android Application Package  APK  to provide the preliminary knowledge nec- essary for analyzing Android applications. Section 7.3 shows various techniques for identifying malware, including machine learning-based techniques. Section 7.4 describes the dataset used. Section 7.5 introduces a technique for detecting malware using an SVM and evaluates its effectiveness on our dataset. Section 7.6 provides a performance comparison with a non-machine learning-based scheme. Section 7.7 introduces a technique for improving the generalization ability of machine learning techniques by evaluating features and removing irrelevant features from the dataset. Section 7.8 discusses several issues and limitations on the use of machine learning techniques for analyzing Android applications. Finally, Sect. 7.9 concludes with a summary of the key points and potential lines of future research.  2A PIN is a numeric or alpha-numeric password or code used for user authentication. 3https:  developer.android.com things  4This article is an extended version of work published in [7, 8].   7 Android Application Analysis Using Machine Learning Techniques  183  Fig. 7.1 APK ﬁle structure  7.2 The Structure of Android Application Packages  Android applications are provided in the form of APKs. An APK ﬁle is a ZIP ﬁle consisting of multiple ﬁles, making it necessary to unzip it before use. As shown in Fig. 7.1, each APK ﬁle contains the ﬁles AndroidManifest.xml and classes.dex as well as signatures ﬁles and resources that are not precompiled. AndroidManifest.xml and classes.dex are often used to analyze and eval- uate the threats and vulnerabilities of APK ﬁles.  7.2.1 Central Conﬁguration  AndroidManifest.xml   All APK ﬁles contain an AndroidManifest.xml ﬁle, containing assorted appli- cation information described in XML, although stored in a binary form. The infor- mation can be extracted from this ﬁle using tools such as Apktool5 and Android Studio.6 Table 7.1 shows some of the tags included in the XML, including, among other things, permission requests and the API levels supported by the application.  Table 7.1 Core information described in AndroidManifest.xml Tag name Application  Content General conﬁguration of application, such as icons, labels, and display theme Range of API levels needed to run the application Permissions requested by the application Libraries used by the application  Uses-sdk  Uses-permission Uses-library  5https:  ibotpeaches.github.io Apktool  6https:  developer.android.com studio    184  T. Takahashi and T. Ban  A permission system is used to restrict access to privileged system resources, and Android application developers have to explicitly declare the permissions in AndroidManifest.xml. The ofﬁcial Android permissions are categorized into four types: Normal, Dangerous, Signature, and SignatureOrSystem, the last of which was discontinued from Android 6.0 onwards. The use of Dangerous permissions requires user approval, because they allow access to restricted resources and may have security implications if used incorrectly. When taken as the input of a machine learning algorithm for malware detection, permissions are usually coded as binary variables, i.e., an element in the vector can take only one of two values: 1 for a requested permission and 0 otherwise. The number of possible Android permissions depends on the version of the OS.  The Android OS is continuously evolving, and the available permission requests and API calls may change from time to time. Therefore, one needs to refer to the uses-sdk tag to obtain the supported API versions, and run risk analysis using the supported permissions and API calls to conduct risk analysis efﬁciently.  7.2.2 Dalvik Bytecode  classes.dex   Android applications are developed in Java and compiled into Java bytecode. The bytecode is then translated into Dalvik bytecode and stored in the Dalvik Executable  DEX  format,7 i.e., classes.dex. Dalvik bytecode is, like Java bytecode, reverse- engineering-friendly, enabling code analysis without the source code.  There are tools that facilitate code analysis. As mentioned earlier, each APK ﬁle is a ZIP ﬁle, and the ﬁles in it are stored in a binary form. Therefore, the ﬁle content cannot be analyzed directly. The aforementioned Apktool converts AndroidManifest.xml into text. It can also generate smali code8 by reverse engineering the bytecode in classes.dex. Although the smali code is not the original Java code, it accurately represents the bytecode in a human-readable man- ner, which is useful for analyzing the bytecode. In fact, an APK decompiled into smali code can be modiﬁed and then recompiled into a working APK. In addition, dex2jar9 can convert a classes.dex ﬁle into a JAR ﬁle, although the JAR ﬁle cannot be recompiled into a working APK. There are several other tools available online that are useful for APK ﬁle analysis.  A set of APIs are invoked during the runtime of each application. Each API is associated with a particular permission. When an API call is made, the approval of its associated permission is checked. The execution of the API is successful only if the necessary permission is granted by the user. This way, the permissions are used  7https:  source.android.com devices tech dalvik dex-format 8https:  github.com JesusFreke smali 9http:  code.google.com p dex2jar downloads list   7 Android Application Analysis Using Machine Learning Techniques  185  to protect the user’s private information from unauthorized access [9]. API calls of the Android application are stored in a smali ﬁle, which can be obtained through reverse engineering.  7.3 Techniques for Identifying Android Malware  Malware is distributed to Android terminals as APKs. To identify malware, APK analysis techniques are needed. Automated techniques are highly desired for malware analysis. In this section, three types of analysis  blacklisting, parameterizing, and classiﬁcation  are introduced.  7.3.1 Blacklisting  Blacklist-based detection techniques are often used in a variety of ﬁelds, such as spam ﬁltering and malware detection. There are speciﬁc blacklists for each application, e.g., blacklists of APK ﬁles, blacklists of URLs, and blacklists of application developer signatures. An APK ﬁle blacklist is a list of hash values of APK ﬁles identiﬁed as malware. A URL blacklist enumerates URLs that host malicious contents, such as malware, and APK ﬁles that communicate with these URLs identiﬁed as malware. A blacklist of application developers is a list of the certiﬁcates of malware developers, and it is very likely that APK ﬁles with these certiﬁcates are indeed malware.  Blacklisting relies on blacklists that have already been created and are readily available. For this reason, alternate means are necessary to evaluate applications that have not been previously evaluated. While manual evaluation is one option, there are also automated approaches, as discussed in the following sections.  7.3.2 Parameterizing  One of the automated approaches to judge whether an application is malware is to deﬁne a numerical parameter that represents the likelihood of the software being malware. If the value of this parameter exceeds a certain value, the software is considered malware.  DroidRisk is such a technique [10]. It quantiﬁes the risk level of an application from its permission requests. First, it quantiﬁes the risk level of each permission by multiplying the probability of the permission being misused by malware by the effect of such misuse. Then it sums the quantiﬁed risks of the permissions requested by the application as follows to produce a parameter that represents the risk level of the application:   cid:3   r =  cid:2   i  L  pi   × I   pi     cid:4  ,   7.1    186  T. Takahashi and T. Ban  where r denotes the quantiﬁed risk level, L  p  denotes the likelihood of permission p being used by malware, and I   p  denotes the effect of permission p being misused by the malware. If the value of r exceeds a predeﬁned threshold, DroidRisk labels the application as malware.  This scheme may be able to recognize many malware applications, but its capabil- ities are limited. For example, application types are not considered. The probability of a permission misuse varies depending on the application type, and this variance is not considered in DroidRisk. For example, there is nothing unusual in a calendar application requesting permission to access the user’s contact list, but it is suspicious if a calculator application does so.  There is a scheme that takes into account the application type. It identiﬁes a par- ticular permission, called category-based rare critical permission  CRCP , for each application type [11]. Although this permission could be used by many applications, it is rarely used by applications of a particular type. If the permission is requested by an application of a particular type, this scheme considers the application to be malware.  The schemes in this category are fairly easy for analysts to understand and verify. However, these schemes utilize only a few features. Using of more features can improve the performance of malware detection, but constructing a parameter using multiple features is a non-trivial task. The state-of-the-art classiﬁcation approach described in the next section can take into account multiple features and can provide better malware identiﬁcation performance than previous approaches.  7.3.3 Classiﬁcation  In our classiﬁcation approach, each sample is classiﬁed instead of deﬁning and using key parameters. If malware detection is the only concern, schemes using this approach classify the samples into two groups. One might argue that the schemes described above can also be considered classiﬁcation schemes, but unlike them, the schemes here usually do not provide human-friendly parameters and use machine learning techniques, which are well-known to be able to outperform other types of techniques in classiﬁcation tasks.  There are several different machine learning techniques, but not all of them are suitable for classifying Android applications. As described in Sect. 7.4, various fea- tures that include API calls are used as SVM input in this chapter. Encoding these features as numerical attributes results in a very large dataset. This is particularly true for the API feature: more than 30,000 unique APIs are used by the APK ﬁles in our data. This high-dimensional and sparse data makes it difﬁcult to use common machine learning techniques. However, according to Vapnik’s statistical learning theory [6], an SVM has guaranteed performance even for extremely high-dimensional data. A linear SVM is particularly preferable for such data because of its fast convergence rate and favorable generalization performance. Another reason to prefer a linear SVM   7 Android Application Analysis Using Machine Learning Techniques  187  over other methods is, as will be discussed in Sect. 7.7, that it facilitates fast feature selection for high-dimensional data. Therefore, only SVM is used for this purpose in this chapter.  Before running machine learning, the dataset must be prepared. This section describes the collection of common data types.  7.4 Dataset Preparation  7.4.1 APK File Analysis  APK ﬁles are required to generate various datasets for analysis. They are available in online APK markets, such as Google Play.10 There are several options to download these ﬁles, and there are tools and APIs specially designed for this purpose that are worth considering [12]. Some markets set several restrictions on the use of and access to APK ﬁles, which are described in the corresponding terms and conditions.  By analyzing APK ﬁles, the data required to build a dataset can be extracted. There are basically two types of analyses for extracting features: static analysis and dynamic analysis.  Static analysis inspects the ﬁles inside an APK ﬁle. Among these ﬁles, AndroidManifest.xml and classes.dex contain data suitable to be used as features. Permission request information can be extracted from Android Manifest.xml, and API calls from classes.dex. Further information, such as information about the intended application, may also be extracted from Android Manifest.xml, although this chapter utilizes permission request information and API call information only.  In contrast to static analysis, dynamic analysis monitors the behavior and activities of running applications. There are tools that facilitates such analyses, e.g., TaintDroid [13] and Epicc [14]. A common drawback of dynamic analysis is the limited interac- tion with the user interface, although some tools, such as Monkey [15] and DroidBot [16], address this issue.  While both types of analysis are important, this chapter focuses on static analyses.  7.4.2 Application Metadata  APK analysis is not the only way to generate data for datasets. Other data sources include, for example, the metadata of APK ﬁles available on online APK markets. The APK ﬁles on APK markets are published with an application description. Application  10https:  play.google.com   188  T. Takahashi and T. Ban  category information and the number of downloads are also often available. These pieces of information can be collected and used as features.  Because application descriptions cannot be handled in their original form, they need to be converted so that they can be used as SVM input. One way to do this is to apply the bag-of-words model, which lists the frequencies of each word appearing in the description [17]. However, the bag-of-words model generates high-dimensional datasets, so for this purpose it is inefﬁcient.  Another approach is to generate application clusters from the descriptions by applying a clustering algorithm, such as k-means [18], to the generated bag of words. CHABADA, for example, uses k-means to deﬁne clusters and then classiﬁes the APK ﬁles into clusters [19]. This process consists of three stages:  1. Data preprocessing stage. Words usable for latent Dirichlet allocation  LDA  [20] are produced from the descriptions. First, the description language is checked and the non-English descriptions are discarded. Non-textual items, i.e., numbers, HTML tags, web links, and email addresses, are also discarded. Then, stems11 are extracted from the descriptions and stop words are truncated. Finally, the number of words in the ﬁnal description is counted. All descriptions that contain less than ten words are discarded.12  2. Topic model generation stage. The words in the remaining descriptions are pro- cessed using LDA. These descriptions are imported and a number of topics are trained. A total of 300 topics were considered, with a topic proportion threshold of 0.05 and a maximum of four topics per entry. As a result, this process outputs several  maximum four  topic number-proportion value pairs.13  3. Cluster generation stage. The APK ﬁles are grouped into clusters in accor- dance with the topic number-proportion value pairs for each description using k-means.14 The number of categories was set to 12, which is identical to the number of categories used in the Opera Mobile Store.15  7.4.3 Label Assignment  To run supervised learning, label information is needed ﬁrst. There are several tech- niques for obtaining this information, one of which is introduced here. It uses Virus- Total,16 an information aggregator that derives data from the output of various eval-  11A stem is a part of a word and is common to all its inﬂected forms. 12We used the language detection library [21] to detect the language, stemmify [22] for the stemming operation, and the stoplist of MALLET [23] as the list of stop words. 13We used MALLET for running LDA and considered 300 topics because the MALLET documen- tation states that “the number of topics should depend to some degree on the size of the collection, but 200–400 will produce reasonably ﬁne-grained results.” 14We used the “kmeans” function of Ruby gem [24]. 15Opera Mobile Store was rebranded and is now called Bemobi Mobile Store [25]. 16https:  www.virustotal.com   7 Android Application Analysis Using Machine Learning Techniques  189  uation engines. If at least two of the results indicate that the ﬁle is malicious, the APK ﬁle is considered malware.  One issue to address is precisely deﬁning malware. Adware is a software type that is merely a nuisance  does not pose a threat  and has characteristics different from those of malware. While adware can be considered malware as well, for our purposes, adware needs to be distinguished from malware. Therefore, any malware applications with a name that includes the string “adware” or any of the strings listed in an adware family name list should be removed. The adware family name list can be built in different ways, one of which is listing all the software names identiﬁed as adware by VirusTotal. Note that the naming rules may differ depending on the evaluation engine, which needs to be considered when building the list.  7.4.4 Data Encoding  Encoding information from various sources into numerical features can be challeng- ing: feature format and availability of features may vary from source to source. For example, while the weight and order may carry essential information for API calls, they are unavailable for features like permission requests and application categories. To encode all features consistently, we encode all the features as binary attributes. For permission requests and API calls, an attribute is set to 1 if the permission API is declared in the manifest smali ﬁle; otherwise it is set to 0. Application categories can be encoded as binary attributes following one-hot encoding: each application category is modeled as a binary attribute, and the attributes are mutually exclusive so that only one in the set can have a value of 1. To encode description text into binary attributes, we ﬁrst perform clustering analysis using the technique mentioned in Sect. 7.4.2 so that each of the APKs is assigned to one of a handful of clusters. After that, encoding can be done following one-hot encoding. Hence, essential discriminant information—the more similar the description text of two APK ﬁles, the more likely they fall into the same class—is encoded as binary attributes.  Exploitation of description text using advanced text mining methods might lead to better generalization performance. Nevertheless, it also introduces new problems, such as signiﬁcantly increased data dimensionality and the reliance on feature weight- ing, so this approach is omitted here.  7.4.5 A Novel Dataset of Safe and Malicious APK Files  We generated our dataset17 using the techniques described in the previous section. We collected 87,182 APK ﬁles from the Opera Mobile Store for the period January– September 2014. The ﬁles from which permission requests could not be extracted were excluded simply because permission requests are strictly required for our ana-  17http:  mobilesec.nict.go.jp   190  T. Takahashi and T. Ban  lyzing schemes. The ﬁles that VirusTotal could not handle were also excluded from the dataset, because VirusTotal evaluation results are needed to label the dataset. Following the procedure described in Sect. 7.4.3, the ﬁles were labeled as malicious or safe, and adware was omitted. The result was a dataset of 61,730 APK ﬁles, consisting of 49,045 safe and 12,685 malicious ﬁles.  We also collected APK ﬁle metadata from the Opera Mobile Store for the same period. This metadata included the application category, the description, and the number of downloads. The description was used to generate the cluster information. The breakdowns of the dataset by category and cluster are shown in Tables 7.2 and 7.3, respectively. All the collected data were encoded using the procedure described in Sect. 7.4.4.  Table 7.2 Dataset by category Category Business and ﬁnance Communication E-books Entertainment Games Health Languages and translators Multimedia Organizers Ringtone Theme skins Travel and maps Total  Safe 3,779 2,114 2,784 14,138 12,090 1,536 734 2,422 1,300 327 5,276 2,545 49,045  Table 7.3 Dataset by cluster Safe Cluster 3,574 1 2 3,883 3,945 3 5,247 4 4,317 5 6 3,820 3,474 7 5,337 8 4,104 9 10 4,346 3,496 11 3,502 12 Total 49,045  Malicious 268 323 479 2,453 2,603 228 41 567 87 132 5,059 445 12,685  Malicious 934 889 976 1,206 1,174 1,077 919 2,091 811 832 818 958 12,685  Total 4,047 2,437 3,263 16,591 14,693 1,764 775 2,989 1,387 459 10,335 2,990 61,730  Total 4,508 4,772 4,921 6,453 5,491 4,897 4,393 7,428 4,915 5,178 4,314 4,460 61,730   7 Android Application Analysis Using Machine Learning Techniques  191  7.5 Detecting Malware Using SVM  In this section, an SVM is used with the generated dataset to detect malware. Different feature schemes are used to achieve better performance. The SVM as well as well- established techniques and metrics for performance evaluation are also explained.  7.5.1 SVM: A Brief Overview  An SVM, as described in previous chapters, is a machine learning model that maps features of data samples and draws a decision hyperplane that divides them into two groups  see Fig. 7.2 .  More precisely, an SVM draws hyperplanes  the dotted lines in the ﬁgure  that crosses the outermost sample for each of the two groups, and draws another hyper- plane  the solid line  that maximizes the distance to each of the hyperplanes. The hyperplane serves as the borderline between the two groups. When applied to Android malware identiﬁcation, an SVM can divide APK ﬁles into two groups by using the features of the APKs and drawing a decision hyperplane between the features of safe software  benignware  and those of malware. The idea of a two-class SVM is described as follows. From a set of training sam- ples D = { xi , yi  xi ∈ Rd , yi ∈ {−1,+1}, i = 1, . . . ,  cid:2 }, a two-class SVM learns a norm 1 linear function,  f  x  =  cid:3 w, x cid:4  + b,   7.2   Fig. 7.2 Division of samples with 2-dimensional features into two groups using an SVM  x2  x1   192  T. Takahashi and T. Ban  determined by weight vector w and threshold b, which represents the maximum margin.18 According to Vapnik’s statistical learning theory, margin maximization in the training set is equivalent to the minimization of the generalization error of the classiﬁer.  If the samples cannot be separated by using a linear hyperplane, there are two options. The ﬁrst option is to use a penalty parameter, C, and a group of slack parameters, ξi   cid:2  0 , to control the trade-off between the magnitude of the margin and the error introduced by the non-separable samples. The second option is to ﬁrst map the input vector xi into a high-dimensional  possibly inﬁnite  feature space, F, through a nonlinear mapping function  cid:4 , such that the improved separability between the training samples from opposite classes could ensure the existence of feasible solutions of the maximum margin hyperplane inF. With the so-called kernel trick, mapping  cid:4  could be implicitly implemented by some kernel function K  ·,· , which corresponds to an inner product in the feature space, i.e.,  K  xi , x j   =  cid:3  cid:4  xi  ,  cid:4  x j   cid:4 .   7.3   In practice, it is common to use both options in the learning phase to gain numerical robustness  from the ﬁrst option  and adaptivity to nonlinear problems and improved separability  from the second option  [26].  For a kernel-based SVM with misclassiﬁed samples being linearly penalized with a weight parameter C, usually referred to as the soft margin parameter, the optimiza- tion problem can be written as  The solution to this problem, generally known as the primal problem, can be obtained using Lagrangian theory such that w can be computed as  ⎧ ⎪⎪⎨ ⎪⎪⎩   cid:2  cid:9  i=1   cid:5 w cid:5 2 + C  1 2 ξi  cid:2  0,  min s.t. yi f   cid:4  xi     cid:2  1 − ξi , i = 1, . . . ,  cid:2 .  ξi ,  w =  cid:2  cid:2  i=1  α∗ i yi  cid:4  xi  ,   7.4    7.5   where α∗ known as the dual problem:  i is the solution of the following quadratic optimization problem, generally  18A margin is the distance from the hyperplane to the nearest training data point of either class.   7 Android Application Analysis Using Machine Learning Techniques  193  αi − 1 2   cid:2  cid:9  i, j=1  αi α j yi y j K xi , x j  ,  ⎧ ⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎩  max W  α  =  cid:2  cid:9  i=1  cid:2  cid:9  yi αi = 0, i=1 0  cid:3  αi  cid:3  C,  s.t.  i = 1, . . . ,  cid:2 .  After training, the SVM predicts the class label of an incoming test sample x on  the basis of the following decision function:   7.6    7.7   f  x  =  cid:2  cid:2  i=1  i yi K  x, xi   + b, α∗  which is a kernelized version of  7.2 . If f  x  > 0, x is assigned to the positive class, otherwise it is assigned to the negative class.  In our proposed scheme, the linear SVM solver proposed in [27] is used to build the model from the training data. In short, the following L2-SVM is solved using the trust-region Newton method introduced in [27] and implemented in the LIBLINEAR toolbox [28]:  g w  = 1 2  min w  wT w + C  max 0, 1 − yi wT xi     cid:11 2   cid:10    cid:2  cid:2   i=1   7.8   7.5.2 Feature Settings  Several academic papers have reported malware detection schemes using SVM. Peiravian and Zhu [29] extracted permission requests and API calls as features and used machine-learning techniques including an SVM. DroidAPIMiner [30] and Li et al. [31] perform feature selection in order to improve the performance of machine learning techniques.  To classify samples with high accuracy, one needs to choose features that capture the characteristics of benignware and malware. Different types of features could be used, including requested permissions, application categories, application descrip- tions, and the number of downloads. This chapter explores the use of an SVM for detecting malware by preparing various features and evaluates which types of fea- tures contribute effectively to the detection performance.  To investigate the relevance of different features in distinguishing malware from benignware, other evaluation metrics and classiﬁcation accuracy are compared using different feature settings. Starting with the major features, side features are added one type at a time to see how much performance improvement can be obtained by incorporating new features into the learning.   194  T. Takahashi and T. Ban  To identify the various feature schemes, the following notation is used in the next sections. SVM  p  denotes an SVM trained exclusively using the permission request feature p. SVM  p, ct  denotes an SVM trained using p and the application category feature ct. SVM  p, cl  denotes an SVM trained using p and the application cluster feature cl. SVM  p, ct, cl  denotes an SVM trained using p, ct, and cl. Other feature schemes can be formulated the same way; e.g., SVM  p, api, ct, cl  denotes an SVM trained using all three features plus the API feature api.  7.5.3 Hyperparameter Tuning  Algorithm performance often depends heavily on the hyperparameters used to train the SVM model. Hyperparameter optimization ﬁnds a tuple of hyperparameters that yields to a model that maximizes the generalization performance on independent data. Before performance evaluation, cross-validation is performed to estimate the generalization performance and identify the optimal hyperparameters for later use. To perform cross-validation, the training set is ﬁrst randomly partitioned into K disjoint folds  we set K = 10 as usual . Then, in the ith iteration, an SVM is trained using the complementary set of the ith fold, and tested against the ith fold. Finally, the evaluation is done by averaging the results over K iterations. For a linear SVM, the only critical parameter is the penalty parameter C in  7.4 , which controls the trade-off between the optimization objective and the number of training errors. We perform a grid search—simply an exhaustive search through a manually speciﬁed subset of C values—to determine the C that produces the best cross-validation result on the training dataset. Parameter C is then used to train an SVM model using all samples in the training set. Finally, the performance is evaluated by comparing the model output against the true labels of the test data.  7.5.4 Evaluation Metrics  There are common metrics for assessing classiﬁer performance. They are calculated using four intermediate parameters: true positive  TP , false positive  FP , true neg- ative  TN , and false negative  FN . TP is the number of predicted positive records classiﬁed correctly, FP is the number of predicted positive records classiﬁed incor- rectly, TN is the number of predicted negative records classiﬁed correctly, and FN is the number of predicted negative records classiﬁed incorrectly. Based on the four intermediate parameters, four metrics are used for performance assessment:   Accuracy: the probability of test records being classiﬁed correctly, i.e.,  Accuracy = TP + TN  n   7.9    7 Android Application Analysis Using Machine Learning Techniques  195    Precision: the probability of predicted positives being classiﬁed correctly, i.e.,  Precision =  TP  TP + FP    Recall: the probability that a record is classiﬁed correctly, i.e.,   7.11    False positive rate  FPR : the probability that a negative result is classiﬁed incor- rectly, i.e.,  Recall =  TP  TP + FN  FPR =  FP  FP + TN   7.10    7.12   When measuring the performance in experiments, the hyperparameters that result in maximum accuracy are considered optimal. For practical applications that provide security alerts, a scheme that minimizes the false negative rate  FNR  is desired, which is equivalent to  1 − recall , while for applications that provide automated countermeasures, minimizing FPR might be a better choice. Which metric to use has to be determined on a case-by-case basis.  7.5.5 Numerical Results  To evaluate the generalization performance of the classiﬁers, the dataset can be randomly divided into training and test sets, and the experiment can be conducted several times. In our experiment, 70% of the data were used for training, and the remainder were used for performance evaluation. The reported results average the results of ten rounds.  In Table 7.4, we compare the overall generalization performance using different  feature schemes.  Train [s]  Test [μs]  Table 7.4 Performance of SVM-based schemes Feature  SVM  p  SVM  p, ct  SVM  p, cl  SVM  p, ct, cl  SVM api  SVM api, ct  SVM api, cl  SVM api, ct, cl  SVM  p, api  SVM  p, api, ct, cl   Accuracy [%] Precision [%] Recall [%] 59.67 ± 0.41 88.87 ± 0.12 60.27 ± 0.80 89.45 ± 0.15 89.38 ± 0.10 60.25 ± 0.38 60.35 ± 0.59 89.45 ± 0.09 83.34 ± 1.00 94.07 ± 0.16 83.26 ± 0.73 94.09 ± 0.17 94.08 ± 0.17 83.40 ± 0.79 83.36 ± 0.89 94.07 ± 0.15 83.39 ± 0.80 94.07 ± 0.19 94.05 ± 0.18 83.28 ± 0.99  81.19 ± 0.65 83.87 ± 0.60 83.48 ± 0.67 83.76 ± 0.53 87.23 ± 0.45 87.37 ± 0.49 87.23 ± 0.53 87.20 ± 0.38 87.19 ± 0.57 87.21 ± 0.31  FPR [%] 3.58 ± 0.16 0.08 3.00 ± 0.15 0.19 3.08 ± 0.16 0.14 3.03 ± 0.14 0.31 3.16 ± 0.15 45.08 3.11 ± 0.14 35.65 3.16 ± 0.16 41.33 3.16 ± 0.12 40.24 3.17 ± 0.16 37.94 3.16 ± 0.10 43.55  0.3 0.3 0.3 0.3 12.4 12.4 12.4 12.4 12.4 12.4   196  T. Takahashi and T. Ban  Because accuracy, precision, and recall show similar tendencies, accuracy is used in the following discussion. The upper block of the table shows the results for when the model was initially trained using the permission request feature, and then the results for when the application category feature and the cluster feature were used as well. On our dataset, the permission request feature resulted in an accuracy of 88.87%. The accuracy increased to 89.45% when the application category feature was added and to 89.38% when the cluster feature was added. A comparison of these values indicates that the application category feature may carry slightly more discriminative information than the cluster feature.  The accuracy remained at 89.45% when these two metadata features were used in addition to the permission request feature. In other words, the two metadata features carry nearly equally valuable information when used for malware detection.  The middle block of Table 7.4 shows the results using the API feature. Here, the learning started with only the API feature; the other two metadata features were added later. Compared to the results for SVM  p , SVM api  shows signiﬁcant improve- ment for all four performance criteria. The 94.07% accuracy is a 5% performance gain compared to the accuracy with SVM  p . However, when the metadata features were added, there was only a slight improvement in accuracy. This suggests that metadata features carry little extra discriminative information that could be useful for classiﬁcation.  The bottom block of Table 7.4 shows the results for SVM  p, api  and SVM   p, api, ct, cl . For both of these feature schemes, accuracy remained at the same level that was obtained with the API feature alone. This conﬁrms that API calls carry more ﬁne-grained discriminative information than permission requests. Because of the associative relationship, permission means no additional discriminative infor- mation in terms of performance gain compared to classiﬁers based on the API fea- ture alone. Compared to SVM  p, api , the accuracy of SVM  p, api, ct, cl  dropped slightly when the metadata features were also used for learning. To sum it up, using metadata features with the API feature provides little additional discriminative infor- mation to be used for classiﬁcation.  The last two columns of Table 7.4 show the training and testing times for all settings. Due to the high performance of LIBLINEAR on mid-scale datasets, com- putation time cannot be considered a bottleneck neither for training nor for testing. More advanced features can also be included in the learning as long as the general- ization performance is improved.  7.6 Comparison with Parameterizing  As mentioned earlier, machine-learning-based approaches outperform other schemes in classiﬁcation. To understand the performance advantage, this section provides a comparison between a parameterizing approach and a machine learning approach  using DroidRisk and SVM, respectively . To enable a fair comparison, DroidRisk   7 Android Application Analysis Using Machine Learning Techniques  197  was extended to be able to incorporate features beyond permission requests. The following sections ﬁrst describe the extensions and then compare the performance to that of the SVM-based scheme.  7.6.1 Extending DroidRisk  We deﬁne a novel scheme, referred to as DR  p, ct , which extends DroidRisk to quantify security risks of APK ﬁles based on their application category ct. DroidRisk determines L  p  and I   p  by analyzing the entire dataset, but the optimal values can differ depending on the application type. For example, many applications in the “Travel&maps” category request permission for using GPS, whereas few within the “Multimedia” category request the same. Thus, DR  p, ct  sets different values for L  p  and I   p  for each application category by analyzing the data in each category as follows:   cid:3    cid:4   r =  cid:2   i  L  pi , ct   × I   pi , ct    .  We also deﬁne another scheme, referred to as DR  p, cl , which extends DroidRisk, and uses application cluster cl, instead of ct, where cl is derived using the scheme described earlier in Sect. 7.4.2. Similar to the category-based scheme, this scheme calculates the risk value r as   7.13    7.14    cid:3   r =  cid:2   i  L  pi , cl  × I   pi , cl    cid:4   .  7.6.2 DroidRisk Performance  Table 7.5 shows the performance of DroidRisk and its extensions deﬁned in the previous section in terms of accuracy, precision, recall, and FPR.  Table 7.5 Performance of DroidRisk  DR -based schemes  DR  p  DR  p, ct  DR  p, cl  DR api  DR api, ct  DR api, cl   Accuracy [%] 83.59±0.14 85.63±0.20 83.88±0.17 79.50±0.04 82.41±0.12 79.53±0.04  Precision [%] 67.02±0.61 59.68±2.19 65.78±0.88 51.77±18.47 45.82±9.01 53.84±9.17  Recall [%] 39.65±1.29 29.85±1.69 42.35±1.75 0.93±0.34 14.07±0.70 0.85±0.26  FPR [%] 5.05±0.25 4.93±0.48 6.05±0.40 0.18±0.06 6.75±0.86 0.13±0.04   198  T. Takahashi and T. Ban  The values are the averages of 10 × 10 cross-validation results. As shown in the table, the performance of DR  p  was improved by the use of application cate- gories and clusters. Compared to application clusters, application categories improve the performance further, although the contribution of application clusters could be improved even further by ﬁne-tuning the cluster generation algorithms. However, the performance improvement is insigniﬁcant. Most probably this is because both DR  p, ct  and DR  p, cl  suffer from the overﬁtting caused by dividing the dataset for contextual-based analysis.  One might argue that API calls should be used instead of permission requests for more accurate analysis. To investigate this, we measured the performance of these schemes using API calls instead of permission requests. More than 30,000 types of API calls, including Android Framework APIs, Java APIs, and third party APIs, were analyzed to calculate parameters L and I . For the sake of simplicity, the value of L was optimized for the top 10% of API calls used by malware, while the value was set to 1 for the remainder. Table 7.5 shows the performances of DR api , DR api, ct , and DR api, cl , where api denotes API calls. The performances of these API-based schemes lag well behind those of the permission-based schemes, most probably because the API-based schemes are affected more by the overﬁtting problem than the permission-based ones. The degree of overﬁtting is determined by the number of samples in the dataset and the dimensionality of features. Because the dataset here is the same, overﬁtting has a higher impact on those API-based schemes that use a higher dimensionality of features than on the permission-based schemes. When using API-calls and metadata, the degree of overﬁtting has to be taken into account.  A comparison of the results in Tables 7.4 and 7.5 clearly shows that SVM- based schemes provide better performance than DroidRisk-based ones. All of the DroidRisk-based schemes provide better performance than a random classiﬁer. In fact, they could be regarded as weak classiﬁers. These results conﬁrm the observation made earlier in Sect. 7.3 that classiﬁcation approaches outperform parameterizing approaches.  7.7 Feature Selection  The four feature types described in Sect. 7.4, when encoded as binary attributes, yield 37,720-dimensional feature vectors  583 dimensions for permission feature, 37,113 for API feature, 12 for category feature, and 12 for cluster feature . However, according to the evaluation results presented in Sect. 7.5.5, not all binary attributes contribute discriminative information equally for classiﬁcation. This observation gives rise to a more rigid analysis of the relevance of features. In the following, except when used in the context of feature selection, we use attribute to denote a single element of the vector input to the classiﬁer. We use feature to refer to a group of variables obtained from a particular source.   7 Android Application Analysis Using Machine Learning Techniques  199  In machine learning, feature selection is a well-explored tool to 1  defy the curse of dimensionality for generalization performance gain, 2  speed up computation in training and prediction, 3  reduce data gathering and storage cost, and 4  facilitate further data investigation. Traditional feature selection methods usually fall into one of two categories: wrapper methods or ﬁlter methods. Recently, more advanced methods have also been introduced, for example embedded methods, which add selection criteria to the objective functions, and hybrid methods, which combine multiple selection strategies.  Previous works on feature selection suggest that feature selection is essential to build strong and robust classiﬁers. This also holds for SVMs, which specialize in handling high-dimensional data. However, feature selection is often computationally intensive: it usually requires repeated training of the same classiﬁers with different feature subsets. Considering the large number of APK ﬁles and the high dimension- ality of our data, a cost-effective feature selection method is needed.  7.7.1 Recursive Feature Elimination  Thanks to the soundness and high efﬁciency of SVMs, several SVM-based feature selection algorithms have been proposed. Among them, recursive feature elimination based on SVM  SVM-RFE  [32] has become widely used for analyzing large-scale, high-dimensional data.  SVM-RFE was originally introduced to select relevant genes for cancer classiﬁ- cation. It uses the backward selection logic: it starts with ﬁtting an SVM model with all the attributes of interest, then ranks the attributes in accordance with the criterion described in the next section, with those that are insigniﬁcant at a chosen critical level omitted. Next, the algorithm successively reﬁts reduced SVM models using the remaining attributes and applies the same rule until all remaining attributes are statistically signiﬁcant or until a predeﬁned number of attributes remain. A modiﬁed version of RFE [32] is shown in Algorithm 11.  Algorithm 11: SVM recursive feature elimination Data: D = { xi , yi  } of dimension D Result: R  Ranked attribute indices with increasing signiﬁcance  1 A = {1,...,D}  Default attribute index set ; 2 while A  cid:2  1 do   = Train A,D ; SVM α∗ Rk =  cid:9  cid:2  i=1 yi α∗ e = arg mink Rk; Push R, e ; A := A − e;  i xik, for k ∈ A;  i  3 4 5 6 7 8 end   200  T. Takahashi and T. Ban  7.7.2 Ranking Criterion  A cost-effective ranking criterion was introduced in [32] for estimating the contribu- tion of an attribute to the classiﬁcation. Given the optimal solution of the objective function for the linear SVM in  7.8  as α∗ w∗ =  cid:2  cid:9  i=1  yi α∗ i xi  and w∗  , we have   7.15   Equation  7.15  shows that if some elements of w∗  are zero, deletion of the associ- ated input attributes will keep the decision function unchanged. Moreover, an attribute associated with an element close to zero in w∗ may be considered insigniﬁcant—its removal could be done without degrading the generalization ability of the classiﬁer. Therefore, the ranking criterion of the kth feature for a linear SVM can be deﬁned as  Rk =  cid:12  cid:5 w∗ cid:5 2 −  cid:5 w∗ k  cid:5 2 =    cid:2  cid:2   i=1  yi α∗  i xik   7.16   where xik is the kth element of xi , and w∗ k  is obtained from w∗ components xik to 0 for i = 1, . . . ,  cid:2 .  by setting all  7.7.3 Experiment  We ran SVM-RFE on our dataset using all four feature types described in Sect. 7.4 to ﬁnd attributes that produce optimal performance. Since there are trade-offs between different types of performance metrics, we chose the F-measure as the metric to be prioritized. It is the harmonic mean of precision and recall:  F-measure = 2 · precision · recall precision + recall  .   7.17   Note that an other metric could be prioritized, as discussed in Sect. 7.5.4.  To speed up the feature selection, when SVM-RFE is applied to our data, we eliminate multiple attributes at each iteration. Let the total number of attributes before feature selection to be D. Let the dimension of the binary input vector of SVM and the number of attributes removed at the ith iteration to be di and  cid:6 i , respectively. In the experiment,  cid:6 i is determined as follows:   7 Android Application Analysis Using Machine Learning Techniques  201  F-Measure Accuracy [%] Precision [%] Recall [%]  Table 7.6 Performance of SVM-RFE No. Dopt 3,129 0.857 1 17,115 0.868 2 3,885 0.853 3 34,112 0.854 4 1,927 0.854 5 2,542 0.853 6 25,869 0.857 7 18,000 0.852 8 10,329 0.853 9 1,734 0.855 10 11 8,918 0.854 Average 11,596 0.855 Median 8,918 0.854  94.18 94.59 94.02 94.08 94.04 94.19 94.21 94.03 94.11 94.07 94.14 94.15 94.11  86.62 88.15 87.40 86.82 87.15 87.75 86.54 86.97 86.78 87.22 86.93 87.12 86.97  84.76 85.50 83.23 84.03 83.74 83.01 84.85 83.50 83.81 83.83 84.01 84.02 83.83  FPR [%] Dbase 3.38 3.02 3.15 3.31 3.25 2.95 3.39 3.24 3.26 3.23 3.25 3.22 3.25  878 1,439 2,734 4,554 666 1,471 3,397 484 7,776 808 963 2,288 1,439   cid:6 i =  ⎧ ⎨ ⎩   cid:7 0.10di cid:8 ,  cid:7 0.005di cid:8 , 1,  di  cid:3  100.  di > 0.80D, 100 < di  cid:3  0.80D,   7.18   In this experiment, SVM-RFE was run 11 times on our dataset. Table 7.6 shows the performances for each run. The column “Dopt ” shows the number of attributes needed to produce the optimal performance. The F-measure, accuracy, precision, recall, and FPR of an SVM using the optimal feature set are also shown. A comparison between Tables 7.4 and 7.6 clearly shows that SVM performance is further improved when selecting relevant attributes and omitting irrelevant attributes with SVM-RFE.  The column “Dbase” in Table 7.6 shows the number of attributes needed to achieve the same performance achieved by an SVM without feature selection, which is shown in Table 7.4. As can be seen, only a median of 1,439 attributes  7,776 at maximum  are needed to achieve the same performance.  To further analyze the results of the experiment, the second run of SVM-RFE on our dataset providing the median value of Dbase was analyzed further. Figure 7.3 shows the result of the second run of SVM-RFE on our dataset.19 As can be seen in the ﬁgure, the performance drastically changes when we add contributing attributes in the early stage, and the curves saturate fast: the performance barely changes when extra attributes are added beyond 2,000 attributes in the training.  By using a limited number of attributes  a median of 8,918 and a maximum of 34,112 , the performance is improved further. The same level of performance as for an SVM without feature selection can be achieved by using an even smaller number of attributes  a median of 1,439 and a maximum of 7,776 , which would  19The shapes of the curves were similar for all the runs.   202  T. Takahashi and T. Ban  Fig. 7.3 Feature selection results  left: for all attributes, right: for top attributes   drastically decrease the complexity of calculation. Therefore, ranking attributes and selecting contributing attributes is an effective way to achieve better performance while minimizing computation cost.  A closer look into the ranked attributes in the results for the second run reveals that API calls are the most useful features in this domain. The ﬁrst appearances of permission requests, API calls, categories, and clusters in the feature ranking were 14th, 1st, 240th, and 942nd, respectively. Because all of these features appeared before the dimension reaches Dopt at least once, they contribute to malware detec- tion. However, the earlier they appear, the more contributing they are. These results support the results presented in Sect. 7.5.5—API calls contribute the most while cluster information contributes little to the classiﬁcation. The results for the second run also revealed that some attributes of permission request feature and application category feature contributes to classiﬁcation performance.  7.8 Issues and Limitations  Android application analysis is important for securing Android devices, and machine learning can efﬁciently assist in doing this. However, the following issues and limi- tations need to be considered:  1. False positives and false negatives. Machine learning techniques can perform classiﬁcation with good accuracy and precision, but false positives and false neg- atives are inevitable. These can cause a signiﬁcant risk in real-world applications. To minimize this risk, additional measures are needed. For example, the classi- ﬁcation results of machine learning should be reviewed by automated security   7 Android Application Analysis Using Machine Learning Techniques  203  tools and, in some cases, by human operators to produce whitelists and blacklists of safe and dangerous applications, which can be offered to users to be used as alternates to classiﬁcation results.  2. Dataset preparation. Android application analysis requires large enough datasets to be able to extract effective statistics and features, and to evaluate the effec- tiveness of assorted classiﬁcation algorithms. Online data collection has barriers such as copyright and usage restrictions. The use of these data sources is therefore determined by the corresponding legislations, terms, and conditions.  3. Criteria for labeling malware. When running a machine learning algorithm, label- ing is essential. In this chapter, applications have been classiﬁed as malware or benignware, and this classiﬁcation was based on labeling. Labeling each and every entry in the dataset as malicious or safe is not always easy because malware char- acteristics are not trivial to deﬁne. For example, it is debatable whether adware is malware or not. As a result, some malware analysis engines in VirusTotal treat adware as malware while other engines do not. Depending on the deﬁnition of malware, labeling results may change, which in turn affects the classiﬁcation results of machine learning. This makes it necessary to have an adequate malware deﬁnition for each application.  7.9 Conclusions  In this chapter, techniques for identifying Android malware have been described, and the usability of machine learning techniques for this purpose have been demon- strated, with the emphasis on SVMs. The dataset used for running an SVM to identify malware was generated from permission requests, API calls, application categories, and descriptions. Our evaluation measured the performance of SVM and DroidRisk on our dataset, leading to the conclusion that an SVM has a clear advantage over DroidRisk. To further improve SVM performance, SVM-RFE was used to evaluate attributes and identify their contributions. Removing the non-contributing features resulted in an accuracy of 94.15%. The evaluation conﬁrmed that API calls are the most dominant features, and that permission requests and application categories also make some contributions to classiﬁcation performance, while application clus- ters derived from application descriptions barely affect classiﬁcation. This chapter focused on SVM, but other machine learning techniques, such as deep learning, can also be considered for identifying Android malware. Different types of features and feature selection schemes could also be explored. Such research efforts would greatly contribute to a more secure Android ecosystem.   204  References  T. Takahashi and T. Ban  1. Van der Meulen R, Forni AA  2017  Gartner says demand for 4G smartphones in emerging markets spurred growth in second quarter of 2017. https:  www.gartner.com newsroom id  3788963  2. Sophos  2017  SophosLabs 2018 malware forecast. https:  www.sophos.com en-us en-us   medialibrary PDFs technical-papers malware-forecast-2018.pdf  3. Lipovsky R  2014  ESET analyzes Simplocker—ﬁrst Android ﬁle-encrypting, TOR-enabled  ransomware. https:  www.welivesecurity.com 2014 06 04 simplocker   4. Stefanko L  2015  Aggressive Android  ransomware  spreading  in  the USA.  https:  www.welivesecurity.com 2015 09 10 aggressive-android-ransomware-spreading- in-the-usa   5. Schölkopf B, Smola AJ  2001  Learning with kernels: support vector machines, regularization,  optimization, and beyond. MIT Press, Cambridge  6. Vapnik VN  1998  Statistical learning theory. Wiley, Hoboken 7. Takahashi T, Ban T, Tien CW, Lin CH, Inoue D, Nakao K  2016  The usability of metadata for Android application analysis. In: Hirose A, Ozawa S, Doya K, Ikeda K, Lee M, Liu D  eds  Proceedings of the 23nd International Conference on Neural Information Processing. Springer, Cham, pp 546–554. https:  doi.org 10.1007 978-3-319-46687-3_60  8. Ban T, Takahashi T, Guo S, Inoue D, Nakao K  2016  Integration of multimodal features for Android malware detection based on linear SVM. In: Proceedings of the 11th Asia Joint Conference on Information Security, IEEE, pp 141–146. https:  doi.org 10.1109 AsiaJCIS. 2016.29  9. Moonsamy V, Rong J, Liu S  2014  Mining permission patterns for contrasting clean and malicious Android applications. Future Gener Comp Syst 36:122–132. https:  doi.org 10.1016  j.future.2013.09.014  10. Wang Y, Zheng J, Sun C, Mukkamala S  2013  Quantitative security risk assessment of Android permissions and applications. In: Wang L, Shaﬁq B  eds  Data and applications security and privacy XXVII. Springer, Heidelberg, pp 226–241. https:  doi.org 10.1007 978-3-642-39256- 6_15  11. Sarma BP, Li N, Gates C, Potharaju R, Nita-Rotaru C, Molloy I  2012  Android permissions: a perspective combining risks and beneﬁts. In: Atluri V, Vaidya J  eds  Proceedings of the 17th ACM Symposium on Access Control Models and Technologies. ACM, New York, pp 13–22. https:  doi.org 10.1145 2295136.2295141  12. Demiroz A  2018  Google play crawler JAVA API. https:  github.com Akdeniz google-play-  crawler  13. Enck W, Gilbert P, Han S, Tendulkar V, Chun BG, Cox LP, Jung J, McDaniel P, Sheth AN  2014  TaintDroid: an information-ﬂow tracking system for realtime privacy monitoring on smartphones. ACM T Comput Syst 32 2 , Article 5. https:  doi.org 10.1145 2619091  14. Octeau D, McDaniel P, Jha S, Bartel A, Bodden E, Klein J, Le Traon Y  2013  Effective inter- component communication mapping in Android with Epicc: an essential step towards holistic security analysis. In: Proceedings of the 22nd USENIX Conference on Security. USENIX Asso- ciation, Berkeley, CA, USA, pp 543–558. https:  www.usenix.org system ﬁles conference  usenixsecurity13 sec13-paper_octeau.pdf  15. Android Developers  2018  UI Application exerciser monkey. https:  developer.android.com   studio test monkey  16. Li Y, Yang Z, Guo Y, Chen X  2017  DroidBot: a lightweight UI-guided test input generator for Android. In: Uchitel S, Orso A, Robillard M  eds  Proceedings of the 39th International Conference on Software Engineering Companion. IEEE Computer Society, Los Alamitos, CA, USA, pp 23–26. https:  doi.org 10.1109 ICSE-C.2017.8  17. Harris ZS  1954  Distributional structure. WORD 10 2–3 :146–162. https:  doi.org 10.1080   00437956.1954.11659520   7 Android Application Analysis Using Machine Learning Techniques  205  18. MacQueen J  1967  Some methods for classiﬁcation and analysis of multivariate observations. In: Le Cam LM, Neyman J  eds  Proceedings of the Fifth Berkeley Symposium on Mathe- matical Statistics and Probability, vol 1. University of California Press, Berkeley, pp 281–297. https:  projecteuclid.org euclid.bsmsp 1200512992  19. Gorla A, Tavecchia I, Gross F, Zeller A  2014  Checking app behavior against app descriptions. In: Jalote P, Briand L, van der Hoek A  eds  Proceedings of the 36th International Conference on Software Engineering. ACM, New York, pp 1025–1035. https:  doi.org 10.1145 2568225. 2568276  20. Blei DM, Ng AY, Jordan MI  2003  Latent Dirichlet allocation. J Mach Learn Res 3:993–1022 21. Shuyo N  2010  Language detection library for Java. https:  github.com shuyo language-  22. Pereda R  2011  Stemmify 0.0.2. https:  rubygems.org gems stemmify 23. McCallum AK  2002  MALLET: a machine learning for language toolkit. http:  mallet.cs.  detection  umass.edu  24. RubyGems.org  2013  kmeans 0.1.1. https:  rubygems.org gems kmeans  25. Apps and Games AS  2018  Bemobi mobile store. http:  apps.bemobi.com 26. Cover TM  1965  Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE Trans Electron EC-14 3 :326–334. https:  doi.org  10.1109 PGEC.1965.264137  27. Lin CJ, Weng RC, Keerthi SS  2007  Trust region Newton methods for large-scale logistic regression. In: Ghahramani Z  ed  Proceedings of the 24th International Conference on Machine Learning. ACM, New York, pp 561–568. https:  doi.org 10.1145 1273496.1273567  28. Fan RE, Chang KW, Hsieh CJ, Wang XR, Lin CJ  2008  LIBLINEAR: a library for large linear  classiﬁcation. J Mach Learn Res 9:1871–1874  29. Peiravian N, Zhu X  2013  Machine learning for Android malware detection using permission and API calls. In: Bourbakis N, Brodsky A  eds  Proceedings of the 25th International Confer- ence on Tools with Artiﬁcial Intelligence. IEEE Computer Society, Los Alamitos, CA, USA, pp 300–305. https:  doi.org 10.1109 ICTAI.2013.53  30. Aafer Y, Du W, Yin H  2013  DroidAPIMiner: mining API-level features for robust malware detection in Android. In: Zia T, Zomaya A, Varadharajan V, Mao M  eds  Security and privacy in communication networks. Springer, Cham, pp 86–103. https:  doi.org 10.1007 978-3-319- 04283-1_6  31. Li W, Ge J, Dai G  2015  Detecting malware for Android platform: an SVM-based approach. In: Qiu M, Zhang T, Das S  eds  Proceedings of the 2nd International Conference on Cyber Security and Cloud Computing. IEEE Computer Society, Los Alamitos, CA, USA, pp 464–469. https:  doi.org 10.1109 CSCloud.2015.50  32. Guyon I, Weston J, Barnhill S, Vapnik V  2002  Gene selection for cancer classiﬁcation using support vector machines. Mach Learn 46 1–3 :389–422. https:  doi.org 10.1023 A: 1012487302797
