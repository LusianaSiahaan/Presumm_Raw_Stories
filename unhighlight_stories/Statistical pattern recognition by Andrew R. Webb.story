Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   Statistical Pattern Recognition   Statistical Pattern Recognition  Second Edition  Andrew R. Webb  QinetiQ Ltd., Malvern, UK   First edition published by Butterworth Heinemann. Copyright c cid:1  2002  John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex PO19 8SQ, England  Telephone  +44  1243 779777  Email  for orders and customer service enquiries : cs-books@wiley.co.uk Visit our Home Page on www.wileyeurope.com or www.wiley.com  All Rights Reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except under the terms of the Copyright, Designs and Patents Act 1988 or under the terms of a licence issued by the Copyright Licensing Agency Ltd, 90 Tottenham Court Road, London W1T 4LP, UK, without the permission in writing of the Publisher. Requests to the Publisher should be addressed to the Permissions Department, John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex PO19 8SQ, England, or emailed to permreq@wiley.co.uk, or faxed to  +44  1243 770571.  This publication is designed to provide accurate and authoritative information in regard to the subject matter covered. It is sold on the understanding that the Publisher is not engaged in rendering professional services. If professional advice or other expert assistance is required, the services of a competent professional should be sought.  Other Wiley Editorial Ofﬁces  John Wiley & Sons Inc., 111 River Street, Hoboken, NJ 07030, USA  Jossey-Bass, 989 Market Street, San Francisco, CA 94103-1741, USA  Wiley-VCH Verlag GmbH, Boschstr. 12, D-69469 Weinheim, Germany  John Wiley & Sons Australia Ltd, 33 Park Road, Milton, Queensland 4064, Australia  John Wiley & Sons  Asia  Pte Ltd, 2 Clementi Loop 02-01, Jin Xing Distripark, Singapore 129809  John Wiley & Sons Canada Ltd, 22 Worcester Road, Etobicoke, Ontario, Canada M9W 1L1  British Library Cataloguing in Publication Data  A catalogue record for this book is available from the British Library  ISBN 0-470-84513-9 Cloth  ISBN 0-470-84514-7 Paper   Typeset from LaTeX ﬁles produced by the author by Laserwords Private Limited, Chennai, India Printed and bound in Great Britain by Biddles Ltd, Guildford, Surrey This book is printed on acid-free paper responsibly manufactured from sustainable forestry in which at least two trees are planted for each one used for paper production.   To Rosemary, Samuel, Miriam, Jacob and Ethan   Contents  Preface  Notation  1.1  1.2 1.3 1.4 1.5  2.1 2.2  2.3  1 Introduction to statistical pattern recognition  Introduction  Statistical pattern recognition 1.1.1 1.1.2 The basic model Stages in a pattern recognition problem Issues Supervised versus unsupervised Approaches to statistical pattern recognition 1.5.1 Elementary decision theory 1.5.2 Discriminant functions  1.6 Multiple regression 1.7 1.8  Outline of book Notes and references Exercises  2 Density estimation – parametric  Further developments Summary  Introduction Normal-based models 2.2.1 Linear and quadratic discriminant functions 2.2.2 Regularised discriminant analysis 2.2.3 Example application study 2.2.4 2.2.5 Normal mixture models 2.3.1 Maximum likelihood estimation via EM 2.3.2 Mixture models for discrimination 2.3.3 How many components? 2.3.4 Example application study 2.3.5 2.3.6  Further developments Summary  xv  xvii  1 1 1 2 3 4 5 6 6 19 25 27 28 30  33 33 34 34 37 38 40 40 41 41 45 46 47 49 49   viii CONTENTS  3 Density estimation – nonparametric  2.4  2.5 2.6 2.7 2.8  3.1 3.2  3.3  3.4 3.5  3.6 3.7 3.8 3.9  Bayesian estimates 2.4.1 Bayesian learning methods 2.4.2 Markov chain Monte Carlo 2.4.3 Bayesian approaches to discrimination 2.4.4 Example application study 2.4.5 2.4.6 Application studies Summary and discussion Recommendations Notes and references Exercises  Further developments Summary  k-nearest-neighbour decision rule Properties of the nearest-neighbour rule  Further developments Summary  Introduction Histogram method 3.2.1 Data-adaptive histograms 3.2.2 Independence assumption 3.2.3 Lancaster models 3.2.4 Maximum weight dependence trees 3.2.5 Bayesian networks 3.2.6 Example application study 3.2.7 3.2.8 k-nearest-neighbour method 3.3.1 3.3.2 3.3.3 Algorithms 3.3.4 Editing techniques 3.3.5 Choice of distance metric 3.3.6 Example application study 3.3.7 3.3.8 Expansion by basis functions Kernel methods 3.5.1 Choice of smoothing parameter 3.5.2 Choice of kernel 3.5.3 Example application study 3.5.4 3.5.5 Application studies Summary and discussion Recommendations Notes and references Exercises  Further developments Summary  Further developments Summary  50 50 55 70 72 75 75 75 77 77 77 78  81 81 82 83 84 85 85 88 91 91 92 93 93 95 95 98 101 102 103 104 105 106 111 113 114 115 115 116 119 120 120 121   Fisher’s criterion – linear discriminant analysis  4 Linear discriminant analysis  4.1 4.2  Perceptron criterion Fisher’s criterion  Introduction Two-class algorithms 4.2.1 General ideas 4.2.2 4.2.3 4.2.4 Least mean squared error procedures 4.2.5 4.2.6 Example application study 4.2.7 4.2.8  Further developments Summary  Support vector machines  4.4  Further developments  4.3 Multiclass algorithms 4.3.1 General ideas 4.3.2 Error-correction procedure 4.3.3 4.3.4 Least mean squared error procedures 4.3.5 Optimal scaling 4.3.6 Regularisation 4.3.7 Multiclass support vector machines 4.3.8 Example application study 4.3.9 4.3.10 Summary Logistic discrimination 4.4.1 Two-group case 4.4.2 Maximum likelihood estimation 4.4.3 Multiclass logistic discrimination 4.4.4 Example application study 4.4.5 4.4.6 Application studies Summary and discussion Recommendations Notes and references Exercises  Further developments Summary  4.5 4.6 4.7 4.8  5 Nonlinear discriminant analysis – kernel methods  5.1 5.2  5.3  Introduction Optimisation criteria 5.2.1 Least squares error measure 5.2.2 Maximum likelihood 5.2.3 Entropy Radial basis functions 5.3.1 Introduction 5.3.2 Motivation 5.3.3  Specifying the model  CONTENTS ix  123 123 124 124 124 128 130 134 141 142 142 144 144 145 145 148 152 155 155 156 156 158 158 158 159 161 162 163 163 163 164 165 165 165  169 169 171 171 175 176 177 177 178 181   x CONTENTS  Simple radial basis function  Further developments Summary  5.3.4 Radial basis function properties 5.3.5 5.3.6 Example application study 5.3.7 5.3.8 Nonlinear support vector machines 5.4.1 Types of kernel 5.4.2 Model selection 5.4.3 5.4.4 Example application study 5.4.5 5.4.6 Application studies Summary and discussion Recommendations Notes and references Exercises  Further developments Summary  Support vector machines for regression  6 Nonlinear discriminant analysis – projection methods  Properties  Introduction The multilayer perceptron 6.2.1 Introduction 6.2.2 Specifying the multilayer perceptron structure 6.2.3 Determining the multilayer perceptron weights 6.2.4 6.2.5 Example application study Further developments 6.2.6 6.2.7 Summary Projection pursuit 6.3.1 6.3.2 6.3.3 Example application study 6.3.4 6.3.5 Application studies Summary and discussion Recommendations Notes and references Exercises  Introduction Projection pursuit for discrimination  Further developments Summary  7 Tree-based methods  Introduction Classiﬁcation trees 7.2.1 Introduction 7.2.2 Classiﬁer tree construction 7.2.3 Other issues 7.2.4 Example application study  5.4  5.5 5.6 5.7 5.8  6.1 6.2  6.3  6.4 6.5 6.6 6.7  7.1 7.2  187 187 187 189 189 190 191 192 192 195 196 197 197 199 199 200 200  203 203 204 204 205 205 212 213 214 216 216 216 218 219 220 220 221 221 222 223 223  225 225 225 225 228 237 239   CONTENTS xi  7.2.5 7.2.6  Further developments Summary  7.3 Multivariate adaptive regression splines  Introduction  Further developments Summary  7.3.1 7.3.2 Recursive partitioning model 7.3.3 Example application study 7.3.4 7.3.5 Application studies Summary and discussion Recommendations Notes and references Exercises  8 Performance  Statistical tests  Further developments Summary  Further developments Summary  Introduction Performance assessment 8.2.1 Discriminability 8.2.2 Reliability 8.2.3 ROC curves for two-class rules 8.2.4 Example application study 8.2.5 8.2.6 Comparing classiﬁer performance 8.3.1 Which technique is best? 8.3.2 8.3.3 Comparing rules when misclassiﬁcation costs are uncertain 8.3.4 Example application study 8.3.5 8.3.6 Combining classiﬁers 8.4.1 Introduction 8.4.2 Motivation 8.4.3 Characteristics of a combination scheme 8.4.4 Data fusion 8.4.5 Classiﬁer combination methods 8.4.6 Example application study 8.4.7 8.4.8 Application studies Summary and discussion Recommendations Notes and references Exercises  Further developments Summary  9 Feature selection and extraction  9.1  Introduction  7.4 7.5 7.6 7.7  8.1 8.2  8.3  8.4  8.5 8.6 8.7 8.8  239 240 241 241 241 244 245 245 245 247 247 248 248  251 251 252 252 258 260 263 264 265 266 266 267 267 269 270 271 271 271 272 275 278 284 297 298 298 299 299 300 300 301  305 305   xii CONTENTS  9.2  9.3  Feature selection criteria Search algorithms for feature selection Suboptimal search algorithms  Further developments Summary  Feature selection 9.2.1 9.2.2 9.2.3 9.2.4 Example application study 9.2.5 9.2.6 Linear feature extraction 9.3.1 9.3.2 Karhunen–Lo`eve transformation 9.3.3 9.3.4 Example application study 9.3.5 9.3.6  Further developments Summary  Principal components analysis  Factor analysis  9.4 Multidimensional scaling 9.4.1 Classical scaling 9.4.2 Metric multidimensional scaling 9.4.3 Ordinal scaling 9.4.4 Algorithms 9.4.5 Multidimensional scaling for feature extraction 9.4.6 Example application study 9.4.7 9.4.8 Application studies Summary and discussion Recommendations Notes and references Exercises  Further developments Summary  9.5 9.6 9.7 9.8  10 Clustering  Introduction  10.1 10.2 Hierarchical methods  10.2.1 Single-link method 10.2.2 Complete-link method 10.2.3 Sum-of-squares method 10.2.4 General agglomerative algorithm 10.2.5 Properties of a hierarchical classiﬁcation 10.2.6 Example application study 10.2.7 Summary 10.3 Quick partitions 10.4 Mixture models  10.5  10.4.1 Model description 10.4.2 Example application study Sum-of-squares methods 10.5.1 Clustering criteria 10.5.2 Clustering algorithms 10.5.3 Vector quantisation  307 308 311 314 317 317 318 318 319 329 335 342 343 344 344 345 346 347 350 351 352 353 353 354 355 355 356 357  361 361 362 364 367 368 368 369 370 370 371 372 372 374 374 375 376 382   10.5.4 Example application study 10.5.5 Further developments 10.5.6 Summary  10.6 Cluster validity  10.6.1 Introduction 10.6.2 Distortion measures 10.6.3 Choosing the number of clusters 10.6.4 Identifying genuine clusters  Summary and discussion  10.7 Application studies 10.8 10.9 Recommendations 10.10 Notes and references  Exercises  11 Additional topics  11.1 Model selection  11.1.1 Separate training and test sets 11.1.2 Cross-validation 11.1.3 The Bayesian viewpoint 11.1.4 Akaike’s information criterion Learning with unreliable classiﬁcation  11.2 11.3 Missing data 11.4 Outlier detection and robust procedures 11.5 Mixed continuous and discrete variables 11.6  Structural risk minimisation and the Vapnik–Chervonenkis dimension 11.6.1 Bounds on the expected risk 11.6.2 The Vapnik–Chervonenkis dimension  A Measures of dissimilarity  A.1 Measures of dissimilarity A.1.1 Numeric variables A.1.2 Nominal and ordinal variables A.1.3 Binary variables A.1.4 Summary Distances between distributions A.2.1 Methods based on prototype vectors A.2.2 Methods based on probabilistic distance A.2.3 Probabilistic dependence Discussion  A.3  A.2  B Parameter estimation  B.1  Parameter estimation B.1.1 Properties of estimators B.1.2 Maximum likelihood B.1.3 Problems with maximum likelihood B.1.4 Bayesian estimates  CONTENTS xiii  394 395 395 396 396 397 397 399 400 402 404 405 406  409 409 410 410 411 411 412 413 414 415  416 416 417  419 419 419 423 423 424 425 425 425 428 429  431 431 431 433 434 434   xiv CONTENTS  C Linear algebra  Basic properties and deﬁnitions Notes and references  D Data  C.1 C.2  D.1 D.2 D.3 D.4 D.5 D.6  E.1 E.2 E.3  Introduction Formulating the problem Data collection Initial examination of data Data sets Notes and references  E Probability theory  Deﬁnitions and terminology Normal distribution Probability distributions  References  Index  437 437 441  443 443 443 444 446 448 448  449 449 454 455  459  491   Preface  This book provides an introduction to statistical pattern recognition theory and techniques. Most of the material presented is concerned with discrimination and classiﬁcation and has been drawn from a wide range of literature including that of engineering, statistics, computer science and the social sciences. The book is an attempt to provide a concise volume containing descriptions of many of the most useful of today’s pattern process- ing techniques, including many of the recent advances in nonparametric approaches to discrimination developed in the statistics literature and elsewhere. The techniques are illustrated with examples of real-world applications studies. Pointers are also provided to the diverse literature base where further details on applications, comparative studies and theoretical developments may be obtained.  Statistical pattern recognition is a very active area of research. Many advances over recent years have been due to the increased computational power available, enabling some techniques to have much wider applicability. Most of the chapters in this book have concluding sections that describe, albeit brieﬂy, the wide range of practical applications that have been addressed and further developments of theoretical techniques.  Thus, the book is aimed at practitioners in the ‘ﬁeld’ of pattern recognition  if such a multidisciplinary collection of techniques can be termed a ﬁeld  as well as researchers in the area. Also, some of this material has been presented as part of a graduate course on information technology. A prerequisite is a knowledge of basic probability theory and linear algebra, together with basic knowledge of mathematical methods  the use of Lagrange multipliers to solve problems with equality and inequality constraints, for example . Some basic material is presented as appendices. The exercises at the ends of the chapters vary from ‘open book’ questions to more lengthy computer projects.  Chapter 1 provides an introduction to statistical pattern recognition, deﬁning some ter- minology, introducing supervised and unsupervised classiﬁcation. Two related approaches to supervised classiﬁcation are presented: one based on the estimation of probability density functions and a second based on the construction of discriminant functions. The chapter concludes with an outline of the pattern recognition cycle, putting the remaining chapters of the book into context. Chapters 2 and 3 pursue the density function approach to discrimination, with Chapter 2 addressing parametric approaches to density estimation and Chapter 3 developing classiﬁers based on nonparametric schemes.  Chapters 4–7 develop discriminant function approaches to supervised classiﬁcation. Chapter 4 focuses on linear discriminant functions; much of the methodology of this chapter  including optimisation, regularisation and support vector machines  is used in some of the nonlinear methods. Chapter 5 explores kernel-based methods, in particular,   xvi PREFACE  the radial basis function network and the support vector machine, techniques for discrimi- nation and regression that have received widespread study in recent years. Related nonlin- ear models  projection-based methods  are described in Chapter 6. Chapter 7 considers a decision-tree approach to discrimination, describing the classiﬁcation and regression tree  CART  methodology and multivariate adaptive regression splines  MARS .  Chapter 8 considers performance: measuring the performance of a classiﬁer and im-  proving the performance by classiﬁer combination.  The techniques of Chapters 9 and 10 may be described as methods of exploratory data analysis or preprocessing  and as such would usually be carried out prior to the supervised classiﬁcation techniques of Chapters 2–7, although they could, on occasion, be post-processors of supervised techniques . Chapter 9 addresses feature selection and feature extraction – the procedures for obtaining a reduced set of variables characterising the original data. Such procedures are often an integral part of classiﬁer design and it is somewhat artiﬁcial to partition the pattern recognition problem into separate processes of feature extraction and classiﬁcation. However, feature extraction may provide insights into the data structure and the type of classiﬁer to employ; thus, it is of interest in its own right. Chapter 10 considers unsupervised classiﬁcation or clustering – the process of grouping individuals in a population to discover the presence of structure; its engineering application is to vector quantisation for image and speech coding.  Finally, Chapter 11 addresses some important diverse topics including model selec- tion. Appendices largely cover background material and material appropriate if this book is used as a text for a ‘conversion course’: measures of dissimilarity, estimation, linear algebra, data analysis and basic probability.  The website www.statistical-pattern-recognition.net contains refer-  ences and links to further information on techniques and applications.  In preparing the second edition of this book I have been helped by many people. I am grateful to colleagues and friends who have made comments on various parts of the manuscript. In particular, I would like to thank Mark Briers, Keith Copsey, Stephen Luttrell, John O’Loghlen and Kevin Weekes  with particular thanks to Keith for examples in Chapter 2 ; Wiley for help in the ﬁnal production of the manuscript; and especially Rosemary for her support and patience.   Notation  Some of the more commonly used notation is given below. I have used some notational conveniences. For example, I have tended to use the same symbol for a variable as well as a measurement on that variable. The meaning should be obvious from the context. Also, I denote the density function of x as p.x  and y as p.y , even though the functions differ. A vector is denoted by a lower-case quantity in bold face, and a matrix by upper case.  p C n ni !i X1; : : : ; X p x1; : : : ; x p x D .x1; : : : ; x p T X D [x1; : : : ; xn]T X D  2 64  : : : : : : : : :  x11 ::: xn1  3 75  x1 p ::: xnp  P.x  D prob.X1  cid:3  x1; : : : ; X p  cid:3  x p  p.x  D @ P=@x p.!i   µ D R x p.x dx µi D R x p.x dx m D .1=n Pn rD1 xr mi D .1=ni  Pn rD1 zir xr  .xr  cid:6  m .xr  cid:6  m T  rD1  Pn O cid:5  D 1 n=.n  cid:6  1  O cid:5   n  number of variables number of classes number of measurements number of measurements in class i label for class i p random variables measurements on variables X1; : : : ; X p measurement vector n ð p data matrix  prior probability of class i population mean mean of class i ; i D 1; : : : ; C sample mean sample mean of class i ; i D 1; : : : ; C zir D 1 if xr 2 !i ; 0 otherwise ni D number of patterns in !i D Pn  sample covariance matrix  maximum likelihood estimate  sample covariance matrix  unbiased estimate   rD1 zir   xviii NOTATION  jD1 zi j .x j  cid:6  mi  .x j  cid:6  mi  T  .mi  cid:6  m .mi  cid:6  m T  O cid:5 i  ni n  O cid:5 i D .1=ni  Pn O cid:5 i Si D ni ni cid:6 1 SW D PC iD1 S D n n cid:6 C SW S B D PC iD1 S B C SW D O cid:5  jjAjj2 D P i j A2 i j N .m;  cid:5   E[YjX] I. cid:8     ni n  sample covariance matrix of class i  maximum likelihood estimate  sample covariance matrix of class i  unbiased estimate  pooled within-class sample covariance matrix pooled within-class sample covariance matrix  unbiased estimate  sample between-class matrix  normal distribution, mean; m covariance matrix  cid:5  expectation of Y given X =1 if  cid:8  = true else 0  Notation for speciﬁc probability density functions is given in Appendix E.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   1  Introduction to statistical pattern  recognition  Overview  Statistical pattern recognition is a term used to cover all stages of an investigation from problem formulation and data collection through to discrimination and clas- siﬁcation, assessment of results and interpretation. Some of the basic terminology is introduced and two complementary approaches to discrimination described.  1.1 Statistical pattern recognition  1.1.1 Introduction  This book describes basic pattern recognition procedures, together with practical appli- cations of the techniques on real-world problems. A strong emphasis is placed on the statistical theory of discrimination, but clustering also receives some attention. Thus, the subject matter of this book can be summed up in a single word: ‘classiﬁcation’, both supervised  using class information to design a classiﬁer – i.e. discrimination  and unsupervised  allocating to groups without class information – i.e. clustering .  Pattern recognition as a ﬁeld of study developed signiﬁcantly in the 1960s. It was very much an interdisciplinary subject, covering developments in the areas of statis- tics, engineering, artiﬁcial intelligence, computer science, psychology and physiology, among others. Some people entered the ﬁeld with a real problem to solve. The large numbers of applications, ranging from the classical ones such as automatic character recognition and medical diagnosis to the more recent ones in data mining  such as credit scoring, consumer sales analysis and credit card transaction analysis , have attracted con- siderable research effort, with many methods developed and advances made. Other re- searchers were motivated by the development of machines with ‘brain-like’ performance, that in some way could emulate human performance. There were many over-optimistic and unrealistic claims made, and to some extent there exist strong parallels with the   2 Introduction to statistical pattern recognition  growth of research on knowledge-based systems in the 1970s and neural networks in the 1980s.  Nevertheless, within these areas signiﬁcant progress has been made, particularly where the domain overlaps with probability and statistics, and within recent years there have been many exciting new developments, both in methodology and applications. These build on the solid foundations of earlier research and take advantage of increased compu- tational resources readily available nowadays. These developments include, for example, kernel-based methods and Bayesian computational methods.  The topics in this book could easily have been described under the term machine learning that describes the study of machines that can adapt to their environment and learn from example. The emphasis in machine learning is perhaps more on computationally intensive methods and less on a statistical approach, but there is strong overlap between the research areas of statistical pattern recognition and machine learning.  1.1.2 The basic model  Since many of the techniques we shall describe have been developed over a range of diverse disciplines, there is naturally a variety of sometimes contradictory terminology. We shall use the term ‘pattern’ to denote the p-dimensional data vector x D .x1; : : : ; x p T of measurements  T denotes vector transpose , whose components xi are measurements of the features of an object. Thus the features are the variables speciﬁed by the investigator and thought to be important for classiﬁcation. In discrimination, we assume that there exist C groups or classes, denoted !1; : : : ; !C, and associated with each pattern x is a categorical variable z that denotes the class or group membership; that is, if z D i, then the pattern belongs to !i , i 2 f1; : : : ; Cg.  Examples of patterns are measurements of an acoustic waveform in a speech recogni- tion problem; measurements on a patient made in order to identify a disease  diagnosis ; measurements on patients in order to predict the likely outcome  prognosis ; measure- ments on weather variables  for forecasting or prediction ; and a digitised image for character recognition. Therefore, we see that the term ‘pattern’, in its technical meaning, does not necessarily refer to structure within images.  The main topic in this book may be described by a number of terms such as pattern classiﬁer design or discrimination or allocation rule design. By this we mean specifying the parameters of a pattern classiﬁer, represented schematically in Figure 1.1, so that it yields the optimal  in some sense  response for a given pattern. This response is usually an estimate of the class to which the pattern belongs. We assume that we have a set of patterns of known class f.xi ; zi  ; i D 1; : : : ; ng  the training or design set  that we use to design the classiﬁer  to set up its internal parameters . Once this has been done, we may estimate class membership for an unknown pattern x.  The form derived for the pattern classiﬁer depends on a number of different factors. It depends on the distribution of the training data, and the assumptions made concerning its distribution. Another important factor is the misclassiﬁcation cost – the cost of making an incorrect decision. In many applications misclassiﬁcation costs are hard to quantify, being combinations of several contributions such as monetary costs, time and other more subjective costs. For example, in a medical diagnosis problem, each treatment has dif- ferent costs associated with it. These relate to the expense of different types of drugs,   Stages in a pattern recognition problem 3  sensor  representation  pattern  feature selector   extractor  classiﬁer   cid:1   decision  feature pattern  Figure 1.1 Pattern classiﬁer  the suffering the patient is subjected to by each course of action and the risk of further complications.  Figure 1.1 grossly oversimpliﬁes the pattern classiﬁcation procedure. Data may un- dergo several separate transformation stages before a ﬁnal outcome is reached. These transformations  sometimes termed preprocessing, feature selection or feature extraction  operate on the data in a way that usually reduces its dimension  reduces the number of features , removing redundant or irrelevant information, and transforms it to a form more appropriate for subsequent classiﬁcation. The term intrinsic dimensionality refers to the minimum number of variables required to capture the structure within the data. In the speech recognition example mentioned above, a preprocessing stage may be to transform the waveform to a frequency representation. This may be processed further to ﬁnd formants  peaks in the spectrum . This is a feature extraction process  taking a possible nonlinear combination of the original variables to form new variables . Feature selection is the process of selecting a subset of a given set of variables.  Terminology varies between authors. Sometimes the term ‘representation pattern’ is used for the vector of measurements made on a sensor  for example, optical imager, radar  with the term ‘feature pattern’ being reserved for the small set of variables obtained by transformation  by a feature selection or feature extraction process  of the original vector of measurements. In some problems, measurements may be made directly on the feature vector itself. In these situations there is no automatic feature selection stage, with the feature selection being performed by the investigator who ‘knows’  through experience, knowledge of previous studies and the problem domain  those variables that are important for classiﬁcation. In many cases, however, it will be necessary to perform one or more transformations of the measured data.  In some pattern classiﬁers, each of the above stages may be present and identiﬁable as separate operations, while in others they may not be. Also, in some classiﬁers, the preliminary stages will tend to be problem-speciﬁc, as in the speech example. In this book, we consider feature selection and extraction transformations that are not application- speciﬁc. That is not to say all will be suitable for any given application, however, but application-speciﬁc preprocessing must be left to the investigator.  1.2 Stages in a pattern recognition problem  A pattern recognition investigation may consist of several stages, enumerated below. Further details are given in Appendix D. Not all stages may be present; some may be merged together so that the distinction between two operations may not be clear, even if both are carried out; also, there may be some application-speciﬁc data processing that may not be regarded as one of the stages listed. However, the points below are fairly typical.   4 Introduction to statistical pattern recognition  1. Formulation of the problem: gaining a clear understanding of the aims of the investi-  gation and planning the remaining stages.  2. Data collection: making measurements on appropriate variables and recording details  of the data collection procedure  ground truth .  3. Initial examination of the data: checking the data, calculating summary statistics and  producing plots in order to get a feel for the structure.  4. Feature selection or feature extraction: selecting variables from the measured set that are appropriate for the task. These new variables may be obtained by a linear or nonlinear transformation of the original set  feature extraction . To some extent, the division of feature extraction and classiﬁcation is artiﬁcial.  5. Unsupervised pattern classiﬁcation or clustering. This may be viewed as exploratory data analysis and it may provide a successful conclusion to a study. On the other hand, it may be a means of preprocessing the data for a supervised classiﬁcation procedure.  6. Apply discrimination or regression procedures as appropriate. The classiﬁer is de-  signed using a training set of exemplar patterns.  7. Assessment of results. This may involve applying the trained classiﬁer to an indepen-  dent test set of labelled patterns.  8. Interpretation.  The above is necessarily an iterative process: the analysis of the results may pose further hypotheses that require further data collection. Also, the cycle may be terminated at different stages: the questions posed may be answered by an initial examination of the data or it may be discovered that the data cannot answer the initial question and the problem must be reformulated.  The emphasis of this book is on techniques for performing steps 4, 5 and 6.  1.3 Issues  The main topic that we address in this book concerns classiﬁer design: given a training set of patterns of known class, we seek to design a classiﬁer that is optimal for the expected operating conditions  the test conditions .  There are a number of very important points to make about the sentence above, straightforward as it seems. The ﬁrst is that we are given a ﬁnite design set. If the classiﬁer is too complex  there are too many free parameters  it may model noise in the design set. This is an example of over-ﬁtting. If the classiﬁer is not complex enough, then it may fail to capture structure in the data. An example of this is the ﬁtting of a set of data points by a polynomial curve. If the degree of the polynomial is too high, then, although the curve may pass through or close to the data points, thus achieving a low ﬁtting error, the ﬁtting curve is very variable and models every ﬂuctuation in the data   Supervised versus unsupervised 5   due to noise . If the degree of the polynomial is too low, the ﬁtting error is large and the underlying variability of the curve is not modelled.  Thus, achieving optimal performance on the design set  in terms of minimising some error criterion perhaps  is not required: it may be possible, in a classiﬁcation problem, to achieve 100% classiﬁcation accuracy on the design set but the generalisation perfor- mance – the expected performance on data representative of the true operating conditions  equivalently, the performance on an inﬁnite test set of which the design set is a sam- ple  – is poorer than could be achieved by careful design. Choosing the ‘right’ model is an exercise in model selection.  In practice we usually do not know what is structure and what is noise in the data. Also, training a classiﬁer  the procedure of determining its parameters  should not be considered as a separate issue from model selection, but it often is.  A second point about the design of optimal classiﬁers concerns the word ‘optimal’. There are several ways of measuring classiﬁer performance, the most common being error rate, although this has severe limitations. Other measures, based on the closeness of the estimates of the probabilities of class membership to the true probabilities, may be more appropriate in many cases. However, many classiﬁer design methods usually optimise alternative criteria since the desired ones are difﬁcult to optimise directly. For example, a classiﬁer may be trained by optimising a squared error measure and assessed using error rate.  Finally, we assume that the training data are representative of the test conditions. If this is not so, perhaps because the test conditions may be subject to noise not present in the training data, or there are changes in the population from which the data are drawn  population drift , then these differences must be taken into account in classiﬁer design.  1.4 Supervised versus unsupervised  There are two main divisions of classiﬁcation: supervised classiﬁcation  or discrimina- tion  and unsupervised classiﬁcation  sometimes in the statistics literature simply referred to as classiﬁcation or clustering .  In supervised classiﬁcation we have a set of data samples  each consisting of mea- surements on a set of variables  with associated labels, the class types. These are used as exemplars in the classiﬁer design.  Why do we wish to design an automatic means of classifying future data? Cannot the same method that was used to label the design set be used on the test data? In some cases this may be possible. However, even if it were possible, in practice we may wish to develop an automatic method to reduce labour-intensive procedures. In other cases, it may not be possible for a human to be part of the classiﬁcation process. An example of the former is in industrial inspection. A classiﬁer can be trained using images of components on a production line, each image labelled carefully by an operator. However, in the practical application we would wish to save a human operator from the tedious job, and hopefully make it more reliable. An example of the latter reason for performing a classiﬁcation automatically is in radar target recognition of objects. For   6 Introduction to statistical pattern recognition  vehicle recognition, the data may be gathered by positioning vehicles on a turntable and making measurements from all aspect angles. In the practical application, a human may not be able to recognise an object reliably from its radar image, or the process may be carried out remotely.  In unsupervised classiﬁcation, the data are not labelled and we seek to ﬁnd groups in the data and the features that distinguish one group from another. Clustering techniques, described further in Chapter 10, can also be used as part of a supervised classiﬁcation scheme by deﬁning prototypes. A clustering scheme may be applied to the data for each class separately and representative samples for each group within the class  the group means, for example  used as the prototypes for that class.  1.5 Approaches to statistical pattern recognition  The problem we are addressing in this book is primarily one of pattern classiﬁca- tion. Given a set of measurements obtained through observation and represented as a pattern vector x, we wish to assign the pattern to one of C possible classes !i , i D 1; : : : ; C. A decision rule partitions the measurement space into C regions  cid:6 i , i D 1; : : : ; C. If an observation vector is in  cid:6 i then it is assumed to belong to class !i . Each region may be multiply connected – that is, it may be made up of several disjoint regions. The boundaries between the regions  cid:6 i are the decision boundaries or decision surfaces. Generally, it is in regions close to these boundaries that the high- est proportion of misclassiﬁcations occurs. In such situations, we may reject the pat- tern or withhold a decision until further information is available so that a classiﬁcation may be made later. This option is known as the reject option and therefore we have C C 1 outcomes of a decision rule  the reject option being denoted by !0  in a C-class problem.  In this section we introduce two approaches to discrimination that will be explored further in later chapters. The ﬁrst assumes a knowledge of the underlying class-conditional probability density functions  the probability density function of the feature vectors for a given class . Of course, in many applications these will usually be unknown and must be estimated from a set of correctly classiﬁed samples termed the design or training set. Chapters 2 and 3 describe techniques for estimating the probability density functions explicitly.  The second approach introduced in this section develops decision rules that use the data to estimate the decision boundaries directly, without explicit calculation of the probability density functions. This approach is developed in Chapters 4, 5 and 6 where speciﬁc techniques are described.  1.5.1 Elementary decision theory  Here we introduce an approach to discrimination based on knowledge of the probability density functions of each class. Familiarity with basic probability theory is assumed. Some basic deﬁnitions are given in Appendix E.   Approaches to statistical pattern recognition 7  Bayes decision rule for minimum error Consider C classes, !1; : : : ; !C , with a priori probabilities  the probabilities of each class occurring  p.!1 ; : : : ; p.!C  , assumed known. If we wish to minimise the probability of making an error and we have no information regarding an object other than the class probability distribution then we would assign an object to class ! j if  p.! j   > p.!k    k D 1; : : : ; C; k 6D j  This classiﬁes all objects as belonging to one class. For classes with equal probabilities, patterns are assigned arbitrarily between those classes.  However, we do have an observation vector or measurement vector x and we wish to assign x to one of the C classes. A decision rule based on probabilities is to assign x to class ! j if the probability of class ! j given the observation x, p.! jjx , is greatest over all classes !1; : : : ; !C. That is, assign x to class ! j if  p.! jjx  > p.!kjx   k D 1; : : : ; C; k 6D j   1.1   This decision rule partitions the measurement space into C regions  cid:6 1; : : : ;  cid:6 C such that if x 2  cid:6  j then x belongs to class ! j . The a posteriori probabilities p.! jjx  may be expressed in terms of the a priori probabilities and the class-conditional density functions p.xj!i   using Bayes’ theorem  see Appendix E  as  p.!ijx  D p.xj!i   p.!i    p.x   and so the decision rule  1.1  may be written: assign x to ! j if  p.xj! j   p.! j   > p.xj!k   p.!k    k D 1; : : : ; C; k 6D j   1.2   This is known as Bayes’ rule for minimum error.  For two classes, the decision rule  1.2  may be written  lr .x  D p.xj!1  p.xj!2   >  p.!2  p.!1   implies x 2 class !1  The function lr .x  is the likelihood ratio. Figures 1.2 and 1.3 give a simple illustration for a two-class discrimination problem. Class !1 is normally distributed with zero mean and unit variance, p.xj!1  D N .xj0; 1   see Appendix E . Class !2 is a normal mixture  a weighted sum of normal densities  p.xj!2  D 0:6N .xj1; 1 C0:4N .xj cid:8 1; 2 . Figure 1.2 plots p.xj!i   p.!i  ; i D 1; 2, where the priors are taken to be p.!1  D 0:5, p.!2  D 0:5. Figure 1.3 plots the likelihood ratio lr .x  and the threshold p.!2 = p.!1 . We see from this ﬁgure that the decision rule  1.2  leads to a disjoint region for class !2.  The fact that the decision rule  1.2  minimises the error may be seen as follows. The  probability of making an error, p.error , may be expressed as  p.error  D CX iD1  p.errorj!i   p.!i     1.3    0.2  0.18  0.16  0.14  0.12  0.1  0.08  0.06  0.04  0.02  0  2  1.8  1.6  1.4  1.2  1  0.8  0.6  0.4  0.2  0  8 Introduction to statistical pattern recognition  p xw1 p w1   p xw2 p w2   −4  −3  −2  −1  1  2  3  4  Figure 1.2 p.xj!i p.!i , for classes !1 and !2  0 x  0 x  p w2  p w1   lr  x   −4  −3  −2  −1  1  2  3  4  Figure 1.3 Likelihood function  where p.errorj!i   is the probability of misclassifying patterns from class !i . This is given by  p.errorj!i   D  p.xj!i   dx  Z  C[ cid:6 i ]   1.4   the integral of the class-conditional density function over C[ cid:6 i ], the region of measure- ment space outside  cid:6 i  C is the complement operator , i.e. PC  cid:6  j . Therefore, we  jD1; j6Di   may write the probability of misclassifying a pattern as  Approaches to statistical pattern recognition 9  Z  C[ cid:6 i ]  p.error  D CX iD1 D CX p.!i   iD1 D 1  cid:8  CX iD1  Z  p.xj!i   p.!i   dx  cid:4  1  cid:8  Z   cid:6 i  p.xj!i   dx  p.xj!i   dx  p.!i     cid:5    cid:6 i  p.!i    p.xj!i   dx  Z   cid:6 i  CX iD1  Z  c D  p.!i   p.xj!i   dx  max  i  Z  eB D 1  cid:8   p.!i   p.xj!i   dx  max  i  from which we see that minimising the probability of making an error is equivalent to maximising  the probability of correct classiﬁcation. Therefore, we wish to choose the regions  cid:6 i so that the integral given in  1.6  is a maximum. This is achieved by selecting  cid:6 i to be the region for which p.!i   p.xj!i   is the largest over all classes and the probability of correct classiﬁcation, c, is  where the integral is over the whole of the measurement space, and the Bayes error is   1.5    1.6    1.7    1.8   This is illustrated in Figures 1.4 and 1.5. Figure 1.4 plots the two distributions p.xj!i  ; i D 1; 2  both normal with unit variance and means š0:5 , and Figure 1.5 plots the functions p.xj!i   p.!i   where p.!1  D 0:3, p.!2  D 0:7. The Bayes deci- sion boundary is marked with a vertical line at x B. The areas of the hatched regions in Figure 1.4 represent the probability of error: by equation  1.4 , the area of the horizontal hatching is the probability of classifying a pattern from class 1 as a pattern from class 2 and the area of the vertical hatching the probability of classifying a pattern from class 2 as class 1. The sum of these two areas, weighted by the priors  equation  1.5  , is the probability of making an error.  Bayes decision rule for minimum error – reject option As we have stated above, an error or misrecognition occurs when the classiﬁer assigns a pattern to one class when it actually belongs to another. In this section we consider the reject option. Usually it is the uncertain classiﬁcations which mainly contribute to the error rate. Therefore, rejecting a pattern  withholding a decision  may lead to a reduction in the error rate. This rejected pattern may be discarded, or set aside until further information allows a decision to be made. Although the option to reject may alleviate or remove the problem of a high misrecognition rate, some otherwise correct   10 Introduction to statistical pattern recognition  p xw1   p xw2   0.4  0.35  0.3  0.25  0.2  0.15  0.1  0.05  0.3  0.25  0.2  0.15  0.1  0.05  0  −4  −3  −2  −1 xB  0  1  2  3  4  Figure 1.4 Class-conditional densities for two normal distributions  p xw2 p w2   p xw1 p w1   0  −4  −3  −2  −1 xB  0  1  2  3  4  Figure 1.5 Bayes decision boundary for two normally distributed classes with unequal priors  classiﬁcations are also converted into rejects. Here we consider the trade-offs between error rate and reject rate.  Firstly, we partition the sample space into two complementary regions: R, a reject  region, and A, an acceptance or classiﬁcation region. These are deﬁned by  R D n A D n  xj1  cid:8  max xj1  cid:8  max  i  i  o p.!ijx  > t   o p.!ijx   cid:10  t     Approaches to statistical pattern recognition 11  t  1 − t  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  p w1x   p w2x   −4  −2  −3  A  −1 R  0  1  3  4  2  A  Figure 1.6 Illustration of acceptance and reject regions  where t is a threshold. This is illustrated in Figure 1.6 using the same distributions as those in Figures 1.4 and 1.5. The smaller the value of the threshold t, the larger is the reject region R. However, if t is chosen such that  or equivalently,  1  cid:8  t <  1 C  C  cid:8  1 C  t >  where C is the number of classes, then the reject region is empty. This is because the minimum value which maxi p.!ijx  can attain is 1=C  since 1 D PC iD1 p.!ijx   cid:10  C maxi p.!ijx  , when all classes are equally likely. Therefore, for the reject option to be activated, we must have t  cid:10  .C  cid:8  1 =C.  Thus, if a pattern x lies in the region A, we classify it according to the Bayes rule  for minimum error  equation  1.2  . However, if x lies in the region R, we reject x.  The probability of correct classiﬁcation, c.t  , is a function of the threshold, t, and is given by equation  1.7 , where now the integral is over the acceptance region, A, only  Z  c.t   D  max  i  A  ð p.!i   p.xj!i  Ł dx  and the unconditional probability of rejecting a measurement x, r, also a function of the threshold t, is  r .t   D  p.x  dx  Z  R   1.9    12 Introduction to statistical pattern recognition  Therefore, the error rate, e  the probability of accepting a point for classiﬁcation and incorrectly classifying it , is  Z  e.t   D  .1  cid:8  max D 1  cid:8  c.t    cid:8  r .t    A  i  p.!ijx   p.x  dx  Thus, the error rate and reject rate are inversely related. Chow  1970  derives a simple functional relationship between e.t   and r .t   which we quote here without proof. Know- ing r .t   over the complete range of t allows e.t   to be calculated using the relationship  e.t   D  cid:8   s dr .s   Z t  0   1.10   The above result allows the error rate to be evaluated from the reject function for the Bayes optimum classiﬁer. The reject function can be calculated using unlabelled data and a practical application is to problems where labelling of gathered data is costly.  Bayes decision rule for minimum risk In the previous section, the decision rule selected the class for which the a posteriori probability, p.! jjx , was the greatest. This minimised the probability of making an error. We now consider a somewhat different rule that minimises an expected loss or risk. This is a very important concept since in many applications the costs associated with misclassiﬁcation depend upon the true class of the pattern and the class to which it is assigned. For example, in a medical diagnosis problem in which a patient has back pain, it is far worse to classify a patient with severe spinal abnormality as healthy  or having mild back ache  than the other way round.  We make this concept more formal by introducing a loss that is a measure of the cost of making the decision that a pattern belongs to class !i when the true class is ! j . We deﬁne a loss matrix  cid:2  with components  ½ ji D cost of assigning a pattern x to !i when x 2 ! j  In practice, it may be very difﬁcult to assign costs. In some situations, ½ may be measured in monetary units that are quantiﬁable. However, in many situations, costs are a combi- nation of several different factors measured in different units – money, time, quality of life. As a consequence, they may be the subjective opinion of an expert. The conditional risk of assigning a pattern x to class !i is deﬁned as ½ ji p.! jjx   li .x  D CX jD1  The average risk over region  cid:6 i is Z  r i D  li .x  p.x  dx CX jD1   cid:6 i Z   cid:6 i  D  ½ ji p.! jjx  p.x  dx   Approaches to statistical pattern recognition 13  and the overall expected cost or risk is Z  r D CX iD1  r i D CX iD1  CX jD1   cid:6 i  ½ ji p.! jjx  p.x  dx   1.11   The above expression for the risk will be minimised if the regions  cid:6 i are chosen such that if  CX jD1  ½ ji p.! jjx  p.x   cid:10  CX jD1  ½ jk p.! jjx  p.x   k D 1; : : : ; C   1.12   then x 2  cid:6 i . This is the Bayes decision rule for minimum risk, with Bayes risk, rŁ, given by  Z  rŁ D  min iD1;:::;C  x  CX jD1  ½ ji p.! jjx  p.x  dx  One special case of the loss matrix  cid:2  is the equal cost loss matrix for which  ½i j D  ² 1 i 6D j 0 i D j  Substituting into  1.12  gives the decision rule: assign x to class !i if CX jD1  p.! jjx  p.x   cid:8  p.!ijx  p.x   cid:10  CX jD1  p.! jjx  p.x   cid:8  p.!kjx  p.x   k D 1; : : : ; C  that is,  k D 1; : : : ; C implies that x 2 class !i ; this is the Bayes rule for minimum error.  p.xj!i   p.!i   ½ p.xj!k  p.!k    Bayes decision rule for minimum risk – reject option As with the Bayes rule for minimum error, we may also introduce a reject option, by which the reject region, R, is deﬁned by  where t is a threshold. The decision is to accept a pattern x and assign it to class !i if  R D n  x  þþþ min  i  o  li .x  > t  li .x  D min  l j .x   cid:10  t  li .x  D min  l j .x  > t  j  j  and to reject x if   14 Introduction to statistical pattern recognition  This decision is equivalent to deﬁning a reject region  cid:6 0 with a constant conditional risk  l0.x  D t  so that the Bayes decision rule is: assign x to class !i if  li .x   cid:10  l j .x   j D 0; 1; : : : ; C  with Bayes risk  rŁ D  t p.x  dx C  Z  R  Z  min iD1;:::;C  A  CX jD1  ½ ji p.! jjx  p.x  dx   1.13   Neyman–Pearson decision rule An alternative to the Bayes decision rules for a two-class problem is the Neyman–Pearson test. In a two-class problem there are two possible types of error that may be made in the decision process. We may classify a pattern of class !1 as belonging to class !2 or a pattern from class !2 as belonging to class !1. Let the probability of these two errors be ž1 and ž2 respectively, so that  ž1 D  p.xj!1  dx D error probability of Type I  and  ž2 D  p.xj!2  dx D error probability of Type II  Z  Z   cid:6 2   cid:6 1  The Neyman–Pearson decision rule is to minimise the error ž1 subject to ž2 being equal to a constant, ž0, say.  If class !1 is termed the positive class and class !2 the negative class, then ž1 is referred to as the false negative rate, the proportion of positive samples incorrectly assigned to the negative class; ž2 is the false positive rate, the proportion of negative samples classed as positive.  An example of the use of the Neyman–Pearson decision rule is in radar detection where the problem is to detect a signal in the presence of noise. There are two types of error that may occur; one is to mistake noise for a signal present. This is called a false alarm. The second type of error occurs when a signal is actually present but the decision is made that only noise is present. This is a missed detection. If !1 denotes the signal class and !2 denotes the noise then ž2 is the probability of false alarm and ž1 is the probability of missed detection. In many radar applications, a threshold is set to give a ﬁxed probability of false alarm and therefore the Neyman–Pearson decision rule is the one usually used.  We seek the minimum of  Z  r D   cid:6 2  p.xj!1  dx C ¼  p.xj!2  dx  cid:8  ž0  ¦  ²Z   cid:6 1   Approaches to statistical pattern recognition 15  where ¼ is a Lagrange multiplier1 and ž0 is the speciﬁed false alarm rate. The equation may be written  r D .1  cid:8  ¼ž0  C  f¼p.xj!2  dx  cid:8  p.xj!1  dxg  Z   cid:6 1  This will be minimised if we choose  cid:6 1 such that the integrand is negative, i.e.  if ¼p.xj!2   cid:8  p.xj!1  < 0;  then x 2  cid:6 1  or, in terms of the likelihood ratio,  p.xj!1  p.xj!2   if  > ¼;  then x 2  cid:6 1   1.14   Thus the decision rule depends only on the within-class distributions and ignores the a priori probabilities.  The threshold ¼ is chosen so that  Z   cid:6 1  p.xj!2  dx D ž0;  the speciﬁed false alarm rate. However, in general ¼ cannot be determined analytically and requires numerical calculation.   cid:6 1  is, the probability of detection  1 cid:8  ž1 D R alarm  ž2 D R  Often, the performance of the decision rule is summarised in a receiver operating characteristic  ROC  curve, which plots the true positive against the false positive  that p.xj!1  dx  against the probability of false p.xj!2  dx   as the threshold ¼ is varied. This is illustrated in Figure 1.7 for the univariate case of two normally distributed classes of unit variance and means separated by a distance, d. All the ROC curves pass through the .0; 0  and .1; 1  points and as the separation increases the curve moves into the top left corner. Ideally, we would like 100% detection for a 0% false alarm rate; the closer a curve is to this the better. decision rules on the basis of the likelihood ratio  ½ii D 0 :  For the two-class case, the minimum risk decision  see equation  1.12   deﬁnes the   cid:6 1  p.xj!1  p.xj!2   if  >  ½21 p.!2  ½12 p.!1   ;  then x 2  cid:6 1   1.15   The threshold deﬁned by the right-hand side will correspond to a particular point on the ROC curve that depends on the misclassiﬁcation costs and the prior probabilities.  In practice, precise values for the misclassiﬁcation costs will be unavailable and we shall need to assess the performance over a range of expected costs. The use of the ROC curve as a tool for comparing and assessing classiﬁer performance is discussed in Chapter 8.  1The method of Lagrange’s undetermined multipliers can be found in most textbooks on mathematical  methods, for example Wylie and Barrett  1995 .   16 Introduction to statistical pattern recognition  d = 4  d = 2  d = 1  1 −  ∋  1  1  0.8  0.6  0.4  0.2  0  0  0.2  0.4  0.6  0.8  1  2∋  Figure 1.7 Receiver operating characteristic for two univariate normal distributions of unit vari- p.xj!1  dx is the true positive  the probability of detection   ance and separation d; 1  cid:8  ž1 D R and ž2 D R  p.xj!2  dx is the false positive  the probability of false alarm    cid:6 1   cid:6 1  Minimax criterion The Bayes decision rules rely on a knowledge of both the within-class distributions and the prior class probabilities. However, situations may arise where the relative frequencies of new objects to be classiﬁed are unknown. In this situation a minimax procedure may be employed. The name minimax is used to refer to procedures for which either the maximum expected loss or the maximum of the error probability is a minimum. We shall limit our discussion below to the two-class problem and the minimum error probability procedure.  Consider the Bayes rule for minimum error. The decision regions  cid:6 1 and  cid:6 2 are  deﬁned by  p.xj!1  p.!1  > p.xj!2  p.!2  implies x 2  cid:6 1  and the Bayes minimum error, eB, is  eB D p.!2   p.xj!2  dx C p.!1   p.xj!1  dx  Z   cid:6 1  Z   cid:6 2  where p.!2  D 1  cid:8  p.!1 . For ﬁxed decision regions  cid:6 1 and  cid:6 2, eB is a linear function of p.!1   we denote this function QeB  attaining its maximum on the region [0; 1] either at p.!1  D 0 or p.!1  D 1. However, since the regions  cid:6 1 and  cid:6 2 are also dependent on p.!1  through the Bayes decision criterion  1.16 , the dependency of eB on p.!1  is more complex, and not necessarily monotonic.  If  cid:6 1 and  cid:6 2 are ﬁxed  determined according to  1.16  for some speciﬁed p.!i   , the error given by  1.17  will only be the Bayes minimum error for a particular value of p.!1 , say pŁ 1  see Figure 1.8 . For other values of p.!1 , the error given by  1.17    1.16    1.17    Approaches to statistical pattern recognition 17  must be greater than the minimum error. Therefore, the optimum curve touches the line at a tangent at pŁ  1 and is concave down at that point.  The minimax procedure aims to choose the partition  cid:6 1,  cid:6 2, or equivalently the value of p.!1  so that the maximum error  on a test set in which the values of p.!i   are unknown  is minimised. For example, in the ﬁgure, if the partition were chosen to correspond to the value pŁ 1 of p.!1 , then the maximum error which could occur would be a value of b if p.!1  were actually equal to unity. The minimax procedure aims to minimise this maximum value, i.e. minimise  maxfQeB .0 ; QeB .1 g  Z   cid:6 1  Z   cid:6 1  or minimise  ²Z  max   cid:6 2  p.xj!1  dx;  p.xj!2  dx  ¦  This is a minimum when  Z   cid:6 2  p.xj!1  dx D  p.xj!2  dx   1.18   which is when a D b in Figure 1.8 and the line QeB . p.!1   is horizontal and touches the Bayes minimum error curve at its peak value.  Therefore, we choose the regions  cid:6 1 and  cid:6 2 so that the probabilities of the two types of error are the same. The minimax solution may be criticised as being over-pessimistic since it is a Bayes solution with respect to the least favourable prior distribution. The  b  a  ~  error, eB, for fixed decision regions  Bayes minimum error, eB  0.0  * p1  p w1   1.0  Figure 1.8 Minimax illustration   18 Introduction to statistical pattern recognition  Z  strategy may also be applied to minimising the maximum risk. In this case, the risk is [½12 p.!1jx  C ½22 p.!2jx ] p.x  dx ½  [½11 p.!1jx  C ½21 p.!2jx ] p.x  dx C  Z   cid:6 2   cid:6 1  D p.!1   C p.!2   Z   cid:14  ½11 C .½12  cid:8  ½11   cid:14  ½22 C .½21  cid:8  ½22    cid:6 2 Z  p.xj!1  dx  ½  p.xj!2  dx   cid:6 1  and the boundary must therefore satisfy  Z  ½11  cid:8  ½22 C .½12  cid:8  ½11   p.xj!1  dx  cid:8  .½21  cid:8  ½22  For ½11 D ½22 and ½21 D ½12, this reduces to condition  1.18 .   cid:6 2  Z   cid:6 1  p.xj!2  dx D 0  Discussion In this section we have introduced a decision-theoretic approach to classifying patterns. This divides up the measurement space into decision regions and we have looked at various strategies for obtaining the decision boundaries. The optimum rule in the sense of minimising the error is the Bayes decision rule for minimum error. Introducing the costs of making incorrect decisions leads to the Bayes rule for minimum risk. The theory developed assumes that the a priori distributions and the class-conditional distributions are known. In a real-world task, this is unlikely to be so. Therefore approximations must be made based on the data available. We consider techniques for estimating distribu- tions in Chapters 2 and 3. Two alternatives to the Bayesian decision rule have also been described, namely the Neyman–Pearson decision rule  commonly used in signal process- ing applications  and the minimax rule. Both require knowledge of the class-conditional probability density functions. The receiver operating characteristic curve characterises the performance of a rule over a range of thresholds of the likelihood ratio.  We have seen that the error rate plays an important part in decision-making and classiﬁer performance assessment. Consequently, estimation of error rates is a problem of great interest in statistical pattern recognition. For given ﬁxed decision regions, we may calculate the probability of error using  1.5 . If these decision regions are chosen according to the Bayes decision rule  1.2 , then the error is the Bayes error rate or optimal error rate. However, regardless of how the decision regions are chosen, the error rate may be regarded as a measure of a given decision rule’s performance.  The Bayes error rate  1.5  requires complete knowledge of the class-conditional den- sity functions. In a particular situation, these may not be known and a classiﬁer may be designed on the basis of a training set of samples. Given this training set, we may choose to form estimates of the distributions  using some of the techniques discussed in Chapters 2 and 3  and thus, with these estimates, use the Bayes decision rule and estimate the error according to  1.5 .  However, even with accurate estimates of the distributions, evaluation of the error requires an integral over a multidimensional space and may prove a formidable task. An alternative approach is to obtain bounds on the optimal error rate or distribution-free estimates. Further discussion of methods of error rate estimation is given in Chapter 8.   Approaches to statistical pattern recognition 19  1.5.2 Discriminant functions  In the previous subsection, classiﬁcation was achieved by applying the Bayesian decision rule. This requires knowledge of the class-conditional density functions, p.xj!i    such as normal distributions whose parameters are estimated from the data – see Chapter 2 , or nonparametric density estimation methods  such as kernel density estimation – see Chapter 3 . Here, instead of making assumptions about p.xj!i  , we make assumptions about the forms of the discriminant functions.  A discriminant function is a function of the pattern x that leads to a classiﬁcation rule. For example, in a two-class problem, a discriminant function h.x  is a function for which  for constant k. In the case of equality  h.x  D k , the pattern x may be assigned arbitrarily to one of the two classes. An optimal discriminant function for the two-class case is   1.19   h.x  > k   x 2 !1 < k   x 2 !2  h.x  D p.xj!1  p.xj!2   with k D p.!2 = p.!1 . Discriminant functions are not unique. If function then  f  is a monotonic  g.x  D f .h.x   > k0   x 2 !1 g.x  D f .h.x   < k0   x 2 !2  where k0 D f .k  leads to the same decision as  1.19 .  In the C-group case we deﬁne C discriminant functions gi .x  such that  gi .x  > g j .x    x 2 !i  j D 1; : : : ; C;  j 6D i  That is, a pattern is assigned to the class with the largest discriminant. Of course, for two classes, a single discriminant function  with k D 0 reduces to the two-class case given by  1.19 .  Again, we may deﬁne an optimal discriminant function as  h.x  D g1.x   cid:8  g2.x   gi .x  D p.xj!i   p.!i    leading to the Bayes decision rule, but as we showed for the two-class case, there are other discriminant functions that lead to the same decision.  The essential difference between the approach of the previous subsection and the discriminant function approach described here is that the form of the discriminant function is speciﬁed and is not imposed by the underlying distribution. The choice of discriminant function may depend on prior knowledge about the patterns to be classiﬁed or may be a   20 Introduction to statistical pattern recognition  particular functional form whose parameters are adjusted by a training procedure. Many different forms of discriminant function have been considered in the literature, varying in complexity from the linear discriminant function  in which g is a linear combination of the xi   to multiparameter nonlinear functions such as the multilayer perceptron.  Discrimination may also be viewed as a problem in regression  see Section 1.6  in which the dependent variable, y, is a class indicator and the regressors are the pattern vectors. Many discriminant function models lead to estimates of E[yjx], which is the aim of regression analysis  though in regression y is not necessarily a class indicator . Thus, many of the techniques we shall discuss for optimising discriminant functions apply equally well to regression problems. Indeed, as we ﬁnd with feature extraction in Chapter 9 and also clustering in Chapter 10, similar techniques have been developed under different names in the pattern recognition and statistics literature.  Linear discriminant functions First of all, let us consider the family of discriminant functions that are linear combina- tions of the components of x D .x1; : : : ; x p T ,  g.x  D wT x C w0 D pX iD1  wi xi C w0   1.20   This is a linear discriminant function, a complete speciﬁcation of which is achieved by prescribing the weight vector w and threshold weight w0. Equation  1.20  is the equation of a hyperplane with unit normal in the direction of w and a perpendicular distance jw0j=jwj from the origin. The value of the discriminant function for a pattern x is a measure of the perpendicular distance from the hyperplane  see Figure 1.9 .  A linear discriminant function can arise through assumptions of normal distributions for the class densities, with equal covariance matrices  see Chapter 2 . Alternatively,  g x   g > 0  origin  g < 0  hyperplane, g = 0  Figure 1.9 Geometry of linear discriminant function given by equation  1.20    Approaches to statistical pattern recognition 21  without making distributional assumptions, we may require the form of the discriminant function to be linear and determine its parameters  see Chapter 4 .  A pattern classiﬁer employing linear discriminant functions is termed a linear machine  Nilsson, 1965 , an important special case of which is the minimum-distance classiﬁer or nearest-neighbour rule. Suppose we are given a set of prototype points p1; : : : ; pC , one for each of the C classes !1; : : : ; !C . The minimum-distance classiﬁer assigns a pattern x to the class !i associated with the nearest point pi . For each point, the squared Euclidean distance is  jx  cid:8  pij2 D xT x  cid:8  2xT pi C pT  i pi  and minimum-distance classiﬁcation is achieved by comparing the expressions xT pi  cid:8  1 2 pT  i pi and selecting the largest value. Thus, the linear discriminant function is  where  gi .x  D wT  i x C wi0  wi D pi wi0 D  cid:8  1  2jpij2  Therefore, the minimum-distance classiﬁer is a linear machine. If the prototype points, pi , are the class means, then we have the nearest class mean classiﬁer. Decision re- gions for a minimum-distance classiﬁer are illustrated in Figure 1.10. Each boundary is the perpendicular bisector of the lines joining the prototype points of regions that are contiguous. Also, note from the ﬁgure that the decision regions are convex  that is, two arbitrary points lying in the region can be joined by a straight line that lies entirely within the region . In fact, decision regions of a linear machine are always convex. Thus, the two class problems, illustrated in Figure 1.11, although separable, cannot be separated by a linear machine. Two generalisations that overcome this difﬁculty are piecewise linear discriminant functions and generalised linear discriminant functions.  Piecewise linear discriminant functions This is a generalisation of the minimum-distance classiﬁer to the situation in which there is more than one prototype per class. Suppose there are ni prototypes in class !i , p1 i  ; i D 1; : : : ; C. We deﬁne the discriminant function for class !i to be  ; : : : ; p  ni i  gi .x  D max jD1;:::;ni  g j i  .x   where g j i  is a subsidiary discriminant function, which is linear and is given by  .x  D xT p  j  i  cid:8  1 2 p  j i  T  p  j i  g j i  j D 1; : : : ; ni ; i D 1; : : : ; C  A pattern x is assigned to the class for which gi .x  is largest; that is, to the class of the nearest prototype vector. This partitions the space into PC iD1 ni regions known as   22 Introduction to statistical pattern recognition  ž  p1   cid:2    cid:2    cid:2    cid:2    cid:2    cid:2    cid:2    cid:2  cid:3  cid:3  cid:3  cid:3  cid:3  cid:3  cid:3  cid:3   ž  p2  ž  p3   cid:5    cid:5    cid:5    cid:5    cid:5   ž  p4   cid:5    cid:5    cid:5   cid:4   cid:4    cid:4   cid:4   cid:4    cid:4  cid:4   Figure 1.10 Decision regions for a minimum-distance classiﬁer  Š Š Š Š  Š  Š Š Š Š žž ž ž ž ž ž  Š Š Š  Š Š ž ž  ž   a   žž ž  ž  ž  ž  ž ž  ž žž ž ž  Š Š Š  Š Š   b   Figure 1.11 Groups not separable by a linear discriminant  the Dirichlet tessellation of the space. When each pattern in the training set is taken as a prototype vector, then we have the nearest-neighbour decision rule of Chapter 3. This discriminant function generates a piecewise linear decision boundary  see Figure 1.12 . Rather than using the complete design set as prototypes, we may use a subset. Methods of reducing the number of prototype vectors  edit and condense  are described in Chapter 3, along with the nearest-neighbour algorithm. Clustering schemes may also be employed.  Generalised linear discriminant function A generalised linear discriminant function, also termed a phi machine  Nilsson, 1965 , is a discriminant function of the form  g.x  D wT φ C w0  where φ D . cid:14 1.x ; : : : ; φ D.x  T is a vector function of x. If D D p, the number of variables, and  cid:14 i .x  D xi , then we have a linear discriminant function.  The discriminant function is linear in the functions  cid:14 i , not in the original measure- ments xi . As an example, consider the two-class problem of Figure 1.13. A linear dis- criminant function will not separate the classes, even though they are separable. However,   Approaches to statistical pattern recognition 23   cid:6    cid:6    cid:6   ž   cid:6    cid:6   ž   cid:6  cid:7  cid:7  cid:7  cid:7  ž  cid:2    cid:6    cid:6    cid:2   cid:3   cid:3   cid:3  cid:3    cid:2   Š   cid:1    cid:1   cid:2    cid:2  Š  cid:2    cid:2    cid:8  cid:8  cid:8   cid:2    cid:2    cid:2  Š   cid:7  cid:7  cid:7  cid:7  cid:7  cid:7   ž  ž   cid:7  cid:7  cid:7  cid:7  cid:7  cid:7    cid:1    cid:1   ž   cid:2    cid:1    cid:2    cid:1    cid:2    cid:2    cid:2    cid:2   Š   cid:1    cid:1    cid:1    cid:1    cid:2    cid:2    cid:2    cid:2   Figure 1.12 Dirichlet tessellation  comprising nearest-neighbour regions for a set of prototypes  and the decision boundary  thick lines  for two classes  ž  ž  x2  cid:9   ž  ž  ž  ž  Š  ž  Š  Š  Š  Š  Š  Š  Š  cid:1   Š  x1   cid:14 2  cid:9  ž ž  ž  Š  ž  ž   cid:7  cid:7  cid:7  cid:7  cid:7  cid:7  cid:7  cid:7  cid:7  cid:7  cid:7  cid:7   Š  cid:1   ž ž  Š  Š  Š  Š  Š  Š  Š   cid:14 1  Figure 1.13 Nonlinear transformation of variables may permit linear discrimination  if we make the transformation   cid:14 1.x  D x 2  cid:14 2.x  D x2  1  then the classes can be separated in the  cid:14 -space by a straight line. Similarly, disjoint classes can be transformed into a  cid:14 -space in which a linear discriminant function could separate the classes  provided that they are separable in the original space .  The problem, therefore, is simple. Make a good choice for the functions  cid:14 i .x , then use a linear discriminant function to separate the classes. But, how do we choose  cid:14 i ? Speciﬁc examples are shown in Table 1.1.  Clearly there is a problem in that as the number of functions that are used as a basis set increases, so does the number of parameters that must be determined using the limited   24 Introduction to statistical pattern recognition  Table 1.1 Discriminant functions,  cid:14   Discriminant function linear quadratic  ¹th-order polynomial  radial basis function multilayer perceptron  k1  xl2 k2  Mathematical form,  cid:14 i .x   cid:14 i .x  D xi , i D 1; : : : ; p  cid:14 i .x  D xl1 l1; l2 D 0 or 1; k1; k2 D 1; : : : ; p l1, l2 not both zero  cid:14 i .x  D xl1  , i D 1; : : : ; . p C 1 . p C 2 =2  cid:8  1  k¹ , i D 1; : : : ;  : : : xl¹ l1; : : : ; l¹ D 0 or 1; k1; : : : ; k¹ D 1; : : : ; p li not all zero   p C ¹   cid:14 i .x  D  cid:14  .jx  cid:8  vij  for centre vi and function  cid:14   cid:14 i .x  D f .xT vi C vi0  for direction vi and offset vi0. f is the logistic function, f .z  D 1=.1 C exp. cid:8 z     cid:8  1  !  k1  ¹  training set. A complete quadratic discriminant function requires D D . p C 1 . p C 2 =2 terms and so for C classes there are C. p C 1 . p C 2 =2 parameters to estimate. We may need to apply a constraint or ‘regularise’ the model to ensure that there is no over-ﬁtting. An alternative to having a set of different functions is to have a set of functions of  the same parametric form, but which differ in the values of the parameters they take,  where vi is a set of parameters. Different models arise depending on the way the variable x and the parameters v are combined. If  that is,  cid:14  is a function only of the magnitude of the difference between the pattern x and the weight vector v, then the resulting discriminant function is known as a radial basis function. On the other hand, if  cid:14  is a function of the scalar product of the two vectors   cid:14 i .x  D  cid:14  .x; vi     cid:14  .x; v  D  cid:14  .jx  cid:8  vj    cid:14  .x; v  D  cid:14  .xT v C v0   then the discriminant function is known as a multilayer perceptron. It is also a model known as projection pursuit. Both the radial basis function and the multilayer perceptron models can be used in regression.  In these latter examples, the discriminant function is no longer linear in the parameters. Speciﬁc forms for  cid:14  for radial basis functions and for the multilayer perceptron models will be given in Chapters 5 and 6.  Summary In a multiclass problem, a pattern x is assigned to the class for which the discriminant function is the largest. A linear discriminant function divides the feature space by a   Multiple regression 25  hyperplane whose orientation is determined by the weight vector w and distance from the origin by the weight threshold w0. The decision regions produced by linear discriminant functions are convex.  A piecewise linear discriminant function permits non-convex and disjoint decision  regions. Special cases are the nearest-neighbour and nearest class mean classiﬁer.  A generalised linear discriminant function, with ﬁxed functions  cid:14 i , is linear in its parameters. It permits non-convex and multiply connected decision regions  for suitable choices of  cid:14 i  . Radial basis functions and multilayer perceptrons can be regarded as generalised linear discriminant functions with ﬂexible functions  cid:14 i whose parameters must be determined or speciﬁed using the training set.  The Bayes decision rule is optimal  in the sense of minimising classiﬁcation error  and with sufﬁcient ﬂexibility in our discriminant functions we ought to be able to achieve optimal performance in principle. However, we are limited by a ﬁnite number of training samples and also, once we start to consider parametric forms for the  cid:14 i , we lose the simplicity and ease of computation of the linear functions.  1.6 Multiple regression  Many of the techniques and procedures described within this book are also relevant to problems in regression, the process of investigating the relationship between a depen- dent  or response  variable Y and independent  or predictor  variables X1; : : : ; X p; a regression function expresses the expected value of Y in terms of X1; : : : ; X p and model parameters. Regression is an important part of statistical pattern recognition and, although the emphasis of the book is on discrimination, practical illustrations are sometimes given on problems of a regression nature.  The discrimination problem itself is one in which we are attempting to predict the values of one variable  the class variable  given measurements made on a set of indepen- dent variables  the pattern vector, x . In this case, the response variable is categorical. Posing the discrimination problem as one in regression is discussed in Chapter 4.  Regression analysis is concerned with predicting the mean value of the response variable given measurements on the predictor variables and assumes a model of the form  Z  E[yjx]  4D  yp.yjx  dy D f .x; θ    where f is a  possibly nonlinear  function of the measurements x and θ, a set of param- eters of f . For example,  f .x; θ   D  cid:17 0 C θ T x  where θ D . cid:17 1; : : : ;  cid:17  p T , is a model that is linear in the parameters and the variables. The model  f .x; θ   D  cid:17 0 C θ T φ.x   where θ D . cid:17 1; : : : ;  cid:17 D T and φ D . cid:14 1.x ; : : : ;  cid:14 D.x  T is a vector of nonlinear func- tions of x, is linear in the parameters but nonlinear in the variables. Linear regression   26 Introduction to statistical pattern recognition  E[yx]  y  16  14  12  10  8  6  4  2  0 −2 −4  0  0.5  1  1.5  2.5  3  3.5  4  2 x  Figure 1.14 Population regression line  solid line  with representation of spread of conditional distribution  dotted lines  for normally distributed error terms, with variance depending on x  refers to a regression model that is linear in the parameters, but not necessarily in the variables.  Figure 1.14 shows a regression summary for some hypothetical data. For each value of x, there is a population of y values that varies with x. The solid line connecting the conditional means, E[yjx], is the regression line. The dotted lines either side represent the spread of the conditional distribution  š1 standard deviation from the mean .  It is assumed that the difference  commonly referred to as an error or residual , ži , between the measurement on the response variable and its predicted value conditional on the measurements on the predictors,  is an unobservable random variable. A normal model for the errors  see Appendix E  is often assumed,  ži D yi  cid:8  E[yjxi ]  p.ž  D 1p  exp  2³ ¦   cid:5    cid:4   cid:8  1 2  ž2 ¦ 2  That is,  p.yijxi ; θ   D 1p  exp  2³ ¦   cid:4   cid:8  1 2¦ 2  .yi  cid:8  f .xi ; θ   2   cid:5   Given a set of data f.yi ; xi  ; i D 1; : : : ; ng, the maximum likelihood estimate of the model parameters  the value of the parameters for which the data are ‘most likely’,   Outline of book 27   1.21   discussed further in Appendix B , θ, is that for which  p.f.yi ; xi  gjθ    nX iD1  .yi  cid:8  f .xi ; θ   2  is a maximum. Assuming independent samples, this amounts to determining the value of θ for which the commonly used least squares error,  is a minimum  see the exercises at the end of the chapter .  For the linear model, procedures for estimating the parameters are described in  Chapter 4.  1.7 Outline of book  The aim in writing this volume is to provide a comprehensive account of statistical pattern recognition techniques with emphasis on methods and algorithms for discrimination and classiﬁcation. In recent years there have been many developments in multivariate analysis techniques, particularly in nonparametric methods for discrimination and classiﬁcation. These are described in this book as extensions to the basic methodology developed over the years.  This chapter has presented some basic approaches to statistical pattern recognition. Supplementary material on probability theory and data analysis can be found in the appendices.  A road map to the book is given in Figure 1.15, which describes the basic pattern recognition cycle. The numbers in the ﬁgure refer to chapters and appendices of this book.  Chapters 2 and 3 describe basic approaches to supervised classiﬁcation via Bayes’ rule and estimation of the class-conditional densities. Chapter 2 considers normal-based models. Chapter 3 addresses nonparametric approaches to density estimation.  Chapters 4–7 take a discriminant function approach to supervised classiﬁcation. Chapter 4 describes algorithms for linear discriminant functions. Chapter 5 considers kernel-based approaches for constructing nonlinear discriminant functions, namely radial basis functions and support vector machine methods. Chapter 6 describes alternative, projection-based methods, including the multilayer perceptron neural network. Chapter 7 describes tree-based approaches.  Chapter 8 addresses the important topic of performance assessment: how good is your designed classiﬁer and how well does it compare with competing techniques? Can improvement be achieved with an ensemble of classiﬁers?  Chapters 9 and 10 consider techniques that may form part of an exploratory data analysis. Chapter 9 describes methods of feature selection and extraction, both linear and nonlinear. Chapter 10 addresses unsupervised classiﬁcation or clustering.  Finally, Chapter 11 covers additional topics on pattern recognition including model  selection.   28 Introduction to statistical pattern recognition   cid:1    cid:11    cid:10   [D]  design of experiments;  methodology;  exploratory data analysis   cid:9    cid:1    cid:1   feature selection; feature extraction  cid:11   [9]   cid:1    cid:9    cid:10   classiﬁcation  cid:11    cid:1    cid:11   supervised   cid:11    cid:1   regression  unsupervised [10]   cid:10  clustering  cid:1  cid:11    cid:9    cid:9    cid:10   via Bayes’ theorem  discriminant analysis   cid:11   cid:1   [2]  [3]  [2,4]   cid:11   cid:1   parametric  nonparametric  linear  nonlinear  [5,6,7]   cid:10    cid:10    cid:10    cid:11    cid:10    cid:11    cid:10   assessment  [8]  Figure 1.15 The pattern recognition cycle; numbers in parentheses refer to chapters and appen- dices of this book  1.8 Notes and references  There was a growth of interest in techniques for automatic pattern recognition in the 1960s. Many books appeared in the early 1970s, some of which are still very relevant today and have been revised and reissued. More recently, there has been another ﬂurry of books on pattern recognition, particularly incorporating developments in neural network methods. A very good introduction is provided by the book of Hand  1981a . Perhaps a lit- tle out of date now, it provides nevertheless a very readable account of techniques for discrimination and classiﬁcation written from a statistical point of view and is to be recommended. Two of the main textbooks on statistical pattern recognition are those by Fukunaga  1990  and Devijver and Kittler  1982 . Written perhaps with an engineering emphasis, Fukunaga’s book provides a comprehensive account of the most important aspects of pattern recognition, with many examples, computer projects and problems. Devijver and Kittler’s book covers the nearest-neighbour decision rule and feature selec- tion and extraction in some detail, though not at the neglect of other important areas of statistical pattern recognition. It contains detailed mathematical accounts of techniques and algorithms, treating some areas in depth.   Notes and references 29  Another important textbook is that by Duda et al.  2001 . Recently revised, this presents a thorough account of the main topics in pattern recognition, covering many recent developments. Other books that are an important source of reference material are those by Young and Calvert  1974 , Tou and Gonzales  1974  and Chen  1973 . Also, good accounts are given by Andrews  1972 , a more mathematical treatment, and Therrien  1989 , an undergraduate text.  Recently, there have been several books that describe the developments in pattern recognition that have taken place over the last decade, particularly the ‘neural network’ aspects, relating these to the more traditional methods. A comprehensive treatment of neural networks is provided by Haykin  1994 . Bishop  1995  provides an excellent introduction to neural network methods from a statistical pattern recognition perspective. Ripley’s  1996  account provides a thorough description of pattern recognition from within a statistical framework. It includes neural network methods, approaches developed in the ﬁeld of machine learning, recent advances in statistical techniques as well as development of more traditional pattern recognition methods and gives valuable insights into many techniques gained from practical experience. Hastie et al.  2001  provide a thorough description of modern techniques in pattern recognition. Other books that deserve a mention are those by Schalkoff  1992  and Pao  1989 .  Hand  1997  gives a short introduction to pattern recognition techniques and the central ideas in discrimination and places emphasis on the comparison and assessment of classiﬁers.  A more specialised treatment of discriminant analysis and pattern recognition is the book by McLachlan  1992a . This is a very good book. It is not an introductory textbook, but provides a thorough account of recent advances and sophisticated developments in discriminant analysis. Written from a statistical perspective, the book is a valuable guide to theoretical and practical work on statistical pattern recognition and is to be recommended for researchers in the ﬁeld.  Comparative treatments of pattern recognition techniques  statistical, neural and ma- chine learning methods  are provided in the volume edited by Michie et al.  1994  who report on the outcome of the Statlog project. Technical descriptions of the methods are given, together with the results of applying those techniques to a wide range of prob- lems. This volume provides the most extensive comparative study available. More than 20 different classiﬁcation procedures were considered for about 20 data sets.  The book by Watanabe  1985 , unlike the books above, is not an account of statis- tical methods of discrimination, though some are included. Rather, it considers a wider perspective of human cognition and learning. There are many other books in this latter area. Indeed, in the early days of pattern recognition, many of the meetings and confer- ences covered the humanistic and biological side of pattern recognition in addition to the mechanical aspects. Although these non-mechanical aspects are beyond the scope of this book, the monograph by Watanabe provides one unifying treatment that we recommend for background reading.  There are many other books on pattern recognition. Some of those treating more speciﬁc parts  such as clustering  are cited in the appropriate chapter of this book. In addition, most textbooks on multivariate analysis devote some attention to discrimination and classiﬁcation. These provide a valuable source of reference and are cited elsewhere in the book.   30 Introduction to statistical pattern recognition  The website www.statistical-pattern-recognition.net contains refer-  ences and links to further information on techniques and applications.  Exercises  In some of the exercises, it will be necessary to generate samples from a multivariate density with mean µ and covariance matrix  cid:9 . Many computer packages offer routines for this. However, it is a simple matter to generate samples from a normal distribution with unit variance and zero mean  for example, Press et al., 1992 . Given a vector Y i of such samples, then the vector U  cid:2 1=2Y i C µ has the required distribution, where U is the matrix of eigenvectors of the covariance matrix and  cid:2 1=2 is the diagonal matrix whose diagonal elements are the square roots of the corresponding eigenvalues  see Appendix C .  1. Consider two multivariate normally distributed classes,  p.xj!i   D  1  .2³   p=2j cid:9 ij1=2 exp  ²  cid:8  1 2  .x  cid:8  µi  T  cid:9    cid:8 1 i  .x  cid:8  µi    ¦  with means µ1 and µ2 and equal covariance matrices,  cid:9 1 D  cid:9 2 D  cid:9 . Show that the logarithm of the likelihood ratio is linear in the feature vector x. What is the equation of the decision boundary?  2. Determine the equation of the decision boundary for the more general case of  cid:9 1 D Þ cid:9 2, for scalar Þ  normally distributed classes as in Exercise 1 . In particular, for two univariate distributions, N .0; 1  and N .1; 1=4 , show that one of the decision regions is bounded and determine its extent.  3. For the distributions in Exercise 1, determine the equation of the minimum risk  decision boundary for the loss matrix  cid:2  D   cid:5    cid:4  0 2 1 0  4. Consider two multivariate normally distributed classes  !2 with mean . cid:8 1; 0 T and !1 with mean .1; 0 T , and identity covariance matrix . For a given threshold ¼  see equation  1.14   on the likelihood ratio, determine the regions  cid:6 1 and  cid:6 2 in a Neyman–Pearson rule.  5. Consider three bivariate normal distributions, !1, !2, !3 with identity covariance matrices and means . cid:8 2; 0 T , .0; 0 T and .0; 2 T . Show that the decision boundaries are piecewise linear. Now deﬁne a class A as the mixture of !1 and !3,  pA.x  D 0:5 p.xj!1  C 0:5 p.xj!3   and class B as bivariate normal with identity covariance matrix and mean .a; b T , for some a, b. What is the equation of the Bayes decision boundary? Under what conditions is it piecewise linear?   Exercises 31  6. Consider two uniform distributions with equal priors  p.xj!1  D  p.xj!2  D  ² 1 when 0  cid:10  x  cid:10  1 0 otherwise   1 2 when 1 0  2  cid:10  x  cid:10  5  otherwise  2  r .t   D    3 8 when 0  cid:10  t  cid:10  1 < t  cid:10  1 0 when 1 3  3  Show that the reject function is given by  Hence calculate the error rate by integrating  1.10 .  7. Reject option. Consider two classes, each normally distributed with means x D 1 and x D  cid:8 1 and unit variances; p.!1  D p.!2  D 0:5. Generate a test set and use it  without using class labels  to estimate the reject rate as a function of the threshold t. Hence, estimate the error rate for no rejection. Compare with the estimate based on a labelled version of the test set. Comment on the use of this procedure when the true distributions are unknown and the densities have to be estimated.  8. The area of a sphere of radius r in p dimensions, Sp, is  Sp D 2³  p  2 r p cid:8 1 0. p=2   where 0 is the gamma function  0.1=2  D ³ 1=2, 0.1  D 1, 0.xC1  D x0.x  . Show that the probability of a sample, x, drawn from a zero-mean normal distribution with covariance matrix ¦ 2I  I is the identity matrix  and having jxj  cid:10  R is  Z R  0  1  Sp.r    .2³ ¦ 2  p=2 exp   cid:5    cid:4   cid:8  r 2 2¦ 2  dr  Evaluate this numerically for R D 2¦ and for p D 1; : : : ; 10. What do the results tell you about the distribution of normal samples in high-dimensional spaces?  9. In a two-class problem, let the cost of misclassifying a class !1 pattern be C1 and the cost of misclassifying a class !2 pattern be C2. Show that the point on the ROC curve that minimises the risk has gradient  C2 p.!2  C1 p.!1   10. Show that under the assumption of normally distributed residuals, the maximum likelihood solution for the parameters of a linear model is equivalent to minimising the sum-square error  1.21 .   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   2  Density estimation – parametric  Overview  A discrimination rule may be constructed through explicit estimation of the class- conditional density functions and the use of Bayes’ rule. One approach is to assume a simple parametric model for the density functions and to estimate the parameters of the model using an available training set.  2.1 Introduction  In Chapter 1 we considered the basic theory of pattern classiﬁcation. All the information regarding the density functions p.xj!i   was assumed known. In practice, this knowledge is often not or only partially available. Therefore, the next question that we must address is the estimation of the density functions themselves. If we can assume some paramet- ric form for the distribution, perhaps from theoretical considerations, then the problem reduces to one of estimating a ﬁnite number of parameters. In this chapter, special con- sideration is given to the normal distribution which leads to algorithms for the Gaussian classiﬁer. of class membership p.!ijx , which may be written  We described in Chapter 1 how the minimum error decision is based on the probability  p.!ijx  D p.!i    p.xj!i   p.x   Assuming that the prior probability p.!i   is known, then in order to make a decision we need to estimate the class-conditional density p.xj!i    the probability density function p.x  is independent of !i and therefore is not required in the decision-making process . The estimation of the density is based on a sample of observations Di D fxi g ; : : : ; xi ni j 2 R p  from class !i . In this chapter and the following one we consider two basic  xi approaches to density estimation: the parametric and nonparametric approaches. In the parametric approach, we assume that the class-conditional density for class !i is of a known form but has an unknown parameter, or set of parameters,  cid:6 i , and we write this  1   34 Density estimation – parametric  as p.xj cid:6 i  . In the estimative approach we use an estimate of the parameter  cid:6 i , based on the samples Di in the density. Thus we take  p.xj!i   D p.xjO cid:6 i    where O cid:6 i D O cid:6 i .Di   is an estimate of the parameter  cid:6 i based on the sample. A different data sample, Di , would give rise to a different estimate O cid:6 i , but the estimative approach does not take into account this sampling variability. In the predictive or Bayesian approach, we write  Z  p.xj!i   D  p.xj cid:6 i   p. cid:6 ijDi   d cid:6 i  where p. cid:6 ijDi   can be regarded as a weighting function based on the data set Di , or as a full Bayesian posterior density function for  cid:6 i based on a prior p. cid:6 i   and the data  Aitchison et al., 1977 . Thus, we admit that we do not know the true value of  cid:6 i and instead of taking a single estimate, we take a weighted sum of the densities p.xj cid:6 i  , weighted by the distribution p. cid:6 ijDi  . This approach may be regarded as making allowance for the sampling variability of the estimate of  cid:6 i .  The alternative nonparametric approach to density estimation that we consider in this  book does not assume a functional form for the density and is discussed in Chapter 3.  2.2 Normal-based models  2.2.1 Linear and quadratic discriminant functions  Perhaps the most widely used classiﬁer is that based on the normal distribution  Appendix E ,  p.xj!i   D  1 2 j cid:2 ij 1 p  2  exp  .2³    ²  cid:7  1 2  .x  cid:7  µi  T  cid:2    cid:7 1 i  .x  cid:7  µi    ¦  Classiﬁcation is achieved by assigning a pattern to a class for which the posterior prob- ability, p.!ijx , is the greatest, or equivalently log. p.!ijx  . Using Bayes’ rule and the normal assumption for the conditional densities above, we have  log. p.!ijx   D log. p.xj!i    C log. p.!i     cid:7  log. p.x   .x  cid:7  µi  T  cid:2  log.j cid:2 ij  log.2³   C log. p.!i     cid:7  log. p.x    .x  cid:7  µi    cid:7  1 2   cid:7 1 i  D  cid:7  1 2  cid:7  p 2  Since p.x  is independent of class, the discriminant rule is: assign x to !i if gi > g j , for all j 6D i, where  gi .x  D log. p.!i     cid:7  1  2 log.j cid:2 ij   cid:7  1  2  .x  cid:7  µi  T  cid:2    cid:7 1 i  .x  cid:7  µi     2.1    Normal-based models 35  Classifying a pattern x on the basis of the values of gi .x ; i D 1; : : : ; C, gives the normal-based quadratic discriminant function  McLachlan, 1992a . In the estimative approach, the quantities µi and  cid:2 i in the above are replaced by estimates based on a training set. Consider a set of samples, fx1; : : : ; xng, x j 2 R p, and a normal distribution characterised by θ D .µ;  cid:2  . Then the likelihood function, L.x1; : : : ; xnjθ  , is  L.x1; : : : ; xnjθ   D nY iD1  1 2 j cid:2 j 1 p  2  .2³    exp  ²  cid:7  1 2  .xi  cid:7  µ T  cid:2    cid:7 1.xi  cid:7  µ   ¦  Differentiating log.L  with respect to θ gives the equations1  @ log.L   @µ  D 1 2  nX iD1   cid:2    cid:7 1.xi  cid:7  µ  C 1 2  nX iD1   cid:7 1 T .xi  cid:7  µ   . cid:2   and  @ log.L   @ cid:2   D  cid:7  n 2   cid:2    cid:7 1 C 1 2  nX iD1  .xi  cid:7  µ .xi  cid:7  µ T  cid:2    cid:7 1 cid:2    cid:7 1  where we have used the result that  @jAj @A  D [adj.A ]T D jAj.A   cid:7 1 T  Equating the above two equations to zero gives the maximum likelihood estimate of the mean as  the sample mean vector, and the covariance matrix estimate as  m D 1 n  nX iD1  xi  O cid:2  D 1 n  nX iD1  .xi  cid:7  m .xi  cid:7  m T  the sample covariance matrix.  Substituting the estimates of the means and the covariance matrices  termed the ‘plug- in estimates’  of each class into  2.1  gives the Gaussian classiﬁer or quadratic discrim- ination rule: assign x to !i if gi > g j , for all j 6D i, where  gi .x  D log. p.!i     cid:7  1  2 log.j O cid:2 ij   cid:7  1  2  .x  cid:7  mi  T O cid:2    cid:7 1 i  .x  cid:7  mi     2.2   If the training data have been gathered by sampling from the classes, then a plug-in j n j , where ni is the number of patterns  estimate for the prior probability, p.!i  , is ni =P in class !i .  1Differentiation with respect to a p-dimensional vector means differentiating with respect to each component of the vector. This gives a set of p equations which may be expressed as a vector equation  see Appendix C .   36 Density estimation – parametric  O cid:2 i  to zero. Another approach is to project the data onto a space in which O cid:2 i  In the above, we may apply the discrimination rule to all members of the design set and the separate test set, if available. Problems will occur in the Gaussian classiﬁer if any of the matrices O cid:2 i is singular. There are several alternatives commonly employed. One is simply to use diagonal covariance matrices; that is, set all off-diagonal terms is of nonsingular, perhaps using a principal components analysis  see Chapter 9 , and then to use the Gaussian classiﬁer in the reduced dimension space. Such an approach is assessed by Schott  1993  and linear transformations for reducing dimensionality are discussed in Chapter 9. A further alternative is to assume that the class covariance matrices  cid:2 1; : : : ;  cid:2 C are all the same, in which case the discriminant function  2.1  simpliﬁes and the discriminant rule becomes: assign x to !i if gi > g j , for all j 6D i, where gi is the linear discriminant  gi .x  D log. p.!i     cid:7  1  2 mT i S   cid:7 1 W mi C xT S   cid:7 1 W mi   2.3   in which SW is the common group covariance matrix. This is the normal-based lin- ear discriminant function. The maximum likelihood estimate is the pooled within-group sample covariance matrix  SW D CX iD1  O cid:2 i  ni n  n  n  cid:7  C  SW  wT x C w0 > 0  The unbiased estimate is given by  A special case of  2.3  occurs when the matrix SW is taken to be the identity and the class priors p.!i   are equal. This is the nearest class mean classiﬁer: assign x to class !i if   cid:7 2xT m j C mT  j m j >  cid:7 2xT mi C mT  i mi for all i 6D j  For the special case of two classes, the rule  2.3  may be written: assign x to class !1 if   2.4    2.5   else assign x to class !2, where in the above .m1  cid:7  m2   cid:8   cid:7  p.!2  p.!1   w D S w0 D  cid:7  log   cid:7 1 W   cid:7  1 2  .m1 C m2 T w  In problems where the data are from multivariate normal distributions with different covariance matrices, there may be insufﬁcient data to obtain good estimates of class covariance matrices. Sampling variability may mean that it is better to assume equal co- variance matrices  leading to the linear discriminant rule  rather than different covariance matrices. However, there are several intermediate covariance matrix structures  Flury, 1987  that may be considered without making the restrictive assumption of equality of   Normal-based models 37  covariance matrices. These include diagonal, but different, covariance matrices; com- mon principal components; and proportional covariance matrices models. These models have been considered in the context of multivariate mixtures  see Section 2.3  and are discussed further in Chapter 9.  The linear discriminant rule  2.3  is quite robust to departures from the equal co- variance matrix assumptions  Wahl and Kronmal, 1977; O’Neill, 1992 , and may give better performance than the optimum quadratic discriminant rule for normally distributed classes when the true covariance matrices are unknown and the sample sizes are small. However, it is better to use the quadratic rule if the sample size is sufﬁcient. The linear discriminant function can be greatly affected by non-normality and, if possible, variables should be transformed to approximate normality before applying the rule. Nonlinear transformations of data are discussed in Chapter 9.  2.2.2 Regularised discriminant analysis  Regularised discriminant analysis  RDA  was proposed by Friedman  1989  for small- sample, high-dimensional data sets as a means of overcoming the degradation in perfor- mance of the quadratic discriminant rule. Two parameters are involved: ½, a complexity parameter providing an intermediate between a linear and a quadratic discriminant rule; and  cid:13  , a shrinkage parameter for covariance matrix updates. Speciﬁcally, O cid:2 i is replaced by a linear combination,  cid:2  ½ matrix O cid:2 i and the pooled covariance matrix SW ,  i , of the sample covariance   2.6   where 0  cid:10  ½  cid:10  1 and  At the extremes of ½ D 0 and ½ D 1 we have covariance matrix estimates that lead to the quadratic discriminant rule and the linear discriminant rule respectively:  ½  i D .1  cid:7  ½ Si C ½S .1  cid:7  ½ ni C ½n   cid:2   Si D ni O cid:2 i ;  S D nSW  ½  i D   cid:2   ² O cid:2 i ½ D 0 SW ½ D 1  The second parameter  cid:13  is used to regularise the sample class covariance matrix further beyond that provided by  2.6 ,  ½; cid:13   i D .1  cid:7   cid:13    cid:2   ½  i C  cid:13  ci .½ I p   cid:2    2.7   where I p is the p ð p identity matrix and  ci .½  D Trf cid:2   ½  i g= p;  ½  i .  The matrix  cid:2   the average eigenvalue of  cid:2  normal-based discriminant rule: assign x to !i if gi > g j ; for all j 6D i, where j  C log. p.!i     ] cid:7 1.x  cid:7  mi    cid:7  1  .x  cid:7  mi  T [ cid:2   is then used as the plug-in estimate of the covariance matrix in the  gi .x  D  cid:7  1  ½; cid:13  i  2 log.j cid:2   ½; cid:13  i  ½; cid:13  i  2   38 Density estimation – parametric  Friedman’s RDA approach involves choosing the values of ½ and  cid:13  to minimise a cross-validated estimate of future misclassiﬁcation cost  that is, a robust estimate of the error rate using a procedure called cross-validation – see Chapter 8 . The strategy is to evaluate the misclassiﬁcation risk on a grid of points  0  cid:10  ½;  cid:13   cid:10  1  and to choose the optimal values of ½ and  cid:13  to be those grid point values with smallest estimated risk.  Robust estimates of the covariance matrices  see Chapter 11  may also be incorporated  in the analysis. Instead of using  cid:2  Q cid:2   ½ i  in  2.7 , Friedman proposes Q cid:2  i D .1  cid:7  ½ QSi C ½QS  ½  W ½ i  ½ i , given by  where  zi j w j .x j  cid:7  Qmi  .x j  cid:7  Qmi  T  QSk  zi j w j x j =Wi  QSi D nX jD1 QS D CX kD1 Qmi D nX jD1 Wi D nX jD1 W D CX iD1 i D .1  cid:7  ½ Wi C ½W W ½  zi j w j  Wi  in which the w j  0  cid:10  w j  cid:10  1  are weights associated with each observation and zi j D 1 if x j 2 class !i and 0 otherwise. For w j D 1 for all j, Q cid:2   i D  cid:2   In order to reduce the computation cost, a strategy that updates a covariance matrix  i .  ½  ½  when one observation is removed from a data set is employed.  Friedman  1989  assesses the effectiveness of RDA on simulated and real data sets. He ﬁnds that model selection based on the cross-validation procedure performs well, and that fairly accurate classiﬁcation can be achieved with RDA for a small ratio of the number of observations  n  to the number of variables   p . He ﬁnds that RDA has the potential for dramatically increasing the power of discriminant analysis when sample sizes are small and the number of variables is large.  In conclusion, RDA can improve classiﬁcation performance when the covariance matrices are not close to being equal and or the sample size is too small for quadratic discriminant analysis to be viable.  2.2.3 Example application study  The problem The purpose of this study is to investigate the feasibility of predicting the degree of recovery of patients entering hospital with severe head injury using data collected shortly after injury  Titterington et al., 1981 .   Normal-based models 39  Summary Titterington et al.  1981  report the results of several classiﬁers, each de- signed using different training sets. In this example, results of the application of a quadratic discriminant rule  2.2  are presented.  The data The data set comprises measurements on patients entering hospital with a head injury involving a minimum degree of brain damage. Measurements are made on six categorical variables: Age, grouped into decades 0–9, 10–19, : : : , 60–69, 70C; EMV score, relating to eye, motor and verbal responses to stimulation, grouped into seven categories; MRP, a summary of the motor responses in all four limbs, graded 1  nil  to 7  normal ; Change, the change in neurological function over the ﬁrst 24 hours, graded 1  deteriorating  to 3  improving ; Eye Indicant, a summary of eye movement scores, graded 1  bad  to 3  good ; Pupils, the reaction of pupils to light, graded 1  non- reacting  or 2  reacting . There are 500 patients in the training and test sets, distributed over three classes related to the predicted outcome  dead or vegetative; severe disability; and moderate disability or good recovery . The number of patterns in each of the three classes for the training and the test sets are: training – 259, 52, 189; test – 250, 48, 202. Thus there is an uneven class distribution. Also, there are many missing values. These have been substituted by class means on training and population means on test. Further details of the data are given by Titterington et al.  1981 .  The model The data in each class are modelled using a normal distribution leading to the discriminant rule  2.2 . Training procedure Training consists of estimating the quantities fmi ; O cid:2 i , p.!i  , i D 1; : : : ; Cg, the sample mean, sample covariance matrix and prior class probability for each class from the data. The prior class probability is taken to be p.!i   D ni =n. Once  cid:7 1 O cid:2 i has been estimated, a numerical procedure must be used to calculate the inverse, O cid:2  , i and the determinant, j cid:2 ij. Once calculated, these quantities are substituted into equation  2.2  to give C functions, gi .x .  For each pattern, x, in the training and test set, gi .x  is calculated and x assigned to  the class for which the corresponding discriminant function, gi .x , is the largest.  Results Results on training and test sets for a Gaussian classiﬁer  quadratic rule  are given in Table 2.1 as misclassiﬁcation matrices or confusion matrices  see also Exer- cise 2 . Note that class 2 is nearly always classiﬁed incorrectly as class 1 or class 3.  Table 2.1 Left: confusion matrix for training data; right: results for the test data True class  True class  Predicted class  1 209 0 50  2 22 1 29  3 15 1 173  1 2 3  Predicted class  1 188 3 59  2 19 1 28  3 29 2 171  1 2 3   40 Density estimation – parametric  2.2.4 Further developments  There have also been several investigations of the robustness of the linear and quadratic discriminant rules to certain types of non-normality  for example, Lachenbruch et al., 1973; Ching´anda and Subrahmaniam, 1979; Ashikaga and Chang, 1981; Balakrishnan and Subrahmaniam, 1985 .  Robustness of the discrimination rule to outliers is discussed by Todorov et al.  1994 ;  see Krusi´nska  1988  for a review and also Chapter 11.  Aeberhard et al.  1994  report an extensive simulation study on eight statistical clas- siﬁcation methods applied to problems when the number of observations is less than the number of variables. They found that out of the techniques considered, RDA was the most powerful, being outperformed by linear discriminant analysis only when the class covariance matrices were identical and for a large training set size. Reducing the dimensionality by feature extraction methods generally led to poorer results. However, Schott  1993  ﬁnds that dimension reduction prior to quadratic discriminant analysis can substantially reduce misclassiﬁcation rates for small sample sizes. It also decreases the sample sizes necessary for quadratic discriminant analysis to be preferred over linear dis- criminant analysis. Alternative approaches to the problem of discriminant analysis with singular covariance matrices are described by Krzanowski et al.  1995 .  Further simulations have been carried out by Rayens and Greene  1991  who com- pare RDA with an approach based on an empirical Bayes framework for addressing the problem of unstable covariance matrices  see also Greene and Rayens, 1989 . Aeberhard et al.  1993  propose a modiﬁed model selection procedure for RDA and Celeux and Mkhadri  1992  present a method of regularised discriminant analysis for discrete data. Expressions for the shrinkage parameter are proposed by Loh  1995  and Mkhadri  1995 . An alternative regularised Gaussian discriminant analysis approach is proposed by Bensmail and Celeux  1996 . Termed eigenvalue decomposition discriminant analysis, it is based on the reparametrisation of the covariance matrix of a class in terms of its eigenvalue decomposition. Fourteen different models are assessed, and results compare favourably with RDA. Raudys  2000  considers a similar development.  Hastie et al.  1995  cast the discrimination problem as one of regression using optimal scaling and use a penalised regression procedure  regularising the within-class covariance matrix . In situations where there are many highly correlated variables, their procedure offers promising results.  Extensions of linear and quadratic discriminant analysis to data sets where the patterns  are curves or functions are developed by James and Hastie  2001 .  2.2.5 Summary  Linear and quadratic discriminants  or equivalently, Gaussian classiﬁers  are widely used methods of supervised classiﬁcation and are supported by many statistical packages. Prob- lems occur when the covariance matrices are close to singular and when class boundaries are nonlinear. The former can be overcome by regularisation. This can be achieved by imposing structure on the covariance matrices, pooling combining covariance matrices or adding a penalty term to the within-class scatter. Friedman  1989  proposes a scheme that includes a combination of matrices and the addition of a penalty term.   Normal mixture models 41  2.3 Normal mixture models  Finite mixture models have received wide application, being used to model distributions where the measurements arise from separate groups, but individual membership is un- known. As methods of density estimation, mixture models are more ﬂexible than the simple normal-based models of Section 2.2, providing improved discrimination in some circumstances. Applications of mixture models include  see Section 2.5  textile ﬂaw de- tection  where the data comprise measurements from a background ‘noise’ and a ﬂaw , waveform classiﬁcation  where the signal may comprise a sample from a waveform or noise  and target classiﬁcation  in which the radar target density is approximated by a sum of simple component densities .  There are several issues associated with mixture models that are of interest. The most important for density estimation concerns the estimation of the model parameters. We may also be interested in how many components are present and whether there are any ‘natural’ groupings in the data that may be identiﬁed. This is the problem of clustering  or unsupervised classiﬁcation  that we return to in Chapter 10.  2.3.1 Maximum likelihood estimation via EM  A ﬁnite mixture model is a distribution of the form  p.x  D gX jD1  ³ j p.x; θ j    where g is the number of mixture components, ³ j ½ 0 are the mixing proportions  Pg ³ j D 1  and p.x; θ j  ; j D 1; : : : ; g, are the component density functions which jD1 depend on a parameter vector θ j . There are three sets of parameters to estimate: the values of ³ j , the components of θ j and the value of g. The component densities may be of different parametric forms and are speciﬁed using knowledge of the data generation process, if available. In the normal mixture model, p.x; θ j   is the multivariate normal distribution, with θ j D fµ j ;  cid:2  jg.  Given a set of n observations  x1; : : : ; xn , the likelihood function is  L. cid:10   D nY iD1  gX jD1  ³ j p.xijθ j     2.8   where  cid:10  denotes the set of parameters f³1; : : : ; ³g; θ 1; : : : ; θ gg and we now denote the dependence of the component densities on their parameters as p.xjθ j  . In general, it is not possible to solve @ L=@ cid:10  D 0 explicitly for the parameters of the model and iterative schemes must be employed. One approach for maximising the likelihood L. cid:10   is to use a general class of iterative procedures known as EM  expectation – maximisation  algorithms, introduced in the context of missing data estimation by Dempster et al.  1977 , though it had appeared in many forms previously. The basic procedure is as follows. We suppose that we have a set of ‘incomplete’ data vectors fxg and we wish to maximise the likelihood L. cid:10   D p.fxgj cid:10  . Let fyg denote a   42 Density estimation – parametric  typical ‘complete’ version of fxg, that is, each vector xi is augmented by the ‘missing’ values so that yT  . There may be many possible vectors yi in which we can embed xi , though there may be a natural choice for some problems. In the ﬁnite mixture case, zi is a class indicator vector zi D .z1i ; : : : ; zgi  T , where z ji D 1 if xi belongs to the jth component and zero otherwise.  i D .xT  ; zT i  i  General EM procedure Let the likelihood of fyg be g.fygj cid:10   whose form we know explicitly so that the likeli- hood p.fxgj cid:10   is obtained from g.fygj cid:10   by integrating over all possible fyg in which the set fxg is embedded:  L. cid:10   D p.fxgj cid:10   D  g.xi ; zj cid:10   dz  Z nY iD1  The EM procedure generates a sequence of estimates of  cid:10 , f cid:10  estimate  cid:10   .0  and consists of two steps:  .m g, from an initial  1. E-step: Evaluate Q. cid:10 ;  cid:10   Q. cid:10 ;  cid:10   .m   D  i  .m    Z X  4D E[log.g.fygj cid:10   jfxg;  cid:10  log.g.xi ; zij cid:10    p.fzgjfxg;  cid:10   .m ], that is,  .m   dz1 : : : dzn  the expectation of the complete data log-likelihood, conditional on the observed data, fxg, and the current value of the parameters,  cid:10   2. M-step: Find  cid:10  D  cid:10   .m . .mC1  that maximises Q. cid:10 ;  cid:10   .m  . Often the solution for the  M-step may be obtained in closed form.  The likelihoods of interest satisfy  Lf cid:10   .mC1 g ½ Lf cid:10   .m g  so they are monotonically increasing  see the exercises .  .m ; f . cid:10   An illustration of the EM iterative scheme is shown in Figure 2.1. It is one of a class of iterative majorisation schemes  de Leeuw 1977; de Leeuw and Heiser 1977, 1980  that .m  , ﬁnd a local maximum of a function f . cid:10   by deﬁning an auxiliary function, Q. cid:10 ;  cid:10  .m    and lies everywhere else below it that touches the function f at the point . cid:10   strictly, iterative majorisation refers to a procedure for ﬁnding the minimum of a function by iteratively deﬁning a majorising function . The auxiliary function is maximised and that is the position of the maximum,  cid:10  greater than at the previous iteration. This process is repeated, with a new auxiliary .mC1   , and continues function being deﬁned that touches the curve at . cid:10  until convergence. The shape as well as the position of the auxiliary function will also .m   differs change as the iteration proceeds. In the case of the EM algorithm, Q. cid:10 ;  cid:10  from the log-likelihood at  cid:10  by H . cid:10 ;  cid:10   .mC1 , gives a value of the original function f  .m    see Dempster et al., 1977, for details :  .mC1 ; f . cid:10   log.L. cid:10    D Q. cid:10 ;  cid:10   .m    cid:7  H . cid:10 ;  cid:10   .m     Normal mixture models 43  log L        Q    ,      m+1   + hm+1  Q    ,      m   + hm   m    m+1    m+2   Figure 2.1 EM illustration: successive maximisation of the function Q. cid:10 ;  cid:10  creases in the log-likelihood  .m   leads to in-  where  H . cid:10 ;  cid:10   .m   D E[log.g.fygj cid:10  = p.fxgj cid:10   jfxg;  cid:10  .m ] log. p.zijfxg;  cid:10    p.zijfxg;  cid:10   Z X  D  i  .m   dz1 : : : dzn   2.9   and in Figure 2.1, hm D  cid:7 H . cid:10   .m ;  cid:10   .m  .  EM algorithm for mixtures Let us now consider the application of the EM algorithm to mixture distributions. For fully labelled data, we deﬁne the complete data vector y to be the observation augmented by a class label; that is, yT D .xT ; zT  , where z is an indicator vector of length g with a 1 in the kth position if x is in category k and zeros elsewhere. The likelihood of y is  which may be written as  since z j is zero except for j D k. The likelihood of x is  p.xj cid:10   D  X  g.yj cid:10    g.yj cid:10   D p.xjz;  cid:10   p.zj cid:10    D p.xjθ k  ³k  g.yj cid:10   D gY jD1  ð p.xjθ j  ³ j  Łz j  all possible z values  D gX jD1  ³ j p.xjθ j     44 Density estimation – parametric  which is a mixture distribution. Thus, we may interpret mixture data as incomplete data where the missing values are the class labels.  For n observations we have  with  g.y1; : : : ; ynj cid:10   D nY iD1  gY jD1  [ p.xijθ j  ³ j ]z ji  log.g.y1; : : : ; ynj cid:10    D nX iD1  zT  i l C nX iD1  zT i ui .θ    where the vector l has jth component log.³ j  , ui has jth component log. p.xijθ j    and zi has components z ji ; j D 1; : : : ; g, where z ji are the indicator variables taking value one if pattern xi is in group j, and zero otherwise. The likelihood of .x1; : : : ; xn  is L0. cid:10  , as follows. given by  2.8 . The steps in the basic iteration are 1. E-step: Form  Q. cid:10 ;  cid:10   .m   D nX iD1  wT  i l C nX iD1  wT  i ui .θ    where  wi D E.zijxi ;  cid:10   .m    with jth component, the probability that xi belongs to group j given the current estimates  cid:10   .m , given by  wi j D ³ .m   P  .m  j  p.xijθ p.xijθ  j ³ .m   k     .m  k     k  2. M-step: This consists of maximising Q with respect to  cid:10 . Consider the parameters ³i , ³ j D  θ i in turn. Maximising Q with respect to ³i  subject to the constraint that Pg jD1  1  leads to the equation  nX iD1  wi j obtained by differentiating Q  cid:7  ½.Pg jD1 Lagrange multiplier. The constraint P ³ j D 1 gives ½ D Pg jD1 we have the estimate of ³ j as  1 ³ j ³ j  cid:7  1  with respect to ³ j , where ½ is a wi j D n and   cid:7  ½ D 0  Pn iD1   2.10    2.11   For normal mixtures, θ i D .µi ;  cid:2 i   and we consider the mean and covariance matrix re-estimation separately. Differentiating Q with respect to µ j and equating to zero gives  O³ j D 1 n  nX iD1  wi j  nX iD1  wi j .xi  cid:7  µ j   D 0   Normal mixture models 45  which gives the re-estimation for µ j as  Differentiating Q with respect to  cid:2  j and equating to zero gives  Oµ j D  Pn iD1 Pn iD1  wi j xi wi j  D 1 n O³ j  nX iD1  wi j xi  O cid:2  j D  Pn iD1  wi j .xi  cid:7  Oµ j  .xi  cid:7  Oµ j  T  Pn iD1  wi j  D 1 n O³ j  nX iD1  wi j .xi  cid:7  Oµ j  .xi  cid:7  Oµ j  T   2.12    2.13   Thus, the EM algorithm for normal mixtures alternates between the E-step of estimat- ing the wi  equation  2.10   and the M-step of calculating O³ j , Oµ j and O cid:2  j . j D 1; : : : ; g  given the values of wi  equations  2.11 ,  2.12  and  2.13  . These estimates become the estimates at stage m C 1 and are substituted into the right-hand side of  2.10  for the next stage of the iteration. The process iterates until convergence of the likelihood.  Discussion The EM procedure is very easy to implement, but the convergence rate can be poor de- pending on the data distribution and the initial estimates for the parameters. Optimisation procedures, in addition to the EM algorithm, include Newton–Raphson iterative schemes  Hasselblad, 1966  and simulated annealing  Ingrassia, 1992 .  One of the main problems with likelihood optimisation is that there is a multitude of ‘useless’ global maxima  Titterington et al., 1985 . For example, if the mean of one of the groups is taken as one of the sample points, then the likelihood tends to inﬁnity as the variance of the component centred on that sample point tends to zero. Similarly, if sample points are close together, then there will be high local maxima of the likelihood function and it appears that the maximum likelihood procedure fails for this class of mixture models. However, provided that we do not allow the variances to tend to zero, perhaps by imposing an equality constraint on the covariance matrices, then the maxi- mum likelihood method is still viable  Everitt and Hand, 1981 . Equality of covariance matrices may be a rather restrictive assumption in many applications. Convergence to parameter values associated with singularities is more likely to occur with small sample sizes and when components are not well separated.  2.3.2 Mixture models for discrimination  If we wish to use the normal mixture model in a discrimination problem, one approach is to obtain the parameters  cid:10 1; : : :  cid:10 C for each of the classes in turn. Then, given a set of patterns xi ; i D 1; : : : ; n, that we wish to classify, we calculate L.xi ;j cid:10  j  , i D 1; : : : ; n; j D 1; : : : ; C, the likelihood of the observation given each of the models, and combine this with the class priors to obtain a probability of class membership.    2.14    2.15   46 Density estimation – parametric  Hastie and Tibshirani  1996  consider the use of mixture models for discrimination, allowing each group within a class to have its own mean vector, but the covariance matrix is common across all mixture components and across all classes. This is one way of restricting the number of parameters to be estimated. If we let ³ jr be the mixing probabilities for the rth subgroup within class ! j ; j D 1; : : : ; C, PR j ³ jr D 1, where class ! j has R j subgroups; µ jr be the mean of the rth rD1 subgroup within class ! j ; and  cid:2  be the common covariance matrix; then the re-estimation equations are  Hastie and Tibshirani, 1996   wi jr D  ³ jr p.xijθ jr   PR j kD1  ³ jk p.xijθ jk    where p.xijθ jr   is the density of the rth subgroup of class ! j evaluated at xi  θ jr denotes the parameters µ jr and  cid:2   and  O³ jr D 1  R jX rD1  wi jr ;  Oµ jr D  O³ jr   X giD j P giD j wi jr xi P giD j wi jr CX X R jX giD j rD1 jD1  O cid:2  D 1 n  wi jr .xi  cid:7  Oµ jr  .xi  cid:7  Oµ jr  T  P  giD j denoting the sum over observations xi where xi belongs to class ! j . Other constraints on the covariance matrix structure may be considered. Within the context of clustering using normal mixtures, Celeux and Govaert  1995  propose a parametrisation of the covariance matrix that covers several different conditions, ranging from equal spherical clusters  covariance matrices equal and proportional to the iden- tity matrix  to different covariance matrices for each cluster. Such an approach can be developed for discrimination.  2.3.3 How many components?  Several authors have considered the problem of testing for the number of components, g, of a normal mixture. This is not a trivial problem and depends on many factors including shape of clusters, separation, relative sizes, sample size and dimension of data. Wolfe  1971  proposes a modiﬁed likelihood ratio test in which the null hypothesis g D g0 is tested against the alternative hypothesis that g D g1. The quantity   cid:11  n  cid:7  1  cid:7  p  cid:7  g1 2   cid:12    cid:7  2 n  log.½   where ½ is the likelihood ratio, is tested as a chi-square with the degrees of freedom, d, being twice the difference in the number of parameters in the two hypotheses  Everitt,   Normal mixture models 47  et al., 2001 , excluding mixing proportions. For components of a normal mixture with arbitrary covariance matrices,  d D 2.g1  cid:7  g0   p. p C 3   2  and with common covariance matrices  the case that was studied by Wolfe, 1971 , d D 2.g1  cid:7  g0  p. This test has been investigated by Everitt  1981  and Anderson  1985 . For the com- mon covariance structure, Everitt ﬁnds that, for testing g D 1 against g D 2 in a two- component mixture, the test is appropriate if the number of observations is at least ten times the number of variables. McLachlan and Basford  1988  recommend that Wolfe’s modiﬁed likelihood ratio test be used as a guide to structure rather than rigidly interpreted. In a discrimination context, we may monitor performance on a separate test set  see Chapter 11  and choose the model that gives best performance on this test set. Note that the test set is part of the training procedure  more properly termed a validation set  and error rates quoted using these data will be optimistically biased.  2.3.4 Example application study  The problem The practical application concerns the automatic recognition of ships using high-resolution radar measurements of the radar cross-section of targets  Webb, 2000 .  Summary This is a straightforward mixture model approach to discrimination, with maximum likelihood estimates of the parameters obtained via the EM algorithm. The mixture component distributions are taken to be gamma distributions.  The data The data consist of radar range proﬁles  RRPs  of ships of seven class types. An RRP describes the magnitude of the radar reﬂections of the ship as a function of distance from the radar. The proﬁles are sampled at 3 m spacing and each RRP comprises 130 measurements. RRPs are recorded from all aspects of a ship as the ship turns through 360 degrees. There are 19 data ﬁles and each data ﬁle comprises between 1700 and 8800 training patterns. The data ﬁles are divided into train and test sets. Several classes have more than one rotation available for training and testing.  The model The density of each class is modelled using a mixture model. Thus, we have  p.x  D gX iD1  ³i p.xjθ i     2.16   where θ i represents the set of parameters of mixture component i. An independence model is assumed for each mixture component, p.xjθ i  , therefore  p.xjθ i   D 130Y jD1  p.x jjθ i j     48 Density estimation – parametric  and the univariate factor, p.x jjθ i j  , is modelled as a gamma distribution2 with parameters θ i j D .mi j ; ¼i j  ,  p.x jjθ i j   D  mi j  .mi j  cid:7  1 !¼i j   cid:7  mi j x j ¼i j   cid:8 mi j cid:7 1   cid:8    cid:7   cid:7  mi j x j ¼i j  exp  where mi j is the order parameter for variable x j of mixture component i and ¼i j is the mean. Thus for each mixture component, there are two parameters associated with each dimension. We denote by θ i the set fθ i j ; j D 1; : : : ; 130g, the parameters of component i. The gamma distribution is chosen from physical considerations – it has special cases of Rayleigh scattering and a non-ﬂuctuating target. Also, empirical measurements have been found to be gamma-distributed.  Training procedure Given a set of n observations  in a given class , the likelihood function is  L. cid:10   D nY iD1  gX jD1  ³ j p.xijθ j    where  cid:10  represents all the parameters of the model,  cid:10  D fθ j ; ³ j ; j D 1; : : : ; gg.  An EM approach to maximum likelihood is taken. If fθ  g denotes the estimate of the parameters of the kth component at the mth stage of the iteration, then the E-step estimates wi j , the probability that xi belongs to group j given the current estimates of the parameters  2.10 ,  ; ³ .m   .m  k  k   2.17    2.18    2.19   The M-step leads to the estimate of the mixture weights, ³ j , as in  2.11 ,  The equation for the mean is given by  2.12 ,  wi j D ³ .m   P  .m  j  p.xijθ p.xijθ  j ³ .m   k     .m  k     k  O³ j D 1 n  nX iD1  wi j  Oµ j D  Pn iD1 Pn iD1  wi j xi wi j  D 1 n O³ j  nX iD1  wi j xi  ¹.m jk   D  cid:7   Pn iD1  wik log.xi j =¼ jk   Pn iD1  wik  but the equation for the gamma order parameters, mi j , cannot be solved in closed form,  where ¹.m  D log.m   cid:7    .m  and   .m  is the digamma function.  Thus, for the gamma mixture problem, an EM approach may be taken, but a numerical root-ﬁnding routine must be used within the EM loop for the gamma distribution order parameters.  2There are several parametrisations of a gamma distribution. We present the one used in the study.   Normal mixture models 49  The number of mixture components per class was varied between 5 and 110 and the model for each ship determined by minimising a penalised likelihood criterion  the likelihood penalised by a complexity term – see Chapter 11 . This resulted in between 50 and 100 mixture components per ship. The density function for each class was constructed using  2.16 . The class priors were taken to be equal. The model was then applied to a separate test set and the error rate estimated.  2.3.5 Further developments  Jamshidian and Jennrich  1993  propose an approach for accelerating the EM algorithm based on a generalised conjugate gradients numerical optimisation scheme  see also Jamshidian and Jennrich, 1997 . Other competing numerical schemes for normal mixtures are described by Everitt and Hand  1981  and Titterington et al.  1985 . Lindsay and Basak  1993  describe a method of moments approach to multivariate normal mixtures that may be used to initialise an EM algorithm.  Further extensions to the EM algorithm are given by Meng and Rubin  1992, 1993 . The SEM  supplemented EM  algorithm is a procedure for computing the asymptotic variance-covariance matrix. The ECM  expectation conditional maximisation  algorithm is a procedure for implementing the M-step when a closed-form solution is not available, replacing each M-step by a sequence of conditional maximisation steps. An alternative gradient algorithm for approximating the M-step is presented by Lange  1995  and the algorithm is further generalised to the ECME  ECM either  algorithm by Liu and Rubin  1994 .  Developments for data containing groups of observations with longer than normal tails are described by Peel and McLachlan  2000 , who develop a mixture of t distributions model, with parameters determined using the ECM algorithm.  Approaches for choosing the number of components of a normal mixture include that of Bozdogan  1993 , who has compared several information-theoretic criteria on simulated data consisting of overlapping and non-overlapping clusters of different shape and compactness. Celeux and Soromenho  1996  propose an entropy criterion, evaluated as a by-product of the EM algorithm, and compare its performance with several other criteria.  2.3.6 Summary  Modelling using normal mixtures is a simple way of developing the normal model to nonlinear discriminant functions. Even if we assume a common covariance matrix for mixture components, the decision boundary is not linear. The EM algorithm provides an appealing scheme for parameter estimation, and there have been various extensions accelerating the technique. A mixture model may also be used to partition a given data set by modelling the data set using a mixture and assigning data samples to the group for which the probability of membership is the greatest. The use of mixture models in this context is discussed further in Chapter 10. Bayesian approaches to mixture modelling have also received attention and are considered in Section 2.4.3 in a discrimination context and in Chapter 10 for clustering.   50 Density estimation – parametric  2.4 Bayesian estimates  2.4.1 Bayesian learning methods  Here we seek to estimate some quantity such as the density at x  p.xjD   where D D fx1; : : : ; xng is the set of training patterns characterising the distribution. The dependence of the density at x on D is through the parameters of the model assumed for the density. If we assume a particular model, p.xjθ  , then the Bayesian approach does not base the density estimate on a single estimate of the parameters, θ, of the probability density function p.xjθ  , but admits that we do not know the true value of θ and we write  where by Bayes’ theorem the posterior density of θ may be expressed as  Z  p.xjD  D  p.xjθ   p.θjD  dθ  p.θjD  D  p.Djθ   p.θ   R p.Djθ   p.θ   dθ   2.20    2.21   Bayes’ theorem allows us to combine any prior, p.θ  , with any likelihood, p.Djθ  , to give the posterior. However, it is convenient for particular likelihood functions to take special forms of the prior that lead to simple, or at least tractable, solutions for the posterior. For a given model, p.xjθ  , the family of prior distributions for which the posterior density, p.θjD , is of the same functional form is called conjugate with respect to p.xjθ  . Some of the more common forms of conjugate priors are given by Bernardo and Smith  1994 .  The posterior density may also be calculated in a recursive manner. If the measure-  ments, xi , are given successively, we may write  2.21  as  p.θjx1; : : : ; xn  D  p.xnjθ   p.θjx1; : : : ; xn cid:7 1  R p.xnjθ   p.θjx1; : : : ; xn cid:7 1  dθ   2.22   for x1; : : : ; xn conditionally independent. This expresses the posterior distribution of θ given n measurements in terms of the posterior distribution given n  cid:7  1 measure- ments. Starting with p.θ  , we may perform the operation  2.22  n times to obtain the posterior.  For situations when there is no conjugate prior distribution, or the denominator in  2.21  cannot be evaluated analytically, we must resort to numerical methods, discussed in the following section.  To illustrate the Bayesian learning approach we shall consider the problem of esti- mating the mean of a univariate normal distribution with known variance, and quote the result for the multivariate normal distribution with unknown mean and covariance matrix. Further details on estimating the parameters of normal models are given in the books by Fukunaga  1990  and Fu  1968 .   Example 1 Estimating the mean of a normal distribution with known variance, ¦ 2.  Let the model for the density be normal with mean ¼ and variance ¦ 2, denoted p.xj¼    ¦ 2 known ,  Bayesian estimates 51  Assume a prior density for the mean ¼ that is also normal with mean ¼0 and variance ¦ 2 0 ,  p.xj¼  D 1p  exp  2³ ¦   cid:7   cid:7  1 2¦ 2   cid:8   .x  cid:7  ¼ 2  p.¼  D  1p 2³ ¦0  exp      cid:7  ¼  cid:7  ¼0 ¦0   cid:7  1 2   cid:8 2   Now,  p.x1; : : : ; xnj¼  p.¼   D p.¼   p.xij¼   nY iD1  D  1p 2³ ¦0  exp      cid:7  ¼  cid:7  ¼0 ¦0   cid:7  1 2  "   cid:8 2  nY iD1  1p 2³ ¦  exp     cid:7  1 2   cid:7  xi  cid:7  ¼   cid:8 2   ¦  This may be written in the form  p.x1; : : : ; xnj¼  p.¼  D 1 ¦ n¦0  1  .2³  .nC1 =2 exp     cid:7  1 2   cid:7  ¼  cid:7  ¼n ¦n   cid:8 2    cid:8    cid:7   cid:7  kn 2  exp  where  1 ¦ 2 n  D 1 ¦ 2 0 ¼n D ¦ 2  n  kn D ¼2  0 ¦ 2 0  C n ¦ 2   ¼0  ¦ 2 0  cid:7  ¼2 n ¦ 2 n  C  !  P xi P x 2  ¦ 2  i  ¦ 2  C  Substituting into equation  2.21  gives the posterior distribution as  cid:7  ¼  cid:7  ¼n ¦n  p.¼jx1; : : : ; xn  D     cid:7  1 2  1p 2³ ¦n  exp   cid:8 2    2.23   which is normal with mean ¼n and variance ¦ 2 n . As n ! 1, ¼n ! the sample mean, m D P i xi =n, and the variance of ¼, namely ¦ 2 n , tends to zero as 1=n. Thus, as more samples are used to obtain the distribution  2.23 , the contribution of the initial guesses ¦0 and ¼0 becomes smaller. This is illus- trated in Figure 2.2. Samples, xi , from a normal distribution with unit mean and unit variance are generated. A normal model is assumed with mean ¼ and unit variance.   52 Density estimation – parametric  p mx1, . . . , xn   n = 25  n = 15  n = 1  prior  0.0  1.0  n = 5  m  Figure 2.2 Bayesian learning of the mean of a normal distribution of known variance  Figure 2.2 plots the posterior distribution of the mean, ¼, given different numbers of samples  using  2.23   for a prior distribution of the mean that is normal with ¼0 D 0 0 D 1. As the number of samples increases, the posterior distribution narrows about and ¦ 2 the true mean. Finally, substituting the density into  2.20  gives the conditional distribution  p.xjx1; : : : ; xn  D  1 .¦ 2 C ¦ 2  1=2  p  2³ which is normal with mean ¼n and variance ¦ 2 C ¦ 2 n .  n  ²  cid:7  1 2  .x  cid:7  ¼n 2 ¦ 2 C ¦ 2  n  ¦  exp   cid:1   Example 2 Estimating the mean and the covariance matrix of a multivariate normal distribution.  In this example, we consider the multivariate problem in which the mean and the covariance matrix of a normal distribution are unknown. Let the model for the data be normal with mean µ and covariance matrix  cid:2 : ²  ¦  p.xjµ;  cid:2   D  1  .2³   p=2j cid:2 j 1  2  exp   cid:7  1 2  .x  cid:7  µ T  cid:2    cid:7 1.x  cid:7  µ   We wish to estimate the distribution of µ and  cid:2  given measurements x1; : : : ; xn.  When the mean µ and the covariance matrix  cid:2  of a normal distribution are to be estimated, an appropriate choice of prior density is the Gauss–Wishart, or normal – Wishart, probability density function in which the mean is normally distributed with mean µ0 and covariance matrix K cid:7 1=½, and K  the inverse of the covariance matrix  cid:2   is distributed according to a Wishart distribution  see Appendix E for a deﬁnition of   Bayesian estimates 53  some of the commonly used distributions  with parameters Þ and β:  p.µ; K  D N p.µjµ0; ½K Wi p.KjÞ; β   ²  cid:7  1 2  D j½Kj1=2 .2³   p=2 exp ð c. p; Þ jβjÞjKj.Þ cid:7 . pC1 =2  expf cid:7 Tr.βK g  ½.µ  cid:7  µ0 T K.µ  cid:7  µ0   ¦   2.24   where  " ³ p. p cid:7 1 =4  c. p; Þ  D   cid:7  2Þ C 1  cid:7  i   cid:8  cid:7 1  2  pY iD1  0  The term ½ expresses the conﬁdence in µ0 as the initial value of the mean and Þ  2Þ > p  cid:7  1  the initial conﬁdence in the covariance matrix. It can be shown that the posterior distribution  p.µ; Kjx1; : : : ; xn  D  p.x1; : : : ; xnjµ; K  p.µ; K   R p.x1; : : : ; xnjµ; K  p.µ;  cid:2   dµ dK  is also Gauss–Wishart with the parameters µ0, β, ½ and Þ replaced by  Fu, 1968   ½n D ½ C n Þn D Þ C n=2 µn D .½µ0 C nm =.½ C n  2βn D 2β C .n  cid:7  1 S C n½ n C ½   2.25   .µ0  cid:7  µ .µ0  cid:7  µ T  where  S D 1 n  cid:7  1  nX iD1  .xi  cid:7  m .xi  cid:7  m T  and m is the sample mean. That is,  p.µ; Kjx1; : : : ; xn   D N p.µjµn ; ½nK Wi p.KjÞn; βn    2.26   The conditional distribution of µ given K is normal. Marginalising  integrating with respect to µ  gives the posterior for K as Wi p.KjÞn; βn . The posterior for µ  integrating with respect to K  is  p.µjx1; : : : ; xn   D St p.µjµn; ½n.Þn  cid:7  . p  cid:7  1 =2 β   cid:7 1 n  ; 2.Þn  cid:7  . p  cid:7  1 =2    which is the p-dimensional generalisation of the univariate Student distribution  see Appendix E for a deﬁnition . Finally, we may substitute into  2.20  to obtain the density   54 Density estimation – parametric  for the case of the normal distribution with unknown mean and covariance matrix, p.xjx1; : : : ; xn  D St p.xjµn; .½n C 1  cid:7 1½n.Þn  cid:7  . p  cid:7  1 =2 β   cid:7 1 n  ; 2.Þn  cid:7  . p  cid:7  1 =2    2.27   which is also a Student distribution.   cid:1  The parameters Þn; ½n; µn and βn may be calculated for each class and the conditional density  2.27  may be used as a basis for discrimination: assign x to class !i for which gi > g j ; j D 1; : : : ; C; j 6D i, where  gi D p.xjx1; : : : ; xni 2 !i   p.!i     2.28   Unknown priors In equation  2.28 , p.!i   represents the prior probabilities for class !i . It may happen that the prior class probabilities, p.!i  , are unknown. In this case, we may treat them as parameters of the model that may be updated using the data. We write  p.!ijx;D  D  Z  Z  Z        p.!i ; π ;jx;D  dπ p.xj!i ; π ;D  p.!i ; πjD  dπ p.xj!i ;D  p.!i ; πjD  dπ  where we use π D .³1; : : : ; ³C   to represent the prior class probabilities.  The term p.!i ; πjD  may be written  p.!i ; πjD  D p.πjD  p.!ijπ ;D   D p.πjD ³i  The aspects of the measurements that inﬂuence the distribution of the class probabil- ities, π, are the numbers of patterns in each class, ni ; i D 1; : : : ; C. A suitable prior for ³i D p.!i   is a Dirichlet prior  see Appendix E , with parameters a0 D .a01; : : : ; a0C  ,  where k D 0.P ³i . This is written π ¾ DiC .πja0  for π D .³1; : : : ; ³C  T . Assuming that the distribution of the data  the ni   given the priors is multinomial,  0.a0i    0 is the gamma function  and ³C D 1  cid:7  PC cid:7 1 iD1  i a0i  =Q  i  p.³1; : : : ; ³C   D k  CY jD1  ³ a0 j cid:7 1  j  p.Djπ   D  n!QC lD1 nl!  CY lD1  ³ nl l  p.πjD    p.Djπ   p.π     2.29    2.30    2.31   then the posterior   Bayesian estimates 55  is also distributed as DiC .πja , where a D a0 C n and n D .n1; : : : ; nC  T , the vector of numbers of patterns in each class. Substituting the Dirichlet distribution for p.πjD  into equation  2.30 , and then p.!i ; πjD  from  2.30  into equation  2.29 , gives Z  p.!ijx;D    p.xj!i ;D   ³i DiC .πja  dπ   2.32   which replaces ³i in  2.28  by its expected value, ai =P  bility of class membership now becomes  j a j . Thus, the posterior proba-  p.!ijx;D  D .ni C a0i   p.xj!i   .ni C a0i   p.xj!i    P  i   2.33   This treatment has assumed that the ³i s are unknown but can be estimated from the training data as well as prior information. Whether the training data can be used depends on the sampling scheme used to gather the data. There are various modiﬁcations to  2.33  depending on the assumed knowledge concerning the ³i  see Geisser, 1964 .  Summary The Bayesian approach described above involves two stages. The ﬁrst stage is concerned with learning about the parameters of the distribution, θ, through the recursive calculation of the posterior density p.θjx1; : : : ; xn  for a speciﬁed prior  p.θjx1; : : : ; xn    p.xnjθ   p.θjx1; : : : ; xn cid:7 1   For a suitable prior and choice of class-conditional densities, the posterior distribution for θ is of the same form as the prior. The second stage is the integration over θ to obtain the conditional density p.xjx1; : : : ; xn   which may be viewed as making allowance for the variability in the estimate due to sampling. Although it is relatively straightforward to perform the integrations for the normal case that has been considered here, it may be necessary to perform two multivariate numerical integrations for more complicated probability density functions. This is the case for the normal mixture model for which there exist no reproducing  conjugate  densities.  2.4.2 Markov chain Monte Carlo  Introduction In the previous section we developed the Bayesian approach to density estimation and illustrated it using two normal distribution examples, for which the integral in the denom- inator of  2.21  may be evaluated analytically for suitable choices of prior distribution. We now consider some of the computational machinery for practical implementation of Bayesian methods to problems for which the normalising integral cannot be evaluated analytically and numerical integration over possibly high-dimensional spaces is infeasi- ble. The following section will illustrate these ideas on an application to a discrimination problem.   56 Density estimation – parametric  Let D denote the observed data. In a classiﬁcation problem, D comprises the training set of patterns and their class labels f.xi ; zi  ; i D 1; : : : ; ng, where xi are the patterns and ; i D 1; : : : ; ntg, a set of patterns for which the zi are the class labels, together with fxt class labels are assumed unknown. Let θ denote the model parameters and the ‘missing data’. In a classiﬁcation problem, the missing data are the class labels of the patterns of unknown class and the model parameters would be, for example, the means and covariance matrices for normally distributed classes. The posterior distribution of θ conditional on the observed data, D, may be written, using Bayes’ theorem, as the normalised product of the likelihood, p.Djθ  , and the prior distribution, p.θ  :  i  p.θjD  D  p.Djθ   p.θ   R p.Djθ   p.θ   dθ  This posterior distribution tells us all that we need to know about θ and can be used  to calculate summary statistics. The posterior expectation of a function h.θ   is  E[h.θ  jD] D  R h.θ   p.Djθ   p.θ   dθ R p.Djθ   p.θ   dθ   2.34    2.35   However, the integrals in equations  2.34  and  2.35  have led to practical difﬁculties for the implementation of Bayesian methods. The normalising constant in  2.34  is often unknown because analytic evaluation of the integral can only be performed for simple models.  A group of methods known as Markov chain Monte Carlo  MCMC  has proven to be effective at generating samples asymptotically from the posterior distribution  without knowing the normalising constant  from which inference about model parameters may be made by forming sample averages. MCMC methodology may be used to analyse complex problems, no longer requiring users to force the problem into an oversimpliﬁed framework for which analytic treatment is possible.  The Gibbs sampler We begin with a description of the Gibbs sampler, one of the most popular MCMC methods, and discuss some of the issues that must be addressed for practical implemen- tation. Both the Gibbs sampler and a more general algorithm, the Metropolis–Hastings algorithm, have formed the basis for many variants.  Let f .θ   denote the posterior distribution from which we wish to draw samples; θ is a p-dimensional parameter vector, . cid:6 1; : : : ;  cid:6  p T . We may not know f exactly, but we know a function g.θ  , where f .θ   D g.θ  =R g.θ   dθ. Let θ .i   be the set of parameters with the ith parameter removed; that is, θ .i   D f cid:6 1; : : : ;  cid:6 i cid:7 1,  cid:6 iC1; : : : ;  cid:6  pg. We assume that we are able to draw samples from the f . cid:6 ijθ .i   , derived from the normalisation of one-dimensional conditional distributions, g. cid:6 ijθ .i   , the function g regarded as a function of  cid:6 i alone, all other parameters being ﬁxed.  Gibbs sampling is a simple algorithm that consists of drawing samples from these  distributions in a cyclical way as follows.   Bayesian estimates 57  q2  3  0  2  1  q1  Figure 2.3 Gibbs sampler illustration  ; : : : ;  cid:6  0 p  Gibbs sampling To generate a sequence from f . cid:6 1; : : : ;  cid:6  p , choose an arbitrary starting value for θ ; θ 0 D . cid:6  0  T from the support of the prior posterior distribution. At stage t of the iter- 1 ation, ž draw a sample,  cid:6  tC1 ž draw a sample,  cid:6  tC1 ž and continue through the variables, ﬁnally drawing a sample,  cid:6  tC1  from f . cid:6 1j cid:6  t from f . cid:6 2j cid:6  tC1  ; : : : ;  cid:6  t p ; : : : ;  cid:6  t p  from f . cid:6  pj cid:6  tC1  ;  cid:6  t 3  ; : : : ;   ;   ;  1  2  2  1  p  1   cid:6  tC1 p cid:7 1   ;  After a large number of iterations, the vectors θ t behave like a random draw from the joint density f .θ    Bernardo and Smith, 1994 .  Figure 2.3 illustrates the Gibbs sampler for a bivariate distribution. The  cid:6 1 and  cid:6 2 components are updated alternately, producing moves in the horizontal and vertical di- rections. In the Gibbs sampling algorithm, the distribution of θ t given all previous values θ 0; θ 1; : : : ; θ t cid:7 1 depends only on θ t cid:7 1. This is the Markov property and the sequence generated is termed a Markov chain.  For the distribution of θ t to converge to a stationary distribution  a distribution that does not depend on θ 0 or t , the chain must be aperiodic, irreducible and positive recurrent. A Markov chain is aperiodic if it does not oscillate between different subsets in a regular periodic way. Recurrence is the property that all sets of θ values will be reached inﬁnitely often at least from almost all starting points. It is irreducible if it can reach all possible θ values from any starting point. Figure 2.4 illustrates a distribution that is uniform on .[0; 1]ð [0; 1] S.[1; 2]ð [1; 2] , the union of   58 Density estimation – parametric   cid:2    cid:6 2  2  1  θ 3   cid:1   cid:2  θ 1  cid:1  cid:2   θ0   cid:4    cid:3  θ 2  1   cid:1   cid:6 1  2  Figure 2.4 Illustration of a reducible Gibbs sampler  two non-overlapping unit squares. Consider the Gibbs sampler that uses the coordinate directions as sampling directions. For a point  cid:6 1 2 [0; 1], the conditional distribution of  cid:6 2 given  cid:6 1 is uniform over [0; 1]. Similarly for a point  cid:6 2 2 [0; 1], the conditional distribution of  cid:6 1 given  cid:6 2 is uniform over [0; 1]. We can see that if the starting point for  cid:6 1 is in [0; 1], then the Gibbs sampler will generate a point  cid:6 2, also in [0; 1]. The next step of the algorithm will generate a value for  cid:6 1, also in [0; 1], and so on. Therefore, successive values of θ t will be uniformly distributed on the square .[0; 1] ð [0; 1] . The square .[1; 2]ð [1; 2]  will not be visited. Conversely, a starting point in .[1; 2]ð [1; 2]  will yield a limiting distribution uniform on .[1; 2]ð[1; 2] . Thus the limiting distribution depends on the starting value and therefore is not irreducible. 0   the probability of moving from θ to θ 0   By designing a transition kernel K.θ ; θ  that satisﬁes detailed balance  time-reversibility  0  D f .θ  f .θ  K.θ ; θ  0 K.θ  0; θ    for all pairs of states .θ ; θ target distribution of interest, f .θ  .  0  in the support of f , then the stationary distribution is the  Summarisation After a sufﬁciently large number of iterations  referred to as the burn-in period , the samples fθ tg will be dependent samples from the posterior distribution f .θ  . These sam- ples may be used to obtain estimators of expectations using ergodic averages. Evaluation of the expectation of a function, h, of interest is achieved by the approximation  E[h.θ  ] ³  1  N  cid:7  M  NX tDMC1  h.θ t     2.36   where N is the number of iterations of the Gibbs sampler and M is the number of iterations in the burn-in period.   Bayesian estimates 59   2.37    2.38    2.39   Other summaries include plots of the marginal densities using some of the general nonparametric methods of density estimation, such as kernel methods  discussed in detail ; t D M C 1; : : : ; Ng is in Chapter 3 . The kernel density estimate of  cid:6 i given samples f cid:6  t  i  where the kernel K . cid:6 ;  cid:6 Ł  is a density centred at  cid:6 Ł. Choices for kernels and their widths are discussed in Chapter 3. Blackwellised estimator, makes use of the conditional densities f . cid:6 ijθ t  An alternative estimator, due to Gelfand and Smith  1990  and termed the Rao-  .i   ,  p. cid:6 i   D  1  N  cid:7  M  NX tDMC1  K . cid:6 i ;  cid:6  t i     p. cid:6 i   D  1  N  cid:7  M  NX tDMC1  f . cid:6 ijθ t  .i     This estimates the tails of the distribution better than more general methods of density estimation  O’Hagan, 1994 .  The Rao-Blackwellised estimator of E[h. cid:6 i  ] is then  E[h. cid:6 i  ] ³  1  N  cid:7  M  NX tDMC1  E[h. cid:6 i  jθ t  .i  ]  The difference between  2.39  and  2.36  is that  2.39  requires an analytic expression for the conditional expectation so that it may be evaluated at each step of the iteration. For reasonably long runs, the improvement in using  2.39  over  2.36  is small. If θ are the parameters of a density and we require p.xjD , we may estimate this by  approximating the integral in  2.20  using a Monte Carlo integration:  p.xjD  D  1  N  cid:7  M  NX tDMC1  p.xjθ t    Convergence In an implementation of Gibbs sampling, there are a number of practical considerations to be addressed. These include the length of the burn-in period, M; the length of the sequence, N ; and the spacing between samples taken from the ﬁnal sequence of iter- ations  the ﬁnal sequence may be subsampled in an attempt to produce approximately independent samples and to reduce the amount of storage required .  The length of the chain should be long enough for it to ‘forget’ its starting value and such that all regions of the parameter space have been traversed by the chain. The limiting distribution should not depend on its starting value, θ 0, but the length of the sequence will depend on the correlation between the variables. Correlation between the  cid:6 i s will tend to slow convergence. It can be difﬁcult to know when a sequence has converged as the Gibbs sampler can spend long periods in a relatively small region, thus giving the impression of convergence.  The most commonly used method for determining the burn-in period is by visually inspecting plots of the output values, θ t , and making a subjective judgement. More formal   60 Density estimation – parametric  tools, convergence diagnostics, exist and we refer to Raftery and Lewis  1996  Gelman  1996  and Mengersen et al.  1999  for further details of the most popular methods. However, convergence diagnostics do not tell when a chain has converged, but tell when it has not converged – sometimes.  There are various approaches for reducing correlation  and hence speeding up con-  vergence  including reparametrisation and grouping variables.  Reparametrisation transforms the set θ using a linear transformation to a new set φ with zero correlation between the variables. The linear transformation is calculated using an estimate of the covariance matrix based on a short initial sequence  in pattern recog- nition, the process of deriving a set of uncorrelated variables that is a linear combination of the original variables is principal components analysis, which we shall describe in Chapter 9 . The Gibbs sampler then proceeds using the variables φ, provided that it is straightforward to sample from the new conditionals f . cid:20 ijφ.i   . The process may be repeated until the correlation in the ﬁnal sequence is small, hopefully leading to more rapid convergence. Grouping variables means that at each step of the iteration a sample from a multivari- ate distribution f .θ ijθ .i    is generated, where θ i is a subvector of θ and θ .i   is the set of remaining variables. Provided correlations between variables are caused primarily by correlations between elements of the subvectors, with low correlations between subvec- tors, we can hope for more rapid convergence. A method for sampling from f .θ ijθ .i     which may be complex  is required.  Starting point The starting point is any point you do not mind having in the sequence. Preliminary runs, started where the last one ended, will give you some feel for suitable starting values. There is some argument to say that since the starting point is a legitimate point from the sequence  although perhaps in the tail of the distribution , it would be visited anyway by the Markov chain, at some stage; hence there is no need for burn-in. However, using a burn-in period and removing initial samples will make estimators approximately unbiased.  Parallel runs Instead of running one chain until convergence, it is possible to run multiple chains  with different starting values  as an approach to monitoring convergence, although more formal methods exist  Roberts, 1996; Raftery and Lewis, 1996 , as well as to obtain- ing independent observations from f .θ  . This is a somewhat controversial issue since independent samples are not required in many cases, and certainly not for ergodic av- eraging  equation  2.37  . Comparing several chains may help in identifying conver- gence. For example, are estimates of quantities of interest consistent between runs? In such cases, it is desirable to choose different starting values, θ 0, for each run, widely dispersed.  In practice, you will probably do several runs if computational resources permit, either to compare related probability models or to gain information about a chosen model such as burn-in length. Then, you would perform a long run in order to obtain samples for computing statistics.   Bayesian estimates 61  Sampling from conditional distributions The Gibbs sampler requires ways of sampling from the conditional distributions f .θ ijθ .i    and it is essential that sampling from these distributions is computationally efﬁcient. If f .θ ijθ .i    is a standard distribution, then it is likely that algorithms exist for drawing samples from it. For algorithms for sampling from some of the more common distribu- tions, see, for example, Devroye  1986  and Ripley  1987 .  As an example, consider the univariate equivalent of the distribution given by  equation  2.26 :  p.¼; 1=¦ 2  D N1.¼j¼n; ½n=¦ 2 Ga.1=¦ 2jÞn; þn  ¦ cid:7  1  ²   cid:8 Þn cid:7 1    1 ¦  exp   cid:7  ½ C n  2¦ 2  .¼  cid:7  ¼n 2  ¦ 2  ¦  ²  cid:7  þn ¦ 2  exp  =[.Þn  cid:7  1 2.Þn  cid:7  2 ], Þn > 2.  Given ¦ 2, ¼ is normally distributed with mean ¼n  the prior updated by the data samples – see  2.25   and variance ¦ 2=.½ C n ; given the mean, 1=¦ 2 has a gamma ; þn C ½n.¼  cid:7  ¼n 2=2  , with marginal distribution  see Appendix E  Ga.1=¦ 2jÞn C 1  integrating over ¼  Ga.1=¦ 2jÞn; þn . The mean of ¦ 2 is þn=.Þn  cid:7  1 , Þn > 1, and variance þ2 n The data, xi , comprise 20 points from a normal distribution with zero mean and unit variance. The priors are ¼ ¾ N1.¼0; ½=¦ 2 ; 1=¦ 2 ¾ Ga.Þ; þ . The parameters of the prior distribution of ¼ and ¦ 2 are taken to be ½ D 1; ¼0 D 1; Þ D 1=2; þ D 2. For this example, the true posteriors of ¼ and ¦ 2 may be calculated; ¼ has a t distribution and the inverse of ¦ 2 has a gamma distribution. The mean and the variance of the true posteriors of ¼ and ¦ 2 are given in Table 2.2.  2  A Gibbs sampling approach is taken for generating samples from the joint posterior density of ¼ and ¦ 2. The steps are to initialise ¦ 2  in this example, a sample is taken from the prior distribution  and then sample ¼; then for a given ¼, sample ¦ 2, and so on, although this is not necessarily the best approach for a diffuse prior.  Figure 2.5 shows the ﬁrst 1000 samples in a sequence of ¼ and ¦ 2 samples. Taking the ﬁrst 500 samples as burn-in, and using the remainder to calculate summary statistics,  Table 2.2 Summary statistics for ¼ and ¦ 2. The true values are calculated from the known marginal poste- rior densities. The short-run values are calculated from a Gibbs sampler run of 1000 samples less a burn-in of 500 samples. The long-run values are calculated after a run of 100 000 samples, less a burn-in of 500 samples Long run mean ¼  cid:7 0.10029  cid:7 0.10274  cid:7 0.10042 var ¼ 0.05781 mean ¦ 2 1.2166 var ¦ 2 0.17260  0.05738 1.236 0.20090  0.05796 1.217 0.17428  Short run  True   62 Density estimation – parametric  1 0.75 0.5 0.25 0 −0.25 −0.5 −0.75  4  3  2  1  0  200  400  600  800  1000  0  200  400  600  800  1000  Figure 2.5 One thousand iterations from Gibbs sampler for a normal-inverse gamma posterior density; left: samples of ¼; right: samples of ¦ 2  gives values for the mean and variance of ¼ and ¦ 2 that are close to the true values  calculated analytically ; see Table 2.2. A longer sequence gives values closer to the truth.  Rejection sampling If the conditional distribution is not recognised to be of a standard form for which ef- f .θ   D ﬁcient sampling exists, then other sampling schemes must be employed. Let g.θ  =R g.θ   dθ be the density from which we wish to sample. Rejection sampling uses a density s.θ   from which we can conveniently sample  cheaply  and requires that g.θ  =s.θ   is bounded. Let an upper bound of g.θ  =s.θ   be A.  The rejection sampling algorithm is as follows.  Rejection sampling algorithm Repeat ž sample a point θ from the known distribution s.θ  ; ž sample y from the uniform distribution on [0; 1]; ž if Ay  cid:10  g.θ  =s.θ   then accept θ; until one θ is accepted.  Depending on the choice of s, many samples could be rejected before one is accepted. If s is close to the shape of g.θ  , so that g.θ  =s.θ   ¾ A for all θ, then the acceptance condition is almost always accepted.  The distribution of the samples, θ, generated is f .θ  .  Ratio of uniforms Let D denote the region in R 2 such that  D D f.u; v ; 0  cid:10  u  cid:10  pg.v=u g;  then sampling a point uniformly from D and taking  cid:6  D v=u gives a sample from the density proportional to g. cid:6   , namely g. cid:6   =R g. cid:6    d cid:6 .   Bayesian estimates 63  R  D  v  v+  v−  0  Figure 2.6 Envelope rectangle, R, for the region D deﬁned by D D f.u; v ; 0  cid:10  u  cid:10  pg.v=u g  u+  u  A sample from D could be drawn by a simple application of rejection sampling: sample uniformly from the rectangle, R, bounding D  see Figure 2.6  and if .u; v  is in D then accept.  Metropolis–Hastings algorithm The Metropolis–Hastings algorithm is a widely used technique for sampling from distri- butions for which the conditional densities cannot be computed or are of a form from which it is difﬁcult to sample. It uses a proposal distribution, from which sampling is easy, and accepts a sample with a probability that depends on the proposal distribution and the  unnormalised  density from which we wish to sample. Let θ t be the current sample. In the Metropolis–Hastings algorithm, a proposal distri- bution, which may depend on θ t , is speciﬁed. We denote this q.θjθ t  . The Metropolis– Hastings algorithm is as follows.  Metropolis–Hastings algorithm ž Sample a point θ from the proposal distribution q.θjθ t  . ž Sample y from the uniform distribution on [0; 1]. ž If  y  cid:10  min   cid:7  1;   cid:8   g.θ  q.θ tjθ   g.θ t  q.θjθ t    then accept θ and set θ tC1 D θ, else reject θ and set θ tC1 D θ t .  It produces a different Markov chain than Gibbs sampling, but with the same limiting  distribution, g.θ  =R g.θ   dθ.   64 Density estimation – parametric  The proposal distribution can take any sensible form and the stationary distribution will still be g.θ  =R g.θ   dθ. For example, q.XjY   may be a multivariate normal distri- bution with mean Y and ﬁxed covariance matrix,  cid:2 . However, the scale of  cid:2  will need to be chosen carefully. If it is too small, then there will be a high acceptance rate, but poor mixing; that is, the chain may not move rapidly throughout the support of the target distribution and will have to be run for longer than necessary to obtain good estimates from equation  2.36 . If the scale of  cid:2  is too large, then there will be a poor acceptance rate, and so the chain may stay at the same value for some time, again leading to poor mixing. For symmetric proposal distributions, q.XjY   D q.YjX , the acceptance probability  reduces to   cid:7  1;  min   cid:8   g.θ   g.θ t    and, in particular, for q.XjY   D q.jX  cid:7  Yj , so that q is a function of the difference between X and Y only, the algorithm is the random-walk Metropolis algorithm.  Single-component Metropolis–Hastings The Metropolis–Hastings algorithm given above updates all components of the parameter vector, θ, in one step. An alternative approach is to update a single component at a time.  Single-component Metropolis–Hastings algorithm At stage t of the iteration, do the following. ž Draw a sample, Y from the proposal distribution q. cid:6 j cid:6  t  ; : : : ;  cid:6  t p   .  1  – Accept the sample with probability g.Yj cid:6  t 1j cid:6  t g. cid:6  t  Þ D min    1;  2  2  ; : : : ;  cid:6  t p ; : : : ;  cid:6  t p   q. cid:6  t  q.Yj cid:6  t  1jY;  cid:6  t 2 ;  cid:6  t 2  1     ; : : : ;  cid:6  t p ; : : : ;  cid:6  t p     !  ž Continue through the variables as in the Gibbs sampler, ﬁnally drawing a sample, Y ,  1 D  cid:6  t 1. ; : : : ;  cid:6  tC1 p cid:7 1  ;  cid:6  t p   .  – If Y is accepted, then  cid:6  tC1  1 D Y , else  cid:6  tC1  from the proposal distribution q. cid:6 j cid:6  tC1 1 – Accept the sample with probability g.Yj cid:6  tC1 ; : : : ;  cid:6  tC1 p cid:7 1 pj cid:6  tC1 ; : : : ;  cid:6  tC1 g. cid:6  t p cid:7 1 p D  cid:6  t p D Y , else  cid:6  tC1 – If Y is accepted, then  cid:6  tC1 p.  Þ D min    1;  1  1  pj cid:6  tC1  q. cid:6  t  q.Yj cid:6  tC1  1  ; : : : ;  cid:6  tC1 p cid:7 1 ; : : : ;  cid:6  tC1 p cid:7 1  1  ; Y    ;  cid:6  t p     !  In this single-component update case, for proposal distributions that are the condi-  tionals of the multivariate distribution that we wish to sample,  q. cid:6 ijθ .i    D f . cid:6 ijθ .i     then the sample is always accepted and the algorithm is identical to Gibbs sampling.   Bayesian estimates 65  Choice of proposal distribution in Metropolis–Hastings If the distribution that we wish to approximate, f , is unimodal, and is not heavy-tailed  loosely, heavy-tailed means that it tends to zero more slowly than the exponential, but also the term is used to describe distributions with inﬁnite variance , then an appropriate choice for the proposal distribution might be normal, with parameters chosen to be a best ﬁt of log.q  to log.g    f D g=R g . For more complex distributions, the proposal could be a multivariate normal, or mixtures of multivariate normal, but for distributions with heavy tails, Student t distributions  see Appendix E  might be used. For compu- tational efﬁciency, q should be chosen so that it can be easily sampled and evaluated. Often a random-walk algorithm is used  symmetric proposal distribution , and can give good results.  Data augmentation Introducing auxiliary variables can often lead to more simple and efﬁcient MCMC sam- pling methods, with improved mixing. If we require samples from a posterior p.θjD , then the basic idea is to notice that it may be easier to sample from p.θ ; φjD , where φ is a set of auxiliary variables. In some applications, the choice of φ may be obvious, in others some experience is necessary to recognise suitable choices. The distribution p.θjD  is then simply a marginal of the augmented distribution, p.θ ; φjD , and the method of sampling is termed the data augmentation method. Statistics concerning the distribution of p.θjD  can be obtained by using the θ components of the samples of the augmented parameter vector .θ ; φ  and ignoring the φ components. One type of problem where data augmentation is used is that involving missing data. Suppose that we have a data set D and some ‘missing values’, φ. In a classiﬁcation problem, where there are some unlabelled data available for training the classiﬁer, φ represents the class labels of these data. Alternatively, there may be incomplete pattern vectors; that is, for some patterns, measurements on some of the variables may be absent.  The posterior distribution of parameters θ is given by  Z  p.θjD     p.D; φjθ   dφ p.θ    However, it may be difﬁcult to marginalise the joint density p.D; φjθ   and it is simpler to obtain samples of the augmented vector .θ ; φ . In this case,  p.θjD; φ   p.φjθ ;D   is the posterior based on the complete data, which is easy to sample, either directly or by use of MCMC methods  for example, Metropolis– Hastings ;  is the sampling distribution for the missing data; again, typically easy to sample.  Examples of missing data problems are given in Section 2.4.3.  Example In this example, we seek to model a time series as a sum of k sinusoids of unknown amplitude, frequency and phase . ; !;  cid:20  . The approach and example are based on work   66 Density estimation – parametric  by Andrieu and Doucet  1999 . We assume a model of the form  y D h.x; ξ   C ž D kX jD1    j cos.! j x C  cid:20  j   C ž;   2.40   where ž ¾ N .0; ¦ 2  and ξ D f.  j ; ! j ;  cid:20  j  ; j D 1; : : : ; kg; thus  p.yjx; θ   D  1p 2³ ¦ 2  expf cid:7 .y  cid:7  h.x; ξ   2=.2¦ 2 g  where the parameters of the density are θ D .ξ ; ¦ 2 . The training data, D D fyi ; i D 1; : : : ; ng, comprise n measurements of y at regular intervals, xi D i, for i D 0; 1; : : : , n  cid:7  1. Assuming independent noise samples, we have  p.Djθ     nY iD1  1 ¦  expf cid:7 .yi  cid:7  h.xi ; ξ   2=.2¦ 2 g  What we would like now is information about the parameters given the data set and the model for predicting y given a new sample xn. Information about the parameters θ requires speciﬁcation of a prior distribution, p.θ  . Then using Bayes’ theorem, we have  For predicting a new sample, we take  p.θjD    p.Djθ  P.θ    p.yjxn  ³ NX tDMC1  p.yjxn; θ t    where M is the burn-in period; N is the length of the sequence; θ t are the parameters at stage t.  It is convenient to reparametrise the model as  yi D kX jD1  fg j cos.! j xi   C h j sin.! j xi  g C ži ;  where g j D   j cos. cid:20  j   and h j D  cid:7   j sin. cid:20  j   represent the new amplitudes of the problem, which lie in the range . cid:7 1;1 . This may be written as  where yT D .y1; : : : ; yn ; aT D .g1; h1; : : : ; gk ; hk   is the 2k-dimensional vector of amplitudes, and D is an n ð 2k matrix, deﬁned by:  y D Da C  cid:25 ;  Di; j D  ² cos.! j xi   sin.! j xi    j odd j even  Data Data are generated according to the model  2.40 , with k D 3, n D 64, f! jg D p 20 , ¦ D 2:239 and f cid:20  jg D 2³.0:2; 0:2 C 1=n; 0:2 C 2=n , f  jg D . .0; ³=4; ³=3 ; the time series is shown in Figure 2.7.  p 2³ ;  p 20;   Bayesian estimates 67  sample, 200  15  10  5  0 −5 −10 −15  10  5  0  −5  −10  0  10  20  30  40  50  60  0  10  20  30  40  50  60  Figure 2.7 Data for sinusoids estimation problem  left ; underlying model  solid line  with re- construction, based on the 200th set of MCMC samples  dashed line, right   Prior The prior distribution for the random variables, .!; ¦ 2; a , is written as  p.!; ¦ 2; a  D p.!  p.¦ 2  p.aj!; ¦ 2 ;  where ! D f! jg. Speciﬁcally, p.!  D 1  ³ k I[! 2 [0; ³]k]  aj!; ¦ 2 ¾ N2k .0; .¦ 2 cid:2   cid:7 1 ; where  cid:2    cid:7 1 D Ž cid:7 2DT D  ¦ 2 ¾ Ig.¹0=2;  cid:13 0=2   for parameters, Ž2; ¹0 and  cid:13 0. Values of Ž2 D 50, ¹0 D 0:01 and  cid:13 0 D 0:01 have been used in the illustrated example.  Posterior The posterior distribution can be rearranged to: ½  cid:25  cid:7 . cid:13 0 C yT P y   1  p.a; !; ¦ 2jD     I[! 2 [0; ³]k]  2 CkC1Ð exp  ¦ 2 cid:23  nC¹0 ð j cid:2 j cid:7 1=2 exp  2¦ 2  2¦ 2  " cid:7 .a  cid:7  m T M cid:7 1.a  cid:7  m      2.41   where   cid:7 1 D DT D C  cid:2   and P D I n  cid:7  DMDT The amplitude, a, and variance, ¦ 2, can be integrated out analytically, giving:   cid:7 1; m D MDT y;  M  p.!jD    . cid:13 0 C yT P y  cid:7  nC¹0  2  This cannot be dealt with analytically so samples are drawn, by sampling from the conditional distributions of the individual components, ! j , using Metropolis–Hastings sampling, which uses  p.! jj!. j  ;D    p.!jD   where !. j   is the set of variables with the jth one omitted.   68 Density estimation – parametric  0.2  0.15  0.1  0.05  0  0  10  20  30  40  50  60  Figure 2.8 Proposal distribution  Proposal At each update choose randomly between two possible proposal distributions. The ﬁrst, chosen with probability 0.2, is given by:  q j .!0  jj!    n cid:7 1X  plI  lD0   cid:25 l³ n  < !0 j  <  .l C 1 ³  ½  ;  n  where pl is the squared modulus of the Fourier transform of the data  see Figure 2.8  at frequency l³=n  this proposal aims to prevent the Markov chain getting stuck with one solution for ! j  . The second is the normally distributed random walk, N .0; ³=.2n    ensuring irreducibility of the Markov chain .  Results Having drawn samples for !, the amplitudes can be sampled using  which comes directly from  2.41 . The noise variance can also be sampled using  aj!; ¦ 2;D ¾ N .m; ¦ 2M   ¦ 2j!;D ¾ Ig   cid:7  n C ¹0 2  ;   cid:13 0 C yT P y   cid:8   2  which comes from  2.41  after analytical integration of a.  The algorithm is initialised with a sample from the prior for !. Convergence for the illustrated example was very quick, with a burn-in of less than 100 iterations required. Figure 2.9 gives plots of samples of the noise, ¦  true value, 2.239 , and frequencies, ! j . Figure 2.7 shows the reconstruction of the data using 200th set of MCMC samples.  Summary MCMC methods can provide effective approaches to inference problems in situations where analytic evaluation of posterior probabilities is not feasible. Their main strength is in their ﬂexibility. They enable Bayesian approaches to be adapted to real-world problems without having to make unnecessarily restrictive assumptions regarding prior   Bayesian estimates 69  3.2  3  2.8  2.6  2.4  2.2  2  1.6  1.5  1.4  1.3  1.2  d e z i l a e r  q e r f   r e i r r a c  0  2000  4000  6000  8000  10000  sample  0  2000  4000  6000  8000 10000  Figure 2.9 Ten thousand iterations from MCMC sampler; left: samples of the noise; right: samples of the frequencies  distributions which may make the mathematics tractable. Originating in the statisti- cal physics literature, the development in Bayesian statistics has been driven by the difﬁculty in performing numerical integration. The main disadvantage concerns uncer- tainty over convergence, and hence over the accuracy of estimates computed from the samples.  In many respects, the implementation of these methods is still something of an art, with several trial runs being performed in order to explore models and parameter values. Techniques are required for reducing the amount of computation per iteration. Run times can be long, caused by poor mixing.  The main features of the MCMC method are as follows.  1. It performs iterative sampling from a proposal distribution. The samples may be uni- variate, multivariate or a subvector of the parameter vector. A special case is Gibbs sampling when samples from conditional probability density functions are made.  2. The samples provide a summary of the posterior probability distribution. They may be used to calculate summary statistics either by averaging functions of the sam- ples  equation  2.36   or by averaging conditional expectations  Rao-Blackwellisation, equation  2.39  .  3. Correlated variables lead to longer convergence.  4. The parameters of the method are N , the sequence length; M, the burn-in period;  q.:j: , the proposal distribution; and s, the subsampling factor.  5. In practice, you would run several chains to estimate parameter values and then one  long chain to calculate statistics.  6. Subsampling of the ﬁnal chain may be performed to reduce the amount of storage  required to represent the distribution.  7. Sampling from standard distributions is readily performed using algorithms in the books by Devroye  1986  and Ripley  1987 , for example. For non-standard distributions, the rejection methods and ratio-of-uniforms methods may be used as well as Metropolis–Hastings, but there are other possibilities  Gilks et al., 1996 .   70 Density estimation – parametric  2.4.3 Bayesian approaches to discrimination  In this section we apply the Bayesian learning methods of Section 2.4.1 to the discrimi- nation problem, making use of analytic solutions where we can, but using the numerical techniques of the previous section where that is not possible. Let D denote the data set used to train the classiﬁer. In the ﬁrst instance, let it comprise a set of labelled patterns f.xi ; zi  ; i D 1; : : : ; ng, where zi D j implies that the pattern xi is in class ! j . Given pattern x from an unknown class, we would like to predict its class membership; that is, we require  p.z D jjD; x   j D 1; : : : ; C  where z is the class indicator variable corresponding to x. The Bayes decision rule for minimum error is to assign x to the class for which p.z D jjD; x  is the greatest. The above may be written  compare with equation  2.32    p.z D jjD; x    p.xjD; z D j   p.z D jjD    2.42  where the constant of proportionality does not depend on class. The ﬁrst term, p.xjD; z D j  , is the probability density of class ! j evaluated at x. If we assume a model for the density, with parameters  cid:28  j , then by  2.20  this may be written  p.xjD; z D j   D  p.xj cid:28  j   p. cid:28  jjD; z D j   d cid:28  j  Z  For certain special cases of the density model, p.xj cid:28  j  , we may evaluate this analyti- cally. For example, as we have seen in Section 2.4.1, a normal model with parameters µ and  cid:2  with Gauss–Wishart priors leads to a posterior distribution of the parame- ters that is also Gauss–Wishart and a multivariate Student t distribution for the density p.xjD; z D j  .  If we are unable to obtain an analytic solution, then a numerical approach will be required. For example, if we use one of the MCMC methods of the previous section to draw samples from the posterior density of the parameters, p. cid:28  jjD; z D j  , we may approximate p.xjD; z D j   by  p.xjD; z D j   ³  1  N  cid:7  M  NX tDMC1  p.xj cid:28 t     j   2.43   where  cid:28 t period and run length, respectively.  j are samples generated from the MCMC process and M and N are the burn-in The second term in  2.42  is the probability of class ! j given the data set, D. Thus, it is the prior probability updated by the data. We saw in Section 2.4.1 that if we assume Dirichlet priors for ³i , the prior probability of class !i , that is,  p.π   D DiC .πja0    Bayesian estimates 71  then  p.z D jjD  D E[³ jjD] D  a0 j C n j PC .a0 j C n j   jD1   2.44   Unlabelled training data The case of classiﬁer design using a normal model when the training data comprise both labelled and unlabelled patterns is considered by Lavine and West  1992 . It provides an example of the Gibbs sampling methods that uses some of the analytic results of Section 2.4.1. We summarise the approach here. i are the unlabelled patterns. Let µ D fµi ; i D 1; : : : ; Cg and  cid:2  D f cid:2 i ; i D 1; : : : ; Cg be the set of class means and covariance matrices and π the class priors. Denote by zu D fzu The parameters of the model are  cid:6  D fµ;  cid:2 ; π ; zug. Taking a Gibbs sampling ap-  Let the data set D D f.xi ; zi  ; i D 1; : : : ; n; xu  ; i D 1; : : : ; nug, the set of unknown class labels.  ; i D 1; : : : ; nug, where xu  i  i  proach, we successively draw samples from three conditional distributions. 1. Sample from p.µ;  cid:2 jπ ; zu ;D . This density may be written p.µi ;  cid:2 ijzu ;D ;  p.µ;  cid:2 jπ ; zu ;D  D CY iD1  the product of C independent Gauss–Wishart distributions given by  2.26 .  2. Sample from p.πjµ;  cid:2 ; zu ;D . This is Dirichlet DiC .πja , a D a0 C n, independent of µ and  cid:2 , with n D .n1; : : : ; nC  , where n j is the number of patterns in class ! j as determined by D and zu.  3. Sample from p.zujµ;  cid:2 ; π ;D . Since the samples zu  i are conditionally independent,  we require samples from  p.zu  i D jjµ;  cid:2 ; π ;D    ³ j p.xijµ j ;  cid:2  j ; zu  i D j     2.45   is then trivial.  the product of the prior and the normal density of class ! j at xi . The constant of proportionality is chosen so that the sum over classes is unity. Sampling a value for zu i The Gibbs sampling procedure produces a set of samples fµt ;  cid:2 t ; π t ; .zu  t ; t D 1; : : : ; Ng, which may be used to classify the unlabelled patterns and future observa- tions. To classify the unlabelled patterns in the training set, we use  p.zu  i D jjD  D  1  N  cid:7  M  NX tDMC1  p.zu  i D jjµt  j  ;  cid:2 t j  ; ³ t j  ;D   where the terms in the summation are, by  2.45 , products of the prior and class- conditional density  normalised , evaluated for each set of parameters in the Markov chain. To classify a new pattern x, we require p.z D jjx;D , given by  p.z D jjx;D    p.z D jjD  p.xjD; z D j     2.46    72 Density estimation – parametric  where the ﬁrst term in the product, p.z D jjD , is  p.z D jjD  D E[³ jjD] D  1  N  cid:7  M  NX tDMC1  E[³ jjD; .zu  t ]   2.47   The expectation in the summation can be evaluated using  2.44 . The second term can be written, by  2.43 , as  p.xjD; z D j   ³  1  N  cid:7  M  NX tDMC1  p.xjD; .zu  t ; z D j    the sum of Student t distributions.  Illustration The illustration given here is based on that of Lavine and West  1992 . Two-dimensional data from three classes are generated from equally weighted non-normal distributions. Deﬁning matrices  C1 D   cid:8    cid:7  5 1 3 5  C2 D   cid:8    cid:7  0 1 1 5  C3 D   cid:8    cid:7  5 0 3 1  then an observation from class !i is generated according to  x j D Ci   cid:8    cid:7  w j 1  cid:7  w j  C  cid:25  j  where w j diagonal covariance matrix, I =2.  is uniform over [0; 1] and  cid:25  j  is normally distributed with zero mean and  The labelled training data are shown in Figure 2.10. Using a normal model, with diagonal covariance matrix, for the density of each class, an MCMC approach using Gibbs sampling is taken. The training set consists of 1200 labelled and 300 unlabelled patterns. The priors for the mean and variances are normal and inverse gamma, respectively. The parameter values are initialised as samples from the prior. Figure 2.11 shows components of the mean and covariance matrix that are produced from the chain. The package WinBugs  Lunn et al., 2000  has been used to complete the MCMC sampling.  2.4.4 Example application study  The problem This application concerns the classiﬁcation of mobile ground targets from inverse synthetic aperture radar  ISAR  images  Copsey and Webb, 2001 .  Summary This study follows the approach above, with the addition that the class- conditional densities are themselves mixtures.   Bayesian estimates 73  6  5  4  3  2  1  −1  mu[1,1]  3.2 3.1 3.0 2.9 2.8 2.7  1.4  1.3  1.2  1.1  1  2  3  4  5  6  Figure 2.10 Three-class, two-dimensional training data  mu[2,2]  3.3 3.2 3.1 3.0 2.9 2.8  1.5 1.4 1.3 1.2 1.1  10850  10950  10900 iteration  10850  10950  10900 iteration  sigma[1,1]  sigma[2,2]  10850  10950  10900 iteration  10850  10950  10900 iteration  Figure 2.11 MCMC samples of µ and  cid:2 . Top left: ¼1 component of class 1; top right: ¼2 component of class 2; bottom left: 61,1 component of class 1; bottom right: 62,2 component of class 2  The data The data comprise ISAR images of three types of vehicle, gathered over a complete rotation of the vehicle on a turntable. An ISAR image is an image generated by processing the signals received by the radar. One axis corresponds to range  distance from the radar ; the second axis corresponds to cross-range. The training data consist   74 Density estimation – parametric  of approximately equal numbers of images  about 2000  per class, collected over single complete rotations at a constant depression angle. The test data comprise six sets of approximately 400 ISAR images collected from single rotations of six vehicles. The images are 38 pixels in range by 26 pixels in cross-range.  The model  The probability density function of the data is written as  where ³ D .³1; : : : ; ³C   is the set of prior class probabilities and p.xj j   is the class- conditional probability density of class j, which is also modelled as a mixture, with the jth class having R j mixture components  termed subclasses   p.x  D CX jD1  ³ j p.xj j    p.xj j   D R jX rD1  ½ j;r p.xj j; r    where ½ j D .½ j;1; : : : ; ½ j;R j   represents the prior subclass probabilities within class j; that is, ½ j;r is the mixing probability for the rth subclass of the jth class, satisfying PR j ½ j;r D 1. The probability density of the data for the subclass r of class j, p.xj j; r  , rD1 is taken to be normal with mean ¼ j;r and covariance matrix 6 j;r . Let ¼ D f¼ j;rg; 6 D f6 j;rg.  Training procedure A Gibbs sampling approach is taken. The random variable set, f³; ½; ¼; 6g is augmented by allocation variables fz; Zg such that .zi D j; Zi D r   implies that observation xi is modelled as being drawn from subclass r of class j; zi is known for labelled training data; Zi is always unknown. Let D denote the measurements and known allocations; zu, the set of unknown class labels; Z D .Z1; : : : ; Zn  the subclass allocation labels. The stages in the Gibbs sampling iterations are as follows. 1. Sample from p.¼; 6j³; ½; zu; Z ;D . 2. Sample from p.³; ½j¼; 6; zu; Z ;D . 3. Sample from p.zu ; Zj¼; 6; ³; ½;D .  Future observations, x are classiﬁed by evaluating p.z D jjxD ,  p.z D jjx;D  D p.z D jjD  p.xjD; z D j    where the ﬁrst term in the product, p.z D jjD , is evaluated using equation  2.47  and the second term is approximated as  p.xjD; z D j   ³  1  N  cid:7  M  NX tDMC1  p.xjD; Z t ; .zu  t ; z D j     Application studies 75  where p.xjD; Z t ; .zu  t ; z D j   is written as a mixture  p.xjD; Z t ; .zu  t ; z D j   D R jX rD1 ð p.Z D rjD; Z t ; .zu  t ; z D j    p.xjD; Z t ; .zu  t ; z D j; Z D r    in which p.Z D rjD; Z t ; .zu  t ; z D j   D E[½ j;rjD; .zu t ; Z t ] and p.xjD; Z t ; .zu  t ; z D j; Z D r   is the predictive density for a data point drawn from subclass r of class j, the parameters being determined using the MCMC algorithm outputs. This predictive distribution is also shown to be a product of Student t distributions. A ﬁxed model order is adopted  R j D 12 for all classes ; the data are preprocessed to produce 35 values  on the principal components ; the burn-in period is 10 000 iterations; 1000 samples are drawn to calculate statistics; the decorrelation gap is 10 iterations.  2.4.5 Further developments  There are many developments of the basic methodology presented in this section, par- ticularly with respect to computational implementation of the Bayesian approach. These include strategies for improving MCMC; monitoring convergence; and adaptive MCMC methods. A good starting point is the book by Gilks et al  1996 .  Developments of the MCMC methodology to problems when observations arrive sequentially and one is interested in performing inference on-line are described by Doucet et al.  2001 .  A Bayesian methodology for univariate normal mixtures that jointly models the num- ber of components and the mixture component parameters is presented by Richardson and Green  1997 .  2.4.6 Summary  A Bayesian approach to density estimation can only be treated analytically for simple distributions. For problems in which the normalising integral in the denominator of the expression for a posterior density cannot be evaluated analytically, Bayesian computa- tional methods must be employed.  Monte Carlo methods, including the Gibbs sampler, can be applied routinely, al- lowing efﬁcient practical application of Bayesian methods, at least for some problems. Approaches to discrimination can make use of unlabelled test samples to reﬁne models. The procedure described in Section 2.4.3 implements an iterative procedure to classify test data. Although the procedure is attractive in that it uses the test data to reﬁne knowl- edge about the parameters, its iterative nature may prevent its application in problems with real-time requirements.  2.5 Application studies  The application of the normal-based linear and quadratic discriminant rules covers a wide range of problems. These include the areas of:   76 Density estimation – parametric  ž Medical research. Aitchison et al.  1977  compare predictive and estimative approaches to discrimination. Harkins et al.  1994  use a quadratic rule for the classiﬁcation of red cell disorders. Hand  1992  reviews statistical methodology in medical research, including discriminant analysis  see also Jain and Jain, 1994 . Stevenson  1993  dis- cusses the role of discriminant analysis in psychiatric research.  ž Machine vision. Magee et al.  1993  use a Gaussian classiﬁer to discriminate bottles  based on ﬁve features derived from images of the bottle tops.  ž Target recognition. Kreithen et al.  1993  develop a target and clutter discrimination  algorithm based on multivariate normal assumptions for the class distributions.  ž Spectroscopic data. Krzanowski et al.  1995  consider ways of estimating linear dis- criminant functions when covariance matrices are singular and analyse data consisting of infrared reﬂectance measurements.  ž Radar. Haykin et al.  1991  evaluate a Gaussian classiﬁer on a clutter classiﬁcation problem. Lee et al.  1994  develop a classiﬁer for polarimetric synthetic aperture radar imagery based on the Wishart distribution.  ž As part of a study by Aeberhard et al.  1994 , regularised discriminant analysis was one of eight discrimination techniques  including linear and quadratic discriminant analysis  applied to nine real data sets. An example is the wine data set – the results of a chemical analysis of wines from the same region of Italy, but derived from different varieties of grape. The feature vectors were 13-dimensional, and the training set was small, comprising only 59, 71 and 48 samples in each of three classes. Since there is no separate test set, a leave-one-out procedure  see Chapter 8  was used to estimate error rate. On all the real data sets, RDA performed best overall.  Comparative studies of normal-based models with other discriminant methods can be found in the papers by Curram and Mingers  1994 ; Bedworth et al.  1989  on a speech recognition problem; and Aeberhard et al.  1994 .  Applications of mixture models include:  ž Plant breeding. Jansen and Den Nijs  1993  use a mixture of normals to model the  distribution of pollen grain size.  ž Image processing. Luttrell  1994  uses a partitioned mixture distribution for low-level  image processing operations.  ž Speech recognition. Rabiner et al.  1985 , and Juang and Rabiner  1985  describe a hidden Markov model approach to isolated digit recognition in which the probability density function associated with each state of the Markov process is a normal mixture model.  ž Handwritten character recognition. Revow et al.  1996  use a development of con- ventional mixture models  in which the means are constrained to lie on a spline  for handwritten digit recognition. Hastie and Tibshirani  1996  apply their mixture discriminant analysis approach to the classiﬁcation of handwritten 3s, 5s and 8s.  ž Motif discovery in biopolymers. Bailey and Elkan  1995  use a two-component mix- ture model to identify motifs  a pattern common to a set of nucleic or amino acid subsequences which share some biological property of interest  in a set of unaligned genetic or protein sequences.   Summary and discussion 77  ž Face detection and tracking. In a study of face recognition  McKenna et al., 1998 , data characterising each subject’s face  20- and 40-dimensional feature vectors  are modelled as a Gaussian mixture, with component parameters estimated using the EM procedure. Classiﬁcation is performed by using these density estimates in Bayes’ rule.  A compilation of examples of applications of Bayesian methodology is given in the book by French and Smith  1997 . This includes applications in clinical medicine, ﬂood damage analysis, nuclear plant reliability and asset management.  2.6 Summary and discussion  The approaches developed in this chapter towards discrimination have been based on estimation of the class-conditional density functions using parametric and semiparametric techniques. It is certainly true that we cannot design a classiﬁer that performs better than the Bayes discriminant rule. No matter how sophisticated a classiﬁer is, or how appealing it may be in terms of reﬂecting a model of human decision processes, it cannot achieve a lower error rate than the Bayes classiﬁer. Therefore a natural step is to estimate the components of the Bayes rule from the data, namely the class-conditional probability density functions and the class priors.  In Section 2.2, we gave a short introduction to discrimination based on normal models. The models are easy to use and have been widely applied in discrimination problems. In Section 2.3 we introduced the normal mixture model and the EM algorithm. We will return to this in Chapter 10 when we shall consider such models for clustering. Section 2.4 considered Bayesian approaches to discrimination  which take into account parameter variability due to sampling , and Bayesian computational procedures that produce samples from the posterior distributions of interest were described. Such techniques remove the mathematical nicety of conjugate prior distributions in a Bayesian analysis, allowing models to be tailored to the beliefs and needs of the user.  2.7 Recommendations  An approach based on density estimation is not without its dangers of course. If incorrect assumptions are made about the form of the distribution in the parametric approach  and in many cases we will not have a physical model of the data generation process to use  or data points are sparse leading to poor estimates, then we cannot hope to achieve optimal performance. However, the linear and quadratic rules are widely used, simple to implement and have been used with success in many applications. Therefore, it is worth applying such techniques to provide at least a baseline performance on which to build. It may prove to be sufﬁcient.  2.8 Notes and references  A comparison of the predictive and estimative approaches is found in the articles by Aitchison et al.  1977  and Moran and Murphy  1979 . McLachlan  1992a  gives a   78 Density estimation – parametric  very thorough account of normal-based discriminant rules and is an excellent source of reference material. Simple procedures for correcting the bias of the discriminant rule are also given. Mkhadri et al.  1997  provide a review of regularisation in discriminant analysis.  Mixture distributions, and in particular the normal mixture model, are discussed in a number of texts. The book by Everitt and Hand  1981  provides a good introduction, and a more detailed treatment is given by Titterington et al.  1985   see also McLachlan and Basford, 1988 . A thorough treatment, with recent methodological and computational developments, applications and software description is presented by McLachlan and Peel  2000 . Lavine and West  1992  discuss Bayesian approaches to normal mixture models for discrimination and classiﬁcation, with posterior probabilities obtained using an iter- ative resampling technique  see also West, 1992 . Several approaches for determining the number of components of a normal mixture have been proposed and are discussed further in the context of clustering in Chapter 10. A review of mixture densities and the EM algorithm is given by Redner and Walker  1984 . A thorough description of the EM algorithm and its extensions is provided in the book by McLachlan and Krishnan  1996 . See also the review by Meng and van Dyk  1997 , where the emphasis is on strategies for faster convergence. Software for the ﬁtting of mixture models is publicly available. Bayesian learning is discussed in many of the standard pattern recognition texts includ- ing Fu  1968 , Fukunaga  1990 , Young and Calvert  1974  and Hand  1981a . Geisser  1964  presents methods for Bayesian learning of means and covariance matrices un- der various assumptions on the parameters. Bayesian methods for discrimination are described by Lavine and West  1992  and West  1992 .  A more detailed treatment of Bayesian inference, with descriptions of computational  procedures, is given by O’Hagan  1994  and Bernardo and Smith  1994 .  Gelfand  2000  gives a review of the Gibbs sampler and its origins  see also Casella and George, 1992 . Monte Carlo techniques for obtaining characteristics of posterior distributions are also reviewed by Tierney  1994 .  The website www.statistical-pattern-recognition.net contains refer-  ences and links to further information on techniques and applications.  Exercises  1. In the example application study of Section 2.2.3, is it appropriate to use a Gaussian  classiﬁer for the head injury data? Justify your answer.  2. Suppose that B D A C uuT , where A is a nonsingular   p ð p  matrix and u is a vector. Show that B cid:7 1 D A cid:7 1  cid:7  kA cid:7 1uuT A cid:7 1, where k D 1=.1 C uT A cid:7 1u .  Krzanowski and Marriott, 1996   3. Show that the estimate of the covariance matrix given by  O cid:2  D 1 n  cid:7  1  nX iD1  .xi  cid:7  m .xi  cid:7  m T  where m is the sample mean, is unbiased.   Exercises 79  4. Suppose that the p-element x is normally distributed N .µ;  cid:2 i   in population i  i D i [.1  cid:7  ²i  I C ²i 11T ] and 1 denotes the p-vector all of whose 1; 2 , where  cid:2 i D ¦ 2 elements are 1. Show that the optimal  i.e. Bayes  discriminant function is given, apart from an additive constant, by   cid:7  1  .c11  cid:7  c12 Q1 C 1 where Q1 D xxT , Q2 D .1T x 2, c1i D [¦ 2 ²i  f1 C . p  cid:7  1 ²ig] cid:7 1.  Krzanowski and Marriott, 1996   .c21  cid:7  c22 Q2 i C .1  cid:7  ²i  ] cid:7 1 and c2i D ²i [¦ 2  2  2  i  .1  cid:7   5. Verify that the simple one-pass algorithm   a  Initialise S D 0, m D 0.  b  For r D 1 to n do i. dr D xr  cid:7  m  cid:7  1  cid:7  1 ii. S D S C r   cid:8   dr d T r  iii. m D m C dr r  results in m as the sample mean and S as n times the sample covariance matrix.  6. Derive the EM update equations  2.14  and  2.15 .  7. Consider a gamma distribution of the form  cid:7  mx  p.xj¼; m  D m  0.m ¼  ¼   cid:8 m cid:7 1   cid:8    cid:7   cid:7  mx ¼  exp  for mean ¼ and order parameter m. Derive the EM update equations for the ³i , ¼i and mi for the gamma mixture  p.x  D gX iD1  ³i p.xj¼i ; mi    8. Given that the log-likelihood in the EM procedure is given by  log.L. cid:10    D Q. cid:10 ;  cid:10   .m    cid:7  H . cid:10 ;  cid:10   .m    where H is given by  2.9 , and using the result that log.x   cid:10  x  cid:7  1, show that  .mC1 ;  cid:10   .m    cid:7  H . cid:10   .m ;  cid:10   .m    cid:10  0  H . cid:10   .mC1   where  cid:10  likelihood is increased: log.L. cid:10   is chosen to maximise Q. cid:10 ;  cid:10  .mC1    ½ log.L. cid:10   .m   .  .m  . Hence, show that  the log-   80 Density estimation – parametric  9. Generate three data sets  train, validation and test sets  for the three-class, 21-variable,  waveform data  Breiman et al., 1984 :  xi D uh1.i   C .1  cid:7  u h2.i   C ži xi D uh1.i   C .1  cid:7  u h3.i   C ži xi D uh2.i   C .1  cid:7  u h3.i   C ži   class 1   class 2   class 3   where i D 1; : : : ; 21; u is uniformly distributed on [0; 1]; ži are normally distributed with zero mean and unit variance; and the hi are shifted triangular waveforms: h1.i   D max.6  cid:7  ji  cid:7  11j; 0 , h2.i   D h1.i  cid:7  4 , h3.i   D h1.i C 4 . Assume equal class priors. Construct a three-component mixture model for each class using a com- mon covariance matrix across components and classes  Section 2.3.2 . Investigate starting values for the means and covariance matrix and choose a model based on the validation set error rate. For this model, evaluate the classiﬁcation error on the test set. Compare the results with a linear discriminant classiﬁer and a quadratic discriminant classiﬁer constructed using the training set and evaluated on the test set.  10. For the distribution illustrated by Figure 2.4, show that a suitable linear transforma- tion of the coordinate system, to new variables  cid:20 1 and  cid:20 2, will lead to an irreducible chain.  11. For a transformation of variables from .X1; : : : ; X p  to .Y1; : : : ; Y p , given by  where g D .g1; g2; : : : ; g p T , the density functions of X and Y are related by  Y D g.X   pY .y  D pX .x  jJj  where jJj is the absolute value of the Jacobian determinant @g1 @x p ::: @gp @x p  J .x1; : : : ; x p  D  @g1 @x1 ::: @gp @x1  þþþþþþþþþþþ  : : :  : : :  : : :  þþþþþþþþþþþ  Let D denote the region in R 2  D D f.u; v ; 0  cid:10  u  cid:10  pg.v=u g  Show that if .u; v  is uniformly distributed over the region D, then the change of variables .U; V   ! .U; X D V =U   gives  p.u; x  D ku  0 < u < pg.x   where k is a constant. Determine the value of k, and then by marginalising with respect to u show that  p.x  D g.x  R g.x  dx   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   3  Density estimation – nonparametric  Overview  Nonparametric methods of density estimation can provide class-conditional density estimates for use in Bayes’ rule. Three main methods are introduced: a histogram approach with generalisation to include Bayesian networks; k-nearest-neighbour methods and variants; and kernel methods of density estimation.  3.1 Introduction  Many of the classiﬁcation methods discussed in this book require knowledge of the class-conditional probability density functions. Given these functions, we can apply the likelihood ratio test  see Chapter 1  and decide the class to which a pattern x can be assigned. In some cases we may be able to make simplifying assumptions regarding the form of the density function; for example, that it is normal or a normal mixture  see Chapter 2 . In these cases we are left with the problem of estimating the parameters that describe the densities from available data samples.  In many cases, however, we cannot assume that the density is characterised by a set of parameters and we must resort to nonparametric methods of density estimation; that is, there is no formal structure for the density prescribed. There are many methods that have been used for statistical density estimation and in the following paragraphs we shall consider four of them, namely the histogram approach, k-nearest-neighbour, expansion by basis functions and kernel-based methods. First, we shall consider some basic properties of density estimators.  Unbiasedness If X1; : : : ; Xn are independent and identically distributed p-dimensional random vari- ables with continuous density p.x ,  p.x  ½ 0  p.x  dx D 1  Z  R p   3.1    82 Density estimation – nonparametric  the problem is to estimate p.x  given measurements on these variables. If the estimator Op.x  also satisﬁes  3.1 , then it is not unbiased  Rosenblatt, 1956 . That is, if we impose the condition that our estimator is itself a density  in that it satisﬁes  3.1  , it is biased:  E[ Op.x ] 6D p.x   where E[ Op.x ] D R Op.xjx1 : : : xn  p.x1  : : : p.xn  dx1 : : : dxn, the expectation over the random variables X1; : : : ; Xn. Although estimators can be derived that are asymptotically unbiased, E[ Op.x ] ! p.x  as n ! 1, in practice we are limited by the number of samples that we have.  Consistency There are other measures of discrepancy between the density and its estimate. The mean squared error  MSE  is deﬁned by  MSEx . Op  D E[. Op.x   cid:8  p.x  2]  where the subscript x is used to denote that MSE is a function of x. The above equation may be written  MSEx . Op  D var. Op.x   C fbias. Op.x  g2  If MSEx ! 0 for all x 2 R p, then Op is a pointwise consistent estimator of p in the quadratic mean. A global measure of accuracy is given by the integrated squared error  ISE   ISE D  [ Op.x   cid:8  p.x ]2 dx  and by the mean integrated squared error  MISE   MISE D E  [ Op.x   cid:8  p.x ]2 dx  ½  Z   cid:3 Z  which represents an average over all possible data sets. Since the order of the expectation and the integral may be reversed, the MISE is equivalent to the integral of the MSE, that is the sum of the integrated squared bias and the integrated variance.  Density estimates Although one might na¨ıvely expect that density estimates have to satisfy the property  3.1 , this need not be the case. We shall want them to be pointwise consistent, so that we can get arbitrarily close to the true density given enough samples. Consideration has been given to density estimates that may be negative in parts in order to improve the convergence properties. Also, as we shall see in a later section, the integral constraint may be relaxed. The k-nearest-neighbour density estimate has an inﬁnite integral.  3.2 Histogram method  The histogram method is perhaps the oldest method of density estimation. It is the classical method by which a probability density is constructed from a set of samples.   Histogram method 83   cid:1  cid:2   dx  Figure 3.1 Histogram   cid:1   x  In one dimension, the real line is partitioned into a number of equal-sized cells  see Figure 3.1  and the estimate of the density at a point x is taken to be  Op.x  D  n jPN  j n j dx  Op.x  D  n jP  j n j dV  where n j is the number of samples in the cell of width dx that straddles the point x, N is the number of cells and dx is the size of the cell. This generalises to  for a multidimensional observation space, where dV is the volume of bin j.  Although this is a very simple concept and easy to implement, and it has the advant- age of not needing to retain the sample points, there are several problems with the basic histogram approach. First of all, it is seldom practical in high-dimensional spaces. In one dimension, there are N cells; in two dimensions, there are N 2 cells  assuming that each variable is partitioned into N cells . For data samples x 2 R p   p-dimensional vector x  there are N p cells. This exponential growth in the number of cells means that in high dimensions a very large amount of data is required to estimate the density. For example, where the data samples are six-dimensional, then dividing each variable range into 10 cells  a not unreasonable ﬁgure  gives a million cells. In order to prevent the estimate being zero over a large region, many observations will be required. A second problem with the histogram approach is that the density estimate is discontinuous and falls abruptly to zero at the boundaries of the region. We shall now consider some of the proposed approaches for overcoming these difﬁculties.  3.2.1 Data-adaptive histograms  One approach to the problem of constructing approximations to probability density func- tions from a limited number of samples using p-dimensional histograms is to allow the   84 Density estimation – nonparametric  Figure 3.2 Variable cell size histogram   cid:1   x  histogram descriptors – location, shape and size – to adapt to the data. This is illustrated in Figure 3.2.  An early approach was by Sebestyen and Edie  1966  who described a sequential method to multivariate density estimation using cells that are hyperellipsoidal in shape.  3.2.2 Independence assumption  Another approach for reducing the number of cells in high-dimensional problems is to make some simplifying assumptions regarding the form of the probability density function. We may assume that the variables are independent so that p.x  may be written in the form  p.x  D pY iD1  p.xi    where p.xi   are the individual  one-dimensional  densities of the components of x. Various names have been used to describe such a model including na¨ıve Bayes, idiot’s Bayes and independence Bayes. A histogram approach may be used for each density individually, giving pN cells  assuming an equal number of cells, N , per variable , rather than N p. A particular implementation of the independence model is  Titterington et al., 1981   p.x  ¾   B    pY rD1  n.xr   C 1 Cr N .r   C 1   3.2   where xr n.xr   N .r    Cr B  is the rth component of x; is the number of samples with value xr on variable r; is the number of observations on variable r  this may vary due to missing data ; is the number of cells in variable r; is an ‘association factor’ representing the ‘proportion of non-redundant information’ in the variables.   Histogram method 85  Note that the above expression takes account of missing data  which may be a problem in some categorical data problems . It has a non-constant number of cells per variable.  3.2.3 Lancaster models  Lancaster models are a means of representing the joint distribution in terms of the marginal distributions, assuming all interactions higher than a certain order vanish. For example, if we assume that all interactions higher than order s D 1 vanish, then a Lancaster model is equivalent to the independence assumption. If we take s D 2, then the probability density function is expressed in terms of the marginals p.xi   and the joint distributions p.xi ; x j  ; i 6D j, as  Zentgraf, 1975   cid:8     X  p.x  D  ½    cid:8  1   cid:11   pindep.x    cid:3  cid:10  p 2  p.xi ; x j   p.xi   p.x j    i; j;i < j  where pindep.x  is the density function obtained by the independence assumption,  pindep.x  D pY kD1  p.xk    Lancaster models permit a range of models from the independence assumption to the full multinomial, but do have the disadvantage that some of the probability density estimates may be negative. Titterington et al.  1981  take the two-dimensional marginal estimates as  p.xi ; x j   D n.xi ; x j   C 1=.Ci C j    N .i; j   C 1  where the deﬁnitions of n.xi ; x j   and N .i; j   are analogous to the deﬁnitions of n.xi   and N .i   given above for the independence model, and  cid:11 B  cid:10  n.xi   C 1=Ci N .i   C 1  p.xi   D  Titterington et al. adopt the independence model whenever the estimate of the joint distribution is negative.  3.2.4 Maximum weight dependence trees  Lancaster models are one way to capture dependencies between variables without making the sometimes unrealistic assumption of total independence, yet having a model that does not require an unrealistic amount of storage or number of observations. Chow and Liu  1968  propose a tree-dependent model in which the probability distribution p.x  is modelled as a tree-dependent distribution pt .x  that can be written as the product of p  cid:8  1 pairwise conditional probability distributions  pt .x  D pY iD1  p.xijx j .i      3.3    86 Density estimation – nonparametric   b   x4   cid:5  cid:5  cid:5  cid:5  cid:5   cid:6  cid:6  cid:6  cid:6  cid:6    cid:3  cid:3   cid:4  cid:4   x2  x5  x6   cid:3  cid:3   cid:4  cid:4   x1  x3  x7  x8   a   x1  x4  x6   cid:3  cid:3    cid:3  cid:3   cid:4  cid:4   x3  x2  x5   cid:3  cid:3   cid:4  cid:4   x7  x8  Figure 3.3 Tree representations  where x j .i   is the variable designated as the parent of xi , with the root x1 chosen arbitrarily and characterised by the prior probability p.x1jx0  D p.x1 . For example, the density  pt .x  D p.x1  p.x2jx1  p.x3jx2  p.x4jx2  p.x5jx4  p.x6jx4  p.x7jx4  p.x8jx7    3.4   has the tree depicted in Figure 3.3a, with root x1. An alternative tree representation is Figure 3.3b, with root x4, since  3.4  may be written using Bayes’ theorem as  pt .x  D p.x4  p.x1jx2  p.x3jx2  p.x2jx4  p.x5jx4  p.x6jx4  p.x7jx4  p.x8jx7   Indeed, any node may be taken as the root node. If each variable can take N values, then the density  3.3  has N .N  cid:8  1  parameters for each of the conditional densities and N  cid:8  1 parameters for the prior probability, giving a total of N .N  cid:8  1 . p  cid:8  1  C N  cid:8  1 parameters to estimate.  The approach of Chow and Liu  1968  is to seek the tree-dependent distribution, pt .x , that best approximates the distribution p.x . They use the Kullback–Leibler cross- entropy measure as the measure of closeness in approximating p.x  by pt .x ,  or, for discrete variables,  Z  D. p; pt   D  p.x  log   cid:11    cid:10  p.x  pt .x   dx  D D X  x  p.x  log   cid:11    cid:10  p.x  pt .x   where the sum is over all values that the variable x can take, and they seek the tree- dependent distribution p− .x  such that D. p.x ; p− .x    cid:14  D. p.x ; pt .x   over all t in the set of possible ﬁrst-order dependence trees. Using the mutual information between variables  I .Xi ; X j   D X  xi ;x j  p.xi ; x j   log   cid:11    cid:10  p.xi ; x j   p.xi   p.x j     to assign weights to every branch of the dependence tree, Chow and Liu  1968  show  see the exercises  that the tree-dependent distribution pt .x  that best approximates p.x  is the one with maximum weight deﬁned by  Histogram method 87  W D pX iD1  I .Xi ; X j .i     This is termed a maximum weight dependence tree  MWDT  or a maximum weight span- ning tree. The steps in the algorithm to ﬁnd a MWDT are as follows. 1. Compute the branch weights for all p. p  cid:8  1 =2 variable pairs and order them in  decreasing magnitude.  2. Assign the branches corresponding to the two largest branches to the tree.  3. Consider the next largest value and add the corresponding branch to the tree if it does  not form a cycle, otherwise discard it.  4. Repeat this procedure until p  cid:8  1 branches have been selected.  5. The probability distribution may be computed by selecting an arbitrary root node and  computing  3.3 .  Applying the above procedure to the six-dimensional head injury data of Titterington et al.  1981  produces  for class 1  the tree illustrated in Figure 3.4. The labels for the variables are described in Section 2.2.3  x1 – Age; x2 – EMV; x3 – MRP; x4 – Change; x5 – Eye Indicant; x6 – Pupils . To determine the tree-dependent distribution, select a root node  say node 1  and write the density using the ﬁgure as pt .x  D p.x1  p.x2jx1  p.x3jx2  p.x5jx2  p.x4jx3  p.x6jx5   The ﬁrst stage in applying MWDTs to a classiﬁcation problem is to apply the algorithm  to the data set for each class individually to give C trees.  There are several features that make MWDTs attractive. The algorithm requires only second-order distributions but, unlike the second-order Lancaster model, it need store  x1   cid:4   x3   cid:3  cid:3    cid:4    cid:4  cid:4   x2   cid:3    cid:4    cid:4  cid:4   x5   cid:4    cid:4  cid:4   x4   cid:4  cid:4   x6  Figure 3.4 MWDT applied to head injury patient data  class 1    88 Density estimation – nonparametric  only p cid:8  1 of these. The tree is computed in O. p2  steps  though additional computation is required in order to obtain the mutual information  and if p.x  is indeed tree-dependent then the approximation pt .x  is a consistent estimate in the sense that  j pt .n   .x   cid:8  p.x j ! 0 with probability 1 as n ! 1  max  x  n  where pt .n  the distribution p.x .  n  is the tree-dependent distribution estimated from n independent samples of  3.2.5 Bayesian networks  In Section 3.2.4 a development of the na¨ıve Bayes model  which assumes independence between variables  was described. This was the MWDT model, which allows pairwise dependence between variables and is a compromise between approaches that specify all relationships between variables and the rather restrictive independence assumption. Bayesian networks also provide an intermediate model between these two extremes and have the tree-based models as a special case.  We introduce Bayesian networks by considering a graphical representation of a mul- tivariate density. The chain rule allows a joint density, p.x1; : : : ; x p , to be expressed in the form  p.x1; : : : ; x p  D p.x pjx1; : : : ; x p cid:8 1  p.x p cid:8 1jx1; : : : ; x p cid:8 2  : : : p.x2jx1  p.x1   We may depict such a representation of the density graphically. This is illustrated in Figure 3.5 for p D 6 variables. Each node in the graph represents a variable and the directed links denote the dependencies of a given variable. The parents of a given variable are those variables with directed links towards it. For example, the parents of x5 are x1; x2; x3 and x4. The root node is the node without parents  the node corresponding to variable x1 . The probability density that such a ﬁgure depicts is the product of conditional densities  p.x1; : : : ; x p  D pY iD1  p.xijπ i     3.5   where π i is the set of parents of xi  cf. equation  3.3  . If π i is empty, p.xijπ i   is set to p.xi  .   cid:1   x2  x1   cid:4    cid:3    cid:4    cid:3  cid:3    cid:3  cid:3  cid:7    cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:11  cid:5  cid:5   cid:6  cid:6  cid:6  cid:6  cid:10  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6    cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:10  cid:6  cid:6   cid:5  cid:5  cid:5  cid:5  cid:11  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5    cid:4  cid:4  cid:9   cid:4  cid:4  cid:8    cid:4   cid:3   cid:4  cid:9   cid:3  cid:7    cid:8   cid:1    cid:3  cid:3    cid:4  cid:4    cid:3  cid:3    cid:4  cid:4    cid:3  cid:3  cid:7    cid:4  cid:4  cid:9    cid:3    cid:4    cid:3   x5   cid:1   x6  x4  x3  Figure 3.5 Graphical representation of the multivariate density p.x1; : : : ; x6    Histogram method 89   cid:1   x2   cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:10  cid:6  cid:6   x1   cid:1    cid:3  cid:3   x4  x2  x3   cid:4  cid:4  cid:9    cid:4  cid:4   x5   cid:1    cid:3  cid:3  cid:7    cid:3  cid:3   x6  x1   cid:3  cid:3  cid:7    cid:4  cid:4  cid:9   x3   cid:3  cid:3  cid:7    cid:4  cid:4  cid:9    cid:4  cid:4   x4   cid:3  cid:3    cid:4  cid:4  cid:9    cid:4  cid:4    cid:3  cid:3  cid:7   x5   cid:4  cid:4    cid:3  cid:3   x6  Figure 3.6 Graphical representations of the multivariate density p.x1; : : : ; x6  D p.x6jx4; x5  p.x5jx3 p.x4jx1; x3 p.x3 p.x2jx1 p.x1   Figure 3.5 is the graphical part of the Bayesian network and equation  3.5  is the density associated with the graphical representation. However, there is little to be gained in representing a full multivariate density p.x1; : : : ; x6  as a product using the chain rule with the corresponding graph  Figure 3.5 , unless we can make some simplifying assumptions concerning the dependence of variables. For example, suppose  p.x6jx1; : : : ; x5  D p.x6jx4; x5   that is, x6 is independent of x1; x2; x3 given x4; x5; also p.x5jx1; : : : ; x4  D p.x5jx3  p.x4jx1; x2; x3  D p.x4jx1; x3   p.x3jx1; x2  D p.x3   Then the multivariate density may be expressed as the product  p.x1; : : : ; x6  D p.x6jx4; x5  p.x5jx3  p.x4jx1; x3  p.x3  p.x2jx1  p.x1    3.6   This is depicted graphically in Figure 3.6; the left-hand graph is obtained by removing links in Figure 3.5 and the right-hand graph is an equivalent graph to make the ‘parentage’ more apparent. This ﬁgure, with the general probability interpretation  3.5 , gives  3.6 . Note that there are two root nodes.  Deﬁnition A graph is a pair .V ; E , where V is a set of vertices and E a set of edges  connections between vertices . A directed graph is a graph in which all edges are directed: the edges are ordered pairs; if .Þ; þ  2 E, for vertices Þ and þ, then .þ; Þ  =2 E. A directed acyclic graph  DAG  is one in which there are no cycles: there is no path Þ1 ! Þ2 ! ÐÐÐ ! Þ1, for any vertex Þ1. A Bayesian network is a DAG where the vertices correspond to variables and associated with a variable X with parents Y1; : : : ; Y p is a conditional probability density function, p.XjY1; : : : ; Y p . The Bayesian network, together with the conditional densities, speciﬁes a joint probability density function by  3.5 .   90 Density estimation – nonparametric  The graphical representation of Bayesian networks is convenient for visualising depen- dencies. The factorisation of a multivariate density into a product of densities deﬁned on perhaps only a few variables allows better nonparametric density estimates. For example, the product  3.6  requires densities deﬁned on at most three variables.  Classiﬁcation As with the MWDT, we may construct a different Bayesian network to model the probability density function of each class p.xj!i   separately. These densities may be substituted into Bayes’ rule to obtain estimates of the posterior probabilities of class membership. Alternatively, we may construct a Bayesian network to model the joint density p.x; ! , where ! is a class label. Again, we may evaluate this for each of the classes and use Bayes’ rule to obtain p.!jx . Separate networks for each class allow a more ﬂexible model. Such a set of networks has been termed a Bayesian multinet  Friedman et al., 1997 .  Specifying the network Specifying the structure of a Bayesian network consists of two parts: specifying the network topology and estimating the parameters of the conditional probability density functions. The topology may be speciﬁed by someone with an expert knowledge of the problem domain and who is able to make some statements about dependencies of vari- ables. Hence, an alternative name for Bayesian networks is probabilistic expert systems: an expert system because the network encodes expert knowledge of a problem in its structure and probabilistic because the dependencies between variables are probabilistic. Acquiring expert knowledge can be a lengthy process. An alternative approach is to learn the graph from data if they are available, perhaps in a similar manner to the MWDT algorithm. In some applications, sufﬁcient data may not be available.  In learning structure from data, the aim is to ﬁnd the Bayesian network that best characterises the dependencies in the data. There are many approaches. Buntine  1996  reviews the literature on learning networks. Ideally, we would want to combine expert knowledge where available and statistical data. Heckerman et al.  1995; see also Cooper and Herskovits, 1992  discuss Bayesian approaches to network learning.  The probability density functions are usually speciﬁed as a conditional probability table, with continuous variables discretised, perhaps as part of the structure learning process. For density estimation, it is not essential to discretise the variables and some nonparametric density estimate, perhaps based on product kernels  see Section 3.5 , could be used.  Discussion Bayesian networks provide a graphical representation of the variables in a problem and the relationship between them. This representation needs to be speciﬁed or learned from data. This structure, together with the conditional density functions, allows the multivariate density function to be speciﬁed through the product rule  3.5 . In a classiﬁcation problem, a density may be estimated for each class and Bayes’ rule used to obtain the posterior probabilities of class membership.  Bayesian networks have been used to model many complex problems, other than ones in classiﬁcation, with the structure being used to calculate the conditional density of a variable given measurements made on some  or all  of the remaining variables. Such a   Histogram method 91  situation may arise when measurements are made sequentially and we wish to update our belief that a variable takes a particular value as measurements arrive. For example, suppose we model the joint density p.y; x1; : : : ; x p  as a Bayesian network, and we are interested in the quantity p.yje , where e comprises measurements made on a subset of the variables, x1; : : : ; x p. Then, using Bayes’ theorem, we may write  p.yje  D  R p.y; e; Qe  dQe RR p.y; e; Qe  dQe dy  where Qe is the set of variables x1; : : : ; x p not instantiated. Efﬁcient algorithms that make use of the graphical structure have been developed for computing p.yje  in the case of discrete variables  Lauritzen and Spiegelhalter, 1988 .  3.2.6 Example application study  The problem Prediction of the occurrence of rainfall using daily observations of me- teorological data  Liu et al., 2001 .  Summary The study used the na¨ıve Bayes  that is, independence assumption  classiﬁer with marginal densities estimated through a histogram approach.  The data The data comprise daily observations from May to October for the years 1984–1992 from Hong Kong Observatory. There are mixed continuous and categori- cal data including wind direction and speed, daily mean atmospheric pressure, ﬁve-day mean pressure, temperature, rainfall and so on. There are 38 basic input variables and three classes of rainfall. There is some data preprocessing to handle missing values and standardisation of variables.  The model The model is a na¨ıve Bayes classiﬁer, with priors calculated from the data and class-conditional densities estimated using histograms.  Training procedure Although training for a histogram-based na¨ıve Bayes classiﬁer is minimal in general, the degree of discretisation must be speciﬁed  here, a class- dependent method was used . Also, some variable selection  see Chapter 9  was carried out. Another difference from the standard histogram approach is that the density estimates were updated using past tested data. Thus, the size of the training set is variable; it increases as we apply the method to test patterns.  Results that other methods also assessed.  For this application, this simple approach worked well and performed better  3.2.7 Further developments  The MWDT tree-dependence approximation can also be derived by minimising an upper bound on the Bayes error rate under certain circumstances  Wong and Poon, 1989 .   92 Density estimation – nonparametric  Computational improvements in the procedure have been proposed by Valiveti and Oommen  1992, 1993 , who suggest a chi-square metric in place of the expected mutual information measure.  A further development is to impose a common tree structure across all classes. The  mutual information between variables is then written as  I .Xi ; X j   D X  xi ;x j ;!  p.xi ; x j ; ! log   cid:10  p.xi ; x jj!  p.xij!  p.x jj!    cid:11   It is termed a tree-augmented na¨ıve Bayesian network by Friedman et al.  1997 , who provide a thorough evaluation of the model on 23 data sets from the UCI repository  Murphy and Aha, 1995  and two artiﬁcial data sets. Continuous attributes are discretised and patterns with missing values omitted from the analysis. Performance is measured in terms of classiﬁcation accuracy with the holdout method used to estimate error rates. See Chapter 8 for performance assessment and Chapter 11 for a discussion of the missing data problem. The model was compared to one in which separate trees were constructed for each class and the na¨ıve Bayes model. Both tree models performed well in practice. One of the disadvantages of histogram procedures for continuous data that we noted in the introduction to this section was that the density is discontinuous at cell boundaries. Procedures based on splines have been proposed for overcoming this difﬁculty. More recent developments in the use of splines for density estimation are described by Gu and Qiu  1993 .  3.2.8 Summary  In the development of the basic histogram approach described in this section, we have concentrated on methods for reducing the number of cells for high-dimensional data. The approaches described assume that the data are categorical with integer-labelled categories, though the categories themselves may or may not be ordered. The simplest models are the independence models, and on the head injury data of Titterington et al.  1981  they gave consistently good performance over a range of values for B, the association factor  0.8 to 1.0 . The independence assumption results in a very severe factorisation of the probability density function – clearly one that is unrealistic for many practical problems. Yet, as discussed by Hand and Yu  2001 , it is a model that has had a long and successful history  see also Domingos and Pazzani, 1997 . Practical studies, particularly in medical areas, have shown it to perform surprisingly well. Hand  1992  provides some reasons why this may be so: its intrinsic simplicity means low variance in its estimates; although its probability density estimates are biased, this may not matter in supervised classiﬁcation so long as Op.!1jx  > Op.!2jx  when p.!1jx  > p.!2jx ; in many cases, variables have undergone a selection process to reduce dependencies.  More complex interactions between variables may be represented using Lancaster models and MWDTs. Introduced by Chow and Liu  1968 , MWDTs provide an efﬁcient means of representing probability density functions using only second-order statistics.  Dependence trees are a special case of Bayesian networks which model a multivariate density as a product of conditional densities deﬁned on a smaller number of variables. These networks may be speciﬁed by an expert, or learned from data.   k-nearest-neighbour method 93  3.3 k-nearest-neighbour method  The k-nearest-neighbour method is a simple method of density estimation. The probability that a point x0 falls within a volume V centred at a point x is given by  where the integral is over the volume V . For a small volume  The probability,  cid:12 , may be approximated by the proportion of samples falling within V . If k is the number of samples, out of a total of n, falling within V  k is a function of x  then  Equations  3.7  and  3.8  combine to give an approximation for the density,   3.7    3.8    3.9   Z   cid:12  D  p.x  dx  V .x    cid:12  ¾ p.x V   cid:12  ¾ k n  Op.x  D k nV  The k-nearest-neighbour approach is to ﬁx the probability k=n  or, equivalently, for a given number of samples n, to ﬁx k  and to determine the volume V which contains k samples centred on the point x. For example, if xk is the kth nearest-neighbour point to x, then V may be taken to be a sphere, centred at x, of radius jjx  cid:8  xkjj  the volume n of a sphere of radius r in n dimensions is 2r n³ 2 =n0.n=2 , where 0.x  is the gamma function . The ratio of the probability to this volume gives the density estimate. This is in contrast to the basic histogram approach which is to ﬁx the cell size and to determine the number of points lying within it.  One of the parameters to choose is the value of k. If it is too large, then the estimate will be smoothed and ﬁne detail averaged out. If it is too small, then the probability density estimate is likely to be spiky. This is illustrated in Figures 3.7  peaks truncated  and 3.8, where 13 samples are plotted on the x-axis, and the k-nearest-neighbour density estimate shown for k D 1 and 2. One thing to note about the density estimate is that it is not in fact a density. The integral under the curve is inﬁnite. This is because for large enough jxj, the estimate varies as 1=jxj. However, it can be shown that the density estimator is asymptotically unbiased and consistent if  lim  n!1 k.n  D 1 D 0 lim n!1  k.n   n  3.3.1 k-nearest-neighbour decision rule  rule. Suppose that in the ﬁrst k samples there are km in class !m  so that PC  Having obtained an expression for a density estimate, we can now use this in a decision mD1 km D k .   94 Density estimation – nonparametric  0  0  2  1.8  1.6  1.4  1.2  1  0.8  0.6  0.4  0.2  1.4  1.2  1  0.8  0.6  0.4  0.2  0  0  1  2  3  4  5  6  Figure 3.7 Nearest-neighbour density estimates for k D 1  1  2  3  4  5  6  Figure 3.8 Nearest-neighbour density estimates for k D 2  Let the total number of samples in class !m be nm  so that PC may estimate the class-conditional density, p.xj!m  , as  mD1 nm D n . Then we   3.10   and the prior probability, p.!m  , as  Op.xj!m   D km nm V  Op.!m   D nm n  Then the decision rule is to assign x to !m if  Op.!mjx  ½ Op.!ijx   for all i   k-nearest-neighbour method 95  or, using Bayes’ theorem,  that is, assign x to !m if  km nm V  nm n  ½ ki ni V  ni n  for all i  km ½ ki  for all i  Thus, the decision rule is to assign x to the class that receives the largest vote amongst the k nearest neighbours. There are several ways of breaking ties. Ties may be broken arbitrarily. Alternatively, x may be assigned to the class, out of the classes with tying values of ki , that has nearest mean vector to x  with the mean vector calculated over the ki samples . Another method is to assign x to the most compact class – that is, to the one for which the distance to the ki th member is the smallest. This does not require any extra computation. Dudani  1976  proposes a distance-weighted rule in which weights are assigned to the k nearest neighbours, with closest neighbours being weighted more heavily. A pattern is assigned to that class for which the weights of the representatives among the k neighbours sum to the greatest value.  3.3.2 Properties of the nearest-neighbour rule  The asymptotic misclassiﬁcation rate of the nearest-neighbour rule, e, satisﬁes the con- dition  Cover and Hart, 1967   eŁ  cid:14  e  cid:14  eŁ  cid:10    cid:11   2  cid:8  CeŁ C  cid:8  1  where eŁ is the Bayes probability of error and C is the number of classes. Thus in the large-sample limit, the nearest-neighbour error rate is bounded above by twice the Bayes error rate. The inequality may be inverted to give  C  cid:8  1 C   cid:8   r C  cid:8  1  r C  cid:8  1  C  C   cid:8  e  cid:14  eŁ  cid:14  e  The leftmost quantity is a lower bound on the Bayes error rate. Therefore, any classiﬁer must have an error rate greater than this value.  3.3.3 Algorithms  Identifying the nearest neighbour of a given observation vector from among a set of training vectors is conceptually straightforward with n distance calculations to be per- formed. However, as the number n in the training set becomes large, this computational overhead may become excessive.  Many algorithms for reducing the nearest-neighbour search time involve the signif- icant computational overhead of preprocessing the prototype data set in order to form a distance matrix  see Dasarathy, 1991, for a summary . There is also the overhead of   96 Density estimation – nonparametric  storing n.n  cid:8  1 =2 distances. There are many approaches to this problem. The linear approximating and eliminating search algorithm  LAESA , a development of the AESA algorithm of Vidal  1986, 1994 , has a preprocessing stage that computes a number of base prototypes that are in some sense maximally separated from among the set of train- ing vectors  Mic´o et al., 1994 . Although it does not guarantee an optimal solution  in the sense that the sum of all pairwise distances between members of the set of base prototypes is a maximum , it can be achieved in linear preprocessing time. However, the LAESA algorithm requires the storage of an array of size n by nb, the number of base prototypes. This will place an upper bound on the permitted number of base proto- types. Figure 3.9 illustrates a set of 21 training samples in two dimensions and four base prototypes chosen by the base prototype algorithm of Mic´o et al.  1994 . The algorithm begins by ﬁrst selecting a base prototype, b1, arbitrarily from the set of prototypes and the distance to every member of the remaining prototypes is calculated and stored in an array A. The second base prototype, b2, is the prototype that is furthest from b1. The distance of this prototype to every remaining prototype is calculated and the array A incremented. Thus, A represents the accumulated distances of non-base prototypes to base prototypes. The third base prototype is the one for which the accumulated distance is the greatest. This process of selecting a base prototype, calculating distances and accu- mulating the distances continues until the required number of base prototypes has been selected.  The LAESA searching algorithm uses the set of base prototypes and the interpoint distances between these vectors and those in the training set as follows. The algorithm uses the metric properties of the data space. Let x be the test sample  whose nearest neighbour from the set of prototypes we seek , n be its current nearest neighbour at a distance dxn, and q be a base prototype whose distance to x has been computed at an earlier stage of the algorithm  see Figure 3.10 . The condition for a prototype p to be rejected as a nearest-neighbour candidate is  dx p ½ dxn  C  C  C  b1  C C C } C C  C  C  C  C C  b4  C  C  C }  C }  b3  C  C  C  C }  b2  1.5  2  1  0.5  0  cid:8 0.5  cid:8 1  cid:8 1.5  cid:8 2   cid:8 2   cid:8 3 Figure 3.9 Selection of base prototypes  Š  from the data set  C    cid:8 1  1  2  0  3   k-nearest-neighbour method 97  q, base prototype  dxq   cid:13  cid:13  n, current nearest neighbour dxn   cid:13    cid:3  cid:3    cid:3    cid:3    cid:3   d pq  cid:3    cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:13    cid:3   dx p  x   cid:3  p  Figure 3.10 Nearest-neighbour selection using LAESA  This requires the calculation of the distance dx p. However, a lower bound on the distance dx p is given by  dx p ½ jd pq  cid:8  dxqj  and if this lower bound  which does not require any additional distance calculation  exceeds the current nearest-neighbour distance then clearly we may reject p. We may go further by stating  dx p ½ G. p   4D max  q  jd pq  cid:8  dxqj   3.11   where the maximum is over all base prototypes considered so far in the iteration process, and therefore if G. p  ½ dxn, we may reject p without computing dx p.  Given x, the algorithm selects a base prototype as an initial candidate s for a nearest neighbour and removes this from the set of prototypes. It then searches through the remaining set and rejects all samples from the set of prototypes whose lower bound on the distance to x exceeds the distance jx  cid:8  sj. At the initial stage, the lower bound  3.11  is based on the selected base prototype, s, only. A record of this lower bound is stored in an array. Out of the remaining prototypes, the EC1 version of the algorithm selects as the next candidate s for the nearest neighbour the base prototype for which this lower bound is a minimum  assuming the base prototypes have not been eliminated, otherwise a non-base prototype sample is chosen .  The candidate vector s need not necessarily be nearer than the previous choice, though if it is the nearest neighbour is updated. The data set is searched through again, rejecting vectors that are greater than the lower bound  which is updated if s is a base prototype . This process is repeated and is summarised in the ﬁve following steps.  1. Distance computing – calculate the distance of x to the candidate s for a nearest  neighbour.  2. Update the prototype nearest to x if necessary.  3. Update the lower bounds, G. p , if s is a base prototype.  4. Eliminate the prototypes with lower bounds greater than dxs.  5. Approximating – select the candidates for the next nearest neighbour.  Further details are given by Mic´o et al.  1994 . Figure 3.11 gives an illustration of the LAESA procedure  using the EC1 condition . The test sample x is at the origin .0; 0    98 Density estimation – nonparametric  1.5  2  1  0.5  0  cid:8 0.5  cid:8 1  cid:8 1.5  cid:8 2  C  C  C  } s1 C C ž x  C  C  C  C C  C  C  }  s2   cid:8 3   cid:8 2   cid:8 1  0  1  2  3  Figure 3.11 Nearest-neighbour selection using LAESA; s1 and s2 are the ﬁrst two candidates for nearest neighbour  and the ﬁrst two choices for s are shown. The remaining samples are those left after two passes through the data set  two distance calculations . There are two factors governing the choice of the number of base prototypes, nb. One is the amount of storage available. An array of size n ð nb must be stored. This could become prohibitive if n and nb are large. At the other extreme, too small a value of nb will result in a large number of distance calculations if n is large. We suggest that you choose a value as large as possible without placing constraints on memory since the number of distance calculations decreases monotonically  approximately  with nb for the EC1 model of Mic´o et al.  1994  given above.  3.3.4 Editing techniques  One of the disadvantages of the k-nearest-neighbour rule is that it requires the storage of all n data samples. If n is large, then this may mean an excessive amount of storage. However, a major disadvantage may be the computation time for obtaining the k nearest neighbours. There have been several studies concerned with reducing the number of class prototypes with the joint aims of increasing computational efﬁciency and increasing the generalisation error rate.  We shall consider algorithms for two procedures for reducing the number of pro- totypes. The ﬁrst of these belongs to the family of editing techniques. These tech- niques process the design set with the aim of removing prototypes that contribute to the misclassiﬁcation rate. This is illustrated in Figure 3.12 for a two-class problem. Figure 3.12 plots samples from two overlapping distributions, together with the Bayes decision boundary. Each region, to the left and right of the boundary, contains proto- types that are misclassiﬁed by a Bayes classiﬁer. Removing these to form Figure 3.13 gives two homogeneous sets of prototypes with a nearest-neighbour decision bound- ary that approximates the Bayes decision boundary. The second technique that we   k-nearest-neighbour method 99  Figure 3.12 Editing illustration – samples and decision boundary  ž  ž  ž  ž  ž  ž  Š  ž  Š  ž  Š ž  Š  Š  Š  ž ž  Š ž Š ž  ž ž  Š  Š Š  Š  Š Š Š Š Š ž ž  ž  Š  Š  Š  ž  ž  ž ž  Š  Š Š  Š  ž  Š Š Š Š Š ž ž  ž  Figure 3.13 Editing illustration – edited data set  consider is that of condensing. This aims to reduce the number of prototypes per class without changing the nearest-neighbour approximation to the Bayes decision boundary substantially.  The basic editing procedure is as follows. Given a design set R  of known classiﬁ- cation , together with a classiﬁcation rule  cid:15 , let S be the set of samples misclassiﬁed by the classiﬁcation rule  cid:15 . Remove these from the design set to form R D R cid:8  S and repeat the procedure until a stopping criterion is met. Thus, we end up with a set of samples correctly classiﬁed by the rule.  One implementation of applying this scheme to the k-nearest-neighbour rule is as  follows.  1. Make a random partition of the data set, R, into N groups R1; : : : ; RN .  2. Classify the samples in the set Ri using the k-nearest-neighbour rule with the union of the ‘next’ M sets R.iC1 modN[ÐÐÐ[ R.iCM cid:8 1 modN as the design set, for i D 1; : : : ; N where 1  cid:14  M  cid:14  N  cid:8  1. Let S be the set of misclassiﬁed samples.  3. Remove all misclassiﬁed samples from the data set to form a new data set, R D R cid:8  S.  4. If the last I iterations have resulted in no samples being removed from the design set,  then terminate the algorithm, otherwise go back to step 1.   100 Density estimation – nonparametric  If M D 1, then we have a modiﬁed holdout method of error estimation, and taking k D 1 gives the multiedit algorithm of Devijver and Kittler  1982 . Taking M D N  cid:8  1  so that all remaining sets are used  gives an N -fold cross-validation error estimate. If N is equal to the number of samples in the design set  and M D N  cid:8  1 , then we have the leave-one-out error estimate which is Wilson’s method of editing  Wilson, 1972 . Note that after the ﬁrst iteration, the number of design samples has reduced and the number of partitions cannot exceed the number of samples. For small data sets, an editing procedure using a cross-validation method of error estimation is preferred to multiedit  Ferri and Vidal, 1992a; see also Ferri et al., 1999 .  The editing algorithm above creates homogeneous sets of clusters of samples. The basic idea behind condensing is to remove those samples deeply embedded within each cluster that do not contribute signiﬁcantly to the nearest-neighbour approximation to the Bayes decision region. The procedure that we describe is due to Hart  1968 . We begin with two areas of store, labelled A and B. One sample is placed in A and the remaining samples in B. Each sample point in B is classiﬁed using the nearest-neighbour rule with the contents of A  initially a single vector  as prototypes. If a sample is classiﬁed correctly, it is returned to B, otherwise it is added to A. The procedure terminates when a complete pass through the set B fails to transfer any points.  The ﬁnal contents of A constitute the condensed subset to be used with the nearest-  neighbour rule.  The result of a condensing procedure is shown in Figure 3.14. There can be consid-  erable reduction in the number of training samples.  Ferri and Vidal  1992b  apply both editing and condensing techniques to image data gathered for a robotic harvesting application. The problem consists of detecting the lo- cation of pieces of fruit within a scene. The data comprise six images, captured into arrays of 128 ð 128 pixels; two are used for training, four for test. From the training images, a training set of 1513 10-dimensional feature vectors  obtained from RGB val- ues at a pixel location and its neighbours  spanning three classes  fruit, leaves, sky  is constructed. The editing algorithm is run with a value of N  the number of partitions of the data set  chosen randomly from f3; 4; 5g at each iteration of the editing process. Results are reported for values of I  the number of iterations in the editing procedure with no  ž  ž  Š  Š  ž  Š  Š  ž  Figure 3.14 Condensing illustration: the solid line represents the Bayes decision boundary; the dotted line is the approximation based on condensing   k-nearest-neighbour method 101  Table 3.1 Results for multiedit and condensing for image segmentation  after Ferri and Vidal, 1992b   Original  1513  18.26  I D 6  I D 10  Multiedit Condense Multiedit Condense 1145  1139  10.58  28 8.95  22 10.10  10.55  Size Average errors over four test images  samples removed from the design set  of 6 and 10. Results for I D 6 are summarised in Table 3.1.  Condensing reduces the size of the data set considerably. Editing reduces the error  rate, with a further smaller reduction after condensing.  3.3.5 Choice of distance metric  The most commonly used metric in measuring the distance of a new sample from a prototype is Euclidean distance. Therefore, since all variables are treated equally, the input variables must be scaled to ensure that the k-nearest-neighbour rule is independent of measurement units. The generalisation of this is  d.x; n  D n  .x  cid:8  n T A.x  cid:8  n   o 1  2   3.12   for a matrix A. Choices for A have been discussed by Fukunaga and Flick  1984 . Tode- schini  1989  assesses six global metrics on ten data sets after four ways of standardising the data. Another development of the Euclidean rule is proposed by van der Heiden and Groen  1997 ,  d p.x; n  D n  . p   cid:8  n  .x  . p  T .x  . p   cid:8  n  . p    o 1  2  where x. p  is a transformation of the vector x deﬁned for each element, xi , of the vector x by  i D x . p     .x p i  cid:8  1 = p log.xi    if 0 < p  cid:14  1 if p D 0  In experiments on radar range proﬁles of aircraft, van der Heiden and Groen  1997  evaluate the classiﬁcation error as a function of p.  Friedman  1994  considers basic extensions to the k-nearest-neighbour method and presents a hybrid between a k-nearest-neighbour rule and a recursive partitioning method  see Chapter 7  in which the metric depends on position in the data space. In some classi- ﬁcation problems  those in which there is unequal inﬂuence of the input variables on the classiﬁcation performance , this can offer improved performance. Myles and Hand  1990  assess local metrics where the distance between x and n depends on local estimates of the posterior probability.  In the discriminant adaptive nearest-neighbour approach  Hastie and Tibshirani, 1996 , a local metric is deﬁned in which, loosely, the nearest-neighbour region is parallel to the   102 Density estimation – nonparametric  }  }  }  C  }  C  C  C  C  C  C C  }  } }  }  C 2  } } } } } C 1 ž C C  x  Figure 3.15 Discriminant adaptive nearest-neighbour illustration  decision boundary. It is in the region of the decision boundary where most misclassiﬁ- cations occur. As an illustration, consider the edited data set of Figure 3.13 shown in Figure 3.15. The nearest neighbour to the point x is that labelled 1, and x is classiﬁed as Š. However, if we measure in a coordinate system orthogonal to the decision bound- ary, then the distance between two points is the difference between their distances from the decision boundary, and the point labelled 2 is the nearest neighbour. In this case, x is classiﬁed as C. The procedure of Hastie and Tibshirani uses a local deﬁnition of the matrix A  based on local estimates of the within- and between-class scatter matri- ces – see Section 9.1 for deﬁnitions, for example . This procedure can offer substantial improvements in some problems.  3.3.6 Example application study  The problem To develop credit scoring techniques for assessing the creditworthiness of consumer loan applicants  Henley and Hand, 1996 .  Summary The approach developed is based on a k-nearest-neighbour method, with an adjusted version of the Euclidean distance metric that attempts to incorporate knowledge of class separation contained in the data.  The data The data comprised measurements on 16 variables  resulting from a vari- able selection procedure , usually nominal or ordinal, for a set of credit applicants, split into training and test sets of sizes 15 054 and 4132 respectively. The data were preprocessed into a ratio form, so that the jth value on the ith feature or variable is replaced by log. pi j =qi j  , where pi j is the proportion of those classiﬁed good in at- tribute j of variable i and qi j is the proportion characterised as bad in attribute j of variable i.  The model A k-nearest-neighbour classiﬁer is used. The quadratic metric  3.12  is used, where the positive deﬁnite symmetric matrix, A, is given by  A D .I C DwwT     k-nearest-neighbour method 103  Here D is a distance parameter and w is in the direction of the equiprobability contours of p.gjx , the posterior probability of class g; that is, w is a vector parallel to the decision boundary. This is similar to the discriminant adaptive nearest-neighbour classiﬁer of Section 3.3.5.  Training procedure The value of D was chosen to minimise the bad risk rate, the proportion of bad risk applicants accepted for a prespeciﬁed proportion accepted, based on the design set. The value of k was also based on the design set. The performance assessment criterion used in this study is not the usual one of error rate used in most studies involving k-nearest-neighbour classiﬁers. In this investigation, the proportion to be accepted is prespeciﬁed and the aim is to minimise the number of bad-risk applicants accepted.  Results The main conclusion of the study was that the k-nearest-neighbour approach was fairly insensitive to the choice of parameters and it is a practical classiﬁcation rule for credit scoring.  3.3.7 Further developments  There are many varieties of the k-nearest-neighbour method. The probability density estimator on which the k-nearest-neighbour decision rule is based has been studied by Buturovi´c  1993 , who proposes modiﬁcations to  3.10  for reducing the bias and the variance of the estimator.  There are other preprocessing schemes to reduce nearest-neighbour search time. Approximation–elimination algorithms for fast nearest-neighbour search are given by Vidal  1994  and reviewed and compared by Ramasubramanian and Paliwal  2000 . Fukunaga and Narendra  1975  structure the design set as a tree. This avoids the com- putation of some of the distances. Jiang and Zhang  1993  use an efﬁcient branch and bound technique. Increased speed of ﬁnding neighbours is usually bought at increased preprocessing or storage  for example, Djouadi and Bouktache, 1997 . Dasarathy  1994a  proposes an algorithm for reducing the number of prototypes in nearest-neighbour clas- siﬁcation using a scheme based on the concept of an optimal subset selection. This ‘minimal consistent set’ is derived using an iterative procedure, and on the data sets tested gives improved performance over condensed nearest-neighbour. The approach by Friedman et al.  1977  for ﬁnding nearest neighbours using a k–d tree algorithm does not rely on the triangle inequality  as the LAESA algorithm does . It can be applied to data with a wide variety of dissimilarity measures. The expected number of samples examined is independent of the number of prototypes. A review of the literature on computational procedures is given by Dasarathy  1991 .  Hamamoto et al.  1997  propose generating bootstrap samples by linearly combin- ing local training samples. This increases the design set, rather than reducing it, but gives improved performance over conventional k-nearest-neighbour, particularly in high dimensions.  There are theoretical bounds on the Bayes error rate for the k-nearest-neighbour method. We have given those for the nearest-neighbour rule. For small samples, the true   104 Density estimation – nonparametric  error rate may be very different from the Bayes error rate. The effects on the error rates of k-nearest-neighbour rules of ﬁnite sample size have been investigated by Fukunaga and Hummels  1987a, 1987b  who demonstrate that the bias in the nearest-neighbour error decreases slowly with sample size, particularly when the dimensionality of the data is high. This indicates that increasing the sample size is not an effective means of reducing bias when dimensionality is high. However, a means of compensating for the bias is to obtain an expression for the rate of convergence of the error rate and to predict the asymptotic limit by evaluating the error rate on training sets of different sample size. This has been explored further by Psaltis et al.  1994  who characterise the error rate as an asymptotic series expansion. Leave-one-out procedures for error rate estimation are presented by Fukunaga and Hummels  1987b, 1989 , who ﬁnd that sensitivity of error rate to the choice of k  in a two-class problem  can be reduced by appropriate selection of a threshold for the likelihood function.  3.3.8 Summary  Nearest-neighbour methods have received considerable attention over the years. The simplicity of the approach has made it very popular with researchers. A comprehensive review is given by Dasarathy  1991 , and many of the important contributions to the literature are included in the same volume. It is perhaps conceptually the simplest of the classiﬁcation rules that we present, and the decision rule has been summed up as ‘judge a person by the company he keeps’  Dasarathy, 1991 . The approach requires a set of labelled templates that are used to classify a set of test patterns. The simple-minded implementation of the k-nearest-neighbour rule  calculating the distance of a test pattern from every member of the training set and retaining the class of the k nearest patterns for a decision  is likely to be computationally expensive for a large data set, but for many applications it may well prove acceptable. If you get the answer in minutes, rather than seconds, you may probably not be too worried. The LAESA algorithm trades off storage requirements against computation. LAESA relies on the selection of base prototypes and computes distances of the stored prototypes from these base prototypes.  Additional means of reducing the search time for classifying a pattern using a nearest- neighbour method  and increasing generalisation  are given by the editing and condensing procedures. Both reduce the number of prototypes; editing with the purpose of increasing generalisation performance and condensing with the aim of reducing the number of prototypes without signiﬁcant degradation of the performance. Experimental studies have been performed by Hand and Batchelor  1978 .  Improvements may be obtained through the use of alternative metrics, either local met- rics that use local measures of within-class and between-class distance or non-Euclidean distance.  One question that we have not addressed is the choice of k. The larger the value of k, the more robust is the procedure. Yet k must be much smaller than the minimum of ni , the number of samples in class i, otherwise the neighbourhood is no longer the local neighbourhood of the sample  Dasarathy, 1991 . In a limited study, Enas and Choi  1986  give a rule k ³ N 2=8 or k ³ N 3=8, where N is the population size. The approach described by Dasarathy is to use a cross-validation procedure to classify each sample in the design set using the remaining samples for various values of k and to determine   Expansion by basis functions 105  overall performance. Take the optimum value of k as the one giving the smallest error rate, though the lower the value of k the better from a computational point of view. Keep this value ﬁxed in subsequent editing and condensing procedures if used.  3.4 Expansion by basis functions  The method of density estimation based on an orthogonal expansion by basis functions was ﬁrst introduced by ˘Cencov  1962 . The basic approach is to approximate a density function, p.x , by a weighted sum of orthogonal basis functions. We suppose that the density admits the expansion  p.x  D  ai  cid:16 i .x   1X iD1   3.13   where the f cid:16 ig form a complete orthonormal set of functions satisfying  k.x  cid:16 i .x  cid:16  j .x  dx D ½i Ži j   3.14  for a kernel or weighting function k.x , and Ži j D 1 if i D j and zero otherwise. Thus, multiplying  3.13  by k.x  cid:16 i .x  and integrating gives  Z  Given fx1; x2; : : : ; xng, a set of independently and identically distributed samples from p.x , then the ai can be estimated in an unbiased manner by  Z  ½i ai D  k.x  p.x  cid:16 i .x  dx  ½i Oai D 1 n  nX jD1  k.x j   cid:16 i .x j    The orthogonal series estimator based on the sample fx1; x2; : : : ; xng of p.x  is then given by  Opn .x  D sX iD1  1 n½i  nX jD1  k.x j   cid:16 i .x j   cid:16 i .x    3.15   where s is the number of terms retained in the expansion. The coefﬁcients Oai may be computed sequentially from  ½i Oai .r C 1  D r  r C 1  ½i Oai .r   C 1 r C 1  k.xrC1  cid:16 i .xrC1   where Oai .r C 1  is the value obtained using r C 1 data samples. This means that, given an extra sample point, it is a simple matter to update the coefﬁcients. Also, a large number of data vectors could be used to calculate the coefﬁcients without storing the data in memory.  A further advantage of the series estimator method is that the ﬁnal estimate is easy to store. It is not in the form of a complicated analytic function, but a set of coefﬁcients.   106 Density estimation – nonparametric  But what are the disadvantages? First of all, the method is limited to low-dimensional data spaces. Although, in principle, the method may be extended to estimate multivariate probability density functions in a straightforward manner, the number of coefﬁcients in the series increases exponentially with dimensionality. It is not an easy matter to calculate the coefﬁcients. Another disadvantage is that the density estimate is not necessarily a density  as in the nearest-neighbour method described earlier in this chapter . This may or may not be a problem, depending on the application. Also, the density estimate is not necessarily non-negative. Many different functions have been used as basis functions. These include Fourier and trigonometric functions on [0; 1], and Legendre polynomials on [ cid:8 1; 1]; and those with unbounded support such as Laguerre polynomials on [0;1] and Hermite functions on the real line. If we have no prior knowledge as to the form of p.x , then the basis functions are chosen for their simplicity of implementation. The most popular orthogonal series estimator for densities with unbounded support is the Hermite series estimator. The normalised Hermite functions are given by   cid:16 k.x  D exp. cid:8 x 2=2   .2kk!³  1 2    1 2  Hk .x   where Hk .x  is the kth Hermite polynomial  Hk .x  D . cid:8 1 k exp.x 2   dk dx k exp. cid:8 x 2   The performance and the smoothness of the density estimator depends on the number of terms used in the expansion. Too few terms leads to over-smoothed densities. Different stopping rules  rules for choosing the number of terms, s, in expansion  3.15   have been proposed and are brieﬂy reviewed by Izenman  1991 . Kronmal and Tarter  1962  propose a stopping rule based on minimising a mean integrated squared error. Termination occurs when the test  fails, where  Oa2  j  >  2 n C 1  Ob2  j  Ob2 j D 1 n  nX kD1   cid:16 2 j  .xk    or, alternatively, when t or more successive terms fail the test. Practical and theoretical difﬁculties are encountered with this test, particularly with sharply peaked or multimodal densities, and it could happen that an inﬁnite number of terms pass the test. Alternative procedures for overcoming these difﬁculties have been proposed  Diggle and Hall, 1986; Hart, 1985 .  3.5 Kernel methods  One of the problems with the histogram approach, as discussed earlier in the chapter, is that for a ﬁxed cell dimension, the number of cells increases exponentially with dimension   Kernel methods 107  of the data vectors. This problem can be overcome somewhat by having a variable cell size. The k-nearest-neighbour method  in its simplest form  overcomes the problem by estimating the density using a cell in which the number of design samples is ﬁxed and ﬁnds the cell volume that contains the nearest k. The kernel method  also known as the Parzen method of density estimation, after Parzen, 1962  ﬁxes the cell volume and ﬁnds the number of samples within the cell and uses this to estimate the density. Let us consider a one-dimensional example and let fx1; : : : ; xng be the set of obser- vations or data samples that we shall use to estimate the density. We may easily write down an estimate of the cumulative distribution function as  OP.x  D number of observations  cid:14  x  The density function, p.x , is the derivative of the distribution, but the distribution is discontinuous  at observation values – see Figure 3.16  and its derivative results in a set of spikes at the sample points, xi , and a value zero elsewhere. However, we may deﬁne an estimate of the density as  where h is a positive number. This is the proportion of observations falling within the interval .x  cid:8  h; x C h  divided by 2h. This may be written as  n  2h  OP.x C h   cid:8  OP.x  cid:8  h   Op.x  D  Op.x  D 1 hn  nX iD1  K   cid:10  x  cid:8  xi   cid:11   h  K .z  D  ² 0  1 2  jzj > 1 jzj  cid:14  1   3.16    3.17   where  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  0.5  Š Š 1  Š  Š  1.5  Š Š 2  Š  Š  2.5  Š 3  Figure 3.16 Cumulative distribution   108 Density estimation – nonparametric  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  0.5  Š Š 1  Š  Š 1.5  Š Š 2  Š  Š  2.5  Š 3  3.5  4  Figure 3.17 Probability density estimate with top hat kernel, h D 0.2  since for sample points, xi , within h of x, the summation gives a value of half the number of observations within the interval. Thus each point within the interval contributes equally to the summation. Figure 3.17 gives an estimate of the density using equations  3.16  and  3.17  for the data used to form the cumulative distribution in Figure 3.16.  Figure 3.17 shows us that the density estimate is itself discontinuous. This arises from 1 2hn to the density and the fact that points within a distance h of x contribute a value points further away a value of zero. It is this jump from 1 2hn to zero that gives rise to the discontinuities. We can remove this, and generalise the estimator, by using a smoother weighting function than that given by  3.17 . For example, we could have a weighting function K1.z   also with the property that the integral over the real line is unity  that decreases as jzj increases. Figure 3.18 plots the density estimate for a weighting given by  K1.z  D 1p 2³  exp  ¦  ²  cid:8  z2 2  and a value of h of 0:2. This gives a smoother density estimate. Of course, it does not mean that this estimate is necessarily more ‘correct’ than that of Figure 3.17, but we might suppose that the underlying density is a smooth function and want a smooth estimate.  The above derivation, together with the three ﬁgures, provides a motivation for the kernel method of density estimation, which we formulate as follows. Given a set of observations fx1; : : : ; xng, an estimate of a density function, in one dimension, is taken to be  Op.x  D 1 nh  nX iD1  K   cid:10  x  cid:8  xi   cid:11   h   3.18   where K .z  is termed the kernel function and h is the spread or smoothing parameter  sometimes termed the bandwidth . Examples of popular univariate kernel functions are given in Table 3.2.   Kernel methods 109  0.6  0.5  0.4  0.3  0.2  0.1  0  Š Š 1  Š  Š 1.5  Š Š 2  Š  Š  Š 3  0.5  0 Figure 3.18 Probability density ﬁgure with Gaussian kernel and h D 0.2  3.5  2.5  4  Table 3.2 Commonly used kernel functions for univariate data  Kernel function  Analytic form, K .x   Rectangular  Triangular  Biweight  Normal  1  2 for jxj < 1; 0 otherwise 1  cid:8  jxj for jxj < 1; 0 otherwise  .1  cid:8  x 2 2 for jxj < 1; 0 otherwise 15 16 2³ exp. cid:8 x 2=2  1p p .1  cid:8  x 2=5 = 5 for jxj <  p  3 4  Bartlett–Epanechnikov  5; 0 otherwise  Multivariate kernels   p variables  are usually radially symmetric univariate densi- ties such as the Gaussian kernel K .x  D .2³   cid:8  p=2 exp. cid:8 xT x=2  and the Bartlett– Epanechnikov kernel K .x  D .1  cid:8  xT x . p C 2 =.2c p  for jxj < 1  0 otherwise  where c p D ³ p=2= 0.. p=2  C 1  is the volume of the p-dimensional unit sphere. If we impose the conditions that the kernel K .z  ½ 0 and R K .z dz D 1, then the density estimate Op.x  given by  3.18  also satisﬁes the necessary conditions for a probability density function, p.x  ½ 0 and R p.x  dx D 1.  The theorem due to Rosenblatt  1956  implies that for positive kernels the density estimate will be biased for any ﬁnite sample size  see Section 3.1 . That is, the estimate of the density averaged over an ensemble of data sets is a biased estimate of the true probability density function. In order to obtain an unbiased estimate, we would be required to relax the condition of positivity of the kernels. Thus the estimate of the density function would not necessarily be a density function itself since it may have negative values. To some people, that would not matter. After all, why should the properties of the estimate be the same as the true density? On the other hand, there are some who could not live with   110 Density estimation – nonparametric  an estimate of a probability that had negative values, and so would readily accept a bias. They would also point out that asymptotically the estimate is unbiased  it is unbiased as the number of samples used in the estimate tends to inﬁnity  and asymptotically consistent if certain conditions on the smoothing parameter and the kernel hold. These conditions on the kernel are  Z  jK .z j dz < 1 Z K .z dz D 1 jK .z j < 1 K .z  is ﬁnite everywhere  sup z lim  z!1 jzK .z j D 0  and the conditions on the smoothing parameter are  lim  n!1 h.n  D 0 n!1 nh.n  D 1 for an asymptotic consistent estimate  for an asymptotic unbiased estimate  lim  The effect of changing the smoothing parameter is shown in Figure 3.19. For a large value of h, the density is smoothed and detail is obscured. As h becomes smaller, the density estimate shows more structure and becomes spiky as h approaches zero.  The extension to multivariate data is straightforward, with the multivariate kernel  density estimate deﬁned as  Op.x  D 1 nh p  nX iD1  K   cid:10  1 h   cid:11   .x  cid:8  xi    0.6  0.5  0.4  0.3  0.2  0.1  0  0  0.5  Š Š 1  Š  Š 1.5  Š Š 2  Š  Š  2.5  Š 3  3.5  4  Figure 3.19 Probability density with different levels of smoothing  h D 0.2 and h D 0.5    Kernel methods 111  where K .x  is a multivariate kernel deﬁned for p-dimensional x  Z  R p  K .x  dx D 1  and h is the window width. One form of the probability density function estimate com- monly used is a sum of product kernels  note that this does not imply independence of the variables   Op.x  D 1 n  1  h1 : : : h p  nX iD1  pY jD1  K j   cid:10  [x  cid:8  xi ] j   cid:11   h j  where there is a different smoothing parameter associated with each variable. The K j can take any of the univariate forms in Table 3.2. Usually, the K j are taken to be the same form.  More generally, we may take  Op.x  D Op.x; H   D 1 n  nX iD1  jHj cid:8 1=2 K .H   cid:8 1=2.x  cid:8  xi     where K is a p-variate spherically symmetric density function and H is a symmetric O cid:14 k positive deﬁnite matrix. In a classiﬁcation context, H is commonly taken to be h2 for class !k, where hk is a scaling for class !k and O cid:14 k is the sample covariance matrix. k Various approximations to the covariance are evaluated by Hamamoto et al.  1996  in situations where the sample size is small and the dimensionality is high.  3.5.1 Choice of smoothing parameter  One of the problems with this ‘nonparametric’ method is the choice of the smoothing parameter, h. If h is too small, the density estimator is a collection of n sharp peaks, positioned at the sample points. If h is too large, the density estimate is smoothed and structure in the probability density estimate is lost. The optimal choice of h depends on several factors. Firstly, it depends on the data: the number of data points and their distribution. It also depends on the choice of the kernel function and on the optimality criterion used for its estimation. The maximum likelihood estimation of h that maximises the likelihood  p.x1; : : : ; xnjh   is given by h D 0 – an estimate that consists of a spike at each data point and zero elsewhere. Therefore, some other technique must be used for estimating h. There are many possible methods. Surveys are given in the articles by Jones et al.  1996  and Marron  1988 .  1. Find the average distance between samples and their kth nearest neighbour and use  this for h. A value of k D 10 has been suggested  Hand, 1981a .  2. Find the value of h that minimises the mean integrated squared error between the density and its approximation. For a radially symmetric normal kernel, Silverman   112 Density estimation – nonparametric   1986  suggests  where a choice for ¦ is  h D ¦   cid:11  1 pC4   cid:10  4 p C 2   cid:8  1 pC4  n  ¦ 2 D 1 p  pX iD1  sii   3.19   and sii are the diagonal elements of a sample covariance matrix, possibly a robust estimate  see Chapter 11 . The above estimate will work well if the data come from a population that is normally distributed, but may over-smooth the density if the population is multimodal. A slightly smaller value may be appropriate. You could try several values and assess the misclassiﬁcation rate.  3. There are more sophisticated ways of choosing kernel widths based on least squares cross-validation and likelihood cross-validation. In likelihood cross-validation the value of h is chosen to maximise  Duin, 1976  Opi .xi    nY iD1  where Opi .xi   is the density estimate based on n  cid:8  1 samples  all samples but the ith . However, a major problem with this method is its reported poor performance in the heavy-tailed case.  4. Many bandwidth estimators have been considered for the univariate case. The basic idea behind the ‘plug-in’ estimate is to plug an estimate for the unknown curvature, 4D R . pii  2 dx  the integral of the square of the second derivative of the density , S into the expression for h that minimises the asymptotic mean integrated squared error,  h D h  i1=5  c  d2Sn  where c D R K 2.t  dt and d D R t 2K .t  dt. Jones and Sheather  1991  propose a kernel estimate for the curvature, but this, in turn, requires an estimate of the bandwidth, which will be different from that used to estimate the density. Cao et al.  1994 , in simulations, use  S D n cid:8 2g cid:8 5 X  K i v  i; j   cid:10  xi  cid:8  x j   cid:11   g   cid:10  2K i v.0    cid:11 1=7 OT  cid:8 1=7n cid:8 1=7  g D  d  where K i v is the fourth derivative of K and the smoothing parameter g is given by  in which OT is a parametric estimator of R piii .t  2 dt. Development of the ‘plug-in’ ideas to bandwidth selectors for multivariate data has been considered by Wand and Jones  1994 .   Kernel methods 113  Cao et al.  1994  perform comparative studies on a range of smoothing methods for univariate densities. Although there is no uniformly best estimator, they ﬁnd that the plug-in estimator of Sheather and Jones  1991  shows satisfactory performance over a range of problems.  The previous discussion has assumed that h is a ﬁxed value over the whole of the space of data samples. The ‘optimal’ value of h may in fact be location-dependent, giving a large value in regions where the data samples are sparse and a small value where the data samples are densely packed. There are two main approaches:  i  h D h.x ; that is, h depends on the location of the sample in the data space. Such approaches are often based on nearest-neighbour ideas.  ii  h D h.xi  ; that is, h is ﬁxed for each kernel and depends on the local density. These are termed variable kernel methods.  One particular choice for h is  Breiman et al., 1977   h j D Þkd jk  where Þk is a constant multiplier and d jk is the distance from x j to its kth nearest neighbour in the training design set. However, we still have the problem of parameter estimation – namely that of estimating Þk and k.  Breiman et al.  1977  ﬁnd that good ﬁts can be obtained over a wide range of k  provided Þk satisﬁes  þk  2  4D Þkdk ¦ .dk   D constant  Pn  where dk is the mean of the kth nearest-neighbour distances   1 jD1 d jk  and ¦ .dk  is n their standard deviation. In their simulations, this constant was 3–4 times larger than the best value of h obtained for the ﬁxed kernel estimator.  Other approaches that have a different bandwidth for the kernel associated with each data point employ a ‘pilot’ density estimate to set the bandwidth. Abramson  1982  has proposed a bandwidth inversely proportional to the square root of the density, hp cid:8 1=2.x , which may lead to O.h4  bias under certain conditions on the density  see Terrell and Scott, 1992; Hall et al. 1995 . Although a pilot estimate of p.z  is required, the method is insensitive to the ﬁne detail of the pilot  Silverman, 1986 .  3.5.2 Choice of kernel  Another choice which we have to make in the form of our density estimate is the kernel function. In practice, the most widely used kernel is the normal form   cid:19  x h  K   cid:20  D 1 p  h  2³  exp  ¦  ²   cid:8  x 2 2h2  with product kernels being used for multivariate density estimation. Alternatively, radially symmetric unimodal probability density functions such as the multivariate normal density function are used. There is evidence that the form is relatively unimportant, though the product form may not be ideal for the multivariate case. There are some arguments in favour of kernels that are not themselves densities and admit negative values  Silverman, 1986 .   114 Density estimation – nonparametric  3.5.3 Example application study  The problem The practical problem addressed relates to one in the oil industry: to predict the type of subsurface  for example, sand, shale, coal – the lithofacies class  from physical properties such as electron density, velocity of sound and electrical resistivity obtained from a logging instrument lowered into a well.  Summary In some practical problems, the probability density function of data may vary with time – termed population drift  Hand, 1997 . Thus, the test conditions differ from those used to deﬁne the training data. Kraaijveld  1996  addresses the problem of classiﬁcation when the training data are only approximately representative of the test conditions using a kernel approach to discriminant analysis.  The data Twelve standard data sets were generated from data gathered from two ﬁelds and 18 different well sites. The data sets comprised different-sized feature sets  4 to 24 features  and 2, 3 and 4 classes.  The model A Gaussian kernel is used, with the width estimated by maximising a modiﬁed likelihood function  solved using a numerical root-ﬁnding procedure and giving a width, s1 . An approximation, based on the assumption that the density at a point is determined by the nearest kernel only, is given by  s2 D  vuut 1  pn  nX iD1  jxŁ i  cid:8  xij2  s4 D  vuut 1  pnt  ntX iD1  i  cid:8  xt jxŁ ij2  for a p-dimensional data set fxi ; i D 1; : : : ; ng, where xŁ  is the nearest sample to xi .  i  A robust estimate is also derived using a test data set, again using a modiﬁed likelihood  criterion, to give s3. The nearest-neighbour approximation is  ; i D 1; : : : ; ntg. This provides a modiﬁcation to the kernel width where the test set is fxt using the test set distribution  but not the test set labels . Thus, the test set is used as part of the training procedure.  i  Training procedure Twelve experiments were deﬁned by using different combina- tions of data sets as train and test and ﬁve classiﬁers assessed  kernels with bandwidth estimators s1 to s4 and nearest neighbour .  Results The robust methods led to an increase in performance  measured in terms of error rate  in all but one of the 12 experiments. The nearest-neighbour approximation to the bandwidth tended to underestimate the bandwidth by about 20%.   Kernel methods 115  3.5.4 Further developments  There have been several papers comparing the variable kernel approach with the ﬁxed kernel method. Breiman et al.  1977  ﬁnd superior performance compared to the ﬁxed kernel approach. It seems that a variable kernel method is potentially advantageous when the underlying density is heavily skewed or long-tailed  Remme et al., 1980; Bowman, 1985 . Terrell and Scott  1992  report good performance of the Breiman et al. model for small to moderate sample sizes, but performance deteriorates as sample size grows compared to the ﬁxed kernel approach.  Further investigations into variable kernel approaches include those of Krzyzak  1983   who examines classiﬁcation rules  and Terrell and Scott  1992 . Terrell and Scott con- clude that it is ‘surprisingly difﬁcult to do signiﬁcantly better than the original ﬁxed kernel scheme’. An alternative to the variable bandwidth kernel is the variable location kernel  Jones, et al., 1994 , in which the location of the kernel is perturbed to reduce bias. For multivariate data, procedures for approximating the kernel density using a reduced number of kernels are described by Fukunaga and Hayes  1989a  and Babich and Camps  1996 . Fukunaga and Hayes select a set from the given data set by minimising an entropy expression. Babich and Camps use an agglomerative clustering procedure to ﬁnd a set of prototypes and weight the kernels appropriately  cf. mixture modelling . Jeon and Landgrebe  1994  use a clustering procedure, together with a branch and bound method, to eliminate data samples from the density calculation.  There are several approaches to density estimation that combine parametric and nonparametric approaches  semiparametric density estimators . Hjort and Glad  1995  describe an approach that multiplies an initial parametric start with a nonparametric kernel-type estimate for the necessary correction factor. Hjort and Jones  1996  ﬁnd the best local parametric approximation to a density p.x; θ  , where the parameter values, θ, depend on x.  The kernel methods described in this chapter apply to real-valued continuous quanti- ties. We have not considered issues such as dealing with missing data  see Titterington and Mill, 1983; Pawlak, 1993  or kernel methods for discrete data  see McLachlan, 1992a, for a summary of kernel methods for other data types .  3.5.5 Summary  Kernel methods, both for multivariate density estimation and regression, have been ex- tensively studied. The idea behind kernel density estimation is very simple – put a ‘bump’ function over each data point and then add them up to form a density. One of the dis- advantages of the kernel approach is the high computational requirements for large data sets – there is a kernel at every data point that contributes to the density at a given point, x. Computation can be excessive and for large data sets it may be appropriate to use kernels other than the normal density in an effort to reduce computation. Also, since kernels are localised, only a small proportion will contribute to the density at a given point. Some preprocessing of the data will enable non-contributing kernels to be identiﬁed and omitted from a density calculation. For univariate density estimates, based on the normal kernel, Silverman  1982  proposes an efﬁcient algorithm for computation   116 Density estimation – nonparametric  based on the fact that the density estimate is a convolution of the data with the kernel and the Fourier transform is used to perform the convolution  see also Silverman, 1986; Jones and Lotwick, 1984 . Speed improvements can be obtained in a similar manner to those used for k-nearest-neighbour methods – by reducing the number of prototypes  as in the edit and condense procedures .  The k-nearest-neighbour methods may also be viewed as kernel approaches to density estimation in which the kernel has uniform density in the sphere centred at a point x and of radius equal to the distance to the kth nearest neighbour. The attractiveness of k- nearest-neighbour methods is that the kernel width varies according to the local density, but is discontinuous. The work of Breiman et al. described earlier in this chapter is an attempt to combine the best features of k-nearest-neighbour methods with the ﬁxed kernel approaches.  There may be difﬁculties in applying the kernel method in high dimensions. Regions of high density may contain few samples, even for moderate sample sizes. For example, in the 10-dimensional unit multivariate normal distribution  Silverman, 1986 , 99% of the mass of the distribution is at points at a distance greater than 1.6, whereas in one dimension, 90% of the distribution lies between š1:6. Thus, reliable estimates of the density can only be made for extremely large samples in high dimensions. As an indica- tion of the sample sizes required to obtain density estimates, Silverman considers again the special case of a unit multivariate normal distribution, and a kernel density estimate with normal kernels where the window width is chosen to minimise the mean squared error at the origin. In order that the relative mean squared error, E[. Op.0  cid:8  p.0  2= p2.0 ], is less than 0.1, a small number of samples is required in one and two dimensions  see Table 3.3 .  However, for 10 dimensions, over 800 000 samples are necessary. Thus, in order to obtain accurate density estimates in high dimensions, an enormous sample size is needed. Further, these results are likely to be optimistic and more samples would be required to estimate the density at other points in the distribution to the same accuracy.  Kernel methods are motivated by the asymptotic results and as such are only re- ally relevant to low-dimensional spaces due to sample size considerations. However, as far as discrimination is concerned, we may not necessarily be interested in accurate estimates of the densities themselves, but rather the Bayes decision region for which approximate estimates of the densities may sufﬁce. In practice, kernel methods do work well on multivariate data, in the sense that error rates similar to other classiﬁers can be achieved.  3.6 Application studies  The nonparametric methods of density estimation described in this chapter have been applied to a wide range of problems. Applications of Bayesian networks include the following. ž Drug safety. Cowell et al.  1991  develop a Bayesian network for analysing a speciﬁc adverse drug reaction problem  drug-induced pseudomembranous colitis . The algo- rithm of Lauritzen and Spiegelhalter  1988  was used for manipulating the probability   Application studies 117  Table 3.3 Required sample size as a function of dimension for a relative mean squared error at the origin of less than 0.1 when estimating a standard multivariate normal density using normal kernels with width chosen so that the mean squared error at the origin is minimised  Silverman, 1986   Dimensionality  Required sample size  1 2 3 4 5 6 7 8 9 10  4 19 67 223 768 2 790 10 700 43 700 187 000 842 000  density functions in Bayesian networks; see also Spiegelhalter et al.  1991  for a clear account of the application of probabilistic expert systems.  ž Endoscope navigation. In a study on the use of computer vision techniques for auto- matic guidance and advice in colon endoscopy, Kwoh and Gillies  1996  construct a Bayesian network  using subjective knowledge from an expert  and compare perfor- mance with a maximum weight dependence tree learned from data. The latter gave better performance.  ž Geographic information processing. Stassopoulou et al.  1996  compare Bayesian net- works and neural networks  see Chapters 5 and 6  in a study to combine remote sensing and other data for assessing the risk of desertiﬁcation of burned forest areas in the Mediterranean region. A Bayesian network is constructed using information provided by experts. An equivalent neural network is trained and its parameters used to set the conditional probability tables in the Bayesian network.  ž Image segmentation. Williams and Feng  1998  use a tree-structured network, in con- junction with a neural network, as part of an image labelling scheme. The conditional probability tables are estimated from training data using a maximum likelihood pro- cedure based on the EM algorithm  see Chapter 2 .  A comparative study on 25 data sets of the na¨ıve Bayes classiﬁer and a tree-structured classiﬁer is performed by Friedman et al.  1997 . The tree-structured classiﬁers outper- form the na¨ıve Bayes while maintaining simplicity.   118 Density estimation – nonparametric  Examples of applications of k-nearest-neighbour methods include the following.  ž Target classiﬁcation. Chen and Walton  1986  apply nearest-neighbour methods to ship and aircraft classiﬁcation using radar cross-section measurements. Drake et al.  1994  compare several methods, including k-nearest-neighbour, to multispectral imagery.  ž Handwritten character recognition. There have been many studies in which nearest- neighbour methods have been applied to handwritten characters. Smith et al.  1994  use three distance metrics for a k-nearest-neighbour application to handwritten digits. Yan  1994  uses nearest-neighbour with a multilayer perceptron to reﬁne prototypes. ž Credit scoring. Von Stein and Ziegler  1984  compare several discriminant analysis methods, including nearest-neighbour, to the problem of credit assessment for com- mercial borrowers.  ž Nuclear reactors. In a small data set study, Dubuisson and Lavison  1980  use both k-nearest-neighbour and kernel density estimators to discriminate between two classes of power signals in the monitoring of a high-ﬂux isotope reactor.  ž Astronomy. Murtagh  1994  uses linear methods and k-nearest-neighbour to classify  stellar objects using a variety of feature sets  up to 48 variables .  Example applications of kernel methods are as follows.  ž Philatelic mixtures. Izenman and Sommer  1988  use nonparametric density estimates   and ﬁnite mixture models  to model postage stamp paper thicknesses.  ž Chest pain. Scott et al.  1978  use a quartic kernel to estimate the density of plasma lipids in two groups  diseased and normal . The aim of the investigation was to as- certain the dependence of the risk of coronary artery disease on the joint variation of plasma lipid concentrations.  ž Fruit ﬂy classiﬁcation. Sutton and Steck  1994  use Epanechnikov kernels in a two-  class fruit ﬂy discrimination problem.  ž Forecasting abundance. Rice  1993  uses kernel methods based on Cauchy distributions  in a study of abundance of ﬁsh.  ž Astronomy. Studies involving kernel methods in astronomy include those of  i  De Jager et al.  1986  who compare histogram and kernel approaches to estimate a light curve, which is characteristic of periodic sources of gamma rays;  ii  Merritt and Tremblay  1994  who estimate surface and space density proﬁles of a spherical stellar system; and  iii  Vio et al.  1994  who consider the application of kernel methods to several problems in astronomy – bimodality, rectiﬁcation and intrinsic shape determi- nation of galaxies.  ž Lithofacies recognition from wireline logs. Kraaijveld  1996  considers the problem of subsurface classiﬁcation from well measurements. In particular, he develops the kernel classiﬁcation method to the case where the test data differ from the training data by noise.  A comparative study of different kernel methods applied to multivariate data is re- ported by Breiman et al.  1977  and Bowman  1985 . Hwang et al.  1994a  compare kernel density estimators with projection pursuit density estimation which interprets   Summary and discussion 119  multidimensional density through several one-dimensional projections  see Chapter 6 . Jones and Signorini  1997  perform a comparison of ‘improved’ univariate kernel density estimators; see also Cao et al.  1994  and Titterington  1980  for a comparison of kernels for categorical data.  The Statlog project  Michie et al., 1994  provides a thorough comparison of a wide range of classiﬁcation methods, including k-nearest-neighbour and kernel discriminant analysis  algorithm ALLOC80 . ALLOC80 has a slightly lower error rate than k-nearest- neighbour  for the special case of k D 1 at any rate , but had longer training and test times. More recent comparisons include Liu and White  1995 .  3.7 Summary and discussion  The approaches to discrimination developed in this chapter have been based on estimation of the class-conditional density functions using nonparametric techniques. It is certainly true that we cannot design a classiﬁer that performs better than the Bayes discriminant rule. No matter how sophisticated a classiﬁer is, or how appealing it may be in terms of reﬂecting a model of human decision processes, it cannot outperform the Bayes classiﬁer for any proper performance criterion; in particular, it cannot achieve a lower error rate. Therefore a natural step is to estimate the components of the Bayes rule from the data, namely the class-conditional probability density functions and the class priors. We shall see in later chapters that we do not need to model the density explicitly to get good estimates of the posterior probabilities of class membership.  We have described four nonparametric methods of density estimation: the histogram approach and developments to reduce the number of parameters  na¨ıve Bayes, tree- structured density estimators and Bayesian networks ; the k-nearest-neighbour method leading to the k-nearest-neighbour classiﬁer; series methods; and ﬁnally, kernel meth- ods of density estimation. With advances in computing in recent years, these methods have now become viable and nonparametric methods of density estimation have had an impact on nonparametric approaches to discrimination and classiﬁcation. Of the meth- ods described, for discrete data the developments of the histogram – the independence model, the Lancaster models and maximum weight dependence trees – are easy to im- plement. Learning algorithms for Bayesian networks can be computationally demanding. For continuous data, the kernel method is probably the most popular, with normal kernels with the same window width for each dimension. However, it is reported by Terrell and Scott  1992  that nearest-neighbour methods are superior to ﬁxed kernel approaches to density estimation beyond four dimensions. The kernel method has also been applied to discrete data.  In conclusion, an approach based on density estimation is not without its dangers of course. If incorrect assumptions are made about the form of the distribution in the parametric approach  and in many cases we will not have a physical model of the data generation process to use  or data points are sparse leading to poor kernel density estimates in the nonparametric approach, then we cannot hope to achieve good density estimates. However, the performance of a classiﬁer, in terms of error rate, may not deteriorate too dramatically. Thus, it is a strategy worth trying.   120 Density estimation – nonparametric  3.8 Recommendations  1. Nearest-neighbour methods are easy to implement and are recommended as a starting point for a nonparametric approach. In the Statlog project  Michie et al., 1994 , the k-nearest-neighbour method came out best on the image data sets  top in four and runner-up in two of the six image data sets  and did very well on the whole.  2. For large data sets, some form of data reduction in the form of condensing editing is  advised.  3. As density estimators, kernel methods are not appropriate for high-dimensional data, but if smooth estimates of the density are required they are to be preferred over k- nearest-neighbour. Even poor estimates of the density may still give good classiﬁcation performance.  4. For multivariate data sets, it is worth trying a simple independence model as a base- line. It is simple to implement, handles missing values easily and can give good performance.  5. Domain-speciﬁc and expert knowledge should be used where available. Bayesian net-  works are an attractive scheme for encoding such knowledge.  3.9 Notes and references  There is a large literature on nonparametric methods of density estimation. A good start- ing point is the book by Silverman  1986 , placing emphasis on the practical aspects of density estimation. The book by Scott  1992  provides a blend of theory and applications, placing some emphasis on the visualisation of multivariate density estimates. The article by Izenman  1991  is to be recommended, covering some of the more recent develop- ments in addition to providing an introduction to nonparametric density estimation. Other texts are Devroye and Gy¨orﬁ  1985  and Nadaraya  1989 . The book by Wand and Jones  1995  presents a thorough treatment of kernel smoothing.  The literature on kernel methods for regression and density estimation is vast. A treatment of kernel density estimation can be found in most textbooks on density es- timation. Silverman  1986  gives a particularly lucid account and the book by Hand  1982  provides a very good introduction and considers the use of kernel methods for discriminant analysis. Other treatments, more detailed than that presented here, may be found in the books by Scott  1992 , McLachlan  1992a  and Nadaraya  1989 . A thorough treatment of kernel smoothing is given in the book by Wand and Jones  1995 .  Good introductions to Bayesian networks are provided by Jensen  1996 , Pearl  1988   and Neapolitan  1990  and the article by Heckerman  1999 .  The website www.statistical-pattern-recognition.net contains refer-  ences and links to further information on techniques and applications.   Exercises 121  Exercises  Data set 1: Generate p-dimensional multivariate data  500 samples in train and test sets, equal priors  for two classes: for !1, x ¾ N .µ1;  cid:14 1  and for !2, x ¾ 0:5N .µ2;  cid:14 1  C 0:5N .µ3;  cid:14 3  where µ1 D .0; : : : ; 0 T , µ2 D .2; : : : ; 2 T , µ3 D . cid:8 2; : : : ; cid:8 2 T and  cid:14 1 D  cid:14 2 D  cid:14 3 D I , the identity matrix. Data set 2: Generate data from p-dimensional multivariate data  500 samples in train and test sets, equal priors  for three normally distributed classes with µ1 D .0; : : : ; 0 T , µ2 D .2; : : : ; 2 T , µ3 D . cid:8 2; : : : ; cid:8 2 T and  cid:14 1 D  cid:14 2 D  cid:14 3 D I , the identity matrix. 1. For three variables, X1; X2 and X3 taking one of two values, 1 or 2, denote by Pi j ab  the probability that Xi D a and X j D b. Specifying the density as 21 D P23  12 D P13 P12  11 D P23 11 D P23 11 D P13 P12  12 D P12 12 D 7 16 ; P12  22 D P13 21 D P13  21 D P23 22 D 1  16  22 D 1  4  show that the Lancaster density estimate of the probability p.X1 D 2; X2 D 1, X3 D 2  is negative  D  cid:8 1=64 .  2. For a tree-dependent distribution  equation  3.3  :  and noting that p.x  log. p.x   does not depend on the tree structure, show that minimising the Kullback–Leibler distance  is equivalent to ﬁnding the tree that minimises  pX iD1  X xi ;x j .i    p.xi ; x j .i    log   cid:10  p.xi ; x j .i    p.xi   p.x j .i      cid:11   3. Verify that the Bartlett–Epanechnikov kernel satisﬁes the properties  pt .x  D pY iD1  p.xijx j .i     D D X  x  p.x  log   cid:11    cid:10  p.x  pt .x   Z Z Z  K .t   dt D 1 t K .t   dt D 0 t 2K .t   dt D k2 6D 0  4. Compare and contrast the k-nearest-neighbour classiﬁer with the Gaussian classi- ﬁer. What assumptions do the models make? Also, consider such issues as training requirements, computation time, and storage and performance in high dimensions.   122 Density estimation – nonparametric  5. Consider a sample of n observations .x1; : : : ; xn  from a density p. An estimate Op is calculated using a kernel density estimate with Gaussian kernels for various bandwidths h. How would you expect the number of relative maxima of Op to vary as h increases? Suppose that the xi ’s are drawn from the Cauchy density p.x  D ³=.1C x 2 . Show that the variance of X is inﬁnite. Does this mean that the variance of the Gaussian kernel density estimate Op is inﬁnite?  6. Consider the multivariate kernel density estimate  x 2 R p ,  cid:11   Op.x  D 1 nh p  nX iD1  K   cid:10  1 h  .x  cid:8  xi    Show that the k-nearest-neighbour density estimate given by  3.9  is a special case of the above for suitable choice of K and h  which varies with position, x .  7. Implement a k-nearest-neighbour classiﬁer using data set 1 and investigate its per- formance as a function of dimensionality p D 1; 3; 5; 10 and k. Comment on the results.  8. For the data of data set 1, implement a Gaussian kernel classiﬁer. Construct a separate validation set to obtain a value of the kernel bandwidth  initialise at the value given by  3.19  and vary from this . Describe the results.  9. Implement a base prototype selection algorithm to select nb base prototypes using data set 2. Implement the LAESA procedure and plot the number of distance calcu- lations in classifying the test data as a function of the number of base prototypes, nb, for p D 2; 4; 6; 8 and 10.  10. Using data set 2, implement a nearest-neighbour classiﬁer with edit and condensing. Calculate the nearest-neighbour error rate, the error rate after editing, and the error rate after editing and condensing.  11. Again, for the three-class data above, investigate procedures for choosing k in the  k-nearest-neighbour method.  12. Consider nearest-neighbour with edit and condense. Suggest ways of reducing the ﬁnal number of prototypes by careful initialisation of the condensing algorithm. Plan a procedure to test your hypotheses. Implement it and describe your results.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   4  Linear discriminant analysis  Overview  Discriminant functions that are linear in the features are constructed, resulting in  piecewise  linear decision boundaries. Different optimisation schemes give rise to different methods including the perceptron, Fisher’s linear discriminant func- tion and support vector machines. The relationship between these methods is dis- cussed.  4.1 Introduction  This chapter deals with the problem of ﬁnding the weights of a linear discriminant function. Techniques for performing this task have sometimes been referred to as learning algorithms, and we retain some of the terminology here even though the methods are ones of optimisation or training rather than learning. A linear discriminant function has already appeared in Chapter 2. In that chapter, it arose as a result of a normal distribution assumption for the class densities in which the class covariance matrices were equal. In this chapter, we make no distributional assumptions, but start from the assumption that the decision boundaries are linear. The algorithms have been extensively treated in the literature, but they are included here as an introduction to the nonlinear models discussed in the following chapter, since a stepping stone to the nonlinear models is the generalised linear model in which the discriminant functions are linear combinations of nonlinear functions.  The treatment is divided into two parts: the binary classiﬁcation problem and the multiclass problem. Although the two-class problem is clearly a special case of the multiclass situation  and in fact all the algorithms in the multiclass section can be applied to two classes , the two-class case is of sufﬁcient interest in its own right to warrant a special treatment. It has received a considerable amount of attention and many different algorithms have been proposed.   124 Linear discriminant analysis  4.2 Two-class algorithms  4.2.1 General ideas  In Chapter 1 we introduced the discriminant function approach to supervised classiﬁca- tion; here we brieﬂy restate that approach for linear discriminant functions.  Suppose we have a set of training patterns x1; : : : ; xn, each of which is assigned to one of two classes, !1 or !2. Using this design set, we seek a weight vector w and a threshold w0 such that  or  wT x C w0  ² > 0 < 0  ² > 0 < 0  vT z  ² !1 !2    x 2 ² !1 !2    x 2  where z D .1; x1; : : : ; x p T is the augmented pattern vector and v is a . p C 1 - dimensional vector .w0; w1; : : : ; w p T . In what follows, z could also be .1;  cid:9 1.x ; : : : ;  cid:9 D.x  T , with v a .D C 1 -dimensional vector of weights, where f cid:9 i ; i D 1; : : : ; Dg is a set of D functions of the original variables. Thus, we may apply these algorithms in a transformed feature space.  A sample in class !2 is classiﬁed correctly if vT z < 0. If we were to redeﬁne all samples in class !2 in the design set by their negative values and denote these redeﬁned samples by y, then we seek a value for v which satisﬁes  vT y > 0  for all yi corresponding to xi in the design set i D .1; xT [yT  i D . cid:7 1; cid:7 xT   ; xi 2 !1; yT   ; xi 2 !2]  i  i   4.1   Ideally, we would like a solution for v that makes vT y positive for as many samples in the design set as possible. This minimises the misclassiﬁcation error on the design set. If vT yi > 0 for all members of the design set then the data are said to be linearly separable.  However, it is difﬁcult to minimise the number of misclassiﬁcations. Usually some other criterion is employed. The sections that follow introduce a range of criteria adopted for discrimination between two classes. Some are suitable if the classes are separable, others for overlapping classes. Some lead to algorithms that are deterministic, others can be implemented using stochastic algorithms.  4.2.2 Perceptron criterion  Perhaps the simplest criterion to minimise is the perceptron criterion function  JP .v  D X yi2Y  . cid:7 vT yi    where Y D fyijvT yi < 0g  the set of misclassiﬁed samples . JP is proportional to the sum of the distances of the misclassiﬁed samples to the decision boundary.   Two-class algorithms 125  Error-correction procedure Since the criterion function JP is continuous, we can use a gradient-based procedure, such as the method of steepest descent  Press et al., 1992 , to determine its minimum:  @ JP @v  D X yi2Y  . cid:7 yi    vkC1 D vk C ²k  X yi2Y  yi  which is the sum of the misclassiﬁed patterns, and the method of steepest descent gives a movement along the negative of the gradient with update rule   4.2    4.3   where ²k is a scale parameter that determines the step size. If the sample sets are sep- arable, then this procedure is guaranteed to converge to a solution that separates the sets. Algorithms of the type  4.2  are sometimes referred to as many-pattern adapta- tion or batch update since all given pattern samples are used in the update of v. The corresponding single-pattern adaptation scheme is  vkC1 D vk C ²k yi  where yi is a training sample that has been misclassiﬁed by vk. This procedure cycles through the training set, modifying the weight vector whenever a sample is misclassiﬁed. There are several types of error-correction procedure of the form of  4.3 . The ﬁxed increment rule takes ²k D ², a constant, and is the simplest algorithm for solving systems of linear inequalities. The error-correction procedure is illustrated geometrically in weight space in Figures 4.1 and 4.2. In Figure 4.1, the plane is partitioned by the line vT yk D 0. Since the current estimate of the weight vector vk has vk yk < 0, the weight vector is   cid:3   v2   cid:4    cid:4  vT yk D 0  C  cid:4   cid:4    cid:4   cid:7    cid:4    cid:4    cid:4    cid:4    cid:4    cid:4    cid:4    cid:1  cid:2   yk  cid:1    cid:1    cid:1    cid:1    cid:4    cid:1   cid:4    cid:4    cid:1  cid:2    cid:1    cid:4    cid:4   vkC1   cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:9   cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:7  cid:1    cid:1    cid:1    cid:1   vk   cid:4   Figure 4.1 Perceptron training   cid:5  v1   126 Linear discriminant analysis   cid:10   vT y2 D 0  cid:1  cid:1  C  cid:7   cid:10   cid:10   cid:1  cid:1    cid:3    cid:11   cid:11   vT y3 D 0 C cid:7   cid:11   cid:11   cid:11  ž  Solution Region  ž v0  vT y4 D 0  ž  cid:14   cid:14   5   cid:10    cid:14   ž 2   cid:1   cid:7   C  cid:1    cid:10   cid:10   cid:1  cid:1    cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:13  cid:13  cid:13  cid:13  cid:13  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:13  cid:13  cid:13  cid:13  cid:13  cid:14   cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8    cid:11   cid:11   cid:11   cid:10   cid:11   cid:1   cid:10   cid:11   cid:1   cid:10  cid:10   cid:1   cid:11  cid:11    cid:14   cid:1  cid:1   cid:14  ž   cid:10   cid:10   cid:1  cid:1   cid:1  cid:1    cid:10   cid:10   cid:1  cid:1   C  cid:7    cid:11   cid:11   cid:11    cid:1  cid:1    cid:1  cid:1    cid:1  cid:1    cid:5    cid:1    cid:1    cid:1    cid:1    cid:1   ž 3   cid:10    cid:10   4  1  Figure 4.2 Perceptron training  vT y1 D 0  updated by  for ² D 1  adding on the pattern vector yk. This moves the weight vector towards and possibly into the region vT yk > 0. In Figure 4.2, the lines vT yk D 0 are plotted for four separate training patterns. If the classes are separable, the solution for v must lie in the shaded region  the solution region for which vT yk > 0 for all patterns yk . A solution path is also shown starting from an initial estimate v0. By presenting the patterns y1; y2; y3; y4 cyclically, a solution for v is obtained in ﬁve steps: the ﬁrst change to v0 occurs when y2 is presented  v0 is already on the positive side of vT y1 D 0, so presenting y1 does not update v . The second step adds on y3; the third y2  y4 and y1 are not used because v is on the positive side of both hyperplanes vT y4 D 0 and vT y1 D 0 ; the ﬁnal two stages are the addition of y3, then y1. Thus, from the sequence  y1; Oy2; Oy3; y4; y1; Oy2; Oy3; y4; Oy1  only those vectors with a caret are used. Note that it is possible for an adjustment to undo a correction previously made. In this example, although the iteration started on the right  positive  side of vT y1 D 0, successive iterations of v gave an estimate with vT y1 < 0  at stages 3 and 4 . Eventually a solution with JP .v  D 0 will be obtained for separable patterns.  Variants There are many variants on the ﬁxed increment rule given in the previous section. We consider just a few of them here.   1  Absolute correction rule Choose the value of ² so that the value of vT positive. Thus  kC1yi is  ² > jvT  k yij=jyij2  where yi is the misclassiﬁed pattern presented at the kth step. This means that the iteration corrects for each misclassiﬁed pattern as it is presented. For example, ² may be taken to be the smallest integer greater than jvT  k yij=jyij2.   Two-class algorithms 127   2  Fractional correction rule This sets ² to be a function of the distance to the hyperplane vT yi D 0, i.e.  ² D ½jvT  k yij=jyij2  where ½ is the fraction of the distance to the hyperplane vT yi D 0, traversed in going from vk to vkC1. If ½ > 1, then pattern yi will be classiﬁed correctly after the adjustment to v.   3  Introduction of a margin, b A margin, b > 0, is introduced  see Figure 4.3  and the weight vector is updated whenever vT yi  cid:11  b. Thus, the solution vector v must lie at a distance greater than b=jyij from each hyperplane vT yi D 0. The training procedures given above are still guaranteed to produce a solution when the classes are separable. One of the reasons often given for the introduction of a threshold is to aid generalisation. Without the threshold, some of the points in the data space may lie close to the separating boundary. Viewed in data space, all points xi lie at a distance greater than b=jwj from the separating hyperplane. Clearly, the solution is not unique and in Section 4.2.5 we address the problem of seeking a ‘maximal margin’ classiﬁer.   4  Variable increment ² One of the problems with the above procedures is that, although they will converge if the classes are separable, the solution for v will oscillate if the classes overlap. The error-correction procedure also converges  for linearly separable classes  if ²k satisﬁes the following conditions ²k ½ 0 ²k D 1  1X kD1  v2   cid:3   vT y2 D 0   cid:15   cid:15   cid:15    cid:1  cid:1   cid:15   cid:15   cid:15   cid:1  cid:1   cid:15   cid:15   cid:15   cid:1  cid:1   cid:15   cid:15   cid:1  cid:1    cid:15   cid:15   cid:15   cid:15   cid:16  cid:16  cid:17  cid:16  cid:16  cid:18 b=jy2j  cid:15   cid:15   cid:15   cid:15   cid:15   cid:1  cid:1   cid:15   cid:15   cid:1  cid:1   Solution Region for v   cid:1  cid:1    cid:1  cid:1    cid:1  cid:1    cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13   cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13  cid:13    cid:1  cid:1   cid:11  cid:11  cid:20  b=jy1j  cid:11  cid:11  cid:19    cid:15   cid:15   cid:15    cid:15   cid:15   cid:15    cid:15   cid:15    cid:15   cid:15   Figure 4.3 Solution region for a margin  vT y1 D 0   cid:5   v1   128 Linear discriminant analysis  and  Pm kD1  cid:4 Pm kD1  ²2 k ²k  Ð2 D 0  lim m!1  In many problems, we do not know a priori whether the samples are separable or not. If they are separable, then we would like a procedure that yields a solution that separates the classes. On the other hand, if they are not separable then a method with ²k ! 0 will decrease the effects of misclassiﬁed samples as the iteration proceeds. One possible choice is ²k D 1=k.   5  Relaxation algorithm The relaxation or Agmon–Mays algorithm minimises the criterion  Jr D 1 2  X yi2Y  .vT yi  cid:7  b 2=jyij2  where Y is fyijyT i v  cid:11  bg. Thus not only do the misclassiﬁed samples contribute to Jr , but so also do those correctly classiﬁed samples lying closer than b=jvj to the boundary vT y D 0. The basic algorithm is  where Yk is fyijyT  i vk  cid:11  bg. This has a single-pattern scheme  vkC1 D vk C ²k  X yi2Yk  b  cid:7  vT k yi jyij2  yi  vkC1 D vk C ²k  b  cid:7  vT k yi jyij2  yi  k yi  cid:11  b  that is, the patterns yi that cause the vector v to be corrected . This is  where vT the same as the fractional correction rule with a margin.  4.2.3 Fisher’s criterion  The approach adopted by Fisher was to ﬁnd a linear combination of the variables that separates the two classes as much as possible. That is, we seek the direction along which the two classes are best separated in some sense. The criterion proposed by Fisher is the ratio of between-class to within-class variances. Formally, we seek a direction w such that   4.4   is a maximum, where m1 and m2 are the group means and SW is the pooled within-class sample covariance matrix, in its bias-corrected form given by  JF D  þþwT .m1  cid:7  m2 þþ2  wT SW w   cid:7   1 n  cid:7  2  n1 O cid:8 1 C n2 O cid:8 2   cid:8    Two-class algorithms 129  where O cid:8 1 and O cid:8 2 are the maximum likelihood estimates of the covariance matrices of classes !1 and !2 respectively and there are ni samples in class !i  n1 C n C 2 D n . Maximising the above criterion gives a solution for the direction w. The threshold weight w0 is determined by an allocation rule. The solution for w that maximises JF can be obtained by differentiating JF with respect to w and equating to zero. This yields  wT .m1  cid:7  m2   ²  wT SW w  2.m1  cid:7  m2  C   cid:9  wT .m1  cid:7  m2    cid:10   ¦  D 0  SW w  wT SW w  Since we are interested in the direction of w  and noting that wT .m1  cid:7  m2 =wT SW w is a scalar , we must have  w   S   cid:7 1 W  .m1  cid:7  m2    4.5   We may take equality without loss of generality. The solution for w is a special case of the more general feature extraction criteria described in Chapter 9 that result in trans- formations that maximise a ratio of between-class to within-class variance. Therefore, it should be noted that Fisher’s criterion does not provide us with an allocation rule, merely a mapping to a reduced dimension  actually one dimension in the two-class situation  in which discrimination is in some sense easiest. If we wish to determine an allocation rule, we must specify a threshold, w0, so that we may assign x to class !1 if  wT x C w0 > 0  In Chapter 2 we have seen that if the data were normally distributed with equal covariance matrices, then the optimal decision rule is linear: assign x to !1 if wT xCw0 > 0 where  equations  2.4  and  2.5     cid:7 1 .m1  cid:7  m2  w D S W w0 D  cid:7  1 .m1 C m2 T S 2   cid:7 1 W  .m1  cid:7  m2   cid:7  log   cid:10    cid:9  p.!2  p.!1   Thus, the direction onto which x is projected is the same as that obtained through .m1 cid:7 m2  maximisation of  4.4  and given by  4.5 . This suggests that if we take w D S  unit constant of proportionality giving equality in  4.5  , then we may choose a threshold to be given by w0 above, although we note that it is optimal for normally distributed classes.   cid:7 1 W  Note, however, that the discriminant direction  4.5  has been derived without any assumptions of normality. We have used normal assumptions to set a threshold for dis- crimination. In non-normal situations, a different threshold may be more appropriate. Nevertheless, we may still use the above rule in the more general non-normal case, giving: assign x to !1 if ² x  cid:7  1 2  .m1 C m2    cid:9  p.!2  p.!1   w > log   4.6   ¦T   cid:10   but it will not necessarily be optimal. Note that the above rule is not guaranteed to give a separable solution even if the two groups are separable.   130 Linear discriminant analysis  4.2.4 Least mean squared error procedures  The perceptron and related criteria in Section 4.2.2 are all deﬁned in terms of misclassiﬁed samples. In this section, all the data samples are used and we attempt to ﬁnd a solution vector for which the equality constraints  vT yi D ti    for xi 2 !1 and yT   and . cid:7 1; cid:7 φT  i D are satisﬁed for positive constants ti .  Recall that the vectors yi are deﬁned by yT   for xi 2 !2, or the .DC1 -dimensional vectors .1; xT i   for transformations φ of the data φi D φ.xi   . In general, it will .1; φT i not be possible to satisfy these constraints exactly and we seek a solution for v that minimises a cost function of the difference between vT yi and ti . The particular cost function we shall consider is the mean squared error.  i D . cid:7 1; cid:7 xT  i  i  Solution Let Y be the n ð . p C 1  matrix of sample vectors, with the ith row yi , and t D .t1; : : : ; tn T . Then the sum-squared error criterion is JS D jjY v  cid:7  tjj2   4.7   The solution for v minimising JS is  see Appendix C   where Y † is the pseudo-inverse of Y . If Y T Y is nonsingular, then another form is   4.8   For the given solution for v, the approximation to t is  A measure of how well the linear approximation ﬁts the data is provided by the absolute error in the approximation, or error sum of squares, which is jjOt  cid:7  tjj2 D jjfY .Y T Y   cid:7 1Y T  cid:7  Igtjj2  and we deﬁne the normalised error as  v D Y †t  v D .Y T Y   cid:7 1Y T t  Ot D Y v D Y .Y T Y   cid:7 1Y T t  ž D  ! 1  2   jjOt  cid:7  tjj2 jjt  cid:7  tjj2  t D 1 n  nX iD1  ti  where t D t1, in which   Two-class algorithms 131  is the mean of the values ti and 1 is a vector of 1s. The denominator, jjt  cid:7  tjj2, is the total sum of squares or total variation.  Thus, a normalised error close to zero represents a good ﬁt to the data and a nor- malised error close to one means that the model predicts the data in the mean and represents a poor ﬁt. The normalised error can be expressed in terms of the multiple coefﬁcient of determination, R2, used in ordinary least squares regression as  Dillon and Goldstein, 1984   R2 D 1  cid:7  ž2  Relationship to Fisher’s linear discriminant We have still not said anything about the choice of the ti . In this section we consider a speciﬁc choice which we write  ti D  ² t1 t2  for all xi 2 !1 for all xi 2 !2  Y D   cid:14  u1 X1  cid:7 u2  cid:7 X2  ½  Order the rows of Y so that the ﬁrst n1 samples correspond to class !1 and the remaining n2 samples correspond to class !2. Write the matrix Y as   4.9   where ui .i D 1; 2  is a vector of ni 1s and there are ni samples in class !i .i D 1; 2 . The matrix Xi has ni rows containing the training set patterns and p columns. Then  4.8  may be written  Y T Y v D Y T t  and on substitution for Y from  4.9  and v as .w0; w T this may be rearranged to give   cid:14   n  n1m1 C n2m2 XT  1 C n2mT n1mT 2 1 X1 C XT 2 X2  ½ cid:14  w0  ½   cid:14   D  w  n1t1  cid:7  n2t2  t1n1m1  cid:7  t2n2m2  ½  where mi is the mean of the rows of Xi . The top row of the matrix gives a solution for w0 in terms of w as  w0 D  cid:7 1  n  .n1mT  1 C n2mT  2   w C n1 n  t1  cid:7  n2 n  t2   4.10   and the second row gives  .n1m1 C n2m2 w0 C .XT  1 X1 C XT  2 X2 w D t1m1n1  cid:7  t2m2n2  Substituting for w0 from  4.10  and rearranging gives  n  nSW C n1n2 n  .m1  cid:7  m2 .m1  cid:7  m2 To  w D .m1  cid:7  m2   n1n2  .t1 C t2   n   4.11   where SW is the estimate of the assumed common covariance matrix, written in terms of mi and Xi , i D 1; 2, as SW D 1 n  2 X2  cid:7  n1m1mT  1  cid:7  n2m2mT  1 X1 C XT XT  o  n  2   132 Linear discriminant analysis  Whatever the solution for w, the term  in  4.11  is in the direction of m1  cid:7  m2. Thus,  4.11  may be written  n1n2  .m1  cid:7  m2 .m1  cid:7  m2 T w  n  nSW w D Þ.m1  cid:7  m2   for some constant of proportionality, Þ, with solution .m1  cid:7  m2   S   cid:7 1 W  w D Þ n  the same solution obtained for Fisher’s linear discriminant  4.5 . Thus, provided that the value of ti is the same for all members of the same class, we recover Fisher’s linear discriminant. We require that t1 C t2 6D 0 to prevent a trivial solution for w. In the spirit of this approach, discrimination may be performed according to whether w0 C wT x is closer in the least squares sense to t1 than  cid:7 w0  cid:7  wT x is to t2. That is, assign x to !1 if jjt1  cid:7  .w0 C wT x jj2 < jjt2 C .w0 C wT x jj2. Substituting for w0 and w, this simpliﬁes to  assuming Þ.t1 C t2  > 0 : assign x to !1 if n2  cid:7  n1   4.12  where m is the sample mean, .n1m1 C n2m2 =n. The threshold on the right-hand side of the inequality above is independent of t1 and t2 – see the exercises at the end of the chapter.  .m1  cid:7  m2   .x  cid:7  m  >  t1 C t2   cid:7  S   cid:7 1 W   cid:8 T  2  Þ  Of course, other discrimination rules may be used, particularly in view of the fact that the least squares solution gives Fisher’s linear discriminant, which we know is the optimal discriminant for two normally distributed classes with equal covariance matrices. Compare the one above with  4.6  that incorporates the numbers in each class in a different way.  Optimal discriminant Another important property of the measured squared error solution is that it approaches the minimum mean squared error approximation to the Bayes discriminant function, g.x , given by  g.x  D p.!1jx   cid:7  p.!2jx   In order to understand what this statement means, consider JS given by  4.7  where  in the limit as the number of samples tends to inﬁnity. ti D 1 for all yi , so that JS D X x2!1  .w0 C wT x  cid:7  1 2 C X x2!2  .w0 C wT x C 1 2   4.13   where we have assumed linear dependence of the yi on the xi . Figure 4.4 illustrates the minimisation process taking place. For illustration, ﬁve samples are drawn from each of   Two-class algorithms 133   cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12   Š ž Š  Š Š  Š  ž  ž  ž  ž  1  0.8  0.6  0.4  0.2  0  cid:7 0.2  cid:7 0.4  cid:7 0.6  cid:7 0.8  cid:7 1   cid:7 2   cid:7 1  0  1  2  3  4  5  6  Figure 4.4 Optimality of least mean squared error rule – illustration of equation  4.13   two univariate normal distributions of unit variance and means 0.0 and 2.0 and plotted on the x-axis of Figure 4.4, Š for class !1 and ž for class !2. Minimising JS means that the sum of the squares of the distances from the straight line in Figure 4.4 to either C1 for class !1 or  cid:7 1 for class !2 is minimised. Also plotted in Figure 4.4 is the optimal Bayes discriminant, g.x , for the two normal distributions.  As the number of samples, n, becomes large, the expression JS=n tends to D p.!1   .w0 C wT x  cid:7  1 2 p.xj!1  dx C p.!2   .w0 C wT x C 1 2 p.xj!2  dx  Z  Z  JS n  Expanding and simplifying, this gives  JS n  Z Z  D D  .w0 C wT x 2 p.x  dx C 1  cid:7  2 .w0 C wT x  cid:7  g.x  2 p.x  dx C 1  cid:7   Z  g2.x  dx  Z  .w0 C wT x g.x  p.x  dx  Since only the ﬁrst integral in the above expression depends on w0 and w, we have the result that minimising  4.13  is equivalent, as the number of samples becomes large, to minimising  Z  .w0 C wT x  cid:7  g.x  2 p.x  dx   4.14   which is the minimum squared error approximation to the Bayes discriminant function. This is illustrated in Figure 4.5. The expression  4.14  above is the squared difference between the optimal Bayes discriminant and the straight line, integrated over the distri- bution, p.x .  Note that if we were to choose a suitable basis  cid:9 1; : : : ;  cid:9 D, transform the feature vector x to . cid:9 1.x ; : : : ;  cid:9 D.x  T and then construct the linear discriminant function, we might get a closer approximation to the optimal discriminant, and the decision boundary   134 Linear discriminant analysis   cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12   Š ž Š  Š Š  Š  ž  ž  ž  ž  1  0.8  0.6  0.4  0.2  0  cid:7 0.2  cid:7 0.4  cid:7 0.6  cid:7 0.8  cid:7 1   cid:7 2   cid:7 1  0  1  2  3  4  5  6  Figure 4.5 Least mean squared error approximation to the Bayes discriminant rule – illustration of equation  4.14   would not necessarily be a straight line  or plane  in the original space of the variables, x. Also, although asymptotically the solution gives the best approximation  in the least squares sense  to the Bayes discriminant function, it is inﬂuenced by regions of high density rather than samples close to the decision boundary. Although Bayesian heuristics motivate the use of a linear discriminant trained by least squares, it can give poor decision boundaries in some circumstances  Hastie et al., 1994 .  4.2.5 Support vector machines  As we stated in the introduction to this section, algorithms for linear discriminant func- tions may be applied to the original variables or in a transformed feature space deﬁned by nonlinear transformations of the original variables. Support vector machines are no exception. They implement a very simple idea – they map pattern vectors to a high- dimensional feature space where a ‘best’ separating hyperplane  the maximal margin hyperplane  is constructed  see Figure 4.6 .  A  Š Š Š  Š  Š ŠŠ Š ŠŠ Š Š  ž ž ž ž  ž ž žž ž žž ž ž ž   a    cid:1  Š  cid:1  Š  cid:1  ŠŠ Š Š Š  cid:1   cid:1  Š  cid:1  ŠŠ  cid:1   cid:1  Š ž ž ž  cid:1   cid:1  ž žž ž Š  cid:1   cid:1  žž  cid:1  ž  cid:1  ž ž ž  cid:1   cid:1  ž  cid:1    cid:1  A   b   Figure 4.6 Two linearly separable sets of data with separating hyperplane. The separating hyper- plane on the right  the thick line  leaves the closest points at maximum distance. The thin lines on the right identify the margin   Two-class algorithms 135  In this section we introduce the basic ideas behind the support vector model and in Chapter 5 we develop the model further in the context of neural network classiﬁers. Much of the work on support vector classiﬁers relates to the binary classiﬁcation problem, with the multiclass classiﬁer constructed by combining several binary classiﬁers  see Section 4.3.7 .  Linearly separable data Consider the binary classiﬁcation task in which we have a set of training patterns fxi ; i D 1; : : : ; ng assigned to one of two classes, !1 and !2, with corresponding labels yi D š1. Denote the linear discriminant function  g.x  D wT x C w0  with decision rule wT x C w0  ² > 0 < 0    x 2  ² !1 with corresponding numeric value yi D C1 !2 with corresponding numeric value yi D  cid:7 1  Thus, all training points are correctly classiﬁed if  yi .wT xi C w0  > 0 for all i  This is an alternative way of writing  4.1 .  Figure 4.6a shows two separable sets of points with a separating hyperplane, A. Clearly, there are many possible separating hyperplanes. The maximal margin classiﬁer determines the hyperplane for which the margin – the distance to two parallel hyperplanes on each side of the hyperplane A that separates the data – is the largest  Figure 4.6b . The assumption is that the larger the margin, the better the generalisation error of the linear classiﬁer deﬁned by the separating hyperplane.  In Section 4.2.2, we saw that a variant of the perceptron rule was to introduce a  margin, b > 0, and seek a solution so that  yi .wT xi C w0  ½ b  The perceptron algorithm yields a solution for which all points xi are at a distance greater than b=jwj from the separating hyperplane. A scaling of b, w0 and w leaves this distance unaltered and the condition  4.15  still satisﬁed. Therefore, without loss of generality, a value b D 1 may be taken, deﬁning what are termed the canonical hyperplanes, H1 : wT x C w0 D C1 and H2 : wT x C w0 D  cid:7 1, and we have   4.15    4.16   wT xi C w0 ½ C1 for yi D C1 wT xi C w0  cid:11   cid:7 1 for yi D  cid:7 1  The distance between each of these two hyperplanes and the separating hyperplane, g.x  D 0, is 1=jwj and is termed the margin. Figure 4.7 shows the separating hyperplane and the canonical hyperplanes for two separable data sets. The points that lie on the canonical hyperplanes are called support vectors  circled in Figure 4.7 .   136 Linear discriminant analysis   cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:3    cid:1   H1 : wT x C w0 D C1  cid:1  Š  cid:1   Š  Š  cid:1  Š  cid:1   cid:1    cid:1   cid:4  cid:21    cid:4   cid:1    cid:1    cid:1    cid:1    cid:4   cid:1    cid:1    cid:1    cid:1   cid:1  ž  cid:4   cid:4  jw0j jwj žž  Š   cid:4   Š  cid:4  cid:21  w Š  Š  Š   cid:1  Š  cid:1   cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1   cid:4    cid:4   cid:1    cid:1    cid:1    cid:1    cid:1    cid:1  ž  cid:1  ž  cid:1  ž   cid:1    cid:1    cid:1  g < 0  cid:1   cid:1  H2 : wT x C w0 D  cid:7 1   cid:1   ž  cid:4  cid:22   ž  cid:4  ž  origin   cid:5    cid:1  g > 0 hyperplane, g.x  D wT x C w0 D 0   cid:1   Figure 4.7 H1 and H2 are the canonical hyperplanes. The margin is the perpendicular distance between the separating hyperplane  g.x   D 0  and a hyperplane through the closest points  marked by a ring around the data points . These are termed the support vectors  Therefore, maximising the margin means that we seek a solution that minimises jwj  subject to the constraints C1 :  yi .wT xi C w0  ½ 1 i D 1; : : : ; n   4.17   A standard approach to optimisation problems with equality and inequality constraints is the Lagrange formalism  Fletcher, 1988  which leads to the primal form of the objective function, L p, given by1  L p D 1 2  wT w  cid:7  nX iD1  Þi .yi .wT xi C w0   cid:7  1    4.18   where fÞi ; i D 1; : : : ; n; Þi ½ 0g are the Lagrange multipliers. The primal parameters are w and w0 and the number of parameters is p C 1, where p is the dimensionality of the feature space.  The solution to the problem of minimising wT w subject to constraints  4.17  is equivalent to determining the saddlepoint of the function L p, at which L p is minimised with respect to w and w0 and maximised with respect to the Þi . Differentiating L p with  1For inequality constraints of the form ci ½ 0, the constraints equations are multiplied by positive Lagrange  multipliers and subtracted from the objective function.   Two-class algorithms 137  respect to w and w0 and equating to zero yields  nX iD1  Þi yi D 0 w D nX iD1  Þi yi xi  Substituting into  4.18  gives the dual form of the Lagrangian  L D D nX iD1  Þi  cid:7  1 2  nX iD1  nX jD1  Þi Þ j yi y j xT  i x j  which is maximised with respect to the Þi subject to  Þi ½ 0  Þi yi D 0  nX iD1  The importance of the dual form is that it expresses the optimisation criterion as in- ner products of patterns, xi . This is a key concept and has important consequences for nonlinear support vector machines discussed in Chapter 5. The dual variables are the Lagrange multipliers, Þi , and so the number of parameters is n, the number of pat- terns.  Karush–Kuhn–Tucker conditions In the above, we have reformulated the primal problem in an alternative dual form which is often easier to solve numerically. The Kuhn–Tucker conditions provide nec- essary and sufﬁcient conditions to be satisﬁed when minimising an objective function subject to inequality and equality constraints. In the primal form of the objective function, these are  Þi yi xi D 0  @ L p @w  @ L p @w0  D w  cid:7  nX iD1 D  cid:7  nX Þi yi D 0 iD1  yi .xT  i w C w0   cid:7  1 ½ 0 Þi ½ 0 i w C w0   cid:7  1  D 0  Þi .yi .xT  i w C w0   cid:7  1  D 0  known as the Karush–Kuhn– In particular, the condition Þi .yi .xT Tucker complementarity condition–product of the Lagrange multiplier and the inequality i wC w0  cid:7  1  D constraint  implies that for active constraints  the solution satisﬁes yi .xT 0  then Þi ½ 0; otherwise, for inactive constraints Þi D 0. For active constraints,   4.19    4.20    4.21    4.22    138 Linear discriminant analysis  the Lagrange multiplier represents the sensitivity of the optimal value of L p to the particular constraint  Cristianini and Shawe-Taylor, 2000 . These data points with non- zero Lagrange multiplier lie on the canonical hyperplanes. These are termed the support vectors and are the most informative points in the data set. If any of the other pat- terns  with Þi D 0  were to be moved around  provided that they do not cross one of the outer–canonical–hyperplanes , they would not affect the solution for the separating hyperplane.  Classiﬁcation Recasting the constrained optimisation problem in its dual form enables numerical quadratic programming solvers to be employed. Once the Lagrange multipliers, Þi , have been obtained, the value of w0 may be found from  Þi .yi .xT  i w C w0   cid:7  1  D 0  using any of the support vectors  patterns for which Þi support vectors  nsvw0 C wT X i2SV  xi D X i2SV  yi  6D 0 , or an average over all  where nsv is the number of support vectors and the summations are over the set of support vectors, SV. The solution for w used in the above is given by  4.19 :   4.23    4.24   since Þi D 0 for other patterns. Thus, the support vectors deﬁne the separating hyperplane.  A new pattern, x, is classiﬁed according to the sign of  w D X i2SV  Þi yi xi  wT x C w0  Substituting for w and w0 gives the linear discriminant: assign x to !1 if  X i2SV  Þi yi xT  i x  cid:7  1 nsv  X i2SV  X j2SV  Þi yi xT  i x j C 1 nsv  X i2SV  yi > 0  Linearly non-separable data In many real-world practical problems there will be no linear boundary separating the classes and the problem of searching for an optimal separating hyperplane is meaningless. Even if we were to use sophisticated feature vectors, φ.x , to transform the data to a high-dimensional feature space in which classes are linearly separable, this would lead to an over-ﬁtting of the data and hence poor generalisation ability. We shall return to nonlinear support vector machines in Chapter 5. However, we can extend the above ideas to handle non-separable data by relaxing the constraints  4.16 . We do this by introducing ‘slack’ variables ¾i ; i D 1; : : : ; n, into   Two-class algorithms 139  the constraints to give  wT xi C w0 ½ C1  cid:7  ¾i wT xi C w0  cid:11   cid:7 1 C ¾i ¾i ½ 0  for yi D C1 for yi D  cid:7 1 i D 1; : : : ; n   4.25    4.26   For a point to be misclassiﬁed by the separating hyperplane, we must have ¾i > 1  see Figure 4.8 . A convenient way to incorporate the additional cost due to non-separability is to introduce an extra cost term to the cost function by replacing wT w=2 by wT w=2CC P ¾i where C is a ‘regularisation’ parameter. The term C P ¾i can be thought of as measuring some amount of misclassiﬁcation – the lower the value of C, the smaller the penalty for ‘outliers’ and a ‘softer’ margin. Other penalty terms are possible, for example, C P ¾ 2 i  see Vapnik, 1998 .  i  i  i  Thus, we minimise  wT w C C  1 2  X  ¾i  i  subject to the constraints  4.25 . The primal form of the Lagrangian  4.18  now becomes  L p D 1 2  wT w C C  X  i  ¾i  cid:7  nX iD1  Þi .yi .wT xi C w0   cid:7  1 C ¾i    cid:7  nX iD1  ri ¾i   4.27   where Þi ½ 0 and ri ½ 0 are Lagrange multipliers; ri are introduced to ensure positi- vity of ¾i .  H1 :  T x +  0 = + 1  i  0  origin  g < 0  g > 0 g x  =  T x +  0 = 0  H1 :  T x +  0 = + − 1  Figure 4.8 Linear separating hyperplane for non-separable data   140 Linear discriminant analysis  Differentiating with respect to w and w0 still results in  4.19   and differentiating with respect to ¾i yields  Substituting the results  4.28  above into the primal form  4.27  and using  4.29  gives the dual form of the Lagrangian  L D D nX iD1  Þi  cid:7  1 2  nX iD1  nX jD1  Þi Þ j yi y j xT  i x j  which is the same form as the maximal margin classiﬁer  4.20 . This is maximised with respect to the Þi subject to   4.28    4.29    4.30   nX iD1  Þi yi D 0 w D nX iD1  Þi yi xi  C  cid:7  Þi  cid:7  ri D 0  nX Þi yi D 0 iD1 0  cid:11  Þi  cid:11  C  The latter condition follows from  4.29  and ri ½ 0. Thus, the only change to the maximisation problem is the upper bound on the Þi .  The Karush–Kuhn–Tucker complementarity conditions are i w C w0   cid:7  1 C ¾i   D 0 ri ¾i D .C  cid:7  Þi  ¾i D 0  Þi .yi .xT  Patterns for which Þi > 0 are termed the support vectors. Those satisfying 0 < Þi < C must have ¾i D 0 – that is, they lie on one of the canonical hyperplanes at a distance of 1=jwj from the separating hyperplane  these support vectors are sometimes termed margin vectors . Non-zero slack variables can only occur when Þi D C. In this case, the points xi are misclassiﬁed if ¾i > 1. If ¾i < 1, they are classiﬁed correctly, but lie closer to the separating hyperplane than 1=jwj. As in the separable case, the value of w0 is determined using the ﬁrst condition above and any support vector or by summing over samples for which 0 < Þi < C  for which ¾i D 0   equation  4.23   and w is given by  4.24 . This gives  w0 D 1 NfSV  8< X : i2fSV  yi  cid:7  X i2SV; j2fSV  9= ;  Þi yi xT  i x j   4.31   where SV is the set of support vectors with associated values of Þi satisfying 0 < Þi  cid:11  C and fSV is the set of NfSV support vectors satisfying 0 < Þi < C  those at the target distance of 1=jwj from the separating hyperplane .   Two-class algorithms 141   cid:1    cid:1    cid:1    cid:1    cid:1    cid:1  cid:2  cid:1    cid:1    cid:1   C   cid:1  cid:2  cid:1    cid:1   C cid:2   C C  C C   cid:1  cid:2  cid:1   C  C  C  C cid:2  C cid:2    cid:1  cid:2  cid:1  cid:2   C C  C C  C  C  Figure 4.9 Linearly separable data  left  and non-separable data  right, C D 20   Figure 4.9 shows the optimal separating hyperplane for linearly separable and non- separable data. The support vectors  Þi > 0  are circled. All points that are not support vectors lie outside the margin strip  Þi D 0; ¾i D 0 . In the right-hand ﬁgure, one of the support vectors  from class C  is incorrectly classiﬁed  ¾i > 1 .  The only free parameter is the regularisation parameter, C. A value may be chosen by varying C through a range of values and monitoring the performance of the classiﬁer on a separate validation set, or by using cross-validation  see Chapter 11 .  4.2.6 Example application study  The problem Classiﬁcation of land cover using remote sensing satellite imagery  Brown et al., 2000 .  Summary A conventional classiﬁcation technique developed in the remote sens- ing community  linear spectral mixture models  is compared, both theoretically and practically, to support vector machines. Under certain circumstances the methods are equivalent.  The data The data comprise measurements of two classes of land cover: developed and other  including slate, tarmac, concrete  and undeveloped and vegetation  including sand, water, soil, grass, shrubs, etc. . The measurements are Landsat images of the suburbs of Leicester, UK, in two frequency bands. Each image is 33ð33 pixels in size and a pattern is a two-dimensional pair of measurements of corresponding pixels in each of the two bands. Training and test sets were constructed, the training set consisting of ‘pure’ pixels  those that relate to a region for which there is a single class .  The model A support vector machine model was adopted. Linear separable and linear non-separable support vector machines were trained  by maximising  4.18  and  4.27   using patterns selected from the training set.   142 Linear discriminant analysis  Training procedure The value of the regularisation parameter, C, was chosen to min- imise a sum-squared error criterion evaluated on the test set.  4.2.7 Further developments  The main developments of the two-class linear algorithms are as follows.  1. Multiclass algorithms. These are discussed in the following section.  2. Nonlinear methods. Many classiﬁcation methods that produce nonlinear decision boundaries are essentially linear models: they are linear combinations of nonlinear functions of the variables. Radial basis function networks are one example. Thus the machinery developed in this chapter is important. This is examined further in Chapter 5.  3. Regularisation – introducing a parameter that controls the sensitivity of the technique to small changes in the data or training procedure and improves generalisation. This includes combining multiple versions of the same classiﬁer, trained under different conditions  see Chapter 8 and Skurichina, 2001 .  4.2.8 Summary  In this section we have considered a range of techniques for performing linear discrim- ination in the two-class case. These are summarised in Table 4.1. They fall broadly into two groups: those techniques that minimise a criterion based on misclassiﬁed sam- ples and those that use all samples, correctly classiﬁed or not. The former group in- cludes the perceptron, relaxation and support vector machine algorithms. The latter group includes Fisher’s criterion and criteria based on a least squares error measure, including the pseudo-inverse method.  Table 4.1 Summary of linear techniques  Procedure name  Perceptron  Relaxation  Fisher  Criterion  .vT yi  cid:7  b 2  . cid:7 vT yi    JP .v  D X yi2Y X yi2Y  Jr D 1 jyij2 2 JF D jwT .m1  cid:7  m2 j2 JS D jjY v  cid:7  tjj2  wT SW w  vkC1 D vk C ²k  vkC1 D vk C ²k  yi  Algorithm X yi2Y b  cid:7  vT k yi jyij2  yi  .m1  cid:7  m2    cid:7 1 W  w   S Ov D Y †t  quadratic programming  Least mean squared error–pseudo-inverse Support vector machine wT w C C P  ¾i  i  subject to constraints  4.25    Two-class algorithms 143  A perceptron is a trainable threshold logic unit. During training, weights are adjusted to minimise a speciﬁc criterion. For two separable classes, the basic error-correction procedure converges to a solution in which the classes are separated by a linear deci- sion boundary. If the classes are not separable, the training procedure must be modiﬁed to ensure convergence. More complex decision surfaces can be implemented by using combinations and layers of perceptrons  Minsky and Papert, 1988; Nilsson, 1965 . This we shall discuss further in Chapter 6.  Some of the techniques will ﬁnd a solution that separates two classes if they are separable, others do not. A further dichotomy is between the algorithms that converge for non-separable classes and those that do not.  The least mean squared error design criterion is widely used in pattern recognition. It can be readily implemented in many of the standard computer programs for regression and we have shown how the discrimination problem may be viewed as an exercise in regression. The linear discriminant obtained by the procedure is optimal in the sense of providing a minimum mean squared error approximation to the Bayes discriminant function. The analysis of this section applies also to generalised linear discriminant functions  the variables x are replaced by φ.x  . Therefore, choosing a suitable basis for the  cid:9  j .x  is important since a good set will lead to a good approximation to the Bayes discriminant function.  One problem with the least mean squared error procedure is that it is sensitive to outliers and does not necessarily produce a separable solution, even when the classes are separable by a linear discriminant. Modiﬁcations of the least mean square rule to ensure a solution for separable sets have been proposed  the Ho–Kashyap procedure which adjusts both the weight vector, v, and the target vector, t , but the optimal approximation to the Bayes discriminant function when the sets overlap is no longer achieved.  The least mean squared error criterion does possess some attractive theoretical prop- erties that we can now quote without proof. Let E1 denote the nearest-neighbour error rate, Emse the least mean squared error rate and let v be the minimum error solution. Then  Devijver and Kittler, 1982   JS.v =n 1  cid:7  JS.v =n  ½ Emse  JS.v =n ½ 2E1 ½ 2EŁ JS.v =n D 2E1   Emse D EŁ   4.32   where EŁ is the optimal Bayes error rate.  The ﬁrst condition gives an upper bound on the error rate  and may easily be computed from the values of JS.v  delivered by the algorithm above . It seems sensible that if we have two possible sets of discriminant functions,  cid:9 i and ¹i , then if J  cid:9  S , then the S set  cid:9  should be preferred since it gives a smaller upper bound for the error rate, Emse. Of course, this is not sufﬁcient but gives us a reasonable guideline. The second two conditions show that the value of the criterion function JS=n is bounded below by twice the nearest-neighbour error rate E1, and JS=n D 2E1 if the linear discriminant function has the same sign as the Bayes discriminant function  crosses the x-axis at the same points .  < J ¹  Support vector machines have been receiving increasing research interest in recent years. They provide an optimally separating hyperplane in the sense that the margin   144 Linear discriminant analysis  between two groups is maximised. Development of this idea to the nonlinear classiﬁer, discussed in Chapter 5, had led to classiﬁers with remarkable good generalisation ability.  4.3 Multiclass algorithms  4.3.1 General ideas  There are several ways of extending two-class procedures to the multiclass case.  One-against-all For C classes, construct C binary classiﬁers. The kth classiﬁer is trained to discriminate patterns in class !k from those in the remaining classes. Thus, determine the weight vector, wk, and the threshold, wk ² > 0 < 0  0, such that ²   x 2  !1; : : : ; !k cid:7 1; !kC1; : : : ; !C  .wk  T x C wk  !k  0  Ideally, for a given pattern x, the quantity gk .x  D .wk T x C wk 0 will be positive for one value of k and negative for the remainder, giving a clear indication of class. However, this procedure may results in a pattern x belonging to more than one class, or belonging to none. If there is more than one class for which the quantity gk .x  is positive, x may be assigned to the class for which ..wk  T x C wk  =jwkj  the distance to the hyperplane  is the largest. If all values of gk .x  are negative, then assign x to the class with smallest value of j..wk  T x C wk   j=jwkj.  0  0  One-against-one Construct C.C  cid:7  1  classiﬁers. Each classiﬁer discriminates between two classes. A pattern x is assigned using each classiﬁer in turn and a majority vote taken. This can lead to ambiguity, with no clear decision for some patterns.  Discriminant functions A third approach is, for C classes, to deﬁne C linear discriminant functions g1.x ; : : : ; gC .x  and assign x to class !i if  gi .x  D max  g j .x   j  that is, x is assigned to the class whose discriminant function is the largest value at x. If  gi .x  D max  g j .x  , p.!ijx  D max  p.! jjx   j  j  then the decision boundaries obtained will be optimal in the sense of the Bayes minimum error.  The structure of this section follows that of Section 4.2, covering error-correction procedures, generalisations of Fisher’s discriminant, minimum squared error procedures and support vector machines.   Multiclass algorithms 145  4.3.2 Error-correction procedure  A generalisation of the two-class error-correction procedure for C > 2 classes is to deﬁne C linear discriminants  gi .x  D vT i z  where z is the augmented data vector, zT D .1; xT  . The generalised error-correction procedure is used to train the classiﬁer. Arbitrary initial values are assigned to the vi and each pattern in the training set is considered one at a time. If a pattern belonging to class !i is presented and the maximum value of the discriminant functions is for the jth discriminant function  i.e. a pattern in class !i is classiﬁed as class ! j   then the weight vectors vi and v j are modiﬁed according to  0 i D vi C cz v 0 j D v j  cid:7  cz  v  where c is a positive correction increment. That is, the value of the ith discriminant function is increased for pattern z and the value of the jth discriminant is decreased. This procedure will converge in a ﬁnite number of steps if the classes are separable  see Nilsson, 1965 . Convergence may require the data set to be cycled through several times  as in the two-class case .  Choosing c according to  c D .v j  cid:7  vi  T z jzj2  will ensure that after adjustment of the weight vectors, z will be correctly classiﬁed.  4.3.3 Fisher’s criterion – linear discriminant analysis  The term linear discriminant analysis  LDA , although generically referring to techniques that produce discriminant functions that are linear in the input variables  and thus applying to the perceptron and all of the techniques of this chapter , is also used in a speciﬁc sense to refer to the technique of this subsection in which a transformation is sought that, in some sense, maximises between-class separability and minimises within-class variability. The characteristics of the method are: 1. A transformation is produced to a space of dimension at most C  cid:7  1, where C is the  2. The transformation is distribution-free – for example, no assumption is made regarding  number of classes.  normality of the data.  3. The axes of the transformed coordinate system can be ordered in terms of ‘importance for discrimination’. Those most important can be used to obtain a graphical represen- tation of the data by plotting the data in this coordinate system  usually two or three dimensions .   146 Linear discriminant analysis  4. Discrimination may be performed in this reduced-dimensional space using any con- venient classiﬁer. Often improved performance is achieved over the application of the rule in the original data space. If a nearest class mean type rule is employed, the decision boundaries are linear  and equal to those obtained by a Gaussian classiﬁer under the assumption of equal covariance matrices for the classes .  5. Linear discriminant analysis may be used as a post-processor for more complex,  nonlinear classiﬁers.  There are several ways of generalising the criterion JF  4.4  to the multiclass case. Optimisation of these criteria yields transformations that reduce to Fisher’s linear dis- criminant in the two-class case and that, in some sense, maximise the between-class scatter and minimise the within-class scatter. We present one approach here.  We consider the criterion  JF .a  D aT S B a aT SW a   4.33   where the sample-based estimates of S B and SW are given by  and  S B D CX iD1  ni n  .mi  cid:7  m .mi  cid:7  m T  SW D CX iD1  O cid:8 i  ni n  where mi and O cid:8 i ; i D 1; : : : ; C, are the sample means and covariance matrices of each class  with ni samples  and m is the sample mean. We seek a set of feature vectors ai that i SW a j D Ži j  class-centralised maximise  4.33  subject to the normalisation constraint aT vectors in the transformed space are uncorrelated . This leads to the generalised sym- metric eigenvector equation  Press et al., 1992   S B A D SW A cid:17    cid:7 1 W S B A D A cid:17   S  where A is the matrix whose columns are the ai and  cid:17  is the diagonal matrix of eigenvalues. If S   cid:7 1 W exists, this may be written  The eigenvectors corresponding to the largest of the eigenvalues are used for feature extraction. The rank of S B is at most C  cid:7  1; therefore the projection will be onto a space of dimension at most C  cid:7  1. The solution for A satisfying  4.34  satisfying the constraint also diagonalises the between-class covariance matrix, AT S B A D  cid:17 , the diagonal matrix of eigenvalues.  When the matrix SW is not ill-conditioned with respect to inversion, the eigenvectors of the generalised symmetric eigenvector equation can be determined by solving the equivalent equation   cid:7 1 W S B a D ½a  S   4.34    4.35    4.36    Multiclass algorithms 147   cid:7 1 though note that the matrix S W S B is not symmetric. However, the system may be reduced to a symmetric eigenvector problem using the Cholesky decomposition  Press et al., 1992  of SW , which allows SW to be written as the product SW D LLT , for a lower triangular matrix L. Then  4.36  is equivalent to  cid:7 1 T y D ½y   cid:7 1S B .L  L  If SW is close to singular, then S  where y D LT a. Efﬁcient routines based on the QR algorithm  Stewart, 1973; Press et al., 1992  may be used to solve the above eigenvector equation.   cid:7 1 W S B cannot be computed accurately. One approach is to use the QZ  Stewart, 1973  algorithm, which reduces S B and SW to upper triangular form  with diagonal elements bi and wi respectively  and the eigenvalues are given by the ratios ½i D bi =wi . If SW is singular, the system will have ‘inﬁnite’ eigenvalues, and the ratio cannot be formed. These ‘inﬁnite’ eigenvalues correspond to eigenvectors in the null space of SW . L.-F. Chen et al.  2000  propose using these eigenvectors, ordered according to bi , for the LDA feature space.  There are other approaches. Instead of solving  4.34  or  4.35 , we may determine A by solving two symmetric eigenvector equations successively. The solution is given by  A D U r  cid:17    cid:7  1 2 r V ¹   4.37  where U r D [u1; : : : ; ur ] are the eigenvectors of SW with non-zero eigenvalues ½1; : : : ; ½r ;  cid:17 r D diag.½1; : : : ; ½r   and V ¹ is the matrix of eigenvectors of S0 B D  cid:7  1 r U T 2 and satisﬁes  4.34 .  This is the Karhunen–Lo`eve transformation pro-   cid:7  1 2 r S B U r  cid:17  r   cid:17  posed by Kittler and Young, 1973; see Chapter 9.   Cheng et al.  1992  describe several methods for determining optimal discriminant  transformations when SW is ill-conditioned. These include:  1. The pseudo-inverse method. Replace S   cid:7 1 W by the pseudo-inverse, S  † W  Tian et al.,  1988 .  2. The perturbation method. Stabilise the matrix SW by adding a small perturbation matrix,  cid:21   Hong and Yang, 1991 . This amounts to replacing the singular values of SW , ½r , by a small ﬁxed positive value, Ž, if ½r < Ž.  3. The rank decomposition method. This is a two-stage process, similar to the one given above  4.37 , with successive eigendecompositions of the total scatter matrix and between-class scatter matrix.  Discrimination As in the two-class case, the transformation in itself does not provide us with a discrim- ination rule. The transformation is independent of the distributions of the classes and is deﬁned in terms of matrices S B and SW . However, if we were to assume that the data were normally distributed, with equal covariance matrices  equal to the within-class co- variance matrix, SW   in each class and means mi , then the discrimination rule is: assign x to class !i if gi ½ g j for all j 6D i; j D 1; : : : ; C, where  cid:7 1 W  gi D log. p.!i     cid:7  1  .x  cid:7  mi  T S  .x  cid:7  mi    2   148 Linear discriminant analysis  or, neglecting the quadratic terms in x,  gi D log. p.!i     cid:7  1  2 mT i S   cid:7 1 W mi C xT S   cid:7 1 W mi   4.38   the normal-based linear discriminant function  see Chapter 2 . If A is the linear dis-  cid:7 1 criminant transformation, then S W may be written  see the exercises at the end of the chapter   cid:7 1 W D AAT C A?AT?  S  where AT?m j D 0 for all discriminant function  j. Using the above expression for S   cid:7 1 W in  4.38  gives a  gi D log. p.!i     cid:7  .y.x   cid:7  yi  T .y.x   cid:7  yi     4.39   and ignoring terms that are constant across classes, discrimination is based on  gi D log. p.!i     cid:7  1  i yi C yT .x yi  2 yT  a nearest class mean classiﬁer in the transformed space, where yi D AT mi and y.x  D AT x.  This is simply the Gaussian classiﬁer of Chapter 2 applied in the transformed space.  4.3.4 Least mean squared error procedures  Introduction As in Section 4.2.4, we seek a linear transformation of the data x  or the transformed data φ.x   that we can use to make a decision and which is obtained by minimising a squared error measure. Speciﬁcally, let the data be denoted by the nð p matrix X D [x1j : : :jxn]T and consider the minimisation of the quantity  E D jjW XT C w01T  cid:7  T Tjj2 D nX iD1  .W xi C w0  cid:7  ti  T .W xi C w0  cid:7  t i     4.40   where W is a C ð p matrix of weights, w0 is a C-dimensional vector of biases and 1 is a vector with each component equal to unity. The n ð C matrix of constants T , sometimes termed the target matrix, is deﬁned so that the ith row is  t i D λ j D  1 CA for xi in class ! j  0 B@  ½ j1 ::: ½ jC  that is, t i has the same value for all patterns in the same class. Minimising  4.40  with respect to w0 gives  w0 D t  cid:7  W m   4.41    Multiclass algorithms 149  where  is the mean ‘target’ vector and  t D 1 n  CX jD1  n j λ j  m D 1 n  nX iD1  xi  OX OT  4D X  cid:7  1mT 4D T  cid:7  1t  T  W D OT  T  . OX  T   †  the mean data vector. Substituting for w0 from  4.41  into  4.40  allows us to express the error, E, as  E D jjW OX  T  cid:7  OT  Tjj2   4.42   where OX and OT are deﬁned as   data and target matrices with zero mean rows ; 1 is an n-dimensional vector of 1s. The minimum  Frobenius  norm solution for W that minimises E is  T  where . OX inverse exists; see Appendix C , with matrix of ﬁtted values   † is the Moore–Penrose pseudo-inverse of OX  T   4.43   X† D .XT X  cid:7 1XT if the  QT D OX OX  † OT C 1t  T   4.44   Thus, we can obtain a solution for the weights in terms of the data and the ‘target  matrix’ T , as yet unspeciﬁed.  Properties Before we consider particular forms for T , let us note one or two properties of the least mean squared error approximation. The large sample limit of  4.40  is  E=n  cid:7 ! E1 D CX jD1  p.! j  E[jjW x C w0  cid:7  λ jjj2] j   4.45   where p.! j   is the prior probability  the limit of n j =n  and the expectation, E[:] j , is with respect to the conditional distribution of x on class ! j , i.e. for any function z of x  Z  E[z.x ] j D  z.x  p.xj! j   dx  The solution for W and w0 that minimises  4.45  also minimises  Devijver, 1973; Wee, 1968; see also the exercises at the end of the chapter   E0 D E[jjW x C w0  cid:7  ρ.x jj2]   4.46    150 Linear discriminant analysis  where the expectation is with respect to the unconditional distribution p.x  of x and ρ.x  is deﬁned as  ρ.x  D CX jD1  λ j p.! jjx    4.47   Thus, ρ.x  may be viewed as a ‘conditional target’ vector; it is the expected target vector given a pattern x, with the property that  Z  E[ρ.x ] D  ρ.x  p.x dx D CX jD1  p.! j  λ j  the mean target vector. From  4.45  and  4.46 , the discriminant vector that minimises E1 has minimum variance from the discriminant vector ρ.  Choice of targets The particular interpretation of ρ depends on the choice we make for the target vectors for each class, λ j . If we interpret the prototype target matrix as  ½ ji D loss in deciding !i when the true class is ! j   4.48   then ρ.x  is the conditional risk vector  Devijver, 1973 , where the conditional risk is the expected loss in making a decision, with the ith component of ρ.x  being the conditional risk of deciding in favour of !i . The Bayes decision rule for minimum conditional risk is  assign x to !i if ²i .x   cid:11  ² j .x ;  j D 1; : : : ; C  From  4.45  and  4.46 , the discriminant rule that minimises the mean squared error E has minimum variance from the optimum Bayes discriminant function ρ as the number of samples tends to inﬁnity.  For a coding scheme in which  ½i j D  ² 1 i D j 0 otherwise   4.49   the vector ρ.x  is equal to p.x , the vector of posterior probabilities. The Bayes dis- criminant rule for minimum error is  assign x to !i if ²i .x  ½ ² j .x ;  j D 1; : : : ; C  The change in the direction of the inequality results from the fact that the terms ½i j are viewed as gains. For this coding scheme, the least mean squared error solution for W and w0 gives a vector discriminant function that asymptotically has minimum variance from the vector of a posteriori probabilities, shown for the two-class case in Section 4.2.4.  Figure 4.10 illustrates the least mean square procedure on some one-dimensional data. Data for three classes are positioned on the x-axis and the linear discriminant functions obtained by a least squares procedure are shown. These discriminant functions divide the data space into three regions, according to which linear discriminant function is the largest.   Multiclass algorithms 151  gŽ  gž  gŠ  6  1  0.8  0.6  0.4  0.2  0  cid:7 0.2   cid:7 0.4  Š  0  Š Š  Š Š  ž ž  Š  Š  ž ž ž  ž  ž  žŽ  Ž Ž Ž  Ž  1  2  3  4  5  Figure 4.10 Linear discriminant functions  Decision rule The above asymptotic results suggest that we use the same decision rules to assign a pattern x to a class assuming that the linear transformation W xC w0 had produced ρ.x . For example, with the coding scheme  4.49  for  cid:17  that gives ρ.x  D p.x , we would assign x to the class corresponding to the largest component of the discriminant function W x C w0. Alternatively, in the spirit of the least squares approach, assign x to !i if  jW x C w0  cid:7  λij2 < jW x C w0  cid:7  λ jj2 for all j 6D i   4.50   which leads to the linear discrimination rule: assign x to class !i if  where  i x C d0i > d T d T  j x C d0 j  8 j 6D i  di D λT d0i D  cid:7 jλij2=2 C wT 0 λi  i W  For λi given by  4.49 , this decision rule is identical to the one that treats the linear discriminant function W x C w0 as p.x , but it is not so in general  Lowe and Webb, 1991 .  We add a word of caution here. The result given above is an asymptotic result only. Even if we had a very large number of samples and a ﬂexible set of basis functions  cid:9  .x   replacing the measurements x , then we do not necessarily achieve the Bayes optimal discriminant function. Our approximation may indeed become closer in the least squares sense, but this is weighted in favour of higher-density regions, not necessarily at class boundaries. In Section 4.3.5 we consider minimum-distance rules in the transformed space further. A ﬁnal result, which we shall quote without proof, is that for the 1-from-C coding scheme  4.49 , the values of the vector W x C w0 do indeed sum to unity  Lowe and   152 Linear discriminant analysis  Webb, 1991 . That is, if we denote the linear discriminant vector z as  z D W x C w0  CX iD1  zi D 1  where W and w0 have been determined using the mean squared error procedure  equations  4.41  and  4.43   with the columns of  cid:17  D [λ1; : : : ; λC ] being the columns of the identity matrix, then  that is, the sum of the discriminant function values is unity. This does not mean that the components of z can necessarily be treated as probabilities since some may be negative.  4.3.5 Optimal scaling  Recall that for a C-class problem, the discriminant function approach constructs C dis- criminant functions gi .x  with decision rule: assign x to class !i if  For a linear discriminant function,  gi .x  > g j .x   for all j 6D i  gi .x  D wT  i x C wi0  and, for a least squares approach, we minimise an error of the form  4.40  with respect to the weight vectors and the offsets. This was illustrated in Figure 4.10 for three classes. Three linear discriminant functions are calculated for the data positioned on the x-axis. These linear discriminant functions can be used to partition the data space into three regions, according to which discriminant function is the largest.  An alternative to the zero–one coding for the response variable is to consider an approach where a single linear regression is determined, with different target values for each class, !i . For example, in Figure 4.11, the target for class Š is  cid:7 1, the target for class ž is 0, and the target for class Ž is taken to be +1. The linear regression g.x  is shown. A pattern x0 may be assigned to the class whose label is closest to g.x0 . Thus, for  cid:7  1 In practice, we want an automatic means of assigning values to class labels and, of course, the values  cid:7 1, 0, C1 used above may not be the best ones to choose, in terms of minimising a least squares criterion.  ; x 2 class ž.  < g.x  < 1 2  2  Therefore we seek to minimise the squared error with respect to the class indicator values and the weight vectors wi . Deﬁne a set of scorings or scalings for the C classes by the vector θ 2 R C . Generally we can ﬁnd K < C independent solutions for θ, namely θ 1; : : : ; θ K , for which the squared error is minimised. Let  cid:29  be the C ð K matrix [θ 1; : : : ; θ K ] and  cid:29 Ł D T  cid:29  be the n ð K matrix of transformed values of the classes for each pattern, where we take the n ð C matrix, T , to be the class indicator matrix with Ti j D 1 if xi 2 class ! j and 0 otherwise. Without loss of generality we shall   Multiclass algorithms 153  1.5  1  0.5   cid:7 0.5   cid:7 1   cid:7 1.5  0  Š  0  Š Š  Š Š  ž ž  Š  Š  žž žžž  ž  ž  žŽ  Ž Ž Ž  Ž  5 1 Figure 4.11 LDA regression for targets  cid:7 1 and C1  4  2  3  6  assume that the data have zero mean and X is the n ð p data matrix with ith row xT i . Therefore, we seek to minimise the squared error  E D jj cid:29   Ł  cid:7  XW Tjj2   4.51  with respect to the scores  cid:29  and the K ð p weight matrix W . Compare the above with the standard least squares criterion  4.42 . Note that an offset term 1wT 0 has not been included. The results of this section carry through with a slight modiﬁcation with the inclusion of an offset.  The solution for W minimising E is given by W T D X† cid:29   Ł  where X† is the pseudo-inverse of X, with error, on substituting for W , of  E D Trf.T  cid:29  T .I  cid:7  XX† T  cid:29 g   4.53  Minimising with respect to  cid:29  subject to the constraint  cid:29 T D p cid:29  D I K  where I K is the K ð K identity matrix and D p D diag.n1=n; : : : ; nC =n  , using the method of Lagrange multipliers, leads to the columns of  cid:29  satisfying the general symmetric eigenvector equation   4.52    4.54   T T .XX† T θ D ½D pθ  1 n  Let the eigenvalues corresponding to solutions θ 1; : : : ; θ K be ½1; : : : ; ½K , in decreasing order. The matrix product .XX† T in the above is the matrix of ﬁtted values, QT , in a regression of T on X  see Section 4.3.4 . Thus, the matrix on the left-hand side of  4.54  is simply the product of the targets and ﬁtted values T T QT . This result is used in developments of this procedure to nonlinear discriminant analysis. The contribution to the average squared error of the lth scaling, θl, is given by  l D 1  cid:7  ½l e2   154 Linear discriminant analysis  Relationship to the linear discriminant analysis solution The between-class covariance matrix  assuming mean-centred data  is  S B D CX iD1  ni n  mi mT  i D 1  n2 XT T  ! cid:7 1     T T T  n  T T X  and using the solution  4.52  for W , we may show that the between-class covariance matrix in the transformed space is  W S B W T D diag.½2  ; : : : ; ½2 K  1   ;   4.55   a diagonal matrix. Similarly, we may show that the transformed within-class covariance matrix is given by  W SW W T D D½.1 cid:7 ½   4D diag.½1.1  cid:7  ½1 ; : : : ; ½K .1  cid:7  ½K      4.56   Thus, the optimal scaling solution for W diagonalises both the between-class and within- class covariance matrices. In particular, the linear discriminant analysis transformation that transforms the within-class covariance matrix to the identity matrix and diagonalises the between-class covariance matrix is given by W L D A D D   4.57   1 2 ½.1 cid:7 ½ W  Discrimination The optimal scaling solution may be used in discrimination in the same way as the linear discriminant solution in Section 4.3.3. Using the relationship between the linear discrim- inant solution and the optimal scaling solution  4.57 , together with the LDA rule  4.39 , discrimination is based on the rule: assign x to class !i if gi > g j for all j 6D i, where  4.58  is the transformation of the mean of the ith class and y O S.x  D W x. An where yi alternative form uses the solutions for the scalings and the transformed vector yO S.x  to give a discriminant function  Breiman and Ihaka, 1984; Hastie et al., 1995   cid:7 1 1 cid:7 ½.θ i  cid:7  y O S.x    cid:7  jjθ ijj2   cid:7 1 ½.1 cid:7 ½ .yi  cid:7  y O S.x    gi D log. p.!i     cid:7  1  gi D log. p.!i     cid:7  1  .yi  cid:7  y O S.x  T D  .θ i  cid:7  y O S.x  T D   4.59   2  2  where θ i is the vector of scalings on class !i  the ith column of  cid:29 T  . Equivalently,  gi D log. p.!i     cid:7  1  .θ i  cid:7  y O S.x  T D  2   cid:7 1 e2  .θ i  cid:7  y O S.x    cid:7  jjθ ijj2   4.60    .  ; : : : ; e2 K  where De2 is the diagonal matrix of contributions to the average squared error by the K solutions, diag.e2 1  The main conclusion of this analysis is that a regression approach based on scalings and a linear transformation leads to a discriminant rule identical to that obtained from LDA. The discriminant function depends upon the optimal scalings, which in turn are eigensolutions of a matrix obtained from the ﬁtted values in a linear regression and the targets. The advantage of this approach is that it provides a basis for developing nonlinear discriminant functions.   Multiclass algorithms 155  4.3.6 Regularisation  If the matrix XT X is close to singular, an alternative to the pseudo-inverse approach is to use a regularised estimator. The error, E  4.42 , is modiﬁed by the addition of a regularisation term to give  E D jjW OX  T  cid:7  OT  Tjj2 C ÞjjWjj2  where Þ is a regularisation parameter or ridge parameter. The solution for W that minimises E is  W D OT  T OX. OX OX  T C ÞI p  cid:7 1  We still have to choose the ridge parameter, Þ, which may be different for each output dimension  corresponding to class in a discrimination problem . There are several possible choices  see Brown, 1993 . The procedure of Golub et al.  1979  is to use a cross-validation estimate. The estimate OÞ of Þ is the value that minimises  jj.I  cid:7  A.Þ  Otjj2 [Tr.I  cid:7  A.Þ  ]2  where  A.Þ  D OX.XT X C ÞI   cid:7 1 OX  T  and Ot is one of the columns of OT , i.e. measurements on one of the output variables, that is being predicted.  4.3.7 Multiclass support vector machines  Support vector machines can be applied in multiclass problems either by using the binary classiﬁer in a one-against-all or one-against-one situation, or by constructing C linear discriminant functions simultaneously  Vapnik, 1998 .  Consider the linear discriminant functions gk .x  D .wk  T x C wk  0  k D 1; : : : ; C  We seek a solution for f.wk; wk class !i if  0   ; k D 1; : : : ; Cg such that the decision rule: assign x to gi .x  D max  g j .x   j  separates the training data without error. That is, there are solutions for f.wk ; wk k D 1; : : : ; Cg such that, for all k D 1; : : : ; C,  0   ;  .wk  T x C wk  0  cid:7  ..w j  T x C w j  0    ½ 1  for all x 2 !k and for all j 6D k. This means that every pair of classes is separable. If a solution is possible, we seek a solution for which  CX kD1  .wk  T wk   156 Linear discriminant analysis  is minimal. If the training data cannot be separated, slack variables are introduced and we minimise  L D CX kD1  .wk T wk C C  nX iD1  ¾i  subject to the constraints  .wk  T xi C wk  0  cid:7  ..w j  T xi C w j  0    ½ 1  cid:7  ¾i  for all xi  where xi 2 !k , and for all j 6D k. The procedure for minimising the quantity L subject to inequality constraints is the same as that developed in the two-class case.  4.3.8 Example application study  The problem Face recognition using LDA  L.-F. Chen et al., 2000 .  Summary A technique, based on LDA, that is appropriate for the small-sample problem  when the within-class matrix, SW , is singular , is assessed in terms of error rate and computational requirements.  The data The data comprise 10 different facial images of 128 people  classes . The raw images are 155ð 175 pixels. These images are reduced to 60ð 60 and, after further processing and alignment, further dimension reduction, based on k-means clustering  see Chapter 10 , to 32, 64, 128 and 256 values is performed.  The model The classiﬁer is simple. The basic method is to project the data to a lower dimension and perform classiﬁcation using a nearest-neighbour rule.  Training procedure The projection to the lower dimension is determined using the training data. Problems occur with the standard LDA approach when the p ð p within- class scatter matrix, SW , is singular  of rank s < p . In this case, let Q D [qsC1; : : : ; q p] be the p ð . p  cid:7  s  matrix of eigenvectors of SW with zero eigenvalue; is, that 4D those that map into the null space of SW . The eigenvectors of QS B, deﬁned by QS B QQT S B .QQT  T , are used to form the most discriminant set for LDA.  Results Recognition rates were calculated using a leave-one-out cross-validation strat- egy. Limited results on small sample size data sets show good performance compared to previously published techniques.  4.3.9 Further developments  The standard generalisation of Fisher’s linear discriminant to the multiclass situation chooses as the columns of the feature extraction matrix A, those vectors ai that maximise  aT i S B ai aT i SW ai   4.61    Multiclass algorithms 157  subject to the orthogonality constraint  i SW a j D Ži j aT  i.e. the within-class covariance matrix in the transformed space is the identity matrix. For the two-class case  C D 2 , only one discriminant vector is calculated. This is Fisher’s linear discriminant.  An alternative approach, proposed by Foley and Sammon  1975  for the two-class case, and generalised to the multiclass case by Okada and Tomita  1985 , is to seek the vector ai that maximises  4.61  subject to the constraints  ai a j D Ži j  The ﬁrst vector, a1, is Fisher’s linear discriminant. The second vector, a2, maximises  4.61  subject to being orthogonal to a1  a2a1 D 0 , and so on. A direct analytic solution for the problem is given by Duchene and Leclercq  1988 , and involves determining an eigenvector of a non-symmetric matrix. Okada and Tomita  1985  propose an iterative procedure for determining the eigenvectors. The transformation derived is not limited by the number of classes.  A development of this approach that uses error probability to select the vectors ai is described by Hamamoto et al.  1991 , and a comparison with the Fisher criterion on the basis of the discriminant score J is given by Hamamoto et al.  1993 .  Another extension of LDA is to a transformation that is not limited by the number of classes. The set of discriminant vectors is augmented by an orthogonal set that max- imises the projected variance. Thus the linear discriminant transformation is composed of two parts: a set of vectors determined, say, by the usual multiclass approach and a set orthogonal to the ﬁrst set for which the projected variance is maximised. This combines linear discriminant analysis with a principal components analysis  Duchene and Leclercq, 1988 .  There have been many developments of Fisher’s linear discriminant both in the two- class case  for example, Aladjem, 1991  and in the multiclass situation  for example, Aladjem and Dinstein, 1992; Liu et al. 1993 . The aims have been either to determine transformations that aid in exploratory data analysis or interactive pattern recognition or that may be used prior to a linear or quadratic discriminant rule  Schott, 1993 . Several methods have been proposed for a small number of samples  Cheng et al., 1992; Hong and Yang, 1991; Tian et al., 1988  when the matrix SW may be singular. These are compared by Liu et al.  1992 .  A development of the least mean squared error approach which weights the error in favour of patterns near the class boundary has been proposed by Al-Alaoui  1977 . Al-Alaoui describes a procedure, starting with the generalised inverse solution, that pro- gressively replicates misclassiﬁed samples in the training set. The linear discriminant is iteratively updated and the procedure is shown to converge for the two-class separa- ble case. Repeating the misclassiﬁed samples in the data set is equivalent to increasing the cost of misclassiﬁcation of the misclassiﬁed samples or, alternatively, weighting the distribution of the training data in favour of samples near the class boundaries  see the description of boosting in Chapter 8 .  The major developments of support vector machines presented in this chapter are to  nonlinear models, to be discussed in Chapter 5.   158 Linear discriminant analysis  4.3.10 Summary  In Chapter 2, we developed a multiclass linear discriminant rule based on normality assumptions for the class-conditional densities  with common covariance matrices . In this section, we have presented a different approach. We have started with the requirement of a linear discriminant rule, and sought ways of determining the weight vectors for each class. Several approaches have been considered, including perceptron schemes, Fisher’s discriminant rule, least mean square procedures and support vector machines.  The least mean squared error approach has the following properties:  1. Asymptotically, the procedure produces discriminant functions that provide a mini-  mum squared error approximation to the Bayes discriminant function.  2. By optimising with respect to a set of scores for each class, in addition to the trans- formation weights, a solution related to the linear discriminant transformation can be obtained and can be more accurate than a straight least squares solution  Hastie et al., 1994 .  3. For a ‘1-from-C’ target coding  Ti j D 0 if xi 2 ! j and 0 otherwise , and minimising  a squared error, the discriminant functions values sum to unity for any pattern x.  One of the problems of the least mean squared error approach is that it places emphasis  on regions of high density, which are not necessarily at class boundaries.  In many practical studies, when a linear discriminant rule is used, it is often the normal-based linear discriminant rule of Chapter 2. However, the least mean square rule, with binary-coded target vectors, is important since it forms a part of some nonlinear regression models. Also, there is a natural development of support vector machines to classiﬁcation problems with nonlinear decision boundaries.  4.4 Logistic discrimination  In the previous sections, discrimination is performed using values of a linear function of the data sample x  or the transformed data samples φ.x  . We continue this theme here. We shall introduce logistic discrimination for the two-group case ﬁrst, and then consider the multigroup situation.  4.4.1 Two-group case  The basic assumption is that the difference between the logarithms of the class-conditional density functions is linear in the variables x:  cid:10   cid:9  p.xj!1  p.xj!2   D þ0 C β T x   4.62   log  This model is an exact description in a wide variety of situations including  Anderson, 1982 :   Logistic discrimination 159  matrices;  terms between groups;  describe each sample.  1. when the class-conditional densities are multivariate normal with equal covariance  2. multivariate discrete distributions following a loglinear model with equal interaction  3. when situations 1 and 2 are combined: both continuous and categorical variables  Hence, the assumption is satisﬁed by many families of distributions and has been found to be applicable to a wide range of real data sets that depart from normality.  It is a simple matter to show from  4.62  that the assumption is equivalent to  1 p.!2jx  D 1 C exp.þ0 0 C β T x  0 C β T x  p.!1jx  D exp.þ0 1 C exp.þ0 0 C β T x    4.63   where þ0  0 D þ0 C log. p.!1 = p.!2  .  Discrimination between two classes depends on the ratio p.!1jx = p.!2jx ,  assign x to  if  ² !1 !2  p.!1jx  p.!2jx   ² > < 1  and substituting the expressions  4.63 , we see that the decision about discrimination is determined solely by the linear function þ0 ² !1 !2  0 C þ T x and is given by if þ0  ² > < 0  0 C þ T x  assign x to  This is an identical rule to that given in Section 4.2 on linear discrimination, and we gave several procedures for estimating the parameters. The only difference here is that we are assuming a speciﬁc model for the ratio of the class-conditional densities that leads to this discrimination rule, rather than specifying the rule a priori. Another difference is that we may use the models for the densities  4.63  to obtain maximum likelihood estimates for the parameters.  4.4.2 Maximum likelihood estimation  The parameters of the logistic discrimination model may be estimated using a maximum likelihood approach  Anderson, 1982; Day and Kerridge, 1967 . An iterative nonlinear optimisation scheme may be employed using the likelihood function and its derivatives. The estimation procedure depends on the sampling scheme used to generate the la- belled training data  Anderson, 1982; McLachlan, 1992a , and three common sampling designs are considered by Anderson. These are:  i  sampling from the mixture distribu- tion;  ii  sampling conditional on x in which x is ﬁxed and one or more samples are taken  which may belong to !1 or !2 ; and  iii  separate sampling for each class in   160 Linear discriminant analysis  which the conditional distributions, p.xj!i  ; i D 1; 2; are sampled. Maximum likelihood estimates of β are independent of the sampling scheme, though one of the sampling designs considered  separate sampling from each group  derives estimates for þ0 rather than þ0 0  which is the term required for discrimination . We assume in our derivation below a mixture sampling scheme, which arises when a random sample is drawn from a mixture of the groups. Each of the sampling schemes above is discussed in detail by McLachlan  1992a .  The likelihood of the observations is  p.x1rj!1   p.x2rj!2   r D 1; : : : ; ns; s D 1; 2:  n2Y rD1  where xsr .s D 1; 2; r D 1; : : : ; ns   are the observations in class !s. This may be rewritten as  p.!1jx1r    1  p.!1 n1 p.!2 n2  p.x1r   p.!1  Y  all x  n2Y rD1  p.x   p.!2jx2r   n1Y rD1  p.x2r   p.!2  n2Y p.!1jx1r   rD1  p.!2jx2r    L D n1Y rD1  L D n1Y rD1  D  The factor  1  p.!1 n1 p.!2 n2  Y  all x  p.x   is independent of the parameters of the model – the assumption in Anderson  1982  and Day and Kerridge  1967  is that we are free to choose p.x ; the only assumption we have made is on the log-likelihood ratio. Therefore, maximising the likelihood L is equivalent to maximising  or  p.!2jx2r    n2Y rD1  p.!1jx1r    L0 D n1Y rD1 log. p.!1jx1r    C n2X rD1  log.L0  D n1X rD1  log. p.!2jx2r     and using the functional forms  4.63   log.L0  D n1X rD1  0 C β T x1r    cid:7  X .þ0  all x  logf1 C exp.þ0  0 C β T x g  The gradient of log.L0  with respect to the parameters þ j is  @logL0 @þ0 0 @logL0 @þ j  D n1  cid:7  X D n1X rD1  p.!1jx  .x1r   j  cid:7  X  all x  all x  p.!1jx x j ;  j D 1; : : : ; p   Logistic discrimination 161  0  Having written down an expression for the likelihood and its derivative, we may now use a nonlinear optimisation procedure to obtain a set of parameter values for which the function log.L0  attains a local maximum. First of all, we need to specify initial starting values for the parameters. Anderson recommends taking zero as a starting value for all pC 1 parameters, þ0 ; þ1; : : : ; þ p. Except in two special cases  see below , the likelihood has a unique maximum attained for ﬁnite β  Albert and Lesaffre, 1986; Anderson, 1982 . Hence, the starting point is in fact immaterial. If the two classes are separable, then there are non-unique maxima at inﬁnity. At each 0C β T x gives complete stage of the optimisation procedure, it is easy to check whether þ0 separation. If it does, then the algorithm may be terminated. The second situation when L does not have a unique maximum at a ﬁnite value of β occurs with discrete data when the proportions for one of the variables are zero for one of the values. In this case, the maximum value of L is at inﬁnity. Anderson  1974  suggests a procedure for overcoming this difﬁculty, based on the assumption that the variable is conditionally independent of the remaining variables in each group.  4.4.3 Multiclass logistic discrimination  In the multiclass discrimination problem, the basic assumption is that, for C classes,   cid:10    cid:9  p.xj!s   p.xj!C    log  D þs0 C β T  s x;  s D 1; : : : ; C  cid:7  1  that is, the log-likelihood ratio is linear for any pair of likelihoods. Again, we may show that the posterior probabilities are of the form s0 C β T exp.þ0 1 C PC cid:7 1 sD1 exp.þ0 1 C PC cid:7 1 sD1 exp.þ0  s x  s0 C β T s0 C β T  s D 1; : : : ; C  cid:7  1  p.!Cjx  D  p.!sjx  D  s x   s x   1  ;  where þ0 depends solely on the linear functions þ0  s0 D þs0 C log. p.!s  = p.!C   . Also, the decision rule about discrimination j x and the rule is: assign x to class ! j if  s0 C β T j0 C β T  j x > 0;  s D 1; : : : ; C  cid:7  1  maxfþ0  s0 C β T otherwise assign x to class !C .  s xg D þ0  The likelihood of the observations is given by  using the notation given previously. As in the two-class case, maximising L is equivalent to maximising  L D CY iD1  niY rD1  p.xirj!i    log.L0  D CX sD1  nsX rD1  log. p.!sjxsr      4.64    4.65    162 Linear discriminant analysis  with derivatives  @log.L0  @þ0 j0 @log.L0  @.þ j  l  D n j  cid:7  X D n jX rD1  p.! jjx  .x jr  l  cid:7  X  all x  all x  p.! jjx xl  Again, for separable classes, the maximum of the likelihood is achieved at a point at inﬁnity in the parameter space, but the algorithm may be terminated when complete separation occurs. Also, zero marginal sample proportions cause maxima at inﬁnity and the procedure of Anderson may be employed.  4.4.4 Example application study  The problem To predict in the early stages of pregnancy the feeding method  bottle or breast  a woman would use after giving birth  Cox and Pearce, 1997 .  Summary A ‘robust’ two-group logistic discriminant rule was developed and compared with the ordinary logistic discriminant. Both methods gave similar  good  performance.  The data Data were collected on 1200 pregnant women from two district general hos- pitals. Eight variables were identiﬁed as being important to the feeding method: presence of children under 16 years of age in the household, housing tenure, lessons at school on feeding babies, feeding intention, frequency of seeing own mother, feeding advice from relatives, how the woman was fed, previous experience of breast feeding.  Some patterns were excluded from the analysis for various reasons: incomplete in- formation, miscarriage, termination, refusal and delivery elsewhere. This left 937 cases for parameter estimation.  The model Two models were assessed: the two-group ordinary logistic discrimination model  4.62  and a robust logistic discrimination model, designed to reduce the effect of outliers on the discriminant rule, p.xj!1  p.xj!2   D c1 C c2 exp[þ0 C β T x] 1 C exp[þ0 C β T x]  where c1 and c2 are ﬁxed positive constants.  Training procedure The prior probabilities p.!1  and p.!2  are estimated from the data and c1 and c2 speciﬁed. This is required for the robust model, although it can be incorporated with þ0 in the standard model  see Section 4.4.1 . Two sets of experiments were performed, both determining maximum likelihood estimates for the parameters: training on the full 937 cases and testing on the same data; training on 424 cases from one hospital and testing on 513 cases from the second hospital.  Results Both model gave similar performance, classifying around 85% cases correctly.   Application studies 163  4.4.5 Further developments  Further developments of the basic logistic discriminant model have been to robust pro- cedures  Cox and Ferry, 1991; Cox and Pearce, 1997  and to more general models. Several other discrimination methods are based on models of discriminant functions that are nonlinear functions of linear projections. These include the multilayer perceptron and projection pursuit discrimination  Chapter 6 , in which the linear projection and the form of the nonlinear function are simultaneously determined.  4.4.6 Summary  Logistic discrimination makes assumptions about the log-likelihood ratios of one popu- lation relative to a reference population. As with the methods discussed in the previous sections, discrimination is made by considering a set of values formed from linear trans- formations of the explanatory variables. These linear transformations are determined by a maximum likelihood procedure. It is a technique which lies between the linear techniques of Chapters 1 and 4 and the nonlinear methods of Chapters 5 and 6 in that it requires a nonlinear optimisation scheme to estimate the parameters  and the posterior probabilities are nonlinear functions of the explanatory variables , but discrimination is made using a linear transformation.  One of the advantages of the maximum likelihood approach is that asymptotic results regarding the properties of the estimators may readily be derived. Logistic discrimination has further advantages, as itemised by Anderson  1982 :  1. It is appropriate for both continuous and discrete-valued variables.  2. It is easy to use.  3. It is applicable over a wide range of distributions.  4. It has a relatively small number of parameters  unlike some of the nonlinear models  discussed in Chapters 5 and 6 .  4.5 Application studies  There have been several studies comparing logistic discrimination with linear discrimi- nant analysis  for example, Bull and Donner, 1987; Press and Wilson, 1978 . Logistic discrimination has been found to work well in practice, particularly for data that depart signiﬁcantly from normality – something that occurs often in practice. The Statlog project  Michie et al., 1994  compared a range of classiﬁcation methods on various data sets. It reports that there is little practical difference between linear and logistic discrimination. Both methods were in the top ﬁve algorithms.  There are many applications of support vector machines, primarily concerned with the nonlinear variant, to be reported in Chapter 5. However, in a communications example, support vector machines have been used successfully to implement a decision feedback equaliser  to combat distortion and interference , giving superior performance to the conventional minimum mean squared error approach  S. Chen et al., 2000 .   164 Linear discriminant analysis  4.6 Summary and discussion  The discriminant functions discussed in the previous section are all linear in the compo- nents of x or the transformed variables  cid:9 i .x   generalised linear discriminant functions . We have described several approaches for determining the parameters of the model  error- correction schemes, least squares optimisation, logistic model , but we have regarded the initial transformation, φ, as being prescribed. Some possible choices for the functions  cid:9 i .x  were given in Chapter 1 and there are many other parametric and nonparametric forms that may be used, as we shall see in later chapters. But how should we choose the functions  cid:9 i ? A good choice for the  cid:9 i will lead to better classiﬁcation performance for a subsequent linear classiﬁer than simply applying a linear classiﬁer to the variables xi . On the other hand, if we were to use a complex nonlinear classiﬁer after the initial transformation of the variables, then the choice for the  cid:9 i may not be too critical. Any inadequacies in a poor choice may be compensated for by the subsequent classiﬁcation process. However, in general, if we have knowledge about which variables or transfor- mations are useful for discrimination, then we should use it, rather than hope that our classiﬁer will ‘learn’ the important relationships.  If we could choose the  cid:9 i so that the conditional risk vector ρ.x   4.47  has compo-  nents ²i .x  of the form  ²i .x  D DX jD1  ai j  cid:9  j .x  C ai0  for weights ai j , then a resulting linear classiﬁer would be asymptotically optimum  De- vijver, 1973 . Unfortunately, this does not give us a prescription for the  cid:9 i .x . Many sets of basis functions have been proposed, and the more basis functions we use in our classiﬁer, the better we might expect our classiﬁer to perform. This is not necessarily the case, since increasing the dimension of the vector φ, by the inclusion of more basis functions, leads to more parameters to estimate in the subsequent classiﬁcation stage. Although error rate on the training set may in fact decrease, the true error rate may increase as generalisation performance deteriorates.  For polynomial basis functions, the number of terms increases rapidly with the order of the polynomial, restricting such an approach to polynomials of low order. However, an important recent development is that of support vector machines, replacing the need to calculate D-dimensional feature vectors φ.x  with the evaluation of a kernel K .x; y  at points x and y in the training set.  In the following chapter, we turn to discriminant functions that are linear combinations of nonlinear functions φ  i.e. generalised linear discriminant functions . The radial basis function network deﬁnes the nonlinear function explicitly. It is usually of a prescribed form with parameters set as part of the optimisation process. They are usually determined through a separate procedure and the linear parameters are obtained using the procedures of this chapter. The support vector machine deﬁnes the nonlinear function implicitly, through the speciﬁcation of a kernel function.   Recommendations 165  4.7 Recommendations  1. Linear schemes provide a baseline from which more sophisticated methods may be judged. They are easy to implement and should be considered before more complex methods.  2. The error-correction scheme, or a support vector machine, can be used to test for sep- arability. This might be important for high-dimensional data sets where classes may be separable due to the ﬁnite training set size. A classiﬁer that achieves linear sepa- rability on a training set does not necessarily mean good generalisation performance, but it may indicate insufﬁcient data to characterise distributions.  3. A regression on binary variables provides a least squares approach to the Bayes optimal discriminant. However, there is evidence to show that optimal scaling  linear discriminant analysis in the space of ﬁtted values  provides a better classiﬁer. The latter method  equivalent to the multiclass extension of Fisher’s linear discriminant  is recommended.  4.8 Notes and references  The theory of algorithms for linear discrimination is well developed. The books by Nilsson  1965  and Duda et al.  2001  provide descriptions of the most commonly used algorithms  see also Ho and Agrawala, 1968; Kashyap, 1970 . A more recent treatment of the perceptron can be found in the book by Minsky and Papert  1988 .  Logistic discrimination is described in the survey article by Anderson  1982  and the  book by McLachlan  1992a .  The development of optimal scaling and the relationship to linear discriminant analysis is described by Breiman and Ihaka  1984 ; the results in this chapter follow more closely the approach of Hastie et al.  1994 .  Support vector machines were introduced by Vapnik and co-workers. The book by Vapnik  1998  provides a very good description with historical perspective. Cristianini and Shawe-Taylor  2000  present an introduction to support vector machines aimed at students and practitioners. Burges  1998  provides a very good tutorial on support vector machines for pattern recognition.  The website www.statistical-pattern-recognition.net contains refer-  ences and links to further information on techniques and applications.  Exercises  1. Linear programming or linear optimisation techniques are procedures for maximising linear functions subject to equality and inequality constraints. Speciﬁcally, we ﬁnd the vector x such that  z D a T 0 x   166 Linear discriminant analysis  is minimised subject to the constraints xi ½ 0  i D 1; : : : ; n   4.66   and the additional constraints  i D 1; : : : ; m1 j D m1 C 1; : : : ; m1 C m2 k D m1 C m2 C 1; : : : ; m1 C m2 C m3  i x cid:11  bi aT j x½ b j ½ 0 aT k xD bk ½ 0 aT for given m1; m2 and m3. Consider optimising the perceptron criterion function as a problem in linear pro- gramming. The perceptron criterion function, with a positive margin vector b, is given by  JP D X yi2Y  .bi  cid:7  vT yi    where now yi are the vectors satisfying vT yi  cid:11  bi . A margin is introduced to prevent the trivial solution v D 0. This can be reformulated as a linear programming problem as follows. We introduce the artiﬁcial variables ai and consider the problem of minimising  Z D nX iD1  ai  ai ½ 0 ai ½ bi  cid:7  vT yi   cid:10    cid:9  vC v cid:7   subject to  Show that minimising Z with respect to a and v will minimise the perceptron criterion.  2. Standard linear programming  see Exercise 1  requires all variables to be positive and the minimum of JP will lead to a solution for v that may have negative components. Convert the . p C 1 -dimensional vector v to a 2. p C 1 -dimensional vector  where vC is the vector v with negative components set to zero, and v cid:7  is the vector v with positive components set to zero and negative components multiplied by  cid:7 1. For example, the vector .1; cid:7 2; cid:7 3; 4; 5 T is written as .1; 0; 0; 4; 5; 0; 2; 3; 0; 0 T . State the error-correction procedure as an exercise in linear programming.  3. Evaluate the constant of proportionality, Þ, in  4.12 , and hence show that the offset on the right-hand side of  4.12  does not depend on the choice for t1 and t2 and is given by  p2  cid:7  p1   cid:9  1 C p1 p2d2   cid:10   2  p1 p2   Exercises 167  where pi D ni =n and d2 is given by  d2 D .m1  cid:7  m2 T S   cid:7 1 W  .m1  cid:7  m2 T  the Mahalanobis distance between two normal distributions of equal covariance ma- trices  see Appendix A . Compare this with the optimal value for normal distribu- tions, given by  4.6 .  4. Show that the maximisation of  TrfS   cid:7 1 W S Bg  in the transformed space leads to the same feature space as the linear discriminant solution.  5. Show that the criterion  J4 D TrfAT S B Ag TrfAT SW Ag  is invariant to an orthogonal transformation of the matrix A.  6. Consider the squared error, CX jD1  where E[:] j writing  p.! j  E[jjW x C w0  cid:7  λ jjj2] j  is with respect to the conditional distribution of x on class ! j . By  E[jjW x C w0  cid:7  λ jjj2] j D E[jj.W x C w0  cid:7  ρ.x   C .ρ.x   cid:7  λ j  jj2] j  and expanding, show that the linear discriminant rule also minimises  E0 D E[jjW x C w0  cid:7  ρ.x jj2]  where the expectation is with respect to the unconditional distribution p.x  of x and ρ.x  is deﬁned as  ρ.x  D CX jD1  λ j p.! jjx    cid:7 1 W D AAT C A?AT?  S  7. Show that if A is the linear discriminant transformation  transforming the within- class covariance matrix to the identity and diagonalising the between-class covariance matrix , then the inverse of the within-class covariance matrix may be written  where AT?m j D 0 for all j.  8. Why is the normalisation criterion  cid:29 T D p cid:29  D I K a convenient mathematical choice in optimal scaling? If an offset term 1T w0 is introduced in  4.51 , what would be the appropriate normalisation constraint?   168 Linear discriminant analysis  9. Using the deﬁnition of m j ,  m j D 1 n j  [XT T ] j  the solution for W  4.52  and the eigenvector equation  4.54 , show that  yT  j D   cid:7 1 ½.1 cid:7 ½ y j D .θ j  T D   cid:7 1 1 cid:7 ½θ j  cid:7  jθ jj2  where θ j are the scorings on class ! j . Hence derive the discriminant function  4.60  from  4.59 .  10. Verify that the optimal scaling solution for W diagonalises the within- and between-  class covariance matrices  equations  4.55  and  4.56  .  11. Show that the outputs of a linear discriminant function, trained using a least squares  approach with 0–1 targets, sum to unity.  12. For normally distributed classes with equal covariance matrix, Fisher’s linear dis- criminant, with a suitable choice of threshold, provides an optimal discriminant in the sense of obtaining the Bayes decision boundary. Is the converse true? That is, if Fisher’s discriminant is identical to the Bayes decision boundary, are the classes normally distributed? Justify your answer.  13. Generate data from three bivariate normal distributions, with means . cid:7 4; cid:7 4 , .0; 0 , .4; 4  and identity covariance matrices; 300 samples in train and test sets; equal pri- ors. Train a least squares classiﬁer  Section 4.3.4  and a linear discriminant function  Section 4.3.3  classiﬁer. For each classiﬁer, obtain the classiﬁcation error and plot the data and the decision boundaries. What do you conclude from the results?  14. Derive the dual form of the Lagrangian for support vector machines applied to the  multiclass case.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   5  Nonlinear discriminant  analysis – kernel methods  Overview  Developed primarily in the neural networks and machine learning literature, the radial basis function  RBF  network and the support vector machine  SVM  are ﬂexible models for nonlinear discriminant analysis that give good performance on a wide range of problems. RBFs are sums of radially symmetric functions; SVMs deﬁne the basis functions implicitly through the speciﬁcation of a kernel.  5.1 Introduction  In the previous chapter, classiﬁcation of an object is achieved by a linear transformation whose parameters were determined as the result of some optimisation procedure. The linear transformation may be applied to the observed data, or some prescribed features of that data. Various optimisation schemes for the parameters were considered, including simple error-correction schemes  as in the case of the perceptron  and least squares error minimisation; in the logistic discrimination model, the parameters were obtained through a maximum likelihood approach using a nonlinear optimisation procedure.  In this chapter and the following one, we generalise the discriminant model still further by assuming parametric forms for the discriminant functions  cid:1 . Speciﬁcally, we assume a discriminant function of the form  g j .x  D mX iD1  w ji  cid:1 i .x; µi   C w j0;  j D 1; : : : ; C   5.1   where there are m ‘basis’ functions,  cid:1 i , each of which has nm parameters µi D f¼ik ; k D 1; : : : ; nmg  the number of parameters may differ between the  cid:1 i , but here we shall assume an equal number , and use the discriminant rule:  assign x to class !i if gi .x  D max  g j .x   j   170 Nonlinear discriminant analysis – kernel methods  that is, x is assigned to the class whose discriminant function is the largest. In  5.1  the parameters of the model are the values w ji and ¼ik and the number of basis functions, m. Equation  5.1  is exactly of the form of a generalised linear discriminant function, but we allow some ﬂexibility in the nonlinear functions  cid:1 i . There are several special cases of  5.1 ; these include  cid:1 i .x; µi    cid:5  .x i linear discriminant function m D p, dimension of x.  cid:1 i .x; µi    cid:5   cid:1 i .x  generalised linear discriminant function with ﬁxed  transformation; it can take any of the forms in Chapter 1, for example.  Equation  5.1  may be written  g.x  D W φ.x  C w0   5.2  where W is the Cðm matrix with .i; j  th component wi j , φ.x  is the m-dimensional vec- tor with ith component  cid:1 i .x; µi   and w0 is the vector .w10; : : : ; wC0 T . Equation  5.2  may be regarded as providing a transformation of a data sample x 2 R p to R C through an intermediate space R m deﬁned by the nonlinear functions  cid:1 i . This is a model of the feed-forward type. As we shall discuss later, models of this form have been widely used for functional approximation and  as with the linear and logistic models , they are not conﬁned to problems in discrimination.  There are two problems to solve with the model  5.2 . The ﬁrst is to determine the complexity of the model or the model order. How many functions  cid:1 i do we use  what is the value of m ? How complex should each function be  how many parameters do we allow ? The answers to these questions are data-dependent. There is an interplay between model order, training set size and the dimensionality of the data. Unfortunately there is no simple equation relating the three quantities – it is very much dependent on the data distribution. The problem of model order selection is non-trivial and is very much an active area of current research  see Chapter 11 . The second problem is to determine the remaining parameters of the model  W and the ¼i  , for a given model order. This is simpler and will involve some nonlinear optimisation procedure for minimising a cost function. We shall discuss several of the most commonly used forms.  In this chapter we introduce models that have been developed primarily in the neural network and machine learning literatures. The types of neural network model that we consider in this chapter are of the feed-forward type, and there is a very strong overlap between these models and those developed in the statistical literature, particularly kernel discrimination, logistic regression and projection pursuit  discussed in the next chapter . The radial basis function network and the multilayer perceptron  the latter also described in the next chapter  may be thought of as a natural progression of the generalised linear discriminant models described in the previous chapter.  Therefore, for the purpose of this chapter and the next, we consider neural network models to provide models of discriminant functions that are linear combinations of simple basis functions, usually of the same parametric form. The parameters of the basis func- tions, as well as the linear weights, are determined by a training procedure. Other models that we have described in earlier chapters could be termed neural network models  for example, linear discriminant analysis . Also, the development of classiﬁcation and regres- sion models in the neural network literature is no longer conﬁned to such simple models.   Optimisation criteria 171  The issues in neural network development are the common ones of pattern recognition: model speciﬁcation, training and model selection for good generalisation performance. Section 5.2 discusses optimisation criteria for models of the form  5.2  above. Sections 5.3 and 5.4 then introduce two popular models, namely the radial basis function network and the support vector machine.  5.2 Optimisation criteria  All the optimisation criteria assume that we have a set of data samples f.xi ; ti  ; i D 1; : : : ; ng that we use to ‘train’ the model. In a regression problem, the xi are measure- ments on the regressors and t i are measurements on the dependent or response variables. In a classiﬁcation problem, ti are the class labels. In both cases, we wish to obtain an estimate of t given x, a measurement. In the neural network literature, t i are referred to as targets, the desired response of the model for measurements or inputs, xi ; the actual responses of the model, g.xi  , are referred to as outputs.  5.2.1 Least squares error measure  As in the linear case, we seek to minimise a squared error measure,  jt i  cid:9  g.xi  j2  E D nX iD1 D jj  cid:9  T T C W  cid:9 T C w01Tjj2   5.3   with respect to the parameters wi j and ¼ jk, where T D [t 1; : : : ; t n]T is an n ð C target matrix whose ith row is the target for input xi ;  cid:9  D [φ.x1 ; : : : ; φ.xn ]T is an n ð m matrix whose ith row is the set of basis function values evaluated at xi ; jjAjj2 D TrfAATg D P  i j ; and 1 is a n ð 1 vector of 1s.  i j A2  Properties In a classiﬁcation problem in which the target for pattern x p 2 ! j is the vector of 1. losses λ j with components ½ ji deﬁned by the loss in deciding !i when the true class is ! j , the solution for g that minimises  5.3  has minimum variance from the conditional risk vector, ρ. In particular, for the 1-from-C coding  ½ ji D 1 for i D j; ½ ji D 0 for i 6D j , the vector g is a minimum square approximation to the vector of a posteriori probabilities asymptotically  see Chapter 4 . This does not mean that the approximation g possesses the properties of the true a posteriori probability distribution  that its elements are positive and sum to unity  but it is an approximation to it  and the one with minimum variance from it .  2. There are two sets of parameters to determine – the linear weights W and the parameters of the nonlinear functions  cid:1 , namely f¼g. For a given set of values of f¼g,   172 Nonlinear discriminant analysis – kernel methods  the solution for W with minimum norm that minimises  5.3  is  W D OT  T  . O cid:9   T   †   5.4  where † denotes the pseudo-inverse of a matrix  see Appendix C . The matrices OT and O cid:9  are zero-mean matrices   OT  where  T 1 D 0; O cid:9  OT O cid:9   T 1 D 0  deﬁned as 4D T  cid:9  1t 4D  cid:9   cid:9  1φ  T  T  t D 1 n φ D 1 n  nX iD1 nX iD1  t i D 1 n  T T 1  φ.xi   D 1 n   cid:9 T 1  are the mean values of the targets and the basis function outputs respectively. The solution for w0 is  w0 D t  cid:9  W φ  Therefore, we may solve for the weights W using a linear method such as a singular value decomposition. However, we must use a nonlinear optimisation scheme to determine the parameters f¼g.  3. In a classiﬁcation problem, in which we have the 1-from-C coding for the class labels  that is, if xi 2 ! j then we have the target ti D .0; 0; : : : ; 0; 1; 0; : : : ; 0 T , where the 1 is in the jth position , then using the pseudo-inverse solution  5.4  for the ﬁnal layer weights gives the property that the components of g sum to unity for any data sample x, i.e.  where  g D OT  T  . O cid:9   T   † cid:9 T C w0  The values of g are not constrained to be positive. One way to ensure positivity is to j exp. cid:9 g j  . This forms the basis  transform the values gi by replacing gi by exp. cid:9 gi  =P  of the generalised logistic model  sometimes called softmax .  If the parameters fW ; w0g are chosen to minimise the least squares error, then it may 4. be shown that the parameters f¼g that minimise the least mean squared error maximise the feature extraction criterion  Lowe and Webb, 1991    5.5    5.6   X  i  gi D 1  TrfS B S  †  Tg   Optimisation criteria 173  where S B and ST are deﬁned as  T O cid:9   S B  ST  4D 1 O cid:9  T OT OT n2 4D 1 T O cid:9  O cid:9  n  The matrices ST and S B have the interpretation of being the total and between-class covariance matrices of the patterns xi in the space spanned by the outputs of the non- linear transformations,  cid:1 i . Precise interpretations depend on the speciﬁc target coding schemes. Thus, the optimal method of solution of such a classiﬁer is to ﬁnd a nonlinear transformation into the space spanned by the nonlinear functions  cid:1 i such that the patterns in different classes are somehow maximally separated  this information is contained in the between-class covariance matrix , while still maintaining an overall total normalisa- tion  through the total covariance matrix . The form of the criterion  5.6  is independent of the transformation from the data space to the space of hidden unit outputs; i.e., it is not dependent on the form of the nonlinear functions  cid:1 , but is a property of the least mean square solution for the ﬁnal layer. The expression  5.6  is also related to optimisation criteria used in clustering  see Chapter 10 .  Incorporating priors and costs A more general form of  5.3  is a weighted error function  dijt i  cid:9  g.xi  j2  E D nX iD1 D jj. cid:9 T T C W  cid:9 T C w01T  Djj2   5.7   where the ith pattern is weighted by the real factor di , and D is diagonal with Dii D p  di . Three different codings for the target matrix T and the weighting matrix D are described.  1. Cost-weighted target coding In a classiﬁcation problem, with C classes, choose a uniform weighting for each pattern in the training set  dk D 1; k D 1; : : : ; n  and employ a target coding scheme which, for a pattern in class ! j , takes as the target vector the vector of losses λ j , whose ith component is the cost of assigning to class !i a pattern that belongs to class ! j . The optimal discriminant vector is  ρ.x  D CX jD1  λ j p.! jjx   which is the Bayes conditional risk vector  see Chapter 4 .  If we make a decision by assigning a pattern to the class for which the classiﬁer outputs are closest to the targets  minimum distance rule  we have: assign x to class !i if  i λi  cid:9  2oT λi  cid:14  λT λT  j λ j  cid:9  2oT λ j ;  j D 1; : : : ; C  where o is the output vector  4.50 . Generally, this is not the same as the rule in which x is assigned to the class corresponding to the smallest value of o  treating o as an approximation to ρ and making a minimum-cost decision .   174 Nonlinear discriminant analysis – kernel methods  2. Prior-weighted patterns In this case, each pattern in the training set is weighted according to the a priori probabilities of class membership and the number in that class as  di D Pk nk =n  for pattern i in class !k  where Pk is the assumed known class probability  derived from knowledge regarding the relative expected class importance, or frequency of occurrence in operation  and nk is the number of patterns in class !k in the training set.  The weighting above would be used in situations where the expected test conditions differ from the conditions described by the training data by the expected proportions in the classes. This may be a result of population drift  see Chapter 1 , or when there are limited data available for training. The above weighting has been used by Munro et al.  1996  for learning low-probability events in order to reduce the size of the training set.  3. Cluster-weighted patterns The computation time for many neural network training schemes increases with the training set size. Clustering  see Chapter 10  is one means of ﬁnding a reduced set of prototypes that characterises the training data set. There are many ways in which clustering may be used to preprocess the data – it could be applied to classes separately, or to the whole training set. For example, when applied to each class separately, the patterns for that class are replaced by the cluster means, with di for the new patterns set proportional to the number in the cluster. When applied to the whole data set, if a cluster contains all members of the same class, those patterns are replaced in the data set by the cluster mean and di is set to the number of patterns in the cluster. If a cluster contains members of different classes, all patterns are retained.  Regularisation Too many parameters in a model may lead to over-ﬁtting of the data by the model and poor generalisation performance  see Chapter 1 . One means of smoothing the model ﬁt is to penalise the sum-squared error. Thus, we modify the squared error measure  5.3  and minimise  jt i  cid:9  g.xi  j2 C Þ  F .g.x   dx  Z  E D nX iD1   5.8   where Þ is a regularisation parameter and F is a function of the complexity of the model. For example, in a univariate curve-ﬁtting problem, a popular choice for F .g  is @2g=@x 2, the second derivative of the ﬁtting function g. In this case, the solution for g that minimises  5.8  is a cubic spline  Green and Silverman, 1994 .  In the neural network literature, a penalising term of the form  Þ X  Qw2  i  i  is often used, where the summation is over all adjustable network parameters, Qw  see Sections 5.3 and 6.2 . This procedure is termed weight decay.   Optimisation criteria 175  For the generalised linear model  5.2  we have seen that the penalised error is taken  to be  see Chapter 4   E D jj  cid:9  OT  T C W O cid:9   Tjj2 C ÞjjWjj2   5.9  expressed in terms of the zero-mean matrices OT and O cid:9 ; and Þ is termed the ridge parameter  see Chapter 4 . The solution for W that minimises E is  W D OT  T O cid:9 . O cid:9   T O cid:9  C ÞI m   cid:9 1  This procedure has been used to regularise radial basis function networks  see Section 5.3 .  5.2.2 Maximum likelihood  An alternative approach to the least squared error measure is to assume a parametric form for the class distributions and to use a maximum likelihood procedure. In the multiclass case, the basic assumption for the generalised logistic discrimination is   cid:5    cid:4  p.xj!s   p.xj!C    log  D þs0 C β T  s φ.x ;  s D 1; : : : ; C  cid:9  1   5.10   where φ.x  is a nonlinear function of the variables x, with parameters f¼g; that is, the log-likelihood ratio is a linear combination of the nonlinear functions,  cid:1 . The posterior probabilities are of the form  see Chapter 4  s0 C β T exp.þ0 1 C PC cid:9 1 jD1 exp.þ0 1 jD1 exp.þ0  s D 1; : : : ; C  cid:9  1  s φ.x   j0 C β T  1 C PC cid:9 1  p.!Cjx  D  p.!sjx  D  j0 C β T  j φ.x    j φ.x     5.11   ;  where þ0 functions þ0  s0 C β T  s  s0 D þs0 C log. p.!s  = p.!C   . Discrimination depends solely on the C  cid:9  1   cid:1  .x  .s D 1; : : : ; C  cid:9  1  with the decision: s φ.x  D þ0  þ0 s0 C β T  max  sD1;:::;C cid:9 1  assign x to class ! j if  j0 C β T  j φ.x  > 0  else assign x to class !C.  For a data set fx1; : : : ; xng, the likelihood of the observations is given by:  L D CY iD1  Y xr2!i  p.xrj!i     5.12    176 Nonlinear discriminant analysis – kernel methods  where xr is the rth pattern of class !i . The parameters of the model  in this case, the þ terms and f¼g, the set of parameters on which  cid:1  depends  may be determined using a maximum likelihood procedure  as in logistic discrimination; see Chapter 4 . Equation  5.12  may be written     L D  1QC iD1[ p.!i  ]ni  ! Y  all x  ! CY iD1  Y xr2!i  p.x   p.!ijxr    and assuming that the factor   5.13    5.14    5.15      1QC iD1[ p.!i  ]ni  ! Y  all x  p.x   L0 D CY iD1  Y xr2!i  p.!ijxr    nX iD1  j.t i  cid:9  p.! j .i  jxi   j2  is independent of the model parameters  the assumption that we are making is on the log-likelihood ratio – we are free to choose p.x ; see Chapter 4 , then maximising L is equivalent to maximising L0 given by  Maximisation of the likelihood is achieved through the use of some form of numerical optimisation scheme, such as conjugate gradient methods or quasi-Newton procedures  see Press et al., 1992 .  The parameters of the generalised logistic model may also be found using a least  squared error procedure by minimising  where t i D .0; 0; : : : ; 0; 1; 0; : : : ; 0 T  the 1 being in the jth position  for xi 2 ! j and p is the model for the vector of a posteriori probabilities given by  5.11 . We know from Section 5.2.1 that p is asymptotically a minimum variance approximation to the Bayes discriminant function.  5.2.3 Entropy  Another optimisation criterion used for classiﬁcation problems is the relative or cross- entropy. If q.!jx  is the true posterior distribution and p.!jx  is the approximation produced by a model, then the entropy measure to be minimised is  G D CX iD1   cid:10  q.!ijx log  E   cid:5 ½   cid:4  q.!ijx  p.!ijx    5.16   where the expectation is with respect to the unconditional distribution of x. If the training data are representative of this distribution, then we may approximate the expectation   Radial basis functions 177   5.17   integral in  5.16  by a ﬁnite sum over the training set,  and minimising  5.17  is equivalent to maximising  cf.  5.12    G D CX iD1  X xr2!i   cid:5    cid:4  q.!ijxr   p.!ijxr    log  OG D CY iD1  Y xr2!i  p.!ijxr     compare with  5.14 ; see the exercises at the end of the chapter . If our approximation for p is of the form  5.11 , for example, then we have the generalised logistic discrimi- nation model. Thus, minimising the cross-entropy is equivalent to maximum likelihood estimation.  5.3 Radial basis functions  5.3.1 Introduction  Radial basis functions  RBFs  were originally proposed in the functional interpolation literature  see the review by Powell, 1987; Lowe, 1995a  and ﬁrst used for discrimination by Broomhead and Lowe  1988 . However, RBFs have been around in one form or another for a very long time. They are very closely related to kernel methods for density estimation and regression developed in the statistics literature  see Chapter 3  and to normal mixture models  Chapter 2 . The RBF may be described mathematically as a linear combination of radially sym- metric nonlinear basis functions. The RBF provides a transformation of a pattern x 2 R p to an n0-dimensional output space according to1  g j .x  D mX iD1  w ji  cid:1 i .jx  cid:9  µij  C w j0;  j D 1; : : : ; n0   5.18   The parameters w ji are often referred to as the weights; w j0 is the bias and the vectors µi are the centres. The model  5.18  is very similar to the kernel density model described in Chapter 3 in which n0 D 1, w10 D 0, and the number of centres m is taken to be equal to the number of data samples n, with µi D xi  a centre at each data sample ; w ji D 1=n and  cid:1  is one of the kernels given in Chapter 3, sometimes referred to as the activation function in the neural network literature. In the case of exact interpolation, a basis function is also positioned at each data point  m D n . Suppose we seek a mapping g from R p to R  taking n0 D 1  through the points .xi ; ti   which satisﬁes the condition that g.xi   D ti ; that is, under the assumed model  5.18   and ignoring the bias , we seek a solution for w D .w1; : : : ; wn T that satisﬁes  1Here we use n0 to denote the dimensionality of the output space. In a classiﬁcation problem, we usually  have n0 D C, the number of classes.  t D  cid:9 w   178 Nonlinear discriminant analysis – kernel methods  2.5  1.5  2  1  0.5  0 −0.5 −1 −1.5  g  g  −2  −1  0  1  2  3  4  5  Figure 5.1 Discriminant functions constructed using a radial basis function network of normal kernels where t D .t1; : : : ; tn T and  cid:9  is the n ð n matrix with .i; j  th element  cid:1  .jxi  cid:9  x jj , for a nonlinear function  cid:1 . For a large class of functions Micchelli  1986  has shown that the inverse of  cid:9  exists and the solution for w is given by  w D  cid:9    cid:9 1t  Exact interpolation is not a good thing to do in general. It leads to poor generalisation performance in many pattern recognition problems  see Chapter 1 . The ﬁtting function can be highly oscillatory. Therefore, we usually take m < n.  An often-cited advantage of the RBF model is its simplicity. Once the forms of the nonlinearity have been speciﬁed and the centres determined, we have a linear model whose parameters can be easily obtained by a least squares procedure, or indeed any appropriate optimisation procedure such as those described in Chapter 4 or Section 5.2. For supervised classiﬁcation, the RBF is used to construct a discriminant function for each class. Figure 5.1 illustrates a one-dimensional example. Data drawn from two univariate normal distributions of unit variance and means of 0.0 and 2.0 are plotted. Normal kernels are positioned over centres selected from these data. The weights w ji are determined using a least squares procedure and the discriminant functions gž and gŠ plotted. Thus, a linear combination of ‘blob’-shaped functions is used to produce two functions that can be used as the basis for discrimination.  5.3.2 Motivation  The RBF model may be motivated from several perspectives. We present the ﬁrst two in a discrimination context and then one from a regression perspective.  Kernel discriminant analysis Consider the multivariate kernel density estimate  see Chapter 3   p.x  D 1 nh p  nX iD1  K   cid:4  1 h   cid:5   .x  cid:9  xi     Radial basis functions 179  where K .x  is deﬁned for p-dimensional x satisfying R R p K .x  dx D 1. Suppose we have a set of samples xi .i D 1; : : : ; n  with n j samples in class ! j . j D 1; : : : ; C . If we construct a density estimate for each class then the posterior probability of class membership can be written  p.! jjx  D p.! j   p.x   1  n j h p  nX iD1  z ji K   cid:5   .x  cid:9  xi     cid:4  1 h  where p.! j   is the prior probability of class ! j and z ji D 1 if xi 2 class ! j , 0 otherwise. Thus, discrimination is based on a model of the form w ji  cid:1 i .x  cid:9  xi     5.20   nX iD1  where  cid:1 i .x  cid:9  xi   D K ..x  cid:9  xi  = h  and   5.19    5.21   w ji D p.! j   n j  z ji   We neglect the term p.x h p in the denominator of  5.19  since it is independent of j.  Equation  5.20  is of the form of an RBF with a centre at each data point and weights determined by class priors  equation  5.21  .  Mixture models In discriminant analysis by Gaussian mixtures  Chapter 2 , the class-conditional density for class ! j is expressed as  p.xj! j   D  ³ jr p.xj cid:17  jr    R jX rD1  where class ! j has R j subgroups, ³ jr are the mixing proportions  PR j ³ jr D 1  and rD1 p.xj cid:17  jr   is the density of the rth subgroup of class ! j evaluated at x;   cid:17  jr denote the subgroup parameters: for normal mixtures, the mean and covariance matrix . The posterior probabilities of class membership are R jX rD1 where p.! j   is the prior probability of class ! j .  For normal mixture components, p.xj cid:17  jr  , with means µ jr ; j D 1; : : : ; C; r D 1; : : : ; R j and common diagonal covariance matrices, ¦ 2I , we have discriminant func- tions of the form  5.18  with – basis functions, where m D PC jD1 R j , weights set by the mixing proportions and class priors, and basis functions centred at the µ jr .  p.! jjx  D p.! j   p.x   ³ jr p.xj cid:17  jr    Regularisation Suppose that we have a data set f.xi ; ti  ; i D 1; : : : ; ng where xi 2 R d, and we seek a smooth surface g,  ti D g.xi   C error   180 Nonlinear discriminant analysis – kernel methods  One such approach is to minimise the penalised sum of squares  S D nX iD1  .ti  cid:9  g.xi   2 C Þ J .g   where Þ is a regularisation or roughness  Green and Silverman, 1994  parameter and J .g  is a penalty term that measures how ‘rough’ the ﬁtted surface is and has the effect of penalising ‘wiggly’ surfaces  see Chapter 4 and Section 5.2.1 .  A popular choice for J is one based on mth derivatives. Taking J as  Z  J .g  D  X m!  R d  ¹1! : : : ¹d!   cid:4   @m g : : : @x ¹d d  @x ¹1 1   cid:5 2  dx1 : : : dxd  where the summation is over all non-negative integers ¹1; ¹2; : : : ; ¹d such that ¹1 C ¹2 C ÐÐÐ C ¹d D m, results in a penalty invariant under translations and rotations of the coordinate system  Green and Silverman, 1994 .  Deﬁning  cid:20 md .r   by   cid:20 md .r   D  ²  cid:17 r 2m cid:9 dlog.r     cid:17 r 2m cid:9 d  if d is even if d is odd  and the constant of proportionality,  cid:17 , by  8>>< >>:   cid:17  D  . cid:9 1 mC1Cd=221 cid:9 2m ³ cid:9 d=2 0.d=2  cid:9  m 2 cid:9 2m ³ cid:9 d=2  1  .m  cid:9  1 ! 1 .m  cid:9  1 !  1  .m  cid:9  d=2 !  if d is even  if d is odd  then  under certain conditions on the points xi and m  the function g minimising J .g  is a natural thin-plate spline. This is a function of the form  g.x  D nX iD1  bi  cid:20 md .jx  cid:9  xij  C MX jD1  a j  cid:22  j .x   where   cid:4  m C d  cid:9  1   cid:5   M D  d  and f cid:22  j ; j D 1; : : : ; Mg is a set of linearly independent polynomials spanning the M- dimensional space of polynomials in R d of degree less than m. The coefﬁcients fa j ; j D 1; : : : ; Mg, fbi ; i D 1; : : : ; ng satisfy certain constraints  see Green and Silverman, 1994, for further details . Thus, the minimising function contains radially symmetric terms,  cid:20 , and polynomials.  An alternative derivation based on a different form for the penalty terms leading to  Gaussian RBFs is provided by Bishop  1995 .   Radial basis functions 181  5.3.3 Specifying the model  The basic RBF model is of the form  g j .x  D mX iD1  w ji  cid:1    cid:4 jx  cid:9  µij   cid:5   h  C w j0;  j D 1; : : : ; n0  that is, all the basis functions are of the same functional form   cid:1 i D  cid:1   and a scaling parameter h has been introduced. In this model, there are ﬁve quantities to prescribe or to determine from the data:  the number of basis functions, m; the form of the basis function,  cid:1 ; the smoothing parameter, h; the positions of the centres, µi ; the weights, w ji , and bias, w j0.  There are three main stages in constructing an RBF model:  1. Specify the nonlinear functions,  cid:1 . To a large extent, this is independent of the data  and the problem  though the parameters of these functions are not .  2. Determine the number and positions of the centres, and the kernel widths.  3. Determine the weights of the RBF. These values are data-dependent.  Stages 2 and 3 above are not necessarily carried out independently. Let us consider each in turn.  Specifying the functional form The ideal choice of basis function is a matter for debate. However, although certain types of problem may be matched inappropriately to certain forms of nonlinearity, the actual form of the nonlinearity is relatively unimportant  as in kernel density estimation  compared to the number and the positions of the centres. Typical forms of nonlinearity are given in Table 5.1. Note that some RBF nonlinearities produce smooth approximations, in that the ﬁtting function and its derivatives are continuous. Others  for example, zlog.z  and exp. cid:9 z   have discontinuous gradients. The two most popular forms are the thin-plate spline,  cid:1  .z  D z 2log.z , and the normal or Gaussian form,  cid:1  .z  D exp. cid:9 z 2 . Each of these functions may be motivated from different perspectives  see Section 5.3.2 : the normal form from a kernel regression and kernel density estimation point of view and the thin-plate spline from curve ﬁtting  Lowe, 1995a . Indeed, each may be shown to be optimal under certain conditions: in ﬁtting data in which there is normally distributed noise on the inputs, the normal form is the optimal basis function in a least squares sense  Webb, 1994 ; in ﬁtting a surface through a set of points and using a roughness penalty, the natural thin-plate spline is the solution  Duchon, 1976; Meinguet, 1979 .   182 Nonlinear discriminant analysis – kernel methods  Table 5.1 Radial basis function nonlinearities  Nonlinearity  Gaussian Exponential Quadratic Inverse quadratic Thin-plate spline Trigonometric  Mathematical form  cid:1  .z , z D jx  cid:9  µj= h exp. cid:9 z2  exp. cid:9 z  z2 C Þz C þ 1=[1 C z2] zÞlog.z  sin.z   These functions are very different: one is compact and positive, the second diverges at inﬁnity and is negative over a region. However, in practice, this difference is to some extent superﬁcial since, for training purposes, the function  cid:1  need only be deﬁned in the feature space over the range [smin; smax], where  and therefore  cid:1  may be redeﬁned over this region as O cid:1 .s  given by  smax D max smin D min  i; j  jxi  cid:9  µ jj jxi  cid:9  µ jj  i; j  O cid:1    cid:9   cid:1  .s= h   cid:9   cid:1 min  cid:1 max  cid:9   cid:1 min  where  cid:1 max and  cid:1 min are the maximum and minimum values of  cid:1  over [0; smax]  taking smin D 0  respectively  0  cid:14  O cid:1   cid:14  1  and s D jx  cid:9  µ jj is the distance in the feature space. Scaling of  cid:1  may simply be compensated for by adjustment of weights fw ji ; i D 1; : : : ; mg and bias w j0, j D 1; : : : ; n0. The ﬁtting function is unaltered.  Figure 5.2 illustrates the normalised form for the normal nonlinearity for several values of the smoothing parameter, h. For the Gaussian basis function, we see that there is little change in the normalised form for h=smax greater than about 2. As h ! 1, the normalised form for the nonlinearity tends to the quadratic  O cid:1 1.s   4D 1  cid:9  s2 s2 max   5.22   Thus, asymptotically, the normalised basis function is independent of h.  For large h=smax  greater than about 2 , changes in the value of h may be compensated for by adjustments of the weights, w ji , and the radial basis function is a quadratic function of the input variables. For smaller values of h, the normalised function tends to the Gaussian form, thus allowing small-scale variation in the ﬁtting function.  In some cases, particularly for the Gaussian kernel, it is important to choose an ‘appropriate’ value for the smoothing parameters in order to ﬁt the structure in the data.   Radial basis functions 183  h=smax D 0:2 h=smax D 0:5 h=smax D 1:0 h=smax D 2:0 quadratic  1  0.8  0.6  0.4  0.2  0  0  0.2  0.4  0.6  0.8  1  s=smax  Figure 5.2 Normalised Gaussian basis functions, O cid:1 .s , for h=smax D 0.2, 0.5, 1.0, 2.0 and the limiting quadratic  This is a compromise between the one extreme of ﬁtting noise in the data and the other of being unable to model the structure. As in kernel density estimation, it is more important to choose appropriate values for the smoothing parameters than the functional form. See Chapter 3 for a discussion on the choice of smoothing parameter in kernel density estimation. There is some limited empirical evidence to suggest that thin-plate splines ﬁt data better in high-dimensional settings  Lowe, 1995b .  Centres and weights The values of centres and weights may be found by minimising a suitable criterion  for example, least squares  using a nonlinear optimisation scheme. However, it is more usual to position the centres ﬁrst, and then to calculate the weights using one of the optimisa- tion schemes appropriate for linear models. Of course, this means that the optimisation criterion will not be at an extremum with respect to the positions of the centres, but in practice this does not matter.  Positioning of the centres can have a major effect on the performance of an RBF for discrimination and interpolation. In an interpolation problem, more centres should be positioned in regions of high curvature. In a discrimination problem, more centres should be positioned near class boundaries. There are several schemes commonly employed.  1. Select from data set–random selection Select randomly from the data set. We would expect there to be more centres in regions where the density is greater. A con- sequence of this is that sparse regions may not be ‘covered’ by the RBF unless the smoothing parameter is adjusted. Random selection is an approach that is commonly used. An advantage is that it is fast. A disadvantage is the failure to take into account the ﬁtting function, or the class labels in a supervised classiﬁcation problem; that is, it is an unsupervised placement scheme and may not provide the best solution for a mapping problem.   184 Nonlinear discriminant analysis – kernel methods  2. Clustering approach The values for the centres obtained by the previous approach could be used as seeds for a k-means clustering algorithm, thus giving centres for RBFs as cluster centres. The k-means algorithm seeks to partition the data into k groups or clusters so that the within-group sum of squares is minimised; that is, it seeks the cluster centres fµ j ; j D 1; : : : ; kg that minimise  kX jD1  S j  where the within-group sum of squares for group j is z jijxi  cid:9  µ jj2 in which z ji D 1 if xi is in group j  of size n j D Pn  S j D nX iD1  the mean of group j,  µ j D 1 n j  nX iD1  z ji xi  iD1 z ji   and zero otherwise; µ j is  Algorithms for computing the cluster centres µ j using k-means are described in Chapter 10.  Alternatively, any other clustering approach could be used: either pattern based, or  dissimilarity matrix based by ﬁrst forming a dissimilarity matrix  see Chapter 10 .  3. Normal mixture model If we are using normal nonlinearities, then it seems sensible to use as centres  and indeed widths  the parameters resulting from a normal mixture model of the underlying distribution p.x   see Chapter 2 . We model the distribution as a mixture of normal models  p.x  D gX jD1  ³ j p.xjµ j ; h   where  p.xjµ j ; h  D  1  .2³   p=2h p exp  ²   cid:9  1 2h2  .x  cid:9  µ j  T .x  cid:9  µ j    ¦  The values of h, ³ j and µ j may be determined using the EM algorithm to maximise the likelihood  see Chapter 2 . The weights ³ j are ignored and the resulting normal basis functions, deﬁned by h and µi , are used in the RBF model.  4. k-nearest-neighbour initialisation The approaches described above use the input data only to deﬁne the centres. Class labels, or the values of the dependent variables in a regression problem, are not used. Thus, unlabelled data from the same distribution as the training data may be used in the centre initialisation process. We now consider some supervised techniques. In Chapter 3 we found that in the k-nearest-neighbour classiﬁer not all data samples are required to deﬁne the decision boundary. We may use an editing   Radial basis functions 185  procedure to remove those prototypes that contribute to the error  with the hope of improving the generalisation performance  and a condensing procedure to reduce the number of samples needed to deﬁne the decision boundary. The prototypes remaining after editing and condensing may be retained as centres for an RBF classiﬁer.  5. Orthogonal least squares The choice of RBF centres can be viewed as a problem in variable selection  see Chapter 9 . Chen et al.  1991; see also Chen et al., 1992  consider the complete set of data samples to be candidates for centres and construct a set of centres incrementally. Suppose that we have a set of k  cid:9  1 centres, positioned over k  cid:9  1 different data points. At the kth stage, the centre selected from the remaining n  cid:9  .k  cid:9  1  data samples that reduces the prediction error the most is added to the set of centres. The number of centres is chosen as that for which an information criterion is minimised. A na¨ıve implementation of this method would solve for the network weights and evaluate equation  5.4  n  cid:9  .k  cid:9  1  times at the kth stage. Chen et al.  1991  propose a scheme that reduces the computation based on an orthogonal least squares algorithm. We seek a solution for the m ð n0 matrix W that minimises2  The matrix O cid:9  is decomposed as O cid:9  D V A, where A is an m ð m upper triangular matrix  jj OT  cid:9  O cid:9 W Tjj2  2  6666664  A D  1 Þ12 Þ23 0 Þ23 : : :  1  : : : : : : : : :  0 ::: 0  0 : : : : : :  3  7777775  Þ1m Þ2m ::: ::: 1  0 and V is an n ð m matrix D [v1; : : : ; vn]T and vT i v j D 0 if i 6D j. The error E is then given by E D jj OT  cid:9  V Gjj2  where G D AW T . Using the minimum norm solution for G that minimises E, we may write  see the exercises   E D Trf OT D Trf OT  T OT g  cid:9  TrfGT V T V Gg   n0X T OT g  cid:9  mX iD1 jD1  jv jj2  !  g2 ji   5.23   The error reduction due to vk is deﬁned as  4D jvkj2  errk  !  g2 ki    n0X iD1  2We work with the zero-mean matrices OT and O cid:9 .   186 Nonlinear discriminant analysis – kernel methods  where Gki D gki . Chen et al.  1991  propose an efﬁcient algorithm for computing this error when an additional centre is introduced and terminate the algorithm using a criterion that balances the ﬁtting error against complexity: speciﬁcally, the selection procedure is terminated when    C k cid:23  is a minimum, where  cid:23  is assigned a value 4 and ¦ 2 k centres.  n log.¦ 2 e  e is the variance of the residuals for  cid:1   Having obtained centres, we now need to specify the smoothing parameters. These depend on the particular form we adopt for the nonlinearity. If it is normal, then the normal mixture approach will lead to values for the widths of the distributions naturally. The other approaches will necessitate a separate estimation procedure. Again, there are several heuristics which were discussed in Chapter 3 on kernel density estimation. Al- though they are suboptimal, they are fast to calculate. An alternative is to choose the smoothing parameter that minimises a cross-validation estimate of the sum-squared error. We have not addressed the question of how to choose the number of centres  except as part of the orthogonal least squares centre selection procedure . This is very similar to many of the problems of model complexity discussed elsewhere in this book  see Chapter 11 ; for example, how many clusters are best, how many components in a normal mixture, how we determine intrinsic dimensionality, etc. It is not easy to answer. The number depends on several factors, including the amount and distribution of the data, the dimension and the form adopted for the nonlinearity. It is probably better to have many centres with limited complexity  single smoothing parameter  than an RBF with few centres and a complex form for the nonlinearity. There are several approaches to determining the number of centres. These include:  1. Using cross-validation. The cross-validation error  minimised over the smoothing pa- rameter  is plotted as a function of the number of centres. The number of centres is chosen as that above which there is no appreciable decrease, or an increase in the cross-validation error  as used by Orr, 1995, in forward selection of centres .  2. Monitoring the performance on a separate test set. This is similar to the cross- validation procedure above, except that the error is evaluated on a separate test set.  3. Using an information complexity criterion. The sum-squared error is augmented by  an additional term that penalises complexity  see for example, Chen et al., 1991 .  4. If we are using a normal mixture model to set the positions and widths of centres, we may use one of the methods for estimating the number of components in a normal mixture  see Chapter 2 .  The ﬁnal stage of determining the RBF is the calculation of the weights, either by optimising the squared error measure  5.3 , the regularised error or the likelihood  5.12 . Section 5.2 has described these procedures in some detail.  Most of the stages in the optimisation of an RBF use techniques described elsewhere in this book  for example, clustering prototype selection, kernel methods, least squares or maximum likelihood optimisation . In many ways, a radial basis function is not new; all its constituent parts are widely used tools of pattern recognition. In Section 5.3.5 we put them together to derive a discrimination model based on normal nonlinearities.   Radial basis functions 187  5.3.4 Radial basis function properties  One of the properties of an RBF that has motivated its use in a wide range of applications both in functional approximation and in discrimination is that it is a universal approx- imator: it is possible  given certain conditions on the kernel function  to construct an RBF that approximates a given  integrable, bounded and continuous  function arbitrarily accurately  Park and Sandberg, 1993; Chen and Chen, 1995 . This may require a very large number of centres. In most, if not all, practical applications the mapping we wish to approximate is deﬁned by a ﬁnite set of data samples providing class-labelled data or, in an approximation problem, data samples and associated function values, and im- plemented in ﬁnite-precision arithmetic. Clearly, this limits the complexity of the model. We refer to Chapter 11 for a discussion of model order selection.  5.3.5 Simple radial basis function  We have now set up the machinery for implementing a simple RBF. The stages in a simple implementation are as follows.  1. Specify the functional form for the nonlinearity.  2. Prescribe the number of centres, m.  3. Determine the positions of the centres  for example, random selection or the k-means  algorithm .  validation .  4. Determine the smoothing parameters  for example, simple heuristic or cross-  5. Map the data to the space spanned by the outputs of the nonlinear functions; i.e. for  a given data set xi ; i D 1; : : : ; n, form the vectors φi D φ.xi  ; i D 1; : : : ; n.  6. Solve for the weights and the biases using  e.g.  least squares or maximum likelihood.  7. Calculate the ﬁnal output on the train and test sets; classify the data if required.  The above is a simple prescription for an RBF network that uses unsupervised tech- niques for centre placement and width selection  and therefore is suboptimal . One of the often-quoted advantages of the RBF network is its simplicity – there is no non- linear optimisation scheme required, in contrast to the multilayer perceptron classiﬁer discussed in the following section. However, many of the sophisticated techniques for centre placement and width determination are more involved and increase the compu- tational complexity of the model substantially. Nevertheless, the simple RBF can give acceptable performance for many applications.  5.3.6 Example application study  The problem The problem of source position estimation using measurements made on a radar focal-plane array using RBFs was treated by Webb and Garner  1999 . This   188 Nonlinear discriminant analysis – kernel methods  particular approach was motivated by a requirement for a compact integrated  hardware  implementation of a bearing estimator in a sensor focal plane  a solution was required that could readily be implemented in silicon on the same substrate as the focal-plane array .  Summary The problem is one of prediction rather than discrimination: given a set of training samples f.xi ;  cid:17 i  I i D 1; : : : ; ng, where xi is a vector of measurements on the independent variables  array calibration measurements in this problem  and  cid:17 i is the response variable  position , a predictor, f , was sought such that given a new measure- ment z, then f .z  is a good estimate of the position of the source that gave rise to the measurement z. However, the problem differs from a standard regression problem in that there is noise on the measurement vector z, and this is similar to errors-in-variables models in statistics. Therefore, we seek a predictor that is robust to noise on the inputs.  The data The training data comprised detector outputs of an array of 12 detectors, positioned in the focal plane of a lens. Measurements were made on the detectors as the lens scanned across a microwave point source  thus providing measurements of the point-spread function of the lens. There were 3721 training samples measured at a signal- to-noise ratio  SNR  of about 45 dB. The test data were recorded for the source at speciﬁc positions over a range of lower SNR.  The model The model adopted is a standard RBF network with a Gaussian kernel with centres deﬁned in the 12-dimensional space. The approach adopted to model pa- rameter estimation was a standard least squares one. The problem may be regarded as one example from a wider class in discrimination and regression in which the expected operating conditions  the test conditions  differ from the training conditions in a known way. For example, in a discrimination problem, the class priors may differ consider- ably from the values estimated from the training data. In a least squares approach, this may be compensated for by modifying the sum-squared error criterion appropriately  see Section 5.2.1 . Also, allowance for expected population drift may be made by modifying the error criterion. In the source position estimation problem, the training conditions are considered ‘noiseless’  obtained through a calibration procedure  and the test conditions differ in that there is noise  of known variance  on the data. Again, this can be taken into account by modifying the sum-squared error criterion.  Training procedure An RBF predictor was designed with centres chosen using a k- means procedure. A ridge regression type solution was obtained for the weights  see Section 5.2.1 on regularisation , with the ridge parameter inversely proportional to an SNR term. Thus, there is no need to perform any search procedure to determine the ridge parameter. It can be set by measuring the SNR of the radar system. The theoretical development was validated by experimental results on a 12-element microwave focal- plane array in which two angle coordinates were estimated.  Results It was shown that it was possible to compensate for noisy test conditions by using a regularisation solution for the parameters, with regularisation parameter propor- tional to the inverse of the SNR.   Radial basis functions 189  5.3.7 Further developments  There have been many developments of the basic RBF model in the areas of RBF design, learning algorithms and Bayesian treatments.  Developments of the k-means approach to take account of the class labels of the data samples are described by Musavi et al.  1992 , in which clusters contain samples from the same class, and Karayiannis and Wi  1997 , in which a localised class-conditional variance is minimised as part of a network growing process.  Chang and Lippmann  1993  propose a supervised approach for allocating RBF cen- tres near class boundaries. An alternative approach that chooses centres for a classiﬁcation task in a supervised way based on the ideas of support vectors is described by Sch¨olkopf et al.  1997   see Chapter 4 and Section 5.4 . In support vector learning of RBF net- works with Gaussian basis functions, the separating surface obtained by a support vector machine  SVM  approach  that is, the decision boundary  is a linear combination of Gaussian functions centred at selected training points  the support vectors . The number and location of centres is automatically determined. In a comparative study reviewing several approaches to RBF training, Schwenker et al.  2001  ﬁnd that the SVM learning approach is often superior on a classiﬁcation task to the standard two-stage learning of RBFs  selecting or adapting the centres followed by calculating the weights .  The orthogonal least squares forward selection procedure has been developed to use a regularised error criterion by Chen et al.  1996  and Orr  1995 , who uses a generalised cross-validation criterion as a stopping condition.  In the basic approach, all patterns are used at once in the calculation for the weights  ‘batch learning’ . On-line learning methods have been developed  for example, Marinaro and Scarpetta, 2000 . These enable the weights of the network to be updated sequentially according to the error computed on the last selected new example. This allows for possible temporal changes in the task being learned.  A Bayesian treatment that considers the number of basis functions and weights to be unknown has been developed by Holmes and Mallick  1998 . A joint probability density function is deﬁned over model dimension and model parameters. Using Markov chain Monte Carlo methods, inference is made by integrating over the model dimension and parameters.  5.3.8 Summary  Radial basis functions are simple to construct, easy to train and ﬁnd a solution for the weights rapidly. They provide a very ﬂexible model and give very good performance over a wide range of problems, both for discrimination and for functional approximation. The RBF model uses many of the standard pattern recognition building blocks  clustering and least squares optimisation, for example . There are many variants of the model  due to the choice of centre selection procedure, form of the nonlinear functions, procedure for determining the weights, and model selection method . This can make it difﬁcult to draw meaningful conclusions about RBF performance over a range of studies on different applications, since the form of an RBF may vary from study to study.  The disadvantages of RBFs also apply to many, if not all, of the discrimination models covered in this book. That is, care must be taken not to construct a classiﬁer   190 Nonlinear discriminant analysis – kernel methods  that models noise in the data, or models the training set too well, which may give poor generalisation performance. Choosing a model of the appropriate complexity is very important, as we emphasise repeatedly in this book. Regularising the solution for the weights can improve generalisation performance. Model selection requirements add to the computational requirements of the model, and thus the often-claimed simplicity of RBF networks is perhaps overstated. Yet it should be said that a simple scheme can give good performance which, not unusually, exceeds that of many of the more ‘traditional’ statistical classiﬁers.  5.4 Nonlinear support vector machines  In Chapter 4 we introduced the support vector machine as a tool for ﬁnding the optimal separating hyperplane for linearly separable data and considered developments of the approach for situations when the data are not linearly separable. As we remarked in that chapter, the support vector algorithm may be applied in a transformed feature space, φ.x , for some nonlinear function φ. Indeed, this is the principle behind many methods of pattern classiﬁcation: transform the input features nonlinearly to a space in which linear methods may be applied  see also Chapter 1 . We discuss this approach further in the context of SVMs.  For the binary classiﬁcation problem, we seek a discriminant function of the form  g.x  D wT φ.x  C w0  with decision rule wT φ.x  C w0  ² > 0 < 0    x 2  ² !1 with corresponding numeric value yi D C1 !2 with corresponding numeric value yi D  cid:9 1  The SVM procedure determines the maximum margin solution through the maximisation of a Lagrangian. The dual form of the Lagrangian  equation  4.30   becomes  L D D nX iD1  Þi  cid:9  1 2  nX iD1  nX jD1  Þi Þ j yi y j φT .xi  φ.x j     5.24   where yi D š1; i D 1; : : : ; n, are class indicator values and Þi ; i D 1; : : : ; n, are Lagrange multipliers satisfying   5.25   for a ‘regularisation’ parameter, C. Maximising  5.24  subject to the constraints  5.25  leads to support vectors identiﬁed by non-zero values of Þi .  The solution for w  see Chapter 4  is  0  cid:14  Þi  cid:14  C nX Þi yi D 0 iD1  w D X i2SV  Þi yi φ.xi     Nonlinear support vector machines 191  and classiﬁcation of a new data sample x is performed according to the sign of  g.x  D X i2SV  Þi yi φT .xi  φ.x  C w0   5.26   where  w0 D 1 NfSV  8 < X : i2fSV  yi  cid:9  X i2SV; j2fSV  9 = ;  Þi yi φT .xi  φ.x j     5.27   in which SV is the set of support vectors with associated values of Þi satisfying 0 < Þi  cid:14  C and fSV is the set of NfSV support vectors satisfying 0 < Þi < C  those at the target distance of 1=jwj from the separating hyperplane . Optimisation of L D  5.24  and the subsequent classiﬁcation of a sample   5.26  and  5.27   relies only on scalar products between transformed feature vectors, which can be replaced by a kernel function  K .x; y  D φT .x φ.y   Thus, we can avoid computing the transformation φ.x  explicitly and replace the scalar product with K .x; y  instead. The discriminant function  5.26  becomes  g.x  D X i2SV  Þi yi K .xi ; x  C w0   5.28   The advantage of the kernel representation is that we need only use K as the training algorithm and even do not need to know φ explicitly, provided that the kernel can be written as an inner product. In some cases  for example, the exponential kernel , the feature space is inﬁnite-dimensional and so it is more efﬁcient to use a kernel.  5.4.1 Types of kernel  There are many types of kernel that may be used in an SVM. Table 5.2 lists some commonly used forms. Acceptable kernels must be expressible as an inner product in a feature space, which means that they must satisfy Mercer’s condition  Courant and Hilbert, 1959; Vapnik, 1998 : a kernel K .x; y ; x; y 2 R p, is an inner product in some feature space, or K .x; y  D φT .x φ.y , if and only if K .x; y  D K .y; x  and  Z  K .x; z  f .x  f .z  dx dz ½ 0  Table 5.2 Support vector machine kernels  Nonlinearity  Mathematical form  Polynomial Gaussian Sigmoid  K .x; y   .1 C xT y d exp. cid:9 jx  cid:9  yj2=¦ 2  tanh.kxT y  cid:9  Ž    192 Nonlinear discriminant analysis – kernel methods  for all functions f satisfying  Z  f 2.x  dx < 1  That is, K .x; y  may be expanded as  where ½ j and  cid:1  j .x  are the eigenvalues and eigenfunctions satisfying  K .x; y  D  ½ j O cid:1  j .x  O cid:1  j .y   1X jD1  Z  K .x; y  cid:1  j .x  dx D ½ j  cid:1  j .x   and O cid:1  j is normalised so that R O cid:1 2  .x  dx D 1.  j  As an example, consider the kernel K .x; y  D .1 C xT y d for d D 2 and x; y 2 R 2.  This may be expanded as  .1 C x1 y1 C x2 y2 2 D 1 C 2x1 y1 C 2x2 y2 C 2x1x2 y1 y2 C x 2  1 C x 2 2 y2 1 y2 2  D φT .x φ.y   where φ.x  D .1;  p 2x1;  p 2x2;  p 2x1x2; x 2 1  ; x 2 2   .  5.4.2 Model selection  The degrees of freedom of the SVM model are the choice of kernel, the parameters of the kernel and the choice of the regularisation parameter, C, which penalises the training errors. For most types of kernel, it is generally possible to ﬁnd values for the kernel parameters for which the classes are separable. However, this is not a sensible strategy and leads to over-ﬁtting of the training data and poor generalisation to unseen data.  The simplest approach to model selection is to reserve a validation set that is used to monitor performance as the model parameters are varied. More expensive alternatives that make better use of the data are data resampling methods such as cross-validation and bootstrapping  see Chapter 11 .  5.4.3 Support vector machines for regression  Support vector machines may also be used for problems in regression. Suppose that we have a data set f.xi ; yi  ; i D 1; : : : ; ng of measurements xi on the independent variables and yi on the response variables. Instead of the constraints  4.25  we have  .wT xi C w0   cid:9  yi  cid:14  ž C ¾i yi  cid:9  .wT xi C w0   cid:14  ž C O¾i ¾i ; O¾ ½ 0  i D 1; : : : ; n i D 1; : : : ; n i D 1; : : : ; n   5.29    Nonlinear support vector machines 193  This allows a deviation between the target values yi and the function f ,  f .x  D wT x C w0  Two slack variables are introduced: one  ¾  for exceeding the target value by more than ž and O¾ for being more than ž below the target value  see Figure 5.3 . As in the classiﬁcation case, a loss function is minimised subject to the constraints  5.29 . For a linear ž-insensitive loss, we minimise  wT w C C  1 2  .¾i C O¾i    nX iD1  The primal form of the Lagrangian is L p D 1 2  .¾i C O¾i    cid:9  nX iD1  wT w C C  nX iD1  Þi .¾i C ž  cid:9  .wT xi C w0  cid:9  yi     cid:9  nX iD1  ri ¾i   cid:9  nX iD1  OÞi .O¾i C ž  cid:9  .yi  cid:9  wT xi  cid:9  w0    cid:9  nX iD1  Ori O¾i  where Þi ; OÞi ½ 0 and ri ; Ori ½ 0 are Lagrange multipliers. Differentiating with respect to w, w0, ¾i and O¾i gives   5.30    5.31   .Þi  cid:9  OÞi  xi D 0  w C nX iD1 nX .Þi  cid:9  OÞi   D 0 iD1 C  cid:9  Þi  cid:9  ri D 0 C  cid:9  OÞi  cid:9  Ori D 0  Substituting for w into  5.30  and using the relations above gives the dual form  L D D nX iD1  .OÞi  cid:9  Þi  yi  cid:9  ž  nX iD1  .OÞi C Þi    cid:9  1 2  nX iD1  nX jD1  .OÞi  cid:9  Þi  .OÞ j  cid:9  Þ j  xT  i x j   5.32   Š   cid:1  cid:2 ž Š  Š  Š   cid:2 O¾  cid:1  Š  Š  Š  Š Š  Š  Š  Š  Š  Š  Š  Š  Š   cid:1  cid:2 ž Š  Š  Š  Š  cid:1   cid:2 ¾  Š  Š Š  Figure 5.3 Linear  left  and nonlinear  right  SVM regression. The variables ¾ and O¾ measure the cost of lying outside the ‘ž-insensitive band’ around the regression function   194 Nonlinear discriminant analysis – kernel methods  This is maximised subject to   5.33    5.34    5.35   which follows from  5.31  and ri ; Ori ½ 0. The Karush–Kuhn–Tucker complementarity conditions are  nX iD1  .Þi  cid:9  OÞi   D 0 0  cid:14  Þi ; OÞi  cid:14  C  Þi .¾i C ž  cid:9  .wT xi C w0  cid:9  yi    D 0 OÞi .¾i C ž  cid:9  .yi  cid:9  wT xi  cid:9  w0   D 0 ri ¾i D .Þi  cid:9  C ¾i D 0 Ori O¾i D . OÞi  cid:9  C O¾i D 0  These imply Þi OÞi D 0 and ¾i O¾i D 0. Those patterns xi with Þi > 0 or OÞi > 0 are support vectors. If 0 < Þi < C or 0 < OÞi < C then .xi ; yi   lies on the boundary of the tube surrounding the regression function at distance ž. If Þi D C or OÞi D C, then the point lies outside the tube.  The solution for f .x  is then  f .x  D nX iD1  .OÞi  cid:9  Þi  xT  i x C w0  using the expression for w in  5.31 . The parameter w0 is chosen so that  f .xi    cid:9  yi D ž f .xi    cid:9  yi D  cid:9 ž  or  for any i with 0 < Þi < C for any i with 0 < OÞi < C  by the Karush–Kuhn–Tucker complementarity conditions above.  Nonlinear regression The above may also be generalised to a nonlinear regression function in a similar manner to the way in which the linear discriminant function, introduced in Chapter 4, was generalised to the nonlinear discriminant function. If the nonlinear function is given by  f .x  D wT φ.x  C w0  then equation  5.32  is replaced by nX iD1  L D D nX iD1  .OÞi  cid:9  Þi  yi  cid:9  ž  .OÞi C Þi    cid:9  1 2  nX iD1  nX jD1  .OÞi  cid:9  Þi  .OÞ j  cid:9  Þ j  K .xi ; x j     5.36  where K .x; y  is a kernel satisfying Mercer’s conditions. This is maximised subject to the constraints  5.33 .  The solution for f .x  is then  compare with  5.35    f .x  D nX iD1  .OÞi  cid:9  Þi  K .x; xi   C w0   Nonlinear support vector machines 195  The parameter w0 is chosen so that f .xi    cid:9  yi D ž f .xi    cid:9  yi D  cid:9 ž  or  for any i with 0 < Þi < C for any i with 0 < OÞi < C  by the Karush–Kuhn–Tucker complementarity conditions  5.34 .  Implementation There are many freely available and commercial software packages for solving quadratic programming optimisation problems, often based on standard numerical methods of non- linear optimisation that iteratively hill-climb to ﬁnd the maximum of the objective func- tion. For very large data sets, however, they become impractical. Traditional quadratic programming algorithms require that the kernel be computed and stored in memory and can involve expensive matrix operations. There are many developments to handle large data sets. Decomposition methods  see Table 5.3  apply the standard optimisation package to a ﬁxed subset of the data and revise the subset in the light of applying the classiﬁcation regression model learned to the training data not in the subset.  A special development of the decomposition algorithm is the sequential minimal optimisation algorithm, which optimises a subset of two points at a time, for which the optimisation admits an analytic solution. Pseudocode for the algorithm can be found in Cristianini and Shawe-Taylor  2000 .  5.4.4 Example application study  The problem To predict protein secondary structure as a step towards the goal of predicting three-dimensional protein structures directly from protein sequences  Hua and Sun, 2001 .  Table 5.3 The decomposition algorithm  1. Set b, the size of the subset  b < n, the total number of patterns . Set  Þi D 0 for all patterns.  2. Choose b patterns from the training data set to form a subset B. 3. Solve the quadratic programming problem deﬁned by the subset B  using a standard routine.  4. Apply the model to all patterns in the training set.  5. If there are any patterns that do not satisfy the Karush–Kuhn–Tucker conditions, replace any patterns in B and the corresponding Þi with these patterns and their Þi values.  6. If not converged, go to step 3.   196 Nonlinear discriminant analysis – kernel methods  Summary As a result of the genome and other sequencing projects, the number of protein sequences is growing rapidly. However, this is much greater than the increasing number of known protein structures. The aim of this study is to classify the secondary structure  as helix  H , sheets  E , or coil  C   based on sequence features. An SVM was compared with an algorithm based on a multilayer perceptron  see Chapter 6 .  The data Two data sets were used to develop and test the algorithms. One is a data set of 126 protein chains. The second has 513 protein chains.  The model A standard SVM model was adopted, with a spherically symmetric Gaus- sian kernel  Table 5.2 , with ¦ 2 D 10:0 and a regularisation parameter C D 1:5.  Training procedure Classiﬁers were constructed for different window lengths, l: the number of amino acids in the sequence. Each amino acid is encoded as a 21-dimensional binary vector corresponding to the 20 types of amino acid and the C or N terminus. Thus the pattern vector is of dimension 21l. Three binary SVM classiﬁers were constructed: H versus  E, C ; E versus  H, C ; and C versus  H, E . These were combined in two main ways. The ﬁrst method assigned a test pattern to the class with the largest positive distance to the optimal separating hyperplane. In the second, the binary classiﬁers were combined in a tree structure. A sevenfold cross-validation scheme was used to obtain results.  Results The performance of the SVM method matched or was signiﬁcantly better than the neural network method.  5.4.5 Further developments  There are many developments of the basic SVM model for discrimination and regression presented in this chapter. Multiclass SVMs may be developed along the lines discussed in Chapter 4, whether by combining binary classiﬁers in a one-against-one or a one- against-all method, or by solving a single multiclass optimisation problem  the ‘all- together’ method . In an assessment of these methods, Hsu and Lin  2002  ﬁnd that the all-together method yields fewer support vectors, but one-against-all is more suitable for practical use.  Incorporation of priors and costs into the SVM model  to allow for test conditions that differ from training conditions, as often happens in practice  is addressed by Lin et al.  2002 .  The basic regression model has been extended to take account of different ž-insensitive loss functions and ridge regression solutions  Vapnik, 1998; Cristianini and Shawe-Taylor, 2000 . The ¹-support vector algorithm  Sch¨olkopf et al., 2000  introduces a parameter to control the number of support vectors and errors. The support vector method has also been applied to density estimation  Vapnik, 1998 .  The relationship of the support vector method to other methods of classiﬁcation is  discussed by Guyon and Stork  1999  and Sch¨olkopf et al.  1997 .   Application studies 197  5.4.6 Summary  Support vector machines comprise a class of algorithms that represent the decision boundary in a pattern recognition problem typically in terms of a small subset of the training samples. This generalises to problems in regression through the ž-insensitive loss function that does not penalise errors below ž > 0.  The loss function that is minimised comprises two terms, a term wT w that char- acterises model complexity and a second term that measures training error. A single parameter, C, controls the trade-off between these two terms. The optimisation prob- lem can be recast as a quadratic programming problem for both the classiﬁcation and regression cases.  Several approaches for solving the multiclass classiﬁcation problem have been pro-  posed: combining binary classiﬁers and a one-step multiclass SVM approach.  SVMs have been applied to a wide range of applications, and demonstrated to be valuable for real-world problems. The generalisation performance often either matches or is signiﬁcantly better that competing methods.  Once the kernel is ﬁxed, SVMs have only one free parameter – the regularisation pa- rameter that controls the balance between model complexity and training error. However, there are often parameters of the kernel that must be set and a poor choice can lead to poor generalisation. The choice of best kernel for a given problem is not resolved and special kernels have been derived for particular problems, for example document classiﬁcation.  5.5 Application studies  There have been very many application studies involving neural networks  including the radial basis function network and the multilayer perceptron described in the next chapter . Examples are given here and in Chapter 6. Some review articles in speciﬁc application domains include the following: ž Face processing. Valentin et al.  1994  review connectionist models of face recognition   see also Samal and Iyengar, 1992 .  ž Speech recognition. Morgan and Bourlard  1995  review the use of artiﬁcial neural networks in automatic speech recognition, and describe hybrid hidden Markov models and artiﬁcial neural network models.  ž Image compression. A summary of the use of neural network models as signal pro-  cessing tools for image compression is provided by Dony and Haykin  1995 .  ž Fault diagnosis. Sorsa et al.  1991  consider several neural architectures, including the  multilayer perceptron, for process fault diagnosis.  ž Chemical science. Sumpter et al.  1994 . ž Target recognition. Reviews of neural networks for automatic target recognition are  provided by Roth  1990  and Rogers et al.  1995 .  ž Financial engineering. Refenes et al.  1997 ; Burrell and Folarin  1997 .   198 Nonlinear discriminant analysis – kernel methods  There have been many special issues of journals focusing on different aspects of neu- ral networks, including everyday applications  Dillon et al., 1997 , industrial electronics  Chow, 1993 , general applications  Lowe, 1994 , signal processing  Constantinides et al., 1997; Unbehauen and Luo, 1998 , target recognition, image processing  Chellappa et al., 1998 , machine vision  Dracopoulos and Rosin, 1998  and oceanic engineering  Simpson, 1992 .  Other application domains include remote sensing, medical image analysis and char-  acter recognition. See also Chapter 6.  There have been many comparative studies assessing the performance of neural net- works in terms of speed of training, memory requirements, and classiﬁcation performance  or prediction error in regression problems , comparing the results with statistical classi- ﬁers. Probably the most comprehensive comparative study is that provided by the Statlog project  Michie et al., 1994 . A neural network method  an RBF  gave best performance on only one out of 22 data sets, but provided close to best performance in nearly all cases. Other comparative studies include, for example, assessments on character recogni- tion  Logar et al., 1994 , ﬁngerprint classiﬁcation  Blue et al., 1994  and remote sensing  Serpico et al., 1996 .  There is an increasing amount of application and comparative studies involving sup-  port vector machines. These include: ž Financial time series prediction. Cao and Tay  2001  investigate the feasibility of using SVMs in ﬁnancial forecasting  see also van Gestel et al., 2001 . An SVM with Gaussian kernel is applied to multivariate data  ﬁve or eight variables  relating to the closing price of the S&P Daily Index in the Chicago Mercantile Exchange.  ž Drug design. This is an application in structure-activity relationship analysis, a tech- nique used to reduce the search for new drugs. Combinatorial chemistry enables the synthesis of millions of new molecular compounds at a time. Statistical techniques that direct the search for new drugs are required to provide an alternative to testing every molecular combination. Burbidge et al.  2001  compare SVMs with an RBF network and a classiﬁcation tree  see Chapter 7 . The training time for the classiﬁcation tree was much smaller than for the other methods, but signiﬁcantly better performance  measured in terms of error rate  was obtained with the SVM.  ž Cancer diagnosis. There have been several applications of SVMs to disease diagnosis. Furey et al.  2000  address the problem of tissue sample labelling using measurements from DNA microarray experiments. The data sets comprise measurements on ovarian tissue; human tumour and normal colon tissues; and bone marrow and blood samples from patients with leukaemia. Similar performance to a linear perceptron was achieved. Further cancer studies are reported by Guyon et al.  2002  and Ramaswamy et al.  2001 .  ž Radar image analysis. Zhao et al.  2000  compare three classiﬁers, including an SVM, on an automatic target recognition task using synthetic aperture radar data. Experi- mental results show that the SVM and a multilayer perceptron  see Chapter 6  gave similar performance, but superior to nearest-neighbour.  ž Text analysis. De Vel et al.  2001  use SVMs to analyse e-mail messages. In the growing ﬁeld of computer forensics, of particular interest to investigators is the misuse of e-mail for the distribution of messages and documents that may be unsolicited,   inappropriate, unauthorised or offensive. The objectives of this study are to classify e-mails as belonging to a particular author.  Summary and discussion 199  5.6 Summary and discussion  In this chapter we have developed the basic linear discriminant model to one in which the model is essentially linear, but the decision boundaries are nonlinear. The radial basis function model is implemented in a straightforward manner and, in its simplest form, it requires little more than a matrix pseudo-inverse operation to determine the weights of the network. This hides the fact that optimum selection of the numbers and positions of centres is a more complicated process. Nevertheless, simple rules of thumb can result in acceptable values giving good performance.  Strongly related to kernel discriminant analysis and kernel regression, it possesses many of the asymptotic properties of those methods. Under appropriate conditions, the model provides a least squares approximation to the posterior probabilities of class mem- bership. This enables changes in priors and costs to be readily incorporated into a trained model, without need for retraining.  The support vector machine deﬁnes the basis functions implicitly through the def- inition of a kernel function in the data space and has been found to give very good performance on many problems. There are few parameters to set: the kernel parameters and the regularisation parameter. These can be varied to give optimum performance on a validation set. The SVM focuses on the decision boundary and the standard model is not suitable for the non-standard situation where the operating conditions differ from the training conditions due to drifts in values for costs and priors. An SVM implementa- tion of the RBF will automatically determine the number of centres, their positions and weights as part of the optimisation process  Vapnik, 1998 .  5.7 Recommendations  The nonlinear discriminant methods described in this chapter are easy to implement and there are many sources of software for applying them to a data set. Before applying these techniques you should consider the reasons for doing so. Do you believe that the decision boundary is nonlinear? Is the performance provided by linear techniques below that desired or believed to be achievable? Moving to neural network techniques or support vector machines may be one way to achieve improved performance. This is not guaranteed. If the classes are not separable, a more complex model will not make them so. It may be necessary to make measurements on additional variables.  It is recommended that:  1. a simple pattern recognition technique  k-nearest-neighbour, linear discriminant anal-  ysis  is implemented as a baseline before considering neural network methods;  2. a simple RBF  unsupervised selection of centres, weights optimised using a squared error criterion  is tried to get a feel for whether nonlinear methods provide some gain for your problem;   200 Nonlinear discriminant analysis – kernel methods  3. a regularised solution for the weights of an RBF is used;  4. for a model that provides approximations to the posterior probabilities that enable changes of priors and costs to be incorporated into a trained model, an RBF is used;  5. knowledge of the data generation process is used, including noise on the data, for  network design or data preprocessing;  6. for classiﬁcation problems in high-dimensional spaces where training data are rep- resentative of test conditions and misclassiﬁcation rate is an acceptable measure of classiﬁer performance, support vector machines are implemented.  5.8 Notes and references  There are many developments of the techniques described in this chapter, in addition to other neural network methods. A description of these is beyond the scope of this book, but the use of neural techniques in pattern recognition and the relationship to statistical and structural pattern recognition can be found in the book by Schalkoff  1992 .  A comprehensive account of feed-forward neural networks for pattern recognition is given in the book by Bishop  1995 . Relationships to other statistical methods and other insights are described by Ripley  1996 . Tarassenko  1998  provides a basic introduc- tion to neural network methods  including radial basis functions, multilayer perceptrons, recurrent networks and unsupervised networks  with an emphasis on applications.  A good summary of Bayesian perspectives is given in the book by Ripley  1996 ; see  also Bishop  1995 ; MacKay  1995 ; Buntine and Weigend  1991 ; Thodberg  1996 .  Although many of the features of support vector machines can be found in the litera- ture of the 1960s  large margin classiﬁers, optimisation techniques and sparseness, slack variables , the basic support vector machine for non-separable data was not introduced until 1995  Cortes and Vapnik, 1995 . The book by Vapnik  1998  provides an excellent description of SVMs. A very good self-contained introduction is provided by Cristianini and Shawe-Taylor  2000 . The tutorial by Burges  1998  is an excellent concise account. A comprehensive treatment of this rapidly developing ﬁeld is provided in the recent book by Sch¨olkopf and Smola  2001 . The decomposition algorithm was suggested by Osuna et al.  1997  and the sequential minimal optimisation algorithm proposed by Platt  1998 . The website www.statistical-pattern-recognition.net contains refer-  ences and links to further information on techniques and applications.  Exercises  Data set 1: 500 samples in training, validation and test sets; p-dimensional; 3 classes; class !1 ¾ N .µ1;  cid:24 1 ; class !2 ¾ 0:5N .µ2;  cid:24 2  C 0:5N .µ3;  cid:24 3 ; class !3 ¾ 0:2N .µ4;  cid:24 4  C 0:8N .µ5;  cid:24 5 ; µ1 D . cid:9 2; 2; : : : ; 2 T , µ2 D . cid:9 4; cid:9 4; : : : ; cid:9 4 T , µ3 D .4; 4; : : : ; 4 T , µ4 D .0; 0; : : : ; 0 T , µ5 D . cid:9 4; 4; : : : ; 4 T and  cid:24 i as the identity matrix; equal class priors.   Exercises 201  Data set 2: Generate time series data according to the iterative scheme   cid:29  1  cid:9  12  2   cid:30   utC1 D 4  ut  cid:9  .2 C ¼1.1  cid:9  u2 2  cid:9  ¼1.1  cid:9  u2 t    t    ut cid:9 1  initialised with u cid:9 1 D u0 D 2. Plot the generated time series. Construct training and test sets of 500 patterns .xi ; ti   where xi D .ui ; uiC1 T and t i D uiC2. Thus the problem is one of time series prediction: predict the next sample in the series given the previous two samples. Take ¼ D 4, 1 D ³=50.  1. Compare a radial basis function classiﬁer with a k-nearest-neighbour classiﬁer. Take into account the type of classiﬁer, computational requirements on training and test operations and the properties of the classiﬁers.  2. Compare and contrast a radial basis function classiﬁer and a classiﬁer based on kernel  density estimation.  3. Implement a simple radial basis function classiﬁer: m Gaussian basis functions of width h and centres selected randomly from the data. Using data from data set 1, evaluate performance as a function of dimensionality, p, and number of basis functions, where h is chosen based on the validation set.  4. Show that by appropriate adjustment of weights and biases in an RBF that the nonlinear function  cid:1  can be normalised to lie between 0 and 1 for a ﬁnite data set, without changing the ﬁtting function. Verify for a Gaussian basis function that  cid:1  may be deﬁned as  5.22  for large h.  5. For data set 2, train an RBF for an exp. cid:9 z2  and a z2log.z  nonlinearity for varying numbers of centres. Once trained, use the RBF in a generative mode: initialise ut cid:9 1; ut  as a sample from the training set , predict utC1; then predict utC2 from ut and utC1 using the RBF. Continue for 500 samples. Plot the generated time series. Investigate the sensitivity of the ﬁnal generated time series to starting values, number of basis functions and the form of the nonlinearity.  6. Consider the optimisation criterion  5.7   E D jj. cid:9 T T C W  cid:9 T C w01T  Djj2  By solving for w0, show that this may be written T C W O cid:9   E D jj. cid:9  OT  T   Djj2  where OT and O cid:9  are zero-mean matrices. By minimising with respect to W and substi- tuting into the expression for E, show that minimising E is equivalent to maximising Tr.S B S   , where  † T  T  O cid:9  ST D 1 n O cid:9  S B D 1 n2  D2 O cid:9  D2 OT OT  T  T  D2 O cid:9    202 Nonlinear discriminant analysis – kernel methods  7. For D equal to the identity matrix in the above and a target coding scheme  .ti  k D  ² ak  0  xi 2 !k otherwise  determine the value of ak for which S B is the conventional between-class covariance matrix of the hidden output patterns.  8. Show that maximising L0 given by  5.14  is equivalent to maximising  5.12  under  the assumption that  5.13  does not depend on the model parameters.  9. Given a normal mixture model for each of C classes  see Chapter 2  with a common covariance matrix across classes, together with class priors p.!i  ; i D 1; : : : ; C, con- struct an RBF network whose outputs are proportional to the posterior probabilities of class membership. Write down the forms for the centres and weights.  10. Given  E D jj OT  cid:9  V Gjj2  in the orthogonal least squares training model, using the minimum norm solution for V and the properties of the pseudo-inverse  Appendix C  show that  E D Trf OT  T OT g  cid:9  TrfGT V T V Gg  11. Implement a support vector machine classiﬁer and assess performance using data set 1. Use a Gaussian kernel and choose a kernel width and regularisation parameter using a validation set. Investigate performance as a function of dimensionality, p. 12. Let K1 and K2 be kernels deﬁned on R P ð R p. Show that the following are also  kernels:  K .x; z  D K1.x; z  C K2.x; z  K .x; z  D aK1.x; z  K .x; z  D f .x  f .z   where a is a positive real number where f .:  is a real-valued function on x  13. For the quadratic ž-insensitive loss, the primal form of the Lagrangian is written  L p D wT w C C  .¾ 2  i C O¾ 2  i     nX iD1  and is minimised subject to the constraints  5.29 . Derive the dual form of the Lagrangian and state the constraints on the Lagrange multipliers.  14. Show that a support vector machine with a spherically symmetric kernel function satisfying Mercer’s conditions implements an RBF classiﬁer with numbers of centres and positions chosen automatically by the SVM algorithm.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   6  Nonlinear discriminant  analysis – projection methods  Overview  Nonlinear discriminant functions are constructed as sums of basis functions. These are nonlinear functions of linear projections of the data. Optimisation of an objective function is with respect to the projection directions and the basis function properties.  6.1 Introduction  In this chapter, further methods of nonlinear discrimination are explored. What makes these methods different from those in Chapter 5? Certainly, the approaches are closely re- lated – discriminant functions are constructed through the linear combination of nonlinear basis functions and are generally of the form  g.x  D mX iD1  wi  cid:4 i .x; µi     6.1   for weights wi and parameters µi of the nonlinear functions  cid:4 i . For radial basis functions, the µi represent the centres. In the support vector machine, the nonlinear functions are not deﬁned explicitly, but implicitly through the speciﬁcation of a kernel deﬁned in the data space. In this chapter, the discriminant functions are again of the form  6.1 , where the combination of x and the parameter vector µ is a scalar product, that is  cid:4 i .x; µi   D  cid:4 i .xT µi  . Two models are described. In the ﬁrst, the form of the function  cid:4  is prescribed and the optimisation procedure determines the parameters wi j and µi simultaneously. In the second, the form of  cid:4  is learned, in addition to the projection directions µi , and optimisation proceeds sequentially.   204 Nonlinear discriminant analysis – projection methods  6.2 The multilayer perceptron  6.2.1 Introduction  The multilayer perceptron  MLP  is yet another tool in the pattern recognition toolbox, and one that has enjoyed considerable use in recent years. It was presented by Rumelhart et al.  1986  as a potential answer to the problems associated with the perceptron, and since that time it has been extensively employed in many pattern recognition tasks. In order to introduce some terminology, let us consider a simple model. We shall then consider some generalisations. The basic MLP produces a transformation of a pattern x 2 R p to an n0-dimensional space according to  g j .x  D mX iD1  w ji  cid:4 i .αT  i x C Þi0  C w j0;  j D 1; : : : ; n0  The functions  cid:4 i are ﬁxed nonlinearities, usually identical and taken to be of the logistic form representing historically the mean ﬁring rate of a neuron as a function of the input current,   cid:4 i .z  D  cid:4  .z  D  1  1 C exp. cid:5 z   Thus, the transformation consists of projecting the data onto each of m directions de- scribed by the vectors αi ; then transforming the projected data  offset by a bias Þi0  by the nonlinear functions  cid:4 i .z ; and ﬁnally, forming a linear combination using the weights w ji .  The MLP is often presented in diagrammatic form  see Figure 6.1 . The input nodes accept the data vector or pattern. There are weights associated with the links between the i xCÞi0 and input nodes and the hidden nodes that accept the weighted combination z D α T perform the nonlinear transformation  cid:4  .z . The output nodes take a linear combination of the outputs of the hidden nodes and deliver these as outputs. In principle, there may   6.2    6.3   x   cid:3  cid:4   Þi j   cid:12  cid:12  cid:12  cid:12  cid:12  cid:13   cid:3  cid:3  cid:3  cid:3  cid:3  cid:4   cid:3  cid:3  cid:3  cid:3  cid:3  cid:4  cid:5    cid:6   cid:8   cid:6   cid:6  cid:7   cid:8   cid:6   cid:10  cid:10  cid:11   cid:1  cid:2   cid:8   cid:6   cid:1   cid:8  cid:8  cid:9   cid:10   cid:6  cid:7   cid:1   cid:10   cid:1  cid:2   cid:10   cid:1   cid:1    cid:1  cid:2   cid:8   cid:6   cid:3  cid:4   cid:8   cid:6   cid:8   cid:1  cid:2   cid:6   cid:6    cid:5    cid:3  cid:4    cid:1  cid:2   cid:1   cid:3  cid:4   cid:1   cid:10   cid:1  cid:2   cid:10   cid:1   cid:3  cid:4   cid:10   cid:1   cid:1  cid:2   wi j   cid:1  cid:2   cid:10  cid:10  cid:11   cid:1   cid:10   cid:1   cid:10   cid:1  cid:2   cid:1   cid:10   cid:1    cid:1   cid:3  cid:4   cid:1   cid:10   cid:1  cid:2   cid:6   cid:10   cid:1   cid:3  cid:4   cid:6   cid:1   cid:10   cid:1  cid:2    cid:3  cid:3  cid:3  cid:3  cid:3  cid:4   cid:12  cid:12  cid:12  cid:12  cid:12  cid:13   cid:12  cid:12  cid:12  cid:12  cid:12  cid:13    cid:5    cid:6    cid:6  cid:7    cid:6    cid:3  cid:4    cid:1  cid:2   cid:3  cid:4    cid:1  cid:2    cid:5    cid:3  cid:4    cid:1  cid:2   cid:3  cid:4    cid:1  cid:2   input layer  hidden layer  output layer  Figure 6.1 Multilayer perceptron   The multilayer perceptron 205  be many hidden layers, each one performing a transformation of a scalar product of the outputs of the previous layer and the vector of weights linking the previous layer to a given node. Also, there may be nonlinearities associated with the output nodes that, instead of producing a linear combination of weights wi j and hidden unit outputs, perform nonlinear transformations of this value.  The emphasis of the treatment of MLPs in this section is on MLPs with a single hidden layer. Further, it is assumed either that the ‘outputs’ are a linear combination of the functions  cid:4 i or at least that, in a discrimination problem, discrimination may be performed by taking a linear combination of the functions  cid:4 i  a logistic discrimination model is of this type . There is some justiﬁcation for using a single hidden layer model in that it has been shown that an MLP with a single hidden layer can approximate an arbitrary  continuous bounded integrable  function arbitrarily accurately  Hornik, 1993; see Section 6.2.4 . Also, practical experience has shown that very good results can be achieved with a single hidden layer on many problems. There may be practical reasons for considering more than one hidden layer, and it is conceptually straightforward to extend the analysis presented here to do so.  The MLP is a nonlinear model: the output is a nonlinear function of its parame- ters and the inputs, and a nonlinear optimisation scheme must be employed to min- imise the selected optimisation criterion. Therefore, all that can be hoped for is a local extremum of the criterion function that is being optimised. This may give satisfactory performance, but several solutions may have to be sought before an acceptable one is found.  6.2.2 Specifying the multilayer perceptron structure  To specify the network structure we must prescribe the number of hidden layers, the number of nonlinear functions within each layer and the form of the nonlinear functions. Most of the MLP networks to be found in the literature consist of layers of logistic processing units  6.3 , with each unit connected to every unit in the previous layer  fully layer connected  and no connections between units in non-adjacent layers.  It should be noted, however, that many examples exist of MLP networks that are not fully layer connected. There has been some success with networks in which each processing unit is connected to only a small subset of the units in the previous layer. The units chosen are often part of a neighbourhood, especially if the input is some kind of image. In even more complex implementations the weights connected to units that examine similar neighbourhoods at different locations may be forced to be identical  shared weights . Such advanced MLP networks, although of great practical importance, are not universally applicable and are not considered further here.  6.2.3 Determining the multilayer perceptron weights  There are two stages to optimisation. The ﬁrst is the initialisation of the parameter values; the second is the implementation of a nonlinear optimisation scheme.   206 Nonlinear discriminant analysis – projection methods  Weight initialisation There are several schemes for initialising the weights. The weights of the MLP are often started at small random values. In addition, some work has been carried out to investigate the beneﬁts of starting the weights at values obtained from simple heuristics.  1. Random initialisation For random initialisation, the weights are initialised with values drawn from a uniform distribution over [ cid:5 1; 1], where 1 depends on the scale of the values taken by the data. If all the variables are equally important and the sample variance is ¦ 2, then 1 D 1=. p¦   is a reasonable choice   p variables . Hush et al.  1992  assess several weight initialisation schemes and support initialisation to small random values.  2. Pattern classiﬁer initialisation An alternative approach is to initialise the MLP to perform as a nearest class mean or nearest-neighbour classiﬁer  Bedworth, 1988 . As we saw in Chapter 3, for the nearest-neighbour classiﬁer the decision surface is piecewise linear and is composed of segments of the perpendicular bisectors of the prototype vectors with dissimilar class labels. This decision surface can be implemented in an MLP with one hidden layer of scalar-product logistic processing units. If the decision regions are convex  as for a class mean classiﬁer , then the structure of the network is particularly simple. There have been other studies that use various pattern recognition schemes for initialisation. For example, Weymaere and Martens  1994  propose a network initialisation procedure that is based on k-means clustering  see Chapter 10  and nearest- neighbour classiﬁcation. Brent  1991  uses decision trees  see Chapter 7  for initialisation. Of course, given good starting values, training time is reduced, albeit at the expense of increased initialisation times.  3. Independence model initialisation An approach that initialises an MLP to deliver class-conditional probabilities under the assumption of independence of variables is des- cribed in Lowe and Webb  1990 . This really only applies to categorical variables in which the data can be represented as binary patterns.  Optimisation Many different optimisation criteria and many nonlinear optimisation schemes have been considered for the MLP. It would not be an exaggeration to say that out of all the classiﬁcation techniques considered in this book, the MLP is the one which has been explored more than any other in recent years, particularly in the engineering literature, sometimes by researchers who do not have a real application, but whose imagination has been stimulated by the ‘neural network’ aspect of the MLP. It would be impossible to offer anything but a brief introduction to optimisation schemes for the MLP.  Most optimisation schemes involve the evaluation of a function and its derivatives with respect to a set of parameters. Here, the parameters are the multilayer perceptron weights and the function is a chosen error criterion. We shall consider two error criteria discussed earlier: one based on least squares minimisation and the other on a logistic discrimination model.   Least squares error minimisation The error to be minimised is the average squared distance between the approximation given by the model and the ‘desired’ value:  The multilayer perceptron 207  where g.xi   is the vector of ‘outputs’ and ti is the desired pattern  sometimes termed the target  for data sample xi . In a regression problem, t i are the dependent variables; in a discrimination problem, ti D .ti1; : : : ; tiC  T are the class labels usually coded as  E D nX iD1  jt i  cid:5  g.xi  j2  ti j D  ² 1 0  if xi is in class ! j otherwise  Modiﬁcations of the error criterion to take into account the effect of priors and alternative target codings to incorporate costs of misclassiﬁcation are described in Section 5.2.1.  Most of the nonlinear optimisation schemes used for the MLP require the error and  its derivative to be evaluated for a given set of weight values.  The derivative of the error with respect to a weight v  which at the moment can represent either a weight Þ, between inputs and hidden units, or a weight w, between the hidden units and the outputs – see Figure 6.1  can be expressed as1  The derivative of gl with respect to v, for v one of the weights w, v D w jk say, is  and for the weights Þ, v D Þ jk,  @ E @v  D  cid:5 2  nX iD1  n0X lD1  .t i  cid:5  g.xi   l  @gl .xi    @v  @gl @w jk  D  ² Žl j Žl j  cid:4 k.αT  k xi C Þk0   k D 0 k 6D 0  D  @gl @Þ jk  8>< >:  wl j  wl j  @ cid:4  j @z @ cid:4  j @z  .xi  k  k D 0 k 6D 0  D  cid:4  .z .1  cid:5   cid:4  .z    @ cid:4  @z  where @ cid:4 i =@z is the derivative of  cid:4  with respect to its argument, given by  for the logistic form  6.3  and evaluated at z D α T j xi C Þ j0. Equations  6.5 ,  6.6  and  6.7  can be combined to give expressions for the derivatives of the squared error with respect to the weights w and Þ.  1Again, we use n0 to denote the output dimensionality; in a classiﬁcation problem, n0 D C, the number of  classes.   6.4    6.5    6.6    6.7    208 Nonlinear discriminant analysis – projection methods  layer o  cid:5  2   cid:3  cid:4   layer o  cid:5  1  wo ji   cid:1  cid:2   cid:10  cid:10  cid:11   cid:1   cid:10   cid:1   cid:10   cid:1  cid:2   cid:1   cid:10   cid:1    cid:1   cid:3  cid:4   cid:1   cid:10   cid:1  cid:2   cid:6   cid:1   cid:10   cid:3  cid:4   cid:6   cid:1   cid:10   cid:1  cid:2    cid:3  cid:3  cid:3  cid:3  cid:3  cid:4   cid:12  cid:12  cid:12  cid:12  cid:12  cid:13   cid:12  cid:12  cid:12  cid:12  cid:12  cid:13    cid:5    cid:6  cid:7    cid:6    cid:6   layer o  cid:3  cid:4    cid:1  cid:2   cid:3  cid:4    cid:1  cid:2    cid:5    cid:3  cid:4    cid:1  cid:2   cid:3  cid:4    cid:1  cid:2   wo cid:5 1 ji   cid:12  cid:12  cid:12  cid:12  cid:12  cid:13   cid:3  cid:3  cid:3  cid:3  cid:3  cid:4   cid:3  cid:3  cid:3  cid:3  cid:3  cid:4  cid:5    cid:6   cid:6   cid:8   cid:6  cid:7   cid:8   cid:6   cid:10  cid:10  cid:11   cid:1  cid:2   cid:8   cid:6   cid:1   cid:8  cid:8  cid:9   cid:10   cid:6  cid:7   cid:1   cid:10   cid:1  cid:2   cid:10   cid:1   cid:1    cid:1  cid:2   cid:6   cid:8   cid:3  cid:4   cid:6   cid:8   cid:8   cid:1  cid:2   cid:6   cid:6    cid:5    cid:3  cid:4    cid:1  cid:2   cid:1   cid:3  cid:4   cid:1   cid:10   cid:1  cid:2   cid:10   cid:1   cid:3  cid:4   cid:10   cid:1   cid:1  cid:2  zo cid:5 2  j  ao cid:5 1  j  zo cid:5 1  j  ao j  Figure 6.2 Notation used for back-propagation  Back-propagation In the above example, we explicitly calculated the derivatives for a single hidden layer network. Here we consider a more general treatment. Back- propagation is the term given to efﬁcient calculation of the derivative of an error function in multilayer networks. We write the error, E, as  E D nX kD1  E k  E k D jt k  cid:5  g.xk  j2  ji zo cid:5 1 wo  i  ao  j D no cid:5 1X iD1 zo cid:5 1 j D  cid:4  .ao cid:5 1  j     where E k is the contribution to the error from pattern k. For example,  in  6.4 . We refer to Figure 6.2 for general notation: let the weights between layer o  cid:5  1 ji  the weight connecting the ith node of layer o  cid:5  1 to the jth node of and layer o be wo layer o ; let ao j be the outputs of the nonlinearity, so that  j be the inputs to the nonlinearity at layer o and zo   6.8   where no cid:5 1 is the number of nodes at layer o  cid:5  1 and  cid:4  is the  assumed common  nonlinearity associated with a node.2 These quantities are calculated during the process termed forward propagation: namely the calculation of the error, E, from its constituent parts, E k.  Let layer o be the ﬁnal layer of the network. The term E k is a function of the inputs  to the ﬁnal layer,  E k D E k .ao  ; : : : ; ao no     1  2Strictly, the terms ao  j and zo cid:5 1  j  should be subscripted by k since they depend on the kth pattern.   For example, with a network with linear output units and using a squared error criterion  The multilayer perceptron 209  where .t k  j is the jth component of the kth target vector.  The derivatives of the error E are simply the sums of the derivatives of E k, the contribution to the error by the kth pattern. The derivatives of E k with respect to the ﬁnal layer of weights, wo  ji , are given by  E k D noX jD1  ..t k   j  cid:5  ao  j   2  @ E k @wo ji  D @ E k @ao j  @ao j @wo ji  D Žo  j zo cid:5 1  i  where we use the convenient shorthand notation Žo with respect to the input of a particular node in the network. The derivatives of E k with respect to the weights wo cid:5 1  are given by  ji  j to denote @ E k =@ao  j , the derivative  where we have again used the notation Žo cid:5 1 using the chain rule for differentiation as  j  for @ E k =@ao cid:5 1  j  . This may be expanded  @ E k @wo cid:5 1 ji  D @ E k @ao cid:5 1  j  j  @ao cid:5 1 @wo cid:5 1 ji  D Žo cid:5 1  zo cid:5 2  j  i  j D noX Žo cid:5 1 lD1  @ E k @ao l  @ao l @ao cid:5 1  j  D noX lD1  Žo l  @ao l @ao cid:5 1  j  and using the relationships  6.8 , we have  @ao l @ao cid:5 1  j  D wo  l j   cid:4 0.ao cid:5 1     j  giving the back-propagation formula Žo cid:5 1 j D  cid:4 0.ao cid:5 1  j   X  l  Žo l  wo l j  The above result allows the derivatives of E k with respect to the inputs to a particular node to be expressed in terms of derivatives with respect to the inputs to nodes higher up the network, that is, nodes in layers closer to the output layer. Once calculated, these are combined with node outputs in equations of the form  6.9  and  6.10  to give derivatives with respect to the weights.  Equation  6.11  requires the derivatives with respect to the nonlinearities  cid:4  to be  speciﬁed. For  cid:4  given by  6.3 ,   cid:4 0.a  D  cid:4  .a .1  cid:5   cid:4  .a     6.12    6.9    6.10    6.11    210 Nonlinear discriminant analysis – projection methods  The derivative calculation also requires the initialisation of Žo  kth component of the error with respect to ao  j , the derivative of the j . For the sum-squared error criterion, this is  j D  cid:5 2..t k   j  cid:5  ao Žo  j      6.13   Each layer in the network, up to the output layer, may have biases: the terms Þi0 and w j0 in the single hidden layer model of  6.2 . A bias node has  cid:4  .:   cid:8  1 and in the general back-propagation scheme, and at a given layer  say o  cid:5  1 ,  j  @ao cid:5 1 @wo cid:5 1 j0  D 1  The above scheme is applied recursively to each layer to calculate derivatives. The term back-propagation networks is often used to describe multilayer perceptrons employ- ing such a calculation, though strictly back-propagation refers to the method of derivative calculation, rather than the type of network.  Logistic classiﬁcation We now consider an alternative error criterion based on the gen- eralised logistic model. The basic assumption for the generalised logistic discrimination for an MLP model is   cid:8    cid:7  p.xj! j   p.xj!C    D mX iD1  log  w ji  cid:4 i .ai   C w j0;  j D 1; : : : ; C  cid:5  1  where  cid:4 i is the ith nonlinearity  logistic function  and ai represents the input to the ith nonlinearity. The terms ai may be linear combinations of the outputs of a previous layer in a layered network system and depend on the parameters of a network. The posterior probabilities are given by  5.11   cid:9 Pm w ji  cid:4 i .ai   C w0 iD1 sD1 exp cid:12 Pm iD1 1 sD1 exp cid:12 Pm iD1 j0 D w j0 C log. p.! j  = p.!C   .  exp 1 C PC cid:5 1  j D 1; : : : ; C  cid:5  1  wsi  cid:4 i .ai   C w0  wsi  cid:4 i .ai   C w0  1 C PC cid:5 1  p.!Cjx  D  p.! jjx  D  where w0   6.14   Ð ;   cid:11   Ð  s0  s0  j0  Discrimination is based on the generalised linear model  g j .x  D mX iD1  w ji  cid:4 i .ai   C w0  ;  j0  j D 1; : : : ; C  cid:5  1  with decision rule:  assign x to class ! j if  max  sD1;:::;C cid:5 1  else assign x to class C.  mX iD1  wsi  cid:4 i .ai   C w0  w ji  cid:4 i .ai   C w0  > 0  j0  s0 D mX iD1   The multilayer perceptron 211  In terms of the MLP model  Figure 6.2 , equation  6.14  can be thought of as a ﬁnal normalising layer in which w ji are the ﬁnal layer weights and the set of C in- puts to the ﬁnal layer  terms of the form P w ji  cid:4 i C w j0; j D 1; : : : ; C  is offset by prior terms log. p.! j  = p.!C    and normalised through  6.14  to give outputs p.! jjx ; j D 1; : : : ; C.  i  Estimates of the parameters of the model may be achieved by maximising the log-  likelihood, under a mixture sampling scheme given by  see Section 5.2.2   where the posterior probability estimates are of the form  6.14 , or equivalently by minimising  where t i is a target vector for xi : a vector of zeros, except that there is a 1 in the position corresponding to the class of xi . The above criterion is of the form P  i E i , where  For the back-propagation algorithm, we require Žo  j , j D 1; : : : ; C  cid:5  1. This is given by  CX kD1  X xi2!k  log. p.!kjxi      cid:5  nX iD1  CX kD1  .ti  k log. p.!kjxi     E i D  cid:5  CX kD1  .ti  klog. p.!kjxi     j D @ E i Žo @ao j  D  cid:5 .ti   j C p.! jjxi  ;   6.15    6.16   the difference between the jth component of the target vector and the jth output for pattern i  compare with  6.13 ; see the exercises .  Now that we can evaluate the error and its derivatives, we can, in principle, use one of many nonlinear optimisation schemes available  Press et al., 1992 . We have found that the conjugate gradients algorithm with the Polak–Ribi`ere update scheme works well on many practical problems. For problems with a small number of parameters  less than about 250 , the Broyden–Fletcher–Goldfarb–Shanno optimisation scheme is recommended  Webb et al., 1988; Webb and Lowe, 1988 . Storage of the inverse Hessian matrix  n p ð n p, where n p is the number of parameters  limits its use in practice for large networks. For further details of optimisation algorithms see, for example, van der Smagt  1994  and Karayiannis and Venetsanopolous  1993 .  Most algorithms for determining the weights of an MLP do not necessarily make direct use of the fact that the discriminant function is linear in the ﬁnal layer of weights w and they solve for all the weights using a nonlinear optimisation scheme. Another approach is to alternate between solving for the weights w  using a linear pseudo-inverse method  and adjusting the parameters of the nonlinear functions  cid:4 , namely Þ. This is akin to the alternating least squares approach in projection pursuit and is equivalent to regarding the ﬁnal layer weights as a function of the parameters Þ  Webb and Lowe, 1988; see also St¨ager and Agarwal, 1997 .   212 Nonlinear discriminant analysis – projection methods  Stopping criterion The most common stopping criterion used in the nonlinear opti- misation schemes is to terminate the algorithm when the relative change in the error is less than a speciﬁed amount  or the maximum number of allowed iterations has been exceeded .  An alternative that has been employed for classiﬁcation problems is to cease training when the classiﬁcation error  either on the training set or preferably a separate validation set  stops decreasing  see Chapter 11 for model selection . Another strategy is based on growing and pruning in which a network larger than necessary is trained and parts that are not needed are removed. See Reed  1993  for a survey of pruning algorithms.  Training strategies There are several training strategies that may be used to minimise the error. The one that we have described above uses the complete set of patterns to calculate the error and its derivatives with respect to the weights, w. These may then be used in a nonlinear optimisation scheme to ﬁnd the values for which the error is a minimum.  A different approach that has been commonly used is to update the parameters using the gradients @ En=@w. That is, calculate the contribution to the derivative of the error due to each pattern and use that in the optimisation algorithm. In practice, although the total error will decrease, it will not converge to a minimum, but tend to ﬂuctuate around the local minimum. A stochastic update scheme that also uses a single pattern at a time will converge to a minimum, by ensuring that the inﬂuence of each gradient calculation decreases appropriately with iteration number.  Finally, incremental training is a heuristic technique for speeding up the overall learn- ing time of neural networks whilst simultaneously improving the ﬁnal classiﬁcation per- formance. The method is simple: ﬁrst the network is partially trained on a subset of the training data  there is no need to train to completion  and then the resulting network is further tuned using the entire training database. The motivation is that the subset training will perform a coarse search of weight space to ﬁnd a region that ‘solves’ the initial problem. The hope is that this region will be a useful place to start for training on the full data set. The technique is often used with a small subset used for initial training and progressing through larger and larger training databases as the performance increases. The number of patterns used in each subset will vary according to the task although one should ensure that sufﬁcient patterns representative of the data distribution are present to prevent the network over-ﬁtting the subset data. Thus, in a discrimination problem, there should be samples from all classes in the initial training subset.  6.2.4 Properties  If we are free to choose the weights and nonlinear functions, then a single-layer MLP can approximate any continuous function to any degree of accuracy if and only if the nonlinear function is not a polynomial  Leshno et al., 1993 . Since such networks are simulated on a computer, the nonlinear function of the hidden nodes must be expressed as a ﬁnite polynomial. Thus, they are not capable of universal approximation  Wray and Green, 1995 . However, for most practical purposes, the lack of a universal approximation property is irrelevant.   The multilayer perceptron 213  Classiﬁcation properties of MLPs are addressed by Farag´o and Lugosi  1993 . Let LŁ be the Bayes error rate  see Chapter 8 ; let gkn be an MLP with one hidden layer of k nodes  with step function nonlinearities  trained on n samples to minimise the number of errors on the training data  with error probability L.gkn  ; then, provided k is chosen so that  as n ! 1, then  k ! 1 n ! 0 k log.n   n!1 L.gkn  D LŁ  lim  with probability 1. Thus, the classiﬁcation error approaches the Bayes error as the number of training samples becomes large, provided k is chosen to satisfy the conditions above. However, although this result is attractive, the problem of choosing the parameters of gkn to give minimum errors on a training set is computationally difﬁcult.  6.2.5 Example application study  The problem The classiﬁcation of radar clutter in a target recognition problem  Blacknell and White, 1994 .  Summary A prerequisite for target detection in synthetic aperture radar imagery is the ability to classify background clutter in an optimal manner. Radar clutter is the unwanted return from the background  ﬁelds, woods, buildings , although in remote sensing land- use applications, the natural vegetation forms the wanted return.  The aim of the study is to investigate how well neural networks can approximate an optimum  Bayesian  classiﬁcation. What form of preprocessing should be performed prior to network training? How should the network be constructed?  The data In terms of synthetic aperture radar imagery, a correlated K distribution pro- vides a reasonable description of natural clutter textures arising from ﬁelds and woods. However, an analytical expression for correlated multivariate K distributions is not avail- able. Therefore, the approach taken in this work is to develop methodology on simulated uncorrelated K -distributed data, before application to correlated data.  The form of the multivariate, uncorrelated K distribution is  x D .x1; : : : ; x p T    p.xj¼; ¹  D pY iD1  2  xi 0.¹    cid:8 .¹C1 =2   cid:7  ¹xi  ¼     s cid:7  ¹xi   cid:8    K¹ cid:5 1  2  ¼  where ¼ is the mean, ¹ is the order parameter and K¹ cid:5 1 is the modiﬁed Bessel function of order ¹  cid:5  1. Data for two 256 ð 256 images were generated, each image with the same mean value but different order parameters. From each image 16-dimensional pattern vectors were extracted. These comprised measurements on 4 ð 4 non-overlapping win- dows.  A second experiment used 2 ð 1 windows.  Several discriminant functions were constructed, including an optimum Bayesian scheme, and evaluated on this two-class problem.   214 Nonlinear discriminant analysis – projection methods  The model The neural network scheme was developed comprising an MLP with 100 hidden nodes, and a squared error objective function was used.  Training procedure Some simple preprocessing of the data  taking logarithms  was implemented to reduce network training times. Weights were optimised using gradient descent. The MLP error rate was 18.5% on the two-pixel data and 23.5% on the 16-pixel data  different order parameters , compared with optimum values of 16.9% and 20.9%. Further improvements in MLP performance  reducing the two-pixel error rate to 17.0%  were reported through the use of a simple factorisation scheme. Separate MLPs were designed for different regions of the data space, these regions being deﬁned by the original MLP classiﬁcation.  Results Preliminary results on correlated data showed that classiﬁer combination  see Chapter 8  can improve performance.  Thus, careful network design and using knowledge of the data distribution to pre- process the data can lead to signiﬁcant improvements in performance over the na¨ıve implementation of an MLP.  6.2.6 Further developments  One important development is the application of Bayesian inference techniques to the ﬁtting of neural network models, which has been shown to lead to practical beneﬁts on real problems. In the predictive approach  Ripley, 1996 , the posterior distribution of the observed target value  t  given an input vector  x  and the data set  D , p.tjx;D , is obtained by integrating over the posterior distribution of network weights, w,  p.tjx;D  D  p.tjx; w  p.wjD dw   6.17  where p.wjD  is the posterior distribution for w and p.tjx; w  is the distribution of outputs for the given model, w, and input, x. For a data set D D f.xi ; ti  ; i D 1; : : : ; ng of measurement vectors xi with associated targets t i , the posterior of the weights w may be written  Z  p.wjD  D p.w;D  p.D   D 1 p.D   nY iD1  p.t ijxi ; w  p.xijw  p.w   assuming independent samples. Further, if we assume that p.xijw  does not depend on w, then  The maximum a posteriori  MAP  estimate of w is that for which p.wjD  is a maximum. Assuming a prior distribution,  p.wjD    nY iD1  p.t ijxi ; w  p.w   p.w    exp   cid:9  cid:5  Þ  jjwjj2 cid:11   2   The multilayer perceptron 215  for parameter Þ, and a zero-mean Gaussian noise mode so that  cid:8  jt  cid:5  g.xI w j2  p.tjx; w    exp   cid:7   cid:5  þ 2  for the diagonal covariance matrix .1=þ I and network output g.x; w , then  p.wjD    exp     cid:5  þ 2  X  i  jt i  cid:5  g.xi ; w j2  cid:5  Þ 2  jjwjj2  !  and the MAP estimate is that for which  S.w   4D X  i  jt i  cid:5  g.xi ; w j2 C Þ  jjwjj2  þ   6.18   is a minimum. This is the regularised solution  equation  5.9   derived as a solution for the MAP estimate for the parameters w.  Equation  6.18  is not a simple function of w and may have many local minima  many local peaks of the posterior density , and the integral in  6.17  is computationally difﬁcult to evaluate. Bishop  1995  approximates S.w  using a Taylor expansion around its minimum value, wMAP  although, as we have noted, there may be many local minima ,  S.w  D S.wMAP  C 1  .w  cid:5  wMAP T A.w  cid:5  wMAP   2  where A is the Hessian matrix  Ai j D @ @wi  @ @w j  S.w   þþþþwDwMAP  to give  p.wjD    exp  .w  cid:5  wMAP T A.w  cid:5  wMAP   ¦  ²  cid:5  þ 2  Also, expanding g.x; w  around wMAP  assuming for simplicity a scalar quantity ,  g.x; w  D g.x; wMAP  C .w  cid:5  wMAP T h  where h is the gradient vector, evaluated at wMAP, gives  Z  ²  cid:5  þ 2  p.tjx;D     1wT A1w where 1w D .w  cid:5  wMAP . This may be evaluated to give  Bishop, 1995   [t  cid:5  g.x; wMAP   cid:5  1wT h]2  cid:5  þ 2  exp  ¦  p.tjx;D  D  1 .2³ ¦ 2 t    1 2  exp  ²  cid:5  1 2¦ 2 t  .t  cid:5  g.x; wMAP  2  ¦  where the variance ¦ 2 t  is given by  t D 1 ¦ 2  þ  .1 C hT A   cid:5 1h   dw   6.19    6.20    6.21   Equation  6.20  describes the distribution of output values for a given input value, x, and given the data set, with the expression above providing error bars on the estimate. Further discussion of the Bayesian approach is given by Bishop  1993  and Ripley  1996 .   216 Nonlinear discriminant analysis – projection methods  6.2.7 Summary  The multilayer perceptron is a model that, in its simplest form, can be regarded as a generalised linear discriminant function in which the nonlinear functions  cid:4  are ﬂexible and adapt to the data. This is the way that we have chosen to introduce it in this chapter as it forms a natural progression from linear discriminant analysis, through generalised linear discriminant functions with ﬁxed nonlinearities to the MLP. It is related to the projection pursuit model described in the next section in that data are projected onto different axes, but unlike the projection pursuit model the nonlinear function does not adapt; it is usually a ﬁxed logistic nonlinearity. The parameters of the model are determined through a nonlinear optimisation scheme, which is one of the drawbacks of the model. Computation time may be excessive.  There have been many assessments of gradient-based optimisation algorithms, vari- ants and alternatives. Webb et al.  1988; see also Webb and Lowe, 1988  compared various gradient-based schemes on several problems. They found that the Levenberg– Marquardt optimisation scheme gave best overall performance for networks with a small number of parameters, but favoured conjugate gradient methods for networks with a large number of parameters. Further comparative studies include those of Karayiannis and Venetsanopolous  1993 , van der Smagt  1994  and St¨ager and Agarwal  1997 . The addition of extra terms to the error involving derivatives of the error with respect to the input has been considered by Drucker and Le Cun  1992  as a means of improving generalisation  see also Bishop, 1993; Webb, 1994 . The latter approach of Webb was motivated from an error-in-variables perspective  noise on the inputs . The addition of noise to the inputs as a means of improving generalisation has also been assessed by Holmstr¨om and Koistinen  1992  and Matsuoka  1992 .  The MLP is a very ﬂexible model, giving good performance on a wide range of problems in discrimination and regression. We have presented only a very basic model. There are many variants, some adapted to particular types of problem such as time series  for example, time-delay neural networks . Growing and pruning algorithms for MLP construction have also been considered in the literature, as well as the introduction of regularisation in the optimisation criteria in order to prevent over-ﬁtting of the data. The implementation in hardware for some applications has also received attention. There are several commercial products available for MLP design and implementation.  6.3 Projection pursuit  6.3.1 Introduction  Projection pursuit is a term used to describe a technique for ﬁnding ‘interesting’ projec- tions of data. It has been used for exploratory data analysis, density estimation and in multiple regression problems  Friedman and Tukey, 1974; Friedman and Stuetzle, 1981; Friedman et al., 1984; Friedman, 1987; Huber, 1985; Jones and Sibson, 1987 . It would therefore be at home in several of the chapters of this book.  One of the advantages of projection pursuit is that it is appropriate for sparse data sets in high-dimensional spaces. That is, in situations in which the sample size is too small   Projection pursuit 217  to make kernel density estimates reliable. A disadvantage is that ‘projection pursuit will uncover not only true but also spurious structure’  Huber, 1985 .  The basic approach in projection pursuit regression is to model the regression surface  as a sum of nonlinear functions of linear combinations of the variables  y D mX jD1   cid:4  j .β T  j x    6.22   where the parameters β j ; j D 1; : : : ; m, and the function  cid:4  j are determined from the data and y is the response variable. This is very similar to the multilayer perceptron model described in Section 6.2 where the  cid:4  j are usually ﬁxed as logistic functions.  Determination of the parameters β j is achieved by optimising a ﬁgure of merit or criterion of ﬁt, I .β , in an iterative fashion. The steps in the optimisation procedure are as follows  Friedman and Stuetzle, 1981; Jones and Sibson, 1987 . 1. Initialise m D 1; initialise residuals ri D yi 2. Initialise β and project the independent variable x onto one dimension zi D β T xi . 3. Calculate a smooth representation  cid:4 þ .z  of the current residuals  univariate nonpara-  4D y.xi  .  metric regression of current residuals on zs .  4. Calculate the ﬁgure of merit; for example, the fraction of unexplained variance for  the linear combination β,  I .β  D 1  cid:5  nX iD1  .ri  cid:5   cid:4 þ .β T xi   2   cid:23  nX iD1  r 2 i  5. Find the vector βm that maximises I .β  and the corresponding  cid:4 þm  .β T  m xi  .  6. Recalculate the ﬁgure of merit. Terminate if the ﬁgure of merit is less than a speci- ﬁed threshold  or the relative change in the ﬁgure of merit is less than a threshold  otherwise set m D m C 1, update the residuals ri D ri  cid:5   cid:4 þm  .β T  mxi    and go to step 2.  Step 3 requires a smoothing procedure to be implemented. There are many procedures for smoothing, including running means, kernel smoothers, median ﬁlters and regression splines  see also Hastie and Tibshirani, 1990 . Green and Silverman  1994  describe efﬁcient algorithms based on smoothing splines. Step 5 requires a search to ﬁnd the minimum of the ﬁgure of merit. Jones and Sibson use an entropy projection index, with density calculated using a kernel density estimate. A steepest descent gradient method is used to determine β. Alternatively, procedures that do not require gradient information may be employed, for example a multidimensional simplex method  Press et al., 1992 . This procedure builds up a sequence of coefﬁcient vectors βi . The projection pur- suit procedure can also be implemented with readjustment of the previously determined coefﬁcient vectors  termed back-ﬁtting by Friedman and Stuetzle, 1981 .   218 Nonlinear discriminant analysis – projection methods  6.3.2 Projection pursuit for discrimination  The use of projection pursuit for discrimination has not been widely studied in the pattern recognition literature, but it is a straightforward development of the regression model given above.  We seek a set of functions φ j and a set of directions β j such that  þþþþþti  cid:5  mX  jD1  nX iD1  φ j .β T  j xi    2  þþþþþ   6.23   is minimised, where t i D .ti1; : : : ; tiC  T are the class indicators  ti j D 1 for xi 2 ! j , ti j D 0 otherwise  and we have taken m terms in the additive model  Hastie and Tibshirani, 1990 . We know that the solution of  6.23  gives asymptotically a minimum variance approximation to the Bayes optimal discriminant function. Thus the function  g.x  D mX jD1  φ j .β T  j x   is our discriminant function.  In  6.23 , there is a set of C nonlinear functions  the elements of the C-dimensional j D 1; : : : ; m. An alternative vector φ j   to estimate for each projection direction β j , approach is to use common basis functions for each class and write φ j D γ j  cid:4  j , for a vector γ j D . cid:21  j1; : : : ;  cid:21  jC  T ; that is, for a given component j, the form of the nonlinear functions is the same for each response variable, but it has different weights  cid:21  jk ; k D 1; : : : ; C. Bias terms are also introduced and we minimise I deﬁned by  I  4D nX iD1  þþþþþt i  cid:5  γ 0  cid:5  mX  jD1  γ j  cid:4  j .β T  j xi    2  þþþþþ  with respect to γ 0; γ j , the projection directions β j . j D 1; : : : ; m  and the functions  cid:4  j . The basic strategy is to proceed incrementally, beginning with m D 1 and increasing the number of basis functions one at a time until I is less than some threshold. An alternating least squares procedure is adopted: alternately minimising with respect to βm and  cid:4 m for ﬁxed γ j ; j D 0; : : : ; m  and ﬁxed β j ,  cid:4  j ; j D 1; : : : ; m  cid:5  1 – determined on previous iterations , and then minimising with respect to γ j . j D 0; : : : ; m  for ﬁxed β j ,  cid:4  j ; j D 1; : : : ; m. Speciﬁcally: 1. Set m D 1. γ 0 D 1 2. At the mth stage, set  iD1 ti .  Pn  n  ri D t i  cid:5  γ 0  cid:5  m cid:5 1X  jD1  γ j  cid:4  j .β T  j xi    and initialise  cid:4 m and γ m.   Projection pursuit 219  3. Find the direction Oβm that minimises I D nX iD1  jri  cid:5  γ m  cid:4 m .β T  mxi  j2   6.24   Set βm D Oβm. Estimate O cid:4 m by smoothing .β T repeat this step several times.  m xi ; γ T  m ri =jγ mj2 . Set  cid:4 m D O cid:4 m and  4. Find the Oγ m that minimises  6.24 . Set γ m D Oγ m. 5. Repeat steps 3 and 4 until the loss function is minimised with respect to γ 0; γ j β j ; . j D 1; : : : ; m  and the functions  cid:4  j . If I is not less than some prespeciﬁed threshold, then set m D m C 1 and go to step 2.  Hwang et al.  1994b  discuss smoothers for determining  cid:4 m in step 3 and implement a parametric smoother based on Hermite polynomials:  H0.z  D 1 H1.z  D 2z Hr .z  D 2.zHr cid:5 1.z   cid:5  .r  cid:5  1 Hr cid:5 2.z    The projection directions, β j , are determined through the use of a Newton–Raphson method  Press et al., 1992  for solving systems of nonlinear equations, which in this case is  D 0  @ I @β j  6.3.3 Example application study  The problem Data reduction of large volumes of hyperspectral imagery for remote sensing applications  Ifarraguerri and Chang, 2000 .  Summary Hyperspectral imaging sensors are capable of generating large volumes of data and it is necessary to exploit techniques for reducing this volume while simulta- neously preserving as much information as possible. A projection pursuit is applied to hyperspectral digital imagery and signiﬁcant reduction demonstrated.  The data The data comprise measurements made using the hyperspectral digital im- agery collection equipment  HYDICE , an imaging spectrometer operating in the visible to short-wave infrared wavebands. There are 224 spectral bands; 256 ð 256 images of scenes including vehicles, trees, roads and other features were collected. Each pixel spectrum  224 elements  is a pattern vector.  The model A projection pursuit model is adopted with a projection index that is based on the information divergence of the projection’s estimated probability distributions from the Gaussian distribution.   220 Nonlinear discriminant analysis – projection methods  Training procedure Each pattern vector is considered as a candidate projection vec- tor. The data are projected onto each candidate vector in turn and the projection index calculated. The pattern vector with the largest value of the projection index is selected as the ﬁrst projection direction. The data are then projected onto the space orthogonal to this and the process repeated: the second projection direction is chosen from among the set of projected patterns. The process is repeated and the result is a set of orthogonal projection vectors.  The projection index is a measure of the difference of the distribution of projected  values  normalised to zero mean, unit variance  from a standard normal distribution.  Results Most of the information was compressed into seven projections, with the components tending to correspond to different types of objects. For example, the ﬁrst captured information relating to two objects in the scene, the second to roads and some of the vehicles and the third and fourth mainly to shadows and trees respectively.  6.3.4 Further developments  One of the main developments of projection pursuit has been in the area of neural network models and, in particular, the link with the multilayer perceptron model. In a comparative study on simulated data, Hwang et al.  1994a  report similar accuracy and similar computation times, but projection pursuit requires fewer functions. Zhao and Atkeson  1996  use a radial basis function smoother for the nonlinear functions, and solve for the weights as part of the optimisation process  see also Kwok and Yeung, 1996 .  6.3.5 Summary  Projection pursuit may produce projections of the data that reveal structure that is not apparent by using the coordinates axes or simple projections such as principal components  see Chapter 9 . As a classiﬁer, there are obvious links to the multilayer perceptron of Section 6.2 and radial basis function models discussed in Chapter 5. If we take the vector of nonlinear functions of projections, φ j , in  6.23  as  φ j .y  D λ j f .β T  j x    6.25   for weights λ j and a ﬁxed nonlinear function f  a logistic function , then our discriminant model is identical to the MLP discussed in Section 6.2.  The optimisation procedures of the two models differ in that in the MLP all the projection directions are determined simultaneously as part of the optimisation procedure. In projection pursuit, they are usually calculated sequentially.  A major difference between the MLP and projection pursuit is one of application. Within recent years, the MLP has been extensively assessed and applied to a diversity of real-world applications. This is not true of projection pursuit.   Application studies 221  6.4 Application studies  The term ‘neural networks’ is used to describe models that are combinations of many simple processing units – for example, the radial basis function is a linear combination of kernel basis functions and the multilayer perceptron is a weighted combination of logistic units – but the boundary between what does and what does not constitute a neural network is rather vague. In many respects, the distinction is irrelevant, but it still persists, largely through historical reasons relating to the application domain, with neural network methods tending to be developed more in the engineering and computer science literature.  Applications of multilayer perceptrons are widespread and many of the studies listed at the end of the previous chapter will include an assessment of an MLP model. Zhang  2000  surveys the area of classiﬁcation from a neural network perspective. Further special issues of journals have been devoted to applications in process engineering  Fern´andez de Ca˜nete and Bulsari, 2000 , human–computer interaction  Yasdi, 2000 , ﬁnancial engineering  Abu-Mostafa et al., 2001  and data mining and knowledge discov- ery  Bengio et al., 2000 . This latter area is one that is currently receiving considerable attention. Over the last decade there has been a huge increase in the amount of infor- mation available from the internet, business and other sources. One of the challenges in the area of data mining and knowledge discovery is to develop models that can handle large data sets, with a large number of variables  high-dimensional . Many standard data analysis techniques may not scale suitably.  Applications of MLPs, trained using a Bayesian approach, to image analysis are given  by Vivarelli and Williams  2001  and Lampinen and Vehtari  2001 .  Applications of projection pursuit include the following:  ž Facial image recognition and document image analysis. Arimura and Hagita  1994  use projection pursuit to design screening ﬁlters for feature extraction in an image recognition application.  ž Target detection. Chiang et al.  2001  use projection pursuit in a target detection  application using data from hyperspectral imagery.  6.5 Summary and discussion  Two basic models have been considered, namely the multilayer perceptron and the pro- jection pursuit model. Both models take a sum of univariate nonlinear functions  cid:4  of linear projections of the data, x, onto a weight vector α, and use this for discrimination  in a classiﬁcation problem . In the MLP, the nonlinear functions are of a prescribed form  usually logistic  and a weighted sum is formed. Optimisation of the objective function is performed with respect to the projection directions and the weights in the sum. Projection pursuit may be used in an unsupervised form, to obtain low-dimensional representations of the data, and also for regression and classiﬁcation purposes. In the pro- jection pursuit model, optimisation is performed with respect to the form of the nonlinear   222 Nonlinear discriminant analysis – projection methods  function and the projection directions and usually proceeds sequentially, ﬁnding the best projection direction ﬁrst, then the second best, and so on. Backﬁtting allows readjustment of solutions. Both models have very strong links to other techniques for discrimination and regression. Consequently, algorithms to implement these models rely heavily on the techniques presented elsewhere in this book. Procedures for initialisation, optimisation of parameters, classiﬁcation and performance estimation are common to other algorithms. There have been many links between neural networks, such as the MLP, and estab- lished statistical techniques. The basic idea behind neural network methods in the com- bination of simple nonlinear processing in a hierarchical manner, together with efﬁcient algorithms for their optimisation. In this chapter we have conﬁned our attention primar- ily to single hidden layer networks, where the links to statistical techniques are more apparent. However, more complex networks can be built for speciﬁc tasks. Also, neural networks are not conﬁned to the feed-forward types for supervised pattern classiﬁcation or regression as presented here. Networks with feedback from one layer to the previous layer, and unsupervised networks have been developed for a range of applications  see Chapter 10 for an introduction to self-organising networks . Views of neural networks from statistical perspectives, and relationships to other pattern classiﬁcation approaches, are provided by Barron and Barron  1988 , Ripley  1994, 1996 , Cheng and Titterington  1994  and Holmstr¨om et al.  1997 .  6.6 Recommendations  1. Before assessing a nonlinear technique, obtain results on some standard linear tech- niques  Chapter 4  and simple classiﬁers  for example, Gaussian classiﬁer, Chapter 2; nearest-neighbour, Chapter 3  for comparison.  2. Standardise the data  zero mean, unit variance  before training an MLP.  3. Take care in model selection so that over-training does not result – either use some form of regularisation in the training procedure or a separate validation set to determine model order.  4. Train the multilayer perceptron using a batch training method unless the data set is very large  in this case, divide the data set into subsets, randomly selected from the original data set .  5. Standardise the data before projection pursuit – zero mean, unit variance.  6. Run the projection pursuit algorithm several  times to obtain suitable projection  directions.  7. If a small number of projections is required  perhaps for visualisation purposes , pro- jection pursuit is preferred since it gives similar performance to a multilayer perceptron with fewer projections.   Notes and references 223  6.7 Notes and references  Bishop  1995  provides a through account of feed-forward neural networks. An engi- neering perspective is provided by Haykin  1994 . Tarassenko  1998  provides a basic introduction to neural network methods with an emphasis on applications.  Projection pursuit was originally proposed by Kruskal  1972  and developed by Fried- man and Tukey  1974 . Projection pursuit has been proposed in a regression context  Friedman and Stuetzle, 1981 , density estimation  Friedman et al., 1984  and exploratory data analysis  Friedman, 1987 . Reviews are provided by Huber  1985  and Jones and Sibson  1987 .  The website www.statistical-pattern-recognition.net contains refer-  ences and links to further information on techniques and applications.  Exercises  show that  1. Discuss the differences and similarities between a multilayer perceptron and a radial basis function classiﬁer with reference to network structure, geometric interpretation, initialisation of parameters and algorithms for parameter optimisation.  2. For the logistic form of the nonlinearity in an MLP,  3. Consider an MLP with a ﬁnal normalising layer  ‘softmax’ . How would you initialise  the MLP to give a nearest class mean classiﬁer?  4. Describe two ways of estimating the posterior probabilities of class membership without modelling the class densities explicitly and using Bayes’ theorem; state the assumptions and conditions of validity.  5. Logistic classiﬁcation. Given  6.15  and  6.14 , derive the expression for the  derivative,  where oi is the output for input pattern xi . Show that for the squared error deﬁned as   cid:4  .z  D  1  1 C exp. cid:5 z   D  cid:4  .z .1  cid:5   cid:4  .z    @ cid:4  @z  D  cid:5 .ti   j C .oi   j  Ž Ei Ža0 j  E D 1 2  nX iD1  jt i  cid:5  g.xi  j2  an identical result is obtained.   224 Nonlinear discriminant analysis – projection methods  6. Write down a training strategy for a multilayer perceptron using the back-ﬁtting procedure of projection pursuit. What are the potential advantages and disadvantages of such a training scheme compared with the conventional nonlinear optimisation scheme usually employed?  7. How could a multilayer perceptron be used as part of projection pursuit learning?  8. What are the main similarities and differences between a multilayer perceptron and a projection pursuit model? Take account of model structure, computational com- plexity, training strategy and optimisation procedures.  9. Using the results that for a nonsingular matrix A and vector u,  cid:5 1=.1 C uT A  .A C uuT   cid:5 1 D A   cid:5 1uuT A   cid:5 1  cid:5  A derive the result  6.20  from  6.19  with ¦ 2 for the matrix A if an RBF model is used?   cid:5 1u   t given by  6.21 . What is the expression  10. Consider a radial basis function network with a data-adaptive kernel  one that adapts to minimise the criterion optimised by training an RBF ; that is, we are allowed to adjust the shape of the kernel to minimise the squared error, for example. Under what conditions will the radial basis function model approximate a projection pursuit model?   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   7  Tree-based methods  Overview  Classiﬁcation or decision trees are capable of modelling complex nonlinear decision boundaries. An overly large tree is constructed and then pruned to minimise a cost–complexity criterion. The resulting tree is easily interpretable and can provide insight into the data structure. A procedure that can be regarded as a generalisation to provide continuous regression functions is described.  7.1 Introduction  Many discrimination methods are based on an expansion into sums of basis functions. The radial basis function uses a weighted sum of univariate functions of radial distances; the multilayer perceptron uses a weighted sum of sigmoidal functions of linear projec- tions; projection pursuit uses a sum of smoothed functions of univariate regressions of projections. In this chapter, we consider two methods that construct the basis functions by recursively partitioning the data space. The classiﬁcation and regression tree  CART  model uses an expansion into indicator functions of multidimensional rectangles. The multivariate adaptive regression spline  MARS  model is based on an expansion into sums of products of univariate linear spline functions.  7.2 Classiﬁcation trees  7.2.1 Introduction  A classiﬁcation tree or a decision tree is an example of a multistage decision pro- cess. Instead of using the complete set of features jointly to make a decision, different subsets of features are used at different levels of the tree. Let us illustrate with an   226 Tree-based methods  x6 < 2  yes  no  x4 < 1  x5 < 5  yes  x1 < 2  no  w1  yes  w3  no  w2  yes  w2  no  w1  Figure 7.1 Classiﬁcation tree  example. Figure 7.1 gives a classiﬁcation tree solution to the head injury data problem1 of Chapter 3. Associated with each internal node of the tree  denoted by a circle  are a variable and a threshold. Associated with each leaf or terminal node  denoted by a square  is a class label. The top node is the root of the tree. Now suppose we wish to classify the pattern x D .5; 4; 6; 2; 2; 3 . Beginning at the root, we compare the value of x6 with the threshold 2. Since the threshold is exceeded, we proceed to the right child. We then compare the value of the variable x5 with the threshold 5. This is not exceeded, so we proceed to the left child. The decision at this node leads us to the terminal node with classiﬁcation label !3. Thus, we assign the pattern x to class !3.  The classiﬁcation tree of Figure 7.1 is a conceptually simple approximation to a complex procedure that breaks up the decision into a series of simpler decisions at each node. The number of decisions required to classify a pattern depends on the pattern. Figure 7.1 is an example of a binary decision tree. More generally, the outcome of a decision could be one of m ½ 2 possible categories. However, we restrict our treatment in this chapter to binary trees.  Binary trees successfully partition the feature space into two parts. In the example above, the partitions are hyperplanes parallel to the coordinate axes. Figure 7.2 illustrates this in two dimensions for a two-class problem and Figure 7.3 shows the corresponding binary tree. The tree gives 100% classiﬁcation performance on the design set. The tree is not unique for the given partition. Other trees also giving 100% performance are possible.  The decisions at each node need not be thresholds on a single variable  giving hyper- planes parallel to coordinate axes as decision boundaries , but could involve a linear or nonlinear combination of variables. In fact, the data in Figure 7.2 are linearly separable  1Variables x1; : : : ; x6 are age, EMV score, MRP, change, eye indicant and pupils and classes  !1, !2 and !3 dead vegetative, severely disabled and moderate or good recovery.   Classiﬁcation trees 227  x2 D mx1 C c  cid:1    cid:1  cid:1   Š  Š   cid:1   cid:1  ž   cid:1   ž  Š  Š  Š   cid:1    cid:1   Š  cid:1   cid:1   ž  ž  ž   cid:2   Š  Š  x2 b3  b2  b1  Š  Š  Š  Š  Š  ž   cid:1   Š  cid:1   cid:1   Š   cid:1    cid:1    cid:1    cid:1    cid:1    cid:1   Š  Š  Š  cid:1  ž  ž  Š  ž  Š  cid:1   cid:1    cid:1    cid:1    cid:1   ž  ž  ž  ž  ž  ž  a2  a1   cid:1   x1  Figure 7.2 Boundaries on a two-class, two-dimension problem  x2 > b2  yes  no  x1 < a2  x1 > a1  yes  no  yes  no  x2 < b3  x2 < b1  yes  no  yes  no  Figure 7.3 Binary decision tree for the two-class, two-dimension data of Figure 7.2  and the decision rule: assign x to class Š if x2  cid:6  mx1  cid:6  c > 0 classiﬁes all samples correctly.  Classiﬁcation trees have been used on a wide range of problems. Their advantages are that they can be compactly stored; they efﬁciently classify new samples and have demonstrated good generalisation performance on a wide variety of problems. Possible   228 Tree-based methods  disadvantages are the difﬁculty in designing an optimal tree, leading perhaps to large trees with poor error rates on certain problems, particularly if the separating boundary is complicated and a binary decision tree with decision boundaries parallel to the coordinate axes is used. Also, most approaches are non-adaptive – they use a ﬁxed training set, and additional data may require redesign of the tree.  There are several heuristic methods for construction of decision-tree classiﬁers; see Safavian and Landgrebe  1991  for a survey of decision-tree classiﬁer methodology. They are usually constructed top-down, beginning at the root node  a classiﬁcation tree is usually drawn with its root at the top and its leaves at the bottom  and successively partitioning the feature space. The construction involves three steps:  1. Selecting a splitting rule for each internal node. This means determining the features,  together with a threshold, that will be used to partition the data set at each node.  2. Determining which nodes are terminal nodes. This means that, for each node, we must decide whether to continue splitting or to make the node a terminal node and assign to it a class label. If we continue splitting until every terminal node has pure class membership  all samples in the design set that arrive at that node belong to the same class  then we are likely to end up with a large tree that over-ﬁts the data and gives a poor error rate on an unseen test set. Alternatively, relatively impure terminal nodes  nodes for which the corresponding subset of the design set has mixed class membership  lead to small trees that may under-ﬁt the data. Several stopping rules have been proposed in the literature, but the approach suggested by Breiman et al.  1984  is successively to grow and selectively prune the tree, using cross-validation to choose the subtree with the lowest estimated misclassiﬁcation rate.  3. Assigning class labels to terminal nodes. This is relatively straightforward and labels  can be assigned by minimising the estimated misclassiﬁcation rate.  We shall now consider each of these stages in turn. The approach we present is based  on the CART2 description of Breiman et al.  1984 .  7.2.2 Classiﬁer tree construction  We begin by introducing some notation. A tree is deﬁned to be a set T of positive integers together with two functions l.:  and r .:  from T to T [ f0g. Each member of T corresponds to a node in the tree. Figure 7.4 shows a tree and the corresponding values of l.t   and r .t    denoting the left and right nodes : 1. For each t 2 T , either l.t   D 0 and r .t   D 0  a terminal node  or l.t   > 0 and  r .t   > 0  a non-terminal node .  2. Apart from the root node  the smallest integer, t D 1 in Figure 7.4  there is a unique parent s 2 T of each node; that is, for t 6D 1, there is an s such that either t D l.s  or t D r .s .  2CART is a registered trademark of California Statistical Software, Inc.   1  2  3  4  5  6  7  Classiﬁcation trees 229  t 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  l t  2 4 6 8 0 10 12 0 0 0 0 14 0 0 0  r  t  3 5 7 9 0 11 13 0 0 0 0 15 0 0 0  8  9  10  11  12  13  14  15  Figure 7.4 Classiﬁcation tree and the values of l.t  and r .t   A subtree is a non-empty subset T1 of T together with two functions l1 and r1 such that  l1.t   D  r1.t   D  ² l.t   0 ² r .t   0  if l.t   2 T1 otherwise if r .t   2 T1 otherwise   7.1   and provided that T1; l1.:  and r1.:  form a tree. For example, the set f3; 6; 7; 10; 11g together with  7.1  forms a subtree, but the sets f2; 4; 5; 3; 6; 7g and f1; 2; 4; 3; 6; 7g do not; in the former case because there is no parent for both 2 and 3 and in the latter case because l1.2  > 0 and r1.2  D 0. A pruned subtree T1 of T is a subtree of T that has the same root. This is denoted by T1  cid:12  T . Thus, example  b  in Figure 7.5 is a pruned subtree, but example  a  is not  though it is a subtree . Let QT denote the set of terminal nodes  the set f5; 8; 9; 10; 11; 13; 14; 15g in Figure 7.4 . Let fu.t  ; t 2 QTg be a partition of the data space R p  that is, u.t   is a subspace of 6D s; t; s 2 QT ; R p associated with a terminal node such that u.t   \ u.s  D Þ for t and [t2 QT u.t   D R p . Let ! j .t  2 f!1; : : : ; !Cg denote one of the class labels. Then a  3  6  7  t 3 6 7 10 11  l1 t  6 10 0 0 0  r1 t  7 11 0 0 0  1  2  3  4  5  t 1 2 3 4 5 8 9  l1 t  2 4 0 8 0 0 0  r1 t  3 5 0 9 0 0 0  10  11   a   8  9   b   Figure 7.5 Possible subtrees of the tree in Figure 7.4;  a  is not a pruned subtree;  b  is a pruned subtree   230 Tree-based methods  classiﬁcation tree consists of the tree T together with the class labels f! j .t ; t 2 QTg and the partition fu.t  ; t 2 QTg. All we are saying here is that associated with each terminal node is a region of the data space that we label as belonging to a particular class. There is also a subspace of R p, u.t  , associated with each non-terminal node, being the union of the subspaces of the terminal nodes that are its descendants. A classiﬁcation tree is constructed using a labelled data set, L D f.xi ; yi  ; i D 1; : : : ; ng where xi are the data samples and yi the corresponding class labels. If we let N .t   denote the number of samples of L for which xi 2 u.t   and N j .t   be the number of samples for which xi 2 u.t   and yi D ! j  P j N j .t   D N .t   , then we may deﬁne  p.t   D N .t   n  an estimate of p.x 2 u.t    based on L;  p.! jjt   D N j .t   N .t   an estimate of p.y D ! jjx 2 u.t    based on L; and  pL D p.tL   p.t    pR D p.tR  p.t     7.2    7.3   where tL D l.t  ; tR D r .t  , as estimates of p.x 2 u.tL  jx 2 u.t    and p.x 2 u.tR jx 2 u.t    based on L respectively.  We may assign a label to each node, t, according to the proportions of samples from  each class in u.t  : assign label ! j to node t if p.! jjt   D max  p.!ijt    i  We have now covered most of the terminology that we shall use. It is not difﬁ- cult to understand, but it may be a bit much to take in all at once. Figure 7.6 and Table 7.1 illustrate some of these concepts using the data of Figure 7.2 and the tree of Figure 7.3.  x2 b3  b2  b1  u 9   u 8   u 4   u 10   u 11   u 6   a2  a1  x1  1  no  3  yes  2  no  yes  5  6 yes  11  no  8  yes  9  yes  4  no  7  no  10  Figure 7.6 Classiﬁcation tree and decision regions. A description of the nodes is given in Table 7.1   Table 7.1 Tree table – class !1 D Š; class !2 D ž  t  node  l.t   r .t   N .t   N1.t   N2.t    p.t    p.1jt    p.2jt    pL  pR  Classiﬁcation trees 231  1  2  3  4  5  6  7  8  9  10  11  x2 > b2  x1 < a2  x1 > a1  x2 > b3  Š  ž  ž Š Š ž  2  4  6  0  9  0  0  0  0  0  3  5  7  0  8  0  0  0  0  0  35  22  13  15  7  9  4  5  2  3  1  20  17  4  15  2  0  3  0  2  3  0  x2 < b1  11  10  15  5  9  0  5  9  1  5  0  0  1  1  22 35  13 35  15 35  7 35  9 35  4 35  5 35  2 35  3 35  1 35  20 35  17 22  4 13  1  2 7  0  3 4  0  1  1  0  15 35  5 22  9 13  0  5 7  1  1 4  1  0  0  1  22 35  15 22  9 13  5 7  3 4  13 35  7 22  4 13  2 7  1 4  Splitting rules A splitting rule is a prescription for deciding which variable, or combination of variables, should be used at a node to divide the samples into subgroups, and for deciding what the threshold on that variable should be. A split consists of a condition on the coordinates of a vector x 2 R p. For example, we may deﬁne a split s p to be  a threshold on an individual feature, or  s p D fx 2 R p; x4  cid:12  8:2g  s p D fx 2 R p; x2 C x7  cid:12  2:0g  a threshold on a linear combination of features. Nonlinear functions have also been considered  Gelfand and Delp, 1991 . Thus, at each non-terminal node, suppose x 2 u.t  . Then if x 2 s p, the next step in the tree is to l.t  , the left branch, otherwise r .t  .  The question we now have to address is how to split the data that lie in the subspace u.t   at node t. Following Breiman et al.  1984 , we deﬁne the node impurity function,   232 Tree-based methods  I.t  , to be  I.t   D  cid:8  . p.!1jt  ; : : : ; p.!Cjt     j q j D 1. It has the following properties:  where  cid:8  is a function deﬁned on all C-tuples .q1; : : : ; qC   satisfying q j ½ 0 and P 1.  cid:8  is a maximum only when q j D 1=C for all j. 2. It is a minimum when q j D 1, qi D 0; i 6D j, for all j. 3. It is a symmetric function of q1; : : : ; qC .  One measure of the goodness of a split is the change in the impurity function. A split that maximises the decrease in the node impurity function when moving from one group to two  1I.s p; t    4D I.t    cid:6  .I.tL   pL C I.tR   pR   over all splits s p is one choice. Several different forms for I.t   have been used. One suggestion is the Gini criterion  I.t   D X i6D j  p.!ijt   p.! jjt    This is easily computed for a given split.  Different splits on the data at node t may now be evaluated using the above criterion, but what strategy do we use to search through the space of possible splits? First of all, we must conﬁne our attention to splits of a given form. We shall choose the possible splits to consist of thresholds on individual variables s p D fx; xk  cid:12  −g  where k D 1; : : : ; p and − ranges over the real numbers. Clearly, we must restrict the number of splits we examine and so, for each variable xk, − is allowed to take one of a ﬁnite number of values within the range of possible values. Thus, we are dividing each variable into a number of categories, though this should be kept reasonably small to prevent excessive computation and need not be larger than the number of samples at each node.  There are many other approaches to splitting. Some are given in the survey by Safavian and Landgrebe  1991 . The approach described above assumes ordinal variables. For a categorical variable with N unordered outcomes, partitioning on that variable means considering 2N partitions. By exhaustive search, it is possible to ﬁnd the optimal one, but this could result in excessive computation time if N is large. Techniques for nominal variables can be found in Breiman et al.  1984  and Chou  1991 .  So now we can grow our tree by successively splitting nodes, but how do we stop? We could continue until each terminal node contained one observation only. This would lead to a very large tree, for a large data set, that would over-ﬁt the data. We could im- plement a stopping rule: we do not split the node if the change in the impurity function is less than a prespeciﬁed threshold. Alternatively, we may grow a tree with termi- nal nodes that would have pure  or nearly pure  class membership and then prune it. This can lead to better performance than a stopping rule. We now discuss one pruning algorithm.   Classiﬁcation trees 233  Pruning algorithm We now consider one algorithm for pruning trees that will be required in the following subsection on classiﬁer tree construction. The pruning algorithm is general in that it applies to trees that are not necessarily classiﬁcation but regression trees. But ﬁrst some more notation. Let R.t   be real numbers associated with each node t of a given tree T . If t is a terminal node, i.e. t 2 QT , then R.t   could represent the proportion of misclassiﬁed samples – the number of samples in u.t   that do not belong to the class associated with the terminal node, deﬁned to be M.t  , divided by the total number of data points, n  Let RÞ.t   D R.t   C Þ for a real number Þ. Set3  R.t   D M.t   n  t 2 QT  R.t    R.T   D X t2 QT RÞ.T   D X t2 QT  RÞ.t   D R.T   C Þj QTj  In a classiﬁcation problem, R.T   is the estimated misclassiﬁcation rate; j QTj denotes the cardinality of the set QT ; RÞ.T   is the estimated complexity–misclassiﬁcation rate of a classiﬁcation tree; and Þ is a constant that can be thought of as the cost of complexity per terminal node. If Þ is small, then there is a small penalty for having a large number of nodes. As Þ increases, the minimising subtree  the subtree T 0  cid:12  T that minimises RÞ.T 0   has fewer terminal nodes.  We shall describe the CART pruning algorithm  Breiman et al., 1984  by means of  an example, using the tree of Figure 7.7. Let the quantity R.t   be given by  R.t   D r .t   p.t    r .t   D 1  cid:6  max! j  p.! jjt    where r .t   is the resubstitution estimate of the probability of misclassiﬁcation  see Chapter 8  given that a case falls into node t,  and p.t   and p.! jjt   are given by  7.2  and  7.3 . Thus, if t is taken to be a terminal node, R.t   is the contribution of that node to the total error.  Let Tt be the subtree with root t. If RÞ.Tt   < RÞ.t  , then the contribution to the cost of complexity of the subtree is less than that for the node t. This occurs for small Þ. As Þ increases, equality is achieved when  Þ D R.t    cid:6  R.Tt   Nd .t    cid:6  1  3The argument of R may be a tree or a node; capital letters denote a tree.   234 Tree-based methods  0.5  1  0.0315  0.2  2  0.03  0.19  3  0.017  0.08  4  0.015  0.09  5  0.06  0.10  6  0.0875  7  8  0.05  9  0.01  0.01  10 0.02  11 0.01  0.04  0.05  12  0.0075  0.05  13  0.02  14 0.01  15 0.03  0.05  16  0.01  17  18  0.02  19  0.01  0.01  20  0.01  21  0.015  Figure 7.7 Pruning example – original tree  where Nd .t   is the number of terminal nodes in Tt , i.e. Nd .t   D j QTtj and termination of the tree at t is preferred. Therefore, we ﬁnally deﬁne  g.t   D R.t    cid:6  R.Tt   Nd .t    cid:6  1   7.4   as a measure of the strength of the link from node t.  In Figure 7.7, each terminal node has been labelled with a single number R.t  , the amount by which that node contributes to the error rate. Each non-terminal node has been labelled by two numbers. The number to the left of the node is the value of R.t  , the contribution to the error rate if that node were a terminal node. The number to the right is g.t  , calculated using  7.4 . Thus, the value of g.t   for node t D 2, say, is 0:03 D [0:2  cid:6  .0:01 C 0:01 C 0:03 C 0:02 C 0:01 ]=4.  The ﬁrst stage of the algorithm searches for the node with the smallest value of g.t  . This is node 12, with a value of 0.0075. This is now made a terminal node and the value of g.t   recalculated for all its ancestors. This is shown in Figure 7.8. The values of the nodes in the subtree beginning at node 2 are unaltered. The process is now repeated. The new tree is searched to ﬁnd the node with the smallest value of g.t  . In this case, there are two nodes, 6 and 9, each with a value of 0.01. Both are made terminal nodes and again the values of g.t   for the ancestors recalculated. Figure 7.9 gives the new tree  T 3 . Node 4 now becomes the terminal node. This continues until all we are left with is the root node. Thus, the pruning algorithm generates a succession of trees. We denote the tree at the kth stage by T k. Table 7.2 gives the value of the error rate for each successive tree, together with the number of terminal nodes in each tree and the value of g.t   at each stage  denoted by Þk  that is used in the pruning to generate tree T k. The tree T k has all internal nodes with a value of g.t   > Þk.  The results of the pruning algorithm are summarised in Figure 7.10. This ﬁgure shows the original tree together with the values g6.t   for the internal nodes, where gk is deﬁned   Classiﬁcation trees 235  0.5  1  0.037   no change for this branch of tree   0.19  3  0.0233  0.10  6  0.01  7  0.04  0.05  13 0.02  12  0.05  18  0.02  19  0.01  Figure 7.8 Pruning example – pruning at node 12  0.5  1  0.054  0.2  2  0.027  0.19  3  0.05  0.08  4  0.02  0.09  5  0.06  8  9  0.01  0.05  10  0.02  11  0.01  6  0.1  7  0.04  Figure 7.9 Pruning example – pruning at nodes 6 and 9  Table 7.2 Tree results  k  Þk  1 0 2 0.0075 3 0.01 4 0.02 5 0.045 6 0.05 7 0.11  j QT kj R.T k   0.185 11 0.2 9 6 0.22 0.25 5 0.34 3 0.39 2 1 0.5  recursively  0  cid:12  k  cid:12  K  cid:6  1; K D the number of pruning stages :  gk D  ² g.t   gk cid:6 1.t   otherwise  t 2 T k  cid:6  QT k   t an internal node of T k   The values of g6.t   together with the tree T 1 are a useful means of summarising the pruning process. The smallest value is 0.0075 and shows that in going from T 1 to T 2   236 Tree-based methods  1  0.11  2  0.045  3  0.05  4  0.02  5  0.06  6  0.01  7  8  9  0.01  10  11  12  0.0075  13  0.02  14  15  16  0.01  17  18  19  20  21  Figure 7.10 Pruning example – summary of pruning process  nodes 16, 17, 20 and 21 are removed. The smallest value of the pruned tree is 0.01 and therefore in going from T 2 to T 3 nodes 12, 13, 14, 15, 18 and 19 are removed; from T 3 to T 4 nodes 8 and 9 are removed; from T 4 to T 5 nodes 4, 5, 10 and 11; from T 5 to T 6 nodes 6 and 7; and ﬁnally, nodes 2 and 3 are removed to obtain T 7 the tree with a single node.  An explicit algorithm for the above process is given by Breiman et al.  1984 .  Classiﬁcation tree construction methods Having considered approaches to growing and pruning classiﬁcation trees and estimating their error rate, we are now in a position to present methods for tree construction using these features.  CART independent training and test set method The stages in the CART independent training and test set method are as follows. Assume that we are given a training set Lr and a test set  or, more correctly, a validation set  Ls of data samples. These are generated by partitioning the design set approximately into two groups.  The CART independent training and test set method is as follows.  1. Use the set Lr to generate a tree T by splitting all the nodes until all the terminal nodes are ‘pure’ – all samples at each terminal node belong to the same class. It may not be possible to achieve this with overlapping distributions, therefore an alternative is to stop when the number in each terminal node is less than a given threshold or the split of a node t results in a left son tL or a right son tR with min.N .tL  ; N .tR   D 0. 2. Use the CART pruning algorithm to generate a nested sequence of subtrees T k using  the set Ls.  3. Select the smallest subtree for which R.T k   is a minimum. CART cross-validation method For the cross-validation method, the training set L is divided into V subsets L1; : : : ;LV , with approximately equal numbers in each class.   Classiﬁcation trees 237  Let Lv D L  cid:6  Lv; v D 1; : : : ; V . We shall denote by T .Þ  the pruned subtree with all internal nodes having a value of g.t   > Þ. Thus, T .Þ  is equal to T k, the pruned subtree at the kth stage, where k is chosen so that Þk  cid:12  Þ  cid:12  ÞkC1  ÞKC1 D 1 .  The CART cross-validation method is as follows.  1. Use the set L to generate a tree T by splitting all the nodes until all the terminal nodes are ‘pure’ – all samples at each terminal node belong to the same class. It may not be possible to achieve this with overlapping distributions, therefore an alternative is to stop when the number in each terminal node is less than a given threshold or the split of a node t results in a left son tL or a right son tR with min.N .tL  ; N .tR   D 0.  This is step 1 of the CART independent training and test set method above.   2. Using the CART pruning algorithm, generate a nested sequence of pruned subtrees  T D T 0 ½ T 1 ½ ÐÐÐ ½ T K D root.T  .  3. Use Lv to generate a tree Tv and assign class labels to the terminal nodes, for v D  1; : : : ; V .  of Tv.  4. Using the CART pruning algorithm, generate a nested sequence of pruned subtrees  5. Calculate Rcv.T k    the cross-validation estimate of the misclassiﬁcation rate  given by  Rcv.T k   D 1 V  VX vD1  p  Rv.Tv.  ÞkÞkC1    where Rv is the estimate of the misclassiﬁcation rate based on the set Lv for the pruned subtree Tv.pÞkÞkC1 . 6. Select the smallest T Ł 2 fT 0; : : : ; T Kg such that  7. Estimate the misclassiﬁcation rate by  Rcv.T Ł  D min  Rcv.T k    k  OR.T Ł  D Rcv.T Ł   The procedure presented in this section implements one of many approaches to re- cursive tree design which use a growing and pruning approach and that have emerged as reliable techniques for determining right-sized trees. The CART approach is appropriate for data sets of continuous or discrete variables with ordinal or nominal signiﬁcance, including data sets of mixed variable types.  7.2.3 Other issues  Missing data The procedure given above contains no mechanism for handling missing data. The CART algorithm deals with this problem through the use of surrogate splits. If the best split of   238 Tree-based methods  a node is ∫ on the variable xm say, then the split ∫Ł that predicts ∫ most accurately, on a variable x j other than xm, is termed the best surrogate for ∫. Similarly, a second best surrogate on a variable other than xm and x j can be found, and so on. A tree is constructed in the usual manner, but at each node t, the best split ∫ on a variable xm is found by considering only those samples for which a value for xm is available. Objects are assigned to the groups corresponding to tL and tR according to the value on xm. If this is missing for a given test pattern, the split is made using the best surrogate for ∫  that is, the split is made on a different variable . If this value is also missing, the second best surrogate is used, and so on until the sample is split. Alternatively, we may use procedures that have been developed for use by conventional classiﬁers for dealing with missing data  see Chapter 11 .  Priors and costs The deﬁnitions  7.2  and  7.3  assume that the prior probabilities for each class, denoted by ³.i  , are equal to N j =n. If the distribution on the design set is not proportional to the expected occurrence of the classes, then the resubstitution estimates of the probabilities that a sample falls into node t, p.t  , and that it is in class ! j given that it falls into node t, p. jjt  , are deﬁned as  p.t   D CX jD1  ³. j    N j .t   N j  p. jjt   D  ³. j  N j .t  =N j PC jD1  ³. j  N j .t  =N j  R.T   D X  q.ij j  ³. j     7.5    7.6   In the absence of costs  or assuming an equal cost loss matrix – see Chapter 1 , the  misclassiﬁcation rate is given by  where q.ij j   is the proportion of samples of class ! j deﬁned as class !i by the tree. If ½ ji to !i , then the misclassiﬁcation cost is  is the cost of assigning an object of class ! j  R.T   D X  ½ ji q.ij j  ³. j    i j  i j  This may be written in the same form as  7.6 , with redeﬁned priors, provided that ½ ji is independent of i; thus, ½ ji D ½ j and the priors are redeﬁned as  ³0. j   D ½ j ³. j   ½ j ³. j    P  j  In general, if there is an asymmetric cost matrix with non-constant costs of misclassiﬁ- cation for each class, then the costs cannot be incorporated into modiﬁed priors.   Classiﬁcation trees 239  7.2.4 Example application study  The problem Identiﬁcation of emitter type based on measurements of electronic char- acteristics  Brown et al., 1993 . This is important in military applications  to distinguish friend from foe  and civil applications such as communication spectrum management and air space management.  Summary A classiﬁcation tree was compared with a multilayer perceptron on the task of identifying the emitter type given measurements made on a received waveform. Both methods produced comparable error rates.  The data The class distributions are typically multimodal since a given emitter can change its electronic signature by changing its settings. The data consisted of simulated measurements on three variables only  frequency, pulse repetition interval and pulse duration  of four radars, each of which had ﬁve operational settings.  The model A decision tree was constructed using the CART algorithm  Section 7.2.2 .  Training procedure CART was trained employing the Gini criterion for impurity. The three features were also augmented by three additional features, being the sums of pairs of the original features. Thus some decision boundaries were not necessarily parallel to coordinate boundaries. This improved performance.  Results The performance of CART and the multilayer perceptron were similar, in terms of error rate, and the training time for the classiﬁcation tree was considerably less than the MLP. Also, the classiﬁcation tree had the advantage that it was easy to interpret, clearly identifying regions of feature space associated with each radar type.  7.2.5 Further developments  The splitting rules described have considered only a single variable. Some data sets may be naturally separated by hyperplanes that are not parallel to the coordinate axes, and Chapter 4 concentrated on means for ﬁnding linear discriminants. The basic CART algo- rithm attempts to approximate these surfaces by multidimensional rectangular regions, and this can result in very large trees. Extensions to this procedure that allow splits not orthogonal to axes are described by Loh and Vanichsetakul  1988  and Wu and Zhang  1991   and indeed can be found in the CART book of Breiman et al., 1984 . Sankar and Mammone  1991  use a neural network to recursively partition the data space and allow general hyperplane partitions. Pruning of this neural tree classiﬁer is also addressed  see also Sethi and Yoo, 1994, for multifeature splits methods using perceptron learning .  Speed-ups to the CART procedure are discussed by Mola and Siciliano  1997 . Also, non-binary splits of the data may be considered  Loh and Vanichsetakul, 1988; Sturt, 1981; and in the vector quantisation context, Gersho and Gray, 1992 .   240 Tree-based methods  There are many growing and pruning strategies. Quinlan  1987  describes and assesses four pruning approaches, the motivation behind the work being to simplify decision trees in order to use the knowledge in expert systems. The use of information-theoretic criteria in tree construction has been considered by several authors. Quinlan and Rivest  1989  describe an approach based on the minimum description length principle and Goodman and Smyth  1990  present a top-down mutual information algorithm for tree design. A comparative study of pruning methods for decision trees is provided by Esposito et al.  1997   see also comments by Kay, 1997; Mingers, 1989 . Averaging, as an alternative to pruning, is discussed by Oliver and Hand  1996 .  Tree-based methods for vector quantisation are described by Gersho and Gray  1992 . A tree-structured approach reduces the search time in the encoding process. Pruning the tree results in a variable-rate vector quantiser and the CART pruning algorithm may be used. Other approaches for growing a variable-length tree  in the vector quantisa- tion context  without ﬁrst growing a complete tree are described by Riskin and Gray  1991 . Crawford  1989  describes some extensions to CART that improve upon the cross-validation estimate of the error rate and allow for incremental learning – updating an existing tree in the light of new data.  One of the problems with nominal variables with a large number of categories is that there may be many possible partitions to consider. Chou  1991  presents a cluster- ing approach to ﬁnding a locally optimum partition without exhaustive search. Buntine  1992  develops a Bayesian statistics approach to tree construction that uses Bayesian techniques for node splitting, pruning and averaging of multiple trees  also assessed as part of the Statlog project . A Bayesian CART algorithm is described by Denison et al.  1998a .  7.2.6 Summary  One of the main attractions of CART is its simplicity: it performs binary splits on single variables in a recursive manner. Classifying a sample may require only a few simple tests. Yet despite its simplicity, it is able to give performance superior to many traditional methods on complex nonlinear data sets of many variables. Of course there are possible generalisations of the model to multiway splitting on linear  or even nonlinear  combinations of variables, but there is no strong evidence that these will lead to improved performance. In fact, the contrary has been reported by Breiman and Friedman  1988 . Also, univariate splitting has the advantage that the models can be interpreted more easily. Lack of interpretability by many, if not most, of the other methods of discrimination described in this book can be a serious shortcoming in many applications.  Another advantage of CART is that it is a procedure that has been extensively eval- uated and tested, both by the workers who developed it and numerous researchers who have implemented the software. In addition, the tree-structured approach may be used for regression using the same impurity function  for example, a least squares error measure  to grow the tree as to prune it.  A possible disadvantage of CART is that training can be time-consuming. An alter- native is to use a parametric approach with the inherent underlying assumptions. The use of nonparametric methods in discrimination, regression and density estimation has been increasing with the continuing development of computing power, and, as Breiman and   Multivariate adaptive regression splines 241  Friedman point out, ‘the cost for computation is decreasing roughly by a factor of two every year, whereas the price paid for incorrect assumptions is remaining the same’. It really depends on the problem. In most applications it is the cost of data collection that far exceeds any other cost and the nonparametric approach is appealing because it does not make the  often gross  assumptions regarding the underlying population distributions that other discrimination methods do.  7.3 Multivariate adaptive regression splines  7.3.1 Introduction  The multivariate adaptive regression spline  MARS   Friedman, 1991  approach may be considered as a continuous generalisation of the regression tree methodology treated separately in Section 7.2, and the presentation here follows Friedman’s approach. Suppose that we have a data set of n measurements on p variables, fxi ; i D 1; : : : ; ng, xi 2 R p, and corresponding measurements on the response variable fyi ; i D 1; : : : ; ng. We shall assume that the data have been generated by a model  yi D f .x  C ž  where ž denotes a residual term. Our aim is to construct an approximation, function f .  Of , to the  7.3.2 Recursive partitioning model  The recursive partitioning model takes the form  The basis functions Bm are  Of .x  D MX mD1  am Bm .x   Bm .x  D Ifx 2 uimg  where I is an indicator function with value unity if the argument is true and zero other- wise. We have used the notation that fui ; i D 1; : : : ; Mg is a partition of the data space R p  that is, ui is a subspace of R p such that ui \ u j D Þ for i 6D j, and [i ui D R p . The set fam ; m D 1; : : : ; Mg are the coefﬁcients in the expansion whose values are de- termined  often  by a least squares minimisation procedure for ﬁtting the approximation to the data. In a classiﬁcation problem, a regression for each class may be performed  using binary variables with value 1 for xi 2 class ! j , and zero otherwise  giving C functions Of j , on which discrimination is based.  The basis functions are produced by a recursive partitioning algorithm  Friedman, 1991  and can be represented as a product of step functions. Consider the partition   242 Tree-based methods  x2 b3  b2  b1  u 9   u 8   u 4   u 10   u 11   u 6   a2  a1  x1  1  no  3  yes  2  no  yes  5  6 yes  11  no  8  yes  9  yes  4  no  7  no  10  Figure 7.11 Classiﬁcation tree and decision regions; the decision nodes  circular  are charac- terised as follows: node 1, x2 > b2; node 2, x1   b3; node 3, x1 > a1; node 7, x2 < b1. The square nodes correspond to regions in the feature space  produced by the tree given in Figure 7.11 . The ﬁrst partition is on the variable x2 and divides the plane into two regions, giving basis functions  where  H[.x2  cid:6  b2 ] and H[ cid:6 .x2  cid:6  b2 ]  H .x  D  ² 1 0 otherwise  x ½ 0  The region x2 < b2 is partitioned again, on variable x1 with threshold a1, giving the basis functions  H[ cid:6 .x2  cid:6  b2 ]H[C.x1  cid:6  a1 ] and H[ cid:6 .x2  cid:6  b2 ]H[ cid:6 .x1  cid:6  a1 ]  The ﬁnal basis functions for the tree comprise the products  H[ cid:6 .x2  cid:6  b2 ]H[ cid:6 .x1  cid:6  a1 ]H[C.x2  cid:6  b1 ] H[ cid:6 .x2  cid:6  b2 ]H[ cid:6 .x1  cid:6  a1 ]H[ cid:6 .x2  cid:6  b1 ] H[ cid:6 .x2  cid:6  b2 ]H[C.x1  cid:6  a1 ] H[C.x2  cid:6  b2 ]H[ cid:6 .x1  cid:6  a2 ] H[C.x2  cid:6  b2 ]H[C.x1  cid:6  a2 ]H[C.x2  cid:6  b3 ] H[C.x2  cid:6  b2 ]H[C.x1  cid:6  a2 ]H[ cid:6 .x2  cid:6  b3 ]  Thus, each basis function is a product of step functions, H.  In general, the basis functions of the recursive partitioning algorithm have the form  Bm .x  D KmY kD1  H[skm .xv.k;m   cid:6  tkm  ]   7.7   where the skm take the values š1 and Km is the number of splits that give rise to Bm .x ; xv.k;m  is the variable split and the tkm are the thresholds on the variables.  The MARS procedure is a generalisation of this recursive partitioning procedure in  the following ways.   Multivariate adaptive regression splines 243  Continuity The recursive partitioning model described above is discontinuous at region boundaries. This is due to the use of the step function H. The MARS procedure replaces these step functions by spline functions. The two-sided truncated power basis functions for qth-order splines are  bš  q  .x  cid:6  t   D [š.x  cid:6  t  ]qC  where [:]C denotes that the positive part of the argument is considered. The basis functions .x  are illustrated in Figure 7.12. The step function, H, is the special case of q D 0. bC q The MARS algorithm employs q D 1. This leads to a continuous function approximation, but discontinuous ﬁrst derivatives.  The basis functions now take the form  m .x  D KmY Bq kD1  [skm .xv.k;m   cid:6  tkm  ]qC   7.8   where the tkm are referred to as the knot locations.  Retention of parent functions The basic recursive partitioning algorithm replaces an existing parent basis function by its product with a step function and the reﬂected step function. The number of basis functions therefore increases by one on each split. The MARS procedure retains the parent basis function. Thus the number of basis functions increases by two at each split. This provides a much more ﬂexible model that is capable of modelling classes of functions that have no strong interaction effects, or strong interactions involving at most a few of the variables, as well as higher-order interactions. A consequence of the retention of the parent functions is that the regions corresponding to the basis functions may overlap.  3.5  4  3  2  1  2.5  1.5  0.5  0  cid:6 0.5   cid:6 2   cid:6 1.5   cid:6 1   cid:6 0.5  0.5 1.5 Figure 7.12 Spline functions, bC q .x , for q D 0; 1; 2  1  0  2  q D 2  q D 1  q D 0   244 Tree-based methods  Multiple splits The basic recursive partitioning algorithm allows multiple splits on a single variable: as part of the modelling procedure, a given variable may be selected repeatedly and partitioned. Thus basis functions comprise products of repeated splits on a given variable  the basis functions for the tree in Figure 7.11 contain repeated splits . In the continuous generalisation, this leads to functional dependencies of higher order than q on individual variables. To take advantage of the properties of tensor product spline basis functions, whose factors involve a different variable, MARS restricts the basis functions to products involving single splits on a given variable. By reselecting the same parent for splitting  on the same variable , the MARS procedure is able to retain the ﬂexibility of the repeated split model  it trades depth of tree for breadth of tree .  The second stage in the MARS strategy is a pruning procedure; basis functions are deleted one at a time – the basis function being removed being that which improves the ﬁt the most  or degrades it the least . A separate validation set could be chosen to estimate the goodness of ﬁt of the model. The lack-of-ﬁt criterion proposed for the MARS algorithms is a modiﬁed form of the generalised cross-validation criterion of Craven and Wahba  1979 . In a discrimination problem, a model could be chosen that minimises error rate on the validation set. In applying MARS to discrimination problems, one approach is to use the usual binary coding for the response function, f j ; j D 1; : : : ; C, with f j .x  D 1 if x is in class ! j and zero otherwise. Each class may have a separate MARS model; the more parsimonious model of having common basis functions for all classes will reduce the computation. The weights ai in the MARS algorithms are replaced by vectors ai , determined to minimise a generalised cross-validation measure.  7.3.3 Example application study  The problem To understand the relationship between sea ﬂoor topography, ocean cir- culation and variations in the sea ice concentration in Antarctic regions  De Veaux et al., 1993 .  Summary The occurrence of polynyas, large areas of open water within an otherwise continuous cover of sea ice, is of interest to oceanographers. In particular, the Weddell Polynya was active for three years and is the subject of this investigation. A nonparametric regression model was used to quantify the effect of sea ﬂoor topography on sea ice surface characteristics.  The data Data were gathered from one ocean region  the Maud Rise . Ice concentration maps were derived from scanning multichannel microwave radiometer data. Bathymetry data were derived, primarily, from soundings taken from ships at approximately a 5 km interval along the ship tracks and at a vertical resolution of 1 m.  The model Five predictor variables were considered  bathymetry, its meridional and zonal derivatives and their derivatives . The MARS model was used with the interaction level restricted to two, resulting in 10 additional potential variables.   Application studies 245  Training procedure The MARS procedure selected three two-variable interactions. Retaining only a few low-order interactions allowed the results of ﬁtting the ﬁnal model to be interpreted. This is important for other applications.  Results The results strongly support the hypothesis that there is a link between sea ice concentration and sea ﬂoor topography. In particular, the sea ice concentration can be predicted to some degree by the ocean depth and its ﬁrst and second derivatives in both meridional and zonal directions.  7.3.4 Further developments  A development of the basic model to include procedures for mixed ordinal and cate- gorical variables is provided by Friedman  1993 . POLYMARS  Stone et al., 1997  is a development to handle a categorical response variable, with application to classiﬁcation problems.  Time series versions of MARS have been developed for forecasting applications  De Gooijer et al., 1998 . A Bayesian approach to MARS ﬁtting, which averages over possible models  with a consequent loss of interpretability of the ﬁnal model , is described by Denison et al.  1998b .  7.3.5 Summary  MARS is a method for modelling high-dimensional data in which the basis functions are a product of spline functions. MARS searches over threshold positions on the variables, or knots, across all variables and interactions. Once it has done this, it uses a least squares regression to provide estimates of the coefﬁcients of the model.  MARS can be used to model data comprising measurements on variables of different type. The optimal model is achieved by growing a large model and then pruning the model by removing basis functions until a lack-of-ﬁt criterion is minimised.  7.4 Application studies  Applications of decision-tree methodology are varied and include the following. ž Predicting stroke in patient rehabilitation outcome. Falconer et al.  1994  develop a classiﬁcation tree model  CART  to predict rehabilitation outcomes for stroke pati- ents. The data comprised measurements on 51 ordinal variables on 225 patients. A classiﬁcation tree was used to identify those variables most informative for predict- ing favourable and unfavourable outcomes. The resulting tree used only 4 of the 51 variables measured on admission to a university-afﬁliated rehabilitation institute, im- proving the ability to predict rehabilitation outcomes, and correctly classifying 88% of the sample.   246 Tree-based methods  ž Gait events. In a study into the classiﬁcation of phases of the gait cycle  as part of the development of a control system for functional electrical stimulation of the lower limbs , Kirkwood et al.  1989  develop a decision-tree approach that enable redundant combinations of variables to be identiﬁed. Efﬁcient rules are derived with high performance accuracy.  ž Credit card applications. Carter and Catlett  1987  use Quinlan’s  1986  ID3 algorithm  to assess credit card applications.  ž Thyroid diseases. In a study of the use of a decision tree to synthesise medical knowl- edge, Quinlan  1986  employs C4, a descendant of ID3 that implements a pruning algorithm, to generate a set of high-performance rules. The data consist of input from a referring doctor  patient’s age, sex and 11 true–false indicators , a clinical laboratory  up to six assay results  and a diagnostician. Thus, variables are of mixed type, with missing values and some misclassiﬁed samples. The pruning algorithm leads to an improved simplicity and intelligibility of derived rules.  Other applications have been in the areas of telecommunications, marketing and in-  dustrial applications.  There have been several comparisons with neural networks and other discrimination  methods: ž Digit recognition. Brown et al.  1993  compare a classiﬁcation tree with a multilayer perceptron on a digit recognition  extracted from licence plate images  problem, with application to highway monitoring and tolling. All features were binary and the classi- ﬁcation tree performance was poorer than the MLP, but performance improved when features that were a combination of the original variables were included.  ž Various data sets. Curram and Mingers  1994  compare a multilayer perceptron with a decision tree on several data sets. The decision tree was susceptible to noisy data, but had the advantage of providing insight. Shavlik et al.  1991  compared ID3 with a multilayer perceptron on four data sets, ﬁnding that the MLP handled noisy data and missing features slightly better than ID3, but took considerably longer to train.  Other comparative studies include speaker-independent vowel classiﬁcation and load forecasting  Atlas et al., 1989 , ﬁnding the MLP superior to a classiﬁcation tree; disk drive manufacture quality control and the prediction of chronic problems in large-scale communication networks  Apt´e et al., 1994 .  The MARS methodology has been applied to problems in classiﬁcation and regression  including the following. ž Economic time series. Sephton  1994  uses MARS to model three economic time series: annual US output, capital and labour inputs; interest rates and exchange rates using a generalised cross-validation score for model selection.  ž Telecommunications. Duffy et al.  1994  compare neural networks with CART and MARS on two telecommunications problems: modelling switch processor memory, a regression problem; and characterising trafﬁc data  speech and modem data at three different baud rates , a four-class discrimination problem.   Summary and discussion 247  ž Particle detection. In a comparative study of four methods of discrimination, Holm- str¨om and Sain  1997  compare MARS with a quadratic classiﬁer, a neural network and kernel discriminant analysis on a problem to detect a weak signal against a domi- nant background. Each event  in the two-class problem  was described by 14 variables and the training set comprised 5000 events. MARS appeared to give best performance.  7.5 Summary and discussion  Recursive partitioning methods have a long history and have been developed in many different ﬁelds of endeavour. Complex decision regions can be approximated by the union of simpler decision regions. A major step in this research was the development of CART, a simple nonparametric method of partitioning data. The approach described in this chapter for constructing a classiﬁcation tree is based on that work.  There have been many comparative studies with neural networks, especially multilayer perceptrons. Both approaches are capable of modelling complex data. The MLP is usually longer to train and does not provide the insight of a tree, but has often shown better performance on the data sets used for the evaluation  which may favour an MLP anyway . Further work is required.  The multivariate adaptive regression spline approach is a recursive partitioning method that utilises products of spline functions as the basis functions. Like CART, it is also well suited to model mixed variable  discrete and continuous  data.  7.6 Recommendations  Although it cannot be said that classiﬁcation trees perform substantially better than other methods  and, for a given problem, there may be a parametric method that will work better – but you do not know which one to choose , their simplicity and consistently good performance on a wide range of data sets have led to their widespread use in many disciplines. It is recommended that you try them for yourselves. Speciﬁcally, classiﬁcation tree approaches are recommended:  1. for complex data sets in which you believe decision boundaries are nonlinear and  decision regions can be approximated by the sum of simpler regions;  2. for problems where it is important to gain an insight into the data structure and the classiﬁcation rule, when the explanatory power of trees may lead to results that are easier to communicate than other techniques;  3. for problems with data consisting of measurements on variables of mixed type  con-  tinuous, ordinal, nominal ;  4. where ease of implementation is required;  5. where speed of classiﬁcation performance is important – the classiﬁer performs simple  tests on  single  variables.   248 Tree-based methods  MARS is simple to use and is recommended for high-dimensional regression prob- lems, for problems involving variables of mixed type and for problems where some degree of interpretability of the ﬁnal solution is required.  7.7 Notes and references  There is a very large literature on classiﬁcation trees in the areas of pattern recognition, artiﬁcial intelligence, statistics and the engineering sciences, but by no means exclusively within these disciplines. Many developments have taken place by researchers working independently and there are many extensions of the approach described in this chapter as well as many alternatives. A survey is provided by Safavian and Landgrebe  1991 ; see also Feng and Michie  1994 . Several tree-based approaches were assessed as part of the Statlog project  Michie et al., 1994  including CART, which proved to be one of the better ones because it incorporates costs into the decision.  MARS was introduced by Friedman  1991 . Software for CART, other decision-tree  software and MARS is publicly available.  The website www.statistical-pattern-recognition.net contains refer-  ences and links to further information on techniques and applications.  Exercises  1. In Figure 7.6, if regions u.4  and u.6  correspond to class !1 and u.8 ; u.9 , u.10  and u.11  to class !2, construct a multilayer perceptron with the same decision boundaries.  2. A standard classiﬁcation tree produces binary splits on a single variable at each node. For a two-class problem, using the results of Chapter 4, describe how to construct a tree that splits on a linear combination of variables at a given node.  3. The predictability index  relative decrease in the proportion of incorrect predictions  for split s on node t  is written  using the notation of this chapter   − .!js  D  jD1 p2.! jjtL   pL C PC PC 1  cid:6  PC  jD1 p2.! jjtR  pR  cid:6  PC jD1 p2.! jjt    jD1 p2.! jjt    Show that the decrease in impurity when passing from one group to two subgroups for the Gini criterion can be written  1I.s; t   D CX jD1  p2.! jjtL   pL C CX jD1  p2.! jjtR  pR  cid:6  CX jD1  p2.! jjt    and hence that maximising the predictability also maximises the decrease in impurity.  4. Consider the two-class problem with bivariate distributions characterised by x1 and x2 which can take three values. The training data are given by the following tables:   Exercises 249  class !1  class !2  1 3 1 4  1 2 3  x2  x1 2 0 6 1  3 0 0 2  1 0 0 1  1 2 3  x2  x1 2 1 5 0  3 4 7 1  where, for example, there are four training samples in class !1 with x1 D 1 and x2 D 3. Using the Gini criterion, determine the split  variable and value  for the root node of a tree.  5. Construct a classiﬁcation tree using data set 1 from the exercises in Chapter 5. Initially allow 10 splits per variable. Monitor the performance on the validation set as the tree is grown. Prune using the validation set to monitor performance. Investigate the performance of the approach for p D 2; 5 and 10. Describe the results and compare with a linear discriminant analysis.  6. Show that MARS can be recast in the form  a0 C X  fi .xi   C X  fi j .xi ; x j   C X  i  i j  i jk  fi jk .xi ; x j ; xk  C ÐÐÐ  7. Write the basis functions for the regions in Figure 7.11 as sums of products of splits on the variables x1 and x2, with at most one split on a given variable in the product.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   8  Performance  Overview  Classiﬁer performance assessment is an important aspect of the pattern recognition cycle. How good is the designed classiﬁer and how well does it compare with competing techniques? Can improvements in performance be achieved with an ensemble of classiﬁers?  8.1 Introduction  The pattern recognition cycle  see Chapter 1  begins with the collection of data and initial data analysis followed, perhaps, by preprocessing of the data and then the design of the classiﬁcation rule. Chapters 2 to 7 described approaches to classiﬁcation rule design, beginning with density estimation methods and leading on to techniques that construct a discrimination rule directly. In this chapter we address two different aspects of performance: performance assessment and performance improvement.  Performance assessment, discussed in Section 8.2, should really be a part of classiﬁer design and not an aspect that is considered separately, as it often is. A sophisticated design stage is often followed by a much less sophisticated evaluation stage, perhaps resulting in an inferior rule. The criterion used to design a classiﬁer is often different from that used to assess it. For example, in constructing a discriminant rule, we may choose the parameters of the rule to optimise a squared error measure, yet assess the rule using a different measure of performance, such as error rate.  A related aspect of performance is that of comparing the performance of several classiﬁers trained on the same data set. For a practical application, we may implement several classiﬁers and want to choose the best, measured in terms of error rate or perhaps computational efﬁciency. In Section 8.3, comparing classiﬁer performance is addressed. Instead of choosing the best classiﬁer from a set of classiﬁers, we ask the question, in Section 8.4, whether we can get improved performance by combining the outputs of sev- eral classiﬁers. This has been an area of growing research in recent years and is related to developments in the data fusion literature where, in particular, the problem of decision fu- sion  combining decisions from multiple target detectors  has been addressed extensively.   252 Performance  8.2 Performance assessment  Three aspects of the performance of a classiﬁcation rule are addressed. The ﬁrst is the discriminability of a rule  how well it classiﬁes unseen data  and we focus on one particular method, namely the error rate. The second is the reliability of a rule. This is a measure of how well it estimates the posterior probabilities of class membership. Finally, the use of the receiver operating characteristic  ROC  as an indicator of performance for two-class rules is considered.  8.2.1 Discriminability  There are many measures of discriminability  Hand, 1997 , the most common being the misclassiﬁcation rate or the error rate of a classiﬁcation rule. Generally, it is very difﬁcult to obtain an analytic expression for the error rate and therefore it must be estimated from the available data. There is a vast literature on error rate estimation, but the error rate suffers from the disadvantage that it is only a single measure of performance, treating all correct classiﬁcations equally and all misclassiﬁcations with equal weight also  corresponding to a zero–one loss function – see Chapter 1 . In addition to computing the error rate, we may also compute a confusion or misclassiﬁcation matrix. The .i; j  th element of this matrix is the number of patterns of class ! j that are classiﬁed as class !i by the rule. This is useful in identifying how the error rate is decomposed. A complete review of the literature on error rate estimation deserves a volume in itself, and is certainly beyond the scope of this book. Here, we limit ourselves to a discussion of the more popular types of error rate estimator. Firstly, let us introduce some notation. Let the training data be denoted by Y D fyi ; i D 1; : : : ; ng, the pattern yi consisting of two parts, yT  , where fxi ; i D 1; : : : ; ng are the measurements and fzi ; i D 1; : : : ; ng are the corresponding class labels, now coded as a vector, .zi   j D 1 if xi 2 class ! j and zero otherwise. Let !.zi   be the corresponding categorical class label. Let the decision rule designed using the training data be  cid:6 .x; Y    that is,  cid:6  is the class to which x is assigned by the classiﬁer designed using Y   and let Q.!.z ;  cid:6 .x; Y    be the loss function  i D .xT  ; zT i  i  Q.!.z ;  cid:6 .x; Y    D  ²0 if !.z  D  cid:6 .x; Y    correct classiﬁcation  1 otherwise  Apparent error rate The apparent error rate, eA, or resubstitution rate is obtained by using the design set to estimate the error rate,  eA D 1 n  nX iD1  Q.!.zi  ;  cid:6 .xi ; Y     It can be severely optimistically biased, particularly for complex classiﬁers and a small data set when there is a danger of over-ﬁtting the data – that is, the classiﬁer models the noise on the data rather than its structure. Increasing the number of training samples reduces this bias.   Performance assessment 253  True error rate The true error rate  or actual error rate or conditional error rate , eT , of a classiﬁer is the expected probability of misclassifying a randomly selected pattern. It is the error rate on an inﬁnitely large test set drawn from the same distribution as the training data.  Expected error rate error rate over training sets of a given size, eE D E[eT ].  The expected error rate, eE , is the expected value of the true  Bayes error rate The Bayes error rate or optimal error rate, eB, is the theoretical minimum of the true error rate, the value of the true error rate if the classiﬁer produced the true posterior probabilities of group membership, p.!ijx ; i D 1; : : : ; C.  Holdout estimate The holdout method splits the data into two mutually exclusive sets, sometimes referred to as the training and test sets. The classiﬁer is designed using the training set and performance evaluated on the independent test set. The method makes inefﬁcient use of the data  using only part of it to train the classiﬁer  and gives a pessimistically biased error estimate  Devijver and Kittler, 1982 . However, it is possible to obtain conﬁdence limits on the true error rate given a set of n independent test samples, drawn from the same distribution as the training data. If the true error rate is eT , and k of the samples are misclassiﬁed, then k is binomially distributed  cid:3 n 4D  p.kjeT ; n  D Bi.kjeT ; n   .1  cid:7  eT  n cid:7 k   8.1    cid:4   ek T  k  The above expression gives the probability that k samples out of n of an independent test set are misclassiﬁed given that the true error rate is eT . Using Bayes’ theorem, we may write the conditional density of the true error rate, given the number of samples misclassiﬁed, as  p.eTjk; n  D  p.kjeT ; n  p.eT ; n  R p.kjeT ; n  p.eT ; n deT  Assuming p.eT ; n  does not vary with eT and p.kjeT ; n  is the binomial distribution, we have a beta distribution for eT ,  p.eTjk; n  D Be.eTjk C 1; n  cid:7  k C 1   4D  .1  cid:7  eT  n cid:7 k ek R ek T .1  cid:7  eT  n cid:7 kdeT  T  where Be.xjÞ; þ  D [0.ÞCþ =.0.Þ 0.þ  ]x Þ cid:7 1.1 cid:7 x þ cid:7 1. The above posterior density provides a complete account of what can be learned given the test error. However, it may be summarised in several ways, one of which is to give an upper and lower bound  a percentage point  on the true error. For a given value of Þ  for example, 0.05 , there are many intervals in which eT lies with probability 1 cid:7  Þ. These are called .1 cid:7  Þ  credible regions, or Bayesian conﬁdence intervals  O’Hagan, 1994 . Among these intervals, the highest posterior density  HPD  credible region is the one with the additional property that every point within it has a higher probability than any point outside. It is also the   254 Performance  true error  1  0.8  0.6  0.4  0.2  0  0  n D 100 n D 50 n D 20 n D 10  n D 3  0.2  0.4  0.6  test error  0.8  1  Figure 8.1 HPD credible region limits as a function of test error  number misclassiﬁed on test size of test set  for several values of n, the number of test samples, and Þ D 0.05  i.e. the 95% credible region limits . From top to bottom, the limit lines correspond to n D 3, 10, 20, 50, 100, 100, 50, 20, 10, 3 shortest .1  cid:7  Þ  credible region. It is the interval EÞ  where c is chosen such that Z   8.2   For multimodal densities, EÞ may be discontinuous. However, for the beta distribution, EÞ is a single region with lower and upper bounds ž1.Þ  and ž2.Þ   both functions of k and n  satisfying  EÞ D feT : p.eTjk; n  ½ cg  p.eTjk; n deT D 1  cid:7  Þ  EÞ  0  cid:10  ž1.Þ  < ž2.Þ   cid:10  1  Turkkan and Pham-Gia  1993  provide a general-purpose Fortran subroutine for com- puting the HPD intervals of a given density function. Figure 8.1 displays the Bayesian conﬁdence intervals as a function of test error for several values of n, the number of samples in the test set, and a value for Þ of 0.05, i.e. the bounds of the 95% credible re- gion. For example, for 4 out of 20 test samples incorrectly classiﬁed, the .1 cid:7  Þ  credible region  for Þ D 0:05  is [0:069; 0:399]. Figure 8.2 plots the maximum length of the 95% HPD credible region  over the test error  as a function of the number of test samples. For example, we can see from the ﬁgure that, to be sure of having a HPD interval of less than 0.1, we must have more than 350 test samples.  Cross-validation Cross-validation  also known as the U -method, the leave-one-out estimate or the deleted estimate  calculates the error by using n  cid:7  1 samples in the design set and testing on the remaining sample. This is repeated for all n subsets of size n  cid:7  1. For large n,   Performance assessment 255  maximum interval  0.55  0.5  0.45  0.4  0.35  0.3  0.25  0.2  0.15  0.1  0.05  0  50  100  150  200  250  300  350  400  450  500  number of test set samples  Figure 8.2 Maximum length of the 95% HPD credible region as a function of the number of test samples  it is computationally expensive, requiring the design of n classiﬁers. However, it is approximately unbiased, although at the expense of an increase in the variance of the estimator. Denoting by Y j the training set with observation x j deleted, then the cross- validation error is  ecv D 1 n  nX jD1  Q.!.z j  ;  cid:6 .x j ; Y j     One of the disadvantages of the cross-validation approach is that it may involve a considerable amount of computation. However, for discriminant rules based on mul- tivariate normal assumptions, the additional computation can be considerably reduced through the application of the Sherman–Morisson formula  Fukunaga and Kessell, 1971; McLachlan, 1992a :  .A C uuT   cid:7 1 D A   cid:7 1  cid:7  A cid:7 1uuT A cid:7 1 1 C uT A cid:7 1u   8.3   for matrix A and vector u. The rotation method or ¹-fold cross-validation partitions the training set into ¹ subsets, training on ¹  cid:7  1 and testing on the remaining set. This procedure is repeated as each subset is withheld in turn. If ¹ D n we have the standard cross-validation, and if ¹ D 2 we have a variant of the holdout method in which the training set and test set are also interchanged. This method is a compromise between the holdout method and cross- validation, giving reduced bias compared to the holdout procedure, but less computation compared to cross-validation.  The jackknife The jackknife is a procedure for reducing the bias of the apparent error rate. As an estimator of the true error rate, the apparent error rate bias is of order n cid:7 1, for n samples. The jackknife estimate reduces the bias to the second order.   256 Performance  Let tn denote a sample statistic based on n observations x1; : : : ; xn. We assume for  large m that the expectation for sample size m takes the form  E[tm] D  cid:14  C a1. cid:14    m  C a2. cid:14     m2 C O.m cid:7 3   where  cid:14  is the asymptotic value of the expectation and a1 and a2 do not depend on m. Let t . j   for the n average of the t . j   n  denote the statistic based on observations excluding x j . Finally, write t .:  n  over j D 1; : : : ; n,  Then,  n D 1 t .:  n  nX jD1  t . j   n   cid:4   C O.n cid:7 2   E[t .:   n ] D 1 n  nX jD1   cid:3   cid:14  C a1. cid:14    n  cid:7  1 C O.n cid:7 2   D  cid:14  C a1. cid:14    n  cid:7  1  From  8.4  and  8.5 , we may ﬁnd a linear combination that has bias of order n cid:7 2,  tJ D ntn  cid:7  .n  cid:7  1 t .:  tJ is termed the jackknifed estimate corresponding to tn.  n  Applying this to error rate estimation, the jackknife version of the apparent error rate,  e0 J , is given by   8.4    8.5   where eA is the apparent error rate; e.:   J D neA  cid:7  .n  cid:7  1 e.:  e0 D eA C .n  cid:7  1 .eA  cid:7  e.:   A     A  A is given by A D 1 e.:  n  nX jD1  e. j   A  where e. j   A observations,  is the apparent error rate when object  j has been removed from the  A D 1 e. j   n  cid:7  1  nX kD1;k6D j  Q.!.zk  ;  cid:6 .xk; Y j     J is of order n cid:7 2. However, as As an estimator of the expected error rate, the bias of e0 an estimator of the true error rate, the bias is still of order n cid:7 1  McLachlan, 1992a . To reduce the bias of e0  J as an estimator of the true error rate to second order, we use  eJ D eA C .n  cid:7  1 .QeA  cid:7  e.:      A   8.6   where QeA is given by  QeA D 1 n2  nX jD1  nX kD1  Q.!.zk  ;  cid:6 .xk; Y j      Performance assessment 257  The jackknife is closely related to the cross-validation method and both methods delete one observation successively to form bias-corrected estimates of the error rate. A difference is that in cross-validation, the contribution to the estimate is from the deleted sample only, classiﬁed using the classiﬁer trained on the remaining set. In the jackknife, the error rate estimate is calculated from all samples, classiﬁed using the classiﬁers trained with each reduced sample set.  Bootstrap techniques The term ‘bootstrap’ refers to a class of procedures that sample the observed distribution, with replacement, to generate sets of observations that may be used to correct for bias. Introduced by Efron  1979 , it has received considerable attention in the literature during the past decade or so. It provides nonparametric estimates of the bias and variance of an estimator and, as a method of error rate estimation, it has proved superior to many other techniques. Although computationally intensive, it is a very attractive technique, and there have been many developments of the basic approach, largely by Efron himself; see Efron and Tibshirani  1986  and Hinkley  1988  for a survey of bootstrap methods. The bootstrap procedure for estimating the bias correction of the apparent error rate  T ; i D 1; : : : ; ng. is implemented as follows. Let the data be denoted by Y D f.xT Let OF be the empirical distribution. Under joint or mixture sampling it is the distribution OFi is the with mass 1=n at each data point xi ; i D 1; : : : ; n. Under separate sampling, distribution with mass 1=ni at point xi in class !i  ni patterns in class !i  . 1. Generate a new set of data  the bootstrap sample  Y b D f.Qx T   T ; i D 1; : : : ; ng  ; QzT  ; zT i  i  i  i  according to the empirical distribution.  2. Design the classiﬁer using Y b. 3. Calculate the apparent error rate for this sample and denote it by QeA. 4. Calculate the actual error rate for this classiﬁer  regarding the set Y as the entire  population  and denote it by Qec.  5. Compute wb D QeA  cid:7  Qec. 6. Repeat steps 1–5 B times.  7. The bootstrap bias of the apparent error rate is  where the expectation is with respect to the sampling mechanism that generates the sets Y b, that is,  8. The bias-corrected version of the apparent error rate is given by  At step 1, under mixture sampling, n independent samples are generated from the distribution OF; some of these may be repeated in forming the set QY and it may happen  Wboot D E[QeA  cid:7  Qec]  Wboot D 1 B  BX bD1  wb  A D eA  cid:7  Wboot e.B    258 Performance  that one or more classes are not represented in the bootstrap sample. Under separate sampling, ni are generated using OFi ; i D 1; : : : ; C. Thus, all classes are represented in the bootstrap sample in the same proportions as the original data.  The number of bootstrap samples, B, used to estimate Wboot may be limited by computational considerations, but for error rate estimation it can be taken to be of the order of 25–100  Efron, 1983, 1990; Efron and Tibshirani, 1986 .  There are many variants of the basic approach described above  Efron, 1983; McLach- lan, 1992a . These include the double bootstrap, the randomised bootstrap and the 0.632 estimator  Efron, 1983 . The double bootstrap corrects for the bias of the ordinary boot- strap using a bootstrap to estimate the bias. The randomised bootstrap  for C D 2 classes  draws samples of size n from a data set of size 2n, Y 2n D f.xT  T ; i D 1; : : : ; ng, where zi is the opposite class to zi . Thus, the original data set is replicated with opposite class labels. The probability of choosing xi is still 1=n  for mixture sam-  T with probabilities ³i and 1 cid:7 ³i pling , but the sample is taken as .xT i respectively. Efron takes ³i equal to 0.9 for all i. This estimate was found to give a lower mean squared error over the ordinary bootstrap estimator.   T or .xT i   T ; .xT i  ; zT i  ; zT i  ; zT i  ; zT i  i  The 0.632 estimator is a linear combination of the apparent error rate and another  bootstrap error estimator, e0,  e0:632 D 0:368eA C 0:632e0  where e0 is an estimator that counts the number of training patterns misclassiﬁed that do not appear in the bootstrap sample. The number of misclassiﬁed samples is summed over all bootstrap samples and divided by the total number of patterns not in the bootstrap sample. If Ab is the set of patterns in Y , but not in bootstrap sample Y b, then  PB bD1  P  e0 D  x2Ab Q.!.z ;  cid:6 .x; Y b   PB  bD1 jAbj  where jAbj is the cardinality of the set Ab. This estimator gave best performance in Efron’s  1983  experiments.  The bootstrap may also be used for parametric distributions in which samples are generated according to the parametric form adopted, with the parameters replaced by their estimates. The procedure is not limited to estimates of the bias in the apparent error rate and has been applied to other measures of statistical accuracy, though bootstrap calculations for conﬁdence limits require more bootstrap replications, typically 1000– 2000  Efron, 1990 . More efﬁcient computational methods aimed at reducing the number of bootstrap replications compared to the straightforward Monte Carlo approach above have been proposed by Davison et al.  1986  and Efron  1990 . Application to classiﬁers such as neural networks and classiﬁcation trees produces difﬁculties, however, due to multiple local optima of the error surface.  8.2.2 Reliability  The reliability  termed imprecision by Hand, 1997  of a discriminant rule is a measure of how well the posterior probabilities of group membership are estimated by the rule.   Performance assessment 259  p.!2jx   Op.!2jx   1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0   cid:7 3   cid:7 4 Figure 8.3 Good discriminability, poor reliability  following Hand, 1994a    cid:7 1   cid:7 2  2  3  1  0  4  Thus, we are not simply interested in the class !i for which p.! jjx  is the greatest, but the value of p.! jjx  itself. Of course, we may not easily be able to estimate the reliability for two reasons. The ﬁrst is that we do not know the true posterior probabilities. Secondly, some discriminant rules do not produce estimates of the posterior probabilities explicitly. Figure 8.3 illustrates a rule with good discriminability but poor reliability for a two- class problem with equal priors. An object, x, is assigned to class !2 if p.!2jx  > p.!1jx , or p.!2jx  > 0:5. The estimated posterior probabilities Op.!ijx  lead to good discriminability in the sense that the decision boundary is the same as a discrimi- Op.!2jx  D 0:5 at the same point nant rule using the true posterior probabilities  i.e. as p.!2jx  D 0:5 . However, the true and estimated posterior probabilities differ.  Why should we want good reliability? Is not good discriminability sufﬁcient? In some cases, a rule with good discriminability may be all that is required. We may be satisﬁed with a rule that achieves the Bayes optimal error rate. On the other hand, if we wish to make a decision based on costs, or we are using the results of the clas- siﬁer in a further stage of analysis, then good reliability is important. Hand  1997  proposes a measure of imprecision obtained by comparing an empirical sample statis- tic with an estimate of the same statistic computed using the classiﬁcation function Op.!ijx ,  R D CX jD1  1 n  nX iD1  ý cid:18  j .xi  [z ji  cid:7  Op.! jjxi  ] cid:9   where z ji D 1 if xi 2 class ! j , 0 otherwise, and  cid:18  j is a function that determines the test statistic  for example,  cid:18  j .xi   D .1  cid:7  Op.! jjxi   2 .  Obtaining interval estimates for the posterior probabilities of class membership is an- other means of assessing the reliability of a rule and is discussed by McLachlan  1992a , both for the multivariate normal class-conditional distribution and in the case of arbitrary class-conditional probability density functions using a bootstrap procedure.   260 Performance  8.2.3 ROC curves for two-class rules  Introduction The receiver operating characteristic  ROC  curve was introduced in Chapter 1, in the context of the Neyman–Pearson decision rule, as a means of characterising the perfor- mance of a two-class discrimination rule and provides a good means of visualising a classiﬁer’s performance in order to select a suitable decision threshold. The ROC curve is a plot of the true positive rate on the vertical axis against the false positive rate on the horizontal axis. In the terminology of signal detection theory, it is a plot of the probability of detection against the probability of false alarm, as the detection threshold is varied. Epidemiology has its own terminology: the ROC curve plots the sensitivity against 1  cid:7  Se, where Se is the speciﬁcity. In practice, the optimal ROC curve  the ROC curve obtained from the true class- conditional densities, p.xj!i    is unknown, like error rate. It must be estimated using a trained classiﬁer and an independent test set of patterns with known classes, although, in common with error rate estimation, a training set reuse method such as cross-validation or bootstrap methods may be used. Different classiﬁers will produce different ROC curves characterising performance of the classiﬁers.  Often however, we may want a single number as a performance indicator of a classi- ﬁer, rather than a curve, so that we can compare the performance of competing classiﬁer schemes.  In Chapter 1 it was shown that the minimum risk decision rule is deﬁned on the basis of the likelihood ratio  see equation  1.15  ; Assuming that there is no loss with correct classiﬁcation, x is assigned to class !1 if p.xj!1  p.xj!2   ½21 p.!2  ½12 p.!1   where ½ ji is the cost of assigning a pattern x to !i when x 2 ! j , or alternatively   8.7   >  ;  p.!1jx  >  ½21  ½12 C ½21   8.8   and thus corresponds to a single point on the ROC curve determined by the relative costs and prior probabilities. The loss is given by  equation  1.11   L D ½21 p.!2 ž2 C ½12 p.!1 ž1   8.9   where p.!i   are the class priors and ži is the probability of misclassifying a class !i object. The ROC curve plots 1  cid:7  ž1 against ž2. In the ROC curve plane  that is, the  1 cid:7  ž1, ž2  plane , lines of constant loss  termed iso-performance lines by Provost and Fawcett, 2001  are straight lines at gradients of ½21 p.!2 =½12 p.!1   see Figure 8.4 , with loss increasing from top left to bottom right in the ﬁgure. Legitimate values for the loss are those for which the loss contours intercept the ROC curve  that is, a possible threshold on the likelihood ratio exists . The solution with minimum loss is that for which the loss contour is tangential with the ROC curve– the point where the ROC curve has gradient ½21 p.!2 =½12 p.!1 . There are no other loss contours that intercept the ROC curve with lower loss.  For different values of the relative costs and priors, the loss contours are at different gradients, in general, and the minimum loss occurs at a different point on the ROC curve.   Performance assessment 261  increasing loss  cid:1    cid:1    cid:1   cid:1  cid:2   1  cid:7  ž1  1  0.8  0.6  0.4  0.2  0  0  0.2  0.4  0.6  0.8  1  ž2  Figure 8.4 ROC curve with selected loss contours  straight lines  superimposed  Practical considerations In many cases, the misclassiﬁcation costs, ½12 and ½21, are unknown and it unreasonable to assume equality  leading to the Bayes rule for minimum error . An alternative strategy 4D p.!1jx  for samples from each of the is to compare the overall distribution of Op.x  classes !1 and !2. We would expect that the values of p.!1jx  are greater for samples x from class !1 than for samples x from class !2. Generally, the larger the difference between the two distributions, the better the classiﬁer. A measure of the separation of these two distributions is the area under the ROC curve  AUC . This provides a single numeric value, based on the ROC curve, that ignores the costs, ½i j . Thus, in contrast to the error rate, which assumes equal misclassiﬁcation costs, it assumes nothing whatever is known about misclassiﬁcation costs and thus is not inﬂuenced by factors that relate to the application of the rule. Both of these assumptions are unrealistic in practice since, usually, something will be known about likely values of the relative cost ½12=½21. Also, the advantage of the AUC as a measure of performance  namely, that it is independent of the threshold applied to the likelihood ratio  can be a disadvantage when comparing rules. If two ROC curves cross each other, then in general one will be superior for some values of the threshold and the other superior for other values of the threshold. AUC fails to take this into account.  Interpretation Let Op.x  D p.!1jx , the estimated probability that an object x belongs to class !1. Let f . Op  D f . Op.x j!1  be the probability density function for Op values for patterns in class !1, and g. Op  D g. Op.x j!2  be the probability density function for Op values for patterns in class !2. If F . Op  and G. Op  are the cumulative distribution functions, then the ROC curve is a plot of 1 cid:7  F . Op  against 1 cid:7  G. Op   see the exercise at the end of the chapter .   262 Performance  The area under the curve is given by  Z  Z  .1  cid:7  F .u  dG.u  D 1  cid:7   F .u g.u  du   8.10   or alternatively  Z  G.u d F .u  D  Z  G.u  f .u  du  For an arbitrary point   8.11  Op.x  D t 2 [0; 1], the probability that a randomly chosen pattern x from class !2 will have a Op.x  value smaller than t is G.t  . If t is chosen from the density f , then the probability that a randomly chosen class !2 pattern has a smaller value than a randomly chosen class !1 pattern is R G.u  f .u  du. This is the same as the A good classiﬁcation rule  a rule for which the estimated values of p.!1jx  are very different for x from each of the two classes  lies in the upper left triangle. The closer that it gets to the upper corner the better.  deﬁnition  8.11  for the area under the ROC curve.  A classiﬁcation rule that is no better than chance produces an ROC curve that follows  the diagonal from the bottom left to the top right.  Calculating the area under the ROC curve The area under the ROC curve is easily calculated by applying the classiﬁcation rule to a test set. For a classiﬁer that produces estimates of p.!1jx  directly, we can obtain values f f1; : : : ; fn1; fi D p.!1jxi  ; xi 2 !1g and fg1; : : : ; gn2; gi D p.!1jxi  ; xi 2 !2g and use these to obtain a measure of how well separated are the distributions of Op.x  for class !1 and class !2 patterns as follows  Hand and Till, 2001 . ; g1; : : : ; gn2g in increasing order and let the rank of the ith pattern from class !1 be ri . Then there are ri  cid:7  i class !2 patterns with estimated value of Op.x  less than that of the ith pattern of class !1. If we sum over class !1 test points, then we see that the number of pairs of points, one from class !1 and one from class !2, with Op.x  smaller for class !2 than for class !1 is  Rank the estimates f f1; : : : ; fn1  n1X iD1  .ri  cid:7  i   D n1X iD1  ri  cid:7  n1X iD1  i D S0  cid:7  1 2  n1.n1 C 1   where S0 is the sum of the ranks of the class !1 test patterns. Since there are n1n2 pairs, the estimate of the probability that a randomly chosen class !2 pattern has a lower estimated probability of belonging to class !1 than a randomly chosen class !1 pattern is  ²  OA D 1 n1n2  S0  cid:7  1 2  n1.n1 C 1   ¦  This is equivalent to the area under the ROC curve and provides an estimate that has been obtained using the rankings alone and has not used threshold values to calculate it.  The standard deviation of the statistic OA is  Hand and Till, 2001  s O cid:14  .1  cid:7  O cid:14    C .n1  cid:7  1 .Q0  cid:7  O cid:14  2  C .n2  cid:7  1 .Q1  cid:7  O cid:14  2   n1n2   Performance assessment 263  where  O cid:14  D S0 n1n2 Q0 D 1 .2n1 C 2n2 C 1 .n1 C n2   cid:7  Q1 6 Q1 D n1X .r j  cid:7  1 2 jD1  An alternative approach, considered by Bradley  1997 , is to construct an estimate of the ROC curve directly for speciﬁc classiﬁers by varying a threshold and then to use an integration rule  for example, the trapezium rule  to obtain an estimate of the area beneath the curve.  8.2.4 Example application study  The problem This study  Bradley, 1997  comprises an assessment of AUC as a perfor- mance measure on six pattern recognition algorithms applied to data sets characterising medical diagnostic problems.  Summary The study estimates AUC through an integration of the ROC curve and its standard deviation is calculated using cross-validation.  The data There are six data sets comprising measurements on two classes: 1. Cervical cancer. Six features, 117 patterns; classes are normal and abnormal cervical  cell nuclei.  2. Post-operative bleeding. Four features, 113 patterns  after removal of incomplete pat-  terns ; classes are normal blood loss and excessive bleeding.  3. Breast cancer. Nine features, 683 patterns; classes are benign and malignant.  4. Diabetes. Eight features, 768 patterns; classes are negative and positive test for dia-  betes.  heart disease absent.  heart disease absent.  5. Heart disease 1. Fourteen features, 297 patterns; classes are heart disease present and  6. Heart disease 2. Eleven features, 261 patterns; classes are heart disease present and  Incomplete patterns  patterns for which measurements on some features are missing  were removed from the data sets.  The models  Six classiﬁers were trained on each data set:  1. quadratic discriminant function  Chapter 2 ;  2. k-nearest-neighbour  Chapter 3 ;   264 Performance  3. classiﬁcation tree  Chapter 7 ;  4. multiscale classiﬁer method  a development of classiﬁcation trees ;  5. perceptron  Chapter 4 ;  6. multilayer perceptron  Chapter 6 .  The models were trained and classiﬁcation performance monitored as a threshold was varied in order to estimate the ROC curves. For example, for the k-nearest-neighbour clas- siﬁer, the ﬁve nearest neighbours in the training set to a test sample are calculated. If the number of neighbours belonging to class !1 is greater than L, where L D [0; 1; 2; 3; 4; 5], then the test sample is assigned to class !1, otherwise it is assigned to the second class. This gives six points on the ROC curve.  For the multilayer perceptron, a network with a single output is trained and during testing, it is thresholded at values of [0; 0:1; 0:2; : : : ; 1:0] to simulate different misclas- siﬁcation costs.  Training procedure A tenfold cross-validation scheme was used, with 90% of the samples used for training and 10% used in the test set, selected randomly. Thus, for each classiﬁer on each data set, there are ten sets of results.  The ROC curve was calculated as a decision threshold was varied for each of the test set partitions and the area under the curve calculated using trapezoidal integration. The AUC for the rule is taken to be the average of the ten AUC values obtained from the ten partitions of the data set.  8.2.5 Further developments  The aspect of classiﬁer performance that has received most attention in the literature is the subject of error rate estimation. Hand  1997  develops a much broader framework for the assessment of classiﬁcation rules, deﬁning four concepts.  Inaccuracy This is a measure of how  in effective is a classiﬁcation rule in assigning an object to the correct class. One example is error rate; another is the Brier or quadratic score, often used as an optimisation criterion for neural networks, deﬁned as  1 n  nX iD1  CX jD1  ýŽ.! jjxi    cid:7  Op.! jjxi   cid:9 2  where Op.! jjxi   is the estimated probability that pattern xi belongs to class ! j , and Ž.! jjxi   D 1 if xi is a member of class ! j and zero otherwise.  Imprecision Equivalent to reliability deﬁned in Section 8.2.2, this is a measure of the Op.! jjx , and the difference between the estimated probabilities of class membership,  unknown  true probabilities, p.! jjx .   Performance assessment 265  Inseparability This measure is evaluated using the true probabilities of belonging to a class, and so it does not depend on a classiﬁer. It measures the similarity of the true probabilities of class membership at a point x, averaged over x. If the probabilities at a point x are similar, then the classes are not separable.  Resemblance It measures the variation between the true probabilities, conditioned on the estimated ones. Does the predicted classiﬁcation separate the true classes well? A low value of resemblance is to be hoped for.  In this section we have been unable to do full justice to the elegance of the bootstrap method, and we have simply presented the basic bootstrap approach for the bias correction of the apparent error rate, with some extensions. Further developments may be found in Efron  1983, 1990 , Efron and Tibshirani  1986  and McLachlan  1992a . Further work on the AUC measure includes that of Hand and Till  2001  who develop it to the multiclass classiﬁcation problem  see also Hajian-Tilaki et al., 1997a, 1997b  and Adams and Hand  1999  who take account of some imprecisely known information on the relative misclassiﬁcation costs  see also Section 8.3.3 .  Provost and Fawcett  2001  propose an approach that uses the convex hull of ROC curves of different classiﬁers. Classiﬁers with ROC curves below the convex hull are never optimal  under any conditions on costs or priors  and can be ignored. Classiﬁers on the convex hull can be combined to produce a better classiﬁer. This idea has been widely used in data fusion and is discussed in Sections 8.3.3 and 8.4.4.  8.2.6 Summary  In this section, we have given a rather brief treatment of classiﬁcation rule performance assessment, covering three measures: discriminability, reliability  or imprecision  and the use of the ROC curve. In particular, we have given emphasis to the error rate of a classiﬁer and schemes for reducing the bias of the apparent error rate, namely cross- validation, the jackknife and the bootstrap methods. These have the advantage over the holdout method in that they do not require a separate test set. Therefore, all the data may be used in classiﬁer design.  The error rate estimators described in this chapter are all nonparametric estimators in that they do not assume a speciﬁc form for the probability density functions. Parametric forms of error rate estimators, for example based on a normal distribution model for the class-conditional densities, can also be derived. However, although parametric rules may be fairly robust to departures from the true model, parametric estimates of error rates may not be  Konishi and Honda, 1990 . Hence our concentration in this chapter on nonparametric forms. For a further discussion of parametric error rate estimators we refer the reader to the book by McLachlan  1992a .  There are many other measures of discriminability, and limiting ourselves to a single measure, the error rate, may hide important information as to the behaviour of a rule. The error rate treats all misclassiﬁcations equally: misclassifying an object from class 0 as class 1 has the same severity as misclassifying an object from class 1 as class 0. Costs   266 Performance  of misclassiﬁcation may be very important in some applications but are rarely known precisely.  The reliability, or imprecision, of a rule tells us how well we can trust the rule – how close the estimated posterior densities are to the true posterior densities. Finally, the area under the ROC curve is a measure that summarises classiﬁer performance over a range of relative costs.  8.3 Comparing classiﬁer performance  8.3.1 Which technique is best?  Are neural network methods better than ‘traditional’ techniques? Is the classiﬁer that you develop better than those previously published in the literature? There have been many comparative studies of classiﬁers and we have referenced these in previous chapters. Perhaps the most comprehensive study is the Statlog project  Michie et al., 1994  which provides a study of more than 20 different classiﬁcation procedures applied to about 20 data sets. Yet comparisons are not easy. Classiﬁer performance varies with the data set, sample size, dimensionality of the data, and skill of the analyst. There are some important issues to be resolved, as outlined by Duin  1996 :  1. An application domain must be deﬁned. Although this is usually achieved by specify- ing a collection of data sets, these may not be representative of the problem domain that you wish to consider. Although a particular classiﬁer may perform consistently badly on these data sets, it may be particularly suited to the one you have.  2. The skill of the analyst needs to be considered  and removed if possible . Whereas some techniques are fairly well deﬁned  nearest-neighbour with a given metric , others require tuning. Can the results of classiﬁcations, using different techniques, on a given data set performed by separate analysts be sensibly compared? If one technique performs better than others, is it due to the superiority of the technique on that data set or the skill of the implementer in obtaining the best out of a favourite method? In fact, some classiﬁers are valuable because they have many free parameters and allow a trained analyst to incorporate knowledge into the training procedure. Others are valuable because they are largely automatic and do not require user input. The Statlog project was an attempt at developing automatic classiﬁcation schemes, encouraging minimal tuning.  Related to the second issue above is that the main contribution to the ﬁnal performance is the initial problem formulation  abstracting the problem to be solved from the customer, selecting variables and so on , again determined by the skill of the analyst. The classiﬁer may only produce second-order improvements to performance.  In addition, what is the basis on which we make a comparison–error rate, reliability,  speed of implementation, speed of testing, etc.?  There is no such thing as a best classiﬁer, but there are several ways in which  comparisons may be performed  Duin, 1996 :   Comparing classiﬁer performance 267  1. A comparison of experts. A collection of problems is sent to experts who may use  whichever technique they feel is appropriate.  2. A comparison of toolsets by nonexperts. Here a collection of toolsets is provided to  nonexperts for evaluation on several data sets.  3. A comparison of automatic classiﬁers  classiﬁers that require no tuning . This is performed by a single researcher on a benchmark set of problems. Although the results will be largely independent of the expert, they will probably be inferior to those obtained if the expert were allowed to choose the classiﬁer.  8.3.2 Statistical tests  Bounds on the error rate are insufﬁcient when comparing classiﬁers. Usually the test sets are not independent – they are common across all classiﬁers. There are several tests for determining whether one classiﬁcation rule outperforms another on a particular dataset. The question of measuring the accuracy of a classiﬁcation rule using an independent training and test set was discussed in Section 8.2. This can be achieved by constructing a conﬁdence interval or HPD region. Here we address the question: given two classiﬁers and sufﬁcient data for a separate test set, which classiﬁer will be more accurate on new test set examples?  Dietterich  1998  assesses ﬁve statistical tests, comparing them experimentally to determine the probability of incorrectly detecting a difference between classiﬁer perfor- mance when no difference exists  Type I error .  Suppose that we have two classiﬁers, A and B. Let  n00 D number of samples misclassiﬁed by both A and B n01 D number of samples misclassiﬁed by A but not by B n10 D number of samples misclassiﬁed by B but not by A n11 D number of samples misclassiﬁed by neither A nor B  Compute the z statistic  z D jn01  cid:7  n10j  cid:7  1 p n10 C n01  The quantity z2 is distributed approximately as  cid:21  2 with one degree of freedom. The null hypothesis  that the classiﬁers have the same error  can be rejected  with probability of incorrect rejection of 0.05  if jzj > 1:96. This is known as McNemar’s test or the Gillick test.  8.3.3 Comparing rules when misclassiﬁcation costs  are uncertain  Introduction Error rate or misclassiﬁcation rate, discussed in Section 8.2, is often used as a criterion for comparing several classiﬁers. It requires no choice of costs, making the assumption that misclassiﬁcation costs are all equal.   268 Performance  An alternative measure of performance is the area under the ROC curve  see also Section 8.2 . This is a measure of the separability of the two distributions f . Op , the probability distribution of Op D p.!1jx  for patterns x in class !1, and g. Op , the prob- ability distribution of Op for patterns x in class !2. It has the advantage that it does not depend on the relative costs of misclassiﬁcation.  There are difﬁculties with the assumptions behind both of these performance measures. In many, if not most, practical applications the assumption of equal costs is unrealistic. Also the minimum loss solution, which requires the speciﬁcation of costs, is not sensi- ble since rarely are costs and priors known precisely. In many real-world environments, misclassiﬁcation costs and class priors are likely to change over time as the environment may change between design and test. Consequently, the point on the ROC curve corre- sponding to the minimum loss solution  where the threshold on the likelihood ratio is ½21 p.!2 =½12 p.!1  – equation  8.7   changes. On the other hand, usually something is known about the relative costs and it is therefore inappropriate to summarise over all possible values.  ROC curves Comparing classiﬁers on the basis of AUC is difﬁcult when the ROC curves cross. Only in the case of one classiﬁer dominating another will the AUC be a valid criterion for comparing different classiﬁers. If two ROC curves cross, then one curve will be superior for some values of the cost ratio and the other classiﬁer will be superior for different values of the cost ratio. Two approaches for handling this situation are presented here.  LC index In this approach for comparing two classiﬁers A and B, the costs of mis- classiﬁcation, ½12 and ½21, are rescaled so that ½12C ½21 D 1 and the loss  8.9  calculated as a function of ½21 for each classiﬁer. A function, L.½21 , is deﬁned to take the value C1 in regions of the [0; 1] interval for which classiﬁer A is superior  it has a lower loss value than classiﬁer B  and  cid:7 1 in regions for which classiﬁer B is superior. The conﬁdence in any value of ½21 is the probability density function D.½21 , deﬁned later, and the LC index is deﬁned as  Z 1  0  D.½ L.½ d½  which ranges over š1, taking positive values when classiﬁer A is more likely to lead to a smaller loss value than classiﬁer B, and negative values when classiﬁer B is more likely to lead to a smaller loss value than classiﬁer A. A value of C1 means that A is certain to be a superior classiﬁer since it is superior for all feasible values of ½21.  How do we decide on the set of feasible values of ½21? That is, what form do we choose for the distribution D.½21 ? One proposal is to specify an interval [a; b] for the cost ratio, ½12=½21, and a most likely value, m, and use this to deﬁne a unit-area triangle with base [a; b] and apex at m. This is because, it is argued  Adams and Hand, 1999 , that experts ﬁnd it convenient to specify a cost ratio ½12=½21 and an interval for the ratio.  The ROC convex hull method In this method a hybrid classiﬁcation system is con- structed from the set of available classiﬁers. For any value of the cost ratio, the combined   Comparing classiﬁer performance 269  a  A  B  C  b  True positive  1  0.8  0.6  0.4  0.2  0  0  0.2  0.4 0.6 False positive  0.8  1  Figure 8.5 ROC convex hull method illustration  classiﬁer will perform at least as well as the best classiﬁer. The combined classiﬁer is constructed to have an ROC curve that is the convex hull of the component classiﬁers. Figure 8.5 illustrates the ROC convex hull method. For some values of costs and priors, the slope of the iso-performance lines is such that the optimal classiﬁer  the point on the ROC curve lying to the top left  is classiﬁer B. Line þ is the iso-performance line with lowest loss that intercepts the ROC curve of classiﬁer B. For much shallower gradients of the iso-performance lines  corresponding to different values of the priors or costs , the optimal classiﬁer is classiﬁer A. Here, line Þ is the lowest value iso- performance line  for a given value of priors and costs  that intercepts the ROC curve of classiﬁer A. Classiﬁer C is not optimal for any value of priors or costs. The points on the convex hull of the ROC curves deﬁne optimal classiﬁers for particular values of priors and costs. Provost and Fawcett  2001  present an algorithm for generating the ROC convex hull.  In practice, we need to store the range of threshold values  ½21 p.!2 =½12 p.!1   for which a particular classiﬁer is optimal. Thus, the range of the threshold is partitioned into regions, each of which is assigned a classiﬁer, the one that is optimal for that range of thresholds.  8.3.4 Example application study  The problem This study  Adams and Hand, 1999  develops an approach for comparing the performance of two classiﬁers when misclassiﬁcation costs are uncertain, but not completely unknown. It is concerned with classifying customers according to their likely response to a promotional scheme.   270 Performance  Summary The LC index above is evaluated to compare a neural network classiﬁer  Chapter 6  with quadratic discriminant analysis  Chapter 2 .  The data The data comprise 8000 records  patterns  of measurements on 25 variables, mainly describing earlier credit card transaction behaviour. The classes are denoted class !1 and class !2, with class !2 thought likely to return a proﬁt. The priors are set as p.!1  D 0:87; p.!2  D 0:13.  The model The multilayer perceptron had 25 input nodes and 13 hidden nodes, trained using ‘weight decay’ to avoid over-ﬁtting, with the penalty term chosen by cross- validation.  Training procedure The LC index and AUC were computed. In order to obtain suit- able values for the ratio of costs, banking experts were consulted and a model developed for the two types of misclassiﬁcation based on factors such as cost of manufacture and distribution of marketing material, cost due to irritation caused by receiving junk mail and loss of potential proﬁt by failing to mail a potential member of class !2.  An interval of possible values for the overall cost ratio ½12=½21 was derived as [0.065,  0.15], with the most probable value at 0.095.  Results The AUC values for the neural network classiﬁer and quadratic discriminant analysis were 0.7102 and 0.7244 respectively, suggesting that the quadratic discriminant is slightly preferable. The LC index was calculated to be  cid:7 0:4, also suggesting that quadratic discriminant analysis is to be preferred.  8.3.5 Further developments  Adams and Hand  2000  present some guidelines for better methodology for comparing classiﬁers. They identify ﬁve common deﬁciencies in the practice of classiﬁer perfor- mance assessment:  1. Assuming equal costs. In many practical applications, the two types of misclassiﬁca-  tion are not equal.  2. Integrating over costs. The AUC summarises performance over the entire range of costs. It is more likely that something will be known about costs and that a narrower range would be more appropriate.  3. Crossing ROC curves. The AUC measure is only appropriate if one ROC curve dominates over the entire range. If the ROC curves cross, then different classiﬁers will dominate for different ranges of the misclassiﬁcation costs.  4. Fixing costs. It is improbable that exact costs can be given in many applications.  5. Variability. Error rate and the AUC measure are sample-based estimates. Standard  errors should be given when reporting results.   Combining classiﬁers 271  8.3.6 Summary  There are several ways in which performance may be compared. It is important to use an assessment criterion appropriate to the real problem under investigation. Misclassiﬁ- cation costs should be taken into account since they can inﬂuence the choice of method. Assuming equal misclassiﬁcation costs is very rarely appropriate. Usually something can be said about costs, even if they are not known precisely.  8.4 Combining classiﬁers  8.4.1 Introduction  The approach to classiﬁer design commonly taken is to identify a candidate set of plausi- ble models, to train the classiﬁers using a training set of labelled patterns and to adopt the classiﬁer that gives the best generalisation performance, estimated using an independent test set assumed representative of the true operating conditions. This results in a single ‘best’ classiﬁer that may then be applied throughout the feature space. Earlier in this chapter we addressed the question of how we measure classiﬁer performance to select a ‘best’ classiﬁer.  We now consider the potential of combining classiﬁers for data sets with complex decision boundaries. It may happen that, out of our set of classiﬁers, no single classiﬁer is clearly best  using some suitable performance measure, such as error rate . However, the set of misclassiﬁed samples may differ from one classiﬁer to another. Thus, the classiﬁers may give complementary information and combination could prove useful. A simple example is illustrated in Figure 8.6. Two linear classiﬁers, Cl1 and Cl2, are deﬁned on a univariate data space. Classiﬁer Cl1 predicts class ž for data points to the left of B and class Š for points to the right of B. Classiﬁer Cl2 predicts class ž for points to the right of A and class Š for points to the left. Neither classiﬁer obtains 100% performance on the data set. However, 100% performance is achieved by combining them with the rule: assign x to ž if Cl1 and Cl2 predict ž else assign x to Š.  Š  Cl2  ž  Š  cid:3    cid:3    cid:3    cid:3   Cl1   cid:3  ž   cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4   ŠŠ ž ž ž ž ž   cid:3   cid:3  ž  ŠŠŠ   cid:3    cid:3    cid:3   Š  ž  Š ŠŠ Š  cid:3   cid:3  B  A  Figure 8.6 Two linear discriminants deﬁned on a univariate data space   272 Performance  The idea of combining classiﬁers is not a new one, but one that has received increasing attention in recent years. Early work on multiclass discrimination developed techniques for combining the results of two-class discrimination rules  Devijver and Kittler, 1982 . Also, recursive partitioning methods  for example, CART; see Chapter 7  lead to the idea of deﬁning different rules for different parts of a feature space. The terms ‘dynamic classiﬁer selection’  Woods et al., 1997  and ‘classiﬁer choice system’  Hand et al., 2001  have been used for classiﬁer systems that attempt to predict the best classiﬁer for a given region of feature space. The term ‘classiﬁer fusion’ or ‘multiple classiﬁer system’ usually refers to the combination of predictions from multiple classiﬁers to yield a single class prediction.  8.4.2 Motivation  There are several ways of characterising multiple classiﬁer systems, as indeed there are with basic component classiﬁers. We deﬁne three broad categories as follows.  C1. Different feature spaces This describes the combination of a set of classiﬁers, each designed on different feature spaces  perhaps using data from different sensors . For example, in a person veriﬁcation application, several classiﬁer systems may be designed for use with different sensor data  for example, retina scan, facial image, handwritten signature  and we wish to combine the outputs of each system to improve performance. Figure 8.7 illustrates this situation. A set of L sensors  S1; S2; : : : ; SL  provides measurements, x1; x2; : : : ; xL, on an object. Associated with each sensor is a classiﬁer  Cl1; Cl2; : : : ; ClL  providing, in this case, estimates of the posterior probabilities of class membership, p.cjxi   for sensor  x1  x2  xL  S1  S2  SL  Cl1  Cl2  ClL  p cx1   p cx2   p cxL   Com  p cx1, . . . , xL   Figure 8.7 Classiﬁer fusion architecture C1–component classiﬁers deﬁned on different feature spaces   Combining classiﬁers 273  Si . The combination rule  denoted Com in the ﬁgure , which is itself a classiﬁer deﬁned on a feature space of posterior probabilities, combines these posterior probabilities to provide an estimate of p.cjx1; : : : ; xL  . Thus, the individual classiﬁers can be thought of as performing a particular form of feature extraction prior to classiﬁcation by the combiner.  The question usually addressed with this architecture is: given the component clas- siﬁers, what is the best combination rule? This is closely related to dynamic clas- siﬁer selection and to architectures developed in the data fusion literature  see Section 8.4.4 .  C2. Common feature space In this case, we have a set of classiﬁers, Cl1; : : : ; ClL, each deﬁned on the same feature space and the combiner attempts to obtain a ‘better’ classiﬁer through combination  see Figure 8.8 . The classiﬁers can differ from each other in several ways.  1. The classiﬁers may be of different type, belonging to the set of favourite classiﬁers of a user: for example, nearest-neighbour, neural network, decision tree and linear discriminant.  2. They may be of similar type  for example, all linear discriminant functions or all neural network models  but trained using different training sets  or subsets of a larger training set , perhaps gathered at different times or with different noise realisations added to the input.  3. The classiﬁers may be of similar type, but with different random initialisation of the classiﬁer parameters  for example, the weights in a neural network of a given architecture  in an optimisation procedure.  In contrast with category C1, a question that we might ask here is: given the combi- nation rule, what is the best set of component classiﬁers? Equivalently, in case 2 above, for example, how do we train our neural network models to give the best performance on combination?  The issue of accuracy and diversity of component classiﬁers  Hansen and Salamon, 1990  is important for this multiple classiﬁer architecture. Each component classiﬁer is  S  Com  p cx   x  x  x  Cl1  p1 cx   p2 cx   pL cx   Cl2  ClL  Figure 8.8 Classiﬁer fusion architecture C2–component classiﬁers deﬁned on a common feature space of measurements x made by sensor S   274 Performance  said to be accurate if it has an error rate lower than that obtained by random guessing on new patterns. Two classiﬁers are diverse if they make different errors in predicting the class of a pattern, x. To see why both accuracy and diversity are important, consider an ensemble of three classiﬁers h1.x ; h2.x  and h3.x , each predicting a class label as output. If all classiﬁers produce identical outputs, then there will be no improvement gained by combining the outputs: when h1 is incorrect, h2 and h3 will also be incorrect. However, if the classiﬁer outputs are uncorrelated, then when h1 is incorrect, h2 and h3 may be correct and, if so, a majority vote will give the correct prediction. More speciﬁcally, consider L classiﬁers h1; : : : ; h L, each with an error rate of p < 1 2 . For the majority vote to be incorrect, we require that L=2 or more classiﬁers be incorrect. The probability that R classiﬁers are incorrect is  The probability that the majority vote is incorrect is therefore less than1  L!  R!.L  cid:7  R !  p R.1  cid:7  p L cid:7 R  LX  RDb.LC1 =2c  L!  R!.L  cid:7  R !  p R.1  cid:7  p L cid:7 R  the area under the binomial distribution where at least L=2 are incorrect b:c denotes the integer part . For example, with L D 11 classiﬁers, each with an error rate p D 0:25, the probability of six or more classiﬁers being incorrect is 0.034, which is much less than the individual error rate.  If the error rates of the individual classiﬁers exceed 0.5, then the error rate of the ma- jority vote will increase, depending on the number of classes and the degree of correlation between the classiﬁer outputs.  In both cases C1 and C2, each classiﬁer is performing a particular form of feature extraction for inputs to the combiner classiﬁer. If this combiner classiﬁer is not ﬁxed  that is, it is allowed to adapt to the forms of the input , then the general problem is one in which we seek the best forms of feature extractor matched to a combiner  see Figure 8.9 . There is no longer a requirement that the outputs of the classiﬁers Cl1; : : : ; ClL are estimates of posterior probabilities  are positive and sum to unity over classes . Indeed, these may not be the best features. Thus the component classiﬁers are not classiﬁers at all and the procedure is essentially the same as many described elsewhere in this book – neural networks, projection pursuit, and so on. In this sense, there is no need for research on classiﬁer combination methods since they impose an unnecessary restriction on the forms of the features input to the combiner.  C3. Repeated measurements The ﬁnal category of combination systems arise due to different classiﬁcation of an object through repeated measurements. This may occur when we have a classiﬁer designed on a feature space giving an estimate of the posterior probabilities of class membership, but in practice several  correlated  measurements may be made on the object  see Figure 8.10 .  1For more than two classes, the majority vote could still produce a correct prediction even if more than  L=2 classiﬁers are incorrect.   Combining classiﬁers 275  S  Com  p cx   x  x  x  Cl1  Cl2  ClL  x1  x2  xL  Figure 8.9 Classiﬁer fusion architecture C2–component classiﬁers deﬁned on a common feature space with L component classiﬁers delivering features xi  S  S  S  x1  x2  xT  Cl  Cl  Cl  p cx1   p cx2   p cxT   Com  p cx1, . . . , xT   Figure 8.10 Classiﬁer fusion architecture C3 repeated measurements on a common feature space. Sensor S produces a sequence of measurements xi; i D 1; : : : ; T, which are input to classiﬁer Cl  An example is that of recognition of aircraft from visual or infrared data. A probability density function of feature vectors may be constructed using a training data set. In the practical application, successive measurements are available from the sensor. How can we combine these predictions? This is sometimes described as multiple observation fusion or temporal fusion.  8.4.3 Characteristics of a combination scheme  Combination schemes may themselves be classiﬁed according to several characteristics, including their structure, input data type, form of component classiﬁers and training   276 Performance  requirements. In this section, we summarise some of the main features of a combiner. We assume that there are measurements from C classes and there are L component classiﬁers.  Level Combination may occur at different levels of component classiﬁer output.  L1. Data level Raw sensor measurements are passed to a combiner that produces an estimate of the posterior probabilities of class membership. This simply amounts to deﬁning a classiﬁer on the augmented feature space comprising measurements on all sensor variables. That is, for sensors producing measurements x; y and z, a classiﬁer is deﬁned on the feature vector .x; y; z . In the data fusion literature  see Section 8.4.4  this is referred to as a centralised system: all the information collected at distributed sensors is passed to a central processing unit that makes a ﬁnal decision. A consequence of this procedure is that the combiner must be constructed using data of a high dimen- sionality. Consequently, it is usual to perform some feature selection or extraction prior to combination.  L2. Feature level Each constituent classiﬁer  we shall use the term ‘classiﬁer’ even though the output may not be an estimate of the posterior probabilities of class mem- bership or a prediction of class  performs some local preprocessing, perhaps to reduce dimensionality. This could be important in some data fusion applications where the communication bandwidth between constituent classiﬁers and combiner is an important consideration. In data fusion, this is termed a decentralised system. The features derived by the constituent classiﬁer for input  transmission  to the combiner can take several forms, including the following.  1. A reduced-dimensional representation, perhaps derived using principal components  analysis.  2. An estimate of the posterior probabilities of class membership. Thus, each constituent processor is itself a classiﬁer. This is a very speciﬁc form of preprocessing that may not be optimum, but it may be imposed by application constraints.  3. A coding of the constituent classiﬁer’s input. For an input, x, the output of the constituent classiﬁer is the index, y, in a codebook of vectors corresponding to the codeword, z, which is nearest to x. At the combiner, the code index y is decoded to produce an approximation to x  namely, z . This is then used, together with approx- imations of the other constituent classiﬁer inputs, to produce a classiﬁcation. Such a procedure is important if there are bandwidth constraints on the links between con- stituent classiﬁer and combiner. Procedures for vector quantisation  see Chapter 10  are relevant to this process.  L3. Decision level Each constituent classiﬁer produces a unique class label. The combiner classiﬁer is then deﬁned on an L-dimensional space of categorical variables, each taking one of C values. Techniques for classiﬁer construction on discrete variables  for example, histograms and generalisations – maximum weight dependence trees and Bayesian networks – see Chapter 3  are appropriate.   Combining classiﬁers 277  Degree of training R1. Fixed classiﬁers In some cases, we may wish to use a ﬁxed combination rule. This may occur if the training data used to design the constituent classiﬁers are unavailable, or different training sets have been used.  R2. Trainable classiﬁers Alternatively, the combination rule is adjusted based on knowledge of the training data used to deﬁne the constituent classiﬁers. This knowledge can take different forms:  1. Probability density functions. The joint probability density function of constituent clas- siﬁer outputs is assumed known for each class through knowledge of the distribution of the inputs and the form of the classiﬁers. For example, in the two-class target detection problem, under the assumption of independent local decisions at each of the constituent classiﬁers, an optimal detection rule for the combiner can be derived, expressed in terms of the probability of false alarm and the probability of missed detection at each sensor  Chair and Varshney, 1986 .  2. Correlations. Some knowledge concerning the correlations between constituent clas- siﬁer outputs is assumed. Again, in the target detection problem under correlated local decisions  correlated outputs of constituent classiﬁers , Kam et al.  1992  ex- pand the probability density function using the Bahadur–Lazarsfeld polynomials to rewrite the optimal combiner rule in terms of conditional correlation coefﬁcients  see Section 8.4.4 .  3. Training data available. It is assumed that the outputs of each of the individual constituent classiﬁers are known for a given input of known class. Thus, we have a set of labelled samples that may be used to train the combiner classiﬁer.  Form of component classiﬁers F1. Common form Classiﬁers may all be of the same form. For example, they all may be neural networks  multilayer perceptrons  of a given architecture, all linear dis- criminants or all decision trees. The particular form may be chosen for several reasons: interpretability  it is easy to interpret the classiﬁcation process in terms of simple rules deﬁned on the input space ; implementability  the constituent classiﬁers are easy to imple- ment and do not require excessive computation ; adaptability  the constituent classiﬁers are ﬂexible and it is easy to implement diverse classiﬁers whose combination leads to a lower error rate than any of the individuals .  F2. Dissimilar form The constituent classiﬁers may be a collection of neural networks, decision trees, nearest-neighbour methods and so on, the set perhaps arising through the analysis of a wide range of classiﬁers on different training sets. Thus, the classiﬁers have not necessarily been chosen so that their combination leads to the best improve- ment.  Structure The structure of a multiple classiﬁer system is often dictated by a practical application.   278 Performance  T1. Parallel The results from the constituent classiﬁers are passed to the combiner together before a decision is made by the combiner.  T2. Serial Each constituent classiﬁer is invoked sequentially, with the results of one classiﬁer being used by the next one in the sequence, perhaps to set a prior on the classes.  T3. Hierarchical The classiﬁers are combined in a hierarchy, with the outputs of one constituent classiﬁer feeding as inputs to a parent node, in a similar manner to decision- tree classiﬁers  see Chapter 7 . Thus the partition into a single combiner with several constituent classiﬁers is less apparent, with each classiﬁer  apart from the leaf and root nodes  taking the output of a classiﬁer as input and passing its own output as input to another classiﬁer.  Optimisation Different parts of the combining scheme may be optimised separately or simultaneously, depending on the motivating problem.  O1. Combiner Optimise the combiner alone. Thus, given a set of constituent classi- ﬁers, we determine the combining rule to give the greatest performance improvement.  O2. Constituent classiﬁers Optimise the constituent classiﬁers. For a ﬁxed combiner rule, and the number and type of constituent classiﬁers, the parameters of these classiﬁers are determined to maximise performance.  O3. Combiner and constituent classiﬁers Optimise both the combiner rule and the parameters of the constituent classiﬁers. In this case, the constituent classiﬁers may not be classiﬁers in the strict sense, performing some form of feature extraction. Practical constraints such as limitations on the bandwidth between the constituent classiﬁers and the combiner may need to be considered.  O4. No optimisation We are provided with a ﬁxed set of classiﬁers and use a standard combiner rule that requires no training  see Section 8.4.5 .  8.4.4 Data fusion  Much of the work on multiple classiﬁer systems reported in the pattern recognition, statistics and machine learning literature has strong parallels, and indeed overlaps sub- stantially, with research on data fusion systems carried out largely within the engineering community  Dasarathy, 1994b; Varshney, 1997; Waltz and Llinas, 1990 . In common with the research on classiﬁer combination, different architectures may be considered  for example, serial or parallel  and different assumptions made concerning the joint distribution of classiﬁer outputs, but the ﬁnal architecture adopted and the constraints under which it is optimised are usually motivated by real problems. One application of   Combining classiﬁers 279  special interest is that of distributed detection: detecting the presence of a target using a distributed array of sensors. In this section we review some of the work in this area.  Architectures Figures 8.11 and 8.12 illustrate the two main architectures for a decentralised distributed detection system – the serial and parallel conﬁgurations. We assume that there are L sensors. The observations at each sensor are denoted by yi and the decision at each sensor by ui ; i D 1; : : : ; L, where  ui D  ²1 if ‘target present’ declared 0 if ‘target absent’ declared  and the ﬁnal decision is denoted by u0. Each sensor can be considered as a binary classiﬁer and the problem is termed as one in decision fusion. This may be thought a very restrictive model, but it is one that may arise in some practical situations.  The main drawback with the serial network structure  Figure 8.11  is that it has a serious reliability problem. This problem arises because if there is a link failure between the .i  cid:7  1 th sensor and the ith sensor then all the information used to make the previous decisions would be lost, resulting in the ith sensor becoming effectively the ﬁrst sensor in the decision process. The parallel system has been widely considered and is shown in Figure 8.12. Each sensor receives a local observation yi , i D 1; : : : ; L, and produces a local decision ui which is sent to the fusion centre. At the fusion centre all the local decisions ui , i D 1; : : : ; L, are combined to obtain a global decision u0. The parallel architecture is  y1  S1  y2  S2  u1  u2  .  .  .  .  uL −1  u0  yL  SL  Figure 8.11 Sensors arranged in a serial conﬁguration  y1  y2  yL  S1  S2  SL  u1  u2  uL−1  fusion centre  u0  Figure 8.12 Sensors arranged in a parallel conﬁguration   280 Performance  far more robust to link failure. A link failure between the ith sensor and the fusion centre does not seriously jeopardize the overall global decision, since it is only the decision of the ith sensor that is lost. The parallel distributed system has been considered under the assumptions of both correlated and independent local decisions, and methods have been proposed for solving for the optimal solution in both these cases.  The parallel decision system has also been extended to allow the local decisions to be passed to intermediate fusion centres, each processing all the L local decisions before passing their decisions on to another layer of intermediate fusion centres  Li and Sethi, 1993; Gini, 1997 . After K such layers these intermediate fusion centre decisions are passed to the fusion centre and a global decision u0 is made. This corresponds to the hierarchical model in multiple classiﬁer systems  see Section 8.4.3 .  The parallel architecture has also been used to handle repeated observations. One approach has been to use a memory term that corresponds to the decision made by the fusion centre using the last set of local decisions  Kam et al., 1999 . The memory term is used in conjunction with the next set of local decisions to make the next global decision. This memory term therefore allows the fusion centre to take into consideration the decision made on the last set of observations.  Bayesian approaches We formulate the Bayes rule for minimum risk for the parallel conﬁguration. Let class !2 be ‘target absent’ and class !1 be ‘target present’. Then the Bayes rule for minimum risk is given by equation  1.12 : declare a target present  class !1  if  ½11 p.!1ju  p.u  C ½21 p.!2ju  p.u   cid:10  ½12 p.!1ju  p.u  C ½22 p.!2ju  p.u   that is,  .½21  cid:7  ½22  p.uj!2  p.!2   cid:10  .½12  cid:7  ½11  p.uj!1  p.!1    8.12  where u D .u1; u2; : : : ; u L  T is the vector of local decisions; p.uj!i  ; i D 1; 2, are the class-conditional probability density functions; p.!i  ; i D 1; 2, are the class priors and ½ ji are the costs of assigning a pattern u to !i when u 2 ! j . In order to evaluate the fusion rule  8.12 , we require knowledge of the class-conditional densities and the costs. Several special cases have been considered. By taking the equal cost loss matrix  see Chapter 1 , and assuming independence between local decisions, the fused decision  based on the evaluation of p.!iju   can be expressed in terms of the probability of false alarm and the probability of missed detection at each sensor  see the exercises at the end of the chapter .  The problem of how to tackle the likelihood ratios when the local decisions are cor- related has been addressed by Kam et al.  1992  who showed that by using the Bahadur– Lazarsfeld polynomials to form an expansion of the probability density functions, it is possible to rewrite the optimal data fusion rule in terms of the conditional correlation coefﬁcients.  The Bahadur–Lazarsfeld expansion expresses the density p.x  as  p.x  D LY jD1  . px j j  .1  cid:7  p j  1 cid:7 x j   ð  " 1 C X  i < j   cid:22 i j zi z j C X   cid:22 i jk zi z j zk C : : :  i < j <k     where the  cid:22  s are the correlation coefﬁcients of the corresponding variables  Combining classiﬁers 281  and  so that  and   cid:22 i j D E[zi z j ]  cid:22 i jk D E[zi z j zk]  cid:22 i j :::L D E[zi z j : : : z L]  pi D P.xi D 1 ;  1  cid:7  pi D P.xi D 0   E[xi ] D 1 ð pi C 0 ð .1  cid:7  pi   D pi var[xi ] D pi .1  cid:7  pi    zi D xi  cid:7  pi  p  var.xi    Substituting each of the conditional densities in equation  8.12  by its Bahadur–Lazarsfeld expansion replaces the unknown densities by unknown correlation coefﬁcients  see the exercises . However, this may simplify considerably under assumptions about the form of the individual detectors  see Kam et al., 1992 .  Neyman–Pearson formulation In the Neyman–Pearson formulation, we seek a threshold on the likelihood ratio so that a speciﬁed false alarm rate is achieved  see Section 1.5.1 . Since the data space is discrete  for an L-dimensional vector, u, there are 2L possible states  the decision rule of Chapter 1 is modiﬁed to become:  p.uj!1  p.uj!2   if  8 < > t D t : < t  then decide u0 D 1  target present declared  then decide u0 D 1 with probability ž then decide u0 D 0  target absent declared    8.13   where ž and t are chosen to achieve the desired false alarm rate.  As an example, consider the case of two sensors, S1 and S2, operating with proba- bilities of false alarm pfa1 and pfa2 respectively, and probabilities of detection pd1 and pd2. Table 8.1 gives the probability density functions for p.uj!1  and p.uj!2  assuming independence. There are four values for the likelihood ratio, p.uj!1 = p.uj!2 , corresponding to u D .0; 0 ; .0; 1 ; .1; 0 ; .1; 1 . For p f a1 D 0:2, p f a2 D 0:4, pd1 D 0:6 and pd2 D 0:7, these values are 0.25, 0.875, 1.5 and 5.25. Figure 8.13 gives the ROC curve for the combiner, combined using rule  8.13 . This is a piecewise linear curve, with four linear segments, each corresponding to one of the values of the likelihood ratio. For example, if we set t D 0:875  one of the values of the likelihood ratio , then u0 D 1 is decided if u D .1; 0  and u D .1; 1 , and also for u D .0; 1  with probability ž. This gives a probability of detection and a probability of false alarm of  using Table 8.1   pd D pd1 pd2 C .1  cid:7  pd2  pd1 C ž.1  cid:7  pd1  pd2 D 0:6 C 0:28ž pfa D pfa1 pfa2 C .1  cid:7  pfa2  pfa1 C ž.1  cid:7  pfa1  pfa2 D 0:2 C 0:32ž  a linear variation  as ž is varied  of .pfa; pd  values between  0.2, 0.6  and  0.52, 0.88 .   282 Performance  Table 8.1 Probability density functions for p.uj!1   top  and p.uj!2   bottom   Sensor S1  u D 0  Sensor S2  .1  cid:7  pd1 .1  cid:7  pd2   .1  cid:7  pd2  pd1  u D 0 u D 1  .1  cid:7  pd1  pd2  u D 1  pd1 pd2  Sensor S1  u D 0  u D 1  Sensor S2  u D 0 u D 1  .1  cid:7  p f a1 .1  cid:7  p f a2   .1  cid:7  p f a1  p f a2  .1  cid:7  p f a2  p f a1  p f a1 p f a2  1  0.8  0.6  0.4  0.2  pd  0  0  pd1+ pd2 1 − pd1   pd1  pd1pd2  0.2 pfa1  pfa1pfa2  0.4  0.6  pfa1 + pfa2 1 − pfa1   0.8  pfa  1  Figure 8.13 ROC curve for two sensors, assuming independence  It may be possible to achieve a better probability of detection of the combiner, for a given probability of false alarm, by operating the individual sensors at different local thresholds. For L sensors, this is an L-dimensional search problem and requires knowl- edge of the ROC of each sensor  Viswanathan and Varshney, 1997   Approaches to the distributed detection problem using the Neyman–Pearson formula- tion have been proposed for correlated decisions. One approach is to expand the likelihood ratio using the Bahadur–Lazarsfeld polynomials, in a similar manner to the Bayesian for- mulation above. The Neyman–Pearson fusion rule can then be expressed as a function of the correlation coefﬁcients.  If the independence assumption is not valid, and it is not possible to estimate the likelihood ratio through other means, then it may still be possible to achieve better performance than individual sensors through a ‘random choice’ fusion system. Consider two sensors S1 and S2, with ROC curves shown in Figure 8.14. For probabilities of false   Combining classiﬁers 283  S2  S1  pdB  0.8  pd  pdA  0.4  1  0.6  0.2  0  0  0.1  0.2  0.3  0.4  0.6  0.7  0.8  0.9  1  pfaA  pfaB  0.5 pfa  Figure 8.14 ROC curves for sensors S1 and S2 and points on the convex hull  alarm greater than pfaB we operate sensor S2, and for probabilities of false alarm less than pfaA we operate sensor S1. If, for probabilities of false alarm between pfaA and pfaB, we operate sensor S1 with probability of false alarm pfaA and sensor S2 at the pfaB point on its ROC curve, and randomly select sensor S1 with probability ž and sensor S2 with probability 1  cid:7  ž, then the probability of false alarm of the random choice fusion system is žpfaA C .1  cid:7  ž  pfaB and the probability of detection is žpdA C .1  cid:7  ž  pdB. Thus, the best performance is achieved on the convex hull of the two ROC curves.  This differs from the example in Figure 8.13, where the combined output of both  sensors is used, rather than basing a decision on a single sensor output.  Trainable rules One of the difﬁculties with the Bayesian and the Neyman–Pearson formulations for a set of distributed sensors is that both methods require some knowledge of the probability density of sensor outputs. Often this information is not available and the densities must be estimated using a training set.  This is simply a problem of classiﬁer design where the classiﬁers are deﬁned on a feature space comprising the outputs of separate sensors  local decisions . Many of the techniques described elsewhere in this book, suitably adapted for binary variables, may be employed.  Fixed rules There are a few ‘ﬁxed’ rules for decision fusion that do not model the joint density of sensor predictions.  AND Class !1  target present  is declared if all sensors predict class !1, otherwise class !2 is declared.   284 Performance  OR Class !1  target present  is declared if at least one of the sensors predicts class !1, otherwise class !2 is declared.  Majority vote Class !1  target present  is declared if a majority of the sensors predicts class !1, otherwise class !2 is declared.  k-out-of-N Class !1  target present  is declared if at least k of the sensors predict class !1, otherwise class !2 is declared. All of the previous three rules are special cases of this rule.  It is difﬁcult to draw general conclusions about the performance of these rules. For low false alarm rates, there is some evidence to show that the OR rule is inferior to the AND and majority-vote rules in a problem of signal detection in correlated noise. For similar local sensors, the optimal rule is the k-out-of-N decision rule, with k calculated from the prior probabilities and the sensor probability of false alarm and probability of detection.  8.4.5 Classiﬁer combination methods  The characterising features of multiple classiﬁer systems have been described in Section 8.4.3, and a practical motivating problem for the fusion of decisions from dis- tributed sensors summarised in Section 8.4.4. We turn now to the methods of classiﬁer fusion, many of which are multiclass generalisations of the binary classiﬁers employed for decision fusion.  We begin with the Bayesian decision rule and, following Kittler et al.  1998 , make certain assumptions to derive combination schemes that are routinely used. Various de- velopments of these methods are described.  We assume that we have an object Z that we wish to classify and that we have L classiﬁers with inputs x1; : : : ; x L  as in Figure 8.7 . The Bayes rule for minimum error  1.1  assigns Z to class ! j if  p.! jjx1; : : : ; x L   > p.!kjx1; : : : ; x L    k D 1; : : : ; C; k 6D j   8.14   or, equivalently  1.2 , assigns Z to class ! j if  p.x1; : : : ; x Lj! j   p.! j   > p.x1; : : : ; x Lj!k   p.!k    k D 1; : : : ; C; k 6D j   8.15   This requires knowledge of the class-conditional joint probability densities p.x1; : : : ; x Lj! j  ; j D 1; : : : ; L, which is assumed to be unavailable.  Product rule If we assume conditional independence  x1; : : : ; x L are conditionally independent given class , then the decision rule  8.15  becomes: assign Z to class ! j if  LY iD1  LY iD1  . p.xij! j    p.! j   >  . p.xij!k    p.!k   k D 1; : : : ; C; k 6D j   8.16    Combining classiﬁers 285  LY iD1  LY iD1  or, in terms of the posterior probabilities of the individual classiﬁers: assign Z to class ! j if  [ p.! j  ] cid:7 .L cid:7 1   p.! jjxi   > [ p.!k  ] cid:7 .L cid:7 1   p.!kjxi    k D 1; : : : ; C; k 6D j  LY iD1   8.17    8.18   This is the product rule and for equal priors simpliﬁes to: assign Z to class ! j if  p.! jjxi   >  p.!kjxi    k D 1; : : : ; C; k 6D j  LY iD1  Both forms  8.17  and  8.18  have been used in studies. The independence assumption may seem rather severe, but it is one that has been successfully used in many practical problems  Hand and Yu, 2001 . The rule requires the individual classiﬁer posterior prob- abilities, p.! jjx ; j D 1; : : : ; C, to be calculated, and they are usually estimated from training data. The main problem with this method is that the product rule is sensitive to errors in the posterior probability estimates, and deteriorates more rapidly than the sum rule  see below  as the estimation errors increase. If one of the classiﬁers reports that the probability of a sample belonging to a particular class is zero, then the product rule will give a zero probability also, even if the remaining classiﬁers report that this is the most probable class.  The product rule would tend to be applied where each classiﬁer receives input from  different sensors.  Sum rule Let us make the  rather strong  assumption that  p.!kjxi   D p.!k  .1 C Žki     8.19  where Žki − 1, that is, the posterior probabilities p.!kjxi   used in the product rule  8.17  do not deviate substantially from the class priors p.!k  . Then substituting for p.!kjxi   in the product rule  8.17 , neglecting second-order and higher terms in Žki , and using  8.19  again leads to the sum rule  see the exercises at the end of the chapter : assign Z to class ! j if  .1  cid:7  L  p.! j   C LX iD1  p.! jjxi   > .1  cid:7  L  p.!k  C LX iD1  p.!kjxi    k D 1; : : : ; C; k 6D j  This is the sum rule and for equal priors it simpliﬁes to: assign Z to class ! j if  LX iD1  LX iD1  p.! jjxi   >  p.!kjxi    k D 1; : : : ; C; k 6D j  The assumption used to derive the sum rule approximation to the product rule, namely that the posterior probabilities are similar to the priors, will be unrealistic in many practical   8.20    8.21    286 Performance  applications. However, it is a rule that is relatively insensitive to errors in the estimation of the joint densities and would be applied to classiﬁers used for a common input pattern  Figure 8.8 .  In order to implement the above rule, each classiﬁer must produce estimates of the posterior probabilities of class membership. In a comparison of the sum and product rules, Tax et al.  2000  concluded that the sum rule is more robust to errors in the estimated posterior probabilities  see also Kittler et al., 1998 . The averaging process reduces any effects of overtraining of the individual classiﬁers and may be thought of as a regularisation process.  It is also possible to apply a weighting to the sum rule to give: assign Z to class ! j if  LX iD1  LX iD1  wi p.! jjxi   >  wi p.!kjxi    k D 1; : : : ; C; k 6D j   8.22   where wi ; i D 1; : : : ; L, are weights for the classiﬁers. A key question here is the choice of weights. These may be estimated using a training set to minimise the error rate of the combined classiﬁer. In this case, the same weighting is applied throughout the data space. An alternative is to allow the weights to vary with the location of a given pattern in the data space. An extreme example of this is dynamic classiﬁer selection where one weight is assigned the value unity and the remaining weights are zero. For a given pattern, dynamic feature selection attempts to select the best classiﬁer. Thus, the feature space is partitioned into regions with a different classiﬁer for each region.  Dynamic classiﬁer selection has been addressed by Woods et al.  1997  who use local regions deﬁned in terms of k-nearest-neighbour regions to select the most accurate classiﬁer  based on the percentage of training samples correctly classiﬁed in the region ; see also Huang and Suen  1995 .  Min, max and median combiners The max combiner may be derived by approximating the posterior probabilities in  8.20  by an upper bound, Lmaxi p.!kjxi  , to give the decision rule: assign Z to class ! j if .1  cid:7  L  p.! j   C L max k D 1; : : : ; C; k 6D j  p.! jjxi   > .1  cid:7  L  p.!k  C L max  p.!kjxi    i  i  This is the max combiner and for equal priors simpliﬁes to  p.! jjxi   > max  p.!kjxi    k D 1; : : : ; CI k 6D j  max   8.24  We can also approximate the product in  8.17  by an upper bound, mini p.!kjxi  , to  i  i  give the decision rule: assign Z to class ! j if [ p.! j  ] cid:7 .L cid:7 1  min  p.! jjxi   > [ p.!k  ] cid:7 .L cid:7 1  min  p.!kjxi    k D 1; : : : ; C; k 6D j  i  This is the min combiner and for equal priors simpliﬁes to: assign Z to class ! j if  p.! jjxi   > min  p.!kjxi    k D 1; : : : ; C; k 6D j  i  i  min  i   8.23    8.25    8.26    Combining classiﬁers 287  Finally, the median combiner is derived by noting that the sum rule calculates the mean of the classiﬁer outputs and that a robust estimate of the mean is the median. Thus, under equal priors, the median combiner is: assign Z to class ! j if  p.! jjxi   > med  p.!kjxi    k D 1; : : : ; C; k 6D j   8.27   med  i  i  The min, max and median combiners are all easy to implement and require no training.  Majority vote Among all the classiﬁer combination methods described in this section, the majority vote is one of the easiest to implement. It is applied to classiﬁers that produce unique class labels as outputs  level L3  and requires no training. It may be considered as an application of the sum rule to classiﬁer outputs where the posterior probabilities, p.!kjxi  , have been ‘hardened’  Kittler et al., 1998 ; that is, p.!kjxi   is replaced by the binary-valued function, 1ki , where    1 if p.!kjxi   D max  p.! jjxi    j  1ki D  0 otherwise  which produces decisions at the classiﬁer outputs rather than posterior probabilities. A decision is made to classify a pattern to the class most often predicted by the constituent classiﬁers. In the event of a tie, a decision can be made according to the largest prior class probability  among the tying classes .  An extension to the method is the weighted majority voting technique in which clas- siﬁers are assigned unequal weights based on their performance. The weights for each classiﬁer may be independent of predicted class, or they may vary across class depend- ing on the performance of the classiﬁer on each class. A key question is the choice of weights. The weighted majority vote combiner requires the results of the individual classiﬁers on a training set as training data for the allocation of the weights. For weights that vary between classiﬁers but are independent of class, there are L  cid:7  1 parameters to estimate for L classiﬁers  we assume that the weights may be normalised to sum to unity . These may be determined by specifying some suitable objective function and an appropriate optimisation procedure. One approach is to deﬁne the objective function  F D Re  cid:7  þ E  where Re is the recognition rate and E is the error rate of the combiner  they do not sum to unity as the individual classiﬁers may reject patterns – see Chapter 1 ; þ is a user-speciﬁed parameter that measures the relative importance of recognition and error rates and is problem-dependent  Lam and Suen, 1995 . Rejection may be treated as an extra class by the component classiﬁers and thus the combiner will reject a pattern if the weighted majority of the classiﬁers also predicts a rejection. In a study of combination schemes applied to a problem in optical character recognition, Lam and Suen  1995  used a genetic optimisation scheme  a scheme that adjusts the weights using a learning method loosely motivated by an analogy to biological evolution  to maximise F and concluded that simple majority voting  all weights equal  gave the easiest and most reliable classiﬁcation.   288 Performance  Borda count The Borda count is a quantity deﬁned on the ranked outputs of each classiﬁer. If we deﬁne Bi . j   as the number of classes ranked below class ! j by classiﬁer i, then the Borda count for class ! j is B j deﬁned as  B j D LX iD1  Bi . j    the sum of the number of classes ranked below ! j by each classiﬁer. A pattern is assigned to the class with the highest Borda count. This combiner requires no training, with the ﬁnal decision being based on an average ranking of the classes.  Combiners trained on class predictions The combiners described so far require no training, at least in their basic forms. General conclusions are that the sum rule and median rule can be expected to give better perfor- mance than other ﬁxed combiners. We now turn to combiners that require some degree of training, and initially we consider combiners acting on discrete variables. Thus, the constituent classiﬁers deliver class labels and the combiner uses these class predictions to make an improved estimate of class  type L combination  – at least, that is what we hope.  ‘Bayesian combiner’ This combiner simply uses the product rule with estimates of the posterior probabilities derived from the classiﬁer predictions of each constituent classiﬁer, together with a summary of their performance on a labelled training set.  Speciﬁcally, the Bayesian combination rule of Lam and Suen  1995  approximates the posterior probabilities by an estimate based on the results of a training procedure. Let D.i   denote the C ð C confusion matrix  see Chapter 1  for the ith classiﬁer based on the results of a classiﬁcation of a training set by classiﬁer i. The . j; k th entry, d .i   jk , is the number of patterns with true class !k that are assigned to ! j by classiﬁer i. The total number of patterns in class !k is  for any i. The number of patterns assigned to class !l is  nk D CX lD1  d .i   lk  CX kD1  d .i   lk  The conditional probability that a sample x assigned to class !l by classiﬁer i actually belongs to !k is estimated as  p !kjclassiﬁer i predicts !l D  d .i   lkPC kD1 d .i   lk   Combining classiﬁers 289  Thus, for a given pattern, the posterior probability depends only on the predicted class: two distinct patterns xi and x j , having the same predicted class, are estimated as having the same posterior probability. Substituting into the product rule  8.18 , equal priors assumed, gives the decision rule: assign the pattern to class ! j if  LY iD1  d .i   li mPC kD1 d .i   li k where !li is the predicted class of pattern xi .  d .i   li jPC kD1 d .i   li k  LY iD1  >  m D 1; : : : ; C; m 6D j  Density estimation in classiﬁer output space An alternative approach is to regard the L class predictions from the constituent classiﬁers for an input, x, as inputs to a classiﬁer, the combiner, deﬁned on an L-dimensional discrete-valued feature space  see Figure 8.15 . Suppose that we have N training patterns .xi ; i D 1; : : : ; N   with associated class labels .yi ; i D 1; : : : ; N  ; then the training data for the combiner comprises N L- dimensional vectors .zi ; i D 1; : : : ; N   with associated class labels .yi ; i D 1; : : : ; N  . Each component of zi is a discrete-valued variable taking 1 of C possible values corres- ponding to the class label from the component classiﬁer  1 of C C 1 if a reject option is included in the constituent classiﬁers . The combiner is trained using the training set f.zi ; yi  ; i D 1; : : : ; Ng and an unknown pattern x classiﬁed by ﬁrst applying each constituent classiﬁer to obtain a vector of predictions z, which is then input to the combiner. conditional probabilities, p.zj!i  ; i D 1; : : : ; C, and to classify z to class ! j if  The most obvious approach to constructing the combiner is to estimate class-  p.zj! j   p.! j   > p.zj!k  p.!k    k D 1; : : : ; C; k 6D j  with the priors, p.! j  , estimated from the training set  or perhaps using domain knowl- edge  and the densities, p.zj! j   estimated using a suitable nonparametric density esti- mation method appropriate for categorical variables.  Perhaps the simplest method is the histogram. This is the approach adopted by Huang and Suen  1995  and termed the behaviour-knowledge space method, and also investi- gated by Mojirsheibani  1999 . However, this has the disadvantage of having to estimate  S  Com  p cx   x  x  x  Cl1  Cl2  ClL  w 1   w 2   w L   Figure 8.15 Classiﬁer fusion architecture C2–component classiﬁers deﬁned on a common feature space   290 Performance  and store high-order distributions which may be computationally expensive. The multidi- mensional histogram has C L cells, which may be large, making reliable density estimation difﬁcult in the absence of a large training data set. In Chapter 3 we described several ways around this difﬁculty:  1. Independence. Approximate the multivariate density as the product of the univariate  estimates.  2. Lancaster models. Approximate the density using marginal distributions.  3. Maximum weight dependence trees. Approximate the density with a product of pair-  wise conditional densities.  tional densities.  4. Bayesian networks. Approximate the density with a product of more complex condi-  Other approaches, based on constructing discriminant functions directly, rather than  estimating the class-conditional densities and using Bayes’ rule, are possible.  Stacked generalisation Stacked generalisation, or simply stacking, constructs a generaliser using training data that consist of the ‘guesses’ of the component generalisers which are taught with different parts of the training set and try to predict the remainder, and whose output is an estimate of the correct class. Thus, in some ways it is similar to the models of the previous section – the combiner is a classiﬁer  generaliser  deﬁned on the outputs of the constituent classiﬁers – -but the training data used to construct the combiner comprise the prediction on held-out samples of the training set. The basic idea is that the output of the constituent classiﬁers, termed level 1 data, L1  level 0, L0, is the input level , has information that can be used to construct good combinations of the classiﬁers. We suppose that we have a set of constituent classiﬁers, f j ; j D 1; : : : ; L, and we seek a procedure for combining them. The level 1 data are constructed as follows. 1. Divide the L0 data  the training data, f.xi ; yi  ; i D 1; : : : ; ng  into V partitions. 2. For each partition, v D 1; : : : ; V , do the following.   a  Repeat the procedure for constructing the constituent classiﬁers using a subset of the data: train the constituent classiﬁer j   j D 1; : : : ; L  on all the training data apart from partition v to give a classiﬁer denoted, f  cid:7 v , on all patterns in partition v.   b  Test each classiﬁer, f  cid:7 v  .  j  j  This gives a data set of L predictions on each pattern in the training set. Together with the labels fyi ; i D 1; : : : ; ng, these comprise the training data for the combiner.  We must now construct a combiner for the outputs of the constituent classiﬁers. If the constituent classiﬁers produce class labels, then the training data for the com- biner comprise L-dimensional measurements on categorical variables. Several methods are available to us, including those based on histogram estimates of the multivariate   Combining classiﬁers 291  density, and variants, mentioned at the end of the previous section; for example, tree- based approaches and neural network methods. Merz  1999  compares an independence model and a multilayer perceptron model for the combiner with an approach based on a multivariate analysis method  correspondence analysis . For the multilayer perceptron, when the ith value of the variable occurs, each categorical variable is represented as C binary-valued inputs, with all inputs assigned the value of zero apart from the ith, which is assigned a value of one.  Mixture of experts The adaptive mixture of local experts model  Jacobs et al., 1991; Jordan and Jacobs, 1994  is a learning procedure that trains several component classiﬁers  the ‘experts’  and a combiner  the ‘gating function’  to achieve improved performance in certain problems. The experts each produce an output vector, oi .i D 1; : : : ; L , for a given input vector, x, and the gating network provides linear combination coefﬁcients for the experts. The gating function may be regarded as assigning a probability to each of the experts, based on the current input  see Figure 8.16 . The emphasis of the training procedure is to ﬁnd the optimal gating function and, for a given gating function, to train each expert to give maximal performance.  In the basic approach, the output of the ith expert, oi .x , is a generalised linear  function of the input, x,  oi .x  D f .wT  i x   where wi is a weight vector associated with the ith expert and f .:  is a ﬁxed continuous nonlinear function. The gating network is also a generalised linear function, g, of its input, with ith component  gi .x  D g.x; vi   D  PL  exp.vT i x  kD1 exp.vT  i x   for weight vectors vi ; i D 1; : : : ; L. These outputs of the gating network are used to weight the outputs of the experts to give the overall output, o.x , of the mixture of  expert 1  o1  combiner  o x   x  expert L  oL  gL  g1  gating network  Figure 8.16 Mixture of experts architecture   292 Performance  experts architectures as  o.x  D LX kD1  gk .x ok .x    8.28   The above algorithm is very similar to  8.22  that giving rise to the weighted sum rule in that the model produces a linear combination of component classiﬁers. The key difference is that in the mixture of experts model, the combination model  the gating network  is trained simultaneously with the constituent classiﬁers  the experts . In many combination models, the basic models are trained ﬁrst and then the combiner tuned to these trained models. A further difference is that the linear combination depends on the input pattern, x. An interpretation is that the gating network provides a ‘soft’ partitioning of the input space, with experts providing local predictions  Jordan and Jacobs, 1994 .  The mixture of experts model  8.28  is also similar to the multilayer perceptron2  Chapter 6  in that the output is a linear combination of nonlinear functions of projections of the data. The difference is that in  8.28  the linear combination depends on the input. A probabilistic interpretation of the model is provided by Jordan and Jacobs  1994 . We assume that we have an input variable x and a response variable y that depends prob- abilistically on x. The mixing proportions, gi .x , are interpreted as multinomial proba- bilities associated with the process that maps x to an output y. For a given x, an output y is generated by selecting an expert according to the values of gk .x ; k D 1; : : : ; L, say expert i, and then generating y according to a probability density p.yjx; wi  , where wi denotes the set of parameters associated with expert i. Therefore, the total probability of generating y from x is the mixture of probabilities of generating y from each component density, where the mixing proportions are the multinomial probabilities gk .x , i.e.  p.yjx;  cid:12   D LX kD1  gk .x  p.yjx; wk     8.29   where  cid:12  is the set of all parameters, including both expert and gating network parameters. The generating density p.yjx; wk  , can be taken to be one of several popular forms; for a problem in regression, a normal distribution with identical covariance matrices ¦ 2I is often assumed,  p.yjx; wk   ¾ exp  .y  cid:7  ok .x  T .y  cid:7  ok.x    ²   cid:7  1 ¦ 2  ¦  For binary classiﬁcation, the Bernoulli distribution is generally assumed  single output, ok; univariate binary response variable y D 0; 1   and for multiclass problems, the multinomial distribution  L binary variables yi ; i D 1; : : : ; L, summing to unity   p.yjx; wk   D oy  .1  cid:7  ok  1 cid:7 y  k  p.yjx; wk   ¾ LY iD1   yi  .oi k  2The basic MLP, with single hidden layer and linear output layer. Further development of the mixture of  experts model for hierarchical models is discussed by Jordan and Jacobs  1994 .   Combining classiﬁers 293  Optimisation of the model  8.29  may be achieved via a maximum likelihood ap- proach. Given a training set f.xi ; yi  ; i D 1; : : : ; ng  in the classiﬁcation case, yi would be a C-dimensional vector coding the class – for class ! j , all entries are zero apart from the jth, which is one , we seek a solution for  cid:12  for which the log-likelihood  X  t  log  " LX kD1    gk .xt   p.ytjxt ; wk    is a maximum. Jordan and Jacobs  1994  propose an approach based on the EM algorithm  see Chapter 2  for adjusting the parameters wk and vk. At stage s of the iteration, the expectation and maximisation procedures are as follows:  1. E-step. Compute the probabilities  i D h.t  for t D 1; : : : ; n; i D 1; : : : ; L.  PL  .s  i  g.xt ; v kD1 g.xt ; v    p.ytjxt ; w  .s  i       p.ytjxt ; w  .s  k  .s  k     2. M-step. For the parameters of the experts solve the maximisation problem  .sC1  i  w  D arg max  wi  nX tD1  h.t  i  log[ p.ytjxt ; wi  ]  and for the parameters of the gating network nX LX kD1 tD1  V .sC1  D arg max  V  h.t  k  log[g.xt ; vk  ]  where V is the set of all vi .  Procedures for solving these maximisation problems are discussed by Chen et al.  1999 , who propose a Newton–Raphson method, but other ‘quasi-Newton’ methods may be used  see Press et al., 1992 .  Bagging Bagging and boosting  see the following section  are procedures for combining differ- ent classiﬁers generated using the same training set. Bagging or bootstrap aggregating  Breiman, 1996  produces replicates of the training set and trains a classiﬁer on each replicate. Each classiﬁer is applied to a test pattern x which is classiﬁed on a majority- vote basis, ties being resolved arbitrarily. Table 8.2 shows the algorithm. A bootstrap sample is generated by sampling n times from the training set with replacement. This provides a new training set, Y b, of size n. B bootstrap data sets, Y b; b D 1; : : : ; B, are generated and a classiﬁer designed for each data set. The ﬁnal classiﬁer is that whose output is the class most often predicted by the subclassiﬁers.  A vital aspect of the bagging technique is that the procedure for producing the classi- ﬁer is unstable. For a given bootstrap sample, a pattern in the training set has a probability of 1 cid:7  .1 cid:7  1=n n of being selected at least once in the n times that patterns are randomly   294 Performance  Table 8.2 The bagging algorithm Assume that we have a training set .xi ; zi  ; i D 1; : : : ; n, of patterns xi and labels zi . 1. For b D 1; : : : ; B, do the following.   a  Generate a bootstrap sample of size n by sampling with replacement from the  training set; some patterns will be replicated, others will be omitted.   b  Design a classiﬁer,  cid:6 b.x .  2. Classify a test pattern x by recording the class predicted by  cid:6 b.x , b D 1; : : : ; B,  and assigning x to the class most represented.  selected from the training set. For large n, this is approximately 1  cid:7  1=e D 0:63, which means that each bootstrap sample contains only about 63% unique patterns from the training set. This causes different classiﬁers to be built. If the change in the classiﬁers is large  that is, small changes in a data set lead to large changes in the predictions , then the procedure is said to be unstable. Bagging of an unstable classiﬁer should result in a better classiﬁer and a lower error rate. However, averaging of a bad classiﬁer can result in a poorer classiﬁer. If the classiﬁer is stable – that is, changes in the training data set lead to small changes in the classiﬁer – then bagging will lead to little improvement.  The bagging procedure is particularly useful in classiﬁcation problems using neural networks  Chapter 6  and classiﬁcation trees  Chapter 7  since these are all unstable processes. For trees, a negative feature is that there is no longer the simple interpretation as there is with a single tree. Nearest-neighbour classiﬁers are stable and bagging offers little, if any, improvement.  In studies on linear classiﬁers, Skurichina  2001  reports that bagging may improve the performance on classiﬁers constructed on critical training sample sizes, but when the classiﬁer is stable, bagging is usually useless. Also, for very large sample sizes, classiﬁers constructed on bootstrap replicates are similar and combination offers no beneﬁt.  The procedure, as presented in Table 8.2, applies to classiﬁers whose outputs are class predictions. For classiﬁer methods that produce estimates of the posterior probabilities, Op.! jjx , two approaches are possible. One is to make a decision for the class based on the maximum value of Op.! jjx  and then to use the voting procedure. Alternatively, the pos- terior probabilities can be averaged over all bootstrap replications, obtaining OpB .! jjx , and then a decision based on the maximum value of OpB .! jjx  is made. Breiman  1996  reports a virtually identical misclassiﬁcation rate for the two approaches in a series of experiments on 11 data sets. However, bagged estimates of the posterior probabilities are likely to be more accurate than single estimates.  Boosting Boosting is a procedure for combining or ‘boosting’ the performance of weak classiﬁers  classiﬁers whose parameter estimates are usually inaccurate and give poor performance  in order to achieve a better classiﬁer. It differs from bagging in that it is a deterministic procedure and generates training sets and classiﬁers sequentially, based on the results of the previous iteration. In contrast, bagging generates the training sets randomly and can generate the classiﬁers in parallel.   Combining classiﬁers 295  Proposed by Freund and Schapire  1996 , boosting assigns a weight to each pattern in the training set, reﬂecting its importance, and constructs a classiﬁer using the training set and the set of weights. Thus, it requires a classiﬁer that can handle weights on the training samples. Some classiﬁers may be unable to support weighted patterns. In this case, a subset of the training examples can be sampled according to the distribution of the weights and these examples used to train the classiﬁer in the next stage of the iteration.  The basic boosting procedure is AdaBoost  Adaptive Boosting; Freund and Schapire, 1996 . Table 8.3 presents the basic AdaBoost algorithm for the binary classiﬁcation problem. Initially, all samples are assigned a weight wi D 1=n. At each stage of the algorithm, a classiﬁer  cid:6 t .x  is constructed using the weights wi  as though they reﬂect the probability of occurrence of the sample . The weight of misclassiﬁed patterns is increased and the weight of correctly classiﬁed patterns is decreased. The effect of this is that the higher-weight patterns inﬂuence the learning classiﬁer more, and thus cause the classiﬁer to focus more on the misclassiﬁcations, i.e. those patterns that are nearest the decision boundaries. There is a similarity with support vector machines in this respect  Chapters 4 and 5 . The error et is calculated, corresponding to the sum of the weights of the misclassiﬁed samples. These get boosted by a factor .1  cid:7  et  =et , increasing the total weight on the misclassiﬁed samples  provided that et < 1=2 . This process is repeated and a set of classiﬁers is generated. The classiﬁers are combined using a linear weighting whose coefﬁcients are calculated as part of the training procedure.  There are several ways in which the AdaBoost algorithm has been generalised. One generalisation is for the classiﬁers to deliver a measure of conﬁdence in the prediction. For example, in the two-class case, instead of the output being š1 corresponding to one of the two classes, the output is a number in the range [ cid:7 1;C1]. The sign of the output is the  Table 8.3 The AdaBoost algorithm 1. Initialise the weights wi D 1=n; i D 1; : : : ; n. 2. For t D 1; : : : ; T ,   a  construct a classiﬁer  cid:6 t .x  from the training data with weights wi ; i D  1; : : : ; n;  patterns;   b  calculate et as the sum of the weights wi corresponding to misclassiﬁed  c  if et > 0:5 or et D 0 then terminate the procedure, otherwise set wi D wi .1  cid:7  et  =et for the misclassiﬁed patterns and renormalise the weights so that they sum to unity. 3. For a two-class classiﬁer, in which  cid:6 t .x  D 1 implies x 2 !1 and  cid:6 t .x  D  cid:7 1  implies x 2 !2, form a weighted sum of the classiﬁers,  cid:6 t ,  O cid:6  D TX tD1  log   cid:4    cid:3 1  cid:7  et et   cid:6 t .x   and assign x to !1 if O cid:6  > 0.   296 Performance  Table 8.4 The AdaBoost.MH algorithm converts the C-class problem into a two-class problem operating on the original training data with an additional ‘feature’ 1. Initialise the weights wi j D 1=.nC ; i D 1; : : : ; n; j D 1; : : : ; C. 2. For t D 1; : : : ; T ,   a  construct a ‘conﬁdence-rated’ classiﬁer  cid:6 t .x; l  from the training data with  weights wi j ; i D 1; : : : ; n; j D 1; : : : ; C;   b  calculate  and  sum to unity.  3. Set  rt D nX iD1  CX lD1  wil yil  cid:6 t .xi ; l   Þt D 1 2  log   cid:4    cid:3  1 C rt 1  cid:7  rt  O cid:6 .x; l  D TX tD1  Þt  cid:6 t .x; l    c  Set wi j D wi j exp. cid:7 Þt yi j  cid:6 t .xi ; j    and renormalise the weights so that they  and assign x to ! j if O cid:6 .x; j   ½ O cid:6 .x; k   k D 1; : : : ; C; k 6D j  predicted class label   cid:7 1 or C1  and the magnitude represents the degree of conﬁdence: close to zero is interpreted as low conﬁdence and close to unity as high conﬁdence.  For the multiclass generalisation, Table 8.4 presents the AdaBoost.MH algorithm  Schapire and Singer, 1999 . The basic idea is to expand the training set  of size n  to a training set of size n ð C pairs,  ..xi ; 1 ; yi1 ; ..xi ; 2 ; yi2 ; : : : ; ..xi ; C ; yiC  ;  i D 1; : : : ; n  Thus, each training pattern is replicated C times and augmented with each of the class labels. The new labels for a pattern .x; l  take the values  yil D  ²C1 if xi 2 class !l  cid:7 1 if xi =2 class !l  A classiﬁer,  cid:6 t .x; l , is trained and the ﬁnal classiﬁer, O cid:6 .x; l , is a weighted sum of the classiﬁers constructed at each stage of the iteration, with decision: assign x to class j if  O cid:6 .x; j   ½ O cid:6 .x; k   k D 1; : : : ; C; k 6D j  Note that the classiﬁer  cid:6 t .x; l  is deﬁned on a data space of possible mixed variable type: real continuous variables x and categorical variable l. Care will be needed in classiﬁer design  see Chapter 11 .   Combining classiﬁers 297  In the study of linear classiﬁers by Skurichina  2001 , it is reported that boosting is only useful for large sample sizes, when applied to classiﬁers that perform poorly. The performance of boosting depends on many factors, including training set size, choice of classiﬁer, the way in which the classiﬁer weights are incorporated and the data distribution.  8.4.6 Example application study  The problem This study  Sharkey et al., 2000  addresses a problem in condition mon- itoring and fault detection – the early detection of faults in a mechanical system by continuous monitoring of sensor data.  Summary A neural network approach is developed and ensembles of networks, based on majority vote, assessed. A multinet system that selects the appropriate ensemble is evaluated.  The data The data source was a four-stroke, two-cylinder air-cooled diesel engine. The recorded data comprised a digitised signal representing cylinder pressure as a function of crank angle position. Data corresponding to normal operating conditions  N  and two fault conditions – leaky exhaust valve  E  and leaky fuel injector  F  – were acquired, each at 15 load levels ranging from no load to full load. Eighty samples representing the entire cycle were acquired for each operating condition and load  giving 1200 D 80 ð 15 data samples for each condition, 3600 in total . Each sample comprised 7200 measurements  two cycles, sampled at intervals of 0.1 degrees . However, only the 200 in the neighbourhood of combustion were used  sometimes subsampled to 100 or 50 samples  to form the pattern vectors input to the classiﬁers.  The data were partitioned into a validation set  600 patterns , a test set  600 patterns , with 1200–2400 of the remainder used for training. The patterns were standardised in two ways: dividing the input pattern by its maximum value; and subtracting the mean of the pattern vector elements and dividing by the standard deviation.  The model The basic classiﬁer was a multilayer perceptron with a single hidden layer comprising a logistic sigmoid nonlinearity and an output layer equal to the number of classes  either two or three, depending on the experiment .  Training procedure For each of four class groupings  NEF, NE, EF and NF , three networks were trained corresponding to different subsamplings of the training vector or different subsets of the training data. The results for the individual networks and for an ensemble based on error rate were recorded. The ensemble classiﬁer was based on a majority vote.  A multinet system was also developed. Here, a pattern was ﬁrst presented to the three- class  NEF  network and if the largest two network outputs were, say, classes N and E, then the pattern was presented to the NE network to make the ﬁnal adjudication. Thus, this is a form of dynamic classiﬁer selection in which a classiﬁer is selected depending on the input pattern.   298 Performance  Results The ensemble classiﬁer gave better results than the individual classiﬁers. The multinet system performance was superior to a single three-class network, indicating that partitioning a multiclass classiﬁcation problem into a set of binary classiﬁcation problems can lead to improved performance.  8.4.7 Further developments  Many of the methods of classiﬁer combination, even the basic non-trainable methods, are active subjects of research and assessment. For example, further properties of ﬁxed rules  sum, voting, ranking  are presented by Kittler and Alkoot  2001  and Saranli and Demirekler  2001 .  Development of stacking to density estimation is reported by Smyth and Wolpert  1999 . Further applications of stacking neural network models are given by Sridhar et al.  1999 .  The basic mixture of experts model has been extended to a tree architecture by Jordan and Jacobs  1994 . Termed ‘hierarchical mixture of experts’, non-overlapping sets of expert networks are combined using gating networks. Outputs of these gating networks are themselves grouped using a further gating network.  Boosting is classed by Breiman  1998  as an ‘adaptive resampling and combining’ or arcing algorithm. Deﬁnitions for the bias and variance of a classiﬁer, C, are introduced and it is shown that  eE D eB C bias.C  C var.C   where eE and eB are the expected and Bayes error rates respectively  see Section 8.2.1 . Unstable classiﬁers can have low bias and high variance on a large range of data sets. Combining multiple versions can reduce variance signiﬁcantly.  8.4.8 Summary  In this section, we have reviewed the characteristics of combination schemes, presented a motivating application  distributed sensor detection  and described the properties of some of the more popular methods of classiﬁer combination. Combining the results of several classiﬁers can give improved performance over a single classiﬁer. To some degree, research in this area has the ﬂavour of a cottage industry, with many ad hoc techniques proposed and assessed. Motivation for some of the methodology is often very weak. On the other hand, some work is motivated by real-world practical applications such as the distributed detection problem and person veriﬁcation using different identiﬁcation systems. Often, in applications such as these, the constituent classiﬁer is ﬁxed and an optimal combination is sought. There is no universal best combiner, but simple methods such as the sum, product and median rules can work well.  Of more interest are procedures that simultaneously construct the component classi- ﬁers and the combination rule. Unstable classiﬁcation methods  classiﬁcation methods for which small perturbations in their training set or construction procedure may result in large changes in the predictor; for example, decision trees  can have their accuracy improved by combining multiple versions of the classiﬁer. Bagging and boosting fall into   Application studies 299  this category. Bagging perturbs the training set repeatedly and combines by simple vot- ing; boosting reweights misclassiﬁed samples and classiﬁers are combined by weighted voting. Unstable classiﬁers such as trees can have a high variance that is reduced by bagging and boosting. However, boosting may increase the variance of a stable classiﬁer and be counter-productive.  8.5 Application studies  One of the main motivating applications for research on multiple classiﬁer systems has been the detection and tracking of targets using a large number of different types of sensors. Much of the methodology developed applies to highly idealised scenarios, often failing to take into account practical considerations such as asynchronous measurements, data rates and bandwidth constraints on the communication channels between the sen- sors and the fusion centre. Nevertheless, methodology developed within the data fusion literature is relevant to other practical problems. Example applications include: ž Biomedical data fusion. Various applications include coronary care monitoring and ul- trasound image segmentation for the detection of the oesophagus  Dawant and Garbay, 1999 .  ž Airborne target identiﬁcation  Raju and Sarma, 1991 .  Examples of the use of classiﬁer fusion techniques described in this chapter include  the following: ž Biometrics. Chatzis et al.  1999  combine the outputs of ﬁve methods for person veriﬁcation, based on image and voice features, in a decision fusion application. Kittler et al.  1997  assess a multiple observation fusion  Figure 8.10  approach to person veriﬁcation. In a writer veriﬁcation application, Zois and Anastassopoulos  2001  use the Bahadur–Lazarsfeld expansion to model correlated decisions. Prabhakar and Jain  2002  use kernel-based density estimates  Chapter 3  to model the distributions of the component classiﬁer outputs, each assumed to provide a measure of conﬁdence in one of two classes, in a ﬁngerprint veriﬁcation application.  ž Chemical process modelling. Sridhar et al.  1996  develop a methodology for stacking  neural networks in plant-process modelling applications.  ž Remote sensing. In a classiﬁcation of land cover from remotely sensed data using decision trees, Friedl et al.  1999  assess a boosting procedure  see also Chan et al., 2001 . In a similar application, Giacinto et al.  2000  assess combination methods applied to ﬁve neural and statistical classiﬁers.  8.6 Summary and discussion  The most common measure of classiﬁer performance assessment is misclassiﬁcation rate or error rate. We have reviewed the different types of error rate and described procedures for error rate estimation. Other performance measures are reliability – how   300 Performance  good is our classiﬁer at estimating the true posterior probabilities – and the area under the receiver operating characteristic curve, AUC. Misclassiﬁcation rate makes the rather strong assumption that misclassiﬁcation costs are equal. In most practical applications, this is unrealistic. The AUC is a measure averaged over all relative costs, and it might be argued that this is equally inappropriate since usually something will be known about relative costs. The LC index was introduced as one attempt to make use of domain knowledge in performance assessment.  Combining the results of several classiﬁers, rather than selecting the best, may offer improved performance. There may be practical motivating problems for this – such as those in distributed data fusion – and many rules and techniques have been proposed and assessed. These procedures differ in several respects: they may be applied at different levels of processing  raw ‘sensor’ data, feature level, decision level ; they may be train- able or ﬁxed; the component classiﬁers may be similar  for example, all decision trees  or of different forms, developed independently; the structure may be serial or parallel; ﬁnally, the combiner may be optimised alone, or jointly with the component classiﬁers. There is no universal best combination rule, but the choice of rule will depend on the  data set and the training set size.  8.7 Recommendations  1. Use error rate with care. Are the assumptions of equal misclassiﬁcation costs appro-  priate for your problem?  2. If you are combining prescribed classiﬁers, deﬁned on the same inputs, the sum rule  is a good start.  gin with.  classiﬁers.  3. For classiﬁers deﬁned on separate features, the product rule is a simple one to be-  4. Boosting and bagging are recommended to improve performance of unstable  8.8 Notes and references  The subject of error rate estimation has received considerable attention. The literature up to 1973 is surveyed in the extensive bibliography of Toussaint  1974 , and more recent advances by Hand  1986  and McLachlan  1987 . The holdout method was considered by Highleyman  1962 . The leave-one-out method for error estimation is usually attributed to Lachenbruch and Mickey  1968  and cross-validation in a wider context to Stone  1974 .  The number of samples required to achieve good error rate estimates is discussed  with application to a character recognition task by Guyon et al.  1998 .  Quenouille  1949  proposed the method of sample splitting for overcoming bias, later termed the jackknife. The bootstrap procedure as a method of error rate estimation has been widely applied following the pioneering work of Efron  1979, 1982, 1983 . Reviews   Exercises 301  of bootstrap methods are provided by Efron and Tibshirani  1986  and Hinkley  1988 . There are several studies comparing the performance of the different bootstrap estimators  Efron, 1983; Fukunaga and Hayes, 1989b; Chernick et al., 1985; Konishi and Honda, 1990 . Davison and Hall  1992  compare the bias and variability of the bootstrap with cross-validation. They ﬁnd that cross-validation gives estimators with higher variance but lower bias than the bootstrap. The main differences between the estimators are when there is large class overlap, when the bias of the bootstrap is an order of magnitude greater than that of cross-validation.  The 0.632 bootstrap for error rate estimation is investigated by Fitzmaurice et al.  1991  and the number of samples required for the double bootstrap by Booth and Hall  1994 . The bootstrap has been used to compute other measures of statistical accuracy. The monograph by Hall  1992  provides a theoretical treatment of the bootstrap with some emphasis on curve estimation  including parametric and nonparametric regression and density estimation .  Reliability of posterior probabilities of group membership is discussed in the book by McLachlan  1992a . Hand  1997  also considers other measures of performance as- sessment.  The use of the ROC curves in pattern recognition for performance assessment and comparison is described by Bradley  1997 , Hand and Till  2001 , Adams and Hand  1999  and Provost and Fawcett  2001 .  There is a large literature on combining classiﬁers. A good starting point is the statistical pattern recognition review by Jain et al.  2000 . Kittler et al.  1998  describe a common theoretical framework for some of the ﬁxed combination rules.  Within the defence and aerospace domain, data fusion has received considerable attention, particularly the detection and tracking of targets using multiple distributed sources  Dasarathy, 1994b; Varshney, 1997; Waltz and Llinas, 1990 , with beneﬁts in robust operational performance, reduced ambiguity, improved detection and improved system reliability  Harris et al., 1997 . Combining neural network models is reviewed by Sharkey  1999 .  Stacking originated with Wolpert  1992  and the mixture of experts model with Jacobs  et al.  1991; see also Jordan and Jacobs, 1994   Bagging is presented by Breiman  1996 . Comprehensive experiments on bagging and boosting for linear classiﬁers are described by Skurichina  2001 . The ﬁrst provable polynomial-time boosting algorithm was presented by Schapire  1990 . The AdaBoost algorithm was introduced by Freund and Schapire  1996, 1999 . Improvements to the basic algorithm are given by Schapire and Singer  1999 . Empirical comparisons of bagging and boosting are given by Bauer and Kohavi  1999 .  A statistical view of boosting is provided by Friedman et al.  1998 . The website www.statistical-pattern-recognition.net contains refer-  ences and pointers to other websites for further information on techniques.  Exercises  1. Two hundred labelled samples are used to train two classiﬁers. In the ﬁrst classiﬁer, the data set is divided into training and test sets of 100 samples each and the classiﬁer   302 Performance  designed using the training set. The performance on the test set is 80% correct. In the second classiﬁer, the data set is divided into a training set of 190 samples and a test set of 10 samples. The performance on the test set is 90%. Is the second classiﬁer ‘better’ than the ﬁrst? Justify your answer.  2. Verify the Sherman–Morisson formula  8.3 . Describe how it may be used to estimate  the error rate of a Gaussian classiﬁer using cross-validation.  3. The ROC curve is a plot of 1 cid:7  ž1, the ‘true positive’, against ž2, the ‘false positive’  as the threshold on  see equation  8.8    is varied, where  p.!1jx   ž1 D ž2 D  Z  Z   cid:26 2   cid:26 1  p.xj!1  dx p.xj!2  dx  and  cid:26 1 is the domain where p.!1jx  lies above the threshold. Show, by conditioning on p.!1jx , that the true positive and false positive  for a threshold ¼  may be written respectively as  and  Z 1  ¼  Z  dc  Z  1  cid:7  ž1 D  p.xj!1  dx  p.!1jx Dc  ž2 D  Z 1  ¼  dc  p.xj!2  dx  p.!1jx Dc  The term R p.!1jx Dc p.xj!1  dx is the density of p.!1jx  values at c for class !1. Hence show that the ROC curve is deﬁned as the cumulative density of Op D p.!1jx  for class !1 patterns plotted against the cumulative density for class !2 patterns.  4. Generate training data consisting of 25 samples from each of two bivariate normal distributions  means . cid:7 d=2; 0  and .d=2; 0  and identity covariance matrix . Com- pute the apparent error rate and a bias-corrected version using the bootstrap. Plot both error rates, together with an error rate computed on a test set  of appropriate size  as a function of separation, d. Describe the results.  5. What is the signiﬁcance of the condition et > 0:5 in step 2 of the boosting algorithm  in Section 8.4.4?  6. Design an experiment to evaluate the boosting procedure. Consider which classiﬁer to use and data sets that may be used for assessment. How would weighted samples be incorporated into the classiﬁer design? How will you estimate generalisation performance? Implement the experiment and describe the results.  7. Repeat Exercise 6, but assess the bagging procedure as a means of improving clas-  siﬁer performance.   Exercises 303  8. Using expression  8.19  for the posterior probabilities, express the product rule in terms of the priors and Žki . Assuming Žki − 1, show that the decision rule may be expressed  under certain assumptions  in terms of sums of the Žki . State your assumptions. Finally, derive  8.20  using  8.19 .  9. Given measurements u D .u1; : : : ; u L   made by L detectors with probability of false alarm pfai and probability of detection pdi ; .i D 1; : : : ; L , show  assuming independence and equal cost loss matrix    cid:4    cid:3  p.!1ju  p.!2ju   log  D log   cid:4    cid:3  p.!1  p.!2   C X  SC  log   cid:4    cid:3  pdi pfai  C X  S cid:7    cid:4    cid:3  1  cid:7  pdi 1  cid:7  pfai  log  where SC is the set of all detectors such that ui D C1  target present declared – class !1  and S cid:7  is the set of all detectors such that ui D 0  target absent declared – class !2 . Therefore, express the data fusion rule as  u0 D  ² 1 if a0 C aT u > 0  0 otherwise   see equation  8.12   and determine a0; a.  10. Write a computer program to produce the ROC curve for the L-sensor fusion problem  L sensors with probability of false alarm pfai and probability of detection pdi ; i D 1; : : : ; L , using the decision rule  8.13 .  11. Using the Bahadur–Lazarsfeld expansion, derive the Bayesian decision rule in terms  of the conditional correlation coefﬁcients, i j :::L D Ei [zi z j : : : z L ] D  cid:22  i  Z  zi z j : : : z L p.uj!i  du  for i D 1; 2.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   9  Feature selection and extraction  Overview  Optimal and suboptimal search techniques for feature selection  selecting a subset of the original variables for classiﬁer design  are considered. Feature extraction seeks a transformation  linear or nonlinear  of the original variables to a smaller set. The most widely used technique is principal components analysis.  9.1 Introduction  This chapter is concerned with representing data in a reduced number of dimensions. Reasons for doing this may be easier subsequent analysis, improved classiﬁcation performance through a more stable representation, removal of redundant or irrelevant information or an attempt to discover underlying structure by obtaining a graphical repre- sentation. Techniques for representing data in a reduced dimension are termed ordination methods or geometrical methods in the multivariate analysis literature. They include such methods as principal components analysis and multidimensional scaling. In the pattern recognition literature they are termed feature selection and feature extraction methods and include linear discriminant analysis and methods based on the Karhunen–Lo`eve ex- pansion. Some of the methods are similar, if not identical, in certain circumstances and will be discussed in detail in the appropriate section of this chapter. Here, we approach the topic initially from a pattern recognition perspective and give a brief description of the terms feature selection and feature extraction.  Given a set of measurements, dimensionality reduction can be achieved in essentially two different ways. The ﬁrst is to identify those variables that do not contribute to the classiﬁcation task. In a discrimination problem, we would neglect those variables that do not contribute to class separability. Thus, the task is to seek d features out of the available p measurements  the number of features d must also be determined . This is termed feature selection in the measurement space or simply feature selection  see Figure 9.1a . There are situations other than for discrimination purposes in which it is desirable to select a subset from a larger number of features or variables. Miller  1990  discusses subset selection in the context of regression.   306 Feature selection and extraction   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   x1  x p   cid:1   f1   cid:1   f2   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   x1  x p  f .x   f .x1; : : : ; x p     cid:1   f1   cid:1   f2   a  feature selector   b  feature extractor  Figure 9.1 Dimensionality reduction by  a  feature selection and  b  feature extraction  The second approach is to ﬁnd a transformation from the p measurements to a lower- dimensional feature space. This is termed feature selection in the transformed space or feature extraction  see Figure 9.1b . This transformation may be a linear or nonlinear combination of the original variables and may be supervised or unsupervised. In the supervised case, the task is to ﬁnd the transformation for which a particular criterion of class separability is maximised. Both of these approaches require the optimisation of some criterion function, J . For feature selection, the optimisation is over the set of all possible subsets of size d, Xd , of the p possible measurements, x1; : : : ; x p. Thus we seek the subset QXd for which  J . QXd   D max X2Xd  J .X    J . QA  D max  A2A J .A.x    In feature extraction, the optimisation is performed over all possible transformations of the variables. The class of transformation is usually speciﬁed  for example, a linear transformation of the variable set  and we seek the transformation, QA, for which  where A is the set of allowable transformations. The feature vector is then y D QA.x . The above description is very much a simpliﬁcation of dimensionality reduction tech- niques. The criterion function J is usually based on some measure of distance or dis- similarity between distributions, which in turn may require distances between objects to be deﬁned. Distance measures, which are also important in some clustering schemes described in Chapter 10, are discussed in Appendix A.  We conclude this introductory section with a summary of notation. We shall denote the population covariance matrix by  cid:4  and the covariance matrix of class !i by  cid:4 i . We shall denote the maximum likelihood estimates of  cid:4  and  cid:4 i by O cid:4  and O cid:4 i ,  O cid:4 i D 1 ni O cid:4  D 1 n  nX jD1 nX jD1  zi j .x j  cid:5  mi  .x j  cid:5  mi  T  .x j  cid:5  m .x j  cid:5  m T   Feature selection 307  where  and ni D Pn  and m the sample mean  jD1 zi j ; mi is the sample mean of class !i given by  zi j D  ² 1 0  if x j 2 !i otherwise  mi D 1 ni  nX jD1  zi j x j  m D CX iD1  ni n  mi  SW D CX iD1  O cid:4 i  ni n  The unbiased estimate of the covariance matrix is n O cid:4 =.n  cid:5  1 . We shall denote by SW the within-class scatter matrix  or pooled within-class sample covariance matrix ,  with unbiased estimate S D nSW =.n cid:5 C . Finally, we denote by S B the sample between- class covariance matrix  S B D CX iD1  ni n  .mi  cid:5  m .mi  cid:5  m T  and note that SW C S B D O cid:4 .  9.2 Feature selection  The problem Given a set of measurements on p variables, what is the best subset of size d? Thus, we are not considering a transformation of the measurements, merely selecting those d variables that contribute most to discrimination.  The solution Evaluate the optimality criterion for all possible combinations of d vari- ables selected from p and select that combination for which this criterion is a maximum. If it were quite so straightforward as this, then this section would not be so long as  it is. The difﬁculty arises because the number of possible subsets is  nd D  p!  . p  cid:5  d !d!  which can be very large even for moderate values of p and d. For example, selecting the best 10 features out of 25 means that 3 268 760 feature sets must be considered, and evaluating the optimality criterion, J , for every feature set in an acceptable time may not   308 Feature selection and extraction  be feasible. Therefore, we must consider ways of searching through the space of possible variable sets that reduce the amount of computation.  One reason for reducing the number of variables to measure is to eliminate redun- dancy. There is no need to waste effort  time and cost  making measurements on un- necessary variables. Also, reducing the number of variables may lead to a lower error rate since, as the number of variables increases, the complexity of the classiﬁer, deﬁned in terms of the number of parameters of the classiﬁer to estimate, also increases. Given a ﬁnite design set, increasing the number of parameters of the classiﬁer can lead to poor generalisation performance, even if the model is ‘correct’, that is, it is the model used to generate the data. The often-quoted example of this is the problem of discrim- inating between two classes of normally distributed data. The discrimination surface is quadratic, but the linear discriminant may give better performance on data sets of limited size.  There are two basic strategies for feature subset selection:  1. Optimal methods: these include exhaustive search methods which are feasible for only very small problems; accelerated search  we shall consider the branch and bound algorithm ; and Monte Carlo methods  such as simulated annealing and genetic al- gorithms; Michalewicz, 1994  which can lead to a globally optimal solution, but are computationally expensive.  2. Suboptimal methods: the optimality of the above strategies is traded for computational  efﬁciency.  The strategy adopted is independent of the optimality criterion, though the computa-  tional requirements do depend on the optimality criterion.  9.2.1 Feature selection criteria  In order to choose a good feature set, we require a means of measuring the ability of a feature set to discriminate accurately between two or more classes. This is achieved by deﬁning a class separability measure that is optimised with respect to the possible subsets. We can choose the feature set in essentially two ways.  1. We can design a classiﬁer on the reduced feature set and choose the feature sets for which the classiﬁer performs well on a separate test validation set. In this approach, the feature set is chosen to match the classiﬁer. A different feature set may result with a different choice of classiﬁer.  2. The second approach is to estimate the overlap between the distributions from which the data are drawn and favour those feature sets for which this overlap is minimal  that is, maximise separability . This is independent of the ﬁnal classiﬁer employed and it has the advantage that it is often fairly cheap to implement, but it has the disadvantage that the assumptions made in determining the overlap are often crude and may result in a poor estimate of the discriminability.   Feature selection 309  Error rate A minimum expected classiﬁcation error rate is often the main aim in classiﬁer de- sign. Error rate  or misclassiﬁcation rate  is simply the proportion of samples incor- rectly classiﬁed. Optimistic estimates of a classiﬁer’s performance will result if the data used to design the classiﬁer are na¨ıvely used to estimate the error rate. Such an estimate is termed the apparent error rate. Estimation should be based on a separate test set, but if data are limited in number, we may wish to use all available data in classiﬁer design. Procedures such as the jackknife and the bootstrap have been devel- oped to reduce the bias of the apparent error rate. Error rate estimates are discussed in Chapter 8.  Probabilistic distance Probabilistic distance measures the distance between two distributions, p.xj!1  and p.xj!2 , and can be used in feature selection. For example, the divergence is given by  Z  JD.!1; !2  D  [ p.xj!1   cid:5  p.xj!2 ] log  ¦  ² p.xj!1  p.xj!2   dx  A review of measures is given in Appendix A. All of the measures given in that appendix can be shown to give a bound on the error probability and have the property that they are maximised when classes are disjoint. In practice, it is not the tightness of the bound that is important but the computational requirements. Many of the commonly used dis- tance measures, including those in Appendix A, simplify for normal distributions. The divergence becomes  JD D 1  2  .µ2  cid:5  µ1 T . cid:4    cid:5 1 1 C  cid:4    cid:5 1 2   .µ2  cid:5  µ1  C Trf cid:4    cid:5 1 1  cid:4 2 C  cid:4    cid:5 1 1  cid:4 2  cid:5  2Ig  for normal distributions with means µ1 and µ2 and covariance matrices  cid:4 1 and  cid:4 2.  In a multiclass problem, the pairwise distance measures must be adapted. We may  take as our cost function, J , the maximum overlap over all pairwise measures,  or the average of the pairwise measures,  J D max i; j .i6D j    J .!i ; ! j    J D X  i < j  J .!i ; ! j   p.!i   p.! j    Recursive calculation of separability measures The search algorithms described later in this chapter, both optimal and suboptimal, con- struct the feature sets at the ith stage of the algorithm from that at the .i  cid:5  1 th stage by the addition or subtraction of a small number of features from the current optimal set. For the parametric forms of the probabilistic distance criteria, the value of the criterion function at stage i can be evaluated by updating its value already calculated for stage i  cid:5  1 instead of computing the criterion functions from their deﬁnitions. This can result in substantial computational savings.   310 Feature selection and extraction  All the parametric measures given in Appendix A, namely the Chernoff, Bhattacharyya, divergence, Patrick–Fischer and Mahalanobis distances, are functions of three basic building blocks of the form  xT S   cid:5 1x; TrfT S   cid:5 1g;  jSj   9.1   where S and T are positive deﬁnite symmetric matrices and x is a vector of parameters. Thus, to calculate the criteria recursively, we only need to consider each of the building blocks. For a k ð k positive deﬁnite matrix S, let QS be the matrix with the kth element of the feature vector removed. Then S may be written ½  S D   cid:6  QS yT  y skk  Assuming that QS   cid:5 1  is known, then S cid:5 1 may be written as  cid:5 1  cid:5  1 d   cid:5 1  QS   cid:5 1 C 1 d  cid:5  1 d  yyT QS  cid:5 1  QS yT QS   cid:5 1 D  S  2 664  QS  1 d   cid:5 1  y  3 775  where d D skk  cid:5  yT QS   cid:5 1  y. Alternatively, if we know S cid:5 1, which may be written as   9.2   then we may calculate QS   cid:5 1  as   cid:5 1 D  S  ½   cid:6  A c cT b  QS   cid:5 1 D A  cid:5  1 b  ccT  In some cases it may not be necessary to calculate the inverse QS  Thus, we can compute the inverse of a matrix if we know the inverse before a feature is added to or deleted from the feature set. from S cid:5 1. Consider the quadratic form xT S cid:5 1x, where x is a k-dimensional vector and Qx denotes the vector with the kth value removed. This can be expressed in terms of the quadratic form before the kth feature is removed as QxT QS   cid:5 1 Qx D xT S  [.cT : b x]2   cid:5 1   cid:5 1x  cid:5  1 b  where [cT : b] is the row of S cid:5 1 corresponding to the feature that is removed. Thus, the calculation of QS can be deferred until it is conﬁrmed that this feature is to be permanently removed from the candidate feature set.  The second term to consider in  9.1  is TrfT S cid:5 1g. We may use the relationship   cid:5 1  Trf QT QS   cid:5 1g D TrfT S   cid:5 1g  cid:5  1 b  .cT : b T   cid:15    cid:14 c  b   9.3   Finally, the determinants satisfy  jSj D .skk  cid:5  yT QSy jQSj   Feature selection 311  Criteria based on scatter matrices The probabilistic distance measures require knowledge, or estimation, of a multivariate probability density function followed by numerical integration, except in the case of a parametric density function. This is clearly computationally very expensive. Other mea- sures have been developed based on the between-class and within-class scatter matrices. Those given in this chapter are discussed in more detail by Devijver and Kittler  1982 .  We deﬁne a measure of the separation between two data sets, !1 and !2, as  for xi 2 !1, y j 2 !2 and d.x; y  a measure of distance between samples x and y. This is the average separation. Deﬁning the average distance between classes as  Jas D 1 n1n2  n1X iD1  n2X jD1  d.xi ; y j    J D 1 2  CX iD1  CX jD1  p.!i    p.! j  Jas .!i ; ! j    where p.!i   is the prior probability of class !i  estimated as pi D ni =n , the measure J may be written, using a Euclidean distance squared for d.x; y ,  J D J1 D TrfSW C S Bg D Trf O cid:4 g  The criterion J1 is not very satisfactory as a feature selection criterion: it is simply  the total variance, which does not depend on class information.  Our aim is to ﬁnd a set of variables for which the within-class spread is small and the between-class spread is large in some sense. Several criteria have been proposed for achieving this. One popular measure is  Another is the ratio of the determinants  the ratio of the total scatter to the within-class scatter. A further measure used is  As with the probabilistic distance measures, each of these distance measures may be calculated recursively.  9.2.2 Search algorithms for feature selection  The problem of feature selection is to choose the ‘best’ possible subset of size d from a set of p features. In this section we consider strategies for doing that – both optimal and  J2 D TrfS   cid:5 1 W S Bg  J3 D j O cid:4 j jSWj  J4 D TrfS Bg TrfSWg   9.4    9.5    9.6    312 Feature selection and extraction  suboptimal. The basic approach is to build up a set of d features incrementally, starting with the empty set  a ‘bottom-up’ method  or to start with the full set of measurements and remove redundant features successively  a ‘top-down’ approach .  If Xk represents a set of k features or variables then, in a bottom-up approach, the QXk, is the set for which the feature  extraction  selection  best set at a given iteration, criterion has its maximum value  J . QXk   D max X2Xk  J .X    The set Xk of all sets of features at a given step is determined from the set at the previous iteration. This means that the set of measurements at one stage of an iterative procedure is used as a starting point to ﬁnd the set at the next stage. This does not imply that the sets are necessarily nested   QXk ² QXkC1 , though they may be.  Branch and bound procedure This is an optimal search procedure that does not involve exhaustive search. It is a top- down procedure, beginning with the set of p variables and constructing a tree by deleting variables successively. It relies on one very important property of the feature selection criterion, namely that for two subsets of the variables, X and Y ,  X ² Y   J .X   < J .Y     9.7   That is, evaluating the feature selection criterion on a subset of variables of a given set yields a smaller value of the feature selection criterion. This is termed the monotonicity property.  We shall describe the method by way of example. Let us assume that we wish to ﬁnd the best three variables out of a set of ﬁve. A tree is constructed whose nodes represent all possible subsets of cardinality 3, 4 and 5 of the total set as follows. Level 0 in the tree contains a single node representing the total set. Level 1 contains subsets of the total set with one variable removed, and level 2 contains subsets of the total set with two variables removed. The numbers to the right of each node in the tree represent a subset of variables. The number to the left represents the variable that has been removed from the subset of the parent node in order to arrive at a subset for the child node. Level 2 contains all possible subsets of ﬁve variables of size three. Note that the tree is not symmetrical. This is because removing variables 4 then 5 from the original set  to give the subset  123   has the same result as removing variable 5 then variable 4. Therefore, in order for the subsets not to be replicated, we have only allowed variables to be removed in increasing order. This removes unnecessary repetitions in the calculation.  Now we have obtained our tree structure, how are we going to use it? The tree is searched from the least dense part to the part with the most branches  right to left in Figure 9.2 . Figure 9.3 gives a tree structure with values of the criterion J printed at the nodes. Starting at the rightmost set  the set  123  with a value of J D 77:2 , the search backtracks to the nearest branching node and proceeds down the rightmost branch evaluating J .f1; 2; 4; 5g , then J .f1; 2; 4g  which gives a lower value than the current maximum value of J , JŁ, and so is discarded. The set J .f1; 2; 5g  is next evaluated and retained as the current best value  largest on a subset of three variables , JŁ D 80:1. J .f1; 3; 4; 5g  is evaluated next, and since this is less than the current best, the search of   Feature selection 313  LEVEL 0   cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1   2  0   cid:2    cid:2    1,2,3,4,5    cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4   cid:3  cid:3  cid:3  cid:3  cid:3  cid:3  cid:3   cid:3  cid:3  cid:3  cid:3  cid:3  cid:3  cid:3    1,2,4,5   cid:7   cid:6   cid:7   cid:6   cid:7   cid:6  cid:6    1,3,4,5   cid:8    cid:2  cid:2   cid:8    cid:8  cid:8    cid:7    cid:7   4  3   cid:6    1,2,3,5   cid:6    cid:6  cid:6   LEVEL 1  1  cid:5   cid:2  cid:2    cid:5   cid:2   cid:2    2,3,4,5   cid:7   cid:6   cid:7   cid:6   cid:7   cid:6  cid:6    cid:7    cid:7    cid:5    cid:5    cid:5   2  3,4,5  3  2,4,5  4  2,3,5  5  2,3,4  3  1,4,5  4  1,3,5  5  1,3,4  4  1,2,5  5  1,2,4  5  1,2,3   Figure 9.2 Tree ﬁgure for branch and bound method   cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1    cid:1  cid:1  cid:9   92.1   2,3,4,5   60.9   1,2,3,4,5    cid:2   cid:2  cid:2  cid:11   cid:2   cid:2  cid:2  cid:10   cid:2  cid:2   cid:8    cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4   cid:3  cid:3  cid:3  cid:3  cid:3  cid:3  cid:3   cid:3  cid:3  cid:3  cid:3  cid:3  cid:3  cid:3    cid:3  cid:3  cid:13   cid:4  cid:4  cid:14   cid:4  cid:4  cid:15   cid:3  cid:3  cid:12   cid:7   cid:6   cid:7   cid:6  cid:6  cid:16   cid:6   cid:6  cid:6  cid:17   cid:6  cid:6    cid:7  cid:7  cid:18   cid:7   cid:7  cid:7  cid:19   cid:7    cid:8  cid:8    cid:7  cid:7    cid:8    1,3,4,5    1,2,4,5   80.4  81.6   1,2,3,5    cid:6   cid:6  cid:6  cid:16   cid:6   cid:6  cid:6  cid:17   cid:6  cid:6   80.1  1,2,5   76.2  1,2,4   77.2  1,2,3    cid:5   cid:2   cid:2    cid:7   cid:6   cid:6   76.7   cid:5   cid:2  cid:2    cid:7   cid:6  cid:6    cid:7    cid:7    cid:7  cid:7    cid:5    cid:5    cid:5  cid:5   Figure 9.3 Tree ﬁgure for branch and bound method with feature selection criterion value at each node  the section of the tree originating from this node is not performed. This is because we know from the monotonicity property that all subsets of this set will yield a lower value of the criterion function. The algorithm then backtracks to the nearest branching node and proceeds down the next rightmost branch  in this case, the ﬁnal branch . J .f2; 3; 4; 5g  is evaluated, and again since this is lower than the current best value on a subset of three variables, the remaining part of the tree is not evaluated.  Thus, although not all subsets of size 3 are evaluated, the algorithm is optimal since  we know by condition  9.7  that those not evaluated will yield a lower value of J .  From the speciﬁc example above, we can see a more general strategy: start at the top level and proceed down the rightmost branch, evaluating the cost function J at each node. If the value of J is less than the current threshold then abandon the search down that particular branch and backtrack to the previous branching node. Continue the search down the next rightmost branch. If, on the search of any branch the bottom level is reached  as is bound to happen on the initial branch , then if the value of J for this level is larger than the current threshold, the threshold is updated and backtracking begins. Note from Figure 9.2  and you can verify that this is true in general , that the candidates for removal at level i, given that variable ni cid:5 1 has been removed at the previous level, are  ni cid:5 1 C 1; : : : ; i C m  where m is the size of the ﬁnal subset.  Note that if the criterion function is evaluated for the successor of a given node, i.e. for a node which is one level below and connected by a single link to a given node, then during the branch and bound procedure it will be evaluated for all ‘brothers and sisters’ of that node – i.e. for all other direct successors of the given node. Now since a node with a low value of J is more likely to be discarded than a node with a high value of J , it is sensible to order these sibling nodes so that those that have lower values have   314 Feature selection and extraction  more branches. This is the case in Figure 9.3 where the nodes at level 2 are ordered so that those yielding a smaller value of J have the larger number of branches. Since all the sibling feature sets will be evaluated anyway, this results in no extra computation. This scheme is due to Narendra and Fukunaga  1977 .  The feature selection criterion most commonly used in the literature for feature se- lection both in regression  Miller, 1990  and in classiﬁcation  Narendra and Fukunaga, 1977  is the quadratic form  xT k S   cid:5 1 k xk  where xk is a k-dimensional vector and Sk is a k ð k positive deﬁnite matrix when k features are used. For example, in a two-class problem, the Mahalanobis distance  see Appendix A  between two groups with means µi .i D 1; 2  and covariance matrices  cid:4 i .i D 1; 2   J D .µ1  cid:5  µ2 T   cid:14   cid:4 1 C  cid:4 2   cid:15  cid:5 1  2  .µ1  cid:5  µ2    9.8   satisﬁes  9.7 . In a multiclass problem, we may take the sum over all pairs of classes. This will also satisfy  9.7  since each component of the sum does. There are many other feature selection criteria satisfying the monotonicity criterion, including probabilistic distance measures  for example, Bhattacharyya distance, divergence  Fukunaga, 1990   and measures based on the scatter matrices  for example, J2  9.4 , but not J3  9.5  or J4  9.6  .  9.2.3 Suboptimal search algorithms  There are many problems where suboptimal methods must be used. The branch and bound algorithm may not be computationally feasible  the growth in the number of possibilities that must be examined is still an exponential function of the number of variables  or may not be appropriate if the monotonicity property  9.7  does not hold. Suboptimal algorithms, although not capable of examining every feature combination, will assess a set of potentially useful feature combinations. We consider several techniques, varying in complexity.  Best individual N The simplest method, and perhaps the one giving the poorest performance, for choosing the best N features is to assign a discrimination power estimate to each of the features in the original set, X , individually. Thus, the features are ordered so that  and we select as our best set of N features the N features with the best individual scores:  J .x1  ½ J .x2  ½ ÐÐÐ ½ J .x p   fxiji  cid:17  Ng  In some cases this method can produce reasonable feature sets, especially if the features in the original set are uncorrelated, since the method ignores multivariate relationships.   Feature selection 315  However, if the features of the original set are highly correlated, the chosen feature set will be suboptimal as some of the features will be adding little discriminatory power. There are cases when the N best features are not the best N features even when the variables are independent  Hand, 1981a .  Sequential forward selection Sequential forward selection  SFS, or the method of set addition  is a bottom-up search procedure that adds new features to a feature set one at a time until the ﬁnal feature set is reached. Suppose we have a set of d1 features, Xd1. For each of the features ¾ j not yet selected  i.e. in X  cid:5  Xd1  the criterion function J j D J .Xd1 C ¾ j   is evaluated. The feature that yields the maximum value of J j is chosen as the one that is added to the set Xd1. Thus, at each stage, the variable is chosen that, when added to the current set, maximises the selection criterion. The feature set is initialised to the null set. When the best improvement makes the feature set worse, or when the maximum allowable number of features is reached, the algorithm terminates. The main disadvantage of the method is that it does not include a mechanism for deleting features from the feature set once they have been added should further additions render them unnecessary.  Generalised sequential forward selection Instead of adding a single feature at a time to a set of measurements, in the generalised sequential forward selection  GSFS  algorithm r features are added as follows. Suppose we have a set of d1 measurements, Xd1. All possible sets of size r are generated from the remaining n  cid:5  d1 features – this gives   cid:14 n  cid:5  d1   cid:15   r  sets. For each set of r features, Yr , the cost function is evaluated for Xd1 C Yr and the set that maximises the cost function is used to increment the feature set. This is more costly than SFS, but has the advantage that at each stage it is possible to take into account to some degree the statistical relationship between the available measurements.  Sequential backward selection Sequential backward selection  SBS , or sequential backward elimination, is the top- down analogy to SFS. Variables are deleted one at a time until d measurements remain. Starting with the complete set, the variable ¾ j is chosen for which J .X  cid:5  ¾ j   is the largest  i.e. ¾ j decreases J the least . The new set is fX  cid:5  ¾ jg. This process is repeated until a set of the required cardinality remains. The procedure has the disadvantage over SFS that it is computationally more demanding since the criterion function J is evaluated over larger sets of variables.  Generalised sequential backward selection If you have read the previous sections, you will not be surprised to learn that generalised sequential backward selection  GSBS  decreases the current set of variables by several variables at a time.   316 Feature selection and extraction  Plus l – take away r selection This is a procedure that allows some backtracking in the feature selection process. If l > r, it is a bottom-up procedure. l features are added to the current set using SFS and then the worst r features are removed using SBS. This algorithm removes the problem of nesting since the set of features obtained at a given stage is not necessarily a subset of the features at the next stage of the procedure. If l < r then the procedure is top-down, starting with the complete set of features, removing r, then adding l successively until the required number is achieved.  Generalised plus l – take away r selection The generalised version of the l–r algorithm uses the GSFS and the GSBS algorithms at each stage rather than the SFS and SBS procedures. Kittler  1978a  generalises the procedure further by allowing the integers l and r to be composed of several components li ; i D 1; : : : ; nl, and r j ; j D 1; : : : ; nr  where nl and nr are the number of components , satisfying  0  cid:17  li  cid:17  l nlX li D l iD1  0  cid:17  r j  cid:17  r nrX r j D r jD1  In this generalisation, instead of applying the generalised sequential forward selection in one step of l variables  denoted GSFS l  , the feature set is incremented in nl steps by adding li features .i D 1; : : : ; nl   at each increment; i.e. apply GSFS li   successively for i D 1; : : : ; nl. This reduces the computational complexity. Similarly, GSBS r  is replaced by applying GSBS r j  , j D 1; : : : ; nr , successively. The algorithm is referred to as the .Zl ; Zr   algorithm, where Zl and Zr denote the sequence of integers li and l j ,  Zl D .l1; l2; : : : ; lnl Zr D .r1; r2; : : : ; rnr        The suboptimal search algorithms discussed in this subsection and the exhaustive search strategy may be considered as special cases of the .Zl ; Zr   algorithm  Devijver and Kittler, 1982 .  Floating search methods Floating search methods, sequential forward ﬂoating selection  SFFS  and sequential backward ﬂoating selection  SBFS , may be regarded as a development of the l–r al- gorithm above in which the values of l and r are allowed to ‘ﬂoat’ – that is, they may change at different stages of the selection procedure. Suppose that at stage k we have a set of subsets X1; : : : ; Xk of sizes 1 to k respectively. Let the corresponding values of the feature selection criteria be J1 to Jk, where Ji D J .Xi  , for the feature selection criterion, J .: . Let the total set of features be X . At the kth stage of the SFFS procedure, do the following. 1. Select the feature x j from Y  cid:5  Xk that increases the value of J the greatest and add  it to the current set, XkC1 D Xk C x j .   Feature selection 317  2. Find the feature, xr , in the current set, XkC1, that reduces the value of J the least; if this feature is the same as x j then set JkC1 D J .XkC1 ; increment k; go to step 1; otherwise remove it from the set to form X0  k D XkC1  cid:5  xr .  3. Continue removing features from the set X0  k to form reduced sets X0  k cid:5 1 while  k cid:5 1    > Jk cid:5 1; k D k  cid:5  1; or k D 2; then continue with step 1.  J .X0 The algorithm is initialised by setting k D 0 and X0 D Þ  the empty set  and using  the SFS method until a set of size 2 is obtained.  9.2.4 Example application study  The problem Classiﬁcation of land use using synthetic aperture radar images  Zongker and Jain, 1996 .  Summary As part of an evaluation of feature selection algorithms on several data sets  real and simulated , classiﬁcation performance using features produced by the SFS algorithm is compared with that using the SFFS algorithm.  The data The data comprised synthetic aperture radar images  approximately 22 000 pixels . A total of 18 features were computed from four different texture models: local statistics  ﬁve features , grey level co-occurrence matrices  six features , fractal features  two features  and a log-normal random ﬁeld model  ﬁve features . One of the goals is to see how measures from different models may be utilised to provide better performance.  The model The best performance was assessed in terms of recognition rate of a 3- nearest-neighbour classiﬁer.  Training procedure The data were divided into independent train and test sets and the performance of the classiﬁer evaluated as a function of the number of features produced by the SFS and SFFS algorithms.  Results The recognition rate is not a monotonic function of the number of features. The best performance was achieved by SFFS using an 11-feature subset.  9.2.5 Further developments  There are other approaches to feature selection. Chang  1973  considers a dynamic pro- gramming approach. Monte Carlo methods based on simulated annealing and genetic algorithms  see, for example, Mitchell, 1997  are described by Siedlecki and Sklansky  1988 , Brill et al.  1992  and Chang and Lippmann  1991 .  Developments of the ﬂoating search methods to adaptive ﬂoating search algorithms that determine the number of additions or deletions dynamically as the algorithm proceeds are proposed by Somol et al.  1999 . Other approaches include node pruning in neural   318 Feature selection and extraction  networks and methods based on modelling the class densities as ﬁnite mixtures of a special type  Pudil et al., 1995 .  9.2.6 Summary  Feature selection is the process of selecting from the original features  or variables  those features that are important for classiﬁcation. A feature selection criterion, J , is deﬁned on subsets of the features and we seek that combination of features for which J is maximised.  In this section we have described some statistical pattern recognition approaches to feature selection, both optimal and suboptimal techniques for maximising J . Some of the techniques are dependent on a speciﬁc classiﬁer through error rate. This may provide computational problems if the classiﬁer is complex. Different feature sets may be obtained for different classiﬁers.  Generally there is a trade-off between algorithms that are computationally feasible, but not optimal, and those that are optimal or close to optimal but are computationally complex even for moderate feature set sizes. Studies of the ﬂoating methods suggest that these offer close to optimal performance at an acceptable cost.  9.3 Linear feature extraction  Feature extraction is the transformation of the original data  using all variables  to a data set with a reduced number of variables.  In the problem of feature selection covered in the previous section, the aim is to select those variables that contain the most discriminatory information. Alternatively, we may wish to limit the number of measurements we make, perhaps on grounds of cost, or we may want to remove redundant or irrelevant information to obtain a less complex classiﬁer.  In feature extraction, all available variables are used and the data are transformed  using a linear or nonlinear transformation  to a reduced dimension space. Thus, the aim is to replace the original variables by a smaller set of underlying variables. There are several reasons for performing feature extraction:  1. to reduce the bandwidth of the input data  with the resulting improvements in speed  and reductions in data requirements ;  2. to provide a relevant set of features for a classiﬁer, resulting in improved performance,  particularly from simple classiﬁers;  3. to reduce redundancy;  4. to recover new meaningful underlying variables or features that describe the data,  leading to greater understanding of the data generation process;   Linear feature extraction 319  5. to produce a low-dimensional representation  ideally in two dimensions  with mini- mum loss of information so that the data may easily be viewed and relationships and structure in the data identiﬁed.  The techniques covered in this section are to be found in the literature on a diverse range of topics. Many are techniques of exploratory data analysis described in textbooks on multivariate analysis. Sometimes referred to as geometric methods or methods of ordination, they make no assumption about the existence of groups or clusters in the data. They have found application in a wide range of subjects including ecology, agricultural science, biology and psychology. Geometric methods are sometimes further categorised as being variable-directed when they are primarily concerned with relationships between variables, or individual-directed when they are primarily concerned with relationships between individuals.  In the pattern recognition literature, the data transformation techniques are termed feature selection in the transformed space or feature extraction and they be supervised  make use of class label information  or unsupervised. They may be based on the optimisation of a class separability measure, such as those described in the previous section.  9.3.1 Principal components analysis  Introduction Principal components analysis originated in work by Pearson  1901 . It is the purpose of principal components analysis to derive new variables  in decreasing order of importance  that are linear combinations of the original variables and are uncorrelated. Geometrically, principal components analysis can be thought of as a rotation of the axes of the original coordinate system to a new set of orthogonal axes that are ordered in terms of the amount of variation of the original data they account for.  One of the reasons for performing a principal components analysis is to ﬁnd a smaller group of underlying variables that describe the data. In order to do this, we hope that the ﬁrst few components will account for most of the variation in the original data. A representation in fewer dimensions may aid the user for the reasons given in the introduction to this chapter. Even if we are able to characterise the data by a few variables, it does not follow that we will be able to assign an interpretation to these new variables. Principal components analysis is a variable-directed technique. It makes no assump- tions about the existence or otherwise of groupings within the data and so is described as an unsupervised feature extraction technique.  So far, our discussion has been purely descriptive. We have introduced many terms without proper deﬁnition. There are several ways in which principal components anal- ysis can be described mathematically, but let us leave aside the mathematics for the time being and continue with a geometrical derivation. We must necessarily conﬁne our illustrations to two dimensions, but nevertheless we shall be able to deﬁne most of the attendant terminology and consider some of the problems of a principal components analysis.  In Figure 9.4 are plotted a dozen objects, with the x and y values for each point in the ﬁgure representing measurements on each of the two variables. They could represent   320 Feature selection and extraction  Š  Š Š  Š  Š  Š  Š  Š  Š  Š  y  1  0.8  0.6  0.4  0.2  0  0  Š  Š  0.2  0.4  0.8  1  0.6  x  Figure 9.4 Principal components line of best ﬁt  the height and weight of a group of individuals, for example, in which case one variable would be measured in metres or centimetres or inches and the other variable in grams or pounds. So the units of measurement may differ.  The problem we want to solve is: what is the best straight line through this set of points? Before we can answer, we must clarify what we mean by ‘best’. If we consider the variable x to be an input variable and y a dependent variable so that we wish to calculate the expected value of y given x, E[yjx], then the best  in a least squares sense  regression line of y on x  y D mx C c  is the line for which the sum of the squared distances of points from the line is a minimum, and the distance of a point from the line is the vertical distance.  If y were the regressor and x the dependent variable, then the linear regression line is the line for which the sum of squares of horizontal distances of points from the line is a minimum. Of course, this gives a different solution.  A good illustration of the two linear regressions on a bivariate distribution is given in Stuart and Ord, 1991.   So we have two lines of best ﬁt, and a point to note is that changing the scale of the variables does not alter the predicted values. If the scale of x is compressed or expanded, the slope of the line changes but the predicted value of y does not alter. Principal components analysis produces a single best line and the constraint that it satisﬁes is that the sum of the squares of the perpendicular distances from the sample points to the line is a minimum  see Figure 9.4 . A standardisation procedure that is often carried out  and almost certainly if the variables are measured in different units  is to make the variance of each variable unity. Thus the data are transformed to new axes, centred at the centroid of the data sample and in coordinates deﬁned in terms of units of standard deviation. The principal components line of best ﬁt is not invariant to changes of scale.  The variable deﬁned by the line of best ﬁt is the ﬁrst principal component. The second principal component is the variable deﬁned by the line that is orthogonal with the ﬁrst and so it is uniquely deﬁned in our two-dimensional example. In a problem with   Linear feature extraction 321  higher-dimensional data, it is the variable deﬁned by the vector orthogonal to the line of best ﬁt of the ﬁrst principal component that, together with the line of best ﬁt, deﬁnes a plane of best ﬁt, i.e. the plane for which the sum of squares of perpendicular distances of points from the plane is a minimum. Successive principal components are deﬁned in a similar way.  Another way of looking at principal components  which we shall derive more formally in Section 9.3.1  is in terms of the variance of the data. If we were to project the data in Figure 9.4 onto the ﬁrst principal axis  that is, the vector deﬁning the ﬁrst principal component , then the variation in the direction of the ﬁrst principal component is proportional to the sum of the squares of the distances from the second principal axis  the constant of proportionality depending on the number of samples, 1=.n  cid:5  1  . Similarly, the variance along the second principal axis is proportional to the sum of the squares of the perpendicular distances from the ﬁrst principal axis. Now, since the total sum of squares is a constant, minimising the sum of squared distances from a given line is the same as maximising the sum of squares from its perpendicular or, by the above, maximising the variance in the direction of the line. This is another way of deriving principal components: ﬁnd the direction that accounts for as much of the variance as possible  the direction along which the variance is a maximum ; the second principal component is deﬁned by the direction orthogonal to the ﬁrst for which the variance is a maximum, and so on. The variances are the principal values.  Principal components analysis produces an orthogonal coordinate system in which the axes are ordered in terms of the amount of variance in the original data for which the corresponding principal components account. If the ﬁrst few principal components account for most of the variation, then these may be used to describe the data, thus leading to a reduced-dimension representation. We might also like to know if the new components can be interpreted as something meaningful in terms of the original variables. This wish is not often granted, and in practice the new components will be difﬁcult to interpret.  Derivation of principal components There are at least three ways in which we can approach the problem of deriving a set of principal components. Let x1; : : : ; x p be our set of original variables and let ¾i ; i D 1; : : : ; p, be linear combinations of these variables  or  ¾i D pX jD1  ai j x j  ξ D AT x  where ξ and x are vectors of random variables and A is the matrix of coefﬁcients. Then we can proceed as follows:  1. We may seek the orthogonal transformation A yielding new variables ¾ j that have stationary values of their variance. This approach, due to Hotelling  1933 , is the one that we choose to present in more detail below.   322 Feature selection and extraction  2. We may seek the orthogonal transformation that gives uncorrelated variables ¾ j .  3. We may consider the problem geometrically and ﬁnd the line for which the sum of squares of perpendicular distances is a minimum, then the plane of best ﬁt and so on. We used this geometric approach in our two-dimensional illustration above  Pearson, 1901 .  Consider the ﬁrst variable ¾1:  ¾1 D pX jD1  a1 j x j  We choose a1 D .a11; a12; : : : ; a1 p T to maximise the variance of ¾1, subject to the constraint aT  1 a1 D ja1j2 D 1. The variance of ¾1 is  var.¾1  D E[¾ 2 D E[aT D aT D aT  1 ]  cid:5  E[¾1]2 1 xxT a1]  cid:5  E[aT .E[xxT ]  cid:5  E[x]E[xT ] a1 1 1  cid:4 a1  1 x]E[xT a1]  where  cid:4  is the covariance matrix of x and E[:] denotes expectation. Finding the sta- 1 a1 D 1 is equivalent to ﬁnding the tionary value of aT unconditional stationary value of  1  cid:4 a1 subject to the constraint aT  f .a1  D aT  1  cid:4 a1  cid:5  ¹aT  1 a1  where ¹ is a Lagrange multiplier.  The method of Lagrange multipliers can be found in most textbooks on mathematical methods; for example, Wylie and Barrett, 1995.  Differentiating with respect to each of the components of a1 in turn and equating to zero gives   cid:4 a1  cid:5  ¹a1 D 0  For a non-trivial solution for a1  that is, a solution other than the null vector , a1 must be an eigenvector of  cid:4  with ¹ an eigenvalue. Now  cid:4  has p eigenvalues ½1; : : : ; ½ p, not all necessarily distinct and not all non-zero, but they can be ordered so that ½1 ½ ½2 ½ ÐÐÐ ½ ½ p ½ 0. We must chose one of these for the value of ¹. Now, since the variance of ξ 1 is  1  cid:4 a1 D ¹aT aT  1 a1  D ¹  j cid:4   cid:5  ¹Ij D 0  and we wish to maximise this variance, then we choose ¹ to be the largest eigenvalue ½1, and a1 is the corresponding eigenvector. This eigenvector will not be unique if the value of ¹ is a repeated root of the characteristic equation  The variable ¾1 is the ﬁrst principal component and has the largest variance of any linear function of the original variables x1; : : : ; x p.   Linear feature extraction 323  The second principal component, ¾2 D aT  2 x, is obtained by choosing the coefﬁcients a2i ; i D 1; : : : ; p, so that the variance of ¾2 is maximised subject to the constraint ja2j D 1 and that ¾2 is uncorrelated with the ﬁrst principal component ¾1. This second constraint implies  E[¾2¾1]  cid:5  E[¾2]E[¾1] D 0  or   9.9  2 a1 D 0, i.e. a2 is orthogonal Using the method of Lagrange’s undetermined multipliers again, we seek the uncon-  and since a1 is an eigenvector of  cid:4 , this is equivalent to aT to a1.  2  cid:4 a1 D 0 aT  strained maximisation of  2  cid:4 a2  cid:5  ¼aT aT  2 a2  cid:5   cid:13 aT  2 a1  Differentiating with respect to the components of a2 and equating to zero gives  2 cid:4 a2  cid:5  2¼a2  cid:5   cid:13 a1 D 0   9.10   Multiplying by aT  1 gives  1 a2 D 0. Also, by  9.9 , aT  1  cid:4 a2 D 0, therefore  cid:13  D 0. Equation  9.10   since aT becomes  2a1 cid:4 a2  cid:5   cid:13  D 0 2  cid:4 a1 D aT   cid:4 a2 D ¼a2  Thus, a2 is also an eigenvector of  cid:4 , orthogonal to a1. Since we are seeking to maximise the variance, it must be the eigenvector corresponding to the largest of the remaining eigenvalues, that is, the second largest eigenvalue overall.  We may continue this argument, with the kth principal component ¾k D aT  k x, where ak is the eigenvector corresponding to the kth largest eigenvalue of  cid:4  and with variance equal to the kth largest eigenvalue.  If some eigenvalues are equal, the solution for the eigenvectors is not unique, but it is always possible to ﬁnd an orthonormal set of eigenvectors for a real symmetric matrix with non-negative eigenvalues.  In matrix notation,  ξ D AT x   9.11   A D [a1; : : : ; a p], the matrix whose columns are the eigenvectors of  cid:4 .  So now we know how to determine the principal components – by performing an eigenvector decomposition of the symmetric positive deﬁnite matrix  cid:4 , and using the eigenvectors as coefﬁcients in the linear combination of the original variables. But how do we determine a reduced-dimension representation of some given data? Let us consider the variance.  The sum of the variances of the principal components is given by  pX iD1  var.¾i   D pX iD1  ½i   324 Feature selection and extraction  the sum of the eigenvalues of the covariance matrix  cid:4 , equal to the total variance of the original variables. We can then say that the ﬁrst k principal components account for  of the total variance.  We can now consider a mapping to a reduced dimension by specifying that the new components must account for at least a fraction d of the total variance. The value of d would be speciﬁed by the user. We then choose k so that  kX iD1  ½i  . pX iD1  ½i  kX iD1  ½i ½ d  pX iD1  ½i ½ k cid:5 1X iD1  ½i  and transform the data to  ξ k D AT k x  where ξ k D .¾1; : : : ; ¾k T and Ak D [a1; : : : ; ak] is a p ð k matrix. Choosing a value of d between 70% and 90% preserves most of the information in x  Jolliffe, 1986 . Jackson  1991  advises against the use of this procedure: it is difﬁcult to choose an appropriate value for d – it is very much problem-speciﬁc.  An alternative approach is to examine the eigenvalue spectrum and see if there is a point where the values fall sharply before levelling off at small values  the ‘scree’ test . We retain those principal components corresponding to the eigenvalues before the cut-off point or ‘elbow’  see Figure 9.5 . However, on occasion the eigenvalues drift downwards with no obvious cutting point and the ﬁrst few eigenvalues account for only a small proportion of the variance.  It is very difﬁcult to determine the ‘right’ number of components and most tests are for limited special cases and assume multivariate normality. Jackson  1991  describes a range of procedures and reports the results of several comparative studies.  4.5  3.5  2.5  1.5  4  3  2  1  0  0.5  0  2  4  6  8  10  12  Figure 9.5 Eigenvalue spectrum   Linear feature extraction 325  Remarks Sampling The derivation of principal components above has assumed that the covari- ance matrix  cid:4  is known. In most practical problems, we shall have an estimate of the sample covariance matrix from a set of sample vectors. We can use the sample covariance matrix to calculate principal components and take these as estimates of the eigenvectors of the covariance matrix  cid:4 . Note also, as far as deriving a reduced-dimension repre- sentation is concerned, the process is distribution-free – a mathematical method with no underlying statistical model. Therefore, unless we are prepared to assume some model for the data, it is difﬁcult to obtain results on how good the estimates of the principal components are.  Standardisation The principal components are dependent on the scales used to mea- sure the original variables. Even if the units used to measure the variables are the same, if one of the variables has a range of values that greatly exceeds the others, then we expect the ﬁrst principal component to lie in the direction of this axis. If the units of each variable differ  for example, height, weight , then the principal components will depend on whether the height is measured in feet, inches or centimetres, etc. This does not occur in regression  which is independent of scale  but it does in principal compo- nents analysis in which we are minimising a perpendicular distance from a point to a line, plane, etc. and right angles do not transform to right angles under changes of scale. The practical solution to this problem is to standardise the data so that the variables have equal range. A common form of standardisation is to transform the data to have zero mean and unit variance, so that we ﬁnd the principal components from the correlation matrix. This gives equal importance to the original variables. We recommend that all data are standardised to zero mean and unit variance. Other forms of standardisation are possible; for example, the data may be transformed logarithmically before a principal components analysis – Baxter  1995  compares several approaches.  Mean correction Equation  9.11  relates the principal components ξ to the observed random vector x. In general, ξ will not have zero mean. In order for the principal components to have zero mean, they should be deﬁned as  ξ D AT .x  cid:5  µ    9.12   for mean µ. In practice µ is the sample mean, m.  Approximation of data samples We have seen that in order to represent data in a reduced dimension, we retain only the ﬁrst few principal components  the number is usually determined by the data . Thus, a data vector x is projected onto the ﬁrst r  say  eigenvectors of the sample covariance matrix, giving .x  cid:5  µ    9.13  where Ar D [a1; : : : ; ar ] is the p ð r matrix whose columns are the ﬁrst r eigenvectors of the sample covariance matrix and ξ r is used to denote the measurements on variables ¾1; : : : ; ¾r  the ﬁrst r principal components .  ξ r D AT  r   326 Feature selection and extraction  In representing a data point as a point in a reduced dimension, there is usually an error involved and it is of interest to know what the point ξ r corresponds to in the original space.  The variable ξ is related to x by equation  9.12   AT D A cid:5 1 , giving  If ξ D .ξ r ; 0 T , a vector with its ﬁrst r components equal to ξ r and the remaining ones equal to zero, then the point corresponding to ξ r , namely xr , is  x D Aξ C µ  xr D A   cid:14 ξ r   cid:15   0  C µ  D Ar ξ r C µ  xr D Ar AT  r  .x  cid:5  µ  C µ  and by  9.13 ,  The transformation Ar AT is of rank r and maps the original data distribution to a r distribution that lies in an r-dimensional subspace  or on a manifold of dimension r  in R p. The vector xr is the position the point x maps down to, given in the original coordinate system  the projection of x onto the space deﬁned by the ﬁrst r principal components ; see Figure 9.6.  Singular value decomposition In Appendix C, the result is given that the right sin- gular vectors of a matrix Z are the eigenvectors of ZT Z. The sample covariance matrix  unbiased estimate  can be written in such a form  1 n  cid:5  1  nX iD1  .xi  cid:5  m .xi  cid:5  m T D 1 n  cid:5  1  QX  T QX  x2  xr2  ¼2   cid:6  cid:17    cid:6   ¾2  ž x  cid:6   cid:6    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:21   ¾1  xr   cid:6    cid:6    cid:6    cid:6    cid:6   Figure 9.6 Reconstruction from projections: x is approximated by x r using the ﬁrst principal component  ¼1  x1 xr1   Linear feature extraction 327  where QX D X  cid:5  1mT , X is the n ð p data matrix, m is the sample mean and 1 is an n-dimensional vector of ones. Therefore, if we deﬁne  Z D  1p n  cid:5  1  QX D  1p n  cid:5  1  .X  cid:5  1mT    then the right singular vectors of Z are the eigenvectors of the covariance matrix and the singular values are standard deviations of the principal components. Furthermore, setting  Z D  1p n  cid:5  1  QXD   cid:5 1 D  1p n  cid:5  1  .X  cid:5  1mT  D   cid:5 1  where D is a p ð p diagonal matrix with Dii equal to the square root of the variance of the original variables, then the right singular vectors of Z are the eigenvectors of the correlation matrix. Thus, given a data matrix, it is not necessary to form the sample covariance matrix in order to determine the principal components. If the singular value decomposition of X  cid:5  1mT is U SV T , where U D [u1; : : : ; u p], S D diag.¦1; : : : ; ¦ p  and V D [v1; : : : ; v p], then the n ð r matrix Zr deﬁned by  Zr D U r  cid:4 r V T  r C 1mT  where U r D [u1; : : : ; ur ], Sr D diag.¦1; : : : ; ¦r   and V r D [v1; : : : ; vr ], is the projection of the original data points onto the hyperplane spanned by the ﬁrst r principal components and passing through the mean.  its eigenvalue ½k exceeds P p iDk  Selection of components There have been many methods proposed for the selection of components in a principal components analysis. There is no single best method as the strategy to adopt will depend on the objectives of the analysis: the set of components that gives a good ﬁt to the data  in a predictive analysis  will differ from that which pro- vides good discrimination  predictive analysis . Ferr´e  1995  and Jackson  1993  provide comparative studies. Jackson ﬁnds the broken stick method one of the most promising, and simple to implement. Observed eigenvectors are considered interpretable if their eigenvalues exceed a threshold based on random data: the kth eigenvector is retained if .1=i  . Prakash and Murty  1995  consider a genetic al- gorithm approach to the selection of components for discrimination. However, probably the most common approach is the percentage of variance method, retaining eigenval- ues that account for approximately 90% of the variance. Another rule of thumb is to retain those eigenvectors with eigenvalues greater than the average  greater than unity for a correlation matrix . For a descriptive analysis, Ferr´e recommends a rule of thumb method. However, if we do use another approach then we must ask ourselves why we are performing a principal components analysis in the ﬁrst place. There is no guarantee that a subset of the variables derived will be better for discrimination than a subset of the original variables.  Interpretation The ﬁrst few principal components are the most important ones, but it may be very difﬁcult to ascribe meaning to the components. One way this may be done is to consider the eigenvector corresponding to a particular component and select those variables for which the coefﬁcients in the eigenvector are relatively large in magnitude. Then a purely subjective analysis takes place in which the user tries to see what these variables have in common.   328 Feature selection and extraction  An alternative approach is to use an algorithm for orthogonal rotation of axes such as the varimax algorithm  Kaiser, 1958, 1959 . This rotates the given set of principal axes so that the variation of the squared loadings for a given component is large. This is achieved by making the loadings large on some variables and small on others, though unfortunately it does not necessarily lead to more easily interpretable principal components. Jackson  1991  considers techniques for the interpretation of principal components, including rotation methods.  Discussion Principal components analysis is often the ﬁrst stage in a data analysis and is often used to reduce the dimensionality of the data while retaining as much as possible of the variation present in the original dataset.  Principal components analysis takes no account of groups within the data  i.e. it is unsupervised . Although separate groups may emerge as a result of projecting data to a reduced dimension, this is not always the case and dimension reduction may obscure the existence of separate groups. Figure 9.7 illustrates a data set in two dimensions with two separate groups and the principal component directions. Projection onto the ﬁrst eigenvector will remove group isolation, while projection onto the second retains group separation. Therefore, although dimension reduction may be necessary, the space spanned by the vectors associated with the ﬁrst few principal components will not necessarily be the best for discrimination.  Summary The stages in performing a principal components analysis are:  1. form the sample covariance matrix or standardise the data by forming the correlation  matrix;  2. perform an eigendecomposition of the correlation matrix.   cid:6  cid:17    cid:6   ¾2  cid:6    cid:6    cid:6    cid:6    cid:6  ?  cid:6  ? ?  cid:6  ? ???  cid:6   x2  ?  ?  ?  ?  ?  ž  ?  ž ž  ?  ?  ?  ?  ž  ž  žž  ?  ž  ž  ž  ž ž   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:21   ¾1   cid:6   žžž  cid:6   cid:6  ž žž  cid:6   cid:6   x1  Figure 9.7 Two-group data and the principal axes   Linear feature extraction 329  Alternatively:  1. standardise the data matrix;  2. perform a singular value decomposition of the standardised data matrix.  For a reduced-dimension representation of the data, project the data onto the ﬁrst m eigenvectors, where, for example, m is chosen using a criterion based on the proportion of variance accounted for.  9.3.2 Karhunen–Lo `eve transformation  A separate section on the Karhunen–Lo`eve transformation, in addition to that on prin- cipal components analysis, may seem superﬂuous at ﬁrst sight. After all, the Karhunen– Lo`eve transformation is, in one of its most basic forms, identical to principal compo- nents analysis. It is included here because there are variants of the method, occurring in the pattern recognition literature under the general heading of Karhunen–Lo`eve expan- sion, that incorporate class information in a way in which principal components analysis does not.  The Karhunen–Lo`eve expansion was originally developed for representing a non- periodic random process as a series of orthogonal functions with uncorrelated coefﬁcients. If x.t   is a random process on [0; T ], then x.t   may be expanded as   9.14   where the xn are random variables and the basis functions  cid:16  are deterministic functions of time satisfying  x.t   D  xn cid:16 n.t    1X nD1  Z T  0   cid:16 n.t   cid:16 Ł  m  .t   D Žmn  where  cid:16 Ł  m is the complex conjugate of  cid:16 m. Deﬁne a correlation function  R.t; s  D E[x.t  xŁ.s ] X  "X X  n  D E D X  n  m  m  xn xŁ  cid:16 n.t   cid:16 Ł  m  m  m   cid:16 n.t   cid:16 Ł .s  .s E[xn xŁ m]    If the coefﬁcients are uncorrelated  E[xn xŁ R.t; s  D X  m] D ¦ 2 ¦ 2 n  n   cid:16 n.t   cid:16 Ł  n  .s   Žmn  then  n  and multiplying by  cid:16 n.s  and integrating gives  Z  R.t; s  cid:16 n.s ds D ¦ 2  n   cid:16 n.t     330 Feature selection and extraction  x D  1X nD1  xnφn  Rφk D ¦ 2  k φk  Thus, the functions  cid:16 n are the eigenfunctions of the integral equation, with kernel R.t; s  and eigenvalues ¦ 2 n . We shall not develop the continuous Karhunen–Lo`eve expansion here but proceed straight away to the discrete case.  If the functions are uniformly sampled, with p samples, then  9.14  becomes   9.15   and the integral equation becomes  where R is now the p ð p matrix with .i; j  th element Ri j D E[xi xŁ j ]. The above equation has only p distinct solutions for φ and so the summation in  9.15  must be truncated to p. The eigenvectors of R are termed the Karhunen–Lo`eve coordinate axes  Devijver and Kittler, 1982 .  Apart from the fact that we have assumed zero mean for the random variable x, this derivation is identical to that for principal components. Other ways of deriving the Karhunen–Lo`eve coordinate axes are given in the literature, but these correspond to other views of principal components analysis, and so the end result is the same.  So where does the Karhunen–Lo`eve transformation differ from principal coordinates analysis in a way which warrants its inclusion in this book? Strictly it does not, except that in the pattern recognition literature various methods for linearly transforming data to a reduced-dimension space deﬁned by eigenvectors of a matrix of second-order statisti- cal moments have been proposed under the umbrella term ‘Karhunen–Lo`eve expansion’. These methods could equally be referred to using the term ‘generalised principal com- ponents’ or something similar.  The properties of the Karhunen–Lo`eve expansion are identical to those of the prin- cipal components analysis. It produces a set of mutually uncorrelated components, and dimensionality reduction can be achieved by selecting those components with the largest variances. There are many variants of the basic method that incorporate class information or that use different criteria for selecting features.  KL1: SELFIC – Self-featuring information-compression In this procedure, class labels are not used and the Karhunen–Lo`eve feature transforma- tion matrix is A D [a1; : : : ; a p], where a j are the eigenvectors of the sample covariance matrix, O cid:4 , associated with the largest eigenvalues  Watanabe, 1985 ,  O cid:4 ai D ½i ai  and ½1 ½ ÐÐÐ ½ ½ p.  This is identical to principal components analysis and is appropriate when class labels  are unavailable  unsupervised learning .  KL2: Within-class information If class information is available for the data, then second-order statistical moments can be calculated in a number of different ways. This leads to different Karhunen–Lo`eve   Linear feature extraction 331  coordinate systems. Chien and Fu  1967  propose using the average within-class covari- ance matrix, SW , as the basis of the transformation. The feature transformation matrix, A, is again the matrix of eigenvectors of SW associated with the largest eigenvalues.  KL3: Discrimination information contained in the means Again, the feature space is constructed from eigenvectors of the averaged within-class covariance matrix, but discriminatory information contained in the class means is used to select the subset of features that will be used in further classiﬁcation studies  Devijver and Kittler, 1982 . For each feature  with eigenvector a j of SW and corresponding eigenvalue ½ j   the quantity  J j D aT  j S B a j ½ j  where S B is the between-class scatter matrix, is evaluated and the coordinate axes ar- ranged in descending order of J j .  KL4: Discrimination information contained in the variances Another means of ordering the feature vectors  eigenvectors a j of SW   is to use the discriminatory information contained in class variances  Kittler and Young, 1973 . There are situations where class mean information is not sufﬁcient to separate the classes and the measure given here uses the dispersion of class-conditional variances. The variance of feature j in the ith class weighted by the prior probability of class !i is given by  where O cid:4 i is the sample covariance matrix of class !i , and, deﬁning ½ j D PC iD1  a discriminatory measure based on the logarithmic entropy function is  ½i j , then  The axes giving low entropy values are selected for discrimination.  A further measure that uses the variances is  Both of the above measures reach their maximum when the factors ½i j =½ j are identical, in which case there is no discriminatory information.  KL5: Compression of discriminatory information contained in class means In the method KL3, the Karhunen–Lo`eve coordinate axes are determined by the eigen- vectors of the averaged within-class covariance matrix and the features that are used  ½i j D p.!i  aT  j  O cid:4 i a j  H j D  cid:5  CX iD1  ½i j ½ j   cid:15    cid:14  ½i j ½ j  log  J j D CY iD1  ½i j ½ j   332 Feature selection and extraction  to represent these data in a reduced-dimension space are determined by ordering the eigenvectors in terms of descending J j . The quantity J j is a measure used to rep- resent the discriminatory information contained in the class means. This discrimina- tory information could be spread over all Karhunen–Lo`eve axes and it is difﬁcult to choose an optimum dimension for the transformed feature space from the values of J j alone. The approach considered here  Kittler and Young, 1973  recognises the fact that in a C-class problem, the class means lie in a space of dimension at most C  cid:5  1, and seeks to ﬁnd a transformation to a space of at most C  cid:5  1 giving uncorrelated features.  This is performed in two stages. First of all, a transformation is found that transforms the data to a space in which the averaged within-class covariance matrix is diagonal. This means that the features in the transformed space are uncorrelated, but also any further orthonormal transformation will still produce uncorrelated features for which the class-centralised vectors are decorrelated. If SW is the average within-class covariance matrix in the original data space, then the transformation that decorrelates the class-centralised vectors is Y D U T X, where U is the matrix of eigenvectors of SW and the average within-class covariance matrix in the transformed space is  0 W D U T SW U D  cid:24   S  where  cid:24  D diag.½1; : : : ; ½n  is the matrix of variances of the transformed features  eigen- values of SW  . If the rank of SW is less than p  equal to r, say , then the ﬁrst stage of dimension reduction is to transform to the r-dimensional space by the transformation r X, U r D [u1; : : : ; ur ], so that U T  0 W D U T  r SW U r D  cid:24 r  S  where  cid:24 r D diag.½1; : : : ; ½r  . If we wish the within-class covariance matrix to be invari- ant to further orthogonal transformations, then it should be the identity matrix. This can be achieved by the transformation Y D  cid:24   cid:5  1 r U T 2  r X so that  cid:5  1 r D I 2   cid:5  1 r U T 2  0 W D  cid:24   r SW Ur  cid:24   S  This is the ﬁrst stage of dimension reduction, and is illustrated in Figure 9.8. It transforms the data so that the average within-class covariance matrix in the new space is the identity matrix. In the new space the between-class covariance matrix, S0  B, is given by  0 B D  cid:24    cid:5  1 r U T 2   cid:5  1 2 r S B Ur  cid:24  r  S  where S B is the between-class covariance matrix in the data space. The second stage of the transformation is to compress the class mean information; i.e. ﬁnd the orthogo- nal transformation that transforms the class mean vectors to a reduced dimension. This transformation, V , is determined by the eigenvectors of S0  B  B V D V Q cid:24  0  S   x2  y2  Linear feature extraction 333  x1  y1  Figure 9.8 contours x T O cid:4   Illustration of the ﬁrst stage of dimension reduction for four groups represented by  cid:5 1 i x D constant  where Q cid:24  D diag.Q½1; : : : ; Q½r   is the matrix of eigenvalues of S0 C  cid:5  1 non-zero eigenvalues and so the ﬁnal transformation is Z D V T [v1; : : : ; v¹] and ¹ is the rank of S0  B. The optimal feature extractor is therefore  B. There are at most ¹ Y , where V ¹ D  where the p ð ¹ linear transformation A is given by  Z D AT X  AT D V T  ¹  cid:24    cid:5  1 r U T 2 r  In this transformed space  00 W D V T 00 B D V T  ¹ S  ¹ S  0 W V ¹ D V T B V ¹ D Q cid:24 ¹ 0  S  S  ¹ V ¹ D I  where Q cid:24 ¹ D diag.Q½1; : : : ; Q½¹  . Thus, the transformation makes the average within-class covariance matrix equal to the identity and the between-class covariance matrix equal to a diagonal matrix  see Figure 9.9 . Usually, all C  cid:5  1 features are selected, but these can be ordered according to the magnitude of Q½i and those with largest eigenvalues selected. The linear transformation can be found by performing two eigenvector decompositions, ﬁrst of SW and then of S0 B, but an alternative approach based on a QR factorisation can be used  Crownover, 1991 .  This two-stage process gives a geometric interpretation of linear discriminant analysis, described in Chapter 4. The feature vectors  columns of the matrix A  can be shown to  z2   cid:22  cid:23   Figure 9.9 Second stage of dimension reduction: orthogonal rotation and projection to diagonalise the between-class covariance matrix  z1   334 Feature selection and extraction  be eigenvectors of the generalised symmetric eigenvector equation  Devijver and Kittler, 1982   S B a D ½SW a  The ﬁrst stage of the transformation is simply a rotation of the coordinate axes fol- lowed by a scaling  assuming that SW is full rank . The second stage comprises a projection of the data onto the hyperplane deﬁned by the class means in this transformed space  of dimension at most C  cid:5  1 , followed by a rotation of the axes. If we are to use all C  cid:5  1 coordinates subsequently in a classiﬁer, then this ﬁnal rotation is irrelevant since any classiﬁer that we construct should be independent of an orthogonal rotation of the axes. Any set of orthogonal axes within the space spanned by the eigenvectors of S0 B with nonzero eigenvalues could be used. We could simply orthogonalise the vectors 1  cid:5  m0; : : : ; m0 m0 i is the mean of class !i in the space deﬁned by the ﬁrst transformation and m0 is the overall mean. However, if we wish to obtain a reduced- dimension display of the data, then it is necessary to perform an eigendecomposition of B to obtain a set of coordinate axes that can be ordered using the values of Q½i , the S0 eigenvalues of S0 B.  C cid:5 1  cid:5  m0, where m0  Example Figures 9.10 and 9.11 give two-dimensional projections for simulated oil pipeline data. This synthetic data set models non-intrusive measurements on a pipeline transporting a mixture of oil, water and gas. The ﬂow in the pipe takes one out of three possible conﬁg- urations: horizontally stratiﬁed, nested annular or homogeneous mixture ﬂow. The data lie in a 12-dimensional measurement space. Figure 9.11 shows that the Karhunen–Lo`eve transformation, KL5, separates the three classes into  approximately  three spherical clus- ters. The principal components projection  Figure 9.10  does not separate the classes, but retains some of the structure  for example, class 3 – denoted  cid:1  – comprises several subgroups .  Discussion All of the above methods have certain features in common. All produce a linear trans- formation to a reduced-dimension space. The transformations are determined using an eigenvector decomposition of a matrix of second-order statistical moments and produce features or components in the new space that are uncorrelated. The features in the new space can be ordered using a measure of discriminability  in the case of labelled data  or approximation error. These methods are summarised in Table 9.1.  We could add to this list the method of common principal components that also determines a coordinate system using matrices of second-order statistical moments. A reduced-dimension representation of the data could be achieved by ordering the principal components using a measure like H j or J j in Table 9.1.  The ﬁnal method  KL5 , derived from a geometric argument, produces a transforma- tion that is identical to the linear discriminant transformation  see Chapter 4 , obtained by maximising a discriminability criterion. It makes no distributional assumptions, but a nearest class mean type rule will be optimal if normal distributions are assumed for the classes.   Linear feature extraction 335   cid:5 1   cid:5 2   cid:5 3   cid:5 4  3  2  1  0  4  3  2  1  0   cid:5 1   cid:5 2   cid:5 3   cid:2   C C C C C C C C C C C C C C C C C C C C C CC C C C C C CC C C C C C C C C C C C C CCCC C C C C C C C C C C C C C CC CC C C C C CC CCC C C C C C C C C C C C CC C C C C C C C C C C C C C CC C C C C C C CC C C C C C C C C C C C C C C C C C C C C C C C C C C C C  cid:2  C C C  cid:2  C C C  cid:2  C C  cid:2   cid:2  C C C C C C  cid:2  C C  cid:2  C C C  cid:2  cid:2   cid:2   cid:2  C C C  cid:2   cid:2   cid:2  CC C C  cid:2  C C C  cid:2   cid:2   cid:2  C C C C C  cid:2   cid:2  C C  cid:2   cid:2   cid:2   cid:2  C C C C C C  cid:2   cid:2   cid:2  cid:2  cid:2   cid:2  C C C  cid:2   cid:2   cid:2  C C C  cid:2   cid:2   cid:2   cid:2  cid:2  C  cid:2   cid:2  cid:2  C C C  cid:2   cid:2   cid:2   cid:2   cid:2  cid:2  cid:2  C  cid:2   cid:2   cid:2  C  cid:2   cid:2   cid:2   cid:2  C  cid:2   cid:2   cid:2   cid:2  C C C  cid:2   cid:2   cid:2   cid:2   cid:2  C C C C C C  cid:2  cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  C C C C  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  CC  cid:2   cid:2  C  cid:2   cid:2  cid:2   cid:2  C C C  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  C C C  cid:2   cid:2   cid:2   cid:2  C CC C  cid:2   cid:2   cid:2   cid:2  C C  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  C  cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  C C  cid:1   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  C C  cid:2  cid:2   cid:2   cid:2   cid:2  cid:2   cid:2  C  cid:2   cid:2   cid:2   cid:2  cid:2  cid:2   cid:2   cid:2   cid:2  cid:2  cid:2  cid:2   cid:2   cid:2   cid:2  C C  cid:2   cid:2   cid:2  cid:2   cid:2  C C  cid:2   cid:2   cid:2   cid:2   cid:2  C CC C  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2  C  cid:2  cid:2   cid:2   cid:2   cid:2   cid:2  C C  cid:2   cid:2   cid:2  C  cid:2   cid:2  C  cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2  C C  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2  C C  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  C C  cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  C C C C  cid:1   cid:1   cid:2   cid:2   cid:2   cid:2   cid:2  C C C  cid:1   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2  cid:2  C C  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  C  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:1   cid:2  C  cid:2   cid:2   cid:1   cid:2   cid:2   cid:2  C C  cid:1   cid:2   cid:2  C C  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:1   cid:1   cid:1   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2  C  cid:1   cid:2   cid:2  C  cid:2   cid:2   cid:2   cid:2   cid:2  C C C  cid:2   cid:2  C C C CC  cid:2  C  cid:2  C C C  cid:1   cid:2   cid:1   cid:2   cid:2  C C  cid:1   cid:2  C  cid:1   cid:2  C  cid:1  C C CC  cid:2  C C C C  cid:1  C  cid:2   cid:2  CC C  cid:1  C C  cid:1  C C C C CC C  cid:1  C C C C  cid:1  C C  cid:1  C C C  cid:1  C C  cid:1  C C  cid:1  C C C  cid:1  C  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1    cid:2    cid:1    cid:1    cid:1    cid:1    cid:1    cid:2  C   cid:1    cid:1    cid:1   cid:1    cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1    cid:1   cid:1   cid:1    cid:1    cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1    cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1    cid:1    cid:1   cid:1    cid:1    cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1  cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1    cid:1    cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1    cid:1    cid:5 8   cid:5 6   cid:5 4   cid:5 2  0  2  4  6  8  Figure 9.10 Projection onto the ﬁrst two principal components for the oil pipeline data  C  C C C C C  C  C C C C C C C C C C C C C C C C C C C C C C  C  C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C CC C C C C C C C C CC C C C C C C C C C CCC C CC CC C CCCC CCC C C C CC CCCC C CC CCC C C CC C C C C C CCCC C C C C C C C CC C C C C C C CC CCC C CCCCC C CC CC C C C CC C C C C C C C C C CC C C C C C C C CC CCCC C C C C C C C C C C C CC C C CC C C CC C C CCC C C C C C C C CC C C C C C C CC C C C C C C C C CC C C C C C C C C C C C CC C C CC CC C C C CC C C C C C C C C C C C C C C C C C C C C C C C C CC C C C C C C C C C C C C C C C C   cid:1   cid:1    cid:1    cid:1   cid:1    cid:2    cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2  cid:2  cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2  cid:2   cid:2  cid:2   cid:2  cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2  cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2  cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2  cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2  cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2   cid:2    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1   cid:1   cid:1    cid:1   cid:1   cid:1    cid:1    cid:1    cid:1    cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1  cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1   cid:1    cid:1    cid:1    cid:5 5   cid:5 4   cid:5 3   cid:5 2   cid:5 1  0  1  2  3  4  Figure 9.11 Projection onto the two linear discriminant directions for the oil pipeline data  9.3.3 Factor analysis  Factor analysis is a multivariate analysis technique that aims to represent a set of variables in terms of a smaller underlying set of variables called factors. Its origin dates back a full the century to work by Charles Spearman, who was concerned with understanding   336 Feature selection and extraction  Table 9.1 Summary of linear transformation methods of feature extraction  Method  Eigenvector  decomposition matrix  Ordering function  KL1  PCA  KL2 KL3  KL4 –  a   KL4 –  b  KL5  O cid:4  SW  SW  SW  SW  SW , S B  ½ j ½ j   cid:15   aT j S B a j =½ j H j D  cid:5  CX ½i j ½ j iD1 J j D aT j S B a j =½ j ½ j  log   cid:14  ½i j ½ j  intelligence, and the technique was developed for analysing the scores of individuals on a number of aptitude tests. Factor analysis has been developed mainly by psychologists and most applications have been in the areas of psychology and the social sciences though others include medicine, geography and meteorology.  Factor analysis is perhaps the most controversial of the multivariate methods and many of the drawbacks to the approach are given by Chatﬁeld and Collins  1980  who recommend that it should not be used in most practical situations. One criticism has been of the subjectivity involved in interpreting the results of a factor analysis and that one of the reasons for the popularity of the method is that ‘it allows the experimenter to impose his preconceived ideas on the data’  Reyment et al., 1984, Chapter 10 . It has also been suggested that factor analysis produces a useful result only in cases where a principal component analysis would have yielded the same result. Whether factor analysis is worth the time to understand it and to carry it out  Hills, 1977  is something for you to judge, but it is a technique which has a considerable amount of support among statisticians.  Given these objections  and there are many more – we shall list some of the drawbacks later on , why do we include a section on factor analysis? Can we not be accused of supporting the myth that factor analysis is actually of some use? Certainly we can; there may be other tools that are more reliable than factor analysis and give the same results. Nevertheless, factor analysis may be appropriate for some particular problems. Another reason for including a short section on factor analysis is to highlight the differences from principal components analysis.  Factor analysis, like principal components analysis, is a variable-directed technique and has sometimes been confused with principal components analysis. Even though both techniques often yield solutions that are very similar, factor analysis differs from principal components analysis in several important respects. The main one is that, whilst principal components analysis is concerned with determining new variables that account of the maximum variance of the observed variables, in factor analysis the factors are chosen to account for the correlations between variables, rather than the variance. Another difference is that in principal components, the new variables  the principal components  are expressed as a linear function of the observed variables, whilst in factor analysis the   Linear feature extraction 337  observed variables are expressed as a linear combination of the unobserved underlying variables or factors. Further differences and similarities will be made clear in the analysis of the following sections.  The factor analysis model Suppose that we have p variables x1; : : : ; x p. The factor analysis model  Harman, 1976  assumes that each variable consists of two parts: a common part and a unique part. Specif- ically, we assume that there are m underlying  or latent  variables or factors ¾1; : : : ; ¾m, so that  xi  cid:5  ¼i D mX kD1  ½ik ¾k C ži  i D 1; : : : ; p   9.16   where µ is the mean of the vector .x1; : : : ; x p T . Without loss of generality, we shall take µ to be zero. The variables ¾k are sometimes termed the common factors since they contribute to all observed variables xi , and ži are the unique or speciﬁc factors, describing the residual variation speciﬁc to the variable xi . The weights ½ik are the factor loadings. Equation  9.16  is usually written as x D  cid:24 ξ C  cid:25 .  In addition, the basic model makes the following assumptions  1. The speciﬁc factors, ži , are uncorrelated with each other and with the common factors,  ¾k; that is, E[ cid:25  cid:25 T ] D  cid:26  D diag. 1; : : : ;   p  and E[ cid:25 ξ T ] D 0.  2. The common factors have zero mean and unit variance. We may make the unit variance assumption since the columns of the p ð m matrix  cid:24 , with .i; k th element ½ik, may be scaled arbitrarily.  With these assumptions we model the data covariance matrix  cid:4  as   cid:4  D  cid:24  cid:27  cid:24 T C  cid:26    cid:4  D  cid:24  cid:24 T C  cid:26   If, further, we assume that the common factors themselves are independent of one another, then  cid:27  D I and   9.17   Equation  9.17  expresses the covariance matrix of the observed variables as the sum of two terms: a diagonal matrix  cid:26  and a matrix  cid:24  cid:24 T that accounts for the covariance in the observed variables. In practice, we use the sample covariance matrix or the sample correlation matrix R in place of  cid:4 . However, a factorisation of the form  9.17  does not necessarily exist, and even if it does it will not be unique. If T is an m ð m orthogonal matrix, then  . cid:24 T  . cid:24 T  T D  cid:24  cid:24 T  Thus, if  cid:24  is a solution for the factor loadings, then so is  cid:24 T , and even though these matrices are different, they can still generate the same covariance structure. Hence, it is always possible to rotate the solution to an alternative ‘better’ solution. The matrix  cid:24  cid:24 T also contributes to the variance of the observed variables, ik C var.ži   ½2  var.xi   D mX kD1   338 Feature selection and extraction  with the contribution to the variance of the variable xi due to the factor loadings being termed the common variance or communality and var.ži   the unique variance of xi .  In addition to the parameters of  cid:24  and  cid:27  that must be obtained, the number of factors is not usually known in advance, though the experimenter may wish to ﬁnd the smallest value of m for which the model ﬁts the data. In practice, this is usually done by increasing m sequentially until a sufﬁcient ﬁt is obtained.  The main theme of this chapter is that of data reduction. In principal components analysis we may project a data vector onto the ﬁrst few eigenvectors of the data covariance matrix to give the component scores  equation  9.13   .x  cid:5  µ   ξ r D AT  r  In factor analysis, the situation is different. The basic equation expresses the observed variables x in terms of the underlying variables ξ x D  cid:24 ξ C  cid:25   For m < p, it is not possible to invert this to express ξ in terms of x and hence calculate factor scores. There are methods for estimating factor scores for a given individual measurement x, and these will be considered later in this section. However, this is yet one more difﬁculty which makes factor analysis less straightforward to use than principal components analysis.  We summarise the basic factor analysis approach in the following steps:  1. Given a set of observations, calculate the sample covariance matrix and perform a  factor analysis for a speciﬁed value of m, the number of factors.  2. Carry out a hypothesis test to see if the data ﬁt the model  Dillon and Goldstein,  1984 . If they do not, return to step 1.  3. Rotate the factors to give as maximum loading on as few factors as possible for each  4.  This step may be omitted if inappropriate.  Group variables under each factor and  variable.  interpret the factors.  5. Estimate the factor scores, giving a representation of the data in a reduced dimension.  Techniques for performing each of these steps will be discussed in subsequent sections. We conclude this section with a summary in Table 9.2 of the main differences between principal components analysis and factor analysis.  Factor solutions In this section, we present two of the available methods of factor extraction. There are other methods available, and a summary of the properties of these different techniques may be found in texts on multivariate analysis  for example, Dillon and Goldstein, 1984 . In general, these different methods will give different solutions that depend on various properties of the data, including data sample size, the number of observation variables   Linear feature extraction 339  Table 9.2 Comparison between factor analysis and principal components analysis  Principal components analysis  Factor analysis  orthogonal set of vectors can easily determine component scores components are unique variable-directed no underlying statistical model explains variance structure pointless if observed variables are uncorrelated scale-dependent  nested solutions  not orthogonal factor scores must be estimated factors are not unique variable-directed model for covariance or correlation matrix covariance structure pointless if observed variables are uncorrelated maximum likelihood estimation overcomes scaling problem not nested solutions  and the magnitude of their communalities. However, for a large number of observations and variables, these methods tend to give similar results, though at the other extreme it is not so clear which is the ‘best’ method.  The principal factor method chooses the ﬁrst factor to account for the largest possible amount of the total communality. The second factor is chosen to account for as much as possible of the remaining total communality, and so on. This is equivalent to ﬁnding the eigenvalues and eigenvectors of the reduced correlation matrix, RŁ, deﬁned as the correlation matrix R with diagonal elements replaced by the communalities. This is the matrix to factor:  Ł D R  cid:5   cid:26  D  cid:24  cid:27  cid:24   R  This of course assumes that we know the communalities. There are several methods for estimating the communalities. One is to estimate the communality of the ith vari- able by the squared multiple correlation of the variable Xi with the remaining p  cid:5  1 variables. Once the communalities have been estimated we may perform an eigenanalysis of RŁ and determine  cid:24 . This may then be used  for a predetermined number of fac- tors  to estimate new communalities and a new RŁ calculated. The procedure iter- ates until convergence of communality estimates. However, there is no guarantee that the procedure will converge. Also, it could lead to negative estimates of the speciﬁc variances.  The principal factor method, like principal components analysis, makes no assump- tion as to the underlying distribution of the data. The maximum likelihood method as- sumes a multivariate normal distribution with covariance matrix  cid:4 . The sample co- variance matrix S is Wishart-distributed and the log-likelihood function we seek to maximise is  log.L  D C  cid:5  n 2  flogj cid:4 j C Tr. cid:4    cid:5 1S g   340 Feature selection and extraction  where C is a constant independent of  cid:4 . This is equivalent to minimising  M D Tr. cid:4    cid:5 1S   cid:5  logj cid:4    cid:5 1j  with respect to the parameters   cid:24 ;  cid:26  and  cid:27   of  cid:4  D  cid:24  cid:27  cid:24  C  cid:26 .  We shall not discuss the details of the method. These may be found in  Lawley and Maxwell, 1971; J¨oreskog, 1977 . However, we do note two advantages of the maximum likelihood method of estimation over the principal factor method. The ﬁrst is that the maximum likelihood method enables statistical tests of signiﬁcance for the number of factors to be carried out. The second advantage is that the estimates of the factor loadings are scale-invariant. Therefore we do not have the problem of having to standardise the variables as we did in principal components analysis. For example, the factor loadings p yielded by an analysis of the sample correlation matrix differ from those of the sample covariance matrix of the data by a factor 1=  sii .  Rotation of factors We showed earlier that the model is invariant to an orthogonal transformation of the factors. That is, we may replace the matrix of factor loadings,  cid:24 , by  cid:24 T , where T is a m ð m orthogonal matrix, without changing the approximation of the model to the covariance matrix. This presents us with a degree of ﬂexibility that allows us to rotate factors and may help in their interpretation.  The main aim of rotation techniques is to rotate the factors so that the variables have high loadings on a small number of factors and very small loadings on the remaining factors. There are two basic approaches to factor rotation: orthogonal rotation in which the transformed factors are still independent  so that the factor axes are perpendicular after rotation , and oblique rotation in which the factors are allowed to become correlated  and the factor axes are not necessarily orthogonal after rotation . An example of the former type is the varimax rotation  Kaiser, 1958, 1959 . There are various methods for oblique rotation, including promax and oblimin  Nie et al., 1975 .  Estimating the factor scores In principal components analysis, the principal components are obtained by a linear transformation of the original variables. This linear transformation is determined by the eigenvectors of the covariance matrix or the correlation matrix. Hence it is straightforward to obtain a reduced-dimension representation of an observation. It is also possible to ‘invert’ the process and obtain an approximation to the original variables given a subset of the principal components. In factor analysis, the original variables are described in terms of the factors  see Figure 9.12  plus an error term. Therefore, to obtain the factor scores this process must be reversed. There are several methods for obtaining the factor scores  Jackson, 1991 . One is based on multiple regression analysis  Dillon and Goldstein, 1984 . We suppose that the factor scores are a linear combination, denoted by the matrix A, of the vector of observations, x D .x1; : : : ; x p T ,  ξ D AT x  Post-multiplying by xT , taking expectations and noting that E[xxT ] D R, E[ξ xT ] D  cid:24 T gives   cid:24 T D AT R or AT D  cid:24 T R   cid:5 1   Linear feature extraction 341   cid:3  cid:4  PC3  cid:1  cid:2   PCA  cid:3  cid:4  PC2  cid:1  cid:2   cid:24   cid:8    cid:3  cid:4  PC1  cid:1  cid:2   cid:8    cid:8    cid:24    cid:8    cid:24    cid:20  cid:20  cid:20  cid:20  cid:20  cid:21  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   cid:3  cid:3  cid:3  cid:3  cid:3  cid:12  cid:3  cid:3  cid:3  cid:3  cid:3  cid:3  cid:3    cid:8   cid:24    cid:24   cid:8    cid:8   cid:24    cid:24   cid:8    cid:8  cid:26    cid:23    cid:24    cid:8  cid:26    cid:24  cid:25    cid:24  cid:25    cid:8    cid:24    cid:8   cid:24  x2   cid:23   x1   cid:23   x3  FA   cid:3  cid:4  CF1  cid:1  cid:2    cid:3  cid:4  CF1  cid:1  cid:2   cid:3  cid:3  cid:13   cid:8   cid:8  cid:30    cid:27    cid:20  cid:20  cid:28   cid:24   cid:24  cid:29    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20    cid:3  cid:3  cid:3  cid:3  cid:3  cid:3    cid:8    cid:24    cid:24   cid:8  x2   cid:27   x3  x1  Figure 9.12 Factor analysis and principal components analysis models: PC – principal component variables; CF – common factors  so that an estimate of the factor scores, Oξ is Oξ D  cid:24 R   cid:5 1x  In practice, the matrix R would be replaced by its sample-based estimate.  How many factors? So far, we have discussed techniques for estimating the factor loadings and factor scores, but we have not answered the very important question as to how many factors to choose. For the principal factor method, a simple criterion is to choose the number of factors to be equal to the number of eigenvalues of the reduced correlation matrix that are greater than unity.  With the maximum likelihood solution, a more formal procedure may be applied. The null hypothesis is that all the population variance has been extracted by the postulated number of factors. The test statistic is  Everitt and Dunn, 1991    cid:14   log  j cid:4 j jSj C TrfS cid:4    cid:15  cid:14    cid:5 1g  cid:5  p  n  cid:5  1  cid:5  1 6  .2 p C 5   cid:5  2 3  m   cid:15   which is asymptotically distributed as  cid:20  2 with .. p cid:5  m 2 cid:5  p cid:5  m =2 degrees of freedom. If, at a speciﬁed probability level, the value of  cid:20  2 is signiﬁcant, then we conclude that more factors are needed to account for the correlations in the observed variables.  Discussion Factor analysis is a very widely used multivariate technique that attempts to discover the relationships between a set of observed variables by expressing those variables in terms of a set of underlying, unobservable variables or factors. As we have presented it, it is an exploratory data analysis technique, though another form that we shall not describe is conﬁrmatory factor analysis. This is used when the experimenter wishes to see if the data ﬁt a particular model.  Factor analysis is a much criticised method and some of the objections to its use are:  1. It is complicated. It has been suggested that in many of the situations when fac- tor analysis gives reasonable results, it is only because it is simulating a principal components analysis, which would be simpler to perform.   342 Feature selection and extraction  2. It requires a large number if assumptions. Even the concept of underlying, unobserv-  able variables may be questionable in many applications.  3. There is no unique solution. There are many methods of obtaining the factor load- ings. Once these have been obtained, the factors may be rotated, giving yet different solutions and a different interpretation. This is where factor analysis has been heavily criticised as the experimenter can be accused of rotating the factors until the factors that are sought are arrived at.  4. As a method of dimension reduction it has the disadvantage that the factor scores are not easily obtained. Unlike in principal components analysis where the component scores may be obtained as a linear function of the observed variable values, the basic factor analysis equation cannot be inverted.  5. The number of factors is unknown and a test must be carried out to see if the assumed model is ‘correct’. Also, because of the arbitrary rotation of the factors, the solution for the factors  as determined by the loadings  for an m-factor model is not a subset of the solution for the factors for an  m C 1 -factor model  i.e. the solutions are not nested .  The main point to make is that the model should not be taken too seriously. We would recommend that in most practical situations it is better to use another multivariate analysis technique. For data reduction, leading to subsequent analysis, we recommend that if you require an unsupervised linear technique then you should use principal components analysis. It is much simpler and will do just as well.  Principal components analysis and factor analysis are not the same procedures al- though there has been some confusion between the two types of analysis, and this is not helped by some statistical software packages  a description of these packages is given by Jackson, 1991 . Sometimes the terms ‘factor analysis’ and ‘principal components analysis’ are used synonymously.  9.3.4 Example application study  The problem Monitoring changes in land use for planners and government ofﬁcials  Li and Yeh, 1998 .  Summary The application of remote sensing to inventory land resources and to evaluate the impacts of urban developments is addressed. Remote sensing is considered as a fast and efﬁcient means of assessing such developments when detailed ‘ground truth’ data are unavailable. The aim is to determine the type, amount and location of land use change.  The data The data consist of satellite images, measured in ﬁve wavebands, from two images of the same region measured ﬁve years apart. A 10-dimensional feature vector is constructed  consisting of pixel measurements in the ﬁve wavebands over the two images  and data gathered over the whole region.  The model A standard principal components analysis is performed.   Linear feature extraction 343  Training procedure Each variable is standardised to zero mean and unit variance and a principal components analysis performed. The ﬁrst few principal components account for 97% of the variance. The data are projected onto the subspace characterised by the principal components, and subregions  identiﬁed from a compressed PCA image  labelled manually according to land use change  16 classes of land use, determined by ﬁeld data .  9.3.5 Further developments  There are various generalisations of the basic linear approaches to feature extraction described in this section. Principal components analysis is still an interesting and active area of research. Common principal components analysis  Flury, 1988 , is a generalisation of principal components analysis to the multigroup situation. The common principal components model assumes that the covariance matrices of each group or class,  cid:4 i , can be written   cid:4 i D β cid:24 i β T  where  cid:24 i D diag.½i1; : : : ; ½i p , the diagonal matrix of eigenvalues of the ith group. Thus, the eigenvectors β are common between groups, but the  cid:24 i are different.  There have been several developments to nonlinear principal components analysis, each taking a particular feature of the linear methods and generalising it. The work described by Giﬁ  1990  applies primarily to categorical data. Other extensions are those of principal curves  Hastie and Stuetzle, 1989; Tibshirani, 1992 , and nonlinear principal components based on radial basis functions  Webb, 1996  and kernel functions  Sch¨olkopf et al., 1999 .  Approaches to principal components analysis have been developed for data that may be considered as curves  Ramsay and Dalzell, 1991; Rice and Silverman, 1991; Silver- man, 1995 . Principal components analysis for categorical data and functions is discussed by Jackson  1991 , who also describes robust procedures.  Independent components analysis  Comon, 1994; Hyv¨arinen and Oja, 2000  aims to ﬁnd a linear representation of non-Gaussian data so that the components are statis- tically independent  or as independent as possible . A linear latent variable model is assumed  x D As  where x are the observations, A is a mixing matrix and s is the vector of latent vari- ables. Given T realisations of x, the problem is to estimate the mixing matrix A and the corresponding realisations of s, under the assumption that the components, si , are statistically independent. This technique has found widespread application to problems in signal analysis  medical, ﬁnancial , data compression, image processing and telecom- munications.  Further developments of the linear factor model include models that are nonlinear functions of the latent variables, but still linear in the weights or factor loadings  Etezadi- Amoli and McDonald, 1983 . A neural network approach, termed generative topographic mappings, has been developed by Bishop et al.  1998 .   344 Feature selection and extraction  9.3.6 Summary  All the procedures described in this section construct linear transformations based on matrices of ﬁrst- and second-order statistics:  1. principal components analysis – an unsupervised method based on a correlation or  covariance matrix;  2. Karhunen–Lo`eve transformation – an umbrella term to cover transformations based  on within- and between-class covariance matrices;  3. factor analysis – models observed variables as a linear combination of underlying or  latent variables.  Algorithms for their implementation are readily available.  9.4 Multidimensional scaling  Multidimensional scaling is a term that is applied to a class of techniques that analyses a matrix of distances or dissimilarities  the proximity matrix  in order to produce a rep- resentation of the data points in a reduced-dimension space  termed the representation space . All of the methods of data reduction presented in this chapter so far have analysed the n ð p data matrix X or the sample covariance or correlation matrix. Thus multidi- mensional scaling differs in the form of the data matrix on which it operates. Of course, given a data matrix, we could construct a dissimilarity matrix  provided we deﬁne a suit- able measure of dissimilarity between objects  and then proceed with an analysis using multidimensional scaling techniques. However, data often arise already in the form of dissimilarities and so there is no recourse to the other techniques. Also, in the methods previously discussed, the data-reducing transformation derived has, in each case, been a linear transformation. We shall see that some forms of multidimensional scaling permit a nonlinear data-reducing transformation  if indeed we do have access to data samples rather than proximities . There are many types of multidimensional scaling, but all address the same basic problem: given an n ð n matrix of dissimilarities and a distance measure  usually Eu- clidean , ﬁnd a conﬁguration of n points x1; : : : ; xn in R e so that the distance between a pair of points is close in some sense to the dissimilarity. All methods must ﬁnd the coordinates of the points and the dimension of the space, e. Two basic types of mul- tidimensional scaling  MDS  are metric and non-metric MDS. Metric MDS assumes that the data are quantitative and metric MDS procedures assume a functional relation- ship between the interpoint distances and the given dissimilarities. Non-metric MDS assumes that the data are qualitative, having perhaps ordinal signiﬁcance, and non-metric MDS procedures produce conﬁgurations that attempt to maintain the rank order of the dissimilarities.  Metric MDS appears to have been introduced into the pattern recognition literature by Sammon  1969 . It has been developed to incorporate class information and has also been used to provide nonlinear transformations for dimension reduction for feature extraction.   We begin our discussion with a description of one form of metric MDS, namely  Multidimensional scaling 345  classical scaling.  9.4.1 Classical scaling  i j D Tii C T j j  cid:5  2Ti j d2  Ti j D pX kD1  xik x jk  Given a set of n points in p-dimensional space, x1; : : : ; xn, it is straightforward to cal- culate the Euclidean distance between each pair of points. Classical scaling  or principal coordinates analysis  is concerned with the converse problem: given a matrix of dis- tances, which we assume are Euclidean, how can we determine the coordinates of a set of points in a dimension e  also to be determined from the analysis ? This is achieved via a decomposition of the n ð n matrix T , the between-individual sums of squares and products matrix   9.18  where X D [x1; : : : ; xn]T is the n ð p matrix of coordinates. The distance between two individuals i and j is  T D XXT   9.19   where  where  If we impose the constraint that the centroid of the points xi ; i D 1; : : : ; p, is at the origin, then  9.19  may be inverted to express the elements of the matrix T in terms of the dissimilarity matrix, giving  Ti j D  cid:5  1  i j  cid:5  d2  i:  cid:5  d2  : j C d2 ::]  2 [d2   9.20   i: D 1 d2 n  nX jD1  d2 i j ;  : j D 1 d2 n  nX iD1  d2 i j ;  :: D 1 d2 n2  nX iD1  nX jD1  d2 i j  Equation  9.20  allows us to construct T from a given n ð n dissimilarity matrix D  assuming that the dissimilarities are Euclidean distances . All we need to do now is to factorise the matrix T to make it of the form  9.18 . Since it is a real symmetric matrix, T can be written in the form  see Appendix C   where the columns of U are the eigenvectors of T and  cid:24  is a diagonal matrix of eigenvalues, ½1; : : : ; ½n. Therefore we take  T D U  cid:24 U T  X D U  cid:24   1 2  as our matrix of coordinates. If the matrix of dissimilarities is indeed a matrix of Euclidean distances between points in R p, then the eigenvalues may be ordered  ½1 ½ ÐÐÐ ½ ½n D 0; ½ pC1 D ÐÐÐ D 0   346 Feature selection and extraction  If we are seeking a representation in a reduced dimension then we would use only those eigenvectors associated with the largest eigenvalues. Methods for choosing the number of eigenvalues were discussed in relation to principal components analysis. Brieﬂy, we choose the number r so that  r cid:5 1X iD1  ½i < k  ½i <  ½i  nX iD1  rX iD1  for some prespeciﬁed threshold, k  0 < k < 1 ; alternatively, we use the ‘scree test’.  Then we take  X D [u1; : : : ; ur ]diag.½  : : : ½  1 2  r   D U r  cid:24   1 2 r  1 2 1  as the n ð r matrix of coordinates, where  cid:24 r is the r ð r diagonal matrix with diagonal elements ½i ; i D 1; : : : ; r.  If the dissimilarities are not Euclidean distances, then T is not necessarily positive semideﬁnite and there may be negative eigenvalues. Again we may choose the eigenvec- tors associated with the largest eigenvalues. If the negative eigenvalues are small then this may still lead to a useful representation of the data. In general, the smallest of the set of largest eigenvalues retained should be larger than the magnitude of the most negative eigenvalue. If there is a large number of negative eigenvalues, or some are large in mag- nitude, then classical scaling may be inappropriate. However, classical scaling appears to be robust to departures from Euclidean distance.  If we were to start with a set of data  rather than a matrix of dissimilarities  and seek a reduced-dimension representation of it using the classical scaling approach  by ﬁrst forming a dissimilarity matrix and carrying out the procedure above , then the reduced- dimension representation is exactly the same as carrying out a principal components analysis and calculating the component scores  provided we have chosen Euclidean dis- tance as our measure of dissimilarity . Thus, there is no point in carrying out classical scaling and a principal components analysis on a data set.  9.4.2 Metric multidimensional scaling  Classical scaling is one particular form of metric multidimensional scaling in which an objective function measuring the discrepancy between the given dissimilarities, Ži j , and the derived distances in R e, di j , is optimised. The derived distances depend on the coordinates of the samples that we wish to ﬁnd. There are many forms that the objective function may take. For example, minimisation of the objective function  X 1 cid:17  j <i cid:17 n  i j  cid:5  d2 .Ž2  i j     yields a projection onto the ﬁrst e principal components if Ži j are exactly Euclidean distances. There are other measures of divergence between the sets fŽi jg and fdi jg and the major MDS programs are not consistent in the criterion optimised  Dillon and Goldstein, 1984 . One particular measure is  S D X  i j  ai j .Ži j  cid:5  di j  2   9.21    Multidimensional scaling 347  for weighting factors ai j . Taking  ai j D  ! cid:5 1   X  i j  d2 i j  p  gives S as similar to Kruskal’s stress  Kruskal, 1964a, 1964b , deﬁned in the following section. There are other forms for the ai j  Sammon, 1969; Koontz and Fukunaga, 1972; de Leeuw and Heiser, 1977; Niemann and Weiss, 1979 . The stress is invariant under rigid transformations of the derived conﬁguration  translations, rotations and reﬂections  and also under uniform stretching and shrinking.  A more general form of  9.21  is  where  cid:16  is a member of a predeﬁned class of functions; for example, the class of all linear functions, giving  S D X  i j  ai j . cid:16  .Ži j    cid:5  di j  2  S D X  i j  ai j .a C bŽi j  cid:5  di j  2   9.22    9.23   for parameters a and b. In general, there is no analytic solution for the coordinates of the points in the representation space. Minimisation of  9.22  can proceed by an alternating least squares approach  see Giﬁ, 1990, for further applications of the alternating least squares principle ; that is, by alternating minimisation over  cid:16  and the coordinates. In the linear regression example  9.23 , we would minimise with respect to a and b, for a given initial set of coordinates  and hence the derived distances di j  . Then, keeping a and b ﬁxed, minimise with respect to the coordinates of the data points. This process is repeated until convergence.  The expression  9.22  may be normalised by a function − 2. cid:16 ; X , that is a function of both the coordinates and the function  cid:16 . Choices for − 2 are discussed by de Leeuw and Heiser  1977 .  In psychology, in particular, the measures of dissimilarity that arise have ordinal signiﬁcance at best: their numerical values have little meaning and we are interested only in their order. We can say that one stimulus is larger than another, without being able to attach a numerical value to it. In this case, a choice for the function  cid:16  above is one that belongs to the class of monotone functions. This is the basis of non-metric multidimensional scaling or ordinal scaling.  9.4.3 Ordinal scaling  Ordinal scaling or non-metric multidimensional scaling is a method of ﬁnding a conﬁg- uration of points for which the rank ordering of the interpoint distance is close to the ranking of the values of the given dissimilarities.  In contrast to classical scaling, there is no analytic solution for the conﬁguration of points in ordinal scaling. Further, the procedure is iterative and requires an initial   348 Feature selection and extraction  conﬁguration for the points to be speciﬁed. Several initial conﬁgurations may be tried before an acceptable ﬁnal solution is achieved.  The desired requirement that the ordering of the distances in the derived conﬁguration is the same as that of the given dissimilarities is of course equivalent to saying that the distances are a monotonic function of the dissimilarities. Figure 9.13 gives a plot of distances di j  obtained from a classical scaling analysis  against dissimilarities Ži j for the British town data given in Table 9.3. The numbers in the table are the distances in miles between 10 towns in Britain along routes recommended by the RAC. The relationship is clearly not monotonic, though, on the whole, the larger the dissimilarity the larger the  ž  ž  ž  ž  distance, di j 450  400  350  300  250  200  150  100  50  0  ž  ž  ž  ž ž  ž ž ž  ž ž  ž ž  ž ž ž žž ž  ž ž ž žž  žž ž  ž ž ž  ž žž žž ž ž ž  ž  ž  ž ž  0  50  100  150  300  350  400  450  200  250 dissimilarity, Ži j  Figure 9.13 Distances v dissimilarities for the British town data  Table 9.3 Dissimilarities between 10 towns in the British Isles  measured as distances in miles along recommended routes   London Birmingham Cambridge Edinburgh Hull Lincoln Manchester Norwich Scarborough Southampton  0 111 55 372 171 133 184 112 214 77  0 101 290 123 85 81 161 163 128  0 330 124 86 155 62 167 130  0 225 254 213 360 194 418  0 0 39 96 84 144 106 43 81 223 185  0 185 105 208  0 187 190  0 266  0 Ldn B’ham Cmbg Edin Hull Lin M c Nwch Scar S’ton   Multidimensional scaling 349  distance. In ordinal scaling, the coordinates of the points in the representation space are adjusted so as to minimise a cost function that is a measure of the degree of deviation from monotonicity of the relationship between di j and Ži j . It may not be possible to obtain a ﬁnal solution that is perfectly monotonic but the ﬁnal ordering on the di j should be ‘as close as possible’ to that of the Ži j .  To ﬁnd a conﬁguration that satisﬁes the monotonicity requirement, we must ﬁrst of all specify a deﬁnition of monotonicity. Two possible deﬁnitions are the primary monotone condition  and the secondary monotone condition  Žrs < Ži j   Odrs  cid:17  Odi j  Žrs  cid:17  Ži j   Odrs  cid:17  Odi j  where Odrs is the point on the ﬁtting line  see Figure 9.14  corresponding to Žrs. The Odrs are termed the disparities or the pseudo-distances. The difference between these two conditions is the way in which ties between the Žs are treated. In the secondary monotone condition, if Žrs D Ži j then Odrs D Odi j , whereas in the primary condition there is no constraint on Odrs and Odi j if Žrs D Ži j : Odrs and Odi j are allowed to differ  which would give rise to vertical lines in Figure 9.14 . The secondary condition is usually regarded as too restrictive, often leading to convergence problems.  Given the above deﬁnition, we can deﬁne a goodness of ﬁt as  Sq D X  .di j  cid:5  Odi j  2  i < j  and minimising gives the primary  or secondary  least squares monotone regression line.  In fact, it is not a line, only being deﬁned at points Ži j .   An example of a least squares monotone regression is given in Figure 9.14. The least squares condition ensures that the sum of squares of vertical displacements from the line is a minimum. Practically, this means that for sections of the data where d is actually a monotonic function of Ž the line passes through the points. If there is a decrease, the value taken is the mean of a set of samples.  The quantity Sq is a measure of the deviation from monotonicity, but it is not invariant to uniform dilation of the geometric conﬁguration. This can be removed by normalisation  d  drs Odrs  ?      ?  ?   cid:31  cid:31   ?   cid:24    cid:31  ? ?   cid:24  ?  cid:24  ?  Figure 9.14 Least squares monotone regression line  Žrs  Ž   350 Feature selection and extraction  and the normalised stress, given by  P  vuut  S D  i < j  .di j  cid:5  Odi j  2 P i < j d2 i j   9.24   used as the measure of ﬁt.  In some texts the square root factor is omitted in the deﬁnition.   Since S is a differentiable function of the desired coordinates, we use a nonlinear optimisation scheme  see, for example, Press et al., 1992  which requires an initial conﬁguration of data points to be speciﬁed. In practice, it has been found that some of the more sophisticated methods do not work so well as steepest descents  Chatﬁeld and Collins, 1980 .  The initial conﬁguration of data points could be chosen randomly, or as the result of a principal coordinates analysis. Of course, there is no guarantee of ﬁnding the global minimum of S and the algorithm may get trapped in poor local minima. Several initial conﬁgurations may be considered before an acceptable value of the minimum stress is achieved.  In the algorithm, a value of the dimension of the representation space, e, is required. This is unknown in general and several values may be tried before a ‘low’ value of the stress is obtained. The minimum stress depends on n  the dimension of the dissimilarity matrix  and e and it is not possible to apply tests of signiﬁcance to see if the ‘true’ dimension has been found  this may not exist . As with principal components analysis, we may plot the stress as a function of dimension and see if there is a change in the slope  elbow in the graph . If we do this, we may ﬁnd that the stress does not decrease as the dimension increases, because of the problem of ﬁnding poor local minima. However, it should always decrease and can be made to do so by initialising the solution for dimension e by that obtained in dimension e  cid:5  1  extra coordinates of zero are added . The summations in the expression for the stress are over all pairwise distances. If the dissimilarity matrix is asymmetric we may include both the pairs .i; j   and . j; i   in the summation. Alternatively, we may carry out ordinal scaling on the symmetric matrix of dissimilarities with  ŽŁ rs D ŽŁ  sr D 1  2  .Žrs C Žsr    Missing values in Ži j can be accommodated by removing the corresponding indices from the summation  9.24  in estimating the stress.  9.4.4 Algorithms  Most MDS programs use standard gradient methods. There is some evidence that sophis- ticated nonlinear optimisation schemes do not work so well  Chatﬁeld and Collins, 1980 . Siedlecki et al.  1988  report that steepest descents outperformed conjugate gradients on a metric MDS optimisation problem, but better performance was given by the coordinate descent procedure of Niemann and Weiss  1979 .  One approach to minimising the objective function, S, is to use the principle of majorisation  de Leeuw and Heiser, 1977; Heiser, 1991, 1994  as part of the alternating   Multidimensional scaling 351  S  W . cid:22 t ;  cid:22    W . cid:22 tC1;  cid:22     Figure 9.15 Illustration of iterative majorisation principle: minimisation of S. cid:22    is achieved through successive minimisations of the majorisation functions, W   cid:22 tC2   cid:22 tC1   cid:22 t   cid:22   least squares process. Given the current values of the coordinates, say  cid:22 t , an upper bound, W . cid:22 t ;  cid:22   , for the criterion function is deﬁned. It is usually a quadratic form with a single minimum as a function of the coordinates  cid:22 . It has the property that W . cid:22 t ;  cid:22    is equal to the value of the objective function at  cid:22 t and greater than it everywhere else. Minimising W . cid:22 t ;  cid:22    with respect to  cid:22  yields a value  cid:22 tC1 at which the objective function is lower. A new majorising function W . cid:22 tC1;  cid:22    is deﬁned and the process repeated  see Figure 9.15 . This generates a sequence of estimates f cid:22 tg for which the objective function decreases and converges to a local minimum.  All algorithms start with an initial conﬁguration and converge to a local minimum. It has been reported that the secondary deﬁnition of monotonicity is more likely to get trapped in poor local minima than the primary deﬁnition  Gordon, 1999 . We recommend that you repeat your experiments for several starting conﬁgurations.  9.4.5 Multidimensional scaling for feature extraction  There are several obstacles in applying multidimensional scaling to the pattern recognition problem of feature extraction that we are addressing in this chapter. The ﬁrst is that usu- ally we are not presented with an nðn matrix of dissimilarities, but with an nð p matrix of observations. Although in itself this is not a problem since we can form a dissimilarity matrix using some suitable measure  e.g. Euclidean distance , the number of patterns n can be very large  in some cases thousands . The storage of an n ð n matrix may present a problem. Further, the number of adjustable parameters is n0 D e ð n, where e is the dimension of the derived coordinates. This may prohibit the use of some nonlinear opti- misation methods, particularly those of the quasi-Newton type which either calculate, or iteratively build up, an approximation to the inverse Hessian matrix of size n0ðn0 D e2n2. Even if these problems can be overcome, multidimensional scaling does not readily deﬁne a transformation that, given a new data sample x 2 R p, produces a result y 2 R e. Further calculation is required.   352 Feature selection and extraction  One approach to this problem is to regard the transformed coordinates y as a nonlinear  parametrised function of the data variables  for parameters  cid:22 . In this case,  y D f .x;  cid:22     di j D jf .xi ;  cid:22     cid:5  f .x j ;  cid:22   j Ži j D jxi  cid:5  x jj  and we may minimise the criterion function, for example  9.21 , with respect to the pa- rameters,  cid:22 , of f rather than with respect to the coordinates of the data points in the transformed space. This is termed multidimensional scaling by transformation. Thus the number of parameters can be substantially reduced. Once the iteration has converged, the function f can be used to calculate the coordinates in R e for any new data sample x. One approach is to model f as a radial basis function network and determine the net- work weights using iterative majorisation, a scheme that optimises the objective function without gradient calculation  Webb, 1995 .  A modiﬁcation to the distance term, Ž, is to augment it with a supervised quantity  giving a distance  .1  cid:5  Þ Ži j C Þsi j   9.25   where 0 < Þ < 1 and si j is a separation between objects using labels associated with the data. For example, si j may represent a class separability term: how separable are the classes to which patterns xi and x j belong? A difﬁculty is the speciﬁcation of the parameter Þ.  9.4.6 Example application study  The problem An exploratory data analysis to investigate relationships between research assessment ratings of UK higher education institutions  Lowe and Tipping, 1996 .  Summary Several methods of feature extraction, both linear and nonlinear, were ap- plied to high-dimensional data records from the 1992 UK Research Assessment Exercise and projections onto two dimensions produced.  The data Institutions supply information on research activities within different subjects in the form of quantitative indicators of their research activity, such as the number of active researchers, postgraduate students, values of grants and numbers of publications. Together with some qualitative data  for example, publications , this forms part of the data input to committees which provide a research rating, on a scale from 1 to 5. There are over 4000 records for all subjects, but the analysis concentrated on three subjects: physics, chemistry and biological sciences.  Preprocessing included the removal of redundant and repeated variables, accumulating indicators that were given for a number of years and standardisation of variables. The training set consisted of 217 patterns, each with 80 variables.   Multidimensional scaling 353  The model Several models were assessed. These included a principal components anal- ysis, a multidimensional scaling  Sammon mapping  and an MDS by transformation modelled as a radial basis function network.  Training procedure For the multidimensional scaling by transformation, the dissimi- larity was augmented with a subjective quantity as in  9.25  where si j is a separation between objects based on the subjective research rating.  Since the objective function is no longer quadratic, an analytic matrix inversion rou- tine cannot be used for the weights of the radial basis function. A conjugate gradients nonlinear function minimisation routine was used to minimise of the stress criterion.  9.4.7 Further developments  Within the pattern recognition literature, there have been several attempts to use multi- dimensional scaling techniques for feature extraction both for exploratory data analysis and classiﬁcation purposes  Sammon, 1969 – see comments by Kruskal, 1971; Koontz and Fukunaga, 1972; Cox and Ferry, 1993 .  Approaches that model the nonlinear dimension-reducing transformation as a radial basis function network are described by Webb  1995  and Lowe and Tipping  1996 . Mao and Jain  1995  model the transformation as a multilayer perceptron. A comparative study of neural network feature extraction methods has been done by Lerner et al.  1999 .  9.4.8 Summary  Multidimensional scaling is a name given to a range of techniques that analyse dissimi- larity matrices and produce coordinates of points in a ‘low’ dimension. Three approaches to multidimensional scaling have been presented:  Classical scaling This assumes that the dissimilarity matrix is Euclidean, though it has been shown to be robust if there are small departures from this condition. An eigen- vector decomposition of the dissimilarity matrix is performed and the resulting set of coordinates is identical to the principal components analysis scores  to within an or- thogonal transformation  if indeed the dissimilarity matrix is a matrix of Euclidean distances. Therefore there is nothing to be gained over a principal components anal- ysis in using this technique as a method of feature extraction, given an n ð p data matrix X.  Metric scaling This method regards the coordinates of the points in the derived space as parameters of a stress function that is minimised. This method allows nonlinear reductions in dimensionality. The procedure assumes a functional relationship between the interpoint distances and the given dissimilarities.  Non-metric scaling As with metric scaling, a criterion function  stress  is minimised but the procedure assumes that the data are qualitative, having perhaps ordinal signiﬁcance at best.   354 Feature selection and extraction  9.5 Application studies  to direction features for the 26 lower-case characters.  Examples of application studies using feature selection methods include: ž Remote sensing. Bruzzone et al.  1995  extend the pairwise Jeffreys–Matusita distance  otherwise known as the Patrick–Fischer distance – see Appendix A  to the multiclass situation and use it as a feature selection criterion in a remote sensing application. The data consist of measurements of six channels in the visible and infrared spectrum on ﬁve agricultural classes. ž Hand-printed character recognition. Zongker and Jain  1996  apply the SFFS algorithm ž Speech. Novovi˘cov´a et al.  1996  apply a feature selection method based on modelling the class densities by ﬁnite mixture distributions to a two-class speech recognition problem. The data comprised 15-dimensional feature vectors  containing ﬁve segments of three features derived by low-order linear prediction analysis . The approach was compared with a method using SFFS and a Gaussian classiﬁer. For this data set, there was no advantage in mixture modelling. ž Image analysis. Pudil et al.  1994c  use an approach to feature selection based on mixture modelling for the classiﬁcation of granite textures. The features comprise a 26-dimensional vector  eight texture features and 18 colour features . In this case, the data are modelled well by mixtures  in contrast to the speech example above .  Feature extraction application studies include: ž Electroencephalogram  EEG . Jobert et al.  1994  use principal components analysis to produce a two-dimensional representation of spectral data  sleep EEG  to view time-dependent variation. ž Positron emission tomography  PET . Pedersen et al.  1994  use principal components analysis for data visualisation purposes on dynamic PET images to enhance clinically interesting information. ž Remote sensing. Eklundh and Singh  1993  compare principal components analysis using correlation and covariance matrices in the analysis of satellite remote sensing data. The correlation matrix gives improvement to the signal-to-noise ratio.  ž Calibration of near-infrared spectra. ž Structure–activity relationships. Darwish et al.  1994  apply principal components anal- ysis  14 variables, nine compounds  in a study to investigate the inhibitory effect of benzine derivatives.  ž Target classiﬁcation. Liu et al.  1994  use principal components analysis for feature  extraction in a classiﬁcation study of materials design.  ž Face recognition. Principal components analysis has been used in several studies on face recognition to produce ‘eigenfaces’. The weights that characterise the expansion of a given facial image in terms of these eigenfaces are features used for face recognition and classiﬁcation  see the review by Chellappa et al., 1995 .  ž Speech. Pinkowski  1997  uses principal components analysis for feature extraction on a speaker-dependent data set consisting of spectrograms of 80 sounds representing 20 speaker-dependent words containing English semivowels.   Summary and discussion 355  Applications of MDS and Sammon mappings include: ž Medical. Ratcliffe et al.  1995  use multidimensional scaling to recover three- dimensional localisation of sonomicrometry transducer elements glued to excised and living ovine hearts. The inter-element distances were measured by the sonomicrometry elements by sequentially activating a single array element followed by eight receiver elements  thus giving inter-transducer distances . ž Bacterial classiﬁcation. Bonde  1976  uses non-metric multidimensional scaling to produce two and three-dimensional plots of groups of organisms  using a steepest descent optimisation scheme . ž Chemical vapour analysis. For a potential application of an ‘artiﬁcial nose’  an array of 14 chemical sensors  to atmosphere pollution monitoring, cosmetics, food and defence applications, Lowe  1993  considers a multidimensional scaling approach to feature extraction, where the dissimilarity matrix is determined by class  concentration of the substance .  9.6 Summary and discussion  In this chapter we have considered a variety of techniques for mapping data to a reduced dimension. As you may imagine, there are many more that we have not covered, and we have tried to point to the literature where some of these may be found. A comprehensive account of data transformation techniques requires a volume in itself and in this chapter we have only been able to consider some of the more popular multivariate methods. In common with the following chapter, many of the techniques are used as part of data preprocessing in an exploratory data analysis.  The techniques vary in their complexity – both from mathematical ease of understand- ing and numerical ease of implementation points of view. Most methods produce linear transformations, but non-metric multidimensional scaling is nonlinear. Some use class information, others are unsupervised, although there are both variants of the Karhunen– Lo`eve transformation. Some techniques, although producing a linear transformation, re- quire the use of a nonlinear optimisation procedure to ﬁnd the parameters. Others are based on eigenvector decomposition routines, perhaps performed iteratively.  To some extent, the separation of the classiﬁer design into two processes of feature extraction and classiﬁcation is artiﬁcial, but there are many reasons, some of which were enumerated at the beginning of this chapter, why dimension reduction may be advisable. Within the pattern recognition literature, many methods for nonlinear dimension reduction and exploratory data analysis have been proposed.  9.7 Recommendations  If explanation is required of the variables that are used in a classiﬁer, then a feature selection process, as opposed to a feature extraction process, is recommended for di- mension reduction. For feature selection, the probabilistic criteria for estimating class   356 Feature selection and extraction  separability are complicated, involving estimation of probability density functions and their numerical integration. Even the simple error rate measure is not easy to evaluate for nonparametric density functions. Therefore, we recommend use of the following.  1. The parametric form of the probabilistic distance measures assumes normal distribu- tions. These have the advantage for feature selection that the value of the criterion for a given set of features may be used in the evaluation of the criterion when an additional feature is included. This reduces the amount of computation in some of the feature set search algorithms.  2. The interclass distance measures, J1 to J4  Section 9.2.1 . 3. Error rate estimation using a speciﬁed classiﬁer. 4. Floating search methods.  Which algorithms should you employ for feature extraction? Whatever your problem, always start with the simplest approach, which for feature extraction is, in our view, a principal components analysis of your data. This will tell you whether your data lie on a linear subspace in the space spanned by the variables and a projection onto the ﬁrst two principal components, and displaying your data may reveal some interesting and unexpected structure.  It is recommended to apply principal components analysis to standardised data for feature extraction, and to consider it particularly when dimensionality is high. Use a simple heuristic to determine the number of principal components to use, in the ﬁrst instance. For class-labelled data, use linear discriminant analysis for a reduced-dimension representation.  If you believe there to be nonlinear structure in the data, then techniques based on multidimensional scaling  for example, multidimensional scaling by transformation  are straightforward to implement. Try several starting conditions for the parameters.  9.8 Notes and references  A good description of feature selection techniques for discrimination may be found in the book by Devijver and Kittler  1982 . The papers by Kittler  1975b, 1978b  and Siedlecki and Sklansky  1988  also provide reviews of feature selection and extraction methods. Chapter 6 of Hand’s  1981a  book on variable selection also discusses several of the methods described in this chapter.  The branch and bound method has been used in many areas of statistics  Hand, 1981b . It was originally proposed for feature subset selection by Narendra and Fuku- naga  1977  and receives a comprehensive treatment by Devijver and Kittler  1982  and Fukunaga  1990 . Hamamoto et al.  1990  evaluate the branch and bound algorithm using a recognition rate measure that does not satisfy the monotonicity condition. Krusi´nska  1988  describes a semioptimal branch and bound algorithm for feature selection in mixed variable discrimination.  Stepwise procedures have been considered by many authors: Whitney  1971  for the sequential forward selection algorithm; Michael and Lin  1973  for the basis of the l–r algorithm; Stearns  1976  for the l–r algorithm.   Exercises 357  Floating search methods were introduced by Pudil et al.  1994b; see also Pudil et al., 1994a, for their assessment with non-monotonic criterion functions and Kudo and Sklansky, 2000, for a comparative study . Error-rate-based procedures are described by McLachlan  1992a . Ganeshanandam and Krzanowski  1989  also use error rate as the selection criterion. Within the context of regression, the book by Miller  1990  gives very good accounts of variable selection.  Many of the standard feature extraction techniques may be found in most textbooks on multivariate analysis. Descriptions  with minimal mathematics  may be found in Reyment et al.  1984, Chapter 3  and Clifford and Stephenson  1975, Chapter 13 .  Thorough treatments of principal components analysis are given in the books by Jolliffe  1986  and Jackson  1991 , the latter providing a practical approach and giving many worked examples and illustrations. Common principal components and related methods are described in the book by Flury  1988 .  Descriptions of factor analysis appear in many textbooks on multivariate analysis  for example, Dillon and Goldstein, 1984; Kendall, 1975 . Jackson  1991  draws out the similarities to and the differences from principal components analysis. There are several specialist books on factor analysis, including those by Harman  1976 , and Lawley and Maxwell  1971 .  Multidimensional scaling is described in textbooks on multivariate analysis  for example, Chatﬁeld and Collins, 1980; Dillon and Goldstein, 1984  and more detailed treatments are given in the books by Schiffman et al.  1981  and Jackson  1991 . Cox and Cox  1994  provide an advanced treatment, with details of some of the specialised procedures. An extensive treatment of non-metric MDS can be found in the collection of articles edited by Lingoes et al.  1979 . There are many computer programs available for performing scaling. The features of some of these are given by Dillon and Goldstein  1984  and Jackson  1991 .  Many of the techniques described in this chapter are available in standard statisti- cal software packages. There is some specialised software for multidimensional scaling publicly available.  The website www.statistical-pattern-recognition.net contains refer-  ences and pointers to websites for further information on techniques.  Exercises  Numerical routines for matrix operations, including eigendecomposition, can be found in many numerical packages. Press et al.  1992  give descriptions of algorithms.  1. Consider the divergence  see Appendix A ,  Z  JD D  . p.xj!1   cid:5  p.xj!2   log   cid:15    cid:14  p.xj!1  p.xj!2   dx  where x D .x1; : : : ; x p T . Show that under conditions of independence, JD may be expressed as  JD D pX jD1  J j .x j     358 Feature selection and extraction  2.  Chatﬁeld and Collins, 1980.  Suppose the p-variate random variable x has covari- ance matrix  cid:4  with eigenvalues f½ig and orthonormal eigenvectors faig. Show that the identity matrix is given by  and that  I D a1aT  1 C ÐÐÐ C a paT  p   cid:4  D ½1a1aT  1 C ÐÐÐ C ½ pa paT  p  The latter result is called the spectral decomposition of  cid:4 .  3. Given a set of n measurements on p variables, describe the stages in performing a  principal components analysis for dimension reduction.  4. Let X1 and X2 be two random variables with covariance matrix   cid:4  D   cid:6   p  9 6  ½  p 6 4  Obtain the principal components. What is the percentage of total variance explained by each component?  5. Athletics records for 55 countries comprise measurements made on eight running events. These are each country’s record times for  1  100 m  s ;  2  200 m  s ;  3  400 m  s ; 800 m  min ;  5  1500 m  min ;  6  5000 m  min ;  7  10 000 m  min ;  8  marathon  min .  Describe how a principal components analysis may be used to obtain a two-dimen- sional representation of the data.  The results of a principal components analysis are shown in the table  Everitt and Dunn, 1991 . Interpret the ﬁrst two principal components.  PC1 ð ½1  100 m 200 m 400 m 800 m 1500 m 5000 m 10 000 m Marathon  Eigenvalue  0.82 0.86 0.92 0.87 0.94 0.93 0.94 0.87  6.41  PC2 ð ½2 0.50 0.41 0.21 0.15  cid:5 0:16  cid:5 0:30  cid:5 0:31  cid:5 0:42  0.89  What is the percentage of the total variance explained by the ﬁrst principal compo- nent? State any assumptions you make.   Exercises 359  6.  Chatﬁeld and Collins, 1980.  Four measurements are made on each of a random sample of 500 animals. The ﬁrst three variables were different linear dimensions, measured in centimetres, while the fourth variable was the weight of the animal measured in grams. The sample covariance matrix was calculated and its four eigen- values were found to be 14.1, 4.3, 1.2 and 0.4. The eigenvectors corresponding to the ﬁrst and second eigenvalues were:  1 D [0:39; 0:42; 0:44; 0:69] uT 2 D [0:40; 0:39; 0:42; cid:5 0:72] uT  Comment on the use of the sample covariance matrix for the principal components analysis for these data. What is the percentage of variance in the original data ac- counted for by the ﬁrst two principal components? Describe the results. Suppose the data were stored by recording the eigenvalues and eigenvectors together with the 500 values of the ﬁrst and second principal components and the mean values for the original variables. Show how to reconstruct the original covariance matrix and an approximation to the original data.  7. Given that  and assuming that QS S given by the above and assuming that S cid:5 1 is known,  is known, verify that S cid:5 1 is given by  9.2 . Conversely, with   cid:5 1  show that the inverse of QS  the inverse of S after the removal of a feature  can be written as  8. For a given symmetric matrix S of known inverse  of the above form  and symmetric matrix T , verify  9.3 , where QT is the submatrix of T after the removal of a feature.  cid:5 1 W S B  , where SW and S B Hence, show that the feature extraction criterion Tr.S are the within- and between-class covariance matrices, satisﬁes the monotonicity property.  9. How could you use ﬂoating search methods for radial basis function centre selection? What are the possible advantages and disadvantages of such methods compared with random selection or k-means, for example?  10. Suppose we take classiﬁcation rate using a nearest class mean classiﬁer as our feature  selection criterion. Show by considering the two distributions,  p.xj!1  D  p.xj!2  D  ² 1 0  cid:17  x1  cid:17  1; 0  cid:17  x2  cid:17  1 ² 1 1  cid:17  x1  cid:17  2; cid:5 0:5  cid:17  x2  cid:17  0:5  0 otherwise  0 otherwise  S D   cid:6  QS yT  ½  y skk   cid:5 1 D  S  ½   cid:6  A c cT b  QS   cid:5 1 D A  cid:5  1 b  cT c   360 Feature selection and extraction  where x D .x1; x2 T , that classiﬁcation rate does not satisfy the monotonicity property.  11. Derive the relationship  9.20  expressing the elements of the sum of squares and products matrix in terms of the elements of the dissimilarity matrix from  9.19  and the deﬁnition of Ti j and zero-mean data.  12. Given n p-dimensional measurements  in the n ð p data matrix X, zero mean, p < n  show that a low-dimensional representation in r < p dimensions obtained by constructing the sums of squares and products matrix, T D XXT , and performing a principal coordinates analysis, results in the same projection as principal components  to within an orthogonal transformation .  13. For two classes normally distributed, N .µ1;  cid:4   and N .µ2;  cid:4   with common covari-  ance matrix,  cid:4 , show that the divergence  Z  JD.!1; !2  D  [ p.xj!1   cid:5  p.xj!2 ] log  ¦  ² p.xj!1  p.xj!2   dx  is given by  .µ2  cid:5  µ1 T  cid:4    cid:5 1.µ2  cid:5  µ1   the Mahalanobis distance  14. Describe how multidimensional scaling solutions that optimise stress can always be constructed that result in a decrease of stress with dimension of the representation space.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   10  Clustering  Overview  Clustering methods are used for data exploration and to provide prototypes for use in supervised classiﬁers. Methods that operate both on dissimilarity matrices and measurements on individuals are described, each implicitly imposing its own structure on the data. Mixtures explicitly model the data structure.  10.1 Introduction  Cluster analysis is the grouping of individuals in a population in order to discover struc- ture in the data. In some sense, we would like the individuals within a group to be close or similar to one another, but dissimilar from individuals in other groups.  Clustering is fundamentally a collection of methods of data exploration. One often uses a method to see if natural groupings are present in the data. If groupings do emerge, these may be named and their properties summarised. For example, if the clusters are compact, then it may be sufﬁcient for some purposes to reduce the information on the original data set to information about a small number of groups, in some cases representing a group of individuals by a single sample. The results of a cluster analysis may produce identiﬁable structure that can be used to generate hypotheses  to be tested on a separate data set  to account for the observed data.  It is difﬁcult to give a universal deﬁnition of the term ‘cluster’. All of the methods described in this chapter can produce a partition of the data set – a division of the data set into mutually non-overlapping groups. However, different methods will often yield different groupings since each implicitly imposes a structure on the data. Also, the techniques will produce groupings even when there is no ‘natural’ grouping in the data. The term ‘dissection’ is used when the data consist of a single homogeneous population that one wishes to partition. Clustering techniques may be used to obtain dissections, but the user must be aware that a structure is being imposed on the data that may not be present. This does not matter in some applications.  Before attempting a classiﬁcation, it is important to understand the problem you are wishing to address. Different classiﬁcations, with consequently different interpretations,   362 Clustering  can be imposed on a sample and the choice of variables is very important. For example, there are different ways in which books may be grouped on your bookshelf – by subject matter or by size – and different classiﬁcations will result from the use of different variables. Each classiﬁcation may be important in different circumstances, depending on the problem under consideration. Once you understand your problem and data, you must choose your method carefully. An inappropriate match of method to data can give results that are misleading.  Related to clustering is clumping, which allows an object to belong to more than one group. An example often cited is the classiﬁcation of words according to their meaning: some words have several meanings and can belong to several groups. However, in this chapter we concentrate on clustering methods.  There is a vast literature on clustering. Some of the more useful texts are given at the end of this chapter. There is a wide range of application areas, sometimes with conﬂicting terminology. This has led to methods being rediscovered in different ﬁelds of study. Much of the early work was in the ﬁelds of biology and zoology, but clustering methods have also been applied in the ﬁelds of psychology, archaeology, linguistics and signal processing. This literature is not without its critics. The paper by Cormack  1971  is worth reading. His remark that ‘the real need of the user is to be forced to sit and think’ is perhaps even more relevant today. The user often does not want to do so, and is often satisﬁed with a ‘black box’ approach to the analysis. However, the ﬁrst thing to do when wishing to apply a technique is to understand the signiﬁcance of the data and to understand what a particular technique does.  Five topics are discussed in this chapter:  1. hierarchical methods, which derive a clustering from a given dissimilarity matrix;  2. quick partitions, methods for obtaining a partition as an initialisation to more elaborate  approaches;  densities;  3. mixture models, which express the probability density function as a sum of component  4. sum-of-squares methods, which minimise a sum-of-squares error criterion, including  k-means, fuzzy k-means, vector quantisation and stochastic vector quantisation;  5. cluster validity, addressing the problem of model selection.  10.2 Hierarchical methods  Hierarchical clustering procedures are the most commonly used method of summarising data structure. A hierarchical tree is a nested set of partitions represented by a tree diagram or dendrogram  see Figure 10.1 . Sectioning a tree at a particular level produces a partition into g disjoint groups. If two groups are chosen from different partitions  the results of partitioning at different levels  then either the groups are disjoint or one group wholly contains the other. An example of a hierarchical classiﬁcation is the classiﬁcation of the animal kingdom. Each species belongs to a series of nested clusters of increasing size with a decreasing number of common characteristics. In producing a tree diagram   Hierarchical methods 363  threshold distance   cid:1   6.0  4.0  2.0  0.0  1  2  5  3  4 Figure 10.1 Dendrogram  6  like that in Figure 10.1, it is necessary to order the points so that branches do not cross. This ordering is somewhat arbitrary, but does not alter the structure of the tree, only its appearance. There is a numerical value associated with each position up the tree where branches join. This is a measure of the distance or dissimilarity between two merged clusters. There are many different measures of distances between clusters  some of these are given in Appendix A  and these give rise to different hierarchical structures, as we shall see in later sections of this chapter. Sectioning a tree partitions the data into a number of clusters of comparable homogeneity  as measured by the clustering criterion .  There are several different algorithms for ﬁnding a hierarchical tree. An agglomerative algorithm begins with n subclusters, each containing a single data point, and at each stage merges the two most similar groups to form a new cluster, thus reducing the number of clusters by one. The algorithm proceeds until all the data fall within a single cluster. A divisive algorithm operates by successively splitting groups, beginning with a single group and continuing until there are n groups, each of a single individual. Generally, divisive algorithms are computationally inefﬁcient  except where most of the variables are binary attribute variables .  From the tree diagram a new set of distances between individuals may be deﬁned, with the distance between individual i and individual j being the distance between the two groups that contain them, when these two groups are amalgamated  i.e. the distance level of the lowest link joining them . Thus, the procedure for ﬁnding a tree diagram may be viewed as a transformation of the original set of dissimilarities di j to a new set Odi j , where the Odi j satisfy the ultrametric inequality  Odi j  cid:2  max. Odik ; Od jk    for all objects i; j; k  This means that the distances between three groups can be used to deﬁne a triangle that is either equilateral or isosceles  either the three distances are the same or two are equal and the third smaller – see Figure 10.1, for example . A transformation D : d ! Od is termed an ultrametric transformation. All of the methods in this section produce a clustering from a given dissimilarity matrix.  It is appropriate to introduce here the concept of a minimum spanning tree. A minimum spanning tree is not a hierarchical tree, but a tree spanning a set of points such that every   364 Clustering   cid:1   6  4  2  0  1   cid:3  *  cid:2    cid:3    cid:3    cid:2  * 2  3 *   cid:3  cid:4    cid:4    cid:4    cid:4    cid:5    cid:5    cid:5    cid:5    cid:5  *  4   cid:5    cid:5    cid:5    cid:4    cid:5    cid:4  cid:5   5 *  * 6  0  4  2 Figure 10.2 Minimum spanning tree  8  6  10   cid:6   12  two vertices are connected and there are no closed loops. Associated with each link in the tree is a value or distance, and the minimum spanning tree is the tree such that the sum of all the distances is a minimum. Figure 10.2 shows the minimum spanning tree for the two-dimensional data used to generate the dendrogram of Figure 10.1, where the distance between individuals is taken to be the Euclidean distance. The minimum spanning tree has been used as the basis of a single-link algorithm. Single-link clusters at level h are obtained by deleting from the minimum spanning tree all edges greater than h in length. The minimum spanning tree should also be useful in the identiﬁcation of clusters, outliers and inﬂuential points  points whose removal can alter the derived clustering appreciably .  10.2.1 Single-link method  The single-link method is one of the oldest methods of cluster analysis. It is deﬁned as follows. Two objects a and b belong to the same single-link cluster at level d if there exists a chain of intermediate objects i1; : : : ; im cid:4 1 linking them such that all the distances  dik ;ikC1  cid:2  d  for k D 0; : : : ; m  cid:4  1  where i0 D a and im D b. The single-link groups for the data of Figure 10.1 for a threshold of d D 2:0, 3.0 and 5.0 are f.1; 2 ; .5; 6 ; .3 ; .4 g, f.1; 2; 3 ; .5; 6 ; .4 g and f.1; 2; 3; 5; 6 ; .4 g.  We shall illustrate the method by example with an agglomerative algorithm in which, at each stage of the algorithm, the closest two groups are fused to form a new group, where the distance between two groups, A and B, is the distance between their closest members, i.e.  dAB D min i2A; j2B  di j   10.1    Consider the dissimilarity matrix for each pair of objects in a set comprising six individuals:  Hierarchical methods 365  3  1 2  4 1 0 4 13 24 0 10 22 2 7 3 0 4 5 6  0  6 5 8 12 10 11 9 3 6 18 0 8:5 0  1 2 .3; 5  12 10 0  1 0 4 0 2 .3; 5  4 6  6 4 8 24 22 10 6 8:5 0 18 0  .1; 2  0  .3; 5   4 10 22 0  6 8 6 8:5 18 0 0  .1; 2  .3; 5  4 6  The closest two groups  which contain a single object each at this stage  are those containing the individuals 3 and 5. These are fused to form a new group f3; 5g and the distances between this new group and the remaining groups calculated according to  10.1  so that d1;.3;5  D minfd13; d15g D 12, d2;.3;5  D minfd23; d25g D 10, d4;.3;5  D 6, d6;.3;5  D 8:5, giving the new dissimilarity matrix  The closest two groups now are those containing objects 1 and 2; therefore these are fused to form a new group .1; 2 . We now have four clusters .1; 2 ; .3; 5 ; 4 and 6. The distance between the new group and the other three clusters is calculated: d.1;2 .3;5  D minfd13; d23; d15; d25g D 10, d.1;2 4 D minfd14; d24g D 22 d.1;2 6 D minfd16; d26g D 8. The new dissimilarity matrix is  The closest two groups are now those containing 4 and .3; 5 . These are fused to form .3; 4; 5  and a new dissimilarity matrix calculated. This is given below with the result of fusing the next two groups. The single-link dendrogram is given in Figure 10.3.  .1; 2  .3; 4; 5  6  .1; 2  0  6 .3; 4; 5  10 8 0 8:5 0  .1; 2; 6  .3; 4; 5   .1; 2; 6  0  .3; 4; 5  8:5 0   366 Clustering  threshold distance   cid:1   8.0  6.0  4.0  2.0  0.0  1  2  6  4  5  3  Figure 10.3 Single-link dendrogram  *  * *  *  *  * *  *  *  *  *  *  *  *  4  2  0   cid:1   *  0  *  *  *  *  *  *  * * *  *  *  *  *  *  * *  ** *  *  * **  *  *   cid:6   2  4  6  8  10  12  14  Figure 10.4 Illustration of chaining with the single-link method  The above agglomerative algorithm for a single-link method illustrates the fact that it takes only a single link to join two distinct groups and that the distance between two groups is the distance of their closest neighbours. Hence the alternative name of nearest-neighbour method. A consequence of this joining together by a single link is that some groups can become elongated, with some distant points, having little in common, being grouped together because there is a chain of intermediate objects. This draw- back of chaining is illustrated in Figures 10.4 and 10.5. Figure 10.4 shows a distri- bution of data samples. Figure 10.5 shows the single-link three-group solution for the data in Figure 10.4. These groups do not correspond to those suggested by the data in Figure 10.4.  There are many algorithms for ﬁnding a single-link tree. Some are agglomerative, like the procedure described above, some are divisive; some are based on an ultrametric transformation and others generate the single-link tree via the minimum spanning tree  see Rohlf, 1982 for a review of algorithms . The algorithms vary in their computational efﬁciency, storage requirements and ease of implementation. Sibson’s  1973  algorithm uses the property that only local changes in the reduced dissimilarity result when two clusters are merged, and it has been extended to the complete-link method discussed in the following section. It has computational requirements O.n2 , for n objects. More time-efﬁcient algorithms are possible if knowledge of the metric properties of the space   Hierarchical methods 367  2 2 2  2  2  2 2  2  2  2  2  2  2  2  4  2  0   cid:1   1  0  2  2  2  2 2  2  2  2  2  2  2  2  2  2  2 2  22 2  2  2 22  2  3   cid:6   2  4  6  8  10  12  14  Figure 10.5 Single-link three-group solution for the data of Figure 10.4  in which the data lie is taken into account. In such circumstances, it is not necessary to compute all dissimilarity coefﬁcients. Also, preprocessing the data to facilitate searches for nearest neighbours can reduce computational complexity.  10.2.2 Complete-link method  In the complete-link or furthest-neighbour method the distance between two groups A and B is the distance between the two furthest points, one taken from each group:  In the example used to illustrate the single-link method, the second stage dissimilarity matrix  after merging the closest groups 3 and 5 using the complete-link rule above  becomes  dAB D max i2A; j2B  di j  1 2 .3; 5   1 0 4 2 0 .3; 5  4 6  6 4 13 24 8 11 22 10 7 9 0 0 18 0  The ﬁnal complete-link dendrogram is shown in Figure 10.6. At each stage, the closest groups are merged of course. The difference between this method and the single-link method is the measure of distance between groups. The groups found by sectioning the complete-link dendrogram at level h have the property that di j < h for all members in the group. The method concentrates on the internal cohesion of groups, in contrast to the single-link method, which seeks isolated groups. Sectioning a single-link dendrogram at a level h gives groups with the property that they are separated from each other by at least a ‘distance’ h.  Defays  1977  provides an algorithm for the complete-link method using the same representation as Sibson. It should be noted that the algorithm is sensitive to the ordering of the data, and consequently has several solutions. Thus it provides only an approximate complete-link clustering.   368 Clustering  threshold distance   cid:1   20.0  15.0  10.0  5.0  0.0  1  2  6  4  3  5  Figure 10.6 Complete-link dendrogram  10.2.3 Sum-of-squares method  The sum-of-squares method is appropriate for the clustering of points in Euclidean space. The aim is to minimise the total within-group sum of squares. Ward’s hierarchical clus- tering method  Ward, 1963  uses an agglomerative algorithm to produce a set of hi- erarchically nested partitions that can be represented by a dendrogram. However, the optimal sum-of-squares partitions for different numbers of groups are not necessarily hierarchically nested. Thus the algorithm is suboptimal.  At each stage of the algorithm, the two groups that produce the smallest increase in the total within-group sum of squares are amalgamated. The dissimilarity between two groups is deﬁned to be the increase in the total sum of squares that would result if they were amalgamated. The updating formula for the dissimilarity matrix is  diC j;k D nk C ni nk C ni C n j  dik C nk C n j nk C ni C n j  d jk  cid:4   nk  nk C ni C n j  di j  where diC j;k is the distance between the amalgamated groups i C j and the group k and ni is the number of objects in group i. Initially, each group contains a single object and the element of the dissimilarity matrix, di j , is the squared Euclidean distance between the ith and the jth object.  10.2.4 General agglomerative algorithm  Many agglomerative algorithms for producing hierarchical trees can be expressed as a special case of a single algorithm. The algorithms differ in the way that the dissimilarity matrix is updated. The Lance–Williams recurrence formula expresses the dissimilarity between a cluster k and the cluster formed by joining i and j as diC j;k D ai dik C a j d jk C bdi j C cjdik  cid:4  d jkj   Hierarchical methods 369  Table 10.1 Special cases of the general agglomerative algorithm  Single link Complete link Centroid  Median  Group average link  Ward’s method  ai  1 2 1 2 ni niCn j  1 2 ni niCn j niCnk niCn jCnk  b  0 0  cid:4  ni n j .niCn j  2  cid:4  1 4 0  cid:4  nk  niCn jCnk  c  cid:4  1  2 1 2 0  0  0  0  where ai ; b and c are parameters that, if chosen appropriately, will give an agglomerative algorithm for implementing some of the more commonly used methods  see Table 10.1 .  Centroid distance This deﬁnes the distance between two clusters to be the distance between the cluster means or centroids.  Median distance When a small cluster is joined to a larger one, the centroid of the result will be close to the centroid of the larger cluster. For some problems this may be a disadvantage. This measure attempts to overcome this by deﬁning the distance between two clusters to be the distance between the medians of the clusters.  Group average link In the group average method, the distance between two clusters is deﬁned to be the average of the dissimilarities between all pairs of individuals, one from each group:  dAB D 1 ni n j  X i2A; j2B  di j  10.2.5 Properties of a hierarchical classiﬁcation  What desirable properties should a hierarchical clustering method possess? It is difﬁcult to write down a set of properties on which everyone will agree. What might be a set of common-sense properties to one person may be the extreme position of another. Jardine and Sibson  1971  suggest a set of six mathematical conditions that an ultrametric transformation should satisfy; for example, that the results of the method should not depend on the labelling of the data. They show that the single-link method is the only one to satisfy all their conditions and recommend this method of clustering. However, this method has its drawbacks  as do all methods , which has led people to question the plausibility of the set of conditions proposed by Jardine and Sibson. We shall not list the conditions here but refer to Jardine and Sibson  1971  and Williams et al.  1971  for further discussion.   370 Clustering  10.2.6 Example application study  The problem Weather classiﬁcation for the study of long-term trends in climate change research  Huth et al., 1993 .  The data Daily weather data in winter months  December–February  at Prague- Clementinum were recorded. The data came from two 14-year periods, 1951–1964 and 1965–1978, each consisting of 1263 days. Daily weather was characterised by eight variables – daily mean temperature, temperature amplitude  daily maximum minus mini- mum , relative humidity, wind speed, zonal and meridional wind components, cloudiness and temperature tendency  the difference between the mean temperature of the day and its predecessor .  These were transformed to ﬁve variables through a principal components analysis of the correlation matrix. The ﬁve variables accounted for 83% of the total variance in the data.  The model The average-link method was used. At the beginning, each day is an indi- vidual cluster. The clusters merge according to the average-link algorithm  Section 10.2.4  until, at the ﬁnal stage, a single cluster, containing all days, is formed. In the authors’ opinion, too many clusters, and one large cluster, are undesirable. Also, it was expected that the number of clusters should correspond roughly with the number of synoptic types of weather for the region  about 30 types .  Training procedure The data were clustered to ﬁnd groups containing days with the weather as ‘uniform as possible’. The average-link procedure terminated when the dif- ference between the properties of merging clusters exceeded a predeﬁned criterion.  Results Removing clusters of sizes less than ﬁve days resulted in 28 clusters for 1965–1978 and 29 clusters for 1951–1964  similar to the number of synoptic types for the region .  10.2.7 Summary  The concept of having a hierarchy of nested clusters was developed primarily in the biological ﬁeld and may be inappropriate to model the structure in some data. Each hierarchical method imposes its own structure on the data. The single-link method seeks isolated clusters, but is generally not favoured, even though it is the only one satisfying the conditions proposed by Jardine and Sibson. It is subject to the chaining effect, which can result in long straggly groups. This may be useful in some circumstances if the clusters you seek are not homogeneous, but it can mean that distinct groups are not resolved because of intermediate points present between the groups. The group average, complete link and Ward’s method tend to concentrate on internal cohesion, producing homogeneous, compact  often spherical  groups.   Quick partitions 371  The centroid and median methods may lead to inversions  a reduction in the dissim- ilarity between an object and a cluster when the cluster increases in size  which make the dendrogram difﬁcult to interpret. Also, ties in the dissimilarities may lead to multiple solutions  non-uniqueness  of which the user should be aware  Morgan and Ray, 1995 . Hierarchical agglomerative methods are one of the most common clustering tech- niques employed. Divisive algorithms are less popular, but efﬁcient algorithms have been proposed, based on recursive partitioning of the cluster with largest diameter  Gu´enoche et al., 1991 .  10.3 Quick partitions  Many of the techniques described subsequently in this chapter are implemented by algo- rithms that require an initial partition of the data. The normal mixture methods require initial estimates of means and covariance matrices. These could be sample-based esti- mates derived from an initial partition. The k-means algorithm also requires an initial set of means. Hierarchical vector quantisation  see Section 10.5.3  requires initial estimates of the code vectors, and similarly the topographic mappings require initialisation of the weight vectors. In the context of discrimination, radial basis functions, introduced in Chapter 5, require initial estimates for ‘centres’. These could be derived using the quick partition methods of this section or be the result of a more principled clustering approach  which in turn may need to be initialised .  Let us suppose that we have a set of n data samples and we wish to ﬁnd an initial partition into k groups, or to ﬁnd k seed vectors. We can always ﬁnd a seed vector, given a group of objects, by taking the group mean. Also, we can partition a set, given k vectors, using a nearest-neighbour assignment rule. There are many heuristic partition methods. We shall consider a few of them.  1. Random k selection We wish to have k different vectors, so we select one randomly from the whole data set, another from the remaining n cid:4  1 samples in the data set, and so on. In a supervised classiﬁcation problem, these vectors should ideally be spread across all classes.  2. Variable division Choose a single variable. This may be selected from one of the measured variables or be a linear combination of variables; for example, the ﬁrst principal component. Divide it into k equal intervals that span the range of the variable. The data are partitioned according to which bin they fall in and k seed vectors are found from the means of each group.  3. Leader algorithm The leader cluster algorithm  Hartigan, 1975; Sp¨ath, 1980  par- titions a data set such that for each group there is a leader object and all other objects within the group are within a distance T of the leading example. Figure 10.7 illustrates a partition in two dimensions. The ﬁrst data point, A, is taken as the centre of the ﬁrst group. Successive data points are examined. If they fall inside the circle centred at A of radius T then they are assigned to group 1. The ﬁrst data sample examined to fall outside   372 Clustering  ž  B  ž A   cid:7  T ž  ž  ž  ž  ž  ž  ž  C  ž  ž  Figure 10.7 Leader clustering  the circle, say at B, is taken as the leader of the second group. Further data points are examined to see if they fall within the ﬁrst two clusters. The ﬁrst one to fall outside, say at C, is taken as the centre of the third cluster and so on.  Points to note about the algorithm:  1. All cluster centres are at least a distance T from each other.  2. It is fast, requiring only one pass through the data set.  3. It can be applied to a given dissimilarity matrix.  4. It is dependent on the ordering of the data set. The ﬁrst point is always a cluster  leader. Also, initial clusters tend to be larger than later ones.  5. The distance T is speciﬁed, not the number of clusters.  10.4 Mixture models  10.4.1 Model description  In the mixture method of clustering, each different group in the population is assumed to be described by a different probability distribution. These different probability distribu- tions may belong to the same family but differ in the values they take for the parameters of the distribution. Alternatively, mixtures may comprise sums of different component densities  for modelling different effects such as a signal and noise . The population is described by a ﬁnite mixture distribution of the form  p.x  D gX iD1 where the ³i are the mixing proportions  Pg ³i D 1  and p.x; θ i   is a p-dimensional iD1 probability function depending on a parameter vector θ i . There are three sets of param- eters to estimate: the values of ³i , the components of the vectors θ i and the value of g, the number of groups in the population.  ³i p.x; θ i     Mixture models 373  Many forms of mixture distributions have been considered and there are many meth- ods for estimating their parameters. An example of a mixture distribution for continuous variables is the mixture of normal distributions  p.x  D gX iD1  ³i p.x;  cid:3 i ; µi    where µi and  cid:3 i are the means and covariance matrices of a multivariate normal distribution  p.x;  cid:3 i ; µi   D  1 2 j cid:3 ij 1 p  2  exp  .2³    ²   cid:4  1 2  .x  cid:4  µi  T  cid:3    cid:4 1 i  .x  cid:4  µi    ¦  and a mixture for binary variables is  where  p.x  D gX iD1  ³i p.x; θ i    p.x; θ j   D pY lD1  .1  cid:4   cid:7  jl  1 cid:4 xl   cid:7  xl jl  is the multivariate Bernoulli density. The value of  cid:7  jl is the probability that variable l in the jth group is unity.  Maximum likelihood procedures for estimating the parameters of normal mixture distributions were given in Chapter 2. Other examples of continuous and discrete mixture distributions, and methods of parameter estimation, can be found in Everitt and Hand  1981  and Titterington et al.  1985 . Also, in some applications, variables are often of a mixed type – both continuous and discrete.  The usual approach to clustering using ﬁnite mixture distributions is ﬁrst of all to specify the form of the component distributions, p.x; θ i  . Then the number of clusters, g, is prescribed. The parameters of the model are now estimated and the objects are grouped on the basis of their estimated posterior probabilities of group membership; that is, the object x is assigned to group i if ³i p.x; θ i   ½ ³ j p.x; θ j    for all j 6D i; j D 1; : : : ; g  Clustering using a normal mixture model may be achieved by using the EM algorithm  described in Chapter 2, to which we refer for further details.  The main difﬁculty with the method of mixtures concerns the number of components, g  see Chapter 2 . This is the question of model selection we return to many times in this book. Many algorithms require g to be speciﬁed before the remaining parameters can be estimated. Several test statistics have been put forward. Many apply to special cases such as assessing the question as to whether or not the data come from a single component distribution or a two-component mixture. However, others have been proposed based on likelihood ratio tests  Everitt and Hand, 1981, Chapter 5; Titterington et al., 1985, Chapter 5 .  Another problem with a mixture model approach is that there may be many local minima of the likelihood function and several initial conﬁgurations may have to be tried   374 Clustering  before a satisfactory clustering is produced. In any case, it is worthwhile trying several initialisations, since agreement between the resulting classiﬁcations lends more weight to the chosen solution. Celeux and Govaert  1992  describe approaches for developing the basic EM algorithm to overcome the problem of local optima.  There are several forms of the normal mixture model that trade off the number of components against the complexity of each component. For example, we may require that, instead of arbitrary covariance matrices, the covariance matrices are proportional to one another or have common principal components. This reduces the number of parameters per mixture component  Celeux and Govaert, 1995 .  10.4.2 Example application study  The problem Flaw detection in textile  denim  fabric before ﬁnal assembly of the gar- ment  Campbell et al., 1997 .  Summary The approach detects a linear pattern in preprocessed images via model- based clustering. It employs an approximate Bayes factor which provides a criterion for assessing the evidence for the presence of a defect.  The data Two-dimensional point pattern data are generated by thresholding and clean- ing, using mathematical morphology, images of fabric. Two fabric images are used, each about 500 ð 500 pixels in size.  The model A two-component mixture model was used to model the data gathered from images of fabric: a Poisson noise component to model the background and a  possibly highly elliptical  Gaussian cluster to model the anomaly.  Training procedure The model parameters were determined by maximising the likeli- hood using the EM algorithm  see Chapter 2 . Taking A to be the area of the data region, the Bayesian information criterion  BIC  was calculated:  BIC D 2 log.L  C 2n log.A   cid:4  6 log.n   where n is the number of samples and L is the value of the likelihood at its maximum. A value of BIC greater than 6 indicates ‘strong evidence’ for a defect.  Results were presented for some representative examples, and contrasted with a Hough  transform.  10.5 Sum-of-squares methods  Sum-of-squares methods ﬁnd a partition of the data that maximises a predeﬁned clustering criterion based on the within-class and between-class scatter matrices. The methods differ in the choice of clustering criterion optimised and the optimisation procedure adopted. However, the problem all methods seek to solve is given a set of n data samples, to partition the data into g clusters so that the clustering criterion is optimised.  Most methods are suboptimal. Computational requirements prohibit optimal schemes, even for moderate values of n. Therefore we require methods that, although producing a   Sum-of-squares methods 375  suboptimal partition, give a value of the clustering criterion that is not much greater than the optimal one. First of all, let us consider the various criteria that have been proposed.  10.5.1 Clustering criteria Let the n data samples be x1; : : : ; xn. The sample covariance matrix, O cid:3 , is given by  O cid:3  D 1 n  nX iD1  .xi  cid:4  m .xi  cid:4  m T  Pn  where m D 1 matrix or pooled within-group scatter matrix is  n  iD1 xi , the sample mean. Let there be g clusters. The within-class scatter  SW D 1 n  gX jD1  nX iD1  z ji .xi  cid:4  m j  .xi  cid:4  m j  T ;  the sum of the sums of squares and cross-products  scatter  matrices over the g groups, where z ji D 1 if xi 2 group j, 0 otherwise, m j D 1 iD1 z ji xi is the mean of cluster j and n j D Pn  iD1 z ji , the number in cluster j. The between-class scatter matrix is  Pn  n j  S B D O cid:3   cid:4  SW D gX jD1  n j n  .m j  cid:4  m .m j  cid:4  m T  and describes the scatter of the cluster means about the total mean.  The most popular optimisation criteria are based on univariate functions of the above matrices and are similar to the criteria given in Chapter 9 on feature selection and extrac- tion. The two areas of clustering and feature selection are very much related. In clustering we are seeking clusters that are internally cohesive but isolated from other clusters. We do not know the number of clusters. In feature selection or extraction, we have labelled data from a known number of groups or classes and we seek a transformation that makes the classes distinct. Therefore one that transforms the data into isolated clusters will achieve this.  1. Minimisation of Tr.SW  The trace of SW is the sum of the diagonal elements  z jijxi  cid:4  m jj2  Tr.SW   D 1 n  nX iD1  S j  gX jD1 gX jD1  D 1 n  where S j D Pn iD1 z jijxi  cid:4  m jj2, the within-group sum of squares for group j. Thus, the minimisation of Tr.SW   is equivalent to minimising the total within-group sum of   376 Clustering   cid:1   ž   cid:1  ž  ž ž  ž  cid:6  ž   cid:6   ž  ž  Figure 10.8 Example of effects of scaling in clustering  squares about the g centroids. Clustering methods that minimise this quantity are some- times referred to as sum-of-squares or minimum-variance methods. They tend to produce clusters that are hyperellipsoidal in shape. The criterion is not invariant to the scale of the axes and usually some form of standardisation of the data must be performed prior to application of the method. Alternatively, criteria that are not invariant to linear transformations of the data may be employed. 2. Minimisation of jSWj=j O cid:3 j This criterion is invariant to nonsingular linear transformations of the data. For a given data set, it is equivalent to ﬁnding the partition of the data that minimises jSWj  the matrix O cid:3  is independent of the partition .  WSB   3. Maximisation of Tr.S-1 This is a generalisation of the sum-of-squares method in that the clusters are no longer hyperspherical, but hyperellipsoidal. It is equivalent to minimising the sum of squares under the Mahalanobis metric. It is also invariant to nonsingular transformations of the data.  4. Minimisation of Tr. O cid:3  This is identical to minimising the sum of squares for data that have been normalised to make the total scatter matrix equal to the identity.  -1SW   Note that the two examples in Figure 10.8 would be clustered differently by the sum-of-squares method  criterion 1 above . However, since they only differ from each other by a linear transformation, they must both be local optima of a criterion invariant to linear transformations. Thus, it is not necessarily an advantage to use a method that is invariant to linear transformations of the data since structure may be lost. The ﬁnal solution will depend very much on the initial assignment of points to clusters.  10.5.2 Clustering algorithms  The problem we are addressing is one in combinatorial optimisation. We seek a non-trivial partition of n objects into g groups for which the chosen criterion is optimised. However,   to ﬁnd the optimum partition requires the examination of every possible partition. The number of non-trivial partitions of n objects into g groups is  Sum-of-squares methods 377  1 g!  gX iD1  . cid:4 1 g cid:4 i   cid:6 g   cid:7   i  i n  with the ﬁnal term in the summation being most signiﬁcant if n × g. This increases rapidly with the number of objects. For example, there are 259  cid:4  1 ³ 6 ð 1017 partitions of 60 objects into two groups. This makes exhaustive enumeration of all possible sub- sets infeasible. In fact, even the branch and bound procedure described in Chapter 9 is impractical for moderate values of n. Therefore, suboptimal solutions must be derived. We now describe some of the more popular approaches. Many of the procedures require initial partitions of the data, from which group means may be calculated, or initial estimates of group means  from which an initial partition may be deduced using a nearest class mean rule . These were discussed in Section 10.3.  k-means The aim of the k-means  which also goes by the names of the c-means or iterative relocation or basic ISODATA  algorithm is to partition the data into k clusters so that the within-group sum of squares  criterion 1 of Section 10.5.1  is minimised. The simplest form of the k-means algorithm is based on alternating two procedures. The ﬁrst is one of assignment of objects to groups. An object is usually assigned to the group to whose mean it is closest in the Euclidean sense. The second procedure is the calculation of new group means based on the assignments. The process terminates when no movement of an object to another group will reduce the within-group sum of squares. Let us illustrate with a very simple example. Consider the two-dimensional data shown in Figure 10.9. Let us set k D 2 and choose two vectors from the data set as initial cluster mean vectors. Those selected are points 5 and 6. We now cycle through the data set and allocate individuals to groups A and B represented by the initial vectors 5 and 6 respectively. Individuals 1, 2, 3, 4 and 5 are allocated to A and individual 6 to B. New means are calculated and the within-group sum of squares is evaluated, giving 6.4. The results of this iteration are summarised in Table 10.2. The process is now repeated, using the new mean vectors as the reference vectors. This time, individuals 1, 2, 3, and 4 are allocated to group A and  Š  5  Š  6  Š  Š  4  3  Š  1  Š  2  1  0   cid:4 1   cid:4 1  0  1  2  3  4  5  Figure 10.9 Data to illustrate the k-means procedure   378 Clustering  Table 10.2 Summary of k-means iterations  Step  Group A  Group B  Tr.W    Membership  Mean  Membership  Mean  1. 2. 3.  1, 2, 3, 4, 5 1, 2, 3, 4 1, 2, 3, 4   1.6, 0.4   1.25, 0.25   1.25, 0.25   6 5, 6 5, 6   4.0, 1.0   3.5, 1.0   3.5, 1.0   6.4 4.0 4.0  ž  Š  4  Š  Š  3  2  1  0   cid:4 1   cid:4 1  Š  1  0  ž  1  Figure 10.10 Data to illustrate the k-means local optimum  2  3  4  5  5 and 6 to group B. The within-group sum of squares has now decreased to 4.0. A third iteration produces no change in the within-group sum of squares.  The iterative procedure of allocating objects to groups on a nearest group mean basis, followed by recalculation of group means, gives the version of the k-means called HMEANS by Sp¨ath  1980 . It is also termed Forgy’s method or the basic ISODATA method.  There are two main problems with HMEANS. It may lead to empty groups and it may lead to a partition for which the sum-squared error could be reduced by moving an individual from one group to another. Thus the partition of the data by HMEANS is not necessarily one for which the within-group sum of squares is a minimum  see Selim and Ismail, 1984a, for a treatment of the convergence of this algorithm . For example, in Figure 10.10 four data points and two groups are illustrated. The means are at positions  1.0, 0.0  and  3.0, 1.0 , with a sum-squared error of 4.0. Repeated iterations of the algorithm HMEANS will not alter that allocation. However, if we allocate object 2 to the group containing objects 3 and 4, the means are now at  0.0, 0.0  and  8 3, 2 3 , and the sum-squared error is reduced to 10 3. This suggests an iterative procedure that cycles through the data points and allocates each to a group for which the within-group sum of squares is reduced the most. Allocation takes place on a sample-by-sample basis, rather than after a pass through the entire data set. An individual xi  in group l  is assigned to group r if  nl nl  cid:4  1  d2 il  >  nr nr C 1  d2 ir   Sum-of-squares methods 379  is the distance to the lth centroid and nl  where dil is the number in group l. The greatest decrease in the sum-squared error is achieved by choosing the group for which =.nr C 1  is a minimum. This is the basis of the k-means algorithm. nr d2 ir There are many variants of the k-means algorithm to improve efﬁciency of the algo- rithm in terms of computing time and of achieving smaller error. Some algorithms allow new clusters to be created and existing ones deleted during the iterations. Others may move an object to another cluster on the basis of the best improvement in the objective function. Alternatively, the ﬁrst encountered improvement during the pass through the data set could be used.  Nonlinear optimisation The within-groups sum-of-squares criterion may be written in the form  Tr.SW   D 1 n  nX iD1  gX kD1  zki  pX jD1  .xi j  cid:4  mk j  2  where xi j is the jth coordinate of the ith point  i D 1; : : : ; n; j D 1; : : : ; p , mk j is the jth coordinate of the mean of the kth group and zki D 1 if the ith point belongs to the kth group and 0 otherwise. The mean quantities mk j may be written as   10.2    10.3   mk j D  Pn iD1 zki xi j Pn iD1 zki  for zki as deﬁned above. To obtain an optimal partition, we must ﬁnd the values of zki  either 0 or 1  for which  10.2  is a minimum. The approach of Gordon and Henderson  1977  is to regard the g ð n matrix Z with .i; j  th element zi j as consisting of real-valued quantities  as opposed to binary quantities  with the property  gX kD1  zki D 1 and  zki ½ 0  .i D 1; : : : ; n; k D 1; : : : ; g    10.4   Minimisation of  10.2  with respect to zki .i D 1; : : : ; n; k D 1; : : : ; g , subject to the constraints above, yields a ﬁnal solution for Z with elements that are all 0 or 1. Therefore we can obtain a partition by minimising  10.2  subject to the constraints  10.4  and assigning objects to groups on the basis of the values zik. Thus, mk j is not equal to a group mean until the iteration has converged.  The problem can be transformed to one of unconstrained optimisation by writing  z ji as  z ji D  Pg  exp.¹ ji   kD1 exp.¹ki    . j D 1; : : : ; g; i D 1; : : : ; n   that is, we regard TrfSWg as a nonlinear function of parameters ¹ki ; i D 1; : : : ; n; k D 1; : : : ; g, and seek a minimum of TrfS W .¹ g. Other forms of transformation to unconstrained optimisation are possible. However, for the particular form given above,   380 Clustering  the gradient of TrfSW .¹ g with respect to ¹ab has the simple form  @TrfSW .¹ g  @¹ab  D 1 n  gX kD1  zkb.Žka  cid:4  zab jxb  cid:4  mkj2   10.5   where Žka D 0; k 6D a, and 1 otherwise. There are many nonlinear optimisation schemes that can be used. The parameters ¹i j must be given initial values. Gordon and Henderson  1977  suggest choosing an initial set of random values z ji uniformly distributed in the range [1; 1 C a] and scaled so that their sum is unity. A value of about 2 is suggested for the parameter a. Then ¹ ji is given by ¹ ji D log.z ji  .  Fuzzy k-means The partitioning methods described so far in this chapter have the property that each object belongs to one group only, though the mixture model can be regarded as providing degrees of cluster membership. Indeed, the early work on fuzzy clustering was closely related to multivariate mixture models. The basic idea of the fuzzy clustering method is that patterns are allowed to belong to all clusters with different degrees of membership. The ﬁrst generalisation of the k-means algorithm was presented by Dunn  1974 . The fuzzy k-means  or fuzzy c-means  algorithm attempts to ﬁnd a solution for parameters y ji  i D 1; : : : ; n; j D 1; : : : ; g  for which gX jD1  Jr D nX iD1  jijxi  cid:4  m jj2 yr   10.6   is minimised subject to the constraints  gX jD1  y ji D 1 1  cid:2  i  cid:2  n y ji ½ 0 i D 1; : : : ; n; j D 1; : : : ; g  The parameter y ji represents the degree of association or membership function of the ith pattern or object with the jth group. In  10.6 , r is a scalar termed the weighting exponent which controls the ‘fuzziness’ of the resulting clusters  r ½ 1  and m j is the ‘centroid’ of the jth group  m j D  Pn iD1 yr ji xi Pn iD1 yr ji   10.7   A value of r D 1 gives the same problem as the nonlinear optimisation scheme presented earlier. In that case, we know that a minimum of  10.6  gives values for the y ji that are either 0 or 1.  The basic algorithm is iterative and can be stated as follows  Bezdek, 1981 .  1. Select r  1 < r < 1 ; initialise the membership function values y ji ; i D 1; : : : ; n;  j D 1; : : : ; g.  2. Compute the cluster centres m j ; j D 1; : : : ; g, according to  10.7 .   3. Compute the distances di j , i D 1; : : : ; n; j D 1; : : : ; g, where di j D jxi  cid:4  m jj. 4. Compute the membership function: if dil D 0 for some l, yli D 1, and y ji D 0, for  all j 6D l; otherwise  Sum-of-squares methods 381  y ji D  1  cid:8  di j  dik  Pg  kD1   cid:9  2 r cid:4 1  5. If not converged, go to step 2. As r ! 1, this algorithm tends to the basic k-means algorithm. Improvements to this basic algorithm, resulting in faster convergence, are described by Kamel and Selim  1994 .  Several stopping rules have been proposed  Ismail, 1988 . One is to terminate the algorithm when the relative change in the centroid values becomes small; that is, termi- nate when  4D  Dz    gX jD1  jm j .k   cid:4  m j .k  cid:4  1 j2  < ž    1  2  where m j .k  is the value of the jth centroid on the kth iteration and ž is a user-speciﬁed threshold. Alternative stopping rules are based on changes in the membership function values, y ji , or the cost function, Jr . Another condition based on the local optimality of the cost function is given by Selim and Ismail  1986 . It is proposed to stop when  max 1 cid:2 i cid:2 n  Þi < ž  where  Þi D max 1 cid:2  j cid:2 g  yr cid:4 1  ji  jxi  cid:4  m jj2  cid:4  min 1 cid:2  j cid:2 g  yr cid:4 1  ji  jxi  cid:4  m jj2  since at a local minimum, Þi D 0; i D 1; : : : ; n.  Complete search Complete search of the space of partitions of n objects into g groups is impractical for all but very small data sets. The branch and bound method  described in a feature subset selection context in Chapter 9  is one approach for ﬁnding the partition that results in the minimum value of the clustering criterion, without exhaustive enumeration. Nevertheless, it may still be impractical. Koontz et al.  1975  have developed an approach that extends the range of problems to which branch and bound can be applied. The criterion they seek to minimise is TrfSWg. Their approach is to divide the data set into 2m independent sets. The branch and bound method is applied to each set separately and then sets are combined in pairs  to give 2m cid:4 1 sets  and the branch and bound method applied to each of these combined sets, using the results obtained from the branch and bound application to the constituent parts. This is continued until the branch and bound procedure is applied to the entire set. This hierarchical approach results in a considerable saving in computer time.  Other approaches based on global optimisation algorithms such as simulated annealing have also been proposed. Simulated annealing is a stochastic relaxation technique in   382 Clustering  which a randomly selected perturbation to the current conﬁguration is accepted or rejected probabilistically. Selim and Al-Sultan  1991  apply the method to the minimisation of TrfSWg. Generally the method is slow, but it can lead to effective solutions.  10.5.3 Vector quantisation  Vector quantisation  VQ  is not a method of producing clusters or partitions of a data set but rather an application of many of the clustering algorithms already presented. Indeed, many clustering techniques have been rediscovered in the vector quantisation literature. On the other hand, there are some important algorithms in the VQ literature that are not found in the standard texts on clustering. This section is included in the section on optimisation methods since in VQ a distortion measure  often, but by no means exclusively, based on the Euclidean distance  is optimised during training. A comprehensive and very readable account of the fundamentals of VQ is given by Gersho and Gray  1992 .  VQ is the encoding of a p-dimensional vector x as one from a codebook of g vectors, z1; : : : ; zg, termed the code vectors or the codewords. The purpose of VQ is primarily to perform data compression. A vector quantiser consists of two components: an encoder and a decoder  see Figure 10.11 .  The encoder maps an input vector, x, to a scalar variable, y, taking discrete values 1; : : : ; g. After transmission of the index, y, the inverse operation of reproducing an approximation x0 to the original vector takes place. This is termed decoding and is a mapping from the index set I D f1; : : : ; gg to the codebook C D fz1; : : : ; zgg. Codebook design is the problem of determining the codebook entries given a set of training samples. From a clustering point of view, we may regard the problem of codebook design as one of clustering the data and then choosing a representative vector for each cluster. These vectors could be cluster means, for example, and they form the entries in the codebook. They are indexed by integer values. Then the code vector for a given input vector x is the representative vector, say z, of the cluster to which x belongs. Membership of a cluster may be determined on a nearest-to-cluster-mean basis. The distortion or error in approximation is then d.x; z , the distance between x and z  see Figure 10.12 .  The problem in VQ is to ﬁnd a set of codebook vectors that characterise a data set. This is achieved by choosing the set of vectors for which a distortion measure between an input vector, x, and its quantised vector, x0, is minimised. Many distortion measures have been proposed, the most common being based on the squared error measure giving average distortion, D2,  Z  Z  D2 D  p.x  d.x; x  0  dx  D  p.x jjx  0.y.x    cid:4  xjj2 dx   10.8   original vector x  encoder  y x   code, y  decoder x′ y   reconstruction  x′  Figure 10.11 The encoding–decoding operation in vector quantisation   Sum-of-squares methods 383  Figure 10.12 VQ distortion for two code vectors. The reconstruction of x after encoding and decoding is z 2, the nearest code vector. The distortion is d.x ; z 2   Ł Ł Ł Ł ž Ł  cid:8  cid:8  cid:9  z2 Ł Ł Ł  Ł  x  Ł  Ł Ł Ł  4  2  0   cid:1   Ł  0  Ł Ł Ł Ł ž z1 Ł  Ł  Ł  Ł  Ł Ł  Ł  Ł  4  2  6  8   cid:6   Table 10.3 Some distortion measures used in vector quantisation  Type of norm  L2, Euclidean  L ¹  Minkowski  Quadratic  for positive deﬁnite symmetric B   d.x; x0  jx0  cid:4  xj ýP p  ¹  iD1 jx0  cid:4  xj¹ cid:14  1 jx0 i  cid:4  xij max 1 cid:2 i cid:2  p .x0  cid:4  x T B.x0  cid:4  x   where p.x  is the probability density function over samples x used to train the vector quantiser and jj:jj denotes the norm of a vector. Other distortion measures are given in Table 10.3.  For a ﬁnite number of training samples, x1; : : : ; xn, we may write the distortion as  where S j is the set of training vectors for which y.x  D j, i.e. those that map onto the jth code vector, z j . For a given set of code vectors z j , the partition that minimises the average distortion is constructed by mapping each xi to the z j for which d.xi ; z j   is a minimum over all z j – i.e. choosing the minimum distortion or nearest-neighbour code vector. Alternatively, for a given partition the code vector of a set S j , z j , is deﬁned to be the vector for which  D D gX jD1  X xi2S j  d.xi ; z j    X xi2S j  d.u; xi    is a minimum with respect to u. This vector is called the centroid  for the squared error measure it is the mean of the vectors xi  .   384 Clustering  This suggests an iterative algorithm for a vector quantiser:  1. Initialise the code vectors.  2. Given a set of code vectors, determine the minimum distortion partition.  3. Find the optimal set of code vectors for a given partition.  4. If the algorithm has not converged, then go to step 2.  This is clearly a variant of the k-means algorithms given earlier. It is identical to the basic k-means algorithm provided that the distortion measure used is the squared error distortion since all the training vectors are considered at each iteration rather than making an adjustment of code vectors by considering each in turn. This is known as the generalised Lloyd algorithm in the VQ literature  Gersho and Gray, 1992  or the LBG algorithm in the data compression literature. One of the main differences between the LBG algorithm and some of the k-means implementations is the method of initialisation of the centroid vectors. The LBG algorithm  Linde et al., 1980  given below starts with a one-level quantiser  a single cluster  and, after obtaining a solution for the code vector z, ‘splits’ the vector z into two close vectors that are used as seed vectors for a two-level quantiser. This is run until convergence and a solution is obtained for the two-level quantiser. Then these two codewords are split to give four seed vectors for a four-level quantiser. The process is repeated so that ﬁnally quantisers of 1; 2; 4; : : : ; N levels are obtained  see Figure 10.13 .  1. Initialise a code vector z1 to be the group mean; initialise  cid:12 . 2. Given a set of m code vectors, ‘split’ each vector zi to form 2m vectors, zi C  cid:12  and  zi  cid:4   cid:12 . Set m D 2m; relabel the code vectors as x0  ; i D 1; : : : ; m.  i  3. Given the set of code vectors, determine the minimum distortion partition.  4. Find the optimal set of code vectors for a given partition.  4  2  0   cid:1   *  0  *  *  *  ž z C  cid:12  *  cid:10  cid:10  cid:11  ž z  cid:10  cid:10  cid:12  ž z  cid:4   cid:12   *  *  *  z2  *  ž *  *  *  *  *  *  *  *  ž *  *  * z1 *  *  *  *  *  *   cid:6   2  4  6  8  Figure 10.13 LBG algorithm illustration: z denotes the group centroid; z 1 and z 2 denote the code vectors for a two-level quantiser   Sum-of-squares methods 385  5. Repeat steps 3 and 4 until convergence. 6. If m 6D N , the desired number of levels, go to step 2.  Although it appears that all we have achieved with the introduction of VQ in this chapter is yet another version of the k-means algorithm, the VQ framework allows us to introduce two important concepts: that of tree-structured codebook search that reduces the search complexity in VQ; and that of topographic mappings in which a topology is imposed on the code vectors.  Tree-structured vector quantisation Tree-structured vector quantisation is a way of structuring the codebook in order to reduce the amount of computation required in the encoding operation. It is a special case of the classiﬁcation trees or decision trees discussed in a discrimination context in Chapter 7. Here we shall consider ﬁxed-rate coding, in which there are the same number of bits used to represent each code vector. Variable-rate coding, which allows pruning of the tree, will not be addressed. Pruning methods for classiﬁcation trees are described in Chapter 7, and in the VQ context by Gersho and Gray  1992 .  We shall begin our description of tree-structured VQ with a simple binary tree example. The ﬁrst stage in the design procedure is to run the k-means algorithm on the entire data set to partition the set into two parts. This leads to two code vectors  the means of each cluster   see Figure 10.14 . Each group is considered in turn and the k-means algorithm applied to each group, partitioning each group into two parts again. This second stage then produces four code vectors and four associated clusters. The mth stage produces 2m code vectors. The total number of code vectors produced in an m-stage design algorithm is Pm iD1 2i D 2mC1  cid:4  2. This process produces a hierarchical clustering in which two clusters are disjoint or one wholly contains the other.  Encoding of a given vector, x, proceeds by starting at the root of the tree  labelled A0 in Figure 10.15  and comparing x with each of the two level 1 code vectors, identifying the nearest. We then proceed along the branch to A1 and compare the vector x with the two code vectors at this level which were generated from members of the training in this group. Thus there are m comparisons in an m-stage encoder. This compares with 2m  0  1  0  * x  0,0  0,1  1,0  1,1  1  − cluster centres after one stage  − cluster centres after two stages  Figure 10.14 Tree-structured vector quantisation   386 Clustering  A0  0  x  1  A1  0,0  0,1  1,0  1,1  Figure 10.15 Tree-structured vector quantisation tree  code vectors at the ﬁnal level. Tree-structured VQ may not be optimal in the sense that the nearest neighbour of the ﬁnal level code vectors is not necessarily found  the ﬁnal partition in Figure 10.14 does not consist of nearest-neighbour regions . However, the code has the property that it is a progressively closer approximation as it is generated and the method can lead to a considerable saving in encoding time.  Self-organising feature maps  Self-organising feature maps are a special kind of vector quantisation in which there is an ordering or topology imposed on the code vectors. The aim of self-organisation is to represent high-dimensional data as a low-dimensional array of numbers  usually a one- or two-dimensional array  that captures the structure in the original data. Distinct clusters of data points in the data space will map to distinct clusters of code vectors in the array, although the converse is not necessarily true: separated clusters in the array do not necessarily imply separated clusters of data points in the original data. In some ways, self-organising feature maps may be regarded as a method of exploratory data analysis in keeping with those described in Chapter 9. The basic algorithm has the k-means algorithm as a special case.  Figures 10.16 and 10.17 illustrate the results of the algorithm applied to data in two  dimensions. ž In Figure 10.16, 50 data samples are distributed in three groups in two dimensions and we have used a self-organisation process to obtain a set of nine ordered cluster centres in one dimension. By a set of ordered cluster centres we mean that centre zi is close in some sense to zi cid:4 1 and ziC1. In the k-means algorithm, the order that the centres are stored in the computer is quite arbitrary and depends on the initialisation of the procedure.  ž In Figure 10.17, the data  not shown  comprise 500 samples drawn from a uniform distribution over a square  [ cid:4 1  cid:2  x; y  cid:2  1]  and do not lie on  or close to  a one-dimensional manifold in the two-dimensional space. Again, we have imposed a one-dimensional topology on the cluster centres, which are joined by straight lines. In this case, we have obtained a space-ﬁlling curve.   Sum-of-squares methods 387  160  140  120  100  80  60  40  20  0  C C Š C CCCCCCCCC Š C C Š C  C C C CCCC  Š  C  CCC C CCCC C Š  C CC C Š Š  CCCC C C Š C C  CCCCC Š  C  0  20  40  60  80  100  120  140  160  Figure 10.16 Topographic mapping. Adjacent cluster centres  Š  in the stored array of code vectors are joined  1  y  0  −1  −1  0 x  1  Figure 10.17 Topographic mapping for data uniformly distributed over a square. Thirty-three centres are determined and again adjacent cluster centres in the stored array are joined  In each of the above illustrations, a transformation to a reduced dimension is achieved using a topographic mapping in which there is an ordering on the cluster centres. Each point in the data space is mapped to the ordered index of its nearest cluster centre. The mapping is nonlinear, and for purposes of illustration we considered mappings to one dimension only. If the data do lie on a reduced-dimension manifold within a high- dimensional space, then it is possible for topographic mappings to capture the structure in the data and present it in a form that may aid interpretation. In a supervised classiﬁcation problem, it is possible to label each cluster centre with a class label according to the   388 Clustering  majority of the objects for which that cluster centre is the nearest. Of course we can do this even if there were no ordering on the cluster centres, but the ordering does allow the relationships between classes  according to decision boundaries  to be viewed easily. The algorithm for determining the cluster centres may take many forms. The basic approach is to cycle through the data set and adjust the cluster centres in the neigh- bourhood  suitably deﬁned  of each data point. The algorithm is often presented as a function of time, where time refers to the number of presentations of a data sample. One algorithm is as follows.  1. Decide on the topology of the cluster centres  code vectors . Initialise Þ, the neigh-  bourhood and the cluster centres z1; : : : ; zN .  2. Repeat until convergence:   a  Select a data sample x  one of the training samples  and ﬁnd the closest centre:  let d jŁ D min j .d j  , where  d j D jx  cid:4  z jj  j D 1; : : : ; N   b  Update the code vectors in the neighbourhood, N jŁ of code vector z jŁ for all centres z 2 N jŁ  z.t C 1  D z.t   C Þ.t  .x.t    cid:4  z.t     where Þ is a learning rate that decreases with iteration number, t  0  cid:2  Þ  cid:2  1 .   c  Decrease the neighbourhood and the learning parameter, Þ.  In order to apply the algorithm an initial set of weight vectors, the learning rate Þ.t   and the change with t of the neighbourhoods must be chosen.  Deﬁnition of topology The choice of topology of the cluster centres requires some prior knowledge of the data structure. For example, if you suspect circular topology in your data, then the topology of your cluster centres should reﬂect this. Alternatively, if you wish to map your data onto a two-dimensional surface, then a regular lattice structure for the code vectors may be sufﬁcient.  Learning rate The learning rate, Þ, is a slowly decreasing function of t. It is suggested by Kohonen  1989  that it could be a linear function of t, stopping when Þ reaches 0, but there are no hard and fast rules for choosing Þ.t  . It could be linear, inversely proportional to t or exponential. Haykin  1994  describes two phases: the ordering phase, of about 1000 iterations, when Þ starts close to unity and decreases, but remains above 0.1; and the convergence phase, when Þ decreases further and is maintained at a small value – 0.01 or less – for typically thousands of iterations.  Initialisation of code vectors Code vectors zi are initialised to m C  cid:12 i , where m is the sample mean and  cid:12 i is a vector of small random values.   Sum-of-squares methods 389  Decreasing the neighbourhood The topological neighbourhood N j of a code vector z j is itself a function of t and decreases as the number of iterations proceeds. Initially the neighbourhood may cover most of the code vectors  z j cid:4 r ; : : : ; z j cid:4 1; z jC1; : : : ; z jCr for large r , but towards the end of the iterations it covers the nearest  topological  neighbours z j cid:4 1 and z jC1 only. Finally it shrinks to zero. The problem is how to initialise the neighbourhood and how to reduce it as a function of t. During the ordering phase, the neighbourhood is decreased to cover only a few neighbours.  An alternative approach proposed by Luttrell  1989  is to ﬁx the neighbourhood size and to start off with a few code vectors. The algorithm is run until convergence, and then the number of vectors is increased by adding vectors intermediate to those already calculated. The process is repeated and continued until a mapping of the desired size has been grown. Although the neighbourhood size is ﬁxed, it starts off by covering a large area  since there are few centres  and the physical extent is reduced as the mapping grows. Speciﬁcally, given a data sample x, if the nearest neighbour is zŁ, then all code vectors z in the neighbourhood of zŁ are updated according to  z ! z C ³.z; z  Ł .x  cid:4  z    10.9   where ³ .> 0  is a function that depends on the position of z in the neighbourhood of zŁ. For example, with a one-dimensional topology, we may take  ³.z; z  Ł  D  ² 0:1 0:01 for z a topographic neighbour of zŁ  for z D zŁ  The Luttrell algorithm for a one-dimensional topographic mapping is as follows.  1. Initialise two code vectors, z1 and z2; set m D 2. Deﬁne the neighbourhood function,  ³; set the number of updates per code vector, u.  2. Repeat until the distortion is small enough or the maximum number of code vectors  is reached:  a  For j D 1 to m ð u do  ž Sample from the data set x1; : : : ; xn, say x. ž Determine the nearest-neighbour code vector, say zŁ. ž Update the code vectors according to: z ! z C ³.z; z  Ł .x  cid:4  z .   b  Deﬁne 2m  cid:4  1 new code vectors: for j D m  cid:4  1 down to 1 do  ž z2 jC1 D z jC1 ž z2 j D z j C z jC1  c  Set m D 2m  cid:4  1.  2  Topographic mappings have received widespread use as a means of exploratory data analysis  for example, Kraaijveld et al., 1992; Roberts and Tarassenko, 1992  They have also been misused and applied when the ordering of the resulting cluster centres is irrelevant in any subsequent data analysis and a simple k-means approach could have   390 Clustering  been adopted. An assessment of the method and its relationship to other methods of multivariate analysis is provided by Murtagh and Hern´andez-Pajares  1995 . Luttrell  1989  has derived an approximation to the basic learning algorithm from a VQ approach assuming a minimum distortion  Euclidean  and a robustness to noise on the codes. This puts the approach on a ﬁrmer mathematical footing. Also, the requirement for ordered cluster centres is demonstrated for a hierarchical vector quantiser.  Learning vector quantisation Vector quantisation or clustering  in the sense of partitioning a data set, not seeking meaningful groupings of objects  is often performed as a preprocessor for supervised classiﬁcation. There are several ways in which vector quantisers or self-organising maps have been used with labelled training data. In the radar target classiﬁcation example of Luttrell  1995 , each class is modelled separately using a self-organising map. This was chosen to feed in prior knowledge that the underlying manifold was a circle. Test data are classiﬁed by comparing each pattern with the prototype patterns in each of the self-organising maps  codebook entries  and classifying on a nearest-neighbour rule basis. An alternative approach that uses vector quantisers in a supervised way is to model the whole of the training data with a single vector quantiser  rather than each class separately . Each training pattern is assigned to the nearest code vector, which is then labelled with the class of the majority of the patterns assigned to it. A test pattern is then classiﬁed using a nearest-neighbour rule using the labelled codebook entries.  Learning vector quantisation is a supervised generalisation of vector quantisation that takes account of class labels in the training process. The basic algorithm is given below.  1. Initialise cluster centres  or code vectors , z1; : : : ; zN , and labels of cluster centres,  !1; : : : ; !N .  2. Select a sample x from the training data set with associated class !x and ﬁnd the  closest centre: let d jŁ D min j .d j  , where d j D jx  cid:4  z jj  j D 1; : : : ; N  with corresponding centre z jŁ and class ! jŁ.  3. If !x D ! jŁ then update the nearest vector, z jŁ, according to z jŁ .t C 1  D z jŁ .t   C Þ.t  .x.t    cid:4  z jŁ .t     where 0 < Þt < 1 and decreases with t, starting at about 0.1. 4. If !x 6D ! jŁ then update the nearest vector, z jŁ, according to z jŁ .t C 1  D z jŁ .t    cid:4  Þ.t  .x.t    cid:4  z jŁ .t     5. Go to 1 and repeat until several passes have been made through the data set.  Correct classiﬁcation of a pattern in the data set leads to a reﬁnement of the code- word in the direction of the pattern. Incorrect classiﬁcation leads to a movement of the codeword away from the training pattern.   Sum-of-squares methods 391  Stochastic vector quantisation In the approach to vector quantisation described in the previous section, a codebook is used to encode each input vector x as a code index y which is then decoded to produce an approximation, x0.y , to the original input vector. Determining the codebook vectors that characterise a data set is achieved by optimising an objective function, the most common being based on the squared error measure  equation  10.8    Z  D2 D  p.x jjx  0.y.x    cid:4  xjj2 dx  Optimisation is achieved through an iterative process using a k-means algorithm or variants  e.g. the LBG algorithm . Encoding is the deterministic process of ﬁnding the nearest entry in the codebook.  Stochastic vector quantisation  SVQ, Luttrell, 1997, 1999a  is a generalisation of the standard approach in which an input vector x is encoded as a vector of code indices y  rather than as a single code index  that are stochastically sampled from a probability distribution p.yjx  that depends on the input vector x. The decoding operation that produces a reconstruction, x0, is also probabilistic, with x0 being a sample drawn from p.xjy  given by  p.xjy  D p.yjx  p.x  R p.yjz  p.z  dz   10.10   One of the key factors motivating the development of the SVQ approach is that of scalability to high dimensions. A problem with standard VQ is that the codebook grows exponentially in size as the dimensionality of the input vector is increased, assuming that the contribution to the reconstruction error from each dimension is held constant. This means that such vector quantisers are not appropriate for encoding extremely high- dimensional input vectors, such as images. An advantage of using the stochastic ap- proach is that it automates the process of splitting high-dimensional input vectors into low-dimensional blocks before encoding them, because minimising the mean Euclidean reconstruction error can encourage different stochastically sampled code indices to be- come associated with different input subspaces.  SVQ provides a unifying framework for many of the methods of the previous subsec- tions, with standard VQ  k-means , fuzzy k-means and topographic mappings emerging as special cases. We denote the number of groups  i.e. the number of codebook entries  by g and let r be the number of samples in the code index vector, y  i.e. y D .y1; : : : ; yr   . The objective function is taken to be the mean Euclidean reconstruction error measure deﬁned by D D  0jy jx  cid:4  x  p.yjx   0.y j2  0 p.x   10.11   ÐÐÐ  dx  Z  Z  gX y2D1  gX yrD1  dx p.x   gX y1D1 Integrating over x0 yields Z  D D 2  dx p.x   gX y1D1  gX y2D1  ÐÐÐ  gX yrD1  p.yjx jx  cid:4  x  0.y j2   10.12    392 Clustering  where the reconstruction vector is deﬁned as  0 D  x  dx p.xjy x  Z  Z Z  D  dx p.yjx  p.x x dz p.yjz  p.z   p.yjx  D rY iD1 0.y  D 1 r  x  p.yijx   rX iD1  0.yi    x  using  10.10 . Equations  10.12  and  10.13  are simply the integral analogues of  10.2  and  10.3  where the role of zki is taken by p.ykjxi  , yk being the kth state of y  there are gr states . Unconstrained minimisation of  10.12  with respect to p.yjx  gives values for p.ykjxi   that are either 0 or 1.  A key step in the SVQ approach is to constrain the minimisation of  10.12  in such a way as to encourage the formation of code schemes in which each component of the code vector codes a different subspace of the input vector x – an essential requirement for scalability to high dimensions. Luttrell  1999a  imposes two constraints on p.yjx  and x0.y , namely  The ﬁrst constraint states that each component yi is an independent sample drawn from the codebook using p.yijx ; the second states that x0.y  is assumed to be a superposition of r contributions x0.yi    i D 1; : : : ; r . These constraints encourage the formation of coding schemes in which independent subspaces are separately coded  Luttrell, 1999a  and allow  10.12  to be bounded above by the sum of two terms D1 C D2 where  Z  dx  D1 D 2 r  gX yD1 Z D2 D 2.r  cid:4  1   r  p.yjx jx  cid:4  x  0.y j2  dx p.x   p.yjx x  0.y   þþþþþx  cid:4  gX  yD1  2  þþþþþ  The term D1 is a stochastic version of the standard VQ term  see  10.8   and dominates for small values of r. D2 is a nonlinear principal components type term, which describes the integrated squared error between x and a ﬁtting surface modelled as a linear combination  speciﬁed by x0.y   of nonlinear basis functions   p.yjx  . Compare this with the lines and planes of closest ﬁt deﬁnition of principal components analysis in Chapter 9. The expression D1 C D2 must now be minimised with respect to x0.y  and p.yjx . The parameters r  the number of samples drawn from the codebook using p.yjx   and g  the size of the codebook  are model order parameters whose values determine the   10.13    10.14    10.15    Sum-of-squares methods 393  nature of the optimum solution. All that remains now is to specify a suitable form for p.yjx . It can be shown  Luttrell, 1999b  that the optimal form of p.yjx  is piecewise linear in x for regions of the data space that contain a non-vanishing probability density. A convenient approximation is  where Q.yjx  is taken to be of the form  p.yjx  D  Q.yjx  y0D1 Q.y0jx   Pg  Q.yjx  D  1  1 C exp. cid:4 wT .y x  cid:4  b.y    Thus, D1 C D2 is minimised with respect to fw.y ; b.y ; y D 1; : : : ; gg, the parameters of p.yjx  and fx0.y ; y D 1; : : : ; gg using some suitable nonlinear optimisation scheme.  Four-dimensional data, x, are generated to lie on a torus: x D .x1; x2 , Illustration where x1 D .cos. cid:7 1 ; sin. cid:7 1   and x2 D .cos. cid:7 2 ; sin. cid:7 2  , for  cid:7 1 and  cid:7 2 uniformly dis- tributed over [1; 2³]. An SVQ is trained using g D 8 codebook entries  8 ‘clusters’  and for two values of r, the number of samples in the code index vector. Figure 10.18 shows a density plot of p.yjx  for each value of the code index, y, and for r D 5 in the . cid:7 1;  cid:7 2  space  of course, in practice, the underlying variables describing the data are unavailable to us . This type of coding, termed joint encoding, has produced approximately circular receptive ﬁelds in the space on which the data lie. Figure 10.19 shows a density plot of p.yjx  for r D 50 in the . cid:7 1;  cid:7 2  space. This type of coding, termed factorial encoding, has produced receptive ﬁelds that respond to independent directions. Factorial coding is the key to scalability in high dimensions.  Posterior Probabilities on a Toroidal Manifold  joint encoder.  2  f  2  f  2  f  2  f  0 1 2 3 4 5 6  0 1 2 3 4 5 6  0 1 2 3 4 5 6  6 5 4 3 2 1 0 0 1 2 3 4 5 6  6 5 4 3 2 1 0  f1  f1  6 5 4 3 2 1 0  6 5 4 3 2 1 0  f1  f1  6 5 4 3 2 1 0  6 5 4 3 2 1 0  f1  f1  2  f  2  f  2  f  2  f  0 1 2 3 4 5 6  0 1 2 3 4 5 6  0 1 2 3 4 5 6  0 1 2 3 4 5 6  6 5 4 3 2 1 0  6 5 4 3 2 1 0  f1  f1  Figure 10.18 Joint encoding   394 Clustering  Posterior Probabilities on a Toroidal Manifold factorial encoder.  2  f  2  f  2  f  2  f  6 5 4 3 2 1 0 0 1 2 3 4 5 6  6 5 4 3 2 1 0 0 1 2 3 4 5 6  6 5 4 3 2 1 0  f1  f1  6 5 4 3 2 1 0  f1  f1  6 5 4 3 2 1 0  6 5 4 3 2 1 0  f1  f1  2  f  2  f  2  f  2  f  0 1 2 3 4 5 6  0 1 2 3 4 5 6  0 1 2 3 4 5 6  0 1 2 3 4 5 6  6 5 4 3 2 1 0  6 5 4 3 2 1 0  f1  f1  0 1 2 3 4 5 6  0 1 2 3 4 5 6  Figure 10.19 Factorial encoding  10.5.4 Example application study  The problem A study of the inﬂuence of early diagenesis  the conversion, by com- paction or chemical reaction, of sediment into rock  on the natural remanent magneti- sation in sediments from the Calabrian ridge in the central Mediterranean  Dekkers et al., 1994 .  Summary A fuzzy k-means algorithm  Section 10.5.2  was applied to measured data and the clusters plotted in two dimensions using a nonlinear mapping  multidimensional scaling–Chapter 9 . The approach appeared to be useful, linking rock parameters to the geochemical environment.  The data Palaeomagnetic samples were taken from a 37 m long piston core from the Calabrian ridge in the central Mediterranean. In total, 337 samples, taken at 10 cm intervals  corresponding to an average resolution of approximately 3000 years , were analysed and measurements made on six variables  two magnetic variables and four chemical variables .  The model A fuzzy k-means clustering approach was adopted. Also, a two-dimensio- nal projection of the six-dimensional data was derived using a nonlinear mapping based on multidimensional scaling.  Training procedure Simple histograms of the data were produced. These revealed approximately log-normal distributions. Therefore, before applying either the clustering or mapping procedures, the data were logarithmically transformed. Models were developed with an increasing number of clusters, and attempts were made to interpret these in a chemical and magnetic context. Clearly, in problems like these, domain knowledge is   Sum-of-squares methods 395  important, but care must be taken that the results are not biased by the investigator’s prejudices.  Results General trends appeared to be best expressed with an eight-cluster model. Models with six or less clusters did not have sufﬁciently homogeneous clusters. These clusters could be divided into two main categories, one expressing mainly lithological features and the other expressing mainly diagenesis.  10.5.5 Further developments  Procedures for reducing the computational load of the k-means algorithm are discussed by Venkateswarlu and Raju  1992 . Further developments of k-means procedures to other metric spaces  with l1 and l1 norms  are described by Bobrowski and Bezdek  1991 . Juan and Vidal  1994  propose a fast k-means algorithm  based on the approximating and eliminating search algorithm, AESA  for the case when data arise in the form of a dissimilarity. That is, the data cannot be represented in a suitable vector space  with- out performing a multidimensional scaling procedure , though the dissimilarity between points is available. Termed the k-centroids procedure, it determines the ‘most centred sample’ as a centroid of a cluster.  There have been many developments of the basic fuzzy clustering approach and many algorithms have been proposed. Sequential approaches are described by de M´antaras and Aguilar-Mart´ın  1985 . In ‘semi fuzzy’ or ‘soft’ clustering  Ismail, 1988; Selim and Is- mail, 1984b  patterns are considered to belong to some, though not necessarily all, clus- ters. In thresholded fuzzy clustering  Kamel and Selim, 1991  membership values below a threshold are set to zero, with the remaining being normalised, and an approach that per- forms a fuzzy classiﬁcation with prior assumptions on the number of clusters is reported by Gath and Geva  1989 . Developments of the fuzzy clustering approach to data arising in the form of dissimilarity matrices are described by Hathaway and Bezdek  1994 .  Developments of the stochastic vector quantiser approach include developments to  hierarchical schemes  folded Markov chains  for encoding data  Luttrell, 2002 .  10.5.6 Summary  The techniques described in this section minimise a squared error objective function. The k-means procedure is a special case of all of the techniques. It is widely used in pattern recognition, forming the basis for many supervised classiﬁcation techniques. It produces a ‘crisp’ coding of the data in that a pattern belongs to one cluster only.  Fuzzy k-means is a development that allows a pattern to belong to more than one cluster. This is controlled by a membership function. The clusters resulting from a fuzzy k-means approach are softly overlapping, in general, with the degree of overlap controlled by a user-speciﬁed parameter. The learning procedure determines the partition.  The vector quantisation approaches are application-driven. The aim is to produce a crisp coding, and algorithms such as tree-structured vector quantisation are motivated by the need for fast coding. Self-organising feature maps produce an ordered coding.   396 Clustering  Stochastic vector quantisation is a procedure that learns both the codebook entries and the membership function. Different codings may result, depending on a user-speciﬁed parameter. Of particular interest is the factorial encoding that scales linearly with the intrinsic dimensionality of the data.  10.6 Cluster validity  10.6.1 Introduction  Cluster validity is an issue fraught with difﬁculties and rarely straightforward. A clus- tering algorithm will partition a data set of objects even if there are no natural clusters within the data. Different clustering methods may produce different classiﬁcations. How do we know whether the structure is a property of the data set and not imposed by the particular method that we have chosen? In some applications of clustering techniques we may not be concerned with groupings in the data set. For example, in vector quantisation we may be concerned with the average distortion in reconstructing the original data or in the performance of any subsequent analysis technique. This may be measured by the error rate in a discrimination problem or diagnostic performance in image reconstruction  see the examples in the following section . In these situations, clustering is simply a means of obtaining a partition, not of discovering structure in the data.  Yet, if we are concerned with discovering groupings within a data set, how do we validate the clustering? A simple approach is to view the clustering in a low-dimensional representation of the data. Linear and nonlinear projection methods have been discussed in Chapter 9. Alternatively, we may perform several analyses using different clustering methods and compare the resulting classiﬁcations to see whether the derived structure is an artefact of a particular method. More formal procedures may also be applied and we discuss some approaches in this section.  There are several related issues in cluster validity. The ﬁrst concerns the goodness of ﬁt of the derived classiﬁcation to the given data: how well does the clustering reﬂect the true data structure? What appropriate measures of distortion or internal criterion measures should be used? A second issue concerns the determination of the ‘correct’ number of groups within the data. This is related to the ﬁrst problem since it often requires the calculation of distortion measures. Finally, we address the issue of identifying genuine clusters in a classiﬁcation based on work by Gordon  1996a .  There are three main classes of null model for the complete absence of structure in a  data set  Gordon 1994b, 1996a .  1. Poisson model  Bock, 1985 . Objects are represented as points uniformly distributed in some region A of the p-dimensional data space. The main problem with this model is the speciﬁcation of A. Standard deﬁnitions include the unit hypercube and the hypersphere.  2. Unimodal model  Bock, 1985 . The variables have a unimodal distribution. The dif- ﬁculty here is the speciﬁcation of this density function. Standard deﬁnitions include the spherical multivariate normal.   Cluster validity 397  3. Random dissimilarity matrix  Ling, 1973 . This is based on data arising in the form of dissimilarities. The elements of the  lower triangle of the  dissimilarity matrix are ranked in random order, all orderings being regarded as equally likely. One of the problems with this is that if objects i and j are close  di j is small , you would expect dik and d jk to have similar ranks for each object k.  The Poisson model and the unimodal model were used as part of the study described  in Section 10.6.4.  10.6.2 Distortion measures  In assessing particular hierarchical schemes we must consider how well the structure in the original data can be described by a dendrogram. However, since the structure in the data is not known  this is precisely what we are trying to determine  and since each clustering is simply a method of exploratory data analysis that imposes its own structure on the data, this is a difﬁcult question to address. One approach is to examine various measures of distortion. Many measures have been proposed  see, for example, Cormack, 1971, for a summary . They are based on differences between the dissimilarity is the matrix d and the matrix of ultrametric dissimilarity coefﬁcients d distance between the groups containing i and j when the groups are amalgamated. Jardine and Sibson  1971  propose several goodness-of-ﬁt criteria. One scale-free measure of classiﬁability is deﬁned by  Ł, where dŁ  i j  11 D  P  i < j jdi j  cid:4  dŁ i jj P  i < j di j  Small values of 11 are indicative that the data are amenable to the classiﬁcation method that produced d  Ł.  There are many other measures of distortion, both for hierarchical and nonhierarchical schemes. Milligan  1981  performed an extensive Monte Carlo study of 30 internal criterion measures applied to the results of hierarchical clusterings, although the results may also apply to non-hierarchical methods.  10.6.3 Choosing the number of clusters  The problem of deciding how many clusters are present in the data is one common to all clustering methods. There are numerous techniques for cluster validity reported in the literature. Many methods are very subjective and can be unreliable, with some criteria indicating clusters present when analysing unstructured data. Many methods have been proposed for determining the number of groups. When applied to hierarchical schemes, these are sometimes referred to as stopping rules. Many intuitive schemes have been proposed for hierarchical methods; for example, we may examine the plot of fusion level against the number of groups, g  see Figure 10.20 , and look for a ﬂattening of the curve showing that little improvement in the description of the data structure is to be gained above a particular value of g. Deﬁning Þ j ; j D 0; : : : ; n  cid:4  1, to be the fusion level   398 Clustering  40.0  30.0  20.0  10.0  0.0  Š  1  Š  2  Š  3  Š  4  Š  5  Š  6  Š  7  Š  8  Š  9  Š  Š  Š  10  11  12  Figure 10.20 Fusion level against the number of clusters  corresponding to the stage with n  cid:4  j clusters, Mojena  1977  proposes a stopping rule that selects the number of groups as g such that Þn cid:4 g is the lowest value of Þ for which  Þn cid:4 g > Þ C ksÞ  where Þ and sÞ are the mean and the unbiased standard deviation of the fusion levels Þ; k is a constant, suggested by Mojena to be in the range 2.75–3.5.  Milligan and Cooper  1985  examine 30 procedures applied to classiﬁcations of data sets containing 2, 3, 4, or 5 distinct non-overlapping clusters by four hierarchical schemes. They ﬁnd that Mojena’s rule performs poorly with only two groups present in the data and the best performance is for 3, 4 or 5 groups with a value of k of 1.25. One of the better criteria that Milligan and Cooper  1985  assess is that of Calinski and Harabasz  1974 . The number of groups is taken to be the value of g that corresponds to the maximum of C, given by  C D Tr.S B   Tr.SW     cid:7    cid:6  n  cid:4  g g  cid:4  1  This is evaluated further by Atlas and Overall  1994  who compare it with a split-sample replication rule of Overall and Magee  1992 . This gave improved performance over the Calinski and Harabasz criterion.  Dubes  1987  also reports the results of a Monte Carlo study on the effectiveness of two internal criterion measures in determining the number of clusters. Jain and Moreau  1987  propose a method to estimate the number of clusters in a data set using the bootstrap technique. A clustering criterion based on the within- and between-group scatter matrices  developing a criterion of Davies and Bouldin, 1979  is proposed and the k- means and three hierarchical algorithms are assessed. The basic method of determining the number of clusters using a bootstrap approach can be used with any cluster method. Several authors have considered the problem of testing for the number of components of a normal mixture  see Chapter 2 . This is not a trivial problem and depends on many factors, including shape of clusters, separation, relative sizes, sample size and dimension of data. Wolfe  1971  proposes a modiﬁed likelihood ratio test in which the null hypothesis g D g0 is tested against the alternative hypothesis g D g1. The quantity   cid:8  n  cid:4  1  cid:4  p  cid:4  g1 2   cid:9    cid:4  2 n  log.½   where ½ is the likelihood ratio, is tested as chi-square with degrees of freedom being twice the difference in the number of parameters in the two hypotheses  Everitt et al.,   Cluster validity 399  2001 , excluding mixing proportions. For components of a normal mixture with arbitrary covariance matrices, the number of parameters, n p, is  n p D 2.g1  cid:4  g0   p. p C 3   2  and with common covariance matrices  the case studied by Wolfe, 1971 , n p D 2.g1  cid:4  g0  p. McLachlan and Basford  1988  recommend that Wolfe’s modiﬁed likelihood ratio test be used as a guide to structure rather than rigidly interpreted. Ismail  1988  reports the results of cluster validity studies within the context of soft clustering and lists nine validity functionals that provide useful tools in determining cluster structure  see also Pal and Bezdek 1995 .  It is not reasonable to expect a single statistic to be suitable for all problems in cluster validity. Many different factors are involved and since clustering is essentially a method of exploratory data analysis, we should not put too much emphasis on the results of a single classiﬁcation, but perform several clusterings using different algorithms and measures of ﬁt.  10.6.4 Identifying genuine clusters We wish to identify whether a cluster C  of size c  deﬁned by C D fi : di j < dik for all j 2 C; k =2 Cg  is a valid cluster. We describe here the procedure of Gordon  1994a  who develops an approach to cluster validation based on a U statistic:  and  Ui jkl D  8< :  0 1 2 1  if di j < dkl if di j D dkl if di j > dkl  U D X .i; j  2W  X .k;l 2B  Ui jkl  for subsets W and B of ordered pairs .i; j   2 W and .k; l  2 B. W is taken to be those pairs where objects i and j both belong to the cluster C and B comprises pairs where one element belongs to C and the other does not.  The basic algorithm is deﬁned as follows:  1. Evaluate U for the cluster C; denote it by UŁ. 2. Generate a random n ð p pattern matrix and cluster it using the same algorithm used  to produce C.  3. Calculate U .k  for each cluster of size k D 2; : : : ; n cid:4 1  arising through the partition- ing of a dendrogram, for example . If there is more than one of a given size, select one of that size randomly.   400 Clustering  4. Repeat steps 2 and 3 until there are m  cid:4  1 values of U .k  for each value of k. 5. If UŁ is less than the jth smallest value of U .k , the null hypothesis of randomness  is rejected at the 100. j=m % level of signiﬁcance.  Note that the values of U .k  are independent of the data set. Gordon takes m D 100 and evaluates the above approach under both the Poisson and unimodal  spherical multivariate normal  models. The clusterings using Ward’s method are assessed on four data sets. Results for the approach are encouraging. Further reﬁne- ments could include the use of other test statistics and developments of the null models.  10.7 Application studies  Applications of hierarchical methods of cluster analysis include the following: ž Flight monitoring. Eddy et al.  1996  consider single-link clustering of large data sets  more than 40 000 observations  of high-dimensional data relating to aircraft ﬂights over the United States.  ž Clinical data. D’Andrea et al.  1994  apply the nearest centroid method to data relating  to adult children of alcoholics.  ž In a comparative study of seven methods of hierarchical cluster analysis on 20 data sets, Morgan and Ray  1995  examine the extent of inversions in dendrograms and non-uniqueness. They conclude that inversions are expected to be encountered and recommend against the use of the median and centroid methods. Also, non-uniqueness is a real possibility for many data sets.  The k-means clustering approach is widely used as a preprocessor for supervised  classiﬁcation to reduce the number of prototypes: ž Coal petrography. In a study to classify the different constituents  macerals  of coal  Mukherjee et al., 1994 , the k-means algorithm was applied to training images  train- ing vectors consist of RGB level values  to determine four clusters of known types  vitrinite, inertinite, exinite and background . These clusters are labelled and test im- ages are classiﬁed using the labelled training vectors.  ž Crop classiﬁcation. Conway et al.  1991  use a k-means algorithm to segment synthetic aperture radar images as part of a study into crop classiﬁcation. k-means is used to identify sets of image regions that share similar attributes prior to labelling. Data were gathered from a ﬁeld of ﬁve known crop types and could be clearly separated into two clusters – one containing the broad-leaved crops and the other the narrow-leaved crops.  k-means is also used for image and speech coding applications  see below .   Application studies 401  Examples of fuzzy k-means applications are as follows:  ž Medical diagnosis. Li et al.  1993  use a fuzzy k-means algorithm for image segmen- tation in a study on automatic classiﬁcation and tissue labelling of two-dimensional magnetic resonance images of the human brain.  ž Acoustic quality control. Meier et al.  1994  describe the application of fuzzy k-means to cluster six-dimensional feature vectors as part of a quality control system for ceramic tiles. The signals are derived by hitting the tiles and digitising and ﬁltering the recorded signal. The resulting classes are interpreted as good or bad tiles.  ž Water quality. Mukherjee et al.  1995  compared fuzzy k-means with two alternative approaches to image segmentation in a study to identify and count bacterial colonies from images.  See also the survey on fuzzy clustering by Yang  1993  for further references to appli- cations of fuzzy k-means.  One example of a Bayesian approach to mixture modelling is worthy of note  other applications of mixture models are given in Chapter 2 . Dellaportas  1998  considers the application of mixture modelling to the classiﬁcation of neolithic ground stone tools. A Bayesian methodology is adopted and developed in three main ways to apply to data  147 measurements on four variables  consisting of variables of mixed type – continuous and categorical. Missing values and measurement errors  errors in variables  in the continuous variables are also treated. Gibbs sampling is used to generate samples from the posterior densities of interest and classiﬁcation to one of two classes is based on the mean of the indicator variable, zi . After a ‘burn-in’ of 4000 iterations, 4000 further samples were used as the basis for posterior inference.  There are several examples of self-organising feature map applications:  ž Engineering applications. Kohonen et al.  1996  review the self-organising map algo- rithm and describe several engineering applications, including fault detection, process analysis and monitoring, computer vision, speech recognition, robotic control and in the area of telecommunications.  ž Human protein analysis. Ferr´an et al.  1994  use a self-organising map to cluster protein sequences into families. Using 1758 human protein sequences, they cluster using two-dimensional maps of various sizes and label the nodes in the grid using proteins belonging to known sequences.  ž Radar target classiﬁcation. Stewart et al.  1994  develop a self-organising map and a learning vector quantisation approach to radar target classiﬁcation using turntable data of four target types. The data consist of 33-dimensional feature vectors  33 range gates  and 36 000 patterns per target were used. Performance as a function of the number of cluster centres is reported, with the performance of learning vector quantisation better than that of a simplistic nearest-neighbour algorithm.  ž Fingerprint classiﬁcation. Halici and Ongun  1996 , in a study on automatic ﬁnger- print classiﬁcation, use a self-organising map, and one modiﬁed by preprocessing the feature vectors by combining them with ‘certainty’ vectors that encode uncertainties in the ﬁngerprint images. Results show an improvement on previous studies using a multilayer perceptron on a database of size 2000.   402 Clustering  Sum-of-squares methods have been applied to language disorders. Powell et al.  1979  use a normal mixture approach and a sum-of-squares method in a study of 86 aphasic cases referred to a speech therapy unit. Four groups are found, which are labelled as severe, high–moderate, low–moderate and mild aphasia.  Vector quantisation has been widely applied as a preprocessor in many studies:  ž Speech recognition. Zhang et al.  1994  assess three different vector quantisers  in- cluding the LBG algorithm and an algorithm based on normal mixture modelling  as preprocessors for a hidden Markov model based recogniser in a small speech recog- nition problem. They found that the normal mixture model gave the best performance of the subsequent classiﬁer. See also Bergh et al.  1985 .  ž Medical diagnosis. Cosman et al.  1993  assess the quality of tree-structured vector quantisation images by the diagnostic performance of radiologists in a study on lung tumour and lymphadenopathy identiﬁcation. Initial results suggest that a 12 bits per pixel  bpp  computerised tomography chest scan image can be compressed to between 1 bpp and 2 bpp with no signiﬁcant change in diagnostic accuracy: subjective quality seems to degrade sooner than diagnostic accuracy falls off.  ž Speaker recognition. Recent advances in speaker recognition are reviewed by Furui  1997 . Vector quantisation methods are used to compress the training data and produce codebooks of representative feature vectors characterising speaker-speciﬁc features. A codebook is generated for each speaker by clustering training feature vectors. At the speaker recognition stage, an input utterance is quantised using the codebook of each speaker and recognition performed by assigning the utterance to the speaker whose codebook produces minimum distortion. A tutorial on vector quantisation for speech coding is given by Makhoul et al.  1985 .  10.8 Summary and discussion  In this chapter we have covered a wide range of techniques for partitioning a data set. This has included approaches based on cluster analysis methods and vector quantisation methods. Although both approaches have much in common – they both produce a dis- section of a given data set – there are differences. In cluster analysis, we tend to look for ‘natural’ groupings in the data that may be labelled in terms of the subject matter of the data. In contrast, the vector quantisation methods are developed to optimise some appropriate criterion from communication theory. One area of common ground we have discussed in this chapter is that of optimisation methods with speciﬁc implementations in terms of the k-means algorithm in cluster analysis and the LBG algorithm in vector quantisation.  As far as cluster analysis or classiﬁcation is concerned, there is no single best tech- nique. Different clustering methods can yield different results and some methods will fail to detect obvious clusters. The reason for this is that each method implicitly forces a structure on the given data. For example, the sum-of-squares methods will tend to produce hyperspherical clusters. Also, the fact that there is a wide range of available methods partly stems from the lack of a single deﬁnition of the word ‘cluster’. There   Summary and discussion 403  is no universal agreement as to what constitutes a cluster and so a single deﬁnition is insufﬁcient.  A further difﬁculty with cluster analysis is in deciding the number of clusters present. This is a trade-off between parsimony and some measure of increase in within-cluster homogeneity. This problem is partly due to the difﬁculty in deciding what a cluster actually is and partly because clustering algorithms tend to produce clusters even when applied to random data.  Both of the above difﬁculties may be overcome to some degree by considering sev- eral possible classiﬁcations or comparing classiﬁcations on each half of a data set  for example, McIntyre and Blashﬁeld, 1980; Breckenridge, 1989 . The interpretation of these is more important than a rigid inference of the number of groups. But which methods should we employ? There are advantages and disadvantages of all the approaches we have described. The optimisation methods tend to require a large amount of computer time  and consequently may be infeasible for large data sets, though this is becoming less critical these days . Of the hierarchical methods, the single link is preferred by many users. It is the only one to satisfy the Jardine–Sibson conditions, yet with noisy data it can join separate clusters  chaining effect . It is also invariant under monotone transfor- mations of the dissimilarity measure. Ward’s method is also popular. The centroid and median methods should be avoided since inversions may make the resulting classiﬁcation difﬁcult to interpret.  There are several aspects of cluster analysis that we have mentioned only brieﬂy in this chapter and we must refer the reader to the literature on cluster analysis for further details. An important problem is the choice of technique for mixed mode data. Everitt and Merette  1990   see also Everitt, 1988  propose a ﬁnite mixture model approach for clustering mixed mode data, but computational considerations may mean that it is not practically viable when the data sets contain a large number of categorical variables.  Clumping methods  or methods of overlapping classiﬁcation in which an object can belong to more than one group  have not been considered explicitly, though the mem- bership function in the fuzzy clustering approach may be regarded as allowing an object partial membership of several groups in some sense. An algorithm for generating over- lapping clusters is given by Cole and Wishart  1970 .  The techniques described in this chapter all apply to the clustering of objects. How- ever, there may be some situations where clustering of variables, or simultaneous clus- tering of objects and variables, is required. In clustering of variables, we seek subsets of variables that are so highly correlated that each can be replaced by any one of the subset, or perhaps a  linear or nonlinear  combination of the members. Many of the techniques described in this chapter can be applied to the clustering of variables, and therefore we require a measure of similarity or dissimilarity between variables. Of course, techniques for feature extraction  for example, principal components analysis  perform this process. Another point to reiterate about cluster analysis is that it is essentially an exploratory method of multivariate data analysis providing a description of the measurements. Once a solution or interpretation has been obtained then the investigator must re-examine and assess the data set. This may allow further hypotheses  perhaps concerning the variables used in the study, the measures of dissimilarity and the choice of technique  to be generated. These may be tested on a new sample of individuals.  Both cluster analysis and vector quantisation are means of reducing a large amount of data to a form in which it is either easier to describe or represent in a machine. Clustering   404 Clustering  of data may be performed prior to supervised classiﬁcation. For example, the number of stored prototypes in a k-nearest-neighbour classiﬁer may be reduced by a clustering procedure. The new prototypes are the cluster means, and the class of the new prototype is decided on a majority basis of the members of the cluster. A development of this approach that adjusts the decision surface by modifying the prototypes is learning vector quantisation  Kohonen, 1989 .  Self-organising maps may be viewed as a form of constrained classiﬁcation: clustering in which there is some form of constraint on the solution. In this particular case, the constraint is an ordering on the cluster centres. Other forms of constraint may be that objects within a cluster are required to comprise a spatially contiguous set of objects  for example, in some texture segmentation applications . This is an example of contiguity- constrained clustering  Gordon, 1999; Murtagh, 1992; see also Murtagh, 1995, for an application to the outputs of the self-organising map . Other forms of constraint may be on the topology of the dendrogram or the size or composition of the classes. We refer to Gordon  1996b  for a review.  Gordon  1994b, 1996a  provides reviews of approaches to cluster validation; see also  Bock  1989  and Jain and Dubes  1988 .  10.9 Recommendations  It has been said that ‘automatic classiﬁcation is rapidly replacing factor analysis and principal component analysis as a “heffalump” trap for the innocent scientist’  Jardine, 1971 . Certainly, there is a large number of techniques to choose from and the availability of computer packages means that analyses can be readily performed. Nevertheless, there are some general guidelines to follow when carrying out a classiﬁcation.  1. Detect and remove outliers. Many clustering techniques are sensitive to the presence of outliers. Therefore, some of the techniques discussed in Chapter 11 should be used to detect and possibly remove these outliers.  2. Plot the data in two dimensions if possible in order to understand structure in the  data. It might be helpful to use the ﬁrst two principal components.  3. Carry out any preprocessing of the data. This may include a reduction in the number  of variables or standardisation of the variables to zero mean and unit variance.  4. If the data are not in the form of a dissimilarity matrix then a dissimilarity measure  must be chosen  for some techniques  and a dissimilarity matrix formed.  5. Choose an appropriate technique. Optimisation techniques are more computationally expensive. Of the hierarchical methods, some studies favour the use of the average-link method, but the single-link method gives solutions that are invariant to a monotone transformation measure. It is the only one to satisfy all the conditions laid down by Jardine and Sibson  1971  and is their preferred method. We advise against the use of the centroid and median methods since inversions are likely to arise.  6. Evaluate the method. Assess the results of the clustering method you have employed. How do the clusters differ? We recommend that you split the data set into two parts and   Notes and references 405  compare the results of a classiﬁcation on each subset. Similar results would suggest that useful structure has been found. Also, use several methods and compare results. With many of the methods some parameters must be speciﬁed and it is worthwhile carrying out a classiﬁcation over a range of parameter values to assess stability.  7. In using vector quantisation as a preprocessor for a supervised classiﬁcation problem, model each class separately rather than the whole data set, and label the resulting codewords.  8. If you require some representative prototypes, we recommend using the k-means  algorithm.  Finally, we reiterate that cluster analysis is usually the ﬁrst stage in an analysis and  unquestioning acceptance of the results of a classiﬁcation is extremely unwise.  10.10 Notes and references  There is a vast literature on cluster analysis. A very good starting point is the book by Everitt et al.  2001 . Now in its fourth edition, this book is a mainly non-mathematical account of the most common clustering techniques, together with their advantages and disadvantages. The practical advice is supported by several empirical investigations. An- other good introduction to methods and assessments of classiﬁcation is the book by Gordon  1999 . McLachlan and Basford  1988  discuss the mixture model approach to clustering in some detail.  Of the review papers, that by Cormack  1971  is worth reading and provides a good summary of the methods and problems of cluster analysis. The article by Diday and Simon  1976  gives a more mathematical treatment of the methods, together with descriptive algorithms for their implementation.  Several books have an orientation towards biological and ecological matters. Jardine and Sibson  1971  give a mathematical treatment. The book by Sneath and Sokal  1973  is a comprehensive account of cluster analysis and the biological problems to which it can be applied. The book by Cliffrd and Stephenson  1975  is a non-mathematical general introduction to the ideas and principles of numerical classiﬁcation and data analysis, though it does not cover many of the approaches described in this chapter, concentrating on hierarchical classiﬁcatory procedures. The book by Jain and Dubes  1988  has a pattern recognition emphasis. McLachlan  1992b  reviews cluster analysis in medical research.  A more specialist book is that of Zupan  1982 . This monograph is concerned with  the problem of implementing hierarchical techniques on large data sets.  The literature on fuzzy techniques in cluster analysis is reviewed by Bezdek and Pal  1992 . This book contains a collection of some of the important papers on fuzzy mod- els for pattern recognition, including cluster analysis and supervised classiﬁer design, together with fairly extensive bibliographies. A survey of fuzzy clustering and its ap- plications is provided by Yang  1993 . An interesting probabilistic perspective of fuzzy methods is provided by Laviolette et al.  1995 .   406 Clustering  Tutorials and surveys of self-organising maps are given by Kohonen  1990, 1997 ,  Kohonen et al.  1996  and Ritter et al.  1992 .  There are various books on techniques for implementing methods, which give algo- rithms in the form of Fortran code or pseudo-code. The books by Anderberg  1973 , Hartigan  1975 , Sp¨ath  1980  and Jambu and Lebeaux  1983  all provide a description of a clustering algorithm, Fortran source code and a supporting mathematical treatment, sometimes with case studies. The book by Murtagh  1985  covers more recent develop- ments and is also concerned with implementation on parallel machines.  There are many software packages publicly available for cluster analysis. The website www.statistical-pattern-recognition.net contains references and point- ers to websites for further information on techniques.  Exercises  Data set 1: Generate n D 500 samples .xi ; yi  ; i D 1; : : : ; n, according to  xi D i n yi D sin  ³ C nx  cid:7   cid:6  i n  ³  C n y  where nx and n y are normally distributed with mean 0.0 and variance 0.01. Data set 2: Generate n samples from a multivariate normal   p variables  of diagonal covariance matrix ¦ 2I , ¦ 2 D 1, and zero mean. Take n D 40, p D 2. Data set 3: This consist of data comprising two classes: class !1 is distributed as 0:5N ..0; 0 ; I   C 0:5N ..2; 2 ; I   and class !2 ¾ N ..2; 0 ; I    generate 500 samples for training and test sets, p.!1  D p.!2  D 0:5 .  1. Is the square of the Euclidean distance a metric? Does it matter for any clustering  algorithm?  2. Observations on six variables are made for seven groups of canines and given in Table 10.4  Krzanowski and Marriott, 1994; Manly, 1986 . Construct a dissimilarity matrix using Euclidean distance after standardising each variable to unit variance. Carry out a single-link cluster analysis.  3. Compare the single-link method of clustering with k-means, discussing computa-  tional requirements, storage, and applicability of the methods.  4. A mixture of two normals divided by a normal density having the same mean and variance as the mixed density is always bimodal. Prove this for the univariate case.  5. Implement a k-means algorithm and test it on two-dimensional normally distributed data  data set 2 with n D 500 . Also, use the algorithm within a tree-structured vector quantiser and compare the two methods.   Exercises 407  Table 10.4 Data on mean mandible measurements  from Manly, 1986   Group  x1  x2  x3  x4  x5  x6  Modern Thai dog Golden jackal Chinese wolf Indian wolf Cuon Dingo Prehistoric dog  9.7 8.1 13.5 11.5 10.7 9.6 10.3  21.0 16.7 27.3 24.3 23.5 22.6 22.1  19.4 18.3 26.8 24.5 21.4 21.1 19.1  7.7 7.0 10.6 9.3 8.5 8.3 8.1  32.0 30.3 41.9 40.0 28.8 34.4 32.3  36.5 32.9 48.1 44.6 37.6 43.1 35.0  6. Using data set 1, code the data using the Luttrell algorithm  plot positions of centres for various numbers of code vectors . Compute the average distortion as a function of the number of code vectors. How would you modify the algorithm for data having circular topology?  7. Using data set 1, construct a tree-structured vector quantiser, partitioning the clusters  with the largest sum-squared error at each stage. Compute the average distortion.  8. Using data set 2, cluster the data using Ward’s method and Euclidean distance. Using Gordon’s approach for identifying genuine clusters  unimodal null model , how many clusters are valid at the 5% level of signiﬁcance?  9. Implement a learning vector quantisation algorithm on data set 3. Plot performance as a function of the number of cluster centres. What would be the advantages and disadvantages of using the resulting cluster centres as centres in a radial basis function network?  10. Show that the single-link dendrogram is invariant to a nonlinear monotone transfor-  mation of the dissimilarities.  11. For a distance between two clusters A and B of objects given by dAB D jmA cid:4 mBj2, where mA is the mean of the objects in cluster A, show that the formula expressing the distance between a cluster k and a cluster formed by joining i and j is  diC j;k D ni  ni C n j  dik C n j  ni C n j  d jk  cid:4  ni n j  .ni C n j  2 di j  where there are ni objects in group i. This is the update rule for the centroid method.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   11  Additional topics  Overview  Two main issues in classiﬁer design are addressed. The ﬁrst concerns model selection–choosing the appropriate type and complexity of classiﬁer. The second concerns problems with data–mixed variables, outliers, missing values and unreli- able labelling.  11.1 Model selection  In many areas of pattern recognition we are faced with the problem of model selection; that is, how complex should we allow our model to be, measured perhaps in terms of the number of free parameters to estimate? The optimum complexity of the model depends on the quantity and the quality of the training data. If we choose a model that is too complex, then we may be able to model the training data very well  and also any noise in the training data , but it is likely to have poor generalisation performance on unseen data, drawn from the same distribution as the training set was drawn from  thus the model over-ﬁts the data . If the model is not complex enough, then it may fail to model structure in the data adequately. Model selection is inherently a part of the process of determining optimum model parameters. In this case, the complexity of the model is a parameter to determine. As a consequence, many model selection procedures are based on optimising a criterion that penalises a goodness-of-ﬁt measure by a model-complexity measure.  The problem of model selection arises with many of the techniques described in this  book. Some examples are:  1. How many components in a mixture model should be chosen to model the data  adequately?  2. How is the optimum tree structure found in a decision-tree approach to discrimina- tion? This is an example in determining an appropriate number of basis functions in an expansion, where the basis functions in this case are hyperrectangles, with sides   410 Additional topics  parallel to the coordinate axes. Other examples include the number of projections in projection pursuit and the number of hinges in the hinging hyperplane model.  3. How many hidden units do we take in a multilayer perceptron, or centres in a radial  basis function network?  4. How many clusters describe the data set?  In this section, we give some general procedures that have been widely used for model selection. A comprehensive review of model selection methods, particularly in the context of time series analysis though with much wider applicability, is given by Glendinning  1993 . Anders and Korn  1999  examine model selection procedures in the context of neural networks, comparing ﬁve strategies in a simulation study.  11.1.1 Separate training and test sets  In the separate training and test set approach, both training and test sets are used for model selection. A test set used in this way is often termed the validation set. The training set is used to optimise a goodness-of-ﬁt criterion and the performance recorded on the validation set. As the complexity of the model is increased, it is expected that performance on the training set will improve  as measured in terms of the goodness-of-ﬁt criterion , while the performance on the validation set will begin to deteriorate beyond a certain model complexity.  This is one approach  though not the preferred approach  used for the classiﬁcation and regression tree training models  Breiman et al., 1984 . A large tree is grown to over- ﬁt the data, and pruned back until the performance on a separate validation set fails to improve. A similar approach may be taken for neural network models.  The separate training and validation set procedure may also be used as part of the op- timisation process for a model of a given complexity, particularly when the optimisation process is carried out iteratively, as in a nonlinear optimisation scheme. The values of the parameters of the model are chosen, not as the ones that minimise the given criterion on the training set, but those for which the validation set performance is minimum. Thus, as training proceeds, the performance on the validation set is monitored  by evaluating the goodness-of-ﬁt criterion using the validation data  and training is terminated when the validation set performance begins to deteriorate.  Note that in this case the validation set is not an independent test set that may be used for error rate estimation. It is part of the training data. A third data set is required for an independent estimate of the error rate.  11.1.2 Cross-validation  Cross-validation as a method of error rate estimation was described in Section 8.2. It is a simple idea. The data set of size n samples is partitioned into two parts. The model parameters are estimated using one set  by minimising some optimisation criterion  and the goodness-of-ﬁt criterion evaluated on the second set. The usual version of cross- validation is the simple leave-one-out method in which the second set consists of a   Model selection 411  single sample. The cross-validation estimate of the goodness-of-ﬁt criterion, CV, is then the average over all possible training sets of size n  cid:1  1. As a means of determining an appropriate model, the cross-validation error, CV, is calculated for each member of the family of candidate models, fMk ; k D 1; : : : ; Kg, and the model MOk chosen, where  Ok D argmin CV.k   Cross-validation tends to over-ﬁt when selecting a correct model; that is, it chooses an over-complex model for the data set. There is some evidence that multifold cross- validation, when d > 1 samples are deleted from the training set, does better than simple leave-one-out cross-validation for model selection purposes  Zhang, 1993 . The use of cross-validation to select a classiﬁcation method is discussed further by Schaffer  1993 .  11.1.3 The Bayesian viewpoint  In the Bayesian approach, prior knowledge about models, Mk, and parameters, θ k, is incorporated into the model selection process. Given a data set X, the distribution of the models may be written using Bayes’ theorem as p.MkjX    p.XjMk   p.Mk    Z  D p.Mk    p.XjMk ; θ k   p.θ kjMk  dθ k  and we are therefore required to specify a prior distribution p.Mk ; θ k  . If a single model is required, we may choose MOk, where  Ok D argmax p.MkjX   However, in a Bayesian approach, all models are considered. Over-complex models are penalised since they predict the data poorly. Yet, there are difﬁculties in specifying priors and several methods have been suggested that use data-dependent ‘priors’  Glendinning, 1993 . Nevertheless, there are good applications of the Bayesian approach in pattern recognition.  11.1.4 Akaike’s information criterion  Akaike, in a series of papers including Akaike  1973, 1974, 1977, 1981, 1985 , used ideas from information theory to suggest a model selection criterion. A good introduction to the general principles is given by Bozdogan  1987; see also Sclove, 1987 . Suppose that we have a family of candidate models fMk ; k D 1; : : : ; Kg, with the kth model depending on a parameter vector θ k D . cid:6 k;1; : : : ;  cid:6 k;ž.k  T , where ž.k  is the number of free parameters of model k. Then the information criterion proposed by Akaike  AIC  is given by  AIC.k  D  cid:1 2 log[L.Oθ k  ] C 2ž.k    11.1    412 Additional topics  where Oθ k is the maximum likelihood estimate of θ k, L[:] is the likelihood function, and the model Mk is chosen as that model, MOk, where  Ok D argmin AIC.k    cid:1 2E[log. p.Xnjθ k   ]  Equation  11.1  represents the unbiased estimate of minus twice the expected log- likelihood,  where Xn is the set of observations fx1; : : : ; xng, characterised by p.xjθ  .  There are a number of difﬁculties in applying  11.1  in practice. The main problem is that the correction for the bias of the log-likelihood, ž.k , is only valid asymptotically. Various other corrections have been proposed that have the same asymptotic performance, but different ﬁnite-sample performance.  11.2 Learning with unreliable classiﬁcation  All of the supervised classiﬁcation procedures that we have considered in this book have assumed that we have a training set of independent pairs of observations and corre- sponding labels. Further, we have assumed that the ‘teacher’ who provides the labels is perfect; that is, the teacher never makes mistakes and classiﬁes each object with certainty. Thus, the problem that we have addressed is as follows. Given a random variable pair .X; Y  , denote the observation x and the class label y. Given a set of measurements S D f.x1; y1 ; : : : ; .xn; yn g, design a classiﬁer to estimate y given x and S.  There are situations in which we do not know the class labels with certainty. This can arise in a number of different ways. Firstly, we may have errors in the labels. Thus the training set comprises S0 D f.x1; z1 ; : : : ; .xn; zn g, where zi are the erroneous labels. The problem now is to estimate y given x and S0, the misclassiﬁed design set. Misclassi- ﬁcation may or may not depend on the feature vector x  termed non-random and random misclassiﬁcation respectively . Much of the early work in this area assumed patterns that were randomly mislabelled  Lachenbruch, 1966; Chitteneni, 1980, 1981; Michalek and Tripathi, 1980 , but it is logical that if the teacher is allowed to be imperfect then it will be more likely that those patterns that are difﬁcult to classify will be mislabelled. Grayson  1987  considers non-random misclassiﬁcation.  Lugosi  1992  investigates the error rate of two nonparametric classiﬁers  nearest- neighbour and an L1 estimator of the posterior probabilities . Three types of dependence of z on x and y are considered:  1. Discrete memoryless channel. The teacher is communicating with a student over a noisy channel so that the true values yi are transmitted, but the training labels zi are received as the output of the channel. The channel is determined by the transition probabilities  a ji D P.Z D ijY D j    2. Misprints in the training sequence. Here, the labels zi can take some arbitrary value  with probability p.   Missing data 413  3. The ‘consequently lying’ teacher. The training set value z is the result of a decision  z D h.x ; h : R p ! f0; 1g  a two-class problem is considered .  This latter model may apply in the medical domain, for example, when a physician makes a diagnosis that may be incorrect. However, in such circumstances it is those patterns that are difﬁcult to recognise that are more likely to be mislabelled. It would be better for the ‘teacher’  the physician in this case  to be allowed to be an imperfect recogniser, indicating the reliability of a decision rather than choosing one class with certainty. This is another way in which we do not know the class labels with certainty and is the problem addressed by Aitchison and Begg  1976 . In many areas it is difﬁcult to classify with certainty, and removing uncertain data may give misleading results. Is it far better to have a classiﬁer that can allow some expression of doubt to be given for the class.  11.3 Missing data  Many classiﬁcation techniques assume that we have a set of observations with measure- ments made on each of p variables. However, missing values are common. For example, questionnaires may be returned incomplete; in an archaeological study, it may not be possible to make a complete set of measurements on an artefact because of missing parts; in a medical problem, a complete set of measurements may not be made on a pa- tient, perhaps owing to forgetfulness by the physician or being prevented by the medical condition of the patient.  How missing data are handled depends on a number of factors: how much is missing; why data are missing; whether the missing values can be recovered; whether values are missing in both the design and test set. There are several approaches to this problem.  1. We may omit all incomplete vectors from our analysis. This may be acceptable in some circumstances, but not if there are many observations with missing values. For example, in the head injury study of Titterington et al.  1981  referred to in Chapter 3, 206 out of the 500 training patterns and 199 out of the 500 test patterns have at least one observation missing. Neglecting an observation because perhaps one out of 100 variables has not been measured means that we are throwing away potentially useful information for classiﬁer design. Also, in an incomplete observation, it may be that the variables that have been measured are the important ones for classiﬁcation anyway.  2. We may use all the available information. The way that we would do this depends on the analysis that we are performing. In estimating means and covariances, we would use only those observations for which measurements have been made on the relevant variables. Thus, the estimates would be made on different numbers of samples. This can give poor results and may lead to covariance matrices that are not positive deﬁnite. Other approaches must be used to estimate principal components when data are missing  Jackson, 1991 . In clustering, we would use a similarity measure that takes missing values into account. In density estimation using an independence assumption, the marginal density estimates will be based on different numbers of samples.   414 Additional topics  3. We may substitute for the missing values and proceed with our analysis as if we had  a complete data set.  There are many approaches to missing value estimation, varying in sophistication and computational complexity. The simplest and perhaps the crudest method is to substitute mean values of the corresponding components. This has been used in many studies. In a supervised classiﬁcation problem, class means may be substituted for the missing values in the training set and the sample mean for the missing values in the test set since in this case we do not know the class. In a clustering problem, we would estimate the missing values of the group of objects to which the object belongs.  A thorough treatment of missing data in statistical analysis is given by Little and Rubin  1987 . A review in the context of regression is given by Little  1992  and a discussion in the classiﬁcation context by Liu et al.  1997 .  11.4 Outlier detection and robust procedures  We now consider the problem of detecting outliers in multivariate data. This is one of the aims of robust statistics. Outliers are observations that are not consistent with the rest of the data. They may be genuine observations  termed discordant observations by Beckman and Cook, 1983  that are surprising to the investigator. Perhaps they may be the most valuable, indicating certain structure in the data that shows deviations from normality. Alternatively, outliers may be contaminants, errors caused by copying and transferring the data. In this situation, it may be possible to examine the original data source and correct for any transcription errors.  In both of the above cases, it is important to detect the outliers and to treat them appropriately. Many of the techniques we have discussed in this book are sensitive to outlying values. If the observations are atypical on a single variable, it may be possible to apply univariate methods to the variable. Outliers in multivariate observations can be difﬁcult to detect, particularly when there are several outliers present. A classical procedure is to compute the Mahalanobis distance for each sample xi .i D 1; : : : ; n   Di D n  .xi  cid:1  m T O cid:5    cid:1 1  .xi  cid:1  m   o 1  2  where m is the sample mean and O cid:5  the sample covariance matrix. Outliers may be iden- tiﬁed as those samples yielding large values of the Mahalanobis distance. This approach suffers from two problems in practice: 1. Masking. Multiple outliers in a cluster will distort m and O cid:5 , attracting m and inﬂating  O cid:5  in their direction, thus giving lower values for the Mahalanobis distance.  2. Swamping. This refers to the effect that a cluster of outliers may have on some obser- vations that are consistent with the majority. The cluster could cause the covariance matrix to be distorted so that high values of D are found for observations that are not outliers.   Mixed continuous and discrete variables 415  One way of overcoming these problems is to use robust estimates for the mean and covariance matrices. Different estimators have different breakdown points, the fraction of outliers that they can tolerate. Rousseeuw  1985  has proposed a minimum volume ellipsoid  MVE  estimator that has a high breakdown point of approximately 50%. It can be computationally expensive, but approximate algorithms have been proposed.  Outlier detection and robust procedures have represented an important area of re- search, investigated extensively in the statistical literature. Chapter 1 of Hampel et al.  1986  gives a good introduction to and background on robust procedures. Robust esti- mates of mean and covariance matrices are reviewed by Rocke and Woodruff  1997 ; further procedures for the detection of outliers in the presence of appreciable masking are given by Atkinson and Mulira  1993 . Krusi´nska  1988  reviews robust methods within the context of discrimination.  11.5 Mixed continuous and discrete variables  In many areas of pattern recognition involving multivariate data, the variables may be of mixed type, comprising perhaps continuous, ordered categorical, unordered categorical and binary variables. If the discrete variable is ordered, and it is important to retain this information, then the simplest approach is to treat the variable as continuous and to use a technique developed for multivariate continuous data. Alternatively, a categorical variable with k states can be coded as k  cid:1  1 dummy binary variables. All take the value zero except the jth if the observed categorical variable is in the jth state, j D 1; : : : ; k cid:1 1. All are zero if the variable is in the kth state. This allows some of the techniques developed for mixtures of binary and continuous variables to be used  with some modiﬁcations, since not all combinations of binary variables are observable .  The above approaches attempt to distort the data to ﬁt the model. Alternatively, we may apply existing methods to mixed variable data with little or no modiﬁcation. These include:  1. nearest-neighbour methods  Chapter 3  with a suitable choice of metric;  2. independence models where each univariate density estimate is chosen to be appro-  priate for the particular variable  Chapter 3 ;  3. kernel methods using product kernels, where the choice of kernel depends on the  variable type  Chapter 3 ;  4. dependence tree models and Bayesian networks, where the conditional densities are  modelled appropriately  for example, using product kernels   Chapter 3 ;  5. recursive partitioning methods such as CART and MARS  Chapter 7 .  The location model, introduced by Olkin and Tate  1961 , was developed speciﬁcally with mixed variables in mind and applied to discriminant analysis by Chang and Aﬁﬁ  1974 . Consider the problem of classifying a vector v which may be partitioned into two parts, v D .zT ; yT  T , where z is a vector of r binary variables and y is a vector of p   416 Additional topics  continuous variables. The random vector z gives rise to 2r different cells. We may order the cells such that, given a measurement z, the cell number of z, cell.z , is given by  cell.z  D 1 C rX iD1  zi 2i cid:1 1  The location model assumes a multivariate normal distribution for y, whose mean de- pends on the state of z and the class from which v is drawn. It also assumes that the covariance matrix is the same for both classes and for all states. Thus, given a measure- ment .z; y  such that m D cell.z , the probability density for class !i  i D 1; 2  is  p.yjz  D  1  .2³   p=2þþ cid:5 þþ exp  .y  cid:1  mm  i   T  cid:5    cid:1 1.y  cid:1  mm     i  ²  cid:1  1 2  ¦  Thus, the means of the distribution depend on z and the class. Then, if the probability of observing z in cell m for class !i is pim, then v may be assigned to !1 if  .mm  1  cid:1  mm  2   T  cid:5   y  cid:1  1  2  .mm  1 C mm  2      cid:1 1  cid:8    cid:9  ½ log. p2m = p1m    The maximum likelihood estimates for the parameters pim, mm  i and  cid:5  are  m  Opim D nim n i D nX Om jD1 2X  cid:5  D 1 n iD1  vim j  y j  1 nim kX nX jD1 mD1  vim j .y j  cid:1  mm  i   .y j  cid:1  mm  i   T  where vim j D 1 if y j is in cell m of class !i , 0 otherwise; and nim is the number of observations in cell m of class !i equal to P  ¹im j .  If the sample size n is very large relative to the number of cells, then these na¨ıve estimates may be sufﬁcient. However, in practice there will be too many parameters to estimate. Some of the cells may not be populated, giving poor estimates for Opim. There have been several developments of the basic approach. For a review, see Krzanowski  1993 .  j  11.6 Structural risk minimisation and the  Vapnik–Chervonenkis dimension  11.6.1 Bounds on the expected risk  There are general bounds in statistical learning theory  Vapnik, 1998  that govern the relationship between the capacity of a learning system and its performance and thus can provide some guidance in the design of such systems.   Structural risk minimisation and the Vapnik–Chervonenkis dimension 417  We assume that we have a training set of independently and identically distributed samples f.xi ; yi  ; i D 1; : : : ; ng drawn from a distribution p.x; y . We wish to learn a mapping x ! y and we have a set of possible functions  classiﬁers  indexed by parameters α, namely f .x; α . A particular choice for α results in a particular classiﬁer or trained machine. We would like to choose α to minimise the classiﬁcation error. If yi takes the value C1 for patterns in class !1 and  cid:1 1 for patterns in class !2, then the expected value of the test error  the true error–see Chapter 8  is  Z  R.α  D 1 2  jy  cid:1  f .x; α j p.x; y  dx dy  This is sometimes termed the expected risk  note that differs from the deﬁnition of risk in Chapter 1 . This is not known in general, but we may estimate it based on a training set, to give the empirical risk,  RK .α  D 1 2n  nX iD1  jyi  cid:1  f .xi ; α j  For any value of  cid:13 , 0  cid:12   cid:13   cid:12  1, the following bound of statistical learning theory holds  Vapnik, 1998   r h.log.2n= h  C 1   cid:1  log. cid:13 =4   R.α   cid:12  RK C   11.2   n  with probability 1 cid:1   cid:13 , where h is a non-negative integer called the Vapnik–Chervonenkis  VC  dimension. The ﬁrst term on the right-hand side of the inequality above depends on the particular function f chosen by the training procedure. The second term, the VC conﬁdence, depends on the class of functions.  11.6.2 The Vapnik–Chervonenkis dimension  The VC dimension is a property of the set of functions f .x; α . If a given set of m points can be labelled in all possible 2m ways using the functions f .x; α , then the set of points is said to be shattered by the set of functions; that is, for any labelling of the set of points, there is a function f .x; α  that correctly classiﬁes the patterns.  The VC dimension of a set of functions is deﬁned as the maximum number of training points that can be shattered. Note that if the VC dimension is m, then there is at least one set of m points that can be shattered, but not necessarily every set of m points can be shattered. For example, if f .x; α  is the set of all lines in the plane, then every set of two points can be shattered, and most sets of three  see Figure 11.1 , but no sets of four points can be shattered by a linear model. Thus the VC dimension is 3. More generally, the VC dimension of a set of hyperplanes in r-dimensional Euclidean space is r C 1.  Inequality  11.2  shows that the risk may be controlled through a balance of optimising a ﬁt to the data and the capacity of functions used in learning. In practice, we would consider sets of models, f , with each set of a ﬁxed VC dimension. For each set, minimise the empirical risk and choose the model over all sets for which the sum of the empirical risk and VC conﬁdence is a minimum. However, the inequality  11.2  is only a guide. There may be models with equal empirical risk but with different VC dimensions. The   418 Additional topics  Š  ž  Š  ž   cid:1  cid:1    cid:1  ž   cid:2  cid:2  cid:4   cid:1   cid:1    cid:1  cid:1   cid:1   cid:2  cid:2  cid:3   cid:1  Š   cid:1   ž  cid:5  cid:5   cid:5   cid:6  cid:6  cid:8   cid:5  Š   cid:5  cid:5   Š  cid:5  ž  ž   cid:5    cid:6  cid:6  cid:7   cid:5   cid:5   Š  ž  Š  Š  ž   cid:10   Š   cid:9   ž  ž  Š  ž  Š   cid:12   ž   cid:11  Š  Figure 11.1 Shattering three points in two dimensions  one with higher VC dimension does not necessarily have poorer performance. A k- nearest-neighbour classiﬁer has zero empirical risk  any labelling of a set of points will be correctly classiﬁed  and inﬁnite VC dimension.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   A  Measures of dissimilarity  A.1 Measures of dissimilarity  Patterns or objects analysed using the techniques described in this book are usually represented by a vector of measurements. Many of the techniques require some measure of dissimilarity or distance between two pattern vectors, although sometimes data can arise directly in the form of a dissimilarity matrix.  A particular class of dissimilarity functions called dissimilarity coefﬁcients are re- quired to satisfy the following conditions. If drs is the dissimilarity of object s from object r, then  drs ½ 0 drr D 0 drs D dsr  for every r; s for every r for every r; s  The symmetry condition is not always satisﬁed by some dissimilarity functions. If the dissimilarity between two places in a city centre is the distance travelled by road between them, then because of one-way systems the distance may be longer in one direction than the other. Measures of dissimilarity can be transformed to similarity measures using various transformations, for example, si j D 1=.1 C di j   or si j D c  cid:4  di j for some constant c, where si j is the similarity between object i and object j.  If, in addition to the three conditions above, the dissimilarity measure satisﬁes the  triangle inequality  drt C dts ½ drs  for every r; s; t   A.1   then the dissimilarity measure is a metric and the term distance is usually used.  A.1.1 Numeric variables  Many dissimilarity measures have been proposed for numeric variables. Table A.1 gives some of the more common measures. The choice of a particular metric depends on the application. Computational considerations aside, for feature selection and extraction purposes you would choose the metric that gives the best performance  perhaps in terms of classiﬁcation error on a validation set .   420 Measures of dissimilarity  Table A.1 Dissimilarity measures for numeric variables  between x and y   Dissimilarity measure  Mathematical form  Euclidean distance  City-block distance  Chebyshev distance  Minkowski distance of order m  Canberra distance  Nonlinear distance  Angular separation    1  2  .xi  cid:4  yi  2  jxi  cid:4  yij jxi  cid:4  yij    1  m  .xi  cid:4  yi  m    pX de D iD1 dcb D pX iD1 dch D max   pX iD1 pX dq D pX iD1 jD1  dM D  i  Q positive deﬁnite jxi  cid:4  yij dca D pX xi C yi iD1   H de > D de  cid:6  D 0 P p iD1 x 2 i  iD1 xi yi P p  dn D  iD1 y2 i  ðP p  Ł1=2  Quadratic distance  .xi  cid:4  yi  Qi j .x j  cid:4  y j  ,  Euclidean distance  de D  .xi  cid:4  yi  2  vuut pX  iD1  The contours of equal Euclidean distance from a point are hyperspheres  circles in two dimensions . It has the  perhaps undesirable  property of giving greater emphasis to larger differences on a single variable.  Although we may wish to use a dissimilarity measure that is a metric, some of the methods do not require the metric condition  A.1  above. Therefore in some cases a monotonic function of the Euclidean metric, which will still be a dissimilarity coefﬁcient but not necessarily a metric, will sufﬁce. For example, squared Euclidean distance is a dissimilarity coefﬁcient but not a metric.  City-block distance  dcb D pX iD1  jxi  cid:4  yij   Measures of dissimilarity 421  Also known as the Manhattan or box-car or absolute value distance, this metric uses a distance calculation which would be suitable for ﬁnding the distances between points in a city consisting of a grid of intersecting thoroughfares  hence the names used . The contours of equal distance from a point for the city-block metric are diamonds in two dimensions. The city-block metric is a little cheaper to compute than the Euclidean distance so it may be used if the speed of a particular application is important.  Chebyshev distance  dch D max  jxi  cid:4  yij  i  The Chebyshev or maximum value distance is often used in cases where the execution speed is so critical that the time involved in calculating the Euclidean distance is un- acceptable. The Chebyshev distance, like the city-block distance, examines the absolute magnitude of the elementwise differences in the pair of vectors. The contour lines of equal Chebyshev distance from a point are squares in two dimensions. Figure A.1 plots the contours of equal distance in R 2 for the Euclidean, city-block and Chebyshev metrics. If the user needs an approximation to Euclidean distance but with a cheaper computa- tional load then the ﬁrst line of approach is to use either the Chebyshev or the city-block metrics. A better approximation can be gained by using a combination of these two distances:  d D max. 2  3 dcb; dch   In two dimensions the contours of equal distance form octagons.  Minkowski distance The Minkowski distance is a more general form of the Euclidean and city-block distances. The Minkowski distance of order m is  dM D  jxi  cid:4  yijm    pX iD1  !1=m  dch  de  dcb  Figure A.1 Contours of equal distance   422 Measures of dissimilarity  The Minkowski distance of the ﬁrst order is the same as the city-block metric and the Minkowski distance of the second order is the Euclidean distance. The contours of equal distance for such metrics form squared-off circles which gradually obtain more abrupt vertices as m increases. The choice of an appropriate value for m depends on the amount of emphasis you would like to give to the larger differences: larger values of m give progressively more emphasis to the larger differences jxi  cid:4  yij, and as m tends to inﬁnity the metric tends to the Chebyshev distance  and square contours .  Quadratic distance  q D .x  cid:4  y T Q.x  cid:4  y  d2  A choice for Q is the inverse of the within-group covariance matrix. This is sometimes referred to as the Mahalanobis distance, because of the similarity to the distance measure between two distributions  see below .  Canberra metric  dca D pX iD1  jxi  cid:4  yij xi C yi  The Canberra metric is a sum of a series of fractions and is suitable for variables taking non-negative values. If both xi and yi are zero the ratio of the difference to the sum is taken to be zero. If only one value is zero, the term is unity, independent of the other value. Thus, 0 and 1 are equally dissimilar to a pair of elements 0 and 106. Sometimes values of 0 are replaced by small positive numbers  smaller than the recorded values of that variable .  Nonlinear distance  dN D  ² 0 if de.x; y  < D H if de.x; y  ½ D  where D is a threshold and H is a constant. Kittler  1975a  shows that an appropriate choice for H and D for feature selection is that they should satisfy  and that D satisﬁes the unbiasedness and consistency conditions of the Parzen estimator, namely D pn ! 1 and D ! 0 as n ! 1, where n is the number of samples in the data set.  Angular separation  H D 0. p=2  p ³ p D p2  pX iD1 " pX iD1  x 2 i  xi yi pX iD1    y2 i   Measures of dissimilarity 423  The angular separation is a similarity rather than a dissimilarity measure that measures the angle between the unit vectors in the direction of the two pattern vectors of interest. This is appropriate when data are collected for which only the relative magnitudes are important.  The choice of a particular proximity measure depends on the application and may depend on several factors, including distribution of data and computational considerations. It is not possible to make recommendations, and studies in this area have been largely empirical, but the method you choose should be the one that you believe will capture the essential differences between objects.  A.1.2 Nominal and ordinal variables  Nominal and ordinal variables are usually represented as a set of binary variables. For example, a nominal variable with s states is represented as s binary variables. If it is in the mth state, then each of the s binary variables has value 0 except the mth, which has the value unity. The dissimilarity between two objects can be obtained by summing the contributions from the individual variables.  For ordinal variables, the contribution to the dissimilarity between two objects from a single variable does not simply depend on whether or not the values are identical. If the contribution for one variable in state m and one in state l  m < l  is Žml, then we require  Žml ½ Žms Žml ½ Žsl  for s < l for s > m  that is, Žml is monotonic down each row and across each column of the half-matrix of distances between states  Ž14 > Ž13 > Ž12 etc.; Ž14 > Ž24 > Ž34 . The values chosen for Žml depend very much on the problem. For example, we may have a variable describing fruits of a plant that can take the values short, long or very long. We would want the dissimilarity between a plant with very long fruit and one with short fruit to be greater than that between one with long fruit and one with short fruit  all other attributes having equal values . A numeric coding of 1, 2, 3 would achieve this, but so would 1, 10, 100.  A.1.3 Binary variables  Various dissimilarity measures have been proposed for binary variables. For vectors of binary variables x and y these may be expressed in terms of quantities a; b; c, and d where  a is equal to the number of occurrences of xi D 1 and yi D 1 b is equal to the number of occurrences of xi D 0 and yi D 1 c is equal to the number of occurrences of xi D 1 and yi D 0 d is equal to the number of occurrences of xi D 0 and yi D 0  This is summarised in Table A.2. Note that aCbCcCd D p, the total number of variables  attributes . It is customary to deﬁne a similarity measure rather than a dissimilarity measure. Table A.3 summarises some of the more commonly-used similarity measures for binary data.   424 Measures of dissimilarity  A.2 Co-occurrence  Table table for binary variables  xi  1 a c  0 b d  yi  1 0  Table A.3 Similarity measures for binary data  Similarity measure  Simple matching coefﬁcient  Russell and Rao  Jaccard  Czekanowski  aCbCcCd  Mathematical form dsm D aCd drr D aCbCcCd d j D a aCbCc dCz D 2a  2aCbCc  a  Simple matching coefﬁcient The simple matching coefﬁcient is the proportion of variables for which two variables have the same value. The dissatisfaction with this measure has been with the term d representing conjoint absences. The fact that two sites in an ecological survey both lack something should not make them more similar. The dissimilarity measure deﬁned by dx y D 1 cid:4  sx y D .bC c = p is proportional to the square of the Euclidean distance, bCc, which is the Hamming distance in communication theory.  Russell and Rao This does not involve the term d in the numerator and is appropriate in certain circumstances. The quantity 1  cid:4  sx y is not a dissimilarity coefﬁcient since the dissimilarity between an object and itself is not necessarily zero.  Jaccard This does not involve the quantity d at all and is used extensively by ecolo- gists. The term dx y D 1  cid:4  sx y is a metric dissimilarity coefﬁcient.  Czekanowski This is similar to the Jaccard measure except that coincidences carry double weight.  Many other coefﬁcients have been proposed that handle the conjoint absences in  various ways  Clifford and Stephenson, 1975; Diday and Simon, 1976 .  A.1.4 Summary  We have listed some of the measures of proximity which can be found in the pat- tern processing and classiﬁcation literature. A general similarity coefﬁcient between two objects x and y encompassing variables of mixed type has been proposed by Gower   Distances between distributions 425   1971 . Of course, there is no such thing as a best measure. Some will be more appropriate for certain tasks than others. Therefore, we cannot make recommendations. However, the user should consider the following points when making a choice:  1  simplicity and ease of understanding;  2  ease of implementation;  3  speed requirements;  4  knowledge of data.  A.2 Distances between distributions  All of the distance measures described so far have been deﬁned between two patterns or objects. We now turn to measures of distances between groups of objects or distributions. These measures are used to determine the discriminatory power of a feature set, discussed in Chapter 9. Many measures have been proposed in the pattern recognition literature, and we introduce two basic types here. The ﬁrst uses prototype vectors for each class together with the distance metrics of the previous section. The second uses knowledge of the class-conditional probability density functions. Many methods of this type are of academic interest only. Their practical application is rather limited since they involve numerical integration and estimation of the probability density functions from samples. They do simplify if the density functions belong to a family of parametric functions such as the exponential family, which includes the normal distribution. The use of both of these approaches for feature selection is described in Chapter 9.  A.2.1 Methods based on prototype vectors  There are many measures of inter-group dissimilarities based on prototype vectors. In the context of clustering, these give rise to different hierarchical schemes, which are discussed in Chapter 10. Here we introduce the average separation, deﬁned to be the average distance between all pairs of points, with one point in each pair coming from each distribution. That is, for n1 points in !1  xi ; i D 1; : : : ; n1  and n2 points in !2  yi ; i D 1; : : : ; n2 ,  Jas.!1; !2  D 1 n1n2  n1X iD1  n2X jD1  d.xi ; y j    where d is a distance between xi and y j .  A.2.2 Methods based on probabilistic distance  These measures use the complete information about the structure of the classes provided by the conditional densities. The distance measure, J , satisﬁes the following conditions: 1. J D 0 if the probability density functions are identical, p.xj!1  D p.xj!2 ; 2. J ½ 0;   426 Measures of dissimilarity  3. J attains its maximum when the classes are disjoint, i.e. when p.xj!1  D 0 and  p.xj!2  6D 0.  Many measures satisfying these conditions have been proposed  Chen, 1976; Devijver and Kittler, 1982 . As an introduction, consider two overlapping distributions with con- ditional densities p.xj!1  and p.xj!2 . The classiﬁcation error, e  see Chapter 8 , is given by  Z  ²  1  cid:4   e D 1 2  j p.!1jx   cid:4  p.!2jx j p.x  dx  ¦  The integral in the equation above,  Z  JK D  j p.!1jx   cid:4  p.!2jx j p.x  dx  is called the Kolmogorov variational distance and has the important property that it is directly related to the classiﬁcation error. Other measures cannot be expressed in terms of the classiﬁcation error, but can be used to provide bounds on the error. Three of these are given in Table A.4. A more complete list can be found in the books by Chen  1976  and Devijver and Kittler  1982 .  One of the main disadvantages of the probabilistic dependence criteria is that they require an estimate of a probability density function and its numerical integration. This restricts their usefulness in many practical situations. However, under certain assumptions regarding the form of the distributions, the expressions can be evaluated analytically.  First of all, we shall consider a speciﬁc parametric form for the distributions, namely normally distributed with means µ1 and µ2 and covariance matrices  cid:5 1 and  cid:5 2. Under these assumptions, the distance measures can be written down as follows.  Table A.4 Probabilistic distance measures  Dissimilarity measure  Mathematical form  Average separation  Chernoff  Bhattacharyya  Divergence  1  nanb  naX iD1  nbX jD1  d.xi ; y j  ; xi 2 !AI y j 2 !BI  d any distance metric Jc D  cid:4 log  Z  ps .xj!1  p1 cid:4 s .xj!2  dx  Z  JB D  cid:4 log  . p.xj!1  p.xj!2    1 2 dx  JD D  [ p.xj!1   cid:4  p.xj!2 ]log  Z  ²Z   cid:18   dx   cid:17  p.xj!1  p.xj!2  ¦ 1  2  Patrick–Fischer  JP D  [ p.xj!1  p1  cid:4  p.xj!2  p2]2 dx   Distances between distributions 427  Chernoff  Jc D 1 2  s.1  cid:4  s .µ2  cid:4  µ1 T [ cid:5 s] cid:4 1.µ2  cid:4  µ1  C 1 2  log   cid:17   j cid:5 sj  j cid:5 1j1 cid:4 sj cid:5 2js   cid:18   where  cid:5 s D .1  cid:4  s  cid:5 1 C s cid:5 2 and s 2 [0; 1]. For s D 0:5, we have the Bhattacharyya distance.  JB D 1 4  .µ2  cid:4  µ1 T [ cid:5 1 C  cid:5 2] cid:4 1 .µ2  cid:4  µ1  C 1 2  log  0 @ j cid:5 1 C  cid:5 2j 2.j cid:5 1jj cid:5 2j   1 2  1 A  Bhattacharyya  Divergence  JD D 1  2  .µ2  cid:4  µ1 T . cid:5    cid:4 1 1 C  cid:5    cid:4 1 2   .µ2  cid:4  µ1  C Trf cid:5    cid:4 1 1  cid:5 2 C  cid:5    cid:4 1 1  cid:5 2  cid:4  2Ig  Patrick–Fischer   cid:23   JP D.2³   cid:4  p=2  j2 cid:5 1j cid:4  1  2 C j2 cid:5 2j cid:4  1  2   cid:4 2j cid:5 1 C  cid:5 2j cid:4  1  2 exp  .µ2  cid:4  µ1 T . cid:5 1 C  cid:5 2  cid:4 1.µ2  cid:4  µ1   ¦½  ²   cid:4  1 2  Finally, if the covariance matrices are equal,  cid:5 1 D  cid:5 2 D  cid:5 , the Bhattacharyya and  divergence distances simplify to  JM D JD D 8JB D .µ2  cid:4  µ1 T  cid:5    cid:4 1.µ2  cid:4  µ1   which is the Mahalanobis distance.  Of course, the means and covariance matrices are not known in practice and must be  estimated from the available training data.  The above parametric forms are useful both in feature selection and extraction. In feature selection, the set of features at the kth stage of an algorithm is constructed from the set of features at the  k  cid:4  1 th stage by the addition or subtraction of a small number of features. The value of the feature selection criterion at stage k C 1 may be computed from that at stage k rather than evaluating the above expressions directly. This saves on computation. Recursive calculation of separability measures is discussed in Chapter 9.  Probabilistic distance measures can also be extended to the multigroup case by eval-  uating all pairwise distances between classes, CX jD1  J D CX iD1  pi p j Ji j  where Ji j is the chosen distance measure evaluated for class !i and class ! j .   428 Measures of dissimilarity  A.2.3 Probabilistic dependence  The probabilistic distance measures are based on discrimination between a pair of classes, using the class-conditional densities to describe each class. Probabilistic dependence measures are multiclass feature selection criteria that measure the distance between the class-conditional density and the mixture probability density function  see Figure A.2 . If p.xj!i   and p.x  are identical then we gain no information about class by observing x, and the ‘distance’ between the two distributions is zero. Thus, x and !i are independent. If the distance between p.xj!i   and p.x  is large, then the observation x is dependent on !i . The greater the distance, the greater the dependence of x on the class !i . Table A.5  p xw1   0.4  0.35  0.3  0.25  0.2  0.15  0.1  0.05  p x   0  −4  −3  −2  −1  1  2  3  4  0 x  Figure A.2 Probabilistic dependence  Table A.5 Probabilistic dependence measures  Dissimilarity measure  Mathematical form  Chernoff  Bhattacharyya  Joshi  Patrick–Fischer  Jc D CX iD1 JB D CX iD1 JD D CX iD1 JP D CX iD1  pi  pi  pi  pi  Z  ²Z  ²  cid:4 log ²  cid:4 log  Z  Z  ¦  ps .xj!i   p1 cid:4 s .x  dx ¦  . p.xj!i   p.x    1 2 dx  [ p.xj!i    cid:4  p.x ]log   cid:17  p.xj!i     cid:18   dx  p.x  ¦ 1  2  [ p.xj!i    cid:4  p.x ]2 dx   Discussion 429  gives the probabilistic dependence measures corresponding to the probabilistic distance measures in Table A.4. In practice, application of probabilistic dependence measures is limited because, even for normally distributed classes, the expressions given in Table A.5 cannot be evaluated analytically since the mixture distribution p.x  is not normal.  A.3 Discussion  This appendix has reviewed some of the distance and dissimilarity measures used in Chapter 9 on feature selection and extraction and Chapter 10 on clustering. Of course the list is not exhaustive and those that are presented may not be the best for your problem. We cannot make rigid recommendations as to which ones you should use since the choice is highly problem-speciﬁc. However, it may be advantageous from a computational point of view to use one that simpliﬁes for normal distributions even if your data are not normally distributed.  The book by Gordon  1999  provides a good introduction to classiﬁcation methods. The chapter on dissimilarity measures also highlights difﬁculties encountered in practice with real data sets. There are many other books and papers on clustering which list other measures of dissimilarity: for example, Diday and Simon  1976 , Cormack  1971  and Clifford and Stephenson  1975 , which is written primarily for biologists but the issues treated occur in many areas of scientiﬁc endeavour. The papers by Kittler  1975b, 1986  provide very good introductions to feature selection and list some of the more commonly used distance measures. Others may be found in Chen  1976 . A good account of probabilistic distance and dependence measures can be found in the book by Devijver and Kittler  1982 .   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   B  Parameter estimation  B.1 Parameter estimation  B.1.1 Properties of estimators  Perhaps before we begin to discuss some of the desirable properties of estimators, we ought to deﬁne what an estimator is. For example, in a measurement experiment we may assume that the observations are normally distributed but with unknown mean, ¼, and variance, ¦ 2. The problem then is to estimate the values of these two parameters from the set of observations. Therefore, an estimator O cid:3  of  cid:3  is deﬁned as any function of the sample values which is calculated to be close in some sense to the true value of the unknown parameter  cid:3 . This is a problem in point estimation, by which is meant deriving a single-valued function of a set of observations to represent the unknown parameter  or a function of the unknown parameters , without explicitly stating the precision of the estimate. The estimation of the conﬁdence interval  the limits within which we expect the parameter to lie  is an exercise in interval estimation. For a detailed treatment of estimation we refer to Stuart and Ord  1991 . Unbiased estimate The estimator O cid:3  of the parameter  cid:3  is unbiased if the expectation over the sampling distribution is equal to  cid:3 , i.e.  E[O cid:3 ]  4D  Z O cid:3  p.x1; : : : ; xn  dx1 : : : dxn D  cid:3   where O cid:3  is a function of the sample vectors x1; : : : ; xn drawn from the distribution p.x1; : : : ; xn  . We might always want estimators to be approximately unbiased, but there is no reason why we should insist on exact unbiasedness. Consistent estimate An estimator O cid:3  of a parameter  cid:3  is consistent if it converges in probability  or converges stochastically  to  cid:3  as the number of observations, n ! 1. That is, for all Ž; ž > 0  p.jjO cid:3   cid:7   cid:3 jj   1  cid:7  Ž  for n > n0  or  n!1 p.jjO cid:3   cid:7   cid:3 jj > ž  D 0  lim   432 Parameter estimation  Efﬁcient estimate The efﬁciency,  cid:12 , of one estimator O cid:3 2 relative to another O cid:3 1 is deﬁned as the ratio of the variance of the estimators   cid:12  D E[jjO cid:3 1  cid:7   cid:3 jj2] E[jjO cid:3 2  cid:7   cid:3 jj2]  O cid:3 1 is an efﬁcient estimator if it has the smallest variance  in large samples  compared to all other estimators, i.e.  cid:12   cid:8  1 for all O cid:3 2. Sufﬁcient estimate A statistic O cid:3 1 D O cid:3 1.x1; : : : ; xn  is termed a sufﬁcient statistic if, for any other statistic O cid:3 2,   B.1  that is, all the relevant information for the estimation of  cid:3  is contained in O cid:3 1 and the addi- tional knowledge of O cid:3 2 makes no contribution. An equivalent condition for a distribution to possess a sufﬁcient statistic is the factorability of the likelihood function  Stuart and Ord, 1991; Young and Calvert, 1974 :  p. cid:3 jO cid:3 1; O cid:3 2  D p. cid:3 jO cid:3 1   p.x1; : : : ; xnj cid:3    D g.O cid:3 j cid:3   h.x1; : : : ; xn    B.2  where h is a function of x1; : : : ; xn and is essentially p.x1; : : : ; xnjO cid:3    and does not de- pend on  cid:3 , and g is a function of the statistic O cid:3  and  cid:3 . Equation  B.2  is also the condition for reproducing densities  Spragins, 1976; Young and Calvert, 1974  or conjugate priors  Lindgren, 1976 : a probability density of  cid:3 , p. cid:3   , is a reproducing density with respect to the conditional density p.x1; : : : ; xnj cid:3    if the posterior density p. cid:3 jx1; : : : ; xn  and the prior density p. cid:3    are of the same functional form. The family is called closed un- der sampling or conjugate with respect to p.x1; : : : ; xnj cid:3   . Conditional densities that admit sufﬁcient statistics of ﬁxed dimension for any sample size  and hence reproducing densities  include the normal, binomial and Poisson density functions  Spragins, 1976 . Example 1 The sample mean x n D 1 tion mean, ¼, since  iD1 xi is an unbiased estimator of the popula-  Pn  n  E[x n] D E  "    1 n  nX iD1  xi  D 1 n  nX iD1  E[xi ] D ¼  but the sample variance  "  E  1 n  nX iD1    .xi  cid:7  x n  2  D E  xi  cid:7  1 n  nX jD1 j  cid:7  1 x 2 n  nX jD1  !23 5  x j    x j xk  X  j  X k;k6D j  2 4 1 n "     nX iD1 n  cid:7  1 n  ¦ 2  E  D 1 n D n  cid:7  1  n   Parameter estimation 433  is not an unbiased estimator of the variance ¦ 2. Therefore, the unbiased estimator  s D 1 n  cid:7  1  nX iD1  .xi  cid:7  x n 2  is usually preferred.   cid:1   Example 2 The sample mean is a consistent estimator of the mean of a normal popu- lation. The sample mean is normally distributed as  p.x n  D  cid:12  n   cid:13  1 2 exp  2³   cid:14   cid:7  1 2   cid:15   n.x n  cid:7   cid:3   2  with mean  cid:3   the mean of the population with unit variance  and variance 1=n. That is, .x n cid:7  cid:3   n 1 2 is normally distributed with zero mean and unit variance. Thus, the probability that j.x n cid:7  cid:3   n 2j  cid:8  žn 2  i.e. j.x n cid:7  cid:3   nj  cid:8  žn  is the value of the normal integral between the limits šžn 2 . By choosing n sufﬁciently large, this can always be larger than 1  cid:7   cid:12  for any given  cid:12 .   cid:1   1  1  1  B.1.2 Maximum likelihood  The likelihood function is the joint density of a set of samples x1; : : : ; xn from a distri- bution p.xij cid:3   , i.e.  L. cid:3    D p.x1; : : : ; xnj cid:3     regarded as a function of the parameters,  cid:3 , rather than the data samples. The method of maximum likelihood is a general method of point estimation in which the estimate of the parameter  cid:3  is taken to be that value for which L. cid:3    is a maximum. That is, we are choosing the value of  cid:3  which is ‘most likely to give rise to the observed data’. Thus, we seek a solution to the equation  or, equivalently,  since any monotonic function of the likelihood, L, will also be a minimum at the same value of  cid:3  as the function L. Under very general conditions  Stuart and Ord, 1991, Chapter 18 , the maximum likelihood estimator is consistent, asymptotically normal and asymptotically efﬁcient. The estimator is not, in general, unbiased though it will be asymptotically unbiased if it is consistent and if the asymptotic distribution has ﬁnite mean. However, for an unbiased estimator, O cid:3 , of a parameter  cid:3 , the lower bound on the variance is given by the Cram´er–Rao bound  @ L @ cid:3   D 0  @ log.L   D 0  @ cid:3   E[.O cid:3   cid:7   cid:3   2] D 1  ¦ 2 n   434 Parameter estimation  where  n D E ¦ 2  " cid:14  @  @ cid:3   log[ p.x1; : : : ; xnj cid:3   ]   cid:15 2  is called the Fisher information in the sample. It follows from the deﬁnition of the efﬁcient estimator that any unbiased estimator that satisﬁes this bound is efﬁcient.  B.1.3 Problems with maximum likelihood  The main difﬁculty with maximum likelihood parameter estimation is obtaining a solution of the equations  D 0  @ L @θ  for a vector of parameters, θ. Unlike in the normal case, these are not always tractable and iterative techniques must be employed. These may be a nonlinear optimisation scheme using gradient information such as conjugate gradient methods or quasi-Newton meth- ods or, for certain parametric forms of the likelihood function, expectation–maximisation  EM  methods may be employed. The problem with the former approach is that maximi- sation of the likelihood function is not simply an exercise in unconstrained optimisation. That is, the quantities being estimated are often required to satisfy some  inequality  constraint. For example, in estimating the covariance matrix of a normal distribution, the elements of the matrix are constrained so that they satisfy the requirements of a covari- ance matrix  positive deﬁniteness, symmetry . The latter methods have been shown to converge for particular likelihood functions.  Maximum likelihood is the most extensively used statistical estimation technique. It may be regarded as an approximation to the Bayesian approach, described below, in which the prior probability, p. cid:3   , is assumed uniform, and is arguably more appealing since it has no subjective or uncertain element represented by p. cid:3   . However, the Bayesian approach is in more agreement with the foundations of probability theory. A detailed treatment of maximum likelihood estimation may be found in Stuart and Ord  1991 .  B.1.4 Bayesian estimates  The maximum likelihood method is a method of point estimation of an unknown param- eter  cid:3 . It may be considered to be an approximation to the Bayesian method described in this section and is used when one has no prior knowledge concerning the distribution of  cid:3 . Given a set of observations x1; : : : ; xn, the probability of obtaining fxig under the assumption that the probability density is p.xj cid:3    is p.x1; : : : ; xnj cid:3    D nY iD1  p.xij cid:3     if the xi are independent. By Bayes’ theorem, we may write the distribution of the parameter  cid:3  as  p. cid:3 jx1; : : : ; xn  D  p.x1; : : : ; xnj cid:3    p. cid:3    R p.x1; : : : ; xnj cid:3    p. cid:3    d cid:3    Parameter estimation 435  where p. cid:3    is the prior probability density of  cid:3 . The quantity above is the probability density of the parameter, given the data samples and is termed the posterior density. Given this quantity, how can we choose a single estimate for the parameter  assuming that we wish to ? There are obviously many ways in which the distribution could be used to generate a single value. For example, we could take the median or the mean as estimates for the parameter. Alternatively, we could select that value of  cid:3  for which the distribution is a maximum:  O cid:3  D arg max   cid:3   p. cid:3 jx1; : : : ; xn    B.3   the value of  cid:3  which occurs with greatest probability. This is termed the maximum a posteriori  MAP  estimate or the Bayesian estimate.  One of the problems with this estimate is that it assumes knowledge of p. cid:3   , the prior probability of  cid:3 . Several reasons may be invoked for neglecting this term. For example, p. cid:3    may be assumed to be uniform  though why should p. cid:3    be uniform on a scale of  cid:3  rather than, say, a scale of  cid:3  2? . Nevertheless, if it can be neglected, then the value of  cid:3  which maximises p. cid:3 jx1; : : : ; xn   is equal to the value of  cid:3  which maximises  p.x1; : : : ; xnj cid:3     This is the maximum likelihood estimate described earlier.  More generally, the Bayes estimate is that value of the parameter that minimises the  Bayes risk or average risk, R, deﬁned by  R D E[C. cid:3 ; O cid:3   ] D  Z  C. cid:3 ; O cid:3    p.x1; : : : ; xn ;  cid:3    dx1 : : : dxn d cid:3    B.4   where C. cid:3 ; O cid:3    is a loss function which depends on the true value  cid:3  of a parameter and its estimate, O cid:3 . Two particular forms for C. cid:3 ; O cid:3    are the quadratic and the uniform loss functions.  The Bayes estimate for the quadratic loss function  minimum mean square estimate   C. cid:3 ; O cid:3    D jj cid:3   cid:7  O cid:3 jj2  is the conditional expectation  expected error of the a posteriori density   Young and Calvert, 1974   O cid:3  D E cid:3 jx1;:::;xn [ cid:3 ] D   cid:3  p. cid:3 jx1; : : : ; xn  d cid:3   Z   B.5   This is also true for cost functions that are symmetric functions of  cid:3  and convex, and if the posterior density is symmetric about its mean. Choosing a uniform loss function  C. cid:3 ; O cid:3    D  ² 0 j cid:3   cid:7  O cid:3 j  cid:8  Ž 1 j cid:3   cid:7  O cid:3 j > Ž  leads to the maximum a posteriori estimate derived above as Ž ! 0.  Minimising the Bayes risk  B.4  yields a single estimate for  cid:3  and if we are only interested in obtaining a single estimate for  cid:3 , then the Bayes estimate is one we might consider.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   C  Linear algebra  C.1 Basic properties and deﬁnitions  Throughout this book, we assume that the reader is familiar with standard operations on vectors and matrices. This appendix is included as a reference for terminology and notation used elsewhere in the book. All of the properties of matrices that we give are stated without proof. Proofs may be found in any good book on elementary linear algebra  see Section C.2 . We also provide information on sources for software for some of the standard matrix operations. Given an n ð m matrix, A, we denote the element of the ith row and jth column by  ai j . The transpose of A is denoted AT , and we note that  The square matrix A is symmetric if ai j D a ji for all i; j. Symmetric matrices occur frequently in this book. The trace of a square matrix A, denoted TrfAg, is the sum of its diagonal elements,  .AB T D B T AT  TrfAg D nX iD1  aii  and satisﬁes TrfABg D TrfBAg provided that AB is a square matrix, though neither A nor B need be square.  The determinant of a matrix A, written jAj, is the sum  jAj D nX jD1  ai j Ai j  for i D 1; : : : ; n  where the cofactor, Ai j , is the determinant of the matrix formed by deleting the ith row and the jth column of A, multiplied by . cid:6 1 iC j . The matrix of cofactors, C  ci j D Ai j  , is called the adjoint of A. If A and B are square matrices of the same order, then jABj D jAjjBj  The inverse of a matrix A is that unique matrix A cid:6 1 with elements such that   cid:6 1A D AA A   cid:6 1 D I   438 Linear algebra  where I is the identity matrix. If the inverse exists, the matrix is said to be nonsingular. If the inverse does not exist, the matrix is singular and jAj D 0. We shall frequently use the properties .AT   cid:6 1 D .A cid:6 1 T , .AB  cid:6 1 D B cid:6 1A cid:6 1 and if A is symmetric, then so is A cid:6 1.  A set of k vectors  of equal dimension  are linearly dependent if there exists a set of  scalars c1; : : : ; ck, not all zero, such that  c1x1 C ÐÐÐ C ck xk D 0  If it is impossible to ﬁnd such a set c1; : : : ; ck then the vectors x1; : : : ; xk are said to be linearly independent. The rank of a matrix is the maximum number of linearly inde- pendent rows  or equivalently, the maximum number of linearly independent columns . An n ð n matrix is of full rank if the rank is equal to n. In this case, the determi- nant is non-zero, i.e. the inverse exists. For a rectangular matrix A of order m ð n, rank.A   cid:9  min.m; n  and  rank.A  D rank.AT   D rank.AT A  D rank.AAT    and the rank is unchanged by pre- or post-multiplication of A by a nonsingular matrix.  A square matrix is orthogonal if  AAT D AT A D I ;  that is, the rows and the columns of A are orthonormal  xT y D 0 and xT x D 1, yT y D 1 for two different columns x and y . An orthogonal matrix represents a linear transfor- mation that preserves distances and angles, consisting of a rotation and or reﬂection. It is clear from the above deﬁnition that an orthogonal matrix is nonsingular and the in- verse of an orthogonal matrix is its transpose: A cid:6 1 D AT . Also the determinant of an orthogonal matrix is š1   cid:6 1 indicates a reﬂection, C1 is a pure rotation . A square matrix A is positive deﬁnite if the quadratic form xT Ax > 0 for all x 6D 0. The matrix is positive semideﬁnite if xT Ax ½ 0 for all x 6D 0. Positive deﬁnite matrices are of full rank. The eigenvalues  or characteristic roots  of a p ð p matrix A are solutions of the  characteristic equation  jA  cid:6  ½Ij D 0  which is a pth-order polynomial in ½. Thus, there are p solutions, which we denote ½1; : : : ; ½ p. They are not necessarily distinct and may be real or complex. Associated with each eigenvalue ½i is an eigenvector ui with the property  Aui D ½i ui  These are not unique, since any scalar multiple of ui also satisﬁes the above equation. i ui D 1. Therefore, the eigenvectors are usually normalised so that uT 1. The product of the eigenvalues is equal to the determinant, i.e. Q p iD1  In this book we use the following properties of eigenvalues and eigenvectors:  ½i D jAj. Thus,  it follows that if the eigenvalues are all non-zero, then the inverse of A exists.   Basic properties and deﬁnitions 439  2. The sum of the eigenvalues is equal to the trace of the matrix, P p iD1 3. If A is a real symmetric matrix, the eigenvalues and eigenvectors are all real.  ½i D TrfAg.  4. If A is positive deﬁnite, the eigenvalues are all greater than zero.  5. If A is positive semideﬁnite of rank m, then there will be m non-zero eigenvalues  and p  cid:6  m eigenvalues with the value zero.  6. Every real symmetric matrix has a set of orthonormal characteristic vectors. Thus, the matrix U, whose columns are the eigenvectors of a real symmetric matrix  U D [u1; : : : ; u p] , is orthogonal, U T U D U U T D I and U T AU D  cid:9  where  cid:9  D diag.½1; : : : ; ½ p , the diagonal matrix with diagonal elements the eigenvalues ½i . Al- ternatively, we may write  A D U  cid:9 U T D pX iD1  ½i ui uT i  If A is positive deﬁnite then A cid:6 1 D U  cid:9  cid:6 1U T , where  cid:9  cid:6 1 D diag.1=½1; : : : ; 1=½ p . The general symmetric eigenvector equation, Au D ½Bu  where A and B are real symmetric matrices, arises in linear discriminant analysis  de- scribed in Chapter 4  and other areas of pattern recognition. If B is positive deﬁnite, then the equation above has p eigenvectors, .u1; : : : ; u p , that are orthonormal with respect to B, that is  and consequently  i Bu j D uT  ² 0 i 6D j 1 i D j  i Au j D uT  ² 0 ½ j  i 6D j i D j  These may be written  U T BU D I U T AU D  cid:9   where U D [u1; : : : ; u p] and  cid:9  D diag.½1; : : : ; ½ p .  Many problems in this book involve the minimisation of a squared error measure. The general linear least squares problem may be solved using the singular value decom- position of a matrix. An m ð n matrix A may be written in the form  A D U  cid:10 V T D rX iD1  ¦i ui vT i  where r is the rank of A; U is an mðr matrix with columns u1; : : : ; ur , the left singular vectors and U T U D I r , the r ð r identity matrix; V is an n ð r matrix with columns v1; : : : ; vr , the right singular vectors and V T V D I r also;  cid:10  D diag.¦1; : : : ; ¦r  , the diagonal matrix of singular values ¦i ; i D 1; : : : ; r.   440 Linear algebra  The singular values of A are the square roots of the non-zero eigenvalues of AAT  or AT A. The pseudo-inverse or generalised inverse is the n ð m matrix A†  A† D V  cid:10    cid:6 1U T D rX iD1  1 ¦i  vi uT i   C.1   and the solution for x that minimises the squared error  is given by  jjAx  cid:6  bjj2  x D A†b  If the rank of A is less than n, then there is not a unique solution for x and singular value decomposition delivers the solution with minimum norm.  The pseudo-inverse has the following properties:  AA†A D A A†AA† D A†  .AA† T D AA† .A†A T D A†A  Finally, in this section, we introduce some results about derivatives. We shall denote  the partial derivative operator by  Thus, the derivative of the scalar function f of the vector x is the vector  Similarly, the derivative of a scalar function of a matrix is denoted by the matrix @ f =@A, where   cid:5  @ @x1  D  @ @x  ; : : : ;   cid:5  @ f @x1  D  @ f @x  ; : : : ;   cid:6 T  @ @x p   cid:6 T  @ f @x p  ½   cid:7  @ f @A  i j  D @ f @ai j  In particular, we have @jAj @A  and for a symmetric matrix  D .adjoint of A T D jAj.A   cid:6 1 T if A   cid:6 1 exists  xT Ax D 2Ax  @ @x  Also, an important derivative involving traces of matrices is  .TrfAT MAg  D MA C M T A  @ @A   Notes and references 441  C.2 Notes and references  In this appendix we have introduced the necessary matrix terminology that will be useful throughout most of this book. We have necessarily been brief on detail and proofs of some of the assertions, together with additional explanation, may be found in most books on linear algebra; for example, the book by Stewart  1973  provides a very good introduction to matrix computations. Also, Press et al.  1992  give clear descriptions of the numerical procedures.  The book by Thisted  1988  gives a good introduction to elements of statistical com- puting, including numerical linear algebra and nonlinear optimisation schemes for func- tion minimisation that are required by some pattern processing algorithms.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   D  Data  D.1 Introduction  Many of the methods described in this book for discrimination and classiﬁcation often have been, and no doubt will continue to be, employed in isolation from other essential stages of a statistical investigation. This is partly due to that fact that data are often collected without the involvement of a statistician. Yet it is the early stages of a statistical investigation that are the most important and may prove critical in producing a satisfactory conclusion. These preliminary stages, if carried out carefully, may indeed be sufﬁcient, in many cases, to answer the questions being addressed. The stages of an investigation may be characterised as follows.  1. Formulation of the problem.  2. Data collection. This relates to questions concerning the type of data, the amount, and  the method and cost of collection.  3. Initial examination of the data. Assess the quality of the data and get a feel for its  structure.  4. Data analysis. Apply discrimination and classiﬁcation methods as appropriate.  5. Assessment of the results.  6. Interpretation.  This is necessarily an iterative process. In particular, the interpretation may pose questions for a further study or lead to the conclusion that the original formulation of the problem needs re-examination.  D.2 Formulating the problem  The success of any pattern recognition investigation depends to a large extent on how well the investigator understands the problem. A clear understanding of the aims of the   444 Data  study is essential. It will enable a set of objectives to be speciﬁed in a way that will allow the problem to be formulated in precise statistical terms.  Although certainly the most important part of a study, problem formulation is probably the most difﬁcult. Care must be taken not to be over-ambitious, and the investigator must try to look ahead, beyond the current study, possibly to further investigations, and understand how the results of the present study will be used and what will be the possible consequences of various outcomes.  Thought should be given to the data collection process. In a discrimination problem, priors and costs will need to be speciﬁed or estimated. Often the most important classes are underrepresented.  A great deal can be learned from past approaches to the particular problem under investigation, and related ones. Data from previous studies may be available. Some preliminary experiments with these data may give an indication as to the choice of important variables for measurement. Previous studies may also highlight good and bad strategies to follow.  Another factor to take into account at the planning stage is cost. More data means a greater cost, both in terms of collection and analysis. Taking too many observations, with measurements on every possible variable, is not a good strategy. Cost is also strongly related to performance and a strategy that meets the desired performance level with acceptable cost is required.  Finally, thought must be given to the interpretation and presentation of the results to the user. For example, in a discrimination problem, it may be inappropriate to use the classiﬁer giving the best results if it is unable to ‘explain’ its decisions. A suboptimal classiﬁer that is easier to understand may be preferable.  D.3 Data collection  In a pattern recognition problem in which a classiﬁer is being designed to automate some particular task, it is important that the data collected are representative of the expected operating conditions. If this is not so, then you must say how and why the data differ.  There are several aspects of data collection that must be addressed. These include the collection of calibration or ‘ground truth’ data; the variables measured and their accuracy; the sampling strategy, including the total sample size and the proportions within each class  if sampling from each class separately ; the costs involved; and the principle of randomisation.  When collecting data it is important to record details of the procedure and the equip- ment used. This may include specifying a type of sensor, calibration values, and a de- scription of the digitising and recording equipment. Conditions prevailing at the time of the experiment may be important for classiﬁer design, particularly when generalisation to other conditions is required.  The choice of which variables to measure is crucial to successful pattern recognition and is based on knowledge of the problem at hand and previous experience. Use knowl- edge of previous experiments whenever this is available. It has been found that increasing the number of variables measured does not necessarily increase classiﬁcation performance   Data collection 445  on a data set of ﬁnite size. This trade-off between sample size and dimensionality, dis- cussed more fully below, is something to consider when taking measurements. Although variables may be pruned using a variable selection technique, such as those described in Chapter 9, if measurements are expensive, then it would be better not to make unneces- sary ones in the ﬁrst place. However, there is sometimes a need for redundant variables as an aid to error checking.  Once the measurement variables have been prescribed, the designer must decide on the sampling strategy and select an appropriate sample size. In a discrimination problem, in which a measurement vector x has an associated class label, perhaps coded as a vector z, there are two main sampling strategies under which the training data f.xi ; zi  ; i D 1; : : : ; pg may be realized. The ﬁrst of these is separate or class-conditional sampling. Here, the feature vectors are sampled from each class separately and therefore this strategy does not give us estimates of the prior probabilities of the classes. Of course, if these are known, then we may sample from the classes in these proportions. The second sampling design is joint or mixture sampling. In this scheme, the feature vector and the class are recorded on the samples drawn from a mixture of groups. The proportions in each group emerge from the data.  The sample size depends on a number of factors: the number of features, the de- sired performance, the complexity of the classiﬁcation rule  in terms of the number of parameters to estimate  and the asymptotic probability of misclassiﬁcation. It is very difﬁcult to obtain theoretical results about the effects of ﬁnite sample size on classiﬁer performance. However, if the number of features is large and the classiﬁer is com- plex then a large number of measurements should be made. Yet, it is difﬁcult to know, before the data collection, how complex the resulting classiﬁer needs to be. In addi- tion, a large number of data samples is necessary if the separation between classes is small or high conﬁdence in the error rate estimate is desired. Further, if little knowl- edge about the problem is available, necessitating the use of nonparametric methods, then generally larger data sets than those used for parametric approaches are required. This must be offset against factors limiting the sample size such as measurement cost. Again, much can be learned from previous work  for example, Fukunaga and Hayes, 1989c; Jain and Chandrasekaran, 1982 . Several papers suggest that the ratio of sam- ple size to number of features is a very important factor in the design of a pattern recognition system  Jain and Chandrasekaran, 1982; Kalayeh and Landgrebe, 1983  giv- ing the general guidance of having 5–10 times more samples per class than feature measurements.  Once gathered, the data are often partitioned into training  or design  and test sets. The training set should be a random sample from the total data set. There are two main reasons for partitioning the data. In one instance, the classiﬁer is trained using the training set and the test set is used to provide an independent estimate of its perfor- mance. This makes inefﬁcient use of the data for training a classiﬁer  see Chapter 11 . The second way in which training and test sets are used is in classiﬁer design. It applies to classiﬁers of perhaps differing complexity or classiﬁers of the same com- plexity  in terms of the architecture and the number of parameters  but with differ- ent initial conditions in a nonlinear optimisation procedure. These are trained using the training set and the performance on the test set monitored. The classiﬁer giv- ing the best performance on the test set is adopted. Using the test set in this man- ner means that it is being used as part of the training process and cannot be used   446 Data  to provide an independent error estimate. Other methods of error rate estimation must be employed.  The test set used in the above manner is more properly termed a validation set, which is used to tune classiﬁer parameters. A third independent data set may required for performance evaluation purposes.  Whatever way we design our classiﬁer, in many practical applications we shall desire good generalisation; that is, good performance on unseen data representative of the true operational conditions in which the classiﬁer is to be deployed. Of course, the aim should be to collect data representative of those conditions  this is the test set . Nevertheless, if your budget allows, it is worthwhile collecting a set of data, perhaps at a different location, perhaps at a different time of year, or by a different group of researchers  if your classiﬁer is meant to be invariant to these factors  and validating your conclusions on this data set.  A ﬁnal point to mention with respect to data collection is randomisation. This reduces the effects of bias. The order of the data should be randomised, not collecting all the examples from class 1, then all the examples from class 2 and so on. This is particularly important if the measurement equipment or the background or environmental conditions change with time. Complete randomisation is not possible and some form of restricted randomisation could be employed  Chatﬁeld, 1988 .  The following points summarise the data-collection strategy.  1. Choose the variables. If prior knowledge is available then this must be used.  2. Decide on the sample size and proportions within each class  for separate sampling .  3. Record the details of the procedure and measuring equipment.  4. Measure an independent test set.  5. Randomise the data collection.  D.4 Initial examination of data  Once the data have been collected it is tempting to rush into an analysis perhaps using complicated multivariate analysis techniques. With the wide availability of computer packages and software, it is relatively easy to perform such analyses without ﬁrst having a careful or even cursory look at the data. The initial examination of the data is one of the most important parts of the data analysis cycle  and we emphasise again the iterative aspect to an investigation . Termed an initial data analysis  IDA  by Chatﬁeld  1985, 1988  or exploratory data analysis by Tukey  1977 , it constitutes the ﬁrst phase of the analysis and comprises three parts:  1. checking the quality of the data;  2. calculating summary statistics;  3. producing plots of the data in order to get a feel for their structure.   Initial examination of data 447  Checking the data There are several factors that degrade data quality, the main ones being due to errors, outliers and missing observations. Errors may occur in several ways. They may be due to malfunctions in recording equipment, for example transcription errors, or they may even be deliberate if a respondent in a survey gives false replies. Some errors may be difﬁcult to detect, particularly if the value in error is consistent with other observations. Alternatively, if the error gives rise to an outlier  an observation that appears to be inconsistent with the remainder of the data  then a simple range test on each variable may pick it up.  Missing values can arise in a number of different ways and it is important to know how and why they occur. Extreme care must be taken in the coding of missing values, not treating them as special numerical values if possible. Procedures for dealing with missing observations are also discussed more fully in Chapter 11.  Summary statistics Summary statistics should be calculated for the whole data set and for each class indi- vidually. The most widely used measures of location and spread are the sample mean and the standard deviation. The sample mean should be calculated on each variable and can be displayed, along with the standard deviation and range, in a table comparing these values with the class-conditional ones. This might provide important clues to the variables important for discriminating particular classes.  Plotting the data Graphical views of the data are extremely useful in providing an insight into the nature of multivariate data. They can prove to be a help as a means of detecting clusters in the data, indicating important  or unimportant  variables, suggesting appropriate transformations of the variables and for detecting outliers.  Histograms of the individual variables and scatterplots of pairwise combinations of variables are easy to produce. Scatterplots produce projections of the data onto various planes and may show up outliers. Different classes can be plotted with different sym- bols. Scatterplots do not always reﬂect the true multidimensional structure and if the number of variables, p, is large then it might be difﬁcult to draw conclusions from the correspondingly large number, p. p  cid:4  1 , of scatterplots.  There are many other plots, some of which project the data linearly or nonlinearly onto two dimensions. These techniques, such as multidimensional scaling, Sammon plots, and principal components analysis, are very useful for exploratory data analysis. They involve more than simply ‘looking’ at the data and are explored more fully in Chapter 9.  Summary The techniques brieﬂy described in this section form an essential part of data analysis in the early stages before submitting the data to a computer package. Initial data analysis provides aids for data description, data understanding and for model formation, perhaps giving vital clues as to which subsequent methods of analysis should be undertaken. In many cases it will save a lot of wasted effort.   448 Data  D.5 Data sets  The book by Andrews and Herzberg  1985  provides a range of data sets from the widely used Fisher–Anderson iris data to the more unusual data set of number of deaths by falling off horseback in the Prussian army. Some of the data sets in the book can be used for discrimination problems. The Handbook of Data Sets of Hand et al.  1993  consists of around 500 small data sets that are very useful for illustration purposes.  There are several electronic sources. The UCI repository of machine learning data- bases and domain theories  Murphy and Aha, 1995  contains over 70 data sets, most of them documented and for which published results of various analyses are available.  In addition to assessing methods on real data, which is an essential part of technique development, simulated data can prove extremely valuable. We can obtain as much  subject to the properties of the random number generation process  or as little as we wish for nominal cost. This enables asymptotic error rates to be estimated. These can be used to assess performance of error rate estimation procedures. Also, if we know the model that has generated the data, then we have a means of assessing our model order selection procedures. For example, if we design a classiﬁer that we believe is optimal  in the sense of producing the Bayes minimum error rate  for the particular case of normally distributed classes, then we ought to be able to test our numerical procedure using such data.  D.6 Notes and references  The book by Chatﬁeld  1988  provides an excellent account of the general principles involved in statistical investigation. Emphasis is placed on initial data analysis  see also Chatﬁeld, 1985; Tukey, 1977 , but all stages from problem formulation to interpretation of results are addressed. On experimental design, the classic texts by Cochran and Cox  1957  and Cox  1958  are still very relevant today. Further guidelines are given by Hahn  1984 , based on experiences of six case studies. The book by Everitt and Hay  1992  provides a very good introduction to the design and analysis of experiments through a case study in psychology. It also presents many recent developments in statistics and is much more widely applicable than the particular application domain described. A further discussion in mapping a scientiﬁc question to a statistical one is given by Hand  1994 . Techniques for visualising multivariate data are described in the books by Chatﬁeld and Collins  1980 , Dillon and Goldstein  1984  and Everitt and Dunn  1991 . Wilkinson  1992  gives a review of graphical displays.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   E  Probability theory  E.1 Deﬁnitions and terminology  The rudiments of probability theory used in the development of decision theory are now presented. Firstly, we introduce the idea of an experiment or observation. The set of all possible outcomes is called the sample space,  cid:1 , of the model, with each possible outcome being a sample point. An event, A, is a set of experimental outcomes, and corresponds to a subset of points in  cid:1 .  A probability measure is a function, P.A , with a set, A, as argument. It can be regarded as the expected proportion of times that A actually occurs and has the following properties: 1. 0  cid:1  P.A   cid:1  1. 2. P. cid:1   D 1. 3. If A and B are mutually exclusive events  disjoint sets  then  P.A [ B  D P.A  C P.B   More generally, when A and B are not necessarily exclusive, P.A [ B  D P.A  C P.B   cid:5  P.A \ B   A random variable is a function that associates a number with each possible outcome ! 2  cid:1 . We denote random variables by upper-case letters here, but in the main body of work we generally use the same symbol for a random variable and a measurement, the meaning being clear from context. Although the argument of P is a set, it is usual to use a description of the event as the argument, regarding this as equivalent to the corresponding set. Thus, we write P.X > Y   for the probability that X is greater than Y , rather than P.f! : X .!  > Y .! g .  The cumulative distribution function, sometimes simply called the distribution func- tion, of a random variable is the probability that the random variable is less than or equal to some speciﬁed value x; that is,  PX .x  D probability that X  cid:1  x   E.1    450 Probability theory  Usually, when there is no ambiguity, we drop the subscript X. The cumulative distribution function is a monotonic function of its argument with the property that P. cid:5 1  D 0; P.1  D 1. The derivative of the distribution function,  p.x  D d P dx  is the probability density function of the random variable X. For sample spaces such as the entire real line, the probability density function, p.x , has the following properties:  p.x  dx D 1  Z 1  cid:5 1 Z b  a  D P.a  cid:1  X  cid:1  b   p.x  ½ 0  p.x  dx D probability that X lies between a and b  Much of the discussion in this book will relate to vector quantities, since the inputs to many pattern classiﬁcation systems may be expressed in vector form. Random vectors are deﬁned similarly to random variables, associating each point in the sample space,  cid:1 , to a point in R p:  X :  cid:1   cid:5 ! R p  The joint distribution of X is the p-dimensional generalisation of  E.1  above: PX .x  D PX .x1; : : : ; x p  D probability that X1  cid:1  x1; : : : ; X p  cid:1  x p  and the joint density function is similarly given by p.x  D @ p P.x  @x1 : : : @x p  Given the joint density of a set of random variables X1; : : : ; X p, then a smaller set  X1; : : : ; Xm .m < p  also possesses a probability density function determined by  p.x1; : : : ; xm   D  Z 1  cid:5 1  Z 1  cid:5 1  : : :  p.x1; : : : ; x p  dxmC1 : : : dx p  This is sometimes known as the marginal density of X1; : : : Xm, although the expression is more usually applied to the single-variable densities, p.x1 ; p.x2 ; : : : ; p.x p  given by  p.xi   D  Z 1  cid:5 1  Z 1  cid:5 1  : : :  p.x1; : : : ; x p  dx1 : : : dxi cid:5 1 dxiC1 : : : dx p  The expected vector, or mean vector, of a random variable x, is deﬁned by  Z  m D E[X] D  x p.x  dx   Deﬁnitions and terminology 451  where dx denotes dx1 : : : dx p and the integral is over the entire space  and unless oth- erwise stated R D R 1  cid:5 1  and E[:] denotes the expectation operator. The ith component of the mean can be calculated by Z  Z  xi p.x1; : : : ; x p  dx1 : : : dx p D  xi p.x  dx  mi D E[Xi ] D D  : : :  Z Z 1  cid:5 1  xi p.xi   dxi  where p.xi   is the marginal density of the single variable Xi given above.  The covariance of two random variables provides a measure of the extent to which the deviations of the random variables from their respective mean values tend to vary together. The covariance of random variables xi and x j , denoted by Ci j , is given by  which may be expressed as  Ci j D E[.X j  cid:5  E[X j ] .Xi  cid:5  E[Xi ] ]  Ci j D E[Xi X j ]  cid:5  E[Xi ]E[X j ]  where E[Xi X j ] is the autocorrelation. The matrix with .i; j  th component Ci j is the covariance matrix  C D E[.X  cid:5  m .X  cid:5  m T ]  Two random variables Xi and X j are uncorrelated if the covariance between the two variables is zero, Ci j D 0, which implies  E[Xi X j ] D E[Xi ]E[X j ]  Two vectors, X and Y , are uncorrelated if  E[XT Y ] D E[X]T E[Y ]   E.2   In the special case where the means of the vectors are zero, so that the relation above becomes E[XT Y ] D 0, then the random variables are said to be mutually orthogonal.  Two events, A and B, are statistically independent if P.A \ B  D P.A P.B   and two random variables Xi and X j are independent if p.xi ; x j   D p.xi   p.x j    If the random variables X1; X2; : : : ; X p are independent then the joint density function may be written as a product of the individual densities:  p.x1; : : : ; x p  D p.x1  : : : p.x p   If two variables are independent then the expectation of X1 X2 is given by  Z Z  E[X1 X2] D  x1x2 p.x1; x2  dx1 dx2   452 Probability theory  and, using the independence property, Z  E[X1 X2] D  Z  x1 p.x1  dx1  x2 p.x2  dx2  D E[X1]E[X2]  This shows that X1 and X2 are uncorrelated. However, this does not imply that two variables that are uncorrelated are statistically independent. Often we shall want to consider a functional transformation from a given set of random variables fX1; X2; : : : ; X pg represented by the vector X to a set fY1; Y2; : : : ; Y pg represented by the vector Y . How do probability density functions change under such a transformation? Let the transformation be given by Y D g.X , where g D .g1; g2; : : : ; g p T . Then the density functions of X and Y are related by  pY .y  D pX .x  jJj  where jJj is the absolute value of the Jacobian determinant @g1 @x p ::: @gp @x p  J .x1; : : : ; x p  D  @g1 @x1 ::: @gp @x1  þþþþþþþþþþþ  : : :  : : :  : : :  þþþþþþþþþþþ  A simple transformation is the linear one  Y D AX C B  Then if X has the probability density pX .x , the probability density of Y is  pY .y  D pX .A cid:5 1.y  cid:5  B    jAj   E.3   where jAj is the absolute value of the determinant of the matrix A.  Given a random system and any two events A and B that can occur together, we can form a new system by taking only those trials in which B occurs. The probability of A in this new system is called the conditional probability of A given B and is denoted by P.AjB ; if P.B  > 0 it is given by  P.AjB  D P.A \ B   P.B   or  P.A \ B  D P.AjB P.B    E.4   This is the total probability theorem.  Now, since P.A \ B  D P.B \ A , we have from  E.4  P.AjB P.B  D P.BjA P.A    Deﬁnitions and terminology 453  or  P.AjB  D P.BjA P.A   P.B   This is Bayes’ theorem.  Now, if A1; A2; : : : ; AN are events which partition the sample space  that is, they are  mutually exclusive and their union is  cid:1   then  P.B  D NX iD1 D NX iD1  P.B \ Ai    P.BjAi  P.Ai     E.5    E.6   and we obtain a more practical form of Bayes’ theorem P.BjA j  P.A j   iD1 P.BjAi  P.Ai    P.A jjB  D  PN  In pattern classiﬁcation problems, B is often an observation event and the A j are pattern classes. The term a priori probability is often used for the quantity P.Ai   and the objective is to ﬁnd P.AijB , which is termed the a posteriori probability of Ai . The conditional distribution, PX .xjA , of a random variable X given the event A is deﬁned as the conditional probability of the event fX  cid:1  xg P.xjA  D P.fX  cid:1  xg; A   P.A   and P.1jA  D 1; P. cid:5 1jA  D 0. The conditional density p.xjA  is the derivative of P.xjA   p.xjA  D d P dx  D lim 1x!0  P.x  cid:1  X  cid:1  x C 1xjA   1x  The extension of the result  E.5  to the continuous case gives  p.x  D NX iD1  p.xjAi   p.Ai    where p.x  is the mixture density and we have taken B D fX  cid:1  xg, and the continuous version of Bayes’ theorem may be written D  p.xjA  D p.Ajx  p.x   p.A   p.AjX D x  p.x  p.AjX D x  p.x  dx  Z 1  cid:5 1  The conditional density of x given that the random vector Y has some speciﬁed value, y, is obtained by letting A D fy  cid:1  Y  cid:1  y C 1yg and taking the limit  lim 1y!0  p.xjfy  cid:1  Y  cid:1  y C 1yg  D lim 1y!0  p.x;fy  cid:1  Y  cid:1  y C 1yg  p.fy  cid:1  Y  cid:1  y C 1yg    454 Probability theory  giving  where p.x; y  is the joint density of X and Y and p.y  is the marginal density  Equations  E.7  and  E.8  lead to the density form of Bayes’ theorem   E.7    E.8   p.xjy  D p.x; y  p.y   Z  p.y  D  p.x; y  dx  p.xjy  D p.yjx  p.x  p.y  p.yjx  p.x  R p.yjx  p.x  dx  D  A generalisation of  E.7  is the conditional density of the random variables XkC1; : : : ; X p given X1; : : : ; Xk:  p.xkC1; : : : ; x pjx1; : : : ; xk  D p.x1; : : : ; x p  p.x1; : : : ; xk     E.9   This leads to the chain rule  p.x1; : : : ; x p  D p.x pjx1; : : : ; x p cid:5 1  p.x p cid:5 1jx1; : : : ; x p cid:5 2  : : : p.x2jx1  p.x1   The results of  E.8  and  E.9  allow unwanted variables in a conditional density to be removed. If they occur to the left of the vertical line, then integrate with respect to them. If they occur to the right, then multiply by the conditional density of the variables given the remaining variables on the right and integrate. For example,  p.ajl; m; n  D p.a; b; cjm  D  p.a; b; cjl; m; n  db dc p.a; b; cjl; m; n  p.l; njm  dl dn  Z Z  E.2 Normal distribution  We shall now illustrate some of the deﬁnitions and results of this section using the Gaussian or normal distribution  we use the two terms interchangeably in this book . It is a distribution to which we often refer in our discussion of pattern recognition algorithms. The standard normal density of a random variable X has zero mean and unit variance,  and has the form  p.x  D 1p 2³  exp  ¦  ²   cid:5  x 2 2   cid:5  1 < x < 1   Probability distributions 455  The distribution function is given by  P.X   D  Z X  cid:5 1  p.x  dx D 1p 2³ 2p ³  ¦  exp  ² Z X  cid:5  1  cid:5 1 2 R x 0 exp. cid:5 x 2  dx.  x 2  where erffxg is the error function,  dx D 1 2  C 1 2  erf  ¦  ² Xp  2  For the function Y D ¼ C ¦ X of the random variable X, the density function of Y is  p.y  D  1p 2¦ 2³  exp  "  cid:5  1 2   cid:9  y  cid:5  ¼   cid:10 2  ;  ¦  which has mean ¼ and variance ¦ 2, and we write Y ¾ N .¼; ¦ 2 .  If X1; X2; : : : ; X p are independently and identically distributed, each following the  standard normal distribution, then the joint density is given by    cid:5  1 2  p.x1; x2; : : : ; x p  D pY iD1  .2³   p=2 exp  p.xi   D  1  pX iD1  x 2 i  The transformation Y D AX C µ leads to the density function for Y  using  E.3    p.y  D  1  .2³   p=2jAj exp  ²   cid:5  1 2  .y  cid:5  µ T .A   cid:5 1 T A   cid:5 1.y  cid:5  µ    E.10      ¦  and since the covariance matrix,  cid:12 , of Y is   cid:12  D E[.Y  cid:5  µ .Y  cid:5  µ T ] D AAT  Equation  E.10  is usually written  p.y  D N .yjµ;  cid:12   D  .2³   p=2j cid:12 j 1 This is the multivariate normal distribution.  2  1  ²  cid:5  1 2  exp  .y  cid:5  µ T  cid:12    cid:5 1.y  cid:5  µ   ¦  Recall from the previous section that if two variables are independent then they are uncorrelated, but that the converse is not necessarily true. However, a special property of the normal distribution is that if two variables are joint normally distributed and uncorrelated, then they are independent.  The marginal densities and conditional densities of a joint normal distribution are  all normal.  E.3 Probability distributions  We introduce some of the more commonly used distributions with pointers to some places where they are used in the book  further probability distributions are listed by Bernardo and Smith, 1994 . If x has a speciﬁc probability density function, R.xjÞ , where Þ is the set of parameters of the speciﬁc functional form R, then for shorthand notation we may write x ¾ R.Þ ; similarly, we use xjþ ¾ R. f .þ  , for some function f , to mean p.xjþ  D R.xj f .þ  .   456 Probability theory  N .xjµ;  cid:12  , pp. 30, 34, 52, 68  Normal 1  p.x  D  1  j cid:12 j1=2.2³   p=2 exp  .x  cid:5  µ T  cid:12    cid:5 1.x  cid:5  µ   ¦  ²   cid:5  1 2   cid:12 , symmetric positive deﬁnite; E[x] D µ; V[x] D  cid:12 .  Sometimes it is convenient to express the normal with the inverse of the covariance matrix as a parameter.  N p.xjµ; λ , pp. 53, 53, 61, 67  Normal 2  p.x  D jλj1=2  .2³   p=2 exp  ²   cid:5  1 2  .x  cid:5  µ T λ.x  cid:5  µ   ¦  λ, symmetric positive deﬁnite; E[x] D µ; V[x] D λ cid:5 1.  Wi p.xjÞ; β , pp. 53, 53  " ³ p. p cid:5 1 =4  p.x  D   cid:9  1 2  pY iD1   cid:10  cid:5 1  .2Þ C 1  cid:5  i    jβjÞjxjÞ cid:5 . pC1 =2 exp. cid:5 Tr.βx    Wishart  x, symmetric positive deﬁnite; β, symmetric nonsingular; 2Þ > p  cid:5  1; E[x] D Þβ E[x cid:5 1] D .Þ  cid:5  . p C 1 =2  cid:5 1β.   cid:5 1;  St p.xjµ; λ; Þ , p. 53  p.x  D 0. 1 2 0. Þ 2  .Þ C p    .Þ³   p=2jλj 1  2   cid:15   1 C 1  .x  cid:5  µ T λ.x  cid:5  µ   Þ  Þ > 0, λ symmetric positive deﬁnite; E[x] D µ; V[x] D λ cid:5 1.Þ  cid:5  2  cid:5 1Þ.  Multivariate Student ½ cid:5 .ÞC p =2  DiC .xja , pp. 54, 70  0 @   cid:19 PC QC   cid:20   1 A  0  p.x  D  x ai cid:5 1 0   0, a D .a1; : : : ; aC  , x D .x1; : : : ; xC  , PC E[xi ] D ai =P  iD1 ai iD1 ai  i  CY iD1  i ai .  iD1 xi D 1;  Dirichlet   Ig.xjÞ; þ , pp. 67, 68  Inverted gamma  Ga.xjÞ; þ , pp. 61, 61  p.x  D þ Þ  0.Þ   x Þ cid:5 1 exp. cid:5 þx   Þ > 0; þ > 0; E[x] D Þþ cid:5 1; V[x] D Þþ cid:5 2.  p.x  D þ Þ  0.Þ   x cid:5 .ÞC1  exp. cid:5 þ=x   Þ > 0; þ > 0; E[x] D þ=.Þ  cid:5  1 . If 1=y ¾ Ga.Þ; þ , then y ¾ Ig.Þ; þ . Be.xjÞ; þ , p. 253  p.x  D 0.Þ C þ   0.Þ 0.þ   x Þ cid:5 1.1  cid:5  x þ cid:5 1  Þ > 0; þ > 0, 0 < x < 1; E[x] D Þ=.Þ C þ .  Bi.xj cid:18 ; n , p. 253   cid:9  n   cid:10   p.x  D   cid:18  x .1  cid:5   cid:18   n cid:5 x  x 0 <  cid:18  < 1, n D 1; 2; : : : ; x D 0; 1; : : : ; n.  Probability distributions 457  Gamma  Beta  Binomial  Muk .xjθ ; n , pp. 54, 292  Multinomial  p.x  D  n!Qk iD1 xi !  kY iD1   cid:18  xi i  0 <  cid:18 i < 1, Pk iD1   cid:18 i D 1, Pk  iD1 xi D n, xi D 0; 1; : : : ; n.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   References  Abramson, I.S.  1982  On bandwidth variation in kernel estimates – a square root law. Annals of  Statistics, 10:1217–1223.  Abu-Mostafa, Y.S., Atiya, A.F., Magdon-Ismail, M. and White, H., eds  2001  Special issue on ‘Neural Networks in Financial Engineering’. IEEE Transactions on Neural Networks, 12 4 .  Adams, N.M. and Hand, D.J.  1999  Comparing classiﬁers when the misallocation costs are  uncertain. Pattern Recognition, 32:1139–1147.  Adams, N.M. and Hand, D.J.  2000  Improving the practice of classiﬁer performance assessment.  Aeberhard, S., Coomans, D. and de Vel, O.  1993  Improvements to the classiﬁcation performance  Neural Computation, 12:305–311.  of RDA. Chemometrics, 7:99–115.  Aeberhard, S., Coomans, D. and de Vel, O.  1994  Comparative analysis of statistical pattern  recognition methods in high dimensional settings. Pattern Recognition, 27 8 :1065–1077.  Aitchison, J. and Begg, C.B.  1976  Statistical diagnosis when basic cases are not classiﬁed with  certainty. Biometrika, 63 1 :1–12.  Aitchison, J., Habbema, J.D.F. and Kay, J.W.  1977  A critical comparison of two methods of  statistical discrimination. Applied Statistics, 26:15–25.  Akaike, H.  1973   Information theory and an extension of the maximum likelihood principle. In B.N. Petrov and B.F. Csaki, eds, Second International Symposium on Information Theory, pp. 267–281, Akad´emiai Kiad´o, Budapest.  Akaike, H.  1974  A new look at the statistical model identiﬁcation.  IEEE Transactions on  Automatic Control, 19:716–723.  Akaike, H.  1977  On entropy maximisation principle. In P.R. Krishnaiah, ed., Proceedings of the  Symposium on Applications of Statistics, pp. 27–47. North Holland, Amsterdam.  Akaike, H.  1981  Likelihood of a model and information criteria. Journal of Econometrics,  16:3–14.  Akaike, H.  1985  Prediction and entropy. In A.C. Atkinson and S.E. Fienberg, eds, A Celebration  of Statistics, pp. 1–24. Springer-Verlag, New York.  Al-Alaoui, M.  1977  A new weighted generalized inverse algorithm for pattern recognition. IEEE  Transactions on Computers, 26 10 :1009–1017.  Aladjem, M.  1991  Parametric and nonparametric linear mappings of multidimensional data.  Aladjem, M. and Dinstein, I.  1992  Linear mappings of local data structures. Pattern Recognition  Pattern Recognition, 24 6 :543–553.  Letters, 13:153–159.  Albert, A. and Lesaffre, E.  1986  Multiple group logistic discrimination. Computers and Mathe-  matics with Applications, 12A 2 :209–224.  Anderberg, M.R.  1973  Cluster Analysis for Applications. Academic Press, New York. Anders, U. and Korn, O.  1999  Model selection in neural networks. Neural Networks, 12:309–323.   460 REFERENCES  Anderson, J.A.  1974  Diagnosis by logistic discriminant function: further practical problems and  results. Applied Statistics, 23:397–404.  Anderson, J.A.  1982  Logistic discrimination. In P.R. Krishnaiah and L.N. Kanal, eds, Handbook  of Statistics, vol. 2, pp. 169–191. North Holland, Amsterdam.  Anderson, J.J.  1985  Normal mixtures and the number of clusters problem. Computational  Statistics Quarterly, 2:3–14.  Andrews, D.F. and Herzberg, A.M.  1985  Data: A Collection of Problems from Many Fields for  the Student and Research Worker. Springer-Verlag, New York.  Andrews, H.C.  1972   Introduction to Mathematical Techniques in Pattern Recognition. Wiley  Interscience, New York.  Andrieu, C. and Doucet, A.  1999  Joint Bayesian model selection and estimation of noisy sinusoids  via reversible jump MCMC. IEEE Transactions on Signal Processing, 47 10 :2667–2676.  Apt´e, C., Sasisekharan, R., Seshadri, S. and Weiss, S.M.  1994  Case studies of high-dimensional  classiﬁcation. Journal of Applied Intelligence, 4:269–281.  Arimura, K. and Hagita, N.  1994  Image screening based on projection pursuit for image recog- In Proceedings of the 12th International Conference on Pattern Recognition, vol. 2,  nition. pp. 414–417, IEEE, Los Alamitos, CA.  Ashikaga, T. and Chang, P.C.  1981  Robustness of Fisher’s linear discriminant function under two- component mixed normal models. Journal of the American Statistical Association, 76:676–680. Atkinson, A.C. and Mulira, H.-M.  1993  The stalactite plot for the detection of multivariate  outliers. Statistics and Computing, 3:27–35.  Atlas, L., Connor, J., Park, D., El-Sharkawi, M., Marks, R., Lippman, A., Cole, R. and Muthusamy, Y.  1989  A performance comparison of trained multilayer perceptrons and trained classiﬁcation trees. In Proceedings of the 1989 IEEE Conference on Systems, Man and Cybernetics, pp. 915–920, IEEE, New York.  Atlas, R.S. and Overall, J.E.  1994  Comparative evaluation of two superior stopping rules for  hierarchical cluster analysis. Psychometrika, 59 4 :581–591.  Babich, G.A. and Camps, O.I.  1996  Weighted Parzen windows for pattern classiﬁcation. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 18 5 :567–570.  Bailey, T.L. and Elkan, C.  1995  Unsupervised learning of multiple motifs in biopolymers using  expectation maximization. Machine Learning Journal, 21:51–83.  Balakrishnan, N. and Subrahmaniam, K.  1985  Robustness to nonnormality of the linear discrim- inant function: mixtures of normal distributions. Communications in Statistics – Theory and Methods, 14 2 :465–478.  Barron, A.R. and Barron, R.L.  1988  Statistical learning networks: a unifying view. In E.J. Weg- man, D.T. Gantz and J.J. Miller, eds, Statistics and Computer Science: 1988 Symposium at the Interface, pp. 192–203. American Statistical Association, Faisfax, VA.  Bauer, E. and Kohavi, R.  1999  An empirical comparison of voting classiﬁcation algorithms:  bagging, boosting and variants. Machine Learning, 36:105–139.  Baxter, M.J.  1995  Standardisation and transformation in principal component analysis with  applications to archaeometry. Applied Statistics, 4 4 :513–527.  Beckman, R.J. and Cook, R.D.  1983  Outlier : : : : : : : : : : s. Technometrics, 25 2 :119–163. Bedworth, M.D.  1988  Improving on standard classiﬁers by implementing them as a multilayer perceptron. RSRE Memorandum, DERA, St Andrews Road, Malvern, Worcs, WR14 3PS, UK. Bedworth, M.D., Bottou, L., Bridle, J.S., Fallside, F., Flynn, L., Fogelman, F., Ponting, K.M. and Prager, R.W.  1989  Comparison of neural and conventional classiﬁers on a speech recognition problem. In IEE International Conference on Artiﬁcial Neural Networks, pp. 86–89, IEE, London.  Bengio, Y., Buhmann, J.M., Embrechts, M. and Zurada, J.M., eds  2000  Special issue on ‘Neural Networks for Data Mining and Knowledge Discovery’. IEEE Transactions on Neural Networks, 11 3 .   REFERENCES 461  Bensmail, H. and Celeux, G.  1996  Regularized Gaussian discriminant analysis through eigenvalue  decomposition. Journal of the American Statistical Association, 91:1743–1748.  Bergh, A.F., Soong, F.K. and Rabiner, L.R.  1985  Incorporation of temporal structure into a vector- quantization-based preprocessor for speaker-independent, isolated-word recognition. AT&T Technical Journal, 64 5 :1047–1063.  Bernardo, J.M. and Smith, A.F.M.  1994  Bayesian Theory. Wiley, Chichester. Bezdek, J.C.  1981  Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum Press,  New York.  Bezdek, J.C. and Pal, S.K., eds  1992  Fuzzy Models for Pattern Recognition. Methods that Search  for Structure in Data. IEEE Press, New York.  Bishop, C.M.  1993  Curvature-driven smoothing: a learning algorithm for feedforward networks.  IEEE Transactions on Neural Networks, 4 5 :882–884.  Bishop, C.M.  1995  Neural Networks for Pattern Recognition. Oxford University Press, Oxford. Bishop, C.M., Svens´en, C.M. and Williams, C.K.I.  1998  GTM: The generative topographic  mapping. Neural Computation, 10:215–234.  Blacknell, D. and White, R.G.  1994  Optimum classiﬁcation of non-Gaussian processes using  neural networks. IEE Proceedings on Vision, Image and Signal Processing, 141 1 :56–66.  Blue, J.L., Candela, G.T., Grother, P.J., Chellappa, R. and Wilson, C.L.  1994  Evaluation of pattern classiﬁers for ﬁngerprint and OCR applications. Pattern Recognition, 27 4 :485–501. IEEE  Bobrowski, L. and Bezdek, J.C.  1991  c-means clustering with the l1 and l1 norms.  Transactions on Systems, Man, and Cybernetics, 21 3 :545–554.  Bock, H.H.  1985  On some signiﬁcance tests in cluster analysis. Journal of Classiﬁcation, 2:  77–108.  Bock, H.H.  1989  Probabilistic aspects in cluster analysis.  In O. Opitz, ed., Conceptual and  Numerical Analysis of Data, pp. 12–44. Springer-Verlag, Berlin.  Bonde, G.J.  1976  Kruskal’s non-metric multidimensional scaling – applied in the classiﬁcation of bacteria. In J. Gordesch and P. Naeve, eds, Proceedings in Computational Statistics, pp. 443– 449. Physica-Verlag, Vienna.  Booth, J.G. and Hall, P.  1994  Monte Carlo approximation and the iterated bootstrap. Biometrika,  81 2 :331–340.  Bowman, A.W.  1985  A comparative study of some kernel-based nonparametric density estima-  tors. Journal of Statistical Computation and Simulation, 21:313–327.  Bozdogan, H.  1987  Model selection and Akaike’s information criterion  AIC : the general theory  and its analytical extensions. Psychometrika, 52 3 :345–370.  Bozdogan, H.  1993  Choosing the number of component clusters in the mixture-model using a new informational complexity criterion of the inverse-Fisher information matrix. In O. Opitz, B. Lausen and R. Klar eds, Information and Classiﬁcation, pp. 40–54. Springer-Verlag, Hei- delberg.  Bradley, A.P.  1997  The use of the area under the ROC curve in the evaluation of machine  learning algorithms. Pattern Recognition, 30 7 :1145–1159.  Breckenridge, J.N.  1989  Replicating cluster analysis: method, consistency, and validity. Multi-  variate Behavioral Research, 24 32 :147–161.  Breiman, L.  1996  Bagging predictors. Machine Learning, 26 2 :123–140. Breiman, L.  1998  Arcing classiﬁers. Annals of Statistics, 26 3 :801–849. Breiman, L. and Friedman, J.H.  1988  Discussion on article by Loh and Vanichsetakul: ‘Tree- structured classiﬁcation via generalized discriminant analysis’. Journal of the American Sta- tistical Association, 83:715–727.  Breiman, L. and Ihaka, R.  1984  Nonlinear discriminant analysis via scaling and ACE. Technical  Report 40, Department of Statistics, University of California, Berkeley.  Breiman, L., Meisel, W. and Purcell, E.  1977  Variable kernel estimates of multivariate densities.  Technometrics, 19 2 :135–144.   462 REFERENCES  Breiman, L., Friedman, J.H., Olshen, R.A. and Stone, C.J.  1984  Classiﬁcation and Regression  Trees. Wadsworth International Group, Belmont, CA.  Brent, R.P.  1991  Fast training algorithms for multilayer neural nets. IEEE Transactions on Neural  Networks, 2 3 :346–354.  Brill, F.Z., Brown, D.E. and Martin, W.N.  1992  Fast genetic selection of features for neural  network classiﬁers. IEEE Transactions on Neural Networks, 3 2 :324–328.  Broomhead, D.S. and Lowe, D.  1988  Multi-variable functional interpolation and adaptive net-  works. Complex Systems, 2 3 :269–303.  Brown, D.E., Corruble, V. and Pittard, C.L.  1993  A comparison of decision tree classiﬁers with backpropagation neural networks for multimodal classiﬁcation problems. Pattern Recognition, 26 6 :953–961.  Brown, M., Lewis, H.G. and Gunn, S.R.  2000  Linear spectral mixture models and support vector machines for remote sensing. IEEE Transactions on Geoscience and Remote Sensing, 38 5 :2346–2360.  Brown, P.J.  1993  Measurement, Regression, and Calibration. Clarendon Press, Oxford. Bruzzone, L., Roli, F. and Serpico, S.B.  1995  An extension of the Jeffreys–Matusita distance to multiclass cases for feature selection. IEEE Transactions on Geoscience and Remote Sensing, 33 6 :1318–1321.  Bull, S.B. and Donner, A.  1987  The efﬁciency of multinomial logistic regression compared with multiple group discriminant analysis. Journal of the American Statistical Association, 82:1118–1121.  Buntine, W.L.  1992  Learning classiﬁcation trees. Statistics and Computing, 2:63–73. Buntine, W.L.  1996  A guide to the literature on learning probabilistic networks from data. IEEE  Transactions on Knowledge and Data Engineering, 8 2 :195–210.  Buntine, W.L. and Weigend, A.S.  1991  Bayesian back-propagation. Complex Systems, 5:603–643. Burbidge, R., Trotter, M., Buxton, B. and Holden, S.  2001  Drug design by machine learning: support vector machines for pharmaceutical data analysis. Computers and Chemistry, 26:5–14. Burges, C.J.C.  1998  A tutorial on support vector machines for pattern recognition. Data Mining  Burrell, P.R. and Folarin, B.O.  1997  The impact of neural networks in ﬁnance. Neural Computing  and Knowledge Discovery, 2:121–167.  and Applications, 6:193–200.  Buturovi´c, L.J.  1993  Improving k-nearest neighbor density and error estimates. Pattern Recog-  Calinski, R.B. and Harabasz, J.  1974  A dendrite method for cluster analysis. Communications  nition, 26 4 :611–616.  in Statistics, 3:1–27.  Campbell, J.G., Fraley, C., Murtagh, F. and Raftery, A.E.  1997  Linear ﬂaw detection in woven  textiles using model-based clustering. Pattern Recognition Letters, 18:1539–1548.  Cao, L. and Tay, F.E.H.  2001  Financial forecasting using support vector machines. Neural  Computing and Applications, 10:184–192.  Cao, R., Cuevas, A. and Manteiga, W.G.  1994  A comparative study of several smoothing methods  in density estimation. Computational Statistics and Data Analysis, 17:153–176.  Carter, C. and Catlett, J.  1987  Assessing credit card applications using machine learning. IEEE  Expert, 2:71–79.  46 3 :167–174.  Casella, G. and George, E.I.  1992  Explaining the Gibbs sampler. American Statistician,  Celeux, G. and Govaert, G.  1992  A classiﬁcation EM algorithm for clustering and two stochastic  versions. Computational Statistics and Data Analysis, 14:315–332.  Celeux, G. and Govaert, G.  1995  Gaussian parsimonious clustering models. Pattern Recognition,  Celeux, G. and Mkhadri, A.  1992  Discrete regularized discriminant analysis. Statistics and  28 5 :781–793.  Computing, 2 3 :143–151.   REFERENCES 463  Celeux, G. and Soromenho, G.  1996  An entropy criterion for assessing the number of clusters  in a mixture model. Journal of Classiﬁcation, 13:195–212.  ˘Cencov, N.N.  1962  Evaluation of an unknown distribution density from observations. Soviet  Mathematics, 3:1559–1562.  Chair, Z. and Varshney, P.R.  1986  Optimal data fusion in multiple sensor detection systems.  IEEE Transactions on Aerospace and Electronic Systems, 22:98–101.  Chan, C.-W.J., Huang, C. and DeFries, R.  2001  Enhanced algorithm performance for land cover classiﬁcation from remotely sensed data using bagging and boosting. IEEE Transactions on Geoscience and Remote Sensing, 39 3 :693–695.  Chang, C.-Y.  1973  Dynamic programming as applied to feature subset selection in a pattern  recognition system. IEEE Transactions on Systems, Man, and Cybernetics, 3 2 :166–171.  Chang, E.I. and Lippmann, R.P.  1991  Using genetic algorithms to improve pattern classiﬁcation In R.P. Lippmann, J.E. Moody and D.S. Touretzky, eds, Advances in Neural  performance. Information Processing Systems, vol. 3, pp. 797–803. Morgan Kaufmann, San Mateo, CA.  Chang, E.I. and Lippmann, R.P.  1993  A boundary hunting radial basis function classiﬁer which allocates centers constructively. In S.J. Hanson, J.D. Cowan and C.L. Giles, eds, Ad- vances in Neural Information Processing Systems, vol. 5, pp. 139–146. Morgan Kaufmann, San Mateo, CA.  Chang, P.C. and Aﬁﬁ, A.A.  1974  Classiﬁcation based on dichotomous and continuous variables.  Journal of the American Statistical Association, 69:336–339.  Chatﬁeld, C.  1985  The initial examination of data  with discussion . Journal of the Royal  Statistical Society Series A, 148:214–253.  Chatﬁeld, C.  1988  Problem Solving. A Statistician’s Guide. Chapman & Hall, London. Chatﬁeld, C. and Collins, A.J.  1980   Introduction to Multivariate Analysis. Chapman & Hall,  London.  Chatzis, V., Bors¸, A.G. and Pitas, I.  1999  Multimodal decision-level fusion for person authenti- cation. IEEE Transactions on Systems, Man, and Cybernetics – Part A: Systems and Humans, 29 6 :674–680.  Chellappa, R., Fukushima, K., Katsaggelos, A.K., Kung, S.-Y., LeCun, Y., Nasrabadi, N.M. and Poggio, T., eds  1998  Special issue on ‘Applications of Artiﬁcial Neural Networks to Image Processing’. IEEE Transactions on Image Processing, 7 8 .  Chellappa, R., Wilson, C.L. and Sirohey, S.  1995  Human and machine recognition of faces: a  survey. Proceedings of the IEEE, 83 5 :705–740.  Chen, C.H.  1973  Statistical Pattern Recognition. Hayden, Washington, DC. Chen, C.H.  1976  On information and distance measures, error bounds, and feature selection.  Information Sciences, 10:159–173.  Chen, J.S. and Walton, E.K.  1986  Comparison of two target classiﬁcation schemes.  IEEE  Transactions on Aerospace and Electronic Systems, 22 1 :15–22.  Chen, K., Xu, L. and Chi, H.  1999   Improved learning algorithms for mixture of experts in  multiclass classiﬁcation. Neural Networks, 12:1229–1252.  Chen, L.-F., Liao, H.-Y.M., Ko, M.-T., Lin, J.-C. and Yu, G.-J.  2000a  A new LDA-based face recognition system which can solve the small sample size problem. Pattern Recognition, 33:1713–1726.  Chen, S., Cowan, C.F.N. and Grant, P.M.  1991  Orthogonal least squares learning algorithm for  radial basis function networks. IEEE Transactions on Neural Networks, 2 2 :302–309.  Chen, S., Grant, P.M. and Cowan, C.F.N.  1992  Orthogonal least-squares algorithm for training  multioutput radial basis function networks. IEE Proceedings, Part F, 139 6 :378–384.  Chen, S., Chng, E.S. and Alkadhimi, K.  1996  Regularised orthogonal least squares algorithm for constructing radial basis function networks. International Journal of Control, 64 5 :829–837. Chen, S., Gunn, S. and Harris, C.J.  2000  Decision feedback equaliser design using support vector  machines. IEE Proceedings on Vision, Image and Signal Processing, 147 3 :213–219.   464 REFERENCES  Chen, T. and Chen, H.  1995  Approximation capability to functions of several variables, nonlinear IEEE Transactions on  functionals, and operators by radial basis function neural networks. Neural Networks, 6 4 :904–910.  Cheng, B. and Titterington, D.M.  1994  Neural networks: a review from a statistical perspective   with discussion . Statistical Science, 9 1 :2–54.  Cheng, Y.-Q., Zhuang, Y.-M. and Yang, J.-Y.  1992  Optimal Fisher discriminant analysis using  rank decomposition. Pattern Recognition, 25 1 :101–111.  Chernick, M.R., Murthy, V.K. and Nealy, C.D.  1985  Application of bootstrap and other resam- pling techniques: evaluation of classiﬁer performance. Pattern Recognition Letters, 3:167–178. Chiang, S.-S., Chang, C.-I. and Ginsberg, I.W.  2001  Unsupervised target detection in hyperspec- tral images using projection pursuit. IEEE Transactions on Geoscience and Remote Sensing, 39 7 :1380–1391.  Chien, Y.T. and Fu, K.S.  1967  On the generalized Karhunen–Lo`eve expansion. IEEE Transac-  tions on Information Theory, 13:518–520.  Ching´anda, E.F. and Subrahmaniam, K.  1979  Robustness of the linear discriminant function to  nonnormality: Johnson’s system. Journal of Statistical Planning and Inference, 3:69–77.  Chitteneni, C.B.  1980  Learning with imperfectly labeled patterns. Pattern Recognition, 12:  281–291.  Chitteneni, C.B.  1981  Estimation of probabilities of label imperfections and correction of misla-  bels. Pattern Recognition, 13:257–268.  Chou, P.A.  1991  Optimal partitioning for classiﬁcation and regression trees. IEEE Transactions  on Pattern Analysis and Machine Intelligence, 13 4 :340–354.  Chow, C.K.  1970  On optimum recognition error and reject tradeoff.  IEEE Transactions on  Information Theory, 16 1 :41–46.  Chow, C.K. and Liu, C.N.  1968  Approximating discrete probability distributions with dependence  trees. IEEE Transactions on Information Theory, 14 3 :462–467.  Chow, M.-Y., ed.  1993  Special issue on ‘Applications of Intelligent Systems to Industrial Elec-  tronics’. IEEE Transactions on Industrial Electronics, 40 2 .  Clifford, H.T. and Stephenson, W.  1975  An Introduction to Numerical Classiﬁcation. Academic  Press, New York.  Cochran, W.G. and Cox, G.M.  1957  Experimental Designs. Wiley, New York. Cole, A.J. and Wishart, D.  1970  An improved algorithm for the Jardine–Sibson method of  generating overlapping clusters. Computer Journal, 13 2 :156–163.  Comon, P.  1994  Independent component analysis, a new concept? Signal Processing, 36:287–314. Constantinides, A.G., Haykin, S., Hu, Y.H., Hwang, J.-N., Katagiri, S., Kung, S.-Y. and IEEE  Poggio, T.A., eds  1997  Special issue on ‘Neural Networks for Signal Processing’. Transactions on Signal Processing, 45 11 .  Conway, J.A., Brown, L.M.J., Veck, N.J. and Cordey, R.A.  1991  A model-based system for  crop classiﬁcation from radar imagery. GEC Journal of Research, 9 1 :46–54.  Cooper, G.F. and Herskovits, E.  1992  A Bayesian method for the induction of probabilistic  networks from data. Machine Learning, 9:309–347.  Copsey, K.D. and Webb, A.R.  2001  Bayesian approach to mixture models for discrimination. In F.J. Ferri, J.M. I˜nesta, A. Amin and P. Pudil, eds, Advances in Pattern Recognition, pp. 491–500. Springer, Berlin.  Cormack, R.M.  1971  A review of classiﬁcation  with discussion . Journal of the Royal Statistical  Society Series A, 134:321–367.  Cortes, C. and Vapnik, V.  1995  Support-vector networks. Machine Learning, 20:273–297. Cosman, P.C., Tseng, C., Gray, R.M., Olshen, R.A., Moses, L.E., Davidson, H.C., Bergin, C.J. and Riskin, E.A.  1993  Tree-structured vector quantization of CT chest scans: image quality and diagnostic accuracy. IEEE Transactions on Medical Imaging, 12 4 :727–739.   REFERENCES 465  Courant, R. and Hilbert, D.  1959  Methods of Mathematical Physics. Wiley, New York. Cover, T.M. and Hart, P.E.  1967  Nearest neighbour pattern classiﬁcation. IEEE Transactions  on Information Theory, 13:21–27.  Cowell, R.G., Dawid, A.P., Hutchinson, T. and Spiegelhalter, D.J.  1991  A Bayesian expert system  for the analysis of an adverse drug reaction. Artiﬁcial Intelligence in Medicine, 3:257–270.  Cox, D.R.  1958  The Planning of Experiments. Wiley, New York. Cox, T.F. and Cox, M.A.A.  1994  Multidimensional Scaling. Chapman & Hall, London. Cox, T.F. and Ferry, G.  1993  Discriminant analysis using non-metric multidimensional scaling.  Pattern Recognition, 26 1 :145–153.  Cox, T.F. and Pearce, K.F.  1997  A robust logistic discrimination model.  Statistics and  Craven, P. and Wahba, G.  1979  Smoothing noisy data with spline functions. Numerische  Crawford, S.L.  1989  Extensions to the CART algorithm. International Journal of Man–Machine  Computing, 7:155–161.  Mathematik, 31:317–403.  Studies, 31:197–217.  Cristianini, N. and Shawe-Taylor, J.  2000   An Introduction to Support Vector Machines.  Cambridge University Press, Cambridge.  Crownover, R.M.  1991  A least squares approach to linear discriminant analysis. SIAM, 12 3 :  595–606.  Curram, S.P. and Mingers, J.  1994  Neural networks, decision tree induction and discriminant Journal of the Operational Research Society, 45 4 :  analysis: an empirical comparison. 440–450.  D’Andrea, L.M., Fisher, G.L. and Harrison, T.C.  1994  Cluster analysis of adult children of  alcoholics. International Journal of Addictions, 29 5 :565–582.  Darwish, Y., Cserh´ati, T. and Forg´acs, E.  1994  Use of principal component analysis and cluster analysis in quantitative structure-activity relationships: a comparative study. Chemometrics and Intelligent Laboratory Systems, 24:169–176.  Dasarathy, B.V.  1991  NN concepts and techniques. an introductory survey. In B.V. Dasarathy, IEEE  ed., Nearest Neighbour Norms: NN Pattern Classiﬁcation Techniques, pp. 1–30. Computer Society Press, Los Alamitos, CA.  Dasarathy, B.V.  1994a  Minimal consistent set  MCS  identiﬁcation for nearest neighbour decision systems design. IEEE Transactions on Systems, Man, and Cybernetics, 24 3 :511–517.  Dasarathy, B.V.  1994b  Decision Fusion. IEEE Computer Society Press, Los Alamitos, CA. Davies, D.L. and Bouldin, D.  1979  A cluster separation measure. IEEE Transactions on Pattern  Analysis and Machine Intelligence, 1:224–227.  Davison, A.C. and Hall, P.  1992  On the bias and variability of bootstrap and cross-validation  estimates of error rate in discrimination problems. Biometrika, 79:279–284.  Davison, A.C., Hinkley, D.V. and Schechtman, E.  1986   Efﬁcient bootstrap simulation.  Biometrika, 73 3 :555–556.  Dawant, B.M. and Garbay, C., eds  1999  Special topic section on ‘Biomedical Data Fusion’.  IEEE Transactions on Biomedical Engineering, 46 10 .  Day, N.E. and Kerridge, D.F.  1967  A general maximum likelihood discriminant. Biometrics,  23:313–323.  De Gooijer, J.G., Ray, B.K. and Horst, K.  1998  Forecasting exchange rates using TSMARS.  Journal of International Money and Finance, 17 3 :513–534.  De Jager, O.C., Swanepoel, J.W.H. and Raubenheimer, B.C.  1986  Kernel density estimators  applied to gamma ray light curves. Astronomy and Astrophysics, 170:187–196.  de Leeuw, J.  1977  Applications of convex analysis to multidimensional scaling. In J.R. Barra, F. Brodeau, G. Romier and B. van Cutsem, eds, Recent Developments in Statistics, pp. 133–145. North Holland, Amsterdam.   466 REFERENCES  de Leeuw, J. and Heiser, W.J.  1977  Convergence of correction matrix algorithms for mul- In J.C. Lingoes, ed., Geometric Representations of Relational Data,  tidimensional scaling. pp. 735–752. Mathesis Press, Ann Arbor, MI.  de Leeuw, J. and Heiser, W.J.  1980  Multidimensional scaling with restrictions on the In P.R. Krishnaiah, ed., Multivariate Analysis, vol. V, pp. 501–522. North  conﬁguration. Holland, Amsterdam.  de M´antaras, R.L. and Aguilar-Mart´ın, J.  1985  Self-learning pattern classiﬁcation using a  sequential clustering technique. Pattern Recognition, 18 3 4 :271–277.  De Veaux, R.D., Gordon, A.L., Comiso, J.C. and Bacherer, N.E.  1993  Modeling of topographic effects on Antarctic sea ice using multivariate adaptive regression splines. Journal of Geo- physical Research, 98 C11 :20307–20319.  de Vel, O. Anderson, A., Corney, M. and Mohay, G.  2001  Mining e-mail content for author  identiﬁcation forensics. SIGMOD Record, 30 4 :55–64.  Defays, D.  1977  An efﬁcient algorithm for a complete link method. Computer Journal,  20 4 :364–366.  Dekkers, M.J., Langereis, C.G., Vriend, S.P., van Santvoort, P.J.M. and de Lange, G.J.  1994  Fuzzy c-means cluster analysis of early diagenetic effects on natural remanent magnetisation acquisition in a 1.1 Myr piston core from the Central Mediterranean. Physics of the Earth and Planetary Interiors, 85:155–171.  Dellaportas, P.  1998  Bayesian classiﬁcation of neolithic tools. Applied Statistics, 47 2 :279–297. Dempster, A.P., Laird, N.M. and Rubin, D.B.  1977  Maximum likelihood from incomplete data  via the EM algorithm. Journal of the Royal Statistical Society Series B, 39:1–38.  Denison, D.G.T., Mallick, B.K. and Smith, A.F.M.  1998a  A Bayesian CART algorithm.  Denison, D.G.T., Mallick, B.K. and Smith, A.F.M.  1998b  Bayesian MARS. Statistics and  Biometrika, 85 2 :363–377.  Computing, 8:337–346.  Devijver, P.A.  1973  Relationships between statistical risks and the least-mean-square error criterion in pattern recognition. In Proceedings of the First International Joint Conference on Pattern Recognition, pp. 139–148.  Devijver, P.A. and Kittler, J.  1982  Pattern Recognition, A Statistical Approach. Prentice Hall,  London.  York.  Devroye, L.  1986  Non-uniform Random Variate Generation. Springer-Verlag, New York. Devroye, L. and Gy¨orﬁ, L.  1985  Nonparametric Density Estimation. The L1 View. Wiley, New  Diday, E. and Simon, J.C.  1976  Clustering analysis. In K.S. Fu, ed., Digital Pattern Recognition,  pp. 47–94. Springer-Verlag, Berlin.  Dietterich, T.G.  1998  Approximate statistical tests for comparing supervised classiﬁcation  learning algorithms. Neural Computation, 10:1895–1923.  Diggle, P.J. and Hall, P.  1986  The selection of terms in an orthogonal series density estimator.  Journal of the American Statistical Association, 81:230–233.  Dillon, T., Arabshahi, P. and Marks, R.J., eds  1997  Special issue on ‘Everyday Applications of  Neural Networks’. IEEE Transactions on Neural Networks, 8 4 .  Dillon, W.R. and Goldstein, M.  1984  Multivariate Analysis Methods and Applications. Wiley,  New York.  Djouadi, A. and Bouktache, E.  1997  A fast algorithm for the nearest-neighbor classiﬁer. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 19 3 :277–282.  Domingos, P. and Pazzani, M.  1997  On the optimality of the simple Bayesian classiﬁer under  zero–one loss. Machine Learning, 29:103–130.  Dony, R.D. and Haykin, S.  1995  Neural network approaches to image compression. Proceedings  of the IEEE, 83 2 :288–303.   REFERENCES 467  Doucet, A., De Freitas, N. and Gordon, N., eds  2001  Sequential Monte Carlo Methods in  Practice. Springer-Verlag, New York.  Dracopoulos, D.C. and Rosin, P.L., eds  1998  Special issue on ‘Machine Vision Using Neural  Networks’. Neural Computing and Applications, 7 3 .  Drake, K.C., Kim, Y., Kim, T.Y. and Johnson, O.D.  1994  Comparison of polynomial network and model-based target recognition. In N. Nandhakumar, ed., Sensor Fusion and Aerospace Applications II, vol. 2233, pp. 2–11. SPIE.  Drucker, H. and Le Cun, Y.  1992   Improving generalization performance using double back-  propagation. IEEE Transactions on Neural Networks, 3 6 :991–997.  Dubes, R.C.  1987  How many clusters are best? – an experiment. Pattern Recognition, 20 6 :  645–663.  Dubuisson, B. and Lavison, P.  1980  Surveillance of a nuclear reactor by use of a pattern recog-  nition methodology. IEEE Transactions on Systems, Man and Cybernetics, 10 10 :603–609.  Duchene, J. and Leclercq, S.  1988  An optimal transformation for discriminant analysis and IEEE Transactions on Pattern Analysis and Machine Intel-  principal component analysis. ligence, 10 6 :978–983.  Duchon, J.  1976  Interpolation des fonctions de deux variables suivant le principe de la ﬂexion  des plaques minces. R.A.I.R.O. Analyse Num´erique, 10 12 :5–12.  Duda, R.O., Hart, P.E. and Stork, D.G.  2001  Pattern Classiﬁcation. 2nd edn., Wiley, New York. Dudani, S.A.  1976  The distance-weighted k-nearest-neighbour rule. IEEE Transactions on Sys-  tems, Man, and Cybernetics, 6 4 :325–327.  Duffy, D., Yuhas, B., Jain, A. and Buja, A.  1994  Empirical comparisons of neural networks and statistical methods for classiﬁcation and regression. In B. Yuhas and N. Ansari, eds, Neural Networks in Telecommunications, pp. 325–349. Kluwer Academic Publishers, Norwell, MA.  Duin, R.P.W.  1976  On the choice of smoothing parameters for Parzen estimators of probability  density functions. IEEE Transactions on Computers, 25:1175–1179.  Duin, R.P.W.  1996  A note on comparing classiﬁers. Pattern Recognition Letters, 17:529–536. Dunn, J.C.  1974  A fuzzy relative of the ISODATA process and its use in detecting compact  well-separated clusters. Journal of Cybernetics, 3 3 :32–57.  Eddy, W.F., Mockus, A. and Oue, S.  1996  Approximate single linkage cluster analysis of large data sets in high-dimensional spaces. Computational Statistics and Data Analysis, 23:29–43. Efron, B.  1979  Bootstrap methods: Another look at the jackknife. Annals of Statistics, 7:1–26. Efron, B.  1982  The Jackknife, the Bootstrap, and Other Resampling Plans. Society for Industrial  and Applied Mathematics, Philadelphia.  Efron, B.  1983  Estimating the error rate of a prediction rule: Improvement on cross-validation.  Journal of the American Statistical Association, 78:316–331.  Efron, B.  1990  More efﬁcient bootstrap computations.  Journal of the American Statistical  Association, 85:79–89.  Efron, B. and Tibshirani, R.J.  1986  Bootstrap methods for standard errors, conﬁdence intervals,  and other measures of statistical accuracy  with discussion . Statistical Science, 1:54–77.  Eklundh, L. and Singh, A.  1993  A comparative analysis of standardised and unstandardised International Journal of Remote Sensing,  principal components analysis in remote sensing. 14 7 :1359–1370.  Enas, G.G. and Choi, S.C.  1986  Choice of the smoothing parameter and efﬁciency of k-nearest  neighbor classiﬁcation. Computers and Mathematics with Applications, 12A 2 :235–244.  Esposito, F., Malerba, D. and Semeraro, G.  1997  A comparative analysis of methods for pruning decision trees. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19 5 : 476–491.  Etezadi-Amoli, J. and McDonald, R.P.  1983  A second generation nonlinear factor analysis.  Psychometrika, 48:315–342.   468 REFERENCES  Everitt, B.S.  1981  A Monte Carlo investigation of the likelihood ratio test for the number of components in a mixture of normal distributions. Multivariate Behavioral Research, 16:171–180.  Everitt, B.S.  1988  A ﬁnite mixture model for the clustering of mixed-mode data. Statistics and  Probability Letters, 6:305–309.  Everitt, B.S. and Dunn, G.  1991  Applied Multivariate Data Analysis. Edward Arnold, London. Everitt, B.S. and Hand, D.J.  1981  Finite Mixture Distributions. Chapman & Hall, London. Everitt, B.S. and Hay, D.F.  1992  Talking about Statistics. A Psychologists Guide to Design and  Analysis. Edward Arnold, London.  Everitt, B.S. and Merette, C.  1990  The clustering of mixed-mode data: a comparison of possible  approaches. Journal of Applied Statistics, 17:283–297.  Everitt, B.S., Landau, S. and Leese, M.  2001  Cluster Analysis. 4th edn., Arnold, London. Falconer, J.A., Naughton, B.J., Dunlop, D.D., Roth, E.J., Strasser, D.C., and Sinacore, J.M.  1994  Predicting stroke in patient rehabilitation outcome using a classiﬁcation tree approach. Archives of Physical Medicine and Rehabilitation, 75:619–625.  Farag´o, A. and Lugosi, G.  1993  Strong universal consistency of neural network classiﬁers.  IEEE Transactions on Information Theory, 39 4 :1146–1151.  Feng, C. and Michie, D.  1994  Machine learning of rules and trees. In D. Michie, D.J. Spiegel- halter and C.C. Taylor, eds, Machine Learning, Neural and Statistical Classiﬁcation. Ellis Horwood, Hemel Hempstead.  Fern´andez de Ca˜nete, J. and Bulsari, A.B., eds  2000  Special issue on ‘Neural Networks in  Process Engineering’. Neural Computing and Applications, 9 3 .  Ferr´an, E.A., Pﬂugfelder, B. and Ferrara, P.  1994  Self-organized neural maps of human protein  sequences. Protein Science, 3:507–521.  Ferr´e, L.  1995  Selection of components in principal components analysis: a comparison of  methods. Computational Statistics and Data Analysis, 19:669–682.  Ferri, F.J. and Vidal, E.  1992a  Small sample size effects in the use of editing techniques. In Proceedings of the 11th IAPR International Conference on Pattern Recognition, pp. 607–610, The Hague, IEEE Computer Society Press, Los Alamitos, CA.  Ferri, F.J. and Vidal, E.  1992b  Colour image segmentation and labeling through multiedit-  condensing. Pattern Recognition Letters, 13:561–568.  Ferri, F.J., Albert, J.V. and Vidal, E.  1999  Considerations about sample-size sensitivity of a family of nearest-neighbor rules. IEEE Transactions on Systems, Man, and Cybernetics, 29 5 . Fitzmaurice, G.M., Krzanowski, W.J. and Hand, D.J.  1991  A Monte Carlo study of the 632  bootstrap estimator of error rate. Journal of Classiﬁcation, 8:239–250. Fletcher, R.  1988  Practical Methods of Optimization. Wiley, New York. Flury, B.  1987  A hierarchy of relationships between covariance matrices. In A.K. Gupta, ed.,  Advances in Multivariate Analysis. D. Reidel, Dordrecht.  Flury, B.  1988  Common Principal Components and Related Multivariate Models. Wiley, New  Foley, D.H. and Sammon, J.W.  1975  An optimal set of discriminant vectors. IEEE Transactions  York.  on Computers, 24 3 :281–289.  French, S. and Smith, J.Q.  1997  The Practice of Bayesian Analysis. Arnold, London. Freund, Y. and Schapire, R.  1996  Experiments with a new boosting algorithm.  In Machine Learning: Proceedings of the 13th International Conference, pp. 148–156, Morgan Kaufmann, San Francisco.  Freund, Y. and Schapire, R.  1999  A short introduction to boosting. Journal of the Japanese  Society for Artiﬁcial Intelligence, 14 5 :771–780.  Friedl, M.A., Brodley, C.E. and Strahler, A.H.  1999  Maximising land cover classiﬁcation accuracies produced by decision trees at continental to global scales. IEEE Transactions on Geoscience and Remote Sensing, 37 2 :969–977.   REFERENCES 469  Friedman, J.H.  1987  Exploratory projection pursuit. Journal of the American Statistical Asso-  Friedman, J.H.  1989  Regularized discriminant analysis. Journal of the American Statistical Asso-  ciation, 82:249–266.  ciation, 84:165–175.  Friedman, J.H.  1991  Multivariate adaptive regression splines. Annals of Statistics, 19 1 :1–141. Friedman, J.H.  1993  Estimating functions of mixed ordinal and categorical variables using adaptive splines. In S. Morgenthaler, E.M.D. Ronchetti and W.A. Stahel, eds, New Directions in Statistical Data Analysis and Robustness, pp. 73–113. Birkh¨auser-Verlag, Basel.  Friedman, J.H.  1994  Flexible metric nearest neighbor classiﬁcation. Report, Department of  Friedman, J.H. and Stuetzle, W.  1981  Projection pursuit regression. Journal of the American  Statistics, Stanford University.  Statistical Association, 76:817–823.  Friedman, J.H. and Tukey, J.W.  1974  A projection pursuit algorithm for exploratory data  analysis. IEEE Transactions on Computers, 23 9 :881–889.  Friedman, J.H., Bentley, J.L. and Finkel, R.A.  1977  An algorithm for ﬁnding best matches in  logarithmic expected time. ACM Transactions on Mathematical Software, 3 3 :209–226.  Friedman, J.H., Stuetzle, W. and Schroeder, A.  1984  Projection pursuit density estimation.  Journal of the American Statistical Association, 79:599–608.  Friedman, J.H., Hastie, T.J. and Tibshirani, R.J.  1998  Additive logistic regression: a statistical view of boosting. Technical report available from the authors’ website: http:  www- stat.stanford.edu \~jhf , Department of Statistics, Stanford University.  Friedman, N., Geiger, D. and Goldszmidt, M.  1997  Bayesian network classiﬁers. Machine  Fu, K.S.  1968  Sequential Methods in Pattern Recognition and Machine Learning. Academic  Fukunaga, K.  1990  Introduction to Statistical Pattern Recognition, 2nd edn., Academic Press,  Learning, 29:131–163.  Press, New York.  London.  Fukunaga, K. and Flick, T.E.  1984  An optimal global nearest neighbour metric.  IEEE  Transactions on Pattern Analysis and Machine Intelligence, 6:314–318.  Fukunaga, K. and Hayes, R.R.  1989a  The reduced Parzen classiﬁer.  IEEE Transactions on  Pattern Analysis and Machine Intelligence, 11 4 :423–425.  Fukunaga, K. and Hayes, R.R.  1989b  Estimation of classiﬁer performance. IEEE Transactions  on Pattern Analysis and Machine Intelligence, 11 10 :1087–1101.  Fukunaga, K. and Hayes, R.R.  1989c  Effects of sample size in classiﬁer design. IEEE Trans-  actions on Pattern Analysis and Machine Intelligence, 11 8 :873–885.  Fukunaga, K. and Hummels, D.M.  1987a  Bias of nearest neighbor error estimates. IEEE Trans-  actions on Pattern Analysis and Machine Intelligence, 9 1 :103–112.  Fukunaga, K. and Hummels, D.M.  1987b  Bayes error estimation using Parzen and k-NN procedures. IEEE Transactions on Pattern Analysis and Machine Intelligence, 9 5 :634–643. Fukunaga, K. and Hummels, D.M.  1989  Leave-one-out procedures for nonparametric error  estimates. IEEE Transactions on Pattern Analysis and Machine Intelligence, 11 4 :421–423.  Fukunaga, K. and Kessell, D.L.  1971  Estimation of classiﬁcation error. IEEE Transactions on  Computers, 20:1521–1527.  Fukunaga, K. and Narendra, P.M.  1975  A branch and bound algorithm for computing k-nearest  neighbors. IEEE Transactions on Computers, 24 7 .  Furey, T.S., Cristianini, N., Duffy, N., Bednarski, D.W., Schummer, M. and Haussler, D.  2000  Support vector machine classiﬁcation and validation of cancer tissue samples using microarray expression data. Bioinformatics, 16 10 :906–914.  Furui, S.  1997  Recent advances in speaker recognition. Pattern Recognition Letters, 18:859–872. Ganeshanandam, S. and Krzanowski, W.J.  1989  On selecting variables and assessing their  performance in linear discriminant analysis. Australian Journal of Statistics, 31 3 :433–447.   470 REFERENCES  Gath, I. and Geva, A.B.  1989  Unsupervised optimal fuzzy clustering.  IEEE Transactions on  Pattern Analysis and Machine Intelligence, 11 7 :773–781.  Geisser, S.  1964  Posterior odds for multivariate normal classiﬁcations. Journal of the Royal  Gelfand, A.E.   2000  Gibbs sampling.  Journal of  the American Statistical Association,  Statistical Society Series B, 26:69–76.  95:1300–1304.  Gelfand, A.E. and Smith, A.F.M.  1990  Sampling-based approaches to calculating marginal  densities. Journal of the American Statistical Association, 85:398–409.  Gelfand, S.B. and Delp, E.J.  1991  On tree structured classiﬁers. In I.K. Sethi and A.K. Jain, eds, Artiﬁcial Neural Networks and Statistical Pattern Recognition, pp. 51–70. North Holland, Amsterdam.  Gelman, A.  1996   Inference and monitoring convergence.  In W.R. Gilks, S. Richardson and D.J. Spiegelhalter, eds, Markov Chain Monte Carlo in Practice, pp. 131–143. Chapman & Hall, London.  Gersho, A. and Gray, R.M.  1992  Vector Quantization and Signal Compression. Kluwer  Academic, Dordrecht.  Giacinto, F., Roli, G. and Bruzzone, L.  2000  Combination of neural and statistical algorithms for supervised classiﬁcation of remote-sensing images. Pattern Recognition Letters, 21:385–397.  Giﬁ, A.  1990  Nonlinear Multivariate Analysis. Wiley, Chichester. Gilks, W.R., Richardson, S. and Spiegelhalter, D.J., eds  1996  Markov Chain Monte Carlo in  Practice. Chapman & Hall, London.  Gini, F.  1997  Optimal multiple level decision fusion with distributed sensors. IEEE Transaction  on Aerospace and Electronic Systems, 33 3 :1037–1041.  Glendinning, R.H.  1993  Model selection for time series. Part I: a review of criterion based meth- ods. DRA Memorandum 4730, QinetiQ, St Andrews Road, Malvern, Worcs, WR14 3PS, UK. Golub, G.H., Heath, M. and Wahba, G.  1979  Generalised cross-validation as a method of  choosing a good ridge parameter. Technometrics, 21:215–223.  Goodman, R.M. and Smyth, P.  1990  Decision tree design using information theory. Knowledge  Gordon, A.D.  1994a   Identifying genuine clusters in a classiﬁcation. Computational Statistics  Acquisition, 2:1–19.  and Data Analysis, 18:561–581.  Gordon, A.D.  1994b  Clustering algorithms and cluster validity. In P. Dirschedl and R. Oster-  mann, eds, Computational Statistics, pp. 497–512. Physica-Verlag, Heidelberg.  Gordon, A.D.  1996a  Null models in cluster validation. In W. Gaul and D. Pfeifer, eds, From Data to Knowledge: Theoretical and Practical Aspects of Classiﬁcation, Data Analysis and Knowledge Organization, pp. 32–44. Springer-Verlag, Berlin.  Gordon, A.D.  1996b  A survey of constrained classiﬁcation. Computational Statistics and Data  Analysis, 21:17–29.  Biometrics, 33:355–362.  27:857–874.  43:975–984.  Gordon, A.D.  1999  Classiﬁcation, 2nd edn., Chapman & Hall CRC, Boca Raton, FL. Gordon, A.D. and Henderson, J.T.  1977  An algorithm for Euclidean sum of squares classiﬁcation.  Gower, J.C.  1971  A general coefﬁcient of similarity and some of its properties. Biometrics,  Grayson, D.A.  1987  Statistical diagnosis and the inﬂuence of diagnostic error. Biometrics,  Green, P.J. and Silverman, B.W.  1994  Nonlinear Regression and Generalized Linear Models. A  Roughness Penalty Approach. Chapman & Hall, London.  Greene, T. and Rayens, W.  1989  Partially pooled covariance estimation in discriminant analysis.  Communications in Statistics, 18 10 :3679–3702.  Gu, C. and Qiu, C.  1993  Smoothing spline density estimation: theory. Annals of Statistics,  21 1 :217–234.   REFERENCES 471  Gu´enoche, A., Hansen, P. and Jaumard, B.  1991  Efﬁcient algorithms for divisive hierarchical  clustering with the diameter criterion. Journal of Classiﬁcation, 8:5–30.  Guyon, I., Makhoul, J., Schwartz, R. and Vapnik, V.  1998  What size test set gives good error rate estimates? IEEE Transactions on Pattern Analysis and Machine Intelligence, 20 1 :52–64. Guyon, I. and Stork, D.G.  1999  Linear discriminant and support vector classiﬁers. In A. Smola, P. Bartlett, B. Sch¨olkopf and C. Schuurmans, eds, Large Margin Classiﬁers, pp. 147–169. MIT Press, Cambridge, MA.  Guyon, I., Weston, J., Barnhill, S. and Vapnik, V.  2002  Gene selection for cancer classiﬁcation  using support vector machines. Machine Learning, 46:389–422.  Hahn, G.J.  1984  Experimental design in the complex world. Technometrics, 26 1 :19–31. Hajian-Tilaki, K.O., Hanley, J.A., Joseph, L. and Collett, J.-P.  1997a  A comparison of parametric and nonparametric approaches to ROC analysis of quantitative diagnostic tests. Medical Decision Making, 17 1 :94–102.  Hajian-Tilaki, K.O., Hanley, J.A., Joseph, L. and Collett, J.-P.  1997b  Extension of receiver operating characteristic analysis to data concerning multiple signal detection. Academic Radiology, 4 3 :222–229.  Halici, U. and Ongun, G.  1996  Fingerprint classiﬁcation through self-organising feature maps  modiﬁed to treat uncertainties. Proceedings of the IEEE, 84 10 :1497–5112.  Hall, P.  1992  The Bootstrap and Edgeworth Expansion. Springer-Verlag, New York. Hall, P., Hu, T.-C. and Marron, J.S.  1995   Improved variable window kernel estimates of  probability densities. Annals of Statistics, 23 1 :1–10.  Hamamoto, Y., Uchimura, S., Matsuura, Y., Kanaoka, T. and Tomita, S.  1990  Evaluation of the  branch and bound algorithm for feature selection. Pattern Recognition Letters, 11:453–456.  Hamamoto, Y., Matsuura, Y., Kanaoka, T. and Tomita, S.  1991  A note on the orthonormal  discriminant vector method for feature extraction. Pattern Recognition, 24 7 :681–684.  Hamamoto, Y., Kanaoka, T. and Tomita, S.  1993  On a theoretical comparison between the orthonormal discriminant vector method and discriminant analysis. Pattern Recognition, 26 12 :1863–1867.  Hamamoto, Y., Fujimoto, Y. and Tomita, S.  1996  On the estimation of a covariance matrix in  designing Parzen classiﬁers. Pattern Recognition, 29 10 :1751–1759.  Hamamoto, Y., Uchimura, S. and Tomita, S.  1997  A bootstrap technique for nearest neigh- IEEE Transactions on Pattern Analysis and Machine Intelligence,  bor classiﬁer design. 19 1 :73–79.  Hampel, F.R., Ronchetti, E.M., Rousseuw, P.J. and Stahel, W.A.  1986  Robust Statistics. The  Approach Based on Inﬂuence Functions. Wiley, New York.  Hand, D.J.  1981a  Discrimination and Classiﬁcation. Wiley, New York. Hand, D.J.  1981b  Branch and bound in statistical data analysis. The Statistician, 30:1–13. Hand, D.J.  1982  Kernel Discriminant Analysis. Research Studies Press, Letchworth. Hand, D.J.  1986  Recent advances in error rate estimation. Pattern Recognition Letters, 4:335–346. Hand, D.J.  1992  Statistical methods in diagnosis. Statistical Methods in Medical Research,  1 1 :49–67.  Hand, D.J.  1994  Assessing classiﬁcation rules. Journal of Applied Statistics, 21 3 :3–16. Hand, D.J.  1997  Construction and Assessment of Classiﬁcation Rules. Wiley, Chichester. Hand, D.J. and Batchelor, B.G.  1978  Experiments on the edited condensed nearest neighbour  rule. Information Sciences, 14:171–180.  Hand, D.J. and Till, R.J.  2001  A simple generalisation of the area under the ROC curve for  multiple class classiﬁcation problems. Machine Learning, 45:171–186.  Hand, D.J. and Yu, K.  2001   Idiot’s Bayes – not so stupid after all? International Statistical  Hand, D.J., Daly, F., McConway, K., Lunn, D. and Ostrowski, E.  1993  Handbook of Data Sets.  Review, 69 3 :385–398.  Chapman & Hall, London.   472 REFERENCES  Hand, D.J., Adams, N.M. and Kelly, M.G.  2001  Multiple classiﬁer systems based on interpretable In J. Kittler and F. Roli, eds, Multiple Classiﬁer Systems, pp. 136–147.  linear classiﬁers. Springer-Verlag, Berlin.  Hansen, P.L. and Salamon, P.  1990  Neural network ensembles. IEEE Transactions on Pattern  Analysis and Machine Intelligence, 12:993–1001.  Harkins, L.S., Sirel, J.M., McKay, P.J., Wylie, R.C., Titterington, D.M., and Rowan, R.M.  1994  Dis- criminant analysis of macrocytic red cells. Clinical and Laboratory Haematology, 16:225–234.  Harman, H.H.  1976  Modern Factor Analysis, 3rd edn., University of Chicago Press, Chicago. Harris, C.J., Bailey, A. and Dodd, T.J.  1997  Multi-sensor data fusion in defence and aerospace.  Aeronautical Journal, 102:229–244.  Hart, J.D.  1985  On the choice of a truncation point in Fourier series density estimation. Journal  of Statistical Computation and Simulation, 21:95–116.  Hart, P.E.  1968  The condensed nearest neighbor rule. IEEE Transactions on Information Theory,  Hartigan, J.A.  1975  Clustering Algorithms. Wiley, New York. Hasselblad, V.  1966  Estimation of parameters for a mixture of normal distributions. Tech-  Hastie, T.J. and Stuetzle, W.  1989  Principal curves. Journal of the American Statistical Associ-  14:515–516.  nometrics, 8:431–444.  ation, 84:502–516.  Hastie, T.J. and Tibshirani, R.J.  1990  Generalized Additive Models. Chapman & Hall, London. Hastie, T.J. and Tibshirani, R.J.  1996  Discriminant analysis by Gaussian mixtures. Journal of  the Royal Statistical Society Series B, 58 1 :155–176.  Hastie, T.J., Tibshirani, R.J. and Buja, A.  1994  Flexible discriminant analysis by optimal  scoring. Journal of the American Statistical Association, 89:1255–1270.  Hastie, T.J., Buja, A. and Tibshirani, R.J.  1995  Penalized discriminant analysis. Annals of  Statistics, 23 1 :73–102.  Hastie, T.J., Tibshirani, R.J. and Friedman, J.H.  2001  The Elements of Statistical Learning: Data  Mining, Inference, and Prediction. Springer, New York.  Hathaway, R.J. and Bezdek, J.C.  1994  NERF c-means: non-Euclidean relational fuzzy clustering.  Haykin, S.  1994  Neural Networks. A Comprehensive Foundation. Macmillan College Publishing,  Pattern Recognition, 27 3 :429–437.  New York.  Haykin, S., Stehwien, W., Deng, C., Weber, P. and Mann, R.  1991  Classiﬁcation of radar clutter  in an air trafﬁc control environment. Proceedings of the IEEE, 79 6 :742–772.  Heckerman, D.  1999  A tutorial on learning with Bayesian networks.  In M.I. Jordan, ed.,  Learning in Graphical Models, pp. 301–354. MIT Press, Cambridge, MA.  Heckerman, D., Geiger, D. and Chickering, D.M.  1995  Learning Bayesian networks: the  combination of knowledge and statistical data. Machine Learning, 20:197–243.  Heiser, W.J.  1991  A generalized majorization method for least squares multidimensional scaling  of pseudodistances that may be negative. Psychometrika, 56 1 :7–27.  Heiser, W.J.  1994  Convergent computation by iterative majorization: theory and applications in multidimensional data analysis. In W.J. Krzanowski, ed., Recent Advances in Descriptive Multivariate Analysis, pp. 157–189. Clarendon Press, Oxford.  Henley, W.E. and Hand, D.J.  1996  A k-nearest-neighbour classiﬁer for assessing consumer  Highleyman, W.H.  1962  The design and analysis of pattern recognition experiments. Bell  credit risk. The Statistician, 45 1 :77–95.  System Technical Journal, 41:723–744.  Hills, M.  1977  Book review. Applied Statistics, 26:339–340. Hinkley, D.V.  1988  Bootstrap methods. Journal of the Royal Statistical Society Series B, 50 3 :  321–337.   REFERENCES 473  Hjort, N.L. and Glad, I.K.  1995  Nonparametric density estimation with a parametric start.  Hjort, N.L. and Jones, M.C.  1996  Locally parametric nonparametric density estimation. Annals  Annals of Statistics, 23 3 :882–904.  of Statistics, 24 4 :1619–1647.  Ho, Y.-C. and Agrawala, A.K.  1968  On pattern classiﬁcation algorithms. Introduction and  survey. Proceedings of the IEEE, 56 12 :2101–2114.  Holmes, C.C. and Mallick, B.K.  1998  Bayesian radial basis functions of variable dimension.  Neural Computation, 10:1217–1233.  Holmstr¨om, L. and Koistinen, P.  1992  Using additive noise in back-propagation training. IEEE  Transactions on Neural Networks, 3 1 :24–38.  Holmstr¨om, L., Koistinen, P., Laaksonen, J. and Oja, E.  1997  Neural and statistical classiﬁers –  taxonomy and two case studies. IEEE Transactions on Neural Networks, 8 1 :5–17.  Holmstr¨om, L. and Sain, S.R.  1997  Multivariate discrimination methods for top quark analysis.  Technometrics, 39 1 :91–99.  Hong, Z.-Q. and Yang, J.-Y.  1991  Optimal discriminant plane for a small number of samples  and design method of classiﬁer on the plane. Pattern Recognition, 24 4 :317–324.  Hornik, K.  1993  Some new results on neural network approximation. Neural Networks,  6:1069–1072.  Hotelling, H.  1933  Analysis of a complex of statistical variables into principal components.  Journal of Educational Psychology, 24:417–444.  Hsu, C.W. and Lin, C.J.  2002  A comparison on methods for multi-class support vector machines.  IEEE Transactions on Neural Networks. To appear.  Hua, S. and Sun, Z.  2001  A novel method of protein secondary structure prediction with high segment overlap measure: support vector machine approach. Journal of Molecular Biology, 308:397–407.  Huang, Y.S. and Suen, C.Y.  1995  A method of combining multiple experts for the recognition of unconstrained handwritten numerals. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17 1 :90–94.  Huber, P.J.  1985  Projection pursuit  with discussion . Annals of Statistics, 13 2 :435–452. Hush, D.R., Horne, W. and Salas, J.M.  1992  Error surfaces for multilayer perceptrons. IEEE  Transactions on Systems, Man, and Cybernetics, 22 5 :1151–1161.  Huth, R., Nemeˇsov´a, I. and Klimperov´a, N.  1993  Weather categorization based on the average linkage clustering technique: an application to European mid-latitudes. International Journal of Climatology, 13:817–835.  Hwang, J.-N., Lay, S.-R. and Lippman, A.  1994a  Nonparametric multivariate density estimation:  a comparative study. IEEE Transactions on Signal Processing, 42 10 :2795–2810.  Hwang, J.-N., Lay, S.-R., Maechler, M., Martin, D. and Schimert, J.  1994b  Regression modeling in back-propagation and projection pursuit learning. IEEE Transactions on Neural Networks, 5 3 :342–353.  Hyv¨arinen, A. and Oja, E.  2000  Independent component analysis: algorithms and applications.  Neural Networks, 13:411–430.  Ifarraguerri, A. and Chang, C.-I.  2000  Unsupervised hyperspectral image analysis with projection  pursuit. IEEE Transactions on Geoscience and Remote Sensing, 38 6 :2529–2538.  Ingrassia, S.  1992  A comparison between the simulated annealing and the EM algorithms in  normal mixture decompositions. Statistics and Computing, 2:203–211.  Ismail, M.A.  1988  Soft clustering: algorithms and validity of solutions.  In M.M. Gupta and T. Yamakawa, eds, Fuzzy Computing, pp. 445–472. Elsevier Science  North Holland , Amsterdam.  Izenman, A.J.  1991  Recent developments in nonparametric density estimation. Journal of the  American Statistical Association, 86:205–223.   474 REFERENCES  Izenman, A.J. and Sommer, C.J.  1988  Philatelic mixtures and multimodal densities. Journal of  the American Statistical Association, 83:941–953.  Jackson, D.A.  1993  Stopping rules in principal components analysis: a comparison of heuristical  and statistical approaches. Ecology, 74 8 :2204–2214.  Jackson, J.E.  1991  A User’s Guide to Principal Components. Wiley, New York. Jacobs, R.A., Jordan, M.I., Nowlan, S.J. and Hinton, G.E.  1991  Adaptive mixtures of local  experts. Neural Computation, 3:79–87.  Jain, A.K. and Chandrasekaran, B.  1982  Dimensionality and sample size considerations in pattern recognition practice. In P.R. Krishnaiah and L.N. Kanal, eds, Handbook of Statistics, pp. 835–855. North Holland, Amsterdam.  Jain, A.K. and Dubes, R.C.  1988  Algorithms for Clustering Data. Prentice Hall, London. Jain, A.K. and Moreau, J.V.  1987  Bootstrap technique in cluster analysis. Pattern Recognition,  20 5 :547–568.  Jain, A.K., Duin, R.P.W. and Mao, J.  2000  Statistical pattern recognition: A review.  IEEE  Transactions on Pattern Analysis and Machine Intelligence, 22 1 :4–37.  Jain, S. and Jain, R.K.  1994  Discriminant analysis and its application to medical research.  Jambu, M. and Lebeaux, M.-O.  1983  Cluster Analysis and Data Analysis. North Holland,  Biomedical Journal, 36 2 :147–151.  Amsterdam.  James, G.M. and Hastie, T.J.  2001  Functional linear discriminant analysis for irregularly  sampled curves. Journal of the Royal Statistical Society Series B, 63 3 :533–550.  Jamshidian, M. and Jennrich, R.I.  1993  Conjugate gradient acceleration of the EM algorithm.  Journal of the American Statistical Association, 88:221–228.  Jamshidian, M. and Jennrich, R.I.  1997  Acceleration of the EM algorithm by using quasi-Newton  methods. Journal of the Royal Statistical Society Series B, 59 3 :569–587.  Jansen, R.C. and Den Nijs, A.P.M.  1993  A statistical mixture model for estimating the proportion of unreduced pollen grains in perennial ryegrass  Lolium perenne l.  via the size of pollen grains. Euphytica, 70:205–215.  Jardine, N.  1971  In discussion on Cormack’s paper: A review of classiﬁcation. Journal of the  Royal Statistical Society Series A, 134:321–367.  Jardine, N. and Sibson, R.  1971  Mathematical Taxonomy. Wiley, London. Jensen, F.V.  1996  An Introduction to Bayesian Networks. UCL Press, London. Jeon, B. and Landgrebe, D.A.  1994  Fast Parzen density estimation using clustering-based branch and bound. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16 9 :950–954. Jiang, Q. and Zhang, W.  1993  An improved method for ﬁnding nearest neighbours. Pattern  Recognition Letters, 14:531–535.  Jobert, M., Escola, H., Poiseau, E. and Gaillard, P.  1994  Automatic analysis of sleep using two parameters based on principal component analysis of electroencephalography spectral data. Biological Cybernetics, 71:197–207.  Jolliffe, I.T.  1986  Principal Components Analysis. Springer-Verlag, New York. Jones, M.C. and Lotwick, H.W.  1984  A remark on algorithm AS176. Kernel density estimation  using the fast Fourier transform. Applied Statistics, 33:120–122.  Jones, M.C. and Sibson, R.  1987  What is projection pursuit?  with discussion . Journal of the  Royal Statistical Society Series A, 150:1–36.  Jones, M.C. and Signorini, D.F.  1997  A comparison of higher order bias kernel density  estimators. Journal of the American Statistical Association, 92:1063–1073.  Jones, M.C. and Sheather, S.J.  1991  Using non-stochastic terms to advantage in kernel-based estimation of integrated squared density derivatives. Statistics and Probability Letters, 11: 511–514.  Jones, M.C., McKay, I.J. and Hu, T.-C.  1994  Variable location and scale kernel density  estimation. Annals of the Institute of Statistical Mathematics, 46 3 :521–535.   REFERENCES 475  Jones, M.C., Marron, J.S. and Sheather, S.J.  1996  A brief survey of bandwidth selection for  density estimation. Journal of the American Statistical Association, 91:401–407.  Jordan, M.I. and Jacobs, R.A.  1994  Hierarchical mixtures of experts and the EM algorithm.  Neural Computation, 6:181–214.  J¨oreskog, K.G.  1977   Factor analysis by least-squares and maximum-likelihood methods. In K. Enslein, A. Ralston and H.S. Wilf, eds, Statistical Methods for Digital Computers, pp. 125–153. Wiley Interscience, New York.  Juan, A. and Vidal, E.  1994  Fast k-means-like clustering in metric spaces. Pattern Recognition  Letters, 15:19–25.  23:187–200.  Juang, B.-H. and Rabiner, L.R.  1985  Mixture autoregressive hidden Markov models for speech  signals. IEEE Transactions on Acoustics, Speech and Signal Processing, 33 6 :1404–1413.  Kaiser, H.F.  1958  The varimax criterion for analytic rotation in factor analysis. Psychometrika,  Kaiser, H.F.  1959  Computer program for varimax rotation in factor analysis. Educational and  Psychological Measurement, 19:413–420.  Kalayeh, H.M. and Landgrebe, D.A.  1983  Predicting the required number of training samples.  IEEE Transactions on Pattern Analysis and Machine Intelligence, 5 6 :664–667.  Kam, M., Zau, Q. and Gray, W.S.  1992  Optimal data fusion of correlated local decisions in multiple sensor detection systems. IEEE Transactions on Aerospace and Electronic Systems, 28 3 :916–920.  Kam, M., Rorres, C., Chang, W. and Zhu, X.  1999  Performance and geometric interpretation for decision fusion with memory. IEEE Transactions on Systems, Man, and Cybernetics. Part A: Systems and Humans, 29 1 :52–62.  Kamel, M.S. and Selim, S.Z.  1991  A thresholded fuzzy c-means algorithm for semi-fuzzy  clustering. Pattern Recognition, 24 9 :825–833.  Kamel, M.S. and Selim, S.Z.  1994  New algorithms for solving the fuzzy clustering problem.  Pattern Recognition, 27 3 :421–428.  Karayiannis, N.B. and Venetsanopolous, A.N.  1993  Efﬁcient learning algorithms for neural net- works  ELEANNE . IEEE Transactions on Systems, Man, and Cybernetics, 23 5 :1372–1383. Karayiannis, N.B. and Wi, G.M.  1997  Growing radial basis neural networks: merging supervised IEEE Transactions on Neural  and unsupervised learning with network growth techniques. Networks, 8 6 :1492–1506.  Kashyap, R.L.  1970  Algorithms for pattern classiﬁcation.  In J.M. Mendel and K.S. Fu, eds, Adaptive, Learning and Pattern Recognition Systems. Theory and Applications, pp. 81–113. Academic Press, New York.  Kay, J.W.  1997  Comments on paper by Esposito et al. IEEE Transactions on Pattern Analysis  and Machine Intelligence, 19 5 :492–493.  Kendall, M.  1975  Multivariate Analysis. Grifﬁn, London. Kirkwood, C.A., Andrews, B.J. and Mowforth, P.  1989  Automatic detection of gait events: a case  study using inductive learning techniques. Journal of Biomedical Engineering, 11:511–516.  Kittler, J.  1975a  A nonlinear distance metric criterion for feature selection in the measurement  Kittler, J.  1975b  Mathematical methods of feature selection in pattern recognition. International  space. Information Sciences, 9:359–363.  Journal of Man–Machine Studies, 7:609–637.  Kittler, J.  1978a  Une g´en´eralisation de quelques algorithmes sous-optimaux de recherche In Proc. Congr`es AFCET IRIA Reconnaissance des Formes et  d’ensembles d’attributs. Traitement des Images, pp. 678–686.  Kittler, J.  1978b  Feature set search algorithms.  In C.H. Chen, ed., Pattern Recognition and  Signal Processing, pp. 41–60. Sijthoff and Noordhoff, Alphen aan den Rijn, Netherlands.  Kittler, J.  1986  Feature selection and extraction. In T.Y. Young and K.S. Fu, eds, Handbook of  Pattern Recognition and Image Processing, pp. 59-83. Academic Press, London.   476 REFERENCES  Kittler, J. and Alkoot, F.M.  2001  Relationship of sum and vote fusion strategies. In J. Kittler and F. Roli, eds, Multiple Classiﬁer SystemsLecture Notes in Computer Science 2096, pp. 339–348. Springer-Verlag, Berlin.  Kittler, J. and Young, P.C.   1973  A new approach to feature selection based on the  Karhunen–Lo`eve expansion. Pattern Recognition, 5:335–352.  Kittler, J., Matas, J., Jonsson, K. and Ramos S´anchez, M.U.  1997  Combining evidence in  personal identity veriﬁcation systems. Pattern Recognition Letters, 18:845–852.  Kittler, J., Hatef, M., Duin, R.P.W. and Matas, J.  1998  On combining classiﬁers.  IEEE  Transactions on Pattern Analysis and Machine Intelligence, 20 3 :226–239.  Kohonen, T.  1989  Self-organization and Associative Memory, 3rd edn., Springer-Verlag, Berlin. Kohonen, T.  1990  The self-organizing map. Proceedings of the IEEE, 78:1464–1480. Kohonen, T.  1997  Self-organizing Maps, 2nd edn., Springer-Verlag, Berlin. Kohonen, T., Oja, E., Simula, O., Visa, A. and Kangas, J.  1996  Engineering applications of the  self-organising map. Proceedings of the IEEE, 84 10 :1358–1384.  Konishi, S. and Honda, M.  1990  Comparison of procedures for estimation of error rates in discriminant analysis under nonnormal populations. Journal of Statistical Computation and Simulation, 36:105–115.  Koontz, W.L.G. and Fukunaga, K.  1972  A nonlinear feature extraction algorithm using distance  transformation. IEEE Transactions on Computers, 21 1 :56–63.  Koontz, W.L.G., Narendra, P.M. and Fukunaga, K.  1975  A branch and bound clustering  algorithm. IEEE Transactions on Computers, 24 9 :908–915.  Kraaijveld, M.A.  1996  A Parzen classiﬁer with an improved robustness against deviations  between training and test data. Pattern Recognition Letters, 17:679–689.  Kraaijveld, M.A., Mao, J. and Jain, A.K.  1992  A non-linear projection method based on Koho- nen’s topology preserving maps. In Proceedings of the 11th IAPR International Conference on Pattern Recognition, The Hague. IEEE Computer Society Press, Los Alamitos, CA.  Kreithen, D.E., Halversen, S.D. and Owirka, G.J.  1993  Discriminating targets from clutter.  Lincoln Laboratory Journal, 6 1 :25–51.  Kronmal, R.A. and Tarter, M.  1962  The estimation of probability densities and cumulatives by  Fourier series methods. Journal of the American Statistical Association, 63:925–952.  Krusi´nska, E.  1988  Robust methods in discriminant analysis. Rivista di Statistica Applicada,  21 3 :239–253.  29 2 :115–129.  Kruskal, J.B.  1964a  Multidimensional scaling by optimizing goodness-of-ﬁt to a nonmetric  hypothesis. Psychometrika, 29:1–28.  Kruskal, J.B.  1964b  Nonmetric multidimensional scaling: a numerical method. Psychometrika,  Kruskal, J.B.  1971  Comments on ‘A nonlinear mapping for data structure analysis’.  IEEE  Transactions on Computers, 20:1614.  Kruskal, J.B.  1972  Linear transformation of multivariate data to reveal clustering. In R.N. Shep- ard, A.K. Romney and S.B. Nerlove, eds, Multidimensional Scaling: Theory and Applications in the Behavioural Sciences., vol. 1, pp. 179–191. Seminar Press, London.  Krzanowski, W.J.  1993  The location model for mixtures of categorical and continuous variables.  Journal of Classiﬁcation, 10 1 :25–49.  Krzanowski, W.J. and Marriott, F.H.C.  1994  Multivariate Analysis. Part 1: Distributions,  Ordination and Inference. Edward Arnold, London.  Krzanowski, W.J. and Marriott, F.H.C.  1996  Multivariate Analysis. Part 2: Classiﬁcation,  Covariance Structures and Repeated Measurements. Edward Arnold, London.  Krzanowski, W.J., Jonathan, P., McCarthy, W.V. and Thomas, M.R.  1995  Discriminant analysis with singular covariance matrices: methods and applications to spectroscopic data. Applied Statistics, 44 1 :101–115.   REFERENCES 477  Krzy˙zak, A.  1983  Classiﬁcation procedures using multivariate variable kernel density estimate.  Kudo, M. and Sklansky, J.  2000  Comparison of algorithms that select features for pattern  Pattern Recognition Letters, 1:293–298.  classiﬁers. Pattern Recognition, 33:25–41.  Intelligence, 88:1–38.  Kwoh, C.-K. and Gillies, D.F.  1996  Using hidden nodes in Bayesian networks. Artiﬁcial  Kwok, T.-Y. and Yeung, D.-Y.  1996  Use of bias term in projection pursuit learning improves approx- imation and convergence properties. IEEE Transactions on Neural Networks, 7 5 :1168–1183. Lachenbruch, P.A.  1966  Discriminant analysis when the initial samples are misclassiﬁed.  Lachenbruch, P.A. and Mickey, M.R.  1968  Estimation of error rates in discriminant analysis.  Technometrics, 8:657–662.  Technometrics, 10:1–11.  Lachenbruch, P.A., Sneeringer, C. and Revo, L.T.  1973  Robustness of the linear and quadratic dis- criminant function to certain types of non-normality. Communications in Statistics, 1 1 :39–56. Lam, L. and Suen, C.Y.  1995  Optimal combinations of pattern classiﬁers. Pattern Recognition  Lampinen, J. and Vehtari, A.  2001  Bayesian approach for neural networks – review and case  Letters, 16:945–954.  studies. Neural Networks, 14:257–274.  Lange, K.  1995  A gradient algorithm locally equivalent to the EM algorithm. Journal of the  Royal Statistical Society Series B, 57 2 :425–437.  Lauritzen, S.L. and Spiegelhalter, D.J.  1988  Local computations with probabilities on graphical structures and their application to expert systems  with discussion . Journal of the Royal Statistical Society Series B, 50:157–224.  Lavine, M. and West, M.  1992  A Bayesian method for classiﬁcation and discrimination.  Canadian Journal of Statistics, 20 4 :451–461.  Laviolette, M., Seaman, J.W., Barrett, J.D. and Woodall, W.H.  1995  A probabilistic and  statistical view of fuzzy methods  with discussion . Technometrics, 37 3 :249–292.  Lawley, D.N. and Maxwell, A.E.  1971  Factor Analysis as a Statistical Method, 2nd edn.,  Butterworths, London.  Lee, J.S., Grunes, M.R. and Kwok, R.  1994  Classiﬁcation of multi-look polarimetric SAR International Journal of Remote Sensing,  imagery based on complex Wishart distribution. 15 11 :2299–2311.  Lerner, B., Guterman, H., Aladjem, M. and Dinstein, I.  1999  A comparative study of neural  network based feature extraction paradigms. Pattern Recognition Letters, 20:7–14.  Leshno, M., Lin, V.Y., Pinkus, A. and Schocken, S.  1993  Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks, 6:861–867. Li, C., Goldgof, D.B. and Hall, L.O.  1993  Knowledge-based classiﬁcation and tissue labeling  of MR images of human brain. IEEE Transactions on Medical Imaging, 12 4 :740–750.  Li, T. and Sethi, I.K.  1993  Optimal multiple level decision fusion with distributed sensors.  IEEE Transaction on Aerospace and Electronic Systems, 29 4 :1252–1259.  Li, X. and Yeh, A.G.O.  1998  Principal component analysis of stacked multi-temporal images for the monitoring of rapid urban expansion in the Pearl River Delta. International Journal of Remote Sensing, 19 8 :1501–1518.  Lin, Y., Lee, Y. and Wahba, G.  2002  Support vector machines for classiﬁcation in nonstandard  situations. Machine Learning, 46 1–3 :191–202.  Linde, Y., Buzo, A. and Gray, R.M.  1980  An algorithm for vector quantizer design.  IEEE  Transactions on Communications, 28 1 :84–95.  Lindgren, B.W.  1976  Statistical Theory, 3rd edn., Macmillan, New York. Lindsay, B.G. and Basak, P.  1993  Multivariate normal mixtures: a fast consistent method of  moments. Journal of the American Statistical Association, 88:468–476.   478 REFERENCES  Ling, R.F.  1973  A probability theory for cluster analysis. Journal of the American Statistical  Lingoes, J.C., Roskam, E.E. and Borg, I., eds  1979  Geometric Representations of Relational  Association, 68:159–164.  Data. Mathesis Press, Ann Arbor, MI.  Association, 87:1227–1237.  Little, R.J.A.  1992  Regression with missing X’s: a review. Journal of the American Statistical  Little, R.J.A. and Rubin, D.B.  1987  Statistical Analysis with Missing Data. Wiley, New York. Liu, C. and Rubin, D.B.  1994  The ECME algorithm: a simple extension of EM and ECM with  faster monotone convergence. Biometrika, 81 4 :633–648.  Liu, H.-L., Chen, N.-Y., Lu, W.-C. and Zhu, X.-W.  1994  Multi-target classiﬁcation pattern recognition applied to computer-aided materials design. Analytical Letters, 27 11 :2195–2203. Liu, J.N.K., Li, B.N.L. and Dillon, T.S.  2001  An improved na¨ıve Bayesian classiﬁer technique IEEE Transactions on Systems, Man, and  coupled with a novel input solution method. Cybernetics – Part C: Applications and Reviews, 31 2 :249–256.  Liu, K., Cheng, Y.Q. and Yang, J.-Y.  1992  A generalized optimal set of discriminant vectors.  Pattern Recognition, 25 7 :731–739.  Liu, K., Cheng, Y.Q. and Yang, J.-Y.  1993  Algebraic feature extraction for image recognition  based on an optimal discriminant criterion. Pattern Recognition, 26 6 :903–911.  Liu, W.Z. and White, A.P.  1995  A comparison of nearest neighbour and tree-based methods of non-parametric discriminant analysis. Journal of Statistical Computation and Simulation, 53:41–50.  Liu, W.Z., White, A.P., Thompson, S.G. and Bramer, M.A.  1997  Techniques for dealing with In X. Liu, P. Cohen and M. Berthold, eds, Advances in  missing values in classiﬁcation. Intelligent Data Analysis. Springer-Verlag, Berlin.  Logar, A.M., Corwin, E.M. and Oldham, W.J.B.  1994  Performance comparisons of classiﬁcation International Journal of Human–Computer  techniques for multi-font character recognition. Studies, 40:403–423.  Loh, W.-L.  1995  On linear discriminant analysis with adaptive ridge classiﬁcation rules. Journal  of Multivariate Analysis, 53:264–278.  Loh, W.-Y. and Vanichsetakul, N.  1988  Tree-structured classiﬁcation via generalized discriminant  analysis  with discussion . Journal of the American Statistical Association, 83:715–727.  Lowe, D.  1993  Novel ‘topographic’ nonlinear feature extraction using radial basis functions for concentration coding on the ‘artiﬁcial nose’. In 3rd IEE International Conference on Artiﬁcial Neural Networks, pp. 95–99, IEE, London.  Lowe, D., ed.  1994  Special  issue on ‘Applications of Artiﬁcial Neural Networks’.  IEE  Proceedings on Vision, Image and Signal Processing, 141 4 .  Lowe, D.  1995a  Radial basis function networks. In M.A. Arbib, ed., The Handbook of Brain  Theory and Neural Networks, pp. 779–782. MIT Press, Cambridge, MA.  Lowe, D.  1995b  On the use of nonlocal and non positive deﬁnite basis functions in radial basis In Proceedings of the 4th International Conference on Artiﬁcial Neural  function networks. Networks, Cambridge, pp. 206–211, IEE, London.  Lowe, D. and Tipping, M.  1996  Feed-forward neural networks and topographic mappings for  exploratory data analysis. Neural Computing and Applications, 4:83–95.  Lowe, D. and Webb, A.R.  1990  Exploiting prior knowledge in network optimization: an  illustration from medical prognosis. Network, 1:299–323.  Lowe, D. and Webb, A.R.  1991  Optimized feature extraction and the Bayes decision in IEEE Transactions on Pattern Analysis and Machine  feed-forward classiﬁer networks. Intelligence, 13 4 :355–364.  Lugosi, G.  1992  Learning with an unreliable teacher. Pattern Recognition, 25 1 :79–87. Lunn, D.J., Thomas, A., Best, N. and Spiegelhalter, D.J.  2000  WinBugs – a Bayesian modelling  framework: concepts, structure and extensibility. Statistics and Computing, 10 4 :325–337. Luttrell, S.P.  1989  Hierarchical vector quantisation. IEE Proceedings Part I, 136 6 :405–413.   REFERENCES 479  Luttrell, S.P.  1994  Partitioned mixture distribution: an adaptive Bayesian network for low-level image processing. IEE Proceedings on Vision, Image and Signal Processing, 141 4 :251–260. Luttrell, S.P.  1995  Using self-organising maps to classify radar range proﬁles. In Proceedings of the 4th International Conference on Artiﬁcial Neural Networks, Cambridge, pp. 335–340, IEE, London.  Luttrell, S.P.  1997  A theory of self-organising neural networks. In S.W. Ellacott, J.C. Mason and I.J. Anderson, eds, Mathematics of Neural Networks: Models, Algorithms and Applications, pp. 240–244. Kluwer Academic Publishers, Dordrecht.  Luttrell, S.P.  1999a  Self-organised modular neural networks for encoding data. In A.J.C. Sharkey, ed., Combining Artiﬁcial Neural Nets: Ensemble and Modular Multi-net Systems, pp. 235–263. Springer-Verlag, London.  Luttrell, S.P.  1999b  An adaptive network for encoding data using piecewise linear functions. In Proceedings of the 9th International Conference on Artiﬁcial Neural Networks  ICANN99 , pp. 198–203.  Luttrell, S.P.  2002 . Using stochastic vectors quantizers to characterize signal and noise In J.G. McWhirter and I.K. Proudler, eds, Mathematics in Signal Processing.  subspaces. Oxford University Press, Oxford.  MacKay, D.J.C.  1995  Probable networks and plausible predictions – a review of practical Bayesian methods for supervised neural networks. Network: Computation in Neural Systems, 6:469–505.  Magee, M., Weniger, R. and Wenzel, D.  1993  Multidimensional pattern classiﬁcation of bottles  using diffuse and specular illumination. Pattern Recognition, 26 11 :1639–1654.  Makhoul, J., Roucos, S. and Gish, H.  1985  Vector quantization in speech coding. Proceedings  of the IEEE, 73 11 :1511–1588.  Manly, B.F.J.  1986  Multivariate Statistical Methods, a Primer. Chapman & Hall, London. Mao, J. and Jain, A.K.  1995  Artiﬁcial neural networks for feature extraction and multivariate  data projection. IEEE Transactions on Neural Networks, 6 2 :296–317.  Marinaro, M. and Scarpetta, S.  2000  On-line learning in RBF neural networks: a stochastic  approach. Neural Networks, 13:719–729.  Marron, J.S.  1988  Automatic smoothing parameter selection: a survey. Empirical Economics,  13:187–208.  Matsuoka, K.  1992  Noise injection into inputs in back-propagation learning. IEEE Transactions  on Systems, Man, and Cybernetics, 22 3 :436–440.  McIntyre, R.M. and Blashﬁeld, R.K.  1980  A nearest-centroid technique for evaluating the  minimum-variance clustering procedure. Multivariate Behavioral Research, 2:225–238.  McKenna, S.J., Gong, S. and Raja, Y.  1998  Modelling facial colour and identity with Gaussian  mixtures. Pattern Recognition, 31 12 :1883–1892.  McLachlan, G.J.  1987  Error rate estimation in discriminant analysis: recent advances.  In  A.K. Gupta, ed., Advances in Multivariate Analysis. D. Reidel, Dordrecht.  McLachlan, G.J.  1992a  Discriminant Analysis and Statistical Pattern Recognition. Wiley, New  York.  McLachlan, G.J.  1992b  Cluster analysis and related techniques in medical research. Statistical  McLachlan, G.J. and Basford, K.E.  1988  Mixture Models: Inference and Applications to  Methods in Medical Research, 1 1 :27–48.  Clustering. Marcel Dekker, New York.  McLachlan, G.J. and Krishnan, T.  1996  The EM Algorithm and Extensions. Wiley, New York. McLachlan, G.J. and Peel, D.  2000  Finite Mixture Models. Wiley, New York. Meier, W., Weber, R. and Zimmermann, H.-J.  1994  Fuzzy data analysis – methods and industrial  applications. Fuzzy Sets and Systems, 61:19–28.  Meinguet, J. Multivariate interpolation at arbitrary points made simple. Zeitschrift F¨ur Angewandte  Mathematik and Physik, 30:292–304.   480 REFERENCES  Meng, X.-L. and Rubin, D.B.  1992  Recent extensions to the EM algorithm. In J.M. Bernado, J.O. Berger, A.P. Dawid and A.F.M. Smith, eds, Bayesian Statistics 4, pp. 307–320. Oxford University Press, Oxford.  Meng, X.-L. and Rubin, D.B.  1993  Maximum likelihood estimation via the ECM algorithm: a  general framework. Biometrika, 80 2 :267–27.  Meng, X.-L. and van Dyk, D.  1997  The EM algorithm – an old folk-song sung to a fast new  tune  with discussion . Journal of the Royal Statistical Society Series B, 59 3 :511–567.  Mengersen, K.L., Robert, C.P. and Guihenneuc-Jouyaux, C.  1999  MCMC convergence diagnostics: a reviewww. In J.M. Bernardo, J.O. Berger, A.P. Dawid and A.F.M. Smith, eds, Bayesian Statistics 6, pp. 399–432. Oxford University Press, Oxford.  Merritt, D. and Tremblay, B.  1994  Nonparametric estimation of density proﬁles. Astronomical  Journal, 108 2 :514–537.  36:33–58.  Merz, C.L.  1999  Using correspondence analysis to combine classiﬁers. Machine Learning,  Micchelli, C.A.  1986  Interpolation of scattered data: distance matrices and conditionally positive  deﬁnite matrices. Constructive Approximation, 2:11–22.  Michael, M. and Lin, W.-C.  1973  Experimental study of information measure and inter-intra class distance ratios on feature selection and orderings. IEEE Transactions on Systems, Man, and Cybernetics, 3 2 :172–181.  Michalek, J.E. and Tripathi, R.C.  1980  The effect of errors in diagnosis and measurement on the estimation of the probability of an event. Journal of the American Statistical Association, 75:713–721.  Michalewicz, Z.  1994  Non-standard methods in evolutionary computation.  Statistics and  Computing, 4:141–155.  Michie, D., Spiegelhalter, D.J. and Taylor, C.C.  1994  Machine Learning, Neural and Statistical  Classiﬁcation. Ellis Horwood Limited, Hemel Hempstead.  Mic´o, M.L., Oncina, J. and Vidal, E.  1994  A new version of the nearest-neighbour approxi- mating and eliminating search algorithm  AESA  with linear preprocessing time and memory requirements. Pattern Recognition, 15:9–17.  Miller, A.J.  1990  Subset Selection in Regression. Chapman & Hall, London. Milligan, G.W.  1981  A Monte Carlo study of thirty internal measures for cluster analysis.  Psychometrika, 46 2 :187–199.  Milligan, G.W. and Cooper, M.C.  1985  An examination of procedures for determining the  number of clusters in a data set. Psychometrika, 50 2 :159–179.  Mingers, J.  1989  An empirical comparison of pruning methods for decision tree inductions.  Minsky, M.L. and Papert, S.A.  1988  Perceptrons. An Introduction to Computational Geometry.  Machine Learning, 4:227–243.  MIT Press, Cambridge, MA.  Mitchell, T.M.  1997  Machine Learning. McGraw-Hill, New York. Mkhadri, A.  1995  Shrinkage parameter for the modiﬁed linear discriminant analysis. Pattern  Recognition Letters, 16:267–275.  Mkhadri, A., Celeux, G. and Nasroallah, A.  1997  Regularization in discriminant analysis: an  overview. Computational Statistics and Data Analysis, 23:403–423.  Mojena, R.  1977  Hierarchical grouping methods and stopping rules: an evaluation. Computer  Mojirsheibani, M.  1999  Combining classiﬁers via discretization.  Journal of the American  Statistical Association, 94 446 :600–609.  Mola, F. and Siciliano, R.  1997  A fast splitting procedure for classiﬁcation trees. Statistics and  Journal, 20:359–363.  Computing, 7:209–216.  Moran, M.A. and Murphy, B.J.  1979  A closer look at two alternative methods of statistical  discrimination. Applied Statistics, 28 3 :223–232.   REFERENCES 481  Morgan, B.J.T. and Ray, A.P.G.  1995  Non-uniqueness and inversions in cluster analysis.  Applied Statistics, 44 1 :117–134.  Morgan, N. and Bourlard, H.A.  1995  Neural networks for the statistical recognition of  continuous speech. Proceedings of the IEEE, 83 5 :742–772.  Mukherjee, D.P., Banerjee, D.K., Uma Shankar, B. and Majumder, D.D.  1994  Coal petrography:  a pattern recognition approach. International Journal of Coal Geology, 25:155–169.  Mukherjee, D.P., Pal, A., Sarma, S.E. and Majumder, D.D.  1995  Water quality analysis: a  pattern recognition approach. Pattern Recognition, 28 2 :269–281.  Munro, D.J., Ersoy, O.K., Bell, M.R. and Sadowsky, J.S.  1996  Neural network learning of low-  probability events. IEEE Transactions on Aerospace and Electronic Systems, 32 3 :898–910. Murphy, P.M. and Aha, D.W.  1995  UCI repository of machine learning databases. Technical Irvine. http:  www.ics.uci.edu n~mlearn   Report University of California, MLRepository.html.  Murtagh, F.  1985  Multidimensional Clustering Algorithms. Physica-Verlag, Vienna. Murtagh, F.  1992  Contiguity-constrained clustering for image analysis. Pattern Recognition  Letters, 13:677–683.  Murtagh, F.  1994  Classiﬁcation: astronomical and mathematical overview. In H.T. MacGillivray et al., ed., Astronomy from Wide-Field Imaging, pp. 227–233, Kluwer Academic Publishers, Dordrecht.  Murtagh, F.  1995   Interpreting the Kohonen self-organizing feature map using contiguity-  constrained clustering. Pattern Recognition Letters, 16:399–408.  Murtagh, F. and Hern´andez-Pajares, M.  1995  The Kohonen self-organizing map method: an  assessment. Journal of Classiﬁcation, 12 2 :165–190.  Musavi, M.T., Ahmed, W., Chan, K.H., Faris, K.B. and Hummels, D.M.  1992  On the training  of radial basis function classiﬁers. Neural Networks, 5:595–603.  Myles, J.P. and Hand, D.J.  1990   The multi-class metric problem in nearest neighbour  discrimination rules. Pattern Recognition, 23 11 :1291–1297.  Nadaraya, E.A.  1989  Nonparametric Estimation of Probability Densities and Regression Curves.  Kluwer Academic Publishers, Dordrecht.  Narendra, P.M. and Fukunaga, K.  1977  A branch and bound algorithm for feature subset  selection. IEEE Transactions on Computers, 26:917–922.  Neapolitan, R.E.  1990  Probabilistic Reasoning in Expert Systems: Theory and Algorithms.  Wiley, New York.  Nie, N.H., Hull, C.H., Jenkins, J.G., Steinbrenner, K. and Brent, D.H.  1975  SPSS: Statistical  Package for the Social Sciences, 2nd edn., McGraw-Hill, New York.  Niemann, H. and Weiss, J.  1979  A fast-converging algorithm for nonlinear mapping of high  dimensional data to a plane. IEEE Transactions on Computers, 28 2 :142–147.  Nilsson, N.J.  1965  Learning Machines: Foundations of Trainable Pattern-Classifying Systems.  Novovi˘cov´a, J., Pudil, P. and Kittler, J.  1996  Divergence based feature selection for mul- IEEE Transactions on Pattern Analysis and Machine Intelligence,  McGraw-Hill, New York.  timodal class densities. 18 2 :218–223.  O’Hagan, A.  1994  Bayesian Inference. Edward Arnold, London. Okada, T. and Tomita, S.  1985  An optimal orthonormal system for discriminant analysis.  Oliver, J.J. and Hand, D.J.  1996  Averaging over decision trees.  Journal of Classiﬁcation,  Pattern Recognition, 18 2 :139–144.  13 2 :281–297.  Olkin, I. and Tate, R.F.  1961  Multivariate correlation models with mixed discrete and continuous  variables. Annals of Mathematical Statistics, 22:92–96.  O’Neill, T.J.  1992  Error rates of non-Bayes classiﬁcation rules and the robustness of Fisher’s  linear discriminant function. Biometrika, 79 1 :177–184.   482 REFERENCES  Orr, M.J.L.  1995  Regularisation in the selection of radial basis function centers. Neural  Computation, 7:606–623.  Osuna, E., Freund, R. and Girosi, F.  1997  Training support vector machines: an application to face detection. In Proceedings of 1997 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 130–136, IEEE Computer Society Press, Los Alamitos, CA.  Overall, J.E. and Magee, K.N.  1992  Replication as a rule for determining the number of clusters  in a hierarchical cluster analysis. Applied Psychological Measurement, 16:119–128.  Pal, N.R. and Bezdek, J.C.  1995  On cluster validity for the fuzzy c-means model.  IEEE  Transactions on Fuzzy Systems, 3 3 :370–379.  Adaptive Pattern Recognition and Neural Networks. Addison-Wesley,  Park, J. and Sandberg, I.W.  1993  Approximation and radial-basis-function networks. Neural  Pao, Y.-H.  1989  Reading, MA.  Computation, 5:305–316.  Parzen, E.  1962  On estimation of a probability density function and mode. Annals of  Pawlak, M.  1993  Kernel classiﬁcation rules from missing data.  IEEE Transactions on  Mathematical Statistics, 33:1065–1076.  Information Theory, 39 3 :979–988.  Pearl, J.  1988  Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.  Morgan Kaufmann Publishers, San Mateo, CA.  Pearson, K.  1901  On lines and planes of closest ﬁt to systems of points in space. Philosophical  Magazine, 2:559–572.  Pedersen, F., Bergstr¨om, M., Bengtsson, E. and Langstr¨om, B.  1994  Principal component analysis of dynamic positron emission tomography images. European Journal of Nuclear Medicine, 21 12 :1285–1292.  Peel, D. and McLachlan, G.J.  2000  Robust mixture modelling using the t distribution. Statistics  Pinkowski, B.  1997  Principal component analysis of speech spectrogram images. Pattern  and Computing, 10 4 :339–348.  Recognition, 30 5 :777–787.  Platt, J.  1998  Fast training of support vector machines using sequential minimal optimisation. In B. Sch¨olkopf, C.J.C. Burges and A.J. Smola, eds, Advances in Kernel Methods: Support Vector Learning, pp. 185–208. MIT Press, Cambridge, MA.  Powell, G.E., Clark, E. and Bailey, S.  1979  Categories of aphasia: a cluster-analysis of Schuell  test proﬁles. British Journal of Disorders of Communication, 14 2 :111–122.  Powell, M.J.D.  1987  Radial basis functions for multivariable interpolation: a review.  In J.C. Mason and M.G. Cox, eds, Algorithms for Approximation, pp. 143–167. Clarendon Press, Oxford.  Prabhakar, S. and Jain, A.K.  2002  Decision-level fusion in ﬁngerprint veriﬁcation. Pattern  Recognition, 35:861–874.  Prakash, M. and Murty, M.N.  1995  A genetic approach for selection of  near- optimal subsets  of principal components for discrimination. Pattern Recognition Letters, 16:781–787.  Press, S.J. and Wilson, S.  1978  Choosing between logistic regression and discriminant analysis.  Journal of the American Statistical Association, 73:699–705.  Press, W.H., Flannery, B.P., Teukolsky, S.A. and Vetterling, W.T.  1992  Numerical Recipes. The  Art of Scientiﬁc Computing, 2nd edn., Cambridge University Press, Cambridge.  Provost, F. and Fawcett, T.  2001  Robust classiﬁcation for imprecise environments. Machine  Learning, 42:203–231.  Psaltis, D., Snapp, R.R. and Venkatesh, S.S.  1994  On the ﬁnite sample performance of the  nearest neighbor classiﬁer. IEEE Transactions on Information Theory, 40 3 :820–837.  Pudil, P., Ferri, F.J., Novovi˘cov´a, J. and Kittler, J.  1994a   Floating search methods for feature selection with nonmonotonic criterion functions. In Proceedings of the International Conference on Pattern Recognition, vol. 2, pp. 279–283, IEEE, Los Alamitos, CA.   REFERENCES 483  Pudil, P., Novovi˘cov´a, J. and Kittler, J.  1994b  Floating search methods in feature selection.  Pattern Recognition Letters, 15:1119–1125.  Pudil, P., Novovi˘cov´a, J. and Kittler, J.  1994c   and important attributes for classiﬁcation problems in image analysis. Computing, 12 3 :193–198.  Simultaneous learning of decision rules Image and Vision  Pudil, P., Novovi˘cov´a, J., Choakjarernwanit, N. and Kittler, J.  1995  Feature selection based on the approximation of class densities by ﬁnite mixtures of special type. Pattern Recognition, 28 9 :1389–1398.  Quenouille, M.H.  1949  Approximate tests of correlation in time series. Journal of the Royal  Statistical Society Series B, 11:68–84.  Quinlan, J.R.  1986  Induction of decision trees. Machine Learning, 1 1 :81–106. Quinlan, J.R.  1987  Simplifying decision trees. International Journal of Man–Machine Studies,  27:221–234.  Quinlan, J.R. and Rivest, R.L.  1989   Inferring decision trees using the minimum description  length principle. Information and Computation, 80:227–248.  Rabiner, L.R., Juang, B.-H., Levinson, S.E. and Sondhi, M.M.  1985  Recognition of isolated digits using hidden Markov models with continuous mixture densities. AT&T Technical Journal, 64 4 :1211–1234.  Raftery, A.E. and Lewis, S.M.  1996  Implementing MCMC. In W.R. Gilks, S. Richardson and D.J. Spiegelhalter, eds, Markov Chain Monte Carlo in Practice, pp. 115–130. Chapman & Hall, London.  Raju, S. and Sarma, V.V.S.  1991  Multisensor data fusion and decision support for airborne  target identiﬁcation. IEEE Transactions on Systems, Man, and Cybernetics, 21 5 .  Ramasubramanian, V. and Paliwal, K.K.  2000  Fast nearest-neighbor search algorithms based  on approximation-elimination search. Pattern Recognition, 33:1497–1510.  Ramaswamy, S., Tamayo, P., Rifkin, R., Mukherjee, S., Yeang, C.-H., Angelo, M., Ladd, C., Reich, M., Latulippe, E., Mesirov, J.P., Poggio, T., Gerald, W., Loda, M., Lander, E.S. and Golub, T.R.  2001  Multiclass cancer diagnosis using tumor gene expression signatures. Proceedings of the National Academy of Sciences of the USA, 98 26 :15149–15154.  Ramsay, J.O. and Dalzell, C.J.  1991  Some tools for functional data analysis  with discussion .  Journal of the Royal Statistical Society Series B, 53:539–572.  Ratcliffe, M.B., Gupta, K.B., Streicher, J.T., Savage, E.B., Bogen, D.K. and Edmunds, L.H.  1995  Use of sonomicrometry and multidimensional scaling to determine the three-dimensional coordinates of multiple cardiac locations: feasibility and initial IEEE Transactions on Biomedical Engineering, 42 6 :587–598.  implementation.  Raudys, S.J.  2000  Scaled rotation regularisation. Pattern Recognition, 33:1989–1998. Rayens, W. and Greene, T.  1991  Covariance pooling and stabilization for classiﬁcation.  Computational Statistics and Data Analysis, 11:17–42.  Redner, R.A. and Walker, H.F.  1984  Mixture densities, maximum likelihood and the EM  algorithm. SIAM Review, 26 2 :195–239.  Reed, R.  1993  4 5 :740–747.  Pruning algorithms – a survey.  IEEE Transactions on Neural Networks,  Refenes, A.-P.N., Burgess, A.N. and Bentz, Y.  1997  Neural networks in ﬁnancial engineering:  a study in methodology. IEEE Transactions on Neural Networks, 8 6 :1222–1267.  Remme, J., Habbema, J.D.F. and Hermans, J.  1980  A simulative comparison of linear, quadratic  and kernel discrimination. Journal of Statistical Computation and Simulation, 11:87–106.  Revow, M., Williams, C.K.I. and Hinton, G.E.  1996  Using generative models for handwritten digit recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18 6 :592–606. Reyment, R.A., Blackith, R.E. and Campbell, N.A.  1984  Multivariate Morphometrics, 2nd edn.,  Academic Press, New York.  Rice, J.A. and Silverman, B.W.  1991  Estimating the mean and covariance structure nonparamet- rically when the data are curves. Journal of the Royal Statistical Society Series B, 53:233–243.   484 REFERENCES  Rice, J.C.  1993  Forecasting abundance from habitat measures using nonparametric density  estimation methods. Canadian Journal of Fisheries and Aquatic Sciences, 50:1690–1698.  Richardson, S. and Green, P.J.  1997  On Bayesian analysis of mixtures with an unknown number of components  with discussion . Journal of the Royal Statistical Society Series B, 59 4 :731–792.  Ripley, B.D.  1987  Stochastic Simulation. Wiley, New York. Ripley, B.D.  1994  Neural and related methods of classiﬁcation. Journal of the Royal Statistical  Ripley, B.D.  1996  Pattern Recognition and Neural Networks. Cambridge University Press,  Society Series B, 56 3 .  Cambridge.  Riskin, E.A. and Gray, R.M.  1991  A greedy tree growing algorithm for the design of variable  rate vector quantizers. IEEE Transactions on Signal Processing.  Ritter, H., Martinetz, T. and Schulten, K.  1992  Neural Computation and Self-Organizing Maps:  An Introduction. Addison-Wesley, Reading, MA.  Roberts, G.O.  1996  Markov chain concepts related to sampling algorithms.  In W.R. Gilks, S. Richardson and D.J. Spiegelhalter, eds, Markov Chain Monte Carlo in Practice, pp. 45–57. Chapman & Hall, London.  Roberts, S. and Tarassenko, L.  1992  Analysis of the sleep EEG using a multilayer network with  spatial organisation. IEE Proceedings Part F, 139 6 :420–425.  Rocke, D.M. and Woodruff, D.L.  1997  Robust estimation of multivariate location and shape.  Journal of Statistical Planning and Inference, 57:245–255.  Rogers, S.K., Colombi, J.M., Martin, C.E., Gainey, J.C., Fielding, K.H., Burns, T.J., Ruck, D.W., Kabrisky, M. and Oxley, M.  1995  Neural networks for automatic target recognition. Neural Networks, 8 7 8 :1153–1184.  Rohlf, F.J.  1982  Single-link clustering algorithms.  In P.R. Krishnaiah and L.N. Kanal, eds,  Handbook of Statistics, vol. 2, pp. 267–284. North Holland, Amsterdam.  Rosenblatt, M.  1956  Remarks on some nonparametric estimates of a density function. Annals  of Mathematical Statistics, 27:832–835.  Roth, M.W.  1990  Survey of neural network technology for automatic target recognition. IEEE  Transactions on Neural Networks, 1 1 :28–43.  Rousseeuw, P.J.  1985  Multivariate estimation with high breakdown point. In W. Grossmann, G. Pﬂug, I. Vincze and W. Wertz, eds, Mathematical Statistics and Applications, pp. 283–297. Reidel, Dordrecht.  Rumelhart, D.E., Hinton, G.E. and Williams, R.J.  1986  Learning internal representation by error propagation. In D.E. Rumelhart, J.L. McClelland and the PDP Research Group, eds, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. 1, pp. 318–362. MIT Press, Cambridge, MA.  Safavian, S.R. and Landgrebe, D.A.  1991  A survey of decision tree classiﬁer methodology.  IEEE Transactions on Systems, Man, and Cybernetics, 21 3 :660–674.  Samal, A. and Iyengar, P.A.  1992  Automatic recognition and analysis of human faces and facial  expressions: a survey. Pattern Recognition, 25:65–77.  Sammon, J.W.  1969  A nonlinear mapping for data structure analysis.  IEEE Transactions on  Computers, 18 5 :401–409.  Sankar, A. and Mammone, R.J.  1991  Combining neural networks and decision trees.  In  S.K. Rogers, ed., Applications of Neural Networks II, vol. 1469, pp. 374–383. SPIE.  Saranli, A. and Demirekler, M.  2001  A statistical framework for rank-based multiple classiﬁer  decision fusion. Pattern Recognition, 34:865–884.  Schaffer, C.  1993  Selecting a classiﬁcation method by cross-validation. Machine Learning,  13:135–143.  Schalkoff, R.  1992  Pattern Recognition. Statistical Structural and Neural. Wiley, New York. Schapire, R.E.  1990  The strength of weak learnability. Machine Learning, 5 2 :197–227.   REFERENCES 485  Schapire, R.E. and Singer, Y.  1999   Improved boosting algorithms using conﬁdence-rated  predictions. Machine Learning, 37:297–336.  Schiffman, S.S., Reynolds, M.L. and Young, F.W.  1981  An Introduction to Multidimensional  Scaling. Academic Press, New York. Sch¨olkopf, B. and Smola, A.J.  2001   Learning with Kernels. Support Vector Machines,  Regularization, Optimization and Beyond. MIT Press, Cambridge, MA.  Sch¨olkopf, B., Sung, K.-K., Burges, C.J.C., Girosi, F., Niyogi, P., Poggio, T. and Vapnik, V.  1997  Comparing support vector machines with Gaussian kernels to radial basis function classiﬁers. IEEE Transactions on Signal Processing, 45 11 :2758–2765.  Sch¨olkopf, B., Smola, A.J. and M¨uller, K.  1999  Kernel principal component analysis.  In B. Sch¨olkopf, C.J.C. Burges and A.J. Smola, eds, Advances in Kernel Methods – Support Vector Learning., pp. 327–352. MIT Press, Cambridge, MA.  Sch¨olkopf, B., Smola, A.J., Williamson, R.C. and Bartlett, P.L.  2000  New support vector  algorithms. Neural Computation, 12:1207–1245.  Schott, J.R.  1993  Dimensionality reduction in quadratic discriminant analysis. Computational  Schwenker, F., Kestler, H.A. and Palm, G.  2001  Three learning phases for radial-basis-function  Statistics and Data Analysis, 16:161–174.  networks. Neural Networks, 14:439–458.  analysis. Psychometrika, 52 3 :333–343.  New York.  Sclove, S.L.  1987  Application of model selection criteria to some problems in multivariate  Scott, D.W.  1992  Multivariate Density Estimation. Theory, Practice and Visualization. Wiley,  Scott, D.W., Gotto, A.M., Cole, J.S. and Gorry, G.A.  1978  Plasma lipids as collateral risk factors in coronary artery disease – a study of 371 males with chest pains. Journal of Chronic Diseases, 31:337–345.  Sebestyen, G. and Edie, J.  1966  An algorithm for non-parametric pattern recognition.  IEEE  Transactions on Electronic Computers, 15 6 :908–915.  Selim, S.Z. and Al-Sultan, K.S.  1991  A simulated annealing algorithm for the clustering  problem. Pattern Recognition, 24 10 :1003–1008.  Selim, S.Z. and Ismail, M.A.  1984a  K -means-type algorithms: a generalized convergence theorem and characterization of local optimality. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6 1 :81–87.  Selim, S.Z. and Ismail, M.A.  1984b  Soft clustering of multidimensional data: a semi-fuzzy  approach. Pattern Recognition, 17 5 :559–568.  Selim, S.Z. and Ismail, M.A.  1986  On the local optimality of the fuzzy isodata cluster- IEEE Transactions on Pattern Analysis and Machine Intelligence, 8 2 :  ing algorithm. 284–288.  Sephton, P.S.  1994  Cointegration tests on MARS. Computational Economics, 7:23–35. Serpico, S.B., Bruzzone, L. and Roli, F.  1996  An experimental comparison of neural and statistical non-parametric algorithms for supervised classiﬁcation of remote-sensing images. Pattern Recognition Letters, 17:1331–1341.  Sethi, I.K. and Yoo, J.H.  1994  Design of multicategory multifeature split decision trees using  perceptron learning. Pattern Recognition, 27 7 :939–947.  Sharkey, A.J.C.  1999  Multi-net systems.  In A.J.C. Sharkey, ed., Combining Artiﬁcial Neural  Nets. Ensemble and Modular Multi-net Systems, pp. 1–30. Springer-Verlag, Berlin.  Sharkey, A.J.C., Chandroth, G.O. and Sharkey, N.E.  2000  A multi-net system for the fault  diagnosis of a diesel engine. Neural Computing and Applications, 9:152–160.  Shavlik, J.W., Mooney, R.J. and Towell, G.G.  1991  Symbolic and neural learning algorithms:  an experimental comparison. Machine Learning, 6:111–143.  Sheather, S.J. and Jones, M.C.  1991  A reliable data-based bandwidth selection method for  kernel density estimation. Journal of the Royal Statistical Society Series B, 53:683–690.   486 REFERENCES  Sibson, R.  1973  Slink: an optimally efﬁcient algorithm for the single-link cluster method.  Computer Journal, 16 1 :30–34.  Siedlecki, W., Siedlecka, K. and Sklansky, J.  1988  An overview of mapping techniques for  exploratory pattern analysis. Pattern Recognition, 21 5 :411–429.  Siedlecki, W. and Sklansky, J.  1988  On automatic feature selection. International Journal of  Pattern Recognition and Artiﬁcial Intelligence, 2 2 :197–220.  Silverman, B.W.  1982  Kernel density estimation using the fast Fourier transform. Applied  Silverman, B.W.  1986  Density Estimation for Statistics and Data Analysis. Chapman & Hall,  Statistics, 31:93–99.  London.  Silverman, B.W.  1995   Incorporating parametric effects into functional principal components  analysis. Journal of the Royal Statistical Society Series B, 57 4 :673–689.  Simpson, P., ed.,  1992  Special issue on ‘Neural Networks for Oceanic Engineering’.  IEEE  Journal of Oceanic Engineering.  Skurichina, M.  2001  Stabilizing Weak Classiﬁers. Technical University of Delft, Delft. Smith, S.J., Bourgoin, M.O., Sims, K. and Voorhees, H.L.  1994  Handwritten character IEEE Transactions on Pattern  classiﬁcation using nearest neighbour in large databases. Analysis and Machine Intelligence, 16 9 :915–919.  Smyth, P. and Wolpert  1999  Linearly combining density estimators via stacking. Machine  Learning, 36:59–83.  Sneath, P.H.A. and Sokal, R.R.  1973  Numerical Taxonomy. Freeman, San Francisco. Somol, P., Pudil, P., Novovi˘cov´a, J. and Pacl´ık  1999  Adaptive ﬂoating search methods in  feature selection. Pattern Recognition Letters, 20:1157–1163.  Sorsa, T., Koivo, H.N. and Koivisto, H.  1991  Neural networks in process fault diagnosis. IEEE  Transactions on Systems, Man, and Cybernetics, 21 4 :815–825.  Sp¨ath, H.  1980  Cluster Analysis Algorithms for Data Reduction and Classiﬁcation of Objects.  Ellis Horwood Limited, Hemel Hempstead.  Spiegelhalter, D.J., Dawid, A.P., Hutchinson, T.A. and Cowell, R.G.  1991  Probabilistic expert systems and graphical modelling: a case study in drug safety. Philosophical Transactions of the Royal Society of London, 337:387–405.  Spragins, J.  1976  A note on the iterative applications of Bayes’ rule.  IEEE Transactions on  Information Theory, 11:544–549.  Sridhar, D.V., Seagrave, R.C. and Bartlett, E.B.  1996  Process modeling using stacked neural  networks. Process Systems Engineering, 42 9 :387–405.  Sridhar, D.V., Bartlett, E.B. and Seagrave, R.C.  1999  An information theoretic approach for  combining neural network process models. Neural Networks, 12:915–926.  St¨ager, F. and Agarwal, M.  1997  Three methods to speed up the training of feedforward and  feedback perceptrons. Neural Networks, 10 8 :1435–1443.  Stassopoulou, A., Petrou, M. and Kittler, J.  1996  Bayesian and neural networks for geographic  information processing. Pattern Recognition Letters, 17:1325–1330.  Stearns, S.D.  1976  On selecting features for pattern classiﬁers.  In Proceedings of the 3rd  International Joint Conference on Pattern Recognition, pp. 71–75, IEEE.  Stevenson, J.  1993  Multivariate statistics VI. The place of discriminant function analysis in  psychiatric research. Nordic Journal of Psychiatry, 47 2 :109–122.  Stewart, C., Lu, Y.-C. and Larson, V.  1994  A neural clustering approach for high resolution  radar target classiﬁcation. Pattern Recognition, 27 4 :503–513.  Stewart, G.W.  1973  Introduction to Matrix Computation. Academic Press, Orlando, FL. Stone, C., Hansen, M., Kooperberg, C. and Truong, Y.  1997  Polynomial splines and their tensor  products  with discussion . Annals of Statistics, 25 4 :1371–1470.  Stone, M.  1974  Cross-validatory choice and assessment of statistical predictions. Journal of the  Royal Statistical Society Series B, 36:111–147.   REFERENCES 487  Stuart, A. and Ord, J.K.  1991  Kendall’s Advanced Theory of Statistics, vol. 2. Edward Arnold,  Sturt, E.  1981  An algorithm to construct a discriminant function in Fortran for categorical data.  London, ﬁfth edition.  Applied Statistics, 30:313–325.  Sumpter, R.G., Getino, C. and Noid, D.W.  1994  Theory and applications of neural computing  in chemical science. Annual Reviews of Physical Chemistry, 45:439–481.  Sutton, B.D. and Steck, G.J.  1994  Discrimination of Caribbean and Mediterranean fruit ﬂy larvae  diptera: tephritidae  by cuticular hydrocarbon analysis. Florida Entomologist, 77 2 :231–237.  Tarassenko, L.  1998  A Guide to Neural Computing Applications. Arnold, London. Tax, D.M.J., van Breukelen, M., Duin, R.P.W. and Kittler, J.  2000  Combining multiple  classiﬁers by averaging or multiplying? Pattern Recognition, 33:1475–1485.  Terrell, G.R. and Scott, D.W.  1992  Variable kernel density estimation. Annals of Statistics,  20 3 :1236–1265.  Hall, New York.  Therrien, C.W.  1989  Decision, Estimation and Classiﬁcation. An Introduction to Pattern  Recognition and Related Topics. Wiley, New York.  Thisted, R.A.  1988  Elements of Statistical Computing. Numerical Computation. Chapman &  Thodberg, H.H.  1996  A review of Bayesian neural networks with application to near infrared  spectroscopy. IEEE Transactions on Neural Networks, 7 1 :56–72.  Tian, Q., Fainman, Y. and Lee, S.H.  1988  Comparison of statistical pattern-recognition Journal of the Optical  algorithms for hybrid processing. II. Eigenvector-based algorithm. Society of America A, 5 10 :1670–1682.  Tibshirani, R.J.  1992  Principal curves revisited. Statistics and Computing, 2 4 :183–190. Tierney, L.  1994  Markov chains for exploring posterior distributions. Annals of Statistics,  Titterington, D.M.  1980  A comparative study of kernel-based density estimates for categorical  22 4 :1701–1762.  data. Technometrics, 22 2 :259–268.  Titterington, D.M. and Mill, G.M.  1983  Kernel-based density estimates from incomplete data.  Journal of the Royal Statistical Society Series B, 45 2 :258–266.  Titterington, D.M., Murray, G.D., Murray, L.S., Spiegelhalter, D.J., Skene, A.M., Habbema, J.D.F. and Gelpke, G.J.  1981  Comparison of discrimination techniques applied to a complex data set of head injured patients  with discussion . Journal of the Royal Statistical Society Series A, 144 2 :145–175.  Titterington, D.M., Smith, A.F.M. and Makov, U.E.  1985  Statistical Analysis of Finite Mixture  Distributions. Wiley, New York.  Todeschini, R.  1989  k-nearest neighbour method: the inﬂuence of data transformations and  metrics. Chemometrics and Intelligent Laboratory Systems, 6:213–220.  Todorov, V., Neykov, N. and Neytchev, P.  1994  Robust two-group discrimination by bounded inﬂuence regression. A Monte Carlo simulation. Computational Statistics and Data Analysis, 17:289–302.  Tou, J.T. and Gonzales, R.C.  1974  Pattern Recognition Principles. Addison-Wesley, New York. Toussaint, G.T.  1974  Bibliography on estimation of misclassiﬁcation. IEEE Transactions on  Information Theory, 20 4 :472–479.  Tukey, J.W.  1977  Exploratory Data Analysis. Addison-Wesley, Reading, MA. Turkkan, N. and Pham-Gia, T.  1993  Computation of the highest posterior density interval in  Bayesian analysis. J. Statistical Computation and Simulation, 44:243–250.  Unbehauen, R. and Luo, F.L., eds  1998  Special issue on ‘Neural Networks’. Signal Processing, 64. Valentin, D., Abdi, H., O’Toole, A.J. and Cottrell, G.W.  1994  Connectionist models of face  processing: a survey. Pattern Recognition, 27 9 :1209–1230.  Valiveti, R.S. and Oommen, B.J.  1992  On using the chi-squared metric for determining  stochastic dependence. Pattern Recognition, 25 11 :1389–1400.   488 REFERENCES  Valiveti, R.S. and Oommen, B.J.  1993  Determining stochastic dependence for normally  distributed vectors using the chi-squared metric. Pattern Recognition, 26 6 :975–987.  van der Heiden, R. and Groen, F.C.A.  1997  The Box–Cox metric for nearest neighbour  classiﬁcation improvement. Pattern Recognition, 30 2 :273–279.  van der Smagt, P.P.  1994  Minimisation methods for training feedforward networks. Neural  Networks, 7 1 :1–11.  van Gestel, T., Suykens, J.A.K., Baestaens, D.-E., Lambrechts, A., Lanckriet, G., Vandaele, B.V., De Moor, B. and Vandewalle, J.  2001  Financial time series prediction using least squares support vector machines within the evidence framework. IEEE Transactions on Neural Networks, 12 4 :809–821.  Vapnik, V.N.  1998  Statistical Learning Theory. Wiley, New York. Varshney, P.K.  1997  Distributed Detection and Data Fusion. Springer-Verlag, New York. Venkateswarlu, N.B. and Raju, P.S.V.S.K.  1992  Fast isodata clustering algorithms. Pattern  Recognition, 25 3 :335–345.  Vidal, E.  1986  An algorithm for ﬁnding nearest neighbours in  approximately  constant average  time. Pattern Recognition Letters, 4 3 :145–157.  Vidal, E.  1994  New formulation and improvements of the nearest-neighbour approximating and  eliminating search algorithm  AESA . Pattern Recognition Letters, 15:1–7.  Vio, R., Fasano, G., Lazzarin, M. and Lessi, O.  1994  Probability density estimation in astronomy.  Astronomy and Astrophysics, 289:640–648.  Viswanathan, R. and Varshney, P.K.  1997  Distributed detection with multiple sensors: Part  1 – fundamentals. Proceedings of the IEEE, 85 1 :54–63.  Vivarelli, F. and Williams, C.K.I.  2001  Comparing Bayesian neural network algorithms for  classifying segmented outdoor images. Neural Networks, 14:427–437.  von Stein, J.H. and Ziegler, W.  1984  The prognosis and surveillance of risks from commercial  credit borrowers. Journal of Banking and Finance, 8:249–268.  Wahl, P.W. and Kronmal, R.A.  1977  Discriminant functions when covariances are unequal and  sample sizes are moderate. Biometrics, 33:479–484.  Waltz, E. and Llinas, J.  1990  Multisensor Data Fusion. Artech House, Boston. Wand, M.P. and Jones, M.C.  1994  Multivariate plug-in bandwidth selection. Computational  Statistics, 9:97–116.  Wand, M.P. and Jones, M.C.  1995  Kernel Smoothing. Chapman & Hall, London. Ward, J.H.  1963  Hierarchical grouping to optimise an objective function.  Journal of the  American Statistical Association, 58:236–244.  Watanabe, S.  1985  Pattern Recognition: Human and Mechanical. Wiley, New York. Webb, A.R.  1994  Functional approximation in feed-forward networks: A least-squares approach  to generalisation. IEEE Transactions on Neural Networks, 5 3 :363–371.  Webb, A.R.  1995  Multidimensional scaling by iterative majorisation using radial basis functions.  Pattern Recognition, 28 5 :753–759.  Webb, A.R.  1996  An approach to nonlinear principal components analysis using radially-  symmetric kernel functions. Statistics and Computing, 6:159–168.  Webb, A.R.  2000  Gamma mixture models for target recognition.  Pattern Recognition,  33:2045–2054.  Webb, A.R. and Garner, P.N.  1999  A basis function approach to position estimation using  microwave arrays. Applied Statistics, 48 2 :197–209.  Webb, A.R. and Lowe, D.  1988  A hybrid optimisation strategy for feed-forward adaptive layered networks. DRA memo 4193, DERA, St Andrews Road, Malvern, Worcs, WR14 3PS. Webb, A.R., Lowe, D. and Bedworth, M.D.  1988  A comparison of nonlinear optimisation strategies for feed-forward adaptive layered networks. DRA Memo 4157, DERA, St Andrews Road, Malvern, Worcs, WR14 3PS.   REFERENCES 489  Wee, W.G.  1968  Generalized inverse approach to adaptive multiclass pattern recognition. IEEE  Transactions on Computers, 17 12 :1157–1164.  West, M.  1992  Modelling with mixtures.  In J.M. Bernardo, J.O. Berger, A.P. Dawid and  A.F.M. Smith, eds, Bayesian Statistics 4, pp. 503–524. Oxford University Press, Oxford.  Weymaere, N. and Martens, J.-P.  1994  On the initialization and optimization of multilayer  perceptrons. IEEE Transactions on Neural Networks, 5 5 :738–751.  Whitney, A.W.  1971  A direct method of nonparametric measurement selection. IEEE Trans-  actions on Computers, 20:1100–1103.  Wilkinson, L.  1992  Graphical displays. Statistical Methods in Medical Research, 1 1 :3–25. Williams, C.K.I. and Feng, X.  1998 . Combining neural networks and belief networks for image In T. Constantinides, S.-Y. Kung, M. Niranjan and E. Wilson, eds, Neural  segmentation. Networks for Signal Processing VIII, IEEE, New York.  Williams, W.T., Lance, G.N., Dale, M.B. and Clifford, H.T.  1971  Controversy concerning the  criteria for taxonomic strategies. Computer Journal, 14:162–165.  Wilson, D.  1972  Asymptotic properties of NN rules using edited data. IEEE Transactions on  Systems, Man, and Cybernetics, 2 3 :408–421.  Wolfe, J.H.  1971  A Monte Carlo study of the sampling distribution of the likelihood ratio for mixtures of multinormal distributions. Technical Bulletin STB 72–2, Naval Personnel and Training Research Laboratory, San Diego, CA.  Wolpert, D.H.  1992  Stacked generalization. Neural Networks, 5 2 :241–260. Wong, S.K.M. and Poon, F.C.S.  1989  Comments on ‘Approximating discrete probability IEEE Transactions on Pattern Analysis and Machine  distributions with dependence trees’. Intelligence, 11 3 :333–335.  Woods, K., Kegelmeyer, W.P. and Bowyer, K.  1997  Combination of multiple classiﬁers using IEEE Transactions on Pattern Analysis and Machine Intelligence,  local accuracy estimates. 19 4 :405–410.  Wray, J. and Green, G.G.R.  1995  Neural networks, approximation theory, and ﬁnite precision  computation. Neural Networks, 8 1 :31–37.  Wu, X. and Zhang, K.  1991  A better tree-structured vector quantizer.  In J.A. Storer and J.H. Reif, eds, Proceedings Data Compression Conference, pp. 392–401. IEEE Computer Society Press, Los Alamitos, CA.  Wylie, C.R. and Barrett, L.C.  1995  Advanced Engineering Mathematics, 6th edn., McGraw-Hill,  Yan, H.  1994  Handwritten digit recognition using an optimised nearest neighbor classiﬁer.  Pattern Recognition Letters, 15:207–211.  Yang, M.-S.  1993  A survey of fuzzy clustering. Mathematical and Computer Modelling,  Yasdi, R., ed.,  2000  Special issue on ‘Neural Computing in Human–Computer Interaction’.  Neural Computing and Applications, 9 4 .  Young, T.Y. and Calvert, T.W.  1974  Classiﬁcation, Estimation and Pattern Recognition.  Zentgraf, R.  1975  A note on Lancaster’s deﬁnition of higher-order interactions. Biometrika,  Zhang, G.P.  2000  Neural networks for classiﬁcation: a survey. IEEE Transactions on Systems,  Man, and Cybernetics – Part C: Applications and Reviews, 30 4 :451–462.  Zhang, P.   1993  Model selection via multifold cross validation.  Annals of Statistics,  Zhang, Y., de Silva, C.J.S., Togneri, R., Alder, M. and Attikiouzel, Y.  1994  Speaker-independent isolated word recognition using multiple hidden Markov models. IEEE Proceedings on Vision, Image and Signal Processing, 141 3 :197–202.  New York.  18 11 :1–16.  Elselvier, New York.  62 2 :375–378.  21 1 :299–313.   490 REFERENCES  Zhao, Q., Principe, J.C., Brennan, V.L., Xu, D. and Wang, Z.  2000  Synthetic aperture radar automatic target recognition with three strategies of learning and representation. Optical Engineering, 39 5 :1230–1244.  Zhao, Y. and Atkeson, C.G.  1996  Implementing projection pursuit learning. IEEE Transactions  Zois, E.N. and Anastassopoulos, V.  2001  Fusion of correlated decisions for writer veriﬁcation.  on Neural Networks, 7 2 :362–373.  Pattern Recognition, 34:47–61.  Zongker, D. and Jain, A.K.  1996  Algorithms for feature selection: an evaluation. In Proceedings of the International Conference on Pattern Recognition, pp. 18–22, Vienna, IEEE Computer Society Press, Los Alamitos, CA.  Zupan, J.  1982  Clustering of Large Data Sets. Research Studies Press, Letchworth.   Statistical Pattern Recognition, Second Edition. Andrew R. Webb Copyright  2002 John Wiley & Sons, Ltd. ISBNs: 0-470-84513-9  HB ; 0-470-84514-7  PB   Index  activation function, 177 application studies  classiﬁcation trees, 245 classiﬁer combination, 299 clustering, 400 data fusion, 299 feature selection and extraction, 354 MARS, 246 mixture models, 76, 401 neural networks, 197, 401 nonparametric methods of density  estimation, 116  normal-based linear and quadratic  discriminant rule, 75  projection pursuit, 221 support vector machines, 198  back-propagation, 208–210 bagging, 293, 302 Bayes  decision rule, see decision rule error, see error rate, Bayes  Bayes’ theorem, 453, 454 Bayesian learning methods, 50–55 Bayesian multinet, 90 Bayesian networks, 88–91 between-class scatter matrix, 307 boosting, 293, 302 bootstrap  in cluster validity, 398 in error rate estimation, see error-rate  estimation, bootstrap  branch and bound  in clustering, 377, 381 in density estimation, 115 in feature selection, 312–314, 356 in nearest-neighbour classiﬁcation,  103  CART, see classiﬁcation trees, CART chain rule, 88, 454 Chebyshev distance, 421 class-conditional probability density  function, 6  classiﬁcation trees, 225  CART, 228 construction, 228 deﬁnition, 230 pruning algorithms, 233 splitting rules, 231  clustering  agglomerative methods, 115 application studies, 400 cluster validity, 396–400 hierarchical methods, 362–371  agglomerative algorithm, 363 complete-link method, 367 divisive algorithm, 363, 371 general agglomerative algorithm,  368–369  inversions, 371, 400 non-uniqueness, 371, 400 single-link method, 364–367 sum-of-squares method, 368  mixture models, 372–374 quick partitions, 371–372 sum-of-squares methods, 374–396  clustering criteria, 375 complete search, 381 fuzzy k-means, 380–381 k-means, 184, 377–379 nonlinear optimisation, 379–380 stochastic vector quantisation,  391–395  vector quantisation, 382–396  common factors, 337 common principal components, 37   492 Index  communality, 338 comparative studies, 76  approximation–elimination search  algorithms, 103  classiﬁcation trees, 246, 247 comparing performance, 266 feature selection, 317, 357 fuzzy k-means, 401 hierarchical clustering methods, 400 kernel bandwidth estimators, 113 kernel methods, 115, 118 linear discriminant analysis – for  small sample size, 157  MARS, 247 maximum weight dependence trees, 92 na¨ıve Bayes, 117 neural networks, 198 neural networks model selection, 410 nonlinear feature extraction, 353 nonlinear optimisation algorithms, 216 number of mixture components, 49 principal components analysis, 324,  projection pursuit, 220 RBF learning, 189 regularised discriminant analysis, 40 tree-pruning methods, 240  complete link, see clustering, hierarchical methods, complete-link method  condensing of nearest-neighbour design set,  327  100  conditional risk, 12 confusion matrix, 39, 252 conjugate gradients, 211 conjugate priors, 432 covariance matrix, see matrix, covariance covariance matrix structures  common principal components, 37 proportional, 37  cross-validation  error rate, see error-rate estimation,  cross-validation  data  collection, 444 data sets, 448 dimension, 3 head injury patient, 38 initial data analysis, 446–447 missing values, 447  test set, 445 training set, 2, 445  data mining, 1, 221 data visualisation, 305, 319 decision rule, 6  Bayes  for minimum error, 7–13, 16 for minimum risk, 12–14  minimax, 16 Neyman–Pearson, 14  decision surfaces, 6 decision theory, 6–18 dendrogram, 362 density estimation  nonparametric, 81–121  expansion by basis functions, 105 properties, 81  parametric  estimate, 34 estimative, 77 predictive, 34, 77 semiparametric, 115  density function  marginal, 450  design set, see data, training set detailed balance, 58 discriminability, see performance  assessment, discriminability  discriminant functions, 19–25 discrimination  normal-based models, 34–40  linear discriminant function, 36 quadratic discriminant function, 35 regularised discriminant analysis, see regularised discriminant analysis  dissimilarity coefﬁcient, 419 distance measures, 306, 419–429  angular separation, 423 binary variables, 423 Canberra, 422 Chebyshev, 421 city-block, 421 distance between distributions, 425  Bhattacharyya, 310, 314, 427 Chernoff, 310, 427 divergence, 309, 310, 314, 427 Kullback–Leibler, 86 Mahalanobis, 167, 310, 360, 427 multiclass measures, 428 Patrick–Fischer, 310, 427   Index 493  Euclidean, 420 Minkowski, 422 mixed variable types, 425 nominal and ordinal variables, 423 nonlinear distance, 422 quadratic distance, 422  distribution  multivariate normal, 455  conditional, 455 marginal, 455  normal, 454  divergence, 309, 314, 427  editing of nearest-neighbour design set, 98 EM algorithm, see estimation, maximum likelihood, EM algorithm, 47  error rate, 252  apparent, 252, 309 Bayes, 9, 253 estimation, see error-rate estimation expected, 253 for feature selection, 309 true, 253  error-rate estimation  bootstrap, 257–258, 309 cross-validation, 254 holdout, 253 jackknife, 255, 309  errors-in-variables models, 188 estimation  Bayesian, 434 maximum likelihood EM algorithm, 42  estimator  consistent, 431 efﬁcient, 432 sufﬁcient, 432 unbiased, 431  factor analysis, see feature extraction, factor  analysis factor loadings, 337 factor scores, 338 feature extraction, 305, 306, 318–355  factor analysis, 335–342  estimating the factor scores, 340 factor solutions, 338 rotation of factors, 340  Karhunen–Lo`eve transformation,  329–334  Kittler–Young, 331 SELFIC, 330  multidimensional scaling, 344–354  classical scaling, 345 metric multidimensional scaling,  ordinal scaling, 347  nonlinear, see nonlinear feature  principal components analysis, 60,  346  extraction  319–329  feature selection, 305, 307–318  algorithms, 311–318  branch and bound, 312 suboptimal methods, 314–318  criteria, 308  error rate, 309 probabilistic distance, 309 scatter matrices, 311  feature selection, 305 features, 2 Fisher information, 434 forward propagation, 208 fuzzy k-means clustering, see clustering, sum-of-squares methods, fuzzy k-means  Gaussian classiﬁer, see discrimination,  normal-based models, 35  Gaussian distribution, 454 general symmetric eigenvector equation,  439  geometric methods, 305 Gibbs sampling, see Markov chain Monte  Carlo algorithms, Gibbs sampling, 401  Gini criterion, 232  Hermite polynomial, 106, 219  imprecision, see performance assessment,  imprecision  independent components analysis, 343 intrinsic dimensionality, 3, 186 iterative majorisation, 42  joint density function, 450  k-means clustering, see clustering,  sum-of-squares methods, k-means   494 Index  k-nearest neighbour  for RBF initialisation, 185 k-nearest-neighbour method, see  nonparametric discrimination, nearest-neighbour methods  Karhunen–Lo`eve transformation, see feature extraction, Karhunen–Lo`eve transformation  Karush–Kuhn–Tucker conditions,  137  kernel methods, 59, 106 Kullback–Leibler distance, 86  latent variables, 337, 343 LBG algorithm, 384 learning vector quantisation, 390 likelihood ratio, 7 linear discriminant analysis, 123–158  error correction procedure  multiclass, 145 two-class, 125 Fisher’s criterion multiclass, 145 two-class, 128  for feature extraction, see feature extraction, Karhunen–Lo`eve transformation  least mean squared error procedures,  130, 148–152  multiclass algorithms, 144–158 perceptron criterion, 124–128 support vector machines, see support  vector machines  two-class algorithms, 124–144  linear discriminant function, see  discrimination, normal-based models, linear discriminant function, 20  generalised, 22 piecewise, 21  logistic discrimination, 158–163 loss matrix, 12  equal cost, 13  Luttrell algorithm, 389  Mahalanobis distance, see distance  measures, distance between distributions, Mahalanobis  marginal density, see density function,  marginal  Markov chain Monte Carlo algorithms,  55–70  Gibbs sampling, 56–62 Metropolis–Hastings, 63–65  MARS, see multivariate adaptive regression  splines  matrix  covariance, 451  maximum likelihood estimate, 35 unbiased estimate, 79  properties, 437–441  MCMC algorithms, see Markov chain  Monte Carlo algorithms  Metropolis–Hastings, see Markov chain  Monte Carlo algorithms, Metropolis–Hasting  minimum-distance classiﬁer, 21, 148 Minkowski metric, 422 misclassiﬁcation matrix, see matrix,  covariance missing data, 413–414 mixed variable types  distance measures, 425  mixture models  in cluster analysis, see clustering,  in discriminant analysis, see normal  mixture models  mixture models  mixture sampling, 160 model selection, 409–412 monotonicity property, 312 multidimensional scaling, see feature  extraction, multidimensional scaling  multidimensional scaling by transformation,  multilayer perceptron, 24, 170, 204–216 multivariate adaptive regression splines,  352  241–245  nearest class mean classiﬁer, 21, 36 neural networks, 169–202 model selection, 410 optimisation criteria, 171–177  nonlinear feature extraction  multidimensional scaling by  transformation, 351 nonparametric discrimination  histogram approximations, 84 Bayesian networks, 88–91 independence, 84   Index 495  Lancaster models, 85 maximum weight dependence trees,  imprecision, 258 reliability, 252, 258  85–91  histogram method, 82, 119  variable cell, 83  kernel methods, 106–116, 119  choice of kernel, 113 choice of smoothing parameter, 111 product kernels, 111 variable kernel, 113  nearest-neighbour algorithms, 95–98  LAESA, 95–98  population drift, 5, 114, 174 primary monotone condition, 349 principal components analysis, see feature extraction, principal components analysis  principal coordinates analysis, 345 probability  a posteriori, 7, 453 a priori, 7, 453 conditional, 452  nearest-neighbour methods, 93–105,  probability density function, 450  choice of k, 104 choice of metric, 101 condensing, 100 discriminant adaptive nearest  neighbour classiﬁcation, 102  editing techniques, 98 k-nearest-neighbour decision rule,  119  93  normal distribution, 454 normal mixture models, 41, 78  cluster analysis, 372 discriminant analysis, 45, 46 EM acceleration, 49 EM algorithm, 42, 78 for RBF initialisation, 184 number of components, 46  normal-based linear discriminant function,  normal-based quadratic discriminant  36  function, 35  optimal scaling, 40, 152, 154 optimisation  conjugate gradients, 49  ordination methods, 305 outlier detection, 414–415  parameter estimation, 431–435 maximum likelihood, 433  pattern  deﬁnition, 2 feature, 3 representation pattern, 3  perceptron criterion  performance assessment  discriminability, 252  perceptron, see linear discriminant analysis,  conditional, 453 mixture, 453 standard normal density, 454  probability measure, 449 projection pursuit, 24, 216–220 pseudo-distances, 349  quadratic discriminant function, see  discrimination, normal-based models, quadratic discriminant function  radial basis function network, 24, 164, 170,  177–190  random variables  autocorrelation, 451 covariance, 451 functions of, 452 independent, 451 mutually orthogonal, 451  ratio of uniforms method, 62 receiver operating characteristic, 15,  260–264  area under the curve, 261–263  regression, 20, 24–27 regularisation, 155, 174–175 regularised discriminant analysis, 37, 78 reject option, 6, 9, 13 rejection sampling, 62 reliability, see performance assessment,  reliability  representation space, 344 robust procedures, 414–415 ROC, see receiver operating characteristic  sampling  mixture, 445 separate, 445   496 Index  scree test, 324, 346 secondary monotone condition, 349 self-organising feature maps, 386–396 Sherman–Morisson formula, 255 simulated annealing  in clustering, 381  single-link, see clustering, hierarchical methods, single-link method  softmax, 172 speciﬁc factors, 337 stochastic vector quantisation, see  clustering, sum-of-squares methods, stochastic vector quantisation  stress, 350 support vector machines, 134, 143, 189, 295  application studies, 163, 198 canonical hyperplanes, 135 linear  two-class algorithms, 134–142  nonlinear, 190–197  surrogate splits, 237  total probability theorem, 452 training set, see data, training set, 6 tree-structured vector quantisation, 385  ultrametric dissimilarity coefﬁcient, 397 ultrametric inequality, 363  validation set, 80, 410, 446 Vapnik–Chervonenkis dimension, 417, 418 variables of mixed type, 415–416 varimax rotation, 328, 340 vector quantisation, see clustering,  sum-of-squares methods, vector quantisation  multiclass algorithms, 155–156  within-class scatter matrix, 307
