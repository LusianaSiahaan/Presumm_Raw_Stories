THE ART OF COMPUTER PROGRAMMING THIRD EDITION   DONALD E. KNUTH Stanford University   cid:54  cid:55  cid:55   ADDISON–WESLEY   Volume 2   Seminumerical Algorithms  THE ART OF COMPUTER PROGRAMMING THIRD EDITION  Upper Saddle River, NJ · Boston · Indianapolis · San Francisco New York · Toronto · Montréal Capetown · Sydney · Tokyo · Singapore · Mexico City  · London · Munich · Paris · Madrid    cid:104  cid:105  cid:106  cid:107  cid:108  cid:109  cid:110  cid:106  is a trademark of Addison–Wesley  TEX is a trademark of the American Mathematical Society  The quotation on page 61 is reprinted by permission of Grove Press, Inc. The author and publisher have taken care in the preparation of this book, but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or consequential damages in connection with or arising out of the use of the information or programs contained herein. The publisher offers excellent discounts on this book when ordered in quantity for bulk purposes or special sales, which may include electronic versions and or custom covers and content particular to your business, training goals, marketing focus, and branding interests. For more information, please contact: U.S. Corporate and Government Sales corpsales@pearsontechgroup.com   800  382–3419  For sales outside the U.S., please contact:  International Sales  international@pearsoned.com  Visit us on the Web: informit.com aw  Library of Congress Cataloging-in-Publication Data Knuth, Donald Ervin, 1938-  The art of computer programming   Donald Ervin Knuth. xiv,764 p. Includes bibliographical references and index. Contents: v. 1. Fundamental algorithms. -- v. 2. Seminumerical  24 cm.  algorithms. -- v. 3. Sorting and searching. -- v. 4a. Combinatorial algorithms, part 1.  Contents: v. 2. Seminumerical algorithms. -- 3rd ed. ISBN 978-0-201-89683-1  v. 1, 3rd ed.  ISBN 978-0-201-89684-8  v. 2, 3rd ed.  ISBN 978-0-201-89685-5  v. 3, 2nd ed.  ISBN 978-0-201-03804-0  v. 4a  1. Electronic digital computers--Programming. 2. Computer  algorithms. QA76.6.K64 005.1--DC21  I. Title.  1997  97-2147  Internet page http:  www-cs-faculty.stanford.edu ~knuth taocp.html contains current information about this book and related books. Electronic version by Mathematical Sciences Publishers  MSP , http:  msp.org Copyright c⃝ 1998 by Addison–Wesley All rights reserved. Printed in the United States of America. This publication is protected by copyright, and permission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, write to:  Pearson Education, Inc. Rights and Contracts Department 501 Boylston Street, Suite 900 Boston, MA 02116 ISBN-13 978-0-201-89684-8 ISBN-10 0-201-89684-2 First digital release, March 2014  Fax:  617  671-3447   PREFACE  O dear Ophelia! I am ill at these numbers: I have not art to reckon my groans. — HAMLET  Act II, Scene 2, Line 120   The algorithms discussed in this book deal directly with numbers; yet I believe they are properly called seminumerical, because they lie on the borderline between numeric and symbolic calculation. Each algorithm not only computes the desired answers to a numerical problem, it also is intended to blend well with the internal operations of a digital computer. In many cases people are not able to appreciate the full beauty of such an algorithm unless they also have some knowledge of a computer’s machine language; the efficiency of the corresponding machine program is a vital factor that cannot be divorced from the algorithm itself. The problem is to find the best ways to make computers deal with numbers, and this involves tactical as well as numerical considerations. Therefore the subject matter of this book is unmistakably a part of computer science, as well as of numerical mathematics.  Some people working in “higher levels” of numerical analysis will regard the topics treated here as the domain of system programmers. Other people working in “higher levels” of system programming will regard the topics treated here as the domain of numerical analysts. But I hope that there are a few people left who will want to look carefully at these basic methods. Although the methods reside perhaps on a low level, they underlie all of the more grandiose applications of computers to numerical problems, so it is important to know them well. We are concerned here with the interface between numerical mathematics and computer programming, and it is the mating of both types of skills that makes the subject so interesting.  There is a noticeably higher percentage of mathematical material in this book than in other volumes of this series, because of the nature of the subjects treated. In most cases the necessary mathematical topics are developed here starting almost from scratch  or from results proved in Volume 1 , but in several easily recognizable sections a knowledge of calculus has been assumed.  This volume comprises Chapters 3 and 4 of the complete series. Chapter 3 is concerned with “random numbers”: It is not only a study of various ways to generate random sequences, it also investigates statistical tests for randomness,  v   vi  PREFACE  as well as the transformation of uniform random numbers into other types of random quantities; the latter subject illustrates how random numbers are used in practice. I have also included a section about the nature of randomness itself. Chapter 4 is my attempt to tell the fascinating story of what people have discovered about the processes of arithmetic, after centuries of progress. It discusses various systems for representing numbers, and how to convert between them; and it treats arithmetic on floating point numbers, high-precision integers, rational fractions, polynomials, and power series, including the questions of factoring and finding greatest common divisors.  Each of Chapters 3 and 4 can be used as the basis of a one-semester college course at the junior to graduate level. Although courses on “Random Numbers” and on “Arithmetic” are not presently a part of many college curricula, I be- lieve the reader will find that the subject matter of these chapters lends itself nicely to a unified treatment of material that has real educational value. My own experience has been that these courses are a good means of introducing elementary probability theory and number theory to college students. Nearly all of the topics usually treated in such introductory courses arise naturally in connection with applications, and the presence of these applications can be an important motivation that helps the student to learn and to appreciate the theory. Furthermore, each chapter gives a few hints of more advanced topics that will whet the appetite of many students for further mathematical study.  For the most part this book is self-contained, except for occasional discus- sions relating to the MIX computer explained in Volume 1. Appendix B contains a summary of the mathematical notations used, some of which are a little different from those found in traditional mathematics books.  and  cid:77  cid:69  cid:84  cid:65  cid:70  cid:79  cid:78  cid:84 . I am now pleased to celebrate the full development of those  Preface to the Third Edition When the second edition of this book was completed in 1980, it represented the first major test case for prototype systems of electronic publishing called TEX systems by returning to the book that inspired and shaped them. At last I am able to have all volumes of The Art of Computer Programming in a consistent format that will make them readily adaptable to future changes in printing and display technology. The new setup has allowed me to make many thousands of improvements that I have been wanting to incorporate for a long time.  In this new edition I have gone over every word of the text, trying to retain the youthful exuberance of my original sentences while perhaps adding some more mature judgment. Dozens of new exercises have been added; dozens of old exercises have been given new and improved answers. Changes appear ev- erywhere, but most significantly in Sections 3.5  about theoretical guarantees of randomness , 3.6  about portable random-number generators , 4.5.2  about the binary gcd algorithm , and 4.7  about composition and iteration of power series .   PREFACE  vii  The Art of Computer Programming is, however, still a work in progress. Research on seminumerical algorithms continues to grow at a phenomenal rate. Therefore some parts of this book are headed by an “under construction” icon, to apologize for the fact that the material is not up-to-date. My files are bursting with important material that I plan to include in the final, glorious, fourth edition of Volume 2, perhaps 16 years from now; but I must finish Volumes 4 and 5 first, and I do not want to delay their publication any more than absolutely necessary.  I am enormously grateful to the many hundreds of people who have helped me to gather and refine this material during the past 35 years. Most of the hard work of preparing the new edition was accomplished by Silvio Levy, who expertly edited the electronic text, and by Jeffrey Oldham, who converted nearly all of the original illustrations to METAPOST format. I have corrected every error that alert readers detected in the second edition  as well as some mistakes that, alas, nobody noticed ; and I have tried to avoid introducing new errors in the new material. However, I suppose some defects still remain, and I want to fix them as soon as possible. Therefore I will cheerfully award $2.56 to the first finder of each technical, typographical, or historical error. The webpage cited on page iv contains a current listing of all corrections that have been reported to me. Stanford, California July 1997  D. E. K.  When a book has been eight years in the making, there are too many colleagues, typists, students, teachers, and friends to thank. Besides, I have no intention of giving such people the usual exoneration from responsibility for errors which remain. They should have corrected me! And sometimes they are even responsible for ideas which may turn out in the long run to be wrong. Anyway, to such fellow explorers, my thanks. — EDWARD F. CAMPBELL, JR.  1975   ‘Defendit numerus,’ [there is safety in numbers] is the maxim of the foolish; ‘Deperdit numerus,’ [there is ruin in numbers] of the wise. — C. C. COLTON  1820    This page intentionally left blank    NOTES ON THE EXERCISES  The exercises in this set of books have been designed for self-study as well as for classroom study. It is difficult, if not impossible, for anyone to learn a subject purely by reading about it, without applying the information to specific problems and thereby being encouraged to think about what has been read. Furthermore, we all learn best the things that we have discovered for ourselves. Therefore the exercises form a major part of this work; a definite attempt has been made to keep them as informative as possible and to select problems that are enjoyable as well as instructive.  In many books, easy exercises are found mixed randomly among extremely difficult ones. A motley mixture is, however, often unfortunate because readers like to know in advance how long a problem ought to take — otherwise they may just skip over all the problems. A classic example of such a situation is the book Dynamic Programming by Richard Bellman; this is an important, pioneering work in which a group of problems is collected together at the end of some chapters under the heading “Exercises and Research Problems,” with extremely trivial questions appearing in the midst of deep, unsolved problems. It is rumored that someone once asked Dr. Bellman how to tell the exercises apart from the research problems, and he replied, “If you can solve it, it is an exercise; otherwise it’s a research problem.”  Good arguments can be made for including both research problems and very easy exercises in a book of this kind; therefore, to save the reader from the possible dilemma of determining which are which, rating numbers have been provided to indicate the level of difficulty. These numbers have the following general significance: Rating  Interpretation  00 An extremely easy exercise that can be answered immediately if the material of the text has been understood; such an exercise can almost always be worked “in your head.”  10 A simple problem that makes you think over the material just read, but is by no means difficult. You should be able to do this in one minute at most; pencil and paper may be useful in obtaining the solution.  20 An average problem that tests basic understanding of the text mate- rial, but you may need about fifteen or twenty minutes to answer it completely.  ix   x  NOTES ON THE EXERCISES  30 A problem of moderate difficulty and or complexity; this one may involve more than two hours’ work to solve satisfactorily, or even more if the TV is on.  40 Quite a difficult or lengthy problem that would be suitable for a term project in classroom situations. A student should be able to solve the problem in a reasonable amount of time, but the solution is not trivial. 50 A research problem that has not yet been solved satisfactorily, as far as the author knew at the time of writing, although many people have tried. If you have found an answer to such a problem, you ought to write it up for publication; furthermore, the author of this book would appreciate hearing about the solution as soon as possible  provided that it is correct .  By interpolation in this “logarithmic” scale, the significance of other rating numbers becomes clear. For example, a rating of 17 would indicate an exercise that is a bit simpler than average. Problems with a rating of 50 that are subsequently solved by some reader may appear with a 40 rating in later editions of the book, and in the errata posted on the Internet  see page iv .  The remainder of the rating number divided by 5 indicates the amount of detailed work required. Thus, an exercise rated 24 may take longer to solve than an exercise that is rated 25, but the latter will require more creativity. All exercises with ratings of 46 or more are open problems for future research, rated according to the number of different attacks that they’ve resisted so far.  The author has tried earnestly to assign accurate rating numbers, but it is difficult for the person who makes up a problem to know just how formidable it will be for someone else to find a solution; and everyone has more aptitude for certain types of problems than for others. It is hoped that the rating numbers represent a good guess at the level of difficulty, but they should be taken as general guidelines, not as absolute indicators.  This book has been written for readers with varying degrees of mathematical training and sophistication; as a result, some of the exercises are intended only for the use of more mathematically inclined readers. The rating is preceded by an M if the exercise involves mathematical concepts or motivation to a greater extent than necessary for someone who is primarily interested only in programming the algorithms themselves. An exercise is marked with the letters “HM” if its solution necessarily involves a knowledge of calculus or other higher mathematics not developed in this book. An “HM” designation does not necessarily imply difficulty.  Some exercises are preceded by an arrowhead, “ cid:120 ”; this designates prob-  lems that are especially instructive and especially recommended. Of course, no reader student is expected to work all of the exercises, so those that seem to be the most valuable have been singled out.  This distinction is not meant to detract from the other exercises!  Each reader should at least make an attempt to solve all of the problems whose rating is 10 or less; and the arrows may help to indicate which of the problems with a higher rating should be given priority.   NOTES ON THE EXERCISES  xi  Solutions to most of the exercises appear in the answer section. Please use them wisely; do not turn to the answer until you have made a genuine effort to solve the problem by yourself, or unless you absolutely do not have time to work this particular problem. After getting your own solution or giving the problem a decent try, you may find the answer instructive and helpful. The solution given will often be quite short, and it will sketch the details under the assumption that you have earnestly tried to solve it by your own means first. Sometimes the solution gives less information than was asked; often it gives more. It is quite possible that you may have a better answer than the one published here, or you may have found an error in the published solution; in such a case, the author will be pleased to know the details. Later printings of this book will give the improved solutions together with the solver’s name where appropriate.  When working an exercise you may generally use the answers to previous exercises, unless specifically forbidden from doing so. The rating numbers have been assigned with this in mind; thus it is possible for exercise n + 1 to have a lower rating than exercise n, even though it includes the result of exercise n as a special case.  Summary of codes:   cid:120   Recommended  M Mathematically oriented HM Requiring “higher math”  00 Immediate 10 Simple  one minute  20 Medium  quarter hour  30 Moderately hard 40 Term project 50 Research problem  EXERCISES   cid:120  1. [00] What does the rating “M20 ” mean?  2. [10] Of what value can the exercises in a textbook be to the reader? 3. [M34] Leonhard Euler conjectured in 1772 that the equation w4 + x4 + y4 = z4 has no solution in positive integers, but Noam Elkies proved in 1987 that infinitely many solutions exist [see Math. Comp. 51  1988 , 825–835]. Find all integer solutions such that 0 ≤ w ≤ x ≤ y < z < 106. 4. [M50] Prove that when n is an integer, n > 4, the equation wn + xn + yn = zn has no solution in positive integers w, x, y, z.  Exercise is the beste instrument in learnyng. — ROBERT RECORDE, The Whetstone of Witte  1557    CONTENTS  .  .  .  .  .  .  . .  . .  . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  3.3. Statistical Tests  3.2.2. Other Methods . .  Chapter 3 — Random Numbers . 3.1. Introduction . . . 3.2. Generating Uniform Random Numbers  3.2.1.1. Choice of modulus . 3.2.1.2. Choice of multiplier . . 3.2.1.3. Potency . . . . .  . . . 3.2.1. The Linear Congruential Method . . . . . .  . . . . . . . . . 3.3.1. General Test Procedures for Studying Random Data . 3.3.2. Empirical Tests . . *3.3.3. Theoretical Tests . 3.3.4. The Spectral Test . . . . . .  . . . 3.4. Other Types of Random Quantities . .  3.4.1. Numerical Distributions . 3.4.2. Random Sampling and Shuffling  *3.5. What Is a Random Sequence? . . 3.6. Summary .  . . . . . . . .  . . . . . . . .  . . . . . . . .  . . . . . . . .  . . . . . . . .  . . . . . . . .  . . . . . . . .  . . . . .  . . . . .  . . .  . . .  . . .  . . .  . . .  . . .  . . .  . . .  . .  . .  . .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . .  . . .  . . .  . . . .  . . . .  . . . .  4.3. Multiple-Precision Arithmetic  Chapter 4 — Arithmetic . . 4.1. Positional Number Systems 4.2. Floating Point Arithmetic .  . . . . . . . . . . 4.2.1. Single-Precision Calculations . . . 4.2.2. Accuracy of Floating Point Arithmetic . *4.2.3. Double-Precision Calculations . 4.2.4. Distribution of Floating Point Numbers . . . . . . . . . . .  . . . . . . 4.5.1. Fractions . 4.5.2. The Greatest Common Divisor . *4.5.3. Analysis of Euclid’s Algorithm . 4.5.4. Factoring into Primes . .  . . 4.3.1. The Classical Algorithms *4.3.2. Modular Arithmetic . . *4.3.3. How Fast Can We Multiply? . . . .  4.4. Radix Conversion . 4.5. Rational Arithmetic .  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  . . .  . . .  . . .  . . .  . . .  . . .  . . .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . .  . xii  . . . . . . . . .  . . . . . . . .  . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . .  1 1 10 10 12 16 23 26 41 42 61 80 93 119 119 142 149 184  194 195 214 214 229 246 253 265 265 284 294 319 330 330 333 356 379   CONTENTS  xiii  .  .  4.6. Polynomial Arithmetic  . . 4.6.1. Division of Polynomials . *4.6.2. Factorization of Polynomials . . 4.6.3. Evaluation of Powers . . 4.6.4. Evaluation of Polynomials . . .  *4.7. Manipulation of Power Series  . .  . .  .  .  .  Answers to Exercises  .  .  .  .  .  .  .  .  . . . . . .  .  . . . . . .  .  . . . . . .  .  . . . . . .  .  . . . . . .  .  . . . . . .  .  . . . . . .  .  Appendix A — Tables of Numerical Quantities . . .  . . Fundamental Constants  decimal  Fundamental Constants  octal  . . Harmonic Numbers, Bernoulli Numbers, Fibonacci Numbers .  1. 2. 3.  . . .  . . .  . . .  . . .  . . .  . . .  . . .  . . .  . .  . .  .  Appendix B — Index to Notations .  .  .  .  .  .  .  .  .  Appendix C — Index to Algorithms and Theorems .  Index and Glossary .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . .  .  .  .  .  . . . . . .  .  .  .  .  . . . . . .  .  .  .  .  . . . . . .  .  .  .  .  . . . . . .  .  .  .  .  . . . . . .  .  .  .  .  . . . . . .  .  .  .  .  . . . . . .  .  . . . .  .  .  .  . . . . . .  .  . . . .  .  .  .  418 420 439 461 485 525  538  726 726 727 728  730  735  737    CHAPTER THREE  RANDOM NUMBERS  Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin. — JOHN VON NEUMANN  1951   Lest men suspect your tale untrue, Keep probability in view. — JOHN GAY  1727   There wanted not some beams of light to guide men in the exercise of their Stocastick faculty. — JOHN OWEN  1662   3.1. INTRODUCTION Numbers that are “chosen at random” are useful in many different kinds of applications. For example: a  Simulation. When a computer is being used to simulate natural phenomena, random numbers are required to make things realistic. Simulation covers many fields, from the study of nuclear physics  where particles are subject to random collisions  to operations research  where people come into, say, an airport at random intervals . b  Sampling. It is often impractical to examine all possible cases, but a random sample will provide insight into what constitutes “typical” behavior. c  Numerical analysis. Ingenious techniques for solving complicated numerical problems have been devised using random numbers. Several books have been written on this subject. d  Computer programming. Random values make a good source of data for testing the effectiveness of computer algorithms. More importantly, they are crucial to the operation of randomized algorithms, which are often far superior to their deterministic counterparts. This use of random numbers is the primary application of interest to us in this series of books; it accounts for the fact that random numbers are already being considered here in Chapter 3, before most of the other computer algorithms have appeared.  1   2  RANDOM NUMBERS  3.1  e  Decision making. There are reports that many executives make their deci- sions by flipping a coin or by throwing darts, etc. It is also rumored that some college professors prepare their grades on such a basis. Sometimes it is important to make a completely “unbiased” decision. Randomness is also an essential part of optimal strategies in the theory of matrix games. f  Cryptography. A source of unbiased bits is crucial for many types of secure communications, when data needs to be concealed. g  Aesthetics. A little bit of randomness makes computer-generated graphics and music seem more lively. For example, a pattern like  tends to look  more appealing than  in certain contexts. [See D. E. Knuth, Bull. Amer. Math. Soc. 1  1979 , 369.] h  Recreation. Rolling dice, shuffling decks of cards, spinning roulette wheels, etc., are fascinating pastimes for just about everybody. These traditional uses of random numbers have suggested the name “Monte Carlo method,” a general term used to describe any algorithm that employs random numbers.  People who think about this topic almost invariably get into philosophical discussions about what the word “random” means. In a sense, there is no such thing as a random number; for example, is 2 a random number? Rather, we speak of a sequence of independent random numbers with a specified distribution, and this means loosely that each number was obtained merely by chance, having nothing to do with other numbers of the sequence, and that each number has a specified probability of falling in any given range of values.  A uniform distribution on a finite set of numbers is one in which each possible number is equally probable. A distribution is generally understood to be uniform unless some other distribution is specifically mentioned.  Each of the ten digits 0 through 9 will occur about 1  10 of the time in a  uniform  sequence of random digits. Each pair of two successive digits should 1 100 of the time, and so on. Yet if we take a truly random sequence occur about of a million digits, it will not always have exactly 100,000 zeros, 100,000 ones, etc. In fact, chances of this are quite slim; a sequence of such sequences will have this character on the average.  Any specified sequence of a million digits is as probable as any other. Thus, if we are choosing a million digits at random and if the first 999,999 of them happen to come out to be zero, the chance that the final digit is zero is still exactly 1 10, in a truly random situation. These statements seem paradoxical to many people, yet no contradiction is really involved. There are several ways to formulate decent abstract definitions of random- ness, and we will return to this interesting subject in Section 3.5; but for the moment, let us content ourselves with an intuitive understanding of the concept. Many years ago, people who needed random numbers in their scientific work would draw balls out of a “well-stirred urn,” or they would roll dice or deal out   3.1  INTRODUCTION  3  cards. A table of over 40,000 random digits, “taken at random from census reports,” was published in 1927 by L. H. C. Tippett. Since then, a number of devices have been built to generate random numbers mechanically. The first such machine was used in 1939 by M. G. Kendall and B. Babington-Smith to produce a table of 100,000 random digits. The Ferranti Mark I computer, first installed in 1951, had a built-in instruction that put 20 random bits into the accumulator using a resistance noise generator; this feature had been recommended by A. M. Turing. In 1955, the RAND Corporation published a widely used table of a million random digits obtained with the help of another special device. A famous random-number machine called ERNIE has been used for many years to pick the winning numbers in the British Premium Savings Bonds lottery. [F. N. David de- scribes the early history in Games, Gods, and Gambling  1962 . See also Kendall and Babington-Smith, J. Royal Stat. Soc. A101  1938 , 147–166; B6  1939 , 51– 61; S. H. Lavington’s discussion of the Mark I in CACM 21  1978 , 4–12; the review of the RAND table in Math. Comp. 10  1956 , 39–43; and the discussion of ERNIE by W. E. Thomson, J. Royal Stat. Soc. A122  1959 , 301–333.]  Shortly after computers were introduced, people began to search for efficient ways to obtain random numbers within computer programs. A table could be used, but this method is of limited utility because of the memory space and input time requirement, because the table may be too short, and because it is a bit of a nuisance to prepare and maintain the table. A machine such as ERNIE might be attached to the computer, as in the Ferranti Mark I, but this has proved to be unsatisfactory since it is impossible to reproduce calculations exactly a second time when checking out a program; moreover, such machines have tended to suffer from malfunctions that are extremely difficult to detect. Advances in technology made tables useful again during the 1990s, because a billion well-tested random bytes could easily be made accessible. George Marsaglia helped resuscitate random tables in 1995 by preparing a demonstration disk that contained 650 random megabytes, generated by combining the output of a noise-diode circuit with deterministically scrambled rap music.  He called it “white and black noise.”   The inadequacy of mechanical methods in the early days led to an interest in the production of random numbers using a computer’s ordinary arithmetic operations. John von Neumann first suggested this approach in about 1946; his idea was to take the square of the previous random number and to extract the middle digits. For example, if we are generating 10-digit numbers and the previous value was 5772156649, we square it to get 33317792380594909201;  the next number is therefore 7923805949.  There is a fairly obvious objection to this technique: How can a sequence generated in such a way be random, since each number is completely determined by its predecessor?  See von Neumann’s comment at the beginning of this chapter.  The answer is that the sequence isn’t random, but it appears to be. In typical applications the actual relationship between one number and   4  RANDOM NUMBERS  3.1  its successor has no physical significance; hence the nonrandom character is not really undesirable. Intuitively, the middle square seems to be a fairly good scrambling of the previous number.  Sequences generated in a deterministic way such as this are often called pseudorandom or quasirandom sequences in the highbrow technical literature, but in most places of this book we shall simply call them random sequences, with the understanding that they only appear to be random. Being “apparently random” is perhaps all that can be said about any random sequence anyway. Random numbers generated deterministically on computers have worked quite well in nearly every application, provided that a suitable method has been carefully selected. Of course, deterministic sequences aren’t always the answer; they certainly shouldn’t replace ERNIE for the lotteries.  Von Neumann’s original “middle-square method” has actually proved to be a comparatively poor source of random numbers. The danger is that the sequence tends to get into a rut, a short cycle of repeating elements. For example, if zero ever appears as a number of the sequence, it will continually perpetuate itself. Several people experimented with the middle-square method in the early 1950s. Working with numbers that have four digits instead of ten, G. E. Forsythe tried 16 different starting values and found that 12 of them led to sequences ending with the cycle 6100, 2100, 4100, 8100, 6100, . . . , while two of them degenerated to zero. More extensive tests were carried out by N. Metropolis, mostly in the binary number system. He showed that when 20-bit numbers are being used, there are 13 different cycles into which the middle-square sequence might degenerate, the longest of which has a period of length 142.  It is fairly easy to restart the middle-square method on a new value when zero has been detected, but long cycles are somewhat harder to avoid. Exercises 6 and 7 discuss some interesting ways to determine the cycles of periodic sequences, using very little memory space.  A theoretical disadvantage of the middle-square method is given in exercises 9 and 10. On the other hand, working with 38-bit numbers, Metropolis obtained a sequence of about 750,000 numbers before degeneracy occurred, and the re- sulting 750,000 × 38 bits satisfactorily passed statistical tests for randomness. [Symp. on Monte Carlo Methods  Wiley, 1956 , 29–36.] This experience showed that the middle-square method can give usable results, but it is rather dangerous to put much faith in it until after elaborate computations have been performed. Many random number generators in use when this chapter was first written were not very good. People have traditionally tended to avoid learning about such subroutines; old methods that were comparatively unsatisfactory have been passed down blindly from one programmer to another, until the users have no understanding of the original limitations. We shall see in this chapter that the most important facts about random number generators are not difficult to learn, although prudence is necessary to avoid common pitfalls.  It is not easy to invent a foolproof source of random numbers. This fact was convincingly impressed upon the author in 1959, when he attempted to create a fantastically good generator using the following peculiar approach:   3.1  INTRODUCTION  5  Algorithm K  “Super-random” number generator . Given a 10-digit decimal number X, this algorithm may be used to change X to the number that should come next in a supposedly random sequence. Although the algorithm might be expected to yield quite a random sequence, reasons given below show that it is not, in fact, very good at all.  The reader need not study this algorithm in great detail except to observe how complicated it is; note, in particular, steps K1 and K2.  K1. [Choose number of iterations.] Set Y ← ⌊X 109⌋, the most significant digit of X.  We will execute steps K2 through K13 exactly Y + 1 times; that is, we will apply randomizing transformations a random number of times.   K2. [Choose random step.] Set Z ← ⌊X 108⌋ mod 10, the second most signifi- cant digit of X. Go to step K 3 + Z .  That is, we now jump to a random step in the program.   K3. [Ensure ≥ 5 × 109.] If X < 5000000000, set X ← X + 5000000000. K4. [Middle square.] Replace X by ⌊X2 105⌋ mod 1010, that is, by the middle  of the square of X.  K5. [Multiply.] Replace X by  1001001001 X  mod 1010. K6. [Pseudo-complement.] If X < 100000000, then set X ← X + 9814055677;  otherwise set X ← 1010 − X.  K7. [Interchange halves.] Interchange the low-order five digits of X with the high-order five digits; that is, set X ← 105 X mod 105  + ⌊X 105⌋, the middle 10 digits of  1010 + 1 X.  K8. [Multiply.] Same as step K5. K9. [Decrease digits.] Decrease each nonzero digit of the decimal representation  K10. [99999 modify.] If X < 105, set X ← X2 + 99999; otherwise set X ←  K11. [Normalize.]  At this point X cannot be zero.  If X < 109, set X ← 10X  of X by one.  X − 99999.  and repeat this step.  K12. [Modified middle square.] Replace X by ⌊X X − 1  105⌋ mod 1010, that  is, by the middle 10 digits of X X − 1 .  K13. [Repeat?] If Y > 0, decrease Y by 1 and return to step K2. If Y = 0, the  algorithm terminates with X as the desired “random” value.   The machine-language program corresponding to this algorithm was intended to be so complicated that a person reading a listing of it without explanatory comments wouldn’t know what the program was doing.   Considering all the contortions of Algorithm K, doesn’t it seem plausible that it should produce almost an infinite supply of unbelievably random numbers? No! In fact, when this algorithm was first put onto a computer, it almost im- mediately converged to the 10-digit value 6065038420, which — by extraordinary   6  RANDOM NUMBERS  3.1  Table 1  A COLOSSAL COINCIDENCE: THE NUMBER 6065038420 IS TRANSFORMED INTO ITSELF BY ALGORITHM K.  Step  X  after   K1 K3 K4 K5 K6 K7 K8 K9 K10 K11 K12 K6 K7 K8 K9 K10 K11 K12 K12 K6 K7 K8  6065038420 6065038420 6910360760 8031120760 1968879240 7924019688 9631707688 8520606577 8520506578 8520506578 0323372207 9676627793 2779396766 4942162766 3831051655 3830951656 3830951656 1905867781 3319967479 6680032521 3252166800 2218966800  Y = 6  Y = 5 Y = 4  Step K9 K10 K11 K12 K5 K6 K7 K8 K9 K10 K11 K12 K11 K12 K4 K5 K6 K7 K8 K9 K10 K11 K12  X  after  1107855700 1107755701 1107755701 1226919902 0048821902 9862877579 7757998628 2384626628 1273515517 1273415518 1273415518 5870802097 5870802097 3172562687 1540029446 7015475446 2984524554 2455429845 2730274845 1620163734 1620063735 1620063735 6065038420  Y = 3  Y = 2  Y = 1  Y = 0  coincidence — is transformed into itself by the algorithm  see Table 1 . With another starting number, the sequence began to repeat after 7401 values, in a cyclic period of length 3178.  The moral of this story is that random numbers should not be generated  with a method chosen at random. Some theory should be used.  In the following sections we shall consider random number generators that are superior to the middle-square method and to Algorithm K. The correspond- ing sequences are guaranteed to have certain desirable random properties, and no degeneracy will occur. We shall explore the reasons for this random-like behavior in some detail, and we shall also consider techniques for manipulating random numbers. For example, one of our investigations will be the shuffling of a simulated deck of cards within a computer program.  Section 3.6 summarizes this chapter and lists several bibliographic sources.  EXERCISES   cid:120  1. [20] Suppose that you wish to obtain a decimal digit at random, not using a  computer. Which of the following methods would be suitable?   3.1  INTRODUCTION  7  a  Open a telephone directory to a random place by sticking your finger in it some-  where, and use the units digit of the first number found on the selected page.  b  Same as  a , but use the units digit of the page number. c  Roll a die that is in the shape of a regular icosahedron, whose twenty faces have been labeled with the digits 0, 0, 1, 1, . . . , 9, 9. Use the digit that appears on top, when the die comes to rest.  A felt-covered table with a hard surface is recommended for rolling dice.   d  Expose a geiger counter to a source of radioactivity for one minute  shielding yourself  and use the units digit of the resulting count. Assume that the geiger counter displays the number of counts in decimal notation, and that the count is initially zero.  e  Glance at your wristwatch; and if the position of the second-hand is between 6n  and 6 n + 1  seconds, choose the digit n.  f  Ask a friend to think of a random digit, and use the digit he names. g  Ask an enemy to think of a random digit, and use the digit he names. h  Assume that 10 horses are entered in a race and that you know nothing whatever about their qualifications. Assign to these horses the digits 0 to 9, in arbitrary fashion, and after the race use the winner’s digit.  2. [M22] In a random sequence of a million decimal digits, what is the probability that there are exactly 100,000 of each possible digit? 3. [10] What number follows 1010101010 in the middle-square method? 4. [20]  a  Why can’t the value of X be zero when step K11 of Algorithm K is performed? What would be wrong with the algorithm if X could be zero?  b  Use Table 1 to deduce what happens when Algorithm K is applied repeatedly with the starting value X = 3830951656. 5. [15] Explain why, in any case, Algorithm K should not be expected to provide infinitely many random numbers, in the sense that  even if the coincidence given in Table 1 had not occurred  one knows in advance that any sequence generated by Algorithm K will eventually be periodic.   cid:120  6. [M21] Suppose that we want to generate a sequence of integers X0, X1, X2, . . . ,  in the range 0 ≤ Xn < m. Let f x  be any function such that 0 ≤ x < m implies 0 ≤ f x  < m. Consider a sequence formed by the rule Xn+1 = f Xn .  Examples are the middle-square method and Algorithm K.  a  Show that the sequence is ultimately periodic, in the sense that there exist numbers  λ and µ for which the values  X0, X1, . . . , Xµ, . . . , Xµ+λ−1  are distinct, but Xn+λ = Xn when n ≥ µ. Find the maximum and minimum possible values of µ and λ. b   R. W. Floyd.  Show that there exists an n > 0 such that Xn = X2n; and the smallest such value of n lies in the range µ ≤ n ≤ µ + λ. Furthermore the value of Xn is unique in the sense that if Xn = X2n and Xr = X2r, then Xr = Xn.  c  Use the idea of part  b  to design an algorithm that calculates µ and λ for any given function f and any given X0, using only O µ + λ  steps and only a bounded number of memory locations.   8  RANDOM NUMBERS   cid:120  7. [M21]  R. P. Brent, 1977.  Let ℓ n  be the greatest power of 2 that is less than  3.1  or equal to n; thus, for example, ℓ 15  = 8 and ℓ ℓ n   = ℓ n . a  Show that, in terms of the notation in exercise 6, there exists an n > 0 such that Xn = Xℓ n −1. Find a formula that expresses the least such n in terms of the periodicity numbers µ and λ. b  Apply this result to design an algorithm that can be used in conjunction with any random number generator of the type Xn+1 = f Xn , to prevent it from cycling indefinitely. Your algorithm should calculate the period length λ, and it should use only a small amount of memory space — you must not simply store all of the computed sequence values!  8. [23] Make a complete examination of the middle-square method in the case of two-digit decimal numbers. a  We might start the process out with any of the 100 possible values 00, 01, . . . , 99. How many of these values lead ultimately to the repeating cycle 00, 00, . . . ? [Example: Starting with 43, we obtain the sequence 43, 84, 05, 02, 00, 00, 00, . . . .]  b  How many possible final cycles are there? How long is the longest cycle? c  What starting value or values will give the largest number of distinct elements  before the sequence repeats?  9. [M14] Prove that the middle-square method using 2n-digit numbers to the base b has the following disadvantage: If the sequence includes any number whose most significant n digits are zero, the succeeding numbers will get smaller and smaller until zero occurs repeatedly. 10. [M16] Under the assumptions of the preceding exercise, what can you say about the sequence of numbers following X if the least significant n digits of X are zero? What if the least significant n + 1 digits are zero?   cid:120  11. [M26] Consider sequences of random number generators having the form de-  scribed in exercise 6. If we choose f x  and X0 at random — in other words, if we assume that each of the mm possible functions f x  is equally probable and that each of the m possible values of X0 is equally probable — what is the probability that the sequence will eventually degenerate into a cycle of length λ = 1? [Note: The assumptions of this problem give a natural way to think of a “random” random number generator of this type. A method such as Algorithm K may be expected to behave somewhat like the generator considered here; the answer to this problem gives a measure of how colossal the coincidence of Table 1 really is.]   cid:120  12. [M31] Under the assumptions of the preceding exercise, what is the average length  of the final cycle? What is the average length of the sequence before it begins to cycle?  In the notation of exercise 6, we wish to examine the average values of λ and of µ+ λ.  13. [M42] If f x  is chosen at random in the sense of exercise 11, what is the average length of the longest cycle obtainable by varying the starting value X0? [Note: We have already considered the analogous problem in the case that f x  is a random permutation; see exercise 1.3.3–23.] 14. [M38] If f x  is chosen at random in the sense of exercise 11, what is the av- erage number of distinct final cycles obtainable by varying the starting value? [See exercise 8 b .] 15. [M15] If f x  is chosen at random in the sense of exercise 11, what is the proba- bility that none of the final cycles has length 1, regardless of the choice of X0?   3.1  INTRODUCTION  9  16. [15] A sequence generated as in exercise 6 must begin to repeat after at most m values have been generated. Suppose we generalize the method so that Xn+1 depends on Xn−1 as well as on Xn; formally, let f x, y  be a function such that 0 ≤ x, y < m implies 0 ≤ f x, y  < m. The sequence is constructed by selecting X0 and X1 arbitrarily, and then letting  Xn+1 = f Xn, Xn−1 ,  for n > 0.  What is the maximum period conceivably attainable in this case? 17. [10] Generalize the situation in the previous exercise so that Xn+1 depends on the preceding k values of the sequence. 18. [M20] Invent a method analogous to that of exercise 7 for finding cycles in the general form of random number generator discussed in exercise 17. 19. [HM47] Solve the problems of exercises 11 through 15 asymptotically for the more general case that Xn+1 depends on the preceding k values of the sequence; each of the mmk functions f x1, . . . , xk  is to be considered equally probable. [Note: The number of functions that yield the maximum period is analyzed in exercise 2.3.4.2–23.] 20. [30] Find all nonnegative X < 1010 that lead ultimately via Algorithm K to the self-reproducing number in Table 1. 21. [40] Prove or disprove: The mapping X →→ f X  defined by Algorithm K has exactly five cycles, of lengths 3178, 1606, 1024, 943, and 1. 22. [21]  H. Rolletschek.  Would it be a good idea to generate random numbers by using the sequence f 0 , f 1 , f 2 , . . . , where f is a random function, instead of using x0, f x0 , f f x0  , etc.?   cid:120  23. [M26]  D. Foata and A. Fuchs, 1970.  Show that each of the mm functions f x   largest subscript such that these k elements are distinct.  considered in exercise 6 can be represented as a sequence  x0, x1, . . . , xm−1  having the following properties: i   x0, x1, . . . , xm−1  is a permutation of  f 0 , f 1 , . . . , f m − 1  . ii   f 0 , . . . , f m − 1   can be uniquely reconstructed from  x0, x1, . . . , xm−1 . iii  The elements that appear in cycles of f are {x0, x1, . . . , xk−1}, where k is the iv  xj  ∈ {x0, x1, . . . , xj−1} implies xj−1 = f xj , unless xj is the smallest element in v   f 0 , f 1 , . . . , f m − 1   is a permutation of  0, 1, . . . , m − 1  if and only if  x0, x1, . . . , xm−1  represents the inverse of that permutation by the “unusual correspondence” of Section 1.3.3. vi  x0 = x1 if and only if  x1, . . . , xm−1  represents an oriented tree by the construction  a cycle of f.  of exercise 2.3.4.4–18, with f x  the parent of x.   10  RANDOM NUMBERS  3.2  3.2. GENERATING UNIFORM RANDOM NUMBERS In this section we shall consider methods for generating a sequence of random fractions — random real numbers Un, uniformly distributed between zero and one. Since a computer can represent a real number with only finite accuracy, we shall actually be generating integers Xn between zero and some number m; the fraction  Un = Xn m  will then lie between zero and one. Usually m is the word size of the computer, so Xn may be regarded  conservatively  as the integer contents of a computer word with the radix point assumed at the extreme right, and Un may be regarded  liberally  as the contents of the same word with the radix point assumed at the extreme left.  3.2.1. The Linear Congruential Method By far the most popular random number generators in use today are special cases of the following scheme, introduced by D. H. Lehmer in 1949. [See Proc. 2nd Symp. on Large-Scale Digital Calculating Machinery  Cambridge, Mass.: Harvard University Press, 1951 , 141–146.] We choose four magic integers:  m, a, c, X0,  0 < m. the modulus; 0 ≤ a < m. the multiplier; 0 ≤ c < m. the increment; the starting value; 0 ≤ X0 < m.   1   The desired sequence of random numbers ⟨Xn⟩ is then obtained by setting  Xn+1 =  aXn + c  mod m,   2  This is called a linear congruential sequence. Taking the remainder mod m is somewhat like determining where a ball will land in a spinning roulette wheel. For example, the sequence obtained when m = 10 and X0 = a = c = 7 is  n ≥ 0.  7, 6, 9, 0, 7, 6, 9, 0, . . . .   3  As this example shows, the sequence is not always “random” for all choices of m, a, c, and X0; the principles of choosing the magic numbers appropriately will be investigated carefully in later parts of this chapter.  Example  3  illustrates the fact that the congruential sequences always get into a loop: There is ultimately a cycle of numbers that is repeated endlessly. This property is common to all sequences having the general form Xn+1 = f Xn , when f transforms a finite set into itself; see exercise 3.1–6. The repeating cycle is called the period; sequence  3  has a period of length 4. A useful sequence will of course have a relatively long period. The special case c = 0 deserves explicit mention, since the number generation process is a little faster when c = 0 than it is when c ̸= 0. We shall see later that the restriction c = 0 cuts down the length of the period of the sequence, but it is still possible to make the period reasonably long. Lehmer’s original   THE LINEAR CONGRUENTIAL METHOD  3.2.1 11 generation method had c = 0, although he mentioned c ̸= 0 as a possibility; the fact that c ̸= 0 can lead to longer periods is due to Thomson [Comp. J. 1  1958 , 83, 86] and, independently, to Rotenberg [JACM 7  1960 , 75–77]. The terms multiplicative congruential method and mixed congruential method are used by many authors to denote linear congruential sequences with c = 0 and c ̸= 0, respectively.  The letters m, a, c, and X0 will be used throughout this chapter in the sense  described above. Furthermore, we will find it useful to define  b = a − 1,   4    5   a ≥ 2,  b ≥ 1.  in order to simplify many of our formulas.  We can immediately reject the case a = 1, for this would mean that Xn =  X0 + nc  mod m, and the sequence would certainly not behave as a random sequence. The case a = 0 is even worse. Hence for practical purposes we may assume that  Now we can prove a generalization of Eq.  2 ,  Xn+k =akXn +  ak − 1 c b mod m,  k ≥ 0, n ≥ 0,  having the multiplier ak mod m and the increment ak − 1 c b mod m.   6  which expresses the  n+k th term directly in terms of the nth term.  The special case n = 0 in this equation is worthy of note.  It follows that the subsequence consisting of every kth term of ⟨Xn⟩ is another linear congruential sequence, An important corollary of  6  is that the general sequence defined by m, a, c, and X0 can be expressed very simply in terms of the special case where c = 1 and X0 = 0. Let   7  According to Eq.  6  we will have Yk ≡  ak−1  b  modulo m , hence the general sequence defined in  2  satisfies  Yn+1 =  aYn + 1  mod m.  Y0 = 0,  Xn =  AYn + X0  mod m,  where A =  X0b + c  mod m.   8   EXERCISES 1. [10] Example  3  shows a situation in which X4 = X0, so the sequence begins again from the beginning. Give an example of a linear congruential sequence with m = 10 for which X0 never appears again in the sequence.   cid:120  2. [M20] Show that if a and m are relatively prime, the number X0 will always  appear in the period. 3. [M10] If a and m are not relatively prime, explain why the sequence will be somewhat handicapped and probably not very random; hence we will generally want the multiplier a to be relatively prime to the modulus m. 4. [11] Prove Eq.  6 . 5. [M20] Equation  6  holds for k ≥ 0. If possible, give a formula that expresses Xn+k in terms of Xn for negative values of k.   12  RANDOM NUMBERS  3.2.1.1  3.2.1.1. Choice of modulus. Our current goal is to find good values for the parameters that define a linear congruential sequence. Let us first consider the proper choice of the number m. We want m to be rather large, since the period cannot have more than m elements.  Even if we intend to generate only random zeros and ones, we should not take m = 2, for then the sequence would at best have the form . . . , 0, 1, 0, 1, 0, 1, . . . ! Methods for getting random zeros and ones from linear congruential sequences are discussed in Section 3.4.   Another factor that influences our choice of m is speed of generation: We  want to pick a value so that the computation of  aXn + c  mod m is fast.  Consider MIX as an example. We can compute y mod m by putting y in registers A and X and dividing by m; assuming that y and m are positive, we see that y mod m will then appear in register X. But division is a comparatively slow operation, and it can be avoided if we take m to be a value that is especially convenient, such as the word size of our computer.  Let w be the computer’s word size, namely 2e on an e-bit binary computer or 10e on an e-digit decimal machine.  In this book we shall often use the letter e to denote an arbitrary integer exponent, instead of the base of natural logarithms, hoping that the context will make our notation unambiguous. Physicists have a similar problem when they use e for the charge on an electron.  The result of an addition operation is usually given modulo w, except on ones’-complement machines; and multiplication mod w is also quite simple, since the desired result is the lower half of the product. Thus, the following program computes the quantity  aX + c  mod w efficiently:  LDA A MUL X SLAX 5 ADD C  rA ← a. rAX ←  rA  · X. rA ← rAX mod w. rA ←  rA + c  mod w.   1   The result appears in register A. The overflow toggle might be on at the conclu- sion of these instructions; if that is undesirable, the code should be followed by, say, ‘JOV *+1’ to turn it off.  A clever technique that is less commonly known can be used to perform computations modulo w + 1. For reasons to be explained later, we will generally want c = 0 when m = w + 1, so we merely need to compute  aX  mod  w + 1 . The following program does this: 01 LDAN X 02 MUL A 03 STX TEMP 04 SUB TEMP 05 JANN *+3 06 INCA 2 07 ADD = w − 1 = rA ← rA + w − 1. Register A now contains the value  aX  mod  w +1 . Of course, this value might lie anywhere between 0 and w, inclusive, so the reader may legitimately wonder how we can represent so many values in the A-register!  The register obviously  rA ← −X. rAX ←  rA  · a. rA ← rA − rX. Exit if rA ≥ 0. rA ← rA + 2.   2    CHOICE OF MODULUS  3.2.1.1 13 cannot hold a number larger than w−1.  The answer is that the result equals w if and only if program  2  turns overflow on, assuming that overflow was initially off. We could represent w by 0, since  2  will not normally be used when X = 0; but it is most convenient simply to reject the value w if it appears in the congruential sequence modulo w + 1. Then we can also avoid overflow, simply by changing lines 05 and 06 of  2  to ‘JANN *+4; INCA 2; JAP *-5’.  To prove that code  2  actually does determine  aX  mod  w +1 , note that in line 04 we are subtracting the lower half of the product from the upper half. No overflow can occur at this step; and if aX = qw + r, with 0 ≤ r < w, we will have the quantity r − q in register A after line 04. Now  aX = q w + 1  +  r − q ,  and we have −w < r − q < w since q < w; hence  aX  mod  w + 1  equals either r − q or r − q +  w + 1 , depending on whether r − q ≥ 0 or r − q < 0.  w − 1 ; see exercise 8.  A similar technique can be used to get the product of two numbers modulo  In later sections we shall require a knowledge of the prime factors of m in order to choose the multiplier a correctly. Table 1 lists the complete factorization of w ± 1 into primes for nearly every known computer word size; the methods of Section 4.5.4 can be used to extend this table if desired. The reader may well ask why we bother to consider using m = w ± 1, when the choice m = w is so manifestly convenient. The reason is that when m = w, the right-hand digits of Xn are much less random than the left-hand digits. If d is a divisor of m, and if  Yn = Xn mod d,   3   we can easily show that  Yn+1 =  aYn + c  mod d.   4   For Xn+1 = aXn + c − qm for some integer q, and taking both sides mod d causes the quantity qm to drop out when d is a factor of m.  To illustrate the significance of Eq.  4 , let us suppose, for example, that we have a binary computer. If m = w = 2e, the low-order four bits of Xn are the numbers Yn = Xn mod 24. The gist of Eq.  4  is that the low-order four bits of ⟨Xn⟩ form a congruential sequence that has a period of length 16 or less. Similarly, the low-order five bits are periodic with a period of at most 32; and the least significant bit of Xn is either constant or strictly alternating. This situation does not occur when m = w±1; in such a case, the low-order bits of Xn will behave just as randomly as the high-order bits do. If, for example, w = 235 and m = 235−1, the numbers of the sequence will not be very random if we consider only their remainders mod 31, 71, 127, or 122921  see Table 1 ; but the low-order bit, which represents the numbers of the sequence taken mod 2, should be satisfactorily random.  Another alternative is to let m be the largest prime number less than w. This prime may be found by using the techniques of Section 4.5.4, and a table of suitably large primes appears in that section.   14  RANDOM NUMBERS  3.2.1.1  Table 1  e 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 59 60 63 64  2e − 1 7 · 31 · 151 3 · 5 · 17 · 257 131071 33 · 7 · 19 · 73 524287 3 · 52 · 11 · 31 · 41 72 · 127 · 337 3 · 23 · 89 · 683 47 · 178481 32 · 5 · 7 · 13 · 17 · 241 31 · 601 · 1801 3 · 2731 · 8191 7 · 73 · 262657 3 · 5 · 29 · 43 · 113 · 127 233 · 1103 · 2089 32 · 7 · 11 · 31 · 151 · 331 2147483647 3 · 5 · 17 · 257 · 65537 7 · 23 · 89 · 599479 3 · 43691 · 131071 31 · 71 · 127 · 122921 33 · 5 · 7 · 13 · 19 · 37 · 73 · 109 223 · 616318177 3 · 174763 · 524287 7 · 79 · 8191 · 121369 3 · 52 · 11 · 17 · 31 · 41 · 61681 13367 · 164511353 32 · 72 · 43 · 127 · 337 · 5419 431 · 9719 · 2099863 3 · 5 · 23 · 89 · 397 · 683 · 2113 7 · 31 · 73 · 151 · 631 · 23311 3 · 47 · 178481 · 2796203 2351 · 4513 · 13264529 32 · 5 · 7 · 13 · 17 · 97 · 241 · 257 · 673 179951 · 3203431780337 32 · 52 · 7 · 11 · 13 · 31 · 41 · 61 · 151 · 331 · 1321 72 · 73 · 127 · 337 · 92737 · 649657 3 · 5 · 17 · 257 · 641 · 65537 · 6700417 10e − 1 33 · 7 · 11 · 13 · 37 32 · 239 · 4649 32 · 11 · 73 · 101 · 137 34 · 37 · 333667 32 · 11 · 41 · 271 · 9091 32 · 21649 · 513239 33 · 7 · 11 · 13 · 37 · 101 · 9901 32 · 11 · 17 · 73 · 101 · 137 · 5882353  PRIME FACTORIZATIONS OF w ± 1 2e + 1 32 · 11 · 331 65537 3 · 43691 5 · 13 · 37 · 109 3 · 174763 17 · 61681 32 · 43 · 5419 5 · 397 · 2113 3 · 2796203 97 · 257 · 673 3 · 11 · 251 · 4051 5 · 53 · 157 · 1613 34 · 19 · 87211 17 · 15790321 3 · 59 · 3033169 52 · 13 · 41 · 61 · 1321 3 · 715827883 641 · 6700417 32 · 67 · 683 · 20857 5 · 137 · 953 · 26317 3 · 11 · 43 · 281 · 86171 17 · 241 · 433 · 38737 3 · 1777 · 25781083 5 · 229 · 457 · 525313 32 · 2731 · 22366891 257 · 4278255361 3 · 83 · 8831418697 5 · 13 · 29 · 113 · 1429 · 14449 3 · 2932031007403 17 · 353 · 2931542417 33 · 11 · 19 · 331 · 18837001 5 · 277 · 1013 · 1657 · 30269 3 · 283 · 165768537521 193 · 65537 · 22253377 3 · 2833 · 37171 · 1824726041 17 · 241 · 61681 · 4562284561 33 · 19 · 43 · 5419 · 77158673929 274177 · 67280421310721 10e + 1 101 · 9901 11 · 909091 17 · 5882353 7 · 11 · 13 · 19 · 52579 101 · 3541 · 27961 112 · 23 · 4093 · 8779 73 · 137 · 99990001 353 · 449 · 641 · 1409 · 69857  e 6 7 8 9 10 11 12 16   3.2.1.1  CHOICE OF MODULUS  15  In most applications, the low-order bits are insignificant, and the choice m = w is quite satisfactory — provided that the programmer using the random numbers does so wisely.  Our discussion so far has been based on a “signed magnitude” computer like MIX. Similar ideas apply to machines that use complement notations, although there are some instructive variations. For example, a DECsystem 20 computer has 36 bits with two’s complement arithmetic; when it computes the product of two nonnegative integers, the lower half contains the least significant 35 bits with a plus sign. On this machine we should therefore take w = 235, not 236. The 32-bit two’s complement arithmetic on IBM System 370 computers is different: The lower half of a product contains a full 32 bits. Some programmers have felt that this is a disadvantage, since the lower half can be negative when the operands are positive, and it is a nuisance to correct this; but actually it is a distinct advantage from the standpoint of random number generation, since we can take m = 232 instead of 231  see exercise 4 . EXERCISES 1. [M12] In exercise 3.2.1–3 we concluded that the best congruential generators will have the multiplier a relatively prime to m. Show that when m = w in this case it is possible to compute  aX + c  mod w in just three MIX instructions, rather than the four in  1 , with the result appearing in register X. 2. [16] Write a MIX subroutine having the following characteristics:  Calling sequence: Entry conditions: Exit conditions:  JMP RANDM Location XRAND contains an integer X. X ← rA ←  aX + c  mod w, rX ← 0, overflow off.   Thus a call on this subroutine will produce the next random number of a linear congruential sequence.    cid:120  3. [M25] Many computers do not provide the ability to divide a two-word number  by a one-word number; they provide only operations on single-word numbers, such as himult x, y  = ⌊xy w⌋ and lomult x, y  = xy mod w, when x and y are nonnegative integers less than the word size w. Explain how to evaluate ax mod m in terms of himult and lomult, assuming that 0 ≤ a, x < m < w and that m ⊥ w. You may use precomputed constants that depend on a, m, and w.   cid:120  4. [21] Discuss the calculation of linear congruential sequences with m = 232 on  two’s-complement machines such as the System 370 series. 5. [20] Given that m is less than the word size, and that x and y are nonnegative integers less than m, show that the difference  x − y  mod m may be computed in just four MIX instructions, without requiring any division. What is the best code for the sum  x + y  mod m?   cid:120  6. [20] The previous exercise suggests that subtraction mod m is easier to perform  than addition mod m. Discuss sequences generated by the rule  Xn+1 =  aXn − c  mod m.  Are these sequences essentially different from linear congruential sequences as defined in the text? Are they more suited to efficient computer calculation?   16  RANDOM NUMBERS  3.2.1.1  7. [M24] What patterns can you spot in Table 1?   cid:120  8. [20] Write a MIX program analogous to  2  that computes  aX  mod  w−1 . The  cid:120  9. [M25] Most high-level programming languages do not provide a good way to  values 0 and w − 1 are to be treated as equivalent in the input and output of your program.  divide a two-word integer by a one-word integer, nor do they provide the himult operation of exercise 3. The purpose of this exercise is to find a reasonable way to cope with such limitations when we wish to evaluate ax mod m for variable x and for constants 0 < a < m. a  Prove that if q = ⌊m a⌋, we have a x −  x mod q   = ⌊x q⌋ m −  m mod a  . b  Use the identity of  a  to evaluate ax mod m without computing any numbers that  exceed m in absolute value, assuming that a2 ≤ m.  10. [M26] The solution to exercise 9 b  sometimes works also when a2 > m. Exactly how many multipliers a are there for which the intermediate results in that method never exceed m, for all x between 0 and m? 11. [M30] Continuing exercise 9, show that it is possible to evaluate ax mod m using only the following basic operations:  i  u × v, where u ≥ 0, v ≥ 0, and uv < m; ii  ⌊u v⌋, where 0 < v ≤ u < m; iii   u − v  mod m, where 0 ≤ u, v < m. In fact, it is always possible to do this with at most 12 operations of types  i  and  ii , and with a bounded number of operations of type  iii , not counting the precomputation of constants that depend on a and m. For example, explain how to proceed when a is 62089911 and m is 231 − 1.  These constants appear in Table 3.3.4–1.    cid:120  12. [M28] Consider computations by pencil and paper or an abacus.  a  What’s a good way to multiply a given 10-digit number by 10, modulo 9999998999? b  Same question, but multiply instead by 999999900  modulo 9999998999 . c  Explain how to compute the powers 999999900n mod 9999998999, for n = 1, 2,  3, . . . .  d  Relate such computations to the decimal expansion of 1 9999998999. e  Show that these ideas make it possible to implement certain kinds of linear con- gruential generators that have extremely large moduli, using only a few operations per generated number.  13. [M24] Repeat the previous exercise, but with modulus 9999999001 and with multipliers 10 and 8999999101. 14. [M25] Generalize the ideas of the previous two exercises, obtaining a large family of linear congruential generators with extremely large moduli.  3.2.1.2. Choice of multiplier. In this section we shall consider how to choose the multiplier a so as to produce a period of maximum length. A long period is essential for any sequence that is to be used as a source of random numbers; indeed, we would hope that the period contains considerably more numbers than will ever be used in a single application. Therefore we shall concern ourselves in this section with the question of period length. The reader should keep in mind, however, that a long period is only one desirable criterion for the randomness of   3.2.1.2  CHOICE OF MULTIPLIER  17  a linear congruential sequence. For example, when a = c = 1, the sequence is simply Xn+1 =  Xn + 1  mod m, and this obviously has a period of length m, yet it is anything but random. Other considerations affecting the choice of a multiplier will be given later in this chapter.  Since only m different values are possible, the period surely cannot be longer than m. Can we achieve the maximum length, m? The example above shows that it is always possible, although the choice a = c = 1 does not yield a desirable sequence. Let us investigate all possible choices of a, c, and X0 that give a period of length m. It turns out that all such values of the parameters can be characterized very simply; when m is the product of distinct primes, only a = 1 will produce the full period, but when m is divisible by a high power of some prime there is considerable latitude in the choice of a. The following theorem makes it easy to tell if the maximum period is achieved. Theorem A. The linear congruential sequence defined by m, a, c, and X0 has period length m if and only if i  c is relatively prime to m; ii  b = a − 1 is a multiple of p, for every prime p dividing m; iii  b is a multiple of 4, if m is a multiple of 4.  then  The ideas used in the proof of this theorem go back at least a hundred years. But the first proof of the theorem in this particular form was given by M. Greenberger in the special case m = 2e [see JACM 8  1961 , 163–167], and the sufficiency of conditions  i ,  ii , and  iii  in the general case was shown by Hull and Dobell [see SIAM Review 4  1962 , 230–254]. To prove the theorem we will first consider some auxiliary number-theoretic results that are of interest in themselves. Lemma P. Let p be a prime number, and let e be a positive integer, where pe > 2. If  x ≡ 1  modulo pe , xp ≡ 1  modulo pe+1 ,  x ̸≡ 1  modulo pe+1 , xp ̸≡ 1  modulo pe+2 .     p   2  Proof. We have x = 1 + qpe for some integer q that is not a multiple of p. By the binomial formula  xp = 1 +  = 1 + qpe+1  qpe + ··· +   p  1 + 1  is divisible by p  see exercise 1.2.6–10 ; hence binomial coefficientp  The quantity in parentheses is an integer, and, in fact, every term inside the parentheses is a multiple of p except the first term. For if 1 < k < p, the  qp−1p p−1 e + qpppe q2p2e + ··· + 1  qp−1p p−1 e   p   p         p   p − 1 qpe + 1  2  3  1  p  p  p  p  .   1   k   p    k  1 p  qk−1p k−1 e   RANDOM NUMBERS  18 3.2.1.2 is divisible by p k−1 e. And the last term is qp−1p p−1 e−1, which is divisible by p since  p − 1 e > 1 when pe > 2. So xp ≡ 1 + qpe+1  modulo pe+2 , and this  completes the proof. Note: A generalization of this result appears in exercise 3.2.2–11 a .  Lemma Q. Let the decomposition of m into prime factors be  m = pe1  1 . . . pet t .  j , a mod pej   3  The length λ of the period of the linear congruential sequence determined by  X0, a, c, m  is the least common multiple of the lengths λj of the periods of the j  , 1 ≤ j ≤ t. linear congruential sequences  X0 mod pej Proof. By induction on t, it suffices to prove that if m1 and m2 are relatively prime, the length λ of the period of the linear congruential sequence determined by the parameters  X0, a, c, m1m2  is the least common multiple of the lengths λ1 and λ2 of the periods of the sequences determined by  X0 mod m1, a mod m1, c mod m1, m1  and  X0 mod m2, a mod m2, c mod m2, m2 . We observed in the previous section, Eq.  4 , that if the elements of these three sequences are respectively denoted by Xn, Yn, and Zn, we will have Zn = Xn mod m2,  Yn = Xn mod m1  for all n ≥ 0.  j , c mod pej  j , pej  and  Xn = Xk  if and only if  Therefore, by Law D of Section 1.2.4, we find that Yn = Yk   4  Let λ′ be the least common multiple of λ1 and λ2; we wish to prove that λ′ = λ. Since Xn = Xn+λ for all suitably large n, we have Yn = Yn+λ  hence λ is a multiple of λ1  and Zn = Zn+λ  hence λ is a multiple of λ2 , so we must have λ ≥ λ′. Furthermore, we know that Yn = Yn+λ′ and Zn = Zn+λ′ for all suitably large n; therefore, by  4 , Xn = Xn+λ′. This proves λ ≤ λ′.  and Zn = Zk.  Now we are ready to prove Theorem A. Lemma Q tells us that it suffices to  prove the theorem when m is a power of a prime number, because 1 . . . pet  t = λ = lcm λ1, . . . , λt  ≤ λ1 . . . λt ≤ pe1  pe1 1 . . . pet  t  will be true if and only if λj = pej  j  for 1 ≤ j ≤ t.  Assume therefore that m = pe, where p is prime and e is a positive integer. The theorem is obviously true when a = 1, so we may take a > 1. The period can be of length m if and only if each possible integer 0 ≤ x < m occurs in the period, since no value occurs in the period more than once. Therefore the period is of length m if and only if the period of the sequence with X0 = 0 is of length m, and we are justified in supposing that X0 = 0. By formula 3.2.1– 6  we have   5  If c is not relatively prime to m, this value Xn could never be equal to 1, so condition  i  of the theorem is necessary. The period has length m if and only  c mod m.  a − 1  Xn =   an − 1     3.2.1.2  CHOICE OF MULTIPLIER  19  if the smallest positive value of n for which Xn = X0 = 0 is n = m. By  5  and condition  i , our theorem now reduces to proving the following fact: Lemma R. Assume that 1 < a < pe, where p is prime. If λ is the smallest positive integer for which  aλ − 1   a − 1  ≡ 0  modulo pe , then   a ≡ 1  modulo p   a ≡ 1  modulo 4   when p > 2, when p = 2.  λ = pe  if and only if  Proof. Assume that λ = pe. If a ̸≡ 1  modulo p , then  an − 1   a − 1  ≡ 0  modulo pe  if and only if an − 1 ≡ 0  modulo pe . The condition ape − 1 ≡ 0  modulo pe  then implies that ape ≡ 1  modulo p ; but by Theorem 1.2.4F we have ape ≡ a  modulo p , hence a ̸≡ 1  modulo p  leads to a contradiction. And if p = 2 and a ≡ 3  modulo 4 , we have   a2e−1− 1   a − 1  ≡ 0  modulo 2e   by exercise 8. These arguments show that it is necessary in general to have a = 1 + qpf, where pf > 2 and q is not a multiple of p, whenever λ = pe.  It remains to be shown that this condition is sufficient to make λ = pe. By  repeated application of Lemma P, we find that  apg ≡ 1  modulo pf+g ,  apg ̸≡ 1  modulo pf+g+1 ,  for all g ≥ 0, and therefore   apg − 1   a − 1  ≡ 0  modulo pg ,  apg − 1   a − 1  ̸≡ 0  modulo pg+1 .   6   In particular,  ape− 1   a − 1  ≡ 0  modulo pe . Now the congruential sequence  0, a, 1, pe  has Xn =  an−1   a−1  mod pe; therefore it has a period of length λ, that is, Xn = 0 if and only if n is a multiple of λ. Hence pe is a multiple of λ. This can happen only if λ = pg for some g, and the relations in  6  imply that λ = pe, completing the proof.  The proof of Theorem A is now complete. We will conclude this section by considering the special case of pure mul- tiplicative generators, when c = 0. Although the random number generation process is slightly faster in this case, Theorem A shows us that the maximum period length cannot be achieved. In fact, this is quite obvious, since the sequence now satisfies the relation  Xn+1 = aXn mod m,   7  and the value Xn = 0 should never appear, lest the sequence degenerate to zero. In general, if d is any divisor of m and if Xn is a multiple of d, all succeeding elements Xn+1, Xn+2, . . . of the multiplicative sequence will be multiples of d. So when c = 0, we will want Xn to be relatively prime to m for all n, and this limits the length of the period to at most φ m , the number of integers between 0 and m that are relatively prime to m.   20  RANDOM NUMBERS  3.2.1.2  It may be possible to achieve an acceptably long period even if we stipulate that c = 0. Let us now try to find conditions on the multiplier so that the period is as long as possible in this special case.  According to Lemma Q, the period of the sequence depends entirely on the periods of the sequences when m = pe, so let us consider that situation. We have Xn = anX0 mod pe, and it is clear that the period will be of length 1 if a is a multiple of p, so we take a to be relatively prime to p. Then the period is the smallest integer λ such that X0 = aλX0 mod pe. If the greatest common divisor of X0 and pe is pf, this condition is equivalent to aλ ≡ 1  modulo pe−f .   8  By Euler’s theorem  exercise 1.2.4–28 , aφ pe−f   ≡ 1  modulo pe−f ; hence λ is a divisor of  φ pe−f  = pe−f−1 p − 1 .  When a is relatively prime to m, the smallest integer λ for which aλ ≡ 1  modulo m  is conventionally called the order of a modulo m. Any such value of a that has the maximum possible order modulo m is called a primitive element modulo m.  Let λ m  denote the order of a primitive element, namely the maximum possible order, modulo m. The remarks above show that λ pe  is a divisor of pe−1 p − 1 ; with a little care  see exercises 11 through 16 below  we can give the precise value of λ m  in all cases as follows:  λ 2  = 1,  λ 4  = 2,  λ pe  = pe−1 p − 1 , 1 . . . pet  t   = lcmλ pe1  λ 2e  = 2e−2  if if p > 2; 1  , . . . , λ pet  t  .  λ pe1  e ≥ 3;   9   Our remarks may be summarized in the following theorem: [C. F. Gauss, Disquisitiones Arithmeticæ  1801 , §90–92.] The Theorem B. maximum period possible when c = 0 is λ m , where λ m  is defined in  9 . This period is achieved if i  X0 is relatively prime to m; ii  a is a primitive element modulo m. Notice that we can obtain a period of length m− 1 if m is prime; this is just one less than the maximum length, so for all practical purposes such a period is as long as we want.  The question now is, how can we find primitive elements modulo m? The exercises at the close of this section tell us that there is a fairly simple answer when m is prime or a power of a prime, namely the results stated in our next theorem. Theorem C. The number a is a primitive element modulo pe if and only if one of the following cases applies: i  p = 2, e = 1, and a is odd;   3.2.1.2  CHOICE OF MULTIPLIER  21  ii  p = 2, e = 2, and a mod 4 = 3; iii  p = 2, e = 3, and a mod 8 = 3, 5, or 7; iv  p = 2, e ≥ 4, and a mod 8 = 3 or 5; v  p is odd, e = 1, a ̸≡ 0  modulo p , and a p−1  q ̸≡ 1  modulo p  for any  prime divisor q of p − 1;  vi  p is odd, e > 1, a satisfies the conditions of  v , and ap−1 ̸≡ 1  modulo p2 .  Conditions  v  and  vi  of this theorem are readily tested on a computer for large values of p, by using the efficient methods for evaluating powers discussed in Section 4.6.3, if we know the factors of p − 1.  Theorem C applies to powers of primes only. But if we are given values aj that are primitive modulo pej j , it is possible to find a single value a such that a ≡ aj  modulo pej j  , for 1 ≤ j ≤ t, using the Chinese remainder algorithm discussed in Section 4.3.2; this number a will be a primitive element modulo t . Hence there is a reasonably efficient way to construct multipliers pe1 1 . . . pet satisfying the condition of Theorem B, for any modulus m of moderate size, although the calculations can be somewhat lengthy in the general case. In the common case m = 2e, with e ≥ 4, the conditions above simplify to the single requirement that a ≡ 3 or 5  modulo 8 . In this case, one-fourth of all possible multipliers will make the period length equal to m 4, and m 4 is the maximum possible when c = 0.  The second most common case is when m = 10e. Using Lemmas P and Q, it is not difficult to obtain necessary and sufficient conditions for the achievement of the maximum period in the case of a decimal computer  see exercise 18 : Theorem D. If m = 10e, e ≥ 5, c = 0, and X0 is not a multiple of 2 or 5, the period of the linear congruential sequence is 5 × 10e−2 if and only if a mod 200 equals one of the following 32 values:  3, 11, 13, 19, 21, 27, 29, 37, 53, 59, 61, 67, 69, 77, 83, 91, 109, 117, 123, 131, 133, 139, 141, 147, 163, 171, 173, 179, 181, 187, 189, 197.   10   EXERCISES 1. [10] What is the length of the period of the linear congruential sequence with X0 = 5772156648, a = 3141592621, c = 2718281829, and m = 10000000000? 2. [10] Are the following two conditions sufficient to guarantee the maximum length period, when m is a power of 2? “ i  c is odd;  ii  a mod 4 = 1.” 3. [13] Suppose that m = 10e, where e ≥ 2, and suppose further that c is odd and not a multiple of 5. Show that the linear congruential sequence will have the maximum length period if and only if a mod 20 = 1. 4. [M20] Assume that m = 2e and X0 = 0. conditions of Theorem A, what is the value of X2e−1? 5. [14] Find all multipliers a that satisfy the conditions of Theorem A when m = 235 + 1.  The prime factors of m may be found in Table 3.2.1.1–1.   If the numbers a and c satisfy the   22  RANDOM NUMBERS   cid:120  6. [20] Find all multipliers a that satisfy the conditions of Theorem A when m =  cid:120  7. [M23] The period of a congruential sequence need not start with X0, but we can  106 − 1.  See Table 3.2.1.1–1.   always find indices µ ≥ 0 and λ > 0 such that Xn+λ = Xn whenever n ≥ µ, and for which µ and λ are the smallest possible values with this property.  See exercises 3.1–6 and 3.2.1–1.  If µj and λj are the indices corresponding to the sequences  3.2.1.2   X0 mod pej  j , a mod pej  j , c mod pej  j , pej  j  ,  t  1 . . . pet  1 . . . pet  and if µ and λ correspond to the composite sequence  X0, a, c, pe1 t  , Lemma Q states that λ is the least common multiple of λ1, . . . , λt. What is the value of µ in terms of the values of µ1, . . . , µt? What is the maximum possible value of µ obtainable by varying X0, a, and c, when m = pe1 8. [M20] Show that if a mod 4 = 3, we have  a2e−1 − 1   a − 1  ≡ 0  modulo 2e  when e > 1.  Use Lemma P.    cid:120  9. [M22]  W. E. Thomson.  When c = 0 and m = 2e ≥ 16, Theorems B and C say  is fixed?  that the period has length 2e−2 if and only if the multiplier a satisfies a mod 8 = 3 or a mod 8 = 5. Show that every such sequence is essentially a linear congruential sequence with m = 2e−2, having full period, in the following sense: a  If Xn+1 =  4c + 1 Xn mod 2e, and Xn = 4Yn + 1, then Yn+1 =   4c + 1 Yn + c  mod 2e−2  .  b  If Xn+1 =  4c − 1 Xn mod 2e, and Xn =   −1 n 4Yn + 1   mod 2e, then  Yn+1 =   1 − 4c Yn − c  mod 2e−2  .  [Note:  In these formulas, c is an odd integer. The literature contains several statements to the effect that sequences with c = 0 satisfying Theorem B are somehow more random than sequences satisfying Theorem A, in spite of the fact that the period is only one-fourth as long in the case of Theorem B. This exercise refutes such statements; in essence, we must give up two bits of the word length in order to save the addition of c, when m is a power of 2.] 10. [M21] For what values of m is λ m  = φ m ?   cid:120  11. [M28] Let x be an odd integer greater than 1.  a  Show that there exists a unique  integer f > 1 such that x ≡ 2f ± 1  modulo 2f+1 .  b  Given that 1 < x < 2e − 1 and that f is the corresponding integer from part  a , show that the order of x modulo 2e is 2e−f.  c  In particular, this proves parts  i – iv  of Theorem C. 12. [M26] Let p be an odd prime. If e > 1, prove that a is a primitive element modulo pe if and only if a is a primitive element modulo p and ap−1 ̸≡ 1  modulo p2 .  For the purposes of this exercise, assume that λ pe  = pe−1 p−1 . This fact is proved in exercises 14 and 16 below.  13. [M22] Let p be prime. Given that a is not a primitive element modulo p, show that either a is a multiple of p or a p−1  q ≡ 1  modulo p  for some prime number q that divides p − 1. 14. [M18] If e > 1 and p is an odd prime, and if a is a primitive element modulo p, prove that either a or a + p is a primitive element modulo pe. [Hint: See exercise 12.]   3.2.1.3  POTENCY  23  2 has order λ modulo m, for suitable integers κ1 and κ2.  15. [M29]  a  Let a1 and a2 be relatively prime to m, and let their orders modulo m be λ1 and λ2, respectively. If λ is the least common multiple of λ1 and λ2, prove that [Hint: Consider first 1 aκ2 aκ1 the case that λ1 is relatively prime to λ2.]  b  Let λ m  be the maximum order of any element modulo m. Prove that λ m  is a multiple of the order of each element modulo m; that is, prove that aλ m  ≡ 1  modulo m  whenever a is relatively prime to m.  Do not use Theorem B.    cid:120  16. [M24]  Existence of primitive roots.  Let p be a prime number.  a  Consider the polynomial f x  = xn + c1xn−1 +··· + cn, where the c’s are integers. Given that a is an integer for which f a  ≡ 0  modulo p , show that there exists a polynomial  q x  = xn−1 + q1xn−2 + ··· + qn−1  with integer coefficients such that f x  ≡  x− a q x   modulo p  for all integers x. b  Let f x  be a polynomial as in  a . Show that f x  has at most n distinct “roots” modulo p; that is, there are at most n integers a, with 0 ≤ a < p, such that f a  ≡ 0  modulo p . c  Because of exercise 15 b , the polynomial f x  = xλ p  −1 has p−1 distinct roots; hence there is an integer a with order p − 1.  17. [M26] Not all of the values listed in Theorem D would be found by the text’s construction; for example, 11 is not primitive modulo 5e. How can this be possible, when 11 is primitive modulo 10e, according to Theorem D? Which of the values listed in Theorem D are primitive elements modulo both 2e and 5e? 18. [M25] Prove Theorem D.  See the previous exercise.  19. [40] Make a table of some suitable multipliers, a, for each of the values of m listed in Table 3.2.1.1–1, assuming that c = 0.   cid:120  20. [M24]  G. Marsaglia.  The purpose of this exercise is to study the period length  of an arbitrary linear congruential sequence. Let Yn = 1 + a + ··· + an−1, so that Xn =  AYn + X0  mod m for some constant A by Eq. 3.2.1– 8 . a  Prove that the period length of ⟨Xn⟩ is the period length of ⟨Yn mod m′⟩, where b  Prove that the period length of ⟨Yn mod pe⟩ satisfies the following when p is prime:  i  If a mod p = 0, it is 1.  ii  If a mod p = 1, it is pe, except when p = 2 and e ≥ 2 and a mod 4 = 3.  iii  If p = 2, e ≥ 2, and a mod 4 = 3, it is twice the order of a modulo pe  see exercise 11 , unless a ≡ −1  modulo 2e  when it is 2.  iv  If a mod p > 1, it is the order of a modulo pe.  m′ = m gcd A, m .  21. [M25] In a linear congruential sequence of maximum period, let X0 = 0 and let s be the least positive integer such that as ≡ 1  modulo m . Prove that gcd Xs, m  = s.   cid:120  22. [M25] Discuss the problem of finding moduli m = bk ± bl ±1 so that the subtract-  with-borrow and add-with-carry generators of exercise 3.2.1.1–14 will have very long periods.  3.2.1.3. Potency. In the preceding section, we showed that the maximum period can be obtained when b = a − 1 is a multiple of each prime dividing m; and b must also be a multiple of 4 if m is a multiple of 4. If z is the radix of the machine being used — so that z = 2 for a binary computer, and z = 10 for a   The code  24  RANDOM NUMBERS  3.2.1.3  decimal computer — and if m is the word size ze, the multiplier   1  satisfies these conditions. Theorem 3.2.1.2A also says that we may take c = 1. The recurrence relation now has the form  a = zk + 1,  2 ≤ k < e  Xn+1 = zk + 1 Xn + 1 mod ze,   2  and this equation suggests that we can avoid the multiplication; merely shifting and adding will suffice.  For example, suppose we choose a = B2 +1, where B is the byte size of MIX.  ADD X;  SLA 2;  LDA X;  INCA 1   3  can be used in place of the instructions given in Section 3.2.1.1, and the execution time decreases from 16u to 7u.  For this reason, multipliers having form  1  have been widely discussed in the literature, and indeed they have been recommended by many authors. However, the early years of experimentation with this method showed conclusively that multipliers having the simple form in  1  should be avoided. The generated numbers just aren’t random enough.  Later in this chapter we shall be discussing some rather sophisticated theory that accounts for the badness of all the linear congruential random number gen-  erators known to be bad. However, some generatorssuch as  2  are sufficiently  awful that a comparatively simple theory can be used to rule them out. This simple theory is related to the concept of “potency,” which we shall now discuss. The potency of a linear congruential sequence with maximum period is  defined to be the least integer s such that  bs ≡ 0  modulo m .   4   Such an integer s will always exist when the multiplier satisfies the conditions of Theorem 3.2.1.2A, since b is a multiple of every prime dividing m.   We may analyze the randomness of the sequence by taking X0 = 0, since 0 occurs somewhere in the period. With this assumption, Eq. 3.2.1– 6  reduces to  Xn = an − 1 c b mod m;  n   n      2  s  and if we expand an − 1 =  b + 1 n − 1 by the binomial theorem, we find that  5   bs−1 mod m.  b + ··· +  n +    Xn = c  All terms in bs, bs+1, etc., may be ignored, since they are multiples of m. Equation  5  can be instructive, so we shall consider some special cases. If a = 1, the potency is 1; and Xn ≡ cn  modulo m , as we have already observed, so the sequence is surely not random. If the potency is 2, we have  , and again the sequence is not very random; indeed,  Xn ≡ cn + cbn  2  Xn+1 − Xn ≡ c + cbn   3.2.1.3  POTENCY  25  in this case, so the differences between consecutively generated numbers change in a simple way from one value of n to the next. The point  Xn, Xn+1, Xn+2  always lies on one of the four planes  x − 2y + z = d + m, x − 2y + z = d,  x − 2y + z = d − m, x − 2y + z = d − 2m,  in three-dimensional space, where d = cb mod m.  If the potency is 3, the sequence begins to look somewhat more random, but there is a high degree of dependency between Xn, Xn+1, and Xn+2; tests show that sequences with potency 3 are still not sufficiently good. Reasonable results have been reported when the potency is 4 or more, but they have been disputed by other people. A potency of at least 5 would seem to be required for sufficiently random values. Suppose, for example, that m = 235 and a = 2k + 1. Then b = 2k, so we find that the value b2 = 22k is a multiple of m when k ≥ 18: The potency is 2. If k = 17, 16, . . . , 12, the potency is 3, and a potency of 4 is achieved for k = 11, 10, 9. The only acceptable multipliers, from the standpoint of potency, therefore have k ≤ 8. This means a ≤ 257, and we shall see later that small multipliers are also to be avoided. We have now eliminated all multipliers of the form 2k + 1 when m = 235. When m is equal to w ± 1, where w is the word size, m is generally not divisible by high powers of primes, and a high potency is impossible  see exer- cise 6 . So in this case, the maximum-period method should not be used; the pure-multiplication method with c = 0 should be applied instead.  It must be emphasized that high potency is necessary but not sufficient for randomness; we use the concept of potency only to reject impotent genera- tors, not to accept the potent ones. Linear congruential sequences should pass the “spectral test” discussed in Section 3.3.4 before they are considered to be acceptably random.  EXERCISES 1. [M10] Show that, no matter what the byte size B of MIX happens to be, the code  3  yields a random number generator of maximum period. 2. [10] What is the potency of the generator represented by the MIX code  3 ? 3. [11] When m = 235, what is the potency of the linear congruential sequence with a = 3141592621? What is the potency if the multiplier is a = 223 + 213 + 22 + 1? 4. [15] Show that if m = 2e ≥ 8, maximum potency is achieved when a mod 8 = 5. 5. [M20] Given that m = pe1 t , where a satisfies the conditions of Theorem 3.2.1.2A and k is relatively prime to m, show that the potency is max ⌈e1 f1⌉, . . . ,⌈et ft⌉ .   cid:120  6. [20] Which of the values of m = w ± 1 in Table 3.2.1.1–1 can be used in a linear  t and a = 1 + kpf1  congruential sequence of maximum period whose potency is 4 or more?  Use the result of exercise 5.   1 . . . pft  1 . . . pet   26  RANDOM NUMBERS  3.2.1.3  7. [M20] When a satisfies the conditions of Theorem 3.2.1.2A, it is relatively prime to m; hence there is a number a′ such that aa′ ≡ 1  modulo m . Show that a′ can be expressed simply in terms of b.   cid:120  8. [M26] A random number generator defined by Xn+1 =  217 + 3 Xn mod 235 and  X0 = 1 was subjected to the following test: Let Yn = ⌊20Xn 235⌋; then Yn should be a random integer between 0 and 19, and the triples  Y3n, Y3n+1, Y3n+2  should take on each of the 8000 possible values from  0, 0, 0  to  19, 19, 19  with nearly equal frequency. But with 1,000,000 values of n tested, many triples never occurred, and others occurred much more often than they should have. Can you account for this failure?  3.2.2. Other Methods Of course, linear congruential sequences are not the only sources of random num- bers that have been proposed for computer use. In this section we shall review the most significant alternatives. Some of these methods are quite important, while others are interesting chiefly because they are not as good as a person might expect.  One of the common fallacies encountered in connection with random number generation is the idea that we can take a good generator and modify it a little, in order to get an “even more random” sequence. This is often false. For example, we know that  Xn+1 =  aXn + c  mod m  Xn+1 = aXn  mod  m + 1  + c mod m   1  leads to reasonably good random numbers; wouldn’t the sequence produced by  2  be even more random? The answer is, the new sequence is probably a great deal less random. For the whole theory breaks down, and in the absence of any theory about the behavior of the sequence  2 , we come into the area of generators of the type Xn+1 = f Xn  with the function f chosen at random; exercises 3.1–11 through 3.1–15 show that these sequences probably behave much more poorly than the sequences obtained from the more disciplined function  1 .  Let us consider another approach, in an attempt to obtain a genuine im- provement of sequence  1 . The linear congruential method can be generalized to, say, a quadratic congruential method:  Xn+1 =  dX2  n + aXn + c  mod m.   3  Exercise 8 generalizes Theorem 3.2.1.2A to obtain necessary and sufficient con- ditions on a, c, and d such that the sequence defined by  3  has a period of the maximum length m; the restrictions are not much more severe than in the linear method.  An interesting quadratic method has been proposed by R. R. Coveyou when  m is a power of two: Let X0 mod 4 = 2,   4  This sequence can be computed with about the same efficiency as  1 , without any worries of overflow. It has an interesting connection with von Neumann’s  Xn+1 = Xn Xn + 1  mod 2e,  n ≥ 0.   3.2.2  OTHER METHODS  27  original middle-square method: If we let Yn be 2eXn, so that Yn is a double- precision number obtained by placing e zeros to the right of the binary represen- tation of Xn, then Yn+1 consists of precisely the middle 2e digits of Y 2 n + 2eYn! In other words, Coveyou’s method is almost identical to a somewhat degenerate double-precision middle-square method, yet it is guaranteed to have a long period; further evidence of its randomness is proved in Coveyou’s paper cited in the answer to exercise 8.  Other generalizations of Eq.  1  also suggest themselves; for example, we might try to extend the period length of the sequence. The period of a linear congruential sequence is fairly long; when m is approximately the word size of the computer, we usually get periods on the order of 109 or more, and typical calculations will use only a very small portion of the sequence. On the other hand, when we discuss the idea of “accuracy” in Section 3.3.4 we will see that the period length influences the degree of randomness achievable in a sequence. Therefore it can be desirable to seek a longer period, and several methods are available for this purpose. One technique is to make Xn+1 depend on both Xn and Xn−1, instead of just on Xn; then the period length can be as high as m2, since the sequence will not begin to repeat until we have  Xn+λ, Xn+λ+1  =  Xn, Xn+1 . John Mauchly, in an unpublished paper presented to a statistics conference in 1949, extended the middle square method by using the recurrence Xn = middle  Xn−1 · Xn−6 . The simplest sequence in which Xn+1 depends on more than one of the  preceding values is the Fibonacci sequence,  Xn+1 =  Xn + Xn−1  mod m.   5  This generator was considered in the early 1950s, and it usually gives a period length greater than m. But tests have shown that the numbers produced by the Fibonacci recurrence are definitely not satisfactorily random, and so our main interest in  5  as a source of random numbers is that it makes a nice “bad example.” We may also consider generators of the form Xn+1 =  Xn + Xn−k  mod m,   6  when k is a comparatively large value. This recurrence was introduced by Green, Smith, and Klem [JACM 6  1959 , 527–537], who reported that, when k ≤ 15, the sequence fails to pass the “gap test” described in Section 3.3.2, although when k = 16 the test was satisfactory.  A much better type of additive generator was devised in 1958 by G. J. Mitchell and D. P. Moore [unpublished], who suggested the somewhat unusual sequence defined by  Xn =  Xn−24 + Xn−55  mod m,   7  where m is even, and where X0, . . . , X54 are arbitrary integers not all even. The constants 24 and 55 in this definition were not chosen at random; they are special values that happen to define a sequence whose least significant bits, ⟨Xn mod 2⟩, will have a period of length 255 − 1. Therefore the sequence ⟨Xn⟩ must have  n ≥ 55,   28  RANDOM NUMBERS  3.2.2  a period at least this long. Exercise 30 proves that  7  has a period of length exactly 2e−1 255 − 1  when m = 2e.  At first glance Eq.  7  may not seem to be extremely well suited to machine implementation, but in fact there is a very efficient way to generate the sequence using a cyclic list: Algorithm A  Additive number generator . Memory cells Y [1], Y [2], . . . , Y [55] are initially set to the values X54, X53, . . . , X0, respectively; j is initially equal to 24 and k is 55. Successive performances of this algorithm will produce the numbers X55, X56, . . . as output. A1. [Add.]  If we are about to output Xn at this point, Y [j] now equals Xn−24 and Y [k] equals Xn−55.  Set Y [k] ←  Y [k]+Y [j]  mod 2e, and output Y [k]. A2. [Advance.] Decrease j and k by 1. If now j = 0, set j ← 55; otherwise if k = 0, set k ← 55.  We cannot have both j = 0 and k = 0.   This algorithm in MIX is simply the following: Program A  Additive number generator . Assuming that index registers 5 and 6, representing j and k, are not touched by the remainder of the program in which this routine is embedded, the following code performs Algorithm A and leaves the result in register A.  LDA Y,6 A1. Add. ADD Y,5 Yk + Yj  overflow possible  → Yk. STA Y,6 A2. Advance. j ← j − 1. DEC5 1 k ← k − 1. DEC6 1 J5P *+2 If j = 0, set j ← 55. ENT5 55 J6P *+2 If k = 0, set k ← 55. ENT6 55 This generator is usually faster than the other methods we have been dis- cussing, since it does not require any multiplication. Besides its speed, it has the longest period we have seen yet, except in exercise 3.2.1.2–22. Furthermore, as Richard Brent has observed, it can be made to work correctly with floating point numbers, avoiding the need to convert between integers and fractions  see exercise 23 . Therefore it may well prove to be the very best source of random numbers for practical purposes. The main reason why it is difficult to recommend sequences like  7  wholeheartedly is that there is still very little theory to prove that they do or do not have desirable randomness properties; essentially all we know for sure is that the period is very long, and this is not enough. John Reiser  Ph.D. thesis, Stanford University, 1977  has shown, however, that an additive sequence like  7  will be well distributed in high dimensions, provided that a certain plausible conjecture is true  see exercise 26 .  The numbers 24 and 55 in  7  are commonly called lags, and the numbers Xn defined by  7  are said to form a lagged Fibonacci sequence. Lags like  24, 55  work well because of theoretical results developed in some of the exercises   3.2.2  OTHER METHODS  29  Table 1   24, 55   38, 89   LAGS THAT YIELD LONG PERIODS MOD 2  576, 3217   4187, 9689    273, 607   1029, 2281    83, 258   107, 378    37, 100   30, 127    7083, 19937   9739, 23209   For extensions of this table, see N. Zierler and J. Brillhart, Information and Control 13  1968 , 541–554, 14  1969 , 566–569, 15  1969 , 67–69; Y. Kurita and M. Matsumoto, Math. Comp. 56  1991 , 817–821; Heringa, Blöte, and Compagner, Int. J. Mod. Phys. C3  1992 , 561–564.  below. It is of course better to use somewhat larger lags when an application happens to use, say, groups of 55 values at a time; the numbers generated by  7  will never have Xn lying strictly between Xn−24 and Xn−55  see exercise 2 . J.-M. Normand, H. J. Herrmann, and M. Hajjar detected slight biases in the numbers generated by  7  when they did extensive high-precision Monte Carlo studies requiring 1011 random numbers [J. Statistical Physics 52  1988 , 441– 446]; but larger values of k decreased the bad effects. Table 1 lists several useful pairs  l, k  for which the sequence Xn =  Xn−l + Xn−k  mod 2e has period length 2e−1 2k − 1 . The case  l, k  =  30, 127  should be large enough for most applications, especially in combination with other randomness-enhancing techniques that we will discuss later.  George Marsaglia [Comp. Sci. and Statistics: Symposium on the Interface  n ≥ 55,  16  1984 , 3–10] has suggested replacing  7  by Xn =  Xn−24 · Xn−55  mod m,   7′  where m is a multiple of 4 and where X0 through X54 are odd, not all congruent to 1  modulo 4 . Then the second-least significant bits have a period of 255 − 1, while the most significant bits are more thoroughly mixed than before since they depend on all bits of Xn−24 and Xn−55 in an essential way. Exercise 31 shows that the period length of sequence  7′  is only slightly less than that of  7 .  Lagged Fibonacci generators have been used successfully in many situations since 1958, so it came as a shock to discover in the 1990s that they actually fail an extremely simple, non-contrived test for randomness  see exercise 3.3.2–31 . A workaround that avoids such problems by discarding appropriate elements of the sequence is described near the end of this section.  Instead of considering purely additive or purely multiplicative sequences, we can construct useful random number generators by taking general linear combinations of Xn−1, . . . , Xn−k for small k. In this case the best results occur when the modulus m is a large prime; for example, m can be chosen to be the largest prime number that fits in a single computer word  see Table 4.5.4–2 . When m = p is prime, the theory of finite fields tells us that it is possible to find multipliers a1, . . . , ak such that the sequence defined by  Xn =  a1Xn−1 + ··· + akXn−k  mod p   8  has period length pk − 1; here X0, . . . , Xk−1 may be chosen arbitrarily but not all zero.  The special case k = 1 corresponds to a multiplicative congruential se- quence with prime modulus, with which we are already familiar.  The constants   30  RANDOM NUMBERS  3.2.2  a1, . . . , ak in  8  have the desired property if and only if the polynomial  f x  = xk − a1xk−1 − ··· − ak   9  is a “primitive polynomial modulo p,” that is, if and only if this polynomial has a root that is a primitive element of the field with pk elements  see exercise 4.6.2–16 . Of course, the mere fact that suitable constants a1, . . . , ak exist giving a period of length pk − 1 is not enough for practical purposes; we must be able to find them, and we can’t simply try all pk possibilities, since p is on the order of the computer’s word size. Fortunately there are exactly φ pk − 1  k suitable choices of  a1, . . . , ak , so there is a fairly good chance of hitting one after making a few random tries. But we also need a way to tell quickly whether or not  9  is a primitive polynomial modulo p; it is certainly unthinkable to generate up to pk − 1 elements of the sequence and wait for a repetition! Methods of testing for primitivity modulo p are discussed by Alanen and Knuth in Sankhy¯a A26  1964 , 305–328. The following criteria can be used: Let r =  pk − 1   p − 1 . i   −1 k−1ak must be a primitive root modulo p.  See Section 3.2.1.2.  ii  The polynomial xr must be congruent to  −1 k−1ak, modulo f x  and p. iii  The degree of xr q mod f x , using polynomial arithmetic modulo p, must  be positive, for each prime divisor q of r. Efficient ways to compute the polynomial xn mod f x , using polynomial  arithmetic modulo a given prime p, are discussed in Section 4.6.2. In order to carry out this test, we need to know the prime factorization of r =  pk − 1   p − 1 , and this is the limiting factor in the calculation; r can be factored in a reasonable amount of time when k = 2, 3, and perhaps 4, but higher values of k are difficult to handle when p is large. Even k = 2 essentially doubles the number of “significant random digits” over what is achievable with k = 1, so larger values of k will rarely be necessary.  An adaptation of the spectral test  Section 3.3.4  can be used to rate the sequence of numbers generated by  8 ; see exercise 3.3.4–24. The considerations of that section show that we should not make the obvious choice of a1 = +1 or −1 when a primitive polynomial of that form exists; it is better to pick large, essentially “random” values of a1, . . . , ak that satisfy the conditions, and to verify the choice by applying the spectral test. A significant amount of computation is involved in finding a1, . . . , ak, but all known evidence indicates that the result will be a very satisfactory source of random numbers. We essentially achieve the randomness of a linear congruential generator with k-tuple precision, using only single precision operations.  The special case p = 2 is of independent interest. Sometimes a random number generator is desired that merely produces a random sequence of bits — zeros and ones — instead of fractions between zero and one. There is a simple way to generate a highly random bit sequence on a binary computer, manipulating k-bit words: Start with an arbitrary nonzero binary word X. To get the next random bit of the sequence, do the following operations, shown in MIX’s language   OTHER METHODS  31  3.2.2   see exercise 16 :  LDA X ADD X JNOV *+2 XOR A STA X   Assume that overflow is now “off.”  Shift left one bit. Jump if the high bit was originally zero. Otherwise adjust the number with “exclusive or.”   10   The fourth instruction here is the “exclusive or” operation found on nearly all binary computers  see exercise 2.5–28 and Section 7.1.3 ; it changes each bit position of rA in which location A has a “1” bit. The value in location A is the binary constant  a1 . . . ak 2, where xk − a1xk−1 − ··· − ak is a primitive polynomial modulo 2 as above. After the code  10  has been executed, the next bit of the generated sequence may be taken as the least significant bit of word X. Alternatively, we could consistently use the most significant bit of X, if the most significant bit is more convenient.  Fig. 1. Successive contents of the computer word X in the binary method, assuming that k = 4 and CONTENTS A  =  0011 2.  1011 0101 1010 0111 1110 1111 1101 1001 0001 0010 0100 1000 0011 0110 1100 1011 For example, consider Fig. 1, which illustrates the sequence generated for k = 4 and CONTENTS A  =  0011 2. This is, of course, an unusually small value for k. The right-hand column shows the sequence of bits of the sequence, namely 1101011110001001 . . . , repeating in a period of length 2k−1 = 15. This sequence is quite random, considering that it was generated with only four bits of memory; to see this, consider the adjacent sets of four bits occurring in the period, namely 1101, 1010, 0101, 1011, 0111, 1111, 1110, 1100, 1000, 0001, 0010, 0100, 1001, 0011, 0110. In general, every possible adjacent set of k bits occurs exactly once in the period, except the set of all zeros, since the period length is 2k − 1; thus, adjacent sets of k bits are essentially independent. We shall see in Section 3.5 that this is a very strong criterion for randomness when k is, say, 30 or more. Theoretical results illustrating the randomness of this sequence are given in an article by R. C. Tausworthe, Math. Comp. 19  1965 , 201–209. Primitive polynomials modulo 2 of degree ≤ 168 have been tabulated by  W. Stahnke, Math. Comp. 27  1973 , 977–980. When k = 35, we may take  CONTENTS A  =  00000000000000000000000000000000101 2,  but the considerations of exercises 18 and 3.3.4–24 imply that it would be better to find “random” constants that define primitive polynomials modulo 2.   32  RANDOM NUMBERS  3.2.2  Caution: Several people have been trapped into believing that this random bit-generation technique can be used to generate random whole-word fractions  .X0X1 . . . Xk−1 2,  .XkXk+1 . . . X2k−1 2, . . . ; but it is actually a poor source of random fractions, even though the bits are individually quite random. Exer- cise 18 explains why.  Mitchell and Moore’s additive generator  7  is essentially based on the concept of primitive polynomials: The polynomial x55 + x24 + 1 is primitive, and Table 1 is essentially a listing of certain primitive trinomials modulo 2. A generator almost identical to that of Mitchell and Moore was independently discovered in 1971 by T. G. Lewis and W. H. Payne [JACM 20  1973 , 456–468], but using “exclusive or” instead of addition; this makes the period length exactly 255 − 1. Each bit position in the sequence of Lewis and Payne runs through the same periodic sequence, but has its own starting point. Experience has shown that  7  gives better results. We have now seen that sequences with 0 ≤ Xn < m and period mk − 1 can be constructed without great difficulty, when Xn is a suitable function of Xn−1, . . . , Xn−k and when m is prime. The highest conceivable period for any sequence defined by a relation of the form  0 ≤ Xn < m,  Xn = f Xn−1, . . . , Xn−k ,   11  is easily seen to be mk. M. H. Martin [Bull. Amer. Math. Soc. 40  1934 , 859– 864] was the first person to show that functions achieving this maximum period are possible for all m and k. His method is easy to state  exercise 17  and reasonably efficient to program  exercise 29 , but it is unsuitable for random number generation because it changes the value of Xn−1 + ··· + Xn−k very slowly: All k-tuples occur, but not in a very random order. A better class of functions f that yield the maximum period mk is considered in exercise 21. The corresponding programs are, in general, not as efficient for random number generation as other methods we have described, but they do give demonstrable randomness when the period as a whole is considered.  Many other schemes have been proposed for random number generation. The most interesting of these alternative methods may well be the inversive congruential sequences suggested by Eichenauer and Lehn [Statistische Hefte 27  1986 , 315–326]:  Xn+1 =  aX−1  n + c  mod p .   12  Here p is prime, Xn ranges over the set {0, 1, . . . , p − 1, ∞}, and inverses are defined by 0−1 = ∞, ∞−1 = 0, otherwise X−1X ≡ 1  modulo p . Since 0 is always followed by ∞ and then by c in this sequence, we could simply define 0−1 = 0 for purposes of implementation; but the theory is cleaner and easier to develop when 0−1 = ∞. Efficient algorithms suitable for hardware implementation are available for computing X−1 modulo p; see, for example, exercise 4.5.2–39. Unfortunately, however, this operation is not in the repertoire of most computers. Exercise 35 shows that many choices of a and c yield the maximum period length p + 1. Exercise 37 demonstrates the most important   3.2.2  OTHER METHODS  33  property: structure that is characteristic of linear congruential sequences.  Inversive congruential sequences are completely free of the lattice  Another important class of techniques deals with the combination of random number generators. There will always be people who feel that the linear con- gruential methods, additive methods, etc., are all too simple to give sufficiently random sequences; and it may never be possible to prove that their skepticism is unjustified — indeed, they may be right — so it is pretty useless to argue the point. There are reasonably efficient ways to combine two sequences into a third one that should be haphazard enough to satisfy all but the most hardened skeptic. Suppose we have two sequences X0, X1, . . . and Y0, Y1, . . . of random numbers between 0 and m − 1, preferably generated by two unrelated methods. Then we can, for example, use one random sequence to permute the elements of another, as suggested by M. D. MacLaren and G. Marsaglia [JACM 12  1965 , 83–89; see also Marsaglia and Bray, CACM 11  1968 , 757–759]: Algorithm M  Randomizing by shuffling . Given methods for generating two sequences ⟨Xn⟩ and ⟨Yn⟩, this algorithm will successively output the terms of a “considerably more random” sequence. We use an auxiliary table V [0], V [1], . . . , V [k − 1], where k is some number chosen for convenience, usually in the neighborhood of 100. Initially, the V -table is filled with the first k values of the X-sequence. M1. [Generate X, Y.] Set X and Y equal to the next members of the sequences  M2. [Extract j.] Set j ← ⌊kY  m⌋, where m is the modulus used in the sequence  ⟨Xn⟩ and ⟨Yn⟩, respectively. ⟨Yn⟩; that is, j is a random value, 0 ≤ j < k, determined by Y.  M3. [Exchange.] Output V [j] and then set V [j] ← X.  As an example, assume that Algorithm M is applied to the following two  sequences, with k = 64:  X0 = 5772156649, Y0 = 1781072418,  Xn+1 =  3141592653Xn + 2718281829  mod 235; Yn+1 =  2718281829Yn + 3141592653  mod 235.   13   On intuitive grounds it appears safe to predict that the sequence obtained by applying Algorithm M to  13  will satisfy virtually anyone’s requirements for randomness in a computer-generated sequence, because the relationship between nearby terms of the output has been almost entirely obliterated. Furthermore, the time required to generate this sequence is only slightly more than twice as long as it takes to generate the sequence ⟨Xn⟩ alone. Exercise 15 proves that the period length of Algorithm M’s output will be the least common multiple of the period lengths of ⟨Xn⟩ and ⟨Yn⟩, in most situations of practical interest. In particular, if we reject the value 0 when it occurs in the Y -sequence, so that ⟨Yn⟩ has period length 235 − 1, the numbers generated by Algorithm M from  13  will have a period of length 270 − 235. [See J. Arthur Greenwood, Computer Science and Statistics: Symposium on the Interface 9  1976 , 222–227.]   34  RANDOM NUMBERS  3.2.2  However, there is an even better way to shuffle the elements of a sequence, discovered by Carter Bays and S. D. Durham [ACM Trans. Math. Software 2  1976 , 59–64]. Their approach, although it appears to be superficially similar to Algorithm M, can give surprisingly better performance even though it requires only one input sequence ⟨Xn⟩ instead of two: Algorithm B  Randomizing by shuffling . Given a method for generating a sequence ⟨Xn⟩, this algorithm will successively output the terms of a “consider- ably more random” sequence, using an auxiliary table V [0], V [1], . . . , V [k − 1] as in Algorithm M. Initially the V -table is filled with the first k values of the X-sequence, and an auxiliary variable Y is set equal to the  k + 1 st value. B1. [Extract j.] Set j ← ⌊kY  m⌋, where m is the modulus used in the sequence  ⟨Xn⟩; that is, j is a random value, 0 ≤ j < k, determined by Y.  B2. [Exchange.] Set Y ← V [j], output Y, and then set V [j] to the next member  of the sequence ⟨Xn⟩. The reader is urged to work exercises 3 and 5, in order to get a feeling for  the difference between Algorithms M and B.  On MIX we may implement Algorithm B by taking k equal to the byte size, obtaining the following simple generation scheme once the initialization has been done:  LD6 Y 1:1  LDA X INCA 1 MUL A STX X LDA V,6 STA Y STX V,6 The output appears in register A. Notice that Algorithm B requires only  j ← high-order byte of Y. rA ← Xn.  see exercise 3.2.1.1–1  rX ← Xn+1. “n ← n + 1.” Y ← V [j]. V [j] ← Xn.   14   four instructions of overhead per generated number.  F. Gebhardt [Math. Comp. 21  1967 , 708–709] found that satisfactory random sequences were produced by Algorithm M even when it was applied to a sequence as nonrandom as the Fibonacci sequence, with Xn = F2n mod m and Yn = F2n+1 mod m. However, it is also possible for Algorithm M to produce a sequence less random than the original sequences, if ⟨Xn⟩ and ⟨Yn⟩ are strongly related, as shown in exercise 3. Such problems do not seem to arise with Algorithm B. Since Algorithm B won’t make a sequence any less random, and since it enhances the randomness with very little extra cost, it can be recommended for use in combination with any other random number generator. Shuffling methods have an inherent defect, however: They change only the order of the generated numbers, not the numbers themselves. For most purposes the order is the critical thing, but if a random number generator fails the “birthday spacings” test discussed in Section 3.3.2 or the random walk test of exercise 3.3.2–31 it will not fare much better after it has been shuffled. Shuffling   3.2.2  OTHER METHODS  35  also has the comparative disadvantage that it does not allow us to start at a given place in the period, or to skip quickly from Xn to Xn+k for large k. Many people have therefore suggested combining two sequences ⟨Xn⟩ and ⟨Yn⟩ in a much simpler way, which avoids both of the defects of shuffling: We can use a combination like  Zn =  Xn − Yn  mod m   15  when 0 ≤ Xn < m and 0 ≤ Yn < m′ ≤ m. Exercises 13 and 14 discuss the period length of such sequences; exercise 3.3.2–23 shows that  15  tends to enhance the randomness when the seeds X0 and Y0 are chosen independently.  An even simpler way to remove the structural biases of arithmetically gen- erated numbers was proposed already in the early days of computing by J. Todd and O. Taussky Todd [Symp. on Monte Carlo Methods  Wiley, 1956 , 15–28]: We can just throw away some numbers of the sequence. Their suggestion was of little use with linear congruential generators, but it has become quite appropriate nowadays in connection with generators like  7  that have extremely long periods, because we have plenty of numbers to discard.  The simplest way to improve the randomness of  7  is to use only every jth term, for some small j. But a better scheme, which may be even simpler, is to use  7  to produce, say, 500 random numbers in an array and to use only the first 55 of them. After those 55 have been consumed, we generate 500 more in the same way. This idea was proposed by Martin Lüscher [Computer Physics Communications 79  1994 , 100–110], motivated by the theory of chaos in dynamical systems: We can regard  7  as a process that maps 55 values  Xn−55, . . . , Xn−1  into another vector of 55 values  Xn+t−55, . . . , Xn+t−1 . Suppose we generate t ≥ 55 values and use the first 55 of them. Then if t = 55 the new vector of values is rather close to the old; but if t ≈ 500 there is almost no correlation between old and new  see exercise 33 . For the analogous case of add-with-carry or subtract-with-borrow generators  exercise 3.2.1.1–14 , the vectors are in fact known to be the radix-b representation of numbers in a linear congruential generator, and the relevant multiplier when we generate t numbers at a time is b−t. Lüscher’s theory for this case can therefore be confirmed with the spectral test of Section 3.3.4. A portable random number generator, based on a lagged Fibonacci sequence enhanced with Lüscher’s approach, appears in Section 3.6, together with further commentary. Random number generators typically do only a few multiplications and or additions to get from one element of the sequence to the next. When such generators are combined as suggested above, common sense tells us that the resulting sequences ought to be indistinguishable from truly random numbers. But intuitive hunches are no substitute for rigorous mathematical proof. If we are willing to do more work — say 1000 or 1000000 times as much — we can obtain sequences for which substantially better theoretical guarantees of randomness are available.  For example, consider the sequence of bits B1, B2, . . . generated by  Xn+1 = X2  n mod M,  Bn = Xn mod 2,   16    36  RANDOM NUMBERS  3.2.2  [Blum, Blum, and Shub, SICOMP 15  1986 , 364–383], or the more elaborate sequence generated by  Bn = Xn · Z mod 2,  Xn+1 = X2  n mod M,   17  where the dot product of r-bit binary numbers  xr−1 . . . x0 2 and  zr−1 . . . z0 2 is xr−1zr−1 + ··· + x0z0; here Z is an r-bit “mask,” and r is the number of bits in M. The modulus M should be the product of two large primes of the form 4k + 3, and the starting value X0 should be relatively prime to M. Rule  17 , suggested by Leonid Levin, is a take-off on von Neumann’s original middle-square method; we will call it the muddle-square method, because it jumbles the bits of the squares. Rule  16  is, of course, the special case Z = 1.  Section 3.5F contains a proof that, when X0, Z, and M are chosen at random, the sequences generated by  16  and  17  pass all statistical tests for randomness that require no more work than factoring large numbers. In other words, the bits cannot be distinguished from truly random numbers by any computation lasting less than 100 years on today’s fastest computers, when M is suitably large, unless it is possible to find the factors of a nontrivial fraction of such numbers much more rapidly than is presently known. Formula  16  is simpler than  17 , but the modulus M in  16  has to be somewhat larger than it does in  17  if we want to achieve the same statistical guarantees.  EXERCISES   cid:120  1. [12] In practice, we form random numbers using Xn+1 =  aXn +c  mod m, where  the X’s are integers, afterwards treating them as the fractions Un = Xn m. The recurrence relation for Un is actually  Un+1 =  aUn + c m  mod 1.  Discuss the generation of random sequences using this relation directly, by making use of floating point arithmetic on the computer.   cid:120  2. [M20] A good source of random numbers will have Xn−1 < Xn+1 < Xn about  one-sixth of the time, since each of the six possible relative orders of Xn−1, Xn, and Xn+1 should be equally probable. However, show that the ordering above never occurs if the Fibonacci sequence  5  is used. 3. [23]  a  What sequence comes from Algorithm M if  X0 = 0, Xn+1 =  5Xn + 3  mod 8,  Y0 = 0,  Yn+1 =  5Yn + 1  mod 8,  and k = 4?  Note that the potency is two, so ⟨Xn⟩ and ⟨Yn⟩ aren’t extremely random to start with.   b  What happens if Algorithm B is applied to this same sequence ⟨Xn⟩ with k = 4? 4. [00] Why is the most significant byte used in the first line of program  14 , instead of some other byte?   cid:120  5. [20] Discuss using Xn = Yn in Algorithm M, in order to improve the speed of  generation. Is the result analogous to Algorithm B? 6. [10] In the binary method  10 , the text states that the low-order bit of X is random, if the code is performed repeatedly. Why isn’t the entire word X random?   3.2.2  OTHER METHODS  37  7. [20] Show that a complete sequence of length 2e  that is, a sequence in which each of the 2e possible sets of e adjacent bits occurs just once in the period  may be obtained if program  10  is changed to the following: JNOV *+3 JAZ *+2  LDA X JANZ *+2  LDA ADD  XOR STA  A X  A X  8. [M39] Prove that the quadratic congruential sequence  3  has period length m if and only if the following conditions are satisfied:  d ≡ a − 1  modulo 2 , if m is a multiple of 2;  i  c is relatively prime to m; ii  d and a − 1 are both multiples of p, for all odd primes p dividing m; iii  d is even, and d ≡ a − 1  modulo 4 , if m is a multiple of 4; iv  d ̸≡ 3c  modulo 9 , if m is a multiple of 9. [Hint: The sequence defined by X0 = 0, Xn+1 = dX2 n + aXn + c modulo m has a period of length m only if the same sequence modulo any divisor r of m has period length r.]   cid:120  9. [M24]  R. R. Coveyou.  Use the result of exercise 8 to prove that the modified  middle-square method  4  has a period of length 2e−2. 10. [M29] Show that if X0 and X1 are not both even and if m = 2e, the period of the Fibonacci sequence  5  is 3 · 2e−1. 11. [M36] The purpose of this exercise is to analyze certain properties of integer sequences satisfying the recurrence relation  Xn = a1Xn−1 + ··· + akXn−k,  n ≥ k.  If we can calculate the period length of this sequence modulo m = pe, when p is prime, the period length with respect to an arbitrary modulus m is the least common multiple of the period lengths for the prime power factors of m. a  If f z , a z , b z  are polynomials with integer coefficients, let us write a z  ≡ b z   modulo f z  and m  if a z  = b z  + f z u z  + mv z  for some polynomials u z  and v z  with integer coefficients. Prove that the following statement holds when f 0  = 1 and pe > 2: If zλ ≡ 1  modulo f z  and pe  and zλ ̸≡ 1  modulo f z  and pe+1 , then zpλ ≡ 1  modulo f z  and pe+1  and zpλ ̸≡ 1  modulo f z  and pe+2 .  b  Let f z  = 1 − a1z − ··· − akzk, and let  G z  = 1 f z  = A0 + A1z + A2z  2 + ··· .  Let λ m  denote the period length of ⟨An mod m⟩. Prove that λ m  is the smallest positive integer λ such that zλ ≡ 1  modulo f z  and m . c  Given that p is prime, pe > 2, and λ pe  ̸= λ pe+1 , prove that λ pe+r  = prλ pe  for all r ≥ 0.  Thus, to find the period length of the sequence ⟨An mod 2e⟩, we can compute λ 4 , λ 8 , λ 16 , . . . until we find the smallest e ≥ 3 such that λ 2e  ̸= λ 4 ; then the period length is determined mod 2e for all e. Exercise 4.6.3–26 explains how to calculate Xn for large n in O log n  operations.   d  Show that any sequence of integers satisfying the recurrence stated at the begin- ning of this exercise has the generating function g z  f z , for some polynomial g z  with integer coefficients. e  Given that the polynomials f z  and g z  in part  d  are relatively prime modulo p  see Section 4.6.1 , prove that the sequence ⟨Xn mod pe⟩ has exactly the same   38  RANDOM NUMBERS  3.2.2  period length as the special sequence ⟨An mod pe⟩ in  b .  No longer period could be obtained by any choice of X0, . . . , Xk−1, since the general sequence is a linear combination of “shifts” of the special sequence.  [Hint: By exercise 4.6.2–22  Hensel’s lemma , there exist polynomials such that a z f z  + b z g z  ≡ 1  modulo pe .]   cid:120  12. [M28] Find integers X0, X1, a, b, and c such that the sequence  Xn+1 =  aXn + bXn−1 + c  mod 2e,  n ≥ 1,  [Hint:  It follows that  has the longest period length of all sequences of this type. Xn+2 =   a + 1 Xn+1 +  b − a Xn − bXn−1  mod 2e; see exercise 11 c .] 13. [M20] Let ⟨Xn⟩ and ⟨Yn⟩ be sequences of integers mod m with periods of lengths λ1 and λ2, and combine them by letting Zn =  Xn + Yn  mod m. Show that if λ1 and λ2 are relatively prime, the sequence ⟨Zn⟩ has a period of length λ1λ2. 14. [M24] Let Xn, Yn, Zn, λ1, λ2 be as in the previous exercise. Suppose that the prime factorization of λ1 is 2e23e35e5 . . . , and similarly suppose that λ2 = 2f23f35f5 . . . . Let gp =  max ep, fp  if ep ̸= fp, otherwise 0 , and let λ0 = 2g23g35g5 . . . . Show that the period length λ′ of the sequence ⟨Zn⟩ is a multiple of λ0, and it is a divisor of λ = lcm λ1, λ2 . In particular, λ′ = λ if  ep ̸= fp or ep = fp = 0  for each prime p. 15. [M27] Let the sequence ⟨Xn⟩ in Algorithm M have period length λ1, and assume that all elements of its period are distinct. Let qn = min{r  r > 0 and ⌊kYn−r m⌋ = ⌊kYn m⌋}. Assume that qn < 1 2 λ1 for all n ≥ n0, and that the sequence ⟨qn⟩ has  cid:120  16. [M28] Let CONTENTS A  in method  10  be  a1a2 . . . ak 2 in binary notation. Show period length λ2. Let λ be the least common multiple of λ1 and λ2. Prove that the output sequence ⟨Zn⟩ produced by Algorithm M has a period of length λ.  that the generated sequence of low-order bits X0, X1, . . . satisfies the relation  Xn =  a1Xn−1 + a2Xn−2 + ··· + akXn−k  mod 2.  [This may be regarded as another way to define the sequence, although the connection between this relation and the efficient code  10  is not apparent at first glance!] 17. [M33]  M. H. Martin, 1934.  Let m and k be positive integers, and let X1 = X2 = ··· = Xk = 0. For all n > 0, set Xn+k equal to the largest nonnegative value y < m such that the k-tuple  Xn+1, . . . , Xn+k−1, y  has not already occurred in the sequence; in other words,  Xn+1, . . . , Xn+k−1, y  must differ from  Xr+1, . . . , Xr+k  for 0 ≤ r < n. In this way, each possible k-tuple will occur at most once in the sequence. Eventually the process will terminate, when we reach a value of n such that  Xn+1, . . . , Xn+k−1, y  has already occurred in the sequence for all nonnegative y < m. For example, if m = k = 3 the sequence is 00022212202112102012001110100, and the process terminates at this point.  a  Prove that when the sequence terminates, we have Xn+1 = ··· = Xn+k−1 = 0.  b  Prove that every k-tuple  a1, a2, . . . , ak  of elements with 0 ≤ aj < m occurs in the sequence; hence the sequence terminates when n = mk. [Hint: Prove that the k-tuple  a1, . . . , as, 0, . . . , 0  appears, when as ̸= 0, by induction on s.] Note that if we now define f Xn, . . . , Xn+k−1  = Xn+k for 1 ≤ n ≤ mk, setting Xmk+k = 0, we obtain a function of maximum possible period. 18. [M22] Let ⟨Xn⟩ be the sequence of bits generated by method  10 , with k = 35 and CONTENTS A  =  00000000000000000000000000000000101 2. Let Un be the binary fraction  .XnkXnk+1 . . . Xnk+k−1 2; show that this sequence ⟨Un⟩ fails the serial test on pairs  Section 3.3.2B  when d = 8.   3.2.2  OTHER METHODS  39  19. [M41] For each prime p specified in the first column of Table 2 in Section 4.5.4, find suitable constants a1 and a2 as suggested in the text, such that the period length of  8 , when k = 2, is p2 − 1.  See Eq. 3.3.4– 39  for an example.  20. [M40] Calculate constants suitable for use as CONTENTS A  in method  10 , having approximately the same number of zeros as ones, for 2 ≤ k ≤ 64. 21. [M35]  D. Rees.  The text explains how to find functions f such that the sequence  11  has period length mk − 1, provided that m is prime and X0, . . . , Xk−1 are not all zero. Show that such functions can be modified to obtain sequences of type  11  with  cid:120  22. [M24] The text restricts discussion of the extended linear sequences  8  to the period length mk, for all integers m. [Hints: Consider the results of exercises 7 and 13, and sequences such as ⟨pX2n + X2n+1⟩.]  case that m is prime. Prove that reasonably long periods can also be obtained when m is “squarefree,” that is, the product of distinct primes.  Examination of Table 3.2.1.1–1 shows that m = w ± 1 often satisfies this hypothesis; many of the results of the text can therefore be carried over to that case, which is somewhat more convenient for calculation.    cid:120  23. [20] Discuss the sequence defined by Xn =  Xn−55 − Xn−24  mod m as an alter-  0 ≤ n < k;  native to  7 . 24. [M20] Let 0 < l < k. Prove that the sequence of bits defined by the recurrence Xn =  Xn−k+l + Xn−k  mod 2 has period length 2k − 1 whenever the sequence defined by Yn =  Yn−l + Yn−k  mod 2 does. 25. [26] Discuss the alternative to Program A that changes all 55 entries of the Y table every 55th time a random number is required. 26. [M48]  J. F. Reiser.  Let p be prime and let k be a positive integer. Given integers a1, . . . , ak and x1, . . . , xk, let λα be the period of the sequence ⟨Xn⟩ generated by the recurrence Xn =  a1Xn−1 + ··· + akXn−k  mod pα, n ≥ k; Xn = xn mod pα, and let Nα be the number of 0s that occur in the period  the number of indices j such that µα ≤ j < µα + λα and Xj = 0 . Prove or disprove the following conjecture: There exists a constant c  depending possibly on p and k and a1, . . . , ak  such that Nα ≤ cpα k−2   k−1  for all α and all x1, . . . , xk. [Notes: Reiser has proved that if the recurrence has maximum period length mod p  that is, if λ1 = pk−1 , and if the conjecture holds, then the k-dimensional discrepancy of ⟨Xn⟩ will be O αkp−α  k−1   as α → ∞; thus an additive generator like  7  would be well distributed in 55 dimensions, when m = 2e and the entire period is considered.  See Section 3.3.4 for the definition of discrepancy in k dimensions.  The conjecture is a very weak condition, for if ⟨Xn⟩ takes on each value about equally often and if λα = pα−1 pk − 1 , the quantity Nα ≈  pk − 1  p does not grow at all as α increases. Reiser has verified the conjecture for k = 3. On the other hand he has shown that it is possible to find unusually bad starting values x1, . . . , xk  depending on α  so that N2α ≥ pα, provided that λα = pα−1 pk − 1  and k ≥ 3 and α is sufficiently large.] 27. [M30] Suppose Algorithm B is being applied to a sequence ⟨Xn⟩ whose period length is λ, where λ ≫ k. Show that for fixed k and all sufficiently large λ, the output of the sequence will eventually be periodic with the same period length λ, unless ⟨Xn⟩ isn’t very random to start with. [Hint: Find a pattern of consecutive values of ⌊kXn m⌋ that causes Algorithm B to “synchronize” its subsequent behavior.]   40  RANDOM NUMBERS  3.2.2  28. [40]  A. G. Waterman.  Experiment with linear congruential sequences with m the square or cube of the computer word size, while a and c are single-precision numbers.   cid:120  29. [40] Find a good way to compute the function f x1, . . . , xk  defined by Martin’s  sequence in exercise 17, given only the k-tuple  x1, . . . , xk . 30. [M37]  R. P. Brent.  Let f x  = xk − a1xk−1 −···− ak be a primitive polynomial modulo 2, and suppose that X0, . . . , Xk−1 are integers not all even. a  Prove that the period of the recurrence Xn =  a1Xn−1 + ··· + akXn−k  mod 2e is 2e−1 2k − 1  for all e ≥ 1 if and only if f x 2 + f −x 2 ̸≡ 2f x2  and f x 2 + f −x 2 ̸≡ 2 −1 kf −x2   modulo 8 . [Hint: We have x2k ≡ −x  modulo 4 and f x   if and only if f x 2 + f −x 2 ≡ 2f x2   modulo 8 .] b  Prove that this condition always holds when the polynomial f x  = xk ± xl ± 1 is  primitive modulo 2 and k > 2.  31. [M30]  G. Marsaglia.  What is the period length of the sequence  7′  when m = 2e ≥ 8? Assume that X0, . . . , X54 are not all ≡ ±1  modulo 8 . 32. [M21] What recurrences are satisfied by the elements of the subsequences ⟨X2n⟩  cid:120  33. [M23]  a  Let gn z  = Xn+30 +Xn+29z+···+Xnz30 +Xn+54z31 +···+Xn+31z54, and ⟨X3n⟩, when Xn =  Xn−24 + Xn−55  mod m? where the X’s satisfy the lagged Fibonacci recurrence  7 . Find a simple relation between gn z  and gn+t z .  b  Express X500 in terms of X0, . . . , X54. 34. [M25] Prove that the inversive congruential sequence  12  has period p + 1 if and only if the polynomial f x  = x2−cx−a has the following two properties:  i  xp+1 mod f x  is a nonzero constant, when computed with polynomial arithmetic modulo p;  ii  x p+1  q mod f x  has degree 1 for every prime q that divides p+1. [Hint: Consider powers of the matrix   0 35. [HM35] How many pairs  a, c  satisfy the conditions of exercise 34? 36. [M25] Prove that the inversive congruential sequence Xn+1 =  aX−1 X0 = 1, e ≥ 3, has period length 2e−1 whenever a mod 4 = 1 and c mod 4 = 2.   cid:120  37. [HM32] Let p be prime and assume that Xn+1 =  aX−1  n + c  mod p defines an inversive congruential sequence of period p + 1. Also let 0 ≤ b1 < ··· < bd ≤ p, and consider the set  n + c  mod 2e,  c .] 1  a  V = { Xn+b1 , Xn+b2 , . . . , Xn+bd   0 ≤ n ≤ p and Xn+bj ̸= ∞ for 1 ≤ j ≤ d}.  This set contains p + 1 − d vectors, any d of which lie in some  d − 1 -dimensional hyperplane H = { v1, . . . , vd   r1v1 +···+ rdvd ≡ r0  modulo p }, where  r1, . . . , rd  ̸≡  0, . . . , 0 . Prove that no d + 1 vectors of V lie in the same hyperplane.   3.3  STATISTICAL TESTS  41  3.3. STATISTICAL TESTS Our main purpose is to obtain sequences that behave as if they are random. So far we have seen how to make the period of a sequence so long that for practical purposes it never will repeat; this is an important criterion, but it by no means guarantees that the sequence will be useful in applications. How then are we to decide whether a sequence is sufficiently random?  If we were to give some randomly chosen man a pencil and paper and ask him to write down 100 random decimal digits, chances are very slim that he would produce a satisfactory result. People tend to avoid things that seem nonrandom, such as pairs of equal adjacent digits  although about one out of every 10 digits should equal its predecessor . And if we would show that same man a table of truly random digits, he would quite probably tell us they are not random at all; his eye would spot certain apparent regularities.  According to Dr. I. J. Matrix and Donald C. Rehkopf  as quoted by Martin Gardner in Scientific American, January, 1965 , “Mathematicians consider the decimal expansion of π a random series, but to a modern numerologist it is rich with remarkable patterns.” Dr. Matrix has pointed out, for example, that the first repeated two-digit number in π’s expansion is 26, and its second appearance comes in the middle of a curious repetition pattern:   cid:7  cid:4    cid:7  cid:4   3.14159265358979323846264338327950   cid:6  cid:5  cid:6  cid:5  cid:6  cid:5    cid:6  cid:5  cid:6  cid:5  cid:6  cid:5    1   After listing a dozen or so further properties of these digits, he observed that π, when correctly interpreted, conveys the entire history of the human race!  We all notice patterns in our telephone numbers, license numbers, etc., as aids to memory. The point of these remarks is that we cannot be trusted to judge by ourselves whether a sequence of numbers is random or not. Some unbiased mechanical tests must be applied.  The theory of statistics provides us with some quantitative measures for randomness. There is literally no end to the number of tests that can be conceived; we will discuss the tests that have proved to be most useful, most instructive, and most readily adapted to computer calculation.  If a sequence behaves randomly with respect to tests T1, T2, . . . , Tn, we cannot be sure in general that it will not be a miserable failure when it is subjected to a further test Tn+1. Yet each test gives us more and more confidence in the randomness of the sequence. In practice, we apply about half a dozen different kinds of statistical tests to a sequence, and if it passes them satisfactorily we consider it to be random — it is then presumed innocent until proven guilty. Every sequence that is to be used extensively should be tested carefully, so the following sections explain how to administer the tests in an appropriate way. Two kinds of tests are distinguished: empirical tests, for which the computer manipulates groups of numbers of the sequence and evaluates certain statistics; and theoretical tests, for which we establish characteristics of the sequence by   42  RANDOM NUMBERS  3.3  using number-theoretic methods based on the recurrence rule used to form the sequence.  If the evidence doesn’t come out as desired, the reader may wish to try the  techniques in How to Lie With Statistics by Darrell Huff  Norton, 1954 .  3.3.1. General Test Procedures for Studying Random Data A. “Chi-square” tests. The chi-square test  χ2 test  is perhaps the best known of all statistical tests, and it is a basic method that is used in connection with many other tests. Before considering the idea in general, let us consider a particular example of the chi-square test as it might be applied to dice throwing. Using two “true” dice  each of which, independently, is assumed to yield the values 1, 2, 3, 4, 5, or 6 with equal probability , the following table gives the probability of obtaining a given total, s, on a single throw: 9 1 9  value of s = 2 probability, ps = 1 36  12 1 36  11 1 18  10 1 12  8 5 36  4 1 12  3 1 18  6 5 36   1   5 1 9  7 1 6  36 = 1  For example, a value of 4 can be thrown in three ways: 1 + 3, 2 + 2, 3 + 1; this constitutes 3  12 = p4 of the 36 possible outcomes.  If we throw the dice n times, we should obtain the value s approximately nps times on the average. For example, in 144 throws we should get the value 4 about 12 times. The following table shows what results were actually obtained in a particular sequence of 144 throws of the dice: 7  4  6  5  8  value of s = 2 observed number, Ys = 2 expected number, nps = 4  3 4 10 12 22 29 21 15 14 8 12 16 20 24 20 16 12  9 10 11 12 6 4  9 8   2   Notice that the observed number was different from the expected number in all cases; in fact, random throws of the dice will hardly ever come out with exactly the right frequencies. There are 36144 possible sequences of 144 throws, all of which are equally likely. One of these sequences consists of all 2s  “snake eyes” , and anyone throwing 144 snake eyes in a row would be convinced that the dice were loaded. Yet the sequence of all 2s is just as probable as any other particular sequence if we specify the outcome of each throw of each die.  In view of this, how can we test whether or not a given pair of dice is loaded? The answer is that we can’t make a definite yes-no statement, but we can give a probabilistic answer. We can say how probable or improbable certain types of events are.  A fairly natural way to proceed in the example above is to consider the squares of the differences between the observed numbers Ys and the expected numbers nps. We can add these together, obtaining  V =  Y2 − np2 2 +  Y3 − np3 2 + ··· +  Y12 − np12 2.   3  A bad set of dice should result in a relatively high value of V; and for any given value of V we can ask, “What is the probability that V is this high, using true   3.3.1  GENERAL TEST PROCEDURES  43  dice?” If this probability is very small, say 1 100, we would know that only about one time in 100 would true dice give results so far away from the expected num- bers, and we would have definite grounds for suspicion.  Remember, however, that even good dice would give such a high value of V about one time in a hundred, so a cautious person would repeat the experiment to see if the high value of V is repeated.  The statistic V in  3  gives equal weight to  Y7 − np7 2 and  Y2 − np2 2, although  Y7 − np7 2 is likely to be a good deal higher than  Y2 − np2 2 since 7s occur about six times as often as 2s. It turns out that the “right” statistic, at least one that has proved to be most important, will give  Y7 − np7 2 only 1 6 as much weight as  Y2 − np2 2, and we should change  3  to the following formula:  V =  Y2 − np2 2  +  Y3 − np3 2  + ··· +  Y12 − np12 2  .  np2  np3  np12   4   8  4  V =  2 − 4 2  This is called the “chi-square” statistic of the observed quantities Y2, . . . , Y12 in the dice-throwing experiment. For the data in  2 , we find that +  6 − 4 2 + ··· +  9 − 8 2 The important question now is, of course, “Does 7 7 48 constitute an improbably high value for V to assume?” Before answering this question, let us consider the general application of the chi-square method.  +  4 − 8 2  = 7 7 48 .  In general, suppose that every observation can fall into one of k categories. We take n independent observations; this means that the outcome of one obser- vation has absolutely no effect on the outcome of any of the others. Let ps be the probability that each observation falls into category s, and let Ys be the number of observations that actually do fall into category s. We form the statistic   5   8  4  In our example above, there are eleven possible outcomes of each throw of the  dice, so k = 11. Eq.  6  is a slight change of notation from Eq.  4 , since we are numbering the possibilities from 1 to k instead of from 2 to 12.  s=1  s in  6 , and using the facts  By expanding  Ys − nps 2 = Y 2  that  k  V =   Ys − nps 2  .  nps  s − 2npsYs + n2p2 Y1 + Y2 + ··· + Yk = n, p1 + p2 + ··· + pk = 1,   Y 2    s ps  k  s=1  − n,  V = 1  n   6    7    8   we arrive at the formula  which often makes the computation of V somewhat easier.   44  RANDOM NUMBERS  3.3.1  SELECTED PERCENTAGE POINTS OF THE CHI-SQUARE DISTRIBUTION  Table 1  p = 1% p = 5% p = 25% p = 50% p = 75% p = 95% p = 99% 0.00016 6.635 9.210 0.02010 11.34 0.1148 0.2971 13.28 15.09 0.5543 16.81 0.8721 1.239 18.48 20.09 1.646 21.67 2.088 2.558 23.21 24.72 3.053 26.22 3.571 5.229 30.58 37.57 8.260 50.89 14.95 29.71 76.15  0.00393 0.1026 0.3518 0.7107 1.1455 1.635 2.167 2.733 3.325 3.940 4.575 5.226 7.261 10.85 18.49 34.76  3.841 5.991 7.815 9.488 11.07 12.59 14.07 15.51 16.92 18.31 19.68 21.03 25.00 31.41 43.77 67.50  1.323 2.773 4.108 5.385 6.626 7.841 9.037 10.22 11.39 12.55 13.70 14.85 18.25 23.83 34.80 56.33 √ 3 + O  1  ν   0.674  0.4549 1.386 2.366 3.357 4.351 5.348 6.346 7.344 8.343 9.342 10.34 11.34 14.34 19.34 29.34 49.33 p − 2 3 x2 0.00  0.1015 0.5754 1.213 1.923 2.675 3.455 4.255 5.071 5.899 6.737 7.584 8.438 11.04 15.45 24.48 42.94 √  2νxp + 2  −2.33  −1.64  −.674  1.64  2.33  ν +  ν = 1 ν = 2 ν = 3 ν = 4 ν = 5 ν = 6 ν = 7 ν = 8 ν = 9 ν = 10 ν = 11 ν = 12 ν = 15 ν = 20 ν = 30 ν = 50 ν > 30 xp =   For further values, see Handbook of Mathematical Functions, edited by M. Abramowitz and I. A. Stegun  Washington, D.C.: U.S. Government Printing Office, 1964 , Table 26.8. See also Eq.  22  and exercise 16.   Now we turn to the important question, “What constitutes a reasonable value of V ?” This is found by referring to a table such as Table 1, which gives val- ues of “the chi-square distribution with ν degrees of freedom” for various values of ν. The line of the table with ν = k−1 is to be used; the number of “degrees of  freedom” is k−1, one less than the number of categories. Intuitively, this means present. This argument is not rigorous, but the theory below justifies it.  that Y1, Y2, . . . , Yk are not completely independent, since Eq.  7  shows that Yk can be computed if Y1, . . . , Yk−1 are known; hence, k − 1 degrees of freedom are  If the table entry in row ν under column p is x, it means, “The quantity V in Eq.  8  will be less than or equal to x with approximate probability p, if n is large enough.” For example, the 95 percent entry in row 10 is 18.31; we will have V > 18.31 only about 5 percent of the time.   3.3.1  GENERAL TEST PROCEDURES  45  Let us assume that our dice-throwing experiment has been simulated on a computer using some sequence of supposedly random numbers, with the following results:  value of s = 2  9 10 11 12 Experiment 1, Ys = 4 10 10 13 20 18 18 11 13 14 13 5 Experiment 2, Ys = 3  7 11 15 19 24 21 17 13  3  4  5  6  7  8  9   9   120, and in the second case we get V2 = 1 17  We can compute the chi-square statistic in the first case, getting the value V1 = 29 59 120. Referring to the table entries for 10 degrees of freedom, we see that V1 is much too high; V will be greater than 23.21 only about one percent of the time!  By using more extensive tables, we find in fact that V will be as high as V1 only 0.1 percent of the time.  Therefore Experiment 1 represents a significant departure from random behavior.  On the other hand, V2 is quite low, since the observed values Ys in Exper- iment 2 are quite close to the expected values nps in  2 . The chi-square table tells us, in fact, that V2 is much too low: The observed values are so close to the expected values, we cannot consider the result to be random!  Indeed, reference to other tables shows that such a low value of V occurs only 0.03 percent of the time when there are 10 degrees of freedom.  Finally, the value V = 7 7 48 computed in  5  can also be checked with Table 1. It falls between the entries for 25 percent and 50 percent, so we cannot consider it to be significantly high or significantly low; thus the observations in  2  are satisfactorily random with respect to this test.  It is somewhat remarkable that the same table entries are used no matter what the value of n is, and no matter what the probabilities ps are. Only the number ν = k − 1 affects the results. In actual fact, however, the table entries are not exactly correct: The chi-square distribution is an approximation that is valid only for large enough values of n. How large should n be? A common rule of thumb is to take n large enough so that each of the expected values nps is five or more; preferably, however, take n much larger than this, to get a more powerful test. In our examples above we took n = 144, so np2 was only 4, violating the stated rule of thumb. This was done only because the author tired of throwing the dice; it makes the entries in Table 1 less accurate for our application. Experiments run on a computer, with n = 1000, or 10000, or even 100000, would be much better than this. We could also combine the data for s = 2 and s = 12; then the test would have only nine degrees of freedom but the chi-square approximation would be more accurate.  4 and p2 = 3  We can get an idea of how crude an approximation is involved by considering the case when there are only two categories, having probabilities p1 and p2. Suppose p1 = 1 4. According to the stated rule of thumb, we should have n ≥ 20 to have a satisfactory approximation, so let’s check that out. When n = 20, the possible values of V are  Y1 − 5 2 5 +  5 − Y1 2 15 = 4 15 r2 for −5 ≤ r ≤ 15; we wish to know how well the row ν = 1 of Table 1 describes the distribution of V. The chi-square distribution varies continuously, while the actual distribution of V has rather big jumps, so we need some convention for   46  RANDOM NUMBERS  3.3.1  If the distinct possible outcomes of the representing the exact distribution. experiment lead to the values V0 ≤ V1 ≤ ··· ≤ Vn with respective proba- bilities π0, π1, . . . , πn, suppose that a given percentage p falls in the range π0 + ··· + πj−1 < p < π0 + ··· + πj−1 + πj. We would like to represent p by a “percentage point” x such that V is less than x with probability ≤ p and V is greater than x with probability ≤ 1−p. It is not difficult to see that the only such number is x = Vj. In our example for n = 20 and ν = 1, it turns out that the percentage points of the exact distribution, corresponding to the approximations in Table 1 for p = 1%, 5%, 25%, 50%, 75%, 95%, and 99%, respectively, are  0,  0,  .27,  .27,  1.07,  4.27,  6.67   to two decimal places . For example, the percentage point for p = 95% is 4.27, while Table 1 gives the estimate 3.841. The latter value is too low; it tells us  incorrectly  to reject the value V = 4.27 at the 95% level, while in fact the probability that V ≥ 4.27 is more than 6.5%. When n = 21, the situation changes slightly because the expected values np1 = 5.25 and np2 = 15.75 can never be obtained exactly; the percentage points for n = 21 are 5.73.  1.29,  3.57,  .02,  .02,  .40,  .14,  We would expect Table 1 to be a better approximation when n = 50, but the corresponding tableau actually turns out to be further from Table 1 in some respects than it was for n = 20:  .03,  .03, Here are the values when n = 300: .07,  .03,  0,  0,  .67,  1.31,  3.23,  6.  .44,  1.44,  4,  6.42.  Even in this case, when nps is ≥ 75 in each category, the entries in Table 1 are good to only about one significant digit. The proper choice of n is somewhat obscure. If the dice are actually biased, the fact will be detected as n gets larger and larger.  See exercise 12.  But large values of n will tend to smooth out locally nonrandom behavior, when blocks of numbers with a strong bias are followed by blocks of numbers with the opposite bias. Locally nonrandom behavior is not an issue when actual dice are rolled, since the same dice are used throughout the test, but a sequence of numbers generated by computer might very well display such anomalies. Perhaps a chi- square test should be made for several different values of n. At any rate, n should always be rather large.  We can summarize the chi-square test as follows. A fairly large number, n, of independent observations is made.  It is important to avoid using the chi-square method unless the observations are independent. See, for example, exercise 10, which considers the case when half of the observations depend on the other half.  We count the number of observations falling into each of k categories and compute the quantity V given in Eqs.  6  and  8 . Then V is compared with the numbers in Table 1, with ν = k − 1. If V is less than the 1% entry or greater than the 99% entry, we reject the numbers as not sufficiently random. If V lies   3.3.1  GENERAL TEST PROCEDURES  47  Fig. 2. Indications of “significant” deviations in 90 chi-square tests  see also Fig. 5 .  between the 1% and 5% entries or between the 95% and 99% entries, the numbers are “suspect”; if  by interpolation in the table  V lies between the 5% and 10% entries, or the 90% and 95% entries, the numbers might be “almost suspect.” The chi-square test is often done at least three times on different sets of data, and if at least two of the three results are suspect the numbers are regarded as not sufficiently random.  For example, see Fig. 2, which shows schematically the results of apply- ing five different types of chi-square tests on each of six sequences of random numbers. Each test in this illustration was applied to three different blocks of numbers of the sequence. Generator A is the MacLaren–Marsaglia method  Algorithm 3.2.2M applied to the sequences in 3.2.2– 13  ; Generator E is the Fibonacci method, 3.2.2– 5 ; and the other generators are linear congruential sequences with the following parameters: a = 3141592653, Generator B: X0 = 0, Generator C: X0 = 0, a = 27 + 1, Generator D: X0 = 47594118, Generator F: X0 = 314159265, From Fig. 2 we conclude that  so far as these tests are concerned  Generators A, B, D are satisfactory, Generator C is on the borderline and should probably be rejected, Generators E and F are definitely unsatisfactory. Generator F has, of course, low potency; Generators C and D have been discussed in the literature, but their multipliers are too small.  Generator D is the original multiplicative generator proposed by Lehmer in 1948; Generator C is the original linear congruential generator with c ̸= 0 proposed by Rotenberg in 1960.   c = 2718281829, m = 235.  a = 23, a = 218 + 1,  c = 0, m = 108 + 1.  c = 1, m = 235.  c = 1, m = 235.  Instead of using the “suspect,” “almost suspect,” etc., criteria for judging the results of chi-square tests, one can employ a less ad hoc procedure discussed later in this section.  A  B  C  D  E  F  Range of V  Indication  Code  0–1 percent, 99–100 percent  1–5 percent, 95–99 percent  Reject  Suspect  5–10 percent, 90–95 percent  Almost suspect   48  RANDOM NUMBERS  3.3.1   a    b    c   Fig. 3. Examples of distribution functions.  B. The Kolmogorov–Smirnov test. As we have seen, the chi-square test applies to the situation when observations can fall into a finite number of cate- gories. It is not unusual, however, to consider random quantities that range over infinitely many values, such as a random fraction  a random real number between 0 and 1 . Even though only finitely many real numbers can be represented in a computer, we want our random values to behave essentially as if all real numbers in [0 . . 1  were equally likely.  A general notation for specifying probability distributions, whether they are finite or infinite, is commonly used in the study of probability and statistics. Suppose we want to specify the distribution of the values of a random quantity, X; we do this in terms of the distribution function F x , where  F x  = Pr X ≤ x  = probability that  X ≤ x .  Three examples are shown in Fig. 3. First we see the distribution function for a random bit, namely for the case when X takes on only the two values 0 and 1, each with probability 1 2. Part  b  of the figure shows the distribution function for a uniformly distributed random real number between zero and one; here the probability that X ≤ x is simply equal to x when 0 ≤ x ≤ 1. For example, the probability that X ≤ 2 3. And part  c  shows the limiting distribution of the value V in the chi-square test  shown here with 10 degrees of freedom ; this is a distribution that we have already seen represented in another way in Table 1. Notice that F x  always increases from 0 to 1 as x increases from −∞ to +∞.  3 is, naturally, 2  y = 1  y = 1 2  x = 0  x = 1 2  x = 1  y = 1  y = 1 2  x = 0  x = 1 2  x = 1  y = 1 y = 3 4 y = 1 2 y = 1 4  x = 3.9  x = 9.3  x = 18.3  x = 6.7  x = 12.6   3.3.1  GENERAL TEST PROCEDURES  49  If we make n independent observations of the random quantity X, thereby obtaining the values X1, X2, . . . , Xn, we can form the empirical distribution function Fn x , where  number of X1, X2, . . . , Xn that are ≤ x  Fn x  =  Figure 4 illustrates three empirical distribution functionsshown as zigzag lines, although strictly speaking the vertical lines are not part of the graph of Fn x ,  superimposed on a graph of the assumed actual distribution function F x . As n gets large, Fn x  should be a better and better approximation to F x .   10   n  .   a    b    c   Fig. 4. Examples of empirical distributions. The x value marked “5%” is the percentage point where F  x  = 0.05.  The Kolmogorov–Smirnov test  KS test  may be used when F x  has no jumps. It is based on the difference between F x  and Fn x . A bad source of random numbers will give empirical distribution functions that do not approxi- mate F x  sufficiently well. Figure 4 b  shows an example in which the Xi are consistently too high, so the empirical distribution function is too low. Part  c  of the figure shows an even worse example; it is plain that such great deviations between Fn x  and F x  are extremely improbable, and the KS test is used to tell us how improbable they are.  5% 25%50% 75% 95% 99%  5% 25%50% 75% 95% 99%  5% 25%50% 75% 95% 99%   50  RANDOM NUMBERS  To make the KS test, we form the following statistics:  √ √  n  n  K +  n = n =  K−  sup  sup  −∞<x<+∞  −∞<x<+∞  Fn x  − F x ; F x  − Fn x .  3.3.1   11   n measures the greatest amount of deviation when Fn is greater than F, n measures the maximum deviation when Fn is less than F. The statistics  Here K + and K− for the examples of Fig. 4 are  Fig. 4 a   Fig. 4 b   Fig. 4 c    12   0.313 2.101  0.134 1.027  0.492 0.536  √  n, K−  n and K−  n and K−  n; hence the factor  n magnifies the statistics K +  K + 20 K− 20  Note: The factor n that appears in Eqs.  11  may seem puzzling at first. Exercise 6 shows that, for fixed x, the standard deviation of Fn x  is proportional √ √ to 1  n in such a way that this standard deviation is independent of n.  n in a percentile table to determine if they are significantly high or low. Table 2 may be used for this purpose, both for K + n . For example, the probability is 75 percent that K− 20 will be 0.7975 or less. Unlike the chi-square test, the table entries are not merely approximations that hold for large values of n; Table 2 gives exact values  except, of course, for roundoff error , and the KS test may be used reliably for any value of n.  As in the chi-square test, we may now look up the values K +  As they stand, formulas  11  are not readily adapted to computer calcula- tion, since we are asking for a least upper bound over infinitely many values of x. But from the fact that F x  is increasing and the fact that Fn x  increases only in finite steps, we can derive a simple procedure for evaluating the statistics K + and K− n : Step 1. Obtain independent observations X1, X2, . . . , Xn . Step 2. Rearrange the observations so that they are sorted into ascending order, X1 ≤ X2 ≤ ··· ≤ Xn.  Efficient sorting algorithms are the subject of Chapter 5. But it is possible to avoid sorting in this case, as shown in exercise 23.  Step 3. The desired statistics are now given by the formulas  n  K +  n =  K−  n =  √  √  n max 1≤j≤n n max 1≤j≤n   j   − F Xj  ; F Xj  − j − 1  n    .  n   13   An appropriate choice of the number of observations, n, is slightly easier to make for this test than it is for the χ2 test, although some of the considerations are similar. If the random variables Xj actually belong to the probability distribution G x , while they were assumed to belong to the distribution given by F x , we want n to be comparatively large, in order to reject the hypothesis that G x  = F x ; for we need n large enough that the empirical distributions   3.3.1  GENERAL TEST PROCEDURES  51  SELECTED PERCENTAGE POINTS OF THE DISTRIBUTIONS K+  n AND K  − n  Table 2  p = 1% p = 5% p = 25% p = 50% p = 75% p = 95% p = 99% 0.01000 0.9900 1.2728 0.01400 1.3589 0.01699 0.01943 1.3777 1.4024 0.02152 1.4144 0.02336 0.02501 1.4246 1.4327 0.02650 1.4388 0.02786 0.02912 1.4440 1.4484 0.03028 1.4521 0.03137 0.03424 1.4606 1.4698 0.03807 0.04354 1.4801  0.9500 1.0980 1.1017 1.1304 1.1392 1.1463 1.1537 1.1586 1.1624 1.1658 1.1688 1.1714 1.1773 1.1839 1.1916 2 ln 1  1 − p   1.2239  0.05000 0.06749 0.07919 0.08789 0.09471 0.1002 0.1048 0.1086 0.1119 0.1147 0.1172 0.1193 0.1244 0.1298 0.1351 yp − 1 0.1601  0.7500 0.7071 0.7539 0.7642 0.7674 0.7703 0.7755 0.7797 0.7825 0.7845 0.7863 0.7880 0.7926 0.7975 0.8036 p = 1 0.8326  0.2500 0.2929 0.3112 0.3202 0.3249 0.3272 0.3280 0.3280 0.3274 0.3297 0.3330 0.3357 0.3412 0.3461 0.3509  0.5000 0.5176 0.5147 0.5110 0.5245 0.5319 0.5364 0.5392 0.5411 0.5426 0.5439 0.5453 0.5500 0.5547 0.5605  6 n−1 2 + O 1 n , where y2  0.07089  0.5887  0.3793  1.5174  n = 1 n = 2 n = 3 n = 4 n = 5 n = 6 n = 7 n = 8 n = 9 n = 10 n = 11 n = 12 n = 15 n = 20 n = 30 n > 30 yp =   To extend this table, see Eqs.  25  and  26 , and the answer to exercise 20.   Gn x  and Fn x  are expected to be observably different. On the other hand, large values of n will tend to average out locally nonrandom behavior, and such undesirable behavior is a significant danger in most computer applications of random numbers; this makes a case for smaller values of n. A good compromise would be to take n equal to, say, 1000, and to make a fairly large number of calculations of K + 1000 on different parts of a random sequence, thereby obtaining values  1000 2 ,  1000 1 ,   14  We can also apply the KS test again to these results: Let F x  now be the distribution function for K + 1000, and determine the empirical distribution Fr x  obtained from the observed values in  14 . Fortunately, the function F x  in this case is very simple; for a large value of n like n = 1000, the distribution of K + is closely approximated by  1000 r .  K +  K +  K+  . . . ,  n  F∞ x  = 1 − e−2x2  ,  x ≥ 0.   15    52  RANDOM NUMBERS  3.3.1  The same remarks apply to K− n have the same expected behavior. This method of using several tests for moderately large n, then combining the observations later in another KS test, will tend to detect both local and global nonrandom behavior.  n , since K +  n and K−  For example, the author conducted the following simple experiment while writing this chapter: The “maximum-of-5” test described in the next section was applied to a set of 1000 uniform random numbers, yielding 200 observations X1, X2, . . . , X200 that were supposed to belong to the distribution F x  = x5 for 0 ≤ x ≤ 1. The observations were divided into 20 groups of 10 each, and the 10 thus obtained statistic K + led to the empirical distributions shown in Fig. 4. The smooth curve shown in each of the diagrams in Fig. 4 is the actual distribution the statistic K + 10 should have. Figure 4 a  shows the empirical distribution of K + 10 obtained from the sequence  10 was computed for each group. The 20 values of K+  Yn+1 =  3141592653Yn + 2718281829  mod 235,  Un = Yn 235,  and it is satisfactorily random. Part  b  of the figure came from the Fibonacci method; this sequence has globally nonrandom behavior — that is, it can be shown that the observations Xn in the maximum-of-5 test do not have the correct distribution F x  = x5. Part  c  came from the notorious and impotent linear congruential sequence Yn+1 =   218 + 1 Yn + 1  mod 235, Un = Yn 235.  The KS test applied to the data in Fig. 4 gives the results shown in  12 . Referring to Table 2 for n = 20, we see that the values of K + 20 for Fig. 4 b  are almost suspect  they lie at about the 5 percent and 88 percent levels , but they are not quite bad enough to be rejected outright. The value of 20 for Fig. 4 c  is, of course, completely out of line, so the maximum-of-5 test K− shows a definite failure of that random number generator.  20 and K−  We would expect the KS test in this experiment to have more difficulty locating global nonrandomness than local nonrandomness, since the basic obser- vations in Fig. 4 were made on samples of only 10 numbers each. If we were to take 20 groups of 1000 numbers each, part  b  would show a much more significant deviation. To illustrate this point, a single KS test was applied to all 200 of the observations that led to Fig. 4, and the following results were obtained:  Fig. 4 a   Fig. 4 b   Fig. 4 c   K + 200 K− 200  0.477 0.817  1.537 0.194  2.819 0.058   16   The global nonrandomness of the Fibonacci generator has definitely been de- tected here.  We may summarize the Kolmogorov–Smirnov test as follows. We are given n independent observations X1, . . . , Xn taken from some distribution specified by a continuous function F x . That is, F x  must be like the functions shown in Fig. 3 b  and 3 c , having no jumps like those in Fig. 3 a . The procedure explained just before Eqs.  13  is carried out on these observations, and we obtain   GENERAL TEST PROCEDURES  53  3.3.1  the statistics K+ Table 2.  n and K−  n . These statistics should be distributed according to  Some comparisons between the KS test and the χ2 test can now be made. In the first place, we should observe that the KS test may be used in conjunction with the χ2 test, to give a better procedure than the ad hoc method we mentioned when summarizing the χ2 test.  That is, there is a better way to proceed than to make three tests and to consider how many of the results were “suspect.”  Suppose we have made, say, 10 independent χ2 tests on different parts of a random sequence, so that values V1, V2, . . . , V10 have been obtained. It is not a good policy simply to count how many of the V ’s are suspiciously large or small. This procedure will work in extreme cases, and very large or very small values may mean that the sequence has too much local nonrandomness; but a better general method would be to plot the empirical distribution of these 10 values and to compare it to the correct distribution, which may be obtained from Table 1. The empirical distribution gives a clearer picture of the results of the χ2 tests, and in fact the statistics K + 10 could be determined from the empirical χ2 values as an indication of success or failure. With only 10 values or even as many as 100 this could all be done easily by hand, using graphical methods; with a larger number of V ’s, a computer subroutine for calculating the chi-square distribution would be necessary. Notice that all 20 of the observations in Fig. 4 c  fall between the 5 and 95 percent levels, so we would not have regarded any of them as suspicious, individually; yet collectively the empirical distribution shows that these observations are not at all right.  10 and K−  An important difference between the KS test and the chi-square test is that the KS test applies to distributions F x  having no jumps, while the chi-square test applies to distributions having nothing but jumps  since all observations are divided into k categories . The two tests are thus intended for different sorts of applications. Yet it is possible to apply the χ2 test even when F x  is continuous, if we divide the domain of F x  into k parts and ignore all variations within each part. For example, if we want to test whether or not U1, U2, . . . , Un can be considered to come from the uniform distribution between zero and one, we want to test if they have the distribution F x  = x for 0 ≤ x ≤ 1. This is a natural application for the KS test. But we might also divide up the interval from 0 to 1 into k = 100 equal parts, count how many U’s fall into each part, and apply the chi-square test with 99 degrees of freedom. There are not many theoretical results available at the present time to compare the effectiveness of the KS test versus the chi-square test. The author has found some examples in which the KS test pointed out nonrandomness more clearly than the χ2 test, and others in which the χ2 test gave a more significant result. If, for example, the 100 categories mentioned above are numbered 0, 1, . . . , 99, and if the deviations from the expected values are positive in compartments 0 to 49 but negative in compartments 50 to 99, then the empirical distribution function will be much further from F x  than the χ2 value would indicate; but if the positive deviations occur in compartments 0, 2, . . . , 98 and the negative ones occur in 1, 3, . . . , 99, the empirical distribution function will tend to hug F x  much more closely. The   54  RANDOM NUMBERS  3.3.1  Range of K+  n  Range of K−n  Fig. 5. The KS tests applied to the same data as Fig. 2.  kinds of deviations measured are therefore somewhat different. A χ2 test was applied to the 200 observations that led to Fig. 4, with k = 10, and the respective values of V were 9.4, 17.7, and 39.3; so in this particular case the values were quite comparable to the KS values given in  16 . Since the χ2 test is intrinsically less accurate, and since it requires comparatively large values of n, the KS test has several advantages when a continuous distribution is to be tested.  200 and K−  A further example will also be of interest. The data that led to Fig. 2 were chi-square statistics based on n = 200 observations of the maximum-of-t criterion for 1 ≤ t ≤ 5, with the range divided into 10 equally probable parts. 200 can be computed from exactly the same sets of 200 KS statistics K + observations, and the results can be tabulated in just the same way as we did in Fig. 2  showing which KS values are beyond the 99-percent level, etc. ; the results in this case are shown in Fig. 5. Notice that Generator D  Lehmer’s original method  shows up very badly in Fig. 5, while chi-square tests on the very same data revealed no difficulty in Fig. 2; contrariwise, Generator E  the Fibonacci method  does not look so bad in Fig. 5. The good generators, A and B, passed all tests satisfactorily. The reasons for the discrepancies between Fig. 2 and Fig. 5 are primarily that  a  the number of observations, 200, is really not large enough for a powerful test, and  b  the “reject,” “suspect,” “almost suspect” ranking criterion is itself suspect.   Incidentally,  it is not fair to blame Lehmer for using a “bad” random number generator in the 1940s, since his actual use of Generator D was quite valid. The ENIAC computer was a highly parallel machine, programmed by means of a plugboard; Lehmer set it up so that one of its accumulators was repeatedly multiplying its own contents by 23  modulo 108 + 1 , yielding a new value every few milliseconds. Since this multiplier 23 is too small, we  A  B  C  D  E  F  A  B  C  D  E  F   3.3.1  GENERAL TEST PROCEDURES  55  know that each value obtained by such a process is too strongly related to the preceding value to be considered sufficiently random; but the durations of time between actual uses of the values in the special accumulator by the accompanying program were comparatively long and subject to some fluctuation. So the effective multiplier was 23k for large, varying values of k.  C. History, bibliography, and theory. The chi-square test was introduced by Karl Pearson in 1900 [Philosophical Magazine, Series 5, 50, 157–175]. Pearson’s important paper is regarded as one of the foundations of modern statistics, since before that time people would simply plot experimental results graphically and assert that they were correct. In his paper, Pearson gave several interesting examples of the previous misuse of statistics; and he also proved that certain runs at roulette  which he had experienced during two weeks at Monte Carlo in 1892  were so far from the expected frequencies that odds against the assumption of an honest wheel were some 1029 to one! A general discussion of the chi-square test and an extensive bibliography appear in the survey article by William G. Cochran, Annals Math. Stat. 23  1952 , 315–345.  Let us now consider a brief derivation of the theory behind the chi-square  test. The exact probability that Y1 = y1, . . . , Yk = yk is easily seen to be   17   If we assume that Ys has the value ys with the Poisson probability  and that the Y ’s are independent, then  Y1, . . . , Yk  will equal  y1, . . . , yk  with probability  and Y1 + ··· + Yk will equal n with probability e−nps nps ys    k  = e−nnn n!  .  ys!  y1+···+yk=n y1,...,yk≥0  s=1  If we assume that they are independent except for the condition Y1+···+Yk = n, the probability that  Y1, . . . , Yk  =  y1, . . . , yk  is the quotient   k  s=1   e−nnn    ,  n!  e−nps nps ys  ys!  which equals  17 . We may therefore regard the Y ’s as independently Poisson distributed, except for the fact that they have a fixed sum.  n!  y1! . . . yk! py1  1 . . . pyk k .  e−nps nps ys  ,  ys!  k  s=1  e−nps nps ys  ,  ys!   56  RANDOM NUMBERS  3.3.1  It is convenient to make a change of variables,  Zs = Ys − nps  √  ,  nps  so that V = Z2 requiring that   18  1 + ··· + Z2 k. The condition Y1 + ··· + Yk = n is equivalent to p1 Z1 + ··· + √ √  19  Let us consider the  k − 1 -dimensional space S of all vectors  Z1, . . . , Zk  such that  19  holds. For large values of n, each Zs has approximately the normal distribution  see exercise 1.2.10–15 ; therefore points in a differential volume dz2 . . . dzk of S occur with probability approximately proportional to exp  − z2 k  2 .  It is at this point in the derivation that the chi-square method becomes only an approximation for large n.  The probability that V ≤ v  pk Zk = 0.   z1,...,zk  in S and z2  1+···+z2  k≤v exp  − z2  1 + ··· + z2  k  2  dz2 . . . dzk   z1,...,zk  in S exp  − z2  1 + ··· + z2  k  2  dz2 . . . dzk  .   20   Since the hyperplane  19  passes through the origin of k-dimensional space, the numerator in  20  is an integration over the interior of a  k − 1 -dimensional hypersphere centered at the origin. An appropriate transformation to generalized polar coordinates with radius χ and angles ω1, . . . , ωk−2 transforms  20  into  is now   1 + ··· + z2     χ2≤v e−χ2 2χk−2f ω1, . . . , ωk−2  dχ dω1 . . . dωk−2   e−χ2 2χk−2f ω1, . . . , ωk−2  dχ dω1 . . . dωk−2  for some function f  see exercise 15 ; then integration over the angles ω1, . . . , ωk−2 gives a constant factor that cancels from numerator and denominator. We finally obtain the formula   21   for the approximate probability that V ≤ v.  Our derivation of  21  uses the symbol χ to stand for the radial length, just as Pearson did in his original paper; this is how the χ2 test got its name. Substituting t = χ2 2, the integrals can be expressed in terms of the incomplete gamma function, which we discussed in Section 1.2.11.3:   k − 1     k − 1    n→∞ Pr V ≤ v  = γ lim   22  This is the definition of the chi-square distribution with k−1 degrees of freedom. We now turn to the KS test. In 1933, A. N. Kolmogorov proposed a test  v 2  2  2  Γ  ,  .  based on the statistic √  Kn =  n max  −∞<x<+∞  Fn x  − F x  = max K +  n  . n , K−   23    √  ∞  v  e−χ2 2χk−2 dχ 0 0 e−χ2 2χk−2 dχ   3.3.1  GENERAL TEST PROCEDURES  57  n and K−  N. V. Smirnov discussed several modifications of this test in 1939, including the n as we have suggested above. There is individual examination of K + a large family of similar tests, but the K + n statistics seem to be most convenient for computer application. A comprehensive review of the literature concerning KS tests and their generalizations, including an extensive bibliogra- phy, appears in a monograph by J. Durbin, Regional Conf. Series on Applied Math. 9  SIAM, 1973 .  n and K−  n and K−  To study the distribution of K +  n , we begin with the following basic fact: If X is a random variable with the continuous distribution F x , then F X  is a uniformly distributed real number between 0 and 1. To prove this, we need only verify that if 0 ≤ y ≤ 1 we have F X  ≤ y with probability y. Since F is continuous, F x0  = y for some x0; thus the probability that F X  ≤ y is the probability that X ≤ x0. By definition, the latter probability is F x0 , that is, it is y. Let Yj = nF Xj , for 1 ≤ j ≤ n, where the X’s have been sorted as in Step 2 preceding Eq.  13 . Then the variables Yj are essentially the same as independent, uniformly distributed random numbers between 0 and n that have been sorted into nondecreasing order, Y1 ≤ Y2 ≤ ··· ≤ Yn; and the first equation of  13  may be transformed into  n = 1√ K+  n  max 1 − Y1, 2 − Y2, . . . , n − Yn .  If 0 ≤ t ≤ n, the probability that K + n is therefore the probability that Yj ≥ j − t for 1 ≤ j ≤ n. This is not hard to express in terms of n-dimensional integrals,  √ n ≤ t    n  n  dyn αn 0 dyn   yn  yn  0  αn−1  dyn−1 . . . y2 dyn−1 . . . y2  dy1 α1 0 dy1  ,  where  αj = max j − t, 0 .   24   The denominator here is immediately evaluated: It is found to be nn n!, which makes sense since the hypercube of all vectors  y1, y2, . . . , yn  with 0 ≤ yj < n has volume nn, and it can be divided into n! equal parts corresponding to each possible ordering of the y’s. The integral in the numerator is a little more difficult, but it yields to the attack suggested in exercise 17, and we get the general formulas      0≤k≤t   n   k    n    t<k≤n  k  = 1 − t nn  Pr  n ≤ t√ K+ n  = t nn   k − t k t + n − k n−k−1   k − t k t + n − k n−k−1.   25    26     The distribution of K− n is exactly the same. Equation  26  was first obtained by N. V. Smirnov [Uspekhi Mat. Nauk 10  1944 , 176–206]; see also Z. W. Birnbaum and Fred H. Tingey, Annals Math. Stat. 22  1951 , 592–596. Smirnov   58  RANDOM NUMBERS  3.3.1  derived the asymptotic formula  n ≤ s  = 1 − e−2s21 − 2  √ 3 s   n + O 1 n   Pr K+   27  for all fixed s ≥ 0; this yields the approximations for large n that appear in Table 2.  Abel’s binomial theorem, Eq. 1.2.6– 16 , shows the equivalence of  25  and  26 . We can extend Table 2 using either formula, but there is an interesting √ n is tradeoff: Although the sum in  25  has only about s given, it must be evaluated with multiple-precision arithmetic, because the terms are large and their leading digits cancel out. No such problem arises in  26 , since √ its terms are all positive; but  26  has n − s  √ n terms, when s = t   n terms.  48 of Eq.  5  is improbably high?  EXERCISES 1. [00] What line of the chi-square table should be used to check whether or not the value V = 7 7 2. [20] If two dice are “loaded” so that, on one die, the value 1 will turn up exactly twice as often as any of the other values, and the other die is similarly biased towards 6, compute the probability ps that a total of exactly s will appear on the two dice, for 2 ≤ s ≤ 12.   cid:120  3. [23] Some dice that were loaded as described in the previous exercise were rolled  144 times, and the following values were observed: 6  value of s = 2 observed number, Ys = 2  4  5  3 6 10 16 18 32 20 13 16  9 10 11 12 2  7  9  8  Apply the chi-square test to these values, using the probabilities in  1 , pretending that the dice are not in fact known to be faulty. Does the chi-square test detect the bad dice? If not, explain why not.   cid:120  4. [23] The author actually obtained the data in experiment 1 of  9  by simulating  dice in which one was normal, the other was loaded so that it always turned up 1 or 6.  The latter two possibilities were equally probable.  Compute the probabilities that replace  1  in this case, and by using a chi-square test decide if the results of that experiment are consistent with the dice being loaded in this way. 5. [22] Let F  x  be the uniform distribution, Fig. 3 b . Find K+ following 20 observations: 0.236, 0.141,  − 20 for the  0.098, 0.354,  0.259, 0.318,  0.189, 0.772,  0.162, 0.017,  0.693, 0.678,  0.302, 0.718,  0.732, 0.434,  0.414, 0.442,  0.442, 0.869,  20 and K  and state whether these observations are significantly different from the expected behavior with respect to either of these two tests. 6. [M20] Consider Fn x , as given in Eq.  10 , for fixed x. What is the probability that Fn x  = s n, given an integer s? What is the mean value of Fn x ? What is the standard deviation? 7. [M15] Show that K+ value K+ 8. [00] The text describes an experiment in which 20 values of the statistic K+ 10 were obtained in the study of a random sequence. These values were plotted, to obtain  n can never be negative. What is the largest possible  n can have?  n and K−   3.3.1  GENERAL TEST PROCEDURES  59  40, K  − 10; since K  Fig. 4, and a KS statistic was computed from the resulting graph. Why were the table entries for n = 20 used to study the resulting statistic, instead of the table entries for n = 10?   cid:120  9. [20] The experiment described in the text consisted of plotting 20 values of K+  10, computed from the maximum-of-5 test applied to different parts of a random sequence. − 10 has the We could have computed also the corresponding 20 values of K 10, we could lump together the 40 values thus obtained  that is, same distribution as K+ − 10’s , and a KS test could be applied so that we would 20 of the K+ − get new values K+ 40. Discuss the merits of this idea.   cid:120  10. [20] Suppose a chi-square test is done by making n observations, and the value V  10’s and 20 of the K  is obtained. Now we repeat the test on these same n observations over again  getting, of course, the same results , and we put together the data from both tests, regarding it as a single chi-square test with 2n observations.  This procedure violates the text’s stipulation that all of the observations must be independent of one another.  How is the second value of V related to the first one? 11. [10] Solve exercise 10 substituting the KS test for the chi-square test. 12. [M28] Suppose a chi-square test is made on a set of n observations, assuming that ps is the probability that each observation falls into category s; but suppose that in actual fact the observations have probability qs ̸= ps of falling into category s.  See exercise 3.  We would, of course, like the chi-square test to detect the fact that the ps assumption was incorrect. Show that this will happen, if n is large enough. Prove also the analogous result for the KS test. 13. [M24] Prove that Eqs.  13  are equivalent to Eqs.  11 .   cid:120  14. [HM26] Let Zs be given by Eq.  18 . Show directly by using Stirling’s approxi-  mation that the multinomial probability  k  Y1! . . . Yk! = e−V  2  2nπ k−1p1 . . . pk + O n−k 2 ,  n! pY1  1 . . . pYk  if Z1, Z2, . . . , Zk are bounded as n → ∞.  This idea leads to a proof of the chi-square test that is much closer to “first principles,” and requires less handwaving, than the derivation in the text.  15. [HM24] Polar coordinates in two dimensions are conventionally defined by the equations x = r cos θ and y = r sin θ. For the purposes of integration, we have dx dy = r dr dθ. More generally, in n-dimensional space we can let  xk = r sin θ1 . . . sin θk−1 cos θk,  Show that in this case  1 ≤ k < n,  and  xn = r sin θ1 . . . sin θn−1.   cid:120  16. [HM35] Generalize Theorem 1.2.11.3A to find the value of  dx1 dx2 . . . dxn = rn−1 sinn−2  θ1 . . . sin θn−2 dr dθ1 . . . dθn−1.  γ x + 1, x + z  2x + y  Γ  x + 1 ,  √  for large x and fixed y, z. Disregard terms of the answer that are O 1 x . Use this result to find the approximate solution, t, to the equation   ν  γ  t 2  2 ,     ν    2  Γ  = p,  for large ν and fixed p, thereby accounting for the asymptotic formulas indicated in Table 1. [Hint: See exercise 1.2.11.3–8.]   60  RANDOM NUMBERS  3.3.1  17. [HM26] Let t be a fixed real number. For 0 ≤ k ≤ n, let  Pnk x  =  dxn  dxn−1 . . .  0 by convention, let P00 x  = 1. Prove the following relations:  n−1−t  k+1−t  n−t  dxk+1  dxk . . .  dx1;   xk+2  xk+2   xk+1  xk+1   x2  x2  0   x  x+t   xn  xn  n  dxn  a  Pnk x  = k+1 b  Pn0 x  =  x + t n n! −  x + t n−1  n − 1 !. c  Pnk x  − Pn k−1  x  =  k − t k  dxn−1 . . .  n−1  k!  P n−k 0 x − k , if 1 ≤ k ≤ n.  dxk+1  dxk . . .  t  t  dx1.  n and K− n and K−  d  Obtain a general formula for Pnk x , and apply it to the evaluation of Eq.  24 . 18. [M20] Give a “simple” reason why K− n has the same probability distribution as K+ n . 19. [HM48] Develop tests, analogous to the Kolmogorov–Smirnov test, for use with multivariate distributions F  x1, . . . , xr  = Pr X1 ≤ x1, . . . , Xr ≤ xr .  Such proce- dures could be used, for example, in place of the “serial test” in the next section.  20. [HM41] Deduce further terms of the asymptotic behavior of the KS distribution, extending  27 . 21. [M40] Although the text states that the KS test should be applied only when F  x  is a continuous distribution function, it is, of course, possible to try to compute n even when the distribution has jumps. Analyze the probable behavior of K+ n for various discontinuous distributions F  x . Compare the effectiveness K+ of the resulting statistical test with the chi-square test on several samples of random numbers. 22. [HM46] Investigate the “improved” KS test suggested in the answer to exercise 6. 23. [M22]  T. Gonzalez, S. Sahni, and W. R. Franta.   a  Suppose that the maxi- mum value in formula  13  for the KS statistic K+ n occurs at a given index j where ⌊nF  Xj ⌋ = k. Prove that F  Xj  = max1≤i≤n{F  Xi   ⌊nF  Xi ⌋ = k}.  b  Design  cid:120  24. [40] Experiment with various probability distributions  p, q, r  on three categories, an algorithm that calculates K+ 25. [HM26] Suppose Yi = n  where p + q + r = 1, by computing the exact distribution of the chi-square statistic V for various n, thereby determining how accurate an approximation the chi-square distribution with two degrees of freedom really is.  j=1 aijXj + µi for 1 ≤ i ≤ m, where X1, . . . , Xn are independent random variables with mean zero and unit variance, and the matrix A =  aij  has rank n. a  Express the covariance matrix C =  cij , where cij = E Yi − µi  Yj − µj , in  n in O n  steps  without sorting .  n and K−  terms of the matrix A.  b  Prove that if C =  ¯cij  is any matrix such that CCC = C, the statistic  m  m  i=1  j=1  W =   Yi − µi  Yj − µj ¯cij  is equal to X2 W has the chi-square distribution with n degrees of freedom.]  n. [Consequently, if the Xj have the normal distribution,  1 + ··· + X2   3.3.2  EMPIRICAL TESTS  61  The equanimity of your average tosser of coins depends upon a law . . . which ensures that he will not upset himself by losing too much nor upset his opponent by winning too often. — TOM STOPPARD, Rosencrantz & Guildenstern are Dead  1966   3.3.2. Empirical Tests In this section we shall discuss eleven kinds of specific tests that have traditionally been applied to sequences in order to investigate their randomness. The discus- sion of each test has two parts:  a  a “plug-in” description of how to perform the test; and  b  a study of the theoretical basis for the test.  Readers who lack math- ematical training may wish to skip over the theoretical discussions. Conversely, mathematically inclined readers may find the associated theory quite interesting, even if they never intend to test random number generators, since some instruc- tive combinatorial questions are involved here. Indeed, this section introduces several topics that will be important to us later in quite different contexts.   Each test is applied to a sequence  ⟨Un⟩ = U0, U1, U2, . . .   1  of real numbers, which purports to be independently and uniformly distributed between zero and one. Some of the tests are designed primarily for integer-valued sequences, instead of the real-valued sequence  1 . In this case, the auxiliary sequence ⟨Yn⟩ = Y0, Y1, Y2, . . .   2   defined by the rule  Yn = ⌊dUn⌋   3  is used instead. This is a sequence of integers that purports to be independently and uniformly distributed between 0 and d − 1. The number d is chosen for convenience; for example, we might have d = 64 = 26 on a binary computer, so that Yn represents the six most significant bits of the binary representation of Un. The value of d should be large enough so that the test is meaningful, but not so large that the test becomes impracticably difficult to carry out.  The quantities Un, Yn, and d will have the significance stated above through- out this section, although the value of d will probably be different in different tests. A. Equidistribution test  Frequency test . The first requirement that sequence  1  must meet is that its numbers are, in fact, uniformly distributed between zero and one. There are two ways to make this test:  a  Use the Kolmogorov–Smirnov test, with F x  = x for 0 ≤ x ≤ 1.  b  Let d be a convenient number, such as 100 on a decimal computer, 64 or 128 on a binary computer, and use the sequence  2  instead of  1 . For each integer r, 0 ≤ r < d, count the number of times that Yj = r for 0 ≤ j < n, and then apply the chi-square test using k = d and probability ps = 1 d for each category.  The theory behind this test has been covered in Section 3.3.1.   62  RANDOM NUMBERS  3.3.2  B. Serial test. More generally, we want pairs of successive numbers to be uniformly distributed in an independent manner. The sun comes up just about as often as it goes down, in the long run, but that doesn’t make its motion random. To carry out the serial test, we simply count the number of times that the pair  Y2j, Y2j+1  =  q, r  occurs, for 0 ≤ j < n; these counts are to be made for each pair of integers  q, r  with 0 ≤ q, r < d, and the chi-square test is applied to these k = d2 categories with probability 1 d2 in each category. As with the equidistribution test, d may be any convenient number, but it will be somewhat smaller than the values suggested above since a valid chi-square test should have n large compared to k  say n ≥ 5d2 at least .  Clearly we can generalize this test to triples, quadruples, etc., instead of pairs  see exercise 2 ; however, the value of d must then be severely reduced in order to avoid having too many categories. When quadruples and larger numbers of adjacent elements are considered, we therefore make use of less exact tests such as the poker test or the maximum test described below.  Notice that 2n numbers of the sequence  2  are used in this test in order to make n observations. It would be a mistake to perform the serial test on the pairs  Y0, Y1 ,  Y1, Y2 , . . . ,  Yn−1, Yn ; can the reader see why? We might perform another serial test on the pairs  Y2j+1, Y2j+2 , and expect the sequence to pass both tests, remembering that the tests aren’t independent of each other. Alternatively, George Marsaglia has proved that, if the pairs  Y0, Y1 ,  Y1, Y2 , . . . ,  Yn−1, Yn  are used, and if we use the usual chi-square method to compute both the statistics V2 for the serial test and V1 for the frequency test on Y0, . . . , Yn−1 with the same value of d, then V2 − V1 should have the chi-square distribution with d d − 1  degrees of freedom when n is large.  See exercise 24.  C. Gap test. Another test is used to examine the length of “gaps” between occurrences of Uj in a certain range. If α and β are two real numbers with 0 ≤ α < β ≤ 1, we want to consider the lengths of consecutive subsequences Uj, Uj+1, . . . , Uj+r in which Uj+r lies between α and β but the other U’s do not.  This subsequence of r + 1 numbers represents a gap of length r.  Algorithm G  Data for gap test . The following algorithm, applied to the sequence  1  for any given values of α and β, counts the number of gaps of lengths 0, 1, . . . , t − 1 and the number of gaps of length ≥ t, until n gaps have been tabulated. G1. [Initialize.] Set j ← −1, s ← 0, and set COUNT[r] ← 0 for 0 ≤ r ≤ t. G2. [Set r zero.] Set r ← 0. G3. [α ≤ Uj < β?] Increase j by 1. If Uj ≥ α and Uj < β, go to step G5. G4. [Increase r.] Increase r by one, and return to step G3. G5. [Record the gap length.]  A gap of length r has now been found.  If r ≥ t,  increase COUNT[t] by one, otherwise increase COUNT[r] by one.  G6. [n gaps found?] Increase s by one. If s < n, return to step G2.   3.3.2  EMPIRICAL TESTS  63  Fig. 6. Gathering data for the gap test.  Algorithms for the “coupon-collector’s test” and the “run test” are similar.   After Algorithm G has been performed, the chi-square test is applied to the k = t + 1 values of COUNT[0], COUNT[1], . . . , COUNT[t], using the following probabilities:  pt =  1 − p t.  pr = p 1 − p r,  for 0 ≤ r ≤ t − 1;   4  Here p = β − α is the probability that α ≤ Uj < β. The values of n and t are to be chosen, as usual, so that each of the values of COUNT[r] is expected to be 5 or more, preferably more.  The gap test is often applied with α = 0 or β = 1 in order to omit one of the comparisons in step G3. The special cases  α, β  =  0, 1 2 , 1  give rise to tests that are sometimes called “runs above the mean” and “runs below the mean,” respectively.  The probabilities in Eq.  4  are easily deduced, so this derivation is left to the reader. Notice that the gap test as described above observes the lengths of n gaps; it does not observe the gap lengths among n numbers. If the sequence ⟨Un⟩ is sufficiently nonrandom, Algorithm G might not terminate. Other gap tests that examine a fixed number of U’s have also been proposed  see exercise 5 . D. Poker test  Partition test . The “classical” poker test considers n groups of five successive integers, {Y5j, Y5j+1, . . . , Y5j+4} for 0 ≤ j < n, and observes which of the following seven patterns is matched by each  orderless  quintuple:  2  or   1  All different: abcde One pair: aabcd Two pairs: aabbc Three of a kind: aaabc Full house: aaabb Four of a kind: aaaab Five of a kind: aaaaa  A chi-square test is based on the number of quintuples in each category.  It is reasonable to ask for a somewhat simpler version of this test, to facilitate the programming involved. A good compromise would simply be to count the  G1. Initialize  G2. Set r zero  G3. α ≤ Uj < β?  No  G4. Increase r  G6. n gaps found?  G5. Record the gap length  Yes  No  Yes   64  RANDOM NUMBERS  3.3.2  number of distinct values in the set of five. We would then have five categories:  5 values = all different; 4 values = one pair; 3 values = two pairs, or three of a kind; 2 values = full house, or four of a kind; 1 value = five of a kind.  This breakdown is easier to determine systematically, and the test is nearly as good.  In general we can consider n groups of k successive numbers, and we can count the number of k-tuples with r different values. A chi-square test is then made, using the probability   k   are defined in Section 1.2.6,   5   r  pr = d d − 1  . . .  d − r + 1  that there are r different.  The Stirling numbersk  dk  r  r  and they can readily be computed using the formulas given there.  Since the probability pr is very small when r = 1 or 2, we generally lump a few categories of low probability together before the chi-square test is applied. To derive the proper formula for pr, we must count how many of the dk k-tuples of numbers between 0 and d − 1 have exactly r different elements, and divide the total by dk. Since d d − 1  . . .  d − r + 1  is the number of ordered  choices of r things from a set of d objects, we need only show that k   is the  number of ways to partition a set of k elements into exactly r parts. Therefore exercise 1.2.6–64 completes the derivation of Eq.  5 . E. Coupon collector’s test. The next test is related to the poker test some- what as the gap test is related to the frequency test. We use the sequence Y0, Y1, . . . , and we observe the lengths of segments Yj+1, Yj+2, . . . , Yj+r that are required to get a “complete set” of integers from 0 to d−1. Algorithm C describes this precisely: Algorithm C  Data for coupon collector’s test . Given a sequence of integers Y0, Y1, . . . , with 0 ≤ Yj < d, this algorithm counts the lengths of n consecutive “coupon collector” segments. At the conclusion of the algorithm, COUNT[r] is the number of segments with length r, for d ≤ r < t, and COUNT[t] is the number of segments with length ≥ t. C1. [Initialize.] Set j ← −1, s ← 0, and set COUNT[r] ← 0 for d ≤ r ≤ t. C2. [Set q, r zero.] Set q ← r ← 0, and set OCCURS[k] ← 0 for 0 ≤ k < d. C3. [Next observation.] Increase r and j by 1. If OCCURS[Yj] ̸= 0, repeat this C4. [Complete set?] Set OCCURS[Yj] ← 1 and q ← q + 1.  The subsequence observed so far contains q distinct values; if q = d, we therefore have a complete set.  If q < d, return to step C3.  step.   3.3.2 65 C5. [Record the length.] If r ≥ t, increase COUNT[t] by one, otherwise increase  EMPIRICAL TESTS  COUNT[r] by one.  C6. [n found?] Increase s by one. If s < n, return to step C2.  For an example of this algorithm, see exercise 7. We may think of a boy col- lecting d types of coupons, which are randomly distributed in his breakfast cereal boxes; he must keep eating more cereal until he has one coupon of each type. A chi-square test is to be applied to COUNT[d], COUNT[d + 1], . . . , COUNT[t], with k = t− d + 1, after Algorithm C has counted n lengths. The corresponding probabilities are pr = d!   r − 1   t − 1  d ≤ r < t;       6   ,  .  dr  d − 1  d  To derive these probabilities, we simply note that if qr denotes the probability that a subsequence of length r is incomplete, then  pt = 1 − d! dt−1    r  qr = 1 − d!  dr  d  by Eq.  5 ; for this means we have an r-tuple of elements that do not have all d different values. Then  6  follows from the relations pt = qt−1 and  pr = qr−1 − qr  for d ≤ r < t.  For formulas that arise in connection with generalizations of the coupon collector’s test, see exercises 9 and 10 and also the papers by George Pólya, Zeitschrift für angewandte Math. und Mech. 10  1930 , 96–97; Hermann von Schelling, AMM 61  1954 , 306–311. F. Permutation test. Divide the input sequence into n groups of t elements each, that is,  Ujt, Ujt+1, . . . , Ujt+t−1  for 0 ≤ j < n. The elements in each group can have t! possible relative orderings; the number of times each ordering appears is counted, and a chi-square test is applied with k = t! and with probability 1 t! for each ordering. For example, if t = 3 we would have six possible categories, according to whether U3j < U3j+1 < U3j+2 or U3j < U3j+2 < U3j+1 or ··· or U3j+2 < U3j+1 < U3j. We assume in this test that equality between U’s does not occur; such an assumption is justified, for the probability that two U’s are equal is zero. A convenient way to perform the permutation test on a computer makes use  of the following algorithm, which is of interest in itself: Algorithm P  Analyze a permutation . Given a sequence of distinct elements  U1, . . . , Ut , we compute an integer f U1, . . . , Ut  such that  0 ≤ f U1, . . . , Ut  < t!,  and f U1, . . . , Ut  = f V1, . . . , Vt  if and only if  U1, . . . , Ut  and  V1, . . . , Vt  have the same relative ordering.   RANDOM NUMBERS  66 3.3.2 P1. [Initialize.] Set r ← t, f ← 0.  During this algorithm we will have 0 ≤ f <  t! r!.   P2. [Find maximum.] Find the maximum of {U1, . . . , Ur}, and suppose that Us  is the maximum. Set f ← r · f + s − 1.  P3. [Exchange.] Exchange Ur ↔ Us. P4. [Decrease r.] Decrease r by one. If r > 1, return to step P2.  The sequence  U1, . . . , Ut  will have been sorted into ascending order when this algorithm stops. To prove that the result f uniquely characterizes the initial order of  U1, . . . , Ut , we note that Algorithm P can be run backwards:  For r = 2, 3, . . . , t,  set s ← f mod r, f ← ⌊f  r⌋, and exchange Ur ↔ Us+1.  It is easy to see that this will undo the effects of steps P2–P4; hence no two permutations can yield the same value of f, and Algorithm P performs as advertised. The essential idea that underlies Algorithm P is a mixed-radix representation called the “factorial number system”: Every integer in the range 0 ≤ f < t! can be uniquely written in the form  f =. . .  ct−1 ×  t − 1  + ct−2  ×  t − 2  + ··· + c2   × 2 + c1  =  t − 1 ! ct−1 +  t − 2 ! ct−2 + ··· + 2! c2 + 1! c1   7   where the “digits” cj are integers satisfying  0 ≤ cj ≤ j,  for 1 ≤ j < t.   8  In Algorithm P, cr−1 = s − 1 when step P2 is performed for a given value of r. G. Run test. A sequence may also be tested for “runs up” and “runs down.” This means that we examine the length of monotone portions of the original sequence  segments that are increasing or decreasing .  As an example of the precise definition of a run, consider the sequence of ten digits “1298536704”. Putting a vertical line at the left and right and between Xj and Xj+1 whenever Xj > Xj+1, we obtain  1 2 9 8 5 3 6 7 0 4 ,   9  which displays the “runs up”: There is a run of length 3, followed by two runs of length 1, followed by another run of length 3, followed by a run of length 2. The algorithm of exercise 12 shows how to tabulate the length of “runs up.”  Unlike the gap test and the coupon collector’s test  which are in many other respects similar to this test , we should not apply a chi-square test to the run counts, since adjacent runs are not independent. A long run will tend to be followed by a short run, and conversely. This lack of independence is enough to   3.3.2  EMPIRICAL TESTS  67  invalidate a straightforward chi-square test. Instead, the following statistic may be computed, when the run lengths have been determined as in exercise 12:   COUNT[i] − nbi  COUNT[j] − nbj aij,   10     1≤i,j≤6  V = 1 n − 6  where n is the length of the sequence, and the matrices of coefficients A =  aij 1≤i,j≤6 and B =  bi 1≤i≤6 are given by    A =  18091  4529.4 9044.9 13568 9044.9 18097 27139 13568 18091 36187 45234 22615 27892 55789  22615 27892 55789 27139 36187 45234 67852 40721 83685 54281 90470 111580 54281 72414 67852 90470 113262 139476 83685 111580 139476 172860  , B =    .  1 6 5 24 11 120 19 720 29 5040  1 840   11   The values of aij shown here are approximate only; exact values can be obtained from formulas derived below.  The statistic V in  10  should have the chi-square distribution with six, not five, degrees of freedom, when n is large. The value of n should be, say, 4000 or more. The same test can be applied to “runs down.” A vastly simpler and more practical run test appears in exercise 14, so a reader who is interested only in testing random number generators should skip the next few pages and go on to the “maximum-of-t test” after looking at exercise 14. On the other hand it is instructive from a mathematical standpoint to see how a complicated run test with interdependent runs can be treated, so we shall now digress for a moment.  Given any permutation of n elements,  let Zpi = 1 if position i is the beginning of an ascending run of length p or more, and let Zpi = 0 otherwise. For example, consider the permutation  9  with n = 10; we have  Z11 = Z21 = Z31 = Z14 = Z15 = Z16 = Z26 = Z36 = Z19 = Z29 = 1,  and all other Z’s are zero. With this notation,  p = Zp1 + Zp2 + ··· + Zpn R′   12   is the number of runs of length ≥ p, and Rp = R′  p − R′   13  is the number of runs of length p exactly. Our goal is to compute the mean value of Rp, and also the covariance  covar Rp, Rq  = meanRp − mean Rp Rq − mean Rq ,  p+1  which measures the interdependence of Rp and Rq. These mean values are to be computed as the average over the set of all n! permutations.   68  RANDOM NUMBERS  3.3.2  Equations  12  and  13  show that the answers can be expressed in terms of the mean values of Zpi and of ZpiZqj, so as the first step of the derivation we obtain the following results  assuming that i < j :    Zpi =  1 n!    1 n!   p + 1 ! , 0,   p + δi1   if i ≤ n − p + 1; otherwise.  if i + p < j ≤ n − q + 1;   14    p + δi1 q  p + 1 !  q + 1 ! ,  p + 1 ! q! − p + q + δi1 p + δi1  p + q + 1 ! , 0,  ZpiZqj =  if i + p = j ≤ n − q + 1; otherwise.  The-signs stand for summation over all possible permutations. To illustrate  the calculations involved here, we will work the most difficult case, when i + p = j ≤ n − q + 1, and when i > 1. The quantity ZpiZqj is either zero or one, so the summation consists of counting all permutations U1U2 . . . Un for which Zpi = Zqj = 1, that is, all permutations such that   n  Ui−1 > Ui   Ui+p < ··· < Ui+p+q−1.   15  The number of such permutations may be enumerated as follows: There are p+q+1 are   ways to choose the elements for the positions indicated in  15 ; there   − p + q + 1   − p + q + 1   p + q   p + q + 1      16   + 1  p  p + 1  1   n   n − p − q − 1 ! times  16  ways in all, and we divide by n! to get the  ways to arrange them in the order  15 , as shown in exercise 13; and there are  n − p − q − 1 ! ways to arrange the remaining elements. Thus there are p+q+1 desired formula.  From relations  14  a rather lengthy calculation leads to  mean R′ p, R′  covar R′  1≤i,j≤n  p  mean R′ q   ZpiZqj − mean R′  1 ≤ p ≤ n; p  mean R′ q   p  =  n + 1 p  p + 1 ! −  p − 1  p!, =   q  − mean R′ p R′ q  = mean R′ 1  mean R′ n!  s 1 − pq  + pq  p + 1 !  q + 1 ! − 2s  s + 1 ! +  s2 − s − 2 pq − s2 − p2q2 + 1  t  + f p, q, n , t  − mean R′   s − 1  if p + q ≤ n, if p + q > n,  p  mean R′ q ,      + 2  =  s!   p + 1 !  q + 1 !   17    18   .   19   mean R′ where t = max p, q , s = p + q, and  f p, q, n  =  n + 1    3.3.2  EMPIRICAL TESTS  69  This expression for the covariance is unfortunately quite complicated, but it is necessary for a successful run test as described above. From these formulas it is easy to compute  mean Rp  = mean R′ q  = covar R′  p  − mean R′ covar Rp, R′ p, R′ covar Rp, Rq  = covar Rp, R′  p+1 , q  − covar R′ q  − covar Rp, R′  p+1, R′ q , q+1 .  In Annals Math. Stat. 15  1944 , 163–165, J. Wolfowitz proved that the quan- t become normally distributed as n → ∞, subject to tities R1, R2, . . . , Rt−1, R′ the mean and covariance expressed above; this implies that the following test for runs is valid: Given a sequence of n random numbers, compute the number of runs Rp of length p for 1 ≤ p < t, and also the number of runs R′ t of length t or more. Let  Q1 = R1 − mean R1 ,  . . . , Qt−1 = Rt−1 − mean Rt−1 , t − mean R′ t .  Qt = R′  Form the matrix C of the covariances of the R′s;  covar R1, R3 , while C1t = covar R1, R′  t . When t = 6, we have  for example, C13 =  C = nC1 + C2,   20    21    22      23 180 −7 360 −5 336 −433 60480 −13 5670 −121 181440  83 180 −29 180 −11 210 −41 12096  25920  91  41  18144  where  C1 =  C2 =  t  −7 360 2843 20160 −989 20160 −7159 362880 −10019 1814400 −1303 907200 −29 180 −305 4032 319 20160 2557 72576 10177 604800  413 64800  −5 336 −989 20160 54563 907200 −21311 1814400 −62369 19958400 −7783 9979200 −11 210 319 20160 −58747 907200 19703 604800 239471 19958400  39517 9979200  −433 60480 −7159 362880 −21311 1814400 886657 39916800 −257699 239500800 −62611 239500800 −41 12096 2557 72576 19703 604800 −220837 4435200 1196401 239500800  360989  239500800  −13 5670 −10019 1814400 −62369 19958400 −257699 239500800 29874811 5448643200 −1407179 21794572800  −121 181440 −1303 907200 −7783 9979200 −62611 239500800 −1407179 21794572800  2134697  1816214400  91  25920 10177 604800 239471 19958400 1196401 239500800 −139126639 7264857600  4577641  10897286400  41  18144 413 64800 39517 9979200 360989  239500800 4577641  10897286400 −122953057 21794572800   ,   if n ≥ 12. Now form A =  aij , the inverse of the matrix C, and compute i,j=1 QiQjaij. The result for large n should have approximately the chi-square The matrix A given earlier in  11  is the inverse of C1 to five significant fig- 1 − 1 . Therefore  distribution with t degrees of freedom. 1 − n−2C−1 ures. The true inverse, A, is n−1C−1 ··· , and it turns out that C−1 1 C2 C−1 1 by  10 , V ≈ QTC−1  1 Q  n − 6 , where Q =  Q1 . . . Qt T .  is very nearly equal to −6C−1  1 + n−3C−1  1 C2 C−1  1 C2 C−1  1 C2 C−1   n−1.  RANDOM NUMBERS  70 3.3.2 H. Maximum-of-t test. For 0 ≤ j < n, let Vj = max Utj, Utj+1, . . . , Utj+t−1 . Now apply the Kolmogorov–Smirnov test to the sequence V0, V1, . . . , Vn−1, with the distribution function F x  = xt, 0 ≤ x ≤ 1. Alternatively, apply the equidistribution test to the sequence V t0 , V t1 , . . . , V t To verify this test, we must show that the distribution function for the Vj is F x  = xt. The probability that max U1, U2, . . . , Ut  ≤ x is the probability that U1 ≤ x and U2 ≤ x and . . . and Ut ≤ x, which is the product of the individual probabilities, namely xx . . . x = xt. I. Collision test. Chi-square tests can be made only when a nontrivial number of items are expected in each category. But another kind of test can be used when the number of categories is much larger than the number of observations; this test is related to “hashing,” an important method for information retrieval that we shall study in Section 6.4.  Suppose we have m urns and we throw n balls at random into those urns, where m is much greater than n. Most of the balls will land in urns that were previously empty, but if a ball falls into an urn that already contains at least one ball we say that a “collision” has occurred. The collision test counts the number of collisions, and a generator passes this test if it doesn’t induce too many or too few collisions.  To fix the ideas, suppose m = 220 and n = 214. Then each urn will receive only one 64th of a ball, on the average. The probability that a given urn will  contain exactly k balls is pk =n m−k 1 − m−1 n−k, so the expected number of  k − 1 pk = collisions per urn is kpk − m−2 − smaller terms, we find that Since p0 =  1 − m−1 n = 1 − nm−1 +n  the average total number of collisions taken over all m urns is slightly less than n2  2m  = 128.  The actual value is ≈ 127.33.   pk = n m  We can use the collision test to rate a random number generator in a large number of dimensions. For example, when m = 220 and n = 214 we can test the 20-dimensional randomness of a number generator by letting d = 2 and forming 20-dimensional vectors Vj =  Y20j, Y20j+1, . . . , Y20j+19  for 0 ≤ j < n. We keep a table of m = 220 bits to determine collisions, one bit for each possible value of the vector Vj; on a computer with 32 bits per word, this amounts to 215 words. Initially all 220 bits of this table are cleared to zero; then for each Vj, if the corresponding bit is already 1 we record a collision, otherwise we set the bit to 1. This test can also be used in 10 dimensions with d = 4, and so on.  − 1 + p0.  To decide if the test is passed, we can use the following table of percentage  k≥1  k≥1  k≥0  2  k  points when m = 220 and n = 214:  collisions ≤ 101 with probability .009  108 .043  119 .244  126 134 .476 .742  145 .946  153 .989  The theory underlying these probabilities is the same we used in the poker test, Eq.  5 ; the probability that c collisions occur is the probability that n − c urns   3.3.2  are occupied, namely  m m − 1  . . .  m − n + c + 1   mn   n  n − c    .  EMPIRICAL TESTS  71  Although m and n are very large, it is not difficult to compute these probabilities using the following method: Algorithm S  Percentage points for collision test . Given m and n, this algorithm determines the distribution of the number of collisions that occur when n balls are scattered into m urns. An auxiliary array A[0], A[1], . . . , A[n] of floating point numbers is used for the computation; actually A[j] will be nonzero only for j0 ≤ j ≤ j1, and j1 − j0 will be at most of order log n, so it would be possible to get by with considerably less storage. S1. [Initialize.] Set A[j] ← 0 for 0 ≤ j ≤ n; then set A[1] ← 1 and j0 ← j1 ← 1.  Then do step S2 exactly n − 1 times and go on to step S3.  set A[j] ←  j m A[j] + 1 + 1 m  −  j m A[j − 1]. If A[j] has become  S2. [Update probabilities.]  Performing this step once corresponds to tossing a ball into an urn; A[j] represents the probability that exactly j of the urns are occupied.  Set j1 ← j1 + 1. Then for j ← j1, j1 − 1, . . . , j0  in this order , very small as a result of this calculation, say A[j] < 10−20, set A[j] ← 0; and in such a case, decrease j1 by 1 if j = j1, or increase j0 by 1 if j = j0. In this step we make use of an auxiliary table  T1, T2, . . . , Ttmax  =  .01, .05, .25, .50, .75, .95, .99, 1.00  containing the specified percentage points of interest. Set p ← 0, t ← 1, and j ← j0−1. Do the following iteration until t = tmax: Increase j by 1, and set p ← p + A[j]; then if p > Tt, output n − j − 1 and 1 − p  meaning that with probability 1 − p there are at most n − j − 1 collisions  and repeatedly increase t by 1 until p ≤ Tt.  S3. [Compute the answers.]  J. Birthday spacings test. George Marsaglia introduced a new kind of test in 1984: We throw n balls into m urns, as in the collision test, but now we think of the urns as “days of a year” and the balls as “birthdays.” Suppose the birthdays are  Y1, . . . , Yn , where 0 ≤ Yk < m. Sort them into nondecreasing order Y 1  ≤ ··· ≤ Y n ; then define n “spacings” S1 = Y 2  − Y 1 , . . . , Sn−1 = Y n  − Y n−1 , Sn = Y 1  + m − Y n ; finally sort the spacings into order, S 1  ≤ ··· ≤ S n . Let R be the number of equal spacings, namely the number of indices j such that 1 < j ≤ n and S j  = S j−1 . When m = 225 and n = 512, we should have 3 or more .078691997  with probability .368801577  .369035243  .183471182  R =  0  1  2   The average number of equal spacings for this choice of m and n should be approximately 1.  Repeat the test 1000 times, say, and do a chi-square test with 3 degrees of freedom to compare the empirical R’s with the correct distribution; this will tell whether or not the generator produces reasonably random birthday spacings. Exercises 28–30 develop the theory behind this test and formulas for other values of m and n.   72  RANDOM NUMBERS  3.3.2  Such a test of birthday spacings is important primarily because of the remarkable fact that lagged Fibonacci generators consistently fail it, although they pass the other traditional tests quite nicely. [Dramatic examples of such failures were reported by Marsaglia, Zaman, and Tsang in Stat. and Prob. Letters 9  1990 , 35–39.] Consider, for example, the sequence Xn =  Xn−24 + Xn−55  mod m of Eq. 3.2.2– 7 . The numbers of this sequence satisfy  Xn + Xn−86 ≡ Xn−24 + Xn−31   modulo m   because both sides are congruent to Xn−24 + Xn−55 + Xn−86. Therefore two pairs of differences are equal:  and  Xn − Xn−24 ≡ Xn−31 − Xn−86,  Xn − Xn−31 ≡ Xn−24 − Xn−86.  Whenever Xn is reasonably close to Xn−24 or Xn−31  as it should be in a truly random sequence , the difference has a good chance of showing up in two of the spacings. So we get significantly more cases of equality — typically R ≈ 2 on the average, not 1. But if we discount from R any equal spacings that arise from the stated congruence, the resulting statistic R′ usually does pass the birthday test.  One way to avoid failure is to discard certain elements of the sequence, using for example only X0, X2, X4, . . . as random numbers; then we never get all four elements of the set {Xn, Xn−24, Xn−31, Xn−86}, and the birthday spacings are no problem. An even better way to avoid the problem is to discard consecutive batches of numbers, as suggested by Lüscher; see Section 3.2.2.  Similar remarks apply to the subtract-with-borrow and add- with-carry generators of exercise 3.2.1.1–14. K. Serial correlation test. We may also compute the following statistic: C = n U0U1+U1U2+···+Un−2Un−1+Un−1U0 − U0+U1+···+Un−1 2 This is the “serial correlation coefficient,” a measure of the extent to which Uj+1 depends on Uj.  n−1  −  U0 + U1 + ··· + Un−1 2  Correlation coefficients appear frequently in statistical work. If we have n quantities U0, U1, . . . , Un−1 and n others V0, V1, . . . , Vn−1, the correlation coefficient between them is defined to be  1 + ··· + U 2  0 + U 2  .  23   n U 2  n UjVj  − Uj  Vj  j − Vj 2n V 2 j − Uj  n U 2  2 .  C =   24   All summations in this formula are to be taken over the range 0 ≤ j < n; Eq.  23  is the special case Vj = U j+1  mod n. The denominator of  24  is zero when U0 = U1 = ··· = Un−1 or V0 = V1 = ··· = Vn−1; we exclude that case from discussion.   3.3.2  73 A correlation coefficient always lies between −1 and +1. When it is zero or very small, it indicates that the quantities Uj and Vj are  relatively speaking  independent of each other, whereas a value of ±1 indicates total linear depen- dence. In fact, Vj = α ± βUj for all j in the latter case, for some constants α and β.  See exercise 17.   EMPIRICAL TESTS  Therefore it is desirable to have C in Eq.  23  close to zero.  In actual fact, since U0U1 is not completely independent of U1U2, the serial correlation coefficient is not expected to be exactly zero.  See exercise 18.  A “good” value of C will be between µn − 2σn and µn + 2σn, where  µn = −1 n − 1 ,  n2  σ2 n =   n − 1 2 n − 2  ,  n > 2.   25   We expect C to be between these limits about 95 percent of the time.  The formula for σ2  n in  25  is an upper bound, valid for serial correlations between independent random variables from an arbitrary distribution. When the U’s are uniformly distributed, the true variance is obtained by subtracting 5 n−2 + O n−7 3 log n .  See exercise 20.  24 Instead of simply computing the correlation coefficient between the obser- vations  U0, U1, . . . , Un−1  and their immediate successors  U1, . . . , Un−1, U0 , we can also compute it between  U0, U1, . . . , Un−1  and any cyclically shifted sequence  Uq, . . . , Un−1, U0, . . . , Uq−1 ; the cyclic correlations should be small for 0 < q < n. A straightforward computation of Eq.  24  for all q would require about n2 multiplications, but it is actually possible to compute all the  correlations in only O n log n  steps by using “fast Fourier transforms.” See Section 4.6.4; see also L. P. Schmid, CACM 8  1965 , 115.  L. Tests on subsequences. External programs often call for random numbers in batches. For example, if a program works with three random variables X, Y, and Z, it may consistently invoke the generation of three random numbers at a time. In such applications it is important that the subsequences consisting of every third term of the original sequence be random. If the program requires q numbers at a time, the sequences  U0, Uq, U2q, . . . ; U1, Uq+1, U2q+1, . . . ;  . . . ; Uq−1, U2q−1, U3q−1, . . .  can each be put through the tests described above for the original sequence U0, U1, U2, . . . .  Experience with linear congruential sequences has shown that these derived sequences rarely if ever behave less randomly than the original sequence, unless q has a large factor in common with the period length. On a binary computer with m equal to the word size, for example, a test of the subsequences for q = 8 will tend to give the poorest randomness for all q < 16; and on a decimal computer, q = 10 yields the subsequences most likely to be unsatisfactory.  This can be explained somewhat on the grounds of potency, since such values of q will tend to lower the potency. Exercise 3.2.1.2–20 provides a more detailed explanation.    74  RANDOM NUMBERS  3.3.2  M. Historical remarks and further discussion. Statistical tests arose naturally in the course of scientists’ efforts to “prove” or “disprove” hypotheses about various observed data. The best-known early papers dealing with the testing of artificially generated numbers for randomness are two articles by M. G. Kendall and B. Babington-Smith in the Journal of the Royal Statistical Society 101  1938 , 147–166, and in the supplement to that journal, 6  1939 , 51–61. Those papers were concerned with the testing of random digits between 0 and 9, rather than random real numbers; for this purpose, the authors discussed the frequency test, serial test, gap test, and poker test, although they misapplied the serial test. Kendall and Babington-Smith also used a variant of the coupon collector’s test; the method described in this section was introduced by R. E. Greenwood in Math. Comp. 9  1955 , 1–5.  The run test has a rather interesting history. Originally, tests were made on runs up and down at once: A run up would be followed by a run down, then another run up, and so on. Note that the run test and the permutation test do not depend on the uniform distribution of the U’s, but only on the fact that Ui = Uj occurs with probability zero when i ̸= j; therefore these tests can be applied to many types of random sequences. The run test in primitive form was originated by J. Bienaymé [Comptes Rendus Acad. Sci. 81  Paris, 1875 , 417– 423]. Some sixty years later, W. O. Kermack and A. G. McKendrick published two extensive papers on the subject [Proc. Royal Society Edinburgh 57  1937 , 228–240, 332–376]; as an example they stated that Edinburgh rainfall between the years 1785 and 1930 was “entirely random in character” with respect to the run test  although they examined only the mean and standard deviation of the run lengths . Several other people began using the test, but it was not until 1944 that the use of the chi-square method in connection with this test was shown to be incorrect. A paper by H. Levene and J. Wolfowitz in Annals Math. Stat. 15  1944 , 58–69, introduced the correct run test  for runs up and down, alternately  and discussed the fallacies in earlier misuses of that test. Separate tests for runs up and runs down, as proposed in the text above, are more suited to computer application, so we have not given the more complex formulas for the alternate-up-and-down case. See the survey paper by D. E. Barton and C. L. Mallows, Annals Math. Stat. 36  1965 , 236–260.  Of all the tests we have discussed, the frequency test and the serial corre- lation test seem to be the weakest, in the sense that nearly all random number generators pass them. Theoretical grounds for the weakness of these tests are discussed briefly in Section 3.5  see exercise 3.5–26 . The run test, on the other hand, is rather strong: The results of exercises 3.3.3–23 and 24 suggest that linear congruential generators tend to have runs somewhat longer than normal if the multiplier is not large enough, so the run test of exercise 14 is definitely to be recommended.  The collision test is also highly recommended, since it has been specially designed to detect the deficiencies of many poor generators that have unfortu- nately become widespread. Based on ideas of H. Delgas Christiansen [Inst. Math. Stat. and Oper. Res., Tech. Univ. Denmark  October 1975 , unpublished], this   3.3.2  EMPIRICAL TESTS  75  test was the first to be developed after the advent of computers; it is specifically intended for computer use, and unsuitable for hand calculation.  The reader probably wonders, “Why are there so many tests?” It has been said that more computer time is spent testing random numbers than using them in applications! This is untrue, although it is possible to go overboard in testing. The need for making several tests has been amply documented. People have found, for example, that some numbers generated by a variant of the middle- square method have passed the frequency test, gap test, and poker test, yet flunked the serial test. Linear congruential sequences with small multipliers have been known to pass many tests, yet fail on the run test because there are too few runs of length one. The maximum-of-t test has also been used to ferret out some bad generators that otherwise seemed to perform respectably. A subtract- with-borrow generator fails the gap test when the maximum gap length exceeds the largest lag; see Vattulainen, Kankaala, Saarinen, and Ala-Nissila, Computer Physics Communications 86  1995 , 209–226, where a variety of other tests are also reported. Lagged Fibonacci generators, which are theoretically guaranteed to have equally distributed least-significant bits, still fail some simple variants of the 1-bit equidistribution test  see exercises 31 and 35, also 3.6–14 .  Perhaps the main reason for doing extensive testing on random number generators is that people misusing Mr. X’s random number generator will hardly ever admit that their programs are at fault: They will blame the generator, until Mr. X can prove to them that his numbers are sufficiently random. On the other hand, if the source of random numbers is only for Mr. X’s personal use, he might decide not to bother to test them, since the techniques recommended in this chapter have a high probability of being satisfactory.  As computers become faster, more random numbers are consumed than ever before, and random number generators that once were satisfactory are no longer good enough for sophisticated applications in physics, combinatorics, stochastic geometry, etc. George Marsaglia has therefore introduced a number of stringent tests, which go well beyond classical methods like the gap and poker tests, in order to meet the new challenges. For example, he found that the sequence Xn+1 =  62605Xn + 113218009  mod 229 had a noticeable bias in the following experiment: Generate 221 random numbers Xn and extract their 10 leading bits Yn = ⌊Xn 219⌋. Count how many of the 220 possible pairs  y, y′  of 10-bit numbers do not occur among  Y1, Y2 ,  Y2, Y3 , . . . ,  Y221−1, Y221 . There ought to be about 141909.33 missing pairs, with standard deviation ≈ 290.46  see exercise 34 . But six consecutive trials, starting with X1 = 1234567, produced counts that were all between 1.5 and 3.5 standard deviations too low. The distribution was a bit too “flat” to be random — probably because 221 numbers is a significant fraction, 1 256, of the entire period. A similar generator with multiplier 69069 and modulus 230 proved to be better. Marsaglia and Zaman call this procedure a “monkey test,” because it counts the number of two-character combinations that a monkey will miss after typing randomly on a keyboard with 1024 keys; see Computers and Math. 26, 9  November 1993 , 1–10, for the analysis of several monkey tests.   76  RANDOM NUMBERS  3.3.2  EXERCISES 1. [10] Why should the serial test described in part B be applied to  Y0, Y1 ,  Y2, Y3 , . . . ,  Y2n−2, Y2n−1  instead of to  Y0, Y1 ,  Y1, Y2 , . . . ,  Yn−1, Yn ? 2. [10] State an appropriate way to generalize the serial test to triples, quadruples, etc., instead of pairs.   cid:120  3. [M20] How many U’s need to be examined in the gap test  Algorithm G  before  n gaps have been found, on the average, assuming that the sequence is random? What is the standard deviation of this quantity? 4. [M12] Prove that the probabilities in  4  are correct for the gap test. 5. [M23] The “classical” gap test used by Kendall and Babington-Smith considers the numbers U0, U1, . . . , UN−1 to be a cyclic sequence with UN+j identified with Uj. Here N is a fixed number of U’s that are to be subjected to the test. If n of the numbers U0, . . . , UN−1 fall into the range α ≤ Uj < β, there are n gaps in the cyclic sequence. gaps of length ≥ t; show that the quantity V =  Let Zr be the number of gaps of length r, for 0 ≤ r < t, and let Zt be the number of 0≤r≤t Zr − npr 2 npr should have the chi-square distribution with t degrees of freedom, in the limit as N goes to infinity, where pr is given in Eq.  4 . 6. [40]  H. Geiringer.  A frequency count of the first 2000 decimal digits in the representation of e = 2.71828 . . . gave a χ2 value of 1.06, indicating that the actual frequencies of the digits 0, 1, . . . , 9 are much too close to their expected values to be considered randomly distributed.  In fact, χ2 ≥ 1.15 with probability 99.9 percent.  The same test applied to the first 10,000 digits of e gives the reasonable value χ2 = 8.61; but the fact that the first 2000 digits are so evenly distributed is still surprising. Does the same phenomenon occur in the representation of e to other bases? [See AMM 72  1965 , 483–500.] 7. [08] Apply the coupon collector’s test procedure  Algorithm C , with d = 3 and n = 7, to the sequence 1101221022120202001212201010201121. What lengths do the seven subsequences have?   cid:120  8. [M22] How many U’s need to be examined in the coupon collector’s test, on the  average, before n complete sets have been found by Algorithm C, assuming that the sequence is random? What is the standard deviation? [Hint: See Eq. 1.2.9– 28 .] 9. [M21] Generalize the coupon collector’s test so that the search stops as soon as w distinct values have been found, where w is a fixed positive integer less than or equal to d. What probabilities should be used in place of  6 ? 10. [M23] Solve exercise 8 for the more general coupon collector’s test described in exercise 9. 11. [00] The “runs up” in a particular permutation are displayed in  9 ; what are the “runs down” in that permutation? 12. [20] Let U0, U1, . . . , Un−1 be n distinct numbers. Write an algorithm that determines the lengths of all ascending runs in the sequence. When your algorithm terminates, COUNT[r] should be the number of runs of length r, for 1 ≤ r ≤ 5, and COUNT[6] should be the number of runs of length 6 or more. 13. [M23] Show that  16  is the number of permutations of p+ q +1 distinct elements having the pattern  15 .   3.3.2   cid:120  14. [M15] If we “throw away” the element that immediately follows a run, so that  EMPIRICAL TESTS  when Xj is greater than Xj+1 we start the next run with Xj+2, the run lengths are independent, and a simple chi-square test may be used  instead of the horribly compli- cated method derived in the text . What are the appropriate run-length probabilities for this simple run test? 15. [M10] In the maximum-of-t test, why are V t0, V t1, . . . , V t formly distributed between zero and one?   cid:120  16. [15] Mr. J. H. Quick  a student  wanted to perform the maximum-of-t test for  n−1 supposed to be uni-  77  several different values of t. a  Letting Zjt = max Uj, Uj+1, . . . , Uj+t−1 , he found a clever way to go from the sequence Z0 t−1 , Z1 t−1 , . . . , to the sequence Z0t, Z1t, . . . , using very little time and space. What was his bright idea? b  He decided to modify the maximum-of-t method so that the jth observation would be max Uj, . . . , Uj+t−1 ; in other words, he took Vj = Zjt instead of Vj = Z tj t as the text says. He reasoned that all of the Z’s should have the same distribution, so the test is even stronger if each Zjt, 0 ≤ j < n, is used instead of just every tth j , he one. But when he tried a chi-square equidistribution test on the values of V t got extremely high values of the statistic V, which got even higher as t increased. Why did this happen?  17. [M25] Given any numbers U0, . . . , Un−1, V0, . . . , Vn−1, let their mean values be    ¯u = 1  n  Uk,  0≤k<n    ¯v = 1  n  Vk.  0≤k<n  a  Let U′k = Uk − ¯u, V ′k = Vk − ¯v. Show that the correlation coefficient C given in Eq.  24  is equal to      2   U′kV ′k  U′k  0≤k<n  0≤k<n  0≤k<n  2.  V ′k  b  Let C = N D, where N and D denote the numerator and denominator of the expression in part  a . Show that N 2 ≤ D2, hence −1 ≤ C ≤ 1; and obtain a formula for the difference D2 − N 2. [Hint: See exercise 1.2.3–30.] c  If C = ±1, show that αUk + βVk = τ, 0 ≤ k < n, for some constants α, β, and τ,  not all zero.  18. [M20]  a  Show that if n = 2, the serial correlation coefficient  23  is always equal to −1  unless the denominator is zero .  b  Similarly, show that when n = 3, the serial correlation coefficient always equals − 1 2.  c  Show that the denominator in  23  is zero if and only if U0 = U1 = ··· = Un−1. 19. [M30]  J. P. Butler.  Let U0, . . . , Un−1 be independent random variables having the same distribution. Prove that the expected value of the serial correlation coeffi- cient  23 , averaged over all cases with nonzero denominator, is −1  n − 1 . 20. [HM41] Continuing the previous exercise, prove that the variance of  23  is equal to n2  n−1 2 n−2 −n3 E  U0−U1 4 D2  2 n−2 , where D is the denominator of  23  and E denotes the expected value over all cases with D ̸= 0. What is the asymptotic value of E  U0 − U1 4 D2  when each Uj is uniformly distributed? 21. [19] What value of f is computed by Algorithm P if it is presented with the permutation  1, 2, 9, 8, 5, 3, 6, 7, 0, 4 ?   78  RANDOM NUMBERS  3.3.2  22. [18] For what permutation of {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} will Algorithm P produce the value f = 1024? 23. [M22] Let ⟨Yn⟩ and ⟨Y ′n⟩ be integer sequences having period lengths λ and λ′, respectively, with 0 ≤ Yn, Y ′n < d; also let Zn =  Yn + Y ′n+r  mod d, where r is chosen at random between 0 and λ′ −1. Show that ⟨Zn⟩ passes the t-dimensional serial test at least as well as ⟨Yn⟩ does, in the following sense: Let P  x1, . . . , xt  and Q x1, . . . , xt  be the probabilities that the t-tuple  x1, . . . , xt  occurs in ⟨Yn⟩ and ⟨Zn⟩:  P  x1, . . . , xt  = 1  λ  λ−1 [ Yn, . . . , Yn+t−1  =  x1, . . . , xt ]; −1 λ−1 λ′  n=0  [ Zn, . . . , Zn+t−1  =  x1, . . . , xt ].   P  x1, . . . , xt  − d−t 2  .  Then   Q x1, . . . , xt  = 1 λλ′   Q x1, . . . , xt  − d−t 2 ≤   n=0  r=0   x1,...,xt    x1,...,xt   24. [HM37]  G. Marsaglia.  Show that the serial test on n overlapping t-tuples  Y1, Y2, . . . , Yt ,  Y2, Y3, . . . , Yt+1 , . . . ,  Yn, Y1, . . . , Yt−1  can be carried out as follows: For each string α = a1 . . . am with 0 ≤ ai < d, let N α  be the number of times α occurs as a substring of Y1Y2 . . . YnY1 . . . Ym−1, and let P  α  = P  a1  . . . P  am  be the probability that α occurs at any given position; individual digits may occur with differing probabilities P  0 , P  1 , . . . , P  d − 1 . Compute the statistic  V = 1  n  N α 2 P  α  − 1  n  N α 2 P  α  .    α=t    α=t−1  1 C2 C−1  1 , when C1 and C2 are the matrices defined  Then V should have the chi-square distribution with dt− dt−1 degrees of freedom when n is large. [Hint: Use exercise 3.3.1–25.] 1 ≈ −6C−1 25. [M46] Why is C−1 after  22 ? 26. [HM30] Let U1, U2, . . . , Un be independent uniform deviates in [0 . . 1 , and let U 1  ≤ U 2  ≤ ··· ≤ U n  be their values after sorting; also define the spacings S1 = U 2  − U 1 , . . . , Sn−1 = U n  − U n−1 , Sn = U 1  + 1 − U n  and sorted spacings S 1  ≤ ··· ≤ S n  as in the birthday spacings test. It is convenient in the following calculations to use the notation xn+ as an abbreviation for the expression xn[x≥ 0]. a  Given any real numbers s1, s2, . . . , sn, prove that the simultaneous inequalities S1 ≥ s1, S2 ≥ s2, . . . , Sn ≥ sn occur with probability  1 − s1 − s2 − ··· − sn n−1 + . b  Consequently the smallest spacing S 1  is ≤ s with probability 1 −  1 − ns n−1 + . c  What are the distribution functions Fk s  = Pr S k  ≤ s , for 1 ≤ k ≤ n?  cid:120  27. [HM26]  Iterated spacings.  In the notation of the previous exercise, show that d  Calculate the mean and variance of each S k . the numbers S′1 = nS 1 , S′2 =  n − 1  S 2  − S 1  , . . . , S′n = 1 S n  − S n−1   have the same joint probability distribution as the original spacings S1, . . . , Sn of n uniform deviates. Therefore we can sort them into order, S′ 1  ≤ ··· ≤ S′ n , and repeat this transformation to get yet another set of random spacings S′′1 , . . . , S′′n, etc. Each successive set of spacings S can be subjected to the Kolmogorov–Smirnov  , . . . , S   k  n   k  1   3.3.2  test, using  EMPIRICAL TESTS  79  +  K  n−1 = K−n−1 =  √  √  n − 1 max 1≤j<n n − 1 max 1≤j<n   j  n − 1 − S 1 + ··· + S S   k    k    k  j  1 − ··· − S j − j − 1 n − 1   k      ,  .      1 − 13α2 288n  in the birthday spacings test is r+1  Examine the transformation from  S1, . . . , Sn  to  S′1, . . . , S′n  in detail in the cases n = 2 and n = 3; explain why continued repetition of this process will break down eventually when it is applied to computer-generated numbers with finite precision.  One way to compare random number generators is to see how long they can continue to survive such a torture test.  28. [M26] Let bnrs m  be the number of n-tuples  y1, . . . , yn  with 0 ≤ yj < m that have exactly r equal spacings and s zero spacings. Thus, the probability that R = r s=0 bnrs m  mn. Also let pn m  be the number of partitions of m into at most n parts  exercise 5.1.1–15 .  a  Express bn00 m  in terms of partitions. [Hint: Consider cases with small m and n.]  b  Show that there is a simple relation between bnrs m  and b n−s  r+1−s 0 m  when s > 0.  c  Deduce an explicit formula for the probability that no spacings are equal. 29. [M35] Continuing exercise 28, find simple expressions for the generating functions  bnr z  = 30. [HM41] Continuing the previous exercises, prove that if m = n3 α we have pn m  = mn−1eα 4 n!  n − 1 ! for fixed α as n → ∞. Find a similar formula for qn m , the number of partitions of m into n distinct positive parts. Deduce the asymptotic probabilities that the birthday spacings test finds R equal to 0, 1, and 2, to within O 1 n .   cid:120  31. [M21] The recurrence Yn =  Yn−24 + Yn−55  mod 2, which describes the least  + 169α4 + 2016α3 − 1728α2 − 41472α  m≥0 bnr0 m zm m, when r = 0, 1, and 2.  half of the time. [Hint: Find the generating function79  significant bits of the lagged Fibonacci generator 3.2.2– 7  as well as the second-least significant bits of 3.2.2– 7′ , is known to have period length 255−1; hence every possible nonzero pattern of bits  Yn, Yn+1, . . . , Yn+54  occurs equally often. Nevertheless, prove that if we generate 79 consecutive random bits Yn, . . . , Yn+78 starting at a random point in the period, the probability is more than 51% that there are more 1s than 0s. If we use such bits to define a “random walk” that moves to the right when the bit is 1 and to the left when the bit is 0, we’ll finish to the right of our starting point significantly more than k=0 Pr Yn+···+Yn+78 = k  zk.] 32. [M20] True or false: If X and Y are independent, identically distributed random variables with mean 0, and if they are more likely to be positive than negative, then X + Y is more likely to be positive than negative. 33. [HM32] Find the asymptotic value of the probability that k + l consecutive bits generated by the recurrence Yn =  Yn−l + Yn−k  mod 2 have more 1s than 0s, when k > 2l and the period length of this recurrence is 2k − 1, assuming that k is large. 34. [HM29] Explain how to estimate the mean and variance of the number of two- letter combinations that do not occur consecutively in a random string of length n on an m-letter alphabet. Assume that m is large and n ≈ 2m2.   cid:120  35. [HM32]  J. H. Lindholm, 1968.  Suppose we generate random bits ⟨Yn⟩ using the  + O n−3   165888n2  recurrence  Yn =  a1Yn−1 + a2Yn−2 + ··· + akYn−k  mod 2 ,   80  RANDOM NUMBERS  3.3.2  for some choice of a1, . . . , ak such that the period length is 2k − 1; start with Y0 = 1 and Y1 = ··· = Yk−1 = 0. Let Zn =  −1 Yn+1 = 2Yn − 1 be a random sign, and consider the statistic Sm = Zn + Zn+1 + ··· + Zn+m−1, where n is a random point in the period. a  Prove that E Sm = m N, where N = 2k − 1. b  What is E S2 c  What would E Sm and E S2 d  Assuming that m ≤ N, prove that E S3  m? Assume that m ≤ N. Hint: See exercise 3.2.2–16.  m be if the Z’s were truly random?  m = m3 N − 6B N + 1  N, where  [ Yi+1Yi+2 . . . Yi+k−1 2 =  Yj+1Yj+2 . . . Yj+k−1 2]  m − j  .  B =   0<i<j<m  e  Evaluate B in the special case considered in exercise 31: m = 79 and Yn =   Yn−24 + Yn−55  mod 2. *3.3.3. Theoretical Tests Although it is always possible to test a random number generator using the methods in the previous section, it is far better to have a priori tests: theoretical results that tell us in advance how well those tests will come out. Such theoretical results give us much more understanding about the generation methods than empirical, trial-and-error results do. In this section we shall study the linear congruential sequences in more detail; if we know what the results of certain tests will be before we actually generate the numbers, we have a better chance of choosing a, m, and c properly.  The development of this kind of theory is quite difficult, although some progress has been made. The results obtained so far are generally for statistical tests made over the entire period. Not all statistical tests make sense when they are applied over a full period — for example, the equidistribution test will give results that are too perfect — but the serial test, gap test, permutation test, maximum test, etc., can be fruitfully analyzed in this way. Such studies will detect global nonrandomness of a sequence, that is, improper behavior in very large samples.  The theory we shall discuss is quite illuminating, but it does not eliminate the need for testing local nonrandomness by the methods of Section 3.3.2. Indeed, the task of proving anything useful about short subsequences appears to be very hard. Only a few theoretical results are known about the behavior of linear congruential sequences over less than a full period; they will be discussed at the end of Section 3.3.4.  See also exercise 18.   Let us begin with a proof of a simple a priori law, for the least complicated case of the permutation test. The gist of our first theorem is that we have Xn+1 < Xn about half the time, provided that the sequence has high potency. Theorem P. Let a, c, and m generate a linear congruential sequence with maximum period; let b = a − 1 and let d be the greatest common divisor of m and b. The probability that Xn+1 < Xn is equal to 1  2 + r, where  r =2 c mod d  − d 2m;   1   hence r < d 2m.   3.3.3  THEORETICAL TESTS  81  Proof. The proof of this theorem involves some techniques that are of interest in themselves. First we define  s x  =  ax + c  mod m.   2  Thus, Xn+1 = s Xn , and the theorem reduces to counting the number of integers x such that 0 ≤ x < m and s x  < x, since every such integer occurs somewhere in the period. We want to show that this number is  The function x − s x  m is equal to 1 when x > s x , and it is 0  otherwise; hence the count we wish to obtain can be written simply as  m + 2 c mod d  − d. − ax + c  ax + c =    − bx + c =    x  ax + c  0≤x<m   x − s x       0≤x<m     4    3   −  m  m  m  m  1 2  .  0≤x<m  m  m   Recall that ⌈−y⌉ = −⌊y⌋ and b = a − 1.  Such sums can be evaluated by the method of exercise 1.2.4–37, where we have proved that 2 + g⌊c g⌋,   hj + c  g = gcd h, k ,  + g − 1       5   2  k  0≤j<k  whenever h and k are integers and k > 0. Since a is relatively prime to m, this  =  h − 1  k − 1   ax + c  bx + c     m  formula yields   0≤x<m  0≤x<m  m  and  3  follows immediately.  =  a − 1  m − 1  =  b − 1  m − 1   2  2  + c, + d − 1  2 + c −  c mod d ,  The proof of Theorem P indicates that a priori tests can indeed be carried out, provided that we are able to deal satisfactorily with sums involving the ⌊ ⌋ and ⌈ ⌉ functions. In many cases the most powerful technique for dealing with floor and ceiling functions is to replace them by two somewhat more symmetrical operations:  δ x  = ⌊x⌋ + 1 − ⌈x⌉ = [x is an integer]; 2 δ x  = x − ⌈x⌉ + 1   x   = x − ⌊x⌋ − 1   6   7  The latter function is a “sawtooth” function familiar in the study of Fourier series; its graph is shown in Fig. 7. The reason for choosing to work with   x   rather than ⌊x⌋ or ⌈x⌉ is that   x   possesses several very useful properties:  2 δ x  = x − 1  2 − 1  2 + 1  2  ⌊x⌋ + ⌈x⌉.    −x   = −  x  ;   8    82  RANDOM NUMBERS  3.3.3    x+ n   =   x  ,    nx   =   x  +  Fig. 7. The sawtooth function   x  .   integer n; x+ 1    n  +···+   x+ n−1    ,  n   9   integer n ≥ 1.   10    See exercises 1.2.4–38 and 1.2.4–39 a,b,g .   In order to get some practice working with these functions, let us prove Theorem P again, this time without relying on exercise 1.2.4–37. With the help of Eqs.  7 ,  8 ,  9 , we can show that   x − s x     m  m  m  m  −     x − s x    x − s x   2 − 1 + 1  x −  ax + c   2 δ + 1  bx + c  2 + 1 sincex − s x  m is never an integer. Now 2   = x − s x  = x − s x  = x − s x   x − s x   −  +  m  m  m  m  = 0  0≤x<m  m   11   since both x and s x  take on each value of {0, 1, . . . , m− 1} exactly once; hence  11  yields     x − s x     0≤x<m  m  =   0≤x<m   bx + c    m  + m 2 .   12   Let b = b0d, m = m0d, where b0 and m0 are relatively prime. We know that  b0x  mod m0 takes on the values {0, 1, . . . , m0 − 1} in some order as x varies from 0 to m0 − 1. By  9  and  10  and the fact that     b x + m0  + c  bx + c   bx + c  bx + c      =  m  m  = d  0≤x<m  m  0≤x<m0  m  we have   +  1 2  0  − 1 2   3.3.3     c  THEORETICAL TESTS     c    .  d  83   13   = d  0≤x<m0  + b0x m0  m  = d  Theorem P follows immediately from  12  and  13 .  One consequence of Theorem P is that practically any choice of a and c will give a reasonable probability that Xn+1 < Xn, at least over the entire period, except those that have large d. A large value of d corresponds to low potency, and we already know that generators of low potency are undesirable.  The next theorem gives us a more stringent condition for the choice of the parameters a and c; we will consider the serial correlation test applied over the entire period. The quantity C defined in Section 3.3.2, Eq.  23 , is    x2 −  m  0≤x<m     0≤x<m  2  x  .  14       0≤x<m  C =  m  xs x  −  Let x′ be the element such that s x′  = 0. We have 2 [x̸= x′].   hj + c  + m  The formulas we are about to derive can be expressed most easily in terms of the sum   15   m  ,  k   16   an important function that arises in several mathematical problems. It is called a generalized Dedekind sum, since Richard Dedekind introduced the function σ h, k, 0  in 1876 when commenting on one of Riemann’s incomplete manuscripts. [See B. Riemann’s Gesammelte math. Werke, 2nd ed.  1892 , 466–478.]  Using the well-known formulas    0≤x<m  x = m m − 1   2  and   0≤x<m  2  m − 1  x2 = m m − 1 3  ,  x  0≤x<m     2  ax + c   j σ h, k, c  = 12   s x  = m  0≤j<k  k  it is a straightforward matter to transform Eq.  14  into C = mσ a, m, c  − 3 + 6 m − x′ − c   .  m2 − 1   See exercise 5.  Since m is usually very large, we may discard terms of order 1 m, and we have the approximation  C ≈ σ a, m, c  m, with an error of less than 6 m in absolute value.  The serial correlation test now reduces to determining the value of the Dedekind sum σ a, m, c . Evaluating σ a, m, c  directly from its definition  16  is hardly any easier than evaluating the correlation coefficient itself directly, but fortunately there are simple methods available for computing Dedekind sums quite rapidly.   17    18    84  RANDOM NUMBERS  3.3.3   19    20    22    23   Lemma B  “Reciprocity law” for Dedekind sums . Let h, k, c be integers. If 0 ≤ c < k, 0 < h ≤ k, and if h is relatively prime to k, then  σ h, k, c  + σ k, h, c  = h k  + k h  + 1  hk  + 6c2  hk  − 6  e h, c  = [c = 0] + [c mod h̸= 0].  where   − 3e h, c ,   c  h  Proof. We leave it to the reader to prove that, under these hypotheses, σ h, k, c  + σ k, h, c  = σ h, k, 0  + σ k, h, 0  + 6c2  See exercise 6.  The lemma now must be proved only in the case c = 0.  − 6  hk  h   c   − 3e h, c  + 3.  21   The proof we will give, based on complex roots of unity, is essentially due to L. Carlitz. There is actually a simpler proof that uses only elementary manipulations of sums  see exercise 7  — but the following method reveals more of the mathematical tools that are available for problems of this kind and it is therefore much more instructive.  Let f x  and g x  be polynomials defined as follows:  f x  = 1 + x + ··· + xk−1 =  xk − 1   x − 1  g x  = x + 2x2 + ··· +  k − 1 xk−1  = xf′ x  = kxk  x − 1  − x xk − 1   x − 1 2.  If ω is the complex kth root of unity e2πi k, we have by Eq. 1.2.9– 13   ω−jrg ωjx  = rxr,  if 0 ≤ r < k.    0≤j<k  1 k  Set x = 1; then g ωjx  = k  ωj − 1  if j ̸= 0, otherwise it equals k k − 1  2. Therefore  2 k − 1 ,  Eq.  23  shows that the right-hand side equals r when 0 ≤ r < k, and it is unchanged when multiples of k are added to r. Hence + 1 2 δ  ω−jr ωj − 1 − 1 2k  if r is an integer.  ω−jr ωj − 1 + 1   r mod k =   r    r  = 1     24   0<j<k  k  k  k  .  0<j<k  This important formula, which holds whenever r is an integer, allows us to reduce many calculations involving   r k   to sums involving kth roots of unity, and it brings a whole new range of techniques into the picture. In particular, we get the following formula when h ⊥ k: = 12 k2  σ h, k, 0  + 3 k − 1   ω−jhr ωj − 1 .  ω−ir ωi − 1         25   k2  0<r<k  0<i<k  0<j<k   n  j=1  THEORETICAL TESTS  85  3.3.3  on r; we have   reduces to  The right-hand side of this formula may be simplified by carrying out the sum 0≤r<k ωrs = f ωs  = 0 if s mod k ̸= 0. Equation  25  now  σ h, k, 0  + 3 k − 1   k  = 12  k  1   ω−jh − 1  ωj − 1  .   26     0<j<k  A similar formula is obtained for σ k, h, 0 , with ζ = e2πi h replacing ω.  It is not obvious what we can do with the sum in  26 , but there is an elegant way to proceed, based on the fact that each term of the sum is a function of ωj, where 0 < j < k; hence the sum is essentially taken over the kth roots of unity other than 1. Whenever x1, x2, . . . , xn are distinct complex numbers, we have the identity   xj − x1  . . .  xj − xj−1  x − xj  xj − xj+1  . . .  xj − xn  1  1  =   x − x1  . . .  x − xn  ,   27   which follows from the usual method of expanding the right-hand side into partial fractions. Moreover, if q x  =  x − y1  x − y2  . . .  x − ym , we have q′ yj  =  yj − y1  . . .  yj − yj−1  yj − yj+1  . . .  yj − ym ;   28  this identity may often be used to simplify expressions like those in the left- hand side of  27 . When h and k are relatively prime, the numbers ω, ω2, . . . , ωk−1, ζ, ζ2, . . . , ζ h−1 are all distinct; we can therefore consider formula  27  in the special case of the polynomial  x − ω  . . .  x − ωk−1  x − ζ  . . .  x − ζ h−1  =  xk − 1  xh − 1   x − 1 2, obtaining the following identity in x:  ζ j ζ j −1 2  ζ jk −1  x− ζ j  + 1  k  ωj ωj −1 2   ωjh−1  x− ωj  =   x−1 2   xh−1  xk −1  .   29     0<j<k  This identity has many interesting consequences, and it leads to numerous reci- procity formulas for sums of the type given in Eq.  26 . For example, if we differentiate  29  twice with respect to x and let x → 1, we find that  2 − 1 + 1 2h Replace j by h − j and by k − j in these sums and use  26  to get  hk  k  − 1 2k  .  ζ j ζ j − 1 2   ζ jk − 1  1 − ζ j 3 + 2  k   σ k, h, 0  + 3 h − 1     h  + 1 6    ωj ωj − 1 2   h  0<j<k  + k h   ωjh − 1  1 − ωj 3 + 1 = 1 6  + 1   σ h, k, 0  + 3 k − 1  k + k h   h  = 1 6  k  hk        0<j<h    0<j<h  1 h  2 h  1 6  2 − 1 + 1 2h  − 1 2k  ,  which is equivalent to the desired result.   86  RANDOM NUMBERS  3.3.3  Lemma B gives us an explicit function f h, k, c  such that   30  whenever 0 < h ≤ k, 0 ≤ c < k, and h is relatively prime to k. From the definition  16  it is clear that  σ h, k, c  = f h, k, c  − σ k, h, c    31  Therefore we can use  30  iteratively to evaluate σ h, k, c , using a process that reduces the parameters as in Euclid’s algorithm.  σ k, h, c  = σ k mod h, h, c mod h .  Further simplifications occur when we examine this iterative procedure more  closely. Let us set m1 = k, m2 = h, c1 = c, and form the following tableau:  m1 = a1m2 + m3 m2 = a2m3 + m4 m3 = a3m4 + m5 m4 = a4m5 aj = ⌊mj mj+1⌋, mj+2 = mj mod mj+1,  c1 = b1m2 + c2 c2 = b2m3 + c3 c3 = b3m4 + c4 c4 = b4m5 + c5 bj = ⌊cj mj+1⌋, cj+1 = cj mod mj+1,   32    33   Here  and it follows that  0 ≤ cj < mj.  0 ≤ mj+1 < mj,   34  We have assumed for convenience that Euclid’s algorithm terminates in  32  after four iterations; this assumption will reveal the pattern that holds in the general case. Since h and k were relatively prime to start with, we must have m5 = 1 and c5 = 0 in  32 . Let us assume also that c3 ̸= 0 but c4 = 0, in order to get a feeling for the  effect this has on the recurrence. Equations  30  and  31  yield  σ h, k, c  = σ m2, m1, c1   = f m2, m1, c1  − σ m3, m2, c2  = ··· = f m2, m1, c1  − f m3, m2, c2  + f m4, m3, c3  − f m5, m4, c4 .  The first part, h k + k h, of the formula for f h, k, c  in  19  contributes  m2 m1  + m1 m2  − m3 m2  − m2 m3  + m4 m3  + m3 m4  − m5 m4  − m4 m5  to the total, and this simplifies to  + m1 − m3  − m2 − m4  + m3 − m5  m2  m3  m4  h k  − m4 m5  = h k  + a1 − a2 + a3 − a4.  The next part of  19 , 1 hk, also leads to a simple contribution; according to Eq. 4.5.3– 9  and other formulas in Section 4.5.3, we have = h′  − 1,   35   1  − 1 m2m3  + 1 m3m4  − 1 m4m5  m1m2  k   3.3.3 where h′ is the unique integer satisfying  THEORETICAL TESTS  87  k     c2  h′h ≡ 1  modulo k ,   36  Adding up all the contributions, and remembering our assumption that c4 = 0  so that e m4, c3  = 0, see  20 , we find that σ h, k, c  = h + h′  +  a1 − a2 + a3 − a4  − 6 b1 − b2 + b3 − b4   0 < h′ ≤ k.  2  1  4  3  + 6  + 2,  m1m2  − c2 m2m3  + c2 m3m4  − c2 m4m5 in terms of the assumed tableau  32 . Similar results hold in general: Theorem D. Let h, k, c be integers with 0 < h ≤ k, 0 ≤ c < k, and h relatively prime to k. Form the “Euclidean tableau” as defined in  33  above, and assume that the process stops after t steps with mt+1 = 1. Let s be the smallest subscript such that cs = 0, and let h′ be defined by  36 . Then σ h, k, c  = h + h′  +     − 2 +  −1 t. + 3 −1 s + δs1  aj − 6bj + 6   −1 j+1  mjmj+1    1≤j≤t  c2  k  j  Euclid’s algorithm is analyzed carefully in Section 4.5.3; the quantities a1, a2, . . . , at are called the partial quotients of h k. Theorem 4.5.3F tells us that the number of iterations, t, will never exceed logϕ k; hence Dedekind sums can be evaluated rapidly. The terms c2 j  mjmj+1 can be simplified further, and an efficient algorithm for evaluating σ h, k, c  appears in exercise 17.  Now that we have analyzed generalized Dedekind sums, let us apply our  knowledge to the determination of serial correlation coefficients. Example 1. Find the serial correlation when m = 235, a = 234 + 1, c = 1. Solution. We have  C =235σ 234 + 1, 235, 1  − 3 + 6 235 −  234 − 1  − 1   270 − 1 ,  by Eq.  17 . To evaluate σ 234 + 1, 235, 1 , we can form the tableau  m1 = 235 m2 = 234 + 1 m3 = 234 − 1 m4 = 2 m5 = 1  a1 = 1 a2 = 1 a3 = 233 − 1 a4 = 2  c1 = 1 c2 = 1 c3 = 1 c4 = 1 c5 = 0  b1 = 0 b2 = 0 b3 = 0 b4 = 1  Since h′ = 234 + 1, the value according to Theorem D comes to 233 − 3 + 2−32. Thus  C =  268 + 5   270 − 1  = 1   37  Such a correlation is much, much too high for randomness. Of course, this generator has very low potency, and we have already rejected it as nonrandom.  ϵ < 2−67.  4 + ϵ,   88  RANDOM NUMBERS  3.3.3  Example 2. Find the approximate serial correlation when m = 1010, a = 10001, c = 2113248653. Solution. We have C ≈ σ a, m, c  m, and the computation proceeds as follows:  c1 = 2113248653 7350 c2 = 50 c3 = c4 = 0  m1 = 10000000000 10001 m2 = 100 m3 = m4 = 1  C ≈ −3 · 10−9.  a1 = 999900 100 a2 = a3 = 100  σ m2, m1, c1  = −31.6926653544;  b1 = 211303 73 b2 = 50 b3 =  38  This is a very respectable value of C indeed. But the generator has a potency of only 3, so it is not really a very good source of random numbers in spite of the fact that it has low serial correlation. It is necessary to have a low serial correlation, but not sufficient. Example 3. Estimate the serial correlation for general a, m, and c. Solution. If we consider just one application of  30 , we have − σ m, a, c .  σ a, m, c  ≈ m a  − 6 c a  Now σ m, a, c  < a by exercise 12, and therefore  C ≈ σ a, m, c   m  1 − 6 c m  + 6  m   39   The error in this approximation is less than  a + 6  m in absolute value.  The estimate in  39  was the first theoretical result known about the random- ness of congruential generators. R. R. Coveyou [JACM 7  1960 , 72–74] obtained it by averaging over all real numbers x between 0 and m instead of considering only the integer values  see exercise 21 ; then Martin Greenberger [Math. Comp. 15  1961 , 383–389] gave a rigorous derivation including an estimate of the error term.  So began one of the saddest chapters in the history of computer science! Although the approximation above is quite correct, it has been grievously mis- applied in practice; people abandoned the perfectly good generators they had been using and replaced them by terrible generators that looked good from the standpoint of  39 . For more than a decade, the most common random number generators in daily use were seriously deficient, solely because of a theoretical advance.  am  + 6 c2   ≈ 1  a   c  2  .  A little Learning is a dang’rous Thing. — ALEXANDER POPE, An Essay on Criticism, 215  1711   If we are to learn by past mistakes, we had better look carefully at how  39  has been misused. In the first place people assumed uncritically that a small serial correlation over the whole period would be a pretty good guarantee of   3.3.3  THEORETICAL TESTS  89  6  3,  √  √  2 ± 1  randomness; but in fact it doesn’t even ensure a small serial correlation for 1000 consecutive elements of the sequence  see exercise 14 . Secondly,  39  and its error term will ensure a relatively small value of C only when a ≈ √ m. In fact, we shall see that nearly all multipliers give a value of C that is substantially less √ than 1  m, hence  39  is not a very good approximation to the true behavior. Minimizing a crude upper bound for C does not minimize C.  m; therefore people suggested choosing multipliers near  c m ≈ 1  In the third place, people observed that  39  yields its best estimate when  40  since these values are the roots of 1− 6x + 6x2 = 0. “In the absence of any other criterion for choosing c, we might as well use this one.” The latter statement is not incorrect, but it is misleading at best, since experience has shown that the value of c has hardly any influence on the true value of the serial correlation when a is a good multiplier; the choice  40  reduces C substantially only in cases like Example 2 above. And we are fooling ourselves in such cases, since the bad multiplier will reveal its deficiencies in other ways.  Clearly we need a better estimate than  39 ; and such an estimate is now available thanks to Theorem D, which stems principally from the work of Ulrich Dieter [Math. Comp. 25  1971 , 855–883]. Theorem D implies that σ a, m, c  will be small if the partial quotients of a m are small. Indeed, by analyzing generalized Dedekind sums still more closely, it is possible to obtain quite a sharp estimate: Theorem K. Under the assumptions of Theorem D, we always have aj − 1 2 .  aj ≤ σ h, k, c  ≤   aj −   aj + 1 2      −1 2   41   1≤j≤t j odd  1≤j≤t j even  1≤j≤t j odd  1≤j≤t j even  Proof. See D. E. Knuth, Acta Arithmetica 33  1977 , 297–325, where it is shown further that these bounds are essentially the best possible when large partial quotients are present. Example 4. Estimate the serial correlation for a = 3141592621, m = 235, c odd. Solution. The partial quotients of a m are 10, 1, 14, 1, 7, 1, 1, 1, 3, 3, 3, 5, 2, 1, 8, 7, 1, 4, 1, 2, 4, 2; hence by Theorem K  −55 ≤ σ a, m, c  ≤ 67.5,  and the serial correlation is guaranteed to be extremely low for all c.  Note that this bound is considerably better than we could obtain from  39 , since the error in  39  is of order a m; our “random” multiplier has turned out to be much better than one specifically chosen to look good on the basis of  39 . j=1 aj, taken over all  In fact, it is possible to show that the average value oft   3.3.3  90  RANDOM NUMBERS  multipliers a relatively prime to m, is  π2  ln m 2 + O log m  log log m 4  6  large t   see exercise 4.5.3–35 . Therefore the probability that a random multiplier has j=1 aj, say larger than  log m 2+ϵ for some fixed ϵ > 0, approaches zero as m → ∞. This substantiates the empirical evidence that almost all linear congruential sequences have extremely low serial correlation over the entire period.  The exercises below show that other a priori tests, such as the serial test over the entire period, can also be expressed in terms of a few generalized Dedekind sums. It follows from Theorem K that linear congruential sequences will pass those tests provided that certain specified fractions  depending on a and m but not on c  have small partial quotients. In particular, the result of exercise 19 implies that the serial test on pairs will be passed satisfactorily if and only if a m has no large partial quotients.  The book Dedekind Sums by Hans Rademacher and Emil Grosswald  Math. Assoc. of America, Carus Monograph No. 16, 1972  discusses the history and properties of Dedekind sums and their generalizations. Further theoretical tests, including the serial test in higher dimensions, are discussed in Section 3.3.4.  EXERCISES — First Set 1. [M10] Express x mod y in terms of the sawtooth and δ functions. 2. [HM22] What is the Fourier series expansion  in terms of sines and cosines  of the function   x  ?  3. [M23]  N. J. Fine.  Prove that n−1  cid:120  4. [M19] If m = 1010, what is the highest possible value of d  in the notation of 2   < 1 for all real numbers x.  k=0  2kx + 1  Theorem P , given that the potency of the generator is 10? 5. [M21] Carry out the derivation of Eq.  17 . 6. [M27] Assume that hh′ + kk′ = 1. a  Show, without using Lemma B, that   h′j σ h, k, c  = σ h, k, 0  + 12    j   h′j   k′j    0<j<c  k  for all integers c ≥ 0. +    = j hk  − 1 2 δ   h′c    k  + 6  k  b  Show that c  Under the assumptions of Lemma B, prove Eq.  21 .   cid:120  7. [M24] Give a proof of the reciprocity law  19 , when c = 0, by using the general  cid:120  8. [M34]  L. Carlitz.  Let  reciprocity law of exercise 1.2.4–45.  if 0 < j < k.  h  h  ρ p, q, r  = 12   0≤j<r   jp   jq    r  r  .   3.3.3  THEORETICAL TESTS  91  By generalizing the method of proof used in Lemma B, prove the following beautiful identity due to H. Rademacher: If each of p, q, r is relatively prime to the other two,  ρ p, q, r  + ρ q, r, p  + ρ r, p, q  = p qr  + q rp  + r pq  − 3.   The reciprocity law for Dedekind sums, with c = 0, is the special case r = 1.  9. [M40] Is there a simple proof of Rademacher’s identity  exercise 8  along the lines of the proof in exercise 7 of a special case? 10. [M20] Show that when 0 < h < k it is possible to express σ k − h, k, c  and σ h, k,−c  easily in terms of σ h, k, c . 11. [M30] The formulas given in the text show us how to evaluate σ h, k, c  when h and k are relatively prime and c is an integer. For the general case, prove that a  σ dh, dk, dc  = σ h, k, c , for integer d > 0; b  σ h, k, c + θ  = σ h, k, c  + 6  h′c k  , for integer c, real 0 < θ < 1, h ⊥ k, and  hh′ ≡ 1  modulo k .  12. [M24] Show that if h is relatively prime to k and c is an integer, σ h, k, c  ≤  k − 1  k − 2  k. 13. [M24] Generalize Eq.  26  so that it gives an expression for σ h, k, c .   cid:120  14. [M20] The linear congruential generator that has m = 235, a = 218 + 1, c = 1,  was given the serial correlation test on three batches of 1000 consecutive numbers, and the result was a very high correlation, between 0.2 and 0.3, in each case. What is the serial correlation of this generator, taken over all 235 numbers of the period? 15. [M21] Generalize Lemma B so that it applies to all real values of c, 0 ≤ c < k. 16. [M24] Given the Euclidean tableau defined in  33 , let p0 = 1, p1 = a1, and pj = ajpj−1 + pj−2 for 1 < j ≤ t. Show that the complicated portion of the sum in Theorem D can be rewritten as follows, making it possible to avoid noninteger computations: [Hint: Prove that  1≤j≤r −1 j+1 mjmj+1 =  −1 r+1pr−1 m1mr+1 for 1 ≤ r ≤ t.]  bj cj + cj+1 pj−1.  = 1 m1  17. [M22] Design an algorithm that evaluates σ h, k, c  for integers h, k, c satisfying the hypotheses of Theorem D. Your algorithm should use only integer arithmetic  of unlimited precision , and it should produce the answer in the form A + B k where A and B are integers.  See exercise 16.  If possible, use only a finite number of variables for temporary storage, instead of maintaining arrays such as a1, a2, . . . , at.   cid:120  18. [M23]  U. Dieter.  Given positive integers h, k, z, let  hj + c   S h, k, c, z  =    −1 j+1   −1 j+1    mjmj+1  1≤j≤t  1≤j≤t  c2  .  j  0≤j<z  k  Show that this sum can be evaluated in closed form, in terms of generalized Dedekind sums and the sawtooth function. [Hint: When z ≤ k, the quantity ⌊j k⌋ − ⌊ j − z  k⌋ equals 1 for 0 ≤ j < z, and it equals 0 for z ≤ j < k, so we can introduce this factor and sum over 0 ≤ j < k.]   92  RANDOM NUMBERS   cid:120  19. [M23] Show that the serial test can be analyzed over the full period, in terms of  generalized Dedekind sums, by finding a formula for the probability that α ≤ Xn < β and α′ ≤ Xn+1 < β′ when α, β, α′, β′ are given integers with 0 ≤ α < β ≤ m and 0 ≤ α′ < β′ ≤ m. [Hint: Consider the quantity ⌊ x − α  m⌋ − ⌊ x − β  m⌋.] 20. [M29]  U. Dieter.  Extend Theorem P by obtaining a formula for the probability that Xn > Xn+1 > Xn+2, in terms of generalized Dedekind sums.  3.3.3  C =   1   1  x{ax + θ} dx −  EXERCISES — Second Set In many cases, exact computations with integers are quite difficult to carry out, but we can attempt to study the probabilities that arise when we take the average over all real values of x instead of restricting the calculation to integer values. Although these results are only approximate, they shed some light on the subject. It is convenient to deal with numbers Un between zero and one; for linear congru- ential sequences, Un = Xn m, and we have Un+1 = {aUn + θ}, where θ = c m and {x} denotes x mod 1. For example, the formula for serial correlation now becomes  2  cid:120  21. [HM23]  R. R. Coveyou.  What is the value of C in the formula just given?  cid:120  22. [M22] Let a be an integer, and let 0 ≤ θ < 1. If x is a random real number,  2 1  uniformly distributed between 0 and 1, and if s x  = {ax + θ}, what is the probability that s x  < x?  This is the “real number” analog of Theorem P.  23. [M28] The previous exercise gives the probability that Un+1 < Un. What is the probability that Un+2 < Un+1 < Un, assuming that Un is a random real number between zero and one? 24. [M29] Under the assumptions of the preceding problem, except with θ = 0, show   that Un > Un+1 > ··· > Un+t−1 occurs with probability 1 + t − 2   1 + 1   1  dx −    x dx  x dx  . . .  x  2  0  0  0  0  .  .  a  a  1 t!  What is the average length of a descending run starting at Un, assuming that Un is selected at random between zero and one?   cid:120  25. [M25] Let α, β, α′, β′ be real numbers with 0 ≤ α < β ≤ 1, 0 ≤ α′ < β′ ≤ 1.  Under the assumptions of exercise 22, what is the probability that α ≤ x < β and α′ ≤ s x  < β′?  This is the “real number” analog of exercise 19.  26. [M21] Consider a “Fibonacci” generator, where Un+1 = {Un + Un−1}. Assuming that U1 and U2 are independently chosen at random between 0 and 1, find the proba- bility that U1 < U2 < U3, U1 < U3 < U2, U2 < U1 < U3, etc. [Hint: Divide the unit square { x, y   0 ≤ x, y < 1} into six parts, depending on the relative order of x, y, and {x + y}, and determine the area of each part.] 27. [M32] In the Fibonacci generator of the preceding exercise, let U0 and U1 be cho- sen independently in the unit square except that U0 > U1. Determine the probability that U1 is the beginning of an upward run of length k, so that U0 > U1   Uk+1. Compare this with the corresponding probabilities for a random sequence. 28. [M35] According to Eq. 3.2.1.3– 5 , a linear congruential generator with potency 2 satisfies the condition Xn−1−2Xn+Xn+1 ≡  a−1 c  modulo m . Consider a generator   3.3.4 93 that abstracts this situation: Let Un+1 = {α + 2Un − Un−1}. As in exercise 26, divide the unit square into parts that show the relative order of U1, U2, and U3 for each pair  U1, U2 . Are there any values of α for which all six possible orders are achieved with probability 1  6, assuming that U1 and U2 are chosen at random in the unit square?  THE SPECTRAL TEST  3.3.4. The Spectral Test In this section we shall study an especially important way to check the quality of linear congruential random number generators. Not only do all good generators pass this test, all generators now known to be bad actually fail it. Thus it is by far the most powerful test known, and it deserves particular attention. Our discussion will also bring out some fundamental limitations on the degree of randomness that we can expect from linear congruential sequences and their generalizations.  The spectral test embodies aspects of both the empirical and theoretical tests studied in previous sections: It is like the theoretical tests because it deals with properties of the full period of the sequence, and it is like the empirical tests because it requires a computer program to determine the results. A. Ideas underlying the test. The most important randomness criteria seem to rely on properties of the joint distribution of t consecutive elements of the sequence, and the spectral test deals directly with this distribution. If we have a sequence ⟨Un⟩ of period m, the basic idea is to analyze the set of all m points  1   {  Un, Un+1, . . . , Un+t−1   0 ≤ n < m}  in t-dimensional space. For simplicity we shall assume that we have a linear congruential sequence  X0, a, c, m  of maximum period length m  so that c ̸= 0 , or that m is prime and c = 0 and the period length is m − 1. In the latter case we shall add the point  0, 0, . . . , 0  to the set  1 , so that there are always m points in all; this extra point has a negligible effect when m is large, and it makes the theory much simpler. Under these assumptions,  1  can be rewritten as  x, s x , s s x  , . . . , s[t−1] x  0 ≤ x < m    1   2   ,  m  where  s x  =  ax + c  mod m   3  is the successor of x. We are considering only the set of all such points in t dimensions, not the order in which those points are actually generated. But the order of generation is reflected in the dependence between components of the vectors; and the spectral test studies such dependence for various dimensions t by dealing with the totality of all points  2 .  For example, Fig. 8 shows a typical small case in 2 and 3 dimensions, for  the generator with  s x  =  137x + 187  mod 256.   4    94  RANDOM NUMBERS  3.3.4   a   Fig. 8.  a  The two-dimensional grid formed by all pairs of suc- cessive points  Xn, Xn+1 , when Xn+1 =  137Xn + 187  mod 256.  b  The three-dimensional grid of triplets  Xn, Xn+1, Xn+2 .   b   Of course a generator with period length 256 will hardly be random, but 256 is small enough that we can draw the diagram and gain some understanding before we turn to the larger m’s that are of practical interest.  Perhaps the most striking thing about the pattern of boxes in Fig. 8 a  is that we can cover them all by a fairly small number of parallel lines; indeed, there are many different families of parallel lines that will hit all the points. For example, a set of 20 nearly vertical lines will do the job, as will a set of 21 lines that tilt upward at roughly a 30◦ angle. We commonly observe similar patterns when driving past farmlands that have been planted in a systematic manner.  If the same generator is considered in three dimensions, we obtain 256 points in a cube, obtained by appending a “height” component s s x   to each of the  256 pointsx, s x  in the plane of Fig. 8 a , as shown in Fig. 8 b . Let’s imagine  that this 3-D crystal structure has been made into a physical model, a cube that we can turn in our hands; as we rotate it, we will notice various families of parallel planes that encompass all of the points. In the words of Wallace Givens, the random numbers stay “mainly in the planes.”  At first glance we might think that such systematic behavior is so nonrandom as to make congruential generators quite worthless; but more careful reflection, remembering that m is quite large in practice, provides a better insight. The regular structure in Fig. 8 is essentially the “grain” we see when examining our random numbers under a high-power microscope. If we take truly random numbers between 0 and 1, and round or truncate them to finite accuracy so that each is an integer multiple of 1 ν for some given number ν, then the t- dimensional points  1  we obtain will have an extremely regular character when viewed through a microscope.  of parallel straight lines that cover the pointsx m, s x  m in two dimen-  Let 1 ν2 be the maximum distance between lines, taken over all families  sions. We shall call ν2 the two-dimensional accuracy of the random number  s x   x  s s x    x  s x    3.3.4  THE SPECTRAL TEST  95  generator, since the pairs of successive numbers have a fine structure that is essentially good to one part in ν2. Similarly, let 1 ν3 be the maximum distance between planes, taken over all families of parallel planes that cover all points  x m, s x  m, s s x   m; we shall call ν3 the accuracy in three dimensions. that cover all pointsx m, s x  m, . . . , s[t−1] x  m.  The t-dimensional accuracy νt is the reciprocal of the maximum distance between hyperplanes, taken over all families of parallel  t − 1 -dimensional hyperplanes  The essential difference between periodic sequences and truly random se- quences that have been truncated to multiples of 1 ν is that the accuracy of truly random sequences is the same in all dimensions, while that of periodic sequences decreases as t increases. Indeed, since there are only m points in the t-dimensional cube when m is the period length, we can’t achieve a t-dimensional accuracy of more than about m1 t.  When the independence of t consecutive values is considered, computer- generated random numbers will behave essentially as if we took truly random numbers and truncated them to lg νt bits, where νt decreases with increasing t. In practice, such varying accuracy is usually all we need. We don’t insist that the 10-dimensional accuracy be 232, in the sense that all  232 10 possible 10-tuples  Un, Un+1, . . . , Un+9  should be equally likely on a 32-bit machine; for such large values of t we want only a few of the leading bits of  Un, Un+1, . . . , Un+t−1  to behave as if they were independently random.  On the other hand when an application demands high resolution of the random number sequence, simple linear congruential sequences will necessarily be inadequate. A generator with longer period should be used instead, even though only a small fraction of the period will actually be generated. Squaring the period length will essentially square the accuracy in higher dimensions; that is, it will double the effective number of bits of precision. The spectral test is based on the values of νt for small t, say 2 ≤ t ≤ 6. Dimensions 2, 3, and 4 seem to be adequate to detect important deficiencies in a sequence, but since we are considering the entire period it is wise to be somewhat cautious and go up into another dimension or two; on the other hand the values of νt for t ≥ 10 seem to be of no practical significance whatever.  This is fortunate, because it appears to be rather difficult to calculate the accuracy νt precisely when t ≥ 10.   There is a vague relation between the spectral test and the serial test; for example, a special case of the serial test, taken over the entire period as in exercise 3.3.3–19, counts the number of boxes in each of 64 subsquares of Fig. 8 a . The main difference is that the spectral test rotates the dots so as to discover the least favorable orientation. We shall return to the serial test later in this section. It may appear at first that we should apply the spectral test only for one suitably high value of t; if a generator passes the test in three dimensions, it seems plausible that it should also pass the 2-D test, hence we might as well omit the latter. The fallacy in this reasoning occurs because we apply more stringent conditions in lower dimensions. A similar situation occurs with the serial test:   96  RANDOM NUMBERS  3.3.4  4 × 1 8 × 1  Consider a generator that  quite properly  has almost the same number of points in each subcube of the unit cube, when the unit cube has been divided into 64 4 × 1 subcubes of size 1 4; this same generator might yield completely empty subsquares of the unit square, when the unit square has been divided into 64 subsquares of size 1 8. Since we increase our expectations in lower dimensions, a separate test for each dimension is required. It is not always true that νt ≤ m1 t, although this upper bound is valid when 274 > 256 in Fig. 8, because a nearly hexagonal structure brings the m points closer  the points form a rectangular grid. For example, it turns out that ν2 = √ together than would be possible in a strictly rectangular arrangement.  In order to develop an algorithm that computes νt efficiently, we must look more deeply at the associated mathematical theory. Therefore a reader who is not mathematically inclined is advised to skip to part D of this section, where the spectral test is presented as a “plug-in” method accompanied by several examples. But the mathematics behind the spectral test requires only some elementary manipulations of vectors.  √  Some authors have suggested using the minimum number Nt of parallel covering lines or hyperplanes as the criterion, instead of the maximum distance 1 νt between them. However, this number Nt does not appear to be as important as the concept of accuracy defined above, because it is biased by how nearly the slope of the lines or hyperplanes matches the coordinate axes of the cube. For example, the 20 nearly vertical lines that cover all the points of Fig. 8 a  √ are actually 1  328 units apart, according to Eq.  14  below with  u1, u2  =  18,−2 ; this might falsely imply an accuracy of one part in 328, or perhaps even an accuracy of one part in 20. The true accuracy of only one part in 274 is realized only for the larger family of 21 lines with a slope of 7 15; another family of 24 lines, with a slope of −11 13, also has a greater inter-line distance than the 20-line family, since 1  328. The precise way in which families of lines act at the boundaries of the unit hypercube does not seem to be an especially “clean” or significant criterion. However, for those people who prefer to count hyperplanes, it is possible to compute Nt using a method quite similar to the way in which we shall calculate νt  see exercise 16 . *B. Theory behind the test. with the observation that s[j] x  =   ajx +  1 + a + ··· + aj−1 c  In order to analyze the basic set  2 , we start  √ 290 > 1   mod 1.     5   √  √  √  1 m  m  We can get rid of the “mod 1” operation by extending the set periodically, making infinitely many copies of the original t-dimensional hypercube, proceeding in all directions. This gives us the set  s x  m  + k2, . . . ,  + k1,  ax m  + k2, . . . ,  s[t−1] x   + kt  + kt  m at−1x m   integer x, k1, k2, . . . , kt   integer x, k1, k2, . . . , kt   ,   x   m  =  V0 +  L =  + k1,   x  m   3.3.4  where  THE SPECTRAL TEST  97  0, c,  1 + a c, . . . ,  1 + a + ··· + at−2 c  V0 = 1  m   6  is a constant vector. The variable k1 is redundant in this representation of L, because we can change  x, k1, k2, . . . , kt  to  x+k1m, 0, k2−ak1, . . . , kt−at−1k1 , reducing k1 to zero without loss of generality. Therefore we obtain the compara- tively simple formula  L = {V0 + y1V1 + y2V2 + ··· + ytVt  integer y1, y2, . . . , yt},   7   where  m   1, a, a2, . . . , at−1 ;  V1 = 1 V2 =  0, 1, 0, . . . , 0 ,  V3 =  0, 0, 1, . . . , 0 ,   8   9  The points  x1, x2, . . . , xt  of L that satisfy 0 ≤ xj < 1 for all j are precisely the m points of our original set  2 . Notice that the increment c appears only in V0, and the effect of V0 is merely to shift all elements of L without changing their relative distances; hence c does not affect the spectral test in any way, and we might as well assume that V0 =  0, 0, . . . , 0  when we are calculating νt. When V0 is the zero vector we have a lattice of points  . . . , Vt =  0, 0, 0, . . . , 1 .  L0 = {y1V1 + y2V2 + ··· + ytVt  integer y1, y2, . . . , yt},   10  and our goal is to study the distances between adjacent  t − 1 -dimensional hyperplanes, in families of parallel hyperplanes that cover all the points of L0. A family of parallel  t − 1 -dimensional hyperplanes can be defined by a nonzero vector U =  u1, . . . , ut  that is perpendicular to all of them; and the set of points on a particular hyperplane is then  { x1, . . . , xt   x1u1 + ··· + xtut = q},   11  where q is a different constant for each hyperplane in the family. In other words, each hyperplane is the set of all vectors X for which the dot product X· U has a given value q. In our case the hyperplanes are all separated by a fixed distance, and one of them contains  0, 0, . . . , 0 ; hence we can adjust the magnitude of U so that the set of all integer values q gives all the hyperplanes in the family. Then the distance between neighboring hyperplanes is the minimum distance from  0, 0, . . . , 0  to the hyperplane for q = 1, namely   x1u1 + ··· + xtut = 1   .  min  real x1,...,xt  1 + ··· + x2 x2  t  Cauchy’s inequality  see exercise 1.2.3–30  tells us that t    u2   x1u1 + ··· + xtut 2 ≤  x2  1 + ··· + x2  hence the minimum in  12  occurs when each xj = uj  u2 between neighboring hyperplanes is  1 + ··· + u2 t  ,  13  1+···+u2 t  ; the distance    1   12    14   1 + ··· + u2 u2  t = 1 length U .   98  RANDOM NUMBERS  3.3.4  In other words, the quantity νt that we seek is precisely the length of the shortest vector U that defines a family of hyperplanes {X· U = q  integer q} containing all the elements of L0.  Such a vector U =  u1, . . . , ut  must be nonzero, and it must satisfy V · U = integer for all V in L0. In particular, since the points  1, 0, . . . , 0 ,  0, 1, . . . , 0 , . . . ,  0, 0, . . . , 1  are all in L0, all of the uj must be integers. Furthermore since V1 is in L0, we must have 1  m u1 + au2 + ··· + at−1ut  = integer, i.e.,  u1 + au2 + ··· + at−1ut ≡ 0  modulo m .   15   Conversely, any nonzero integer vector U =  u1, . . . , ut  satisfying  15  defines a family of hyperplanes with the required properties, since all of L0 will be covered: The dot product  y1V1+···+ytVt · U will be an integer for all integers y1, . . . , yt. We have proved that  u2  mx1−ax2−a2x3− ··· −at−1xt 2+x2   u1+au2+ ··· +at−1ut ≡ 0  modulo m   .  1+ ··· +u2  3+ ··· +x2  2+x2  t  t  min  ν2 t =  u1,...,ut ̸= 0,...,0  =  x1,...,xt ̸= 0,...,0   min   16   C. Deriving a computational method. We have now reduced the spectral test to the problem of finding the minimum value  16 ; but how on earth can we determine that minimum value in a reasonable amount of time? A brute-force search is out of the question, since m is very large in cases of practical interest. It will be interesting and probably more useful if we develop a computational method for solving an even more general problem: Find the minimum value of the quantity  f x1, . . . , xt  =  u11x1 + ··· + ut1xt 2 + ··· +  u1tx1 + ··· + uttxt 2   17   over all nonzero integer vectors  x1, . . . , xt , given any nonsingular matrix of coefficients U =  uij . The expression  17  is called a “positive definite quadratic form” in t variables. Since U is nonsingular,  17  cannot be zero unless the xj are all zero.  Let us write U1, . . . , Ut for the rows of U. Then  17  may be written  f x1, . . . , xt  =  x1U1 + ··· + xtUt ·  x1U1 + ··· + xtUt ,   18  the square of the length of the vector x1U1 +··· + xtUt. The nonsingular matrix U has an inverse, which means that we can find uniquely determined vectors V1, . . . , Vt such that  Ui · Vj = δij,  1 ≤ i, j ≤ t.   19    3.3.4  THE SPECTRAL TEST  99  For example, in the special form  16  that arises in the spectral test, we have  U1 =   m, 0, 0, . . . , 0 , U2 =   −a, 1, 0, . . . , 0 , U3 =   −a2, 0, 1, . . . , 0 ,  Ut =  −at−1, 0, 0, . . . , 1 ,  .  .  .  .  .  .  .  .  m 1, a, a2, . . . , at−1 , 0 , 0 ,  V1 = 1 V2 =  0, 1, 0, . . . , V3 =  0, 0, 1, . . . , . Vt =  0, 0, 0, . . . ,  1 .  .  .  .   20   These Vj are precisely the vectors  8 ,  9  that we used to define our original lattice L0. As the reader may well suspect, this is not a coincidence — indeed, if we had begun with an arbitrary lattice L0, defined by any set of linearly inde- pendent vectors V1, . . . , Vt, the argument we have used above can be generalized to show that the maximum separation between hyperplanes in a covering family is equivalent to minimizing  17 , where the coefficients uij are defined by  19 .  See exercise 2.   Our first step in minimizing  18  is to reduce it to a finite problem, namely to show that we won’t need to test infinitely many vectors  x1, . . . , xt  when finding the minimum. This is where the vectors V1, . . . , Vt come in handy; we have  and Cauchy’s inequality tells us that  xk =  x1U1 + ··· + xtUt · Vk,   x1U1 + ··· + xtUt · Vk  2 ≤ f x1, . . . , xt  Vk · Vk .  Hence we have derived a useful upper bound on each coordinate xk: Lemma A. Let  x1, . . . , xt  be a nonzero vector that minimizes  18  and let  y1, . . . , yt  be any nonzero integer vector. Then  k ≤ f y1, . . . , yt  Vk · Vk , x2  for 1 ≤ k ≤ t.   21   In particular, letting yi = δij for all i, k ≤  Uj · Uj  Vk · Vk , x2  for 1 ≤ j, k ≤ t.   22  Lemma A reduces the problem to a finite search, but the right-hand side of  21  is usually much too large to make an exhaustive search feasible; we need at least one more idea. On such occasions, an old maxim provides sound advice: “If you can’t solve a problem as it is stated, change it into a simpler problem that has the same answer.” For example, Euclid’s algorithm has this form; if we don’t know the gcd of the input numbers, we change them into smaller numbers having the same gcd.  In fact, a slightly more general approach probably underlies the discovery of nearly all algorithms: “If you can’t solve a problem directly, change it into one or more simpler problems, from whose solution you can solve the original one.”   In our case, a simpler problem is one that requires less searching because the right-hand side of  22  is smaller. The key idea we shall use is that it is possible to change one quadratic form into another one that is equivalent for all practical   1, . . . , x′  RANDOM NUMBERS  for i ̸= j; i̸=j qiUi.  j = Uj +  i = xi − qixj, x′ x′ j = xj,  U′ i = Ui, U′ 1, . . . , U′  100 3.3.4 purposes. Let j be any fixed subscript, 1 ≤ j ≤ t; let  q1, . . . , qj−1, qj+1, . . . , qt  be any sequence of t − 1 integers; and consider the following transformation of the vectors: i = Vi − qiVj, V ′ V ′ j = Vj,  It is easy to see that the new vectors U′ for which f′ x′ condition  19  remains valid, because it is easy to check that U′  x1, . . . , xt  runs through all nonzero integer vectors, so does  x′ the new form f′ has the same minimum as f. Our goal is to use transformation  23 , replacing Ui by U′   23  t define a quadratic form f′ t  = f x1, . . . , xt ; furthermore the basic orthogonality j = δij. As t ; hence i for all i, in order to make the right-hand side of  22  small; and the right-hand side of  22  will be small when both Uj · Uj and Vk · Vk are small. Therefore it is natural to ask the following two questions about the transformation  23 : a  What choice of qi makes V ′ b  What choice of q1, . . . , qj−1, qj+1, . . . , qt makes U′  j as small as possible? It is easiest to solve these questions first for real values of the qi. Question  a   i · V ′ 1, . . . , x′ i and Vi by V ′  i as small as possible? j · U′  i · V ′  is quite simple, since  Vi− qiVj · Vi− qiVj  = Vi· Vi−2qi Vi· Vj + q2  =  Vj · Vj qi− Vi· Vj Vj · Vj 2+ Vi· Vi− Vi· Vj 2 Vj · Vj,  i Vj · Vj  and the minimum occurs when  qi = Vi · Vj   Vj · Vj.   24  Geometrically, we are asking what multiple of Vj should be subtracted from Vi so that the resulting vector V ′ i has minimum length, and the answer is to choose i · Vj = 0 ; the following qi so that V ′ diagram makes this plain.  i is perpendicular to Vj  that is, to make V ′  Turning to question  b , we want to choose the qi so that Uj +  i̸=j qiUi has minimum length; geometrically, we want to start with Uj and add some vector in the  t − 1 -dimensional hyperplane whose points are the sums of multiples of {Ui j is perpendicular to the hyperplane, making U′   i ̸= j}. Again the best solution is to choose things so that U′  qi Ui · Uk  = 0,  j · Uk = 0 for all k ̸= j: k ̸= j.  1 ≤ k ≤ t,  Uj · Uk +  i̸=j   25    26   −qiVj  Vi  V ′i  Vj = V ′j   3.3.4  THE SPECTRAL TEST  101  j · U′  i · V ′   See exercise 12 for a rigorous proof that a solution to question  b  must satisfy these t − 1 equations.  Now that we have answered questions  a  and  b , we are in a bit of a quandary; should we choose the qi according to  24 , so that the V ′ i are minimized, or according to  26 , so that U′ j is minimized? Either of these alternatives makes an improvement in the right-hand side of  22 , so it is not immediately clear which choice should get priority. Fortunately, there is a very simple answer to this dilemma: Conditions  24  and  26  are exactly the same!  See exercise 7.  Therefore questions  a  and  b  have the same answer; we have a happy state of affairs in which we can reduce the length of both the U’s and the V ’s simultaneously. Indeed, we have just rediscovered the Gram–Schmidt orthogonalization process [see Crelle 94  1883 , 41–73].  see  25 . It turns out that this is not always the best solution to question  b ;  Our joy must be tempered with the realization that we have dealt with questions  a  and  b  only for real values of the qi. Our application restricts us to integer values, so we cannot make V ′ i exactly perpendicular to Vj. The best we can do for question  a  is to let qi be the nearest integer to Vi · Vj   Vj · Vj in fact U′ j may at times be longer than Uj. However, the bound  21  is never increased, since we can remember the smallest value of f y1, . . . , yt  found so far. Thus a choice of qi based solely on question  a  is quite satisfactory.  If we apply transformation  23  repeatedly in such a way that none of the vectors Vi gets longer and at least one gets shorter, we can never get into a loop; that is, we will never be considering the same quadratic form again after a sequence of nontrivial transformations of this kind. But eventually we will get stuck, in the sense that none of the transformations  23  for 1 ≤ j ≤ t will be able to shorten any of the vectors V1, . . . , Vt. At that point we can revert to an exhaustive search, using the bounds of Lemma A, which will now be quite small in most cases. Occasionally these bounds  21  will be poor, and another type of transformation will usually get the algorithm unstuck again and reduce the bounds  see exercise 18 . However, transformation  23  by itself has proved to be quite adequate for the spectral test; in fact, it has proved to be amazingly powerful when the computations are arranged as in the algorithm discussed below. *D. How to perform the spectral test. Here now is an efficient computational procedure that follows from our considerations. R. W. Gosper and U. Dieter have observed that it is possible to use the results of lower dimensions to make the spectral test significantly faster in higher dimensions. This refinement has been incorporated into the following algorithm, together with Gauss’s significant simplification of the two-dimensional case  exercise 5 . Algorithm S  The spectral test . This algorithm determines the value of   x1 + ax2 + ··· + at−1xt ≡ 0  modulo m     27  for 2 ≤ t ≤ T, given a, m, and T, where 0 < a < m and a is relatively prime to m.  The minimum is taken over all nonzero integer vectors  x1, . . . , xt , and the  1 + ··· + x2 x2    νt = min  t   102  RANDOM NUMBERS  3.3.4  number νt measures the t-dimensional accuracy of random number generators, as discussed in the text above.  All arithmetic within this algorithm is done on integers whose magnitudes rarely if ever exceed m2, except in step S7; in fact, nearly all of the integer variables will be less than m in absolute value during the computation. When νt is being calculated for t ≥ 3, the algorithm works with two t × t matrices U and V, whose row vectors are denoted by Ui =  ui1, . . . , uit  and Vi =  vi1, . . . , vit  for 1 ≤ i ≤ t. These vectors satisfy the conditions 1 ≤ i ≤ t;  ui1 + aui2 + ··· + at−1uit ≡ 0  modulo m , 1 ≤ i, j ≤ t.   28   29   Thus the Vj of our previous discussion have been multiplied by m, to ensure that their components are integers.  There are three other auxiliary vectors, X =  x1, . . . , xt , Y =  y1, . . . , yt , and Z =  z1, . . . , zt . During the entire algorithm, r will denote at−1 mod m and s will denote the smallest upper bound for ν2 S1. [Initialize.] Set t ← 2, h ← a, h′ ← m, p ← 1, p′ ← 0, r ← a, s ← 1 + a2.  The first steps of this algorithm handle the case t = 2 by a special method, very much like Euclid’s algorithm; we will have and  t that has been discovered so far.  hp′ − h′p = ±m  30   Ui · Vj = mδij,  h − ap ≡ h′ − ap′ ≡ 0  modulo m  during this phase of the calculation.   S2. [Euclidean step.] Set q ← ⌊h′ h⌋, u ← h′ − qh, v ← p′ − qp. If u2 + v2 < s,  set s ← u2 + v2, h′ ← h, h ← u, p′ ← p, p ← v, and repeat step S2.  S3. [Compute ν2.] Set u ← u−h, v ← v−p; and if u2 +v2 < s, set s ← u2 +v2, h′ ← u, p′ ← v. Then output s = ν2.  The validity of this calculation for the two-dimensional case is proved in exercise 5. Now we will set up the U and V matrices satisfying  28  and  29 , in preparation for calculations in higher dimensions.  Set  √  −h  −h′    ,  p p′  U ←   p′  h′ −p −h    ,  V ← ±  where the − sign is chosen for V if and only if p′ > 0.  S4. [Advance t.] If t = T, the algorithm terminates.  Otherwise we want to increase t by 1. At this point U and V are t × t matrices satisfying  28  and  29 , and we must enlarge them by adding an appropriate new row and column.  Set t ← t + 1 and r ←  ar  mod m. Set Ut to the new row  −r, 0, 0, . . . , 0, 1  of t elements, and set uit ← 0 for 1 ≤ i < t. Set Vt to the new row  0, 0, 0, . . . , 0, m . Finally, for 1 ≤ i < t, set q ← round vi1 r m , vit ← vi1r− qm, and Ut ← Ut + qUi.  Here “round x ” denotes the nearest integer to x, e.g., ⌊x + 1 2⌋. We are essentially setting vit ← vi1r and immediately applying transformation  23  with j = t, since the numbers vi1r are so large they ought to be reduced at once.  Finally set s ← min s, Ut · Ut , k ← t, and j ← 1.  In the following steps, j denotes the   3.3.4  THE SPECTRAL TEST  103  current row index for transformation  23 , and k denotes the last such index where the transformation shortened at least one of the Vi.  S5. [Transform.] For 1 ≤ i ≤ t, do the following operations: If i ̸= j and 2Vi · Vj > Vj · Vj, set q ← round Vi · Vj   Vj · Vj , Vi ← Vi − qVj, Uj ← Uj + qUi, s ← min s, Uj · Uj , and k ← j.  We omit the transformation when 2Vi · Vj exactly equals Vj · Vj; exercise 19 shows that this precaution keeps the algorithm from looping endlessly.  S6. [Advance j.] If j = t, set j ← 1; otherwise set j ← j + 1. Now if j ̸= k, return to step S5.  If j = k, we have gone through t − 1 consecutive cycles of no transformation, so the transformation process is stuck.   ,  S7. [Prepare for search.]   Now the absolute minimum will be determined, using an exhaustive search over all  x1, . . . , xt  satisfying condition  21  of Lemma A.  Set X ← Y ←  0, . . . , 0 , set k ← t, and set for 1 ≤ j ≤ t.  zj ←⌊ Vj · Vj s m2⌋   31   We will examine all X =  x1, . . . , xt  with xj ≤ zj for 1 ≤ j ≤ t. Usually zj ≤ 1, but L. C. Killingbeck noticed in 1999 that larger values occur for about 0.00001 of all multipliers when m = 264. During the exhaustive search, the vector Y will always be equal to x1U1 + ··· + xtUt, so that f x1, . . . , xt  = Y · Y . Since f −x1, . . . ,−xt  = f x1, . . . , xt , we shall ex- amine only vectors whose first nonzero component is positive. The method is essentially that of counting in steps of one, regarding  x1, . . . , xt  as the digits in a balanced number system with mixed radices  2z1+1, . . . , 2zt+1 ; see Section 4.1.  Y ← Y + Uk. and repeat step S9. But if k > t, set s ← min s, Y · Y  .  S8. [Advance xk.] If xk = zk, go to S10. Otherwise increase xk by 1 and set S9. [Advance k.] Set k ← k + 1. Then if k ≤ t, set xk ← −zk, Y ← Y − 2zkUk, S10. [Decrease k.] Set k ← k − 1. If k ≥ 1, return to S8. Otherwise output  s  the exhaustive search is completed  and return to S4.  √  νt =  In practice Algorithm S is applied for T = 5 or 6, say; it usually works reasonably well when T = 7 or 8, but it can be terribly slow when T ≥ 9 since the exhaustive search tends to make the running time grow as 3T.  If the minimum value νt occurs at many different points, the exhaustive search will hit them all; hence we typically find that all zk = 1 for large t. As remarked above, the values of νt are generally irrelevant for practical purposes when t is large.   An example will help to make Algorithm S clear. Consider the linear  congruential sequence defined by  m = 1010,   32  Six cycles of the Euclidean algorithm in steps S2 and S3 suffice to prove that the minimum nonzero value of x2  a = 3141592621,  X0 = 0.  c = 1,  1 + x2  2 with  x1 + 3141592621x2 ≡ 0  modulo 1010      .    .  104  RANDOM NUMBERS  3.3.4  occurs for x1 = 67654, x2 = 226; hence the two-dimensional accuracy of this generator is  ν2 =676542 + 2262 ≈ 67654.37748.  Passing to three dimensions, we seek the minimum nonzero value of x2 such that  1 + x2  2 + x2 3  x1 + 3141592621x2 + 31415926212x3 ≡ 0  modulo 1010 .   33   −191 −44190611  ,  V =  191 0 33 1  −226 0  67654  2564918569 1307181134 0 10000000000  Step S4 sets up the matrices   −67654 −226 0    U =  −44190611 5793866  −21082801    U =  97 4 −44190611 191 0 33 1  5793866  ,  V =  The first iteration of step S5, with q = 1 for i = 2 and q = 4 for i = 3, changes them to  −191 −44190611  2564918569 −35 44258265 −1257737435 764 176762444 −259674276   The first row U1 has actually gotten longer in this transformation, although eventually the rows of U should get shorter.  The next fourteen iterations of step S5 have  j, q1, q2, q3  =  2, −2, ∗, 0 ,  3, 0, 3, ∗ ,  1, ∗, −10, −1 ,  2, −1, ∗, −6 ,  3, −1, 0, ∗ ,  1, ∗, 0, 2 ,  2, 0, ∗, −1 ,  3, 3, 4,∗ ,  1,∗, 0, 0 ,  2,−5,∗, 0 ,  3, 1, 0,∗ ,  1,∗,−3,−1 ,  2, 0,∗, 0 ,  3, 0, 0,∗ . Now the transformation process is stuck, but the rows of the matrices have become significantly shorter: 616 −2777 −3022 104 918 −227 −983 −130  601246 −2994234 −2809871 438109 1593689 −854296 −9749816 −1707736   −888874  −1479      V =  U =   34   ,  .  The search limits  z1, z2, z3  in step S7 turn out to be  0, 0, 1 , so U3 is the shortest solution to  33 ; we have  ν3 =2272 + 9832 + 1302 ≈ 1017.21089.  Only a few iterations were needed to find this value, although condition  33  looks quite formidable at first glance. Our computation has proved that all points  Un, Un+1, Un+2  produced by the random number generator  32  lie on a family of parallel planes about 0.001 units apart, but not on any family of planes that differ by more than 0.001 units.  The exhaustive search in steps S8–S10 reduces the value of s only rarely. One such case, found in 1982 by R. Carling and K. Levine, occurs when a = 464680339, m = 229, and t = 5; another case arose when the author calculated 6 for line 21 of Table 1, later in this section. ν2 E. Ratings for various generators. So far we haven’t really given a criterion that tells us whether or not a particular random number generator passes or flunks the spectral test. In fact, spectral success depends on the application, since some applications demand higher resolution than others. It appears that   3.3.4 105 νt ≥ 230 t for 2 ≤ t ≤ 6 will be quite adequate for most purposes  although the author must admit choosing this criterion partly because 30 is conveniently divisible by 2, 3, 5, and 6 .  THE SPECTRAL TEST  For some purposes we would like a criterion that is relatively independent of m, so we can say that a particular multiplier is good or bad with respect to the set of all other multipliers for the given m, without examining any others. A reasonable figure of merit for rating the goodness of a particular multiplier seems to be the volume of the ellipsoid in t-space defined by the relation   x1m − x2a − ··· − xtat−1 2 + x2  2 + ··· + x2  t ≤ ν2 t ,  since this volume tends to indicate how likely it is that nonzero integer points  x1, . . . , xt  — corresponding to solutions of  15  — are in the ellipsoid. We there- fore propose to calculate this volume, namely µt = πt 2 νt  t 2 ! m √ 1   as an indication of the effectiveness of the multiplier a for the given m. In this formula,   t   t     35   ,  t  for t odd.   36    t 2 − 1  2  . . .  2  π,  ! =  2  Thus, in six or fewer dimensions the merit is computed as follows:  µ2 = πν2  2  m, µ5 = 8  µ3 = 4 5  m,  15 π2ν5  3 πν3  3  m,  µ4 = 1 6  m.  6 π3ν6  µ6 = 1  2 π2ν4  4  m,  We might say that the multiplier a passes the spectral test if µt is 0.1 or more for 2 ≤ t ≤ 6, and it “passes with flying colors” if µt ≥ 1 for all these t. A low value of µt means that we have probably picked a very unfortunate multiplier, since very few lattices will have integer points so close to the origin. Conversely, a high value of µt means that we have found an unusually good multiplier for the given m; but it does not mean that the random numbers are necessarily very good, since m might be too small. Only the values νt truly indicate the degree of randomness.  Table 1 shows what sorts of values occur in typical sequences. Each line of t , µt, and the “number of the table considers a particular generator, and lists ν2 bits of accuracy” lg νt. Lines 1 through 4 show the generators that were the sub- ject of Figs. 2 and 5 in Section 3.3.1. The generators in lines 1 and 2 suffer from too small a multiplier; a diagram like Fig. 8 will have a nearly vertical “stripes” when a is small. The terrible generator in line 3 has a good µ2 but very poor µ3 6 and ν4 = 2  see and µ4; like nearly all generators of potency 2, it has ν3 = exercise 3 . Line 4 shows a “random” multiplier; this generator has satisfactorily passed numerous empirical tests for randomness, but it does not have especially high values of µ2, . . . , µ6. In fact, the value of µ5 flunks our criterion.  Line 5 shows the generator of Fig. 8. It passes the spectral test with very high-flying colors, when µ2 through µ6 are considered, but of course m is so small that the numbers can hardly be called random; the νt values are terribly low.  √   106  RANDOM NUMBERS  3.3.4  SAMPLE RESULTS OF THE SPECTRAL TEST  Table 1  a 23 27+1 218+1  3141592653  137  3141592621 3141592221 4219755981 4160984121 224+213+5  513  216+3  1812433253 1566083941  69069  2650845021 314159269 62089911  16807 48271 40692  Line 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  m  108+1  235 235 235 256 1010 1010 1010 1010 235 235 229 232 232 232 232 231−1 231−1 231−1 231−1 231−249 246 248  ν2 ν2 ν2 ν2 ν2 4 2 3 5 6 530 530 530 530 447 16642 16642 16642 15602 252 4 34359738368 6 4 4 27822 2997222016 1026050 1118 1118 14 274 30 6 4 62454 4577114792 1034718 1776 542 97450 4293881050 276266 3366 2382 49362 10721093248 2595578 5868 820 16686 9183801602 4615650 6840 1344 21476 8364058 8364058 16712 1496 113374 33161885770 2925242 13070 2256 116 536936458 118 116 116 15082 4326934538 1462856 4866 906 44902 4659748970 2079590 4652 662 52804 4243209856 2072544 6990 242 68342 4938969760 2646962 8778 1506 36985 1432232969 899290 3427 1144 48191 1977289717 1662317 6101 1462 21682 282475250 408197 4439 895 47418 1990735345 1433881 4404 1402 42475 1655838865 1403422 6507 1438 5.6×1013 1180915002 1882426 279928 26230 3.2×1014 4111841446 17341510 306326 59278 1.9×109 4.7×1011 2.4×1018 3194548 1611610  231−1 2 1.4×1012 643578623 12930027 837632 4.1×109 45662836 1846368 8.8×1018 6.4×1012 262+1 4281084902 2.2×109 1.8×109 1862407 3.5×10115 4.4×1086 1.8×10173 5×1057 1×10207 2×10165 8×10137 1.6×10414 8.6×10275  2×1069  44485709377909  31167285  see  38  see  39   see the text see the text 2−24·389  232−5 −400  264 ≈ 278 ≈ 2576 ≈ 21376  Line 6 is the generator discussed in  32  above. Line 7 is a similar example, having an abnormally low value of µ3. Line 8 shows a nonrandom multiplier for the same modulus m; all of its partial quotients are 1, 2, or 3. Such multipliers have been suggested by I. Borosh and H. Niederreiter because the Dedekind sums are likely to be especially small and because they produce best results in the two-dimensional serial test  see Section 3.3.3 and exercise 30 . The particular example in line 8 has only one ‘3’ as a partial quotient; there is no multiplier congruent to 1 modulo 20 whose partial quotients with respect to 1010 are only 1s and 2s. The generator in line 9 shows another multiplier chosen with malice aforethought, following a suggestion by A. G. Waterman that guarantees a reasonably high value of µ2  see exercise 11 . Line 10 is interesting because it has high µ3 in spite of very low µ2  see exercise 8 .  Line 11 of Table 1 is a reminder of the good old days — it once was used ex- tensively, following a suggestion of O. Taussky in the early 1950s. But computers for which 235 was an appropriate modulus began to fade in importance during the late 60s, and they disappeared almost completely in the 80s, as machines   3.3.4  THE SPECTRAL TEST  107  lg ν2 4.5 7.0 17.5 15.7 4.0 16.0 16.0 16.7 16.5 11.5 17.5 14.5 16.0 16.1 16.0 16.1 15.2 15.4 14.0 15.4 15.3 22.8 24.1 30.5 31.0 31.5 31.0 288. 688.  lg ν4 4.5 7.0 1.0 7.4 1.9 8.0 8.3 7.8 7.0 7.2 8.4 3.4 6.9 7.7 7.8 8.0 7.6 7.8 7.2 7.8 7.7 10.4 12.0 15.4 14.6 16.0 15.5 144. 344.  lg ν3 µ2 2ϵ5 4.5 2ϵ6 7.0 3.14 1.3 0.27 10.0 3.36 2.5 1.44 10.0 1.35 9.0 3.37 10.7 2.89 11.1 8ϵ4 11.5 3.03 10.7 3.14 3.4 3.16 10.2 3.41 10.5 3.10 10.5 3.61 10.7 2.10 9.9 2.89 10.3 0.41 9.3 2.91 10.2 2.42 10.2 2.48 15.1 3.60 16.0 1.65 19.4 3.14 20.2 1.50 21.3 5ϵ5 16.0 2.27 192. 458. 3.10 upper bounds from  40 : 3.63  lg ν5 4.5 7.0 1.0 5.1 1.3 5.4 5.9 6.3 6.4 7.0 6.8 3.4 6.1 6.1 6.4 6.6 5.9 6.3 6.1 6.1 6.3 9.0 9.1 10.8 11.8 12.7 15.4 115. 275.  lg ν6 4.4 4.0 1.0 5.1 1.0 4.5 5.6 4.8 5.2 5.3 5.6 3.4 4.9 4.7 4.0 5.3 5.1 5.3 4.9 5.2 5.2 7.3 7.9 10.3 9.8 10.4 10.4 95.9 229.   ϵ = 1 10    µ3 5ϵ4 3ϵ4 2ϵ9 0.13 2.69 0.44 0.06 1.75 4.15 2.95 0.61 ϵ5 1.73 2.92 2.91 4.20 1.66 4.18 0.51 3.35 3.24 2.42 3.92 0.29 1.49 3.68 4ϵ9 3.46 2.04 5.92  µ4 0.01 0.04 2ϵ9 0.11 3.78 1.92 4.69 1.20 0.14 0.07 1.85 ϵ4 0.26 2.32 3.20 5.37 3.14 5.34 1.08 5.17 4.15 0.25 5.27 3.88 0.44 4.52 8ϵ5 3.92 2.85 9.87  µ5 0.34 4.66 5ϵ9 0.01 1.81 0.07 0.35 1.39 2.04 5.53 2.99 ϵ3 2.02 1.81 5.01 8.85 1.69 7.13 3.22 3.15 8.37 3.10 0.97 0.02 0.69 4.02 2.56 2.49 1.15 14.89  Line 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  µ6 4.62 2ϵ3 ϵ8 0.21 1.29 0.08 6.98 0.28 1.25 0.50 1.73 0.02 0.89 0.35 0.02 4.11 3.60 7.52 1.73 6.63 7.16 1.33 3.82 4.69 0.66 1.76 ϵ4 2.98 1.33 23.87  X0 odd,  with 32-bit arithmetic began to proliferate. This switch to a comparatively small word size called for comparatively greater care. Line 12 was, alas, the generator actually used on such machines in most of the world’s scientific computing centers for more than a decade; its very name RANDU is enough to bring dismay into the eyes and stomachs of many computer scientists! The actual generator is defined by  37  and exercise 20 indicates that 229 is the appropriate modulus for the spectral test. Since 9Xn − 6Xn+1 + Xn+2 ≡ 0  modulo 231 , the generator fails most three-dimensional criteria for randomness, and it should never have been used. Almost any multiplier ≡ 5  modulo 8  would be better.  A curious fact about 116, RANDU, noticed by R. W. Gosper, is that ν4 = ν5 = ν6 = ν7 = ν8 = ν9 = hence µ9 is a spectacular 11.98.  Lines 13 and 14 are the Borosh–Niederreiter and Waterman multipliers for modulus 232. Line 16 was found by L. C. Killingbeck, who carried out an exhaustive search of all multipliers a ≡ 1 mod 4 when m = 232. Line 23, similarly, was found by M. Lavaux and F. Janssens in a  Xn+1 =  65539Xn  mod 231,  √   108  RANDOM NUMBERS  3.3.4   nonexhaustive  computer search for spectrally good multipliers having a very high µ2. Line 22 is for the multiplier used with c = 0 and m = 248 in the Cray X- MP library; line 26  whose excellent multiplier 6364136223846793005 is too big to fit in the column  is due to C. E. Haynes. Line 15 was nominated by George Marsaglia as “a candidate for the best of all multipliers,” after a computer search for nearly cubical lattices in dimensions 2 through 5, partly because it is easy to remember [Applications of Number Theory to Numerical Analysis, edited by S. K. Zaremba  New York: Academic Press, 1972 , 275]. Line 17 uses a random primitive root, modulo the prime 231−1, as multiplier. Line 18 shows the spectrally best primitive root for 231−1, found in an exhaustive search by G. S. Fishman and L. R. Moore III [SIAM J. Sci. Stat. Comput. 7  1986 , 24–45]. The adequate but less outstanding multiplier 16807 = 75 in line 19 is actually used most often for that modulus, after being proposed by Lewis, Goodman, and Miller in IBM Systems J. 8  1969 , 136–146; it has been one of the main generators in the popular IMSL subroutine library since 1971. The main reason for continued use of a = 16807 is that a2 is less than the modulus m, hence ax mod m can be implemented with reasonable efficiency in high-level languages using the technique of exercise 3.2.1.1–9. However, such small multipliers have known defects. S. K. Park and K. W. Miller noticed that the same implementation technique applies also to certain multipliers greater than m, so they asked G. S. Fishman to find the best “efficiently portable” multiplier in this wider class; the result appears in line 20 [CACM 31  1988 , 1192–1201]. Line 21 shows another good multiplier, due to P. L’Ecuyer [CACM 31  1988 , 742–749, 774]; this one uses a slightly smaller prime modulus. suggested in Eq. 3.2.2– 15 , so that the generated numbers ⟨Zn⟩ satisfy Yn+1 = 40692Yn mod  231 − 249 ,  When the generators of lines 20 and 21 are combined by subtraction as   38  exercise 32 shows that it is reasonable to rate ⟨Zn⟩ with the spectral test for m =  231−1  231−249  and a = 1431853894371298687.  This value of a satisfies a mod  231 − 1  = 48271 and a mod  231 − 249  = 40692.  The results appear on line 24. We needn’t worry too much about the low value of µ5, since ν5 > 1000. Generator  38  has a period of length  231 − 2  231 − 250  62 ≈ 7 × 1016.  Xn+1 = 48271Xn mod  231 − 1 ,  Zn =  Xn − Yn  mod  231 − 1 ,  √  Line 25 of the table represents the sequence  Xn =  271828183Xn−1 − 314159269Xn−2  mod  231 − 1 ,   39  which can be shown to have period length  231 − 1 2 − 1; it has been analyzed with the generalized spectral test of exercise 24.  The last three lines of Table 1 are based on add-with-carry and subtract- with-borrow methods, which simulate linear congruential sequences that have extremely large moduli  see exercise 3.2.1.1–14 . Line 27 is for the generator  Xn =  Xn−1 + 65430Xn−2 + Cn  mod 231,  Cn+1 = Xn−1 + 65430Xn−2 + Cn  231,   3.3.4 109 which corresponds to Xn+1 =  65430· 231 + 1 Xn mod  65430· 262 + 231 − 1 ; the numbers in the table refer to the “super-values”  THE SPECTRAL TEST  Xn =  65430 · 231 + 1 Xn−1 + 65430Xn−2 + Cn  rather than to the values Xn actually computed and used as random numbers. Line 28 represents a more typical subtract-with-borrow generator  Xn =  Xn−10 − Xn−24 − Cn  mod 224, Cn+1 = [Xn−10 < Xn−24 + Cn],  but modified by generating 389 elements of the sequence and then using only the first  or last  24. This generator, called RANLUX, was recommended by Martin Lüscher after it passed many stringent tests that previous generators failed [Computer Physics Communications 79  1994 , 100–110]. A similar sequence, Xn =  Xn−22 − Xn−43 − Cn  mod  232 − 5 , Cn+1 = [Xn−22 < Xn−43 + Cn], with 43 elements used after 400 are generated, appears in line 29; this sequence is discussed in the answer to exercise 3.2.1.2–22. In both cases the table entries refer to the spectral test on multiprecision numbers Xn instead of to the individual “digits” Xn, but the high µ values indicate that the process of generating 389 or 400 numbers before selecting 24 or 43 is an excellent way to remove biases due to the extreme simplicity of the generation scheme.  Theoretical upper bounds on µt, which can never be transcended for any m, are shown just below Table 1; it is known that every lattice with m points per unit volume has  νt ≤ γ  1 2 t m1 t,   40   where γt takes the respective values 21 2,   4 3 1 2,  2  43 7,  21 3,  23 5,   64 3 1 6,   41  for t = 2, . . . , 8. [See exercise 9 and J. W. S. Cassels, Introduction to the Geometry of Numbers  Berlin: Springer, 1959 , 332; J. H. Conway and N. J. A. Sloane, Sphere Packings, Lattices and Groups  New York: Springer, 1988 , 20.] These bounds hold for lattices generated by vectors with arbitrary real coordinates. For example, the optimum lattice for t = 2 is hexagonal, and it √ is generated by vectors of length 2  3m that form two sides of an equilateral triangle. In three dimensions the optimum lattice is generated by vectors V1, V2, V3 that can be rotated into the form  v, v,−v ,  v,−v, v ,  −v, v, v , where v = 1  *F. Relation to the serial test. In a series of important papers published during the 1970s, Harald Niederreiter showed how to analyze the distribution of the t-dimensional vectors  1  by means of exponential sums. One of the main consequences of his theory is that the serial test in several dimensions will be passed by any generator that passes the spectral test, even when we consider only a sufficiently large part of the period instead of the whole period. We shall now turn briefly to a study of his interesting methods, in the case of linear congruential sequences  X0, a, c, m  of period length m.  4m.  3√   110  RANDOM NUMBERS  3.3.4  The first idea we need is the notion of discrepancy in t dimensions, a quantity that we shall define as the difference between the expected number and the actual number of t-dimensional vectors  xn, xn+1, . . . , xn+t−1  falling into a hyper-rectangular region, maximized over all such regions. To be precise, let ⟨xn⟩ be a sequence of integers in the range 0 ≤ xn < m. We define   number of  xn, . . . , xn+t−1  in R for 0 ≤ n < N   t  N = max D    mt  N  R  − volume of R  42   where R ranges over all sets of points of the form  R = { y1, . . . , yt   α1 ≤ y1 < β1, . . . , αt ≤ yt < βt};   43  here αj and βj are integers in the range 0 ≤ αj < βj ≤ m, for 1 ≤ j ≤ t. The volume of R is clearly  β1 − α1  . . .  βt − αt . To get the discrepancy D  t  N , we imagine looking at all these sets R and finding the one with the greatest excess or deficiency of points  xn, . . . , xn+t−1 .  An upper bound for the discrepancy can be found by using exponential sums. Let ω = e2πi m be a primitive mth root of unity. If  x1, . . . , xt  and  y1, . . . , yt  are two vectors with all components in the range 0 ≤ xj, yj < m, we have  ω x1−y1 u1+···+ xt−yt ut =  if  x1, . . . , xt  =  y1, . . . , yt , if  x1, . . . , xt  ̸=  y1, . . . , yt .  0≤u1,...,ut<m    ωxnu1+···+xn+t−1ut   Therefore the number of vectors  xn, . . . , xn+t−1  in R for 0 ≤ n < N, when R is defined by  43 , can be expressed as  0≤n<N  1 mt When u1 = ··· = ut = 0 in this sum, we get N mt times the volume of R; hence we can express D   t  N as the maximum over R of  ω− y1u1+···+ytut .  0≤u1,...,ut<m  α1≤y1<β1  αt≤yt<βt  . . .       mt  0      . . .  ω− y1u1+···+ytut   α1≤y1<β1  αt≤yt<βt  Since complex numbers satisfy w + z ≤ w + z and wz = wz, it follows that   1      N mt  0≤n<N  0≤u1,...,ut<m   u1,...,ut ̸= 0,...,0     N ≤ max  t  D  R  1 mt  0≤u1,...,ut<m   u1,...,ut ̸= 0,...,0     ≤ 1 mt  0≤u1,...,ut<m   u1,...,ut ̸= 0,...,0   ωxnu1+···+xn+t−1ut          α1≤y1<β1  α1≤y1<β1  . . .  . . .  αt≤yt<βt  αt≤yt<βt  R  max  .  g u1, . . . , ut   g u1, . . . , ut   ω− y1u1+···+ytut   ω− y1u1+···+ytut      0≤u1,...,ut<m   u1,...,ut ̸= 0,...,0   3.3.4  =  where  f u1, . . . , ut  g u1, . . . , ut ,  THE SPECTRAL TEST  111   44      .  g u1, . . . , ut  =  ωxnu1+···+xn+t−1ut   1  N  0≤n<N      1   1 mt  m  α1≤y1<β1  ;  1    . . .  f u1, . . . , ut  = max  R  . . .  α1≤y1<β1  αt≤yt<βt  ω− y1u1+···+ytut      ,  = max  R  ω−u1y1  ω−utyt  m  αt≤yt<βt  Both f and g can be simplified further in order to get a good upper bound on  t  N . We have D   1  m    α≤y<β  ω−uy   ≤  m  ω−βu − ω−αu  ω−u − 1   =  1 r u1, . . . , ut  =   when u ̸= 0, and the sum is ≤ 1 when u = 0; hence  f u1, . . . , ut  ≤ r u1, . . . , ut ,  where  1  m sin πuk m  .  1≤k≤t uk̸=0  2  m ωu − 1 =  1  m sin πu m    45    46   Furthermore, when ⟨xn⟩ is generated modulo m by a linear congruential se- quence, we have  xnu1+···+ xn+t−1ut = xnu1+ axn + c u2+···+at−1xn+ c at−2+···+1 ut  =  u1+ au2+···+ at−1ut xn+ h u1, . . . , ut   where h u1, . . . , ut  is independent of n; hence  g u1, . . . , ut  =  ωq u1,...,ut xn   47    1  N    0≤n<N  where  q u1, . . . , ut  = u1 + au2 + ··· + at−1ut.   48  Now here is where the connection to the spectral test comes in: We will show that the sum g u1, . . . , ut  is rather small unless q u1, . . . , ut  ≡ 0  modulo m ; in other words, the contributions to  44  arise mainly from the solutions to  15 . Furthermore exercise 27 shows that r u1, . . . , ut  is rather small when  u1, . . . , ut   t  is a “large” solution to  15 . Hence the discrepancy D N will be rather small   112  RANDOM NUMBERS  3.3.4  when  15  has only “large” solutions, namely when the spectral test is passed. Our remaining task is to quantify these qualitative statements by making careful calculations.  In the first place, let’s consider the size of g u1, . . . , ut . When N = m, so that the sum  47  is over an entire period, we have g u1, . . . , ut  = 0 except when  u1, . . . , ut  satisfies  15 , so the discrepancy is bounded above in this case by the sum of r u1, . . . , ut  taken over all the nonzero solutions of  15 . But let’s consider also what happens in a sum like  47  when N is less than m and q u1, . . . , ut  is not a multiple of m. We have    0≤n<N  1 m  0≤n<N  ωxn =   1 =  Skl =   0≤k<m  m     0≤k<m  ω−nk    0≤j<m  ω−nk  Sk0,  0≤n<N  ωxj+jk  ωxj+l+jk.  0≤j<m   49    50   where  Now Skl = ω−lkSk0, so Skl = Sk0 for all l, and we can calculate this common value by further exponential-summery:       m  Sk02 = 1 = 1 = 1 = 1  m  m  Skl2  0≤l<m  0≤l<m  0≤j<m   ω j−i k    ωxj+l+jk  ω j−i k   0≤l<m  0≤i<m ωxj+l−xi+l  0≤i,j<m  m  0≤i<m  i≤j<m+i  0≤l<m  ω−xi+l−ik  ω aj−i−1 xi+l+ aj−i−1 c  a−1 .  Let s be minimum such that as ≡ 1  modulo m , and let  s′ =  as − 1 c  a − 1  mod m.  Then s is a divisor of m  see Lemma 3.2.1.2P , and xn+js ≡ xn+js′  modulo m . The sum on l vanishes unless j − i is a multiple of s, so we find that    Sk02 = m  ω jsk+js′  .  0≤j<m s  We have s′ = q′s where q′ is relatively prime to m  see exercise 3.2.1.2–21 , so it turns out that  Sk0 =  √ m   s  if k + q′ ̸≡ 0  modulo m s , if k + q′ ≡ 0  modulo m s .   51    0    52    53   3.3.4  THE SPECTRAL TEST  113  Putting this information back into  49 , and recalling the derivation of  45 , shows that  ω xn   ≤ m√  ≤ 2 The same upper bound applies also to        0≤n<N  0≤n<N  ω xn  √  π    s  k  r k ,   m√    .  s  s ln s + O  where the sum is over 0 ≤ k < m such that k+q′ ≡ 0  modulo m s . Exercise 25 can now be used to estimate the remaining sum, and we find that  √  0≤n<N ω qxn for any q ̸≡ 0  modulo m , since the effect is to replace m in this derivation by a divisor of m. In fact, the upper bound gets even smaller when q has a factor in common with m, since s and m   s generally become smaller.  See exercise 26.   We have now proved that the g u1, . . . , ut  part of our upper bound  44  on the discrepancy is small, if N is large enough and if  u1, . . . , ut  does not satisfy the spectral test congruence  15 . Exercise 27 proves that the f u1, . . . , ut  part of our upper bound is small, when summed over all the nonzero vectors  u1, . . . , ut  satisfying  15 , provided that all such vectors are far away from  0, . . . , 0 . Putting these results together leads to the following theorem of Niederreiter: Theorem N. Let ⟨Xn⟩ be a linear congruential sequence  X0, a, c, m  of period length m > 1, and let s be the least positive integer such that as ≡ 1  modulo m .  t  Then the t-dimensional discrepancy D N corresponding to the first N values of ⟨Xn⟩, as defined in  42 , satisfies    + O   m log m t    √  N  s  + O log m t rmax  ;  54   √   55  Here rmax is the maximum value of the quantity r u1, . . . , ut  defined in  46 , taken over all nonzero integer vectors  u1, . . . , ut  satisfying  15 . Proof. The first two O-terms in  54  come from vectors  u1, . . . , ut  in  44  that do not satisfy  15 , since exercise 25 proves that f u1, . . . , ut  summed over  all  u1, . . . , ut  is O  2 π  ln m t and exercise 26 bounds each g u1, . . . , ut . These terms are missing from  55  since g u1, . . . , ut  = 0 in that case. The  remaining O-term in  54  and  55  comes from nonzero vectors  u1, . . . , ut  that do satisfy  15 , using the bound derived in exercise 27.  By examining this proof carefully, we could replace each O in these formulas by an explicit function of t.   s log s  log m t   t  N = O D D t   m = O log m t rmax  N  .  Eq.  55  relates to the serial test in t dimensions over the entire period, while Eq.  54  gives us useful information about the distribution of the first N generated values when N is less than m, provided that N is not too small.   114  RANDOM NUMBERS  √  r , then s equals pe1−f1  s term will dominate. If m = pe1  Notice that  54  will guarantee low discrepancy only when s is sufficiently large, r and gcd a − 1, m  = otherwise the m  by Lemma 3.2.1.2P; thus, the largest pf1 1 . . . pfr values of s correspond to high potency. In the common case m = 2e and a ≡ 5  modulo 8 , we have s = 1 It is not difficult to prove that  m  log m t+1 N+O log m trmax  4 m, so D  . . . per−fr  1 . . . per  .  1  r  3.3.4   56    t   N is O√ rmax ≤ 1√ 8 νt  √  m  log m t+1.   see exercise 29 . Therefore Eq.  54  says in particular that the discrepancy will be low in t dimensions if the spectral test is passed and if N is somewhat larger than  √ In a sense Theorem N is almost too strong, for the result in exercise 30 shows that linear congruential sequences like those in lines 8 and 13 of Table 1 have a discrepancy of order  log m 2 m in two dimensions. The discrepancy in this case is extremely small in spite of the fact that there are parallelogram-shaped regions of area ≈ 1  m containing no points  Un, Un+1 . The fact that discrepancy can change so drastically when the points are rotated warns us that the serial test may not be as meaningful a measure of randomness as the rotation-invariant spectral test. G. Historical remarks. In 1959, while deriving upper bounds for the error in the evaluation of t-dimensional integrals by the Monte Carlo method, N. M. Korobov devised a way to rate the multiplier of a linear congruential sequence. His rather complicated formula is related to the spectral test, since it is strongly influenced by “small” solutions to  15 ; but it is not quite the same. Korobov’s test has been the subject of an extensive literature, surveyed by Kuipers and Niederreiter in Uniform Distribution of Sequences  New York: Wiley, 1974 , §2.5. The spectral test was originally formulated by R. R. Coveyou and R. D. MacPherson [JACM 14  1967 , 100–119], who introduced it in an interesting Instead of working with the grid structure of successive points, indirect way. they considered random number generators as sources of t-dimensional “waves.” t such that x1 + ··· + at−1xt ≡ 0  modulo m  in their original treatment were the wave “frequencies,” or points in the “spectrum” defined by the random number generator, with low-frequency waves being the most damaging to randomness; hence the name spectral test. Coveyou and MacPherson introduced a procedure analogous to Algorithm S for performing their test, based on the principle of Lemma A. However, their original procedure  which used matrices U U T and V V T instead of U and V   dealt with extremely large numbers; the idea of working directly with U and V was independently sug- gested by F. Janssens and by U. Dieter. [See Math. Comp. 29  1975 , 827–833.] Several other authors pointed out that the spectral test could be understood in far more concrete terms; by introducing the study of the grid and lattice struc- tures corresponding to linear congruential sequences, the fundamental limitations on randomness became graphically clear. See G. Marsaglia, Proc. Nat. Acad. Sci.  The numbers x2  1 + ··· + x2   3.3.4  THE SPECTRAL TEST  115  61  1968 , 25–28; W. W. Wood, J. Chem. Phys. 48  1968 , 427; R. R. Coveyou, Studies in Applied Math. 3  Philadelphia: SIAM, 1969 , 70–111; W. A. Beyer, R. B. Roof, and D. Williamson, Math. Comp. 25  1971 , 345–360; G. Marsaglia and W. A. Beyer, Applications of Number Theory to Numerical Analysis, edited by S. K. Zaremba  New York: Academic Press, 1972 , 249–285, 361–370.  R. G. Stoneham showed, by using estimates of exponential sums, that p1 2+ϵ or more elements of the sequence akX0 mod p have asymptotically small dis- crepancy, when a is a primitive root modulo the prime p [Acta Arithmetica 22  1973 , 371–389]. This work was extended as explained above in a number of papers by Harald Niederreiter [Math. Comp. 28  1974 , 1117–1132; 30  1976 , 571–597; Advances in Math. 26  1977 , 99–181; Bull. Amer. Math. Soc. 84  1978 , 957–1041]. See also Niederreiter’s book Random Number Generation and Quasi-Monte Carlo Methods  Philadelphia: SIAM, 1992 .  EXERCISES 1. [M10] To what does the spectral test reduce in one dimension?  In other words, what happens when t = 1?  2. [HM20] Let V1, . . . , Vt be linearly independent vectors in t-space, let L0 be the lattice of points defined by  10 , and let U1, . . . , Ut be defined by  19 . Prove that the maximum distance between  t−1 -dimensional hyperplanes, over all families of parallel hyperplanes that cover L0, is 1 min{f x1, . . . , xt 1 2   x1, . . . , xt  ̸=  0, . . . , 0 }, where f is defined in  17 . 3. [M24] Determine ν3 and ν4 for all linear congruential generators of potency 2 and period length m.   cid:120  4. [M23] Let u11, u12, u21, u22 be elements of a 2 × 2 integer matrix such that  u11 + au12 ≡ u21 + au22 ≡ 0  modulo m  and u11u22 − u21u12 = m. a  Prove that all integer solutions  y1, y2  to the congruence y1 +ay2 ≡ 0  modulo m  b  If, in addition, 2u11u21 + u12u22 ≤ u2  have the form  y1, y2  =  x1u11+x2u21, x1u12+x2u22  for integer x1, x2.  22, prove that  y1, y2  =  12 ≤ u2  21 + u2  11 + u2   u11, u12  minimizes y2  1 + y2  2 over all nonzero solutions to the congruence.  5. [M30] Prove that steps S1 through S3 of Algorithm S correctly perform the spec- tral test in two dimensions. [Hint: See exercise 4, and prove that  h′ + h 2 + p′ + p 2 ≥ h2 + p2 at the beginning of step S2.] 6. [M30] Let a0, a1, . . . , at−1 be the partial quotients of a m as defined in Section 3.3.3, and let A = max0≤j  2π  A + 1 + 1 A . 7. [HM22] Prove that questions  a  and  b  following Eq.  23  have the same solution for real values of q1, . . . , qj−1, qj+1, . . . , qt  see  24  and  26  . 8. [M18] Line 10 of Table 1 has a very low value of µ2, yet µ3 is quite satisfactory. What is the highest possible value of µ3 when µ2 = 10−6 and m = 1010? 9. [HM32]  C. Hermite, 1846.  Let f x1, . . . , xt  be a positive definite quadratic form, defined by the matrix U as in  17 , and let θ be the minimum value of f at 3  t−1  2det U2 t. [Hints: If W is any integer nonzero integer points. Prove that θ ≤   4 matrix of determinant 1, the matrix W U defines a form equivalent to f; and if S is any orthogonal matrix  that is, if S−1 = ST  , the matrix U S defines a form identically equal to f. Show that there is an equivalent form g whose minimum θ occurs at   116  RANDOM NUMBERS  3.3.4  2, y2  1+y2  1+u2  1 +y2  1+u2  1 + y2  1 + u2  2 , and  u2  2 = min u2  1 + y2 2  ≥ m2.  Hence ν2   1, 0, . . . , 0 . Then prove the general result by induction on t, writing g x1, . . . , xt  = θ x1 + β2x2 + ··· + βtxt 2 + h x2, . . . , xt  where h is a positive definite quadratic form in t − 1 variables.] 10. [M28] Let y1 and y2 be relatively prime integers such that y1+ay2 ≡ 0  modulo m  and y2  modulo m , u1y2 − u2y1 = m, 2u1y1 + u2y2 ≤ min u2 2 ×  y2  2 <4 3 m. Show that there exist integers u1 and u2 such that u1+au2 ≡ 0  cid:120  11. [HM30]  Alan G. Waterman, 1974.  Invent a reasonably efficient procedure that 2 =4 3 m− ϵ, where ϵ > 0 is value 4 3 m. In practice we will compute several such multipliers having small ϵ,  computes multipliers a ≡ 1  modulo 4  for which there exists a relatively prime solution to the congruence y1 + ay2 ≡ 0  modulo m  with y2 as small as possible, given m = 2e.  By exercise 10, this choice of a will guarantee that 2 ≥ m2  y2 ν2 2 will be near its optimum  2  >3 4 m, and there is a chance that ν2  2, y2 2  by exercise 4.   choosing the one with best spectral values ν2, ν3, . . . .  12. [HM23] Prove, without geometrical handwaving, that any solution to question  b  following Eq.  23  must also satisfy the set of equations  26 . 13. [HM22] Lemma A uses the fact that U is nonsingular to prove that a positive definite quadratic form attains a definite, nonzero minimum value at nonzero integer points. Show that this hypothesis is necessary, by exhibiting a quadratic form  19  whose matrix of coefficients is singular, and for which the values of f x1, . . . , xt  get arbitrarily near zero  but never reach it  at nonzero integer points  x1, . . . , xt . 14. [24] Perform Algorithm S by hand, for m = 100, a = 41, T = 3.   cid:120  15. [M20] Let U be an integer vector satisfying  15 . How many of the  t − 1 -  1 + y2  1 + y2  dimensional hyperplanes defined by U intersect the unit hypercube { x1, . . . , xt   0 ≤ xj < 1 for 1 ≤ j ≤ t}?  This is approximately the number of hyperplanes in the family that will suffice to cover L0.  16. [M30]  U. Dieter.  Show how to modify Algorithm S in order to calculate the minimum number Nt of parallel hyperplanes intersecting the unit hypercube as in exercise 15, over all U satisfying  15 . [Hint: What are appropriate analogs to positive definite quadratic forms and to Lemma A?] 17. [20] Modify Algorithm S so that, in addition to computing the quantities νt, it outputs all integer vectors  u1, . . . , ut  satisfying  15  such that u2 t , for 2 ≤ t ≤ T. 18. [M30] This exercise is about the worst case of Algorithm S. a  By considering “combinatorial matrices,” whose elements have the form y + xδij  see exercise 1.2.3–39 , find 3×3 matrices of integers U and V satisfying  29  such that the transformation of step S5 does nothing for any j, but the corresponding values of zk in  31  are so huge that exhaustive search is out of the question.  The matrix U need not satisfy  28 ; we are interested here in arbitrary positive definite quadratic forms of determinant m.   1 + ··· + u2  t = ν2  another transformation that does produce a substantial reduction.  b  Although transformation  23  is of no use for the matrices constructed in  a , find   cid:120  19. [HM25] Suppose step S5 were changed slightly, so that a transformation with 2⌋ q = 1 would be performed when 2Vi · Vj = Vj · Vj.  Thus, q = ⌊ Vi · Vj   Vj · Vj  + 1 whenever i ̸= j.  Would it be possible for Algorithm S to get into an infinite loop?   3.3.4  THE SPECTRAL TEST  117  √  20. [M23] Discuss how to carry out an appropriate spectral test for linear congruential sequences having c = 0, X0 odd, m = 2e, a mod 8 = 3 or 5.  See exercise 3.2.1.2–9.  21. [M20]  R. W. Gosper.  A certain application uses random numbers in batches of four, but “throws away” the second of each set. How can we study the grid structure  of 1 m X4n, X4n+2, X4n+3 , given a linear congruential generator of period m = 2e? maximum value4 3 π? What is the best upper bound on µ2, given that µ3 is very  22. [M46] What is the best upper bound on µ3, given that µ2 is very near its  2?  3 π  near its maximum value 4 23. [M46] Let Ui, Vj be vectors of real numbers with Ui · Vj = δij for 1 ≤ i, j ≤ t, and such that Ui · Ui = 1, 2Ui · Uj ≤ 1, 2Vi · Vj ≤ Vj · Vj for i ̸= j. How large can V1 · V1 be?  This question relates to the bounds in step S7, if both  23  and the transformation of exercise 18 b  fail to make any reductions. The maximum value known to be achievable is  t + 2  3, which occurs when U1 = I1, Uj = 1 3 Ij, V1 = I1 −  I2 + ··· + It   3, for 2 ≤ j ≤ t, where  I1, . . . , It  is the identity matrix; this construction is due to B. V. Alexeev.    cid:120  24. [M28] Generalize the spectral test to second-order sequences of the form Xn =  aXn−1 + bXn−2  mod p, having period length p2 − 1.  See Eq. 3.2.2– 8 .  How should 25. [HM24] Let d be a divisor of m and let 0 ≤ q < d. Prove that  r k , summed Algorithm S be modified?  √ √ 3, Vj = 2Ij   over all 0 ≤ k < m such that k mod d = q, is at most  2 dπ  ln m d  + O 1 .  Here r k  is defined in Eq.  46  when t = 1.  26. [M22] Explain why the derivation of  53  leads to a similar bound on  2 I1 + 1 2  √     0≤n<N    ωqxn  for 0 < q < m. 27. [HM39]  E. Hlawka, H. Niederreiter.  Let r u1, . . . , ut  be the function defined  u1, . . . , ut  ̸=  0, . . . , 0  and  15  holds, is at most 2  π + 2π lg m t rmax , where rmax is the maximum term r u1, . . . , ut  in the sum.  in  46 . Prove that  r u1, . . . , ut , summed over all 0 ≤ u1, . . . , ut < m such that  cid:120  28. [M28]  H. Niederreiter.  Find an analog of Theorem N for the case m = prime, m−1 = Ot log m t φ m − 1 , hence good primitive  c = 0, a = primitive root modulo m, X0 ̸≡ 0  modulo m . [Hint: Your exponential sums should involve ζ = e2πi  m−1  as well as ω.] Prove that in this case the “average” primitive root has discrepancy D roots exist for all m. √ 29. [HM22] Prove that the quantity rmax of exercise 27 is never larger than 1   8 νt . 30. [M33]  S. K. Zaremba.  Prove that rmax = O max a1, . . . , as  m  in two dimen- sions, where a1, . . . , as are the partial quotients obtained when Euclid’s algorithm is applied to m and a. [Hint: We have a m =   a1, . . . , as  , in the notation of Section 4.5.3; apply exercise 4.5.3–42.] 31. [HM48]  I. Borosh and H. Niederreiter.  Prove that for all sufficiently large m there exists a number a relatively prime to m such that all partial quotients of a m are ≤ 3. Furthermore the set of all m satisfying this condition but with all partial quotients ≤ 2 has positive density.   t    3.3.4  118  RANDOM NUMBERS   cid:120  32. [M21] Let m1 = 231 − 1 and m2 = 231 − 249 be the moduli of generator  38 .  a  Show that if Un =  Xn m1 − Yn m2  mod 1, we have Un ≈ Zn m1. b  Let W0 =  X0m2 − Y0m1  mod m and Wn+1 = aWn mod m, where a and m have the values stated in the text following  38 . Prove that there is a simple relation between Wn and Un. In the next edition of this book, I plan to introduce a new Section 3.3.5, entitled “The L3 Algorithm.” It will be a digression from the general topic of Random Numbers, but it will continue the discussion of lattice basis reduction in Section 3.3.4. Its main topic will be the now-classic algorithm of A. K. Lenstra, H. W. Lenstra, Jr., and L. Lovász [Math. Annalen 261  1982 , 515–534] for finding a near-optimum set of basis vectors, and improvements to that algorithm made subsequently by other researchers. Examples of the latter can be found in the following papers and their bibliographies: M. Seysen, Combinatorica 13  1993 , 363–375; C. P. Schnorr and H. H. Hörner, Lecture Notes in Comp. Sci. 921  1995 , 1–12.   3.4.1  NUMERICAL DISTRIBUTIONS  119  3.4. OTHER TYPES OF RANDOM QUANTITIES We have now seen how to make a computer generate a sequence of numbers U0, U1, U2, . . . that behaves as if each number were independently selected at random between zero and one with the uniform distribution. Applications of random numbers often call for other kinds of distributions, however; for example, if we want to make a random choice from among k alternatives, we want a random integer between 1 and k. If some simulation process calls for a random waiting time between occurrences of independent events, a random number with the exponential distribution is desired. Sometimes we don’t even want random numbers — we want a random permutation  a random arrangement of n objects  or a random combination  a random choice of k objects from a collection of n . In principle, any of these other random quantities can be obtained from the uniform deviates U0, U1, U2, . . . ; people have devised a number of important “random tricks” for the efficient transformation of uniform deviates. A study of these techniques also gives us insight into the proper use of random numbers in any Monte Carlo application.  It is conceivable that someday somebody will invent a random number generator that produces one of these other random quantities directly, instead of getting it indirectly via the uniform distribution. But no direct methods have as yet proved to be practical, except for the “random bit” generator described in Section 3.2.2.  See also exercise 3.4.1–31, where the uniform distribution is used primarily for initialization, after which the method is almost entirely direct.   The discussion in the following section assumes the existence of a random sequence of uniformly distributed real numbers between zero and one. A new uniform deviate U is generated whenever we need it. These numbers are usually represented in a computer word with the radix point assumed at the left.  3.4.1. Numerical Distributions This section summarizes the best techniques known for producing numbers from various important distributions. Many of the methods were originally suggested by John von Neumann in the early 1950s, and they have gradually been improved upon by other people, notably George Marsaglia, J. H. Ahrens, and U. Dieter. A. Random choices from a finite set. The simplest and most common type of distribution required in practice is a random integer. An integer between 0 and 7 can be extracted from three bits of U on a binary computer; in such a case, these bits should be extracted from the most significant  left-hand  part of the computer word, since the least significant bits produced by many random number generators are not sufficiently random.  See the discussion in Section 3.2.1.1.  In general, to get a random integer X between 0 and k − 1, we can multiply by k, and let X = ⌊kU⌋. On MIX, we would write  LDA U MUL K   1    120  RANDOM NUMBERS  3.4.1  and after these two instructions have been executed the desired integer will appear in register A. If a random integer between 1 and k is desired, we add one  to this result. The instruction ‘INCA 1’ would follow  1 .  This method gives each integer with nearly equal probability. There is a slight error because the computer word size is finite  see exercise 2 ; but the error is quite negligible if k is small, for example if k m < 1 10000.  In a more general situation we might want to give different weights to different integers. Suppose that the value X = x1 is to be obtained with probability p1, and X = x2 with probability p2, . . . , X = xk with probability pk. We can generate a uniform number U and let if 0 ≤ U < p1; if p1 ≤ U < p1 + p2;  x1, x2,   2     ...  xk,  X =  if p1 + p2 + ··· + pk−1 ≤ U < 1.  36, 2  36, . . . , 6   Note that p1 + p2 + ··· + pk = 1.  There is a “best possible” way to do the comparisons of U against various values of p1 + p2 + ··· + ps, as implied in  2 ; this situation is discussed in Section 2.3.4.5. Special cases can be handled by more efficient methods; for example, to obtain one of the eleven values 2, 3, . . . , 12 with the respective “dice” probabilities 1 36, we could compute two independent random integers between 1 and 6 and add them together. However, there is actually a faster way to select x1, . . . , xk with arbitrarily given probabilities, based on an ingenious approach introduced by A. J. Walker [Electronics Letters 10, 8  1974 , 127–128; ACM Trans. Math. Software 3  1977 , 253–256]. Suppose we form kU and consider the integer part K = ⌊kU⌋ and fraction part V =  kU  mod 1 separately; for example, after the code  1  we will have K in register A and V in register X. Then we can always obtain the desired distribution by doing the operations  36, . . . , 2  36, 1  if V < PK then X ← xK+1   3  for some appropriate tables  P0, . . . , Pk−1  and  Y0, . . . , Yk−1 . Exercise 7 shows how such tables can be computed in general. Walker’s method is sometimes called the method of “aliases.”  otherwise X ← YK,  On a binary computer it is usually helpful to assume that k is a power of 2, so that multiplication can be replaced by shifting; this can be done without loss of generality by introducing additional x’s that occur with probability zero. For example, let’s consider dice again; suppose we want X = j to occur with the following 16 probabilities:  j = 0 1 2 1 pj = 0 0 36  3 2 36  4 3 36  5 4 36  6 5 36  7 6 36  8 5 36  9 10 11 12 13 14 15 4 0 36  1 36 0  2 36  3 36  0   4 9  7 9 4  3 8 9 4  4 1 ∗  6 1 ∗  7 1 ∗  8 1 ∗  NUMERICAL DISTRIBUTIONS  j = 0 1 2 4 Pj = 0 0 9 Yj = 5 9 7  3.4.1 121 We can do this using  3 , if k = 16 and xj+1 = j for 0 ≤ j < 16, and if the P and Y tables are set up as follows: 5 7 9 6  9 10 11 12 13 14 15 7 0 9 8 8  16 · 1− P2  + P7 +  1− P11  +  1− P14  = 6   When Pj = 1, Yj is not used.  For example, the value 7 occurs with probability 1 36 as required. It is a peculiar way to throw dice, but the results are indistinguishable from the real thing. The probabilities pj can be represented implicitly by nonnegative weights w1, w2, . . . , wk; if we denote the sum of the weights by W, then pj = wj W. In many applications the individual weights vary dynamically. Matias, Vitter, and Ni [SODA 4  1993 , 361–370] have shown how to update a weight and generate X in constant expected time. B. General methods for continuous distributions. The most general real- valued distribution can be expressed in terms of its “distribution function” F x , which specifies the probability that a random quantity X will not exceed x:  8 0 9 7 10 6  0 7  F x  = Pr X ≤ x .   4   if x1 ≤ x2;  F x1  ≤ F x2 ,  This function always increases monotonically from zero to one; that is, F +∞  = 1.  5  Examples of distribution functions are given in Section 3.3.1, Fig. 3. If F x  is continuous and strictly increasing  so that F x1  < F x2  when x1 < x2 , it takes on all values between zero and one, and there is an inverse function F [−1] y  such that, for 0 < y < 1,  F −∞  = 0,  y = F x    6  In general, when F x  is continuous and strictly increasing, we can compute a random quantity X with distribution F x  by setting  if and only if  x = F [−1] y .  X = F [−1] U ,   7  where U is uniform. This works because the probability that X ≤ x is the prob- ability that F [−1] U  ≤ x, namely the probability that U ≤ F x , namely F x . The problem now reduces to one of numerical analysis, namely to find good methods for evaluating F [−1] U  to the desired accuracy. Numerical analysis lies outside the scope of this seminumerical book; yet a number of important shortcuts are available to speed up the general approach of  7 , and we will consider them here.  In the first place, if X1 is a random variable having the distribution F1 x  and if X2 is an independent random variable with the distribution F2 x , then  max X1, X2  has the distribution F1 x F2 x , min X1, X2  has the distribution F1 x  + F2 x  − F1 x F2 x .   8    122  RANDOM NUMBERS  3.4.1  √  if U1, U2,   See exercise 4.  For example, a uniform deviate U has the distribution F x  = x, for 0 ≤ x ≤ 1; . . . , Ut are independent uniform deviates, then max U1, U2, . . . , Ut  has the distribution function F x  = xt, for 0 ≤ x ≤ 1. This formula is the basis of the “maximum-of-t test” given in Section 3.3.2; the inverse function is F [−1] y  = t√ y. In the special case t = 2, we see therefore that the two formulas  U  and  X =  X = max U1, U2    9  will give equivalent distributions to the random variable X, although this is not obvious at first glance. We need not take the square root of a uniform deviate. The number of tricks like this is endless: Any algorithm that employs random numbers as input will give a random quantity with some distribution as output. The problem is to find general methods for constructing the algorithm, given the distribution function of the output. Instead of discussing such methods in purely abstract terms, we shall study how they can be applied in important cases. C. The normal distribution. Perhaps the most important nonuniform, con- tinuous distribution is the normal distribution with mean zero and standard deviation one:   x  −∞  F x  = 1√ 2π  e−t2 2 dt.   10   The significance of this distribution was indicated in Section 1.2.10. In this case the inverse function F [−1] is not especially easy to compute; but we shall see that several other techniques are available. 1  The polar method, due to G. E. P. Box, M. E. Muller, and G. Marsaglia.  See Annals Math. Stat. 29  1958 , 610–611; and Boeing Scientific Res. Lab. report D1-82-0203  1962 .  Algorithm P  Polar method for normal deviates . This algorithm calculates two independent normally distributed variables, X1 and X2. P1. [Get uniform variables.] Generate two independent random variables, U1 and U2, uniformly distributed between zero and one. Set V1 ← 2U1 − 1, V2 ← 2U2 − 1.  Now V1 and V2 are uniformly distributed between −1 and +1. On most computers it will be preferable to have V1 and V2 represented in floating point form.   P2. [Compute S.] Set S ← V 2 P3. [Is S ≥ 1?] If S ≥ 1, return to step P1.  Steps P1 through P3 are executed 1.27 times on the average, with a standard deviation of 0.59; see exercise 6.   1 + V 2 2 .  P4. [Compute X1, X2.] If S = 0, set X1 ← X2 ← 0; otherwise set  −2 ln S  ,  S  −2 ln S  .  S  X1 ← V1  X2 ← V2   11   These are the normally distributed variables desired.   3.4.1  NUMERICAL DISTRIBUTIONS  123  To prove the validity of this method, we use elementary analytic geometry and calculus: If S < 1 in step P3, the point in the plane with Cartesian coordinates  V1, V2  is a random point uniformly distributed inside the unit circle. Transforming to polar coordinates V1 = R cos Θ, V2 = R sin Θ, we find √−2 ln S sin Θ.  √−2 ln S cos Θ, X2 =  S = R2, X1 =  Using also the polar coordinates X1 = R′ cos Θ′, X2 = R′ sin Θ′, we find that √−2 ln S. It is clear that R′ and Θ′ are independent, since Θ′ = Θ and R′ = R and Θ are independent inside the unit circle. Also, Θ′ is uniformly distributed between 0 and 2π; and the probability that R′ ≤ r is the probability that −2 ln S ≤ r2, namely the probability that S ≥ e−r2 2. This equals 1 − e−r2 2, since S = R2 is uniformly distributed between zero and one. The probability that R′ lies between r and r + dr is therefore the differential of 1 − e−r2 2, namely re−r2 2 dr. Similarly, the probability that Θ′ lies between θ and θ + dθ is  1 2π  dθ. The joint probability that X1 ≤ x1 and that X2 ≤ x2 now can be computed; it is    { r,θ   r cos θ≤x1, r sin θ≤x2}  e−r2 2 r dr dθ  1 2π = 1 2π    1  =   x1  { x,y   x≤x1, y≤x2} e−x2 2 dx  2π  −∞  e− x2+y2  2 dx dy   1   x2  2π  −∞    e−y2 2 dy  .  This calculation proves that X1 and X2 are independent and normally distrib- uted, as desired. 2  The rectangle-wedge-tail method, introduced by G. Marsaglia. Here we use the function  √  F x  = erf x   2   =  e−t2 2 dt,  x ≥ 0,   12    2   x  π  0  which gives the distribution of the absolute value of a normal deviate. After X has been computed according to distribution  12 , we will attach a random sign to its value, and this will make it a true normal deviate.  The rectangle-wedge-tail approach is based on several important general techniques that we shall explore as we develop the algorithm. The first key idea is to regard F x  as a mixture of several other functions, namely to write  F x  = p1F1 x  + p2F2 x  + ··· + pnFn x ,   13  where F1, F2, . . . , Fn are appropriate distributions and p1, p2, . . . , pn are nonnegative probabilities that sum to 1. If we generate a random variable X by choosing distribution Fj with probability pj, it is easy to see that X will have distribution F overall. Some of the distributions Fj x  may be rather difficult to handle, even harder than F itself, but we can usually arrange things so that the   124  RANDOM NUMBERS  3.4.1  Fig. 9. The density function divided into 31 parts. The area of each part represents the average number of times a random number with that density is to be computed.  probability pj is very small in that case. Most of the distributions Fj x  will be quite easy to accommodate, since they will be trivial modifications of the uniform distribution. The resulting method yields an extremely efficient program, since its average running time is very small.  It is easier to understand the method if we work with the derivatives of the  distributions instead of the distributions themselves. Let ′ x   f x  = F ′ x ,  fj x  = Fj  f x  = p1f1 x  + p2f2 x  + ··· + pnfn x .  be the density functions of the probability distributions. Equation  13  becomes  14  Each fj x  is ≥ 0, and the total area under the graph of fj x  is 1; so there is a convenient graphical way to display the relation  14 : The area under f x  is divided into n parts, with the part corresponding to fj x  having area pj. See Fig. 9, which illustrates the situation in the case of interest to us here, with  f x  = F ′ x  =2 π e−x2 2; the area under this curve has been divided into n =  31 parts. There are 15 rectangles, which represent p1f1 x , . . . , p15f15 x ; there are 15 wedge-shaped pieces, which represent p16f16 x , . . . , p30f30 x ; and the remaining part p31f31 x  is the “tail,” namely the entire graph of f x  for x ≥ 3. The rectangular parts f1 x , . . . , f15 x  represent uniform distributions. For example, f3 x  represents a random variable uniformly distributed between 5 and 3 2 5. The altitude of pjfj x  is f j 5 , hence the area of the jth rectangle is   2  25π  pj = 1  5 f j 5  =  e−j2 50,  for 1 ≤ j ≤ 15.  In order to generate such rectangular portions of the distribution, we simply compute   15    16   X = 1  5 U + S,  f16  f17  f18  f19  f20  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0.0  f21  f22  f23  f24  f25f26f27  f29  f28  f31  f30  f1 f2 f3 f4 f5 f6 f7 0  1  2  3  4   3.4.1  NUMERICAL DISTRIBUTIONS  125  Fig. 10. Density functions for which Algorithm L may be used to generate random numbers. where U is uniform and S takes the value  j − 1  5 with probability pj. Since p1 + ··· + p15 = .9183, we can use simple uniform deviates like this about 92 percent of the time. In the remaining 8 percent, we will usually have to generate one of the wedge-shaped distributions F16, . . . , F30. Typical examples of what we need to do are shown in Fig. 10. When x < 1, the curved part is concave, and when x > 1 it is convex, but in each case the curved part is reasonably close to a straight line, and it can be enclosed in two parallel lines as shown.  To handle these wedge-shaped distributions, we will rely on yet another general technique, von Neumann’s rejection method for obtaining a complicated density from another one that “encloses” it. The polar method described above is a simple example of such an approach: Steps P1–P3 obtain a random point inside the unit circle by first generating a random point in a larger square, rejecting it and starting over again if the point was outside the circle.  f t  ≤ cg t   The general rejection method is even more powerful than this. To generate a random variable X with density f, let g be another probability density function such that  17  for all t, where c is a constant. Now generate X according to density g, and also generate an independent uniform deviate U. If U ≥ f X  cg X , reject X and start again with another X and U. When the condition U < f X  cg X  finally  occurs, the resulting X will have density f as desired. Proof: X ≤ x will occur g t  dt · f t  cg t  + qp x , where the quantity with probability p x  =  x g t  dt· 1− f t  cg t   = 1−1 c is the probability of rejection; hence q = ∞ −∞ f t  dt. p x  = x  The rejection technique is most efficient when c is small, since there will be c iterations on the average before a value is accepted.  See exercise 6.  In some cases f x  cg x  is always 0 or 1; then U need not be generated. In other cases if f x  cg x  is hard to compute, we may be able to “squeeze” it between two bounding functions  −∞  −∞  r x  ≤ f x  cg x  ≤ s x    18   b aa  0  s  s+h  b aa  0  s  s+h   126  RANDOM NUMBERS  3.4.1  Fig. 11. Region of “acceptance” in Algorithm L.  that are much simpler, and the exact value of f x  cg x  need not be calculated unless r x  ≤ U < s x . The following algorithm solves the wedge problem by developing the rejection method still further. Algorithm L  Nearly linear densities . This algorithm may be used to gen- erate a random variable X for any distribution whose density f x  satisfies the following conditions  see Fig. 10 : f x  = 0,  for x   s + h; for s ≤ x ≤ s + h.   19   a − b x − s  h ≤ f x  ≤ b − b x − s  h,  L1. [Get U ≤ V.] Generate two independent random variables U and V, uni-  formly distributed between zero and one. If U > V, exchange U ↔ V.  L2. [Easy case?] If V ≤ a b, go to L4. L3. [Try again?] If V > U +  1 b f s + hU , go back to step L1.  If a b is close  to 1, this step of the algorithm will not be necessary very often.   L4. [Compute X.] Set X ← s + hU.  When step L4 is reached, the point  U, V   is a random point in the area shaded in Fig. 11, namely, 0 ≤ U ≤ V ≤ U +  1 b f s + hU . Conditions  19  ensure that  ≤ U + 1  f s + hU  ≤ 1.  a b  Now the probability that X ≤ s + hx, for 0 ≤ x ≤ 1, is the area that lies to the left of the vertical line U = x in Fig. 11, divided by the total area, namely   x  0  1 b   1  0  b  1 b  f s + hu  du  f s + hu  du =  f v  dv;   s+hx  s  therefore X has the correct distribution. With appropriate constants aj, bj, sj, Algorithm L will take care of the wedge-shaped densities fj+15 of Fig. 9, for 1 ≤ j ≤ 15. The final distribution, F31, needs to be treated only about one time in 370; it is used whenever a result X ≥ 3 is to be computed. Exercise 11 shows that a standard rejection scheme can be used for this “tail.” We are ready to consider the procedure in its entirety:  1 a b  V  0  U  x   3.4.1  NUMERICAL DISTRIBUTIONS  127  Fig. 12. The “rectangle-wedge-tail” algorithm for generating normal deviates.  Algorithm M  Rectangle-wedge-tail method for normal deviates . For this algorithm we use auxiliary tables  P0, . . . , P31 ,  Q1, . . . , Q15 ,  Y0, . . . , Y31 ,  Z0, . . . , Z31 ,  S1, . . . , S16 ,  D16, . . . , D30 ,  E16, . . . , E30 , constructed as ex- plained in exercise 10; examples appear in Table 1. We assume that a binary computer is being used; a similar procedure could be worked out for decimal machines. M1. [Get U.] Generate a uniform random number U =  .b0b1b2 . . . bt 2.  Here the b’s are the bits in the binary representation of U. For reasonable accuracy, t should be at least 24.  Set ψ ← b0.  Later, ψ will be used to determine the sign of the result.   M2. [Rectangle?] Set j ←  b1b2b3b4b5 2, a binary number determined by the leading bits of U, and set f ←  .b6b7 . . . bt 2, the fraction determined by the remaining bits. If f ≥ Pj, set X ← Yj + f Zj and go to M9. Otherwise  if j ≤ 15  that is, b1 = 0 , set X ← Sj + f Qj and go to M9. This is an adaptation of Walker’s alias method  3 .  M3. [Wedge or tail?]  Now 16 ≤ j ≤ 31, and each particular value j occurs with  probability pj.  If j = 31, go to M7.  M4. [Get U ≤ V.] Generate two new uniform deviates, U and V; if U > V, exchange U ↔ V.  We are now performing a special case of Algorithm L.  Set X ← Sj−15 + 1  5 U.  M5. [Easy case?] If V ≤ Dj, go to M9.  M1. Get U  M2. Rectangle?  Yes  No  Tail  M3. Wedge or tail?  Wedge  M4. Get U ≤ V  M5. Easy case?  Yes  No  Yes  M6. Another try?  No  M7. Get supertail deviate  M9. Attach sign  M8. Reject?  No  Yes   128  RANDOM NUMBERS  3.4.1  Table 1  j 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  Pj .000 .849 .970 .855 .994 .995 .933 .923 .727 1.000 .691 .454 .287 .174 .101 .057  Qj  Zj 0.20 1.32 6.66 1.38 34.96 41.31 2.97 2.61 0.73  EXAMPLE OF TABLES USED WITH ALGORITHM M* Pj+16 .067 .161 .236 .285 .308 .304 .280 .241 .197 .152 .112 .079 .052 .033 .020 .086  Yj Yj+16 0.00 0.59 .236 − 0.92 0.96 .206 − 5.86 −0.06 .234 − 0.58 0.12 .201 −33.16 1.31 .201 −39.51 0.31 1.12 .214 − 2.57 0.54 .217 − 1.61 0.67 .275 0.75 .200 0.56 0.35 0.17 .289 0.38 .440 − 0.17 .698 0.92 −0.01 0.39 0.36 1.150 1.974 − 0.02 0.20 0.78 0.19 3.526  Zj+16 Sj+1 Dj+15 Ej+15 0.21 0.24 0.26 0.28 0.29 0.29 0.28 0.26 0.25 0.24 0.23 0.22 0.21 0.21 0.20 0.22  25.00 12.50 8.33 6.25 5.00 4.06 3.37 2.86 2.47 2.16 1.92 1.71 1.54 1.40 1.27  .505 .773 .876 .939 .986 .995 .987 .979 .972 .966 .960 .954 .948 .942 .936  0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0  0.65 0.37 0.28 0.24 0.22 0.21  *In practice, this data would be given with much greater precision; the table shows only enough figures so that interested readers will be able to test their own algorithms for computing the values more accurately. The values of Q0, Y9, Z9, D15, and E15 are not used. M6. [Another try?]  j−14−X2  2 − 1 , go back to step M4;  If V > U + Ej e S2  M7. [Get supertail deviate.] Generate two new independent uniform deviates,  otherwise go to M9.  This step is executed with low probability.  U and V, and set X ← √ one-twelfth as often as we reach step M8.   If U X ≥ 3, go back to step M7.  This will occur only about  M8. [Reject?]  9 − 2 ln V .  M9. [Attach sign.] If ψ = 1, set X ← −X.  This algorithm is a very pretty example of mathematical theory intimately interwoven with programming ingenuity — a fine illustration of the art of com- puter programming! Only steps M1, M2, and M9 need to be performed most of the time, and the other steps aren’t terribly slow either. The first publica- tions of the rectangle-wedge-tail method were by G. Marsaglia, Annals Math. Stat. 32  1961 , 894–899; G. Marsaglia, M. D. MacLaren, and T. A. Bray, CACM 7  1964 , 4–10. Further refinements of Algorithm M have been developed by G. Marsaglia, K. Ananthanarayanan, and N. J. Paul, Inf. Proc. Letters 5  1976 , 27–30. 3  The odd-even method, due to G. E. Forsythe. An amazingly simple technique for generating random deviates with a density of the general exponential form  20   f x  = Ce−h x  [a≤ x < b],  when  0 ≤ h x  ≤ 1   21  was discovered by John von Neumann and G. E. Forsythe about 1950. The idea is based on the rejection method described earlier, letting g x  be the uniform distribution on [a . . b : We set X ← a +  b − a U, where U is a uniform deviate,  for a ≤ x < b,   NUMERICAL DISTRIBUTIONS  3.4.1 129 and then we want to accept X with probability e−h X . The latter operation could be done by comparing e−h X  to V, or h X  to − ln V, when V is another uniform deviate, but the job can be done without applying any transcendental functions in the following interesting way. Set V0 ← h X , then generate uniform deviates V1, V2, . . . until finding some K ≥ 1 with VK−1 < VK. For fixed X and k, the probability that h X  ≥ V1 ≥ ··· ≥ Vk is 1 k! times the probability that max V1, . . . , Vk  ≤ h X , namely h X k k!; hence the probability that K = k is h X k−1  k − 1 ! − h X k k!, and the probability that K is odd is     h X k−1  k − 1 ! − h X k k!    k odd, k≥1  = e−h X .   22   Therefore we reject X and try again if K is even; we accept X as a random variable with density  20  if K is odd. We usually won’t have to generate many V ’s in order to determine K, since the average value of K  given X   is  k≥0 Pr K > k  =  k≥0 h X k k! = eh X  ≤ e.  Forsythe realized some years later that this approach leads to an efficient method for calculating normal deviates, without the need for any auxiliary routines to calculate square roots or logarithms as in Algorithms P and M. His procedure, with an improved choice of intervals [a . . b  due to J. H. Ahrens and U. Dieter, can be summarized as follows. Algorithm F  Odd-even method for normal deviates . This algorithm generates normal deviates on a binary computer, assuming approximately t + 1 bits of accuracy. It requires a table of values dj = aj − aj−1, for 1 ≤ j ≤ t + 1, where  aj is defined by the relation 2   ∞  e−x2 2 dx = 1 2j .  π  aj   23   F1. [Get U.] Generate a uniform random number U =  .b0b1 . . . bt 2, where b0, b1, . . . , bt denote the bits in binary notation. Set ψ ← b0, j ← 1, and a ← 0. F2. [Find first zero bj.] If bj = 1, set a ← a + dj, j ← j + 1, and repeat this  F3. [Generate candidate.] Now a = aj−1, and the current value of j occurs with Exercise 12 proves that h x  ≤ 1 as required in  21 . Set Y ← dj times  step.  If j = t + 1, treat bj as zero.  probability ≈ 2−j. We will generate X in the range [aj−1 . . aj , using the rejection method above, with h x  = x2 2−a2 2 = y2 2+ay where y = x−a.  .bj+1 . . . bt 2 and V ←   1 2 Y + a Y.  Since the average value of j is 2, there will usually be enough significant bits in  .bj+1 . . . bt 2 to provide decent accuracy. The calculations are readily done in fixed point arithmetic.   F4. [Reject?] Generate a uniform deviate U.  If V < U, go on to step F5. Otherwise set V to a new uniform deviate; and repeat step F4 if the new V is ≤ U. Otherwise  that is, if K is even, in the discussion above , replace U by a new uniform deviate  .b0b1 . . . bt 2 and go back to F3.  F5. [Return X.] Set X ← a + Y. If ψ = 1, set X ← −X.   130  RANDOM NUMBERS  3.4.1  Fig. 13. Region of “acceptance” in the ratio-of-uniforms method for normal deviates. Lengths of lines with coordinate ratio x have the normal distribution.  Values of dj for 1 ≤ j ≤ 47 appear in a paper by Ahrens and Dieter, Math. Comp. 27  1973 , 927–937; their paper discusses refinements of the algorithm that improve its speed at the expense of more tables. Algorithm F is attractive since it is almost as fast as Algorithm M and it is easier to implement. The average number of uniform deviates per normal deviate is 2.53947; R. P. Brent [CACM 17  1974 , 704–705] has shown how to reduce this number to 1.37446 at the expense of two subtractions and one division per uniform deviate saved. 4  Ratios of uniform deviates. There is yet another good way to generate normal deviates, discovered by A. J. Kinderman and J. F. Monahan in 1976. Their idea is to generate a random point  U, V   in the region defined by  −2u ln 1 u  ≤ v ≤ 2u ln 1 u ,  0 < u ≤ 1,   24  and then to output the ratio X ← V U. The shaded area of Fig. 13 is the magic region  24  that makes this all work. Before we study the associated theory, let us first state the algorithm so that its efficiency and simplicity are manifest: Algorithm R  Ratio method for normal deviates . This algorithm generates normal deviates X. R1. [Get U, V.] Generate two independent uniform deviates U and V, where  Now X is the ratio of  U is nonzero, and set X ← 8 eV − 1 the coordinatesU,8 eV − 1   of a random point in the rectangle that    U.  encloses the shaded region in Fig. 13. We will accept X if the corresponding point actually lies “in the shade,” otherwise we will try again.   2  2   0, cid:112 2 e   x = 3  x = 1   1, cid:112 2 e   x = 1 3  x = 0  x = −1 3   1, − cid:112 2 e   x = −1   0, − cid:112 2 e   x = −3   3.4.1 131 R2. [Optional upper bound test.] If X2 ≤ 5 − 4e1 4U, output X and terminate the algorithm.  This step can be omitted if desired; it tests whether or not the selected point is in the interior region of Fig. 13, making it unnecessary to calculate a logarithm.   NUMERICAL DISTRIBUTIONS  R3. [Optional lower bound test.] If X2 ≥ 4e−1.35 U + 1.4, go back to R1.  This step could also be omitted; it tests whether or not the selected point is outside the exterior region of Fig. 13, making it unnecessary to calculate a logarithm.  R4. [Final test.]  If X2 ≤ −4 ln U, output X and terminate the algorithm.  Otherwise go back to R1. Exercises 20 and 21 work out the timing analysis; four different algorithms are analyzed, since steps R2 and R3 can be included or omitted depending on one’s preference. The following table shows how many times each step will be performed, on the average, depending on which of the optional tests is applied:  Step R1 R2 R3 R4  Neither 1.369  0 0  1.369  R2 only 1.369 1.369  0  0.467  R3 only 1.369  0  1.369 1.134  Both 1.369 1.369 0.467 0.232   25   Thus it pays to omit the optional tests if there is a very fast logarithm operation, but if the log routine is rather slow it pays to include them. But why does it work? One reason is that we can calculate the probability that X ≤ x, and it turns out to be the correct value  10 . But such a calculation isn’t very easy unless one happens to hit on the right trick, and anyway it is better to understand how the algorithm might have been discovered in the first place. Kinderman and Monahan derived it by working out the following theory that can be used with any well-behaved density function f x  [see ACM Trans. Math. Software 3  1977 , 257–260].  In general, suppose that a point  U, V   has been generated uniformly over  the region of the  u, v -plane defined by  u > 0,   26  for some nonnegative integrable function g. If we set X ← V U, the probability that X ≤ x can be calculated by integrating du dv over the region defined by the two relations in  26  plus the auxiliary condition v u ≤ x, then dividing by the same integral without this extra condition. Letting v = tu, so that dv = u dt,  u2 ≤ g v u   the integral becomes x  Hence the probability that X ≤ x is  −∞   x  −∞  g t  dt.  dt   √  x  0  −∞  g t   u du = 1 2   +∞  −∞  g t  dt  g t  dt.   27    132  RANDOM NUMBERS  3.4.1 The normal distribution comes out when g t  = e−t2 2; and the condition u2 ≤ g v u  simplifies in this case to  v u 2 ≤ −4 ln u. It is easy to see that the set of all such pairs  u, v  is entirely contained in the rectangle of Fig. 13.  The bounds in steps R2 and R3 define interior and exterior regions with  simpler boundary equations. The well-known inequality  ex ≥ 1 + x,  which holds for all real numbers x, can be used to show that 1 + ln c − cu ≤ − ln u ≤ 1  cu  − 1 + ln c   28  for any constant c > 0. Exercise 21 proves that c = e1 4 is the best possible constant to use in step R2. The situation is more complicated in step R3, and there doesn’t seem to be a simple expression for the optimum c in that case, but computational experiments show that the best value for R3 is ≈ e1.35. The approximating curves  28  are tangent to the true boundary when u = 1 c.  With an improved approximation to the acceptance region [see J. L. Leva, ACM Trans. Math. Software 18  1992 , 449–455] we can, in fact, reduce the expected number of logarithm computations to only 0.012.  It is possible to obtain a faster method by partitioning the region into subregions, most of which can be handled more quickly. Of course, this means that auxiliary tables will be needed, as in Algorithms M and F. An interesting alternative that requires fewer auxiliary table entries has been suggested by Ahrens and Dieter in CACM 31  1988 , 1330–1337. 5  Normal deviates from normal deviates. Exercise 31 discusses an interesting approach that saves time by working directly with normal deviates instead of basing everything on uniform deviates. This method, introduced by C. S. Wallace in 1996, has comparatively little theoretical support at the present time, but it has successfully passed a number of empirical tests. 6  Variations of the normal distribution. So far we have considered the normal distribution with mean zero and standard deviation one. If X has this distribu- tion, then   29  has the normal distribution with mean µ and standard deviation σ. Furthermore, if X1 and X2 are independent normal deviates with mean zero and standard deviation one, and if  Y = µ + σX  Y2 = µ2 + σ2  Y1 = µ1 + σ1X1,   30  then Y1 and Y2 are dependent random variables, normally distributed with means µ1, µ2 and standard deviations σ1, σ2, and with correlation coefficient ρ.  For a generalization to n variables, see exercise 13.  D. The exponential distribution. After uniform deviates and normal de- viates, the next most important random quantity is an exponential deviate. Such numbers occur in “arrival time” situations; for example, if a radioactive  ρX1 +1 − ρ2 X2  ,   3.4.1  NUMERICAL DISTRIBUTIONS  133  substance emits alpha particles at a rate such that one particle is emitted every µ seconds on the average, then the time between two successive emissions has the exponential distribution with mean µ. This distribution is defined by the formula   31  1  Logarithm method. Clearly, if y = F x  = 1 − e−x µ, then x = F [−1] y  = −µ ln 1− y . Therefore −µ ln 1− U  has the exponential distribution by Eq.  7 . Since 1 − U is uniformly distributed when U is, we conclude that  F x  = 1 − e−x µ,  x ≥ 0.  X = −µ ln U   32  is exponentially distributed with mean µ.  The case U = 0 must be treated specially; we can substitute any convenient value ϵ for 0, since the probability of this case is extremely small.  2  Random minimization method. We saw in Algorithm F that there are simple and fast alternatives to calculating the logarithm of a uniform deviate. The following especially efficient approach has been developed by G. Marsaglia, M. Sibuya, and J. H. Ahrens [see CACM 15  1972 , 876–877]: Algorithm S  Exponential distribution with mean µ . This algorithm produces exponential deviates on a binary computer, using uniform deviates with  t + 1 - bit accuracy. The constants Q[k] = ln 2  1! +  ln 2 2  2! + ··· +  ln 2 k k!  should be precomputed, extending until Q[k] > 1 − 2−t. S1. [Get U and shift.] Generate a  t + 1 -bit uniform random binary fraction U =  .b0b1b2 . . . bt 2; locate the first zero bit bj, and shift off the leading j +1 bits, setting U ←  .bj+1 . . . bt 2.  As in Algorithm F, the average number of discarded bits is 2.  S2. [Immediate acceptance?] If U < ln 2, set X ← µ j ln 2 + U  and terminate  k ≥ 1,   33   ,  the algorithm.  Note that Q[1] = ln 2.   S3. [Minimize.] Find the least k ≥ 2 such that U < Q[k]. Generate k new  uniform deviates U1, . . . , Uk and set V ← min U1, . . . , Uk .  S4. [Deliver the answer.] Set X ← µ j + V   ln 2.  Alternative ways to generate exponential deviates  for example, a ratio of  uniforms as in Algorithm R  might also be used. E. Other continuous distributions. Let us now consider briefly how to handle some other distributions that arise reasonably often in practice. 1  The gamma distribution of order a > 0 is defined by  ta−1e−t dt,  x ≥ 0.   34    x  0  F x  = 1 Γ a    134  RANDOM NUMBERS  3.4.1  When a = 1, this is the exponential distribution with mean 1; when a = 1 2, it is the distribution of 1 2 Z2, where Z has the normal distribution  mean 0, variance 1 . If X and Y are independent gamma-distributed random variables, of order a and b, respectively, then X + Y has the gamma distribution of order a + b. Thus, for example, the sum of k independent exponential deviates with mean 1 has the gamma distribution of order k. If the logarithm method  32  is being used to generate these exponential deviates, we need compute only one logarithm: X ← − ln U1 . . . Uk , where U1, . . . , Uk are nonzero uniform deviates. This technique handles all integer orders a; to complete the picture, a suitable method for 0 < a < 1 appears in exercise 16. The simple logarithm method is much too slow when a is large, since it requires ⌊a⌋ uniform deviates. Moreover, there is a substantial risk that the product U1 . . . U⌊a⌋ will cause floating point underflow. For large a, the following algorithm due to J. H. Ahrens is reasonably efficient, and it is easy to write in terms of standard subroutines. [See Ann. Inst. Stat. Math. 13  1962 , 231–237.] Algorithm A  Gamma distribution of order a > 1 . A1. [Generate candidate.] Set Y ← tan πU , where U is a uniform deviate, and  method, calculating a ratio V2 V1 as in step P4 of Algorithm P. set X ← √ and return to A1 if V >  1 + Y 2  exp a − 1  lnX  a − 1  − √  2a − 1 Y + a − 1. In place of tan πU  we could use a polar 2a − 1 Y.  A2. [Accept?] If X ≤ 0, return to A1. Otherwise generate a uniform deviate V,  Otherwise accept X.  The average number of times step A1 is performed is < 1.902 when a ≥ 3.  There is also an attractive approach for large a based on the remarkable fact that gamma deviates are approximately equal to aX3, where X is normally √ distributed with mean 1−1  9a  and standard deviation 1  9a; see E. B. Wilson and M. M. Hilferty, Proc. Nat. Acad. Sci. 17  1931 , 684–688; G. Marsaglia, Computers and Math. 3  1977 , 321–325.*  For a somewhat complicated but significantly faster algorithm, which gener- ates a gamma deviate in about twice the time to generate a normal deviate, see J. H. Ahrens and U. Dieter, CACM 25  1982 , 47–54. This article contains an instructive discussion of the design principles used to construct the algorithm. 2  The beta distribution with positive parameters a and b is defined by  ta−1 1 − t b−1 dt,  0 ≤ x ≤ 1.   35   F x  = Γ a + b  Γ a  Γ b    x  0  Let X1 and X2 be independent gamma deviates of order a and b, respectively, and set X ← X1  X1 + X2 . Another method, useful for small a and b, is to set  1 b 2 repeatedly until Y1 + Y2 ≤ 1; then X ← Y1  Y1 + Y2 . [See M. D. Jöhnk, Metrika 8  1964 , 5–15.] Still another approach, if a and b are integers and not  Y1 ← U 1 a 1  Y2 ← U  and  * Change “+ 3a − 1 ” to “− 3a − 1 ” in Step 3 of the algorithm on page 323.   NUMERICAL DISTRIBUTIONS  3.4.1 135 too large, is to set X to the bth largest of a+ b−1 independent uniform deviates  see exercise 9 at the beginning of Chapter 5 . See also the more direct method described by R. C. H. Cheng, CACM 21  1978 , 317–322. 3  The chi-square distribution with ν degrees of freedom  Eq. 3.3.1– 22   is obtained by setting X ← 2Y, where Y is a random variable having the gamma distribution of order ν 2. 4  The F-distribution  variance-ratio distribution  with ν1 and ν2 degrees of freedom is defined by  F x  = νν1 2  νν2 2 2 Γ ν1 2  Γ ν2 2   1  tν1 2−1 ν2 + ν1t −ν1 2−ν2 2 dt,   36  where x ≥ 0. Let Y1 and Y2 be independent, having the chi-square distribution with ν1 and ν2 degrees of freedom, respectively; set X ← Y1ν2 Y2ν1. Or set X ← ν2Y  ν1 1 − Y  , where Y is a beta variate with parameters ν1 2 and ν2 2. 5  The t-distribution with ν degrees of freedom is defined by  0  Γ ν1 + ν2  2   x  F x  = Γ ν + 1  2  √  πν Γ ν 2    x  −∞   1 + t2 ν − ν+1  2 dt.   37   Y1 Y2 ν. Alternatively, when ν > 2, let Y1 be a normal deviate and let  Let Y1 be a normal deviate  mean 0, variance 1  and let Y2 be independent of Y1, having the chi-square distribution with ν degrees of freedom; set X ← Y2 independently have the exponential distribution with mean 2  ν − 2 ; set Z ← Y 2  1   ν − 2  and reject  Y1, Y2  if e−Y2−Z ≥ 1 − Z, otherwise set  X ← Y1  1 − 2 ν  1 − Z .  The latter method is due to George Marsaglia, Math. Comp. 34  1980 , 235–236. [See also A. J. Kinderman, J. F. Monahan, and J. G. Ramage, Math. Comp. 31  1977 , 1009–1018.] 6  Random point on an n-dimensional sphere with radius one. Let X1, X2, . . . , Xn be independent normal deviates  mean 0, variance 1 ; the desired point on the unit sphere is  2 + ··· + X2 n.  X2  1 + X2  where r =   X1 r, X2 r, . . . , Xn r ,   38  If the X’s are calculated using the polar method, Algorithm P, we compute two 2 = −2 ln S in the notation independent X’s each time, and we have X2 of that algorithm; this saves a little of the time needed to evaluate r. The validity of  38  comes from the fact that the distribution function for the point  X1, . . . , Xn  has a density that depends only on its distance from the origin, so when it is projected onto the unit sphere it has the uniform distribution. This method was first suggested by G. W. Brown, in Modern Mathematics for the Engineer, First series, edited by E. F. Beckenbach  New York: McGraw–Hill,  1 + X2     136  RANDOM NUMBERS  3.4.1  1956 , 302. To get a random point inside the n-sphere, R. P. Brent suggests taking a point on the surface and multiplying it by U 1 n. In three dimensions a significantly simpler method can be used, since each individual coordinate is uniformly distributed between −1 and 1: Find V1, V2, and S by steps P1–P3 of Algorithm P; then the desired random point on the √ surface of a globe is  αV1, αV2, 2S − 1 , where α = 2 1 − S. [Robert E. Knop, CACM 13  1970 , 326.] F. Important integer-valued distributions. A probability distribution that is nonzero only at integer values can essentially be handled by the techniques described at the beginning of this section; but some of these distributions are so important in practice, they deserve special mention here. 1  The geometric distribution. If some event occurs with probability p, the number N of independent trials needed between occurrences of the event  or until the event occurs for the first time  has the geometric distribution. We have N = 1 with probability p, N = 2 with probability  1 − p p, . . . , N = n with probability  1 − p n−1p. This is essentially the situation we have already considered in the gap test of Section 3.3.2; it is also directly related to the number of times certain loops in the algorithms of this section are executed, like steps P1–P3 of the polar method.  A convenient way to generate a variable with this distribution is to set  To check this formula, we observe that ln U   ln 1 − p  = n if and only if  n−1   U ≥  1− p n, and this happens with probability  1 − p n−1p as required. The quantity ln U can optionally be replaced by −Y, where Y has the exponential distribution with mean 1. 2 is quite simple on a binary computer, since for- mula  39  reduces to setting N ← ⌈− lg U⌉; that is, N is one more than the number of leading zero bits in the binary representation of U. 2  The binomial distribution  t, p . If some event occurs with probability p, and if we carry out t independent trials, the total number N of occurrences equals n   pn 1 − p t−n.  See Section 1.2.10.  In other words if we  with probability  t  The special case p = 1  N ←ln U   ln 1 − p .  generate U1, . . . , Ut, we want to count how many of these are < p. For small t we can obtain N in exactly this way. For large t, we can generate a beta variate X with integer parameters a and b where a + b − 1 = t; this effectively gives us the bth largest of t elements, without bothering to generate the other elements. Now if X ≥ p, we set N ← N1 where N1 has the binomial distribution  a−1, p X , since this tells us how many of a − 1 random numbers in the range [0 . . X  are < p; and if X < p, we set since N1 tells us how many of b−1 random numbers in the range [X . . 1  are < p. By choosing a = 1 + ⌊t 2⌋, the parameter t will be reduced to a reasonable size after about lg t reductions of this kind.  This approach is due to J. H. Ahrens, who has also suggested an alternative for medium-sized t; see exercise 27.   N ← a + N1 where N1 has the binomial distributionb − 1,  p − X   1 − X ,   39   n   3.4.1  NUMERICAL DISTRIBUTIONS  137  3  The Poisson distribution with mean µ. The Poisson distribution is related to the exponential distribution as the binomial distribution is related to the geometric: It represents the number of occurrences, per unit time, of an event that can occur at any instant of time. For example, the number of alpha particles emitted by a radioactive substance in a single second has a Poisson distribution. According to this principle, we can produce a Poisson deviate N by gener- ating independent exponential deviates X1, X2, . . . with mean 1 µ, stopping as soon as X1 + ··· + Xm ≥ 1; then N ← m − 1. The probability that X1 + ··· + Xm ≥ 1 is the probability that a gamma deviate of order m is ≥ µ, µ tm−1e−t dt  m − 1 !; hence the probability that N = n is  40   and this comes to ∞  tne−t dt −   ∞   ∞  n ≥ 0.  1  tn−1e−t dt = e−µ µn n! ,   n − 1 !  1 n!  µ  µ  If we generate exponential deviates by the logarithm method, the recipe above tells us to stop when − ln U1 + ··· + ln Um  µ ≥ 1. Simplifying this expression, we see that the desired Poisson deviate can be obtained by calculating e−µ, converting it to a fixed point representation, then generating one or more uniform deviates U1, U2, . . . until the product satisfies U1 . . . Um ≤ e−µ, finally setting N ← m−1. On the average this requires the generation of µ+1 uniform deviates, so it is a very useful approach when µ is not too large.  When µ is large, we can obtain a method of order log µ by using the fact that we know how to handle the gamma and binomial distributions for large orders: First generate X with the gamma distribution of order m = ⌊αµ⌋, where α is a suitable constant.  Since X is equivalent to − ln U1 . . . Um , we are essentially bypassing m steps of the previous method.  If X < µ, set N ← m + N1, where N1 is a Poisson deviate with mean µ − X; and if X ≥ µ, set N ← N1, where N1 has the binomial distribution  m − 1, µ X . This method is due to J. H. Ahrens and U. Dieter, whose experiments suggest that 7 8 is a good choice for α. The validity of the stated reduction when X ≥ µ is a consequence of the following important principle: “Let X1, . . . , Xm be independent exponential deviates with the same mean; let Sj = X1 + ··· + Xj and let Vj = Sj Sm for 1 ≤ j ≤ m. Then the distribution of V1, V2, . . . , Vm−1 is the same as the distribution of m − 1 independent uniform deviates sorted into increasing order.” To establish this principle formally, we compute the probability that V1 ≤ v1, . . . , Vm−1 ≤ vm−1, given the value of Sm = s, for arbitrary values 0 ≤ v1 ≤ ··· ≤ vm−1 ≤ 1: Let f v1, v2, . . . , vm−1  be the  m − 1 -fold integral   v1s  0  then  µe−t1 µ dt1  0  µe−t2 µ dt2 . . .   v2s−t1  vm−1s−t1−···−tm−2 0 du1  v2  v1  1 0 du1  1  =  u1  0  u1  ×  f v1, v2, . . . , vm−1   f 1, 1, . . . , 1   µe−tm−1 µ dtm−1 · µe− s−t1−···−tm−1  µ ;  du2 . . . vm−1 du2 . . . 1  um−2  um−2  dum−1 dum−1  ,   RANDOM NUMBERS  138 3.4.1 by making the substitution t1 = su1, t1 + t2 = su2, . . . , t1 + ··· + tm−1 = sum−1. The latter ratio is the corresponding probability that uniform deviates U1, . . . , Um−1 satisfy U1 ≤ v1, . . . , Um−1 ≤ vm−1, given that they also satisfy U1 ≤ ··· ≤ Um−1. A more efficient but somewhat more complicated technique for binomial and Poisson deviates is sketched in exercise 22. G. For further reading. A facsimile of a letter from von Neumann dated May 21, 1947, in which the rejection method first saw the light of day, appears in Stanislaw Ulam 1909–1984, a special issue of Los Alamos Science  Los Alamos National Lab., 1987 , 135–136. The book Non-Uniform Random Variate Gen- eration by L. Devroye  Springer, 1986  discusses many more algorithms for the generation of random variables with nonuniform distributions, together with a careful consideration of the efficiency of each technique on typical computers.  W. Hörmann and G. Derflinger [ACM Trans. Math. Software 19  1993 , 489–495] have pointed out that it can be dangerous to use the rejection method in connection with linear congruential generators that have small multipliers a ≈ √ From a theoretical point of view it is interesting to consider optimal ways to generate random variables with a given distribution, in the sense that the method produces the desired result from the minimum possible number of random bits. For the beginnings of a theory dealing with such questions, see D. E. Knuth and A. C. Yao, Algorithms and Complexity, edited by J. F. Traub  New York: Academic Press, 1976 , 357–428.  Exercise 16 is recommended as a review of many of the techniques in this  m.  section.  EXERCISES 1. [10] If α and β are real numbers with α < β, how would you generate a random real number uniformly distributed between α and β? 2. [M16] Assuming that mU is a random integer between 0 and m − 1, what is the exact probability that ⌊kU⌋ = r, if 0 ≤ r < k? Compare this with the desired probability 1 k.   cid:120  3. [14] Discuss treating U as an integer and computing its remainder mod k to get  a random integer between 0 and k − 1, instead of multiplying as suggested in the text. Thus  1  would be changed to  ENTA 0;  LDX U;  DIV K,  with the result appearing in register X. Is this a good method? 4. [M20] Prove the two relations in  8 .   cid:120  5. [21] Suggest an efficient way to compute a random variable with the distribution  F  x  = px + qx2 + rx3, where p ≥ 0, q ≥ 0, r ≥ 0, and p + q + r = 1. 6. [HM21] A quantity X is computed by the following method:  Step 1. Generate two independent uniform deviates U and V. Step 2. If U 2 + V 2 ≥ 1, return to step 1; otherwise set X ← U.   3.4.1  NUMERICAL DISTRIBUTIONS  139  What is the distribution function of X? How many times will step 1 be performed?  Give the mean and standard deviation.    cid:120  7. [20]  A. J. Walker.  Suppose we have a bunch of cubes of k different colors, say  nj cubes of color Cj for 1 ≤ j ≤ k, and we also have k boxes {B1, . . . , Bk} each of which can hold exactly n cubes. Furthermore n1 + ··· + nk = kn, so the cubes will just fit in the boxes. Prove  constructively  that there is always a way to put the cubes into the boxes so that each box contains at most two different colors of cubes; in fact, there is a way to do it so that, whenever box Bj contains two colors, one of those colors is Cj. Show how to use this principle to compute the P and Y tables required in  3 , given a probability distribution  p1, . . . , pk . 8. [M15] Show that operation  3  could be changed to  if U < PK then X ← xK+1  otherwise X ← YK   thus using the original value of U instead of V   if this were more convenient, by suitably modifying P0, P1, . . . , Pk−1. 9. [HM10] Why is the curve f x  of Fig. 9 concave for x   1?   cid:120  10. [HM24] Explain how to calculate auxiliary constants Pj, Qj, Yj, Zj, Sj, Dj, Ej  cid:120  11. [HM27] Prove that steps M7–M8 of Algorithm M generate a random variable  so that Algorithm M delivers answers with the correct distribution.  with the appropriate tail of the normal distribution; in other words, the probability that X ≤ x should be exactly   x  3   ∞  3  e−t2 2  dt  e−t2 2  dt,  x ≥ 3.  [Hint: Show that it is a special case of the rejection method, with g t  = Cte−t2 2 for some C.] 12. [HM23]  R. P. Brent.  Prove that the numbers aj defined in  23  satisfy the relation  [Hint: If f x  = ex2 2 ∞  j − a 2 2 a  j−1 < 2 ln 2  for all j ≥ 1.  x e−t2 2 dt, show that f x  > f y  for 0 ≤ x < y.]  . . . ,  Y2 = b2 + a21X1 + a22X2,  13. [HM25] Given a set of n independent normal deviates, X1, X2, . . . , Xn, with mean 0 and variance 1, show how to find constants bj and aij, 1 ≤ j ≤ i ≤ n, so that if Yn = bn + an1X1 + ··· + annXn, Y1 = b1 + a11X1, then Y1, Y2, . . . , Yn are dependent normally distributed variables, Yj has mean µj, and the Y ’s have a given covariance matrix  cij .  The covariance, cij, of Yi and Yj is defined to be the average value of  Yi − µi  Yj − µj . In particular, cjj is the variance of Yj, the square of its standard deviation. Not all matrices  cij  can be covariance matrices, and your construction is, of course, only supposed to work whenever a solution to the given conditions is possible.  14. [M21] If X is a random variable with the continuous distribution F  x , and if c is a  possibly negative  constant, what is the distribution of cX? 15. [HM21] If X1 and X2 are independent random variables with the respective distributions F1 x  and F2 x , and with densities f1 x  = F ′1 x , f2 x  = F ′2 x , what are the distribution and density functions of the quantity X1 + X2?   140  RANDOM NUMBERS   cid:120  16. [HM22]  J. H. Ahrens.  Develop an algorithm for gamma deviates of order a  cid:120  17. [M24] What is the distribution function F  x  for the geometric distribution with  when 0 < a ≤ 1, using the rejection method with cg t  = ta−1 Γ  a  for 0 < t < 1, and with cg t  = e−t Γ  a  for t ≥ 1.  3.4.1  n  probability p? What is the generating function G z ? What are the mean and standard deviation of this distribution? 18. [M24] Suggest a method to compute a random integer N for which N takes the value n with probability np2 1 − p n−1, n ≥ 0.  The case of particular interest is when p is rather small.  19. [22] The negative binomial distribution  t, p  has integer values N = n with  pt 1 − p n.  Unlike the ordinary binomial distribution, t need not  probability t−1+n  be an integer, since this quantity is nonnegative for all n whenever t > 0.  Generalizing exercise 18, explain how to generate integers N with this distribution when t is a small positive integer. What method would you suggest if t = p = 1 2? 20. [M20] Let A be the area of the shaded region in Fig. 13, and let R be the area of the enclosing rectangle. Let I be the area of the interior region recognized by step R2, and let E be the area between the exterior region rejected in step R3 and the outer rectangle. Determine the number of times each step of Algorithm R is performed, for each of its four variants as in  25 , in terms of A, R, I, and E. 21. [HM29] Derive formulas for the quantities A, R, I, and E defined in exercise 20.  For I and especially E you may wish to use an interactive computer algebra system.  Show that c = e1 4 is the best possible constant in step R2 for tests of the form “X2 ≤ 4 1 + ln c  − 4cU.” 22. [HM40] Can the exact Poisson distribution for large µ be obtained by generating an appropriate normal deviate, converting it to an integer in some convenient way, and applying a  possibly complicated  correction a small percent of the time? 23. [HM23]  J. von Neumann.  Are the following two ways to generate a random quantity X equivalent  that is, does the quantity X have the same distribution ?  Method 1: Set X ← sin  π 2 U , where U is uniform. Method 2: Generate two uniform deviates, U and V; if U 2 + V 2 ≥ 1, repeat  until U 2 + V 2 < 1. Then set X ← U 2 − V 2  U 2 + V 2 .   x  24. [HM40]  S. Ulam, J. von Neumann.  Let V0 be a randomly selected real number between 0 and 1, and define the sequence ⟨Vn⟩ by the rule Vn+1 = 4Vn 1 − Vn . If this computation is done with perfect accuracy, the result should be a sequence with the distribution sin2 πU, where U is uniform, that is, with distribution function F  x  = √ 2πx 1 − x  . For if we write Vn = sin2 πUn, we find that Un+1 =  2Un  mod 1; 0 dx  and by the fact that almost all real numbers have a random binary expansion  see Section 3.5 , this sequence Un is equidistributed. But if the computation of Vn is done with only finite accuracy, the argument breaks down because we soon are dealing with noise from the roundoff error. [See von Neumann’s Collected Works 5, 768–770.] Analyze the sequence ⟨Vn⟩ defined in the preceding paragraph, when only finite ac- curacy is present, both empirically  for various different choices of V0  and theoretically. Does the sequence have a distribution resembling the expected distribution? 25. [M25] Let X1, X2, . . . , X5 be binary words each of whose bits is independently 0 or 1 with probability 1 2. What is the probability that a given bit position of X1   X2 &  X3   X4 & X5    contains a 1? Generalize.   3.4.1  NUMERICAL DISTRIBUTIONS  141  26. [M18] Let N1 and N2 be independent Poisson deviates with means µ1 and µ2, where µ1 > µ2 ≥ 0. Prove or disprove:  a  N1 + N2 has the Poisson distribution with mean µ1 + µ2.  b  N1 − N2 has the Poisson distribution with mean µ1 − µ2. 27. [22]  J. H. Ahrens.  On most binary computers there is an efficient way to count the number of 1s in a binary word  see Section 7.1.3 . Hence there is a nice way to obtain the binomial distribution  t, p  when p = 1 2, simply by generating t random bits and counting the number of 1s. Design an algorithm that produces the binomial distribution  t, p  for arbitrary p, using only a subroutine for the special case p = 1 2 as a source of random data. [Hint: Simulate a process that first looks at the most significant bits of t uniform deviates, then at the second bit of those deviates whose leading bit is not sufficient to determine whether or not their value is < p, etc.] 28. [HM35]  R. P. Brent.  Develop a method to generate a random point on the  surface of the ellipsoid defined by akx2  k = 1, where a1 ≥ ··· ≥ an > 0.  29. [M20]  J. L. Bentley and J. B. Saxe.  Find a simple way to generate n numbers X1, . . . , Xn that are uniform between 0 and 1 except for the fact that they are sorted: X1 ≤ ··· ≤ Xn. Your algorithm should take only O n  steps. 30. [M30] Explain how to generate a set of random points  Xj, Yj  such that, if R is any rectangle of area α contained in the unit square, the number of  Xj, Yj  lying in R has the Poisson distribution with mean αµ. 31. [HM39]  Direct generation of normal deviates.  a  Prove that if a2  1 +··· + a2 k = 1 and if X1, . . . , Xk are independent normal deviates with mean 0 and variance 1, then a1X1 + ··· + akXk is a normal deviate with mean 0 and variance 1.  b  The result of  a  suggests that we can generate new normal deviates from old ones, just as we obtain new uniform deviates from old ones. For example, we might use the idea of 3.2.2– 7 , but with a recurrence like  Xn =  Xn−24 + Xn−55    or  Xn = 3  5 Xn−24 + 4  5 Xn−55,  after a set of normal deviates X0, . . . , X54 has been computed initially. Explain why this is not a good idea.  c  Show, however, that there is a suitable way to generate normal deviates quickly from other normal deviates, by using a refinement of the idea in  a  and  b . [Hint: If X and Y are independent normal deviates, so are X′ = X cos θ + Y sin θ and Y ′ = −X sin θ + Y cos θ, for any angle θ.]  32. [HM30]  C. S. Wallace.  Let X and Y be independent exponential deviates with mean 1. Show that X′ and Y ′ are, likewise, independent exponential deviates with mean 1, if we obtain them from X and Y in any of the following ways: a  Given 0 < λ < 1,  X′ =  1 − λ X − λY +  X + Y  [ 1 − λ X < λY ],  Y ′ = X + Y − X′.  √ 2  b   X′, Y ′  = c  If X =  . . . x2x1x0.x−1x−2x−3 . . .  2 and Y =  . . . y2y1y0.y−1y−2y−3 . . .  2 in bi-  nary notation, then X′ and Y ′ have the “shuffled” values   2Y, X − Y  ,  if X ≤ Y ; if X > Y .    2X, Y − X ,  X′ =  . . . x2y1x0.y−1x−2y−3 . . .  2,  Y ′ =  . . . y2x1y0.x−1y−2x−3 . . .  2.   142  RANDOM NUMBERS  3.4.1  33. [20] Algorithms P, M, F, and R generate normal deviates by consuming an unknown number of uniform random variables U1, U2, . . . . How can they be modified so that the output is a function of just one U?  3.4.2. Random Sampling and Shuffling Many data processing applications call for an unbiased choice of n records at random from a file containing N records. This problem arises, for example, in quality control or other statistical calculations where sampling is needed. Usually N is very large, so that it is impossible to contain all the data in memory at once; and the individual records themselves are often very large, so that we can’t even hold n records in memory. Therefore we seek an efficient procedure for selecting n records by deciding either to accept or to reject each record as it comes along, writing the accepted records onto an output file.  Several methods have been devised for this problem. The most obvious approach is to select each record with probability n N; this may sometimes be appropriate, but it gives only an average of n records in the sample. The  standard deviation isn 1 − n N , and the sample might turn out to be either  too large for the desired application or too small to give the necessary results. Fortunately, a simple modification of the “obvious” procedure gives us what we want: The  t+1 st record should be selected with probability  n−m   N−t , if m items have already been selected. This is the appropriate probability, since of all the possible ways to choose n things from N such that m values occur in the first t, exactly   N − t − 1  n − m − 1   N − t    n − m  = n − m N − t   1   of them select the  t + 1 st element.  The idea developed in the preceding paragraph leads immediately to the  following algorithm: Algorithm S  Selection sampling technique . To select n records at random from a set of N, where 0 < n ≤ N. S1. [Initialize.] Set t ← 0, m ← 0.  During this algorithm, m represents the number of records selected so far, and t is the total number of input records that we have dealt with.   S2. [Generate U.] Generate a random number U, uniformly distributed between  zero and one.  S3. [Test.] If  N − t U ≥ n − m, go to step S5. S4. [Select.] Select the next record for the sample, and increase m and t by 1. If m < n, go to step S2; otherwise the sample is complete and the algorithm terminates.  S5. [Skip.] Skip the next record  do not include it in the sample , increase t  by 1, and go back to step S2.   3.4.2  RANDOM SAMPLING AND SHUFFLING  143  This algorithm may appear to be unreliable at first glance and, in fact, to be incorrect; but a careful analysis  see the exercises below  shows that it is completely trustworthy. It is not difficult to verify that a  At most N records are input  we never run off the end of the file before  choosing n items .  b  The sample is completely unbiased. In particular, the probability that any  given element is selected, such as the last element of the file, is n N. Statement  b  is true in spite of the fact that we are not selecting the  t+1 st item with probability n N, but rather with the probability in Eq.  1 ! This has caused some confusion in the published literature. Can the reader explain this seeming contradiction?   Note: When using Algorithm S, one should be careful to use a different source of random numbers U each time the program is run, to avoid connections between the samples obtained on different days. This can be done, for example, by choosing a different value of X0 for the linear congruential method each time. The seed value X0 could be set to the current date, or to the last random number X that was generated on the previous run of the program.   We will usually not have to pass over all N records. In fact, since  b  above says that the last record is selected with probability n N, we will terminate the algorithm before considering the last record exactly  1 − n N  of the time. The average number of records considered when n = 2 is about 2 3 N, and the general formulas are given in exercises 5 and 6. Algorithm S and a number of other sampling techniques are discussed in a paper by C. T. Fan, Mervin E. Muller, and Ivan Rezucha, J. Amer. Stat. Assoc. 57  1962 , 387–402. The method was independently discovered by T. G. Jones, CACM 5  1962 , 343.  A problem arises if we don’t know the value of N in advance, since the precise value of N is crucial in Algorithm S. Suppose we want to select n items at random from a file, without knowing exactly how many are present in that file. We could first go through and count the records, then take a second pass to select them; but it is generally better to sample m ≥ n of the original items on the first pass, where m is much less than N, so that only m items must be considered on the second pass. The trick, of course, is to do this in such a way that the final result is a truly random sample of the original file.  Since we don’t know when the input is going to end, we must keep track of a random sample of the input records seen so far, thus always being prepared for the end. As we read the input we will construct a “reservoir” that contains only the records that have appeared among the previous samples. The first n records always go into the reservoir. When the  t + 1 st record is being input, for t ≥ n, we will have in memory a table of n indices pointing to the records that we have chosen from among the first t. The problem is to maintain this situation with t increased by one, namely to find a new random sample from among the t + 1 records now known to be present. It is not hard to see that we should include   144  RANDOM NUMBERS  3.4.2  the new record in the new sample with probability n  t + 1 , and in such a case it should replace a random element of the previous sample.  Thus, the following procedure does the job:  Algorithm R  Reservoir sampling . To select n records at random from a file of unknown size ≥ n, given n > 0. An auxiliary file called the “reservoir” contains all records that are candidates for the final sample. The algorithm uses a table of distinct indices I[j] for 1 ≤ j ≤ n, each of which points to one of the records in the reservoir. R1. [Initialize.] Input the first n records and copy them to the reservoir. Set I[j] ← j for 1 ≤ j ≤ n, and set t ← m ← n.  If the file being sampled has fewer than n records, it will of course be necessary to abort the algorithm and report failure. During this algorithm, indices I[1], . . . , I[n] point to the records in the current sample; m is the size of the reservoir; and t is the number of input records dealt with so far.   R2. [End of file?] If there are no more records to be input, go to step R6. R3. [Generate and test.]  Increase t by 1, then generate a random integer M  between 1 and t  inclusive . If M > n, go to R5.  R4. [Add to reservoir.] Copy the next record of the input file to the reservoir, increase m by 1, and set I[M] ← m.  The record previously pointed to by I[M] is being replaced in the sample by the new record.  Go back to R2. R5. [Skip.] Skip over the next record of the input file  do not include it in the  reservoir , and return to step R2.  R6. [Second pass.] Sort the I table entries so that I[1] < ··· < I[n]; then go through the reservoir, copying the records with these indices into the output file that is to hold the final sample. Algorithm R is due to Alan G. Waterman. The reader may wish to work  out the example of its operation that appears in exercise 9.  If the records are sufficiently short, it is of course unnecessary to have a reservoir at all; we can keep the n records of the current sample in memory at all times, and the algorithm becomes much simpler  see exercise 10 .  The natural question to ask about Algorithm R is, “What is the expected size of the reservoir?” Exercise 11 shows that the average value of m is exactly  n 1 + HN − Hn ; this is approximately n1 + ln N n . So if N n = 1000, the  reservoir will contain only about 1 125 as many items as the original file.  Notice that Algorithms S and R can be used to obtain samples for several independent categories simultaneously. For example, if we have a large file of names and addresses of U.S. residents, we could pick random samples of exactly 10 people from each of the 50 states without making 50 passes through the file, and without first sorting the file by state.  Significant improvements to both Algorithms S and R are possible, when n N is small, if we generate a single random variable to tell us how many records should be skipped instead of deciding whether or not to skip each record.  See exercise 8.    3.4.2  RANDOM SAMPLING AND SHUFFLING  145  The sampling problem can be regarded as the computation of a random combination, according to the conventional definition of combinations of N things taken n at a time  see Section 1.2.6 . Now let us consider the problem of computing a random permutation of t objects; we will call this the shuffling problem, since shuffling a deck of cards is nothing more than subjecting the deck to a random permutation.  A moment’s reflection is enough to convince any card player that traditional shuffling procedures are miserably inadequate. There is no hope of obtaining each of the t! permutations with anywhere near equal probability by such methods. Expert bridge players reportedly make use of this fact when deciding whether or not to finesse. At least seven “riffle shuffles” of a 52-card deck are needed to reach a distribution within 10% of uniform, and 14 random riffles are guaranteed to do so [see Aldous and Diaconis, AMM 93  1986 , 333–348].  If t is small, we can obtain a random permutation very quickly by generating a random integer between 1 and t!. For example, when t = 4, a random number between 1 and 24 suffices to select a random permutation from a table of all possibilities. But for large t, it is necessary to be more careful if we want to claim that each permutation is equally likely, since t! is much larger than the accuracy of individual random numbers.  A suitable shuffling procedure can be obtained by recalling Algorithm 3.3.2P, which gives a simple correspondence between each of the t! possible permutations and a sequence of numbers  c1, c2, . . . , ct−1 , with 0 ≤ cj ≤ j. It is easy to compute such a set of numbers at random, and we can use the correspondence to produce a random permutation. Algorithm P  Shuffling . Let  X1, X2, . . . , Xt  be a sequence of t numbers to be shuffled. P1. [Initialize.] Set j ← t. P2. [Generate U.] Generate a random number U, uniformly distributed between  zero and one.  P3. [Exchange.] Set k ← ⌊jU⌋ + 1.  Now k is a random integer, between 1 and j. Exercise 3.4.1–3 explains that k should not be computed by taking a remainder modulo j.  Exchange Xk ↔ Xj.  P4. [Decrease j.] Decrease j by 1. If j > 1, return to step P2.  This algorithm was first published by R. A. Fisher and F. Yates [Statistical Tables  London, 1938 , Example 12], in ordinary language, and by R. Durstenfeld [CACM 7  1964 , 420] in computer language. If we merely wish to generate a ran- dom permutation of {1, . . . , t} instead of shuffling a given sequence  X1, . . . , Xt , we can avoid the exchange operation Xk ↔ Xj by letting j increase from 1 to t and setting Xj ← Xk, Xk ← j; see D. E. Knuth, The Stanford GraphBase  New York: ACM Press, 1994 , 104. R. Salfi [COMPSTAT 1974  Vienna: 1974 , 28–35] has pointed out that Algorithm P cannot possibly generate more than m distinct permutations when we obtain the uniform U’s with a linear congruential sequence of modulus m,   146  RANDOM NUMBERS  3.4.2  or indeed whenever we use a recurrence Un+1 = f Un  for which Un can take only m different values, because the final permutation in such cases is entirely determined by the value of the first U that is generated. Thus, for example, if m = 232, certain permutations of 13 elements will never occur, since 13! ≈ 1.45×232. In most applications we don’t really want to see all 13! permutations; yet it is disconcerting to know that the excluded ones are determined by a fairly simple mathematical rule such as a lattice structure  see Section 3.3.4 .  This problem does not arise when we use a lagged Fibonacci generator like 3.2.2– 7  with a sufficiently long period. But even with such methods we cannot get all permutations uniformly unless we are able to specify at least t! different seed values to initialize the generator. In other words, we can’t get lg t! truly random bits out unless we put lg t! truly random bits in. Section 3.5 shows that we need not despair about this.  Algorithm P can easily be modified to yield a random permutation of a random combination  see exercise 15 . For a discussion of random combinatorial objects of other kinds  e.g., partitions , see Section 7.2 and or the book Combi- natorial Algorithms by Nijenhuis and Wilf  New York: Academic Press, 1975 .  EXERCISES 1. [M12] Explain Eq.  1 . 2. [20] Prove that Algorithm S never tries to read more than N records of its input file.   cid:120  3. [22] The  t+1 st item in Algorithm S is selected with probability  n−m   N −t ,  not n N, yet the text claims that the sample is unbiased; thus each item should be selected with the same probability. How can both of these statements be true? 4. [M23] Let p m, t  be the probability that exactly m items are selected from among the first t in the selection sampling technique. Show directly from Algorithm S that   t   N − t   N    m  n − m  ,  n  p m, t  =  for 0 ≤ t ≤ N.  5. [M24] What is the average value of t when Algorithm S terminates?  In other words, how many of the N records have been passed, on the average, before the sample is complete?  6. [M24] What is the standard deviation of the value computed in exercise 5? 7. [M25] Prove that any given choice of n records from the set of N is obtained by  Algorithm S with probability 1 N . Therefore the sample is completely unbiased.  cid:120  8. [M39]  J. S. Vitter.  Algorithm S computes one uniform deviate for each input  record it handles. The purpose of this exercise is to consider a more efficient approach in which we calculate more quickly the proper number X of input records to skip before the first selection is made. a  What is the probability that X ≥ k, given k? b  Show that the result of  a  allows us to calculate X by generating only one c  Show that we may also set X ← min YN , YN−1, . . . , YN−n+1 , where the Y ’s are  uniform U and then doing O X  other calculations. independent and each Yt is a random integer in the range 0 ≤ Yt < t.  n   3.4.2  RANDOM SAMPLING AND SHUFFLING  147  d  For maximum speed, show that X can also be calculated in O 1  steps, on the  average, using a “squeeze method” like Eq. 3.4.1– 18 .  9. [12] Let n = 3. If Algorithm R is applied to a file containing 20 records numbered 1 thru 20, and if the random numbers generated in step R3 are respectively  4, 1, 6, 7, 5, 3, 5, 11, 11, 3, 7, 9, 3, 11, 4, 5, 4,  which records go into the reservoir? Which are in the final sample? 10. [15] Modify Algorithm R so that the reservoir is eliminated, assuming that the n records of the current sample can be held in memory.   cid:120  11. [M25] Let pm be the probability that exactly m elements are put into the reservoir   during the first pass of Algorithm R. Determine the generating function G z  = m pmzm, and find the mean and standard deviation.  Use the ideas of Section 1.2.10.  12. [M26] The gist of Algorithm P is that any permutation π can be uniquely written as a product of transpositions in the form π =  att  . . .  a33  a22 , where 1 ≤ aj ≤ j for t ≥ j > 1. Prove that there is also a unique representation of the form π =  b22  b33  . . .  btt , where 1 ≤ bj ≤ j for 1 < j ≤ t, and design an algorithm that computes the b’s from the a’s in O t  steps. 13. [M23]  S. W. Golomb.  One of the most common ways to shuffle cards is to divide the deck into two parts as equal as possible, and to “riffle” them together.  According to the discussion of card-playing etiquette in Hoyle’s rules of card games, “A shuffle of this sort should be made about three times to mix the cards thoroughly.”  Consider a deck of 2n − 1 cards X1, X2, . . . , X2n−1; a “perfect shuffle” s divides this deck into X1, X2, . . . , Xn and Xn+1, . . . , X2n−1, then perfectly interleaves them to obtain X1, Xn+1, X2, Xn+2, . . . , X2n−1, Xn. The “cut” operation cj changes X1, X2, . . . , X2n−1 into Xj+1, . . . , X2n−1, X1, . . . , Xj. Show that by combining perfect shuffles and cuts, at most  2n − 1  2n − 2  different arrangements of the deck are possible, if n > 1. 14. [22] A cut-and-riffle permutation of a0 a1 . . . an−1 changes it to a sequence that contains the subsequences  and  ax a x+1  mod n . . . a y−1  mod n  ay a y+1  mod n . . . a x−1  mod n  2 ♣  9 ♣  8 ♣  7 ♣  6 ♣  4 ♣  J ♣  10 ♣  intermixed in some way, for some x and y. Thus, 3890145267 is a cut-and-riffle of 0123456789, with x = 3 and y = 8. a  Beginning with 52 playing cards arranged in the standard order 4 3 3 ♣ ♠ ♠  5 A ♠, ♣ Mr. J. H. Quick  a student  did a random cut-and-riffle; then he removed the leftmost card and inserted it in a random place, obtaining the sequence 8 7 2 J K ♣ ♣ ♦ ♡ ♡ Which card did he move from the leftmost position?  8 ♣.  10 ♡  10 ♡  10 ♠  10 ♣  10 ♦  10 ♠  10 ♦  Q ♡  Q ♡  K ♡  K ♡  A ♡  A ♡  Q ♦  Q ♠  Q ♦  Q ♠  Q ♣  Q ♣  K ♣  K ♣  K ♦  K ♠  K ♠  A ♣  A ♣  A ♦  A ♠  A ♦  2 ♡  3 ♡  4 ♡  5 ♡  6 ♡  7 ♡  8 ♡  9 ♡  J ♡  2 ♡  3 ♡  4 ♡  5 ♡  6 ♡  9 ♡  J ♡  2 ♦  3 ♦  4 ♦  5 ♦  6 ♦  7 ♦  8 ♦  9 ♦  J ♦  2 ♦  3 ♦  2 ♠  3 ♠  4 ♠  5 ♠  6 ♠  7 ♠  4 ♦  8 ♠  9 ♠  5 ♦  6 ♦  J ♠  7 ♦  8 ♦  9 ♦  J ♦  6 ♠  2 ♠  7 ♠  8 ♠  9 ♠  J ♠  3 ♣  4 ♣  5 ♣  6 ♣  7 ♣  5 ♠  9 ♣  b  Starting again with the deck in its original order, Quick now did three cut-and-  10 ♦  J ♣  Q ♣  riffles before moving the leftmost card to a new place: 8 5 7 3 9 ♠ ♣ ♣ ♣ ♦ Which card did he move this time?  K ♡  A ♡  Q ♦  Q ♠  K ♣  K ♦  K ♠  A ♠  A ♣  A ♦  8 ♡  6 ♡  7 ♡  9 ♡  4 ♡  5 ♡  6 ♦  J ♠  J ♦  4 ♠  2 ♣  4 ♣  5 ♠  6 ♠  7 ♠  6 ♣  7 ♣  3 ♣  10 ♡  8 ♠  10 ♣  8 ♦  2 ♡  5 ♦  J ♡  2 ♦  3 ♦  Q ♡  4 ♦  9 ♠  3 ♡  2 ♠  9 ♦  10 ♠.   148  3.4.2  RANDOM NUMBERS   cid:120  15. [30]  Ole-Johan Dahl.  If Xk = k for 1 ≤ k ≤ t at the start of Algorithm P, and if we terminate the algorithm when j reaches the value t − n, the sequence Xt−n+1,  cid:120  16. [M25] Devise a way to compute a random sample of n records from N, given N . . . , Xt is a random permutation of a random combination of n elements. Show how to simulate the effect of this procedure using only O n  cells of memory.  and n, based on the idea of hashing  Section 6.4 . Your method should use O n  storage locations and an average of O n  units of time, and it should present the sample as a sorted set of integers 1 ≤ X1 < X2 < ··· < Xn ≤ N. 17. [M22]  R. W. Floyd.  Prove that the following algorithm generates a random sample S of n integers from {1, . . . , N}: Set S ← ∅; then for j ← N − n + 1, N − n + 2, . . . , N  in this order , set k ← ⌊jU⌋ + 1 and   cid:120  18. [M32] People sometimes try to shuffle n items  X1, X2, . . . , Xn  by successively  S ∪ {j},  S ←  if k  ∈ S; if k ∈ S.   S ∪ {k},  interchanging  X1 ↔ Xk1 , X2 ↔ Xk2 , . . . , Xn ↔ Xkn ,  where the indices kj are independent and uniformly random between 1 and n. Consider the directed graph with vertices {1, 2, . . . , n} and with arcs from j to kj for 1 ≤ j ≤ n. Describe the digraphs of this type for which, if we start with the elements  X1, X2, . . . , Xn  =  1, 2, . . . , n , the stated interchanges produce the respective permutations  a   n, 1, 2, . . .  ;  b   1, 2, . . . , n ;  c   2, . . . , n, 1 . Conclude that these three permutations are obtained with wildly different probabilities.   cid:120  19. [M28]  Priority sampling.  Consider a file of N items in which the kth item  if qk < r; if qk ≥ r;   max wk, 1 r ,  has a positive weight wk. Let qk = Uk wk for 1 ≤ k ≤ N, where {U1, . . . , UN} are independent uniform deviates in [0 . . 1 . If r is any real number, define   max wk, 1 r , w Ew 1 w  n−k th smallest element of {qk+1, . . . , qN}, we have w b  Consequently Ew c  Show that, if n > 2, the variance Var w  1 . . .w  Notice that the quantity s is independent of {U1, . . . , Uk}.  = wj1 . . . wjk when j1 < ··· < jk.   is Var w  a  If r is the nth smallest element of {q1, . . . , qN}, prove that the expected value is w1w2 . . . wk, for 1 ≤ k < n ≤ N. Hint: Show that, if s is the  s+  . k   . d  Given n, explain how to modify the reservoir sampling method so that the value of r and the n − 1 items with subscripts {j  qj < r} can be obtained with one pass through a file of unknown size N. Hint: Use a priority queue of size n.  k = w . . .w j1  +···+Var w  j1 +···+w  j1 . . .w  if qk ≤ r; if qk > r.   r  . . . w k   r  k =   r+  k   s+  1  w   r  jk   r  jk   r  jk   r  2  0,  0,  =   r    r    r    r    r    r   By means of the thread one understands the ball of yarn, so we’ll be satisfied and assured by having this sample. — MIGUEL DE CERVANTES, El Ingenioso Hidalgo Don Quixote de la Mancha  1605    3.5  WHAT IS A RANDOM SEQUENCE?  149  *3.5. WHAT IS A RANDOM SEQUENCE? A. Introductory remarks. We have seen in this chapter how to generate sequences  ⟨Un⟩ = U0, U1, U2, . . .   1  of real numbers in the range 0 ≤ Un < 1, and we have called them “random” sequences even though they are completely deterministic in character. To justify this terminology, we claimed that the numbers “behave as if they are truly random.” Such a statement may be satisfactory for practical purposes  at the present time , but it sidesteps a very important philosophical and theoretical question: Precisely what do we mean by “random behavior”? A quantitative definition is needed. It is undesirable to talk about concepts that we do not really understand, especially since many apparently paradoxical statements can be made about random numbers.  The mathematical theory of probability and statistics scrupulously avoids the issue. It refrains from making absolute statements, and instead expresses everything in terms of how much probability is to be attached to statements involving random sequences of events. The axioms of probability theory are set up so that abstract probabilities can be computed readily, but nothing is said about what probability really signifies, or how this concept can be applied meaningfully to the actual world. In the book Probability, Statistics, and Truth  New York: Macmillan, 1957 , R. von Mises discusses this situation in detail, and presents the view that a proper definition of probability depends on obtaining a proper definition of a random sequence.  Let us paraphrase here some statements made by two of the many authors  who have commented on the subject.  D. H. Lehmer  1951 : “A random sequence is a vague notion embodying the idea of a sequence in which each term is unpredictable to the uninitiated and whose digits pass a certain number of tests, traditional with statisticians and depending somewhat on the uses to which the sequence is to be put.” J. N. Franklin  1962 : “The sequence  1  is random if it has every property that is shared by all infinite sequences of independent samples of random variables from the uniform distribution.” Franklin’s statement essentially generalizes Lehmer’s to say that the se- quence must satisfy all statistical tests. His definition is not completely precise, and we will see later that a reasonable interpretation of his statement leads us to conclude that there is no such thing as a random sequence! So let us begin with Lehmer’s less restrictive statement and attempt to make it precise. What we really want is a relatively short list of mathematical properties, each of which is satisfied by our intuitive notion of a random sequence; furthermore, the list is to be complete enough so that we are willing to agree that any sequence satisfying these properties is “random.” In this section, we will develop what seems to be an adequate definition of randomness according to these criteria, although many interesting questions remain to be answered.   150  RANDOM NUMBERS  3.5 Let u and v be real numbers, 0 ≤ u < v ≤ 1. If U is a random variable that is uniformly distributed between 0 and 1, the probability that u ≤ U < v is equal to v − u. For example, the probability that 1 5. How can we translate this property of the single number U into a property of the infinite sequence U0, U1, U2, . . . ? The obvious answer is to count how many times Un lies between u and v, and the average number of times should equal v − u. Our intuitive idea of probability is based in this way on the frequency of occurrence. More precisely, let ν n  be the number of values of j, 0 ≤ j < n, such that u ≤ Uj < v; we want the ratio ν n  n to approach the value v−u as n approaches infinity:  5 ≤ U < 3  5 is 2  lim n→∞  ν n  n  = v − u.   2   If this condition holds for all choices of u and v, the sequence is said to be equidistributed. Let S n  be a statement about the integer n and the sequence U0, U1, . . . ; for example, S n  might be the statement considered above, “u ≤ Un < v.” We can generalize the idea used in the preceding paragraph to define the probability that S n  is true with respect to a particular infinite sequence. Definition A. Let ν n  be the number of values of j, 0 ≤ j < n, such that S j  is true. We say that S n  is true with probability λ if the limit as n tends to infinity  of ν n  n equals λ. Symbolically: PrS n  = λ if limn→∞ ν n  n = λ.  In terms of this notation, the sequence U0, U1, . . . is equidistributed if and  only if Pr u ≤ Un < v  = v − u, for all real numbers u, v with 0 ≤ u < v ≤ 1.  A sequence might be equidistributed without being random. For example, if U0, U1, . . . and V0, V1, . . . are equidistributed sequences, it is not hard to show that the sequence  2 + 1  2 + 1  2 + 1  2 V0, 1  2 U0, 1  2 U1, 1  W0, W1, W2, W3, . . . = 1   3  is also equidistributed, since the subsequence 1 2 U0, 1 2 U1, . . . is equidistributed 2 V0, 1 between 0 and 1 2, while the alternate terms 1 2 + 1 2 V1, . . . , are equi- distributed between 1 2 is always followed by a value greater than or equal to 1 2, and conversely; hence the sequence is not random by any reasonable definition. A stronger property than equidistribution is needed.  2 and 1. But in the sequence of W’s, a value less than 1  2 V1, . . .  A natural generalization of the equidistribution property, which removes the objection stated in the preceding paragraph, is to consider adjacent pairs of numbers of our sequence. We can require the sequence to satisfy the condition  4  for any four numbers u1, v1, u2, v2 with 0 ≤ u1 < v1 ≤ 1, 0 ≤ u2 < v2 ≤ 1. And in general, for any positive integer k we can require our sequence to be k-distributed in the following sense:  Pr u1 ≤ Un < v1 and u2 ≤ Un+1 < v2  =  v1 − u1  v2 − u2    151   5   3.5  WHAT IS A RANDOM SEQUENCE?  Definition B. The sequence  1  is said to be k-distributed if  Pr u1 ≤ Un < v1, . . . , uk ≤ Un+k−1 < vk  =  v1 − u1  . . .  vk − uk  for all choices of real numbers uj, vj, with 0 ≤ uj < vj ≤ 1 for 1 ≤ j ≤ k.  An equidistributed sequence is a 1-distributed sequence. Notice that if k > 1, a k-distributed sequence is always  k − 1 -distributed, since we may set uk = 0 and vk = 1 in Eq.  5 . Thus, in particular, any sequence that is known to be 4-distributed must also be 3-distributed, 2-distributed, and equidistributed. We can investigate the largest k for which a given sequence is k-distributed; and this leads us to formulate a stronger property: Definition C. A sequence is said to be ∞-distributed if it is k-distributed for all positive integers k.  So far we have considered “[0 . . 1  sequences,” that is, sequences of real numbers lying between zero and one. The same ideas apply to integer-valued sequences; let us say that the sequence ⟨Xn⟩ = X0, X1, X2, . . . is a b-ary sequence if each Xn is one of the integers 0, 1, . . . , b− 1. Thus, a 2-ary  binary  sequence is a sequence of zeros and ones. We also define a k-digit b-ary number as a string of k integers x1x2 . . . xk, where 0 ≤ xj < b for 1 ≤ j ≤ k. Definition D. A b-ary sequence is said to be k-distributed if Pr XnXn+1 . . . Xn+k−1 = x1x2 . . . xk  = 1 bk   6   for all b-ary numbers x1x2 . . . xk.  If we set uj = xj b, vj =  xj + 1  b, Xn = ⌊bUn⌋, Eq.  5  becomes Eq.  6 .  It is clear from this definition that if U0, U1, . . . is a k-distributed [0 . . 1  sequence, then the sequence ⌊bU0⌋, ⌊bU1⌋, . . . is a k-distributed b-ary sequence. Furthermore, every k-distributed b-ary sequence is also  k − 1 -distributed, if k > 1: We add together the probabilities for the b-ary numbers x1 . . . xk−1 0, x1 . . . xk−1 1, . . . , x1 . . . xk−1  b − 1  to obtain  Pr Xn . . . Xn+k−2 = x1 . . . xk−1  = 1 bk−1.   Probabilities for disjoint events are additive; see exercise 5.  It therefore is natural to speak of an ∞-distributed b-ary sequence, as in Definition C above. The representation of a positive real number in the radix-b number system may be regarded as a b-ary sequence; for example, π corresponds to the 10-ary sequence 3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, . . . . People have conjectured that this sequence is ∞-distributed, but nobody has yet been able to prove that it is even 1-distributed.  Let us analyze these concepts a little more closely in the case when k equals a million. A binary sequence that is 1000000-distributed is going to have runs of a million zeros in a row! Similarly, a [0 . . 1  sequence that is 1000000-distributed is going to have runs of a million consecutive values each of which is less than 1 2.   152  RANDOM NUMBERS  3.5  It is true that this will happen only   1 2 1000000 of the time, on the average, but the fact is that it does happen. Indeed, this phenomenon will occur in any truly random sequence, using our intuitive notion of “truly random.” One can easily imagine that such a situation will have a drastic effect if this set of a million “truly random” numbers is being used in a computer-simulation experiment; there would be good reason to complain about the random number generator. However, if we have a sequence of numbers that never has runs of a million consecutive U’s less than 1 2, the sequence is not random, and it will not be a suitable source of numbers for other conceivable applications that use extremely long blocks of U’s as input. In summary, a truly random sequence will exhibit local nonrandomness. Local nonrandomness is necessary in some applications, but it is disastrous in others. We are forced to conclude that no sequence of “random” numbers can be adequate for every application.  In a similar vein, one may argue that it is impossible to judge whether a finite sequence is random or not; any particular sequence is just as likely as any other one. These facts are definitely stumbling blocks if we are ever to have a useful definition of randomness, but they are not really cause for alarm. It is still possible to give a definition for the randomness of infinite sequences of real numbers in such a way that the corresponding theory  viewed properly  will give us a great deal of insight concerning the ordinary finite sequences of rational numbers that are actually generated on a computer. Furthermore, we shall see later in this section that there are several plausible definitions of randomness for finite sequences. B. ∞-distributed sequences. Let us now make a brief study of the theory of sequences that are ∞-distributed. To describe the theory adequately, we will need to use a bit of higher mathematics, so we assume in the remainder of this subsection that the reader knows the material ordinarily taught in an “advanced calculus” course.  First it is convenient to generalize Definition A, since the limit appearing  there does not exist for all sequences. We define  ν n  n  Then PrS n , if it exists, is the common value of PrS n  and PrS n .  n→∞  n→∞  ,  .  We have seen that a k-distributed [0 . . 1  sequence leads to a k-distributed b-ary sequence, if U is replaced by ⌊bU⌋. Our first theorem shows that a converse result is also true. Theorem A. Let ⟨Un⟩ = U0, U1, U2, . . . be a [0 . . 1  sequence. If the sequence   7   ν n  n  PrS n  = lim sup  PrS n  = lim inf  ⟨⌊bjUn⌋⟩ = ⌊bjU0⌋, ⌊bjU1⌋, ⌊bjU2⌋, . . .  is a k-distributed bj-ary sequence for all bj in an infinite sequence of integers 1 < b1 < b2 < b3 < ··· , then the original sequence ⟨Un⟩ is k-distributed.  As an example of this theorem, suppose that bj = 2j. The sequence ⌊2jU0⌋, ⌊2jU1⌋, . . . is essentially the sequence of the first j bits of the binary   3.5  WHAT IS A RANDOM SEQUENCE?  153  j, v′  j + 1 b,  representations of U0, U1, . . . . If all these integer sequences are k-distributed, in the sense of Definition D, then the real-valued sequence U0, U1, . . . must also be k-distributed in the sense of Definition B. Proof of Theorem A. If the sequence ⌊bU0⌋, ⌊bU1⌋, . . . is k-distributed, it follows by the addition of probabilities that Eq.  5  holds whenever each uj and vj is a rational number with denominator b. Now let uj, vj be any real numbers, and let u′  j be rational numbers with denominator b such that j + 1 b.  j ≤ uj < u′ u′  j ≤ vj < v′ v′  1 ≤ Un < v′ u′  Let S n  be the statement that u1 ≤ Un < v1, . . . , uk ≤ Un+k−1 < vk. We have   PrS n  ≤ Pr   PrS n  ≥ Pr  k − 1 j ±1 b − vj − uj  ≤ 2 b. Since our inequalities hold for all b = bj, Now v′  v1 − u1  . . .  vk − uk  ≤ PrS n  ≤ PrS n  ≤  v1 − u1  . . .  vk − uk .   k + 1 b 1 + 1 k − u′ v′  ≤ Un+k−1 < v′ 1 − 1 k − u′ v′   k ≤ Un+k−1 < v′ 1 − u′ v′  k + 1 1 − u′ v′  and since bj → ∞ as j → ∞, we have  1, . . . , u′ =  ≤ Un < v′  1 + 1 u′  , . . . , u′        k + 1  1 + 1  j − u′  . . .  . . .  =  b  b  b  b  b  b  b  ;  k  .  The next theorem is our main tool for proving things about k-distributed  sequences. Theorem B. Suppose that ⟨Un⟩ is a k-distributed [0 . . 1  sequence, and let f x1, x2, . . . , xk  be a Riemann-integrable function of k variables; then  f Uj, Uj+1, . . . , Uj+k−1  =  f x1, x2, . . . , xk  dx1 . . . dxk.  8   Proof. The definition of a k-distributed sequence states that this result is true in the special case that  f x1, . . . , xk  = [ u1 ≤ x1 < v1, . . . , uk ≤ xk < vk ]   9  for some constants u1, v1, . . . , uk, vk. Therefore Eq.  8  is true whenever f = a1f1 + a2f2 + ··· + amfm and when each fj is a function of type  9 ; in other words, Eq.  8  holds whenever f is a “step-function” obtained by partitioning the unit k-dimensional cube into subcells whose faces are parallel to the coordinate axes, and assigning a constant value to f on each subcell.  Now let f be any Riemann-integrable function. If ϵ is any positive number, we know  by the definition of Riemann-integrability  that there exist step func- tions f and f such that f x1, . . . , xk  ≤ f x1, . . . , xk  ≤ f x1, . . . , xk , and such    0≤j<n  lim n→∞  1 n   1  0  ···   1  0   154  RANDOM NUMBERS  3.5  that the difference of the integrals of f and f is less than ϵ. Since Eq.  8  holds for f and f, and since    0≤j<n  1 n  f Uj, . . . , Uj+k−1  ≤ 1 ≤ 1  n  f Uj, . . . , Uj+k−1   f Uj, . . . , Uj+k−1 ,     0≤j<n  n  0≤j<n  we conclude that Eq.  8  is true also for f.  Theorem B can be applied, for example, to the permutation test of Sec- tion 3.3.2. Let  p1, p2, . . . , pk  be any permutation of the numbers {1, 2, . . . , k}; we want to show that  Pr Un+p1−1 < Un+p2−1 < ··· < Un+pk−1  = 1 k!.   10   To prove this, assume that the sequence ⟨Un⟩ is k-distributed, and let  We have  f x1, . . . , xk  = [xp1 < xp2 <··· < xpk ].  1  1  Pr Un+p1−1 < Un+p2−1 < ··· < Un+pk−1   xp2   1  xpk  f x1, . . . , xk  dx1 . . . dxk   xp3  ···  =  0  0  =  0  dxpk  0  ···  dxp2  0  0  dxp1 = 1 k! .  Corollary P. If a [0 . . 1  sequence is k-distributed, it satisfies the permutation test of order k, in the sense of Eq.  10 .  We can also show that the serial correlation test is satisfied:  Corollary S. If a [0 . . 1  sequence is  k + 1 -distributed, the serial correlation coefficient between Un and Un+k tends to zero:   1  n  lim n→∞  n  n  1 n    Uj+k   1  UjUj+k − 1  Uj  Uj+k j+k − 1 2 1 j − 1  U 2  U 2  Uj  Uj,  U 2   U 2  n  n  n   All summations here are for 0 ≤ j < n.  Proof. By Theorem B, the quantities 1 n   UjUj+k,  j , 4, 1 tend to the respective limits 1  3, 1  3, 1  1 n  1 n  2, 1  1 n  j+k, 2 as n → ∞.  2 = 0.   Uj+k  1 n   3.5  WHAT IS A RANDOM SEQUENCE?  155  Let us now consider some slightly more general distribution properties of sequences. We have defined the notion of k-distribution by considering all of the adjacent k-tuples; for example, a sequence is 2-distributed if and only if the points   U0, U1 ,  U1, U2 ,  U2, U3 ,  U3, U4 ,  U4, U5 , . . .  are equidistributed in the unit square. It is quite possible, however, that this can happen while alternate pairs of points  U1, U2 ,  U3, U4 ,  U5, U6 , . . . are not equidistributed; if the density of points  U2n−1, U2n  is deficient in some area, the other points  U2n, U2n+1  might compensate. For example, the periodic binary sequence  . . . ,  ⟨Xn⟩ = 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,   11  with a period of length 16, is seen to be 3-distributed; yet the sequence of even- numbered elements ⟨X2n⟩ = 0, 0, 0, 0, 1, 0, 1, 0, . . . has three times as many zeros as ones, while the subsequence of odd-numbered elements ⟨X2n+1⟩ = 0, 1, 0, 1, 1, 1, 1, 1, . . . has three times as many ones as zeros. Suppose the sequence ⟨Un⟩ is ∞-distributed. Example  11  shows that the subsequence of alternate terms ⟨U2n⟩ = U0, U2, U4, U6, . . . is not obviously guaranteed to be ∞-distributed or even 1-distributed. But we shall see that ⟨U2n⟩ is, in fact, ∞-distributed, and much more is true. Definition E. A [0 . . 1  sequence ⟨Un⟩ is said to be  m, k -distributed if . . . , uk ≤ Umn+j+k−1 < vk   Pr u1 ≤ Umn+j < v1, u2 ≤ Umn+j+1 < v2,  =  v1 − u1  . . .  vk − uk   for all choices of real numbers ur, vr with 0 ≤ ur < vr ≤ 1 for 1 ≤ r ≤ k, and for all integers j with 0 ≤ j < m. Thus a k-distributed sequence is the special case m = 1 in Definition E; the case m = 2 means that the k-tuples starting in even positions must have the same density as the k-tuples starting in odd positions, etc.  The following properties of Definition E are obvious:  An  m, k -distributed sequence is  m, κ -distributed for 1 ≤ κ ≤ k.  12  An  m, k -distributed sequence is  d, k -distributed for all divisors d of m.  13   See exercise 8.  We can also define the concept of an  m, k -distributed b-ary sequence, as in Definition D; and the proof of Theorem A remains valid for  m, k -distributed sequences. The next theorem, which is in many ways rather surprising, shows that the property of being ∞-distributed is very strong indeed, much stronger than we imagined it to be when we first considered the definition of the concept. Theorem C  Ivan Niven and H. S. Zuckerman . An ∞-distributed sequence is  m, k -distributed for all positive integers m and k.   156  RANDOM NUMBERS  3.5  Proof. It suffices to prove the theorem for b-ary sequences, by using the general- ization of Theorem A just mentioned. Furthermore, we may assume that m = k, because  12  and  13  tell us that the sequence will be  m, k -distributed if it is  mk, mk -distributed. So we will prove that any ∞-distributed b-ary sequence X0, X1, . . . is  m, m - distributed for all positive integers m. Our proof is a simplified version of the original one given by Niven and Zuckerman in Pacific J. Math. 1  1951 , 103–109. The key idea we shall use is an important technique that applies to many situations in mathematics: “If the sum of m quantities and the sum of their squares are both consistent with the hypothesis that the m quantities are equal, then that hypothesis is true.” In a strong form, this principle may be stated as follows: Lemma E. Given m sequences of numbers ⟨yjn⟩ = yj0, yj1, . . . for 1 ≤ j ≤ m, suppose that  n→∞ y1n + y2n + ··· + ymn  = mα, lim mn  ≤ mα2. lim sup n→∞  2n + ··· + y2 Then for each j, limn→∞ yjn exists and equals α. An incredibly simple proof of this lemma is given in exercise 9.  1n + y2   y2  Resuming our proof of Theorem C, let x = x1x2 . . . xm be a b-ary number, and say that x occurs at position p if Xp−m+1Xp−m+2 . . . Xp = x. Let νj n  be the number of occurrences of x at position p when p < n and p mod m = j. Let yjn = νj n  n; we wish to prove that  n→∞ yjn = 1 lim mbm .  First we know that  n→∞ y0n + y1n + ··· + y m−1 n  = 1 lim bm ,   16  since the sequence is m-distributed. By Lemma E and Eq.  16 , the theorem will be proved if we can show that 0n + y2  1n + ··· + y2   17    y2  lim sup n→∞  This inequality is not obvious yet; some rather delicate maneuvering is  necessary before we can prove it. Let q be a multiple of m, and consider  mb2m .   m−1 n  ≤ 1  νj n  − νj n − q    .  2  C n  =   0≤j<m  This is the number of pairs of occurrences of x in positions p1 and p2 for which n − q ≤ p1 < p2 < n and p2 − p1 is a multiple of m. Consider now the sum   14    15    18    19   N+q  n=1  SN =  C n .   3.5  WHAT IS A RANDOM SEQUENCE?  157  Each pair of occurrences of x in positions p1 and p2 with p1 < p2 < p1 + q, where p2 − p1 is a multiple of m and p1 ≤ N, is counted exactly p1 + q − p2 times in the total SN  namely, when p2 < n ≤ p1 + q ; and the pairs of such occurrences with N < p1 < p2 < N + q are counted exactly N + q − p2 times.  Let dt n  be the number of pairs of occurrences of x in positions p1 and p2  with p1 + t = p2 < n. The analysis above shows that   q − mt dmt N + q  ≥ SN ≥      q − mt dmt N .   20   0<t<q m  0<t<q m  Since the original sequence is q-distributed,  dmt N  = 1 b2m for all t, 0 < t < q m, and therefore by  20  we have  lim N→∞  1 N  =   0<t<q m  lim N→∞  SN N  q − mt b2m = q q − m  2mb2m .  This fact will prove the theorem, after some manipulation.  By definition,  N+q    n=1  0≤j<m  2SN =   νj n  − νj n − q  2 −  νj n  − νj n − q  ,  and we can remove the unsquared terms by applying  16  to get   21    22    23   .  lim N→∞  n=1  TN N  0≤j<m  N+q  = q q − m  mb2m + q bm ,  νj n  − νj n − q 2 2  r ≤ r νj n − νj n− q 2  N+q N+q  n=1  1 r  j=1  a2  aj  j  νj n  =  ≤ q q− m   mb2m + q bm . νj n  − νj n − q  ≤ qνj N + q ,   24   where  TN =  Using the inequality  j=1  see exercise 1.2.3–30 , we find that    lim sup N→∞  N N + q   0≤j<m  1  We also have  q νj N  ≤   N <n≤N+q  n=1   158  RANDOM NUMBERS  and putting this into  24  gives     νj N   2 ≤ q − m  qmb2m + 1 qbm .  lim sup N→∞  0≤j<m  N  3.5   25   This formula has been established whenever q is a multiple of m; and if we let q → ∞ we obtain  17 , completing the proof.  For a possibly simpler proof, see J. W. S. Cassels, Pacific J. Math. 2  1952 ,  555–557.  Exercises 29 and 30 illustrate the nontriviality of this theorem, and they also demonstrate the fact that a q-distributed sequence will have probabilities √ deviating from the true  m, m -distribution probabilities by essentially 1  q at  most. See  25 . The full hypothesis of ∞-distribution is necessary for the  proof of the theorem.  As a result of Theorem C, we can prove that an ∞-distributed sequence passes the serial test, the maximum-of-t test, the collision test, the birthday spacings test, and the tests on subsequences mentioned in Section 3.3.2. It is not hard to show that the gap test, the poker test, and the run test are also satisfied  see exercises 12 through 14 . The coupon collector’s test is considerably more difficult to deal with, but it too is passed  see exercises 15 and 16 .  The existence of ∞-distributed sequences of a rather simple type is guaran-  teed by the next theorem. Theorem F  J. N. Franklin . The [0 . . 1  sequence U0, U1, U2, . . . with  Un = θn mod 1   26   is ∞-distributed for almost all real numbers θ > 1. That is, the set  {θ  θ > 1 and  26  is not ∞-distributed}  is of measure zero. The proofs of this theorem and some generalizations are given in Math. Comp. 17  1963 , 28–59.  Franklin has shown that θ must be a transcendental number for  26  to be ∞-distributed. Early in the 1960s, the powers ⟨πn mod 1⟩ were laboriously computed for n ≤ 10000 using multiple-precision arithmetic; and the most significant 35 bits of each of these numbers, stored on a disk file, were used successfully as a source of uniform deviates. According to Theorem F, the probability that the powers ⟨πn mod 1⟩ are ∞-distributed is equal to 1; yet there are uncountably many real numbers, so the theorem gives us no information about whether the sequence for π is really ∞-distributed or not. It is a fairly safe bet that nobody in our lifetimes will ever prove that this particular sequence is not ∞-distributed; but it might not be. Because of these considerations, one may legitimately wonder if there is any explicit sequence that is ∞-distributed: Is there an algorithm to compute real numbers Un for all n ≥ 0, such that   WHAT IS A RANDOM SEQUENCE?  3.5 159 the sequence ⟨Un⟩ is ∞-distributed? The answer is yes, as shown for example by D. E. Knuth in BIT 5  1965 , 246–250. The sequence constructed there consists entirely of rational numbers; in fact, each number Un has a terminating representation in the binary number system. Another construction of an explicit ∞-distributed sequence, somewhat more complicated than the sequence just cited, follows from Theorem W below. See also N. M. Korobov, Izv. Akad. Nauk SSSR 20  1956 , 649–660. C. Does ∞-distributed = random? In view of all the theoretical results about ∞-distributed sequences, we can be sure of one thing: The concept of an ∞-distributed sequence is an important one in mathematics. There is also a good deal of evidence that the following statement might be a valid formulation of the intuitive idea of randomness: Definition R1. A [0 . . 1  sequence is defined to be “random” if it is an ∞- distributed sequence.  We have seen that sequences meeting this definition will satisfy all the statistical tests of Section 3.3.2 and many more. Let us attempt to criticize this definition objectively. First of all, is every “truly random” sequence ∞-distributed? There are uncountably many sequences U0, U1, . . . of real numbers between zero and one. If a truly random number generator is sampled to give values U0, U1, . . . , any of the possible sequences may be considered equally likely, and some of the sequences  indeed, uncountably many of them  are not even equidistributed. On the other hand, using any reasonable definition of probability on this space of all possible sequences leads us to conclude that a random sequence is ∞-distributed with probability one. We are therefore led to formalize Franklin’s definition of randomness  as given at the beginning of this section  in the following way: Definition R2. A [0 . . 1  sequence ⟨Un⟩ is defined to be “random” if, whenever P is a property such that P ⟨Vn⟩  holds with probability one for a sequence ⟨Vn⟩ of independent samples of random variables from the uniform distribution, then P ⟨Un⟩  is true.  Is it perhaps possible that Definition R1 is equivalent to Definition R2? Let us try out some possible objections to Definition R1, and see whether these criticisms are valid. In the first place, Definition R1 deals only with limiting properties of the sequence as n → ∞. There are ∞-distributed sequences in which the first million elements are all zero; should such a sequence be considered random?  This objection is not very substantial.  If ϵ is any positive number, there is no reason why the first million elements of a sequence should not all be less than ϵ. With probability one, a truly random sequence contains infinitely many runs of a million consecutive elements less than ϵ, so why can’t this happen at the beginning of the sequence?   160  RANDOM NUMBERS  3.5  On the other hand, consider Definition R2 and let P be the property that all elements of the sequence are distinct; P is true with probability one, so any sequence with a million zeros is not random by this criterion.  Now let P be the property that no element of the sequence is equal to zero; again, P is true with probability one, so by Definition R2 any sequence with a zero element is nonrandom. More generally, however, let x0 be any fixed number between zero and one, and let P be the property that no element of the sequence is equal to x0; Definition R2 now says that no random sequence may contain the element x0! We can now prove that no sequence satisfies the condition of Definition R2.  For if U0, U1, . . . is such a sequence, take x0 = U0.  Therefore if R1 is too weak a definition, R2 is certainly too strong. The “right” definition must be less strict than R2. We have not really shown that R1 is too weak, however, so let us continue to attack it some more. As mentioned above, an ∞-distributed sequence of rational numbers has been constructed.  Indeed, this is not so surprising; see exercise 18.  Almost all real numbers are irrational; perhaps we should insist that  Pr Un is rational  = 0  for a random sequence.  The definition of equidistribution, Eq.  2 , says that Pr u ≤ Un < v  = v−u. There is an obvious way to generalize this definition, using measure theory: “If S ⊆ [0 . . 1  is a set of measure µ, then  Pr Un ∈ S  = µ, In particular,   27  for all random sequences ⟨Un⟩.” if S is the set of rationals, it has measure zero, so no sequence of rational numbers is equidistributed in this generalized sense. It is reasonable to expect that Theorem B could be extended to Lebesgue integration instead of Riemann integration, if property  27  is stipulated. However, once again we find that definition  27  is too strict, for no sequence satisfies that property. If U0, U1, . . . is any sequence, the set S = {U0, U1, . . .} is of measure zero, yet Pr Un ∈ S  = 1. Thus, by the force of the same argument we used to exclude rationals from random sequences, we can exclude all random sequences.  So far Definition R1 has proved to be defensible. There are, however, some quite valid objections to it. For example, if we have a random sequence in the intuitive sense, the infinite subsequence  U0, U1, U4, U9, . . . , Un2 , . . .   28  should also be a random sequence. This is not always true for an ∞-distributed sequence. In fact, if we take any ∞-distributed sequence and set Un2 ← 0 for all n, the counts νk n  that appear in the test of k-distributivity are changed by at most n, so the limits of the ratios νk n  n remain unchanged. Definition R1 unfortunately fails to satisfy this randomness criterion.  √  Perhaps we should strengthen R1 as follows:   3.5  WHAT IS A RANDOM SEQUENCE?  161  Definition R3. A [0 . . 1  sequence is said to be “random” if each of its infinite subsequences is ∞-distributed. Once again, however, the definition turns out to be too strict; any equidistributed sequence ⟨Un⟩ has a monotonic subsequence with Us0 < Us1 < Us2 < ··· . The secret is to restrict the subsequences so that they could be defined by a person who does not look at Un before deciding whether or not it is to be in the subsequence. The following definition now suggests itself: Definition R4. A [0 . . 1  sequence ⟨Un⟩ is said to be “random” if, for every effective algorithm that specifies an infinite sequence of distinct nonnegative integers sn for n ≥ 0, the subsequence Us0, Us1, Us2, . . . corresponding to this algorithm is ∞-distributed.  The algorithms referred to in Definition R4 are effective procedures that compute sn, given n.  See the discussion in Section 1.1.  Thus, for example, the sequence ⟨πn mod 1⟩ will not satisfy R4, since it is either not equidistributed or there is an effective algorithm that determines an infinite subsequence sn with  πs0 mod 1  <  πs1 mod 1  <  πs2 mod 1  < ··· . Similarly, no explicitly defined sequence can satisfy Definition R4; this is appropriate, if we agree that no explicitly defined sequence can really be random. The explicit-looking sequence ⟨θ n mod 1⟩ actually does, however, satisfy Definition R4, for almost all real numbers θ > 1; this is no contradiction, since almost all θ are uncom- putable by algorithms. J. F. Koksma proved that ⟨θ sn mod 1⟩ is 1-distributed for almost all θ > 1, if ⟨sn⟩ is any sequence of distinct positive integers [Com- positio Math. 2  1935 , 250–258]; H. Niederreiter and R. F. Tichy strengthened Koksma’s theorem, replacing “1-distributed” by “∞-distributed” [Mathematika 32  1985 , 26–32]. Only countably many sequences ⟨sn⟩ are effectively definable, so ⟨θ n mod 1⟩ almost always satisfies R4. Definition R4 is much stronger than Definition R1; but it is still reasonable to claim that Definition R4 is too weak. For example, let ⟨Un⟩ be a truly random sequence, and define the subsequence ⟨Usn⟩ by the following rules: s0 = 0; and if n > 0, sn is the smallest integer ≥ n for which Usn−1, Usn−2, . . . , Usn−n are all less than 1 2. Thus we are considering the subsequence of values following the first consecutive run of n values less than 1 2” corresponds to the value “heads” in the flipping of a coin. Gamblers tend to feel that a long run of “heads” makes the opposite condition, “tails,” more probable, assuming that a true coin is being used; and the subsequence ⟨Usn⟩ just defined corresponds to a gambling system for a man who places his nth bet on the coin toss following the first run of n consecutive “heads.” The gambler may think that Pr Usn ≥ 1 2  is 2, but of course in a truly random sequence ⟨Usn⟩ will be completely more than 1 random. No gambling system will ever be able to beat the odds! Definition R4 says nothing about subsequences formed according to such a gambling system, so apparently we need something more. Let us define a “subsequence rule” R as an infinite sequence of functions ⟨fn x1, . . . , xn ⟩ where, for n ≥ 0, fn is a function of n variables, and the  2. Suppose that “Un < 1   162  RANDOM NUMBERS  3.5  value of fn x1, . . . , xn  is either 0 or 1. Here x1, . . . , xn are elements of some set S.  Thus, in particular, f0 is a constant function, either 0 or 1.  A sub- sequence rule R defines a subsequence of any infinite sequence ⟨Xn⟩ of elements of S as follows: The nth term Xn is in the subsequence ⟨Xn⟩R if and only if fn X0, X1, . . . , Xn−1  = 1. Note that the subsequence ⟨Xn⟩R thus defined is not necessarily infinite, and it may in fact contain no elements at all. For example, the gambler’s subsequence just described corresponds to the following subsequence rule: “f0 = 1; and for n > 0, fn x1, . . . , xn  = 1 if and only if there is some k in the range 0 < k ≤ n such that the k consecutive parameters xm, xm−1, . . . , xm−k+1 are all < 1 2 when m = n but not when k ≤ m < n.” A subsequence rule R is said to be computable if there is an effective algorithm that determines the value of fn x1, . . . , xn , when n and x1, . . . , xn are given as input. We had better restrict ourselves to computable subsequence rules when trying to define randomness, lest we obtain an overly restrictive definition like R3 above. But effective algorithms cannot deal nicely with arbitrary real numbers as inputs; for example, if a real number x is specified by an infinite radix-10 expansion, there is no algorithm to determine if x is < 1 3 or not, since all digits of the number 0.333 . . . have to be examined. Therefore computable subsequence rules do not apply to all [0 . . 1  sequences, and it is convenient to base our next definition on b-ary sequences. Definition R5. A b-ary sequence is said to be “random” if every infinite sub- sequence defined by a computable subsequence rule is 1-distributed. A [0 . . 1  sequence ⟨Un⟩ is said to be “random” if the b-ary sequence ⟨⌊bUn⌋⟩ is “random” for all integers b ≥ 2.  Note that Definition R5 says only “1-distributed,” not “∞-distributed.” It is interesting to verify that this may be done without loss of generality. For we may define an obviously computable subsequence rule R a1 . . . ak  as follows, given any b-ary number a1 . . . ak: Let fn x1, . . . , xn  = 1 if and only if n ≥ k − 1 and xn−k+1 = a1, . . . , xn−1 = ak−1, xn = ak. Now if ⟨Xn⟩ is a k-distributed b-ary sequence, this rule R a1 . . . ak  — which selects the subsequence consisting of those terms just following an occurrence of a1 . . . ak — defines an infinite sub- sequence; and if this subsequence is 1-distributed, each of the  k + 1 -tuples a1 . . . akak+1 for 0 ≤ ak+1 < b occurs with probability 1 bk+1 in ⟨Xn⟩. Thus we can prove that a sequence satisfying Definition R5 is k-distributed for all k, by induction on k. Similarly, by considering the “composition” of subsequence rules — if R1 defines an infinite subsequence ⟨Xn⟩R1, then we can define R1R2 to be the subsequence rule for which ⟨Xn⟩R1R2 =  ⟨Xn⟩R1 R2 — we find that all subsequences considered in Definition R5 are ∞-distributed.  See exercise 32.  The fact that ∞-distribution comes out of Definition R5 as a very special case is encouraging, and it is a good indication that we may at last have found the definition of randomness we have been seeking. But alas, there still is a problem. It is not clear that sequences satisfying Definition R5 must satisfy Definition R4. The “computable subsequence rules” we have just specified always enumerate   WHAT IS A RANDOM SEQUENCE?  3.5 163 subsequences ⟨Xsn⟩ for which s0 < s1 < ··· , but ⟨sn⟩ does not have to be monotone in R4; it must only satisfy the condition sn ̸= sm for n ̸= m. To meet this objection, we may combine Definitions R4 and R5 as follows: Definition R6. A b-ary sequence ⟨Xn⟩ is said to be “random” if, for every effective algorithm that specifies an infinite sequence of distinct nonnegative integers ⟨sn⟩ as a function of n and the values of Xs0, . . . , Xsn−1, the subsequence ⟨Xsn⟩ corresponding to this algorithm is “random” in the sense of Definition R5. A [0 . . 1  sequence ⟨Un⟩ is said to be “random” if the b-ary sequence ⟨⌊bUn⌋⟩ is “random” for all integers b ≥ 2.  The author contends* that this definition surely meets all reasonable philo- sophical requirements for randomness, so it provides an answer to the principal question posed in this section. D. Existence of random sequences. We have seen that Definition R3 is too strong, in the sense that no sequence can satisfy that definition; and the formulation of Definitions R4, R5, and R6 above was carried out in an attempt to recapture the essential characteristics of Definition R3. In order to show that Definition R6 is not overly restrictive, it is still necessary for us to prove that sequences satisfying all these conditions exist. Intuitively, we feel quite sure that there is no problem, because we believe that a truly random sequence exists and satisfies R6; but a proof is really necessary to show that the definition is consistent.  An interesting method for constructing sequences satisfying Definition R5 has been found by A. Wald, starting with a very simple 1-distributed sequence. Lemma T. Let the sequence of real numbers ⟨Vn⟩ be defined in terms of the binary system as follows: V1 = .1,  V4 = .001,  . . .   29  Let Ib1...br denote the set of all real numbers in [0 . . 1  whose binary representa- tion begins with 0.b1 . . . br; thus  Ib1...br = 0.b1 . . . br 2 . .  0.b1 . . . br 2 + 2−r.   30   Then if ν n  denotes the number of Vk in Ib1...br for 0 ≤ k < n, we have  V2 = .01,  V3 = .11, if n = 2r + c12r−1 + ··· + cr.  V0 = 0, Vn = .cr . . . c11   31  Proof. Since ν n  is the number of k for which k mod 2r =  br . . . b1 2, we have  ν n  = t or t + 1 when ⌊n 2r⌋ = t. Henceν n  − n 2r ≤ 1.  It follows from  31  that the sequence ⟨⌊2rVn⌋⟩ is an equidistributed 2r-ary sequence; hence by Theorem A, ⟨Vn⟩ is an equidistributed [0 . . 1  sequence. In- deed, it is pretty clear that ⟨Vn⟩ is about as equidistributed as a [0 . . 1  sequence can be.  For further discussion of this and related sequences, see J. G. van der  ν n  n − 2−r ≤ 1 n.  * At least, he made such a contention when originally preparing this material in 1966.   164  RANDOM NUMBERS  3.5  Corput, Proc. Koninklijke Nederl. Akad. Wetenschappen 38  1935 , 813–821, 1058–1066; J. H. Halton, Numerische Math. 2  1960 , 84–90, 196; S. Haber, J. Research National Bur. Standards B70  1966 , 127–136; R. Béjian and H. Faure, Comptes Rendus Acad. Sci. A285  Paris, 1977 , 313–316; H. Faure, J. Number Theory 22  1986 , 4–20; S. Tezuka, ACM Trans. Modeling and Comp. Simul. 3  1993 , 99–107. L. H. Ramshaw has shown that the sequence ⟨ϕn mod 1⟩ is slightly more equally distributed than ⟨Vn⟩; see J. Number Theory 13  1981 , 138–175.  Now let R1, R2, . . . be infinitely many subsequence rules; we seek a sequence ⟨Un⟩ for which all the infinite subsequences ⟨Un⟩Rj are equidistributed. Algorithm W  Wald sequence . Given an infinite sequence of subsequence rules R1, R2, . . . that define subsequences of [0 . . 1  sequences of rational numbers, this procedure defines a [0 . . 1  sequence ⟨Un⟩. The computation involves infinitely many auxiliary variables C[a1, . . . , ar], where r ≥ 1 and where aj = 0 or 1 for 1 ≤ j ≤ r. These variables are initially all zero. W1. [Initialize n.] Set n ← 0. W2. [Initialize r.] Set r ← 1. W3. [Test Rr.] If the element Un is to be in the subsequence defined by Rr, based on the values of Uk for 0 ≤ k < n, set ar ← 1; otherwise set ar ← 0. W4. [Is case [a1, . . . , ar] unfinished?] If C[a1, . . . , ar] < 3 · 4r−1, go to W6. W5. [Increase r.] Set r ← r + 1 and return to W3. W6. [Set Un.] Increase the value of C[a1, . . . , ar] by 1 and let k be its new value.  Set Un ← Vk, where Vk is defined in Lemma T above.  W7. [Advance n.] Increase n by 1 and return to W2.  Strictly speaking, this is not an algorithm, since it doesn’t terminate; but we could of course easily modify the procedure to make it stop when n reaches a given value. In order to grasp the idea of the construction, the reader is advised to try it out manually, replacing the number 3 · 4r−1 of step W4 by 2r during this exercise.  Algorithm W is not meant to be a practical source of random numbers. It  is intended to serve only a theoretical purpose: Theorem W. Let ⟨Un⟩ be the sequence of rational numbers defined by Algo- rithm W, and let k be a positive integer. If the subsequence ⟨Un⟩Rk is infinite, it is 1-distributed. Proof. Let A[a1, . . . , ar] denote the  possibly empty  subsequence of ⟨Un⟩ con- taining precisely those elements Un that, for all j ≤ r, belong to subsequence ⟨Un⟩Rj if aj = 1 and do not belong to subsequence ⟨Un⟩Rj if aj = 0. It suffices to prove, for all r ≥ 1 and all pairs of binary numbers a1 . . . ar and b1 . . . br, that Pr Un ∈ Ib1...br  = 2−r with respect to the subsequence the infinite sequence ⟨Un⟩Rk is the finite union of the disjoint subsequences  A[a1, . . . , ar], whenever the latter is infinite. See Eq.  30 . For if r ≥ k,   WHAT IS A RANDOM SEQUENCE?  3.5 165 A[a1, . . . , ar] for ak = 1 and aj = 0 or 1 for 1 ≤ j ≤ r, j ̸= k; and it follows that Pr Un ∈ Ib1...br  = 2−r with respect to ⟨Un⟩Rk.  See exercise 33.  This is enough to show that the sequence is 1-distributed, by Theorem A. Let B[a1, . . . , ar] denote the subsequence of ⟨Un⟩ that consists of the values for those n in which C[a1, . . . , ar] is increased by one in step W6 of the algo- rithm. By the algorithm, B[a1, . . . , ar] is a finite sequence with at most 3 · 4r−1 elements. All but a finite number of the members of A[a1, . . . , ar] come from the subsequences B[a1, . . . , ar, . . . , at], where aj = 0 or 1 for r < j ≤ t. Now assume that A[a1, . . . , ar] is infinite, and let A[a1, . . . , ar] = ⟨Usn⟩, where s0 < s1 < s2 < ··· . If N is a large integer, with 4r ≤ 4q < N ≤ 4q+1, it follows that the number of values of k < N for which Usk is in Ib1...br is  except for finitely many elements at the beginning of the subsequence   ν N  = ν N1  + ··· + ν Nm .  Here m is the number of subsequences B[a1, . . . , at] listed above in which Usk appears for some k < N; Nj is the number of values of k with Usk in the corresponding subsequence; and ν Nj  is the number of such values that are also in Ib1...br. Therefore by Lemma T,  ν N  − 2−rN =ν N1  − 2−rN1 + ··· + ν Nm  − 2−rNm   + ··· +ν Nm  − 2−rNm  ≤ν N1  − 2−rN1    ≤ m ≤ 1 + 2 + 4 + ··· + 2q−r+1 < 2q+1.  √  N.  The inequality on m follows here from the fact that, by our choice of N, the element UsN is in B[a1, . . . , at] for some t ≤ q + 1.  We have proved that ν N  N − 2−r ≤ 2q+1 N < 2  To show finally that sequences satisfying Definition R5 exist, we note first that if ⟨Un⟩ is a [0 . . 1  sequence of rational numbers and if R is a computable sub- sequence rule for a b-ary sequence, we can make R into a computable subsequence rule R′ for ⟨Un⟩ by letting f′ n x1, . . . , xn  in R′ equal fn ⌊bx1⌋, . . . ,⌊bxn⌋  in R. If the [0 . . 1  sequence ⟨Un⟩R′ is equidistributed, so is the b-ary sequence ⟨⌊bUn⌋⟩R. Now the set of all computable subsequence rules for b-ary sequences, for all values of b, is countable  since only countably many effective algorithms are possible , so they may be listed in some sequence R1, R2, . . . ; therefore Algorithm W defines a [0 . . 1  sequence that is random in the sense of Defini- tion R5.  This brings us to a somewhat paradoxical situation. As we mentioned earlier, no effective algorithm can define a sequence that satisfies Definition R4, and for the same reason there is no effective algorithm that defines a sequence satisfying Definition R5. A proof of the existence of such random sequences is necessarily nonconstructive; how then can Algorithm W construct such a sequence?  There is no contradiction here; we have merely stumbled on the fact that the set of all effective algorithms cannot be enumerated by an effective algorithm. In other words, there is no effective algorithm to select the jth computable   RANDOM NUMBERS  166 3.5 subsequence rule Rj; this happens because there is no effective algorithm to de- termine if a computational method ever terminates. But important large classes of algorithms can be systematically enumerated; thus, for example, Algorithm W shows that it is possible to construct, with an effective algorithm, a sequence that satisfies Definition R5 if we restrict consideration to subsequence rules that are “primitive recursive.” By modifying step W6 of Algorithm W, so that it sets Un ← Vk+t instead of Vk, where t is any nonnegative integer depending on a1, . . . , ar, we can show that there are uncountably many [0 . . 1  sequences satisfying Definition R5.  The following theorem shows still another way to prove the existence of uncountably many random sequences, using a less direct argument based on measure theory, even if the strong definition R6 is used: Theorem M. Let the real number x, 0 ≤ x < 1, correspond to the binary sequence ⟨Xn⟩ if the binary representation of x is  0.X0X1 . . .  2. Under this correspondence, almost all x correspond to binary sequences that are random in the sense of Definition R6.  In other words, the set of all real x that correspond to a binary sequence that is nonrandom by Definition R6 has measure zero.  Proof. Let S be an effective algorithm that determines an infinite sequence of distinct nonnegative integers ⟨sn⟩, where the choice of sn depends only on n and Xsk for 0 ≤ k < n; and let R be a computable subsequence rule. Then any binary sequence ⟨Xn⟩ leads to a subsequence ⟨Xsn⟩R, and Definition R6 says this subsequence must either be finite or 1-distributed. It suffices to prove that for fixed R and S the set N R,S  of all real x corresponding to ⟨Xn⟩, such that ⟨Xsn⟩R is infinite and not 1-distributed, has measure zero. For x has a the countably many choices of R and S. Therefore let R and S be fixed. Consider the set T a1a2 . . . ar  defined for all binary numbers a1a2 . . . ar as the set of all x corresponding to ⟨Xn⟩, such that ⟨Xsn⟩R has ≥ r elements whose first r elements are respectively equal to a1, a2, . . . , ar. Our first result is that  nonrandom binary representation if and only if x is in  N R,S , taken over  T a1a2 . . . ar  has measure ≤ 2−r.   32  To prove this, we start by observing that T a1a2 . . . ar  is a measurable set: Each element of T a1a2 . . . ar  is a real number x =  0.X0X1 . . .  2 for which there exists an integer m such that algorithm S determines distinct values s0, s1, . . . , sm, and rule R determines a subsequence of Xs0, Xs1, . . . , Xsm such that Xsm is the rth element of this subsequence. The set of all real y =  0.Y0Y1 . . .  2 such that Ysk = Xsk for 0 ≤ k ≤ m also belongs to T a1a2 . . . ar , and this is a mea- surable set consisting of the finite union of dyadic subintervals Ib1...bt. Since there are only countably many such dyadic intervals, we see that T a1a2 . . . ar  is a countable union of dyadic intervals, and it is therefore measurable. Furthermore, this argument can be extended to show that the measure of T a1 . . . ar−1 0  equals the measure of T a1 . . . ar−1 1 , since the latter is a union of dyadic intervals   3.5 167 obtained from the former by requiring that Ysk = Xsk for 0 ≤ k < m and Ysm ̸= Xsm. Now since  WHAT IS A RANDOM SEQUENCE?  T a1 . . . ar−1 0  ∪ T a1 . . . ar−1 1  ⊆ T a1 . . . ar−1 ,  the measure of T a1a2 . . . ar  is at most one-half the measure of T a1 . . . ar−1 . The inequality  32  follows by induction on r.  Now that  32  has been established, the remainder of the proof is essentially to show that the binary representations of almost all real numbers are equidis-  tributed. For 0 < ϵ < 1, let B r, ϵ  be T a1 . . . ar , where the union is taken 2 r ≥ ϵr.  summed over all values of k The number of such binary strings is C r, ϵ  =r  over all binary strings a1 . . . ar for which the number ν r  of ones among a1 . . . ar satisfies  ν r  − 1  with k − 1 by  32 ,  2 r ≥ ϵr. Exercise 1.2.10–21 proves that C r, ϵ  ≤ 2r+1e−ϵ2r; hence  33   B r, ϵ  has measure ≤ 2−rC r, ϵ  ≤ 2e−ϵ2r.  k  The next step is to define  The measure of B∗ r, ϵ  is at most  convergent series, so  B∗ r, ϵ  = B r, ϵ  ∪ B r + 1, ϵ  ∪ B r + 2, ϵ  ∪ ··· .  k≥r 2e−ϵ2k, and this is the remainder of a   34   Now if x is a real number whose binary expansion  0.X0X1 . . .  2 leads to an infinite sequence ⟨Xsn⟩R that is not 1-distributed, and if ν r  denotes the number of ones in the first r elements of the latter sequence, then  measure of B∗ r, ϵ  = 0.  lim r→∞   ≥ ϵ, ν r  r − 1  N R,S  =   2  t≥2  r≥1  B∗ r, 1 t ;  for some ϵ > 0 and infinitely many r. This means x is in B∗ r, ϵ  for all r. So finally we find that  r≥1 B∗ r, 1 t  has measure zero for all t. Hence N R,S  has  and, by  34 ,   measure zero.  From the existence of binary sequences satisfying Definition R6, we can show the existence of [0 . . 1  sequences that are random in this sense. For details, see exercise 36. The consistency of Definition R6 is thereby established. E. Random finite sequences. An argument was given above to indicate that it is impossible to define the concept of randomness for finite sequences: Any given finite sequence is as likely as any other. Still, nearly everyone would agree that the sequence 011101001 is “more random” than 101010101, and even the latter sequence is “more random” than 000000000. Although it is true that truly   168  RANDOM NUMBERS  3.5  random sequences will exhibit locally nonrandom behavior, we would expect such behavior only in a long finite sequence, not in a short one.  Several ways to define the randomness of a finite sequence have been pro- posed, and only a few of the ideas will be sketched here. For simplicity, we shall restrict our consideration to the case of b-ary sequences.  Given a b-ary sequence X0, X1, . . . , XN−1, we can say that  PrS n  ≈ p,  if ν N  N − p ≤ 1   √  N ,   35  where ν n  is the quantity appearing in Definition A at the beginning of this section. The sequence above can be called “k-distributed” if Pr XnXn+1 . . . Xn+k−1 = x1x2 . . . xk  ≈ 1 bk  for all b-ary numbers x1x2 . . . xk. Compare with Definition D. Unfortunately not  k − 1 -distributed.  a sequence might turn out to be k-distributed by this new definition when it is  A definition of randomness may now be given analogous to Definition R1,  as follows: Definition Q1. A b-ary sequence of length N is “random” if it is k-distributed  in the sense above  for all positive integers k ≤ logb N.   36   According to this definition, for example, there are 178 nonrandom binary  sequences of length 11:  00000001111 10000000111 11000000011 00000001110 10000000110 11000000010 00000001101 10000000101 11000000001 00000001011 10000000011 01000000011 00000000111  11100000001 11100000000 10100000001 01100000001  11110000000 11010000000 10110000000 01110000000  plus 01010101010 and all sequences with nine or more zeros, plus all sequences obtained from the preceding sequences by interchanging ones and zeros.  Similarly, we can formulate a definition for finite sequences analogous to Definition R6. Let A be a set of algorithms, each of which is a selection-and- choice procedure that gives a subsequence ⟨Xsn⟩R as in the proof of Theorem M. Definition Q2. The b-ary sequence X0, X1, . . . , XN−1 is  n, ϵ -random with respect to a set of algorithms A, if for every subsequence Xt1, Xt2, . . . , Xtm determined by an algorithm of A we have either m < n or  νa Xt1 , . . . , Xtm  − 1  for 0 ≤ a < b.   ≤ ϵ  b   1  m  Here νa x1, . . . , xm  is the number of a’s in the sequence x1, . . . , xm.   In other words, every sufficiently long subsequence determined by an algo- rithm of A must be approximately equidistributed.  The basic idea in this case is to let A be a set of “simple” algorithms; the number  and the complexity  of the algorithms in A can grow as N grows.   3.5  WHAT IS A RANDOM SEQUENCE?  169  As an example of Definition Q2, let us consider binary sequences, and let A  be just the following four algorithms: a  Take the whole sequence. b  Take alternate terms of the sequence, starting with the first. c  Take the terms of the sequence following a zero. d  Take the terms of the sequence following a one.  Now a sequence X0, X1, . . . , X7 is  4, 1  8 -random with respect to A if:  by  a ,  1 by  b ,  1  8 X0 + X1 + ··· + X7  − 1 4 X0 + X2 + X4 + X6  − 1 even-numbered positions;  2  2   ≤ 1  ≤ 1  8, that is, if there are 3, 4, or 5 ones; 8, that is, if there are exactly 2 ones in  by  c , there are three possibilities depending on how many zeros occupy posi- tions X0, . . . , X6: If there are 2 or 3 zeros here, there is no condition to test  since n = 4 ; if there are 4 zeros, they must respectively be followed by two zeros and two ones; and if there are 5 zeros, they must respectively be followed by two or three zeros;  by  d , we get conditions similar to those implied by  c .  It turns out that only the following binary sequences of length 8 are  4, 1 8 -  random with respect to these rules:  00001011 00011010 00011011 00100011 00100110 00100111  00101001 00101100 00110010 00110011 00110110 00111001  01001110 01011011 01011110 01100010 01100011 01100110  01101000 01101100 01101101 01110010 01110110  plus those obtained by interchanging 0 and 1 consistently.  It is clear that we could make the set of algorithms so large that no sequences satisfy the definition, when n and ϵ are reasonably small. A. N. Kolmogorov has proved that an  n, ϵ -random binary sequence will always exist, for any given N, if the number of algorithms in A does not exceed  2 e2nϵ2 1−ϵ . 1   37  This result is not nearly strong enough to show that sequences satisfying Defi- nition Q1 will exist, but the latter can be constructed efficiently using the procedure of Rees in exercise 3.2.2–21. A generalized spectral test, based on discrete Fourier transforms, can be used to test how well a sequence measures up to Definition Q1 [see A. Compagner, Physical Rev. E52  1995 , 5634–5645]. Still another interesting approach to a definition of randomness has been taken by Per Martin-Löf [Information and Control 9  1966 , 602–619]. Given a finite b-ary sequence X1, . . . , XN, let l X1, . . . , XN  be the length of the shortest Turing machine program that generates this sequence.  Alternatively, we could use other classes of effective algorithms, such as those discussed in Section 1.1.  Then l X1, . . . , XN  is a measure of the “patternlessness” of   170  RANDOM NUMBERS  3.5  the sequence, and we may equate this idea with randomness. The sequences of length N that maximize l X1, . . . , XN  may be called random.  From the standpoint of practical random number generation by computer, this is, of course, the worst definition of “randomness” that can be imagined!   Essentially the same definition of randomness was given independently by G. Chaitin at about the same time; see JACM 16  1969 , 145–159. It is interest- ing to note that even though this definition makes no reference to equidistribution properties as our other definitions have, Martin-Löf and Chaitin have proved that random sequences of this type also have the expected equidistribution properties. In fact, Martin-Löf has demonstrated that such sequences satisfy all computable statistical tests for randomness, in an appropriate sense.  For further developments in the definition of random finite sequences, see A. K. Zvonkin and L. A. Levin, Uspekhi Mat. Nauk 25, 6  November 1970 , 85–127 [English translation in Russian Math. Surveys 25, 6  November 1970 , 83–124]; L. A. Levin, Doklady Akad. Nauk SSSR 212  1973 , 548–550; L. A. Levin, Information and Control 61  1984 , 15–37. F. Pseudorandom numbers. It is comforting from a theoretical standpoint to know that random finite sequences of various flavors exist, but such theorems don’t answer the questions faced by real-world programmers. More recent devel- opments have led to a more relevant theory, based on the study of sets of finite sequences. More precisely, we consider multisets in which sequences may appear more than once.  Let S be a multiset containing bit strings  binary sequences  of length N; we call S an N-source. Let $N denote the special N-source that contains all 2N possible N-bit strings. Each element of S represents a sequence that we might use as a source of pseudorandom bits; choosing different “seed” values leads to different elements of S. For example, S might be  {B1B2 . . . BN  Bj is the most significant bit of Xj}   38  in the linear congruential sequence defined by Xj+1 =  aXj + c  mod 2e, where there is one string B1B2 . . . BN for each of the 2e starting values X0.  The basic idea of pseudorandom sequences, as we have seen throughout this chapter, is to get N bits that appear to be random, although we rely only on a few “truly random” bits when we choose the seed value. In the example just considered, we need e truly random bits to select X0; in general, selecting a member of S amounts to using lg S truly random bits, after which we proceed deterministically. If N = 106 and S = 232, we are getting more than 30,000 “apparently random” bits for each truly random bit expended. With $N instead of S, we get no such amplification, because lg $N = N.  What does it mean to be “apparently random”? A. C. Yao proposed a good definition in 1982: Consider any algorithm A that looks at a bit string B = B1 . . . BN and outputs the value A B  = 0 or 1. We may think of A as a test for randomness; for example, A might compute the distribution of runs of consecutive 0s and 1s, outputting 1 if the run lengths differ significantly from   3.5  WHAT IS A RANDOM SEQUENCE?  171  the expected distribution. Whatever A does, we can consider the probability P A, S  that A B  = 1 when B is a randomly chosen element of S, and we can compare it to the probability P A, $N  that A B  = 1 when B is a truly random bit string of length N. If P A, S  is extremely close to P A, $N  for all statistical tests A, we cannot tell the difference between the sequences of S and truly random binary sequences. Definition P. We say that an N-source S passes statistical test A with toler-  ance ϵ if P A, S −P A, $N  < ϵ. It fails the test if P A, S −P A, $N  ≥ ϵ.  The algorithm A need not be designed by statisticians. Any algorithm can be considered a statistical test for randomness, according to Definition P. We allow A to flip coins  that is, to use truly random bits  as it performs its calculations. The only requirement is that A must output 0 or 1.  Well, actually there is another requirement: We insist that A must deliver its output in a reasonable time, at least on the average. We’re not interested in algorithms that will take many years to run, because we will never notice any disparities between S and $N if our computers cannot detect them during our lifetime. The sequences of S contain only lg S bits of information, so there surely are algorithms that will eventually detect the redundancy; but we don’t care, as long as S is able to pass all the tests that really matter.  These qualitative ideas can be quantified, as we will now see. The theory is rather subtle, but it is sufficiently beautiful and important that readers who take the time to study the details carefully will be amply rewarded.  k a prediction test.  k  B  =Ak B  + Bk+1 + 1 mod 2. Thus AP  In the following discussion, the running time T A  of an algorithm A on N-bit strings is defined to be the maximum of the expected number of steps needed to output A B , maximized over all B ∈ $N; the expected number is averaged over all coin flips made by the algorithm. The first step in our quantitative analysis is to show that we may restrict the tests to be of a very special kind. Let Ak be an algorithm that depends only on the first k bits of the input string B = B1 . . . BN, where 0 ≤ k < N, and let k outputs 1 if and only if Ak has AP successfully predicted Bk+1; we call AP Lemma P1. Let S be an N-source. If S fails test A with tolerance ϵ, there is an integer k ∈ {0, 1, . . . , N−1} and a prediction test AP k   ≤ T A +O N  such that S fails AP Proof. By complementing the output of A, if necessary, we may assume that P A, S −P A, $N  ≥ ϵ. Consider the algorithms Fk that begin by flipping N −k coins and replacing Bk+1 . . . BN by random bits B′ N before executing A. Algorithm FN is the same as A, while F0 acts on S as if A were acting on $N. Let k=0  pk+1 − pk  = pN − p0 = P A, S  − P A, $N  ≥ ϵ, there is some k such that pk+1 − pk ≥ ϵ N. k be the algorithm that performs the computations of Fk and predicts the value  Fk B  + B′ AP  pk = P Fk, S . SinceN−1  k  B  =Fk B  + Bk+1 + B′  k+1 + 1  mod 2; in other words, it outputs  k with tolerance ϵ N.   mod 2.  k with T AP  k+1 . . . B′  Let AP   39   k+1   172  RANDOM NUMBERS  A careful analysis of probabilities shows that P AP  See exercise 40.   3.5 k , $N  = pk+1− pk.  k , S − P AP  2 + ϵ N.  Most N-sources S of practical interest are shift-symmetric in the sense that every substring B1 . . . Bk, B2 . . . Bk+1, . . . , BN−k+1 . . . BN of length k has the same probability distribution. This holds, for example, when S corresponds to a linear congruential sequence as in  38 . In such cases we can improve on Lemma P1 by taking k = N − 1: Lemma P2. If S is a shift-symmetric N-source that fails test A with tolerance ϵ, there is an algorithm A′ with T A′  ≤ T A  + O N  that predicts BN from B1 . . . BN−1 with probability at least 1 k in the proof of Lemma P1, Proof. but applied to BN−k . . . BN−10 . . . 0 instead of B1 . . . BN. Then A′ has the same average behavior, because of shift-symmetry. If P A, S  < P A, $N , let A′ be 1 − AP  k in the same fashion. Clearly P A′, $N  = 1 2. Now let’s specialize S even more, by supposing that each of the sequences  B1B2 . . . BN has the form fg X0 fg g X0   . . . fg[N] X0  as X0 ranges  If P A, S  > P A, $N , let A′ be the AP  over some set X, where g is a permutation of X and f x  is 0 or 1 for all x ∈ X. Our linear congruential example satisfies this restriction, with X = {0, 1, . . . , 2e − 1}, g x  =  ax + c  mod 2e, and f x  = most significant bit of x. Such N-sources will be called iterative. Lemma P3. If S is an iterative N-source that fails test A with tolerance ϵ, there is an algorithm A′ with T A′  ≤ T A  + O N  that predicts B1 from B2 . . . BN with probability at least 1 Proof. An iterative N-source is shift-symmetric, and so is its reflection SR = {BN . . . B1  B1 . . . BN ∈ S}. Therefore Lemma P2 applies to SR.  2 + ϵ N.  The permutation g x  =  ax + c  mod 2e is easy to invert, in the sense that we can determine x from g x  whenever a is odd. But many easily computed permutation functions are “one-way” — hard to invert — and we will see that this makes them provably good sources of pseudorandom numbers. Lemma P4. Let S be an iterative N-source corresponding to f, g, and X. If S fails test A with tolerance ϵ, there is an algorithm G that correctly guesses f x , given g x , with probability ≥ 1 2 + ϵ N, when x is a random element of X. The  running time T G  is at most T A  + O N T f  + T g . Proof. Given y = g x , the desired algorithm G computes B2 = fg x , B3 = fg g x  , . . . , BN = fg[N−1] x  and applies the algorithm A′ of Lemma P3.  It guesses f x  = B1 with probability ≥ 1 2 + ϵ N, because g is a permutation of X, and B1 . . . BN is the element of S corresponding to the seed value X0 for which we have g X0  = x.  In order to use Lemma P4, we need to amplify the ability to guess a single bit f x  to an ability to guess x itself, given only the value of g x . There is   3.5  WHAT IS A RANDOM SEQUENCE?  173  a nice general way to do this, using the properties of Boolean functions, if we extend S so that many different functions f x  need to be guessed.  However, the method is somewhat technical, so the first-time reader may want to skip down to Theorem G before looking closely at the details that follow.  Suppose G z1 . . . zR  is a binary-valued function on R-bit strings that is good at guessing a function of the form f z1 . . . zR  =  x1z1 +··· + xRzR  mod 2 for some fixed x = x1 . . . xR. It is convenient to measure the success of G by computing the expected value  s = E −1 G z1...zR +x1z1+···+xRzR ,  2 s.  2 + 1 4  and p = 7   40  averaged over all possibilities for z1 . . . zR. This is the sum of correct guesses minus incorrect guesses, divided by 2R; so if p is the probability that G is correct, we have s = p −  1 − p , or p = 1 For example, suppose R = 4 and G z1z2z3z4  = [z1 ̸= z2][z3 + z4 < 2]. This function has success rate s = 3 8  if x = 1100, because it equals x · z mod 2 =  z1 + z2  mod 2 for all 4-bit strings z except 0111 or 1011. It also has success rate 1 4 when x = 0000, 0011, 1101, or 1110; so there are five plausible possibilities for x. The other eleven x’s make s ≤ 0. The following algorithm magically discovers x in most cases when G is a successful guesser in the sense just described. More precisely, the algorithm constructs a short list that has a good chance of containing x. Algorithm L  Amplification of linear guesses . Given a binary-valued function G z1 . . . zR  and a positive integer k, this algorithm outputs a list of 2k binary sequences x = x1 . . . xR with the property that x is likely to be output when G z1 . . . zR  is a good approximation to the function  x1z1 + ··· + xRzR  mod 2. L1. [Construct a random matrix.] Generate random bits Bij for 1 ≤ i ≤ k and L2. [Compute signs.] For 1 ≤ i ≤ R, and for all bit strings b = b1 . . . bk, compute  41   hi b  =   −1 b·c+G cB+ei   1 ≤ j ≤ R.  c̸=0  where ei is the R-bit string 0 . . . 010 . . . 0 having 1 in position i, and where cB is the string d1 . . . dR with dj =  B1jc1 +···+Bkjck  mod 2.  In other words the binary vector c1 . . . ck is multiplied by the k × R binary matrix B.  The sum is taken over all 2k − 1 bit strings c1 . . . ck ̸= 0 . . . 0. It can be evaluated for each i with k · 2k additions and subtractions, using Yates’s method for the Hadamard transform; see the remarks following Eq. 4.6.4– 38 .  L3. [Output the guesses.] For all 2k choices of b = b1 . . . bk, output the string  x b  = [h1 b  < 0] . . . [hR b  < 0]. To prove that Algorithm L works properly, we must show that a given string x will probably be output whenever it deserves to be. Notice first that if we change G to G′, where G′ z  =  G z  + zj  mod 2, the original G z  is a good approximation to x · z mod 2 if and only if the new G′ z  is a good   i b  =  h′   b + Bj  mod 2 ,  RANDOM NUMBERS  174 3.5 approximation to  x + ej  · z mod 2, where ej is the unit-vector string defined in step L2. Moreover, if we apply the algorithm to G′ instead of G, we get   −1 b·c+G cB+ei + cB+ei ·ej =  −1 δij hi  c̸=0  x b + Bj  mod 2 + ej, modulo 2. As b runs through all k-bit strings, so does  where Bj is column j of B. Therefore step L3 outputs the vectors x′ b  =   b + Bj  mod 2, and the effect is to complement bit j of every x in the output. We need therefore prove only that the vector x = 0 . . . 0 is likely to be output whenever G z  is a good approximation to the constant function 0. We will show, in fact, that x 0 . . . 0  equals 0 . . . 0 in step L3 with high probability, whenever G z  is a lot more likely to be 0 than 1 and k is sufficiently large. More  precisely, the condition    −1 G cB+ei  > 0  c̸=0  2, if s = E −1 G z  is positive when  holds for 1 ≤ i ≤ R with probability > 1 averaged over all 2R possibilities for z and if k is large enough.  of Chebyshev’s inequality that, for any fixed i, the sum   The key observation is that, for each fixed c = c1 . . . ck ̸= 0 . . . 0, the string d = cB is uniformly distributed: Every value of d occurs with probability 1 2R, because the bits of B are random. Furthermore, when c ̸= c′ = c′ 1 . . . c′ k, the strings d = cB and d′ = c′B are independent: Every value of the pair  d, d′  occurs with probability 1 22R. Therefore we can argue as in the proof c̸=0 −1 G cB+ei  is negative with probability at most 1   2k − 1 s2 .  Exercise 42 contains the details.  It follows that R   2k − 1 s2  is an upper bound on the probability that x 0  is nonzero in step L3. Theorem G. outputs x with probability ≥ 1 make 2kR evaluations of G.  If s = E −1 G z +x·z > 0 and 2k > ⌈2R s2⌉, Algorithm L  2. The running time is O k2kR  plus the time to  Now we are ready to prove that the muddle-square sequence of Eq. 3.2.2– 17  is a good source of  pseudo random numbers. Suppose 2R−1 < M = P Q < 2R, where P and Q are prime numbers of the form 4k + 3 in the respective ranges 2 R−2  2 < P < 2 R−1  2, 2R 2 < Q < 2 R+1  2. We will call M an R-bit Blum integer, because the importance of such numbers for cryptography was first pointed out by Manuel Blum [COMPCON 24  Spring 1982 , 133–137]. Blum originally suggested that P and Q both have R 2 bits, but Algorithm 4.5.4D shows that it is better to choose P and Q as stated here so that Q−P > .29×2R 2. Choose X0 at random in the range 0 < X0 < M, with X0 ⊥ M; also let Z be a random R-bit mask. We can construct an iterative N-source S by letting X be the set of all  x, z, m  that are possibilities for  X0, Z, M , with the further restriction that x ≡ a2  modulo m  for some a. The function g x, z, m  =  x2 mod m, z, m  is easily shown to be a permutation of X  see, for example, exercise 4.5.4–35 . The function f x, z, m  that extracts bits in this   0 mod M.  WHAT IS A RANDOM SEQUENCE?  E −1 G y,z,m +z·x ≥   1  3.5 175 iterative source is x · z mod 2. Our starting value  X0, Z, M  isn’t necessarily in X, but g X0, Z, M  is uniformly distributed in X, because exactly four values of X0 have a given square X2 Theorem P. Let S be the N-source defined by the muddle-square method on R-bit moduli, and suppose S fails some statistical test A with tolerance ϵ ≥ 1 2N. Then we can construct an algorithm F that finds factors of random R-bit Blum integers M = P Q having the form described above, with success probability at least ϵ  4N  and with running time T F  = O N 2R2ϵ−2T A  + N 3R4ϵ−2 . Proof. Multiplication mod M can be done in O R2  steps; hence T f  + T g  = O R2 . Lemma P4 therefore asserts the existence of a guessing algorithm G with success rate ϵ N and T G  ≤ T A + O N R2 . We can construct G from A using the method of exercise 41. This algorithm G has the property that s = 2 − ϵ N  = 2ϵ N, where the expected value is taken over all  x, z, m  ∈ X, and where  y, z, m  = g x, z, m . The desired algorithm F proceeds as follows. Given a random M = P Q with unknown P and Q, it computes a random X0 between 0 and M, and stops immediately with a known factorization if gcd X0, M  ̸= 1. Otherwise it applies 0 mod M, z, M  and k = ⌈lg 1 + 2N 2R ϵ2 ⌉. If Algorithm L with G z  = G X2 one of the 2k values x output by that algorithm satisfies x2 ≡ X2 0  modulo M , there is a 50:50 chance that x ̸≡ ±X0; then gcd X0 − x, M  and gcd X0 + x, M  are the prime factors of M.  See Rabin’s “SQRT box” in Section 4.5.4.  The running time of this algorithm is clearly O N 2R2ϵ−2T A  + N 3R4ϵ−2 , since ϵ ≥ 2−N. The probability that it succeeds in factoring M can be esti- let sxm = 2−R −1 G y,z,m +z·x summed over all R-bit numbers z; thus s = mated as follows. Let n = X 2R be the number of choices of  x, m , and  x,m sxm n ≥ 2ϵ N. Let t be the number of  x, m  such that sxm ≥ ϵ N. The  2 + ϵ N  −   1  probability that our algorithm deals with such a pair  x, m  is  ≥  x,m  t n  [sxm ≥ ϵ N ] sxm n  =  x,m  ≥ 2ϵ  N  1 − [sxm < ϵ N ] sxm −  n [sxm < ϵ N ] sxm n  x,m  ≥ ϵ N  .  And in such a case it finds x with probability ≥ 1 2k > ⌈2R s2  xm⌉; so it finds a factor with probability ≥ 1 4.  2, by Theorem G, since we have  What does Theorem P imply, from a practical standpoint? Our proof shows that the constant implied by the O is small; let us assume that the running time for factoring is at most 10 N 2R2ϵ−2T A + N 3R4ϵ−2 . Many of the world’s greatest mathematicians have worked on the problem of factoring large numbers, especially after factoring was shown to be highly relevant to cryptography in the late 1970s. Since they haven’t found a good solution, we have excellent reason to believe that factoring is hard; hence Theorem P will show that T A  must be large on all algorithms that detect nonrandomness of muddle-square bits.   176  RANDOM NUMBERS  3.5  Long computations are conveniently measured in MIP-years, the number of instructions executed per Gregorian year by a machine that performs a million instructions per Gregorian second — namely 31,556,952,000,000 ≈ 3.16×1013. In 1995, the time to factor a number of 120 decimal digits  400 bits , using the most highly tuned algorithms, was more than 250 MIP-years. The most optimistic researchers who have worked on factorization would be astonished if an algorithm  were discovered that requires only expR1 4 ln R 3 4 instructions as R → ∞.  But let us assume that such a breakthrough has been achieved, for at least a not-too-small fraction of the R-bit Blum integers M. Then we could factor many numbers of about 50000 bits  15000 digits  in 2×1025 MIP-years. If we generate N = 1000 random bits by muddle-square with R = 50000, and if we assume that all algorithms that are good enough to factor at least 400000 of the 50000-bit Blum integers must run at least 2 × 1025 MIP-years, Theorem P tells us that every such set of 1000 bits will pass all statistical tests for randomness whose running time T A  is less than 70000 MIP-years: No such algorithm A will be able to distinguish such bits from a truly random sequence with probability ≥ ϵ = 1 100. Impressive? No. Such a result is hardly surprising, since we need to specify about 150000 truly random bits just to start up the muddle-square method with X0, Z, and M when R = 50000. Of course we should be able to get 1000 random bits back from such an investment!  1  But in general, the formula becomes  100000 N−2R−2 expR1 4 ln R 3 4 − N R2,  1  T A  ≥  under our conservative assumptions, when ϵ = 1 100; the N R2 term is negligible when R is large. So let’s set R = 200000 and N = 1010. Then we get ten billion pseudorandom muddle-bits from ≈ 3R = 600000 truly random bits, passing all statistical tests that require fewer than 7.486×1010 MIP-years = 74.86 gigaMIP- years. With R = 333333 and N = 1013 the computation time needed to detect any statistical bias increases to 535 teraMIP-years.  The simple pseudorandom generator 3.2.2– 16 , which avoids the random mask Z, can also be shown to pass all polynomial-time tests for randomness if fac- toring is intractable.  See exercise 4.5.4–43.  But the known performance guar- antees for the simpler method are somewhat weaker than for muddle-square; cur-  rently they are ON 4Rϵ−4 log NRϵ−1  versus the O N 2R2ϵ−2  of Theorem P.  Everyone believes that there is no factoring algorithm for R-bit numbers whose running time is polynomial in R. If that conjecture is true in a stronger form, so that we cannot even factor 1 Rk of the R-bit Blum integers in poly- nomial time for any fixed k, Theorem P proves that the muddle-square method generates pseudorandom numbers that pass all polynomial-time statistical tests for randomness.  Stating this another way: If you generate random bits with the muddle- square method for suitably chosen N and R, you either get numbers that pass all reasonable statistical tests, or you get fame and fortune for discovering a new factorization algorithm.   3.5  WHAT IS A RANDOM SEQUENCE?  177  G. Summary, history, and bibliography. We have defined several degrees of randomness that a sequence might possess. An infinite sequence that is ∞-distributed satisfies a great many useful properties that are expected of random sequences, and there is a rich theory con- cerning ∞-distributed sequences.  The exercises below develop several important properties of such sequences that have not been mentioned in the text.  Defini- tion R1 is therefore an appropriate basis for theoretical studies of randomness. The concept of an ∞-distributed b-ary sequence was introduced in 1909 by Emile Borel. He essentially defined the concept of an  m, k -distributed sequence, and showed that the b-ary representations of almost all real numbers are  m, k - distributed for all m and k. He called such numbers entirely normal to base b, and he stated Theorem C informally without apparently realizing that it required proof [Rendiconti Circ. Mat. Palermo 27  1909 , 247–271, §12.] The notion of an ∞-distributed sequence of real numbers, also called a completely equidistributed sequence, first appeared in a note by N. M. Korobov in Doklady Akad. Nauk SSSR 62  1948 , 21–22. Korobov and several of his colleagues developed the theory of such sequences quite extensively in a series of papers during the 1950s. Completely equidistributed sequences were inde- pendently studied by Joel N. Franklin, Math. Comp. 17  1963 , 28–59, in a paper that is particularly noteworthy because it was inspired by the problem of random number generation. The book Uniform Distribution of Sequences by L. Kuipers and H. Niederreiter  New York: Wiley, 1974  is an extraordinarily complete source of information about the rich mathematical literature concerning k-distributed sequences of all kinds. We have seen, however, that ∞-distributed sequences need not be suffi- ciently haphazard to qualify completely as “random.” Three definitions, R4, R5, and R6, were formulated above to provide the additional conditions; and Definition R6, in particular, seems to be an appropriate way to define the concept of an infinite random sequence. It is a precise, quantitative statement that may well coincide with the intuitive idea of true randomness.  Historically, the development of these definitions was primarily influenced by the quest of R. von Mises for a good definition of “probability.” In Math. Zeitschrift 5  1919 , 52–99, von Mises proposed a definition similar in spirit to Definition R5, although stated too strongly  like our Definition R3  so that no sequences satisfying the conditions could possibly exist. Many people no- ticed this discrepancy, and A. H. Copeland [Amer. J. Math. 50  1928 , 535– 552] suggested weakening von Mises’s definition by substituting what he called “admissible numbers”  or Bernoulli sequences . These are equivalent to ∞- distributed [0 . . 1  sequences in which all entries Un have been replaced by 1 if Un < p or by 0 if Un ≥ p, for a given probability p. Thus Copeland was essentially suggesting a return to Definition R1. Then Abraham Wald showed that it is not necessary to weaken von Mises’s definition so drastically, and he proposed substituting a countable set of subsequence rules. In an important paper [Ergebnisse eines math. Kolloquiums 8  Vienna: 1937 , 38–72], Wald essentially proved Theorem W, although he made the erroneous assertion that   178  RANDOM NUMBERS  3.5  the sequence constructed by Algorithm W also satisfies the stronger condition that Pr Un ∈ A  = measure of A, for all Lebesgue measurable A ⊆ [0 . . 1 . We have observed that no sequence can satisfy this property. The concept of “computability” was still very much in its infancy when Wald wrote his paper, and A. Church [Bull. Amer. Math. Soc. 46  1940 , 130– 135] showed how the precise notion of “effective algorithm” could be added to Wald’s theory to make his definitions completely rigorous. The extension to Definition R6 was due essentially to A. N. Kolmogorov [Sankhy¯a A25  1963 , 369–376], who proposed Definition Q2 for finite sequences at the same time. Another definition of randomness for finite sequences, somewhere “between” Def- initions Q1 and Q2, had been formulated many years earlier by A. S. Besicovitch [Math. Zeitschrift 39  1934 , 146–156].  The publications of Church and Kolmogorov considered only binary se- quences for which Pr Xn = 1  = p for a given probability p. Our discussion in this section has been slightly more general, since a [0 . . 1  sequence essentially represents all p at once. The von Mises–Wald–Church definition has been refined in yet another interesting way by J. V. Howard, Zeitschr. für math. Logik und Grundlagen der Math. 21  1975 , 215–224.  Another important contribution was made by Donald W. Loveland [Zeitschr. für math. Logik und Grundlagen der Math. 12  1966 , 279–294], who discussed Definitions R4, R5, R6, and several intermediate concepts. Loveland proved that there are R5-random sequences that do not satisfy R4, thereby establishing the need for a stronger definition such as R6. In fact, he defined a rather simple permutation ⟨f n ⟩ of the nonnegative integers, and an Algorithm W′ analogous to Algorithm W, such that  Pr Uf n  ≥ 1  2  − Pr Uf n  ≥ 1  2  ≥ 1  2  for every R5-random sequence ⟨Un⟩ produced by Algorithm W′ when it is given an infinite set of subsequence rules Rk. Although Definition R6 is intuitively much stronger than R4, it is apparently not a simple matter to prove this rigorously, and for several years it was an open question whether or not R4 implies R6. Finally Thomas Herzog and James C. Owings, Jr., discovered how to construct a large family of sequences that satisfy [See Zeitschr. für math. Logik und Grundlagen der Math. 22 R4 but not R6.  1976 , 385–389.]  Kolmogorov wrote another significant paper [Problemy Peredači Informatsii 1  1965 , 3–11] in which he considered the problem of defining the “information content” of a sequence, and this work led to Chaitin and Martin-Löf’s interesting definition of finite random sequences via “patternlessness.” [See IEEE Trans. IT-14  1968 , 662–664.] The ideas can also be traced to R. J. Solomonoff, Information and Control 7  1964 , 1–22, 224–254; IEEE Trans. IT-24  1978 , 422–432; J. Computer and System Sciences 55  1997 , 73–88.  For a philosophical discussion of random sequences, see K. R. Popper, The Logic of Scientific Discovery  London, 1959 , especially the interesting construc- tion on pages 162–163, which he first published in 1934.   3.5  WHAT IS A RANDOM SEQUENCE?  179  Further connections between random sequences and recursive function the- ory have been explored by D. W. Loveland, Trans. Amer. Math. Soc. 125  1966 , 497–510. See also C.-P. Schnorr [Zeitschr. Wahr. verw. Geb. 14  1969 , 27–35], who found strong relations between random sequences and the “species of measure zero” defined by L. E. J. Brouwer in 1919. Schnorr’s subsequent book Zufälligkeit und Wahrscheinlichkeit [Lecture Notes in Math. 218  Berlin: Springer, 1971 ] gives a detailed treatment of the entire subject of randomness and makes an excellent introduction to the ever-growing advanced literature on the topic. Important developments during the next two decades are surveyed in An Introduction to Kolmogorov Complexity and Its Applications  Springer, 1993 , by Ming Li and Paul M. B. Vitányi.  The foundations of the theory of pseudorandom sequences and effective information were laid by Manuel Blum, Silvio Micali, and Andrew Yao [FOCS 23  1982 , 80–91, 112–117; SICOMP 13  1984 , 850–864], who constructed the first explicit sequences that pass all feasible statistical tests. Blum and Micali introduced the notion of a “hard-core bit,” a Boolean function f such that f x   and g x  are easily computed although fg[−1] x  is not; their paper was the  origin of Lemma P4. Leonid Levin developed the theory further [Combinatorica 7  1987 , 357–363], then he and Oded Goldreich [STOC 21  1989 , 25–32] analyzed algorithms such as the muddle-square method and showed that similar use of a mask yields hard-core bits in many further cases. Finally Charles Rackoff refined the methods of that paper by introducing and analyzing Algorithm L [see L. Levin, J. Symbolic Logic 58  1993 , 1102–1103].  Many other authors have contributed to the theory — notably Impagliazzo, Levin, Luby, and Håstad, who showed [SICOMP 28  1999 , 1364–1396] that pseudorandom sequences can be constructed from any one-way function — but such results are beyond the scope of this book. The practical implications of theoretical work on pseudorandomness were first investigated empirically by P. L’Ecuyer and R. Proulx, Proc. Winter Simulation Conf. 22  1989 , 467–476.  If the numbers are not random, they are at least higgledy-piggledy. — GEORGE MARSAGLIA  1984   EXERCISES 1. [10] Can a periodic sequence be equidistributed? 2. [10] Consider the periodic binary sequence 0, 0, 1, 1, 0, 0, 1, 1, 1-distributed? Is it 2-distributed? Is it 3-distributed? 3. [M22] Construct a periodic ternary sequence that is 3-distributed. 4. [HM14] Prove that Pr S n  and T  n  +Pr S n  or T  n   = Pr S n  +Pr T  n  , for any two statements S n  and T  n , provided that at least three of the limits exist. For example, if a sequence is 2-distributed, we would find that  Is it  . . . .  Pr u1 ≤ Un < v1 or u2 ≤ Un+1 < v2  = v1 − u1 + v2 − u2 −  v1 − u1  v2 − u2 .   3.5  180  2 ?  RANDOM NUMBERS   cid:120  5. [HM22] Let Un =  2⌊lg n+1 ⌋ 3  mod 1. What is Pr Un < 1 6. [HM23] Let S1 n , S2 n , . . . be an infinite sequence of statements about mutually disjoint events; that is, Si n  and Sj n  cannot simultaneously be true if i ̸= j. Assume  that Pr Sj n   exists for each j ≥ 1. Show that Pr Sj n  is true for some j ≥ 1  ≥ j≥1 Pr Sj n  , and give an example to show that equality need not hold. 7. [HM27] Let {Sij n } be a family of statements such that Pr Sij n   exists for all If i, j ≥ 1. Assume that for all n > 0, Sij n  is true for exactly one pair of integers i, j. for all i ≥ 1, and that it equals i,j≥1 Pr Sij n   = 1, does it follow that “Pr Sij n  is true for some j ≥ 1 ” exists j≥1 Pr Sij n  ? 9. [HM20] Prove Lemma E. [Hint: Considerm  cid:120  10. [HM22] Where was the fact that m divides q used in the proof of Theorem C?  8. [M15] Prove  13 .  j=1 yjn − α 2.]  11. [M10] Use Theorem C to prove that if a sequence ⟨Un⟩ is ∞-distributed, so is the subsequence ⟨U2n⟩. 12. [HM20] Show that a k-distributed sequence passes the “maximum-of-k test,” in  cid:120  13. [HM27] Show that an ∞-distributed [0 . . 1  sequence passes the “gap test” in the the following sense: Pr u ≤ max Un, Un+1, . . . , Un+k−1  < v  = vk − uk. following sense: If 0 ≤ α < β ≤ 1 and p = β − α, let f 0  = 0, and for n ≥ 1 let f n  be the smallest integer m > f n − 1  such that α ≤ Um < β; then  Pr f n  − f n − 1  = k  = p 1 − p k−1  .  14. [HM25] Show that an ∞-distributed sequence passes the “run test” in the follow- ing sense: If f 0  = 0 and if, for n ≥ 1, f n  is the smallest integer m > f n − 1  such that Um−1 > Um, then  Pr f n  − f n − 1  = k  = 2k  k + 1 ! − 2 k + 1   k + 2 !.   cid:120  15. [HM30] Show that an ∞-distributed sequence passes the “coupon-collector’s test” when there are only two kinds of coupons, in the following sense: Let X1, X2, . . . be an ∞-distributed binary sequence. Let f 0  = 0, and for n ≥ 1 let f n  be the smallest integer m > f n − 1  such that {Xf n−1 +1, . . . , Xm} is the set {0, 1}. Prove that Pr f n  − f n − 1  = k  = 21−k, for k ≥ 2.  See exercise 7.  16. [HM38] Does the coupon-collector’s test hold for ∞-distributed sequences when there are more than two kinds of coupons?  See the previous exercise.  17. [HM50] If r is any given rational number, Franklin has proved that the sequence ⟨rn mod 1⟩ is not 2-distributed. But is there any rational number r for which this sequence is equidistributed? In particular, is the sequence equidistributed when r = 3 2? [See K. Mahler, Mathematika 4  1957 , 122–124.]   cid:120  18. [HM22] Prove that if U0, U1, . . . is k-distributed, so is the sequence V0, V1, . . . ,  where Vn = ⌊nUn⌋ n. 19. [HM35] Consider a modification of Definition R4 that requires the subsequences to be only 1-distributed instead of ∞-distributed. Is there a sequence that satisfies this weaker definition, but that is not ∞-distributed?  Is the weaker definition really weaker?    3.5   cid:120  20. [HM36]  N. G. de Bruijn and P. Erdős.  The first n points of any [0 . . 1  sequence  WHAT IS A RANDOM SEQUENCE?  ⟨Un⟩ with U0 = 0 divide the interval [0 . . 1  into n subintervals; let those subintervals n +···+l  1   n  have lengths l n = 1. One way to measure the equitability of the distribution of ⟨Un⟩ is to consider  n ≥ ··· ≥ l  2    n  n , because l   n  n . Clearly l  n ≥ 1  1   n ≥ l  1   n ≥ l  181   1  n   n  n .  nl  nl  and  L = lim sup n→∞  L = lim inf n→∞ a  What are L and L for van der Corput’s sequence  29 ? for 1 ≤ k ≤ n. Use this result to prove that L ≥ 1  ln 2. b  Show that l c  Prove that L ≤ 1  ln 4. [Hint: For each n there are numbers a1, . . . , a2n such that for 1 ≤ k ≤ 2n. Moreover, each integer 2, . . . , n occurs at most d  Show that the sequence ⟨Wn⟩ defined by Wn = lg 2n + 1  mod 1 satisfies 1  ln 2 >  2n ≥ l  k  l twice in {a1, . . . , a2n}.]  n+k−1 ≥ l   n+ak  n+ak   k  n   1   n ≥ nl  1   nl   n  n > 1  ln 4 for all n; hence it achieves the optimum L and L.  21. [HM40]  L. H. Ramshaw.  a  Continuing the previous exercise, is the sequence ⟨Wn⟩ equidistributed?  b  Show that ⟨Wn⟩ is the only [0 . . 1  sequence for which we have k  n ≤  j  lg 1 + k n  whenever 1 ≤ k ≤ n. c  Let ⟨fn l1, . . . , ln ⟩ be any sequence of continuous functions on the sets of n-tuples { l1, . . . , ln   l1 ≥ ··· ≥ ln and l1 + ··· + ln = 1}, satisfying the following two properties:  j=1 l  fmn  1  if k  m l1, . . . , 1  j=1 lj ≥k  m l2, . . . , 1  m l2, . . . , 1  m l1, 1 j=1 l′j for 1 ≤ k ≤ n n ; −nl  1   m ln  = fn l1, . . . , ln ;  m ln, . . . , 1 then fn l1, . . . , ln  ≥ fn l′1, . . . , l′n . n + ··· + l  1 2  n   1  n   n , . . . , l   n 2 n   1  n  l   .] Let  [Examples are: nl   n  n ; n l fn l   n  n ; l F = lim sup n→∞ for the sequence ⟨Wn⟩. Show that fn l n   ≤ F for all n, with respect to  n  ⟨Wn⟩; also lim supn→∞ n   ≥ F with respect to every other [0 . . 1   n  sequence.    cid:120  22. [HM30]  Hermann Weyl.  Show that the [0 . . 1  sequence ⟨Un⟩ is k-distributed if  and only if   1  n , . . . , l   1  n , . . . , l  fn l  exp 2πi c1Un + ··· + ckUn+k−1   = 0  1 N  lim N→∞  for every set of integers c1, c2, . . . , ck not all zero. 23. [M32]  a  Show that a [0 . . 1  sequence ⟨Un⟩ is k-distributed if and only if all of the sequences ⟨ c1Un +c2Un+1 +···+ckUn+k−1  mod 1⟩ are 1-distributed, whenever c1, c2, . . . , ck are integers not all zero.  b  Show that a b-ary sequence ⟨Xn⟩ is k-distributed if and only if all of the sequences ⟨ c1Xn + c2Xn+1 + ··· + ckXn+k−1  mod b⟩ are 1-  cid:120  24. [M35]  J. G. van der Corput.   a  Prove that the [0 . . 1  sequence ⟨Un⟩ is equidis- distributed, whenever c1, c2, . . . , ck are integers with gcd c1, . . . , ck  = 1. tributed whenever the sequences ⟨ Un+k − Un  mod 1⟩ are equidistributed for all k > 0.  b  Consequently ⟨ αdnd + ··· + α1n + α0  mod 1⟩ is equidistributed, when d > 0 and αd is irrational.  0≤n<N   182  RANDOM NUMBERS  3.5  25. [HM20] A sequence is called a “white sequence” if all serial correlations are zero; that is, if the equation in Corollary S is true for all k ≥ 1.  By Corollary S, an ∞- distributed sequence is white.  Show that if a [0 . . 1  sequence is equidistributed, it is white if and only if   Uj − 1  2  Uj+k − 1  2  = 0,  for all k ≥ 1.    0≤j<n  1 n  lim n→∞  26. [HM34]  J. Franklin.  A white sequence, as defined in the previous exercise, can definitely fail to be random. Let U0, U1, . . . be an ∞-distributed sequence, and define the sequence V0, V1, . . . as follows:  where G is the set   V2n−1, V2n  =  U2n−1, U2n   V2n−1, V2n  =  U2n, U2n−1   if  U2n−1, U2n  ∈ G, if  U2n−1, U2n   ∈ G,  { x, y   x − 1  2 ≤ y ≤ x or x + 1  2 ≤ y}.  Show that  a  V0, V1, . . . is equidistributed and white;  b  Pr Vn > Vn+1  = 5 points out the weakness of the serial correlation test.  27. [HM48] What is the highest possible value for Pr Vn > Vn+1  in an equidistrib- uted, white sequence?  D. Coppersmith has constructed such a sequence achieving the value 7 8.    cid:120  28. [HM21] Use the sequence  11  to construct a [0 . . 1  sequence that is 3-distributed,  for which Pr U2n ≥ 1 29. [HM34] Let X0, X1, . . . be a  2k -distributed binary sequence. Show that  8.  This  2  = 3 4.   cid:120  30. [M39] Construct a binary sequence that is  2k -distributed, and for which  Pr X2n = 0  ≤ 1  2 +  22k.  2k − 1 2k − 1  k     k  22k.  Pr X2n = 0  = 1  2 +   Therefore the inequality in the previous exercise is the best possible.  31. [M30] Show that [0 . . 1  sequences exist that satisfy Definition R5, yet νn n ≥ 1 2 for all n > 0, where νn is the number of j < n for which Uj < 1 2.  This might be considered a nonrandom property of the sequence.  32. [M24] Given that ⟨Xn⟩ is a “random” b-ary sequence according to Definition R5, and that R is a computable subsequence rule that specifies an infinite subsequence ⟨Xn⟩R, show that the latter subsequence is not only 1-distributed, it is “random” by Definition R5. 33. [HM22] Let ⟨Urn⟩ and ⟨Usn⟩ be infinite disjoint subsequences of a sequence ⟨Un⟩.  Thus, r0 < r1 < r2 < ··· and s0 < s1 < s2 < ··· are increasing sequences of integers and rm ̸= sn for any m, n.  Let ⟨Utn⟩ be the combined subsequence, so that t0 < t1 < t2 < ··· and the set {tn} = {rn}∪{sn}. Show that if Pr Urn ∈ A  = Pr Usn ∈ A  = p, then Pr Utn ∈ A  = p.   cid:120  34. [M25] Define subsequence rules R1, R2, R3, . . . such that Algorithm W can be  used with these rules to give an effective algorithm to construct a [0 . . 1  sequence satisfying Definition R1.   3.5  183   cid:120  35. [HM35]  D. W. Loveland.  Show that if a binary sequence ⟨Xn⟩ is R5-random,  WHAT IS A RANDOM SEQUENCE?  and if ⟨sn⟩ is any computable sequence as in Definition R4, then Pr Xsn = 1  ≥ 1 2 and Pr Xsn = 1  ≤ 1 2. 36. [HM30] Let ⟨Xn⟩ be a binary sequence that is “random” according to Defini- tion R6. Show that the [0 . . 1  sequence ⟨Un⟩ defined in binary notation by the scheme U0 =  0.X0 2, U1 =  0.X1X2 2, U2 =  0.X3X4X5 2, U3 =  0.X6X7X8X9 2, . . . is random in the sense of Definition R6. 37. [M37]  D. Coppersmith.  Define a sequence that satisfies Definition R4 but not Definition R5. [Hint: Consider changing U0, U1, U4, U9, . . . in a truly random sequence.] 38. [M49]  A. N. Kolmogorov.  Given N, n, and ϵ, what is the smallest number of algorithms in a set A such that no  n, ϵ -random binary sequences of length N exist with respect to A?  If exact formulas cannot be given, can asymptotic formulas be found? The point of this problem is to discover how close the bound  37  comes to being “best possible.”  39. [HM45]  W. M. Schmidt.  Let Un be a [0 . . 1  sequence, and let νn u  be the number of nonnegative integers j ≤ n such that 0 ≤ Uj < u. Prove that there is a positive constant c such that, for any N and for any [0 . . 1  sequence ⟨Un⟩, we have  νn u  − un > c ln N  for some n and u with 0 ≤ n < N, 0 ≤ u < 1.  In other words, no [0 . . 1  sequence can be too equidistributed.  40. [M28] Complete the proof of Lemma P1. 41. [M21] Lemma P2 shows the existence of a prediction test, but its proof relies on the existence of a suitable k without explaining how we could find k constructively from A. Show that any algorithm A can be converted into an algorithm A′ with T  A′  ≤ T  A  + O N  that predicts BN from B1 . . . BN−1 with probability at least  cid:120  42. [M28]  Pairwise independence.  2 +  P  A, S  − P  A, $N   N on any shift-symmetric N-source S.  a  Let X1, . . . , Xn be random variables having mean value µ = E Xj and variance  1  σ2 = E X2  j −  E Xj 2 for 1 ≤ j ≤ n. Prove Chebyshev’s inequality  Pr  X1 + ··· + Xn − nµ 2 ≥ tnσ  2  ≤ 1 t,  under the additional assumption that E XiXj  =  E Xi  E Xj  whenever i ̸= j. b  Let B be a random k × R binary matrix. Prove that if c and c′ are fixed nonzero k-bit vectors, with c ̸= c′, the vectors cB and c′B are independent random R-bit vectors  modulo 2 .  c  Apply  a  and  b  to the analysis of Algorithm L.  43. [20] It seems just as difficult to find the factors of any fixed R-bit Blum integer M as to find the factors of a random R-bit integer. Why then is Theorem P stated for random M instead of fixed M?   cid:120  44. [16]  I. J. Good.  Can a valid table of random digits contain just one misprint?   184  RANDOM NUMBERS  3.6  3.6. SUMMARY We have covered a fairly large number of topics in this chapter: How to generate random numbers, how to test them, how to modify them in applications, and how to derive theoretical facts about them. Perhaps the main question in many readers’ minds will be, “What is the result of all this theory? What is a simple, virtuous generator that I can use in my programs in order to have a reliable source of random numbers?”  X ←  aX + c  mod m  The detailed investigations in this chapter suggest that the following proce- dure gives the simplest random number generator for the machine language of most computers: At the beginning of the program, set an integer variable X to some value X0. This variable X is to be used only for the purpose of random number generation. Whenever a new random number is required by the program, set   1  and use the new value of X as the random value. It is necessary to choose X0, a, c, and m properly, and to use the random numbers wisely, according to the following principles: i  The “seed” number X0 may be chosen arbitrarily.  If the program is run several times and a different source of random numbers is desired each time, set X0 to the last value attained by X on the preceding run; or  if more convenient  set X0 to the current date and time. If the program may need to be rerun later with the same random numbers  for example, when debugging , be sure to print out X0 if it isn’t otherwise known.  ii  The number m should be large, say at least 230. It may conveniently be taken as the computer’s word size, since this makes the computation of  aX + c  mod m quite efficient. Section 3.2.1.1 discusses the choice of m in more detail. The computation of  aX + c  mod m must be done exactly, with no roundoff error.  iii  If m is a power of 2  that is, if a binary computer is being used , pick a so that a mod 8 = 5. If m is a power of 10  that is, if a decimal computer is being used , choose a so that a mod 200 = 21. This choice of a together with the choice of c given below ensures that the random number generator will produce all m different possible values of X before it starts to repeat  see Section 3.2.1.2  and ensures high “potency”  see Section 3.2.1.3 .  iv  The multiplier a should preferably be chosen between .01m and .99m, and its binary or decimal digits should not have a simple, regular pattern. By  choosing some haphazard constant like a = 3141592621 which satisfies both of the conditions in  iii , one almost always obtains a reasonably good  multiplier. Further testing should of course be done if the random number generator is to be used extensively; for example, there should be no large quotients when Euclid’s algorithm is used to find the gcd of a and m  see Section 3.3.3 . The multiplier should pass the spectral test  Section 3.3.4  and several tests of Section 3.3.2, before it is considered to have a truly clean bill of health.   3.6  SUMMARY  185  v  The value of c is immaterial when a is a good multiplier, except that c must have no factor in common with m when m is the computer’s word size. Thus we may choose c = 1 or c = a.  People who use c = 0 together with m = 2e are sacrificing two bits of accuracy and half of the seed values just to save a few nanoseconds of running time; see exercise 3.2.1.2–9.   vi  The least significant  right-hand  digits of X are not very random, so de- cisions based on the number X should always be influenced primarily by the most significant digits. It is generally best to think of X as a random fraction X m between 0 and 1, that is, to visualize X with a radix point at its left, rather than to regard X as a random integer between 0 and m − 1. To compute a random integer between 0 and k−1, one should multiply by k and truncate the result.  Don’t divide by k; see exercise 3.4.1–3.   vii  An important limitation on the randomness of sequence  1  is discussed in Section 3.3.4, where it is shown that the “accuracy” in t dimensions will be only about one part in t√ m. Monte Carlo applications requiring higher resolution can improve the randomness by employing techniques discussed in Section 3.2.2.  viii  At most about m 1000 numbers should be generated; otherwise the future will behave more and more like the past. If m = 232, this means that a new scheme  for example, a new multiplier a  should be adopted after every few million random numbers are consumed. The comments above apply primarily to machine-language coding. Some of the ideas work fine also in higher-level languages for programming; for example,  1  becomes just ‘X=a*X+c’ in the C language, if X is of type unsigned long and if m is the modulus of unsigned long arithmetic  usually 232 or 264 . But C gives us no good way to regard X as a fraction, as required in  vi  above, unless we convert to double-precision floating point numbers.  Another variant of  1  is therefore often used in languages like C: We choose m to be a prime number near the largest easily computed integer, and we let a be a primitive root of m; the appropriate increment c for this case is zero. Then  1  can be implemented entirely with simple arithmetic on numbers that remain between −m and +m, using the technique of exercise 3.2.1.1–9. For example, when a = 48271 and m = 231 − 1  see line 20 of Table 3.3.4–1 , we can compute X ← aX mod m with the C code  define MM 2147483647  * a Mersenne prime *  define AA 48271  * this does well in the spectral test *  define QQ 44488  * MM   AA *  define RR 3399 X=AA* X%QQ -RR* X QQ ; if  X<0  X+=MM;   * MM % AA; it is important that RR<QQ *   here X is type long, and X should be initialized to a nonzero seed value less than MM. Since MM is prime, the least-significant bits of X are just as random as the most-significant bits, so the precautions of  vi  no longer need to be taken.   186  RANDOM NUMBERS  3.6  If you need millions and millions of random numbers, you can combine that  routine with another, as in Eq. 3.3.4– 38 , by writing some additional code:  * a non-Mersenne prime *   * another spectral success story *   * MMM   AAA *   * MMM % AAA; again less than QQQ *   define MMM 2147483399 define AAA 40692 define QQQ 52774 define RRR 3791 Y=AAA* Y%QQQ -RRR* Y QQQ ; if  Y<0  Y+=MMM; Z=X-Y; if  Z<=0  Z+=MM-1;  Like X, the variable Y needs to be initially nonzero. This code deviates slightly from 3.3.4– 38  so that the output, Z, always lies strictly between 0 and 231 − 1, as recommended by Liviu Lalescu. The period length of the Z sequence is about 74 quadrillion, and its numbers now have about twice as many bits of accuracy as the X numbers do.  This method is portable and fairly simple, but not very fast. An alternative scheme based on lagged Fibonacci sequences with subtraction  exercise 3.2.2– 23  is even more attractive, because it not only allows easy portability between computers, it is considerably faster, and it delivers random numbers of better quality because the t-dimensional accuracy is probably good for t ≤ 100. Here is a C subroutine ran array long aa[ ], int n  that generates n new random numbers and places them into a given array aa, using the recurrence   2  This recurrence is particularly well suited to modern computers. The value of n must be at least 100; larger values like 1000 are recommended.  Xj =  Xj−100 − Xj−37  mod 230.   * the long lag *  define KK 100  * the short lag *  define LL 37  * the modulus *  define MM  1L<<30   *  x-y  mod MM *  define mod_diff x,y     x - y  & MM-1   long ran_x[KK];  * the generator state *  void ran_array long aa[],int n  {  * put n new values in aa *   register int i,j; for  j=0;j<KK;j++  aa[j]=ran_x[j]; for  ;j<n;j++  aa[j]=mod_diff aa[j-KK],aa[j-LL] ; for  i=0;i<LL;i++,j++  ran_x[i]=mod_diff aa[j-KK],aa[j-LL] ; for  ;i<KK;i++,j++  ran_x[i]=mod_diff aa[j-KK],ran_x[i-LL] ;  } All information about numbers that will be generated by future calls to ran array appears in ran x, so you can make a copy of that array in the midst of a computation if you want to restart at the same point later without going all the way back to the beginning of the sequence. The tricky thing about using a recurrence like  2  is, of course, to get everything started properly in the first place, by setting up suitable values of X0, . . . , X99. The following subroutine   3.6  SUMMARY  187  ran start long seed  initializes the generator nicely when given any seed number between 0 and 230 − 3 = 1,073,741,821 inclusive:   * guaranteed separation between streams *  define TT 70 define is_odd x    x &1   * the units bit of x *  void ran_start long seed  {  * use this to set up ran_array *   register int t,j; long x[KK+KK-1]; register long ss= seed+2 & MM-2 ; for  j=0;j<KK;j++  {  x[j]=ss; ss =MM  ss-=MM-2;  } x[1]++; for  ss=seed& MM-1 ,t=TT-1; t;   {  for  j=KK-1;j>0;j--   x[j+j]=x[j], x[j+j-1]=0; for  j=KK+KK-2;j>=KK;j--    * the preparation buffer *    * bootstrap the buffer *   * cyclic shift 29 bits *    * make x[1]  and only x[1]  odd *    * "square" *    * "multiply by z" *    * shift the buffer cyclically *   x[j- KK-LL ]=mod_diff x[j- KK-LL ],x[j] , x[j-KK]=mod_diff x[j-KK],x[j] ;  if  is_odd ss   {  for  j=KK;j>0;j--  x[0]=x[KK]; x[LL]=mod_diff x[LL],x[KK] ;  x[j]=x[j-1];  } if  ss  ss>>=1; else t--;  } for  j=0;j<LL;j++  ran_x[j+KK-LL]=x[j]; for  ;j<KK;j++  ran_x[j-LL]=x[j]; for  j=0;j<10;j++  ran_array x,KK+KK-1 ;  }   * warm it up *    This program incorporates improvements to the author’s original ran start rou- tine, recommended by Richard Brent and Pedro Gimeno in November 2001.   The somewhat curious maneuverings of ran start are explained in exercise 9, which proves that the sequences of numbers generated from different starting seeds are independent of each other: Every block of 100 consecutive values Xn, Xn+1, . . . , Xn+99 in the subsequent output of ran array will be distinct from the blocks that occur with another seed.  Strictly speaking, this is known to be true only when n < 270; but there are fewer than 255 nanoseconds in a year.  Several processes can therefore start in parallel with different seeds and be sure that they are doing independent calculations; different groups of scientists working on a problem in different computer centers can be sure that they are not duplicating the work of others if they restrict themselves to disjoint sets of seeds. Thus, more than one billion essentially disjoint batches of random numbers are provided by the single routines ran array and ran start. And if that is not enough, you can replace the program parameters 100 and 37 by other values from Table 3.2.2–1.   188  RANDOM NUMBERS  3.6  These C routines use the bitwise-and operation ‘&’ for efficiency, so they are not strictly portable unless the computer uses two’s complement representation for integers. Almost all modern computers are based on two’s complement arithmetic, but ‘&’ is not really necessary for this algorithm. Exercise 10 shows how to get exactly the same sequences of numbers in FORTRAN, using no such tricks. Although the programs illustrated here are designed to generate 30-bit integers, they are easily modified to generate random 52-bit fractions between 0 and 1, on computers that have reliable floating point arithmetic; see exercise 11. You may wish to include ran array in a library of subroutines, or you may find that somebody else has already done so. One way to check whether an implementation of ran array and ran start conforms with the code above is to run the following rudimentary test program:  int main   { register int m; long a[2009];  ran_start 310952 ; for  m=0;m<2009;m++  ran_array a,1009 ; printf "%ld\n", ran_x[0] ; ran_start 310952 ; for  m=0;m<1009;m++  ran_array a,2009 ; printf "%ld\n", ran_x[0] ; return 0;  }  The printed output should be 995235265  twice .  Caution: The numbers generated by ran array fail the birthday spacings test of Section 3.3.2J, and they have other deficiencies that sometimes show up in high-resolution simulations  see exercises 3.3.2–31 and 3.3.2–35 . One way to avoid the birthday spacings problem is simply to use only half of the numbers  skipping the odd-numbered elements ; but that doesn’t cure the other problems. An even better procedure is to follow Martin Lüscher’s suggestion, discussed in Section 3.2.2: Use ran array to generate, say, 1009 numbers, but use only the first 100 of these.  See exercise 15.  This method has modest theoretical support and no known defects. Most users will not need such a precaution, but it is definitely less risky, and it allows a convenient tradeoff between randomness and speed.  A great deal is known about linear congruential sequences like  1 , but comparatively little has yet been proved about the randomness properties of lagged Fibonacci sequences like  2 . Both approaches seem to be reliable in practice, if they are used with the caveats already stated.  When this chapter was first written in the late 1960s, a truly horrible random number generator called RANDU was commonly used on most of the world’s computers  see Section 3.3.4 . The authors of many contributions to the science of random number generation have often been unaware that particular methods they were advocating would prove to be inadequate. A particularly noteworthy example was the experience of Alan M. Ferrenberg and his colleagues, reported in Physical Review Letters 69  1992 , 3382–3384: They tested their algorithms for a three-dimensional problem by considering first a related two-dimensional problem with a known answer, and discovered that supposedly super-quality   3.6  SUMMARY  189  modern random number generators gave wrong results in the fifth decimal place. By contrast, an old-fashioned run-of-the-mill linear congruential generator, X ← 16807X mod  231−1 , worked fine. Perhaps further research will show that even the random number generators recommended here are unsatisfactory; we hope this is not the case, but the history of the subject warns us to be cautious. The most prudent policy for a person to follow is to run each Monte Carlo program at least twice using quite different sources of random numbers, before taking the answers of the program seriously; this will not only give an indication of the stability of the results, it also will guard against the danger of trusting in a generator with hidden deficiencies.  Every random number generator will fail in at least one application.   Excellent bibliographies of the pre-1972 literature on random number gen- eration have been compiled by Richard E. Nance and Claude Overstreet, Jr., Computing Reviews 13  1972 , 495–508, and by E. R. Sowey, International Stat. Review 40  1972 , 355–371. The period 1972–1984 is covered by Sowey in International Stat. Review 46  1978 , 89–102; J. Royal Stat. Soc. A149  1986 , 83–107. Subsequent developments are discussed by Shu Tezuka, Uniform Random Numbers  Boston: Kluwer, 1995 .  For a detailed study of the use of random numbers in numerical analysis, see J. M. Hammersley and D. C. Handscomb, Monte Carlo Methods  London: Methuen, 1964 . This book shows that some numerical methods are enhanced by using numbers that are “quasirandom,” designed specifically for a certain purpose  not necessarily satisfying the statistical tests we have discussed . The origins of Monte Carlo methods for computers are discussed by N. Metropolis and R. Eckhardt in Stanislaw Ulam 1909–1984, a special issue of Los Alamos Science 15  1987 , 125–137.  Every reader is urged to work exercise 6 in the following set of problems.  Almost all good computer programs contain at least one random-number generator. — DONALD E. KNUTH, Seminumerical Algorithms  1969   EXERCISES 1. [21] Write a MIX subroutine with the following characteristics, using method  1 :  Calling sequence: JMP RANDI Entry conditions: Exit conditions:   cid:120  2. [15] Some people have been afraid that computers will someday take over the  rA = k, a positive integer < 5000. rA ← a random integer Y, 1 ≤ Y ≤ k, with each integer about equally probable; rX =?; overflow off.  world; but they are reassured by the statement that a machine cannot do anything really new, since it is only obeying the commands of its master, the programmer. Lady Lovelace wrote in 1844, “The Analytical Engine has no pretensions to originate anything. It can do whatever we know how to order it to perform.” Her statement has been elaborated further by many philosophers. Discuss this topic, with random number generators in mind.   190  RANDOM NUMBERS  3.6  3. [32]  A dice game.  Write a program that simulates a roll of two dice, each of which takes on the values 1, 2, . . . , 6 with equal probability. If the total is 7 or 11 on the first roll, the game is won; a total of 2, 3, or 12 loses; and on any other total, call that total the “point” and continue rolling dice until either a 7 occurs  a loss  or the point occurs again  a win .  Play ten games. The result of each roll of the dice should be printed in the form m n, where m and n are the contents of the two dice, followed by some appropriate comment  like “snake eyes” or “little Joe” or “the hard way”, etc. . 4. [40]  Solitaire or patience.  Some people spend a lot of valuable time playing card games of solitaire, and perhaps automation will make an important inroad in this area. Write a program that  i  shuffles a simulated deck of cards;  ii  plays some common game of solitaire based on the order of the cards in the deck; and  iii  displays the result of the game, showing how close the program came to winning. Cumulative statistics should be kept. The program might be set up to “cheat” upon request. 5. [46]  Creative writing by computer.  A television program entitled “The Thinking Machine,” broadcast by the CBS television network on October 26, 1960, featured  among other things  two Western-style playlets that were written by a computer program. Here are the two scripts as they were printed out by the computer: Saga 1.  The gun is in the right hand; the money is in the left hand; the glass is on the table; the bottle is on the table; the holster is on the robber; the sheriff’s gun is in the sheriff’s right hand; the sheriff’s holster is on the sheriff.  ROBBER:  The robber is at the window.  Go to door; open door; go thru door; close door; go to corner; put money down at corner; go to table; put gun on table; sit and fidget; sit at table; pick up glass with right hand  empty ; put glass on table; pick up bottle with right hand; pour; put bottle on table; pick up glass with right hand; take a drink from glass; put glass on table; pick up bottle with right hand; sit at table; sit at table; go to corner; go to window; go to table; put bottle on table; sit and fidget; sit at table; sit and fidget; go to window; go to table; pick up glass with right hand.  SHERIFF:  The sheriff is at the window.  See robber;  robber sees sheriff ; go to  door.  ROBBER: Put glass on table; pick up gun with right hand; check gun. SHERIFF: Wait; open door; see robber;  robber sees sheriff ; go thru door. ROBBER: Go to window; aim; fire; SHERIFF NICKED. SHERIFF: Go to window; aim; fire; MISSED; go to door; go to window. ROBBER: Go to door; aim; aim. SHERIFF: Aim; fire; MISSED. ROBBER: Fire; SHERIFF NICKED. SHERIFF: Go to door; aim; fire; MISSED; go thru door; aim. ROBBER: Aim; fire; MISSED; aim; fire; MISSED. SHERIFF: Fire; MISSED; go to window; aim; fire; MISSED. ROBBER: Aim; fire; MISSED; aim; fire; MISSED; aim; fire; SHERIFF NICKED. SHERIFF: Aim; fire; ROBBER HIT. ROBBER: Drop gun; robber dies. SHERIFF: Put gun in holster; go to table; pick up glass with right hand  empty ; take glass from right hand with left hand; pick up bottle with right hand; pour; put bottle on table; take glass from left hand with right hand; take a drink from glass; take glass from right hand with left hand; pick up bottle with right hand; pour; put bottle on table; take glass from left hand with right hand; take a drink from glass; put glass on table; go to corner; pick up money with right hand; go to door; go thru door; close door. CURTAIN.   3.6  SUMMARY  191  Saga 2.  The gun is in the right hand; the money is in the left hand; the glass is on the table; the bottle is on the table; the holster is on the robber; the sheriff’s gun is in the sheriff’s right hand; the sheriff’s holster is on the sheriff.  ROBBER:  The robber is at the window.  Go to door; open door; go thru door; close door; go to corner; put money down at corner; go to window; put gun down at window; lean on window and look; lean on window and look; go to corner; count money; go to table; pick up glass with right hand  empty ; take glass from right hand with left hand; pick up bottle with right hand; pour; put bottle on table; take glass from left hand with right hand; take a drink from glass; put glass on table; pick up bottle with right hand; pour; go to corner; put bottle down at corner; go to window; pick up gun with right hand; check gun; put gun in holster; go to table; pick up glass with right hand; take a drink from glass; go to window; put glass down at window.  SHERIFF:  The sheriff is at the window.  See robber;  robber sees sheriff ; go to  ROBBER: Take gun from holster with right hand; check gun; go to door; check gun;  door.  put gun down at door.  SHERIFF: Open door; see robber;  robber sees sheriff ; go thru door; go to window. ROBBER: Pick up gun with right hand. SHERIFF: Go to table. ROBBER: Aim; fire; MISSED; aim; fire; SHERIFF HIT; blow out barrel; put gun in  holster.  SHERIFF: Drop gun; sheriff dies. ROBBER: Go to corner; pick up money with right hand; go to door; go thru door;  close door. CURTAIN.  A careful reading of these scripts reveals the highly intense drama present here. The computer program was careful to keep track of the locations of each player, the contents of his hands, etc. Actions taken by the players were random, governed by certain probabilities; the probability of a foolish action was increased depending on how much that player had had to drink and on how often he had been nicked by a shot. The reader will be able to deduce further properties of the program by studying the sample scripts.   192  RANDOM NUMBERS  3.6  Of course, even the best scripts are rewritten before they are produced, and this is especially true when an inexperienced writer has prepared the original draft. Here are the scripts just as they were actually used in the show: Saga 1. Music up. MS Robber peering thru window of shack. CU Robber’s face. MS Robber entering shack. CU Robber sees whiskey bottle on table. CU Sheriff outside shack. MS Robber sees sheriff. LS Sheriff in doorway over shoulder of robber, both draw. MS Sheriff drawing gun. LS Shooting it out. Robber gets shot. MS Sheriff picking up money bags. MS Robber staggering. MS Robber dying. Falls across table, after trying to take last shot at sheriff. MS Sheriff walking thru doorway with money. MS of robber’s body, now still, lying across table top. Camera dollies back.  Laughter  Saga 2. Music up. CU of window. Robber appears. MS Robber entering shack with two sacks of money. MS Robber puts money bags on barrel. CU Robber — sees whiskey on table. MS Robber pouring himself a drink at table. Goes to count money. Laughs. MS Sheriff outside shack. MS thru window. MS Robber sees sheriff thru window. LS Sheriff entering shack. Draw. Shoot it out. CU Sheriff. Writhing from shot. M 2 shot Sheriff staggering to table for a drink . . . falls dead. MS Robber leaves shack with money bags.* [Note: CU = “close up”, MS = “medium shot”, etc. The details above were kindly furnished to the author by Thomas H. Wolf, producer of the television show, who sug- gested the idea of a computer-written playlet in the first place, and also by Douglas T. Ross and Harrison R. Morse who produced the computer program.]  In the summer of 1952, Christopher Strachey had used the hardware random  number generator of the Ferranti Mark I to compose the following letter:  Honey Dear  My sympathetic affection beautifully attracts your affectionate enthusi- asm. You are my loving adoration: my breathless adoration. My fellow feeling breathlessly hopes for your dear eagerness. My lovesick adoration cherishes your avid ardour.  Yours wistfully,  M. U. C.  [Encounter 3  1954 , 4, 25–31; another example appears in the article on Electronic Computers in the 64th edition of Pears Cyclopedia  London, 1955 , 190–191.]  * c⃝ 1962 by Columbia Broadcasting System, Inc. All Rights Reserved. Used by permission. For further information, see J. E. Pfeiffer, The Thinking Machine  New York: J. B. Lippin- cott, 1962 .   3.6  SUMMARY  193  do creative writing; and that is the point of this exercise.  The reader will undoubtedly have many ideas about how to teach a computer to   cid:120  6. [40] Look at the subroutine library of each computer installation in your organi-  cid:120  7. [M40] A programmer decided to encipher his files by using a linear congruential  zation, and replace the random number generators by good ones. Try to avoid being too shocked at what you find.  sequence ⟨Xn⟩ of period 232 generated by  1  with m = 232. He took the most significant bits ⌊Xn 216⌋ and exclusive-or’ed them onto his data, but kept the parameters a, c, and X0 secret. Show that this isn’t a very secure scheme, by devising a method that deduces the multiplier a and the first difference X1 − X0 in a reasonable amount of time, given only the values of ⌊Xn 216⌋ for 0 ≤ n < 150. 8. [M15] Suggest a good way to test whether an implementation of linear congruen- tial generators is working properly. 9. [HM32] Let X0, X1, . . . be the numbers produced by ran array after ran start has initialized the generation process with seed s, and consider the polynomials  99 + Xn+61z  98 + ··· + Xnz  36 + ··· + Xn+64z + Xn+63. Pn z  = Xn+62z a  Prove that Pn z  ≡ zh s −n  modulo 2 and z100 + z37 +1 , for some exponent h s . b  Express h s  in terms of the binary representation of s. c  Prove that if X′0, X′1, . . . is the sequence of numbers produced by the same routines from the seed s′ ̸= s, we have Xn+k ≡ X′n′+k  modulo 2  for 0 ≤ k < 100 only if n − n′ ≥ 270 − 1.  37 + Xn+99z  10. [22] Convert the C code for ran array and ran start to FORTRAN 77 subroutines that generate exactly the same sequences of numbers.  properly rounded in the sense of Section 4.2.2  hence exact when the values are suitably restricted , convert the C routines ran array and ran start to similar programs that deliver double-precision random fractions in the range [0 . . 1 , instead of 30-bit integers.   cid:120  11. [M25] Assuming that floating point arithmetic on numbers of type double is  cid:120  12. [M21] What random number generator would be suitable for a minicomputer that  cid:120  14. [M35]  The future versus the past.  Let Xn =  Xn−37 + Xn−100  mod 2 and  does arithmetic only on integers in the range [−32768 . . 32767]? 13. [M25] Compare the subtract-with-borrow generators of exercise 3.2.1.1–12 to the lagged Fibonacci generators implemented in the programs of this section.  consider the sequence ⟨Y0, Y1, . . .⟩ = ⟨X0, X1, . . . , X99, X200, X201, . . . , X299, X400, X401, . . . , X499, X600, . . .⟩.  This sequence corresponds to calling ran array a, 200  repeatedly and looking only at the least significant bits, after discarding half of the elements.  The following experiment was repeated one million times using the sequence ⟨Yn⟩: “Generate 100 random bits; then if 60 or more of them were 0, generate one more bit and print it.” The result was to print 14527 0s and 13955 1s; but the probability that 28482 random bits contain at most 13955 1s is only about .000358.   cid:120  15. [25] Write C code that makes it convenient to generate the random integers  Give a mathematical explanation why so many 0s were output.  obtained from ran array by discarding all but the first 100 of every 1009 elements, as recommended in the text.   CHAPTER FOUR  ARITHMETIC  Seeing there is nothing  right well beloued Students in the Mathematickes  that is so troublesome to Mathematicall practise, nor that doth more molest and hinder Calculators, then the Multiplications, Diuisions, square and cubical Extractions of great numbers, which besides the tedious expence of time, are for the most part subiect to many slippery errors. I began therefore to consider in my minde, by what certaine and ready Art I might remoue those hindrances. — JOHN NEPAIR [NAPIER]  1616   I do hate sums. There is no greater mistake than to call arithmetic an exact science. There are . . . hidden laws of Number which it requires a mind like mine to perceive. For instance, if you add a sum from the bottom up, and then again from the top down, the result is always different. — M. P. LA TOUCHE  1878   I cannot conceive that anybody will require multiplications at the rate of 40,000, or even 4,000 per hour; such a revolutionary change as the octonary scale should not be imposed upon mankind in general for the sake of a few individuals. — F. H. WALES  1936   Most numerical analysts have no interest in arithmetic. — B. PARLETT  1979   The chief purpose of this chapter is to make a careful study of the four basic processes of arithmetic: addition, subtraction, multiplication, and divi- sion. Many people regard arithmetic as a trivial thing that children learn and computers do, but we will see that arithmetic is a fascinating topic with many interesting facets. It is important to make a thorough study of efficient meth- ods for calculating with numbers, since arithmetic underlies so many computer applications.  Arithmetic is, in fact, a lively subject that has played an important part in the history of the world, and it still is undergoing rapid development. In this chapter, we shall analyze algorithms for doing arithmetic operations on many types of quantities, such as “floating point” numbers, extremely large numbers, fractions  rational numbers , polynomials, and power series; and we will also discuss related topics such as radix conversion, factoring of numbers, and the evaluation of polynomials.  194   4.1  POSITIONAL NUMBER SYSTEMS  195  4.1. POSITIONAL NUMBER SYSTEMS The way we do arithmetic is intimately related to the way we represent the numbers we deal with, so it is appropriate to begin our study of the subject with a discussion of the principal means for representing numbers.  Positional notation using base b  or radix b  is defined by the rule   . . . a3a2a1a0.a−1a−2 . . .  b  = ··· + a3b3 + a2b2 + a1b1 + a0 + a−1b−1 + a−2b−2 + ··· ;   1  for example,  520.3 6 = 5 · 62 + 2 · 61 + 0 + 3 · 6−1 = 192 1 2. Our conventional decimal number system is, of course, the special case when b is ten, and when the a’s are chosen from the “decimal digits” 0, 1, 2, 3, 4, 5, 6, 7, 8, 9; in this case the subscript b in  1  may be omitted.  The simplest generalizations of the decimal number system are obtained when we take b to be an integer greater than 1 and when we require the a’s to be integers in the range 0 ≤ ak < b. This gives us the standard binary  b = 2 , ternary  b = 3 , quaternary  b = 4 , quinary  b = 5 , . . . number systems. In general, we could take b to be any nonzero number, and we could choose the a’s from any specified set of numbers; this leads to some interesting situations, as we shall see.  The dot that appears between a0 and a−1 in  1  is called the radix point.  When b = 10, it is also called the decimal point, and when b = 2, it is sometimes called the binary point, etc.  Continental Europeans often use a comma instead of a dot to denote the radix point; the English formerly used a raised dot.  The a’s in  1  are called the digits of the representation. A digit ak for large k is often said to be “more significant” than the digits ak for small k; accordingly, the leftmost or “leading” digit is referred to as the most significant digit and the rightmost or “trailing” digit is referred to as the least significant digit. In the standard binary system the binary digits are often called bits; in the standard hexadecimal system  radix sixteen  the hexadecimal digits zero through fifteen are usually denoted by  either 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d, e, f or 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F.  The historical development of number representations is a fascinating story, since it parallels the development of civilization itself. We would be going far afield if we were to examine this history in minute detail, but it will be instructive to look at its main features here.  The earliest forms of number representations, still found in primitive cul- tures, are generally based on groups of fingers, piles of stones, etc., usually with special conventions about replacing a larger pile or group of, say, five or ten objects by one object of a special kind or in a special place. Such systems lead naturally to the earliest ways of representing numbers in written form, as in the systems of Babylonian, Egyptian, Greek, Chinese, and Roman numerals; but such notations are comparatively inconvenient for performing arithmetic operations except in the simplest cases.   196  ARITHMETIC  4.1  2 is 1  During the twentieth century, historians of mathematics have made exten- sive studies of early cuneiform tablets found by archæologists in the Middle East. These studies show that the Babylonian people actually had two distinct systems of number representation: The numbers used in everyday business transactions were written in a notation based on grouping by tens, hundreds, etc.; this notation was inherited from earlier Mesopotamian civilizations, and large numbers were seldom required. When more difficult mathematical problems were considered, however, Babylonian mathematicians made extensive use of a sexagesimal  radix sixty  positional notation that was highly developed at least as early as 1750 B.C. This notation was unique in that it was actually a floating point form of representation with exponents omitted; the proper scale factor or power of sixty was to be supplied by the context, so that, for example, the numbers 2, 120, 7200, and 1 30 were all written in an identical manner. The notation was especially convenient for multiplication and division, using auxiliary tables, since radix-point alignment had no effect on the answer. As examples of this Babylonian notation, consider the following excerpts from early tables: The square of 30 is 15  which may also be read, “The square of 1 4” ; the reciprocal of 81 =  1 21 60 is  44 26 40 60; and the square of the latter is  32 55 18 31 6 40 60. The Babylonians had a symbol for zero, but because of their “floating point” philosophy, it was used only within numbers, not at the right end to denote a scale factor. For the interesting story of early Babylonian mathematics, see O. Neugebauer, The Exact Sciences in Antiquity  Princeton, N. J.: Princeton University Press, 1952 , and B. L. van der Waerden, Science Awakening, translated by A. Dresden  Groningen: P. Noordhoff, 1954 ; see also D. E. Knuth, CACM 15  1972 , 671–677; 19  1976 , 108.  Fixed point positional notation was apparently first conceived by the Maya Indians in central America some 2000 years ago; their radix-20 system was highly developed, especially in connection with astronomical records and calendar dates. They began to use a written sign for zero about A.D. 200. But the Spanish con- querors destroyed nearly all of the Maya books on history and science, so we have comparatively little knowledge about the degree of sophistication that native Americans had reached in arithmetic. Special-purpose multiplication tables have been found, but no examples of division are known. [See J. Eric S. Thompson, Contrib. to Amer. Anthropology and History 7  Carnegie Inst. of Washington, 1941 , 37–67; J. Justeson, “Pratiche di calcolo nell’antica mesoamerica,” Storia della Scienza 2  Rome: Istituto della Enciclopedia Italiana, 2001 , 976–990.]  Several centuries before Christ, the Greek people employed an early form of the abacus to do their arithmetical calculations, using sand and or pebbles on a board that had rows or columns corresponding in a natural way to our decimal system. It is perhaps surprising to us that the same positional notation was never adapted to written forms of numbers, since we are so accustomed to decimal reckoning with pencil and paper; but the greater ease of calculating by abacus  since handwriting was not a common skill, and since abacus users need not memorize addition and multiplication tables  probably made the Greeks feel it would be silly even to suggest that computing could be done better on “scratch   4.1  POSITIONAL NUMBER SYSTEMS  197  paper.” At the same time Greek astronomers did make use of a sexagesimal positional notation for fractions, which they had learned from the Babylonians. Our decimal notation, which differs from the more ancient forms primarily because of its fixed radix point, together with its symbol for zero to mark an empty position, was developed first in India within the Hindu culture. The exact date when this notation first appeared is quite uncertain; about A.D. 600 seems to be a good guess. Hindu science was highly developed at that time, particularly in astronomy. The earliest known Hindu manuscripts that show decimal notation have numbers written backwards  with the most significant digit at the right , but soon it became standard to put the most significant digit at the left.  The Hindu principles of decimal arithmetic were brought to Persia about A.D. 750, as several important works were translated into Arabic; a picturesque account of this development is given in a Hebrew document by Abraham Ibn Ezra, which has been translated into English in AMM 25  1918 , 99–108. Not long after this, al-Khw¯arizm¯ı wrote his Arabic textbook on the subject.  As noted in Chapter 1, our word “algorithm” comes from al-Khw¯arizm¯ı’s name.  His work was translated into Latin and was a strong influence on Leonardo Pisano  Fibonacci , whose book on arithmetic  A.D. 1202  played a major role in the spreading of Hindu-Arabic numerals into Europe. It is interesting to note that the left-to-right order of writing numbers was unchanged during these two transitions, although Arabic is written from right to left while Hindu and Latin scholars generally wrote from left to right. A detailed account of the subsequent propagation of decimal numeration and arithmetic into all parts of Europe during the period 1200–1600 has been given by David Eugene Smith in his History of Mathematics 1  Boston: Ginn and Co., 1923 , Chapters 6 and 8.  Decimal notation was applied at first only to integer numbers, not to frac- tions. Arabic astronomers, who required fractions in their star charts and other tables, continued to use the notation of Ptolemy  the famous Greek astronomer , a notation based on sexagesimal fractions. This system still survives today in our trigonometric units of degrees, minutes, and seconds, and also in our units of time, as a remnant of the original Babylonian sexagesimal notation. Early European mathematicians also used sexagesimal fractions when dealing with noninteger numbers; for example, Fibonacci gave the value  1◦ 22′ 7′′ 42′′′ 33IV 4V 40VI  as an approximation to the root of the equation x3 + 2x2 + 10x = 20.  The correct answer is 1◦ 22′ 7′′ 42′′′ 33IV 4V 38VI 30VII 50VIII 15IX 43X . . . .   The use of decimal notation also for tenths, hundredths, etc., in a similar way seems to be a comparatively minor change; but, of course, it is hard to break with tradition, and sexagesimal fractions have an advantage over decimal fractions because numbers such as 1 3 can be expressed exactly, in a simple way. Chinese mathematicians — who never used sexagesimals — were apparently the first people to work with the equivalent of decimal fractions, although their numeral system  lacking zero  was not originally a positional number system in the strict sense. Chinese units of weights and measures were decimal, so that   198  ARITHMETIC  4.1  Tsu Ch’ung-Chih  who died in A.D. 500 or 501  was able to express an approxi- mation to π in the following form:  3 chang, 1 ch’in, 4 ts’un, 1 fen, 5 li, 9 hao, 2 miao, 7 hu.  Here chang, . . . , hu are units of length; 1 hu  the diameter of a silk thread  equals 1 10 miao, etc. The use of such decimal-like fractions was fairly widespread in China after about 1250.  An embryonic form of truly positional decimal fractions appeared in a 10th- century arithmetic text, written in Damascus by an obscure mathematician named al-Uql¯ıdis¯ı  “the Euclidean” . He occasionally marked the place of a decimal point, for example in connection with a problem about compound in- terest, the computation of 135 times  1.1 n for 1 ≤ n ≤ 5. [See A. S. Saidan, The Arithmetic of al-Uql¯ıdis¯ı  Dordrecht: D. Reidel, 1975 , 110, 114, 343, 355, 481–485.] But he did not develop the idea very fully, and his trick was soon forgotten. Al-Samaw’al of Baghdad and Baku, writing in 1172, understood that √ 10 = 3.162277 . . . , but he had no convenient way to write such approximations down. Several centuries passed before decimal fractions were reinvented by a Per- sian mathematician, al-K¯ash¯ı, who died in 1429. Al-K¯ash¯ı was a highly skillful calculator, who gave the value of 2π as follows, correct to 16 decimal places:  integer 0 6  fractions  2  8  3  1  8  5  3  0  7  1  7  9  5  8  6  5  This was by far the best approximation to π known until Ludolph van Ceulen laboriously calculated 35 decimal places during the period 1586–1610. Decimal fractions began to appear sporadically in Europe; for example, a so-called “Turkish method” was used to compute 153.5 × 16.25 = 2494.375. Giovanni Bianchini developed them further, with applications to surveying, prior to 1450; but like al-Uql¯ıdis¯ı, his work seems to have had little influence. Christof Rudolff and François Viète suggested the idea again in 1525 and 1579. Finally, an arithmetic text by Simon Stevin, who independently hit on the idea of decimal fractions in 1585, became popular. Stevin’s work, and the discovery of logarithms soon afterwards, made decimal fractions commonplace in Europe during the 17th century. [For further remarks and references, see D. E. Smith, History of Mathematics 2  1925 , 228–247; V. J. Katz, A History of Mathematics  1993 , 225–228, 345–348; and G. Rosińska, Quart. J. Hist. Sci. Tech. 40  1995 , 17–32.] The binary system of notation has its own interesting history. Many prim- itive tribes in existence today are known to use a binary or “pair” system of counting  making groups of two instead of five or ten , but they do not count in a true radix-2 system, since they do not treat powers of 2 in a special manner. See The Diffusion of Counting Practices by Abraham Seidenberg, Univ. of Calif. Publ. in Math. 3  1960 , 215–300, for interesting details about primitive number systems. Another “primitive” example of an essentially binary system is the conventional musical notation for expressing rhythms and durations of time.  Nondecimal number systems were discussed in Europe during the seven- teenth century. For many years astronomers had occasionally used sexagesimal   4.1  POSITIONAL NUMBER SYSTEMS  199  arithmetic both for the integer and the fractional parts of numbers, primarily when performing multiplication [see John Wallis, Treatise of Algebra  Oxford: 1685 , 18–22, 30]. The fact that any integer greater than 1 could serve as radix was apparently first stated in print by Blaise Pascal in De Numeris Multiplicibus, which was written about 1658 [see Pascal’s Œuvres Complètes  Paris: Éditions du Seuil, 1963 , 84–89]. Pascal wrote, “Denaria enim ex instituto hominum, non ex necessitate naturæ ut vulgus arbitratur, et sane satis inepte, posita est”; i.e., “The decimal system has been established, somewhat foolishly to be sure, according to man’s custom, not from a natural necessity as most people think.” He stated that the duodecimal  radix twelve  system would be a welcome change, and he gave a rule for testing a duodecimal number for divisibility by nine. Erhard Weigel tried to drum up enthusiasm for the quaternary  radix four  system in a series of publications beginning in 1673. A detailed discussion of radix-twelve arithmetic was given by Joshua Jordaine, Duodecimal Arithmetick  London: 1687 .  Although decimal notation was almost exclusively used for arithmetic during that era, other systems of weights and measures were rarely if ever based on multiples of 10, and business transactions required a good deal of skill in adding quantities such as pounds, shillings, and pence. For centuries merchants had therefore learned to compute sums and differences of quantities expressed in pe- culiar units of currency, weights, and measures; thus they were doing arithmetic in nondecimal number systems. The common units of liquid measure in England, dating from the 13th century or earlier, are particularly noteworthy:  2 gills = 1 chopin  2 chopins = 1 pint 2 pints = 1 quart 2 quarts = 1 pottle 2 pottles = 1 gallon 2 gallons = 1 peck 2 pecks = 1 demibushel  2 demibushels = 1 bushel or firkin  2 firkins = 1 kilderkin  2 kilderkins = 1 barrel  2 barrels = 1 hogshead  2 hogsheads = 1 pipe 2 pipes = 1 tun  Quantities of liquid expressed in gallons, pottles, quarts, pints, etc. were essen- tially written in binary notation. Perhaps the true inventors of binary arithmetic were British wine merchants!  The first known appearance of pure binary notation was about 1605 in some unpublished manuscripts of Thomas Harriot  1560–1621 . Harriot was a creative man who first became famous by coming to America as a representative of Sir Walter Raleigh. He invented  among other things  a notation like that now used for “less than” and “greater than” relations; but for some reason he chose not to publish many of his discoveries. Excerpts from his notes on binary arithmetic have been reproduced by John W. Shirley, Amer. J. Physics 19  1951 , 452–454; Harriot’s discovery of binary notation was first cited by Frank Morley in The Scientific Monthly 14  1922 , 60–66.  The first published treatment of the binary system appeared in the work of a prominent Cistercian bishop, Juan Caramuel de Lobkowitz, Mathesis Biceps 1   200  ARITHMETIC  4.1   Campaniæ: 1670 , 45–48. Caramuel discussed the representation of numbers in radices 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, and 60 at some length, but gave no examples of arithmetic operations in nondecimal systems except in the sexagesimal case. Ultimately, an article by G. W. Leibniz [Mémoires de l’Académie Royale des Sciences  Paris, 1703 , 110–116], which illustrated binary addition, subtraction, multiplication, and division, really brought binary notation into the limelight, and his article is usually referred to as the birth of radix-2 arithmetic. Leibniz later referred to the binary system quite frequently. He did not recommend it for practical calculations, but he stressed its importance in number-theoretical inves- tigations, since patterns in number sequences are often more apparent in binary notation than they are in decimal; he also saw a mystical significance in the fact that everything is expressible in terms of zero and one. Leibniz’s unpublished manuscripts show that he had been interested in binary notation as early as 1679, when he referred to it as a “bimal” system  analogous to “decimal” .  A careful study of Leibniz’s early work with binary numbers has been made by Hans J. Zacher, Die Hauptschriften zur Dyadik von G. W. Leibniz  Frankfurt am Main: Klostermann, 1973 . Zacher points out that Leibniz was familiar with John Napier’s so-called “local arithmetic,” a way for calculating with stones that amounts to using a radix-2 abacus. [Napier had published the idea of local arithmetic as part three of his little book Rabdologiæ in 1617; it may be called the world’s first “binary computer,” and it is surely the world’s cheapest, although Napier felt that it was more amusing than practical. See Martin Gardner’s discussion in Knotted Doughnuts and Other Mathematical Entertainments  New York: Freeman, 1986 , Chapter 8.]  It is interesting to note that the important concept of negative powers to the right of the radix point was not yet well understood at that time. Leibniz asked James Bernoulli to calculate π in the binary system, and Bernoulli “solved” the problem by taking a 35-digit approximation to π, multiplying it by 1035, and then expressing this integer in the binary system as his answer. On a smaller scale this would be like saying that π ≈ 3.14, and  314 10 =  100111010 2; hence π in binary is 100111010! [See Leibniz, Math. Schriften, edited by C. I. Gerhardt, 3  Halle: 1855 , 97; two of the 118 bits in the answer are incorrect, due to computational errors.] The motive for Bernoulli’s calculation was apparently to see whether any simple pattern could be observed in this representation of π. Charles XII of Sweden, whose talent for mathematics perhaps exceeded that of all other kings in the history of the world, hit on the idea of radix-8 arithmetic about 1717. This was probably his own invention, although he had met Leibniz briefly in 1707. Charles felt that radix 8 or 64 would be more convenient for calculation than the decimal system, and he considered introducing octal arithmetic into Sweden; but he died in battle before decreeing such a change. [See The Works of Voltaire 21  Paris: E. R. DuMont, 1901 , 49; E. Swedenborg, Gentleman’s Magazine 24  1754 , 423–424.]  Octal notation was proposed also in colonial America before 1750, by the Rev. Hugh Jones, professor at the College of William and Mary [see Gentleman’s Magazine 15  1745 , 377–379; H. R. Phalen, AMM 56  1949 , 461–465].   4.1  POSITIONAL NUMBER SYSTEMS  201  More than a century later, a prominent Swedish-American civil engineer named John W. Nystrom decided to carry Charles XII’s plans a step further, by devising a complete system of numeration, weights, and measures based on radix-16 arithmetic. He wrote, “I am not afraid, or do not hesitate, to advocate a binary system of arithmetic and metrology. I know I have nature on my side; if I do not succeed to impress upon you its utility and great importance to mankind, it will reflect that much less credit upon our generation, upon our scientific men and philosophers.” Nystrom devised special means for pronouncing hexadecimal numbers; for example,  C0160 16 was to be read “vybong, bysanton.” His entire system was called the Tonal System, and it is described in J. Franklin Inst. 46  1863 , 263–275, 337–348, 402–407. A similar system, but using radix 8, was worked out by Alfred B. Taylor [Proc. Amer. Pharmaceutical Assoc. 8  1859 , 115–216; Proc. Amer. Philosophical Soc. 24  1887 , 296–366]. Increased use of the French  metric  system of weights and measures prompted extensive debate about the merits of decimal arithmetic during that era; indeed, octal arithmetic was even being proposed in France [J. D. Collenne, Le Système Octaval  Paris: 1845 ; Aimé Mariage, Numération par Huit  Paris: Le Nonnant, 1857 ].  The binary system was well known as a curiosity ever since Leibniz’s time, and about 20 early references to it have been compiled by R. C. Archibald [AMM 25  1918 , 139–142]. It was applied chiefly to the calculation of powers, as explained in Section 4.6.3, and to the analysis of certain games and puzzles. Giuseppe Peano [Atti della R. Accademia delle Scienze di Torino 34  1898 , 47– 55] used binary notation as the basis of a “logical” character set of 256 symbols. Joseph Bowden [Special Topics in Theoretical Arithmetic  Garden City: 1936 , 49] gave his own system of nomenclature for hexadecimal numbers.  The book History of Binary and Other Nondecimal Numeration by Anton Glaser  Los Angeles: Tomash, 1981  contains an informative and nearly complete discussion of the development of binary notation, including English translations of many of the works cited above [see Historia Math. 10  1983 , 236–243].  Much of the recent history of number systems is connected with the develop- ment of calculating machines. Charles Babbage’s notebooks for 1838 show that he considered using nondecimal numbers in his Analytical Engine [see M. V. Wilkes, Historia Math. 4  1977 , 421]. Increased interest in mechanical devices for arithmetic, especially for multiplication, led several people in the 1930s to consider the binary system for this purpose. A particularly delightful account of such activity appears in the article “Binary Calculation” by E. William Phillips [Journal of the Institute of Actuaries 67  1936 , 187–221] together with a record of the discussion that followed a lecture he gave on the subject. Phillips began by saying, “The ultimate aim [of this paper] is to persuade the whole civilized world to abandon decimal numeration and to use octonal [that is, radix 8] numeration in its place.”  Modern readers of Phillips’s article will perhaps be surprised to discover that a radix-8 number system was properly referred to as “octonary” or “octonal,” according to all dictionaries of the English language at that time, just as the radix-10 number system is properly called either “denary” or “decimal”; the   202  ARITHMETIC  4.1  . 146  word “octal” did not appear in English language dictionaries until 1961, and it apparently originated as a term for the base of a certain class of vacuum tubes. The word “hexadecimal,” which has crept into our language even more recently, is a mixture of Greek and Latin stems; more proper terms would be “senidenary” or “sedecimal” or even “sexadecimal,” but the latter is perhaps too risqué for computer programmers.  The comment by Mr. Wales that is quoted at the beginning of this chapter has been taken from the discussion printed with Phillips’s paper. Another man who attended the same lecture objected to the octal system for business purposes: “5% becomes 3.  . 3 per 64, which sounds rather horrible.”  Phillips got the inspiration for his proposals from an electronic circuit that was capable of counting in binary [C. E. Wynn-Williams, Proc. Roy. Soc. London A136  1932 , 312–324]. Electromechanical and electronic circuitry for general arithmetic operations was developed during the late 1930s, notably by John V. Atanasoff and George R. Stibitz in the U.S.A., L. Couffignal and R. Valtat in France, Helmut Schreyer and Konrad Zuse in Germany. All of these inventors used the binary system, although Stibitz later developed excess-3 binary-coded- decimal notation. A fascinating account of these early developments, including reprints and translations of important contemporary documents, appears in Brian Randell’s book The Origins of Digital Computers  Berlin: Springer, 1973 . The first American high-speed computers, built in the early 1940s, used decimal arithmetic. But in 1946, an important memorandum by A. W. Burks, H. H. Goldstine, and J. von Neumann, in connection with the design of the first stored-program computers, gave detailed reasons for making a radical departure from tradition and using base-two notation [see John von Neumann, Collected Works 5, 41–65]. Since then binary computers have multiplied. After a dozen years of experience with binary machines, a discussion of the relative advantages and disadvantages of radix-2 notation was given by W. Buchholz in his paper “Fingers or Fists?” [CACM 2, 12  December 1959 , 3–11].  The MIX computer used in this book has been defined so that it can be either binary or decimal. It is interesting to note that nearly all MIX programs can be expressed without knowing whether binary or decimal notation is being used — even when we are doing calculations involving multiple-precision arith- metic. Thus we find that the choice of radix does not significantly influence computer programming.  Noteworthy exceptions to this statement, however, are the “Boolean” algorithms discussed in Section 7.1; see also Algorithm 4.5.2B.  There are several different ways to represent negative numbers in a computer, and this sometimes influences the way arithmetic is done. In order to understand these notations, let us first consider MIX as if it were a decimal computer; then each word contains 10 digits and a sign, for example   2  This is called the signed magnitude representation. Such a representation agrees with common notational conventions, so it is preferred by many programmers. A potential disadvantage is that minus zero and plus zero can both be represented,  −12345 67890.   4.1  POSITIONAL NUMBER SYSTEMS  203  while they usually should mean the same number; this possibility requires some care in practice, although it turns out to be useful at times.  Most mechanical calculators that do decimal arithmetic use another system called ten’s complement notation. If we subtract 1 from 00000 00000, we get 99999 99999 in this notation; in other words, no explicit sign is attached to the number, and calculation is done modulo 1010. The number −12345 67890 would appear as  87654 32110   3  in ten’s complement notation. It is conventional to regard any number whose leading digit is 5, 6, 7, 8, or 9 as a negative value in this notation, although with respect to addition and subtraction there is no harm in regarding  3  as the number +87654 32110 if it is convenient to do so. Notice that there is no problem of minus zero in such a system.  The major difference between signed magnitude and ten’s complement no- tations in practice is that shifting right does not divide the magnitude by ten; for example, the number −11 = . . . 99989, shifted right one, gives . . . 99998 = −2  assuming that a shift to the right inserts “9” as the leading digit when the num- ber shifted is negative . In general, x shifted right one digit in ten’s complement notation will give ⌊x 10⌋, whether x is positive or negative.  A possible disadvantage of the ten’s complement system is the fact that it is not symmetric about zero; the p-digit negative number 500 . . . 0 is not the negative of any p-digit positive number. Thus it is possible that changing x to −x will cause overflow.  See exercises 7 and 31 for a discussion of radix-complement notation with infinite precision.   Another notation that has been used since the earliest days of high-speed computers is called nines’ complement representation. In this case the number −12345 67890 would appear as  87654 32109.   4  Each digit of a negative number  −x  is equal to 9 minus the corresponding digit of x. It is not difficult to see that the nines’ complement notation for a negative number is always one less than the corresponding ten’s complement notation. Addition and subtraction are done modulo 1010 − 1, which means that a carry off the left end is to be added at the right end.  See the discussion of arithmetic modulo w − 1 in Section 3.2.1.1.  Again there is a potential problem with minus zero, since 99999 99999 and 00000 00000 denote the same value.  The ideas just explained for radix-10 arithmetic apply in a similar way to radix-2 arithmetic, where we have signed magnitude, two’s complement, and ones’ complement notations. Two’s complement arithmetic on n-bit numbers is arithmetic modulo 2n; ones’ complement arithmetic is modulo 2n − 1. The MIX computer, as used in the examples of this chapter, deals only with signed magnitude arithmetic; however, alternative procedures for complement notations are discussed in the accompanying text when it is important to do so.  Detail-oriented readers and copy editors should notice the position of the apostrophe in terms like “two’s complement” and “ones’ complement”: A two’s   204  ARITHMETIC  4.1  complement number is complemented with respect to a single power of 2, while a ones’ complement number is complemented with respect to a long sequence of 1s. Indeed, there is also a “twos’ complement notation,” which has radix 3 and complementation with respect to  2 . . . 22 3.  Descriptions of machine language often tell us that a computer’s circuitry is set up with the radix point at a particular place within each numeric word. Such statements should usually be disregarded. It is better to learn the rules concerning where the radix point will appear in the result of an instruction if we assume that it lies in a certain place beforehand. For example, in the case of MIX we could regard our operands either as integers with the radix point at the extreme right, or as fractions with the radix point at the extreme left, or as some mixture of these two extremes; the rules for the appearance of the radix point after addition, subtraction, multiplication, or division are straightforward. It is easy to see that there is a simple relation between radix b and radix bk:  5    . . . a3a2a1a0.a−1a−2 . . . b =  . . . A3A2A1A0.A−1A−2 . . . bk ,  where  Aj =  akj+k−1 . . . akj+1akj b;  see exercise 8. Thus we have simple techniques for converting at sight between, say, binary and hexadecimal notation.  Many interesting variations on positional number systems are possible in addition to the standard b-ary systems discussed so far. For example, we might have numbers in base  −10 , so that  . . . a3a2a1a0.a−1a−2 . . .  −10  = ··· + a3 −10 3 + a2 −10 2 + a1 −10 1 + a0 + ··· = ··· − 1000a3 + 100a2 − 10a1 + a0 − 1  10 a−1 + 1  100 a−2 − ··· .  Here the individual digits satisfy 0 ≤ ak ≤ 9 just as in the decimal system. The number 12345 67890 appears in the “negadecimal” system as   6  since the latter represents 10305070900 − 9070503010. It is interesting to note that the negative of this number, −12345 67890, would be written   1 93755 73910 −10,   7  and, in fact, every real number whether positive or negative can be represented without a sign in the −10 system.   28466 48290 −10,  Negative-base systems were first considered by Vittorio Grünwald [Giornale di Matematiche di Battaglini 23  1885 , 203–221, 367], who explained how to perform the four arithmetic operations in such systems; Grünwald also discussed root extraction, divisibility tests, and radix conversion. However, his work seems to have had no effect on other research, since it was published in a rather obscure journal, and it was soon forgotten. The next publication about negative- base systems was apparently by A. J. Kempner [AMM 43  1936 , 610–617],   4.1  POSITIONAL NUMBER SYSTEMS  205  who discussed the properties of noninteger radices and remarked in a footnote that negative radices would be feasible too. After twenty more years the idea was rediscovered again, this time by Z. Pawlak and A. Wakulicz [Bulletin de l’Académie Polonaise des Sciences, Classe III, 5  1957 , 233–236; Série des sciences techniques 7  1959 , 713–721], and also by L. Wadel [IRE Transactions EC-6  1957 , 123]. Experimental computers called SKRZAT 1 and BINEG, which used −2 as the radix of arithmetic, were built in Poland in the late 1950s; see N. M. Blachman, CACM 4  1961 , 257; R. W. Marczyński, Ann. Hist. Computing 2  1980 , 37–48. For further references see IEEE Transactions EC- 12  1963 , 274–277; Computer Design 6  May 1967 , 52–63. There is evidence that the idea of negative bases occurred independently to quite a few people. For example, D. E. Knuth had discussed negative-radix systems in 1955, together with a further generalization to complex-valued bases, in a short paper submitted to a “science talent search” contest for high-school seniors.  The base 2i gives rise to a system called the “quater-imaginary” number system  by analogy with “quaternary” , which has the unusual feature that every complex number can be represented with the digits 0, 1, 2, and 3 without a sign. [See D. E. Knuth, CACM 3  1960 , 245–247; 4  1961 , 355.] For example,  11210.31 2i = 1·16+1· −8i +2· −4 +1· 2i +3· − 1 4  = 7 3 Here the number  a2n . . . a1a0.a−1 . . . a−2k 2i is equal to  2 i +1 − 1  4 −7 1 2 i.   a2n . . . a2a0.a−2 . . . a−2k −4 + 2i a2n−1 . . . a3a1.a−1 . . . a−2k+1 −4,  so conversion to and from quater-imaginary notation reduces to conversion to and from negative quaternary representation of the real and imaginary parts. The interesting property of this system is that it allows multiplication and division of complex numbers to be done in a fairly unified manner without treating real and imaginary parts separately. For example, we can multiply two numbers in this system much as we do with any base, merely using a different carry rule: Whenever a digit exceeds 3 we subtract 4 and carry −1 two columns to the left; when a digit is negative, we add 4 to it and carry +1 two columns to the left. The following example shows this peculiar carry rule at work:  [9 − 10i] [9 − 10i]  1 2 2 3 1 × 1 2 2 3 1 1 2 2 3 1  1 0 3 2 0 2 1 3  1 3 0 2 2  1 3 0 2 2  1 2 2 3 1 0 2 1 3 3 3 1 2 1  [−19 − 180i]  √  A similar system that uses just the digits 0 and 1 may be based on 2 i, but this requires an infinite nonrepeating expansion for the simple number “i” √ itself. Vittorio Grünwald proposed using the digits 0 and 1  2 in odd-numbered positions, to avoid such a problem; but that actually spoils the whole system [see Commentari dell’Ateneo di Brescia  1886 , 43–54].    cid:114   206  −1+i  −1   cid:114    cid:114   −1−i  ARITHMETIC  67 67 99 99 66402641 99999999 99 9999 99 41 0001 67 99999999 40002641  27 27 99 99 00000027 99999999 99 9999 99 27 0027 27 99999999 00000027  99 99 67 67 66400001 99999999 99 99 27 27 00000027 99999999 99999999 6640264026400001 99999999 99999999 0000000000000027 99999999 99 9999 99 99 9999 99 41 000000000001 67 99 9999 99 99 9999 99 27 000000000027 27 4000000000002641 99999999 99999999 99999999 99999999 0000000000000027 99 99 99 99 99 99 66400001 01 01 99 99 99 99 00000027 27 27 99 99 99999999 66400001 99999999 00000027 99 99 9 41 0001 67 99 99 99 99 99 27 0027 27 99 9999 99 40002641 9999 99 00000027 99 99 41 01 99 99 27 27 99  9   cid:114   +i   cid:114   −i   cid:114   4.1  +1+i   cid:114   +1   cid:114   +1−i  Fig. 1. The fractal set S called the “twindragon.”  Another “binary” complex number system may be obtained by using the  base i − 1, as suggested by W. Penney [JACM 12  1965 , 247–248]:    . . . a4a3a2a1a0.a−1 . . .  i−1    = ··· − 4a4 +  2i+2 a3 − 2ia2 +  i−1 a1 + a0 − 1  2 i+1 a−1 + ··· . In this system, only the digits 0 and 1 are needed. One way to demonstrate that every complex number has such a representation is to consider the interesting set S shown in Fig. 1; this set is, by definition, all points that can be written as k≥1ak i − 1 −k, for an infinite sequence a1, a2, a3, . . . of zeros and ones. It is also known as the “twindragon fractal” [see M. F. Barnsley, Fractals Everywhere, second edition  Academic Press, 1993 , 306, 310]. Figure 1 shows that S can be decomposed into 256 pieces congruent to 1 16 S. Notice that if the diagram of S is rotated counterclockwise by 135◦, we obtain two adjacent sets congruent to   4.1  207 2  S, because  i − 1 S = S ∪  S + 1 . For details of a proof that S contains  √  1  all complex numbers that are of sufficiently small magnitude, see exercise 18.  POSITIONAL NUMBER SYSTEMS  Perhaps the prettiest number system of all is the balanced ternary notation, which consists of radix-3 representation using −1, 0, and +1 as “trits”  ternary digits  instead of 0, 1, and 2. If we let the symbol 1 stand for −1, we have the following examples of balanced ternary numbers:  Balanced ternary  Decimal  1 0 1 1 1 1 0.1 1 1 1 1 0.1 1 1 1 1 0  0.1 1 1 1 1 . . .  8 32 5 9 −32 5 9 −33  1 2  One way to find the representation of a number in the balanced ternary system is to start by representing it in ordinary ternary notation; for example,  208.3 =  21201.022002200220 . . .  3.   A very simple pencil-and-paper method for converting to ternary notation is given in exercise 4.4–12.  Now add the infinite number . . . 11111.11111 . . . in ternary notation; we obtain, in the example above, the infinite number   . . . 11111210012.210121012101 . . .  3.  Finally, subtract . . . 11111.11111 . . . by decrementing each digit; we get   8  This process may clearly be made rigorous if we replace the artificial infinite number . . . 11111.11111 . . . by a number with suitably many ones.  208.3 =  101101.101010101010 . . .  3.  The balanced ternary number system has many pleasant properties:  a  The negative of a number is obtained by interchanging 1 and 1. b  The sign of a number is given by its most significant nonzero trit, and in general we can compare any two numbers by reading them from left to right and using lexicographic order, as in the decimal system.  c  The operation of rounding to the nearest integer is identical to truncation; in other words, we simply delete everything to the right of the radix point.  Addition in the balanced ternary system is quite simple, using the table 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 10 11 1 11 1 0 1 0 1 11 1 0 1 0 1 0 1 11 1 0 1 0 1 11 1 11 10  The three inputs to the addition are the digits of the numbers to be added and the carry digit.  Subtraction is negation followed by addition. Multiplication   208  ARITHMETIC  4.1  also reduces to negation and addition, as in the following example:  1 1 0 1 × 1 1 0 1 1 1 0 1  1 1 0 1  1 1 0 1 0 1 1 1 1 0 1  [17] [17]  [289]  Representation of numbers in the balanced ternary system is implicitly present in a famous mathematical puzzle, commonly called “Bachet’s problem of weights” — although it was already stated by Fibonacci four centuries before Bachet wrote his book, and by T. abar¯ı in Persia more than 100 years before Fi- bonacci. [See W. Ahrens, Mathematische Unterhaltungen und Spiele 1  Leipzig: Teubner, 1910 , Section 3.4; H. Hermelink, Janus 65  1978 , 105–117.] Positional number systems with negative digits were invented by J. Colson [Philos. Trans. 34  1726 , 161–173], then forgotten and rediscovered about 100 years later by Sir John Leslie [The Philosophy of Arithmetic  Edinburgh: 1817 ; see pages 33–34, 54, 64–65, 117, 150], and by A. Cauchy [Comptes Rendus Acad. Sci. 11  Paris, 1840 , 789–798]. Cauchy pointed out that negative digits make it unnecessary for a person to memorize the multiplication table past 5×5. A claim that such num- ber systems were known in India long ago [J. Bharati, Vedic Mathematics  Delhi: Motilal Banarsidass, 1965 ] has been refuted by K. S. Shukla [Mathematical Education 5, 3  1989 , 129–133]. The first true appearance of “pure” balanced ternary notation was in an article by Léon Lalanne [Comptes Rendus Acad. Sci. 11  Paris, 1840 , 903–905], who was a designer of mechanical devices for arithmetic. Thomas Fowler independently invented and constructed a balanced ternary calculator at about the same time [see Report British Assoc. Adv. Sci. 10  1840 , 55; 11  1841 , 39–40]. The balanced ternary number system was men- tioned only rarely for the next 100 years, until the development of the first elec- tronic computers at the Moore School of Electrical Engineering in 1945–1946; at that time it was given serious consideration as a possible replacement for the dec- imal system. The complexity of arithmetic circuitry for balanced ternary arith- metic is not much greater than it is for the binary system, and a given number requires only ln 2  ln 3 ≈ 63% as many digit positions for its representation. Dis- cussions of the balanced ternary system appear in AMM 57  1950 , 90–93, and in High-speed Computing Devices, Engineering Research Associates  McGraw– Hill, 1950 , 287–289. The experimental Russian computer SETUN was based on balanced ternary notation [see CACM 3  1960 , 149–150], and perhaps the sym- metric properties and simple arithmetic of this number system will prove to be quite important someday — when the “flip-flop” is replaced by a “flip-flap-flop.” Positional notation generalizes in another important way to a mixed-radix system. Given a sequence of numbers ⟨bn⟩  where n may be negative , we define  . . . , a3, a2, a1, a0; a−1, a−2, . . .   . . . , b3, b2, b1, b0; b−1, b−2, . . .   4.1  POSITIONAL NUMBER SYSTEMS  209  = ··· + a3b2b1b0 + a2b1b0 + a1b0 + a0 + a−1 b−1 + a−2 b−1b−2 + ··· .   9  In the simplest mixed-radix systems, we work only with integers; we let b0, b1, b2, . . . be integers greater than one, and deal only with numbers that have no radix point, where an is required to lie in the range 0 ≤ an < bn.  One of the most important mixed-radix systems is the factorial number system, where bn = n + 2. Using this system, which was known in 13th-century India, we can represent every positive integer uniquely in the form  cn n! + cn−1  n − 1 ! + ··· + c2 2! + c1,   10  where 0 ≤ ck ≤ k for 1 ≤ k ≤ n, and cn ̸= 0.  See Algorithms 3.3.2P and 3.4.2P.  Mixed-radix systems are familiar in everyday life, when we deal with units of measure. For example, the quantity “3 weeks, 2 days, 9 hours, 22 minutes, 57 seconds, and 492 milliseconds” is equal to  3, 2, 9, 22, 57; 492   7, 24, 60, 60; 1000  seconds.  The quantity “10 pounds, 6 shillings, and thruppence ha’penny” was once equal   pence in British currency, before Great Britain changed to a  to  10,  6, 20,  3; 12;  1 2  purely decimal monetary system.  It is possible to add and subtract mixed-radix numbers by using a straight- forward generalization of the usual addition and subtraction algorithms, provided of course that the same mixed-radix system is being used for both operands  see exercise 4.3.1–9 . Similarly, we can easily multiply or divide a mixed-radix number by small integer constants, using simple extensions of the familiar pencil- and-paper methods.  Mixed-radix systems were first discussed in full generality by Georg Cantor [Zeitschrift für Math. und Physik 14  1869 , 121–128]. Exercises 26 and 29 give further information about them.  Several questions concerning irrational radices have been investigated by  W. Parry, Acta Math. Acad. Sci. Hung. 11  1960 , 401–416.  Besides the systems described in this section, several other ways to represent numbers are mentioned elsewhere in this series of books: the combinatorial num- ber system  exercise 1.2.6–56 ; the Fibonacci number system  exercises 1.2.8–34, 5.4.2–10 ; the phi number system  exercise 1.2.8–35 ; modular representations  Section 4.3.2 ; Gray code  Section 7.2.1 ; and Roman numerals  Section 9.1 .  EXERCISES 1. [15] Express −10, −9, . . . , 9, 10 in the number system whose radix is −2.   cid:120  2. [24] Consider the following four number systems:  a  binary  signed magnitude ;   b  negabinary  radix −2 ;  c  balanced ternary; and  d  radix b = 1 10. Use each of these four number systems to express each of the following three numbers:  i  −49;  ii  −3 1 3. [20] Express −49 + i in the quater-imaginary system.  7  show the repeating cycle ;  iii  π  to a few significant figures .   210  ARITHMETIC  4.1  4. [15] Assume that we have a MIX program in which location A contains a number for which the radix point lies between bytes 3 and 4, while location B contains a number whose radix point lies between bytes 2 and 3.  The leftmost byte is number 1.  Where will the radix point be, in registers A and X, after the following instructions?   a  LDA A;  MUL B   b  LDA A;  SRAX 5;  DIV B  5. [00] Explain why a negative integer in nines’ complement notation has a represen- tation in ten’s complement notation that is always one greater, if the representations are regarded as positive. 6. [16] What are the largest and smallest p-bit integers that can be represented in  a  signed magnitude binary notation  including one bit for the sign ,  b  two’s complement notation,  c  ones’ complement notation? 7. [M20] The text defines ten’s complement notation only for integers represented in a single computer word. Is there a way to define a ten’s complement notation for all real numbers, having “infinite precision,” analogous to the text’s definition? Is there a similar way to define a nines’ complement notation for all real numbers? 8. [M10] Prove Eq.  5 .   cid:120  9. [15] Change the following octal numbers to hexadecimal notation, using the hexa-  decimal digits 0, 1, . . . , 9, A, B, C, D, E, F: 12; 5655; 2550276; 76545336; 3726755. 10. [M22] Generalize Eq.  5  to mixed-radix notation as in  9 . 11. [22] Design an algorithm that uses the −2 number system to compute the sum of  an . . . a1a0 −2 and  bn . . . b1b0 −2, obtaining the answer  cn+2 . . . c1c0 −2. 12. [23] Specify algorithms that convert  a  the binary signed magnitude number ± an . . . a0 2 to its negabinary form  bn+2 . . . b0 −2; and  b  the negabinary number  cid:120  13. [M21] In the decimal system there are some numbers with two infinite decimal  bn+1 . . . b0 −2 to its signed magnitude form ± an+1 . . . a0 2. expansions; for example, 2.3599999 . . . = 2.3600000 . . . . Does the negadecimal  base −10  system have unique expansions, or are there real numbers with two different infinite expansions in this base also? 14. [14] Multiply  11321 2i by itself in the quater-imaginary system using the method 15. [M24] What are the sets S = { illustrated in the text. k≥1 akb−k  ak an allowable digit}, analogous to Fig. 1, for the negative decimal and for the quater-imaginary number systems? 16. [M24] Design an algorithm to add 1 to  an . . . a1a0 i−1 in the i−1 number system. 17. [M30] It may seem peculiar that i − 1 has been suggested as a number-system base, instead of the similar but intuitively simpler number i + 1. Can every complex number a+bi, where a and b are integers, be represented in a positional number system to base i + 1, using only the digits 0 and 1? 18. [HM32] Show that the twindragon of Fig. 1 is a closed set that contains a neighbor- hood of the origin.  Consequently, every complex number has a binary representation with radix i − 1.    cid:120  19. [23]  David W. Matula.  Let D be a set of b integers, containing exactly one  solution to the congruence x ≡ j  modulo b  for 0 ≤ j < b. Prove that all integers m  positive, negative, or zero  can be represented in the form m =  an . . . a0 b, where all the aj are in D, if and only if all integers in the range l ≤ m ≤ u can be so represented,   4.1  POSITIONAL NUMBER SYSTEMS  211  where l = − max{a  a ∈ D}  b − 1  and u = − min{a  a ∈ D}  b − 1 . For example, D = {−1, 0, . . . , b− 2} satisfies the conditions for all b ≥ 3. [Hint: Design an algorithm that constructs a suitable representation.] 20. [HM28]  David W. Matula.  Consider a decimal number system that uses the digits D = {−1, 0, 8, 17, 26, 35, 44, 53, 62, 71} instead of {0, 1, . . . , 9}. The result of exercise 19 implies  as in exercise 18  that all real numbers have an infinite decimal expansion using digits from D.  2 , 1  2 , 3 1  2 , 2 1  2 , 4 1  2 , 1 1  2 ,− 1  2 ,−1 1  2 ,−2 1  2 ,−3 1  In the usual decimal system, exercise 13 points out that some numbers have two representations.  a  Find a real number that has more than two D-decimal represen- tations.  b  Show that no real number has infinitely many D-decimal representations.  c  Show that uncountably many numbers have two or more D-decimal representations.   cid:120  21. [M22]  C. E. Shannon.  Can every real number  positive, negative, or zero  be expressed in a “balanced decimal” system, that is, in the form  k≤nak10k, for some integer n and some sequence an, an−1, an−2, . . . , where each ak is one of the ten numbers {−4 1 2}?  Although zero is not one of the allowed digits, we implicitly assume that an+1, an+2, . . . are zero.  Find all representations of zero in this number system, and find all representations of unity.  22. [HM25] Let α = − there is a “decimal” representation such that 0 < x−n has a representation   m≥1 10−m2. Given ϵ > 0 and any real number x, prove that k=0 ak10k < ϵ, where each ak is allowed to be only one of the three values 0, 1, or α.  No negative powers of 10 are used in this representation!  23. [HM30] Let D be a set of b real numbers such that every positive real number k≤n akbk with all ak ∈ D. Exercise 20 shows that there may be many numbers without unique representations; but prove that the set T of all such numbers has measure zero, if 0 ∈ D. Show that this conclusion need not be true if 0  ∈ D. 24. [M35] Find infinitely many different sets D of ten nonnegative integers satisfying the following three conditions:  i  gcd D  = 1;  ii  0 ∈ D;  iii  every positive real  number can be represented in the form  25. [M25]  S. A. Cook.  Let b, u, and v be positive integers, where b ≥ 2 and 0 < v < bm. Show that the radix-b representation of u v does not contain a run of m consecutive digits equal to b − 1, anywhere to the right of the radix point.  By convention, no runs of infinitely many  b − 1 ’s are permitted in the standard radix-b representation.    cid:120  26. [HM30]  N. S. Mendelsohn.  Let ⟨βn⟩ be a sequence of real numbers defined for  k≤n ak10k with all ak ∈ D.  all integers n, −∞ < n < ∞, such that lim n→∞  βn < βn+1;  βn = ∞;  βn = 0.  lim n→−∞  x =  Let ⟨cn⟩ be an arbitrary sequence of positive integers that is defined for all integers n, −∞ < n < ∞. Let us say that a number x has a “generalized representation” if there is an integer n and an infinite sequence of integers an, an−1, an−2, . . . such that k≤n akβk, where an ̸= 0, 0 ≤ ak ≤ ck, and ak < ck for infinitely many k. Show that every positive real number x has exactly one generalized representation  if and only if  βn+1 =  k≤n  ckβk  for all n.   212  ARITHMETIC  4.1   Consequently, the mixed-radix systems with integer bases all have this property; and mixed-radix systems with β1 =  c0 +1 β0, β2 =  c1 +1  c0 +1 β0, . . . , β−1 = β0  c−1 + 1 , . . . are the most general number systems of this type.  27. [M21] Show that every nonzero integer has a unique “reversing binary representa- tion”  where e0 < e1 < ··· < et.   cid:120  28. [M24] Show that every nonzero complex number of the form a + bi where a and b  2e0 − 2e1 + ··· +  −1 t2et ,  are integers has a unique “revolving binary representation”   1 + i e0 + i 1 + i e1 −  1 + i e2 − i 1 + i e3 + ··· + it 1 + i et ,  where e0 < e1 < ··· < et.  Compare with exercise 27.  29. [M35]  N. G. de Bruijn.  Let S0, S1, S2, . . . be sets of nonnegative integers; we will say that the collection {S0, S1, S2, . . .} has Property B if every nonnegative integer n can be written in the form  n = s0 + s1 + s2 + ··· ,  sj ∈ Sj,  in exactly one way.  Property B implies that 0 ∈ Sj for all j, since n = 0 can only be represented as 0 + 0 + 0 + ··· .  Any mixed-radix number system with radices b0, b1, b2, . . . provides an example of a collection of sets satisfying Property B, if we let Sj = {0, Bj, . . . ,  bj − 1 Bj}, where Bj = b0b1 . . . bj−1; here the representation of n = s0+s1+s2+··· corresponds in an obvious manner to its mixed-radix representation  9 . Furthermore, if the collection {S0, S1, S2, . . .} has Property B, and if A0, A1, A2, . . . is any partition of the nonnegative integers  so that we have A0 ∪ A1 ∪ A2 ∪ ··· = collection {T0, T1, T2, . . .} also has Property B, where Tj is the set of all sums {0, 1, 2, . . .} and Ai ∩ Aj = ∅ for i ̸= j; some Aj’s may be empty , then the “collapsed” si taken over all possible choices of si ∈ Si. Prove that any collection {T0, T1, T2, . . .} that satisfies Property B may be obtained by collapsing some collection {S0, S1, S2, . . .} that corresponds to a mixed-radix number system. 30. [M39]  N. G. de Bruijn.  The negabinary number system shows us that every integer  positive, negative, or zero  has a unique representation of the form  i∈Aj   −2 e1 +  −2 e2 + ··· +  −2 et ,  t ≥ 0. The purpose of this exercise is to explore generalizations of this phenomenon. a  Let b0, b1, b2, . . . be a sequence of integers such that every integer n has a unique  e1 > e2 > ··· > et ≥ 0,  representation of the form  n = be1 + be2 + ··· + bet ,  e1 > e2 > ··· > et ≥ 0,  t ≥ 0.   Such a sequence ⟨bn⟩ is called a “binary basis.”  Show that there is an index j such that bj is odd, but bk is even for all k ̸= j.  b  Prove that a binary basis ⟨bn⟩ can always be rearranged into the form d0, 2d1,  4d2, . . . = ⟨2ndn⟩, where each dk is odd.  c  If each of d0, d1, d2, . . . in  b  is ±1, prove that ⟨bn⟩ is a binary basis if and only  if there are infinitely many +1’s and infinitely many −1’s.  d  Prove that 7, −13 · 2, 7 · 22, −13 · 23, . . . , 7 · 22k, −13 · 22k+1, . . . is a binary basis,  and find the representation of n = 1.   4.1   cid:120  31. [M35] A generalization of two’s complement arithmetic, called “2-adic numbers,”  POSITIONAL NUMBER SYSTEMS  was introduced by K. Hensel in Crelle 127  1904 , 51–84.  In fact he treated p-adic numbers, for any prime p.  A 2-adic number may be regarded as a binary number  213  u =  . . . u3u2u1u0.u−1 . . . u−n 2,  whose representation extends infinitely far to the left of the binary point, but only finitely many places to the right. Addition, subtraction, and multiplication of 2-adic numbers are done according to the ordinary procedures of arithmetic, which can in principle be extended indefinitely to the left. For example,  7 =   . . . 000000000000111 2 −7 =   . . . 111111111111001 2 7 4 =   . . . 000000000000001.11 2  √−7 =   . . . 100000010110101 2  1 7 =   . . . 110110110110111 2 − 1 7 =   . . . 001001001001001 2 1 10 =   . . . 110011001100110.1 2   . . . 011111101001011 2.  or  7 and − 1  Here 7 appears as the ordinary binary integer seven, while −7 is its two’s comple- ment  extending infinitely to the left ; it is easy to verify that the ordinary procedure for addition of binary numbers will give −7+7 =   . . . 00000 2 = 0, when the procedure is continued indefinitely. The values of 1 7 are the unique 2-adic numbers that, when formally multiplied by 7, give 1 and −1, respectively. The values of 7 4 and 1 10 are examples of 2-adic numbers that are not 2-adic “integers,” since they have nonzero √−7, which are negatives of bits to the right of the binary point. The two values of each other, are the only 2-adic numbers that, when formally squared, yield the value   . . . 111111111111001 2. a  Prove that any 2-adic number u can be divided by any nonzero 2-adic number v to obtain a unique 2-adic number w satisfying u = vw.  Hence the set of 2-adic numbers forms a “field”; see Section 4.6.1.   b  Prove that the 2-adic representation of the rational number −1  2n + 1  may be obtained as follows, when n is a positive integer: First find the ordinary binary expansion of +1  2n+1 , which has the periodic form  0.ααα . . .  2 for some string α of 0s and 1s. Then −1  2n + 1  is the 2-adic number   . . . ααα 2.  c  Prove that the representation of a 2-adic number u is ultimately periodic  that is, uN+λ = uN for all large N, for some λ ≥ 1  if and only if u is rational  that is, u = m n, for some integers m and n .  √ n is a 2-adic number if and only if it satisfies n mod 22k+3 = 22k for some nonnegative integer k.  Thus, the possibilities are either n mod 8 = 1, or n mod 32 = 4, etc.   d  Prove that, when n is an integer,  32. [M40]  I. Z. Ruzsa.  Construct infinitely many integers whose ternary represen- tation uses only 0s and 1s and whose quinary representation uses only 0s, 1s, and 2s. 33. [M40]  D. A. Klarner.  Let D be any set of integers, let b be any positive integer, and let kn be the number of distinct integers that can be written as n-digit numbers  an−1 . . . a1a0 b to base b with digits ai in D. Prove that the sequence ⟨kn⟩ satisfies  a linear recurrence relation, and explain how to compute the generating function  cid:120  34. [22]  G. W. Reitwiesner, 1960.  Explain how to represent a given integer n in the n knzn. Illustrate your algorithm by showing that kn is a Fibonacci number in the case b = 3 and D = {−1, 0, 3}.  form   . . . a2a1a0 2, where each aj is −1, 0, or 1, using the fewest nonzero digits.   214  ARITHMETIC  4.2  4.2. FLOATING POINT ARITHMETIC In this section we shall study the basic principles of arithmetic operations on “floating point” numbers, by analyzing the internal mechanisms underlying such calculations. Perhaps many readers will have little interest in this subject, since their computers either have built-in floating point instructions or their operating systems include suitable subroutines. But, in fact, the material of this section should not merely be the concern of computer-design engineers or of a small clique of people who write library subroutines for new machines; every well- rounded programmer ought to have a knowledge of what goes on during the ele- mentary steps of floating point arithmetic. This subject is not at all as trivial as most people think, and it involves a surprising amount of interesting information.  4.2.1. Single-Precision Calculations A. Floating point notation. We have discussed “fixed point” notation for numbers in Section 4.1; in such a case the programmer knows where the radix point is assumed to lie in the numbers being manipulated. For many purposes, however, it is considerably more convenient to let the position of the radix point be dynamically variable or “floating” as a program is running, and to carry with each number an indication of its current radix point position. This idea has been used for many years in scientific calculations, especially for expressing very large numbers like Avogadro’s number N = 6.02214×1023, or very small numbers like Planck’s constant h = 6.6261 × 10−27 erg sec.  In this section we shall work with base b, excess q, floating point numbers with p digits: Such numbers will be represented by pairs of values  e, f , denoting  1  Here e is an integer having a specified range, and f is a signed fraction. We will adopt the convention that   e, f  = f × be−q.  f < 1;  in other words, the radix point appears at the left of the positional representation of f. More precisely, the stipulation that we have p-digit numbers means that bpf is an integer, and that   2  The term “floating binary” implies that b = 2, “floating decimal” implies b = 10, etc. Using excess-50 floating decimal numbers with 8 digits, we can write, for example,  −bp < bpf < bp.  Avogadro’s number N =  74, +.60221400 ; Planck’s constant h =  24, +.66261000 .   3   The two components e and f of a floating point number are called the exponent and the fraction parts, respectively.  Other names are occasionally used for this purpose, notably “characteristic” and “mantissa”; but it is an abuse of terminology to call the fraction part a mantissa, since that term has quite a different meaning in connection with logarithms. Furthermore the English word mantissa means “a worthless addition.”    4.2.1  SINGLE-PRECISION CALCULATIONS  The MIX computer assumes that its floating point numbers have the form  ± e  f  f  f  f  .  215   4   Here we have base b, excess q, floating point notation with four bytes of precision, 2 b⌋. where b is the byte size  e.g., b = 64 or b = 100 , and q is equal to ⌊ 1 The fraction part is ± f f f f, and e is the exponent, which lies in the range 0 ≤ e < b. This internal representation is typical of the conventions in most existing computers, although b is a much larger base than usual. B. Normalized calculations. A floating point number  e, f  is normalized if the most significant digit of the representation of f is nonzero, so that  1 b ≤ f < 1;   5  or if f = 0 and e has its smallest possible value. It is possible to tell which of two normalized floating point numbers has a greater magnitude by comparing the exponent parts first, and then testing the fraction parts only if the exponents are equal.  Most floating point routines now in use deal almost entirely with normalized numbers: Inputs to the routines are assumed to be normalized, and the outputs are always normalized. Under these conventions we lose the ability to represent a few numbers of very small magnitude — for example, the value  0, .00000001  can’t be normalized without producing a negative exponent — but we gain in speed, uniformity, and the ability to give relatively simple bounds on the relative error in our computations.  Unnormalized floating point arithmetic is discussed in Section 4.2.2.   Let us now study the normalized floating point operations in detail. At the same time we can consider the construction of subroutines for these operations, assuming that we have a computer without built-in floating point hardware.  Machine-language subroutines for floating point arithmetic are usually writ- ten in a very machine-dependent manner, using many of the wildest idiosyn- crasies of the computer at hand. Therefore floating point addition subroutines for two different machines usually bear little superficial resemblance to each other. Yet a careful study of numerous subroutines for both binary and decimal computers reveals that these programs actually have quite a lot in common, and it is possible to discuss the topics in a machine-independent way.  The first  and by far the most difficult!  algorithm we shall discuss in this  section is a procedure for floating point addition,   6  Since floating point arithmetic is inherently approximate, not exact, we will use “round” symbols   eu, fu  ⊕  ev, fv  =  ew, fw .  ⊕, ⊖, ⊗, ⊘  to stand for floating point addition, subtraction, multiplication, and division, respectively, in order to distinguish approximate operations from the true ones.   216  ARITHMETIC  4.2.1  Fig. 2. Floating point addition.  The basic idea involved in floating point addition is fairly simple: Assuming that eu ≥ ev, we take ew = eu, fw = fu + fv beu−ev  thereby aligning the radix points for a meaningful addition , and normalize the result. But several situations can arise that make this process nontrivial, and the following algorithm explains the method more precisely. Algorithm A  Floating point addition . Given base b, excess q, p-digit, nor- malized floating point numbers u =  eu, fu  and v =  ev, fv , this algorithm forms the sum w = u ⊕ v. The same procedure may be used for floating point subtraction, if −v is substituted for v. A1. [Unpack.] Separate the exponent and fraction parts of the representations  of u and v.  A2. [Assume eu ≥ ev.] If eu < ev, interchange u and v.  In many cases, it is  best to combine step A2 with step A1 or with some of the later steps.   A3. [Set ew.] Set ew ← eu. A4. [Test eu− ev.] If eu− ev ≥ p+2  large difference in exponents , set fw ← fu and go to step A7.  Actually, since we are assuming that u is normalized, we could terminate the algorithm; but it is occasionally useful to be able to normalize a possibly unnormalized number by adding zero to it.   A5. [Scale right.] Shift fv to the right eu − ev places; that is, divide it by beu−ev. [Note: This will be a shift of up to p + 1 places, and the next step  which adds fu to fv  thereby requires an accumulator capable of holding 2p + 1 base-b digits to the right of the radix point. If such a large accumulator is not available, it is possible to shorten the requirement to p + 2 or p + 3 places if proper precautions are taken; the details are given in exercise 5.]  A6. [Add.] Set fw ← fu + fv.  A1. Unpack  A2. Assume eu ≥ ev  A3. Set ew  A4. Test eu− ev  eu ≥ ev+p+2  A5. Scale right  A6. Add  A7. Normalize   4.2.1  SINGLE-PRECISION CALCULATIONS  217  Fig. 3. Normalization of  e, f .  A7. [Normalize.]  At this point  ew, fw  represents the sum of u and v, but fw may have more than p digits, and it may be greater than unity or less than 1 b.  Perform Algorithm N below, to normalize and round  ew, fw  into the final answer.  Algorithm N  Normalization . A “raw exponent” e and a “raw fraction” f are converted to normalized form, rounding if necessary to p digits. This algorithm assumes that f < b. N1. [Test f.] If f ≥ 1  “fraction overflow” , go to step N4. If f = 0, set e to  its lowest possible value and go to step N7.  N2. [Is f normalized?] If f ≥ 1 b, go to step N5. N3. [Scale left.] Shift f to the left by one digit position  that is, multiply it  by b , and decrease e by 1. Return to step N2.  N4. [Scale right.] Shift f to the right by one digit position  that is, divide it  by b , and increase e by 1.  N5. [Round.] Round f to p places.  We take this to mean that f is changed to the nearest multiple of b−p. It is possible that  bpf  mod 1 = 1 2 so that there are two nearest multiples; if b is even, we change f to the nearest multiple f′ of b−p such that bpf′ + 1 2 b is odd. Further discussion of rounding appears in Section 4.2.2.  It is important to note that this rounding operation can make f = 1  “rounding overflow” ; in such a case, return to step N4.  N6. [Check e.]  If e is too large, that is, greater than its allowed range, an exponent overflow condition is sensed. If e is too small, an exponent under- flow condition is sensed.  See the discussion below; since the result cannot be expressed as a normalized floating point number in the required range, special action is necessary.   N7. [Pack.] Put e and f together into the desired output representation.  Some simple examples of floating point addition are given in exercise 4.  f ≥ 1  N4. Scale right  Rounding overﬂow  N1. Test f  f = 0  N2. Is f normalized?  N5. Round  Yes  No  N6. Check e  N3. Scale left  Overﬂow or underﬂow  N7. Pack   218  ARITHMETIC  4.2.1   7    8   The following MIX subroutines, for addition and subtraction of numbers having the form  4 , show how Algorithms A and N can be expressed as computer programs. The subroutines below are designed to take one input u from symbolic location ACC, and the other input v comes from register A upon entrance to the subroutine. The output w appears both in register A and location ACC. Thus, a fixed point coding sequence  LDA A;  ADD B; SUB C; STA D  would correspond to the floating point coding sequence  LDA A, STA ACC;  LDA B, JMP FADD;  LDA C, JMP FSUB;  STA D.  Program A  Addition, subtraction, and normalization . The following program is a subroutine for Algorithm A, and it is also designed so that the normalization portion can be used by other subroutines that appear later in this section. In this program and in many others throughout this chapter, OFLO stands for a subroutine that prints out a message to the effect that MIX’s overflow toggle was unexpectedly found to be on. The byte size b is assumed to be a multiple of 4. The normalization routine NORM assumes that rI2 = e and rAX = f, where rA = 0 implies rX = 0 and rI2 < b. 00 BYTE EQU 1 4:4  01 EXP EQU 1:1 02 FSUB STA TEMP LDAN TEMP 03 04 FADD STJ EXITF JOV OFLO 05 STA TEMP 06 LDX ACC 07 CMPA ACC EXP  08 JGE 1F 09 STX FU 0:4  10 LD2 ACC EXP  11 STA FV 0:4  12 LD1N TEMP EXP  13 JMP 4F 14 15 1H STA FU 0:4  LD2 TEMP EXP  16 STX FV 0:4  17 LD1N ACC EXP  18 INC1 0,2 19 4H 20 5H LDA FV ENTX 0 21 SRAX 0,1 22 ADD FU 23 6H JOV N4 24 JXZ NORM 25 LD1 FV 0:1  26 JAP 1F 27  Byte size b Definition of exponent field Floating point subtraction subroutine: Change sign of operand. Floating point addition subroutine: Ensure that overflow is off. TEMP ← v. rX ← u. Steps A1, A2, A3 are combined here: Jump if ev ≥ eu. FU ← ± f f f f 0. rI2 ← ew. rI1 ← −ev. FU ← ± f f f f 0  u, v interchanged . rI2 ← ew. rI1 ← −ev. rI1 ← eu − ev.  Step A4 unnecessary.  A5. Scale right. Clear rX. Shift right eu − ev places. A6. Add. A7. Normalize. Jump if fraction overflow. Easy case? Check for opposite signs.   4.2.1  SINGLE-PRECISION CALCULATIONS  219  If not, normalize.  rX ↔ rA.  rX is positive.   The operands had opposite signs;  we must adjust the registers before rounding and normalization.   Complement the least significant portion. Jump into normalization routine.  J1N N2 JMP 2F J1P N2 SRC 5 DECX 1 STA TEMP STA HALF 0:0  LDAN TEMP ADD HALF ADD HALF SRC 5 JMP N2  CON 0 CON 0  28 29 30 1H 31 2H 32 33 34 35 36 37 38 39 40 HALF CON 1  2 41 FU 42 FV 43 NORM JAZ ZRO 44 N2 45 46 N3 47 48 49 N4 50 51 52 N5 53 54 55 56 57 58 59 5H 60 61 62 N6 63 N7 64 65 ZRO 66 8H 67 EXITF J2N * 68 EXPOV HLT 2 69 EXPUN HLT 1 70 ACC CON 0  One half the word size  Sign varies  Fraction part fu Fraction part fv N1. Test f. N2. Is f normalized? CMPA =0= 1:1  JNE N5 To N5 if leading byte nonzero. N3. Scale left. SLAX 1 DEC2 1 Decrease e by 1. JMP N2 Return to N2. N4. Scale right. ENTX 1 SRC 1 Shift right, insert “1” with proper sign. INC2 1 Increase e by 1. CMPA =BYTE 2= 5:5  N5. Round. Is tail < 1 N6 JL JG 5F Is tail > 1 JXNZ 5F tail = 1 STA TEMP LDX TEMP 4:4  JXO N6 STA *+1 0:0  INCA BYTE JOV N4 J2N EXPUN ENTX 0,2 SRC 1 DEC2 BYTE STA ACC  To N6 if rX is odd. Store sign of rA. Add b−4 to f.  Sign varies  Check for rounding overflow. N6. Check e. Underflow if e < 0. N7. Pack. rX ← e. rI2 ← e − b. Exit, unless e ≥ b. Exponent overflow detected Exponent underflow detected Floating point accumulator  2 b; round to odd.  2 b? 2 b?  The rather long section of code from lines 26 to 40 is needed because MIX has only a 5-byte accumulator for adding signed numbers while in general 2p+1 = 9 places of accuracy are required by Algorithm A. The program could be shortened to about half its present length if we were willing to sacrifice a little bit of its accuracy, but we shall see in the next section that full accuracy is important. Line 58 uses a nonstandard MIX instruction defined in Section 4.5.2. The running   220  ARITHMETIC  4.2.1  time for floating point addition and subtraction depends on several factors that are analyzed in Section 4.2.4.  Now let us consider multiplication and division, which are simpler than  addition, and somewhat similar to each other. Algorithm M  Floating point multiplication or division . Given base b, excess q, p-digit, normalized floating point numbers u =  eu, fu  and v =  ev, fv , this algorithm forms the product w = u ⊗ v or the quotient w = u ⊘ v. M1. [Unpack.] Separate the exponent and fraction parts of the representations  Sometimes it is convenient, but not necessary, to test the  of u and v. operands for zero during this step.   M2. [Operate.] Set  ew ← eu + ev − q, ew ← eu − ev + q + 1,  fw ← fu fv fw ←  b−1fu  fv  for multiplication; for division.   9    Since the input numbers are assumed to be normalized, it follows that either fw = 0, or 1 b2 ≤ fw < 1, or a division-by-zero error has occurred.  If necessary, the representation of fw may be reduced to p+2 or p+3 digits at this point, as in exercise 5.  M3. [Normalize.] Perform Algorithm N on  ew, fw  to normalize, round, and pack the result.  Note: Normalization is simpler in this case, since scaling left occurs at most once, and since rounding overflow cannot occur after division.  The following MIX subroutines,  intended to be used in connection with  q is half the byte size Floating point multiplication subroutine: Ensure that overflow is off. TEMP ← v. rX ← u. FU ← ± f f f f 0.  Program A, illustrate the machine considerations that arise in Algorithm M. Program M  Floating point multiplication and division . 01 Q EQU BYTE 2 02 FMUL STJ EXITF JOV OFLO 03 STA TEMP 04 LDX ACC 05 STX FU 0:4  06 LD1 TEMP EXP  07 LD2 ACC EXP  08 INC2 -Q,1 09 SLA 1 10 MUL FU 11 JMP NORM 12 13 FDIV STJ EXITF JOV OFLO 14 STA TEMP 15 STA FV 0:4  16 LD1 TEMP EXP  17 LD2 ACC EXP  18 DEC2 -Q,1 19  Multiply fu times fv. Normalize, round, and exit. Floating point division subroutine: Ensure that overflow is off. TEMP ← v. FV ← ± f f f f 0.  rI2 ← eu − ev + q.  rI2 ← eu + ev − q.   4.2.1  SINGLE-PRECISION CALCULATIONS  221  20 21 22 23 24 25 26 27 28 29 DVZRO HLT 3  ENTX 0 LDA ACC SLA 1 CMPA FV 1:5  JL SRA 1 INC2 1 DIV FV JNOV NORM  *+3  rA ← fu. Jump if fu < fv. Otherwise, scale fu right and increase rI2 by 1.  Divide. Normalize, round, and exit. Unnormalized or zero divisor  The most noteworthy feature of this program is the provision for division in lines 23–26, which is made in order to ensure enough accuracy to round the answer. If fu < fv, straightforward application of Algorithm M would leave a result of the form “± 0 f f f f ” in register A, and this would not allow a proper rounding without a careful analysis of the remainder  which appears in register X . So the program computes fw ← fu fv in this case, ensuring that fw is either zero or normalized in all cases; rounding can proceed with five significant bytes, possibly testing whether the remainder is zero.  We occasionally need to convert values between fixed and floating point representations. A “fix-to-float” routine is easily obtained with the help of the normalization algorithm above; for example, in MIX, the following subroutine converts an integer to floating point form: 01 FLOT STJ EXITF Assume that rA = u, an integer. 02 03 04 05 A “float-to-fix” subroutine is the subject of exercise 14.  Ensure that overflow is off. Set raw exponent.  JOV OFLO ENT2 Q+5 ENTX 0 JMP NORM  Normalize, round, and exit.   10   The debugging of floating point subroutines is usually a difficult job, since there are so many cases to consider. Here is a list of common pitfalls that often trap a programmer or machine designer who is preparing floating point routines: 1  Losing the sign. On many machines  not MIX , shift instructions between registers will affect the sign, and the shifting operations used in normalizing and scaling numbers must be carefully analyzed. The sign is also lost frequently when minus zero is present.  For example, Program A is careful to retain the sign of register A in lines 33–37. See also exercise 6.  2  Failure to treat exponent underflow or overflow properly. The size of ew should not be checked until after the rounding and normalization, because preliminary tests may give an erroneous indication. Exponent underflow and overflow can occur on floating point addition and subtraction, not only during multiplication and division; and even though this is a rather rare occurrence, it must be tested each time. Enough information should be retained so that mean- ingful corrective actions are possible after overflow or underflow has occurred.   222  ARITHMETIC  It has unfortunately become customary in many instances to ignore exponent underflow and simply to set underflowed results to zero with no indication of error. This causes a serious loss of accuracy in most cases  indeed, it is the loss of all the significant digits , and the assumptions underlying floating point arithmetic have broken down; so the programmer really must be told when underflow has occurred. Setting the result to zero is appropriate only in certain cases when the result is later to be added to a significantly larger quantity. When exponent underflow is not detected, we find mysterious situations in which  u⊗v ⊗w is zero, but u⊗ v⊗w  is not, since u⊗v results in exponent underflow but u ⊗  v ⊗ w  can be calculated without any exponents falling out of range. Similarly, we can find positive numbers a, b, c, d, and y such that  4.2.1   11    a ⊗ y ⊕ b  ⊘  c ⊗ y ⊕ d  ≈ 2 3 ,  a ⊕ b ⊘ y  ⊘  c ⊕ d ⊘ y  = 1  if exponent underflow is not detected.  See exercise 9.  Even though floating point routines are not precisely accurate, such a disparity as  11  is certainly unexpected when a, b, c, d, and y are all positive! Exponent underflow is usually not anticipated by a programmer, so it needs to be reported.* 3  Inserted garbage. When scaling to the left it is important to keep from introducing anything but zeros at the right. For example, note the ‘ENTX 0’ instruction in line 21 of Program A, and the all-too-easily-forgotten ‘ENTX 0’ instruction in line 04 of the FLOT subroutine  10 .  But it would be a mistake to clear register X after line 27 in the division subroutine.  4  Unforeseen rounding overflow. When a number like .999999997 is rounded to 8 digits, a carry will occur to the left of the decimal point, and the result must be scaled to the right. Many people have mistakenly concluded that rounding overflow is impossible during multiplication, since they look at the maximum value of fufv, which is 1 − 2b−p + b−2p; and this cannot round up to 1. The fallacy in this reasoning is exhibited in exercise 11. Curiously, it turns out that the phenomenon of rounding overflow is impossible during floating point division  see exercise 12 .  * On the other hand, we must admit that today’s high-level programming languages give the programmer little or no satisfactory way to make use of the information that a floating point routine wants to provide; and the MIX programs in this section, which simply halt when errors are detected, are even worse. There are numerous important applications in which exponent underflow is relatively harmless, and it is desirable to find a way for programmers to cope with such situations easily and safely. The practice of silently replacing underflows by zero has been thoroughly discredited, but there is another alternative that has recently been gaining much favor, namely to modify the definition that we have given for floating point numbers, allowing an unnormalized fraction part when the exponent has its smallest possible value. This idea of “gradual underflow,” which was first embodied in the hardware of the Electrologica X8 computer, adds only a small amount of complexity to the algorithms, and it makes exponent underflow impossible during addition or subtraction. The simple formulas for relative error in Section 4.2.2 no longer hold in the presence of gradual underflow, so the topic is beyond the scope of this book. However, by using formulas like round x  = x 1−δ +ϵ, where δ < b1−p 2 and ϵ < b−p−q 2, one can show that gradual underflow succeeds in many important cases. See W. M. Kahan and J. Palmer, ACM SIGNUM Newsletter  October 1979 , 13–21.   4.2.1  SINGLE-PRECISION CALCULATIONS  223  There is a school of thought that says it is harmless to “round” a value like .999999997 to .99999999 instead of to 1.0000000, since this does not increase the worst-case bounds on relative error. The floating decimal number 1.0000000 may be said to represent all real values in the interval  [1.0000000 − 5 × 10−8 . . 1.0000000 + 5 × 10−8], while .99999999 represents all values in the much smaller interval  .99999999 − 5 × 10−9 . . .99999999 + 5 × 10−9 .  Even though the latter interval does not contain the original value .999999997, each number of the second interval is contained in the first, so subsequent calculations with the second interval are no less accurate than with the first. This ingenious argument is, however, incompatible with the mathematical philosophy of floating point arithmetic expressed in Section 4.2.2. 5  Rounding before normalizing. Inaccuracies are caused by premature round- ing in the wrong digit position. This error is obvious when rounding is being done to the left of the appropriate position; but it is also dangerous in the less obvious cases where rounding is first done too far to the right, followed by rounding in the true position. For this reason it is a mistake to round during the “scaling-right” operation in step A5, except as prescribed in exercise 5.  The special case of rounding in step N5, then rounding again after rounding overflow has occurred, is harmless, however, because rounding overflow always yields ±1.0000000 and such values are unaffected by the subsequent rounding process.  6  Failure to retain enough precision in intermediate calculations. Detailed analyses of the accuracy of floating point arithmetic, made in the next section, suggest strongly that normalizing floating point routines should always deliver a properly rounded result to the maximum possible accuracy. There should be no exceptions to this dictum, even in cases that occur with extremely low probability; the appropriate number of significant digits should be retained throughout the computations, as stated in Algorithms A and M. C. Floating point hardware. Nearly every large computer intended for scientific calculations includes floating point arithmetic as part of its repertoire of built-in operations. Unfortunately, the design of such hardware usually includes some anomalies that result in dismally poor behavior in certain circumstances, and we hope that future computer designers will pay more attention to providing the proper behavior than they have in the past. It costs only a little more to build the machine right, and considerations in the following section show that substantial benefits will be gained. Yesterday’s compromises are no longer appropriate for modern machines, based on what we know now.  The MIX computer, which is being used as an example of a “typical” machine in this series of books, has an optional “floating point attachment”  available at extra cost  that includes the following seven operations:   FADD, FSUB, FMUL, FDIV, FLOT, FCMP  C = 1, 2, 3, 4, 5, 56, respectively; F = 6 . The contents of rA after the operation ‘FADD V’ are precisely the same as the   224  ARITHMETIC  contents of rA after the operations  4.2.1  STA ACC;  LDA V; JMP FADD  where FADD is the subroutine that appears earlier in this section, except that both operands are automatically normalized before entry to the subroutine if they were not already in normalized form.  If exponent underflow occurs during this pre-normalization, but not during the normalization of the answer, no underflow is signalled.  Similar remarks apply to FSUB, FMUL, and FDIV. The contents of rA after the operation ‘FLOT’ are the contents after ‘JMP FLOT’ in the subroutine  10  above.  The contents of rA are unchanged by the operation ‘FCMP V’. This instruc- tion sets the comparison indicator to LESS, EQUAL, or GREATER, depending on whether the contents of rA are “definitely less than,” “approximately equal to,” or “definitely greater than” V, as discussed in the next section. The precise action is defined by the subroutine FCMP of exercise 4.2.2–17 with EPSILON in location 0.  No register other than rA is affected by any of the floating point operations. If exponent overflow or underflow occurs, the overflow toggle is turned on and the exponent of the answer is given modulo the byte size. Division by zero leaves undefined garbage in rA. Execution times: 4u, 4u, 9u, 11u, 3u, 4u, respectively.   FIX  C = 5; F = 7 . The contents of rA are replaced by the integer “round rA ”, rounding to the nearest integer as in step N5 of Algorithm N. However, if this answer is too large to fit in the register, the overflow toggle is set on and the result is undefined. Execution time: 3u.  Sometimes it is helpful to use floating point operators in a nonstandard way. For example, if the operation FLOT had not been included as part of MIX’s floating point attachment, we could easily achieve its effect on 4-byte numbers by writing  FLOT STJ 9F SLA 1 ENTX Q+4 SRC 1 FADD =0= JMP *  9H   12   This routine is not strictly equivalent to the FLOT operator, since it assumes that the 1:1 byte of rA is zero, and it destroys rX. The handling of more general situations is a little tricky, because rounding overflow can occur even during a FLOT operation.  Similarly, suppose MIX had a FADD operation but not FIX. If we wanted to round a number u from floating point form to the nearest fixed point integer, and if we knew that the number was nonnegative and would fit in at most three bytes, we could write  FADD  FUDGE   4.2.1  SINGLE-PRECISION CALCULATIONS  225  ;  .  where location FUDGE contains the constant 0  Q+4  +  1  0  0  the result in rA would be +  Q+4  1  round u    13   D. History and bibliography. The origins of floating point notation can be traced back to Babylonian mathematicians  1800 B.C. or earlier , who made extensive use of radix-60 floating point arithmetic but did not have a notation for the exponents. The appropriate exponent was always somehow “understood” by whoever was doing the calculations. At least one case has been found in which the wrong answer was given because addition was performed with improper alignment of the operands, but such examples are very rare; see O. Neugebauer, The Exact Sciences in Antiquity  Princeton, N. J.: Princeton University Press, 1952 , 26–27. Another early contribution to floating point notation is due to the Greek mathematician Apollonius  3rd century B.C. , who apparently was the first to explain how to simplify multiplication by collecting powers of 10 separately from their coefficients, at least in simple cases. [For a discussion of Apollonius’s method, see Pappus, Mathematical Collections  4th century A.D. .] After the Babylonian civilization died out, the first significant uses of floating point notation for products and quotients did not emerge until much later, about the time logarithms were invented  1600  and shortly afterwards when Oughtred invented the slide rule  1630 . The modern notation “ xn ” for exponents was being introduced at about the same time; separate symbols for x squared, x cubed, etc., had been in use before this.  Floating point arithmetic was incorporated into the design of some of the ear- liest computers. It was independently proposed by Leonardo Torres y Quevedo in Madrid, 1914; by Konrad Zuse in Berlin, 1936; and by George Stibitz in New Jersey, 1939. Zuse’s machines used a floating binary representation that he called “semi-logarithmic notation”; he also incorporated conventions for dealing with special quantities like “∞” and “undefined.” The first American computers to operate with floating point arithmetic hardware were the Bell Laboratories’ Model V and the Harvard Mark II, both of which were relay calculators designed in 1944. [See B. Randell, The Origins of Digital Computers  Berlin: Springer, 1973 , 100, 155, 163–164, 259–260; Proc. Symp. Large-Scale Digital Calculating Machinery  Harvard, 1947 , 41–68, 69–79; Datamation 13  April 1967 , 35–44  May 1967 , 45–49; Zeit. für angew. Math. und Physik 1  1950 , 345–346.]  The use of floating binary arithmetic was seriously considered in 1944–1946 by researchers at the Moore School in their plans for the first electronic digital computers, but they found that floating point circuitry was much harder to implement with tubes than with relays. The group realized that scaling was a problem in programming; but they knew that it was only a very small part of a total programming job in those days. Indeed, explicit fixed-point scaling seemed to be well worth the time and trouble it took, since it tended to keep programmers   226  ARITHMETIC  4.2.1  aware of the numerical accuracy they were getting. Furthermore, the machine de- signers argued that floating point representation would consume valuable mem- ory space, since the exponents must be stored; and they noted that floating point hardware was not readily adapted to multiple-precision calculations. [See von Neumann’s Collected Works 5  New York: Macmillan, 1963 , 43, 73–74.] At that time, of course, they were designing the first stored-program computer and the second electronic computer, and their choice had to be either fixed point or float- ing point arithmetic, not both. They anticipated the coding of floating binary subroutines, and in fact “shift left” and “shift right” instructions were put into their design primarily to make such routines more efficient. The first machine to have both kinds of arithmetic in its hardware was apparently a computer devel- oped at General Electric Company [see Proc. 2nd Symp. Large-Scale Digital Cal- culating Machinery  Cambridge, Mass.: Harvard University Press, 1951 , 65–69]. Floating point subroutines and interpretive systems for early machines were coded by D. J. Wheeler and others, and the first publication of such routines was in The Preparation of Programs for an Electronic Digital Computer by Wilkes, Wheeler, and Gill  Reading, Mass.: Addison–Wesley, 1951 , subroutines A1–A11, pages 35–37 and 105–117. It is interesting to note that floating decimal subroutines are described here, although a binary computer was being used; in other words, the numbers were represented as 10ef, not 2ef, and therefore the scaling operations required multiplication or division by 10. On this particular machine such decimal scaling was almost as easy as shifting, and the decimal approach greatly simplified input output conversions.  Most published references to the details of floating point arithmetic rou- tines are scattered in technical memorandums distributed by various computer manufacturers, but there have been occasional appearances of these routines in the open literature. Besides the reference above, the following are of historical interest: R. H. Stark and D. B. MacMillan, Math. Comp. 5  1951 , 86–92, where a plugboard-wired program is described; D. McCracken, Digital Computer Programming  New York: Wiley, 1957 , 121–131; J. W. Carr III, CACM 2, 5  May 1959 , 10–15; W. G. Wadey, JACM 7  1960 , 129–139; D. E. Knuth, JACM 8  1961 , 119–128; O. Kesner, CACM 5  1962 , 269–271; F. P. Brooks and K. E. Iverson, Automatic Data Processing  New York: Wiley, 1963 , 184–199. For a discussion of floating point arithmetic from a computer designer’s standpoint, see “Floating point operation” by S. G. Campbell, in Planning a Computer System, edited by W. Buchholz  New York: McGraw–Hill, 1962 , 92–121; A. Padegs, IBM Systems J. 7  1968 , 22–29. Additional references, which deal primarily with the accuracy of floating point methods, are given in Section 4.2.2.  A revolutionary change in floating point hardware took place when most manufacturers began to adopt ANSI IEEE Standard 754 during the late 1980s. IEEE Micro 4  1984 , 86–100; W. J. Cody, Comp. Relevant references are: Sci. and Statistics: Symp. on the Interface 15  1983 , 133–139; W. M. Kahan, Mini Micro West-83 Conf. Record  1983 , Paper 16 1; D. Goldberg, Computing Surveys 23  1991 , 5–48, 413; W. J. Cody and J. T. Coonen, ACM Trans. Math. Software 19  1993 , 443–451.   4.2.1  SINGLE-PRECISION CALCULATIONS  227  The MMIX computer, which will replace MIX in the next edition of this book, will naturally conform to the new standard.  EXERCISES 1. [10] How would Avogadro’s number and Planck’s constant  3  be represented in base 100, excess 50, four-digit floating point notation?  This would be the representa- tion used by MIX, as in  4 , when the byte size is 100.  2. [12] Assume that the exponent e is constrained to lie in the range 0 ≤ e ≤ E; what are the largest and smallest positive values that can be written as base b, excess q, p-digit floating point numbers? What are the largest and smallest positive values that can be written as normalized floating point numbers with these specifications? 3. [11]  K. Zuse, 1936.  Show that if we are using normalized floating binary arithmetic, there is a way to increase the precision slightly without loss of memory space: A p-bit fraction part can be represented using only p − 1 bit positions of a computer word, if the range of exponent values is decreased very slightly.   cid:120  4. [16] Assume that b = 10, p = 8. What result does Algorithm A give for  cid:120  5. [24] Let us say that x ∼ y  with respect to a given radix b  if x and y are real   50, +.98765432  ⊕  49, +.33333333 ? For  53,−.99987654  ⊕  54, +.10000000 ? For  45,−.50000001  ⊕  54, +.10000000 ?  numbers satisfying the following conditions:  ⌊x b⌋ = ⌊y b⌋;  x mod b = 0 ⇐⇒ y mod b = 0;  0 < x mod b < 1 x mod b = 1  2 b ⇐⇒ 0 < y mod b < 1 2 b; 2 b ⇐⇒ y mod b = 1 2 b;  1  2 b < x mod b < b ⇐⇒ 1  2 b < y mod b < b.  Prove that if fv is replaced by b−p−2Fv between steps A5 and A6 of Algorithm A, where Fv ∼ bp+2fv, the result of that algorithm will be unchanged.  If Fv is an integer and b is even, this operation essentially truncates fv to p+2 places while remembering whether any nonzero digits have been dropped, thereby minimizing the length of register that is needed for the addition in step A6.  6. [20] If the result of a FADD instruction is zero, what will be the sign of rA, according to the definitions of MIX’s floating point attachment given in this section? 7. [27] Discuss floating point arithmetic using balanced ternary notation. 8. [20] Give examples of normalized eight-digit floating decimal numbers u and v for which addition yields  a  exponent underflow,  b  exponent overflow, assuming that exponents must satisfy 0 ≤ e < 100. 9. [M24]  W. M. Kahan.  Assume that the occurrence of exponent underflow causes the result to be replaced by zero, with no error indication given. Using excess zero, eight-digit floating decimal numbers with e in the range −50 ≤ e < 50, find positive values of a, b, c, d, and y such that  11  holds. 10. [12] Give an example of normalized eight-digit floating decimal numbers u and v for which rounding overflow occurs in addition.   cid:120  11. [M20] Give an example of normalized, excess 50, eight-digit floating decimal  numbers u and v for which rounding overflow occurs in multiplication.   228  ARITHMETIC  4.2.1  12. [M25] Prove that rounding overflow cannot occur during the normalization phase of floating point division. 13. [30] When doing “interval arithmetic” we don’t want to round the results of a floating point computation; we want rather to implement operations such as ▽+ and △+ , which give the tightest possible representable bounds on the true sum:  u ▽+ v ≤ u + v ≤ u △+ v.  How should the algorithms of this section be modified for such a purpose? 14. [25] Write a MIX subroutine that begins with an arbitrary floating point number in register A, not necessarily normalized, and converts it to the nearest fixed point integer  or determines that the number is too large in absolute value to make such a conversion possible .   cid:120  15. [28] Write a MIX subroutine, to be used in connection with the other subroutines of this section, that calculates u  cid:88 mod 1, namely u−⌊u⌋ rounded to the nearest floating negative number, u  cid:88 mod 1 should be rounded so that the result is unity  even though  point number, given a floating point number u. Notice that when u is a very small  u mod 1 has been defined to be always less than unity, as a real number . 16. [HM21]  Robert L. Smith.  Design an algorithm to compute the real and imagi- nary parts of the complex number  a+ bi   c+ di , given real floating point values a, b, c, and d with c+ di ̸= 0. Avoid the computation of c2 + d2, since it would cause floating point overflow even when c or d is approximately the square root of the maximum allowable floating point value. 17. [40]  John Cocke.  Explore the idea of extending the range of floating point numbers by defining a single-word representation in which the precision of the fraction decreases as the magnitude of the exponent increases. 18. [25] Consider a binary computer with 36-bit words, on which positive floating binary numbers are represented as  0 e1e2 . . . e8f1f2 . . . f27 2; here  e1e2 . . . e8 2 is an excess  10000000 2 exponent and  f1f2 . . . f27 2 is a 27-bit fraction. Negative floating point numbers are represented by the two’s complement of the corresponding positive representation  see Section 4.1 . Thus, 1.5 is 201600000000 in octal notation, while −1.5 is 576200000000; the octal representations of 1.0 and −1.0 are 201400000000 and 576400000000, respectively.  A vertical line is used here to show the boundary between exponent and fraction.  Note that bit f1 of a normalized positive number is always 1, while it is almost always zero for negative numbers; the exceptional cases are representations of −2k. Suppose that the exact result of a floating point operation has the octal code 57274000000001; this  negative  33-bit fraction must be normalized and rounded to 27 bits. If we shift left until the leading fraction bit is zero, we get 57600000000020, but this rounds to the illegal value 576000000000; we have over-normalized, since the correct answer is 575400000000. On the other hand if we start  in some other problem  with the value 57274000000005 and stop before over-normalizing it, we get 57540000000050, which rounds to the unnormalized number 575400000001; subse- quent normalization yields 576000000002 while the correct answer is 576000000001. Give a simple, correct rounding rule that resolves this dilemma on such a machine   without abandoning two’s complement notation . 19. [24] What is the running time for the FADD subroutine in Program A, in terms of relevant characteristics of the data? What is the maximum running time, over all inputs that do not cause exponent overflow or underflow?   4.2.2  ACCURACY OF FLOATING POINT ARITHMETIC  229  Round numbers are always false. — SAMUEL JOHNSON  1750   I shall speak in round numbers, not absolutely accurate, yet not so wide from truth as to vary the result materially. — THOMAS JEFFERSON  1824   4.2.2. Accuracy of Floating Point Arithmetic Floating point computation is by nature inexact, and programmers can easily misuse it so that the computed answers consist almost entirely of “noise.” One of the principal problems of numerical analysis is to determine how accurate the results of certain numerical methods will be. There’s a credibility gap: We don’t know how much of the computer’s answers to believe. Novice computer users solve this problem by implicitly trusting in the computer as an infallible authority; they tend to believe that all digits of a printed answer are significant. Disillusioned computer users have just the opposite approach; they are constantly afraid that their answers are almost meaningless. Many serious mathematicians have attempted to analyze a sequence of floating point operations rigorously, but have found the task so formidable that they have tried to be content with plausibility arguments instead.  A thorough examination of error analysis techniques is beyond the scope of this book, but in the present section we shall study some of the low-level characteristics of floating point arithmetic errors. Our goal is to discover how to perform floating point arithmetic in such a way that reasonable analyses of error propagation are facilitated as much as possible.  A rough  but reasonably useful  way to express the behavior of floating point arithmetic can be based on the concept of “significant figures” or relative If we are representing an exact real number x inside a computer by error. using the approximation ˆx = x 1 + ϵ , the quantity ϵ =  ˆx − x  x is called the relative error of approximation. Roughly speaking, the operations of floating point multiplication and division do not magnify the relative error by very much; but floating point subtraction of nearly equal quantities  and floating point addition, u ⊕ v, where u is nearly equal to −v  can very greatly increase the relative error. So we have a general rule of thumb, that a substantial loss of accuracy is expected from such additions and subtractions, but not from multiplications and divisions. On the other hand, the situation is somewhat paradoxical and needs to be understood properly, since the “bad” additions and subtractions are always performed with perfect accuracy!  See exercise 25.   One of the consequences of the possible unreliability of floating point addi-  tion is that the associative law breaks down:   u ⊕ v  ⊕ w ̸= u ⊕  v ⊕ w ,  for many u, v, w.   1   For example,  11111113. ⊕ −11111111.  ⊕ 7.5111111 = 2.0000000 ⊕ 7.5111111 = 9.5111111; 11111113. ⊕  −11111111. ⊕ 7.5111111  = 11111113. ⊕ −11111103. = 10.000000.   230  ARITHMETIC  4.2.2   All examples in this section are given in eight-digit floating decimal arithmetic, with exponents indicated by an explicit decimal point. Recall that, as in Section 4.2.1, the symbols ⊕, ⊖, ⊗, ⊘ are used to stand for floating point operations that correspond to the exact operations +, −, ×,  .   or “n  In view of the failure of the associative law, the comment of Mrs. La Touche that appears at the beginning of this chapter makes a good deal of sense with respect to floating point arithmetic. Mathematical notations like “a1 + a2 + a3” k=1 ak” are inherently based upon the assumption of associativity, so a programmer must be especially careful not to assume implicitly that the associative law is valid. A. An axiomatic approach. Although the associative law is not valid, the commutative law  u ⊕ v = v ⊕ u   2  does hold, and this law can be a valuable conceptual asset in programming and in the analysis of programs. Equation  2  suggests that we should look for additional examples of important laws that are satisfied by ⊕, ⊖, ⊗, and ⊘; it is not unreasonable to say that floating point routines should be designed to preserve as many of the ordinary mathematical laws as possible. If more axioms are valid, it becomes easier to write good programs, and programs also become more portable from machine to machine.  Let us therefore consider some of the other basic laws that are valid for normalized floating point operations as described in the previous section. First we have  u ⊖ v = u ⊕ −v; − u ⊕ v  = −u ⊕ −v;  if and only if  u ⊕ 0 = u.  u ⊕ v = 0  v = −u;  From these laws we can derive further identities; for example  exercise 1 ,  u ⊖ v = − v ⊖ u .  Identities  2  to  6  are easily deduced from the algorithms in Section 4.2.1.  The following rule is slightly less obvious: then  u ≤ v  if  u ⊕ w ≤ v ⊕ w.   8  Instead of attempting to prove this rule by analyzing Algorithm 4.2.1A, let us go back to the basic principle by which that algorithm was designed.  Algorithmic proofs aren’t always easier than mathematical ones.  Our idea was that the floating point operations should satisfy u ⊕ v = round u + v , u ⊗ v = round u × v ,  u ⊖ v = round u − v , u ⊘ v = round u   v ,   9    3   4   5   6    7    4.2.2  ACCURACY OF FLOATING POINT ARITHMETIC  231  where round x  denotes the best floating point approximation to x as defined in Algorithm 4.2.1N. We have  round −x  = −round x ,   10   11  and these fundamental relations yield properties  2  through  8  immediately. We can also write down several more identities:  round x  ≤ round y ,  implies  x ≤ y  u ⊗ v = v ⊗ u, u ⊗ v = 0   −u  ⊗ v = − u ⊗ v , if and only if  u = 0 or v = 0;  1 ⊗ v = v;   −u  ⊘ v = u ⊘  −v  = − u ⊘ v ;  0 ⊘ v = 0,  u ⊘ 1 = u,  u ⊘ u = 1.  If u ≤ v and w > 0, then u ⊗ w ≤ v ⊗ w and u ⊘ w ≤ v ⊘ w; also w ⊘ u ≥ w ⊘ v when v ≥ u > 0. If u⊕ v = u + v, then  u⊕ v ⊖ v = u; and if u⊗ v = u× v ̸= 0, then  u ⊗ v  ⊘ v = u. We see that a good deal of regularity is present in spite of the inexactness of the floating point operations, when things have been defined properly.  Several familiar rules of algebra are still, of course, conspicuously absent from the collection of identities above. The associative law for floating point multiplication is not strictly true, as shown in exercise 3, and the distributive law between ⊗ and ⊕ can fail rather badly: Let u = 20000.000, v = −6.0000000, and w = 6.0000003; then   u ⊗ v  ⊕  u ⊗ w  = −120000.00 ⊕ 120000.01 = .010000000  u ⊗  v ⊕ w  = 20000.000 ⊗ .00000030000000 = .0060000000  so   12  On the other hand we do have b ⊗  v ⊕ w  =  b ⊗ v  ⊕  b ⊗ w , when b is the floating point radix, since  u ⊗  v ⊕ w  ̸=  u ⊗ v  ⊕  u ⊗ w .  round bx  = b round x .   13   Strictly speaking, the identities and inequalities we are considering in this section implicitly assume that exponent underflow and overflow do not occur. The function round x  is undefined when x is too small or too large, and equations such as  13  hold only when both sides are defined.   The failure of Cauchy’s fundamental inequality  1 + ··· + x2  x2  n  y2  1 + ··· + y2  n  ≥  x1y1 + ··· + xnyn 2  is another important example of the breakdown of traditional algebra in the presence of floating point arithmetic. Exercise 7 shows that Cauchy’s inequality can fail even in the simple case n = 2, x1 = x2 = 1. Novice programmers who   232  ARITHMETIC  4.2.2  calculate the standard deviation of some observations by using the textbook formula    n    1≤k≤n  k − x2  2     xk  1≤k≤n  σ =  n n − 1    14   often find themselves taking the square root of a negative number! A much better way to calculate means and standard deviations with floating point arithmetic is to use the recurrence formulas  M1 = x1, S1 = 0,  Mk = Mk−1 ⊕  xk ⊖ Mk−1  ⊘ k, Sk = Sk−1 ⊕  xk ⊖ Mk−1  ⊗  xk ⊖ Mk ,  for 2 ≤ k ≤ n, where σ =Sn  n − 1 .   15   16  [See B. P. Welford, Technometrics 4  1962 , 419–420.] With this method Sn can never be negative, and we avoid other serious problems encountered by the naïve method of accumulating sums, as shown in exercise 16.  See exercise 19 for a summation technique that provides an even better guarantee on the accuracy.  Although algebraic laws do not always hold exactly, we can often show that they aren’t too far off base. When be−1 ≤ x < be we have round x  = x + ρ x , where ρ x  ≤ 1  2 be−p; hence  round x  = x1 + δ x ,  < 1  2 b1−p.  2 be−p  where the relative error is bounded independently of x: 2 be−p 1 be−1 + 1  δ x  = ρ x  x ≤  be−1 + ρ x  ≤  ρ x   point calculations in a simple way, since u ⊕ v =  u + v 1 + δ u + v , etc.  We can use this inequality to estimate the relative error of normalized floating  As an example of typical error-estimation procedures, let us consider the associative law for multiplication. Exercise 3 shows that  u ⊗ v  ⊗ w is not in general equal to u⊗  v ⊗ w ; but the situation in this case is much better than it was with respect to the associative law of addition  1  and the distributive law  12 . In fact, we have   u ⊗ v  ⊗ w = uv  1 + δ1  ⊗ w = uvw 1 + δ1  1 + δ2 , u ⊗  v ⊗ w  = u ⊗ vw  1 + δ3  = uvw 1 + δ3  1 + δ4 ,  for some δ1, δ2, δ3, δ4, provided that no exponent underflow or overflow occurs, where δj < 1  2 b1−p for each j. Hence   17    18    u ⊗ v  ⊗ w u ⊗  v ⊗ w  =  1 + δ1  1 + δ2  2 b1−p2  δ < 2b1−p 1 − 1   1 + δ3  1 + δ4  = 1 + δ,  where   19  The number b1−p occurs so often in such analyses, it has been given a special name, one ulp, meaning one unit in the last place of the fraction part. Floating  .   4.2.2  ACCURACY OF FLOATING POINT ARITHMETIC  233  point operations are correct to within half an ulp, and the calculation of uvw by two floating point multiplications will be correct within about one ulp  ignoring second-order terms . Hence the associative law for multiplication holds to within about two ulps of relative error. We have shown that  u ⊗ v  ⊗ w is approximately equal to u ⊗  v ⊗ w , except when exponent overflow or underflow is a problem. It is worthwhile to study this intuitive idea of approximate equality in more detail; can we make such a statement more precise in a reasonable way?  Programmers who use floating point arithmetic almost never want to test if two computed values are exactly equal to each other  or at least they hardly ever should try to do so , because this is an extremely improbable occurrence. For example, if a recurrence relation  xn+1 = f xn   is being used, where the theory in some textbook says that xn approaches a limit as n → ∞, it is usually a mistake to wait until xn+1 = xn for some n, since the sequence xn might be periodic with a longer period due to the rounding of intermediate results. The proper procedure is to wait until xn+1 − xn < δ, for some suitably chosen number δ; but since we don’t necessarily know the order of magnitude of xn in advance, it is even better to wait until  xn+1 − xn ≤ ϵxn;   20  now ϵ is a number that is much easier to select. Relation  20  is another way of saying that xn+1 and xn are approximately equal; and our discussion indicates that a relation of “approximately equal” would be more useful than the traditional relation of equality, when floating point computations are involved, if we could only define a suitable approximation relation.  In other words, the fact that strict equality of floating point values is of little importance implies that we ought to have a new operation, floating point comparison, which is intended to help assess the relative values of two floating point quantities. The following definitions seem to be appropriate for base b, excess q, floating point numbers u =  eu, fu  and v =  ev, fv :   ϵ   ϵ   ϵ   ϵ   u ≺ v u ∼ v u ≻ v u ≈ v  if and only if if and only if if and only if if and only if  v − u > ϵ max beu−q, bev−q ; v − u ≤ ϵ max beu−q, bev−q ; u − v > ϵ max beu−q, bev−q ; v − u ≤ ϵ min beu−q, bev−q .   21   22   23   24  These definitions apply to unnormalized values as well as to normalized ones. Notice that exactly one of the conditions u ≺ v  definitely less than , u ∼ v  approximately equal to , or u ≻ v  definitely greater than  must always hold for any given pair of values u and v. The relation u ≈ v is somewhat stronger than u ∼ v, and it might be read “u is essentially equal to v.” All of the relations are specified in terms of a positive real number ϵ that measures the degree of approximation being considered.   234  ARITHMETIC  4.2.2   25   26   27   28   29   30   31    33    34   35   One way to view the definitions above is to associate a “neighborhood” set N u  = {x  x − u ≤ ϵbeu−q} with each floating point number u; thus, N u  represents a set of values near u based on the exponent of u’s floating point rep- resentation. In these terms, we have u ≺ v if and only if N u  < v and u < N v ; u ∼ v if and only if u ∈ N v  or v ∈ N u ; u ≻ v if and only if u > N v  and N u  > v; u ≈ v if and only if u ∈ N v  and v ∈ N u .  Here we are assuming that the parameter ϵ, which measures the degree of approximation, is a constant; a more complete notation would indicate the dependence of N u  upon ϵ.   Here are some simple consequences of definitions  21 – 24 :  if if  u ≺ v u ≈ v  then  ϵ   ϵ  then u ≈ u  ϵ ; then  ϵ   u ≺ v  v ≻ u  ϵ ; u ∼ v  ϵ ;  u < v;  if u ≺ v u ∼ v u ≈ v  ϵ1    ϵ1   if if if u ≺ v  u ≈ v  if  if   ϵ1  and ϵ1 ≥ ϵ2  ϵ1  and ϵ1 ≤ ϵ2  ϵ1  and ϵ1 ≤ ϵ2 and v ≺ w  ϵ2  and v ≈ w  ϵ2   then then then  then  then  u ≺ v u ∼ v u ≈ v   ϵ2 ;  ϵ2 ;  ϵ2 ;  u ≺ w min ϵ1, ϵ2 ;  32   u ∼ w  ϵ1 + ϵ2 .  Moreover, we can prove without difficulty that  u − v ≤ ϵu u − v ≤ ϵu  and or  u − v ≤ ϵv u − v ≤ ϵv  implies implies  u ≈ v u ∼ v   ϵ ;  ϵ ;  and conversely, for normalized floating point numbers u and v, when ϵ < 1,   ϵ   ϵ   implies implies  u ≈ v  36  u ∼ v  37  Let ϵ0 = b1−p be one ulp. The derivation of  17  establishes the inequality  u − v ≤ bϵu and u − v ≤ bϵv; u − v ≤ bϵu u − v ≤ bϵv.  2 ϵ0 minx,round x , hence  x − round x  = ρ x  < 1  or  it follows that u ⊕ v ≈ u + v   1 multiplication derived above can be recast as follows: We have    1 2 ϵ0 ;   38  2 ϵ0 , etc. The approximate associative law for  x ≈ round x    u ⊗ v  ⊗ w − u ⊗  v ⊗ w  <  u ⊗  v ⊗ w   2ϵ0  1 − 1 2 ϵ0 2  by  19 , and the same inequality is valid with  u ⊗ v  ⊗ w and u ⊗  v ⊗ w  interchanged. Hence by  34 ,  whenever ϵ ≥ 2ϵ0  1 − 1 ϵ = 0.00000021.   u ⊗ v  ⊗ w ≈ u ⊗  v ⊗ w   39  2 ϵ0 2. For example, if b = 10 and p = 8 we may take   ϵ    4.2.2  ACCURACY OF FLOATING POINT ARITHMETIC  235 The relations ≺, ∼, ≻, and ≈ are useful within numerical algorithms, and it is therefore a good idea to provide routines for comparing floating point numbers as well as for doing arithmetic on them.  Let us now shift our attention back to the question of finding exact relations that are satisfied by the floating point operations. It is interesting to note that floating point addition and subtraction are not completely intractable from an axiomatic standpoint, since they do satisfy the nontrivial identities stated in the following theorems. Theorem A. Let u and v be normalized floating point numbers. Then   u ⊕ v  ⊖ u + u ⊕ v  ⊖ u ⊕ v  ⊖ u = u ⊕ v,   40   provided that no exponent overflow or underflow occurs. This rather cumbersome-looking identity can be rewritten in a simpler manner: Let  u′ =  u ⊕ v  ⊖ v , u′′ =  u ⊕ v  ⊖ v′,  v′ =  u ⊕ v  ⊖ u ; v′′ =  u ⊕ v  ⊖ u′.  Intuitively, u′ and u′′ should be approximations to u, and v′ and v′′ should be approximations to v. Theorem A tells us that  This is a stronger statement than the identity  u ⊕ v = u′ + v′′ = u′′ + v′.  u ⊕ v = u′ ⊕ v′′ = u′′ ⊕ v′,  which follows by rounding  42 . Proof. Let us say that t is a tail of x modulo be if  t ≡ x  modulo be ,   44  thus, x − round x  is always a tail of x. The proof of Theorem A rests largely on the following simple fact proved in exercise 11: Lemma T. If t is a tail of the floating point number x, then x ⊖ t = x − t.  t ≤ 1  2 be;  Let w = u ⊕ v. Theorem A holds trivially when w = 0. By multiplying all variables by a suitable power of b, we may assume without loss of generality that ew = p. Then u + v = w + r, where r is a tail of u + v modulo 1. Furthermore u′ = round w− v  = round u− r  = u− r− t, where t is a tail of u− r modulo be and e = eu′ − p. If e ≤ 0, then t ≡ u − r ≡ −v  modulo be , hence t is a tail of −v and v′′ = round w − u′  = round v + t  = v + t; this proves  40 . If e > 0, then 2, we have u ≥ bp − 1. It follows that u is u − r ≥ bp − 1 an integer, so r is a tail of v modulo 1. If u′ = u, then t = −r is a tail of −v. Otherwise the relation round u − r  ̸= u implies that u = bp − 1, r = 1 2, u′ = bp, t = r; again t is a tail of −v.  2; and since r ≤ 1   41    42    43    236  ARITHMETIC  4.2.2  Theorem A exhibits a regularity property of floating point addition, but it doesn’t seem to be an especially useful result. The following identity is more significant: Theorem B. Under the hypotheses of Theorem A and  41 ,  u + v =  u ⊕ v  + u ⊖ u′  ⊕  v ⊖ v′′ .   45  In fact, we can show that u ⊖ u′ = u − u′, v ⊖ v′′ = v − v′′, and Proof.  u− u′ ⊕  v − v′′  =  u− u′  +  v − v′′ , hence  45  will follow from Theorem A. Using the notation of the preceding proof, these relations are respectively equiv- alent to  round t  = t,  round t + r  = t + r,   46  Exercise 12 establishes the theorem in the special case eu − ev ≥ p. Otherwise u + v has at most 2p significant digits and it is easy to see that round r  = r. If now e > 0, the proof of Theorem A shows that t = −r or t = r = ± 1 2. If e ≤ 0 we have t + r ≡ u and t ≡ −v  modulo be ; this is enough to prove that t + r and t round to themselves, provided that eu ≥ e and ev ≥ e. But either eu < 0 or ev < 0 would contradict our hypothesis that eu − ev < p, since ew = p.  round r  = r.  Theorem B gives an explicit formula for the difference between u + v and u⊕ v, in terms of quantities that can be calculated directly using five operations of floating point arithmetic. If the radix b is 2 or 3, we can improve on this result, obtaining the exact value of the correction term with only two floating point operations and one  fixed point  comparison of absolute values: Theorem C. If b ≤ 3 and u ≥ v, then  u + v =  u ⊕ v  +u ⊖  u ⊕ v  ⊕ v.   47  Proof. Following the conventions of preceding proofs again, we wish to show that v ⊖ v′ = r. It suffices to show that v′ = w − u, because  46  will then yield v ⊖ v′ = round v − v′  = round u + v − w  = round r  = r. We shall in fact prove  47  whenever b ≤ 3 and eu ≥ ev. If eu ≥ p, then r is a tail of v modulo 1, hence v′ = w ⊖ u = v ⊖ r = v − r = w − u as desired. If eu < p, then we must have eu = p − 1, and w − u is a multiple of b−1; it will therefore round to itself if its magnitude is less than bp−1 + b−1. Since b ≤ 3, we have indeed w − u ≤ w − u − v + v ≤ 1 2 +  bp−1 − b−1  < bp−1 + b−1. This completes the proof.  The proofs of Theorems A, B, and C do not rely on the precise definitions of round x  in the ambiguous cases when x is exactly midway between consecutive floating point numbers; any way of resolving the ambiguity will suffice for the validity of everything we have proved so far.  No rounding rule can be best for every application. For example, we gener- ally want a special rule when computing our income tax. But for most numerical calculations the best policy appears to be the rounding scheme specified in Algorithm 4.2.1N, which insists that the least significant digit should always   4.2.2  ACCURACY OF FLOATING POINT ARITHMETIC  237  be made even  or always odd  when an ambiguous value is rounded. This is not a trivial technicality, of interest only to nit-pickers; it is an important practical consideration, since the ambiguous case arises surprisingly often and a biased rounding rule produces significantly poor results. For example, consider decimal arithmetic and assume that remainders of 5 are always rounded upwards. Then if u = 1.0000000 and v = 0.55555555 we have u⊕v = 1.5555556; and if we floating- subtract v from this result we get u′ = 1.0000001. Adding and subtracting v from u′ gives 1.0000002, and the next time we get 1.0000003, etc.; the result keeps growing although we are adding and subtracting the same value.  This phenomenon, called drift, will not occur when we use a stable rounding  rule based on the parity of the least significant digit. More precisely:  Theorem D.   u ⊕ v  ⊖ v  ⊕ v ⊖ v =  u ⊕ v  ⊖ v.  For example, if u = 1.2345679 and v = −0.23456785, we find  u ⊕ v = 1.0000000,   u ⊕ v  ⊖ v  ⊕ v = 0.99999995,    u ⊕ v  ⊖ v  ⊕ v ⊖ v = 1.2345678.   u ⊕ v  ⊖ v = 1.2345678,  The proof for general u and v seems to require a case analysis even more detailed than that in the theorems above; see the references below.  Theorem D is valid both for “round to even” and “round to odd”; how should we choose between these possibilities? When the radix b is odd, ambiguous cases never arise except during floating point division, and the rounding in such cases is comparatively unimportant. For even radices, there is reason to prefer the following rule: “Round to even when b 2 is odd, round to odd when b 2 is even.” The least significant digit of a floating point fraction occurs frequently as a remainder to be rounded off in subsequent calculations, and this rule avoids generating the digit b 2 in the least significant position whenever possible; its effect is to provide some memory of an ambiguous rounding so that subsequent rounding will tend to be unambiguous. For example, if we were to round to odd in the decimal system, repeated rounding of the number 2.44445 to one less place each time leads to the sequence 2.4445, 2.445, 2.45, 2.5, 3; if we round to even, such situations do not occur, although repeated rounding of a number like 2.5454 will lead to almost as much error. [See Roy A. Keir, Inf. Proc. Letters 3  1975 , 188–189.] Some people prefer rounding to even in all cases, so that the least significant digit will tend to be 0 more often. Exercise 23 demonstrates this advantage of round-to-even. Neither alternative conclusively dominates the other; fortunately the base is usually b = 2 or b = 10, when everyone agrees that round-to-even is best. A reader who has checked some of the details of the proofs above will realize the immense simplification that has been afforded by the simple rule u ⊕ v = round u + v . If our floating point addition routine would fail to give this result even in a few rare cases, the proofs would become enormously more complicated and perhaps they would even break down completely. Theorem B fails if truncation arithmetic is used in place of rounding, that is, if we let u ⊕ v = trunc u + v  and u ⊖ v = trunc u − v , where trunc x  for a   ARITHMETIC  238 4.2.2 positive real x is the largest floating point number ≤ x. An exception to Theo- rem B would then occur for cases such as  20, +.10000001 ⊕  10,−.10000001  =  20, +.10000000 , when the difference between u+v and u⊕v cannot be expressed exactly as a floating point number; and also for cases such as 12345678 ⊕ .012345678, when it can be.  Many people feel that, since floating point arithmetic is inexact by nature, there is no harm in making it just a little bit less exact in certain rather rare cases, if it is convenient to do so. This policy saves a few cents in the design of computer hardware, or a small percentage of the average running time of a subroutine. But our discussion shows that such a policy is mistaken. We could save about five percent of the running time of the FADD subroutine, Program 4.2.1A, and about 25 percent of its space, if we took the liberty of rounding incorrectly in a few cases, but we are much better off leaving it as it is. The reason is not to glorify “bit chasing”; a more fundamental issue is at stake here: Numerical subroutines should deliver results that satisfy simple, useful mathematical laws whenever possible. The crucial formula u ⊕ v = round u + v  is a regularity property that makes a great deal of difference between whether mathematical analysis of computational algorithms is worth doing or worth avoiding. Without any underlying symmetry properties, the job of proving interesting results becomes extremely unpleasant. The enjoyment of one’s tools is an essential ingredient of successful work. B. Unnormalized floating point arithmetic. The policy of normalizing all floating point numbers may be construed in two ways: We may look on it favor- ably by saying that it is an attempt to get the maximum possible accuracy ob- tainable with a given degree of precision, or we may consider it to be potentially dangerous since it tends to imply that the results are more accurate than they really are. When we normalize the result of  1, +.31428571  ⊖  1, +.31415927  to  −2, +.12644000 , we are suppressing information about the possibly greater inaccuracy of the latter quantity. Such information would be retained if the answer were left as  1, +.00012644 .  The input data to a problem is frequently not known as precisely as the floating point representation allows. For example, the values of Avogadro’s number and Planck’s constant are not known to eight significant digits, and it might be more appropriate to denote them, respectively, by  −23, +.00066261    27, +.00060221   and  instead of by  24, +.60221400  and  −26, +.66261000 . It would be nice if we could give our input data for each problem in an unnormalized form that expresses how much precision is assumed, and if the output would indicate just how much precision is known in the answer. Unfortunately, this is a terribly difficult problem, although the use of unnormalized arithmetic can help to give some indication. For example, we can say with a fair degree of certainty that the product of Avogadro’s number by Planck’s constant is  1, +.00039903 , and that their sum is  27, +.00060221 .  The purpose of this example is not to suggest that   4.2.2  ACCURACY OF FLOATING POINT ARITHMETIC  239  any important physical significance should be attached to the sum and product of these fundamental constants; the point is that it is possible to preserve a little of the information about precision in the result of calculations with imprecise quantities, when the original operands are independent of each other.   The rules for unnormalized arithmetic are simply this: Let lu be the number of leading zeros in the fraction part of u =  eu, fu , so that lu is the largest integer ≤ p with fu < b−lu. Then addition and subtraction are performed just as in Algorithm 4.2.1A, except that all scaling to the left is suppressed. Multiplication and division are performed as in Algorithm 4.2.1M, except that the answer is scaled right or left so that precisely max lu, lv  leading zeros appear. Essentially the same rules have been used in manual calculation for many years.  It follows that, for unnormalized computations,  eu⊕v, eu⊖v = max eu, ev  +  0 or 1   eu⊗v = eu + ev − q − min lu, lv  −  0 or 1  eu⊘v = eu − ev + q − lu + lv + max lu, lv  +  0 or 1 .   48   49   50  When the result of a calculation is zero, an unnormalized zero  often called an “order of magnitude zero”  is given as the answer; this indicates that the answer may not truly be zero, we just don’t know any of its significant digits.  Error analysis takes a somewhat different form with unnormalized floating  point arithmetic. Let us define δu = 1   51  This quantity depends on the representation of u, not just on the value beu−qfu. Our rounding rule tells us that  if u =  eu, fu .  2 beu−q−p  u ⊕ v −  u + v  ≤ δu⊕v, u ⊗ v −  u × v  ≤ δu⊗v,  u ⊖ v −  u − v  ≤ δu⊖v, u ⊘ v −  u   v  ≤ δu⊘v.  exponent of the result of each operationEqs.  48  to  50 .  These inequalities apply to normalized as well as unnormalized arithmetic; the main difference between the two types of error analysis is the definition of the We have remarked that the relations ≺, ∼, ≻, and ≈ defined earlier in this section are valid and meaningful for unnormalized numbers as well as for normalized numbers. As an example of the use of these relations, let us prove an approximate associative law for unnormalized addition, analogous to  39 :   u ⊕ v  ⊕ w ≈ u ⊕  v ⊕ w    ϵ ,  for suitable ϵ. We have   u⊕ v ⊕ w −  u + v + w  ≤ u⊕ v ⊕ w − u⊕ v  + w +u⊕ v −  u + v    52   ≤ δ u⊕v ⊕w + δu⊕v ≤ 2δ u⊕v ⊕w.  A similar formula holds for u ⊕  v ⊕ w  −  u + v + w . Now since e u⊕v ⊕w = max eu, ev, ew + 0, 1, or 2 , we have δ u⊕v ⊕w ≤ b2δu⊕ v⊕w . Therefore we find   ARITHMETIC  240 4.2.2 that  52  is valid when ϵ ≥ b2−p + b−p; unnormalized addition is not as erratic as normalized addition with respect to the associative law.  It should be emphasized that unnormalized arithmetic is by no means a panacea. There are examples where it indicates greater accuracy than is present  for example, addition of a great many small quantities of about the same magni- tude, or evaluation of xn for large n ; and there are many more examples when it indicates poor accuracy while normalized arithmetic actually does produce good results. There is an important reason why no straightforward one-operation-at- a-time method of error analysis can be completely satisfactory, namely the fact that operands are usually not independent of each other. This means that errors tend to cancel or reinforce each other in strange ways. For example, suppose that x is approximately 1 2, and suppose that we have an approximation y = x + δ with absolute error δ. If we now wish to compute x 1− x , we can form y 1− y ; 2 + ϵ we find y 1 − y  = x 1 − x  − 2ϵδ − δ2, so the absolute error has if x = 1 decreased substantially: It has been multiplied by a factor of 2ϵ + δ. This is just one case where multiplication of imprecise quantities can lead to a quite accurate result when the operands are not independent of each other. A more obvious example is the computation of x⊖ x, which can be obtained with perfect accuracy regardless of how bad an approximation to x we begin with.  The extra information that unnormalized arithmetic gives us can often be more important than the information it destroys during an extended calcula- tion, but  as usual  we must use it with care. Examples of the proper use of unnormalized arithmetic are discussed by R. L. Ashenhurst and N. Metropolis in Computers and Computing, AMM, Slaught Memorial Papers 10  February 1965 , 47–59; by N. Metropolis in Numer. Math. 7  1965 , 104–112; and by R. L. Ashenhurst in Error in Digital Computation 2, edited by L. B. Rall  New York: Wiley, 1965 , 3–37. Appropriate methods for computing standard mathematical functions with both input and output in unnormalized form are given by R. L. Ashenhurst in JACM 11  1964 , 168–187. An extension of unnormalized arithmetic, which remembers that certain values are known to be exact, has been discussed by N. Metropolis in IEEE Trans. C-22  1973 , 573–576. C. Interval arithmetic. Another approach to the problem of error determi- nation is the so-called interval or range arithmetic, in which rigorous upper and lower bounds on each number are maintained during the calculations. Thus, for example, if we know that u0 ≤ u ≤ u1 and v0 ≤ v ≤ v1, we represent this by the interval notation u = [u0 . . u1], v = [v0 . . v1]. The sum u⊕v is [u0▽+ v0 . . u1△+ v1], where ▽+ denotes “lower floating point addition,” the greatest representable number less than or equal to the true sum, and △+ is defined similarly  see exercise 4.2.1–13 . Furthermore u⊖ v = [u0 ▽− v1 . . u1 △− v0]; and if u0 and v0 are positive, we have u ⊗ v = [u0 ▽× v0 . . u1 △× v1], u ⊘ v = [u0 ▽  v1 . . u1 △  v0]. For example, we might represent Avogadro’s number and Planck’s constant as  N = 24, +.60221331  . .  24, +.60221403 , h = −26, +.66260715  . .  −26, +.66260795 ;   4.2.2  ACCURACY OF FLOATING POINT ARITHMETIC  241  their sum and product would then turn out to be  N ⊕ h = 24, +.60221331  . .  24, +.60221404 , N ⊗ h = −2, +.39903084  . .  −2, +.39903181 .  If we try to divide by [v0 . . v1] when v0 < 0 < v1, there is a possibility of division by zero. Since the philosophy underlying interval arithmetic is to provide rigorous error estimates, a divide-by-zero error should be signalled in this case. However, overflow and underflow need not be treated as fatal errors in interval arithmetic, if special conventions are introduced as discussed in exercise 24.  Interval arithmetic takes only about twice as long as ordinary arithmetic, and it provides truly reliable error estimates. Considering the difficulty of mathematical error analyses, this is indeed a small price to pay. Since the intermediate values in a calculation often depend on each other, as explained above, the final estimates obtained with interval arithmetic will tend to be pessimistic; and iterative numerical methods often have to be redesigned if we want to deal with intervals. However, the prospects for effective use of interval arithmetic look very good, so efforts should be made to increase its availability and to make it as user-friendly as possible. D. History and bibliography. Jules Tannery’s classic treatise on decimal calculations, Leçons d’Arithmétique  Paris: Colin, 1894 , stated that positive numbers should be rounded upwards if the first discarded digit is 5 or more; since exactly half of the decimal digits are 5 or more, he felt that this rule would round upwards exactly half of the time, on the average, so it would produce compensating errors. The idea of “round to even” in the ambiguous cases seems to have been mentioned first by James B. Scarborough in the first edition of his pioneering book Numerical Mathematical Analysis  Baltimore: Johns Hopkins Press, 1930 , 2; in the second  1950  edition he amplified his earlier remarks, stating that “It should be obvious to any thinking person that when a 5 is cut off, the preceding digit should be increased by 1 in only half the cases,” and he recommended round-to-even in order to achieve this.  The first analysis of floating point arithmetic was given by F. L. Bauer and K. Samelson, Zeitschrift für angewandte Math. und Physik 4  1953 , 312–316. The next publication was not until over five years later: J. W. Carr III, CACM 2, 5  May 1959 , 10–15. See also P. C. Fischer, Proc. ACM Nat. Meeting 13  1958 , Paper 39. The book Rounding Errors in Algebraic Processes  Englewood Cliffs: Prentice–Hall, 1963 , by J. H. Wilkinson, shows how to apply error analysis of the individual arithmetic operations to the error analysis of large-scale problems; see also his treatise on The Algebraic Eigenvalue Problem  Oxford: Clarendon Press, 1965 .  Additional early work on floating point accuracy is summarized in two important papers that can be especially recommended for further study: W. M. Kahan, Proc. IFIP Congress  1971 , 2, 1214–1239; R. P. Brent, IEEE Trans. C-22  1973 , 601–607. Both papers include useful theory and demonstrate that it pays off in practice.   242  ARITHMETIC  4.2.2 The relations ≺, ∼, ≻, ≈ introduced in this section are similar to ideas published by A. van Wijngaarden in BIT 6  1966 , 66–81. Theorems A and B above were inspired by some related work of Ole Møller, BIT 5  1965 , 37–50, 251–255; Theorem C is due to T. J. Dekker, Numer. Math. 18  1971 , 224– 242. Extensions and refinements of all three theorems have been published by S. Linnainmaa, BIT 14  1974 , 167–202. W. M. Kahan introduced Theorem D in some unpublished notes; for a complete proof and further commentary, see J. F. Reiser and D. E. Knuth, Inf. Proc. Letters 3  1975 , 84–87, 164.  Unnormalized floating point arithmetic was recommended by F. L. Bauer and K. Samelson in the article cited above, and it was independently used by J. W. Carr III at the University of Michigan in 1953. Several years later, the MANIAC III computer was designed to include both kinds of arithmetic in its hardware; see R. L. Ashenhurst and N. Metropolis, JACM 6  1959 , 415–428, IEEE Trans. EC-12  1963 , 896–901; R. L. Ashenhurst, Proc. Spring Joint Com- puter Conf. 21  1962 , 195–202. See also H. L. Gray and C. Harrison, Jr., Proc. Eastern Joint Computer Conf. 16  1959 , 244–248, and W. G. Wadey, JACM 7  1960 , 129–139, for further early discussions of unnormalized arithmetic.  For early developments in interval arithmetic, and some modifications, see A. Gibb, CACM 4  1961 , 319–320; B. A. Chartres, JACM 13  1966 , 386– 403; and the book Interval Analysis by Ramon E. Moore  Prentice–Hall, 1966 . The subsequent flourishing of this subject is described in Moore’s later book, Methods and Applications of Interval Analysis  Philadelphia: SIAM, 1979 .  An extension of the Pascal language that allows variables to be of type “interval” was developed at the University of Karlsruhe in the early 1980s. For a description of this language, which also includes numerous other features for scientific computing, see Pascal-SC by Bohlender, Ullrich, Wolff von Gudenberg, and Rall  New York: Academic Press, 1987 .  The book Grundlagen des numerischen Rechnens: Mathematische Begrün- dung der Rechnerarithmetik by Ulrich Kulisch  Mannheim: Bibl. Inst., 1976  is entirely devoted to the study of floating point arithmetic systems. See also Kulisch’s article in IEEE Trans. C-26  1977 , 610–621, and his more recent book written jointly with W. L. Miranker, entitled Computer Arithmetic in Theory and Practice  New York: Academic Press, 1981 .  An excellent summary of more recent work on floating point error analysis appears in the book Accuracy and Stability of Numerical Algorithms by N. J. Higham  Philadelphia: SIAM, 1996 . EXERCISES Note: Normalized floating point arithmetic is assumed unless the contrary is specified. 1. [M18] Prove that identity  7  is a consequence of  2  through  6 . 2. [M20] Use identities  2  through  8  to prove that  u ⊕ x  ⊕  v ⊕ y  ≥ u ⊕ v whenever x ≥ 0 and y ≥ 0. 3. [M20] Find eight-digit floating decimal numbers u, v, and w such that  u ⊗  v ⊗ w  ̸=  u ⊗ v  ⊗ w,  and such that no exponent overflow or underflow occurs during the computations.   4.2.2  ACCURACY OF FLOATING POINT ARITHMETIC  243  4. [10] Is it possible to have floating point numbers u, v, and w for which exponent overflow occurs during the calculation of u ⊗  v ⊗ w  but not during the calculation of  u ⊗ v  ⊗ w? 5. [M20] Is u⊘ v = u⊗ 1⊘ v  an identity, for all floating point numbers u and v ̸= 0 such that no exponent overflow or underflow occurs? 6. [M22] Are either of the following two identities valid for all floating point num- bers u?  a  0 ⊖  0 ⊖ u  = u;  b  1 ⊘  1 ⊘ u  = u. 7. [M21] Let u 2⃝ stand for u ⊗ u. Find floating binary numbers u and v such that  u ⊕ v  2⃝ > 2 u 2⃝ + v 2⃝ .   cid:120  8. [20] Let ϵ = 0.0001; which of the relations  u ≺ v   ϵ ,  u ∼ v   ϵ ,  u ≻ v   ϵ ,  u ≈ v   ϵ   hold for the following pairs of base 10, excess 0, eight-digit floating point numbers? a  u =  1, +.31415927 , v =  1, +.31416000 ; b  u =  0, +.99997000 , v =  1, +.10000039 ; c  u =  24, +.60221400 , v =  27, +.00060221 ; d  u =  24, +.60221400 , v =  31, +.00000006 ; e  u =  24, +.60221400 , v =  28, +.00000000 . 9. [M22] Prove  33 , and explain why the conclusion cannot be strengthened to the relation u ≈ w  ϵ1 + ϵ2 .   cid:120  10. [M25]  W. M. Kahan.  A certain computer performs floating point arithmetic  without proper rounding, and, in fact, its floating point multiplication routine ignores all but the first p most significant digits of the 2p-digit product fufv.  Thus when fufv < 1 b, the least-significant digit of u ⊗ v always comes out to be zero, due to subsequent normalization.  Show that this causes the monotonicity of multiplication to fail; in other words, exhibit positive normalized floating point numbers u, v, and w such that u   v ⊗ w on this machine. 11. [M20] Prove Lemma T. 12. [M24] Carry out the proof of Theorem B and  46  when eu − ev ≥ p.   cid:120  13. [M25] Some programming languages  and even some computers  make use of  floating point arithmetic only, with no provision for exact calculations with integers. If operations on integers are desired, we can, of course, represent an integer as a floating point number; and when the floating point operations satisfy the basic definitions in  9 , we know that all floating point operations will be exact, provided that the operands and the answer can each be represented exactly with p significant digits. Therefore — so long as we know that the numbers aren’t too large — we can add, subtract, or multiply integers with no inaccuracy due to rounding errors. But suppose that a programmer wants to determine if m is an exact multiple of n, when m and n ̸= 0 are integers. Suppose further that a subroutine is available to  calculate the quantity round u mod 1  = u  cid:88 mod 1 for any given floating point num- multiple of n might be to test whether or not  m ⊘ n   cid:88 mod 1 = 0, using the assumed is a multiple of n if and only if  m ⊘ n   cid:88 mod 1 = 0. In other words, show that if m  subroutine; but perhaps rounding errors in the floating point calculations will invalidate this test in certain cases. Find suitable conditions on the range of integer values n ̸= 0 and m, such that m  ber u, as in exercise 4.2.1–15. One good way to determine whether or not m is a  and n are not too large, this test is valid.   244  ARITHMETIC  4.2.2  14. [M27] Find a suitable ϵ such that  u⊗v ⊗w ≈ u⊗ v⊗w   ϵ , when unnormalized multiplication is being used.  This generalizes  39 , since unnormalized multiplication is exactly the same as normalized multiplication when the input operands u, v, and w are normalized.    cid:120  15. [M24]  H. Björk.  Does the computed midpoint of an interval always lie between  the endpoints?  In other words, does u ≤ v imply that u ≤  u ⊕ v  ⊘ 2 ≤ v?  16. [M28]  a  What is  ···   x1⊕x2 ⊕x3 ⊕···⊕xn  when n = 106 and xk = 1.1111111 for all k, using eight-digit floating decimal arithmetic?  b  What happens when Eq.  14  is used to calculate the standard deviation of these particular values xk? What happens when Eqs.  15  and  16  are used instead?  c  Prove that Sk ≥ 0 in  16 , for all choices of x1, . . . , xk. 17. [28] Write a MIX subroutine, FCMP, that compares the floating point number u in location ACC with the floating point number v in register A, setting the comparison indicator to LESS, EQUAL, or GREATER according as u ≺ v, u ∼ v, or u ≻ v  ϵ ; here ϵ is stored in location EPSILON as a nonnegative fixed point quantity with the radix point assumed at the left of the word. Assume normalized inputs. 18. [M40] In unnormalized arithmetic is there a suitable number ϵ such that   cid:120  19. [M30]  W. M. Kahan.  Consider the following procedure for floating point sum-  u ⊗  v ⊕ w  ≈  u ⊗ v  ⊕  u ⊗ w    ϵ  ?  mation of x1, x2, . . . , xn:  s0 = c0 = 0; yk = xk ⊖ ck−1,  sk = sk−1 ⊕ yk,  ck =  sk ⊖ sk−1  ⊖ yk,  for 1 ≤ k ≤ n.  Let the relative errors in these operations be defined by the equations sk =  sk−1 + yk  1 + σk ,  yk =  xk − ck−1  1 + ηk ,  ck =   sk − sk−1  1 + γk  − yk  1 + δk ,  where ηk,σk,γk,δk ≤ ϵ. Prove that sn − cn = n k=1 1 + θk xk, where θk ≤ 2ϵ+O nϵ2 . [Theorem C says that if b = 2 and sk−1 ≥ yk we have sk−1+yk = sk−ck exactly. But in this exercise we want to obtain an estimate that is valid even when floating point operations are not carefully rounded, assuming only that each operation has bounded relative error.] 20. [25]  S. Linnainmaa.  Find all u and v for which u ≥ v and  47  fails. 21. [M35]  T. J. Dekker.  Theorem C shows how to do exact addition of floating binary numbers. Explain how to do exact multiplication: Express the product uv in the form w + w′, where w and w′ are computed from two given floating binary numbers u and v, using only the operations ⊕, ⊖, and ⊗. 22. [M30] Can drift occur in floating point multiplication division? Consider the sequence x0 = u, x2n+1 = x2n ⊗ v, x2n+2 = x2n+1 ⊘ v, given u and v ̸= 0; what is the largest subscript k such that xk ̸= xk+2 is possible?   cid:120  23. [M26] Prove or disprove: u ⊖  u  cid:88 mod 1  = ⌊u⌋, for all floating point u.  24. [M27] Consider the set of all intervals [ul . . ur], where ul and ur are either nonzero floating point numbers or the special symbols +0, −0, +∞, −∞; each interval must   4.2.2  ACCURACY OF FLOATING POINT ARITHMETIC  245  have ul ≤ ur, and ul = ur is allowed only when ul is finite and nonzero. The interval [ul . . ur] stands for all floating point x such that ul ≤ x ≤ ur, where we agree that  −∞ < −x < −0 < 0 < +0 < +x < +∞   Thus, [1 . . 2] means 1 ≤ x ≤ 2;  [+0 . . 1] means 0 < x ≤ 1; for all positive x. [−0 . . 1] means 0 ≤ x ≤ 1; [−0 . . +0] denotes the single value 0; and [−∞ . . +∞] stands for everything.  Show how to define appropriate arithmetic operations on all such intervals, without resorting to overflow or underflow or other anomalous indications except when dividing by an interval that includes zero.   cid:120  25. [15] When people speak about inaccuracy in floating point arithmetic they often  ascribe errors to “cancellation” that occurs during the subtraction of nearly equal quantities. But when u and v are approximately equal, the difference u⊖ v is obtained exactly, with no error. What do these people really mean? 26. [M21] Given that u, u′, v, and v′ are positive floating point numbers with u ∼ u′  ϵ  and v ∼ v′  ϵ , prove that there’s a small ϵ′ such that u ⊕ v ∼ u′ ⊕ v′  ϵ′ , assuming normalized arithmetic. 27. [M27]  W. M. Kahan.  Prove that 1 ⊘  1 ⊘  1 ⊘ u   = 1 ⊘ u for all u ̸= 0. 28. [HM30]  H. G. Diamond.  Suppose f x  is a strictly increasing function on some interval [x0 . . x1], and let g x  be the inverse function.  For example, f and g might be “exp” and “ln”, or “tan” and “arctan”.  If x is a floating point number such that x0 ≤ x ≤ x1, let ˆf x  = round f x  , and if y is another such that f x0  ≤ y ≤ f x1 , let ˆg y  = round g y  ; furthermore, let h x  = ˆg  ˆf x  , whenever this is defined. Although h x  won’t always be equal to x, due to rounding, we expect h x  to be fairly near x.  Prove that if the precision bp is at least 3, and if f is strictly concave or strictly convex  that is, f′′ x  has the same sign for all x in [x0 . . x1] , then repeated application of h will be stable in the sense that  h h h x    = h h x  ,  for all x such that both sides of this equation are defined. In other words, there will be no “drift” if the subroutines are properly implemented.   cid:120  29. [M25] Give an example to show that the condition bp ≥ 3 is necessary in the  cid:120  30. [M30]  W. M. Kahan.  Let f x  = 1 + x + ··· + x106 =  1 − x107   1 − x  for  previous exercise.  3 − y2  3 + 3.45y2   for 0 < y < 1. Evaluate g y  on one or x < 1, and let g y  = f   1 more pocket calculators, for y = 10−3, 10−4, 10−5, 10−6, and explain all inaccuracies in the results you obtain.  Since most present-day calculators do not round correctly, the results are often surprising. Note that g ϵ  = 107 − 10491.35ϵ2 + 659749.9625ϵ4 − 30141386.26625ϵ6 + O ϵ8 .  31. [M25]  U. Kulisch.  When the polynomial 2y2 + 9x4 − y4 is evaluated for x = 408855776 and y = 708158977 using standard 53-bit double-precision floating point arithmetic, the result is ≈ −3.7 × 1019. Evaluating it in the alternative form 2y2 +  3x2 − y2  3x2 + y2  gives ≈ +1.0 × 1018. The true answer, however, is 1.0  exactly . Explain how to construct similar examples of numerical instability. 32. [M21] For what pairs  a, b  is round to even x  = ⌊ax + b⌋ + ⌈ax − b⌉ for all x?   246  ARITHMETIC  4.2.3  *4.2.3. Double-Precision Calculations Up to now we have considered “single-precision” floating point arithmetic, which essentially means that the floating point values we have dealt with can be stored in a single machine word. When single-precision floating point arithmetic does not yield sufficient accuracy for a given application, the precision can be increased by suitable programming techniques that use two or more words of memory to represent each number.  Although we shall discuss the general question of high-precision calculations in Section 4.3, it is appropriate to give a separate discussion of double-precision here. Special techniques apply to double precision that are comparatively inap- propriate for higher precisions; and double precision is a reasonably important topic in its own right, since it is the first step beyond single precision and it is applicable to many problems that do not require extremely high precision.  Well, that paragraph was true when the author wrote the first edition of this book in the 1960s. But computers have evolved in such a way that the old motivations for double-precision floating point have mostly disappeared; the present section is therefore primarily of historical interest. In the planned fourth edition of this book, Section 4.2.1 will be renamed “Normalized Calculations,” and the present Section 4.2.3 will be replaced by a discussion of “Exceptional Numbers.” The new material will focus on special aspects of ANSI IEEE Stan- dard 754: subnormal numbers, infinities, and the so-called NaNs that represent undefined or otherwise unusual quantities.  See the references at the end of Section 4.2.1.  Meanwhile, let us take one last look at the older ideas, in order to see what lessons they can still teach us.  Double-precision calculations are almost always required for floating point rather than fixed point arithmetic, except perhaps in statistical work where fixed point double-precision is commonly used to calculate sums of squares and cross products; since fixed point versions of double-precision arithmetic are simpler than floating point versions, we shall confine our discussion here to the latter.  Double precision is quite frequently desired not only to extend the precision of the fraction parts of floating point numbers, but also to increase the range of the exponent part. Thus we shall deal in this section with the following two-word format for double-precision floating point numbers in the MIX computer:  ± e  e  f  f  f  f  f  f  f  f  .   1   Here two bytes are used for the exponent and eight bytes are used for the fraction. The exponent is “excess b2 2,” where b is the byte size. The sign will appear in the most significant word; it is convenient to ignore the sign of the other word completely.  Our discussion of double-precision arithmetic will be quite machine-oriented, because it is only by studying the problems involved in coding these routines that a person can properly appreciate the subject. A careful study of the MIX programs below is therefore essential to the understanding of the material.   4.2.3  DOUBLE-PRECISION CALCULATIONS  247  In this section we shall depart from the idealistic goals of accuracy stated in the previous two sections; our double-precision routines will not round their results, and a little bit of error will sometimes be allowed to creep in. Users dare not trust these routines too much. There was ample reason to squeeze out every possible drop of accuracy in the single-precision case, but now we face a different situation:  a  The extra programming required to ensure true double- precision rounding in all cases is considerable; fully accurate routines would take, say, twice as much space and half again as much time. It was comparatively easy to make our single-precision routines perfect, but double precision brings us face to face with our machine’s limitations. A similar situation occurs with respect to other floating point subroutines; we can’t expect the cosine routine to compute round cos x  exactly for all x, since that turns out to be virtually impossible. Instead, the cosine routine should provide the best relative error it can achieve with reasonable speed, for all reasonable values of x. Of course, the designer of the routine should try to make the computed function satisfy simple mathematical laws whenever possible — for example,    cid:88 cos x ≤ 1;  cid:88 cos x ≥  cid:88 cos y for 0 ≤ x ≤ y < π.   cid:88 cos  −x  =  cid:88 cos x;   b  Single-precision arithmetic is a “staple food” that everybody who wants to employ floating point arithmetic must use, but double precision is usually for situations where such clean results aren’t as important. The difference between seven- and eight-place accuracy can be noticeable, but we rarely care about the difference between 15- and 16-place accuracy. Double precision is most often used for intermediate steps during the calculation of single-precision results; its full potential isn’t needed.  c  It will be instructive for us to analyze these procedures in order to see how inaccurate they can be, since they typify the types of short cuts generally taken in bad single-precision routines  see exercises 7 and 8 .  Let us now consider addition and subtraction operations from this stand- point. Subtraction is, of course, converted to addition by changing the sign of the second operand. Addition is performed by separately adding together the least-significant halves and the most-significant halves, propagating “carries” appropriately.  A difficulty arises, however, since we are doing signed magnitude arithmetic: it is possible to add the least-significant halves and to get the wrong sign  namely, when the signs of the operands are opposite and the least-significant half of the smaller operand is bigger than the least-significant half of the larger operand . The simplest solution is to anticipate the correct sign; so in step A2 of Algorithm 4.2.1A we will now assume not only that eu ≥ ev but also that u ≥ v. Then we can be sure that the final sign will be the sign of u. In other respects, double- precision addition is very much like its single-precision counterpart, except that everything needs to be done twice. Program A  Double-precision addition . The subroutine DFADD adds a double- precision floating point number v, having the form  1 , to a double-precision   248  ARITHMETIC  4.2.3  1F 2F  floating point number u, assuming that v is initially in rAX  registers A and X , and that u is initially stored in locations ACC and ACCX. The answer appears both in rAX and in  ACC, ACCX . The subroutine DFSUB subtracts v from u under the same conventions.  If v > u, interchange u ↔ v.  Field definition for absolute value Field definition for sign Double-precision exponent field Double-precision subtraction: Change sign of v. Double-precision addition: Compare v with u.  Both input operands are assumed to be normalized, and the answer is normalized. The last portion of this program is a double-precision normalization procedure that is used by other subroutines of this section. Exercise 5 shows how to improve the program significantly. EQU 1:5 01 ABS EQU 0:0 02 SIGN 03 EXPD EQU 1:2 04 DFSUB STA TEMP LDAN TEMP 05 06 DFADD STJ EXITDF 07 08 09 10 11 12 1H 13 14 15 16 17 18 2H 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 1H 39 40  CMPA ACC ABS  JG JL CMPX ACCX ABS  JLE 2F STA ARG STX ARGX LDA ACC LDX ACCX ENT1 ACC MOVE ARG 2  STA TEMP LD1N TEMP EXPD  rI1 ← −ev. rI2 ← eu. LD2 ACC EXPD  rI1 ← eu − ev. INC1 0,2 SLAX 2 Remove exponent. SRAX 1,1 Scale right. STA ARG 0 v1 v2 v3 v4 STX ARGX STA ARGX SIGN  Store true sign of v in both halves. LDA ACC LDX ACCX SLAX 2 STA ACC SLAX 4 ENTX 1 STX EXPO SRC 1 STA 1F SIGN  ADD ARGX 0:4  SRAX 4 DECA 1 ADD ACC 0:4  ADD ARG   We know that u has the sign of the answer.  rAX ← u. Remove exponent. u1 u2 u3 u4 u5  EXPO ← 1  see below . 1 u5 u6 u7 u8 A trick, see comments in text. Add 0 v5 v6 v7 v8.  Recover from inserted 1.  Sign varies  Add most significant halves.  Overflow cannot occur    ACC and ACCX are in consecutive  locations.   v5 v6 v7 v8 v9   4.2.3  DOUBLE-PRECISION CALCULATIONS  249  41 DNORM JANZ 1F JXNZ 1F 42 43 DZERO STA ACC JMP 9F 44 SLAX 1 45 2H DEC2 1 46 CMPA =0= 1:1  47 1H JE 2B 48 SRAX 2 49 STA ACC 50 LDA EXPO 51 INCA 0,2 52 JAN EXPUND 53 STA ACC EXPD  54 CMPA =1 3:3 = 55 8F JL 56 57 EXPOVD HLT 20 58 EXPUND HLT 10 59 8H LDA ACC 60 9H STX ACCX 61 EXITDF JMP * CON 0 62 ARG CON 0 63 ARGX 64 ACC CON 0 CON 0 65 ACCX 66 EXPO CON 0  Normalization routine: fw in rAX, ew = EXPO + rI2. If fw = 0, set ew ← 0.  Normalize to the left.  Is the leading byte zero?   Rounding omitted   Compute final exponent.  Is it negative?  Is it more than two bytes?  Bring answer into rA.  Exit from subroutine.  Floating point accumulator  Part of “raw exponent”  When the least-significant halves are added together in this program, an extra digit “1” is inserted at the left of the word that is known to have the correct sign. After the addition, this byte can be 0, 1, or 2, depending on the circumstances, and all three cases are handled simultaneously in this way.  Compare this with the rather cumbersome method of complementation that is used in Program 4.2.1A.   It is worth noting that register A can be zero after the instruction on line 40 has been performed; and, because of the way MIX defines the sign of a zero result, the accumulator contains the correct sign that is to be attached to the result if register X is nonzero. If lines 39 and 40 were interchanged, the program would be incorrect, even though both instructions are ‘ADD’!  Now let us consider double-precision multiplication. The product has four components, shown schematically in Fig. 4. Since we need only the leftmost eight bytes, it is convenient to ignore the digits to the right of the vertical line in the diagram; in particular, we need not even compute the product of the two least-significant halves. Program M  Double-precision multiplication . The input and output conven- tions for this subroutine are the same as for Program A. 01 BYTE EQU 1 4:4  02 QQ  EQU BYTE*BYTE 2 Excess of double-precision exponent  Byte size   250  ARITHMETIC  4.2.3  u u u u u v v v v v x x x x x x x x 0 0 x x x 0 0  u u u 0 0 = um + ϵul v v v 0 0 = vm + ϵvl x 0 0 0 0 = ϵ2ul × vl = ϵ um × vl = ϵ ul × vm = um × vm  x x x x x x x x x x x x x x x  x x x x x w w w w w w w w w w w w w w w w 0 0 0 0 Fig. 4. Double-precision multiplication of eight-byte fraction parts.  Double-precision multiplication:  03 DFMUL STJ EXITDF 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  STA TEMP SLAX 2 STA ARG STX ARGX LDA TEMP EXPD  ADD ACC EXPD  STA EXPO ENT2 -QQ LDA ACC LDX ACCX SLAX 2 STA ACC STX ACCX MUL ARGX STA TEMP LDA ARG ABS  MUL ACCX ABS  SRA 1 ADD TEMP 1:4  STA TEMP LDA ARG MUL ACC STA TEMP SIGN  STA ACC STX ACCX LDA ACCX 0:4  ADD TEMP SRAX 4 ADD ACC JMP DNORM  Remove exponent. vm vl  EXPO ← eu + ev. rI2 ← −QQ.  Remove exponent. um ul um × vl  vm × ul 0 x x x x  Overflow cannot occur   vm × um Store true sign of result. Now prepare to add all the  partial products together.  0 x x x x  Overflow cannot occur    Overflow cannot occur  Normalize and exit.  Notice the careful treatment of signs in this program, and note also the fact that the range of exponents makes it impossible to compute the final exponent using an index register. Program M is perhaps too slipshod in accuracy, since it uses only the information to the left of the vertical line in Fig. 4; this can make the least significant byte as much as 2 in error. A little more accuracy can be achieved as discussed in exercise 4.   4.2.3  DOUBLE-PRECISION CALCULATIONS  251  Double-precision floating division is the most difficult routine, or at least the most frightening prospect we have encountered so far in this chapter. Actually, it is not terribly complicated, once we see how to do it; let us write the numbers to be divided in the form  um + ϵul   vm + ϵvl , where ϵ is the reciprocal of the word size of the computer, and where vm is assumed to be normalized. The fraction can now be expanded as follows:     um + ϵul vm + ϵvl  = um + ϵul = um + ϵul  vm  1   vl  1 + ϵ vl vm  1 − ϵ     + ϵ2 vl  2 − ···    .  vm  vm  vm   2  Since 0 ≤ vl < 1 and 1 b ≤ vm < 1, we have vl vm < b, and the error from dropping terms involving ϵ2 can be disregarded. Our method therefore is to compute wm + ϵwl =  um + ϵul  vm, and then to subtract ϵ times wmvl vm from the result.  Double-precision division: Ensure that overflow is off.  Remove exponent. vm vl  In the following program, lines 27–32 do the lower half of a double-precision addition, using another method for forcing the appropriate sign as an alternative to the trick of Program A. Program D  Double-precision division . This program adheres to the same conventions as Programs A and M. 01 DFDIV STJ EXITDF 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  JOV OFLO STA TEMP SLAX 2 STA ARG STX ARGX LDA ACC EXPD  SUB TEMP EXPD  STA EXPO ENT2 QQ+1 LDA ACC LDX ACCX SLAX 2 SRAX 1 DIV ARG STA ACC SLAX 5 DIV ARG STA ACCX LDA ARGX 1:4  ENTX 0 DIV ARG ABS  JOV DVZROD MUL ACC ABS  SRAX 4 SLC 5  Remove exponent.  See Algorithm 4.2.1M  If overflow, it is detected below. wm Use remainder in further division. ±wl  rA ← ⌊b4vl vm⌋ b5. Did division cause overflow? rAX ← wmvl bvm, approximately. Multiply by b, and save  EXPO ← eu − ev. rI2 ← QQ + 1.  the leading byte in rX.   252  ARITHMETIC  SUB ACCX ABS  DECA 1 SUB WM1 JOV *+2 INCX 1 SLC 5 ADD ACC ABS  STA ACC ABS  LDA ACC JMP DNORM  27 28 29 30 31 32 33 34 35 36 37 DVZROD HLT 30 38 1H 39 WM1  4.2.3  Subtract wl. Force minus sign.  If no overflow, carry one more to upper half.  Now rA ≤ 0  rA ← wm − rA.  Now rA ≥ 0  rA ← wm with correct sign. Normalize and exit. Unnormalized or zero divisor  EQU 1 1:1  CON 1B-1,BYTE-1 1:1  Word size minus one  Here is a table of the approximate average computation times for these double-precision subroutines, compared to the single-precision subroutines that appear in Section 4.2.1:  Single precision  Double precision  Addition Subtraction Multiplication Division  45.5u 49.5u 48u 52u  84u 88u 109u 126.5u  For extension of the methods of this section to triple-precision floating point  fraction parts, see Y. Ikebe, CACM 8  1965 , 175–177.  EXERCISES 1. [16] Try the double-precision division technique by hand, with ϵ = 1 1000, when di- viding 180000 by 314159.  Thus, let  um, ul  =  .180, .000  and  vm, vl  =  .314, .159 , and find the quotient using the method suggested in the text following  2 .  2. [20] Would it be a good idea to insert the instruction ‘ENTX 0’ between lines 30 and 31 of Program M, in order to keep unwanted information left over in register X from interfering with the accuracy of the results? 3. [M20] Explain why overflow cannot occur during Program M. 4. [22] How should Program M be changed so that extra accuracy is achieved, essentially by moving the vertical line in Fig. 4 over to the right one position? Specify all changes that are required, and determine the difference in execution time caused by these changes.   cid:120  5. [24] How should Program A be changed so that extra accuracy is achieved, essen-  tially by working with a nine-byte accumulator instead of an eight-byte accumulator to the right of the radix point? Specify all changes that are required, and determine the difference in execution time caused by these changes. 6. [23] Assume that the double-precision subroutines of this section and the single- precision subroutines of Section 4.2.1 are being used in the same main program. Write a subroutine that converts a single-precision floating point number into double-precision form  1 , and write another subroutine that converts a double-precision floating point   4.2.4  DISTRIBUTION OF FLOATING POINT NUMBERS  253  number into single-precision form  reporting exponent overflow or underflow if the conversion is impossible .   cid:120  7. [M30] Estimate the accuracy of the double-precision subroutines in this section,  by finding bounds δ1, δ2, and δ3 on the relative errors    u ⊕ v  −  u + v    u + v  ,    u ⊗ v  −  u × v    u × v  ,    u ⊘ v  −  u v    u v  .  8. [M28] Estimate the accuracy of the “improved” double-precision subroutines of exercises 4 and 5, in the sense of exercise 7. 9. [M42] T. J. Dekker [Numer. Math. 18  1971 , 224–242] has suggested an alter- native approach to double precision, based entirely on single-precision floating binary calculations. For example, Theorem 4.2.2C states that u + v = w + r, where w = u⊕ v and r =  u ⊖ w  ⊕ v, if u ≥ v and the radix is 2; here r ≤ w 2p, so the pair  w, r  may be considered a double-precision version of u + v. To add two such pairs  u, u′  ⊕  v, v′ , where u′ ≤ u 2p and v′ ≤ v 2p and u ≥ v, Dekker suggests computing u + v = w + r  exactly , then s =  r ⊕ v′ ⊕ u′  an approximate remainder , and finally returning the value  w ⊕ s,  w ⊖  w ⊕ s   ⊕ s .  Study the accuracy and efficiency of this approach when it is used recursively to  produce quadruple-precision calculations.  4.2.4. Distribution of Floating Point Numbers In order to analyze the average behavior of floating point arithmetic algorithms  and in particular to determine their average running time , we need some statistical information that allows us to determine how often various cases arise. The purpose of this section is to discuss the empirical and theoretical properties of the distribution of floating point numbers. A. Addition and subtraction routines. The execution time for a floating point addition or subtraction depends largely on the initial difference of expo- nents, and also on the number of normalization steps required  to the left or to the right . No way is known to give a good theoretical model that tells what characteristics to expect, but extensive empirical investigations have been made by D. W. Sweeney [IBM Systems J. 4  1965 , 31–42].  By means of a special tracing routine, Sweeney ran six “typical” large-scale numerical programs, selected from several different computing laboratories, and examined each floating addition or subtraction operation very carefully. Over 250,000 floating point addition-subtractions were involved in gathering this data. About one out of every ten instructions executed by the tested programs was either FADD or FSUB.  Subtraction is the same as addition preceded by negating the second operand, so we can give all the statistics as if we were merely doing addition. Sweeney’s results can be summarized as follows:  One of the two operands to be added was found to be equal to zero about 9 percent of the time, and this was usually the accumulator  ACC . The other 91 percent of the cases split about equally between operands of the same or of   254  ARITHMETIC  4.2.4  EMPIRICAL DATA FOR OPERAND ALIGNMENTS BEFORE ADDITION  eu − ev  0 1 2 3 4 5  over 5 average  b = 2 0.33 0.12 0.09 0.07 0.07 0.04 0.28 3.1  Table 1  b = 10 0.47 0.23 0.11 0.03 0.01 0.01 0.13 0.9  Table 2  b = 16 0.47 0.26 0.10 0.02 0.01 0.02 0.11 0.8  b = 64 0.56 0.27 0.04 0.02 0.02 0.00 0.09 0.5  EMPIRICAL DATA FOR NORMALIZATION AFTER ADDITION  Shift right 1 No shift Shift left 1 Shift left 2 Shift left 3 Shift left 4 Shift left > 4  b = 2 0.20 0.59 0.07 0.03 0.02 0.02 0.06  b = 10 0.07 0.80 0.08 0.02 0.00 0.01 0.02  b = 16 0.06 0.82 0.07 0.01 0.01 0.00 0.02  b = 64 0.03 0.87 0.06 0.01 0.00 0.01 0.02  opposite signs, and about equally between cases where u ≤ v or v ≤ u. The computed answer was zero about 1.4 percent of the time.  The difference between exponents had a behavior approximately given by the probabilities shown in Table 1, for various radices b.  The “over 5” line of that table includes essentially all of the cases when one operand was zero, but the “average” line does not include these cases.   When u and v have the same sign and are normalized, then u + v either requires one shift to the right  for fraction overflow , or no normalization shifts whatever. When u and v have opposite signs, we have zero or more left shifts during the normalization. Table 2 gives the observed number of shifts required; the last line of that table includes all cases where the result was zero. The average number of left shifts per normalization was about 0.9 when b = 2; about 0.2 when b = 10 or 16; and about 0.1 when b = 64. B. The fraction parts. Further analysis of floating point routines can be based on the statistical distribution of the fraction parts of randomly chosen normalized floating point numbers. The facts are quite surprising, and there is an interesting theory that accounts for the unusual phenomena that are observed.  For convenience let us assume temporarily that we are dealing with floating decimal arithmetic  radix 10 ; modifications of the following discussion to any other positive integer base b will be very straightforward. Suppose we are given a “random” positive normalized number  e, f  = 10e · f. Since f is normalized, we know that its leading digit is 1, 2, 3, 4, 5, 6, 7, 8, or 9, and we might naturally   4.2.4  DISTRIBUTION OF FLOATING POINT NUMBERS  255  expect each of these nine possible leading digits to occur about one-ninth of the time. But, in fact, the behavior in practice is quite different. For example, the leading digit tends to be equal to 1 more than 30 percent of the time!  One way to test the assertion just made is to take a table of physical con- stants  like the speed of light or the acceleration of gravity  from some standard reference. If we look at the Handbook of Mathematical Functions  U.S. Dept of Commerce, 1964 , for example, we find that 8 of the 28 different physical con- stants given in Table 2.3, roughly 29 percent, have leading digit equal to 1. The decimal values of n! for 1 ≤ n ≤ 100 include exactly 30 entries beginning with 1; so do the decimal values of 2n and of Fn, for 1 ≤ n ≤ 100. We might also try look- ing at census reports, or a Farmer’s Almanack  but not a telephone directory . In the days before pocket calculators, the pages in well-used tables of loga- rithms tended to get quite dirty in the front, while the last pages stayed relatively clean and neat. This phenomenon was apparently first mentioned in print by the astronomer Simon Newcomb [Amer. J. Math. 4  1881 , 39–40], who gave good grounds for believing that the leading digit d occurs with probability log10 1 + 1 d . The same distribution was discovered empirically, many years later, by Frank Benford, who reported the results of 20,229 observations taken from many different sources [Proc. Amer. Philosophical Soc. 78  1938 , 551–572]. In order to account for this leading-digit law, let’s take a closer look at the way we write numbers in floating point notation. If we take any positive number u, its fraction part is determined by the formula 10fu = 10 log10 u  mod 1; hence its leading digit is less than d if and only if  √   log10 u  mod 1 < log10 d.   1  Now if we have a “random” positive number U, chosen from some reasonable distribution that might occur in nature, we might expect that  log10 U  mod 1 would be uniformly distributed between zero and one, at least to a very good approximation.  Similarly, we expect U mod 1, U 2 mod 1, U + π mod 1, etc., to be uniformly distributed. We expect a roulette wheel to be unbiased, for essen- tially the same reason.  Therefore by  1  the leading digit will be 1 with probabil- ity log10 2 ≈ 30.103 percent; it will be 2 with probability log10 3−log10 2 ≈ 17.609 percent; and, in general, if r is any real value between 1 and 10, we ought to have 10fU ≤ r approximately log10 r of the time. The fact that leading digits tend to be small makes the most obvious tech- niques of “average error” estimation for floating point calculations invalid. The relative error due to rounding is usually a little more than expected.  Of course, it may justly be said that the heuristic argument above does not prove the stated law. It merely shows us a plausible reason why the leading digits behave the way they do. An interesting approach to the analysis of leading digits has been suggested by R. Hamming: Let p r  be the probability that 10fU ≤ r, where 1 ≤ r ≤ 10 and fU is the normalized fraction part of a random normalized floating point number U. If we think of random quantities in the real world, we observe that they are measured in terms of arbitrary units; and if we were to change the definition of a meter or a gram, many of the fundamental   if c ≤ r;  if c ≥ r;  256  ARITHMETIC  4.2.4  physical constants would have different values. Suppose then that all of the numbers in the universe are suddenly multiplied by a constant factor c; our universe of random floating point quantities should be essentially unchanged by this transformation, so p r  should not be affected.  Multiplying everything by c has the effect of transforming  log10 U  mod 1 into  log10 U + log10 c  mod 1. It is now time to set up formulas that describe the desired behavior; we may assume that 1 ≤ c ≤ 10. By definition,  By our assumption, we should also have  p r  = Pr log10 U  mod 1 ≤ log10 r. p r  = Pr log10 U + log10 c  mod 1 ≤ log10 r Pr log10 U mod 1  ≤ log10 r − log10 c  or  log10 U mod 1  ≥ 1 − log10 c, Pr log10 U mod 1  ≤ log10 r + 1 − log10 c and  log10 U mod 1  ≥ 1 − log10 c,  p r c  + 1 − p 10 c ,  =  if c ≤ r; if c ≥ r.  =  p 10r c  − p 10 c ,   2  Let us now extend the function p r  to values outside the range 1 ≤ r ≤ 10, by defining p 10nr  = p r + n; then if we replace 10 c by d, the last equation of  2  may be written   3  If our assumption about invariance of the distribution under multiplication by a constant factor is valid, then Eq.  3  must hold for all r > 0 and 1 ≤ d ≤ 10. The facts that p 1  = 0 and p 10  = 1 now imply that  p rd  = p r  + p d .  10 n = p  n√  10  + p  n√  10 n−1 = ··· = np  n√  1 = p 10  = p  n√  10 ;  hence we deduce that p 10m n  = m n for all positive integers m and n. If we now decide to require that p is continuous, we are forced to conclude that p r  = log10 r, and this is the desired law.  Although this argument may be more convincing than the first one, it doesn’t really hold up under scrutiny if we stick to conventional notions of probability. The traditional way to make the argument above rigorous is to assume that there is some underlying distribution of numbers F u  such that a given positive number U is ≤ u with probability F u ; then the probability of concern to us is  4   F 10mr  − F 10m ,  p r  =  summed over all values −∞ < m < ∞. Our assumptions about scale invariance and continuity have led us to conclude that  m  p r  = log10 r.   4.2.4  DISTRIBUTION OF FLOATING POINT NUMBERS  Using the same argument, we could “prove” that  F bmr  − F bm  = logb r,    m  257   5   for each integer b ≥ 2, when 1 ≤ r ≤ b. But there is no distribution function F that satisfies this equation for all such b and r!  See exercise 7.   One way out of the difficulty is to regard the logarithm law p r  = log10 r as only a very close approximation to the true distribution. The true distribution itself may perhaps be changing as the universe expands, becoming a better and better approximation as time goes on; and if we replace 10 by an arbitrary base b, the approximation might be less accurate  at any given time  as b gets larger. Another rather appealing way to resolve the dilemma, by abandoning the traditional idea of a distribution function, has been suggested by R. A. Raimi, AMM 76  1969 , 342–348.  The hedging in the last paragraph is probably a very unsatisfactory ex- planation, and so the following further calculation  which sticks to rigorous mathematics and avoids any intuitive, yet paradoxical, notions of probability  should be welcome. Let us consider the distribution of the leading digits of the positive integers, instead of the distribution for some imagined set of real numbers. The investigation of this topic is quite interesting, not only because it sheds some light on the probability distributions of floating point data, but also because it makes a particularly instructive example of how to combine the methods of discrete mathematics with the methods of infinitesimal calculus. In the following discussion, let r be a fixed real number, 1 ≤ r ≤ 10; we will attempt to make a reasonable definition of p r , the “probability” that the representation 10eN·fN of a “random” positive integer N has 10fN < r, assuming infinite precision. To start, let us try to find the probability using a limiting method like the definition of “Pr” in Section 3.5. One nice way to rephrase that definition is to define  P0 n  =n = 10e · f where 10f < r = log10 n  mod 1 < log10 r.   6  Now P0 1 , P0 2 , . . . is an infinite sequence of zeros and ones, with ones to represent the cases that contribute to the probability we are seeking. We can try to “average out” this sequence, by defining  n  k=1  P1 n  = 1  n  P0 k .   7   Thus if we generate a random integer between 1 and n using the techniques of Chapter 3, and convert it to floating decimal form  e, f , the probability that 10f < r is exactly P1 n . It is natural to let limn→∞ P1 n  be the “probability” p r  we are after, and that is just what we did in Definition 3.5A.  But in this case the limit does not exist. For example, let us consider the  subsequence  P1 s , P1 10s , P1 100s , . . . , P1 10ns , . . . ,   4.2.4  ARITHMETIC  258 where s is a real number, 1 ≤ s ≤ 10. If s ≤ r, we find that P1 10ns  = 1 10ns = 1 10ns = 1 10ns  ⌈r⌉−1+⌈10r⌉−10+···+⌈10n−1r⌉−10n−1 +⌊10ns⌋+1−10n r 1+10+···+10n−1 + O n +⌊10ns⌋−1−10−···−10n  1 9 10nr−10n+1 +⌊10ns⌋+ O n .   8  As n → ∞, P1 10ns  therefore approaches the limiting value 1+ r−10  9s. The same calculation is valid for the case s > r if we replace ⌊10ns⌋ + 1 by ⌈10nr⌉; thus we obtain the limiting value 10 r − 1  9s when s ≥ r. [See J. Franel, Naturforschende Gesellschaft, Vierteljahrsschrift 62  Zürich: 1917 , 286–295.] In other words, the sequence ⟨P1 n ⟩ has subsequences ⟨P1 10ns ⟩ whose limit goes from  r − 1  9 up to 10 r − 1  9r and down again to  r − 1  9, as s goes from 1 to r to 10. We see that P1 n  has no limit as n → ∞; and the values of P1 n  for large n are not particularly good approximations to our conjectured limit log10 r either!  Since P1 n  doesn’t approach a limit, we can try to use the same idea as  7   once again, to “average out” the anomalous behavior. In general, let  Pm+1 n  = 1  n  Pm k .  n  k=1  Then Pm+1 n  will tend to be a more well-behaved sequence than Pm n . Let us try to confirm this with quantitative calculations; our experience with the special case m = 0 indicates that it might be worthwhile to consider the subsequence Pm+1 10ns . The following results can, in fact, be derived: Lemma Q. For any integer m ≥ 1 and any real number ϵ > 0, there are functions Qm s , Rm s  and an integer Nm ϵ , such that whenever n > Nm ϵ  and 1 ≤ s ≤ 10, we have  Pm 10ns  − Qm s  − Rm s [s > r] < ϵ.  Furthermore the functions Qm s  and Rm s  satisfy the relations  Qm−1 t  dt +  Qm−1 t  dt + 1 9  Rm−1 t  dt  ;   10  r   10  1  1  s  9  r  s  Qm s  = 1 Rm s  = 1 s Q0 s  = 1,  Rm−1 t  dt; R0 s  = −1.   s  1  Proof. Consider the functions Qm s  and Rm s  defined by  11 , and let  Sm t  = Qm t  + Rm t [t > r].  We will prove the lemma by induction on m.   9    10      11    12    ,  s  1  1  1  10j  259  4.2.4  0≤j<n  10n−j  10j≤k≤10j q  10j≤k≤10j q    10j≤k<10j+1  10n≤k≤10ns    1 10j Sm−1  Pm 10ns  = 1  Now for m > 1, we have  DISTRIBUTION OF FLOATING POINT NUMBERS  and we want to approximate this quantity. By induction, the difference  is less than qϵ when 1 ≤ q ≤ 10 and j > Nm−1 ϵ . Since Sm−1 t  is continuous, it is a Riemann-integrable function; and the difference  R1 s  =  r − s  s. From  8  we find that P1 10ns  − S1 s  = O n  10n; this establishes the lemma when m = 1.  First note that Q1 s  =1 +  s − 1  −  10 − r  9 s = 1 +  r − 10  9s, and  1 10n Pm−1 k   10j Pm−1 k +   k       10j Pm−1 k  −     −  k Pm 10ns  − 1    10 is bounded by N j=0 M 10n−j  +  Pm 10ns  − 1 0≤j<n 1 10n−j , which appears in  15 , is equal to  1 − 1 10n  9; so  10  1 N <j<n 11ϵ 10n−j  + 11ϵ, if M is an upper bound for  13  +  14  that is valid for all positive integers j. Finally, the sum  is less than ϵ for all j greater than some number N, independent of q, by the definition of integration. We may choose N to be > Nm−1 ϵ . Therefore for n > N, the difference  1 10j Sm−1  Sm−1 t  dt +  Sm−1 t  dt +      Sm−1 t  dt  Sm−1 t  dt  Sm−1 t  dt  1  10j≤k≤10j q   s   s   q  10n−j  0≤j<n   13    15    14   10j  9  1  s  s  1  1  1  1  can be made smaller than, say, 20ϵ, if n is taken large enough. Comparing this with  10  and  11  completes the proof.  The gist of Lemma Q is that we have the limiting relationship  lim n→∞ Pm 10ns  = Sm s . Also, since Sm s  is not constant as s varies, the limit   16   lim n→∞ Pm n    which would be our desired “probability”  does not exist for any m. The situation is shown in Fig. 5, which shows the values of Sm s  when m is small and r = 2.   260  ARITHMETIC  4.2.4  Fig. 5. The probability that the leading digit is 1.  Even though Sm s  is not a constant, so that we do not have a definite limit for Pm n , notice that already for m = 3 in Fig. 5 the value of Sm s  stays very close to log10 2 ≈ 0.30103. Therefore we have good reason to suspect that Sm s  is very close to log10 r for all large m, and, in fact, that the sequence of functions ⟨Sm s ⟩ converges uniformly to the constant function log10 r. It is interesting to prove this conjecture by explicitly calculating Qm s  and Rm s  for all m, as in the proof of the following theorem: Theorem F. Let Sm s  be the limit defined in  16 . For all ϵ > 0, there exists a number N ϵ  such that  Sm s  − log10 r < ϵ,  for 1 ≤ s ≤ 10,   17   Qm s  − log10 r < ϵ  whenever m > N ϵ . Proof. In view of Lemma Q, we can prove this result if we can show that there is a number M depending on ϵ such that, for 1 ≤ s ≤ 10 and for all m > M, we have   18  It is not difficult to solve the recurrence formula  11  for Rm : We have  R0 s  = −1, R1 s  = −1 + r s, R2 s  = −1 +  r s 1 + ln s r , and in general For the stated range of s, this converges uniformly to −1+ r s  expln s r  = 0.  Rm s  = −1 + r s  m−1  Rm s  < ϵ.   m − 1 !  + ··· +  ln s r     19   and  1  r  .  The recurrence  11  for Qm takes the form  1 + 1 1! ln s   cm + 1 +  Qm−1 t  dt  where  Qm s  = 1  10  s  cm = 1 9  1  Qm−1 t  dt +  Rm−1 t  dt  − 1.      ,   s  10  1  r   20    21     s   m S  0.6  0.5  0.4  0.3  0.2  0.1  0.0  1  m = 3  m = 2  m = 1  2  3  4  5  6  7  8  9  10  s   4.2.4  DISTRIBUTION OF FLOATING POINT NUMBERS  261  And the solution to recurrence  20  is easily found by trying out the first few cases and guessing at a formula that can be proved by induction; we find that  Qm s  = 1 + 1  cm + 1  1! cm−1 ln s + ··· +   m − 1 ! c1 ln s m−1  1   22   It remains for us to calculate the coefficients cm, which by  19 ,  21 , and    s    .   22  satisfy the relations  c1 =  r − 10  9;   cm ln 10 + 1  cm+1 = 1 9   2! cm−1 ln 10 2 + ··· + 1 1 + 1 + ··· + 1 + r m!  1! ln 10  r   m! c1 ln 10 m  m  ln 10  r    − 10  .   23   This sequence appears at first to be very complicated, but actually we can analyze it without difficulty with the help of generating functions. Let  C z  = c1z + c2z2 + c3z3 + ··· ;  then since 10z = 1 + z ln 10 +  1 2!  z ln 10 2 + ··· , we deduce that  10 cm+1+ 9 cm+1 = 1 cm+1+cm ln10+···+ 1 = 1 10  10 cm+1  + r 10    ln 10  r  m  −1  is the coefficient of zm+1 in the function   m! c1 ln10 m 1+···+ 1 m!  − z z z 10   10 r z−1 − 1  1 − z  1 − z  r  .  10z−1 − 1  .  1 10 C z 10z + r 10  C z  = −z 1 − z  This condition holds for all values of m, so  24  must equal C z , and we obtain the explicit formula  We want to study asymptotic properties of the coefficients of C z , to complete our analysis. The large parenthesized factor in  25  approaches ln 10 r  ln 10 = 1 − log10 r as z → 1, so we see that   24    25    26   is an analytic function of the complex variable z in the circle  C z  + 1 − log10 r 1 − z  = R z   1 + 2πi  ln 10  .  z <   262  ARITHMETIC  4.2.4  In particular, R z  converges for z = 1, so its coefficients approach zero. This proves that the coefficients of C z  behave like those of  log10 r − 1   1 − z , that is,  Finally, we may combine this with  22 , to show that Qm s  approaches  m→∞ cm = log10 r − 1. lim   2! ln s 2 + ···  1 + ln s + 1  = log10 r  1 + log10 r − 1  s uniformly for 1 ≤ s ≤ 10.  Therefore we have established the logarithmic law for integers by direct calculation, at the same time seeing that it is an extremely good approximation to the average behavior although it is never precisely achieved.  The proofs of Lemma Q and Theorem F given above are slight simplifica- tions and amplifications of methods due to B. J. Flehinger, AMM 73  1966 , 1056–1061. Many authors have written about the distribution of initial digits, showing that the logarithmic law is a good approximation for many underlying distributions; see the surveys by Ralph A. Raimi, AMM 83  1976 , 521–538, and Peter Schatte, J. Information Processing and Cybernetics 24  1988 , 443–455, for a comprehensive review of the literature.  Exercise 17 discusses an approach to the definition of probability under which the logarithmic law holds exactly, over the integers. Furthermore, ex- ercise 18 demonstrates that any reasonable definition of probability over the integers must lead to the logarithmic law, if it assigns a value to the probability of leading digits.  Floating point computations operate primarily on noninteger numbers, of course; we have studied integers because of their familiarity and their simplic- ity. When arbitrary real numbers are considered, theoretical results are more difficult to obtain, but evidence is accumulating that the same statistics apply, in the sense that repeated calculations with real numbers will nearly always tend to yield better and better approximations to a logarithmic distribution of fraction parts. For example, Peter Schatte [Zeitschrift für angewandte Math. und Mechanik 53  1973 , 553–565] showed that, under mild restrictions, the products of independent, identically distributed random real variables approach the logarithmic distribution. The sums of such variables do too, but only in the sense of repeated averaging. Similar results have been obtained by J. L. Barlow and E. H. Bareiss, Computing 34  1985 , 325–347. See also A. Berger, L. A. Bunimovich, and T. P. Hill, Trans. Amer. Math. Soc. 357  2004 , 197–219.  EXERCISES 1. [13] Given that u and v are nonzero floating decimal numbers with the same sign, what is the approximate probability that fraction overflow occurs during the calculation of u ⊕ v, according to Tables 1 and 2? 2. [42] Make further tests of floating point addition and subtraction, to confirm or improve on the accuracy of Tables 1 and 2.   4.2.4  DISTRIBUTION OF FLOATING POINT NUMBERS  263  3. [15] What is the probability that the two leading digits of a floating decimal number are “23”, according to the logarithmic law? 4. [M18] The text points out that the front pages of a well-used table of logarithms get dirtier than the back pages do. What if we had an antilogarithm table instead, namely a table that tells us the value of x when log10 x is given; which pages of such a table would be the dirtiest?   cid:120  5. [M20] Let U be a random real number that is uniformly distributed in the interval  0 < U < 1. What is the distribution of the leading digits of U? 6. [23] If we have binary computer words containing n + 1 bits, we might use p bits for the fraction part of floating binary numbers, one bit for the sign, and n − p bits for the exponent. This means that the range of values representable, namely the ratio of the largest positive normalized value to the smallest, is essentially 22n−p. The same computer word could be used to represent floating hexadecimal numbers, that is, floating point numbers with radix 16, with p + 2 bits for the fraction part   p + 2  4 hexadecimal digits  and n− p− 2 bits for the exponent; then the range of values would be 162n−p−2 = 22n−p, the same as before, and with more bits in the fraction part. This may sound as if we are getting something for nothing, but the normalization condition for base 16 is weaker in that there may be up to three leading zero bits in the fraction part; thus not all of the p + 2 bits are “significant.”   cid:120  10. [HM28] The text shows that cm = log10 r − 1 + ϵm, where ϵm approaches zero as  On the basis of the logarithmic law, what are the probabilities that the fraction part of a positive normalized radix 16 floating point number has exactly 0, 1, 2, and 3 leading zero bits? Discuss the desirability of hexadecimal versus binary. 7. [HM28] Prove that there is no distribution function F  u  that satisfies  5  for each integer b ≥ 2, and for all real values r in the range 1 ≤ r ≤ b. 8. [HM23] Does  10  hold when m = 0 for suitable N0 ϵ ? 9. [HM25]  P. Diaconis.  Let P1 n , P2 n , . . . be any sequence of functions defined by repeatedly averaging a given function P0 n  according to Eq.  9 . Prove that limm→∞ Pm n  = P0 1  for all fixed n. m → ∞. Obtain the next term in the asymptotic expansion of cm. 11. [M15] Given that U is a random variable distributed according to the logarithmic law, prove that 1 U is also. 12. [HM25]  R. W. Hamming.  The purpose of this exercise is to show that the result of floating point multiplication tends to obey the logarithmic law more perfectly than the operands do. Let U and V be random, normalized, positive floating point numbers, whose fraction parts are independently distributed with the respective density functions 1 b f x g y  dy dx, for 1 b ≤ r, s ≤ 1. Let h x  be the density function of the fraction part of U × V  unrounded . Define the abnormality A f  of a density function f to be the maximum relative error,  f x  and g x . Thus, fu ≤ r and fv ≤ s with probability  r   s  1 b   f x  − l x   l x    ,  A f  = max  1 b≤x≤1  where l x  = 1  x ln b  is the density of the logarithmic distribution.  Prove that A h  ≤ min A f , A g  .  In particular, if either factor has logarithmic  distribution the product does also.    264  4.2.4  ARITHMETIC   cid:120  13. [M20] The floating point multiplication routine, Algorithm 4.2.1M, requires zero  cid:120  14. [HM30] Let U and V be random, normalized, positive floating point numbers  or one left shifts during normalization, depending on whether fufv ≥ 1 b or not. Assuming that the input operands are independently distributed according to the logarithmic law, what is the probability that no left shift is needed for normalization of the result?  whose fraction parts are independently distributed according to the logarithmic law, and let pk be the probability that the difference in their exponents is k. Assuming that the distribution of the exponents is independent of the fraction parts, give an equation for the probability that “fraction overflow” occurs during the floating point addition of U ⊕ V, in terms of the base b and the quantities p0, p1, p2, . . . . Compare this result with exercise 1.  Ignore rounding.  15. [HM28] Let U, V, p0, p1, . . . be as in exercise 14, and assume that radix 10 arithmetic is being used. Show that regardless of the values of p0, p1, p2, . . . , the sum U ⊕ V will not obey the logarithmic law exactly, and in fact the probability that U ⊕ V has leading digit 1 is always strictly less than log10 2. 16. [HM28]  P. Diaconis.  Let P0 n  be 0 or 1 for each n, and define “probabilities” Pm+1 n  by repeated averaging, as in  9 . Show that if limn→∞ P1 n  does not exist, [Hint: Prove that an → 0 whenever we have neither does limn→∞ Pm n  for any m.  a1 + ··· + an  n → 0 and an+1 ≤ an + M n, for some fixed constant M > 0.]   cid:120  17. [HM25]  M. Tsuji.  Another way to define the value of Pr S n   is to evaluate the  k=1[S k ] k ; it can be shown that this harmonic probability quantity limn→∞ H−1 exists and is equal to Pr S n  , whenever the latter exists according to Definition 3.5A. Prove that the harmonic probability of the statement “ log10 n  mod 1 < r” exists and equals r.  Thus, initial digits of integers satisfy the logarithmic law exactly in this sense.    cid:120  18. [HM30] Let P  S  be any real-valued function defined on sets S of positive integers,  n  but not necessarily on all such sets, satisfying the following rather weak axioms: i  If P  S  and P  T   are defined and S ∩ T = ∅, then P  S ∪ T   = P  S  + P  T  . ii  If P  S  is defined, then P  S + 1  = P  S , where S + 1 = {n + 1  n ∈ S}. iii  If P  S  is defined, then P  2S  = 1 iv  If S is the set of all positive integers, then P  S  = 1. v  If P  S  is defined, then P  S  ≥ 0. Assume furthermore that P  La  is defined for all positive integers a, where La is the set of all integers whose decimal representation begins with a:  2 P  S , where 2S = {2n  n ∈ S}.  n  La = {n  10ma ≤ n < 10m a + 1  for some integer m} .   In this definition, m may be negative; for example, 1 is an element of L10, but not of L11.  Prove that P  La  = log10 1 + 1 a  for all integers a ≥ 1. 19. [HM25]  R. L. Duncan.  Prove that the leading digits of Fibonacci numbers obey the logarithmic law of fraction parts: Pr 10fFn < r  = log10 r. 20. [HM40] Sharpen  16  by finding the asymptotic behavior of Pm 10ns − Sm s  as n → ∞.   4.3.1  THE CLASSICAL ALGORITHMS  265  4.3. MULTIPLE-PRECISION ARITHMETIC Let us now consider operations on numbers that have arbitrarily high precision. For simplicity in exposition, we shall assume that we are working with integers, instead of with numbers that have an embedded radix point.  4.3.1. The Classical Algorithms In this section we shall discuss algorithms for a  addition or subtraction of n-place integers, giving an n-place answer and a  b  multiplication of an m-place integer by an n-place integer, giving an  m+n -  carry;  place answer;  c  division of an  m+ n -place integer by an n-place integer, giving an  m+1 -  place quotient and an n-place remainder.  These may be called the classical algorithms, since the word “algorithm” was used only in connection with these processes for several centuries. The term “n-place integer” means any nonnegative integer less than bn, where b is the radix of ordinary positional notation in which the numbers are expressed; such numbers can be written using at most n “places” in this notation.  It is a straightforward matter to apply the classical algorithms for integers to numbers with embedded radix points or to extended-precision floating point numbers, in the same way that arithmetic operations defined for integers in MIX are applied to these more general problems.  In this section we shall study algorithms that do operations  a ,  b , and  c  above for integers expressed in radix b notation, where b is any given integer that is 2 or more. Thus the algorithms are quite general definitions of arithmetic processes, and as such they are unrelated to any particular computer. But the discussion in this section will also be somewhat machine-oriented, since we are chiefly concerned with efficient methods for doing high-precision calculations by computer. Although our examples are based on the mythical MIX, essentially the same considerations apply to nearly every other machine.  The most important fact to understand about extended-precision numbers is that they may be regarded as numbers written in radix w notation, where w is the computer’s word size. For example, an integer that fills 10 words on a computer whose word size is w = 1010 has 100 decimal digits; but we will consider it to be a 10-place number to the base 1010. This viewpoint is justified for the same reason that we may convert, say, from binary to hexadecimal notation,  simply by grouping the bits together. See Eq. 4.1– 5 .  In these terms, we are given the following primitive operations to work with: a0  addition or subtraction of one-place integers, giving a one-place answer and  b0  multiplication of a one-place integer by another one-place integer, giving a  a carry;  two-place answer;  c0  division of a two-place integer by a one-place integer, provided that the  quotient is a one-place integer, and yielding also a one-place remainder.   266  ARITHMETIC  4.3.1  By adjusting the word size, if necessary, nearly all computers will have these three operations available; so we will construct algorithms  a ,  b , and  c  mentioned above in terms of the primitive operations  a0 ,  b0 , and  c0 .  Since we are visualizing extended-precision integers as base b numbers, it is sometimes helpful to think of the situation when b = 10, and to imagine that we are doing the arithmetic by hand. Then operation  a0  is analogous to mem- orizing the addition table;  b0  is analogous to memorizing the multiplication table; and  c0  is essentially memorizing the multiplication table in reverse. The more complicated operations  a ,  b ,  c  on high-precision numbers can now be done using the simple addition, subtraction, multiplication, and long-division procedures that children are taught in elementary school. In fact, most of the algorithms we shall discuss in this section are essentially nothing more than mechanizations of familiar pencil-and-paper operations. Of course, we must state the algorithms much more precisely than they have ever been stated in the fifth grade, and we should also attempt to minimize computer memory and running time requirements.  To avoid a tedious discussion and cumbersome notations, we shall assume first that all the numbers we deal with are nonnegative. The additional work of computing the signs, etc., is quite straightforward, although some care is necessary when dealing with complemented numbers on computers that do not use a signed magnitude representation. Such issues are discussed near the end of this section.  First comes addition, which of course is very simple, but it is worth careful  study since the same ideas occur also in the other algorithms. Algorithm A  Addition of nonnegative integers . Given nonnegative n-place integers  un−1 . . . u1u0 b and  vn−1 . . . v1v0 b, this algorithm forms their radix-b sum,  wnwn−1 . . . w1w0 b. Here wn is the carry, and it will always be equal to 0 or 1. A1. [Initialize.] Set j ← 0, k ← 0.  The variable j will run through the various digit positions, and the variable k will keep track of carries at each step.   A2. [Add digits.] Set wj ←  uj + vj + k  mod b, and k ← ⌊ uj + vj + k  b⌋. By equivalently, k ← [uj + vj + k ≥ b].  Thus k is being set to 1 or 0, depending on whether a carry occurs or not;  uj + vj + k ≤  b − 1  +  b − 1  + 1 < 2b.  induction on the computation, we will always have  A3. [Loop on j.] Increase j by one. Now if j < n, go back to step A2; otherwise  set wn ← k and terminate the algorithm.  For a formal proof that Algorithm A is valid, see exercise 4.  A MIX program for this addition process might take the following form:  Program A  Addition of nonnegative integers . Let LOC uj  ≡ U+ j, LOC vj  ≡ V + j, LOC wj  ≡ W + j, rI1 ≡ j − n, rA ≡ k, word size ≡ b, N ≡ n.   4.3.1  THE CLASSICAL ALGORITHMS  267  1 1  A2. Add digits.  ENN1 N JOV OFLO  Al. Initialize. j ← 0. Ensure that overflow is off.  N + 1 − K k ← 0. N + 1 − K Exit the loop if j = n.  01 02 03 1H ENTA 0 J1Z 3F 04 05 2H ADD U+N,1 ADD V+N,1 06 STA W+N,1 07 INC1 1 08 JNOV 1B 09 ENTA 1 10 J1N 2B 11 12 3H STA W+N The running time for this program is 10N +6 cycles, independent of the number of carries, K. The quantity K is analyzed in detail at the close of this section. Many modifications of Algorithm A are possible, and only a few of these are mentioned in the exercises below. A chapter on generalizations of this algorithm might be entitled “How to design addition circuits for a digital computer.”  A3. Loop on j. j ← j + 1. If no overflow, set k ← 0. Otherwise, set k ← 1. To A2 if j < n. Store final carry in wn.  N N N N N K K 1  The problem of subtraction is similar to addition, but the differences are  worth noting: Algorithm S  Subtraction of nonnegative integers . Given nonnegative n-place integers  un−1 . . . u1u0 b ≥  vn−1 . . . v1v0 b, this algorithm forms their nonneg- ative radix-b difference,  wn−1 . . . w1w0 b. S1. [Initialize.] Set j ← 0, k ← 0. S2. [Subtract digits.] Set wj ←  uj − vj + k  mod b, and k ← ⌊ uj − vj + k  b⌋.  In other words, k is set to −1 or 0, depending on whether a borrow occurs or not, namely whether uj − vj + k < 0 or not. In the calculation of wj, we must have −b = 0 −  b − 1  +  −1  ≤ uj − vj + k ≤  b − 1  − 0 + 0 < b; hence 0 ≤ uj − vj + k + b < 2b, and this suggests the method of computer implementation explained below.   S3. [Loop on j.] Increase j by one. Now if j < n, go back to step S2; otherwise terminate the algorithm.  When the algorithm terminates, we should have k = 0; the condition k = −1 will occur if and only if  vn−1 . . . v1v0 b >  un−1 . . . u1u0 b, contrary to the given assumptions. See exercise 12.  In a MIX program to implement subtraction, it is most convenient to retain the value 1 + k instead of k throughout the algorithm, so that we can calculate uj − vj +  1 + k  +  b − 1  in step S2.  Recall that b is the word size.  This is illustrated in the following code. Program S  Subtraction of nonnegative integers . This program is analogous to the code in Program A, but with rA ≡ 1 + k. Here, as in other programs of this section, location WM1 contains the constant b − 1, the largest possible value that can be stored in a MIX word; see Program 4.2.3D, lines 38–39. 01 02  S1. Initialize. Ensure that overflow is off.  ENN1 N JOV OFLO  j ← 0.  1 1   268  ARITHMETIC  4.3.1  ENTA 1  K N N N N N N  K + 1 Terminate if j = n.  Set k ← 0. S2. Subtract digits. Compute uj − vj + k + b.  03 1H J1Z DONE 04 05 2H ADD U+N,1 SUB V+N,1 06 ADD WM1 07 STA W+N,1 08 INC1 1 09 JOV 1B 10 ENTA 0 11 J1N 2B 12 HLT 5 13 The running time for this program is 12N + 3 cycles, slightly longer than the corresponding amount for Program A.   May be minus zero  S3. Loop on j. j ← j + 1. If overflow, set k ← 0. N − K Otherwise set k ← −1. N − K Back to S2 if j < n.  The reader may wonder if it would not be worthwhile to have a combined addition-subtraction routine in place of the two algorithms A and S. But an examination of the code shows that it is generally better to use two different routines, so that the inner loops of the computations can be performed as rapidly as possible, since the programs are so short.   Error, v > u   Our next problem is multiplication, and here we carry the ideas used in  Algorithm A a little further: Algorithm M  Multiplication of nonnegative integers . Given nonnegative integers  um−1 . . . u1u0 b and  vn−1 . . . v1v0 b, this algorithm forms their radix-b product  wm+n−1 . . . w1w0 b.  The conventional pencil-and-paper method is based on forming the partial products  um−1 . . . u1u0 b × vj first, for 0 ≤ j < n, and then adding these products together with appropriate scale factors; but in a computer it is simpler to do the addition concurrently with the multiplication, as described in this algorithm.  M1. [Initialize.] Set wm−1, wm−2, . . . , w0 all to zero. Set j ← 0.  If those positions were not cleared to zero in this step, one can show that the steps below would set   wm+n−1 . . . w0 b ←  um−1 . . . u0 b ×  vn−1 . . . v0 b +  wm−1 . . . w0 b.  This more general multiply-and-add operation is often useful.   M2. [Zero multiplier?] If vj = 0, set wj+m ← 0 and go to step M6.  This test might save time if there is a reasonable chance that vj is zero, but it may be omitted without affecting the validity of the algorithm.   M3. [Initialize i.] Set i ← 0, k ← 0. M4. [Multiply and add.] Set t ← ui × vj + wi+j + k; then set wi+j ← t mod b and k ← ⌊t b⌋.  Here the carry k will always be in the range 0 ≤ k < b; see below.  set wj+m ← k.  M5. [Loop on i.] Increase i by one. Now if i < m, go back to step M4; otherwise  M6. [Loop on j.] Increase j by one. Now if j < n, go back to step M2; otherwise  the algorithm terminates.   4.3.1  THE CLASSICAL ALGORITHMS  269  Table 1  MULTIPLICATION OF 914 BY 84 w3 . . . 3 3 3 6 6  w4 . . . . . . . 7  t 16 05 36 36 37 17 76 76  ui 4 1 9 . 4 1 9 .  vj 4 4 4 4 8 8 8 8  j 0 0 0 0 1 1 1 1  i 0 1 2 3 0 1 2 3  w2 0 0 6 6 6 7 7 7  w1 0 5 5 5 7 7 7 7  w0 6 6 6 6 6 6 6 6  Step M5 M5 M5 M6 M5 M5 M5 M6  Algorithm M is illustrated in Table 1, assuming that b = 10, by showing the states of the computation at the beginning of steps M5 and M6. A proof of Algorithm M appears in the answer to exercise 14.  The two inequalities  0 ≤ t < b2,   1  are crucial for an efficient implementation of this algorithm, since they point out how large a register is needed for the computations. These inequalities may be proved by induction as the algorithm proceeds, for if we have k < b at the start of step M4, we have  0 ≤ k < b  ui × vj + wi+j + k ≤  b − 1  ×  b − 1  +  b − 1  +  b − 1  = b2 − 1 < b2. The following MIX program shows the considerations that are necessary when Algorithm M is implemented on a computer. The coding for step M4 would be a little simpler if our computer had a “multiply-and-add” instruction, or if it had a double-length accumulator for addition. Program M  Multiplication of nonnegative integers . This program is analo- gous to Program A. rI1 ≡ i− m, rI2 ≡ j − n, rI3 ≡ i + j, CONTENTS CARRY  ≡ k. 01 02 03 04 05 06 07 1H LDX V+N,2 08 09 10 11 12 2H STX CARRY LDA U+M,1 13 MUL V+N,2 14 SLC 5 15 ADD W,3 16  M1. Initialize. Ensure that overflow is off. wrI1 ← 0. Repeat for m > rI1 ≥ 0. j ← 0. M2. Zero multiplier? If vj = 0, set wj+m ← 0 and go to M6. M3. Initialize i. i ← 0.  i + j  ← j. k ← 0.   N − Z M M4. Multiply and add.  N − Z M  N − Z M rAX ← ui × vj.  N − Z M Interchange rA ↔ rX.  N − Z M Add wi+j to lower half.  ENT1 M-1 JOV OFLO STZ W,1 DEC1 1 J1NN *-2 ENN2 N  JXZ 8F ENN1 M ENT3 N,2 ENTX 0  N − Z N − Z N − Z  1 1 M M M 1 N N   4.3.1  270  ARITHMETIC  JNOV *+2 INCX 1 ADD CARRY JNOV *+2 INCX 1 STA W,3 INC1 1 INC3 1 J1N 2B  17 18 19 20 21 22 23 24 25 26 8H STX W+M+N,2 27 28  INC2 1 J2N 1B  K  If so, carry 1 into upper half.   N − Z M Did overflow occur?  N − Z M Add k to lower half.  N − Z M Did overflow occur?  N − Z M wi+j ← t mod b.  N − Z M M5. Loop on i. i ← i + 1.  N − Z M  i + j  ←  i + j  + 1.  N − Z M Back to M4 with rX = ⌊t b⌋ if i < m.  If so, carry 1 into upper half.  K′  N N N  Set wj+m ← k. M6. Loop on j. j ← j + 1. Repeat until j = n.  The execution time of Program M depends on the number of places, M, in the multiplicand u; the number of places, N, in the multiplier v; the number of zeros, Z, in the multiplier; and the number of carries, K and K′, that occur during the addition to the lower half of the product in the computation of t. If we approximate both K and K′ by the reasonable  although somewhat pessimistic  2 N − Z M, we find that the total running time comes to 28MN + 4M + values 1 10N + 3 − Z 28M + 3  cycles. If step M2 were deleted, the running time would be 28MN + 4M + 7N + 3 cycles, so that step is advantageous only if the density of zero positions within the multiplier is Z N > 3  28M + 3 . If the multiplier is chosen completely at random, the ratio Z N is expected to be only about 1 b, which is extremely small. We conclude that step M2 is usually not worthwhile, unless b is small.  Algorithm M is not the fastest way to multiply when m and n are large, although it has the advantage of simplicity. Speedier but more complicated methods are discussed in Section 4.3.3; it is possible to multiply numbers faster than Algorithm M even when m = n = 4.  The final algorithm of concern to us in this section is long division, in which we want to divide  m + n -place integers by n-place integers. Here the ordinary pencil-and-paper method involves a certain amount of guesswork and ingenuity on the part of the person doing the division; we must either eliminate this guess- work from the algorithm or develop some theory to explain it more carefully.  A moment’s reflection about the ordinary process of long division shows that the general problem breaks down into simpler steps, each of which is the division of an  n + 1 -place dividend u by the n-place divisor v, where 0 ≤ u v < b; the remainder r after each step is less than v, so we may use the quantity rb +  next place of dividend  as the new u in the succeeding step. For example, if we are asked to divide 3142 by 53, we first divide 314 by 53, getting 5 and a remainder of 49; then we divide 492 by 53, getting 9 and a remainder of 15; thus we have a quotient of 59 and a remainder of 15. It is clear that this same idea works in general, and so our search for an appropriate division algorithm reduces to the following problem  Fig. 6 : Let u =  unun−1 . . . u1u0 b and v =  vn−1 . . . v1v0 b be nonnegative integers in radix-b notation, where u v < b. Find an algorithm to determine q = ⌊u v⌋.   4.3.1  THE CLASSICAL ALGORITHMS  271  Fig. 6. Wanted: a way to determine q rapidly.  q  vn−1 . . . v1v0  unun−1 . . . u1u0 ←−−−−qv−−−−→ ←−−−−r−−−−→  We may observe that the condition u v < b is equivalent to the condition that u b < v, which is the same as ⌊u b⌋ < v. This is simply the condition that  unun−1 . . . u1 b <  vn−1vn−2 . . . v0 b. Furthermore, if we write r = u − qv, then q is the unique integer such that 0 ≤ r < v. The most obvious approach to this problem is to make a guess about q, based on the most significant digits of u and v. It isn’t obvious that such a method will be reliable enough, but it is worth investigating; let us therefore set   unb + un−1    vn−1    , b − 1  .  ˆq = min   2   This formula says that ˆq is obtained by dividing the two leading digits of u by the leading digit of v; and if the result is b or more we can replace it by  b − 1 . It is a remarkable fact, which we will now investigate, that this value ˆq is always a very good approximation to the desired answer q, so long as vn−1 is reasonably large. In order to analyze how close ˆq comes to q, we will first prove that ˆq is never too small. Theorem A. In the notation above, ˆq ≥ q. Proof. Since q ≤ b − 1, the theorem is certainly true if ˆq = b − 1. Otherwise we have ˆq = ⌊ unb + un−1  vn−1⌋, hence ˆqvn−1 ≥ unb + un−1 − vn−1 + 1. It follows that  u − ˆqv ≤ u − ˆqvn−1bn−1  ≤ unbn + ··· + u0 −  unbn + un−1bn−1 − vn−1bn−1 + bn−1  = un−2bn−2 + ··· + u0 − bn−1 + vn−1bn−1 < vn−1bn−1 ≤ v.  Since u − ˆqv < v, we must have ˆq ≥ q.  We will now prove that ˆq cannot be much larger than q in practical situa-  tions. Assume that ˆq ≥ q + 3. We have  ˆq ≤ unb + un−1  The case v = bn−1 is impossible, for if v =  100 . . . 0 b then q = ˆq. Furthermore,  vn−1bn−1 <  v − bn−1 .  vn−1bn−1  vn−1  = unbn + un−1bn−1  ≤  u  u  the relation q >  u v  − 1 implies that v − bn−1 − u  v − bn−1  3 ≤ ˆq − q <  Therefore  u  v       bn−1  v − bn−1  + 1.  + 1 = u v  > 2  u v  bn−1  ≥ 2 vn−1 − 1 .   ARITHMETIC  272 4.3.1 Finally, since b − 4 ≥ ˆq − 3 ≥ q = ⌊u v⌋ ≥ 2 vn−1 − 1 , we have vn−1 < ⌊b 2⌋. This proves the result we seek: Theorem B. If vn−1 ≥ ⌊b 2⌋, then ˆq − 2 ≤ q ≤ ˆq.  The most important part of this theorem is that the conclusion is indepen- dent of b; no matter how large the radix is, the trial quotient ˆq will never be more than 2 in error. The condition that vn−1 ≥ ⌊b 2⌋ is very much like a normalization require- ment; in fact, it is exactly the condition of floating-binary normalization in a binary computer. One simple way to ensure that vn−1 is sufficiently large is to multiply both u and v by ⌊b  vn−1 + 1 ⌋; this does not change the value of u v, nor does it increase the number of places in v, and exercise 23 proves that it will always make the new value of vn−1 large enough.  Another way to normalize the divisor is discussed in exercise 28.   Now that we have armed ourselves with all of these facts, we are in a position to write the desired long-division algorithm. This algorithm uses a slightly improved choice of ˆq in step D3, which guarantees that q = ˆq or ˆq − 1; in fact, the improved choice of ˆq made here is almost always accurate. Algorithm D  Division of nonnegative integers . Given nonnegative integers u =  um+n−1 . . . u1u0 b and v =  vn−1 . . . v1v0 b, where vn−1 ̸= 0 and n > 1, we form the radix-b quotient ⌊u v⌋ =  qmqm−1 . . . q0 b and the remainder u mod v =  rn−1 . . . r1r0 b.  When n = 1, the simpler algorithm of exercise 16 should be used.  D1. [Normalize.] Set d ← ⌊b  vn−1 + 1 ⌋. Then set  um+num+n−1 . . . u1u0 b equal to  um+n−1 . . . u1u0 b times d; similarly, set  vn−1 . . . v1v0 b equal to  vn−1 . . . v1v0 b times d.  Notice the introduction of a new digit position um+n at the left of um+n−1; if d = 1, all we need to do in this step is to set um+n ← 0. On a binary computer it may be preferable to choose d to be a power of 2 instead of using the value suggested here; any value of d that results in vn−1 ≥ ⌊b 2⌋ will suffice. See also exercise 37.   D3. [Calculate ˆq.] Set ˆq ← uj+nb+ uj+n−1  vn−1  D2. [Initialize j.] Set j ← m.  The loop on j, steps D2 through D7, will be essentially a division of  uj+n . . . uj+1uj b by  vn−1 . . . v1v0 b to get a single quotient digit qj; see Fig. 6.   uj+nb + uj+n−1  mod vn−1. Now test if ˆq ≥ b or ˆqvn−2 > bˆr + uj+n−2; if so, decrease ˆq by 1, increase ˆr by vn−1, and repeat this test if ˆr < b.  The test on vn−2 determines at high speed most of the cases in which the trial value ˆq is one too large, and it eliminates all cases where ˆq is two too large; see exercises 19, 20, 21.    and let ˆr be the remainder,  D4. [Multiply and subtract.] Replace  uj+nuj+n−1 . . . uj b by  uj+nuj+n−1 . . . uj b − ˆq  0 vn−1 . . . v1v0 b.  This computation  analogous to steps M3, M4, and M5 of Algorithm M  consists of a simple multiplication by a one-place number, combined with   4.3.1  THE CLASSICAL ALGORITHMS  273  Fig. 7. Long division.  a subtraction. The digits  uj+n, uj+n−1, . . . , uj  should be kept positive; if the result of this step is actually negative,  uj+nuj+n−1 . . . uj b should be left as the true value plus bn+1, namely as the b’s complement of the true value, and a “borrow” to the left should be remembered.  D5. [Test remainder.] Set qj ← ˆq. If the result of step D4 was negative, go to  step D6; otherwise go on to step D7.  D6. [Add back.]  The probability that this step is necessary is very small, on the order of only 2 b, as shown in exercise 21; test data to activate this step should therefore be specifically contrived when debugging. See exercise 22.  Decrease qj by 1, and add  0 vn−1 . . . v1v0 b to  uj+nuj+n−1 . . . uj+1uj b.  A carry will occur to the left of uj+n, and it should be ignored since it cancels with the borrow that occurred in D4.   D7. [Loop on j.] Decrease j by one. Now if j ≥ 0, go back to D3. D8. [Unnormalize.] Now  qm . . . q1q0 b is the desired quotient, and the desired  remainder may be obtained by dividing  un−1 . . . u1u0 b by d. The representation of Algorithm D as a MIX program has several points of  interest: Program D  Division of nonnegative integers . The conventions of this program are analogous to Program A; rI1 ≡ i − n, rI2 ≡ j, rI3 ≡ i + j. 001 D1 JOV OFLO ··· 039 D2 ENT2 M 040 041 D3 LDA U+N,2 1:5  M + 1 M + 1 042 M + 1 043 M + 1 044 M + 1 045 M + 1 046 M + 1 047  D1. Normalize.  See exercise 25  D2. Initialize j. j ← m. Set vn ← 0, for convenience in D4. D3. Calculate ˆq. rAX ← uj+nb + uj+n−1. rA ← ⌊rAX vn−1⌋. Jump if quotient ≥ b. ˆq ← rA. ˆr ← uj+nb + uj+n−1 − ˆqvn−1  LDX U+N-1,2 DIV V+N-1 JOV 1F STA QHAT STX RHAT JMP 2F  =  uj+nb + uj+n−1  mod vn−1.  STZ V+N  1 1  1  D1. Normalize  D2. Initialize j  q  cid:54 = ˆq  D6. Add back  D3. Calculate ˆq  D4. Multiply and subtract  D5. Test remainder  D7. Loop on j  D8. Unnormalize  j ≥ 0   4.3.1  rX ← b − 1. rA ← uj+n−1.  Here uj+n = vn−1.   Decrease ˆq by one. Adjust ˆr accordingly: ˆq ← rX. rA ← ˆr + vn−1.  If ˆr will be ≥ b, ˆqvn−2 will be < ˆrb.  ˆr ← rA.  E E E E E E E E  E  M + E + 1 M + E + 1 M + E + 1  Test if ˆqvn−2 ≤ ˆrb + uj+n−2.  M + 1 M + 1 M + 1  If not, ˆq is too large. D4. Multiply and subtract. i ← 0.  i + j  ← j.  Here 1 − b < rX ≤ +1.  rAX ← −ˆqvi. Interchange rA ↔ rX.   M + 1  N + 1   M + 1  N + 1   M + 1  N + 1   M + 1  N + 1   M + 1  N + 1  Add the contribution from the  M + 1  N + 1  digit to the right, plus 1.  K  K′  If sum is ≤ −b, carry −1.  M + 1  N + 1  Add ui+j.  M + 1  N + 1  Add b − 1 to force + sign. If no overflow, carry −1.  M + 1  N + 1  rX ≡ carry + 1.  M + 1  N + 1  ui+j ← rA  may be minus zero .  M + 1  N + 1   M + 1  N + 1   M + 1  N + 1  Repeat for 0 ≤ i ≤ n. D5. Test remainder. Set qj ← ˆq.  Here rX = 0 or 1, since vn = 0.  D6. Add back. Set qj ← ˆq − 1. i ← 0.  i + j  ← j.  This is essentially Program A.   M + 1 M + 1 M + 1  274  ARITHMETIC  D4 3B  LDA U+N-1,2 JMP 4F  048 1H LDX WM1 049 050 051 3H LDX QHAT DECX 1 052 LDA RHAT 053 054 4H STX QHAT ADD V+N-1 055 JOV D4 056 STA RHAT 057 LDA QHAT 058 059 2H MUL V+N-2 CMPA RHAT 060 JL 061 JG 062 CMPX U+N-2,2 063 3B JG 064 065 D4 ENTX 1 ENN1 N 066 ENT3 0,2 067 068 2H STX CARRY LDAN V+N,1 069 MUL QHAT 070 SLC 5 071 ADD CARRY 072 JNOV *+2 073 DECX 1 074 ADD U,3 075 ADD WM1 076 JNOV *+2 077 INCX 1 078 STA U,3 079 INC1 1 080 INC3 1 081 J1NP 2B 082 083 D5 LDA QHAT STA Q,2 084 JXP D7 085 086 D6 DECA 1 087 088 089 090 1H ENTA 0 091 2H ADD U,3 092 093 094 095 096  ADD V+N,1 STA U,3 INC1 1 INC3 1 JNOV 1B  STA Q,2 ENN1 N ENT3 0,2   THE CLASSICAL ALGORITHMS  275  4.3.1  ENTA 1 097 J1NP 2B 098 099 D7 DEC2 1 J2NN D3 100 101 D8 ···  M + 1 M + 1  D7. Loop on j. Repeat for m ≥ j ≥ 0.  See exercise 26   Note how easily the rather complex-appearing calculations and decisions of step D3 can be handled inside the machine. Notice also that the program for step D4 is analogous to Program M, except that the ideas of Program S have also been incorporated. The running time for Program D can be estimated by considering the quan- tities M, N, E, K, and K′ shown in the program.  These quantities ignore several situations that occur only with very low probability; for example, we may assume that lines 048–050, 063–064, and step D6 are never executed.  Here M + 1 is the number of words in the quotient; N is the number of words in the divisor; E is the number of times ˆq is adjusted downwards in step D3; K and K′ are the number of times certain carry adjustments are made during the multiply- If we assume that K + K′ is approximately  N + 1  M + 1 , subtract loop. and that E is approximately 1 2 M, we get a total running time of approximately 30MN + 30N + 89M + 111 cycles, plus 67N + 23.5M + 4 more if d > 1.  The program segments of exercises 25 and 26 are included in these totals.  When M and N are large, this is only about seven percent longer than the time needed by Program M to multiply the quotient by the divisor.  When the radix b is comparatively small, so that b2 is less than the com- puter’s word size, multiprecision division can be speeded up by not reducing individual digits of intermediate results to the range [0 . . b ; see D. M. Smith, Math. Comp. 65  1996 , 157–163. Further commentary on Algorithm D appears in the exercises at the close of this section.  It is possible to debug programs for multiple-precision arithmetic by using the multiplication and addition routines to check the result of the division routine, etc. The following type of test data is occasionally useful:   tm − 1  tn − 1  = tm+n − tn − tm + 1.  If m < n, this number has the radix-t expansion   t − 1       . . .  m−1 places     t − 1    t − 2    t − 1       . . .   t − 1     n−m places      . . .  0 0 m−1 places    1;  for example,  103 − 1  108 − 1  = 99899999001. In the case of Program D, it is also necessary to find some test cases that cause the rarely executed parts of the program to be exercised; some portions of that program would probably never get tested even if a million random test cases were tried.  See exercise 22.   Now that we have seen how to operate with signed magnitude numbers, let us consider what approach should be taken to the same problems when a computer with complement notation is being used. For two’s complement and ones’ complement notations, it is usually best to let the radix b be one half of the   276  ARITHMETIC  4.3.1  word size; thus for a 32-bit computer word we would use b = 231 in the algorithms above. The sign bit of all but the most significant word of a multiple-precision number will be zero, so that no anomalous sign correction takes place during the computer’s multiplication and division operations. In fact, the basic meaning of complement notation requires that we consider all but the most significant word to be nonnegative. For example, assuming an 8-bit word, the two’s complement number  11011111 1111110 1101011   where the sign bit is shown only in the most significant word  is properly thought of as  −221 +  1011111 2 · 214 +  1111110 2 · 27 +  1101011 2.  On the other hand, some binary computers that work with two’s complement let x notation also provide true unsigned arithmetic as well. For example, and y be 32-bit operands. A computer might regard them as two’s complement numbers in the range −231 ≤ x, y < 231, or as unsigned numbers in the range 0 ≤ x, y < 232. If we ignore overflow, the 32-bit sum  x + y  mod 232 is the same under either interpretation; but overflow occurs in different circumstances when we change the assumed range. If the computer allows easy computation of the carry bit ⌊ x + y  232⌋ in the unsigned interpretation, and if it provides a full 64-bit product of unsigned 32-bit integers, we can use b = 232 instead of b = 231 in our high-precision algorithms.  Addition of signed numbers is slightly easier when complement notations are being used, since the routine for adding n-place nonnegative integers can be used for arbitrary n-place integers; the sign appears only in the first word, so the less significant words may be added together irrespective of the actual sign.  Special attention must be given to the leftmost carry when ones’ complement notation is being used, however; it must be added into the least significant word, and possibly propagated further to the left.  Similarly, we find that subtraction of signed numbers is slightly simpler with complement notation. On the other hand, multiplication and division seem to be done most easily by working with nonnegative quantities and doing suitable complementation operations before- hand to make sure that both operands are nonnegative. It may be possible to avoid this complementation by devising some tricks for working directly with negative numbers in a complement notation, and it is not hard to see how this could be done in double-precision multiplication; but care should be taken not to slow down the inner loops of the subroutines when high precision is required. Let us now turn to an analysis of the quantity K that arises in Program A, namely the number of carries that occur when two n-place numbers are being added together. Although K has no effect on the total running time of Pro- gram A, it does affect the running time of the Program A’s counterparts that deal with complement notations, and its analysis is interesting in itself as a significant application of generating functions. Suppose that u and v are independent random n-place integers, uniformly distributed in the range 0 ≤ u, v < bn. Let pnk be the probability that exactly k carries occur in the addition of u to v, and that one of these carries occurs   4.3.1 277 in the most significant position  so that u + v ≥ bn . Similarly, let qnk be the probability that exactly k carries occur, but that there is no carry in the most significant position. Then it is not hard to see that, for all k and n,  THE CLASSICAL ALGORITHMS  p0k = 0,  q0k = δ0k,  p n+1  k+1  = b + 1 2b q n+1 k = b − 1 2b  pnk + b − 1 2b pnk + b + 1 2b  qnk,  qnk;   3   this happens because  b − 1  2b is the probability that un−1 + vn−1 ≥ b and  b+1  2b is the probability that un−1+vn−1+1 ≥ b, when un−1 and vn−1 are in- dependently and uniformly distributed integers in the range 0 ≤ un−1, vn−1 < b. To obtain further information about these quantities pnk and qnk, we set up the generating functions  pnk zktn,  qnk zktn.   4   Q z, t  =  P z, t  =  k,n  From  3  we have the basic relations    k,n   b + 1 P z, t  + b − 1  b − 1 2b  2b  P z, t  + b + 1 2b  2b  Q z, t   ,    Q z, t   .  P z, t  = zt  Q z, t  = 1 + t  G z, t  = P z, t  + Q z, t  =  Gn z tn,  n  These two equations are readily solved for P z, t  and Q z, t ; and if we let  where Gn z  is the generating function for the total number of carries when n-place numbers are added, we find that  ∂G ∂z ∂2G  2 1 + b  1 + z t + zt2.  G z, t  =  b − zt  p z, t , where p z, t  = b − 1  p z, t  + t b − zt  b + 1 − 2t    5  Note that G 1, t  = 1  1 − t , and this checks with the fact that Gn 1  must equal 1  it is the sum of all the possible probabilities . Taking partial derivatives = of  5  with respect to z, we find that n z tn = −t G′ ∂z2 = n z tn = −t2 b + 1 − 2t  G′′  1  1− t 2 −  1  1− t 3 −  2p z, t 2 + t2 b − zt  b + 1 − 2t 2   b−1  1− t  +  b−1 2 1− t  +   b−1  b− t   b−1 2 b− t  +  Now let us put z = 1 and expand in partial fractions:  G′ n 1 tn = t 2 n 1 tn = t2 G′′ 2   b−1  b− t 2     2p z, t 3  p z, t 2      1  1  1  1  1  ;  n  n  n  ,  .  .  n   278  ARITHMETIC  4.3.1   6   It follows that the average number of carries, the mean value of K, is    n 1  = 1 G′ 2  n − 1 b − 1  ;  n 1 −1  n − 1  1  b  1  2n  the variance is  n 1  − G′ n 1  + G′ G′′ n + 2n = 1 4  n 1 2  b − 1 − 2b + 1   b − 1 2 + 2b + 2  b − 1 2  b   b − 1 2  b  .   7   So the number of carries is just slightly less than 1 2 n under these assumptions. History and bibliography. The early history of the classical algorithms described in this section is left as an interesting project for the reader, and only the history of their implementation on computers will be traced here.  The use of 10n as an assumed radix when multiplying large numbers on a desk calculator was discussed by D. N. Lehmer and J. P. Ballantine, AMM 30  1923 , 67–69.  Double-precision arithmetic on digital computers was first treated by J. von Neumann and H. H. Goldstine in their introductory notes on programming, originally published in 1947 [J. von Neumann, Collected Works 5, 142–151]. Theorems A and B above are due to D. A. Pope and M. L. Stein [CACM 3  1960 , 652–654], whose paper also contains a bibliography of earlier work on double-precision routines. Other ways of choosing the trial quotient ˆq have been discussed by A. G. Cox and H. A. Luther, CACM 4  1961 , 353 [divide by vn−1+1 instead of vn−1], and by M. L. Stein, CACM 7  1964 , 472–474 [divide by vn−1 or vn−1 + 1 according to the magnitude of vn−2]; E. V. Krishnamurthy [CACM 8  1965 , 179–181] showed that examination of the single-precision remainder in the latter method leads to an improvement over Theorem B. Krishnamurthy and Nandi [CACM 10  1967 , 809–813] suggested a way to replace the normalization and unnormalization operations of Algorithm D by a calculation of ˆq based on several leading digits of the operands. G. E. Collins and D. R. Musser have carried out an interesting analysis of the original Pope and Stein algorithm [Information Processing Letters 6  1977 , 151–155].  Several alternative approaches to division have also been suggested:  1  “Fourier division” [J. Fourier, Analyse des Équations Déterminées  Paris: 1831 , §2.21]. This method, which was often used on desk calculators, essentially obtains each new quotient digit by increasing the precision of the divisor and the dividend at each step. Some rather extensive tests by the author have indicated that such a method is inferior to the divide-and-correct technique above, but there may be some applications in which Fourier division is practical. See D. H. Lehmer, AMM 33  1926 , 198–206; J. V. Uspensky, Theory of Equations  New York: McGraw–Hill, 1948 , 159–164. 2  “Newton’s method” for evaluating the reciprocal of a number was extensively used in early computers when there was no single-precision division instruction. The idea is to find some initial approximation x0 to the number 1 v, then to let   4.3.1 279 xn+1 = 2xn − vx2 n. This method converges rapidly to 1 v, since xn =  1 − ϵ  v implies that xn+1 =  1 − ϵ2  v. Convergence to third order, with ϵ replaced by O ϵ3  at each step, can be obtained using the formula  THE CLASSICAL ALGORITHMS  xn+1 = xn + xn 1 − vxn  + xn 1 − vxn 2  1 +  1 − vxn  1 +  1 − vxn  ,  = xn  and similar formulas hold for fourth-order convergence, etc.; see P. Rabinowitz, CACM 4  1961 , 98. For calculations on extremely large numbers, Newton’s second-order method and subsequent multiplication by u can actually be consid- erably faster than Algorithm D, if we increase the precision of xn at each step and if we also use the fast multiplication routines of Section 4.3.3.  See Algorithm 4.3.3R for details.  Some related iterative schemes have been discussed by E. V. Krishnamurthy, IEEE Trans. C-19  1970 , 227–231. 3  Division methods have also been based on the evaluation of    1 − ϵ double-precision caseEq. 4.2.3– 2 .  = u v  v + ϵ  u  v   + ϵ  2 − ϵ  3  v  v    + ···  .  See H. H. Laughlin, AMM 37  1930 , 287–293. We have used this idea in the  Besides the references just cited, the following early articles concerning multiple-precision arithmetic are also of interest: High-precision routines for floating point calculations using ones’ complement arithmetic were described by A. H. Stroud and D. Secrest, Comp. J. 6  1963 , 62–66. Extended-precision subroutines for use in FORTRAN programs were described by B. I. Blum, CACM 8  1965 , 318–320, and for use in ALGOL by M. Tienari and V. Suokonautio, BIT 6  1966 , 332–338. Arithmetic on integers with unlimited precision, making use of linked memory allocation techniques, was elegantly introduced by G. E. Collins, CACM 9  1966 , 578–589. For a much larger repertoire of multiple- precision operations, including logarithms and trigonometric functions, see R. P. Brent, ACM Trans. Math. Software 4  1978 , 57–81; D. M. Smith, ACM Trans. Math. Software 17  1991 , 273–283, 24  1998 , 359–367.  Human progress in calculation has traditionally been measured by the num- ber of decimal digits of π that were known at a given point in history. Section 4.1 mentions some of the early developments; by 1719, Thomas Fantet de Lagny had computed π to 127 decimal places [Mémoires Acad. Sci.  Paris, 1719 , 135– 145; a typographical error affected the 113th digit]. After better formulas were discovered, a famous mental calculator from Hamburg named Zacharias Dase needed less than two months to calculate 200 decimal digits correctly in 1844 [Crelle 27  1844 , 198]. Then William Shanks published 607 decimals of π in 1853, and continued to extend his calculations until he had obtained 707 digits in 1873. [See W. Shanks, Contributions to Mathematics  London: 1853 ; Proc. Royal Soc. London 21  1873 , 318–319; 22  1873 , 45–46; J. C. V. Hoffmann, Zeit. für math. und naturwiss. Unterricht 26  1895 , 261–264.] Shanks’s 707- place value was widely quoted in mathematical reference books for many years,   280  ARITHMETIC  4.3.1  but D. F. Ferguson noticed in 1945 that it contained several mistakes beginning at the 528th decimal place [Math. Gazette 30  1946 , 89–90]. G. Reitwiesner and his colleagues used 70 hours of computing time on ENIAC during Labor Day weekend in 1949 to obtain 2037 correct decimals [Math. Tables and Other Aids to Comp. 4  1950 , 11–15]. F. Genuys reached 10,000 digits in 1958, after 100 minutes on an IBM 704 [Chiffres 1  1958 , 17–22]; shortly afterwards, the first 100,000 digits were published by D. Shanks [no relation to William] and J. W. Wrench, Jr. [Math. Comp. 16  1962 , 76–99], after about 8 hours on an IBM 7090 and another 4.5 hours for checking. Their check actually revealed a transient hardware error, which went away when the computation was repeated. One million digits of π were computed by Jean Guilloud and Martine Bouyer of the French Atomic Energy Commission in 1973, after nearly 24 hours of computer time on a CDC 7600 [see A. Shibata, Surikagaku 20  1982 , 65–73]. Amazingly, Dr. I. J. Matrix had correctly predicted seven years earlier that the millionth digit would turn out to be “5” [Martin Gardner, New Mathematical Diversions  Simon and Schuster, 1966 , addendum to Chapter 8]. The billion-digit barrier was passed in 1989 by Gregory V. Chudnovsky and David V. Chudnovsky, and independently by Yasumasa Kanada and Yoshiaki Tamura; the Chudnovskys extended their calculation to two billion digits in 1991, after 250 hours of computation on a home-built parallel machine. [See Richard Preston, The New Yorker 68, 2  2 March 1992 , 36–67. The novel formula used by the Chudnovskys is described in Proc. Nat. Acad. Sci. 86  1989 , 8178–8182.] Yasumasa Kanada and Daisuke Takahashi obtained more than 51.5 billion digits in July, 1997, using two independent methods that required respectively 29.0 and 37.1 hours on a HITACHI SR2201 computer with 1024 processing elements. By 2011 the world record had risen to ten trillion digits ! , obtained by A. J. Yee and S. Kondo using the Chudnovsky formula together with exercise 39.  We have restricted our discussion in this section to arithmetic techniques for use in computer programming. Many algorithms for hardware implementation of arithmetic operations are also quite interesting, but they appear to be inap- plicable to high-precision software routines; see, for example, G. W. Reitwiesner, “Binary Arithmetic,” Advances in Computers 1  New York: Academic Press, 1960 , 231–308; O. L. MacSorley, Proc. IRE 49  1961 , 67–91; G. Metze, IRE Trans. EC-11  1962 , 761–764; H. L. Garner, “Number Systems and Arith- metic,” Advances in Computers 6  New York: Academic Press, 1965 , 131– 194. An infamous but very instructive bug in the division routine of the 1994 Pentium chip is discussed by A. Edelman in SIAM Review 39  1997 , 54–67. The minimum achievable execution time for hardware addition and multiplication operations has been investigated by S. Winograd, JACM 12  1965 , 277–285, 14  1967 , 793–802; by R. P. Brent, IEEE Trans. C-19  1970 , 758–759; and by R. W. Floyd, FOCS 16  1975 , 3–5. See also Section 4.3.3E.  EXERCISES 1. [42] Study the early history of the classical algorithms for arithmetic by looking up the writings of, say, Sun Ts˘u, al-Khw¯arizm¯ı, al-Uql¯ıdis¯ı, Fibonacci, and Robert Recorde,   4.3.1  THE CLASSICAL ALGORITHMS  281  and by translating their methods as faithfully as possible into precise algorithmic notation. 2. [15] Generalize Algorithm A so that it does “column addition,” obtaining the sum of m nonnegative n-place integers.  Assume that m ≤ b.  3. [21] Write a MIX program for the algorithm of exercise 2, and estimate its running time as a function of m and n. 4. [M21] Give a formal proof of the validity of Algorithm A, using the method of inductive assertions explained in Section 1.2.1. 5. [21] Algorithm A adds the two inputs by going from right to left, but sometimes the data is more readily accessible from left to right. Design an algorithm that produces the same answer as Algorithm A, but that generates the digits of the answer from left to right, going back to change previous values if a carry occurs to make a previous value incorrect. [Note: Early Hindu and Arabic manuscripts dealt with addition from left to right in this way, probably because it was customary to work from left to right on an abacus; the right-to-left addition algorithm was a refinement due to al-Uql¯ıdis¯ı, perhaps because Arabic is written from right to left.]   cid:120  6. [22] Design an algorithm that adds from left to right  as in exercise 5 , but never  stores a digit of the answer until this digit cannot possibly be affected by future carries; there is to be no changing of any answer digit once it has been stored. [Hint: Keep track of the number of consecutive  b − 1 ’s that have not yet been stored in the answer.] This sort of algorithm would be appropriate, for example, in a situation where the input and output numbers are to be read and written from left to right on magnetic tapes, or if they appear in straight linear lists. 7. [M26] Determine the average number of times the algorithm of exercise 5 will find that a carry makes it necessary to go back and change k digits of the partial answer, for k = 1, 2, . . . , n.  Assume that both inputs are independently and uniformly distributed between 0 and bn − 1.  8. [M26] Write a MIX program for the algorithm of exercise 5, and determine its average running time based on the expected number of carries as computed in the text.   cid:120  9. [21] Generalize Algorithm A to obtain an algorithm that adds two n-place num-  bers in a mixed-radix number system, with bases b0, b1, . . .  from right to left . Thus the least significant digits lie between 0 and b0 − 1, the next digits lie between 0 and b1 − 1, etc.; see Eq. 4.1– 9 . 10. [18] Would Program S work properly if the instructions on lines 06 and 07 were interchanged? If the instructions on lines 05 and 06 were interchanged? 11. [10] Design an algorithm that compares two nonnegative n-place integers u =  un−1 . . . u1u0 b and v =  vn−1 . . . v1v0 b, to determine whether u   v. 12. [16] Algorithm S assumes that we know which of the two input operands is the larger; if this information is not known, we could go ahead and perform the subtraction anyway, and we would find that an extra borrow is still present at the end of the algorithm. Design another algorithm that could be used  if there is a borrow present at the end of Algorithm S  to complement  wn−1 . . . w1w0 b and therefore to obtain the absolute value of the difference of u and v. 13. [21] Write a MIX program that multiplies  un−1 . . . u1u0 b by v, where v is a single- precision number  that is, 0 ≤ v < b , producing the answer  wn . . . w1w0 b. How much running time is required?   282  ARITHMETIC   cid:120  14. [M22] Give a formal proof of the validity of Algorithm M, using the method of  4.3.1  inductive assertions explained in Section 1.2.1.  See exercise 4.  15. [M20] If we wish to form the product of two n-place fractions,  .u1u2 . . . un b ×  .v1v2 . . . vn b, and to obtain only an n-place approximation  .w1w2 . . . wn b to the result, Algorithm M could be used to obtain a 2n-place answer that is subsequently rounded to the desired approximation. But this involves about twice as much work as is necessary for reasonable accuracy, since the products uivj for i+ j > n+2 contribute very little to the answer.  Give an estimate of the maximum error that can occur, if these products uivj for i + j > n + 2 are not computed during the multiplication, but are assumed to be zero.   cid:120  16. [20]  Short division.  Design an algorithm that divides a nonnegative n-place  integer  un−1 . . . u1u0 b by v, where v is a single-precision number  that is, 0 < v < b , producing the quotient  wn−1 . . . w1w0 b and remainder r. 17. [M20] In the notation of Fig. 6, assume that vn−1 ≥ ⌊b 2⌋; show that if un = vn−1, we must have q = b − 1 or b − 2. 18. [M20] In the notation of Fig. 6, show that if q′ = ⌊ unb+ un−1   vn−1 +1 ⌋, then  cid:120  19. [M21] In the notation of Fig. 6, let ˆq be an approximation to q, and let ˆr = q′ ≤ q. unb + un−1 − ˆqvn−1. Assume that vn−1 > 0. Show that if ˆqvn−2 > bˆr + un−2, then q < ˆq. [Hint: Strengthen the proof of Theorem A by examining the influence of vn−2.] 20. [M22] Using the notation and assumptions of exercise 19, show that if ˆqvn−2 ≤  cid:120  21. [M23] Show that if vn−1 ≥ ⌊b 2⌋, and if ˆqvn−2 ≤ bˆr + un−2 but ˆq ̸= q in the bˆr + un−2 and ˆq < b, then ˆq = q or q = ˆq − 1. notation of exercises 19 and 20, then u mod v ≥  1 − 2 b v.  The latter event occurs  cid:120  22. [24] Find an example of a four-digit number divided by a three-digit number for with approximate probability 2 b, so that when b is the word size of a computer we must have qj = ˆq in Algorithm D except in very rare circumstances.   which step D6 is necessary in Algorithm D, when the radix b is 10. 23. [M23] Given that v and b are integers, and that 1 ≤ v < b, prove that we always have ⌊b 2⌋ ≤ v⌊b  v + 1 ⌋ <  v + 1 ⌊b  v + 1 ⌋ ≤ b. 24. [M20] Using the law of the distribution of leading digits explained in Section 4.2.4, give an approximate formula for the probability that d = 1 in Algorithm D.  When d = 1, we can omit most of the calculation in steps D1 and D8.  25. [26] Write a MIX routine for step D1, which is needed to complete Program D. 26. [21] Write a MIX routine for step D8, which is needed to complete Program D. 27. [M20] Prove that at the beginning of step D8 in Algorithm D, the unnormalized remainder  un−1 . . . u1u0 b is always an exact multiple of d. 28. [M30]  A. Svoboda, Stroje na Zpracování Informací 9  1963 , 25–32.  Let v =  vn−1 . . . v1v0 b be any radix b integer, where vn−1 ̸= 0. Perform the following opera- tions: N1. If vn−1 < b 2, multiply v by ⌊ b + 1   vn−1 + 1 ⌋. Let the result of this step N2. If vn = 0, set v ← v +  1 b ⌊b b − vn−1   vn−1 + 1 ⌋v; let the result of this  be  vnvn−1 . . . v1v0 b. step be  vnvn−1 . . . v0.v−1 . . .  b. Repeat step N2 until vn ̸= 0.   4.3.1  THE CLASSICAL ALGORITHMS  283  Prove that step N2 will be performed at most three times, and that we must always have vn = 1, vn−1 = 0 at the end of the calculations. [Note: If u and v are both multiplied by the constants above, we do not change the value of the quotient u v, and the divisor has been converted into the form  10vn−2 . . . v0.v−1v−2v−3 b. This form of the divisor is very convenient because, in the notation of Algorithm D, we may simply take ˆq = uj+n as a trial divisor at the beginning of step D3, or ˆq = b − 1 when  uj+n+1, uj+n  =  1, 0 .] 29. [15] Prove or disprove: At the beginning of step D7 of Algorithm D, we always have uj+n = 0.   cid:120  30. [22] If memory space is limited, it may be desirable to use the same storage  locations for both input and output during the performance of some of the algorithms in this section. Is it possible to have w0, w1, . . . , wn−1 stored in the same respective locations as u0, . . . , un−1 or v0, . . . , vn−1 during Algorithm A or S? Is it possible to have the quotient q0, . . . , qm occupy the same locations as un, . . . , um+n in Algorithm D? Is there any permissible overlap of memory locations between input and output in Algorithm M? 31. [28] Assume that b = 3 and that u =  um+n−1 . . . u1u0 3, v =  vn−1 . . . v1v0 3 are integers in balanced ternary notation  see Section 4.1 , vn−1 ̸= 0. Design a long-division algorithm that divides u by v, obtaining a remainder whose absolute value does not 2v. Try to find an algorithm that would be efficient if incorporated into the exceed 1 arithmetic circuitry of a balanced ternary computer. 32. [M40] Assume that b = 2i and that u and v are complex numbers expressed in the quater-imaginary number system. Design algorithms that divide u by v, perhaps obtaining a suitable remainder of some sort, and compare their efficiency. 33. [M40] Design an algorithm for taking square roots, analogous to Algorithm D and to the traditional pencil-and-paper method for extracting square roots. 34. [40] Develop a set of computer subroutines for doing the four arithmetic opera- tions on arbitrary integers, putting no constraint on the size of the integers except for the implicit assumption that the total memory capacity of the computer should not be exceeded.  Use linked memory allocation, so that no time is wasted in finding room to put the results.  35. [40] Develop a set of computer subroutines for “decuple-precision floating point” arithmetic, using excess 0, base b, nine-place floating point number representation, where b is the computer word size, and allowing a full word for the exponent.  Thus each floating point number is represented in 10 words of memory, and all scaling is done by moving full words instead of by shifting within the words.  36. [M25] Explain how to compute ln ϕ to high precision, given a suitably precise approximation to ϕ, using only multiprecision addition, subtraction, and division by small numbers.   cid:120  37. [20]  E. Salamin.  Explain how to avoid the normalization and unnormalization  steps of Algorithm D, when d is a power of 2 on a binary computer, without changing the sequence of trial quotient digits computed by that algorithm.  How can ˆq be computed in step D3 if the normalization of step D1 hasn’t been done?  38. [M35] Suppose u and v are integers in the range 0 ≤ u, v < 2n. Devise a way to compute the geometric mean ⌊√ 2⌋ by doing O n  operations of addition, subtraction, and comparison of  n+2 -bit numbers. [Hint: Use a “pipeline” to combine the classical methods of multiplication and square rooting.]  uv + 1   284  ARITHMETIC  4.3.1  39. [25]  D. Bailey, P. Borwein, and S. Plouffe, 1996.  Explain how to compute the nth bit of the binary representation of π without knowing the previous n − 1 bits, by using the identity   4 8k + 1 − 2  1 16k  π =  k≥0  8k + 4 − 1  8k + 5 − 1 8k + 6    and doing O n log n  arithmetic operations on O log n -bit integers.  Assume that the binary digits of π do not have surprisingly long stretches of consecutive 0s or 1s.  40. [M24] Sometimes we want to divide u by v when we know that the remainder will be zero. Show that if u is a 2n-place number and v is an n-place number with u mod v = 0, we can save about 75% of the work of Algorithm D if we compute half of the quotient from left to right and the other half from right to left.   cid:120  41. [M26] Many applications of high-precision arithmetic require repeated calcula-  tions modulo a fixed n-place number w, where w is relatively prime to the base b. We can speed up such calculations by using a trick due to Peter L. Montgomery [Math. Comp. 44  1985 , 519–521], which streamlines the remaindering process by essentially working from right to left instead of left to right. a  Given u = ± um+n−1 . . . u1u0 b, w =  wn−1 . . . w1w0 b, and a number w′ such that w0w′ mod b = 1, show how to compute v = ± vn−1 . . . v1v0 b such that bmv mod w = u mod w. b  Given n-place signed integers u, v, w with u,v < w, and given w′ as in  a , show how to calculate an n-place integer t such that t < w and bnt ≡ uv  modulo w . c  How do the algorithms of  a  and  b  facilitate arithmetic mod w? 42. [HM35] Given m and b, let Pnk be the probability that ⌊ u1 +··· + um  bn⌋ = k, when u1, . . . , um are random n-place integers in radix b.  This is the distribution of wn in the column addition algorithm of exercise 2.  Show that Pnk = 1 m!  + O b−n , wherem  cid:120  43. [22] Shades of gray or components of color values in digitized images are usually   is an Eulerian number  see Section 5.1.3 .  represented as 8-bit numbers u in the range [0 . . 255], denoting the fraction u 255. Given two such fractions u 255 and v 255, graphical algorithms often need to compute their approximate product w 255, where w is the nearest integer to uv 255. Prove that w can be obtained from the efficient formula  m  k  k  t = uv + 128,  w = ⌊ ⌊t 256⌋ + t  256⌋.  *4.3.2. Modular Arithmetic Another interesting alternative is available for doing arithmetic on large integer numbers, based on some simple principles of number theory. The idea is to have several moduli m1, m2, . . . , mr that contain no common factors, and to work indirectly with residues u mod m1, u mod m2, . . . , u mod mr instead of directly with the number u.  For convenience in notation throughout this section, let  u1 = u mod m1,   1  It is easy to compute  u1, u2, . . . , ur  from an integer number u by means of division; and it is important to note that no information is lost in this process  if  u2 = u mod m2,  ur = u mod mr.  . . . ,   4.3.2  MODULAR ARITHMETIC  285  u isn’t too large , since we can recompute u from  u1, u2, . . . , ur . For example, if 0 ≤ u < v ≤ 1000, it is impossible to have  u mod 7, u mod 11, u mod 13  equal to  v mod 7, v mod 11, v mod 13 . This is a consequence of the “Chinese remainder theorem” stated below.  We may therefore regard  u1, u2, . . . , ur  as a new type of internal computer  representation, a “modular representation,” of the integer u.  The advantages of a modular representation are that addition, subtraction,  and multiplication are very simple:   u1, . . . , ur  +  v1, . . . , vr  = u1 + v1  mod m1, . . . ,  ur + vr  mod mr  u1, . . . , ur  −  v1, . . . , vr  = u1 − v1  mod m1, . . . ,  ur − vr  mod mr  u1, . . . , ur  ×  v1, . . . , vr  = u1 × v1  mod m1, . . . ,  ur × vr  mod mr   2   3   4   , , .  To derive  4 , for example, we need to show that  uv mod mj =  u mod mj  v mod mj  mod mj  for each modulus mj. But this is a basic fact of elementary number theory: x mod mj = y mod mj if and only if x ≡ y  modulo mj ; furthermore if x ≡ x′ and y ≡ y′, then xy ≡ x′y′  modulo mj ; hence  u mod mj  v mod mj  ≡ uv  modulo mj .  The main disadvantage of a modular representation is that we cannot easily test whether  u1, . . . , ur  is greater than  v1, . . . , vr . It is also difficult to test whether or not overflow has occurred as the result of an addition, subtraction, or multiplication, and it is even more difficult to perform division. When such operations are required frequently in conjunction with addition, subtraction, and multiplication, the use of modular arithmetic can be justified only if fast means of conversion to and from the modular representation are available. Therefore conversion between modular and positional notation is one of the principal topics of interest to us in this section.  The processes of addition, subtraction, and multiplication using  2 ,  3 , and  4  are called residue arithmetic or modular arithmetic. The range of num- bers that can be handled by modular arithmetic is equal to m = m1m2 . . . mr, the product of the moduli; and if each mj is near our computer’s word size we can deal with n-place numbers when r ≈ n. Therefore we see that the amount of time required to add, subtract, or multiply n-place numbers using modular arithmetic is essentially proportional to n  not counting the time to convert in and out of modular representation . This is no advantage at all when addition and subtraction are considered, but it can be a considerable advantage with respect to multiplication since the conventional method of Section 4.3.1 requires an execution time proportional to n2.  Moreover, on a computer that allows many operations to take place simul- taneously, modular arithmetic can be a significant advantage even for addition and subtraction; the operations with respect to different moduli can all be done at the same time, so we obtain a substantial increase in speed. The same kind of decrease in execution time could not be achieved by the conventional techniques   286  ARITHMETIC  4.3.2  discussed in the previous section, since carry propagation must be considered. Perhaps highly parallel computers will someday make simultaneous operations commonplace, so that modular arithmetic will be of significant importance in “real-time” calculations when a quick answer to a single problem requiring high precision is needed.  With highly parallel computers, it is often preferable to run k separate programs simultaneously, instead of running a single program k times as fast, since the latter alternative is more complicated but does not utilize the machine any more efficiently. “Real-time” calculations are exceptions that make the inherent parallelism of modular arithmetic more significant.   Now let us examine the basic fact that underlies the modular representation  of numbers: Theorem C  Chinese Remainder Theorem . Let m1, m2, . . . , mr be positive integers that are relatively prime in pairs; that is,   5  Let m = m1m2 . . . mr, and let a, u1, u2, . . . , ur be integers. Then there is exactly one integer u that satisfies the conditions  when j ̸= k.  mj ⊥ mk  and  for 1 ≤ j ≤ r.  u ≡ uj  modulo mj   a ≤ u < a + m,  6  If u ≡ v  modulo mj  for 1 ≤ j ≤ r, then u − v is a multiple of mj for Proof. all j, so  5  implies that u− v is a multiple of m = m1m2 . . . mr. This argument shows that there is at most one solution of  6 . To complete the proof we must now show the existence of at least one solution, and this can be done in two simple ways: Method 1  “Nonconstructive” proof . As u runs through the m distinct values a ≤ u < a + m, the r-tuples  u mod m1, . . . , u mod mr  must also run through m distinct values, since  6  has at most one solution. But there are exactly m1m2 . . . mr possible r-tuples  v1, . . . , vr  such that 0 ≤ vj < mj. Therefore each r-tuple must occur exactly once, and there must be some value of u for which  u mod m1, . . . , u mod mr  =  u1, . . . , ur . Method 2  “Constructive” proof . We can find numbers Mj for 1 ≤ j ≤ r such that  Mj ≡ 1  modulo mj    7  This follows because  5  implies that mj and m mj are relatively prime, so we may take  Mj ≡ 0  modulo mk   for k ̸= j.  and  Mj =  m mj φ mj   by Euler’s theorem  exercise 1.2.4–28 . Now the number  u = a + u1M1 + u2M2 + ··· + urMr − a  mod m   8    9   satisfies all the conditions of  6 .   4.3.2  MODULAR ARITHMETIC  287  A very special case of this theorem was stated by the Chinese mathematician Sun Tsˇu, who gave a rule called tái-yen  “great generalization” . The date of his writing is very uncertain; it is thought to be between A.D. 280 and 473. Mathematicians in mediæval India developed the techniques further, with their methods of kut.t.aka  see Section 4.5.2 , but Theorem C was first stated and proved in its proper generality by Ch’in Chiu-Shao in his Shu Shu Chiu Chang  1247 ; the latter work considers also the case where the moduli might have common [See J. Needham, Science and Civilisation in China 3 factors as in exercise 3.  Cambridge University Press, 1959 , 33–34, 119–120; Y. Li and S. Du, Chinese Mathematics  Oxford: Clarendon, 1987 , 92–94, 105, 161–166; K. Shen, Archive for History of Exact Sciences 38  1988 , 285–305.] Numerous early contributions to this theory have been summarized by L. E. Dickson in his History of the Theory of Numbers 2  Carnegie Inst. of Washington, 1920 , 57–64.  As a consequence of Theorem C, we may use modular representation for numbers in any consecutive interval of m = m1m2 . . . mr integers. For example, we could take a = 0 in  6 , and work only with nonnegative integers u less than m. On the other hand, when addition and subtraction are being done, as well as multiplication, it is usually most convenient to assume that all of the moduli m1, m2, . . . , mr are odd numbers, so that m = m1m2 . . . mr is odd, and to work with integers in the range   10   − m  m 2 ,  2 < u < which is completely symmetrical about zero. In order to perform the basic operations listed in  2 ,  3 , and  4 , we need to compute  uj + vj  mod mj,  uj − vj  mod mj, and ujvj mod mj, when 0 ≤ uj, vj < mj. If mj is a single-precision number, it is most convenient to form ujvj mod mj by doing a multiplication and then a division operation. For addition and subtraction, the situation is a little simpler, since no division is necessary; the following formulas may conveniently be used:   uj + vj  mod mj = uj + vj − mj[uj + vj ≥ mj ].  uj − vj  mod mj = uj − vj + mj[uj < vj ].   11   12   See Section 3.2.1.1.  Since we want m to be as large as possible, it is easiest to let m1 be the largest odd number that fits in a computer word, to let m2 be the largest odd number < m1 that is relatively prime to m1, to let m3 be the largest odd number < m2 that is relatively prime to both m1 and m2, and so on until enough mj’s have been found to give the desired range m. Efficient ways to determine whether or not two integers are relatively prime are discussed in Section 4.5.2.  As a simple example, suppose that we have a decimal computer whose words hold only two digits, so that the word size is 100. Then the procedure described in the previous paragraph would give  m1 = 99, m2 = 97, m3 = 95, m4 = 91, m5 = 89, m6 = 83,   13   and so on.   288  ARITHMETIC  4.3.2  On binary computers it is sometimes desirable to choose the mj in a different  way, by selecting  mj = 2ej − 1.   14  In other words, each modulus is one less than a power of 2. Such a choice of mj often makes the basic arithmetic operations simpler, because it is relatively easy to work modulo 2ej − 1, as in ones’ complement arithmetic. When the moduli are chosen according to this strategy, it is helpful to relax the condition 0 ≤ uj < mj slightly, so that we require only  0 ≤ uj < 2ej ,  uj ≡ u  modulo 2ej − 1 .   15  Thus, the value uj = mj = 2ej −1 is allowed as an optional alternative to uj = 0; this does not affect the validity of Theorem C, and it means we are allowing uj to be any ej -bit binary number. Under this assumption, the operations of addition and multiplication modulo mj become the following:  uj ⊕ vj = uj + vj  mod 2ej + [uj + vj ≥ 2ej ].  uj ⊗ vj =  ujvj mod 2ej  ⊕ ⌊ujvj 2ej⌋.  Here ⊕ and ⊗ refer to the operations done on the individual components of convention  15 . Equation  12  is still good for subtraction, or we can use   u1, . . . , ur  and  v1, . . . , vr  when adding or multiplying, respectively, using the  uj ⊖ vj = uj − vj  mod 2ej − [uj < vj ].   18  These operations can be performed efficiently even when 2ej is larger than the computer’s word size, since it is a simple matter to compute the remainder of a positive number modulo a power of 2, or to divide a number by a power of 2. In  17  we have the sum of the “upper half” and the “lower half” of the product, as discussed in exercise 3.2.1.1–8. If moduli of the form 2ej − 1 are to be used, we must know under what conditions the number 2e−1 is relatively prime to the number 2f−1. Fortunately, there is a very simple rule:   16   17   gcd 2e − 1, 2f − 1  = 2gcd e,f  − 1.   19  This formula states in particular that 2e − 1 and 2f − 1 are relatively prime if and only if e and f are relatively prime. Equation  19  follows from Euclid’s algorithm and the identity   2e − 1  mod  2f − 1  = 2e mod f − 1.   20   See exercise 6.  On a computer with word size 232, we could therefore choose m1 = 232 − 1, m2 = 231 − 1, m3 = 229 − 1, m4 = 227 − 1, m5 = 225 − 1; this would permit efficient addition, subtraction, and multiplication of integers in a range of size m1m2m3m4m5 > 2143.  As we have already observed, the operations of conversion to and from modular representation are very important. If we are given a number u, its modular representation  u1, . . . , ur  may be obtained by simply dividing u by   4.3.2  MODULAR ARITHMETIC  289  m1, . . . , mr and saving the remainders. A possibly more attractive procedure, if u =  vmvm−1 . . . v0 b, is to evaluate the polynomial  . . .  vmb + vm−1 b + ··· b + v0  using modular arithmetic. When b = 2 and when the modulus mj has the special form 2ej −1, both of these methods reduce to quite a simple procedure: Consider the binary representation of u with blocks of ej bits grouped together,  u = atAt + at−1At−1 + ··· + a1A + a0,   21   where A = 2ej and 0 ≤ ak < 2ej for 0 ≤ k ≤ t. Then  u ≡ at + at−1 + ··· + a1 + a0  modulo 2ej − 1 ,   22  since A ≡ 1, so we obtain uj by adding the ej -bit numbers at ⊕ ··· ⊕ a1 ⊕ a0, using  16 . This process is similar to the familiar device of “casting out nines” that determines u mod 9 when u is expressed in the decimal system.  Conversion back from modular form to positional notation is somewhat more difficult. It is interesting in this regard to notice how the study of computation changes our viewpoint towards mathematical proofs: Theorem C tells us that the conversion from  u1, . . . , ur  to u is possible, and two proofs are given. The first proof we considered is a classical one that relies only on very simple concepts, namely the facts that i  any number that is a multiple of m1, of m2, . . . , and of mr, must be a multiple of m1m2 . . . mr when the mj’s are pairwise relatively prime; and ii  if m pigeons are put into m pigeonholes with no two pigeons in the same  hole, then there must be one in each hole.  By traditional notions of mathematical aesthetics, this is no doubt the nicest proof of Theorem C; but from a computational standpoint it is completely worthless. It amounts to saying, “Try u = a, a + 1, . . . until you find a value for which u ≡ u1  modulo m1 , . . . , u ≡ ur  modulo mr .” The second proof of Theorem C is more explicit; it shows how to compute r new constants M1, . . . , Mr, and to get the solution in terms of these constants by formula  9 . This proof uses more complicated concepts  for example, Euler’s theorem , but it is much more satisfactory from a computational standpoint, since the constants M1, . . . , Mr need to be determined only once. On the other hand, the determination of Mj by Eq.  8  is certainly not trivial, since the evaluation of Euler’s φ-function requires, in general, the factorization of mj into prime powers. There are much better ways to compute Mj than to use  8 ; in this respect we can see again the distinction between mathematical elegance and computational efficiency. But even if we find Mj by the best possible method, we’re stuck with the fact that Mj is a multiple of the huge number m mj. Thus,  9  forces us to do a lot of high-precision calculation, and such calculation is just what we wished to avoid by modular arithmetic in the first place.  So we need an even better proof of Theorem C if we are going to have a really usable method of conversion from  u1, . . . , ur  to u. Such a method was    24    25   290  ARITHMETIC  suggested by H. L. Garner in 1958; it can be carried out usingr  4.3.2   constants cij  2  for 1 ≤ i < j ≤ r, where  cij mi ≡ 1  modulo mj .   23  These constants cij are readily computed using Euclid’s algorithm, since for any given i and j Algorithm 4.5.2X will determine a and b such that ami + bmj = gcd mi, mj  = 1, and we may take cij = a. When the moduli have the special form 2ej − 1, a simple method of determining cij is given in exercise 6.  Once the cij have been determined satisfying  23 , we can set  v1 ← u1 mod m1, v2 ←  u2 − v1  c12 mod m2,  v3 ← u3 − v1  c13 − v2 vr ←. . .   ur − v1  c1r − v2  c2r − ··· − vr−1 ...   c23 mod m3,   c r−1 r mod mr.  Then  u = vrmr−1 . . . m2m1 + ··· + v3m2m1 + v2m1 + v1  is a number satisfying the conditions  0 ≤ u < m,  for 1 ≤ j ≤ r.  u ≡ uj  modulo mj    26   See exercise 8; another way of rewriting  24  that does not involve as many auxiliary constants is given in exercise 7.  Equation  25  is a mixed-radix repre- sentation of u, which can be converted to binary or decimal notation using the methods of Section 4.4. If 0 ≤ u < m is not the desired range, an appropriate multiple of m can be added or subtracted after the conversion process.  The advantage of the computation shown in  24  is that the calculation of vj can be done using only arithmetic mod mj, which is already built into the modular arithmetic algorithms. Furthermore,  24  allows parallel computation: We can start with  v1, . . . , vr  ←  u1 mod m1, . . . , ur mod mr , then at time j for 1 ≤ j < r we simultaneously set vk ←  vk − vj  cjk mod mk for j < k ≤ r. An alternative way to compute the mixed-radix representation, allowing similar possibilities for parallelism, has been discussed by A. S. Fraenkel, Proc. ACM Nat. Conf. 19  Philadelphia: 1964 , E1.4.  It is important to observe that the mixed-radix representation  25  is suffi- cient to compare the magnitudes of two modular numbers. For if we know that 0 ≤ u < m and 0 ≤ u′ < m, then we can tell if u < u′ by first doing the conversion to  v1, . . . , vr  and  v′ r, or if vr = v′ and vr−1 < v′ It is not necessary to convert all the way to binary or decimal notation if we only want to know whether  u1, . . . , ur  is less than  u′ The operation of comparing two numbers, or of deciding if a modular number is negative, is intuitively very simple, so we would expect to have a much easier way to make this test than the conversion to mixed-radix form. But the following  r−1, etc., according to lexicographic order.  r , then testing if vr < v′  1, . . . , u′ r .  1, . . . , v′  r   4.3.2  MODULAR ARITHMETIC  291  theorem shows that there is little hope of finding a substantially better method, since the range of a modular number depends essentially on all bits of all the residues  u1, . . . , ur : Theorem S  Nicholas Szabó, 1961 . that m1 <  In terms of the notation above, assume  m, and let L be any value in the range  √   27  Let g be any function such that the set {g 0 , g 1 , . . . , g m1−1 } contains fewer than m1 values. Then there are numbers u and v such that  m1 ≤ L ≤ m − m1.  for 2 ≤ j ≤ r;  u mod mj = v mod mj  0 ≤ u < L ≤ v < m.  g u mod m1  = g v mod m1 ,   28   29  Proof. By hypothesis, there must exist numbers u ̸= v satisfying  28 , since g must take on the same value for two different residues. Let  u, v  be a pair of values with 0 ≤ u < v < m satisfying  28 , for which u is a minimum. Since u′ = u − m1 and v′ = v − m1 also satisfy  28 , we must have u′ < 0 by the minimality of u. Hence u < m1 ≤ L; and if  29  does not hold, we must have v   u, and v − u is a multiple of m2 . . . mr = m m1, so v ≥ v − u ≥ m m1 > m1. Therefore, if  29  does not hold for  u, v , it will be satisfied for the pair  u′′, v′′  =  v − m1, u + m − m1 .  Of course, a similar result can be proved for any mj in place of m1; and we could also replace  29  by the condition “a ≤ u < a + L ≤ v < a + m” with only minor changes in the proof. Therefore Theorem S shows that many simple functions cannot be used to determine the range of a modular number.  Let us now reiterate the main points of the discussion in this section: Mod- ular arithmetic can be a significant advantage for applications in which the pre- dominant calculations involve exact multiplication  or raising to a power  of large integers, combined with addition and subtraction, but where there is very little need to divide or compare numbers, or to test whether intermediate results “overflow” out of range.  It is important not to forget the latter restriction; methods are available to test for overflow, as in exercise 12, but they are so complicated that they nullify the advantages of modular arithmetic.  Several applications of modular computations have been discussed by H. Takahasi and Y. Ishibashi, Information Proc. in Japan 1  1961 , 28–42.  An example of such an application is the exact solution of linear equations with rational coefficients. For various reasons it is desirable in this case to assume that the moduli m1, m2, . . . , mr are all prime numbers; the linear equations can be solved independently modulo each mj. A detailed discussion of this procedure has been given by I. Borosh and A. S. Fraenkel [Math. Comp. 20  1966 , 107– 112], with further improvements by A. S. Fraenkel and D. Loewenthal [J. Res. National Bureau of Standards 75B  1971 , 67–75]. By means of their method, the nine independent solutions of a system of 111 linear equations in 120 un- knowns were obtained exactly in less than 20 minutes on a CDC 1604 computer. The same procedure is worthwhile also for solving simultaneous linear equations   292  ARITHMETIC  4.3.2  with floating point coefficients, when the matrix of coefficients is ill-conditioned. The modular technique  treating the given floating point coefficients as exact rational numbers  gives a method for obtaining the true answers in less time than conventional methods can produce reliable approximate answers! [See M. T. McClellan, JACM 20  1973 , 563–588, for further developments of this approach; and see also E. H. Bareiss, J. Inst. Math. and Appl. 10  1972 , 68–104, for a discussion of its limitations.]  The published literature concerning modular arithmetic is mostly oriented towards hardware design, since the carry-free properties of modular arithmetic make it attractive from the standpoint of high-speed operation. The idea was first published by A. Svoboda and M. Valach in the Czechoslovakian journal Stroje na Zpracování Informací  Information Processing Machines  3  1955 , 247–295; then independently by H. L. Garner [IRE Trans. EC-8  1959 , 140– 147]. The use of moduli of the form 2ej − 1 was suggested by A. S. Fraenkel [JACM 8  1961 , 87–96], and several advantages of such moduli were demon- strated by A. Schönhage [Computing 1  1966 , 182–196]. See the book Residue Arithmetic and Its Applications to Computer Technology by N. S. Szabó and R. I. Tanaka  New York: McGraw–Hill, 1967 , for additional information and a comprehensive bibliography of the subject. A Russian book published in 1968 by I. Y. Akushsky and D. I. Yuditsky includes a chapter about complex moduli [see Rev. Roumaine de Math. Pures et Appl. 15  1970 , 159–160].  Further discussion of modular arithmetic can be found in Section 4.3.3B.  The notice-board had said he was in Room 423, but the numbering system, nominally consecutive, seemed to have been applied on a plan that could only have been the work of a lunatic or a mathematician. — ROBERT BARNARD, The Case of the Missing Brontë  1983   EXERCISES 1. [20] Find all integers u that satisfy all of the following conditions: u mod 7 = 1, u mod 11 = 6, u mod 13 = 5, 0 ≤ u < 1000. 2. [M20] Would Theorem C still be true if we allowed a, u1, u2, . . . , ur and u to be arbitrary real numbers  not just integers ?   cid:120  3. [M26]  Generalized Chinese Remainder Theorem.  Let m1, m2, . . . , mr be posi-  tive integers. Let m be the least common multiple of m1, m2, . . . , mr, and let a, u1, u2, . . . , ur be any integers. Prove that there is exactly one integer u that satisfies the conditions  a ≤ u < a + m,  u ≡ uj  modulo mj ,  1 ≤ j ≤ r,  provided that  ui ≡ uj   modulo gcd mi, mj  ,  1 ≤ i < j ≤ r; and there is no such integer u when the latter condition fails to hold. 4. [20] Continue the process shown in  13 ; what would m7, m8, m9, . . . be?   cid:120  5. [M23]  a  Suppose that the method of  13  is continued until no more mj can be  chosen. Does this “greedy” method give the largest attainable value m1m2 . . . mr such   4.3.2  MODULAR ARITHMETIC  293  that the mj are odd positive integers less than 100 that are relatively prime in pairs?  b  What is the largest possible m1m2 . . . mr when each residue uj must fit in eight bits of memory? 6. [M22] Let e, f, and g be nonnegative integers. a  Show that 2e ≡ 2f  modulo 2g − 1  if and only if e ≡ f  modulo g . b  Given that e mod f = d and ce mod f = 1, prove the identity    1 + 2d + ··· + 2 c−1 d  ·  2e − 1   mod  2f − 1  = 1.   Thus, we have a comparatively simple formula for the inverse of 2e − 1, modulo 2f − 1, as required in  23 .    cid:120  7. [M21] Show that  24  can be rewritten as follows:  v1 ← u1 mod m1, v2 ←  u2 − v1  c12 mod m2, v3 ←  u3 −  v1 + m1v2   c13c23 mod m3, ... vr ←  ur −  v1 + m1 v2 + m2 v3 + ··· + mr−2vr−1  . . .     c1r . . . c r−1 r mod mr.  If the formulas are rewritten in this way, we see that only r − 1 constants Cj = c1j . . . c j−1 j mod mj are needed instead of r r − 1  2 constants cij as in  24 . Discuss the relative merits of this version of the formula as compared to  24 , from the stand- point of computer calculation. 8. [M21] Prove that the number u defined by  24  and  25  satisfies  26 . 9. [M20] Show how to go from the values v1, . . . , vr of the mixed-radix notation  25  back to the original residues u1, . . . , ur, using only arithmetic mod mj to compute the value of uj. 10. [M25] An integer u that lies in the symmetrical range  10  might be represented by finding the numbers u1, . . . , ur such that u ≡ uj  modulo mj  and −mj 2 < uj < mj 2, instead of insisting that 0 ≤ uj < mj as in the text. Discuss the modular arithmetic procedures that would be appropriate in connection with such a symmetrical representation  including the conversion process,  24  . 11. [M23] Assume that all the mj are odd, and that u =  u1, . . . , ur  is known to be even, where 0 ≤ u < m. Find a reasonably fast method to compute u 2 using modular arithmetic. 12. [M10] Prove that, if 0 ≤ u, v < m, the modular addition of u and v causes overflow  lies outside the range allowed by the modular representation  if and only if the sum is less than u.  Thus the overflow detection problem is equivalent to the comparison problem.    cid:120  13. [M25]  Automorphic numbers.  An n-digit decimal number x > 1 is called an  “automorph” by recreational mathematicians if the last n digits of x2 are equal to x. [See Scientific For example, 9376 is a 4-digit automorph, since 93762 = 87909376. American 218, 1  January 1968 , 125.] a  Prove that an n-digit number x > 1 is an automorph if and only if x mod 5n = 0 or 1 and x mod 2n = 1 or 0, respectively.  Thus, if m1 = 2n and m2 = 5n, the only two n-digit automorphs are the numbers M1 and M2 in  7 .    294  ARITHMETIC  4.3.2  b  Prove that if x is an n-digit automorph, then  3x2 − 2x3  mod 102n is a 2n-digit c  Given that cx ≡ 1  modulo y , find a simple formula for a number c′ depending  automorph. on c and x but not on y, such that c′x2 ≡ 1  modulo y2 .   cid:120  14. [M30]  Mersenne multiplication.  The cyclic convolution of  x0, x1, . . . , xn−1  and   y0, y1, . . . , yn−1  is defined to be  z0, z1, . . . , zn−1 , where  zk =   i+j≡k  modulo n   xi yj,  for 0 ≤ k < n.  We will study efficient algorithms for cyclic convolution in Sections 4.3.3 and 4.6.4.  Consider q-bit integers u and v that are represented in the form  n−1  k=0  n−1  k=0  u =  uk2⌊kq n⌋,  v =  vk2⌊kq n⌋,  where 0 ≤ uk, vk < 2⌊ k+1 q n⌋−⌊kq n⌋.  This representation is a mixture of radices 2⌊q n⌋ and 2⌈q n⌉.  Suggest a good way to find the representation of  w =  uv  mod  2q − 1  ,  using an appropriate cyclic convolution. arithmetic.]  [Hint: Do not be afraid of floating point  *4.3.3. How Fast Can We Multiply? The conventional procedure for multiplication in positional number systems, Al- gorithm 4.3.1M, requires approximately cmn operations to multiply an m-place number by an n-place number, where c is a constant. In this section, let us assume for convenience that m = n, and let us consider the following question: Does every general computer algorithm for multiplying two n-place numbers require an execution time proportional to n2, as n increases?   In this question, a “general” algorithm means one that accepts, as input, the number n and two arbitrary n-place numbers in positional notation; the algorithm is supposed to output their product in positional form. Certainly if we were allowed to choose a different algorithm for each value of n, the question would be of no interest, since multiplication could be done for any specific value of n by a “table-lookup” operation in some huge table. The term “computer algorithm” is meant to imply an algorithm that is suitable for implementation on a digital computer like MIX, and the execution time is to be the time it takes to perform the algorithm on such a computer.  A. Digital methods. The answer to the question above is, rather surprisingly, “No,” and, in fact, it is not very difficult to see why. For convenience, let us assume throughout this section that we are working with integers expressed in binary notation. If we have two 2n-bit numbers u =  u2n−1 . . . u1u0 2 and v =  v2n−1 . . . v1v0 2, we can write  u = 2nU1 + U0,  v = 2nV1 + V0,   1    4.3.3  HOW FAST CAN WE MULTIPLY?  295  where U1 =  u2n−1 . . . un 2 is the “most significant half” of the number u and U0 =  un−1 . . . u0 2 is the “least significant half”; similarly V1 =  v2n−1 . . . vn 2 and V0 =  vn−1 . . . v0 2. Now we have  uv =  22n + 2n U1V1 + 2n U1 − U0  V0 − V1  +  2n + 1 U0V0.   2  This formula reduces the problem of multiplying 2n-bit numbers to three mul- tiplications of n-bit numbers, namely U1V1,  U1 − U0  V0 − V1 , and U0V0, plus some simple shifting and adding operations. Formula  2  can be used to multiply double-precision inputs when we want a quadruple-precision result, and it will be just a little faster than the traditional method on many machines. But the main advantage of  2  is that we can use it to define a recursive process for multiplication that is significantly faster than the familiar order-n2 method when n is large: If T n  is the time required to perform multiplication of n-bit numbers, we have T 2n  ≤ 3T n  + cn   3  for some constant c, since the right-hand side of  2  uses just three multiplications plus some additions and shifts. Relation  3  implies by induction that  k ≥ 1,  T 2k  ≤ c 3k − 2k ,   4  if we choose c to be large enough so that this inequality is valid when k = 1; therefore we have  T n  ≤ T2⌈lg n⌉ ≤ c3⌈lg n⌉ − 2⌈lg n⌉ < 3c · 3lg n = 3cnlg 3.   5  Relation  5  shows that the running time for multiplication can be reduced from order n2 to order nlg 3 ≈ n1.585, so the recursive method is much faster than the traditional method when n is large. Exercise 18 discusses an implementation of this approach.   A similar but slightly more complicated method for doing multiplication with running time of order nlg 3 was apparently first suggested by A. Karatsuba in Doklady Akad. Nauk SSSR 145  1962 , 293–294 [English translation in Soviet Physics–Doklady 7  1963 , 595–596]. Curiously, this idea does not seem to have been discovered before 1962; none of the “calculating prodigies” who have become famous for their ability to multiply large numbers mentally have been reported to use any such method, although formula  2  adapted to decimal notation would seem to lead to a reasonably easy way to multiply eight-digit numbers in one’s head.   The running time can be reduced still further, in the limit as n approaches infinity, if we observe that the method just used is essentially the special case r = 1 of a more general method that yields  T r + 1 n ≤  2r + 1 T n  + cn   6   for any fixed r. This more general method can be obtained as follows: Let  u =  u r+1 n−1 . . . u1u0 2  and  v =  v r+1 n−1 . . . v1v0 2   4.3.3   7    8   296  ARITHMETIC  be broken into r + 1 pieces,  u = Ur2rn + ··· + U12n + U0,  v = Vr2rn + ··· + V12n + V0,  where each Uj and each Vj is an n-bit number. Consider the polynomials V  x  = Vrxr + ··· + V1x + V0,  U x  = Urxr + ··· + U1x + U0,  and let  W x  = U x V  x  = W2rx2r + ··· + W1x + W0.   9  Since u = U 2n  and v = V  2n , we have uv = W 2n , so we can easily compute uv if we know the coefficients of W x . The problem is to find a good way to compute the coefficients of W x  by using only 2r + 1 multiplications of n- bit numbers plus some further operations that involve only an execution time proportional to n. This can be done by computing  . . . , U 2r V  2r  = W 2r .  U 0 V  0  = W 0 , U 1 V  1  = W 1 ,   10  The coefficients of a polynomial of degree 2r can be written as a linear com- bination of the values of that polynomial at 2r + 1 distinct points; computing such a linear combination requires an execution time at most proportional to n.  Actually, the products U j V  j  are not strictly products of n-bit numbers, but they are products of at most  n + t -bit numbers, where t is a fixed value depending on r. It is easy to design a multiplication routine for  n + t -bit numbers that requires only T n  + c1n operations, where T n  is the number of operations needed for n-bit multiplications, since two products of t-bit by n-bit numbers can be done in c2n operations when t is fixed.  Therefore we obtain a method of multiplication satisfying  6 . Relation  6  implies that T n  ≤ c3nlogr+1 2r+1  < c3n1+logr+1 2, if we argue  as in the derivation of  5 , so we have now proved the following result: Theorem A. Given ϵ > 0, there exists a multiplication algorithm such that the number of elementary operations T n  needed to multiply two n-bit numbers satisfies  T n  < c ϵ n1+ϵ,   11   for some constant c ϵ  independent of n.  This theorem is still not the result we are after.  It is unsatisfactory for practical purposes because the method becomes quite complicated as ϵ → 0  and therefore as r → ∞ , causing c ϵ  to grow so rapidly that extremely huge values of n are needed before we have any significant improvement over  5 . And it is unsatisfactory for theoretical purposes because it does not make use of the full power of the polynomial method on which it is based. We can obtain a better result if we let r vary with n, choosing larger and larger values of r as n increases. This idea is due to A. L. Toom [Doklady Akad. Nauk SSSR 150  1963 , 496– 498, English translation in Soviet Mathematics 4  1963 , 714–716], who used it to show that computer circuitry for the multiplication of n-bit numbers can be   4.3.3  HOW FAST CAN WE MULTIPLY?  297  constructed with a fairly small number of components as n grows. S. A. Cook [On the Minimum Computation Time of Functions  Thesis, Harvard University, 1966 , 51–77] showed later that Toom’s method can be adapted to fast computer programs.  Before we discuss the Toom–Cook algorithm any further, let us study a small example of the transition from U x  and V  x  to the coefficients of W x . This example will not demonstrate the efficiency of the method, since the numbers are too small, but it reveals some useful simplifications that we can make in the general case. Suppose that we want to multiply u = 1234 times v = 2341; in binary notation this is  u =  0100 1101 0010 2 times v =  1001 0010 0101 2.   12   Let r = 2; the polynomials U x  and V  x  in  8  are  U x  = 4x2 + 13x + 2,  V  x  = 9x2 + 2x + 5.  Hence we find, for W x  = U x V  x ,  U 0  = 2, U 1  = 19, U 2  = 44, U 3  = 77, U 4  = 118; V  0  = 5, V  1  = 16, V  2  = 45, V  3  = 92, V  4  = 157; W 0  = 10, W 1  = 304, W 2  = 1980, W 3  = 7084, W 4  = 18526.   13   Our job is to compute the five coefficients of W x  from the latter five values. An attractive little algorithm can be used to compute the coefficients of a polynomial W x  = Wm−1xm−1 +··· + W1x + W0 when the values W 0 , W 1 , . . . , W m − 1  are given. Let us first write  W x  = am−1 xm−1 + am−2 xm−2 + ··· + a1x1 + a0,   14  where xk = x x − 1  . . .  x − k + 1 , and where the coefficients aj are unknown. The falling factorial powers have the important property that  W x + 1  − W x  =  m − 1 am−1 xm−2 +  m − 2 am−2 xm−3 + ··· + a1;    hence by induction we find that, for all k ≥ 0, 1 k!  W x+k−1  +  W x+k  − k    m − 1    k   m − 2  2  1  am−1 xm−1−k +  =  W x+k−2  − ··· +  −1 kW x    k    am−2 xm−2−k + ··· +  ak.   15   k  k Denoting the left-hand side of  15  by  1 k!  ∆k W x , we see that  k  1 k! ∆k W x  = 1  k  1   k − 1 ! ∆k−1W x + 1  −   k − 1 ! ∆k−1W x   1         298  ARITHMETIC  4.3.3  and  1 k!  ∆k W 0  = ak. So the coefficients aj can be evaluated using a very simple method, illustrated here for the polynomial W x  in  13 :   16   144 4 = 36  294 1676 5104 11442  1023 3 = 341 1455 3 = 485  1382 2 = 691 3428 2 = 1714 6338 2 = 3169  10 304 1980 7084 18526 The leftmost column of this tableau is a listing of the given values of W 0 , W 1 , . . . , W 4 ; the kth succeeding column is obtained by computing the difference between successive values of the preceding column and dividing by k. The coefficients aj appear at the top of the columns, so that a0 = 10, a1 = 294, . . . , a4 = 36, and we have  W x  = 36x4 + 341x3 + 691x2 + 294x1 + 10  =  36 x − 3  + 341  x − 2  + 691  x − 1  + 294x + 10. W x =. . .   am−1 x−m+2 +am−2  x−m+3 +am−3  x−m+4 +···+a1  In general, we can write  x+a0,   17   and this formula shows how the coefficients Wm−1, . . . , W1, W0 can be obtained from the a’s:  36  36  36  36  341 −3 · 36 233 −2 · 36 161 −1 · 36 125  691 −2 · 233 225 −1 · 161 64   18   294 −1 · 225 69  10  Here the numbers below the horizontal lines successively show the coefficients of the polynomials  am−1, am−1 x − m + 2  + am−2,  am−1 x − m + 2  + am−2   x − m + 3  + am−3,  etc.  From this tableau we have  W x  = 36x4 + 125x3 + 64x2 + 69x + 10,   19  so the answer to our original problem is 1234 · 2341 = W 16  = 2888794, where W 16  is obtained by adding and shifting. A generalization of this method for obtaining coefficients is discussed in Section 4.6.4.  The basic Stirling number identity of Eq. 1.2.6– 45 ,   n    n   n    1   n    ,  0  xn =  xn + ··· +  x1 +   4.3.3  HOW FAST CAN WE MULTIPLY?  299  shows that if the coefficients of W x  are nonnegative, so are the numbers aj, and in such a case all of the intermediate results in the computation above are nonnegative. This further simplifies the Toom–Cook multiplication algorithm, which we will now consider in detail.  Impatient readers should, however, skip to subsection C below.  Algorithm T  High-precision multiplication of binary numbers . Given a pos- itive integer n and two nonnegative n-bit integers u and v, this algorithm forms their 2n-bit product, w. Four auxiliary stacks are used to hold the long numbers that are manipulated during the procedure:  Stacks U, V : Stack C: Stack W:  Temporary storage of U j  and V  j  in step T4. Numbers to be multiplied, and control codes. Storage of W j .  These stacks may contain either binary numbers or special control symbols called code-1, code-2, and code-3. The algorithm also constructs an auxiliary table of numbers qk, rk; this table is maintained in such a manner that it may be stored as a linear list, where a single pointer that traverses the list  moving back and forth  can be used to access the current table entry of interest.   Stacks C and W are used to control the recursive mechanism of this multi- plication algorithm in a reasonably straightforward manner that is a special case of general procedures discussed in Chapter 8.  T1. [Compute q, r tables.] Set stacks U, V, C, and W empty. Set  Q ← 4,  R ← 2.  k ← 1,  q0 ← q1 ← 16,  Now if qk−1 + qk < n, set  r0 ← r1 ← 4,  R ← ⌊Q⌋,  rk ← 2R,  qk ← 2Q,  Q ← Q + R,  and repeat this operation until qk−1 + qk ≥ n. Note: The calculation of k ← k + 1, R ← ⌊√ Q⌋ does not require a square root to be taken, since we may simply set R ← R + 1 if  R + 1 2 ≤ Q and leave R unchanged if  R + 1 2 > Q; see exercise 2. In this step we build the sequences 5 213 23  k = 0 qk = 24 rk = 22  4 210 23  6 216 24  3 28 22  1 24 22  2 26 22  . . .  . . .  . . .  The multiplication of 70000-bit numbers would cause this step to terminate  with k = 6, since 70000 < 213 + 216.  T2. [Put u, v on stack.] Put code-1 on stack C, then place u and v onto stack C  as numbers of exactly qk−1 + qk bits each.  T3. [Check recursion level.] Decrease k by 1. If k = 0, the top of stack C now contains two 32-bit numbers, u and v; remove them, set w ← uv using a built-in routine for multiplying 32-bit numbers, and go to step T10. If k > 0, set r ← rk, q ← qk, p ← qk−1 + qk, and go on to step T4.   300  ARITHMETIC  4.3.3  Fig. 8. The Toom–Cook algorithm for high-precision multiplication.  T4. [Break into r + 1 parts.] Let the number at the top of stack C be regarded as a list of r + 1 numbers with q bits each,  Ur . . . U1U0 2q.  The top of stack C now contains an  r + 1 q =  qk + qk+1 -bit number.  For j = 0, 1, . . . , 2r, compute the p-bit numbers  . . .  Urj + Ur−1 j + ··· + U1  j + U0 = U j   and successively put these values onto stack U.  The bottom of stack U now contains U 0 , then comes U 1 , etc., with U 2r  on top. We have  U j  ≤ U 2r  < 2q 2r r +  2r r−1 + ··· + 1 < 2q+1 2r r ≤ 2p,  by exercise 3.  Then remove Ur . . . U1U0 from stack C.  Now the top of stack C contains another list of r + 1 q-bit numbers,  Vr . . . V1V0, and the p-bit numbers  . . .  Vrj + Vr−1 j + ··· + V1  j + V0 = V  j   should be put onto stack V in the same way. After this has been done, remove Vr . . . V1V0 from stack C.  T5. [Recurse.] Successively put the following items onto stack C, at the same  time emptying stacks U and V :  code-2, V  2r , U 2r , code-3, V  2r − 1 , U 2r − 1 , . . . ,  code-3, V  1 , U 1 , code-3, V  0 , U 0 .  Go back to step T3.  T6. [Save one product.] At this point the multiplication algorithm has set w to one of the products W j  = U j V  j . Put w onto stack W.  This  number w contains 2 qk + qk−1  bits.  Go back to step T3.  T7. [Find a’s.] Set r ← rk, q ← qk, p ← qk−1 + qk.  At this point stack W contains a sequence of numbers ending with W 0 , W 1 , . . . , W 2r  from bottom to top, where each W j  is a 2p-bit number.   T1. Compute  q, r tables  T2. Put u, v  on stack  T3. Check  recursion level  k = 0  k > 0  T4. Break into  r+1 parts  T5. Recurse  T6. Save  one product  T7. Find a’s  T8. Find W ’s  T9. Set answer  T10. Return  code-3  code-1  code-2   4.3.3  301  HOW FAST CAN WE MULTIPLY?  Now for j = 1, 2, 3, . . . , 2r, perform the following loop: For t = 2r,  2r − 1, 2r − 2, . . . , j, set W t  ← W t  − W t − 1  j. Here j must increase and t must decrease. The quantityW t −W t−1  j will always be a nonnegative integer that fits in 2p bits; see  16 . t = j, j + 1, . . . , 2r − 1, set W t  ← W t  − jW t + 1 . Here j must nonnegative 2p-bit integer; see  18 .  decrease and t must increase. The result of this operation will again be a  T8. [Find W’s.] For j = 2r − 1, 2r − 2, . . . , 1, perform the following loop: For  T9. [Set answer.] Set w to the 2 qk + qk+1 -bit integer  . . .W 2r 2q + W 2r − 1 2q + ··· + W 1 2q + W 0 .  Remove W 2r , . . . , W 0  from stack W.  T10. [Return.] Set k ← k + 1. Remove the top of stack C. If it is code-3, go to step T6. If it is code-2, put w onto stack W and go to step T7. And if it is code-1, terminate the algorithm  w is the answer . Let us now estimate the running time, T n , for Algorithm T, in terms of some things we shall call “cycles,” that is, elementary machine operations. Step T1 takes O qk  cycles, even if we represent the number qk internally as a long string of qk bits followed by some delimiter, since qk + qk−1 + ··· + q0 will be O qk . Step T2 obviously takes O qk  cycles. Now let tk denote the amount of computation required to get from step T3 to step T10 for a particular value of k  after k has been decreased at the beginning of step T3 . Step T3 requires O q  cycles at most. Step T4 involves r multiplications of q-bit numbers by  lg 2r -bit numbers, and r additions of p-bit numbers, all repeated 4r + 2 times. Thus we need a total of O r2q log r  cycles. Step T5 requires moving 4r+2 p-bit numbers, so it involves O rq  cycles. Step T6 requires O q  cycles, and it is done 2r + 1 times per iteration. The recursion involved when the algorithm essentially invokes itself  by returning to step T3  requires tk−1 cycles, 2r + 1 times. Step T7 requires O r2  subtractions of p-bit numbers and divisions of 2p-bit by  lg 2r -bit numbers, so it requires O r2q log r  cycles. Similarly, step T8 requires O r2q log r  cycles. Step T9 involves O rq  cycles, and T10 takes hardly any time at all.  Summing up, we have T n  = O qk  + O qk  + tk−1, where  if q = qk and  r = rk  the main contribution to the running time satisfies  tk = O q  + O r2q log r  + O rq  +  2r + 1 O q  + O r2q log r   + O r2q log r  + O rq  + O q  +  2r + 1 tk−1  = O r2q log r  +  2r + 1 tk−1. Thus there is a constant c such that  To complete the estimation of tk we can prove by brute force that  tk ≤ cr2  kqk lg rk +  2rk + 1 tk−1.  tk ≤ C qk+122.5  √  lg qk+1   20    302  ARITHMETIC  4.3.3  for some constant C. Let us choose C > 20c, and let us also take C large enough so that  20  is valid for k ≤ k0, where k0 will be specified below. Then when k > k0, let Qk = lg qk, Rk = lg rk; we have by induction Qk = C qk+122.5  k lg rk +  2rk + 1 C qk22.5  lg qk+1 η1 + η2 ,  tk ≤ cqkr2  √  √  where  since  Thus  η1 = c C    √    Rk2Rk−2.5 2 + 1 rk √  22.5   Qk =  Qk+1 −    η2 = √  Qk+1 < Qk−√ √  1 20 Rk2−Rk < 0.05, Qk+1  → 2−1 4 < 0.85,  √ Qk + ⌊  Qk⌋ −  √  Qk → 1 2  as k → ∞. It follows that we can find k0 such that η2   k0, and this completes the proof of  20  by induction. Finally, therefore, we are ready to estimate T n . Since n > qk−1 + qk−2, rk−1 = 2⌊√  √ qk = rk−1qk−1 < n2  we have qk−1 < n; hence  lg qk−1⌋ < 2  lg n,  lg n.  and  √  tk−1 ≤ C qk22.5  √  √ lg qk < C n2  √  lg n +2.5   lg n +1 ,  and, since T n  = O qk  + tk−1, we have derived the following theorem: Theorem B. There is a constant c0 such that the execution time of Algorithm T is less than c0 n23.5  √  Since n23.5  lg n, this result is noticeably stronger than The- orem A. By adding a few complications to the algorithm, pushing the ideas to their apparent limits  see exercise 5 , we can improve the estimated execution time to  √  lg n cycles. √ lg n = n1+3.5   √ T n  = O n2  2 lg n log n .   21   *B. A modular method. There is another way to multiply large numbers very rapidly, based on the ideas of modular arithmetic as presented in Section 4.3.2. It is very hard to believe at first that this method can be of advantage, since a multiplication algorithm based on modular arithmetic must include the choice of moduli and the conversion of numbers into and out of modular representation, besides the actual multiplication operation itself. In spite of these formidable difficulties, A. Schönhage discovered that all of these operations can be carried out quite rapidly.  In order to understand the essential mechanism of Schönhage’s method, we  shall look at a special case. Consider the sequence defined by the rules  qk+1 = 3qk − 1,  22  so that qk = 3k − 3k−1 − ··· − 1 = 1 2 3k + 1 . We will study a procedure that multiplies pk-bit numbers, where pk =  18qk + 8 , in terms of a method  q0 = 1,   4.3.3  HOW FAST CAN WE MULTIPLY?  303  for multiplying pk−1-bit numbers. Thus, if we know how to multiply numbers having p0 = 26 bits, the procedure to be described will show us how to multiply numbers of p1 = 44 bits, then 98 bits, then 260 bits, etc., eventually increasing the number of bits by almost a factor of 3 at each step.  When multiplying pk-bit numbers, the idea is to use the six moduli m3 = 26qk+2 − 1, m6 = 26qk+7 − 1.  m1 = 26qk−1 − 1, m4 = 26qk+3 − 1,  m2 = 26qk+1 − 1, m5 = 26qk+5 − 1,   23   These moduli are relatively prime, by Eq. 4.3.2– 19 , since the exponents  6qk − 1,  6qk + 1,  6qk + 3,  6qk + 2,   24  are always relatively prime  see exercise 6 . The six moduli in  23  are capable of representing numbers up to m = m1m2m3m4m5m6 > 236qk+16 = 22pk, so there is no chance of overflow in the multiplication of pk-bit numbers u and v. Thus we can use the following method, when k > 0: a  Compute u1 = u mod m1, . . . , u6 = u mod m6; and v1 = v mod m1, . . . ,  6qk + 5,  6qk + 7  v6 = v mod m6.  b  Multiply u1 by v1, u2 by v2, . . . , u6 by v6. These are numbers of at most 6qk + 7 = 18qk−1 + 1 < pk−1 bits, so the multiplications can be performed by using the assumed pk−1-bit multiplication procedure.  c  Compute w1 = u1v1 mod m1, w2 = u2v2 mod m2, . . . , w6 = u6v6 mod m6. d  Compute w such that 0 ≤ w < m, w mod m1 = w1, . . . , w mod m6 = w6.  Let tk be the amount of time needed for this process. It is not hard to see that operation  a  takes O pk  cycles, since the determination of u mod  2e−1  is quite simple  like “casting out nines” , as shown in Section 4.3.2. Similarly, operation  c  takes O pk  cycles. Operation  b  requires essentially 6tk−1 cycles. This leaves us with operation  d , which seems to be quite a difficult computation; but Schönhage has found an ingenious way to perform step  d  in O pk log pk  cycles, and this is the crux of the method. As a consequence, we have  tk = 6tk−1 + O pk log pk .  Since pk = 3k+2 + 17, we can show that the time for n-bit multiplication is  T n  = O nlog3 6  = O n1.631 .   25    See exercise 7.   Although the modular method is more complicated than the O nlg 3  pro- cedure discussed at the beginning of this section, Eq.  25  shows that it does, in fact, lead to an execution time substantially better than O n2  for the multi- plication of n-bit numbers. Thus we have seen how to improve on the classical method by using either of two completely different approaches. positive integers e1 < e2 < ··· < er, relatively prime in pairs; let  Let us now analyze operation  d  above. Assume that we are given a set of  m1 = 2e1 − 1,  m2 = 2e2 − 1,  . . . ,  mr = 2er − 1.   26    ARITHMETIC  304 4.3.3 We are also given numbers w1, . . . , wr such that 0 ≤ wj ≤ mj. Our job is to determine the binary representation of the number w that satisfies the conditions  0 ≤ w < m1m2 . . . mr,   27    28   w ≡ w1  modulo m1 ,  . . . ,  w ≡ wr  modulo mr .  The method is based on  24  and  25  of Section 4.3.2. First we compute  w′  j =. . .   wj − w′ w =. . .  w′  for j = 2, . . . , r, where w′  2  c2j − ··· − w′  1  c1j − w′ 1 = w1 mod m1; then we compute rmr−1 + w′  r−1  mr−2 + ··· + w′ 2  j−1   c j−1 j mod mj,  m1 + w′  2  1.   29  Here cij is a number such that cijmi ≡ 1  modulo mj ; these numbers cij are not given, they must be determined from the ej’s. 2  The calculation of  28  for all j involves r of which takes O er  cycles, plus r   additions modulo mj, each  multiplications by cij, modulo mj. The  calculation of w by formula  29  involves r additions and r multiplications by mj; it is easy to multiply by mj, since this is just adding, shifting, and subtracting, so it is clear that the evaluation of Eq.  29  takes O r2er  cycles. We will soon see that each of the multiplications by cij, modulo mj, requires only O er log er  cycles, and therefore it is possible to complete the entire job of conversion in O r2er log er  cycles.  These observations leave us with the following problem to solve: Given relatively prime positive integers e and f with e < f, and a nonnegative integer u < 2f, compute the value of  cu  mod  2f − 1 , where c is the number such that  2e − 1 c ≡ 1  modulo 2f − 1 ; this entire computation must be done in O f log f  cycles. The result of exercise 4.3.2–6 gives a formula for c that suggests a suitable procedure. First we find the least positive integer b such that  Euclid’s algorithm will discover b in O log f 3 cycles, since it requires O log f  iterations when applied to e and f, and each iteration requires O log f 2 cycles.  be ≡ 1  modulo f .  Alternatively, we could be very sloppy here without violating the total time constraint, by simply trying b = 1, 2, etc., until  30  is satisfied; such a process would take O f log f  cycles in all. Once b has been found, exercise 4.3.2–6 tells us that   30   c = c[b] =  2je  mod  2f − 1 .   31        0≤j<b  A brute-force multiplication of  cu  mod  2f − 1  would not be good enough to solve the problem, since we do not know how to multiply general f-bit numbers in O f log f  cycles. But the special form of c provides a clue: The binary representation of c is composed of bits in a regular pattern, and Eq.  31  shows that the number c[2b] can be obtained in a simple way from c[b]. This suggests   4.3.3  HOW FAST CAN WE MULTIPLY?  305  that we can rapidly multiply a number u by c[b] if we build c[b]u up in lg b steps in a suitably clever manner, such as the following: Suppose b is  b =  bs . . . b2b1b0 2  in binary notation; we can calculate four sequences ak, dk, uk, vk defined by  a0 = e, d0 = b0e, u0 = u, v0 = b0u,  ak = 2ak−1 mod f; dk =  dk−1 + bk ak  mod f; uk =  uk−1 + 2ak−1 uk−1  mod  2f − 1 ; vk =  vk−1 + bk 2dk−1 uk  mod  2f − 1 .   32   It is easy to prove by induction on k that  ak =  2ke  mod f;  dk = bk . . . b1b0 2 e mod f;  uk =  c[2k]u  mod  2f − 1 ;  vk =c[ bk . . . b1b0 2]u mod  2f − 1 .   33   Hence the desired result,  c[b]u  mod  2f − 1 , is vs. The calculation of ak, dk, uk, and vk from ak−1, dk−1, uk−1, vk−1 takes O log f  + O log f  + O f  + O f  = O f  cycles; consequently the entire calculation can be done in s O f  = O f log f  cycles as desired.  The reader will find it instructive to study the ingenious method represented by  32  and  33  very carefully. Similar techniques are discussed in Section 4.6.3. Schönhage’s paper [Computing 1  1966 , 182–196] shows that these ideas can be extended to the multiplication of n-bit numbers using r ≈ 2 2 lg n moduli, obtaining a method analogous to Algorithm T. We shall not dwell on the details here, since Algorithm T is always superior; in fact, an even better method is next on our agenda. C. Discrete Fourier transforms. The critical problem in high-precision multiplication is the determination of “convolution products” such as  √  urv0 + ur−1v1 + ··· + u0vr,   34  and there is an intimate relation between convolutions and an important math- ematical concept called “Fourier transformation.” If ω = exp 2πi K  is a Kth root of unity, the one-dimensional Fourier transform of the sequence of complex numbers  u0, u1, . . . , uK−1  is defined to be the sequence  ˆu0, ˆu1, . . . , ˆuK−1 , where  ωstut,  0 ≤ s < K.  ˆus =   0≤t<K  Letting  ˆv0, ˆv1, . . . , ˆvK−1  be defined in the same way, as the Fourier transform of  v0, v1, . . . , vK−1 , it is not difficult to see that  ˆu0ˆv0, ˆu1ˆv1, . . . , ˆuK−1ˆvK−1  is the transform of  w0, w1, . . . , wK−1 , where  wr = urv0 + ur−1v1 + ··· + u0vr + uK−1vr+1 + ··· + ur+1vK−1   35    36     =  i+j≡r  modulo K   uivj.   ARITHMETIC  306 4.3.3 When K ≥ 2n − 1 and un = un+1 = ··· = uK−1 = vn = vn+1 = ··· = vK−1 = 0, the w’s are just what we need for multiplication, since the terms uK−1vr+1 + ··· + ur+1vK−1 vanish when 0 ≤ r ≤ 2n − 2. In other words, the transform of a convolution product is the ordinary product of the transforms.  This idea is actually a special case of Toom’s use of polynomialssee  10 , with  x replaced by roots of unity.  If K is a power of 2, the discrete Fourier transform  35  can be obtained quite rapidly when the computations are arranged in a certain way, and so can the inverse transform  determining the w’s from the ˆw’s . This property of Fourier transforms was exploited by V. Strassen in 1968, who discovered how to multiply large numbers faster than was possible under all previously known schemes. He and A. Schönhage later refined the method and published improved procedures in Computing 7  1971 , 281–292. Similar ideas, but with all-integer methods, had been worked out independently by J. M. Pollard [Math. Comp. 25  1971 , 365–374]. In order to understand their approach to the problem, let us first take a look at the mechanism of fast Fourier transforms.  Given a sequence of K = 2k complex numbers  u0, . . . , uK−1 , and given the  complex number  ω = exp 2πi K ,   37  the sequence  ˆu0, . . . , ˆuK−1  defined in  35  can be calculated rapidly by carrying out the following scheme.  In these formulas the parameters sj and tj are either 0 or 1, so that each “pass” represents 2k elementary computations.  Pass 0. Let A[0] tk−1, . . . , t0  = ut, where t =  tk−1 . . . t0 2. Pass 1. Set A[1] sk−1, tk−2, . . . , t0  ←  A[0] 0, tk−2, . . . , t0  + ω2k−1sk−1A[0] 1, tk−2, . . . , t0 .  Pass 2. Set A[2] sk−1, sk−2, tk−3, . . . , t0  ←  A[1] sk−1, 0, tk−3, . . . , t0  + ω2k−2 sk−2sk−1 2 A[1] sk−1, 1, tk−3, . . . , t0 .  . . .  Pass k. Set A[k] sk−1, . . . , s1, s0  ←  A[k−1] sk−1, . . . , s1, 0  + ω s0s1...sk−1 2 A[k−1] sk−1, . . . , s1, 1 .  It is fairly easy to prove by induction that we have  A[j] sk−1, . . . , sk−j, tk−j−1, . . . , t0  =   where t =  tk−1 . . . t1t0 2, so that  0≤tk−1,...,tk−j≤1  ω2k−j sk−j ...sk−1 2  tk−1...tk−j 2 ut,  38   A[k] sk−1, . . . , s1, s0  = ˆus,  where s =  s0s1 . . . sk−1 2.   39    It is important to notice that the binary digits of s are reversed in the final result  39 . Section 4.6.4 contains further discussion of transforms such as this.    4.3.3  HOW FAST CAN WE MULTIPLY?  307    To get the inverse Fourier transform  u0, . . . , uK−1  from the values of   ˆu0, . . . , ˆuK−1 , notice that the “double transform” is  ˆˆur =   ωrsˆus =  =  since the geometric series  0≤s<K  0≤s,t<K  ωrsωstut     ut  ωs t+r   = Ku −r  mod K,   40   0≤s<K  0≤t<K 0≤s<K ωsj sums to zero unless j is a multiple of K. Therefore the inverse transform can be computed in the same way as the trans- form itself, except that the final results must be divided by K and shuffled slightly.  Returning to the problem of integer multiplication, suppose we wish to compute the product of two n-bit integers u and v. As in Algorithm T we shall work with groups of bits; let 2n ≤ 2k l < 4n,  K = 2k,  L = 2l,   41   and write  v =  VK−1 . . . V1V0 L,  u =  UK−1 . . . U1U0 L,   42  regarding u and v as K-place numbers in radix L so that each digit Uj or Vj is an l-bit integer. Actually the leading digits Uj and Vj are zero for all j ≥ K 2, because 2k−1l ≥ n. We will select appropriate values for k and l later; at the moment our goal is to see what happens in general, so that we can choose k and l intelligently when all the facts are before us.  The next step of the multiplication procedure is to compute the Fourier transforms  ˆu0, . . . , ˆuK−1  and  ˆv0, . . . , ˆvK−1  of the sequences  u0, . . . , uK−1  and  v0, . . . , vK−1 , where we define ut = Ut 2k+l,   43  This scaling is done for convenience so that each ut and vt is less than 2−k, ensuring that the absolute values ˆus and ˆvs will be less than 1 for all s.  vt = Vt 2k+l.  An obvious problem arises here, since the complex number ω can’t be represented exactly in binary notation. How are we going to compute a reliable Fourier transform? By a stroke of good luck, it turns out that everything will work properly if we do the calculations with only a modest amount of precision. For the moment let us bypass this question and assume that infinite-precision calculations are being performed; we shall analyze later how much accuracy is actually needed. Once the ˆus and ˆvs have been found, we let ˆws = ˆusˆvs for 0 ≤ s < K and determine the inverse Fourier transform  w0, . . . , wK−1 . As explained above, we now have  wr =   uivj =   i+j=r  i+j=r  UiVj 22k+2l,   308  ARITHMETIC  4.3.3  Table 1  MULTIPLICATION VIA DISCRETE FOURIER TRANSFORMATION  27ˆvs 16  27ˆus 19  214 ˆws 304  −2 + 13i  s 0 1 2 + 4i + 13ω 5 + 9i + 2ω −26 + 64i + 69ω − 125 ¯ω 2 3 2 − 4i − 13 ¯ω 5 − 9i − 2¯ω −26 − 64i + 125ω − 69 ¯ω 4 5 2 + 4i − 13ω 5 + 9i − 2ω −26 + 64i − 69ω + 125 ¯ω 6 7 2 − 4i + 13 ¯ω 5 − 9i + 2¯ω −26 − 64i − 125ω + 69 ¯ω  −18 + 56i  −18 − 56i  −2 − 13i  −4 + 2i  −4 − 2i  −84  −7  12  214 ˆˆws 80 0 0 0 288 1000 512 552  214ws = Ws  10 69 64 125 36 0 0 0  so the integers Wr = 22k+2lwr are the coefficients in the desired product  u · v = WK−2 LK−2 + ··· + W1L + W0.   44  Since 0 ≤ Wr <  r + 1 L2 < KL2, each Wr has at most k + 2l bits, so it will not be difficult to compute the binary representation when the W’s are known unless k is large compared to l.  For example, suppose we want to multiply u = 1234 times v = 2341 when the parameters are k = 3 and l = 4. The computation of  ˆu0, . . . , ˆu7  from u  proceeds as followssee  12 :   r, s, t  =  0, 0, 0   0, 0, 1   0, 1, 0    0, 1, 1   1, 0, 0   1, 0, 1   1, 1, 0   1, 1, 1   0 2  0 4  2 + 4i  4 4 −2  27A[0] r, s, t  = 2 27A[1] r, s, t  = 2 27A[2] r, s, t  = 6 27A[3] r, s, t  = 19  0 13 13 13 13 13 −7 −2 + 13i −2 − 13i α + β α − β  0 0 13 √ Here α = 2 + 4i, β = 13ω, and ω =  1 + i   2; this gives us the column headed 27 ˆus in Table 1. The column for ˆvs is obtained from v in the same way; then we multiply ˆus by ˆvs to get ˆws. Transforming again gives us ws and Ws, using relation  40 . Once again we obtain the convolution products in  19 , this time using complex numbers instead of sticking to an all-integer method.  0 0 13 ¯α + ¯β  2 − 4i ¯α − ¯β  Let us try to estimate how much time this method takes on large numbers, if m-bit fixed point arithmetic is used in calculating the Fourier transforms. Exercise 10 shows that all of the quantities A[j] during all the passes of the transform calculations will be less than 1 in magnitude because of the scaling  43 , hence it suffices to deal with m-bit fractions  .a−1 . . . a−m 2 for the real and imaginary parts of all the intermediate quantities. Simplifications are possible because the inputs ut and vt are real-valued; only K real values instead of 2K need to be carried in each step  see exercise 4.6.4–14 . We will ignore such refinements in order to keep complications to a minimum.  The first job is to compute ω and its powers. For simplicity we shall make  a table of the values ω0, . . . , ωK−1. Let  ωr = exp 2πi 2r ,   45     1 + xr  ,  2  xr+1 =  4.3.3 HOW FAST CAN WE MULTIPLY? √ so that ω1 = −1, ω2 = i, ω3 =  1 + i   r ≥ 2, we have ωr+1 = xr+1 + iyr+1 where  2, . . . , ωk = ω. If ωr = xr + iyr and  309  yr+1 = yr 2xr+1  .   46   [See S. R. Tate, IEEE Transactions SP-43  1995 , 1709–1711.] The calculation of ω1, ω2, . . . , ωk takes negligible time compared with the other computations we need, so we can use any straightforward algorithm for square roots. Once the ωr have been calculated we can compute all of the powers ω j by noting that  k  1  . . . ωj1  k−1ωj0  ω j = ωjk−1  if j =  jk−1 . . . j1j0 2.   47  This method of calculation keeps errors from propagating, since each ω j is a product of at most k of the ωr’s. The total time to calculate all the ω j is O KM , where M is the time to do an m-bit complex multiplication, because only one multiplication is needed to obtain each ω j from a previously computed value. The subsequent steps will require more than O KM  cycles, so the powers of ω have been computed at negligible cost. Each of the three Fourier transformations comprises k passes, each of which involves K operations of the form a ← b + ω jc, so the total time to calculate the Fourier transforms is  OK k + l  = O n+ nk l . Summing over all operations, we find that the total  Finally, the work involved in computing the binary digits of u · v using  44  is  O kKM  = O M nk l .  time to multiply n-bit numbers u and v will be O n  + O M nk l .  Now let’s see how large the intermediate precision m needs to be, so that we know how large M needs to be. For simplicity we shall be content with safe estimates of the accuracy, instead of finding the best possible bounds. It will be convenient to compute all the ω j in such a way that our approximations  ω j ′ will satisfy  ω j ′ ≤ 1; this condition is easy to guarantee if we truncate towards zero r +2xr   2+ 2xr  in  46 . instead of rounding, because x2 The operations we need to perform with m-bit fixed point complex arithmetic are all obtained by replacing an exact computation of the form a ← b + ω jc by the approximate computation  r+1 =  1+ x2  r+1 + y2  r + y2  a′ ← truncateb′ +  ω j ′c′,   48  where b′,  ω j ′, and c′ are previously computed approximations; all of these complex numbers and their approximations are bounded by 1 in absolute value. If b′ − b ≤ δ1,  ω j ′ − ω j ≤ δ2, and c′ − c ≤ δ3, it is not difficult to see that we will have a′ − a < δ + δ1 + δ2 + δ3, where  because we have  ω j ′c′ − ω jc =  ω j ′ − ω jc′ + ω j c′ − c  ≤ δ2 + δ3, and  δ = 2−m + 2−m i = 21 2−m,  δ exceeds the maximum truncation error. The approximations  ω j ′ are obtained by starting with approximations ω′ r to the numbers defined in  46 , and we may   49    ARITHMETIC  Then  47  implies that  ω j ′ − ω j <  2k − 1 δ for all j, because the error is  310 assume that  46  is performed with sufficient precision to make ω′ due to at most k approximations and k − 1 truncations.  4.3.3 r − ωr < δ.  If we have errors of at most ϵ before any pass of the fast Fourier transform, the operations of that pass therefore have the form  48  where δ1 = δ3 = ϵ and δ2 =  2k − 1 δ; the errors after the pass will then be at most 2ϵ + 2kδ. There is no error in Pass 0, so we find by induction on j that the maximum error after Pass j is bounded by  2j − 1  · 2kδ, and the computed values of ˆus will satisfy  ˆus ′ − ˆus <  2k − 1  · 2kδ. A similar formula will hold for  ˆvs ′; and we will have   ˆws ′ − ˆws < 2 2k − 1  · 2kδ + δ <  4k2k − 2k δ.  During the inverse transformation there is an additional accumulation of errors, but the division by K = 2k ameliorates most of this; by the same argument we find that the computed values w′   ˆˆwr ′ − ˆˆwr   < 2k 4k2k−2k δ +  2k−1 2kδ;  r will satisfy  w′  r − wr < 4k2kδ.   50  r round to the correct integer Wr,  We need enough precision to make 22k+2l w′ hence we need  22k+2l+2+lg k+k+1 2−m ≤ 1 2;   51   that is, m ≥ 3k + 2l + lg k + 7 2. This will hold if we simply require that  k ≥ 7  m ≥ 4k + 2l.  and   52  Relations  41  and  52  can be used to determine parameters k, l, m so that multiplication takes O n  + O M nk l  units of time, where M is the time to multiply m-bit fractions.  If we are using MIX, for example, suppose we want to multiply binary num- bers having n = 213 = 8192 bits each. We can choose k = 11, l = 8, m = 60, so that the necessary m-bit operations are nothing more than double-precision arithmetic. The running time M needed to do fixed point m-bit complex multi- plication will therefore be comparatively small. With triple-precision operations we can go up for example to k = l = 15, n ≤ 15· 214, which takes us way beyond MIX’s memory capacity. On a larger machine we could multiply a pair of gigabit numbers if we took k = l = 27 and m = 144.  Further study of the choice of k, l, and m leads in fact to a rather surprising conclusion: For all practical purposes we can assume that M is constant, and the Schönhage–Strassen multiplication technique will have a running time linearly proportional to n. The reason is that we can choose k = l and m = 6k; this choice of k is always less than lg n, so we will never need to use more than sextuple precision unless n is larger than the word size of our computer.  In particular, n would have to be larger than the capacity of an index register, so we probably couldn’t fit the numbers u and v in main memory.   The practical problem of fast multiplication is therefore solved, except for improvements in the constant factor. In fact, the all-integer convolution algo- rithm of exercise 4.6.4–59 is probably a better choice for practical high-precision   4.3.3  HOW FAST CAN WE MULTIPLY?  311  multiplication. Our interest in multiplying large numbers is partly theoretical, however, because it is interesting to explore the ultimate limits of computational complexity. So let’s forget practical considerations momentarily and suppose that n is extremely huge, perhaps much larger than the number of atoms in the universe. We can let m be approximately 6 lg n, and use the same algorithm recursively to do the m-bit multiplications. The running time will satisfy T n  =  OnT log n ; hence  T n  ≤ C n C lg n  C lg lg n  C lg lg lg n  . . . ,   53   where the product continues until reaching a factor with lg . . . lg n ≤ 2.  Schönhage and Strassen showed how to improve this theoretical upper bound to O n log n log log n  in their paper, by using integer numbers ω to carry out fast Fourier transforms on integers, modulo numbers of the form 2e + 1. This upper bound applies to Turing machines, namely to computers with bounded memory and a finite number of arbitrarily long tapes.  If we allow ourselves a more powerful computer, with random access to any number of words of bounded size, Schönhage has pointed out that the upper bound drops to O n log n . For we can choose k = l and m = 6k, and we have time to build a complete multiplication table of all possible products xy for 0 ≤ x, y < 2⌈m 12⌉.  The number of such products is 2k or 2k+1, and we can compute each table entry by addition from one of its predecessors in O k  steps, hence O k2k  = O n  steps will suffice for the calculation.  In this case M is the time needed to do 12-place arithmetic in radix 2⌈m 12⌉, and it follows that M = O k  = O log n  because 1-place multiplication can be done by table lookup.  The time to access a word of memory is assumed to be proportional to the number of bits in the address of that word.   Moreover, Schönhage discovered in 1979 that a pointer machine can carry out n-bit multiplication in O n  steps; see exercise 12. Such devices  which are also called “storage modification machines” and “linking automata”  seem to provide the best models of computation when n → ∞, as discussed at the end of Section 2.6. So we can conclude that multiplication in O n  steps is possible for theoretical purposes as well as in practice.  An unusual general-purpose computer called Little Fermat, with a spe- cial ability to multiply large integers rapidly, was designed in 1986 by D. V. Chudnovsky, G. V. Chudnovsky, M. M. Denneau, and S. G. Younis. Its hardware featured fast arithmetic modulo 2256 + 1 on 257-bit words; a convolution of 256- word arrays could then be done using 256 single-word multiplications, together with three discrete transforms that required only addition, subtraction, and shifting. This made it possible to multiply two 106-bit integers in less than 0.1 second, based on a pipelined cycle time of approximately 60 nanoseconds [Proc. Third Int. Conf. on Supercomputing 2  International Supercomputing Institute, 1988 , 498–499; Contemporary Math. 143  1993 , 136]. D. Division. Now that we have efficient routines for multiplication, let’s consider the inverse problem. It turns out that division can be performed just as fast as multiplication, except for a constant factor.   312  ARITHMETIC  4.3.3  To divide an n-bit number u by an n-bit number v, we can first find an n-bit approximation to 1 v, then multiply by u to get an approximation ˆq to u v; finally, we can make the slight correction necessary to ˆq to ensure that 0 ≤ u − qv < v by using another multiplication. From this reasoning, we see that it suffices to have an efficient way to approximate the reciprocal of an n- bit number. The following algorithm does this, using “Newton’s method” as explained at the end of Section 4.3.1. Algorithm R  High-precision reciprocal . Let v have the binary representation v =  0.v1v2v3 . . . 2, where v1 = 1. This algorithm computes an approximation z to 1 v, such that  z − 1 v ≤ 2−n.   54   4⌊32  4v1 + 2v2 + v3 ⌋ and k ← 0.  R1. [Initial approximation.] Set z ← 1 R2. [Newtonian iteration.]   At this point we have a number z of the binary form  xx.xx . . . x 2 with 2k + 1 places after the radix point, and z ≤ 2.  Calculate z2 =  xxx.xx . . . x 2 exactly, using a high-speed multiplication routine. Then calculate Vk z2 exactly, where Vk =  0.v1v2 . . . v2k+1+3 2. Then set z ← 2z − Vk z2 + r, where 0 ≤ r < 2−2k+1−1 is added if necessary to round z up so that it is a multiple of 2−2k+1−1. Finally, set k ← k + 1. If 2k < n, go back to step R2; otherwise the algorithm  R3. [Test for end.]  terminates. This algorithm is based on a suggestion by S. A. Cook. A similar technique has been used in computer hardware [see Anderson, Earle, Goldschmidt, and Powers, IBM J. Res. Dev. 11  1967 , 48–52]. Of course, it is necessary to check the accuracy of Algorithm R quite carefully, because it comes very close to being inaccurate. We will prove by induction that  and at the beginning and end of step R2.  z ≤ 2  z − 1 v ≤ 2−2k   55   For this purpose, let δk = 1 v−zk, where zk is the value of z after k iterations  of step R2. To start the induction on k, we have  δ0 = 1 v − 8 v′ +  32 v′ − ⌊32 v′⌋  4 = η1 + η2, where v′ =  v1v2v3 2 and η1 =  v′ − 8v  vv′, so that we have − 1 0 ≤ η2 < 1  2 < η1 ≤ 0 and 2. Now suppose that  55  has been verified for k; then  4. Hence δ0 < 1 δk+1 = 1 v − zk+1 = 1 v − zk − zk 1 − zkVk  − r  = δk − zk 1 − zkv  − z2 = δk −  1 v − δk vδk − z2 = vδ2  k v − Vk  − r.  k v − Vk  − r  k v − Vk  − r  k − z2 k ≤  2−2k 2 = 2−2k+1, and  k < δ2 0 ≤ z2 v − Vk  + r < 4 2−2k+1−3  + 2−2k+1−1 = 2−2k+1 ,  Now 0 ≤ vδ2   HOW FAST CAN WE MULTIPLY?  4.3.3 313 so δk+1 ≤ 2−2k+1. We must still verify the first inequality of  55 ; to show that zk+1 ≤ 2, there are three cases: a  Vk = 1 b  Vk ̸= 1 c  Vk−1 ̸= 1  2; then zk+1 = 2. 2 = Vk−1; then zk = 2, so 2zk − z2  2; then zk+1 = 1 v − δk+1   0.  kVk ≤ 2 − 2−2k+1−1.  The running time of Algorithm R is bounded by  2T 4n  + 2T 2n  + 2T n  + 2T  1  2 n  + ··· + O n   steps, where T n  is an upper bound on the time needed to do a multiplication of n-bit numbers. If T n  has the form nf n  for some monotonically nondecreasing function f n , we have  T 4n  + T 2n  + T n  + ··· < T 8n ,   56  so division can be done with a speed comparable to that of multiplication except for a constant factor.  be evaluated to n significant bits in OM n  log n steps, if it takes M n  units  R. P. Brent has shown that functions such as log x, exp x, and arctan x can  of time to multiply n-bit numbers [JACM 23  1976 , 242–251]. E. Multiplication in real time. It is natural to wonder if multiplication of n-bit numbers can be accomplished in just n steps. We have come from order n2 down to order n, so perhaps we can squeeze the time down to the absolute minimum. In fact, it is actually possible to output the answer as fast as we input the digits, if we leave the domain of conventional computer programming and allow ourselves to build a computer that has an unlimited number of components all acting at once.  A linear iterative array of automata is a set of devices M1, M2, M3, . . . that can each be in a finite set of “states” at each step of a computation. The machines M2, M3, . . . all have identical circuitry, and their state at time t + 1 is a function of their own state at time t as well as the states of their left and right neighbors at time t. The first machine M1 is slightly different: Its state at time t + 1 is a function of its own state and that of M2, at time t, and also of the input at time t. The output of a linear iterative array is a function defined on the states of M1.  Let u =  un−1 . . . u1u0 2, v =  vn−1 . . . v1v0 2, and q =  qn−1 . . . q1q0 2 be binary numbers, and let uv + q = w =  w2n−1 . . . w1w0 2. It is a remarkable fact that a linear iterative array can be constructed, independent of n, that will output w0, w1, w2, . . . at times 1, 2, 3, . . . , if it is given the inputs  u0, v0, q0 ,  u1, v1, q1 ,  u2, v2, q2 , . . . at times 0, 1, 2, . . . .  We can state this phenomenon in the language of computer hardware by saying that it is possible to design a single integrated circuit module with the fol- lowing property: If we wire together sufficiently many of these chips in a straight line, with each module communicating only with its left and right neighbors, the resulting circuitry will produce the 2n-bit product of n-bit numbers in exactly 2n clock pulses.   314  ARITHMETIC  4.3.3  Table 2  MULTIPLICATION IN A LINEAR ITERATIVE ARRAY  Time  Input  Module M1  uj vj  qj  x0 y0  x1 y1  x y  z2 z1 z0  Module M2  x0 y0  x1 y1  x y  z2 z1 z0  Module M3  x0 y0  x1 y1  x y  z2 z1 z0  c  0  1  2  3  3  3  3  3  3  3  3  3  0 0  1 1  1 1  1 1  1 1  1 1  1 1  1 1  1 1  1 1  1 1  1 1  0 0  0 0  1 1  1 1  1 1  1 1  1 1  1 1  1 1  1 1  1 1  1 1  0 0  0 0  0 0  1 1  0 0  1 1  0 0  0 0  0 0  0 0  0 0  0 0  0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0  c  0  0  0  0  1  2  3  3  3  3  3  3  0 0  0 0  0 0  0 0  1 1  1 1  1 1  1 1  1 1  1 1  1 1  1 1  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  1 1  0 0  0 0  0 0  0 0  0 0  0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0  c  0  0  0  0  0  0  0  1  2  3  3  3  0 0  0 0  0 0  0 0  0 0  0 0  0 0  1 1  1 1  1 1  1 1  1 1  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0  0  1  2  3  4  5  6  7  8  9  10  11  1 1  1 1  1 1  0 0  1 1  0 0  0 0  0 0  0 0  0 0  0 0  0 0  1  1  0  1  0  0  0  0  0  0  0  0   4.3.3  HOW FAST CAN WE MULTIPLY?  315  The basic idea can be understood as follows. At time 0, machine M1 senses  u0, v0, q0  and it therefore is able to output  u0v0 + q0  mod 2 at time 1. Then it sees  u1, v1, q1  and it can output  u0v1 + u1v0 + q1 + k1  mod 2, where k1 is the “carry” left over from the previous step, at time 2. Next it sees  u2, v2, q2  and outputs  u0v2 + u1v1 + u2v0 + q2 + k2  mod 2; furthermore, its state holds the values of u2 and v2 so that machine M2 will be able to sense these values at time 3, and M2 will be able to compute u2v2 for the benefit of M1 at time 4. Machine M1 essentially arranges to start M2 multiplying the sequence  u2, v2 ,  u3, v3 , . . . , and M2 will ultimately give M3 the job of multiplying  u4, v4 ,  u5, v5 , etc. Fortunately, things just work out so that no time is lost. The reader will find it interesting to deduce further details from the formal description that follows. Each automaton has 211 states  c, x0, y0, x1, y1, x, y, z2, z1, z0 , where 0 ≤ c < 4 and each of the x’s, y’s, and z’s is either 0 or 1. Initially, all the devices are in state  0, 0, 0, 0, 0, 0, 0, 0, 0, 0 . Suppose that a machine Mj, for j > 1, is in state  c, x0, y0, x1, y1, x, y, z2, z1, z0  at time t, and its left neighbor Mj−1 is in state  cl, xl0, yl0, xl1, yl1, xl, y l, zl2, zl1, zl0  while its right neighbor Mj+1 is in state  cr, xr0, yr0, xr1, yr1, xr, yr, zr2, zr1, zr0  at that time. Then machine Mj will go into state  c′, x′  1, x′, y′, z′  0, x′  1, y′  0, y′  c′ = min c + 1, 3  0, y′  x′ 0  =  xl, y l  1, y′  x′ 1  =  xl, y l   x′, y′  =  xl, y l   1, z′ 2, z′ if cl = 3, if c = 0, if c = 1, if c ≥ 2,  0  at time t + 1, where 0 otherwise;  x0, y0  otherwise;  x1, y1  otherwise; otherwise;  x, y   and  z′  2z′  1z′  0 2 is the binary notation for   xly l  x0y l + xly0 x0y l + x1y1 + xly0 x0y l + x1y + xy1 + xly0  if c = 0; if c = 1; if c = 2; if c = 3.  0 + z1 + zl zr  2 +   57    58   The leftmost machine M1 behaves in almost the same way as the others; it acts exactly as if there were a machine to its left in state  3, 0, 0, 0, 0, u, v, q, 0, 0  when it is receiving the inputs  u, v, q . The output of the array is the z0 component of M1.  Table 2 shows an example of this array acting on the inputs q =   . . . 00001011 2.  u = v =   . . . 00010111 2,  The output sequence appears in the lower right portion of the states of M1:  0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, . . . ,  representing the number  . . . 01000011100 2 from right to left.  This construction is based on a similar one first published by A. J. Atrubin,  IEEE Trans. EC-14  1965 , 394–399.  Fast as it is, the iterative array is optimum only when the input bits arrive one at a time. If the input bits are all present simultaneously, we prefer parallel circuitry that will obtain the product of two n-bit numbers after O log n  levels   316  ARITHMETIC  4.3.3  of delay. Efficient circuits of that kind have been described, for example, by C. S. Wallace, IEEE Trans. EC-13  1964 , 14–17; D. E. Knuth, The Stanford GraphBase  New York: ACM Press, 1994 , 270–279.  S. Winograd [JACM 14  1967 , 793–802] has investigated the minimum multiplication time achievable in a logical circuit when n is given and when the inputs are available all at once in arbitrarily coded form. For similar questions when multiplication and addition must both be supported simultaneously, see A. C. Yao, STOC 13  1981 , 308–311; Mansour, Nisan, and Tiwari, STOC 22  1990 , 235–243.  Multiplication is mie vexation, And Division is quite as bad: The Golden Rule is mie stumbling stule, And Practice drives me mad. — Manuscript collected by J. O. HALLIWELL  c. 1570   EXERCISES 1. [22] The idea expressed in  2  can be generalized to the decimal system, if the radix 2 is replaced by 10. Using this generalization, calculate 1234 times 2341  reducing this product of four-digit numbers to three products of two-digit numbers, and reducing each of the latter to products of one-digit numbers . 2. [M22] Prove that, in step T1 of Algorithm T, the value of R either stays the same or increases by one when we set R ← ⌊√ Q⌋.  Therefore, as observed in that step, we need not calculate a square root.  3. [M22] Prove that the sequences qk and rk defined in Algorithm T satisfy the  cid:120  4. [28]  K. Baker.  Show that it is advantageous to evaluate the polynomial W  x  inequality 2qk+1 2rk rk ≤ 2qk−1+qk, when k > 0.  at the points x = −r, . . . , 0, . . . , r instead of at the nonnegative points x = 0, 1, . . . , 2r as in Algorithm T. The polynomial U x  can be written  U x  = Ue x  2  + xUo x  2 ,  2Q⌉ + 1 instead of Q⌋, with suitable initial values of q0, q1, r0, and r1, then  20  can be  and similarly V  x  and W  x  can be expanded in this way; show how to exploit this  cid:120  5. [35] Show that if in step T1 of Algorithm T we set R ← ⌈√ idea, obtaining faster calculations in steps T7 and T8. setting R ← ⌊√ improved to tk ≤ qk+12√2 lg qk+1  lg qk+1 . 6. [M23] Prove that the six numbers in  24  are relatively prime in pairs. 7. [M23] Prove  25 . 8. [M20] True or false: We can ignore the bit reversal  sk−1, . . . , s0  →  s0, . . . , sk−1  in  39 , because the inverse Fourier transform will reverse the bits again anyway. 9. [M15] Suppose the Fourier transformation method of the text is applied with all occurrences of ω replaced by ωq, where q is some fixed integer. Find a simple relation between the numbers  ˜u0, ˜u1, . . . , ˜uK−1  obtained by this general procedure and the numbers  ˆu0, ˆu1, . . . , ˆuK−1  obtained when q = 1.   4.3.3  HOW FAST CAN WE MULTIPLY?  317  10. [M26] The scaling in  43  makes it clear that all the complex numbers A[j] computed by pass j of the transformation subroutine will be less than 2j−k in absolute value, during the calculations of ˆus and ˆvs in the Schönhage–Strassen multiplication algorithm. Show that all of the A[j] will be less than 1 in absolute value during the third Fourier transformation  the calculation of ˆˆwr .   cid:120  11. [M26] If n is fixed, how many of the automata in the linear iterative array defined  cid:120  12. [M41]  A. Schönhage.  The purpose of this exercise is to prove that a simple  by  57  and  58  are needed to compute the product of n-bit numbers?  Notice that the automaton Mj is influenced only by the component zr0 of the machine on its right, so we may remove all automata whose z0 component is always zero whenever the inputs are n-bit numbers.   form of pointer machine can multiply n-bit numbers in O n  steps. The machine has no built-in facilities for arithmetic; all it does is work with nodes and pointers. Each node has the same finite number of link fields, and there are finitely many link registers. The only operations this machine can do are:  i  read one bit of input and jump if that bit is 0; ii  output 0 or 1; iii  load a register with the contents of another register or with the contents of a  link field in a node pointed to by a register;  iv  store the contents of a register into a link field in a node pointed to by a register; v  jump if two registers are equal; vi  create a new node and make a register point to it; vii  halt.  Implement the Fourier-transform multiplication method efficiently on such a machine. [Hints: First show that if N is any positive integer, it is possible to create N nodes representing the integers {0, 1, . . . , N − 1}, where the node representing p has pointers to the nodes representing p + 1, ⌊p 2⌋, and 2p. These nodes can be created in O N  steps. Show that arithmetic with radix N can now be simulated without difficulty: For example, it takes O log N  steps to find the node for  p+ q  mod N and to determine if p + q ≥ N, given pointers to p and q; and multiplication can be simulated in O log N 2 steps. Now consider the algorithm in the text, with k = l and m = 6k and N = 2⌈m 13⌉, so that all quantities in the fixed point arithmetic calculations are 13-place integers with radix N. Finally, show that each pass of the fast Fourier transformations can be done in O K + N log N 2  = O K  steps, using the following idea: Each of the K necessary assignments can be “compiled” into a bounded list of instructions for a simulated MIX- like computer whose word size is N, and instructions for K such machines acting in parallel can be simulated in O K +  N log N 2  steps if they are first sorted so that all identical instructions are performed together.  Two instructions are identical if they have the same operation code, the same register contents, and the same memory operand contents.  Note that N 2 = O n12 13 , so  N log N 2 = O K .] 13. [M25]  A. Schönhage.  What is a good upper bound on the time needed to multiply an m-bit number by an n-bit number, when both m and n are very large but n is much larger than m, based on the results discussed in this section for the case m = n? 14. [M42] Write a program for Algorithm T, incorporating the improvements of exercise 4. Compare it with a program for Algorithm 4.3.1M and with a program based on  2 , to see how large n must be before Algorithm T is an improvement.   318  ARITHMETIC  4.3.3  15. [M49]  S. A. Cook.  A multiplication algorithm is said to be online if the  k+1 st input bits of the operands, from right to left, are not read until the kth output bit has been produced. What are the fastest possible online multiplication algorithms achievable on various species of automata?   cid:120  16. [25] Prove that it takes only O K log K  arithmetic operations to evaluate the  discrete Fourier transform  35 , even when K is not a power of 2. [Hint: Rewrite  35  in the form  ˆus = ω−s2 2   0≤t<K   s+t 2 2  ω−t2 2  ut  ω  and express this sum as a convolution product.] 17. [M26] Karatsuba’s multiplication scheme  2  does Kn 1-place multiplications when it forms the product of n-place numbers, where K1 = 1, K2n = 3Kn, and K2n+1 = 2Kn+1 + Kn for n ≥ 1. “Solve” this recurrence by finding an explicit formula for Kn when n = 2e1 + 2e2 + ··· + 2et, e1 > e2 > ··· > et ≥ 0.   cid:120  18. [M30] Devise a scheme to allocate memory for the intermediate results when  cid:120  19. [M23] Show how to compute uv mod m with a bounded number of operations that  multiplication is performed by a recursive algorithm based on  2 : Given two N-place integers u and v, each in N consecutive places of memory, show how to arrange the computation so that the product uv appears in the least significant 2N places of a  3N + O log N  -place area of working storage.  meet the ground rules of exercise 3.2.1.1–11, if you are also allowed to test whether one operand is less than another. Both u and v are variable, but m is constant. Hint: Consider the decomposition in  2 .   4.4  RADIX CONVERSION  319  4.4. RADIX CONVERSION If our ancestors had invented arithmetic by counting with their two fists or their eight fingers, instead of their ten “digits,” we would never have to worry about writing binary-decimal conversion routines.  And we would perhaps never have learned as much about number systems.  In this section, we shall discuss the conversion of numbers from positional notation with one radix into positional notation with another radix; this process is, of course, most important on binary computers when converting decimal input data into binary form, and converting binary answers into decimal form. A. The four basic methods. Binary-decimal conversion is one of the most machine-dependent operations of all, since computer designers keep inventing different ways to provide for it in the hardware. Therefore we shall discuss only the general principles involved, from which programmers can select the procedures that are best suited to their machines.  We shall assume that only nonnegative numbers enter into the conversion,  since the manipulation of signs is easily accounted for.  Let us assume that we are converting from radix b to radix B.   Mixed- radix generalizations are considered in exercises 1 and 2.  Most radix-conversion routines are based on multiplication and division, using one of the four methods below. The first two methods apply to integers  radix point at the right , and the others to fractions  radix point at the left . It is often impossible to express a ter- minating radix-b fraction  0.u−1u−2 . . . u−m b exactly as a terminating radix-B fraction  0.U−1U−2 . . . U−M B. For example, the fraction 1 10 has the infinite binary representation  0.0001100110011 . . .  2. Therefore methods of rounding the result to M places are sometimes necessary. Method 1a  Division by B using radix-b arithmetic . Given an integer u, we can obtain its radix-B representation   . . . U2U1U0 B as follows:  U0 = u mod B,  U1 = ⌊u B⌋ mod B, stopping when ⌊. . .⌊⌊u B⌋ B⌋ . . .  B⌋ = 0. Method 1b  Multiplication by b using radix-B arithmetic . If u has the radix-b representation  um . . . u1u0 b, we can use radix-B arithmetic to evaluate the polynomial umbm + ··· + u1b + u0 = u in the form  U2 = ⌊⌊u B⌋ B⌋ mod B,  . . . ,   . . .  um b + um−1  b + ···   b + u1   b + u0.  Method 2a  Multiplication by B using radix-b arithmetic . Given a fractional number u, we can obtain the digits of its radix-B representation  .U−1U−2 . . .  B as follows:  U−1 = ⌊uB⌋,  U−3 = ⌊{{uB}B}B⌋, U−2 = ⌊{uB}B⌋, where {x} denotes x mod 1 = x − ⌊x⌋. If it is desired to round the result to M places, the computation can be stopped after U−M has been calculated,  . . . ,   ARITHMETIC  320 4.4 and U−M should be increased by unity if {. . .{{uB}B} . . . B} is greater than 1 2. be incorporated into the answer using radix-B arithmetic. It would be simpler to 2 B−M to the original number u before the calculation begins, add the constant 1 2 B−M cannot be represented but this may lead to an incorrect answer when 1 exactly as a radix-b number inside the computer. Note further that it is possible  Note, however, that this may cause carries to propagate, and these carries must for the answer to round up to  1.00 . . . 0 B, if bm ≥ 2BM.  Exercise 3 shows how to extend this method so that M is variable, just large enough to represent the original number to a specified accuracy. In this case the problem of carries does not occur.  Method 2b  Division by b using radix-B arithmetic . If u has the radix-b representation  0.u−1u−2 . . . u−m b, we can use radix-B arithmetic to evaluate u−1b−1 + u−2b−2 + ··· + u−mb−m in the form   . . .  u−m b + u1−m  b + ··· + u−2  b + u−1   b.  Care should be taken to control errors that might occur due to truncation or rounding in the division by b; these are often negligible, but not always.  To summarize, Methods 1a, 1b, 2a, and 2b give us two ways to convert integers and two ways to convert fractions; and it is certainly possible to convert between integers and fractions by multiplying or dividing by an appropriate power of b or B. Therefore there are at least four methods to choose from when trying to do radix conversion.  B. Single-precision conversion. To illustrate these four methods, let us assume that MIX is a binary computer, and suppose that we want to convert a nonnegative binary integer u to a decimal integer. Thus b = 2 and B = 10. Method 1a could be programmed as follows:  1H DIV =10=  Set j ← 0. ENT1 0 LDX U Set rAX ← u. ENTA 0  rA, rX  ←  ⌊rAX 10⌋, rAX mod 10 . STX ANSWER,1 Uj ← rX. j ← j + 1. INC1 1 rAX ← rA. SRAX 5 JXP 1B Repeat until result is zero. This requires 18M + 4 cycles to obtain M digits.  Method 1a uses division by 10; Method 2a uses multiplication by 10, so it might be a little faster. But in order to use Method 2a, we must deal with fractions, and this leads to an interesting situation. Let w be the word size of the computer, and assume that u < 10n < w. With a single division we can find q and r, where  wu = 10nq + r,  0 ≤ r < 10n.   1    2     3    4   4.4  RADIX CONVERSION  321  Now if we apply Method 2a to the fraction  q + 1  w, we will obtain the digits of u from left to right, in n steps, since   10n q + 1    w    =  u + 10n − r  w  = u.     This idea is due to P. A. Samet, Software Practice & Experience 1  1971 , 93–96.   Here is the corresponding MIX program:  JOV OFLO LDA U LDX =10n= DIV =10n= JOV ERROR ENT1 n-1 2H MUL =10=  STA ANSWER,1 SLAX 5 DEC1 1 J1NN 2B  Ensure that overflow is off. rAX ← wu + 10n. rA ← q + 1, rX ← r. Jump if u ≥ 10n. Set j ← n − 1. Now imagine the radix point at the left, rA = x. Set Uj ← ⌊10x⌋. x ← {10x}. j ← j − 1. Repeat for n > j ≥ 0.  This slightly longer routine requires 16n + 19 cycles, so it is a little faster than program  1  if n = M ≥ 8; when leading zeros are present,  1  will be faster. Program  4  as it stands cannot be used to convert integers u ≥ 10m when 10m < w < 10m+1, since we would need to take n = m + 1. In this case we can obtain the leading digit of u by computing ⌊u 10m⌋; then u mod 10m can be converted as above with n = m.  The fact that the answer digits are obtained from left to right may be an advantage in some applications  for example, when typing out an answer one digit at a time . Thus we see that a fractional method can be used for conversion of integers, although the use of inexact division makes a little bit of numerical analysis necessary.  We can avoid the division by 10 in Method 1a if we do two multiplications instead. This alternative can be important, because radix conversion is often done by “satellite” computers that have no built-in division capability. If we let x be an approximation to 1  10, so that  1 10 < x <  1 10 + 1  w  ,  it is easy to prove  see exercise 7  that ⌊ux⌋ = ⌊u 10⌋ or ⌊u 10⌋ + 1, so long as 0 ≤ u < w. Therefore, if we compute u − 10⌊ux⌋, we will be able to determine the value of ⌊u 10⌋:  ⌊u 10⌋ = ⌊ux⌋ −u < 10⌊ux⌋.   5  At the same time we will have determined u mod 10. A MIX program for conver- sion using  5  appears in exercise 8; it requires about 33 cycles per digit.   322  ARITHMETIC  4.4  If the computer has neither division nor multiplication in its repertoire of built-in instructions, we can still use Method 1a for conversion by judiciously shifting and adding, as explained in exercise 9.  Another way to convert from binary to decimal is to use Method 1b, but to do this we need to simulate doubling in a decimal number system. This approach is generally most suitable for incorporation into computer hardware; however, it is possible to program the doubling process for decimal numbers, using binary addition, binary shifting, and binary extraction or masking  bitwise AND  as shown in Table 1, which was suggested by Peter L. Montgomery.  Table 1  DOUBLING A BINARY-CODED DECIMAL NUMBER  General form  Example  u11 u10 u9 u8 u7 u6 u5 u4 u3 u2 u1 u0  0011 0110 1001 = 3 6 9  v11 v10 v9 v8 v7 v6 v5 v4 v3 v2 v1 v0  0110 1001 1100  v11 0 0 0 v7 0 0 0 v3 0 0 0  0000 1000 1000  0 v11 v110 0 v7 v7 0 0 v3 v3 0  0000 0110 0110  w11w10w9w8 w7w6w5w4 w3w2w1w0  0011 1100 1111  Operation  1. Given  number 2. Add 3 to each digit  3. Extract each  high bit  4. Shift right 2 and subtract 5. Add original  number  6. Add original  0 0111 0011 1000 = 7 3 8  x12 x11 x10 x9 x8 x7 x6 x5 x4 x3 x2 x1 x0  number This method changes each individual digit d into 2d when 0 ≤ d ≤ 4, and into 6 + 2d =  2d − 10  + 24 when 5 ≤ d ≤ 9; and that is just what is needed to double decimal numbers encoded with 4 bits per digit.  Another related idea is to keep a table of the powers of two in decimal form, and to add the appropriate powers together by simulating decimal addition. A survey of bit-manipulation techniques appears in Section 7.1.3.  Finally, even Method 2b can be used for the conversion of binary integers to decimal integers. We can find q as in  2 , and then we can simulate the decimal division of q + 1 by w, using a “halving” process  exercise 10  that is similar to the doubling process just described, retaining only the first n digits to the right of the radix point in the answer. In this situation, Method 2b does not seem to offer advantages over the other three methods already discussed, but we have confirmed the remark made earlier that at least four distinct methods are available for converting integers from one radix to another.  Now let us consider decimal-to-binary conversion  so that b = 10, B = 2 . Method 1a simulates a decimal division by 2; this is feasible  see exercise 10 , but it is primarily suitable for incorporation in hardware instead of programs. Method 1b is the most practical method for decimal-to-binary conversion in the great majority of cases. The following MIX code assumes that there   4.4  RADIX CONVERSION  323  are at least two digits in the number  um . . . u1u0 10 being converted, and that 10m+1 < w so that overflow is not an issue:  ENT1 M-1 LDA INPUT+M  1H MUL =10=  Set j ← m − 1. Set U ← um.  SLAX 5 ADD INPUT,1 U ← 10U + uj. DEC1 1 J1NN 1B  Repeat for m > j ≥ 0.   6   The multiplication by 10 could be replaced by shifting and adding. A trickier but perhaps faster method, which uses about lg m multiplications, extractions, and additions instead of m − 1 multiplications and additions, is described in exercise 19.  For the conversion of decimal fractions  0.u−1u−2 . . . u−m 10 to binary form, we can use Method 2b; or, more commonly, we can first convert the integer  u−1u−2 . . . u−m 10 by Method 1b and then divide by 10m. C. Hand calculation. It is occasionally necessary for computer programmers to convert numbers by hand, and since this is a subject not yet taught in elementary schools, it may be worthwhile to examine it briefly here. There are simple pencil- and-paper methods for converting between decimal and octal notations, and these methods are easily learned, so they should be more widely known. Converting octal integers to decimal. The simplest conversion is from octal to decimal; this technique was apparently first published by Walter Soden, Math. Comp. 7  1953 , 273–274. To do the conversion, write down the given octal num- ber; then at the kth step, double the k leading digits using decimal arithmetic, and subtract this from the k + 1 leading digits using decimal arithmetic. The process terminates in m steps if the given number has m + 1 digits. It is a good idea to insert a radix point to show which digits are being doubled, as shown in the following example, in order to prevent embarrassing mistakes. Example 1. Convert  5325121  8 to decimal.  5.3 2 5 1 2 1  4 3.2 5 1 2 1  3 4 6.5 1 2 1  2 7 7 3.1 2 1  − 1 0 − 8 6 − 6 9 2 − 5 5 4 6 − 4 4 3 7 0 1 7 7 4 8 2.1 − 3 5 4 9 6 4 1 4 1 9 8 5 7  2 2 1 8 5.2 1  Answer:  1419857 10.   324  ARITHMETIC  4.4  A reasonably good check on the computations may be had by “casting out nines”: The sum of the digits of the decimal number must be congruent modulo 9 to the alternating sum and difference of the digits of the octal number, with the rightmost digit of the latter given a plus sign. In the example above, we have 1 + 4 + 1 + 9 + 8 + 5 + 7 = 35, and 1 − 2 + 1 − 5 + 2 − 3 + 5 = −1; the difference is 36  a multiple of 9 . If this test fails, it can be applied to the k + 1 leading digits after the kth step, and the error can be located using a “binary search” procedure; in other words, we can locate the error by first checking the middle result, then using the same procedure on the first or second half of the calculation, depending on whether the middle result is incorrect or correct.  The “casting-out-nines” process is only about 89 percent reliable, because there is one chance in nine that two random integers will differ by a multiple of nine. An even better check is to convert the answer back to octal by using an inverse method, which we shall now consider. Converting decimal integers to octal. A similar procedure can be used for the opposite conversion: Write down the given decimal number; then at the kth step, double the k leading digits using octal arithmetic, and add these to the k + 1 leading digits using octal arithmetic. The process terminates in m steps if the given number has m + 1 digits. Example 2. Convert  1419857 10 to octal.  1 .4 1 9 8 5 7  + 2  1 6 .1 9 8 5 7  + 3 4  2 1 5 .9 8 5 7  + 4 3 2  2 6 1 3 .8 5 7  + 5 4 2 6  3 3 5 6 6 .5 7  + 6 7 3 5 4  4 2 5 2 4 1 .7 + 1 0 5 2 5 0 2 5 3 2 5 1 2 1  Answer:  5325121  8.   Notice that the nonoctal digits 8 and 9 enter into this octal computation.  The answer can be checked as discussed above. This method was published by Charles P. Rozier, IEEE Trans. EC-11  1962 , 708–709.  The two procedures just given are essentially Method 1b of the general radix-conversion procedures. Doubling and subtracting in decimal notation is like multiplying by 10 − 2 = 8; doubling and adding in octal notation is like multiplying by 8 + 2 = 10. There is a similar method for hexadecimal decimal conversions, but it is a little more difficult since it involves multiplication by 6 instead of by 2.   4.4  RADIX CONVERSION  325  To keep these two methods straight in our minds, it is not hard to remember that we must subtract to go from octal to decimal, since the decimal representa- tion of a number is smaller; similarly we must add to go from decimal to octal. The computations are performed using the radix of the answer, not the radix of the given number, otherwise we couldn’t get the desired answer. Converting fractions. No equally fast method of converting fractions manually is known. The best way seems to be Method 2a, with doubling and adding or subtracting to simplify the multiplications by 10 or by 8. In this case, we reverse the addition-subtraction criterion, adding when we convert to decimal and subtracting when we convert to octal; we also use the radix of the given input number, not the radix of the answer, in this computation  see Examples 3 and 4 . The process is about twice as hard as the method that we used for integers. Example 3. Convert  .14159 10 to octal.  Example 4. Convert  .110374  8 to decimal.  Answer:  .110374 . . .  8.  .1 4 1 5 9  2 8 3 1 8−  1 .1 3 2 7 2  2 6 5 4 4−  1 .0 6 1 7 6  1 2 3 5 2−  0 .4 9 4 0 8  3 .9 5 2 6 4  9 8 8 1 6− 1 9 0 5 2 8− 7 .6 2 1 1 2  1 2 4 2 2 4− 4 .9 6 8 9 6  .1 1 0 3 7 4  2 2 0 7 7 0 +  1.3 2 4 7 3 0  6 5 1 6 6 0 +  4.1 2 1 1 6 0  2 4 2 3 4 0 +  1.4 5 4 1 4 0  1 1 3 0 3 0 0 + 5.6 7 1 7 0 0  1 5 6 3 6 0 0 + 8.5 0 2 6 0 0  1 2 0 5 4 0 0 + 6.2 3 3 4 0 0  Answer:  .141586 . . .  10.   326  ARITHMETIC  4.4  D. Floating point conversion. When floating point values are to be con- verted, it is necessary to deal with both the exponent and the fraction parts simultaneously, since conversion of the exponent will affect the fraction part. Given the number f · 2e to be converted to decimal, we may express 2e in the form F · 10E  usually by means of auxiliary tables , and then convert F f to decimal. Alternatively, we can multiply e by log10 2 and round this to the nearest integer E; then divide f ·2e by 10E and convert the result. Conversely, given the number F · 10E to be converted to binary, we may convert F and then multiply it by the floating point number 10E  again by using auxiliary tables . Obvious techniques can be used to reduce the maximum size of the auxiliary tables by using several multiplications and or divisions, although this can cause rounding errors to propagate. Exercise 17 considers the minimization of error. E. Multiple-precision conversion. When converting extremely long numbers, it is most convenient to start by converting blocks of digits, which can be handled by single-precision techniques, and then to combine these blocks by using simple multiple-precision techniques. For example, suppose that 10n is the highest power of 10 less than the computer word size. Then: a  To convert a multiple-precision integer from binary to decimal, divide it repeatedly by 10n  thus converting from binary to radix 10n by Method 1a . Single-precision operations will give the n decimal digits for each place of the radix-10n representation. b  To convert a multiple-precision fraction from binary to decimal, proceed similarly, multiplying by 10n  that is, using Method 2a with B = 10n . c  To convert a multiple-precision integer from decimal to binary, convert blocks of n digits first; then use Method 1b to convert from radix 10n to binary. d  To convert a multiple-precision fraction from decimal to binary, convert first to radix 10n as in  c , then use Method 2b. F. History and Bibliography. Radix-conversion techniques implicitly origi- nated in ancient problems dealing with weights, measures, and currencies, where mixed-radix systems were generally involved. Auxiliary tables were usually prepared to help people make the conversions. During the seventeenth century, when sexagesimal fractions were being supplanted by decimal fractions, it was necessary to convert between the two systems in order to use existing books of astronomical tables; a systematic method to transform fractions from radix 60 to radix 10 and vice versa was given in the 1667 edition of William Oughtred’s Clavis Mathematicæ, Chapter 6, Section 18.  This material was not present in the original 1631 edition of Oughtred’s book.  Conversion rules had already been given by al-K¯ash¯ı of Samarkand in his Key to Arithmetic  1427 , where Methods 1a, 1b, and 2a are clearly explained [Istoriko-Mat. Issled. 7  1954 , 126–135], but his work was unknown in Europe. The 18th century American mathematician Hugh Jones used the words “octavation” and “decimation” to describe octal decimal conversions, but his methods were not as clever as his terminology. A. M. Legendre [Théorie des Nombres  Paris: 1798 , 229] noted   4.4  RADIX CONVERSION  327  that positive integers may be conveniently converted to binary form if they are repeatedly divided by 64.  In 1946, H. H. Goldstine and J. von Neumann gave prominent consideration to radix conversion in their classic memoir, Planning and Coding Problems for an Electronic Computing Instrument, because it was necessary to justify the use of binary arithmetic; see John von Neumann, Collected Works 5  New York: Macmillan, 1963 , 127–142. Another early discussion of radix conversion on binary computers was published by F. Koons and S. Lubkin, Math. Comp. 3  1949 , 427–431, who suggested a rather unusual method. The first discussion of floating point conversion was given somewhat later by F. L. Bauer and K. Samelson [Zeit. für angewandte Math. und Physik 4  1953 , 312–316].  The following articles are, similarly, of historic interest: A note by G. T. Lake [CACM 5  1962 , 468–469] mentioned some hardware techniques for con- version and gave clear examples. A. H. Stroud and D. Secrest [Comp. J. 6  1963 , 62–66] discussed conversion of multiple-precision floating point numbers. The conversion of unnormalized floating point numbers, preserving the amount of “significance” implied by the representation, was discussed by H. Kanner [JACM 12  1965 , 242–246] and by N. Metropolis and R. L. Ashenhurst [Math. Comp. 19  1965 , 435–441]. See also K. Sikdar, Sankhy¯a B30  1968 , 315–334, and the references cited in his paper.  Detailed subroutines for formatted input and output of integers and floating point numbers in the C programming language have been given by P. J. Plauger in The Standard C Library  Prentice–Hall, 1992 , 301–331.  EXERCISES   cid:120  1. [25] Generalize Method 1b so that it works with arbitrary mixed-radix notations, into AM BM−1 . . . B1B0 + ··· + A1B0 + A0, ambm−1 . . . b1b0 + ··· + a1b0 + a0  converting  where 0 ≤ aj < bj and 0 ≤ AJ < BJ for 0 ≤ j < m and 0 ≤ J < M.  Give an example of your generalization by manually converting “3 days, 9 hours, 12 minutes, and 37 seconds” into long tons, hundredweights, stones, pounds, and ounces.  Let one second equal one ounce. The British system of weights has 1 stone = 14 pounds, 1 hundredweight = 8 stone, 1 long ton = 20 hundredweight.  In other words, let b0 = 60, b1 = 60, b2 = 24, m = 3, B0 = 16, B1 = 14, B2 = 8, B3 = 20, M = 4; the problem is to find A4, . . . , A0 in the proper ranges such that 3b2b1b0 +9b1b0 +12b0 +37 = A4B3B2B1B0 + A3B2B1B0 + A2B1B0 + A1B0 + A0, using a systematic method that generalizes Method 1b.  All arithmetic is to be done in a mixed-radix system.  2. [25] Generalize Method 1a so that it works with mixed-radix notations, as in exercise 1, and give an example of your generalization by manually solving the same conversion problem stated in exercise 1.   cid:120  3. [25]  D. Taranto.  When fractions are being converted, there is no obvious way to  decide how many digits to give in the answer. Design a simple generalization of Method 2a that, given two positive radix-b fractions u and ϵ between 0 and 1, converts u to a rounded radix-B equivalent U that has just enough places M to the right of the radix   328  ARITHMETIC  4.4  point to ensure that U − u < ϵ.  In particular if u is a multiple of b−m and ϵ = b−m 2, the value of U will have just enough digits so that u can be recomputed exactly, given U and m. Note that M might be zero; for example, if ϵ ≤ 1 2 and u > 1 − ϵ, the proper answer is U = 1.  4. [M21]  a  Prove that every real number with a terminating binary representation also has a terminating decimal representation.  b  Find a simple condition on the positive integers b and B that is satisfied if and only if every real number that has a terminating radix-b representation also has a terminating radix-B representation. 5. [M20] Show that program  4  would still work if the instruction ‘LDX =10n=’ were replaced by ‘LDX =c=’ for certain other constants c. 6. [30] Discuss using Methods 1a, 1b, 2a, and 2b when b or B is −2. 7. [M18] Given that 0 < α ≤ x ≤ α + 1 w and 0 ≤ u ≤ w, where u is an integer, prove that ⌊ux⌋ is equal to either ⌊αu⌋ or ⌊αu⌋ + 1. Furthermore ⌊ux⌋ = ⌊αu⌋ exactly, if u < αw and α−1 is an integer. 8. [24] Write a MIX program analogous to  1  that uses  5  and includes no division instructions.   cid:120  9. [M29] The purpose of this exercise is to compute ⌊u 10⌋ with binary shifting and  addition operations only, when u is a nonnegative integer. Let v0 u  = 3⌊u 2⌋ + 3 and  vk+1 u  = vk u  + ⌊vk u  22k+2⌋  for k ≥ 0.  Given k, what is the smallest nonnegative integer u such that ⌊vk[u] 16⌋ ̸= ⌊u 10⌋? 10. [22] Table 1 shows how a binary-coded decimal number can be doubled by using various shifting, extracting, and addition operations on a binary computer. Give an analogous method that computes half of a binary-coded decimal number  throwing away the remainder if the number is odd . 11. [16] Convert  57721  8 to decimal.   cid:120  12. [22] Invent a rapid pencil-and-paper method for converting integers from ternary  cid:120  13. [25] Assume that locations U + 1, U + 2, . . . , U + m contain a multiple-precision  notation to decimal, and illustrate your method by converting  1212011210210 3 into decimal. How would you go from decimal to ternary?  fraction  .u−1u−2 . . . u−m b, where b is the word size of MIX. Write a MIX routine that converts this fraction to decimal notation, truncating it to 180 decimal digits. The answer should be printed on two lines, with the digits grouped into 20 blocks of nine each separated by blanks.  Use the CHAR instruction.    cid:120  14. [M27]  A. Schönhage.  The text’s method of converting multiple-precision in-  tegers requires an execution time of order n2 to convert an n-place integer, when n is large. Show that it is possible to convert n-digit decimal integers into binary notation in O M n  log n  steps, where M n  is an upper bound on the number of steps needed to multiply n-bit binary numbers that satisfies the “smoothness condition” M 2n  ≥ 2M n . 15. [M47] Can the upper bound on the time to convert large integers given in the preceding exercise be substantially lowered?  See exercise 4.3.3–12.  16. [41] Construct a fast linear iterative array for radix conversion from decimal to binary  see Section 4.3.3E .   4.4  RADIX CONVERSION  329  17. [M40] Design “ideal” floating point conversion subroutines, taking p-digit decimal numbers into P -digit binary numbers and vice versa, in both cases producing a true rounded result in the sense of Section 4.2.2. 18. [HM34]  David W. Matula.  Let roundb u, p  be the function of b, u, and p that represents the best p-digit base b floating point approximation to u, in the sense of Section 4.2.2. Under the assumption that logB b is irrational and that the range of exponents is unlimited, prove that  u = roundb roundB u, P  , p   holds for all p-digit base b floating point numbers u if and only if BP−1 ≥ bp.  In other words, an “ideal” input conversion of u into an independent base B, followed by an “ideal” output conversion of this result, will always yield u again if and only if the intermediate precision P is suitably large, as specified by the formula above.    cid:120  19. [M23] Let the decimal number u =  u7 . . . u1u0 10 be represented as the binary-  coded decimal number U =  u7 . . . u1u0 16. Find appropriate constants ci and masks mi so that the operation U ← U − ci U & mi , repeated for i = 1, 2, 3, will convert U to the binary representation of u, where “&” denotes extraction  bitwise AND .   330  ARITHMETIC  4.5  4.5. RATIONAL ARITHMETIC It is often important to know that the answer to some numerical problem is exactly 1 3, not a floating point number that gets printed as “0.333333574”. If arithmetic is done on fractions instead of on approximations to fractions, many computations can be done entirely without any accumulated rounding errors. This results in a comfortable feeling of security that is often lacking when floating point calculations have been made, and it means that the accuracy of the calculation cannot be improved upon.  Irrationality is the square root of all evil. — DOUGLAS HOFSTADTER, Metamagical Themas  1983   4.5.1. Fractions When fractional arithmetic is desired, the numbers can be represented as pairs of integers,  u u′ , where u and u′ are relatively prime to each other and u′ > 0. The number zero is represented as  0 1 . In this form,  u u′  =  v v′  if and only if u = v and u′ = v′. Multiplication of fractions is, of course, easy; to form  u u′  ×  v v′  =  w w′ , we can simply compute uv and u′v′. The two products uv and u′v′ might not be relatively prime, but if d = gcd uv, u′v′ , the desired answer is w = uv d, w′ = u′v′ d.  See exercise 2.  Efficient algorithms to compute the greatest common divisor are discussed in Section 4.5.2. Another way to perform the multiplication is to find d1 = gcd u, v′  and d2 = gcd u′, v ; then the answer is w =  u d1  v d2 , w′ =  u′ d2  v′ d1 .  See exercise 3.  This method requires two gcd calculations, but it is not really slower than the former method; the gcd process involves a number of iterations that is essentially proportional to the logarithm of its inputs, so the total number of iterations needed to evaluate both d1 and d2 is essentially the same as the number of iterations during the single calculation of d. Furthermore, each iteration in the evaluation of d1 and d2 is potentially faster, because comparatively small numbers are being examined. If u, u′, v, and v′ are single-precision quantities, this method has the advantage that no double-precision numbers appear in the calculation unless it is impossible to represent both of the answers w and w′ in single-precision form.  cedure is to set  u u′  ±  v v′  =  uv′ ± u′v  u′v′ and then to reduce this  Division may be done in a similar manner; see exercise 4. Addition and subtraction are slightly more complicated. The obvious pro- fraction to lowest terms by calculating d = gcd uv′ ± u′v, u′v′ , as in the first multiplication method. But again it is possible to avoid working with such large numbers, if we start by calculating d1 = gcd u′, v′ . If d1 = 1, then the desired numerator and denominator are w = uv′ ± u′v and w′ = u′v′.  According to Theorem 4.5.2D, d1 will be 1 about 61 percent of the time, if the denominators u′ and v′ are randomly distributed, so it is wise to single out this case.  If d1 > 1, then let t = u v′ d1  ± v u′ d1  and calculate d2 = gcd t, d1 ; finally the answer is w = t d2, w′ =  u′ d1  v′ d2 .  Exercise 6 proves that these values   FRACTIONS  4.5.1 331 of w and w′ are relatively prime to each other.  If single-precision numbers are being used, this method requires only single-precision operations, except that t may be a double-precision number or slightly larger  see exercise 7 ; since gcd t, d1  = gcd t mod d1, d1 , the calculation of d2 does not require double precision. then t = 7 · 2 + 17 · 11 = 201, and d2 = gcd 201, 6  = 3, so the answer is  For example, to compute  7 66  +  17 12 , we form d1 = gcd 66, 12  = 6;  66 6 · 12 3    201 3  = 67 44.  To help check out subroutines for rational arithmetic, inversion of matrices  with known inverses  like Cauchy matrices, exercise 1.2.3–41  is suggested. Experience with fractional calculations shows that in many cases the num- bers grow to be quite large. So if u and u′ are intended to be single-precision numbers for each fraction  u u′ , it is important to include tests for overflow in each of the addition, subtraction, multiplication, and division subroutines. For numerical problems in which perfect accuracy is important, a set of subrou- tines for fractional arithmetic with arbitrary precision allowed in numerator and denominator is very useful. The methods of this section extend also to other number fields besides the rational numbers; for example, we could do arithmetic on quantities of the form  u + u′√ 5   u′′, where u, u′, u′′ are integers, gcd u, u′, u′′  = 1, and u′′ > 0; or on quantities of the form  u + u′ 3√ Instead of insisting on exact calculations with fractions, it is interesting to consider also “fixed slash” and “floating slash” numbers, which are analogous to floating point numbers but based on rational fractions instead of radix-oriented fractions. In a binary fixed-slash scheme, the numerator and denominator of a representable fraction each consist of at most p bits, for some given p. In a floating-slash scheme, the sum of numerator bits plus denominator bits must be a total of at most q, for some given q, and another field of the representation is used to indicate how many of these q bits belong to the numerator. Infinity can be represented as  1 0 . To do arithmetic on such numbers, we define x ⊕ y = round x+y , x⊖y = round x−y , etc., where round x  = x if x is representable, otherwise it is one of the two representable numbers that surround x.  2 + u′′ 3√  4   u′′′, etc.  It may seem at first that the best definition of round x  would be to choose the representable number that is closest to x, by analogy with the way we round in floating point arithmetic. But experience has shown that it is best to bias our rounding towards “simple” numbers, since numbers with small numerator and denominator occur much more often than complicated fractions do. We want more numbers to round to 1 255. The rounding rule that turns out to be most successful in practice is called “mediant rounding”: If  u u′  and  v v′  are adjacent representable numbers, so that whenever u u′ ≤ x ≤ v v′ we must have round x  equal to  u u′  or  v v′ , the mediant rounding rule says that  2 than to 127  round x  = u  u′ for x <  u + v u′ + v′ ,  round x  = v  v′ for x >  u + v u′ + v′ .   1    ARITHMETIC  332 4.5.1 If x =  u + v   u′ + v′  exactly, we let round x  be the neighboring fraction with the smallest denominator  or, if u′ = v′, with the smallest numerator . Exercise 4.5.3–43 shows that it is not difficult to implement mediant rounding efficiently. For example, suppose we are doing fixed slash arithmetic with p = 8, so that the representable numbers  u u′  have −128 < u < 128 and 0 ≤ u′ < 256 and u ⊥ u′. This isn’t much precision, but it is enough to give us a feel for slash arithmetic. The numbers adjacent to 0 =  0 1  are  −1 255  and  1 255 ; according to the mediant rounding rule, we will therefore have round x  = 0 if and only if x ≤ 1 256. Suppose we have a calculation that would take the overall form 22 1113 if we were working in exact rational arithmetic, but the intermediate quantities have had to be rounded to representable numbers. In this case 314 1113 would round to  7 6 . The rounded terms sum to 79 120, which rounds to  22 7 ; so we have obtained the correct answer even though three roundings were required. This example was not specially contrived. When the answer to a problem is a simple fraction, slash arithmetic tends to make the intermediate rounding errors cancel out.  7 = 314 159 would round to  79 40  and 1300  159 + 1300  6 = 377  Exact representation of fractions within a computer was first discussed in the literature by P. Henrici, JACM 3  1956 , 6–9. Fixed and floating slash arithmetic were proposed by David W. Matula, in Applications of Number Theory to Numerical Analysis, edited by S. K. Zaremba  New York: Academic Press, 1972 , 486–489. Further developments of the idea are discussed by Matula and Kornerup in Proc. IEEE Symp. Computer Arith. 4  1978 , 29–38, 39–47; Lecture Notes in Comp. Sci. 72  1979 , 383–397; Computing, Suppl. 2  1980 , 85–111; IEEE Trans. C-32  1983 , 378–388; IEEE Trans. C-34  1985 , 3–18; IEEE Trans. C-39  1990 , 1106–1115.  40 + 7  EXERCISES 1. [15] Suggest a reasonable computational method for comparing two fractions, to test whether or not  u u′  <  v v′ . 2. [M15] Prove that if d = gcd u, v  then u d and v d are relatively prime. 3. [M20] Prove that u ⊥ u′ and v ⊥ v′ implies gcd uv, u′v′  = gcd u, v′  gcd u′, v . 4. [11] Design a division algorithm for fractions, analogous to the second multipli- cation method of the text.  Note that the sign of v must be considered.  5. [10] Compute  17 120  +  −27 70  by the method recommended in the text.   cid:120  6. [M23] Show that u ⊥ u′ and v ⊥ v′ implies gcd uv′ + vu′, u′v′  = d1d2, where  d1 = gcd u′, v′  and d2 = gcd d1, u v′ d1  + v u′ d1  .  uv′ + vu′  ⊥ u′v′.  7. [M22] How large can the absolute value of the quantity t become, in the addition- subtraction method recommended in the text, if the numerators and denominators of the inputs are less than N in absolute value?   cid:120  8. [22] Discuss using  1 0  and  −1 0  as representations for ∞ and −∞, and or as   Hence if d1 = 1 we have  representations of overflow. 9. [M23] If 1 ≤ u′, v′ < 2n, show that ⌊22nu u′⌋ = ⌊22nv v′⌋ implies u u′ = v v′.   4.5.2  THE GREATEST COMMON DIVISOR  333  √  10. [41] Extend the subroutines suggested in exercise 4.3.1–34 so that they deal with “arbitrary” rational numbers. 11. [M23] Consider fractions of the form  u + u′ 5   u′′, where u, u′, u′′ are integers, gcd u, u′, u′′  = 1, and u′′ > 0. Explain how to divide two such fractions and to obtain a quotient having the same form. 12. [M16] What is the largest finite floating slash number, given a bound q on the numerator length plus the denominator length? Which numbers round to  0 1 ? 13. [20]  Matula and Kornerup.  Discuss the representation of floating slash numbers in a 32-bit word. 14. [M23] Explain how to compute the exact number of pairs of integers  u, u′  such that M1 < u ≤ M2 and N1 < u′ ≤ N2 and u ⊥ u′.  This can be used to determine how many numbers are representable in slash arithmetic. According to Theorem 4.5.2D, the number will be approximately  6 π2  M2 − M1  N2 − N1 .  15. [42] Modify one of the compilers at your installation so that it will replace all floating point calculations by floating slash calculations. Experiment with the use of slash arithmetic by running existing programs that were written by programmers who actually had floating point arithmetic in mind.  When special subroutines like square root or logarithm are called, your system should automatically convert slash numbers to floating point form before the subroutine is invoked, then back to slash form again afterwards. There should be a new option to print slash numbers in a fractional format; however, you should also print slash numbers in decimal notation as usual, if no changes are made to a user’s source program.  Are the results better or worse, when floating slash numbers are substituted? 16. [40] Experiment with interval arithmetic on slash numbers.  4.5.2. The Greatest Common Divisor If u and v are integers, not both zero, we say that their greatest common divisor, gcd u, v , is the largest integer that evenly divides both u and v. This definition makes sense, because if u ̸= 0 then no integer greater than u can evenly divide u, but the integer 1 does divide both u and v; hence there must be a largest integer that divides them both. When u and v are both zero, every integer evenly divides zero, so the definition above does not apply; it is convenient to set  gcd 0, 0  = 0.   1   The definitions just given obviously imply that  gcd u, v  = gcd v, u , gcd u, v  = gcd −u, v , gcd u, 0  = u.   2   3   4  In the previous section, we reduced the problem of expressing a rational number in lowest terms to the problem of finding the greatest common divisor of its numerator and denominator. Other applications of the greatest common divisor have been mentioned for example in Sections 3.2.1.2, 3.3.3, 4.3.2, 4.3.3. So the concept of gcd u, v  is important and worthy of serious study.   334  ARITHMETIC  4.5.2  The least common multiple of two integers u and v, written lcm u, v , is a related idea that is also important. It is defined to be the smallest positive integer that is an integer multiple of both u and v; and lcm u, 0  = lcm 0, v  = 0. The classical method for teaching children how to add fractions u u′ + v v′ is to train them to find the “least common denominator,” which is lcm u′, v′ .  According to the “fundamental theorem of arithmetic”  proved in exercise  1.2.4–21 , each positive integer u can be expressed in the form  u = 2u23u35u57u711u11 . . . =    5   pup ,  p prime  where the exponents u2, u3, . . . are uniquely determined nonnegative integers, and where all but a finite number of the exponents are zero. From this canonical factorization of a positive integer, we immediately obtain one way to compute the greatest common divisor of u and v: By  2 ,  3 , and  4 , we may assume that u and v are positive integers, and if both of them have been canonically factored into primes we have  gcd u, v  =  lcm u, v  =   p prime  p prime  pmin up,vp ,  pmax up,vp .   6    7   Thus, for example, the greatest common divisor of u = 7000 = 23 · 53 · 7 and v = 4400 = 24 · 52 · 11 is 2min 3,4  5min 3,2  7min 1,0  11min 0,1  = 23 · 52 = 200. The least common multiple of the same two numbers is 24 · 53 · 7 · 11 = 154000.  From formulas  6  and  7  we can easily prove a number of basic identities  concerning the gcd and the lcm:  if w ≥ 0; if w ≥ 0; if u, v ≥ 0;  u · v = gcd u, v  · lcm u, v ,  gcd u, v w = gcd uw, vw , lcm u, v w = lcm uw, vw ,  gcdlcm u, v , lcm u, w  = lcmu, gcd v, w ; lcmgcd u, v , gcd u, w  = gcdu, lcm v, w .   8   9   10   11   12  The latter two formulas are “distributive laws” analogous to the familiar identity uv + uw = u v + w . Equation  10  reduces the calculation of gcd u, v  to the calculation of lcm u, v , and conversely. Euclid’s algorithm. Although Eq.  6  is useful for theoretical purposes, it is generally no help for calculating a greatest common divisor in practice, because it requires that we first determine the canonical factorization of u and v. There is no known way to find the prime factors of an integer very rapidly  see Section 4.5.4 . But fortunately the greatest common divisor of two integers can be found efficiently without factoring, and in fact such a method was discovered more than 2250 years ago; it is Euclid’s algorithm, which we have already examined in Sections 1.1 and 1.2.1.   4.5.2  THE GREATEST COMMON DIVISOR  335  Euclid’s algorithm is found in Book 7, Propositions 1 and 2 of his Elements  c. 300 B.C. , but it probably wasn’t his own invention. Some scholars believe that the method was known up to 200 years earlier, at least in its subtractive form, and it was almost certainly known to Eudoxus  c. 375 B.C. ; see K. von Fritz, Ann. Math.  2  46  1945 , 242–264. Aristotle  c. 330 B.C.  hinted at it in his Topics, 158b, 29–35. However, very little hard evidence about such early history has survived [see W. R. Knorr, The Evolution of the Euclidean Elements  Dordrecht: 1975 ].  We might call Euclid’s method the granddaddy of all algorithms, because it is the oldest nontrivial algorithm that has survived to the present day.  The chief rival for this honor is perhaps the ancient Egyptian method for multiplication, which was based on doubling and adding, and which forms the basis for efficient calculation of nth powers as explained in Section 4.6.3. But the Egyptian manuscripts merely give examples that are not completely systematic, and the examples were certainly not stated systematically; the Egyptian method is there- fore not quite deserving of the name “algorithm.” Several ancient Babylonian methods, for doing such things as solving special sets of quadratic equations in two variables, are also known. Genuine algorithms are involved in this case, not just special solutions to the equations for certain input parameters; even though the Babylonians invariably presented each method in conjunction with an example worked with particular input data, they regularly explained the general procedure in the accompanying text. [See D. E. Knuth, CACM 15  1972 , 671– 677; 19  1976 , 108.] Many of these Babylonian algorithms predate Euclid by 1500 years, and they are the earliest known instances of written procedures for mathematics. But they do not have the stature of Euclid’s algorithm, since they do not involve iteration and since they have been superseded by modern algebraic methods.   In view of the importance of Euclid’s algorithm, for historical as well as practical reasons, let us now consider how Euclid himself treated it. Paraphrased into modern terminology, this is essentially what he wrote:  Proposition. Given two positive integers, find their greatest common divisor. Let A and C be the two given positive integers; it is required to find their greatest common divisor. If C divides A, then C is a common divisor of C and A, since it also divides itself. And it clearly is in fact the greatest, since no greater number than C will divide C. But if C does not divide A, then continually subtract the lesser of the numbers A, C from the greater, until some number is left that divides the previous one. This will eventually happen, for if unity is left, it will divide the previous number. Now let E be the positive remainder of A divided by C; let F be the positive remainder of C divided by E; and suppose that F is a divisor of E. Since F divides E and E divides C − F , F also divides C − F ; but it also divides itself, so it divides C. And C divides A − E; therefore F also divides A − E. But it also divides E; therefore it divides A. Hence it is a common divisor of A and C. I now claim that it is also the greatest. For if F is not the greatest common divisor of A and C, some larger number will divide them both. Let such a number be G.   336  ARITHMETIC  4.5.2  Now since G divides C while C divides A− E, G divides A− E. G also divides the whole of A, so it divides the remainder E. But E divides C − F ; therefore G also divides C − F . And G also divides the whole of C, so it divides the remainder F ; that is, a greater number divides a smaller one. This is impossible. Therefore no number greater than F will divide A and C, so F is their greatest common divisor. Corollary. This argument makes it evident that any number dividing two num- bers divides their greatest common divisor. Q.E.D.  Euclid’s statements have been simplified here in one nontrivial respect: Greek mathematicians did not regard unity as a “divisor” of another positive integer. Two positive integers were either both equal to unity, or they were relatively prime, or they had a greatest common divisor. In fact, unity was not even considered to be a “number,” and zero was of course nonexistent. These rather awkward conventions made it necessary for Euclid to duplicate much of his discussion, and he gave two separate propositions that are each essentially like the one appearing here.  In his discussion, Euclid first suggests subtracting the smaller of the two current numbers from the larger, repeatedly, until we get two numbers where one is a multiple of the other. But in the proof he really relies on taking the remainder of one number divided by another; and since he has no simple concept of zero, he cannot speak of the remainder when one number divides the other. It is reasonable to say that he imagines each division  not the individual subtractions  as a single step of the algorithm, and hence an “authentic” rendition of his algorithm can be phrased as follows: Algorithm E  Original Euclidean algorithm . Given two integers A and C greater than unity, this algorithm finds their greatest common divisor. E1. [Is A divisible by C?] If C divides A, the algorithm terminates with C as  the answer.  E2. [Replace A by remainder.] If A mod C is equal to unity, the given numbers were relatively prime, so the algorithm terminates. Otherwise replace the pair of values  A, C  by  C, A mod C  and return to step E1. Euclid’s “proof” quoted above is especially interesting because it is not really a proof at all! He verifies the result of the algorithm only if step E1 is performed once or thrice. Surely he must have realized that step E1 could take place more than three times, although he made no mention of such a possibility. Not having the notion of a proof by mathematical induction, he could only give a proof for a finite number of cases.  In fact, he often proved only the case n = 3 of a theorem that he wanted to establish for general n.  Although Euclid is justly famous for the great advances he made in the art of logical deduction, techniques for giving valid proofs by induction were not discovered until many centuries later, and the crucial ideas for proving the validity of algorithms are only now becoming really clear.  See Section 1.2.1 for a complete proof of Euclid’s algorithm, together with a short discussion of general proof procedures for algorithms.    4.5.2  THE GREATEST COMMON DIVISOR  337  It is worth noting that this algorithm for finding the greatest common divisor was chosen by Euclid to be the very first step in his development of the theory of numbers. The same order of presentation is still in use today in modern textbooks. Euclid also gave a method  Proposition 34  to find the least common multiple of two integers u and v, namely to divide u by gcd u, v  and to multiply the result by v; this is equivalent to Eq.  10 .  If we avoid Euclid’s bias against the numbers 0 and 1, we can reformulate  Algorithm E in the following way. Algorithm A  Modern Euclidean algorithm . Given nonnegative integers u  and v, this algorithm finds their greatest common divisor. Note: The greatest algorithm to u and v, because of Eqs.  2  and  3 .  common divisor of arbitrary integers u and v may be obtained by applying this  A1. [v = 0?] If v = 0, the algorithm terminates with u as the answer. A2. [Take u mod v.] Set r ← u mod v, u ← v, v ← r, and return to A1.  The operations of this step decrease the value of v, but they leave gcd u, v  unchanged.   For example, we may calculate gcd 40902, 24140  as follows:  gcd 40902, 24140  = gcd 24140, 16762  = gcd 16762, 7378   = gcd 7378, 2006  = gcd 2006, 1360  = gcd 1360, 646  = gcd 646, 68  = gcd 68, 34  = gcd 34, 0  = 34.  The validity of Algorithm A follows readily from Eq.  4  and the fact that  13  if q is any integer. Equation  13  holds because any common divisor of u and v is a divisor of both v and u − qv, and, conversely, any common divisor of v and u − qv must divide both u and v.  gcd u, v  = gcd v, u − qv ,  The following MIX program illustrates the fact that Algorithm A can easily  be implemented on a computer: Program A  Euclid’s algorithm . Assume that u and v are single-precision, nonnegative integers, stored respectively in locations U and V; this program puts gcd u, v  into rA.  LDX U JMP 2F 1H STX V SRAX 5 DIV V 2H LDA V  1 1 T T T  rX ← u. v ← rX. rAX ← rA. rX ← rAX mod v.  1 + T rA ← v.  JXNZ 1B 1 + T Done if rX = 0.  The running time for this program is 19T + 6 cycles, where T is the number of divisions performed. The discussion in Section 4.5.3 shows that we may take T = 0.842766 ln N + 0.06 as an approximate average value, when u and v are independently and uniformly distributed in the range 1 ≤ u, v ≤ N.   338  ARITHMETIC  4.5.2  A binary method. Since Euclid’s patriarchal algorithm has been used for so many centuries, it is rather surprising that it might not be the best way to find the greatest common divisor after all. A quite different gcd algorithm, primarily suited to binary arithmetic, was devised by Josef Stein in 1961 [see J. Comp. Phys. 1  1967 , 397–405]. This new algorithm requires no division instruction; it relies solely on the operations of subtraction, parity testing, and halving of even numbers  which corresponds to a right shift in binary notation .  The binary gcd algorithm is based on four simple facts about positive inte-  gers u and v: a  If u and v are both even, then gcd u, v  = 2 gcd u 2, v 2 . [See Eq.  8 .] b  If u is even and v is odd, then gcd u, v  = gcd u 2, v . [See Eq.  6 .] c  As in Euclid’s algorithm, gcd u, v  = gcd u − v, v . [See Eqs.  13 ,  2 .] d  If u and v are both odd, then u − v is even, and u − v < max u, v . Algorithm B  Binary gcd algorithm . Given positive integers u and v, this algorithm finds their greatest common divisor. B1. [Find power of 2.] Set k ← 0, and then repeatedly set k ← k + 1, u ← u 2,  v ← v 2, zero or more times until u and v are not both even.  B2. [Initialize.]  Now the original values of u and v have been divided by 2k, and at least one of their present values is odd.  If u is odd, set t ← −v and go to B4. Otherwise set t ← u.  B3. [Halve t.]  At this point, t is even, and nonzero.  Set t ← t 2. B4. [Is t even?] If t is even, go back to B3. B5. [Reset max u, v .] If t > 0, set u ← t; otherwise set v ← −t.  The larger of u and v has been replaced by t, except perhaps during the first time this step is performed.   B6. [Subtract.] Set t ← u − v. If t ̸= 0, go back to B3. Otherwise the algorithm  terminates with u · 2k as the output. As an example of Algorithm B, let us consider u = 40902, v = 24140, the same numbers we used when trying out Euclid’s algorithm. Step B1 sets k ← 1, u ← 20451, v ← 12070. Then t is set to −12070, and replaced by −6035; then v is replaced by 6035, and the computation proceeds as follows:  u  20451 901 901 901 17 17 17  v  6035 6035 2567 833 833 51 17  t  +14416, +7208, +3604, +1802, +901; −5134, −2567; −1666, −833; +68, +34, +17; −816, −408, −204, −102, −51; −34, −17; 0.  The answer is 17 · 21 = 34. A few more iterations were necessary here than we needed with Algorithm A, but each iteration was somewhat simpler since no division steps were used.   4.5.2  THE GREATEST COMMON DIVISOR  339  Fig. 9. Binary algorithm for the greatest common divisor.  A MIX program for Algorithm B requires a bit more code than for Algo- rithm A, but the steps are elementary. In order to make such a program fairly typical of a binary computer’s representation of Algorithm B, let us assume that MIX is extended to include the following operators:   SLB  shift left AX binary . C = 6; F = 6. The contents of registers A and X are “shifted left” M binary places; that is, rAX ← 2MrAX mod B10, where B is the byte size.  As with all MIX shift commands, the signs of rA and rX are not affected.    SRB  shift right AX binary . C = 6; F = 7. The contents of registers A and X are “shifted right” M binary places; that is, rAX ← ⌊rAX 2M⌋.   JAE, JAO  jump A even, jump A odd . C = 40; F = 6, 7, respectively. A JMP occurs if rA is even or odd, respectively.   JXE, JXO  jump X even, jump X odd . C = 47; F = 6, 7, respectively. Analogous to JAE, JAO. Program B  Binary gcd algorithm . Assume that u and v are single-precision positive integers, stored respectively in locations U and V; this program uses Algorithm B to put gcd u, v  into rA. Register assignments: rA ≡ t, rI1 ≡ k. 01 ABS EQU 1:5 02 B1 ENT1 0 LDX U 03 LDAN V 04 JMP 1F 05 06 2H SRB 1 INC1 1 07 STX U 08 STA V ABS  09 10 1H JXO B4 11 B2 JAE 2B  Halve rA, rX. k ← k + 1. u ← u 2. v ← v 2. To B4 with t ← −v if u is odd. B2. Initialize.  B1. Find power of 2. rX ← u. rA ← −v.  1 1 1 1 A A A A  1 + A B + A  B1. Find power of 2  B2. Initialize  B3. Halve t  B4. Is t even?  B5. Reset max u, v   Yes  No  u  cid:54 = v  B6. Subtract  u = v   4.5.2  340  ARITHMETIC  LDA U 12 13 B3 SRB 1 14 B4 JAE B3 15 B5 JAN 1F STA U 16 SUB V 17 JMP 2F 18 19 1H STA V ABS  20 B6 ADD U 21 2H JANZ B3 LDA U 22 ENTX 0 23 SLB 0,1 24  t ← u. B3. Halve t. 1 − B + D B4. Is t even?  B D  C − E C − E  C E E E  C 1 1 1  B5. Reset max u, v . If t > 0, set u ← t. t ← u − v. If t < 0, set v ← −t. B6. Subtract. To B3 if t ̸= 0. rA ← u. rX ← 0. rA ← 2k · rA.  The running time of this program is  9A + 2B + 6C + 3D + E + 13  3, B = 1  units, where A = k, B = 1 if t ← u in step B2  otherwise B = 0 , C is the number of subtraction steps, D is the number of halvings in step B3, and E is the number of times t > 0 in step B5. Calculations discussed later in this section 3, C = 0.71N − 0.5, D = 1.41N − 2.7, and imply that we may take A = 1 E = 0.35N − 0.4 as average values for these quantities, assuming random inputs u and v in the range 1 ≤ u, v < 2N. The total running time is therefore about 8.8N +5.2 cycles, compared to about 11.1N +7.1 for Program A under the same assumptions. The worst possible running time for u and v in this range occurs when A = 0, B = 1, C = N, D = 2N − 2, E = N − 1; this amounts to 13N + 8 cycles.  The corresponding value for Program A is 26.8N + 19.   Thus the greater speed of the iterations in Program B, due to the simplicity of the operations, compensates for the greater number of iterations required. We have found that the binary algorithm is about 20 percent faster than Euclid’s algorithm on the MIX computer. Of course, the situation may be different on other computers, and in any event both programs are quite efficient; but it appears that not even a procedure as venerable as Euclid’s algorithm can withstand progress.  The binary gcd algorithm itself might have a distinguished pedigree, since it may well have been known in ancient China. Chapter 1, Section 6 of a classic text called Chiu Chang Suan Shu, the “Nine Chapters on Arithmetic”  c. 1st century A.D. , gives the following method for reducing a fraction to lowest terms:  If halving is possible, take half. Otherwise write down the denominator and the numerator, and subtract the smaller from the greater. Repeat until both numbers are equal. Simplify with this common value.  If the repeat instruction means to go back to the halving step instead of to repeat the subtraction step — this point isn’t clear — the method is essentially Algorithm B. [See Y. Mikami, The Development of Mathematics in China   4.5.2  THE GREATEST COMMON DIVISOR  341  and Japan  Leipzig: 1913 , 11; K. Vogel, Neun Bücher arithmetischer Technik  Braunschweig: Vieweg, 1968 , 8.]  V. C. Harris [Fibonacci Quarterly 8  1970 , 102–103; see also V. A. Le- besgue, J. Math. Pures Appl. 12  1847 , 497–520] has suggested an interesting cross between Euclid’s algorithm and the binary algorithm. If u and v are odd, with u ≥ v > 0, we can always write  u = qv ± r  where 0 ≤ r < v and r is even; if r ̸= 0 we set r ← r 2 until r is odd, then set u ← v, v ← r and repeat the process. In subsequent iterations, q ≥ 3. Extensions. We can extend the methods used to calculate gcd u, v  in order to solve some slightly more difficult problems. For example, assume that we want to compute the greatest common divisor of n integers u1, u2, . . . , un.  One way to calculate gcd u1, u2, . . . , un , assuming that the u’s are all nonnegative, is to extend Euclid’s algorithm in the following way: If all uj are zero, the greatest common divisor is taken to be zero; otherwise if only one uj is nonzero, it is the greatest common divisor; otherwise replace uk by uk mod uj for all k ̸= j, where uj is the minimum of the nonzero u’s, and repeat the process. The algorithm sketched in the preceding paragraph is a natural generaliza- tion of Euclid’s method, and it can be justified in a similar manner. But there is a simpler method available, based on the easily verified identity  gcd u1, u2, . . . , un  = gcdu1, gcd u2, . . . , un .   14   To calculate gcd u1, u2, . . . , un , we may therefore proceed as follows: Algorithm C  Greatest common divisor of n integers . Given integers u1, u2, . . . , un, where n ≥ 1, this algorithm computes their greatest common divisor, using an algorithm for the case n = 2 as a subroutine. C1. Set d ← un, k ← n − 1. C2. If d ̸= 1 and k > 0, set d ← gcd uk, d  and k ← k − 1 and repeat this step.  Otherwise d = gcd u1, . . . , un .  This method reduces the calculation of gcd u1, . . . , un  to repeated calculations of the greatest common divisor of two numbers at a time. It makes use of the fact that gcd u1, . . . , uk, 1  = 1; and this will be helpful, since we will already have gcd un−1, un  = 1 more than 60 percent of the time, if un−1 and un are chosen at random. In most cases the value of d will decrease rapidly during the first few stages of the calculation, and this will make the remainder of the computation quite fast. Here Euclid’s algorithm has an advantage over Algorithm B, because its running time is primarily governed by the value of min u, v , while the running time for Algorithm B is primarily governed by max u, v ; it would be reasonable to perform one iteration of Euclid’s algorithm, replacing u by u mod v if u is much larger than v, and then to continue with Algorithm B.   342  ARITHMETIC  4.5.2  The assertion that gcd un−1, un  will be equal to unity more than 60 percent of the time for random inputs is a consequence of the following well-known result of number theory: Theorem D. [G. Lejeune Dirichlet, Abhandlungen Königlich Preuß. Akad. Wiss.  1849 , 69–83.] If u and v are integers chosen at random, the probability that gcd u, v  = 1 is 6 π2 ≈ .60793.  A precise formulation of this theorem, which defines carefully what is meant by being “chosen at random,” appears in exercise 10 with a rigorous proof. Let us content ourselves here with a heuristic argument that shows why the theorem is plausible. If we assume, without proof, the existence of a well-defined probability p that u ⊥ v, then we can determine the probability that gcd u, v  = d for any positive integer d, because gcd u, v  = d if and only if u is a multiple of d and v is a multiple of d and u d ⊥ v d. Thus the probability that gcd u, v  = d is equal to 1 d times 1 d times p, namely p d2. Now let us sum these probabilities over all possible values of d; we should get  1 =  p d2 = p1 + 1  16 + ···.  4 + 1  9 + 1  d≥1 9 + ··· = H  Since the sum 1 + 1 p = 6 π2 in order to make this equation come out right.  4 + 1   2 ∞ is equal to π2 6 by Eq. 1.2.7– 7 , we need  Euclid’s algorithm can be extended in another important way: We can  calculate integers u′ and v′ such that  uu′ + vv′ = gcd u, v    15  at the same time gcd u, v  is being calculated. This extension of Euclid’s algo- rithm can be described conveniently in vector notation: Algorithm X  Extended Euclid’s algorithm . Given nonnegative integers u and v, this algorithm determines a vector  u1, u2, u3  such that uu1 + vu2 = u3 = gcd u, v . The computation makes use of auxiliary vectors  v1, v2, v3 ,  t1, t2, t3 ; all vectors are manipulated in such a way that the relations  ut1 + vt2 = t3,  uu1 + vu2 = u3,  uv1 + vv2 = v3   16   hold throughout the calculation. X1. [Initialize.] Set  u1, u2, u3  ←  1, 0, u ,  v1, v2, v3  ←  0, 1, v . X2. [Is v3 = 0?] If v3 = 0, the algorithm terminates. X3. [Divide, subtract.] Set q ← ⌊u3 v3⌋, and then set   t1, t2, t3  ←  u1, u2, u3  −  v1, v2, v3 q,   u1, u2, u3  ←  v1, v2, v3 ,   v1, v2, v3  ←  t1, t2, t3 .  Return to step X2.   4.5.2  THE GREATEST COMMON DIVISOR  343  u3  For example, let u = 40902, v = 24140. At step X2 we have v2 1 −1 2 −5 17 −22 61 −571 1203  u2 0 1 −1 2 −5 17 −22 61 −571  v1 0 1 −1 3 −10 13 −36 337 −710  40902 24140 16762 7378 2006 1360 646 68 34  u1 1 0 1 −1 3 −10 13 −36 337  q — 1 1 2 3 1 2 9 2  v3  24140 16762 7378 2006 1360 646 68 34 0  The solution is therefore 337 · 40902 − 571 · 24140 = 34 = gcd 40902, 24140 .  Algorithm X can be traced to the ¯Aryabhat.¯ıya  A.D. 499  by ¯Aryabhat.a of northern India. His description was rather cryptic, but later commentators such as Bh¯askara I in the seventh century clarified the rule, which was called kut.t.aka  “the pulverizer” . [See B. Datta and A. N. Singh, History of Hindu Mathematics 2  Lahore: Motilal Banarsi Das, 1938 , 89–116.] Its validity follows from  16  and the fact that the algorithm is identical to Algorithm A with respect to its manipulation of u3 and v3; a detailed proof of Algorithm X is discussed in Section 1.2.1. Gordon H. Bradley has observed that we can avoid a good deal of the calculation in Algorithm X by suppressing u2, v2, and t2; then u2 can be determined afterwards using the relation uu1 + vu2 = u3. Exercise 15 shows that the values of u1, u2, v1, and v2 remain bounded by the size of the inputs u and v. Algorithm B, which computes the greatest common divisor using properties of binary notation, can be extended in a similar way; see exercise 39. For some instructive extensions to Algorithm X, see exercises 18 and 19 in Section 4.6.1.  The ideas underlying Euclid’s algorithm can also be applied to find a general solution in integers of any set of linear equations with integer coefficients. For example, suppose that we want to find all integers w, x, y, z that satisfy the two equations  17   18   10w + 3x + 3y + 8z = 1, 6w − 7x − 5z = 2.  We can introduce a new variable  ⌊10 3⌋w + ⌊3 3⌋x + ⌊3 3⌋y + ⌊8 3⌋z = 3w + x + y + 2z = t1,  and use it to eliminate y; Eq.  17  becomes   10 mod 3 w +  3 mod 3 x + 3t1 +  8 mod 3 z = w + 3t1 + 2z = 1,   19  and Eq.  18  remains unchanged. The new equation  19  may be used to elim- inate w, and  18  becomes  6 1 − 3t1 − 2z  − 7x − 5z = 2;   4.5.2   20    23   344  ARITHMETIC  that is,  Now as before we introduce a new variable  7x + 18t1 + 17z = 4.  x + 2t1 + 2z = t2  and eliminate x from  20 :  7t2 + 4t1 + 3z = 4.   21  Another new variable can be introduced in the same fashion, in order to eliminate the variable z, which has the smallest coefficient: 2t2 + t1 + z = t3.  Eliminating z from  21  yields   22  and this equation, finally, can be used to eliminate t2. We are left with two independent variables, t1 and t3; substituting back for the original variables, we obtain the general solution  t2 + t1 + 3t3 = 4,  w = 17 − 5t1 − 14t3, x = 20 − 5t1 − 17t3, y = −55 + 19t1 + 45t3, z = −8 + t1 + 7t3.  In other words, all integer solutions  w, x, y, z  to the original equations  17  and  18  are obtained from  23  by letting t1 and t3 independently run through all integers.  The general method that has just been illustrated is based on the following procedure: Find a nonzero coefficient c of smallest absolute value in the system of equations. Suppose that this coefficient appears in an equation having the form  cx0 + c1x1 + ··· + ckxk = d;   24  and assume for simplicity that c > 0. If c = 1, use this equation to eliminate the variable x0 from the other equations remaining in the system; then repeat the procedure on the remaining equations.  If no more equations remain, the computation stops, and a general solution in terms of the variables not yet eliminated has essentially been obtained.  If c > 1, then if c1 mod c = ··· = ck mod c = 0 check that d mod c = 0, otherwise there is no integer solution; then divide both sides of  24  by c and eliminate x0 as in the case c = 1. Finally, if c > 1 and not all of c1 mod c, . . . , ck mod c are zero, then introduce a new variable  ⌊c c⌋x0 + ⌊c1 c⌋x1 + ··· + ⌊ck c⌋xk = t;   25    4.5.2  THE GREATEST COMMON DIVISOR  345   26   eliminate the variable x0 from the other equations, in favor of t, and replace the original equation  24  by  See  19  and  21  in the example above.  ct +  c1 mod c x1 + ··· +  ck mod c xk = d.  This process must terminate, since each step reduces either the number of equations or the size of the smallest nonzero coefficient in the system. When this procedure is applied to the equation ux + vy = 1, for specific integers u and v, it runs through essentially the steps of Algorithm X.  The transformation-of-variables procedure just explained is a simple and straightforward way to solve linear equations when the variables are allowed to take on integer values only, but it isn’t the best method available for this problem. Substantial refinements are possible, but beyond the scope of this book. [See Henri Cohen, A Course in Computational Algebraic Number Theory  New York: Springer, 1993 , Chapter 2.]  Variants of Euclid’s algorithm can be used also with Gaussian integers u+iu′ and in certain other quadratic number fields. See, for example, A. Hurwitz, Acta Math. 11  1887 , 187–200; E. Kaltofen and H. Rolletschek, Math. Comp. 53  1989 , 697–720; A. Knopfmacher and J. Knopfmacher, BIT 31  1991 , 286– 292. High-precision calculation. If u and v are very large integers, requiring a multiple-precision representation, the binary method  Algorithm B  is a simple and fairly efficient means of calculating their greatest common divisor, since it involves only subtractions and shifting.  By contrast, Euclid’s algorithm seems much less attractive, since step A2 requires a multiple-precision division of u by v. But this difficulty is not really as bad as it seems, since we will prove in Section 4.5.3 that the quotient ⌊u v⌋ is almost always very small. For example, assuming random inputs, the quotient ⌊u v⌋ will be less than 1000 approximately 99.856 percent of the time. Therefore it is almost always possible to find ⌊u v⌋ and  u mod v  using single-precision calculations, together with the comparatively simple operation of calculating u − qv where q is a single-precision number. Furthermore, if it does turn out that u is much larger than v  for instance, the initial input data might have this form , we don’t really mind having a large quotient q, since Euclid’s algorithm makes a great deal of progress when it replaces u by u mod v in such a case.  A significant improvement in the speed of Euclid’s algorithm when high- precision numbers are involved can be achieved by using a method due to D. H. Lehmer [AMM 45  1938 , 227–233]. Working only with the leading digits of large numbers, it is possible to do most of the calculations with single-precision arithmetic, and to make a substantial reduction in the number of multiple- precision operations involved. The idea is to save time by doing a “virtual” calculation instead of the actual one.  For example, let us consider the pair of eight-digit numbers u = 27182818, v = 10000000, assuming that we are using a machine with only four-digit words.   ARITHMETIC  346 4.5.2 Let u′ = 2718, v′ = 1001, u′′ = 2719, v′′ = 1000; then u′ v′ and u′′ v′′ are approximations to u v, with  u′ v′ < u v < u′′ v′′.   27  The ratio u v determines the sequence of quotients obtained in Euclid’s algo- rithm. If we perform Euclid’s algorithm simultaneously on the single-precision values  u′, v′  and  u′′, v′′  until we get a different quotient, it is not difficult to see that the same sequence of quotients would have appeared to this point if we had worked with the multiple-precision numbers  u, v . Thus, consider what happens when Euclid’s algorithm is applied to  u′, v′  and to  u′′, v′′ : q′′ 2 1 2 1 1 3  u′ 2718 1001 716 285 146 139  u′′ 2719 1000 719 281 157 124  v′ 1001 716 285 146 139 7  v′′ 1000 719 281 157 124 33  q′ 2 1 2 1 1 19  The first five quotients are the same in both cases, so they must be the true ones. But on the sixth step we find that q′ ̸= q′′, so the single-precision calculations are suspended. We have gained the knowledge that the calculation would have proceeded as follows if we had been working with the original multiple-precision numbers:  v v0  u u0 v0  u0 − 2v0 u0 − 2v0 −u0 + 3v0 3u0 − 8v0 −u0 + 3v0 3u0 − 8v0 −4u0 + 11v0 7u0 − 19v0 −4u0 + 11v0  q 2 1 2 1 1 ?   28    The next quotient lies somewhere between 3 and 19.  No matter how many digits are in u and v, the first five steps of Euclid’s algorithm would be the same as  28 , so long as  27  holds. We can therefore avoid the multiple-precision operations of the first five steps, and replace them all by a multiple-precision calculation of −4u0 + 11v0 and 7u0 − 19v0. In this case we obtain u = 1268728, v = 279726; the calculation can now continue in a similar manner with u′ = 1268, v′ = 280, u′′ = 1269, v′′ = 279, etc. If we had a larger accumulator, more steps could be done by single-precision calculations. Our example showed that only five cycles of Euclid’s algorithm were combined into one multiple step, but with  say  a word size of 10 digits we could do about twelve cycles at a time. Results proved in Section 4.5.3 imply that the number of multiple-precision cycles that can be replaced at each iteration is essentially proportional to the number of digits used in the single-precision calculations.  Lehmer’s method can be formulated as follows:   4.5.2  THE GREATEST COMMON DIVISOR  347  Algorithm L  Euclid’s algorithm for large numbers . Let u and v be nonnegative integers, with u ≥ v, represented in multiple precision. This algorithm computes the greatest common divisor of u and v, making use of auxiliary single-precision p-digit variables ˆu, ˆv, A, B, C, D, T, q, and auxiliary multiple-precision variables t and w. L1. [Initialize.]  If v is small enough to be represented as a single-precision value, calculate gcd u, v  by Algorithm A and terminate the computation. Otherwise, let ˆu be the p leading digits of u, and let ˆv be the corresponding digits of v; in other words, if radix-b notation is being used, ˆu ← ⌊u bk⌋ and ˆv ← ⌊v bk⌋, where k is as small as possible consistent with the condition ˆu < bp. Set A ← 1, B ← 0, C ← 0, D ← 1.  These variables represent the  coefficients in  28 , where  u = Au0 + Bv0,   29  in the equivalent actions of Algorithm A on multiple-precision numbers. We also have  and v = Cu0 + Dv0,  u′ = ˆu + B,  v′ = ˆv + D,  u′′ = ˆu + A,  v′′ = ˆv + C   30   in terms of the notation in the example worked above.   L2. [Test quotient.] Set q ← ⌊ ˆu + A   ˆv + C ⌋.  If q ̸= ⌊ ˆu + B   ˆv + D ⌋, go to step L4.  This step tests if q′ ̸= q′′, in the notation of the example above. Single-precision overflow can occur in special circumstances during the computation in this step, but only when ˆu = bp − 1 and A = 1 or when ˆv = bp − 1 and D = 1; the conditions  0 ≤ ˆu + A ≤ bp, 0 ≤ ˆu + B < bp,  0 ≤ ˆv + C < bp, 0 ≤ ˆv + D ≤ bp   31   will always hold, because of  30 . It is possible to have ˆv+C = 0 or ˆv+D = 0, but not both simultaneously; therefore division by zero in this step is taken to mean “Go directly to L4.”   L3. [Emulate Euclid.] Set T ← A − qC, A ← C, C ← T, T ← B − qD, B ← D,  D ← T, T ← ˆu − qˆv, ˆu ← ˆv, ˆv ← T, and go back to step L2. These single- in  28 , under the conventions of  29 .  precision calculations are the equivalent of multiple-precision operations, as  L4. [Multiprecision step.]  If B = 0, set t ← u mod v, u ← v, v ← t, using multiple-precision division.  This happens only if the single-precision oper- ations cannot simulate any of the multiple-precision ones. It implies that Euclid’s algorithm requires a very large quotient, and this is an extremely rare occurrence.  Otherwise, set t ← Au, t ← t+Bv, w ← Cu, w ← w+Dv, u ← t, v ← w  using straightforward multiple-precision operations . Go back to step L1.   348  ARITHMETIC  4.5.2  The values of A, B, C, D remain as single-precision numbers throughout  this calculation, because of  31 .  Algorithm L requires a somewhat more complicated program than Algo- rithm B, but with large numbers it will be faster on many computers. The binary technique of Algorithm B can, however, be speeded up in a similar way  see exercise 38 , to the point where it continues to win. Algorithm L has the advantage that it determines the sequence of quotients obtained in Euclid’s algo- rithm, and this sequence has numerous applications  see, for example, exercises 43, 47, 49, and 51 in Section 4.5.3 . See also exercise 4.5.3–46. *Analysis of the binary algorithm. Let us conclude this section by studying the running time of Algorithm B, in order to justify the formulas stated earlier. An exact determination of Algorithm B’s behavior appears to be exceedingly difficult to derive, but we can begin to study it by means of an approximate model. Suppose that u and v are odd numbers, with u > v and  Thus, u is an  m + 1 -bit number, and v is an  n + 1 -bit number. Consider  ⌊lg u⌋ = m,  ⌊lg v⌋ = n.  a subtract-and-shift cycle of Algorithm B, namely an operation that starts at step B6 and then stops after step B5 is finished. Every subtract-and-shift cycle with u > v forms u − v and shifts this quantity right until obtaining an odd number u′ that replaces u. Under random conditions, we would expect to have u′ =  u − v  2 about one-half of the time, u′ =  u − v  4 about one-fourth of the time, u′ =  u − v  8 about one-eighth of the time, and so on. We have   32   ⌊lg u′⌋ = m − k − r,   33  where k is the number of places that u − v is shifted right, and where r is ⌊lg u⌋ − ⌊lg u − v ⌋, the number of bits lost at the left during the subtraction of v from u. Notice that r ≤ 1 when m ≥ n + 2, and r ≥ 1 when m = n.  The interaction between k and r is quite messy  see exercise 20 , but Richard Brent discovered a nice way to analyze the approximate behavior by assuming that u and v are large enough that a continuous distribution describes the ratio v u, while k varies discretely. [See Algorithms and Complexity, edited by J. F. Traub  New York: Academic Press, 1976 , 321–355.] Let us assume that u and v are large integers that are essentially random, except that they are odd and their ratio has a certain probability distribution. Then the least significant bits of the quantity t = u − v in step B6 will be essentially random, except that t will be even. Hence t will be an odd multiple of 2k with probability 2−k; this is the approximate probability that k right shifts will be needed in the subtract- and-shift cycle. In other words, we obtain a reasonable approximation to the behavior of Algorithm B if we assume that step B4 always branches to B3 with probability 1 2. Let Gn x  be the probability that min u, v  max u, v  is ≥ x after n subtract- If u ≥ v and if and-shift cycles have been performed under this assumption. exactly k right shifts are performed, the ratio X = v u is changed to X′ =    34    35    36    37    38    39   4.5.2  min 2kv  u − v ,  u − v  2kv  = min2kX  1 − X ,  1 − X  2kX. Thus we will  have X′ ≥ x if and only if 2kX  1 − X  ≥ x and  1 − X  2kX ≥ x; and this is the same as  THE GREATEST COMMON DIVISOR  349  1 + 2kx Therefore Gn x  satisfies the interesting recurrence  1 + 2k x  1  2−k  ≤ X ≤    1  Gn  1 + 2k x  1  .   − Gn      ,  1  1 + 2kx  Gn+1 x  =  k≥1  where G0 x  = 1 − x for 0 ≤ x ≤ 1. Computational experiments indicate that Gn x  converges rapidly to a limiting distribution G∞ x  = G x , although a formal proof of convergence seems to be difficult. We shall assume that G x  exists; hence it satisfies  1  1 + 2k x  1  1 + 2kx  for 0 < x ≤ 1;  G x  =  k≥1  G   2−k  1  G 0  = 1;    1 + 2x 2−kG    1  1 + 2kx   − G   1   ;  S x  = 1 2 G  =  k≥1  Let  ,    1  G 1  = 0.      + 1 4 G  1 + 4x  + 1 8 G  1 + 8x  + ···  then we have  G x  = S 1 x  − S x .  It is convenient to define  G 1 x  = −G x ,   40  so that  39  holds for all x > 0. As x runs from 0 to ∞, S x  increases from 0 to 1, hence G x  decreases from +1 to −1. Of course G x  is no longer a probability when x > 1; but it is meaningful nevertheless  see exercise 23 .  We will assume that there are power series α x , β x , γm x , δm x , λ x ,  µ x , σm x , τm x , and ρ x  such that  ∞ ∞  m=1  γm x  cos 2πm lg x + δm x  sin 2πm lg x,  41  σm x  cos 2πm lg x + τm x  sin 2πm lg x,  42   S x  = λ x  lg x + µ x  + ρ x  = G 1 + x  = ρ1x + ρ2x2 + ρ3x3 + ρ4x4 + ρ5x5 + ρ6x6 + ··· ,   43  because it can be shown that the solutions Gn x  to  35  have this property for n ≥ 1.  See, for example, exercise 30.  The power series converge for x < 1.  m=1  G x  = α x  lg x + β x  +   350  ARITHMETIC  4.5.2  Fig. 10. The limiting distribution of ratios in the binary gcd algorithm.  What can we deduce about α x , . . . , ρ x  from equations  36 – 43 ? In the  first place we have  2S x  = G1  1 + 2x  + S 2x  = S 2x  − ρ 2x    44   from  38 ,  40 , and  43 . Consequently Eq.  42  holds if and only if  2λ x  = λ 2x ; 2µ x  = µ 2x  + λ 2x  − ρ 2x ; 2σm x  = σm 2x ,   45   46   47  Relation  45  tells us that λ x  is simply a constant multiple of x; we will write  48   2τm x  = τm 2x ,  λ x  = −λx  for m ≥ 1.  because the constant is negative.  The relevant coefficient turns out to be  λ = 0.39792 26811 88316 64407 67071 61142 65498 23098+,   49  but no easy way to compute it is known.  Relation  46  tells us that ρ1 = −λ, and that 2µk = 2kµk − 2kρk when k > 1; in other words, for k ≥ 2.  µk = ρk  1 − 21−k ,   50   We also know from  47  that the two families of power series  are simply linear functions. This is not true for γm x  and δm x .  σm x  = σm x,  τm x  = τm x  Replacing x by 1 2x in  44  yields   51   2S 1 2x  = S 1 x  + G x  1 + x  ,   52  and  39  converts this equation to a relation between G and S when x is near 0:  53   2G 2x  + 2S 2x  = G x  + S x  + G x  1 + x  .  G x   1.0 0.8 0.6 0.4 0.2 0.0 −0.2 −0.4 −0.6 −0.8 −1.0  0.0  1.0  2.0  3.0  4.0  x   4.5.2  THE GREATEST COMMON DIVISOR  351  The coefficients of lg x must agree when both sides of this equation are expanded in power series, hence  2α 2x  − 4λx = α x  − λx + α x  1 + x  .   54   Equation  54  is a recurrence that defines α x . In fact, let us consider the  function ψ z  that satisfies     z    2   z  2 + z    ,  z + ψ  + ψ  ψ z  = 1 2  Then  54  says that  α x  = 3 2 λψ x . 1  4 + 1 4 + z  + 1 4  Moreover, iteration of  55  yields  1 2 + 1  2 + z 1  1 1 + 1  2 1 2k  k≥0  2k + jz  .  0≤j<2k  ψ z  = z 2 = z 2  It follows that the power series expansion of ψ z  is  ψ 0  = 0, ψ′ 0  = 1.   55   + 1  4 + 2z  + 1  4 + 3z   56     + ···   57   ψ z  =  n≥1   −1 n−1ψnzn,  ψn = 1 2n  Bk  2k+1 − 1  + δn1 2 ;   58   n−1  k=0   n    k  see exercise 27. This formula for ψn is surprisingly similar to an expression that arises in connection with digital search tree algorithms, Eq. 6.3– 18 . Exercise 28 proves that ψn = Θ n−2 . We now know α x , except for the constant λ = −ρ1, and  50  relates µ x  to ρ x  except for the coefficient µ1. The answer to exercise 25 shows that the coefficients of ρ x  can all be expressed in terms of ρ1, ρ3, ρ5, . . . ; moreover, the constants σm and τm can be computed by the method used to solve exercise 29, and complicated relations also hold between the coefficients of the functions γm x  and δm x . However, there seems to be no way to compute all the coefficients of the various functions that enter into G x  except to iterate the recurrence  36  by elaborate numerical methods. Once we have computed a good approximation to G x , we can estimate the asymptotic average running time of Algorithm B as follows: If u ≥ v and if k right shifts are performed, the quantity Y = uv is changed to Y ′ =  u − v v 2k; hence the ratio Y  Y ′ is 2k  1−X , where X = v u is ≥ x with probability G x . Therefore the number of bits in uv decreases on the average by the constant  b = E lg Y  Y ′  =  2−k  k≥1   1  0  fk 0  +  G x f′  k x  dx  ,     352  ARITHMETIC  where fk x  = lg2k  1 − x ; we have  b =  k≥1  2−k  k +   1  0     1  0  G x  dx  1 − x  ln 2  = 2 +  G x  dx  1 − x  ln 2 .  4.5.2   59   When eventually u = v, the expected value of lg uv will be approximately 0.9779  see exercise 14 ; therefore the total number of subtract-and-shift cycles of Algo- rithm B will be approximately 1 b times the initial value of lg uv. By symmetry, this is about 2 b times the initial value of lg u. Numerical computations carried out by Richard Brent in 1997 give the value  2 b = 0.70597 12461 01916 39152 93141 35852 88176 66677+   60   for this fundamental constant.  A deeper study of these functions by Brigitte Vallée led her to suspect that  the constants λ and b might be related by the remarkable formula   61  Sure enough, the values computed by Brent agree perfectly with this tantalizing conjecture. Vallée has successfully analyzed Algorithm B using rigorous “dy- namical” methods of great interest [see Algorithmica 22  1998 , 660–685].  .  λ b  = 2 ln 2 π2  Let us return to our assumption in  32  that u and v are odd and in the ranges 2m ≤ u < 2m+1 and 2n ≤ v < 2n+1. Empirical tests of Algorithm B with several million random inputs and with various values of m and n in the range 29 ≤ m, n ≤ 37 indicate that the actual average behavior of the algorithm is given by  C ≈ 1 2 m + 0.203n + 1.9 − 0.4 0.6 m−n, D ≈ m + 0.41n − 0.5 − 0.7 0.6 m−n,  m ≥ n,   62   with a rather small standard deviation from these observed average values. The coefficients 1  2 and 1 of m in  62  can be verified rigorously  see exercise 21 .  If we assume instead that u and v are to be any integers, independently and  uniformly distributed over the ranges 1 ≤ u < 2N ,  1 ≤ v < 2N ,   63  then we can calculate the average values of C and D from the data already given:  64   See exercise 22.  This agrees perfectly with the results of further empirical tests, made on several million random inputs for N ≤ 30; the latter tests show that we may take  D ≈ 1.41N + O 1 .  C ≈ 0.70N + O 1 ,   65  as decent estimates of the values, given this distribution of the inputs u and v. The theoretical analysis in Brent’s continuous model of Algorithm B predicts that C and D will be asymptotically equal to 2N b and 4N b under assump- tion  63 , where 2 b ≈ 0.70597 is the constant in  60 . The agreement with  C = 0.70N − 0.5,  D = 1.41N − 2.7   4.5.2  THE GREATEST COMMON DIVISOR  353  experiment is so good that Brent’s constant 2 b must be the true value of the number “0.70” in  65 , and we should replace 0.203 by 0.206 in  62 .  This completes our study of the average values of C and D. The other three quantities that appear in the running time of Algorithm B are quite easy to analyze; see exercises 6, 7, and 8.  Now that we know approximately how Algorithm B behaves on the average, let’s consider a “worst case” scenario: What values of u and v are in some sense the hardest to handle? If we assume as before that  ⌊lg u⌋ = m  and  ⌊lg v⌋ = n,  we want to find u and v that make the algorithm run most slowly. The subtrac- tions take somewhat longer than the shifts, when the auxiliary bookkeeping is considered, so this question may be rephrased by asking for the inputs u and v that require the most subtractions. The answer is somewhat surprising; the maximum value of C is exactly  max m, n  + 1,   66  although a naïve analysis would predict that substantially higher values of C are possible  see exercise 35 . The derivation of the worst case  66  is quite interesting, so it has been left as an amusing problem for readers to work out for themselves  see exercises 36 and 37 .  EXERCISES 1. [M21] How can  8 ,  9 ,  10 ,  11 , and  12  be derived easily from  6  and  7 ? 2. [M22] Given that u divides v1v2 . . . vn, prove that u divides  gcd u, v1  gcd u, v2  . . . gcd u, vn .  3. [M23] Show that the number of ordered pairs of positive integers  u, v  such that lcm u, v  = n is the number of divisors of n2. 4. [M21] Given positive integers u and v, show that there are divisors u′ of u and v′ of v such that u′ ⊥ v′ and u′v′ = lcm u, v .   cid:120  5. [M26] Invent an algorithm  analogous to Algorithm B  for calculating the greatest  common divisor of two integers based on their balanced ternary representation. Dem- onstrate your algorithm by applying it to the calculation of gcd 40902, 24140 . 6. [M22] Given that u and v are random positive integers, find the mean and the standard deviation of the quantity A that enters into the timing of Program B.  This is the number of right shifts applied to both u and v during the preparatory phase.  7. [M20] Analyze the quantity B that enters into the timing of Program B.   cid:120  8. [M25] Show that in Program B, the average value of E is approximately equal to  1 2 Cave, where Cave is the average value of C. 9. [18] Using Algorithm B and hand calculation, find gcd 31408, 2718 . Also find integers m and n such that 31408m + 2718n = gcd 31408, 2718 , using Algorithm X.   354  ARITHMETIC   cid:120  10. [HM24] Let qn be the number of ordered pairs of integers  u, v  lying in the range  1 ≤ u, v ≤ n such that u ⊥ v. The object of this exercise is to prove that we have limn→∞ qn n2 = 6 π2, thereby establishing Theorem D. a  Use the principle of inclusion and exclusion  Section 1.3.3  to show that  4.5.2  2 −  ⌊n p1⌋2 +   p1  p1<p2  qn = n  ⌊n p1p2⌋2 − ··· ,  b  The Möbius function µ n  is defined by the rules µ 1  = 1, µ p1p2 . . . pr  =  −1 r if p1, p2, . . . , pr are distinct primes, and µ n  = 0 if n is divisible by the square of  where the sums are taken over all prime numbers pi.  a prime. Show that qn = c  As a consequence of  b , prove that limn→∞ qn n2 = k≥1 µ k ⌊n k⌋2. k≥1 µ k  k2   d  Prove that       =  lutely convergent we have  bm mz  ak kz  k≥1  m≥1  n≥1  d\n  k≥1 µ k  k2.   adbn d  nz.  m≥1 1 m2  = 1. Hint: When the series are abso-  11. [M22] What is the probability that gcd u, v  ≤ 3?  See Theorem D.  What is the average value of gcd u, v ? 12. [M24]  E. Cesàro.  If u and v are random positive integers, what is the aver- age number of  positive  divisors they have in common? [Hint: See the identity in exercise 10 d , with ak = bm = 1.] 13. [HM23] Given that u and v are random odd positive integers, show that they are relatively prime with probability 8 π2.  positive integers?  b  random positive odd integers? 15. [M21] What are the values of v1 and v2 when Algorithm X terminates?   cid:120  14. [HM25] What is the expected value of ln gcd u, v  when u and v are  a  random  cid:120  16. [M22] Design an algorithm to divide u by v modulo m, given positive integers u,  cid:120  17. [M20] Given two integers u and v such that uv ≡ 1  modulo 2e , explain how to  cid:120  18. [M24] Show how Algorithm L can be extended  as Algorithm A was extended to  compute an integer u′ such that u′v ≡ 1  modulo 22e . [This leads to a fast algorithm for computing the reciprocal of an odd number modulo a power of 2, since we can start with a table of all such reciprocals for e = 8 or e = 16.]  v, and m, with v relatively prime to m. In other words, your algorithm should find w, in the range 0 ≤ w < m, such that u ≡ vw  modulo m .  Algorithm X  to obtain solutions of  15  when u and v are large. 19. [21] Use the text’s method to find a general solution in integers to the following sets of equations:  a   3x + 7y + 11z = 1 5x + 7y − 5z = 3  b   3x + 7y + 11z = 1 5x + 7y − 5z = −3  20. [M37] Let u and v be odd integers, independently and uniformly distributed in the ranges 2m ≤ u < 2m+1, 2n ≤ v < 2n+1. What is the exact probability that a single subtract-and-shift cycle in Algorithm B reduces u and v to the ranges 2m′ ≤ u < 2m′+1, 2n′ ≤ v < 2n′+1, as a function of m, n, m′, and n′?   4.5.2  THE GREATEST COMMON DIVISOR  355  21. [HM26] Let Cmn and Dmn be the average number of subtraction steps and shift steps, respectively, in Algorithm B, when u and v are odd, ⌊lg u⌋ = m, ⌊lg v⌋ = n. Show that for fixed n, Cmn = 1 22. [M28] Continuing the previous exercise, show that if Cmn = αm + βn + γ for some constants α, β, and γ, then  2 m + O 1  and Dmn = m + O 1  as m → ∞.   N − m  N − n 2m+n−2  Cmn = 22N  11  27 α + β N + O 1  ,    1≤n<m≤N    1≤n≤N   N − n 222n−2  Cnn = 22N  5  27 α + β N + O 1  .   cid:120  23. [M20] What is the probability that v u ≤ x after n subtract-and-shift cycles of  Algorithm B, when the algorithm begins with large random integers?  Here x is any real number ≥ 0; we do not assume that u ≥ v.  24. [M20] Suppose u > v in step B6, and assume that the ratio v u has Brent’s limiting distribution G. What is the probability that u < v the next time step B6 is encountered? 25. [M21] Equation  46  implies that ρ1 = −λ; prove that ρ2 = λ 2. 26. [M22] Prove that when G x  satisfies  36 – 40  we have 2G x  − 5G 2x  + 2G 4x  = G 1 + 2x  − 2G 1 + 4x  + 2G 1 + 1 x  − G 1 + 1 2x . 27. [M22] Prove  58 , which expresses ψn in terms of Bernoulli numbers. 28. [HM36] Study the asymptotic behavior of ψn. Hint: See exercise 6.3–34.   cid:120  29. [HM26]  R. P. Brent.  Find G1 x , the distribution of min u, v  max u, v  after ∞k=1 2−kGn 1  1+2kx  , and use the method of Mellin transforms for harmonic sums  the first subtract-and-shift cycle of Algorithm B as defined in  35 . Hint: Let Sn+1 x  =  [see P. Flajolet, X. Gourdon, and P. Dumas, Theor. Comp. Sci. 144  1995 , 3–58]. 30. [HM39] Continuing the previous exercise, determine G2 x . 31. [HM46] Prove or disprove Vallée’s conjecture  61 . 32. [HM42] Is there a unique continuous function G x  that satisfies  36  and  37 ? 33. [M46] Analyze Harris’s “binary Euclidean algorithm,” stated after Program B. 34. [HM49] Find a rigorous proof that Brent’s model describes the asymptotic be- havior of Algorithm B. 35. [M23] Consider a directed graph with vertices  m, n  for all nonnegative integers m, n ≥ 0, having arcs from  m, n  to  m′, n′  whenever it is possible for a subtract-and- shift cycle of Algorithm B to transform integers u and v with ⌊lg u⌋ = m and ⌊lg v⌋ = n into integers u′ and v′ with ⌊lg u′⌋ = m′ and ⌊lg v′⌋ = n′; there also is a special “Stop” vertex, with arcs from  n, n  to Stop for all n ≥ 0. What is the length of the longest path from  m, n  to Stop?  This gives an upper bound on the maximum running time of Algorithm B.    cid:120  36. [M28] Given m ≥ n ≥ 1, find values of u and v with ⌊lg u⌋ = m and ⌊lg v⌋ = n  cid:120  38. [M32]  R. W. Gosper.  Demonstrate how to modify Algorithm B for large num-  such that Algorithm B requires m + 1 subtraction steps. 37. [M32] Prove that the subtraction step B6 of Algorithm B is never executed more than 1 + ⌊lg max u, v ⌋ times.  bers, using ideas analogous to those in Algorithm L.   356  ARITHMETIC   cid:120  39. [M28]  V. R. Pratt.  Extend Algorithm B to an Algorithm Y that is analogous  cid:120  40. [M25]  R. P. Brent and H. T. Kung.  The following variant of the binary gcd  to Algorithm X.  algorithm is better than Algorithm B from the standpoint of hardware implementation, because it does not require testing the sign of u − v. Assume that u is odd; u and v can be either positive or negative.  4.5.2  K1. [Initialize.] Set c ← 0.  This counter estimates the difference between lg u  and lg v.   K2. [Done?] If v = 0, terminate with u as the answer. K3. [Make v odd.] Set v ← v 2 and c ← c + 1 zero or more times, until v is odd. K4. [Make c ≤ 0.] If c > 0, interchange u ↔ v and set c ← −c. K5. [Reduce.] Set w ←  u+v  2. If w is even, set v ← w; otherwise set v ← w−v.  Return to step K2.  Prove that step K2 is performed at most 2 + 2 lg max u,v  times. 41. [M22] Use Euclid’s algorithm to find a simple formula for gcd 10m − 1, 10n − 1  when m and n are nonnegative integers. 42. [M30] Evaluate the determinant    gcd 1, 1  gcd 2, 1   gcd 1, 2  gcd 2, 2   . . . . . .  gcd 1, n  gcd 2, n   ...  ...  ...  gcd n, 1   gcd n, 2   . . . gcd n, n    .  *4.5.3. Analysis of Euclid’s Algorithm The execution time of Euclid’s algorithm depends on T, the number of times the division step A2 is performed.  See Algorithm 4.5.2A and Program 4.5.2A.  The quantity T is also an important factor in the running time of other algo- rithms, such as the evaluation of functions satisfying a reciprocity formula  see Section 3.3.3 . We shall see in this section that the mathematical analysis of this quantity T is interesting and instructive. Relation to continued fractions. Euclid’s algorithm is intimately connected with continued fractions, which are expressions of the form  = b1 a1+b2  a2+b3  ···   an−1+bn an  . . .   .   1   b1 a1 + b2  a2 + b3 ···  an−1+ bn an  Continued fractions have a beautiful theory that is the subject of several classic books, such as O. Perron, Die Lehre von den Kettenbrüchen, 3rd edition  Stutt- gart: Teubner, 1954 , 2 volumes; A. Khinchin, Continued Fractions, translated by Peter Wynn  Groningen: P. Noordhoff, 1963 ; and H. S. Wall, Analytic Theory   4.5.3  ANALYSIS OF EUCLID’S ALGORITHM  357  of Continued Fractions  New York: Van Nostrand, 1948 . See also Claude Brezinski, History of Continued Fractions and Padé Approximants  Springer, 1991 , for the early history of the subject. It is necessary to limit ourselves to a comparatively brief treatment of the theory here, studying only those aspects that give us more insight into the behavior of Euclid’s algorithm.  The continued fractions of primary interest to us are those in which all of  the b’s in  1  are equal to unity. For convenience in notation, let us define    x1, x2, . . . , xn   = 1 x1 + 1  x2 + 1  ···   xn−1 + 1 xn  . . .   .   2   Thus, for example,   1,    x1   = 1  ,  x1    x1, x2   =  1  x1 + 1 x2  =  x2  x1x2 + 1 .   3   If n = 0, the symbol   x1, . . . , xn   is taken to mean 0. Let us also define the so-called continuant polynomials Kn x1, x2, . . . , xn  of n variables, for n ≥ 0, by the rule  if n = 0; if n = 1; if n > 1.   4   Kn x1, x2, . . . , xn  =  x1, x1Kn−1 x2, . . . , xn + Kn−2 x3, . . . , xn ,  Thus K2 x1, x2  = x1x2 + 1, K3 x1, x2, x3  = x1x2x3 + x1 + x3, etc. In general, as noted by L. Euler in the eighteenth century, Kn x1, x2, . . . , xn  is the sum of all terms obtainable by starting with x1x2 . . . xn and deleting zero or more nonoverlapping pairs of consecutive variables xjxj+1; there are Fn+1 such terms.  The basic property of continuants is the explicit formula   x1, x2, . . . , xn   = Kn−1 x2, . . . , xn  Kn x1, x2, . . . , xn ,  n ≥ 1.   5   This can be proved by induction, since it implies that  x0 +   x1, . . . , xn   = Kn+1 x0, x1, . . . , xn  Kn x1, . . . , xn ;  hence   x0, x1, . . . , xn   is the reciprocal of the latter quantity.  The K-polynomials are symmetrical in the sense that  Kn x1, x2, . . . , xn  = Kn xn, . . . , x2, x1 .  This follows from Euler’s observation above, and as a consequence we have  Kn x1, . . . , xn  = xnKn−1 x1, . . . , xn−1  + Kn−2 x1, . . . , xn−2   for n > 1. The K-polynomials also satisfy the important identity  Kn x1, . . . , xn Kn x2, . . . , xn+1  − Kn+1 x1, . . . , xn+1 Kn−1 x2, . . . , xn  n ≥ 1.  =  −1 n,   See exercise 4.  The latter equation in connection with  5  implies that   x1, . . . , xn   = 1  − ··· +  −1 n−1 qn−1qn where qk = Kk x1, . . . , xk .  + 1 q2q3  − 1 q1q2  q0q1  ,   6    7    8    9    358  ARITHMETIC  4.5.3  Thus the K-polynomials are intimately related to continued fractions. defined as follows: Let X0 = X, and for all n ≥ 0 such that Xn ̸= 0 let  Every real number X in the range 0 ≤ X < 1 has a regular continued fraction  An+1 = ⌊1 Xn⌋,   10  If Xn = 0, the quantities An+1 and Xn+1 are not defined, and the regular continued fraction for X is   A1, . . . , An  . If Xn ̸= 0, this definition guarantees that 0 ≤ Xn+1 < 1, so each of the A’s is a positive integer. Definition  10  also implies that  Xn+1 = 1 Xn − An+1.  X = X0 =  1  A1 + X1  1  =  A1 + 1  A2 + X2  = ··· ;  hence  X =   A1, . . . , An−1, An + Xn     11  for all n ≥ 1, whenever Xn is defined. In particular, we have X =   A1, . . . , An   If Xn ̸= 0, the number X always lies between   A1, . . . , An   when Xn = 0. and   A1, . . . , An + 1  , since by  7  the quantity qn = Kn A1, . . . , An + Xn  increases monotonically from Kn A1, . . . , An  up to Kn A1, . . . , An + 1  as Xn increases from 0 to 1, and by  9  the continued fraction increases or decreases when qn increases, according as n is even or odd. In fact,  X −   A1, . . . , An   =   A1, . . . , An + Xn   −   A1, . . . , An   =   A1, . . . , An, 1 Xn   −   A1, . . . , An   =   Kn A2, . . . , An, 1 Xn  Kn+1 A1, . . . , An, 1 Xn  − Kn−1 A2, . . . , An  = 1 Kn A1, . . . , An Kn+1 A1, . . . , An, 1 Xn  Kn A1, . . . , An  ≤ 1 Kn A1, . . . , An Kn+1 A1, . . . , An, An+1    12  by  5 ,  7 ,  8 , and  10 . Therefore   A1, . . . , An   is an extremely close approx- imation to X, unless n is small. If Xn is nonzero for all n, we obtain an infinite continued fraction   A1, A2, A3, . . .   , whose value is defined to be    n→∞   A1, A2, . . . , An  ; lim  from inequality  12  it is clear that this limit equals X.  The regular continued fraction expansion of real numbers has several prop- erties analogous to the representation of numbers in the decimal system. If we use the formulas above to compute the regular continued fraction expansions of some familiar real numbers, we find, for example, that  8 29 =   3, 1, 1, 1, 2  ; 29 =   1, 1, 9, 2, 2, 3, 2, 2, 9, 1, 2, 1, 9, 2, 2, 3, 2, 2, 9, 1, 2, 1, 9, 2, 2, 3, 2, 2, 9, 1, . . .   ; 3√ 2 = 1 +   3, 1, 5, 1, 1, 4, 1, 1, 8, 1, 14, 1, 10, 2, 1, 4, 12, 2, 3, 2, 1, 3, 4, 1, 1, 2, 14, 3, . . .   ; π = 3 +   7, 15, 1, 292, 1, 1, 1, 2, 1, 3, 1, 14, 2, 1, 1, 2, 2, 2, 2, 1, 84, 2, 1, 1, 15, 3, 13, . . .   ;   8   4.5.3  ANALYSIS OF EUCLID’S ALGORITHM  359  e = 2 +   1, 2, 1, 1, 4, 1, 1, 6, 1, 1, 8, 1, 1, 10, 1, 1, 12, 1, 1, 14, 1, 1, 16, 1, 1, 18, 1, . . .   ; γ =   1, 1, 2, 1, 2, 1, 4, 3, 13, 5, 1, 1, 8, 1, 2, 4, 1, 1, 40, 1, 11, 3, 7, 1, 7, 1, 1, 5, 1, 49, . . .   ; ϕ = 1 +   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, . . .   .  13  pattern that appears in the partial quotients for8 29, ϕ, and e; the reasons for The numbers A1, A2, . . . are called the partial quotients of X. Notice the regular this behavior are discussed in exercises 12 and 16. There is no apparent pattern in the partial quotients for 3√ It is interesting to note that the ancient Greeks’ first definition of real numbers, once they had discovered the existence of irrationals, was essentially stated in terms of infinite continued fractions.  Later they adopted the suggestion of Eudoxus that x = y should be defined instead as “x < r if and only if y < r, for all rational r.”  See O. Becker, Quellen und Studien zur Geschichte Math., Astron., Physik B2  1933 , 311–333.  2, π, or γ.  When X is a rational number, the regular continued fraction corresponds in a natural way to Euclid’s algorithm. Let us assume that X = v u, where u > v ≥ 0. The regular continued fraction process starts with X0 = X; let us define U0 = u, V0 = v. Assuming that Xn = Vn Un ̸= 0,  10  becomes Xn+1 = Un Vn − An+1 =  Un mod Vn  Vn.   14   An+1 = ⌊Un Vn⌋, Therefore, if we define  Un+1 = Vn,  Vn+1 = Un mod Vn,   15  the condition Xn = Vn Un holds throughout the process. Furthermore,  15  is precisely the transformation made on the variables u and v in Euclid’s algorithm  see Algorithm 4.5.2A, step A2 . For example, since 8 29 =   3, 1, 1, 1, 2  , we know that Euclid’s algorithm applied to u = 29 and v = 8 will require exactly five division steps, and the quotients ⌊u v⌋ in step A2 will be successively 3, 1, 1, 1, and 2. The last partial quotient An must always be 2 or more when Xn = 0 and n ≥ 1, since Xn−1 is less than unity. From this correspondence with Euclid’s algorithm we can see that the regular continued fraction for X terminates at some step with Xn = 0 if and only if X is rational; for it is obvious that Xn cannot be zero if X is irrational, and, conversely, we know that Euclid’s algorithm always terminates. If the partial quotients obtained during Euclid’s algorithm are A1, A2, . . . , An, then we have, by  5 ,  = Kn−1 A2, . . . , An  Kn A1, A2, . . . , An  .   16  This formula holds also if Euclid’s algorithm is applied for u < v, when A1 = 0. Furthermore, because of relation  8 , the continuants Kn−1 A2, . . . , An  and Kn A1, A2, . . . , An  are relatively prime, and the fraction on the right-hand side of  16  is in lowest terms; therefore  v u  u = Kn A1, A2, . . . , An d,  v = Kn−1 A2, . . . , An d,   17   where d = gcd u, v .   360  ARITHMETIC  4.5.3  The worst case. We can now apply these observations to determine the behavior of Euclid’s algorithm in the worst case, or in other words to give an upper bound on the number of division steps. The worst case occurs when the inputs are consecutive Fibonacci numbers: Theorem F. For n ≥ 1, let u and v be integers with u > v > 0 such that Euclid’s algorithm applied to u and v requires exactly n division steps, and such that u is as small as possible satisfying these conditions. Then u = Fn+2 and v = Fn+1. Proof. By  17 , we must have u = Kn A1, A2, . . . , An d, where A1, A2, . . . , An, and d are positive integers and An ≥ 2. Since Kn is a polynomial with nonnegative coefficients, involving all of the variables, the minimum value is achieved only when A1 = 1, . . . , An−1 = 1, An = 2, d = 1. Putting these values in  17  yields the desired result.  This theorem has the historical claim of being the first practical application of the Fibonacci sequence; since then many other applications of Fibonacci numbers to algorithms and to the study of algorithms have been discovered. The result is essentially due to T. F. de Lagny [Mém. Acad. Sci. 11  Paris, 1733 , 363– 364], who tabulated the first several continuants and observed that Fibonacci numbers give the smallest numerator and denominator for continued fractions of a given length. He did not explicitly mention gcd calculation, however; the connection between Fibonacci numbers and Euclid’s algorithm was first pointed out by É. Léger [Correspondance Math. et Physique 9  1837 , 483–485.]  If 0 ≤ v < N, the number of division steps required when  Shortly afterwards, P. J. É. Finck [Traité Élémentaire d’Arithmétique  Stras- bourg: 1841 , 44] proved by another method that gcd u, v  takes at most 2 lg v+1 steps, when u > v > 0; and G. Lamé [Comptes Rendus Acad. Sci. 19  Paris, 1844 , 867–870] improved this to 5⌈log10 v + 1 ⌉. Full details about these pioneering studies in the analysis of algorithms appear in an interesting review by J. O. Shallit, Historia Mathematica 21  1994 , 401–419. A more precise estimate of the worst case is, however, a direct consequence of Theorem F: Corollary L.  Algorithm 4.5.2A is applied to u and v is at mostlogϕ  3 − ϕ N. 5 < N see Eq. 1.2.8– 15 ; thus ϕn <    Proof. After step A1 we have v > u mod v. Therefore by Theorem F, the maximum number of steps, n, occurs when v = Fn+1 and u mod v = Fn. Since 5 ϕ N = Fn+1 < N, we have ϕn+1   3 − ϕ N. The quantity logϕ  3 − ϕ N is approximately equal to 2.078 ln N + .6723 ≈ 4.785 log10 N + .6723. See exercises 31, 36, and 38 for extensions of Theorem F. An approximate model. Now that we know the maximum number of division steps that can occur, let us attempt to find the average number. Let T m, n  be the number of division steps that occur when u = m and v = n are input to Euclid’s algorithm. Thus  √  √  T m, 0  = 0;  T m, n  = 1 + T n, m mod n   if n ≥ 1.   18    4.5.3  ANALYSIS OF EUCLID’S ALGORITHM  361  Let Tn be the average number of division steps when v = n and when u is chosen at random; since only the value of u mod v affects the algorithm after the first division step, we have    Tn = 1  n  0≤k<n  T k, n .   19   For example, T 0, 5  = 1, T 1, 5  = 2, T 2, 5  = 3, T 3, 5  = 4, T 4, 5  = 3, so  T5 = 1  5 1 + 2 + 3 + 4 + 3  = 2 3 5 .  Our goal is to estimate Tn for large n. One idea is to try an approximation suggested by R. W. Floyd: We might assume that, for 0 ≤ k < n, the value of n is essentially “random” modulo k, so that we can set  Tn ≈ 1 + 1  n   T0 + T1 + ··· + Tn−1 .  Then Tn ≈ Sn, where the sequence ⟨Sn⟩ is the solution to the recurrence relation  20    S0 + S1 + ··· + Sn−1 ,  Sn = 1 + 1  S0 = 0,  n ≥ 1.  This recurrence is easy to solve by noting that  n  n Sn − 1  + Sn  n + 1  S0 + S1 + ··· + Sn−1 + Sn   = Sn + 1  Sn+1 = 1 + 1 = 1 + 1 n + 1 2 + ··· + 1 hence Sn is 1 + 1 n = Hn, a harmonic number. The approximation Tn ≈ Sn now suggests that we might have Tn ≈ ln n + O 1 . Comparison of this approximation with tables of the true value of Tn show, however, that ln n is too large; Tn does not grow this fast. Our tentative assumption that n is random modulo k must therefore be too pessimistic. And indeed, a closer look shows that the average value of n mod k is less than the average value of 1  2 k, in the range 1 ≤ k ≤ n:  n + 1;    1≤k≤n  1 n   n mod k  = 1  n  1≤k,q≤n     n − qk ⌊n  q + 1 ⌋ < k ≤ ⌊n q⌋  −⌊n  q + 1 ⌋ + 1     ⌊n q⌋ + 1  ⌊n q⌋ + 1  2  2  2  q  1≤q≤n  n  1≤q≤n  = n − 1  = n − 1   n    1 − π2 12  =  see exercise 4.5.2–10 c . This is only about .1775n, not .25n; so the value of  n + O log n   n mod k tends to be smaller than Floyd’s model predicts, and Euclid’s algorithm works faster than we might expect.   21    362  ARITHMETIC  4.5.3  A continuous model. The behavior of Euclid’s algorithm with v = N is essentially determined by the behavior of the regular continued fraction process when X = 0 N, 1 N, . . . ,  N −1  N. When N is very large, we therefore want to study the behavior of regular continued fractions when X is essentially a random real number, uniformly distributed in [0 . . 1 . Consider the distribution function  22  given a uniform distribution of X = X0. By the definition of regular continued fractions, we have F0 x  = x, and  Fn x  = Pr Xn ≤ x ,  for 0 ≤ x ≤ 1,  k≥1  Fn+1 x  = = = F x  =  k≥1  k≥1  k≥1  Pr k ≤ 1 Xn ≤ k + x   Pr1  k + x  ≤ Xn ≤ 1 k 1  k + x . Fn 1 k  − Fn F 1 k  − F1  k + x .  If the distributions F0 x , F1 x , limiting distribution F∞ x  = F x , we will have  . . . defined by these formulas approach a   An analogous relation, 4.5.2– 36 , arose in our study of the binary gcd algo- rithm.  One function that satisfies  24  is F x  = logb 1+ x , for any base b > 1; see exercise 19. The further condition F 1  = 1 implies that we should take b = 2. Thus it is reasonable to make a guess that F x  = lg 1 + x , and that Fn x  approaches this behavior. 2  ≈ 0.58496; let us see 2  = 0.50000, and  2  comes to this value for small n. We have F0  1  We might conjecture, for example, that F  1  how close Fn  1  2  = lg  3  F1 x  =  k≥1   1  k    − 1 k + x  = Hx;  F1  1 F2  1  2  = H1 2 = 2 − 2 ln 2 ≈ 0.61371; 2  = H2 2 − H2 3 + H2 4 − H2 5 + H2 6 − H2 7 + ··· .   See Table 3 of Appendix A.  The power series expansion  Hx = ζ 2 x − ζ 3 x2 + ζ 4 x3 − ζ 5 x4 + ···   25   makes it feasible to compute the numerical value  F2  1  2  = 0.57655 93276 99914 08418 82618 72122 27055 92452 − .   26  We’re getting closer to 0.58496; but it is not immediately clear how to get a good estimate of Fn  1  2  for n = 3, much less for really large values of n.   23    24    4.5.3  ANALYSIS OF EUCLID’S ALGORITHM  363  The distributions Fn x  were first studied by C. F. Gauss, who first thought of the problem on the 5th day of February in 1799. His notebook for 1800 lists various recurrence relations and gives a brief table of values, including the 2  ≈ 0.5748. After performing these calculations,  inaccurate  approximation F2  1 Gauss wrote, “Tam complicatæ evadunt, ut nulla spes superesse videatur”; i.e., “They come out so complicated that no hope appears to be left.” Twelve years later, he wrote a letter to Laplace in which he posed the problem as one he could not resolve to his satisfaction. He said, “I found by very simple reasoning that, for n infinite, Fn x  = log 1 + x   log 2. But the efforts that I made since then in my inquiries to assign Fn x  − log 1 + x   log 2 for very large but not infinite values of n were fruitless.” He never published his “very simple reasoning,” and it is not completely clear that he had found a rigorous proof. [See Gauss’s Werke, vol. 101, 552–556.] More than 100 years went by before a proof was finally published, by R. O. Kuz’min [Atti del Congresso Internazionale dei Matematici 6  Bologna, 1928 , 83–89], who showed that  √ Fn x  = lg 1 + x  + O e−A  n    for some positive constant A. The error term was improved to O e−An  by Paul Lévy shortly afterwards [Bull. Soc. Math. de France 57  1929 , 178–194]*; but Gauss’s problem, namely to find the asymptotic behavior of Fn x  − lg 1 + x , was not really resolved until 1974, when Eduard Wirsing published a beautiful analysis of the situation [Acta Arithmetica 24  1974 , 507–528]. We shall study the simplest aspects of Wirsing’s approach here, since his method is an instructive use of linear operators. If G is any function of x defined for 0 ≤ x ≤ 1, let SG be the function  defined by  SG x  =  k≥1     1  k  G   1  − G  k + x    .   27   Thus, S is an operator that changes one function into another. In particular, by  23  we have Fn+1 x  = SFn x , hence   28   In this discussion Fn stands for a distribution function, not for a Fibonacci number.  Notice that S is a “linear operator”; that is, S cG  = c SG  for all constants c, and S G1 + G2  = SG1 + SG2.  Fn = SnF0.  Now if G has a bounded first derivative, we can differentiate  27  term by  term to show that   SG ′ x  =   k + x 2 G′ 1  1    ;  hence SG also has a bounded first derivative. Term-by-term differentiation  of a convergent series is justified when the series of derivatives is uniformly  k + x  k≥1   29   * An exposition of Lévy’s interesting proof appeared in the first edition of this book.   k≥1  k≥1  h x  =  = T g x  =   k≥1    k≥1    = −  k≥1  convergent; see, for example, K. Knopp, Theory and Application of Infinite Let H = SG, and let g x  =  1 + x G′ x , h x  =  1 + x H′ x . It follows  364  ARITHMETIC  that  Series  Glasgow: Blackie, 1951 , §47.  1 + 1 k + x − k − 1 k + x  1 + x  k + x 2  k + 1 + x    k  In other words, h = T g, where T is the linear operator defined by    k + x  g  −1  1   1  1   k + x  g  g  k + x  .    .  4.5.3   30   k  k + 1 + x  − k − 1 k + x  Continuing, we see that if g has a bounded first derivative, we can differen-  tiate term by term to show that T g does also:   T g ′ x  = −  +  g  k  k  1    k + x  k + 1 + x     1  k + 1 + x 2 − k − 1    k + x 2 g′ 1  k + x 2 − k − 1    − g  1 k + x   k + x 3 k + 1 + x  g′ 1  1  k+x   k + 1 + x  k + x  k + x  1 + x  k + x  1  k  g  .  +   k + 1 + x 2  1+ x  φ t  dt +     1   k + x 3 k +1+ x  φ  k + x  There is consequently a third linear operator, U, such that  T g ′ = −U g′ , namely    U φ x  =  k≥1  k   k +1+ x 2  1  k+1+x   Fn x  = lg 1 + x  + Rn fn x  =  1 + x  F ′  What is the relevance of all this to our problem? Well, if we set  lg 1 + x , 1 + R′  lg 1 + x ,  n  n x  = 1 lg 1 + x  ln 2 2 1 + x ; ln 2 lg 1 + x  =  1 + x  ln 2 2 U nf′  we have  f′ n x  = R′′   34  the effect of the lg 1 + x  term disappears, after these transformations. Further- more, since Fn = SnF0, we have fn = T nf0 and f′ 0. Both Fn and fn have bounded derivatives, by induction on n. Thus  34  becomes  n =  −1 nU nf′  n   −1 nR′′  n  0 x .   35     .   31    32   33     36   ANALYSIS OF EUCLID’S ALGORITHM  4.5.3 Now F0 x  = x, f0 x  = 1 + x, and f′ 0 x  is the constant function 1. We are going to show that the operator U n takes the constant function into a function with very small values, hence R′′ n x  must be very small for 0 ≤ x ≤ 1. Finally we can clinch the argument by showing that Rn x  itself is small: Since we have Rn 0  = Rn 1  = 0, it follows from a well-known interpolation formula  see exercise 4.6.4–15 with x0 = 0, x1 = x, x2 = 1  that  365  Rn x  = − x 1 − x   R′′  n  2  ξn x    − lim  1  =      1  1  1 + x    .  for some function ξn x , where 0 ≤ ξn x  ≤ 1 when 0 ≤ x ≤ 1. Thus everything hinges on our being able to prove that U n produces small function values, where U is the linear operator defined in  31 . Notice that U is a positive operator, in the sense that U φ x  ≥ 0 for all x if φ x  ≥ 0 for all x. It follows that U is order-preserving: If φ1 x  ≤ φ2 x  for all x then we have U φ1 x  ≤ U φ2 x  for all x. One way to exploit this property is to find a function φ for which we can calculate U φ exactly, and to use constant multiples of this function to bound the ones that we are really interested in. First let us look for a function g such that T g is easy to compute. If we consider functions defined for all x ≥ 0, instead of only on [0 . . 1], it is easy to remove the summation from  27  by observing that SG x + 1  − SG x  = G   − G 0   37  when G is continuous. Since T 1 + x G′ =  1 + x  SG ′, it follows  see   1   1  k→∞ G  k + x  1 + x  1 + x  = G  exercise 20  that  T g x  1 + x  − T g 1 + x  2 + x  − 1 2 + x  g  1 + x   38   hence  If we set T g x  = 1  1 + x , we find that the corresponding value of g x  is 1 + x−1  1+x . Let φ x  = g′ x  = 1+1  1+x 2, so that U φ x  = − T g ′ x  = 1  1 + x 2; this is the function φ we have been looking for. For this choice of φ we have 2 ≤ φ x  Uφ x  =  1+x 2+1 ≤ 5 for 0 ≤ x ≤ 1,  By the positivity of U and φ we can apply U to this inequality again, obtaining 25 φ ≤ 1 1  5 U φ ≤ U 2φ ≤ 1  2 U φ ≤ 1  1  2 φ.  5 φ ≤ U φ ≤ 1 4 φ; and after n − 1 applications we have 5−nφ ≤ U nφ ≤ 2−nφ   39  0 x  = 1 be the constant function; then for  for this particular φ. Let χ x  = f′ 0 ≤ x ≤ 1 we have 5 85−nχ ≤ 1  4 χ ≤ φ ≤ 2χ, hence 25−nφ ≤ 1  2 U nφ ≤ U nχ ≤ 4  5  5 U nφ ≤ 4  52−nφ ≤ 8  52−nχ.  It follows by  35  that  5  8 ln 2 25−n ≤  −1 nR′′  n x  ≤ 16  5  ln 2 22−n,  for 0 ≤ x ≤ 1;  hence by  32  and  36  we have proved the following result:   ARITHMETIC  366 4.5.3 Theorem W. The distribution Fn x  equals lg 1 + x  + O 2−n  as n → ∞. In fact, Fn x  − lg 1 + x  lies between 5 and 8  5 −1 n+12−nln 1 + x ln 2  1 + x , for 0 ≤ x ≤ 1.  16 −1 n+15−nln 1 + x ln 2  1 + x   With a slightly different choice of φ, we can obtain tighter bounds  see  exercise 21 . In fact, Wirsing went much further in his paper, proving that  Fn x  = lg 1 + x  +  −λ nΨ x  + Ox 1 − x  λ − 0.031 n,   40   where  λ = 0.30366 30028 98732 65859 74481 21901 55623 31109− =   3, 3, 2, 2, 3, 13, 1, 174, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, . . .     is a fundamental constant  apparently unrelated to more familiar constants , and where Ψ is an interesting function that is analytic in the entire complex plane except for the negative real axis from −1 to −∞. Wirsing’s function satisfies Ψ 0  = Ψ 1  = 0, Ψ′ 0  < 0, and SΨ = −λΨ; thus by  37  it satisfies the identity   1  Ψ  1 + z    .  Furthermore, Wirsing demonstrated that  Ψ z  − Ψ z + 1  = 1   λ  − u  v  Ψ  + i N  = cλ−n log N + O 1   as N → ∞,  where c is a constant and n = T u, v  is the number of iterations when Euclid’s algorithm is applied to the integers u > v > 0.  A complete solution to Gauss’s problem was found a few years later by K. I. Babenko [Doklady Akad. Nauk SSSR 238  1978 , 1021–1024], who used powerful techniques of functional analysis to prove that  Fn x  = lg 1 + x  +  j Ψj x  λn  j≥2  for all 0 ≤ x ≤ 1, n ≥ 1. Here λ2 > λ3 ≥ λ4 ≥ ··· , and each Ψj z  is an analytic function in the complex plane except for a cut at [−∞ . . − 1]. The function Ψ2 is Wirsing’s Ψ, and λ2 = −λ, while λ3 ≈ 0.10088, λ4 ≈ −0.03550, λ5 ≈ 0.01284, λ6 ≈ −0.00472, λ7 ≈ 0.00175. Babenko also es- tablished further properties of the eigenvalues λj, proving in particular that they are exponentially small as j → ∞, and that the sum for j ≥ k in  44  is bounded by  π2 6 λkn−1 min x, 1− x . [Further information appears in papers by Babenko and Yuriev, Doklady Akad. Nauk SSSR 240  1978 , 1273–1276; Mayer and Roepstorff, J. Statistical Physics 47  1987 , 149–171; 50  1988 , 331– 344; D. Hensley, J. Number Theory 49  1994 , 142–182; Daudé, Flajolet, and Vallée, Combinatorics, Probability and Computing 6  1997 , 397–433; Flajolet and Vallée, Theoretical Comp. Sci. 194  1998 , 1–34.] The 40-place value of λ in  41  was computed by John Hershberger.   41    42    43    44    4.5.3  ANALYSIS OF EUCLID’S ALGORITHM  367  From continuous to discrete. We have now derived results about the prob- ability distributions for continued fractions when X is a real number uniformly distributed in the interval [0 . . 1 . But a real number is rational with probability zero — almost all numbers are irrational — so these results do not apply directly to Euclid’s algorithm. Before we can apply Theorem W to our problem, some technicalities must be overcome. Consider the following observation based on elementary measure theory: Lemma M. Let I1, I2, . . . , J1, J2, . . . be pairwise disjoint intervals contained in the interval [0 . . 1 , and let  I =   k≥1  Ik,  J =   Jk,  k≥1  K = [0 . . 1] \  I ∪ J  .  Assume that K has measure zero. Let Pn be the set {0 n, 1 n, . . . ,  n − 1  n}. Then  I ∩ Pn  n  lim n→∞  = µ I .  denotes the number of elements in the set I ∩ Pn.  Here µ I  is the Lebesgue measure of I, namely, Proof. Let IN = Ik ∪   1≤k≤N Ik and JN = KN = K ∪   enough so that µ IN  + µ JN  ≥ 1 − ϵ, and let  Jk.  k>N  k>N   45  k≥1 length Ik ; and I ∩ Pn  1≤k≤N Jk. Given ϵ > 0, find N large  If I is an interval, having any of the forms  a . . b  or [a . . b  or  a . . b] or [a . . b], it is clear that µ I  = b − a and  nµ I  − 1 ≤ I ∩ Pn ≤ nµ I  + 1.  Now let rn = IN ∩ Pn, sn = JN ∩ Pn, tn = KN ∩ Pn; we have  rn + sn + tn = n;  nµ IN  − N ≤ rn ≤ nµ IN  + N; nµ JN  − N ≤ sn ≤ nµ JN  + N.  Furthermore rn ≤ I ∩ Pn ≤ rn + tn, because IN ⊆ I ⊆ IN ∪ K. Consequently µ I − N n  − ϵ ≤ µ IN − N n  ≤ rn n  ≤ µ I  + N n Given ϵ, this holds for all n; so limn→∞ rn n = limn→∞ rn + tn  n = µ I .  ≤ 1− µ JN  + N n  + ϵ.  ≤ rn + tn n = 1− sn n  Exercise 25 shows that Lemma M is not trivial, in the sense that some rather  restrictive hypotheses are needed to prove  45 . Distribution of partial quotients. Now we put Theorem W and Lemma M together to derive some solid facts about Euclid’s algorithm.   368  ARITHMETIC  4.5.3  Theorem E. Let pk a, n  be the probability that the  k + 1 st quotient Ak+1 in Euclid’s algorithm is equal to a, when u = n and when v is equally likely to be any of the numbers {0, 1, . . . , n − 1}. Then   − Fk   1  1  a  a + 1    ,  lim n→∞ pk a, n  = Fk  where Fk x  is the distribution function  22 . Proof. The set I of all X in [0 . . 1  for which Ak+1 = a is a union of disjoint intervals, and so is the set J of all X for which Ak+1 ̸= a. Lemma M therefore applies, with K the set of all X for which Ak+1 is undefined. Furthermore, Fk 1 a  − Fk µ I , the probability that Ak+1 = a.  1  a + 1  is the probability that 1  a + 1  < Xk ≤ 1 a, which is lg 1 + 1 a  − lg1 + 1  a + 1  = lg a + 1 2  a + 1 2 − 1.  As a consequence of Theorems E and W, we can say that a quotient equal  to a occurs with the approximate probability  Thus  a quotient of 1 occurs about lg  4 a quotient of 2 occurs about lg  9 a quotient of 3 occurs about lg  16 a quotient of 4 occurs about lg  25  3  ≈ 41.504 percent of the time; 8  ≈ 16.993 percent of the time; 15  ≈ 9.311 percent of the time; 24  ≈ 5.889 percent of the time.  if Euclid’s algorithm produces the quotients A1, A2,  Actually, . . . , At, the nature of the proofs above will guarantee this behavior only for Ak when k is comparatively small with respect to t; the values At−1, At−2, . . . are not covered by this proof. But we can in fact show that the distribution of the last quotients At−1, At−2, . . . is essentially the same as the first.  For example, consider the regular continued fraction expansions for the set  of all proper fractions whose denominator is 29: 1 29 =   29   2 29 =   14, 2   3 29 =   9, 1, 2   4 29 =   7, 4   5 29 =   5, 1, 4   6 29 =   4, 1, 5   7 29 =   4, 7   Several things can be observed in this table. a  As mentioned earlier, the last quotient is always 2 or more. Furthermore, we have the obvious identity  15 29 =   1, 1, 14   16 29 =   1, 1, 4, 3   17 29 =   1, 1, 2, 2, 2   18 29 =   1, 1, 1, 1, 1, 3   19 29 =   1, 1, 1, 9   20 29 =   1, 2, 4, 2   21 29 =   1, 2, 1, 1, 1, 2    8 29 =   3, 1, 1, 1, 2   9 29 =   3, 4, 2   10 29 =   2, 1, 9   11 29 =   2, 1, 1, 1, 3   12 29 =   2, 2, 2, 2   13 29 =   2, 4, 3   14 29 =   2, 14    22 29 =   1, 3, 7   23 29 =   1, 3, 1, 5   24 29 =   1, 4, 1, 4   25 29 =   1, 6, 4   26 29 =   1, 8, 1, 2   27 29 =   1, 13, 2   28 29 =   1, 28      x1, . . . , xn−1, xn + 1   =   x1, . . . , xn−1, xn, 1  ,   46    4.5.3  ANALYSIS OF EUCLID’S ALGORITHM  369  which shows how continued fractions whose last quotient is unity are related to regular continued fractions. b  The values in the right-hand columns have a simple relationship to the values in the left-hand columns; can the reader see the correspondence before reading any further? The relevant identity is  1 −   x1, x2, . . . , xn   =   1, x1 − 1, x2, . . . , xn  ;   47   see exercise 9. c  There is symmetry between left and right in the first two columns: If   A1, A2, . . . , At   occurs, so does   At, . . . , A2, A1  . This will always be the case  see exercise 26 . d  If we examine all of the quotients in the table, we find that there are 96 in 96 ≈ 21.9 percent are equal to 2, all, of which 39 96 ≈ 8.3 percent are equal to 3; this agrees reasonably well with the probabilities 8 listed above. The number of division steps. Let us now return to our original problem and  investigate Tn, the average number of division steps when v = n. See Eq.  19 .  96 ≈ 40.6 percent are equal to 1, 21  Here are some sample values of Tn:  96  98 4.8  99 4.7  100 101 n = 95 97 4.6 5.3 Tn = 5.0 4.4 5.3 998 999 1000 1001 n = 996 6.7 Tn = 6.5 7.0 ··· n = 49998 49999 ··· 10.6 Tn =  6.4 50001 10.0  6.8 50000 9.7  997 7.3  9.8  102 4.6 ··· ··· 99999 10.7  103 5.3 9999 8.6  104 4.7 10000 8.3 100000 10.3  105 4.6 10001 9.1 100001 11.0  Notice the somewhat erratic behavior; Tn tends to be larger than its neighbors when n is prime, and it is correspondingly lower when n has many divisors.  In this list, 97, 101, 103, 997, and 49999 are primes; 10001 = 73 · 137; 49998 = 2· 3· 13· 641; 50001 = 3· 7· 2381; 99999 = 3· 3· 41· 271; and 100001 = 11· 9091.  It is not difficult to understand why this happens: If gcd u, v  = d, Euclid’s algorithm applied to u and v behaves essentially the same as if it were applied to u d and v d. Therefore, when v = n has several divisors, there are many choices of u for which n behaves as if it were smaller.  Accordingly let us consider another quantity, τn, which is the average num-  ber of division steps when v = n and when u is relatively prime to n. Thus  τn = 1 φ n   T m, n .     0≤m<n m⊥n  Tn = 1  n  d\n  φ d τd.   48    49   It follows that   370  ARITHMETIC  4.5.3  Here is a table of τn for the same values of n considered above:  96  98  97  100 5.2  n = 95 99 101 102 τn = 5.4 5.3 5.3 5.6 5.2 5.4 5.3 ··· 998 999 1000 1001 n = 996 ··· τn = 7.2 7.3 7.4 ··· 99999 n = 49998 49999 ··· τn = 10.59 10.58 11.170  7.3 50001 10.59  7.3 50000 10.57  997 7.3  103 5.4 9999 9.21  104 5.3 10000 9.21 100000 11.172  105 5.6 10001 9.22 100001 11.172  Clearly τn is much more well-behaved than Tn, and it should be more susceptible to analysis. Inspection of a table of τn for small n reveals some curious anomalies; for example, τ50 = τ100 and τ60 = τ120. But as n grows, the values of τn behave quite regularly indeed, as the table indicates, and they show no significant relation to the factorization properties of n. If these values τn are plotted as functions of ln n on graph paper, for the values of τn given above, they lie very nearly on the straight line  τn ≈ 0.843 ln n + 1.47.   50   We can account for this behavior if we study the regular continued fraction  process a little further. In Euclid’s algorithm as expressed in  15  we have  V0 U0  V1 U1  . . .  Vt−1 Ut−1  = Vt−1 U0  ,  X0X1 . . . Xt−1 = 1 U.  since Uk+1 = Vk; therefore if U = U0 and V = V0 are relatively prime, and if there are t division steps, we have  Setting U = N and V = m < N, we find that  ln X0 + ln X1 + ··· + ln Xt−1 = − ln N.   51  We know the approximate distribution of X0, X1, X2, . . . , so we can use this equation to estimate  t = T N, m  = T m, N  − 1.  Returning to the formulas preceding Theorem W, we find that the average  value of ln Xn, when X0 is a real number uniformly distributed in [0 . . 1 , is   1  0   1  0  ln x F ′  n x  dx =  ln x fn x  dx  1 + x ,  where fn x  is defined in  33 . Now   52    53   fn x  = 1  ln 2 + O 2−n ,   4.5.3  ANALYSIS OF EUCLID’S ALGORITHM  371  using the facts we have derived earlier  see exercise 23 ; hence the average value of ln Xn is very well approximated by   1  0  1 ln 2  ln x 1 + x  0  ue−u 1 + e−u du   ∞  −1 k+1 ∞  ue−ku du  25 − ··· k≥1 9 − 1 4 + 1 1 − 1 16 + 1 1  36 + ··· 1 + 1 16 + 1 4 + 1 4 + 1  9 + ··· 9 + ··· − 2 4 + 1 1 + 1  0  dx = − 1 ln 2 = − 1 ln 2 = − 1 ln 2 = − 1 ln 2 = − 1 2 ln 2  = −π2  12 ln 2 .  By  51  we therefore expect to have the approximate formula  that is, t should be approximately equal to  12 ln 2  π2 ln N. This constant  −tπ2  12 ln 2  ≈ − ln N;   12 ln 2  π2 = 0.842765913 . . . agrees perfectly with the empirical formula  50  obtained earlier, so we have good reason to believe that the formula  indicates the true asymptotic behavior of τn as n → ∞.  If we assume that  54  is valid, we obtain the formula  ln n + 1.47  τn ≈ 12 ln 2 π2  ln n −    Λ d   d  d\n  + 1.47,  where Λ d  is von Mangoldt’s function defined by the rules  Λ n  =  if n = pr for p prime and r ≥ 1; otherwise.   See exercise 27.  For example,  Tn ≈ 12 ln 2 π2 ln p,   0,    T100 ≈ 12 ln 2 π2  ln 100 − ln 2  2 − ln 2  4 − ln 5  5 − ln 5 25  + 1.47  ≈  0.843  4.605 − 0.347 − 0.173 − 0.322 − 0.064  + 1.47 ≈ 4.59;  the exact value of T100 is 4.56.   54    55    56    372  ARITHMETIC  4.5.3   57   We can also estimate the average number of division steps when u and v are  both uniformly distributed between 1 and N, by calculating 2 − 1 2N  T m, n  = 2 N 2  nTn − 1  1 N 2  N  N  N  .  m=1  n=1  n=1  Assuming formula  55 , exercise 29 shows that this sum has the form  12 ln 2  12 ln 2   58  and empirical calculations with the same numbers used to derive Eq. 4.5.2– 65  show good agreement with the formula  ln N + O 1 ,  π2  π2  ln N + 0.06.   59  Of course we have not yet proved anything about Tn and τn in general; so far we have only been considering plausible reasons why certain formulas ought to hold. Fortunately it is now possible to supply rigorous proofs, based on a careful analysis by several mathematicians. The leading coefficient 12π−2 ln 2 in the formulas above was established first, in independent studies by Gustav Lochs, John D. Dixon, and Hans A. Heilbronn. Lochs [Monatshefte für Math. 65  1961 , 27–52] derived a formula equivalent to the fact that  57  equals  12π−2 ln 2  ln N + a + O N−1 2 , where a ≈ 0.065. Unfortunately his paper remained essentially unknown for many years, perhaps because it computed only an average value from which we cannot derive definite information about Tn for any particular n. Dixon [J. Number Theory 2  1970 , 414–422] developed the theory of the Fn x  distributions to show that individual partial quotients are essentially independent of each other in an appropriate sense, and proved that for all positive ϵ we have T m, n −  12π−2 ln 2  ln n < 1 ≤ m   0. Heilbronn’s approach was completely different, working entirely with integers instead of continuous variables. His idea, which is presented in slightly modified form in exercises 33 and 34, is based on the fact that τn can be related to the number of ways to represent n in a certain manner. Furthermore, his paper [Number Theory and Analysis, edited by Paul Turán  New York: Plenum, 1969 , 87–96] shows that the distribution of individual partial quotients 1, 2, . . . that we have discussed above actually applies to the entire collection of partial quotients belonging to the fractions having a given denominator; this is a sharper form of Theorem E. A still sharper result was obtained several years later by J. W. Porter [Mathematika 22  1975 , 20–28], who established that   ln n  1 2 +ϵ except for exp−c ϵ  log N ϵ 2N 2 values of m and n in the range  where C ≈ 1.46707 80794 is the constant  ln n + C + O n−1 6+ϵ ,  τn = 12 ln 2  π2  6 ln 2 π2  3 ln 2 + 4γ − 24  π2 ζ′ 2  − 2   − 1  2;   60    61    4.5.3  ANALYSIS OF EUCLID’S ALGORITHM  373  see D. E. Knuth, Computers and Math. with Applic. 2  1976 , 137–139. Thus the conjecture  50  is fully proved. Using  60 , Graham H. Norton [J. Sym- bolic Computation 10  1990 , 53–58] extended the calculations of exercise 29 to confirm Lochs’s work, proving that the empirical constant 0.06 in  59  is actually  3 ln 2 + 4γ − 12  π2 ζ′ 2  − 3 − 1 = 0.06535 14259 . . . .  6 ln 2 π2   62   D. Hensley proved in J. Number Theory 49  1994 , 142–182, that the variance of τn is proportional to log n.  The average running time for Euclid’s algorithm on multiple-precision inte-  gers, using classical algorithms for arithmetic, was shown to be of order  1 + logmax u, v  gcd u, v  log min u, v    63   by G. E. Collins, in SICOMP 3  1974 , 1–10. Summary. We have found that the worst case of Euclid’s algorithm occurs when its inputs u and v are consecutive Fibonacci numbers  Theorem F ; the number of division steps when 0 ≤ v < N will never exceed ⌈4.8 log10 N − 0.32⌉. We have determined the frequency of the values of various partial quotients, showing, for example, that the division step finds ⌊u v⌋ = 1 about 41 percent of the time  Theorem E . And, finally, the theorems of Heilbronn and Porter prove that the average number Tn of division steps when v = n is approximately   12 ln 2  π2 ln n ≈ 1.9405 log10 n,  minus a correction term based on the divisors of n as shown in Eq.  55 .  EXERCISES   cid:120  1. [20] Since the quotient ⌊u v⌋ is equal to unity more than 40 percent of the time  in Algorithm 4.5.2A, it may be advantageous on some computers to make a test for this case and to avoid the division when the quotient is unity. Is the following MIX program for Euclid’s algorithm more efficient than Program 4.5.2A? rAX ← rA. rX ← rAX mod v. rA ← v.  2F Is u − v < v? V V  rX ← u. v ← rX. rA ← u − v.  SRAX 5 JL DIV 2H LDA  LDX U JMP 2F 1H STX V SUB V CMPA V  2. [M21] Evaluate the matrix product  3. [M21] What is the value of det  4. [M20] Prove Eq.  8 .   x1  1  JXNZ 1B Done if rX = 0.   x2  1  1 0    1 0  . . .    .  1 0    1 x1 −1 x2 0 −1 ... 0  0  . . .  0 1 1 x3 ... −1 . . . −1   xn  1   ?  0 0 ... 1 xn   374  ARITHMETIC  4.5.3  5. [HM25] Let x1, x2, . . . be a sequence of real numbers that are each greater than some positive real number ϵ. Prove that the infinite continued fraction   x1, x2, . . .    = limn→∞   x1, . . . , xn   exists. Show also that   x1, x2, . . .    need not exist if we assume only that xj > 0 for all j. 6. [M23] Prove that the regular continued fraction expansion of a number is unique in the following sense: If B1, B2, . . . are positive integers, then the infinite continued fraction   B1, B2, . . .    is an irrational number X between 0 and 1 whose regular continued fraction has An = Bn for all n ≥ 1; and if B1, . . . , Bm are positive integers with Bm > 1, then the regular continued fraction for X =   B1, . . . , Bm   has An = Bn for 1 ≤ n ≤ m. 7. [M26] Find all permutations p 1 p 2  . . . p n  of the integers {1, 2, . . . , n} such that Kn x1, x2, . . . , xn  = Kn xp 1 , xp 2 , . . . , xp n   is an identity for all x1, x2, . . . , xn. 8. [M20] Show that −1 Xn =   An, . . . , A1,−X  , whenever Xn is defined, in the regular continued fraction process. 9. [M21] Show that continued fractions satisfy the following identities: a    x1, . . . , xn   =   x1, . . . , xk +   xk+1, . . . , xn    , b    0, x1, x2, . . . , xn   = x1 +   x2, . . . , xn  , c    x1, . . . , xk−1, xk,0, xk+1, xk+2, . . . , xn   =   x1, . . . , xk−1, xk + xk+1, xk+2, . . . , xn  , d  1 −   x1, x2, . . . , xn   =   1, x1 − 1, x2, . . . , xn  , 10. [M28] By the result of exercise 6, every irrational real number X has a unique regular continued fraction representation of the form  1 ≤ k < n;  1 ≤ k ≤ n;  n ≥ 1;  n ≥ 1.  X = A0 +   A1, A2, A3, . . .   ,  where A0 is an integer and A1, A2, A3, . . . are positive integers. Show that if X has this representation then the regular continued fraction for 1 X is  1 X = B0 +   B1, . . . , Bm, A5, A6, . . .      The case A0 < 0 is, of course, the most  for suitable integers B0, B1, . . . , Bm. interesting.  Explain how to determine the B’s in terms of A0, A1, A2, A3, and A4. 11. [M30]  J.-A. Serret, 1850.  Let X = A0 +   A1, A2, A3, A4, . . .    and Y = B0 +   B1, B2, B3, B4, . . .    be the regular continued fraction representations of two real numbers X and Y, in the sense of exercise 10. Show that these representations “eventually agree,” in the sense that Am+k = Bn+k for some m and n and for all k ≥ 0, if and only if we have X =  qY + r   sY + t  for some integers q, r, s, t with qt − rs = 1.  This theorem is the analog, for continued fraction representations, of the simple result that the representations of X and Y in the decimal system eventually agree if and only if X =  10qY + r  10s for some integers q, r, and s.    cid:120  12. [M30] A quadratic irrationality is a number of the form    D − U  V, where D, U, and V are integers, D > 0, V ̸= 0, and D is not a perfect square. We may assume without loss of generality that V is a divisor of D − U 2, for otherwise the number may be rewritten as   a  Prove that the regular continued fraction expansion  in the sense of exercise 10  of √ D − U  V is obtained by the following formulas:  a quadratic irrationality X =    DV 2 − UV    V V  .  √  √  V0 = V,  Vn+1 =  D − U  2 n   Vn,  A0 = ⌊X⌋, √ An+1 = ⌊   D + Un  Vn+1⌋,  U0 = U + A0V ;  Un+1 = An+1Vn+1 − Un.   √  4.5.3  ANALYSIS OF EUCLID’S ALGORITHM  375  b  Prove that 0 < Un <  √ D, for all n > N, where N is some integer depending on X; hence the regular continued fraction representation of every quadratic irrationality is eventually periodic. [Hint: Show that  D, 0 < Vn < 2  √  √ √ D − U  V = A0 +   A1, . . . , An,−Vn     −  D + Un   ,  c  Letting pn = Kn+1 A0, A1, . . . , An  and qn = Kn A1, . . . , An , prove the identity  and use Eq.  5  to prove that    V p2  n + 2U pnqn +   U 2 − D  V  q2  D + Un  Vn is positive when n is large.] n =  −1 n+1Vn+1.  d  Prove that the regular continued fraction representation of an irrational number X is eventually periodic if and only if X is a quadratic irrationality.  This is the continued fraction analog of the fact that the decimal expansion of a real number X is eventually periodic if and only if X is rational.   13. [M40]  J. Lagrange, 1767.  Let f x  = anxn + ··· + a0, an > 0, be a polynomial having exactly one real root ξ > 1, where ξ is irrational and f′ ξ  ̸= 0. Experiment with a computer program to find the first thousand or so partial quotients of ξ, using the following algorithm  which essentially involves only addition :  L1. Set A ← 1. L2. For k = 0, 1, . . . , n− 1  in this order  and for j = n− 1, . . . , k  in this order , set aj ← aj+1 + aj.  This step replaces f x  by g x  = f x + 1 , a polynomial whose roots are one less than those of f.   L3. If an + an−1 + ··· + a0 < 0, set A ← A + 1 and return to L2. L4. Output A  which is the value of the next partial quotient . Replace the coeffi- cients  an, an−1, . . . , a0  by  −a0,−a1, . . . ,−an  and return to L1.  This step replaces f x  by a polynomial whose roots are reciprocals of those of f.  For example, starting with f x  = x3 −2, the algorithm will output “1”  changing  f x  to x3 − 3x2 − 3x − 1 ; then “3”  changing f x  to 10x3 − 6x2 − 6x − 1 ; etc. 14. [M22]  A. Hurwitz, 1891.  Show that the following rules make it possible to find the regular continued fraction expansion of 2X, given the partial quotients of X:  2   2a, b, c, . . .    =    a, 2b + 2  c, . . .     ;  2   2a + 1, b, c, . . .    =    a, 1, 1 + 2  b − 1, c, . . .     .  Use this idea to find the regular continued fraction expansion of 1 of e in  13 .   cid:120  15. [M31]  R. W. Gosper.  Generalizing exercise 14, design an algorithm that com-  2 e, given the expansion  putes the continued fraction X0 +   X1, X2, . . .    for  ax + b   cx + d , given the continued fraction x0 +   x1, x2, . . .    for x, and given integers a, b, c, d with ad ̸= bc. Make your algorithm an “online coroutine” that outputs as many Xk as possible before inputting each xj. Demonstrate how your algorithm computes  97x + 39   −62x− 25  when x = −1 +   5, 1, 1, 1, 2, 1, 2  . 16. [HM30]  L. Euler, 1731.  Let f0 z  =  ez − e−z   ez + e−z  = tanh z, and let fn+1 z  = 1 fn z  −  2n + 1  z. Prove that, for all n, fn z  is an analytic function of the complex variable z in a neighborhood of the origin, and it satisfies the differential equation f′n z  = 1 − fn z 2 − 2nfn z  z. Use this fact to prove that  tanh z =   z−1  , 3z−1  , 5z−1  , 7z−1  , . . .   ;   376  ARITHMETIC  4.5.3  then apply Hurwitz’s rule  exercise 14  to prove that e−1 n =    1,  2m + 1 n − 1, 1  ,  m ≥ 0.   This notation denotes the infinite continued fraction    1, n − 1, 1, 1, 3n − 1, 1, 1, 5n − 1, 1, . . .   .  Also find the regular continued fraction expansion of e−2 n when  cid:120  17. [M23]  a  Prove that   x1,−x2   =   x1 − 1, 1, x2 − 1  . n > 0 is odd.  b  Generalize this identity, obtaining a formula for   x1,−x2, x3,−x4, x5,−x6, . . . , x2n−1,−x2n   in which all partial quotients are positive integers when the x’s are large positive integers.  c  The result of exercise 16 implies that tan 1 =   1,−3, 5,−7, . . .   . Find the regular continued fraction expansion of tan 1. 18. [M25] Show that   a1, a2, . . . , am, x1, a1, a2, . . . , am, x2, a1, a2, . . . , am, x3, . . .    −   am, . . . , a2, a1, x1, am, . . . , a2, a1, x2, am, . . . , a2, a1, x3, . . .    does not depend on x1, x2, x3, . . . . Hint: Multiply both continued fractions by Km a1, a2, . . . , am . 19. [M20] Prove that F  x  = logb 1 + x  satisfies Eq.  24 . 20. [HM20] Derive  38  from  37 . 21. [HM29]  E. Wirsing.  The bounds  39  were obtained for a function φ corre- sponding to g with T g x  = 1  x + 1 . Show that the function corresponding to T g x  = 1  x + c  yields better bounds, when c > 0 is an appropriate constant. 22. [HM46]  K. I. Babenko.  Develop efficient means to calculate accurate approxi- mations to the quantities λj and Ψj x  in  44 , for small j ≥ 3 and for 0 ≤ x ≤ 1. 23. [HM23] Prove  53 , using results from the proof of Theorem W. 24. [M22] What is the average value of a partial quotient An in the regular continued fraction expansion of a random real number? 25. [HM25] Find an example of a set I = I1 ∪ I2 ∪ I3 ∪ ··· ⊆ [0 . . 1], where the I’s are disjoint intervals, for which  45  does not hold. 26. [M23] Show that if the numbers {1 n, 2 n, . . . ,⌊n 2⌋ n} are expressed as regular continued fractions, the result is symmetric between left and right, in the sense that   At, . . . , A2, A1   appears whenever   A1, A2, . . . , At   does. 27. [M21] Derive  55  from  49  and  54 . 28. [M23] Prove the following identities involving the three number-theoretic func- tions φ n , µ n , Λ n :  a   c  Λ n  =  d\n  µ d  = δn1.   ln d,   n  d  µ  b  ln n =  n φ n  =  d\n µ    Λ d ,  d.  d  n =  d\n  φ d .  d\n  d\n  29. [M23] Assuming that Tn is given by  55 , show that  57  equals  58 .   cid:120  30. [HM32] The following “greedy” variant of Euclid’s algorithm is often suggested:  Instead of replacing v by u mod v during the division step, replace it by  u mod v − v if u mod v > 1 2 v. Thus, for example, if u = 26 and v = 7, we have gcd 26, 7  = gcd −2, 7  = gcd 7, 2 ; −2 is the remainder of smallest magnitude when multiples of 7 are subtracted from 26. Compare this procedure with Euclid’s algorithm; estimate the number of division steps this method saves, on the average.   4.5.3   cid:120  31. [M35] Find the worst case of the modification of Euclid’s algorithm suggested in  ANALYSIS OF EUCLID’S ALGORITHM  exercise 30: What are the smallest inputs u > v > 0 that require n division steps? 32. [20]  a  A Morse code sequence of length n is a string of r dots and s dashes, where r + 2s = n. For example, the Morse code sequences of length 4 are  377   cid:113  cid:113  cid:113  cid:113 ,   cid:113  cid:113   ,   cid:113    cid:113 ,   cid:113  cid:113 ,  .  Noting that the continuant K4 x1, x2, x3, x4  is x1x2x3x4 + x1x2 + x1x4 + x3x4 + 1, find and prove a simple relation between Kn x1, . . . , xn  and Morse code sequences of length n.  b   L. Euler, Novi Comm. Acad. Sci. Pet. 9  1762 , 53–69.  Prove that  Km+n x1, . . . , xm+n  = Km x1, . . . , xm Kn xm+1, . . . , xm+n   + Km−1 x1, . . . , xm−1 Kn−1 xm+2, . . . , xm+n .  33. [M32] Let h n  be the number of representations of n in the form  x ⊥ y,  x > y > 0,  x′ > y′ > 0,  integer x, x′, y, y′.  n = xx′ + yy′, a  Show that if the conditions are relaxed to allow x′ = y′, the number of represen- tations is h n  + ⌊ n − 1  2⌋. b  Show that for fixed y > 0 and 0 < t ≤ y, where t ⊥ y, and for each fixed x′ in the range 0 < x′ < n  y + t  such that x′t ≡ n  modulo y , there is exactly one representation of n satisfying the restrictions of  a  and the condition x ≡ t  modulo y . all positive integers y, t, t′ such that t ⊥ y, t ≤ y, t′ ≤ y, tt′ ≡ n  modulo y .  c  Consequently, h n  =⌈ n  y + t  − t′   y⌉−⌊ n− 1  2⌋, where the sum is over  d  Show that each of the h n  representations can be expressed uniquely in the form  x = Km x1, . . . , xm , x′ = Kk xm+1, . . . , xm+k  d,  y = Km−1 x1, . . . , xm−1 , y′ = Kk−1 xm+2, . . . , xm+k  d,  Prove that  where m, k, d, and the xj are positive integers with x1 ≥ 2, xm+k ≥ 2, and d is a di- visor of n. The identity of exercise 32 now implies that n d = Km+k x1, . . . , xm+k . Conversely, any given sequence of positive integers x1, . . . , xm+k such that x1 ≥ 2, xm+k ≥ 2, and Km+k x1, . . . , xm+k  divides n, corresponds in this way to m+k−1 representations of n.  e  Therefore nTn = ⌊ 5n − 3  2⌋ + 2h n . 34. [HM40]  H. Heilbronn.  Let hd n  be the number of representations of n as in exercise 33 such that xd < x′, plus half the number of representations with xd = x′. a  Let g n  be the number of representations without the requirement that x ⊥ y.  h n  =   n    g n  = 2   n    .  ,  d  d  hd  d\n  µ d g  b  Generalizing exercise 33 b , show that for d ≥ 1, hd n  = n  y y + t   + O n , where the sum is over all integers y and t such that t ⊥ y and 0 < t ≤ y <n d. c  Show that y  y + t   = φ y  ln 2 + O σ−1 y  , where the sum is over the range 0 < t ≤ y, t ⊥ y, and where σ−1 y  = d  Show thatn 2  ln n −  y=1 φ y  y2 =n  d\y 1 d . d=1 µ d H⌊n d⌋ d2.  e  Hence we have the asymptotic formula  Tn =   12 ln 2  π  Λ d  d  + O σ−1 n 2 .  d\n  d\n   4.5.3  378  ARITHMETIC  35. [HM41]  A. C. Yao and D. E. Knuth.  Prove that the sum of all partial quotients  for the fractions m n, for 1 ≤ m < n, is equal to 2 ⌊x y⌋ +⌊n 2⌋ , where the sum is that⌊x y⌋ = 3π−2n ln n 2 + O n log n  log log n 2 , and apply this to the “ancient”  over all representations n = xx′ + yy′ satisfying the conditions of exercise 33 a . Show  form of Euclid’s algorithm that uses only subtraction instead of division. 36. [M25]  G. H. Bradley.  What is the smallest value of un such that the calculation of gcd u1, . . . , un  by Algorithm 4.5.2C requires N divisions, if Euclid’s algorithm is used throughout? Assume that N ≥ n ≥ 3. 37. [M38]  T. S. Motzkin and E. G. Straus.  Let a1, . . . , an be positive integers. Show that max Kn ap 1 , . . . , ap n  , over all permutations p 1  . . . p n  of {1, 2, . . . , n}, occurs when ap 1  ≥ ap n  ≥ ap 2  ≥ ap n−1  ≥ ··· ; and the minimum occurs when ap 1  ≤ ap n  ≤ ap 3  ≤ ap n−2  ≤ ap 5  ≤ ··· ≤ ap 6  ≤ ap n−3  ≤ ap 4  ≤ ap n−1  ≤ ap 2 . 38. [M25]  J. Mikusiński.  Let L n  = maxm≥0 T  m, n . Theorem F shows that  cid:120  39. [M25]  R. W. Gosper.  If a baseball player’s batting average is .334, what is the √ L n  ≤ logϕ   cid:120  40. [M28]  The Stern–Brocot tree.  Consider an infinite binary tree in which each  smallest possible number of times he has been at bat? [Note for non-baseball-fans: Batting average =  number of hits   times at bat , rounded to three decimal places.]  5 n + 1  − 2; prove that 2L n  ≥ logϕ   √ 5 n + 1  − 2.  node is labeled with the fraction  pl + pr   ql + qr , where pl ql is the label of the node’s nearest left ancestor and pr qr is the label of the node’s nearest right ancestor.  A left ancestor is one that precedes a node in symmetric order, while a right ancestor follows the node. See Section 2.3.1 for the definition of symmetric order.  If the node has no left ancestors, pl ql = 0 1; if it has no right ancestors, pr qr = 1 0. Thus the label of the root is 1 1; the labels of its two children are 1 2 and 2 1; the labels of the four nodes on level 2 are 1 3, 2 3, 3 2, and 3 1, from left to right; the labels of the eight nodes on level 3 are 1 4, 2 5, 3 5, 3 4, 4 3, 5 3, 5 2, 4 1; and so on.  Prove that p is relatively prime to q in each label p q; furthermore, the node labeled p q precedes the node labeled p′ q′ in symmetric order if and only if the labels satisfy p q < p′ q′. Find a connection between the continued fraction for the label of a node and the path to that node, thereby showing that each positive rational number appears as the label of exactly one node in the tree. 41. [M40]  J. Shallit, 1979.  Show that the regular continued fraction expansion of  21 + 1 1  23 + 1  27 + ··· =  n≥1  1  22n−1  of Liouville’s numbers   contains only 1s and 2s and has a fairly simple pattern. Prove that the partial quotients n≥1 l−n! also have a regular pattern, when l is any integer ≥ 2. [The latter numbers, introduced by J. Liouville in J. de Math. Pures et Appl. 16  1851 , 133–142, were the first explicitly defined numbers to be proved transcendental. The former number and similar constants were first proved transcendental by A. J. Kempner, Trans. Amer. Math. Soc. 17  1916 , 476–482.] 42. [M30]  J. Lagrange, 1798.  Let X have the regular continued fraction expansion   A1, A2, . . .   , and let qn = Kn A1, . . . , An . Let ∥x∥ denote the distance from x to the nearest integer, namely minp x − p. Show that ∥qX∥ ≥ ∥qn−1X∥ for 1 ≤ q < qn.  Thus the denominators qn of the so-called convergents pn qn =   A1, . . . , An   are the “record-breaking” integers that make ∥qX∥ achieve new lows.    4.5.4  FACTORING INTO PRIMES  379  √  N and no overflow occurs.  43. [M30]  D. W. Matula.  Show that the “mediant rounding” rule for fixed slash or floating slash numbers, Eq. 4.5.1– 1 , can be implemented simply as follows, when the number x > 0 is not representable: Let the regular continued fraction expansion of x be a0 +   a1, a2, . . .   , and let pn = Kn+1 a0, . . . , an , qn = Kn a1, . . . , an . Then round x  =  pi qi , where  pi qi  is representable but  pi+1 qi+1  is not. [Hint: See exercise 40.] 44. [M25] Suppose we are doing fixed slash arithmetic with mediant rounding, where the fraction  u u′  is representable if and only if u < M and 0 ≤ u′ < N and u ⊥ u′. Prove or disprove the identity   u u′  ⊕  v v′   ⊖  v v′  =  u u′  for all representable  u u′  and  v v′ , provided that u′ < 45. [M25] Show that Euclid’s algorithm  Algorithm 4.5.2A  applied to two n-bit binary numbers requires O n2  units of time, as n → ∞.  The same upper bound obviously holds for Algorithm 4.5.2B.  46. [M43] Can the upper bound O n2  in exercise 45 be decreased, if another algo- rithm for calculating the greatest common divisor is used? 47. [M40] Develop a computer program to find as many partial quotients of x as possible, when x is a real number given with high precision. Use your program to calculate the first several thousand partial quotients of Euler’s constant γ, which can be calculated as explained by D. W. Sweeney in Math. Comp. 17  1963 , 170–178.  If γ is a rational number, you might discover its numerator and denominator, thereby resolving a famous problem in mathematics. According to the theory in the text, we expect to get about 0.97 partial quotients per decimal digit, when the given number is random. Multiprecision division is not necessary; see Algorithm 4.5.2L and the article by J. W. Wrench, Jr. and D. Shanks, Math. Comp. 20  1966 , 444–447.  48. [M21] Let T0 =  1, 0, u , T1 =  0, 1, v , . . . , Tn+1 =   −1 n+1v d,  −1 nu d, 0  be the sequence of vectors computed by Algorithm 4.5.2X  the extended Euclidean algorithm , and let   a1, . . . , an   be the regular continued fraction for v u. Express Tj in terms of continuants involving a1, . . . , an, for 1 < j ≤ n. 49. [M33] By adjusting the final iteration of Algorithm 4.5.2X so that an is optionally replaced by two partial quotients  an − 1, 1 , we can assume that the number of iterations, n, has a given parity. Continuing the previous exercise, let λ and µ be  arbitrary positive real numbers and let θ =λµv d, where d = gcd u, v . Prove that  cid:120  50. [M25] Given an irrational number α ∈  0 . . 1  and real numbers β and γ with  cid:120  51. [M30]  Rational reconstruction.  The number 28481 turns out to be equal to 41 316  modulo 199999 , in the sense that 316 · 28481 ≡ 41. How could a person discover this? Given integers a and m with m > a > 1, explain how to find integers x m 2, and y ≤ √ and y such that ax ≡ y  modulo m , x ⊥ y, 0 < x ≤ √ m 2, or to determine that no such x and y exist. Can there be more than one solution?  0 ≤ β < γ < 1, let f α, β, γ  be the smallest nonnegative integer n such that β ≤ αn mod 1 < γ.  Such an integer exists because of Weyl’s theorem, exercise 3.5–22.  Design an algorithm to compute f α, β, γ .  if n is even, and if Tj =  xj, yj, zj , we have minn+1  j=1λxj + µzj − [j even] θ ≤ θ.  4.5.4. Factoring into Primes Several of the computational methods we have encountered in this book rest on the fact that every positive integer n can be expressed in a unique way in the   4.5.4  ARITHMETIC  380  form  p1 ≤ p2 ≤ ··· ≤ pt,  n = p1p2 . . . pt,   1  where each pk is prime.  When n = 1, this equation holds for t = 0.  It is unfortunately not a simple matter to find this prime factorization of n, or to determine whether or not n is prime. So far as anyone knows, it is a great deal harder to factor a large number n than to compute the greatest common divisor of two large numbers m and n; therefore we should avoid factoring large numbers whenever possible. But several ingenious ways to speed up the factoring process have been discovered, and we will now investigate some of them. [A comprehensive history of factoring before 1950 has been compiled by H. C. Williams and J. O. Shallit, Proc. Symp. Applied Math. 48  1993 , 481–531.] Divide and factor. First let us consider the most obvious algorithm for factor- ization: If n > 1, we can divide n by successive primes p = 2, 3, 5, . . . until discovering the smallest p for which n mod p = 0. Then p is the smallest prime factor of n, and the same process may be applied to n ← n p in an attempt to divide this new value of n by p and by higher primes. If at any stage we find that n mod p ̸= 0 but ⌊n p⌋ ≤ p, we can conclude that n is prime; for if n is not prime, then by  1  we must have n ≥ p2 1, but p1 > p implies that 1 ≥  p + 1 2 > p p + 1  > p2 +  n mod p  ≥ ⌊n p⌋p +  n mod p  = n. This leads p2 us to the following procedure: Algorithm A  Factoring by division . Given a positive integer N, this algorithm finds the prime factors p1 ≤ p2 ≤ ··· ≤ pt of N as in Eq.  1 . The method makes use of an auxiliary sequence of trial divisors  2  which includes all prime numbers ≤ √ N  and possibly values that are not prime, if convenient . The sequence of d’s must also include at least one value such that dk ≥ √ A1. [Initialize.] Set t ← 0, k ← 0, n ← N.  During this algorithm the variables t, k, n are related by the following condition: “n = N p1 . . . pt, and n has no prime factors less than dk.”   2 = d0 < d1 < d2 < d3 < ··· ,  N.  and remainder obtained when n is divided by dk.   A2. [n = 1?] If n = 1, the algorithm terminates. A3. [Divide.] Set q ← ⌊n dk⌋, r ← n mod dk.  Here q and r are the quotient A4. [Zero remainder?] If r ̸= 0, go to step A6. A5. [Factor found.] Increase t by 1, and set pt ← dk, n ← q. Return to step A2. A6. [Low quotient?] If q > dk, increase k by 1 and return to step A3. A7. [n is prime.] Increase t by 1, set pt ← n, and terminate the algorithm.  As an example of Algorithm A, consider the factorization of the number N = 25852. We find immediately that N = 2·12926; hence p1 = 2. Furthermore, 12926 = 2· 6463, so p2 = 2. But now n = 6463 is not divisible by 2, 3, 5, . . . , 19;   4.5.4  FACTORING INTO PRIMES  381  Fig. 11. A simple factoring algorithm.  √  we find that n = 23 · 281, hence p3 = 23. Finally 281 = 12 · 23 + 5 and 12 ≤ 23; hence p4 = 281. The determination of 25852’s factors has therefore involved a total of 12 division operations; on the other hand, if we had tried to factor the slightly smaller number 25849  which is prime , at least 38 division operations would have been performed. This illustrates the fact that Algorithm A requires pt  .  If t = 1, this formula a running time roughly proportional to max pt−1, is valid if we adopt the convention p0 = 1.   The sequence d0, d1, d2, . . . of trial divisors used in Algorithm A can be taken to be simply 2, 3, 5, 7, 11, 13, 17, 19, 23, 25, 29, 31, 35, . . . , where we alternately add 2 and 4 after the first three terms. This sequence contains all numbers that are not multiples of 2 or 3; it also includes numbers such as 25, 35, 49, etc., which are not prime, but the algorithm will still give the correct answer. A further savings of 20 percent in computation time can be made by removing the numbers 30m ± 5 from the list for m ≥ 1, thereby eliminating all of the spurious multiples of 5. The exclusion of multiples of 7 shortens the list by 14 percent more, etc. A compact bit table can be used to govern the choice of trial divisors.  If N is known to be small, it is reasonable to have a table of all the necessary primes as part of the program. For example, if N is less than a million, we need only include the 168 primes less than a thousand  followed by the value d168 = 1000, to terminate the list in case N is a prime larger than 9972 . Such a table can be set up by means of a short auxiliary program; see, for example, Algorithm 1.3.2P or exercise 8. How many trial divisions are necessary in Algorithm A? Let π x  be the number of primes ≤ x, so that π 2  = 1, π 10  = 4; the asymptotic behavior of this function has been studied extensively by many of the world’s greatest mathematicians, beginning with Legendre in 1798. Numerous advances made during the nineteenth century culminated in 1899, when Charles de La Vallée Poussin proved that, for some A > 0,   x  2  dt ln t  + Oxe−A  √  log x.  π x  =   3   A1. Initialize  No  A2. n = 1?  Yes  A3. Divide  A5. Factor  found  Yes  A4. Zero  remainder?  No  No  A6. Low quotient?  Yes  A7. n is  prime   382  ARITHMETIC  4.5.4    π x  = x ln x   ln x 2 + 2! x + x   ln x 3 + ··· + r! x  [Mém. Couronnés Acad. Roy. Belgique 59  1899 , 1–74; see also J. Hadamard, Bull. Soc. Math. de France 24  1896 , 199–220.] Integrating by parts yields   for example, it can be replaced by Ox exp−A log x 3 5  log log x 1 5. x + ···   4  for all fixed r ≥ 0. The error term in  3  has subsequently been improved; [See A. Walfisz, Weyl’sche Exponentialsummen in der neueren Zahlentheorie  Berlin: 1963 , Chapter 5.] Bernhard Riemann conjectured in 1859 that  x = L x  − 1 2 L√  x − 1 3 L 3√   ln x r+1 + O  L k√   log x r+2  π x  ≈  lg x  µ k  k   5   x  k=1 2 dt ln t, and his formula agrees well with actual counts when x  where L x  = x  is of reasonable size:  x 103 106 109 1012 1015 1018  π x   168 78498 50847534 37607912018 29844570422669 24739954287740860  L x   Riemann’s formula 168.3 78527.4 50847455.4 37607910542.2 29844570495886.9 24739954309690414.0 24739954284239494.4  176.6 78626.5 50849233.9 37607950279.8 29844571475286.5   See exercise 41.  However, the distribution of large primes is not that simple, and Riemann’s conjecture  5  was disproved by J. E. Littlewood in 1914; see Hardy and Littlewood, Acta Math. 41  1918 , 119–196, where it is shown that there is a positive constant C such that √  π x  > L x  + C  x log log log x log x  for infinitely many x. Littlewood’s result shows that prime numbers are inher- ently somewhat mysterious, and it will be necessary to develop deep properties of mathematics before their distribution is really understood. Riemann made another much more plausible conjecture, the famous “Riemann hypothesis,” which states that the complex function ζ z  is zero only when the real part of z is equal to 1 2, except in the trivial cases where z is a negative even integer. This  hypothesis, if true, would imply that π x  = L x + O√  x log x; see exercise 25.  Richard Brent has used a method of D. H. Lehmer to verify Riemann’s hypothesis computationally for all “small” values of z, by showing that ζ z  has exactly 75,000,000 zeros whose imaginary part is in the range 0 < ℑz < 32585736.4; all of these zeros have ℜz = 1 2 and ζ′ z  ̸= 0. [Math. Comp. 33  1979 , 1361–1372.] In order to analyze the average behavior of Algorithm A, we would like to know how large the largest prime factor pt will tend to be. This question was first investigated by Karl Dickman [Arkiv för Mat., Astron. och Fys. 22A, 10  1930 , 1–14], who studied the probability that a random integer between 1 and x will have its largest prime factor ≤ xα. Dickman gave a heuristic argument to show    α  0   t   dt  F  1 − t  ,  t  F α  =  4.5.4 383 that this probability approaches the limiting value F α  as x → ∞, where F can be calculated from the functional equation  FACTORING INTO PRIMES  for 0 ≤ α ≤ 1;  F α  = 1,  for α ≥ 1.   6   His argument was essentially this: Given 0 < t < 1, the number of integers less than x whose largest prime factor is between xt and xt+dt is xF ′ t  dt. The xt dt t. For every such p, the number of integers n such that “np ≤ x and the largest prime factor of n is ≤ p” is the number of n ≤ x1−t whose largest  number of primes p in that range is π xt+dt −π xt  = πxt+ ln x xt dt−π xt  = prime factor is ≤  x1−t t  1−t , namely x1−t Ft  1 − t . Hence xF ′ t  dt =  xt dt t x1−tFt  1 − t , and  6  follows by integration. This heuristic argu-  ment can be made rigorous; V. Ramaswami [Bull. Amer. Math. Soc. 55  1949 , 1122–1127] showed that the probability in question for fixed α is asymptotically F α +O 1 log x , as x → ∞, and many other authors have extended the analysis [see the survey by Karl K. Norton, Memoirs Amer. Math. Soc. 106  1971 , 9–27].  If 1  2 ≤ α ≤ 1, formula  6  simplifies to   1  α   t  F  1 − t   dt  t   1  α  dt t  F α  = 1 −  = 1 −  = 1 + ln α.  √  Thus, for example, the probability that a random positive integer ≤ x has a x is 1 − F  1 prime factor > In all such cases, Algorithm A must work hard.  2  = ln 2, about 69 percent.  The net result of this discussion is that Algorithm A will give the answer rather quickly if we want to factor a six-digit number; but for large N the amount of computer time for factorization by trial division will rapidly exceed practical limits, unless we are unusually lucky.  Later in this section we will see that there are fairly good ways to determine whether or not a reasonably large number n is prime, without trying all divisors up to n. Therefore Algorithm A would often run faster if we inserted a primality test between steps A2 and A3; the running time for this improved algorithm would then be roughly proportional to pt−1, the second-largest prime pt  . By an argument analogous to Dick- factor of N, instead of to max pt−1, man’s  see exercise 18 , we can show that the second-largest prime factor of a random integer ≤ x will be ≤ xβ with approximate probability G β , where  √  √   β     t   − F   t   dt  1 − t  1 − t 2.  See Fig. 12.  Numerical evaluation of  6  and  7   t  ,  for 0 ≤ β ≤ 1 2 .   7   G β  =  0  G Clearly G β  = 1 for β ≥ 1 yields the following “percentage points”: F  α , G β  = .01 .35  .10  .05  .99 α ≈ .2697 .3348 .3785 .4430 .5220 .6065 .7047 .8187 .9048 .9512 .9900 β ≈ .0056 .0273 .0531 .1003 .1611 .2117 .2582 .3104 .3590 .3967 .4517 Thus, the second-largest prime factor will be ≤ x.2117 about half the time, etc.  .50  .20  .65  .80  .90  .95   384  ARITHMETIC  4.5.4  Fig. 12. Probability distribution functions for the two largest prime factors of a random integer ≤ x.   c  ln ln x approaches e−u2 2 du  1√ 2π  The total number of prime factors, t, has also been intensively analyzed. Obviously 1 ≤ t ≤ lg N, but these lower and upper bounds are seldom achieved. It is possible to prove that if N is chosen at random between 1 and x, the probability that t ≤ ln ln x + c  √   8  as x → ∞, for any fixed c. In other words, the distribution of t is essentially normal, with mean and variance ln ln x; about 99.73 percent of all the large integers ≤ x have t − ln ln x ≤ 3 ln ln x. Furthermore the average value of t − ln ln x for 1 ≤ N ≤ x is known to approach  −∞  √  γ +   ln 1 − 1 p  + 1  p − 1  = γ +  φ n  ln ζ n   p prime  n≥2  n  = 1.03465 38818 97437 91161 97942 98464 63825 46703+ .   9  [See G. H. Hardy and E. M. Wright, An Introduction to the Theory of Numbers, 5th edition  Oxford, 1979 , §22.11; see also P. Erdös and M. Kac, Amer. J. Math. 62  1940 , 738–742.]  The size of prime factors has a remarkable connection with permutations: The average number of bits in the kth largest prime factor of a random n-bit integer is asymptotically the same as the average length of the kth largest cycle of a random n-element permutation, as n → ∞. [See D. E. Knuth, Selected Papers on Analysis of Algorithms  2000 , 329–330, 336–337, for references to the relevant literature.] It follows that Algorithm A usually finds a few small factors and then begins a long-drawn-out search for the big ones that are left.  An excellent exposition of the probability distribution of the prime factors of a random integer has been given by Patrick Billingsley, AMM 80  1973 , 1099–1115; see also his paper in Annals of Probability 2  1974 , 749–791. Factoring by pseudorandom cycles. Near the beginning of Chapter 3, we observed that “a random number generator chosen at random isn’t very random.” This principle, which worked against us in that chapter, has the redeeming virtue  Second largest  1.0  0.8  0.6  0.4  0.2  0.0  Largest  x0  x.1 x.2 x.3 x.4 x.5 x.6 x.7 x.8 x.9  x1   4.5.4  FACTORING INTO PRIMES  385  that it leads to a surprisingly efficient method of factorization, discovered by J. M. Pollard [BIT 15  1975 , 331–334]. The number of computational steps in Pollard’s method is on the order of √ pt−1, so it is significantly faster than Algorithm A when N is large. According to  7  and Fig. 12, the running time will usually be well under N 1 4.  Let f x  be any polynomial with integer coefficients, and consider the two  sequences defined by  x0 = y0 = A;  xm+1 = f xm  mod N,  ym+1 = f ym  mod p,   10   where p is any prime factor of N. It follows that  for m ≥ 1.  √  ym = xm mod p,   11  Now exercise 3.1–7 shows that we will have ym = yℓ m −1 for some m ≥ 1, where ℓ m  is the greatest power of 2 that is ≤ m. Thus xm − xℓ m −1 will be a multiple of p. Furthermore if f y  mod p behaves as a random mapping from the set {0, 1, . . . , p − 1} into itself, exercise 3.1–12 shows that the average value of the least such m will be of order p. In fact, exercise 4 below shows that this average value for random mappings is less than 1.625 Q p , where the  function Q p  ≈πp 2 was defined in Section 1.2.11.3. If the different prime  divisors of N correspond to different values of m  as they almost surely will, when N is large , we will be able to find them by calculating gcd xm − xℓ m −1, N  for m = 1, 2, 3, . . . , until the unfactored residue is prime. Pollard called his technique the “rho method,” because an eventually periodic sequence such as y0, y1, . . . is reminiscent of the Greek letter ρ.  From the theory in Chapter 3, we know that a linear polynomial f x  = ax + c will not be sufficiently random for our purposes. The next-simplest case is quadratic, say f x  = x2 + 1. We don’t know that this function is sufficiently random, but our lack of knowledge tends to support the hypothesis of randomness, and empirical tests show that this f does work essentially as predicted. In fact, f is probably slightly better than random, since x2 + 1 takes on only 1 2 p + 1  distinct values mod p; see Arney and Bender, Pacific J. Math. 103  1982 , 269–294. Therefore the following procedure is reasonable: Algorithm B  Factoring by the rho method . This algorithm outputs the prime factors of a given integer N ≥ 2, with high probability, although there is a chance that it will fail.  B1. [Initialize.] Set x ← 5, x′ ← 2, k ← 1, l ← 1, n ← N. During this A = 2, l = ℓ m , and k = 2l − m.  algorithm, n is the unfactored part of N, and the variables x and x′ represent the quantities xm mod n and xℓ m −1 mod n in  10 , where f x  = x2 + 1,  B2. [Test primality.]  If n is prime  see the discussion below , output n; the  algorithm terminates.  B3. [Factor found?] Set g ← gcd x′−x, n . If g = 1, go on to step B4; otherwise output g. Now if g = n, the algorithm terminates  and it has failed, because   386  ARITHMETIC  4.5.4 we know that n isn’t prime . Otherwise set n ← n g, x ← x mod n, x′ ← x′ mod n, and return to step B2.  Note that g may not be prime; this should be tested. In the rare event that g isn’t prime, its prime factors won’t be determinable with this algorithm.   B4. [Advance.] Set k ← k − 1.  If k = 0, set x′ ← x, l ← 2l, k ← l. Set  x ←  x2 + 1  mod n and return to B3. As an example of Algorithm B, let’s try to factor N = 25852 again. The third execution of step B3 will output g = 4  which isn’t prime . After six more iterations the algorithm finds the factor g = 23. Algorithm B has not distinguished itself in this example, but of course it was designed to factor big numbers. Algorithm A takes much longer to find large prime factors, but it can’t be beat when it comes to removing the small ones. In practice, we should run Algorithm A awhile before switching over to Algorithm B.  We can get a better idea of Algorithm B’s prowess by considering the ten largest six-digit primes. The number of iterations, m p , that Algorithm B needs to find the factor p is given in the following table:  395  474  409  1091  2106  1561  1819  1593 √  p, and it never exceeds 16  p = 999863 999883 999907 999917 999931 999953 999959 999961 999979 999983 276 m p  = 814 Experiments by Tomás Oliveira e Silva indicate that m p  has an average value √ p when p < 1000000000. The maximum of about 2 √ m p  for p < 109 is m 850112303  = 416784; and the maximum of m p   p occurs when p = 695361131, m p  = 406244. According to these experimental results, almost all 18-digit numbers can be factored in fewer than 64,000 itera- tions of Algorithm B  compared to roughly 50,000,000 divisions in Algorithm A . The time-consuming operations in each iteration of Algorithm B are the multiple-precision multiplication and division in step B4, and the gcd in step B3. The technique of “Montgomery multiplication”  exercise 4.3.1–41  will speed this up. Moreover, if the gcd operation is slow, Pollard suggests gaining speed by accumulating the product mod n of, say, ten consecutive  x′ − x  values before taking each gcd; this replaces 90 percent of the gcd operations by a single multiplication mod N while only slightly increasing the chance of failure. He also suggests starting with m = q instead of m = 1 in step B1, where q is, say, one tenth of the number of iterations you are planning to use. In those rare cases where failure occurs for large N, we could try using f x  = x2 + c for some c ̸= 0 or 1. The value c = −2 should also be avoided, m − 2 has solutions of the form xm = r2m + r−2m. since the recurrence xm+1 = x2 Other values of c do not seem to lead to simple relationships mod p, and they should all be satisfactory when used with suitable starting values.  Richard Brent used a modification of Algorithm B to discover the prime factor 1238926361552897 of 2256 + 1. [See Math. Comp. 36  1981 , 627–630; 38  1982 , 253–255.] Fermat’s method. Another approach to the factoring problem, which was used by Pierre de Fermat in 1643, is more suited to finding large factors than small   4.5.4  FACTORING INTO PRIMES  387  ones. [Fermat’s original description of his method, translated into English, can be found in L. E. Dickson’s monumental History of the Theory of Numbers 1  Carnegie Inst. of Washington, 1919 , 357. An equivalent idea had in fact been used already by N¯ar¯ayan.a Pan.d.ita in his remarkable book Gan. ita Kaumud¯ı  1356 ; see Parmanand Singh, Gan. ita Bh¯arat¯ı 22  2000 , 72–74.] Assume that N = uv, where u ≤ v. For practical purposes we may assume that N is odd; this means that u and v are odd, and we can let  y =  v − u  2, 0 ≤ y < x ≤ N.  x =  u + v  2, N = x2 − y2,   12   13  Fermat’s method consists of searching systematically for values of x and y that satisfy Eq.  13 . The following algorithm shows how factoring can therefore be done without using any multiplication or division: Algorithm C  Factoring by addition and subtraction . Given an odd number N, this algorithm determines the largest factor of N less than or equal to C1. [Initialize.] Set a ← 2⌊√  N⌋2 − N.  During this algorithm a, b, and r correspond respectively to 2x+1, 2y+1, and x2−y2−N as we search for a solution to  13 ; we will have r < a and b < a.   N.  √  N⌋ + 1, b ← 1, r ← ⌊√ N = a − b  2 a + b − 2  2,  C2. [Done?] If r = 0, the algorithm terminates; we have  and  a − b  2 is the largest factor of N less than or equal to  C3. [Increase a.] Set r ← r + a and a ← a + 2. C4. [Increase b.] Set r ← r − b and b ← b + 2. C5. [Test r.] Return to step C4 if r > 0, otherwise go back to C2.  √  N.  The reader may find it amusing to find the factors of 377 by hand, using this algorithm. The number of steps needed to find the factors u and v of N = uv is essentially proportional to  a+ b−2  2−⌊√ N⌋; this can, of course, be a very large number, although each step can be done very rapidly on most computers. An improvement that requires only O N 1 3  operations in the worst case has been developed by R. S. Lehman [Math. Comp. 28  1974 , 637–646].  N⌋ = v−⌊√  It is not quite correct to call Algorithm C “Fermat’s method,” since Fermat used a somewhat more streamlined approach. Algorithm C’s main loop is quite fast on computers, but it is not very suitable for hand calculation. Fermat didn’t actually maintain the running value of y; he would look at x2 − N and guess whether or not this quantity was a perfect square by looking at its least significant digits.  The last two digits of a perfect square must be 00, e1, e4, 25, o6, or e9, where e is an even digit and o is an odd digit.  Therefore he avoided the operations of steps C4 and C5, replacing them by an occasional determination that a certain number is not a perfect square.  Fermat’s method of looking at the rightmost digits can, of course, be general- ized by using other moduli. Suppose for clarity that N = 8616460799, a number   388  ARITHMETIC  4.5.4  if x mod m is 0, 1, 2 0, 1, 2, 3, 4 0, 1, 2, 3, 4, 5, 6 0, 1, 2, 3, 4, 5, 6, 7 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10  whose historic significance is explained below, and consider the following table: and  x2 − N  mod m is m 1, 2, 2 3 1, 2, 0, 0, 2 5 7 5, 6, 2, 0, 0, 2, 6 1, 2, 5, 2, 1, 2, 5, 2 8 11 10, 0, 3, 8, 4, 2, 2, 4, 8, 3, 0 If x2 − N is to be a perfect square y2, it must have a residue mod m consistent with this fact, for all m. For example, if N = 8616460799 and x mod 3 ̸= 0, then  x2 − N  mod 3 = 2, so x2 − N cannot be a perfect square; therefore x must be a multiple of 3 whenever N = x2 − y2. The table tells us, in fact, that  then x2 mod m is 0, 1, 1 0, 1, 4, 4, 1 0, 1, 4, 2, 2, 4, 1 0, 1, 4, 1, 0, 1, 4, 1 0, 1, 4, 9, 5, 3, 3, 5, 9, 4, 1  x mod 3 = 0; x mod 5 = 0, 2, or 3; x mod 7 = 2, 3, 4, or 5; x mod 8 = 0 or 4  hence x mod 4 = 0 ; x mod 11 = 1, 2, 4, 7, 9, or 10.   14   This narrows down the search for x considerably. For example, x must be a multiple of 12. We must have x ≥ ⌈√ N ⌉ = 92825, and the least such multiple of 12 is 92832. This value has residues  2, 5, 3  modulo  5, 7, 11  respectively, so it fails  14  with respect to modulus 11. Increasing x by 12 changes the residue mod 5 by 2, mod 7 by 5, and mod 11 by 1; so it is easy to see that the first value of x ≥ 92825 that satisfies all of the conditions in  14  is x = 92880. Now 928802 − N = 10233601, and the pencil-and-paper method for square root tells us that 10233601 = 31992 is indeed a perfect square. Therefore we have found the desired solution x = 92880, y = 3199, and the factorization is  8616460799 =  x − y  x + y  = 89681 · 96079.  This value of N is interesting because the English economist and logician W. S. Jevons introduced it as follows in a well-known book: “Given any two num- bers, we may by a simple and infallible process obtain their product, but it is quite another matter when a large number is given to determine its factors. Can the reader say what two numbers multiplied together will produce the number 8,616,460,799? I think it unlikely that anyone but myself will ever know.” [The Principles of Science  1874 , Chapter 7.] We have just seen, however, that Fer- mat could have factored N in less than 10 minutes, on the back of an envelope! Jevons’s point about the difficulty of factoring versus multiplying is well taken, but only if we form the product of numbers that aren’t so close to each other. In place of the moduli considered in  14 , we can use any powers of distinct primes. For example, if we had used 25 in place of 5, we would find that the only permissible values of x mod 25 are 0, 5, 7, 10, 15, 18, and 20. This gives more information than  14 . In general, we will get more information modulo p2 than we do modulo p, for odd primes p, whenever x2 − N ≡ 0  modulo p  has a solution x. Individual primes p and q are, however, preferable to moduli like p2 unless p is quite small, because we tend to get even more information mod pq.   4.5.4  FACTORING INTO PRIMES  389  √  The modular method just used is called a sieve procedure, since we can imagine passing all integers through a “sieve” for which only those values with x mod 3 = 0 come out, then sifting these numbers through another sieve that allows only numbers with x mod 5 = 0, 2, or 3 to pass, etc. Each sieve by itself will remove about half of the remaining values  see exercise 6 ; and when we sieve with respect to moduli that are relatively prime in pairs, each sieve is independent of the others because of the Chinese remainder theorem  Theorem 4.3.2C . So if we sieve with respect to, say, 30 different primes, only about one value in every 230 will need to be examined to see if x2 − N is a perfect square y2. Algorithm D  Factoring with sieves . Given an odd number N, this algorithm determines the largest factor of N less than or equal to N. The procedure uses moduli m1, m2, . . . , mr that are relatively prime to each other in pairs and relatively prime to N. We assume that we have access to r sieve tables S[i, j] for 0 ≤ j < mi, 1 ≤ i ≤ r, where  S[i, j] =j2 − N ≡ y2  modulo mi  has a solution y.  D1. [Initialize.] Set x ← ⌈√ N ⌉, and set ki ←  −x  mod mi for 1 ≤ i ≤ r.  Throughout this algorithm the index variables k1, k2, . . . , kr will be set so that ki =  −x  mod mi.   x2 − N⌋ or to ⌈√  D2. [Sieve.] If S[i, ki] = 1 for 1 ≤ i ≤ r, go to step D4. D3. [Step x.] Set x ← x+1, and set ki ←  ki −1  mod mi for 1 ≤ i ≤ r. Return D4. [Test x2 − N.] Set y ← ⌊√ If y2 = x2 − N, then  x − y  is the desired factor, and the algorithm terminates. Otherwise return to step D3. There are several ways to make this procedure run fast. For example, we have seen that if N mod 3 = 2, then x must be a multiple of 3; we can set x = 3x′, and use a different sieve corresponding to x′, increasing the speed threefold. If N mod 9 = 1, 4, or 7, then x must be congruent respectively to ±1, ±2, or ±4  modulo 9 ; so we run two sieves  one for x′ and one for x′′, where x = 9x′ + a and x = 9x′′ − a  to increase the speed by a factor of 4 1 If N mod 4 = 3, 2. then x mod 4 is known and the speed is increased by an additional factor of 4; in the other case, when N mod 4 = 1, x must be odd so the speed may be doubled. Another way to double the speed of the algorithm  at the expense of storage space  is to combine pairs of moduli, using mr−k mk in place of mk for 1 ≤ k < 1  An even more important method of speeding up Algorithm D is to use the Boolean operations found on most binary computers. Let us assume, for example, that MIX is a binary computer with 30 bits per word. The tables S[i, ki] can be kept in memory with one bit per entry; thus 30 values can be stored in a single word. The operation AND, which replaces the kth bit of the accumulator by zero if the kth bit of a specified word in memory is zero, for 1 ≤ k ≤ 30, can be used to process 30 values of x at once! For convenience,  x2 − N ⌉.  to step D2.  2 r.   390  ARITHMETIC  4.5.4  we can make several copies of the tables S[i, j] so that the table entries for mi involve lcm mi, 30  bits; then the sieve tables for each modulus fill an integral number of words. Under these assumptions, 30 executions of the main loop in Algorithm D are equivalent to code of the following form: D2 LD1 K1  LDA S1,1 DEC1 1 J1NN *+2 INC1 M1 ST1 K1 LD1 K2 AND S2,1 DEC1 1 J1NN *+2 INC1 M2 ST1 K2 LD1 K3 ··· ST1 Kr INCX 30 JAZ D2  rI1 ← k′1. rA ← S′[1, rI1]. rI1 ← rI1 − 1. If rI1 < 0, set rI1 ← rI1 + lcm m1, 30 . k′1 ← rI1. rI1 ← k′2. rA ← rA & S′[2, rI1]. rI1 ← rI1 − 1. If rI1 < 0, set rI1 ← rI1 + lcm m2, 30 . k′2 ← rI1. rI1 ← k′3.  m3 through mr are like m2  k′r ← rI1. x ← x + 30. Repeat if all sieved out.  if r = 11, this The number of cycles for 30 iterations is essentially 2 + 8r; means three cycles are being used on each iteration, just as in Algorithm C, and Algorithm C involves y = 1  2 v − u  more iterations.  If the table entries for mi do not come out to be an integral number of words, further shifting of the table entries would be necessary on each iteration in order to align the bits properly. This would add quite a lot of coding to the main loop and it would probably make the program too slow to compete with Algorithm C unless v u ≤ 100  see exercise 7 .  Sieve procedures can be applied to a variety of other problems, not neces- sarily having much to do with arithmetic. A survey of these techniques has been prepared by Marvin C. Wunderlich, JACM 14  1967 , 10–19.  F. W. Lawrence proposed the construction of special sieve machines for factorization in the 19th century [Quart. J. of Pure and Applied Math. 28  1896 , 285–311], and E. O. Carissan completed such a device with 14 moduli [See Shallit, Williams, and Morain, Math. Intelligencer 17, 3  1995 , in 1919. 41–47, for the interesting story of how Carissan’s long-lost sieve was rediscovered and preserved for posterity.] D. H. Lehmer and his associates constructed and used many different sieve devices during the period 1926–1989, beginning with bicycle chains and later using photoelectric cells and other kinds of technology; see, for example, AMM 40  1933 , 401–406. Lehmer’s electronic delay-line sieve, which began operating in 1965, processed one million numbers per second. By 1995 it was possible to construct a machine that sieved 6144 million numbers per second, performing 256 iterations of steps D2 and D3 in about 5.2 nanoseconds [see Lukes, Patterson, and Williams, Nieuw Archief voor Wiskunde  4  13  1995 ,   4.5.4  FACTORING INTO PRIMES  391  113–139]. Another way to factor with sieves was described by D. H. and Emma Lehmer in Math. Comp. 28  1974 , 625–635. Primality testing. None of the algorithms we have discussed so far is an efficient way to determine that a large number n is prime. Fortunately, there are other methods available for settling this question; efficient techniques have been devised by É. Lucas and others, notably D. H. Lehmer [see Bull. Amer. Math. Soc. 33  1927 , 327–340].  According to Fermat’s theorem  Theorem 1.2.4F , we have  xp−1 mod p = 1  whenever p is prime and x is not a multiple of p. Furthermore, there are efficient ways to calculate xn−1 mod n, requiring only O log n  operations of multiplication mod n.  We shall study them in Section 4.6.3 below.  Therefore we can often determine that n is not prime when this relationship fails.  For example, Fermat once verified that the numbers 21 + 1, 22 + 1, 24 + 1, 28 + 1, and 216 + 1 are prime. In a letter to Mersenne written in 1640, Fermat conjectured that 22n + 1 is always prime, but said he was unable to determine definitely whether the number 4294967297 = 232 + 1 is prime or not. Neither Fermat nor Mersenne ever resolved this problem, although they could have done it as follows: The number 3232 mod  232 + 1  can be computed by doing 32 operations of squaring modulo 232 + 1, and the answer is 3029026160; therefore  by Fermat’s own theorem, which he discovered in the same year 1640!  the number 232 + 1 is not prime. This argument gives us absolutely no idea what the factors are, but it answers Fermat’s question.  Fermat’s theorem is a powerful test for showing nonprimality of a given number. When n is not prime, it is always possible to find a value of x < n such that xn−1 mod n ̸= 1; experience shows that, in fact, such a value can almost always be found very quickly. There are some rare values of n for which xn−1 mod n is frequently equal to unity, but then n has a factor less than 3√ n; see exercise 9. The same method can be extended to prove that a large prime number n really is prime, by using the following idea: If there is a number x for which the order of x modulo n is equal to n − 1, then n is prime.  The order of x modulo n is the smallest positive integer k such that xk mod n = 1; see Section 3.2.1.2.  For this condition implies that the numbers x1 mod n, . . . , xn−1 mod n are distinct and relatively prime to n, so they must be the numbers 1, 2, . . . , n−1 in some order; thus n has no proper divisors. If n is prime, such a number x  called a primitive root of n  will always exist; see exercise 3.2.1.2–16. In fact, primitive roots are rather numerous. There are φ n − 1  of them, and this is quite a substantial number, since n φ n − 1  = O log log n . It is unnecessary to calculate xk mod n for all k ≤ n − 1 to determine if the order of x is n − 1 or not. The order of x will be n − 1 if and only if i  xn−1 mod n = 1; ii  x n−1  p mod n ̸= 1 for all primes p that divide n − 1.   392  ARITHMETIC  4.5.4  For xs mod n = 1 if and only if s is a multiple of the order of x modulo n. If the two conditions hold, and if k is the order of x modulo n, we therefore know that k is a divisor of n − 1, but not a divisor of  n − 1  p for any prime factor p of n− 1; the only remaining possibility is k = n− 1. This completes the proof that conditions  i  and  ii  suffice to establish the primality of n.  Exercise 10 shows that we can in fact use different values of x for each of the primes p, and n will still be prime. We may restrict consideration to prime values of x, since the order of uv modulo n divides the least common multiple of the orders of u and v by exercise 3.2.1.2–15. Conditions  i  and  ii  can be tested efficiently by using the rapid methods for evaluating powers of numbers discussed in Section 4.6.3. But it is necessary to know the prime factors of n−1, so we have an interesting situation in which the factorization of n depends on that of n− 1. An example. The study of a reasonably typical large factorization will help to fix the ideas we have discussed so far. Let us try to find the prime factors of 2214 + 1, a 65-digit number. The factorization can be initiated with a bit of clairvoyance if we notice that  2214 + 1 =  2107 − 254 + 1  2107 + 254 + 1 ;   15  this is a special case of the factorization 4x4 + 1 =  2x2 + 2x + 1  2x2 − 2x + 1 , which Euler communicated to Goldbach in 1742 [P. H. Fuss, Correspondance Math. et Physique 1  1843 , 145]. The problem now boils down to examining each of the 33-digit factors in  15 . A computer program readily discovers that 2107−254 +1 = 5·857· n0, where  16  is a 29-digit number having no prime factors less than 1000. A multiple-precision calculation using Algorithm 4.6.3A shows that  n0 = 37866809061660057264219253397  3n0−1 mod n0 = 1,  so we suspect that n0 is prime. It is certainly out of the question to prove that n0 is prime by trying the 10 million million or so potential divisors, but the method discussed above gives a feasible test for primality: Our next goal is to factor n0 − 1. With little difficulty, our computer will tell us that  n0 − 1 = 2 · 2 · 19 · 107 · 353 · n1,  n1 = 13191270754108226049301.  Here 3n1−1 mod n1 ̸= 1, so n1 is not prime; by continuing Algorithm A or Algo- rithm B we obtain another factor,  n1 = 91813 · n2,  n2 = 143675413657196977.  This time 3n2−1 mod n2 = 1, so we will try to prove that n2 is prime. Casting out factors < 1000 yields n2−1 = 2·2·2·2·3·3·547· n3, where n3 = 1824032775457. Since 3n3−1 mod n3 ̸= 1, we know that n3 cannot be prime, and Algorithm A finds that n3 = 1103 · n4, where n4 = 1653701519. The number n4 behaves like a prime  that is, 3n4−1 mod n4 = 1 , so we calculate  n4 − 1 = 2 · 7 · 19 · 23 · 137 · 1973.   4.5.4  FACTORING INTO PRIMES  393  Good; this is our first complete factorization. We are now ready to backtrack to the previous subproblem, proving that n4 is prime. Using the procedure suggested by exercise 10, we compute the following values:  x n4−1  p mod n4  xn4−1 mod n4  x 2 2 2 2 2 2 3 5 7  p 2 7 19 23 137 1973 2 2 2  1  1 1  766408626 332952683 1154237810 373782186 490790919  1653701518   1   1   1   1   1   1   1   1  1   17    Here “ 1 ” means a result of 1 that needn’t be computed since it can be deduced from previous calculations.  Thus n4 is prime, and n2 − 1 has been completely factored. A similar calculation shows that n2 is prime, and this complete factorization of n0 − 1 finally shows, after still another calculation like  17 , that n0 is prime. The last three lines of  17  represent a search for an integer x that satisfies x n4−1  2 ̸≡ xn4−1 ≡ 1  modulo n4 . If n4 is prime, we have only a 50-50 chance of success, so the case p = 2 is typically the hardest one to verify. We could streamline this part of the calculation by using the law of quadratic reciprocity  see exercise 23 , which tells us for example that 5 q−1  2 ≡ 1  modulo q  whenever q is a prime congruent to ±1  modulo 5 . Merely calculating n4 mod 5 would have told us right away that x = 5 could not possibly help in showing that n4 is prime. In fact, however, the result of exercise 26 implies that the case p = 2 doesn’t really need to be considered at all when testing n for primality, unless n − 1 is divisible by a high power of 2, so we could have dispensed with the last three lines of  17  entirely.  The next quantity to be factored is the other half of  15 , namely  n5 = 2107 + 254 + 1.  Since 3n5−1 mod n5 ̸= 1, we know that n5 is not prime, and Algorithm B shows that n5 = 843589 · n6, where n6 = 192343993140277293096491917. Unfortu- nately, 3n6−1 mod n6 ̸= 1, so we are left with a 27-digit nonprime. Continuing Algorithm B might well exhaust our patience  not our budget — we’re using idle time on a weekend rather than “prime time” . But the sieve method of Algorithm D will be able to crack n6 into its two factors, n6 = 8174912477117 · 23528569104401.   It turns out that Algorithm B would also have succeeded, after 6,432,966 iter- ations.  The factors of n6 could not have been discovered by Algorithm A in a reasonable length of time.   394  ARITHMETIC  4.5.4  Now the computation is complete: 2214 + 1 has the prime factorization  5 · 857 · 843589 · 8174912477117 · 23528569104401 · n0,  where n0 is the 29-digit prime in  16 . A certain amount of good fortune entered into these calculations, for if we had not started with the known factorization  15  it is quite probable that we would first have cast out the small factors, reducing n to n6n0. This 55-digit number would have been much more difficult to factor — Algorithm D would be useless and Algorithm B would have to work overtime because of the high precision necessary.  Dozens of further numerical examples can be found in an article by John  Brillhart and J. L. Selfridge, Math. Comp. 21  1967 , 87–96. Improved primality tests. The procedure just illustrated requires the com- plete factorization of n−1 before we can prove that n is prime, so it will bog down for large n. Another technique, which uses the factorization of n + 1 instead, is described in exercise 15; if n− 1 turns out to be too hard, n + 1 might be easier. Significant improvements are available for dealing with large n. For example, it is not difficult to prove a stronger converse of Fermat’s theorem that requires only a partial factorization of n − 1. Exercise 26 shows that we could have avoided most of the calculations in  17 ; the three conditions 2n4−1 mod n4 = gcd 2 n4−1  23 − 1, n4  = gcd 2 n4−1  1973 − 1, n4  = 1 are sufficient by them- selves to prove that n4 is prime. Brillhart, Lehmer, and Selfridge have in fact developed a method that works when the numbers n − 1 and n + 1 have been only partially factored [Math. Comp. 29  1975 , 620–647, Corollary 11]: Suppose n − 1 = f −r− and n + 1 = f +r+, where we know the complete factorizations of f − and f +, and we also know that all factors of r− and r+ are ≥ b. If the  productb3f −f + max f −, f +  is greater than 2n, a small amount of additional  In practice, when n has no small prime factors and 3n−1 mod n = 1, further  computation, described in their paper, will determine whether or not n is prime. Therefore numbers of up to 35 digits can usually be tested for primality in a fraction of a second, simply by casting out all prime factors < 30030 from n ± 1 [see J. L. Selfridge and M. C. Wunderlich, Congressus Numerantium 12  1974 , 109–120]. The partial factorization of other quantities like n2 ± n + 1 and n2 + 1 can be used to improve this method still further [see H. C. Williams and J. S. Judd, Math. Comp. 30  1976 , 157–172, 867–886].  calculations almost always show that n is prime. One of the rare exceptions in 7 228 − 9  = 2341 · 16381. On the other hand,  the author’s experience is n = 1 some nonprime values of n are definitely bad news for the primality test we have discussed, because it might happen that xn−1 mod n = 1 for all x relatively prime to n  see exercise 9 . The smallest such number is n = 3·11·17 = 561; here λ n  = lcm 2, 10, 16  = 80 in the notation of Eq. 3.2.1.2– 9 , so x80 mod 561 = 1 = x560 mod 561 whenever x is relatively prime to 561. Our procedure would repeatedly fail to show that such an n is nonprime, until we had stumbled across one of its divisors. To improve the method, we need a quick way to determine the nonprimality of nonprime n, even in such pathological cases.   4.5.4  FACTORING INTO PRIMES  395  The following surprisingly simple procedure is guaranteed to do the job with  high probability: Algorithm P  Probabilistic primality test . Given an odd integer n, this algo- rithm attempts to decide whether or not n is prime. By repeating the algorithm several times, as explained in the remarks below, it is possible to be extremely confident about the primality of n, in a precise sense, yet the primality will not be rigorously proved. Let n = 1 + 2kq, where q is odd. P1. [Generate x.] Let x be a random integer in the range 1 < x < n. P2. [Exponentiate.] Set j ← 0 and y ← xq mod n.  As in our previous primality  test, xq mod n should be calculated in O log q  steps; see Section 4.6.3.   P3. [Done?]  Now y = x2jq mod n.  If y = n−1, or if y = 1 and j = 0, terminate the algorithm and say “n is probably prime.” If y = 1 and j > 0, go to P5. P4. [Increase j.] Increase j by 1. If j < k, set y ← y2 mod n and return to P3. P5. [Not prime.] Terminate and say “n is definitely not prime.”  The idea underlying Algorithm P is that if xq mod n ̸= 1 and n = 1 + 2kq is  prime, the sequence of values  xq mod n,  x2kq mod n  x2q mod n,  x4q mod n,  . . . ,  will end with 1, and the value just preceding the first appearance of 1 will be  n − 1. The only solutions to y2 ≡ 1  modulo p  are y ≡ ±1, when p is prime, since  y − 1  y + 1  must be a multiple of p.  Exercise 22 proves the basic fact that Algorithm P will be wrong at most 1 4 of the time, for all n. Actually it will rarely fail at all, for most n; but the crucial point is that the probability of failure is bounded regardless of the value of n.  Suppose we invoke Algorithm P repeatedly, choosing x independently and at random whenever we get to step P1. If the algorithm ever reports that n is nonprime, we can be sure this is so. But if the algorithm reports 25 times in a row that n is “probably prime,” we can say that n is “almost surely prime.” For the probability is less than  1 4 25 that such a 25-times-in-a-row procedure gives the wrong information about its input. This is less than one chance in a quadrillion; even if we tested a billion different numbers with such a procedure, the expected 1000000. It’s much more likely that our number of mistakes would be less than computer has dropped a bit in its calculations, due to hardware malfunctions or cosmic radiations, than that Algorithm P has repeatedly guessed wrong!  Probabilistic algorithms like this lead us to question our traditional stan- dards of reliability. Do we really need to have a rigorous proof of primality? For people unwilling to abandon traditional notions of proof, Gary L. Miller has demonstrated  in slightly weaker form  that if a certain well-known conjecture in number theory called the Extended Riemann Hypothesis can be proved, then either n is prime or there is an x < 2 ln n 2 such that Algorithm P will discover [See J. Comp. System Sci. 13  1976 , 300–317. The the nonprimality of n. constant 2 in this upper bound is due to Eric Bach, Math. Comp. 55  1990 , 355–380. See Chapter 8 of Algorithmic Number Theory 1 by E. Bach and J. O.  1   396  ARITHMETIC  4.5.4  Shallit  MIT Press, 1996 , for an exposition of various generalizations of the Riemann hypothesis.] Thus, we would have a rigorous way to test primality in O log n 5 elementary operations, as opposed to a probabilistic method whose running time is O log n 3, if the Extended Riemann Hypothesis were proved. But one might well ask whether any purported proof of that hypothesis will ever be as reliable as repeated application of Algorithm P on random x’s.  A probabilistic test for primality was proposed in 1974 by R. Solovay and V. Strassen, who devised the interesting but more complicated test described [See SICOMP 6  1977 , 84–85; 7  1978 , 118.] Algorithm P in exercise 23 b . is a simplified version of a procedure due to M. O. Rabin, based in part on ideas of Gary L. Miller [see Algorithms and Complexity  1976 , 35–36], and independently discovered by J. L. Selfridge. B. Arazi [Comp. J. 37  1994 , 219– 222] has observed that Algorithm P can be speeded up significantly for large n by using Montgomery’s fast method for remainders  exercise 4.3.1–41 .   z + a n ≡ zn + a   modulo zr − 1 and n   r lg n.  See exercise 3.2.2–11 a .   A completely rigorous and deterministic way to test for primality in poly- nomial time was finally discovered in 2002 by Manindra Agrawal, Neeraj Kayal, and Nitin Saxena, who proved the following result: Theorem A. Let r be an integer such that n ⊥ r and the order of n modulo r exceeds  lg n 2. Then n is prime if and only if the polynomial congruence holds for 0 ≤ z ≤ √ An excellent exposition of this theorem has been prepared by Andrew Gran- ville [Bull. Amer. Math. Soc. 42  2005 , 3–38], who presents an elementary proof that it yields a primality test with running time Ω log n 6 and O log n 11. He also explains a subsequent improvement due to H. Lenstra and C. Pomerance, who showed that the running time can be reduced to O log n 6+ϵ if the poly- nomial zr − 1 is replaced by a more general family of polynomials. And he discusses refinements by P. Berrizbeitia, Q. Cheng, P. Mihăilescu, R. Avanzi, and D. Bernstein, leading to a probabilistic algorithm by which a proof of primality can almost surely be found in O log n 4+ϵ steps whenever n is prime. Factoring via continued fractions. The factorization procedures we have discussed so far will often balk at numbers of 30 digits or more, and another idea is needed if we are to go much further. Fortunately there is such an idea; in fact, there were two ideas, due respectively to A. M. Legendre and M. Kraitchik, which led D. H. Lehmer and R. E. Powers to devise a new technique many years ago [Bull. Amer. Math. Soc. 37  1931 , 770–776]. However, the method was not used at the time because it was comparatively unsuitable for desk calculators. This negative judgment prevailed until the late 1960s, when John Brillhart found that the Lehmer–Powers approach deserved to be resurrected, since it was quite well suited to computer programming. In fact, he and Michael A. Morrison later developed it into the champion of all multiprecision factorization methods that were known in the 1970s. Their program would handle typical 25-digit numbers in about 30 seconds, and 40-digit numbers in about 50 minutes, on an IBM   4.5.4  FACTORING INTO PRIMES  397  360 91 computer [see Math. Comp. 29  1975 , 183–205]. The method had its first triumphant success in 1970, discovering that 2128+1 = 59649589127497217· 5704689200685129054721.  The basic idea is to search for numbers x and y such that  x ̸= y,  x + y ̸= N.  x2 ≡ y2  modulo N ,  0 < x, y < N,   18  Fermat’s method imposes the stronger requirement x2 − y2 = N, but actually the congruence  18  is enough to split N into factors: It implies that N is a divisor of x2 − y2 =  x − y  x + y , yet N divides neither x − y nor x + y; hence gcd N, x − y  and gcd N, x + y  are proper factors of N that can be found by the efficient methods of Section 4.5.2. One way to discover solutions of  18  is to look for values of x such that x2 ≡ a  modulo N , for small values of a. As we will see, it is often a simple matter to piece together solutions of this congruence to obtain solutions of  18 . Now if x2 = a+ kN d2 for some k and d, with small a, the fraction x d is a good approximation to kN ; conversely, if x d is an especially good approximation kN, the difference x2 − kN d2 will be small. This observation suggests to looking at the continued fraction expansion of kN, since we have seen in Eq. 4.5.3– 12  and exercise 4.5.3–42 that continued fractions yield good rational approximations.  Continued fractions for quadratic irrationalities have many pleasant prop- erties, which are proved in exercise 4.5.3–12. The algorithm below makes use of these properties to derive solutions to the congruence  √  √  √  x2 ≡  −1 e0pe1  1 pe2  2 . . . pem  m  modulo N .   19  Here we use a fixed set of small primes p1 = 2, p2 = 3, . . . , up to pm; only primes p such that either p = 2 or  kN  p−1  2 mod p ≤ 1 should appear in this list, since other primes will never be factors of the numbers generated by the algorithm  see exercise 14 . If  x1, e01, e11, . . . , em1 , . . . ,  xr, e0r, e1r, . . . , emr  are solutions of  19  such that the vector sum   e01, e11, . . . , em1  + ··· +  e0r, e1r, . . . , emr  =  2e′  0, 2e′  1, . . . , 2e′ m    20   y = −1 e′  is even in each component, then x =  x1 . . . xr  mod N,  1 . . . pe′ 0pe′  1  m  m   mod N   21  yields a solution to  18 , except for the possibility that x ≡ ±y. Condition  20  essentially says that the vectors are linearly dependent modulo 2, so we must have a solution to  20  if we have found at least m + 2 solutions to  19 . Algorithm E  Factoring via continued fractions . Given a positive integer N and a positive integer k such that kN is not a perfect square, this algorithm attempts to discover solutions to the congruence  19  for a given sequence of primes p1, . . . , pm, by analyzing the convergents of the continued fraction for √ kN.  Another algorithm, which uses the outputs to discover factors of N, is the subject of exercise 12.    398  ARITHMETIC  4.5.4  Table 1  AN ILLUSTRATION OF ALGORITHM E  N = 197209, k = 1, m = 3, p1 = 2, p2 = 3, p3 = 5  After E1: After E4: After E4: After E4: After E4: After E4: After E4: After E4: After E4: After E4: After E4: After E4:  U 876 882 857 751 852 681 863 883 821 877 875 490  V 73 145 37 720 143 215 656 33 136 405 24 477  A 12 6 23 1 5 3 1 26 6 2 36 1  P 5329 5329 32418 159316 191734 131941 193139 127871 165232 133218 37250 93755  S 1 0 1 0 1 0 1 0 1 0 1 0  T — 29 37 1 143 43 41 11 17 1 1 53  Output  1593162 ≡ +24 · 32 · 51  1332182 ≡ +20 · 34 · 51 372502 ≡ −23 · 31 · 50  E1. [Initialize.] Set D ← kN, R ← ⌊√  D⌋, R′ ← 2R, U′ ← R′, V ← D − R2, V ′ ← 1, A ← ⌊R′ V ⌋, U ← R′− R′ mod V  , P ′ ← R, P ←  AR+1  mod N, S ← 1.  This algorithm follows the general procedure of exercise 4.5.3–12, kN. The variables U, U′, V, V ′, finding the continued fraction expansion of P, P ′, A, and S represent, respectively, what that exercise calls ⌊√ D⌋+ Un, ⌊√ D⌋ + Un−1, Vn, Vn−1, pn mod N, pn−1 mod N, An, and n mod 2, where n is initially 1. We will always have 0 < V ≤ U ≤ R′, so the highest precision is needed only for P and P ′.   √  U′ ← U, U ← R′ −  U mod V  , S ← 1 − S.  E2. [Advance U, V, S.] Set T ← V, V ← A U′ − U  + V ′, V ′ ← T, A ← ⌊U V ⌋,  E3. [Factor V.] Now we have P 2−kN Q2 =  −1 SV, for some Q relatively prime to P, by exercise 4.5.3–12 c . Set  e0, e1, . . . , em  ←  S, 0, . . . , 0 , T ← V.  Now do the following, for 1 ≤ j ≤ m: If T mod pj = 0, set T ← T  pj and ej ← ej + 1, and repeat this process until T mod pj ̸= 0.  E4. [Solution?] If T = 1, output the values  P, e0, e1, . . . , em , which comprise a solution to  19 .  If enough solutions have been generated, we may terminate the algorithm now.   E5. [Advance P, P ′.] If V ̸= 1, set T ← P, P ←  AP + P ′  mod N, P ′ ← T, and return to step E2. Otherwise the continued fraction process has started to repeat its cycle, except perhaps for S, so the algorithm terminates.  The cycle will usually be so long that this doesn’t happen.  We can illustrate the application of Algorithm E to relatively small numbers by considering the case N = 197209, k = 1, m = 3, p1 = 2, p2 = 3, p3 = 5. The computation begins as shown in Table 1.  Continuing the computation gives 25 outputs in the first 100 iterations; in other words, the algorithm is finding solutions quite rapidly. But some of the solutions are trivial. For example, if the computation above were continued 14   4.5.4 399 more times, we would obtain the output 1971972 ≡ 24 · 32 · 50, which is of no interest since 197197 ≡ −12. The first two solutions above are already enough to complete the factorization: We have found that  FACTORING INTO PRIMES   159316 · 133218 2 ≡  22 · 33 · 51 2  modulo 197209 ;  thus  18  holds with x =  159316 · 133218  mod 197209 = 126308, y = 540. By Euclid’s algorithm, gcd 126308−540, 197209  = 199; hence we obtain the pretty factorization  197209 = 199 · 991.  √ 2    log 2  √ N log pm  We can get some understanding of why Algorithm E factors large numbers so successfully by considering a heuristic analysis of its running time, following unpublished ideas that R. Schroeppel communicated to the author in 1975. Let us assume for convenience that k = 1. The number of outputs needed to produce a factorization of N will be roughly proportional to the number m of small primes being cast out. Each execution of step E3 takes about order m log N units of time, so the total running time will be roughly proportional to m2 log N P, where P is the probability of a successful output per iteration. If we make the conservative assumption that V is randomly distributed between 0 and 2 N, the probability P is  2 N whose prime factors are all in the set {p1, . . . , pm}. Exercise 29 gives a lower bound for P, from which we conclude that the running time is at most of order  √ N  −1 times the number of integers < 2  √  √  N m2 log N mr r!  .   22   , √  where r =  If we let ln m be approximately 1 2 assuming that pm = O m log m , so formula  22  reduces to  ln N ln ln N, we have r ≈ ln N  ln ln N −1, exp2 ln N  ln ln N  + O log N 1 2 log log N −1 2 log log log N . 2 ln ln N ln N goes to 0 as N → ∞.  Stating this another way, the running time of Algorithm E is expected to be at most N ϵ N  under reasonably plausible assumptions, where the exponent ϵ N  ≈  When N is in a practical range, we should of course be careful not to take such asymptotic estimates too seriously. For example, if N = 1050 we have N 1 α =  lg N α when α ≈ 4.75, and the same relation holds for α ≈ 8.42 when N = 10200. The function N ϵ N  has an order of growth that is sort of a cross between N 1 α and  lg N α; but all three of these forms are about the same, unless N is intolerably large. Extensive computational experiments by M. C. Wunderlich have shown that a well-tuned version of Algorithm E performs much better than our estimate would indicate [see Lecture Notes in Math. 751   1979 , 328–342]; although 2 ln ln N ln N ≈ .41 when N = 1050, he obtained  running times of about N 0.15 while factoring thousands of numbers in the range 1013 ≤ N ≤ 1042.  Algorithm E begins its attempt to factorize N by essentially replacing N by kN, and this is a rather curious way to proceed  if not downright stupid .   400  ARITHMETIC  4.5.4  “Excuse me, do you mind if I multiply your number by 3 before I try to factor it?” Nevertheless, it turns out to be a good idea, since certain values of k will make the V numbers potentially divisible by more small primes, hence they will be more likely to factor completely in step E3. On the other hand, a large value of k will make the V numbers larger, hence they will be less likely to factor completely; we want to balance these tendencies by choosing k wisely. Consider, for example, the divisibility of V by powers of 5. We have P 2− kN Q2 =  −1 SV in step E3, so if 5 divides V we have P 2 ≡ kN Q2  modulo 5 . In this congruence Q cannot be a multiple of 5, since it is relatively prime to P, so we may write  P Q 2 ≡ kN  modulo 5 . If we assume that P and Q are random relatively prime integers, so that the 24 possible pairs  P mod 5, Q mod 5  ̸=  0, 0  are equally likely, the probability that 5 divides V is therefore 4 24, 0, 0, or 8 24 according as kN mod 5 is 0, 1, 2, 3, or 4. Similarly the probability that 25 divides V is 0, 40 600 respectively, unless kN is a multiple of 25. In general, given an odd prime p with  kN  p−1  2 mod p = 1, we find that V is a multiple of pe comes to 2p  p2 − 1 . This analysis, suggested by R. Schroeppel, suggests that the best choice of k is the value that maximizes f pj, kN  log pj − 1  with probability 2 pe−1 p + 1 ; and the average number of times p divides V  600, 0, 0, 40  m  24, 8   23   2 log k,  j=1  √  where f is the function defined in exercise 28, since this is essentially the expected value of ln   N T   when we reach step E4.  Best results will be obtained with Algorithm E when both k and m are well chosen. The proper choice of m can only be made by experimental testing, since the asymptotic analysis we have made is too crude to give sufficiently precise information, and since a variety of refinements to the algorithm tend to have unpredictable effects. For example, we can make an important improvement by comparing step E3 with Algorithm A: The factoring of V can stop whenever we find T mod pj ̸= 0 and ⌊T  pj⌋ ≤ pj, since T will then be either 1 or prime. If T is m+pm−1 in such a case , we can still a prime greater than pm  it will be at most p2 output  P, e0, . . . , em, T , since a complete factorization has been obtained. The second phase of the algorithm will use only those outputs whose prime T’s have occurred at least twice. This modification gives the effect of a much longer list of primes, without increasing the factorization time. Wunderlich’s experiments indicate that m ≈ 150 works well in the presence of this refinement, when N is in the neighborhood of 1040.  Since step E3 is by far the most time-consuming part of the algorithm, Morrison, Brillhart, and Schroeppel have suggested several ways to abort this step when success becomes improbable:  a  Whenever T changes to a single- precision value, continue only if ⌊T  pj⌋ > pj and 3T−1 mod T ̸= 1.  b  Give  c  Cast out factors up if T is still > p2 only up to p5, say, for batches of 100 or so consecutive V ’s; continue the factorization later, but only on the V from each batch that has produced the  m after casting out factors < 1  10 pm.   4.5.4  FACTORING INTO PRIMES  401  i  4 pf5  1 pf2  3 pf4  2 pf3  2 pf3  1 pf2  3 pf4  smallest residual T.  Before casting out the factors up to p5, it is wise to calculate V mod pf1 4 pf5 5 fit in single precision, but large enough to make V mod pfi+1 = 0 unlikely. One single-precision remainder will therefore characterize the value of V modulo five small primes.   5 , where the f’s are small enough to make pf1  For estimates of the cycle length in the output of Algorithm E, see H. C.  Williams, Math. Comp. 36  1981 , 593–601. *A theoretical upper bound. From the standpoint of computational complex- ity, we would like to know if there is any method of factorization whose expected running time can be proved to be O N ϵ N  , where ϵ N  → 0 as N → ∞. We have seen that Algorithm E probably has such behavior, but it seems hopeless to find a rigorous proof, because continued fractions are not sufficiently well disciplined. The first proof that a good factorization algorithm exists in this sense was discovered by John Dixon in 1978; Dixon showed, in fact, that it suffices to consider a simplified version of Algorithm E, in which the continued fraction apparatus is removed but the basic idea of  18  remains.  V = pe1  1 . . . pem  Dixon’s method [Math. Comp. 36  1981 , 255–260] is simply this, assuming that N is known to have at least two distinct prime factors, and that N is not divisible by the first m primes p1, p2, . . . , pm: Choose a random integer X in the range 0 < X < N, and let V = X2 mod N. If V = 0, the number gcd X, N  is a proper factor of N. Otherwise cast out all of the small prime factors of V as in step E3; in other words, express V in the form m T,   24  where T is not divisible by any of the first m primes. If T = 1, the algorithm proceeds as in step E4 to output  X, e1, . . . , em , which represents a solution to  19  with e0 = 0. This process continues with new random values of X until there are sufficiently many outputs to discover a factor of N by the method of exercise 12.  In order to analyze this algorithm, we want to find bounds on  a  the probability that a random X will yield an output, and  b  the probability that a large number of outputs will be required before a factor is found. Let P m, N  be the probability  a , namely the probability that T = 1 when X is chosen at random. After M values of X have been tried, we will obtain M P m, N  outputs, on the average; and the number of outputs has a binomial distribution, so the standard deviation is less than the square root of the mean. The probability  b  is fairly easy to deal with, since exercise 13 proves that the algorithm needs more than m + k outputs with probability ≤ 2−k. Exercise 30 proves that P m, N  ≥ mr  r! N  when r = 2⌊log N  2 log pm ⌋, so we can estimate the running time almost as we did in  22  but with the √ quantity 2  N replaced by N. This time we choose  r =2 ln N  ln ln N + θ,  where θ ≤ 1 and r is even, and we choose m so that  r = ln N  ln pm + O 1  log log N ;   4.5.4  402  ARITHMETIC  this means  2  ln N ln ln N ln N ln ln N = exp−  2 √  =  mr r! N  ln pm = ln m = ln π pm  = ln pm − ln ln pm + O 1  log pm   2 ln ln N + O 1 ,  − θ  − θ + 1 2  ln ln N + O log log log N ,  2 ln N ln ln N + O r log log log N .  We will choose M so that M mr  r! N  ≥ 4m; thus the expected number of outputs M P m, N  will be at least 4m. The running time of the algorithm is of order M m log N, plus O m3  steps for exercise 12; it turns out that O m3  is less than M m log N, which is  exp8 ln N  ln ln N  + O log N 1 2 log log N −1 2 log log log N .  √  ϵ N  = c ln ln N ln N and c is any constant greater than  The probability that this method fails to find a factor is negligibly small, since the probability is at most e−m 2 that fewer than 2m outputs are obtained  see exercise 31 , while the probability is at most 2−m that no factors are found from the first 2m outputs, and m ≫ ln N. We have proved the following slight strengthening of Dixon’s original theorem: Theorem D. There is an algorithm whose running time is O N ϵ N  , where 8, that finds a nontrivial factor of N with probability 1− O 1 N , whenever N has at least two distinct prime divisors. Other approaches. Another factorization technique was suggested by John M. Pollard [Proc. Cambridge Phil. Soc. 76  1974 , 521–528], who gave a practical way to discover prime factors p of N when p − 1 has no large prime factors. The latter algorithm  see exercise 19  is probably the first thing to try after Algorithms A and B have run too long on a large N.  A survey paper by R. K. Guy, written in collaboration with J. H. Conway, Congressus Numerantium 16  1976 , 49–89, gave a unique perspective on the de- velopments up till that time. Guy stated, “I shall be surprised if anyone regularly factors numbers of size 1080 without special form during the present century”; and he was indeed destined to be surprised many times during the next 20 years. Tremendous advances in factorization techniques for large numbers were made during the 1980s, beginning with Carl Pomerance’s quadratic sieve method 673], which heuristically is expected to take about exp 2 + ϵ  ln p  ln ln p  of 1981 [see Lecture Notes in Comp. Sci. 209  1985 , 169–182]. Then Hendrik Lenstra devised the elliptic curve method [Annals of Math.  2  126  1987 , 649– multiplications to find a prime factor p. This is asymptotically the square root of the running time in our estimate for Algorithm E when p ≈ √ N, and it becomes even better when N has relatively small prime factors. An excellent exposition of this method has been given by Joseph H. Silverman and John Tate in Rational Points on Elliptic Curves  New York: Springer, 1992 , Chapter 4.   4.5.4  FACTORING INTO PRIMES  403  John Pollard came back in 1988 with another new technique, which has become known as the number field sieve; see Lecture Notes in Math. 1554  1993  for a series of papers about this method, which is the current champion for factoring extremely large integers. Its running time is predicted to be of order  25  as N → ∞. The crossover point at which a well-tuned version of the number field sieve begins to beat a well-tuned version of the quadratic sieve appears to be at N ≈ 10112, according to A. K. Lenstra.  exp 64 9 + ϵ 1 3 ln N 1 3 ln ln N 2 3  Details of the new methods are beyond the scope of this book, but we can get an idea of their effectiveness by noting some of the early success stories in which unfactored Fermat numbers of the form 22k+1 were cracked. For example, the factorization  2512 + 1 = 2424833 ·  7455602825647884208337395736200454918783366342657 · p99  was found by the number field sieve, after four months of computation that occu- pied otherwise idle time on about 700 workstations [Lenstra, Lenstra, Manasse, and Pollard, Math. Comp. 61  1993 , 319–349; 64  1995 , 1357]; here p99 denotes a 99-digit prime number. The next Fermat number has twice as many digits, but it yielded to the elliptic curve method on October 20, 1995:  21024 + 1 = 45592577 · 6487031809 ·  4659775785220018543264560743076778192897 · p252.  [Richard Brent, Math. Comp. 68  1999 , 429–451.] In fact, Brent had already used the elliptic curve method to resolve the next case as early as 1988:  22048 + 1 = 319489 · 974849 ·  167988556341760475137 · 3560841906445833920513 · p564;  by a stroke of good luck, all but one of the prime factors was < 1022, so the elliptic curve method was a winner.  What about 24096 + 1? At present, that number seems completely out of reach. It has five factors < 1016, but the unfactored residual has 1187 decimal digits. The next case, 28192 + 1, has four known factors < 1027 [Crandall and Fagin, Math. Comp. 62  1994 , 321; Brent, Crandall, Dilcher, and van Halewyn, Math. Comp. 69  2000 , 1297–1304] and a huge unfactored residual. Secret factors. Worldwide interest in the problem of factorization increased dramatically in 1977, when R. L. Rivest, A. Shamir, and L. Adleman discovered a way to encode messages that can apparently be decoded only by knowing the factors of a large number N, even though the method of encoding is known to everyone. Since a significant number of the world’s greatest mathematicians have been unable to find efficient methods of factoring, this scheme [CACM 21  1978 , 120–126] almost certainly provides a secure way to protect confidential data and communications in computer networks.   404  ARITHMETIC  4.5.4  Let us imagine a small electronic device called an RSA box that has two large prime numbers p and q stored in its memory. We will assume that p−1 and q−1 are not divisible by 3. The RSA box is connected somehow to a computer, and it has told the computer the product N = pq; however, no human being will be able to discover the values of p and q except by factoring N, since the RSA box is cleverly designed to self-destruct if anybody tries to tamper with it. In other words, it will erase its memory if it is jostled or if it is subjected to any radiation that could change or read out the data stored inside. Furthermore, the RSA box is sufficiently reliable that it never needs to be maintained; we simply would discard it and buy another, if an emergency arose or if it wore out. The prime factors p and q were generated by the RSA box itself, using some scheme based on truly random phenomena in nature like cosmic rays. The important point is that nobody knows p or q, not even a person or organization that owns or has access to this RSA box; there is no point in bribing or blackmailing anyone or holding anybody hostage in order to discover N’s factors.  To send a secret message to the owner of an RSA box whose product number is N, you break the message up into a sequence of numbers  x1, . . . , xk , where each xi lies in the range 0 ≤ xi < N; then you transmit the numbers   x3  1 mod N, . . . , x3  k mod N .  3√  N about 1  computed a number d < N such that 3d ≡ 1 modulo  p − 1  q − 1 ; it can  The RSA box, knowing p and q, can decode the message, because it has pre-  now compute each secret component  x3 i mod N d mod N = xi in a reasonable amount of time, using the method of Section 4.6.3. Naturally the RSA box keeps this magic number d to itself; in fact, the RSA box might choose to remember only d instead of p and q, because its only duties after having computed N are to protect its secrets and to take cube roots mod N. Such an encoding scheme is ineffective if x <  N, since x3 mod N = x3 and the cube root will easily be found. The logarithmic law of leading digits in Section 4.2.4 implies that the leading place x1 of a k-place message  x1, . . . , xk  will be less than 3√ 3 of the time, so this is a problem that needs to be resolved. Exercise 32 presents one way to avoid the difficulty. The security of the RSA encoding scheme relies on the fact that nobody has been able to discover how to take cube roots quickly mod N without knowing N’s factors. It seems likely that no such method will be found, but we cannot be absolutely sure. So far all that can be said for certain is that all of the ordinary ways to discover cube roots will fail. For example, there is essentially no point in trying to compute the number d as a function of N; the reason is that if d is known, or in fact if any number m of reasonable size is known such that xm mod N = 1 holds for a significant number of x’s, then we can find the factors of N in a few more steps  see exercise 34 . Thus, any method of attack based explicitly or implicitly on finding such an m can be no better than factoring.  Some precautions are necessary, however.  If the same message is sent to three different people on a computer network, a person who knows x3 modulo N1, N2, and N3 could reconstruct x3 mod N1N2N3 = x3 by the Chinese remainder   4.5.4  FACTORING INTO PRIMES  405  theorem, so x would no longer be a secret. In fact, even if a “time-stamped” message  2⌈lg ti⌉x + ti 3 mod Ni is sent to seven different people, with known or guessable ti, the value of x can be deduced  see exercise 44 . Therefore some cryptographers have recommended encoding with the exponent 216 + 1 = 65537 instead of 3; this exponent is prime, and the computation of x65537 mod N takes only about 8.5 times as long as the computation of x3 mod N. [CCITT Recommendations Blue Book  Geneva: International Telecommunication Union, 1989 , Fascicle VIII.8, Recommendation X.509, Annex C, pages 74–76.] The original proposal of Rivest, Shamir, and Adleman was to encode x by xa mod N where a is any exponent prime to φ N , not just a = 3; in practice, however, we prefer an exponent for which encoding is faster than decoding. The numbers p and q shouldn’t merely be “random” primes in order to make the RSA scheme effective. We have mentioned that p− 1 and q − 1 should not be divisible by 3, since we want to ensure that unique cube roots exist modulo N. Another condition is that p − 1 should have at least one very large prime factor, and so should q−1; otherwise N can be factored using the algorithm of exercise 19. In fact, that algorithm essentially relies on finding a fairly small number m with the property that xm mod N is frequently equal to 1, and we have just seen that such an m is dangerous. When p−1 and q−1 have large prime factors p1 and q1, the theory in exercise 34 implies that m is either a multiple of p1q1  hence m will be hard to discover  or the probability that xm ≡ 1 will be less than 1 p1q1  hence xm mod N will almost never be 1 . Besides this condition, we don’t want p and q to be close to each other, lest Algorithm D succeed in discovering them; in fact, we don’t want the ratio p q to be near a simple fraction, otherwise Lehman’s generalization of Algorithm C could find them.  The following procedure for generating p and q is almost surely unbreakable: Start with a truly random number p0 between, say, 1080 and 1081. Search for the first prime number p1 greater than p0; this will require testing about 2 ln p0 ≈ 90 odd numbers, and it will be sufficient to have p1 a “probable prime” 1 with probability > 1−2−100 after 50 trials of Algorithm P. Then choose another truly random number p2 between, say, 1039 and 1040. Search for the first prime number p of the form kp1 + 1 where k ≥ p2, k is even, and k ≡ p1  modulo 3 . 3 ln p1p2 ≈ 90 numbers before a prime p is found. This will require testing about 1 The prime p will be about 120 digits long; a similar construction can be used to find a prime q about 130 digits long. For extra security, it is probably advisable to check that neither p+1 nor q+1 consists entirely of rather small prime factors  see exercise 20 . The product N = pq, whose order of magnitude will be about 10250, now meets all of our requirements, and it is inconceivable at this time that such an N could be factored.  For example, suppose we knew a method that could factor a 250-digit number N in N 0.1 microseconds. This amounts to 1025 microseconds, and there are only 31,556,952,000,000 µs per year, so we would need more than 3 × 1011 years of CPU time to complete the factorization. Even if a government agency purchased 10 billion computers and set them all to working on this problem, it would take more than 31 years before one of them would crack N into factors;   406  ARITHMETIC  4.5.4  meanwhile the fact that the government had purchased so many specialized machines would leak out, and people would start using 300-digit N’s. Since the encoding method x →→ x3 mod N is known to everyone, there are additional advantages besides the fact that the code can be cracked only by the RSA box. Such “public key” systems were first published by W. Diffie and M. E. Hellman in IEEE Trans. IT-22  1976 , 644–654. As an example of what can be done when the encoding method is public knowledge, suppose Alice wants to communicate with Bob securely via electronic mail, signing her letter so that Bob can be sure nobody else has forged it. Let EA M  be the encoding function for messages M sent to Alice, let DA M  be the decoding done by Alice’s RSA box, and let EB M , DB M  be the corresponding encoding and decoding functions for Bob’s RSA box. Then Alice can send a signed message by affixing her name and the date to some confidential message, then transmitting EB message, his RSA box converts it to DA M , and he knows EA so he can compute M = EA  DA M  to Bob, using her machine to compute DA M . When Bob gets this DA M . This should convince him that the message did indeed come from Alice; nobody else could have sent the message DA M . Well, Bob himself DA M  to indicate that it is for Bob’s eyes only.  now knows DA M , so he could impersonate Alice by passing EX Xavier. To defeat any such attempted forgery, the content of M should clearly  We might ask, how do Alice and Bob know each other’s encoding functions EA and EB? It wouldn’t do simply to have them stored in a public file, since some Charlie could tamper with that file, substituting an N that he has computed by himself; Charlie could then surreptitiously intercept and decode a private message before Alice or Bob would discover that something is amiss. The solution is to keep the product numbers NA and NB in a special public directory that has its own RSA box and its own widely publicized product number ND. When Alice wants to know how to communicate with Bob, she asks the directory for Bob’s product number; the directory computer sends her a signed message giving the value of NB. Nobody can forge such a message, so it must be legitimate.  An interesting alternative to the RSA scheme has been proposed by Michael Rabin [M.I.T. Lab. for Comp. Sci., report TR-212  1979 ], who suggests encod- ing by the function x2 mod N instead of x3 mod N. In this case the decoding mechanism, which we can call a SQRT box, returns four different messages; the reason is that four different numbers have the same square modulo N, namely x, −x, f x mod N, and  −f x  mod N, where  f =  pq−1 − q p−1  mod N. If we agree in advance that x is even, or that x < 1 2 N, then the ambiguity drops to two messages, presumably only one of which makes any sense. The ambiguity can in fact be eliminated entirely, as shown in exercise 35. Rabin’s scheme has the important property that it is provably as difficult to find square roots mod N as to find the factorization N = pq; for by taking the square root of x2 mod N when x is chosen at random, we have a 50-50 chance of finding a value y such that x2 ≡ y2 and x ̸≡ ±y, after which gcd x − y, N  = p or q. However, the   4.5.4  FACTORING INTO PRIMES  407  system has a fatal flaw that does not seem to be present in the RSA scheme  see exercise 33 : Anyone with access to a SQRT box can easily determine the factors of its N. This not only permits cheating by dishonest employees, or threats of extortion, it also allows people to reveal their p and q, after which they might claim that their “signature” on some transmitted document was a forgery. Thus it is clear that the goal of secure communication leads to subtle problems quite different from those we usually face in the design and analysis of algorithms.  Historical note: It was revealed in 1997 that Clifford Cocks had considered the encoding of messages by the transformation xpq mod pq already in 1973, but his work was kept secret. The largest known primes. We have discussed several computational methods elsewhere in this book that require the use of large prime numbers, and the techniques just described can be used to discover primes of up to, say, 25 digits or fewer, with relative ease. Table 2 shows the ten largest primes that are less than the word size of typical computers.  Some other useful primes appear in the answers to exercises 3.2.1.2–22 and 4.6.4–57.   Actually much larger primes of special forms are known, and it is occasionally important to find primes that are as large as possible. Let us therefore conclude this section by investigating the interesting manner in which the largest explicitly known primes have been discovered. Such primes are of the form 2n − 1, for various special values of n, and so they are especially suited to certain applications of binary computers. A number of the form 2n−1 cannot be prime unless n is prime, since 2uv −1 is divisible by 2u−1. In 1644, Marin Mersenne astonished his contemporaries by stating, in essence, that the numbers 2p − 1 are prime for p = 2, 3, 5, 7, 13, 17, 19, 31, 67, 127, 257, and for no other p less than 257.  This statement appeared in connection with a discussion of perfect numbers in the preface to his Cogitata Physico-Mathematica. Curiously, he also made the following remark: “To tell if a given number of 15 or 20 digits is prime or not, all time would not suffice for the test, whatever use is made of what is already known.”  Mersenne, who had corresponded frequently with Fermat, Descartes, and others about similar topics in previous years, gave no proof of his assertions, and for over 200 years nobody knew whether he was correct. Euler showed that 231 − 1 is prime in 1772, after having tried unsuccessfully to prove this in previous years. About 100 years later, É. Lucas discovered that 2127 − 1 is prime, but 267 − 1 was questionable; therefore Mersenne might not be completely accurate. Then I. M. Pervushin proved in 1883 that 261 − 1 is prime [see Istoriko-Mat. Issledovani⁀ıa 6  1953 , 559], and this touched off speculation that Mersenne had only made a copying error, writing 67 for 61. Eventually other errors in Mersenne’s statement were discovered; R. E. Powers [AMM 18  1911 , 195] showed that 289 − 1 is prime, as had been conjectured by some earlier writers, and three years later he proved that 2107 −1 also is prime. M. Kraitchik found in 1922 that 2257 −1 is not prime [see his Recherches sur la Théorie des Nombres  Paris: 1924 , 21]; computational errors may have crept in to his calculations, but his conclusion has turned out to be correct.   408  ARITHMETIC  4.5.4  Table 2  USEFUL PRIME NUMBERS  N 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 259 260 263 264 106 107 108 109 1010 1011 1012 1016  a1 19 15 1 5 1 3 9 3 15 3 39 5 39 57 3 35 1 5 9 41 31 5 25 45 7 87 21 11 57 17 55 21 115 59 55 93 25 59 17 9 11 63 33 23 11 63  a2 49 17 9 11 19 5 19 17 21 17 49 27 79 89 33 41 19 17 25 77 49 17 31 87 19 167 31 17 67 117 69 57 127 65 99 107 165 83 21 27 29 71 57 53 39 83  a3 51 39 13 17 27 17 21 27 27 33 61 45 111 95 43 83 61 65 49 113 61 23 45 107 67 195 55 33 117 119 81 63 147 89 225 173 259 95 39 29 41 107 71 57 41 113  a4 55 57 31 23 31 27 55 33 37 63 85 87 115 119 63 101 69 99 79 131 69 65 69 131 91 203 63 53 175 129 93 77 279 93 427 179 301 179 41 57 59 117 119 93 63 149  a5 61 87 49 33 45 59 61 57 61 75 91 101 135 125 73 105 85 107 105 143 79 117 123 153 135 213 73 65 255 143 121 167 297 147 517 257 375 189 47 63 69 203 149 129 101 183  a6 75 89 61 35 57 69 69 87 69 77 115 107 187 143 75 107 99 135 285 165 121 137 141 185 165 285 75 143 267 149 133 197 339 165 607 279 387 257 69 69 153 239 167 149 123 191  a7 81 99 63 41 67 129 105 105 135 89 141 111 199 165 93 135 105 153 301 185 141 159 199 191 219 293 91 161 291 287 139 237 435 189 649 369 391 279 83 71 161 243 183 167 137 329  a8 115 113 85 65 69 143 111 113 147 95 159 117 219 183 99 153 151 185 303 207 247 173 201 227 231 299 111 165 309 327 159 287 541 233 687 395 409 323 93 93 173 249 213 171 143 357  a9 121 117 91 75 85 153 121 117 157 117 165 125 231 213 121 161 159 209 321 227 309 189 351 231 241 389 133 215 319 359 193 305 619 243 861 399 457 353 117 99 179 261 219 179 153 359  a10 135 123 99 93 87 185 129 123 159 167 183 135 235 273 133 173 171 267 355 281 325 233 375 257 301 437 139 227 369 377 229 311 649 257 871 453 471 363 137 111 213 267 231 231 233 369  The ten largest primes less than N are N − a1, . . . , N − a10.   4.5.4  409 Numbers of the form 2p − 1 are now called Mersenne numbers, and it is  FACTORING INTO PRIMES  known that Mersenne primes are obtained for p equal to  2, 3, 5, 7, 13, 17, 19, 31, 61, 89, 107, 127, 521, 607, 1279, 2203, 2281, 3217, 4253, 4423, 9689, 9941, 11213, 19937, 21701, 23209, 44497, 86243,  110503, 132049, 216091, 756839, 859433, 1257787, 1398269, 2976221, 3021377, 6972593, 13466917, 20996011, 24036583, 25964951, 30402457,  32582657, 37156667, 42643801, 43112609, 57885161, . . . .   26  The first entries above 100000 were found by David Slowinski and associates while testing new supercomputers [see J. Recreational Math. 11  1979 , 258– 261]; he found 756839, 859433, and 1257787 in collaboration with Paul Gage during the 1990s. But the remaining exponents, beginning with 1398269, were found respectively by Joël Armengaud, Gordon Spence, Roland Clarkson, Nayan Hajratwala, Michael Cameron, Michael Shafer, Josh Findley, Martin Nowak, Curtis Cooper Steven Boone, Hans-Michael Elvenich, Odd Magnar Strindmo, and Edson Smith using off-the-shelf personal computers, most recently in 2013. They used a program by George Woltman, who launched the Great Internet Mersenne Prime Search project  GIMPS  in 1996, with Internet administrative software contributed subsequently by Scott Kurowski. Notice that the prime 8191 = 213 − 1 does not occur in  26 ; Mersenne had stated that 28191 − 1 is prime, and others had conjectured that any Mersenne prime could perhaps be used in the exponent.  The search for large primes has not been systematic, because people have generally tried to set a hard-to-beat world record instead of spending time with smaller exponents. For example, 2132049 − 1 was proved prime in 1983, and 2216091 − 1 in 1984, but the case 2110503 − 1 was not discovered until 1988. Therefore one or more unknown Mersenne primes less than 257885161 − 1 might still exist. According to Woltman, all exponents < 25,000,000 were checked as of March 1, 2008; his volunteers are systematically filling the remaining gaps. Since 257885161 − 1 has more than 17 million decimal digits, it is clear that some special techniques have been used to prove that such numbers are prime. An efficient way to test the primality of a given Mersenne number 2p − 1 was first devised by É. Lucas [Amer. J. Math. 1  1878 , 184–239, 289–321, especially page 316] and improved by D. H. Lehmer [Annals of Math.  2  31  1930 , 419– 448, especially page 443]. The Lucas–Lehmer test, which is a special case of the method now used for testing the primality of n when the factors of n + 1 are known, is the following: Theorem L. Let q be an odd prime, and define the sequence ⟨Ln⟩ by the rule  27   n − 2  mod  2q − 1 .  Ln+1 =  L2  L0 = 4,  Then 2q − 1 is prime if and only if Lq−2 = 0.  For example, 23 − 1 is prime since L1 =  42 − 2  mod 7 = 0. This test is particularly well suited to binary computers, since calculation mod  2q − 1  is so convenient; see Section 4.3.2. Exercise 4.3.2–14 explains how to save time when q is extremely large.   410  ARITHMETIC  Proof. We will prove Theorem L using only very simple principles of number theory, by investigating several features of recurring sequences that are of inde- pendent interest. Consider the sequences ⟨Un⟩ and ⟨Vn⟩ defined by  U0 = 0, V0 = 2,  U1 = 1, V1 = 4,  Un+1 = 4Un − Un−1; Vn+1 = 4Vn − Vn−1.  The following equations are readily proved by induction: √  Vn = Un+1 − Un−1;  Un = 2 +  √ √ 3  n −  2 − √ √ 3  n +  2 − Um+n = UmUn+1 − Um−1Un.  Vn =  2 +  3  n   3  n;  12;  4.5.4   28    29   30   31   32   Let us now prove an auxiliary result, when p is prime and e ≥ 1 :  if  then  Un ≡ 0  modulo pe   Unp ≡ 0  modulo pe+1 .   33  This follows from the more general considerations of exercise 3.2.2–11, but a direct proof can be given for sequence  28 . Assume that Un = bpe, Un+1 = a. By  32  and  28 , U2n = bpe 2a − 4bpe  ≡ 2aUn  modulo pe+1 , while we have n ≡ a2. Similarly, U3n = U2n+1Un − U2nUn−1 ≡ 3a2Un and U2n+1 = U 2 U3n+1 = U2n+1Un+1 − U2nUn ≡ a3. In general,  n+1 − U 2  Ukn ≡ kak−1Un  and  Ukn+1 ≡ ak  modulo pe+1 ,  From formulas  30  and  31  we can obtain other expressions for Un and Vn,  so  33  follows if we take k = p. expanding  2 ± √ Un = Now if we set n = p, where p is an odd prime, and if we use the fact thatp   n Vn =  3  n by the binomial theorem:   n  2n−2k−13k,  2n−2k+13k.   is  2k + 1       34   2k  k  k  a multiple of p except when k = 0 or k = p, we find that  Vp ≡ 4  Up ≡ 3 p−1  2,   35  If p ̸= 3, Fermat’s theorem tells us that 3p−1 ≡ 1; hence  3 p−1  2 − 1 ×  3 p−1  2 + 1  ≡ 0, and 3 p−1  2 ≡ ±1. When Up ≡ −1, we have Up+1 = 4Up−Up−1 = 4Up+Vp−Up+1 ≡ −Up+1; hence Up+1 mod p = 0. When Up ≡ +1, we have Up−1 = 4Up− Up+1 = 4Up− Vp− Up−1 ≡ −Up−1; hence Up−1 mod p = 0. We have proved that, for all primes p, there is an integer ϵ p  such that   modulo p .   36  Now if N is any positive integer, and if m = m N  is the smallest positive  Up+ϵ p  mod p = 0,  ϵ p  ≤ 1.  k  integer such that Um N  mod N = 0, we have  Un mod N = 0   37   This number m N  is called the rank of apparition of N in the sequence.  To prove  37 , observe that the sequence Um, Um+1, Um+2, . . . is congruent  n is a multiple of m N .  if and only if   4.5.4  FACTORING INTO PRIMES  411   modulo N  to aU0, aU1, aU2, . . . , where a = Um+1 mod N is relatively prime to N because gcd Un, Un+1  = 1.  With these preliminaries out of the way, we are ready to prove Theorem L.  By  27  and induction,  Ln = V2n mod  2q − 1 .   38  Furthermore, the identity 2Un+1 = 4Un + Vn implies that gcd Un, Vn  ≤ 2, since any common factor of Un and Vn must divide Un and 2Un+1, while Un ⊥ Un+1. So Un and Vn have no odd factor in common, and if Lq−2 = 0 we must have  U2q−1 = U2q−2V2q−2 ≡ 0  modulo 2q − 1 , U2q−2 ̸≡ 0  modulo 2q − 1 .   pr + ϵr ,  t = lcmpe1−1  Now if m = m 2q −1  is the rank of apparition of 2q −1, it must be a divisor of 2q−1 but not of 2q−2; thus m = 2q−1. We will prove that n = 2q − 1 must therefore be prime: Let the factorization of n be pe1 r . All primes pj are greater than 3, since n is odd and congruent to  −1 q − 1 = −2  modulo 3 . From  33 ,  36 , and  37  we know that Ut ≡ 0  modulo 2q − 1 , where  1 . . . per   p1 + ϵ1 , . . . , per−1  j  j  r  1   pj + 1  5 pj  =   6  j=1 pej−1   pj + ϵj ; we have n0 ≤r  r and each ϵj is ±1. It follows that t is a multiple of m = 2q−1. Let n0 = j=1 pej−1 5 rn. Also, because pj + ϵj is even, t ≤ n0 2r−1, since a factor of two is lost each time the least common multiple of two even numbers is taken. Combining these results, we have m ≤ t ≤ 2  3 5 rm < 3m; hence r ≤ 2 and t = m or t = 2m, a power of 2. Therefore e1 = 1, er = 1, and if n is not prime we must have n = 2q − 1 =  2k + 1  2l − 1  where 2k + 1 and 2l − 1 are prime. The latter factorization is obviously impossible when q is odd, so n is prime. Conversely, suppose that n = 2q − 1 is prime; we must show that V2q−2 ≡ 0  modulo n . For this purpose it suffices to prove that V2q−1 ≡ −2  modulo n , since V2q−1 =  V2q−2 2 − 2. Now  5 rn < 4  3  V2q−1 =   √  2 +  = 2−n  k  √  √  6   2n+1 +  √  n + 1 √ 2 − 2 n+1−2k√   n   n + 1  6   2n+1 6 2k = 2 1−n  2  n    2k  k  2k  =  +  2k  2k − 1  Since n is an odd prime, the binomial coefficient   n + 1    2k  3k.  is divisible by n except when 2k = 0 and 2k = n + 1; hence 2 n−1  2 V2q−1 ≡ 1 + 3 n+1  2  modulo n .  Here 2 ≡  2 q+1  2 2, so 2 n−1  2 ≡  2 q+1  2  n−1  ≡ 1 by Fermat’s theorem. Finally, by a simple case of the law of quadratic reciprocity  see exercise 23 , 3 n−1  2 ≡ −1, since n mod 3 = 1 and n mod 4 = 3. This means V2q−1 ≡ −2, so we must have V2q−2 ≡ 0 as desired.   412  ARITHMETIC  4.5.4  An anonymous author whose works are now preserved in Italian libraries had discovered by 1460 that 217 − 1 and 219 − 1 are prime. Ever since then, the world’s largest explicitly known prime numbers have almost always been Mersenne primes. But the situation might change, since Mersenne primes are getting harder to find, and since exercise 27 presents an efficient test for primes [See E. Picutti, Historia Math. 16  1989 , 123–136; Hugh C. of other forms. Williams, Édouard Lucas and Primality Testing  1998 , Chapter 2.]  EXERCISES 1. [10] If the sequence d0, d1, d2, . . . of trial divisors in Algorithm A contains a number that is not prime, why will it never appear in the output? 2. [15] If it is known that the input N to Algorithm A is equal to 3 or more, could step A2 be eliminated? 3. [M20] Show that there is a number P with the following property: If 1000 ≤ n ≤ 1000000, then n is prime if and only if gcd n, P   = 1. 4. [M29] In the notation of exercise 3.1–7 and Section 1.2.11.3, prove that the average value of the least n such that Xn = X ℓ n −1 lies between 1.5Q m  − 0.5 and 1.625Q m  − 0.5. 5. [21] Use Fermat’s method  Algorithm D  to find the factors of 11111 by hand, when the moduli are 3, 5, 7, 8, and 11. 6. [M24] If p is an odd prime and if N is not a multiple of p, prove that the number of integers x such that 0 ≤ x < p and x2 − N ≡ y2  modulo p  has a solution y is equal to  p ± 1  2. 7. [25] Discuss the problems of programming the sieve of Algorithm D on a binary computer when the table entries for modulus mi do not exactly fill an integral number of memory words.   cid:120  8. [23]  The sieve of Eratosthenes, 3rd century B.C.  The following procedure evi-  dently discovers all odd prime numbers less than a given integer N, since it removes all the nonprime numbers: Start with all the odd numbers between 1 and N; then successively strike out the multiples p2 k, pk pk + 2 , pk pk + 4 , . . . , of the kth prime pk, for k = 2, 3, 4, . . . , until reaching a prime pk with p2  Show how to adapt the procedure just described into an algorithm that is directly  suited to efficient computer calculation, using no multiplication. 9. [M25] Let n be an odd number, n ≥ 3. Show that if the number λ n  of Theorem 3.2.1.2B is a divisor of n−1 but not equal to n−1, then n must have the form p1p2 . . . pt where the p’s are distinct primes and t ≥ 3.   cid:120  10. [M26]  John Selfridge.  Prove that if, for each prime divisor p of n − 1, there is  cid:120  12. [M28] Design an algorithm that uses the outputs of Algorithm E to find a proper  a number xp such that x 11. [M20] What outputs does Algorithm E give when N = 197209, k = 5, m = 1? [Hint:  5 · 197209 = 992 +   1, 495, 2, 495, 1, 1984  .]  p mod n = 1, then n is prime.  mod n ̸= 1 but xn−1  factor of N, if Algorithm E has produced enough outputs to deduce a solution of  18 . 13. [HM25]  J. D. Dixon.  Prove that whenever the algorithm of exercise 12 is pre- sented with a solution  x, e0, . . . , em  whose exponents are linearly dependent modulo 2  k > N.   n−1  p  √  p   4.5.4  FACTORING INTO PRIMES  413  on the exponents of previous solutions, the probability is 21−d that a factorization will not be found, when N has d distinct prime factors and x is chosen at random.  cid:120  15. [M34]  Lucas and Lehmer.  Let P and Q be relatively prime integers, and let 14. [M20] Prove that the number T in step E3 of Algorithm E will never be a multiple of an odd prime p for which  kN  p−1  2 mod p > 1. U0 = 0, U1 = 1, Un+1 = P Un − QUn−1 for n ≥ 1. Prove that if N is a positive integer relatively prime to 2P 2 − 8Q, and if UN+1 mod N = 0, while U N+1  p mod N ̸= 0 for each prime p dividing N + 1, then N is prime.  This gives a test for primality when the factors of N + 1 are known instead of the factors of N − 1. We can evaluate Um in O log m  steps as in exercise 4.6.3–26.  [Hint: See the proof of Theorem L.] 16. [M50] Are there infinitely many Mersenne primes? 17. [M25]  V. R. Pratt.  A complete proof of primality by the converse of Fermat’s theorem takes the form of a tree whose nodes have the form  q, x , where q and x are positive integers satisfying the following arithmetic conditions:  i  If  q1, x1 , . . . ,  qt, xt  are the children of  q, x  then q = q1 . . . qt+1. [In particular, if  q, x  is childless, then q = 2.]  ii  If  r, y  is a child of  q, x , then x q−1  r mod q ̸= 1.  iii  For each node  q, x , we have xq−1 mod q = 1. From these conditions it follows that q is prime and x is a primitive root modulo q, for all nodes  q, x . [For example, the tree  pt ?  demonstrates that 1009 is prime.] Prove that such a tree with root  q, x  has at most f q  nodes, where f is a rather slowly growing function.   cid:120  18. [HM23] Give a heuristic proof of  7 , analogous to the text’s derivation of  6 .  cid:120  19. [M25]  J. M. Pollard.  Show how to compute a number M that is divisible by What is the approximate probability that pt−1 ≤ √ all odd primes p such that p − 1 is a divisor of some given number D. [Hint: Consider numbers of the form an − 1.] Such an M is useful in factorization, for by computing gcd M, N  we may discover a factor of N. Extend this idea to an efficient method that has high probability of discovering prime factors p of a given large number N, when all prime power factors of p − 1 are less than 103 except for at most one prime factor less than 105. [For example, the second-largest prime dividing  15  would be detected by this method, since it is 1 + 24 · 52 · 67 · 107 · 199 · 41231.] 20. [M40] Consider exercise 19 with p + 1 replacing p − 1. 21. [M49]  R. K. Guy.  Let m p  be the number of iterations required by Algorithm B √ to cast out the prime factor p. Is m p  = O    cid:120  22. [M30]  M. O. Rabin.  Let pn be the probability that Algorithm P guesses wrong,  is defined to be −1, 0, or +1 for all integers p ≥ 0 23. [M35] The Jacobi symbol p and all odd integers q > 1 by the rules p  ≡ p q−1  2  modulo q  when q is prime;  when n is an odd integer ≥ 3. Show that pn < 1  p log p   as p → ∞?  4 for all n.  q  q   1009, 11    2, 1   2, 1    2, 1    2, 1    2, 1    7, 3    3, 2    3, 2   3, 2    2, 1    3, 2    2, 1    2, 1    2, 1    q  p  414  q  qt  q1  ARITHMETIC  Thus it generalizes the Legendre symbol of exercise 1.2.4–47.   when q is the product q1 . . . qt of t primes  not necessarily distinct . p  . . . p  = p  satisfies the following relationships, hence it can be computed effi- a  Prove thatp  =  −1  q2  = p p′  = 0; 1 ; ciently: 0 ; 2  =  −1  p−1  q−1  4q p  if both p and q are odd. , has  to the evaluation of q reciprocity relation reducing the evaluation of p   = p mod q  −1  8; pp′   = 1; p  [The latter law, which is a  p  q  q  q  q  q  q  q  q  q  q  4.5.4  been proved in exercise 1.2.4–47 d  when both p and q are prime, so you may assume its validity in that special case.] b   Solovay and Strassen.  Prove that if n is odd but not prime, the number of integers x such that 1 ≤ x < n and 0 ̸=   x n  ≡ x n−1  2  modulo n  is at most 2 φ n .  Thus, the following testing procedure correctly determines whether or 1 not a given n is prime, with probability at least 1 2 for all fixed n: “Generate x at random with 1 ≤ x < n. If 0 ̸=   x n  ≡ x n−1  2  modulo n , say that n is probably prime, otherwise say that n is definitely not prime.”  c   L. Monier.  Prove that if n and x are numbers for which Algorithm P concludes that “n is probably prime”, then 0 ̸=   x [Hence Algo- rithm P is always superior to the test in  b .]   cid:120  24. [M25]  L. Adleman.  When n > 1 and x > 1 are integers, n odd, let us say that  n  ≡ x n−1  2  modulo n .  n “passes the x test of Algorithm P” if either x mod n = 0 or if steps P2–P5 lead to the conclusion that n is probably prime. Prove that, for any N, there exists a set of positive integers x1, . . . , xm ≤ N with m ≤ ⌊lg N⌋ such that a positive odd integer in the range 1 < n ≤ N is prime if and only if it passes the x test of Algorithm P for x = x1 mod n, . . . , x = xm mod n. Thus, the probabilistic test for primality can in principle be converted into an efficient test that leaves nothing to chance.  You need not show how to compute the xj efficiently; just prove that they exist.  25. [HM41]  B. Riemann.  Prove that  π x  + π x1 2   + π x1 3   + ··· =  2  3  e t+iτ  ln x dt  t + iτ  + O 1  ,  where the sum is over all complex σ + iτ such that τ > 0 and ζ σ + iτ  = 0.   cid:120  26. [M25]  H. C. Pocklington, 1914.  Let N = f r + 1, where 0 < r ≤ f + 1. Prove  cid:120  27. [M30] Show that there is a way to test numbers of the form N = 5·2n + 1 for  that N is prime if, for every prime divisor p of f, there is an integer xp such that p mod N = gcd x xN−1  − 1, N  = 1.   N−1  p  −∞  p  primality, using approximately the same number of squarings mod N as the Lucas– Lehmer test for Mersenne primes in Theorem L. [Hint: See the previous exercise.] 28. [M27] Given a prime p and a positive integer d, what is the value of f p, d , the average number of times that p divides A2−dB2  counting multiplicity , when A and B are random integers that are independent except for the condition A ⊥ B? 29. [M25] Prove that the number of positive integers ≤ n whose prime factors are all contained in a given set of primes {p1, . . . , pm} is at least mr r!, when r = ⌊log n log pm⌋ and p1 < ··· < pm. 30. [HM35]  J. D. Dixon and Claus-Peter Schnorr.  Let p1 < ··· < pm be primes that do not divide the odd number N, and let r be an even integer ≤ log N log pm. Prove that the number of integers X in the range 0 ≤ X < N such that X2 mod N =   x  2  dt ln t  − 2 σ   4.5.4  FACTORING INTO PRIMES  415  m is at least mr r!. Hint: Let the prime factorization of N be qf1  d . 1 . . . qfd pe1 1 . . . pem Show that a sequence of exponents  e1, . . . , em  leads to 2d solutions X whenever we m is a quadratic residue modulo qi for 1 ≤ i ≤ d. have e1 + ··· + em ≤ r and pe1 Such exponent sequences can be obtained as ordered pairs  e′1, . . . , e′m; e′′1, . . . , e′′m  where e′1 + ··· + e′m ≤ 1  pe′ 1 . . . pe′  2 r and e′′1 + ··· + e′′m ≤ 1 m   qi−1  2 ≡  pe′′ 1 . . . pe′′  m   qi−1  2  modulo qi   for 1 ≤ i ≤ d.  1 . . . pem  2 r and  m  m  1  1  31. [M20] Use exercise 1.2.10–21 to estimate the probability that Dixon’s factoriza- tion algorithm  as described preceding Theorem D  obtains fewer than 2m outputs.   cid:120  32. [M21] Show how to modify the RSA encoding scheme so that there is no problem  3√ N, in such a way that the length of messages is not substantially  with messages < increased. 33. [M50] Prove or disprove: If a reasonably efficient algorithm exists that has a nonnegligible probability of being able to find x mod N, given a number N = pq whose prime factors satisfy p ≡ q ≡ 2  modulo 3  and given the value of x3 mod N, then there is a reasonably efficient algorithm that has a nonnegligible probability of being able to find the factors of N. [If this could be proved, it would not only show that the cube root problem is as difficult as factoring, it would also show that the RSA scheme has the same fatal flaw as the SQRT scheme.] 34. [M30]  Peter Weinberger.  Suppose N = pq in the RSA scheme, and suppose you know a number m such that xm mod N = 1 for at least 10−12 of all positive integers x. Explain how you could go about factoring N without great difficulty, if m is not too large  say m < N 10 .   cid:120  35. [M25]  H. C. Williams, 1979.  Let N be the product of two primes p and q,  N   = N  , and use this property to design an encoding decoding scheme analogous  where p mod 8 = 3 and q mod 8 = 7. Prove that the Jacobi symbol satisfies   −x N   = −  2x   x to Rabin’s SQRT box but with no ambiguity of messages. 36. [HM24] The asymptotic analysis following  22  is too coarse to give meaningful values unless N is extremely large, since ln ln N is always rather small when N is in a practical range. Carry out a more precise analysis that gives insight into the behavior of  22  for reasonable values of N; also explain how to choose a value of ln m that minimizes  22  except for a factor of size at most exp O log log N  . 37. [M27] Prove that the square root of every positive integer D has a periodic continued fraction of the form unless D is a perfect square, where R = ⌊√ is, ai = an+1−i for 1 ≤ i ≤ n . 38. [25]  Useless primes.  For 0 ≤ d ≤ 9, find Pd, the largest 50-digit prime number that has the maximum possible number of decimal digits equal to d.  First maximize the number of d’s, then find the largest such prime.  39. [40] Many primes p have the property that 2p + 1 is also prime; for example, 5 → 11 → 23 → 47. More generally, say that q is a successor of p if p and q are both prime and q = 2kp + 1 for some k ≥ 0. For example, 2 → 3 → 7 → 29 → 59 → 1889 → 3779 → 7559 → 4058207223809 → 32465657790473 → 4462046030502692971872257 → 95⟨30 omitted digits⟩37 → ··· ; the smallest successor of 95 . . . 37 has 103 digits.  √ D = R +   a1, . . . , an, 2R, a1, . . . , an, 2R, a1, . . . , an, 2R, . . .   ,  D⌋ and  a1, . . . , an  is a palindrome  that  Find the longest chain of successive primes that you can.   k  416  4.5.4  ARITHMETIC   cid:120  40. [M36]  A. Shamir.  Consider an abstract computer that can perform the opera-  tions x + y, x − y, x · y, and ⌊x y⌋ on integers x and y of arbitrary length, in just one unit of time, no matter how large those integers are. The machine stores integers in a random-access memory and it can select different program steps depending on whether or not x = y, given x and y. The purpose of this exercise is to demonstrate that there is an amazingly fast way to factorize numbers on such a computer.  Therefore it will probably be quite difficult to show that factorization is inherently complicated on real machines, although we suspect that it is.  a  Find a way to compute n! in O log n  steps on such a computer, given an integer [Hint: If A is a sufficiently large integer, the binomial coefficients   = m!  m − k ! k! can be computed readily from the value of  A + 1 m.]  value n ≥ 2.  m  b  Show how to compute a number f n  in O log n  steps on such a computer, given an integer value n ≥ 2, having the following properties: f n  = n if n is prime, otherwise f n  is a proper  but not necessarily prime  divisor of n. [Hint: If n ̸= 4, one such function f n  is gcd m n , n , where m n  = min{m  m! mod n = 0}.]  As a consequence of  b , we can completely factor a given number n by doing only O log n 2 arithmetic operations on arbitrarily large integers: Given a partial factor- ization n = n1 . . . nr, each nonprime ni can be replaced by f ni  ·  ni f ni   in   O log ni = O log n  steps, and this refinement can be repeated until all ni are prime.   cid:120  41. [M28]  Lagarias, Miller, and Odlyzko.  The purpose of this exercise is to show  that the number of primes less than N 3 can be calculated by looking only at the primes less than N 2, and thus to evaluate π N 3  in O N 2+ϵ  steps.  Say that an “m-survivor” is a positive integer whose prime factors all exceed m; thus, an m-survivor remains in the sieve of Eratosthenes  exercise 8  after all multiples of primes ≤ m have been sieved out. Let f x, m  be the number of m-survivors that are ≤ x, and let fk x, m  be the number of such survivors that have exactly k prime factors  counting multiplicity . a  Prove that π N 3  = π N  + f N 3, N  − 1 − f2 N 3, N . b  Explain how to compute f2 N 3, N  from the values of π x  for x ≤ N 2. Use your  c  Same question as  b , but evaluate f N 3, N  instead of f2 N 3, N .  method to evaluate f2 1000, 10  by hand. [Hint: Use the identity f x, pj  = f x, pj−1  − f x pj, pj−1 , where pj is the jth prime and p0 = 1.] d  Discuss data structures for the efficient evaluation of the quantities in  b  and  c . 42. [M35]  H. W. Lenstra, Jr.  Given 0 < r < s < N with r ⊥ s and N ⊥ s, show that it is possible to find all divisors of N that are ≡ r  modulo s  by performing O ⌈N s3⌉1 2 log s  well-chosen arithmetic operations on  lg N -bit numbers. [Hint: Apply exercise 4.5.3–49.]   cid:120  43. [M43] Let m = pq be an r-bit Blum integer as in Theorem 3.5P, and let Qm = {y  y = x2 mod m for some x}. Then Qm has  p + 1  q + 1  4 elements, and every element y ∈ Qm has a unique square root x = √ y such that x ∈ Qm. Suppose G y  is an algorithm that correctly guesses √ 2 + ϵ, when y is a random element of Qm. The goal of this exercise is to prove that the problem solved by G is almost as hard as the problem of factoring m. a  Construct an algorithm A G, m, ϵ, y, δ  that uses random numbers and algorithm G to guess whether a given integer y is in Qm, without necessarily computing √ y. Your algorithm should guess correctly with probability ≥ 1 − δ, and its running  y mod 2 with probability ≥ 1   4.5.4  FACTORING INTO PRIMES  417  time T  A  should be at most O ϵ−2 log δ−1 T  G  , assuming that T  G  ≥ r2.  If T  G  < r2, replace T  G  by  T  G  + r2  in this formula.   b  Construct an algorithm F  G, m, ϵ  that finds the factors of m with expected  running time T  F   = O r2 ϵ−6 + ϵ−4 log ϵ−1 T  G   .  √ Hints: For fixed y ∈ Qm, and for 0 ≤ v < m, let τ v = v y mod m and λv = τ v mod 2. Notice that λ −v  + λv = 1 and λ v1 + ··· + vn  =  λv1 + ··· + λvn + ⌊ τ v1 + ··· + τ vm  m⌋  mod 2. Furthermore we have τ  1 2 τ v + mλv ; here 1 2 v  = 1 √ 2 v 2 v  mod m. If ±v ∈ Qm we have τ ±v  = stands for   m+1 v2y; therefore algorithm G gives us a way to guess λv for about half of all v. 44. [M35]  J. Håstad.  Show that it is not difficult to find x when ai0 + ai1x+ ai2x2 + ai3x3 ≡ 0  modulo mi , 0   1027 for 1 ≤ i ≤ 7, if mi ⊥ mj for 1 ≤ i < j ≤ 7.  All variables are integers; all but x are known.  Hint: When L is any nonsingular matrix of real numbers, the algorithm of  cid:120  45. [M41]  J. M. Pollard and Claus-Peter Schnorr.  Find an efficient way to solve the Lenstra, Lenstra, and Lovász [Mathematische Annalen 261  1982 , 515–534] efficiently finds a nonzero integer vector v =  v1, . . . , vn  such that length vL  ≤ √ n2n  det L1 n. congruence  2 − ay x  2 ≡ b   modulo n   1  x2  1 − ay2  2 − ay2  for integers x and y, given integers a, b, and n with ab ⊥ n and n odd, even if the 2  = x2 − ay2, factorization of n is unknown. [Hint: Use the identity  x2 where x = x1x2 − ay1y2 and y = x1y2 + x2y1.] 46. [HM30]  L. Adleman.  Let p be a rather large prime number and let a be a primitive root modulo p; thus, all integers b in the range 1 ≤ b < p can be written b = an mod p, for some unique n with 1 ≤ n < p.  Design an algorithm that almost always finds n, given b, in O pϵ  steps for all ϵ > 0, using ideas similar to those of Dixon’s factoring algorithm. [Hint: Start by building a repertoire of numbers ni such that ani mod p has only small prime factors.] 47. [M50] A certain literary quotation x = x1x2, represented in ASCII code, has the enciphered value  x3  1 mod N, x3  2 mod N  =   8372e6cadf564a9ee347092daefc242058b8044228597e5f2326bbbff1583ea4200d895d9564d39229c79af8 72a72e38bb92852a22679080e269c30690fab0ec19f78e9ef8bae74b600f4ebef42a1dd5a6d806dc70b96de2 bf4a6c7d2ebb51bfd156dd8ac3cb0ae1c1c38d76a3427bcc3f12af7d4d04314c0d8377a0c79db1b1f0cd1702, 2aabcd0f9f1f9fb382313246de168bae6a28d177963a8ebe6023f1c5bd8632caee9604f63c6a6e33ceb1e1bd 4732a2973f5021e96e05e0da932b5b1d2bc618351ca584bb6e49255ba22dca55ebd6b93a9c94d8749bb53be2 90650878b17f4fe30bbb08453929a94a2efe3367e2cd92ea31a5e0d9f466870b162272e9e164e8c3238da519   in hexadecimal notation, where N is  c97d1cbcc3b67d1ba197100df7dbd2d2864c4fef4a78e62ddd1423d972bc7a420f66046386462d260d68a8b2 3fbf12354705d874f79c22698f750c1b4435bc99174e58180bd18560a5c69c4eafb573446f79f588f624ec18 4c3e7098e65ac7b88f89e1fadcdc3558c878dde6bc7c32be57c5e7e8d95d697ad3c6c343485132dcbb74f411.  What is x?  The problem of distinguishing prime numbers from composites, and of resolving composite numbers into their prime factors, is one of the most important and useful in all of arithmetic. . . . The dignity of science seems to demand that every aid to the solution of such an elegant and celebrated problem be zealously cultivated. — C. F. GAUSS, Disquisitiones Arithmeticæ, Article 329  1801    418  ARITHMETIC  4.6  4.6. POLYNOMIAL ARITHMETIC The techniques we have been studying apply in a natural way to many types of mathematical quantities, not simply to numbers. In this section we shall deal with polynomials, which are the next step up from numbers. Formally speaking, a polynomial over S is an expression of the form  u x  = unxn + ··· + u1x + u0,   1  where the coefficients un, . . . , u1, u0 are elements of some algebraic system S, and the variable x may be regarded as a formal symbol with an indeterminate meaning. We will assume that the algebraic system S is a commutative ring with identity; this means that S admits the operations of addition, subtraction, and multiplication, satisfying the customary properties: Addition and multiplication are binary operations defined on S; they are associative and commutative, and multiplication distributes over addition. There is an additive identity element 0 and a multiplicative identity element 1, such that a + 0 = a and a · 1 = a for all a in S. Subtraction is the inverse of addition, but we assume nothing about the possibility of division as an inverse to multiplication. The polynomial 0xn+m +··· + 0xn+1 + unxn +··· + u1x + u0 is regarded as the same polynomial as  1 , although its expression is formally different. We say that  1  is a polynomial of degree n and leading coefficient un if un ̸= 0; and in this case we write  deg u  = n,  ℓ u  = un.   2   By convention, we also set  deg 0  = −∞,  ℓ 0  = 0,   3  where “0” denotes the zero polynomial whose coefficients are all zero. We say that u x  is a monic polynomial if its leading coefficient ℓ u  is 1.  Arithmetic on polynomials consists primarily of addition, subtraction, and multiplication; in some cases, further operations such as division, exponentiation, factoring, and taking the greatest common divisor are important. Addition, subtraction, and multiplication are defined in a natural way, as though the variable x were an element of S: We add or subtract polynomials by adding or subtracting the coefficients of like powers of x. Multiplication is done by the rule   urxr + ··· + u0  vsxs + ··· + v0  = wr+sxr+s + ··· + w0,  where  wk = u0vk + u1vk−1 + ··· + uk−1v1 + ukv0.   4   In the latter formula ui or vj are treated as zero if i > r or j > s.  The algebraic system S is usually the set of integers, or the rational numbers; or it may itself be a set of polynomials  in variables other than x , in which case  1  is a multivariate polynomial, a polynomial in several variables. Another important case occurs when the algebraic system S consists of the integers 0, 1, . . . , m − 1, with addition, subtraction, and multiplication performed mod m   4.6  419  see Eq. 4.3.2– 11 ; this is called polynomial arithmetic modulo m. Polyno-  POLYNOMIAL ARITHMETIC  mial arithmetic modulo 2, when each of the coefficients is 0 or 1, is especially important.  The reader should note the similarity between polynomial arithmetic and multiple-precision arithmetic  Section 4.3.1 , where the radix b is substituted for x. The chief difference is that the coefficient uk of xk in polynomial arithmetic bears no essential relation to its neighboring coefficients uk±1, so the idea of “carrying” from one place to the next is absent. In fact, polynomial arithmetic modulo b is essentially identical to multiple-precision arithmetic with radix b, except that all carries are suppressed. For example, compare the multiplication of  1101 2 by  1011 2 in the binary number system with the analogous multipli- cation of x3 + x2 + 1 by x3 + x + 1 modulo 2:  Binary system  Polynomials modulo 2  1101 × 1011 1101 1101  1101 10001111  1101 × 1011 1101 1101  1101 1111111  The product of these polynomials modulo 2 is obtained by suppressing all carries, and it is x6 + x5 + x4 + x3 + x2 + x+1. If we had multiplied the same polynomials over the integers, without taking residues modulo 2, the result would have been x6 + x5 + x4 + 3x3 + x2 + x + 1; again carries are suppressed, but in this case the coefficients can get arbitrarily large.  In view of this strong analogy with multiple-precision arithmetic, it is unnec- essary to discuss polynomial addition, subtraction, and multiplication much fur- ther in this section. However, we should point out some aspects that often make polynomial arithmetic somewhat different from multiple-precision arithmetic in practice: There is often a tendency to have a large number of zero coefficients, and polynomials of huge degrees, so special forms of representation are desirable; see Section 2.2.4. Furthermore, arithmetic on polynomials in several variables leads to routines that are best understood in a recursive framework; this situation is discussed in Chapter 8.  Although the techniques of polynomial addition, subtraction, and multi- plication are comparatively straightforward, several other important aspects of polynomial arithmetic deserve special examination. The following subsections therefore discuss division of polynomials, with associated techniques such as finding greatest common divisors and factoring. We shall also discuss the prob- lem of efficient evaluation of polynomials, namely the task of finding the value of u x  when x is a given element of S, using as few operations as possible. The special case of evaluating xn rapidly when n is large is quite important by itself, so it is discussed in detail in Section 4.6.3.  The first major set of computer subroutines for doing polynomial arithmetic was the ALPAK system [W. S. Brown, J. P. Hyde, and B. A. Tague, Bell System   420  ARITHMETIC  4.6  Tech. J. 42  1963 , 2081–2119; 43  1964 , 785–804, 1547–1562]. Another early landmark in this field was the PM system of George Collins [CACM 9  1966 , 578–589]; see also C. L. Hamblin, Comp. J. 10  1967 , 168–171.  EXERCISES 1. [10] If we are doing polynomial arithmetic modulo 10, what is 7x+2 minus x2+5? What is 6x2 + x + 3 times 5x2 + 2? 2. [17] True or false:  a  The product of monic polynomials is monic.  b  The product of polynomials of degrees m and n has degree m+n.  c  The sum of polynomials of degrees m and n has degree max m, n . 3. [M20] If each of the coefficients ur, . . . , u0, vs, . . . , v0 in  4  is an integer satisfying the conditions ui ≤ m1, vj ≤ m2, what is the maximum absolute value of the product coefficients wk?   cid:120  4. [21] Can the multiplication of polynomials modulo 2 be facilitated by using the  cid:120  5. [M21] Show how to multiply two polynomials of degree ≤ n, modulo 2, with  ordinary arithmetic operations on a binary computer, if coefficients are packed into computer words?  an execution time proportional to O nlg 3  when n is large, by adapting Karatsuba’s method  see Section 4.3.3 .  4.6.1. Division of Polynomials It is possible to divide one polynomial by another in essentially the same way that we divide one multiple-precision integer by another, when arithmetic is being done on polynomials over a field. A field S is a commutative ring with identity, in which exact division is possible as well as the operations of addition, subtraction, and multiplication; this means as usual that whenever u and v are elements of S, and v ̸= 0, there is an element w in S such that u = vw. The most important fields of coefficients that arise in applications are a  the rational numbers  represented as fractions, see Section 4.5.1 ; b  the real or complex numbers  represented within a computer by means of  floating point approximations; see Section 4.2 ;  c  the integers modulo p where p is prime  where division can be implemented  as suggested in exercise 4.5.2–16 ;  d  rational functions over a field, that is, quotients of two polynomials whose  coefficients are in that field, the denominator being monic.  Of special importance is the field of integers modulo 2, whose only elements are 0 and 1. Polynomials over this field  namely polynomials modulo 2  have many analogies to integers expressed in binary notation; and rational functions over this field have striking analogies to rational numbers whose numerator and denominator are represented in binary notation. Given two polynomials u x  and v x  over a field, with v x  ̸= 0, we can divide u x  by v x  to obtain a quotient polynomial q x  and a remainder polynomial r x  satisfying the conditions u x  = q x  · v x  + r x ,  deg r  < deg v .   1    421  4.6.1  DIVISION OF POLYNOMIALS  It is easy to see that there is at most one pair of polynomials q x , r x  satisfying these relations; for ifq1 x , r1 x  andq2 x , r2 x  both satisfy  1  q2 x v x + r2 x , soq1 x − q2 x v x  = r2 x − r1 x . Now if q1 x − q2 x  is nonzero, we have deg q1−q2 ·v = deg q1−q2 +deg v  ≥ deg v  > deg r2−r1 ,  with respect to the same polynomials u x  and v x , then q1 x v x  + r1 x  =  a contradiction; hence q1 x  − q2 x  = 0 and r1 x  = r2 x . The following algorithm, which is essentially the same as Algorithm 4.3.1D for multiple-precision division but without any concerns of carries, may be used to determine q x  and r x : Algorithm D  Division of polynomials over a field . Given polynomials v x  = vnxn + ··· + v1x + v0  u x  = umxm + ··· + u1x + u0,  over a field S, where vn ̸= 0 and m ≥ n ≥ 0, this algorithm finds the polynomials  q x  = qm−nxm−n + ··· + q0,  r x  = rn−1xn−1 + ··· + r0  over S that satisfy  1 . D1. [Iterate on k.] Do step D2 for k = m − n, m − n − 1, . . . , 0; then terminate  the algorithm with  rn−1, . . . , r0  =  un−1, . . . , u0 .  D2. [Division loop.] Set qk ← un+k vn, and then set uj ← uj − qkvj−k for j = n + k − 1, n + k − 2, . . . , k.  The latter operation amounts to replacing u x  by u x  − qkxkv x , a polynomial of degree < n + k.  An example of Algorithm D appears below in  5 . The number of arithmetic operations is essentially proportional to n m− n+1 . Note that explicit division of coefficients is done only at the beginning of step D2, and the divisor is always vn; thus if v x  is a monic polynomial  with vn = 1 , there is no actual division at all. If multiplication is easier to perform than division it will be preferable to compute 1 vn at the beginning of the algorithm and to multiply by this quantity in step D2.  We shall often write u x  mod v x  for the remainder r x  in  1 .  Unique factorization domains. If we restrict consideration to polynomials over a field, we are not coming to grips with many important cases, such as polynomials over the integers or polynomials in several variables. Let us therefore now consider the more general situation that the algebraic system S of coefficients is a unique factorization domain, not necessarily a field. This means that S is a commutative ring with identity, and that i  uv ̸= 0, whenever u and v are nonzero elements of S; ii  every nonzero element u of S is either a unit or has a “unique” representation  as a product of primes p1, . . . , pt:  u = p1 . . . pt,  t ≥ 1.   2   A unit is an element that has a reciprocal, namely an element u such that uv = 1 for some v in S; and a prime is a nonunit element p such that the equation p = qr   422  ARITHMETIC  4.6.1  can be true only if either q or r is a unit. The representation  2  is to be unique in the sense that if p1 . . . pt = q1 . . . qs, where all the p’s and q’s are primes, then s = t and there is a permutation π1 . . . πt of {1, . . . , t} such that p1 = a1qπ1, . . . , pt = atqπt for some units a1, . . . , at. In other words, factorization into primes is unique, except for unit multiples and except for the order of the factors.  Any field is a unique factorization domain, in which each nonzero element is a unit and there are no primes. The integers form a unique factorization domain in which the units are +1 and −1, and the primes are ±2, ±3, ±5, ±7, ±11, etc. The case that S is the set of all integers is of principal importance, because it is often preferable to work with integer coefficients instead of arbitrary rational coefficients.  One of the key facts about polynomials  see exercise 10  is that the poly- nomials over a unique factorization domain form a unique factorization domain. A polynomial that is prime in this domain is usually called an irreducible polyno- mial. By using the unique factorization theorem repeatedly, we can prove that multivariate polynomials over the integers, or over any field, in any number of variables, can be uniquely factored into irreducible polynomials. For example, the multivariate polynomial 90x3 − 120x2y + 18x2yz − 24xy2z over the integers is the product of five irreducible polynomials 2 · 3 · x ·  3x − 4y  ·  5x + yz . The same polynomial, as a polynomial over the rationals, is the product of three irreducible polynomials  6x  ·  3x − 4y  ·  5x + yz ; this factorization can also be written x ·  90x − 120y  ·  x + 1 5 yz  and in infinitely many other ways, although the factorization is essentially unique. As usual, we say that u x  is a multiple of v x , and that v x  is a divisor of u x , if u x  = v x q x  for some polynomial q x . If we have an algorithm to tell whether or not u is a multiple of v for arbitrary nonzero elements u and v of a unique factorization domain S, and to determine w if u = v·w, then Algorithm D gives us a method to tell whether or not u x  is a multiple of v x  for arbitrary nonzero polynomials u x  and v x  over S. For if u x  is a multiple of v x , it is easy to see that un+k must be a multiple of vn each time we get to step D2, hence the quotient u x  v x  will be found. Applying this observation recursively, we obtain an algorithm that decides if a given polynomial over S, in any number of variables, is a multiple of another given polynomial over S, and the algorithm will find the quotient when it exists.  A set of elements of a unique factorization domain is said to be relatively prime if no prime of that unique factorization domain divides all of them. A polynomial over a unique factorization domain is called primitive if its coefficients are relatively prime.  This concept should not be confused with the quite different idea of “primitive polynomials modulo p” discussed in Section 3.2.2.  The following fact, introduced for the case of polynomials over the integers by C. F. Gauss in article 42 of his celebrated book Disquisitiones Arithmeticæ  Leipzig: 1801 , is of prime importance:  Lemma G  Gauss’s Lemma . The product of primitive polynomials over a unique factorization domain is primitive.   DIVISION OF POLYNOMIALS  4.6.1 423 Proof. Let u x  = umxm + ··· + u0 and v x  = vnxn + ··· + v0 be primitive polynomials. If p is any prime of the domain, we must show that p does not divide all the coefficients of u x v x . By assumption, there is an index j such that uj is not divisible by p, and an index k such that vk is not divisible by p. Let j and k be as small as possible; then the coefficient of xj+k in u x v x  is  ujvk + uj+1vk−1 + ··· + uj+kv0 + uj−1vk+1 + ··· + u0vk+j,  and it is easy to see that this is not a multiple of p  since its first term isn’t, but all of its other terms are .  If a nonzero polynomial u x  over a unique factorization domain S is not primitive, we can write u x  = p1 · u1 x , where p1 is a prime of S dividing all the coefficients of u x , and where u1 x  is another nonzero polynomial over S. All of the coefficients of u1 x  have one less prime factor than the corresponding coefficients of u x . Now if u1 x  is not primitive, we can write u1 x  = p2·u2 x , etc.; this process must ultimately terminate in a representation u x  = c · uk x , where c is an element of S and uk x  is primitive. In fact, we have the following companion to Lemma G: Lemma H. Any nonzero polynomial u x  over a unique factorization domain S can be factored in the form u x  = c· v x , where c is in S and v x  is primitive. Furthermore, this representation is unique, in the sense that if u = c1 · v1 x  = c2 · v2 x , then c1 = ac2 and v2 x  = av1 x  where a is a unit of S. Proof. We have shown that such a representation exists, so only the uniqueness needs to be proved. Assume that c1 · v1 x  = c2 · v2 x , where v1 x  and v2 x  are primitive. Let p be any prime of S. If pk divides c1, then pk also divides c2; otherwise pk would divide all the coefficients of c2 · v2 x , so p would divide all the coefficients of v2 x , a contradiction. Similarly, pk divides c2 only if pk divides c1. Hence, by unique factorization, c1 = ac2 where a is a unit; and  0 = ac2 · v1 x  − c2 · v2 x  = c2 ·av1 x  − v2 x , so av1 x  − v2 x  = 0. where cont u , the content of u, is an element of S, and ppu x , the primitive to define cont u  = ppu x  = 0. Combining Lemmas G and H gives us the  part of u x , is a primitive polynomial over S. When u x  = 0, it is convenient  u x  = cont u  · ppu x ,  Therefore we may write any nonzero polynomial u x  as   3   relations  cont u · v  = a cont u  cont v ,  ppu x  · v x  = b ppu x  ppv x ,  where a and b are units, depending on the way contents are calculated, with ab = 1. When we are working with polynomials over the integers, the only units  are +1 and −1, and it is conventional to define ppu x  so that its leading polynomials over a field we may take cont u  = ℓ u , so that ppu x  is monic;  coefficient is positive; then  4  is true with a = b = 1. When working with  in this case again  4  holds with a = b = 1, for all u x  and v x .   4    424  ARITHMETIC  4.6.1  For example, if we are dealing with polynomials over the integers, let u x  =  −26x2 + 39 and v x  = 21x + 14. Then  cont u  = −13, cont v  = +7, cont u · v  = −91,  ppu x  = 2x2 − 3, ppv x  = 3x + 2,  ppu x  · v x  = 6x3 + 4x2 − 9x − 6.  Greatest common divisors. When there is unique factorization, it makes sense to speak of a greatest common divisor of two elements; this is a common  divisor that is divisible by as many primes as possible. See Eq. 4.5.2– 6 . Since  a unique factorization domain may have many units, however, there is ambiguity in this definition of greatest common divisor; if w is a greatest common divisor of u and v, so is a · w, when a is any unit. Conversely, the assumption of unique factorization implies that if w1 and w2 are both greatest common divisors of u and v, then w1 = a · w2 for some unit a. In other words it does not make sense, in general, to speak of “the” greatest common divisor of u and v; there is a set of greatest common divisors, each one being a unit multiple of the others.  Let us now consider the problem of finding a greatest common divisor of two given polynomials over an algebraic system S, a question originally raised by Pedro Nuñez in his Libro de Algebra  Antwerp: 1567 . If S is a field, the problem is relatively simple; our division algorithm, Algorithm D, can be extended to an algorithm that computes greatest common divisors, just as Euclid’s algorithm  Algorithm 4.5.2A  yields the greatest common divisor of two given integers based on a division algorithm for integers:  If v x  = 0, then gcdu x , v x  = u x ; otherwise gcdu x , v x  = gcdv x , r x ,  where r x  is given by  1 . This procedure is called Euclid’s algorithm for polynomials over a field. It was first used by Simon Stevin in L’Arithmetique  Leiden: 1585 ; see A. Girard, Les Œuvres Mathématiques de Simon Stevin 1  Leiden: 1634 , 56.  For example, let us determine the gcd of x8 + x6 +10x4 +10x3 +8x2 +2x+8 and 3x6+5x4+9x2+4x+8, mod 13, by using Euclid’s algorithm for polynomials over the integers modulo 13. First, writing only the coefficients to show the steps of Algorithm D, we have  3 0 5 0 9 4 8  1 0 1 0 10 10 8 2 8  9 0 7  0 1 2 8 0 11 2 4 0 3 0 4 so that x8 + x6 + 10x4 + 10x3 + 8x2 + 2x + 8 equals  1 0 6 0 0 8 0  3 10 7 8 0 9 0 11  7   9x2 + 7  3x6 + 5x4 + 9x2 + 4x + 8  +  11x4 + 3x2 + 4 .   5    4.6.1  Similarly,  DIVISION OF POLYNOMIALS  425  3x6 + 5x4 + 9x2 + 4x + 8 =  5x2 + 5  11x4 + 3x2 + 4  +  4x + 1 ;  11x4 + 3x2 + 4 =  6x3 + 5x2 + 6x + 5  4x + 1  + 12;  4x + 1 =  9x + 12  · 12 + 0.   6   The equality sign here means congruence modulo 13, since all arithmetic on the coefficients has been done mod 13.  This computation shows that 12 is a greatest common divisor of the two original polynomials. Now any nonzero element of a field is a unit of the domain of polynomials over that field, so it is conventional in the case of fields to divide the result of the algorithm by its leading coefficient, producing a monic polynomial that is called the greatest common divisor of the two given polynomials. The gcd computed in  6  is accordingly taken to be 1, not 12. The last step in  6  could have been omitted,  for if deg v  = 0, then gcdu x , v x  = 1, no matter what polynomial is chosen  for u x . Exercise 4 determines the average running time for Euclid’s algorithm on random polynomials modulo p.  Let us now turn to the more general situation in which our polynomials are given over a unique factorization domain that is not a field. From Eqs.  4  we can deduce the important relations  contgcd u, v  = a · gcdcont u , cont v , ppgcd u x , v x   = b · gcdppu x , ppv x ,  where a and b are units. Here gcdu x , v x  denotes any particular polynomial  in x that is a greatest common divisor of u x  and v x . Equations  7  reduce the problem of finding greatest common divisors of arbitrary polynomials to the problem of finding greatest common divisors of primitive polynomials.  Algorithm D for division of polynomials over a field can be generalized to a pseudo-division of polynomials over any algebraic system that is a commutative ring with identity. We can observe that Algorithm D requires explicit division only by ℓ v , the leading coefficient of v x , and that step D2 is carried out exactly m − n + 1 times; thus if u x  and v x  start with integer coefficients, and if we are working over the rational numbers, then the only denominators that appear in the coefficients of q x  and r x  are divisors of ℓ v m−n+1. This suggests that we can always find polynomials q x  and r x  such that   7   ℓ v m−n+1u x  = q x v x  + r x ,   8  where m = deg u  and n = deg v , for any polynomials u x  and v x  ̸= 0, provided that m ≥ n. Algorithm R  Pseudo-division of polynomials . Given polynomials  deg r  < n,  u x  = umxm + ··· + u1x + u0,  v x  = vnxn + ··· + v1x + v0,  where vn ̸= 0 and m ≥ n ≥ 0, this algorithm finds polynomials q x  = qm−nxm−n + ··· + q0 and r x  = rn−1xn−1 + ··· + r0 satisfying  8 .   ARITHMETIC  426 4.6.1 R1. [Iterate on k.] Do step R2 for k = m − n, m − n − 1, . . . , 0; then terminate the algorithm with  rn−1, . . . , r0  =  un−1, . . . , u0 . n, and set uj ← vnuj − un+kvj−k for R2. [Multiplication loop.] Set qk ← un+kvk j = n + k − 1, n + k − 2, . . . , 0.  When j < k this means that uj ← vnuj, since we treat v−1, v−2, . . . as zero. These multiplications could have been avoided if we had started the algorithm by replacing ut by vm−n−t ut, for 0 ≤ t < m − n.  An example calculation appears below in  10 . It is easy to prove the validity of Algorithm R by induction on m−n, since each execution of step R2 essentially replaces u x  by ℓ v u x  − ℓ u xkv x , where k = deg u  − deg v . Notice that no division whatever is used in this algorithm; the coefficients of q x  and r x  are themselves certain polynomial functions of the coefficients of u x  and v x . If vn = 1, the algorithm is identical to Algorithm D. If u x  and v x  are polynomials over a unique factorization domain, we can prove as before that the polynomials q x  and r x  are unique; therefore another way to do the pseudo- division over a unique factorization domain is to multiply u x  by vm−n+1 and apply Algorithm D, knowing that all the quotients in step D2 will exist.  n  n  Algorithm R can be extended to a “generalized Euclidean algorithm” for primitive polynomials over a unique factorization domain, in the following way: Let u x  and v x  be primitive polynomials with deg u  ≥ deg v , and determine the polynomial r x  satisfying  8  by means of Algorithm R. Now we can prove  that gcdu x , v x  = gcdv x , r x : Any common divisor of u x  and v x  ℓ v m−n+1u x , and it must be primitivesince v x  is primitive so it divides u x . If r x  = 0, we therefore have gcdu x , v x  = v x ; on the other hand if r x  ̸= 0, we have gcdv x , r x  = gcdv x , ppr x  since v x  is primitive,  divides v x  and r x ; conversely, any common divisor of v x  and r x  divides  so the process can be iterated. Algorithm E  Generalized Euclidean algorithm . Given nonzero polynomials u x  and v x  over a unique factorization domain S, this algorithm calculates a greatest common divisor of u x  and v x . We assume that auxiliary algorithms exist to calculate greatest common divisors of elements of S, and to divide a by b in S when b ̸= 0 and a is a multiple of b.  E1. [Reduce to primitive.] Set d ← gcdcont u , cont v , using the assumed algorithm for calculating greatest common divisors in S. By definition, cont u  is a greatest common divisor of the coefficients of u x . Replace u x  by the polynomial u x  cont u  = ppu x ; similarly, replace v x  by ppv x . E2. [Pseudo-division.] Calculate r x  using Algorithm R.It is unnecessary to calculate the quotient polynomial q x . If r x  = 0, go to E4. If deg r  = 0, ppr x . Go back to step E2.  This is the “Euclidean step,” analogous  E3. [Make remainder primitive.] Replace u x  by v x  and replace v x  by  replace v x  by the constant polynomial “1” and go to E4.  to the other instances of Euclid’s algorithm that we have seen.    4.6.1 427 E4. [Attach the content.] The algorithm terminates, with d · v x  as the desired  DIVISION OF POLYNOMIALS  answer. As an example of Algorithm E, let us calculate the gcd of the polynomials  u x  = x8 + x6 − 3x4 − 3x3 + 8x2 + 2x − 5, v x  = 3x6 + 5x4 − 4x2 − 9x + 21,   9  over the integers. These polynomials are primitive, so step E1 sets d ← 1. In step E2 we have the pseudo-division  3 0 5 0 −4 −9 21  1 0  3 0 3 0  0 −6 1 2 −5 1 0 −3 −3 8 6 −15 3 0 −9 −9 24 5 0 −4 −9 21 6 −15 3 0 9 18 −45 0 0 0 0 9 18 −45 0 0 27 54 −135 0 24 54 −126 0 −9 0  0 −2 0 −5 0 −6 0 −15 0 0 0 0 −6 0 −15 −18 0 −45 −18 0 −30 −15  3  0   10   Here the quotient q x  is 1 · 32x2 + 0 · 31x + −6 · 30; we have 27u x  = v x  9x2 − 6  +  −15x4 + 3x2 − 9 .  Now step E3 replaces u x  by v x  and v x  by ppr x  = 5x4 − x2 + 3. The  subsequent calculation is summarized in the following table, where only the coefficients are shown:   11   u x   v x   r x   1, 0, 1, 0, −3, −3, 8, 2, −5 3, 0, 5, 0, −4, −9, 21 5, 0, −1, 0, 3 13, 25, −49  3, 0, 5, 0, −4, −9, 21 −15, 0, 3, 0, −9 5, 0, −1, 0, 3 −585, −1125, 2205 −233150, 307500 13, 25, −49 4663, −6150 143193869   12  It is instructive to compare this calculation with the computation of the same greatest common divisor over the rational numbers, instead of over the integers, by using Euclid’s algorithm for polynomials over a field as described earlier in this section. The following surprisingly complicated sequence appears:  u x   1, 0, 1, 0, −3, −3, 8, 2, −5 3, 0, 5, 0, −4, −9, 21 − 5 9 , 0, − 1 9 , 0, 1 3 − 117 25 , −9, 441 19773 , − 102500 233150  6591  25  v x   3, 0, 5, 0, −4, −9, 21 − 5 9 , 0, − 1 9 , 0, 1 3 25 , −9, 441 − 117 19773 , − 102500 233150 6591 − 1288744821 543589225  25   13    428  ARITHMETIC  4.6.1  To improve that algorithm, we can reduce u x  and v x  to monic polynomi- als at each step, since this removes unit factors that make the coefficients more complicated than necessary; this is actually Algorithm E over the rationals:  u x   1, 0, 5  1, 0, 1, 0, −3, −3, 8, 2, −5 3 , −3, 7 3 , 0, − 4 1, 0, − 1 5 , 0, 3 5 13 , − 49 1, 25 13 1, − 6150 4663  1, 0, 5  v x  3 , −3, 7 3 , 0, − 4 1, 0, − 1 5 , 0, 3 5 13 , − 49 1, 25 13 1, − 6150 4663 1  9 x4+ 1  5 x2+ 3  9 x2− 1   14  In both  13  and  14  the sequence of polynomials is essentially the same as  12 , which was obtained by Algorithm E over the integers; the only differ- ence is that the polynomials have been multiplied by certain rational numbers. Whether we have 5x4−x2+3 or − 5 5, the computations are essentially the same. But either algorithm using rational arithmetic tends to run slower than the all-integer Algorithm E, since rational arithmetic usually requires more evaluations of integer gcds within each step when the polynomials have large degree.  3 or x4− 1  It is instructive to compare  12 ,  13 , and  14  with  6  above, where we determined the gcd of the same polynomials u x  and v x  modulo 13 with considerably less labor. Since ℓ u  and ℓ v  are not multiples of 13, the fact  that gcdu x , v x  = 1 modulo 13 is sufficient to prove that u x  and v x   are relatively prime over the integers  and therefore over the rational numbers . We will return to this time-saving observation at the close of Section 4.6.2. The subresultant algorithm. An ingenious algorithm that is generally supe- rior to Algorithm E, and that gives us further information about Algorithm E’s behavior, was discovered by George E. Collins [JACM 14  1967 , 128–142] and subsequently improved by W. S. Brown and J. F. Traub [JACM 18  1971 , 505– 514; see also W. S. Brown, ACM Trans. Math. Software 4  1978 , 237–249]. This algorithm avoids the calculation of primitive parts in step E3, dividing instead by an element of S that is known to be a factor of r x : Algorithm C  Greatest common divisor over a unique factorization domain . This algorithm has the same input and output assumptions as Algorithm E, and has the advantage that fewer calculations of greatest common divisors of coefficients are needed.  C1. [Reduce to primitive.] As in step E1 of Algorithm E, set d ← gcdcont u , cont v , and replaceu x , v x  byppu x , ppv x . Set g ← h ← 1.  C2. [Pseudo-division.] Set δ ← deg u  − deg v . Calculate r x  using Algo- rithm R. If r x  = 0, go to C4. If deg r  = 0, replace v x  by the constant polynomial “1” and go to C4.  C3. [Adjust remainder.] Replace the polynomial u x  by v x , and replace v x   by r x  ghδ. At this point all coefficients of r x  are multiples of ghδ.   4.6.1  Then set g ← ℓ u , h ← h1−δgδ and return to C2. The new value of h will be in the domain S, even if δ > 1. C4. [Attach the content.] Return d · ppv x  as the answer.  DIVISION OF POLYNOMIALS  429  If we apply this algorithm to the polynomials  9  considered earlier, the  following sequence of results is obtained at the beginning of step C2:  u x   v x   1, 0, 1, 0, −3, −3, 8, 2, −5 3, 0, 5, 0, −4, −9, 21 −15, 0, 3, 0, −9 65, 125, −245  3, 0, 5, 0, −4, −9, 21 −15, 0, 3, 0, −9 65, 125, −245 −9326, 12300 At the conclusion of the algorithm, r x  ghδ = 260708.  g 1 3 −15 65  h 1 9 25 169   15   The sequence of polynomials consists of integral multiples of the polynomials in the sequence produced by Algorithm E. In spite of the fact that the polyno- mials are not reduced to primitive form, the coefficients are kept to a reasonable size because of the reduction factor in step C3.  In order to analyze Algorithm C and to prove that it is valid, let us call the sequence of polynomials it produces u1 x , u2 x , u3 x , . . . , where u1 x  = u x  and u2 x  = v x . Let δj = nj − nj+1 for j ≥ 1, where nj = deg uj ; and let g1 = h1 = 1, gj = ℓ uj , hj = h  gδj−1  1−δj−1 j−1  j  gδ1+1 2 gδ2+1 3 gδ3+1 4  u1 x  = u2 x q1 x  + g1hδ1 u2 x  = u3 x q2 x  + g2hδ2 u3 x  = u4 x q3 x  + g3hδ3  for j ≥ 2. Then we have n3 < n2; 1 u3 x , 2 u4 x , n4 < n3; n5 < n4; 3 u5 x ,   16   j  and so on. The process terminates when nk+1 = deg uk+1  ≤ 0. We must show that u3 x , u4 x , . . . , have coefficients in S, namely that the factors gjhδj exactly divide all coefficients of the remainders, and we must also show that the hj values all belong to S. The proof is rather involved, and it can be most easily understood by considering an example. Suppose, as in  15 , that n1 = 8, n2 = 6, n3 = 4, n4 = 2, n5 = 1, n6 = 0, so that δ1 = δ2 = δ3 = 2, δ4 = δ5 = 1. Let us write u1 x  = a8x8 + a7x7 + ··· + a0, u2 x  = b6x6 + b5x5 +··· + b0, . . . , u5 x  = e1x + e0, u6 x  = f0, so that h1 = 1, 4. In these terms it is helpful to consider the h2 = b2 array shown in Table 1. For concreteness, let us assume that the coefficients 6u1 x  = u2 x q1 x  + u3 x ; so if of the polynomials are integers. We have b3 6 and subtract appropriate multiples of rows B7, B6, we multiply row A5 by b3 and B5 multiply row A4 by b3 6 and subtract multiples of rows B6, B5, and B4, we get row C4. In a similar way, we have c3 6u4 x ; so we can multiply row B3 by c3 4, subtract integer multiples of rows C5, C4, and C3, then divide by b5  corresponding to the coefficients of q1 x  we will get row C5. If we also  4u2 x  = u3 x q2 x  + b5  6, h4 = d2  6 to obtain row D3.  6, h3 = c2  6 c2  4 b2  2b2   4.6.1   17    18    = M.  = M′.   = ±b4  430  ARITHMETIC  matrix  In order to prove that u4 x  has integer coefficients, let us consider the     A2 A1 A0 B4 B3 B2 B1 B0  B4 B3 B2 B1 C2 C1 C0 D0  a8 a7 a6 a5 a4 a3 a2 a1 a0 0 0 0 a8 a7 a6 a5 a4 a3 a2 a1 a0 0 0 0 a8 a7 a6 a5 a4 a3 a2 a1 a0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0  b6 b5 b4 b3 b2 b1 b0 0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 0 c4 c3 c2 c1 c0 0 0 0 0 0 0 0 c4 c3 c2 c1 c0 0 0 0 0 0 0 0 c4 c3 c2 c1 c0 0 0 0 0 0 0 0 0 d2 d1 d0  The indicated row operations and a permutation of rows will transform M into  Because of the way M′ has been derived from M, we must have  6 · b3 b3  6 · b3  6 ·  c3  4 b5  6  · det M0 = ± det M′ 0,  if M0 and M′ 0 represent any square matrices obtained by selecting eight corre- sponding columns from M and M′. For example, let us select the first seven columns and the column containing d1; then    a8 a7 a6 a5 a4 a3 a2 0 0 a8 a7 a6 a5 a4 a3 a0 0 0 a8 a7 a6 a5 a4 a1 b6 b5 b4 b3 b2 b1 b0 0 0 b6 b5 b4 b3 b2 b1 0 0 0 b6 b5 b4 b3 b2 0 0 0 0 b6 b5 b4 b3 b0 0 0 0 0 b6 b5 b4 b1  6 · b3 b3  6 · b3  6 ·  c3  4 b5  6  · det  6 · c3  4 · d1.  Since b6 c4 ̸= 0, this proves that d1 is an integer. Similarly, d2 and d0 are integers. In general, we can show that uj+1 x  has integer coefficients in a similar manner. If we start with the matrix M consisting of rows An2−nj through A0 and Bn1−nj through B0, and if we perform the row operations indicated in Table 1, we will obtain a matrix M′ consisting in some order of rows Bn1−nj through Bn3−nj+1, then Cn2−nj through Cn4−nj+1, . . . , Pnj−2−nj through P1, then Qnj−1−nj through Q0, and finally R0  a row containing the coefficients of  uj+1 x . Extracting appropriate columns shows that   gδ1+1  2   g1hδ1  1  n2−nj+1 gδ2+1  3   g2hδ2  × det M0 = ±gn1−n3  2  n3−nj+1 . . .  gδj−1+1  gj−1hδj−1 j−1  . . . gnj−2−nj  j−1  nj−nj+1 gnj−1−nj+1 rt,  gn2−n4 3  2  j  j   19    4.6.1  Row name  A5 A4 A3 A2 A1 A0 B7 B6 B5 B4 B3 B2 B1 B0 C5 C4 C3 C2 C1 C0 D3 D2 D1 D0 E1 E0 F0  DIVISION OF POLYNOMIALS  431  COEFFICIENTS THAT ARISE IN ALGORITHM C  Table 1  Row  Multiply Replace by row  a8 a7 a6 a5 a4 a3 a2 a1 a0 0 0 0 0 0 b6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 a8 a7 a6 a5 a4 a3 a2 a1 a0 0 0 0 0 b5 b6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 a8 a7 a6 a5 a4 a3 a2 a1 a0 0 0 0 b4 b5 b6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 a8 a7 a6 a5 a4 a3 a2 a1 a0 0 0 b3 b4 b5 b6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 a8 a7 a6 a5 a4 a3 a2 a1 a0 0 b2 b3 b4 b5 b6 0 0 0 c4 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 a8 a7 a6 a5 a4 a3 a2 a1 a0 0 b1 0 b2 0 b3 0 b4 0 b5 0 b6 0 0 0 b0 0 c3 0 c4 0 0 0 0 0 0 0 c0 0 0 0 0 0 0 0 d2 d1 d0 0 0 e1 0 0 e0 0 0 f0  0 0 0 0 0 b0 0 b1 b2 b0 b3 b1 b4 b2 b5 b3 0 c0 0 c1 c2 c0 c3 c1 c4 c2 0 c3 d2 d1 d0 0 0 0 0 0 0  0 0 0 0 0 0 b0 b1 0 0 0 0 c0 c1 0 0 d2 d1 d0 0 0 0 0  0 0 0 0 0 b0 b1 b2 0 0 0 c0 c1 c2 0 d2 d1 d0 0 0 0 0 0  0 b0 b1 b2 b3 b4 b5 b6 c1 c2 c3 c4 0 0 0 0 0 0 0 0 0  b0 b1 b2 b3 b4 b5 b6 0 c2 c3 c4 0 0 0 0 0 0 0 0 0 0  0 0 0 b0 b1 b2 b3 b4 0 c0 c1 c2 c3 c4  e0 e1 0  by b3 6 b3 6 b3 6 b3 6 b3 6 b3 6  c3 4 b5 6 c3 4 b5 6 c3 4 b5 6 c3 4 b5 6  d2 2b4 d2 2b4  6 c5 4 6 c5 4  C5 C4 C3 C2 C1 C0  D3 D2 D1 D0  E1 E0  e2 2c2  4 d3  2b2 6  F0  where rt is a given coefficient of uj+1 x  and M0 is a submatrix of M. The h’s have been chosen very cleverly so that this equation simplifies to  det M0 = ± rt   20   see exercise 24 . Therefore every coefficient of uj+1 x  can be expressed as the determinant of an  n1 + n2−2nj +2 × n1 + n2−2nj +2  matrix whose elements are coefficients of u x  and v x . It remains to be shown that the cleverly chosen h’s also are integers. A  similar technique applies: Let’s look, for example, at the matrix    A1 A0 B3 B2 B1 B0  a8 a7 a6 a5 a4 a3 a2 a1 a0 0 0 a8 a7 a6 a5 a4 a3 a2 a1 a0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0   = M.   21    432  ARITHMETIC  4.6.1     = M′;  Row operations as specified in Table 1, and permutation of rows, leads to   22   B3 B2 B1 B0 C1 C0  b6 b5 b4 b3 b2 b1 b0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 0 0 c4 c3 c2 c1 c0 0 0 0 0 0 0 c4 c3 c2 c1 c0 hence if we consider any submatrices M0 and M′ 6·det M0 = ± det M′ corresponding columns of M and M′ we have b3 is chosen to be the first six columns of M, we find that det M0 = ±c2 so h3 is an integer.  0 obtained by selecting six 0. When M0 6 = ±h3, 4 b2 In general, to show that hj is an integer for j ≥ 3, we start with the matrix M consisting of rows An2−nj−1 through A0 and Bn1−nj−1 through B0; then we perform appropriate row operations until obtaining a matrix M′ consisting of rows Bn1−nj−1 through Bn3−nj, then Cn2+nj−1 through Cn4−nj, . . . , Pnj−2−nj−1 through P0, then Qnj−1−nj−1 through Q0. Letting M0 be the first n1 + n2 − 2nj columns of M, we obtain 1  n2−nj gδ2+1  gδ1+1   g1hδ1   g2hδ2  2  n3−nj . . .  gδj−1+1 gn2−n4 3  = ±gn1−n3  j−1  nj−nj det M0  gj−1hδj−1 gnj−1−nj , j−1  . . . gnj−2−nj  6·b3   23   2  3  2  j  j  an equation that neatly simplifies to  det M0 = ±hj.   24   This proof, although stated for the domain of integers, obviously applies to any unique factorization domain.   In the process of verifying Algorithm C, we have also learned that every element of S dealt with by the algorithm can be expressed as a determinant whose entries are the coefficients of the primitive parts of the original polynomials. A well-known theorem of Hadamard  see exercise 15  states that   det aij  ≤      1 2  ;  a2  ij  1≤i≤n  1≤j≤n   25   therefore every coefficient appearing in the polynomials computed by Algo- rithm C is at most  N m+n m + 1 n 2 n + 1 m 2,   26  if all coefficients of the given polynomials u x  and v x  are bounded by N in absolute value. This same upper bound applies to the coefficients of all polynomials u x  and v x  computed during the execution of Algorithm E, since the polynomials obtained in Algorithm E are always divisors of the polynomials obtained in Algorithm C.  This upper bound on the coefficients is extremely gratifying, because it is much better than we would ordinarily have a right to expect. For example, consider what happens if we avoid the corrections in steps E3 and C3, merely   4.6.1  DIVISION OF POLYNOMIALS  433  replacing v x  by r x . This is the simplest gcd algorithm, and it is the one that traditionally appears in textbooks on algebra  for theoretical purposes, not intended for practical calculations . If we suppose that δ1 = δ2 = ··· = 1, we find that the coefficients of u3 x  are bounded by N 3, the coefficients of u4 x  are bounded by N 7, those of u5 x  by N 17, . . . ; the coefficients of uk x  are bounded by N ak, where ak = 2ak−1 + ak−2. Thus the upper bound, in place of  26  for m = n + 1, would be approximately N 0.5 2.414 n ,   27  and experiments show that the simple algorithm does in fact have this behavior; the number of digits in the coefficients grows exponentially at each step! In Algorithm E, by contrast, the growth in the number of digits is only slightly more than linear at most.  Another byproduct of our proof of Algorithm C is the fact that the degrees of the polynomials will almost always decrease by 1 at each step, so that the number of iterations of step C2  or E2  will usually be deg v  if the given polynomials are “random.” In order to see why this happens, notice for example that we could have chosen the first eight columns of M and M′ in  17  and  18 ; then we would have found that u4 x  has degree less than 3 if and only if d3 = 0, that is, if and only if    det  a8 a7 a6 a5 a4 a3 a2 a1 0 a8 a7 a6 a5 a4 a3 a2 0 0 a8 a7 a6 a5 a4 a3 b6 b5 b4 b3 b2 b1 b0 0 0 b6 b5 b4 b3 b2 b1 b0 0 0 b6 b5 b4 b3 b2 b1 0 0 0 b6 b5 b4 b3 b2 0 0 0 0 b6 b5 b4 b3   = 0.  In general, δj will be greater than 1 for j > 1 if and only if a similar determinant in the coefficients of u x  and v x  is zero. Since such a determinant is a nonzero multivariate polynomial in the coefficients, it will be nonzero “almost always,” or “with probability 1.”  See exercise 16 for a more precise formulation of this statement, and see exercise 4 for a related proof.  The example polynomials in  15  have both δ2 and δ3 equal to 2, so they are exceptional indeed.  The considerations above can be used to derive the well-known fact that two polynomials are relatively prime if and only if their resultant is nonzero; the resultant is a determinant having the form of rows A5 through A0 and B7 through B0 in Table 1. Further properties of resultants are discussed in B. L. van der Waerden, Modern  This is “Sylvester’s determinant”; see exercise 12. Algebra, translated by Fred Blum  New York: Ungar, 1949 , Sections 27–28.  From the standpoint discussed above, we could say that the gcd is “almost always” of degree zero, since Sylvester’s determinant is almost never zero. But many calculations of practical interest would never be undertaken if there weren’t some reasonable chance that the gcd would be a polynomial of positive degree.   434  ARITHMETIC  4.6.1  We can see exactly what happens during Algorithms E and C when the gcd is not 1 by considering u x  = w x u1 x  and v x  = w x u2 x , where u1 x  and u2 x  are relatively prime and w x  is primitive. Then if the polynomials u1 x , u2 x , u3 x , . . . are obtained when Algorithm E works on u x  = u1 x  and v x  = u2 x , it is easy to see that the sequence obtained for u x  = w x u1 x  and v x  = w x u2 x  is simply w x u1 x , w x u2 x , w x u3 x , w x u4 x , etc. With Algorithm C the behavior is different: If the polynomials u1 x , u2 x , u3 x , . . . are obtained when Algorithm C is applied to u x  = u1 x  and v x  = u2 x , and if we assume that deg uj+1  = deg uj  − 1  which is almost always true when j > 1 , then the sequence  w x u1 x , w x u2 x , ℓ2w x u3 x , ℓ4w x u4 x , ℓ6w x u5 x , . . .   28  is obtained when Algorithm C is applied to u x  = w x u1 x  and v x  = w x u2 x , where ℓ = ℓ w .  See exercise 13.  Even though these additional ℓ-factors are present, Algorithm C will be superior to Algorithm E, because it is easier to deal with slightly larger polynomials than to calculate primitive parts repeatedly.  Polynomial remainder sequences such as those in Algorithms C and E are not useful merely for finding greatest common divisors and resultants. Another important application is to the enumeration of real roots, for a given polynomial in a given interval, according to the famous theorem of J. Sturm [Mém. Présentés par Divers Savants 6  Paris, 1835 , 271–318]. Let u x  be a polynomial over the real numbers, having distinct complex roots. We shall see in the next section  that the roots are distinct if and only if gcdu x , u′ x  = 1, where u′ x  is the  derivative of u x ; accordingly, there is a polynomial remainder sequence proving that u x  is relatively prime to u′ x . We set u0 x  = u x , u1 x  = u′ x , and  following Sturm  we negate the sign of all remainders, obtaining  c1u0 x  = u1 x q1 x  − d1u2 x , c2u1 x  = u2 x q2 x  − d2u3 x ,  ...  ckuk−1 x  = uk x qk x  − dkuk+1 x ,   29   for some positive constants cj and dj, where deg uk+1  = 0. We say that the variation V  u, a  of u x  at a is the number of changes of sign in the sequence u0 a , u1 a , . . . , uk+1 a , not counting zeros. For example, if the sequence of signs is 0, +, −, −, 0, +, +, −, we have V  u, a  = 3. Sturm’s theorem asserts that the number of roots of u x  in the interval a < x ≤ b is V  u, a  − V  u, b ; and the proof is surprisingly short  see exercise 22 .  Although Algorithms C and E are interesting, they aren’t the whole story. Important alternative ways to calculate polynomial gcds over the integers are discussed at the end of Section 4.6.2. There is also a general determinant- evaluation algorithm that may be said to include Algorithm C as a special case; see E. H. Bareiss, Math. Comp. 22  1968 , 565–578.   4.6.1  DIVISION OF POLYNOMIALS  435  In the fourth edition of this book I plan to redo the exposition of the present section, taking into proper account the 19th-century research on determinants, as well as the work of W. Habicht, Comm. Math. Helvetici 21  1948 , 99–116. An excellent discussion of the latter has been given by R. Loos in Computing, Supplement 4  1982 , 115–137. An interesting method for evalu- ating determinants, published in 1853 by Felice Chiò and rediscovered by C. L. Dodgson  aka Lewis Carroll , is also highly relevant. See D. E. Knuth, Electronic J. Combinatorics 3, 2  1996 , paper R5, §3, for a summary of the early history of identities between determinants of submatrices. EXERCISES 1. [10] Compute the pseudo-quotient q x  and pseudo-remainder r x , namely the polynomials satisfying  8 , when u x  = x6 + x5 − x4 + 2x3 + 3x2 − x + 2 and v x  = 2x3 + 2x2 − x + 3, over the integers. 2. [15] What is the greatest common divisor of 3x6 + x5 + 4x4 + 4x3 + 3x2 + 4x + 2 and its “reverse” 2x6 + 4x5 + 3x4 + 4x3 + 4x2 + x + 3, modulo 7?   cid:120  3. [M25] Show that Euclid’s algorithm for polynomials over a field S can be extended  to find polynomials U x  and V  x  over S such that  u x V  x  + U x v x  = gcd u x , v x  .   See Algorithm 4.5.2X.  What are the degrees of the polynomials U x  and V  x  that are computed by this extended algorithm? Prove that if S is the field of rational numbers, and if u x  = xm − 1 and v x  = xn − 1, then the extended algorithm yields polynomials U x  and V  x  having integer coefficients. Find U x  and V  x  when u x  = x21 − 1 and v x  = x13 − 1.   cid:120  4. [M30] Let p be prime, and suppose that Euclid’s algorithm applied to the poly-  nomials u x  and v x  modulo p yields a sequence of polynomials having respective degrees m, n, n1, . . . , nt, −∞, where m = deg u , n = deg v , and nt ≥ 0. Assume that m ≥ n. If u x  and v x  are monic polynomials, independently and uniformly distributed over all the pm+n pairs of monic polynomials having respective degrees m and n, what are the average values of the three quantities t, n1 + ··· + nt, and  n − n1 n1 + ··· +  nt−1 − nt nt, as functions of m, n, and p?  These three quantities are the fundamental factors in the running time of Euclid’s algorithm applied to polynomials modulo p, assuming that division is done by Algorithm D.  [Hint: Show that u x  mod v x  is uniformly distributed and independent of v x .] 5. [M22] What is the probability that u x  and v x  are relatively prime modulo p, if u x  and v x  are independently and uniformly distributed monic polynomials of degree n? 6. [M23] We have seen that Euclid’s Algorithm 4.5.2A for integers can be directly adapted to an algorithm for the greatest common divisor of polynomials. Can the binary gcd algorithm, Algorithm 4.5.2B, be adapted in an analogous way to an algo- rithm that applies to polynomials? 7. [M10] What are the units in the domain of all polynomials over a unique factor- ization domain S?   cid:120  8. [M22] Show that if a polynomial with integer coefficients is irreducible over the  domain of integers, it is irreducible when considered as a polynomial over the field of rational numbers.   436  ARITHMETIC  4.6.1  9. [M25] Let u x  and v x  be primitive polynomials over a unique factorization domain S. Prove that u x  and v x  are relatively prime if and only if there are polynomials U x  and V  x  over S such that u x V  x  + U x v x  is a polynomial of degree zero. [Hint: Extend Algorithm E, as Algorithm 4.5.2A is extended in exercise 3.] 10. [M28] Prove that the polynomials over a unique factorization domain form a unique factorization domain. [Hint: Use the result of exercise 9 to help show that there is at most one kind of factorization possible.] 11. [M22] What row names would have appeared in Table 1 if the sequence of degrees had been 9, 6, 5, 2, −∞ instead of 8, 6, 4, 2, 1, 0?   cid:120  12. [M24] Let u1 x , u2 x , u3 x , . . . be a sequence of polynomials obtained during a  run of Algorithm C. “Sylvester’s matrix” is the square matrix formed from rows An2−1 through A0 and Bn1−1 through B0  in a notation analogous to that of Table 1 . Show that if u1 x  and u2 x  have a common factor of positive degree, then the determinant of Sylvester’s matrix is zero; conversely, given that deg uk  = 0 for some k, show that the determinant of Sylvester’s matrix is nonzero by deriving a formula for its absolute value in terms of ℓ uj  and deg uj , 1 ≤ j ≤ k. 13. [M22] Show that the leading coefficient ℓ of the primitive part of gcd u x , v x   enters into Algorithm C’s polynomial sequence as shown in  28 , when δ1 = δ2 = ··· = δk−1 = 1. What is the behavior for general δj? 14. [M29] Let r x  be the pseudo-remainder when u x  is pseudo-divided by v x . If deg u  ≥ deg v  + 2 and deg v  ≥ deg r  + 2, show that r x  is a multiple of ℓ v . 15. [M26] Prove Hadamard’s inequality  25 . [Hint: Consider the matrix AAT.]   cid:120  16. [M22] Let f x1, . . . , xn  be a multivariate polynomial that is not identically zero,  and let r S1, . . . , Sn  be the set of roots  x1, . . . , xn  of f x1, . . . , xn  = 0 such that x1 ∈ S1, . . . , xn ∈ Sn. If the degree of f is at most dj ≤ Sj in the variable xj, prove that  r S1, . . . , Sn  ≤ S1 . . .Sn −  S1 − d1  . . .  Sn − dn  .  of symbols. A string polynomial on A is a finite sum U = k skβk  = defined in an obvious manner; thus,    r S1, . . . , Sn  S1 . . .Sn, Therefore the probability of finding a root at random, approaches zero as the sets Sj get bigger. [This inequality has many applications in the design of randomized algorithms, because it provides a good way to test whether a complicated sum of products of sums is identically zero without expanding out all the terms.] 17. [M32]  P. M. Cohn’s algorithm for division of string polynomials.  Let A be an alphabet, that is, a set of symbols. A string α on A is a sequence of n ≥ 0 symbols, α = a1 . . . an, where each aj is in A. The length of α, denoted by α, is the number n k rk αk, where each rk is a nonzero rational number and each αk is a string on A; we assume that αj ̸= αk when j ̸= k. The degree of U, deg U , is defined to be −∞ if U = 0  that is, if the sum is empty , otherwise deg U  = max αk. The sum and product of string polynomials are j,k rjskαjβk, where the product of two strings is obtained by simply juxtaposing them, after which we collect like terms. For example, if A = {a, b}, U = ab + ba − 2a − 2b, and V = a + b − 1, then deg U  = 2, deg V   = 1, V 2 = aa+ ab+ ba+ bb−2a−2b+1, and V 2 − U = aa+ bb+1. Clearly deg U V   = deg U  + deg V  , and deg U + V   ≤ max deg U , deg V   , with equality in the latter formula if deg U  ̸= deg V  .  String polynomials may be regarded as ordinary multivariate polynomials over the field of rational numbers, except that the variables are not commutative under multiplication. In the conventional language of  j rjαj     4.6.1  DIVISION OF POLYNOMIALS  437  pure mathematics, the set of string polynomials with the operations defined here is the “free associative algebra” generated by A over the rationals.  a  Let Q1, Q2, U, and V be string polynomials with deg U  ≥ deg V   and such that deg Q1U − Q2V   < deg Q1U . Give an algorithm to find a string polynomial Q such that deg U − QV   < deg U .  Thus if we are given U and V such that Q1U = Q2V + R and deg R  < deg Q1U , for some Q1 and Q2, then there is a solution to these conditions with Q1 = 1.  b  Given that U and V are string polynomials with deg V   > deg Q1U − Q2V   for some Q1 and Q2, show that the result of  a  can be improved to find a quotient Q such that U = QV + R, deg R  < deg V  .  This is the analog of  1  for string polynomials; part  a  showed that we can make deg R  < deg U , under weaker hypotheses.   c  A homogeneous polynomial is one whose terms all have the same degree  length . If U1, U2, V1, V2 are homogeneous string polynomials with U1V1 = U2V2 and deg V1  ≥ deg V2 , show that there is a homogeneous string polynomial U such that U2 = U1U and V1 = U V2.  d  Given that U and V are homogeneous string polynomials with U V = V U, prove that there is a homogeneous string polynomial W such that U = rW m, V = sW n for some integers m, n and rational numbers r, s. Give an algorithm to compute such a W having the largest possible degree.  This algorithm is of interest, for example, when U = α and V = β are strings satisfying αβ = βα; then W is simply a string γ. When U = xm and V = xn, the solution of largest degree is the string W = xgcd m,n , so this algorithm includes a gcd algorithm for integers as a special case.    cid:120  18. [M24]  Euclidean algorithm for string polynomials.  Let V1 and V2 be string  polynomials, not both zero, having a common left multiple.  This means that there exist string polynomials U1 and U2, not both zero, such that U1V1 = U2V2.  The purpose of this exercise is to find an algorithm to compute their greatest common right divisor gcrd V1, V2  and their least common left multiple lclm V1, V2 . The latter quantities are defined as follows: gcrd V1, V2  is a common right divisor of V1 and V2  that is, V1 = W1 gcrd V1, V2  and V2 = W2 gcrd V1, V2  for some W1 and W2 , and any common right divisor of V1 and V2 is a right divisor of gcrd V1, V2 ; lclm V1, V2  = Z1V1 = Z2V2 for some Z1 and Z2, and any common left multiple of V1 and V2 is a left multiple of lclm V1, V2 . For example, let U1 = abbbab + abbab − bbab + ab − 1, V1 = babab + abab + ab − b; U2 = abb + ab − b, V2 = babbabab + bababab + babab + abab − babb − 1. Then we have U1V1 = U2V2 = abbbabbabab + abbabbabab + abbbababab + abbababab − bbabbabab + abbbabab − bbababab + 2abbabab − abbbabb + ababab − abbabb − bbabab − babab + bbabb − abb − ab + b. For these string polynomials it can be shown that gcrd V1, V2  = ab + 1, and lclm V1, V2  = U1V1. The division algorithm of exercise 17 may be restated thus: If V1 and V2 are string polynomials, with V2 ̸= 0, and if U1 ̸= 0 and U2 satisfy the equation U1V1 = U2V2, then there exist string polynomials Q and R such that  V1 = QV2 + R,  where deg R  < deg V2 .  It follows readily that Q and R are uniquely determined; they do not depend on the given U1 and U2. Furthermore the result is right-left symmetric, in the sense that where deg R′  = deg U1  − deg V2  + deg R  < deg U1 .  U2 = U1Q + R′,   438  ARITHMETIC  4.6.1  Show that this division algorithm can be extended to an algorithm that computes lclm V1, V2  and gcrd V1, V2 ; in fact, the extended algorithm finds string polynomials Z1 and Z2 such that Z1V1 + Z2V2 = gcrd V1, V2 . [Hint: Use auxiliary variables u1, u2, v1, v2, w1, w2, w′1, w′2, z1, z2, z′1, z′2, whose values are string polynomials; start by setting u1 ← U1, u2 ← U2, v1 ← V1, v2 ← V2, and throughout the algorithm maintain the conditions  U1w1 + U2w2 = u1, U1w′1 + U2w′2 = u2, u1z1 − u2z′1 =  −1 nU1, −u1z2 + u2z′2 =  −1 nU2,  z1V1 + z2V2 = v1, z′1V1 + z′2V2 = v2, w1v1 − w′1v2 =  −1 nV1, −w2v1 + w′2v2 =  −1 nV2  4  and   4 2 2  at the nth iteration. This might be regarded as the “ultimate” extension of Euclid’s algorithm.] 19. [M39]  Common divisors of square matrices.  Exercise 18 shows that the concept of greatest common right divisor can be meaningful when multiplication is not commu- tative. Prove that any two n× n matrices A and B of integers have a greatest common right matrix divisor D. [Suggestion: Design an algorithm whose inputs are A and B, and whose outputs are integer matrices D, P , Q, X, Y, where A = P D, B = QD, and D = XA + Y B.] Find a greatest common right divisor of the matrices   1 3 1 . 3 20. [M40] Investigate approximate polynomial gcds and the accuracy of Euclid’s al- gorithm: What can be said about calculation of the greatest common divisor of poly- nomials whose coefficients are floating point numbers? 21. [M25] Prove that the computation time required by Algorithm C to compute the gcd of two nth degree polynomials over the integers is O n4 log N n 2 , if the coefficients of the given polynomials are bounded by N in absolute value. 22. [M23] Prove Sturm’s theorem. [Hint: Some sign sequences are impossible.] 23. [M22] Prove that if u x  in  29  has deg u  real roots, then we have deg uj+1  = deg uj  − 1 for 0 ≤ j ≤ k. 24. [M21] Show that  19  simplifies to  20  and  23  simplifies to  24 . 25. [M24]  W. S. Brown.  Prove that all the polynomials uj x  in  16  for j ≥ 3 are multiples of gcd ℓ u , ℓ v  , and explain how to improve Algorithm C accordingly.   cid:120  26. [M26] The purpose of this exercise is to give an analog for polynomials of the fact  that continued fractions with positive integer entries give the best approximations to real numbers  exercise 4.5.3–42 .  Let u x  and v x  be polynomials over a field, with deg u  > deg v , and let a1 x , a2 x , . . . be the quotient polynomials when Euclid’s algorithm is applied to u x  and v x . For example, the sequence of quotients in  5  and  6  is 9x2 +7, 5x2 +5, 6x3 + 5x2 + 6x + 5, 9x + 12. We wish to show that the convergents pn x  qn x  of the continued fraction   a1 x , a2 x , . . .    are the “best approximations” of low degree to the rational function v x  u x , where we have pn x  = Kn−1 a2 x , . . . , an x   and qn x  = Kn a1 x , . . . , an x   in terms of the continuant polynomials of Eq. 4.5.3– 4 . By convention, we let p0 x  = q−1 x  = 0, p−1 x  = q0 x  = 1. Prove that if p x  and q x  are polynomials such that deg q  < deg qn  and deg pu − qv  ≤ deg pn−1u − qn−1v , for some n ≥ 1, then p x  = cpn−1 x  and q x  = cqn−1 x  for some constant c. In particular, each qn x  is a “record-breaking” polynomial in the sense that no nonzero polynomial q x  of smaller degree can make   4.6.2  FACTORIZATION OF POLYNOMIALS  439  the quantity p x u x  − q x v x , for any polynomial p x , achieve a degree as small as pn x u x  − qn x v x . 27. [M23] Suggest a way to speed up the division of u x  by v x  when we know in advance that the remainder will be zero.  *4.6.2. Factorization of Polynomials Let us now consider the problem of factoring polynomials, not merely finding the greatest common divisor of two or more of them. Factoring modulo p. As in the case of integer numbers  Sections 4.5.2, 4.5.4 , the problem of factoring seems to be more difficult than finding the greatest common divisor. But factorization of polynomials modulo a prime integer p is not as hard to do as we might expect. It is much easier to find the factors of an arbitrary polynomial of degree n, modulo 2, than to use any known method to find the factors of an arbitrary n-bit binary number. This surprising situation is a consequence of an instructive factorization algorithm discovered in 1967 by Elwyn R. Berlekamp [Bell System Technical J. 46  1967 , 1853–1859].  Let p be a prime number; all arithmetic on polynomials in the following discussion will be done modulo p. Suppose that someone has given us a polyno- mial u x , whose coefficients are chosen from the set {0, 1, . . . , p − 1}; we may assume that u x  is monic. Our goal is to express u x  in the form   1    2   u x  = p1 x e1 . . . pr x er ,  where p1 x , . . . , pr x  are distinct, monic, irreducible polynomials.  As a first step, we can use a standard technique to determine whether any  of the exponents e1, . . . , er are greater than unity. If  u x  = unxn + ··· + u0 = v x 2w x ,  gcdu x , u′ x  = d x .  then the derivative  formed in the usual way, but modulo p  is  u′ x  = nunxn−1 + ··· + u1 = 2v x v′ x w x  + v x 2w′ x ,   3  and this is a multiple of the squared factor v x . Therefore our first step in factoring u x  is to form   4  If d x  is equal to 1, we know that u x  is squarefree, the product of distinct primes p1 x  . . . pr x . If d x  is not equal to 1 and d x  ̸= u x , then d x  is a proper factor of u x ; the relation between the factors of d x  and the factors of u x  d x  speeds up the factorization process nicely in this case  see exercises 34 and 36 . Finally, if d x  = u x , we must have u′ x  = 0; hence the coefficient uk of xk is nonzero only when k is a multiple of p. This means that u x  can be written as a polynomial of the form v xp , and in such a case we have   5  the factorization process can be completed by finding the irreducible factors of v x  and raising them to the pth power.  u x  = v xp  =v x p;   440  ARITHMETIC  4.6.2  Identity  5  may appear somewhat strange to the reader; it is an important fact that is basic to Berlekamp’s algorithm and to several other methods we shall discuss. We can prove it as follows: If v1 x  and v2 x  are any polynomials modulo p, then  v1 x + v2 x p = v1 x p +p since the binomial coefficientsp  v1 x p−1v2 x +···+ p , . . . , p  v1 x v2 x p−1 + v2 x p  are all multiples of p. Furthermore  if a is any integer, we have ap ≡ a  modulo p  by Fermat’s theorem. Therefore when v x  = vmxm + vm−1xm−1 + ··· + v0, we find that  = v1 x p + v2 x p,  p−1  p−1  1  1  v x p =  vmxm p +  vm−1xm−1 p + ··· +  v0 p  = vmxmp + vm−1x m−1 p + ··· + v0 = v xp .  The remarks above show that the problem of factoring a polynomial reduces to the problem of factoring a squarefree polynomial. Let us therefore assume that  6  is the product of distinct primes. How can we be clever enough to discover the pj x ’s when only u x  is given? Berlekamp’s idea is to make use of the Chinese remainder theorem, which is valid for polynomials just as it is valid for integers  see exercise 3 . If  s1, s2, . . . , sr  is any r-tuple of integers mod p, the Chinese remainder theorem implies that there is a unique polynomial v x  such that  u x  = p1 x p2 x  . . . pr x   modulo p1 x ,  v x  ≡ s1  modulo pr x ,  v x  ≡ sr  . . . ,   7   deg v  < deg p1  + deg p2  + ··· + deg pr  = deg u .  The notation “g x  ≡ h x  modulo f x ” that appears here has the same meaning as “g x  ≡ h x  modulo f x  and p” in exercise 3.2.2–11, since we gcdu x , v x  − s1  are considering polynomial arithmetic modulo p. The polynomial v x  in  7  gives us a way to get at the factors of u x , for if r ≥ 2 and s1 ̸= s2, we will have Since this observation shows that we can get information about the factors of u x  from appropriate solutions v x  of  7 , let us analyze  7  more closely. In the first place we can observe that the polynomial v x  satisfies the condition v x p ≡ sp   divisible by p1 x  but not by p2 x .  j = sj ≡ v x modulo pj x  for 1 ≤ j ≤ r; therefore v x p ≡ v x  modulo u x , xp − x ≡  x − 0  x − 1  . . .x −  p − 1   modulo p  v x p − v x  =v x  − 0v x  − 1 . . .v x  −  p − 1    10  is an identity for any polynomial v x , when we are working modulo p. If v x  satisfies  8 , it follows that u x  divides the left-hand side of  10 , so every  In the second place we have the basic polynomial identity   see exercise 6 ; hence  deg v  < deg u .   8    9    4.6.2  FACTORIZATION OF POLYNOMIALS  441  where  irreducible factor of u x  must divide one of the p relatively prime factors of the right-hand side of  10 . In other words, all solutions of  8  must have the form of  7 , for some s1, s2, . . . , sr; there are exactly pr solutions of  8 .  The solutions v x  to congruence  8  therefore provide a key to the factor- ization of u x . It may seem harder to find all solutions to  8  than to factor u x  in the first place, but in fact this is not true, since the set of solutions to  8  is closed under addition. Let deg u  = n; we can construct the n × n matrix   q0,0  ...  Q =  q0,1  . . .  q0,n−1  ...  qn−1,0  qn−1,1  . . .  qn−1,n−1    ... modulo u x .  xpk ≡ qk,n−1xn−1 + ··· + qk,1x + qk,0  Then v x  = vn−1xn−1 + ··· + v1x + v0 is a solution to  8  if and only if   v0, v1, . . . , vn−1 Q =  v0, v1, . . . , vn−1 ;    vjxj =  for the latter equation holds if and only if  vkqk,jxj ≡  v x  = vkxpk = v xp  ≡ v x p modulo u x . gcdu x , u′ x  ̸= 1, reduce the problem of factoring u x , as stated earlier  B1. [Remove duplicate factors.] Ensure that u x  is squarefree; in other words, if  Berlekamp’s factoring algorithm therefore proceeds as follows:  k  k  j  j   11    12    13   in this section.  B2. [Get Q.] Form the matrix Q defined by  11  and  12 . This can be done in  different ways, depending on the size of p, as explained below.  B3. [Find null space.] “Triangularize” the matrix Q − I, where I =  δij  is the n× n identity matrix, finding its rank n− r and finding linearly independent  vectors v[1], . . . , v[r] such that v[j] Q− I  =  0, 0, . . . , 0  for 1 ≤ j ≤ r. The column operations, as explained in Algorithm N below. At this point, r is  first vector v[1] may always be taken as  1, 0, . . . , 0 , representing the trivial solution v[1] x  = 1 to  8 . The computation can be done using appropriate  the number of irreducible factors of u x , because the solutions to  8  are the pr polynomials corresponding to the vectors t1v[1] + ··· + trv[r] for all choices of integers 0 ≤ t1, . . . , tr < p. Therefore if r = 1 we know that u x  is irreducible, and the procedure terminates.  B4. [Split.] Calculate gcdu x , v[2] x  − s for 0 ≤ s < p, where v[2] x  is  the polynomial represented by vector v[2]. The result will be a nontrivial factorization of u x , because v[2] x − s is nonzero and has degree less than deg u , and by exercise 7 we have  u x  =   gcdv x  − s, u x    14   whenever v x  satisfies  8 .  0≤s<p   442  ARITHMETIC  4.6.2  further factors can be obtained by calculating gcdv[k] x  − s, w x  for are obtained. If we choose si ̸= sj in  7 , we obtain a solution v x  to  8  factors.  If the use of v[2] x  does not succeed in splitting u x  into r factors, 0 ≤ s < p and all factors w x  found so far, for k = 3, 4, . . . , until r factors that distinguishes pi x  from pj x ; some v[k] x  − s will be divisible by pi x  and not by pj x , so this procedure will eventually find all of the  If p is 2 or 3, the calculations of this step are quite efficient; but if p is more than 25, say, there is a much better way to proceed, as we shall see later.  Historical notes: M. C. R. Butler [Quart. J. Math. 5  1954 , 102–107] observed that the matrix Q−I corresponding to a squarefree polynomial with r irreducible factors will have rank n − r, modulo p. Indeed, this fact was implicit in a more general result of K. Petr [Časopis pro Pěstování Matematiky a Fysiky 66  1937 , 85–94], who determined the characteristic polynomial of Q. See also Š. Schwarz, Quart. J. Math. 7  1956 , 110–124.  As an example of Algorithm B, let us now determine the factorization of  u x  = x8 + x6 + 10x4 + 10x3 + 8x2 + 2x + 8   15  modulo 13.  This polynomial appears in several of the examples in Section 4.6.1.   A quick calculation using Algorithm 4.6.1E shows that gcdu x , u′ x  = 1;  therefore u x  is squarefree, and we turn to step B2. Step B2 involves calculating the Q matrix, which in this case is an 8 × 8 array. The first row of Q is always  1, 0, 0, . . . , 0 , representing the polynomial x0 mod u x  = 1. The second row represents x13 mod u x , and, in general, xk mod u x  may readily be determined as follows  for relatively small values of k : If  u x  = xn + un−1xn−1 + ··· + u1x + u0  xk ≡ ak,n−1xn−1 + ··· + ak,1x + ak,0  modulo u x ,  xk+1 ≡ ak,n−1xn + ··· + ak,1x2 + ak,0x  ≡ ak,n−1 −un−1xn−1 − ··· − u1x − u0  + ak,n−2xn−1 + ··· + ak,0x = ak+1,n−1xn−1 + ··· + ak+1,1x + ak+1,0,  ak+1,j = ak,j−1 − ak,n−1uj.   16  In this formula ak,−1 is treated as zero, so that ak+1,0 = −ak,n−1u0. The simple “shift register” recurrence  16  makes it easy to calculate xk mod u x  for k = 1, 2, 3, . . . ,  n−1 p. Inside a computer, this calculation is of course generally done by maintaining a one-dimensional array  an−1, . . . , a1, a0  and repeatedly setting  t ← an−1, an−1 ←  an−2 − tun−1  mod p, . . . , a1 ←  a0 − tu1  mod p,  and if  then  where   443  4.6.2  and a0 ←  −tu0  mod p. We have seen similar procedures in connection with random number generation, 3.2.2– 10 . For the example polynomial u x   FACTORIZATION OF POLYNOMIALS  in  15 , we obtain the following sequence of coefficients of xk mod u x , using arithmetic modulo 13: ak,7 0 0 0 0 0 0 0 1 0 12 0 4 3 11  ak,4 0 0 0 0 1 0 0 0 3 3 2 8 12 10  ak,0 1 0 0 0 0 0 0 0 5 0 8 0 7 2  ak,2 0 0 1 0 0 0 0 0 5 11 0 2 2 7  ak,1 0 1 0 0 0 0 0 0 11 5 2 8 5 1  ak,3 0 0 0 1 0 0 0 0 3 5 8 0 1 11  ak,5 0 0 0 0 0 1 0 0 0 3 3 2 8 12  ak,6 0 0 0 0 0 0 1 0 12 0 4 3 11 5  k 0 1 2 3 4 5 6 7 8 9 10 11 12 13  0  0  Q =  Therefore the second row of Q is  2, 1, 7, 11, 10, 12, 5, 11 . Similarly we may determine x26 mod u x , . . . , x91 mod u x , and we find that 0 11 2 3 11 9 12 12 0 11 2 3 11 9 12 11  0 0 7 11 10 12 0 4 4 3 1 6 6 5 3 1 8 8 2 8 7 6 0 11 7 10 0 11 5 12 0 0 0 0 7 11 10 12 0 4 3 3 1 6 6 4 2 1 8 8 2 8 6 6 0 11 7 10 12 5 0 11  0 1 1 2 6 3 4 3 2 11 6 11 5 11 3 3 0 0 2 0 6 3 4 3 2 11 6 11 5 11 3 3   ,  .  0 5 7 2 3 10 7 9 0 5 7 2 3 10 6 9     Q − I =   17   That finishes step B2; the next step of Berlekamp’s procedure requires In general, suppose that A is an n × n finding the “null space” of Q − I. matrix over a field, whose rank n − r is to be determined; suppose further that we wish to determine linearly independent vectors v[1], v[2], . . . , v[r] such that v[1]A = v[2]A = ··· = v[r]A =  0, . . . , 0 . An algorithm for this calculation can be based on the observation that any column of A may be multiplied by a nonzero quantity, and any multiple of one of its columns may be added to a different column, without changing the rank or the vectors v[1], . . . , v[r].  These   444  ARITHMETIC  4.6.2  transformations amount to replacing A by AB, where B is a nonsingular matrix.  The following well-known “triangularization” procedure may therefore be used. Algorithm N  Null space algorithm . Let A be an n × n matrix, whose elements aij belong to a field and have subscripts in the range 0 ≤ i, j < n. This algorithm outputs r vectors v[1], . . . , v[r], which are linearly independent over the field and satisfy v[j]A =  0, . . . , 0 , where n − r is the rank of A. N1. [Initialize.] Set c0 ← c1 ← ··· ← cn−1 ← −1, r ← 0.   During the calculation we will have cj ≥ 0 only if acj j = −1 and all other entries of row cj are zero.  N2. [Loop on k.] Do step N3 for k = 0, 1, . . . , n − 1, then terminate the  N3. [Scan row for dependence.] If there is some j in the range 0 ≤ j < n such that akj ̸= 0 and cj < 0, then do the following: Multiply column j of A by −1 akj  so that akj becomes equal to −1 ; then add aki times column j to column i for all i ̸= j; finally set cj ← k.  Since it is not difficult to show that asj = 0 for all s < k, these operations have no effect on rows 0, 1, . . . , k − 1 of A.  On the other hand, if there is no j in the range 0 ≤ j < n such that akj ̸= 0 and cj < 0, then set r ← r + 1 and output the vector  algorithm.  defined by the rule  v[r] =  v0, v1, . . . , vn−1    aks,  1, 0,  vj =  if cs = j ≥ 0; if j = k; otherwise.   18   An example will reveal the mechanism of this algorithm. Let A be the matrix Q − I of  17  over the field of integers modulo 13. When k = 0, we output the vector v[1] =  1, 0, 0, 0, 0, 0, 0, 0 . When k = 1, we may take j in step N3 to be either 0, 2, 3, 4, 5, 6, or 7; the choice here is completely arbitrary, although it affects the particular vectors that are chosen to be output by the algorithm. For hand calculation, it is most convenient to pick j = 5, since a15 = 12 = −1 already; the column operations of step N3 then change A to the matrix    0 0 11 3 4 5 1 12  0 0 0 0 6 5 3 9 11 2 11 11 11 6 3 11   cid:7  cid:4  12 cid:6  cid:5  0  0  0 0 8 5 6 7 1 9  0  0 0 4 1 6 9 1 12 10 6 6 11 6 11  1 6 8 1 9 12   .  0 0 7 4 9 10 3 2   The circled element in column “5”, row “1”, is used here to indicate that c5 = 1. Remember that Algorithm N numbers the rows and columns of the matrix starting with 0, not 1.  When k = 2, we may choose j = 4 and proceed   FACTORIZATION OF POLYNOMIALS  445  4.6.2     0 0 0 8 2 12 0 11  0 0 0 0 0 1 8 1  in a similar way, obtaining the following matrices, which all have the same null space as Q − I:  k = 2 0 0 0 0 0 0 0 0 3 11 1 7 0 5 5 2 7 0  4 1 3 7 7   cid:7  cid:4  12 cid:6  cid:5  0  cid:7  cid:4  12 cid:6  cid:5  0  0  0  0 9 10 9 5 5 4 3 0 0 6  0 0 0 6 3 5 0 12  0 0 0 1 4 3 1 6  0  0   cid:7  cid:4  12 cid:6  cid:5  0  cid:7  cid:4  12 cid:6  cid:5  0  0 0 0  k = 4 0 0 0 0 0 0 0 0 0 0 4 11 4 6 10 11 11 4 11 0  0 0 4  2  0 0 0 0   cid:7  cid:4  12 cid:6  cid:5   0 0 0 0 0 0 9 0 10  0 0 0   cid:7  cid:4  12 cid:6  cid:5  0  0 10 2 6        0 0 0   cid:7  cid:4  12 cid:6  cid:5  0  0 0 0  0 0 0 0 9 9 1 10 5 12 2 7  8 4 12 2  0 0 0  0 0 0  0 0 0 0 0   cid:7  cid:4  12 cid:6  cid:5  0  cid:7  cid:4  12 cid:6  cid:5  0  0  5 12  0 9  0 0 0 0   cid:7  cid:4  12 cid:6  cid:5  0  cid:7  cid:4  12 cid:6  cid:5  0  0  0  0 0 8 0 6 11  0 0 0 0 0 0  0 8 4 4 11  0 0 0 5 9   cid:7  cid:4  12 cid:6  cid:5  0  cid:7  cid:4  12 cid:6  cid:5  0  0  0  k = 3 0 0 0 0 0 0 9 11 7 12  0 11 4 3 9  k = 5 0 0 0 0 0 0 0 0 0 0  0 0 0 5 11  0 0 0 0 5 0 7 2     0 0 0 0   cid:7  cid:4  12 cid:6  cid:5   0 9 10  Now every column that has no circled entry is completely zero; so when k = 6 and k = 7 the algorithm outputs two more vectors, namely  v[2] =  0, 5, 5, 0, 9, 5, 1, 0 ,  v[3] =  0, 9, 11, 9, 10, 12, 0, 1 .  Finally we can go to step B4 of the factoring procedure. The calculation of  From the form of matrix A after k = 5, it is evident that these vectors satisfy the equation vA =  0, . . . , 0 . Since the computation has produced three linearly independent vectors, u x  must have exactly three irreducible factors.  gcdu x , v[2] x  − s for 0 ≤ s < 13, where v[2] x  = x6 + 5x5 + 9x4 + 5x2 + 5x, two of the three factors. Turning to gcdv[3] x  − s, x5 + 5x4 + 9x3 + 5x + 5,  gives x5 + 5x4 + 9x3 + 5x + 5 as the answer when s = 0, and x3 + 8x2 + 4x + 12 when s = 2; the gcd is unity for other values of s. Therefore v[2] x  gives us only  where v[3] x  = x7+12x5+10x4+9x3+11x2+9x, we obtain the factor x4+2x3+ 3x2 + 4x + 6 when s = 6, x + 3 when s = 8, and unity otherwise. Thus the complete factorization is  u x  =  x4 + 2x3 + 3x2 + 4x + 6  x3 + 8x2 + 4x + 12  x + 3 .   19  Let us now estimate the running time of Berlekamp’s method when an nth degree polynomial is factored modulo p. First assume that p is relatively small, so that the four arithmetic operations can be done modulo p in essentially a fixed length of time.  Division modulo p can be converted to multiplication, by storing a table of reciprocals as suggested in exercise 9; for example, when working modulo 13, we have 1 3 = 9, etc.  The computation in step B1 takes O n2   2 = 7, 1   446  ARITHMETIC  4.6.2  units of time; step B2 takes O pn2 . For step B3 we use Algorithm N, which requires O n3  units of time at most. Finally, in step B4 we can observe that  the calculation of gcdf x , g x  by Euclid’s algorithm takes Odeg f  deg g  units of time; hence the calculation of gcdv[j] x  − s, w x  for fixed j and s  and for all factors w x  of u x  found so far takes O n2  units. Step B4 therefore requires O prn2  units of time at most. Berlekamp’s procedure factors an arbitrary polynomial of degree n, modulo p, in O n3 + prn2  steps, when p is a small prime; and exercise 5 shows that the average number of factors, r, is approximately ln n. Thus the algorithm is much faster than any known methods of factoring n-digit numbers in the p-ary number system.  Of course, when n and p are small, a trial-and-error factorization procedure analogous to Algorithm 4.5.4A will be even faster than Berlekamp’s method. Exercise 1 implies that it is a good idea to cast out factors of small degree first when p is small, before going to any more complicated procedure, even when n is large.  When p is large, a different implementation of Berlekamp’s procedure would be used for the calculations. Division modulo p would not be done with an auxiliary table of reciprocals; instead the method of exercise 4.5.2–16, which  takes O log p 2 steps, would probably be used. Then step B1 would take On2 log p 2 units of time; similarly, step B3 would take On3 log p 2. In step  B2, we can form xp mod u x  in a more efficient way than  16  when p is large: Section 4.6.3 shows that this value can be obtained by essentially using O log p  operations of squaring mod u x , going from xk mod u x  to x2k mod u x , to- gether with the operation of multiplying by x. The squaring operation is rel- atively easy to perform if we first make an auxiliary table of xm mod u x  for m = n, n + 1, . . . , 2n − 2; if xk mod u x  = cn−1xn−1 + ··· + c1x + c0, then  n−1x2n−2 + ··· +  c1c0 + c1c0 x + c2 0  where x2n−2, . . . , xn can be replaced by polynomials in the auxiliary table. The  total time to compute xp mod u x  comes to On2 log p 3 units, and we obtain analogous to squaring mod u x ; step B2 is completed in On3 log p 2 additional units of time. Thus steps B1, B2, and B3 take a total of On2 log p 3+n3 log p 2  the second row of Q. To get further rows of Q, we can compute x2p mod u x , x3p mod u x , . . . , simply by multiplying repeatedly by xp mod u x , in a fashion  time units; these three steps tell us the number of factors of u x .  But when p is large and we get to step B4, we are asked to calculate a greatest common divisor for p different values of s, and that is out of the question if p is even moderately large. This hurdle was first surmounted by Hans Zassenhaus [J. Number Theory 1  1969 , 291–311], who showed how to determine all of the “useful” values of s  see exercise 14 ; but an even better way to proceed was found by Zassenhaus and Cantor in 1980. If v x  is any solution to  8 , we know  that u x  divides v x p − v x  = v x ·v x  p−1  2 + 1·v x  p−1  2 − 1. This  x2k mod u x  =c2   mod u x ,  suggests that we calculate  gcdu x , v x  p−1  2 − 1;   20    4.6.2  FACTORIZATION OF POLYNOMIALS  447  ,  2p  1 +         r  with a little bit of luck,  20  will be a nontrivial factor of u x . In fact, we can determine exactly how much luck is involved, by considering  7 . Let v x  ≡ sj  modulo pj x   for 1 ≤ j ≤ r; then pj x  divides v x  p−1  2 − 1 if and only if  p−1  2 ≡ 1  modulo p . We know that exactly  p− 1  2 of the integers s in the s range 0 ≤ s < p satisfy s p−1  2 ≡ 1  modulo p , hence about half of the pj x  j will appear in the gcd  20 . More precisely, if v x  is a random solution of  8 , where all pr solutions are equally likely, the probability that the gcd  20  equals u x  is exactly  and the probability that it equals 1 is  p + 1  2pr. The probability that a 1 − p − 1 ≥ 4 9 ,  2p for all r ≥ 2 and p ≥ 3. It is therefore a good idea to replace step B4 by the following procedure, unless p is quite small: Set v x  ← a1v[1] x  + a2v[2] x  + ··· + arv[r] x , where the coefficients aj are randomly chosen in the range 0 ≤ aj < p. Let the current partial factorization of u x  be u1 x  . . . ut x  where t is initially 1. Compute   p − 1  2pr  r   nontrivial factor will be obtained is therefore  r = 1 − 1  r − p + 1  the value of t, whenever a nontrivial gcd is found. Repeat this process for different choices of v x  until t = r.  for all i such that deg ui  > 1; replace ui x  by gi x ·ui x  gi x  and increase this alternative to step B4. It takes Orn log p 2 steps to compute v x ; and if deg ui  = d, it takes Od2 log p 3 steps to compute v x  p−1  2 mod ui x  and Od2 log p 2 further steps to compute gcdui x , v x  p−1  2 − 1. Thus the total time is O n2 log p 3 log r.  If we assume  as we may  that only O log r  random solutions v x  to  8  will be needed, we can give an upper bound on the time required to perform  gi x  = gcdui x , v x  p−1  2 − 1  p−4 + ···  p−2 +  2r−1  2  4  Distinct-degree factorization. We shall now turn to a somewhat simpler way to find factors modulo p. The ideas we have studied so far in this section involve many instructive insights into computational algebra, so the author does not apologize to the reader for presenting them; but it turns out that the problem of factorization modulo p can actually be solved without relying on so many concepts. In the first place we can make use of the fact that an irreducible polynomial q x  of degree d is a divisor of xpd − x, and it is not a divisor of xpc − x for 1 ≤ c < d; see exercise 16. We can therefore cast out the irreducible factors of each degree separately, by adopting the following strategy. D1. [Go squarefree.] Rule out squared factors, as in Berlekamp’s method. Also set v x  ← u x , w x  ← “x”, and d ← 0.  Here v x  and w x  are variables that have polynomials as values.    448  ARITHMETIC  4.6.2  D2. [If not done, take pth power.]  At this point w x  = xpd mod v x ; all of the irreducible factors of v x  are distinct and have degree > d.  If d + 1 > 1 2 deg v , the procedure terminates since we either have v x  = 1 or v x  is irreducible. Otherwise increase d by 1 and replace w x  by w x p mod v x .  D3. [Extract factors.] Find gd x  = gcdw x  − x, v x . This is the product of all the irreducible factors of u x  whose degree is d. If gd x  ̸= 1, replace  v x  by v x  gd x  and w x  by w x  mod v x ; and if the degree of gd x  is greater than d, use the algorithm below to find its factors. Return to step D2. This procedure determines the product of all irreducible factors of each degree d, and therefore it tells us how many factors there are of each degree. Since the three factors of our example polynomial  19  have different degrees, they would all be discovered without any need to factorize the polynomials gd x . To complete the method, we need a way to split the polynomial gd x  into its irreducible factors when deg gd  > d. Michael Rabin pointed out in 1976 that this can be done by doing arithmetic in the field of pd elements. David G. Cantor and Hans Zassenhaus discovered in 1979 that there is an even simpler way to proceed, based on the following identity: If p is any odd prime, we have  gd x  = gcdgd x , t x  gcdgd x , t x  pd−1  2 +1 gcdgd x , t x  pd−1  2−1 exercise 16.  Now exercise 29 shows that gcdgd x , t x  pd−1  2 − 1 will be a   21  for all polynomials t x , since t x pd − t x  is a multiple of all irreducible poly- nomials of degree d.  We may regard t x  as an element of the field of size pd, when that field consists of all polynomials modulo an irreducible f x  as in  nontrivial factor of gd x  about 50 percent of the time, when t x  is a random polynomial of degree ≤ 2d − 1; hence we will not need many random trials to discover all of the factors. We may assume without loss of generality that t x  is monic, since integer multiples of t x  make no difference except possibly to change t x  pd−1  2 into its negative. Thus in the case d = 1, we can take t x  = x + s, where s is chosen at random.  Sometimes this procedure will in fact succeed for d > 1 when only linear polynomials t x  are used. For example, there are eight irreducible polynomials f x  of degree 3, modulo 3, and they will all be distinguished by calculating  gcdf x ,  x + s 13 − 1 for 0 ≤ s < 3:  f x   2x + 1 x3 + 2x + 2 x3 + x3 + x2 + 2 x3 + x2 + x + 2 x3 + x2 + 2x + 1 x3 + 2x2 + 1 x3 + 2x2 + x + 1 x3 + 2x2 + 2x + 2  s = 0  1  f x  f x  f x   1 1 1  f x   s = 1  s = 2  f x  f x   f x  f x   1  1  1 1  f x   f x  f x   f x   1  1  1  1   4.6.2  FACTORIZATION OF POLYNOMIALS  449  Exercise 31 contains a partial explanation of why linear polynomials can be effec- tive. But when there are more than 2p irreducible polynomials of degree d, some irreducibles must exist that cannot be distinguished by linear choices of t x .  An alternative to  21  that works when p = 2 is discussed in exercise 30. Faster algorithms for distinct-degree factorization when p is very large have been found by J. von zur Gathen, V. Shoup, and E. Kaltofen; the running time is O n2+ϵ + n1+ϵ log p  arithmetic operations modulo p for numbers of practical size, and O n 5+ω+ϵ  4 log p  such operations as n → ∞, when ω is the exponent of “fast” matrix multiplication in exercise 4.6.4–66. [See Computational Com- plexity 2  1992 , 187–224; J. Symbolic Comp. 20  1995 , 363–397; Math. Comp. 67  1998 , 1179–1197.]  Historical notes: The idea of finding all the linear factors of a squarefree  polynomial f x  modulo p by first calculating g x  = gcdxp−1 − 1, f x  and then calculating gcdg x ,  x + s  p−1  2 ± 1 for arbitrary s is due to A. M.  Legendre, Mémoires Acad. Sci.  Paris, 1785 , 484–490; his motive was to find all of the integer solutions to Diophantine equations of the form f x  = py, that is, f x  ≡ 0  modulo p . The more general degree-separation technique embodied in Algorithm D was discovered by C. F. Gauss before 1800, but not published [see his Werke 2  1876 , 237], and then by Évariste Galois in the now-classic paper that launched the theory of finite fields [Bulletin des Sciences Mathématiques, Physiques et Chimiques 13  1830 , 428–435; reprinted in J. de Math. Pures et Appliquées 11  1846 , 398–407]. However, this work of Gauss and Galois was ahead of its time, and not well understood until J. A. Serret gave a detailed exposition somewhat later [Mémoires Acad. Sci., series 2, 35  Paris, 1866 , 617–688; Algorithm D is in §7]. Special procedures for splitting gd x  into irreducible factors were devised subsequently by various authors, but methods of full generality that would work efficiently for large p were apparently not discovered until the advent of computers made them desirable. The first such randomized algorithm with a rigorously analyzed running time was published by E. Berlekamp [Math. Comp. 24  1970 , 713–735]; it was refined and simplified by Robert T. Moenck [Math. Comp. 31  1977 , 235–250], M. O. Rabin [SICOMP 9  1980 , 273–280], D. G. Cantor and H. J. Zassenhaus [Math. Comp. 36  1981 , 587–592]. Paul Camion independently found a generalization to special classes of multivariate polynomials [Comptes Rendus Acad. Sci. A291  Paris, 1980 , 479–482; IEEE Trans. IT-29  1983 , 378–385].  The average number of operations needed to factor a random polynomial mod p has been analyzed by P. Flajolet, X. Gourdon, and D. Panario, Lecture Notes in Comp. Sci. 1099  1996 , 232–243. Factoring over the integers. It is somewhat more difficult to find the complete factorization of polynomials with integer coefficients when we are not working modulo p, but some reasonably efficient methods are available for this purpose. Isaac Newton gave a method for finding linear and quadratic factors of polynomials with integer coefficients in his Arithmetica Universalis  1707 . His method was extended by N. Bernoulli in 1708 and, more explicitly, by an as-   450  ARITHMETIC  4.6.2  tronomer named Friedrich von Schubert in 1793, who showed how to find all factors of degree n in a finite number of steps; see M. Mignotte and D. Ştefănescu, Revue d’Hist. Math. 7  2001 , 67–89. L. Kronecker rediscovered their approach independently, about 90 years later; but unfortunately the method is very ineffi- cient when n is five or more. Much better results can be obtained with the help of the “mod p” factorization methods presented above.  Suppose that we want to find the irreducible factors of a given polynomial  u x  = unxn + un−1xn−1 + ··· + u0,  un ̸= 0,  over the integers. As a first step, we can divide by the greatest common divisor of the coefficients; this leaves us with a primitive polynomial. We may also assume  that u x  is squarefree, by dividing out gcdu x , u′ x  as in exercise 34.  Now if u x  = v x w x , where each of these polynomials has integer coef- ficients, we obviously have u x  ≡ v x w x   modulo p  for all primes p, so there is a nontrivial factorization modulo p unless p divides ℓ u . An efficient algorithm for factoring u x  modulo p can therefore be used in an attempt to reconstruct possible factorizations of u x  over the integers.  For example, let  u x  = x8 + x6 − 3x4 − 3x3 + 8x2 + 2x − 5.   22   We have seen above in  19  that  u x  ≡  x4 +2x3 +3x2 +4x+6  x3 +8x2 +4x+12  x+3   modulo 13 ;  23  and the complete factorization of u x  modulo 2 shows one factor of degree 6 and another of degree 2  see exercise 10 . From  23  we can see that u x  has no factor of degree 2, so it must be irreducible over the integers.  This particular example was perhaps too simple; experience shows that most irreducible polynomials can be recognized as such by examining their factors modulo a few primes, but it is not always so easy to establish irreducibility. For example, there are polynomials that can be properly factored modulo p for all primes p, with consistent degrees of the factors, yet they are irreducible over the integers  see exercise 12 .  A large family of irreducible polynomials is exhibited in exercise 38, and exercise 27 proves that almost all polynomials are irreducible over the integers. But we usually aren’t trying to factor a random polynomial; there is probably some reason to expect a nontrivial factor or else the calculation would not have been attempted in the first place. We need a method that identifies factors when they are there.  In general if we try to find the factors of u x  by considering its behavior modulo different primes, the results will not be easy to combine. For example, if u x  is actually the product of four quadratic polynomials, we will have trouble matching up their images with respect to different prime moduli. Therefore it is desirable to stick to a single prime and to see how much mileage we can get out of it, once we feel that the factors modulo this prime have the right degrees.  One idea is to work modulo a very large prime p, big enough so that the coefficients in any true factorization u x  = v x w x  over the integers must   4.6.2 451 actually lie between −p 2 and p 2. Then all possible integer factors can be read off from the factors that we know how to compute mod p.  FACTORIZATION OF POLYNOMIALS  Exercise 20 shows how to obtain fairly good bounds on the coefficients of polynomial factors. For example, if  22  were reducible it would have a factor v x  of degree ≤ 4, and the coefficients of v would be at most 34 in magnitude by the results of that exercise. So all potential factors of u x  will be fairly evident if we work modulo any prime p > 68. Indeed, the complete factorization modulo 71 is   x + 12  x + 25  x2 − 13x − 7  x4 − 24x3 − 16x2 + 31x − 12 ,  and we see immediately that none of these polynomials could be a factor of  22  over the integers since the constant terms do not divide 5; furthermore there is no way to obtain a divisor of  22  by grouping two of these factors, since none of the conceivable constant terms 12 × 25, 12 ×  −7 , 12 ×  −12  is congruent to ±1 or ±5  modulo 71 .  Incidentally, it is not trivial to obtain good bounds on the coefficients of polynomial factors, since a lot of cancellation can occur when polynomials are multiplied. For example, the innocuous-looking polynomial xn−1 has irreducible factors whose coefficients exceed exp n1 lg lg n  for infinitely many n. [See R. C. Vaughan, Michigan Math. J. 21  1974 , 289–295.] The factorization of xn − 1 is discussed in exercise 32.  Instead of using a large prime p, which might need to be truly enormous if u x  has large degree or large coefficients, we can also make use of small p, pro- vided that u x  is squarefree mod p. For in this case, an important construction known as Hensel’s Lemma can be used to extend a factorization modulo p in a unique way to a factorization modulo pe for arbitrarily high exponents e  see exercise 22 . If we apply Hensel’s Lemma to  23  with p = 13 and e = 2, we obtain the unique factorization  u x  ≡  x − 36  x3 − 18x2 + 82x − 66  x4 + 54x3 − 10x2 + 69x + 84   2 . . 169   modulo 169 . Calling these factors v1 x v3 x v4 x , we see that v1 x  and v3 x  are not factors of u x  over the integers, nor is their product v1 x v3 x  when the coefficients have been reduced modulo 169 to the range  − 169 2  . Thus we have exhausted all possibilities, proving once again that u x  is irreducible over the integers — this time using only its factorization modulo 13.  The example we have been considering is atypical in one important respect: We have been factoring the monic polynomial u x  in  22 , so we could assume that all its factors were monic. What should we do if un > 1? In such a case, the leading coefficients of all but one of the polynomial factors can be varied almost arbitrarily modulo pe; we certainly don’t want to try all possibilities. Perhaps the reader has already noticed this problem. Fortunately there is a simple way out: The factorization u x  = v x w x  implies a factorization unu x  = v1 x w1 x  where ℓ v1  = ℓ w1  = un = ℓ u .  “Excuse me, do you mind if I multiply your polynomial by its leading coefficient before I factor it?”  We can proceed essentially as above, but using pe > 2B where B now bounds the maximum   452  ARITHMETIC  4.6.2  coefficient for factors of unu x  instead of u x . Another way to solve the leading coefficient problem is discussed in exercise 40.  Putting these observations all together results in the following procedure: F1. [Factor modulo a prime power.] Find the unique squarefree factorization  u x  ≡ ℓ u v1 x  . . . vr x   modulo pe ,  where pe is sufficiently large as explained above, and where the vj x  are monic.  This will be possible for all but a few primes p; see exercise 23.  Also set d ← 1.  2 r.  2 r.  2 pe . . 1  vi1 x  . . . vid x , with i1 = 1 if d = 1 ℓ u v x   modulo pe  whose coefficients all lie in the interval [− 1  If ¯v x  divides ℓ u u x , output the factor pp¯v x , divide u x  by this  F2. [Try the d-element subfactors.] For every combination of factors v x  = 2 r, form the unique polynomial ¯v x  ≡ 2 pe . factor, and remove the corresponding vi x  from the list of factors modulo pe; decrease r by the number of factors removed, and terminate if d > 1  F3. [Loop on d.] Increase d by 1, and return to F2 if d ≤ 1 At the conclusion of this process, the current value of u x  will be the final irreducible factor of the originally given polynomial. Notice that if u0 < un, it is preferable to do all of the work with the reverse polynomial u0xn +··· + un, whose factors are the reverses of the factors of u x . The procedure as stated requires pe > 2B, where B is a bound on the coefficients of any divisor of unu x , but we can use a much smaller value of B if we only guarantee it to be valid for divisors of degree ≤ 1 2 deg u . In this case the divisibility test in step F2 should be applied to w x  = v1 x  . . . vr x  v x  instead of v x , whenever deg v  > 1  We can decrease B still more if we decide to guarantee only that B should bound the coefficients of at least one proper divisor of u x .  For example, when we’re factoring a nonprime integer N instead of a polynomial, some of the divisors might be very large, but at least one will be ≤ √ N.  This idea, due to B. Beauzamy, V. Trevisan, and P. S. Wang [J. Symbolic Comp. 15  1993 , 393–413], is discussed in exercise 21. The divisibility test in step F2 must then be applied to both v x  and w x , but the computations are faster because pe is often much smaller. The algorithm above contains an obvious bottleneck: We may have to test as many as 2r−1 − 1 potential factors v x . The average value of 2r in a random situation is about n, or perhaps n1.5  see exercise 5 , but in nonrandom situations we will want to speed up this part of the routine as much as we can. One way to rule out spurious factors quickly is to compute the trailing coefficient ¯v 0  first, continuing only if this divides ℓ u u 0 ; the complications explained in the preceding paragraphs do not have to be considered unless this divisibility condition is satisfied, since such a test is valid even when deg v  > 1  Another important way to speed up the procedure is to reduce r so that it tends to reflect the true number of factors. The distinct degree factorization algorithm above can be applied for various small primes pj, thus obtaining for  2 deg u .  2 deg u .   4.6.2  FACTORIZATION OF POLYNOMIALS  453  each prime a set Dj of possible degrees of factors modulo pj; see exercise 26. We can represent Dj as a string of n binary bits. Now we compute the intersection   Dj, namely the bitwise “and” of these strings, and we perform step F2 only  deg vi1  + ··· + deg vid  ∈ Dj.  for  Furthermore p is chosen to be that pj having the smallest value of r. This technique is due to David R. Musser, whose experience suggests trying about five primes pj [see JACM 25  1978 , 271–282]. Of course we would stop immediately  if the current Dj shows that u x  is irreducible.  Musser has given a complete discussion of a factorization method similar to the steps above, in JACM 22  1975 , 291–308. Steps F1–F3 incorporate an improvement suggested in 1978 by G. E. Collins, namely to look for trial divisors by taking combinations of d factors at a time rather than combinations of total degree d. This improvement is important because of the statistical behavior of the modulo-p factors of polynomials that are irreducible over the rationals  see exercise 37 .  A. K. Lenstra, H. W. Lenstra, Jr., and L. Lovász introduced their famous “LLL algorithm” in order to obtain rigorous worst-case bounds on the amount of computation needed to factor a polynomial over the integers [Math. Annalen 261  1982 , 515–534]. Their method requires no random numbers, and its running  time for u x  of degree n is On12 + n9 log ∥u∥ 3 bit operations, where ∥u∥ is  defined in exercise 20. This estimate includes the time to search for a suitable prime number p and to find all factors modulo p with Algorithm B. Of course, heuristic methods that use randomization run noticeably faster in practice. Greatest common divisors. Similar techniques can be used to calculate  greatest common divisors of polynomials: If gcdu x , v x  = d x  over the integers, and if gcdu x , v x  = q x   modulo p  where q x  is monic, then  d x  is a common divisor of u x  and v x  modulo p; hence  d x  divides q x   modulo p .   24  If p does not divide the leading coefficients of both u and v, it does not divide the leading coefficient of d; in such a case deg d  ≤ deg q . When q x  = 1 for such  a prime p, we must therefore have deg d  = 0, and d x  = gcdcont u , cont v . gcdu x , v x  modulo 13 in 4.6.1– 6  is enough to prove that u x  and v x  are  This justifies the remark made in Section 4.6.1 that the simple computation of  relatively prime over the integers; the comparatively laborious calculations of Algorithm 4.6.1E or Algorithm 4.6.1C are unnecessary. Since two random prim- itive polynomials are almost always relatively prime over the integers, and since they are relatively prime modulo p with probability 1 − 1 p by exercise 4.6.1–5, it is usually a good idea to do the computations modulo p.  As remarked before, we need good methods also for the nonrandom poly- nomials that arise in practice. Therefore we wish to sharpen our techniques and  discover how to find gcdu x , v x  in general, over the integers, based entirely  on information that we obtain working modulo primes p. We may assume that u x  and v x  are primitive.   ¯d x  = c · gcdu x , v x , ℓ ¯d  = gcdℓ u , ℓ v .  454  ARITHMETIC  Instead of calculating gcdu x , v x  directly, it will be convenient to search  4.6.2  instead for the polynomial   25   where the constant c is chosen so that   26  This condition will always hold for suitable c, since the leading coefficient of any  common divisor of u x  and v x  must be a divisor of gcdℓ u , ℓ v . Once ¯d x  has been found satisfying these conditions, we can readily compute pp¯d x ,  When pp¯q x  divides both u x  and v x , it must equal gcdu x , v x  because  which is the true greatest common divisor of u x  and v x . Condition  26  is convenient since it avoids the uncertainty of unit multiples of the gcd; we have used essentially the same idea to control the leading coefficients in our factorization routine. If p is a sufficiently large prime, based on the bounds for coefficients in exercise 20 applied either to ℓ ¯d u x  or ℓ ¯d v x , let us compute the unique polynomial ¯q x  ≡ ℓ ¯d q x   modulo p  having all coefficients in [− 1 2 p . of  24 . On the other hand if it does not divide both u x  and v x  we must have deg q  > deg d . A study of Algorithm 4.6.1E reveals that this will be the case only if p divides the leading coefficient of one of the nonzero remainders computed by that algorithm with exact integer arithmetic; otherwise Euclid’s algorithm modulo p deals with precisely the same sequence of polynomials as Algorithm 4.6.1E except for nonzero constant multiples  modulo p . So only a small number of “unlucky” primes can cause us to miss the gcd, and we will soon find a lucky prime if we keep trying. If the bound on coefficients is so large that single-precision primes p are insufficient, we can compute ¯d x  modulo several primes p until it has been determined via the Chinese remainder algorithm of Section 4.3.2. This approach, which is due to W. S. Brown and G. E. Collins, has been described in detail by Brown in JACM 18  1971 , 478–504. Alternatively, as suggested by J. Moses and D. Y. Y. Yun [Proc. ACM Conf. 28  1973 , 159–166], we can use Hensel’s method to determine ¯d x  modulo pe for sufficiently large e. Hensel’s construction appears to be computationally superior to the Chinese remainder approach; but it is valid directly only when  2 p . . 1  or  d x  ⊥ u x  d x   d x  ⊥ v x  d x ,   27  since the idea is to apply the techniques of exercise 22 to one of the factorizations ℓ ¯d u x  ≡ ¯q x u1 x  or ℓ ¯d v x  ≡ ¯q x v1 x   modulo p . Exercises 34 and 35 show that it is possible to arrange things so that  27  holds whenever necessary.  The notation   28  used in  27  means that u x  and v x  are relatively prime, by analogy with the notation used for relatively prime integers.   u x  ⊥ v x    4.6.2  FACTORIZATION OF POLYNOMIALS  455  The gcd algorithms sketched here are significantly faster than those of Sec- tion 4.6.1 except when the polynomial remainder sequence is very short. Per- haps the best general procedure would be to start with the computation of  gcdu x , v x  modulo a fairly small prime p, not a divisor of both ℓ u  and ℓ v .  If the result q x  is 1, we’re done; if it has high degree, we use Algorithm 4.6.1C; otherwise we use one of the methods above, first computing a bound for the coefficients of ¯d x  based on the coefficients of u x  and v x , and on the  small  degree of q x . As in the factorization problem, we should apply this procedure to the reverses of u x , v x  and reverse the result, if the trailing coefficients are simpler than the leading ones. Multivariate polynomials. Similar techniques lead to useful algorithms for factorization or gcd calculations on multivariate polynomials with integer coeffi- cients. It is convenient to deal with the polynomial u x1, . . . , xt  by working modulo the irreducible polynomials x2 − a2, . . . , xt − at, which play the role of p in the discussion above. Since v x  mod  x − a  = v a , the value of  u x1, . . . , xt  mod {x2 − a2, . . . , xt − at}  is the univariate polynomial u x1, a2, . . . , at . When the integers a2, . . . , at are chosen so that u x1, a2, . . . , at  has the same degree in x1 as the original poly- nomial u x1, x2, . . . , xt , an appropriate generalization of Hensel’s construction will “lift” squarefree factorizations of this univariate polynomial to factorizations modulo { x2 − a2 n2, . . . ,  xt − at nt}, where nj is the degree of xj in u; at the same time we can also work modulo an appropriate integer prime p. As many as possible of the aj should be zero, so that sparseness of the intermediate results is retained. For details, see P. S. Wang, Math. Comp. 32  1978 , 1215–1231, in addition to the papers by Musser and by Moses and Yun cited earlier.  Significant computational experience has been accumulating since the days when the pioneering papers cited above were written. See R. E. Zippel, Effective Polynomial Computation  Boston: Kluwer, 1993  for a more recent survey. More- over, it is now possible to factor polynomials that are given implicitly by a “black box” computational procedure, even when both input and output polynomials would fill the universe if they were written out explicitly [see E. Kaltofen and B. M. Trager, J. Symbolic Comp. 9  1990 , 301–320; Y. N. Lakshman and B. David Saunders, SICOMP 24  1995 , 387–397].  The asymptotically best algorithms frequently turn out to be worst on all problems for which they are used. — D. G. CANTOR and H. ZASSENHAUS  1981   EXERCISES   cid:120  1. [M24] Let p be prime, and let u x  be a random polynomial of degree n, assuming  that each of the pn monic polynomials is equally likely. Show that if n ≥ 2, the probability that u x  has a linear factor mod p lies between  1+p−1  2 and  2+p−2  3, inclusive. Give a closed form for this probability when n ≥ p. What is the average number of linear factors?   456  ARITHMETIC   cid:120  2. [M25]  a  Show that any monic polynomial u x , over a unique factorization  4.6.2  domain, may be expressed uniquely in the form u x  = v x 2  w x ,  n≥1 µ n f zn  nt.] What is limp→∞ anp pn?  modulo a prime p. Find a formula for the generating function Gp z  =  [Hint: Prove the following identity connecting power series: f z  =  and only if g z  =  where w x  is squarefree  has no factor of positive degree of the form d x 2  and both v x  and w x  are monic.  b   E. R. Berlekamp.  How many monic polynomials of degree n are squarefree modulo p, when p is prime? 3. [M25]  The Chinese remainder theorem for polynomials.  Let u1 x , . . . , ur x  be polynomials over a field S, with uj x  ⊥ uk x  for all j ̸= k. For any given polynomials w1 x , . . . , wr x  over S, prove that there is a unique polynomial v x  over S such that deg v  < deg u1  + ··· + deg ur  and v x  ≡ wj x   modulo uj x   for 1 ≤ j ≤ r. Does this result hold also when S is the set of all integers? 4. [HM28] Let anp be the number of monic irreducible polynomials of degree n, n anpzn. j≥1 g zj  jt if 5. [HM30] Let Anp be the average number of irreducible factors of a randomly selected polynomial of degree n, modulo a prime p. Show that limp→∞ Anp = Hn. What is the limiting average value of 2r, when r is the number of irreducible factors? 6. [M21]  J. L. Lagrange, 1771.  Prove the congruence  9 . [Hint: Factor xp − x in the field of p elements.] 7. [M22] Prove Eq.  14 . 8. [HM20] How can we be sure that the vectors output by Algorithm N are linearly independent? 9. [20] Explain how to construct a table of reciprocals mod 101 in a simple way, given that 2 is a primitive root of 101.   cid:120  10. [21] Find the complete factorization of the polynomial u x  in  22 , modulo 2,  cid:120  12. [M22] Use Berlekamp’s algorithm to determine the number of factors of u x  =  using Berlekamp’s procedure. 11. [22] Find the complete factorization of the polynomial u x  in  22 , modulo 5.  x4 + 1, modulo p, for all primes p. [Hint: Consider the cases p = 2, p = 8k + 1, p = 8k + 3, p = 8k + 5, p = 8k + 7 separately; what is the matrix Q? You need not discover the factors; just determine how many there are.] 13. [M25] Continuing the previous exercise, give an explicit formula for the factors √−2 of x4 + 1, modulo p, for all odd primes p, in terms of the quantities 14. [M25]  H. Zassenhaus.  Let v x  be a solution to  8 , and let w x  = x − s  when such square roots exist modulo p.  cid:120  15. [M27]  Square roots modulo a prime.  Design an algorithm to calculate the square  where the product is over all 0 ≤ s < p such that gcd u x , v x  − s  ̸= 1. Explain how to compute w x , given u x  and v x . [Hint: Eq.  14  implies that w x  is the polynomial of least degree such that u x  divides w v x  .]  root of a given integer u modulo a given prime p, that is, to find an integer v such that v2 ≡ u  modulo p  whenever such a v exists. Your algorithm should be efficient even for very large primes p.  For p ̸= 2, a solution to this problem leads to a procedure for solving any given quadratic equation modulo p, using the quadratic formula in the usual  √−1,  √  2,   4.6.2  FACTORIZATION OF POLYNOMIALS  457  way.  Hint: Consider what happens when the factorization methods of this section are applied to the polynomial x2 − u. 16. [M30]  Finite fields.  The purpose of this exercise is to prove basic properties of the fields introduced by É. Galois in 1830. a  Given that f x  is an irreducible polynomial modulo a prime p, of degree n, prove that the pn polynomials of degree less than n form a field under arithmetic modulo f x  and p. [Note: The existence of irreducible polynomials of each degree is proved in exercise 4; therefore fields with pn elements exist for all primes p and all n ≥ 1.] the elements of the field are {0, 1, ξ, ξ2, . . . , ξpn−2}. provides a proof in the special case n = 1.]  b  Show that any field with pn elements has a “primitive root” element ξ such that [Hint: Exercise 3.2.1.2–16 c  If f x  is an irreducible polynomial modulo p, of degree n, prove that xpm − x is divisible by f x  if and only if m is a multiple of n.  It follows that we can test irreducibility rather quickly: A given nth degree polynomial f x  is irreducible modulo p if and only if xpn − x is divisible by f x  and xpn q − x ⊥ f x  for all primes q that divide n.   17. [M23] Let F be a field with 132 elements. How many elements of F have order f, for each integer f with 1 ≤ f < 132?  The order of an element a is the least positive integer m such that am = 1.    cid:120  18. [M25] Let u x  = unxn +··· + u0, un ̸= 0, be a primitive polynomial with integer  coefficients, and let v x  be the monic polynomial defined by  v x  = un−1  n  · u x un  = xn + un−1xn−1 + un−2unxn−2 + ··· + u0un−1  n  .  n  for 0 ≤ k < m.   a  Given that v x  has the complete factorization p1 x  . . . pr x  over the integers, where each pj x  is monic, what is the complete factorization of u x  over the integers?  b  If w x  = xm + wm−1xm−1 +···+ w0 is a factor of v x , prove that wk is a multiple of um−1−k 19. [M20]  Eisenstein’s criterion.  Perhaps the best-known class of irreducible poly- nomials over the integers was introduced by T. Schönemann in Crelle 32  1846 , 100, then popularized by G. Eisenstein in Crelle 39  1850 , 166–169: Let p be prime and let u x  = unxn + ··· + u0 have the following properties:  i  un is not divisible by p;  ii  un−1, . . . , u0 are divisible by p;  iii  u0 is not divisible by p2. Show that u x  is irreducible over the integers. 20. [HM33] If u x  = unxn + ··· + u0 is any polynomial over the complex numbers, let ∥u∥ =  un2 + ··· + u02 1 2. a  Let u x  =  x− α w x  and v x  =  ¯αx−1 w x , where α is any complex number  and ¯α is its complex conjugate. Prove that ∥u∥ = ∥v∥.  b  Let un x− α1  . . .  x− αn  be the complete factorization of u x  over the complex  j=1 max 1,αj . Prove that M u  ≤ ∥u∥.  numbers, and write M u  = unn c  Show that uj ≤n−1 M u  +n−1 vj ≤m−1  j−1  j  j  un, for 0 ≤ j ≤ n. un. ∥u∥ +m−1  j−1  d  Combine these results to prove that if u x  = v x w x  and v x  = vmxm+···+v0, where u, v, w all have integer coefficients, then the coefficients of v are bounded by     r,s≥0  458  ARITHMETIC  4.6.2  21. [HM32] Continuing exercise 20, we can also derive useful bounds on the coeffi- cients of multivariate polynomial factors over the integers. For convenience we will let boldface letters stand for sequences of t integers; thus, instead of writing  u x1, . . . , xt  =   j1,...,jt  uj1...jt xj1  1 . . . xjt  t  j ujxj. Notice the convention for xj; we also write  we will write simply u x  =   j! = j1! . . . jt! and Σ j = j1 + ··· + jt. a  Prove the identity    j,k  1 j! k!    p,q≥0  [p − j = q − k] apbq  =  i!   p! q!  p − j !  [r − j = s − k] crds    r! s!  r − j ! [q + r = i] bqcr .  [p + s = i] apds  i≥0  b  The polynomial u x  = of coefficients B u  =  c  The Bombieri norm [u] of a polynomial u x  is defined to be B u  n! when u  j ujxj is called homogeneous of degree n if each term has total degree n; thus we have Σ j = n whenever uj ̸= 0. Consider the weighted sum j j!uj2. Use part  a  to show that B u  ≥ B v B w   whenever u x  = v x w x  is homogeneous.  p,s≥0  q,r≥0  is homogeneous of degree n. It is also defined for nonhomogeneous polynomials, by adding a new variable xt+1 and multiplying each term by a power of xt+1 so that u becomes homogeneous without increasing its maximum degree. For example, let u x  = 4x3 + x − 2; the corresponding homogeneous polynomial is 4x3 + xy2 − 2y3, and we have [u]2 =  3! 0! 42 + 1! 2! 12 + 0! 3! 22  3! = 16 + 1 3 + 4. If u x, y, z  = 3xy3 − z2 we have, similarly, [u]2 =  1! 3! 0! 0! 32 + 0! 0! 2! 2! 12  4! = 9 4 + 1 6. What does part  b  tell us about the relation between [u], [v], and [w], when u x  = v x w x ?  d  Prove that if u x  is a reducible polynomial of degree n in one variable, it has a factor whose coefficients are at most n!1 4[u]1 2  n 4 ! in absolute value. What is the corresponding result for homogeneous polynomials in t variables?  e  Calculate [u] both explicitly and asymptotically when u x  =  x2 − 1 n. f  Prove that [u][v] ≥ [uv]. g  Show that 2−n 2M u  ≤ [u] ≤ 2n 2M u , when u x  is a polynomial of degree n and M u  is the quantity defined in exercise 20.  Therefore the bound in part  d  is roughly the square root of the bound we obtained in that exercise.    cid:120  22. [M24]  Hensel’s Lemma.  Let u x , ve x , we x , a x , b x  be polynomials with  integer coefficients, satisfying the relations  u x  ≡ ve x we x   modulo pe ,  a x ve x  + b x we x  ≡ 1  modulo p ,  where p is prime, e ≥ 1, ve x  is monic, deg a  < deg we , deg b  < deg ve , and deg u  = deg ve  + deg we . Show how to compute polynomials ve+1 x  ≡ ve x  and we+1 x  ≡ we x   modulo pe , satisfying the same conditions with e increased by 1. Furthermore, prove that ve+1 x  and we+1 x  are unique, modulo pe+1.  Use your method for p = 2 to prove that  22  is irreducible over the integers,  Note that Euclid’s  starting with its factorization modulo 2 found in exercise 10. extended algorithm, exercise 4.6.1–3, will get the process started for e = 1.    4.6.2  FACTORIZATION OF POLYNOMIALS  459  23. [HM23] Let u x  be a squarefree polynomial with integer coefficients. Prove that there are only finitely many primes p such that u x  is not squarefree modulo p. 24. [M20] The text speaks only of factorization over the integers, not over the field of rational numbers. Explain how to find the complete factorization of a polynomial with rational coefficients, over the field of rational numbers. 25. [M25] What is the complete factorization of x5 + x4 + x2 + x + 2 over the field of rational numbers? 26. [20] Let d1, . . . , dr be the degrees of the irreducible factors of u x  modulo p, with proper multiplicity, so that d1 + ··· + dr = n = deg u . Explain how to compute the set {deg v   u x  ≡ v x w x   modulo p  for some v x , w x } by performing O r  operations on binary bit strings of length n. 27. [HM30] Prove that a random primitive polynomial over the integers is “almost always” irreducible, in some appropriate sense. 28. [M25] The distinct-degree factorization procedure is “lucky” when there is at most one irreducible polynomial of each degree d; then gd x  never needs to be broken into factors. What is the probability of such a lucky circumstance, when factoring a random polynomial of degree n, modulo p, for fixed n as p → ∞? 29. [M22] Let g x  be a product of two or more distinct irreducible polynomials of degree d, modulo an odd prime p. Prove that gcd g x , t x  pd−1  2 − 1  will be a proper factor of g x  with probability ≥ 1 2−1  2p2d , for any fixed g x , when t x  is selected at random from among the p2d polynomials of degree < 2d modulo p. 30. [M25] Prove that if q x  is an irreducible polynomial of degree d, modulo p, and if t x  is any polynomial, then the value of  t x +t x p +t x p2 +···+t x pd−1  mod q x  is an integer  that is, a polynomial of degree ≤ 0 . Use this fact to design a randomized algorithm for factoring a product gd x  of degree-d irreducibles, analogous to  21 , for the case p = 2. 31. [HM30] Let p be an odd prime and let d ≥ 1. Show that there exists a number n p, d  having the following two properties:  i  For all integers t, exactly n p, d  irreducible polynomials q x  of degree d, modulo p, satisfy  x+t  pd−1  2 mod q x  = 1.  ii  For all integers 0 ≤ t1 < t2 < p, exactly n p, d  irreducible polynomials q x  of degree d, modulo p, satisfy  x + t1  pd−1  2 mod q x  =  x + t2  pd−1  2 mod q x . 1≤k≤n, k⊥n x − ωk , where ω = e2πi n; thus, the roots of Ψn x  are the complex nth roots of unity that aren’t mth roots for m < n. a  Prove that Ψn x  is a polynomial with integer coefficients, and that   cid:120  32. [M30]  Cyclotomic polynomials.  Let Ψn x  = Ψn x  =  xn − 1 =   xd − 1 µ n d  .  Ψd x ;  d\n  d\n   See exercises 4.5.2–10 b  and 4.5.3–28 c .  b  Prove that Ψn x  is irreducible over the integers, hence the formula above is the complete factorization of xn − 1 over the integers. [Hint: If f x  is an irreducible factor of Ψn x  over the integers, and if ζ is a complex number with f ζ  = 0, prove that f ζ p  = 0 for all primes p not dividing n. It may help to use the fact that xn − 1 is squarefree modulo p for all such primes.]  c  Discuss the calculation of Ψn x , and tabulate the values for n ≤ 15.   460  ARITHMETIC  4.6.2  33. [M18] True or false: If u x  ̸= 0 and the complete factorization of u x  modulo p is p1 x e1 . . . pr x er, then u x  gcd u x , u′ x   = p1 x  . . . pr x .   cid:120  34. [M25]  Squarefree factorization.  It is clear that any primitive polynomial of a  unique factorization domain can be expressed in the form u x  = u1 x u2 x 2u3 x 3. . . , where the polynomials ui x  are squarefree and relatively prime to each other. This representation, in which uj x  is the product of all the irreducible polynomials that divide u x  exactly j times, is unique except for unit multiples; and it is a useful way to represent polynomials that participate in multiplication, division, and gcd operations.  Let GCD u x , v x   be a procedure that returns three answers:  GCD u x , v x   =  d x , u x  d x , v x  d x  , where d x  = gcd u x , v x  .  The modular method described in the text following Eq.  25  always ends with a trial division of u x  d x  and v x  d x , to make sure that no “unlucky prime” has been used, so the quantities u x  d x  and v x  d x  are byproducts of the gcd computation; thus we can compute GCD u x , v x   essentially as fast as gcd u x , v x   when we are using a modular method.  Devise a procedure that obtains the squarefree representation  u1 x , u2 x , . . .  of a given primitive polynomial u x  over the integers. Your algorithm should perform exactly e computations of a GCD, where e is the largest subscript with ue x  ̸= 1; furthermore, each GCD calculation should satisfy  27 , so that Hensel’s construction can be used. 35. [M22]  D. Y. Y. Yun.  Design an algorithm that computes the squarefree rep- resentation  w1 x , w2 x , . . .  of w x  = gcd u x , v x   over the integers, given the squarefree representations  u1 x , u2 x , . . .  and  v1 x , v2 x , . . .  of u x  and v x . 36. [M27] Extend the procedure of exercise 34 so that it will obtain the squarefree representation  u1 x , u2 x , . . .  of a given polynomial u x  when the coefficient arith- metic is performed modulo p. 37. [HM24]  George E. Collins.  Let d1, . . . , dr be positive integers whose sum is n, and let p be prime. What is the probability that the irreducible factors of a random nth- degree integer polynomial u x  have degrees d1, . . . , dr, when it is completely factored modulo p? Show that this probability is asymptotically the same as the probability that a random permutation on n elements has cycles of lengths d1, . . . , dr. 38. [HM27]  Perron’s criterion.  Let u x  = xn+un−1xn−1+···+u0 be a polynomial with integer coefficients such that u0 ̸= 0 and either un−1 > 1 + un−2 + ··· + u0 or  un−1 = 0 and un−2 > 1 + un−3 + ··· + u0 . Show that u x  is irreducible over the integers. [Hint: Prove that almost all of u’s roots are less than 1 in absolute value.] 39. [HM42]  David G. Cantor.  Show that if the polynomial u x  is irreducible over the integers, it has a “succinct” proof of irreducibility, in the sense that the number of bits in the proof is at most a polynomial in deg u  and the length of the coefficients.  Only a bound on the length of proof is requested here, as in exercise 4.5.4–17, not a bound on the time needed to find such a proof.  Hint: If v x  is irreducible and t is any polynomial over the integers, all factors of v t x   have degree ≥ deg v . Perron’s criterion gives a large supply of irreducible polynomials v x .   cid:120  40. [M20]  P. S. Wang.  If un is the leading coefficient of u x  and B is a bound on  coefficients of some factor of u, the text’s factorization algorithm requires us to find a factorization modulo pe where pe > 2unB. But un might be larger than B, when B is chosen by the method of exercise 21. Show that if u x  is reducible, there is a way   4.6.3  EVALUATION OF POWERS  461  to recover one of its true factors from a factorization modulo pe whenever pe ≥ 2B2, by using the algorithm of exercise 4.5.3–51. 41. [M47]  Beauzamy, Trevisan, and Wang.  Prove or disprove: There is a constant c such that, if f x  is any integer polynomial with all coefficients ≤ B in absolute value, then one of its irreducible factors has coefficients bounded by cB.  4.6.3. Evaluation of Powers In this section we shall study the interesting problem of computing xn efficiently, given x and n, where n is a positive integer. Suppose, for example, that we need to compute x16; we could simply start with x and multiply by x fifteen times. But it is possible to obtain the same answer with only four multiplications, if we repeatedly take the square of each partial result, successively forming x2, x4, x8, x16.  The same idea applies, in general, to any value of n, in the following way: Write n in the binary number system  suppressing zeros at the left . Then replace each “1” by the pair of letters SX, replace each “0” by S, and cross off the “SX” that now appears at the left. The result is a rule for computing xn, if “S” is interpreted as the operation of squaring, and if “X” is interpreted as the operation of multiplying by x. For example, if n = 23, its binary representation is 10111; so we form the sequence SX S SX SX SX and remove the leading SX to obtain the rule SSXSXSX. This rule states that we should “square, square, multiply by x, square, multiply by x, square, and multiply by x”; in other words, we should successively compute x2, x4, x5, x10, x11, x22, x23.  This binary method is easily justified by a consideration of the sequence of exponents in the calculation: If we reinterpret “S” as the operation of multiplying by 2 and “X” as the operation of adding 1, and if we start with 1 instead of x, the rule will lead to a computation of n because of the properties of the binary number system. The method is quite ancient; it appeared before A.D. 400 in Pi˙ngala’s Hindu classic Chandah. ś¯astra [see B. Datta and A. N. Singh, History of Hindu Mathematics 2  Lahore: Motilal Banarsi Das, 1935 , 76]. There seem to be no other references to this method outside of India during the next several centuries, but a clear discussion of how to compute 2n efficiently for arbitrary n was given by al-Uql¯ıdis¯ı of Damascus in A.D. 952; see The Arithmetic of al-Uql¯ıdis¯ı by A. S. Saidan  Dordrecht: D. Reidel, 1975 , 341–342, where the general ideas are illustrated for n = 51. See also al-B¯ır¯un¯ı’s Chronology of Ancient Nations, edited and translated by E. Sachau  London: 1879 , 132–136; this eleventh-century Arabic work had great influence.  The S-and-X binary method for obtaining xn requires no temporary storage except for x and the current partial result, so it is well suited for incorporation in the hardware of a binary computer. The method can also be readily programmed; but it requires that the binary representation of n be scanned from left to right. Computer programs generally prefer to go the other way, because the available operations of division by 2 and remainder mod 2 will deduce the binary representation from right to left. Therefore the following algorithm, based on a right-to-left scan of the number, is often more convenient:   462  ARITHMETIC  4.6.3  Fig. 13. Evaluation of xn, based on a right-to-left scan of the binary notation for n.  Algorithm A  Right-to-left binary method for exponentiation . This algorithm evaluates xn, where n is a positive integer.  Here x belongs to any algebraic system in which an associative multiplication, with identity element 1, has been defined.  A1. [Initialize.] Set N ← n, Y ← 1, Z ← x. A2. [Halve N.]  At this point, xn = Y Z N.  Set t ← N mod 2 and N ← ⌊N 2⌋.  If t = 0, skip to step A5.  A3. [Multiply Y by Z.] Set Y ← Z times Y. A4. [N = 0?] If N = 0, the algorithm terminates, with Y as the answer. A5. [Square Z.] Set Z ← Z times Z, and return to step A2.  As an example of Algorithm A, consider the steps in the evaluation of x23:  N After step A1 23 After step A5 11 5 After step A5 After step A5 2 1 After step A5 After step A4 0  Z Y 1 x x2 x x4 x3 x8 x7 x7 x16 x23 x16  A MIX program corresponding to Algorithm A appears in exercise 2.  The great calculator al-K¯ash¯ı stated Algorithm A in A.D. 1427 [Istoriko-Mat. Issledovani⁀ıa 7  1954 , 256–257]. The method is closely related to a procedure for multiplication that was actually used by Egyptian mathematicians as early as 2000 B.C.; for if we change step A3 to “Y ← Y +Z” and step A5 to “Z ← Z+Z”, and if we set Y to zero instead of unity in step A1, the algorithm terminates with Y = nx. [See A. B. Chace, The Rhind Mathematical Papyrus  1927 ; W. W. Struve, Quellen und Studien zur Geschichte der Mathematik A1  1930 .] This is a practical method for multiplication by hand, since it involves only the simple operations of doubling, halving, and adding. It is often called the “Russian peasant method” of multiplication, since Western visitors to Russia in the nineteenth century found the method in wide use there.  A1. Initialize  A2. Halve N  A3. Multiply Y by Z  Odd  Even  A5. Square Z  No  A4. N = 0?  Yes   4.6.3  EVALUATION OF POWERS  463  The number of multiplications required by Algorithm A is  ⌊lg n⌋ + ν n ,  where ν n  is the number of ones in the binary representation of n. This is one more multiplication than the left-to-right binary method mentioned at the beginning of this section would require, due to the fact that the first execution of step A3 is simply a multiplication by unity. Because of the bookkeeping time required by this algorithm, the binary method is usually not of importance for small values of n, say n ≤ 10, unless the time for a multiplication is comparatively large. If the value of n is known in advance, the left-to-right binary method is preferable. In some situations, such as the calculation of xn mod u x  discussed in Section 4.6.2, it is much easier to multiply by x than to perform a general multiplication or to square a value, so binary methods for exponentiation are primarily suited for quite large n in such cases. If we wish to calculate the exact multiple-precision value of xn, when x is an integer greater than the computer word size, binary methods are not much help unless n is so huge that the high-speed multiplication routines of Section 4.3.3 are involved; and such applications are rare. Similarly, binary methods are usually inappropriate for raising a polynomial to a power; see R. J. Fateman, SICOMP 3  1974 , 196–213, for a discussion of the extensive literature on polynomial exponentiation. The point of these remarks is that binary methods are nice, but not a panacea. They are most applicable when the time to multiply xj·xk is essentially independent of j and k  for example, when we are doing floating point multi- plication, or multiplication mod m ; in such cases the running time is reduced from order n to order log n. Fewer multiplications. Several authors have published statements  without proof  that the binary method actually gives the minimum possible number of multiplications. But that is not true. The smallest counterexample is n = 15, when the binary method needs six multiplications, yet we can calculate y = x3 in two multiplications and x15 = y5 in three more, achieving the desired result with only five multiplications. Let us now discuss some other procedures for evaluating xn, assuming that n is known in advance. Such procedures are of interest, for example, when an optimizing compiler is generating machine code. The factor method is based on a factorization of n. If n = pq, where p is the smallest prime factor of n and q > 1, we may calculate xn by first calculating xp and then raising this quantity to the qth power. If n is prime, we may calculate xn−1 and multiply by x. And, of course, if n = 1, we have xn with no calculation at all. Repeated application of these rules gives a procedure for evaluating xn, given any value of n. For example, if we want to calculate x55, we first evaluate y = x5 = x4x =  x2 2x; then we form y11 = y10y =  y2 5y. The whole process takes eight multiplications, while the binary method would have required nine. The factor method is better than the binary method on the average, but there are cases  n = 33 is the smallest example  where the binary method excels.   464  ARITHMETIC  4.6.3  Fig. 14. The “power tree.”  The binary method can be generalized to an m-ary method as follows: Let n = d0mt + d1mt−1 +···+ dt, where 0 ≤ dj < m for 0 ≤ j ≤ t. The computation begins by forming x, x2, x3, . . . , xm−1.  Actually, only those powers xdj such that dj appears in the representation of n are needed, and this observation often saves some of the work.  Then raise xd0 to the mth power and multiply by xd1; we have computed y1 = xd0m+d1. Next, raise y1 to the mth power and multiply by xd2, obtaining y2 = xd0m2+d1m+d2. The process continues in this way until yt = xn has been computed. Whenever dj = 0, it is of course unnecessary to multiply by xdj. Notice that this method reduces to the left-to-right binary method discussed earlier, when m = 2; there is also a less obvious right-to- left m-ary method that takes more memory but only a few more steps  see exercise 9 . If m is a small prime, the m-ary method will be particularly efficient for calculating powers of one polynomial modulo another, when the coefficients are treated modulo m, because of Eq. 4.6.2– 5 .  A systematic method that gives the minimum number of multiplications for all of the relatively small values of n  in particular, for most n that occur in practical applications  is indicated in Fig. 14. To calculate xn, find n in this tree; then the path from the root to n indicates a sequence of exponents that occur in an efficient evaluation of xn. The rule for generating this “power tree” appears in exercise 5. Computer tests have shown that the power tree gives optimum results for all of the n listed in the figure. But for large enough values of n the power tree method is not always optimum; the smallest examples are n = 77, 154, 233. The first case for which the power tree is superior to both the binary method and the factor method is n = 23. The first case for which the factor method beats the power tree method is n = 19879 = 103 · 193; such cases are quite rare.  For n ≤ 100,000 the power tree method is better than the factor method 88,803 times; it ties 11,191 times; and it loses only 6 times.   1  2  3  4  5  6  8  7  10  14  11  13  15  9  18  20  40  12  24  48  16  17  32  19  21  28  22 23  26  25  30  27  36  33 34  64  38  35  42  29  31  56  44  46  39  52  50  45  60  41  43  80  54  37  72  49  51  96  66 68  65  128   4.6.3  EVALUATION OF POWERS  465  Fig. 15. A tree that minimizes the number of multiplications, for n ≤ 100.  Addition chains. The most economical way to compute xn by multiplication is a mathematical problem with an interesting history. We shall now examine it in detail, not only because it is classical and interesting in its own right, but because it is an excellent example of the theoretical questions that arise in the study of optimum methods of computation.  Although we are concerned with multiplication of powers of x, the problem can easily be reduced to addition, since the exponents are additive. This leads us to the following abstract formulation: An addition chain for n is a sequence of integers  1 = a0,  a1,  a2,  . . . ,  ar = n   1   with the property that  for some k ≤ j < i,  ai = aj + ak,   2  for all i = 1, 2, . . . , r. One way of looking at this definition is to consider a simple computer that has an accumulator and is capable of the three operations LDA, STA, and ADD; the machine begins with the number 1 in its accumulator, and it proceeds to compute the number n by adding together previous results. Notice that a1 must equal 2, and a2 is either 2, 3, or 4.  The shortest length, r, for which there exists an addition chain for n is denoted by l n . Thus l 1  = 0, l 2  = 1, l 3  = l 4  = 2, etc. Our goal in the remainder of this section is to discover as much as we can about this function l n . The values of l n  for small n are displayed in tree form in Fig. 15, which shows how to calculate xn with the fewest possible multiplications for all n ≤ 100. The problem of determining l n  was apparently first raised by H. Dellac in 1894, and a partial solution by E. de Jonquières mentioned the factor method  1  2  3  4  5  6  8  7  10  12  9  16  14  11  20  15  24  19  28  21 22 23  40  27  30  25  48  13  26  17  34  18  36  32  33  64  38  29  56  31  42  44  46  41  80  39  54  45  60  50 51  96  35  52  43  68  37  72  49  66  65  76  58  57  59  62  84  88  47  92  82 83  85  78  55  90  63  75  100  53  97  99  70  61  77  86  69  74  73  98  67  81  89  94  93  95  79  91  71  87   466  ARITHMETIC  4.6.3  [see L’Intermédiaire des Mathématiciens 1  1894 , 20, 162–164]. In his solution, de Jonquières listed what he felt were the values of l p  for all prime numbers p < 200, but his table entries for p = 107, 149, 163, 179, 199 were one too high.  The factor method tells us immediately that l mn  ≤ l m  + l n ,   3  since we can take the chains 1, a1, . . . , ar = m and 1, b1, . . . , bs = n and form the chain 1, a1, . . . , ar, arb1, . . . , arbs = mn. We can also recast the m-ary method into addition-chain terminology. Con- sider the case m = 2k, and write n = d0mt + d1mt−1 + ··· + dt in the m-ary number system; the corresponding addition chain takes the form  1, 2, 3, . . . , m − 2, m − 1,  2d0, 4d0, . . . , md0, md0 + d1,  2 md0+d1 , 4 md0+d1 , . . . , m md0 + d1 , m2d0 + md1 + d2,  . . . ,  mtd0 + mt−1d1 + ··· + dt.   4  The length of this chain is m−2+ k+1 t; and it can often be reduced by deleting certain elements of the first row that do not occur among the coefficients dj, plus elements among 2d0, 4d0, . . . that already appear in the first row. Whenever digit dj is zero, the step at the right end of the corresponding line may, of course, be dropped. Furthermore, we can omit all the even numbers  except 2  in the first row, if we bring values of the form dj 2e into the computation e steps earlier. [See E. Wattel and G. A. Jensen, Math. Centrum Report ZW1968-001  1968 , 18 pp.; E. G. Thurber, Duke Math. J. 40  1973 , 907–913.]  The simplest case of the m-ary method is the binary method  m = 2 , when the general scheme  4  simplifies to the “S” and “X” rule mentioned at the beginning of this section: The binary addition chain for 2n is the binary chain for n followed by 2n; for 2n + 1 it is the binary chain for 2n followed by 2n + 1. From the binary method we conclude that l 2e0 + 2e1 + ··· + 2et  ≤ e0 + t,   5  Let us now define two auxiliary functions for convenience in our subsequent discussion:  if e0 > e1 > ··· > et ≥ 0.  λ n  = ⌊lg n⌋; ν n  = number of 1s in the binary representation of n.   6   7  Thus λ 17  = 4, ν 17  = 2; these functions may be defined by the recurrence relations  λ 1  = 0, ν 1  = 1,   8   9  In terms of these functions, the binary addition chain for n requires exactly λ n  + ν n  − 1 steps, and  5  becomes  λ 2n  = λ 2n + 1  = λ n  + 1; ν 2n  = ν n ,  ν 2n + 1  = ν n  + 1.  l n  ≤ λ n  + ν n  − 1.   10    4.6.3  EVALUATION OF POWERS  467  Special classes of chains. We may assume without any loss of generality that an addition chain is ascending,  1 = a0 < a1 < a2 < ··· < ar = n.  It is convenient at this point to define a few special terms relating to addition   11  For if any two a’s are equal, one of them may be dropped; and we can also rearrange the sequence  1  into ascending order and remove terms > n without destroying the addition chain property  2 . From now on we shall consider only ascending chains, without explicitly mentioning this assumption. chains. By definition we have, for 1 ≤ i ≤ r, ai = aj + ak   12  for some j and k, 0 ≤ k ≤ j < i. If this relation holds for more than one pair  j, k , we let j be as large as possible. Let us say that step i of  11  is a doubling, if j = k = i − 1; then ai has the maximum possible value 2ai−1 that can follow the ascending chain 1, a1, . . . , ai−1. If j  but not necessarily k  equals i − 1, let us say that step i is a star step. The importance of star steps is explained below. Finally let us say that step i is a small step if λ ai  = λ ai−1 . Since ai−1 < ai ≤ 2ai−1, the quantity λ ai  is always equal to either λ ai−1  or λ ai−1  + 1; it follows that, in any chain  11 , the length r is equal to λ n  plus the number of small steps.  Several elementary relations hold between these types of steps: Step 1 is always a doubling. A doubling obviously is a star step, but never a small step. A doubling must be followed by a star step. Furthermore if step i is not a small step, then step i + 1 is either a small step or a star step, or both; putting this another way, if step i + 1 is neither small nor star, step i must have been small. A star chain is an addition chain that involves only star steps. This means that each term ai is the sum of ai−1 and a previous ak; the simple “computer” discussed above after Eq.  2  makes use only of the two operations STA and ADD  not LDA  in a star chain, since each new term of the sequence utilizes the preceding result in the accumulator. Most of the addition chains we have discussed so far are star chains. The minimum length of a star chain for n is denoted by l∗ n ; clearly   13  We are now ready to derive some nontrivial facts about addition chains. First we can show that there must be fairly many doublings if r is not far from λ n . If the addition chain  11  includes d doublings and f = r − d Theorem A. nondoublings, then  l n  ≤ l∗ n .   14  Proof. By induction on r = d + f, we see that  14  is certainly true when r = 1. When r > 1, there are three cases: If step r is a doubling, then If steps r and r − 1 are both 2 n = ar−1 ≤ 2d−2Ff+3; hence  14  follows. 1 nondoublings, then ar−1 ≤ 2d−1Ff+2 and ar−2 ≤ 2d−1Ff+1; hence n = ar ≤  n ≤ 2d−1Ff+3.   ARITHMETIC  468 4.6.3 ar−1 + ar−2 ≤ 2d−1 Ff+2 + Ff+1  = 2d−1Ff+3 by the definition of the Fibonacci sequence. Finally, if step r is a nondoubling but step r − 1 is a doubling, then ar−2 ≤ 2d−2Ff+2 and n = ar ≤ ar−1 + ar−2 = 3ar−2. Now 2Ff+3 − 3Ff+2 = Ff+1 − Ff ≥ 0; hence n ≤ 2d−1Ff+3 in all cases.  The method of proof we have used shows that inequality  14  is “best  possible” under the stated assumptions; the addition chain  1, 2, . . . , 2d−1, 2d−1F3, 2d−1F4, . . . , 2d−1Ff+3   15   has d doublings and f nondoublings. Corollary A. steps, then  If the addition chain  11  includes f nondoublings and s small  s ≤ f ≤ 3.271s.   16  Proof. Obviously s ≤ f. We have 2λ n  ≤ n ≤ 2d−1Ff+3 ≤ 2dϕf = 2λ n +s ϕ 2 f, since d + f = λ n  + s, and since Ff+3 ≤ 2ϕf when f ≥ 0. Hence 0 ≤ s ln 2 + f ln ϕ 2 , and  16  follows from the fact that ln 2  ln 2 ϕ  ≈ 3.2706. Values of l n  for special n. and therefore lg n ≤ r in any addition chain  11 . Hence  It is easy to show by induction that ai ≤ 2i,   17  This lower bound, together with the upper bound  10  given by the binary method, gives us the values  l n  ≥ ⌈lg n⌉.  if A > B.  l 2A  = A;  l 2A + 2B  = A + 1,   18   19  In other words, the binary method is optimum when ν n  ≤ 2. With some further calculation we can extend these formulas to the case ν n  = 3: Theorem B.  20  Proof. We can, in fact, prove a stronger result that will be of use to us later in this section: All addition chains with exactly one small step have one of the following six types  where all steps indicated by “. . . ” represent doublings :  l 2A + 2B + 2C  = A + 2,  if A > B > C.  Type 1. 1, . . . , 2A, 2A + 2B, . . . , 2A+C + 2B+C; A > B ≥ 0, C ≥ 0. Type 2. 1, . . . , 2A, 2A + 2B, 2A+1 + 2B, . . . , 2A+C+1 + 2B+C; A > B ≥ 0,  C ≥ 0.  Type 3. 1, . . . , 2A, 2A + 2A−1, 2A+1 + 2A−1, 2A+2, . . . , 2A+C; A > 0, C ≥ 2. Type 4. 1, . . . , 2A, 2A + 2A−1, 2A+1 + 2A, 2A+2, . . . , 2A+C; A > 0, C ≥ 2. Type 5. 1, . . . , 2A, 2A + 2A−1, . . . , 2A+C + 2A+C−1, 2A+C+1 + 2A+C−2, . . . ,  2A+C+D+1 + 2A+C+D−2; A > 0, C > 0, D ≥ 0.  Type 6. 1, . . . , 2A, 2A + 2B, 2A+1, . . . , 2A+C; A > B ≥ 0, C ≥ 1.   4.6.3  EVALUATION OF POWERS  469  A straightforward hand calculation shows that these six types exhaust all possibilities. By Corollary A, there are at most three nondoublings when there is one small step; this maximum occurs only in sequences of Type 3. All of the above are star chains, except Type 6 when B < A − 1. The theorem now follows from the observation that  l 2A + 2B + 2C  ≤ A + 2;  and l 2A + 2B + 2C  must be greater than A + 1, since none of the six possible types have ν n  > 2.  E. de Jonquières stated without proof in 1894 that l n  ≥ λ n  + 2 when M. V. Subbarao, and M. Sugunamma in Duke Math. J. 29  1962 , 481–487.  ν n  > 2. The first published demonstration of Theorem B was by A. A. Gioia,  The calculation of l 2A + 2B + 2C + 2D , when A > B > C > D, is more involved. By the binary method it is at most A+3, and by the proof of Theorem B it is at least A + 2. The value A + 2 is possible, since we know that the binary method is not optimal when n = 15 or n = 23. The complete behavior when ν n  = 4 can be determined, as we shall now see. If ν n  ≥ 4 then l n  ≥ λ n  + 3, except in the following Theorem C. circumstances when A > B > C > D and l 2A + 2B + 2C + 2D  equals A + 2:  Case 1. A − B = C − D.  Example: n = 15.  Case 2. A − B = C − D + 1.  Example: n = 23.  Case 3. A − B = 3, C − D = 1.  Example: n = 39.  Case 4. A − B = 5, B − C = C − D = 1.  Example: n = 135.   Proof. When l n  = λ n  + 2, there is an addition chain for n having just two small steps; such an addition chain starts out as one of the six types in the proof of Theorem B, followed by a small step, followed by a sequence of nonsmall steps. Let us say that n is “special” if n = 2A + 2B + 2C + 2D for one of the four cases listed in the theorem. We can obtain addition chains of the required form for each special n, as shown in exercise 13; therefore it remains for us to prove that no chain with exactly two small steps contains any elements with ν ai  ≥ 4 except when ai is special. Let a “counterexample chain” be an addition chain with two small steps such that ν ar  ≥ 4, but ar is not special. If counterexample chains exist, let 1 = a0 < a1 < ··· < ar = n be a counterexample chain of shortest possible length. Then step r is not a small step, since none of the six types in the proof of Theorem B can be followed by a small step with ν n  ≥ 4 except when n is special. Furthermore, step r is not a doubling, otherwise a0, . . . , ar−1 would be a shorter counterexample chain; and step r is a star step, otherwise a0, . . . , ar−2, ar would be a shorter counterexample chain. Thus  ar = ar−1 + ar−k,  k ≥ 2;  and λ ar  = λ ar−1  + 1.   21    470  ARITHMETIC  4.6.3   22   Let c be the number of carries that occur when ar−1 is added to ar−k in the  binary number system by Algorithm 4.3.1A. Using the fundamental relation  ν ar  = ν ar−1  + ν ar−k  − c,  we can prove that step r − 1 is not a small step  see exercise 14 . Let m = λ ar−1 . Since neither r nor r − 1 is a small step, c ≥ 2; and c = 2 can hold only when ar−1 ≥ 2m + 2m−1. Now let us suppose that r − 1 is not a star step. Then r − 2 is a small step, and a0, . . . , ar−3, ar−1 is a chain with only one small step; hence ν ar−1  ≤ 2 and ν ar−2  ≤ 4. The relation  22  can now hold only if ν ar  = 4, ν ar−1  = 2, k = 2, c = 2, ν ar−2  = 4. From c = 2 we conclude that ar−1 = 2m + 2m−1; hence a0, a1, . . . , ar−3 = 2m−1 + 2m−2 is an addition chain with only one small step, and it must be of Type 1, so ar belongs to Case 3. Thus r−1 is a star step. Now assume that ar−1 = 2tar−k for some t. If ν ar−1  ≤ 3, then by  22 , c = 2, k = 2, and we see that ar must belong to Case 3. On the other hand, if ν ar−1  = 4 then ar−1 is special, and it is easy to see by considering each case that ar also belongs to one of the four cases.  Case 4 arises, for example, when ar−1 = 90, ar−k = 45; or ar−1 = 120, ar−k = 15.  Therefore we may conclude that ar−1 ̸= 2tar−k for any t. We have proved that ar−1 = ar−2 + ar−q for some q ≥ 2. If k = 2, then q > 2, and a0, a1, . . . , ar−2, 2ar−2, 2ar−2 + ar−q = ar is a counterexample sequence in which k > 2; therefore we may assume that k > 2. Let us now suppose that λ ar−k  = m − 1; the case λ ar−k  < m − 1 may be ruled out by similar arguments, as shown in exercise 14. If k = 4, both r − 2 and r − 3 are small steps; hence ar−4 = 2m−1, and  22  is impossible. Therefore k = 3; step r − 2 is small, ν ar−3  = 2, c = 2, ar−1 ≥ 2m + 2m−1, and ν ar−1  = 4. There must be at least two carries when ar−2 is added to ar−1 − ar−2; hence ν ar−2  = 4, and ar−2  being special and ≥ 1 2 ar−1  has the form 2m−1+2m−2+2d+1+2d for some d. Now ar−1 is either 2m+2m−1+2d+1+2d or 2m + 2m−1 + 2d+2 + 2d+1, and in both cases ar−3 must be 2m−1 + 2m−2, so ar belongs to Case 3.  E. G. Thurber [Pacific J. Math. 49  1973 , 229–242] has extended Theorem C to show that l n  ≥ λ n  + 4 when ν n  > 8. It seems reasonable to conjecture that l n  ≥ λ n  + lg ν n  in general, since A. Schönhage has come very close to proving this  see exercise 28 . *Asymptotic values. Theorem C indicates that it is probably quite difficult to get exact values of l n  for large n, when ν n  > 4; however, we can determine the approximate behavior in the limit as n → ∞. Theorem D.  [A. Brauer, Bull. Amer. Math. Soc. 45  1939 , 736–739.]  n→∞ l∗ n  λ n  = lim lim  n→∞ l n  λ n  = 1.   23   Proof. The addition chain  4  for the 2k-ary method is a star chain if we delete the second occurrence of any element that appears twice in the chain; for if ai   4.6.3  EVALUATION OF POWERS  471  is the first element among 2d0, 4d0, . . . of the second line that is not present in the first line, we have ai ≤ 2 m − 1 ; hence ai =  m − 1  + aj for some aj in the first line. By totaling up the length of the chain, we have   24   2 lg λ n ⌋.  λ n  ≤ l n  ≤ l∗ n  <  1 + 1 k  lg n + 2k for all k ≥ 1. The theorem follows if we choose, say, k = ⌊ 1  λλ n , we obtain the stronger asymptotic bound  If we let k = λλ n  − 2λλλ n  in  24  for large n, where λλ n  denotes  l n  ≤ l∗ n  ≤ λ n  + λ n  λλ n  + Oλ n λλλ n  λλ n 2.   25  The second term λ n  λλ n  is essentially the best that can be obtained from  24 . A much deeper analysis of lower bounds can be carried out, to show that this term λ n  λλ n  is, in fact, essential in  25 . In order to see why this is so, let us consider the following fact: Theorem E. positive real number. The number of addition chains  11  such that  [Paul Erdős, Acta Arithmetica 6  1960 , 77–81.] Let ϵ be a  λ n  = m,  r ≤ m +  1 − ϵ m λ m    26  is less than αm, for some α < 2, for all suitably large m.  In other words, the number of addition chains so short that  26  is satisfied is substantially less than the number of values of n such that λ n  = m, when m is large.  Proof. We want to estimate the number of possible addition chains, and for this purpose our first goal is to get an improvement of Theorem A that enables us to deal more satisfactorily with nondoublings. 2 − 1 be a fixed positive real number. Call step i of an Lemma P. Let δ < addition chain a “ministep” if it is not a doubling and if ai < aj 1 + δ i−j for some j, where 0 ≤ j < i. If the addition chain contains s small steps and t ministeps, then  where  1 + δ 2 = 2θ.   27  Proof. For each ministep ik, 1 ≤ k ≤ t, we have aik < ajk 1 + δ ik−jk for some jk < ik. Let I1, . . . , It be the intervals  j1 . . i1], . . . ,  jt . . it], where the notation  j . . i ] stands for the set of all integers k such that j < k ≤ i. It is possible  see exercise 17  to find nonoverlapping intervals J1, . . . , Jh =  j′ h . . i′ h ] such that  1], . . . ,  j′  t ≤ s  1 − θ ,  1 . . i′  √  I1 ∪ ··· ∪ It = J1 ∪ ··· ∪ Jh,  1 + δ 2 i′  k−j′  k ,   28  Now for all steps i outside of the intervals J1, . . . , Jh we have ai ≤ 2ai−1; hence if we let  for 1 ≤ k ≤ h.  < aj′  ai′  k  k  we have 2λ n  ≤ n ≤ 2r−q 1 + δ 2q = 2λ n +s− 1−θ q ≤ 2λ n +s− 1−θ t.  q =  i′  1 − j′  1  + ··· +  i′  h − j′ h ,   472  ARITHMETIC  4.6.3 Returning to the proof of Theorem E, let us choose δ = 2ϵ 4 − 1, and let us  divide the r steps of each addition chain into three classes:  t ministeps,   29  Counting another way, we have s small steps, where s + m = r. By the hypoth- eses, Theorem A, and Lemma P, we obtain the relations  t + u + v = r.  v other steps,  u doublings,  s ≤  1 − ϵ m λ m ,  t + v ≤ 3.271s,  t ≤ s  1 − ϵ 2 .  Given s, t, u, v satisfying these conditions, there are   r  t, u, v    =   r   t + v    t + v  v   30    31   ways to assign the steps to the specified classes. Given such a distribution of the steps, let us consider how the non-ministeps can be selected: If step i is one of the “other” steps in  29 , ai ≥  1 + δ ai−1, so ai = aj + ak, where δai−1 ≤ ak ≤ aj ≤ ai−1. Also aj ≤ ai  1 + δ i−j ≤ 2ai−1  1 + δ i−j, so δ ≤ 2  1 + δ i−j. This gives at most β choices for j, where β is a constant that depends only on δ. There are also at most β choices for k, so the number of ways to assign j and k for each of the non-ministeps is at most   32  Finally, once the “j ” and “k” have been selected for each of the non-  β2v.  ministeps, there are fewer than  t   33  ways to choose the j and the k for the ministeps: We select t distinct pairs  j1, k1 , . . . ,  jt, kt  of indices in the range 0 ≤ kh ≤ jh < r, in fewer than  33  ways. Then for each ministep i, in turn, we use a pair of indices  jh, kh  such that a  jh < i; b  ajh + ak h is as small as possible among the pairs not already used for smaller   r2    c  ai = ajh + ak h satisfies the definition of ministep. If no such pair  jh, kh  exists, we get no addition chain; on the other hand, any addition chain with ministeps in the designated places must be selected in one of these ways, so  33  is an upper bound on the possibilities.  Thus the total number of possible addition chains satisfying  26  is bounded by  31  times  32  times  33 , summed over all relevant s, t, u, and v. The proof of Theorem E can now be completed by means of a rather standard estimation of these functions  exercise 18 . Corollary E. The value of l n  is asymptotically λ n + λ n  λλ n , for almost all n. More precisely, there is a function f n  such that f n  → 0 as n → ∞, and  Prl n  − λ n  − λ n  λλ n  ≥ f n λ n  λλ n  = 0.   34    See Section 3.5 for the definition of this probability “Pr”.   ministeps i;   4.6.3  EVALUATION OF POWERS  473  Proof. The upper bound  25  shows that  34  holds without the absolute value signs. The lower bound comes from Theorem E, if we let f n  decrease to zero slowly enough so that, when f n  ≤ ϵ, the value N is so large that at most ϵN values n ≤ N have l n  ≤ λ n  +  1 − ϵ λ n  λλ n . *Star chains. Optimistic people find it reasonable to suppose that l n  = l∗ n ; given an addition chain of minimal length l n , it appears hard to believe that we cannot find one of the same length that satisfies the  apparently mild  star condition. But in 1958 Walter Hansen proved the remarkable theorem that, for certain large values of n, the value of l n  is definitely less than l∗ n , and he also proved several related theorems that we shall now investigate. Hansen’s theorems begin with an investigation of the detailed structure of a star chain. Let n = 2e0 + 2e1 + ··· + 2et, where e0 > e1 > ··· > et ≥ 0, and let 1 = a0 < a1 < ··· < ar = n be a star chain for n. If there are d doublings in this chain, we define the auxiliary sequence  0 = d0 ≤ d1 ≤ d2 ≤ ··· ≤ dr = d,   35  where di is the number of doublings among steps 1, 2, . . . , i. We also define a sequence of “multisets” S0, S1, . . . , Sr, which keep track of the powers of 2 present in the chain.  A multiset is a mathematical entity that is like a set, but it is allowed to contain repeated elements; an object may be an element of a multiset several times, and its multiplicity of occurrences is relevant. See exercise 19 for familiar examples of multisets.  The multisets Si are defined by the rules a  S0 = {0}; b  If ai+1 = 2ai, then Si+1 = Si + 1 = {x + 1  x ∈ Si}; c  If ai+1 = ai + ak, k < i, then Si+1 = Si ⊎ Sk.   The symbol ⊎ means that the multisets are combined, adding the multi-  plicities.  From this definition it follows that 2x,  ai =   x∈Si  where the terms in this sum are not necessarily distinct. In particular,  n = 2e0 + 2e1 + ··· + 2et =   2x.  x∈Sr  The number of elements in the latter sum is at most 2f, where f = r − d is the number of nondoublings.  Since n has two different binary representations in  37 , we can partition  the multiset Sr into multisets M0, M1, . . . , Mt such that  2ej =   x∈Mj  2x,  0 ≤ j ≤ t.  This can be done by arranging the elements of Sr into nondecreasing order x1 ≤ x2 ≤ ··· and taking Mt = {x1, x2, . . . , xk}, where 2x1 + ··· + 2xk = 2et.   36    37    38    474  ARITHMETIC  4.6.3  for all x ∈ Mj.  This must be possible, since et is the smallest of the e’s. Similarly, Mt−1 = {xk+1, xk+2, . . . , xk′}, and so on; the process is easily visualized in binary nota- tion. An example appears below. Let Mj contain mj elements  counting multiplicities ; then mj ≤ 2f − t, since Sr has at most 2f elements and it has been partitioned into t+1 nonempty multisets. By Eq.  38 , we can see that ej ≥ x > ej − mj,   39  Our examination of the star chain’s structure is completed by forming the is  multisets Mij that record the ancestral history of Mj. The multiset Si partitioned into t + 1 multisets as follows: a  Mrj = Mj; b  If ai+1 = 2ai, then Mij = M i+1 j − 1 = {x − 1  x ∈ M i+1 j}; c  If ai+1 = ai + ak, k < i, then  since Si+1 = Si ⊎ Sk  we let Mij = M i+1 j minus Sk, that is, we remove the elements of Sk from M i+1 j. If some element of Sk appears in two or more different multisets M i+1 j, we remove it from the set with the largest possible value of j; this rule uniquely defines Mij for each j, when i is fixed. From this definition it follows that  ej + di − d ≥ x > ej + di − d − mj,   40  As an example of this detailed construction, let us consider the star chain 1, 2, 3, 5, 10, 20, 23, for which t = 3, r = 6, d = 3, f = 3. We obtain the following array of multisets: 0 1  3 2 10 20 23  for all x ∈ Mij.  1 3  1 2  1 5  3   0  0 M3 1 M2 2 M1 3 M0 3 S0 S1 S2 S3 S4 S5 S6    1 2 2  2 3 3  0 1 1  0 1  1  e3 = 0, m3 = 1 e2 = 1, m2 = 1 e1 = 2, m1 = 1 e0 = 4, m0 = 2  Thus M40 = {2, 2}, etc. From the construction we can see that di is the largest element of Si; hence  41  The most important part of this structure comes from Eq.  40 ; one of its  di ∈ Mi0.  immediate consequences is Lemma K. If Mij and Muv both contain a common integer x, then  −mv <  ej − ev  −  du − di  < mj.   42  Although Lemma K may not look extremely powerful, it says  when mj and mv are reasonably small and when Mij contains an element in common   d0, d1, . . . , d6  :  a0, a1, . . . , a6  :  M03, M13, . . . , M63  :  M02, M12, . . . , M62  :  M01, M11, . . . , M61  :  M00, M10, . . . , M60  :   4.6.3  EVALUATION OF POWERS  475  with Muv  that the number of doublings between steps u and i is approximately equal to the difference between the exponents ev and ej. This imposes a certain amount of regularity on the addition chain; and it suggests that we might be able to prove a result analogous to Theorem B above, that l∗ n  = e0 + t, if the exponents ej are far enough apart. The next theorem shows how this can in fact be done. Theorem H. ··· + 2et, where e0 > e1 > ··· > et ≥ 0. If  [W. Hansen, Crelle 202  1959 , 129–136.] Let n = 2e0 + 2e1 +  e0 > 2e1 + 2.271 t − 1   and  ei−1 ≥ ei + 2m for 1 ≤ i ≤ t,   43   where m = 2⌊3.271 t−1 ⌋ − t, then l∗ n  = e0 + t. Proof. We may assume that t > 2, since the result of the theorem is true without restriction on the e’s when t ≤ 2. Suppose that we have a star chain 1 = a0 < a1 < ··· < ar = n for n with r ≤ e0 + t − 1. Let the integers d, f, d0, . . . , dr, and the multisets Mj, Si, Mij reflect the structure of this chain, as defined above. By Corollary A, we know that f ≤ ⌊3.271 t − 1 ⌋; therefore the value of m is a bona fide upper bound for the number mj of elements in each multiset Mj.       x∈Mi0  ai =  2x  +     x∈Mi1    2x  + ··· +    2x  ,     x∈Mit  In the summation  system, since the e’s are so far apart. See  40 . In particular, the sum of all  no carries propagate from the term corresponding to Mij to the term correspond- ing to Mi j−1 , if we think of this sum as being carried out in the binary number the terms for j ̸= 0 will not carry up to affect the terms for j = 0, so we must have  2x ≥ 2λ ai ,  0 ≤ i ≤ r.   44   ai ≥   x∈Mi0  In order to prove Theorem H, we would like to show that in some sense the t extra powers of n must be put in “one at a time,” so we want to find a way to tell at which step each of these terms essentially enters the addition chain.  Let j be a number between 1 and t. Since M0j is empty and Mrj = Mj is  nonempty, we can find the first step i for which Mij is not empty. From the way in which the Mij are defined, we know that step i is a non- doubling: ai = ai−1+au for some u < i−1. We also know that all the elements of Mij are elements of Su. We will prove that au must be relatively small compared to ai. Let xj be an element of Mij. Then since xj ∈ Su, there is some v for which xj ∈ Muv. It follows that  45  that is, at least m+1 doublings occur between steps u and i. For if di − du ≤ m, Lemma K tells us that ej − ev < 2m; hence v = j. But this is impossible, because Muj is empty by our choice of step i.  di − du > m,   476  ARITHMETIC  4.6.3 All elements of Su are less than or equal to e1 + di − d. For if x ∈ Su ⊆ Si and x > e1 + di − d, then x ∈ Mu0 and x ∈ Mi0 by  40 ; so Lemma K implies that di − du < m, contradicting  45 . In fact, this argument proves that Mi0 has no elements in common with Su, so M i−1 0 = Mi0. From  44  we have ai−1 ≥ 2λ ai , and therefore step i is a small step. We can now deduce what is probably the key fact in this entire proof: All elements of Su are in Mu0. For if not, let x be an element of Su with x  ∈ Mu0. Since x ≥ 0,  40  implies that e1 ≥ d − du, hence  e0 = f + d − s ≤ 2.271s + d ≤ 2.271 t − 1  + e1 + du.  By hypothesis  43 , this implies du > e1. But du ∈ Su by  41 , and it cannot be in Mi0, hence du ≤ e1 + di − d ≤ e1, a contradiction. Going back to our element xj in Mij, we have xj ∈ Muv; and we have proved that v = 0. Therefore, by equation  40  again,  e0 + du − d ≥ xj > e0 + du − d − m0.   46  For all j = 1, 2, . . . , t we have determined a number xj satisfying  46 , and a small step i at which the term 2ej may be said to have entered into the addition chain. If j ̸= j′, the step i at which this occurs cannot be the same for both j and j′; for  46  would tell us that xj − xj′ < m, while elements of Mij and Mij′ must differ by more than m, since ej and ej′ are so far apart. We are forced to conclude that the chain contains at least t small steps; but this is a contradiction. Theorem F  W. Hansen .  if λ x  + λ y  ≤ A.  l 2A + xy  ≤ A + ν x  + ν y  − 1,   47  Proof. An addition chain  which is not a star chain in general  may be con- structed by combining the binary and factor methods. Let x = 2x1 + ··· + 2xu and y = 2y1 + ··· + 2yv, where x1 > ··· > xu ≥ 0 and y1 > ··· > yv ≥ 0. The first steps of the chain form successive powers of 2, until 2A−y1 is reached; in between these steps, the additional values 2xu−1 + 2xu, 2xu−2 + 2xu−1 + 2xu, . . . , and x are inserted in the appropriate places. After a chain up to 2A−yi + x 2y1−yi + ··· + 2yi−1−yi  has been formed, we continue by adding x and doubling the resulting sum yi − yi+1 times; this yields 2A−yi+1 + x 2y1−yi+1 + ··· + 2yi−yi+1 .  If this construction is done for i = 1, 2, . . . , v, assuming for convenience that yv+1 = 0, we have an addition chain for 2A + xy as desired.  Theorem F enables us to find values of n for which l n  < l∗ n , since Theorem H gives an explicit value of l∗ n  in certain cases. For example, let x = 21016 + 1, y = 22032 + 1, and let  n = 26103 + xy = 26103 + 23048 + 22032 + 21016 + 1.  According to Theorem F, we have l n  ≤ 6106. But Theorem H also applies, with m = 508, and this proves that l∗ n  = 6107.   4.6.3  EVALUATION OF POWERS  477  Extensive computer calculations have shown that n = 12509 is the smallest value with l n  < l∗ n . No star chain for this value of n is as short as the sequence 1, 2, 4, 8, 16, 17, 32, 64, 128, 256, 512, 1024, 1041, 2082, 4164, 8328, 8345, 12509. The smallest n with ν n  = 5 and l n  ̸= l∗ n  is 16537 = 214+9·17  see exercise 15 .  Jan van Leeuwen has generalized Theorem H to show that l∗ k2e0  + t ≤ l∗ kn  ≤ l∗ k2et  + e0 − et + t  for all fixed k ≥ 1, if the exponents e0 > ··· > et are far enough apart [Crelle 295  1977 , 202–207]. Some conjectures. Although it was reasonable to guess at first glance that l n  = l∗ n , we have now seen that this is false. Another plausible conjecture [first made by A. Goulard, and supposedly “proved” by E. de Jonquières in L’Interméd. des Math. 2  1895 , 125–126] is that l 2n  = l n +1; a doubling step is so efficient, it seems unlikely that there could be any shorter chain for 2n than to add a doubling step to the shortest chain for n. But computer calculations show that this conjecture also fails, since l 191  = l 382  = 11.  A star chain of length 11 for 382 is not hard to find; for example, 1, 2, 4, 5, 9, 14, 23, 46, 92, 184, 198, 382. The number 191 is minimal such that l n  = 11, and it seems to be nontrivial to prove by hand that l 191  > 10. The author’s computer-generated proof of this fact, using a backtrack method that will be sketched in Section 7.2.2, involved a detailed examination of 102 cases.  The smallest four values of n such that l 2n  = l n  are n = 191, 701, 743, 1111; E. G. Thurber proved in Pacific J. Math. 49  1973 , 229–242, that the third of these is a member of an infinite family of such n, namely 23·2k +7 for all k ≥ 5. Neill Clift found in 2007 that l n  = l 2n  = l 4n  = 31 when n = 30958077; and in 2008, astonishingly, he discovered that l n  > l 2n  = 34 when n = 375494703. Kevin R. Hebb has shown that l n  − l mn  can get arbitrarily large, for all fixed integers m not a power of 2 [Notices Amer. Math. Soc. 21  1974 , A–294]. The smallest case in  which l n  > l mn  is l 213 + 1  3 = 15.  Let c r  be the smallest value of n such that l n  = r. The computation  of l n  seems to be hardest for this sequence of n’s, which begins as follows:  c r  r 2 1 3 2 5 3 7 4 11 5 19 6 29 7 47 8 9 71 10 127 11 191 12 379 13 607  c r  r 1087 14 1903 15 3583 16 6271 17 11231 18 18287 19 34303 20 65131 21 110591 22 196591 23 357887 24 25 685951 26 1176431  r 27 28 29 30 31 32 33 34 35 36 37 38 39  c r   2211837 4169527 7624319 14143037 25450463 46444543 89209343 155691199 298695487 550040063 994660991 1886023151 3502562143   ARITHMETIC  but the result of Theorem D with n = c r  implies that r lg c r  → 1 as  478 4.6.3 For r ≤ 11, the value of c r  is approximately equal to c r − 1  + c r − 2 , and this fact led to speculation by several people that c r  grows like the function ϕr; r → ∞. The values listed here for r > 18 have been computed by Achim Flammenkamp, except that c 24  was first computed by Daniel Bleichenbacher, and c 29  through c 39  by Neill Clift. Flammenkamp notes that c r  is fairly well approximated by the formula 2r exp −θr lg r  for 10 ≤ r ≤ 39, where θ is near ln 2; this agrees nicely with the upper bound  25 . Several people had conjectured at one time that c r  would always be a prime number, in view of the factor method; but c 15 , c 18 , and c 21  are all divisible by 11. Perhaps no conjecture about addition chains is safe! Tabulated values of l n  show that this function is surprisingly smooth; for example, l n  = 13 for all n in the range 1125 ≤ n ≤ 1148. The computer calculations show that a table of l n  may be prepared for 2 ≤ n ≤ 1000 by using the formula   48  where ln = ∞ if n is prime, otherwise ln = l p + l n p  if p is the smallest prime dividing n; and δn = 1 for n in Table 1, δn = 0 otherwise.  l n  = min l n − 1  + 1, ln  − δn,  Let d r  be the number of solutions n to the equation l n  = r. The following  table lists the first few values of d r , according to Flammenkamp and Clift:  r d r  1 1 2 2 3 3 4 5 9 5  r d r  15 6 26 7 8 44 9 78 10 136  r d r  246 11 432 12 13 772 14 1382 15 2481  r d r  4490 16 17 8170 18 14866 19 27128 20 49544  d r  r 21 90371 22 165432 23 303475 24 558275 25 1028508  d r   r 1896704 26 3501029 27 28 6465774 29 11947258 30 22087489  Surely d r  must be an increasing function of r, but there is no evident way to prove this seemingly simple assertion, much less to determine the asymptotic growth of d r  for large r.  The most famous problem about addition chains that is still outstanding is  the Scholz–Brauer conjecture, which states that  l 2n − 1  ≤ n − 1 + l n .   49  Notice that 2n−1 is the worst case for the binary method, because ν 2n−1  = n. E. G. Thurber [Discrete Math. 16  1976 , 279–289] has shown that several of these values, including the case n = 32, can actually be calculated by hand. Computer calculations by Neill Clift [Computing 91  2011 , 265–284] show that l 2n − 1  is in fact exactly equal to n − 1 + l n  for 1 ≤ n ≤ 64. Arnold Scholz coined the name “addition chain”  in German  and posed  49  as a problem in 1937 [Jahresbericht der Deutschen Mathematiker-Vereinigung, Abteilung II, 47  1937 , 41–42]; Alfred Brauer proved in 1939 that  l∗ 2n − 1  ≤ n − 1 + l∗ n .   50    4.6.3  EVALUATION OF POWERS  479  Table 1  229 233 281 283 293 311 317  23 43 59 77 83 107 149  599 611 619 623 631 637 643  553 557 561 569 571 573 581  453 455 457 479 503 509 551  413 419 421 423 429 437 451  371 373 377 381 382 395 403  VALUES OF n FOR SPECIAL ADDITION CHAINS 741 319 749 323 759 347 779 349 787 355 803 359 809 367  903 163 905 165 923 179 941 203 947 211 955 213 983 227 Hansen’s theorems show that l n  can be less than l∗ n , so more work is definitely necessary in order to prove or disprove  49 . As a step in this direction, Hansen has defined the concept of an l0-chain, which lies “between” l-chains and l∗-chains. In an l0-chain, some of the elements are underlined; the condition is that ai = aj + ak, where aj is the largest underlined element less than ai. As an example of an l0-chain  certainly not a minimum one , consider  645 659 667 669 677 683 691  707 709 711 713 715 717 739  813 825 835 837 839 841 845  849 863 869 887 893 899 901   51  it is easy to verify that the difference between each element and the previous underlined element is in the chain. We let l0 n  denote the minimum length of an l0-chain for n. Clearly l n  ≤ l0 n  ≤ l∗ n .  1, 2, 4, 5, 8, 10, 12, 18;  l0 2n − 1  ≤ n − 1 + l0 n .  Hansen pointed out that the chain constructed in Theorem F is an l0-chain  see exercise 22 ; and he also established the following improvement of Eq.  50 : Theorem G. Proof. Let 1 = a0, a1, . . . , ar = n be an l0-chain of minimum length for n, and let 1 = b0, b1, . . . , bt = n be the subsequence of underlined elements.  We may assume that n is underlined.  Then we can get an l0-chain for 2n − 1 as follows: a  Include the l0 n  + 1 numbers 2ai − 1, for 0 ≤ i ≤ r, underlined if and only if ai is underlined. b  Include the numbers 2i 2bj − 1 , for 0 ≤ j < t and for 0 < i ≤ bj+1 − bj, all underlined.  This is a total of b1 − b0 + ··· + bt − bt−1 = n − 1 numbers.   c  Sort the numbers from  a  and  b  into ascending order.  We may easily verify that this gives an l0-chain: The numbers of  b  are all equal to twice some other element of  a  or  b ; furthermore, this element is the preceding underlined element. If ai = bj + ak, where bj is the largest underlined element less than ai, then ak = ai − bj ≤ bj+1 − bj, so 2ak 2bj − 1  = 2ai − 2ak appears underlined in the chain, just preceding 2ai − 1. Since 2ai − 1 is equal to  2ai − 2ak  +  2ak − 1 , where both of these values appear in the chain, we have an addition chain with the l0 property.  The chain corresponding to  51 , constructed in the proof of Theorem G, is  1, 2, 3, 6, 12, 15, 30, 31, 60, 120, 240, 255, 510, 1020, 1023, 2040,  4080, 4095, 8160, 16320, 32640, 65280, 130560, 261120, 262143.  Computations by Neill Clift have shown that l n  < l0 n  when n = 5784689   see exercise 42 . This is the smallest case where Eq.  49  remains in doubt.   480  ARITHMETIC  Graphical representation. An addition chain  1  corresponds in a natural way to a directed graph, where the vertices are labeled ai for 0 ≤ i ≤ r, and where we draw arcs from aj to ai and from ak to ai as a representation of each step ai = aj + ak in  2 . For example, the addition chain 1, 2, 3, 6, 12, 15, 27, 39, 78, 79 that appears in Fig. 15 corresponds to the directed graph  4.6.3  .  If ai = aj + ak for more than one pair of indices  j, k , we choose a definite j and k for purposes of this construction.  In general, all but the first vertex of such a directed graph will be at the head of exactly two arcs; however, this is not really an important property of the graph, because it conceals the fact that many different addition chains can be essentially equivalent. If a vertex has out-degree 1, it is used in only one later step, hence the later step is essentially a sum of three inputs aj + ak + am that might be computed either as  aj+ak +am or as aj+ ak+am  or as ak+ aj+am . These three choices are immaterial, but the addition-chain conventions force us to distinguish between them. We can avoid such redundancy by deleting any vertex whose out-degree is 1 and attaching the arcs from its predecessors to its successor. For example, the graph above would become  .   52  We can also delete any vertex whose out-degree is 0, except of course the final vertex ar, since such a vertex corresponds to a useless step in the addition chain. In this way every addition chain leads to a reduced directed graph that contains one “source” vertex  labeled 1  and one “sink” vertex  labeled n ; every vertex but the source has in-degree ≥ 2 and every vertex but the sink has out-degree ≥ 2. Conversely, any such directed graph without oriented cycles corresponds to at least one addition chain, since we can topologically sort the vertices and write down d − 1 addition steps for each vertex of in-degree d > 0. The length of the addition chain, exclusive of useless steps, can be reconstructed by looking at the reduced graph; it is   number of arcs  −  number of vertices  + 1, since deletion of a vertex of out-degree 1 also deletes one arc.  We say that two addition chains are equivalent if they have the same reduced directed graph. For example, the addition chain 1, 2, 3, 6, 12, 15, 24, 39, 40, 79 is equivalent to the chain we began with, since it also leads to  52 . This example shows that a non-star chain can be equivalent to a star chain. An addition chain is equivalent to a star chain if and only if its reduced directed graph can be topologically sorted in only one way.   53   1  2  3  6  12  15  27  39  78  79  1  3  6  12  39  79   4.6.3  EVALUATION OF POWERS  481  An important property of this graph representation has been pointed out by N. Pippenger: The label of each vertex is exactly equal to the number of oriented paths from the source to that vertex. Thus, the problem of finding an optimal addition chain for n is equivalent to minimizing the quantity  53  over all directed graphs that have one source vertex and one sink vertex and exactly n oriented paths from the source to the sink.  This characterization has a surprising corollary, because of the symmetry of the directed graph. If we reverse the directions of all the arcs, the source and the sink exchange roles, and we obtain another directed graph corresponding to a set of addition chains for the same n; these addition chains have the same length  53  as the chain we started with. For example, if we make the arrows in  52  run from right to left, and if we relabel the vertices according to the number of paths from the right-hand vertex, we get  .   54   One of the star chains corresponding to this reduced directed graph is  1, 2, 4, 6, 12, 24, 26, 52, 78, 79;  we may call this a dual of the original addition chain.  Exercises 39 and 40 discuss important consequences of this graphical repre-  sentation and the duality principle.  EXERCISES 1. [15] What is the value of Z when Algorithm A terminates? 2. [24] Write a MIX program for Algorithm A, to calculate xn mod w given integers n and x, where w is the word size. Assume that MIX has the binary operations SRB, JAE, etc., that are described in Section 4.5.2. Write another program that computes xn mod w in a serial manner  multiplying repeatedly by x , and compare the running times of these programs.   cid:120  3. [22] How is x975 calculated by  a  the binary method?  b  the ternary method?  cid:120  5. [24] Figure 14 shows the first eight levels of the “power tree.” The  k + 1 st level   c  the quaternary method?  d  the factor method? 4. [M20] Find a number n for which the octal  23-ary  method gives ten fewer multiplications than the binary method.  of this tree is defined as follows, assuming that the first k levels have been constructed: Take each node n of the kth level, from left to right in turn, and attach below it the nodes  n + 1, n + a1, n + a2, . . . , n + ak−1 = 2n   in this order , where 1, a1, a2, . . . , ak−1 is the path from the root of the tree to n; but discard any node that duplicates a number that has already appeared in the tree. Design an efficient algorithm that constructs the first r+1 levels of the power tree. [Hint: Make use of two sets of variables LINKU[j], LINKR[j] for 0 ≤ j ≤ 2r; these point upwards and to the right, respectively, if j is a number in the tree.]  79  26  12  6  2  1   482  ARITHMETIC  4.6.3  6. [M26] If a slight change is made to the definition of the power tree that is given in exercise 5, so that the nodes below n are attached in decreasing order  instead of increasing order, we get a tree whose first five levels are  n + ak−1, . . . , n + a2, n + a1, n + 1  Show that this tree gives a method of computing xn that requires exactly as many multiplications as the binary method; therefore it is not as good as the power tree, although it has been constructed in almost the same way. 7. [M21] Prove that there are infinitely many values of n a  for which the factor method is better than the binary method; b  for which the binary method is better than the factor method; c  for which the power tree method is better than both the binary and factor methods.  Here the “better” method is the one that computes xn using fewer multiplications.  8. [M21] Prove that the power tree  exercise 5  never gives more multiplications for the computation of xn than the binary method.   cid:120  9. [25] Design an exponentiation procedure that is analogous to Algorithm A, but  based on radix m = 2e. Your method should perform approximately lg n + ν + m multiplications, where ν is the number of nonzero digits in the m-ary representation of n. 10. [10] Figure 15 shows a tree that indicates one way to compute xn with the fewest possible multiplications, for all n ≤ 100. How can this tree be conveniently represented within a computer, in just 100 memory locations?   cid:120  11. [M26] The tree of Fig. 15 depicts addition chains a0, a1, . . . , ar having l ai  = i  for all i in the chain. Find all addition chains for n that have this property, when n = 43 and when n = 77. Show that any tree such as Fig. 15 must include either the path 1, 2, 4, 8, 9, 17, 34, 43, 77 or the path 1, 2, 4, 8, 9, 17, 34, 68, 77. 12. [M10] Is it possible to extend the tree shown in Fig. 15 to an infinite tree that yields a minimum-multiplication rule for computing xn, for all positive integers n? 13. [M21] Find a star chain of length A + 2 for each of the four cases listed in Theorem C.  Consequently Theorem C holds also with l replaced by l∗.  14. [M29] Complete the proof of Theorem C, by demonstrating that  a  step r − 1 is not a small step; and  b  λ ar−k  cannot be less than m − 1, where m = λ ar−1 . 15. [M43] Write a computer program to extend Theorem C, characterizing all n such that l n  = λ n  + 3 and characterizing all n such that l∗ n  = λ n  + 3. 16. [HM15] Show that Theorem D is not trivially true just because of the binary method; if lB n  denotes the length of the addition chain for n produced by the binary S-and-X method, the ratio lB n  λ n  does not approach a limit as n → ∞.  1  2  5  4  3  8  16  12  10  9  6  7   4.6.3  EVALUATION OF POWERS  483  17. [M25] Explain how to find the intervals J1, . . . , Jh that are required in the proof of Lemma P. 18. [HM24] Let β be a positive constant. Show that there is a constant α < 2 such that   m + s   t + v    2v m + s 2    t + v  β  v  t  < αm  primes, where  for all large m, where the sum is over all s, t, v satisfying  30 . 19. [M23] A “multiset” is like a set, but it may contain identical elements repeated a finite number of times. If A and B are multisets, we define new multisets A ⊎ B, A∪ B, and A∩ B in the following way: An element occurring exactly a times in A and b times in B occurs exactly a + b times in A ⊎ B, exactly max a, b  times in A ∪ B, and exactly min a, b  times in A ∩ B.  A “set” is a multiset that contains no elements more than once; if A and B are sets, so are A∪ B and A∩ B, and the definitions given in this exercise agree with the customary definitions of set union and intersection.  a  The prime factorization of a positive integer n is a multiset N whose elements are p∈N p = n. The fact that every positive integer can be uniquely factored into primes gives us a one-to-one correspondence between the positive integers and the finite multisets of prime numbers; for example, if n = 22 · 33 · 17, the corresponding multiset is N = {2, 2, 3, 3, 3, 17}. If M and N are the multisets corresponding respectively to m and n, what multisets correspond to gcd m, n , lcm m, n , and mn?  way to the multiset F of its “roots”; we have f z  =   b  Every monic polynomial f z  over the complex numbers corresponds in a natural If f z  and g z  are the polynomials corresponding to the finite multisets F and G of complex numbers, what polynomials correspond to F ⊎ G, F ∪ G, and F ∩ G? respect to the three operations ⊎, ∪, ∩.  c  Find as many interesting identities as you can that hold between multisets, with  ζ∈F  z − ζ .  20. [M20] What are the sequences Si and Mij  0 ≤ i ≤ r, 0 ≤ j ≤ t  arising in Hansen’s structural decomposition of star chains  a  of Type 3?  b  of Type 5?  The six “types” are defined in the proof of Theorem B.    cid:120  21. [M26]  W. Hansen.  Let q be any positive integer. Find a value of n such that  l n  ≤ l∗ n  − q. 22. [M20] Prove that the addition chain constructed in the proof of Theorem F is an l0-chain. 23. [M20] Prove Brauer’s inequality  50 .   cid:120  24. [M22] Generalize the proof of Theorem G to show that l0  Bn − 1   B − 1   ≤   n − 1  l0 B  + l0 n , for any integer B > 1; and prove that l 2mn − 1  ≤ l 2m − 1  + mn − m + l0 n . 25. [20] Let y be a fraction, 0 < y < 1, expressed in the binary number system as y =  .d1 . . . dk 2. Design an algorithm to compute xy using the operations of multiplication and square-root extraction.   cid:120  26. [M25] Design an efficient algorithm that computes the nth Fibonacci number Fn,  modulo m, given large integers n and m. 27. [M23]  A. Flammenkamp.  What is the smallest n for which every addition chain contains at least eight small steps?   484  ARITHMETIC  4.6.3  28. [HM33]  A. Schönhage.  The object of this exercise is to give a fairly short proof that l n  ≥ λ n  + lg ν n  − O log log ν n  + 1  . a  When x =  xk . . . x0.x−1 . . .  2 and y =  yk . . . y0.y−1 . . .  2 are real numbers written in binary notation, let us write x ⊆ y if xj ≤ yj for all j. Give a simple rule for con- structing the smallest number z with the property that x′ ⊆ x and y′ ⊆ y implies x′ + y′ ⊆ z. Denoting this number by x∇y, prove that ν x∇y  ≤ ν x  + ν y .  b  Given any addition chain  11  with r = l n , let the sequence d0, d1, . . . , dr be defined as in  35 , and define the sequence A0, A1, . . . , Ar by the following rules: A0 = 1; if ai = 2ai−1 then Ai = 2Ai−1; otherwise if ai = aj + ak for some 0 ≤ k ≤ j < i, then Ai = Ai−1∇ Ai−1 2dj−dk . Prove that this sequence “covers” the given chain, in the sense that ai ⊆ Ai for 0 ≤ i ≤ r. c  Let δ be a positive integer  to be selected later . Call the nondoubling step ai = aj + ak a “baby step” if dj − dk ≥ δ, otherwise call it a “close step.” Let B0 = 1; Bi = 2Bi−1 if ai = 2ai−1; Bi = Bi−1∇ Bi−1 2dj−dk  if ai = aj + ak is a baby step; and Bi = ρ 2Bi−1  otherwise, where ρ x  is the least number y such that x 2e ⊆ y for 0 ≤ e ≤ δ. Show that Ai ⊆ Bi and ν Bi  ≤  1 + δci 2bi for 0 ≤ i ≤ r, where bi and ci respectively denote the number of baby steps and close steps ≤ i. [Hint: Show that the 1s in Bi appear in consecutive blocks of size ≥ 1 + δci.] d  We now have l n  = r = br + cr + dr and ν n  ≤ ν Br  ≤  1 + δcr 2br. Explain how to choose δ in order to obtain the inequality stated at the beginning of this [Hint: See  16 , and note that n ≤ 2rαbr for some α < 1 depending exercise. on δ.]  29. [M49]  K. B. Stolarsky, 1969.  Is ν n  ≤ 2l n −λ n  for all positive integers n?  If so, we have the lower bound l 2n − 1  ≥ n − 1 + ⌈lg n⌉; see  17  and  49 .  30. [20] An addition-subtraction chain has the rule ai = aj ± ak in place of  2 ; the imaginary computer described in the text has a new operation code, SUB.  This corresponds in practice to evaluating xn using both multiplications and divisions.  Find an addition-subtraction chain, for some n, that has fewer than l n  steps. 31. [M46]  D. H. Lehmer.  Explore the problem of minimizing ϵq +  r − q  in an addition chain  1 , where q is the number of “simple” steps in which ai = ai−1 + 1, given a small positive “weight” ϵ.  This problem comes closer to reality for many calculations of xn, if multiplication by x is simpler than a general multiplication; see the applications in Section 4.6.2.  32. [M30]  A. C. Yao, F. F. Yao, R. L. Graham.  Associate the “cost” ajak with each step ai = aj + ak of an addition chain  1 . Show that the left-to-right binary method yields a chain of minimum total cost, for all positive integers n. 33. [15] How many addition chains of length 9 have  52  as their reduced directed graph? 34. [M23] The binary addition chain for n = 2e0 + ··· + 2et, when e0 > ··· > et ≥ 0, is 1, 2, . . . , 2e0−e1, 2e0−e1 + 1, . . . , 2e0−e2 + 2e1−e2, 2e0−e2 + 2e1−e2 + 1, . . . , n. This corresponds to the S-and-X method described at the beginning of this section, while Algorithm A corresponds to the addition chain obtained by sorting the two sequences  1, 2, 4, . . . , 2e0  and  2et−1 +2et , 2et−2 +2et−1 +2et , . . . , n  into increasing order. Prove or disprove: Each of these addition chains is a dual of the other. 35. [M27] How many addition chains without useless steps are equivalent to each of the addition chains discussed in exercise 34, when e0 > e1 + 1?   485  1 xn2  4.6.4  2 . . . xnm   cid:120  36. [25]  E. G. Straus.  Find a way to compute a general monomial xn1  EVALUATION OF POLYNOMIALS  in at most 2λ max n1, n2, . . . , nm   + 2m − m − 1 multiplications. 37. [HM30]  A. C. Yao.  Let l n1, . . . , nm  be the length of the shortest addition chain that contains m given numbers n1 < ··· < nm. Prove that l n1, . . . , nm  ≤ λ nm  + mλ nm  λλ nm  + O λ nm λλλ nm  λλ nm 2 , thereby generalizing  25 . 38. [M47] What is the asymptotic value of l 1, 4, 9, . . . , m2  − m, as m → ∞, in the notation of exercise 37?   cid:120  39. [M25]  J. Olivos, 1979.  Let l [n1, n2, . . . , nm]  be the minimum number of mul-  cid:120  40. [M21]  J. Olivos.  Generalizing the factor method and Theorem F, prove that  m in the sense of exercise 36, tiplications needed to evaluate the monomial xn1 where each ni is a positive integer. Prove that this problem is equivalent to the problem of exercise 37, by showing that l [n1, n2, . . . , nm]  = l n1, n2, . . . , nm  + m − 1. [Hint: Consider directed graphs like  52  that have more than one source vertex.]  2 . . . xnm  1 xn2  m  l m1n1 + ··· + mtnt  ≤ l m1, . . . , mt  + l n1, . . . , nt  + t − 1,  where l n1, . . . , nt  is defined in exercise 37. 41. [M40]  P. Downey, B. Leong, R. Sethi.  Let G be a connected graph with n vertices {1, . . . , n} and m edges, where the edges join uj to vj for 1 ≤ j ≤ m. Prove that l 1, 2, . . . , 2An, 2Au1 +2Av1 +1, . . . , 2Aum +2Avm +1  = An+m+k for all sufficiently large A, where k is the minimum number of vertices in a vertex cover for G  namely a set that contains either uj or vj for 1 ≤ j ≤ m . 42. [22]  Neill Clift, 2005.  Show that neither 1, 2, 4, 8, 16, 32, 64, 65, 97, 128, 256, 353, 706, 1412, 2824, 5648, 11296, 22592, 45184, 90368, 180736, 361472, 361537, 723074, 1446148, 2892296, 5784592, 5784689 nor its dual is an l0-chain. 43. [M50] Is l 2n−1  ≤ n−1+l n  for all integers n > 0? Does equality always hold? 4.6.4. Evaluation of Polynomials Now that we know efficient ways to evaluate the special polynomial xn, let us consider the general problem of computing an nth degree polynomial un ̸= 0,  u x  = unxn + un−1xn−1 + ··· + u1x + u0,   1   for given values of x. This problem arises frequently in practice.  In the following discussion we shall concentrate on minimizing the number of operations required to evaluate polynomials by computer, blithely assuming that all arithmetic operations are exact. Polynomials are most commonly evaluated using floating point arithmetic, which is not exact, and different schemes for the evaluation will, in general, give different answers. A numerical analysis of the accuracy achieved depends on the coefficients of the particular polynomial being considered, and is beyond the scope of this book; the reader should be careful to investigate the accuracy of any calculations undertaken with floating point arithmetic. In most cases the methods we shall describe turn out to be reasonably satisfactory from a numerical standpoint, but many bad examples can also be given. [See Webb Miller, SICOMP 4  1975 , 97–107, for a survey of the literature on stability of fast polynomial evaluation, and for a demonstration that certain kinds of numerical stability cannot be guaranteed for some families of high-speed algorithms.]   486  ARITHMETIC  4.6.4  Throughout this section we will act as if the variable x were a single number. But it is important to keep in mind that most of the methods we will discuss are valid also when the variables are large objects like multiprecision numbers, polynomials, or matrices. In such cases efficient formulas lead to even bigger payoffs, especially when we can reduce the number of multiplications.  A beginning programmer will often evaluate the polynomial  1  in a man- ner that corresponds directly to its conventional textbook form: First unxn is calculated, then un−1xn−1, . . . , u1x, and finally all of the terms of  1  are added together. But even if the efficient methods of Section 4.6.3 are used to evaluate the powers of x in this approach, the resulting calculation is needlessly slow unless nearly all of the coefficients uk are zero. If the coefficients are all nonzero, an obvious alternative would be to evaluate  1  from right to left, computing the values of xk and ukxk +··· + u0 for k = 1, . . . , n. Such a process involves 2n− 1 multiplications and n additions, and it might also require further instructions to store and retrieve intermediate results from memory. Horner’s rule. One of the first things a novice programmer is usually taught is an elegant way to rearrange this computation, by evaluating u x  as follows:  2  Start with un, multiply by x, add un−1, multiply by x, . . . , multiply by x, add u0. This form of the computation is usually called “Horner’s rule”; we have already seen it used in connection with radix conversion in Section 4.4. The entire process requires n multiplications and n additions, minus one addition for each coefficient that is zero. Furthermore, there is no need to store partial results, since each quantity arising during the calculation is used immediately after it has been computed.  u x  =. . .  unx + un−1 x + ···x + u0.  W. G. Horner gave this rule early in the nineteenth century [Philosophical Transactions, Royal Society of London 109  1819 , 308–335] in connection with a procedure for calculating polynomial roots. The fame of the latter method [see J. L. Coolidge, Mathematics of Great Amateurs  Oxford, 1949 , Chapter 15] accounts for the fact that Horner’s name has been attached to  2 ; but actually Isaac Newton had made use of the same idea more than 150 years earlier. For example, in a well-known work entitled De Analysi per Æquationes Infinitas, originally written in 1669, Newton wrote  y − 4 × y : + 5 × y : − 12 × y : + 17  for the polynomial y4 − 4y3 + 5y2 − 12y + 17, while illustrating what later came to be known as Newton’s method for rootfinding. This clearly shows the idea of  2 , since he often denoted grouping by using horizontal lines and colons instead of parentheses. Newton had been using the idea for several years in unpublished notes. [See The Mathematical Papers of Isaac Newton, edited by D. T. Whiteside, 1  1967 , 490, 531; 2  1968 , 222.] Independently, a method equivalent to Horner’s had in fact been used in 13th-century China by Ch’in Chiu-Shao [see Y. Mikami, The Development of Mathematics in China and Japan  1913 , 73–77].   4.6.4  EVALUATION OF POLYNOMIALS  487  Several generalizations of Horner’s rule have been suggested. Let us first consider evaluating u z  when z is a complex number, while the coefficients uk are real. In particular, when z = eiθ = cos θ + i sin θ, the polynomial u z  is essentially two Fourier series,   u0 + u1 cos θ + ··· + un cos nθ  + i u1 sin θ + ··· + un sin nθ .  Complex addition and multiplication can obviously be reduced to a sequence of ordinary operations on real numbers:  real + complex complex + complex real × complex complex × complex  requires requires requires requires or  1 addition 2 additions 2 multiplications 4 multiplications, 2 additions 3 multiplications, 5 additions   See exercise 41. Subtraction is considered here as if it were equivalent to addition.  Therefore Horner’s rule  2  uses either 4n − 2 multiplications and 3n − 2 additions or 3n − 1 multiplications and 6n − 5 additions to evaluate u z  when z = x+iy is complex. Actually 2n−4 of these additions can be saved, since we are multiplying by the same number z each time. An alternative procedure for evaluating u x + iy  is to let  s = x2 + y2;  r = x + x, 1 < j ≤ n.  a1 = un, aj = bj−1 + raj−1,  b1 = un−1, bj = un−j − saj−1,   3  Then it is easy to prove by induction that u z  = zan + bn. This scheme [BIT 5  1965 , 142; see also G. Goertzel, AMM 65  1958 , 34–35] requires only 2n + 2 multiplications and 2n + 1 additions, so it is an improvement over Horner’s rule when n ≥ 3. In the case of Fourier series, when z = eiθ, we have s = 1, so the number of multiplications drops to n + 1. The moral of this story is that a good programmer does not make indiscriminate use of the built-in complex-arithmetic features of high-level programming languages. Consider the process of dividing the polynomial u x  by x − x0, using Algorithm 4.6.1D to obtain u x  =  x − x0 q x  + r x ; here deg r  < 1, so r x  is a constant independent of x, and u x0  = 0 · q x0  + r = r. An examination of this division process reveals that the computation is essentially the same as Horner’s rule for evaluating u x0 . Similarly, if we divide u z  by the polynomial  z − z0  z − ¯z0  = z2 − 2x0z + x2 0, the resulting computation turns out to be equivalent to  3 ; we obtain u z  =  z − z0  z − ¯z0 q z  + anz + bn, hence u z0  = anz0 + bn. In general, if we divide u x  by f x  to obtain u x  = f x q x  + r x , and if f x0  = 0, we have u x0  = r x0 ; this observation leads to further generalizations of Horner’s rule. For example, we may let f x  = x2 − x2 0; this yields the “second-order” Horner’s rule  u x  =. . .  u2⌊n 2⌋x2 + u2⌊n 2⌋−2 x2 + ··· x2 + u0  + . . .  u2⌈n 2⌉−1 x2 + u2⌈n 2⌉−3 x2 + ···  x2 + u1   x.  0 + y2   4    488  ARITHMETIC  4.6.4  The second-order rule uses n+1 multiplications and n additions  see exercise 5 ; so it is no improvement over Horner’s rule from this standpoint. But there are at least two circumstances in which  4  is useful: If we want to evaluate both u x  and u −x , this approach yields u −x  with just one more addition operation; two values can be obtained almost as cheaply as one. Moreover, if we have a computer that allows parallel computations, the two lines of  4  may be evaluated independently, so we save about half the running time.  once, a “kth-order” Horner’s rule obtained in a similar manner from f x  =  may be used. Another attractive method for parallel computation has  xk − xk0 been suggested by G. Estrin [Proc. Western Joint Computing Conf. 17  1960 , 33–40]; for n = 7, Estrin’s method is:  When our computer allows parallel computation on k arithmetic units at  x2 x4  Processor 5  Processor 2 b1 = u5x + u4  Processor 4 d1 = u1x + u0  Processor 3 c1 = u3x + u2 c2 = c1x2 + d1  Processor 1 a1 = u7x + u6 a2 = a1x2 + b1 a3 = a2x4 + c2 Here a3 = u x . However, an interesting analysis by W. S. Dorn [IBM J. Res. and Devel. 6  1962 , 239–245] shows that these methods might not actually be an improvement over the second-order rule, if each arithmetic unit must access a memory that communicates with only one processor at a time. Tabulating polynomial values. If we wish to evaluate an nth degree polyno-  mial at many points in an arithmetic progressionthat is, if we want to calculate u x0 , u x0 + h , u x0 + 2h , . . ., the process can be reduced to addition  only, after the first few steps. For if we start with any sequence of numbers  α0, α1, . . . , αn  and apply the transformation  α0 ← α0 + α1, α1 ← α1 + α2, we find that k applications of  5  yields  . . . , αn−1 ← αn−1 + αn,   5    k  j = α  βj +  βj+1 +  βj+2 + ··· ,  0 ≤ j ≤ n,  where βj denotes the initial value of αj and βj = 0 for j > n. In particular,   k    0   k  k  1     0     k  k   2  1   k  0 = α  β0 +  β1 + ··· +   6    k    n  βn  is a polynomial of degree n in k. By properly choosing the β’s, as shown  k  in exercise 7, we can set things up so that this quantity α is the desired 0 value u x0 + kh , for all k. In other words, each execution of the n additions in  5  will produce the next value of the given polynomial.  Caution: Rounding errors can accumulate after many repetitions of  5 , and an error in αj produces a corresponding error in the coefficients of x0, . . . , xj in the polynomial being computed. Therefore the values of the α’s should be “refreshed” after a large number of iterations.   4.6.4  EVALUATION OF POLYNOMIALS  489  Derivatives and changes of variable. Sometimes we want to find the coeffi- cients of u x+ x0 , given a constant x0 and the coefficients of u x . For example, if u x  = 3x2 +2x−1, then u x−2  = 3x2−10x+7. This is analogous to a radix conversion problem, converting from base x to base x + 2. By Taylor’s theorem, the new coefficients are given by the derivatives of u x  at x = x0, namely  u x + x0  = u x0  + u′ x0 x +u′′ x0  2!x2 + ··· +u n  x0  n!xn,   7   so the problem is equivalent to evaluating u x  and all its derivatives.  If we write u x  = q x  x − x0  + r, then u x + x0  = q x + x0 x + r; so r is the constant coefficient of u x + x0 , and the problem reduces to finding the coefficients of q x+ x0 , where q x  is a known polynomial of degree n−1. Thus the following algorithm is indicated: H1. Set vj ← uj for 0 ≤ j ≤ n. H2. For k = 0, 1, . . . , n − 1  in this order , set vj ← vj + x0vj+1 for j = n − 1,  . . . , k + 1, k  in this order .  0, . . . , xn0.  At the conclusion of step H2 we have u x + x0  = vnxn + ··· + v1x + v0. This procedure was a principal part of Horner’s root-finding method, and when k = 0 it is exactly rule  2  for evaluating u x0 .  Horner’s method requires  n2+n  2 multiplications and  n2+n  2 additions; but notice that if x0 = 1 we avoid all of the multiplications. Fortunately we can reduce the general problem to the case x0 = 1 by introducing comparatively few multiplications and divisions: S1. Compute and store the values x2 S2. Set vj ← ujxj  S3. Perform step H2 but with x0 = 1. Now v x  = ux0 x+1  = u x0x+x0 .  0 for 0 ≤ j ≤ n. Now v x  = u x0x . 0 for 0 < j ≤ n. Now v x  = u x + x0  as desired.  S4. Set vj ← vj xj This idea, due to M. Shaw and J. F. Traub [JACM 21  1974 , 161–167], has the same number of additions and the same numerical stability as Horner’s method; but it needs only 2n−1 multiplications and n−1 divisions, since vn = un. About 1 2 n of these multiplications can, in turn, be avoided  see exercise 6 . If we want only the first few or the last few derivatives, Shaw and Traub have observed that there are further ways to save time. For example, if we just want to evaluate u x  and u′ x , we can do the job with 2n − 1 additions and about n +  D1. Compute and store the values x2, x3, . . . , xt, x2t, where t =n 2. D2. Set vj ← ujxf j  for 0 ≤ j ≤ n, where f j  = t − 1 − n − 1 − j  mod 2t  2n multiplications divisions as follows:  √  for 0 ≤ j < n, and f n  = t.  D3. Set vj ← vj + vj+1xg j  for j = n−1, . . . , 1, 0; here g j  = 2t when n−1− j is a positive multiple of 2t, otherwise g j  = 0 and the multiplication by xg j  need not be done. D4. Set vj ← vj + vj+1xg j  for j = n − 1, . . . , 2, 1. Now v0 xf 0  = u x  and  v1 xf 1  = u′ x .   490  ARITHMETIC  4.6.4  Adaptation of coefficients. Let us now return to our original problem of evaluating a given polynomial u x  as rapidly as possible, for “random” values of x. The importance of this problem is due partly to the fact that standard functions such as sin x, cos x, ex, etc., are usually computed by subroutines that rely on the evaluation of certain polynomials; such polynomials are evaluated so often, it is desirable to find the fastest possible way to do the computation.  Arbitrary polynomials of degree five and higher can be evaluated with fewer operations than Horner’s rule requires, if we first “adapt” or “precondition” the coefficients u0, u1, . . . , un. This adaptation process might involve a lot of work, as explained below; but the preliminary calculation is not wasted, since it must be done only once while the polynomial will be evaluated many times. For examples of “adapted” polynomials for standard functions, see V. Y. Pan, USSR Computational Math. and Math. Physics 2  1963 , 137–146.  The simplest case for which adaptation of coefficients is helpful occurs for a  fourth degree polynomial:  u4 ̸= 0.  α4,  y =  x + α0 x + α1,  u x  = u4x4 + u3x3 + u2x2 + u1x + u0,  u x  = y + x + α2 y + α3   8  This equation can be rewritten in a form originally suggested by T. S. Motzkin,  9  for suitably “adapted” coefficients α0, α1, α2, α3, α4. The computation in this scheme involves three multiplications, five additions, and  on a one-accumulator machine like MIX  one instruction to store the partial result y into temporary storage. By comparison with Horner’s rule, we have traded a multiplication for an addition and a possible storage command. Even this comparatively small change is worthwhile if the polynomial is to be evaluated often.  Of course, if the time for multiplication is comparable to the time for addition,  9  gives no improvement; we will see that a general fourth-degree polynomial always requires at least eight arithmetic operations for its evaluation.   By equating coefficients in  8  and  9 , we obtain formulas for computing  the αj’s in terms of the uk’s:  2 u3 u4 − 1 ,  β = u2 u4 − α0 α0 + 1 , α3 = u0 u4 − α1 α1 + α2 ,  α0 = 1 α2 = β − 2α1,  10  A similar scheme, which evaluates a fourth-degree polynomial in the same num- ber of steps as  9 , appears in exercise 18; this alternative method will give greater numerical accuracy than  9  in certain cases, although it yields poorer accuracy in others.  α1 = u1 u4 − α0β, α4 = u4.  Polynomials that arise in practice often have a rather small leading coeffi- cient, so that the division by u4 in  10  leads to instability. In such a case it is usually preferable to replace x by u41 4 x as the first step, reducing  8  to a polynomial whose leading coefficient is ±1. A similar transformation applies to polynomials of higher degrees. This idea is due to C. T. Fike [CACM 10  1967 , 175–178], who has presented several interesting examples.   4.6.4  EVALUATION OF POLYNOMIALS  491  α5.  Any polynomial of the fifth degree may be evaluated using four multiplica- tions, six additions, and one storing, by using the rule u x  = U x x+ u0, where U x  = u5x4+u4x3+u3x2+u2x+u1 is evaluated as in  9 . Alternatively, we can do the evaluation with four multiplications, five additions, and three storings, if the calculations take the form  u x  =  y + α1 y + α2  x + α3  + α4  y =  x + α0 2,   11  The determination of the α’s this time requires the solution of a cubic equation  see exercise 19 .  On many computers the number of “storing” operations required by  11  is less than 3; for example, we may be able to compute  x + α0 2 without storing x+α0. In fact, most computers nowadays have more than one arithmetic register for floating point calculations, so we can avoid storing altogether. Because of the wide variety of features available for arithmetic on different computers, we shall henceforth in this section count only the arithmetic operations, not the operations of storing and loading an accumulator. The computation schemes can usually be adapted to any particular computer in a straightforward manner, so that very few of these auxiliary operations are necessary; on the other hand, it must be remembered that overhead costs might well overshadow the fact that we are saving a multiplication or two, especially if the machine code is being produced by a compiler that does not optimize. A polynomial u x  = u6x6 + ··· + u1x + u0 of degree six can always be  evaluated using four multiplications and seven additions, with the scheme  z =  x + α0 x + α1,  u x  = w + z + α4 w + α5  α6.  w =  x + α2 z + α3,   12  [See D. E. Knuth, CACM 5  1962 , 595–599.] This saves two of the six multi- plications required by Horner’s rule. Here again we must solve a cubic equation: Since α6 = u6, we may assume that u6 = 1. Under this assumption, let  β1 =  u5 − 1  2,  β2 = u4 − β1 β1 + 1 ,  β3 = u3 − β1β2,  β4 = β1 − β2,  β5 = u2 − β1β3.  Let β6 be a real root of the cubic equation  2y3 +  2β4 − β2 + 1 y2 +  2β5 − β2β4 − β3 y +  u1 − β2β5  = 0.   13   This equation always has a real root, since the polynomial on the left approaches +∞ for large positive y, and it approaches −∞ for large negative y; it must assume the value zero somewhere in between.  Now if we define β8 = β3 − β6 − β7,  6 + β4β6 + β5,  β7 = β2  we have finally  α0 = β2 − 2β6, α3 = β7 − α1α2,  α2 = β1 − α0, α4 = β8 − β7 − α1,  α1 = β6 − α0α2, α5 = u0 − β7β8.   14    492  ARITHMETIC  4.6.4   15   We can illustrate this procedure with a contrived example: Suppose that we want to evaluate x6 + 13x5 + 49x4 + 33x3 − 61x2 − 37x + 3. We obtain α6 = 1, β1 = 6, β2 = 7, β3 = −9, β4 = −1, β5 = −7, and so we meet with the cubic equation  2y3 − 8y2 + 2y + 12 = 0.  This equation has β6 = 2 as a root, and we continue to find  β7 = −5,  β8 = −6,  α0 = 3, α2 = 3, α1 = −7, α3 = 16, α4 = 6, α5 = −27.  The resulting scheme is therefore  z =  x + 3 x − 7,  w =  x + 3 z + 16,  u x  =  w + z + 6 w − 27.  By sheer coincidence the quantity x + 3 appears twice here, so we have found a method that uses three multiplications and six additions.  Another method for handling sixth-degree equations has been suggested by V. Y. Pan [Problemy Kibernetiki 5  1961 , 17–29]. His method requires one more addition operation, but it involves only rational operations in the preliminary steps; no cubic equation needs to be solved. We may proceed as follows:  z =  x + α0 x + α1,  u x  =  z − x + α3 w + α4 z + α5  w = z + x + α2,  α6.   16  To determine the α’s, we divide the polynomial once again by u6 = α6 so that u x  becomes monic. It can then be verified that α0 = u5 3 and that 0   u3 − 2α0u4 + 5α3 0 .   17  Note that Pan’s method requires that the denominator in  17  does not vanish. In other words,  16  can be used only when  α1 =  u1 − α0u2 + α2  0u3 − α3  0u4 + 2α5   18  in fact, this quantity should not be so small that α1 becomes too large. Once α1 has been determined, the remaining α’s may be determined from the equations  6 − 18u6u5u4 + 5u3  5 ̸= 0;  27u3u2  β2 = u4 − α0β1 − α1, β4 = u2 − α0β3 − α1β2,  0 − 1  − α1,  β3 −  α0 − 1 β2 +  α0 − 1  α2  β1 = 2α0, β3 = u3 − α0β2 − α1β1, α3 = 1 2 α2 = β2 −  α2 α5 = u0 − α1β4.  19  We have discussed the cases of degree n = 4, 5, 6 in detail because the smaller values of n arise most frequently in applications. Let us now consider a general evaluation scheme for nth degree polynomials, a method that involves at most ⌊n 2⌋ + 2 multiplications and n additions.  α4 = β4 −  α2 + α1  α3 + α1 ,  0 − 1  − α3 − 2α1,   y = x + c,  EVALUATION OF POLYNOMIALS  4.6.4 493 Theorem E. Every nth degree polynomial  1  with real coefficients, n ≥ 3, can be evaluated by the scheme    uny + α0 y + β0, n even, u x  =. . .   z w − α1  + β1  w − α2  + β2  . . . w − αm  + βm,   20  for suitable real parameters c, αk and βk, where m = ⌈n 2⌉ − 1. In fact, it is possible to select these parameters so that βm = 0. Proof. Let us first examine the circumstances under which the α’s and β’s can be chosen in  20 , if c is fixed. Let  uny + β0,  w = y2;  n odd,  z =  p x  = u x − c  = anxn + an−1xn−1 + ··· + a1x + a0.   21  We want to show that p x  has the form p1 x  x2−αm +βm for some polynomial p1 x  and some constants αm, βm. If we divide p x  by x2 − αm, we can see that the remainder βm is a constant only if the auxiliary polynomial  q x  = a2m+1xm + a2m−1xm−1 + ··· + a1,   22  formed from every odd-numbered coefficient of p x , is a multiple of x − αm. Conversely, if q x  has x− αm as a factor, then p x  = p1 x  x2 − αm  + βm, for some constant βm that may be determined by division. Similarly, we want p1 x  to have the form p2 x  x2 − αm−1  + βm−1, and this is the same as saying that q x   x − αm  is a multiple of x − αm−1; for if q1 x  is the polynomial corresponding to p1 x  as q x  corresponds to p x , we have q1 x  = q x   x − αm . Continuing in the same way, we find that the parameters α1, β1, . . . , αm, βm will exist if and only if  q x  = a2m+1 x − α1  . . .  x − αm .   23  In other words, either q x  is identically zero  and this can happen only when n is even , or else q x  is an mth degree polynomial having all real roots. Now we have a surprising fact discovered by J. Eve [Numer. Math. 6  1964 , 17–21]: If p x  has at least n − 1 complex roots whose real parts are all nonneg- ative, or all nonpositive, then the corresponding polynomial q x  is identically zero or has all real roots.  See exercise 23.  Since u x  = 0 if and only if p x + c  = 0, we need merely choose the parameter c large enough that at least n−1 of the roots of u x  = 0 have a real part ≥ −c, and  20  will apply whenever an−1 = un−1 − ncun ̸= 0. We can also determine c so that these conditions are fulfilled and also that βm = 0. First the n roots of u x  = 0 are determined. If a + bi is a root having the largest or the smallest real part, and if b ̸= 0, let c = −a and αm = −b2; then x2 − αm is a factor of u x− c . If the root with smallest or largest real part is real, but the root with second smallest  or second largest  real part is nonreal, the same transformation applies. If the two roots with smallest  or largest  real parts are both real, they can be expressed in the form a−b and a+b, respectively; let c = −a and αm = b2. Again x2− αm is a factor of u x− c .  Still other values   494  ARITHMETIC  4.6.4  of c are often possible; see exercise 24.  The coefficient an−1 will be nonzero for at least one of these alternatives, unless q x  is identically zero.  Note that this method of proof usually gives at least two values of c, and we also have the chance to permute α1, . . . , αm−1 in  m − 1 ! ways. Some of these alternatives may give more desirable numerical accuracy than others. Questions of numerical accuracy do not arise, of course, when we are working with integers modulo m instead of with real numbers. Scheme  9  works for n = 4 when m is relatively prime to 2u4, and  16  works for n = 6 when m is relatively prime to 6u6 and to the denominator of  17 . Exercise 44 shows that n 2+ O log n  multiplications and O n  additions suffice for any monic nth degree polynomial modulo any m. *Polynomial chains. Now let us consider questions of optimality. What are the best possible schemes for evaluating polynomials of various degrees, in terms of the minimum possible number of arithmetic operations? This question was first analyzed by A. M. Ostrowski in the case that no preliminary adaptation of coefficients is allowed [Studies in Mathematics and Mechanics Presented to R. von Mises  New York: Academic Press, 1954 , 40–48], and by T. S. Motzkin in the case of adapted coefficients [see Bull. Amer. Math. Soc. 61  1955 , 163]. In order to investigate this question, we can extend Section 4.6.3’s concept of addition chains to the notion of polynomial chains. A polynomial chain is a sequence of the form  x = λ0,  λ1,  . . . , λr = u x ,   24   where u x  is some polynomial in x, and for 1 ≤ i ≤ r  0 ≤ j, k < i, 0 ≤ k < i.  either λi =  ±λj  ◦ λk,  or λi = αj ◦ λk,   25  Here “◦” denotes any of the three operations “+”, “−”, or “×”, and αj denotes a so-called parameter. Steps of the first kind are called chain steps, and steps of the second kind are called parameter steps. We shall assume that a different parameter αj is used in each parameter step; if there are s parameter steps, they should involve α1, α2, . . . , αs in this order.  It follows that the polynomial u x  at the end of the chain has the form  u x  = qnxn + ··· + q1x + q0,   26  where qn, . . . , q1, q0 are polynomials in α1, α2, . . . , αs with integer coefficients. We shall interpret the parameters α1, α2, . . . , αs as real numbers, and we shall therefore restrict ourselves to considering the evaluation of polynomials with real coefficients. The result set R of a polynomial chain is defined to be the set of all vectors  qn, . . . , q1, q0  of real numbers that occur as α1, α2, . . . , αs independently assume all possible real values. If for every choice of t + 1 distinct integers j0, . . . , jt ∈ {0, 1, . . . , n} there is a nonzero multivariate polynomial fj0...jt with integer coefficients such that fj0...jt qj0 , . . . , qjt  = 0 for all  qn, . . . , q1, q0  in R, let us say that the result   4.6.4  EVALUATION OF POLYNOMIALS  495  set R has at most t degrees of freedom, and that the chain  24  has at most t degrees of freedom. We also say that the chain  24  computes a given polynomial u x  = unxn + ··· + u1x + u0 if  un, . . . , u1, u0  is in R. It follows that a polynomial chain with at most n degrees of freedom cannot compute all nth degree polynomials  see exercise 27 .  As an example of a polynomial chain, consider the following chain corre-  sponding to Theorem E, when n is odd:  λ0 = x λ1 = α1 + λ0 λ2 = λ1 × λ1 λ3 = α2 × λ1  λ1+3i λ2+3i λ3+3i  = α1+2i + λ3i = α2+2i + λ2 = λ1+3i × λ2+3i   1 ≤ i < n 2.   27   There are ⌊n 2⌋ + 2 multiplications and n additions; ⌊n 2⌋ + 1 chain steps and n + 1 parameter steps. By Theorem E, the result set R includes the set of all  un, . . . , u1, u0  with un ̸= 0, so  27  computes all polynomials of degree n. We cannot prove that R has at most n degrees of freedom, since the result set has n + 1 independent components.  A polynomial chain with s parameter steps has at most s degrees of freedom. In a sense, this is obvious: We can’t compute a function with t degrees of freedom using fewer than t arbitrary parameters. But this intuitive fact is not easy to prove formally; for example, there are continuous functions  “space-filling curves”  that map the real line onto a plane, and such functions map a single parameter into two independent parameters. For our purposes, we need to verify that no polynomial functions with integer coefficients can have such a property; a proof appears in exercise 28.  Given this fact, we can proceed to prove the results we seek:  Theorem M  T. S. Motzkin, 1954 . A polynomial chain with m > 0 multipli- cations has at most 2m degrees of freedom. Proof. Let µ1, µ2, . . . , µm be the λi’s of the chain that are multiplication operations. Then  for 1 ≤ i ≤ m  µi = S2i−1 × S2i  and   28  where each Sj is a certain sum of µ’s, x’s, and α’s. Write Sj = Tj + βj, where Tj is a sum of µ’s and x’s while βj is a sum of α’s.  u x  = S2m+1,  Now u x  is expressible as a polynomial in x, β1, . . . , β2m+1 with integer coefficients. Since the β’s are expressible as linear functions of α1, . . . , αs, the set of values represented by all real values of β1, . . . , β2m+1 contains the result set of the chain. Therefore there are at most 2m+1 degrees of freedom; this can be improved to 2m when m > 0, as shown in exercise 30.   496  ARITHMETIC  4.6.4  An example of the construction in the proof of Theorem M appears in  exercise 25. A similar result can be proved for additions: Theorem A  É. G. Belaga, 1958 . A polynomial chain containing q additions and subtractions has at most q + 1 degrees of freedom. Proof. chain that correspond to addition or subtraction operations. Then  [Problemy Kibernetiki 5  1961 , 7–15.] Let κ1, . . . , κq be the λi’s of the  and  for 1 ≤ i ≤ q  κi = ±T2i−1 ± T2i  u x  = T2q+1,   29  where each Tj is a product of κ’s, x’s, and α’s. We may write Tj = Aj Bj, where Aj is a product of α’s and Bj is a product of κ’s and x’s. The following transformation may now be made to the chain, successively for i = 1, 2, . . . , q: Let βi = A2i A2i−1, so that κi = A2i−1 ±B2i−1 ± βi B2i . Then change κi to ±B2i−1 ± βi B2i, and replace each occurrence of κi in future formulas T2i+1, T2i+2, . . . , T2q+1 by A2i−1κi.  This replacement may change the values of A2i+1, A2i+2, . . . , A2q+1.   After the transformation has been done for all i, let βq+1 = A2q+1; then u x  can be expressed as a polynomial in β1, . . . , βq+1, and x, with integer coefficients. We are almost ready to complete the proof, but we must be careful because the polynomials obtained as β1, . . . , βq+1 range over all real values may not include all polynomials representable by the original chain  see exercise 26 ; it is possible to have A2i−1 = 0, for some values of the α’s, and this makes βi undefined. To complete the proof, let us observe that the result set R of the original chain can be written R = R1 ∪ R2 ∪ ··· ∪ Rq ∪ R′, where Ri is the set of result vectors possible when A2i−1 = 0, and where R′ is the set of result vectors possible when all α’s are nonzero. The discussion above proves that R′ has at most q + 1 degrees of freedom. If A2i−1 = 0, then T2i−1 = 0, so addition step κi may be dropped to obtain another chain computing the result set Ri; by induction we see that each Ri has at most q degrees of freedom. Hence by exercise 29, R has at most q + 1 degrees of freedom. Theorem C. If a polynomial chain  24  computes all nth degree polynomials u x  = unxn + ··· + u0, for some n ≥ 2, then it includes at least ⌊n 2⌋ + 1 multiplications and at least n addition-subtractions. Proof. Let there be m multiplication steps. By Theorem M, the chain has at most 2m degrees of freedom, so 2m ≥ n + 1. Similarly, by Theorem A there are ≥ n addition-subtractions.  This theorem states that no single method having fewer than ⌊n 2⌋ + 1 multiplications or fewer than n additions can evaluate all possible nth degree polynomials. The result of exercise 29 allows us to strengthen this and say that no finite collection of such polynomial chains will suffice for all polynomials of a given degree. Some special polynomials can, of course, be evaluated more efficiently; all we have really proved is that polynomials whose coefficients are algebraically independent, in the sense that they satisfy no nontrivial polynomial equation,   EVALUATION OF POLYNOMIALS  4.6.4 497 require ⌊n 2⌋ + 1 multiplications and n additions. Unfortunately the coeffi- cients we deal with in computers are always rational numbers, so the theorems above don’t really apply; in fact, exercise 42 shows that we can always get by √ with O  n   multiplications  and a possibly huge number of additions . From a practical standpoint, the bounds of Theorem C apply to “almost all” coefficients, and they seem to apply to all reasonable schemes for evaluation. Furthermore it is possible to obtain lower bounds corresponding to those of Theorem C even in the rational case: By strengthening the proofs above, V. Strassen has shown, for example, that the polynomial  u x  =  22kn3  xk   30   n  k=0  cannot be evaluated by any polynomial chain of length < n2  lg n unless the 2 n−2 multiplications and n−4 additions [SICOMP 3  1974 , chain has at least 1 128–149]. The coefficients of  30  are very large; but it is also possible to find polynomials whose coefficients are just 0s and 1s, such that every polynomial chain computing them involves at least n  4 lg n  chain multiplications, for all sufficiently large n, even when the parameters αj are allowed to be arbitrary complex numbers. [See R. J. Lipton, SICOMP 7  1978 , 61–69; C.-P. Schnorr, Lecture Notes in Comp. Sci. 53  1977 , 135–147.] Jean-Paul van de Wiele has shown that the evaluation of certain 0–1 polynomials requires a total of at least cn log n arithmetic operations, for some c > 0 [FOCS 19  1978 , 159–165].  √  A gap still remains between the lower bounds of Theorem C and the actual operation counts known to be achievable, except in the trivial case n = 2. Theorem E gives ⌊n 2⌋ + 2 multiplications, not ⌊n 2⌋ + 1, although it does achieve the minimum number of additions. Our special methods for n = 4 and n = 6 have the minimum number of multiplications, but one extra addition. When n is odd, it is not difficult to prove that the lower bounds of Theorem C cannot be achieved simultaneously for both multiplications and additions; see exercise 33. For n = 3, 5, and 7, it is possible to show that at least ⌊n 2⌋ + 2 multiplications are necessary. Exercises 35 and 36 show that the lower bounds of Theorem C cannot both be achieved when n = 4 or n = 6; thus the methods we have discussed are best possible, for n < 8. When n is even, Motzkin proved that ⌊n 2⌋ + 1 multiplications are sufficient, but his construction involves an indeterminate number of additions  see exercise 39 . An optimal scheme for n = 8 was found by V. Y. Pan, who showed that n + 1 additions are necessary and sufficient for this case when there are ⌊n 2⌋ + 1 multiplications; he also showed that ⌊n 2⌋ + 1 multiplications and n + 2 additions will suffice for all even n ≥ 10. Pan’s paper [STOC 10  1978 , 162–172] also establishes the exact minimum number of multiplications and additions needed when calculations are done entirely with complex numbers instead of reals, for all degrees n. Exercise 40 discusses the interesting situation that arises for odd values of n ≥ 9.  It is clear that the results we have obtained about chains for polynomials in a single variable can be extended without difficulty to multivariate polynomials.   498  ARITHMETIC  4.6.4  For example, if we want to find an optimum scheme for polynomial evaluation without adaptation of coefficients, we can regard u x  as a polynomial in the n + 2 variables x, un, . . . , u1, u0; exercise 38 shows that n multiplications and n additions are necessary in this case. Indeed, A. Borodin [Theory of Machines and Computations, edited by Z. Kohavi and A. Paz  New York: Academic Press, 1971 , 45–58] has proved that Horner’s rule  2  is essentially the only way to compute u x  in 2n operations without preconditioning.  With minor variations, the methods above can be extended to chains involv- ing division, that is, to rational functions as well as polynomials. Curiously, the continued-fraction analog of Horner’s rule now turns out to be optimal from an operation-count standpoint, if multiplication and division speeds are equal, even when preconditioning is allowed  see exercise 37 .  Sometimes division is helpful during the evaluation of polynomials, even though polynomials are defined only in terms of multiplication and addition; we have seen examples of this in the Shaw–Traub algorithms for polynomial derivatives. Another example is the polynomial xn + ··· + x + 1;  since this polynomial can be written  xn+1 − 1   x − 1 , we can evaluate it with l n + 1  multiplications  see Section 4.6.3 , two subtractions, and one division, while techniques that avoid division seem to require about three times as many operations  see exercise 43 . Special multivariate polynomials. The determinant of an n× n matrix may be considered to be a polynomial in n2 variables xij, 1 ≤ i, j ≤ n. If x11 ̸= 0, we have    x11 x12 . . . x1n x21 x22 . . . x2n x31 x32 . . . x3n  det  ...  ...  xn1 xn2 . . . xnn   = x11 det    x22 −  x21 x11 x12 . . . x2n −  x21 x11 x1n x32 −  x31 x11 x12 . . . x3n −  x31 x11 x1n  xn2 −  xn1 x11 x12 . . . xnn −  xn1 x11 x1n  ...  ...   .   31   The determinant of an n × n matrix may therefore be evaluated by evaluating the determinant of an  n − 1  ×  n − 1  matrix and performing an additional  n − 1 2 + 1 multiplications,  n − 1 2 additions, and n − 1 divisions. Since a 2 × 2 determinant can be evaluated with two multiplications and one addition, we see that the determinant of almost all matrices  namely those for which no division by zero is needed  can be computed with at most  2n3 −3n2 +7n−6  6 multiplications,  2n3 − 3n2 + n  6 additions, and  n2 − n − 2  2 divisions. if x11 = 0 but x21 ̸= 0, we have  When zero occurs, the determinant is even easier to compute. For example,    det  0 x12 . . . x1n x21 x22 . . . x2n x31 x32 . . . x3n  ...  ...  xn1 xn2 . . . xnn   = −x21 det    x32 −  x31 x21 x22 . . . x3n −  x31 x21 x2n  . . .  x1n  x12  ...  xn2 −  xn1 x21 x22 . . . xnn −  xn1 x21 x2n  ...   .   32   ...  ...   EVALUATION OF POLYNOMIALS  4.6.4 499 Here the reduction to an  n − 1  ×  n − 1  determinant saves n − 1 of the multiplications and n−1 of the additions used in  31 , in compensation for the ad- ditional bookkeeping required to recognize this case. Thus any determinant can be evaluated with roughly 2 3 n3 arithmetic operations  including division ; this is remarkable, since it is a polynomial with n! terms and n variables in each term. If we want to evaluate the determinant of a matrix with integer elements, the procedure of  31  and  32  appears to be unattractive since it requires rational arithmetic. However, we can use the method to evaluate the determinant mod p, for any prime p, since division mod p is possible  exercise 4.5.2–16 . If this is done for sufficiently many primes, the exact value of the determinant can be found as explained in Section 4.3.2, since Hadamard’s inequality 4.6.1– 25  gives an upper bound on the magnitude. The coefficients of the characteristic polynomial det xI − X  of an n× n ma- trix X can also be computed in O n3  steps; see J. H. Wilkinson, The Algebraic Eigenvalue Problem  Oxford: Clarendon Press, 1965 , 353–355, 410–411. Exer- cise 70 discusses an interesting division-free method that involves O n4  steps. The permanent of a matrix is a polynomial that is very similar to the determinant; the only difference is that all of its nonzero coefficients are +1. Thus we have  x11  ...  per  . . . x1n  ...  xn1  . . . xnn   =  x1j1 x2j2 . . . xnjn ,   33   summed over all permutations j1 j2 . . . jn of {1, 2, . . . , n}. It would seem that this function should be even easier to compute than its more complicated-looking cousin, but no way to evaluate the permanent as efficiently as the determinant is known. Exercises 9 and 10 show that substantially fewer than n! operations will suffice, for large n, but the execution time of all known methods still grows exponentially with the size of the matrix. In fact, Leslie G. Valiant has shown that it is as difficult to compute the permanent of a given 0–1 matrix as it is to count the number of accepting computations of a nondeterministic polynomial- time Turing machine, if we ignore polynomial factors in the running time of the calculation. Therefore a polynomial-time evaluation algorithm for permanents would imply that scores of other well known problems that have resisted efficient solution would be solvable in polynomial time. On the other hand, Valiant proved that the permanent of an n × n integer matrix can be evaluated modulo 2k in O n4k−3  steps for all k ≥ 2. [See Theoretical Comp. Sci. 8  1979 , 189–201.] Another fundamental operation involving matrices is, of course, matrix mul- tiplication: If X =  xij  is an m × n matrix, Y =  yjk  is an n × s matrix, and Z =  zik  is an m × s matrix, then the formula Z = XY means that  zik =  xij yjk,  1 ≤ i ≤ m,  1 ≤ k ≤ s.   34   n  j=1  This equation may be regarded as the computation of ms simultaneous polynomi- als in mn + ns variables; each polynomial is the “inner product” of two n-place   4.6.4  bk =   1≤j≤n 2  500  ARITHMETIC  zik =   1≤j≤n 2  ai =   1≤j≤n 2  vectors. A straightforward calculation would involve mns multiplications and ms n − 1  additions; but S. Winograd discovered in 1967 that there is a way to trade about half of the multiplications for additions:   xi,2j + y 2j−1,k  xi,2j−1 + y 2j,k  − ai − bk + xinynk[n odd];  xi,2j xi,2j−1;  y 2j−1,k y 2j,k.   35      a   A  This scheme uses ⌈n 2⌉ms + ⌊n 2⌋ m + s  multiplications and  n + 2 ms +  ⌊n 2⌋ − 1  ms + m + s  additions or subtractions; the total number of oper- ations has increased slightly, but the number of multiplications has roughly [See IEEE Trans. C-17  1968 , 693–694.] Winograd’s surprising been halved. construction led many people to look more closely at the problem of matrix mul- tiplication, and it touched off widespread speculation that n3 2 multiplications might be necessary to multiply n × n matrices, because of the somewhat similar lower bound that was known to hold for polynomials in one variable. An even better scheme for large n was discovered by Volker Strassen in 1968; he found a way to compute the product of 2 × 2 matrices with only seven multiplications, without relying on the commutativity of multiplication as in  35 . Since 2n × 2n matrices can be partitioned into four n × n matrices, his idea can be used recursively to obtain the product of 2k × 2k matrices with only 7k multiplications instead of  2k 3 = 8k. The number of additions also grows as order 7k. Strassen’s original 2 × 2 identity [Numer. Math. 13  1969 , 354–356] used 7 multiplications and 18 additions; S. Winograd later discovered the following more economical formula: aA+bB   36  where u =  c− a  C − D , v =  c + d  C − A , w = aA +  c + d− a  A + D − C . If intermediate results are appropriately saved, this involves 7 multiplications and only 15 additions; by induction on k, we can multiply 2k × 2k matrices with 7k multiplications and 5 7k − 4k  additions. The total number of operations needed to multiply n × n matrices has therefore been reduced from order n3 to O nlg 7  = O n2.8074 . A similar reduction applies also to the evaluation of determinants and matrix inverses; see J. R. Bunch and J. E. Hopcroft, Math. Comp. 28  1974 , 231–236. Strassen’s exponent lg 7 resisted numerous attempts at improvement until 1978, when Viktor Pan discovered that it could be lowered to log70 143640 ≈ 2.795  see exercise 60 . This new breakthrough led to further intensive analysis of the problem, and the combined efforts of D. Bini, M. Capovani, D. Coppersmith, G. Lotti, F. Romani, A. Schönhage, V. Pan, and S. Winograd, produced a dramatic reduction in the asymptotic running time. Exercises 60–67 discuss some of the interesting techniques by which such upper bounds have been estab- lished; in particular, exercise 66 contains a reasonably simple proof that O n2.55   w+u+d B+C−A−D   w+v+ a+b−c−d D  w+u+v      C D  b d  =  B  c  ,   4.6.4  EVALUATION OF POLYNOMIALS  501  operations suffice. The best upper bound known as of 1997 is O n2.376 , due to Coppersmith and Winograd [J. Symbolic Comp. 9  1990 , 251–280]. By contrast, the best current lower bound is 2n2 − 1  see exercise 12 .  These theoretical results are quite striking, but from a practical standpoint they are of little use because n must be very large before we overcome the effect of additional bookkeeping costs. Richard Brent [Stanford Computer Science report CS157  March 1970 , see also Numer. Math. 16  1970 , 145–156] found that a careful implementation of Winograd’s scheme  35 , with appropriate scaling for numerical stability, became better than the conventional method only when n ≥ 40, and it saved only about 7 percent of the running time when n = 100. For complex arithmetic the situation was somewhat different; scheme  35  became advantageous for n > 20, and saved 18 percent when n = 100. He estimated that Strassen’s scheme  36  would not begin to excel over  35  until n ≈ 250; and such enormous matrices rarely occur in practice unless they are very sparse, when other techniques apply. Furthermore, the known methods of order nω where ω < 2.7 have such large constants of proportionality that they require more than 1023 multiplications before they start to beat  36 .  By contrast, the methods we shall discuss next are eminently practical and have found wide use. The discrete Fourier transform f of a complex-valued function F of n variables, over respective domains of m1, . . . , mn elements, is defined by the equation  f s1, . . . , sn  =      s1t1  m1  exp  2πi  + ··· + sntn mn  F t1, . . . , tn    37     0≤t1<m1 0≤tn<mn  · · ·  for 0 ≤ s1 < m1, . . . , 0 ≤ sn < mn; the name “transform” is justified because we can recover the values F t1, . . . , tn  from the values f s1, . . . , sn , as shown in exercise 13. In the important special case that all mj = 2, we have  −1 s1t1+···+sntnF t1, . . . , tn   f s1, . . . , sn  =    38   0≤t1,...,tn≤1  for 0 ≤ s1, . . . , sn ≤ 1, and this may be regarded as a simultaneous evaluation of 2n linear polynomials in 2n variables F t1, . . . , tn . A well-known technique due to F. Yates [The Design and Analysis of Factorial Experiments  Harpenden: Imperial Bureau of Soil Sciences, 1937 ] can be used to reduce the number of additions implied in  38  from 2n 2n − 1  to n2n. Yates’s method can be understood by considering the case n = 3: Let Xt1t2t3 = F t1, t2, t3 . Given First step X000 X000+X001 X000+X001+X010+X011 X000+X001+X010+X011+X100+X101+X110+X111 X001 X010+X011 X100+X101+X110+X111 X000−X001+X010−X011+X100−X101+X110−X111 X010 X100+X101 X000−X001+X010−X011 X000+X001−X010−X011+X100+X101−X110−X111 X011 X110+X111 X100−X101+X110−X111 X000−X001−X010+X011+X100−X101−X110+X111 X100 X000−X001 X000+X001−X010−X011 X000+X001+X010+X011−X100−X101−X110−X111 X101 X010−X011 X100+X101−X110−X111 X000−X001+X010−X011−X100+X101−X110+X111 X110 X100−X101 X000−X001−X010+X011 X000+X001−X010−X011−X100−X101+X110+X111 X111 X110−X111 X100−X101−X110+X111 X000−X001−X010+X011−X100+X101+X110−X111  Second step  Third step   502  ARITHMETIC  4.6.4  To get from the “Given” to the “First step” requires four additions and four subtractions; and the interesting feature of Yates’s method is that exactly the same transformation that takes us from “Given” to “First step” will take us from “First step” to “Second step” and from “Second step” to “Third step.” In each case we do four additions, then four subtractions; and after three steps we magically have the desired Fourier transform f s1, s2, s3  in the place originally occupied by F s1, s2, s3 .  This special case is often called the Hadamard transform or the Walsh transform of 2n data elements, since the corresponding pattern of signs was studied by J. Hadamard [Bull. Sci. Math.  2  17  1893 , 240–246] and by J. L. Walsh [Amer. J. Math. 45  1923 , 5–24]. Notice that the number of sign changes from left to right in the “Third step” assumes the respective values  0, 7, 3, 4, 1, 6, 2, 5;  this is a permutation of the numbers {0, 1, 2, 3, 4, 5, 6, 7}. Walsh observed that there will be exactly 0, 1, if we permute the transformed elements appropriately, so the coefficients provide discrete approximations to sine waves with various frequencies.  See Section 7.2.1.1 for further discussion of the Hadamard–Walsh coefficients.   . . . , 2n − 1 sign changes in the general case,  Yates’s method can be generalized to the evaluation of any discrete Fourier transform, and, in fact, to the evaluation of any set of sums that can be written in the general form f s1, s2, . . . , sn  =  g1 s1, s2, . . . , sn, t1 g2 s2, . . . , sn, t2  . . . gn sn, tn F t1, t2, . . . , tn    39   · · ·  0≤t1<m1 0≤tn<mn for 0 ≤ sj < mj, given the functions gj sj, . . . , sn, tj . We proceed as follows.    f0 t1, t2, t3, . . . , tn  = F t1, t2, t3, . . . , tn ;  f1 sn, t1, t2, . . . , tn−1  =  f2 sn−1, sn, t1, . . . , tn−2  =  fn s1, s2, s3, . . . , sn  =   ...  0≤tn<mn  0≤tn−1<mn−1  0≤t1<m1  gn sn, tn f0 t1, t2, . . . , tn ;  gn−1 sn−1, sn, tn−1 f1 sn, t1, . . . , tn−1 ;  g1 s1, . . . , sn, t1 fn−1 s2, s3, . . . , sn, t1 ;  f s1, s2, s3, . . . , sn  = fn s1, s2, s3, . . . , sn .   40  For Yates’s method as shown above, gj sj, . . . , sn, tj  =  −1 sj tj; f0 t1, t2, t3  represents the “Given”; f1 s3, t1, t2  represents the “First step”; and so on. Whenever a desired set of sums can be put into the form of  39 , for reasonably   4.6.4  EVALUATION OF POLYNOMIALS  503  simple functions gj sj, . . . , sn, tj , the scheme  40  will reduce the amount of com- putation from order N 2 to order N log N or thereabouts, where N = m1 . . . mn is the number of data points. Furthermore this scheme is ideally suited to parallel computation. The important special case of one-dimensional Fourier transforms is discussed in exercises 14 and 53; we have considered the one-dimensional case also in Section 4.3.3C.  Let us consider one more special case of polynomial evaluation. Lagrange’s  interpolation polynomial of order n, which we shall write as  u[n] x  = y0   x− x1  x− x2  . . .  x− xn   x0− x1  x0− x2  . . .  x0− xn  + y1   x− x0  x− x2  . . .  x− xn   x1− x0  x1− x2  . . .  x1− xn   + ··· + yn   x− x0  x− x1  . . .  x− xn−1   xn− x0  xn− x1  . . .  xn− xn−1  ,  y0, y1, . . . , yn at the n + 1 distinct points x = x0, x1, . . . , xn. For it is evident than n, so g x  = 0. If we assume that the values of a function in some table   41  is the only polynomial of degree ≤ n in x that takes on the respective values from  41  that u[n] xk  = yk for 0 ≤ k ≤ n. If f x  is any such polynomial of degree ≤ n, then g x  = f x  − u[n] x  is of degree ≤ n, and g x  is zero for x = x0, x1, . . . , xn; therefore g x  must be a multiple of the polynomial  x − x0  x − x1  . . .  x − xn . The degree of the latter polynomial is greater are well approximated by a polynomial, formula  41  may therefore be used to “interpolate” for values of the function at points x not appearing in the table. Lagrange presented  41  to his class at the Paris École Normale in 1795 [see his Œuvres 7  Paris: 1877 , 286]; but Edward Waring of Cambridge University actually deserves the credit, because he had already presented the same formula quite clearly and explicitly in Philosophical Transactions 69  1779 , 59–67.  There seem to be quite a few additions, subtractions, multiplications, and divisions in Waring and Lagrange’s formula; in fact, there are exactly n additions, 2n2 + 2n subtractions, 2n2 + n − 1 multiplications, and n + 1 divisions. But fortunately  as we might be conditioned to suspect by now , improvement is possible.  The basic idea for simplifying  41  is to exploit the fact that for x = x0, . . . , xn−1;  u[n] x  − u[n−1] x  = 0  thus u[n] x  − u[n−1] x  is a polynomial of degree n or less, and a multiple of  x − x0  . . .  x − xn−1 . We conclude that u[n] x  = αn x − x0  . . .  x − xn−1  + u[n−1] x , where αn is a constant. This leads us to Newton’s interpolation formula  u[n] x  = αn x − x0  x − x1  . . .  x − xn−1  + ···  + α2 x − x0  x − x1  + α1 x − x0  + α0,   42  where the α’s are some coefficients that we want to determine from the given numbers x0, x1, . . . , xn, y0, y1, . . . , yn. Notice that this formula holds for all n; the coefficient αk does not depend on xk+1, . . . , xn, or on yk+1, . . . , yn. Once   504  ARITHMETIC  4.6.4  .  the α’s are known, Newton’s interpolation formula is convenient for calculation, since we may generalize Horner’s rule once again and write  u[n] x  = . . .  αn x−xn−1  + αn−1  x−xn−2  + ···   x−x0  + α0   43  This requires n multiplications and 2n additions. Alternatively, we may evaluate each of the individual terms of  42  from right to left; with 2n−1 multiplications and 2n additions we thereby calculate all of the values u[0] x , u[1] x , . . . , u[n] x , and this indicates whether or not an interpolation process is converging.  The coefficients αk in Newton’s formula may be found by computing the   y1− y0   x1− x0  = y′ 1  y2− y1   x2− x1  = y′ 2  y3− y2   x3− x2  = y′ 3  divided differences in the following tableau  shown for n = 3 : y0 y1 2    x3− x0  = y′′′ 3 y2  44  y3 It is possible to prove that α0 = y0, α1 = y′ 2 , etc., and to show that the divided differences have important relations to the derivatives of the function being interpolated; see exercise 15. Therefore the following calculation  corresponding to  44  may be used to obtain the α’s:  1   x2− x0  = y′′ 2 2   x3− x1  = y′′ 3  2− y′  y′ 3− y′  y′  1, α2 = y′′  3 − y′′  y′′  Start with  α0, α1, . . . , αn  ←  y0, y1, . . . , yn ; then, for k = 1, 2, . . . , n  in this order ,  set αj ←  αj − αj−1   xj − xj−k  for j = n, n − 1, . . . , k  in this order .  This process requires 1 three-fourths of the work implied in  41  has been saved.  2 n2 + n  divisions and n2 + n subtractions, so about For example, suppose that we want to estimate 1.5! from the values of  0!, 1!, 2!, and 3!, using a cubic polynomial. The divided differences are  y′  y′′  y′′′  1 3  1 2 3 2  0 1 4  x 0 1 2 3  y 1 1 2 6 2 x x − 1  + 1, u[3] x  = 1 3 x x − 1  x − 2  + so u[0] x  = u[1] x  = 1, u[2] x  = 1 2 x x−1 +1. Setting x = 1.5 in u[3] x  gives −.125+.375+1 = 1.25; presumably 1 √ π ≈ 1.33.  But there are of course many other the “correct” value is Γ 2.5  = 3 4 sequences that begin with the numbers 1, 1, 2, and 6.   If we want to interpolate several polynomials that have the same interpola- tion points x0, x1, . . . , xn but varying values y0, y1, . . . , yn, it is desirable to rewrite  41  in a form suggested by W. J. Taylor [J. Research Nat. Bur. Standards 35  1945 , 151–155]:  u[n] x  =  + ··· + ynwn x − xn  x − x0  + ··· + wn x − xn   45    y0w0  x − x0   w0    ,   4.6.4 when x  ∈ {x0, x1, . . . , xn}, where  EVALUATION OF POLYNOMIALS  505  wk = 1  xk − x0  . . .  xk − xk−1  xk − xk+1  . . .  xk − xn .   46  This form is also recommended for its numerical stability [see P. Henrici, Essen- tials of Numerical Analysis  New York: Wiley, 1982 , 237–243]. The denominator of  45  is the partial fraction expansion of 1  x − x0  x − x1  . . .  x − xn .  An important and somewhat surprising application of polynomial interpo- lation was discovered by Adi Shamir [CACM 22  1979 , 612–613], who observed that polynomials mod p can be used to “share a secret.” This means that we can design a system of secret keys or passwords such that the knowledge of any n+1 of the keys enables efficient calculation of a magic number N that unlocks a door  say , but the knowledge of any n of the keys gives no information whatsoever about N. Shamir’s amazingly simple solution to this problem is to choose a random polynomial u x  = unxn+···+u1x+u0, where 0 ≤ ui < p and p is a large prime number. Each part of the secret is an integer x in the range 0 < x < p, together with the value of u x  mod p; and the supersecret number N is the constant term u0. Given n + 1 values u xi , we can deduce N by interpolation. But if only n values of u xi  are given, there is a unique polynomial u x  having a given constant term but the same values at x1, . . . , xn; thus the n values do not make one particular N more likely than any other.  It is instructive to note that evaluation of the interpolation polynomial is just a special case of the Chinese remainder algorithm of Section 4.3.2 and exercise 4.6.2–3, since we know the values of u[n] x  modulo the relatively prime  polynomials x − x0, . . . , x − xn. As we have seen in Section 4.6.2 and in the discussion following  3 , f x  mod  x− x0  = f x0 . Under this interpretation,  Newton’s formula  42  is precisely the “mixed-radix representation” of Eq. 4.3.2–  25 ; and 4.3.2– 24  yields another way to compute α0, . . . , αn using the same number of operations as  44 .  for interpolation to On  log n 2, and a similar reduction can also be made for  By applying fast Fourier transforms, it is possible to reduce the running time  related algorithms such as the solution to the Chinese remainder problem and the evaluation of an nth degree polynomial at n different points. [See E. Horowitz, Inf. Proc. Letters 1  1972 , 157–163; A. Borodin and R. Moenck, J. Comp. Syst. Sci. 8  1974 , 336–385; A. Borodin, Complexity of Sequential and Parallel Numerical Algorithms, edited by J. F. Traub  New York: Academic Press, 1973 , 149–180; D. Bini and V. Pan, Polynomial and Matrix Computations 1  Boston: Birkhäuser, 1994 , Chapter 1.] However, these observations are primarily of theoretical interest, since the known algorithms have a rather large overhead factor that makes them unattractive unless n is quite large.  A remarkable extension of the method of divided differences, which applies to quotients of polynomials as well as to polynomials, was introduced by T. N. Thiele in 1909. Thiele’s method of “reciprocal differences” is discussed in L. M. Milne-Thompson’s Calculus of Finite Differences  London: MacMillan, 1933 , Chapter 5; see also R. W. Floyd, CACM 3  1960 , 508.   506  ARITHMETIC  4.6.4   47   *Bilinear forms. Several of the problems we have considered in this section are special cases of the general problem of evaluating a set of bilinear forms  m  n  i=1  j=1  zk =  tijkxiyj,  for 1 ≤ k ≤ s,  where the tijk are specific coefficients belonging to some given field. The three- dimensional array  tijk  is called an m × n × s tensor, and we can display it by writing down s matrices of size m × n, one for each value of k. For example, the problem of multiplying complex numbers, namely the problem of evaluating  z1 + iz2 =  x1 + ix2  y1 + iy2  =  x1y1−x2y2  + i x1y2+x2y1 ,   48  is the problem of computing the bilinear form specified by the 2 × 2 × 2 tensor   1  0 0 −1   0 1    1 0  .  Matrix multiplication as defined in  34  is the problem of evaluating a set of bilinear forms corresponding to a particular mn × ns × ms tensor. Fourier transforms  37  can also be cast in this mold, although they are linear instead of bilinear, if we let the x’s be constant rather than variable.  The evaluation of bilinear forms is most easily studied if we restrict our- selves to what might be called normal evaluation schemes, in which all chain multiplications take place between a linear combination of the x’s and a linear combination of the y’s. Thus, we form r products  wl =  a1l x1 + ··· + aml xm  b1l y1 + ··· + bnl yn ,  for 1 ≤ l ≤ r,   49   and obtain the z’s as linear combinations of these products,  zk = ck1w1 + ··· + ckr wr,   50  Here all the a’s, b’s, and c’s belong to a given field of coefficients. By comparing  50  to  47 , we see that a normal evaluation scheme is correct for the tensor  tijk  if and only if  for 1 ≤ k ≤ s.  tijk = ai1bj1ck1 + ··· + air bjr ckr   51   for 1 ≤ i ≤ m, 1 ≤ j ≤ n, and 1 ≤ k ≤ s.  A nonzero tensor  tijk  is said to be of rank one if there are three vectors  a1, . . . , am ,  b1, . . . , bn ,  c1, . . . , cs  such that tijk = ai bj ck for all i, j, k. We can extend this definition to all tensors by saying that the rank of  tijk  is the minimum number r such that  tijk  is expressible as the sum of r rank-one tensors in the given field. Comparing this definition with Eq.  51  shows that the rank of a tensor is the minimum number of chain multiplications in a normal evaluation of the corresponding bilinear forms. Incidentally, when s = 1 the tensor  tijk  is just an ordinary matrix, and the rank of  tij1  as a tensor is the same as its rank as a matrix  see exercise 49 . The concept of tensor rank was introduced by F. L. Hitchcock in J. Math. and Physics 6  1927 , 164–189; its   4.6.4  EVALUATION OF POLYNOMIALS  507  1 0 0 0  0 1 0 0 0 0 0 0 0 0 0 0  application to the complexity of polynomial evaluation was pointed out in an important paper by V. Strassen, Crelle 264  1973 , 184–202.  Winograd’s scheme  35  for matrix multiplication is “abnormal” because it mixes x’s and y’s before multiplying them. The Strassen–Winograd scheme  36 , on the other hand, does not rely on the commutativity of multiplication, so it is normal. In fact,  36  corresponds to the following way to represent the 4× 4× 4 tensor for 2 × 2 matrix multiplication as a sum of seven rank-one tensors:  =  0 0 0 0 1 0 0 0 0 1 0 0  0 0 0 0 0 0 1 0 0 0 0 1  0 0 0 1 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1   0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 1 1 1 1  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  0 1 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 1 0 0 0 1 0 0 0 1  0 0 0 0 0 0 0 0 0 0 0 0  +  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  1 0 0 0      1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0  0 0 1 1 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 . 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0  0 0 0 0 1 0 1 0 1 0 1 0  0 0 0 0 1 0 1 0 1 0 1 0  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 1 1 0 0 0 0  0 0 0 0 0 0 1 1 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0  +  +  0 0 0 0 1 0 1 1 1 0 1 1  0 0 0 0 1 0 1 1 1 0 1 1  0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 1 0 1 1 1 0 1 1  +  +  +   52    Here 1 stands for −1.   The fact that  51  is symmetric in i, j, k and invariant under a variety of transformations makes the study of tensor rank mathematically tractable, and it also leads to some surprising consequences about bilinear forms. We can permute the indices i, j, k to obtain “transposed” bilinear forms, and the transposed tensor clearly has the same rank; but the corresponding bilinear forms are conceptually quite different. For example, a normal scheme for evaluating an  m× n  times  n× s  matrix product implies the existence of a normal scheme to evaluate an  n×s  times  s×m  matrix product, using the same number of chain multiplications. In matrix terms these two problems hardly seem to be related at all — they involve different numbers of dot products, on vectors of different sizes — but in tensor terms they are equivalent. [See V. Y. Pan, Uspekhi Mat. Nauk 27, 5  September–October 1972 , 249–250; J. E. Hopcroft and J. Musinski, SICOMP 2  1973 , 159–173.] When the tensor  tijk  can be represented as a sum  51  of r rank-one tensors, let A, B, C be the matrices  ail ,  bjl ,  ckl  of respective sizes m × r, n × r, s × r; we shall say that  A, B, C  is a realization of the tensor  tijk . For example, the realization of 2 × 2 matrix multiplication in  52  can be specified by the matrices  1 0 1 0 0 1 1  0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1  , B =  1 0 0 1 1 0 1  0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1  , C =  1 1 0 0 0 0 0  1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1  .  A =   53    508  ARITHMETIC  4.6.4 An m × n × s tensor  tijk  can also be represented as a matrix by grouping its subscripts together. We shall write  t ij k  for the mn× s matrix whose rows are indexed by the pair of subscripts ⟨i, j⟩ and whose columns are indexed by k. Similarly,  tk ij   stands for the s × mn matrix that contains tijk in row k and column ⟨i, j⟩;  t ik j  is an ms × n matrix, and so on. The indices of an array need not be integers, and we are using ordered pairs as indices here. We can use this notation to derive the following simple but useful lower bound on the rank of a tensor. Lemma T. Let  A, B, C  be a realization of an m × n × s tensor  tijk . Then rank A  ≥ rank ti jk  , rank B  ≥ rank tj ik  , and rank C  ≥ rank tk ij  ; consequently  rank tijk  ≥ maxrank ti jk  , rank tj ik  , rank tk ij  .  It suffices by symmetry to show that r ≥ rank A  ≥ rank ti jk  . Since Proof. A is an m × r matrix, it is obvious that A cannot have rank greater than r. Furthermore, according to  51 , the matrix  ti jk   is equal to AQ, where Q is the r × ns matrix defined by Ql⟨j,k⟩ = bjl ckl. If x is any row vector such that xA = 0 then xAQ = 0, hence all linear dependencies in A occur also in AQ. It follows that rank AQ  ≤ rank A .  As an example of the use of Lemma T, let us consider the problem of polynomial multiplication. Suppose we want to multiply a general polynomial of degree 2 by a general polynomial of degree 3, obtaining the coefficients of the product:   x0 + x1u + x2u2  y0 + y1u + y2u2 + y3u3    54  This is the problem of evaluating six bilinear forms corresponding to the 3×4×6  = z0 + z1u + z2u2 + z3u3 + z4u4 + z5u5.  tensor1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0  0 0 0 0    0 0 0 0 0 0 0 0  1 0 0 0 0 0 0 0  0 1 0 0 1 0 0 0  0 0 1 0 0 1 0 0  0 0 0 1 0 0 1 0  0 0 0 0 0 0 0 1  .   55   For brevity, we may write  54  as x u y u  = z u , letting x u  denote the polynomial x0 + x1u + x2u2, etc.  We have come full circle from the way we began this section, since Eq.  1  refers to u x , not x u ; the notation has changed because the coefficients of the polynomials are now the variables of interest to us.  If each of the six matrices in  55  is regarded as a vector of length 12 indexed by ⟨i, j⟩, it is clear that the vectors are linearly independent, since they are nonzero in different positions; hence the rank of  55  is at least 6 by Lemma T. Conversely, it is possible to obtain the coefficients z0, z1, . . . , z5 by making only six chain multiplications, for example by computing   56  this gives the values of z 0 , z 1 , . . . , z 5 , and the formulas developed above for interpolation will yield the coefficients of z u . The evaluation of x j   x 0 y 0 , x 1 y 1 , . . . , x 5 y 5 ;   4.6.4  EVALUATION OF POLYNOMIALS  509  and y j  can be carried out entirely in terms of additions and or parameter multiplications, and the interpolation formula merely takes linear combinations of these values. Thus, all of the chain multiplications are shown in  56 , and the rank of  55  is 6.  We used essentially this same technique when multiplying high-precision numbers in Algorithm 4.3.3T.   The realization  A, B, C  of  55  sketched in the paragraph above turns out  to be1 1 1 1 1 1    0 1 2 3 4 5 0 1 4 9 16 25  ,  1 1 1 1 1  1 0 1 2 3 4 5 0 1 4 9 16 25 0 1 8 27 64 125    ,   120  0 0 0 400 −150 −274 24 225 −770 1070 −780 305 −50 −85 490 −205 35 15 −70 55 −10 130 −120 10 −5 −1 1  0 0 600 −600 355 −590 5 −10  × 1  120 .  57   Thus, the scheme does indeed achieve the minimum number of chain multipli- cations, but it is completely impractical because it involves so many additions and parameter multiplications. We shall now study a practical approach to the generation of more efficient schemes, introduced by S. Winograd.  In the first place, to evaluate the coefficients of x u y u  when deg x  = m  and deg y  = n, we can use the identity  x u y u  =x u y u  mod p u  + xmyn p u ,   58  when p u  is any monic polynomial of degree m+ n. The polynomial p u  should be chosen so that the coefficients of x u y u  mod p u  are easy to evaluate.  the polynomial p u  can be factored into q u r u  where gcdq u , r u  = 1, we x u y u  mod q u r u  =a u r u  x u y u  mod q u    In the second place, to evaluate the coefficients of x u y u  mod p u , when  + b u q u  x u y u  mod r u   mod q u r u    59  where a u r u + b u q u  = 1; this is essentially the Chinese remainder theorem applied to polynomials.  can use the identity  x u y u  mod p u  by using the trivial identity  In the third place, we can always evaluate the coefficients of the polynomial  x u y u  mod p u  =x u  mod p u y u  mod p u  mod p u .   60  Repeated application of  58 ,  59 , and  60  tends to produce efficient schemes, as we shall see. For our example problem  54 , let us choose p u  = u5−u and apply  58 ; the reason for this choice of p u  will appear as we proceed. Writing p u  = u u4−1 , rule  59  reduces to  x u y u  mod u u4 − 1  =− u4 − 1 x0y0 + u4 x u y u  mod  u4 − 1     61  Here we have used the fact that x u y u  mod u = x0y0; in general it is a good idea to choose p u  in such a way that p 0  = 0, so that this simplification can be  mod  u5 − u .   510  ARITHMETIC  4.6.4  used. If we could now determine the coefficients w0, w1, w2, w3 of the polynomial x u y u  mod  u4 − 1  = w0 + w1u + w2u2 + w3u3, our problem would be solved, since  u4x u y u  mod  u4 − 1  mod  u5 − u  = w0u4 + w1u + w2u2 + w3u3,  and the combination of  58  and  61  would reduce to x u y u  = x0y0 +  w1 − x2y3 u + w2u2 + w3u3 +  w0 − x0y0 u4 + x2y3u5.  62   This formula can, of course, be verified directly.   The problem remaining to be solved is to compute x u y u  mod  u4 − 1 ; and this subproblem is interesting in itself. Let us momentarily allow x u  to be of degree 3 instead of degree 2. Then the coefficients of x u y u  mod  u4 − 1  are respectively  x0y0 + x1y3 + x2y2 + x3y1,  x0y1 + x1y0 + x2y3 + x3y2,  and the corresponding tensor is  x0y3 + x1y2 + x2y1 + x3y0,  x0y2 + x1y1 + x2y0 + x3y3,  1 0 0 0  0 0 0 1 0 0 1 0 0 1 0 0  0 1 0 0  1 0 0 0 0 0 0 1 0 0 1 0  0 0 1 0  0 1 0 0 1 0 0 0 0 0 0 1  0 0 0 1  0 0 1 0 0 1 0 0 1 0 0 0   .   63   kth coefficient wk is the bilinear form  xi yj summed over all i and j with  In general when deg x  = deg y  = n−1, the coefficients of x u y u  mod  un−1  are called the cyclic convolution of  x0, x1, . . . , xn−1  and  y0, y1, . . . , yn−1 . The i + j ≡ k  modulo n . The cyclic convolution of degree 4 can be obtained by applying rule  59 . The first step is to find the factors of u4 − 1, namely  u − 1  u + 1  u2 + 1 . We could write this as  u2 − 1  u2 + 1 , then apply rule  59 , then use  59  again on the part modulo  u2−1  =  u−1  u+1 ; but it is easier to generalize the Chinese remainder rule  59  directly to the case of several relatively prime factors. For example, we have x u y u  mod q1 u q2 u q3 u   + a3 u q1 u q2 u x u y u  mod q3 u  mod q1 u q2 u q3 u ,  =a1 u q2 u q3 u x u y u  mod q1 u +a2 u q1 u q3 u x u y u  mod q2 u  where a1 u q2 u q3 u  + a2 u q1 u q3 u  + a3 u q1 u q2 u  = 1. This equation sion of 1 q1 u q2 u q3 u  is a1 u  q1 u + a2 u  q2 u + a3 u  q3 u . From  64  x u y u  mod  u4−1  = 1  4 u3−u2+u−1 x −1 y −1   65  The remaining problem is to evaluate x u y u  mod  u2 + 1 , and it is time to invoke rule  60 . First we reduce x u  and y u  mod  u2 + 1 , obtaining  2 u2−1 x u y u  mod  u2+1  mod  u4−1 .  can also be understood in another way, by noting that the partial fraction expan-  4 u3+u2+u+1 x 1 y 1 − 1 − 1  we obtain   64    EVALUATION OF POLYNOMIALS  4.6.4 511 X u  =  x0 − x2  +  x1 − x3 u, Y  u  =  y0 − y2  +  y1 − y3 u. Then  60  tells us to evaluate X u Y  u  = Z0 + Z1u + Z2u2, and to reduce this in turn modulo  u2 + 1 , obtaining  Z0 − Z2  + Z1u. The job of computing X u Y  u  is simple; we can use rule  58  with p u  = u u + 1  and we get  Z0 = X0Y0, Z1 = X0Y0 −  X0−X1  Y0−Y1  + X1Y1, Z2 = X1Y1.   We have thereby rediscovered the trick of Eq. 4.3.3– 2  in a more systematic way.  Putting everything together yields the following realization  A, B, C  of degree-4 cyclic convolution:  1 1 1 0 1   ,  1 1 1 0 1   ,  1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 Here 1 stands for −1 and 2 stands for −2.  1 1 0 1 1 1 1 1 0 1 1 1 0 1 1  1 1 2 2 0  1 1 2 2 2 1 1 2 2 0 1 1 2 2 2   × 1  4 .   66   The tensor for cyclic convolution of degree n satisfies   67  treating the subscripts modulo n, since tijk = 1 if and only if i + j ≡ k  modulo n . Thus if  ail ,  bjl ,  ckl  is a realization of the cyclic convolution, so is  ckl ,  b−j,l ,  ail ; in particular, we can realize  63  by transforming  66  into  ti,j,k = tk,−j,i,  1 1 2 2 0  1 1 2 2 2 1 1 2 2 0 1 1 2 2 2   × 1  4 ,  1 1 1 0 1  1 1 0 1 1 1 1 1 0 1 1 1 0 1 1   ,  1 1 1 0 1  1 1 0 1 1 1 1 1 0 1 1 1 0 1 1   .   68   Now all of the complicated scalars appear in the A matrix. This is important in practice, since we often want to compute the convolution for many values of y0, y1, y2, y3 but for a fixed choice of x0, x1, x2, x3. In such a situation, the arithmetic on x’s can be done once and for all, and we need not count it. Thus  68  leads to the following scheme for evaluating the cyclic convolution w0, w1, w2, w3 when x0, x1, x2, x3 are known in advance:  s1 = y0 + y2,  s2 = y1 + y3,  s3 = s1 + s2,  s4 = s1 − s2,  m3= 1  s5 = y0 − y2,  s6 = y3 − y1, 4 x0 + x1 + x2 + x3  · s3, m2 = 1  s7 = s5 − s6; 4 x0 − x1 + x2 − x3  · s4, t4 = m4 − m5;  m1 = 1 2 x0+x1−x2−x3 ·s5, m4= 1 t1 = m1 + m2,  2 −x0+x1+x2−x3 ·s6, m5= 1 w0 = t1 + t2, w1 = t3 + t4, w2 = t1 − t2, w3 = t3 − t4.  t3 = m1 − m2,   69  There are 5 multiplications and 15 additions, while the definition of cyclic convolution involves 16 multiplications and 12 additions. We will prove later that 5 multiplications are necessary.  2 x3−x1 ·s7;  t2 = m3 + m5,   Going back to our original multiplication problem  54 , using  62 , we have  512  ARITHMETIC  derived the realization  4 0 1 1 2 2 0    0 0 1 1 2 2 2 0 4 1 1 2 2 0  × 1 4 ,  1 0 1 1 1 0 1  0 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1   ,  4.6.4   70     1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0   .  This scheme uses one more than the minimum number of chain multiplications, but it requires far fewer parameter multiplications than  57 . Of course, it must be admitted that the scheme is still rather complicated: If our goal is simply to compute the coefficients z0, z1, . . . , z5 of the product of two given polynomials  x0 + x1u + x2u2  y0 + y1u + y2u2 + y3u3 , as a one-shot problem, our best bet may well be to use the obvious method that does 12 multiplications and 6 additions — unless  say  the x’s and y’s are matrices. Another reasonably attractive scheme, which requires 8 multiplications and 18 additions, appears in exercise 58 b . Notice that if the x’s are fixed as the y’s vary,  70  does the evaluation with 7 multiplications and 17 additions. Even though this scheme isn’t especially useful as it stands, our derivation has illustrated important techniques that are useful in a variety of other situations. For example, Winograd has used this approach to compute Fourier transforms using significantly fewer multiplications than the fast Fourier transform algorithm needs  see exercise 53 . Let us conclude this section by determining the exact rank of the n × n × n tensor that corresponds to the multiplication of two polynomials modulo a third, z0 + z1u + ··· + zn−1un−1  =  x0 + x1u + ··· + xn−1un−1  y0 + y1u + ··· + yn−1un−1  mod p u .  71  Here p u  stands for any given monic polynomial of degree n; in particular, p u  might be un − 1, so one of the results of our investigation will be to deduce the rank of the tensor corresponding to cyclic convolution of degree n. It will be convenient to write p u  in the form  The tensor element tijk is the coefficient of uk in ui+j mod p u ; and this is  p u  = un − pn−1un−1 − ··· − p1u − p0,  the element in row i, column k of the matrix P j, where  so that un ≡ p0 + p1u + ··· + pn−1un−1 modulo p u .     . . . . . .  P =  0 0 ... 1  0 0 ... 0 p0  1 0 ... 0 p1  0 1 ... 0 p2  . . . . . . pn−1   72    73   is called the companion matrix of p u .  The indices i, j, k in our discussion will run from 0 to n − 1 instead of from 1 to n.  It is convenient to transpose the   n−1  k=0  4.6.4 513 tensor, for if Tijk = tikj the individual layers of  Tijk  for k = 0, 1, 2, . . . , n − 1 are simply given by the matrices  EVALUATION OF POLYNOMIALS  P 2  I  P  . . .  combinationn−1   74  The first rows of the matrices in  74  are respectively the unit vectors  1, 0, 0, . . . , 0 ,  0, 1, 0, . . . , 0 ,  0, 0, 1, . . . , 0 , . . . ,  0, 0, 0, . . . , 1 , hence a linear k=0 vk P k will be the zero matrix if and only if the coefficients vk are all zero. Furthermore, most of these linear combinations are actually non- singular matrices, for we have  P n−1.  modulo p u ,   w0, w1, . . . , wn−1   vk P k =  0, 0, . . . , 0   if and only if  v u w u  ≡ 0  Thus, n−1  where v u  = v0 + v1u +··· + vn−1un−1 and w u  = w0 + w1u +··· + wn−1un−1. k=0 vk P k is a singular matrix if and only if the polynomial v u  is a multiple of some factor of p u . We are now ready to prove the desired result. Theorem W  S. Winograd, 1975 . Let p u  be a monic polynomial of degree n whose complete factorization over a given infinite field is  p u  = p1 u e1 . . . pq u eq .   75  Then the rank of the tensor  74  corresponding to the bilinear forms  71  is 2n−q over this field. Proof. The bilinear forms can be evaluated with only 2n−q chain multiplications by using rules  58 ,  59 ,  60  in an appropriate fashion, so we must prove only that the rank r is ≥ 2n − q. The discussion above establishes the fact that rank T ij k  = n; hence by Lemma T, any n × r realization  A, B, C  of  Tijk  has rank C  = n. Our strategy will be to use Lemma T again, by finding a vector  v0, v1, . . . , vn−1  that has the following two properties: i  The vector  v0, v1, . . . , vn−1 C has at most q + r − n nonzero coefficients.  ii  The matrix v P  =n−1 r  k=0 vk P k is nonsingular.     n−1  k=0  ail bjl  vk ckl  = v P ij  l=1  This and Lemma T will prove that q + r − n ≥ n, since the identity  shows how to realize the n × n × 1 tensor v P  of rank n with q + r − n chain multiplications. We may assume for convenience that the first n columns of C are linearly independent. Let D be the n× n matrix such that the first n columns of DC are equal to the identity matrix. Our goal will be achieved if there is a linear combi- nation  v0, v1, . . . , vn−1  of at most q rows of D, such that v P  is nonsingular; such a vector will satisfy conditions  i  and  ii .   514  ARITHMETIC  4.6.4  Since the rows of D are linearly independent, no irreducible factor pλ u  can  divide the polynomials corresponding to every row. Given a vector  w =  w0, w1, . . . , wn−1 ,  let covered w  be the set of all λ such that w u  is not a multiple of pλ u . From two vectors v and w we can find a linear combination v + αw such that  covered v + αw  = covered v  ∪ covered w ,   76  for some α in the field. The reason is that if λ is covered by v or w but not both, then λ is covered by v + αw for all nonzero α; if λ is covered by both v and w but λ is not covered by v + αw, then λ is covered by v + βw for all β ̸= α. By trying q +1 different values of α, at least one must yield  76 . In this way we can systematically construct a linear combination of at most q rows of D, covering all λ for 1 ≤ λ ≤ q.  √  2 1 +  One of the most important corollaries of Theorem W is that the rank of a tensor can depend on the field from which we draw the elements of the realization  A, B, C . For example, consider the tensor corresponding to cyclic convolution of degree 5; this is equivalent to multiplication of polynomials mod p u  = u5−1. Over the field of rational numbers, the complete factorization of p u  is  u − 1 ×  u4 + u3 + u2 + u + 1  by exercise 4.6.2–32, so the tensor rank is 10− 2 = 8. On the other hand, the complete factorization over the real numbers, in terms of 5  , is  u − 1  u2 + ϕu + 1  u2 − ϕ−1u + 1 ; thus, the the number ϕ = 1 rank is only 7, if we allow arbitrary real numbers to appear in A, B, C. Over the complex numbers the rank is 5. This phenomenon does not occur in two- dimensional tensors  matrices , where the rank can be determined by evaluating determinants of submatrices and testing for 0. The rank of a matrix does not change when the field containing its elements is embedded in a larger field, but the rank of a tensor can decrease when the field gets larger. In the paper that introduced Theorem W [Math. Systems Theory 10  1977 , 169–180], Winograd went on to show that all realizations of  71  in 2n − q chain multiplications correspond to the use of  59 , when q is greater than 1. Furthermore he has shown that the only way to evaluate the coefficients of x u y u  in deg x  + deg y  + 1 chain multiplications is to use interpolation or to use  58  with a polynomial that splits into distinct linear factors in the field. Finally he has proved that the only way to evaluate x u y u  mod p u  in 2n − 1 chain multiplications when q = 1 is essentially to use  60 . These results hold for all polynomial chains, not only “normal” ones. He has extended the results to multivariate polynomials in SICOMP 9  1980 , 225–229. The tensor rank of an arbitrary m × n × 2 tensor in a suitably large field has been determined by Joseph Ja’Ja’, SICOMP 8  1979 , 443–462; JACM 27  1980 , 822–830. See also his interesting discussion of commutative bilinear forms in SICOMP 9  1980 , 713–728. However, the problem of computing the tensor rank of an arbitrary n × n × n tensor over any finite field is NP-complete [J. Håstad, Journal of Algorithms 11  1990 , 644–654].   4.6.4  EVALUATION OF POLYNOMIALS  515  u x  = u2n+1x  2n+1 + u2n−1x  2n−1 + ··· + u1x?  For further reading. In this section we have barely scratched the surface of a very large subject in which many beautiful theories are emerging. Considerably more comprehensive treatments can be found in the books Computational Com- plexity of Algebraic and Numeric Problems by A. Borodin and I. Munro  New York: American Elsevier, 1975 ; Polynomial and Matrix Computations 1 by D. Bini and V. Pan  Boston: Birkhäuser, 1994 ; Algebraic Complexity Theory by P. Bürgisser, M. Clausen, and M. Amin Shokrollahi  Heidelberg: Springer, 1997 . EXERCISES 1. [15] What is a good way to evaluate an “odd” polynomial   cid:120  2. [M20] Instead of computing u x + x0  by steps H1 and H2 as in the text, discuss two variables  the application of Horner’s rule  2  when polynomial multiplication and addition are used instead of arithmetic in the domain of coefficients. 3. [20] Give a method analogous to Horner’s rule, for evaluating a polynomial in i+j≤n uijxiyj.  This polynomial has  n + 1  n + 2  2 coefficients, and its “total degree” is n.  Count the number of additions and multiplications you use. 4. [M20] The text shows that scheme  3  is superior to Horner’s rule when we are evaluating a polynomial with real coefficients at a complex point z. Compare  3  to Horner’s rule when both the coefficients and the variable z are complex numbers; how many  real  multiplications and addition-subtractions are required by each method? 5. [M15] Count the number of multiplications and additions required by the second- order rule  4 . 6. [22]  L. de Jong and J. van Leeuwen.  Show how to improve on steps S1, . . . , S4 of the Shaw–Traub algorithm by computing only about 1 7. [M25] How can β0, . . . , βn be calculated so that  6  has the value u x0 + kh  for all integers k?   = x x − 1  . . .  x − k + 1 . 8. [M20] The factorial power xk is defined to be k!x per X  = −1 n−ϵ1−···−ϵn    Explain how to evaluate unxn + ··· + u1x1 + u0 with at most n multiplications and 2n − 1 additions, starting with x and the n + 3 constants un, . . . , u0, 1, n − 1. 9. [M25]  H. J. Ryser.  Show that if X =  xij  is an n × n matrix, then  2 n powers of x0.  ϵjxij  k  1≤i≤n  1≤j≤n  summed over all 2n choices of ϵ1, . . . , ϵn equal to 0 or 1 independently. Count the number of addition and multiplication operations required to evaluate per X  by this formula. 10. [M21] The permanent of an n× n matrix X =  xij  may be calculated as follows: quantities AkS have been computed, for all k-element subsets S of {1, 2, . . . , n}, where  Start with the n quantities x11, x12, . . . , x1n. For 1 ≤ k < n, assume that the n  AkS = x1j1 . . . xkjk summed over all k! permutations j1 . . . jk of the elements of S;  k  then form all of the sums  A k+1 S =  j∈S  Ak S\{j}  x k+1 j.  We have per X  = An{1,...,n} method require? How much temporary storage is needed?  . How many additions and multiplications does this   516  ARITHMETIC  4.6.4  11. [M46] Is there any way to evaluate the permanent of a general n× n matrix using fewer than 2n arithmetic operations? 12. [M50] What is the minimum number of multiplications required to form the product of two n × n matrices? What is the smallest exponent ω such that O nω+ϵ  multiplications are sufficient for all ϵ > 0?  Find good upper and lower bounds for small n as well as large n.  13. [M23] Find the inverse of the general discrete Fourier transform  37 , by express- ing F  t1, . . . , tn  in terms of the values of f s1, . . . , sn . [Hint: See Eq. 1.2.9– 13 .]   cid:120  14. [HM28]  Fast Fourier transforms.  Show that the scheme  40  can be used to  evaluate the one-dimensional discrete Fourier transform  F  t  ωst,  ω = e  2πi 2n  ,  0 ≤ s < 2n,  f s  =   0≤t<2n  distinct points x0, x1, . . . , xn is defined by the formula  using arithmetic on complex numbers. Estimate the number of arithmetic operations performed.   cid:120  15. [HM28] The nth divided difference f x0, x1, . . . , xn  of a function f x  at n + 1 k=0 f xk   for n > 0. Thus f x0, x1, . . . , xn  =n  tn−1  0≤j≤n, j̸=k xk − xj  is a symmetric function of its n + 1 arguments.  a  Prove that f x0, . . . , xn  = f  n  θ  n!, for some θ between min x0, . . . , xn  and max x0, . . . , xn , if the nth derivative f  n  x  exists and is continuous. [Hint: Prove the identity  f x0, x1, . . . , xn  =  f x0, x1, . . . , xn−1  − f x1, . . . , xn−1, xn    x0 − xn ,   t1   1  f x0, x1, . . . , xn  =  dt1  dt2 . . .   n  x0 1 − t1  + x1 t1 − t2  + ···  dtnf  0  0  0  + xn−1 tn−1 − tn  + xn tn − 0  . This formula also defines f x0, x1, . . . , xn  in a useful manner when the xj are not distinct.]  b  If yj = f xj , show that αj = f x0, . . . , xj  in Newton’s interpolation polynomial  42 . 16. [M22] How can we readily compute the coefficients of u[n] x  = unxn +···+ u0, if we are given the values of x0, x1, . . . , xn−1, α0, α1, . . . , αn in Newton’s interpolation polynomial  42 ? 17. [M20] Show that the interpolation formula  45  reduces to a very simple expres- sion involving binomial coefficients when xk = x0 + kh for 0 ≤ k ≤ n. [Hint: See exercise 1.2.6–48.] 18. [M20] If the fourth-degree scheme  9  were changed to  u x  =   y − x + α2 y + α3 α4,  y =  x + α0 x + α1,  what formulas for computing the αj’s in terms of the uk’s would take the place of  10 ?   cid:120  19. [M24] Explain how to determine the adapted coefficients α0, α1, . . . , α5 in  11   cid:120  20. [21] Write a MIX program that evaluates a fifth-degree polynomial according to  from the coefficients u5, . . . , u1, u0 of u x , and find the α’s for the particular poly- nomial u x  = x5 + 5x4 − 10x3 − 50x2 + 13x + 60.  scheme  11 ; try to make the program as efficient as possible, by making slight mod- ifications to  11 . Use MIX’s floating point arithmetic operators FADD and FMUL, which are described in Section 4.2.1.   4.6.4  EVALUATION OF POLYNOMIALS  517  21. [20] Find two additional ways to evaluate the polynomial x6 + 13x5 + 49x4 + 33x3 − 61x2 − 37x + 3 by scheme  12 , using the two roots of  15  that were not considered in the text. 22. [18] What is the scheme for evaluating x6 − 3x5 + x4 − 2x3 + x2 − 3x − 1, using Pan’s method  16 ? 23. [HM30]  J. Eve.  Let f z  = anzn + an−1zn−1 + ··· + a0 be a polynomial of degree n with real coefficients, having at least n− 1 roots with a nonnegative real part. Let  g z  = anzn + an−2 zn−2 + ··· + an mod 2 zn mod 2 h z  = an−1zn−1 + an−3zn−3 + ··· + a n−1  mod 2 z  ,   n−1  mod 2  .  Assume that h z  is not identically zero. a  Show that g z  has at least n− 2 imaginary roots  that is, roots whose real part is zero , and h z  has at least n − 3 imaginary roots. [Hint: Consider the number of times the path f z  circles the origin as z goes around the path shown in Fig. 16, for a sufficiently large radius R.]  b  Prove that the squares of the roots of g z  = 0 and h z  = 0 are all real.   cid:120  24. [M24] Find values of c and αk, βk satisfying the conditions of Theorem E, for the  Fig. 16. Proof of Eve’s theorem.  polynomial u x  =  x + 7  x2 + 6x + 10  x2 + 4x + 5  x + 1 . Choose these values so that β2 = 0. Give two different solutions. 25. [M20] When the construction in the proof of Theorem M is applied to the  ineffi- cient  polynomial chain  λ1 = α1 + λ0, λ5 = λ0 − λ0,  λ2 = −λ0 − λ0, λ6 = α6 − λ5,  λ3 = λ1 + λ1, λ7 = α7 × λ6,  λ4 = α2 × λ3, λ8 = λ7 × λ7,  λ9 = λ1 × λ4,  λ10 = α8 − λ9,  λ11 = λ3 − λ10,  how can β1, β2, . . . , β9 be expressed in terms of α1, . . . , α8?   cid:120  26. [M21]  a  Give the polynomial chain corresponding to Horner’s rule for evaluating  polynomials of degree n = 3.  b  Using the construction that appears in the text’s proof of Theorem A, express κ1, κ2, κ3, and the result polynomial u x  in terms of β1, β2, β3, β4, and x.  c  Show that the result set obtained in  b , as β1, β2, β3, and β4 independently assume all real values, omits certain vectors in the result set of  a . 27. [M22] Let R be a set that includes all  n+1 -tuples  qn, . . . , q1, q0  of real numbers such that qn ̸= 0; prove that R does not have at most n degrees of freedom.  −R  iR  0  −iR   518  ARITHMETIC  4.6.4  28. [HM20] Show that if f0 α1, . . . , αs , . . . , fs α1, . . . , αs  are multivariate polyno- mials with integer coefficients, then there is a nonzero polynomial g x0, . . . , xs  with integer coefficients such that g f0 α1, . . . , αs , . . . , fs α1, . . . , αs   = 0 for all real α1, . . . , αs.  Hence any polynomial chain with s parameters has at most s degrees of freedom.  [Hint: Use the theorems about “algebraic dependence” that are found, for example, in B. L. van der Waerden’s Modern Algebra, translated by Fred Blum  New York: Ungar, 1949 , Section 64.]   cid:120  29. [M20] Let R1, R2, . . . , Rm all be sets of  n + 1 -tuples of real numbers having at  cid:120  30. [M28] Prove that a polynomial chain with mc chain multiplications and mp  most t degrees of freedom. Show that the union R1 ∪ R2 ∪ ··· ∪ Rm also has at most t degrees of freedom.  parameter multiplications has at most 2mc + mp + δ0mc degrees of freedom. [Hint: Generalize Theorem M, showing that the first chain multiplication and each parameter multiplication can essentially introduce only one new parameter into the result set.] 31. [M23] Prove that a polynomial chain capable of computing all monic polynomials of degree n has at least ⌊n 2⌋ multiplications and at least n addition-subtractions. 32. [M24] Find a polynomial chain of minimum possible length that can compute all polynomials of the form u4x4 + u2x2 + u0; and prove that its length is minimal.   cid:120  33. [M25] Let n ≥ 3 be odd. Prove that a polynomial chain with ⌊n 2⌋ + 1 multi-  plication steps cannot compute all polynomials of degree n unless it has at least n + 2 addition-subtraction steps. [Hint: See exercise 30.] 34. [M26] Let λ0, λ1, . . . , λr be a polynomial chain in which all of the addition and subtraction steps are parameter steps, and in which there is at least one parameter multiplication. Assume that this scheme has m multiplications and k = r−m addition- subtractions, and that the polynomial computed by the chain has maximum degree n. Prove that all polynomials computable by this chain, for which the coefficient of xn is not zero, can be computed by another chain that has at most m multiplications and at most k additions, and no subtractions; furthermore the last step of the new chain should be the only parameter multiplication.   cid:120  35. [M25] Show that any polynomial chain that computes a general fourth-degree  polynomial using three multiplications must have at least five addition-subtractions. [Hint: Assume that there are only four addition-subtractions, and show that exer- cise 34 applies; therefore the scheme must have a particular form that is incapable of representing all fourth-degree polynomials.] 36. [M27] Continuing the previous exercise, show that any polynomial chain that computes a general sixth-degree polynomial using only four multiplications must have at least seven addition-subtractions. 37. [M21]  T. S. Motzkin.  Show that “almost all” rational functions of the form   unxn + un−1xn−1 + ··· + u1x + u0   xn + vn−1xn−1 + ··· + v1x + v0 ,  with coefficients in a field S, can be evaluated using the scheme  α1 + β1  x + α2 + β2  x + ··· + βn  x + αn+1  . . .   ,  for suitable αj, βj in S.  This continued fraction scheme has n divisions and 2n additions; by “almost all” rational functions we mean all except those whose coefficients satisfy some nontrivial polynomial equation.  Determine the α’s and β’s for the rational function  x2 + 10x + 29   x2 + 8x + 19 .   4.6.4   cid:120  38. [HM32]  V. Y. Pan, 1962.  The purpose of this exercise is to prove that Horner’s  EVALUATION OF POLYNOMIALS  519  rule is really optimal if no preliminary adaptation of coefficients is made; we need n multiplications and n additions to compute unxn + ··· + u1x + u0, if the variables un, . . . , u1, u0, x, and arbitrary constants are given. Consider chains that are as before except that un, . . . , u1, u0, x are each considered to be variables; we may say, for example, that λ−j−1 = uj, λ0 = x. In order to show that Horner’s rule is best, it is convenient to prove a somewhat more general theorem: Let A =  aij , 0 ≤ i ≤ m, 0 ≤ j ≤ n, be an  m + 1  ×  n + 1  matrix of real numbers, of rank n + 1; and let B =  b0, . . . , bm  be a vector of real numbers. Prove that any polynomial chain that computes  P  x; u0, . . . , un  =   ai0u0 + ··· + ainun + bi xi  m  i=0  involves at least n chain multiplications.  Note that this does not mean only that we are considering some fixed chain in which the parameters αj are assigned values depending on A and B; it means that both the chain and the values of the α’s may depend on the given matrix A and vector B. No matter how A, B, and the values of αj are chosen, it is impossible to compute P  x; u0, . . . , un  without doing n “chain- step” multiplications.  The assumption that A has rank n + 1 implies that m ≥ n. [Hint: Show that from any such scheme we can derive another that has fewer chain multiplications and that has n decreased by one.] 39. [M29]  T. S. Motzkin, 1954.  Show that schemes of the form wk = wk−1 w1 + γkx + αk  + δkx + βk  w1 = x x + α1  + β1,  for 1 < k ≤ m,  where the αk, βk are real and the γk, δk are integers, can be used to evaluate all monic polynomials of degree 2m over the real numbers.  We may have to choose αk, βk, γk, and δk differently for different polynomials.  Try to let δk = 0 whenever possible. 40. [M41] Can the lower bound in the number of multiplications in Theorem C be raised from ⌊n 2⌋ + 1 to ⌈n 2⌉ + 1?  See exercise 33.  41. [22] Show that the real and imaginary parts of  a + bi  c + di  can be obtained by doing 3 multiplications and 5 additions of real numbers, where two of the additions involve a and b only. 42. [36]  M. Paterson and L. Stockmeyer.   a  Prove that a polynomial chain with m ≥ 2 chain multiplications has at most m2 + 1 degrees of freedom.  b  Show that for all n ≥ 2 there exist polynomials of degree n, all of whose coefficients are 0 or 1, that cannot be evaluated by any polynomial chain with fewer than ⌊√ n⌋ multiplications, if we require all parameters αj to be integers.  c  Show that any polynomial of degree n with integer coefficients can be evaluated by an all-integer algorithm that performs at most 2⌊√ 43. [22] Explain how to evaluate xn + ··· + x + 1 with 2l n + 1  − 2 multiplications and l n + 1  additions  no divisions or subtractions , where l n  is the function studied  cid:120  44. [M25] Show that any monic polynomial u x  = xn + un−1xn−1 + ··· + u0 can be in Section 4.6.3.  n⌋ multiplications, if we don’t care how many additions we do.  evaluated with 1 α1, α2, . . . that are polynomials in un−1, un−2, . . . with integer coefficients. Consider first the case n = 2l.]  4 n additions, using parameters [Hint:  2 n + O log n  multiplications and ≤ 5   520  ARITHMETIC   cid:120  45. [HM22] Let  tijk  be an m× n× s tensor, and let F, G, H be nonsingular matrices  4.6.4  of respective sizes m × m, n × n, s × s. If  Tijk =m  n  s  i′=1  j′=1  k′=1 Fii′ Gjj′ Hkk′ ti′j′k′  for all i, j, k, prove that the tensor  Tijk  has the same rank as  tijk . [Hint: Consider what happens when F −1, G−1, H−1 are applied in the same way to  Tijk .] 46. [M28] Prove that all pairs  z1, z2  of bilinear forms in  x1, x2  and  y1, y2  can be evaluated with at most three chain multiplications. In other words, show that every 2 × 2 × 2 tensor has rank ≤ 3. 47. [M25] Prove that for all m, n, and s there exists an m × n × s tensor whose rank is at least ⌈mns  m + n + s ⌉. Conversely, show that every m × n × s tensor has rank at most mns max m, n, s . 48. [M21] If  tijk  and  t′ijk  are tensors of sizes m×n×s and m′×n′×s′, respectively, their direct sum  tijk  ⊕  t′ijk  =  t′′ijk  is the  m + m′  ×  n + n′  ×  s + s′  tensor defined by t′′ijk = tijk if i ≤ m, j ≤ n, k ≤ s; t′′ijk = t′i−m,j−n,k−s if i > m, j > n, k > s; and t′′ijk = 0 otherwise. Their direct product  tijk  ⊗  t′ijk  =  t′′′ijk  is the mm′ × nn′ × ss′ tensor defined by t⟨ii′ = tijkt′i′j′k′. Derive the upper bounds  cid:120  49. [HM25] Show that the rank of an m × n × 1 tensor  tijk  is the same as its rank rank t′′ijk  ≤ rank tijk  + rank t′ijk  and rank t′′′ijk  ≤ rank tijk  · rank t′ijk .  as an m × n matrix  tij1 , according to the traditional definition of matrix rank as the maximum number of linearly independent rows. 50. [HM20]  S. Winograd.  Let  tijk  be the mn × n × m tensor corresponding to multiplication of an m × n matrix by an n × 1 column vector. Prove that the rank of  tijk  is mn.   cid:120  51. [M24]  S. Winograd.  Devise an algorithm for cyclic convolution of degree 2 that  ⟩⟨kk′  ⟩⟨jj′  ⟩  uses 2 multiplications and 4 additions, not counting operations on the xi. Similarly, devise an algorithm for degree 3, using 4 multiplications and 11 additions.  See  69 , which solves the analogous problem for degree 4.  52. [M25]  S. Winograd.  Let n = n′n′′ where n′ ⊥ n′′. Given normal schemes for cyclic convolutions of degrees n′ and n′′, using respectively  m′, m′′  chain multiplica- tions,  p′, p′′  parameter multiplications, and  a′, a′′  additions, show how to construct a normal scheme for cyclic convolution of degree n using m′m′′ chain multiplications, p′n′′ + m′p′′ parameter multiplications, and a′n′′ + m′a′′ additions. 53. [HM40]  S. Winograd.  Let ω be a complex mth root of unity, and consider the one-dimensional discrete Fourier transform  m  t=1  f s  =  F  t  ωst,  for 1 ≤ s ≤ m.  a  When m = pe is a power of an odd prime, show that efficient normal schemes for computing cyclic convolutions of degrees  p − 1 pk, for 0 ≤ k < e, will lead to efficient algorithms for computing the Fourier transform on m complex numbers. Give a similar construction for the case p = 2.  b  When m = m′m′′ and m′ ⊥ m′′, show that Fourier transformation algorithms for m′ and m′′ can be combined to yield a Fourier transformation algorithm for m elements.   4.6.4  EVALUATION OF POLYNOMIALS  521  i=1  2rank tijk  chain multiplications.  quadratic forms n  n j=1 τijkxixj for 1 ≤ k ≤ s must use at least 1  54. [M23] Theorem W refers to an infinite field. How many elements must a finite field have in order for the proof of Theorem W to be valid? 55. [HM22] Determine the rank of tensor  74  when P is an arbitrary n × n matrix. 56. [M32]  V. Strassen.  Show that any polynomial chain that evaluates a set of 2rank τijk + τjik  chain multiplications altogether. [Hint: Show that the minimum number of chain multiplications is the minimum rank of  tijk  taken over all tensors  tijk  such that tijk + tjik = τijk + τjik for all i, j, k.] Conclude that if a polynomial chain evaluates a set of bilinear forms  47  corresponding to a tensor  tijk , whether normal or abnormal, it must use at least 1 57. [M20] Show that fast Fourier transforms can be used to compute the coefficients of the product x u y u  of two given polynomials of degree n, using O n log n  operations of  exact  addition and multiplication of complex numbers. [Hint: Consider the product of Fourier transforms of the coefficients.] 58. [HM28]  a  Show that any realization  A, B, C  of the polynomial multiplication tensor  55  must have the following property: Any nonzero linear combination of the three rows of A must be a vector with at least four nonzero elements; and any nonzero linear combination of the four rows of B must have at least three nonzero elements.  b  Find a realization  A, B, C  of  55  that uses only 0, +1, and −1 as elements, where r = 8. Try to use as many 0s as possible.   cid:120  59. [M40]  H. J. Nussbaumer, 1980.  The text defines the cyclic convolution of two sequences  x0, x1, . . . , xn−1  and  y0, y1, . . . , yn−1  to be the sequence  z0, z1, . . . , zn−1  where zk = x0yk +···+ xky0 + xk+1yn−1 +···+ xn−1yk+1. Let us define the negacyclic convolution similarly, but with  zk = x0yk + ··· + xky0 −  xk+1yn−1 + ··· + xn−1yk+1 .  Construct efficient algorithms for cyclic and negacyclic convolution over the integers when n is a power of 2. Your algorithms should deal entirely with integers, and they should perform at most O n log n  multiplications and at most O n log n log log n  additions or subtractions or divisions of even numbers by 2. [Hint: A cyclic convolution of order 2n can be reduced to cyclic and negacyclic convolutions of order n, using  59 .] 60. [M27]  V. Y. Pan.  The problem of  m × n  times  n × s  matrix multiplication corresponds to an mn × ns × sm tensor  t⟨i,j′   where t⟨i,j′ = 1 if and only if i′ = i and j′ = j and k′ = k. The rank of this tensor T  m, n, s  is the smallest number r such that numbers aij′l, bjk′l, cki′l exist satisfying  ⟩⟨j,k′  ⟩⟨j,k′  ⟩⟨k,i′  ⟩⟨k,i′  ⟩  ⟩    xij yjk zki =      1≤i≤m 1≤j≤n 1≤k≤s  1≤l≤r  1≤i≤m 1≤j′ ≤n  aij′l xij′  bjk′l yjk′  cki′l zki′  .     1≤j≤n 1≤k′ ≤s     1≤k≤s 1≤i′ ≤m    Let M n  be the rank of T  n, n, n . The purpose of this exercise is to exploit the symmetry of such a trilinear representation, obtaining efficient realizations of matrix multiplication over the integers when m = n = s = 2ν. For convenience we divide the indices {1, . . . , n} into two subsets O = {1, 3, . . . , n − 1} and E = {2, 4, . . . , n} of ν elements each, and we set up a one-to-one correspondence between O and E by the rule ˜ı = i + 1 if i ∈ O; ˜ı = i − 1 if i ∈ E. Thus we have ˜˜ı = i for all indices i.   4.6.4  522  ARITHMETIC  a  The identity  implies that  xij yjk zki =   1≤i,j,k≤n   i,j,k ∈S  abc + ABC =  a + A  b + B  c + C  −  a + A bC − A b + B c − aB c + C    xij + x˜κ˜ı  yjk + y˜ı˜ȷ  zki + z˜ȷ˜κ  − Σ1 − Σ2 − Σ3,  where S = E×E×E ∪ E×E×O ∪ E×O×E ∪ O×E×E is the set of all triples of indices containing at most one odd index; Σ1 is the sum of all terms of the form  xij + x˜κ˜ı yjk z˜ȷ˜κ for  i, j, k  ∈ S; and Σ2, Σ3 similarly are sums of the terms 2 n3 terms. Show that x˜κ˜ı yjk + y˜ı˜ȷ zki, xij y˜ı˜ȷ zki + z˜ȷ˜κ . Clearly S has 4ν3 = 1 each of Σ1, Σ2, Σ3 can be realized as the sum of 3ν2 trilinear terms; furthermore, if the 3ν triples of the forms  i, i,˜ı  and  i,˜ı, i  and  ˜ı, i, i  are removed from S, we can modify Σ1, Σ2, and Σ3 in such a way that the identity is still valid, without adding any new trilinear terms. Thus M n  ≤ 1 2 n when n is even. b  Apply the method of  a  to show that two independent matrix multiplication problems of the respective sizes m × n × s and s × m × n can be performed with mns + mn + ns + sm noncommutative multiplications.  4 n2 − 3  2 n3 + 9  61. [M26] Let  tijk  be a tensor over an arbitrary field. We define rankd tijk  as the minimum value of r such that there is a realization of the form  r  l=1  ail u bjl u ckl u  = tijkud + O ud+1 ,  2  b  rank tijk  ≤d+2  where ail u , bjl u , ckl u  are polynomials in u over the field. Thus rank0 is the ordinary rank of a tensor. Prove that a  rankd+1 tijk  ≤ rankd tijk ; c  rankd  tijk  ⊕  t′ijk   ≤ rankd tijk  + rankd t′ijk , in the sense of exercise 48; d  rankd+d′  tijk  ⊗  t′ijk   ≤ rankd tijk  · rankd′ t′ijk ; e  rankd+d′  tijk  ⊗  t′ijk   ≤ rankd′ r t′ijk  , where r = rankd tijk  and rT denotes   rankd tijk ;  the direct sum T ⊕ ··· ⊕ T of r copies of T .   has rank 3 but  where rankd is defined in exercise 61. Prove that the tensor 1  62. [M24] The border rank of  tijk , denoted by rank tijk , is mind≥0 rankd tijk , border rank 2, over every field. 63. [HM30] Let T  m, n, s  be the tensor for matrix multiplication as in exercise 60, and let M N  be the rank of T  N, N, N . a  Show that T  m, n, s  ⊗ T  M, N, S  = T  mM, nN, sS . b  Show that rankd T  mN, nN, sN   ≤ rankd M N T  m, n, s    see exercise 61 e  . c  If T  m, n, s  has rank ≤ r, show that M N  = O N ω m,n,s,r   as N → ∞, where d  If T  m, n, s  has border rank ≤ r, show that M N  = O N ω m,n,s,r  log N 2 . 64. [M30]  A. Schönhage.  Show that rank2 T  3, 3, 3   ≤ 21, so M N  = O N 2.78 .   cid:120  65. [M27]  A. Schönhage.  Show that rank2 T  m, 1, n  ⊕ T  1,  m−1  n−1 , 1   =  ω m, n, s, r  = 3 log r  log mns.   0  1 0  0 1  0  0  mn + 1. Hint: Consider the trilinear form   xi + uXij  yj + uYij  Z + u  2  zij  −  x1 + ··· + xm  y1 + ··· + yn Z  m  n  i=1  j=1   4.6.4  whenm  i=1 Xij =n  j=1 Yij = 0.  EVALUATION OF POLYNOMIALS  523  rank t . Hint: Consider direct products of t with itself.  66. [HM33] We can now use the result of exercise 65 to sharpen the asymptotic bounds of exercise 63. a  Prove that the limit ω = limn→∞ log M n  log n exists. b  Prove that  mns ω 3 ≤ rank T  m, n, s  . c  Let t be the tensor T  m, n, s ⊕ T  M, N, S . Prove that  mns ω 3 +  M N S ω 3 ≤ d  Therefore 16ω 3 + 9ω 3 ≤ 17, and we have ω < 2.55. 67. [HM40]  D. Coppersmith and S. Winograd.  By generalizing exercises 65 and 66 we can obtain even better upper bounds on ω. a  Say that the tensor  tijk  is nondegenerate if rank ti jk   = m, rank tj ki   = n, and rank tk ij   = s, in the notation of Lemma T. Prove that the tensor T  m, n, s  for mn × ns matrix multiplication is nondegenerate.  b  Show that the direct sum of nondegenerate tensors is nondegenerate. c  An m × n × s tensor t with realization  A, B, C  of length r is called improv- r able if it is nondegenerate and there are nonzero elements d1, . . . , dr such that l=1 ailbjldl = 0 for 1 ≤ i ≤ m and 1 ≤ j ≤ n. Prove that in such a are q × r matrices V and W such that r case t ⊕ T  1, q, 1  has border rank ≤ r, where q = r − m − n. Hint: There r l=1 ailwjldl = 0 and  l=1 vilbjldl = r  l=1 vilwjldl = δij for all relevant i and j.  d  Explain why the result of exercise 65 is a special of  c . e  Prove that rank T  m, n, s   ≤ r implies  rank2 T  m, n, s  ⊕ T  1, r − n m + s − 1 , 1   ≤ r + n.  f  Therefore ω is strictly less than log M n  log n for all n > 1. g  Generalize  c  to the case where  A, B, C  realizes t only in the weaker sense of exercise 61. h  From  d  we have rank T  3, 1, 3 ⊕ T  1, 4, 1   ≤ 10; thus by exercise 61 d  we also have rank T  9, 1, 9  ⊕ 2T  3, 4, 3  ⊕ T  1, 16, 1   ≤ 100. Prove that if we simply delete the rows of A and B that correspond to the 16 + 16 variables of T  1, 16, 1 , we obtain a realization of T  9, 1, 9  ⊕ 2T  3, 4, 3  that is improvable. Therefore we have in fact rank T  9, 1, 9  ⊕ 2T  3, 4, 3  ⊕ T  1, 34, 1   ≤ 100.  i  Generalizing exercise 66 c , show that   mpnpsp ω 3 ≤ rank  T  mp, np, sp   .   t  p=1    j  Therefore ω < 2.5.  68. [M45] Is there a way to evaluate the polynomial  xixj = x1x2 + ··· + xn−1xn  1≤i<j≤n  with fewer than n − 1 multiplications and 2n − 4 additions?  There aren  terms.   cid:120  69. [HM27]  V. Strassen, 1973.  Show that the determinant  31  of an n × n matrix  can be evaluated by doing O n5  multiplications and O n5  additions or subtractions, and no divisions. [Hint: Consider det I + Y   where Y = X − I.]  2  t  p=1     524  4.6.4  ARITHMETIC   cid:120  70. [HM25] The characteristic polynomial fX λ  of a matrix X is defined to be  Y  , where X, u, v, and Y are respectively of  det  λI − X . Prove that if X =   x sizes n × n, 1 ×  n − 1 ,  n − 1  × 1, and  n − 1  ×  n − 1 , we have λ2 − uY 2v λ3 − ···  I   A − BD−1C B  Show that this relation allows us to compute the coefficients of fX with about 1 4 n4 addition-subtractions, and no divisions. Hint: Use the identity multiplications, 1   A B  λ − x − uv λ  fX λ  = fY  λ    I  − uY v          4 n4  u  .  v  =  0 0 D  C D  0  I  0 D−1C I  ,  which holds for any matrices A, B, C, and D of respective sizes l × l, l × m, m× l, and m × m when D is nonsingular.   cid:120  71. [HM30] A quolynomial chain is like a polynomial chain except that it allows  division as well as addition, subtraction, and multiplication. Prove that if f x1, . . . , xn  can be computed by a quolynomial chain that has m chain multiplications and d di- visions, then f x1, . . . , xn  and all n of its partial derivatives ∂f x1, . . . , xn  ∂xk for 1 ≤ k ≤ n can be computed by a single quolynomial chain that has at most 3m+d chain multiplications and 2d divisions.  Consequently, for example, any efficient method for calculating the determinant of a matrix leads to an efficient method for calculating all of its cofactors, hence an efficient method for computing the inverse matrix.  72. [M48] Is it possible to determine the rank of any given tensor  tijk  over, say, the field of rational numbers, in a finite number of steps? 73. [HM25]  J. Morgenstern, 1973.  Prove that any polynomial chain for the discrete Fourier transform  37  has at least 1 2 m1 . . . mn lg m1 . . . mn addition-subtractions, if there are no chain multiplications and if every parameter multiplication is by a complex- valued constant with αj ≤ 1. Hint: Consider the matrices of the linear transforma- tions computed by the first k steps. How big can their determinants be? 74. [HM35]  A. Nozaki, 1978.  Most of the theory of polynomial evaluation is con- cerned with bounds on chain multiplications, but multiplication by noninteger constants can also be essential. The purpose of this exercise is to develop an appropriate theory of constants. Let us say that vectors v1, . . . , vs of real numbers are Z-dependent if there are integers  k1, . . . , ks  such that gcd k1, . . . , ks  = 1 and k1v1 + ··· + ksvs is an all-integer vector. If no such  k1, . . . , ks  exist, the vectors v1, . . . , vs are Z-independent. a  Prove that if the columns of an r × s matrix V are Z-independent, so are the columns of V U, when U is any s × s unimodular matrix  a matrix of integers whose determinant is ±1 . b  Let V be an r × s matrix with Z-independent columns. Prove that a poly- . . . , xs, where c  Let V be an r × t matrix having s columns that are Z-independent. Prove that a polynomial chain to evaluate the elements of V x from inputs x1, . . . , xt, where x =  x1, . . . , xt T , needs at least s multiplications. d  Show how to compute the pair of values {x 2 + y, x + y 3} from x and y using only one multiplication, although two multiplications are needed to compute the pair {x 2 + y, x + y 2}.  nomial chain to evaluate the elements of V x from inputs x1, x =  x1, . . . , xs T , needs at least s multiplications.   4.7  MANIPULATION OF POWER SERIES  525  *4.7. MANIPULATION OF POWER SERIES If we are given two power series U z  = U0 + U1z + U2z2 + ··· ,   1  whose coefficients belong to a field, we can form their sum, their product, and sometimes their quotient, to obtain new power series. A polynomial is obviously a special case of a power series, in which there are only finitely many terms.  V  z  = V0 + V1z + V2z2 + ··· ,  Of course, only a finite number of terms can be represented and stored within a computer, so it makes sense to ask whether power series arithmetic is even possible on computers; and if it is possible, what makes it different from polynomial arithmetic? The answer is that we work with only the first N coefficients of the power series, where N is a parameter that may in principle be arbitrarily large; instead of ordinary polynomial arithmetic, we are essentially doing polynomial arithmetic modulo zN, and this often leads to a somewhat different point of view. Furthermore, special operations like “reversion” can be performed on power series but not on polynomials, since polynomials are not closed under those operations.  Manipulation of power series has many applications to numerical analysis, but perhaps its greatest use is the determination of asymptotic expansions  as we have seen in Section 1.2.11.3 , or the calculation of quantities defined by certain generating functions. The latter applications make it desirable to calculate the coefficients exactly, instead of with floating point arithmetic. All of the algorithms in this section, with obvious exceptions, can be done using rational operations only, so the techniques of Section 4.5.1 can be used to obtain exact results when desired. The calculation of W z  = U z  ± V  z  is, of course, trivial, since we have Wn = [zn] W z  = Un ± Vn for n = 0, 1, 2, . . . . It is also easy to calculate the coefficients of W z  = U z V  z , using the familiar convolution rule  Wn =  UkVn−k = U0Vn + U1Vn−1 + ··· + UnV0.   2   The quotient W z  = U z  V  z , when V0 ̸= 0, can be obtained by inter-  changing U and W in  2 ; we obtain the rule    Wn =  WkVn−k  V0  =  Un − W0Vn − W1Vn−1 − ··· − Wn−1V1  V0.   3  This recurrence relation for the W’s makes it easy to determine W0, W1, W2, . . . successively, without inputting Un and Vn until after Wn−1 has been computed. A power series manipulation algorithm with that property is traditionally called online; with an online algorithm, we can determine N coefficients W0, W1, . . . , WN−1 of the result without knowing N in advance, so we could in principle run the algorithm indefinitely and compute the entire power series. We can also run  n  k=0   Un − n−1  k=0   526  ARITHMETIC  4.7  an online algorithm until any desired condition is met.  The opposite of “online” is “offline.”   If the coefficients Uk and Vk are integers but the Wk are not, the recurrence relation  3  involves computation with fractions. This can be avoided by the all-integer approach described in exercise 2.  Let us now consider the operation of computing W z  = V  z α, where α is an “arbitrary” power. For example, we could calculate the square root of V  z  2, or we could find V  z −10 or even V  z π. If Vm is the first by taking α = 1 nonzero coefficient of V  z , we have  V  z  = Vm zm1 +  Vm+1 Vm z +  Vm+2 Vm z2 + ···, m zαm1 +  Vm+1 Vm z +  Vm+2 Vm z2 + ···α  V  z α = V α   4   .  This will be a power series if and only if αm is a nonnegative integer. If α itself is not an integer, there’s more than one possibility for V α  From  4  we can see that the problem of computing general powers can be reduced to the case that V0 = 1; then the problem is to compute the coefficients of  5   W z  =  1 + V1z + V2z2 + V3z3 + ···  α.  mzαm here.  Clearly W0 = 1α = 1.  The obvious way to find the coefficients of  5  is to use the binomial theorem, Eq. 1.2.9– 19 , or  if α is a positive integer  to try repeated squaring as in Section 4.6.3. But Leonhard Euler discovered a much simpler and more efficient way to obtain power series powers [Introductio in Analysin Infinitorum 1  1748 , §76]: If W z  = V  z α, we have by differentiation  W1 + 2W2z + 3W3z2 + ··· = W ′ z  = αV  z α−1V ′ z ;  n   α + 1  k=0  therefore  W ′ z V  z  = αW z V ′ z .  If we now equate the coefficients of zn−1 in  7 , we find that  n − k WkVn−k,  kWkVn−k = α  n  k=0    n  k=1  Wn =  k − 1  VkWn−k  and this gives us a useful computational rule valid for all n ≥ 1:  n = α+1−n V1Wn−1 +  2α+2−n V2Wn−2 + ··· + nαVnW0   9  Equation  9  leads to a simple online algorithm by which we can successively determine W1, W2, . . . , using approximately 2n multiplications to compute the nth coefficient. Notice the special case α = −1, in which  9  becomes the special case U z  = V0 = 1 of  3 .  A similar technique can be used to form fV  z  when f is any function   n.  that satisfies a simple differential equation.  For example, see exercise 4.  A comparatively straightforward “power series method” is often used to obtain   6    7    8    4.7  MANIPULATION OF POWER SERIES  527  Fig. 17. Power series reversion by Algorithm L.  the solution of differential equations; this technique is explained in nearly all textbooks about differential equations. Reversion of series. The transformation of power series that is perhaps of greatest interest is called “reversion of series.” This problem is to solve the equation  z = t + V2t2 + V3t3 + V4t4 + ···   10   for t, obtaining the coefficients of the power series  t = z + W2z2 + W3z3 + W4z4 + ··· .   11  Several interesting ways to achieve such a reversion are known. We might say that the “classical” method is one based on Lagrange’s remarkable inversion formula [Mémoires Acad. Royale des Sciences et Belles-Lettres de Berlin 24  1768 , 251–326], which states that  4  Wn = 1  For example, we have  1−t −5 =4 t+6 W5, in the reversion of z = t − t2 is equal to8  +5  t2+··· ; hence the fifth coefficient,  5 = 14. This checks with the  [tn−1]  1 + V2t + V3t2 + ···  −n.   12   n  4  4  4  formulas for enumerating binary trees in Section 2.3.4.4.  Relation  12 , which has a simple algorithmic proof  see exercise 16 , shows that we can revert the series  10  if we successively compute the negative powers  1 + V2t + V3t2 + ···  −n for n = 1, 2, 3, . . . . A straightforward application of this idea would lead to an online reversion algorithm that uses approximately N 3 2 multiplications to find N coefficients, but Eq.  9  makes it possible to work with only the first n coefficients of  1 + V2t + V3t2 + ···  −n, obtaining an online algorithm that requires only about N 3 6 multiplications. Algorithm L  Lagrangian power series reversion . This online algorithm inputs the value of Vn in  10  and outputs the value of Wn in  11 , for n = 2, 3, 4, . . . , N.  The number N need not be specified in advance; any desired termination criterion may be substituted.  L1. [Initialize.] Set n ← 1, U0 ← 1.  The relation   1 + V2t + V3t2 + ···  −n = U0 + U1t + ··· + Un−1tn−1 + O tn    13   will be maintained throughout this algorithm.   L1. Initialize  n > N  L2. Input Vn  L3. Divide  L4. Output Wn   528  ARITHMETIC  4.7  L2. [Input Vn.] Increase n by 1. If n > N, the algorithm terminates; otherwise L3. [Divide.] Set Uk ← Uk − Uk−1V2 − ··· − U1Vk − U0Vk+1, for k = 1, 2, . . . ,  input the next coefficient, Vn. n − 2  in this order ; then set  Un−1 ← −2Un−2V2 − 3Un−3V3 − ··· −  n − 1 U1Vn−1 − nU0Vn.  We have thereby divided U z  by V  z  z; see  3  and  9 .  L4. [Output Wn.] Output Un−1 n  which is Wn  and return to L2. When applied to the example z = t − t2, Algorithm L computes  n 1 2 3 4 5  Vn 1 −1 0 0 0  U0 1 1 1 1 1  U1  U2  U3  U4  2 3 4 5  6 10 15  20 35  70  Wn 1 1 2 5 14  Exercise 8 shows that a slight modification of Algorithm L will solve a consider- ably more general problem with only a little more effort.  Let us now consider solving the equation  U1z + U2z2 + U3z3 + ··· = t + V2t2 + V3t3 + ···   14   for t, obtaining the coefficients of the power series  t = W1z + W2z2 + W3z3 + W4z4 + ··· .   15  Eq.  10  is the special case U1 = 1, U2 = U3 = ··· = 0. If U1 ̸= 0, we may assume that U1 = 1, if we replace z by  U1z ; but we shall consider the general equation  14 , since U1 might equal zero. Algorithm T  General power series reversion . This online algorithm inputs the values of Un and Vn in  14  and outputs the value of Wn in  15 , for n = 1, 2, 3, . . . , N. An auxiliary matrix Tmn, 1 ≤ m ≤ n ≤ N, is used in the calculations. T1. [Initialize.] Set n ← 1. Let the first two inputs  namely, U1 and V1  be  stored in T11 and V1, respectively.  We must have V1 = 1.   T2. [Output Wn.] Output the value of T1n  which is Wn . T3. [Input Un, Vn.]  Increase n by 1.  If n > N, the algorithm terminates;  otherwise store the next two inputs  namely, Un and Vn  in T1n and Vn.  T4. [Multiply.] Set  Tmn ← T11Tm−1,n−1 + T12Tm−1,n−2 + ··· + T1,n−m+1Tm−1,m−1  and T1n ← T1n − VmTmn, for 2 ≤ m ≤ n. After this step we have m = 1, we have Un = T1n + V2T2n + ··· + VnTnn by  14  and  16 . Return   16  for 1 ≤ m ≤ n. It is easy to verify  16  by induction for m ≥ 2, and when  tm = Tmmzm + Tm,m+1zm+1 + ··· + Tmnzn + O zn+1 ,  to step T2.   4.7  MANIPULATION OF POWER SERIES  529  Equation  16  explains the mechanism of this algorithm, which is due to Henry C. Thacher, Jr. [CACM 9  1966 , 10–11]. The running time is essentially the same as Algorithm L, but considerably more storage space is required. An example of this algorithm is worked out in exercise 9.  Still another approach to power series reversion has been proposed by R. P. Brent and H. T. Kung [JACM 25  1978 , 581–595], based on the fact that standard iterative procedures used to find roots of equations over the real num- bers can also be applied to equations over power series. In particular, we can consider Newton’s method for computing approximations to a real number t such that f t  = 0, given a function f that is well-behaved near t: If x is a good approximation to t, then ϕ x  = x − f x  f′ x  will be even better, for if we write x = t + ϵ we have f x  = f t  + ϵf′ t  + O ϵ2 , f′ x  = f′ t  + O ϵ ; Applying this idea to power series, let f x  = V  x  − U z , where U and V are the power series in Eq.  14 . We wish to find the power series t in z such that f t  = 0. Let x = W1z + ··· + Wn−1zn−1 = t + O zn  be an “approximation” to t of order n; then ϕ x  = x− f x  f′ x  will be an approximation of order 2n, since the assumptions of Newton’s method hold for this f and t.  consequently ϕ x  = t + ϵ −0 + ϵf′ t  + O ϵ2  f′ t  + O ϵ  = t + O ϵ2 .  In other words, we can use the following procedure:  Algorithm N  General power series reversion by Newton’s method . This “semi- online” algorithm inputs the values of Un and Vn in  14  for 2k ≤ n < 2k+1 and then outputs the values of Wn in  15  for 2k ≤ n < 2k+1, thereby producing its answers in batches of 2k at a time, for k = 0, 1, 2, . . . , K. N1. [Initialize.] Set N ← 1.  We will have N = 2k.  Input the first coefficients  U1 and V1  where V1 = 1 , and set W1 ← U1.  N2. [Output.] Output Wn for N ≤ n < 2N. N3. [Input.] Set N ← 2N.  input the values Un and Vn for N ≤ n < 2N.  If N > 2K, the algorithm terminates; otherwise  N4. [Newtonian step.] Use an algorithm for power series composition  see exer- cise 11  to evaluate the coefficients Qj and Rj  0 ≤ j < N  in the power series U1z + ··· + U2N−1z2N−1 − V  W1z + ··· + WN−1zN−1   = R0zN + R1zN+1 + ··· + RN−1z2N−1 + O z2N ,  V ′ W1z + ··· + WN−1zN−1  = Q0 + Q1z + ··· + QN−1zN−1 + O zN ,  where V  x  = x+ V2x2 +··· and V ′ x  = 1+2V2x+··· . Then set WN, . . . , W2N−1 to the coefficients in the power series  R0+R1z+··· +RN−1zN−1 Q0+Q1z+··· +QN−1zN−1 = WN + ··· + W2N−1zN−1 + O zN   and return to step N2.   4.7  530  ARITHMETIC  is T N , where  The running time for this algorithm to obtain the coefficients up to N = 2K  f [m+n] x  = f [m]f [n] x   T 2N  = T N  +  time to do step N4  + O N .   17  Straightforward algorithms for composition and division in step N4 will take order N 3 steps, so Algorithm N will run slower than Algorithm T. However, Brent and Kung have found a way to do the required composition of power series with O N log N 3 2 arithmetic operations, and exercise 6 gives an even faster algorithm for division; hence  17  shows that power series reversion can be achieved by doing only O N log N 3 2 operations as N → ∞.  On the other hand the constant of proportionality is such that N must be really large before Algorithms L and T lose out to this “high-speed” method.   Historical note: J. N. Bramhall and M. A. Chapple published the first O N 3  method for power series reversion in CACM 4  1961 , 317–318, 503. It was an offline algorithm essentially equivalent to the method of exercise 16, with running time approximately the same as that of Algorithms L and T. Iteration of series. If we want to study the behavior of an iterative process xn ← f xn−1 , we are interested in studying the n-fold composition of a given  function f with itself, namely xn = ff . . . f x0  . . .  . Let us define f [0] x  = x and f [n] x  = ff [n−1] x , so that   18  for all integers m, n ≥ 0. In many cases the notation f [n] x  makes sense also when n is a negative integer, namely if f [n] and f [−n] are inverse functions  such that x = f [n]f [−n] x ; if inverse functions are unique,  18  holds for all that z = VW z  and that t = WV  t , so W = V [−1].  integers m and n. Reversion of series is essentially the operation of finding the inverse power series f [−1] x ; for example, Eqs.  10  and  11  essentially state Suppose we are given two power series V  z  = z + V2z2 + ··· and W z  = z+W2z2+··· such that W = V [−1]. Let u be any nonzero constant, and consider the function  It is easy to see that UU z  = Wu2V  z , and in general that  U z  = WuV  z . U [n] z  = WunV  z    20  for all integers n. Therefore we have a simple expression for the nth iterate U [n], which can be calculated with roughly the same amount of work for all n. Furthermore, we can even use  20  to define U [n] for noninteger values of n; the  “half iterate” U [1 2], for example, is a function such that U [1 2]U [1 2] z  = U z . There are two such functions U [1 2], obtained by using the value of u1 2 in  20 . u and −√  We obtained the simple state of affairs in  20  by starting with V and u, then defining U. But in practice we generally want to go the other way: Starting with  u as   19   √   4.7  MANIPULATION OF POWER SERIES  531  VU z  = u V  z .  some given function U, we want to find V and u such that  19  holds, namely such that   21  Such a function V is called the Schröder function of U, because it was introduced by Ernst Schröder in Math. Annalen 3  1871 , 296–322. Let us now look at the problem of finding the Schröder function V  z  = z + V2z2 + ··· of a given power series U z  = U1z + U2z2 + ··· . Clearly u = U1 if  21  is to hold. Expanding  21  with u = U1 and equating coefficients of z leads to a sequence of equations that begins  U 2 1 V2 + U2 = U1V2 , U 3 1 V3 + 2U1U2V2 + U3 = U1V3 , 2 V2 + U4 = U1V4 ,  U 4 1 V4 + 3U 2  1 U2V3 + 2U1U3V2 + U 2  and so on. Clearly there is no solution when U1 = 0  unless trivially U2 = U3 = ··· = 0 ; otherwise there is a unique solution unless U1 is a root of unity. We might have expected that something funny would happen when U n1 = 1, since Eq.  20  tells us that U [n] z  = z if the Schröder function exists in that case. For the moment let us assume that U1 is nonzero and not a root of unity; then the Schröder function does exist, and the next question is how to compute it without doing too much work.  The following procedure has been suggested by R. P. Brent and J. F. Traub. Equation  21  leads to subproblems of a similar but more complicated form, so we set ourselves a more general task whose subtasks have the same form: Let us try to find V  z  = V0 + V1z + ··· + Vn−1zn−1 such that  VU z  = W z V  z  + S z  + O zn ,   22  given U z , W z , S z , and n, where n is a power of 2 and U 0  = 0. If n = 1  we simply let V0 = S 0 1 − W 0 , with V0 = 1 if S 0  = 0 and W 0  = 1.  Furthermore it is possible to go from n to 2n: First we find R z  such that  VU z  = W z V  z  + S z  − znR z  + O z2n .   23   Then we compute  ˆW z  = W z z U z n + O zn ,  ˆS z  = R z z U z n + O zn ,  and find ˆV  z  = Vn + Vn+1z + ··· + V2n−1zn−1 such that  ˆVU z  = ˆW z  ˆV  z  + ˆS z  + O zn . V ∗U z  = W z V ∗ z  + S z  + O z2n ,  It follows that the function V ∗ z  = V  z  + zn ˆV  z  satisfies  as desired.  The running time T n  of this procedure satisfies  T 2n  = 2T n  + C n ,   24    25    26    VU z  = U′ z V  z .  ARITHMETIC  532 4.7 where C n  is the time to compute R z , ˆW z , and ˆS z . The function C n  is  dominated by the time to compute VU z  modulo z2n, and C n  presumably  grows faster than order n1+ϵ; therefore the solution T n  to  26  will be of order C n . For example, if C n  = cn3 we have T n  ≈ 4 3 cn3; or if C n  is O n log n 3 2 using “fast” composition, we have T n  = O n log n 3 2. The procedure breaks down when W 0  = 1 and S 0  ̸= 0, so we need to investigate when this can happen. It is easy to prove by induction on n that the solution of  22  by the Brent–Traub method entails consideration of exactly n subproblems, in which the coefficient of V  z  on the right-hand side takes  the respective values W z z U z j + O zn  for 0 ≤ j < n in some order. If  W 0  = U1 and if U1 is not a root of unity, we therefore have W 0  = 1 only when j = 1; the procedure will fail in this case only if  22  has no solution for n = 2.  Consequently the Schröder function for U can be found by solving  22  for n = 2, 4, 8, 16, . . . , with W z  = U1 and S z  = 0, whenever U1 is nonzero and not a root of unity.  If U1 = 1, there is no Schröder function unless U z  = z. But Brent and Traub have found a fast way to compute U [n] z  even when U1 = 1, by making use of a function V  z  such that  VP z  = P ′ z V  z   check that their composition U ˆU z  does too; therefore all iterates of U z  are   27  If two functions U z  and ˆU z  both satisfy  27 , for the same V, it is easy to solutions of  27 . Suppose we have U z  = z + Uk zk + Uk+1zk+1 + ··· where k ≥ 2 and Uk ̸= 0. Then it can be shown that there is a unique power series of the form V  z  = zk + Vk+1zk+1 + Vk+2zk+2 + ··· satisfying  27 . Conversely if such a function V  z  is given, and if k ≥ 2 and Uk are given, then there is a unique power series of the form U z  = z+Ukzk +Uk+1zk+1 +··· satisfying  27 . The desired iterate U [n] z  is the unique power series P z  satisfying   28  such that P z  = z + nUk zk + ··· . Both V  z  and P z  can be found by appropriate algorithms  see exercise 14 . If U1 is a kth root of unity, but not equal to 1, the same method can be applied to the function U [k] z  = z +··· , and U [k] z  can be found from U z  by doing l k  composition operations  see Section 4.6.3 . We can also handle the case U1 = 0: If U z  = Uk zk + Uk+1zk+1 +··· where k ≥ 2 and Uk ̸= 0, the idea  is to find a solution to the equation VU z  = UkV  z k; then V  z kn.   29  Finally, if U z  = U0 + U1z + ··· where U0 ̸= 0, let α be a “fixed point” such that U α  = α, and let  U [n] z  = V [−1]U  [ kn−1   k−1 ] k  ˆU z  = U α + z  − α = zU′ α  + z2U′′ α  2! + ··· ;   30    MANIPULATION OF POWER SERIES  4.7 533 then U [n] z  = ˆU [n] z−α +α. Further details can be found in Brent and Traub’s paper [SICOMP 9  1980 , 54–66]. The V function of  27  had previously been considered by M. Kuczma, Functional Equations in a Single Variable  Warsaw: PWN–Polish Scientific, 1968 , Lemma 9.4, and implicitly by E. Jabotinsky a few years earlier  see exercise 23 . Algebraic functions. The coefficients of each power series W z  that satisfies a general equation of the form  An z W z n + ··· + A1 z W z  + A0 z  = 0,   31  where each Ai z  is a polynomial, can be computed efficiently by using methods due to H. T. Kung and J. F. Traub; see JACM 25  1978 , 245–260. See also D. V. Chudnovsky and G. V. Chudnovsky, J. Complexity 2  1986 , 271–294; 3  1987 , 1–25. EXERCISES 1. [M10] The text explains how to divide U z  by V  z  when V0 ̸= 0; how should the division be done when V0 = 0? 2. [20] If the coefficients of U z  and V  z  are integers and V0 ̸= 0, find a recurrence relation for the integers V n+1 0 Wn, where Wn is defined by  3 . How could you use this for power series division? 3. [M15] Does formula  9  give the right results when α = 0? When α = 1?   cid:120  4. [HM23] Show that simple modifications of  9  can be used to calculate eV  z  when  cid:120  6. [M21]  H. T. Kung.  Apply Newton’s method to the computation of W  z  =  V0 = 0, and ln V  z  when V0 = 1. 5. [M00] What happens when a power series is reverted twice — that is, if the output of Algorithm L or T is reverted again?  1 V  z , when V  0  ̸= 0, by finding the power series root of the equation f x  = 0, where f x  = x−1 − V  z . 7. [M23] Use Lagrange’s inversion formula  12  to find a simple expression for the coefficient Wn in the reversion of z = t − tm.   cid:120  8. [M25] If W  z  = W1z + W2z2 + W3z3 + ··· = G1t + G2t2 + G3t3 + ··· = G t ,  where z = V1t + V2t2 + V3t3 + ··· and V1 ̸= 0, Lagrange proved that  Wn = 1  n  [tn−1] G′ t   V1 + V2t + V3t  2 + ···  n .   Equation  12  is the special case G1 = V1 = 1, G2 = G3 = ··· = 0.  Extend Algorithm L so that it obtains the coefficients W1, W2, . . . in this more general situation, without substantially increasing its running time. 9. [11] Find the values of Tmn computed by Algorithm T as it determines the first five coefficients in the reversion of z = t − t2. 10. [M20] Given that y = xα + a1xα+1 + a2xα+2 + ··· , α ̸= 0, show how to compute the coefficients in the expansion x = y1 α + b2y2 α + b3y3 α + ··· .   cid:120  11. [M25]  Composition of power series.  Let  U z  = U0 + U1z + U2z  2 + V3z Design an algorithm that computes the first N coefficients of U V  z  .  and V  z  = V1z + V2z  2 + ···  3 + ··· .   534  ARITHMETIC  4.7  12. [M20] Find a connection between polynomial division and power series division: Given polynomials u x  and v x  of respective degrees m and n over a field, show how to find the polynomials q x  and r x  such that u x  = q x v x + r x  and deg r  < n, using only operations on power series. 13. [M27]  Rational function approximation.  It is occasionally desirable to find polynomials whose quotient has the same initial terms as a given power series. For example, if W  z  = 1 + z + 3z2 + 7z3 + ··· , there are essentially four different ways to express W  z  as w1 z  w2 z  + O z4  where w1 z  and w2 z  are polynomials with deg w1  + deg w2  < 4:   1 + z + 3z  3 − 4z + 2z  2 + 7z 2     3 − 7z  = 1 + z + 3z2 + 7z3 + 49  3    1 = 1 + z + 3z2 + 7z3 + 0z4 + ··· , 3 z4 + ··· , 2  = 1 + z + 3z2 + 7z3 + 17z4 + ··· , 3  = 1 + z + 3z2 + 7z3 + 15z4 + ··· .   1 − z     1 − 2z − z 1    1 − z − 2z 2 − 2z  Rational functions of this kind are commonly called Padé approximations, since they were studied extensively by H. E. Padé [Annales Scient. de l’École Normale Supérieure  3  9  1892 , S1–S93;  3  16  1899 , 395–426].  Show that all Padé approximations W  z  = w1 z  w2 z  + O zN  with deg w1  + deg w2  < N can be obtained by applying an extended Euclidean algorithm to the polynomials zN and W0 + W1z +··· + WN−1zN−1; and design an all-integer algorithm  cid:120  14. [HM30] Fill in the details of Brent and Traub’s method for calculating U [n] z  for the case that each Wi is an integer. [Hint: See exercise 4.6.1–26.]  when U z  = z + Uk zk + ··· , using  27  and  28 . 15. [HM20] For what functions U z  does V  z  have the simple form zk in  27 ? What do you deduce about the iterates of U z ? 16. [HM21] Let W  z  = G t  as in exercise 8. The “obvious” way to find the is to proceed as follows: Set n ← 1 and R1 t  ← G t . coefficients W1, W2, W3, . . . Then preserve the relation WnV  t  + Wn+1V  t 2 + ··· = Rn t  by repeatedly setting Wn ← [t] Rn t  V1, Rn+1 t  ← Rn t  V  t  − Wn, n ← n + 1. Prove Lagrange’s formula of exercise 8 by showing that [tn−1] R′k+1 t  tn V  t n = 1   cid:120  17. [M20] Given the power series V  z  = V1z + V2z2 + V3z3 +··· , we define the power  n + 1[tn] R′k t  tn+1  for all n ≥ 1 and k ≥ 1.   V  t n+1  1 n  ,  Vn x + y  =  matrix of V as the infinite array of coefficients vnk = n! k! [zn]V  z k; the nth poweroid of V is then defined to be Vn x  = vn0 + vn1x + ··· + vnnxn. Prove that poweroids satisfy the general convolution law    n  by Eq. 1.2.9– 26 ; hence the poweroid When V  z  = ln 1  1 − z   we have vnk =n ez − 1 we have Vn x  =  l + m  k  n =  n xk and the formula is equivalent to  n   n − k   For example, when V  z  = z we have Vn x  = xn, and this is the binomial theorem.  Vn x  is xn, and the identity is the result proved in exercise 1.2.6–33. When V  z  =  Vk x Vn−k y  .    k  k  k  k  k  ,  m  l + m  k  l  m  k   4.7  MANIPULATION OF POWER SERIES  535  an identity we haven’t seen before. Several other triangular arrays of coefficients that arise in combinatorial mathematics and the analysis of algorithms also turn out to be the power matrices of power series.  18. [HM22] Continuing exercise 17, prove that poweroids also satisfy  xVn x + y  =  x + y      n − 1  k − 1  k  Vk x Vn−k y  .  [Hint: Consider the derivative of exV  z .] 19. [M25] Continuing exercise 17, express all the numbers vnk in terms of the numbers vn = vn1 = n! Vn of the first column, and find a simple recurrence by which all columns can be computed from the sequence v1, v2, . . . . Show in particular that if all the vn are integers, then all the vnk are integers. 20. [HM20] Continuing exercise 17, suppose we have W  z  = U V  z   and U0 = 0. Prove that the power matrix of W is the product of the power matrices of V and U:  wnk =  cid:120  21. [HM27] Continuing the previous exercises, suppose V1 ̸= 0 and let W  z  = well-known Stirling triangles vnk =n  −V [−1] −z . The purpose of this exercise is to show that the power matrices of V and W are “dual” to each other; for example, when V  z  = ln 1  1 − z   we have V [−1] z  = 1 − e−z, W  z  = ez − 1, and the corresponding power matrices are the  j vnjujk.  a  Prove that the inversion formulas 1.2.6– 47  for Stirling numbers hold in general:  , wnk =n . vnkwkm −1 n−k =  k  k  wnkvkm −1 n−k = δmn .    k  k  b  The relation vn n−k  = nk [zk]  V  z  z n−k shows that, for fixed k, the quantity  vn n−k  V n1 is a polynomial in n of degree ≤ 2k. We can therefore define  vα α−k  = αk [zk]  V  z  z α−k   cid:120  22. [HM27] Given U z  = U0+U1z+U2z2+··· with U0 ̸= 0, the αth induced function  for arbitrary α when k is a nonnegative integer, as we did for Stirling numbers in Section 1.2.6. Prove that v −k  −n  = wnk.  This generalizes Eq. 1.2.6– 58 .   U{α} z  is the power series V  z  defined implicitly by the equation  V  z  = U zV  z α  .  a  Prove that U{0} z  = U z  and U{α}{β} z  = U{α+β} z . b  Let B z  be the simple binomial series 1 + z. Where have we seen B{2} z  before? x+nα [zn] U z x+nα. Hint: If W  z  = z U z α, we c  Prove that [zn] U{α} z x = x have U{α} z  =  W [−1] z  z 1 α.  d  Consequently any poweroid Vn x  satisfies not only the identities of exercises 17  and 18, but also   x + y Vn x + y + nα   x + y + nα  Vn x + y  y − nα   n = =  x + y    xVk x + kα   n − 1  x + kα  k  k  k − 1  k  yVn−k y +  n − k α   Vk x + kα   ; Vn−k y − kα   y +  n − k α  y − kα  .  x + kα  [Special cases include Abel’s binomial theorem, Eq. 1.2.6– 16 ; Rothe’s identities 1.2.6– 26  and 1.2.6– 30 ; Torelli’s sum, exercise 1.2.6–34.]    n    k  n  k=2    536  ARITHMETIC  4.7  23. [HM35]  E. Jabotinsky.  Continuing in the same vein, suppose that U =  unk  is the power matrix of U z  = z + U2z2 + ··· . Let un = un1 = n! Un. a  Explain how to compute a matrix ln U so that the power matrix of U [α] z  is  exp α ln U  = I + α ln U +  α ln U 2 2! + ··· .  b  Let lnk be the entry in row n and column k of ln U, and let  Prove that lnk = n  ln = ln1,  L z  = l2  ln+1−k for 1 ≤ k ≤ n. [Hint: U [ϵ] z  = z + ϵL z  + O ϵ2 .]  z3 3! + l4  z4 4! + ··· .  z2 2! + l3  c  Considering U [α] z  as a function of both α and z, prove that  k−1  ∂ ∂α  U  [α] z  = L z  ∂ ∂z  [α] z  = L U  [α] z   .  U   Consequently L z  =  lk k! V  z , where V  z  is the function in  27  and  28 .   d  Show that if u2 ̸= 0, the numbers ln can be computed from the recurrence  l2 = u2,  lkun+1−k =  lkunk .  n  k=2  How would you use this recurrence when u2 = 0?  e  Prove the identity  n−1  m=0  n! m!  un =  where nj = 1 + k1 + ··· + kj − j.  k1+···+km=n+m−1  k1,...,km≥2  n0 k1!  n1 k2! . . .  nm−1 km!  lk1 lk2 . . . lkm ,  24. [HM25] Given the power series U z  = U1z + U2z2 + ··· , where U1 is not a root of unity, let U =  unk  be the power matrix of U z . a  Explain how to compute a matrix ln U so that the power matrix of U [α] z  is  exp α ln U  = I + α ln U +  α ln U 2 2! + ··· .  b  Show that if W  z  is not identically zero and if U W  z   = W U z  , then W  z  =  U [α] z  for some complex number α.  25. [M24] If U z  = z + Ukzk + Uk+1zk+1 +··· and V  z  = z + Vlzl + Vl+1zl+1 +··· , where k ≥ 2, l ≥ 2, Uk ̸= 0, Vl ̸= 0, and U V  z   = V  U z  , prove that we must have k = l and V  z  = U [α] z  for α = Vk Uk. 26. [M22] Show that if U z  = U0 + U1z + U2z2 + ··· and V  z  = V1z + V2z2 + ··· are power series with all coefficients 0 or 1, we can obtain the first N coefficients of U V  z   mod 2 in O N 1+ϵ  steps, for any ϵ > 0. 27. [M22]  D. Zeilberger.  Find a recurrence analogous to  9  for computing the coefficients of W  z  = V  z V  qz  . . . V  qm−1z , given q, m, and the coefficients of V  z  = 1 + V1z + V2z2 + ··· . Assume that q is not a root of unity.   cid:120  28. [HM26] A Dirichlet series is a sum of the form V  z  = V1 1z+V2 2z+V3 3z+··· ;  the product U z V  z  of two such series is the Dirichlet series W  z  where  Wn =  d\n  UdVn d .   4.7  MANIPULATION OF POWER SERIES  537  Ordinary power series are special cases of Dirichlet series, since we have V0 + V1z + V2z2 + V3z3 + ··· = V0 1s + V1 2s + V2 4s + V3 8s + ··· when z = 2−s. In fact, Dirichlet series are essentially equivalent to power series V  z1, z2, . . .   in arbitrarily many variables, where zk = p−s  k and pk is the kth prime number.  Find recurrence relations that generalize  9  and the formulas of exercise 4, assum- ing that a Dirichlet series V  z  is given and that we want to calculate  a  W  z  = V  z α when V1 = 1;  b  W  z  = exp V  z  when V1 = 0;  c  W  z  = ln V  z  when V1 = 1. [Hint: Let t n  be the total number of prime factors of n, including multiplicity, and let n t n Vn nz. Show that δ is analogous to a derivative; for example,  δ n Vn nz =  δeV  z  = eV  z δV  z .]  It seems impossible that any thing should really alter the series of things, without the same power which first produced them. — EDWARD STILLINGFLEET, Origines Sacræ, 2:3:2  1662   This business of series, the most disagreeable thing in mathematics, is no more than a game for the English; Stirling’s book, and the one by de Moivre, are proof. — PIERRE DE MAUPERTUIS, letter to d’Ortous de Mairan  30 Oct 1730   He was daunted and bewildered by their almost infinite series. — G. K. CHESTERTON, The Man Who Was Thursday  1907    ANSWERS TO EXERCISES  This branch of mathematics is the only one, I believe, in which good writers frequently get results entirely erroneous. It may be doubted if there is a single extensive treatise on probabilities in existence which does not contain solutions absolutely indefensible. — C. S. PEIRCE, in Popular Science Monthly  1878   . . .  NOTES ON THE EXERCISES 1. An average problem for a mathematically inclined reader. 3.  Solution by Roger Frye, after about 110 hours of computation on a Connection Machine in 1987.  958004 + 2175194 + 4145604 = 4224814 and  therefore  1916004 + 4350384 + 8291204 = 8449624. 4.  One of the readers of the preliminary manuscript for this book reported that he had found a truly remarkable proof. But unfortunately the margin of his copy was too small to contain it.   SECTION 3.1 1.  a  This will usually fail, since “round” telephone numbers are often selected by the telephone user when possible. In some communities, telephone numbers are perhaps assigned randomly. But it would be a mistake in any case to try to get several successive random numbers from the same page, since the same telephone number is often listed several times in a row.   b  But do you use the left-hand page or the right-hand page? Say, use the left- hand page number, divide by 2, and take the units digit. The total number of pages should be a multiple of 20; but even so, this method will have some bias.   c  The markings on the faces will slightly bias the die, but for practical purposes this method is quite satisfactory  and it has been used by the author in the preparation of several examples in this set of books . See Math. Comp. 15  1961 , 94–95, for further discussion of icosahedral dice.  digit 0 is selected with probability e−m   d   This is a hard question thrown in purposely as a surprise.  The number is not quite uniformly random. If the average number of emissions per minute is m, the probability that the counter registers k is e−mmk k!  the Poisson distribution ; so the k≥0 m10k  10k !, etc. In particular, the units 2 e−2m, and this is never equal  digit will be even with probability e−m cosh m = 1 to 1  2  although the error is negligibly small when m is large .  2 + 1  538   3.1  ANSWERS TO EXERCISES  539  It is almost legitimate to take ten readings  m0, . . . , m9  and then to output j if mj is strictly less than mi for all i ̸= j; try again if the minimum value appears more than once.  See  h .  However, the parameter m isn’t really constant in the real world.  e  Okay, provided that the time since the previous digit selected in this way is  random. However, there is possible bias in borderline cases.  10 of assigning   f, g  No. People usually think of certain digits  like 7  with higher probability.  h  Okay; your assignment of numbers to the horses had probability 1  a given digit to the winning horse  unless you know, say, the jockey . 2. The number of such sequences is the multinomial coefficient 1000000!  100000! 10; the probability is this number divided by 101000000, the total number of sequences of a million digits. By Stirling’s approximation we find that the probability is close to 1  16π41022√ 3. 3040504030. 4.  a  Step K11 can be entered only from step K10 or step K2, and in either case we find it impossible for X to be zero by a simple argument. If X could be zero at that point, the algorithm would not terminate.  2π  ≈ 2.56 × 10−26, roughly one chance in 4 × 1025.   b  If X is initially 3830951656, the computation is like many of the steps that appear in Table 1 except that we reach step K11 with Y = 3 instead of Y = 5; hence 3830951656 → 5870802097. Similarly, 5870802097 → 1226919902 → 3172562687 → 3319967479 → 6065038420 → 6065038420 → ··· . 5. Since only 1010 ten-digit numbers are possible, some value of X must be repeated during the first 1010+1 steps; and as soon as a value is repeated, the sequence continues to repeat its past behavior. 6.  a  Arguing as in the previous exercise, the sequence must eventually repeat a value; let this repetition occur for the first time at step µ + λ, where Xµ+λ = Xµ.  This condition defines µ and λ.  We have 0 ≤ µ < m, 0 < λ ≤ m, µ + λ ≤ m. The values µ = 0, λ = m are attained if and only if f is a cyclic permutation; and µ = m−1, λ = 1 occurs, e.g., if X0 = 0, f x  = x + 1 for x < m − 1, and f m − 1  = m − 1.  b  We have, for r > n, Xr = Xn if and only if r − n is a multiple of λ and n ≥ µ. Hence X2n = Xn if and only if n is a multiple of λ and n ≥ µ. The desired results now follow immediately. [Note: Equivalently, the powers of an element in a finite semigroup include a unique idempotent element: Take X1 = a, f x  = ax. See G. Frobenius, Sitzungsberichte preußische Akademie der Wissenschaften  1895 , 82–83.]  c  Once n has been found, generate Xi and Xn+i for i ≥ 0 until first finding Xi = Xn+i; then µ = i. If none of the values of Xn+i for 0 < i < µ is equal to Xn, it follows that λ = n, otherwise λ is the smallest such i. 7.  a  The least n > 0 such that n −  ℓ n  − 1  is a multiple of λ and ℓ n  − 1 ≥ µ is n = 2⌈lg max µ+1, λ ⌉ − 1 + λ. [This may be compared with the least n > 0 such that X2n = Xn, namely λ ⌈µ λ⌉ + δµ0 .]  b  Start with X = Y = X0, k = m = 1.  At key places in this algorithm we will have X = X2m−k−1, Y = Xm−1, and m = ℓ 2m − k .  To generate the next random number, do the following steps: Set X ← f X  and k ← k − 1. If X = Y, stop  the period length λ is equal to m − k . Otherwise if k = 0, set Y ← X, m ← 2m, k ← m. Output X. Notes: Brent has also considered a more general method in which the successive values of Y = Xni satisfy n1 = 0, ni+1 = 1 + ⌊pni⌋ where p is any number greater than 1. He showed that the best choice of p, approximately 2.4771, saves about 3 percent of the iterations by comparison with p = 2.  See exercise 4.5.4–4.    540  ANSWERS TO EXERCISES  3.1  The method in part  b  has a serious deficiency, however, since it might generate a lot of nonrandom numbers before shutting off. For example, we might have a particularly bad case such as λ = 1, µ = 2k. A method based on Floyd’s idea in exercise 6 b , namely one that maintains Y = X2n and X = Xn for n = 0, 1, 2, . . . , will require a few more function evaluations than Brent’s method, but it will stop before any number has been output twice.  On the other hand, if f is unknown  for example, if we are receiving the values X0, X1, . . . online from an outside source  or if f is difficult to apply, the following cycle detection algorithm due to R. W. Gosper will be preferable: Maintain an auxiliary table T0, T1, . . . , Tm, where m = ⌊lg n⌋ when receiving Xn. Initially T0 ← X0. For n = 1, 2, . . . , compare Xn with each of T0, . . . , T⌊lg n⌋ ; if no match is found, set Te n  ← Xn, where e n  = ρ n + 1  = max{e  2e divides n + 1}. But if a match Xn = Tk is found, then λ = n−max{ l  l < n and e l  = k}. After Xn has been stored in Te n , it is subsequently compared with Xn+1, Xn+2, . . . , Xn+2e n +1. Therefore the procedure stops immediately after generating Xµ+λ+j, where j ≥ 0 is minimum with e µ + j  ≥ ⌈lg λ⌉− 1. With this method, no X value is generated more than twice, and at most max 1, 2⌈lg λ⌉−1  values are generated more than once. [MIT AI Laboratory Memo 239  29 February 1972 , Hack 132.] R. Sedgewick, T. G. Szymanski, and A. C. Yao have analyzed a more complex algorithm based on parameters m ≥ 2 and g ≥ 1: An auxiliary table of size m contains X0, Xb, . . . , Xqb at the moment that Xn is computed, where b = 2⌈lg n m⌉ and q = ⌈n b⌉ − 1. If n mod gb < b, Xn is compared to the entries in the table; eventually equality occurs, and we can reconstruct µ and λ after doing at most  g +1 2⌈lg µ+λ ⌉+1 further evaluations of f. If the evaluation of f costs τ units of time, and if testing Xn for membership in the table costs σ units, then g can be chosen so that the total running time is  µ + λ  τ + O  στ m  1 2 ; this is optimum if σ τ = O m . Moreover, Xn is not computed unless µ+ λ > mn  m+4g +2 , so we can use this method “online” to output elements that are guaranteed to be distinct, making only 2+ O m−1 2  function evaluations per output. [SICOMP 11  1982 , 376–390.] 8.  a, b  00, 00, . . . [62 starting values]; 10, 10, . . . [19]; 60, 60, . . . [15]; 50, 50, . . . [1]; 24, 57, 24, 57, . . . [3].  c  42 or 69; these both lead to a set of fifteen distinct values, namely  42 or 69 , 76, 77, 92, 46, 11, 12, 14, 19, 36, 29, 84, 05, 02, 00. 9. Since X < bn, we have X2 < b2n, and the middle square is ⌊X2 bn⌋ ≤ X2 bn. If X > 0, then X2 bn < Xbn bn = X. 10. If X = abn, the next number of the sequence has the same form; it is equal to  a2 mod bn bn. If a is a multiple of all the prime factors of b, the sequence will soon degenerate to zero; if not, the sequence will degenerate into a cycle of numbers having the same general form as X.  Further facts about the middle-square method have been found by B. Jansson, Random Number Generators  Stockholm: Almqvist & Wiksell, 1966 , Section 3A. Numerologists will be interested to learn that the number 3792 is self-reproducing in the four-digit middle-square method, since 37922 = 14379264; furthermore  as Jansson observed , it is “self-reproducing” in another sense, too, since its prime factorization is 3 · 79 · 24! 11. The probability that µ = 0 and λ = 1 is the probability that X1 = X0, namely 1 m. The probability that  µ, λ  =  1, 1  or that  µ, λ  =  0, 2  is the probability that X1 ̸= X0 and that X2 has a certain value, so it is  1 − 1 m  1 m . Similarly, the   3.1  ANSWERS TO EXERCISES  541      .  .  2  m  m  m  m  m  m  k=1  + 6  µ≥0  1 m              1 + 3  + ···  Q m ,  = 1  n≥0 an  1 − 1  1 − 1  1≤k<µ+λ  1≤λ≤m 0≤µ<m  1 − k m  1 − k m  λP  µ, λ  = 1  For the probability that λ = 1, we have  P  µ, λ  = 1 µ   probability that the sequence has any given µ and λ is a function only of µ+ λ, namely  where Q m  is defined in Section 1.2.11.3, Eq.  2 . By Eq.  25  in that section, √ m. The chance of Algorithm K converging as it did is only about one in 80000; the author was decidedly unlucky. But see exercise 15 for further comments on the “colossalness.”    the probability is approximately π 2m ≈ 1.25    12.  1 − 2  See the previous answer. In general if f a0, a1, . . .   = µ + 1  is approximately πm 8 + 1 approximatelyπm 2 − 1  n k=1 1 − k m  then f a0, a1, . . .   = a0 + f a1, a2, . . .   − f a1, 2a2, . . .   m; apply this identity with an =  n + 1  2.  Therefore the average value of λ  and, by symmetry of P  µ, λ , also of 3. The average value of µ + λ is exactly Q m , 3. [For alternative derivations and further results, including asymptotic values for the moments, see A. Rapoport, Bull. Math. Biophysics 10  1948 , 145–157, and B. Harris, Annals Math. Stat. 31  1960 , 1045–1062; see also I. M. Sobol, Theory of Probability and Its Applications 9  1964 , 333–338. Sobol discusses the asymptotic period length for the more general sequence Xn+1 = f Xn  if n ̸≡ 0  modulo m , Xn+1 = g Xn  if n ≡ 0  modulo m , with both f and g random.] 13. [Paul Purdom and John Williams, Trans. Amer. Math. Soc. 133  1968 , 547–551.] Let Tmn be the number of functions that have n one-cycles and no cycles of length greater than one. Then  = 1 + Q m    This ism by a permutation of the n elements that were the one-cycles. Hence  n≥1 TmnPnk. To get the average value of k, we compute  by the result of exercise 1.3.3–23 is  r m, m−n  in exercise 2.3.4.4–25.  Any function is such a function followed n≥1 Tmn n! = Let Pnk be the number of permutations of n elements in which the longest cycle is of length k. Then the number of functions with a maximum cycle of length k is n≥1 kTmnPnk, which 2 c + O n−1   where c ≈ .62433. 2 c + O m1 2 .  This is not substantially Summing, we get the average value cQ m  + 1 larger than the average value when X0 is selected at random. The average value of max µ is asymptotic to Q m  ln 4, and the average value of max µ + λ  is asymptotic to 1.9268Q m ; see Flajolet and Odlyzko, Lecture Notes in Comp. Sci. 434  1990 , 329–354.  14. Let cr m  be the number of functions with exactly r different final cycles. From by counting the number of functions whose image contains at most m − k elements, we find the solution c1 m  = mm−1Q m .  See exercise 1.2.11.3–16.  Another way   −1 k m − k kc1 m − k , which comes  the recurrence c1 m  =  m − 1 ! −  n≥1 Tmn n! cn + 1   m − 1  m  Tmn =  n − 1    mm−n.  mm.    k≥1  k>0  n  k   Em = 1   = 1 + 1 2  m  m  H1 + 2H2 m − 1 + 1 3  ∞  542  ANSWERS TO EXERCISES  3.1  to obtain the value of c1 m , which is perhaps more elegant and revealing, is given in exercise 2.3.4.4–17. The value of cr m  may be determined as in exercise 13: m − 2   m − 1   1   n  2  1  3        cr m  =  + ···  .  = mm−1  + 1 1!  0!  r  r  m  + 1 2!  r  m  Tmn  r  n≥1  The desired average value can now be computed; it is  see exercise 12    m − 1   m  + ···  m − 1  m − 1  m − 2  + 3H3 m − 2  m m − 1  m  m  m  m + ··· .  ,  0  m  Em =  e−x dx x  2 ln m  = 1  1 + x  This latter formula was obtained by quite different means by Martin D. Kruskal, AMM 61  1954 , 392–397. Using the integral representation  m − 1 he proved the asymptotic relation limm→∞ Em − 1 2 γ + ln 2 . For further results and references, see John Riordan, Annals Math. Stat. 33  1962 , 178–185. 15. The probability that f x  ̸= x for all x is  m−1 m mm, which is approximately 1 e. The existence of a self-repeating value in an algorithm like Algorithm K is therefore not “colossal” at all — it occurs with probability 1− 1 e ≈ .63212. The only “colossal” thing was that the author happened to hit such a value when X0 was chosen at random  see exercise 11 . 16. The sequence will repeat when a pair of successive elements occurs for the second time. The maximum period is m2.  See the next exercise.  17. After selecting X0, . . . , Xk−1 arbitrarily, let Xn+1 = f Xn, . . . , Xn−k+1 , where 0 ≤ x1, . . . , xk < m implies that 0 ≤ f x1, . . . , xk  < m. The maximum period is mk. This is an obvious upper bound, but it is not obvious that it can be attained; for constructive proofs that it can always be attained for suitable f, see exercises 3.2.2–17 and 3.2.2–21, and for the number of ways to attain it see exercise 2.3.4.2–23. 18. Same as exercise 7, but use the k-tuple of elements  Xn, . . . , Xn−k+1  in place of the single element Xn. 19. Clearly Pr no final cycle has length 1  =  m−1 m mm. R. Pemantle [J. Algorithms 54  2005 , 72–84] has shown that Pr λ = 1  = Θ mk 2 , and that Pr  µ + λ 2 > 2mkx and λ  µ + λ  ≤ y  rapidly approaches ye−x, when x > 0, 0 < y < 1, and m → ∞. The k-dimensional analogs of exercises 13 and 14 remain unsolved. 20. It suffices to consider the simpler mapping g X  defined by steps K2–K13. Work- ing backward from 6065038420, we obtain a total of 597 solutions; the smallest is 0009612809 and the largest is 9995371004. 21. We may work with g X  as in the previous exercise, but now we want to run the function forward instead of backward. There is an interesting tradeoff between time and space. Notice that the mechanism of step K1 tends to make the period length small. So does the existence of X’s with large in-degree; for example, 512 choices of X = ∗6∗∗∗∗∗∗∗∗ in step K2 will go to K10 with X ← 0500000000. Scott Fluhrer has discovered another fixed point of Algorithm K, namely the value 5008502835 ! . He also found the 3-cycle 0225923640 → 2811514413 → 0590051662 →   3.2.1.1  ANSWERS TO EXERCISES  543  0225923640, making a total of seven cycles in all. Only 128 starting numbers lead to the repeating value 5008502835. Algorithm K is a terrible random number generator. 22. If f were truly random, this would be ideal; but how do we construct such f? The function defined by Algorithm K would work much better under this scheme, although it does have decidedly nonrandom properties  see the previous answer . 23. The function f permutes its cyclic elements; let  x0, . . . , xk−1  be the “unusual” representation of the inverse of that permutation. Then proceed to define xk, . . . , xm−1 as in exercise 2.3.4.4–18. [See J. Combinatorial Theory 8  1970 , 361–375.] For example, if m = 10 and  f 0 , . . . , f 9   =  3, 1, 4, 1, 5, 9, 2, 6, 5, 4 , we have  x0, . . . , x9  =  4, 9, 5, 1, 1, 3, 4, 2, 6, 5 ; if  x0, . . . , x9  =  3, 1, 4, 1, 5, 9, 2, 6, 5, 4 , we have  f 0 , . . . , f 9   =  6, 4, 9, 3, 1, 1, 2, 5, 4, 5 .  SECTION 3.2.1 1. Take X0 even, a even, c odd. Then Xn is odd for n > 0. 2. Let Xr be the first repeated value in the sequence. If Xr were equal to Xk for some k where 0 < k < r, we could prove that Xr−1 = Xk−1, since Xn uniquely determines Xn−1 when a is prime to m. Hence k = 0. 3. If d is the greatest common divisor of a and m, the quantity aXn can take on at most m d values. The situation can be even worse; for example, if m = 2e and if a is even, Eq.  6  shows that the sequence is eventually constant. 4. Induction on k. 5. If a is relatively prime to m, there is a number a′ for which aa′ ≡ 1  modulo m . Then Xn−1 =  a′Xn − a′c  mod m; and in general, if b = a − 1,  mod m Xn−k =   a′ kXn − c a′ + ··· +  a′ k   mod m  = a′ kXn +   a′ k − 1 c b  when k ≥ 0, n− k ≥ 0. If a is not relatively prime to m, it is not possible to determine Xn−1 when Xn is given; multiples of m gcd a, m  may be added to Xn−1 without changing the value of Xn.  See also exercise 3.2.1.3–7.   SECTION 3.2.1.1 1. Let c′ be a solution to the congruence ac′ ≡ c  modulo m .  Thus, c′ = a′c mod m, if a′ is the number in the answer to exercise 3.2.1–5.  Then we have  LDA X;  ADD CPRIME; MUL A. Overflow is possible on this addition operation.  From results derived later in the chapter, it is probably best to save a unit of time, taking c = a and replacing the ADD instruction by ‘INCA 1’. Then if X0 = 0, overflow will not occur until the end of the period, so it won’t occur in practice.  2. RANDM STJ 1F XRAND LDA MUL 2F SLAX 5 3F ADD STA XRAND  *-1 XRAND CON X0 2H 3H   or, INCA c, if c is small   CON a CON c  JNOV * JMP  1H   544  ANSWERS TO EXERCISES  3.2.1.1  3. Let a′ = aw mod m, and let m′ be such that mm′ ≡ 1  modulo w . Set y ← lomult a′, x , z ← himult a′, x , t ← lomult m′, y , u ← himult m, t . Then we have mt ≡ a′x  modulo w , hence a′x − mt =  z − u w, hence ax ≡ z − u  modulo m ; it follows that ax mod m = z − u + [z < u]m. 4. Define the operation x mod 2e = y if and only if x ≡ y  modulo 2e  and −2e−1 ≤ y < 2e−1. The congruential sequence ⟨Yn⟩ defined by  Y0 = X0 mod 232  ,  Yn+1 =  aYn + c  mod 232  is easy to compute on 370-style machines, since the lower half of the product of y and z is  yz  mod 232 for all two’s complement numbers y and z, and since addition ignoring overflow also delivers its result mod 232. This sequence has all the random- ness properties of the standard linear congruential sequence ⟨Xn⟩, since Yn ≡ Xn  modulo 232 . Indeed, the two’s complement representation of Yn is identical to the binary representation of Xn, for all n. [G. Marsaglia and T. A. Bray first pointed this out in CACM 11  1968 , 757–759.] 5.  a  Subtraction: LDA X; SUB Y; JANN *+2; ADD M.  b  Addition: LDA X; SUB M; ADD Y; JANN *+2; ADD M.  Note that if m is more than half the word size, the instruction ‘SUB M’ must precede the instruction ‘ADD Y’.  6. The sequences are not essentially different, since adding the constant  m − c  has the same effect as subtracting the constant c. The operation must be combined with multiplication, so a subtractive process has little merit over the additive one  at least in MIX’s case , except when it is necessary to avoid affecting the overflow toggle. 7. The prime factors of zk − 1 appear in the factorization of zkr − 1. If r is odd, the prime factors of zk + 1 appear in the factorization of zkr + 1. And z2k − 1 equals  zk − 1  zk + 1 . 8. JOV *+1   Ensure that overflow is off.   LDA X MUL A STX TEMP ADD TEMP Add lower half to upper half. JNOV *+2 INCA 1  If ≥ w, subtract w − 1.  Overflow is impossible in this step.   Note: Since addition on an e-bit ones’-complement computer is mod  2e − 1 , it is possible to combine the techniques of exercises 4 and 8, producing  yz  mod  2e − 1  by adding together the two e-bit halves of the product yz, for all ones’ complement numbers y and z regardless of sign. 9.  a  Both sides equal aq⌊x q⌋.  b  Set t ← a x mod q  − r⌊x q⌋, where r = m mod a; the constants q and r can be precomputed. Then ax mod m = t + [t   −m: Clearly a x mod q  ≤ a q − 1  < m. Also r⌊x q⌋ ≤ r⌊ m − 1  q⌋ = r⌊a +  r − 1  q⌋ = ra ≤ qa < m if 0 < r ≤ q; and a2 ≤ m implies r < a ≤ q. [This technique is implicit in a program published by B. A. Wichmann and I. D. Hill, Applied Stat. 31  1982 , 190.] 10. If r > q and x = m−1 we have r⌊x q⌋ ≥  q+1  a+1  > m. So the condition r ≤ q is necessary and sufficient for method 9 b  to be valid; this means m q . Let t = ⌊√ q ] are disjoint for 1 ≤ q ≤ t, and they include exactly 1 or 2 integers, depending on whether q is a divisor of m. These intervals account for  m⌋. The intervals [ m  q − 1 ≤ a ≤ m  q −1 . . m   3.2.1.1  ANSWERS TO EXERCISES  545  m mod 1  < 1  √ √ m; they also include the case a = t, if   √ m mod 1  < 1  all solutions with a > 2, and the case a = t − 1 if m = t2. Thus the total number of “lucky” multipliers is exactly 2⌊√ 2 ]−1, where d m  is the number of divisors of m. m⌋+⌊d m  2⌋−[  11. We can assume that a ≤ 1 2 m; otherwise we can obtain ax mod m from  m − a x mod m. Then we can represent a = a′a′′ − a′′′, where a′, a′′, and a′′′ are all less than √ m; for example, we can take a′ ≈ √ m− 1 and a′′ = ⌈a a′⌉. It follows that ax mod m is  a′ a′′x mod m  mod m −  a′′′x mod m   mod m, and the inner three operations can all be handled by exercise 9. When m = 231 − 1 we can take advantage of the fact that m − 1 has 192 divisors to find cases in which m = q′a′ + 1, simplifying the general method because r′ = 1. It turns out that 86 of these divisors lead to lucky a′′ and a′′′, when a = 62089911; the best such case is probably a′ = 3641, a′′ = 17053, a′′′ = 62, because 3641 and 62 both divide m − 1. This decomposition yields the scheme  t ← 17053 x mod 125929  − 16410⌊x 125929⌋ , t ← 3641 t mod 589806  − ⌊t 589806⌋ , t ← t −  62 x mod 34636833  − ⌊x 34636833⌋  ,  where “−” denotes subtraction mod m. The mod operations count as one multiplication and one subtraction, because x mod q = x−q⌊x q⌋ and the operation ⌊x q⌋ has already been done; thus, we have performed seven multiplications, three divisions, and seven subtractions. But it’s even better to notice that 62089911 itself has 24 divisors; they lead to five suitable factorizations with a′′′ = 0. For example, when a′ = 883 and a′′ = 70317 we need only six multiplications, two divisions, four subtractions:  t ← 883 x mod 2432031  − 274⌊x 2432031⌋ , t ← 70317 t mod 30540  − 2467⌊t 30540⌋ .  [Can the worst-case number of multiplications plus divisions be reduced to at most 11, for all a and m, or is 12 the best upper bound? Another way to achieve 12 appears in exercise 4.3.3–19.] 12.  a  Let m = 9999998999 = 1010 − 103 − 1. To multiply  x9x8 . . . x0 10 by 10 modulo m, use the fact that 1010x9 ≡ 103x9 + x9: Add  x9000 10 to  x8x7 . . . x0x9 10. And to avoid circular shifting, imagine that the digits are arranged on a wheel: Just add the high-order digit x9 to the digit x2 three positions left, and point to x8 as If x9 + x2 ≥ 10, a carry propagates to the left. And if the new high-order digit. this carry ripples all the way to the left of x8, it propagates not only to x9 but also to the x2 position; it may continue to propagate from both x9 and x2 before finally settling down.  The numbers might also become slightly larger than m. For example, 0999999900 goes to 9999999000 = m + 1, which goes to 9999999009 = m + 10. But a redundant representation isn’t necessarily harmful.    b  This is the operation of dividing by 10, so we do the opposite of  a : Move the high-order digit pointer cyclically left, and subtract the new high-order digit from the digit three to its left. If the result of subtraction is negative, “borrow” in the usual fashion  Algorithm 4.3.1S ; that is, decrease the preceding digit by 1. Borrowing may propagate as in  a , but never past the high-order digit position. This operation keeps the numbers nonnegative and less than m.  Thus, division by 10 turns out to be easier than multiplication by 10.    c  We can remember the borrow-bit instead of propagating it, because it can be incorporated into the subtraction on the next step. Thus, if we define digits xn and   546  ANSWERS TO EXERCISES  borrow-bits bn by the recurrence  3.2.1.1  xn =  xn−10 − xn−3 − bn  mod 10 = xn−10 − xn−3 − bn + 10bn+1,  we have 999999900n mod 9999998999 = Xn by induction on n, where  Xn =  xn−1xn−2xn−3xn−4xn−5xn−6xn−7xn+2xn+1xn 10 − 1000bn+3  =  xn−1xn−2 . . . xn−10 10 −  xn−1xn−2xn−3 10 − bn,  provided that the initial conditions are set up to make X0 = 1. Notice that 10Xn+1 =  xnxn−1xn−2xn−3xn−4xn−5xn−6xn+3xn+2xn+10 10−10000bn+4 = mxn+Xn; it follows that 0 ≤ Xn < m for all n ≥ 0.  d  If 0 ≤ U < m, the first digit of the decimal representation of U m is ⌊10U m⌋, and the subsequent digits are the decimal representation of  10U mod m  m; see, for example, Method 2a in Section 4.4. Thus U m =  .u1u2 . . .  10 if we set U0 = U and Un = 10Un−1 mod m = 10Un−1 − mun. Informally, the digits of 1 m are the leading digits of 10n mod m for n = 1, 2, . . . , a sequence that is eventually periodic; these are the leading digits of 10−n mod m in reverse order, so we have calculated them in  c . A rigorous proof is, of course, preferable to handwaving. Let λ be the least positive integer with 10λ ≡ 1  modulo m , and define xn = xn mod λ, bn = bn mod λ, Xn = Xn mod λ for all n < 0. Then the recurrences for xn, bn, and Xn in  c  are valid for all integers n. If U0 = 1 it follows that Un = X−n and un = x−n; hence  999999900n mod 9999998999  9999998999  =  .xn−1xn−2xn−3 . . .  10.   e  Let w be the computer’s word size, and use the recurrence  xn =  xn−k − xn−l − bn  mod w = xn−k − xn−l − bn + wbn+1,  where 0 < l < k and k is large. Then  .xn−1xn−2xn−3 . . .  w = Xn m, where m = wk − wl − 1 and Xn+1 =  wk−1 − wl−1 Xn mod m. The relation Xn =  xn−1 . . . xn−k w −  xn−1 . . . xn−l w − bn  holds for n ≥ 0; the values of x−1, . . . , x−k, and b0 should be such that 0 ≤ X0 < m. Such random number generators, and the similar ones in the following exercise, were introduced by G. Marsaglia and A. Zaman [Annals of Applied Probability 1  1991 , 462–480], who called the method subtract-with-borrow. Their starting point was the radix-w representation of fractions with denominator m. The relation to linear congruential sequences was noticed by Shu Tezuka, and analyzed in detail by Tezuka, L’Ecuyer, and Couture [ACM Trans. Modeling and Computer Simulation 3  1993 , 315–331]. The period length is discussed in exercise 3.2.1.2–22. 13. Multiplication by 10 now requires negating the digit that is added. For this purpose it is convenient to represent a number with its last three digits negated; for example, 9876543210 =  9876544¯7¯9¯0 10. Then 10 times  x9 . . . x3¯x2¯x1¯x0 10 is  x8 . . . x3x′¯x1¯x0¯x9 10 where x′ = x9 − x2. Similarly,  x9 . . . x3¯x2¯x1¯x0 10 divided by 10 is  x0x9 . . . x4¯x′′¯x2¯x1 10 where x′′ = x0 − x3. The recurrence  xn =  xn−3 − xn−10 − bn  mod 10 = xn−3 − xn−10 − bn + 10bn+1  yields 8999999101n mod 9999999001 = Xn where  Xn =  xn−1xn−2xn−3xn−4xn−5xn−6xn−7¯xn+2¯xn+1¯xn 10 + 1000bn+3  =  xn−1xn−2 . . . xn−10 10 −  xn−1xn−2xn−3 10 + bn.   3.2.1.2  ANSWERS TO EXERCISES  547  When the radix is generalized from 10 to w, we find that the inverse powers of w  modulo wk − wl + 1 are generated by  xn =  xn−l − xn−k − bn  mod w = xn−l − xn−k − bn + wbn+1   the same as in exercise 12 but with k and l interchanged . 14. Mild generalization: We can effectively divide by b modulo bk − bl ± 1 for any b less than or equal to the word size w, since the recurrence for xn is almost as efficient when b < w as it is when b = w.   Strong generalization: The recurrence xn =  a1xn−1 + ··· + akxn−k + cn  mod b, is equivalent to Xn = b−1Xn−1 mod m in the sense that Xn m =  .xn−1xn−2 . . .  b, if we define   a1xn−1 + ··· + akxn−k + cn  cn+1 =  b  m = akbk + ··· + a1b − 1  and  Xn =  aj xn−1 . . . xn−j b + cn   sign m  .  The initial values x−1 . . . x−k and c0 should be selected so that 0 ≤ X0 < m; we will then have xn =  bXn+1 − Xn  m for n ≥ 0. The values of xj for j < 0 that appear in the formula Xn m =  .xn−1xn−2 . . .  b are properly regarded as xj mod λ, where bλ ≡ 1  modulo m ; these values may differ from the numbers x−1, . . . , x−k that were initially supplied. The carry digits cn will satisfy    k  j=1  min 0, aj  ≤ cn <  max 0, aj    k  j=1  k  j=1  if the initial carry c0 is in this range.  The special case m = bk + bl − 1, for which aj = δjl + δjk, is of particular interest because it can be computed so easily; Marsaglia and Zaman called this the add-with- carry generator:  xn =  xn−l + xn−k + cn  mod b = xn−l + xn−k + cn − b cn+1.  Another potentially attractive possibility is to use k = 2 in a generator with, say, b = 231 and m = 65430b2 + b − 1. This modulus m is prime, and the period length turns out to be  m− 1  2. The spectral test of Section 3.3.4 indicates that the spacing between planes is good  large ν values , although of course the multiplier b−1 is poor in comparison with other multipliers for this particular modulus m.  Exercise 3.2.1.2–22 contains additional information about subtract-with-borrow  and add-with-carry moduli that lead to extremely long periods.  SECTION 3.2.1.2 1. Period length m, by Theorem A.  See exercise 3.  2. Yes, these conditions imply the conditions in Theorem A, since the only prime divisor of 2e is 2, and any odd number is relatively prime to 2e.  In fact, the conditions of the exercise are necessary and sufficient.  3. By Theorem A, we need a ≡ 1  modulo 4  and a ≡ 1  modulo 5 . By Law D of Section 1.2.4, this is equivalent to a ≡ 1  modulo 20 .   ANSWERS TO EXERCISES  548 3.2.1.2 4. We know X2e−1 ≡ 0  modulo 2e−1  by using Theorem A in the case m = 2e−1. Also using Theorem A for m = 2e, we know that X2e−1 ̸≡ 0  modulo 2e . It follows that X2e−1 = 2e−1. More generally, we can use Eq. 3.2.1– 6  to prove that the second half of the period is essentially like the first half, since Xn+2e−1 =  Xn + 2e−1  mod 2e.  The quarters are similar too, see exercise 21.  5. We need a ≡ 1  modulo p  for p = 3, 11, 43, 281, 86171. By Law D of Section 1.2.4, this is equivalent to a ≡ 1  modulo 3 · 11 · 43 · 281 · 86171 , so the only solution is the terrible multiplier a = 1. 6.  See the previous exercise.  The congruence a ≡ 1  modulo 3 · 7 · 11 · 13 · 37  implies that the solutions are a = 1 + 111111k, for 0 ≤ k ≤ 8. 7. Using the notation of the proof of Lemma Q, µ is the smallest value such that Xµ+λ = Xµ; so it is the smallest value such that Yµ+λ = Yµ and Zµ+λ = Zµ. This shows that µ = max µ1, . . . , µt . The highest achievable µ is max e1, . . . , et , but nobody really wants to achieve it. 8. We have a2 ≡ 1  modulo 8 ; so a4 ≡ 1  modulo 16 , a8 ≡ 1  modulo 32 , etc. If a mod 4 = 3, then a− 1 is twice an odd number; so  a2e−1− 1   a− 1  ≡ 0  modulo 2e  if and only if  a2e−1− 1  2 ≡ 0  modulo 2e+1 2 , which is true. 9. Substitute for Xn in terms of Yn and simplify. If X0 mod 4 = 3, the formulas of the exercise do not apply; but they do apply to the sequence Zn =  −Xn  mod 2e, which has essentially the same behavior. 10. Only when m = 1, 2, 4, pe, and 2pe, for odd primes p. In all other cases, the result of Theorem B is an improvement over Euler’s theorem  exercise 1.2.4–28 . 11.  a  Either x+1 or x−1  not both  will be a multiple of 4, so x∓1 = q2f, where q is odd and f is greater than 1.  b  In the given circumstances, f < e and so e ≥ 3. We have ±x ≡ 1  modulo 2f  and ±x ̸≡ 1  modulo 2f+1  and f > 1. Hence, by applying Lemma P, we find that  ±x 2e−f−1 ̸≡ 1  modulo 2e , while x2e−f =  ±x 2e−f ≡ 1  modulo 2e . So the order is a divisor of 2e−f, but not a divisor of 2e−f−1.  c  1 has order 1; 2e − 1 has order 2; the maximum period when e ≥ 3 is therefore 2e−2, and for e ≥ 4 it is necessary to have f = 2, that is, x ≡ 4 ± 1  modulo 8 . 12. If k is a proper divisor of p − 1 and if ak ≡ 1  modulo p , then by Lemma P we have akpe−1 ≡ 1  modulo pe . Similarly, if ap−1 ≡ 1  modulo p2 , we find that a p−1 pe−2 ≡ 1  modulo pe . So in these cases a is not primitive. Conversely, if ap−1 ̸≡ 1  modulo p2 , Theorem 1.2.4F and Lemma P tell us that a p−1 pe−2 ̸≡ 1  modulo pe , but a p−1 pe−1 ≡ 1  modulo pe . So the order is a divisor of  p − 1 pe−1 but not of  p−1 pe−2; it therefore has the form kpe−1, where k divides p−1. But if a is primitive modulo p, the congruence akpe−1 ≡ ak ≡ 1  modulo p  implies that k = p − 1. 13. Suppose a mod p ̸= 0, and let λ be the order of a modulo p. By Theorem 1.2.4F, λ is a divisor of p − 1. If λ < p − 1, then  p − 1  λ has a prime factor, q. 14. Let 0 < k < p. If ap−1 ≡ 1  modulo p2 , then  a + kp p−1 ≡ ap−1 +  p − 1 ap−2kp  modulo p2 ; and this is ̸≡ 1, since  p − 1 ap−2k is not a multiple of p. By exercise 12, a + kp is primitive modulo pe. 15.  a  If λ1 = pe1 where  t and κ2 = ph1  t , let κ1 = pg1  t , 1 . . . pht  1 . . . pgt  1 . . . pet  t and λ2 = pf1 gj = ej gj = 0  1 . . . pft and hj = 0, and hj = fj,  if if  ej < fj, ej ≥ fj.   3.2.1.2  ANSWERS TO EXERCISES  549  1 and aκ2  Now aκ1 2 have periods λ1 κ1 and λ2 κ2, and the latter are relatively prime. Furthermore  λ1 κ1  λ2 κ2  = λ, so it suffices to consider the case when λ1 is relatively prime to λ2, that is, when λ = λ1λ2. Now let λ′ be the order of a1a2. Since  a1a2 λ′ ≡ 1, we have 1 ≡  a1a2 λ′λ1 ≡ aλ′λ1 ; hence λ′λ1 is a multiple of λ2. This implies that λ′ is a multiple of λ2, since λ1 is relatively prime to λ2. Similarly, λ′ is a multiple of λ1; hence λ′ is a multiple of λ1λ2. But obviously  a1a2 λ1λ2 ≡ 1, so λ′ = λ1λ2.   b  If a1 has order λ m  and if a2 has order λ, by part  a  λ m  must be a multiple of λ, otherwise we could find an element of higher order, namely of order lcm λ, λ m  . 16.  a  f x  =  x − a  xn−1 +  a + c1 xn−2 + ··· +  an−1 + ··· + cn−1   + f a .  b  The statement is clear when n = 0. If a is one root, f x  ≡  x − a q x ; therefore, if a′ is any other root,  2  0 ≡ f a′  ≡  a′ − a q a′ ,  and since a′ − a is not a multiple of p, a′ must be a root of q x . So if f x  has more than n distinct roots, q x  has more than n − 1 distinct roots. [J. L. Lagrange, Mém. Acad. Roy. Sci. Berlin 24  1768 , 181–250, §10.]  c  λ p  ≥ p− 1, since f x  must have degree ≥ p− 1 in order to possess so many roots. But λ p  ≤ p− 1 by Theorem 1.2.4F. 17. By Lemma P, 115 ≡ 1  modulo 25 , 115 ̸≡ 1  modulo 125 , etc.; so the order of 11 is 5e−1  modulo 5e , not the maximum value λ 5e  = 4 · 5e−1. But by Lemma Q the total period length is the least common multiple of the period modulo 2e  namely 2e−2  and the period modulo 5e  namely 5e−1 , and this is 2e−25e−1 = λ 10e . The period modulo 5e may be 5e−1 or 2 · 5e−1 or 4 · 5e−1, without affecting the length of period modulo 10e, since the least common multiple is taken. The values that are primitive modulo 5e are those congruent to 2, 3, 8, 12, 13, 17, 22, 23 modulo 25  see exercise 12 , namely 3, 13, 27, 37, 53, 67, 77, 83, 117, 123, 133, 147, 163, 173, 187, 197. 18. According to Theorem C, a mod 8 must be 3 or 5. Knowing the period of a modulo 5 and modulo 25 allows us to apply Lemma P to determine admissible values of a mod 25. Period = 4 · 5e−1: 2, 3, 8, 12, 13, 17, 22, 23; period = 2 · 5e−1: 4, 9, 14, 19; period = 5e−1: 6, 11, 16, 21. Each of these 16 values yields one value of a, 0 ≤ a < 200, with a mod 8 = 3, and another value of a with a mod 8 = 5. 19. Several examples appear in lines 17–20 of Table 3.3.4–1. 20.  a  We have AYn + X0 ≡ AYn+k + X0  modulo m  if and only if Yn ≡ Yn+k  modulo m′ .  b  i  Obvious.  ii  Theorem A.  iii   an − 1   a − 1  ≡ 0  modulo 2e  if and only if an ≡ 1  modulo 2e+1 ; if a ̸≡ −1, the order of a modulo 2e+1 is twice its order modulo 2e.  iv   an − 1   a − 1  ≡ 0  modulo pe  if and only if an ≡ 1. 21. Xn+s ≡ Xn + Xs by Eq. 3.2.1– 6 ; and s is a divisor of m, since s is a power of p when m is a power of p. Hence a given integer q is a multiple of m s if and only if Xqs ≡ 0, if and only if q is a multiple of m gcd Xs, m . 22. Algorithm 4.5.4P is able to test numbers of the form m = bk±bl±1 for primality in a reasonable time when, say, b ≈ 232 and l < k ≈ 100; the calculations should be done in radix b so that the special form of m speeds up the operation of squaring mod m.  Consider, for example, squaring mod 9999998999 in decimal notation.  Algorithm 4.5.4P should, of course, be used only when m is known to have no small divisors. Marsaglia and Zaman [Annals of Applied Probability 1  1991 , 474–475] showed that m = b43−b22+1 is prime with primitive root b when b is the prime number 232−5. This required factoring m−1 = b22 b−1  b6+b5+b4+b3+b2+b+1  b14+b7+1  in order to establish the primitivity of b; one of the 17 prime factors of m − 1 has 99 decimal digits. As a result, we can be sure that the sequence xn =  xn−22−xn−43−cn  mod b =   ANSWERS TO EXERCISES  550 3.2.1.2 xn−22 − xn−43 − cn + bcn+1 has period length m − 1 ≈ 10414 for every nonzero choice of seed values 0 ≤ x−1, . . . , x−43 < b when c0 = 0. However, 43 is still a rather small value for k from the standpoint of the birthday spacings test  see Section 3.3.2J , and 22 is rather near 43 2. Considerations of “mixing” indicate that we prefer values of k and l for which the first few partial quotients in the continued fraction of l k are small. To avoid potential problems with this generator, it’s a good idea to discard some of the numbers, as recommended by Lüscher  see Section 3.2.2 . Here are some prime numbers of the form bk ± bl ± 1 that satisfy the mixing constraint when b = 232 and 50 < k ≤ 100: For subtract-with-borrow, b57 − b17 − 1, b73 − b17 − 1, b86 − b62 − 1, b88 − b52 − 1, b95 − b61 − 1; b58 − b33 + 1, b62 − b17 + 1, b69 − b24 + 1, b70 − b57 + 1, b87 − b24 + 1. For add-with-carry, b56 + b22 − 1, b61 + b44 − 1, b74 + b27 − 1, b90 + b65 − 1.  Less desirable from a mixing standpoint are the primes b56 − b5 − 1, b56 − b32 − 1, b66 − b57 − 1, b76 − b15 − 1, b84 − b26 − 1, b90 − b42 − 1, b93− b18−1; b52− b8 +1, b60− b12 +1, b67− b8 +1, b67− b63 +1, b83− b14 +1; b65 + b2−1, b76 + b11 − 1, b88 + b30 − 1, b92 + b48 − 1.  To calculate the period of the resulting sequences, we need to know the factors of m − 1; but this isn’t feasible for such large numbers unless we are extremely lucky. Suppose we do succeed in finding the prime factors q1, . . . , qt; then the probability that b m−1  q mod m = 1 is extremely small, only 1 q, except for the very small primes q. Therefore we can be quite confident that the period of bn mod m is extremely long even though we cannot factor m − 1.  Indeed, the period is almost certainly very long even if m is not prime. Consider, for example, the case k = 10, l = 3, b = 10  which is much too small for random number generation but small enough that we can easily compute the exact results . In this case ⟨10n mod m⟩ has period length lcm 219, 11389520  = 2494304880 when m = 9999998999 = 439 · 22779041; 4999999500 when m = 9999999001; 5000000499 when m = 10000000999; and lcm 1, 16, 2686, 12162  = 130668528 when m = 10000001001 = 3·17·2687·72973. Rare choices of the seed values may shorten the period when m is not prime. But we can hardly go wrong if we choose, say, k = 1000, l = 619, and b = 216.  SECTION 3.2.1.3 1. c = 1 is always relatively prime to B5; and every prime dividing m = B5 is a divisor of B, so it divides b = B2 to at least the second power. 2. Only 3, so the generator is not recommended in spite of its long period. 3. The potency is 18 in both cases  see the next exercise . 4. Since a mod 4 = 1, we must have a mod 8 = 1 or 5, so b mod 8 = 0 or 4. If b is an odd multiple of 4, and if b1 is a multiple of 8, clearly bs ≡ 0  modulo 2e  implies that bs1 ≡ 0  modulo 2e , so b1 cannot have higher potency than b. 5. The potency is the smallest value of s such that fjs ≥ ej for all j. 6. The modulus must be divisible by 27 or by p4  for odd prime p  in order to have a potency as high as 4. The only values are m = 227 + 1 and 109 − 1. 7. a′ =  1 − b + b2 − ···   mod m, where the terms in bs, bs+1, etc., are dropped  if s is the potency . 8. Since Xn is always odd,  Xn+2 =  234 + 3 · 218 + 9 Xn mod 235 =  234 + 6Xn+1 − 9Xn  mod 235  .   3.2.2  ANSWERS TO EXERCISES  551  Given Yn and Yn+1, the possibilities for  Yn+2 ≈  10 + 6 Yn+1 + ϵ1  − 9 Yn + ϵ2   mod 20,  with 0 ≤ ϵ1 < 1, 0 ≤ ϵ2 < 1, are limited and nonrandom. Note: If the multiplier suggested in exercise 3 were, say, 233 + 218 + 22 + 1, instead of 223 + 213 + 22 + 1, we would similarly find Xn+2 − 10Xn+1 + 25Xn ≡ constant  modulo 235 . In general, we do not want a ± δ to be divisible by high powers of 2 when δ is small, else we get “second-order impotency.” See Section 3.3.4 for a more detailed discussion.  The generator that appears in this exercise is discussed in an article by MacLaren and Marsaglia, JACM 12  1965 , 83–89. The deficiencies of such generators were first demonstrated by M. Greenberger, CACM 8  1965 , 177–179. Yet generators like this were still in widespread use more than ten years later  see the discussion of RANDU in Section 3.3.4 .  SECTION 3.2.2 1. The method is useful only with great caution. In the first place, aUn is likely to be so large that the addition of c m that follows will lose almost all significance, and the “mod 1” operation will nearly destroy any vestiges of significance that might remain. We conclude that double-precision floating point arithmetic is necessary. Even with double precision, one must be sure that no rounding, etc., occurs to affect the numbers of the sequence in any way, since that would destroy the theoretical grounds for the good behavior of the sequence.  But see exercise 23.  2. Xn+1 equals either Xn−1 + Xn or Xn−1 + Xn − m. If Xn+1 < Xn we must have Xn+1 = Xn−1 + Xn − m; hence Xn+1 < Xn−1. 3.  a  The underlined numbers are V [j] after step M3.  and repeats.  Output: initial 0 3 2 5  V [0]: V [1]: V [2]: V [3]: X: Y :  0 4 5 6 2 0 3 2 7 4 1 6 3 0 5  4 7 7 7 7 7 7 7 4 7 7 7 7 7 7 7 4 7 . . . 3 3 3 3 3 3 2 5 5 5 5 5 5 5 2 5 5 5 . . . 2 2 2 2 0 3 3 3 3 3 3 3 0 3 3 3 3 3 . . . 5 5 6 1 1 1 1 1 1 1 6 1 1 1 1 1 1 1 . . . 4 7 6 1 0 3 2 5 4 7 6 1 0 3 2 5 4 7 . . . 0 1 6 7 4 5 2 3 0 1 6 7 4 5 2 3 0 1 . . .  So the potency has been reduced to 1! exercise 15.    b  The underlined numbers are V [j] after step B2.   See further comments in the answer to  Output: initial 0 3 2 5 4  V [0]: V [1]: V [2]: V [3]: X:  2 3 6 5 7 0 0 5 3 . . . 4 6 3 0 . . . 4 7  . . . 0 0 0 0 0 0 5 4 4 . . . 1 1 1 1 . . . 1 1 . . . 3 6 1 1 1 1 1 1 1 . . . 0 0 0 4 . . . 0 0 . . . 7 7 7 7 3 3 3 3 7 . . . 6 2 2 2 . . . 7 2 . . . 5 5 5 0 0 2 2 2 2 . . . 3 3 5 5 . . . 3 3 . . . 7 6 1 0 3 2 5 4 7 . . . 3 2 5 4 . . . 3 2 . . .   552  ANSWERS TO EXERCISES  3.2.2  In this case the output is considerably better than the input; it enters a repeating cycle of length 40 after 46 steps: 236570 05314 72632 40110 37564 76025 12541 73625 03746  30175 24061 52317 46203 74531 60425 16753 02647 . The cycle can be found easily by applying the method of exercise 3.1–7 to the array above until a column is repeated. 4. The low-order byte of many random sequences  e.g., linear congruential sequences with m = word size  is much less random than the high-order byte. See Section 3.2.1.1. 5. The randomizing effect would be quite minimized, because V [j] would always contain a number in a certain range, essentially j k ≤ V [j] m <  j + 1  k. However, some similar approaches could be used: We could take Yn = Xn−1, or we could choose j from Xn by extracting some digits from the middle instead of at the extreme left. None of these suggestions would produce a lengthening of the period analogous to the behavior of Algorithm B.  Exercise 27 shows, however, that Algorithm B doesn’t necessarily increase the period length.  6. For example, if Xn m < 1 7. [W. Mantel, Nieuw Archief voor Wiskunde  2  1  1897 , 172–184.] 00. . .01 00. . .10 . . . 10. . .00 00. . .00  The subsequence of X values:  00. . .01 00. . .10 . . . 10. . .00  2, then Xn+1 = 2Xn.  becomes:  CONTENTS A   CONTENTS A   8. We may assume that X0 = 0 and m = pe, as in the proof of Theorem 3.2.1.2A. First suppose that the sequence has period length pe; it follows that the period of the sequence mod pf has length pf, for 1 ≤ f ≤ e, otherwise some residues mod pf would never occur. Clearly, c is not a multiple of p, for otherwise each Xn would If p ≤ 3, it is easy to establish the necessity of conditions  iii  be a multiple of p. and  iv  by trial and error, so we may assume that p ≥ 5. If d ̸≡ 0  modulo p  then dx2 + ax + c ≡ d x + a1 2 + c1  modulo pe  for some integers a1 and c1 and for all integers x; this quadratic takes the same value at the points x and −x − 2a1, so it cannot assume all values modulo pe. Hence d ≡ 0  modulo p ; and if a ̸≡ 1, we would have dx2 + ax + c ≡ x  modulo p  for some x, contradicting the fact that the sequence mod p has period length p. To show the sufficiency of the conditions, we may assume by Theorem 3.2.1.2A and consideration of some trivial cases that m = pe where e ≥ 2. If p = 2, we have Xn+2 ≡ Xn+2  modulo 4 , by trial; and if p = 3, we have Xn+3 ≡ Xn−d+3c  modulo 9 , using  i  and  ii . For p ≥ 5, we can prove that Xn+p ≡ Xn + pc  modulo p2 : Let d = pr, a = 1+ ps. Then if Xn ≡ cn+ pYn  modulo p2 , we must have Yn+1 ≡ n2c2r + ncs+ Yn   c2r + cs   modulo p . Thus Yp mod p = 0, and   modulo p ; hence Yn ≡n  2c2r +n  2 the desired relation has been proved.  3  Now we can prove that the sequence ⟨Xn⟩ of integers defined in the “hint” satisfies  Xn+pf ≡ Xn + tpf  modulo pf+1 ,  n ≥ 0,  for some t with t mod p ̸= 0, and for all f ≥ 1. This suffices to prove that the sequence ⟨Xn mod pe⟩ has period length pe, for the length of the period is a divisor of pe but not a divisor of pe−1. The relation above has already been established for f = 1, and for f > 1 it can be proved by induction in the following manner: Let  the relation  Xn+pf ≡ Xn + tpf + Znpf+1  modulo pf+2 ;   3.2.2  ANSWERS TO EXERCISES  553  then the quadratic law for generating the sequence, with d = pr, a = 1 + ps, yields Zn+1 ≡ 2rtnc + st + Zn  modulo p . It follows that Zn+p ≡ Zn  modulo p ; hence  Xn+kpf ≡ Xn + k tpf + Znpf+1   modulo pf+2   for k = 1, 2, 3, . . . ; setting k = p completes the proof.  Notes:  n + 5Yn + 1  mod 2e−2.  If f x  is a polynomial of degree higher than 2 and Xn+1 = f Xn , the analysis is more complicated, although we can use the fact that f m + pk  = f m  + pkf′ m  + p2kf′′ m  2! + ··· to prove that many polynomial recurrences give the maximum period. For example, Coveyou has proved that the period is m = 2e if f 0  is odd, f′ j  ≡ 1, f′′ j  ≡ 0, and f j + 1  ≡ f j  + 1  modulo 4  for j = 0, 1, 2, 3. [Studies in Applied Math. 3  Philadelphia: SIAM, 1969 , 70–111.] 9. Let Xn = 4Yn + 2; then the sequence Yn satisfies the quadratic recurrence Yn+1 =  4Y 2 10. Case 1: X0 = 0, X1 = 1; hence Xn ≡ Fn. We seek the smallest n for which Fn ≡ 0 and Fn+1 ≡ 1  modulo 2e . Since F2n = Fn Fn−1 + Fn+1 , F2n+1 = F 2 n+1, we find by induction on e that, for e > 1, F3·2e−1 ≡ 0 and F3·2e−1+1 ≡ 2e + 1  modulo 2e+1 . This implies that the period is a divisor of 3 · 2e−1 but not a divisor of 3 · 2e−2, so it is either 3 · 2e−1 or 2e−1. But F2e−1 is always odd  since only F3n is even . Case 2: X0 = a, X1 = b. Then Xn ≡ aFn−1 + bFn; we need to find the smallest positive n with a Fn+1 − Fn  + bFn ≡ a and aFn + bFn+1 ≡ b. This implies that  b2 − ab − a2 Fn ≡ 0,  b2 − ab − a2  Fn+1 − 1  ≡ 0. And b2 − ab − a2 is odd  that is, prime to m ; so the condition is equivalent to Fn ≡ 0, Fn+1 ≡ 1. Methods to determine the period of ⟨Fn⟩ for any modulus appear in an article by D. D. Wall, AMM 67  1960 , 525–532. Further facts about the Fibonacci sequence mod 2e have been derived by B. Jansson [Random Number Generators  Stockholm: Almqvist & Wiksell, 1966 , Section 3C1]. 11.  a  We have zλ = 1 + f z u z  + pev z  for some u z  and v z , where v z  ̸≡ 0  modulo f z  and p . By the binomial theorem, 2e+1  n + F 2  zλp = 1 + pe+1  v z  + p  v z 2 p − 1  2  plus further terms congruent to zero  modulo f z  and pe+2 . Since pe > 2, we have zλp ≡ 1 + pe+1v z   modulo f z  and pe+2 . If pe+1v z  ≡ 0  modulo f z  and pe+2 , there must exist polynomials a z  and b z  such that pe+1 v z  + pa z   = f z b z . Since f 0  = 1, this implies that b z  is a multiple of pe+1  by Gauss’s Lemma 4.6.1G ; hence v z  ≡ 0  modulo f z  and p , a contradiction.   b  If zλ − 1 = f z u z  + pev z , then  G z  = u z   zλ − 1  + pev z  f z  zλ − 1 ;  hence An+λ ≡ An  modulo pe  for large n. Conversely, if ⟨An⟩ has the latter property then G z  = u z  + v z   1 − zλ  + peH z , for some polynomials u z  and v z , and some power series H z , all with integer coefficients. This implies the identity 1 − zλ = u z f z  1 − zλ  + v z f z  + peH z f z  1 − zλ ; and H z f z  1 − zλ  is a polynomial since the other terms of the equation are polynomials.  c  It suffices to prove that λ pe  ̸= λ pe+1  implies that λ pe+1  = pλ pe  ̸= λ pe+2 . Applying  a  and  b , we know that λ pe+2  ̸= pλ pe , and that λ pe+1  is a divisor of pλ pe  but not of λ pe . Hence if λ pe  = pfq, where q mod p ̸= 0, then λ pe+1  must be pf+1d, where d is a divisor of q. But now Xn+pf+1d ≡ Xn  modulo pe ; hence pf+1d is a multiple of pfq, hence d = q. [Note: The hypothesis pe > 2 is   554  ANSWERS TO EXERCISES  3.2.2  necessary; for example, let a1 = 4, a2 = −1, k = 2; then ⟨An⟩ = 1, 4, 15, 56, 209, 780, . . . ; λ 2  = 2, λ 4  = 4, λ 8  = 4.]  d  g z  = X0+ X1−a1X0 z+···+ Xk−1−a1Xk−2−a2Xk−3−···−ak−1X0 zk−1.  e  The derivation in  b  can be generalized to the case G z  = g z  f z ; then the assumption of period length λ implies that g z  1− zλ  ≡ 0  modulo f z  and pe ; we treated only the special case g z  = 1 above. But both sides of this congruence can be multiplied by Hensel’s b z , and we obtain 1 − zλ ≡ 0  modulo f z  and pe .  Note: A more “elementary” proof of the result in  c  can be given without using generating functions, using methods analogous to those in the answer to exercise 8: If Aλ+n = An + peBn, for n = r, r + 1, . . . , r + k − 1 and some integers Bn, then this same relation holds for all n ≥ r if we define Br+k, Br+k+1, . . . by the given recurrence relation. Since the resulting sequence of B’s is some linear combination of shifts of the sequence of A’s, we will have Bλ+n ≡ Bn  modulo pe  for all large enough values of n. Now λ pe+1  must be some multiple of λ = λ pe ; for all large enough n we have An+jλ = An + pe Bn + Bn+λ + Bn+2λ + ··· + Bn+ j−1 λ  ≡ An + jpeBn  modulo p2e  for j = 1, 2, 3, . . . . No k consecutive B’s are multiples of p; hence λ pe+1  = pλ pe  ̸= λ pe+2  follows immediately when e ≥ 2. We still must prove that λ pe+2  ̸= pλ pe   modulo p  when n is large enough. Then An+p ≡ An + p2Bn +p Cn   modulo p3 , when p is odd and e = 1; here we let Bλ+n = Bn + pCn, and observe that Cn+λ ≡ Cn  2  and the proof is readily completed.  For the history of this problem, see Morgan Ward, Trans. Amer. Math. Soc. 35   1933 , 600–628; see also D. W. Robinson, AMM 73  1966 , 619–621. 12. The period length mod 2 can be at most 4; and the period length mod 2e+1 is at most twice the maximum length mod 2e, by the considerations of the previous exercise. So the maximum conceivable period length is 2e+1; this is achievable, for example, in the trivial case a = 0, b = c = 1. 13, 14. Clearly Zn+λ = Zn, so λ′ is certainly a divisor of λ. Let the least common 1 ≡ multiple of λ′ and λ1 be λ′1, and define λ′2 similarly. We have Xn +Yn ≡ Zn ≡ Zn+λ′ Xn + Yn+λ′ 1, so λ′1 is a multiple of λ2. Similarly, λ′2 is a multiple of λ1. This yields the desired result.  The result is “best possible” in the sense that sequences for which λ′ = λ0 can be constructed, as well as sequences for which λ′ = λ.  15. Algorithm M generates  Xn+k, Yn  in step M1 and outputs Zn = Xn+k−qn in step M3, for all sufficiently large n. Thus ⟨Zn⟩ has a period of length λ′, where λ′ is the least positive integer such that Xn+k−qn = Xn+λ′+k−qn+λ′ for all large n. Since λ is a multiple of λ1 and λ2, it follows that λ′ is a divisor of λ.  These observations are due to Alan G. Waterman.  We also have n + k − qn ≡ n + λ′ + k − qn+λ′  modulo λ1  for all large n, by the distinctness of the X’s. The bound on ⟨qn⟩ implies that qn+λ′ = qn + c for all large n, where c ≡ λ′  modulo λ1  and c < 1 2 λ1. But c must be 0 since ⟨qn⟩ is bounded. Hence λ′ ≡ 0  modulo λ1 , and qn+λ′ = qn for all large n; it follows that λ′ is a multiple of λ2 and λ1, so λ′ = λ. Note: The answer to exercise 3.2.1.2–4 implies that when ⟨Yn⟩ is a linear congru- ential sequence of maximum period modulo m = 2e, the period length λ2 will be at most 2e−2 when k is a power of 2. 16. There are several methods of proof. In the field with 2k elements let ξ satisfy  1  Using the theory of finite fields. ξk = a1ξk−1 + ··· + ak. Let f b1ξk−1 + ··· + bk  = bk, where each bj is either zero   3.2.2  ANSWERS TO EXERCISES  555  or one; this is a linear function. If word X in the generation algorithm is  b1b2 . . . bk 2 before  10  is executed, and if b1ξk−1+···+bkξ0 = ξn, then word X represents ξn+1 after  10  is executed. Hence the sequence is f ξn , f ξn+1 , f ξn+2 , . . . ; and f ξn+k  = f ξnξk  = f a1ξn+k−1 + ··· + akξn  = a1f ξn+k−1  + ··· + akf ξn . n ≥ 0, 1 ≤ j ≤ k, satisfying   2  Using brute force, or elementary ingenuity. We are given a sequence Xnj,  X n+1 j ≡ Xn j+1  + ajXn1,  1 ≤ j < k;  X n+1 k ≡ akXn1  modulo 2 .  We must show that this implies Xnk ≡ a1X n−1 k +···+ akX n−k k, for n ≥ k. Indeed, it implies Xnj ≡ a1X n−1 j + ··· + akX n−k j when 1 ≤ j ≤ k ≤ n. This is clear for j = 1, since Xn1 ≡ a1X n−1 1 + X n−1 2 ≡ a1X n−1 1 + a2X n−2 1 + X n−2 3, etc. For j > 1, we have by induction    ≡  Xnj ≡ X n+1  j−1  − aj−1Xn1 ≡  1≤i≤k ≡ a1X n−1 j + ··· + akX n−k j.  aiX n+1−i  j−1  − aj−1 ai X n+1−i  j−1  − aj−1X n−i 1   1≤i≤k  1≤i≤k  aiX n−i 1  This proof does not depend on the fact that operations were done modulo 2, or modulo any prime number. 17.  a  When the sequence terminates, the  k − 1 -tuple  Xn+1, . . . , Xn+k−1  occurs for the  m + 1 st time. A given  k − 1 -tuple  Xr+1, . . . , Xr+k−1  can have only m distinct predecessors Xr, so one of these occurrences must be for r = 0.  b  Since the  k − 1 -tuple  0, . . . , 0  occurs  m + 1  times, each possible predecessor appears, so the k-tuple  a1, 0, . . . , 0  appears for all a1, 0 ≤ a1 < m. Let 1 ≤ s < k and suppose we have proved that all k-tuples  a1, . . . , as, 0, . . . , 0  appear in the sequence when as ̸= 0. By the construction, this k-tuple would not be in the sequence unless  a1, . . . , as, 0, . . . , 0, y  had appeared earlier for 1 ≤ y < m. Hence the  k − 1 -tuple  a1, . . . , as, 0, . . . , 0  has appeared m times, and all m possible predecessors appear; this means that  a, a1, . . . , as, 0, . . . , 0  appears for 0 ≤ a < m. The proof is now complete by induction. The result also follows from Theorem 2.3.4.2D, using the directed graph of exercise 2.3.4.2–23. The arcs from  x1, . . . , xj, 0, . . . , 0  to  x2, . . . , xj, 0, 0, . . . , 0 , where xj ̸= 0 and 1 ≤ j ≤ k, form an oriented subtree related neatly to Dewey decimal notation. 18. By exercise 16, the most significant bit of Un+1 is completely determined by the first and third bits of Un, so only 32 of the 64 possible pairs  ⌊8Un⌋,⌊8Un+1⌋  occur. [Notes: If we had used, say, 11-bit numbers Un =  .X11nX11n+1 . . . X11n+10 2, the sequence would be satisfactory for many applications. If another constant appears in A having more 1 bits, the generalized spectral test might give some indication of its suitability. See exercise 3.3.4–24; we could examine νt in dimensions t = 36, 37, 38, . . . .] 20. For k = 64 one can use CONTENTS A  =  243F6A8885A308D3 16  the bits of π! . 21. [J. London Math. Soc. 21  1946 , 169–172.] Any sequence of period length mk−1 with no k consecutive zeros leads to a sequence of period length mk by inserting a zero in the appropriate place, as in exercise 7; conversely, we can start with a sequence of period length mk and delete an appropriate zero from the period, to form a sequence of the other type. Let us call these “ m, k  sequences” of types A and B. The hypothesis   556  ANSWERS TO EXERCISES  3.2.2  assures us of the existence of  p, k  sequences of type A, for all primes p and all k ≥ 1; hence we have  p, k  sequences of type B for all such p and k.  To get a  pe, k  sequence of type B, let e = qr, where q is a power of p and r is not a multiple of p. Start with a  p, qrk  sequence of type A, namely X0, X1, X2, . . . ; then  using the p-ary number system  the grouped digits  X0 . . . Xq−1 p,  Xq . . . X2q−1 p, . . . form a  pq, rk  sequence of type A, since q is relatively prime to pqrk − 1 and the sequence therefore has a period length of pqrk − 1. This leads to a  pq, rk  sequence ⟨Yn⟩ of type B; and  Y0Y1 . . . Yr−1 pq,  YrYr+1 . . . Y2r−1 pq, . . . is a  pqr, k  sequence of type B by a similar argument, since r is relatively prime to pqk. To get an  m, k  sequence of type B for arbitrary m, we can combine  pe, k  sequences for each of the prime power factors of m using the Chinese remainder theorem; but a simpler method is available. Let ⟨Xn⟩ be an  r, k  sequence of type B, and let ⟨Yn⟩ be an  s, k  sequence of type B, where r and s are relatively prime; then ⟨ Xn + Yn  mod rs⟩ is an  rs, k  sequence of type B, by exercise 13.  A simple, uniform construction that yields  2, k  sequences for arbitrary k has  been discovered by A. Lempel [IEEE Trans. C-19  1970 , 1204–1209]. 22. By the Chinese remainder theorem, we can find constants a1, . . . , ak having desired residues modulo each prime divisor of m. If m = p1p2 . . . pt, the period length will be lcm pk1 −1, . . . , pk t −1 . In fact, we can achieve reasonably long periods for arbitrary m  not necessarily squarefree , as shown in exercise 11. 23. Subtraction may be faster than addition, see exercise 3.2.1.1–5; the period length is still 2e−1 255 − 1 , by exercise 30. R. Brent has pointed out that the calculations can be done exactly on floating point numbers in [0 . . 1 ; see exercise 3.6–11. 24. Run the sequence backwards. if Zn = Y−n we have Zn =  Zn−k+l − Zn−k  mod 2 =  Zn−k+l + Zn−k  mod 2. 25. This idea can save most of the overhead of subroutine calls. For example, suppose Program A is invoked by calling JMP RANDM, where we have  In other words,  The cost per random number is then 14 + 2 55 units of time. But suppose we generate random numbers by saying ‘DEC6 1; J6Z RNGEN; LDA Y,6’ instead, with the subroutine  1F Y,6  RANDM STJ LDA ... ENT6 55 JMP *  1H   Program A  RNGEN STJ  1F ENT6 24 LDA ADD STA DEC6 1 J6P  *-4  Y+31,6 Y,6 Y+31,6  ENT6 31 LDA Y,6 Y+24,6 ADD STA Y,6 DEC6 1 *-4 J6P ENT6 55 *  1H JMP  The cost is now only  12+ 6 55 u. [A similar implementation, expressed in the C language, is used in The Stanford GraphBase  New York: ACM Press, 1994 , GB FLIP.] Indeed, many applications find it preferable to generate an array of random numbers all at once. Moreover, the latter approach is essentially mandatory when we enhance the randomness with Lüscher’s method; see the C and FORTRAN routines in Section 3.6.   3.2.2  ANSWERS TO EXERCISES  557  27. Let Jn = ⌊kXn m⌋. Lemma. After the  k2 + 7k − 2  2 consecutive values  0k+2 1 0k+1 2 0k . . .  k − 1  03  occur in the ⟨Jn⟩ sequence, Algorithm B will have V [j] < m k for 0 ≤ j < k, and also Y < m k. Proof. Let Sn be the set of positions j such that V [j] < m k just before Xn is generated, and let jn be the index such that V [jn] ← Xn. If jn  ∈ Sn and Jn = 0, then Sn+1 = Sn ∪ {jn} and jn+1 > 0; if jn ∈ Sn and Jn = 0, then Sn+1 = Sn and jn+1 = 0. After k +2 successive 0s, we must therefore have 0 ∈ Sn and jn+1 = 0. Then after “1 0k+1” we must have {0, 1} ⊆ Sn and jn+1 = 0; after “2 0k” we must have {0, 1, 2} ⊆ Sn and jn+1 = 0; and so on. Corollary. Let l =  k2 + 7k − 2  2. If λ ≥ lkl, either Algorithm B yields a period of length λ or the sequence ⟨Xn⟩ is poorly distributed. Proof. The probability that any given length-l pattern of J’s does not occur in a random sequence of length λ is less than  1 − k−l λ l < exp −k−lλ l  ≤ e−1; hence the stated pattern should appear. After it does, the subsequent behavior of Algorithm B will be the same each time it reaches this part of the period.  When k > 4, we are requiring λ > 1021, so this result is purely academic. But smaller bounds may be possible.  29. The following algorithm performs about k2 operations in the worst case, but its average running time is much faster, perhaps O log k  or even O 1 :  X1. Set  a0, a1, . . . , ak  ←  x1, . . . , xk, m−1 . X2. Let i be minimum with ai > 0 and i > 0. Do subroutine Y for j = i + 1,  . . . , k, while ak > 0.  X3. If a0 > ak, f x1, . . . , xk  = a0; otherwise if a0 > 0, f x1, . . . , xk  = a0 − 1;  otherwise f x1, . . . , xk  = ak. Y1. Set l ← 0.  The subroutine in steps Y1–Y3 essentially tests the lexicographic relation  ai, . . . , ai+k−1  ≥  aj, . . . , aj+k−1 , decreasing ak if necessary to make this inequality true. We assume that ak+1 = a1, ak+2 = a2, etc.  Y2. If ai+l > aj+l, exit the subroutine. Otherwise if j + l = k, set ak ← ai+l. Otherwise if ai+l = aj+l, go on to step Y3. Otherwise if j + l > k, decrease ak by 1 and exit. Otherwise set ak ← 0 and exit.  Y3. Increase l by 1, and return to step Y2 if l < k. This problem was first solved by H. Fredricksen when m = 2 [J. Combinatorial Theory 9  1970 , 1–5; A12  1972 , 153–154]; in that special case the algorithm is simpler and it can be done with k-bit registers. See also H. Fredricksen and J. Maiorana, Discrete Math. 23  1978 , 207–210, who essentially discovered Algorithm 7.2.1.1F. 30.  a  By exercise 11, it suffices to show that the period length mod 8 is 4 2k−1 ; this will be true if and only if x2 2k−1  ̸≡ 1  modulo 8 and f x  , if and only if x2k−1 ̸≡ 1 2 f x + f −x  .  modulo 4 and f x  . Write f x  = fe x2 + xfo x2 , where fe x2  = 1 Then f x 2 + f −x 2 ≡ 2f x2   modulo 8  if and only if fe x 2 + xfo x 2 ≡ f x   modulo 4 ; and the latter condition holds if and only if fe x 2 ≡ −xfo x 2  modulo 4 and f x  , because fe x 2+xfo x 2 = f x +O xk−1 . Furthermore, working modulo 2 and f x , we have fe x 2 ≡ fe x2  ≡ xfo x2  ≡ x2kfo x 2, hence fe x  ≡ x2k−1fo x . Therefore fe x 2 ≡ x2kfo x 2  modulo 4 and f x  , and the hint follows. A similar argument proves that x2k ≡ x  modulo 4 and f x   if and only if f x 2 + f −x 2 ≡ 2 −1 kf −x2   modulo 8 .   b  The condition can hold only when l is odd and k = 2l. But then f x  is  primitive modulo 2 only when k = 2. [Math. Comp. 63  1994 , 389–401.]   558  ANSWERS TO EXERCISES  3.2.2  1  n X3nz3n = 1  n Xnzn is a polynomial multiple of 1  1 − z24 − z55 ; hence   31. We have Xn ≡  −1 Yn3Zn mod 2e for some Yn and Zn, by Theorem 3.2.1.2C; hence Yn =  Yn−24 + Yn−55  mod 2 and Zn =  Zn−24 + Zn−55  mod 2e−2. Since Zk is odd if and only if Xk mod 8 = 3 or 5, the period length is 2e−3 255 −1  by the previous exercise. g z  =  32. We can ignore the ‘mod m’ and put it back afterwards. The generating function n X2nz2n = 2 g z  + g −z   is a polynomial divided by  1 − z24 − z55  1 − z24 + z55  = 1 − 2z24 + X2 n−55   mod m. Similarly, z48 − z110. The first desired recurrence is therefore X2n =  2X2 n−12  − X2 n−24  + 3 g z  + g ωz  + g ω2z   where ω = e2πi 3, and we find X3n =  3X3 n−8  − 3X3 n−16  + X3 n−24  + X3 n−55   mod m. 33.  a  gn+t z  ≡ ztgn z   modulo m and 1 + z31 − z55 , by induction on t.  b  Since z500 mod  1 + z31 − z55  = 792z2 + z5 + 17z6 + 715z9 + 36z12 + z13 + 364z16 + 210z19 + 105z23+462z26+16z30+1287z33+9z36+18z37+1001z40+120z43+z44+455z47+462z50+ 120z54  see Algorithm 4.6.1D , we have X500 =  792X2 + X5 + ··· + 120X54  mod m. [It is interesting to compare the similar formula X165 =  X0 + 3X7 + X14 + 3X31 +4X38 + X45  mod m to the sparser recurrence for ⟨X3n⟩ in the previous exercise. Lüscher’s method of generating 165 numbers and using only the first 55 is clearly superior to the idea of generating 165 and using only X3, X6, . . . , X165.] qn+1  , 34. Let q0 = 0, q1 = 1, qn+1 = cqn + aqn−1. Then we have   0 Xn =  qn+1X0 + aqn   qnX0 + aqn−1 , and xn mod f x  ≡ qnx + aqn−1, for n ≥ 1. Thus if X0 = 0 we have Xn = 0 if and only if xn mod f x  is a nonzero constant. 35. Conditions  i  and  ii  imply that f x  is irreducible. For if f x  =  x− r1  x− r2  and r1r2 ̸= 0 we have xp−1 ≡ 1 if r1 ̸= r2 and xp ≡ r1 if r1 = r2. Let ξ be a primitive root of a field with p2 elements, and suppose ξ2k = ckξk + ak. The quadratic polynomials we seek are precisely the polynomials fk x  = x2 − ckx− ak where 1 ≤ k < p2 − 1 and k ⊥ p + 1.  See exercise 4.6.2–16.  Each polynomial occurs q\p+1, q prime 1 − 1 q . for two values of k; hence the number of solutions is 1 n exists mod 2e. The sequence ⟨qn⟩ defined in 36. In this case Xn is always odd, so X−1 answer 34 is 0, 1, 2, 1, 0, 1, 2, 1, . . . modulo 4. We also have q2n = qn qn+1 + aqn−1  n; hence q2n+1 − aq2n−1 =  qn+1 − aqn−1  qn+1 + aqn+1 . Since and q2n−1 = aq2 qn+1 + aqn+1 ≡ 2  modulo 4  when n is even, we deduce that q2e is an odd multiple of 2e and q2e+1 − aq2e−1 is an odd multiple of 2e+1, for all e ≥ 0. Therefore  2 p2 − 1   c n =   aqn−1 1  n−1 + q2  aqn  qn  a  q2e + aq2e−1 ≡ q2e+1 + aq2e + 2e+1  modulo 2e+2  .  And X2e−2 ≡  q2e−2+1 + aq2e−2   q2e−2 + aq2e−2−1  ̸≡ 1  modulo 2e , while X2e−1 ≡ 1. Conversely, we need a mod 4 = 1 and c mod 4 = 2; otherwise X2n ≡ 1  modulo 8 . [Eichenauer, Lehn, and Topuzoˇglu, Math. Comp. 51  1988 , 757–759.] The low-order bits of this sequence have a short period, so inversive generators with prime modulus are preferable. 37. We can assume that b1 = 0. By exercise 34, a typical vector in V is  x,  s′2x + as2   s2x + as′′2 , . . . ,  s′dx + asd   sdx + as′′d  ,  where sj = qbj , s′j = qbj+1, s′′j = qbj−1. This vector belongs to the hyperplane H if and only if  r1x + r2t2 x + u2  + ··· + rdtd x + ud  ≡ r0 − r2s′2s−1  2 − ··· − rds′ds−1  d   modulo p  ,   559  j and uj = as′′j s−1  j = − −a bj s−2  ANSWERS TO EXERCISES  3.3.1 where tj = a−as′js′′j s−2 . But this relation is equivalent to a polynomial congruence of degree ≤ d; so it cannot hold for d + 1 values of x unless it holds for all x, including the distinct points x = u2, . . . , x = ud. Hence r2 = ··· = rd ≡ 0, and r1 ≡ 0. [See J. Eichenauer-Herrmann, Math. Comp. 56  1991 , 297–301.] Notes: If we consider the  p+1− d × d+1  matrix M with rows { 1, v1, . . . , vd    v1, . . . , vd  ∈ V }, this exercise is equivalent to the assertion that any d + 1 rows of M are linearly independent modulo p. It is interesting to plot the points  Xn, Xn+1  for p ≈ 1000 and 0 ≤ n ≤ p; traces of circles, rather than straight lines, meet the eye.  j  49, 6  49, 3  49, 4  49, 5  49, 6  2 49, 3  49, 9  49, 2 49.  SECTION 3.3.1 1. There are k = 11 categories, so the line ν = 10 should be used. 2. 49, 4 49, 5 3. V = 7 173 240, only very slightly higher than that obtained from the good dice! There are two reasons why we do not detect the weighting:  a  The new probabilities  see exercise 2  are not really very far from the old ones in Eq.  1 . The sum of the two dice tends to smooth out the probabilities; if we counted instead each of the 36 possible pairs of values, we would probably detect the difference quite rapidly  assuming that the two dice are distinguishable .  b  A far more important reason is that n is too small for a significant difference to be detected. If the same experiment is done for large enough n, the faulty dice will be discovered  see exercise 12 . 4. ps = 1 2, which falls between the 75% and 95% entries in Table 1; so it is reasonable, in spite of the fact that not too many sevens actually turned up. − 5. K+ 20 = 0.215; these values do not differ significantly from random behavior  being at about the 94% and 86% levels , but they are mighty close.  The data values in this exercise come from Appendix A, Table 1.  6. The probability that Xj ≤ x is F  x , so we have the binomial distribution discussed  in Section 1.2.10: Fn x  = s n with probability n is F  x ; the standard deviation is F  x  1 − F  x   n.   F  x s 1 − F  x  n−s; the mean  12 for 2 ≤ s ≤ 12 and s ̸= 7; p7 = 1  6. The value of V is 16 1  [See Eq. 1.2.10– 19 . This  20 = 1.15; K  s  suggests that a slightly better statistic would be to define   Fn x  − F  x   F  x  1 − F  x  ;  √  K+  n =  n max  −∞<x<∞  see exercise 22. We can calculate the mean and standard deviation of Fn y  − Fn x , for x < y, and obtain the covariance of Fn x  and Fn y . Using these facts, it can be shown that for large values of n the function Fn x  behaves as a “Brownian motion,” and techniques from this branch of probability theory may be used to study it. The situation is exploited in articles by J. L. Doob and M. D. Donsker, Annals Math. Stat. 20  1949 , 393–403 and 23  1952 , 277–281; their approach is generally regarded as the most enlightening way to study the KS tests.] 7. Set j = n in Eq.  13  to see that K+ √ n. Similarly, set j = 1 to make the same observations about K− n . as 8. The new KS statistic was computed for 20 observations. The distribution of K+ 10 was used as F  x  when the KS statistic was computed. 9. The idea is erroneous, because all of the observations must be independent. There is a relation between the statistics K+ n on the same data, so each test should be  n is never negative, and that it can get as high  n and K−   560  ANSWERS TO EXERCISES  3.3.1  judged separately.  A high value of one tends to give a low value of the other.  Similarly, the entries in Figs. 2 and 5, which show 15 tests for each generator, do not show 15 independent observations, because the maximum-of-5 test is not independent of the maximum-of-4 test. The three tests of each horizontal row are independent  because they were done on different parts of the sequence , but the five tests in a column are somewhat correlated. The net effect of this is that the 95-percent probability levels, etc., which apply to one test, cannot legitimately be applied to a whole group of tests on the same data. Moral: When testing a random number generator, we may expect it to “pass” each of several tests, like the frequency test, maximum test, and run test; but an array of data from several different tests should not be considered as a unit since the tests themselves may not be independent. The K+ n statistics should be considered as two separate tests; a good source of random numbers will pass both. 10. Each Ys is doubled, and nps is doubled, so the numerators of  6  are quadrupled while the denominators only double. Hence the new value of V is twice as high as the old one. 11. The empirical distribution function stays the same; the values of K+ multiplied by 12. Let Zs =  Ys − nqs    √ nqs. The value of V is n times  n and K−  n and K−  n are  √  2.   qs − ps +qs nZs 2   ps,  k  s=1  and the latter quantity stays bounded away from zero as n increases  since Zsn−1 4 is bounded with probability 1 . Hence the value of V will increase to a value that is extremely improbable under the ps assumption. For the KS test, let F  x  be the assumed distribution, G x  the actual distribution, and let h = max G x  − F  x . Take n large enough so that Fn x  − G x  > h 2 occurs with very small probability; then Fn x  − F  x  will be improbably high under the assumed distribution F  x . 13.  The “max” notation should really be replaced by “sup” since a least upper bound is meant; however, “max” was used in the text to avoid confusing too many readers by the less familiar “sup” notation.  For convenience, let X0 = −∞, Xn+1 = +∞. When Xj ≤ x < Xj+1, we have Fn x  = j n; therefore max Fn x  − F  x   = j n − F  Xj  and max F  x  − Fn x   = F  Xj+1  − j n in this interval. As j varies from 0 to n, all real values of x are considered; this proves that   j   n   − F  Xj  F  Xj  − j − 1  ;    .  n  √  √  K+  n =  − n =  K  n max 0≤j≤n n max  1≤j≤n+1  These equalities are equivalent to  13 , since the extra term under the maximum signs is nonpositive and it must be redundant by exercise 7. 14. The logarithm of the left-hand side simplifies to    − k  s=1    Ys ln  1+ Zs√ nps  + 1−k 2  ln 2πn − 1 2  ln ps− 1 2  ln  1+ Zs√ nps  + O  k  s=1    k  s=1     1    ,  n   3.3.1  thatk  √  s=1 Zs  and this quantity simplifies further  upon expanding ln 1 + Zs   k  nps = 0  to −1 2  Z  s=1  2  s + 1 − k 2  ln 2πn  − 1  2 ln p1 . . . pk  + O     1√  .  n  ANSWERS TO EXERCISES  561  √ nps  and realizing  15. The corresponding Jacobian determinant is easily evaluated by  i  removing the factor rn−1 from the determinant,  ii  expanding the resulting determinant by the co- factors of the row containing “cos θ1 − sin θ1 0 . . . 0”  each of the cofactor determinants may be evaluated by induction , and  iii  recalling that sin2 θ1 + cos2 θ1 = 1.  + ···   z√2x+y  16.  0  The latter integral is  exp  − u2  z√2x  2x  0  du = ye−z2 + O   z√2x e−u2 2x du + 1 3x2  z√2  0  e−u2 2  −∞  When all is put together, the final result is  γ x + 1, x + z  2x + y   √ Γ  x + 1  √ 2 = xp and write  = 1√ 2π  If we set z 1√ 2π   z√2  e−u2 2 √  + ···  du.     1√  x  +  exp   z√2x  1√  0  − u2   2x  .  x  e−u2 2xu  3  du + O   1    .  x  √  du + e−z2 2πx  ν   y − 2  3 − 2  3 z2  + O    Γ   ν    2  = p,  du = p,  x + 1 = ν 2 ,  γ  t 2  2 ,  −∞ √ 3 1 + z2  + O 1  x  , where t 2 = x + z √ which is consistent with the analysis above. The solution is therefore t = ν + 2 ν z + √ 3 z2 − 2 4 ν  . 17.  a  Change of variable, xj ← xj + t.  2x + y, we can solve for y to obtain y = 2  3 + O 1    b  Induction on n; by definition, Pn0 x − t  =  c  The left-hand side is   x+t  n   xk+2  k+1  dxn . . .  dxk+1  times   d  From  b  and  c  we have Pnk x  =   k k  t  r=0   x  n  P n−1 0 xn − t  dxn.  x2  xk  dxk  r − t r  t  r!  dxk−1 . . .  t   x + t − r n−r−1   n − r !  dx1.   x + t − n .   n .  n evaluated for X1, . . . , Xn equals K−  The numerator in  24  is Pn⌊t⌋ 18. We may assume that F  x  = x for 0 ≤ x ≤ 1, as remarked in the text’s derivation of  24 . If 0 ≤ X1 ≤ ··· ≤ Xn ≤ 1, let Zj = 1 − Xn+1−j. We have 0 ≤ Z1 ≤ ··· ≤ Zn ≤ 1; and K+ n evaluated for Z1, . . . , Zn. This symmetrical relation gives a one-to-one correspondence between sets of equal volume for which K+ 20. For example, the term O 1 n  is −  4 3 s2  n+O n−3 2 . A complete expansion has been obtained by H. A. Lauwerier, Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 2  1963 , 61–68.  n fall in a given range.  n and K−  9 s4− 2   562  ANSWERS TO EXERCISES  3.3.1  23. Let m be any number ≥ n.  a  If ⌊mF  Xi ⌋ = ⌊mF  Xj ⌋ and i > j, then i n − F  Xi  > j n − F  Xj .  b  Start with ak = 1.0, bk = 0.0, and ck = 0 for 0 ≤ k < m. Then do the following for each observation Xj: Set Y ← F  Xj , k ← ⌊mY ⌋, ak ← min ak, Y  , bk ← max bk, Y  , ck ← ck + 1.  Assume that F  Xj  < 1 so that k < m.  Then set j ← 0, r+ ← r− ← 0, and for k = 0, 1, . . . , m − 1  in this order  do the following whenever ck > 0: Set r− ← max r−, ak − j n , j ← j + ck, r+ ← max r+, j n − bk . Finally set K+ n r−. The time required is O m + n , and the precise value of n need not be known in advance.  If the estimate  k + 1 2  m is used for ak and bk, so that only the values ck are actually computed for each k, we obtain estimates of K+ n m, even when m < n.  [ACM Trans. Math. Software 3  1977 , 60–64.]  n ← √ √  n good to within 1 2  n ← √  n and K−  n r+, K−  25.  a  Since cij = E n  k=1 aikXk  n l=1 ajlXl  =n  k=1 aikajk, we have C = AAT.   b  Consider the singular value decomposition A = U DV T, where U and V are orthogonal of sizes m × m and n × n, and D is m × n with entries dij = [i = j]σj; the singular values σj are all positive. [See, for example, Golub and Van Loan, Matrix Computations  1996 , §2.5.3.] If CCC = C we have SBS = S, where S = DDT j , where we let σn+1 = ··· = σm = 0, and and B = U TCU. Thus sij = [i = j]σ2 j if i, j ≤ n, and we deduce that DTBD is the n × n identity matrix. Let Y =  Y1 − µ1, . . . , Ym − µm T and X =  X1, . . . , Xn T; it follows that W = Y TCY = X TAT CAX = X T V DTBDV TX = X TX.  j bij. Consequently bij = [i = j] σ2  sij =  k,l sikbklslj = σ2  i σ2  SECTION 3.3.2 1. The observations for a chi-square test must be independent. In the second se- quence, successive observations are manifestly dependent, since the second component of one equals the first component of the next. 2. Form t-tuples  Yjt, . . . , Yjt+t−1 , for 0 ≤ j < n, and count how many of them are equal to each possible value. Apply the chi-square test with k = dt and with probability 1 dt in each category. The number of observations, n, should be at least 5dt. 3. The probability that exactly j values are examined, namely the probability that Uj−1 is the nth element that lies in the range α ≤ Uj−1 < β, is easily seen to be     j − 1  n − 1  pn 1 − p j−n,  by enumeration of the possible places in which the other n − 1 occurrences can appear and by evaluation of the probability of such a pattern. The generating function is G z  =  pz  1 −  1 − p z  n, which makes sense since the given distribution is the n-fold convolution of the same thing for n = 1. Hence the mean and variance are proportional to n; the number of U’s to be examined is now easily found to have the  characteristics  min n, ave n p, max ∞, devn 1 − p  p . A more detailed discussion  of this probability distribution when n = 1 may be found in the answer to exercise 3.4.1–17; see also the considerably more general results of exercise 2.3.4.2–26. 4. The probability of a gap of length ≥ r is the probability that r consecutive U’s lie outside the given range, namely  1 − p r. The probability of a gap of length exactly r is the probability for length ≥ r minus the probability for length ≥  r + 1 . 5. As N goes to infinity, so does n  with probability 1 , hence this test is just the same as the gap test described in the text except for the length of the very last gap. And the text’s gap test certainly is asymptotic to the chi-square distribution stated,   3.3.2  ANSWERS TO EXERCISES  563  since the length of each gap is independent of the length of the others. [Notes: A quite complicated proof of this result by E. Bofinger and V. J. Bofinger appears in Annals Math. Stat. 32  1961 , 524–534. Their paper is noteworthy because it discusses several interesting variations of the gap test; they show, for example, that the quantity    0≤r≤t   Yr −  Np pr 2   Np pr  does not approach a chi-square distribution, although others had suggested this statistic as a “stronger” test because Np is the expected value of n.] 7. 5, 3, 5, 6, 5, 5, 4. 8. See exercise 10, with w = d. 9.  Change d to w in steps C1 and C4.  We have  10. As in exercise 3, we really need consider only the case n = 1. The generating function for the probability that a coupon set has length r is   r − 1    w − 1 + ··· +  ,  1  d  dr  0!  pr = d d − 1  . . .  d − w + 1    t − 1  1 pt = 1 − d! dt−1  z  r − 1   d d − 1 − 1 d − H   d − w !  w − 1  r>0  d!   2    2   d  r = zw    d − w + 1 − 1 d−w  − d Hd − Hd−w  = σ  + ··· +  d  2  .    G z  =  mean G  = w + 2 H  var G  = d    for w ≤ r < t;   t − 1  d − w + 1  w  .  . . .  d −  w − 1 z     d − w !     d − 1  d − z  = d Hd − Hd−w  = µ;  by the previous exercise and Eq. 1.2.9– 28 . The mean and variance are readily computed using Theorem 1.2.10A and exercise 3.4.1–17. We find that  The number of U’s examined, as the search for a coupon set is repeated n times, therefore has the characteristics  min wn, ave µn, max ∞, dev σ 11. 12. Algorithm R  Data for run test .  1 2 9 8 5 3 6 7 0 4 .  √ n .  R1. [Initialize.] Set j ← −1, and set COUNT[1] ← COUNT[2] ← ··· ← COUNT[6] ← 0.  Also set Un ← Un−1, for convenience in terminating the algorithm.  R2. [Set r zero.] Set r ← 0. R3. [Is Uj < Uj+1?] Increase r and j by 1. If Uj < Uj+1, repeat this step. R4. [Record the length.] If r ≥ 6, increase COUNT[6] by one, otherwise increase  COUNT[r] by one.  R5. [Done?] If j < n − 1, return to step R2.  13. There are  p + q + 1 p+q Ui+p+q−1; subtract p+q+1   ways to have Ui−1 >  for those ways in which Ui−1 < Ui, and subtract p+q+1  < Ui+p < ··· < for those in which Ui+p−1 < Ui+p; then add in 1 for the case that both Ui−1 < Ui and Ui+p−1 < Ui+p, since this case has been subtracted out twice.  This is a special case of the inclusion-exclusion principle, which is explained further in Section 1.3.3.      p+1  1  p   564  ANSWERS TO EXERCISES  3.3.2  14. A run of length r occurs with probability 1 r! − 1  r + 1 !, assuming distinct U’s. Therefore we use pr = 1 r! − 1  r + 1 ! for r < t and pt = 1 t! for runs of length ≥ t. 15. This is always true of F  X  when F is continuous and X has distribution F; see the remarks following Eq. 3.3.1– 23 . 16.  a  Zjt = max Zj t−1 , Z j+1  t−1  . If the Zj t−1  are stored in memory, it is therefore a simple matter to transform this array into the set of Zjt with no auxiliary storage required.  b  With his “improvement,” each of the V ’s should indeed have the stated distribution, but the observations are no longer independent. In fact, when Uj is a relatively large value, all of Zjt, Z j−1 t, . . . , Z j−t+1 t will be equal to Uj; so we almost have the effect of repeating the same data t times  and that would multiply V by t, as in exercise 3.3.1–10 . 0≤k<j<n U′kV ′j − U′jV ′k 2, and this is certainly nonnegative.  c  Therefore if D2 = N 2, we must have U′kV ′j − U′jV ′k = 0, for all pairs j, k. This means that the matrix  17.  b  By Binet’s identity, the difference is    U′0 U′1  V ′0 V ′1    . . . U′n−1 . . . V ′n−1  1 + U 2  0 + U 2  0  D, E S2  n−1, X = U0U1 + ··· + Un−2Un−1 + Un−1U0, and D = nS2 − S2  0 + ··· − U2U0 .  c  The denominator always equals   has rank < 2, so its rows are linearly dependent.  A more elementary proof can be given, using the fact that U′0V ′j − U′jV ′0 = 0 for 1 ≤ j < n implies the existence of constants α, β such that αU′j + βV ′j = 0 for all j, provided that U′0 and V ′0 are not both zero; the latter case can be avoided by a suitable renumbering.  18.  a  The numerator is − U0 − U1 2, the denominator is  U0 − U1 2.  b  The nu- merator in this case is − U 2 2 − U0U1 − U1U2 − U2U0 ; the denominator 0≤j<k<n Uj − Uk 2, by is 2 U 2 exercise 1.2.3–30 or 1.2.3–31. 19. The stated result holds, in fact, whenever the joint distribution of U0, . . . , Un−1 is symmetrical  unchanged under permutations . Let S1 = U0 + ··· + Un−1, S2 = 0 + ··· + U 2 U 2 1. Also let E f U0, . . . , Un−1  denote the expected value of f U0, . . . , Un−1  subject to the condition D ̸= 0. Since D is a symmetric function, we have E f U0, . . . , Un−1  = E f Up 0 , . . . , Up n−1   for all permutations p of {0, . . . , n − 1}. Therefore E S2 D = 1  D = n n − 1  E U0U1 D  + n E U 2 n E U 2 0  D, and E X D = n E U0U1 D . It follows that 1 = E  nS2 − S2 1  D.  Strictly speaking, E S2 D and E S2 1  D might be infinite, so we should be careful to work only with linear combinations of expected values that are known to exist.  20. Let E1111, E211, E22, E31, and E4 denote the respective values E U0U1U2U3 D2 , 0 U1U2 D2 , E U 2 1  D2 , E U 3 E U 2 2  D2 = 0 U 2 n n−1 E22+nE4, E S2S2 1  D2  = n n−1  n−2 E211+n n−1 E22+2n n−1 E31+nE4, 1  D2 = n n − 1  n − 2  n − 3 E1111 + 6n n − 1  n − 2 E211 + 3n n − 1 E22 + E S4 4n n − 1 E31 + nE4, E X2 D2 = n n − 3 E1111 + 2nE211 + nE22, E XS2 1  D2  = n n − 2  n − 3 E1111 + 5n n − 2 E211 + 2nE22 + 2nE31, E  U0 − U1 4 D2  = 6E22 − 8E31 + 2E4, and the first result follows.  1  D = − n − 1  E  nX − S2  Let δ = α  ln n  n 1 3, M = α3 2 + 1 3, and m = ⌈1 δ⌉.  If we divide the range of the distribution into m equiprobable parts, we can show that each part will contain between nδ 1 − δ  and nδ 1 + δ  points, with probability ≥ 1 − O n−M , using the tail inequalities 1.2.10– 24  and  25 . Hence, if the distribution is uniform, 12 n2 1 + O δ   with at least this probability. If D is not in that range, we have D = 1  0  D2 . Then we have E S2  0 U1 D2 , E U 4   3.3.2  0 ≤  U0 − U1 4 D2 ≤ 1. Since E  U0 − U1 4  =  1   1 ANSWERS TO EXERCISES 0  x − y 4 dx dy = 1  0  conclude that E  U0 − U1 4 D2  = 48  5 n−4 1 + O δ   + O n−M .  Note: Let N be the numerator of  23 . When the variables all have the normal  distribution, W. J. Dixon proved that the expected value of e wN+zD  n is   1 − 2z − 2w 1 2 1 − 2z + 1 − 2z 2 − 4w2  −n 2 + O wn .  15, we may  565   y1,...,yt   2 k  n + 1  2 k  n − 1  2 k, E N D 2k =  + 1  Differentiating with respect to w and integrating with respect to z, he found the moments E N D 2k−1 =  − 1 2 k, when n > 2k. In particular, the variance in this case is exactly 1  n + 1  − 1  n − 1 2. [Annals of Math. Stat. 15  1944 , 119–144.] 21. The successive values of cr−1 = s − 1 in step P2 are 2, 3, 7, 6, 4, 2, 2, 1, 0; hence f = 886862. 22. 1024 = 6! + 2 · 5! + 2 · 4! + 2 · 3! + 2 · 2! + 0 · 1!, so we want the successive values of s − 1 in step P2 to be 0, 0, 0, 1, 2, 2, 2, 2, 0; working backwards, the permutation is  9, 6, 5, 2, 3, 4, 0, 1, 7, 8 . 23. Let P ′ x1, . . . , xt  = 1  λ′λ′ Q x1, . . . , xt  =  more compactly, Q x  =   E X 2 ≤ E X2, we have   y P ′ y  P  x − y  − d−t 2 =    n=0 [ Y ′n, . . . , Y ′n+t−1  =  x1, . . . , xt ]. Then we have −1 P ′ y1, . . . , yt P  x1 − y1  mod d, . . . ,  xt − yt  mod d  ; x  x Q x  − d−t 2 =  y P ′ y  x P  x  − d−t 2 =   y P ′ y P  x − y . Hence, using the general inequality y P ′ y  P  x − y  − d−t  2 ≤ x P  x  − d−t 2. [See G. Marsaglia, Comp. Sci. and Statistics: Symp. on the Interface 16  1984 , 5–6. The result is of interest only when dt ≤ 2λ, since each P  x  is a multiple of 1 λ.] 24. Write k : α and α : k for the first k and last k elements of string α. Let K α, β  = [α = β] P  α , and let C be the dt × dt matrix with entries ¯cαβ = K α, β − K t−1 : α, t − 1 : β . Let C be the covariance matrix of the random variables N α  for α = t, a=0 N aα  α=t N α  = n; but all other linear constraints are derivable from these  see Theorem 2.3.4.2G . Therefore C has rank dt − dt−1, and by exercise 3.3.1–25 we need only show that CCC = C. k<t Tk α, β , where Tk α, β  is a term corresponding to the overlap that might occur when we superimpose β on α and slide it k positions to the right:  divided by n. These variables are subject to the constraintd−1 for each of dt−1 strings α, and we also have  It is not difficult to verify that cαβ = P  αβ   a=0 N αa  =d−1  x   K t + k : α, β : t + k  − 1,  K α : t − k, t − k : β  − 1,  if k ≤ 0; if k ≥ 0.  Tk α, β  =  For example, if d = 2, t = 5, α = 01101, and β = 10101, we have cαβ = P  0 4P  1 6 ×  P  01 −1 + P  101 −1 + P  1 −1 − 9 . Entry αβ of CCC is therefore P  αβ  times    d−1  P  γab      γ=t−1  a,b=0  k<t  l<t  Tk α, γa  K a, b  − 1 Tl γb, β  .  Given k and l, the product Tk α, γa  K a, b −1 Tl γb, β  expands to eight terms, each of which usually sums to ±1 when multiplied by P  γab  and summed over all γab. For example, the sum of P  γab K 2 : α, γa : 2 K a, b K 3 : γb, β : 3 , when α = a1 . . . at,   ANSWERS TO EXERCISES  566 3.3.2 β = b1 . . . bt, γ = c1 . . . ct−1, and t ≥ 5, is the sum of P  c4 . . . ct−2 , which is 1. If t = 4, the same sum would be K a1, b4 , but it would cancel with the sum of P  γab K 2 : α, γa : 2  −1 K 3 : γb, β : 3 . The net result is therefore 0 unless k ≤ 0 ≤ l; otherwise it turns out to be K i :  α : i − k , i :  β : i + l   − K i − 1 :  α : i − k , i − 1 :  β : i + l  , where i = min t + k, t − l . The sum over k and l telescopes to cαβ. 25. Empirical tests show, in fact, that when  22  is generalized to arbitrary t the ratios are very nearly −t, when t ≥ 5. For of corresponding elements of C−1 1 example, when t = 6 they all lie between −6.039 and −6.111; when t = 20 they all lie between −20.039 and −20.045. This phenomenon demands an explanation. 26.  a  The vectors  S1, . . . , Sn  are uniformly distributed points in the  n − 1 - dimensional polyhedron defined by the inequalities S1 ≥ 0, . . . , Sn ≥ 0 in the hyper- plane S1 + ··· + Sn = 1. An easy induction proves that dtn−1 [1 − t1 − ··· − tn−1 ≥ sn] =   1 − s1 − s2 − ··· − sn n−1   ∞   ∞   ∞  1 C2 C−1 1  and C−1  dt2 ···  dt1   n − 1 !  +  .  s1  s2  sn−1  To get the probability, divide this integral by its value in the special case s1 = ··· = sn = 0. [Bruno de Finetti, Giornale Istituto Italiano degli Attuari 27  1964 , 151–173.]  b  The probability that S 1  ≥ s is the probability that S1 ≥ s, . . . , Sn ≥ s.  c  The probability that S k  ≥ s is the probability that at most k − 1 of the  times the probability that that exactly j spacings are < s. By symmetry, Gj s  isn Sj are < s; hence 1 − Fk s  = G1 s  + ··· + Gk−1 s , where Gj s  is the probability S1 < s, . . . , Sj < s, Sj+1 ≥ s, . . . , Sn ≥ s; and the latter is Pr S1 < s, . . . , Sj−1 < s, j  application of  a  shows that Gj s  =n Sj ≥ 0, Sj+1 ≥ s, . . . , Sn ≥ s −Pr S1 < s, . . . , Sj−1 < s, Sj ≥ s, . . . , Sn ≥ s . Repeated   n − l − 1   −1 j−l 1 −  n − l s n−1  + ; hence  j  l  l  k − l − 1   −1 k−l−1 1 −  n − l s n−1 + .  In particular, the largest spacing S n  has distribution  l  j  l   n 1 − Fk s  =   n−l−1  n Fn s  = 1−  d  From the formulas E sr = r 1  n−l−1  l  l  r   2   n − H  2  n − H  2   0 sr 1 − ks n−1  n−1 n + 1 −1 H n−1 n + 1 −1 H  k−r−1n−1n+r  [Incidentally, the similar quantity xn−1 n − 1 !−1Fn x−1  turns out to be the density function for the sum U1 + ··· + Un of uniform deviates.]  −1, we find E S k  = n−1 Hn−Hn−k  and, with a bit of algebra, E S2 + ds =  k  = n−k +  Hn − Hn−k 2 . Thus the variance of S k  is equal to n−k −  Hn − Hn−k 2 n . [The distributions Fk s  were first found by W. A. Whitworth, in problem 667 of DCC Exercises in Choice and Chance  Cambridge, 1897 . Whitworth also discovered an elegant way to compute the expected value of any polynomial in the functions Gk s  = Fk s  − Fk+1 s ; this was published in a booklet entitled The Expectation of Parts  Cambridge, 1898 , and incorporated into the fifth edition of Choice and Chance  1901 . Simplified expressions for the mean and variance and for a variety of more general spacing statistics were found by Barton and David, J. Royal Stat. Soc. B18  1956 , 79–94. See R. Pyke, J. Royal Stat. Soc. B27  1965 , 395–449, for a survey of   2      n + = 0  1 − F  s  sr−1 ds and  1  l  l   −1 n−l−1 1−  n− l s n−1   −1 l 1− ls n−1 + .   3.3.2  ANSWERS TO EXERCISES  567  the ways in which statisticians have traditionally analyzed spacings as clues to potential biases in data.] 27. Consider the polyhedron in the hyperplane S1 + ··· + Sn = 1 defined by the inequalities S1 ≥ 0, . . . , Sn ≥ 0. This polyhedron consists of n! congruent subpolyhedra defined by the ordering of the S’s  assuming that the S’s are distinct , and the operation of sorting is an n!-to-1 folding of the large polyhedron to the subpolyhedron in which S1 ≤ ··· ≤ Sn. The transformation that takes  S 1 , . . . , S n   to  S′1, . . . , S′n  is a 1-to-1 mapping that expands differential volumes by the factor n!. It takes the vertices   1 n−1 , . . . ,  0, . . . , 0, 1  of the subpolyhedron into the respective vertices  1, 0, . . . , 0 ,  0, 1, 0, . . . , 0 , . . . ,  0, . . . , 0, 1 , linearly stretching and distorting the overall shape in the process.  The Euclidean distance between vertices k   in the subpolyhedron is j−1 − k−11 2; the  0, . . . , 0, 1 transformation produces a regular simplex in which all n vertices are  j   and  0, . . . , 0, 1  1 n−1 , . . . ,  n , . . . , 1  k , . . . , 1  j , . . . , 1  2 apart.   n ,  0,  √  1  The behavior of iterated spacings is easiest to understand if we examine the details graphically when n = 3. In this case the polyhedron is simply an equilateral triangle, whose points are represented with barycentric coor- dinates  x, y, z , x + y + z = 1. The accompanying diagram illustrates the first two levels of a recursive decom- position of this triangle. Each of the 62 subtriangles has been labeled with a two-digit code pq, where p repre- sents the applicable permutation when  x, y, z  =  S1, S2, S3  is sorted into  S 1 , S 2 , S 3  , and q represents the permutation in the next stage when S′1, S′2, and S′3 are sorted, according to the following code: 3: y < z < x, 0: x < y < z,  1: x < z < y,  2: y < x < z,  4: z < x < y,  5: z < y < x.  For example, the points of subtriangle 34 have S2 < S3 < S1 and S′3 < S′1 < S′2. We can continue this process to infinitely many levels; all points of the triangle with irrational barycentric coordinates thereby acquire a unique representation as an infinite radix-6 expansion. A tetrahedron can be subdivided similarly into 24, 242, 243, . . . subtetrahedra, and in general this procedure constructs a radix-n! expansion for the points of any  n − 1 -dimensional simplex. 2 , 1}, the transforma- tion takes spacings  x, 1 − x  =  x, y  into either  2x mod 1, 2y mod 1  or  2y mod 1, 2x mod 1 , depending on whether x   y. Repeated tests therefore essentially shift the binary representation left one bit, possibly complementing the result. After at most e+1 iterations on e-bit numbers the process must converge to the fixed point  0, 1 . Permutation coding in the case n = 2 corresponds simply to folding and stretching a line; the first four levels of subdivision have the following four-bit codes:  When n = 2 the process is especially simple: If x  ∈ {0, 1   0, 1    1, 0   0000 0001 0011 0010 0110 0111 0101 0100 1100 1101 1111 1110 1010 1011 1001 1000   0, 0, 1   x < z  y < z  20  00  22  02  21  24  23  03  01  04  25  05  34  35  14  15  x > z  31  30  32  33  53  51  52  50  55  45  54  44  11  10  12  13  43  41  42  40  y > z   1, 0, 0   x > y  x < y   0, 1, 0    568  ANSWERS TO EXERCISES  3.3.2  This sequence is exactly the Gray binary code studied in Section 7.2.1. In general, the radix-n! permutation code for an n-simplex has the property that adjacent regions have identical codes except in one digit position. Each iteration of the spacing transformation shifts off the leftmost digit of the representation of each point. Note that equal birthday spacings are points near the boundary of the first-level decomposition.  This fundamental transformation from  S1, . . . , Sn  to  S′1, . . . , S′n  is implicit in Whitworth’s proof of Proposition LVI in the fifth edition of Choice and Chance  see the reference in answer 26 . It was first studied explicitly by J. Durbin [Biometrika 48  1961 , 41–55], who was inspired by a similar construction of P. V. Sukhatme [Annals of Eugenics 8  1937 , 52–56]. The permutation coding for iterated spacings was introduced by H. E. Daniels [Biometrika 49  1962 , 139–149].  by exercise 5.1.1–16. These partitions can be permuted in n! ways to yield n-tuples  y1, . . . , yn  with 0 = y1 < y2 < ··· < yn < m; and each of these n-tuples leads to  n−1 ! n-tuples that have y1 = 0 and 0 < y2, . . . , yn < m. Now add a constant mod m  b  Zero spacings correspond to balls in the same urn, and they contribute s − 1  28.  a  The number of partitions of m into n distinct positive parts is pn m−n+1  , to each yj; this preserves the spacings. Hence bn00 m  = mn! n − 1 ! pn m −n+1  . to the count of equal spacings. Therefore bnrs m  = n  b n−s  r+1−s 0 m .  − 1  m − n   , the probability is  m − n + 1    c  Since n   =n  n! n − 1 ! m  1−n  n−1  n−s  2  2  2  .  pn  2  2 pn−1  2  29. By the previous answer and exercise 5.1.1–15 we have bn0 z  = n!  n − 1 ! z n+1 2     1 − z  . . .  1 − zn . When r = 1, the n! in our previous derivation becomes n! 2, and the number of solutions to 0 < s1 < ··· < sk ≤ sk+1 < ··· < sn with s1 + ··· + sn = m is the number of solutions to 0 ≤ s1 − 1 ≤ ··· ≤ sk − k ≤ sk+1 − k ≤ ··· ≤ sn − n + 1 2   1− z  . . .  1− zn . A similar argument shows  with  s1 − 1  + ··· +  sk − k  +  sk+1 − k  + ··· +  sn − n + 1  = m −n   − k. Hence  2  2 n!  n− 1 !n k=1 zk − zn  z n  1   bn1 z  = 1 that  bn2 z  n! n−1 ! =  2!2!  1≤j<k<n   zj − zn  zk − zn−1 + 1 3!   zk − zn  zk − zn−1     1≤k<n    ×  z n−1 2     1− z  . . .  1− zn  .   w  b1  zn−1   w  bn−1  . . .  z1  We can obtain bnr z  for general r from the formula   n!  n − 1 ! zn =   r bnr z wr  0≤b1,...,bn−1≤1   z − b1zn  . . .  zn−1 − bn−1zn  c1 . . . cn−1 1 − z  . . .  1 − zn   where ck = 1 + bk + bkbk−1 + ··· + bk . . . b2b1 = 1 + bkck−1.  The special case w = 1 is interesting because the left side sums to  1 − z −n n! in that case.   ef z  dz 30. This is a good problem for the saddle point method [N. G. de Bruijn, Asymp- totic Methods in Analysis  North-Holland, 1961 , Chapter 5]. We have pn m  =  π δ 1 n m; 2πi −π δ exp f e−ρ+itδ   dt. It is integrating on the path z = e−ρ+itδ gives pn m  = δ 2π  z , where f z  = −m ln z −n  k=1 ln 1 − zk . Let ρ = n m and δ =  √   n    k=1  l≥j  lj Bl l · l! klρl−j,  3.3.2  ANSWERS TO EXERCISES  569  convenient to use the identity  n  j=0   t  0  g set  =  tj j! ϑjg s  +  un n! ϑn+1  g set−u  du ,  where g = g z  is any analytic function and ϑ is the operator z d dz . When the function ϑjg is evaluated at ez the result is the same as when g ez  is differentiated j times with respect to z. This principle leads to the formula  because of another handy identity,  ϑjf e−ρ  = −m[ j = 1] + j! n  ln  1 − e−z  ρj +  −1 j  =  ijδjtj j! ϑjf e−ρ   n≥1  z  Bnzn n · n! .    j≥0  Therefore we obtain an asymptotic expansion of the integrand,  exp f e−ρ+itδ  = exp  = e−t2 2+f e−ρ  exp ic1t− c2t  2− ic3t  3+··· ,  where c1 =   n n+1  cj = O n−3  for j ≥ 3. Factoring out the constant term  2 B1 + n n+1 2  n+1   6  B2ρ δ + O n−3 , etc.; and it turns out that  ef e−ρ  =  δ 2π   2π n! ρne−mρ exp  √ n mn−1en+α 4  δ  2π n! nn   − n 1 + 18α − α2 72n  l≥1  k=1  =  Bl l · l! klρl    + 108α2 − 36α3 + α4  10368n2  + O n−3     2  −∞  2πn exp  1  12 n−1 + O n−3   suffice to complete the evaluation.  With qn m  = pn m −n+1  leaves us with an integral whose integrand is exponentially small when t ≥ nϵ. We can ignore larger values of t, because partial fraction expansion shows that the integrand is O  m n n 2 ; none of the other roots of unity occurs more than n 2 times as a integrate over all t. The formulas  ∞ pole of the denominator. Hence we are allowed to “trade tails” [CMath, §9.4] and √ e−t2 2tj dt =  j − 1  j − 3  . . .  1  2π [j even] and n! =  n e n√  . We get exp −ρn+1 same way but with c1 increased by 1  qn m  = mn−1e−α 4 1 − 13α2 ; n!  n − 1 ! 288n if we define pn m  = rn 2m +n+1 this matches the formula for pn m  except that α has been changed to −α.  In fact, function Rn z  =  m rn zm  = n k=1 z−k − zk −1 satisfies Rn 1 z  =  −1 nRn z . This implies a duality formula rn −m  =  −1 n−1rn m , in the sense that this equation is identically true when we express rn m  as a polynomial in m and roots of unity. Therefore we may say that qn m  = pn −m . A general treatment of such duality can be found in G. Pólya, Math. Zeitschrift 29  1928 , 549–640, §44.  For further    in place of pn m  the calculation proceeds in the 2 α n1 2 − n−1 2  and with the additional factor   , the generating  + 169α4 − 2016α3 − 1728α2 + 41472α   and qn m  = rn 2m −n+1  + O n−3   165888n2  2  2  2   570  ANSWERS TO EXERCISES  3.3.2  2  ρ + O n3ρ2  + itO n2δ  − 1  The exact value of qn m  when m = 225 and n = 512 is 7.08069 34695 90264  1≤j<k<n z−j − 1  z−k − 1  essentially multiplies by 1  because gn e−ρ+itδ  =n extra factor  2n e−α 4 = O n−1 . Inserting the factor gn z  = n−1  information see G. Szekeres, Quarterly J. Math. Oxford 2  1951 , 85–108; 4  1953 , 96–111. 094 . . . × 101514; our approximation gives the estimate 7.080693501 × 101514. The probability that the birthday test finds R = 0 spacings is bn00 m  mn = n!  n − 1 ! m1−nqn m  = e−α 4 + O n−1 , by exercise 28, because the contribution from bn01 m  is ≈ α k=1 z−k − 1  2 + O n−1 , into the integrand for qn m  has the effect of multiplying the result by α 2 t2O n3δ2  + ··· . Similarly, the 8 α2, plus O n−1 ; other contributions to the probability that R = 2 are O n−1 . In this way we find that the probability of r equal spacings is e−α 4 α 4 r r! + O n−1 , a Poisson distribution; more complicated terms arise if we carry the expansion out to O n−2 . 31. The 79 bits consist of 24 sets of three, {Yn, Yn+31, Yn+55}, {Yn+1, Yn+32, Yn+56}, . . . , {Yn+23, Yn+54, Yn+78}, plus 7 additional bits Yn+24, . . . , Yn+30. The latter bits are equally likely to be 0 or 1, but in each group of three the probability is 1 4 that the bits 4 that they will be {0, 1, 1}. Therefore the probability generating will be {0, 0, 0} and 3 function for the sum of bits is f z  =   1+z  24, a polynomial of degree 55.  Well, not quite; strictly speaking, it is  255f z  − 1   255 − 1 , because the all-0 case is excluded.  The coefficients of 255f z  are easily computed by machine, and we find that the probability of more 1s than 0s is 18509401282464000  255 − 1  ≈ 0.51374.  2  7  1+3z2 4  8 n4ρ2 = 1  Notes: This exercise is based on the discovery by Vattulainen, Ala-Nissila, and Kankaala [Physical Review Letters 73  1994 , 2513–2516] that a lagged Fibonacci generator fails a more complicated two-dimensional random walk test. Notice that the sequence Y2n, Y2n+2, . . . will fail the test too, because it satisfies the same recurrence. The bias toward 1s also carries over into the subsequence consisting of the even- valued elements generated by Xn =  Xn−55 ± Xn−24  mod 2e; we tend to have more occurrences of   . . . 10 2 than   . . . 00 2 in binary notation. There’s nothing magic about the number 79 in this test; experiments show that a significant bias towards a majority of 1s is present also in random walks of length 101 or 1001 or 10001. But a formal proof seems to be difficult. After 86 steps the generating function is   1+3z2  7; then we get the factors  1 + 2z2 + 5z3 + 5z4 + 10z5+8z6+z7  32; then  1+2z2+7z3+7z4+15z5+25z6+29z7+28z8+13z9+z10  128, etc. The analysis becomes more and more complicated as the walks get longer.   17  1+2z2+4z3+z4  Intuitively, the preponderance of 1s that arise in the first 79 steps ought to persist as long as the subsequent numbers are reasonably balanced between 0 and 1. The accompanying diagram shows the results of a much smaller case, the generator Yn =  Yn−2 + Yn−11  mod 2, which is easy to analyze exhaustively. In this case random walks of length 445 have a 64% chance of finishing to the right of the starting point; this bias disappears only when the length of the walk increases to half the period length  after which, of course, 0s are more likely, although the full period does lack one 0 .  4  8  The probability that 1s outnumber 0s in random m-tuples when Yn = Yn−2 ⊕ Yn−11.  0.7  0.6  0.5  0.4  0.3  m=0  256  512  768  1024  1280  1536  1792  2047   3.3.2  ANSWERS TO EXERCISES  571  2  dz  u = 1  eu2 2 du  2 and 1 2πi  √ 2 m.   eg z   Lüscher’s discarding technique can be used to avoid the bias toward 1s  see the end of Section 3.2.2 . For example, with lags 55 and 24, no deviation for randomness is observed for random walks of length 1001 when the numbers are generated in batches of 165, if only the first 55 numbers of each batch are used. 32. Not if, say, X and Y each take the values  −n, m  with the respective probabilities  m  m+n , n  m+n  , where m < n <  1 + [Suppose two competitors differ by X after playing one round of golf. Then they are of equal strength based on their mean scores, but one might be more likely to win a one-round tournament while the other will more often win in two rounds. See T. M. Cover, Amer. Statistician 43  1989 , 277–278, for a discussion of similar phenomena.]  l  1 − z . Let m = k − 2l and 33. We essentially want [z k+l−1  2]   1+z  − n = l; the desired coefficient is 1 2πi   m+3n−1   ln z. It is convenient  and saddle-wise  to integrate along the path z = eϵu where ϵ2 = 4  m + 3n  and u = −1 + it for −∞ < t < ∞. We have g eϵu  = −ϵu 2+u2 2+c3ϵu3+c4ϵ2u4+··· , where ck = ϵ2ϑkg 1  k! = O 1 . Also 1  1 − eϵu  = 2 − B2ϵu 2! − ··· . Multiplying out the integrand and using the facts that −1 ϵu + 1 1 2π 2πi yields the asymptotic formula 1 If m + 3n is even, the same asymptotic formula holds, provided that we give half of the π m+3n  1 2 + coefficient of z m+3n  2 to the 1s and half to the 0s.  This coefficient is   O  m−3n −3 2 .  34. The number of strings of length n that exclude a given two-letter substring or pair of substrings is the coefficient of zn in an appropriate generating function, and it can be written cenτ mn + O 1  where c and τ have series expansions in powers of ϵ = 1 m: Case Excluded Generating function  √ eu2 2u2k du =  −1 k 2k − 1  2k − 3  . . .  1  a−i∞ 2 +  2π −1 2n m + 3n −3 2 + O  m + 3n −3 2 .  2  k−2l  1+3z2 4 z 1−z , where g z  = m ln  1+z   1+i∞1−i∞  2  +n ln  1+3z2 4   a+i∞  aa, bb aa, bc ab, bc ab, cd  −ϵ2+ϵ3− 5 −ϵ2− 3   1+z  p z  1  1−mz+z2   1+z   p z +z2   1+z   p z +z2+z3   1+z   1−mz+2z2−z3  1  1−mz+2z2   1+ϵ2−2ϵ3+ ··· 2 ϵ4+ ··· 1+ϵ2+3ϵ4+ ··· 1+2ϵ2−4ϵ3+ ··· −2ϵ2+2ϵ3−8ϵ4+ ··· 1+2ϵ2−2ϵ3+ ··· −2ϵ2+ϵ3−7ϵ4+ ··· 1+2ϵ2−2ϵ3+ ··· −2ϵ2+ϵ3−6ϵ4+ ··· 1+2ϵ2+12ϵ4+ ··· −2ϵ2−6ϵ4+ ···  1 2 3 4 5 6  Here a, b, c, d denote distinct letters and p z  = 1 −  m − 1  z + z2 . It turns out that the effect of excluding {ab, ba} or {aa, ab} is equivalent to excluding {aa, bb}; excluding {ab, ac} is equivalent to excluding {ab, cd}.  Let S  j  n be the coefficient of zn in Case j and let X be the total number of two-letter combinations that do not appear. Then E X =  mS  2 ϵ4+ ···   2  n   mn and   1  n + m2 S  aa ab  τ  c  2  n=0  3 S  2 S  E X   5  n + S   4  n + S   1  n + m   2  n + 6S  2 =  mS  4  6  n   + m   3  n   + 2m  35.  a  E Sm = N−1N−1 N−1  n=0 Zn+j = m N, because n=0 Zn+j = 2k−1 −  2k−1 − 1  = 1.  b  Let ξk = a1ξk−1 + ··· + ak, and define the linear function f as in the first solution to exercise 3.2.2–16. Then Yn = f ξn , and it follows that Yn+i + Yn+j = f ξn+i  + f ξn+j  ≡ f ξn+i + ξn+j  = f ξnα   modulo 2 , where α is nonzero when i ̸≡ j  modulo N . Hence E S2 n=0 Zn+iZn+j =  m−1 j=0 Zn+j = N−1m−1 m = N−1m−1 N−1  N−1 m−1  n=0 Zn  = m − m m − 1  N.  n+i − 2  m−1  N−1  N−1  N−1  n   mn.  6   n=0 Z2  j=0  j=0  i=0  i=0  S  0≤i<j<m   572    ANSWERS TO EXERCISES   c  Em−1 n+j + 0≤i<j<m E Zn+i  E Zn+j  = m when each Zn is truly random. Thus the mean and  d  E S3 n=0 Zn+hZn+iZn+j. If any of h, i, or j  j=0 E Zn+j = 0 and E m−1 j=0 Zn+j = m−1 m−1 m = N−1m−1   j=0 Zn+j 2 = m−1  variance of Sm are very close to the correct values when m ≪ N.  i=0 are equal, the sum on n is 1; hence  N−1 m−1 3 + 6   j=0 E Z2  Zn+hZn+iZn+j    E S  h=0  j=0  m  .  3.3.2  3  3 − m  N−1 m = 1 m = m3−6B N +1  N, where B =  0≤h<i<j<m  n=0  N    only if f ξi+l  = f ξj+l  for 0 < l < k, assuming that 0 < i < j < N.  Arguing as in  b , we find that the sum on n will be 1 if ξh+ξi+ξj ̸= 0; otherwise it will be −N. Thus E S3 0≤h<i<j<m[ξh + ξi + ξj = 0] = 0<i<j<m[1 + ξi + ξj = 0]  m − j . Finally observe that 1 + ξi = ξj in the field if and  e  The only nonzero term occurs for i = 31 and j = 55; hence B = 79 − 55 = 24.  The next nonzero term occurs when i = 62 and j = 110.  In a truly random situation, 79 ≈ −144 is distinctly nonrandom. Curiously it E S3 is negative, although exercise 31 showed that S79 is usually positive. The value of S79 tends to be more seriously negative when it does dip below zero.  m should be zero, so this value E S3  Reference: IEEE Trans. IT-14  1968 , 569–576. Experiments by M. Matsumoto and Y. Kurita [ACM Trans. Modeling and Comp. Simul. 2  1992 , 179–194; 4  1994 , 254–266] confirm that trinomial-based generators fail such distribution tests even when the lags are quite large. See also ACM Trans. Modeling and Comp. Simul. 6  1996 , 99–106, where they exhibit exponentially long subsequences of low density.  SECTION 3.3.3 1. y  x y   + 1  2.   x   = −  2 y − 1 n≥1  2 yδ x y . 1 nπ sin 2πnx, which converges for all x.  The representation in Eq.  24  may be considered a “finite” Fourier series, for the case when x is rational.  3. The sum is   2nx   −   x  . [See Trans. Amer. Math. Soc. 65  1949 , 401.] 4. dmax = 210 · 5. Note that we have Xn+1 < Xn with probability 1 2 + ϵ, where  ϵ < d  2 · 1010  ≤ 1  2 · 59 ;  hence every potency-10 generator is respectable from the standpoint of Theorem P.  x m  0≤x<m    s x  m  6.  a  Use induction and the formula  5. An intermediate result: = 1 4 − c 12 σ a, m, c  + m 2m   hj + c   hj + c − 1  hj + c  b  Use the fact that − h′j  = − j  − hj − hj − 1 kj  hj hj +2  − 1 2 δ − k′j h      = 1  −  =  hk  k  k  k  k  k  k  k  2  + 1 2  0<j<h  h  0<j<k  k  k  − x′ 2m  .    k′j  .  h  k  − 1 2 δ   hj + c − 1 − j  k′j + 1 2 δ − kj    + 1 2  h  j = kh h−1 .    .  hk 7. Take m = h, n = k, k = 2 in the second formula of exercise 1.2.4–45:  h   3.3.3  ANSWERS TO EXERCISES  573  The sums on the left simplify, and by standard manipulations we get  2  2  k − hk.  4 − h  2 + h2 6k  k − hk − h  12 + 1 + k  12 σ 1, k, 0  = h  h Since σ 1, k, 0  =  k − 1  k − 2  k, this reduces to the reciprocity law. 8. See Duke Math. J. 21  1954 , 391–397.  6 σ h, k, 0  − h 9. Begin with the interesting identity r−1 q−1  6 σ k, h, 0  + 1 k=0⌊kp r⌋⌊kq r⌋ +p−1  k=0⌊kq p⌋⌊kr p⌋ + k=0⌊kr q⌋⌊kp q⌋ =  p − 1  q − 1  r − 1 , for which a simple geometric proof is [U. Dieter, Abh. Math. Sem. Univ.  possible, assuming that p ⊥ q, q ⊥ r, and r ⊥ p. Hamburg 21  1957 , 109–125.] 10. Obviously σ k − h, k, c  = −σ h, k,−c , by  8 . Replace j by k − j in definition  16 , to deduce that σ h, k, c  = σ h, k,−c .  11.  a     j  hj + c + θ   hj + c    =    hj + c  0≤i<d 0≤j<k  0≤j<dk  dk  k   ik + j  dk     hj + c   hj + c  k  ; use  10  to sum on i.   b   =  k  k  = 12    k 12. Since    hj+c k   in some order, Cauchy’s k    runs through the same values as    j inequality implies that σ h, k, c 2 ≤ σ h, k, 0 2; and σ 1, k, 0  may be summed directly, see exercise 7. 13. σ h, k, c  + 3 k − 1  k if hh′ ≡ 1  modulo k . 14.  238 −3·220 +5   270 −1  ≈ 2−32. An extremely satisfactory global value, in spite of the local nonrandomness! 15. Replace c2 where it appears in  19  by ⌊c⌋⌈c⌉. follows by induction.  See also exercise 4.5.3–32.  Now replace cj by  16. The hinted identity is equivalent to m1 = prmr+1 + pr−1mr+2 for 1 ≤ r ≤ t; this j≤r≤t brmr+1 and compare coefficients of bibj on both sides of the identity to be proved.   ω−hj − 1  ωj − 1  + 6   h′c   c mod k  − 6    0<j<k  ω−cj  k  k  k  ,  + θ k  − 1 2 δ  ; now sum.  Note: For all exponents e ≥ 1, a similar argument gives j − ce ce j+1 cj − cj+1  = 1 m1   −1 j+1   −1 j+1      mjmj+1  ce j  bj  pj−1.  1≤j≤t  1≤j≤t  17. During this algorithm we will have k = mj, h = mj+1, c = cj, p = pj−1, p′ = pj−2, s =  −1 j+1 for j = 1, 2, . . . , t + 1.  D1. [Initialize.] Set A ← 0, B ← h, p ← 1, p′ ← 0, s ← 1. D2. [Divide.] Set a ← ⌊k h⌋, b ← ⌊c h⌋, r ← c mod h.  Now a = aj, b = bj, and  r = cj+1.   D3. [Accumulate.] Set A ← A +  a− 6b s, B ← B + 6bp c + r s. If r ̸= 0 or c = 0, set A ← A − 3s. If h = 1, set B ← B + ps.  This subtracts 3e mj+1, cj  and  also takes care of the −1 j+1 mjmj+1 terms.   D4. [Prepare for next iteration.] Set c ← r, s ← −s; set r ← k − ah, k ← h,  h ← r; set r ← ap + p′, p′ ← p, p ← r. If h > 0, return to D2.   574  ANSWERS TO EXERCISES  3.3.3  At the conclusion of this algorithm, p will be equal to the original value k0 of k, so the desired answer will be A+ B p. The final value of p′ will be h′ if s < 0, otherwise p′ will be k0 − h′. It would be possible to maintain B in the range 0 ≤ B < k0, by making appropriate adjustments to A, thereby requiring only single-precision operations  with double-precision products and dividends  if k0 is a single-precision number. 18. A moment’s thought shows that the formula  is in fact valid for all z ≥ 0, not only when k ≥ z. Writing ⌊j k⌋ − ⌊ j − z  k⌋ = k    −    j k +    j−z S h, k, c, z  = zd k  0≤j<k ⌊j k⌋ − ⌊ j − z  k⌋     hj + c  k    − 1 k   and carrying out the sums yields 2 δ  j−z  2 δj0 − 1 + 1 12 σ h, k, hz + c  − 1  12 σ h, k, c  + 1 2   hz + c   c    2  k  k  ,  z  S h, k, c, z  =  c  k   + 1    [This formula allows us to express the probability that Xn+1 <  d where d = gcd h, k . Xn < α in terms of generalized Dedekind sums, given α.] 19. The desired probability is  m−1  = m−1  m  m  m  m  x=0   x − α  − x − β m−1  β − α  x − β m−1  s x −β′ × β′−α′    − s x  − β′   s x  − α′  x − β  x − α    − 1  − x − α  s x −β′ − 1  s x −α′  − s x −α′   m σ a, m, c + aα − α′  − σ a, m, c + aα − β′   + + 1 12m + σ a, m, c + aβ − β′  − σ a, m, c + aβ − α′   + 1 2 δ + 1 2 δ  m β′ − α′  + ϵ,  2 δ  2 δ  x=0  +  m  m  m  m  m  m  m  m  m  m  = β − α    where ϵ ≤ 2.5 m.  m is bounded by t  m  β′  −α′  m the discrepancy cannot exceed O 1   0≤x<m⌈ x − s x   m⌉⌈ s x  − s s x    m⌉ m =   [This approach is due to U. Dieter. The discrepancy between the true probability j=1 aj 4m, according to Theorem K; and the ideal value β−α conversely, by choosing α, β, α′, β′ appropriately we will obtain a discrepancy of at least half this bound when there are large partial quotients, using the fact that Theorem K is “best possible.” Note that when a ≈ √ √ m  , so even the locally nonrandom generator of exercise 14 will look good on the serial test 20.  over the full period; it appears that we should insist on an extremely small discrepancy.] 0≤x<m  x − s x   m + 2  m; and x m =   x m  + 2 δ  ax + c  m , s s x   m =    a2x + 2 δ  a2x + ac + c  m . Let s x′  = s s x′′   = 0 and d = gcd b, m .  c  − c − ac     bx+c  m  + 1 2 − 1 1 ac + c  m   + 1 The sum now reduces to 1 4 + 1 12m + 1 2m  2   s x −s s x    m+  a bx+c  m  + 1 2 − 1   S1 − S2 + S3 − S4 + S5 − S6 + S7 − S8 + S9  + d m  2 δ x m , s x  m =    ax + c  m   + 1   x′ − x′′  − x′   ac + c  −1   x′′      2 − 1    +  +  m  m  m  m  m  m  2  d  ,   3.3.3  ANSWERS TO EXERCISES  575  where S1 = σ a, m, c , S2 = σ a2, m, ac + c , S3 = σ ab, m, ac , S4 = σ 1, m, 0  =  m − 1  m − 2  m, S5 = σ a, m, c , S6 = σ b, m, c , S7 = −σ a′ − 1, m, a′c , and S8 = −σ a′ a′ − 1 , m,  a′ 2c , if a′a ≡ 1  modulo m ; and finally  m  m  = 12d  0≤x<m     a bx + c    bx + c S9 = 12   a x + c0 d   x + c0 d    x   ac0  0≤x<m d σ ad, m, ac0  + 12 c0 m   − 6  0≤x<m d  + c0 m  = 12d  − 1  2 δx0    = d  m d  m d  m d  d    a x + c0 d   ac0    m d  m    where c0 = c mod d. The grand total will be near 1 6 when d is small and when the fractions a m,  a2 mod m  m,  ab mod m  m, b m,  a′ − 1  m,  a′ a′ − 1  mod m  m,   ad  mod m  m all have small partial quotients.  Note that a′ − 1 ≡ −b + b2 − ··· , as in exercise 3.2.1.3–7.  21. Notice first that the main integral decomposes nicely:   xn+1  1   2 + n 2 x{ax+ θ} dx = s0 + s1 +···+ sa−1 +  x{ax+ θ} dx = 1 a2  1 3 − θ  xn  ,  sn =  s =  0   0  ;  if xn = n− θ  ax+ θ  dx = 1 3a  a  −θ a  − θ 2a  + a−1 4a  + θ2 2a  .  Therefore C =  s −   1 22. We have s x  < x in the disjoint intervals [ 1−θ which have total length  2 2  =  1 − 6θ + 6θ2  a. a . . 1−θ  2 2    1  3 −   1  1 +   0<j≤a−1   j − θ  a − 1   −    j − θ    a  0<j≤a  = 1 + a  a . . j−θ a . . j−θ  23. We have s s x   < s x  < x when x is in [ k−θ a−1 , for 0 < j ≤ k < a; or when x is in [ a−θ [ j−θ a−1  for 0 < j ≤ ⌊aθ⌋ or in [ ⌊aθ⌋+1−θ [ j−θ a2 a − 1  +   j − θ a2 a − 1  + 1 ⌊aθ⌋ ⌊aθ⌋ + 1 − 2θ  + 1 a2  0<j≤⌊aθ⌋ − θ 2a  = 1 6 + 1 6a  2 a − 1   0<j≤k<a  j − θ  a  a . . 1 ,  a−1 , [ 2−θ a . . 2−θ 2 − θ − a + 1  a−1 , . . . , [ a−θ 2 + θ = 1 2 .  a−1  and ax + θ − k is in a . . k−θ a . . 1  and ax + θ − a is either in  . . θ . The desired probability is a2 max 0,{aθ} + θ − 1     + max 0,{aθ} + θ − 1   ,  6 +  1 − 3θ + 3θ2  6a + O 1 a2  for large a. Note that 1 − 3θ + 3θ2 ≥ 1  which is 1 can’t be chosen to make this probability come out right. 24. Proceed as in the previous exercise; the sum of the interval lengths is  4, so θ    0<j1≤···≤jt−1<a  j1  at−1 a − 1  =  at−1 a − 1   1   a + t − 2    .  t   576  ANSWERS TO EXERCISES  3.3.3  Fig. A–1. Permutation regions for the Fibonacci generator.  Fig. A–2. Run-length regions for the Fibonacci generator.  To compute the average length, let pk be the probability of a run of length ≥ k; the  average is   pk =  k≥1  k≥1   a + k − 2    k   a  a − 1  a − a  a − 1 .  1  ak−1 a − 1  =  The value for a truly random sequence would be e − 1; and our value is e − 1 +  e 2 − 1  a + O 1 a2 . [Note: The same result holds for an ascending run, since we have Un > Un+1 if and only if 1 − Un < 1 − Un+1. This would lead us to suspect that runs in linear congruential sequences might be slightly longer than normal, so the run test should be applied to such generators.] 25. x must be in the interval [ k + α′ − θ  a . .  k + β′ − θ  a  for some k, and also in the interval [α . . β . Let k0 = ⌈aα + θ − β′⌉, k1 = ⌈aβ + θ − β′⌉. With due regard to boundary conditions, we get the probability   k1 − k0  β′ − α′  a + max 0, β −  k1 + α′ − θ  a  − max 0, α −  k0 + α′ − θ  a .  This is  β − α  β′ − α′  + ϵ, where ϵ < 2 β′ − α′  a. 26. See Fig. A–1. The orderings U1 < U3 < U2 and U2 < U3 < U1 are impossible; the other four each have probability 1 4. 27. Un = {Fn−1U0 + FnU1}. We need to have both Fk−1U0 + FkU1 < 1 and FkU0 + Fk+1U1 > 1. The half-unit-square in which U0 > U1 is broken up as shown in Fig. A–2, with various values of k indicated. The probability for a run of length k is 1 2, if k = 1; it is 1 Fk−1 Fk+1 − 1 Fk Fk+2, if k > 1. The corresponding probabilities for a random sequence are 2k  k + 1 ! − 2 k + 1   k + 2 !; the following table compares the first few values.  k:  Probability in Fibonacci case: Probability in random case:  1 1 2 1 3  2 1 3 5 12  3 1 10 11 60  4 1 24 19 360  5 1 65 29 2520  28. Fig. A–3 shows the various regions in the general case. The “213” region means U2 < U1 < U3, if U1 and U2 are chosen at random; the “321” region means that 4 − α 2 + α2 2; the U3 < U2 < U1, etc. The probabilities for 123 and 321 are 1 probabilities for all other cases are 1 6, we must have  8 +α 4−α2 4. To have all equal to 1  123  321  312  213  1  2  3  4  5≥6   3.3.4  ANSWERS TO EXERCISES  577  Fig. A–3. Permutation regions for a generator with potency 2; α =  a − 1 c m.  1 − 6α + 6α2 = 0. [This exercise establishes a theorem due to J. N. Franklin, Math. Comp. 17  1963 , 28–59, Theorem 13; other results of Franklin’s paper are related to exercises 22 and 23.]  SECTION 3.3.4 1. For generators of maximum period, the 1-D accuracy ν1 is always m, and µ1 = 2. 2. Let V be the matrix whose rows are V1, . . . , Vt. To minimize Y · Y, subject to the condition that Y ̸=  0, . . . , 0  and V Y is an integer column vector X, is equivalent to minimizing  V −1X ·  V −1X , subject to the condition that X is a nonzero integer column vector. The columns of V −1 are U1, . . . , Ut. 3. a2 ≡ 2a−1 and a3 ≡ 3a−2  modulo m . By considering all short solutions of  15 , 4 = 4, for the respective vectors  1,−2, 1  and  1,−1,−1, 1 , we find that ν2 except in the following cases:  3 = 6 and ν2  m = 9, a = 4 or 7, ν2 3 = 5; m = 9q, a = 3q + 1 or 6q + 1, ν2  2 = ν2  4 = 2.   cid:16    0, 1    cid:17   0, 1− α 2   0, 1−α    cid:19    cid:18   0,  1 2  − α 2  y = x+1−α  y =  1 2  x+1− α 2  y = x+  1 2  − α 2  y = x  y = x+1− α 2   cid:16  α   cid:17   , 1  2  132  312   α, 1    1, 1    cid:18  1   cid:19   , 1  α 2  +   cid:17    cid:16   2 1, 1− α 2  321  y = x− α 2  y =  x+  1 2  y = x−α  1 2  − α 2  123  132  312   1−α, 1−α    1, 1−α    cid:18   1,  1 2  − α 2  − α 2  y = x− 1 2 x− α 2  1 2   cid:19 y =  123  213  231  321   cid:16  α   cid:17   , 0  2  231   cid:19   213   cid:18  1  2  +  , 0  α 2   0, 0    α, 0    1, 0    578  12  u2  11 + u2  11 + u2  21 + u2  11 + u2 12.  2 ≤  u2  21 + u2 11 + u2  22  m2 and that x2  ANSWERS TO EXERCISES  12  + x2 2 u2 2 − x1x2  u2  3.3.4 m y1u22 − y2u21,−y1u12 + y2u11 , and this 4.  a  The unique choice for  x1, x2  is 1 m y1u22 + y2au22,−y1u12 − y2au12  ≡  0, 0   modulo 1 ; that is, x1 and x2 are is ≡ 1 integers.  b  When  x1, x2  ̸=  0, 0 , we have  x1u11 + x2u21 2 +  x1u12 + x2u22 2 = 11 + u2 x2 1 u2 22  + 2x1x2 u11u21 + u12u22 , and by hypothesis this is ≥  x2 12  ≥ u2 1 + x2 [Note that this is a stronger result than Lemma A, which tells us only that 1 ≤  u2 x2 12 2 m2, where the latter can be ≥ 1. The idea is essentially Gauss’s notion of a reduced binary quadratic form, Disquisitiones Arithmeticæ  Leipzig: 1801 , §171.] 5. Conditions  30  remain invariant; hence h cannot be zero in step S2, when a is relatively prime to m. Since h always decreases in that step, S2 eventually terminates with u2 + v2 ≥ s. Notice that pp′ ≤ 0 throughout the calculation. The hinted inequality surely holds the first time step S2 is encountered. The integer q′ that minimizes  h′ − q′h 2 +  p′ − q′p 2 is q′ = round  h′h + p′p   h2 + p2  , by Eq.  24 . If  h′ − q′h 2 +  p′ − q′p 2 < h2 + p2 we must have q′ ̸= 0, q′ ̸= −1, hence  p′ − q′p 2 ≥ p2, hence  h′ − q′h 2 < h2, i.e., h′ − q′h < h, i.e., q′ is q or q +1. We have hu+ pv ≥ h h′ − q′h + p p′ − q′p  ≥ − 1 2 h2 + p2 , so if u2 + v2 < s the next iteration of step S2 will preserve the assumption in the hint. If u2 + v2 ≥ s >  u− h 2 + v− p 2, we have 2h u−h +p v−p  = 2 h h−u +p p−v   =  u−h 2+ v−p 2+h2+p2− u2+v2  ≤  u− h 2 + v − p 2 ≤ h2 + p2, hence  u− h 2 + v − p 2 is minimal by exercise 4. Finally if both u2 + v2 and  u − h 2 +  v − p 2 are ≥ s, let u′ = h′ − q′h, v′ = p′ − q′p; then 2hu′ + pv′ ≤ h2 + p2 ≤ u′2 + v′2, and h2 + p2 is minimal by exercise 4.  [Generalizations to finding the shortest 2-D vector with respect to other metrics  2 = min0≤j<t m2  j−1 , in the notation of exercise 3.3.3–16.  are discussed by Kaib and Schnorr, J. Algorithms 21  1996 , 565–578.] 6. If u2 + v2 ≥ s >  u − h 2 +  v − p 2 in the previous answer, we have  v − p 2 > v2, hence  u − h 2 < u2; and if q = aj, so that h′ = ajh + u, we must have aj+1 = 1. It follows that ν2 Now we have m0 = mjpj + mj+1pj−1 = ajmjpj−1 + mjpj−2 + mj+1pj−1 <  aj + 1 + 1 aj mjpj−1 ≤  A + 1 + 1 A mjpj−1, and m2 j−1 ≥ 2mjpj−1, hence the result. 7. We shall prove, using condition  19 , that Uj · Uk = 0 for all k ̸= j if and only if Vj · Vk = 0 for all k ̸= j. Assume that Uj · Uk = 0 for all k ̸= j, and let Uj = α1V1 + ···+αtVt. Then Uj · Uk = αk for all k, hence Uj = αjVj, and Vj · Vk = α−1 j  Uj · Vk  = 0 for all k ̸= j. A symmetric argument proves the converse. 8. Clearly νt+1 ≤ νt  a fact used implicitly in Algorithm S, since s is not changed when t increases . For t = 2 this is equivalent to  mµ2 π 1 2 ≥   3 4 mµ3 π 1 3, i.e., √ µ3 ≤ 4 310−4  π with the given parameters, but 3 for large m and fixed µ2 the bound  40  is better. 9. Let f y1, . . . , yt  = θ; then gcd y1, . . . , yt  = 1, so there is an integer matrix W of determinant 1 having  y1, . . . , yt  as its first row.  Prove the latter fact by induction on the magnitude of the smallest nonzero entry in the row.  Now if X =  x1, . . . , xt  is a row vector, we have XW = X′ if and only if X = X′W −1, and W −1 is an integer matrix of determinant 1, hence the form g defined by W U satisfies g x1, . . . , xt  = f x′1, . . . , x′t ; furthermore g 1, 0, . . . , 0  = θ.  . This bound reduces to 4  m π µ  Without loss of generality, assume that f = g. If now S is any orthogonal matrix, the matrix U S defines the same form as U, since  XU S  XU S T =  XU  XU T. Choosing S so that its first column is a multiple of U T1 and its other columns are any  j + p2  j + p2  3 2 2   3.3.4  suitable vectors, we have  ANSWERS TO EXERCISES  579    α1 α2  ...  αt  U S =    0  . . .  0  U′  for some α1, α2, . . . , αt and some  t − 1  ×  t − 1  matrix U′. Hence f x1, . . . , xt  = √  α1x1 +···+ αtxt 2 + h x2, . . . , xt . It follows that α1 = θ for 1 ≤ j ≤ t] and that h is a positive definite quadratic form defined by U′, where det U′ =  det U    √ θ. By induction on t, there are integers  x2, . . . , xt  with h x2, . . . , xt  ≤   4  θ [in fact, αj =  U1 · Uj    3  t−2  2det U2  t−1   and for these integer values we can choose x1 so that x1 + α2x2 +···+ αtxt  α1 ≤ 1 2; equivalently,  α1x1 + ··· + αtxt 2 ≤ 1  1  t−1   √   θ  ,  4 θ. Hence  θ ≤ f x1, . . . , xt  ≤ 1  4 θ +   4  3  t−2  2det U2  t−1   1  t−1    θ  2 γ <  1+y2  1 + y2  2 = γm; we have 0 ≤ α ≤ 1  √ γ2 − 1 implies that γ2 > 4  2 = βm, y2 2 β and βγ ≥ 1. The identity  u1y2 − u2y1 2 +  u1y1 + u2y2 2 =  u2 2 γ. But 1  and the desired inequality follows immediately. [Note: For t = 2 the result is best possible. For general t, Hermite’s theorem implies that µt ≤ πt 2 4 3 t t−1  4  t 2 ! . A fundamental theorem due to Minkowski  “Every t-dimensional convex set symmetric about the origin with volume ≥ 2t contains a nonzero integer point”  gives µt ≤ 2t; this is stronger than Hermite’s theorem for t ≥ 9. Even stronger results are known, see  41 .] 10. Since y1 and y2 are relatively prime, we can solve u1y2 − u2y1 = m; furthermore  u1+qy1 y2− u2+qy2 y1 = m for all q, so we can ensure that 2u1y1+u2y2 ≤ y2 2 by choosing an appropriate integer q. Now y2 u1+au2  ≡ y2u1−y1u2 ≡ 0  modulo m , and y2 must be relatively prime to m, hence u1 + au2 ≡ 0. Finally let u1y1 + u2y2 = αm, u2 1 + u2 2 γ, and it remains to be shown that α ≤ 1 2  y2 1 + u2 1 + y2 2  2 β, we have 2αγ > 1 + α2, that is, γ − √ γ2 − 1 < implies that 1 + α2 = βγ. If α > 1 α ≤ 1 11. Since a is odd, y1 + y2 must be even. To avoid solutions with y1 and y2 both even, √ let y1 = x1 + x2, y2 = x1 − x2, and solve x2 3 − ϵ, with x1 ⊥ x2 and x1 even; the corresponding multiplier a will be the solution to  x2 − x1 a ≡ x2 + x1  modulo 2e . It is not difficult to prove that a ≡ 1  modulo 2k+1  if and only if x1 ≡ 0  modulo 2k , so we get the best potency when x1 mod 4 = 2. The problem reduces to finding relatively prime solutions to x2 2 = N where N is a large integer of the form 4k + 1. By factoring N over the Gaussian integers, we can see that solutions exist if and only if each prime factor of N  over the usual integers  has the form 4k + 1. According to a famous theorem of Fermat, every prime p of the form 4k + 1 can be written p = u2 + v2 =  u + iv  u − iv , v even, in a unique way except for the signs of u and v. The numbers u and v can be calculated efficiently by solving x2 ≡ −1  modulo p , then calculating u + iv = gcd x + i, p  by Euclid’s algorithm over the [We can take x = n p−1  4 mod p for almost half of all integers n. Gaussian integers. This application of a Euclidean algorithm is essentially the same as finding the least nonzero u2 + v2 such that u ± xv ≡ 0  modulo p . The values of u and v also appear when Euclid’s algorithm for integers is applied in the ordinary way to p and x; see J. A. Serret and C. Hermite, J. de Math. Pures et Appl. 13  1848 , 12–15.] If the prime  3, a contradiction.  2 = m   1 + x2  1 + x2   580  1 + x2  1 . . . per  ANSWERS TO EXERCISES  √ Line 14 of the table was obtained as follows: ⌊232   3.3.4 r =  u1 + iv1 e1 u1 − iv1 e1 . . .  ur + ivr er ur − ivr er, we factorization of N is pe1 2 = N, x1 ⊥ x2, x1 even, by letting x2 + ix1 = get 2r−1 distinct solutions to x2  u1 + iv1 e1 u2 ± iv2 e2 . . .  ur ± ivr er; and all such solutions are obtained in this way. Note: When m = 10e, a similar procedure can be used, but it is five times as much work since we must keep trying until finding a solution with x1 ≡ 0  modulo 10 . √ 3⌋ = 5773502691, and 5773502689 = For example, when m = 1010 we have ⌊m  53 · 108934013 =  7 + 2i  7 − 2i  2203 + 10202i  2203 − 10202i . Of the two solutions x2 + ix1 =  7 + 2i  2203 + 10202i  or  7 + 2i  2203 − 10202i , the former gives x1 = 67008  no good  and the latter gives x1 = 75820, x2 = 4983  which is usable . Line 9 of Table 1 was obtained by taking x1 = 75820, x2 = −4983. 3⌋ = 2479700524; we drop down to N = 2479700521, which equals 37 · 797 · 84089 and has four solutions N = 43642 + 496052 = 263642 + 422452 = 386402 + 314112 = 119602 + 483392. The corresponding multipliers are 2974037721, 2254986297, 4246248609, and 956772177. We try also N − 4, but it is ineligible because it is divisible by 3. On the other hand the prime number N − 8 = 450882 + 211372 leads to the multiplier 3825140801. Similarly, we get additional multipliers from N −20, N −44, N −48, etc. The multiplier on line 14 is the best of the first sixteen multipliers found by this procedure; it’s one  of the four obtained from N − 68. k̸=j qiqk Ui · Uk . The partial derivative with respect to qk is twice the left-hand side of  26 . If the minimum can be achieved, these partial derivatives must all vanish. 13. u11 = 1, u21 = irrational, u12 = u22 = 0. 14. After three Euclidean steps we find ν2  12. Uj′ · Uj′ = Uj · Uj + 2  i̸=j qi Ui · Uj  +  2 = 52 + 52, then S4 produces  i̸=j  U =  5 −18 −2 1 −2   −5  ,  0 0 1  V =   , −2 −22 −2  2 1 18 −5 −8 −7 −5 −5 −5 1 −2 9 −31 1 29 √ 6, as we already knew from exercise 3.  V =  −3  U =   ,  18  38 −5 −5 −5 0 100  0   .  Transformations  j, q1, q2, q3  =  1,∗, 0, 2 ,  2,−4,∗, 1 ,  3, 0, 0,∗ ,  1,∗, 0, 0  result in  Thus ν3 = 15. The largest achievable q in  11 , minus the smallest achievable, plus 1, is u1 + ··· + ut − δ, where δ = 1 if uiuj < 0 for some i and j, otherwise δ = 0. For example if t = 5, u1 > 0, u2 > 0, u3 > 0, u4 = 0, and u5 < 0, the largest achievable value is q = u1 + u2 + u3 − 1 and the smallest is q = u5 + 1 = −u5 + 1.  [Note that the number of hyperplanes is unchanged when c varies, hence the same answer applies to the problem of covering L instead of L0. However, the stated formula is not always exact for covering L0, since the hyperplanes that intersect the unit hypercube may not all contain points of L0. In the example above, we can never achieve the value q = u1 + u2 + u3 − 1 in L0 if u1 + u2 + u3 > m; it is achievable if and only if there is a solution to m − u1 − u2 − u3 = x1u1 + x2u2 + x3u3 + x4u5 in nonnegative integers  x1, x2, x3, x4 . It may be true that the stated limits are always achievable when u1 + ··· + ut is minimal, but this does not appear to be obvious.] 16. It suffices to determine all solutions to  15  having minimum u1 + ··· + ut, subtracting 1 if any one of these solutions has components of opposite sign.  Z =  0  0  1 .   3.3.4  ANSWERS TO EXERCISES  581  Instead of positive definite quadratic forms, we work with the somewhat similar function f x1, . . . , xt  = x1U1 + ··· + xtUt, defining Y  = y1 + ··· + yt. Inequality  21  can be replaced by xk ≤ f y1, . . . , yt   max1≤j≤t vkj . Thus a workable algorithm can be obtained as follows. Replace steps S1 through S3 by: “Set U ←  m , V ←  1 , r ← 1, s ← m, t ← 1.”  Here U and V are 1 × 1 matrices; thus the two-dimensional case will be handled by the general method. A special procedure for t = 2 could, of course, be used; see the reference following the answer to exercise 5.  In steps S4 and S7, set s ← min s,Uk . In step S7, set zk ← ⌊max1≤j≤t vkj s m⌋. In step S9, set s ← min s, Y  − δ ; and in step S10, output s = Nt. Otherwise leave the algorithm as it stands, since it already produces suitably short vectors. [Math. Comp. 29  1975 , 827–833.] 17. When k > t in S9, and if Y · Y ≤ s, output Y and −Y; furthermore if Y · Y < s, take back the previous output of vectors for this t. [In the author’s experience preparing Table 1, there was exactly one vector  and its negative  output for each νt, except when y1 = 0 or yt = 0.] 18.  a  Let x = m, y =  1 − m  3, vij = y + xδij, uij = −y + δij. Then Vj · Vk = 3 m2 − 1  for j ̸= k, Vk · Vk = 2 9 m.  This 1 example satisfies  28  with a = 1 and works for all m ≡ 1  modulo 3 .   b  Interchange the roles of U and V in step S5. Also set s ← min s, Ui · Ui  for all Ui that change. For example, when m = 64 this transformation with j = 1, applied to the matrices of  a , reduces  2 , Uj · Uj = 1  3 m2 + 1  −21 −21 −21   43 −21 −21  1  1 1 43 −21 −21 −21 −21 43  43 −21 43   , U =  , U =  V =  V =  to  3 m2 + 2 , zk ≈ 2  22  22    .  21 22 21  21 21 22  21 21  21 21 0 1 0 1  −1 −1  [Since the transformation can increase the length of Vj, an algorithm that incorporates both transformations must be careful to avoid infinite looping. See also exercise 23.] 19. No, since a product of non-identity matrices with all off-diagonal elements non- negative and all diagonal elements 1 cannot be the identity. [However, looping would be possible if a subsequent transformation with q = −1 were performed when −2Vi · Vj = Vj · Vj; the rounding rule must be asymmetric with respect to sign if non-shortening transformations are allowed.] 20. When a mod 8 = 5, the points 2−e x, s x , . . . , s[t−1] x   for x in the period are the same as the points 22−e y, σ y , . . . , σt−1 y   for 0 ≤ y < 2e−2, plus 2−e t, . . . , t , where σ y  =  ay + ⌊a 4⌋t  mod 2e−2 and t = X0 mod 4. So in this case we should use Algorithm S with m = 2e−2.  When a mod 8 = 3, the maximum distance between parallel hyperplanes that cover the points 2−e x, s x , . . . , s[t−1] x   modulo 1 is the same as the maximum distance covering the points 2−e x,−s x , . . . ,  −1 t−1s[t−1] x  , because the negation of coordinates doesn’t change distance. The latter points are 22−e y, σ y , . . . , σt−1 y   where σ y  =  −ay − ⌈a 4⌉t  mod 2e−2, plus a constant offset. Again we apply Algorithm S with m = 2e−2; changing a to m − a has no effect on the result. 21. X4n+4 ≡ X4n  modulo 4 , so it is now appropriate to let V1 =  4, 4a2, 4a3  m, V2 =  0, 1, 0 , V3 =  0, 0, 1  define the corresponding lattice L0.   582  ANSWERS TO EXERCISES  3.3.4  24. Let m = p; an analysis paralleling the text can be given. For example, when t = 4 we have Xn+3 =   a2 + b Xn+1 + abXn  mod m, and we want to minimize 4 ̸= 0 such that u1 + bu3 + abu4 ≡ u2 + au3 +  a2 + b u4 ≡ 0 u2 2 + u2 1 + u2  modulo m .  3 + u2  Replace steps S1 through S3 by the operations of setting   m  0    ,  0 m  U ←  1  0    ,  0 1  V ←  1  0    ,  0 1  R ←  s ← m 2  ,  t ← 2,  and outputting ν2 = m. Replace step S4 by  If t = T, the algorithm terminates. Otherwise set t ← t + 1 S4′. [Advance t.] and R ← R  0 a  mod m. Set Ut to the new row  −r12,−r22, 0, . . . , 0, 1  of t 1 elements, and set uit ← 0 for 1 ≤ i < t. Set Vt to the new row  0, . . . , 0, m . For 1 ≤ i < t, set q ← round  vi1r12 + vi2r22  m , vit ← vi1r12 + vi2r22 − qm, and Ut ← Ut + qUi. Finally set s ← min s, Ut · Ut , k ← t, j ← 1.  b  [A similar generalization applies to all sequences of length pk − 1 that satisfy the linear recurrence 3.2.2– 8 . Additional numerical examples have been given by A. Grube, Zeitschrift für angewandte Math. und Mechanik 53  1973 , T223–T225; L’Ecuyer, Blouin, and Couture, ACM Trans. Modeling and Comp. Simul. 3  1993 , 87–98.]  25. The given sum is at most twice the quantity  d f m d ,  where  f m  = 1    m 2 [When d = 1, we have  1≤k≤m 2  = 1  m  m  1     1  csc πk m   0≤k≤m  2d  r dk  = 1 + 1   1 0≤k<m r k  =  2 π  ln m + 1 +  2 π  ln 2e π  + O 1 m .] 1 . . . pdr 1 . . . per  r and gcd a − 1, m  = pf1  ln tan π  m 2  csc πx m  dx + O  = 1  + O  2m  m  m  π  x  1  .  r  26. If gcd q, m  = d, the same derivation goes through with m replaced by m d. Suppose we have m = pe1 r . If m is replaced by m d, then s is replaced by p . Since m d > 1, we can also replace N by N mod  m d . 27. It is convenient to use the following functions: ρ x  = 1 if x = 0, ρ x  = x if 0 < x ≤ m 2, ρ x  = m − x if m 2 < x < m; trunc x  = ⌊x 2⌋ if 0 ≤ x ≤ m 2, trunc x  = m − ⌊ m − x  2⌋ if m 2 < x < m; L x  = 0 if x = 0, L x  = ⌊lg x⌋ + 1 if 0 < x ≤ m 2, L x  = − ⌊lg m − x ⌋ + 1  if m 2 < x < m; and l x  = max 1, 2x−1 . Note that l L x   ≤ ρ x  < 2l L x   and 2ρ x  ≤ 1 r x  = m sin πx m  < πρ x , for 0 < x < m.  r and d = pd1 1 . . . pfr . . . p  max 0,e1−f1−d1  1  max 0,er−fr−dr   Say that a vector  u1, . . . , ut  is bad if it is nonzero and satisfies  15 ; and let ρmin be the minimum value of ρ u1  . . . ρ ut  over all bad  u1, . . . , ut . The vector  u1, . . . , ut  is said to be in class  L u1 , . . . , L ut  . Thus there are at most  2 lg m + 1 t classes, and class  L1, . . . , Lt  contains at most l L1  . . . l Lt  vectors. Our proof is based on showing  that the bad vectors in each fixed class contribute at most 2 ρmin to r u1, . . . , ut ;  this establishes the desired bound, since 1 ρmin < πtrmax.  Let µ = ⌊lg ρmin⌋. The µ-fold truncation operator on a vector is defined to be the following operation repeated µ times: “Let j be minimal such that ρ uj  > 1, and replace uj by trunc uj ; but do nothing if ρ uj  = 1 for all j.”  This operation essentially throws away one bit of information about  u1, . . . , ut .  If  u′1, . . . , u′t  and  u′′1 , . . . , u′′t   are two vectors of the same class having the same µ-fold truncation, we say   √ m log m  N .  ANSWERS TO EXERCISES  3.3.4 583 they are similar; in this case it follows that ρ u′1 − u′′1  . . . ρ u′t − u′′t   < 2µ ≤ ρmin. For example, any two vectors of the form   1x2x1 2, 0, m− 1x3 2,  101x5x4 2,  1101 2  are similar when m is large and µ = 5; the µ-fold truncation operator successively removes x1, x2, x3, x4, x5. Since the difference of two bad vectors satisfies  15 , it is impossible for two unequal bad vectors to be similar. Therefore class  L1, . . . , Lt  can contain at most max 1, l L1  . . . l Lt  2µ  bad vectors. If class  L1, . . . , Lt  contains exactly one bad vector  u1, . . . , ut , we have r u1, . . . , ut  ≤ rmax ≤ 1 ρmin; if it contains ≤ l L1  . . . l Lt  2µ bad vectors, each of them has r u1, . . . , ut  ≤ 1 ρ u1  . . . ρ ut  ≤ 1 l L1  . . . l Lt , and we have 1 2µ < 2 ρmin.  0≤j<m−1 ωxj+l ζ jk. The analog of  51  is  Sk0 =  √ m, hence the analog of  53  is  28. Let ζ = e2πi  m−1  and let Skl =   = O   + O log m t rmax ,  N−1    The analogous theorem now states that  m  log m t+1  √  0≤n<N  ωxn  N  D  D   t    t    t  N = O  m−1 ≤ m−2 m−1  In fact, D 1 m−1 by exercise 25 with d = 1, and the former sum is treated as in exercise 27.  m−1 = O  log m t rmax .  r u1, . . . , ut  [summed over nonzero solutions of  15 ] +  r u1, . . . , ut  [summed over all nonzero  u1, . . . , ut ]. The latter sum is O log m t Let us now consider the quantity R a  =  r u1, . . . , ut  summed over nonzero 0<a<m R a  ≤  t − 1  r u1, . . . , ut  = O t log m t .  solutions of  15 . Since m is prime, each  u1, . . . , ut  can be a solution to  15  for at It follows that the average value of R a  taken over all φ m − 1  primitive roots is O t log m t φ m − 1  .  most t − 1 values of a, hence   Note: In general 1 φ n  = O log log n n ; we have therefore proved that for all prime m and for all T there exists a primitive root a modulo m such that the linear m−1 = O m−1T  log m T log log m  congruential sequence  1, a, 0, m  has discrepancy D for 1 ≤ t ≤ T. This method of proof does not extend to a similar result for linear con- gruential generators of period 2e modulo 2e, since for example the vector  1,−3, 3,−1  solves  15  for about 22e 3 values of a. 29. To get an upper bound, allow the nonzero components of u =  u1, . . . , ut  to be any real values 1 ≤ uj ≤ 1 2 m. If k components are nonzero, we have r u  ≤ 1  2kρ u   1 + ··· + u2 in the notation of the answer to exercise 27. And if u2 t has a given value ν2, we minimize ρ u  by taking u1 = ··· = uk−1 = 1 and u2 k = ν2 − k + 1. Thus r u  ≤ 1  2k√ 30. Let’s first minimize qaq − mp for 1 ≤ q < m and 0 ≤ p < a. In the notation of exercise 4.5.3–42, we have aqn − mpn =  −1 nKs−n−1 an+2, . . . , as  for 0 ≤ n ≤ s. In the range qn−1 ≤ q < qn we have aq − mp ≥ aqn−1 − mpn−1; consequently qaq − mp ≥ qn−1aqn−1 − mpn−1, and the minimum is min0≤n<s qnaqn − mpn = min0≤n<s Kn a1, . . . , an Ks−n−1 an+2, . . . , as . By exercise 4.5.3–32 we have m = Kn a1, . . . , an an+1Ks−n−1 an+2, . . . , as  + Kn a1, . . . , an Ks−n−2 an+3, . . . , as  + Kn−1 a1, . . . , an−1 Ks−n−1 an+2, . . . , as ; and our problem is essentially that of max- imizing the quantity m Kn a1, . . . , an Ks−n−1 an+2, . . . , as , which lies between an+1 and an+1 + 2.  ν2 − k + 1 . But 2k√  ν2 − k + 1 ≥ √  8ν, since ν ≥ k ≥ 2.   t    584  ANSWERS TO EXERCISES  3.3.4  √  Now let A = max a1, . . . , as . Since r m − u  = r u , we can assume that rmax = r u r au mod m  for some u with 1 ≤ u ≤ 1 2 m. Setting u′ = min au mod m,  −au  mod m , we have rmax = r u r u′ . We know from the previous paragraph that uu′ ≥ qq′, where A m ≤ 1 qq′ ≤  A + 2  m. Furthermore 2u ≤ r u −1 ≤ πu for 0 < u ≤ 1 2 m, so rmax ≤ 1  4uu′ . Hence we have rmax ≤  A + 2   4m .  There is a similar lower bound, namely rmax > A  π2m .  31. Equivalently, the conjecture is that all large m can be written m = Kn a1, . . . , an  for some n and some ai ∈ {1, 2, 3}. For fixed n the 3n numbers Kn a1, . . . , an  have an average value of order  1+ 2  n, and their standard deviation is of order  2.51527 n; so the conjecture is almost surely true. S. K. Zaremba conjectured in 1972 that all m can be represented with ai ≤ 5; T. W. Cusick made some early progress on this problem in Mathematika 24  1977 , 166–172, and an excellent survey of later work has been prepared by A. Kontorovich in Bull. Amer. Math. Soc. 50  2013 , 187–228. It appears that only the cases m = 54 and m = 150 require ai = 5, and the largest m’s that require 4s are 2052, 2370, 5052, and 6234; at least, the author has found representations with ai ≤ 3 for all other integers less than 2000000. When we require ai ≤ 2, the average of 5 −2 −n, while the standard deviation grows as  2.04033 n. Kn a1, . . . , an  is 4 The density of such numbers in the author’s experiments  which considered 26 blocks of 214 numbers each, for m ≤ 220  appears to vary between .50 and .65.  [See I. Borosh and H. Niederreiter, BIT 23  1983 , 65–74, for a computational method that finds multipliers with small partial quotients. They have found 2-bounded solutions with m = 2e for 25 ≤ e ≤ 35.] 32.  a  Un− Zn m1 ≡  m2− m1 Yn m1m2  modulo 1 , and  m1− m2  m1m2 ≈ 2−54.  Therefore we can analyze the high-order bits of Zn by analyzing Un. The low-order bits are probably random too, but this argument does not apply to them.   b  We have Un = Wn m for all n. The Chinese remainder theorem tells us that we need only verify the congruences Wn ≡ Xnm2  modulo m1  and Wn ≡ −Ynm1  modulo m2 , because m1 ⊥ m2. [Pierre L’Ecuyer and Shu Tezuka, Math. Comp. 57  1991 , 735–746.]  5 2n + 1  SECTION 3.4.1 1. α +  β − α U. 2. Let U = X m; then ⌊kU⌋ = r ⇐⇒ r ≤ kX m < r + 1 ⇐⇒ mr k ≤ X < m r + 1  k ⇐⇒ ⌈mr k⌉ ≤ X < ⌈m r + 1  k⌉. The exact probability is given by the formula  1 m  ⌈m r + 1  k⌉ − ⌈mr k⌉  = 1 k + ϵ, where ϵ < 1 m. 3. If full-word random numbers are given, the result will deviate from the correct distribution by at most 1 m, as in exercise 2; but all of the excess is given to the smallest results. Thus if k ≈ m 3, the result will be less than k 2 about 2 3 of the time. It is much better to obtain a perfectly uniform distribution by rejecting U if U ≥ k⌊m k⌋; see D. E. Knuth, The Stanford GraphBase  New York: ACM Press, 1994 , 221.  On the other hand, if a linear congruential sequence is used, k must be relatively prime to the modulus m, lest the numbers have a very short period, by the results of Section 3.2.1.1. For example, if k = 2 and m is even, the numbers will at best be alternately 0 and 1. The method is slower than  1  in nearly every case, so it is not recommended.  Unfortunately, however, the “himult” operation in  1  is not supported in many high-level languages; see exercise 3.2.1.1–3. Division by m k may be best when himult is unavailable.   3.4.1  ANSWERS TO EXERCISES  585  Fig. A–4. Region of “acceptance” for the algorithm of exercise 6.  4. max X1, X2  ≤ x if and only if X1 ≤ x and X2 ≤ x; min X1, X2  ≥ x if and only if X1 ≥ x and X2 ≥ x. The probability that two independent events both happen is the product of the individual probabilities. 5. Obtain independent uniform deviates U1 and U2. Set X ← U2. If U1 ≥ p, If U1 ≥ p + q, also set set X ← max X, U3 , where U3 is a third uniform deviate. X ← max X, U4 , where U4 is a fourth uniform deviate. This method can obviously be generalized to any polynomial, and indeed even to infinite power series  as shown for example in Algorithm S, which uses minimization instead of maximization . We could also proceed as follows  suggested by M. D. MacLaren : If U1 < p, set X ← U1 p; otherwise if U1 < p + q, set X ← max  U1 − p  q, U2 ; otherwise set X ← max  U1 − p − q  r, U2, U3 . This method requires less time than the other to obtain the uniform deviates, although it involves further arithmetical operations and it is slightly less stable numerically. 6. F  x  = A1  A1 + A2 , where A1 and A2 are the areas in Fig. A–4; so   x  1  0  0  √ √  F  x  =  1 − y2 dy 1 − y2 dy  = 2  π  arcsin x + 2  π  √ x  1 − x2.  The probability of termination at step 2 is p = π 4, each time step 2 is encountered, so the number of executions of step 2 has the geometric distribution. The characteristics  of this number are  min 1, ave 4 π, max ∞, dev  4 π 1 − π 4 , by exercise 17.  7. If k = 1, then n1 = n and the problem is trivial. Otherwise it is always possible to find i ̸= j such that ni ≤ n ≤ nj. Fill Bi with ni cubes of color Ci and n − ni of color Cj, then decrease nj by n − ni and eliminate color Ci. We are left with the same sort of problem but with k reduced by 1; by induction, it’s possible.  The following algorithm can be used to compute the P and Y tables: Form a list of pairs  p1, 1  . . .  pk, k  and sort it by first components, obtaining a list  q1, a1  . . .  qk, ak  where q1 ≤ ··· ≤ qk. Set n ← k; then repeat the following operations until n = 0: Set P [a1 − 1] ← kq1 and Y [a1 − 1] ← xan. Delete  q1, a1  and  qn, an , then insert the new entry  qn −  1 k − q1 , an  into its proper place in the list and decrease n by 1.  If pj < 1 k the algorithm will never put xj in the Y table; this fact is used implicitly in Algorithm M. The algorithm attempts to maximize the probability that V < PK in  3 , by always robbing from the richest remaining element and giving it to the poorest. However, it is very difficult to determine the absolute maximum of this probability, since such a task is at least as difficult as the “bin-packing problem”; see Section 7.9.   1  V  0  Circle U 2+V 2 = 1  A1  A2  U  x  1   3.4.1  586  ANSWERS TO EXERCISES  8. Replace Pj by  j + Pj  k for 0 ≤ j < k.  9. Consider the sign of f′′ x  =2 π  x2 − 1 e−x2 2.  Let h = 1  10. Let Sj =  j−1  5 for 1 ≤ j ≤ 16 and pj+15 = F  Sj+1 −F  Sj −pj for 1 ≤ j ≤ 15; also let p31 = 1 − F  3  and p32 = 0.  Eq.  15  defines p1, . . . , p15.  The algorithm of exercise 7 can now be used with k = 32 to compute Pj and Yj, after which we will have 1 ≤ Yj ≤ 15 for 1 ≤ j ≤ 32. Set P0 ← P32  which is 0  and Y0 ← Y32. Then set Zj ← 1  5 − 5Pj  and Yj ← 1  5 and fj+15 x  = 2 π e−x2 2 − e−j2 50  pj+15 for Sj ≤ x ≤ Sj + h.  5 Yj − Zj for 0 ≤ j < 32; Qj ← 1  5Pj  for 1 ≤ j ≤ 15.  Then let aj = fj+15 Sj  for 1 ≤ j ≤ 5, bj = fj+15 Sj  for 6 ≤ j ≤ 15; also bj = −hf′j+15 Sj + h  for 1 ≤ j ≤ 5, and aj = fj+15 xj  +  xj − Sj bj h for 6 ≤ j ≤ 15, where xj is the root of the equation f′j+15 xj  = −bj h. Finally set Dj+15 ← aj bj for 1 ≤ j ≤ 15 and Ej+15 ← 25 j for 1 ≤ j ≤ 5, Ej+15 ← 1  e 2j−1  50 −1  for 6 ≤ j ≤ 15. Table 1 was computed while making use of the following intermediate values:  p1, . . . , p31  =  .156, .147, .133, .116, .097, .078, .060, .044, .032, .022, .014, .009, .005, .003, .002, .002, .005, .007, .009, .010, .009, .009, .008, .006, .005, .004, .002, .002, .001, .001, .003 ;  x6, . . . , x15  =  1.115, 1.304, 1.502, 1.700, 1.899, 2.099, 2.298, 2.497, 2.697, 2.896 ;  a1, . . . , a15  =  7.5, 9.1, 9.5, 9.8, 9.9, 10.0, 10.0, 10.1, 10.1, 10.1, 10.1, 10.2, 10.2, 10.2, 10.2 ;  b1, . . . , b15  =  14.9, 11.7, 10.9, 10.4, 10.1, 10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 10.7, 10.7, 10.8, 10.9 .  −9  2, a random variable X with density g can be computed by setting X ← G[−1] 1−V   = 9 − 2 ln V . Now e−t2 2 ≤  t 3 e−t2 2 for t ≥ 3, so we obtain a valid rejection method if we accept X with probability f X  cg X  = 3 X.  11. Let g t  = e9 2te−t2 2 for t ≥ 3. Since G x  = x 12. We have f′ x  = xf x  − 1 < 0 for x ≥ 0, since f x  = x−1 − ex2 2 ∞ 2 π e−x2 2f x  = 2−j,  for x > 0. Let x = aj−1 and y2 = x2 + 2 ln 2; then  2 π e−x2 2f y  < 1  2 π ∞  3 g t  dt = 1−e− x2  x e−t2 2 dt t2  y e−t2 2 dt = 1 2  √  2  hence y > aj. 13. Take bj = µj; consider now the problem with µj = 0 for each j. In matrix notation, if Y = AX, where A =  aij , we need AAT = C =  cij .  In other notation, if  Yj = ajkXk, then the average value of YiYj is aikajk.  If this matrix equation can  11 = c11, a11a21 = c12, a2  be solved for A, it can be solved when A is triangular, since A = BU for some orthogonal matrix U and some triangular B, and BBT = C. The desired triangular solution can be obtained by solving the equations a2 22 = c22, a11a31 = c13, a21a31 + a22a32 = c23, . . . , successively for a11, a21, a22, a31, a32, etc. [Note: The  covariance matrix must be positive semidefinite, since the average value of   yjYj 2 is  cijyiyj, which must be nonnegative. And there is always a solution when C is 15. Distribution  ∞ 16. It is clear that f t  ≤ cg t  for all t as required. Since  ∞0 g t  dt = 1 we have  positive semidefinite, since C = U−1diag λ1, . . . , λn U, where the eigenvalues λj are √ nonnegative, and U−1diag  14. F  x c  if c > 0; the step function [x≥ 0] if c = 0; or 1 − F  x c  if c < 0.  F1 x − t  dF2 t . Density  ∞  g t  = Cta−1 for 0 ≤ t < 1, Ce−t for t ≥ 1, where C = ae  a + e . A random variable with density g is easy to obtain as a mixture of two distributions, G1 x  = xa for 0 ≤ x < 1, and G2 x  = 1 − e1−x for x ≥ 1:  the convolution of the given distributions.  f1 x − t f2 t  dt. This is called  λn  U is a solution.]  21+a2  λ1, . . . ,  −∞  −∞  √   ANSWERS TO EXERCISES  587  3.4.1  used.   G1. [Initialize.] Set p ← e  a + e .  This is the probability that G1 should be  G2. [Generate G deviate.] Generate independent uniform deviates U and V, where V ̸= 0. If U < p, set X ← V 1 a and q ← e−X; otherwise set X ← 1 − ln V and q ← X a−1.  Now X has density g, and q = f X  cg X .   G3. [Reject?] Generate a new uniform deviate U. If U ≥ q, return to G2.  The average number of iterations is c =  a + e   eΓ  a + 1   < 1.4.  √  It is possible to streamline this procedure in several ways. First, we can replace V by an exponential deviate Y of mean 1, generated by Algorithm S, say, and then we set X ← e−Y  a or X ← 1 + Y in the two cases. Moreover, if we set q ← pe−X in the first case and q ← p +  1 − p X a−1 in the second, we can use the original U instead of a newly generated one in step G3. Finally if U < p e we can accept V 1 a immediately, avoiding the calculation of q about 30 percent of the time. 17.  a  F  x  = 1 −  1 − p ⌊x⌋, for x ≥ 0.  b  G z  = pz  1 −  1 − p z .  c  Mean 1 − p p. To do the latter calculation, observe that if H z  = 1 p, standard deviation q +  1− q z, then H′ 1  = 1− q and H′′ 1  + H′ 1 −  H′ 1  2 = q 1− q , so the mean and variance of 1 H z  are q − 1 and q q − 1 , respectively.  See Section 1.2.10.  In this case, q = 1 p; the extra factor z in the numerator of G z  adds 1 to the mean. 18. Set N ← N1 + N2 − 1, where N1 and N2 independently have the geometric distribution for probability p.  Consider the generating function.  19. Set N ← N1 + ··· + Nt − t, where the Nj have the geometric distribution for p.  This is the number of failures before the tth success, when a sequence of independent trials are made each of which succeeds with probability p.  2, and in general when the mean value  namely t 1 − p  p  of the  distribution is small, we can simply evaluate the probabilities pn =t−1+n  pt 1 − p n  For t = p = 1  consecutively for n = 0, 1, 2, . . . as in the following algorithm:  n  N1. [Initialize.] Set N ← 0, q ← pt, r ← q, and generate a random uniform deviate U.  We will have q = pN and r = p0 +··· + pN during this algorithm, which stops as soon as U < r.   N2. [Iterate.] If U ≥ r, set N ← N + 1, q ← q 1 − p  t − 1 + N  N, r ← r + q,  and repeat this step. Otherwise return N and terminate.  [An interesting technique for the negative binomial distribution, for arbitrarily large real values of t, has been suggested by R. Léger: First generate a random gamma deviate X of order t, then let N be a random Poisson deviate of mean X 1 − p  p.] 20. R1 = 1 +  1 − A R  · R1. When R2 is performed, the algorithm terminates with probability I R; when R3 is performed, it goes to R1 with probability E R. We have  R1 R2 R3 R4  R A  0 0  R A R A  0 √  21. R =8 e ≈ 1.71553; A =  R A  R A  R A − I A  R A − E A  2 Γ  3 2  =π 2 ≈ 1.25331. Since 3 a b2,  a − bu du =  a − bu 3 2 2  5 a − bu  − 2    √  u  R A  0  R A R A  R A − I A  R A − I A − E A   588  3.4.1  0  u  √  ANSWERS TO EXERCISES  bu−au2 du= 1 bu+au2 du=− 1  5 e ≈ 1.13020. Finally the following integration  4 ba−1√ bu−au2  2ua b−1 , 8 b2a−3 2 arcsin 2ua b−1 + 1 √ √ √ 8 b2a−3 2 ln  a + 1 a+b 2 bu+au2+u  a − bu du = 8 15 a5 2 b2 where a = 4 1 + ln c  and b = 4c; when c = e1 4, I has its maximum value 5 6 formulas are needed for E:  we have I = 2 a b √ √ 4 ba−1√ bu+au2 2ua b+1 , where a, b > 0. Let the test in step R3 be “X2 ≥ 4ex−1 U−4x”; then the exterior region hits the top of the rectangle when u = r x  =  ex − √ e2x − 2ex  2ex.  Incidentally, have E = 2 r x  r x  reaches its maximum value at x = 1 2, a point where it is not differentiable!  We bu − au2  du where b = 4ex−1 and a = 4x. The maximum fined by G x  =  ∞  value of E occurs near x = −.35, where we have E ≈ .29410. 22.  Solution by G. Marsaglia.  Consider the “continuous Poisson distribution” de- if X has this distribution then ⌊X⌋ is Poisson distributed, since G x + 1  − G x  = e−µµx x!. If µ is large, G is approximately normal, hence G[−1] Fµ x   is approximately linear, where Fµ x  is the distribution function for a normal deviate with mean and variance µ; that is, √ Fµ x  = F  x − µ   µ , where F  x  is the normal distribution function  10 . Let g x  be an efficiently computable function such that G[−1] Fµ x   − g x  < ϵ for −∞ < x < ∞; we can now generate Poisson deviates efficiently as follows: Generate a normal deviate X, and set Y ← g µ + √ 2⌋. Then if Y − M > ϵ, output N; otherwise output M − [ G[−1] F  X   < M ].  µ e−ttx−1 dt   Γ  x , for x > 0;  µ X , N ← ⌊Y ⌋, M ← ⌊Y + 1   2 e − √  0  This approach applies also to the binomial distribution, with   1  p  G x  =  ux−1 1 − u n−x du  Γ  t + 1   Γ  x  Γ  t + 1 − x  ,  since ⌊G[−1] U ⌋ is binomial with parameters  t, p  and G is approximately normal.  [See also the alternative method proposed by Ahrens and Dieter in Computing 25   1980 , 193–208.] 23. Yes. The second method calculates cos 2θ, where θ is uniformly distributed between 0 and π 2.  Let U = r cos θ, V = r sin θ.  32 =  .10101 2. In general, the binary representation is formed by using 1 for  25. 21 and 0 for &, from left to right, then suffixing 1. This technique [see K. D. Tocher, J. Roy. Stat. Soc. B16  1954 , 49] can lead to efficient generation of independent bits having a given probability p, and it can also be applied to the geometric and binomial distributions.  k Pr N1 = k  Pr N2 = n − k  = e−µ1−µ2 µ1 + µ2 n n!.  b  False,  unless µ2 = 0; otherwise N1 − N2 might be negative. 27. Let the binary representation of p be  .b1b2b3 . . .  2, and proceed according to the following rules:  26.  a  True:   B1. [Initialize.] Set m ← t, N ← 0, j ← 1.  During this algorithm, m represents the number of simulated uniform deviates whose relation to p is still unknown, since they match p in their leading j−1 bits; and N is the number of simulated deviates known to be less than p.   B2. [Look at next column of bits.] Generate a random integer M with the binomial 2 .  Now M represents the number of unknown deviates that  distribution  m, 1 fail to match bj.  Set m ← m − M, and if bj = 1 set N ← N + M.   3.4.1  ANSWERS TO EXERCISES  589   n     B3. [Done?] If m = 0, or if the remaining bits  .bj+1bj+2 . . .  2 of p are all zero,  the algorithm terminates. Otherwise, set j ← j +1 and return to step B2.  [When bj = 1 for infinitely many j, the average number of iterations At satisfies  An = 1 + 1 2n  for n ≥ 1.  A0 = 0;  k  n  k  k  Ak,  k≥1   n  1 − e−z + A  1  2 z ez 2. Therefore A z e−z = n≥1 −z n  n! 2n − 1  , and  Letting A z  = Anzn n!, we have A z  = ez − 1 + A  1 k≥0 1 − e−z 2k  = 1 − e−z − 2 z e−z 2 =  −1 k+1 Am = 1 + ln 2 + 1 2k − 1 = 1 + Vn+1 2 + f0 n  + O n−1  28. Generate a random point  y1, . . . , yn  on the unit sphere, and let ρ = aky2  a2 point  y1 ρ, . . . , yn ρ ; otherwise start over. Here K2 = min{  aky2    a2  y2 k = 1} = an−1  ky2 k n+1 if nan ≥ a1,   n + 1   a1 + an  n+1 a1an n n otherwise.  Generate an independent uniform deviate U, and if ρn+1U < K  k. k, output the k   ky2  in the notation of exercise 5.2.2–48.]  n + 1 = lg n + γ  29. Let Xn+1 = 1, then set Xk ← Xk+1U or Xk ← Xk+1e−Yk k for k = n, n − 1, . . . , 1, where Uk is uniform or Yk is exponential. [ACM Trans. Math. Software 6  1980 , 359–364. This technique was introduced in the 1960s by David Seneschal; see Amer. Statistician 26, 4  October 1972 , 56–57. The alternative of generating n uniform numbers and sorting them is probably faster, with an appropriate sorting method, but the method suggested here is particularly valuable if only a few of the largest or smallest X’s are desired. Notice that  F [−1] X1 , . . . , F [−1] Xn   will be sorted deviates having distribution F.] 30. Generate random numbers Z1 = −µ−1 ln U1, Z2 = Z1 − µ−1 ln U2, . . . , until Zm+1 ≥ 1. Output  Xj, Yj  = f Zj  for 1 ≤ j ≤ m, where f  .b1b2 . . . b2r 2  =   .b1b2 . . . br 2,  .br+1br+2 . . . b2r 2 . If the less significant bits are significantly less random than the more significant bits, it’s safer  but slower  to let f  .b1b2 . . . b2r 2  =   .b1b3 . . . b2r−1 2,  .b2b4 . . . b2r 2 . 31.  a  It suffices to consider the case k = 2, since a1X1+···+akXk = X cos θ+Y sin θ when X = X1, cos θ = a1, and Y =  a2X2 + ··· + akXk   sin θ. And  1 k k  Pr X cos θ + Y sin θ ≤ x  = 1 2π = 1 2π  e−s2 2−t2 2  ds dt [s cos θ + t sin θ ≤ x]  e−u2 2−v2 2  du dv [u≤ x] =  10 ,  from the substitution u = s cos θ + t sin θ, v = −s sin θ + t cos θ.  5 β−24+ 4 3 of linear recurrences.   b  There are numbers α > 1 and β > 1 such that  α−24 + α−55    2 = 1 and 5 β−55 = 1; so the numbers Xn will grow exponentially with n, by the properties If we break out of the linear recurrence mold by, say, using the recurrence Xn = Xn−24 cos θn + Xn−55 sin θn, where θn is chosen uniformly in [0 . . 2π , we probably will obtain decent results; but this alternative would involve much more computation.  c  Start with, say, 2048 normal deviates X0, . . . , X1023, Y0, . . . , Y1023. After having used about 1 3 of them, generate 2048 more as follows: Choose integers a, b, c,  √     s,t  u,v   590  ANSWERS TO EXERCISES  3.4.1  and d uniformly in [0 . . 1024 , with a and c odd; then set  X′j ← X aj+b  mod 1024 cos θ + Y cj+d  mod 1024 sin θ, Y ′j ← −X aj+b  mod 1024 sin θ + Y cj+d  mod 1024 cos θ,  j + Y 2  for 0 ≤ j < 1024, where cos θ and sin θ are random ratios  U 2 − V 2   U 2 + V 2  and 2U V   U 2 + V 2 , chosen as in exercise 23. We can reject U and V unless  cos θ ≥ 1 2 and  sin θ ≥ 1 2. The 2048 new deviates now replace the old ones. Notice that only a few operations were needed per new deviate. This method does not diverge like the sequences considered in  b , because the sum  j   =  X′j 2 +  Y ′j  2  remains at the constant value S ≈ 2048,  of squares X2  except for a slight roundoff error. On the other hand, the constancy of S is actually a defect of the method, because the sum of squares should really have the χ2 distribution with 2048 degrees of freedom. To overcome this problem, the normal deviates actually 4095 2 S delivered to the user should be not Xj but αXj, where α2 = 1 4095 2 will be a reasonable is a precomputed scale factor.  The quantity 1 approximation to the χ2 deviate desired.   2 Y1023 +  2 Y1023 +  References: C. S. Wallace [ACM Trans. on Math. Software 22  1996 , 119–127];  R. P. Brent [Lecture Notes in Comp. Sci. 1470  1998 , 1–20]. 32.  a  This mapping  X′, Y ′  = f X, Y   is a one-to-one correspondence from the set {x, y ≥ 0} to itself such that x′ + y′ = x + y and dx′ dy′ = dx dy. We have  √  √   X  X + Y    X′  X′ + Y ′  =  − λ  mod 1,  Y ′  X′ + Y ′  =   Y  X + Y    + λ  mod 1.   b  This mapping is a two-to-one correspondence such that x′ + y′ = x + y and  dx′ dy′ = 2 dx dy.   c  It suffices to consider the “j-flip” transformation  X′ =  . . . xj+2xj+1xjyj−1yj−2yj−3 . . .  2, Y ′ =  . . . yj+2yj+1yjxj−1xj−2xj−3 . . .  2,  for a fixed integer j, and then to compose j-flips for j = 0, 1, −1, 2, −2, . . . , noticing that the joint probability distribution of X′ and Y ′ converges as j → ∞. Each j-flip is one-to-one, with x′ + y′ = x + y and dx′ dy′ = dx dy. 33. Use U1 as the seed for another random number generator  perhaps a linear con- gruential generator with a different multiplier ; take U2, U3, . . . from that one.  SECTION 3.4.2  1. There are  N−t   ways to pick n − m records from the last N − t, and  N−t−1  n−m  n−m−1 ways to pick n − m − 1 from N − t − 1 after selecting the  t + 1 st item. 2. Step S3 will never go to step S5 when the number of records left to be examined is equal to n − m. 3. We should not confuse conditional and unconditional probabilities. The quan- tity m depends randomly on the selections that took place among the first t elements; if we take the average over all possible choices that could have occurred among these elements, we will find that  n − m   N − t  is exactly n N on the average. For example, consider the second element; if the first element was selected in the sample  this happens with probability n N , the second element is selected with probability  n − 1   N − 1 ; if the first element was not selected, the second is selected with     3.4.2  ANSWERS TO EXERCISES  591  probability n  N − 1 . The overall probability of selecting the second element is  n N   n − 1   N − 1   +  1 − n N  n  N − 1   = n N. 4. From the algorithm,    p m, t + 1  =   p n, k  − p n, k − 1  =k−1 6. Similarly,N The choice is obtained with probability p =  1 − n − m N − t  N  n−1  n  The desired formula can be proved by induction on t. In particular, p n, N  = 1. 5. In the notation of exercise 4, the probability that t = k at termination is qk =  . The average isN  k=0 kqk =  N + 1 n  n + 1 .  k=0 k k + 1 qk =  N + 2  N + 1 n  n + 2 ; the variance is therefore  N + 1  N − n n  n + 2  n + 1 2. 7. Suppose the choice is 1 ≤ x1 < x2 < ··· < xn ≤ N. Let x0 = 0, xn+1 = N + 1.  p m, t  + n −  m − 1   N − t  p m − 1, t .    N −  t − 1  − n + m   N −  t − 1  ,  1≤t≤N pt, where  pt =   n − m   N −  t − 1  ,  for xm < t < xm+1; for t = xm+1.  k  k  n  n  n  4 5  2 6  1 2  3 4  2 3  3 7  1 1.   N   N   =N−n   N − j  .  This method is good if, say, n ≤ 5.   Example: n = 3, N = 8,  x1, x2, x3  =  2, 3, 7 ; p = 5 8  The denominator of the product p is N!; the numerator contains the terms N − n, N − n − 1, . . . , 1 for those t’s that are not x’s, and the terms n, n − 1, . . . , 1 for those t’s that are x’s. Hence p =  N − n ! n! N!.   samples omit the first k records.  8.  a  p 0, k  =N−k  of theN j=0 Pr YN−j ≥ k  = n−1  c  Pr min YN , . . . , YN−n+1  ≥ k  = n−1   b  Set X ← k − 1, where k is minimum with U ≥ Pr X ≥ k . Thus, start with X ← 0, p ← N − n, q ← N, R ← p q, and while U < R set X ← X + 1, p ← p − 1, q ← q − 1, R ← Rp q.  This method is good when n N is, say, ≥ 1 5. We can assume that n N ≤ 1 2; otherwise it’s better to select N − n unsampled items.  j=0   N−j−k    d   See exercise 3.4.1–29.  The value X ← ⌊N 1 − U 1 n ⌋ needs to be rejected with probability only O n N . Precise details are worked out carefully in CACM 27  1984 , 703–718, and a practical implementation appears in ACM Trans. Math. Software 13  1987 , 58–67.  This method is good when, say, 5 < n < 1 After skipping X records and selecting the next, we set n ← n−1, N ← N −X−1, and repeat the process until n = 0. A similar approach speeds up the reservoir method; see ACM Trans. Math. Software 11  1985 , 37–57. 9. The reservoir gets seven records: 1, 2, 3, 5, 9, 13, 16. The final sample consists of records 2, 5, 16. 10. Delete step R6 and the variable m. Replace the I table by a table of records, initialized to the first n records in step R1, and with the new record replacing the Mth table entry in step R4. 11. Arguing as in Section 1.2.10, which considers the special case n = 1, we see that the generating function is  5 N.    2 n + 2 + n     N − n    n + 1 z  n + 1 + n n<t≤N n t  = n 1 + HN − Hn ; and the variance turns out to be N − H  n + 2 z  + n N  n  .  2   . . .   2   N  z  .  G z  = zn 1 The mean is n +  n HN − Hn  − n2 H   592  ANSWERS TO EXERCISES  3.4.2  ♦  5 ♦  12.  Note that π−1 =  btt  . . .  b33  b22 , so we seek an algorithm that goes from the representation of π to that for π−1.  Set bj ← j for 1 ≤ j ≤ t. Then for j = 2, 3, . . . , t  in this order , interchange bj ↔ baj . Finally for j = t, . . . , 3, 2  in this order , set baj ← bj.  The algorithm is based on the fact that  att π1 = π1 btt .  13. Renumbering the deck 0, 1, . . . , 2n − 2, we find that s takes card number x into card number  2x  mod  2n−1 , while c takes card x into  x−1  mod  2n−1 . We have  c followed by s  = cs = sc2. Therefore any product of c’s and s’s can be transformed into the form sick. Also 2φ 2n−1  ≡ 1 modulo  2n−1 ; since sφ 2n−1  and c2n−1 are the identity permutation, at most  2n−1 φ 2n−1  arrangements are possible.  The exact number of different arrangements is  2n−1 k, where k is the order of 2 modulo  2n−1 . For if sk = cj, then cj fixes the card 0, so sk = cj = identity.  For further details, see SIAM Review 3  1961 , 293–297. 14.  a  Q ♡. We could have deduced this regardless of where he had moved it, unless he had put it into one of the first three or last two positions.  b  5 ♦. Three cut-and- riffles will produce an intermixture of at most eight cyclically increasing subsequences axj a xj+1  mod n . . . a xj+1−1  mod n; hence the subsequence 6 4 ♦ is a dead giveaway. [Several magic tricks are based on the fact that three cut-and-riffles are highly non- random; see Martin Gardner, Mathematical Magic Show  Knopf, 1977 , Chapter 7.] 15. Set Yj ← j for t − n < j ≤ t. Then for j = t, t − 1, . . . , t − n + 1 do the following operations: Set k ← ⌊jU⌋ +1. If k > t− n then set Xj ← Yk and Yk ← Yj; otherwise if k = Xi for some i > j  a symbol table algorithm could be used , then set Xj ← Yi and Yi ← Yj; otherwise set Xj ← k.  The idea is to let Yt−n+1, . . . , Yj represent Xt−n+1, . . . , Xj, and if i > j and Xi ≤ t − n also to let Yi represent XXi, in the execution of Algorithm P. It is interesting to prove the correctness of Dahl’s algorithm. One basic observation is that, in step P2, Xk ̸= k implies Xk > j, for 1 ≤ k ≤ j.  16. We may assume that n ≤ 1 2 N, otherwise it suffices to find the N − n elements not in the sample. Using a hash table of size 2n, the idea is to generate random numbers between 1 and N, storing them in the table and discarding duplicates, until n distinct numbers have been generated. The average number of random numbers generated is N N + N  N − 1  + ··· + N  N − n + 1  < 2n, by exercise 3.3.2–10, and the average time to process each number is O 1 . We want to output the results in increasing order, and this can be done as follows: Using an ordered hash table  exercise 6.4–66  with linear probing, the hash table will appear as if the values had been inserted in increasing order and the average total number of probes will be less than 5 2 n. Thus if we use a monotonic hash address such as ⌊2n k − 1  N⌋ for the key k, it will be a simple matter to output the keys in sorted order by making at most two passes over the table. [See CACM 29  1986 , 366–367.] 17. Show inductively that before step j, the set S is a random sample of j − N − 1 + n integers from {1, . . . , j − 1}. [CACM 30  1987 , 754–757. Floyd’s method can be used to speed up the solution to exercise 16. It is essentially dual to Dahl’s algorithm in exercise 15, which operates for decreasing values of j; see exercise 12.] 18.  a  Oriented trees that essentially merge  1, 2, . . .   with  n, n − 1, . . .  , such as  1 2  3  4  5 6 7 8 9 10  11 12  13  14  26  25  24  23  22  21  20  19  18  17  16  15   3.5  ANSWERS TO EXERCISES  593  n!, see 5.1.4– 40 ;  c 2n   b  Collections of 1-cycles and 2-cycles.  c  Binary search trees on the keys  1, 2, . . . , n , with kj the parent of j  or j, at the root ; see Section 6.2.2. The number of  k1, . . . , kn  in each case is  a  2n−1;  b  tn ≥ √ n+1. [Case  a  represents the least common permutation; case  b  represents the most common, when n ≥ 18. See D. P. Robbins and E. D. Bolker, Æquationes Mathematicæ 22  1981 , 268–292; D. Goldstein and D. Moews, Æquationes Mathematicæ 65  2003 , 3–30.] 19. See N. Duffield, C. Lund, and M. Thorup, JACM 54  2007 , 32:1–32:37.   1  n  3, 2  3, 2  3, 2  3, 2  3, 2  3, 1  3, 1  3, 1  3, 2  3, 1  3, 2  3, 2  3, 2  3, 2  3 and approximately 2  SECTION 3.5 1. A b-ary sequence, yes  see exercise 2 ; a [0 . . 1  sequence, no  since only finitely many values are assumed by the elements . 2. It is 1-distributed and 2-distributed, but not 3-distributed  the binary number 111 never appears . 3. Repeat the sequence in exercise 3.2.2–17, with a period of length 27. 4. If ν1 n , ν2 n , ν3 n , ν4 n  are the counts for the four probabilities, we have ν1 n +ν2 n  = ν3 n +ν4 n  for all n. So the desired result follows by addition of limits. 5. The sequence begins 1 3, etc. When n = 1, 3, 7, 15, . . . we have ν n  = 1, 1, 5, 5, . . . so that ν 22k−1 − 1  = ν 22k − 1  =  22k− 1  3; hence ν n  n oscillates between 1 3, and no limit exists. The probability is undefined. [The methods of Section 4.2.4 show, however, that a numerical value can meaningfully be assigned to Pr Un < 1 2  = Pr leading digit of the radix-4 representation of n + 1 is 1 , namely log4 2 = 1 2.]  j=1 Pr Sj n  . As k → ∞, the latter is a monotone sequence bounded by 1, so it converges; and j=1 Pr Sj n   for all k. For a counterexample to equal- ity, it is not hard to arrange things so that Sj n  is always true for some j, yet Pr Sj n   = 0 for all j.  6. By exercise 4 and induction, Pr Sj n  for some j, 1 ≤ j ≤ k  =k Pr Sj n  for some j ≥ 1  ≥k 7. Let pi = to Pr Sj n  for some j ≥ 1  ≥  j≥1 Pr Sij n  . The result of the preceding exercise can be generalized So we have 1 = Pr Sij n  for some i, j ≥ 1  ≥  j≥1 Pr Sj n  , for any disjoint statements Sj n .  i≥1 Pr Sij n  for some j ≥ 1  ≥ enough so thatI i≥1 pi = 1, and hence Pr Sij n  for some j ≥ 1  = pi. Given ϵ > 0, let I be large ClearlyI  i=1 ϕi N  ≤ 1, and for all large enough N we haveI  i=2 pi−ϵ; hence ϕ1 N  ≤ 1−ϕ2 N −···−ϕI N  ≤ 1−p2−···−pI+ϵ ≤ 1− 1−ϵ−p1 +ϵ = p1+2ϵ. This proves that Pr S1j n  for some j ≥ 1  ≤ p1 + 2ϵ; hence Pr S1j n  for some j ≥ 1  = p1, and the desired result holds for i = 1. By symmetry of the hypotheses, it holds for any value of i. 8. Add together the probabilities for j, j + d, j + 2d, . . . , m + j − d in Definition E. 9. lim supn→∞ lim sup n→∞   an + bn  ≤ lim supn→∞   y1n − α 2 + ··· +  ymn − α 2  ≤ mα  ϕi N  =  number of n < N with Sij n  true for some j ≥ 1  N.  i=2 ϕi N  ≥I  i=1 pi ≥ 1 − ϵ. Let  an + lim supn→∞  bn; hence we find that  2 − 2mα  2 + mα  2 = 0,  and this can happen only if each  yjn − α  tends to zero. 10. In the evaluation of the sum in Eq.  22 .   594  ANSWERS TO EXERCISES  3.5  11. ⟨U2n⟩ is k-distributed if ⟨Un⟩ is  2, 2k − 1 -distributed. 12. Apply Theorem B with f x1, . . . , xk  = [u≤ max x1, . . . , xk  < v]. 13. Let  pk = Pr Un begins a gap of length k − 1   .  2 1 − p k−1  = Pr Un−1 ∈ [α . . β , Un  ∈ [α . . β , . . . , Un+k−2  ∈ [α . . β , Un+k−1 ∈ [α . . β   = p It remains to translate this into the probability that f n  − f n − 1  = k. Let νk n  =  number of j ≤ n with f j  − f j − 1  = k ; let µk n  = number of j ≤ n with Uj the beginning of a gap of length k−1 ; and let µ n  similarly count the number of 1 ≤ j ≤ n with Uj ∈ [α . . β . We have µk f n   = νk n , µ f n   = n. As n → ∞, we must have f n  → ∞, hence      νk n  n =  µk f n   f n   ·  f n  µ f n    → pk p = p 1 − p k−1 . [We have only made use of the fact that the sequence is  k + 1 -distributed.] 14. Let pk = Pr Un begins a run of length k    k+2   k+1   − k+2   − k+2  k  1  1  1  1  1  =  + 1  = Pr Un−1 > Un   Un+k  =   k+2 !   k+1 ! − k+1  k+2 !  see exercise 3.3.2–13 . Now proceed as in the previous exercise to transfer this to Pr f n −f n−1  = k . [We have assumed only that the sequence is  k+2 -distributed.] 15. For s, t ≥ 0 let pst = Pr Xn−2t−3 = Xn−2t−2 ̸= Xn−2t−1 ̸=···̸= Xn−1 and Xn =··· = Xn+s ̸= Xn+s+1  Pr Xn is not the beginning of a coupon set  = for t ≥ 0 let qt = Pr Xn−2t−2 = Xn−2t−1 ̸= ··· ̸= Xn−1  = 2−2t−1. By exercise 7, Pr Xn is the beginning of coupon set of length s + 2  =  = 2−s−2t−3;  t≥0 qt = 2 3; 3 · 2−s−1. t≥0 pst = 1  Now proceed as in exercise 13. 16.  Solution by R. P. Stanley.  Whenever the subsequence S =  b − 1 ,  b − 2 , . . . , 1, 0, 0, 1, . . . ,  b − 2 ,  b − 1  appears, a coupon set must end at the right of S, since some coupon set is completed in the first half of S. We now proceed to calculate the probability that a coupon set begins at position n by manipulating the probabilities that the last prior appearance of S ends at position n− 1, n− 2, etc., as in exercise 15. 18. Proceed as in the proof of Theorem A to calculate Pr and Pr. 19.  Solution by T. Herzog.  Yes. For example, apply exercise 33 to the sequence ⟨U⌊n 2⌋ 20.  a  2 and 1  ⟩, when ⟨Un⟩ satisfies R4  or even its weaker version .  1  n in half.   2.  When n increases, we break l   b  Each new point breaks a single interval into two parts. Let ρ be equal to k=0  n + k l k=0 ρ  n + k  = ρ ln 2 +   k  maxn−1 k=1 l m ≥ 1  ln 2 + O 1 m .  1  O 1 n . So infinitely many m have ml  n+k . Then 1 =n  n+k ≤n−1  n ≤n−1  and set ak = max m−n, m′−n, 1 . Then ρ = min2n   c  To verify the hint, let l  2n k=1 ρ  n + ak  ≥ 2ρn   k  2n come from the interval with endpoints Um and Um′, 2n ≥ k=1 1  n + k ; hence 2ρ ≤ 1  H2n − Hn  = 1  ln 2 + O 1 n .  m implies 1 =2n  m=n+1 ml  k=0 l  k=1 l   m    k    1    1    max{c1,...,ck}>r    1  ···  0   1  0  3.5  ANSWERS TO EXERCISES  595   1  n , . . . , l   n  n   =  lg n+1  n , lg n+2   d  We have  l  n+1 , . . . , lg 2n  point always breaks the largest interval into intervals of length lg 2n+1 2n [Indagationes Math. 11  1949 , 14–17.] 21.  a  No! We have Pr Wn < 1 and Pr Wn < 1 1 2  n k=0 2k+1 2 − 2k  + O n .  b, c  See Indagationes Math. 40  1978 , 527–541.  2  ≤ lim inf n→∞ ν 2n  2n =  2n−1 , because the  n + 1 st and lg 2n+2 2n+1. ν ⌈2n−1 2⌉  ⌈2n−1 2⌉ = 2 − √ 2, √ 2 − 1, because ν ⌈2n−1 2⌉  = ν 2n  =  2  ≥ lim supn→∞  22. If the sequence is k-distributed, the limit is zero by integration and Theorem B. Conversely, note that if f x1, . . . , xk  has an absolutely convergent Fourier series a c1, . . . , ck  exp 2πi c1x1 + ··· + ckxk  ,  f x1, . . . , xk  =   we have limN→∞  1 N   −∞<c1,...,ck<∞ 0≤n<N f Un, . . . , Un+k−1  = a 0, . . . , 0  + ϵr, where ϵr ≤  a c1, . . . , ck ,  so ϵr can be made arbitrarily small. Hence this limit is equal to  a 0, . . . , 0  =  f x1, . . . , xk  dx1 . . . dxk,  and Eq.  8  holds for all sufficiently smooth functions f. The remainder of the proof shows that the function in  9  can be approximated by smooth functions to any desired accuracy. 23.  a  This follows immediately from exercise 22.  b  Use a discrete Fourier transform in an analogous way; see D. E. Knuth, AMM 75  1968 , 260–264. 24.  a  Let c be any nonzero integer; we must show, by exercise 22, that  1 N  2πicUn → 0 e  as N → ∞.  This follows because, if K is any positive integer, we have K−1 KN−1 N−1  n=0 e2πicUn + O K2 . Hence, by Cauchy’s inequality,  2πicUn+k e  = 1  2  2πicUn e  1 N 2  K2N 2  + O  n=0  n=0  k=0  N  N−1  n=0 e2πicUn+k =  ≤ 1 K2N  = 1  + 2 K2N  ℜ  2πicUn+k e  + O     K   → 1  .  2πic Un+k−Un+j  e  + O  K  K  b  When d = 1, exercise 22 tells us that ⟨ α1n + α0  mod 1⟩ is equidistributed if and only if α1 is irrational. When d > 1, we can use  a  and induction on d. [Acta Math. 56  1931 , 373–456. The result in  b  had previously been obtained in a  0≤j<k<K  n=0  N  n=0  N−1 N−1 K−1 K−1 N−1    n=0  k=0  k=0    K  K   N  2 2 N−1   596  ANSWERS TO EXERCISES  3.5  more complicated way by H. Weyl, Nachr. Gesellschaft der Wiss. Göttingen, Math.- Phys. Kl.  1914 , 234–244. A similar argument proves that the polynomial sequence is equidistributed if at least one of the coefficients αd, . . . , α1 is irrational.] 25. If the sequence is equidistributed, the denominator in Corollary S approaches 1 12, and the numerator approaches the quantity in this exercise. 26. See Math. Comp. 17  1963 , 50–54. [Consider also the following example by A. G. Waterman: Let ⟨Un⟩ be an equidistributed [0 . . 1  sequence and ⟨Xn⟩ an ∞-distributed according as Xn is 0 or 1. Then ⟨Vn⟩ is binary sequence. Let Vn = U⌈√n ⌉ 2. Let Wn =  Vn − ϵn  mod 1 where equidistributed and white, but Pr Vn = Vn+1  = 1 ⟨ϵn⟩ is any sequence that decreases monotonically to 0; then ⟨Wn⟩ is equidistributed and white, yet Pr Wn < Wn+1  = 3 4.] 28. Let ⟨Un⟩ be ∞-distributed, and consider the sequence ⟨ 1 2 Xn + Un ⟩. This is 3-distributed, using the fact that ⟨Un⟩ is  16, 3 -distributed. 29. If x = x1x2 . . . xt is any binary number, we can consider the number νE times Xp . . . Xp+t−1 = x, where 1 ≤ p ≤ n and p is even. Similarly, let νO  n  ≈ ··· ≈ the number of times when p is odd. Let νE  x  n  = νx n . Now x  n  + νO νE ∗∗0...∗  0  n  =   n  ≈   n  ≈  x  n  of x  n  count  or 1− U⌈√n ⌉  where the ν’s in these summations have 2k subscripts, 2k − 1 of which are asterisks  meaning that they are being summed over — each sum is taken over 22k−1 combina- tions of zeros and ones , and where “≈” denotes approximate equality  except for an error of at most 2k due to end conditions . Therefore we find that n2kνE0  n  = 1 1 where x = x1 . . . x2k contains r x  zeros in odd positions and s x  zeros in even posi- tions. By  2k -distribution, the parenthesized quantity tends to k 22k−1  22k = k 2. The remaining sum is clearly a maximum if νE x  n  = νx n  when r x  > s x , and x  n  = 0 when r x  < s x . So the maximum of the right-hand side becomes νE  n   ν∗0∗...∗ n  + ··· + ν∗∗∗...0 n   1   x r x  − s x  νE  x  n  + O 1  ∗∗∗...0 n  νO  νO ∗0∗...∗  νE 0∗∗...∗   ,  νE  n  n   k    s  k   r − s    k 2 +   n  n  Now Pr X2n = 0  ≤ lim supn→∞  have  n   n   0≤s<r≤k  r,s  s  r  r  r  s  r,s  max r, s  = 2n22n−2 + n  min r, s  = 2n22n−2 − n  k  2k − 1 2n − 1 2n − 1  n      ;  .  n  22k = k  2 + k  22k.  νE0  2n  n, so the proof is complete. Note that we  30. Construct a digraph with 22k nodes labeled  Ex1 . . . x2k−1  and  Ox1 . . . x2k−1 , where each xj is either 0 or 1. Let there be 1 + f x1, x2, . . . , x2k  directed arcs from  Ex1 . . . x2k−1  to  Ox2 . . . x2k , and 1 − f x1, x2, . . . , x2k  directed arcs leading from  Ox1 . . . x2k−1  to  Ex2 . . . x2k , where f x1, x2, . . . , x2k  = sign x1 − x2 + x3 − x4 + ··· − x2k . We find that each node has the same number of arcs leading into it as there are leading out; for example,  Ex1 . . . x2k−1  has 1 − f 0, x1, . . . , x2k−1  + 1 − f 1, x1, . . . , x2k−1  leading in and 1 + f x1, . . . , x2k−1, 0  + 1 + f x1, . . . , x2k−1, 1  leading out, and f x, x1, . . . , x2k−1  = −f x1, . . . , x2k−1, x . Drop all nodes that have no paths leading either in or out, namely  Ex1 . . . x2k−1  if f 0, x1, . . . , x2k−1  = +1,   3.5  ANSWERS TO EXERCISES  597  Fig. A–5. Directed graph for the construction in exercise 30.  or  Ox1 . . . x2k−1  if f 1, x1, . . . , x2k−1  = −1. The resulting directed graph is seen to be connected, since we can get from any node to  E1010 . . . 1  and from this to any desired node. By Theorem 2.3.4.2G, there is a cyclic path traversing each arc; this path has length 22k+1, and we may assume that it starts at node  E00 . . . 0 . Construct a cyclic sequence with X1 = ··· = X2k−1 = 0, and Xn+2k−1 = x2k if the nth arc of the path is from  Ex1 . . . x2k−1  to  Ox2 . . . x2k  or from  Ox1 . . . x2k−1  to  Ex2 . . . x2k . For example, the graph for k = 2 is shown in Fig. A–5; the arcs of the cyclic path are numbered from 1 to 32, and the cyclic sequence is   00001000110010101001101110111110  00001 . . .  .  Notice that Pr X2n = 0  = 11 uted, since each  2k -tuple x1x2 . . . x2k occurs  16 in this sequence. The sequence is clearly  2k -distrib-  1 + f x1, . . . , x2k  + 1 − f x1, . . . , x2k  = 2  times in the cycle. The fact that Pr X2n = 0  has the desired value comes from the fact that the maximum value on the right-hand side in the proof of the preceding exercise has been achieved by this construction. 31. Use Algorithm W with rule R1 selecting the entire sequence. [For a generalization of this type of nonrandom behavior in R5-sequences, see Jean Ville, Étude Critique de la Notion de Collectif  Paris: 1939 , 55–62. Perhaps R6 is also too weak, from this standpoint, but no such counterexample is presently known.] 32. If R,R′ are computable subsequence rules, so is R′′ = RR′ defined by the following functions: f′′n x0, . . . , xn−1  = 1 if and only if R defines the subsequence xr1, . . . , xrk of x0, . . . , xn−1, where k ≥ 0 and 0 ≤ r1 < ··· < rk < n and f′k xr1 , . . . , xrk  = 1. 33. Given ϵ > 0, find N0 such that N > N0 implies that both νr N  N − p < ϵ and νs N  N − p   N1 implies that tN is rM or sM for  Now ⟨Xn⟩RR′ is  ⟨Xn⟩R R′. The result follows immediately.  O 1 0  0  E 0 0  1  O 0 1  0  E 1 0  0  O 0 0  1  9  7  12  14  13  15  30  18  E 1 1  0  O 0 1  1  E 1 0  1  O 1 1  0  E 0 1  1  17  19  10  8  2  6  3  11  21  25  26  22  E 0 0  0  1  32  O 0 0  0  E 1 1  1  27  28  O 1 1  1  5  31  4  16  20  24  29  23   598  ANSWERS TO EXERCISES  3.5  some M > N0. Now N > N1 implies that − p   νr Nr  + νs Ns    νt N    =  − p   =  N  N   νr Nr  − pNr + νs Ns  − pNs  Nr + Ns   < ϵ.  34. For example, if the binary representation of t is  1 0b−2 1 0a1 1 1 0a2 1 . . . 1 0ak 2, where “0a” stands for a sequence of a consecutive zeros, let the rule Rt accept Un if and only if ⌊bUn−k⌋ = a1, . . . , ⌊bUn−1⌋ = ak. 35. Let a0 = s0 and am+1 = max{sk  0 ≤ k < 2am}. Construct a subsequence rule that selects element Xn if and only if n = sk for some k < 2am, when n is in the range am ≤ n < am+1. Then limm→∞ ν am  am = 1 2. 36. Let b and k be arbitrary but fixed integers greater than 1. Let Yn = ⌊bUn⌋. An arbitrary infinite subsequence ⟨Zn⟩ = ⟨Ysn⟩R determined by algorithms S and R  as in the proof of Theorem M  corresponds in a straightforward but notationally hopeless manner to algorithms S′ and R′ that inspect Xt, Xt+1, . . . , Xt+s and or select Xt, Xt+1, . . . , Xt+min k−1,s  of ⟨Xn⟩ if and only if S and R inspect and or select Ys, where Us =  0.XtXt+1 . . . Xt+s 2. Algorithms S′ and R′ determine an infinite 1-distributed subsequence of ⟨Xn⟩ and in fact  as in exercise 32  this subsequence is ∞-distributed so it is  k, 1 -distributed. Hence we find that Pr Zn = a  and Pr Zn = a  differ from 1 b by less than 1 2k.  [The result of this exercise is true if “R6” is replaced consistently by “R4” or “R5”;  k , $N  = 1  k = 1  =   2  might be identically zero.]  2 − q0 + q1 + 1− q · q0 + 1 Pr B1 . . . Bk  Pr AP  but it is false if “R1” is used, since X n 37. For n ≥ 2 replace Un2 by 1 2 Un2 + δn , where δn = 0 or 1 according as the set {U n−1 2+1, . . . , Un2−1} contains an even or odd number of elements less than 1 2. [Advances in Math. 14  1974 , 333–334; see also the Ph.D. thesis of Thomas N. Herzog, Univ. of Maryland  1975 .] 39. See Acta Arithmetica 21  1972 , 45–50. The best possible value of c is unknown. 40. Since Fk depends only on B1 . . . Bk, we have P  AP 2. Let q B1 . . . Bk  = Pr Bk+1 = 1  B1 . . . Bk , where the probability is taken over all elements of S having B1 . . . Bk as the first k bits. Similarly, let qb B1 . . . Bk  = Pr Fk = 1 and B′k+1 = b  k = 1  B1 . . . Bk  = Pr  Fk+Bk+1+B′k+1  mod 2 = 1  B1 . . . Bk . Then we have Pr AP 2 − q0 + q1 +2 qq1 + 1 − q q0  = B1 . . . Bk  = q·  1 2 − Pr Fk = 1  B1 . . . Bk  + 2 Pr Fk = 1 and B′k+1 = Bk+1  B1 . . . Bk . Hence 1 2 − Pr Fk = 1  + Pr AP [See Theorem 4 of Goldreich, Goldwasser, and Micali in JACM 33 Pr Fk+1 = 1 .  1986 , 792–807.] 41. Choose k uniformly from {0, . . . , N − 1} and use the construction in the proof of Lemma P1. Then the proof of P1 shows that A′ will be equal to 1 with probability  N−1 2 − pk + pk+1  N. 42.  a  Let X = X1 + ··· + Xn. Clearly E X  = nµ; and we have E  X − nµ 2  = Also E  X − nµ 2  = j − nµ2 = nσ2. E X2 − n2µ2 = n E X2  x≥tnσ2 x Pr  X − nµ 2 = x  ≥ x≥tnσ2 tnσ2 Pr  X − nµ 2 = x  = tnσ2 Pr  X − nµ 2 ≥ tnσ2 .  b  There is a position i where ci ̸= c′i, say ci = 0 and c′i = 1. Then there’s a position j where cj = 1. For any fixed setting of B in the k − 2 rows other than i or j, we have  cB, c′B  =  d, d′  if and only if rows i and j have particular values; this occurs with probability 1 22R.  j + 2 x≥0 x Pr  X − nµ 2 = x  ≥  2 − q1  = 1 k = 1  B1 . . . Bk  = 1  1≤i<j≤n E Xi  E Xj  − n2µ2 = n E X2  k=0   1  B1...Bk   3.6  ANSWERS TO EXERCISES  599  µ = s and σ2 = 1 − s2. The probability that X =   c  In the notation of Algorithm L, take n = 2k − 1 and Xc =  −1 G cB+ei ; then c̸=0 Xc is negative is at most the probability that  X − nµ 2 ≥ n2µ2. By  a  this is at most σ2  nµ2 . 43. The conclusion for fixed M would be of no interest, since there obviously exists an algorithm to factor any fixed M  namely, an algorithm that knows the factors . The theory applies to all algorithms that have short running time, not only to algorithms that are effectively discoverable. 44. If every one-digit change to a random table yields a random table, all tables are random  or none are . If we don’t allow degrees of randomness, the answer must therefore be, “Not always.”  9F 8F XRAND 7F  SECTION 3.6 1. RANDI STJ STA LDA MUL INCX 1009 JOV *+1 SLAX 5 XRAND STA MUL 8F INCA 1 * 9H JMP 1 XRAND CON 0 CON 8H 3141592621 The multiplier a. 7H CON  Store exit location. Store value of k. rA ← X. rAX ← aX. rX ←  aX + c  mod m. Ensure that overflow is off. rA ←  aX + c  mod m. Store X. rA ← ⌊kX m⌋. Add 1, so that 1 ≤ Y ≤ k. Return. Value of X; X0 = 1. Temp storage of k.  2. Putting a random number generator into a program makes the results essentially unpredictable to the programmer. If the behavior of the machine on each problem were known in advance, few programs would ever be written. As Turing has said, the actions of a computer quite often do surprise its programmer, especially when a program is being debugged.  So the world had better watch out.  7. In fact, you only need the 2-bit values ⌊Xn 216⌋ mod 4; see D. E. Knuth, IEEE Trans. IT-31  1985 , 49–52. J. Reeds, Cryptologia 1  1977 , 20–26, 3  1979 , 83–95, initiated the study of related problems; see also J. Boyar, J. Cryptology 1  1989 , 177– 184. In SICOMP 17  1988 , 262–280, Frieze, Håstad, Kannan, Lagarias, and Shamir discuss general techniques that are useful in problems like this. 8. We can, say, generate X1000000 by making one million successive calls, and compare it to the correct value  a1000000X0 +  a1000000 − 1 c  a − 1   mod m, which can also be expressed as   a1000000 X0 a − 1  + c  − c  mod  a − 1 m   a − 1 . The latter can be evaluated quickly by an independent method  see Algorithm 4.6.3A . For example, 482711000000 mod 2147483647 = 1263606197. Most errors will be detected, because recurrence  1  is not self-correcting. 9.  a  The values of X0, X1, . . . , X99 are not all even. The polynomial z100 + z37 + 1 is primitive  see Section 3.2.2 ; hence there is a number h s  such that P0 z  ≡ zh s    600  ANSWERS TO EXERCISES  3.6   modulo 2 and z100 + z37 + 1 . Now zPn+1 z  = Pn z − Xnz37 − Xn+63 + Xn+63z100 + Xn+100z37 ≡ Pn z +Xn+63 z100+z37+1   modulo 2 , so the result holds by induction.  b  The operations “square” and “multiply by z” in ran start change p z  = x99z99 + ··· + x1z + x0 to p z 2 and zp z , respectively, modulo 2 and z100 + z37 + 1, because p z 2 ≡ p z2 .  We consider here only the low-order bits. The other bits are manipulated in an ad hoc way that tends to preserve and or enhance whatever disorder they already have.  Therefore if s =  1sj . . . s1s0 2 we have h s  =  1s0s1 . . . sj1 2 ·269.  c  zh s −n ≡ zh s′ −n′  modulo 2 and z100 + z37 + 1  implies that h s  − n ≡ h s′  − n′  modulo 2100 − 1 . Since 269 ≤ h s  < 2100 − 269, we have n − n′ ≥ h s  − h s′  ≥ 270.  [This method of initialization was inspired by comments of R. P. Brent, Proc. Australian Supercomputer Conf. 5  1992 , 95–104, although Brent’s algorithm was completely different. In general if the lags are k > l, if 0 ≤ s < 2e, and if the separation parameter t satisfies t + e ≤ k, this method of proof shows that n − n′ ≥ 2t − 1, with 2t − 1 occurring only if {s, s′} = {0, 2e − 1}.] 10. The following code belongs to the simplified language Subset FORTRAN, as de- fined by the American National Standards Institute, except for its use of PARAMETER statements for readability.  SUBROUTINE RNARRY AA,N  IMPLICIT INTEGER  A-Z  DIMENSION AA *  PARAMETER  KK=100  PARAMETER  LL=37  PARAMETER  MM=2**30  COMMON  RSTATE  RANX KK  SAVE  RSTATE  DO 1 J=1,KK  AA J =RANX J   DO 2 J=KK+1,N  1  2  3  4  AA J =AA J-KK -AA J-LL  IF  AA J  .LT. 0  AA J =AA J +MM  CONTINUE DO 3 J=1,LL  CONTINUE DO 4 J=LL+1,KK  CONTINUE END  RANX J =AA N+J-KK -AA N+J-LL  IF  RANX J  .LT. 0  RANX J =RANX J +MM  RANX J =AA N+J-KK -RANX J-LL  IF  RANX J  .LT. 0  RANX J =RANX J +MM  SUBROUTINE RNSTRT SEED  IMPLICIT INTEGER  A-Z  PARAMETER  KK=100  PARAMETER  LL=37  PARAMETER  MM=2**30  PARAMETER  TT=70    3.6  ANSWERS TO EXERCISES  601  PARAMETER  KKK=KK+KK-1  DIMENSION X KKK  COMMON  RSTATE  RANX KK  SAVE  RSTATE  IF  SEED .LT. 0  THEN  SSEED=MM-1-MOD -1-SEED,MM   ELSE  SSEED=MOD SEED,MM   END IF SS=SSEED-MOD SSEED,2 +2 DO 1 J=1,KK X J =SS SS=SS+SS IF  SS .GE. MM  SS=SS-MM+2  CONTINUE X 2 =X 2 +1 SS=SSEED T=TT-1 DO 12 J=KK,2,-1 X J+J-1 =X J  X J+J-2 =0  DO 14 J=KKK,KK+1,-1  END IF IF  SS .NE. 0  THEN  SS=SS 2  ELSE  T=T-1  END IF IF  T .GT. 0  GO TO 10 DO 20 J=1,LL  RANX J+KK-LL =X J   DO 21 J=LL+1,KK  RANX J-LL =X J   DO 22 J=1,10  CALL RNARRY X,KKK   END  1  10  12  14  16  20  21  22  X J- KK-LL  =X J- KK-LL  -X J  IF  X J- KK-LL   .LT. 0  X J- KK-LL  =X J- KK-LL  +MM X J-KK =X J-KK -X J  IF  X J-KK  .LT. 0  X J-KK =X J-KK +MM  CONTINUE IF  MOD SS,2  .EQ. 1  THEN  DO 16 J=KK,1,-1 X J+1 =X J   X 1 =X KK+1  X LL+1 =X LL+1 -X KK+1  IF  X LL+1  .LT. 0  X LL+1 =X LL+1 +MM   602  ANSWERS TO EXERCISES  3.6  11. Floating point arithmetic on 64-bit operands conforming to ANSI IEEE Standard 754 allows us to compute Un =  Un−100 − Un−37  mod 1 with perfect accuracy for fractions Un that are integer multiples of 2−53. However, the following program uses the additive recurrence Un =  Un−100 + Un−37  mod 1 on integer multiples of 2−52 instead, because pipelined computers can subtract an integer part more quickly than they can branch conditionally on the sign of an intermediate result. The theory of exercise 9 applies equally well to this sequence.  A FORTRAN translation similar to the code in exercise 10 will generate exactly  the same numbers as this C routine.   * the long lag *  define KK 100  * the short lag *  define LL 37  *  x+y  mod 1.0 *  define mod_sum x,y     x + y  - int   x + y    double ran_u[KK];  * the generator state *  void ranf_array double aa[],int n  {  * aa gets n random fractions *   register int i,j; for  j=0;j<KK;j++  aa[j]=ran_u[j]; for  ;j<n;j++  aa[j]=mod_sum aa[j-KK],aa[j-LL] ; for  i=0;i<LL;i++,j++  ran_u[i]=mod_sum aa[j-KK],aa[j-LL] ; for  ;i<KK;i++,j++  ran_u[i]=mod_sum aa[j-KK],ran_u[i-LL] ;  } define TT define is_odd s    s &1  void ranf_start long seed  {  70   * guaranteed separation between streams *    * do this before using ranf_array *   register int t,s,j; double u[KK+KK-1]; double ulp= 1.0  1L<<30    1L<<22 ; double ss=2.0*ulp*  seed&0x3fffffff +2 ; for  j=0;j<KK;j++  {  u[j]=ss; ss+=ss; if  ss>=1.0  ss-=1.0-2*ulp;  } u[1]+=ulp; for  s=seed&0x3fffffff,t=TT-1; t;   {  for  j=KK-1;j>0;j--   u[j+j]=u[j],u[j+j-1]=0.0; for  j=KK+KK-2;j>=KK;j--  {  u[j- KK-LL ]=mod_sum u[j- KK-LL ],u[j] ; u[j-KK]=mod_sum u[j-KK],u[j] ;  } if  is_odd s   {  for  j=KK;j>0;j--  u[j]=u[j-1]; u[0]=u[KK]; u[LL]=mod_sum u[LL],u[KK] ;  } if  s  s>>=1; else t--;  }   * 2 to the -52 *    * bootstrap the buffer *    * cyclic shift of 51 bits *    * make u[1]  and only u[1]  "odd" *    * "square" *    * "multiply by z" *    * shift the buffer cyclically *    3.6  ANSWERS TO EXERCISES  603  for  j=0;j<LL;j++  ran_u[j+KK-LL]=u[j]; for  ;j<KK;j++  ran_u[j-LL]=u[j]; for  j=0;j<10;j++  ranf_array u,KK+KK-1 ;  } int main   {  register int m; double a[2009]; ranf_start 310952 ; for  m=0;m<2009;m++  ranf_array a,1009 ;  printf "%.20f\n", ran_u[0] ; ranf_start 310952 ; for  m=0;m<1009;m++  ranf_array a,2009 ;  printf "%.20f\n", ran_u[0] ; return 0;  }   * warm everything up *    * a rudimentary test *    * 0.36410514377569680455 *    * 0.36410514377569680455 *   12. A simple linear congruential generator like  1  would fail, because m would be much too small. Good results are possible by combining three  not two  such generators, with multipliers and moduli  157, 32363 ,  146, 31727 ,  142, 31657 , as suggested by P. L’Ecuyer in CACM 31  1988 , 747–748. However, the best method is probably to use the C programs ran array and ran start, with the following changes to keep all numbers in range: ‘long’ becomes ‘int’; ‘MM’ is defined to be ‘ 1U<<15 ’; and the type of variable ss should be unsigned int. This generates 15-bit integers, all of whose bits are usable. The seed is now restricted to the range [0 . . 32765]. The “rudimentary test routine” will print X1009×2009 = 24130, given the seed 12509. 13. A program for subtract-with-borrow would be very similar to ran array, but slower because of the carry maintenance. As in exercise 11, floating point arithmetic could be used with perfect accuracy. It is possible to guarantee disjointness of the sequences produced from different seeds s by initializing the generator with the  −n th element of the sequence, where n = 270s; this requires computing bn mod  bk − bl ±1 . Squaring a radix-b number mod bk − bl ± 1 is, however, considerably more complicated than the analogous operation in program ran start, and for k in a practical range it takes about k1.6 operations instead of O k .  Both methods probably generate sequences of the same quality in practice, when they have roughly the same value of k. The only significant difference between them is a better theoretical guarantee and a provably immense period for the subtract-with- borrow method; the analysis of lagged Fibonacci generators is less complete. Experience shows that we should not reduce the value of k in subtract-with-borrow just because of these theoretical advantages. When all is said and done, lagged Fibonacci generators seem preferable from a practical standpoint; the subtract-with-borrow method is then valuable chiefly because of the insight it gives us into the excellent behavior of the simpler approach. 14. We have Xn+200 ≡  Xn + Xn+126   modulo 2 ; see exercise 3.2.2–32. Hence Yn+100 ≡ Yn + Yn+26 when n mod 100 > 73. Similarly Xn+200 ≡ Xn + Xn+26 + Xn+89; hence Yn+100 ≡ Yn+Yn+26+Yn+89 when n mod 100 < 11. Thus Yn+100 is a sum of only two or three elements of {Yn, . . . , Yn+99}, in 26% + 11% of all cases; a preponderance of 0s will then tend to make Yn+100 = 0.   604  ANSWERS TO EXERCISES  3.6  More precisely, consider the sequence ⟨u1, u2, . . .⟩ = ⟨126, 89, 152, 115, 78, . . . , 100,  63, 126, . . .⟩ where un+1 = un − 37 + 100[un < 100]. Then we have  Xn+200 =  Xn + Xn+v1 + ··· + Xn+vk−2 + Xuk−1  mod 2,  where vj = uj +  −1 [uj≥100]100; for example, Xn+200 ≡ Xn + Xn+26 + Xn+189 + Xn+152 ≡ Xn + Xn+26 + Xn+189 + Xn+52 + Xn+115. If the subscripts are all < n + t and ≥ n+100+ t, we obtain a k-term expression for Yn+100 when n mod 100 = 100− t, for 1 ≤ t ≤ 100. The case t = 63 is an exception, because Xn + Xn+1 + ··· + Xn+62 + Xn+163 + Xn+164 + ··· + Xn+199 ≡ 0; in this case Yn+100 is independent of {Yn, . . . , Yn+99}. The case t = 64 is interesting because it gives the 99-term relation Yn+100 ≡ Yn+1 + Yn+2 + ··· + Yn+99; this tends to be 0 in spite of the large number of terms, because most of the 100-tuples that have 40 or fewer 1s have even parity.  When there is a k-term relation, the probability that Yn+100 = 1 is  40  k  l=0  j=1  100 − k   k    l − j  j  pk =  [j odd]   40  100    .  l  l=0  l  l=0  100  The quantity t takes the values 100, 99, . . . , 1, 100, 99, . . . , 1, . . . as bits are printed; so we find that the expected number of 1s printed is 106 26p2 + 11p3 + 26p4 + 11p6 + 11p9 +4p12 +4p20 +3p28 + p47 + p74 + p99 +1 2  100 ≈ 14043. The expected number of   2100 ≈ 28444, so the expected number of 0s is ≈ 14401.  digits printed is 10640  The detectable bias goes away if more elements are discarded. For example, if we use only 100 elements of ran array a,300 , the probability can be shown to be  26p5 + 22p6+19p10+···   100; with ran array a,400  it is worse,  15p3+37p6+15p9+···   100, because Xn+400 ≡ Xn + Xn+252. With ran array a,1009  as recommended in the text we have  17p7+10p11+2p12+···   100, which can only be detected by such experiments if the threshold for printing is raised from 60 to, say, 75; but then the expected number of outputs is only about 0.28 per million trials.  [This exercise is based on ideas of Y. Kurita, H. Leeb, and M. Matsumoto, com-  municated to the author in 1997.] 15. The following program makes it possible to obtain a new random integer quickly with the expression ran arr next  , once ran start has been called to get things started: define QUALITY 1009  * recommended quality level for high-res use *  define KK 100  * the long lag *  long ran_arr_buf[QUALITY]; long ran_arr_sentinel=-1; long *ran_arr_ptr=&ran_arr_sentinel;  * the next random number, or -1 *  define ran_arr_next    *ran_arr_ptr>=0? *ran_arr_ptr++: ran_arr_cycle    long ran_arr_cycle   {  ran_array ran_arr_buf,QUALITY ; ran_arr_buf[KK]=-1; ran_arr_ptr=ran_arr_buf+1; return ran_arr_buf[0];  } Reset ran arr ptr = &ran arr sentinel if ran start is used again. SECTION 4.1 1.  1010 −2,  1011 −2,  1000 −2, . . . ,  11000 −2,  11001 −2,  11110 −2.   4.1  ANSWERS TO EXERCISES  605  2.  a  − 110001 2, − 11.001001001001 . . .  2,  11.00100100001111110110101 . . .  2.  b   11010011 −2,  1101.001011001011 . . .  −2,  111.0110010001000000101 . . .  −2.  c   11111 3,  10.011011011011 . . .  3,  10.0111111100010111110111111110 . . .  3.  d  − 9.4 1 10, −  . . . 7582417582413 1 10,  . . . 3462648323979853562951413 1 10.  3.  1010113.2 2i. 4.  a  Between rA and rX.  b  The remainder in rX has radix point between bytes 3 and 4; the quotient in rA has radix point one byte to the right of the least significant portion of the register. 5. It has been subtracted from 999 . . . 9 = 10p − 1, instead of from 1000 . . . 0 = 10p. 6.  a, c  2p−1 − 1, − 2p−1 − 1 ;  b  2p−1 − 1, −2p−1. 7. A ten’s complement representation for a negative number x can be obtained by considering 10n + x  where n is large enough for this to be positive  and extending it on the left with infinitely many nines. The nines’ complement representation can be obtained in the usual manner.  These two representations are equal for nonterminating decimals, otherwise the nines’ complement representation has the form . . .  a 99999 . . . while the ten’s complement representation has the form . . .  a + 1 0000 . . . .  The representations may be considered sensible if we regard the value of the infinite sum N = 9 + 90 + 900 + 9000 + ··· as −1, since N − 10N = 9.  See also exercise 31, which considers p-adic number systems. The latter agree with the p’s complement notations considered here, for numbers whose radix-p representation is terminating, but there is no simple relation between the field of p-adic numbers and the field of real numbers.  j ajbj =  8.  10.  . . . , a3, a2, a1, a0; a−1, a−2, . . .  . . . , b3, b2, b1, b0; b−1, b−2, . . .  9. A BAD AD0BE FACADE FADED. [Note: Other possible “number sentences” would be D0 A DEED A DECADE; A CAD FED A BABE BEEF, C0C0A, C0FFEE; B0B FACED A DEAD D0D0.]   =   . . . , A3, A2, A1, A0; A−1, A−2, . . .  . . . , B3, B2, B1, B0; B−1, B−2, . . .    ,  ,  Bj = bkj+1−1 . . . bkj ,  if   akj+1−1, akj+1−2, . . . , akj  bkj+1−2, . . . , bkj  Aj =  j akj+k−1bk−1 + ··· + akj bkj.  where ⟨kn⟩ is any doubly infinite sequence of integers with kj+1 > kj and k0 = 0. 11.  The following algorithm works both for addition or subtraction, depending on whether the plus or minus sign is chosen.   Start by setting k ← an+1 ← an+2 ← bn+1 ← bn+2 ← 0; then for m = 0, 1, . . . , n + 2 do the following: Set cm ← am ± bm + k; then if cm ≥ 2, set k ← −1 and cm ← cm − 2; otherwise if cm < 0, set k ← 1 and cm ← cm + 2; otherwise  namely if 0 ≤ cm ≤ 1 , set k ← 0. 12.  a  Subtract ± . . . a30a10 −2 from ± . . . a40a20a0 −2 in the negabinary system.  See also exercise 7.1.3–7 for a trickier solution that uses full-word bitwise operations.   b  Subtract  . . . b30b10 2 from  . . . b40b20b0 2 in the binary system. 13.  1.909090 . . .  −10 =  0.090909 . . .  −10 = 1 11.   606  14.  ANSWERS TO EXERCISES  1 1 3 2 1 1 1 3 2 1 1 1 3 2 1  1 1 2 0 2  1 2 1 2 3  1 1 3 2 1  1 1 3 2 1 0 1 0 3 1 1 2 0 1  [5 − 4i] [5 − 4i]  [9 − 40i]  4.1  11 . . 1   α0 P = α1;   αx1 P = αQx0.  α1 −P = α0.  11], and the rectangle on the right.  Fig. A–6. Fundamental region 15. [− 10 for quater-imaginary numbers. 16. It is tempting to try to do this in a very simple way, by using the rule 2 =  1100 i−1 to take care of carries; but that leads to a nonterminating method if, for example, we try to add 1 to  11101 i−1 = −1. The following solution does the job by providing four related algorithms  namely for adding or subtracting 1 or i . If α is a string of zeros and ones, let αP be a string of zeros and ones such that  αP  i−1 =  α i−1 + 1; and let α−P , αQ, α−Q be defined similarly, with −1, +i, and −i respectively in place of +1. Then  α0 Q = αP 1;  α0 −Q = αQ1;   α1 Q = α−Q0.  αx0 −P = α−Qx1;  α1 −Q = α−P 0. Here x stands for either 0 or 1, and the strings are extended on the left with zeros if necessary. The processes will clearly always terminate. Hence every number of the form a + bi with a and b integers is representable in the i − 1 system. 17. No  in spite of exercise 28 ; the number −1 cannot be so represented. This can be proved by constructing a set S as in Fig. 1. We do have the representations −i =  0.1111 . . .  1+i, i =  100.1111 . . . 1+i. 18. Let S0 be the set of points  a7a6a5a4a3a2a1a0 i−1, where each ak is 0 or 1.  Thus, yn = ∞k=1 ank16−k, where each ank is in S0. Construct a tree whose nodes are S0 is given by the 256 interior dots shown in Fig. 1, if that picture is multiplied by 16.  If {y1, y2, . . .} is an infinite subset of S, we have We first show that S is closed:  an1, . . . , anr , for 1 ≤ r ≤ n, and let a node of this tree be an ancestor of another node this tree has an infinite path  a1, a2, a3, . . .  ; consequently if it is an initial subsequence of that node. By the infinity lemma  Theorem 2.3.4.3K  k≥1 ak16−k is a limit point of {y1, y2, . . .} in S. By the answer to exercise 16, all numbers of the form  a+bi  16k are representable, when a and b are integers. Therefore if x and y are arbitrary reals and k ≥ 1, the number zk =  ⌊16kx⌋ + ⌊16ky⌋i  16k is in S + m + ni for some integers m and n. It can be shown that S + m + ni is bounded away from the origin when  m, n  ̸=  0, 0 . Consequently if x and y are sufficiently small and k is sufficiently large, we have zk ∈ S, and limk→∞ zk = x + yi is in S. [B. Mandelbrot named S the “twindragon” because he noticed that it is essentially obtained by joining two “dragon curves” belly-to-belly; see his book Fractals: Form, Chance, and Dimension  San Francisco: Freeman, 1977 , 313–314, where he also stated that the dimension of the boundary is 2 lg x ≈ 1.523627, where x = 1+2x−2 ≈ 1.69562. Other properties of the dragon curve are described in C. Davis and D. E. Knuth, J. Recr.  −4 + 2i  5  1 + 2i  5  −4− 8i  5  1− 8i  5   4.1  ANSWERS TO EXERCISES  607  Math. 3  1970 , 66–81, 133–149. The sets S for digits {0, 1} and other complex bases are illustrated and analyzed by D. Goffinet in AMM 98  1991 , 249–255.] I. Kátai and J. Szabó have shown that the radix −d+i yields a number system with digits {0, 1, . . . , d2}; see Acta Scient. Math. 37  1975 , 255–260. Further properties of such systems have been investigated by W. J. Gilbert, Canadian J. Math. 34  1982 , 1335–1348; Math. Magazine 57  1984 , 77–81. Another interesting case, with digits {0, 1, i,−1,−i} and radix 2 + i, has been suggested by V. Norton [Math. Magazine 57  1984 , 250–251]. For studies of number systems based on more general algebraic integers, see I. Kátai and B. Kovács, Acta Math. Acad. Sci. Hung. 37  1981 , 159–164, 405–407; B. Kovács, Acta Math. Hung. 58  1991 , 113–120; B. Kovács and A. Pethő, Studia Scient. Math. Hung. 27  1992 , 169–172. 19. If m > u or m < l, find a ∈ D such that m ≡ a  modulo b ; the desired representation will be a representation of m′ =  m − a  b followed by a. Note that m > u implies l < m′ < m; m < l implies m < m′ < u; so the algorithm terminates. [There are no solutions when b = 2. The representation will be unique if and only if 0 ∈ D; nonunique representation occurs for example when D = {−3,−1, 7}, b = 3, since  α 3 =  3773α 3. When b ≥ 3 it is not difficult to show that there are exactly 2b−3 solution sets D in which a < b for all a ∈ D. Furthermore the set D = {0, 1, 2 − ϵ2bn, 3 − ϵ3bn, . . . , b − 2 − ϵb−2bn, b − 1 − bn} gives unique representations, for all b ≥ 3 and n ≥ 1, when each ϵj is 0 or 1. References: Proc. IEEE Symp. Comp. Arith. 4  1978 , 1–9; JACM 29  1982 , 1131–1143.] 20.  a  0.111 . . . = 1.888 . . . = 18. 111 111 . . . has nine representations.  b  A “D-fraction” .a1a2 . . . always lies between −1 9 and +71 9. Suppose x has ten or more D-decimal representations. Then for sufficiently large k, 10kx has ten representations that differ to the left of the decimal point: 10kx = n1 + f1 = ··· = n10 + f10 where each fj is a D-fraction. By uniqueness of integer representations, the nj are distinct, say n1 < ··· < n10, hence n10 − n1 ≥ 9; but this implies f1 − f10 ≥ 9 > 71 9 −  −1 9 , a contradiction.  c  Any number of the form 0.a1a2 . . . , where each aj is −1 or 8, equals 1.a′1a′2 . . . where a′j = aj + 9  and it even has six more representations 18.a′′1 a′′2 . . . , etc. . 21. We can convert to such a representation by using a method like that suggested in the text for converting to balanced ternary.  666 . . . = ··· = 18 123456  777 . . . = 18 1  765432 . 777  2 ∗, 5 − 4 1  2 ∗, 5 − 3 1  2 + 1 2 ∗ =  ±4 1  2 ∗, 50 − 45 − 4 1  k≥1 −4 1 2 ∗, 50 − 45 − 3 1  2  10−1 + 10−2 + ···  . [AMM 57  1950 , 90–93.]  In contrast to the system of exercise 20, zero can be represented in infinitely 2  · 10−k  or from the negative of this many ways, all obtained from 1 representation  by multiplying it by a power of ten. The representations of unity are 2 − 1 2 ∗, 1 1 1 2 ∗, etc., where ± 1  22. Given some approximation bn . . . b1b0 with errorn started by finding a suitable n k=0 bk10k ≈n and 10n  +n 23. The set S = { measurable, and in fact it has positive measure. Since bS =  bµ S  = µ bS  ≤  k=0 bk10k − x > 10−t for t > 0, we will show how to reduce the error by approximately 10−t.  The process can be k=0 bk10k > x; then a finite number of reductions of this type will make the error less than ϵ.  Simply choose m > n so large that the decimal representation of −10mα has a one in position 10−t and no ones in positions 10−t+1, 10−t+2, . . . , 10n. Then 10mα +  a suitable sum of powers of 10 between 10m k≥1 akb−k  ak ∈ D} is closed as in exercise 18, hence it is a∈D µ a + S  = a∈D a + S , we have a∈D µ S  = bµ S , and we must therefore have  k=0 bk10k − 10−t.  2 +  2 − 1  2 − 1  2 + 1  2 + 1  7 . 222   608  ANSWERS TO EXERCISES  4.1  µ  a + S  ∩  a′ + S   = 0 when a ̸= a′ ∈ D. Now T has measure zero if 0 ∈ D, since T is a union of countably many sets of the form bk n +   a + S  ∩  a′ + S   , a ̸= a′, each of measure zero. On the other hand, as pointed out by K. A. Brakke, every real number has infinitely many representations in the number system of exercise 21.  the digits must be integers. Proof. Let S = { [0 . .∞  =   [The set T cannot be empty, since the real numbers cannot be written as a countable union of disjoint, closed, bounded sets; see AMM 84  1977 , 827–828, and the more detailed analysis by Petkovšek in AMM 97  1990 , 408–411. If D has fewer than b elements, the set of numbers representable with radix b and digits from D has measure zero. If D has more than b elements and represents all reals, T has infinite measure.] 24. {2a · 10k + a′  0 ≤ a < 5, 0 ≤ a′ < 2} or {5a′ · 10k + a  0 ≤ a < 5, 0 ≤ a′ < 2}, for k ≥ 0. [R. L. Graham has shown that there are no more sets of integer digits with these properties. And Andrew Odlyzko has shown that the restriction to integers is superfluous, in the sense that if the smallest two elements of D are 0 and 1, all k<0 akbk  ak ∈ D} be the set of “fractions,” and let X = { an . . . a0 b  ak ∈ D} be the set of “whole numbers”; then x∈X x + S , and  x + S  ∩  x′ + S  has measure zero for x ̸= x′ ∈ X. We have  0 . . 1  ⊆ S, and by induction on m we will prove that  m . . m + 1  ⊆ xm + S for some xm ∈ X. Let xm ∈ X be such that  m . . m + ϵ  ∩  xm + S  has positive measure for all ϵ > 0. Then xm ≤ m, and xm must be an integer lest x⌊xm⌋ + S overlap xm + S too much. If xm > 0, the fact that  m − xm . . m − xm + 1  ∩ S has positive measure implies by induction that this measure is 1, and  m . . m+1  ⊆ xm + S since S is closed. If xm = 0 and  m . . m + 1  ̸⊆ S, we must have m < x′m < m + 1 for some x′m ∈ X, where  m . . x′m  ⊆ S; but then 1 + S overlaps x′m + S. See Proc. London Math. Soc.  3  18  1978 , 581–595.] Note: If we drop the restriction 0 ∈ D, there are many other cases, some of which are quite interesting, especially {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {1, 2, 3, 4, 5, 51, 52, 53, 54, 55}, and {2, 3, 4, 5, 6, 52, 53, 54, 55, 56}. Alternatively if we allow negative digits we obtain many other solutions by the method of exercise 19, plus further sets of unusual digits like {−1, 0, 1, 2, 3, 4, 5, 6, 7, 18} that don’t meet the conditions stated there. It appears hopeless to find a nice characterization of all solutions with negative digits. 25. A positive number whose radix-b representation has m consecutive  b− 1 ’s to the right of the radix point must have the form c bn +  bm − θ  bn+m, where c and n are nonnegative integers and 0 < θ ≤ 1. So if u v has this form, we find that bm+nu = bmcv + bmv − θv. Therefore θv is an integer that is a multiple of bm. But 0 < θv ≤ v < bm. [There can be arbitrarily long runs of other digits a, if 0 ≤ a < b − 1, for example in the representation of a  b − 1 .] 26. The proof of “sufficiency” is a straightforward generalization of the usual proof for base b, by successively constructing the desired representation. The proof of “necessity” k≤n ckβk for some n, then βn+1 − ϵ k≤n ckβk for all n, but equality does [See  breaks into two parts: If βn+1 is greater than  has no representation for small ϵ. If βn+1 ≤   not always hold, we can show that there are two representations for certain x. Transactions of the Royal Society of Canada, series III, 46  1952 , 45–55.] 27. Proof by induction on n: If n is even we must take e0 > 0, and the result follows by induction, since n 2 has a unique such representation. If n is odd, we must take e0 = 0, and the problem reduces to representing − n−1  2; if the latter quantity is either zero or one, there is obviously only one way to proceed, otherwise it has a unique reversing [A. D. Booth, in Quarterly J. Mechanics and Applied representation by induction. Math. 4  1951 , 236–240, applied this principle to two’s complement multiplication.]   4.1  ANSWERS TO EXERCISES  609  [It follows that every positive integer has exactly two such representations with  decreasing exponents e0 > e1 > ··· > et: one with t even and the other with t odd.] 28. A proof like that of exercise 27 may be given. Note that a + bi is a multiple of 1 + i by a complex integer if and only if a + b is even. This representation is intimately related to the dragon curve discussed in the answer to exercise 18. 29. It suffices to prove that any collection {T0, T1, T2, . . .} satisfying Property B may be obtained by collapsing some collection {S0, S1, S2, . . .}, where S0 = {0, 1, . . . , b − 1} and all elements of S1, S2, . . . are multiples of b. To prove the latter statement, we may assume that 1 ∈ T0 and that there is a least element b > 1 such that b  ∈ T0. We will prove, by induction on n, that if nb  ∈ T0, then nb + 1, nb + 2, . . . , nb + b − 1 are not in any of the Tj’s; but if nb ∈ T0, then so are nb + 1, . . . , nb + b − 1. The result then follows with S1 = {nb  nb ∈ T0}, S2 = T1, S3 = T2, etc. If nb  ∈ T0, then nb = t0 + t1 + ··· , where t1, t2, . . . are multiples of b; hence t0 < nb is a multiple of b. By induction,  t0 + k  + t1 + t2 + ··· is the representation of nb + k, for 0 < k < b; hence nb + k  ∈ Tj for any j. If nb ∈ T0 and 0 < k < b, let the representation of nb + k be t0 + t1 + ··· . We cannot have tj = nb + k for j ≥ 1, lest nb + b have two representations  b − k  + ··· +  nb+ k +··· =  nb +···+ b+··· . By induction, t0 mod b = k; and the representation nb =  t0 − k  + t1 + ··· implies that t0 = nb + k.  [Reference: Nieuw Archief voor Wiskunde  3  4  1956 , 15–17. A finite analog of this result was derived by P. A. MacMahon, Combinatory Analysis 1  1915 , 217–223.] 30.  a  Let Aj be the set of numbers n whose representation does not involve bj; then by the uniqueness property, n ∈ Aj if and only if n + bj  ∈ Aj. Consequently we have n ∈ Aj if and only if n + 2bj ∈ Aj. It follows that, for j ̸= k, n ∈ Aj ∩ Ak if and only if n + 2bjbk ∈ Aj ∩ Ak. Let m be the number of integers n ∈ Aj ∩ Ak such that 0 ≤ n < 2bjbk. Then this interval contains exactly m integers that are in Aj but not Ak, exactly m in Ak but not Aj, and exactly m in neither Aj nor Ak; hence 4m = 2bjbk. Therefore bj and bk cannot both be odd. But at least one bj is odd, of course, since odd numbers can be represented.   b  According to  a  we can renumber the b’s so that b0 is odd and b1, b2, . . . are even; then 1 2 b2, . . . must also be a binary basis, and the process can be iterated.  c  If it is a binary basis, we must have positive and negative dk’s for arbitrarily large k, in order to represent ±2n when n is large. Conversely, the following algorithm may be used:  2 b1, 1  S1. [Initialize.] Set k ← 0. S2. [Done?] If n = 0, terminate. S3. [Choose.] If n is even, set n ← n 2. Otherwise include 2kdk in the represen-  tation, and set n ←  n − dk  2.  S4. [Advance k.] Increase k by 1 and return to S2. At each step the choice is forced; furthermore step S3 always decreases n unless n = −dk, hence the algorithm must terminate.  d  Two iterations of steps S2–S4 in the preceding algorithm will change 4m → m, 4m + 1 → m + 5, 4m + 2 → m + 7, 4m + 3 → m − 1. Arguing as in exercise 19, we need only show that the algorithm terminates for −2 ≤ n ≤ 8; all other values of n are moved toward this interval. In this range 3 → −1 → −2 → 6 → 8 → 2 → 7 → 0 and 4 → 1 → 5 → 6. Thus 1 = 7 · 20 − 13 · 21 + 7 · 22 − 13 · 23 − 13 · 25 − 13 · 29 + 7 · 210.   610  ANSWERS TO EXERCISES  4.1  Note: The choice d0, d1, d2, . . . = 5, −3, 3, 5, −3, 3, . . . also yields a binary basis. For further details see Math. Comp. 18  1964 , 537–546; A. D. Sands, Acta Math. Acad. Sci. Hung. 8  1957 , 65–86. 31.  See also the related exercises 3.2.2–11, 4.3.2–13, 4.6.2–22.    a  By multiplying numerator and denominator by suitable powers of 2, we may assume that u =   . . . u2u1u0 2 and v =   . . . v2v1v0 2 are 2-adic integers, where v0 = 1. The following computational method now determines w, using the notation u n  to stand for the integer  un−1 . . . u0 2 = u mod 2n when n > 0: Let w0 = u0 and w 1  = w0. For n = 1, 2, . . . , assume that we have found an integer w n  =  wn−1 . . . w0 2 such that u n  ≡ v n w n   modulo 2n . Then we have u n+1  ≡ v n+1 w n   modulo 2n , hence wn = 0 or 1 according as the quantity  u n+1  − v n+1 w n   mod 2n+1 is 0 or 2n.  b  Find the smallest integer k such that 2k ≡ 1  modulo 2n + 1 . Then we have 1  2n + 1  = m  2k − 1  for some integer m, 1 ≤ m < 2k−1. Let α be the k-bit binary representation of m; then  0.ααα . . .  2 times 2n + 1 is  0.111 . . .  2 = 1 in the binary system, and   . . . ααα 2 times 2n + 1 is   . . . 111 2 = −1 in the 2-adic system.   c  If u is rational, say u = m  2en  where n is odd and positive, the 2-adic representation of u is periodic, because the set of numbers with periodic expansions includes −1 n and is closed under the operations of negation, division by 2, and addition. Conversely, if uN+λ = uN for all sufficiently large N, the 2-adic number  2λ − 1 2ru is an integer for all sufficiently large r.   d  The square of any number of the form   . . . u2u11 2 has the form   . . . 001 2, hence the condition is necessary. To show the sufficiency, we can use the following procedure to compute v =  √ n when n mod 8 = 1:  H1. [Initialize.] Set m ←  n − 1  8, k ← 2, v0 ← 1, v1 ← 0, v ← 1.  During this  H2. [Transform.] If m is even, set vk ← 0, m ← m 2. Otherwise set vk ← 1,  algorithm we will have v =  vk−1 . . . v1v0 2 and v2 = n − 2k+1m.  m ←  m − v − 2k−1  2, v ← v + 2k.  H3. [Advance k.] Increase k by 1 and return to H2.  32. A more general result appears in Math. Comp. 29  1975 , 84–86. 33. Let Kn be the set of all such n-digit numbers, so that kn = Kn. If S and T are any finite sets of integers, we shall say S ∼ T if S = T + x for some integer x, and we shall write kn S  = Kn S , where Kn S  is the family of all subsets of Kn that are ∼ S. When n = 0, we have kn S  = 0 unless S ≤ 1, since zero is the only “0-digit” number. When n ≥ 1 and S = {s1, . . . , sr}, we have  Kn S  =     {{t1b + a1, . . . , trb + ar}  0≤j<b   a1,...,ar   {t1, . . . , tr} ∈ Kn−1 { si + j − ai  b  1 ≤ i ≤ r} }, where the inner union is over all sequences of digits  a1, . . . , ar  satisfying the con- dition ai ≡ si + j  modulo b  for 1 ≤ i ≤ r. In this formula we require ti − ti′ =  si − ai  b −  si′ − ai′  b for 1 ≤ i < i′ ≤ r, so that the naming of subscripts is uniquely determined. By the principle of inclusion and exclusion, therefore, we have m≥1 −1 m−1f S, m, j , where f S, m, j  is the number of sets of integers that can be expressed as {t1b + a1, . . . , trb + ar} in the manner above for m different sequences  a1, . . . , ar , summed over all choices of m different sequences r   for 1 ≤ l ≤ m, the number of  l   a1, . . . , ar . Given m different sequences  a  kn S  =     0≤j<b   l  1 , . . . , a   4.2.1 such sets is kn−1 { si + j − a of sets T  S  such that  611 i   b  1 ≤ i ≤ r, 1 ≤ l ≤ m} . Thus there is a collection  ANSWERS TO EXERCISES   l   kn S  =   T∈T  S   cT kn−1 T  ,  where each cT is an integer. Furthermore if T ∈ T  S , its elements are near those of S; we have min T ≥  min S − max D  b and max T ≤  max S + b − 1 − min D  b. Thus we obtain simultaneous recurrence relations for the sequences ⟨kn S ⟩, where S runs through the nonempty integer subsets of [l . . u + 1], in the notation of exercise 19. Since kn = kn S  for any one-element set S, the sequence ⟨kn⟩ appears among these recurrences. The coefficients cT can be computed from the first few values of kn S , so we can obtain a system of equations defining the generating functions kS z  =  For example, when D = {−1, 0, 3} and b = 3 we have l = − 3  2, so the relevant sets S are {0}, {0, 1}, {−1, 1}, and {−1, 0, 1}. The corresponding sequences for n ≤ 3 are ⟨1, 3, 8, 21⟩, ⟨0, 1, 3, 8⟩, ⟨0, 0, 1, 4⟩, and ⟨0, 0, 0, 0⟩; so we obtain  2 and u = 1  T∈T  S  cT kT  z . [See J. Algorithms 2  1981 , 31–43.]   kn S zn = [S≤ 1] + z  k0 z  = 1 + z 3k0 z  − k01 z  , k01 z  = zk0 z ,  k02 z  = z k01 z  + k02 z  , k012 z  = 0,  and k z  = 1  1 − 3z + z2 . In this case kn = F2n+2 and kn {0, 2}  = F2n−1 − 1. 34. There is exactly one string αn on the symbols {1, 0, 1} such that n =  αn 2 and αn has no leading zeros or consecutive nonzeros: α0 is empty, otherwise α2n = αn0, α4n+1 = αn01, α4n−1 = αn01. Any string that represents n can be converted to this “canonical signed bit representation” by using the reductions 11 → 01, 11 → 01, 01 . . . 11 → 10 . . . 01, 01 . . . 11 → 10 . . . 01, and inserting or deleting leading zeros. Since these reductions do not increase the number of nonzero digits, αn has the fewest. [Advances in Computers 1  1960 , 244–260.] The number of nonzero digits in αn, denoted by ν n , is the number of 1s in the ordinary representation that are immediately preceded by 0 or by the substring 00 10 k1 for some k ≥ 0.  See exercise 7.1.3–35.   A generalization to radix b > 2 has been given by J. von zur Gathen, Computa-  tional Complexity 1  1991 , 360–394.  SECTION 4.2.1 1. N =  62, +.60 22 14 00 ; h =  37, +.66 26 10 00 . Note that the quantity 10h would be  38, +.06 62 61 00 . 2. bE−q 1 − b−p , b−q−p; bE−q 1 − b−p , b−q−1. 3. When e does not have its smallest value, the most significant “one” bit  which appears in all such normalized numbers  need not appear in the computer word. 4.  51, +.10209877 ;  50, +.12346000 ;  53, +.99999999 . The third answer would be  54, +.10000000  if the first operand had been  45,−.50000000 , since b 2 is odd. 5. If x ∼ y and m is an integer then mb + x ∼ mb + y. Furthermore x ∼ y implies x b ∼ y b, by considering all possible cases. Another crucial property is that x and y will round to the same integer, whenever bx ∼ by. Now if b−p−2Fv ̸= fv we must have  bp+2fv  mod b ̸= 0; hence the transformation leaves fv unchanged unless eu − ev ≥ 2. Since u was normalized, it is nonzero and fu + fv > b−1 − b−2 ≥ b−2: The leading nonzero digit of fu + fv must be at most two places to the right of the radix point, and the rounding operation will convert   612  ANSWERS TO EXERCISES  4.2.1  bp+j fu + fv  to an integer, where j ≤ 1. The proof will be complete if we can show that bp+j+1 fu + fv  ∼ bp+j+1 fu + b−p−2Fv . By the previous paragraph, we have bp+2 fu + fv  ∼ bp+2fu + Fv = bp+2 fu + b−p−2Fv , which implies the desired result for all j ≤ 1. Similar remarks apply to step M2 of Algorithm M.  6 < f < 1  2 b is possible.   01, +.10345678  ⊕  00,−.94000000 ;  Note that, when b > 2 is even, such an integer Fv always exists; but when b = 2 we require p+3 bits  let 2Fv be an integer . When b is odd, an integer Fv always exists except in the case of division by Algorithm M, when a remainder of 1 6.  Consider the case eu = ev, fu = −fv in Program A.  Register A retains its previous sign, as in ADD. 7. Say that a number is normalized if and only if it is zero or its fraction part lies in the range 1 2. A  p + 1 -place accumulator suffices for addition and subtraction; rounding  except during division  is equivalent to truncation. A very pleasant system indeed! We might represent numbers with excess-zero exponent, inserted between the first and subsequent digits of the fraction, and complemented if the fraction is negative, so that the order of fixed point numbers is preserved. 8.  a   06, +.12345679  ⊕  06,−.12345678 ,  b   99, +.87654321  ⊕ itself,  99, +.99999999  ⊕  91, +.50000000 . 9. a = c =  −50, +.10000000 , b =  −41, +.20000000 , d =  −41, +.80000000 , y =  11, +.10000000 . 10.  50, +.99999000  ⊕  55, +.99999000 . 11.  50, +.10000001  ⊗  50, +.99999990 . 12. If 0 < fu < fv, then fu ≤ fv − b−p; hence 1 b < fu fv ≤ 1 − b−p fv < 1 − b−p. If 0 < fv ≤ fu, we have 1 b ≤ fu fv b ≤   1 − b−p   1 b   b = 1 − b−p. 13. See J. Michael Yohe, IEEE Trans. C-22  1973 , 577–586; see also exercise 4.2.2–24. 14. FIX STJ 9F TEMP STA TEMP EXP  rI1 ← e. LD1 1 SLA JAZ 9F DEC1 1 CMPA =0= 1:1  JE ENN1 -Q-4,1 J1N ENTX 0 SRAX 0,1 CMPX =1  2= JL JG JAO STA INCA 1 * JMP STJ EXITF OFLO JOV STA TEMP  The ambiguous case becomes odd, since b 2 is even. Round, if necessary. Add ±1  overflow is impossible . Exit from subroutine.  Fractional part subroutine: Ensure that overflow is off. TEMP ← u.  rA ← ± f f f f 0. Is input zero?  9F *+2 9F *+1 0:0   Is magnitude too large?  Float-to-fix subroutine:  If leading byte is zero,  shift left again.  9H 15. FP  FIXOVFLO  *-4   4.2.2  ANSWERS TO EXERCISES  613  rA ← fu. rI2 ← eu.  Remove integer part of u.  Fraction is negative: Find  its complement.  ENTX 0 1 SLA LD2 TEMP EXP  DEC2 Q J2NP *+3 SLA 0,2 ENT2 0 JANN 1F ENN2 0,2 SRAX 0,2 ENT2 0 JXNZ *+3 JAZ *+2 INCA 1 ADD INC2 Q JMP 8H EQU WM1 CON  WM1  1H  Add word size minus one. Prepare to normalize the answer. Normalize, round, and exit.  NORM 1 1:1  8B-1,8B-1 1:4  Word size minus one  16. If c ≥ d, then set r ← d ⊘ c, s ← c ⊕  r ⊗ d ; x ←  a ⊕  b ⊗ r   ⊘ s, y ←  b ⊖  a ⊗ r   ⊘ s. Otherwise set r ← c ⊘ d, s ← d ⊕  r ⊗ c ; x ←   a ⊗ r  ⊕ b  ⊘ s, y ←   b ⊗ r  ⊖ a  ⊘ s. Then x + iy is the desired approximation to  a + bi   c + di . Computing s′ ← 1 ⊘ s and multiplying twice by s′ may be better than dividing twice by s. As with  11 , gradual underflow is recommended for the calculation of r unless [CACM 5  1962 , 435. Other algorithms for complex special precautions are taken. arithmetic and function evaluation are given by P. Wynn, BIT 2  1962 , 232–255. For a + bi, see Paul Friedland, CACM 10  1967 , 665.] 17. See Robert Morris, IEEE Trans. C-20  1971 , 1578–1579. Error analysis is more difficult with such systems, so interval arithmetic is correspondingly more desirable. 18. For positive numbers: Shift fraction left until f1 = 1, then round, then if the fraction is zero  rounding overflow  shift it right again. For negative numbers: Shift fraction left until f1 = 0, then round, then if the fraction is zero  rounding underflow  shift it right again. 2 0 . . . 0]  6−[magnitude is rounded up] +[ev<eu]+ 19.  73− 5−[rounding digits are b 2 ]−[fraction overflow]−10[result zero]+7[rounding overflow]+ [first rounding digit is b 7N +  3 +  16 + [result negative] [opposite signs] X u, where N is the number of left shifts during normalization, and X is the condition that rX receives nonzero digits and there is no fraction overflow. The maximum time of 84u occurs for example when  u = −50 01 00 00 00,  v = +45 49 99 99 99,  b = 100.  [The average time, considering the data in Section 4.2.4, will be less than 47u.] SECTION 4.2.2 1. u ⊖ v = u ⊕ −v = −v ⊕ u = − v ⊕ −u  = − v ⊖ u . 2. u ⊕ x ≥ u ⊕ 0 = u, by  8 ,  2 ,  6 ; hence by  8  again,  u ⊕ x  ⊕ v ≥ u ⊕ v. Similarly,  8  and  6  together with  2  imply that  u ⊕ x  ⊕  v ⊕ y  ≥  u ⊕ x  ⊕ v. 3. u = 8.0000001, v = 1.2500008, w = 8.0000008;  u ⊗ v  ⊗ w = 80.000064, yet u ⊗  v ⊗ w  = 80.000057.   614  ANSWERS TO EXERCISES  4.2.2  4. Yes; let 1 u ≈ v = w, where v is large. 5. Not always; in decimal arithmetic take u = v = 9. 6.  a  Yes.  b  Only for b + p ≤ 4  try u = 1 − b−p . But see exercise 27. 7. If u and v are consecutive floating binary numbers, u ⊕ v = 2u or 2v. When it is 2v we often have u 2⃝ ⊕ v 2⃝ < 2v 2⃝. For example, u =  .10 . . . 001 2, v =  .10 . . . 010 2, u ⊕ v = 2v, and u 2⃝ + v 2⃝ =  .10 . . . 011 2. 8.  a  ∼, ≈;  b  ∼, ≈;  c  ∼, ≈;  d  ∼;  e  ∼. 9. u−w ≤ u−v+v−w ≤ ϵ1 min beu−q, bev−q +ϵ2 min bev−q, bew−q  ≤ ϵ1beu−q+ ϵ2bew−q ≤  ϵ1 + ϵ2  max beu−q, bew−q . The result cannot be strengthened in general, since for example we might have eu very small compared to both ev and ew, and this means that u − w might be fairly large under the hypotheses. 10. We have  .a1 . . . ap−1ap b⊗ .9 . . . 99 b =  .a1 . . . ap−1 ap−1  b if ap ≥ 1 and a1 ≥ b 2; here “9” stands for b−1. Furthermore,  .a1 . . . ap−1ap b⊗ 1.0 . . . 0 b =  .a1 . . . ap−10 b, so the multiplication is not monotone if b > 2 and ap ≥ 1 + [a1 ≥ b 2 ]. But when b = 2, this argument can be extended to show that multiplication is monotone; obviously the “certain computer” had b > 2. 11. Without loss of generality, let x be an integer, 0 ≤ x < bp. If e ≤ 0, then t = 0. If 0   p, then x − t = 0. [The result holds also under the weaker hypothesis t < be; in that case we might have x − t = be when e > p.] 12. Assume that eu = p, ev ≤ 0, u > 0. Case 1, u > bp−1. Case  1a , w = u + 1, v ≥ 1 2, ev = 0. Then u′ = u or u + 1, v′ = 1, u′′ = u, v′′ = 1 or 0. Case  1b , w = u, v ≤ 1 2 and more general rounding is permitted we might also have u′ = u ± 1, v′′ = ∓1. Case  1c , w = u − 1, v ≤ − 1 2, ev = 0. Then u′ = u or u − 1, v′ = −1, u′′ = u, v′′ = −1 or 0. Case 2, 2, ev = 0. Like  1a . Case  2b , w = u, v ≤ 1 u = bp−1. Case  2a , w = u + 1, v ≥ 1 2, 2, u′ < u. Then u′ = u − j b where u′ ≥ u. Like  1b . Case  2c , w = u, v ≤ 1 v = j b + v1 and v1 ≤ 1 2 b; we have v′ = 0, u′′ = u, v′′ = j b. Case  2d , w < u. Then w = u − j b where v = −j b + v1 and v1 ≤ 1 2 b−1 for some positive integer j ≤ b; we have  v′, u′′  =  −j b, u , and  u′, v′′  =  u,−j b  2 b−1. In all cases u⊖ u′ = u− u′, or  u−1 b,  1− j  b , the latter case only when v1 = 1 v ⊖ v′ = v − v′, u ⊖ u′′ = u − u′′, v ⊖ v′′ = v − v′′, round w − u − v  = w − u − v. 13. Since round x  = 0 if and only if x = 0, we want to find a large set of integer pairs  m, n  with the property that m ⊘ n is an integer if and only if m n is. Assume that m,n < bp. If m n is an integer, then m ⊘ n = m n is also. Conversely if m n is not an integer, but m ⊘ n is, we have 1 n ≤ m ⊘ n − m n < 1 2m nb1−p, hence m > 2bp−1. Our answer is therefore to require m ≤ 2bp−1 and 0 < n < bp.  Slightly weaker hypotheses are also possible.  14.  u ⊗ v  ⊗ w − uvw ≤  u ⊗ v  ⊗ w −  u ⊗ v w + wu ⊗ v − uv ≤ δ u⊗v ⊗w + bew−q−lw δu⊗v ≤  1 + b δ u⊗v ⊗w. Now e u⊗v ⊗w − eu⊗ v⊗w  ≤ 2, so we may take ϵ = 1 15. u ≤ v implies that  u ⊕ u  ⊘ 2 ≤  u ⊕ v  ⊘ 2 ≤  v ⊕ v  ⊘ 2, so the condition holds for all u and v if and only if it holds whenever u = v. For base b = 2, the condition is therefore always satisfied  barring overflow ; but for b > 2 there are numbers v ̸= w such that v ⊕ v = w ⊕ w, hence the condition fails. [On the other hand, the formula u ⊕   v ⊖ u  ⊘ 2  does give a midpoint in the correct range. Proof. It suffices to  2. Then u′ = u, v′ = 0, u′′ = u, v′′ = 0. If v = 1  2 b−1 for some positive integer j ≤ 1  2 1 + b  1 + b2 b−p.   4.2.2  ANSWERS TO EXERCISES  615  show that u +  v ⊖ u  ⊘ 2 ≤ v, i.e.,  v ⊖ u  ⊘ 2 ≤ v − u; and it is easy to verify that round  1  10 = 11.111111,  90009 = 100000.91,   91 = 101.11110,   901 = 900819 = 1000000.0; therefore  2round x   ≤ x for all x ≥ 0.]  16.  a  Exponent changes occur at  1001.1102,    9001 = 10001.020,   b  After calculating n  1000000 = 1109099.1.  k=1 1.2345679 = 1224782.1,  14  tries to take the square root of −.0053187053. But  15  and  16  are exact in this case. [If, however, xk = 1+⌊ k−1  2⌋10−7,  15  and  16  have errors of order n. See Chan and Lewis, CACM 22  1979 , 526–531, for further results on the accuracy of standard deviation calculations.]  c  We need to show that u ⊕   v ⊖ u  ⊘ k  lies between u and v; see exercise 15.  Set rX to zero with the sign of fv.  Floating point comparison subroutine: Ensure that overflow is off. v ← −v.  *+2  FV 0:0   17. FCMP STJ 9F OFLO JOV STA TEMP LDAN TEMP  Copy here lines 07–20 of Program 4.2.1A.  LDX DEC1 5 J1N ENT1 0 SRAX 5,1 FU ADD JOV 7F CMPA EPSILON 1:5  8F JG 6F JL 9F JXZ 1F JXP 9F JAP JMP 8F ENTX 1 1 SRC JMP 8F JAP 8F ENTA 0 CMPA =0= JMP  1H 6H 8H 9H  7H  *  by a smaller one.  Replace large difference in exponents rA ← difference of operands. Fraction overflow: not ∼. Jump if not ∼. Jump if ∼. Jump if ∼. If rA = ϵ, check sign of rA × rX. Jump if ∼.  rA ̸= 0   Make rA nonzero with same sign. Jump if not ∼.  rA ̸= 0   Set comparison indicator. Exit from subroutine.  19. Let γk = δk = ηk = σk = 0 for k > n. It suffices to find the coefficient of x1, since the coefficient of xk will be just the same except with all subscripts increased by k − 1. Let  fk, gk  denote the coefficient of x1 in  sk − ck, ck  respectively. Then f1 =  1+η1  1−γ1−γ1δ1−γ1σ1−δ1σ1−γ1δ1σ1 , g1 =  1+δ1  1+η1  γ1+σ1+γ1σ1 , and fk =  1−γkσk−δkσk−γkδkσk fk−1+ γk−ηk+γkδk+γkηk+γkδkηk+γkηkσk+δkηkσk+ γkδkηkσk gk−1, gk = σk 1+ γk  1+ δk fk−1 − 1+ δk  γk + γkηk + ηkσk + γkηkσk gk−1, for 1 < k ≤ n. Thus fn = 1 + η1 − γ1 +  4n terms of 2nd order  +  higher order terms  = 1 + η1 − γ1 + O nϵ2  is sufficiently small. [The Kahan summation formula Kahan observed that sn⊖ cn =n was first published in CACM 8  1965 , 40; see also Proc. IFIP Congress  1971 , 2, 1232, and further developments by K. Ozawa, J. Information Proc. 6  1983 , 226–230. k=1 1+ ϕk xk where ϕk ≤ 2ϵ+ O  n+1− k ϵ2 . For another approach to accurate summation, see R. J. Hanson, CACM 18  1975 , 57–58.   616  ANSWERS TO EXERCISES  4.2.2  When some x’s are negative and others are positive, we may be able to match them advantageously, as explained by T. O. Espelid, SIAM Review 37  1995 , 603–607. See also G. Bohlender, IEEE Trans. C-26  1977 , 621–632, for algorithms that compute round x1 + ··· + xn  and round x1 . . . xn  exactly, given {x1, . . . , xn}.] 20. By the proof of Theorem C,  47  fails for ew = p only if v + 1 2 ≥ w − u ≥ bp−1 + b−1; hence fu ≥ fv ≥ 1 −   1 2 b − 1 b−p. We now find that a necessary and sufficient condition for failure is that fw is essentially rounded to 2 during the normalization process  actually to 2 b after scaling right for fraction overflow  — a very rare case indeed! 21.  Solution by G. W. Veltkamp.  Let c = 2⌈p 2⌉ + 1; we may assume that p ≥ 2, so c is representable. First compute u′ = u ⊗ c, u1 =  u ⊖ u′  ⊕ u′, u2 = u ⊖ u1; similarly, v′ = v ⊗ c, v1 =  v ⊖ v′  ⊕ v′, v2 = v ⊖ v1. Then set w ← u ⊗ v, w′ ←    u1 ⊗ v1 ⊖ w  ⊕  u1 ⊗ v2   ⊕  u2 ⊗ v1   ⊕  u2 ⊗ v2 . It suffices to prove this when u, v > 0 and eu = ev = p, so that u and v are integers ∈ [2p−1 . . 2p . Then u = u1 + u2 where 2p−1 ≤ u1 ≤ 2p, u1 mod 2⌈p 2⌉ = 0, and u2 ≤ 2⌈p 2⌉−1; similarly v = v1 + v2. The operations during the calculation of w′ are exact, because w − u1v1 is a multiple of 2p−1 such that w − u1v1 ≤ w − uv + u2v1 + u1v2 + u2v2 ≤ 2p−1 + 2p+⌈p 2⌉ + 2p−1; and similarly w − u1v1 − u1v2 ≤ w − uv + u2v < 2p−1 + 2⌈p 2⌉−1+p, where w − u1v1 − u1v2 is a multiple of 2⌈p 2⌉. 22. We may assume that bp−1 ≤ u, v < bp. If uv ≤ b2p−1, then x1 = uv − r where 2 bp−1, hence x2 = round u − r v  = x0  since r v ≤ 1 r ≤ 1 2, and If uv > b2p−1, then x1 = uv − r where equality implies v = bp−1 hence r = 0 . 2 b and x2 ≤ bp. If x2 = bp, then x3 = x1 2 bp, hence x1 v = u − r v < bp + 1 r ≤ 1 2 v ≤ x1 implies that x1 is a multiple of bp, and we have  since the condition  bp − 1 x1 < bp v + 1 2; we have x3 = round x1 + qv  = x1. Finally if x2 < bp, x1 = b2p−1, and x3 < b2p−1, then x4 = x2 by the first case above. This situation arises, for example, when b = 10, p = 2, u = 19, v = 55, x1 = 1000, x2 = 18, x3 = 990.  23. If u ≥ 0 or u ≤ −1 we have u  cid:88 mod 1 = u mod 1, so the identity holds. −1 < u < 0, then u  cid:88 mod 1 = u ⊕ 1 = u + 1 + r where r ≤ 1  If 2 b−p; the identity holds if and only if round 1 + r  = 1, so it always holds if we round to even. With the text’s rounding rule the identity fails if and only if b is a multiple of 4 and −1 < u < 0 and u mod 2b−p = 3 24. Let u = [ul . . ur], v = [vl . . vr]. Then u⊕v = [ul▽+ vl . . ur△+ vr], where x△+ y = y△+ x, x △+ +0 = x for all x, x △+ −0 = x for all x ̸= +0, x △+ +∞ = +∞ for all x ̸= −∞, and x △+ −∞ needn’t be defined; x ▽+ y = −  −x  △+  −y  . If x ⊕ y would overflow in normal floating point arithmetic because x + y is too large, then x △+ y is +∞ and x ▽+ y is the largest representable number.  2  . If x2   b2p−1, then let x2 = x1 v + q where q ≤ 1  2 b−p  for example, p = 3, b = 8, u = − .0124 8 .  For subtraction, let u ⊖ v = u ⊕  −v , where −v = [−vr . .−vl]. Multiplication is somewhat more complicated. The correct procedure is to let u⊗ v = [min ul▽× vl, ul▽× vr, ur ▽× vl, ur ▽× vr  . . max ul△× vl, ul△× vr, ur △× vl, ur △× vr ], where x △× y = y △× x, x △×  −y  = − x ▽× y  =  −x  △× y; x △× +0 =  +0 for x > 0, −0 for x   +0, −∞ for x < −0 .  It is possible to determine the min and max simply by looking at the signs of ul, ur, vl, and vr, thereby computing only two of the eight products, except when ul < 0 < ur and vl < 0 < vr; in the latter case we compute four products, and the answer is [min ul ▽× vr, ur ▽× vl  . . max ul △× vl, ur △× vr ].   2 bp−1 bp−1 ≤ 1   4.2.3  ANSWERS TO EXERCISES  617  l  r  2 be′′  x + ϵ +  and v−1  √ x  .  −p − u′ − v′ + 1  ⊕v′ , and assume that −p ≤ ϵbe + ϵbe′ + be′′ −p,  Finally, u ⊘ v is undefined if vl < 0 < vr; otherwise we use the formulas for , where x △× y−1 =  multiplication with vl and vr replaced respectively by v−1 x △  y, x ▽× y−1 = x ▽  y,  ±0 −1 = ±∞,  ±∞ −1 = ±0. [See E. R. Hansen, Math. Comp. 22  1968 , 374–384. An alternative scheme, in which division by 0 gives no error messages and intervals may be neighborhoods of ∞, has been proposed by W. M. Kahan. In Kahan’s scheme, for example, the reciprocal of [−1 . . +1] is [+1 . .−1], and an attempt to multiply an interval containing 0 by an interval containing ∞ yields [−∞ . . +∞], the set of all numbers. See Numerical Analysis, Univ. Michigan Engineering Summer Conf. Notes No. 6818  1968 .] 25. Cancellation reveals previous errors in the computation of u and v. For example, if ϵ is small, we often get poor accuracy when computing f x + ϵ  ⊖ f x , because the rounded calculation of f x + ϵ  destroys much of the information about ϵ. It is desirable to rewrite such formulas as ϵ ⊗ g x, ϵ , where g x, ϵ  =  f x + ϵ  − f x   ϵ is √ first computed symbolically. Thus, if f x  = x2 then g x, ϵ  = 2x + ϵ; if f x  = √ x then g x, ϵ  = 1   26. Let e = max eu, eu′ , e′ = max ev, ev′ , e′′ = max eu⊕v, eu′ 2 be′′ q = 0. Then  u⊕ v − u′ ⊕ v′  ≤ u+ v + 1 and e′′ ≥ max e, e′ . Hence u ⊕ v ∼ u′ ⊕ v′  2ϵ + b−p . If b = 2 this estimate can be improved to 1.5ϵ + b−p. For ϵ + b−p is an upper bound if u − u′ and v − v′ have opposite signs, and in the other case we cannot have e = e′ = e′′. 27. The stated identity is a consequence of the fact that 1 ⊘  1 ⊘ u  = u whenever b−1 ≤ fu ≤ b−1 2. If the latter were false, there would be integers x and y such that bp−1 < x < bp−1 2 and either y− 1 2  < b2p−1 x ≤ y + 1 2  > b2p−1, yet the latter condition implies y = ⌊bp−1 2⌋ = x. 28. See Math. Comp. 32  1978 , 227–232. 29. When b = 2 and p = 1 and x > 0, we have round x  = 2e x  where e x  = ⌊lg 4 Let f x  = xα and let t n  = ⌊⌊αn+lg 4 we find ˆh 2e  = 2e−1 for 41 < e ≤ 58. 31. According to the theory in Section 4.5.3, the convergents to the continued frac- √ tion 3 = 1 +   1, 2, 1, 2, . . .    are pn qn = Kn+1 1, 1, 2, 1, 2, . . .   Kn 1, 2, 1, 2, . . .  . √ 3, hence 3q2 in fact, These convergents are excellent approximations to 31 − p2 n = 2 − 3 n mod 2 . The example given is 2p2 3q2 31 +  3q2 31  = 31 − 1 + p2 2p2 31  = 1. Floating point subtraction of p2 31 from 3q2 31 yields zero, 31 from 9q4 unless we can represent 3q2 31 generally gives rounding errors much larger than 2p2 31. Similar examples can be based on continued fraction approximations to any algebraic number. 32.  J. Ziegler Hunts, 2014.  a = 1 2 and b mod 1 = 1 4.  3 x⌋. 3⌋. Then ˆh 2e  = 2t e . When α = .99  2. But that is clearly impossible unless we have x x + 1  31 almost perfectly; subtracting p4  2 ≤ b2p−1 x < b2p−1  x− 1  2  ≤ y or y ≤ b2p−1  x+ 1  n ≈ p2 n; 31  3q2 31 + p2  n − p2 31 −  p2  3⌋ α+lg 4  SECTION 4.2.3 1. First,  wm, wl  =  .573, .248 ; then wmvl vm = .290; so the answer is  .572, .958 . This in fact is the correct result to six decimals. 2. The answer is not affected, since the normalization routine truncates to eight places and can never look at this particular byte position.  Scaling to the left occurs at most once during normalization, since the inputs are normalized.    618  ANSWERS TO EXERCISES  4.2.3  3. Overflow obviously cannot occur at line 09, since we are adding two-byte quantities, or at line 22, since we are adding four-byte quantities. In line 30 we are computing the sum of three four-byte quantities, so this cannot overflow. Finally, in line 32, overflow is impossible because the product fufv must be less than unity. 4. Insert ‘JOV OFLO; ENT1 0’ between lines 03 and 04. Also replace lines 21–22 by ‘ADD TEMP ABS ; JNOV *+2; INC1 1’, and change lines 28–31 to ‘SLAX 5; ADD TEMP; JNOV *+2; INC1 1; ENTX 0,1; SRC 5’. This adds five lines of code and only 1, 2, or 3 units of execution time. 5. Insert ‘JOV OFLO’ after line 06. Change lines 23, 31, 39 respectively to ‘SRAX 0,1’, ‘SLAX 5’, ‘ADD ACC’. Between lines 40 and 41, insert ‘DEC2 1; JNOV DNORM; INC2 1; INCX 1; SRC 1’.  It’s tempting to remove the ‘DEC2 1’ in favor of ‘STZ EXPO’, but then ‘INC2 1’ might overflow rI2!  This adds six lines of code; the running time decreases by 3u, unless there is fraction overflow, when it increases by 7u. 6. DOUBLE STJ  EXITDF  TEMP TEMP EXP   ENTX 0 STA LD2 INC2 QQ-Q STZ EXPO SLAX 1 JMP SINGLE STJ JOV STA LD2 DEC2 QQ-Q SLAX 2 JMP  NORM  DNORM EXITF OFLO TEMP TEMP EXPD  rI2 ← e.  Convert to double precision: Clear rX. rI2 ← e. Correct for difference in excess. EXPO ← 0. Remove exponent. Normalize and exit. Convert to single precision: Ensure that overflow is off.  Correct for difference in excess. Remove exponent. Normalize, round, and exit.  7. All three routines give zero as the answer if and only if the exact result would be zero, so we need not worry about zero denominators in the expressions for relative error. The worst case of the addition routine is pretty bad: Visualized in decimal notation, if the inputs are 1.0000000 and −.99999999, the answer is b−7 instead of b−8; thus the maximum relative error δ1 is b − 1, where b is the byte size.  For multiplication and division, we may assume that both operands are positive and have the same exponent QQ. The maximum error in multiplication is readily bounded by considering Fig. 4: When uv ≥ 1 b, we have 0 ≤ uv − u ⊗ v < 3b−9 +  b − 1 b−9, so the relative error is bounded by  b + 2 b−8. When 1 b2 ≤ uv < 1 b, we have 0 ≤ uv − u ⊗ v < 3b−9, so the relative error in this case is bounded by 3b−9 uv ≤ 3b−7. We take δ2 to be the larger of the two estimates, namely 3b−7. Division requires a more careful analysis of Program D. The quantity actually computed by the subroutine is α − δ − bϵ  α − δ′′  β − δ′  − δ′′′  − δn where α =  um + ϵul  bvm, β = vl bvm, and the nonnegative truncation errors  δ, δ′, δ′′, δ′′′  are respectively less than  b−10, b−5, b−5, b−6 ; finally δn  the truncation during normal- ization  is nonnegative and less than either b−9 or b−8, depending on whether scaling occurs or not. The actual value of the quotient is α  1 + bϵβ  = α − bϵαβ + b2αβ2δ′′′′, where δ′′′′ is the nonnegative error due to truncation of the infinite series  2 ; here δ′′′′ < ϵ2 = b−10, since it is an alternating series. The relative error is therefore the absolute value of  bϵδ′ + bϵδ′′β α + bϵδ′′′ α −  δ α + bϵδ′δ′′ α + b2β2δ′′′′ + δn α , times   4.2.4  ANSWERS TO EXERCISES  619   1 + bϵβ . The positive terms in this expression are bounded by b−9 + b−8 + b−8, and the negative terms are bounded by b−8 + b−12 + b−8 plus the contribution by the normalizing phase, which can be about b−7 in magnitude. It is therefore clear that the potentially greatest part of the relative error comes during the normalization phase, and that δ3 =  b + 2 b−8 is a safe upper bound for the relative error. 8. Addition: If eu ≤ ev + 1, the entire relative error occurs during the normalization phase, so it is bounded above by b−7. If eu ≥ ev +2, and if the signs are the same, again the entire error may be ascribed to normalization; if the signs are opposite, the error due to shifting digits out of the register is in the opposite direction from the subsequent error introduced during normalization. Both of these errors are bounded by b−7, hence δ1 = b−7.  This is substantially better than the result in exercise 7.   Multiplication: An analysis as in exercise 7 gives δ2 =  b + 2 b−8.  2 91%   ≈ 15%.  SECTION 4.2.4 1. Since fraction overflow can occur only when the operands have the same sign, this is the probability that fraction overflow occurs divided by the probability that the operands have the same sign, namely, 7%   1 3. log10 2.4 − log10 2.3 ≈ 1.84834%. 4. The pages would be uniformly gray. 5. The probability that 10fU ≤ r is  r − 1  10 +  r − 1  100 + ··· =  r − 1  9. So in this case the leading digits are uniformly distributed; for example, the leading digit is 1 with probability 1 9. 6. The probability that there are three leading zero bits is log16 2 = 1 4; the probability that there are two leading zero bits is log16 4 − log16 2 = 1 4; and similarly for the other two cases. The “average” number of leading zero bits is 1 1 2, so the “average” number of 2. The worst case, p − 1 bits, occurs however with rather high “significant bits” is p + 1 probability. In practice, it is usually necessary to base error estimates on the worst case, since a chain of calculations is only as strong as its weakest link. In the error analysis of Section 4.2.2, the upper bound on relative rounding error for floating hex is 21−p. In the binary case we can have p + 1 significant bits in all normalized numbers  see exercise 4.2.1–3 , with relative rounding errors bounded by 2−1−p. Extensive computational experience confirms that floating binary produces significantly more accurate results than the equivalent floating hex, even when the binary numbers have a precision of p bits instead of p + 1.  Tables 1 and 2 show that hexadecimal arithmetic can be done a little faster, since fewer cycles are needed when scaling to the right or normalizing to the left. But this fact is insignificant compared to the substantial advantages of b = 2 over other radices  see also Theorem 4.2.2C and exercises 4.2.2–13, 15, 21 , especially since floating binary can be made as fast as floating hex with only a tiny increase in total processor cost. m F  10km·5k − F  10km   = log 5k  log 10k and also  7. For example, suppose that that  m F  10km · 4k  − F  10km   = log 4k  log 10k; then    m   F  10km · 5k  − F  10km · 4k   = log10  5 4  for all k. But now let ϵ be a small positive number, and choose δ > 0 so that F  x  < ϵ for 0   0 so that F  x  > 1 − ϵ for x > M. We can take k so   620  ANSWERS TO EXERCISES  4.2.4  large that 10−k · 5k   M; hence by the monotonicity of F,    m   F  10km·5k − F  10km·4k     F  10km·5k − F  10k m−1 ·5k  +  ≤   m<0  = F  10−k ·5k +1− F  4k  < 2ϵ.  m≥0   F  10k m+1 ·4k − F  10km·4k    10. When 1 < r < 10 the generating function C z  has simple poles at the points 1 + wn, where wn = 2πni ln 10, hence  8. When s > r, P0 10ns  is 1 for small n, and 0 when ⌊10ns⌋ > ⌊10nr⌋. The least n for which this happens may be arbitrarily large, so no uniform bound can be given for N0 ϵ  independent of s.  In general, calculus textbooks prove that such a uniform bound would imply that the limit function S0 s  would be continuous, and it isn’t.    + ··· for all n. It follows  0  1  0  1   + q2n−1  + ··· for all m and n.  9. Let q1, q2, . . . be such that P0 n  = q1n−1 that Pm n  = 1−mq1n−1   + 2−mq2n−1 +  ln 10  z − 1 − wn  + E z  ℜ e−wn ln r − 1  where E z  is analytic in the entire plane. Thus if θ = arctan 2π ln 10 , cm = log10 r − 1 − 2 + em ln 10 = log10 r − 1 + sin mθ + 2π log10 r  − sin mθ   C z  = log10 r − 1   e−wn ln r − 1  wn 1 + wn m  1 + wn  1 − z    + O  n̸=0  n>0  wn  1  π 1 + 4π2  ln 10 2 m 2   1 + 16π2  ln 10 2 m 2    .  11. When  logb U  mod 1 is uniformly distributed in [0 . . 1 , so is  logb 1 U  mod 1 =  1 − logb U  mod 1. 12. We have  h z  =  f x  dx g z bx  bx +  f x  dx g z x  x;  consequently  h z  − l z    z  g z bx  − l z bx   g z x  − l z x   .  l z x   =  l z   Since f x  ≥ 0,  h z  − l z   l z  ≤  z  l z bx   1 b  f x  dx  +  f x  dx  1 b f x  dx A g  + 1  z  z f x  dx A g  for all z, [Bell System Tech. J. 49  1970 ,  hence A h  ≤ A g . By symmetry, A h  ≤ A f . 1609–1625.] 13. Let X =  logb U  mod 1 and Y =  logb V   mod 1, so that X and Y are inde- pendently and uniformly distributed in [0 . . 1 . No left shift is needed if and only if X + Y ≥ 1, and that occurs with probability 1 2.   Similarly, the probability is 1 2 that floating point division by Algorithm 4.2.1M needs no normalization shifts; this analysis needs only the weaker assumption that both of the operands independently have the same distribution.    1  z   1   z  1 b   4.2.4  ANSWERS TO EXERCISES  621  14. For convenience, the calculations are shown here If k = 0, the probability of a carry is for b = 10.  2   1  10  ln 10  dx x  dy y  .  1≤x,y≤10 x+y≥10  ,  0  0  and  dy y  dy y  dx x  dx x  − 2  10−y   10   10    See Fig. A–7.  The value of the integral is   1  1  t  t Fig. A–7. 10 + t2 400 + t3 10 + y when k = 0 is  1 ln 10 2 π2 6 − 2 k = 0, fraction overflow always occurs, so this derivation proves that  9000 + ··· .  The latter integral is essentially a “dilogarithm.”  Hence the probability of a carry [Note: When b = 2 and n≥1 1 n22n =  3000 + ···  1 − y 10  dy = t    10−y  dy y  ln  =  1  0  0  π2 12 −  ln 2 2 2.]  200 + y2 n≥1 1 n210n  ≈ .27154.  1  2  When k > 0, the probability is   1  2 101−k  ln 10  10−k  dy y   10  10−y  dx x  =  ln 10  n210nk −  1  n≥1  n≥1  1  n210n k+1     .  Thus when b = 10, fraction overflow should occur with approximate probability .272p0+ .017p1 + .002p2 +··· . When b = 2 the corresponding figures are p0 + .655p1 + .288p2 + .137p3 + .067p4 + .033p5 + .016p6 + .008p7 + .004p8 + .002p9 + .001p10 + ··· .  Now if we use the probabilities from Table 1, dividing by .91 to eliminate zero operands and assuming that the probabilities are independent of the operand signs, we predict a probability of about 14 percent when b = 10, instead of the 15 percent in exercise 1. For b = 2, we predict about 48 percent, while the table yields 44 percent. These results are certainly in agreement within the limits of experimental error. 15. When k = 0, the leading digit is 1 if and only if there is a carry.  It is possible for fraction overflow and subsequent rounding to yield a leading digit of 2, when b ≥ 4, but we are ignoring rounding in this exercise.  The probability of fraction overflow is approximately .272, as shown in the previous exercise, and .272 < log10 2.  When k > 0, the leading digit is 1 with probability     1  dx x  <  ln 10  2 101−k  dy y  10−k      dx x  1≤x≤2  = log10 2.   1  ln 10  2 101−k  dy y  10−k    1≤x<2−y  or 10−y≤x<10  16. To prove the hint [which is due to Landau, Prace Matematyczno-Fizyczne 21  1910 , 103–113], assume first that lim sup an = λ > 0. Let ϵ = λ  λ+4M  and choose 10 ϵλn for all n > N. Let n > N  1 − ϵ , n > 5 ϵ be such N so that a1 + ··· + an < 1 4 λ for 0 ≤ k < ϵn, that an > 1  2 λ. Then, by induction, an−k ≥ an − kM  n − ϵn  > 1  and  n−ϵn<k≤n ak ≥ 1  4 λ ϵn − 1  > 1   =  5 λϵn. But   1≤k≤n ak −   ≤ 1  n−ϵn<k≤n ak  1≤k≤n−ϵn ak  5 ϵλn  since n − ϵn > N. A similar contradiction applies if lim inf an < 0.    0 1  10  0  0  1 0  10   622  ANSWERS TO EXERCISES  4.2.4  a′ ≤ 2m10na < 2m10nb ≤ 2m′10n′  Assuming that Pm+1 n  → λ as n → ∞, let ak = Pm k  − λ. If m > 0, the ak satisfy the hypotheses of the hint  see Eq. 4.2.2– 15  , since 0 ≤ Pm k  ≤ 1; hence Pm n  → λ. 17. See J. Math. Soc. Japan 4  1952 , 313–322.  The fact that harmonic prob- [Atti della ability extends ordinary probability follows from a theorem of Cesàro, Reale Accademia dei Lincei, Rendiconti  4  4  1888 , 452–457]. Persi Diaconis [Ph.D. thesis, Harvard University, 1974] has shown among other things that the definition of probability by repeated averaging is weaker than harmonic probability, in the following precise sense: If limm→∞ lim inf n→∞ Pm n  = limm→∞ lim supn→∞ Pm n  = λ then the harmonic probability is λ. On the other hand the statement “10k2 ≤ n < 10k2+k for some integer k > 0” has harmonic probability 1 2, while repeated averaging never 18. Let p a  = P  La  and p a, b  = settles down to give it any particular probability.  a≤k<b p k  for 1 ≤ a < b. Since La = L10a ∪ L10a+1 ∪ ··· ∪ L10a+9 for all a, we have p a  = p 10a, 10 a + 1   by  i . Furthermore since P  S  = P  2S  + P  2S + 1  by  i ,  ii ,  iii , we have p a  = p 2a, 2 a + 1  . It follows that p a, b  = p 2m10na, 2m10nb  for all m, n ≥ 0. If 1 < b a < b′ a′, then p a, b  ≤ p a′, b′ . The reason is that there exist integers m, n, m′, n′ such that 2m′10n′ b′ as a consequence of the fact that log 2 log 10 is irrational, hence we can apply  v .  See exercise 3.5–22 with k = 1 and Un = n log 2 log 10.  In particular, p a  ≥ p a + 1 , and it follows that p a, b  p a, b + 1  ≥  b − a   b + 1 − a .  See Eq. 4.2.2– 15 .  Now we can prove that p a, b  = p a′, b′  whenever b a = b′ a′; for p a, b  = p 10na, 10nb  ≤ cnp 10na, 10nb − 1  ≤ cnp a′, b′ , for arbitrarily large values of n, where cn = 10n b − a   10n b − a  − 1  = 1 + O 10−n . For any positive integer n we have p an, bn  = p an, ban−1  + p ban−1, b2an−2  + ··· + p bn−1a, bn  = np a, b . If 10m ≤ an ≤ 10m+1 and 10m′ ≤ bn ≤ 10m′+1, then p 10m+1, 10m′  ≤ p an, bn  ≤ p 10m, 10m′+1  by  v . But p 1, 10  = 1 by  iv , hence p 10m, 10m′  = m′ − m for all m′ ≥ m. We conclude that ⌊log10 bn⌋ − ⌊log10 an⌋ − 1 ≤ np a, b  ≤ ⌊log10 bn⌋ + ⌊log10 an⌋ + 1 for all n, and p a, b  = log10 b a . [This exercise was inspired by D. I. A. Cohen, who proved a slightly weaker result in J. Combinatorial Theory A20  1976 , 367–370.] 19. Equivalently, ⟨ log10 Fn  mod 1⟩ is equidistributed in the sense of Definition 3.5B. Since log10 Fn = n log10 ϕ − log10 5 + O ϕ−2n  by 1.2.8– 14 , this is equivalent to equidistribution of ⟨n log10 ϕ⟩, which follows from ex. 3.5–22. [Fibonacci Quarterly 5  1967 , 137–140.] The same proof shows that the sequences ⟨bn⟩ obey the logarithmic law for all integers b > 1 that aren’t powers of 10 [Yaglom and Yaglom, Challeng- ing Problems with Elementary Solutions  Moscow: 1954; English translation, 1964 , Problem 91b]. Notes: Many other sequences of integers have this property. For example, Persi Diaconis [Annals of Probability 5  1977 , 72–81] showed that ⟨n!⟩ is one such sequence, and that binomial coefficients obey the logarithmic law too, in the sense that  √  n  k=0  1  n + 1  lim n→∞  [10f n  k  < r] = log10 r .  P. Schatte [Math. Nachrichten 148  1990 , 137–144] proved that the denominators of continued fraction approximations have logarithmic fraction parts, whenever the partial quotients have a repeating pattern with polynomial variation as in exercise   4.3.1  ANSWERS TO EXERCISES  623  4.5.3–16. One interesting open question is whether the sequence ⟨2!,  2! !,   2! ! !, . . .⟩ has logarithmic fraction parts; see J. H. Conway and M. J. T. Guy, Eureka 25  1962 , 18–19.  SECTION 4.3.1 2. If the ith number to be added is ui =  ui n−1  . . . ui1ui0 b, use Algorithm A with step A2 changed to the following:  A2′. [Add digits.] Set  wj ←  u1j + ··· + umj + k  mod b,  and k ← ⌊ u1j + ··· + umj + k  b⌋.   The maximum value of k is m − 1, so step A3 would have to be altered if m > b.  3.  OFLO  ENN1 N JOV ENTX 0 2H SLAX 5 ENT3 M*N,1 N  LOC uij  ≡ U + n i − 1  + j   Ensure that overflow is off. k ← 0.  1 1 1 N  rX ≡ next value of k  MN rA ← rA + uij. MN K Carry one. MN Repeat for m ≥ i ≥ 1. MN  rI3 ≡ n i − 1  + j   3H ADD  W+N,1 N wj ← rA.  U,3 JNOV *+2 INCX 1 DEC3 N J3NN 3B STA INC1 1 2B J1N STX W+N  N N Repeat for 0 ≤ j < n. 1 Store final carry in wn.  Running time, assuming that K = 1 4. We may make the following assertion before A1: “n ≥ 1; and 0 ≤ ui, vi < b for 0 ≤ i < n.” Before A2, we assert: “0 ≤ j < n; 0 ≤ ui, vi < b for 0 ≤ i < n; 0 ≤ wi < b for 0 ≤ i < j; 0 ≤ k ≤ 1; and  uj−1 . . . u0 b +  vj−1 . . . v0 b =  kwj−1 . . . w0 b.” The latter statement means more precisely that  2 MN, is 5.5MN + 7N + 4 cycles.    ulbl +   vlbl = kbj +   0≤l<j  0≤l<j  wlbl.  0≤l<j  Before A3, we assert: “0 ≤ j < n; 0 ≤ ui, vi < b for 0 ≤ i < n; 0 ≤ wi < b for 0 ≤ i ≤ j; 0 ≤ k ≤ 1; and  uj . . . u0 b +  vj . . . v0 b =  kwj . . . w0 b.” After A3, we assert that 0 ≤ wi < b for 0 ≤ i < n; 0 ≤ wn ≤ 1; and  un−1 . . . u0 b+ vn−1 . . . v0 b =  wn . . . w0 b. It is a simple matter to complete the proof by verifying the necessary implications between the assertions and by showing that the algorithm always terminates. 5. B1. Set j ← n − 1, wn ← 0.  B2. Set t ← uj + vj, wj ← t mod b, i ← j. B3. If t ≥ b, set i ← i + 1, t ← wi + 1, wi ← t mod b, and repeat this step until  t < b.  B4. Decrease j by one, and if j ≥ 0 go back to B2.  6. C1. Set j ← n − 1, i ← n, r ← 0.   624  ANSWERS TO EXERCISES  4.3.1  C2. Set t ← uj + vj. If t ≥ b, set wi ← r + 1 and wk ← 0 for i > k > j; then set i ← j and r ← t mod b. Otherwise if t < b − 1, set wi ← r and wk ← b − 1 for i > k > j; then set i ← j and r ← t.  If j ≥ 0, go back to C2; otherwise set wi ← r, and  C3. Decrease j by one.  wk ← b − 1 for i > k ≥ 0.  7. When j = n − 3, for example, we have k = 0 with probability  b + 1  2b; k = 1 with probability   b − 1  2b  1 − 1 b , namely the probability that a carry occurs and that the preceding digit wasn’t b− 1; k = 2 with probability   b− 1  2b  1 b  1− 1 b ; and k = 3 with probability   b − 1  2b  1 b  1 b  1 . For fixed k we may add the probabilities as j varies from n − 1 to 0; this gives the mean number of times the carry propagates back k places,      mk = b − 1 2bk   n + 1 − k   1 − 1  b  .     + 1   1 −1  b  b  n  ,   n − 1 b − 1  m1 + 2m2 + ··· + nmn = 1 2  As a check, we find that the average number of carries is  in agreement with  6 . 8. 1 ENT1 N-1 1 OFLO JOV 1 STZ W+N U,1 N 2H LDA V,1 N ADD W,1 N STA JNOV 4F N ENT2 1,1 L  3H LDA  INCA 1 STA INC2 1 JOV  W,2 K K W,2 K K 3B K N J1NN 2B N  4H DEC1 1  The running time depends on L, the number of positions in which uj + vj ≥ b; and on K, the total number of carries. It is not difficult to see that K is the same quantity that appears in Program A. The analysis in the text shows that L has the average value N  b − 1  2b , and K has the average value 1 2 N − b−1 − b−2 − ··· − b−n . So if we ignore terms of order 1 b, the running time is 9N + L + 7K + 3 ≈ 13N + 3 cycles. 9. Replace “b” by “bj” everywhere in step A2. 10. If lines 06 and 07 were interchanged, we would almost always have overflow, but register A might have a negative value at line 08, so this would not work. If the instructions on lines 05 and 06 were interchanged, the sequence of overflows occurring in the program would be slightly different in some cases, but the program would still be right. 11. This is equivalent to lexicographic comparison of strings:  i  Set j ← n − 1;  ii  if uj < vj, terminate [u < v]; if uj = vj and j = 0, terminate [u = v]; if uj = vj and j > 0, set j ← j − 1 and repeat  ii ; if uj > vj, terminate [u > v]. This algorithm tends to be quite fast, since there is usually low probability that j will have to decrease very much before we encounter a case with uj ̸= vj. 12. Use Algorithm S with uj = 0 and vj = wj. Another borrow will occur at the end of the algorithm; this time it should be ignored.   4.3.1  13.  ANSWERS TO EXERCISES  625  ENN1 N JOV ENTX 0  OFLO  1 1 1 CARRY N U+N,1 N  2H STX LDA  MUL SLC ADD JNOV *+2 INCX 1  V N 5 N CARRY N N K  STA INC1 1 2B J1N STX W+N  W+N,1 N N N 1  The running time is 23N + K + 5 cycles, and K is roughly 1 14. The key inductive assertion is the one that should be valid at the beginning of step M4; all others are readily filled in from this one, which is as follows: 0 ≤ i < m; 0 ≤ j < n; 0 ≤ ul < b for 0 ≤ l < m; 0 ≤ vl < b for 0 ≤ l < n; 0 ≤ wl < b for 0 ≤ l < j + m; 0 ≤ k < b; and, in the notation of the answer to exercise 4,  2 N.   wj+m−1 . . . w0 b + kbi+j = u ×  vj−1 . . . v0 b +  ui−1 . . . u0 b × vjbj.  15. The error is nonnegative and less than  n − 2 b−n−1. [Similarly, if we ignore the products with i + j > n + 3, the error is bounded by  n − 3 b−n−2, etc.; but, in some cases, we must compute all of the products if we want to get the true rounded result. Further analysis shows that correctly rounded results of multiprecision floating point fractions can almost always be obtained by doing only about half the work needed to compute the full double-length product; moreover, a simple test will identify the rare cases for which full precision is needed. See W. Krandick and J. R. Johnson, Proc. IEEE Symp. Computer Arithmetic 11  1993 , 228–233.] 16. Q1. Set r ← 0, j ← n − 1.  Q2. Set wj ← ⌊ rb + uj  v⌋, r ←  rb + uj  mod v. Q3. Decrease j by 1, and return to Q2 if j ≥ 0.  17. u v > unbn  vn−1 + 1 bn−1 = b 1 − 1  vn−1 + 1   > b 1 − 1  b 2   = b − 2. 18.  unb + un−1   vn−1 + 1  ≤ u  vn−1 + 1 bn−1 < u v. 19. u− ˆqv ≤ u− ˆqvn−1bn−1 − ˆqvn−2bn−2 = un−2bn−2 +···+ u0 + ˆrbn−1 − ˆqvn−2bn−2 < bn−2 un−2 + 1 + ˆrb − ˆqvn−2  ≤ 0. Since u − ˆqv < 0, q < ˆq. 20. If q ≤ ˆq − 2, then u <  ˆq − 1 v < ˆq vn−1bn−1 +  vn−2 + 1 bn−2 − v < ˆqvn−1bn−1 + ˆqvn−2bn−2 + bn−1 − v ≤ ˆqvn−1bn−1 +  bˆr + un−2 bn−2 + bn−1 − v = unbn + un−1bn−1 + un−2bn−2 + bn−1 − v ≤ unbn + un−1bn−1 + un−2bn−2 ≤ u. In other words, u < u, and this is a contradiction. 21.  Solution by G. K. Goyal.  The inequality ˆqvn−2 ≤ bˆr + un−2 implies that we have ˆq ≤  unb2 + un−1b + un−2   vn−1b + vn−2  ≤ u   vn−1b + vn−2 bn−2 . Now u mod v = u − qv = v 1 − α  where 0 < α = 1 + q − u v ≤ ˆq − u v ≤ u 1   vn−1b + vn−2 bn−2  − 1 v  = u vn−3bn−3 + ···     vn−1b + vn−2 bn−2v  < 2 b−1 . u  vn−1bv  ≤ ˆq  vn−1b  ≤  b−1   vn−1b , and this is at most 2 b since vn−1 ≥ 1 5 ⌋ = 8, but 8·8 > 10 41−40 +0. Then 22. Let u = 4100, v = 588. We first try ˆq = ⌊ 41 we set ˆq = 7, and now we find 7 · 8 < 10 41 − 35  + 0. But 7 times 588 equals 4116, so the true quotient is q = 6.  Incidentally, this example shows that Theorem B cannot be improved under the given hypotheses, when b = 10. Similarly, when b = 216 we can let u =  7fff800100000000 16, v =  800080020005 16.  23. Obviously v⌊b  v + 1 ⌋ <  v + 1 ⌊b  v + 1 ⌋ ≤ b; and the lower bound certainly holds if v ≥ b 2. Otherwise v⌊b  v + 1 ⌋ ≥ v b − v   v + 1  ≥  b − 1  2 > ⌊b 2⌋ − 1. 2.  For example, if b = 232, the 24. The approximate probability is only logb 2, not 1 probability that vn−1 ≥ 231 is approximately 1 32; this is still high enough to warrant the special test for d = 1 in steps D1 and D8.    626  ANSWERS TO EXERCISES  4.3.1  Jump if vn−1 = b − 1. Otherwise compute ⌊b  vn−1 + 1 ⌋. Jump if vn−1 = 0.  Jump if d ̸= 1. Set um+n ← 0.  Multiply v by d.  1 1 1 1 1 1 1 1 1 1 1  1 − A 1 − A  A A AN AN AN  AN A  V+N-1 TEMP  TEMP DIVBYZERO D DECA 1 JANZ *+3 U+M+N STZ JMP D2 ENN1 N ENTX 0  CARRY V+N,1 D  25. 002 ENTA 1 003 ADD 004 STA 005 ENTA 1 006 JOV 1F 007 ENTX 0 008 DIV 009 JOV 010 1H STA 011 012 013 014 015 016 017 2H STX 018 LDA 019 MUL ··· 026 027 028 2H STX 029 LDA ··· 037 038  J1N STX  J1N 2B ENN1 M+N   as in exercise 13    Now rX = 0.   CARRY U+M+N,1  A M + N  Multiply u by d. A M + N    as in exercise 13   2B U+M+N  A M + N   A 26.  See the algorithm of exercise 16.   D DECA 1 JAZ DONE ENT1 N-1 ENTA 0  101 D8 LDA 102 103 104 105 106 1H LDX 107 DIV 108 STA 109 SLAX 5 110 DEC1 1 111 J1NN 1B  U,1 D U,1   Remainder will be left in  locations U through U+N-1   1 1 1 Terminate if d = 1. A rI1 ≡ j; j ← n − 1. A r ← 0. AN rAX ← rb + uj. AN AN AN  uj, r  ←  ⌊rAX d⌋, rAX mod d . AN j ← j − 1. AN Repeat for n > j ≥ 0.  At this point, the division routine is complete; and by the next exercise, rAX = 0. 27. It is du mod dv = d u mod v . 28. For convenience, let us assume that v has a decimal point at the left, i.e., v =  vn.vn−1vn−2 . . .  b. After step N1 we have 1 ≤ v b + 1  vn−1 + 1 = ≥ v b + 1 − vn−1   2 ≤ v < 1 + 1 b: For  1 b  vn−1 + 1  < 1 + 1 vn−1 b + 1 − vn−1    b + 1  b + 1  vn−1 + 1  v 1 + 1 b      ≥ 1  and  v  v  b  ,  .  vn−1 + 1  vn−1 + 1  b  vn−1 + 1   4.3.1  ANSWERS TO EXERCISES  627  t  t  b  b  b2  b2       v  b2 =  = 1 + 1  vn−1 + 1  vn−1 + 1  vn−1 + 1  The latter quantity takes its smallest value when vn−1 = 1, since it is a concave function and the other extreme value is greater. , so we see as above that v will never become ≥ 1 + 1 b.  The formula in step N2 may be written v ← b b + 1   b b + 1 − vn−1  b The minimum value of v after one iteration of step N2 is ≥  ≥ b b + 1 − vn−1   vn−1   v  b b + 1  + 1− t  t− 1  + 2 b2 − 1  , if t = vn−1 + 1. The minimum of this quantity occurs for t = b 2 + 1; a lower bound is 1 − 3 2b. Hence vn−1 ≥ b − 2, after one iteration of step N2. Finally, we have  1 − 3 2b  1 + 1 b 2 > 1, when b ≥ 5, so at most two more iterations are needed. The assertion is easily verified when b < 5. 29. True, since  uj+n . . . uj b < v. 30. In Algorithms A and S, such overlap is possible if the algorithms are rewritten slightly; for example, in Algorithm A we could rewrite step A2 thus: “Set t ← uj+vj+k, wj ← t mod b, k ← ⌊t b⌋.”  t + b b + 1  + 1  In Algorithm M, vj may be in the same location as wj+n.  In Algorithm D, it is most convenient  as in Program D, exercise 26  to let rn−1 . . . r0 be the same as un−1 . . . u0; and we can also let qm . . . q0 be the same as um+n . . . un, provided that no alteration of uj+n is made in step D6.  Line 098 of Program D can safely be changed to ‘J1N 2B’, since uj+n isn’t used in the subsequent calculation.  31. Consider the situation of Fig. 6 with u =  uj+n . . . uj+1uj 3 as in Algorithm D. If the leading nonzero digits of u and v have the same sign, set r ← u − v, q ← 1; otherwise set r ← u + v, q ← −1. Now if r > u, or if r = u and the first nonzero digit of uj−1 . . . u0 has the same sign as the first nonzero digit of r, set q ← 0; otherwise set uj+n . . . uj equal to the digits of r. 32. See M. Nadler, CACM 4  1961 , 192–193; Z. Pawlak and A. Wakulicz, Bull. de l’Acad. Polonaise des Sciences, Classe III, 5  1957 , 233–236  see also pages 803–804 ; and exercise 4.1–15. 34. See, for example, R. E. Maeder, The Mathematica Journal 6, 2  Spring 1996 , 32–40; 6, 3  Summer 1996 , 37–43. 36. Given ϕ with an accuracy of ±2−2n, we can successively compute ϕ−1, ϕ−2, . . . by subtraction until ϕ−k < 2−n; the accumulated error will not exceed 21−n. Then 5 ϕ−15 + ···  . we can use the series ln ϕ = ln  1 + ϕ−3   1 − ϕ−3   = 2 ϕ−3 + 1 [See William Schooling’s article in Napier Tercentenary Memorial, edited by C. G. Knott  London: Longmans, 1915 , 337–344.] An even better procedure, suggested in 1965 by J. W. Wrench, Jr., is to evaluate  3 ϕ−9 + 1  ln ϕ = 1  2 ln  1 + 5−1 2   1 − 5−1 2   =  2ϕ − 1  5−1 + 1  35−2 + 1  55−3 + ···  .  37. Let d = 2e so that b > dvn−1 ≥ b 2. Instead of normalizing u and v in step D1, simply compute the two leading digits v′v′′ of 2e vn−1vn−2vn−3 b by shifting left e bits. In step D3, use  v′, v′′  instead of  vn−1, vn−2  and  u′, u′′, u′′′  instead of  uj+n, uj+n−1, uj+n−2 , where the digits u′u′′u′′′ are obtained from  uj+n . . . uj+n−3 b by shifting left e bits. Omit division by d in step D8.  In essence, u and v are being “virtually” shifted. This method saves computation when m is small compared to n.    628  ANSWERS TO EXERCISES  4.3.1  38. Set k ← n, r ← 0, s ← 1, t ← 0, w ← u; we will preserve the invariant relation uv = 22k r + s2 − s  + 22k−nt + 22k−2nvw with 0 ≤ t, w < 2n, and with 0 < r ≤ 2s unless  r, s  =  0, 1 . While k > 0, let 4w = 2nw′ + w′′ and 4t + w′v = 2nt′ + t′′, where 0 ≤ w′′, t′′ < 2n and 0 ≤ t′ ≤ 6; then set t ← t′′, w ← w′′, s ← 2s, r ← 4r + t′ − s, k ← k − 1. If r ≤ 0, set s ← s − 1 and r ← r + 2s; otherwise, if r > 2s, set r ← r − 2s and s ← s + 1  this correction might need to be done twice . Repeat until k = 0. Then uv = r + s2 − s, since w is always a multiple of 22n−2k. Consequently r = 0 if and only if uv = 0; otherwise the answer is s, because uv − s ≤ s2 < uv + s.  39. Let Sj = 2n−1Sj is congruent  modulo 1  to  0≤k<n 4 anjk  8k + j +  k≥0 16−k  8k+j . We want to know whether or not 2n−1π mod 1 < 1 2. Since π = 4S1 −2S4 − S5 − S6, it suffices to have good estimates of 2n−1Sj mod 1. Now k≥n 4 2n−1−4k  8k + j , where anjk = 2n−1−4k mod  8k + j . Each term in the first sum can be approximated within 2−m by computing anjk in O log n  operations  Section 4.6.3  and then finding the scaled quotient ⌊2manjk  8k + j ⌋. The second sum can be approximated within 2−m by computing 2m times its first m 4 terms. If m ≈ 2 lg n, the range of uncertainty will be ≈ 1 n, and this will almost always be accurate enough. [Math. Comp. 66  1997 , 903–913.] 2 be an 8th root of unity, and consider the √ values lj = ln 1 − ζ j  2 − i arctan 1, √ l2 = l6 = 1 2  . 8 l0 + ζ−jl1 + ··· + ζ−7jl7  for 1 ≤ j ≤ 8 by 1.2.9– 13 . Therefore Also −Sj 2j 2 = 1 4S1 −2S4 − S5 − S6 = 2l0 − 2−2i 2l1 +2l4 + 2+2i l7 = π. Other identities of interest are:  √ 2  , l1 = l7 = 1 2 − i arctan 1 3 , l4 = ln 1 + 1   √ Notes: Let ζ = eπi 4 =  1 + i    2  . Then l0 = ln 1 − 1  2 ln 5  √ 2  , l3 = l5 = 1  2 − i arctan 1   2 ln 3  2 ln 1  √  4 S6 + 1  8 S8;  2 S4 + 1 2 S6;  ln 2 = S2 + 1 ln 3 = 2S2 + 1 ln 5 = 2S2 + 2S4 + 1 2 S3 + 1 2 S3 + 1 arctan 1 3  = S1 − S2 − 1  2 + 1  = S1 + 1 √ 2   = S1 − 1  √  2 ln   2 S6; 4 S5 + 1 8 S7; 4 S5 − 1 8 S7; 2 S4 − 1 4 S5;  √ 2 arctan 1   0 = 8S1 − 8S2 − 4S3 − 8S4 − 2S5 − 2S6 + S7.  In general we have  z8k+1 8k + 1 = A + B + C + D, z8k+3 8k + 3 = A − B − C + D,  k≥0    k≥0  where     k≥0  k≥0  z8k+5 8k + 5 = A − B + C − D, z8k+7 8k + 7 = A + B − C − D,  ,  A = 1 C = 1  8 ln 1 + z 1 − z 4 arctan z,  B = 1  D = 1  √ 2z + z2 27 2 ln 1 + 1 − √ 2z + z2 , √ 2z 25 2 arctan 1 − z2 ;   ANSWERS TO EXERCISES  629  4.3.1  and         ln 1 − z  +  −1 a[m even] ln 1 + z  + fam z   ,  zmk+a mk + a  ⌊ m−1  2⌋  k≥0  k=1  m  = − 1  cos 2πka  m    fam z  =  2  1 − 2z cos 2πk  ln  m  + z − 2 sin 2πka  z sin 2πk m   .  m  arctan  k=1 ≈ 1  1 − z cos 2πk m   40. To get the most significant n 2 places, we need aboutn 2  8 n2 basic operations  see exercise 15 . And we can get the least significant n 2 places by using a b-adic method when b is a power of 2  see exercise 4.1–31 : The problem is easily reduced to the case where v is odd. Let u =   . . . u2u1u0 b, v =   . . . v2v1v0 b, and w =   . . . w2w1w0 b, where we want to solve u = vw  modulo bn 2 . Compute v′ such that v′v mod b = 1  see exercise 4.5.2–17 . Then w0 = v′u0 mod b, and we can compute u′ = u − w0v, 8 n2 basic w1 = v′u′0 mod b, etc. The rightmost n 2 places are found after about 1 4 n2 + O n , while Algorithm D needs about n2 + O n . A operations. So the total is 1 2 n2 + O n . [See A. Schönhage pure right-to-left method for all n digits would require 1 and E. Vetter, Lecture Notes in Comp. Sci. 855  1994 , 448–459; W. Krandick and T. Jebelean, J. Symbolic Computation 21  1996 , 441–455.] 41.  a  If m = 0, let v = u. Otherwise subtract xw from  um+n−1 . . . u1u0 b, where x = u0w′ mod b; this zeroes out the units digit, so we have effectively reduced m by 1.  This operation is closely related to the computation of u w in b-adic arithmetic, since u w = q + bmv w for some integer q; see exercise 4.1–31. It wins over ordinary division because we never have to correct a trial divisor. To compute w′ when b is a power of 2, notice that if w0w′ ≡ 1  modulo 2e  then w0w′′ ≡ 1  modulo 22e  when w′′ =  2 − w0w′ w′, by the 2-adic analog of “Newton’s method.”   b  Apply  a  to the product uv. Memory space is conserved if we interlace multiplication and modulation as follows: Set k ← 0, t ← 0. Then while k < n, preserve the invariant relation bkt ≡  uk−1 . . . u0 v  modulo w  by setting t ← t + ukv, t ←  t−xw  b, k ← k+1, where x = t0w′ mod b is chosen to make t−xw a multiple of b. This solution assumes that t, u, and v have a signed magnitude representation; we can work also with nonnegative numbers < 2w or with complement notations, as discussed by Shand and Vuillemin and by Kornerup, [IEEE Symp. Computer Arithmetic 11  1993 , 252–259, 277–283]. If n is large, the techniques of Section 4.3.3 speed up the multiplication.  c  Represent all numbers congruent to u  modulo w  by an internal value r u  where r u  ≡ bnu. Then addition and subtraction are handled as usual, while mul- tiplication is r uv  = bmult r u , r v  , where bmult is the operation of  b . At the beginning of the computation, replace each operand u by r u  = bmult u, a , using the precomputed constant a = b2n mod w. At the end, replace each r u  by u = bmult r u , 1 . [In the application to RSA encryption, Section 4.5.4, we could redefine the coding scheme so that precomputation and postcomputation are unnecessary.] 42. An interesting analysis by J. M. Holte in AMM 104  1997 , 138–149, establishes the exact formula   m     m − j  b−jn   m + 1 k    r=0  r  Pnk = 1 m!  j   k + 1 − r m−j.   ANSWERS TO EXERCISES  630  The inner sum isk  r=0 −1 rm+1  r   k + 1 − r m =m   when j = 0.  Exercise 5.1.3–25  4.3.1  explains why Eulerian numbers arise in this connection.  43. By exercise 1.2.4–35 we have w = ⌊W 216⌋, where W =  28+1 t =  28+1  uv+27 . 2, we have c < 28, hence w ≥ ⌊ 216 c+1 +28−c  216⌋ ≥ c+1; Therefore if uv 255 > c+ 1 2, we have w ≤ ⌊ 216 c + 1  − c − 1  216⌋ = c. [See J. F. Blinn, IEEE if uv 255 < c + 1 Computer Graphics and Applic. 14, 6  November 1994 , 78–82.]  k  SECTION 4.3.2 1. The solution is unique since 7·11·13 = 1001. The constructive proof of Theorem C tells us that the answer is   11·13 6+6· 7·13 10+5· 7·11 12  mod 1001. But this answer is perhaps not explicit enough! By  24  we have v1 = 1, v2 =  6 − 1  · 8 mod 11 = 7, v3 =   5 − 1  · 2 − 7  · 6 mod 13 = 6, so u = 6 · 7 · 11 + 7 · 7 + 1 = 512. 2. No. There is at most one such u; the additional condition u1 ≡ ··· ≡ ur  modulo 1  is necessary and sufficient, and it follows that such a generalization is not very interesting. 3. u ≡ ui  modulo mi  implies that u ≡ ui  modulo gcd mi, mj  , so the condition ui ≡ uj  modulo gcd mi, mj   must surely hold if there is a solution. Furthermore if u ≡ v  modulo mj  for all j, then u − v is a multiple of lcm m1, . . . , mr  = m; hence there is at most one solution. The proof can now be completed in a nonconstructive manner by counting the number of different r-tuples  u1, . . . , ur  satisfying the conditions 0 ≤ uj < mj and ui ≡ uj  modulo gcd mi, mj  . If this number is m, there must be a solution since  u mod m1, . . . , u mod mr  takes on m distinct values as u goes from a to a + m − 1. Assume that u1, . . . , ur−1 have been chosen satisfying the given conditions; we must now pick ur ≡ uj  modulo gcd mj, mr   for 1 ≤ j < r, and by the generalized Chinese remainder theorem for r − 1 elements there are mr lcm gcd m1, mr , . . . , gcd mr−1, mr   = mr gcd lcm m1, . . . , mr−1 , mr   [This proof is based on identities  10 ,  11 ,  12 , and  14  of  = lcm m1, . . . , mr  lcm m1, . . . , mr−1   ways to do this. Section 4.5.2.]  A constructive proof [A. S. Fraenkel, Proc. Amer. Math. Soc. 14  1963 , 790–791] generalizing  25  can be given as follows. Let Mj = lcm m1, . . . , mj ; we wish to find u = vrMr−1 + ··· + v2M1 + v1, where 0 ≤ vj < Mj Mj−1. Assume that v1, . . . , vj−1 have already been determined; then we must solve the congruence vjMj−1 + vj−1Mj−2 + ··· + v1 ≡ uj  modulo mj .  Here vj−1Mj−2 + ··· + v1 ≡ ui ≡ uj  modulo gcd mi, mj   for i < j by hypothesis, so c = uj −  vj−1Mj−2 + ··· + v1  is a multiple of  lcm gcd m1, mj , . . . , gcd mj−1, mj   = gcd Mj−1, mj  = dj.  We therefore must solve vjMj−1 ≡ c  modulo mj . By Euclid’s algorithm there is a number cj such that cjMj−1 ≡ dj  modulo mj ; hence we may take  vj =  cj c  dj mod  mj dj .  Notice that, as in the nonconstructive proof, we have mj dj = Mj Mj−1.   4.3.2  ANSWERS TO EXERCISES  631  4.  After m4 = 91 = 7 · 13, we have used up all products of two or more odd primes that can be less than 100, so m5, . . . must all be prime.  We find  m7 = 79, m12 = 59, m17 = 37,  m9 = 71, m14 = 47, m19 = 29, and then we are stuck  m22 = 1 does no good . 5.  a  No. The obvious upper bound,  m8 = 73, m13 = 53, m18 = 31,  345272111  p⌊logp 100⌋,  . . . =   p odd p prime  m10 = 67, m15 = 43, m20 = 23,  m11 = 61, m16 = 41, m21 = 17,  is attained if we choose m1 = 34, m2 = 52, etc.  It is more difficult, however, to maximize m1 . . . mr when r is fixed, or to maximize e1+···+er with relatively prime ej as we would attempt to do when using moduli 2ej − 1.   b  Replacing 100 by 256 and allowing even moduli gives 283553 . . . 2511 ≈ 1.67 · 10109. 6.  a  If e = f + kg, then 2e = 2f 2g k ≡ 2f · 1k  modulo 2g − 1 . So if 2e ≡ 2f  modulo 2g − 1 , we have 2e mod g ≡ 2f mod g  modulo 2g − 1 ; and since the latter quantities lie between zero and 2g − 1 we must have e mod g = f mod g.  b  By part  a ,  1 + 2d +··· + 2 c−1 d ·  2e − 1  ≡  1 + 2d +··· + 2 c−1 d ·  2d − 1  = 2cd − 1 ≡ 2ce − 1 ≡ 21 − 1 = 1  modulo 2f − 1 . 7. We have vjmj−1 . . . m1 ≡ uj − vj−1mj−2 . . . m1 +···+ v1  and Cjmj−1 . . . m1 ≡ 1  modulo mj  by  23 ,  25 , and  26 ; see P. A. Pritchard, CACM 27  1984 , 57. This method of rewriting the formulas uses the same number of arithmetic oper- ations and fewer constants; but the number of constants is fewer only if we order the moduli so that m1 < m2 < ··· < mr, otherwise we would need a table of mi mod mj. This ordering of the moduli might seem to require more computation than if we made m1 the largest, m2 the next largest, etc., since there are many more operations to be done modulo mr than modulo m1; but since vj can be as large as mj −1, we are better off with m1 < m2 < ··· < mr in  24  also. So this idea appears to be preferable to the formulas in the text, although Section 4.3.3B shows that the formulas in the text are advantageous when the moduli have the form  14 . 8. Modulo mj: mj−1 . . . m1vj ≡ mj−1 . . . m1 . . .   uj−v1 c1j−v2 c2j−···−vj−1 × c j−1 j ≡ mj−2 . . . m1 . . .  uj − v1 c1j − ··· − vj−2 c j−2 j − vj−1mj−2 . . . m1 ≡ ··· ≡ uj − v1 − v2m1 − ··· − vj−1mj−2 . . . m1. 9. ur ←   . . .  vrmr−1 + vr−1  mr−2 + ···   m1 + v1  mod mr, u2 ←  v2m1 + v1  mod m2, u1 ← v1 mod m1.  The computation should be done in this order, if we want to let uj and vj share the same memory locations, as they can in  24 .  10. If we redefine the “mod” operator so that it produces residues in the symmetrical range, the basic formulas  2 ,  3 ,  4  for arithmetic and  24 ,  25  for conversion remain the same, and the number u in  25  lies in the desired range  10 .  Here  25  is a balanced mixed-radix notation, generalizing balanced ternary notation.  The compar- ison of two numbers may still be done from left to right, in the simple manner described in the text. Furthermore, it is possible to retain the value uj in a single computer word, if we have signed magnitude representation within the computer, even if mj is almost twice the word size. But the arithmetic operations analogous to  11  and  12  are more  . . . ,   632  ANSWERS TO EXERCISES  4.3.2  2 m + 1  =   1  2 m1 + 1 , . . . , 1  2 mr + 1  . Note that 2t · m+1  difficult, so it appears that this idea would result in slightly slower operation on most computers. 2 ≡ t 11. Multiply by 1  modulo m . In general if v is relatively prime to m, then we can find  by Euclid’s algorithm  a number v′ =  v′1, . . . , v′r  such that vv′ ≡ 1  modulo m ; and then if u is known to be a multiple of v we have u v = uv′, where the latter is computed with modular multiplication. When v is not relatively prime to m, division is much harder. 12. Replace mj by m in  11 . [Another way to test for overflow, if m is odd, is to maintain extra bits u0 = u mod 2 and v0 = v mod 2. Then overflow has occurred if and only if u0 + v0 ̸≡ w1 + ··· + wr  modulo 2 , where  w1, . . . , wr  are the mixed-radix digits corresponding to u + v.] 13.  a  x2 − x =  x− 1 x ≡ 0  modulo 10n  is equivalent to  x− 1 x ≡ 0  modulo pn  for p = 2 and 5. Either x or x−1 must be a multiple of p, and then the other is relatively prime to pn; so either x or x− 1 must be a multiple of pn. If x mod 2n = x mod 5n = 0 or 1, we must have x mod 10n = 0 or 1; hence automorphs have x mod 2n ̸= x mod 5n.  b  If x = qpn + r, where r = 0 or 1, then r ≡ r2 ≡ r3, so 3x2 − 2x3 ≡  6qpnr + 3r  −  6qpnr + 2r  ≡ r  modulo p2n .  c  Let c′ be  3 cx 2 − 2 cx 3  x2 = 3c2 − 2c3x. Note: Since the last k digits of an n-digit automorph form a k-digit automorph, it makes sense to speak of the two ∞-digit automorphs, x and 1− x, which are 10-adic numbers  see exercise 4.1–31 . The set of 10-adic numbers is equivalent under modular arithmetic to the set of ordered pairs  u1, u2 , where u1 is a 2-adic number and u2 is a 5-adic number. 14. Find the cyclic convolution  z0, z1, . . . , zn−1  of floating point approximations to  a0u0, a1u1, . . . , an−1un−1  and  a0v0, a1v1, . . . , an−1vn−1 , where the constants ak = k=0 ukak2kq n and v = k=0 tkak2kq n where tk ≈ zk ak. If sufficient accuracy has been maintained, each tk will be very close to an integer. The represen- tation of w can readily be found from those integers. [R. Crandall and B. Fagin, Math. Comp. 62  1994 , 305–324. For improved error bounds, and extensions to moduli of the form k · 2n ± 1, see Colin Percival, Math. Comp. 72  2002 , 387–395.]  2− kq mod n  n have been precomputed. The identities u = n−1 n−1 k=0 vkak2kq n now imply that w =n−1  SECTION 4.3.3 1.  12 × 23 :  34 × 41 :  22 × 18 :  1234 × 2341 :    0276 0276 −0396 1394  02 02 +00 16 16 0396  12 12 +03 04 04 1394 √  02 02 − 01 06 06 Q⌋ ≤ 0276 Q + ⌊√ Q⌋+1. 2. 3. The result is true when k ≤ 2, so assume that k > 2. Let qk = 2Qk, rk = 2Rk, so that Rk = ⌊√ Qk⌋ and Qk = Qk−1+Rk−1. We must show that 1+ Rk+1 2Rk ≤ 2Qk−1; this inequality isn’t close at all. One way is to observe that 1 +  Rk + 1 2Rk ≤ 1 + 22Rk and 2Rk   2.  The fact that 2Rk < Qk−1 is readily proved by induction since Rk+1 − Rk ≤ 1 and Qk − Qk−1 ≥ 2.   1394 2888794 Q+1, so ⌊√ √  Q + R⌋ ≤ ⌊√  √ Q + 1 =    Q + 2  Q +  Q <   4.3.3  ANSWERS TO EXERCISES  633  4. For j = 1, . . . , r, calculate Ue j2 , jUo j2 , Ve j2 , jVo j2 ; and by recursively calling the multiplication algorithm, calculate 2  + jUo j 2  − jUo j  W  j  =  Ue j W  −j  =  Ue j  2  + jVo j 2  − jVo j  2   Ve j 2   Ve j  2  , 2  .  √    2Qk+1 =  Then we have We j2  =  W  j  + W  −j   2, Wo j2  =  W  j  − W  −j    2j . Also calculate We 0  = U 0 V  0 . Now construct difference tables for We and Wo, which are polynomials whose respective degrees are r and r − 1.  This method reduces the size of the numbers being handled, and reduces the number of additions and multiplications. Its only disadvantage is a longer program  since the control is somewhat more complex, and some of the calculations must be done with signed numbers .  Another possibility would perhaps be to evaluate We and Wo at 12, 22, 42, . . . ,  2r 2; although the numbers involved are larger, the calculations are faster, since all multiplications are replaced by shifting and all divisions are by binary numbers of the form 2j 2k − 1 .  Simple procedures are available for dividing by such numbers.  5. Start the q and r sequences out with q0 and q1 large enough so that the inequality in exercise 3 is valid. Then we will find in the formulas like those preceding Theorem B that we have η1 → 0 and η2 =  1 + 1  2rk  21+√2Qk−√2Qk+1  Qk Qk+1 . The factor Qk Qk+1 → 1 as k → ∞, so we can ignore it if we want to show that η2 < 1 − ϵ 2Qk + 1  + 1 ≥ for all large k. Now √ 2Qk + 1 + 1  3Rk . Hence η2 ≤  1 + 1  2rk  2−1  3Rk , and lg η2 < 0 for large enough k. Note: Algorithm T can also be modified to define a sequence q0, q1, . . . of a similar type that is based on n, so that n ≈ qk + qk+1 after step T1. This modification leads to the estimate  21 . 6. Any common divisor of 6q+d1 and 6q+d2 must also divide their difference d2−d1.   differences are 2, 3, 4, 6, 8, 1, 2, 4, 6, 1, 3, 5, 2, 4, 2, so we must only show  2Qk ⌉ + 2 ≥  2Qk + 2⌈√  The 6  that at most one of the given numbers is divisible by each of the primes 2, 3, 5. Clearly only 6q + 2 is even, and only 6q + 3 is a multiple of 3; and there is at most one multiple of 5, since qk ̸≡ 3  modulo 5 . tk−1 6k−1 + ck 2k ≤ t0 + c 7. Let pk−1 < n ≤ pk. We have tk ≤ 6tk−1 + ck3k for some constant c; so tk 6k ≤ 8. False. To see the fallacy, try it with k = 2. 9. ˜us = ˆu qs  mod K. In particular, if q = −1 we get ˆu −r  mod K, which avoids data- flipping when computing inverse transforms.  10. A[j] sk−1, . . . , sk−j, tk−j−1, . . . , t0  can be written  j≥1 j 2j = M. Thus tk ≤ M · 6k = O p   2Qk + 2  log3 6 k  √   .  2        2k−j sk−j ...sk−1 2· tk−1...tk−j 2  ω  ωtqvq  ,  ωtpup  0≤p<K  0≤q<K  p,q upvqS p, q , where S p, q  = 0 or 2j. We have S p, q  = 2j for  exactly 22k 2j values of p and q. 11. An automaton cannot have z2 = 1 until it has c ≥ 2, and this occurs first for Mj at time 3j − 1. It follows that Mj cannot have z2z1z0 ̸= 000 until time 3 j − 1 . Furthermore, if Mj has z0 ̸= 0 at time t, we cannot change this to z0 = 0 without   and this is   0≤tk−1,...,tk−j≤1   634  ANSWERS TO EXERCISES  4.3.3  affecting the output; but the output cannot be affected by this value of z0 until at least time t + j − 1, so we must have t + j − 1 ≤ 2n. Since the first argument we gave proves that 3 j − 1  ≤ t, we must have 4 j − 1  ≤ 2n, that is, j − 1 ≤ n 2, i.e., j ≤ ⌊n 2⌋ + 1. This is the best possible bound, since the inputs u = v = 2n − 1 require the use of Mj for all j ≤ ⌊n 2⌋ + 1.  For example, Table 2 shows that M2 is needed to multiply two-bit numbers, at time 3.  12. We can “sweep through” K lists of MIX-like instructions, executing the first instruc- tion on each list, in O K +  N log N 2  steps as follows:  i  A radix list sort  Section 5.2.5  will group together all identical instructions, in time O K +N .  ii  Each set of j identical instructions can be performed in O log N 2 + O j  steps, and there are O N 2  sets. A bounded number of sweeps will finish all the lists. The remaining details are straightforward; for example, arithmetic operations can be simulated by converting p and q to binary. [SICOMP 9  1980 , 490–508.] 13. If it takes T  n  steps to multiply n-bit numbers, we can accomplish m-bit times n-bit multiplication by breaking the n-bit number into ⌈n m⌉ m-bit groups, using ⌈n m⌉T  m  + O n + m  operations. The results cited in the text therefore give an estimated running time of O n log m log log m  on Turing machines, or O n log m  on machines with random access to words of bounded size, or O n  on pointer machines. 15. The best upper bound known is O n log n 2 log log n , due to M. J. Fischer and L. J. Stockmeyer [J. Comp. and Syst. Sci. 9  1974 , 317–331]; their construction works on multitape Turing machines, and is O n log n  on pointer machines. The best lower bound known is of order n log n log log n, due to M. S. Paterson, M. J. Fischer, and A. R. Meyer [SIAM AMS Proceedings 7  1974 , 97–111]; this applies to multitape Turing machines but not to pointer machines. 16. Let 2k be the smallest power of 2 that exceeds 2K. Set at ← ω−t2 2ut and bt ← ω 2K−2−t 2 2, where ut = 0 for t ≥ K. We want to evaluate the convolutions j=0 ajbr−j for r = 2K − 2− s, when 0 ≤ s < K. The convolutions can be found by using three fast Fourier transformations of order 2k, as in the text’s multiplication procedure. [Note that this technique, sometimes called the “chirp transform,” works for any complex number ω, not necessarily a root of unity. See L. I. Bluestein, Northeast Electronics Res. and Eng. Meeting Record 10  1968 , 218–219; D. H. Bailey and P. N. Swarztrauber, SIAM Review 33  1991 , 389–404.] 17. The quantity Dn = Kn+1 − Kn satisfies D1 = 2, D2n = 2Dn, and D2n+1 = It follows that Kn = 3e1 + Dn; hence Dn = 2e1−t+2 when n has the stated form.  cr =r  l=2 3el2e1−el−l+3, by induction on n.  Incidentally, Kn is odd, and we can multiply an n-place integer by an  n + 1 - place integer with  Kn + Kn+1  2 1-place multiplications. The generating function n≥1 Knzn satisfies zK z  + z2 = K z2  z + 1  z + 2 ; hence K −1  = 1 and K 1  = 1 5. 18. The following scheme uses 3N + SN places of working storage, where S1 = 0, S2n = Sn, and S2n−1 = Sn + 1, hence Sn = e1 − et − t + 2 − [t = 1] in the notation of the previous exercise. Let N = 2n− ϵ, where ϵ is 0 or 1, and assume that N > 1. Given N-place numbers u = 2nU1+U0 and v = 2nV1+V0, we first form U0−U1 and V0−V1 in two n-place areas starting at positions 0 and n of the  3N + SN -place working area. Then we place their product into the working area starting at position 3n + Sn. The next step is to form the 2 n − ϵ -place product U1V1, starting in position 0; using that product, we change the 3n − 2ϵ places starting at position 3n + Sn to the value of  t K z  =   4.4  ANSWERS TO EXERCISES  635  U1V1− U0−U1  V0−V1 +2nU1V1.  Notice that 3n−2ϵ+3n+Sn = 3N +SN.  Finally, we form the 2n-place product U0V0 starting at position 0, and add it to the partial result starting at positions 2n + Sn and 3n + Sn. We must also move the 2N-place answer to its final position by shifting it down 2n + Sn positions.  The final move could be avoided by a trickier variation that cyclically rotates its output by a given amount within a designated working area. If the 2N-place product is not allowed to be adjacent to the auxiliary working space, we need about N more places of memory  that is, a total of about 6N instead of 5N places, for the input, output, and temporary storage ; see R. Maeder, Lecture Notes in Comp. Sci. 722  1993 , 59–65. 19. Let m = s2 + r where −s < r ≤ s. We can use  2  with U1 = ⌊u s⌋, U0 = u mod s, V1 = ⌊v s⌋, V0 = v mod s, and with s playing the role of 2n. If we know the signs of U1 − U0 and V1 − V0 we know how to compute the product U1 − U0V1 − V0, which is < m, and whether to add or subtract it. It remains to multiply by s and by s2 ≡ −r. Each of these can be done with four multiplication divisions, using exercise 3.2.1.1–9, but only seven are needed because one of the multiplications needed to compute sx mod m is by r or r+s. Thus 14 multiplication divisions are sufficient  or 12, in case u = v or u is constant . Without the ability to compare operands, we can still do the job with one more multiplication, by computing U0V1 and U1V0 separately.  SECTION 4.4 1. We compute  . . .  ambm−1+am−1 bm−2+···+a1 b0+a0 by adding and multiplying in the BJ system.  = 20 cwt.  = 8 st.  = 14 lb. = 16 oz.     Start with zero Add 3 Multiply by 24 Add 9 Multiply by 60 Add 12 Multiply by 60 Add 37  T. 0 0 0 0 0 0 8 8  0 0 0 0 2 2 3 3  0 0 0 0 5 5 1 1  0 0 4 5 9 10 0 2  0 3 8 1 12 8 0 5   Addition and multiplication by a constant in a mixed-radix system are readily done using a simple generalization of the usual carry rule; see exercise 4.3.1–9.  2. We compute ⌊u B0⌋, ⌊⌊u B0⌋ B1⌋, etc., and the remainders are A0, A1, etc. The division is done in the bj system.  d. 3 0 0 0 0 0  = 24 h.  = 60 m.  = 60 s.    Start with u Divide by 16 Divide by 14 Divide by 8 Divide by 20 Divide by ∞  9 5 0 0 0 0 Answer: 8 T. 3 cwt. 1 st. 2 lb. 5 oz. 3. The following procedure due to G. L. Steele Jr. and Jon L White generalizes Taranto’s algorithm for B = 2 originally published in CACM 2, 7  July 1959 , 27.  Remainder = 5 Remainder = 2 Remainder = 1 Remainder = 3 Remainder = 8  37 32 45 43 8 0  12 4 21 2 0 0   636  ANSWERS TO EXERCISES  4.4  will satisfy the given conditions.   A1. [Initialize.] Set M ← 0, U0 ← 0. A2. [Done?] If u   1 − ϵ, go to step A4.  Otherwise no M-place fraction A3. [Transform.] Set M ← M + 1, U−M ← ⌊Bu⌋, u ← Bu mod 1, ϵ ← Bϵ, and return to A2.  This transformation returns us to essentially the same state we were in before; the remaining problem is to convert u to U with fewest radix-B places so that U − u < ϵ. Note, however, that ϵ may now be ≥ 1; in this case we could go immediately to step A4 instead of storing the new value of ϵ.   A4. [Round.] If u ≥ 1  2, increase U−M by 1.  If u = 1  2 exactly, another rounding rule such as “increase U−M by 1 only when it is odd” might be preferred; see Section 4.2.2.  Step A4 will never increase U−M from B − 1 to B; for if U−M = B − 1 we must have M > 0, but no  M − 1 -place fraction was sufficiently accurate. Steele and White go on to consider floating point conversions in their paper [SIGPLAN Notices 25, 6  June 1990 , 112–126]. See also D. E. Knuth in Beauty is Our Business, edited by W. H. J. Feijen et al.  New York: Springer, 1990 , 233–242. 4.  a  1 2k = 5k 10k.  b  Every prime divisor of b divides B. 5. If and only if 10n − 1 ≤ c < w; see  3 . 7. αu ≤ ux ≤ αu + u w ≤ αu + 1, hence ⌊αu⌋ ≤ ⌊ux⌋ ≤ ⌊αu + 1⌋. Furthermore, in the special case cited we have ux < αu + α and ⌊αu⌋ = ⌊αu + α − ϵ⌋ for 0 < ϵ ≤ α. 8.   Can occur only on  ENT1 0 U LDA =1  10= 1H MUL TEMP 3H STA MUL =-10= SLAX 5 ADD U JANN 2F  TEMP  LDA DECA 1 3B JMP ANSWER,1  May be minus zero.  2H STA LDA TEMP INC1 1 JAP 1B  the first iteration, by exercise 7.   5  1 − 1 pk  ⌊u 2⌋ + 1 ; 9. Let pk = 22k+2. By induction on k we have vk u  ≤ 16 hence ⌊vk u  16⌋ ≤ ⌊⌊u 2⌋ 5⌋ = ⌊u 10⌋ for all integers u ≥ 0. Furthermore, since vk u + 1  ≥ vk u , the smallest counterexample to ⌊vk u  16⌋ = ⌊u 10⌋ must occur when u is a multiple of 10. Now let u = 10m be fixed, and suppose vk u  mod pk = rk so that vk+1 u  = vk u  +  vk u  − rk  pk. The fact that p2 k = pk+1 implies that there exist integers m0, m1, m2, . . . such that m0 = m, vk u  =  pk − 1 mk + xk, and mk = mk+1pk + xk − rk, where xk+1 =  pk + 1 xk − pkrk. Unwinding this recurrence yields  vk u  =  pk − 1 mk + ck − k−1  k−1  pjrj   pi + 1 ,  j=0  i=j+1  ck = 3 pk − 1 p0 − 1 .  Furthermore vk u  + mk = vk+1 u  + mk+1 is independent of k, and it follows that vk u  16 = m +  3− mk  16. So the minimal counterexample u = 10yk is obtained for 0 ≤ k ≤ 4 by setting mk = 4 and rj = pj − 1 in the formula yk = 1 16 vk + mk − c0 . In hexadecimal notation, yk turns out to be the final 2k digits of 434243414342434. Since v4 10y4  is less than 264, the same counterexample is also minimal for all k > 4. One way to work with larger operands is to modify the method by starting with   4.4  ANSWERS TO EXERCISES  637  v0 u  = 6⌊u 2⌋ + 6 and letting ck = 6 pk − 1   p0 − 1 , m0 = 2m.  In effect, we are truncating one bit further to the right than before.  Then ⌊vk u  32⌋ = ⌊u 10⌋ when u is less than 10zk, for 1 ≤ k ≤ 7, where zk = 1 32 vk + mk − 6  when mk = 7, r0 = 14, and rj = pj − 1 for j > 0. For example, z4 = 1c342c3424342c34. [This exercise is based on ideas of R. A. Vowels, Australian Comp. J. 24  1992 , 81–85.] 10.  i  Shift right one;  ii  Extract the left bit of each group;  iii  Shift the result of  ii  right two;  iv  Shift the result of  iii  right one, and add it to the result of  iii ;  v  Subtract the result of  iv  from the result of  i . 11.  5.7 7 2 1  Answer:  24529 10.  12. First convert the ternary number to nonary  radix 9  notation, then proceed as in octal-to-decimal conversion but without doubling. Decimal to nonary is similar. In the given example, we have  4 7.7 2 1  3 8 3.2 1  − 1 0 − 9 4 − 7 6 6 3 0 6 6.1 − 6 1 3 2 2 4 5 2 9  1.7 6 4 7 2 3  1 6.6 4 7 2 3  1 5 0.4 7 2 3  − 1 − 1 6 − 1 5 0 − 1 3 5 4 − 1 2 1 9 3 1 0 9 7 3 9.3 − 1 0 9 7 3 9  1 2 1 9 3.2 3  1 3 5 4.7 2 3  13. BUF  START JOV  8H 1H  2H  ALF .␣␣␣␣ ORIG *+39 OFLO ENT2 -40 ENT3 10 ENT1 m ENTX 0 STX ··· J1P 2B SLAX 5 CHAR STA  CARRY  9.8 7 6 5 4  +  9  1 1 8.7 6 5 4  + 1 1 8  1 3 1 6.6 5 4  + 1 3 1 6  1 4 4 8 3.5 4  + 1 4 4 8 3  1 6 0 4 2 8.4 + 1 6 0 4 2 8   Radix point on first line   Ensure that overflow is off. Set buffer pointer. Set loop counter. Begin multiplication routine.   See exercise 4.3.1–13, with v = 109 and W = U.  rA ← next nine digits.  BUF+40,2 2:5   Store next nine digits.  9 8 7 6 5 4 Answer:  987654 10.  1 7 6 4 7 2 3 Answer:  1764723 9.   638  ANSWERS TO EXERCISES  4.4  BUF+41,2  STX INC2 2 DEC3 1 1B J3P BUF+20,2 PRINTER  OUT J2N 8B  Increase buffer pointer.  Repeat ten times.  Repeat until both lines are printed.  2 BE−P and U ≥ u + 1  2 BE−P and U ≤ u − 1  14. Let K n  be the number of steps required to convert an n-digit decimal number to binary and at the same time to compute the binary representation of 10n. Then we have K 2n  ≤ 2K n  + O M n  . Proof. Given the number U =  u2n−1 . . . u0 10, compute U1 =  u2n−1 . . . un 10 and U0 =  un−1 . . . u0 10 and 10n, in 2K n  steps, then compute U = 10nU1 + U0 and 102n = 10n · 10n in O M n   steps. It follows that K 2n  = O M 2n  + 2M 2n−1  + 4M 2n−2  + ···  = O nM 2n  . [Similarly, Schönhage has observed that we can convert a  2n lg 10 -bit number U from binary to decimal, in O nM 2n   steps. First form V = 102n−1 in O M 2n−1  + M 2n−2  + ···  = O M 2n   steps, then compute U0 =  U mod V   and U1 = ⌊U V ⌋ in O M 2n   further steps, then convert U0 and U1.] 17. See W. D. Clinger, SIGPLAN Notices 25, 6  June 1990 , 92–101, and the paper by Steele and White cited in the answer to exercise 3. 18. Let U = roundB u, P   and v = roundb U, p . We may assume that u > 0, so that U > 0 and v > 0. Case 1: v < u. Determine e and E such that be−1 < u ≤ be, 2 be−p; hence BP−1 ≤ BE−1 ≤ U < BE. Then u ≤ U + 1 BP−EU   u. Determine e and E such that be−1 ≤ u < be, BE−1 < U ≤ BE. Then u ≥ U − 1 2 be−p; hence BP−1 ≤ BP−E U − BE−P   < BP−Eu ≤ bp−eu < bp. Thus we have proved that BP−1 < bp whenever v ̸= u. Conversely, if BP−1 < bp, the proof above suggests that the most likely example for which u ̸= v will occur when u is a power of b and at the same time it is close to a 2  bp − 1 power of B. We have BP−1bp < BP−1bp + 1 2 ; hence 1 < α = 1  1 − 1 2 B1−P = β. There are integers e and E such that logB α < e logB b− E < logB β, by exercise 4.5.3–50. Hence α < be BE < β, for some e and E. Now we have roundB be, P   = BE, and roundb BE, p  < be. [CACM 11  1968 , 47–50; Proc. Amer. Math. Soc. 19  1968 , 716–723.] For example, if bp = 210 and BP = 104, the number u = 26408 ≈ .100049 · 101930 rounds down to U = .1 · 101930 ≈  .111111111101111111111 2 · 26408, which rounds down to 26408−26398.  The smallest example is actually round  .1111111001 2·2784  = .1011· 10236, round .1011· 10235  =  .11111110010 2 · 2784, found by Fred J. Tydeman.  19. m1 =  F0F0F0F0 16, c1 = 1 − 10 16 makes U =   u7u6 10 . . .  u1u0 10 256; then m2 =  FF00FF00 16, c2 = 1 − 102 162 makes U =   u7u6u5u4 10 u3u2u1u0 10 65536; and m3 =  FFFF0000 16, c3 = 1 − 104 164 finishes the job. [Compare with Schönhage’s algorithm in exercise 14. This technique is due to Roy A. Keir, circa 1958.] SECTION 4.5.1 1. Test whether or not uv′ < u′v, since the denominators are positive.  See also the answer to exercise 4.5.3–39.  2. If c > 1 divides both u d and v d, then cd divides both u and v. If pe is a divisor of uv and u′v′ for e ≥ 1, then either pe\u 3. Let p be prime. and pe\v′ or pe\u′ and pe\v; hence pe\ gcd u, v′  gcd u′, v . The converse follows by reversing the argument.  4 =  BP−1 + 1  2 b−p  < 1 + 1  2 BP−1 − 1  2 bp − 1   4.5.1  ANSWERS TO EXERCISES  639  4. Let d1 = gcd u, v , d2 = gcd u′, v′ ; the answer is w =  u d1  v′ d2 sign v , w′ =  u′ d2  v d1 , with a “divide by zero” error message if v = 0. 5. d1 = 10, t = 17 · 7 − 27 · 12 = −205, d2 = 5, w = −41, w′ = 168. 6. Let u′′ = u′ d1, v′′ = v′ d1; our goal is to show that gcd uv′′ + u′′v, d1  = gcd uv′′ + u′′v, d1u′′v′′ . If p is a prime that divides u′′, then p does not divide u or v′′, so p does not divide uv′′ + u′′v. A similar argument holds for prime divisors of v′′, so no prime divisors of u′′v′′ affect the given gcd. 7.  N − 1 2 +  N − 2 2 = 2N 2 −  6N − 5 . If the inputs are n-bit binary numbers, 2n + 1 bits may be necessary to represent t. 8. For multiplication and division these quantities obey the rules x 0 = sign x ∞,  ±∞ × x = x× ±∞  =  ±∞  x = ±sign x ∞, x  ±∞  = 0, provided that x is finite and nonzero, without change to the algorithms described. Furthermore, the algorithms can readily be modified so that 0 0 = 0× ±∞  =  ±∞ ×0 = “ 0 0 ”, where the latter is a representation of “undefined.” If either operand is undefined the result should be undefined also.  Since the multiplication and division subroutines can yield these fairly natural rules of extended arithmetic, it is sometimes worthwhile to modify the addition and subtraction operations so that they satisfy the rules x ± ∞ = ±∞, x ±  −∞  = ∓∞, for x finite;  ±∞  +  ±∞  = ±∞ −  ∓∞  = ±∞; furthermore  ±∞  +  ∓∞  =  ±∞  −  ±∞  =  0 0 ; and if either or both operands are  0 0 , the result should also be  0 0 . Equality tests and comparisons may be treated in a similar manner. The remarks above are independent of “overflow” indications. If ∞ is being used to suggest overflow, it is incorrect to let 1 ∞ be equal to zero, lest inaccurate results be regarded as true answers. It is far better to represent overflow by  0 0 , and to adhere to the convention that the result of any operation is undefined if at least one of the inputs is undefined. This type of overflow indication has the advantage that final results of an extended calculation reveal exactly which answers are defined and which are not. 9. If u u′ ̸= v v′, then 1 ≤ uv′ − u′v = u′v′u u′ − v v′ < 22nu u′ − 22nv v′; two quantities differing by more than unity cannot have the same “floor.”  In other words, the first 2n bits to the right of the binary point are enough to characterize the value of a binary fraction, when there are n-bit denominators. We cannot improve this to 2n − 1 bits, for if n = 4 we have 1 11. To divide by  v + v′ reciprocal,  v − v′ 12.   2q−1 − 1  1 ; round x  =  0 1  if and only if x ≤ 21−q. Similarly, round x  =  1 0  if and only if x ≥ 2q−1. 13. One idea is to limit numerator and denominator to a total of 27 bits, where we need only store 26 of these bits  since the leading bit of the denominator is 1 unless the denominator has length 0 . This leaves room for a sign and five bits to indicate the denominator size. Another idea is to use 28 bits for numerator and denominator, which are to have a total of at most seven hexadecimal digits, together with a sign and a 3-bit field to indicate the number of hexadecimal digits in the denominator.  5   v′′, when v and v′ are not both zero, multiply by the  5  v′′  v2 − 5v′2 , and reduce to lowest terms.  14 =  .00010010 . . .  2.   13 =  .00010011 . . .  2, 1  [Using the formulas in the next exercise, the first alternative leads to exactly 2140040119 finite representable numbers, while the second leads to 1830986459. The first alternative is preferable because it represents more values, and because it is cleaner  √  √   640  ANSWERS TO EXERCISES  4.5.1  and makes smoother transitions between ranges. With 64-bit words we would, similarly, limit numerator and denominator to a total of at most 64 − 6 = 58 bits.] 14. The number of multiples of n in the interval  a . . b ] is ⌊b n⌋ − ⌊a n⌋. Hence, by inclusion and exclusion, the answer to this problem is S0 − S1 + S2 − ··· , where Sk is   ⌊M2 P⌋ − ⌊M1 P⌋  ⌊N2 P⌋ − ⌊N1 P⌋ , summed over all products P of k distinct  primes. We can also express the answer as  min M2,N2   n=1  µ n   ⌊M2 n⌋ − ⌊M1 n⌋   ⌊N2 n⌋ − ⌊N1 n⌋ .  SECTION 4.5.2 1. Substitute min, max, + consistently for gcd, lcm, ×, respectively  after making sure that the identities are correct when any variable is zero . 2. For prime p, let up, v1p, . . . , vnp be the exponents of p in the canonical factor- izations of u, v1, . . . , vn. By hypothesis, up ≤ v1p + ··· + vnp. We must show that up ≤ min up, v1p  + ··· + min up, vnp , and this is certainly true if up is greater than or equal to each vjp, or if up is less than some vjp. 3. Solution 1: If n = pe1 r , the number in each case is  2e1 + 1  . . .  2er + 1 . Solution 2: A one-to-one correspondence is obtained if we set u = gcd d, n  and v = [E. Cesàro, Annali di Matematica Pura ed Ap- n2  lcm d, n  for each divisor d of n2. plicata  2  13  1885 , 235–250, §12.] 4. See exercise 3.2.1.2–15 a . 5. Shift u and v right until neither is a multiple of 3, remembering the proper power of 3 that will appear in the gcd. Each subsequent iteration sets t ← u + v or t ← u − v  whichever is a multiple of 3 , shifts t right until it is not a multiple of 3, then replaces max u, v  by the result.  1 . . . per  u  13634 13634 1904 1904 34 34 34  v  24140 3502 3502 1802 1802 68 34  t  10506, 3502; 17136, 5712, 1904; 5406, 1802; 102, 34; 1836, 612, 204, 68; 102, 34; 0.  The evidence that gcd 40902, 24140  = 34 is now overwhelming. 6. The probability that both u and v are even is 1 multiples of four is 1  4; the probability that both are 16; etc. Thus A has the distribution given by the generating function  3 4 + 3  16 z + 3 64 z  3, and the standard deviation is  2  2 + ··· = 3 4  1 − z 4 . 3 − 1 9 + 1  The mean is 1 If u and v are independently and uniformly distributed with 1 ≤ u, v < 2N, some small correction terms are needed; the mean is then actually  2N−k − 1 2 = 1   2N − 1 −2  9 = 2 3.  N  3 2N − 1 −1 + N 2N − 1 −2  3 − 4  .  k=1   4.5.2  ANSWERS TO EXERCISES  641  7. When u and v are not both even, each of the cases  even, odd ,  odd, even ,  odd, odd  is equally probable, and B = 1, 0, 0 in these cases. Hence B = 1 3 on the average. Actually, as in exercise 6, a small correction should be given to be strictly accurate when 1 ≤ u, v < 2N; the probability that B = 1 is actually 3 − 1   2N−k − 1 2N−k = 1  3 2N − 1 −1   2N − 1 −2  N  .  k=1  2 Cave − 1  + Bave.  8. Let F be the number of subtraction steps in which u > v; then E = F + B. If we change the inputs from  u, v  to  v, u , the value of C stays unchanged, while F becomes C − 1 − F . Hence Eave = 1 9. The binary algorithm first gets to B6 with u = 1963, v = 1359; then t ← 604, 302, 151, etc. The gcd is 302. Using Algorithm X we find that 2 · 31408 − 23 · 2718 = 302. 10.  a  Two integers are relatively prime if and only if they are not both divisible by k=1 µ k  n k 2 = n any prime number.  b  Rearrange the sum in  a , with denominators k = p1 . . . pr.  c  Since  n k 2 − ⌊n k⌋2 =  Each of the sums in  a  and  b  is actually finite.  k=1 O n k  = O nHn . Furthermore s = ns − n d\n µ d  = δ1n. [In fact, we have the more general result  O n k , we have qn −n  k>n n k 2 = O n .  d  µ d  n  s + n  s − ··· ,    d  p  pq  d\n  r .] 1 . . . per  r  if n = pe1  1 ζ k  = 1    as in part  b , where the sums on the right are over the prime divisors of n, and this is equal to ns 1 − 1 ps1  . . .  1 − 1 ps  Notes: Similarly, we find that a set of k integers is relatively prime with probability n≥1 1 nk . This proof of Theorem D is due to F. Mertens, Crelle 77  1874 , 289–291. The technique actually gives a much stronger result, namely that 6π−2mn + O n log m  pairs of integers u ∈ [f m  . . f m  + m , v ∈ [g n  . . g n  + n  are relatively prime, when m ≤ n, f m  = O m , and g n  = O n . 9, namely 49  6π2  ≈ .82746.  b  6 π2 times 1 1 + 2 4 + 11.  a  6 π2 times 1 + 1 3 9 + ··· , namely ∞.  This is true in spite of the results of exercises 12 and 14.  12. [Annali di Mat.  2  13  1885 , 235–250, §3.] Let σ n  be the number of positive  4 + 1  divisors of n. The answer is  σ k  ·  6 π2k2 = 6  π2  1 k2  = π2 6 .  k≥1  2    k≥1  4 + 1  9 + 1  9 + ··· − 1  [Thus, the average is less than 2, although there are always at least two common divisors when u and v are not relatively prime.] 4 1 + 1 13. 1 + 1  14.  a  L =  6 π2   b   8 π2   d≥1 d−2 ln d = −ζ′ 2  ζ 2  =  25 + ··· = 1 + 1 d≥1[d odd] d−2 ln d = L − 1  4 + 1 3 ln 2 ≈ 0.33891.  9 + ··· . p prime ln p   p2 − 1  ≈ 0.56996.  15. v1 = ±v u3, v2 = ∓u u3  the sign depends on whether the number of iterations is even or odd . This follows from the fact that v1 and v2 are relatively prime to each other  throughout the algorithm , and that v1u = −v2v. [Hence v1u = lcm u, v  at the close of the algorithm, but this is not an especially efficient way to compute the least common multiple. For a generalization, see exercise 4.6.1–18.]  Further details can be found in exercise 4.5.3–48.   642  ANSWERS TO EXERCISES  4.5.2  16. Apply Algorithm X to v and m, thus obtaining a value x such that xv ≡ 1  modulo m .  This can be done by simplifying Algorithm X so that u2, v2, and t2 are not computed, since they are never used in the answer.  Then set w ← ux mod m. [It follows, as in exercise 4.5.3–45, that this process requires O n2  units of time, when it is applied to large n-bit numbers. See exercises 17 and 39 for alternatives to Algorithm X.] 17. We can let u′ =  2u − vu2  mod 22e, as in Newton’s method  see the end of Sec- tion 4.3.1 . Equivalently, if uv ≡ 1 + 2ew  modulo 22e , let u′ = u + 2e  −uw  mod 2e . 18. Let u1, u2, u3, v1, v2, v3 be multiprecision variables, in addition to u and v. The extended algorithm will act the same on u3 and v3 as Algorithm L does on u and v. New multiprecision operations are to set t ← Auj, t ← t + Bvj, w ← Cuj, w ← w + Dvj, uj ← t, vj ← w for all j, in step L4; also if B = 0 in that step to set t ← uj − qvj, uj ← vj, vj ← t for all j and for q = ⌊u3 v3⌋. A similar modification is made to step L1 if v3 is small. The inner loop  steps L2 and L3  is unchanged. 19.  a  Set t1 = x+2y+3z; then 3t1+y+2z = 1, 5t1−3y−20z = 3. Eliminate y, then 14t1 − 14z = 6: No solution.  b  This time 14t1 − 14z = 0. Divide by 14, eliminate t1; the general solution is x = 8z − 2, y = 1 − 5z, z arbitrary. 20. We can assume that m ≥ n. If m > n = 0 we get to  m − t, 0  with probability 2−t for 1 ≤ t < m, to  0, 0  with probability 21−m. Valida vi, the following values can be obtained for n > 0:  Case 1, m = n. From  n, n  we go to  n−t, n  with probability t 2t−5 2t+1+3 22t, for 2 ≤ t < n. 27 7 256, . . . .  To  0, n  the probability is 64, n 2n−1 − 1 2n−2 + 1 22n−2. To  n, k  the probability is the same as to  k, n . The algorithm terminates with probability 1 2n−1.   These values are 1 16,  Case 2, m = n+1. From  n+1, n  we get to  n, n  with probability 1  8 when n > 1, or 0 when n = 1; to  n − t, n  with probability 11 2t+3 − 3 22t+1, for 1 ≤ t < n − 1. 128, . . . .  We get to  1, n  with probability 5 2n+1 − 3 22n−1,  These values are 5 for n > 1; to  0, n  with probability 3 2n − 1 22n−1.  4, 19  16, 1  Case 3, m ≥ n + 2. The probabilities are given by the following table:   m − 1, n  :  m − t, n  :  m − n, n  :  m − n − t, n  :  0, n  :  1 2 − 3 2m−n+2 − δn1 2m+1; 1 2t + 3 2m−n+t+1, 1 2n + 1 2m, 1 2n+t + δt1 2m−1, 1 2m−1.  n > 1;  1 < t < n;  1 ≤ t < m − n;  The only thing interesting about these results is that they are so messy; but that  makes them uninteresting. 21. Show that for fixed v and for 2m < u < 2m+1, when m is large, each subtract- and-shift cycle of the algorithm reduces ⌊lg u⌋ by two, on the average. 22. Exactly  N − m 2m−1+δm0 integers u in the range 1 ≤ u < 2N have ⌊lg u⌋ = m, after u has been shifted right until it is odd. Thus  N − n 2n−1   2N − 1 2    C00 + 2N  C = N  Cn0  2  + 2   1≤n<m≤N  1≤n≤N  N − m  N − n 2m+n−2   N − n 222n−2  Cnn.  Cmn +   1≤n≤N   The same formula holds for D in terms of Dmn.    4.5.2  The middle sum is 22N−2  22N−2   m2−m = 2 −  n + 1 21−n  0≤m<n the sum on m is  ANSWERS TO EXERCISES  643 and  0≤m<n<N mn2−m−n  α + β N + γ − αm− βn . Since 2 + n + 2 21−n, 2+n+2 21−n    m m − 1 2−m = 4 −  n  n2−n γ−α−βn+ α+β N  2− n+1 21−n −α 4− n  0≤m<n  0≤n<N   α+β N  n2−n 2− n+1 21−n +O 1   .  = 22N−2    n≥0  Thus the coefficient of  α + β N in the answer is found to be 2−2 4 −   4 A similar argument applies to the other sums.  3 3  = 11 27. Note: The exact value of the sums may be obtained after some tedious calculation  by means of the general summation-by-parts formula    0≤k<n  km zk = m! zm   1 − z m+1 − m  k=0  mk nm−k zn+k   1 − z k+1  .  24.   23. If x ≤ 1 it is Pr u ≥ v and v u ≤ x  = 1 and v u ≥ 1 x  = 1  2 Gn 1 x ; this also equals 1  2 + 1  2 1−Gn x  . And if x ≥ 1 it is 1  2 1 − Gn x   by  40 .  2 +Pr u ≤ v  k≥1 2−kG 1  2k + 1   = S 1 . This value, which has no obvious connection to classical constants, is approximately 0.5432582959. 25. Richard Brent has noted that G e−y  is an odd function that is analytic for all real values of y. If we let G e−y  = λ1y + λ3y3 + λ5y5 + ··· = ρ e−y − 1 , we have −ρ1 = λ1 = λ, ρ2 = 1  5 λ + 7  4 λ3 + λ5;  4 λ + 3  2 λ, −ρ3 = 1   n  −1 nρn =   k!  3 λ + λ3, ρ4 = 1 n! λk;  k  k  2 λ3, −ρ5 = 1   n λn = −   k!  n! ρk.  k  k  √   l≥0 jz 2k l =   The first few values are λ1 ≈ .3979226812, λ3 ≈ −.0210096400, λ5 ≈ .0013749841, λ7 ≈ −.0000960351. Wild conjecture: limk→∞ −λ2k+1 λ2k−1  = 1 π2. 26. The left side is 2S 1 x −5S 1 2x +2S 1 4x −2S x +5S 2x −2S 4x  by  39 ; the right side is S 2x −2S 4x +2S 1 x −S 1 2x −2S x +4S 2x −4S 1 2x +2S 1 4x  by  44 . The cases x = 1, x = 1  2, and x = ϕ are perhaps the most interesting; for example, x = ϕ gives 2G 4ϕ  − 5G 2ϕ  + G ϕ2 2  − G ϕ3  = 2G 2ϕ2 .  k≥0 2−2k2k−1 Bl2k n−l  n by exercise 1.2.11.2–4, when n > 1; and of course n  27. 2ψn = [zn] z  k≥1 2−k n+1 n−1  28. Letting Sn m  = m−1 k≥1 2−k l+1  = 1  2l+1 − 1 . 6.3–34 b , we find Sn m  = Tn m  + O e−n mn m2  and 2ψn+1 = τn + O n−3 , where τn =  3 2+i∞    k=1  1 − k m n and Tn m  = 1  en m − 1  as in exercise j≥1 2−2jSn 2j  = j≥1 2−2jTn 2j . Since τn+1 < τn and 4τ2n− τn = 1  en−1  it follows that τn = Θ n−2 . More detailed  3 2+i∞  is positive but exponentially small, information can be obtained by writing  k≥1 2−k n+1 2k−1  j=0 jn−1 =  ζ z Γ  z n−z  ζ z Γ  z n−z  j=0  l=0  1  l  2j 2−z   dz = 1 2πi  22−z−1  dz.  3 2−i∞  1 22j  en 2j−1  = 1 2πi  j≥1  j≥1  3 2−i∞   644  ANSWERS TO EXERCISES  4.5.2  f n  = 2  The integral is the sum of the residues at the poles 2 + 2πik  ln 2, namely n−2 times π2  6 ln 2  + f n , where  ℜ ζ 2 + 2πik  ln 2 Γ  2 + 2πik  ln 2  exp −2πik lg n   ln 2   k≥1  is a periodic function of lg n whose “average” value is zero.  29.  Solution by P. Flajolet and B. Vallée.  If f x  =   ∞0 g x xs−1 dx, then f∗ s  =   c+i∞  k≥1 2−kg 2kx  and g∗ s  = k≥1 2−k s+1 g∗ s  = g∗ s   2s+1 − 1 , and f x  = f∗ s x−s ds under appropriate conditions. Letting g x  = 1  1 + x , we find  1 2πi that the transform in this case is g∗ s  = π sin πs when 0 < ℜs < 1; hence  c−i∞  f x  = ∞  k=1  It follows that f x  is the sum of the residues of 1 + x lg x + 1  2 x + xP  lg x  − 2  7 x4 + ··· , where  1 2k  1  1 + 2kx  = 1 2πi  3 x3 − 8 ∞  1 x2 + 4 P  t  = 2π ln 2  m=1  sin 2πmt  sinh 2mπ2 ln 2    1 2+i∞  πx−s ds   2s+1 − 1  sin πs  .  1 2−i∞ sin πs x−s  2s+1−1  for ℜs ≤ 0, namely  π   −1 2+i∞  is a periodic function whose absolute value never exceeds 8 × 10−12.  The fact that P  t  is so small caused Brent to overlook it in his original paper.  The Mellin transform of f 1 x  is f∗ −s  = π   1−21−s  sin πs  for −1 < ℜs < 0; sin πs x−s ds  1 − 21−s , and we now want the residues of 7 x2 + ··· . [This formula could also have  thus f 1 x  = 1 2πi the integrand with ℜs ≤ −1: f 1 x  = 1 been obtained directly.] We have S1 x  = 1 − f x , and it follows that +  1 − x  G1 x  = f x  − f 1 x  = x lg x + 1  −1 2−i∞  2 ϕ x ,  π  where ϕ x  =∞k=0 −1 kxk  2k+1 − 1 .  30. We have G2 x  = Σ1 x  − Σ1 1 x  + Σ2 x  − Σ2 1 x , where  Σ1 x  =   1 2k+l  k,l≥1  The Mellin transforms are Σ∗1 s  = π where  1 + 2l 1 + 2kx  ,  sin πs a s   2s+1−1 , Σ∗2 s  = π  1 2k  1  k,l≥1  3 x − 1 2 x + xP  lg x  − x2 1 + x Σ2 x  =    s − 1 =   s − 1  2l + 1 s−1 = 2  − a′ 1 x ln 2 + xA lg x  − 2  − b′ 1 x ln 2 + xB lg x  −   1 + 2−l s−1  k≥0  k≥0  k≥2  22l  1  1  k  k  2k+2 − 1 ,  2k+1−s − 1 .  a s  = b s  =  l≥1  l≥1  Therefore we obtain the following expansions for 0 ≤ x ≤ 1: Σ1 x  = a 0  + a −1 x lg x+ 1  Σ2 x  = b 0  + b −1 x lg x+ 1  2k−1 2k−1−1 a −k  −x k, 2k−1 2k−1−1 b −k  −x k,  k≥2  1  .  1 + 2l + 2kx sin πs b s   2s+1−1 ,   4.5.2  ANSWERS TO EXERCISES  645  −a k  −x k 2k+1 − 1 ,  −x k 2k+1 − 1  Σ1 1 x  = Σ2 1 x  = s−2  k≥1  k≥1  ˆb s  =  k=0  A t  = 1 ln 2 B t  = 1 ln 2 Pk t  = 1 ln 2  k   s − 1     m≥1  m≥1  ℜ  ℜ  ℜ  m≥1  ,  1  1    2k+1 − 1 + Hk−1  lg x − ˆb k  − 1 2 −   ln 2 + Pk lg x     2k+1−s − 1 ; 2πi   sinh 2mπ2 ln 2  a −1 + 2mπi ln 2  e−2mπit  sinh 2mπ2 ln 2  b −1 + 2mπi ln 2  e−2mπit   k − 1 − 2mπi ln 2      2πi  2πi  ,  ,  e−2mπit  .  sinh 2mπ2 ln 2   k − 1  32. Yes: See G. Maze, J. Discrete Algorithms 5  2007 , 176–186. 34. Brigitte Vallée [Algorithmica 22  1998 , 660–685] has found an elegant and rig- orous analysis of Algorithm B, using an approach quite different from that of Brent. Indeed, her methods are sufficiently different that they are not yet known to predict the same behavior as Brent’s heuristic model. Thus the problem of analyzing the binary gcd algorithm, now solved rigorously for the first time, continues to lead to ever more tantalizing questions of higher mathematics. 35. By induction, the length is m+⌊n 2⌋+1−[m = n = 1] when m ≥ n. But exercise 37 shows that the algorithm cannot go as slowly as this. 36. Let an =  2n −  −1 n  3; then a0, a1, a2, . . . = 0, 1, 1, 3, 5, 11, 21, . . . .  This sequence of numbers has an interesting pattern of zeros and ones in its binary representation. Notice that an = an−1 + 2an−2, and an + an+1 = 2n.  For m > n, let u = 2m+1 − an+2, v = an+2. For m = n > 0, let u = an+2 and v = u +  −1 n. Another example for the case m = n > 0 is u = 2n+1 − 2, v = 2n+1 − 1; this choice takes more shifts, and gives B = 1, C = n + 1, D = 2n, E = n, the worst case for Program B. 37.  Solution by J. O. Shallit.  This is a problem where it appears to be necessary to prove more than was asked just to prove what was asked. Let S u, v  be the number of subtraction steps taken by Algorithm B on inputs u and v. We will prove that S u, v  ≤ lg u + v . This will imply that S u, v  ≤ ⌊lg u + v ⌋ ≤ ⌊lg 2 max u, v ⌋ = 1 + ⌊lg max u, v ⌋ as desired.  It follows, incidentally, that the smallest case requiring n subtraction steps is  Notice that S u, v  = S v, u . If u is even, S u, v  = S u 2, v ; hence we may assume that u and v are odd. We may also assume that u > v, since S u, u  = 1. Then S u, v  = 1 + S  u − v  2, v  ≤ 1 + lg  u − v  2 + v  = lg u + v  by induction. u = 2n−1 + 1, v = 2n−1 − 1. 38. Keep track of the most significant and least significant words of the operands  the most significant is used to guess the sign of t and the least significant is to determine the amount of right shift , while building a 2 × 2 matrix A of single-precision integers such  , where w is the computer word size and where u′ and v′ are smaller  that Au   =u′w  than u and v.  Instead of dividing the simulated even operand by 2, multiply the other one by 2, until obtaining multiples of w after exactly lg w shifts.  Experiments show  v′w  v   646  ANSWERS TO EXERCISES  4.5.2  this algorithm running four times as fast as Algorithm L, on at least one computer. With the similar algorithm of exercise 40 we don’t need the most significant words.  A possibly faster binary algorithm has been described by J. Sorenson, J. Algo- rithms 16  1994 , 110–144; Shallit and Sorenson, Lecture Notes in Comp. Sci. 877  1994 , 169–183. 39.  Solution by Michael Penk.  Assume that u and v are positive.  Y1. [Find power of 2.] Same as step B1. Y2. [Initialize.] Set  u1, u2, u3  ←  1, 0, u  and  v1, v2, v3  ←  v, 1 − u, v . If u is odd, set  t1, t2, t3  ←  0,−1,−v  and go to Y4. Otherwise set  t1, t2, t3  ←  1, 0, u .  Y3. [Halve t3.] If t1 and t2 are both even, set  t1, t2, t3  ←  t1, t2, t3  2; otherwise set  t1, t2, t3  ←  t1 + v, t2 − u, t3  2.  In the latter case, t1 + v and t2 − u will both be even.   Y4. [Is t3 even?] If t3 is even, go back to Y3. Y5. [Reset max u3, v3 .] If t3 is positive, set  u1, u2, u3  ←  t1, t2, t3 ; otherwise  set  v1, v2, v3  ←  v − t1,−u − t2,−t3 .  Y6. [Subtract.] Set  t1, t2, t3  ←  u1, u2, u3  −  v1, v2, v3 . Then if t1 ≤ 0, set  t1, t2  ←  t1 + v, t2 − u . If t3 ̸= 0, go back to Y3. Otherwise the algorithm terminates with  u1, u2, u3 · 2k  as the output.  It is clear that the relations in  16  are preserved, and that 0 ≤ u1, v1, t1 ≤ v, 0 ≥ u2, v2, t2 ≥ −u, 0 < u3 ≤ u, 0 < v3 ≤ v after each of steps Y2–Y6. If u is odd after step Y1, then step Y3 can be simplified, since t1 and t2 are both even if and only if t2 is even; similarly, if v is odd, then t1 and t2 are both even if and only if t1 is even. Thus, as in Algorithm X, it is possible to suppress all calculations involving u2, v2, and t2, provided that v is odd after step Y1. This condition is often known in advance  for example, it holds when v is prime and we are trying to compute u−1 modulo v .  See also A. W. Bojanczyk and R. P. Brent, Computers and Math. 14  1987 , 233,  for a similar extension of the algorithm in exercise 40. 40. Let m = lg max u,v . We can show inductively that u ≤ 2m− s−c  2, v ≤ 2m− s+c  2 after we have performed the operation c ← c + 1 in step K3 s times. Therefore s ≤ 2m. If K2 is executed t times, we have t ≤ s + 2, because s increases every time except the first and last. [See VLSI ’83  North-Holland, 1983 , 145–154.] Notes: When u = 1 and v = 3 · 2k − 1 and k ≥ 2, we have m = k + 2, s = 2k, t = k + 4. When u = uj and v = 2uj−1 in the sequence defined by u0 = 3, u1 = 1, uj+1 = min 3uj − 16uj−1, 5uj − 16uj−1 , we have s = 2j + 2, t = 2j + 3, and  empirically  m ≈ ϕj. Can t be asymptotically larger than 2m ϕ? 41. In general, since  au − 1  mod  av − 1  = au mod v − 1  see Eq. 4.3.2– 20  , we find that gcd am − 1, an − 1  = agcd m,n  − 1 for all positive integers a. 42. Subtract the kth column from the 2kth, 3kth, 4kth, etc., for k = 1, 2, 3, . . . . The d\m xd.  [In general, “Smith’s determinant,” in which the  i, j  element is f gcd i, j   for d\m µ m d f d , by the same argument. See L. E. Dickson, History of the Theory of Numbers 1  Carnegie Inst. of Washington, 1919 , 122–123.]  result is a triangular matrix with xk on the diagonal in column k, where m = an arbitrary function f, is equal to n  It follows that xm = φ m , so the determinant is φ 1 φ 2  . . . φ n .  m=1   4.5.3  ANSWERS TO EXERCISES  647  .    Kn−1 x2, . . . , xn−1, xn   Kn−1 x1, x2, . . . , xn−1  Kn−2 x2, . . . , xn−1    Kn x1, x2, . . . , xn−1, xn   SECTION 4.5.3 1. The running time is about 19.02T + 6, just a trifle slower than Program 4.5.2A. 2. 3. Kn x1, . . . , xn . 4. By induction, or by taking the determinant of the matrix product in exercise 2. 5. When the x’s are positive, the q’s of  9  are positive, and qn+1 > qn−1; hence  9  is an alternating series of decreasing terms, and it converges if and only if qnqn+1 → ∞. By induction, if the x’s are greater than ϵ, we have qn ≥  1 + ϵ 2 nc, where c is chosen small enough to make this inequality valid for n = 1 and 2. But if xn = 1 2n, we have qn ≤ 2 − 1 2n. 6. It suffices to prove that A1 = B1; and from the fact that 0 ≤   x1, . . . , xn   < 1 whenever x1, . . . , xn are positive integers, we have B1 = ⌊1 X⌋ = A1. 7. Only 1 2 . . . n and n . . . 2 1.  The variable xk appears in exactly Fk Fn+1−k terms; hence x1 and xn can only be permuted into x1 and xn. If x1 and xn are fixed by the permutation, it follows by induction that x2, . . . , xn−1 are also fixed.  8. This is equivalent to  Kn−2 An−1, . . . , A2  − XKn−1 An−1, . . . , A1   Kn−1 An, . . . , A2  − XKn An, . . . , A1   = − 1 Xn  ,  and by  6  it is equivalent to  X = Kn−1 A2, . . . , An  + XnKn−2 A2, . . . , An−1  Kn A1, . . . , An  + XnKn−1 A1, . . . , An−1  .  9.  a  By definition.  b, d  Prove this when n = 1, then apply  a  to get the result for general n.  c  Prove it when n = k + 1, then apply  a . 10. If A0 > 0, then B0 = 0, B1 = A0, B2 = A1, B3 = A2, B4 = A3, B5 = A4, m = 5. If A0 = 0, then B0 = A1, B1 = A2, B2 = A3, B3 = A4, m = 3. If A0 = −1 and A1 = 1, then B0 = − A2 + 2 , B1 = 1, B2 = A3 − 1, B3 = A4, m = 3. If A0 = −1 and A1 > 1, then B0 = −2, B1 = 1, B2 = A1 − 2, B3 = A2, B4 = A3, B5 = A4, m = 5. If A0 < −1, then B0 = −1, B1 = 1, B2 = −A0 − 2, B3 = 1, B4 = A1 − 1, B5 = A2, B6 = A3, B7 = A4, m = 7. [Actually, the last three cases involve eight subcases; if any of the B’s is set to zero, the values should be “collapsed together” by using the rule of exercise 9 c . For example, if A0 = −1 and A1 = A3 = 1, we actually have B0 = − A2 + 2 , B1 = A4 + 1, m = 1. Double collapsing occurs when A0 = −2 and A1 = 1.] 11. Let qn = Kn A1, . . . , An , q′n = Kn B1, . . . , Bn , pn = Kn+1 A0, . . . , An , p′n = Kn+1 B0, . . . , Bn . By  5  and  11  we have X =  pm + pm−1Xm   qm + qm−1Xm , Y =  p′n + p′n−1Yn   q′n + q′n−1Yn ; therefore if Xm = Yn, the stated relation between X and Y holds by  8 . Conversely, if X =  qY + r   sY + t  and qt − rs = 1, we may assume that s ≥ 0, and we can show that the partial quotients of X and Y eventually agree, by induction on s. The result is clear when s = 0, by exercise 9 d . If s > 0, let q = as + s′, where 0 ≤ s′ < s. Then X = a + 1   sY + t   s′Y + r − at  ; since s r − at  − ts′ = sr − tq, and s′ < s, we know by induction and exercise 10 that the partial quotients of X and Y eventually agree. [J. de Math. Pures et Appl. 15  1850 , 153–155. The fact that m is always odd in exercise 10 shows, by a close inspection of this proof, that Xm = Yn if and only if X =  qY + r   sY + t , where qt − rs =  −1 m−n.]   ANSWERS TO EXERCISES  648 12.  a  Since VnVn+1 = D − U 2 n+1 is a multiple of Vn+1; hence √ D−Un  Vn, where Un and Vn are integers. [Notes: An algorithm by induction Xn =   based on this process has many applications to the solution of quadratic equations in integers; see, for example, H. Davenport, The Higher Arithmetic  London: Hutchinson, 1952 ; W. J. LeVeque, Topics in Number Theory  Reading, Mass.: Addison–Wesley, 1956 ; and see also Section 4.5.4. By exercise 1.2.4–35, we have  n, we know that D − U 2  4.5.3  An+1 =  D⌋ + Un  Vn+1⌋, D⌋ + 1 + Un  Vn+1⌋,  if Vn+1 > 0, if Vn+1 < 0; hence such an algorithm need only work with the positive integer ⌊√ D⌋. Moreover, the identity Vn+1 = An Un−1 − Un  + Vn−1 makes it unnecessary to divide when Vn+1 is being determined.] D− U  V, Yn =  −√  b  Let Y =  −√ D− Un  Vn. The stated identity obviously √ D by −√  holds by replacing  D in the proof of  a . We have Y =  pn Yn + pn−1   qn Yn + qn−1 , where pn and qn are defined in part  c  of this exercise; hence  ⌊ ⌊√ ⌊ ⌊√  Yn =  −qn qn−1  Y − pn qn   Y − pn−1 qn−1 .  √  √  √ Finally, we want to show that Un > 0. Since Xn    But by  12 , pn−1 qn−1 and pn qn are extremely close to X; since X ̸= Y, Y − pn qn and Y − pn−1 qn−1 will have the same sign as Y − X for all large n. This proves that Yn < 0 for all large n; hence 0 < Xn < Xn − Yn = 2 D Vn; Vn must be positive. Also √ D, since Vn ≤ AnVn < D, since Xn > 0. Hence Vn < 2 Un < D − Vn, so √ D. Then Un = AnVn − Un−1 ≥ Vn − Un−1 > we need only consider the case Vn > √ D − Un−1, and this is positive as we have already observed. √ √ D − Un−1  > Vn; hence D + Un = AnVn +   Notes: In the repeating cycle, √ √ √ ⌊  D + Un  Vn+1⌋. D + Un+1  Vn+1⌋ = ⌊An+1 + Vn   In other words An+1 is determined by Un+1 and Vn+1; we can determine  Un, Vn  √ D + Un from its successor  Un+1, Vn+1  in the period. √ √ and 0 < Un < D + Un+1 and √ 0 < Un+1 < D ; moreover, if the pair  Un+1, Vn+1  follows  U′, V ′  with 0 < V ′ < √ √ D, then U′ = Un and V ′ = Vn. Hence  Un, Vn  is part of the cycle if and only if 0 < Vn <  D, the arguments above prove that 0 < Vn+1 <  D + Un ⌋ = An+1 = ⌊   In fact, when 0 < Vn <  D + U′ and 0 < U′ <  D + Un−1. √  √ D.   c  −Vn+1  Vn  = XnYn =  √ D + Un and 0 < Un <  qnX − pn  qnY − pn    qn−1X − pn−1  qn−1Y − pn−1  .  There is also a companion identity, namely  V pnpn−1 + U pnqn−1 + pn−1qn  +   U  2 − D  V  qnqn−1 =  −1 nUn.   d  If Xn = Xm for some n ̸= m, then X is an irrational number that satisfies the quadratic equation  qnX − pn   qn−1X − pn−1  =  qmX − pm   qm−1X − pm−1 . The ideas underlying this exercise go back at least to Jayadeva in India, prior to A.D. 1073; see K. S. Shukla, Gan. ita 5  1954 , 1–20; C.-O. Selenius, Historia Math. 2  1975 , 167–184. Some of its aspects had also been discovered in Japan before 1750; see Y. Mikami, The Development of Mathematics in China and Japan  1913 , 223–229. But the main principles of the theory of continued fractions for quadratics are largely   4.5.3  ANSWERS TO EXERCISES  649  due to Euler [Novi Comment. Acad. Sci. Petrop. 11  1765 , 28–66] and Lagrange [Hist. Acad. Sci. 24  Berlin: 1768 , 111–180]. 14. As in exercise 9, we need only verify the stated identities when c is the last partial quotient, and this verification is trivial. Now Hurwitz’s rule gives 2 e =   1, 2, 1, 2, 0, 1, 1, 1, 1, 1, 0, 2, 3, 2, 0, 1, 1, 3, 1, 1, 0, 2, 5, . . .   . Taking the reciprocal, col- lapsing out the zeros as in exercise 9, and taking note of the pattern that appears, we find  see exercise 16  that e 2 = 1 +    2, 2m + 1, 3, 1, 2m + 1, 1, 3  , m ≥ 0. [Schriften der phys.-ökon. Gesellschaft zu Königsberg 32  1891 , 59–62. Hurwitz also explained how to multiply by an arbitrary positive integer, in Vierteljahrsschrift der Naturforschenden Gesellschaft in Zürich 41  1896 , Jubelband II, 34–64, §2.] 15.  This procedure maintains four integers  A, B, C, D  with the invariant meaning that “our remaining job is to output the continued fraction for  Ay+B   Cy+D , where y is the input yet to come.”  Initially set j ← k ← 0,  A, B, C, D  ←  a, b, c, d ; then input xj and set  A, B, C, D  ←  Axj +B, A, Cxj +D, C , j ← j+1, one or more times until C + D has the same sign as C.  When j ≥ 1 and the input has not terminated, we know that 1 < y < ∞; and when C + D has the same sign as C we know therefore that  Ay + B   Cy + D  lies between  A + B   C + D  and A C.  Now comes the general step: If no integer lies strictly between  A+B   C+D  and A C, output Xk ← min ⌊A C⌋, ⌊ A + B   C + D ⌋ , and set  A, B, C, D  ←  C, D, A− XkC, B − XkD , k ← k + 1; otherwise input xj and set  A, B, C, D  ←  Axj + B, A, Cxj + D, C , j ← j + 1. The general step is repeated ad infinitum. However, if at any time the final xj is input, the algorithm immediately switches gears: It outputs the continued fraction for  Axj + B   Cxj + D , using Euclid’s algorithm, and terminates.  The following tableau solves the requested example, where the matrix   B begins at the upper left corner, then shifts right one on input, down one on output:  C    D  A  5  −1 1 97 −58 −193 123 53 17 2 3  37 16 5 1  xj 39  Xk −2 −25 −62 2 3 7 1 1 1 12 2 ∞  1  22 3 1 2  1  5 4 1  2  5 3 2 1  1  2  ∞  14 7 7 0  9 1  25 2 1 0  M. Mendès France has shown that the number of quotients output per quotient input is asymptotically bounded between 1 r and r, where r = 2⌊L ad − bc  2⌋ + 1 and L is the function defined in exercise 38; this bound is best possible. [Topics in Number Theory, edited by P. Turán, Colloquia Math. Soc. János Bolyai 13  1976 , 183–194.] Gosper has also shown that the algorithm above can be generalized to compute the continued fraction for  axy + bx + cy + d   Axy + Bx + Cy + D  from those of x and y  in particular, to compute sums and products . [MIT AI Laboratory Memo 239  29 February 1972 , Hack 101.] For further developments, see J. Vuillemin, ACM Conf. LISP and Functional Programming 5  1988 , 14–27.   650  ANSWERS TO EXERCISES  4.5.3  16. It is not difficult to prove by induction that fn z  = z  2n + 1  + O z3  is an odd function with a convergent power series in a neighborhood of the origin, and that it satisfies the given differential equation. Hence  f0 z  =   z−1 + f1 z    = ··· =   z−1  , 3z−1  , . . . ,  2n + 1 z−1 + fn+1 z   .    It remains to prove that limn→∞   z−1, 3z−1, . . . ,  2n + 1 z−1   = f0 z . [Actually Euler, age 24, obtained continued fraction expansions for the considerably more general differential equation f′n z  = azm + bfn z zm−1 + cfn z 2; but he did not bother to prove convergence, since formal manipulation and intuition were good enough in the eighteenth century.]  There are several ways to prove the desired limiting equation. First, letting fn z  =  k ankzk, we can argue from the equation 2 +  2n + 5 an5z  4 + ··· = 1 −  an1z + an3z  5 + ···  2  2n + 1 an1 +  2n + 3 an3z that  −1 kan 2k+1  is a sum of terms of the form ck  2n+1 k+1 2n+ bk1  . . .  2n+ bkk , where the ck and bkm are positive integers independent of n. For example, we have −an7 = 4  2n + 1 4 2n + 3  2n + 5  2n + 7  + 1  2n + 1 4 2n + 3 2 2n + 7 . Thus a n+1 k ≤ ank, and fn z  ≤ tan z for z < π 2. This uniform bound on fn z  makes the convergence proof very simple. Careful study of this argument reveals that the power series for fn z  actually converges for z < π 2n + 1 2; therefore the singularities of fn z  get farther and farther away from the origin as n grows, and the continued fraction actually represents tanh z throughout the complex plane.  3 + an5z  √  Another proof gives further information of a different kind: If we let   n + k ! zn−k k!  n − k ! = zn 2F0 n + 1,−n; ;−1 z ,   zk k! =  k≥0  An z  = n!  then  n  k=0  n  2n − k An+1 z  = 1 3  k≥0  Kn  z  ,  =  4n + 2 An z  + z  2  It follows, by induction, that 3 z  , . . . ,     2n − 1  2n − 1  z  z  Kn−1  , . . . ,  z    z−1  , 3z−1  , . . . ,  2n − 1 z−1  .  ,  2n+1zn  2n+1zn  = An 2z  + An −2z  = An 2z  − An −2z     = An 2z  − An −2z  An 2z  + An −2z  , 2n − m   =    −1 k     2n+1  k≥0  zm n! m! .  n  m≥0  n + k ! zk   2n + k + 1 ! k! .  and we want to show that this ratio approaches tanh z. By Equations 1.2.9– 11  and 1.2.6– 24 ,  ezAn −z  = n!  zm m!  m≥0   n   m  2n − k  k  k=0  n  ezAn −z  − An z  = Rn z  =  −1 nz  Hence  Hence   n + k − 1 !   4n + 2 k +  n + 1 − k  n − k    zn+1−k  k!  n + 1 − k ! An−1 z .   4.5.3  ANSWERS TO EXERCISES  651  1 2An 2z  + An −2z  ≥ n!  We now have  e2z − 1  An 2z  + An −2z   −  e2z + 1  An 2z  − An −2z   = 2Rn 2z ; hence  tanh z −   z−1  , 3z−1  , . . . ,  2n − 1 z−1     =  2Rn 2z    An 2z  + An −2z   e2z + 1  ,  and we have an exact formula for the difference. When 2z ≤ 1, the factor e2z + 1 is bounded away from zero, Rn 2z  ≤ e n!  2n + 1 !, and  2n   n   −2n − 2 16 − 1 4 − 1   −2n − 4 64 − ···  n  n   −2n − 6   − ···    n  1 − 1  ≥  2n ! n!  = 2 3 Thus convergence is very rapid, even for complex values of z. To go from this continued fraction to the continued fraction for ez, we have tanh z = 1 − 2  e2z + 1 ; hence we get the continued-fraction representation for  e2z + 1  2 by simple manipulations. Hurwitz’s rule gives the expansion of e2z + 1, from which we may subtract unity. For n odd,   2n ! n!  .  m ≥ 0.  e−2 n =    1, 3mn + ⌊n 2⌋,  12m + 6 n,  3m + 2 n + ⌊n 2⌋, 1  , Another derivation has been given by C. S. Davis, J. London Math. Soc. 20  1945 , 194–198. The continued fraction for e was first found empirically by Roger Cotes, Philosophical Transactions 29  1714 , 5–45, Proposition 1, Scholium 3. Euler com- municated his results in a letter to Goldbach on November 25, 1731 [Correspondance Mathématique et Physique, edited by P. H. Fuss, 1  St. Petersburg: 1843 , 56–60], and he eventually published fuller descriptions in Commentarii Acad. Sci. Petropolitanæ 9  1737 , 98–137; 11  1739 , 32–81. 17.  b    x1 − 1, 1, x2 − 2, 1, x3 − 2, 1, . . . , 1, x2n−1 − 2, 1, x2n − 1  . [Note: One can remove negative parameters from continuants by using the identity Km+n+1 x1, . . . , xm,−x, yn, . . . , y1   =  −1 n−1  Km+n+2 x1, . . . , xm−1, xm − 1, 1, x − 1,−yn, . . . ,−y1 ,  from which we obtain  Km+n+1 x1, . . . , xm,−x, yn, . . . , y1   = −Km+n+3 x1, . . . , xm−1, xm − 1, 1, x − 2, 1, yn − 1, yn−1, . . . , y1   after a second application. A similar identity appears in exercise 41.]   c  1 +   1, 1, 3, 1, 5, 1, . . .    = 1 +   2m + 1, 1  , m ≥ 0.  18. Since we have Km a1, a2, . . . , am    a1, a2, . . . , am, x   = Km−1 a2, . . . , am  +  −1 m  Km−1 a1, . . . , am−1  + Km a1, a2, . . . , am x  by Eqs.  5  and  8 , we also have Km a1, a2, . . . , am    a1, a2, . . . , am, x1, a1, a2, . . . , am, x2, a1, a2, . . . , am, x3, a1, . . .    = Km−1 a2, . . . , am  +    −1 m C + Ax1 , C + Ax2,  −1 m C + Ax3 , . . .   , where A = Km a1, a2, . . . , am  and C = Km−1 a2, . . . , am  + Km−1 a1, . . . , am−1 . Consequently the stated difference is  Km−1 a2, . . . , am −Km−1 a1, . . . , am−1   Km a1, a2, . . . , am , by  6 . [The case m = 2 was discussed by Euler in Commentarii Acad. Sci. Petropoli- tanæ 9  1737 , 98–137, §24–26.] 19. The sum for 1 ≤ k ≤ N is logb  1 + x  N + 1   N + 1 + x  .   652  ANSWERS TO EXERCISES  4.5.3  20. Let H = SG, g x  =  1 + x G′ x , h x  =  1 + x H′ x . Then  37  implies that h x + 1   x + 2  − h x   x + 1  = − 1 + x −2g 1  1 + x    1 + 1  1 + x  . 21. φ x  = c  cx + 1 2 +  2− c    c− 1 x + 1 2, U φ x  = 1  x + c 2. When c ≤ 1, the minimum of φ x  U φ x  occurs at x = 0 and is 2c2 ≤ 2. When c ≥ ϕ, the minimum occurs at x = 1 and is ≤ ϕ2. When c ≈ 1.31266 the values at x = 0 and x = 1 are nearly equal and the minimum is > 3.2; the bounds  0.29 nφ ≤ U nφ ≤  0.31 nφ are obtained. Still better bounds come from well-chosen linear combinations of the form  T g x  = aj  x + cj .  k≥1 Ik \  1≤j<k Ij , and this is a disjoint union in which Ik \   k≥1 Ik = can be expressed as a finite union of disjoint intervals. Therefore we may take I = Ik,  23. By the interpolation formula of exercise 4.6.4–15 with x0 = 0, x1 = x, x2 = x + ϵ, letting ϵ → 0, we have the general identity R′n x  =  Rn x − Rn 0   x + 1 2 xR′′n θn x   for some θn x  between 0 and x, whenever Rn is a function with continuous second derivative. Hence in this case R′n x  = O 2−n . 24. ∞. [A. Khinchin, in Compos. Math. 1  1935 , 361–382, proved that the sum A1 +··· + An of the first n partial quotients of a real number X will be asymptotically n lg n, for almost all X. Exercise 35 shows that the behavior is different for rational X.] 25. Any union of intervals can be written as a union of disjoint intervals, since we have 1≤j<k Ij where Ik is an interval of length ϵ 2k containing the kth rational number in [0 . . 1], using some enumeration of the rationals. In this case µ I  ≤ ϵ, but I ∩ Pn = n for all n. 26. The continued fractions   A1, . . . , At   that appear are precisely those for which A1 > 1, At > 1, and Kt A1, A2, . . . , At  is a divisor of n. Therefore  6  completes the proof. [Note: If m1 n =   A1, . . . , At   and m2 n =   At, . . . , A1  , where m1 and m2 are relatively prime to n, then m1m2 ≡ ±1  modulo n ; this rule defines the correspondence. When A1 = 1 an analogous symmetry is valid, according to  46 .] 27. First prove the result for n = pe, then for n = rs, where r and s are relatively prime. Alternatively, use the formulas in the next exercise. 28.  a  The left-hand side is multiplicative  see exercise 1.2.4–31 , and it is easily evaluated when n is a power of a prime.  c  From  a , we have Möbius’s inversion  formula: If f n  = 29. We have N exercise 1.2.11.2–7 . AlsoN O N  d\n g d , then g n  = d=1 Λ d N 2 d2  = O N 2 . Indeed,  d\n Λ d  d =N d≥1 Λ d  d2 = −ζ′ 2  ζ 2 .  2 N 2 ln N + O N 2  by Euler’s summation formula  see 1≤k≤N d k, and this is 30. The modified algorithm affects the calculation if and only if the following division step in the unmodified algorithm would have the quotient 1, and in this case it avoids the following division step. The probability that a given division step is avoided is the probability that Ak = 1 and that this quotient is preceded by an even number of quotients equal to 1. By the symmetry condition, this is the probability that Ak = 1 and is followed by an even number of quotients equal to 1. The latter happens if and only if Xk−1 > ϕ−1 = 0.618 . . . , where ϕ is the golden ratio: For Ak = 1 and Ak+1 > 1 8 ≤ if and only if 2 3; etc. Thus we save approximately Fk−1 1  − Fk−1 ϕ − 1  ≈ 1 − lg ϕ ≈ 0.306 Xk−1 < 2 of the division steps. The average number of steps is approximately   12 ln ϕ  π2  ln n, when v = n and u is relatively prime to n.  3 ≤ Xk−1   1 if and only if 5  d\n µ n d f d . d=1 Λ d   n=1 n  n=1 n ln n = 1   4.5.3  ANSWERS TO EXERCISES  653  K. Vahlen [Crelle 115  1895 , 221–233] considered all algorithms that replace  u, v  by  v,  ±u  mod v  at each iteration when u mod v ̸= 0. If u ⊥ v there are exactly v such algorithms, and they can be represented as a binary tree with v leaves. The shallowest leaves, which correspond to the shortest possible number of iterations over all such gcd algorithms, occur when the least remainder is taken at each step; the deepest leaves occur when the greatest remainder is always chosen. [Similar ideas had been considered by Lagrange in Hist. Acad. Sci. 23  Berlin: 1768 , 111–180, §58.] For further results see N. G. de Bruijn and W. M. Zaring, Nieuw Archief voor Wiskunde  3  1  1953 , 105–112; G. J. Rieger, Math. Nachr. 82  1978 , 157–180.  On many computers, the modified algorithm makes each division step longer; the idea of exercise 1, which saves all division steps when the quotient is unity, would be preferable in such cases. √ 31. Let a0 = 0, a1 = 1, an+1 = 2an + an−1; then an =   1 + 2, and the worst case  in the sense of Theorem F  occurs when u = an + an−1, v = an, n ≥ 2. This result is due to A. Dupré [J. de Math. 11  1846 , 41–64], who also investigated more general “look-ahead” procedures suggested by J. Binet. 32.  b  Km−1 x1, . . . , xm−1 Kn−1 xm+2, . . . , xm+n  corresponds to those Morse code sequences of length m + n in which a dash occupies positions m and m + 1; the other term corresponds to the opposite case.  Alternatively, use exercise 2. The more general identity  √ 2 n −  1 − √  2 n  2  Km+n x1, . . . , xm+n Kk xm+1, . . . , xm+k  =  Km+k x1, . . . , xm+k Kn xm+1, . . . , xm+n   +  −1 kKm−1 x1, . . . , xm−1 Kn−k−1 xm+k+2, . . . , xm+n  also appeared in Euler’s paper. Incidentally, “Morse code” was really invented by F. C. Gerke in 1848; Morse’s prototypes were quite different.  33.  a  The new representations are x = m d, y =  n − m  d, x′ = y′ = d = 2 n < m < n.  b  The relation  n x′  − y ≤ x < n x′ defines x. gcd m, n − m , for 1  c  Count the x′ satisfying  b .  d  A pair of integers x > y > 0 with x ⊥ y can  be uniquely written in the form x = Km x1, . . . , xm , y = Km−1 x1, . . . , xm−1 , where x1 ≥ 2 and m ≥ 1; here y x =   xm, . . . , x1  .  e  It suffices to show that 34.  a  Dividing x and y by gcd x, y  yields g n  = 1≤k≤n 2 T  k, n  = 2⌊n 2⌋ + h n ; this follows from exercise 26. d\n h n d ; apply exercise 28 c , representations. Now sum for 0 < t ≤ y <n d. and use the symmetry between primed and unprimed variables.  b  For fixed y and t, √ the representations with xd ≥ x′ have x′ < d\y s d  = y H2y − Hy  = k y , say; hence s y  =   nd y  such d\y µ d  yd =  y=1 φ y  y2 = n y ln 2 − 1 4 + O 1 y .  d  n   c  If s y  is the given sum, then d\y µ d k y d . Now k y  = y=1 σ−1 y  y2 = O 1 .   e n  Similarly,n cd≤n µ d  cd2. 4.5.2–10 d  ; and n k=1 µ k  k2 = 6 π2 + O 1 n   see exercise O n  for d ≥ 1. Finally h n  = 2 cd\n µ d hc n cd  =   6 ln 2  π2 n ln n−−′ + k=1 µ k  log k k2 = O 1 . Hence hd n  = n  3 ln 2  π2  ln n d  + O nσ−1 n 2 , where the remaining sums are = cd\n µ d  ln cd  cd = 0 and′ =  cd\n µ d  ln c cd =  d\n Λ d  d.  [It is well known that σ−1 n  = O log log n ; see Hardy and Wright, An Introduction to the Theory of Numbers, §22.9.] 35. See Proc. Nat. Acad. Sci. 72  1975 , 4720–4722. M. L. V. Pitteway and C. M. A. Castle [Bull. Inst. Math. and Its Applications 24  1988 , 17–20] have found strong and  nd; hence there are O   √  y=1   654  ANSWERS TO EXERCISES  4.5.3  tantalizing empirical evidence that the sum of all partial quotients is actually    π2  24 ln 2 2  Tn + 1  2 − 18 ln 2 2  π2  + 6 π2  2  4r pr − p + 1  p2r    pr − 1 p − 1   ln p 2    p prime pr\n  − 2.542875 + O n−1 2 .  36. Working the algorithm backwards, assuming that tk − 1 divisions occur in step C2 for a given value of k, we obtain minimum un when gcd uk+1, . . . , un  = Ft1 . . . Ftk and uk ≡ Ft1 . . . Ftk−1 Ftk−1  modulo gcd uk+1, . . . , un  ; here the t’s are ≥ 2, t1 ≥ 3, and t1 + ··· + tn−1 = N + n − 1. One way to minimize un = Ft1 . . . Ftn−1 under these conditions is to take t1 = 3, t2 = ··· = tn−2 = 2, un = 2FN−n+2. If we stipulate also that u1 ≥ u2 ≥ ··· ≥ un, the solution u1 = 2FN−n+3 + 1, u2 = ··· = un−1 = 2FN−n+3, un = 2FN−n+2 has minimum u1. [See CACM 13  1970 , 433–436, 447–448.] 37. See Proc. Amer. Math. Soc. 7  1956 , 1014–1021; see also exercise 6.1–18. 38. Let m = ⌈n ϕ⌉, so that m n = ϕ−1 + ϵ =   a1, a2, . . .    where 0 < ϵ < 1 n. Let k be minimal such that ak ≥ 2; then  ϕ1−k +  −1 kFk−1ϵ   ϕ−k −  −1 kFkϵ  ≥ 2, hence √ k is even and ϕ−2 = 2 − ϕ ≤ ϕkFk+2 ϵ =  ϕ2k+2 − ϕ−2 ϵ  [Ann. Polon. Math. 1  1954 , 203–206.] 39. At least 287 at bats;   2, 1, 95   = 96 287 ≈ .33449477, and no fraction with denominator < 287 lies in the interval  5.  [.3335 . . .3345] = [  2, 1, 666   . .   2, 1, 94, 1, 1, 3  ].  To solve the general question of the fraction in [a . . b] with smallest denominator, where 0 < a < b < 1, note that in terms of regular continued-fraction representations we have   x1, x2, . . .    <   y1, y2, . . .    if and only if  −1 jxj <  −1 jyj for the smallest j with xj ̸= yj, where we place “∞” after the last partial quotient of a rational number. Thus if a =   x1, x2, . . .    and b =   y1, y2, . . .   , and if j is minimal with xj ̸= yj, the fractions in [a . . b] have the form c =   x1, . . . , xj−1, zj, . . . , zm   where   zj, . . . , zm   lies between   xj, xj+1, . . .    and   yj, yj+1, . . .    inclusive. Let K−1 = 0. The denominator  Kj−1 x1, . . . , xj−1 Km−j+1 zj, . . . , zm  + Kj−2 x1, . . . , xj−2 Km−j zj+1, . . . , zm   of c is minimized when m = j and zj =  j odd ⇒ yj + [yj+1 ̸=∞]; xj + [xj+1 ̸=∞] . [Another way to derive this method comes from the theory in the following exercise.] 40. One can prove by induction that prql − plqr = 1 at each node, hence pl and ql are relatively prime. Since p q < p′ q′ implies that p q <  p + p′   q + q′  < p′ q′, it is also clear that the labels on all left descendants of p q are less than p q, while the labels on all its right descendants are greater. Therefore each rational number occurs at most once as a label.  It remains to show that each rational does appear. If p q =   a1, . . . , ar, 1  , where each ai is a positive integer, one can show by induction that the node labeled p q is found by going left a1 times, then right a2 times, then left a3 times, etc.  [The sequence of labels on successive levels of this tree was first studied by M. A. Stern, Crelle 55  1858 , 193–220, although the relation to binary trees is not explicit in his paper. The notion of obtaining all possible fractions by successively interpolating  p + p′   q + q′  between adjacent elements p q and p′ q′ goes back much further: The essential ideas were published by Daniel Schwenter [Deliciæ Physico-Mathematicæ   4.5.3  ANSWERS TO EXERCISES  655   Nürnberg: 1636 , Part 1, Problem 87; Geometria Practica, 3rd edition  1641 , 68; see M. Cantor, Geschichte der Math. 2  1900 , 763–765], and by John Wallis in his Treatise of Algebra  1685 , Chapters 10–11. C. Huygens put such ideas to good use when designing the gear-wheels of his planetarium [see Descriptio Automati Planetarii  1703 , published after his death]. Lagrange gave a full description in Hist. Acad. Sci. 23  Berlin: 1767 , 311–352, §24, and in his additions to the French translation of Euler’s algebra  1774 , §18–§20. See also exercise 1.3.2–19; A. Brocot, Revue Chronométrique 3  1861 , 186–194; D. H. Lehmer, AMM 36  1929 , 59–67.] 41. In fact, the regular continued fractions for numbers of the general form  1 l1  +  −1 e1  l2 1l2  +  −1 e2 l4 1l2 2l3  + ···  have an interesting pattern, based on the continuant identity Km+n+1 x1, . . . , xm−1, xm − 1, 1, yn − 1, yn−1, . . . , y1  =  xmKm−1 x1, . . . , xm−1 Kn yn, . . . , y1   +  −1 nKm+n x1, . . . , xm−1, 0,−yn,−yn−1, . . . ,−y1 .  This identity is most interesting when yn = xm−1, yn−1 = xm−2, etc., since  Kn+1 z1, . . . , zk, 0, zk+1, . . . , zn  = Kn−1 z1, . . . , zk−1, zk + zk+1, zk+2, . . . , zn .  In particular we find that if pn qn = Kn−1 x2, . . . , xn  Kn x1, . . . , xn  =   x1, . . . , xn  , then pn qn +  −1 n q2 nr =   x1, . . . , xn, r − 1, 1, xn − 1, xn−1, . . . , x1  . By changing   x1, . . . , xn   to   x1, . . . , xn−1, xn − 1, 1  , we can control the sign  −1 n as desired. For example, the partial sums of the first series have the following continued frac- tions of even length:   1, 1  ;   1, 1, 1, 1, 0, 1   =   1, 1, 1, 2  ;   1, 1, 1, 2, 1, 1, 1, 1, 1, 1  ;   1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1   =   1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1  ; and from this point on the sequence settles down and obeys a simple reflecting pattern. We find that the nth partial quotient an can be computed rapidly as follows, if n − 1 = 20q + r where 0 ≤ r < 20:    an =  1, 2, 1 +  q + r  mod 2, 2 − dq, 1 + dq+1,  if r = 0, 2, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 17, or 19; if r = 3 or 16; if r = 8 or 11; if r = 1; if r = 18.  Here dn is the “dragon sequence” defined by the rules d0 = 1, d2n = dn, d4n+1 = 0, n   is 1 − 2dn. The dragon curve discussed in exercise d4n+3 = 1; the Jacobi symbol   −1 4.1–18 turns right at its nth step if and only if dn = 1. Liouville’s numbers with l ≥ 3 are equal to   l−1, l +1, l2 −1, 1, l, l−1, l12 −1, 1, l − 2, l, 1, l2 − 1, l + 1, l − 1, l72 − 1, . . .   . The nth partial quotient an depends on the dragon sequence on n mod 4 as follows: If n mod 4 = 1 it is l−2+dn−1 + ⌊n 2⌋ mod 4  and if n mod 4 = 2 it is l+2−dn+2− ⌊n 2⌋ mod 4 ; if n mod 4 = 0 it is 1 or lk! k−1 −1, depending on whether or not dn = 0 or 1, where k is the largest power of 2 dividing n; and if n mod 4 = 3 it is lk! k−1  − 1 or 1, depending on whether dn+1 = 0 or 1, where k is the largest power of 2 dividing n + 1. When l = 2 the same rules apply, except that 0s must be removed, so there is a more complicated pattern depending on n mod 24.   656  ANSWERS TO EXERCISES  4.5.3  [References: J. O. Shallit, J. Number Theory 11  1979 , 209–217; Allouche, Lubiw,  3,  v v′  = 67  Mendès France, van der Poorten, and Shallit, Acta Arithmetica 77  1996 , 77–96.] 42. Suppose that ∥qX∥ = qX − p. We can always find integers u and v such that q = uqn−1 + vqn and p = upn−1 + vpn, where pn = Kn−1 A2, . . . , An , since qnpn−1 − qn−1pn = ±1. The result is clear if v = 0. Otherwise we must have uv < 0, hence u qn−1X − pn−1  has the same sign as v qnX − pn , and qX − p is equal to uqn−1X − pn−1 + vqnX − pn. This completes the proof, since u ̸= 0. See Theorem 6.4S for a generalization. 43. If x is representable, so is the parent of x in the Stern–Brocot tree of exercise 40; thus the representable numbers form a subtree of that binary tree. Let  u u′  and  v v′  be adjacent representable numbers. Then one is an ancestor of the other; say  u u′  is an ancestor of  v v′ , since the other case is similar. Then  u u′  is the nearest left ancestor of  v v′ , so all numbers between u u′ and v v′ are left descendants of  v v′  and the mediant   u + v   u′ + v′   is its left child. According to the relation between regular continued fractions and the binary tree, the mediant and all of its left descendants will have  u u′  as their last representable pi qi, while all of the mediant’s right descendants will have  v v′  as one of the pi qi.  The numbers pi qi label the parents of the “turning-point” nodes on the path to x.  44. A counterexample for M = N = 100 is  u u′  = 1 99. However, the identity is almost always true, because of  12 ; it fails only when u u′ + v v′ is very nearly equal to a fraction that is simpler than  u u′ . 45. To determine A and r such that u = Av+r, 0 ≤ r < v, using ordinary long division, takes O  1 + log A  log u   units of time. If the quotients during the algorithm are A1, A2, . . . , Am, then A1A2 . . . Am ≤ u, so log A1+···+log Am ≤ log u. Also m = O log u  by Corollary L. 46. Yes, to O n log n 2 log log n  , even if we also need to compute the sequence of partial quotients that would be computed by Euclid’s algorithm; see A. Schönhage, Acta Informatica 1  1971 , 139–144. Moreover, Schönhage’s algorithm is asymptotically op- timal for computing a continued fraction expansion, with respect to the multiplications and divisions it performs [V. Strassen, SICOMP 12  1983 , 1–27]. Algorithm 4.5.2L is better in practice unless n is quite large, but an efficient implementation for numbers exceeding about 1800 bits is sketched in the book Fast Algorithms by A. Schönhage, A. F. W. Grotefeld, and E. Vetter  Heidelberg: Spektrum Akademischer Verlag, 1994 , §7.2. 48. Tj =  Kj−2 −a2, . . . ,−aj−1 , Kj−1 −a1, . . . ,−aj−1 , Kn−j aj+1, . . . , an d  =   −1 jKj−2 a2, . . . , aj−1 ,  −1 j−1Kj−1 a1, . . . , aj−1 , Kn−j aj+1, . . . , an d . 49. Since λx1 + µz1 = µv and λxn+1 + µzn+1 = −λv d, there is an odd value of j such that λxj + µzj ≥ 0 and λxj+2 + µzj+2 ≤ 0. If λxj + µzj > θ and λxj+2 + µzj+2   θ zj and λ > −θ xj+2. It follows that 0 < λxj+1 + µzj+1 < λµxj+1zj θ − λµzj+1xj+2 θ ≤ 2λµv θ = 2θ, because we have xk+1zk = Kk−1 a2, . . . , ak Kn−k ak+1, . . . , an  ≤ Kn−1 a2, . . . , an  = v d for all k. [H. W. Lenstra, Jr., Math. Comp. 42  1984 , 331–340.] 50. Let k = ⌈β α⌉. If kα < γ, the answer is k; otherwise it is   f  1 α  mod 1, k − γ α, k − β α     k − 1 +  α  .  51. If ax − mz = y and x ⊥ y we have x ⊥ mz. Consider the Stern–Brocot tree of exercise 40, with an additional node labeled 0 1. Attach the tag value y = ax − mz   4.5.4  ANSWERS TO EXERCISES  657  together with each node label z x. We want to find all nodes z x whose tag y is at most √ m 2 in absolute value and whose denominator x is also ≤ θ. The only possible θ = path to such nodes keeps a positive tag to the left and a negative tag to the right. This rule defines a unique path, which moves to the right when the tag is positive and to the left when the tag is negative, stopping when the tag becomes zero. The same path is followed implicitly when Algorithm 4.5.2X is performed with u = m and v = a, except that the algorithm skips ahead — it visits only nodes of the path just before the tag changes sign  the parents of the “turning point” nodes as in exercise 43 . Let z x be the first node of the path whose tag y satisfies y ≤ θ. If x > θ, there is no solution, since subsequent values on the path have even larger denominators. Otherwise  ±x,∓y  is a solution, provided that x ⊥ y. It is easy to see that there is no solution if y = 0, and that if y ̸= 0 the tag on the next node of the path will not have the same sign as y. Therefore node z x will be visited by Algorithm 4.5.2X, and we will have x = xj = Kj−1 a1, . . . , aj−1 , y = yj =  −1  j−1 Kn−j aj+1, . . . , an d, z = zj = Kj−2 a2, . . . , aj−1  for some j  see exercise 48 . The next possibility for a solution will be the node labeled z′ x′ =  zj−1 + kzj   xj−1 + kxj  with tag y′ = yj−1 + kyj, where k is as small as possible such that y′ ≤ θ; we have y′y < 0. However, x′ must now exceed θ; otherwise we would have m = Kn a1, . . . , an d = x′y + xy′ ≤ θ2 + θ2 = m, and equality cannot hold. This discussion proves that the problem can be solved efficiently by applying Algorithm 4.5.2X with u = m and v = a, but with the following replacement for step X2: “If v3 ≤ √ m 2, the algorithm terminates. The pair  x, y  =  v2, v3 sign v2   is then the unique solution, provided that x ⊥ y and x ≤ √ m 2; otherwise there is no solution.” [P. S. Wang, Lecture Notes in Comp. Sci. 162  1983 , 225–235; P. Kornerup and R. T. Gregory, BIT 23  1983 , 9–20.] A similar method works if we require 0 < x ≤ θ1 and y ≤ θ2, whenever 2θ1θ2 ≤ m.  SECTION 4.5.4 1. If dk isn’t prime, its prime factors are cast out before dk is tried. 2. No; the algorithm would fail if pt−1 = pt, giving “1” as a spurious prime factor. 3. Let P be the product of the first 168 primes. [Note: Although P = 19590 . . . 5910 is a 416-digit number, such a gcd can be computed in much less time than it would take to do 168 divisions, if we just want to test whether or not n is prime.] 4. In the notation of exercise 3.1–11,  2⌈lg max µ+1,λ ⌉P  µ, λ  = 1  f l     l≥1  m    l−1  k=1  1 − k m    ,    µ, λ  where f l  =  1≤λ≤l 2⌈lg max l+1−λ,λ ⌉. If l = 2k+θ, where 0 < θ ≤ 1, we have  f l  = l  2 3 · 2−θ − 2 · 2−2θ ,  Notes: Richard Brent has observed that, as m → ∞, the densityl−1  where the function 3 · 2−θ − 2 · 2−2θ reaches a maximum of 9 8 at θ = lg 4 3  and has a minimum of 1 at θ = 0 and 1. Therefore the average value of 2⌈lg max µ+1,λ ⌉ lies between 1.0 and 1.125 times the average value of µ + λ, and the result follows. k=1 1−k m  = exp −l l − 1  2m + O l3 m2   approaches a normal distribution, and we may assume that θ is uniformly distributed. Then 3·2−θ − 2·2−2θ takes the average value 3  4 ln 2 , and the average number of iterations needed by Algorithm B comes to approximately   658  4.5.4  √  ANSWERS TO EXERCISES √ √  3  4 ln 2  + 1 2  m. A similar analysis of the more general method πm 2 = 1.98277 √ in the answer to exercise 3.1–7 gives ∼ 1.92600 m, when p ≈ 2.4771366 is chosen “optimally” as the root of  p2 − 1  ln p = p2 − p + 1. See BIT 20  1980 , 176–184.  Algorithm B is a refinement of Pollard’s original algorithm, which was based on exercise 3.1–6 b  instead of the yet undiscovered result in exercise 3.1–7. He showed √ that the least n such that X2n = Xn has average value ∼  π2 12 Q m  ≈ 1.0308 m; this constant π2 12 is explained by Eq. 4.5.3– 21 . Hence the average amount of work √ m gcds  or multiplications mod m  needed by his original algorithm is about 1.03081 √ and 3.09243 m squarings. This will actually be better than Algorithm B when the cost of gcd is more than about 1.17 times the cost of squaring — as it usually is with large numbers. Brent noticed, however, that Algorithm B can be improved by not checking the gcd when k > l 2; if step B4 is repeated until k ≤ l 2, we will still detect the cycle, after λ⌊ℓ µ  λ⌋ = ℓ µ  −  ℓ µ  mod λ  further iterations. The average cost now becomes √ πm 2 ≈ 1.35611 m iterations when we square without approximately  3  4 ln 2   √ √ taking the gcd, plus   ln π − γ   4 ln 2  + 1 πm 2 ≈ .88319 m iterations when we 2  [See the analysis by Henri Cohen in A Course in Computational Algebraic do both. Number Theory  Berlin: Springer, 1993 , §8.5.] 5. Remarkably, 11111 ≡ 8616460799  modulo 3 · 7 · 8 · 11 , so  14  is correct also for N = 11111 except with respect to the modulus 5. Since the residues  x2 − N  mod 5 are 4, 0, 3, 3, 0, we must have x mod 5 = 0, 1, or 4. The first x ≥ ⌈√ N ⌉ = 106 that satisfies all the conditions is x = 144; but the square root of 1442 − 11111 = 9625 is not an integer. The next case, however, gives 1562 − 11111 = 13225 = 1152, and 11111 =  156 − 115  ·  156 + 115  = 41 · 271. 6. Let us count the number of solutions  x, y  of the congruence N ≡  x − y  x + y   modulo p , where 0 ≤ x, y < p. Since N ̸≡ 0 and p is prime, x + y ̸≡ 0. For each v ̸≡ 0 there is a unique u  modulo p  such that N ≡ uv. The congruences x − y ≡ u, x+ y ≡ v now uniquely determine x mod p and y mod p, since p is odd. Thus the stated congruence has exactly p − 1 solutions  x, y . If  x, y  is a solution, so is  x, p − y  if y ̸= 0, since  p− y 2 ≡ y2; and if  x, y1  and  x, y2  are solutions with y1 ̸= y2, we have 1 ≡ y2 2; hence y1 = p − y2. Thus the number of different x values among the solutions y2  x, y  is  p − 1  2 if N ≡ x2 has no solutions, or  p + 1  2 if N ≡ x2 has solutions. 7. One procedure is to keep two indices for each modulus, one for the current word position and one for the current bit position; loading two words of the table and doing an indexed shift command will bring the table entries into proper alignment.  Many computers have special facilities for such bit manipulation.  8.  We may assume that N = 2M is even.  The following algorithm uses an auxiliary table X[1], X[2], . . . , X[M − 1], where X[k] represents the primality of 2k + 1.  S1. Set X[k] ← 1 for 1 ≤ k < M. Also set j ← 1, k ← 1, p ← 3, q ← 4.  During  this algorithm p = 2j + 1 and q = 2j + 2j2.   S2. If X[j] = 0, go to S4. Otherwise output p, which is prime, and set k ← q. S3. If k < M, then set X[k] ← 0, k ← k + p, and repeat this step. S4. Set j ← j + 1, p ← p + 2, q ← q + 2p − 2. If j < M, return to S2.  A major part of this calculation could be made noticeably faster if q  instead of j  were tested against M in step S4, and if a new loop were appended that outputs 2j + 1 for all remaining X[j] that equal 1, suppressing the manipulation of p and q.   4.5.4  ANSWERS TO EXERCISES  659  Nicomachus’s Introduction to Arithmetic. It is well known that  ln ln N + M + O  log N −10000 , where M = γ +  Notes: The original sieve of Eratosthenes was described in Book 1, Chapter 13 of p prime[p≤ N ] p = k≥2 µ k  ln ζ k  k is Mertens’s constant 0.26149 72128 47642 78375 54268 38608 69585 90516−; see F. Mertens, Crelle 76  1874 , 46–62; Greene and Knuth, Mathematics for the Analysis of Algorithms  Boston: Birkhäuser, 1981 , §4.2.3. In particular, the number of operations in the original algorithm described by Nicomachus is N ln ln N + O N . Improvements in the efficiency of sieve methods for generating primes are discussed in exercise 5.2.3–15 and in Section 7.1.3. 9. If p2 is a divisor of n for some prime p, then p is a divisor of λ n , but not of n−1. If n = p1p2, where p1 < p2 are primes, then p2 − 1 is a divisor of λ n  and therefore p1p2 − 1 ≡ 0  modulo p2 − 1 . Since p2 ≡ 1, this means p1 − 1 is a multiple of p2 − 1, contradicting the assumption p1 < p2. [Values of n for which λ n  properly divides n − 1 are called Carmichael numbers. For example, here are some small Carmichael numbers with up to six prime factors: 3·11·17, 5·13·17, 7·11·13·41, 5·7·17·19·73, 5 · 7 · 17 · 73 · 89 · 107. There are 8241 Carmichael numbers less than 1012, and there are at least Ω N 2 7  Carmichael numbers less than N; see W. R. Alford, A. Granville, and C. Pomerance, Annals of Math.  2  139  1994 , 703–722.] 10. Let kp be the order of xp modulo n, and let λ be the least common multiple of all the kp’s. Then λ is a divisor of n − 1 but not of any  n − 1  p, so λ = n − 1. Since p mod n = 1, φ n  is a multiple of kp for all p, so φ n  ≥ λ. But φ n  < n − 1 xφ n  when n is not prime.  Another way to carry out the proof is to construct an element x of order n − 1 from the xp’s, by the method of exercise 3.2.1.2–15.  11.  Output  A  V  U 1984 1981 1983 1983 1981 1984 1984  1 1981 4 991 4 1981 1  0 1 495 2 495 1 1984  P 992 992 993 98109 2 99099 99101  S 0 1 0 1 0 1 0  T — 1981 1 991 1 1981 1  9932 ≡ +22 22 ≡ +22 991012 ≡ +20  The factorization 199·991 is evident from the first or last outputs. The shortness of the cycle, and the appearance of the notorious number 1984, are probably just coincidences. 12. The following algorithm makes use of an auxiliary  m + 1  ×  m + 1  matrix of integers Ejk, 0 ≤ j, k ≤ m; a single-precision vector  b0, b1, . . . , bm ; and a multiple- precision vector  x0, x1, . . . , xm  with entries in the range 0 ≤ xk < N.  F1. [Initialize.] Set bi ← −1 for 0 ≤ i ≤ m; then set j ← 0. F2. [Next solution.] Get the next output  x, e0, e1, . . . , em  from Algorithm E.  It  is convenient to regard Algorithms E and F as coroutines.  Set k ← m.  F3. [Search for odd.] If k < 0 go to step F5. Otherwise if ek is even, set k ← k − 1  and repeat this step.  F4. [Linear dependence?] If bk ≥ 0, then set i ← bk, x ←  xix  mod N, er ← er + Eir for 0 ≤ r ≤ m; set k ← k−1 and return to F3. Otherwise set bk ← j, xj ← x, Ejr ← er for 0 ≤ r ≤ m; set j ← j + 1 and return to F2.  In the latter case we have a new linearly independent solution, modulo 2, whose first   660  ANSWERS TO EXERCISES  4.5.4  odd component is ek. The values Ejr are not guaranteed to remain single- precision, but they tend to remain small when k decreases from m to 0 as recommended by Morrison and Brillhart.   F5. [Try to factor.]  Now e0, e1, . . . , em are even.  Set  y ←   −1 e0 2  pe1 2 1  . . . pem 2  m   mod N.  If x = y or if x + y = N, return to F2. Otherwise compute gcd x − y, N , which is a proper factor of N, and terminate the algorithm.  k  1  m  √   n  2 P +  1 . . . qfd  . . . pem 2  i arbitrarily when N = qf1  t and y ≡  −1 e0 2pe1 2  This algorithm finds a factor whenever it is possible to deduce one from the given outputs of Algorithm E. [Proof. Let the outputs of Algorithm E be  Xi, Ei0, . . . , Eim  for 1 ≤ i ≤ t, and suppose that we could find a factorization N = N1N2 when x ≡  modulo N , where ej = a1E1j+···+atEtj X a1 1 . . . X at is even for all j. Then x ≡ ±y  modulo N1  and x ≡ ∓y  modulo N2 . It is not difficult to see that this solution can be transformed into a pair  x, y  that appears in step F5, by a series of steps that systematically replace  x, y  by  xx′, yy′  where x′ ≡ ±y′  modulo N .] 13. There are 2d values of x having the same exponents  e0, . . . , em , since we can choose the sign of x modulo qfi d . Exactly two of these 2d values will fail to yield a factor. 14. Since P 2 ≡ kN Q2  modulo p  for any prime divisor p of V, we get 1 ≡ P 2 p−1  2 ≡  kN Q2  p−1  2 ≡  kN  p−1  2  modulo p , if P ̸≡ 0. 15. Un =  an − bn   2k+1  2n−1Un = 2 P −√ Similarly, if Vn = an + bn = Un+1 − QUn−1, then 2n−1Vn =   P n−2k−1Dk; so Up ≡ D p−1  2  modulo p  if p is an odd prime. P n−2kDk, and Vp ≡ P p ≡ P . Thus if Up ≡ −1, we find that Up+1 mod p = 0. If Up ≡ 1, we find that  QUp−1  mod p = 0; here if Q is a multiple of p, Un ≡ P n−1  modulo p  for n > 0, so Un is never a multiple of p; if Q is not a multiple of p, Up−1 mod p = 0. Therefore as in  pj +ϵj  . Theorem L, Ut mod N = 0 if N = pe1 that each pj is odd and each ϵj is ±1, so t ≤ 21−r pej−1 Under the assumptions of this exercise, the rank of apparition of N is N + 1; hence N is prime to Q and t is a multiple of N + 1. Also, the assumptions of this exercise imply 3 rN; hence  r , N ⊥ Q, and t = lcm1≤j≤r pej−1  D , D = P 2 − 4Q. Then  √ D , b = 1  D, where a = 1  3 pj  = 2  2   n   pj + 1 . Finally, therefore, e1 = 1 and ϵ1 = 1.  r = 1 and t = pe1  Note: If this test for primality is to be any good, we must choose P and Q in such a way that the test will probably work. Lehmer suggests taking P = 1 so that D = 1 − 4Q, and choosing Q so that N ⊥ QD.  If the latter condition fails, we know already that N is not prime, unless QD ≥ N.  Furthermore, the derivation above shows that we will want ϵ1 = 1, that is, D N−1  2 ≡ −1  modulo N . This is another condition that determines the choice of Q. Furthermore, if D satisfies this condition, and if UN+1 mod N ̸= 0, we know that N is not prime. Example: If P = 1 and Q = −1, we have the Fibonacci sequence, with D = 5. Since 511 ≡ −1  modulo 23 , we might attempt to prove that 23 is prime by using the Fibonacci sequence: ⟨Fn mod 23⟩ = 0, 1, 1, 2, 3, 5, 8, 13, 21, 11, 9, 20, 6, 3, 9, 12, 21, 10, 8, 18, 3, 21, 1, 22, 0, . . . , so 24 is the rank of apparition of 23 and the test works. However, the Fibonacci sequence cannot be used in this way to prove the primality of 13 or 17, since F7 mod  1 + ϵ1pe1−1  1 . . . per  2k  1  k  j  j   4.5.4  ANSWERS TO EXERCISES  661  2  + 3F   1  3  + ··· = eγ.  13 = 0 and F9 mod 17 = 0. When p ≡ ±1  modulo 10 , we have 5 p−1  2 mod p = 1, so Fp−1  not Fp+1  is divisible by p. 17. Let f q  = 2 lg q − 1. When q = 2 or 3, the tree has at most f q  nodes. When q > 3 is prime, let q = 1 + q1 . . . qt where t ≥ 2 and q1, . . . , qt are prime. The size of  the tree is ≤ 1 + f qk  = 2 + f q − 1  − t < f q . [SICOMP 4  1975 , 214–220.]  18. x G α − F  α   is the number of n ≤ x whose second-largest prime factor is ≤ xα and whose largest prime factor is > xα. Hence  xG′ t  dt =  π xt+dt  − π xt   · x  1−t G t  1 − t   − F t  1 − t   .  that this also equals  1 The probability that pt−1 ≤ √  0 F t 2 1 − t  t−1 dt. [Curiously, it can be shown 0 F t  1 − t   dt, the average value of log pt log x, and it also equals the Dickman–Golomb constant .62433 of exercises 1.3.3–23 and 3.1–13. The derivative G′ 0  can be shown to equal  pt is 1   1 0 F t  1 − t  t−2 dt = F  1  + 2F   1  The third-largest prime factor has H α  = α  j mod N, where qj = pej  0  H t  1 − t   − G t  1 − t   t−1 dt and H′ 0  = ∞. See P. Billingsley, Period. Math. Hungar. 2  1972 , 283–289; J. Galambos, Acta Arith. 31  1976 , 213–218; D. E. Knuth and L. Trabb Pardo, Theoretical Comp. Sci. 3  1976 , 321–348; J. L. Hafner and K. S. McCurley, J. Algorithms 10  1989 , 531–556.] 19. M = 2D − 1 is a multiple of all p for which the order of 2 modulo p divides D. To extend this idea, let a1 = 2 and aj+1 = aqj j , pj is the jth prime, and ej = ⌊log 1000 log pj⌋; let A = a169. Now compute bq = gcd Aq − 1, N  for all primes q between 103 and 105. One way to do this is to start with A1009 mod N and then to multiply alternately by A4 mod N and A2 mod N.  A similar method was used in the 1920s by D. N. Lehmer, but he didn’t publish it.  As with Algorithm B we can avoid most of the gcds by batching; for example, since b30r−k = gcd A30r − Ak, N , we might try batches of 8, computing cr =  A30r − A29  A30r − A23  . . .  A30r − A  mod N, then gcd cr, N  for 33 < r ≤ 3334. 20. See H. C. Williams, Math. Comp. 39  1982 , 225–234. 21. Some interesting theory relevant to this conjecture has been introduced by Eric Bach, Information and Computation 90  1991 , 139–155. 22. Algorithm P fails only when the random number x does not reveal the fact that n is nonprime. Say x is bad if xq mod n = 1 or if one of the numbers x2jq is ≡ −1  modulo n  for 0 ≤ j < k. Since 1 is bad, we have pn = [n nonprime] bn −1   n−2  < [n nonprime]bn  n − 1 , where bn is the number of bad x such that 1 ≤ x < n. Every bad x satisfies xn−1 ≡ 1  modulo n . When p is prime, the number of solutions to the congruence xq ≡ 1  modulo pe  for 1 ≤ x < pe is the same as the number of solutions of qy ≡ 0  modulo pe−1 p − 1   for 0 ≤ y < pe−1 p − 1 , namely gcd q, pe−1 p − 1  , since we may replace x by ay where a is a primitive root. r r , where the ni are distinct primes. According to the Chinese 1 . . . ner remainder theorem, the number of solutions to the congruence xn−1 ≡ 1  modulo n  is i=1 gcd n − 1, nei−1 i=1 ni − 1  since ni is relatively prime to n − 1. If some ei > 1, we have ni − 1 ≤ 2 i , hence the number of solutions is at most 2 ni = 1 + 2ki qi, where k1 ≤ ··· ≤ kr. Then gcd n − 1, ni − 1  = 2k′  Therefore we may assume that n is the product n1 . . . nr of distinct primes. Let i q′i, where k′i =   ni − 1  , and this is at mostr  9 n; in this case bn ≤ 2  4 n − 1 , since n ≥ 9.  Let n = ne1  9 n ≤ 1  9 nei  i     i,j pi−1  qj−1  4 q  =  −1     ,  p  ANSWERS TO EXERCISES  662 4.5.4 min k, ki  and q′i = gcd q, qi . Modulo ni, the number of x such that xq ≡ 1 is q′i; and the number of x such that x2jq ≡ −1 is 2jq′i for 0 ≤ j < k′i, otherwise 0. Since k ≥ k1, 4 φ n ,  To complete the proof, it suffices to show that bn ≤ 1  4 q1 . . . qr2k1+···+kr = 1  0≤j<k1 2jr .  we have bn = q′1 . . . q′r  1 +  1 +  since φ n  < n − 1. We have  0≤j<k1 2jr  2k1+···+kr ≤  1 +  0≤j<k1 2jr  2k1r  = 1  2r − 1  +  2r − 2   2k1r 2r − 1   ≤ 1 2r−1  ,  so the result follows unless r = 2 and k1 = k2. If r = 2, exercise 9 shows that n − 1 is not a multiple of both n1 − 1 and n2 − 1. Thus if k1 = k2 we cannot have both q′1 = q1 and q′2 = q2; it follows that q′1q′2 ≤ 1  3 q1q2 and bn ≤ 1  6 φ n  in this case.  [Reference: J. Number Theory 12  1980 , 128–138.] This proof shows that pn is near 1 4 in only two cases, when n is  1 + 2q1  1 + 4q1  or a Carmichael number of the special form  1 + 2q1  1 + 2q2  1 + 2q3 . For example, when n = 49939 · 99877 we 4 49938 · 99876  and pn ≈ .24999; when n = 1667 · 2143 · 4523, we have have bn = 1 bn = 1 23.  a  The proofs are simple except perhaps for the reciprocity law. Let p = p1 . . . ps and q = q1 . . . qr, where the pi and qj are prime. Then  4 1666 · 2142 · 4522 , pn ≈ .24968. See the next answer for further remarks.]       pi =   −1  pi−1  qj−1  4 qj  p = so we need only verify that i,j  pi − 1  qj − 1  4 =   i pi − 1  2     qj  pi  i,j  i,j  q  bad if it falsely makes n look prime. Let Πn =r  i,j  pi − 1  qj − 1  4 ≡  p − 1  q − 1  4  modulo 2 . But j qj − 1  2  is odd if and only if an odd number of the pi and an odd number of the qj are ≡ 3  modulo 4 , and this holds if and only if  p−1  q−1  4 is odd. [C. G. J. Jacobi, Bericht Königl. Preuß. Akad. Wiss. Berlin 2  1837 , 127–136; V. A. Lebesgue, J. Math. Pures Appl. 12  1847 , 497–520, discussed the efficiency.]  b  As in exercise 22, we may assume that n = n1 . . . nr where the ni = 1 + 2ki qi are distinct primes, and k1 ≤ ··· ≤ kr; we let gcd n − 1, ni − 1  = 2k′ i q′i and we call x i=1 q′i 2min ki,k−1  be the number of solutions of x n−1  2 ≡ 1. The number of bad x with   x n  = 1 is Πn, times an extra   = −1 for an even factor of 1 n  = −1 is Πn if k1 = k, number of the ni with ki < k.  The number of bad x with   x otherwise 0. [If x n−1  2 ≡ −1  modulo ni , we have   x   = +1 if ki > k, and a contradiction if ki < k. If k1 = k, there are an odd number of ki equal to k.] 4 only if n is a Carmichael number with kr < k; for example, n = 7 · 13 · 19 = 1729, a number made famous by Ramanujan in another context. Louis Monier has extended the analyses above to obtain the following closed formulas for the number of bad x in general:  Notes: The probability of a bad guess is > 1  2 is needed to ensure that   x  2 if k1 < k.  This factor 1    = −1 if ki = k,   x  ni  ni  ni    bn =  1 + 2rk1 − 1 2r − 1   r  i=1  q′i ;  b′n = δn  gcd  , ni − 1  .   n − 1  2  r  i=1    Here b′n is the number of bad x in this exercise, and δn is either 2  if k1 = k , or 1 ki < k and ei is odd for some i , or 1  otherwise .  2  if n . If x2jq ≡ −1  modulo n , then the order of x modulo ni must be an odd multiple of 2j+1 for all prime divisors ni   c  If xq mod n = 1, then 1 =   xq  n q =   x  n   =   x   4.5.4  ANSWERS TO EXERCISES  663 n  = +1 or −1  of n. Let n = ne1  according as  eiq′′i is even or odd. Since n ≡  1 + 2j+1 eiq′′i    modulo 2j+2 , the sum eiq′′i is odd if and only if j + 1 = k. [Theoretical Comp. Sci. 12  1980 , 97–108.]  r and ni = 1 + 2j+1q′′i ; then   x ni    =  −1 q′′  1 . . . ner  i , so   x  3 r, 2  4 n − 1  6 n ≤ 1  4 n, r  = 1  3 N + min  1  3 qn + min  1  0.35201 65995 57547 47542 73567 67736 43656 84471+.  305], who showed in fact that the O 1  term is C+ ∞ is the kth power of a prime. The constant C is li 2−ln 2 = γ+ln ln 2+  24. Let M1 be a matrix having one row for each nonprime odd number n in the range 1 ≤ n ≤ N and having N −1 columns numbered from 2 to N; the entry in row n column x is 1 if n fails the x test of Algorithm P, otherwise it is zero. When N = qn + r and 0 ≤ r < n, we know that row n contains at most −1 + q bn + 1  + min bn + 1, r  < 4 n − 1  + 1  + min bn + 1, r  ≤ 1 3 r  ≤ q  1 1 3 N + 1 2 N entries equal to 0, so at least half of the entries in the matrix are 1. Thus, some column x1 of M1 has at least half of its entries equal to 1. Removing column x1 and all rows in which this column contains 1 leaves a matrix M2 having similar properties; a repetition of this construction produces matrix Mr with N − r 2 N − 1  entries per row equal columns and fewer than N 2r rows, and with at least 1 to 1. [See FOCS 19  1978 , 78.] [A similar proof implies the existence of a single infinite sequence x1 < x2 < ··· such that the number n > 1 is prime if and only if it passes the x test of Algorithm P 2⌊lg n⌋ ⌊lg n⌋ − 1 . Does there exist a sequence for x = x1, . . . , x = xm, where m = 1 x1 < x2 < ··· having this property but with m = O log n ?] 25. This theorem was first proved rigorously by von Mangoldt [Crelle 114  1895 , 255– x dt   t2−1 t ln t , minus 1 2k if x n≥2 ln 2 n nn! = [For a summary of developments during the 100 years following von Mangoldt’s paper, see A. A. Karatsuba, Complex Analysis in Number Theory  CRC Press, 1995 . See also Eric Bach and Jeffrey Shallit, Algorithmic Number Theory 1  MIT Press, 1996 , Chapter 8, for an excellent introduction to the connection between Riemann’s hypothesis and concrete problems about integers.] 26. If N is not prime, it has a prime factor q ≤ √ N. By hypothesis, every prime divisor p of f has an integer xp such that the order of xp modulo q is a divisor of N − 1 but not of  N − 1  p. Therefore if pk divides f, the order of xp modulo q is a multiple of pk. Exercise 3.2.1.2–15 now tells us that there is an element x of order f modulo q. But this is impossible, since it implies that q2 ≥  f + 1 2 ≥  f + 1  r ≥ N, and equality cannot hold. [Proc. Camb. Phil. Soc. 18  1914 , 29–30.] 27. If k is not divisible by 3 and if k ≤ 2n + 1, the number k·2n + 1 is prime if and only if 32n−1k ≡ −1  modulo k·2n + 1 . For if this condition holds, k·2n + 1 is prime by exercise 26; and if k·2n + 1 is prime, the number 3 is a quadratic nonresidue mod k·2n +1 by the law of quadratic reciprocity, since  k·2n +1  mod 12 = 5. [This test was stated without proof by Proth in Comptes Rendus Acad. Sci. 87  Paris, 1878 , 926.] To implement Proth’s test with the necessary efficiency, we need to be able to compute x2 mod  k·2n + 1  with about the same speed as we can compute the quantity x2 mod  2n − 1 . Let x2 = A·2n + B; then x2 ≡ B − ⌊A k⌋ + 2n A mod k , so the remainder is easily obtained when k is small.  See also exercise 4.3.2–14.  [To test numbers of the form 3·2n + 1 for primality, the job is only slightly more difficult; we first try random single-precision numbers until finding one that is a quadratic nonresidue mod 3·2n + 1 by the law of quadratic reciprocity, then use this number in place of “3” in the test above. If n mod 4 ̸= 0, the number 5 can be used. It turns out that 3·2n + 1 is prime when n = 1, 2, 5, 6, 8, 12, 18, 30, 36, 41, 66, 189, 201, 209, 276, 353, 408, 438, 534, 2208, 2816, 3168, 3189, 3912, 20909, 34350, 42294,   664  ANSWERS TO EXERCISES  4.5.4  r  1  m  3 + 1  3 + 1  3 + 1  6 + 1  1 . . . pem  1 . . . pem  1 . . . pxm  12 +··· = 4   pairs  xi ism+r  i  modulo qi , we can find yi such that pe1  m ≡  ±yi 2 1 . . . pem i  , hence by the Chinese remainder theorem we obtain 2d values of X such  m  modulo N . Such  e1, . . . , em  correspond to at most r   ≥ mr r!, and each of these corresponds to a unique integer px1  42665, 44685, 48150, 55182, 59973, 80190, 157169, 213321, and no other n ≤ 300000; and 5·2n + 1 is prime when n = 1, 3, 7, 13, 15, 25, 39, 55, 75, 85, 127, 1947, 3313, 4687, 5947, 13165, 23473, 26607, 125413, 209787, 240937, and no other n ≤ 300000. See R. M. Robinson, Proc. Amer. Math. Soc. 9  1958 , 673–681; G. V. Cormack and H. C. Williams, Math. Comp. 35  1980 , 1419–1421; H. Dubner and W. Keller, Math. Comp. 64  1995 , 397–405; J. S. Young, Math. Comp. 67  1998 , 1735–1738.] 28. f p, p2d  = 2  p + 1  + f p, d  p, since 1  p + 1  is the probability that A is a multiple of p. f p, pd  = 1  p + 1  when d mod p ̸= 0. f 2, 4k + 3  = 1 3 since 3 since A2− 8k+5 B2 cannot A2− 4k+3 B2 cannot be a multiple of 4; f 2, 8k+5  = 2 3. f p, d  =  2p  p2−1 , 0  be a multiple of 8; f 2, 8k+1  = 1 if d p−1  2 mod p =  1, p − 1 , respectively, for odd p. 29. The number of solutions to the inequality x1 +···+xm ≤ r in nonnegative integers m ≤ n. [For sharper estimates, in the special case that pj is the jth prime for all j, see N. G. de Bruijn, Indag. Math. 28  1966 , 240–247; H. Halberstam, Proc. London Math. Soc.  3  21  1970 , 102–107.] m ≡ x2 30. If pe1  modulo qdi that X2 ≡ pe1  e′1, . . . , e′m; e′′1, . . . , e′′m  having the hinted properties. Now for each of the 2d binary numbers a =  a1 . . . ad 2, let na be the number of exponents  e′1, . . . , e′m  such that integers X is ≥ 2d   pe′ 1 . . . pe′ m   qi−1  2 ≡  −1 ai  modulo qi ; we have proved that the required number of r 2 objects from a set of m objects with repetitions permitted, namely m+r 2 a ≥m+r 2 have  a na is the number of ways to choose at most [See J. Algorithms 3  1982 , 101–127, where Schnorr presents many further refinements of Theorem D.] 31. Set n = M, pM = 4m, and ϵM = 2m to show that Pr X ≤ 2m  ≤ e−m 2. 32. Let M = ⌊ 3√ N⌋, and let the places xi of each message be restricted to the range 0 ≤ x < M 3 − M 2. If x ≥ M, encode it as x3 mod N as before, but if x < M change the encoding to  x + yM 3 mod N, where y is a random number in the range M 2 − M ≤ y < M 2. To decode, first take the cube root; and if the result is M 3 − M 2 or more, take the remainder mod M. 34. Let P be the probability that xm mod p = 1 and let Q be the probability that xm mod q = 1. The probability that gcd xm−1, N  = p or q is P  1 − Q  + Q 1 − P   = 2, this probability is ≥ 2 10−6 − 10−12 , so we have a P + Q − 2P Q. If P ≤ 1 good chance of finding a factor after about 106 log m arithmetic operations modulo N. 2 then P ≈ Q ≈ 1, since we have the general On the other hand if P > 1 formula P = gcd m, p− 1  p; thus m is a multiple of lcm p− 1, q − 1  in this case. Let m = 2kr where r is odd, and form the sequence xr mod N, x2r mod N, . . . , x2kr mod N; we find as in Algorithm P that the first appearance of 1 is preceded by a value y other than N − 1 with probability ≥ 1 35. Let f =  pq−1 − qp−1  mod N. Since p mod 4 = q mod 4 = 3, we have   −1 p   = −  f   −1 q   =   f the range 0 ≤ x ≤ 1 transmit the message ¯x2 mod N.  q   = −1, and we also have   2 8 N − 5 , let ¯x = 4x + 2 or 8x + 4, whichever satisfies   ¯x  . Since a   r 2 2d ≥ mr  2d r 2 !2 .  p   = q   = −1. Given a message x in N   ≥ 0; then  2, hence gcd y − 1, N  = p or q.  2 and Q > 1  , we  2 or Q ≤ 1  p  = −  2  a n2  a n2  r 2  r 2  r 2  r 2   4.5.4  ANSWERS TO EXERCISES  665  N  that y2 ≡ ¯x2 mod N and  y   ≥ 0 and y is even. Then y = ¯x, since the other square  To decode this message, we first use a SQRT box to find the unique number y such roots of ¯x2 are N − ¯x and  ±f ¯x  mod N; the first of these is odd, and the other two either have negative Jacobi symbols or are simply ¯x and N − ¯x. The decoding is now completed by setting x ← ⌊y 4⌋ if y mod 4 = 2, otherwise x ← ⌊y 8⌋.  Anybody who can decode such encodings can also find the factors of N, because N   = −1 reveals  ±f  mod N, and the decoding of a false message ¯x2 mod N when   ¯x   ±f  mod N  − 1 has a nontrivial gcd with N. [Reference: IEEE Transactions IT-26  1980 , 726–729.] 36. The mth prime equals m ln m + m ln ln m − m + m ln ln m ln m − 2m ln m + O m log log m 2 log m −2 , by  4 , although for this problem we need only the weaker estimate pm = m ln m + O m log log m .  We will assume that pm is the mth prime, since this corresponds to the assumption that V is uniformly distributed.  If we choose ln N ln ln N, where c = O 1 , we find that r = c−1√ √ ln N  ln ln N − ln m = 1 2 c c−2 − c−2 ln ln ln N  ln ln N − 2c−2 ln 1 ln ln N  ln N  . The estimated √ running time  22  now simplifies somewhat surprisingly to exp f c, N  ln N ln ln N + O log log N  , where we have f c, N  = c +  1 −  1 + ln 2  ln ln N c−1. The value of c that minimizes f c, N  is  1 −  1 + ln 2  ln ln N, so we obtain the estimate  √ 2 c  ln ln N + O   √  √  √  exp 2  ln N ln ln N  1 −  1 + ln 2  ln ln N + O log log N  .  n − Dq2  √ n = 2  Dqn + O q−2  Dqnpn − √  Note: The partial quotients of  When N = 1050 this gives ϵ N  ≈ .33, which is still much larger than the observed behavior.  √ D seem to behave according to the distribution obtained for random real numbers in Section 4.5.3. For example, the first million partial quotients of the square root of the number 1018 + 314159 include exactly  415236, 169719, 93180, 58606  cases where An is respectively  1, 2, 3, 4 . Moreover, we have Vn+1 = p2 n   by exercise 4.5.3–12 c  and √ D to behave essentially like the quantity Eq. 4.5.3– 12 . Therefore we can expect Vn 2 θn x  = qnpn − xqn, where x is a random real number. The random variable θn is known to have the approximate density min 1, θ−1 − 1  ln 2 for 0 ≤ θ ≤ 1 [see Bosma, Jager, and Wiedijk, Indag. Math. 45  1983 , 281–299], which is uniform when θ ≤ 1 2. So something besides the size of Vn must account for the unreasonable effectiveness of Algorithm E. √ 37. Apply exercise 4.5.3–12 to the number D + R, to see that the periodic part begins immediately, and run the period backwards to verify the palindromic property. [It follows that the second half of the period gives the same V ’s as the first, and Algorithm E could be shut down earlier by terminating it when U = U′ or V = V ′ in step E5. However, the period is generally so long, we never even get close to halfway through it, so there is no point in making the algorithm more complicated.] 38. Let r =  1050 − 1  9. Then P0 = 1049 + 9; P1 = r + 3 · 1046; P2 = 2r + 3 · 1047 + 7; P3 = 3r + 2 · 1049; P4 = 4r + 2 · 1049 − 3; P5 = 5r + 3 · 1049 + 4; P6 = 6r + 2 · 1048 + 3; P7 = 7r + 2 · 1025  very pretty ; P8 = 8r + 1038 − 7; P9 = 9r − 8000. 39. Notice that it’s easy to prove the primality of q when q − 1 has just 2 and p as prime factors. The only successors of 2 are Fermat primes, and the existence or nonexistence of a sixth Fermat prime is one of the most famous unsolved problems of number theory. Thus we probably will never know how to determine whether or not an arbitrary integer has any successors. In some cases, however, this is possible; for example, John Selfridge proved in 1962 that 78557 and 271129 have none [see AMM 70   666  ANSWERS TO EXERCISES  4.5.4  n  m   1963 , 101–102], after W. Sierpiński had proved the existence of infinitely many odd numbers without a successor [Elemente der Math. 15  1960 , 73–74]. Perhaps 78557 is the smallest of these, although 69 other contenders for that honor still existed in 1983, according to G. Jaeschke and W. Keller [Math. Comp. 40  1983 , 381–384, 661–673; 45  1985 , 637]. For information on the more traditional “Cunningham” form of prime chain, in which the transitions are p → 2p±1, see Günter Löh, Math. Comp. 53  1989 , 751–759. In particular, Löh found that 554688278430 · 2k − 1 is prime for 0 ≤ k < 12. 40. [Inf. Proc. Letters 8  1979 , 28–31.] Notice that x mod y = x − y ⌊x y⌋ can be computed easily on such a machine, and we can get simple constants like 0 = x − x, 1 = ⌊x x⌋, 2 = 1 + 1; we can test x > 0 by testing whether x = 1 or ⌊x  x − 1 ⌋ ̸= 0.  a  First compute l = ⌊lg n⌋ in O log n  steps, by repeatedly dividing by 2; at the same time compute k = 2l and A = 22l+1 in O log n  steps by repeatedly setting k ← 2k, A ← A2. For the main computation, suppose we know that t = Am, u =  A + 1 m, and v = m!; then we can increase the value of m by 1 by setting m ← m + 1, t ← At, u ←  A + 1 u, v ← vm; and we can double the value of m by setting m ← 2m, u ← u2, v ←  ⌊u t⌋ mod A v2, t ← t2, provided that A is sufficiently large.  Consider the  let nj =  al . . . aj 2; if m = nj and k = 2j and j > 0 we can decrease j by 1 by setting k ← ⌊k 2⌋, m ← 2m +  ⌊n k⌋ mod 2 . Hence we can compute nj! for j = l, l − 1, . . . , 0 in O log n  steps. [Another solution, due to Julia Robinson, is to compute   b  First compute A = 22l+2 as in  a , then find the least k ≥ 0 such that If gcd n, 2k!  ̸= 1, let f n  be this value; note that this gcd can 2k+1! mod n = 0. be computed in O log n  steps by Euclid’s algorithm. Otherwise we will find the least  number u in radix-A notation; A must be greater than2m n! = ⌊Bn B integer m such that  m  mod n = 0 if and only if m! mod n = 0. Furthermore n ̸= 4.   m u =  A + 1 2Fj , u′ =  A + 1 2Fj+1, v = Am, w =  A + 1 2m, 2m  mod n ̸= 0, and  mod n = 0. It is easy to reach this state of affairs with m = Fj+1, for suitably 2 m+s   case 2k < m ≤ 2k+1, hence ⌈m 2⌉ ≤ 2k and ⌈m 2⌉! is relatively prime to n; therefore ⌊m 2⌋   mod n = 0, and let f n  = gcd m, n .  Note that in this  ⌋ when B >  2n n+1; see AMM 80  1973 , 250–251, 266.]  To compute m with a bounded number of registers, we can use Fibonacci numbers  see Algorithm 6.2.1F . Suppose we know that s = Fj, s′ = Fj+1, t = AFj , t′ = AFj+1,  .  Now if n =  al . . . a0 2,  large j, in O log n  steps; furthermore A will be larger than 22 m+s . If s = 1, we set f n  = gcd 2m+1, n  or gcd 2m+2, n , whichever is ̸= 1, and terminate the algorithm. Otherwise we reduce j by 1 as follows: Set r ← s, s ← s′ − s, s′ ← r, r ← t, t ← ⌊t′ t⌋, t′ ← r, r ← u, u ← ⌊u′ u⌋, u′ ← r; then if  ⌊wu vt⌋ mod A  mod n ̸= 0, set m ← m+s, w ← wu, v ← vt.  ⌊m 2⌋  [Can this problem be solved with fewer than O log n  operations? Can the small-  est, or the largest, prime factor of n be computed in O log n  operations?] 41.  a  Clearly π x  = π m  + f1 x, m  = π m  + f x, m  − f0 x, m  − f2 x, m  − f3 x, m  − ··· when 1 ≤ m ≤ x. Set x = N 3, m = N, and note that fk N 3, N  = 0 for k > 2. , where p and q range over primes. Hence N <p≤N3 2 π N 3 p −π p +1  = 31   −   b  We have f2 N 3, N  = N <p≤N3 2 π N 3 p  −π N3 2    +π 10  π 31    = 24 + 21 + 16 + 15 + 14 + 11 + 11 − 55 + 6 = 63.  N <p≤q[pq ≤ N 3] =  +π N   2 13   + π  1000  f2 1000, 10  = π  1000  2 17   + π  1000  19   + π  1000  23   + π  1000  11   + π  1000  29   + π  1000  m+s  m  2  2   4.5.4  ANSWERS TO EXERCISES  667   c  The hinted identity says simply that a pj-survivor is a pj−1-survivor that isn’t a multiple of pj. Clearly f N 3, N  = f N 3, pπ N  . Apply the identity until reaching terms f x, pj  where either j = 0 or x ≤ N 2; the result is  N−1  k=1   N 3  k   − π N   j=1    f N  3 , N  =  µ k f  , 1   N 3  kpj    µ k f  , pj−1  [k is a pj-survivor].  N pj≤k<N 10 , 1  − f  1000  O log log N .  15 , 2  − f  1000  14 , 1  − f  1000  Now f x, 1  = ⌊x⌋, so the first sum is 1000 − 500 − 333 − 200 + 166 − 142 = −9 when N = 10. The second sum is −f  1000 21 , 2  − 35 , 3  = −100 − 71 − 33 − 24 − 9 = −237. Hence f 1000, 10  = −9 + 237 = 228, f  1000 and π 1000  = 4 + 228 − 1 − 63 = 168.  d  If N 2 ≤ 2m we can construct an array in which a2m−1+n = [n + 1 is a pj- survivor] for 1 ≤ n ≤ N 2 represents a sieve after j passes, and an = a2n + a2n+1 for 1 ≤ n < 2m. Then it is easy to compute f x, pj  in O m  steps when x ≤ N 2, and to remove multiples of p from the sieve in O N 2m p  steps. The total running j=1 1 pj = The storage requirement can be reduced from 2N 2m to 2N m if we break the sieve into N parts of size N and work on each part separately. Auxiliary tables of pj for 1 ≤ j ≤ π N , and of µ k  and the least prime factor of k for 1 ≤ k ≤ N, are helpful and easily constructed before the main computation begins.  time to compute f N 3, N  will come to O N 2 log N log log N , because π N   [See Math. Comp. 44  1985 , 537–560. A similar procedure was first introduced by D. F. E. Meissel, Math. Annalen 2  1870 , 636–642; 3  1871 , 523–525; 21  1883 , 304; 25  1885 , 251–257. D. H. Lehmer made several refinements in Illinois J. Math. 3  1959 , 381–388. Neither Meissel nor Lehmer had a stopping rule for the recurrence that was as efficient as the method described above. Further refinements due to Marc Deléglise, Joël Rivat, Xavier Gourdon, and Tomás Oliveira e Silva have made it possible to deduce that π 1023  = 1925320391606803968923; see Revista do DETUA 4  2006 , 759–768. Lagarias and Odlyzko also developed a completely different approach whereby π N  can be evaluated in O N 1 2+ϵ  steps, using principles of analytic number theory; see J. Algorithms 8  1987 , 173–191. But the constant in that O is impracticably large. 42. L1. [Initialize.] Find ¯r such that r¯r ≡ 1  modulo s ; then set r′ ← n¯r mod s,  1, u ,  v1, v3  ←  0, v .  We want to find all pairs of integers  λ, µ  such that √  λs + r  µs + r′  = N; this implies λu + µ ≡ w  modulo s  and λµv ≤ θ. We will perform Algorithm 4.5.2X with t2, u2, v2 suppressed; the relations  modulo s   u ← r′¯r mod s, v ← s, w ←  n − rr′ ¯r s mod s, θ ← ⌊N s⌋,  u1, u3  ←  λu3 + µu1 ≡ wu1,  λv3 + µv1 ≡ wv1  λt3 + µt1 ≡ wt1,  will remain invariant.   L2. [Try for divisors.] If v1 = 0, output λs + r whenever λs + r divides N and 0 ≤ λ ≤ θ s. If v3 = 0, output N  µs + r′  whenever µs + r′ divides N and 0 ≤ µ ≤ θ s. Otherwise, for all k such that wv1 + ks ≤ θ if v1 < 0, or 0   0, and for σ = +1 and −1, output λs + r if d =  wv1s+ ks2 + v3r + v1r′ 2 −4v1v3N is a perfect square and if the numbers λ = wv1s + ks2 − v3r + v1r′ + σ d are positive integers.  λs + r  µs + r′  = N.    These are the solutions to λv3 + µv1 = wv1 + ks,  , µ = wv1s + ks2 + v3r − v1r′ − σ  2v3s  2v3s  √  √  d   668  ANSWERS TO EXERCISES  4.5.4  L3. [Done?] If v3 = 0, the algorithm terminates. L4. [Divide and subtract.] Set q ← ⌊u3 v3⌋. If u3 = qv3 and v1 < 0, decrease q  by 1. Then set  and return to step L2.   t1, t3  ←  u1, u3  −  v1, v3 q,   u1, u3  ←  v1, v3 ,   v1, v3  ←  t1, t3   m  j mod m  =  yx2  [See Math. Comp. 42  1984 , 331–340. The bounds in step L2 can be sharpened, for example to ensure that d ≥ 0. Some factors may be output more than once.]  43.  a  First make sure that the Jacobi symbol y   is +1.  If it’s 0, the task is easy;  j mod m  mod 2]. If y ∈ Qm we have E Xj ≥ 1  2 − ϵ. Report that y ∈ Qm if X1 + ··· + Xn ≥ 1   b  Find an x with Jacobi symbol  x 2 ϵ−2 ln δ−1⌉.  if it’s −1, then y  ∈ Qm.  Then choose random integers x1, . . . , xn in [0 . . m  and let Xj = [G y2x4 2 + ϵ; otherwise m − y ∈ Qm and E Xj ≤ 1 2 n. The probability of failure is at most e−2ϵ2n, by exercise 1.2.10–21. Therefore we choose n = ⌈ 1 prime factors of m are gcd x + √ y, m  and gcd x − √ y, m , so our task is to find ±√ y when y ∈ Qm is given. If we can find τ v for any nonzero v, we are done, since √ y =  v−1τ v  mod m unless gcd v, m  is a factor of m. Assume that ϵ = 2−e for some e ≥ 1. Choose random integers a and b in [0 . . m ,   = −1, and set y ← x2 mod m. Then the  m  and assume that we know the binary fractions α0 and β0 such that   τ a  m   <  − α0  ϵ 64 ,   τ b  m   <  ϵ3 64;  − β0  Define the numbers utj = 2−t a+ j + 1  here α0 is an odd multiple of ϵ 64, while β0 is an odd multiple of ϵ3 64. Assume also that we know λa and λb. Of course we don’t really know α0, β0, λa, or λb, but we will try all 32ϵ−1 × 32ϵ−3 × 2 × 2 possibilities. Spurious branches of the program, which operate under incorrect assumptions, will cause no harm. 2 b  mod m and vtj = 2−t−1 a+jb  mod m. Both utj and vtj are uniformly distributed in [0 . . m , because a and b were chosen at random. Furthermore, for fixed t, the numbers utj for j0 ≤ j < j0 + l are pairwise independent, and so are the numbers vtj for j0 ≤ j < j0 + l, as long as l does not exceed the smallest prime factor of m. We will make use of utj and vtj only for −2rϵ−2 ≤ j < 2rϵ−2; if any of these values has a nonzero factor in common with m, we’re done. For all v ⊥ m we define χv = +1 if v ∈ Qm, χv = −1 if −v ∈ Qm, and χv = 0   = −1. Notice that χu t+2 j = χutj, since utj =  22u t+2 j  mod m. Therefore  we can determine χutj and χvtj for all t and j by applying algorithm A to utj and vtj for 0 ≤ t ≤ 1 and −2rϵ−2 ≤ j < 2rϵ−2. Setting δ = 1 1440 ϵ2r−1 in that algorithm will ensure that all χ values are correct with probability ≥ 1 − 1 90. The algorithm works in at most r stages. At the beginning of stage t, for 0 ≤ t < r,  if  v  m  we assume that we know λ2−ta, λ2−tb, and fractions αt, βt such that   τ2−ta   <  − αt  ϵ 2t+6 , 2 αt + λ2−ta  and βt+1 = 1  m  Define αt+1 = 1 The next step is to find λ2−t−1b, which satisfies   τ2−tb   <  τ2−ta + jτ2−tb + τ2−t−1b  − βt  m  ϵ3 2t+6 .    m  λutj + λ2−ta + jλ2−tb + λ2−t−1  b +  2 βt + λ2−tb ; this preserves the inequalities.  ≡ 0   modulo 2  .   4.5.4  ANSWERS TO EXERCISES  669  Let n = 4 min r, 2t  ϵ−2; then when j ≤ n + τ2−t−1b  τ2−tb  + j  m  m   τ2−ta  m  2 we have  −  αt + jβt + βt+1    <  ϵ 16 .  ⌊ τ2−ta + jτ2−tb + τ2−t−1 16 m or τ utj >  1 − ϵ  Therefore if χutj = 1 it is likely that λ2−t−1b = Gj, where Gj =  G u2 λ2−ta + jλ2−tb + ⌊αtjβt + βt+1⌋  mod 2. More precisely, we will have b  m⌋ = ⌊αt + jβt + βt+1⌋  tjy mod m  +   t≤r r−1 = 4  16 m or G u2  j=−n 2 Yj ≥ 0].  16 m or τ utj >  1− ϵ  16 m. Let Yj =  2Gj − 1 χutj. If Yj = +1, it is unless τ utj < ϵ a vote for λ2−t−1b = 1; if Yj = −1, it is a vote for λ2−t−1b = 0; if Yj = 0, it is an  abstention. We will be democratic and set λ2−t−1b = [n 2−1 have the same distribution. Let Z =n 2−1  What is the probability that λ2−t−1b is correct? Let Zj = −1 if χutj ̸= 0 and tjy mod m  ̸= λutj ; otherwise let Zj = χutj.  τ utj < ϵ Since Zj is a function of utj, the random variables Zj are pairwise independent and j=−n 2 Zj; if Z > 0, the value of λ2−t−1b will be correct. The probability that Zj = 0 is 1 2, and the probability that Zj = +1 is ≥ 1 2. So the chance of error, in the branch of the program that has the correct assumptions, is at most Pr Z ≤ 0  ≤ Pr  Z − n E Zj 2 ≥ 9 9 min r, 2t −1, by Chebyshev’s inequality  exercise 3.5–42 . A similar method, with vtj in place of utj, can be used to determine λ2−t−1a with error ≤ 2 9 min r, 2t −1. Eventually we will have ϵ3 2t+6 < 1  2m , so τ2−tb will be the nearest integer to mβt. Then we can compute √ y =  2tb−1τ2−tb  mod m; squaring this quantity will tell us if we are correct.  4 ϵ. Clearly var Zj  ≤ 1 9 n−1ϵ2 = 2  8; therefore E Zj ≥ 3 16 n2ϵ2  ≤ 8  2 − ϵ  4 + ϵ  The total chance of making a mistake is bounded by 4 9  t≥1 2−t = 4 t < lg n, and by 4 9 including the possibility that the χ values were not all correct, is at most 4 9 + 4 At least 1 will be found after repeating the process at most 10 times, on the average.  9 in stages 9 in subsequent stages. So the total chance of error, 90 = 9 10. y; hence the factors of m  10 of all runs of the program will succeed in finding √  The total running time is dominated by O rϵ−4 log rϵ−2 T  G   for the χ compu- tation, plus O r2ϵ−2T  G   for subsequent guessing, plus O r2ϵ−6  for the calculations of αt, βt, λ2−ta, and λ2−tb in all branches.  This procedure, which nicely illustrates many of the basic paradigms of randomized algorithms, is due to R. Fischlin and C. P. Schnorr [J. Cryptology 13  2000 , 221– 244], who derived it from earlier approaches by Alexi, Chor, Goldreich, and Schnorr [SICOMP 17  1988 , 194–209] and by Ben-Or, Chor, and Shamir [STOC 15  1983 , 421–430]. When we combine it with Lemma 3.5P4, we get a theorem analogous to Theorem 3.5P, but with the sequence 3.2.2– 16  instead of 3.2.2– 17 . Fischlin and Schnorr showed how to streamline the calculations so that their factoring algorithm takes O rϵ−4 log rϵ−1 T  G   steps; the resulting time bound for “cracking” 3.2.2– 16  is T  F   = O RN 4ϵ−4 log RN ϵ−1  T  G  + R2  . The constant factor implied by this O is rather large, but not enormous. A similar method finds x from the RSA function y = xa mod m when a ⊥ φ m , if we can guess y1 a mod 2 with probability ≥ 1  2 + ϵ. j=0 aijxj ≡ 0  modulo mi , gcd ai0, ai1, . . . , ai d−1 , mi  = 1, and x < mi for 1 ≤ i ≤ k = d d − 1  2 + 1, where mi ⊥ mj for 1 ≤ i < j ≤ k. Also assume that m = min{m1, . . . , mk} > nn 22n2 2dd, where n = d + k. First find  44. Suppose d−1    9 + 1   4.5.4      M 0 ... 0  ...  670  ANSWERS TO EXERCISES  u1, . . . , uk such that uj mod mi = δij. Then set up the n × n matrix  L =  a10u1 ma11u1 a20u2 ma21u2  . . . md−1a1 d−1 u1 M m1d . . . md−1a2 d−1 u2  ...  . . .  mM  ... 0  ...  md−1M  ...  0 ... 0  M m2d  ... 0  ... . . . M mkd  ak0uk mak1uk  . . . md−1ak d−1 uk  n  If   b  −ja  = −1; then x2  where M = m1m2 . . . mk; all entries above the diagonal are zero, hence det L = have length vL  < M d. Let cj = tjM +k M n−1mk−1d−k. Now let v =  t0, . . . , td−1, v1, . . . , vk  be a nonzero integer vector with length vL  ≤ √ n2nM  n−1  nm k−1  nd−k n. Since M  n−1  n < M mk n, we i=1 aijuivi and P  x  = c0 + c1x + ··· + cd−1xd−1. Then P  x  ≡ vi ai0 + ai1x + ··· + ai d−1 xd−1  ≡ 0  modulo mi , for 1 ≤ i ≤ k; hence P  x  ≡ 0  modulo M . Also mjcj < M d; it follows that P  x  = 0. But P  x  is not identically zero, because the conditions viaij ≡ 0  modulo mi  and gcd ai0, . . . , ai d−1 , mi  = 1 imply vi ≡ 0  modulo mi , while  viM mid < M d implies vi < mi; we cannot have v1 = ··· = vk = 0. Thus we can find x  more precisely, at most d − 1 possibilities for x , and the total running time is polynomial in lg M. [Lecture Notes in Comp. Sci. 218  1985 , 403–408.] n  = 1, 45. Fact 1. A solution always exists. Suppose first that n is prime. n  = −1, let j > 0 be minimum such that we have there is a solution with y = 0. If   b 0 − a ≡ −ja and b ≡ −ja y0 2 for some x0 and y0  modulo n , hence  x0y0 2 − ay2 0 ≡ b. Suppose next that we have found a solution x2 − ay2 ≡ b  modulo n  and we want to extend this to a solution modulo n2. We can always find c and d such that  x+cn 2−a y+dn 2 ≡ b  modulo n2 , because  x+cn 2−a y+dn 2 ≡ x2 − ay2 +  2cx − 2ayd n and gcd 2x, 2ay  ⊥ n. Thus a solution always exists when n is a power of an odd prime.  We need to assume that n is odd because, for example, there is no solution to x2 ± y2 ≡ 3  modulo 8 .  Finally, a solution exists for all odd n, by the Chinese remainder theorem. Fact 2. The number of solutions, given a and n with a ⊥ n, is the same for all b ⊥ n. This follows from the hinted identity and Fact 1, for if x2 1 ≡ b then  x1x2 − ay1y2, x1y2 + x2y1  runs through all solutions of x2 − ay2 ≡ b as  x2, y2  runs through all solutions of x2 − ay2 ≡ 1. In other words,  x2, y2  is uniquely determined by  x1, y1  and  x, y , when x2 Fact 3. Given integers  a, s, z  such that z2 ≡ a  modulo s , we can find integers 3a. For if z2 = a+ms,  x, y, m, t  with x2−ay2 = m2st, where  x, y  ̸=  0, 0  and t2 ≤ 4 let  u, v  be a nonzero pair of integers that minimizes  zu + mv 2 + au2. We can find  u, v  efficiently using the methods of Section 3.3.4, and  zu + mv 2 +au2 ≤   4 3a 1 2 by exercise 3.3.4–9. Therefore  zu + mv 2 − au2 = mt where t2 ≤ 4 3a. The hinted identity now solves x2 − ay2 =  ms  mt . Fact 4. It is easy to solve x2−y2 ≡ b  modulo n : Let x =  b+1  2, y =  b−1  2. Fact 5. It is not difficult to solve x2 + y2 ≡ b  modulo n , because the method in exercise 3.3.4–11 solves x2 + y2 = p when p is prime and p mod 4 = 1; one of the numbers b, b + n, b + 2n, . . . will be such a prime. Now to solve the stated problem when a > 1 we can proceed as follows. Choose u and v at random between 1 and n − 1, then compute w =  u2 − av2  mod n and  1 − ay2  1 − ay2  1 ⊥ n.   4.6  ANSWERS TO EXERCISES  671  j  j=1 peij  d = gcd w, n . If 1   1 we can reduce n; the methods used to prove Fact 1 will lift solutions for factors of n to solutions for n itself. If d = n and v ⊥ n, we have  u v 2 ≡ a  modulo n , hence we can reduce a to 1. Otherwise d = 1; let s = bw mod n. This number s is uniformly distributed among the numbers s   = 1, try to solve z2 ≡ a  modulo s , assuming that s is prime to n, by Fact 2. If   a prime  exercise 4.6.2–15 . If unsuccessful, start over with another random choice of u and v. If successful, let z2 = a + ms and compute d = gcd ms, n . If d > 1, reduce the problem as before. Otherwise use Fact 3 to find x2 − ay2 = m2st with t2 ≤ 4 3a; this makes  x m 2 − a y m 2 ≡ st  modulo n . If t = 0, reduce a to 1. Otherwise apply the algorithm recursively to solve X2 − tY 2 ≡ a  modulo n .  Since t is much smaller than a, only O log log n  levels of recursion will be necessary.  If gcd Y, n  > 1 we can reduce n or a; otherwise  X Y  2 − a 1 Y  2 ≡ t  modulo n . Finally the hinted identity yields a solution to x′2 − ay′2 ≡ s  see Fact 2 , which leads in turn to the desired solution because u2 − av2 ≡ s b.  j, we have n + n′ ≡ m  In practice only O log n  random trials are needed before the assumptions about prime numbers made in this algorithm turn out to be true. But a formal proof would re- quire us to assume the Extended Riemann Hypothesis [IEEE Trans. IT-33  1987 , 702– 709]. Adleman, Estes, and McCurley [Math. Comp. 48  1987 , 17–28] have developed a slower and more complicated algorithm that does not rely on any unproved hypotheses. for enough ni, i xijkeij +  p − 1 tjk = δjk in integers xijk, tjk for 1 ≤ j, k ≤ m  for i xijkejk  mod  p−1  j=1 e′jNj [Improved algorithms are known; see, for example, Coppersmith,  46. [FOCS 20  1979 , 55–60.] After finding ani mod p = m we can solve  example, as in 4.5.2– 23  , thereby knowing the solutions Nj =   to aNj mod p = pj. Then if ban′ mod p = m   modulo p − 1 . Odlyzko, and Schroeppel, Algorithmica 1  1986 , 1–15.] 47. Earlier printings of this book had a 211-digit N, which was cracked in 2012 using the elliptic curve method and the general number field method by Greg Childers and about 500 volunteers ! . SECTION 4.6 1. 9x2 + 7x + 7; 5x3 + 7x2 + 2x + 6. 2.  a  True.  b  False if the algebraic system S contains zero divisors, that is, nonzero numbers whose product is zero, as in exercise 1; otherwise true.  c  True when m ̸= n, but false in general when m = n, since the leading coefficients might cancel. 3. Assume that r ≤ s. For 0 ≤ k ≤ r the maximum is m1m2 k + 1 ; for r ≤ k ≤ s it is m1m2 r + 1 ; for s ≤ k ≤ r + s it is m1m2 r + s + 1 − k . The least upper bound valid for all k is m1m2 r + 1 .  The solver of this exercise will know how to factor the polynomial x7 + 2x6 + 3x5 + 3x4 + 3x3 + 3x2 + 2x + 1.  4. If one of the polynomials has fewer than 2t nonzero coefficients, the product can be formed by putting exactly t− 1 zeros between each of the coefficients, then multiplying in the binary number system, and finally using a bitwise AND instruction  present on most binary computers, see Algorithm 4.5.4D  to zero out the extra bits. For example, if t = 3, the multiplication in the text would become  1001000001 2 ×  1000001001 2 =  1001001011001001001 2; the desired answer is obtained if we AND this result with the constant  1001001 . . . 1001 2. A similar technique can be used to multiply polynomials with nonnegative coefficients that are not too large. 5. Polynomials of degree ≤ 2n can be written U1 x xn + U0 x  where deg U1  ≤ n and deg U0  ≤ n; and  U1 x xn + U0 x   V1 x xn + V0 x   = U1 x V1 x  x2n + xn  +  e′ j=1 pȷ   672  ANSWERS TO EXERCISES  4.6   U1 x  + U0 x   V1 x  + V0 x  xn + U0 x V0 x  xn + 1 .  This equation assumes that arithmetic is being done modulo 2.  Thus Eqs. 4.3.3– 3  and 4.3.3– 5  hold.  Notes: S. A. Cook has shown that Algorithm 4.3.3T can be extended in a similar way; and A. Schönhage [Acta Informatica 7  1977 , 395–398] has explained how to multiply polynomials mod 2 with only O n log n log log n  bit operations. In fact, polynomials over any ring S can be multiplied with only O n log n log log n  algebraic operations, even when S is an algebraic system in which multiplication need not be commutative or associative [D. G. Cantor and E. Kaltofen, Acta Informatica 28  1991 , 693–701]. See also exercises 4.6.4–57 and 4.6.4–58. But these ideas are not useful for sparse polynomials  having mostly zero coefficients .  SECTION 4.6.1 1. q x  = 1 · 23x3 + 0 · 22x2 − 2 · 2x + 8 = 8x3 − 4x + 8; r x  = 28x2 + 4x + 8. 2. The monic sequence of polynomials produced during Euclid’s algorithm has the coefficients  1, 5, 6, 6, 1, 6, 3 ,  1, 2, 5, 2, 2, 4, 5 ,  1, 5, 6, 2, 3, 4 ,  1, 3, 4, 6 , 0. Hence the greatest common divisor is x3 + 3x2 + 4x + 6.  The greatest common divisor of a polynomial and its reverse is always symmetric, in the sense that it is a unit multiple of its own reverse.  3. The procedure of Algorithm 4.5.2X is valid, with polynomials over S substituted for integers. When the algorithm terminates, we have U x  = u2 x , V  x  = u1 x . Let m = deg u , n = deg v . It is easy to prove by induction that deg u3  + deg v1  = n, deg u3  + deg v2  = m, after step X3, throughout the execution of the algorithm, provided that m ≥ n. Hence if m and n are greater than d = deg gcd u, v   we have deg U  < m − d, deg V   < n − d; the exact degrees are m − d1 and n − d1, where d1 is the degree of the next-to-last nonzero remainder. If d = min m, n , say d = n, we have U x  = 0 and V  x  = 1. When u x  = xm − 1 and v x  = xn − 1, the identity  xm − 1  mod  xn − 1  = xm mod n − 1 shows that all polynomials occurring during the calculation are monic, with integer coefficients. When u x  = x21 − 1 and v x  = x13 − 1, we have V  x  = x11 + x8 + x6 + x3 + 1 and U x  = − x19 + x16 + x14 + x11 + x8 + x6 + x3 + x . [See also Eq. 3.3.3– 29 , which gives an alternative formula for U x  and V  x . See also exercise 4.3.2–6, with 2 replaced by x.] 4. Since the quotient q x  depends only on v x  and the first m−n coefficients of u x , the remainder r x  = u x − q x v x  is uniformly distributed and independent of v x . Hence each step of the algorithm may be regarded as independent of the others; this algorithm is much more well-behaved than Euclid’s algorithm over the integers. The probability that n1 = n − k is p1−k 1 − 1 p , and t = 0 with probability p−n. Each succeeding step has essentially the same behavior; hence we can see that any given sequence of degrees n, n1, . . . , nt, −∞ occurs with probability  p − 1 t pn. To find the average value of f n1, . . . , nt , let St be the sum of f n1, . . . , nt  over all sequences t St p − 1 t pn.  n > n1 > ··· > nt ≥ 0 having a given value of t; then the average is t, so the average is n 1 − 1 p . Similarly, if Let f n1, . . . , nt  = t; then St =n  1 − 1 p . , and the average is n n−1 f n1, . . . , nt  = n1 + ··· + nt, then St = n St =n+2 ,  +n+1  −  n + 1 n+1 n Finally, if f n1, . . . , nt  =  n − n1 n1 + ··· +  nt−1 − nt nt, then and the average isn+1  −  n + 1 p  p − 1  +  p  p − 1  2 1 − 1 pn+1 .  t−1  t+1  t+2  2  2  2  t  t  2   4.6.1  ANSWERS TO EXERCISES  673   The probability that nj+1 = nj − 1 for 1 ≤ j ≤ t = n is  1 − 1 p n, obtained by setting St = [t = n]; so this probability approaches 1 as p → ∞. As a consequence we have further evidence for the text’s claim that Algorithm C almost always finds δ2 = δ3 = ··· = 1, because any polynomials that fail the latter condition will fail the former condition modulo p for all p.  5. Using the formulas developed in exercise 4, with f n1, . . . , nt  = [nt = 0], we find that the probability is 1 − 1 p if n > 0, 1 if n = 0. 6. Assuming that the constant terms u 0  and v 0  are nonzero, imagine a “right- to-left” division algorithm, u x  = v x q x  + xm−nr x , where deg r  < deg v . We obtain a gcd algorithm analogous to Algorithm 4.5.2B, which is essentially Euclid’s algorithm applied to the “reverse” of the original inputs  see exercise 2 , afterwards reversing the answer and multiplying by an appropriate power of x.  There is a similar algorithm analogous to the method of exercise 4.5.2–40. The average number of iterations for both algorithms has been found by G. H. Norton, SICOMP 18  1989 , 608–624; K. Ma and J. von zur Gathen, J. Symbolic Comp. 9  1990 , 429–455. 7. The units of S  as polynomials of degree zero . 8. If u x  = v x w x , where u x  has integer coefficients while v x  and w x  have rational coefficients, there are nonzero integers m and n such that m· v x  and n· w x  have integer coefficients. Now u x  is primitive, so Eq.  4  implies that  u x  = pp  m · v x   n · w x    = ± pp m · v x   pp n · w x  .  9. We can extend Algorithm E as follows: Let  u1 x , u2 x , u3, u4 x   and  v1 x , v2 x , v3, v4 x   be quadruples that satisfy the relations u1 x u x +u2 x v x  = u3u4 x  and v1 x u x  + v2 x v x  = v3v4 x . The extended algorithm starts with the quadru- ples  1, 0, cont u , pp u x    and  0, 1, cont v , pp v x    and manipulates them in such a way as to preserve the conditions above, where u4 x  and v4 x  run through the same sequence as u x  and v x  do in Algorithm E. If au4 x  = q x v4 x  + br x , we have av3 u1 x , u2 x   − q x u3 v1 x , v2 x   =  r1 x , r2 x  , where r1 x u x  + r2 x v x  = bu3v3r x , so the extended algorithm can preserve the desired relations. If u x  and v x  are relatively prime, the extended algorithm eventually finds r x  of degree zero, and we obtain U x  = r2 x , V  x  = r1 x  as desired.  In practice we would divide r1 x , r2 x , and bu3v3 by gcd cont r1 , cont r2  .  Conversely, if such U x  and V  x  exist, then u x  and v x  have no common prime divisors, since they are primitive and have no common divisors of positive degree. 10. By successively factoring reducible polynomials into polynomials of smaller de- gree, we must obtain a finite factorization of any polynomial into irreducibles. The factorization of the content is unique. To show that there is at most one factorization of the primitive part, the key result is to prove that if u x  is an irreducible factor of v x w x , but not a unit multiple of the irreducible polynomial v x , then u x  is a factor of w x . This can be proved by observing that u x  is a factor of v x w x U x  = rw x  − w x u x V  x  by the result of exercise 9, where r is a nonzero constant. 11. The only row names needed would be A1, A0, B4, B3, B2, B1, B0, C1, C0, D0. In general, let uj+2 x  = 0; then the rows needed for the proof are An2−nj through A0, Bn1−nj through B0, Cn2−nj through C0, Dn3−nj through D0, etc. 12. If nk = 0, the text’s proof of  24  shows that the value of the determinant is ±hk, and this equals ±ℓnk−1 . If the polynomials have a factor of positive     1<j<k ℓδj−1 δj−1   k  j   674  ANSWERS TO EXERCISES  4.6.1  i=1   −1 mnbmu β1  . . . u βn  = anbmm  degree, we can artificially assume that the polynomial zero has degree zero and use the same formula with ℓk = 0. Notes: The value R u, v  of Sylvester’s determinant is called the resultant of u and v, and the quantity  −1 deg u  deg u −1  2ℓ u −1R u, u′  is called the discriminant of u, where u′ is the derivative of u. If u x  has the factored form a x − α1  . . .  x − αm , n and if v x  = b x − β1  . . .  x − βn , the resultant R u, v  is anv α1  . . . v αm  = j=1 αi − βj . It follows that the polynomials of degree mn in y defined as the respective resultants with v x  of u y − x , u y + x , xmu y x , and u yx  have as respective roots the sums αi + βj, differences αi − βj, products αiβj, and quotients αi βj  when v 0  ̸= 0 . This idea has been used by R. G. K. Loos to construct algorithms for arithmetic on algebraic numbers [Computing, Supplement 4  1982 , 173–187].  If we replace each row Ai in Sylvester’s matrix by  b0Ai + b1Ai+1 + ··· + bn2−1−iAn2−1  −  a0Bi + a1Bi+1 + ··· + an2−1−iBn2−1 ,  and then delete rows Bn2−1 through B0 and the last n2 columns, we obtain an n1 × n1 determinant for the resultant instead of the original  n1 + n2 ×  n1 + n2  determinant. In some cases the resultant can be evaluated efficiently by means of this determinant; see CACM 12  1969 , 23–30, 302–303.  J. T. Schwartz has shown that it is possible to evaluate resultants and Sturm sequences for polynomials of degree n with a total of O n log n 2  arithmetic operations as n → ∞. [See JACM 27  1980 , 701–717.] 13. One can show by induction on j that the values of  uj+1 x , gj+1, hj  are replaced respectively by  ℓ1+pj w x uj x , ℓ2+pj gj, ℓpj hj  for j ≥ 2, where pj = n1 + n2 − 2nj. [In spite of this growth, the bound  26  remains valid.] 14. Let p be a prime of the domain, and let j, k be maximum such that pk\vn = ℓ v , pj\vn−1. Let P = pk. By Algorithm R we may write q x  = a0 + P a1x +··· + P sasxs, where s = m− n ≥ 2. Let us look at the coefficients of xn+1, xn, and xn−1 in v x q x , namely P a1vn + P 2a2vn−1 + ··· , a0vn + P a1vn−1 + ··· , and a0vn−1 + P a1vn−2 + ··· , each of which is a multiple of P 3. We conclude from the first that pj\a1, from the second that pmin k,2j \a0, then from the third that P\a0. Hence P\r x . [If m were only n + 1, the best we could prove would be that p⌈k 2⌉ divides r x ; for example, consider u x  = x3 + 1, v x  = 4x2 + 2x + 1, r x  = 18. On the other hand, an argument based on determinants of matrices like  21  and  22  can be used to show that ℓ r deg v −deg r −1r x  is always a multiple of ℓ v  deg u −deg v   deg v −deg r −1 .] 15. Let cij = ai1aj1 + ··· + ainajn; we may assume that cii > 0 for all i. If cij ̸= 0 for some i ̸= j, we can replace row i and column i by  ci1 − tcj1, . . . , cin − tcjn , where t = cij cjj; this does not change the value of det C, and it decreases the value of the upper bound we wish to prove, since cii is replaced by cii − c2 ij cjj. Such replacements can be done in a systematic way for increasing i and for j < i, until cij = 0 for all i ̸= j. [The latter algorithm is called the Gram–Schmidt orthogonalization process: See Crelle 94  1883 , 41–73; Math. Annalen 63  1907 , 442.] Then det A 2 = det AAT   = c11 . . . cnn. 16. A univariate polynomial of degree d over any unique factorization domain has at most d roots  see exercise 3.2.1.2–16 b  ; so if n = 1 it is clear that r S1  ≤ d1. If n > 1 we have f x1, . . . , xn  = g0 x2, . . . , xn  + x1g1 x2, . . . , xn  + ··· + xd1 1 gd1 x2, . . . , xn  where gk is nonzero for at least one k. Given  x2, . . . , xn , it follows that f x1, . . . , xn  is zero for at most d1 choices of x1, unless gk x2, . . . , xn  = 0; hence r S1, . . . , Sn  ≤   4.6.1  ANSWERS TO EXERCISES  675          d1 S2−d2  . . .  Sn−dn +S1 S2 . . .Sn− S2−d2  . . .  Sn−dn  . [R. A. DeMillo and R. J. Lipton, Inf. Proc. Letters 7  1978 , 193–195.]  polynomial f x1, . . . , xn  = {xj − sk  sk ∈ Sj, 1 ≤ k ≤ dj, 1 ≤ j ≤ n}. But  Notes: The stated upper bound is best possible, because equality occurs for the  there is another sense in which the upper bound can be significantly improved: Let f1 x1, . . . , xn  = f x1, . . . , xn , and let fj+1 xj+1, . . . , xn  be any nonzero coefficient of a power of xj in fj xj, . . . , xn . Then we can let dj be the degree of xj in fj instead of the  often much larger  degree of xj in f. For example, we could let d1 = 3 and d2 = 1 in the polynomial x3 2 + 5. This observation ensures that d1 + ··· + dn ≤ d when each term of f has total degree ≤ d; hence the probability in such cases is  1x2 + x100  2 − 3x2  1x9  r S, . . . , S   ≤ d1 + ··· + dn  . . .  S  S  ≤ 1 −  1 − d1 S  1 − dnS when all sets Sj are equal. If this probability is ≤ 1 2, and if f x1, . . . , xn  turns out to be zero for 50 randomly selected vectors  x1, . . . , xn , then f x1, . . . , xn  is identically zero with probability at least 1 − 2−50. j fj+1 xj+1, . . . , xn  with ej > 0 we can take dj = 1, because xj must then be 0 when fj+1 xj+1, . . . , xn  ̸= 0. A sparse polynomial with only m nonzero terms will therefore have dj ≤ 1 for at least n − lg m values of j.  Moreover, if fj xj, . . . , xn  has the special form xej  ≤ d S  Applications of this inequality to gcd calculation and other operations on sparse multivariate polynomials were introduced by R. Zippel, Lecture Notes in Comp. Sci. 72  1979 , 216–226. J. T. Schwartz [JACM 27  1980 , 701–717] gave further extensions, including a way to avoid large numbers by means of modular arithmetic: If the coeffi- cients of f are integers, if P is a set of prime numbers all ≥ q, and if f x1, . . . , xn  ≤ L whenever each xj ∈ Sj, then the number of solutions to f x1, . . . , xn  ≡ 0  modulo p  for p ∈ P is at most  S1 . . .SnP −  S1 − d1  . . .  Sn − dn  P − logq L .  17.  a  For convenience, let us describe the algorithm only for A = {a, b}. The hy- potheses imply that deg Q1U  = deg Q2V   ≥ 0, deg Q1  ≤ deg Q2 . If deg Q1  = 0, then Q1 is just a nonzero rational number, so we set Q = Q2 Q1. Otherwise we let Q1 = aQ11 + bQ12 + r1, Q2 = aQ21 + bQ22 + r2, where r1 and r2 are rational numbers; it follows that  Q1U − Q2V = a Q11U − Q21V   + b Q12U − Q22V   + r1U − r2V.  We must have either deg Q11  = deg Q1 −1 or deg Q12  = deg Q1 −1. In the former case, deg Q11U − Q21V   < deg Q11U , by considering the terms of highest degree that start with a; so we may replace Q1 by Q11, Q2 by Q21, and repeat the process. Similarly in the latter case, we may replace  Q1, Q2  by  Q12, Q22  and repeat the process.  b  We may assume that deg U  ≥ deg V  . If deg R  ≥ deg V  , note that Q1U − Q2V = Q1R− Q2 − Q1Q V has degree less than deg V   ≤ deg Q1R , so we can repeat the process with U replaced by R; we obtain R = Q′V + R′, U =  Q + Q′ V + R′, where deg R′  < deg R , so eventually a solution will be obtained.   c  The algorithm of  b  gives V1 = U V2 + R, deg R  < deg V2 ; by homogeneity,  R = 0 and U is homogeneous.   676  ANSWERS TO EXERCISES  4.6.1   d  We may assume that deg V   ≤ deg U . If deg V   = 0, set W ← U; otherwise use  c  to find U = QV, so that QV V = V QV,  QV − V Q V = 0. This implies that QV = V Q, so we can set U ← V, V ← Q and repeat the process.  For further details about the subject of this exercise, see P. M. Cohn, Proc. Cambridge Phil. Soc. 57  1961 , 18–30. The considerably more difficult problem of characterizing all string polynomials such that U V = V U has been solved by G. M. Bergman [Ph.D. thesis, Harvard University, 1967]. 18. [P. M. Cohn, Transactions of the Amer. Math. Soc. 109  1963 , 332–356.]  S1. Set u1 ← U1, u2 ← U2, v1 ← V1, v2 ← V2, z1 ← z′2 ← w1 ← w′2 ← 1,  z′1 ← z2 ← w′1 ← w2 ← 0, n ← 0.  S2.  At this point the identities given in the exercise hold, and u1v1 = u2v2; v2 = 0 if and only if u1 = 0.  If v2 = 0, the algorithm terminates with gcrd V1, V2  = v1, lclm V1, V2  = z′1V1 = −z′2V2.  Also, by symmetry, we have gcld U1, U2  = u2 and lcrm U1, U2  = U1w1 = −U2w2.   S3. Find Q and R such that v1 = Qv2 + R, where deg R  < deg v2 .  We have  u1 Qv2 + R  = u2v2, so u1R =  u2 − u1Q v2 = R′v2.   S4. Set  w1, w2, w′1, w′2, z1, z2, z′1, z′2, u1, u2, v1, v2  ←  w′1 − w1Q, w′2 − w2Q, w1, w2, z′1, z′2, z1 − Qz′1, z2 − Qz′2, u2 − u1Q, u1, v2, v1 − Qv2  and n ← n + 1. Go back to S2.  This extension of Euclid’s algorithm includes most of the features we have seen in previous extensions, all at the same time, so it provides new insight into the special cases already considered. To prove that it is valid, note first that deg v2  decreases in step S4, so the algorithm certainly terminates. At the conclusion of the algorithm, v1 is a common right divisor of V1 and V2, since w1v1 =  −1 nV1 and −w2v1 =  −1 nV2; also if d is any common right divisor of V1 and V2, it is a right divisor of z1V1 + z2V2 = v1. Hence v1 = gcrd V1, V2 . Also if m is any common left multiple of V1 and V2, we may assume without loss of generality that m = U1V1 = U2V2, since the sequence of values of Q does not depend on U1 and U2. Hence m =  −1 n −u2z′1 V1 =  −1 n u2z′2 V2 is a multiple of z′1V1. In practice, if we just want to calculate gcrd V1, V2 , we may suppress the compu- tation of n, w1, w2, w′1, w′2, z1, z2, z′1, z′2. These additional quantities were added to the algorithm primarily to make its validity more readily established.  Note: Nontrivial factorizations of string polynomials, such as the example given  with this exercise, can be found from matrix identities such as  0  1  1 0  0  1  1 −c  0  1  1 −b    1 −a  =  1  0    ,  0 1   c  1  1 0   b  1  1 0   a  1  since these identities hold even when multiplication is not commutative. For example,   abc + a + c  1 + ba  =  ab + 1  cba + a + c .  Compare this with the continuant polynomials of Section 4.5.3.  19. [See Eugène Cahen, Théorie des Nombres 1  Paris: 1914 , 336–338.] If such an algorithm exists, D is a gcrd by the argument in exercise 18. Let us regard A and B as a single 2n × n matrix C whose first n rows are those of A, and whose second n rows are those of B. Similarly, P and Q can be combined into a 2n × n matrix R; X and Y can be combined into an n × 2n matrix Z. The desired conditions now reduce to two equations C = RD, D = ZC. If we can find a 2n × 2n integer matrix U   4.6.1  ANSWERS TO EXERCISES  677  with determinant ±1 such that the last n rows of U−1C are all zero, then R =  first n columns of U , D =  first n rows of U−1C , Z =  first n rows of U−1  solves the desired conditions. Hence, for example, the following algorithm may be used  with m = 2n : Algorithm T  Triangularization . Let C be an m × n matrix of integers. This algorithm finds m × m integer matrices U and V such that U V = I and V C is upper triangular.  This means that the entry in row i and column j of V C is zero if i > j.  T1. [Initialize.] Set U ← V ← I, the m × m identity matrix; and set T ← C.   Throughout the algorithm we will have T = V C and U V = I.   T2. [Iterate on j.] Do step T3 for j = 1, 2, . . . , min m, n , then terminate the  algorithm.  T3. [Zero out column j.] Perform the following actions zero or more times until Tij is zero for all i > j: Let Tkj be a nonzero element of {Tij, T i+1 j, . . . , Tmj} having the smallest absolute value. Interchange rows k and j of T and of V; interchange columns k and j of U. Then subtract ⌊Tij Tjj⌋ times row j from row i, in matrices T and V, and add the same multiple of column i to column j in matrix U, for j < i ≤ m.  2  3  0  2  2  2  2  3   1 5 0  2   1 0 0  4  +   0 2 1  −2   1  1  =   4 3 2  4  =   1 2 3  −1 ,   4  −1  =   1  −1 , 1 .  Actually any matrix with determinant ±1 would  For the stated example, the algorithm yields   1 3   1 0   4 0 3 0 2 be a gcrd in this particular case.  20. See V. Y. Pan, Information and Computation 167  2001 , 71–85. 21. To get an upper bound, we may assume that Algorithm R is used only when m − n ≤ 1; furthermore, the coefficients are bounded by  26  with m = n. [The stated formula is, in fact, the execution time observed in practice, not merely an upper bound. For more detailed information see G. E. Collins, Proc. 1968 Summer Inst. on Symbolic Mathematical Computation, edited by Robert G. Tobey  IBM Federal Systems Center: June 1969 , 195–231.] 22. A sequence of signs cannot contain two consecutive zeros, since uk+1 x  is a nonzero constant in  29 . Moreover we cannot have “+, 0, +” or “−, 0, −” as subsequences. The formula V  u, a  − V  u, b  is clearly valid when b = a, so we must only verify it as b increases. The polynomials uj x  have finitely many roots, and V  u, b  changes only when b encounters or passes such roots. Let x be a root of some  possibly several  uj. When b increases from x − ϵ to x, the sign sequence near j goes from “+, ±, −” to “+, 0, −” or from “−, ±, +” to “−, 0, +” if j > 0; and from “+, −” to “0, −” or from “−, +” to “0, +” if j = 0.  Since u′ x  is the derivative, u′ x  is negative when u x  is decreasing.  Thus the net change in V is −δj0. When b increases from x to x + ϵ, a similar argument shows that V remains unchanged.  [L. E. Heindel, JACM 18  1971 , 533–548, has applied these ideas to construct algorithms for isolating the real zeros of a given polynomial u x , in time bounded by a polynomial in deg u  and log N, where all coefficients yj are integers with uj ≤ N, and all operations are guaranteed to be exact.] 23. If v has n−1 real roots occurring between the n real roots of u, then  by considering sign changes  u x  mod v x  has n − 2 real roots lying between the n − 1 roots of v. 24. First show that hj = gδj−1 . Then show that the exponent of g2 on the left-hand side of  18  has the form δ2 + δ1x, where x =  . . . gδ1 1−δ2 ... 1−δj−1   gδj−2 1−δj−1  j−1  2  j   ANSWERS TO EXERCISES  678 4.6.1 δ2 + ··· + δj−1 + 1 − δ2 δ3 + ··· + δj−1 + 1  − δ3 1 − δ2  δ4 + ··· + δj−1 + 1  − ··· − δj−1 1 − δ2  . . .  1 − δj−2  1 . But x = 1, since it is seen to be independent of δj−1 and we can set δj−1 = 0, etc. A similar derivation works for g3, g4, . . . , and a simpler derivation works for  23 . 25. Each coefficient of uj x  can be expressed as a determinant in which one column contains only ℓ u , ℓ v , and zeros. To use this fact, modify Algorithm C as follows: In step C1, set g ← gcd ℓ u , ℓ v   and h ← 0. In step C3, if h = 0, set u x  ← v x , v x  ← r x  g, h ← ℓ u δ g, g ← ℓ u , and return to C2; otherwise proceed as in the unmodified algorithm. The effect of this new initialization is simply to replace uj x  by uj x  gcd ℓ u , ℓ v   for all j ≥ 3; thus, ℓ2j−4 will become ℓ2j−5 in  28 . 26. In fact, even more is true. Note that the algorithm in exercise 3 computes ±pn x  and ∓qn x  for n ≥ −1. Let en = deg qn  and dn = deg pnu−qnv ; we observed in exer- cise 3 that dn−1+en = deg u  for n ≥ 0. We shall prove that the conditions deg q  < en and deg pu−qv  < dn−2 imply that p x  = c x pn−1 x  and q x  = c x qn−1 x : Given such p and q, we can find c x  and d x  such that p x  = c x pn−1 x  + d x pn x  and q x  = c x qn−1 x  + d x qn x , since pn−1 x qn x  − pn x qn−1 x  = ±1. Hence pu− qv = c pn−1u− qn−1v  + d pnu− qnv . If d x  ̸= 0, we must have deg c  + en−1 = deg d  + en, since deg q    deg d  + dn, since this is surely true if dn = −∞ and otherwise we have dn−1+en = dn+en+1 > dn+en−1. Therefore deg pu − qv  = deg c  + dn−1. But we have assumed that deg pu − qv  < dn−2 = dn−1 + en − en−1; so deg c  < en − en−1 and deg d  < 0, a contradiction. [This result is essentially due to L. Kronecker, Monatsberichte Königl. preuß. Akad. Wiss.  Berlin: 1881 , 535–600. It implies the following theorem: “Let u x  and v x  be relatively prime polynomials over a field and let d ≤ deg v  < deg u . If q x  is a polynomial of least degree such that there exist polynomials p x  and r x  with p x u x − q x v x  = r x  and deg r  = d, then p x  q x  = pn x  qn x  for some n.” For if dn−2 > d ≥ dn−1, there are solutions q x  with deg q  = en−1 + d − dn−1 < en, and we have proved that all solutions of such low degree have the stated property.] 27. The ideas of answer 4.3.1–40 apply, but in simpler fashion because polynomial arithmetic is carry-free; right-to-left division uses 4.7– 3 . Alternatively, with large values of n, we could divide Fourier transforms of the coefficients, using exercise 4.6.4– 57 in reverse.  p  1.3.3 , the number of polynomials without linear factors is  SECTION 4.6.2 1. For any choice of k ≤ n distinct roots, there are pn−k monic polynomials having those roots at least once. Therefore by the principle of inclusion and exclusion  Section is alternately ≤ and ≥ the partial sums of this series. The stated bounds correspond to k ≤ 2 and k ≤ 3. When n ≥ p the probability of at least one linear factor is 1 −  1 − 1 p p. The average number of linear factors is p times the average number of times x divides u x , so it is 1 + p−1 + ··· + p1−n = p  pn−k −1 k, and it   −1 kp−2k; this probability lies between 3  2 − 1 1 average number of such factors is 1  [In a similar way, we find that there is an irreducible factor of degree 2 with 4 p−1 and 2 p−1  + O p−2  as n → ∞. The 2 p−1 when n ≥ 2 and it approaches 1 − e−1 2 1 + 1 Note: Let u x  be a fixed polynomial with integer coefficients. Peter Weinberger has observed that, if u x  is irreducible over the integers, the average number of linear  probability   p p−1  2  p−1 1 − p−n .  2 p−2⌊n 2⌋.]  8 − 1  2 − 1  k≤n 2  k≤n  k  k   4.6.2  ANSWERS TO EXERCISES  679  factors of u x  modulo p approaches 1 as p → ∞, because the Galois group of u x  is transitive and the average number of 1-cycles in a randomly chosen element of any transitive permutation group is 1. Thus, the average number of linear factors of u x  modulo p is the number of irreducible factors of u x  over the integers, as p → ∞. [See the remarks in the answer to exercise 37, and Proc. Symp. Pure Math. 24  Amer. Math. Soc., 1972 , 321–332.] 2.  a  We know that u x  has a representation as a product of irreducible polynomi- als; and the leading coefficients of these polynomials must be units, since they divide the leading coefficient of u x . Therefore we may assume that u x  has a representation as a product of monic irreducible polynomials p1 x e1 . . . pr x er, where p1 x , . . . , pr x  are distinct. This representation is unique, except for the order of the factors, so the conditions on u x , v x , w x  are satisfied if and only if  .  . . . pr x er mod 2  w x  = p1 x e1 mod 2  v x  = p1 x ⌊e1 2⌋ . . . pr x ⌊er  2⌋,  b  The generating function for the number of monic polynomials of degree n is 1+pz+p2z2+··· = 1  1−pz . The generating function for the number of polynomials of degree n having the form v x 2, where v x  is monic, is 1+pz2+p2z4+··· = 1  1−pz2 . If the generating function for the number of monic squarefree polynomials of degree n is g z , then we must have 1  1 − pz  = g z   1 − pz2  by part  a . Hence g z  =  1− pz2   1− pz  = 1 + pz +  p2 − p z2 +  p3 − p2 z3 +··· . The answer is pn − pn−1 for n ≥ 2. [Curiously, this proves that u x  ⊥ u′ x  with probability 1−1 p; it is the same as the probability that u x  ⊥ v x  when u x  and v x  are independent, by exercise 4.6.1–5.]  Note: By a similar argument, every u x  has a unique representation v x w x r, where v x  is not divisible by the rth power of any irreducible; the number of such monic polynomials v x  is pn − pn−r+1 for n ≥ r. 3. Let u x  = u1 x  . . . ur x . There is at most one such v x , by the argument of Theorem 4.3.2C. There is at least one if, for each j, we can solve the system with k̸=j uk x , where  wj x  = 1 and wk x  = 0 for k ̸= j. A solution to the latter is v1 x   v1 x  and v2 x  can be found satisfying  k̸=j uk x  + v2 x uj x  = 1,  deg v1  < deg uj ,  by the extension of Euclid’s algorithm  exercise 4.6.1–3 .  when deg v  < 2.  logarithms, this can be rewritten  ln 1  1 − pz   =  Over the integers we cannot make v x  ≡ 1  modulo x  and v x  ≡ 0  modulo x−2  n≥1 1 − zn −anp; after taking j≥1 Gp zj  j. m≥1 µ m m−1 ln 1  1 − pzm  ,  4. By unique factorization, we have  1 − pz −1 =  k,j≥1 akpzkj j = The stated identity now yields the answer Gp z  = from which we obtain anp = m≥1 g zm m−t  n,j≥1 µ n g znj n−tj−t =   d\nµ n d pd n; thus limp→∞ anp pn = 1 n.  To prove the stated identity, note that  n\m µ n  = g z .  [The numbers anp were first found by Gauss; see his Werke 2, 219–222.]  v1 x    4.6.2  680  ANSWERS TO EXERCISES  5. Let anpr be the number of monic polynomials of degree n modulo p having exactly k≥1 Gp zk wk k  =  r irreducible factors. Then Gp z, w  = n,r≥0 anprznwr = exp  exp   n≥0 Anpzn = dGp z p, w  dw w=1 =   m≥1 amw ln 1  1 − pz−m   ; see Eq. 1.2.9– 38 . We have =   k≥1 Gp zk pk  Gp z p, 1  n≥1 ln 1  1 − p1−nzn  φ n  n   1 − z , hence Anp = Hn +1 2p+ O p−2  for n ≥ 2. The average value of 2r is [zn]Gp z p, 2  = n + 1 +  n − 1  p + O np−2 .  The variance is of order n3, however: Set w = 4.  6. For 0 ≤ s < p, x−s is a factor of xp−x  modulo p  by Fermat’s theorem. So xp−x is a multiple of lcm x − 0, x − 1, . . . , x −  p − 1   = xp. [Note: Therefore the Stirling   are multiples of p except when k = 1 or k = p. Equation 1.2.6– 45  shows  numbersp that the same statement is valid for Stirling numbersp   of the other kind.]  k  7. The factors on the right are relatively prime, and each is a divisor of u x , so their product divides u x . On the other hand, u x  divides  k  v x p − v x  =  0≤s<p v x  − s ,  so it divides the right-hand side by exercise 4.5.2–2. 8. The vector  18  is the only output whose kth component is nonzero. 9. For example, start with x ← 1 and y ← 1; then repeatedly set R[x] ← y, x ← 2x mod 101, y ← 51y mod 101, one hundred times. 10. The matrix Q − I below has a null space generated by the two vectors v[1] =  1, 0, 0, 0, 0, 0, 0, 0 , v[2] =  0, 1, 1, 0, 0, 1, 1, 1 . The factorization is   x  6 + x  5 + x  4 + x + 1  x  2 + x + 1 .    0 0 0 0 1 1 0 1  p = 2 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1  0 1 0 0 0 0 0 1  0 1 1 0 0 1 1 0  0 0 0 0 0 0 1 1  0 0 0 1 1 0 0 0    0 0 0 0 0 0 1 1    0 0 0 0 2 0 3  p = 5  0 0 2 4 2 4 2  0 0 0 4 3 0 1  0 0 4 4 4 1 4  0 1 3 2 3 3 2  0 4 2 1 2 0 0    0 0 4 1 2 2 1  11. Removing the trivial factor x, the matrix Q − I above has a null space generated by  1, 0, 0, 0, 0, 0, 0  and  0, 3, 1, 4, 1, 2, 1 . The factorization is  x x  2 + 3x + 4  x  5 + 2x  4 + x  3 + 4x  2 + x + 3 .  12. If p = 2,  x + 1 4 = x4 + 1. If p = 8k + 1, Q − I is the zero matrix, so there are four factors. For other values of p we have  p = 8k + 5   0  p = 8k + 3  0 0 0 0 −1 0 1 0 −2 0 0 0 −1 0 1     0  0 0 −2 0 0 0 0     0  p = 8k + 7  0 0 0 0 −1 0 −1 0 −2 0 0 0 −1 0 −1  .  0 0 0 0 0 0 0 −2  Q − I =   4.6.2  ANSWERS TO EXERCISES  681  2±√  √−1   √  2 × √−2x − 1  x2 − √−2x − 1 . Case 2x + 1 .  Here Q− I has rank 2, so there are 4−2 = 2 factors. [But it is easy to prove that x4 +1 is irreducible over the integers, since it has no linear factors and the coefficient of x in any factor of degree two must be less than or equal to 2 in absolute value by exercise 20.  See also exercise 32, since x4 + 1 = Ψ8 x .  For all k ≥ 2, H. P. F. Swinnerton-Dyer has exhibited polynomials of degree 2k that are irreducible over the integers, but they split completely into linear and quadratic factors modulo every prime. For degree 8, his example is x8−16x6 +88x4 +192x2 +144, having roots ±√ 3± i [see Math. Comp. 24  1970 , 733–734]. According to the theorem of Frobenius cited in exercise 37, any irreducible polynomial of degree n whose Galois group contains no n-cycles will have factors modulo almost all primes.] √−1   2  x +  1 − √−1   √ √ 2  x −  1 + 13. Case p = 8k+1:  x +  1 + √  x −  1 − √−1   2 . Case p = 8k + 3:  x2 + √−1  x2 − √−1 . Case p = 8k +7:  x2 + 2x + 1  x2 − √ √ p = 8k +5:  x2 + The factorization for p = 8k + 7 also holds over the field of real numbers. 14. Algorithm N can be adapted to find the coefficients of w: Let A be the  r + 1 × n matrix whose kth row contains the coefficients of v x k mod u x , for 0 ≤ k ≤ r. Apply the method of Algorithm N until the first dependence is found in step N3; then the algorithm terminates with w x  = v0 + v1x + ··· + vkxk, where vj is defined in  18 . At this point 2 ≤ k ≤ r; it is not necessary to know r in advance, since we can check for dependency after generating each row of A. 15. We may assume that u ̸= 0 and that p is odd. Berlekamp’s method applied to the polynomial x2 − u tells us that a square root exists if and only if Q − I = O if and only if u p−1  2 mod p = 1; but we already knew that. The method of Cantor and Zassenhaus suggests that gcd x2 − u,  sx + t  p−1  2 − 1  will often be a nontrivial factor; and indeed one can show that  p− 1  2 +  0, 1, or 2  values of s will succeed. In practice, sequential choices seem to work just as well as random choices, so we obtain the following algorithm: “Evaluate gcd x2 − u, x p−1  2 −1 , gcd x2 − u,  x+1  p−1  2 −1 , gcd x2 − u,  x + 2  p−1  2 − 1 , . . . , until finding the first case where the gcd has the √ u = ±v.” The expected running time  with random s  will be form x + v. Then O log p 3 for large p. A closer look shows that the first step of this algorithm succeeds if and only if p mod 4 = 3. For if p = 2q + 1 where q is odd, we have xq mod  x2 − u  = u q−1  2x, and gcd x2 − u, xq − 1  ≡ x− u q+1  2 since uq ≡ 1  modulo p . In fact, we see that the √ u = ±u p+1  4 mod p gives the square root directly whenever p mod 4 = 3. formula But when p mod 4 = 1, we will have x p−1  2 mod  x2 − u  = u p−1  4, and the gcd will be 1. The algorithm above should therefore be used only when p mod 4 = 1, and the first gcd should then be omitted. A direct method that works nicely when p mod 8 = 5 was discovered in the 1990s by A. O. L. Atkin, based on the fact that 2 p−1  2 ≡ −1 in that case: Set v ← √ u = ± uv i − 1   mod p, and we also  2u  p−5  8 mod p and i ←  2uv2  mod p; then √−1 = ±i. [Computational Perspectives on Number Theory  Cambridge, Mass.: have International Press, 1998 , 1–11; see also H. C. Pocklington, Proc. Camb. Phil. Soc. 19  1917 , 57–59.]  When p mod 8 = 1, a trial-and-error method seems to be necessary. The following procedure due to Daniel Shanks often outperforms all other known algorithms in such cases: Suppose p = 2eq + 1 where e ≥ 3.  S1. Choose x at random in the range 1 < x < p, and set z = xq mod p.  If z2e−1 mod p = 1, repeat this step.  The average number of repetitions will   682  ANSWERS TO EXERCISES  4.6.2  be less than 2. Random numbers will not be needed in steps S2 and S3. In practice we can save time by trying small odd prime numbers x, and stopping with z = xq mod p when p x−1  2 mod x = x − 1; see exercise 1.2.4–47.   S2. Set y ← z, r ← e, x ← u q−1  2 mod p, v ← ux mod p, w ← ux2 mod p. S3. If w = 1, stop; v is the answer. Otherwise find the smallest k such that w2k mod p is equal to 1. If k = r, stop  there is no answer ; otherwise set  y, r, v, w  ←  y2r−k, k, vy2r−k−1  , wy2r−k  and repeat step S3.  2  2  The validity of this algorithm follows from the invariant congruences uw ≡ v2, y2r−1 ≡ −1, w2r−1 ≡ 1  modulo p . When w ̸= 1, step S3 performs r+2 multiplications  , mod p; hence the maximum number of multiplications in that step is less thane+3 e+4 . Thus the running time is O log p 3 for  and the average number is less than 1 2 steps S1 and S2 plus order e2 log p 2 for step S3, compared to just O log p 3 for the randomized method based on  21 . But the constant factors in Shanks’s method are small. [Congressus Numerantium 7 1972 , 58–62. A related but less efficient method was published by A. Tonelli, Göttinger Nachrichten  1891 , 344–346. The first person to discover a square root algorithm with expected running time O log p 3 was M. Cipolla, Rendiconti Accad. Sci. Fis. Mat. Napoli 9  1903 , 154–163.] 16.  a  Substitute polynomials modulo p for integers, in the proof for n = 1.  b  The proof for n = 1 carries over to any finite field.  c  Since x = ξk for some k, xpn = x in the field defined by f x . Furthermore, the elements y that satisfy the equation ypm = y in the field are closed under addition, and closed under multiplication; so if xpm = x, then ξ  being a polynomial in x with integer coefficients  satisfies ξpm = ξ. 17. If ξ is a primitive root, each nonzero element is some power of ξ. Hence the order must be a divisor of 132 − 1 = 23 · 3 · 7, and φ f  elements have order f.  φ f   f φ f  1 2 4 8  φ f  12 12 24 48 18.  a  pp p1 unx   . . . pp pr unx  , by Gauss’s lemma. For example, let  φ f  6 6 12 24  f 21 42 84 168  f 7 14 28 56  f 3 6 12 24  2 2 4 8  1 1 2 4  u x  = 6x  3 − 3x  2 + 2x − 1,  v x  = x  3 − 3x  2 + 12x − 36 =  x  2 + 12  x − 3 ;  then pp 36x2 + 12  = 3x2 + 1, pp 6x − 3  = 2x − 1.  This is a modern version of a fourteenth-century trick used for many years to help solve algebraic equations.   b  Let pp w unx   = ¯wmxm + ··· + ¯w0 = w unx  c, where c is the content of n  xm + ··· + c ¯w0, hence c ¯wm = um w unx  as a polynomial in x. Then w x  =  c ¯w um n ; since ¯wm is a divisor of un, c is a multiple of um−1 . 19. If u x  = v x w x  with deg v  deg w  ≥ 1, then unxn ≡ v x w x   modulo p . By unique factorization modulo p, all but the leading coefficients of v and w are multiples of p, and p2 divides v0w0 = u0.  20.  a   αuj − uj−1  ¯α¯uj − ¯uj−1  =  uj − ¯αuj−1  ¯uj − α¯uj−1 . assume that u0 ̸= 0. Let m u  =n  αi1 . . . αin−j , an elementary symmetric function, hence uj ≤ un βi1 . . . βin−j where βi = max 1,αi . We complete the proof by showing   b  We may j=1 min 1,αj  = u0 M u . Whenever αj < 1, change the factor x − αj to ¯αjx − 1 in u x ; this doesn’t affect ∥u∥, but it changes u0 to M u .  c  uj = ±un that when x1 ≥ 1, . . . , xn ≥ 1, and x1 . . . xn = M, the elementary symmetric function  n   683  4.6.2  j  j  j  k  j−1  j−1  j−1  k−1  n x− α2  ANSWERS TO EXERCISES  σnk = xi1 . . . xik is ≤n−1 M +n−1 , the value assumed when x1 = ··· = xn−1 =  For if x1 ≤ ··· ≤ xn < M, the transformation xn ← xn−1xn, M v +m−1 M u +m−1 vm ≤m−1 m−1 un since M v  ≤ M u  and vm ≤ un. 1 and xn = M. xn−1 ← 1 increases σnk by σ n−2  k−1  xn − 1  xn−1 − 1 , which is positive.   d  vj ≤ M u +m−1 Notes: This solution shows thatm−1 un is an upper bound, so we [M. Mignotte, Math. Comp. 28  1974 , 1153–1157.]  would like to have a better estimate of M u . Several methods are known [W. Specht, Math. Zeit. 53  1950 , 357–363; Cerlienco, Mignotte, and Piras, J. Symbolic Comp. 4  1987 , 21–33]. The simplest and most rapidly convergent is perhaps the following procedure [see C. H. Graeffe, Auflösung der höheren numerischen Gleichungen  Zürich: x  u −√ √ 1837 ]: Assuming that u x  = un x − α1  . . .  x − αn , let ˆu x  = u  x   =  −1 nu2 n . Then M u 2 = M ˆu  ≤ ∥ˆu∥. Hence we may set c ← ∥u∥, v ← u c, t ← 0, and then repeatedly set t ← t + 1, c ← ∥ˆv∥1 2t c, v ← ˆv ∥ˆv∥. The invariant relations M u  = cM v 1 2t and ∥v∥ = 1 guarantee that M u  ≤ c at each step of the iteration. Notice that when v x  = v0 x2  + xv1 x2 , we have ˆv x  = v0 x 2 − xv1 x 2. It can be shown that if each αj is ≤ ρ or ≥ 1 ρ, then M u  = ∥u∥ 1 + O ρ  ; hence c will be M u  1 + O ρ2t   after t steps.  1  . . .  x− α2  For example, if u x  is the polynomial of  22 , the successive values of c for t = 0, 1, 2, . . . turn out to be 10.63, 12.42, 6.85, 6.64, 6.65, 6.6228, 6.62246, 6.62246, . . . . In this example ρ ≈ .90982. Notice that convergence is not monotonic. Eventually v x  will converge to the monomial xm, where m is the number of roots with αj < 1, assuming that αj ̸= 1 for all j; in general, if there are k roots with αj = 1, the coefficients of xm and xm+k will not approach zero, while the coefficients of higher and lower powers of x will. A famous formula due to Jensen [Acta Math. 22  1899 , 359–364] proves that M u  lnf eiθ  dθ . is the geometric mean of u x  on the unit circle, namely exp  1 Exercise 21 a  will show, similarly, that ∥u∥ is the root-mean-square of u x  on the unit circle. The inequality M u  ≤ ∥u∥, which goes back to E. Landau [Bull. Soc. Math. de France 33  1905 , 251–261], can therefore be understood as a relation between mean values. The number M u  is often called the Mahler measure of a polynomial, because Kurt Mahler used it in Mathematika 7  1960 , 98–100. Incidentally, Jensen also proved that 1 2π 21.  a  The coefficient of apbqcrds is zero on both sides unless p + s = q + r. And when this condition holds, the coefficient on the right is  p + s !; on the left it is   2π 0 eimθ lnf eiθ  dθ = −n   j   2m max αj, 1 2m  when m > 0.  p + s   s  p   2π  j=1 αm      2π  0  q! r! =  q! r! =  q + r ! .  j  r − j  j  r  [B. Beauzamy and J. Dégot, Trans. Amer. Math. Soc. 345  1995 , 2607–2619; D. Zeil- berger, AMM 101  1994 , 894–896.]   b  Let ap = vp, bq = wq, cr = vr, ds = ws. Then the right side of  a  is B u , and the left side is a sum of nonnegative terms for each j and k. If we consider only the terms where Σ j is the degree of v, the terms vp  p− j ! vanish except when p = j.  Those terms therefore reduce to  1 j! k! vjwk j! k!2 = B v B w  .  j,k  [B. Beauzamy, E. Bombieri, P. Enflo, and H. Montgomery, J. Number Theory 36  1990 , 219–245.]   684  ANSWERS TO EXERCISES  4.6.2   c  Adding a new variable, if needed to make everything homogeneous, does not change the relation u = vw. Thus if v and w have total degrees m and n, respectively,  we have  m + n ! [u]2 ≥ m! [v]2 n! [w]2; in other words, [v][w] ≤m+n  1 2[u].  Incidentally, one nice way to think of the Bombieri norm is to imagine that the variables are noncommutative. For example, instead of 3xy3 − z2w2 we could write 3 4 xyyy+ 3 6 wwzz. Then the Bombieri norm is the ∥ ∥ norm on the new coefficients. Another interesting formula, when u is homogeneous of degree n, is  6 wzzw− 1  6 zzww− 1  6 zwzw− 1  6 zwwz− 1  6 wzwz− 1  4 yxyy+ 3  4 yyxy+ 3  m  [u]2 = 1 n! πn  e−x2  1−···−x2  t −y2  1−···−y2  t u x + iy 2  dx dy .  4 yyyx− 1    x  y  k  k  k  k  n  n  2k  n−k   2n   =    m t !t ≤ m!1 2  n 2 2n   d  The one-variable case corresponds to t = 2. Suppose u = vw where v is homogeneous of degree m in t variables. Then vk2 k! m! ≤ [v]2 for all k, and k! ≥  m t !t since log Γ  x  is convex for x > 0; therefore vk2 ≤ m! [v]2  m t !t. We can assume that m! [v]2  m t !t ≤ m′! [w]2  m′ t !t, where m′ = n − m is the degree of w. Then vk2 ≤ m! [v]2 m′!1 2 [v][w]  m t !t 2 m′ t !t 2 ≤ n!1 2 [u]  n 2t !t .  A better bound is obtained if we maximize the next-to-last expression over all de- grees m for which a factor has not been ruled out.  The quantity n!1 4  n 2t !t 2 is ct 2t n 4n− 2t−1  8 1 + O  1  n  , where ct = 21 8π− 2t−1  8tt 4 is ≈ 1.004 when t = 2.   f  Let u and v be homogeneous of degree m and n. Then  Notice that we have not demonstrated the existence of an irreducible factor with  such small coefficients; further splitting may be needed. See exercise 41.   e  [u]2 =    √ πn + O n−1 2 . If v x  =  x − 1 n and w x  =  x + 1 n, we have [v]2 = [w]2 = 2n; hence the inequality of  c  is an equality in this case.   = 4n 2n  = 2n−2k 2k       n m  [uv]2 ≤ ≤  vk−j2 uj2m  n m+n m+n  j ujvk−j 2 −1M u 2 ≤  n  g  By exercise 20,  n −1 −1∥u∥2 =  n M u 2 = 2nM u 2. The upper inequality also follows n −1uj2 ≤ n j=1 x − αj  we have [u]2 ≤ un2n n j=1 1 + αj2  ≤ un2n j=1 2 max 1,αj 2  = 2nM u 2.  by Cauchy’s inequality. [B. Beauzamy, J. Symbolic Comp. 13  1992 , 465–472, Propo- sition 5.] j uj2 ≤ j=1[x − αj]2 =  [u]2 = un2n  from  f , for if u x  = un  22. More generally, assume that u x  ≡ v x w x   modulo q , a x v x +b x w x  ≡ 1  modulo p , c · ℓ v  ≡ 1  modulo r , deg a  < deg w , deg b  < deg v , and deg u  = deg v  + deg w , where r = gcd p, q  and p, q needn’t be prime. We shall construct polynomials V  x  ≡ v x  and W  x  ≡ w x   modulo q  such that u x  ≡ V  x W  x   modulo qr , ℓ V   = ℓ v , deg V   = deg v , deg W   = deg w ; furthermore, if r is prime, the results will be unique modulo qr. w x  + q ¯w x , deg ¯v  < deg v , deg  ¯w  ≤ deg w ; and the other condition  The problem asks us to find ¯v x  and ¯w x  with V  x  = v x  + q¯v x , W  x  =  = [u]2[v]2  k−j k  ⌊n 2⌋  ⌊n 2⌋  ⌊n 2⌋  k−j  k  k  k  j  j  j  j  j  j  j  j   v x  + q¯v x   w x  + q ¯w x   ≡ u x   modulo qr    4.6.2  ANSWERS TO EXERCISES  685  is equivalent to ¯w x v x  + ¯v x w x  ≡ f x   modulo r , where f x  satisfies u x  ≡ v x w x  + qf x   modulo qr . We have   a x f x  + t x w x  v x  +  b x f x  − t x v x  w x  ≡ f x   modulo r   √  for all t x . Since ℓ v  has an inverse modulo r, we can find a quotient t x  by Algorithm 4.6.1D such that deg bf −tv  < deg v ; for this t x , deg af +tw  ≤ deg w , since we have deg f  ≤ deg u  = deg v  + deg w . Thus the desired solution is ¯v x  = b x f x  − t x v x  = b x f x  mod v x , ¯w x  = a x f x  + t x w x . If  ¯¯v x , ¯¯w x   is another solution, we have   ¯w x  − ¯¯w x  v x  ≡  ¯¯v x  − ¯v x  w x   modulo r . Thus if r is prime, v x  must divide ¯¯v x  − ¯v x ; but deg ¯¯v − ¯v  < deg v , so ¯¯v x  = ¯v x  and ¯¯w x  = ¯w x . b x W  x  ≡ 1  modulo p , as required by Hensel’s Lemma.  If p divides q, so that r = p, our choices of V  x  and W  x  also satisfy a x V  x +  For p = 2, the factorization proceeds as follows  writing only the coefficients, and using bars for negative digits : Exercise 10 says that v1 x  =  1 1 1 , w1 x  =  1 1 1 0 0 1 1  in one-bit two’s complement notation. Euclid’s extended algorithm yields a x  =  1 0 0 0 0 1 , b x  =  1 0 . The factor v x  = x2 + c1x + c0 must have c1 ≤ ⌊1 + 113⌋ = 11, c0 ≤ 10, by exercise 20. Three applications of Hensel’s lemma yield v4 x  =  1 3 1 , w4 x  =  1 3 5 4 4 3 5 . Thus c1 ≡ 3 and c0 ≡ −1  modulo 16 ; the only possible quadratic factor of u x  is x2 + 3x − 1. Division fails, so u x  is irreducible.  Since we have now proved the irreducibility of this beloved polynomial by four separate methods, it is unlikely that it has any factors.   Hans Zassenhaus has observed that we can often speed up such calculations by increasing p as well as q: When r = p in the notation above, we can find A x , B x  such that A x V  x  + B x W  x  ≡ 1  modulo p2 , namely by taking A x  = a x  + p¯a x , B x  = b x  + p¯b x , where ¯a x V  x  + ¯b x W  x  ≡ g x   modulo p , a x V  x  + b x W  x  ≡ 1−pg x   modulo p2 . We can also find C with ℓ V  C ≡ 1  modulo p2 . In this way we can lift a squarefree factorization u x  ≡ v x w x   modulo p  to its unique extensions modulo p2, p4, p8, p16, etc. However, this “accelerated” procedure reaches a point of diminishing returns in practice, as soon as we get to double-precision moduli, since the time for multiplying multiprecision numbers in practical ranges outweighs the advantage of squaring the modulus directly. From a computational standpoint it seems best to work with the successive moduli p, p2, p4, p8, . . . , pE, pE+e, pE+2e, pE+3e, . . . , where E is the smallest power of 2 with pE greater than single precision and e is the largest integer such that pe has single precision.  “Hensel’s Lemma” was actually invented by C. F. Gauss about 1799, in the draft of an unfinished book called Analysis Residuorum, §373–374. Gauss incorporated most of the material from that manuscript into his Disquisitiones Arithmeticæ  1801 , but his ideas about polynomial factorization were not published until after his death [see his Werke 2  Göttingen, 1863 , 238]. Meanwhile T. Schönemann had indepen- dently discovered the lemma and proved uniqueness [Crelle 32  1846 , 93–105, §59]. Hensel’s name was attached to the method because it is basic to the theory of p- adic numbers  see exercise 4.1–31 . The lemma can be generalized in several ways. First, if there are more factors, say u x  ≡ v1 x v2 x v3 x   modulo p , we can find a1 x , a2 x , a3 x  such that a1 x v2 x v3 x  + a2 x v1 x v3 x  + a3 x v1 x v2 x  ≡  In essence, 1 u x  is expanded in partial 1  modulo p  and deg ai  < deg vi .  fractions as  ai x  vi x .  An exactly analogous construction now allows us to  lift the factorization without changing the leading coefficients of v1 and v2; we take ¯v1 x  = a1 x f x  mod v1 x , ¯v2 x  = a2 x f x  mod v2 x , etc. Another important   686  ANSWERS TO EXERCISES  4.6.2  generalization is to several simultaneous moduli, of the respective forms pe,  x2−a2 n2, . . . ,  xt − at nt, when performing multivariate gcds and factorizations. See D. Y. Y. Yun, Ph.D. Thesis  M.I.T., 1974 . 23. The discriminant of pp u x   is a nonzero integer  see exercise 4.6.1–12 , and there are multiple factors modulo p if and only if p divides the discriminant. [The factorization of  22  modulo 3 is  x + 1  x2 − x − 1 2 x3 + x2 − x + 1 ; squared factors for this polynomial occur only for p = 3, 23, 233, and 121702457. It is not difficult to prove that the smallest prime that is not unlucky is at most O n log N n , if n = deg u  and if N bounds the coefficients of u x .] 24. Multiply a monic polynomial with rational coefficients by a suitable nonzero inte- ger, to get a primitive polynomial over the integers. Factor this polynomial over the integers, and then convert the factors back to monic.  No factorizations are lost in this way; see exercise 4.6.1–8.  25. Consideration of the constant term shows there are no factors of degree 1, so if the polynomial is reducible, it must have one factor of degree 2 and one of degree 3. Modulo 2 the factors are x x + 1 2 x2 + x + 1 ; this is not much help. Modulo 3 the factors are  x + 2 2 x3 + 2x + 2 . Modulo 5 they are  x2 + x + 1  x3 + 4x + 2 . So we see that the answer is  x2 + x + 1  x3 − x + 2 . 26. Begin with D ←  0 . . . 01 , representing the set {0}. Then for 1 ≤ j ≤ r, set D ← D   D ≪ dj , where  denotes bitwise “or” and D ≪ d denotes D shifted left d bit positions.  Actually we need only work with a bit vector of length ⌈ n + 1  2⌉, since n − m is in the set if and only if m is.  27. Exercise 4 says that a random polynomial of degree n is irreducible modulo p with rather low probability, about 1 n. But the Chinese remainder theorem implies that a random monic polynomial of degree n over the integers will be reducible with respect to each of k distinct primes with probability about  1−1 n k, and this approaches zero as k → ∞. Hence almost all polynomials over the integers are irreducible with respect to infinitely many primes; and almost all primitive polynomials over the integers are irreducible. [Another proof has been given by W. S. Brown, AMM 70  1963 , 965–969.] 28. See exercise 4; the probability is [zn]  1+a1pz p  1+a2pz2 p2  1+a3pz3 p3  . . . , 3 z3  . . . . For 1 ≤ n ≤ 10 the 2 z2  1 + 1 which has the limiting value g z  =  1 + z  1 + 1 1680. [Let f y  = ln 1 + y − y = O y2 . answers are 1, 1 We have  210, 1033 and it can be shown that the limiting probability is h 1  = exp  n≥1 f zn n   = h z   1 − z ,  n≥1 f 1 n   = e−γ ≈ .56146 as n → ∞. Indeed, N. G. de Bruijn has established the asymptotic formula limp→∞ anp = e−γ + e−γ n + O n−2 log n . [See D. H. Lehmer, Acta Arith. 21  1972 , 379–388; D. H. Greene and D. E. Knuth, Math. for the Analysis of Algorithms  Boston: Birkhäuser, 1981 , §4.1.6.] On the other hand the answers for 1 ≤ n ≤ 10 256, 109 when p = 2 are smaller: 1, 1 256. A. Knopfmacher and R. Warlimont [Trans. Amer. Math. Soc. 347  1995 , 2235–2243] have shown that for m≥1 e−1 m 1+amp pm , c2 ≈ .397.] 29. Let q1 x  and q2 x  be any two of the irreducible divisors of g x . By the Chinese remainder theorem  exercise 3 , choosing a random polynomial t x  of degree < 2d is equivalent to choosing two random polynomials t1 x  and t2 x  of degrees < d, where ti x  = t x  mod qi x . The gcd will be a proper factor if t1 x  pd−1  2 mod q1 x  = 1  fixed p the probability is cp+O 1 n , where cp =  n≥1 zn n +  g z  = exp   256, 109  168, 127  120, 173  280, 101  64, 111  60, 79  16, 27  12, 37  16, 7  16, 7  2, 7  6, 7  4, 1  2, 5   k  2  2  ANSWERS TO EXERCISES  4.6.2 687 and t2 x  pd−1  2 mod q1 x  ̸= 1, or vice versa, and this condition holds for exactly 2  pd − 1  2   pd + 1  2  =  p2d − 1  2 choices of t1 x  and t2 x . Notes: We are considering here only the behavior with respect to two irreducible factors, but the true behavior is probably much better. Suppose that each irreducible 2 of dividing t x  pd−1  2 − 1 for each t x , independent of factor qi x  has probability 1 the behavior for other qj x  and t x ; and assume that g x  has r irreducible factors in all. Then if we encode each qi x  by a sequence of 0s and 1s according as qi x  does or doesn’t divide t x  pd−1  2 − 1 for the successive t’s tried, we obtain a random binary trie with r lieves  see Section 6.3 . The cost associated with an internal node of this trie, having m lieves as descendants, is O m2 log p  ; and the solution to the recurrence  , by exercise 5.2.2–36. Hence the sum of costs  Ak is An = 2n   + 21−nn  An =n  It follows thatp−1  in the given random trie — representing the expected time to factor g x  completely — is O r2 log p 3  under this plausible assumption. The plausible assumption becomes rigorously true if we choose t x  at random of degree < rd instead of restricting it to degree < 2d. 30. Let T  x  = x + xp +··· + xpd−1 be the trace of x and let v x  = T t x   mod q x . Since t x pd = t x  in the field of polynomial remainders modulo q x , we have v x p = v x  in that field; in other words, v x  is one of the p roots of the equation y p − y = 0. Hence v x  is an integer. s=0 gcd gd x , T t x  − s  = gd x . In particular, when p = 2 we can argue as in exercise 29 that gcd gd x , T t x    will be a proper factor of gd x  with probability ≥ 1 2 when gd x  has at least two irreducible factors and t x  is a random binary polynomial of degree < 2d. [Note that T t x   mod g x  can be computed by starting with u x  ← t x  and setting u x  ←  t x  + u x p  mod g x  repeatedly, d − 1 times. The method of this s=0  T  x  − s , which holds for any p, while formula  21  is based on the polynomial factorization xpd − x = x x pd−1  2 + 1  x pd−1  2 − 1  for odd p.] The trace was introduced by Richard Dedekind, Abhandlungen der Königl. Gesell- schaft der Wissenschaften zu Göttingen 29  1882 , 1–56. The technique of calculating gcd f x , T  x  − s  to find factors of f x  can be traced to A. Arwin, Arkiv för Mat., Astr. och Fys. 14, 7  1918 , 1–46; but his method was incomplete because he did not consider T t x   for t x  ̸= x. A complete factorization algorithm using traces was devised later by R. J. McEliece, Math. Comp. 23  1969 , 861–867; see also von zur Gathen and Shoup, Computational Complexity 2  1992 , 187–224, Algorithm 3.6, for asymptotically fast results.  exercise is based on the polynomial factorization xpd − x = p−1  Henri Cohen has observed that for p = 2 it suffices to test at most d special cases t x  = x, x3, . . . , x2d−1 when applying this method. One of these choices of t x  is guaranteed to split gd x  whenever gd is reducible, because we can obtain the effects of all polynomials t x  of degree < 2d from these special cases using the facts that T t x p  ≡ T t x   and T u x + t x   ≡ T u x  + T t x    modulo gd x  . [A Course in Computational Algebraic Number Theory  Springer, 1993 , Algorithm 3.4.8.] 31. If α is an element of the field of pd elements, let d α  be the degree of α, namely the smallest exponent e such that αpe = α. Then consider the polynomial  Pα x  =  x − α  x − αp  . . .  x − αpd−1  = qα x d d α  ,  where qα x  is an irreducible polynomial of degree d α . As α runs through all elements of the field, the corresponding qα x  runs through every irreducible polynomial of   688  ANSWERS TO EXERCISES  4.6.2  [We have n p, d  = 1  degree e dividing d, where every such irreducible occurs exactly e times. We have  x + t  pd−1  2 mod qα x  = 1 if and only if  α + t  pd−1  2 = 1 in the field. If t is an integer, we have d α + t  = d α , hence n p, d  is d−1 times the number of elements α of degree d such that α pd−1  2 = 1. Similarly, if t1 ̸= t2 we want to count the number of elements of degree d such that  α + t1  pd−1  2 =  α + t2  pd−1  2, or equivalently   α + t1   α + t2   pd−1  2 = 1. As α runs through all the elements of degree d, so does the quantity  α + t1   α + t2  = 1 +  t1 − t2   α + t2 . c\d 3 +  −1 c µ c  pd c − 1 , which is about half the total number of irreducibles — exactly half, in fact, when d is odd. This proves that gcd gd x ,  x + t  pd−1  2 − 1  has a good chance of finding factors of gd x  when t is fixed and gd x  is chosen at random; but a randomized algorithm is supposed to work with guaranteed probability for fixed gd x  and random t, as in exercise 29.]  4 d−1 32.  a  Clearly xn − 1 =   d\nΨd x , since every complex nth root of unity is a primitive dth root for some unique d\n. The second identity follows from the first; and Ψn x  has integer coefficients since it is expressed in terms of products and quotients of monic polynomials with integer coefficients.  b  The condition in the hint suffices to prove that f x  = Ψn x , so we shall take the hint. When p does not divide n, we have xn − 1 ⊥ nxn−1 modulo p, hence xn − 1 is squarefree modulo p. Given f x  and ζ as in the hint, let g x  be the irreducible factor of Ψn x  such that g ζ p  = 0. If g x  ̸= f x  then f x  and g x  are distinct factors of Ψn x , hence they are distinct factors of xn−1, hence they have no irreducible factors in common modulo p. However, ζ is a root of g xp , so gcd f x , g xp   ̸= 1 over the integers, hence f x  is a divisor of g xp . By  5 , f x  is a divisor of g x p, modulo p, contradicting the assumption that f x  and g x  have no irreducible factors in common. Therefore f x  = g x . [The irreducibility of Ψn x  was first proved for prime n by C. F. Gauss in Disquisitiones Arithmeticæ  Leipzig: 1801 , Art. 341, and for general n by L. Kronecker, J. de Math. Pures et Appliquées 19  1854 , 177–192.]  c  Ψ1 x  = x − 1; and when p is prime, Ψp x  = 1 + x + ··· + xp−1. If n > 1 is odd, it is not difficult to prove that Ψ2n x  = Ψn −x . If p divides n, the second identity in  a  shows that Ψpn x  = Ψn xp . If p does not divide n, we have Ψpn x  = Ψn xp  Ψn x . For nonprime n ≤ 15 we have Ψ4 x  = x2 + 1, Ψ6 x  = x2 − x + 1, Ψ8 x  = x4 +1, Ψ9 x  = x6 + x3 +1, Ψ10 x  = x4 − x3 + x2 − x+1, Ψ12 x  = x4 − x2 +1, Ψ14 x  = x6 − x5 + x4 − x3 + x2 − x + 1, Ψ15 x  = x8 − x7 + x5 − x4 + x3 − x + 1. [The formula Ψpq x  =  1 + xp + ··· + x q−1 p  x − 1   xq − 1  can be used to show that Ψpq x  has all coefficients ±1 or 0 when p and q are prime; but the coefficients of Ψpqr x  can be arbitrarily large.] 33. False; we lose all pj with ej divisible by p. True if p > deg u . [See exercise 36.] 34. [D. Y. Y. Yun, Proc. ACM Symp. Symbolic and Algebraic Comp.  1976 , 26–35.] Set  t x , v1 x , w1 x   ← GCD u x , u′ x  . If t x  = 1, set e ← 1; otherwise set  ui x , vi+1 x , wi+1 x   ← GCD vi x , wi x  − v′i x   for i = 1, 2, . . . , e − 1, until finding we x  − v′e x  = 0. Finally set ue x  ← ve x . To prove the validity of this algorithm, we observe that it computes the polyno- mials t x  = u2 x u3 x 2u4 x 3 . . . , vi x  = ui x ui+1 x ui+2 x  . . . , and wi x  = u′i x ui+1 x ui+2 x  . . .+2ui x u′i+1 x ui+2 x  . . .+3ui x ui+1 x u′i+2 x  . . .+··· . We have t x  ⊥ w1 x , since an irreducible factor of ui x  divides all but the ith term of w1 x , and it is relatively prime to that term. Furthermore we clearly have ui x  ⊥ vi+1 x .   4.6.2  ANSWERS TO EXERCISES  689  [Although exercise 2 b  proves that most polynomials are squarefree, nonsquarefree polynomials actually occur often in practice; hence this method turns out to be quite important. See Paul S. Wang and Barry M. Trager, SICOMP 8  1979 , 300–305, for suggestions on how to improve the efficiency. Squarefree factorization modulo p is discussed by Bach and Shallit, Algorithmic Number Theory 1  MIT Press, 1996 , answer to exercise 7.27.] 35. We have wj x  = gcd uj x , v∗j  x   · gcd u∗j+1 x , vj x  , where  u∗j x  = uj x uj+1 x  . . .  and  v∗j  x  = vj x vj+1 x  . . . .  [Yun notes that the running time for squarefree factorization by the method of exer- cise 34 is at most about twice the running time to calculate gcd u x , u′ x  . Further- more if we are given an arbitrary method for discovering squarefree factorization, the method of this exercise leads to a gcd procedure.  When u x  and v x  are squarefree, their gcd is simply w2 x  where w x  = u x v x  = w1 x w2 x 2; the polynomials uj x , vj x , u∗j x , and v∗j  x  are all squarefree.  Hence the problem of converting a primitive polynomial of degree n to its squarefree representation is computationally equivalent to the problem of calculating the gcd of two nth degree polynomials, in the sense of asymptotic worst-case running time.] 36. Let Uj x  be the value computed for “uj x ” by the procedure of exercise 34. If deg U1 +2 deg U2 +··· = deg u , then uj x  = Uj x  for all j. But in general we will k≥0 uj+pk x  for 1 ≤ j < p. To separate these factors further, j≥p uj x p⌊j p⌋ = z xp . After recursively finding the squarefree representation of z x  =  z1 x , z2 x , . . . , we will 0≤j<p uj+pk x , so we can calculate the individual ui x  by the formula gcd Uj x , zk x   = uj+pk x  for 1 ≤ j < p. The polynomial upk x  will be left when the other factors of zk x  have been removed.  have e < p and Uj x  = we can calculate t x   U2 x U3 x 2 . . . Up−1 x p−2  = have zk x  =  37. The exact probability is  Note: This procedure is fairly simple but the program is lengthy. If one’s goal is to have a short program for complete factorization modulo p, rather than an extremely efficient one, it is probably easiest to modify the distinct-degree factorization routine so that it casts out gcd xpd − x, u x   several times for the same value of d until the gcd is 1. In this case you needn’t begin by calculating gcd u x , u′ x   and removing multiple factors as suggested in the text, since the polynomial xpd − x is squarefree. j≥1 ajp pj kj kj!, where kj is the number of di that are equal to j. Since ajp pj ≈ 1 j by exercise 4, we get the formula of exercise 1.3.3–21. Notes: This exercise says that if we fix the prime p and let the polynomial u x  be random, it will have a certain probability of splitting in a given way modulo p. A much harder problem is to fix the polynomial u x  and to let p be “random”; it turns out that the same asymptotic result holds for almost all u x . G. Frobenius proved in 1880 that the integer polynomial u x  splits modulo p into factors of degrees d1, . . . , dr, when p is a large prime chosen at random, with probability equal to the number of permutations in the Galois group G of u x  having cycle lengths {d1, . . . , dr} divided by the total number of permutations in G. [If u x  has rational coefficients and distinct roots ξ1, . . . , ξn over the complex numbers, its Galois group is the  unique  p 1 ...p n ∈G z + ξp 1 y1 + ··· + ξp n yn  = U z, y1, . . . , yn  has rational coefficients and is irreducible over the rationals; see G. Frobenius, Sitzungsberichte Königl. preuß. Akad. Wiss.  Berlin: 1896 , 689–703. The linear mapping x →→ xp is traditionally called the Frobenius automorphism because  group G of permutations such that the polynomial    690  ANSWERS TO EXERCISES  4.6.2  of this famous paper.] Furthermore B. L. van der Waerden proved in 1934 that almost all polynomials of degree n have the set of all n! permutations as their Galois group [Math. Annalen 109  1934 , 13–16]. Therefore almost all fixed irreducible polynomials u x  will factor as we might expect them to, with respect to randomly chosen large primes p. See also N. Chebotarev, Math. Annalen 95  1926 , for a generalization of Frobenius’s theorem to conjugacy classes of the Galois group. 38. The conditions imply that when z = 1 we have either un−2zn−2 + ··· + u0 < un−1 − 1 ≤ zn + un−1zn−1 or un−3zn−3 + ··· + u0 < un−2 − 1 ≤ zn + un−2zn−2. Therefore by Rouché’s theorem [J. École Polytechnique 21, 37  1858 , 1–34], u z  has at least n − 1 or n − 2 roots inside the circle z = 1. If u z  is reducible, it can be written v z w z  where v and w are monic integer polynomials. The products of the roots of v and of w are nonzero integers, so each factor has a root of absolute value ≥ 1. Hence the only possibility is that v and w both have exactly one such root and that un−1 = 0. These roots must be real, since the complex conjugates are roots; hence u z  has a real root z0 with z0 ≥ 1. But this cannot be, for if r = 1 z0 we have 0 = 1 + un−2r2 + ··· + u0rn ≥ 1 + un−2r2 − un−3r3 − ··· − u0rn > 1. [O. Perron, Crelle 132  1907 , 288–307; for generalizations, see A. Brauer, Amer. J. Math. 70  1948 , 423–432, 73  1951 , 717–720.] 39. First we prove the hint: Let u x  = a x− α1  . . .  x− αn  have integer coefficients. The resultant of u x  with the polynomial y−t x  is a determinant, so it is a polynomial rt y  = adeg t  y− t α1   . . .  y− t αn   with integer coefficients  see exercise 4.6.1–12 . If u x  divides v t x   then v t α1   = 0, hence rt y  has a factor in common with v y . So if v is irreducible, we have deg u  = deg rt  ≥ deg v . Given an irreducible polynomial u x  for which a short proof of irreducibility is desired, we may assume that u x  is monic, by exercise 18, and that deg u  ≥ 3. The idea is to show the existence of a polynomial t x  such that v y  = rt y  is irreducible by the criterion of exercise 38. Then all factors of u x  divide the polynomial v t x  , and this will prove that u x  is irreducible. The proof will be succinct if the coefficients of t x  are suitably small. The polynomial v y  =  y− β1  . . .  y− βn  can be shown to satisfy the criterion of exercise 38 if n ≥ 3 and β1 . . . βn ̸= 0, and if the following “smallness condition” holds: βj ≤ 1  4n  except when j = n or when βj = βn and ℜβj ≤ 1  4n . The calculations are straightforward, using the fact that v0 + ··· + vn ≤  1 + β1  . . .  1 + βn . to be ℜ n−1 Let α1, . . . , αr be real and αr+1, . . . , αr+s be complex, where n = r + 2s and αr+s+j = αr+j for 1 ≤ j ≤ s. Consider the linear expressions Sj a0, . . . , an−1  defined n−1 If i=0 αij⌉, we have Sj a1, . . . , an−1  < bB. Thus if we 0 ≤ ai < b and B = ⌈maxn−1 choose b >  16nB n−1, there must be distinct vectors  a0, . . . , an−1  and  a′0, . . . , a′n−1  such that ⌊8nSj a0, . . . , an−1 ⌋ = ⌊8nSj a′0, . . . , a′n−1 ⌋ for 1 ≤ j < n, since there are bn vectors but at most  16nbB n−1 < bn possible  n − 1 -tuples of values. Let t x  =  a0−a′0 +···+ an−1−a′n−1 xn−1 and βj = t αj . Then the smallness condition is satisfied. Furthermore βj ̸= 0; otherwise t x  would divide u x . [J. Algorithms 2  1981 , 385–392.] 40. Given a candidate factor v x  = xd + ad−1xd−1 + ··· + a0, change each aj to a rational fraction  modulo pe , with numerators and denominators ≤ B. Then multiply by the least common denominator, and see if the resulting polynomial divides u x  over the integers. If not, no factor of u x  with coefficients bounded by B is congruent modulo pe to a multiple of v x .  j  for 1 ≤ j ≤ r + s and ℑ n−1  j  for r + s < j ≤ n.  i=0 aiαi  i=0 aiαi  j=1   4.6.3  ANSWERS TO EXERCISES  691  41. David Boyd notes that 4x8 + 4x6 + x4 + 4x2 + 4 =  2x4 + 4x3 + 5x2 + 4x + 2 ×  2x4 − 4x3 + 5x2 − 4x + 2 , and he has found examples of higher degree to prove that c must be > 2 if it exists.  SECTION 4.6.3 1. xm, where m = 2⌊lg n⌋ is the highest power of 2 less than or equal to n. 2. Assume that x is input in register A, and n in location NN; the output is in register X.  01 A1 ENTX 1 02 STX Y 03 STA Z 04 LDA NN 05 JAP 2F 06 DONE JMP 07 5H SRB 1 08 N STA 09 A5 LDA Z 10 MUL Z 11 Z STX 12 A2 LDA N 13 2H JAE 5B 14 SRB 1 15 A4 JAZ 4F 16 N STA 17 A3 LDA Z 18 Y MUL 19 STX Y 20 A5 JMP 21 4H LDA Z 22 MUL Y  1 1 1 1 1 0  A1. Initialize. Y ← 1. Z ← x. N ← n. To A2. Otherwise the answer is 1.  L + 1 − K L + 1 − K N ← ⌊N 2⌋. A5. Square Z. Z ← Z × Z mod w. A2. Halve N. To A5 if N is even.  L + 1  L L L L  K K  K − 1 K − 1 K − 1 K − 1 K − 1  1 1  Jump if N = 1. N ← ⌊N 2⌋. A3. Multiply Y by Z. Y ← Z × Y mod w. To A5.  Do the final multiplication.  The running time is 21L + 16K + 8, where L = λ n  is one less than the number of bits in the binary representation of n, and K = ν n  is the number of 1-bits in that representation.  For the serial program, we may assume that n is small enough to fit in an index register; otherwise serial exponentiation is out of the question. The following program leaves the output in register A: 1 1 1  01 S1 LD1 NN 02 X STA 03 2F JMP X N − 1 04 1H MUL SLAX 5 N − 1 05 06 2H DEC1 1 07 1B  J1P  N N  rI1 ← n. X ← x. rA × X mod w rI1 ← rI1 − 1. Multiply again if rI1 > 0.  → rA.  The running time for this program is 14N − 7; it is faster than the previous program when n ≤ 7, slower when n ≥ 8.   692  ANSWERS TO EXERCISES  4.6.3  3. The sequences of exponents are:  a  1, 2, 3, 6, 7, 14, 15, 30, 60, 120, 121, 242, 243, 486, 487, 974, 975 [16 multiplications];  b  1, 2, 3, 4, 8, 12, 24, 36, 72, 108, 216, 324, 325, 650, 975 [14 multiplications];  c  1, 2, 3, 6, 12, 15, 30, 60, 120, 240, 243, 486, 972, 975 [13 multiplications];  d  1, 2, 3, 6, 12, 15, 30, 60, 75, 150, 300, 600, 900, 975 [13 multiplications]. [The smallest possible number of multiplications is 12; this is obtainable by combining the factor method with the binary method, since 975 = 15 ·  26 + 1 .] 4.  777777 8 = 218 − 1. 5. T1. [Initialize.] Set LINKU[j] ← 0 for 0 ≤ j ≤ 2r, and set k ← 0, LINKR[0] ← 1,  LINKR[1] ← 0.  T2. [Change level.]  Now level k of the tree has been linked together from left to right, starting at LINKR[0].  If k = r, the algorithm terminates. Otherwise set n ← LINKR[0], m ← 0.  T3. [Prepare for n.]  Now n is a node on level k, and m points to the rightmost  node currently on level k + 1.  Set q ← 0, s ← n.  T4. [Already in tree?]   Now s is a node in the path from the root to n.   If  LINKU[n + s] ̸= 0, go to T6  the value n + s is already in the tree .  If q = 0, set m′ ← n + s. Then set LINKR[n + s] ← q,  T5. [Insert below n.]  LINKU[n + s] ← n, q ← n + s.  T6. [Move up.] Set s ← LINKU[s]. If s ̸= 0, return to T4. T7. [Attach group.] If q ̸= 0, set LINKR[m] ← q, m ← m′. T8. [Move n.] Set n ← LINKR[n]. If n ̸= 0, return to T3. T9. [End of level.] Set LINKR[m] ← 0, k ← k + 1, and return to T2.  6. Prove by induction that the path to the number 2e0 + 2e1 + ··· + 2et, if e0 > e1 > ··· > et ≥ 0, is 1, 2, 22, . . . , 2e0, 2e0 + 2e1, . . . , 2e0 + 2e1 + ··· + 2et; furthermore, the sequences of exponents on each level are in decreasing lexicographic order. 7. The binary and factor methods require one more step to compute x2n than xn; the power tree method requires at most one more step. Hence  a  15 · 2k;  b  33 · 2k;  c  23 · 2k; k = 0, 1, 2, 3, . . . . 8. The power tree always includes the node 2m at one level below m, unless it occurs at the same level or an earlier level; and it always includes the node 2m + 1 at one level below 2m, unless it occurs at the same level or an earlier level. [It is not true that 2m is a child of m in the power tree for all m; the smallest example where this fails is m = 2138, which appears on level 15, while 4276 appears elsewhere on level 16. In fact, 2m sometimes occurs on the same level as m; the smallest example is m = 6029.] 9. Start with N ← n, Z ← x, and Yq ← 1 for 1 ≤ q < m, q odd; in general we will have xn = Y1Y 3 m−1 Z N as the algorithm proceeds. Assuming that N > 0, set k ← N mod m, N ← ⌊N m⌋. Then if k = 0, set Z ← Zm and repeat; otherwise if k = 2pq where q is odd, set Z ← Z2p, Yq ← Yq · Z, and if N > 0 set Z ← Z2e−p and repeat. Finally set Yk ← Yk · Yk+2 for k = m − 3, m − 5, . . . , 1; the answer is Y1 Y3Y5 . . . Ym−1 2.  About m 2 of the multiplications are by 1.  10. By using the “PARENT” representation discussed in Section 2.3.3: Make use of a table p[j], 1 ≤ j ≤ 100, such that p[1] = 0 and p[j] is the number of the node just above j for j ≥ 2.  The fact that each node of this tree has degree at most two has no effect on the efficiency of this representation; it just makes the tree look prettier as an illustration.   5 . . . Y m−1  3 Y 5   4.6.3  ANSWERS TO EXERCISES  693  11. 1, 2, 3, 5, 10, 20,  23 or 40 , 43; 1, 2, 4, 8, 9, 17,  26 or 34 , 43; 1, 2, 4, 8, 9, 17, 34,  43 or 68 , 77; 1, 2, 4, 5, 9, 18, 36,  41 or 72 , 77. If either of the last two paths were in the tree we would have no possibility for n = 43, since the tree must contain either 1, 2, 3, 5 or 1, 2, 4, 8, 9. 12. No such infinite tree can exist, since l n  ̸= l∗ n  for some n. 13. For Case 1, use a Type-1 chain followed by 2A+C + 2B+C + 2A + 2B; or use the factor method. For Case 2, use a Type-2 chain followed by 2A+C+1 + 2B+C + 2A + 2B. For Case 3, use a Type-5 chain followed by addition of 2A + 2A−1, or use the factor method. For Case 4, n = 135 · 2D, so we may use the factor method. 14.  a  It is easy to verify that steps r − 1 and r − 2 are not both small, so let us assume that step r − 1 is small and step r − 2 is not. If c = 1, then λ ar−1  = λ ar−k , so k = 2; and since 4 ≤ ν ar  = ν ar−1  + ν ar−k  − 1 ≤ ν ar−1  + 1, we have ν ar−1  ≥ 3, making r − 1 a star step  lest a0, a1, . . . , ar−3, ar−1 include only one small step . Then ar−1 = ar−2 + ar−q for some q, and if we replace ar−2, ar−1, ar by ar−2, 2ar−2, 2ar−2 + ar−q = ar, we obtain another counterexample chain in which step r is small; but this is impossible. On the other hand, if c ≥ 2, then 4 ≤ ν ar  ≤ ν ar−1  + ν ar−k  − 2 ≤ ν ar−1 ; hence ν ar−1  = 4, ν ar−k  = 2, and c = 2. This leads readily to an impossible situation by a consideration of the six types in the proof of Theorem B.  b  If λ ar−k  < m − 1, we have c ≥ 3, so ν ar−k  + ν ar−1  ≥ 7 by  22 ; therefore both ν ar−k  and ν ar−1  are ≥ 3. All small steps must be ≤ r − k, and λ ar−k  = m − k + 1. If k ≥ 4, we must have c = 4, k = 4, ν ar−1  = ν ar−4  = 4; thus ar−1 ≥ 2m + 2m−1 + 2m−2, and ar−1 must equal 2m + 2m−1 + 2m−2 + 2m−3; but ar−4 ≥ 1 8 ar−1 now implies that ar−1 = 8ar−4. Thus k = 3 and ar−1 > 2m + 2m−1. Since ar−2 < 2m and ar−3 < 2m−1, step r − 1 must be a doubling; but step r − 2 is a nondoubling, since ar−1 ̸= 4ar−3. Furthermore, since ν ar−3  ≥ 3, r − 3 is a star step; and ar−2 = ar−3 + ar−5 would imply that ar−5 = 2m−2, hence we must have ar−2 = ar−3 + ar−4. As in a similar case treated in the text, the only possibility is now seen to be ar−4 = 2m−2 + 2m−3, ar−3 = 2m−2 + 2m−3 + 2d+1 + 2d, ar−1 = 2m + 2m−1 + 2d+2 + 2d+1, and even this possibility is impossible. 15. Achim Flammenkamp [Diplomarbeit in Mathematics  Bielefeld University, 1991 , Part 1] has shown that the numbers n with λ n  + 3 = l n  < l∗ n  all have the form 2A + 2B + 2C + 2D + 2E where A > B > C > D > E and B + E = C + D; moreover, they are described precisely by not matching any of the following eight patterns where ϵ ≤ 1: 2A + 2A−3 + 2C + 2C−1 + 22C+2−A, 2A + 2A−1 + 2C + 2D + 2C+D+1−A, 2A + 2B + 22B−A+3 + 22B+2−A + 23B+5−2A, 2A + 2B + 22B−A+ϵ + 2D + 2B+D+ϵ−A, 2A + 2B + 2B−1 + 2D + 2D−1, 2A + 2B + 2B−2 + 2D + 2D−2  A > B + 1 , 2A + 2B + 2C + 22B+ϵ−A + 2B+C+ϵ−A, 2A + 2B + 2C + 2B+C+ϵ−A + 22C+ϵ−A. 16. lB n  = λ n  + ν n  − 1; so if n = 2k, lB n  λ n  = 1, but if n = 2k+1 − 1, lB n  λ n  = 2. 17. Let i1 < ··· < it. Delete any intervals Ik that can be removed without affecting the union I1 ∪ ··· ∪ It.  The interval  jk . . ik] may be dropped out if either jk+1 ≤ jk or j1 < j2 < ··· and jk+1 ≤ ik−1.  Now combine overlapping intervals  j1 . . i1], . . . ,  jd . . id] into an interval  j′ . . i′] =  j1 . . id] and note that  ai′ < aj′ 1 + δ i1−j1+···+id−jd ≤ aj′ 1 + δ 2 i′  −j′   ,  since each point of  j′ . . i′] is covered at most twice in  j1 . . i1] ∪ ··· ∪  jd . . id].   694  ANSWERS TO EXERCISES  4.6.3  t  v  t+v  mg m   is equivalent to saying that g m  log 1 g m   → 0.  18. Call f m  a “nice” function if  log f m   m → 0 as m → ∞. A polynomial in m is nice. The product of nice functions is nice. If g m  → 0 and c is a positive  constant, then cmg m  is nice; also 2m any s, t, v. The total number of terms is nice, and so arem+s because  t + v  m → 0. Finally, m+s 2   is nice, for by Stirling’s approximation this  ≤ 2t+v, and β2v,  ≤  2m 2t t! <  4em2 t t, where  4e t is nice.  Now replace each term of the summation by the maximum term that is attained for  Replacing t by its upper bound  1− ϵ 2 m λ m  shows that  m2 t t ≤ 2m 1−ϵ 2 f m , where f m  is nice. Hence the entire sum is less than αm for large m if α = 21−η, where 0 < η < 1 2 ϵ. 19.  a  M ∩ N, M ∪ N, M ⊎ N, respectively; see Eqs. 4.5.2– 6 , 4.5.2– 7 .  ,t+v   b  f z g z , lcm f z , g z  , gcd f z , g z  .  For the same reasons as  a , be- cause the monic irreducible polynomials over the complex numbers are precisely the polynomials z − ζ.   c  Commutative laws A⊎ B = B⊎ A, A∪ B = B∪ A, A∩ B = B∩ A. Associative laws A⊎  B ⊎ C  =  A⊎ B ⊎ C, A∪  B ∪ C  =  A∪ B ∪ C, A∩  B ∩ C  =  A∩ B ∩ C. Distributive laws A ∪  B ∩ C  =  A ∪ B  ∩  A ∪ C , A ∩  B ∪ C  =  A ∩ B  ∪  A ∩ C , A ⊎  B ∪ C  =  A ⊎ B  ∪  A ⊎ C , A ⊎  B ∩ C  =  A ⊎ B  ∩  A ⊎ C . Idempotent laws A ∪ A = A, A ∩ A = A. Absorption laws A ∪  A ∩ B  = A, A ∩  A ∪ B  = A, A ∩  A ⊎ B  = A, A ∪  A ⊎ B  = A ⊎ B. Identity and zero laws ∅ ⊎ A = A, ∅ ∪ A = A, ∅ ∩ A = ∅, where ∅ is the empty multiset. Counting law A ⊎ B =  A ∪ B  ⊎  A ∩ B . Further properties analogous to those of sets come from the partial ordering defined by the rule A ⊆ B if and only if A ∩ B = A  if and only if A ∪ B = B .  Notes: Other common applications of multisets are zeros and poles of meromor- phic functions, invariants of matrices in canonical form, invariants of finite Abelian groups, etc.; multisets can be useful in combinatorial counting arguments and in the development of measure theory. The terminal strings of a noncircular context-free grammar form a multiset that is a set if and only if the grammar is unambiguous. The author’s paper in Theoretical Studies in Computer Science, edited by J. D. Ullman  Academic Press, 1992 , 1–13, discusses further applications to context-free grammars, and introduces the operation A ∩. B, where each element that occurs a times in A and b times in B occurs ab times in A ∩. B.  Although multisets appear frequently in mathematics, they often must be treated rather clumsily because there is currently no standard way to treat sets with repeated elements. Several mathematicians have voiced their belief that the lack of adequate terminology and notation for this common concept has been a definite handicap to the development of mathematics.  A multiset is, of course, formally equivalent to a mapping from a set into the nonnegative integers, but this formal equivalence is of little or no practical value for creative mathematical reasoning.  The author discussed this matter with many people during the 1960s in an attempt to find a good remedy. Some of the names suggested for the concept were list, bunch, bag, heap, sample, weighted set, collection, suite; but these words either conflicted with present terminology, had an improper connotation, or were too much of a mouthful to say and to write conveniently. Finally it became clear that such an important concept deserves a name of its own, and the word “multiset” was coined by N. G. de Bruijn. His suggestion was widely adopted during the 1970s, and it is now standard terminology. The notation “A⊎B” has been selected by the author to avoid conflict with existing notations and to stress the analogy with set union. It would not be as desirable to use   4.6.3  ANSWERS TO EXERCISES  695  n∈A 1 nz, h z  =  G z  = generating functions g z  =  “A+ B” for this purpose, since algebraists have found that A+ B is a good notation for the multiset {α + β  α ∈ A and β ∈ B}. If A is a multiset of nonnegative integers, let n∈A zn be a generating function corresponding to A.  Generating functions with nonnegative integer coefficients obviously correspond one-to-one with multisets of nonnegative integers.  If G z  corresponds to A and H z  to B, then G z  + H z  corresponds to A ⊎ B and G z H z  corresponds to A + B. If we form “Dirichlet” n∈B 1 nz, then the product g z h z  corresponds to the multiset product AB. 20. Type 3:  S0, . . . , Sr  =  M00, . . . , Mr0  =  {0}, . . . , {A}, {A−1, A}, {A−1, A, A}, {A − 1, A − 1, A, A, A}, . . . , {A + C − 3, A + C − 3, A + C − 2, A + C − 2, A + C − 2} . Type 5:  M00, . . . , Mr0  =  {0}, . . . , {A}, {A − 1, A}, . . . , {A + C − 1, A + C}, {A + C − 1, A + C − 1, A + C}, . . . , {A + C + D − 1, A + C + D − 1, A + C + D} ;  M01, . . . , Mr1  =  ∅, . . . ,∅, ∅, . . . , ∅, {A+C−2}, . . . , {A+C+D−2} , Si = Mi0⊎Mi1. 21. For example, let u = 28q+5, x =  2 q+1 u − 1   2u − 1  = 2qu + ··· + 2u + 1, y = 2 q+1 u + 1. Then xy =  22 q+1 u − 1   2u − 1 . If n = 24 q+1 u + xy, we have l n  ≤ 4 q + 1 u + q + 2 by Theorem F, but l∗ n  = 4 q + 1 u + 2q + 2 by Theorem H. 22. Underline everything except the u − 1 insertions used in the calculation of x. 23. Theorem G  everything underlined . 24. Use the numbers  Bai − 1   B − 1 , 0 ≤ i ≤ r, underlined when ai is underlined; and ckBi−1 Bbj −1   B−1  for 0 ≤ j < t, 0 < i ≤ bj+1−bj, 1 ≤ k ≤ l0 B , underlined when ck is underlined, where c0, c1, . . . is a minimum length l0-chain for B. To prove the second inequality, let B = 2m and use  3 .  The second inequality is rarely, if ever, an improvement on Theorem G.  25. We may assume that dk = 1. Use the rule R Ak−1 . . . A1, where Aj = “XR” if dj = 1, Aj = “R” otherwise, and where “R” means take the square root, “X” means multiply by x. For example, if y =  .1101101 2, the rule is R R XR XR R XR XR.  There exist binary square-root extraction algorithms suitable for computer hardware, requiring an execution time comparable to that of division; computers with such hardware could therefore calculate more general fractional powers using the technique in this exercise.  26. If we know the pair  Fk, Fk−1 , then we have  Fk+1, Fk  =  Fk + Fk−1, Fk  and  F2k, F2k−1  =  F 2 k−1 ; so a binary method can be used to calculate  Fn, Fn−1 , using O log n  arithmetic operations. Perhaps better is to use the pair of values  Fk, Lk , where Lk = Fk−1 + Fk+1  see exercise 4.5.4–15 ; then we have  Fk+1, Lk+1  =   1 For the general linear recurrence xn = a1xn−1 + ··· + adxn−d, we can compute xn in O d3 log n  arithmetic operations by computing the nth power of an appropriate d × d matrix. [This observation is due to J. C. P. Miller and D. J. Spencer Brown, Comp. J. 9  1966 , 188–190.] In fact, as Richard Brent has observed, the number of operations can be reduced to O d2 log n , or even to O d log d log n  using exercise 4.7–6, if we first compute xn mod  xd − a1xd−1 − ··· − ad  and then replace xj by xj. 27. The smallest n requiring s small steps must be c r  for some r. For if c r  < n < c r+1  we have l n −λ n  ≤ r−λ c r   = l c r  −λ c r  . The answers for 1 ≤ s ≤ 8 are therefore 3, 7, 29, 127, 1903, 65131, 4169527, 994660991. 28.  a  x∇y = x  y   x + y , where “” is bitwise “or”, see exercise 4.6.2–26; clearly ν x∇y  ≤ ν x  y + ν x& y  = ν x + ν y .  b  Note first that Ai−1 2di−1 ⊆ Ai 2di for 1 ≤ i ≤ r. Secondly, note that dj = di−1 in a nondoubling; for otherwise ai−1 ≥ 2aj ≥  2 5Fk + Lk  ,  F2k, L2k  =  FkLk, L2  k +2FkFk−1, F 2  k − 2 −1 k .  2 Fk + Lk , 1  k +F 2   ANSWERS TO EXERCISES  696 4.6.3 aj + ak = ai. Hence Aj ⊆ Ai−1 and Ak ⊆ Ai−1 2dj−dk.  c  An easy induction on i, except that close steps need closer attention. Let us say that m has property P  α  if the 1s in its binary representation all appear in consecutive blocks of ≥ α in a row. If m and m′ have P  α , so does m∇m′; if m has P  α  then ρ m  has P  α + δ . Hence Bi has P  1 + δci . Finally if m has P  α  then ν ρ m   ≤  α + δ ν m  α; for ν m  = ν1 +···+ νq, where each block size νj is ≥ α, hence ν ρ m   ≤  ν1 + δ +···+ νq + δ  ≤  1+δ α ν1+···+ 1+δ α νq.  d  Let f = br +cr be the number of nondoublings and s the number of small steps. If f ≥ 3.271 lg ν n  we have s ≥ lg ν n  as desired, by  16 . Otherwise we have ai ≤  1 + 2−δ bi2ci+di for 0 ≤ i ≤ r, hence n ≤   1 + 2−δ  2 br2r, and r ≥ lg n + br − br lg 1 + 2−δ  ≥ lg n + lg ν n  − lg 1 + δcr  − br lg 1 + 2−δ . Let δ = ⌈lg f + 1 ⌉; then ln 1 + 2−δ  ≤ ln 1 + 1  f + 1   ≤ 1  f + 1  ≤ δ  1 + δf , and it follows that lg 1 + δx  +  f − x  lg 1 + 2−δ  ≤ lg 1 + δf  for 0 ≤ x ≤ f. Hence finally l n  ≥ lg n + lg ν n − lg 1 +  3.271 lg ν n  ⌈lg 1 + 3.271 lg ν n  ⌉ . [Theoretical Comp. Sci. 1  1975 , 1–12.] 29. [Canadian J. Math. 21  1969 , 675–683. Schönhage refined the method of exer- cise 28 to prove that l n  ≥ lg n + lg ν n  − 2.13. Can the remaining gap be closed?] 30. n = 31 is the smallest example; l 31  = 7, but 1, 2, 4, 8, 16, 32, 31 is an addition- subtraction chain of length 6. [After proving Theorem E, Erdős stated that the same result holds also for addition-subtraction chains. Schönhage has extended the lower bound of exercise 28 to addition-subtraction chains, with ν n  replaced by ν n  as defined in exercise 4.1–34. A generalized right-to-left binary method for exponentiation, which uses λ n +ν n −1 multiplications when both x and x−1 are given, can be based on the representation αn of that exercise.] 32. See Discrete Math. 23  1978 , 115–119. [This cost model corresponds to mul- tiplication of large numbers by a classical method like Algorithm 4.3.1M. Empirical results with a more general model in which the cost is  ajak β 2 have been obtained by D. P. McCarthy, Math. Comp. 46  1986 , 603–608; this model comes closer to the “fast multiplication” methods of Section 4.3.3, when two n-bit numbers are multiplied in O nβ  steps, but the cost function ajaβ−1 would actually be more appropriate  see exercise 4.3.3–13 . H. Zantema has analyzed the analogous problem when the cost of step i is aj + ak instead of ajak; see J. Algorithms 12  1991 , 281–307. In this case 2 n + O n1 2 . Furthermore the optimum additive the optimum chains have total cost 5 2 n − 1 , with equality if and only if n can be written as cost when n is odd is at least 5 a product of numbers of the form 2k + 1.] 33. Eight; there are four ways to compute 39 = 12 + 12 + 12 + 3 and two ways to compute 79 = 39 + 39 + 1. 34. The statement is true. The labels in the reduced graph of the binary chain are ⌊n 2k⌋ for k = e0, . . . , 0; they are 1, 2, . . . , 2e0, n in the dual graph. [Similarly, the right-to-left m-ary method of exercise 9 is the dual of the left-to-right method.] 35. 2t are equivalent to the binary chain; it would be 2t−1 if e0 = e1 + 1. The number of chains equivalent to the scheme of Algorithm A is the number of ways to compute the sum of t + 2 numbers of which two are identical. This is 1 2 ft, where fm is the number of ways to compute the sum of m + 1 distinct numbers. When we take commutativity into account, we see that fm is 2−m times  m + 1 ! times the number of binary trees on m nodes, so fm =  2m − 1  2m − 3  . . . 1. 36. First form the 2m − m−1 products xe1 m , for all sequences of exponents such that 0 ≤ ek ≤ 1 and e1 + ··· + em ≥ 2. Let nk =  dkλ . . . dk1dk0 2; to complete the  2 ft+1 + 1  1 . . . xem  k   4.6.3  . . . xdmλ  ANSWERS TO EXERCISES  m , then square and multiply by xd1i1  697 m , for i = λ− 1, calculation, take xd1λ1 . . . , 1, 0. [Straus showed in AMM 71  1964 , 807–808, that 2λ n  may be replaced by  1+ϵ λ n  for any ϵ > 0, by generalizing this binary method to 2k-ary as in Theorem D.] 37.  Solution by D. J. Bernstein.  Let n = nm. First compute 2e for 1 ≤ e ≤ λ n , then compute each nj in λ n  λλ n  + O λ n λλλ n  λλ n 2  further steps by the following variant of the 2k-ary method, where k = ⌊lg lg n − 2 lg lg lg n⌋: For all odd k lg n⌋  q < 2k, compute yq ={2kt+e  dt = 2eq} where nj =  . . . d1d0 2k, in at most ⌊ 1 steps; then use the method in the final stages of answer 9 to compute nj = qyq with  . . . xdmi  at most 2k − 1 further additions.  [A generalization of Theorem E gives the corresponding lower bound. Reference:  SICOMP 5  1976 , 100–103.] 38. The following construction due to D. J. Newman provides the best upper bound currently known: Let k = p1 . . . pr be the product of the first r primes. Compute k and all quadratic residues mod k in O 2−rk log k  steps  because there are approximately 2−rk quadratic residues . Also compute all multiples of k that are ≤ m2, in about m2 k further steps. Now m additions suffice to compute 12, 22, . . . , m2. We have k = exp pr + O pr  log pr 1000   where pr is given by the formula in the answer to exercise 4.5.4–36; see, for example, Greene and Knuth, Math. for the Analysis of Algorithms  Boston: Birkhäuser, 1981 , §4.1.6. So by choosing  r = ⌊ 1 + 1  2 ln 2 lg lg m  ln m ln ln m⌋  it follows that l 12, . . . , m2  = m + O m · exp −  1  2 ln 2 − ϵ  ln m ln ln m  .  On the other hand, D. Dobkin and R. Lipton have shown that, for any ϵ > 0, l 12, . . . , m2  > m + m2 3−ϵ when m is sufficiently large [SICOMP 9  1980 , 121–125]. 39. The quantity l [n1, n2, . . . , nm]  is the minimum of arcs−vertices+m taken over all directed graphs having m vertices sj whose in-degree is zero and one vertex t whose out- degree is zero, where there are exactly nj oriented paths from sj to t for 1 ≤ j ≤ m. The quantity l n1, n2, . . . , nm  is the minimum of arcs − vertices + 1 taken over all directed graphs having one vertex s whose in-degree is zero and m vertices tj whose out-degree is zero, where there are exactly nj oriented paths from s to tj for 1 ≤ j ≤ m. These problems are dual to each other, if we change the direction of all the arcs. [See J. Algorithms 2  1981 , 13–21.] Note: C. H. Papadimitriou has observed that this is a special case of a much more general theorem. Let N =  nij  be an m × p matrix of nonnegative integers having no row or column entirely zero. We can define l N  to be the minimum number of m  1 ≤ j ≤ p}. multiplications needed to compute the set of monomials {xn1j 1 Now l N  is also the minimum of arcs − vertices + m taken over all directed graphs having m vertices si whose in-degree is zero and p vertices tj whose out-degree is zero, where there are exactly nij oriented paths from si to tj for each i and j. By duality we have l N  = l N T   + m − p. [Bulletin of the EATCS 13  February 1981 , 2–3.] N. Pippenger has considerably extended the results of exercises 36 and 37. For example, if L m, p, n  is the maximum of l N  taken over all m × p matrices N of nonnegative integers nij ≤ n, he showed that L m, p, n  = min m, p  lg n + H lg H + O m + p + H log log H 1 2 log H −3 2 , where H = mp lg n + 1 . [See SICOMP 9  1980 , 230–250.] 40. By exercise 39, it suffices to show that l m1n1 + ··· + mtnt  ≤ l m1, . . . , mt  + l [n1, . . . , nt] . But this is clear, since we can first form {xm1 , . . . , xmt} and then compute the monomial  xm1 n1 . . .  xmt nt.  . . . xnmj   4.6.3  698  ANSWERS TO EXERCISES  are any addition chains, then l  cijaibj  ≤ r + s + cij − 1 for any  r + 1  ×  s + 1   Note: One strong way to state Olivos’s theorem is that if a0, . . . , ar and b0, . . . , bs  complete. But it seems plausible that an optimum chain for, say, m−1  matrix of nonnegative integers cij. 41. [SICOMP 10  1981 , 638–646.] The stated formula can be proved whenever A ≥ 9m2. Since this is a polynomial in m, and since the problem of finding a minimum vertex cover is NP-hard  see Section 7.9 , the problem of computing l n1, . . . , nm  is NP-complete. [It is unknown whether or not the problem of computing l n  is NP- k=0 nk+12Ak2 would entail an optimum chain for {n1, . . . , nm}, when A is sufficiently large.] 42. The condition fails at 128  and in the dual 1, 2, . . . , 16384, 16385, 16401, 32768, . . . at 32768 . Only two reduced digraphs of cost 27 exist; hence l0 5784689  = 28. Furthermore, Clift’s programs proved that l0 n  = l n  for all smaller values of n.  SECTION 4.6.4 1. Set y ← x2, then compute   . . .  u2n+1y + u2n−1 y + ···  y + u1 x. 2. Replacing x in  2  by the polynomial x + x0 leads to the following procedure:  G1. Do step G2 for k = n, n − 1, . . . , 0  in this order , and stop. G2. Set vk ← uk, and then set vj ← vj + x0vj+1 for j = k, k + 1, . . . , n − 1.   When k = n, this step simply sets vn ← un.   The computations turn out to be identical to those in H1 and H2, but performed in a different order.  This process was Newton’s original motivation for using scheme  2 .  3. The coefficient of xk is a polynomial in y that may be evaluated by Horner’s rule:  . . .  un,0x+ un−1,1y+ un−1,0  x+··· x+  . . .  u0,ny+ u0,n−1 y+···  y+ u0,0 . [For a “homogeneous” polynomial, such as unxn + un−1xn−1y +···+ u1xyn−1 + u0yn, another scheme is more efficient: If 0 < x ≤ y, first divide x by y, evaluate a polynomial in x y, then multiply by yn.] 4. Rule  2  involves 4n or 3n real multiplications and 4n or 7n real additions;  3  is worse, it takes 4n + 2 or 4n + 1 multiplications, 4n + 2 or 4n + 5 additions. 5. One multiplication to compute x2; ⌊n 2⌋ multiplications and ⌊n 2⌋ additions to evaluate the first line; ⌈n 2⌉ multiplications and ⌈n 2⌉ − 1 additions to evaluate the second line; and one addition to add the two lines together. Total: n+1 multiplications and n additions. 6. J1. Compute and store the values x2  0, . . . , x⌈n 2⌉  .  0  J2. Set vj ← ujxj−⌊n 2⌋ J3. For k = 0, 1, . . . , n − 1, set vj ← vj + vj+1 for j = n − 1, . . . , k + 1, k. J4. Set vj ← vjx⌊n 2⌋−j  for 0 ≤ j ≤ n.  0  0  0, x3 for 0 ≤ j ≤ n.  There are  n2 +n  2 additions, n+⌈n 2⌉−1 multiplications, n divisions. Another mul- tiplication and division can be saved by treating vn and v0 as special cases. Reference: SIGACT News 7, 3  Summer 1975 , 32–34. 7. Let xj = x0 + jh, and consider  42  and  44 . Set yj ← u xj  for 0 ≤ j ≤ n. For k = 1, 2, . . . , n  in this order , set yj ← yj − yj−1 for j = n, n − 1, . . . , k  in this order . Now set βj ← yj for all j. However, rounding errors will accumulate as explained in the text, even if the operations of  5  are done with perfect accuracy. A better way to do the initialization,   4.6.4  ANSWERS TO EXERCISES  699  when  5  is performed with fixed point arithmetic, is to choose β0, . . . , βn so that    0   0 d  nd  ...  0  0  1   0 d  nd  ...  1  1      n   0 d  nd  ...  n  n  β0  β1...  βn   =     +     ,  ϵ0  ϵ1...  ϵn  u x0  u xd  ...  u xnd   ··· ···  ···  where ϵ0, ϵ1, . . . , ϵn are as small as possible. [H. Hassler, Proc. 12th Spring Conf. Computer Graphics  Bratislava: Comenius University, 1996 , 55–66.] 8. See  43 . 9. [Combinatorial Mathematics  Buffalo: Math. Assoc. of America, 1963 , 26–28.] This formula can be regarded as an application of the principle of inclusion and exclusion  Section 1.3.3 , since the sum of the terms for n − ϵ1 − ··· − ϵn = k is the sum of all x1j1 x2j2 . . . xnjn for which k values of the ji do not appear. A direct proof can be given by observing that the coefficient of x1j1 . . . xnjn is   −1 n−ϵ1−···−ϵn ϵj1 . . . ϵjn;  if the j’s are distinct, this equals unity, but if j1, . . . , jn ̸= k then it is zero, since the terms for ϵk = 0 cancel the terms for ϵk = 1. To evaluate the sum efficiently, we can start with ϵ1 = 1, ϵ2 = ··· = ϵn = 0, and we can then proceed through all combinations of the ϵ’s in such a way that only one ϵ changes from one term to the next.  See “Gray binary code” in Section 7.2.1.1.  The first term costs n − 1 multiplications; the subsequent 2n − 2 terms each involve n additions, then n − 1 multiplications, then one more addition. Total:  2n − 1  n − 1  multiplications, and  2n − 2  n + 1  additions. Only n + 1 temporary storage locations are needed, one for the main partial sum and one for each factor of the current product.   = n 2n−1 − 1  multiplications and   n2n−1 − 2n + 1 additions. This is approximately half as many arithmetic operations as the method of exercise 9, although it requires a more complicated program to control   =  temporary storage locations must be  1≤k<n k n  1≤k<n k + 1  n  10.  the sequence. Approximately n   +  k+1  k+1  n  used, and this grows exponentially large  on the order of 2n   ⌈n 2⌉  ⌈n 2⌉−1  The method in this exercise is equivalent to the unusual matrix factorization of the permanent function given by Jurkat and Ryser in J. Algebra 3  1966 , 1–27. It may also be regarded as an application of  39  and  40 , in an appropriate sense. 11. Efficient methods are known for computing an approximate value, if the matrix is sufficiently dense; see A. Sinclair, Algorithms for Random Generation and Counting  Boston: Birkhäuser, 1993 . But this problem asks for the exact value. There may be a way to evaluate the permanent with O cn  operations for some c < 2. 12. Here is a brief summary of progress on this famous research problem: J. Hopcroft and L. R. Kerr proved, among other things, that seven multiplications are necessary in 2 × 2 matrix multiplication modulo 2 [SIAM J. Appl. Math. 20  1971 , 30–36]. R. L. Probert showed that all 7-multiplication schemes, in which each multiplication takes a linear combination of elements from one matrix and multiplies by a linear combination of elements from the other, must have at least 15 additions [SICOMP 5  1976 , 187– 203]. The tensor rank of 2 × 2 matrix multiplication is 7 over every field [V. Y. Pan, J. Algorithms 2  1981 , 301–310]; the rank of T  2, 3, 2 , the tensor for the product of a 2 × 3 matrix by a 3 × 2 matrix, is 11 [V. B. Alekseyev, J. Algorithms 6  1985 ,  √ n  .   700  ANSWERS TO EXERCISES  4.6.4  71–85]. For n × n matrix multiplication, the best upper bound known when n = 3 is due to J. D. Laderman [Bull. Amer. Math. Soc. 82  1976 , 126–128], who showed that 23 noncommutative multiplications suffice. His construction has been generalized by Ondrej Sýkora, who exhibited a method requiring n3 −  n − 1 2 noncommutative multiplications and n3 − n2 +11 n−1 2 additions; this result also reduces to  36  when n = 2 [Lecture Notes in Comp. Sci. 53  1977 , 504–512]. For n = 5, the current record is 100 noncommutative multiplications [O. M. Makarov, USSR Comp. Math. and Math. Phys. 27, 1  1987 , 205–207]. The best lower bound known so far is due to Markus Bläser, who showed that 2n2 + n− 3 nonscalar multiplications are necessary for n ≥ 2, and mn + ns + m− n + s− 3 in the m× n× s case for n ≥ 2 and s ≥ 2 [Computational Complexity 8  1999 , 203–226]. If all calculations must be done without division, slightly better lower bounds were obtained by N. H. Bshouty [SICOMP 18  1989 , 759–765], who proved that m× n by n× s matrix multiplication mod 2 requires at least 2 n +  n mod j   n −  n mod j  − j  + n mod j multiplications when n ≥ s ≥ j ≥ 1; setting m = n = s and j ≈ lg n gives 2.5n2 − 1 The best upper bounds known for large n are discussed in the text, following  36 .  2 n lg n + O n .  13. By summing geometric series, we find that F  t1, . . . , tn  equals  j−1 k=0⌊ms 2k⌋ + 1   1 . . . xtn  0≤s1<m1,...,0≤sn<mn  exp −2πi s1t1 m1 + ··· + sntn mn f s1, . . . , sn   m1 . . . mn. The inverse transform times m1 . . . mn can be found by doing a regular transform and interchanging tj with mj − tj when tj ̸= 0; see exercise 4.3.3–9.  [If we regard F  t1, . . . , tn  as the coefficient of xt1  n in a multivariate polyno- mial, the discrete Fourier transform amounts to evaluation of this polynomial at roots of unity, and the inverse transform amounts to finding the interpolating polynomial.] 14. Let m1 = ··· = mn = 2, F  t1, t2, . . . , tn  = F  2n−1tn + ··· + 2t2 + t1 , and f s1, s2, . . . , sn  = f 2n−1s1 + 2n−2s2 + ··· + sn ; note the reversal between t’s and s’s. Also let gk sk, . . . , sn, tk  be ω raised to the 2k−1tk sn + 2sn−1 + ··· + 2n−ksk  power. Replace fk sn−k+1, . . . , sn, t1, . . . , tn−k  by fk t1, . . . , tn−k, sn−k+1, . . . , sn  in  40  if you prefer to work in situ. At each iteration we essentially take 2n−1 pairs of complex numbers  α, β  and replace them by  α+ζβ, α−ζβ , where ζ is a suitable power of ω, hence ζ = cos θ+i sin θ for some θ. If we take advantage of simplifications when ζ = ±1 or ±i, the total work comes to   n − 3  · 2n−1 + 2  complex multiplications and n · 2n complex additions; the techniques of exercise 41 can be used to reduce the real multiplications and additions used to implement these complex operations.  δ, α − ζβ + ζ  δ, α + iζβ − ζ  The number of complex multiplications can be reduced about 25 percent without changing the number of additions by combining passes k and k + 1 for k = 1, 3, . . . ; this means that 2n−2 quadruples  α, β, γ, δ  are being replaced by  α + ζβ + ζ δ . The total number of complex multiplications when n is even is thereby reduced to  3n − 2 2n−3 − 5⌊2n−1 3⌋. These calculations assume that the given numbers F  t  are complex. If the F  t  are real, then f s  is the complex conjugate of f 2n−s , so we can avoid the redundancy by computing only the 2n independent real numbers f 0 , ℜf 1 , . . . , ℜf 2n−1 − 1 , f 2n−1 , ℑf 1 , . . . , ℑf 2n−1 − 1 . The entire calculation in this case can be done by working with 2n real values, using the fact that fk sn−k+1, . . . , sn, t1, . . . , tn−k  will be the complex conjugate of fk s′n−k+1, . . . , s′n, t1, . . . , tn−k  when  s1 . . . sn 2 +  δ, α − iζβ − ζ  γ − iζ  γ − ζ  γ + iζ  γ + ζ  3  2  3  3  2  3  2  2   ANSWERS TO EXERCISES  4.6.4 701  s′1 . . . s′n 2 ≡ 0  modulo 2n . About half as many multiplications and additions are needed as in the complex case. [The fast Fourier transform algorithm was discovered by C. F. Gauss in 1805 and independently rediscovered many times since, most notably by J. W. Cooley and J. W. Tukey, Math. Comp. 19  1965 , 297–301. Its interesting history has been traced by J. W. Cooley, P. A. W. Lewis, and P. D. Welch, Proc. IEEE 55  1967 , 1675–1677; M. T. Heideman, D. H. Johnson, and C. S. Burrus, IEEE ASSP Magazine 1, 4  October 1984 , 14–21. Details concerning its use have been discussed by hundreds of authors, admirably summarized by Charles Van Loan, Computational Frameworks for the Fast Fourier Transform  Philadelphia: SIAM, 1992 . For a survey of fast Fourier transforms on finite groups, see M. Clausen and U. Baum, Fast Fourier Transforms  Mannheim: Bibliographisches Institut Wissenschaftsverlag, 1993 .] 15.  a  The hint follows by integration and induction. Let f  n  θ  take on all values be- tween A and B inclusive, as θ varies from min x0, . . . , xn  to max x0, . . . , xn . Replac- ing f  n  by each of these bounds, in the stated integral, yields A n! ≤ f x0, . . . , xn  ≤  b  It suffices to prove this for j = n. Let f be Newton’s interpolation B n!. [See The Mathematical Papers of Isaac polynomial, then f  n  is the constant n! αn. Newton, edited by D. T. Whiteside, 4  1971 , 36–51, 70–73.] 16. Carry out the multiplications and additions of  43  as operations on polynomials.  The special case x0 = x1 = ··· = xn is considered in exercise 2. We have used this method in step T8 of Algorithm 4.3.3T.  17. For example, when n = 5 we have  u[5] x  =  y0  x − x0  − 5y1 x − x1  1  x − x0  − 5  x − x1  + 10y2 x − x2 + 10 x − x2  − 10y3 x − x3 − 10 x − x3  + 5y4 x − x4  − y5  x − x5  + 5  x − x4  − 1  x − x5  ,  independent of the value of h. 2 u3 u4 + 1 , β = u2 u4 − α0 α0 − 1 , α1 = α0β − u1 u4, α2 = β − 2α1, 18. α0 = 1 α3 = u0 u4 − α1 α1 + α2 , α4 = u4. 19. Since α5 is the leading coefficient, we may assume without loss of generality that u x  is monic  namely that u5 = 1 . Then α0 is a root of the equation 40z3 − 24u4z2 + 4 + 2u3 z +  u2 − u3u4  = 0; this equation always has at least one real root, and it  4u2 may have three. Once α0 is determined, we have α3 = u4−4α0, α1 = u3−4α0α3−6α2 0, α2 = u1 − α0 α0α1 + 4α2 For the given polynomial we are to solve the cubic equation 40z3 − 120z2 + 80z = 0; this leads to three solutions  α0, α1, α2, α3, α4, α5  =  0,−10, 13, 5,−5, 1 ,  1,−20, 68, 1, 11, 1 ,  2,−10, 13,−3, 27, 1 . 20. LDA TEMP2 TEMP2 TEMP2  0 , α4 = u0 − α3 α4  0α3 + 2α1α3 + α3  =α1= TEMP2 =α2=  TEMP1 =α4= =α5=  FADD FMUL FADD  STA FMUL STA  FMUL FADD FMUL  0 + α1α2  0 + α2 .  X  FADD =α3= STA TEMP1 FADD =α0-α3=  21. z =  x + 1 x − 2, w =  x + 5 z + 9, u x  =  w + z − 8 w − 8; or z =  x + 9 x + 26, w =  x − 3 z + 73, u x  =  w + z − 24 w − 12. 22. α6 = 1, α0 = −1, α1 = 1, β1 = −2, β2 = −2, β3 = −2, β4 = 1, α3 = −4, α2 = 0, α4 = 4, α5 = −2. We form z =  x−1 x+1, w = z+x, and u x  =   z−x−4 w+4 z−2. Here one of the seven additions can be saved if we compute w = x2 + 1, z = w − x.   702  ANSWERS TO EXERCISES  4.6.4  3  − 74  3   x2 − 1  9 ; α0 = 6, β0 = 16  3 . Let α2 = −1, α1 = 1  23.  a  We may use induction on n; the result is trivial if n < 2. If f 0  = 0, then the result is true for the polynomial f z  z, so it holds for f z . If f iy  = 0 for some real y ̸= 0, then g ±iy  = h ±iy  = 0; since the result is true for f z   z2 + y2 , it holds also for f z . Therefore we may assume that f z  has no roots whose real part is zero. Now the net number of times the given path circles the origin is the number of roots of f z  inside the region, which is at most 1. When R is large, the path f Reit  for π 2 ≤ t ≤ 3π 2 will circle the origin clockwise approximately n 2 times; so the path f it  for −R ≤ t ≤ R must go counterclockwise around the origin at least n 2− 1 times. For n even, this implies that f it  crosses the imaginary axis at least n−2 times, and the real axis at least n − 3 times; for n odd, f it  crosses the real axis at least n − 2 times and the imaginary axis at least n − 3 times. These are roots respectively of g it  = 0, h it  = 0.  b  If not, g or h would have a root of the form a + bi with a ̸= 0 and b ̸= 0. But this would imply the existence of at least three other such roots, namely a − bi and −a ± bi, while g z  and h z  have at most n roots. 24. The roots of u are −7, −3 ± i, −2 ± i, and −1; permissible values of c are 2 and 4  but not 3, since c = 3 makes the sum of the roots equal to zero . Case 1: c = 2. Then p x  =  x + 5  x2 + 2x + 2  x2 + 1  x − 1  = x6 + 6x5 + 6x4 + 4x3 − 5x2 − 2x − 10; q x  = 6x2 + 4x − 2 = 6 x + 1  x − 1 3; p1 x  = x4 + 6x3 + 5x2 − 2x − 10 =  x2 + 6x + 16 3 , β1 = − 74 9 . Case 2: c = 4. A similar analysis gives α2 = 9, α1 = −3, α0 = −6, β0 = 12, β1 = −26. 25. β1 = α2, β2 = 2α1, β3 = α7, β4 = α6, β5 = β6 = 0, β7 = α1, β8 = 0, β9 = 2α1−α8. 26.  a  λ1 = α1 × λ0, λ2 = α2 + λ1, λ3 = λ2 × λ0, λ4 = α3 + λ3, λ5 = λ4 × λ0, λ6 = α4 + λ5.  b  κ1 = 1 + β1x, κ2 = 1 + β2κ1x, κ3 = 1 + β3κ2x, u x  = β4κ3 = β1β2β3β4x3 + β2β3β4x2 + β3β4x + β4.  c  If any coefficient is zero, the coefficient of x3 must also be zero in  b , while  a  yields an arbitrary polynomial α1x3+α2x2+α3x+α4 of degree ≤ 3. 27. Otherwise there would be a nonzero polynomial f qn, . . . , q1, q0  with integer coeffi- cients such that qn · f qn, . . . , q1, q0  = 0 for all sets  qn, . . . , q0  of real numbers. This cannot happen, since it is easy to prove by induction on n that a nonzero polynomial always takes on some nonzero value.  See exercise 4.6.1–16. However, this result is false for finite fields in place of the real numbers.  28. The indeterminate quantities α1, . . . , αs form an algebraic basis for the polynomial domain Q[α1, . . . , αs], where Q is the field of rational numbers. Since s + 1 is greater than the number of elements in a basis, the polynomials fj α1, . . . , αs  are algebraically dependent; this means that there is a nonzero polynomial g with rational coefficients such that g f0 α1, . . . , αs , . . . , fs α1, . . . , αs   is identically zero. 29. Given j0, . . . , jt ∈ {0, 1, . . . , n}, there are nonzero polynomials with integer coeffi- cients such that gj qj0 , . . . , qjt  = 0 for all  qn, . . . , q0  in Rj, 1 ≤ j ≤ m. The product g1g2 . . . gm is therefore zero for all  qn, . . . , q0  in R1 ∪ ··· ∪ Rm. 30. Starting with the construction in Theorem M, we will prove that mp+ 1−δ0mc  of the β’s may effectively be eliminated: If µi corresponds to a parameter multiplication, we have µi = β2i−1 ×  T2i + β2i ; add cβ2i−1β2i to each βj for which cµi occurs in Tj, and replace β2i by zero. This removes one parameter for each parameter multiplication. If µi is the first chain multiplication, then µi =  γ1x + θ1 + β2i−1  ×  γ2x + θ2 + β2i , where γ1, γ2, θ1, θ2 are polynomials in β1, . . . , β2i−2 with integer coefficients. Here θ1 and θ2 can be “absorbed” into β2i−1 and β2i, respectively, so we may assume that   4.6.4  ANSWERS TO EXERCISES  703  θ1 = θ2 = 0. Now add cβ2i−1β2i to each βj for which cµi occurs in Tj; add β2i−1γ2 γ1 to β2i; and set β2i−1 to zero. The result set is unchanged by this elimination of β2i−1, except for the values of α1, . . . , αs such that γ1 is zero. [This proof is essentially due to V. Y. Pan, Uspekhi Mat. Nauk 21, 1  January–February 1966 , 103–134.] The latter case can be handled as in the proof of Theorem A, since the polynomials with γ1 = 0 can be evaluated by eliminating β2i  as in the first construction, where µi corresponds to a parameter multiplication . 31. Otherwise we could add one parameter multiplication as a final step, and violate Theorem C.  The exercise is an improvement over Theorem A, in this special case, since there are only n degrees of freedom in the coefficients of a monic polynomial of degree n.  32. λ1 = λ0 × λ0, λ2 = α1 × λ1, λ3 = α2 + λ2, λ4 = λ3 × λ1, λ5 = α3 + λ4. We need at least three multiplications to compute u4x4  see Section 4.6.3 , and at least two additions by Theorem A. 33. We must have n + 1 ≤ 2mc + mp + δ0mc, and mc + mp =  n + 1  2; so there are no parameter multiplications. Now the first λi whose leading coefficient  as a polynomial in x  is not an integer must be obtained by a chain addition; and there must be at least n + 1 parameters, so there are at least n + 1 parameter additions. 34. Transform the given chain step by step, and also define the “content” ci of λi, as follows:  Intuitively, ci is the leading coefficient of λi.  Define c0 = 1.  a  If the step has the form λi = αj + λk, replace it by λi = βj + λk, where βj = αj ck; and define ci = ck.  b  If the step has the form λi = αj − λk, replace it by λi = βj + λk, where βj = −αj ck; and define ci = −ck.  c  If the step has the form λi = αj × λk, replace it by λi = λk  the step will be deleted later ; and define ci = αjck.  d  If the step has the form λi = λj × λk, leave it unchanged; and define ci = cjck. After this process is finished, delete all steps of the form λi = λk, replacing λi by λk in each future step that uses λi. Then add a final step λr+1 = β × λr, where β = cr. This is the desired scheme, since it is easy to verify that the new λi are just the old ones divided by the factor ci. The β’s are given functions of the α’s; division by zero is no problem, because if any ck = 0 we must have cr = 0  hence the coefficient of xn is zero , or else λk never contributes to the final result. 35. Since there are at least five parameter steps, the result is trivial unless there is at least one parameter multiplication; considering the ways in which three multiplications can form u4x4, we see that there must be one parameter multiplication and two chain multiplications. Therefore the four addition-subtractions must each be parameter steps, and exercise 34 applies. We can now assume that only additions are used, and that we have a chain to compute a general monic fourth-degree polynomial with two chain multiplications and four parameter additions. The only possible scheme of this type that calculates a fourth-degree polynomial has the form  λ1 = α1 + λ0 λ2 = α2 + λ0 λ3 = λ1 × λ2 λ4 = α3 + λ3 λ5 = α4 + λ3 λ6 = λ4 × λ5 λ7 = α5 + λ6.   704  ANSWERS TO EXERCISES  4.6.4  Actually this chain has one addition too many, but any correct scheme can be put into this form if we restrict some of the α’s to be functions of the others. Now λ7 has the form  x2 + Ax + B  x2 + Ax + C  + D = x4 + 2Ax3 +  E + A2 x2 + EAx + F, where A = α1 + α2, B = α1α2 + α3, C = α1α2 + α4, D = α6, E = B + C, F = BC + D; and since this involves only three independent parameters it cannot represent a general monic fourth-degree polynomial. 36. As in the solution to exercise 35, we may assume that the chain computes a general monic polynomial of degree six, using only three chain multiplications and six parameter additions. The computation must take one of two general forms  λ1 = α1 + λ0 λ2 = α2 + λ0 λ3 = λ1 × λ2 λ4 = α3 + λ0 λ5 = α4 + λ3 λ6 = λ4 × λ5 λ7 = α5 + λ6 λ8 = α6 + λ6 λ9 = λ7 × λ8 λ10 = α7 + λ9  λ1 = α1 + λ0 λ2 = α2 + λ0 λ3 = λ1 × λ2 λ4 = α3 + λ3 λ5 = α4 + λ3 λ6 = λ4 × λ5 λ7 = α5 + λ3 λ8 = α6 + λ6 λ9 = λ7 × λ8 λ10 = α7 + λ9  where, as in exercise 35, an extra addition has been inserted to cover a more general case. Neither of these schemes can calculate a general sixth-degree monic polynomial, since the first case is a polynomial of the form  3 + Ax and the second case is a polynomial of the form  2 + Bx + C  x  3 + Ax   x  2 + Bx + D  + E,   x  2 x  4 + 2Ax  3 +  E + A  2 + Ax + G  + H;  2 + EAx + F  x both of these involve only five independent parameters. 37. Let p0 x  = unxn + un−1xn−1 +···+ u0 and q0 x  = xn + vn−1xn−1 +···+ v0. For 1 ≤ j ≤ n, divide pj−1 x  by the monic polynomial qj−1 x , obtaining pj−1 x  = αjqj−1 x  + βjqj x . Assume that a monic polynomial qj x  of degree n − j ex- ists satisfying this relation; this will be true for almost all rational functions. Let pj x  = qj−1 x  − xvqj x . These definitions imply that deg pn  < 1, so we may let αn+1 = pn x .  For the given rational function we have  j 0 1 2  αj  βj  1 3  2 4  x2 + 8x + 19  qj x   x + 5  1  pj x   x2 + 10x + 29  3x + 19  5  so u x  v x  = p0 x  q0 x  = 1 + 2  x + 3 + 4  x + 5  .  Notes: A general rational function of the stated form has 2n + 1 “degrees of freedom,” in the sense that it can be shown to have 2n + 1 essentially independent parameters. If we generalize polynomial chains to quolynomial chains, which allow division operations as well as addition, subtraction, and multiplication  see exercise 71 , we can obtain the following results with slight modifications to the proofs of Theorems A and M: A quolynomial chain with q addition-subtraction steps has at most q + 1   4.6.4  ANSWERS TO EXERCISES  705  degrees of freedom. A quolynomial chain with m multiplication-division steps has at most 2m + 1 degrees of freedom. Therefore a quolynomial chain that computes almost all rational functions of the stated form must have at least 2n addition-subtractions, and n multiplication-divisions; the method in this exercise is optimal. 38. The theorem is certainly true if n = 0. Assume that n is positive, and that a polynomial chain computing P  x; u0, . . . , un  is given, where each of the parameters αj has been replaced by a real number. Let λi = λj × λk be the first chain multiplication step that involves one of u0, . . . , un; such a step must exist because of the rank of A. Without loss of generality, we may assume that λj involves un; thus, λj has the form h0u0 + ··· + hnun + f x , where h0, . . . , hn are real, hn ̸= 0, and f x  is a polynomial with real coefficients.  The h’s and the coefficients of f x  are derived from the values assigned to the α’s.  Now change step i to λi = α× λk, where α is an arbitrary real number.  We could take α = 0; general α is used here merely to show that there is a certain amount of flexibility available in the proof.  Add further steps to calculate λ =  α − f x  − h0u0 − ··· − hn−1un−1  hn;  these new steps involve only additions and parameter multiplications  by suitable new parameters . Finally, replace λ−n−1 = un everywhere in the chain by this new element λ. The result is a chain that calculates Q x; u0, . . . , un−1  = P x; u0, . . . , un−1,  α − f x  − h0u0 − ··· − hn−1un−1  hn ; and this chain has one less chain multiplication. The proof will be complete if we can show that Q satisfies the hypotheses. The quantity  α − f x   hn leads to a possibly increased value of m, and a new vector B′. If the columns of A are A0, A1, . . . , An  these vectors being linearly independent over the reals , the new matrix A′ corresponding to Q has the column vectors  A0 −  h0 hn An,  . . . ,  An−1 −  hn−1 hn An,  plus perhaps a few rows of zeros to account for an increased value of m, and these columns are clearly also linearly independent. By induction, the chain that computes Q has at least n − 1 chain multiplications, so the original chain has at least n.  [Pan showed also that the use of division would give no improvement; see Problemy Kibernetiki 7  1962 , 21–30. Generalizations to the computation of several polynomials in several variables, with and without various kinds of preconditioning, have been given by S. Winograd, Comm. Pure and Applied Math. 23  1970 , 165–179.] 39. By induction on m. Let wm x  = x2m + u2m−1x2m−1 + ··· + u0, wm−1 x  = ur+i+2j aibj. x2m−2 + v2m−3x2m−3 + ··· + v0, a = α1 + γm, b = αm, and let  f r  =  i,j≥0 −1 i+ji+j  It follows that vr = f r + 2  for r ≥ 0, and δm = f 1 . If δm = 0 and a is given, we have a polynomial of degree m − 1 in b, with leading coefficient ± u2m−1 − ma  = ± γ2 + ··· + γm − mγm . In Motzkin’s unpublished notes he arranged to make δk = 0 almost always, by choosing γ’s so that this leading coefficient is ̸= 0 when m is even and = 0 when m is odd; then we can almost always let b be a  real  root of an odd-degree polynomial.  j   706  ANSWERS TO EXERCISES  4.6.4  40. No; S. Winograd found a way to compute all polynomials of degree 13 with only 7  possibly complex  multiplications [Comm. Pure and Applied Math. 25  1972 , 455– 457]. L. Revah found schemes that evaluate almost all polynomials of degree n ≥ 9 with ⌊n 2⌋ + 1  possibly complex  multiplications [SICOMP 4  1975 , 381–392]; she also showed that when n = 9 it is possible to achieve ⌊n 2⌋+1 multiplications only with at least n + 3 additions. By appending sufficiently many additions  see exercise 39 , the “almost all” and “possibly complex” provisos disappear. V. Y. Pan [STOC 10  1978 , 162–172; IBM Research Report RC7754  1979 ] found schemes with ⌊n 2⌋ + 1  complex  multiplications and the minimum number n+2+ δn9 of  complex  additions, for all odd n ≥ 9; his method for n = 9 is  v x  =   x + α 2 + β  x + γ ,  w x  = v x  + x,  t1 x  =  v x  + δ1  w x  + ϵ1 ,  t2 x  =  v x  + δ2  w x  + ϵ2 ,  u x  =  t1 x  + ζ  t2 x  − t1 x  + η  + κ.  The minimum number of real additions necessary, when the minimum number of  real  multiplications is achieved, remains unknown for n ≥ 9. 41. a c + d −  a + b d + i a c + d  +  b− a c . [Beware of numerical instability. Three multiplications are necessary, since complex multiplication is a special case of  71  with p u  = u2 + 1. Without the restriction on additions there are other possibilities. For example, the symmetric formula ac − bd + i  a + b  c + d  − ac − bd  was suggested by Peter Ungar in 1963; Eq. 4.3.3– 2  is similar, with 2n in the role of i. See I. Munro, STOC 3  1971 , 40–44; S. Winograd, Linear Algebra and Its Applications 4  1971 , 381–388.] Alternatively, if a2+b2 = 1 and t =  1−a  b = b  1+a , the algorithm “w = c−td, v = d + bw, u = w − tv” for calculating the product  a + bi  c + di  = u + iv has been suggested by Oscar Buneman [J. Comp. Phys. 12  1973 , 127–128]. In this method if a = cos θ and b = sin θ, we have t = tan θ 2 .  Helmut Alt and Jan van Leeuwen [Computing 27  1981 , 205–215] have shown that four real multiplications or divisions are necessary for computing 1  a + bi , and four are sufficient for computing  a  b + ci  =  a  b + c c b  − i   c b a b + c c b  .  Six multiplication-division operations and three addition-subtractions are necessary and sufficient to compute  a + bi   c + di . [T. Lickteig, SICOMP 16  1987 , 278–311]. In spite of these lower bounds, one should remember that complex arithmetic need not be implemented in terms of real arithmetic. For example, the time needed to multiply two n-place complex numbers is asymptotically only about twice the time to multiply two n-place real numbers, using fast Fourier transforms. 42.  a  Let π1, . . . , πm be the λi’s that correspond to chain multiplications; then πi = P2i−1 × P2i and u x  = P2m+1, where each Pj has the form βj + βj0x + βj1π1 + ···+βjr j πr j , where r j  ≤ ⌈j 2⌉−1 and each of the βj and βjk is a polynomial in the The result set now has at most m + 1 +2m α’s with integer coefficients. We can systematically modify the chain  see exercise 30  so that βj = 0 and βjr j  = 1, for 1 ≤ j ≤ 2m; furthermore we can assume that β30 = 0. j=1 ⌈j 2⌉ − 1  = m2 + 1 degrees of freedom.  b  Any such polynomial chain with at most m chain multiplications can be simulated by one with the form considered in  a , except that now we let r j  = ⌈j 2⌉−1 for 1 ≤ j ≤ 2m + 1, and we do not assume that β30 = 0 or that βjr j  = 1 for j ≥ 3.   4.6.4  ANSWERS TO EXERCISES  707   c  Set m ← ⌊√  This single canonical form involves m2 + 2m parameters. As the α’s run through all integers and as we run through all chains, the β’s run through at most 2m2+2m sets of values mod 2, hence the result set does also. In order to obtain all 2n polynomials of degree n with 0–1 coefficients, we need m2 + 2m ≥ n. n⌋ and compute x2, x3, . . . , xm. Let u x  = um+1 x x m+1 m + ··· + u1 x xm + u0 x , where each uj x  is a polynomial of degree ≤ m with integer coefficients  hence it can be evaluated without any more multiplications . Now evaluate u x  by rule  2  as a polynomial in xm with known coefficients.  The number of additions used is approximately the sum of the absolute values of the coefficients, so this algorithm is efficient on 0–1 polynomials. Paterson and Stockmeyer also gave another algorithm that uses about  √ 2n multiplications.   References: SICOMP 2  1973 , 60–66; see also J. E. Savage, SICOMP 3  1974 , 150–158; J. Ganz, SICOMP 24  1995 , 473–483. For analogous results about additions, see Borodin and Cook, SICOMP 5  1976 , 146–157; Rivest and Van de Wiele, Inf. Proc. Letters 8  1979 , 178–180. 43. When ai = aj + ak is a step in some optimal addition chain for n + 1, compute xi = xjxk and pi = pkxj + pj, where pi = xi−1 + ··· + x + 1; omit the final calculation of xn+1. We save one multiplication whenever ak = 1, in particular when i = 1.  See 2.  exercise 4.6.3–31 with ϵ = 1 44. Let l = ⌊lg n⌋, and suppose x, x2, x4, . . . , x2l have been precomputed. If u x  is monic of degree n = 2m + 1, we can write u x  =  xm+1 + α v x  + w x , where v x  and w x  are monic of degree m. This yields a method for n = 2l+1 − 1 ≥ 3 that requires 2l − 1 further multiplications and 2l+1 + 2l−1 − 2 additions. If n = 2l we can apply Horner’s rule to reduce n by 1. And if m = 2l < n < 2l+1 − 1, we can write u x  = xmv x  + w x  where v and w are monic of degrees n − m and m, respectively; by induction on l, this requires at most 1 4 n additions, [See S. Winograd, IBM Tech. Disclosure Bull. 13  1970 , after the precomputation. 1133–1135.]  √ n   multiplications and n   additions, under the same ground rules, if our goal is to minimize multi-  Note: It is also possible to evaluate u x  with 1  √ n + O  plications + additions. The generic polynomial  2 n + l − 1 multiplications and 5  2 n + O   pjkm x  =  . . .    xm + α0  xj+1 + β1  + α1  xj+2 + β2   + α2  ···     xk + βk−j  + αk−j   xj + β0        k + 1   − j  “covers” the coefficients of exponents {j, j + k, j + k +  k − 1 , . . . , j + k +  k − 1  + ··· +  j + 1 , m′ − k, m′ − k + 1, . . . , m′ − j}, where m′ = m + j +  j + 1  + ··· + k = m +   +k−j+2 j+1  , we obtain an arbitrary monic polynomial of degree k2 + k + 1. [Rabin  By adding together such polynomials p1km1 x , p2km2 x , . . . , pkkmk x  for mj = and Winograd, Comm. on Pure and Applied Math. 25  1972 , 433–458, §2; this paper 2 n + O log n  multiplications and ≤  1 + ϵ n also proves that constructions with 1 additions are possible for all ϵ > 0, if n is large enough.] 45. It suffices to show that  Tijk ’s rank is at most that of  tijk , since we can obtain  tijk  back from  Tijk  by transforming it in the same way with F −1, G−1, H−1. If  2  2  2  2  .   4.6.4  ANSWERS TO EXERCISES  708  tijk =r  l=1 ail bjl ckl then it follows immediately that  Tijk =  1≤l≤r m  i′=1 Fii′ ai′l  n  j′=1 Gjj′ bj′l  s  k′=1 Hkk′ ck′l .  q r  0 0  0 0  0 1  0 1  0 0  0 0  0 0  [H. F. de Groote has proved that all normal schemes that yield 2 × 2 matrix products with seven chain multiplications are equivalent, in the sense that they can be obtained from each other by nonsingular matrix multiplication as in this exercise. In this sense Strassen’s algorithm is unique. See Theor. Comp. Sci. 7  1978 , 127–148.] 46. By exercise 45 we can add any multiple of a row, column, or plane to another one without changing the rank; we can also multiply a row, column, or plane by a nonzero constant, or transpose the tensor. A sequence of such operations can always be found to  reduce a given 2×2×2 tensor to one of the forms0 0 , 0 0 ,1 0 0 0 ,1 0 0 0 ,1 0 0 0 . The last tensor has rank 3 or 2 according as the polynomial u2 − ru − q has 1 0 0 1  0 0 0 1 one or two irreducible factors in the field of interest, by Theorem W  see  74  . 47. A general m × n × s tensor has mns degrees of freedom. By exercise 28 it is impossible to express all m × n × s tensors in terms of the  m + n + s r elements of a realization  A, B, C  unless  m + n + s r ≥ mns. On the other hand, assume that m ≥ n ≥ s. The rank of an m × n matrix is at most n, so we can realize any tensor in ns chain multiplications by realizing each matrix plane separately. [Exercise 46 shows that this lower bound on the maximum tensor rank is not best possible, nor is the upper bound. Thomas D. Howell  Ph.D. thesis, Cornell Univ., 1976  has shown that there are tensors of rank ≥ ⌈mns  m + n + s − 2 ⌉ over the complex numbers.] 48. If  A, B, C  and  A′, B′, C′  are realizations of  tijk  and  t′ijk  of respective lengths r and r′, then A′′ = A⊕A′, B′′ = B⊕B′, C′′ = C⊕C′, and A′′′ = A⊗A′, B′′′ = B⊗B′, C′′′ = C ⊗ C′, are realizations of  t′′ijk  and  t′′′ijk  of respective lengths r + r′ and r · r′. Note: Many people have made the natural conjecture that rank  tijk  ⊕  t′ijk   = rank tijk  + rank t′ijk , but the constructions in exercise 60 b  and exercise 65 make this seem much less plausible than it once was. 49. By Lemma T, rank tijk  ≥ rank ti jk  . Conversely if M is a matrix of rank r we can transform it by row and column operations, finding nonsingular matrices F and G such that F M G has all entries 0 except for r diagonal elements that are 1; see Algorithm 4.6.2N. The tensor rank of F M G is therefore ≤ r; and it is the same as the tensor rank of M, by exercise 45. 50. Let i = ⟨i′, i′′⟩ where 1 ≤ i′ ≤ m and 1 ≤ i′′ ≤ n; then t⟨i′,i′′ ⟩jk = δi′′jδi′k, and it is clear that rank ti jk   = mn since  ti jk   is a permutation matrix. By Lemma L, rank tijk  ≥ mn. Conversely, since  tijk  has only mn nonzero entries, its rank is clearly ≤ mn.  There is consequently no normal scheme requiring fewer than the mn obvious multiplications. There is no such abnormal scheme either [Comm. Pure and Appl. Math. 3  1970 , 165–179]. But some savings can be achieved if the same matrix is used with s > 1 different column vectors, since this is equivalent to  m × n  times  n × s  matrix multiplication.  2 x0 − x1 s2; w0 = 51.  a  s1 = y0 + y1, s2 = y0 − y1; m1 = 1 m1 + m2, w1 = m1 − m2.  b  Here are some intermediate steps, using the methodology in the text:   x0 − x2  +  x1 − x2 u   y0 − y2  +  y1 − y2 u  mod  u2 + u + 1  =   x0 − x2  y0 − y2  −  x1 − x2  y1 − y2   +   x0 − x2  y0 − y2  −  x1 − x0  y1 − y0  u.  2 x0 + x1 s1, m2 = 1  The first realization is 1 1 1 0    1 0 1 1 1 1 0 1  ,   1 1 1 0    1 0 1 1 1 1 0 1  ,   1 1 1 2    1 1 2 1 1 2 1 1  × 1 3 .   4.6.4  The second realization is   1 1 1 2    1 1 2 1 1 2 1 1  × 1 3 ,   1 1 1 0    1 1 0 1 1 0 1 1  ,   1 1 1 0    1 0 1 1 1 1 0 1  .  ANSWERS TO EXERCISES  709  ⟩  ⟩  0≤l≤e  ⟩y⟨j′,j′′  3 x0+x1+x2 s5, m2 = 1  3 x0+x1−2x2 s2, m3 = 1  =  x⟨i′,i′′  Then f aipk  = ys =   0≤j<n l  ωg i,j,k,l F  ajpl  where g i, j, k, l  = ai+jpk+l.  The resulting algorithm computes s1 = y0 + y1, s2 = y0 − y1, s3 = y2 − y0, s4 = y2 − y1, 3 x0−2x1+x2 s3, s5 = s1+y2; m1 = 1 3 −2x0 + x1 + x2 s4; t1 = m1 + m2, t2 = m1 − m2, t3 = m1 + m3, w0 = t1 − m3, m4 = 1 w1 = t3 + m4, w2 = t2 − m4. 52. Let k = ⟨k′, k′′⟩ when k mod n′ = k′ and k mod n′′ = k′′. Then we wish to compute summed for i′ + j′ ≡ k′  modulo n′  and i′′ + j′′ ≡ k′′ w⟨k′,k′′  modulo n′′ . This can be done by applying the n′ algorithm to the 2n′ vectors Xi′ and Yj′ of length n′′, obtaining the n′ vectors Wk′. Each vector addition becomes n′′ additions, each parameter multiplication becomes n′′ parameter multiplications, and each chain multiplication of vectors is replaced by a cyclic convolution of degree n′′. [If the subalgorithms use the minimum number of chain multiplications over the rationals, this algorithm uses 2 n′ − d n′   n′′ − d n′′   more than the minimum, where d n  is the number of divisors of n, because of exercise 4.6.2–32 and Theorem W.] 53.  a  Let n k  =  p − 1 pe−k−1 = φ pe−k  for 0 ≤ k < e, and n k  = 1 for k ≥ e. Represent the numbers {1, . . . , m} in the form aipk  modulo m , where 0 ≤ k ≤ e and 0 ≤ i < n k , and a is a fixed primitive element modulo pe. For example, when m = 9 we can let a = 2; the values are {2030, 2130, 2031, 2230, 2530, 2131, 2430, 2330, 2032}. We shall compute fikl = 0≤j<n l  ωg i,j,k,l F  ajpl  for 0 ≤ i < n k  and for each 0≤j<n l [s + j ≡ 0  modulo n k + l  ] F  ajpl , since fikl is xrys summed over k and l. This is a cyclic convolution of degree n k + l  on the values xi = ωaipk+l and r+s ≡ i  modulo n k+l  . The Fourier transform is obtained by summing appropriate fikl’s. [Note: When linear combinations of the xi are formed, for example as in  69 , the result will be purely real or purely imaginary, when the cyclic convolution algorithm has been constructed by using rule  59  with un k  −1 =  un k  2 −1  un k  2 +1 . The reason is that reduction mod  un k  2 − 1  produces a polynomial with real coefficients ωj + ω−j while reduction mod  un k  2 + 1  produces a polynomial with imaginary coefficients ωj − ω−j.] When p = 2 an analogous construction applies, using the representation  −1 iaj2k  modulo m , where 0 ≤ k ≤ e and 0 ≤ i ≤ min e − k, 1  and 0 ≤ j < 2e−k−2. In this case we use the construction of exercise 52 with n′ = 2 and n′′ = 2e−k−2; although these numbers are not relatively prime, the construction does yield the desired direct product of cyclic convolutions.  b  Let a′m′ + a′′m′′ = 1; and let ω′ = ωa′′m′′, ω′′ = ωa′m′. Define s′ = s mod m′, It F  t′, t′′ ; in other words, the one-dimensional Fourier transform on m elements is actually a two-dimensional Fourier transform on m′ × m′′ elements, in slight disguise.  s′′ = s mod m′′, t′ = t mod m′, t′′ = t mod m′′, so that ωst =  ω′ s′t′ ω′′ s′′t′′.  follows that f s′, s′′  = m′  t′′=0  ω′ s′t′ ω′′ s′′t′′  m′′  We shall deal with “normal” algorithms consisting of  i  a number of sums si of the F ’s and s’s; followed by  ii  a number of products mj, each of which is obtained by multiplying one of the F ’s or S’s by a real or imaginary number αj; followed by  iii  a number of further sums tk, each of which is formed from m’s or t’s  not F ’s or s’s . The final values must be m’s or t’s. For example, the “normal” Fourier transform  −1 t′=0  −1   710  ANSWERS TO EXERCISES  4.6.4  4 ω+ω2+ω4+ω3 s3, m2 = 1 2 −ω + ω2 + ω4 − ω3 s6, m5 = 1  scheme for m = 5 constructed from  69  and the method of part  a  is as follows: s1 = F  1  + F  4 , s2 = F  3  + F  2 , s3 = s1 + s2, s4 = s1 − s2, s5 = F  1  − F  4 , s6 = F  2 −F  3 , s7 = s5−s6; m1 = 1 4 ω−ω2+ω4−ω3 s4, 2 ω3 − ω2 s7, 2 ω + ω2 − ω4 − ω3 s5, m4 = 1 m3 = 1 m6 = 1 · F  5 , m7 = 1 · s3; t0 = m1 + m6, t1 = t0 + m2, t2 = m3 + m5, t3 = t0 − m2, t4 = m4 − m5, t5 = t1 + t2, t6 = t3 + t4, t7 = t1 − t2, t8 = t3 − t4, t9 = m6 + m7. Note the multiplication by 1 shown in m6 and m7; this is required by our conventions, and it is important to include such cases for use in recursive constructions  although the multiplications need not really be done . Here m6 = f001, m7 = f010, t5 = f000 + f001 = f 20 , t6 = f100 + f101 = f 21 , etc. We can improve the scheme by introducing s8 = s3 + F  5 , replacing m1 by   1 4 s3], replacing m6 by 1·s8, and deleting m7 and t9; this saves one of the trivial multiplications by 1, and it will be advantageous when the scheme is used to build larger ones. In the improved scheme, f 5  = m6, f 1  = t5, f 2  = t6, f 3  = t8, f 4  = t7. Now suppose we have normal one-dimensional schemes for m′ and m′′, using respectively  a′, a′′  complex additions,  t′, t′′  trivial multiplications by ±1 or ±i, and a total of  c′, c′′  complex multiplications including the trivial ones.  The nontrivial complex multiplications are all “simple” since they involve only two real multiplications and no real additions.  We can construct a normal scheme for the two-dimensional m′ × m′′ case by applying the m′ scheme to vectors F  t′,∗  of length m′′. Each si step becomes m′′ additions; each mj becomes a Fourier transform on m′′ elements, but with all of the α’s in this algorithm multiplied by αj; and each tk becomes m′′ additions. Thus the new algorithm has  a′m′′ + c′a′′  complex additions, t′t′′ trivial multiplications, and a total of c′c′′ complex multiplications.  4 ω + ω2 + ω4 + ω3 − 1 s3 [this is − 5  Using these techniques, Winograd has found normal one-dimensional schemes for  the following small values of m with the following costs  a, t, c :  m = 2 m = 3 m = 4 m = 5    2, 2, 2    6, 1, 3    8, 4, 4   17, 1, 6   m = 7 m = 8 m = 9 m = 16   36, 1, 9   26, 6, 8   46, 1, 12   74, 8, 18   By combining these schemes as described above, we obtain methods that use fewer arithmetic operations than the “fast Fourier transform”  FFT  discussed in exercise 14. For example, when m = 1008 = 7·9·16, the costs come to  17946, 8, 1944 , so we can do a Fourier transform on 1008 complex numbers with 3872 real multiplications and 35892 real additions. It is possible to improve on Winograd’s method for combining relatively prime moduli by using multidimensional convolutions, as shown by Nussbaumer and Quandalle in IBM J. Res. and Devel. 22  1978 , 134–144; their ingenious approach reduces the amount of computation needed for 1008-point complex Fourier transforms to 3084 real multiplications and 34668 real additions. By contrast, the FFT on 1024 complex numbers involves 14344 real multiplications and 27652 real additions. If the two-passes-at-once improvement in the answer to exercise 14 is used, however, the FFT on 1024 complex numbers needs only 10936 real multiplications and 25948 additions, and it is not difficult to implement. Therefore the subtler methods are faster only on machines that take significantly longer to multiply than to add.  [References: Proc. Nat. Acad. Sci. USA 73  1976 , 1005–1006; Math. Comp. 32  1978 , 175–199; Advances in Math. 32  1979 , 83–117; IEEE Trans. ASSP-27  1979 , 169–181.] 54. max 2e1deg p1  − 1, . . . , 2eqdeg pq  − 1, q + 1 .   4.6.4  ANSWERS TO EXERCISES  711  j bjl xj  = l=1 cl   l=1 ckl  i ail xi   i αil xi  βl +  i,j tijkxixj = i ail xi    of rank r, thenr the product  αl + can be expressed as a linear combinationr  55. 2n′ − q′, where n′ is the degree of the minimum polynomial of P  the monic polynomial µ of least degree such that µ P   is the zero matrix  and q′ is the number of distinct irreducible factors it has.  Reduce P by similarity transformations.  56. Let tijk + tjik = τijk + τjik, for all i, j, k. If  A, B, C  is a realization of  tijk  i,j τijkxixj for all k. Conversely, let the lth chain multiplication of a polynomial chain, for 1 ≤ l ≤ r, be j βjlxj , where αl and βl denote possible constant terms and or nonlinear terms. All terms of degree 2 appearing at any step of the chain j bjl xj ; hence the chain defines a tensor  tijk  of rank ≤ r such that tijk +tjik = τijk +τjik. This establishes the hint. Now rank τijk + τjik  = rank tijk + tjik  ≤ rank tijk  + rank tjik  = 2 rank tijk . A bilinear form in x1, . . . , xm, y1, . . . , yn is a quadratic form in m + n variables, where τijk = ti,j−m,k for i ≤ m and j > m, otherwise τijk = 0. Now rank τijk  + rank τjik  ≥ rank tijk , since we obtain a realization of  tijk  by suppressing the last n rows of A and the first m rows of B in a realization  A, B, C  of  τijk + τjik . vn+1 = ··· = vN−1 = 0. If Us = N−1 57. Let N be the smallest power of 2 that exceeds 2n, and let un+1 = ··· = uN−1 = where ω = e2πi N, thenN−1 s=0 ω−stUsVs = N ut1 vt2, where the latter sum is taken t=0 ωstvt for 0 ≤ s < N, over all t1 and t2 with 0 ≤ t1, t2 < N, t1 + t2 ≡ t  modulo N . The terms vanish unless t1 ≤ n and t2 ≤ n, so t1 + t2 < N; thus the sum is the coefficient of zt in the product u z v z . If we use the method of exercise 14 to compute the Fourier transforms and the inverse transforms, the number of complex operations is O N log N +O N log N + O N  + O N log N ; and N ≤ 4n. [See Section 4.3.3C and the paper by J. M. Pollard, Math. Comp. 25  1971 , 365–374.]  t=0 ωstut and Vs = N−1  When multiplying integer polynomials, it is possible to use an integer number ω that is of order 2t modulo a prime p, and to determine the results modulo sufficiently many primes. Useful primes in this regard, together with their least primitive roots r  from which we take ω = r p−1  2t mod p when p mod 2t = 1 , can be found as described in Section 4.5.4. For t = 9, the ten largest cases < 235 are p = 235 − 512a + 1, where  a, r  =  28, 7 ,  31, 10 ,  34, 13 ,  56, 3 ,  58, 10 ,  76, 5 ,  80, 3 ,  85, 11 ,  91, 5 ,  101, 3 ; the ten largest cases < 231 are p = 231 − 512a + 1, where  a, r  =  1, 10 ,  11, 3 ,  19, 11 ,  20, 3 ,  29, 3 ,  35, 3 ,  55, 19 ,  65, 6 ,  95, 3 ,  121, 10 . For larger t, all primes p of the form 2tq + 1 where q < 32 is odd and 224 < p < 236 are given by  p − 1, r  =  11 · 221, 3 ,  25 · 220, 3 ,  27 · 220, 5 ,  25 · 222, 3 ,  27 · 222, 7 ,  5 · 225, 3 ,  7 · 226, 3 ,  27 · 226, 13 ,  15 · 227, 31 ,  17 · 227, 3 ,  3 · 230, 5 ,  13 · 228, 3 ,  29 · 227, 3 ,  23· 229, 5 . Some of the latter primes can be used with ω = 2e for appropriate small e. For a discussion of such primes, see R. M. Robinson, Proc. Amer. Math. Soc. 9  1958 , 673–681; S. W. Golomb, Math. Comp. 30  1976 , 657–663. Additional all-integer methods are cited in the answer to exercise 4.6–5.  However, the method of exercise 59 will almost always be preferable in practice. 58.  a  In general if  A, B, C  realizes  tijk , then   x1, . . . , xm A, B, C  is a realization  of the 1× n× s matrix whose entry in row j, column k is xitijk. So there must be at  least as many nonzero elements in  x1, . . . , xm A as the rank of this matrix. In the case of the m× n×  m + n− 1  tensor corresponding to polynomial multiplication of degree m − 1 by degree n − 1, the corresponding matrix has rank n whenever  x1, . . . , xm  ̸=  0, . . . , 0 . A similar statement holds with A ↔ B and m ↔ n.  Notes: In particular, if we work over the field of 2 elements, this says that the rows of A modulo 2 form a “linear code” of m vectors having distance at least n,   712  ANSWERS TO EXERCISES  4.6.4  whenever  A, B, C  is a realization consisting entirely of integers. This observation, due to R. W. Brockett and D. Dobkin [Linear Algebra and Its Applications 19  1978 , 207–235, Theorem 14; see also Lempel and Winograd, IEEE Trans. IT-23  1977 , 503– 508; Lempel, Seroussi, and Winograd, Theoretical Comp. Sci. 22  1983 , 285–296], can be used to obtain nontrivial lower bounds on the rank over the integers. For example, M. R. Brown and D. Dobkin [IEEE Trans. C-29  1980 , 337–340] have used it to show that realizations of n × n polynomial multiplication over the integers must have rank ≥ αn for all sufficiently large n, when α is any real number less than  αmin = 3.52762 68026 32407 48061 54754 08128 07512 70182+;  here αmin = 1 H sin2 θ, cos2 θ , where H p, q  = p lg 1 p  + q lg 1 q  is the binary entropy function and θ ≈ 1.34686 is the root of sin2 θ − π 4  = H sin2 θ, cos2 θ . An all-integer realization of rank O n log n , based on cyclotomic polynomials, has been constructed by M. Kaminski [J. Algorithms 9  1988 , 137–147].  The following economical ways to realize the multiplication of general polynomials of degrees 2, 3, and 4 have been presented by H. Cohen and A. K. Lenstra [see Math.   1 0 0 0 0 1 1 1    0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1  ,   b    1 0 0 0 0 1 1 1  0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1  Comp. 48  1987 , S1–S2]: 1 0 0 1 1 0  0 1 0 1 0 1 0 0 1 0 1 1    1 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1    1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1   ,  ,   ,  same,  same,  same,   ,     .  1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0   ;  ;    1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0    .   4.6.4  ANSWERS TO EXERCISES  713  In each case the A and B matrices are identical. 59. [IEEE Trans. ASSP-28  1980 , 205–215.] Note that cyclic convolution is polyno- mial multiplication mod un−1, and negacyclic convolution is polynomial multiplication mod un +1. Let us now change notation, replacing n by 2n; we shall consider recursive algorithms for cyclic and negacyclic convolution  z0, . . . , z2n−1  of  x0, . . . , x2n−1  with  y0, . . . , y2n−1 . The algorithms are presented in unoptimized form, for brevity and ease in exposition; readers who implement them will notice that many things can be streamlined. For example, the final value of Z2m−1 w  in step N5 will always be zero.  C1. [Test for simple case.] If n = 1, set  z0 ← x0y0 + x1y1,  z1 ←  x0 + x1  y0 + y1  − z0,  and terminate. Otherwise set m ← 2n−1.  C2. [Remainderize.] For 0 ≤ k < m, set  xk, xm+k  ←  xk + xm+k, xk − xm+k  and  yk, ym+k  ←  yk +ym+k, yk−ym+k .  Now we have x u  mod  um−1  = x0 + ··· + xm−1um−1 and x u  mod  um + 1  = xm + ··· + x2m−1um−1; we will compute x u y u  mod  um −1  and x u y u  mod  um +1 , then we will combine the results by  59 .   C3. [Recurse.] Set  z0, . . . , zm−1  to the cyclic convolution of  x0, . . . , xm−1  with  y0, . . . , ym−1 . Also set  zm, . . . , z2m−1  to the negacyclic convolution of  xm, . . . , x2m−1  with  ym, . . . , y2m−1 . 2 zk + zm+k, zk − zm+k . Now  z0, . . . , z2m−1  is the desired answer.  C4. [Unremainderize.] For 0 ≤ k < m, set  zk, zm+k  ← 1  N1. [Test for simple case.] If n = 1, set t ← x0 y0 + y1 , z0 ← t −  x0 + x1 y1, z1 ← t+ x1−x0 y0, and terminate. Otherwise set m ← 2⌊n 2⌋ and r ← 2⌈n 2⌉.  The following steps use 2n+1 auxiliary variables Xij for 0 ≤ i < 2m and 0 ≤ j < r, to represent 2m polynomials Xi w  = Xi0 + Xi1w +··· + Xi r−1 wr−1; similarly, there are 2n+1 auxiliary variables Yij.  N2. [Initialize auxiliary polynomials.] Set Xij ← X i+m j ← xmj+i, Yij ← Y i+m j ← ymj+i, for 0 ≤ i < m and 0 ≤ j < r.  At this point we have x u  = X0 um  + uX1 um  + ··· + um−1Xm−1 um , and a similar formula holds for y u . Our strategy will be to multiply these polynomials modulo  umr + 1  =  u2n + 1 , by operating modulo  wr + 1  on the polynomials X w  and Y  w , finding their cyclic convolution of length 2m and thereby obtaining x u y u  ≡ Z0 um  + uZ1 um  + ··· + u2m−1Z2m−1 um .  N3. [Transform.]  Now we will essentially do a fast Fourier transform on the poly- nomials  X0, . . . , Xm−1, 0, . . . , 0  and  Y0, . . . , Ym−1, 0, . . . , 0 , using wr m as a  2m th root of unity. This is efficient, because multiplication by a power of w is not really a multiplication at all.  For j = ⌊n 2⌋−1, . . . , 1, 0  in this order , do the following for all m binary numbers s + t =  s⌊n 2⌋ . . . sj+10 . . . 0 2 +  0 . . . 0tj−1 . . . t0 2: Replace  Xs+t w , Xs+t+2j  w   by the pair of polynomi- Xs+t+2j  w , Xs+t w  − w r m s′ als  Xs+t w  + w r m s′ Xs+t+2j  w  , where  2.  We are evaluating 4.3.3– 39 , with K = 2m and s′ = 2j sj+1 . . . s⌊n 2⌋ ω = wr m; notice the bit-reversal in s′. The polynomial operation Xi w  ← Xi w  + wkXl w  means, more precisely, that we set Xij ← Xij + Xl j−k  for k ≤ j < r, and Xij ← Xij − Xl j−k+r  for 0 ≤ j < k. A copy of Xl w  can be made without wasting much space.  Do the same transformation on the Y ’s.   714  ANSWERS TO EXERCISES  4.6.4 N4. [Recurse.] For 0 ≤ i < 2m, set  Zi0, . . . , Zi r−1   to the negacyclic convolution N5. [Untransform.] For j = 0, 1, . . . , ⌊n 2⌋  in this order , and for all m choices  of  Xi0, . . . , Xi r−1   and  Yi0, . . . , Yi r−1  . of s and t as in steps N3, set  Zs+t w , Zs+t+2j  w   to  1  2 Zs+t w  + Zs+t+2j  w , w− r m s′ Zs+t w  − Zs+t+2j  w   .  +  ⌊n 2⌋ + 1 2n+1 + 2n, Dn = 2⌊n 2⌋+1D⌈n 2⌉  N6. [Repack.]  Now we have accomplished the goal stated at the end of step N2, since it is easy to show that the transform of the Z’s is the product of the transforms of the X’s and the Y ’s.  Set zi ← Zi0 − Z m+i  r−1  and zmj+i ← Zij + Z m+i  j−1  for 0 < j < r, for 0 ≤ i < m. It is easy to verify that at most n extra bits of precision are needed for the intermediate variables in this calculation; for example, if xi ≤ M for 0 ≤ i < 2n at the beginning of the algorithm, then all of the x and X variables will be bounded by 2nM throughout. All of the z and Z variables will be bounded by  2nM 2, which is n more bits than required to hold the final convolution. Algorithm N performs An addition-subtractions, Dn halvings, and Mn multipli- cations, where A1 = 5, D1 = 0, M1 = 3; for n > 1 we have An = ⌊n 2⌋2n+2 + +  ⌊n 2⌋ + 1 2n+1, and 2⌊n 2⌋+1A⌈n 2⌉ . The solutions are An = 11 · 2n−1+⌈lg n⌉ − 3 · 2n + 6 · 2nSn, Mn = 2⌊n 2⌋+1M⌈n 2⌉ Dn = 4 · 2n−1+⌈lg n⌉ − 2 · 2n + 2 · 2nSn, Mn = 3 · 2n−1+⌈lg n⌉; here Sn satisfies the +⌊n 2⌋, and it is not difficult to prove the inequalities recurrence S1 = 0, Sn = 2S⌈n 2⌉ 2 n⌈lg n⌉ ≤ Sn ≤ Sn+1 ≤ 1 2 n lg n + n for all n ≥ 1. Algorithm C does approximately 1 the same amount of work as Algorithm N. 60.  a  In Σ1, for example, we can group all terms having a common value of j and k into a single trilinear term; this gives ν2 trilinear terms when  j, k  ∈ E×E, plus ν2 when  j, k  ∈ E×O and ν2 when  j, k  ∈ O×E. When ˜ȷ = k we can also include −xjj yj˜ȷ z˜ȷj in Σ1, free of charge. [In the case n = 10, the method multiplies 10 × 10 matrices with 710 noncommutative multiplications; this is almost as good as seven 5 × 5 multiplications by the method of Makarov cited in the answer to exercise 12, although Winograd’s scheme  35  uses only 600 when commutativity is allowed. With a similar scheme, Pan showed for the first time that M n  < n2.8 for all large n, and this awakened great interest in the problem. See SICOMP 9  1980 , 321–342.]  b  Here we simply let S be all the indices  i, j, k  of one problem, ˜S the indices [k, i, j] of the other, and work with an  mn+sm × ns+mn × sm+ns  tensor. [When m = n = s = 10, the result is quite surprising: We can multiply two separate 10 × 10 matrices with 1300 noncommutative multiplications, while no scheme is known that would multiply each of them with 650.]  µ ailµuµ, etc., in a polynomial l=1 ailµbjlν cklσ. [This result can be improved to rank tijk  ≤  2d + 1  rankd tijk  in an infinite field, because µ+ν+σ=d aµbν cσ corresponds to multiplication of polynomials mod- ulo ud+1, as pointed out by Bini and Pan. See Calcolo 17  1980 , 87–97.]  c, d  This is clear from the realizations in exercise 48.  61.  a  Replace ail u  by uail u .  b  Let ail u  = realization of length r = rankd tijk . Then tijk = the trilinear form  e  Suppose we have realizations of t and rt′ such that r O ud+1  andR r R  ⟩L = [i = j = k] t′i′j′k′ ud′ + O ud′+1 . Then r ⟩L = tijkt′i′j′k′ ud+d′ + O ud+d′+1 .  l=1 ailbjlckl = tijkud +  bjmB⟨mj′  L=1 A⟨ii′  cknC⟨nk′  r  r  ⟩LC⟨kk′  ailA⟨li′  ⟩LB⟨jj′  µ+ν+σ=d  ⟩L  ⟩L  L=1  l=1  m=1  n=1   715  4.6.4  ′  ′  u  u  2  1  0  ⟩  ⟩  ⟩  ⟩  0 1  1 0  1 0  ⟩⟨K,I  0 −1  62. The rank is 3, by the method of proof in Theorem W with P = 0 of the realization 1  rank cannot be 1, since we cannot have a1 u b1 u c1 u  ≡ a1 u b2 u c2 u  ≡ ud and a1 u b2 u c1 u  ≡ a1 u b1 u c2 u  ≡ 0  modulo ud+1 . The border rank is 2 because  . The border  ANSWERS TO EXERCISES  , u  , 1  .  ⟩⟨k,i′  ⟩⟨j,k′  ⟩⟨j,k′  ⟩⟨K,I′  The notion of border rank was introduced by Bini, Capovani, Lotti, and Romani  ⟩⟨J,K′ ⟩⟨K,I′  , respectively. Each element T  ⟩⟨k,i′ ⟩ of the direct prod- ×  [This result is due to Bini and Schönhage, 1979.]  by definition, so it is [I′ =I and J ′ =J and K′ =K].  M nh  ≤ hd+2 64. We have   d  We have Md mns  ≤ r3 for some d, where Md n  = rankd T  n, n, n  .  in Information Processing Letters 8  1979 , 234–235. 63.  a  Let the elements of T  m, n, s  and T  M, N, S  be denoted by t⟨i,j′ and T⟨I,J′ ′ ⟨I,J ⟩⟨J ,K uct, where I = ⟨i, I⟩, J = ⟨j, J⟩, and K = ⟨k, K⟩, is equal to t⟨i,j′ ⟩⟨J,K′ T⟨I,J′  b  Apply exercise 61 e  with M N  = rank0 T  N, N, N  .  c  We have M mns  ≤ r3, since T  mns, mns, mns  = T  m, n, s  ⊗ T  n, s, m  ⊗ T  s, m, n . If M n  ≤ R we have M nh  ≤ Rh for all h, and it follows that M N  ≤ M n⌈logn N⌉  ≤ R⌈logn N⌉ ≤ RN log R log n. [This result appears in Pan’s paper of 1972.] If Md n  ≤ R we have Mhd nh  ≤ Rh for all h, and the stated formula follows since  Rh by exercise 61 b . In an infinite field we save a factor of log N. k fk u + 1≤i,j,k≤3 xijyjkzki+O u3 , when fk u  =  xk1 + u2xk2  y2k + u2y1k zkk +  xk1 + u2xk3 y3k  1 + u zkk − u z1k + z2k + z3k   − xk1 y2k + y3k  zk1 + zk2 + zk3  and gjk u  =  xk1 + u2xj3  y3k + uy1j  zkj + uzjk  +  xk1 + u2xj2  y2k − uy1j zkj. [The best upper bound known for rank T  3, 3, 3   is 23; see the answer to exercise 12. The border rank of T  2, 2, 2  remains unknown.]  j̸=k gj,k u   = u2 n 65. The polynomial in the hint is u2m −m−1 i=1 Xij, Yin = −n−1 the indeterminates we can compute xiyj for each i and j and alsom n−1 m−1 also derived, among other things, the results of exercises 64, 66, and 67 i .] 66.  a  Let ω = lim inf n→∞ log M n  log n; we have ω ≥ 2 by Lemma T. For all ϵ > 0, there is an N with M N  < N ω+ϵ. The argument of exercise 63 c  now shows that log M n  log n < ω + 2ϵ for all sufficiently large n.  j=1 xiyjzij + XijYijZ  + O u3 . Let Xij and Yij be indeterminates for 1 ≤ i < m and 1 ≤ j < n; also set Xin = Ymj = 0, Xmj = j=1 Yij. Thus with mn + 1 multiplications of polynomials in j=1 XijYij = [SICOMP 10  1981 , 434–455. In this classic paper Schönhage  j=1 XijYij.  n   b  This is an immediate consequence of exercise 63 d .  c  Let r = rank t , q =  mns ω 3, Q =  M N S ω 3. Given ϵ > 0, there is an integer constant cϵ such that M p  ≤ cϵ pω+ϵ for all positive integers p. For every  i=1  i=1  i=1  integer h > 0 we have th = Given h and k, let p = ⌊h  1  ω+ϵ ⌋. Then  T  mkM h−k, nkN h−k, skSh−k , and rank th  ≤ rh. T  mkM h−k, nkN h−k, skSh−k    h  rank T  pmkM h−k, pnkN h−k, pskSh−k   ≤ rank M p T  mkM h−k, nkN h−k, skSh−k    h  k  k  k  ≤ rank cϵ ≤ cϵrh  k  by exercise 63 b , and it follows from part  b  that  pωqkQh−k =  pmkM h−kpnkN h−kpskSh−k ω 3 ≤ cϵrh.   4.6.4  716  Since p ≥h  k  ANSWERS TO EXERCISES  1  ω+ϵ    h  k   2 we have  qkQh−k ≤ h  ϵ  ω+ϵ   k   2p ωqkQh−k ≤ 2ϵh  ω+ϵ 2ωcϵrh.  ⟩⟨ki′  ⟩ ⟨jk′   d  Set m = n = 4 in exercise 65, and note that 160.85 + 90.85 > 17.  Therefore  q + Q h ≤  h + 1 2ϵh  ω+ϵ 2ωcϵrh for all h. And it follows that we must have q + Q ≤ 2ϵ  ω+ϵ r for all ϵ > 0. 67.  a  The mn × mns2 matrix  t⟨ij′ ⟩   has rank mn because it is a permu- tation matrix when restricted to the mn rows for which k = k′ = 1.  b    t ⊕ t′ i jk   is essentially  ti jk   ⊕  t′i jk  , plus n′s + sn′ additional columns of zeros. [Similarly we have   t ⊗ t′ i jk   =  ti jk   ⊗  t′i jk   for the direct product.]  c  Let D be the diagonal matrix diag d1, . . . , dr , so that ADBT = O. We know by Lemma T that rank A  = m and rank B  = n; hence rank AD  = m and rank DBT   = n. We can assume without loss of generality that the first m columns of A are linearly independent. Since the columns of BT are in the null space of AD, we may also assume that the last n columns of B are linearly independent. Write A in the partitioned form  A1 A2 A3  where A1 is m × m  and nonsingular , A2 is m × q, and A3 is m × n. Also partition D so that AD =  A1D1 A2D2 A3D3 . Then there is a q × r matrix W =  W1 I O  such that ADW T = O, namely W1 = −D2AT2 A−T 1 D−1 1 . Similarly, we may write B =  B1 B2 B3 , and we find V DBT = O when V =  O I V3  is the q × r matrix with V3 = −D2BT2 B−T 3 . Notice that UDV T = D2, so the hint is established  more or less — after all, it was just a hint . Now we let Ail u  = ail for 1 ≤ i ≤ m, A m+i l u  = uvil dm+i; Bjl u  = bjl for thatr 1 ≤ j ≤ n, B n+j l u  = wjlu; Ckl u  = u2ckl for 1 ≤ k ≤ s, C s+1 l u  = dl. It follows l=1 Ail u Bjl u Ckl u  = u2tijk + O u3  if k ≤ s, u2[i > m][j > n] if k = s + 1. [In this proof we did not need to assume that t is nondegenerate with respect to C.]  d  Consider the following realization of T  m, 1, n  with r = mn+1: ail = [⌊l n⌋ = i − 1], bjl = [l mod n = j], b⟨ij⟩l = [l =  i−1 n + j], if l ≤ mn; air = 1, bjr = −1, c⟨ij⟩r = 0. This is improvable with dl = 1 for 1 ≤ l ≤ r.  e  The idea is to find an improvable realization of T  m, n, s . Suppose  A, B, C  is a realization of length r. Given arbitrary integers α1, . . . , αm, β1, . . . , βs, extend A, B, and C by defining  3 D−1  A⟨ij′  If dl =m r+n  ⟩ r+p  = αi[j′ = p], B⟨jk′ ⟩l for l ≤ r and dl = −1 otherwise, we have s i′=1 αi′ βk  ⟩ r+p  = βk′[j = p], C⟨ki′ r  s k=1 αi′ βk c⟨ki′ ⟩ldl =  ⟩l − n  m  ⟩lB⟨jk′  ⟩lB⟨jk′  A⟨ij′  A⟨ij′  ⟩ r+p  = 0, for 1 ≤ p ≤ n.  αi[j′ = p] βk′[j = p]  i′=1  ⟩lC⟨ki′ = [j = j′] αiβk′ − [j = j′] αiβk′ = 0;  k=1  l=1  p=1  so this is improvable if d1 . . . dr ̸= 0. But d1 . . . dr is a polynomial in  α1, . . . , αm, β1, . . . , βs , not identically zero, since we can assume without loss of generality that C has no all-zero columns. Therefore some choice of α’s and β’s will work.  l=1   f  If M n  = nω we have M nh  = nhω, hence  rank T  nh, nh, nh  ⊕ T  1, nhω − nh 2nh − 1 , 1   ≤ nhω + nh.  Exercise 66 c  now implies that nhω +  nhω − 2n2h + nh ω 3 ≤ nhω + nh for all h. Therefore ω = 2; but this contradicts the lower bound 2n2 − 1  see the answer to exercise 12 .   4.6.4  ANSWERS TO EXERCISES  717   g  Let f u  and g u  be polynomials such that the elements of V f u  and W g u   are polynomials. Then we redefine  ckl,  r  A i+m l = ud+1  vilf u  di+m, B j+n l = ud+1  where f u g u  = pue + O ue+1 . It follows thatr  wjlg u  p, Ckl = ud+e+2 l=1 Ail u Bjl u Ckl u  is equal to ud+e+2tijk + O ud+e+3  if k ≤ s, ud+e+2[i > m][j > n] if k = s + 1. [Note: The result of  e  therefore holds over any field, if rank2 is replaced by rank, since we can choose the α’s and β’s to be polynomials of the form 1 + O u .]   h  Let row p of C refer to the component T  1, 16, 1 . The key point is that l=1 ail u bjl u cpl u  is zero  not simply O ud+1   for all i and j that remain after deletion; moreover, cpl u  ̸= 0 for all l. These properties are true in the constructions of parts  c  and  g , and they remain true when we take direct products.   i  The proof generalizes from binomials to multinomials in a straightforward way.  j  After part  h  we have 81ω 3 + 2 36ω 3  + 34ω 3 ≤ 100, so ω < 2.52. Squar- ing once again gives rank T  81, 1, 81  ⊕ 4T  27, 4, 27  ⊕ 2T  9, 34, 9  ⊕ 4T  9, 16, 9  ⊕ 4T  3, 136, 3  ⊕ T  1, 3334, 1   ≤ 10000; this yields ω < 2.4999. Success! Continued squaring leads to better and better bounds that converge rapidly to 2.497723729083 . . . . If we had started with T  4, 1, 4 ⊕T  1, 9, 1  instead of T  3, 1, 3 ⊕T  1, 4, 1 , the limiting bound would have been 2.51096309 . . . .  [Similar tricks yield ω < 2.496; see SICOMP 11  1982 , 472–492. The best current  1 + ··· + x2  bound, ω < 2.3727, is due to V. Vassilevska Williams, STOC 44  2012 , 887–898.] 68. T. M. Vari has shown that n − 1 multiplications are necessary, by proving that n multiplications are necessary to compute x2 n [Cornell Computer Science Report 120  1972 ]. C. Pandu Rangan showed that if we compute the polynomial as L1R1 + ··· + Ln−1Rn−1, where the L’s and R’s are linear combinations of the x’s, at least n − 2 additions are needed to form the L’s and R’s [J. Algorithms 4  1983 , 282–285]. But his lower bound does not obviously apply to all polynomial chains. 69. Let yij = xij − [i = j], and apply the recursive construction  31  to the matrix I + Y , using arithmetic on power series in the n2 variables yij but ignoring all terms of total degree > n. Each entry h of the array is represented as a sum h0 + h1 + ··· + hn, where hk is the value of a homogeneous polynomial of degree k. Then every addition step becomes n + 1 additions, and every multiplication step becomes ≈ 1 2 n2 multiplications and ≈ 1 2 n2 additions. Furthermore, every division is by a quantity of the form 1 + h1 + ··· + hn, since all divisions in the recursive construction are by 1 when the yij are entirely zero; therefore division is slightly easier than multiplication  see Eq. 4.7– 3  when V0 = 1 . Since we stop when reaching a 2 × 2 determinant, we need not subtract 1 from yjj when j > n − 2. It turns out that when redundant 6 n5− O n4  of each. A similar method can be used to eliminate division in many other cases; see Crelle 264  1973 , 184–202.  But the next exercise constructs an even faster divisionless scheme for determinants.  70. Set A = λ − x, B = −u, C = −v, and D = λI − Y in the hinted identity, then take the determinant of both sides, using the fact that I λ + Y  λ2 + Y 2 λ3 + ··· is the inverse of D as a formal power series in 1 λ. We need to compute uY kv only for 0 ≤ k ≤ n − 2, because we know that fX λ  is a polynomial of degree n; thus, only n3+ O n2  multiplications and n3+ O n2  additions are needed to advance from degree n − 1 to degree n. Proceeding recursively, we obtain the coefficients of fX from the  computations are suppressed, this method requires 20n multiplications and 20n  +12n − n additions, thus 1  +5n−4  +24n  +8n  −4n  +8n  +4n  4  3  2  5  5  4  3  2   718  4.6.4  2  3  4  2  2  3  4  2  ANSWERS TO EXERCISES  elements of X after doing 6n tions andn   multiplications and 6n If we only want to compute det X =  −1 nfX 0 , we save 3n   + 5n  + 2n   − n + 1 multiplica-  additions. This division-free method for determinant evaluation is in fact  addition-subtractions.   + 7n   + 2n  quite economical when n has a moderate size; it beats the obvious cofactor expansion scheme when n > 4.  If ω is the exponent of matrix multiplication in exercise 66, the same approach leads to a division-free computation in O nω+1+ϵ  steps, because the vectors uY k for 0 ≤ k < n can be evaluated in O M n  log n  steps: Take a matrix whose first 2l rows are uY k for 0 ≤ k < 2l and multiply it by Y 2l; then the first 2l rows of the product are uY k for 2l ≤ k < 2l+1. [See S. J. Berkowitz, Inf. Processing Letters 18  1984 , 147–150.] Of course such asymptotically “fast” matrix multiplication is strictly of theoretical interest. E. Kaltofen has shown how to evaluate determinants with only  O n2+ϵM n    additions, subtractions, and multiplications [Proc. Int. Symp. Symb.  Alg. Comp. 17  1992 , 342–349]; his method is interesting even with M n  = n3. 71. Suppose g1 = u1 ◦ v1, . . . , gr = ur ◦ vr, and f = α1g1 + ··· + αrgr + p0, where uk = βk1g1 +···+βk k−1 gk−1 +pk, vk = γk1g1 +···+γk k−1 gk−1 +qk, each ◦ is “×” or “ ”, and each pj or qj is a polynomial of degree ≤ 1 in x1, . . . , xn. Compute auxiliary quantities wk, yk, zk for k = r, r − 1, . . . , 1 as follows: wk = αk + β k+1 kyk+1 + γ k+1 kzk+1 + ··· + βrkyr + γrkzr, and  yk = wk × vk, yk = wk vk,  zk = wk × uk, zk = −yk × gk,  if gk = uk × vk; if gk = uk vk.  Then f′ = p′0 + p′1yr + q′1z1 + ··· + p′ryr + q′rzr, where ′ denotes the derivative with [W. Baur and V. Strassen, Theoretical Comp. Sci. 22 respect to any of x1, . . . , xn.  1983 , 317–330. A related method had been published by S. Linnainmaa, BIT 16  1976 , 146–160, who applied it to analysis of rounding errors.] We save two chain multiplications if gr = ur × vr, since wr = αr. Repeating the construction gives all second partial derivatives with at most 9m + 3d chain multiplications and 4d divisions. 72. There is an algorithm to compute the tensor rank over algebraically closed fields like the complex numbers, since this is a special case of the results of Alfred Tarski, A Decision Method for Elementary Algebra and Geometry, 2nd edition  Berkeley, California: Univ. of California Press, 1951 ; but the known methods do not make this computation really feasible except for very small tensors. Over the field of rational numbers, the problem isn’t even known to be solvable in finite time. 73. In such a polynomial chain on N variables, the determinant of any N × N matrix for N of the linear forms known after l addition-subtraction steps is at most 2l. And in the discrete Fourier transform, the matrix of the final N = m1 . . . mn linear forms has determinant N N 2, since its square is N times a permutation matrix by exercise 13. [JACM 20  1973 , 305–306.] 74.  a  If k =  k1, . . . , ks T is a vector of relatively prime integers, so is U k, since any common divisor of the elements of U k divides all elements of k = U−1U k. Therefore V U k cannot have all integer components.  b  Suppose there is a polynomial chain for V x with t multiplications. If t = 0, the entries of V must all be integers, so s = 0. Otherwise let λi = α × λk or λi = λj × λk be the first multiplication step. We can assume that λk = n1x1 + ··· + nsxs + β where n1, . . . , ns are integers, not all zero, and β is constant. Find a unimodular matrix U such that  n1, . . . , ns U =  0, . . . , 0, d , where d = gcd n1, . . . , ns .  The algorithm   4.7  ANSWERS TO EXERCISES  719  discussed before Eq. 4.5.2– 14  implicitly defines such a U.  Construct a new polyno- mial chain with inputs y1, . . . , ys−1 as follows: First calculate x =  x1, . . . , xs T = U y1, . . . , ys−1,−β d T , then continue with the assumed polynomial chain for V x. When step i of that chain is reached, we will have λk =  n1, . . . , ns x + β = 0, so we can simply set λi = 0 instead of multiplying. After V x has been evaluated, add the constant vector wβ d to the result, where w is the rightmost column of V U, and let W be the other s − 1 columns of V U. The new polynomial chain has computed V x + wβ d = V U y1, . . . , ys−1,−β d T + wβ d = W  y1, . . . , ys−1 T , with t − 1 multi- plications. But the columns of W are Z-independent, by part  a ; hence t − 1 ≥ s − 1, by induction on s, and we have t ≥ s.  c  Let xj = 0 for the t − s values of j that aren’t in the set of Z-independent columns. Any chain for V x then evaluates V ′x′ for a matrix V ′ to which part  b  applies.  d  λ1 = x − y, λ2 = λ1 + λ1, λ3 = λ2 + x, λ4 =  1 6  × λ3, λ5 = λ4 + λ4, λ6 = λ5 + y  = x+ y 3 , λ7 = λ6− λ1, λ8 = λ7 + λ4  = x 2+ y . But {x 2+ y, x+ y 2} needs two multiplications, since the columns of   1 2 1 2  are Z-independent. [Journal 1 1 of Information Processing 1  1978 , 125–129.]  SECTION 4.7 1. Find the first nonzero coefficient Vm, as in  4 , and divide both U z  and V  z  by zm  shifting the coefficients m places to the left . The quotient will be a power series if and only if U0 = ··· = Um−1 = 0. 0 Wn = V n0 Un −  V 1 Vn−1  − ··· − j ≥ 1, then set Wn ← Un −n−1 2. We have V n+1  V n0 Wn−1  V 0 Vj  for k=0 WkVn−k for n ≥ 0, finally replace Wj by Wj V j+1 for j ≥ 0. Similar techniques are possible in connection with other algorithms in this section. 3. Yes. When α = 0, it is easy to prove by induction that W1 = W2 = ··· = 0. When α = 1, we find Wn = Vn, by the cute identity  0 W1  V n−2 0 0 V1 . Thus, we can start by replacing  Uj, Vj  by  V j  Vn  −  V 2  0 W0  V n−1  0 Uj, V j−1  0  0  0  n  k=1     k −  n − k  n  n  Wn =  k n  VkWn−k,  k=1  VkVn−k = VnV0.  for n ≥ 1.  4. If W  z  = eV  z , then W ′ z  = V ′ z W  z ; we find W0 = eV0, and  W0 = 0 and Wn = Vn +n−1  If W  z  = ln V  z , the roles of V and W are reversed; hence when V0 = 1 the rule is  k=1 k n − 1 VkWn−k for n ≥ 1.  [By exercise 6, the logarithm can be obtained to order n in O n log n  operations. R. P. Brent observes that exp V  z   can also be calculated with this asymptotic speed by applying Newton’s method to f x  = ln x − V  z ; therefore general exponentiation  1+V  z  α = exp α ln 1+V  z    is O n log n  too. Reference: Analytic Computational Complexity, edited by J. F. Traub  New York: Academic Press, 1975 , 172–176.] 5. We get the original series back. This can be used to test a reversion algorithm. 6. ϕ x  = x + x 1 − xV  z  ; see Algorithm 4.3.3R. Thus after W0, . . . , WN−1 are known, the idea is to input VN, . . . , V2N−1, compute  W0 + ··· + WN−1zN−1 ×  V0 + ··· + V2N−1z2N−1  = 1 + R0zN + ··· + RN−1z2N−1 + O z2N , and let WN + ··· + W2N−1zN−1 = − W0 + ··· + WN−1zN−1  R0 + ··· + RN−1zN−1  + O zN .   720  ANSWERS TO EXERCISES  4.7  k  7. Wn =mk G3. Set Uk ←  Uk −k then set Un−1 ← −n G4. Output Wn =n  [Numer. Math. 22  1974 , 341–348; this algorithm was, in essence, first published by M. Sieveking, Computing 10  1972 , 153–156.] Note that the total time for N coef- ficients is O N log N  arithmetic operations if we use “fast” polynomial multiplication  exercise 4.6.4–57 .   n when n =  m − 1 k + 1, otherwise 0.  See exercise 2.3.4.4–11.   8. G1. Input G1 and V1; set n ← 1, U0 ← 1 V1; output W1 = G1U0.  and Gn.  G2. Increase n by 1. Terminate the algorithm if n > N; otherwise input Vn j=1 Uk−jVj+1  V1 for k = 0, 1, . . . , n − 2  in this order ; k=1 kUn−kGk n and return to G2.  k=2 kUn−kVk V1.   The running time of the order N 3 algorithm is hereby increased by only order N 2.  Note: Algorithms T and N determine V [−1] U z  ; the algorithm in this exercise determines G V [−1] z  , which is somewhat different. Of course, the results can all be obtained by a sequence of operations of reversion and composition  exercise 11 , but it is helpful to have more direct algorithms for each case. 9.  n = 1 n = 2 n = 3 n = 4 n = 5  1  1 1  T1n T2n T3n T4n T5n  2 2 1  5 5 3 1  14 14 9 4 1  10. Form y1 α = x 1 + a1x + a2x2 + ··· 1 α = x 1 + c1x + c2x2 + ···   by means of Eq.  9 ; then revert the latter series.  See the remarks following Eq. 1.2.11.3– 11 .  11. Set W0 ← U0, and set  Tk, Wk  ←  Vk, 0  for 1 ≤ k ≤ N. Then for n = 1, 2, . . . , N, do the following: Set Wj ← Wj + UnTj for n ≤ j ≤ N; and then set Tj ← Tj−1V1 + ··· + TnVj−n for j = N, N − 1, . . . , n + 1. Here T  z  represents V  z N. An online power series algorithm for this problem, analogous to Algorithm T, could be constructed, but it would require about N 2 2 storage locations. There is also an online algorithm that solves this exercise and needs only O N  storage locations: We may assume that V1 = 1, if Uk is replaced by UkV k1 and Vk is replaced by Vk V1 for all k. Then we may revert V  z  by Algorithm L, and use its output as input to the algorithm of exercise 8 with G1 = U1, G2 = U2, etc., thus computing U V [−1][−1] z   − U0. See also exercise 20.  Brent and Kung have constructed several algorithms that are asymptotically faster. For example, we can evaluate U x  for x = V  z  by a slight variant of exercise 4.6.4– N chain multiplications of cost M N  and about N parameter 42 c , doing about 2 multiplications of cost N, where M N  is the number of operations needed to multiply √ N M N  + N 2  = O N 2 . power series to order N; the total time is therefore O  A still faster method can be based on the identity U V0 z  + zmV1 z   = U V0 z   + zmU′ V0 z  V1 z +z2mU′′ V0 z  V1 z 2 2!+··· , extending to about N m terms, where  we choose m ≈ N log N; the first term U V0 z   is evaluated in O mN log N 2   operations using a method somewhat like that in exercise 4.6.4–43. Since we can go from U  k  V0 z   to U  k+1  V0 z   in O N log N  operations by differentiating and dividing by V ′0 z , the entire procedure takes O mN log N 2+ N m  N log N  = O N log N 3 2 operations. [JACM 25  1978 , 581–595.]  √   4.7  ANSWERS TO EXERCISES  721  When the polynomials have m-bit integer coefficients, this algorithm involves roughly N 3 2+ϵ multiplications of  N lg m -bit numbers, so the total running time will be more than N 5 2. An alternative approach with asymptotic running time O N 2+ϵ  has been developed by P. Ritzmann [Theoretical Comp. Sci. 44  1986 , 1–16]. Composition can be done much faster modulo a small prime p  see exercise 26 . 12. Polynomial division is trivial unless m ≥ n ≥ 1. Assuming the latter, the equation u x  = q x v x  + r x  is equivalent to U z  = Q z V  z  + zm−n+1R z  where U x  = xmu x−1 , V  x  = xnv x−1 , Q x  = xm−nq x−1 , and R x  = xn−1r x−1  are the “reverse” polynomials of u, v, q, and r. To find q x  and r x , compute the first m − n + 1 coefficients of the power series U z  V  z  = W  z  + O zm−n+1 ; then compute the power series U z  − V  z W  z , which has the form zm−n+1T  z  where T  z  = T0 + T1z +··· . Note that Tj = 0 for all j ≥ n; hence Q z  = W  z  and R z  = T  z  satisfy the requirements. 13. Apply exercise 4.6.1–3 with u z  = zN and v z  = W0 + ··· + WN−1zN−1; the desired approximations are the values of v3 z  v2 z  obtained during the course of the algorithm. Exercise 4.6.1–26 tells us that there are no further possibilities with relatively prime numerator and denominator. If each Wi is an integer, an all-integer extension of Algorithm 4.6.1C will have the desired properties.  Notes: See the book History of Continued Fractions and Padé Approximants by Claude Brezinski  Berlin: Springer, 1991  for further information. The case N = 2n+1 and deg w1  = deg w2  = n is of particular interest, since it is equivalent to a so-called Toeplitz system; asymptotically fast methods for Toeplitz systems are surveyed in Bini and Pan, Polynomial and Matrix Computations 1  Boston: Birkhäuser, 1994 , §2.5. The method of this exercise can be generalized to arbitrary rational interpolation of the form W  z  ≡ p z  q z   modulo  z − z1  . . .  z − zN  , where the zi’s need not be distinct; thus, we can specify the value of W  z  and some of its derivatives at several points. See Richard P. Brent, Fred G. Gustavson, and David Y. Y. Yun, J. Algorithms 1  1980 , 259–295. V  U z   − U′ z V  z  is  14. If U z  = z+Ukzk+··· and V  z  = zk+Vk+1zk+1+··· , we find that the difference j≥1 z2k+j−1j UkVk+j − Uk+j +  polynomial involving only Uk, . . . , Uk+j−1, Vk+1, . . . , Vk+j−1  ; hence V  z  is unique if U z  is given and U z  is unique if V  z  and Uk are given. The solution depends on two auxiliary algorithms, the first of which solves the equation V  z+zkU z   =  1+zk−1W  z  V  z +zk−1S z +O zk−1+n  for V  z  = V0+ V1z + ··· + Vn−1zn−1, given U z , W  z , S z , and n. If n = 1, let V0 = −S 0  W  0 ; or let V0 be arbitrary when S 0  = W  0  = 0. To go from n to 2n, let  V  z + zkU z   =  1 + zk−1 W  z  V  z  + zk−1 1 + zk−1 ˆW  z  =  z  z + zkU z   n 1 + zk−1  S z  − zk−1+nR z  + O zk−1+2n , W  z   + O zk−1+n ,  ˆS z  =  z  z + zkU z   nR z  + O zn , and let ˆV  z  = Vn + Vn+1z + ··· + V2n−1zn−1 satisfy  ˆV  z + zkU z   =  1 + zk−1 ˆW  z   ˆV  z  + zk−1 ˆS z  + O zk−1+n .  The second algorithm solves W  z U z  + zU′ z  = V  z  + O zn  for U z  = U0+U1z+···+Un−1zn−1, given V  z , W  z , and n. If n = 1, let U0 = V  0  W  0 , or let U0 be arbitrary in case V  0  = W  0  = 0. To go from n to 2n, let W  z U z +zU′ z  =   ANSWERS TO EXERCISES  722 4.7 V  z  − znR z  + O z2n , and let ˆU z  = Un + ··· + U2n−1zn−1 be a solution to the equation  n + W  z   ˆU z  + z ˆU′ z  = R z  + O zn . Resuming the notation of  27 , the first algorithm can be used to solve ˆV  U z   = U′ z  z U z  k ˆV  z  to any desired accuracy, and we set V  z  = zk ˆV  z . To find P  z , suppose we have V  P  z   = P ′ z V  z  + O z2k−1+n , an equation that holds for n = 1 when P  z  = z + αzk and α is arbitrary. We can go from n to 2n by letting V  P  z   = P ′ z V  z  + z2k−1+nR z  + O z2k−1+2n  and replacing P  z  by P  z  + zk+n ˆP  z , where the second algorithm is used to find the polynomial ˆP  z  such that  k + n − zV ′ P  z   V  z   ˆP  z  + z ˆP ′ z  =  zk V  z  R z  + O zn . 15. The differential equation U′ z  U z k = 1 zk implies that U z 1−k = z1−k + c for some constant c. So we find U [n] z  = z  1 + cnz1−k 1  k−1 .  A similar argument solves  27  for arbitrary V  z : If W ′ z  = 1 V  z , we have  W U [n] z   = W  z  + nc for some c. 16. We want to show that [tn] tn+1  n+1 R′k+1 t  V  t n−nR′k t  V  t n+1  = 0. This follows since  n + 1 R′k+1 t  V  t n − nR′k t  V  t n+1 = d dt Rk t  V  t n+1 . Conse- quently we have n−1[tn−1] R′1 t  tn V  t n =  n − 1 −1[tn−2] R′2 t  tn−1 V  t n−1 = ··· = 17. Equating coefficients of xlym, the convolution formula states thatl+m 1−1[t0] R′n t  t V  t  = [t] Rn t  V1 = Wn. n   vkl v n−k m, which is the same as [zn] V  z l+m =  k [zk]V  z l  [zn−k] V  z m , Notes: The name “poweroid” was introduced by J. F. Steffensen, who was the first of many authors to study the striking properties of these polynomials in general [Acta Mathematica 73  1941 , 333–366]. For a review of the literature, and for further discussion of the topics in the next several exercises, see D. E. Knuth, The Mathematica Journal 2  1992 , 67–78. One of the results proved in that paper is the asymptotic es n 1 − V2y + O y2  + O x−1  , if V1 = 1 and sV ′ s  = y and formula Vn x  = exV  s   n y = n x is bounded as x → ∞ and n → ∞.  vn l+m  =  which is a special case of  2 .  m  k  k  k xkn! [zn] V  z k k! = n! [zn] exV  z . Consequently Vn x  x =  n − 1 ! [zn−1] V ′ z  exV  z  when n > 0. We get the stated identity by equating the coefficients of zn−1 in V ′ z  e x+y V  z  = V ′ z  exV  z eyV  z . 19. We have  18. We have Vn x  =   v1 1! z + v2 m! [zn]  2! z  3 + ···m  v1 k1 v2  k2  2 + v3 3! z n!  vnm = n!  =   vn  kn  k1! k2! . . . kn!  1!  2!  . . .  n!  k1+k2+···+kn=m k1+2k2+···+nkn=n k1,k2,...,kn≥0  by the multinomial theorem 1.2.6– 42 . These coefficients, called partial Bell polyno- mials [see Annals of Math.  2  35  1934 , 258–277], arise also in Arbogast’s formula, exercise 1.2.5–21, and we can associate the terms with set partitions as explained in the answer to that exercise. The recurrence  shows how to calculate column k from columns 1 and k−1; it is readily interpreted with  respect to partitions of {1, . . . , n}, since there aren−1   ways to include the element n  j  vjv n−j  k−1   j−1  vnk =     n − 1  j − 1   4.7  ANSWERS TO EXERCISES  723  in a subset of size j. The first few rows of the matrix are  v2 1 3v1v2  v1 v2 v3 v4 v5  v3 1 6v2 1v2 2 + 10v2  v4 1 10v3 1v2  4v1v3 + 3v2 2 5v1v4 + 10v2v3  v5 1  1v3  15v1v2  k vnkukm correspond to the identity function z, by exercise 20.  j [zj] U z k  [zn] V  z j ; hence wnk =  n! k!  20. [zn] W  z k =  ular, if U z  = V [−1] z  = −W  −z  we have unk =  −1 n−kwnk. So    j  k! j! ujk ×   j! n! vnj . [E. Jabotinsky, Comptes Rendus Acad. Sci. 224  Paris, 1947 , 323–324.] k! [zn]  αW  β z  k = αkβnwnk; in partic- 21.  a  If U z  = αW  βz  we have unk = n! k unkvkm and  b  [Solution by Ira Gessel.] This identity is, in fact, equivalent to Lagrange’s inversion formula: We have wnk =  −1 n−kunk =  −1 n−k n! k! [zn] V [−1] z k, and the coefficient of zn in V [−1] z k is n−1 [tn−1] ktn+k−1 V  t n by exercise 16. On the other hand we have defined v −k  −n  to be  −k n−k [zn−k]  V  z  z  −n, which equals  −1 n−k n − 1  . . .  k + 1 k [zn−1] zn+k−1 V  z n. 22.  a  If V  z  = U{α} z  and W  z  = V {β} z , we have W  z  = V  zW  z β  = U zW  z β V  zW  z β α  = U zW  z α+β .  Notice the contrast between this law and the similar formulas U [1] z  = U z , U [α][β] z  = U [αβ] z  that apply to iteration.   b  B{2} z  is the generating function for binary trees, 2.3.4.4– 12 , which is W  z  z in the example z = t − t2 following Algorithm L. Moreover, B{t} z  is the generating function for t-ary trees, exercise 2.3.4.4–11.   c  The hint is equivalent to zU{α} z α = W [−1] z , which is equivalent to the formula zU{α} z α U zU{α} z α α = z. Now Lagrange’s inversion theorem  exercise 8  says that [zn] W [−1] z x = x n[z−x] W  z −n when x is a positive integer.  Here W  z −n is a Laurent series — a power series divided by a power of z; we can use the notation [zm] V  z  for Laurent series as well as for power series.  Therefore [zn] U{α} z x = [zn]  W [−1] z  z x α = [zn+x α] W [−1] z x α is equal to n+x α [z−x α] W  z −n−x α = x+nα [z−x α] z−n−x αU z x+nα when x α is a positive integer. We have verified the result for infinitely many α; that is sufficient, since the coefficients of U{α} z x are polynomials in α. memorable consequence of the hint is the case α = −1:  We’ve seen special cases of this result in exercises 1.2.6–25 and 2.3.4.4–29. One  x α  x  W  z  = zU z   if and only if  [−1] z  = z U{−1} z  .  W  3 T 3 −··· will have the property that exp α ln U  = I +α   d  If U0 = 1 and Vn x  is the poweroid for V  z  = ln U z , we’ve just proved that xVn x + nα   x + nα  is the poweroid for ln U{α} z . So we can plug this poweroid into the former identities, changing y to y − αn in the second formula. 23.  a  We have U = I + T where T n is zero in rows ≤ n. Hence ln U = T − 1 2 T 2 + 1 entry of U α is a polynomial in α, and the relations of exercise 19 hold whenever α is a positive integer; therefore U α is a power matrix for all α, and its first column defines U [α] z .  In particular, U−1 is a power matrix; this is another way to revert U z .   T 2 +··· = U α. Each  T +α  2  1   b  Since U ϵ = I + ϵ ln U + O ϵ2 , we have  lnk = [ϵ]u  [ϵ]  nk = n!  k! [zn][ϵ]  z + ϵL z  + O ϵ  2  k = n!  k! [zn] kzk−1  L z .   724  ANSWERS TO EXERCISES  4.7   c  ∂  ∂α U [α] z  = [ϵ] U [α+ϵ] z , and we have [ϵ] z   = U  [α+ϵ] z  = U  [α] U  U  [α] z + ϵL z  + O ϵ  2  .  k  2  3 +  2 u2  2 u3  2 u2u4 − 5u2  2, l5 = u5 − 15  k−1 2u3 − 20u4 2.  2, l4 = u4 − 5u2u3 + 9  Also U [α+ϵ] z  = U [ϵ] U [α] z   = U [α] z  + ϵL U [α] z   + O ϵ2 . on the right is un n−1  =n u2. Similarly, if u2 = ··· = uk−1 = 0 and uk ̸= 0, we have  d  The identity follows from the fact that U commutes with ln U. It determines ln−1 when n ≥ 4, because the coefficient of ln−1 on the left is nu2, while the coefficient form ln + n ln+1−kuk + ··· . lk = uk and the recurrence for n ≥ 2k determines lk+1, lk+2, . . . : The left side has the In general, l2 = u2, l3 = u3 − 3  e  We have U = from the mth term is  lnmnm−1 . . . ln2n1 ln1n0 summed over n = nm > ··· > n1 > 6 u2 185  ln+1−kuk + ··· and the right side has the form ln +n  m ln U m m!, and for fixed m the contribution to un = un1 n0 = 1. Now apply the result of part  b . [See Trans. Amer. Math. Soc. 108  1963 , 457–477.] 24.  a  By  21  and exercise 20, we have U = VDV −1 where V is the power matrix of the Schröder function and D is the diagonal matrix diag u, u2, u3, . . .  . So we may take ln U = V diag ln u, 2 ln u, 3 ln u, . . .  V −1.  b  The equation W VDV −1 = VDV −1W implies  V −1W V  D = D V −1W V  . The diagonal entries of D are distinct, so V −1W V must be a diagonal matrix D′. Thus W = VD′V −1, and W has the same Schröder function as U. It follows that W1 ̸= 0 and W = VDαV −1, where α =  ln W1   ln U1 . 25. We must have k = l because [zk+l−1] U V  z   = Uk+l−1 + Vk+l−1 + kUkVl. To uk; and unj = 0 for have unk − vnk =n complete the proof it suffices to show that Uk = Vk and U V  z   = V  U z   implies U z  = V  z . Suppose l is minimal with Ul ̸= Vl, and let n = k + l − 1. Then we l < j < n. Now the sum   =k+l−1  j vnjuj; so we findn  junjvj = un + unkvk + ··· + unlvl + vn must be equal to   ul − vl ; unj = vnj for all j > k; unl =n  ul − vl vk =n  vk ul − vl . But we havek+l−1  k  l  k  k  l  l  if and only if k = l.  [From this exercise and the previous one, we might suspect that U V  z   = V  U z   only when one of U and V is an iterate of the other. But this is not necessarily true when U1 and V1 are roots of unity. For example, if V1 = −1 and U z  = V [2] z , V is not an iterate of U [1 2], nor is U [1 2] an iterate of V .] 26. Writing U z  = U[0] z2  + zU[1] z2 , we have U V  z   ≡ U[0] V1z2 + V2z4 + ···   + V  z U[1] V1z2 + V2z4 +···    modulo 2 . The running time satisfies T  N  = 2T  N 2 + C N , where C N  is essentially the time for polynomial multiplication mod zN. We can make C N  = O N 1+ϵ  by the method of, say, exercise 4.6.4–59; see also the answer to exercise 4.6–5.  A similar method works mod p in time O pN 1+ϵ . [D. J. Bernstein, J. Symbolic  Wn =n on n; and this is the identity we need to show that δeV  z  =   Computation 26  1998 , 339–341.] 27. From  W  qz  − W  z  V  z  = W  z  V  qmz  − V  z   we obtain the recurrence k=1 VkWn−k qkm − qn−k   qn − 1 . [J. Difference Eqs. and Applics. 1  1995 , 57–60.] 28. Note first that δ U z V  z   =  δU z  V  z  + U z  δV  z  , because t mn  = t m  + t n . Therefore δ V  z n  = nV  z n−1δV  z  for all n ≥ 0, by induction n≥0 δ V  z n n!  = eV  z δV  z . Replacing V  z  by ln V  z  in this equation gives V  z  δ ln V  z  = δV  z ;   4.7  ANSWERS TO EXERCISES  725  hence δ V  z α  = δeα ln V  z  = eα ln V  z  δ α ln V  z   = αV  z α−1 for all complex numbers α.  It follows that the desired recurrences are   a  W1 = 1, Wn =  b  W1 = 1, Wn =  c  W1 = 0, Wn = Vn +  d\n, d>1  α + 1 t d  t n  − 1 VdWn d ; d\n, d>1 t d  t n  VdWn d ; d\n, d>1 t d  t n  − 1 VdWn d .  [See H. W. Gould, AMM 81  1974 , 3–14. These formulas hold when t is any function such that t m  + t n  = t mn  and t n  = 0 if and only if n = 1, but the suggested t is simplest. The method discussed here works also for power series in arbitrarily many variables; then t is the total degree of a term.]  “It is certainly an idea you have there,” said Poirot, with some interest. “Yes, yes, I play the part of the computer. One feeds in the information — ” “And supposing you come up with all the wrong answers?” said Mrs. Oliver. “That would be impossible,” said Hercule Poirot. “Computers do not do that sort of a thing.” “They’re not supposed to,” said Mrs. Oliver, “but you’d be surprised at the things that happen sometimes.” — AGATHA CHRISTIE, Hallowe’en Party  1969    APPENDIX A  TABLES OF NUMERICAL QUANTITIES  QUANTITIES THAT ARE FREQUENTLY USED IN STANDARD SUBROUTINES  AND IN ANALYSIS OF COMPUTER PROGRAMS  40 DECIMAL PLACES   Table 1  √ 2 = 1.41421 35623 73095 04880 16887 24209 69807 85697− √ 3 = 1.73205 08075 68877 29352 74463 41505 87236 69428+ √ 5 = 2.23606 79774 99789 69640 91736 68731 27623 54406+ √ 10 = 3.16227 76601 68379 33199 88935 44432 71853 37196− 3√ 2 = 1.25992 10498 94873 16476 72106 07278 22835 05703− 3√ 3 = 1.44224 95703 07408 38232 16383 10780 10958 83919− 4√ 2 = 1.18920 71150 02721 06671 74999 70560 47591 52930− ln 2 = 0.69314 71805 59945 30941 72321 21458 17656 80755+ ln 3 = 1.09861 22886 68109 69139 52452 36922 52570 46475− ln 10 = 2.30258 50929 94045 68401 79914 54684 36420 76011+ 1 ln 2 = 1.44269 50408 88963 40735 99246 81001 89213 74266+ 1 ln 10 = 0.43429 44819 03251 82765 11289 18916 60508 22944− π = 3.14159 26535 89793 23846 26433 83279 50288 41972− 1◦ = π 180 = 0.01745 32925 19943 29576 92369 07684 88612 71344+ 1 π = 0.31830 98861 83790 67153 77675 26745 02872 40689+ π2 = 9.86960 44010 89358 61883 44909 99876 15113 53137− √ π = Γ  1 2  = 1.77245 38509 05516 02729 81674 83341 14518 27975+ Γ  1 3  = 2.67893 85347 07747 63365 56929 40974 67764 41287− Γ  2 3  = 1.35411 79394 26400 41694 52880 28154 51378 55193+ e = 2.71828 18284 59045 23536 02874 71352 66249 77572+ 1 e = 0.36787 94411 71442 32159 55237 70161 46086 74458+ e2 = 7.38905 60989 30650 22723 04274 60575 00781 31803+ γ = 0.57721 56649 01532 86060 65120 90082 40243 10422− ln π = 1.14472 98858 49400 17414 34273 51353 05871 16473− ϕ = 1.61803 39887 49894 84820 45868 34365 63811 77203+ eγ = 1.78107 24179 90197 98523 65041 03107 17954 91696+ eπ 4 = 2.19328 00507 38015 45655 97696 59278 73822 34616+ sin 1 = 0.84147 09848 07896 50665 25023 21630 29899 96226− cos 1 = 0.54030 23058 68139 71740 09366 07442 97660 37323+ −ζ′ 2  = 0.93754 82543 15843 75370 25740 94567 86497 78979− ζ 3  = 1.20205 69031 59594 28539 97381 61511 44999 07650− ln ϕ = 0.48121 18250 59603 44749 77589 13424 36842 31352− 1 ln ϕ = 2.07808 69212 35027 53760 13226 06117 79576 77422− −ln ln 2 = 0.36651 29205 81664 32701 24391 58232 66946 94543−  726   TABLES OF NUMERICAL QUANTITIES  727  Table 2  QUANTITIES THAT ARE FREQUENTLY USED IN STANDARD SUBROUTINES  AND IN ANALYSIS OF COMPUTER PROGRAMS  45 OCTAL PLACES   The names at the left of the “=” signs are given in decimal notation.  0.1 = 0.06314 63146 31463 14631 46314 63146 31463 14631 46315− 0.01 = 0.00507 53412 17270 24365 60507 53412 17270 24365 60510− 0.001 = 0.00040 61115 64570 65176 76355 44264 16254 02030 44672+ 0.0001 = 0.00003 21556 13530 70414 54512 75170 33021 15002 35223− 0.00001 = 0.00000 24761 32610 70664 36041 06077 17401 56063 34417− 0.000001 = 0.00000 02061 57364 05536 66151 55323 07746 44470 26033+ 0.0000001 = 0.00000 00153 27745 15274 53644 12741 72312 20354 02151+ 0.00000001 = 0.00000 00012 57143 56106 04303 47374 77341 01512 63327+ 0.000000001 = 0.00000 00001 04560 27640 46655 12262 71426 40124 21742+ 0.0000000001 = 0.00000 00000 06676 33766 35367 55653 37265 34642 01627− √ 2 = 1.32404 74631 77167 46220 42627 66115 46725 12575 17435+ √ 3 = 1.56663 65641 30231 25163 54453 50265 60361 34073 42223− √ 5 = 2.17067 36334 57722 47602 57471 63003 00563 55620 32021− √ 10 = 3.12305 40726 64555 22444 02242 57101 41466 33775 22532+ 3√ 2 = 1.20505 05746 15345 05342 10756 65334 25574 22415 03024+ 3√ 3 = 1.34233 50444 22175 73134 67363 76133 05334 31147 60121− 4√ 2 = 1.14067 74050 61556 12455 72152 64430 60271 02755 73136+ ln 2 = 0.54271 02775 75071 73632 57117 07316 30007 71366 53640+ ln 3 = 1.06237 24752 55006 05227 32440 63065 25012 35574 55337+ ln 10 = 2.23273 06735 52524 25405 56512 66542 56026 46050 50705+ 1 ln 2 = 1.34252 16624 53405 77027 35750 37766 40644 35175 04353+ 1 ln 10 = 0.33626 75425 11562 41614 52325 33525 27655 14756 06220− π = 3.11037 55242 10264 30215 14230 63050 56006 70163 21122+ 1◦ = π 180 = 0.01073 72152 11224 72344 25603 54276 63351 22056 11544+ 1 π = 0.24276 30155 62344 20251 23760 47257 50765 15156 70067− π2 = 11.67517 14467 62135 71322 25561 15466 30021 40654 34103− √ π = Γ  1 2  = 1.61337 61106 64736 65247 47035 40510 15273 34470 17762− Γ  1 3  = 2.53347 35234 51013 61316 73106 47644 54653 00106 66046− Γ  2 3  = 1.26523 57112 14154 74312 54572 37655 60126 23231 02452+ e = 2.55760 52130 50535 51246 52773 42542 00471 72363 61661+ 1 e = 0.27426 53066 13167 46761 52726 75436 02440 52371 03355+ e2 = 7.30714 45615 23355 33460 63507 35040 32664 25356 50217+ γ = 0.44742 14770 67666 06172 23215 74376 01002 51313 25521− ln π = 1.11206 40443 47503 36413 65374 52661 52410 37511 46057+ ϕ = 1.47433 57156 27751 23701 27634 71401 40271 66710 15010+ eγ = 1.61772 13452 61152 65761 22477 36553 53327 17554 21260+ eπ 4 = 2.14275 31512 16162 52370 35530 11342 53525 44307 02171− sin 1 = 0.65665 24436 04414 73402 03067 23644 11612 07474 14505− cos 1 = 0.42450 50037 32406 42711 07022 14666 27320 70675 12321+ −ζ′ 2  = 0.74001 45144 53253 42362 42107 23350 50074 46100 27706+ ζ 3  = 1.14735 00023 60014 20470 15613 42561 31715 10177 06614+ ln ϕ = 0.36630 26256 61213 01145 13700 41004 52264 30700 40646+ 1 ln ϕ = 2.04776 60111 17144 41512 11436 16575 00355 43630 40651+ −ln ln 2 = 0.27351 71233 67265 63650 17401 56637 26334 31455 57005−   728  APPENDIX A  Several of the 40-digit values in Table 1 were computed on a desk calculator by John W. Wrench, Jr., for the first edition of this book. When computer software for such calculations became available during the 1970s, all of his contributions proved to be correct. The 40-digit values of other fundamental constants can be found in Eqs. 4.5.2– 60 , 4.5.3– 26 , 4.5.3– 41 , 4.5.4– 9 , and the answers to exercises 4.5.4–8, 4.5.4–25, 4.6.4–58.  VALUES OF HARMONIC NUMBERS, BERNOULLI NUMBERS,  AND FIBONACCI NUMBERS, FOR SMALL VALUES OF n  Table 3  n 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  Hn 0 1 3 2 11 6 25 12 137 60 49 20 363 140 761 280 7129 2520 7381 2520 83711 27720 86021 27720 1145993 360360 1171733 360360 1195757 360360 2436559 720720 42142223 12252240 14274301 4084080 275295799 77597520 55835135 15519504 18858053 5173168 19093197 5173168 444316699 118982864 1347822955 356948592 34052522467 8923714800 34395742267 8923714800 312536252003 80313433200 315404588903 80313433200 9227046511387 2329089562800 9304682830147 2329089562800  Bn 1 −1 2 1 6 0 −1 30  0 1 42 0 −1 30  0 5 66 0  0 7 6 0  −691 2730  −3617 510  43867 798 −174611 330  0  0  0  0  854513 138 −236364091 2730  0  0 8553103 6 0  −23749461029 870  8615841276005 14322  Fn 0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 10946 17711 28657 46368 75025 121393 196418 317811 514229 832040  n 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30   For any x, let Hx =   1  n  n≥1    TABLES OF NUMERICAL QUANTITIES − 1 n + x  . Then  729  2 ln 3, 2 ln 3,  H1 2 = 2 − 2 ln 2, √ 3 − 3 H1 3 = 3 − 1 2 π  √ 3 − 3 H2 3 = 3 2 + 1 2 π  2 π − 3 ln 2, H1 4 = 4 − 1 2 π − 3 ln 2, 3 + 1 H3 4 = 4 H1 5 = 5 − 1 2 πϕ3 25−1 4 − 5 2 πϕ−3 25−1 4 − 5 2 − 1 H2 5 = 5 2 πϕ−3 25−1 4 − 5 H3 5 = 5 3 + 1 2 πϕ3 25−1 4 − 5 4 + 1 H4 5 = 5 √ H1 6 = 6 − 1 3 − 2 ln 2 − 3 2 π √ 3 − 2 ln 2 − 3 5 + 1 H5 6 = 6 2 π  √ 4 ln 5 − 1 5 ln ϕ, √ 2 4 ln 5 + 1 √ 2 4 ln 5 + 1 √ 2 4 ln 5 − 1 2 2 ln 3, 2 ln 3,  5 ln ϕ, 5 ln ϕ, 5 ln ϕ,  and, in general, when 0 < p < q  see exercise 1.2.9–19 , cos 2pn  π − ln 2q + 2   Hp q = q p  − π 2 cot p  q  q  1≤n<q 2  π · ln sin n q  π.   APPENDIX B  INDEX TO NOTATIONS  In the following formulas, letters that are not further qualified have the following significance:  j, k m, n x, y z f S, T  integer-valued arithmetic expression nonnegative integer-valued arithmetic expression real-valued arithmetic expression complex-valued arithmetic expression real-valued or complex-valued function set or multiset  Formal symbolism  Meaning  An or A[n] Amn or A[m, n]  end of algorithm, program, or proof the nth element of linear array A the element in row m and column n of rectangular array A  V ← E give variable V the value of expression E U ↔ V  R? a: b   interchange the values of variables U and V conditional expression: denotes  a if relation R is true, b if R is false  [R]  characteristic function of relation R:   R? 1: 0      R k   R k  min R k   max R k   δkj Kronecker delta: [j = k]  [zn] g z  f k   coefficient of zn in power series g z  sum of all f k  such that the variable k is an integer and relation R k  is true  f k  product of all f k  such that the variable k  is an integer and relation R k  is true  f k  minimum value of all f k  such that the var- iable k is an integer and relation R k  is true f k  maximum value of all f k  such that the var- iable k is an integer and relation R k  is true  730  Where defined 1.1 1.1  1.1 1.1 1.1  1.2.3 1.2.3 1.2.9  1.2.3  1.2.3  1.2.3  1.2.3   INDEX TO NOTATIONS  731  Formal symbolism ℜz ℑz z AT  Meaning  real part of z imaginary part of z complex conjugate: ℜz − iℑz transpose of rectangular array A:  AT [j, k] = A[k, j]  Where defined 1.2.2 1.2.2 1.2.2  1.2.2  1.2.2  1.2.5  1.2.5 1.2.9 1.2.10  1.2.11.2 4.7  4.7 1.2.7  1.2.7  1.2.8 1.2.11.2 3.3.4  1.2.4  4.2.1  xy xk  x to the y power  when x is positive  x to the kth power:    x: 1 x−k  0≤j<k   k ≥ 0?   k ≥ 0?   k ≥ 0?   0≤j<k  0≤j<k  xk x to the k rising: Γ x + k  Γ x  =   x + j : 1  x + k −k  1.2.5  xk  x to the k falling: x!  x − k ! =   x − j : 1  x − k −k     n! n factorial: Γ n + 1  = nn  second derivative of f at x  f′ x  derivative of f at x f′′ x   f  n  x  nth derivative: n = 0? f x : g′ x , f [n] x  nth iterate: n = 0? x: f f [n−1] x   f{n} x  = fxf{n} x n harmonic number of order x:   f{n} x  nth induced function:  H x   n  where g x  = f  n−1  x   1 kx  1≤k≤n  harmonic number: H  Hn Fn Fibonacci number:   1  n   n ≤ 1? n: Fn−1 + Fn−2  Bn Bernoulli number: n! [zn] z  ez − 1   X · Y j\k S \ T  dot product of vectors X =  x1, . . . , xn  and Y =  y1, . . . , yn : x1y1 + ··· + xnyn j divides k: k mod j = 0 and j > 0 set difference: {a  a in S and a not in T}  ⊕ ⊖ ⊗⊘ rounded or special operations   732  APPENDIX B  Formal symbolism  . . . a1a0.a−1 . . .  b   x1, x2, . . . , xn      n  n1, n2, . . . , nm  k akbk  Meaning  continued fraction:  binomial coefficient:  k < 0? 0: xk k!   radix-b positional notation:  1x1 + 1  x2 + 1  ··· + 1  xn  . . .      multinomial coefficient  defined only when    Stirling number of the first kind:  n = n1 + n2 + ··· + nm   0<k1<k2<···<kn−m<n  k1k2 . . . kn−m  Stirling number of the second kind:      x  k   n  n  m  m  1≤k1≤k2≤···≤kn−m≤m  k1k2 . . . kn−m  1.2.6  {a  R a } {a1, . . . , an} {x}  set of all a such that the relation R a  is true the set or multiset {ak  1 ≤ k ≤ n} fractional part  used in contexts where a real value, not a set, is implied : x − ⌊x⌋ closed interval: {x  a ≤ x ≤ b} open interval: {x  a < x < b}  [a . . b]  a . . b  [a . . b  half-open interval: {x  a ≤ x < b}  a . . b] half-closed interval: {x  a < x ≤ b}  √ z¯z  cardinality: the number of elements in set S absolute value of x:  x ≥ 0? x: − x  absolute value of z:  S x z ⌊x⌋ floor of x, greatest integer function: maxk≤x k ⌈x⌉ ceiling of x, least integer function: mink≥x k sawtooth function   x   ⟨Xn⟩ the infinite sequence X0, X1, X2, . . .  here the letter n is part of the symbolism   Where defined 4.1  4.5.3  1.2.6  1.2.6  1.2.6  1.2.11.2 1.2.2 1.2.2 1.2.2 1.2.2  1.2.2 1.2.4 1.2.4 3.3.3  1.2.9   INDEX TO NOTATIONS  733  Formal symbolism  γ x, y  Γ x  δ x   Meaning  γ Euler’s constant: limn→∞ Hn − ln n   incomplete gamma function:  y e base of natural logarithms:   gamma function:  x − 1 ! = γ x,∞  characteristic function of the integers n≥0 1 n!  when x > 1   0 e−ttx−1 dt   x  n  zeta function: limn→∞ H continuant polynomial leading coefficient of polynomial u length of shortest addition chain for n von Mangoldt’s function  ζ x  Kn x1, . . . , xn  ℓ u  l n  Λ n  µ n  Möbius function ν n   sideways sum  n≤x[n is prime] n≥0  −1 n  2n + 1   Of n  big-oh of f n , as the variable n → ∞ Of z  big-oh of f z , as the variable z → 0 Ωf n  big-omega of f n , as the variable n → ∞ Θf n  big-theta of f n , as the variable n → ∞ π x  prime count:  π circle ratio: 4 5 1 + φ n  Euler’s totient function:  sign of x: x = 0? 0: x x ppu x  primitive part of polynomial u  ∞ infinity: larger than any number det A  determinant of square matrix A sign x  deg u  degree of polynomial u content of polynomial u cont u   √ empty set: {x  0 = 1}  ϕ golden ratio: 1 2 ∅  0≤k<n[k ⊥ n]  logb x logarithm, base b, of x  when x > 0,  b > 0, and b ̸= 1 : the y such that x = by  ln x natural logarithm: loge x lg x binary logarithm: log2 x exp x exponential of x: ex j ⊥ k  j is relatively prime to k: gcd j, k  = 1  Where defined 1.2.7 1.2.11.3 1.2.5 3.3.3 1.2.2 1.2.7 4.5.3 4.6 4.6.3 4.5.3 4.5.2 4.6.3 1.2.11.1 1.2.11.1 1.2.11.1 1.2.11.1 4.5.4 4.3.1 1.2.8  1.2.4 4.2.2 1.2.3  4.6 4.6.1 4.6.1  1.2.2 1.2.2 1.2.2 1.2.9 1.2.4   734  APPENDIX B  Formal symbolism gcd j, k   Meaning     greatest common divisor of j and k:    x mod y mod function: y = 0? x: x − y⌊x y⌋  least common multiple of j and k:  j = k = 0? 0: max d\j, d\k  jk = 0? 0:  d>0, j\d, k\d  min  lcm j, k   d  d  x ≡ x′  modulo y   u x  mod v x   remainder of polynomial u after division by polynomial v relation of congruence: x mod y = x′ mod y  x ≈ y x is approximately equal to y  PrS n  probability that statement S n  is true, for PrS X  probability that statement S X  is true, for  random positive integers n  E X expected value of X:   random values of X  x x Pr X = x    min x1, ave x2,  max x3, dev x4   mean g  mean value of the probability distribution represented by generating function g: g′ 1  variance of the probability distribution represented by generating function g:  var g   g′′ 1  + g′ 1  − g′ 1 2  a random variable having minimum value x1, average  expected  value x2, maximum value x3, standard deviation x4 one blank space  ␣ rA register A  accumulator  of MIX rX register X  extension  of MIX  rI1, . . . , rI6 rJ   index  registers I1, . . . , I6 of MIX  jump  register J of MIX   L:R  partial field of MIX word, 0 ≤ L ≤ R ≤ 5  OP ADDRESS,I F  notation for MIX instruction  u unit of time in MIX * 0F, 1F, 2F, . . . , 9F 0B, 1B, 2B, . . . , 9B 0H, 1H, 2H, . . . , 9H  “self” in MIXAL “forward” local symbol in MIXAL “backward” local symbol in MIXAL “here” local symbol in MIXAL  Where defined  4.5.2  4.5.2  1.2.4  4.6.1 1.2.4 3.5, 4.2.2  3.5  1.2.10 1.2.10  1.2.10  1.2.10  1.2.10 1.3.1 1.3.1 1.3.1 1.3.1 1.3.1 1.3.1 1.3.1, 1.3.2 1.3.1 1.3.2 1.3.2 1.3.2 1.3.2   APPENDIX C  INDEX TO ALGORITHMS AND THEOREMS  Algorithm 3.1K, 5. Theorem 3.2.1.2A, 17–19. Theorem 3.2.1.2B, 20. Theorem 3.2.1.2C, 20–21. Theorem 3.2.1.2D, 21. Lemma 3.2.1.2P, 17–18. Lemma 3.2.1.2Q, 18. Lemma 3.2.1.2R, 19. Algorithm 3.2.2A, 28. Program 3.2.2A, 28. Algorithm 3.2.2B, 34. Program 3.2.2B, 34. Algorithm 3.2.2M, 33. Algorithm 3.2.2X, 557. Algorithm 3.2.2Y, 557. Algorithm 3.3.2C, 64–65. Algorithm 3.3.2G, 62. Algorithm 3.3.2P, 65–66. Algorithm 3.3.2R, 563. Algorithm 3.3.2S, 71. Lemma 3.3.3B, 84–85. Algorithm 3.3.3D, 573. Theorem 3.3.3D, 87. Theorem 3.3.3K, 89. Theorem 3.3.3P, 80–81. Lemma 3.3.4A, 99. Theorem 3.3.4N, 113. Algorithm 3.3.4S, 101–103. Algorithm 3.3.4S′, 582. Algorithm 3.4.1A, 134. Algorithm 3.4.1B, 588–589. Algorithm 3.4.1F, 129. Algorithm 3.4.1G, 587. Algorithm 3.4.1L, 126. Algorithm 3.4.1M, 127–128. Algorithm 3.4.1N, 587. Algorithm 3.4.1P, 122. Algorithm 3.4.1R, 130–131.  Algorithm 3.4.1S, 133. Algorithm 3.4.2P, 145. Algorithm 3.4.2R, 144. Algorithm 3.4.2S, 142. Definition 3.5A, 150. Theorem 3.5A, 152–153. Definition 3.5B, 151. Theorem 3.5B, 153–154. Definition 3.5C, 151. Theorem 3.5C, 155–158. Definition 3.5D, 151. Definition 3.5E, 155. Lemma 3.5E, 156. Theorem 3.5F, 158. Theorem 3.5G, 174. Algorithm 3.5L, 173. Theorem 3.5M, 166–167. Corollary 3.5P, 154. Definition 3.5P, 171. Theorem 3.5P, 175. Lemma 3.5P1, 171–172. Lemma 3.5P2, 172. Lemma 3.5P3, 172. Lemma 3.5P4, 172. Definition 3.5Q1, 168. Definition 3.5Q2, 168. Definition 3.5R1, 159. Definition 3.5R2, 159. Definition 3.5R3, 161. Definition 3.5R4, 161. Definition 3.5R5, 162. Definition 3.5R6, 163. Corollary 3.5S, 154. Lemma 3.5T, 163. Algorithm 3.5W, 164. Theorem 3.5W, 164–165. Algorithm 4.1H, 610. Algorithm 4.1S, 609.  735  Algorithm 4.2.1A, 216–217. Program 4.2.1A, 218–219. Algorithm 4.2.1M, 220. Program 4.2.1M, 220–221. Algorithm 4.2.1N, 217. Theorem 4.2.2A, 235. Theorem 4.2.2B, 236. Theorem 4.2.2C, 236. Theorem 4.2.2D, 237. Lemma 4.2.2T, 235. Program 4.2.3A, 247–249. Program 4.2.3D, 251–252. Program 4.2.3M, 249–250. Theorem 4.2.4F, 260–262. Lemma 4.2.4Q, 258–259. Algorithm 4.3.1A, 266. Program 4.3.1A, 266–267. Theorem 4.3.1A, 271. Algorithm 4.3.1A′, 623. Program 4.3.1A′, 623. Algorithm 4.3.1B, 623. Program 4.3.1B, 624. Theorem 4.3.1B, 272. Algorithm 4.3.1C, 623–624. Algorithm 4.3.1D, 272–273. Program 4.3.1D, 273–275, 626. Algorithm 4.3.1M, 268. Program 4.3.1M, 269–270. Algorithm 4.3.1N, 282. Algorithm 4.3.1Q, 625. Algorithm 4.3.1S, 267. Program 4.3.1S, 267–268. Theorem 4.3.2C, 286. Theorem 4.3.2S, 291. Theorem 4.3.3A, 296. Theorem 4.3.3B, 302. Algorithm 4.3.3R, 312. Algorithm 4.3.3T, 299–301.   736  APPENDIX C  Algorithm 4.4A, 636. Algorithm 4.5.2A, 337. Program 4.5.2A, 337. Program 4.5.2A′, 373. Algorithm 4.5.2B, 338. Program 4.5.2B, 339–340. Algorithm 4.5.2C, 341. Theorem 4.5.2D, 342. Algorithm 4.5.2E, 336. Algorithm 4.5.2K, 356. Algorithm 4.5.2L, 347. Algorithm 4.5.2X, 342. Algorithm 4.5.2Y, 646. Theorem 4.5.3E, 368. Theorem 4.5.3F, 360. Algorithm 4.5.3L, 375. Corollary 4.5.3L, 360. Lemma 4.5.3M, 367. Theorem 4.5.3W, 366. Algorithm 4.5.4A, 380. Theorem 4.5.4A, 396. Algorithm 4.5.4B, 385–386. Algorithm 4.5.4C, 387. Algorithm 4.5.4D, 389. Program 4.5.4D, 390. Theorem 4.5.4D, 402.  Algorithm 4.5.4E, 397–398. Algorithm 4.5.4F, 659–660. Algorithm 4.5.4L, 667–668. Theorem 4.5.4L, 409–411. Algorithm 4.5.4P, 395. Algorithm 4.5.4S, 658. Algorithm 4.6.1C, 428–429. Algorithm 4.6.1D, 421. Algorithm 4.6.1E, 426–427. Lemma 4.6.1G, 422–423. Lemma 4.6.1H, 423. Algorithm 4.6.1R, 425–426. Algorithm 4.6.1S, 676. Algorithm 4.6.1T, 677. Algorithm 4.6.2B, 441–442. Algorithm 4.6.2D, 447–448. Algorithm 4.6.2F, 452. Algorithm 4.6.2N, 444. Algorithm 4.6.2S, 681–682. Algorithm 4.6.3A, 462. Corollary 4.6.3A, 468. Program 4.6.3A, 691. Theorem 4.6.3A, 467–468. Theorem 4.6.3B, 468–469. Theorem 4.6.3C, 469–470. Theorem 4.6.3D, 470–471.  Corollary 4.6.3E, 472–473. Theorem 4.6.3E, 471–472. Theorem 4.6.3F, 476. Theorem 4.6.3G, 479. Theorem 4.6.3H, 475–476. Lemma 4.6.3K, 474. Lemma 4.6.3P, 471. Algorithm 4.6.3T, 692. Theorem 4.6.4A, 496. Algorithm 4.6.4C, 713. Theorem 4.6.4C, 496. Algorithm 4.6.4D, 489. Theorem 4.6.4E, 493–494. Algorithm 4.6.4G, 698. Algorithm 4.6.4H, 489. Algorithm 4.6.4J, 698. Theorem 4.6.4M, 495. Algorithm 4.6.4N, 713–714. Algorithm 4.6.4S, 489. Lemma 4.6.4T, 508. Theorem 4.6.4W, 513–514. Algorithm 4.7G, 720. Algorithm 4.7L, 527–528. Algorithm 4.7N, 529. Algorithm 4.7T, 528.  At any step, arbitrary combinations of algorithms and theorems can be applied to solve a given problem. — KARSTEN HOMANN and JACQUES CALMET  1995    INDEX AND GLOSSARY Seek and ye shall find. — Matthew 7:7  When an index entry refers to a page containing a relevant exercise, see also the answer to that exercise for further information. An answer page is not indexed here unless it refers to a topic not included in the statement of the exercise.  0-origin indexing, 444, 512. 0–1 matrices, 499. 0–1 polynomials, 497, 519, 707. [0 . . 1  sequence, 151. 2-adic numbers, 213, 629. 10-adic numbers, 632. 1009, vi, 188, 413, 661. 69069, 75, 106, 108. ∞, representation of, 225, 244–245, 332. ∞-distributed sequence, 151–161, γ  Euler’s constant , 359, 379, 726–727, 733. π  circle ratio , 41, 151, 158, 161, 198, 200, 209, 279–280, 284, 358, 726–727, 733. as “random” example, 21, 25, 33, 47, 52,  177, 180–182.  89, 103, 106, 108, 184, 238, 243, 252, 324–325, 555, 593, 599, 665. π x   prime count , 381–382, 416. ρ n   ruler function , 540. ϕ  golden ratio , 164, 283, 359, 360, 514,  652, 726–727, 733.  logarithm of, 283. number system, 209.  φ n   totient function , 19–20, 289,  369, 376, 583, 646.  χ2, 42, 56, see Chi-square.  A priori tests, 80. Abacus, 196. binary, 200.  Abel, Niels Henrik, binomial theorem,  58, 535.  Abramowitz, Milton, 44. Absolute error, 240, 309, 312–313. Absorption laws, 694. Abuse of probability, 433. Abuse of theory, 88. ACC: Floating point accumulator,  218–219, 248–249.  Acceptance-rejection method, 125–126,  128–129, 134, 138, 139, 591.  Accuracy of floating point arithmetic, 222,  229–245, 253, 329, 438, 485.  Accuracy of random number generation,  27, 95, 105, 185.  Adaptation of coefficients, 490–494, 516–517. Add-with-carry sequence, 23, 35, 72,  108, 547.  Addition, 194, 207, 210, 213, 265–267.  235–238, 253–254, 602.  complex, 487. continued fractions, 649. double-precision, 247–249, 251. floating point, 215–220, 227–228, 230–231, fractions, 330–331. left to right, 281. mixed-radix, 281. mod m, 12, 15, 203, 287–288. modular, 285–286, 293. multiprecision, 266–267, 276–278, polynomial, 418–420. power series, 525. sideways, 463, 466. ascending, 467. dual, 481, 485. l0-, 479, 483, 485. star, 467, 473–477, 480, 482.  281, 283.  Addition chains, 465–485, 494, 519.  39–40, 186–188, 193. 414, 417, 671.  Addition-subtraction chains, 484. Additive random number generation, 27–29, Adleman, Leonard Max, 403, 405, Admissible numbers, 177. Agrawal, Manindra   cid:109  cid:90  cid:70  cid:6  cid:100  cid:253  a cid:103  cid:125  cid:118  cid:65  cid:108  , 396. Ahrens, Joachim Heinrich Lüdecke,  119, 129–130, 132–134, 136, 137, 140, 141, 588.  al-Khw¯arizmı, Ab¯u ‘Abd All¯ah  Ahrens, Wilhelm Ernst Martin Georg, 208. Akushsky, Izrail Yakovlevich  Akuxski cid:26 , al-Bır¯unı, Ab¯u al-Rayh. an Muh. ammad  Izrail~  cid:23 kovleviq , 292. ibn Ah. mad  ˛n cid:132  cid:218  cid:143 ¿m  cid:216 pc  cid:222 ˇ cid:215  cid:143  cid:219 q¿m  cid:139  cid:204  cid:131 c  cid:209 p  cid:139  cid:204  cid:132 ¸ , 461. al-K¯ashı, Jamshıd ibn Mas‘¯ud   cid:222  cid:151 n…¿m  cid:138  cid:216 ‹ cid:148 ¸  cid:209 p  cid:139  cid:219  cid:152  cid:204 ~ , 198, 326, 462. Muh. ammad ibn M¯us¯a   cid:222 ¸ cid:144  cid:142 m cid:216  cid:136 ¿m  cid:220  cid:147  cid:216 ¸  cid:209 p  cid:139  cid:204  cid:132 ¸ ?m  cid:139 q«  cid:216 pc , 197, 280. ibn Yah. y¯a ibn Yah¯uda al-Maghribı  `« cid:216  cid:204  cid:148 ¿m  cid:222 p cid:143  cid:176  cid:204 ¿m m cid:138  cid:216  cid:212  cid:218   cid:209 p  cid:220  cid:219  cid:132  cid:218   cid:209 pm , 198. al-Uqlıdisı, Ab¯u al-H. asan Ah. mad ibn Ibr¯ahım   cid:222  cid:147  cid:139  cid:219  cid:192  ˜m ˝ cid:219  cid:211 m cid:143 pg  cid:209 p  cid:139  cid:204  cid:131 c  cid:209  cid:148  cid:132 ¿m  cid:216 pc , 198, 280–281, 461.  al-Samaw’al  = as-Samaw’al ,  Ala-Nissilä, Tapio, 75, 570.  737   738  INDEX AND GLOSSARY  Alanen, Jack David, 30. Aldous, David John, 145. Alekseyev, Valery Borisovich  Alekseev,  Valeri cid:26  Borisoviq , 699.  Alexeev, Boris Vasilievich  Alekseev,  Boris Vasil~eviq , 117.  Alexi, Werner, 669. Alford, William Robert, 659. Algebra, free associative, 437. Algebraic dependence, 496, 518. Algebraic functions, 533. Algebraic integers, 396. Algebraic number fields, 331, 333,  345, 403, 674.  Algebraic system: A set of elements together with operations defined on them, see Field, Ring, Unique factorization domain.  ALGOL language, 279. Algorithms: Precise rules for transforming specified inputs into specified outputs in a finite number of steps.  analysis of, 7–9, 76, 140, 147, 276–278,  281, 301–302, 348–356, 360–373, 377–378, 382–384, 399–400, 435, 445–447, 455–456, 530–532, 658, 714.  complexity of, 138, 178–179, 280, 294–318,  396, 401–402, 416, 453, 465–485, 494–498, 516–524, 720.  discovery of, 99. historical development of, 335, 461–462. proof of, 281–282, 336–337, 592.  Alias method, 120, 127, 139. Allouche, Jean-Paul, 656. ALPAK system, 419. Alt, Helmut, 706. American National Standards Institute, AMM: American Mathematical Monthly,  226, 246, 600, 602.  published by the Mathematical Association of America since 1894.  Amplification of guesses, 172–174, 416–417. Analysis of algorithms, 7–9, 76, 140, 147,  276–278, 281, 301–302, 348–356, 360–373, 377–378, 382–384, 399–400, 435, 445–447, 455–456, 530–532, 658, 714. history, 360.  Analytical Engine, 189, 201. Ananthanarayanan, Kasi  xW cid:146  A cid:128 i cid:127  cid:129 W cid:133 W cid:132 ~h , 128. AND  bitwise and , 140, 188, 322, 328–329, 389–390, 453, 671.  Anderson, Stanley Frederick, 312. ANSI: The American National Standards  Institute, 226, 246, 600, 602.  Antanairesis, 335–336, 378. Apollonius of Perga  >Apoll‚nioc   cid:229  Perga cid:216 oc , 225.  Apparently random numbers, 3–4, 170–171. Apparition, rank of, 410–411.  239–240, 244. 239, 242–243, 245.  Approximate associative law, 232–233, Approximate equality, 224, 233–235, Approximately linear density, 126. Approximation, by rational functions,  by rational numbers, 331–332,  438–439, 534. 378–379, 617. 326, 461–462.  Arabic mathematics, 197, 280–281, Arazi, Benjamin  IFX@ OINIPA , 396. Arbitrary precision, 279, 283, 331, 416, see also Multiple-precision. Arbogast, Louis François Antoine, 722. Archibald, Raymond Clare, 201. Arctangent, 313, 628. Aristotle of Stagira, son of Nicomachus  Arithmetic, 194–537, see Addition,   >AristotŁlhc Nikom cid:136 qou  cid:229  Stagir—thc , 335. Comparison, Division, Doubling, Exponentiation, Greatest common divisor, Halving, Multiplication, Reciprocals, Square root, Subtraction. 487, 501, 506, 519, 700, 706.  complex, 205, 228, 283, 292, 307–310, floating point, 214–264. fractions, 330–333, 420, 526. fundamental theorem of, 334, 422, 483. mod m, 12–16, 185–186, 203, 284, modular, 284–294, 302–305, 450, 454, 499. multiprecision, 265–318. polynomial, 418–524. power series, 525–537. rational, 330–333, 420, 526.  287–288.  Information Interchange, 417.  Arithmetic chains, see Quolynomial chains. Armengaud, Joël, 409. Arney, James W., 385. Arrival time, 132. Arwin, Axel, 687. ¯Aryabhat.a I  a cid:65  cid:121  cid:13  cid:66  cid:86  , 343. ASCII: The American Standard Code for Ashenhurst, Robert Lovett, 240, 242, 327. Associative law, 229–233, 242, 341, 418, 694. Asymptotic values: Functions that express  approximate, 232–233, 239–240, 244. the limiting behavior approached by numerical quantities, 59–60, 79, 263–264, 355, 372–373, 377–378, 415, 472, 525, 541–542, 659, 686, 722.  Atanasoff, John Vincent, 202. Atkin, Arthur Oliver Lonsdale, 681. Atrubin, Allan Joseph, 315. Automata  plural of Automaton , Automorphic numbers, 293–294. Avogadro di Quaregna e Cerreto, Lorenzo  313–317, 329, 416.  Romano Amedeo Carlo, number, 214, 227, 238, 240. Axioms for floating point arithmetic, 230–231, 242–245. Avanzi, Roberto Maria, 396.   b-ary number, 151. b-ary sequence, 151–153, 177. Babbage, Charles, 201. Babenko, Konstantin Ivanovich  Babenko,  Konstantin Ivanoviq , 366, 376.  Babington-Smith, Bernard, 3, 74, 76. Babylonian mathematics, 196, 225, 335. Bach, Carl Eric, 395, 661, 663, 689. Bachet, Claude Gaspard, sieur de  Méziriac, 208.  Bag, 694. Bailey, David Harold, 284, 634. Baker, Kirby Alan, 316. Balanced binary number system, 213. Balanced decimal number system, 211. Balanced mixed-radix number system,  103, 293, 631.  Balanced ternary number system, 207–208,  209, 227, 283, 353.  Ballantine, John Perry, 278. Bareiss, Erwin Hans, 262, 292, 434. Barlow, Jesse Louis, 262. Barnard, Robert, 292. Barnsley, Michael Fielding, 206. Barton, David Elliott, 74, 566. Barycentric coordinates, 567. Base of representation, 195.  floating point, 214–215, 254, 263.  Baseball, 378. Bauer, Friedrich Ludwig, 241–242, 327. Baum, Ulrich, 701. Baur, Walter, 718. Bays, John Carter, 34. Beauzamy, Bernard, 452, 461, 683, 684. Beckenbach, Edwin Ford, 135. Becker, Oskar Joachim, 359. Béjian, Robert, 164. Belaga, Edward Grigorievich  Belaga,   cid:3 duard Grigor~eviq , 496.  Bell, Eric Temple, polynomials, 722. Bell Telephone Laboratories Model V, 225. Bellman, Richard Ernest, ix. Ben-Or, Michael  XE@ -OA L@KIN , 669. Bender, Edward Anton, 385. Benford, Frank, 255. Bentley, Jon Louis, 141. Berger, Arno, 262. Bergman, George Mark, 676. Berkowitz, Stuart J., 718. Berlekamp, Elwyn Ralph, 439, 449, 456, 681.  algorithm, 439–447.  Bernoulli, Jacques  = Jakob = James , 200.  numbers Bn, 355, 569. numbers, table, 728. sequences, 177.  Bernoulli, Nicolas  = Nikolaus , 449. Bernstein, Daniel Julius, 396, 697, 724. Berrizbeitia Aristeguieta, Pedro José de  la Santísima Trinidad, 396. Besicovitch, Abram Samoilovitch  INDEX AND GLOSSARY  739  Beta distribution, 134–135. Beyer, William Aaron, 115. Bharati Krishna Tirthaji Maharaja,  Jagadguru Swami Sri   cid:106  cid:103 dg cid:0  cid:122   cid:45  cid:118  cid:65  cid:109  cid:70   cid:128  cid:70   cid:66  cid:65  cid:114  cid:116  cid:70   cid:107  cid:2   cid:9  cid:90   cid:116  cid:70  cid:84  cid:13  cid:106  cid:70   cid:109  cid:104  cid:65  cid:114  cid:65  cid:106  , 208.  Bh¯askara I, ¯Ac¯arya   cid:66  cid:65  cid:45  cid:107  cid:114  cid:65  cid:99  cid:65  cid:121  cid:13  , 343. Bienaymé, Irénée Jules, 74. Bilinear forms, 506–514, 520–524. Billingsley, Patrick Paul, 384, 661. Bin-packing problem, 585. Binary abacus, 200. Binary basis, 212. Binary-coded decimal, 202, 322, 328–329. Binary computer: A computer that  manipulates numbers primarily in the binary  radix 2  number system, 30–32, 201–202, 276, 328, 339, 389–390. Binary-decimal conversion, 319–329. Binary digit, 195, 200. Binary gcd algorithms, 338–341,  348–356, 435.  compared to Euclid’s, 341. extended, 356.  Binary method for exponentiation,  461–463, 466, 482, 696.  Binary number systems, 195, 198–206,  209–213, 419, 461, 483.  Binary point, 195. Binary recurrences, 318, 466, 634, 692, 714. Binary search, 324. Binary search trees, 593. Binary shift, 322, 339, 481, 637, 686. Binary trees, 378, 527, 696, 723. BINEG computer, 205. Binet, Jacques Philippe Marie, 653.  identity: n n 1≤j<k≤n aj bk − akbj  xj yk − xkyj ,  k=1 bkyk =  k=1 bkxk +  n  n  j=1 aj xj  j=1 aj yj  564.  Bini, Dario Andrea, 500, 505, 515,  714, 715, 721.  Binomial coefficients, 416, 516, 622. Binomial distribution, 136–138, 141,  401, 559. tail of, 167.  Binomial number system, see Combinatorial  number system.  Binomial theorem, 526, 534. Birnbaum, Zygmunt Wilhelm, 57. Birthday spacings, 34, 71–72, 78–79, 188. BIT: Nordisk Tidskrift for Informations- Behandling, an international journal published in Scandinavia since 1961.  Bit: “Binary digit”, either zero or  unity, 195, 200.  random, 12, 30–32, 35–36, 38, 48,   Bezikoviq, Abram Samo cid:26 loviq , 178.  119–120, 170–176.   740  INDEX AND GLOSSARY  Bitwise operations, 30–31, 140, 202,  328–329, 389–390, 459, 605.  and, 140, 188, 322, 328–329, 389–390,  453, 671.  exclusive or, 31, 32, 193, 419. or, 140, 686, 695. shifts, 322, 339, 481, 637, 686.  Björk, Johan Harry, 244. Blachman, Nelson Merle, 205. Black box, 455. Bläser, Markus, 700. Bleichenbacher, Daniel, 478. Blinn, James Frederick, 630. Blöte, Hendrik Willem Jan, 29. Blouin, François Joseph Raymond  Marcel, 582.  Bluestein, Leo Isaac, 634. Blum, Bruce Ivan, 279. Blum, Fred, 433, 518. Blum, Lenore Carol, 36. Blum, Manuel, 36, 174, 179. integer, 174–176, 183, 416.  Bofinger, Eve, 563. Bofinger, Victor John, 563. Bohlender, Gerd, 242, 616. Bojańczyk, Adam Wojciech, 646. Bolker, Ethan David, 593. Bombieri, Enrico, 683.  norm, 458, 684.  Boolean functions, 173–174. Boolean operations, see Bitwise operations. Boone, Steven Richard, 409. Booth, Andrew Donald, 608. Border rank, 522–523. Borel, Émile Félix Édouard Justin, 177. Borodin, Allan Bertram, 498, 505, 515, 707. Borosh, Itzhak, 106–107, 117, 291, 584. Borrow: A negative carry, 267, 273,  281, 545.  Borwein, Peter Benjamin, 284. Bosma, Wiebren, 665. Bouyer, Martine, 280. Bowden, Joseph, 201. Box, George Edward Pelham, 122. Boyar, Joan, 599. Boyd, David William, 691. Bradley, Gordon Hoover, 343, 378. Brakke, Kenneth Allen, 608. Bramhall, Janet Natalie, 530. Brauer, Alfred Theodor, 470, 478, 483, 690. Bray, Thomas Arthur, 33, 128, 544. Brent, Richard Peirce, 8, 28, 40, 130, 136, 139, 141, 187, 241, 279, 280, 313, 348, 352–353, 355, 356, 382, 386, 403, 501, 529–534, 539–540, 556, 590, 600, 643, 644, 646, 657, 658, 695, 719–721.  Brezinski, Claude, 357, 721. Brillhart, John David, 29, 394, 396, 400, 660. Brockett, Roger Ware, 712. Brocot, Achille, 655. Brontë, Emily Jane, 292.  Brooks, Frederick Phillips, Jr., 226. Brouwer, Luitzen Egbertus Jan, 179. Brown, David, see Spencer Brown. Brown, George William, 135. Brown, Mark Robbin, 712. Brown, Robert, see Brownian motion. Brown, William Stanley, 419, 428,  438, 454, 686.  Brownian motion, 559. Bruijn, Nicolaas Govert de, 181, 212,  568, 653, 664, 686, 694.  cycle, 38–40.  Brute force, 642. Bshouty, Nader Hanna   cid:222 v cid:216  cid:152 p n— cid:131   cid:142  cid:138 nˇ , 700. Buchholz, Werner, 202, 226. Bunch, James Raymond, 500. Buneman, Oscar, 706. Bunimovich, Leonid Abramovich   Bunimoviq, Leonid Abramoviq , 262.  Bürgisser, Peter, 515. Burks, Arthur Walter, 202. Burrus, Charles Sidney, 701. Butler, James Preston, 77. Butler, Michael Charles Richard, 442.  C language, 185–188, 193, 327, 556. CACM: Communications of the ACM, a publication of the Association for Computing Machinery since 1958.  Cahen, Eugène, 676. Calculating prodigies, 279, 295. Calmet, Jacques Francis, 736. Cameron, Michael James, 409. Camion, Paul Frédéric Roger, 449. Campbell, Edward Fay, Jr., vii. Campbell, Sullivan Graham, 226. Cancellation error, 58, 245.  avoiding, 617.  Canonical signed bit representation, 611. Cantor, David Geoffrey, 446, 448, 449,  455, 460, 672, 681.  Cantor, Georg Ferdinand Ludwig  Philipp, 209.  Cantor, Moritz Benedikt, 655. Capovani, Milvio, 500, 715. Caramuel de Lobkowitz, Juan, 199–200. Cards, playing, 2, 145, 147, 190. Carissan, Eugène Olivier, 390. Carling, Robert Laurence, 104. Carlitz, Leonard, 84, 90. Carmichael, Robert Daniel, numbers,  659, 662.  Carr, John Weber, III, 226, 241, 242. Carroll, Lewis  = Dodgson, Charles  Lutwidge , 435.  Carry: An amount propagated to the  current digit position from the digits in less significant positions, 205, 247, 266, 268, 273, 276–278, 281, 419, 470, 547.  Cassels, John William Scott, 109, 158. Casting out nines, 289, 303, 324.   Castle, Clive Michael Anthony, 653. Catalan, Eugène Charles, numbers, 723. Cauchy, Augustin Louis, 208.  inequality, 97, 231. matrices, 331.  CCITT: The International Telegraph  and Telephone Consultative Committee of the ITU  International Telecommunication Union , 405.  CDC 1604 computer, 291. CDC 7600 computer, 280. Ceiling function ⌈x⌉, 81, 732. Cellular automaton, see Linear iterative  array.  Cerlienco, Luigi, 683. Certificate of irreducibility, 460. Certificate of primality, 413. Cervantes Saavedra, Miguel de, 148. Cesàro, Ernesto, 354, 622, 640. Ceulen, Ludolph van, 198. Chace, Arnold Buffum, 462. Chain multiplications, 518, 519, 524. Chain steps, 494. Chains of primes, 415, 666. Chaitin, Gregory John, 170, 178. Chan, Tony Fan-Cheong   Chapple, Milton Arthur, 530. CHAR  convert to characters , 328. Characteristic, 214, see Exponent part. Characteristic polynomial, 499, 524. Charles XII of Sweden, 200. Chartres, Bruce Aylwin, 242. Chebotarev, Nikolai Grigorievich   , 615.   Qebotar cid:27 v, Nikola cid:26  Grigor~eviq , 690.  Chebyshev  = Tschebyscheff , Pafnutii  Lvovich  Qebyxev cid:127 , Pafnut cid:12  cid:26  L~voviq cid:127  = Qebyxev, Pafnuti cid:26  L~voviq , inequality, 183, 669.   , 396.  Cheng, Qi   Cheng, Russell Ch’uan Hsun   Chesterton, Gilbert Keith, 417, 537. Chi-square distribution, 44, 48, 60,   , 135.  69, 135, 590.  table, 44.  Chi-square test, 42–47, 53–56, 58–60. Childers, James Gregory, 671. Ch’in Chiu-Shao  = Qín Jiˇusháo       , 287, 486.  Chinese mathematics, 197–198, 287,  340–341, 486.  Chinese remainder algorithm, 21, 289–290,  293, 304–305, 505.  Chinese remainder theorem, 285–290,  389, 404, 584.  for polynomials, 440, 456, 509–510. generalized, 292.  Chiò, Felice, 435. Chirp transform, 634. Chiu Chang Suan Shu   Choice, random, 2, 119–121, 142.   , 340.  INDEX AND GLOSSARY  741  Chor, Benny  XEY OEIV -OA , 669. Christiansen, Hanne Delgas, 74. Christie Mallowan, Agatha Mary Clarissa Chudnovsky, David Volfovich  Qudnovski cid:26 , Chudnovsky, Gregory Volfovich  Miller, 725. David Vol~foviq , 280, 311, 533.  Qudnovski cid:26 , Grigori cid:26  Vol~foviq , 280, 311, 533.  Church, Alonzo, 178. Cipolla, Michele, 682. Clarkson, Roland Hunter, 409. Classical algorithms, 265–284. Clausen, Michael Hermann, 515, 701. Clift, Neill Michael, 477–479, 485. Clinger, William Douglas, 638. CMath: Concrete Mathematics, a book  by R. L. Graham, D. E. Knuth, and O. Patashnik.  Cochran, William Gemmell, 55. Cocke, John, 228. Cocks, Clifford Christopher, 407. Codes, linear, 711. Codes for difficulty of exercises, ix–xi. Cody, William James, Jr., 226. Coefficients of a polynomial, 418.  adaptation of, 490–494, 516–517. leading, 418, 451–452, 454. size of, 420, 451, 457–458, 461. Cohen, Daniel Isaac Aryeh, 622. Cohen, Henri José, 345, 658, 687, 712. Cohn, Paul Moritz, 436, 676. Coincidence, 6, 8. Collenne, Joseph Désiré, 201. Collins, George Edwin, 278, 279, 373, 420, 428, 453, 454, 460, 677. Collision test, 70–71, 74, 158. Color values, 284. Colson, John, 208. Colton, Charles Caleb, vii. Column addition, 281, 284. Combination, random, 142–148. Combination of random number generators, Combinations with repetitions, 664. Combinatorial matrices, 116. Combinatorial number system, 209. Commutative law, 230, 333, 418, 500, Commutative ring with identity, 418, Comp. J.: The Computer Journal, a  694, 696. 420, 425. publication of the British Computer Society since 1958. Compagner, Aaldert, 29, 169. Companion matrix, 512. Comparison: Testing for  .  33–36, 38, 39.  239, 242–243.  continued fractions, 654. floating point numbers, 233–235, fractions, 332. mixed-radix, 290. modular, 290. multiprecision, 281.   742  INDEX AND GLOSSARY  Complement notations for numbers, 15,  203–204, 210, 213, 228, 275–276.  Complete binary tree, 667. Completely equidistributed sequence, 177. Complex arithmetic, 205, 228, 283, 292,  307–310, 487, 501, 506, 519, 700, 706.  Complex numbers, 420, 497.  representation of, 205–206, 209–210, 292.  Complex radices, 205–206, 209–210. Complexity of calculation, 138, 178–179, 280, 294–318, 396, 401–402, 416, 453, 465–485, 494–498, 516–524, 720.  Composition of power series, 533,  535–536, 720.  Computability, 162–163, 178. Concave function, 125, 139, 245, 627. Conditional expression, 730. Congruential sequence, inversive, 32–33, 40. Congruential sequence, linear, 10–26,  145–146, 184–186, 193.  choice of increment, 10–11, 17, 22,  89, 97, 185.  choice of modulus, 12–16, 23, 184. choice of multiplier, 16–26, 88–89,  105–109, 184–185.  choice of seed, 17, 20, 143, 184. period length, 16–23. subsequence of, 11, 73.  Congruential sequence, quadratic, 26–27, 37. Conjugate of a complex number, 700, 731. Connection Machine, 538. Content of a polynomial, 423. Context-free grammar, 694. Continuant polynomials, 357, 360, 374, 377,  379, 438, 647, 651, 655, 676.  Continued fractions, 356–359, 396–401.  infinite, 358–359, 374. quadratic irrationalities, 358, 374–375,  397–401, 412, 415, 665.  regular, 346, 358–359, 368, 374–379,  with polynomial quotients, 438–439,  412, 415, 665.  498, 518.  Continuous binomial distribution, 588. Continuous distribution functions, 49,  53, 57, 60, 121–136.  Continuous Poisson distribution, 588. Convergents, 378, 397, 438–439, 617, 622. Conversion of representations, 221, 228,  252–253, 265, 288–290, 293, 304–305, see also Radix conversion.  Convex function, 125, 139, 245, 684. Convolution, 305, 318, 525, 586.  cyclic, 294, 305–307, 510–512, 520, 521. multidimensional, 710. negacyclic, 521. polynomials, see Poweroids.  Conway, John Horton, 109, 402, 623. Cook, Stephen Arthur, 211, 297, 299,  312, 318, 672, 707.  Cooley, James William, 701.  Coolidge, Julian Lowell, 486. Coonen, Jerome Toby, 226. Cooper, Curtis Niles, 409. Copeland, Arthur Herbert, 177. Coppersmith, Don, 182, 183, 500,  501, 523, 671.  Cormack, Gordon Villy, 664. Coroutine, 375. Corput, Johannes Gualtherus van der,  163–164, 181.  Correlation coefficient, 72–73, 77, 132. Cosine, 247, 490. Cotes, Roger, 651. Couffignal, Louis, 202. Counting law, 694. Coupon collector’s test, 63–65, 74,  76, 158, 180.  Couture, Raymond, 546, 582. Covariance, 67.  matrix, 60, 69, 139.  Cover, Thomas Merrill, 571. Coveyou, Robert Reginald, 26–27, 37,  88, 92, 114, 115, 553. Cox, Albert George, 278. Crandall, Richard Eugene, 403, 632. Craps, 190. Cray T94 computer, 409. Cray X-MP computer, 108. Creative writing, 190–193. Crelle: Journal für die reine und angewandte  Mathematik, an international journal founded by A. L. Crelle in 1826. Cryptography, 2, 193, 403–407, 415,  Cube root modulo m, 404, 415. Cunningham, Allan Joseph Champneys, 666. Cusick, Thomas William, 584. Cut-and-riffle, 147. Cycle in a random permutation, 384, 460. Cycle in a sequence, 4, 10, 22, 37–40.  417, 505.  detection of, 7–8.  Cyclic convolution, 294, 305–307,  510–512, 520, 521.  Cyclotomic polynomials, 394, 451,  459, 510, 514.  Dahl, Ole-Johan, 148, 592. Daniels, Henry Ellis, 568. Dase, Johann Martin Zacharias, 279. Datta, Bibhutibhusan  ib& it& W, d¿  = Bidy¯aranya, Swami  Sv;mI ibd*;r,* , 343, 461.  Daudé, Hervé, 366. Davenport, Harold, 648. David, Florence Nightingale, 3, 566. Davis, Chandler, 606. Davis, Clive Selwyn, 651. de Bruijn, Nicolaas Govert, 181, 212,  568, 653, 664, 686, 694.  cycle, 38–40.  de Finetti, Bruno, 566.   de Groote, Hans Friedrich, 708. de Jong, Lieuwe Sytse, 515. de Jonquières, Jean Philippe Ernest de Fauque, 465–466, 469, 477. de La Vallée Poussin, Charles Jean  Gustave Nicolas, 381.  de Lagny, Thomas Fantet, 279, 360. de Mairan, Jean-Jacques d’Ortous, 537. de Maupertuis, Pierre-Louis Moreau, 537. de Moivre, Abraham, 537. Debugging, 193, 221–223, 275, 331. Decimal computer: A computer that  manipulates numbers primarily in the decimal  radix ten  number system, 21, 202–203. Decimal digits, 195, 319. Decimal fractions, history, 197–198, 326. Decimal number system, 197–199, 210,  320–326, 374. Decimal point, 195. Decimation, 326, 328. Decision, unbiased, 2, 119–121. DECsystem 20 computer, 15. Decuple-precision floating point, 283. Dedekind, Julius Wilhelm Richard, 83, 687.  sums, generalized, 83–92, 106.  Definitely greater than, 224, 233–235,  Definitely less than, 224, 233–235,  239, 242–243.  239, 242–243.  Definition of randomness, 2, 149–183. Dégot, Jérôme, 683. Degree of a polynomial, 418, 420, 436. Degrees of freedom, 44, 495, 517–518, 704. Dekker, Theodorus Jozef, 242, 244, 253. Deléglise, Marc, 667. Dellac, Hippolyte, 465. DeMillo, Richard Allan, 675. Denneau, Monty Montague, 311. Density function, 124–126, 139.  nearly linear, 126.  Dependent normal deviates, 132, 139. Derandomization, 414. Derflinger, Gerhard, 138. Derivatives, 124, 439, 489, 524, 526, 537. Descartes, René, 407. Determinants, 356, 373, 432, 434,  498–500, 523–524.  Deviate: A random number. Devroye, Luc Piet-Jan Arthur, 138. Dewey, Melville  = Melvil  Louis Kossuth,  notation for trees, 555.  Diaconis, Persi Warren, 145, 263, 264, 622. Diamond, Harold George, 245. Dice, 2, 7, 42–43, 45–46, 58, 120–121, 190. Dickman, Karl Daniel, 382–383. Dickman–Golomb constant, 661. Dickson, Leonard Eugene, 287, 387, 646. Dictionaries, 201–202.  INDEX AND GLOSSARY  743  Dieter, Ulrich Otto, 89, 91, 92, 101,  114, 116, 119, 129–130, 132, 134, 137, 573, 574, 588.  Differences, 297–298, 504, 516. Differential equations, 526–527. Differentiation, see Derivatives. Diffie, Bailey Whitfield, 406. Digit: One of the symbols used in positional  notation; usually a decimal digit, one of the symbols 0, 1, . . . , or 9.  binary, 195, 200. decimal, 195, 319. hexadecimal, 195, 210. octal, 210.  Dilcher, Karl Heinrich, 403. Dilogarithm, 621. Diophantine equations, 343–345, 354, 417, 449, 648. Diophantus of Alexandria  Di cid:236 fantoc  cid:229  >Alexandre cid:212 c , see Diophantine equations.  Direct product, 520, 522–523. Direct sum, 520, 522–523. Directed graph, 480–481, 484–485. Dirichlet, Johann Peter Gustav  conjecture, 708.  Lejeune, 342.  series, 536–537, 695.  Discrepancy, 39, 110–115. Discrete distribution functions, 48, Discrete Fourier transforms, 169, 305–311,  120–121, 136–138. 316–318, 501–503, 506, 512, 516, 520–521, 524, 595.  Discrete logarithms, 417. Discriminant of a polynomial, 674, 686. Distinct-degree factorization, 447–449, Distribution: A specification of probabilities  459, 689. that govern the value of a random variable, 2, 119, 121.  beta, 134–135. binomial, 136–138, 141, 401, 559. chi-square, 44, 48, 60, 69, 135, 590. exponential, 133, 137, 589. F -, 135. of floating point numbers, 253–264. gamma, 253–264. geometric, 136, 137, 140, 585. integer-valued, 136–141. Kolmogorov–Smirnov, 57–60. of leading digits, 254–264, 282, 404. negative binomial, 140. normal, 56, 122, 132, 139, 384, 565. partial quotients of regular continued Poisson, 55, 137–138, 140, 141, 538, 570. of prime factors, 382–384, 413. of prime numbers, 381–382, 405. Student’s, 135. t-, 135. tail of binomial, 167. tail of normal, 139. uniform, 2, 10, 48, 61, 119, 121, 124, 263. variance-ratio, 135. wedge-shaped, 125–126.  fractions, 362–369, 665.   744  INDEX AND GLOSSARY  Distribution functions, 48, 121, 140,  263, 362, 382–384.  continuous, 49, 53, 57, 60, 121–136. discrete, 48, 120–121, 136–138. empirical, 49. mixture of, 123–124, 138. polynomial, 138. product of, 121.  Distributive laws, 231, 334, 418, 694. Divide-and-correct, 270–275, 278, 282–284. Divided differences, 504, 516. Dividend: The quantity u while computing ⌊u v⌋ and u mod v, 270. 311–313.  Division, 194, 265, 270–275, 278, 282–284,  algebraic numbers, 333, 674. avoiding, 523–524. balanced ternary, 283. by ten, 321, 328. by zero, 220, 224, 241, 639. complex, 228, 283, 706. continued fractions, 649. double-precision, 251–252, 278–279. exact, 284. floating point, 220–221, 230–231, 243. fractions, 330. long, 270–275, 278, 282–284. mixed-radix, 209, 635. mod m, 354, 445, 499. multiprecision, 270–275, 278–279,  282–283, 311–313.  multiprecision by single-precision, 282. polynomial, 420–439, 487, 534. power series, 525–526, 533–534. pseudo-, 425–426, 435–436. quater-imaginary, 283. short, 282. string polynomials, 436–437. ⌊u v⌋ and u mod v, 270. Divisor: x is a divisor of y if y mod x = 0 and x > 0; it is a proper divisor if it is a divisor such that 1 < x < y.  Divisor: The quantity v while computing  polynomial, 422.  Dixon, John Douglas, 372, 401–402,  412, 414, 415, 417.  Dixon, Wilfrid Joseph, 565. Dobell, Alan Rodney, 17. Dobkin, David Paul, 697, 712. Dodgson, Charles Lutwidge, 435. Donsker, Monroe David, 559. Doob, Joseph Leo, 559. Dorn, William Schroeder, 488. Dot product, 36, 97, 173–174, 499–501. Double-precision arithmetic, 246–253,  278–279, 295. Doubling, 322, 462.  continued fraction, 375.  Doubling step, 467. Downey, Peter James, 485. Dragon curve, 606, 609, 655.   , 287.  Dragon sequence, 655. Dresden, Arnold, 196. Drift, 237, 244. Du Shiran   Dual of an addition chain, 481, 484, 485. Duality formula, 569. Duality principle, 481, 485, 507, 535, 718. Dubner, Harvey Allen, 664. Duffield, Nicholas Geoffrey, 593. Dumas, Philippe, 355. Duncan, Robert Lee, 264. Duodecimal number system, 199–200. Dupré, Athanase, 653. Durbin, James, 57, 568. Durham, Stephen Daniel, 34. Durstenfeld, Richard, 145.  584, 603.  e  base of natural logarithms , 12, 76,  359, 726–727, 733.  as “random” example, 21, 33, 47, 52, 108.  Earle, John Goodell, 312. Eckhardt, Roger Charles, 189. L’Ecuyer, Pierre, 108, 179, 546, 582,  Edelman, Alan Stuart, 280. Edinburgh rainfall, 74. EDVAC computer, 225–226. Effective algorithms, 161–166, 169, 178. Effective information, 179. Egyptian mathematics, 335, 462. Eichenauer-Herrmann, Jürgen, 32, 558, 559. Eisenstein, Ferdinand Gotthold Max, 457. Electrologica X8 computer, 222. Electronic mail, 406. Elementary symmetric functions, 682–683. Elkies, Noam David, xi. Ellipsoid, 105.  random point on, 141.  Elliptic curve method, 402, 671. Elvenich, Hans-Michael, 409. Empirical distribution functions, 49. Empirical tests for randomness, 41, 61–80. Encoding a permutation, 65–66, 77–78, 145. Encoding secret messages, 193, 403–407,  415, 417.  Enflo, Per, 683. Engineering Research Associates, 208. Enhancing randomness, 26, 34. ENIAC computer, 54, 280. Entropy, 712. Enumerating binary trees, 527, 696, 723. Enumerating prime numbers, 382, 412, 416. Equality, approximate, 224, 233–235,  239, 242–243, 245.  essential, 233–235, 239–240, 242–244.  Equidistributed sequence, 150, 163,  177, 179–183.  Equidistribution test, 61, 74, 75. Equitable distribution, 181. Equivalent addition chains, 480, 484.   Eratosthenes of Cyrene  >EratosjŁnhc   cid:229  Kurhna cid:216 oc , 412.  Erdős, Pál  = Paul , 181, 384, 471, 696. ERH, see Extended Riemann hypothesis. ERNIE machine, 3–4. Error, absolute, 240, 309, 312–313. Error, relative, 222, 229, 232, 253, 255. Error estimation, 222, 229, 232, 253,  255, 309–310.  Espelid, Terje Oskar, 616. Essential equality, 233–235, 239–240,  242–244.  Estes, Dennis Ray, 671. Estrin, Gerald, 488. Euclid  E˛kle—dhc , 335–337. Euclid’s algorithm, 86, 99, 102, 117, 184,  288, 290, 304, 334–337, 340, 579.  analysis of, 356–379. compared to binary algorithm, 341. extended, 342–343, 354, 379, 435–436, 534. for polynomials, 424, 438–439. for polynomials, extended, 437, 458. for string polynomials, 426–428. generalized to the hilt, 426–428. multiprecision, 345–348, 373. original form, 335–336.  Eudoxus of Cnidus, son of Æschinus   E cid:214 doxoc A˚sq—nou  cid:229  Kn—dioc .  Euler, Leonhard  E cid:26 ler cid:127 , Leonard cid:127  =  cid:3  cid:26 ler, Leonard , xi, 357, 375, 377, 392, 407, 526, 649–651, 653, 655. constant γ, 359, 379, 726–727, 733. theorem, 20, 286, 548. totient function φ n , 19–20, 289,  369, 376, 583, 646. Eulerian numbers, 284. Evaluation: Computing the value.  of determinants, 498–500, 523–524. of mean and standard deviation, 232, 244. of monomials, 485, 697. of polynomials, 485–524. of powers, 461–485. Eve, James, 493, 517. Eventually periodic sequence, 7, 22,  375, 385.  Exact division, 439. Excess q exponent, 214–215, 227, 246. Exclusive or, 31, 32, 193, 419. Exercises, notes on, ix–xi. Exhaustive search, 103. Exponent overflow, 217, 221, 227, 231,  241, 243, 249.  Exponent part of a floating point number,  214–215, 246, 263, 283.  Exponent underflow, 217, 221–222, 227,  Exponential deviates, generating,  231, 241, 249.  132–133, 137.  Exponential distribution, 133, 137, 589. Exponential function, 313, 490, 533, 537.  INDEX AND GLOSSARY  745  Exponential sums, 84–85, 110–115, 181, Exponentiation: Raising to a power,  305, 382, 501. 461–485.  multiprecision, 463. of polynomials, 463–464. of power series, 526, 537, 719.  Extended arithmetic, 244–245, 639. Extended Euclidean algorithm, 342–343,  354, 379, 435–436, 534. for polynomials, 437, 458.  Extended Riemann Hypothesis,  395–396, 671.  465, 482, 485.  F -distribution, 135. Factor: A quantity being multiplied. Factor method of exponentiation, 463, Factorial number system, 66, 209. Factorial powers, 297, 515, 534, 643, 731. Factorials, 416, 622. Factorization: Discovering factors. of integers, 13–14, 175, 379–394, of polynomials, 439–461, 514. of polynomials mod p, 439–449, 455–456. of polynomials over the integers, 449–453. of polynomials over the rationals, 459. optimistic estimates of running time, 176. uniqueness of, 436.  396–403, 412–417.   , 143.  238, 253, 516.  FADD  floating add , 223–224, 227–228, Fagin, Barry Steven, 403, 632. Fallacious reasoning, 26, 74, 88, 95–96, 222. Falling powers, 297, 731. Fan, Chung Teh   Fast Fourier transform, 73, 306–310, 318, 502, 505, 512, 516, 521, 706, 710, 713–714. history of, 701. Fateman, Richard J, 463. Faure, Henri, 164. FCMP  floating compare , 223, 244. FDIV  floating divide , 223. Feijen, Wilhelmus  = Wim  Hendricus Ferguson, Donald Fraser, 280. Fermat, Pierre de, 386–388, 391, 407, 579.  Johannes, 636.  factorization method, 386–391, 412. numbers, 14, 386, 397, 403. theorems, 391, 413, 440, 579, 680.  filio Bonacii Pisano , 197, 208, 280.  Ferranti Mark I computer, 3, 192. Ferrenberg, Alan Milton, 188. FFT, 516, see Fast Fourier transform. Fibonacci, Leonardo, of Pisa  = Leonardo generator, 27, 34, 36, 37, 47, 52, 54, 92. number system, 209. numbers Fn: Elements of the Fibonacci numbers, table of, 728. sequence, 27, 37, 213, 264, 360, 468, sequence, lagged, 27–29, 35, 40, 72, 75,  483, 660, 666. 79–80, 146, 186–188, 193.  sequence, 731.   746  INDEX AND GLOSSARY  Field: An algebraic system admitting  addition, subtraction, multiplication, and division, 213, 331, 420, 422, 506, 525.  finite, 29–30, 449, 457, 554–555, 702.  Fike, Charles Theodore, 490. Finck, Pierre Joseph Étienne, 360. Findley, Josh Ryan, 409. Fine, Nathan Jacob, 90. Finetti, Bruno de, 566. Finite fields, 29–30, 449, 457, 554–555, 702. Finite Fourier transform, see Discrete  Fourier transform.  Finite sequences, random, 167–170, 178. Fischer, Michael John, 634. Fischer, Patrick Carl, 241. Fischlin, Roger, 669. Fisher, Ronald Aylmer, 145. Fishman, George Samuel, 108. FIX  convert to fixed point , 224. Fix-to-float conversion, 221, 223–224. Fixed point arithmetic, 214, 225–226,  308–310, 532.  Fixed slash arithmetic, 331–333, 379. Flajolet, Philippe Patrick Michel, 355,  366, 449, 541, 644.  Flammenkamp, Achim, 478, 483, 693. Flat distribution, see Uniform distribution. Flehinger, Betty Jeanne, 262. Float-to-fix conversion, 224–225, 228. Floating binary numbers, 214, 225,  Floating decimal numbers, 214, 226,  227, 254, 263.  254–264.  Floating hexadecimal numbers, 254, 263. Floating point arithmetic, 36, 188, 193,  196, 214–264, 292.  accuracy of, 222, 229–245, 253, 329,  438, 485.  addition, 215–220, 227–228, 230–231,  235–238, 253–254, 602.  addition, exact, 236. axioms, 230–231, 242–245. comparison, 233–235, 239, 242–243. decuple-precision, 283. division, 220–221, 230–231, 243. double-precision, 246–253, 278–279. hardware, 223–226. intervals, 228, 240–242, 244–245, 333, 613. mod, 228, 243, 244. multiplication, 220, 230–231, 243,  263–264.  multiplication, exact, 244. operators of MIX, 215, 223–225, 516. quadruple-precision, 253. reciprocal, 243, 245, 263. single-precision, 214–228. subtraction, 216, 230–231, 235–238,  245, 253, 556, 602. summation, 232, 244. triple-precision, 252. unnormalized, 238–240, 244, 327.  Floating point numbers, 196, 214–215,  222, 228, 246.  radix-b, excess-q, 214–215. statistical distribution, 253–264. two’s complement, 228.  245, 247, 490.  Floating point radix conversion, 326–329. Floating point trigonometric subroutines, Floating slash arithmetic, 331, 333. Floor function ⌊x⌋, 81, 732. FLOT  convert to floating point , 223. Floyd, Robert W, 7, 148, 280, 361, 505, 540. Fluhrer, Scott, 542. FMUL  floating multiply , 223, 516. Foata, Dominique Cyprien, 9. FOCS: Proceedings of the IEEE Symposia on Foundations of Computer Science  1975– , formerly called the Symposia on Switching Circuit Theory and Logic Design  1960–1965 , Symposia on Switching and Automata Theory  1966–1974 .  Forsythe, George Elmer, 4, 128. FORTRAN language, 188, 193, 279, 600, 602. Fourier, Jean Baptiste Joseph, 278.  division method, 278. series, 90, 487. transform, discrete, 169, 305–311, 316–318, 501–503, 506, 512, 516, 520–521, 524, 595. Fowler, Thomas, 208. Fractals, 206. Fraction overflow, 217, 254, 262, 264. Fraction part of a floating point number,  214–215, 246, 263.  distribution of, 254–264. Fractions: Numbers in [0 . . 1 , 36. conversion, 319–328. decimal, history, 197–198, 326. exponentiation, 483. random, see Uniform deviates. terminating, 328.  180, 182, 577.  420, 526. 291, 292, 630.  Fractions: Rational numbers, 330–333, Fraenkel, Aviezri S  LWPXT IXFRIA@ , 290, Franel, Jérome, 258. Franklin, Joel Nick, 149, 158, 159, 177, Franta, William Ray, 60. Fredricksen, Harold Marvin, 557. Free associative algebra, 437. Frequency function, see Density function. Frequency test, 61, 74, 75. Friedland, Paul, 613. Frieze, Alan Michael, 599. Fritz, Kurt von, 335. Frobenius, Ferdinand Georg, 539, 681, 689. automorphism, 689. Frye, Roger Edward, 538. FSUB  floating subtract , 223, 253. Fuchs, Aimé, 9. Fundamental theorem of arithmetic, Fuss, Paul Heinrich von  Fuss, Pavel  334, 422, 483. Nikolaeviq , 392, 651.   Gage, Paul Vincent, 409. Galambos, János, 661. Galois, Évariste, 449, 457. fields, see Finite fields. groups, 679, 681, 689, 690.  Gambling systems, 161. Gamma distribution, 133–134, 140. Gamma function, incomplete, 56, 59, 133. Ganz, Jürg Werner, 707. Gap test, 62–63, 74–76, 136, 158, 180. Gardner, Martin, 41, 200, 280, 592. Garner, Harvey Louis, 280, 290, 292. Gathen, Joachim Paul Rudolf von zur,  449, 611, 673, 687.  Gauß  = Gauss , Johann Friderich Carl   = Carl Friedrich , 20, 101, 363, 417, 422, 449, 578, 679, 685, 688, 701.  integers, 292, 345, 579. lemma about polynomials, 422–423, 682.  Gay, John, 1. gcd: Greatest common divisor. Gebhardt, Friedrich, 34. Geiger, Hans, counter, 7. Geiringer, Hilda, von Mises, 76. Generalized Dedekind sums, 83–92, 106. Generalized Riemann hypothesis, 396. Generating functions, 140, 147, 213, 261,  276–278, 525, 562–563, 679–680, 686, 695.  Generation of uniform deviates, 10–40,  184–189, 193.  Genuys, François, 280. Geometric distribution, 136, 137, 140, 585. Geometric mean, 283. Geometric series, 84, 307, 519, 700. Gerhardt, Carl Immanuel, 200. Gerke, Friedrich Clemens, 653. Gessel, Ira Martin, 723. Gibb, Allan, 242. Gilbert, William John, 607. Gill, Stanley, 226. Gimeno Fortea, Pedro, 187. GIMPS project, 409. Gioia, Anthony Alfred, 469. Girard, Albert, 424. Givens, James Wallace, Jr., 94. Glaser, Anton, 201. Globally nonrandom behavior, 51–52, 80. Goertzel, Gerald, 487. Goffinet, Daniel, 607. Goldbach, Christian, 392, 651. Goldberg, David Marc, 226. Golden ratio, 164, 283, 359, 360, 514,  652, 726–727, 733.  Goldreich, Oded  JIIXCLEB CCER , 179,  598, 669.  Goldschmidt, Robert Elliott, 312. Goldstein, Daniel John, 593. Goldstine, Herman Heine, 202, 278, 327. Goldwasser, Shafrira, 598. Golomb, Solomon Wolf, 147, 661, 711.  INDEX AND GLOSSARY  747   cid:103  cid:111  cid:121  cid:108  , 625.  Golub, Gene Howard, 562. Gonzalez, Teofilo, 60. Good, Irving John, 183. Goodman, Allan Sheldon, 108. Gosper, Ralph William, Jr., 101, 107, 117, 355, 375, 378, 540, 649. Gosset, William Sealy  = Student , t-distribution, 135. Goulard, Achille, 477. Gould, Henry Wadsworth, 725. Gourdon, Xavier Richard, 355, 449, 667. Goyal, Girish Kumar   cid:69  cid:103  cid:114  cid:70  cid:102   cid:107  cid:0   cid:109  cid:65  cid:114  Gradual underflow, 222. Gräffe, Carl Heinrich, 683. Graham, Ronald Lewis, 484, 608, 741. Gram, Jørgen Pedersen, 674. Gram–Schmidt orthogonalization process, 101, 674. Granville, Andrew James, 396, 659. Graph, 480–481, 485. Graphics, 284. Gray, Frank, binary code, 209, 568, 699. Gray, Herbert Lee Roy, 242. Gray levels, multiplication of, 284. Great Internet Mersenne Prime Search, 409. Greater than, definitely, 224, 233–235, Greatest common divisor, 330–356, 483.  239, 242–243.  335–337, 359.  348–356, 435. algorithm. 379, 656.  binary algorithms for, 338–341, Euclidean algorithm for, see Euclid’s multiprecision, 345–348, 354, 355, of n numbers, 341, 378. of polynomials, 424–439, 460, 453–455. within a unique factorization domain, 424. Greatest common right divisor, 437–438. Greedy algorithm, 293. Greek mathematics, 196–197, 225, Green, Bert Franklin, Jr., 27. Greenberger, Martin, 17, 88, 551. Greene, Daniel Hill, 659, 686, 697. Greenwood, Joseph Arthur, 33. Greenwood, Robert Ewing, 74. Gregory, Robert Todd, 657. GRH: The ERH for algebraic numbers, 396. Groote, Hans Friedrich de, 708. Grosswald, Emil, 90. Grotefeld, Andreas Friedrich Wilhelm, 656. Groups, 701. Galois, 679, 681, 689, 690. Grube, Andreas, 582. Grünwald, Vittorio, 204, 205. Guaranteed randomness, 35–36, 170–176. Guard digits, 227. Gudenberg, see Wolff von Gudenberg. Guessing, amplified, 172–174, 416–417. Guilloud, Jean, 280. Gustavson, Fred Gehrung, 721. Guy, Michael John Thirian, 623. Guy, Richard Kenneth, 402, 413.   748  INDEX AND GLOSSARY  Haber, Seymour, 164. Habicht, Walter, 435. Hadamard, Jacques Salomon, 382, 432.  inequality, 432, 436, 499. transform, 173, 502.  Hafner, James Lee, 661. Hajjar, Mansour   cid:142 n cid:128  cid:131   cid:142  cid:216  cid:156 —¸ , 29. Hajratwala, Nayan  ,yn "%Jrtv;l; , 409. HAKMEM, 540, 649. Halberstam, Heini, 664. Halewyn, Christopher Neil van, 403. Halliwell-Phillipps, James Orchard, 316. Halton, John Henry, 164. Halving, 328, 338, 339, 462.  continued fraction, 375. modular, 293.  Hamblin, Charles Leonard, 420. Hamlet, Prince of Denmark, v. Hammersley, John Michael, 189. Hamming, Richard Wesley, 255, 263. Handscomb, David Christopher, 189. Handy identities, 628–629. Hansen, Eldon Robert, 617. Hansen, Walter, 473, 475, 476, 479, 483. Hanson, Richard Joseph, 615. Haralambous, Yannis  Qaral cid:136 mpouc,  >Iw cid:136 nnhc , 764. Hard-core bit, 179. Hardware: Computer circuitry. algorithms suitable for, 228,  244  exercise 17 , 280, 312, 313–316, 322, 327–329, 338, 356, 461, 695. Hardy, Godfrey Harold, 382, 384, 653. Harmonic numbers Hn, 731.  fractional, 362, 729. table of, 728–729.  Harmonic probability, 264. Harmonic sums, 355. Harriot, Thomas, 199. Harris, Bernard, 541. Harris, Vincent Crockett, 341, 355. Harrison, Charles, Jr., 242. Hashing, 70, 148. Hassler, Hannes, 699. Håstad, Johan Torkel, 179, 417, 514, 599. Haynes, Charles Edmund, Jr., 108. hcf, see Greatest common divisor. Hebb, Kevin Ralph, 477. Heideman, Michael Thomas, 701. Heilbronn, Hans Arnold, 372, 377. Heindel, Lee Edward, 677. Hellman, Martin Edward, 406. Henrici, Peter Karl Eugen, 332, 505. Hensel, Kurt Wilhelm Sebastian, 213, 685. lemma, 38, 451, 454–455, 458, 685–686.  Hensley, Douglas Austin, 366, 373. Heringa, Jouke Reyn, 29. Hermelink, Heinrich, 208. Hermite, Charles, 115, 579. Herrmann, Hans Jürgen, 29. Hershberger, John Edward, 366.  Herzog, Thomas Nelson, 178, 594, 598. Hexadecimal digits, 195, 210. Hexadecimal number system, 201–202,  204, 210, 324, 639.  floating point, 254, 263. nomenclature for, 201.  Higham, Nicholas John, 242. Hilferty, Margaret Mary, 134. Hill, Ian David, 544. Hill, Theodore Preston, 262. himult, 15, 584. Hindu mathematics, 197, 208, 209, 281,  287, 343, 387, 461, 648.  HITACHI SR2201 computer, 280. Hitchcock, Frank Lauren, 506. Hlawka, Edmund, 117. HLT  halt , 222. Hobby, John Douglas, 764. Hoffmann, Immanuel Carl Volkmar, 279. Hofstadter, Douglas Richard, 330. Holte, John Myrom, 629. Homann, Karsten, 736. Homogeneous polynomial, 437, 458, 698. Hopcroft, John Edward, 500, 507, 699. Hörmann, Wolfgang, 138. Hörner, Horst Helmut, 118. Horner, William George, 486.  rule for polynomial evaluation, 486–489,  498, 504, 515, 517, 519.  Horowitz, Ellis, 505. Howard, John Vernon, 178. Howell, Thomas David, 708. Hoyle, Edmond, rules, 147. Huff, Darrell Burton, 42. Hull, Thomas Edward, 17. Hurwitz, Adolf, 345, 375, 376, 649. Huygens  = Huyghens , Christiaan, 655. Hyde, John Porter, 419. Hyperbolic tangent, 375. Hyperplanes, 96, 97, 116.  IBM 704 computer, 280. IBM 7090 computer, 280. IBM System 360 computers, 396–397, 614. IBM System 370 computers, 15. Ibn Ezra  = Ben Ezra , Abraham ben Meir  @XFR OA@ XI@N OA MDXA@ , also known as Ab¯u Ish. a¯q Ibr¯ahim al-M¯ajid  m cid:142  cid:145 «  cid:209 p  cid:139 ~n cid:204 ¿m ˝ cid:219  cid:211 m cid:143 pg „ cid:132  cid:147 g  cid:216 pc , 197.  Idempotent, 539, 694. Identity element, 418. IEEE standard floating point, 226, 246, 602. Ikebe, Yasuhiko   Ill-conditioned matrix, 292. Images, digitized, 284. Imaginary radix, 205–206, 209–210, 283. Impagliazzo, Russell Graham, 179. Improving randomness, 26, 34. IMSL: The International Mathematics and   , 252.  Statistics Library, 108. in situ transformation, 700.   Inclusion and exclusion principle, 354,  563, 610, 640, 678, 699.  Incomplete gamma function, 56, 59, 133. Increment in a linear congruential sequence,  10–11, 17, 22, 89, 97, 185.  Independence, algebraic, 496, 518. Independence, linear, 443–444, 508, 659–660. Independence of random numbers, 2, 43,  46, 55, 59, 66, 95, 240, 559, 562.  Index modulo p, 417. Indian mathematics, 197, 208, 209, 281,  287, 343, 387, 461, 648.  Induced functions, 535. Induction, mathematical, 336.  on the course of computation, 266,  269, 337.  Inductive assertions, 281–282. Infinite continued fractions, 358–359, 374. Infinity, representation of, 225, 244–245, 332. Inner product, 97, 499–501, 520. Integer, random,  among all positive integers, 257,  264, 342, 354.  in a bounded set, 119–121, 138, 185–186.  Integer solution to equations, 343–345,  354, 417, 449, 648.  Integer-valued distributions, 136–141. Integrated circuit module, 313. Integration, 153–154, 259. Interesting point, 642. Internet, iv, x. Interpolation, 297, 365, 503–505, 509,  516, 700, 721.  Interpretive routines, 226. Interval arithmetic, 228, 240–242,  244–245, 333, 613.  Inverse Fourier transform, 307, 316,  Inverse function, 121, see also Reversion  516, 633.  of power series.  Inverse matrix, 98, 331, 500, 524. Inverse modulo 2e, 213, 629. Inverse modulo m, 26, 354, 445, 456, 646. Inversive congruential sequence, 32–33, 40. Irrational numbers: Real numbers that  are not rational, 181, 359.  multiples of, mod 1, 164, 379, 622. transcendental, 378. Irrational radix, 209. Irrationality, quadratic, 358, 374–375,  397–401, 412, 415, 665.  Irreducible polynomial, 422, 435, 450,  456–457, 460.  Ishibashi, Yoshihiro   Islamic mathematics, 197, 280–281,   , 291.  326, 461–462.  Iteration of power series, 530–536, 723. Iterative n-source, 172. Iverson, Kenneth Eugene, 226.  INDEX AND GLOSSARY  749  Jabotinsky, Eri, 533, 536, 723. JACM: Journal of the ACM, a publication  of the Association for Computing Machinery since 1954.  Jacobi, Carl Gustav Jacob, 662.  symbol, 413–414, 415, 655, 662, 668.  JAE  jump A even , 339, 481. Jaeschke, Gerhard Paul Werner, 666. Jager, Hendrik, 665. Ja’Ja’  = JaJa , Joseph Farid   › cid:128 ‹~  cid:139  cid:218  cid:143 ‡  cid:181  cid:218  cid:144  cid:216 ~ , 514. Janssens, Frank, 107, 114. Jansson, Birger, 540, 553. JAO  jump A odd , 339, 612. Japanese mathematics, 648. Jayadeva, ¯Ac¯arya  a cid:65  cid:99  cid:65  cid:121  cid:13   cid:106  cid:121  cid:100  cid:3  cid:118  , 648. Jebelean, Tudor, 629. Jefferson, Thomas, 229. Jensen, Geraldine Afton, 466. Jensen, Johan Ludvig William Valdemar,  683.   , 340.  Jevons, William Stanley, 388. Jiuzhang Suanshu   Jöhnk, Max Detlev, 134. Johnson, Don Herrick, 701. Johnson, Jeremy Russell, 625. Johnson, Samuel, 229. Jokes, 3, 417. Jones, Hugh, 200, 326. Jones, Terence Gordon, 143. Jong, Lieuwe Sytse de, 515. Jonquières, Jean Philippe Ernest de Fauque  de, 465–466, 469, 477.  Jordaine, Joshua, 199. Judd, John Stephen, 394. Jurkat, Wolfgang Bernhard, 699. Justeson, John Stephen, 196. JXE  jump X even , 339. JXO  jump X odd , 219, 339. k-distributed sequence, 151–155, 168,  177, 179–182.  Kac, Mark, 384. Kahan, William Morton, 222, 226, 227,  241–245, 617.  summation formula, 615. Kaib, Michael Andreas, 578. Kaltofen, Erich Leo, 345, 449, 455, 672, 718. Kaminski, Michael, 712. Kanada, Yasumasa    , 280. Kankaala, Kari Veli Antero, 75, 570. Kannan, Ravindran   cid:133 ‚i cid:151  cid:133 h Kanner, Herbert, 327. Karatsuba, Anatolii Alekseevich  Karacuba,  xf~h , 599.  Anatoli cid:26  Alekseeviq , 295, 318, 420, 663.  Karlsruhe, University of, 242. Kátai, Imre, 607. Katz, Victor Joseph, 198. Kayal, Neeraj   cid:110  cid:70  cid:114  cid:106   cid:107  cid:121  cid:65  cid:108  , 396. Keir, Roy Alex, 237, 638.   750  INDEX AND GLOSSARY  Keller, Wilfrid, 664, 666. Kempner, Aubrey John, 204, 378. Kendall, Maurice George, 3, 74, 76. Kermack, William Ogilvy, 74. Kerr, Leslie Robert, 699. Kesner, Oliver, 226. Khinchin, Alexander Yakovlevich  Hinqin,  Aleksandr  cid:23 kovleviq , 356, 652.  Killingbeck, Lynn Carl, 103, 107. Kinderman, Albert John, 130–131, 135. Klarner, David Anthony, 213. Klem, Laura, 27. Knop, Robert Edward, 136. Knopfmacher, Arnold, 345, 686. Knopfmacher, John Peter Louis, 345. Knopp, Konrad Hermann Theodor, 364. Knorr, Wilbur Richard, 335. Knott, Cargill Gilston, 627. Knuth, Donald Ervin     , ii, iv, vii, 2, 4, 30, 89, 138, 145, 159, 189, 196, 205, 226, 242, 316, 335, 373, 378, 384, 435, 491, 584, 595, 599, 606, 636, 659, 661, 686, 694, 697, 722, 741, 764.  Knuth, Jennifer Sierra   Knuth, John Martin   Kohavi, Zvi  IAKEK IAV , 498. Koksma, Jurjen Ferdinand, 161. Kolmogorov, Andrei Nikolaevich   , xiv.   , xiv.   Kolmogorov, Andre cid:26  Nikolaeviq , 56, 169, 178, 183.  Kolmogorov–Smirnov distribution, 57–60.  table, 51.  Kolmogorov–Smirnov test, 48–60. Kondo, Shigeru   Kontorovich, Alex Vladimir  Kontoroviq,   , 280.  Aleksandr Vladimiroviq , 584.  Koons, Florence, 327. Kornerup, Peter, 332–333, 629, 657. Korobov, Nikolai Mikhailovich  Korobov,  Nikola cid:26  Miha cid:26 loviq , 114, 159, 177.  Kovács, Béla, 607. Kraïtchik, Maurice Borisovitch  Kra cid:26 qik,  Meer Borisoviq , 396, 407.  Krandick, Werner, 625, 629. Krishnamurthy, Edayathumangalam  Venkataraman  G} cid:132 Wg˙ cid:131 ax cid:135 k T cid:136 ax} cid:133 W cid:131 h  cid:144 ˝q~ºmg cid:151  , 278, 279. Kronecker, Leopold, 450, 678, 688, 730. Kruskal, Martin David, 542. KS test, see Kolmogorov–Smirnov test. Kuczma, Marek, 533. Kuipers, Lauwerens, 114, 177. Kulisch, Ulrich Walter Heinz, 242, 245. Kung, Hsiang Tsung   529–530, 533, 720.   , 356,  Kurita, Yoshiharu   Kurowski, Scott James, 409. Kut.t.aka   cid:107  cid:0   cid:107  , 287, 343. Kuz’min, Rodion Osievich  Kuz~min,  Rodion Osieviq , 363.  l0-chain, 479, 483, 485. L3 algorithm, 118, 417, 453. La Touche, Maria Price, 194, 230. La Vallée Poussin, Charles Jean Gustave  Nicolas de, 381.  Laderman, Julian David, 700. Lagarias, Jeffrey Clark, 416, 599, 667. Lagged Fibonacci sequences, 27–29, 35, 40,  72, 75, 79–80, 146, 186–188, 193. Lagny, Thomas Fantet de, 279, 360. Lagrange  = de la Grange , Joseph Louis, Comte, 375, 378, 456, 503, 527, 533, 549, 649, 653, 655.  interpolation formula, 503–505. inversion formula, 527–528, 533–534, 723.  Lags, 28. Lake, George Thomas, 327. Lakshman, Yagati Narayana  h  cid:215  cid:128  S cid:130  X cid:129  f cid:129  e cid:128  S e cid:128  G cid:128  „ , 455. Lalanne, Léon Louis Chrétien, 208. Lalescu, Gheorghe Liviu, 186. Lamé, Gabriel, 360. Landau, Edmund Georg Hermann, 621, 683. Laplace  = de la Place , Pierre Simon,  Lapko, Olga Georgievna  Lapko, Ol~ga  Marquis de, 363.  Georgievna , 764.  Large prime numbers, 407–412, 549–550,  663–664.  Las Vegas algorithms: Computational  methods that use random numbers and always produce the correct answer if they terminate, 447–449, 459, 681–682.  Lattice of points, 97. Lattice reduction, see Short vectors. Laughlin, Harry Hamilton, 279. Laurent, Paul Mathieu Hermann, series, 723. Lauwerier, Hendrik Adolf, 561. Lavaux, Michel, 107. Lavington, Simon Hugh, 3. Lawrence, Frederick William, 390. lcm: Least common multiple. Leading coefficient, 418, 451–452, 454. Leading digit, 195, 239. Leading zeros, 222, 238–240, 327. Least common left multiple, 437–438. Least common multiple, 18, 23, 292, 334,  337, 353, 411, 483, 641.  Least remainder algorithm, 376. Least significant digit, 195. Lebesgue, Henri Léon, measure, 160,  166–167, 178, 367.  Lebesgue  = Le Besgue , Victor Amédée,   , 29, 572, 604.  L’Ecuyer, Pierre, 108, 179, 546, 582,  341, 662.  584, 603.  Leeb, Hannes, 604. Leeuwen, Jan van, 477, 515, 706. Left multiple, least common, 437–438.   Legendre  = Le Gendre , Adrien Marie,  326–327, 381, 396, 449.  symbol, 414.  Léger, Émile, 360. Léger, Roger, 587. Lehman, Russell Sherman, 387, 405. Lehmer, Derrick Henry, 10–11, 47, 54, 149, 278, 345–346, 382, 390, 391, 394, 396, 409, 413, 414, 484, 655, 660, 667, 686.  Lehmer, Derrick Norman, 278, 661. Lehmer, Emma Markovna Trotskaia, 391. Lehn, Jürgen, 32, 558. Leibniz, Gottfried Wilhelm, Freiherr  Lempel, Abraham, 556, 712. Lenstra, Arjen Klaas, 118, 403, 417,  von, 200.  453, 712.  Lenstra, Hendrik Willem, Jr., 118, 396,  402–403, 416, 417, 453, 656. Leonardo Pisano, see Fibonacci. Leong, Benton Lau   Leslie, John, 208. Less than, definitely, 224, 233–235,   , 485.  239, 242–243.  Leva, Joseph Leon, 132. Levene, Howard, 74. LeVeque, William Judson, 648. Levin, Leonid Anatolievich  Levin, Leonid  Anatol~eviq , 36, 170, 179.  Levine, Kenneth Allan, 104. Lévy, Paul, 363. Levy, Silvio Vieira Ferreira, vii. Lewis, John Gregg, 615. Lewis, Peter Adrian Walter, 108, 701. Lewis, Theodore Gyle, 32. Lexicographic order, 207, 624. li: Logarithmic integral function. Li, Ming   Li Yan   Lickteig, Thomas Michael, 706. Lindholm, James H., 79. Linear congruential sequence, 10–26,   , 287.   , 179.  145–146, 184–186, 193.  choice of increment, 10–11, 17, 22,  89, 97, 185.  choice of modulus, 12–16, 23, 184. choice of multiplier, 16–26, 88–89,  105–109, 184–185.  choice of seed, 17, 20, 143, 184. period length, 16–23. subsequence of, 11, 73.  Linear equations, 291–292.  integer solution to, 343–345, 354.  Linear factors mod p, 455. Linear iterative array, 313–317, 328. Linear lists, 279, 281, 283. Linear operators, 363–366, 376. Linear probing, 592. Linear recurrences, 29–32, 409–411, 695.  mod m, 37–40.  INDEX AND GLOSSARY  751  Linearly independent vectors, 443–444,  508, 659–660.  Linked memory, 279, 281, 283, 419. Linking automaton, 311. Linnainmaa, Seppo Ilmari, 242, 244, 718. Liouville, Joseph, 378. Lipton, Richard Jay, 497, 675, 697. Liquid measure, 199. Little Fermat computer, 311. Littlewood, John Edensor, 382. LLL algorithm, 118, 417, 453. Local arithmetic, 200. Locally nonrandom behavior, 46,  51–52, 152, 168.  Lochs, Gustav, 372–373. Loewenthal, Dan  LHPAL OC , 291. Logarithm, 279, 313.  discrete, 417. of a matrix, 536. of a power series, 533, 537. of a uniform deviate, 133. of ϕ, 283.  Logarithmic integral, 381–382, 414, 663. Logarithmic law of leading digits,  254–264, 282, 404.  Logarithmic sums, 628–629. Logical operations, see Boolean operations. Löh, Günter, 666. lomult, 15. Long division, 270–275, 278–279. Loos, Rüdiger Georg Konrad, 435, 674. Lotti, Grazia, 500, 715. Lovász, László, 118, 417, 453. Lovelace, Augusta Ada Byron King,  Countess of, 189.  Loveland, Donald William, 178, 179, 183. Lubiw, Anna, 656. Lubkin, Samuel, 327. Luby, Michael George, 179. Lucas, François Édouard Anatole, 391,  407, 409, 413, 414.  numbers Ln, 695. Lukes, Richard Francis, 390. Lund, Carsten, 593. Lüscher, Martin, 35, 72, 109, 188,  550, 556, 571.  Luther, Herbert Adesla, 278.  m-ary method of exponention, 464, 466,  470–471, 481–482.  Ma, Keju   Machine language versus higher-level   , 673.  languages, 16, 185.  MacLaren, Malcolm Donald, 33, 47,  128, 551, 585.  MacMahon, Percy Alexander, 609. MacMillan, Donald Bashford, 226. MacPherson, Robert Duncan, 114. MacSorley, Olin Lowe, 280. Maeder, Roman Erich, 627, 635. Mahler, Kurt, 180.  measure, 683.   752  INDEX AND GLOSSARY  Maiorana, James Anthony, 557. Mairan, Jean-Jacques d’Ortous de, 537. Makarov, Oleg Mikhailovich  Makarov,  Oleg Miha cid:26 loviq , 700, 714.  Mallows, Colin Lingwood, 74. Manasse, Mark Steven, 403. Manchester University Computer, 192. Mandelbrot, Benoît Baruch, 606. Mangoldt, Hans Carl Friedrich von, 663.  function, 371, 376.  MANIAC III computer, 242. Mansour, Yishay  XEVPN IYI , 316. Mantel, Willem, 552. Mantissa, 214, see Fraction part. Marczyński, Romuald Władysław, 205. Mariage, Aimé, 201. Mark I computer  Ferranti , 3. Mark II Calculator  Harvard , 225. Marsaglia, George, 3, 23, 29, 33, 40, 47,  62, 71, 72, 75, 78, 108, 114–115, 119, 122, 123, 128, 133–135, 179, 544, 546–547, 549, 551, 565, 588.  Martin, Monroe Harnish, 32, 38, 40. Martin-Löf, Per Erik Rutger, 169–170, 178. Masking, 322, 328–329, 389–390, 671. Math. Comp.: Mathematics of Computation  1960–  , a publication of the American Mathematical Society since 1965; founded by the National Research Council of the National Academy of Sciences under the original title Mathematical Tables and Other Aids to Computation  1943–1959 .  Mathematical aesthetics, 289. Matias, Yossi  Q@IHN IQEI , 121. Matrix: A rectangular array, 486.  characteristic polynomial, 499, 524. determinant, 356, 373, 432, 434,  498–500, 523–524.  greatest common right divisor, 438. inverse, 98, 331, 500, 524. multiplication, 499–501, 506–507,  516, 520–523, 699.  null space, 443–444, 456, 659–660, 681. permanent, 499, 515–516. rank, 443–444, 506, 508, 520. semidefinite, 586. singular, 98, 116, 513, 520. triangularization, 444, 659–660, 677. Matrix  Bush , Irving Joshua, 41, 280. Matsumoto, Makoto   Matthew, Saint   cid:147 Agioc Matja cid:216 oc   , 29, 572, 604.   cid:229  E˛aggelist cid:160 c , 735.  Matula, David William, 210, 211, 329,  332–333, 379.  Mauchly, John William, 27. Maupertuis, Pierre-Louis Moreau de, 537. Maximum of random deviates, 122. Maximum-of-t test, 52, 54, 59, 70, 75,  77, 122, 158, 180.  Maya Indians, 196.  Mayer, Dieter Heinz-Jörg, 366. Maze, Gérard, 645. McCarthy, Daniel Patrick, 696. McClellan, Michael Terence, 292. McCracken, Daniel Delbert, 226. McCurley, Kevin Snow, 661, 671. McEliece, Robert James, 687. McKendrick, Anderson Gray, 74. Mean, evaluation of, 232, 244. Measure, units of, 198–199, 201, 209,  255, 326, 327.  Measure theory, 160, 166–167, 178, 367. Mediant rounding, 331–332, 379. Meissel, Daniel Friedrich Ernst, 667. Mellin, Robert Hjalmar, transforms,  355, 644.  Mendelsohn, Nathan Saul, 211. Mendès France, Michel, 649, 656. Mental arithmetic, 279, 295. Merit, figure of, 105. Mersenne, Marin, 391, 407.  multiplication, 294. numbers, 14, 409. primes, 185, 409, 412, 413.   cid:111  cid:112  cid:113  cid:114  cid:115  cid:116  cid:117  cid:113 , iv, vi, 764.  constant, 659.  Mertens, Franz Carl Joseph, 641, 659.  METAPOST, vii, 764. Metrology, 201. Metropolis, Nicholas Constantine   Mhtr cid:236 polhc, Nik cid:236 laoc Kwnstant—nou , 4, 189, 240, 242, 327.  Metze, Gernot, 280. Meyer, Albert Ronald da Silva, 634. Micali, Silvio, 179, 598. Michigan, University of, 242, 617. Middle-square method, 3–4, 7–8, 27, 36, 75. Midpoint, 244. Mignotte, Maurice, 450, 683. Mihăilescu, Preda-Mihai, 396. Mikami, Yoshio   Mikusiński, Jan, 378. Miller, Gary Lee, 395–396. Miller, James  = Jimmy  Milton, 108. Miller, Jeffrey Charles Percy, 695. Miller, Kenneth William, 108. Miller, Victor Saul, 416. Miller, Webb Colby, 485. Milne-Thompson, Louis Melville, 505. Minimizing a quadratic form, 98–101,   , 340, 486, 648.  105, 115–118.  Minimum polynomial, 711. Minkowski, Hermann, 579. Minus zero, 202, 244–245, 249, 268, 274. MIP-years, 176, 405. Miranker, Willard Lee, 242. Mises, Richard, Edler von, 149, 177, 494. Mitchell, Gerard Joseph Francis Xavier,  27, 32.   MIX computer, vi, 209.  binary version, 202–204, 339,  floating point attachment, 215,  389–390, 481.  223–225, 516.  Mixed congruential method, 11, see Linear  congruential sequence.  Mixed-radix number systems, 66, 199,  208–211, 290, 293, 505.  addition and subtraction, 209, 281. balanced, 103, 293, 631. comparison, 290. counting by 1s, 103. multiplication and division, 209. radix conversion, 327.  Mixture of distribution functions,  123–124, 138.  Möbius, August Ferdinand, function,  354, 376, 456, 459.  inversion formula, 456, 652.  mod, 228, 421, 544, 734. mod m arithmetic,  addition, 12, 15, 203, 287–288. division, 354, 445, 499; see also Inverse  modulo m. halving, 293. multiplication, 12–16, 284, 287–288,  294, 318, 663.  on polynomial coefficients, 418–420. square root, 406–407, 415, 456–457,  681–682.  subtraction, 15, 186, 203, 287–288.  Model V computer, 225. Modular arithmetic, 284–294, 302–305,  450, 454, 499.  complex, 292.  453, 460.  10–16, 23, 184.  Modular method for polynomial gcd,  Modulus in a linear congruential sequence,  Moenck, Robert Thomas, 449, 505. Moews, David John, 593. Moivre, Abraham de, 537. Møller, Ole, 242. Monahan, John Francis, 130, 131, 135. Monic polynomial, 418, 420, 421, 425,  435, 452, 457, 518.  Monier, Louis Marcel Gino, 414, 662. Monkey tests, 75. Monomials, evaluation of, 485, 697. Monotonicity, 230, 243. Monte Carlo, 2, 29, 55, 114, 185, 189. Monte Carlo method: Any computational  method that uses random numbers  possibly not producing a correct answer ; see also Las Vegas algorithms, Randomized algorithms.  Montgomery, Hugh Lowell, 683. Montgomery, Peter Lawrence, 284, 322. multiplication mod m, 284, 386, 396.  Moore, Donald Philip, 27, 32.  INDEX AND GLOSSARY  753  Moore, Louis Robert, III, 108. Moore, Ramon Edgar, 242. Moore School of Electrical Engineering,  208, 225.  Morain, François, 390. Morgenstern, Jacques, 524. Morley, Frank Vigor, 199. Morris, Robert, 613. Morrison, Michael Allan, 396, 400, 660. Morse, Harrison Reed, III, 192. Morse, Samuel Finley Breese, code, 377. Moses, Joel, 454–455. Most significant digit, 195. Motzkin, Theodor  = Theodore  Samuel  OIWVEN L@ENY XECE@IZ , 378, 490, 494, 495, 497, 518, 519, 705.  Muddle-square method, 36, 174–176, 179. Muller, Mervin Edgar, 122, 143. Multinomial coefficients, 539. Multinomial theorem, 722. Multiple-precision arithmetic, 58, 202,  265–318, 419, 486.  addition, 266–267, 276–278, 281, 283. comparison, 281. division, 270–275, 278–279, 282–283,  311–313.  greatest common divisor, 345–348,  354, 355, 379, 656.  multiplication, 268–270, 283, 294–318. radix conversion, 326, 328. subtraction, 267–268, 276, 281, 283.  Multiple-precision constants, 352, 362, 366,  384, 659, 663, 712, 726–728.  Multiples, 422. Multiples of an irrational number mod 1,  164, 379, 622.  Multiplication, 194, 207–208, 265, 294, 462. complex, 205, 307–310, 487, 506, 519, 706. double-precision, 249–250, 252, 295. fast  asymptotically , 294–318. floating point, 220, 230–231, 243, 263–264. fractions, 282, 330. matrix, 499–501, 506–507, 516,  520–523, 699. Mersenne, 294. mixed-radix, 209. mod m, 12–16, 284, 287–288, 294,  318, 663.  mod u x , 446. modular, 285–286, 302–305. multiprecision, 268–270, 283, 294–318. multiprecision by single-precision, 281. polynomial, 418–420, 508, 512, 521,  712, 713.  power series, 525. two’s complement, 608.  Multiplicative congruential method, 11,  19–23, 185–186.  Multiplier in a linear congruential sequence, 10–11, 16–26, 88–89, 105–109, 184–185.  Multiply-and-add algorithm, 268, 313.   754  INDEX AND GLOSSARY  Multiprecision: Multiple-precision or  Arbitrary precision.  Multiprimality: Total number of prime  factors, 384.  Multisets, 170, 473, 483.  operations on, 483, 694–695. terminological discussion, 694.  Multivariate polynomials, 418–419,  422, 455, 518.  chains, 497–498, 514. factors, 458. noncommutative, 436. roots of, 436.  Munro, James Ian, 515, 706. Musical notation, 198. Musinski, Jean Elisabeth Abramson, 507. Musser, David Rea, 278, 453, 455.  N-source, 170. Nadler, Morton, 627. Nance, Richard Earle, 189. Nandi, Salil Kumar  sill k m;r nNdI , 278. NaNs, 245, 246, 639. Napier, John, Laird of Merchiston, 194, 200. N¯ar¯ayan. a Pan. d. ita, son of Nr.si ˙mha Native American mathematics, 196. Needham, Noel Joseph Terence Montgomery    cid:110  cid:65  cid:114  cid:65  cid:121  cid:90   cid:112  cid:69  cid:23  cid:88  cid:116 ,  cid:110  cid:2   cid:69  cid:115  cid:92  cid:104  cid:45  cid:121   cid:112  cid:0   cid:47  cid:44  , 387.      , 287.  Negabinary number system, 204–205,  209–210, 212, 328.  Negacyclic convolution, 521. Negadecimal number system, 204, 210. Negative binomial distribution, 140. Negative digits, 207–213, 696. Negative numbers, representation of,  202–205, 275–276.  Negative radices, 204–205, 209–210,  Neighborhood of a floating point  212, 328.  number, 234.  Neugebauer, Otto Eduard, 196, 225. Neumann, John von  = Margittai Neumann  János , 1, 3–4, 26, 36, 119, 125, 128, 138, 140, 202, 226, 278, 327.  Newcomb, Simon, 255. Newman, Donald Joseph, 697. Newton, Isaac, 449, 486, 698, 701.  interpolation formula, 503–505, 516. method for rootfinding, 278–279, 312,  486, 529, 629, 719.  Ni, Wen-Chun   Nicomachus of Gerasa  Nik cid:236 maqoc   , 121.   cid:229   cid:226 k Ger cid:136 swn , 659.  Niederreiter, Harald Günther, 106–107, 109,  113–115, 117, 161, 177, 584.  Nijenhuis, Albert, 146. Nine Chapters on Arithmetic, 340. Nines, casting out, 289, 303, 324. Nines’ complement notation, 203, 210.  Nisan, Noam  OQIP MREP , 316. Niven, Ivan Morton, 155–156. Nonary  radix 9  number system, 200, 637. Noncommutative multiplication, 436–438,  500, 507, 672, 684.  Nonconstructive proofs, 286, 289, 583. Nonnegative: Zero or positive. Nonsingular matrix: A matrix with nonzero  determinant, 98, 116, 513, 520.  Norm of a polynomial, 457–458. Normal deviates: Random numbers with the  normal distribution, 122–132, 142.  dependent, 132, 139. direct generation, 141. square of, 134.  tail of, 139. variations, 132, 139.  Normal distribution, 56, 122, 384, 565.  Normal evaluation schemes, 506, 709–710. Normal numbers, 177. Normalization of divisors, 272–273, 282–283. Normalization of floating point numbers,  215–217, 227–228, 238, 248–249, 254, 616.  Normand, Jean-Marie, 29. Norton, Graham Hilton, 373, 673. Norton, Karl Kenneth, 383. Norton, Victor Thane, Jr., 607. Notations, index to, 730–734. Nowak, Martin R., 409. Nozaki, Akihiro   NP-complete problems, 499, 514, 585, 698. Null space of a matrix, 443–444, 456,   , 524.  659–660, 681.  Number field sieve, 403, 671. Number fields, 331, 333, 345, 403, 674. Number sentences, 605. Number system: A language for representing  numbers.  balanced binary, 213. balanced decimal, 211. balanced mixed-radix, 103, 293, 631. balanced ternary, 207–208, 209,  227, 283, 353.  419, 461, 483.  binary  radix 2 , 195, 198–206, 209–213,  combinatorial, 209. complex, 205–206, 209–210, 292. decimal  = denary, radix ten , 197–199,  210, 320–326, 374.  duodecimal  radix twelve , 199–200. factorial, 66, 209. Fibonacci, 209. floating point, 196, 214–215, 222, 228, 246. hexadecimal  radix sixteen , 201–202,  204, 210, 324, 639.  mixed-radix, 66, 199, 208–211, 290,  293, 505. modular, 284–285. negabinary  radix −2 , 204–205, 209–210, 212, 328. negadecimal, 204, 210.   nonary  radix 9 , 200, 637. octal  = octonary = octonal, radix 8 ,  194, 200–202, 210, 228, 323–325, 328, 481, 727.  p-adic, 213, 605, 632, 685. phi, 209. positional, 151, 166–167, 177, 195–213,  319–329.  primitive tribal, 195, 198. quater-imaginary  radix 2i , 205,  209–210, 283.  quaternary  radix 4 , 195, 200. quinary  radix 5 , 195, 200, 213. rational, 330, 420. regular continued fraction, 346, 358–359,  368, 374–379, 412, 415, 665.  reversing binary, 212. revolving binary, 212. senary  radix 6 , 200. senidenary  = hexadecimal , 202. septenary  radix 7 , 200. sexagesimal  radix sixty , 196–200,  225, 326.  slash, 331–333, 379. ternary  radix 3 , 195, 200, 204, 213, 328. vigesimal  radix twenty , 196.  Numerical instability, 245, 292, 485,  Nunes  = Nuñez Salaciense = Nonius ,  489, 490.  Pedro, 424.  Nussbaumer, Henri Jean, 521, 710. Nystrom, John William, 201.  Octal  radix 8  number system, 194, 200–202, 210, 228, 323–325, 328, 481, 727.  Octavation, 326. Odd-even method, 128–130, 139. Odlyzko, Andrew Michael, 416, 541,  608, 667, 671.  OFLO, 218. Oldham, Jeffrey David, vii. Oliver, Ariadne, 725. Oliveira e Silva, Tomás António Mendes,  Olivos Aravena, Jorge Augusto Octavio,  386, 667.  485, 698.  One-way function, 172, 179. Ones’ complement notation, 12, 203–204,  275–276, 279, 288, 544.  Online algorithms, 318, 525–526, 720. Operands: Quantities that are operated on,  such as u and v in the calculation of u + v.  Ophelia, daughter of Polonius, v. Optimum methods of computation,  see Complexity.  OR  bitwise or , 140, 686, 695. Order of a modulo m, 20–23, 391–392. Order of an element in a field, 457. Order of magnitude zero, 239.  INDEX AND GLOSSARY  755  Order statistics, 135. Ordered hash table, 592. Organ-pipe order, 378. Oriented binary tree, 692. Oriented tree, 9, 464–465, 481–482. Ostrowski, Alexander Markus, 494. Oughtred, William, 225, 326. Overflow, 12–13, 252, 267, 293, 332,  exponent, 217, 221, 227, 231, 241,  543, 639.  243, 249.  fraction, 217, 254, 262, 264. rounding, 217, 220, 222, 224, 227–228.  Overstreet, Claude Lee, Jr., 189. Owen, John, 1. Owings, James Claggett, Jr., 178. Ozawa, Kazufumi    , 615.  p-adic numbers, 213, 605, 632, 685. Packing, 109. Padé, Henri Eugène, 357, 534. Padegs, Andris, 226. Pairwise independence, 183, 668–669. Palindromes, 415. Palmer, John Franklin, 222. Pan, Victor Yakovlevich  Pan, Viktor   cid:23 kovleviq , 490, 492, 497, 500, 505, 507, 515, 517, 519, 521, 677, 699, 703, 705, 706, 714, 715, 721.  Panario Rodríguez, Daniel Nelson, 449. Pandu Rangan, Chandrasekaran  Papadimitriou, Christos Harilaos   zi cid:151  cid:133 Uzx cid:133 h  cid:130 Wf¯ cid:133 axh , 717.  Papadhmhtr—ou, Qr—stoc Qaril cid:136 ou , 697.  Pappus of Alexandria  P cid:136 ppoc   cid:229  >Alexandrin cid:236 c , 225.  Paradox, 257. Parallel computation, 286, 317, 488, 503. Parameter multiplications, 518, 524. Parameter step, 494, 518. Pardo, see Trabb Pardo. Park, Stephen Kent, 108. Parlett, Beresford Neill, 194. Parry, William, 209. Partial derivatives, 524. Partial fraction expansion, 85, 510, 685. Partial ordering, 694. Partial quotients, 87, 106, 117, 346, 359,  367–369, 379, 656.  distribution of, 362–369, 665.  Partition test, 63–64, 74, 158. Partitions of a set, 64, 722. Partitions of an integer, 79, 146. Pascal, Blaise, 199. Pascal-SC language, 242. Patashnik, Oren, 741. Paterson, Michael Stewart, 519, 634, 707. Patience, 190. Patterson, Cameron Douglas, 390. Paul, Nicholas John, 128. Pawlak, Zdzisław, 205, 627.   756  INDEX AND GLOSSARY  Payne, William Harris, 32. Paz, Azaria  FT DIXFR , 498. Peano, Giuseppe, 201. Pearson, Karl, 55, 56. Peirce, Charles Santiago Sanders, 538. Pemantle, Robin Alexander, 542. Penk, Michael Alexander, 646. Penney, Walter Francis, 206. Pentium computer chip, 280, 409. Percentage points, 44, 46, 51, 70–71, 383. Percival, Colin Andrew, 632. Perfect numbers, 407. Perfect squares, 387–388. Period in a sequence, 7–9.  length of, 4, 16–23, 37–40, 95.  Periodic continued fraction, 375, 415. Permanent, 499, 515–516. Permutation: An ordered arrangement  mapped to integers, 65–66, 77–78, 145. random, 145–148, 384, 460, 679.  Permutation test, 65–66, 77–78, 80–81,  of a set.  91, 154.  Perron, Oskar, 356, 460, 690. Persian mathematics, 197, 326, 462. Pervushin, Ivan Mikheevich  Pervuxin,  Ivan Miheeviq , 407.  Pethő, Attila, 607. Petkovšek, Marko, 608. Petr, Karel, 442. Pfeiffer, John Edward, 192. Phalen, Harold Romaine, 200. Phi  ϕ , 164, 209, 283, 359, 360, 514,  652, 726–727, 733.  Phillips, Ernest William, 201–202. Pi  π , 41, 151, 158, 161, 198, 200, 209,  279–280, 284, 358, 726–727, 733.  as “random” example, 21, 25, 33, 47, 52,  89, 103, 106, 108, 184, 238, 243, 252, 324–325, 555, 593, 599, 665.  Picutti, Ettore, 412. Pigeonhole principle, 286. Pi ˙ngala, ¯Ac¯arya  a cid:65  cid:99  cid:65  cid:121  cid:13   cid:69  cid:112 ŋg cid:108  , 461. Pipeline, 283. Pippenger, Nicholas John, 481, 697. Piras, Francesco, 683. Pitfalls of random number generation,  6, 29, 88, 188–189.  Pitteway, Michael Lloyd Victor, 653. Places, 265. Planck, Karl Ernst Ludwig Marx  = Max ,  constant, 214, 227, 238, 240.  Plauger, Phillip James, 327. Playwriting, 190–192. Plouffe, Simon, 284. PM system, 420. Pocklington, Henry Cabourn, 414, 681. Pointer machine, 311, 317, 634. Poirot, Hercule, 725. Poisson, Siméon Denis, distribution, 55,  137–138, 140, 141, 538, 570.  Poker test, 63–64, 74, 158. Polar coordinates, 56, 59, 123. Polar method, 122–123, 125, 135. Pollard, John Michael, 306, 385–386,  402–403, 413, 417, 658, 711.  Pólya, György  = George , 65, 569. Polynomial, 418–420, 486.  addition, 418–420. arithmetic modulo m, 37–40,  419–420, 464.  degree of, 418, 420, 436. derivative of, 439, 489, 524, 537. discriminant of, 674, 686. distribution function, 138. division, 420–439, 487, 534. evaluation, 485–524. factorization, 439–461, 514. greatest common divisor, 424–439,  interpolation, 297, 365, 503–505, 509,  460, 453–455.  516, 700, 721.  irreducible, 422, 435, 450, 456–457, 460. leading coefficient, 418, 451–452, 454. monic, 418, 420, 421, 425, 435, 452,  multiplication, 418–420, 508, 512,  457, 518.  521, 712, 713.  multivariate, 418–419, 422, 455, 518. norms, 457–458. over a field, 420–425, 435, 439–449,  455–459.  over a unique factorization domain,  421–439, 449–461. primitive, 422, 436. primitive modulo p, 30–32, 422. primitive part, 423–425. random, 435, 448, 455, 459. remainder sequence, 427–429, 438,  455, 721.  resultant, 433, 674, 690. reverse of, 435, 452, 673, 721. roots of, 23, 434, 436, 483, 493. sparse, 455, 672. squarefree, 439, 456, 459. string, 436–438. subtraction, 418–420.  Polynomial chains, 494–498, 517–524. Pomerance, Carl, 396, 402, 659. Poorten, Alfred Jacobus van der, 656. Pope, Alexander, 88. Pope, David Alexander, 278. Popper, Karl Raimund, 178. Portable random number generators,  185–188, 193.  Porter, John William, 372. Positional representation of numbers, 151,  166–167, 177, 195–213, 319–329.  Positive definite quadratic form, 98, 115. Positive operator, 365. Positive semidefinite matrix, 586.   Potency, 24–26, 36, 47, 52, 73, 83, 87–88,  Probability: Ratio of occurrence, 150,  92, 105, 184.    Power matrix, 534–536. Power series: A sum of the form  k≥0 akzk, see Generating functions.  manipulation of, 525–537.  Power tree, 464, 481. Poweroids, 534–536, 722. Powers, Donald  = Don  Michael, 312. Powers, evaluation of, 461–485.  multiprecision, 463. polynomial, 463–464. power series, 526, 537, 719.  Powers, Ralph Ernest, 396, 407. pp: Primitive part, 423–425. Pr: Probability, 150, 152, 168, 179–180,  257, 264, 472, 734.  Pratt, Vaughan Ronald, 356, 413. Precision: The number of digits in a  representation.  double, 246–253, 278–279, 295. multiple, 58, 202, 265–318, 419, 486. quadruple, 253, 295. single: fitting in one computer word, 215. unlimited, 279, 283, 331, 416, see also  Multiple-precision.  Preconditioning, see Adaptation. Prediction tests, 171, 183. Preston, Richard McCann, 280. Primality testing, 380, 391–396,  409–414, 549.  Prime chains, 415, 666. Prime numbers: Integers greater than unity  having no proper divisors, 380.  distribution of, 381–382, 405. enumeration of, 381–382, 416. factorization into, 334. largest known, 407–412. Mersenne, 185, 409, 412, 413. size of mth, 665. useful, 291, 405, 407–408, 549–550, 711. useless, 415. verifying primality of, 380, 391–396,  409–414, 549.  421–422.  Primes in a unique factorization domain,  Primitive element modulo m, 20–23. Primitive notations for numbers, 195, 198. Primitive part of a polynomial, 423–425. Primitive polynomial, 422, 436. Primitive polynomial modulo p, 30–32, 422. Primitive recursive function, 166. Primitive root: A primitive element in a finite field, 20–23, 185, 391, 417, 456, 457.  Priority sampling, 148. Pritchard, Paul Andrew, 631. Probabilistic algorithms, see Randomized  algorithms.  INDEX AND GLOSSARY  757  177, 257.  abuse of, 433. over the integers, 150, 152, 257, 264, 472.  Probert, Robert Lorne, 699. Programming languages, 16, 185, 222. Pronouncing hexadecimal numbers, 201. Proof of algorithms, 281–282, 336–337, 592. Proofs, constructive versus nonconstructive,  286, 289, 583, 630.  Proper factor of v: A factor that is neither  a unit nor a unit multiple of v.  Proth, François Toussaint, 663. Proulx, René, 179. Pseudo-division of polynomials, 425–426,  Pseudorandom sequences, 4, 170–176, 179. Ptolemy, Claudius  Ptolema cid:216 oc Kla cid:212 dioc ,  435–436.  197.  Public key cryptography, 406. Purdom, Paul Walton, Jr., 541. Pyke, Ronald, 566.  q-series, 536. Quadratic congruences, solving, 417. Quadratic congruential sequences, 26–27, 37. Quadratic forms, 98, 521.  minimizing, over the integers, 98–101,  105, 115–118.  Quadratic irrationalities, continued  fractions for, 358, 374–375, 397–401, 412, 415, 665.  Quadratic reciprocity law, 393, 411,  414, 663.  Quadratic residues, 415, 697. Quadratic sieve method, 402. Quadruple-precision arithmetic, 253, 295. Quandalle, Philippe, 710. Quasirandom numbers, 4, 189. Quater-imaginary number system, 205,  209–210, 283.  Quaternary number system, 195, 200. Quick, Jonathan Horatio, 77, 147. Quinary number system, 195, 200, 213. Quolynomial chains, 498, 524, 704–705. Quotient: ⌊u v⌋, 265, see Division. of polynomials, 420–421, 425–426, 534. partial, 87, 106, 117, 346, 359, 362–369,  379, 656, 665.  trial, 270–272, 278, 282.  Rabin, Michael Oser  OIAX XFER L@KIN , 175,  396, 406, 413, 415, 448, 449, 707.  Rabinowitz, Philip, 279. Rackoff, Charles Weill, 179. Rademacher, Hans, 90, 91. Radioactive decay, 7, 132, 137. Radix: Base of positional notation, 195.  complex, 205–206, 209–210. irrational, 209. mixed, 66, 199, 208–211, 290, 293, 505. negative, 204–205, 209–210, 212, 328.   Radix conversion, 200, 204, 205, 207,  Range arithmetic, 228, 240–242, 244–245,  758  INDEX AND GLOSSARY  210, 319–329, 486, 489.  floating point, 326–329. multiprecision, 326, 328.  Radix point, 10, 185, 195, 204, 209, 214, 319. Raimi, Ralph Alexis, 257, 262. Raleigh  = Ralegh , Walter, 199. Rall, Louis Baker, 240, 242. Ramage, John Gerow, 135. Ramanujan Iyengar, Srinivasa   cid:255  cid:152  cid:136 W cid:138   cid:133 W cid:131 W¨{h I cid:132 axWm , 662. Ramaswami, Vammi   cid:136 k cid:155   cid:133 W cid:131 zW cid:155  , 383. Ramshaw, Lyle Harold, 164, 181. ran array, 186–188, 193. RAND Corporation, 3. Randell, Brian, 202, 225. Random bits, 12, 30–32, 35–36, 38, 48,  119–120, 170–176.  Random combinations, 142–148. Random directions, 135–136. Random fractions, 10, see Uniform deviates. Random functions, 4–9, 385. Random integers,  among all positive integers, 257,  264, 342, 354.  in a bounded set, 2–3, 6–7, 119–121,  138, 162–163, 185–186.  Random mappings, 4–9, 385, 657–658. Random number generators, 1–193. for nonuniform deviates, 119–148. for uniform deviates, 10–40, 184–189, 193. machines, 3, 404. summary, 184–193. tables, 3. testing, 41–118. using, 1–2, 119–148, 664, see also  Randomized algorithms.  Random permutations, 145–148, 384,  460, 679.  of a random combination, 148.  Random point, in a circle, 123.  in a sphere, 136. on an ellipsoid, 141. on a sphere, 135.  Random polynomials, 435, 448, 455, 459. Random random number generators,  6–9, 26.  Random real numbers, 255, 263. Random samples, 142–148. Random sequences, meaning of, 2, 149–183.  finite, 167–176, 178–179, 183.  Random walk test, 34, 79. Randomized algorithms: Algorithms that  use random numbers and usually produce a correct answer, 1–2, 171, 395–396, 401–402, 413–417, 436, 447–449, 453, 459, 669, 688.  Randomness, guaranteed, 35–36, 170–176. RANDU, 26, 107, 188, 551. Rangan, see Pandu Rangan.  333, 613.  Rank, of apparition, 410–411.  of a matrix, 443–444, 506, 508, 520. of a tensor, 506, 508, 514, 520–524.  RANLUX, 109. Rap music, 3. Rapoport, Anatol, 541. Ratio method, 130–132, 133, 140. Rational arithmetic, 69, 330–333,  Rational function approximation,  427–428, 526.  438–439, 534.  Rational functions, 420, 498, 518.  approximation and interpolation,  438–439, 505, 534.  Rational numbers, 330, 420, 459.  approximation by, 331–332, 378–379, 617. mod m, 379. polynomials over, 428, 459. positional representation of, 16,  211, 213, 328.  Rational reconstruction, 379. Real numbers, 420. Real time, 286. Realization of a tensor, 507. Reciprocal differences, 505. Reciprocals, 278–279, 312–313, 421.  floating point, 243, 245, 263. mod 2e, 213, 629. mod m, 26, 213, 354, 445, 456, 646. power series, 537.  Reciprocity laws, 84, 90, 393, 414. Recorde, Robert, xi, 280–281. Rectangle-wedge-tail method, 123–128, 139. Rectangular distribution, see Uniform  distribution.  Recurrence relations, 10, 26–33, 37–40,  260–261, 295, 301–302, 313, 318, 351, 362, 386, 409–411, 442, 525, 634, 687, 695, 714.  Recursive processes, 253, 295, 299–303,  318, 419, 500, 531, 689, 713. Reeds, James Alexander, III, 599. Rees, David, 39, 169. Registers, 491. Regular continued fractions, 346, 358–359,  368, 374–379, 412, 415, 665.  Rehkopf, Donald Caspar, 41. Reiser, John Fredrick, 28, 39, 242. Reitwiesner, George Walter, 213, 280. Rejection method, 125–126, 128–129,  134, 138, 139, 591.  Relative error, 222, 229, 232, 253, 255. Relatively prime: Having no common prime factors, 11, 19, 286, 330, 332, 342, 354.  polynomials, 422, 436, 454.  Remainder: Dividend minus quotient times divisor, 265, 272–273, 420–421, 425–426, 437, 534, see also mod.  Replicative law, 90.   Representation of numbers, see Number  Rounding overflow, 217, 220, 222,  systems.  Representation of trees, 482. Representation of ∞, 225, 244–245, 332. Reservoir sampling, 143–144, 147, 148. Residue arithmetic, 284–294, 302–305,  450, 454, 499.  Result set, 494, 517. Resultant of polynomials, 433, 674, 690. Revah, Ludmila, 706. Reverse of a polynomial, 435, 452, 673, 721. Reversing binary number system, 212. Reversion of power series, 527–530, 533–536. Revolving binary number system, 212. Rezucha, Ivan, 143. Rhind papyrus, 462. Rho method for factoring, 384–386,  393–394, 413.  Riccati, Jacopo Francesco, equation, 650. Rieger, Georg Johann, 653. Riemann, Georg Friedrich Bernhard,  83, 382, 414.  hypothesis, 382, 663. hypothesis, generalized 395–396, 671. integration, 153–154, 259.  Riffle shuffles, 145, 147. Right divisor, greatest common, 437–438. Ring with identity, commutative, 418. Riordan, John, 542. Rising powers, 534, 731. Ritzmann, Peter, 721. Rivat, Joël, 667. Rivest, Ronald Linn, 403, 405, 707. Robber, 190–192. Robbins, David Peter, 593. Robinson, Donald Wilford, 554. Robinson, Julia Bowman, 666. Robinson, Raphael Mitchel, 664, 711. Roepstorff, Gert, 366. Rolletschek, Heinrich Franz, 9, 345. Roman numerals, 195, 209. Romani, Francesco, 500, 715. Roof, Raymond Bradley, 115. Roots of a polynomial, 23, 434, 483, 493.  multivariate, 436.  Roots of unity, 84, 531–532, 700; see  also Cyclotomic polynomials, Exponential sums.  Rosińska, Izabela Grażyna, 198. Ross, Douglas Taylor, 192. Rotenberg, Aubey, 11, 47. Rothe, Heinrich August, 535. Rouché, Eugène, theorem, 690. Roulette, 2, 10, 55. Round to even, 237, 241, 245. Round to odd, 237. Rounding, 102, 207, 217, 222, 223,  230–231, 236–237.  mediant, 331–332, 379.  Rounding errors, 232, 242, 698, 718.  INDEX AND GLOSSARY  759  224, 227–228.  Rozier, Charles Preston, 324. RSA box, 404, 406. RSA encryption, 403–407, 415, 629, 669. Rudolff, Christof, 198. Ruler function ρ n , 540. Run test, 63, 66–69, 74–77, 158, 180. Runs above  or below  the mean, 63. Runs in a permutation, 66, 74, 76. Russian peasant method, 462. Ruzsa, Imre Zoltán, 213. Ryser, Herbert John, 515, 699.  198, 461.  $N , 170. Saarinen, Jukka Pentti Päiviö, 75. Sachau, Karl Eduard, 461. Saddle point method, 568. Sahni, Sartaj Kumar, 60. Saidan, Ahmad Salim   cid:209 m cid:139  cid:218  cid:144  ˝¿n cid:147   cid:139  cid:204  cid:131 c , Salamin, Eugene, 283. Salfi, Robert, 145. Samelson, Klaus, 241–242, 327. Samet, Paul Alexander, 321. Sampling  without replacement , 1, 142–148. Sands, Arthur David, 610. Saunders, Benjamin David, 455. Savage, John Edmund, 707. Sawtooth function   x  , 81–82, 90–91. Saxe, James Benjamin, 141. Saxena, Nitin   cid:69  cid:110  cid:69  cid:116  cid:110   cid:115  cid:63  cid:115  cid:3  cid:110  cid:65  , 396. Scarborough, James Blaine, 241. Schatte, Peter, 262, 622. Schelling, Hermann von, 65. Schmid, Larry Philip, 73. Schmidt, Erhard, 101, 674. Schmidt, Wolfgang M, 183. Schnorr, Claus-Peter, 118, 179, 414, 417,  497, 578, 664, 669.  Scholz, Arnold, 478. Scholz–Brauer conjecture, 478–479, 485. Schönemann, Theodor, 457, 685. Schönhage, Arnold, 292, 302–303, 305, 306, 311, 317, 328, 470, 484, 500, 522, 629, 638, 656, 672, 696, 715.  Schönhage–Strassen algorithm, 306–311, 317. Schooling, William, 627. Schreyer, Helmut, 202. Schröder, Friedrich Wilhelm Karl Ernst, 531.  function, 531–532, 724.  Schroeppel, Richard Crabtree, 399, 400, 671. Schubert, Friedrich Theodor von, 450. Schwartz, Jacob Theodore, 674, 675. Schwarz  = Švarc , Štefan, 442. Schwenter, Daniel, 654. Secrest, Don, 279, 327. Secret keys, 193, 403–407, 415, 417, 505. Secure communications, 2, 403–407, 415. Sedgewick, Robert, 540.   760  INDEX AND GLOSSARY  Seed  starting value , 143, 146, 170,  187–188, 193, 550, 590.  in a linear congruential sequence,  10, 17, 20, 184.  Seidenberg, Abraham, 198. Selection sampling, 142–143, 146. Selenius, Clas-Olof, 648. Self-reproducing numbers, 6, 293–294, 540. Selfridge, John Lewis, 394, 396, 412, 665. Semi-online algorithm, 529. Semigroup, 539. Seneschal, David, 589. Septenary  radix 7  number system, 200. Serial correlation coefficient, 77. Serial correlation test, 72–74, 91, 83,  154, 182.  Serial test, 39, 60, 62, 74, 75, 78, 95,  106, 109–115, 158.  Seroussi Blusztein, Gadiel  IQEXQ L@ICB , 712. Serret, Joseph Alfred, 374, 449, 579. Sethi, Ravi, 485. SETUN computer, 208. Sexagesimal number system, 196–200,  225, 326.  Seysen, Martin, 118. Shafer, Michael William, 409. Shakespeare  = Shakspere , William, v. Shallit, Jeffrey Outlaw, 360, 378, 380, 390,  395–396, 645, 646, 656, 663, 689. Shamir, Adi  XINY ICR , 403, 405, 416,  505, 599, 669.  Shand, Mark Alexander, 629. Shanks, Daniel Charles, 280, 379, 681–682. Shanks, William, 279–280. Shannon, Claude Elwood, Jr., 211. Shaw, Mary Margaret, 489, 498, 515. Shen, Kangshen   Sheriff, 190–192. Shibata, Akihiko   Shift operators of MIX, 339. Shift register recurrences, 27–32, 36–40,   , 280.   , 287.  186–188, 193.  Shift-symmetric N-source, 172, 183. Shirley, John William, 199. Shokrollahi, Mohammad Amin   cid:220  cid:212 ¿m cid:143 … cid:151   cid:209  cid:219 ¸m  cid:139  cid:204  cid:132 ¸ , 515. Short vectors, 98–101, 118. Shoup, Victor John, 449, 687. Shub, Michael Ira, 36. Shuffled digits, 141. Shuffling a sequence, 33–36, 38, 39. Shuffling cards, 145–147. Shukla, Kripa Shankar   cid:107  cid:2   cid:112  cid:65   cid:102  cid:21  cid:107  cid:114   , 133. Sibuya, Masaaki   SICOMP: SIAM Journal on Computing,   cid:102  cid:0   cid:63  cid:108  cid:65  , 208, 648.  published by the Society for Industrial and Applied Mathematics since 1972.  Sideways addition, 463, 466. Sierpiński, Wacław Franciszek, 666. Sieve methods, 389–391, 402–403, 412.  Sieve  k cid:236 skinon  of Eratosthenes, 412,  416, 667.  Sieveking, Malte, 720. Signatures, digital, 406. Signed magnitude representation, 202–203,  209–210, 247, 266.  ixkd;r , 327.   cid:69  cid:115  cid:21  cid:104  , 343, 461.  Significant digits, 195, 229, 238. Sikdar, Kripasindhu  k cid:128 p;is·  Silverman, Joseph Hillel, 402. Simplex, recursively subdivided, 567. Simulation, 1. Sinclair, Alistair, 699. Sine, 490. Singh, Avadhesh Narayan  a cid:118  cid:68  cid:3  cid:102   cid:110  cid:65  cid:114  cid:65  cid:121  cid:90  Singh, Parmanand   cid:112  cid:114  cid:109  cid:65  cid:110  cid:6  cid:100   cid:69  cid:115  cid:21  cid:104  , 387. Sink vertex, 480. SKRZAT 1 computer, 205. Slash arithmetic, 331–333, 379. SLB  shift left rAX binary , 339, 340. Slide rule, 225. Sloane, Neil James Alexander, 109. Slowinski, David Allen, 409. Small step, 467. Smirnov, Nikolai Vasilievich  Smirnov,  Nikola cid:26  Vasil~eviq , 57. Smith, David Eugene, 197, 198. Smith, David Michael, 275, 279. Smith, Edson McIntyre, 409. Smith, Henry John Stephen, 646. Smith, James Everett Keith, 27. Smith, Robert LeRoy, 228. Sobol, Ilya Meerovich  Sobol~, Il~ cid:31  Meeroviq , 541. SODA: Proceedings of the ACM–SIAM Symposia on Discrete Algorithms, inaugurated in 1990.  Soden, Walter, 323. Solitaire, 190. Solomonoff, Ray Joseph, 178. Solovay, Robert Martin, 396, 414. Sorenson, Jonathan Paul, 646. Sorted uniform deviates, 57, 71, 135,  137, 141.  Source vertex, 480. Sowey, Eric Richard, 189. Space-filling curves, 495. Spacings, 71, 78–79, 181. Sparse polynomials, 455, 672. Specht, Wilhelm, 683. Species of measure zero, 179. Spectral test, 30, 35, 93–118, 169, 184.  algorithm for, 101–104. examples, 105–109. generalized, 108, 117.  Spence, Gordon McDonald, 409. Spencer Brown, David John, 695. Sphere, n-dimensional, 56.  random point in, 136. random point on, 135. volume of, 105.   Spherical coordinates, 59. SQRT box, 175, 406–407, 415. Square root, 122, 213, 283, 374–375,  397–398, 483.  modulo m, 406–407, 415. modulo p, 456–457, 681–682. of power series, 526, 537. of uniform deviate, 122.  Squarefree factorization, 460. Squarefree polynomials, 439, 456, 459. Squares, sum of two, 579–580. Squeamish ossifrage, 417. Squeeze method, 125–126, 147. SRB  shift right rAX binary , 339, 340, 481. Stability of polynomial evaluation,  485, 489, 490.  Stack: Linear list with last-in-first-out  growth pattern, 299–301.  Stahnke, Wayne Lee, 31. Standard deviation, evaluation of, 232, 244. Stanley, Richard Peter, 594. Star chains, 467, 473–477, 480, 482. Star step, 467. Stark, Richard Harlan, 226. Starting value in a linear congruential  sequence, 10, 17, 20, 184.  Statistical tests, 171, see Testing. Steele, Guy Lewis, Jr., 635–636, 638. Ştefănescu, Doru, 450. Steffensen, Johan Frederik, 722. Stegun, Irene Anne, 44. Stein, Josef, 338. Stein, Marvin Leonard, 278. Stern, Moritz Abraham, 654. Stern–Brocot tree, 378, 656. Stevin, Simon, 198, 424. Stibitz, George Roberto, 202, 225. Stillingfleet, Edward, 537. Stirling, James, 537. approximation, 59. numbers, 64–65, 298, 534–535, 542,  STOC: Proceedings of the ACM  680, 732. Symposia on Theory of Computing, inaugurated in 1969.  Stockmeyer, Larry Joseph, 519, 634, 707. Stoneham, Richard George, 115. Stoppard, Tom  = Straussler, Tomas , 61. Storage modification machines, 311. Strachey, Christopher, 192. Straight-line program, 494. Strassen, Volker, 306, 311, 317, 396, 414,  497, 500, 507, 521, 523, 656, 708, 718.  Straus, Ernst Gabor, 378, 485. Strindmo, Odd Magnar, 409. String polynomials, 436–438. Stringent tests, 75. Stroud, Arthur Howard, 279, 327. Struve, Wassilij Wassiliewitsch  Struve,  Vasili cid:26  Vasil~eviq , 462.  INDEX AND GLOSSARY  761  Student  = William Sealy Gosset ,  t-distribution, 135.  Sturm, Jacques Charles François, 434,  438, 674. Subbarao, Mathukumalli Venkata   cid:128 e  cid:128 V cid:135 ‚  cid:128 e – cid:222   cid:141 k cid:9  cid:128 G Q  cid:128 n cid:135 c cid:130 « cid:129 g k cid:128  cid:210  , 469.  Subexponential  nice  functions, 694. Subnormal floating point numbers, 246. Subresultant algorithm, 428–436, 438, 455. Subsequence rules, 161–162, 168–169,  177–178, 182.  Subsequence tests, 73, 158. Subsequences, 40, 193. Subset FORTRAN language, 600. Subtract-and-shift cycle, 338, 348. Subtract-with-borrow sequence, 23, 35,  72, 75, 108, 193, 546.  Subtraction, 194, 207, 213, 265,  267–268, 281.  complex, 487. continued fractions, 649. double-precision, 247–249. floating point, 216, 230–231, 235–238,  245, 253, 556, 602.  fractions, 330–331. mod m, 15, 186, 203, 287–288. modular, 285–286. multiprecision, 267–268, 276, 281, 283. polynomial, 418–420. power series, 525.  39–40, 186–188, 193.  Subtractive random number generator, Sugunamma, Mantri   cid:131 i cid:151  cid:157  ´ cid:192 ~k cid:131 W , Sukhatme, Pandurang Vasudeo   cid:112  cid:65  cid:21  cid:119  cid:0   cid:114  cid:21  cid:103  Sum of periodic sequences, mod m,   cid:118  cid:65  cid:115  cid:0   cid:100  cid:3  cid:118   cid:115  cid:0   cid:75  cid:65  cid:40  cid:109  cid:3  , 568.  469.  35, 38, 78, 108.  Summation by parts, 643. Sun Tsˇu  = S¯unzˇı, Master Sun      ,  280, 287.  Sun SPARCstation, 764. Suokonautio, Vilho, 279. Svoboda, Antonín, 282, 292. Swarztrauber, Paul Noble, 634. Swedenborg, Emanuel, 200. Sweeney, Dura Warren, 253, 379. Swinnerton-Dyer, Henry Peter Francis, 681. Sýkora, Ondrej, 700. Sylvester, James Joseph, matrix, 433,  436, 674.  Szabó, József, 607. Szabó, Nicholas Sigismund, 291, 292. Szekeres, György  = George , 570. Szymanski, Thomas Gregory, 540. t-ary trees, 723. T. abarı, Moh. ammed ben Ayy¯ub Tables of fundamental constants,    cid:217  cid:143 q£ o cid:216  cid:218 c  cid:209 p  cid:139  cid:204  cid:132 ¸ , 208. 358–359, 726–729.  Tabulating polynomial values, 488, 515. Tague, Berkley Arnold, 419.    ,  762  INDEX AND GLOSSARY  292.  Tail of a floating point number, 235. Tail of the binomial distribution, 167. Tail of the normal distribution, 139. Takahashi, Daisuke    , 280.  , 291. Takahasi, Hidetosi   Tamura, Yoshiaki    , 280. Tanaka, Richard Isamu    Tangent, 376. tanh, 375. Tannery, Jules, 241. Taranto, Donald Howard, 327, 635. Tarski  Tajtelbaum , Alfred, 718. Tate, John Torrence, Jr., 402. Tate, Stephen Ralph, 309. Taussky Todd, Olga, 35, 106. Tausworthe, Robert Clem, 31. Taylor, Alfred Bower, 201. Taylor, Brook, theorem, 489. Taylor, William Johnson, 504. Television script, 190–192. Ten’s complement notation, 203, 210. Tensors, 506–514, 520–524. Term: A quantity being added. Terminating fractions, 328. Ternary number system, 195, 200,  204, 213, 328.  balanced, 207–208, 209, 227, 283, 353.  Testing for randomness, 41–118.  a priori tests, 80. chi-square test, 42–47, 53–56, 58–60. collision test, 70–71, 74, 158. coupon collector’s test, 63–65, 74,  76, 158, 180.  empirical tests, 41, 61–80. equidistribution test, 61, 74, 75. frequency test, 61, 74, 75. gap test, 62–63, 74–76, 136, 158, 180. Kolmogorov–Smirnov test, 48–60. maximum-of-t test, 52, 54, 59, 70, 75,  77, 122, 158, 180.  partition test, 63–64, 74, 158. permutation test, 65–66, 77–78,  80–81, 91, 154.  run test, 63, 66–69, 74–77, 158, 180. serial correlation test, 72–74, 91,  83, 154, 182.  serial test, 39, 60, 62, 74, 75, 78, 95,  106, 109–115, 158.  spectral test, 30, 35, 93–118, 169, 184. subsequence tests, 73, 158. theoretical tests, 41–42, 80–93. torture test, 79.  TEX, iv, vi, 764. Tezuka, Shu   Thacher, Henry Clarke, Jr., 529. Theoretical tests for randomness,   , 164, 189, 546, 584.  41–42, 80–93.  Thiele, Thorvald Nicolai, 505. Thompson, John Eric Sidney, 196. Thomson, William Ettrick, 3, 11, 22.  Thorup, Mikkel, 593. Thurber, Edward Gerrish, 466, 470,  477, 478.  Tichy, Robert Franz, 161. Tienari, Martti Johannes, 279. Tingey, Fred Hollis, 57. Tippett, Leonard Henry Caleb, 3. Tiwari, Prasoon  pr cid:115  cid:1  cid:110   cid:69  cid:116  cid:118  cid:65  cid:114  cid:70  , 316. Tobey, Robert George, 677. Tocher, Keith Douglas, 588. Todd, John, 35. Todd, Olga Taussky, 35, 106. Toeplitz, Otto, system, 721. Tonal System, 201. Tonelli, Alberto, 682. Toolkit philosophy, 487. Toom, Andrei Leonovich  Toom, Andre cid:26   Leonoviq , 296, 299, 306.  Toom–Cook algorithm, 299–302,  316–317, 672.  Topological sorting, 480. Topuzoˇglu, Alev, 558. Torelli, Gabriele, 535. Torres y Quevedo, Leonardo de, 225. Torture test, 79. Totient function φ n , 19–20, 289, 369,  376, 583, 646.  Touchy-feely mathematics, 466, 477. Trabb Pardo, Luis Isidoro, 661. Trace of a field element, 687. Trager, Barry Marshall, 455, 689. Trailing digit, 195. Transcendental numbers, 378. Transitive permutation groups, 679. Transpose of a tensor, 507, 512–513. Transpositions, 147. Traub, Joseph Frederick, 138, 348, 428, 489,  498, 505, 515, 531–534, 719.  Trees: Branching information structures,  413.  binary, 378, 527, 696, 723. complete binary, 667. enumeration of, 527, 696, 723. oriented, 9, 464–465, 481–482. t-ary, 723.  Trevisan, Vilmar, 452, 461. Trial quotients, 270–272, 278, 282. Triangularization of matrices, 444,  659–660, 677.  Tries, 687. Trigonometric functions, 279, 313, 490. Trilinear representation of tensors, 521–522. Trinomials, 32, 40, 572. Triple-precision floating point, 252. Trits, 207. Truncation: Suppression of trailing digits,  207, 237–238, 309.  Tsang, Wai Wan   Tsu Ch’ung-Chih  = Zˇu Ch¯ongzh¯ı    , 72.      , 198. Tsuji, Masatsugu     , 264.   Uniform distribution, 2, 10, 48, 61, 119,  function, 371, 376.  Tukey, John Wilder, 701. Turán, Pál  = Paul , 372, 649. Turing, Alan Mathison, 3, 599.  machines, 169, 499, 634.  Twindragon fractal, 206, 210, 606. Two squares, sum of, 579–580. Two’s complement notation, 15, 188,  203–204, 228, 275–276, 608. Twos’ complement notation, 204. Tydeman, Frederick John, 638.  Ulam, Stanisław Marcin, 138, 140, 189. Ullman, Jeffrey David, 694. Ullrich, Christian, 242. Ulp, 232–233. Underflow, exponent, 217, 221–222,  227, 231, 241, 249.  gradual, 222.  Ungar, Peter, 706. Uniform deviates: Random numbers with  the uniform distribution, 138. generating, 10–40, 184–189, 193. logarithm of, 133. sorted, 57, 71, 135, 137, 141. square root of, 122.  121, 124, 263.  Unimodular matrix, 524. Unique factorization domain, 421–424, 436. Units in a unique factorization domain,  421–422, 435.  Unity: The number one, 336.  roots of, 84, 531–532, 700; see  also Cyclotomic polynomials, Exponential sums.  Unlimited precision, 279, 283, 331, 416,  see also Multiple-precision.  Unnormalized floating point arithmetic,  238–240, 244, 327.  Unusual correspondence, 9. Useful primes, 291, 405, 407–408,  549–550, 711.  Uspensky, James Victor, 278.  Vahlen, Karl Theodor, 653. Valach, Miroslav, 292. Valiant, Leslie Gabriel, 499. Vallée, Brigitte, 352, 355, 366, 644, 645. Vallée Poussin, Charles Jean Gustave  Nicolas de la, 381. Valtat, Raymond, 202. van Ceulen, Ludolph, 198. van de Wiele, Jean-Paul, 497, 707. van der Corput, Johannes Gualtherus,  163–164, 181.  van der Poorten, Alfred Jacobus, 656. van der Waerden, Bartel Leendert, 196,  433, 518, 690.  van Halewyn, Christopher Neil, 403. van Leeuwen, Jan, 477, 515, 706. Van Loan, Charles Francis, 562, 701.  INDEX AND GLOSSARY  763  van Wijngaarden, Adriaan, 242. Vari, Thomas Michael, 717. Variables, 418, 486. Variance, unbiased estimate of, 232. Variance-ratio distribution, 135. Vassilevska Williams, Virginia Panayotova  Vasilevska, Virgini cid:31  Pana cid:26 otova , 717.  Vattulainen, Ilpo Tapio, 75, 570. Vaughan, Robert Charles, 451. Velthuis, Frans Jozef, 764. Veltkamp, Gerhard Willem, 616. Vertex cover, 485. Vetter, Herbert Dieter Ekkehart, 629, 656. Viète, François, 198. Ville, Jean André, 597. Vitányi, Pál Mihály  = Paul Michael   Béla, 179.  Vitter, Jeffrey Scott   Vogel, Otto Hermann Kurt, 341. Voltaire, de  = Arouet, François   , 121, 146.  Marie , 200.  Volume of sphere, 105. von Fritz, Kurt, 335. von Mangoldt, Hans Carl Friedrich, 663.  von Mises, Richard, Edler, 149, 177, 494. von Neumann, John  = Margittai Neumann  János , 1, 3–4, 26, 36, 119, 125, 128, 138, 140, 202, 226, 278, 327.  von Schelling, Hermann, 65. von Schubert, Friedrich Theodor, 450. von zur Gathen, Joachim Paul Rudolf,  449, 611, 673, 687.  Vowels, Robin Anthony, 637. Vuillemin, Jean Etienne, 629, 649.  Wadel, Louis Burnett, 205. Wadey, Walter Geoffrey, 226, 242. Waerden, Bartel Leendert van der, 196,  433, 518, 690.  Waiting time, 119, 136. Wakulicz, Andrzej, 205, 627. Wald, Abraham  = Ábrahám , 163,  177–178.  sequence, 164–165.  Wales, Francis Herbert, 194, 202. Walfisz, Arnold, 382. Walker, Alastair John, 120, 127, 139. Wall, Donald Dines, 553. Wall, Hubert Stanley, 356. Wallace, Christopher Stewart, 132,  141, 316, 590.  Wallis, John, 199, 655. Walsh, Joseph Leonard, 502. Wang, Paul Shyh-Horng    455, 460–461, 657, 689.  Ward, Morgan, 554. Waring, Edward, 503. Warlimont, Richard Clemens, 686. Watanabe, Masatoshi     , 764.   , 452,   Waterman, Alan Gaisford, 40, 106–107,  XOR  bitwise exclusive-or , 31, 32, 193, 419.  764  INDEX AND GLOSSARY  116, 144, 554, 596.  Wattel, Evert, 466. Weather, 74. Wedge-shaped distributions, 125–126. Weigel, Erhard, 199. Weighing problem, 208. Weights and measures, 198–199, 201,  209, 255, 326, 327.  Weinberger, Peter Jay, 415, 678. Welch, Peter Dunbar, 701. Welford, Barry Payne, 232. Weyl, Claus Hugo Hermann, 181,  379, 382, 596.  Wheeler, David John, 226. White, Jon L  = Lyle , 635–636, 638. White sequence, 182. Whiteside, Derek Thomas, 486, 701. Whitworth, William Allen, 566, 568. Wichmann, Brian Anderson, 544. Wiedijk, Frederik, 665. Wiele, Jean-Paul van de, 497, 707. Wijngaarden, Adriaan van, 242. Wilf, Herbert Saul, 146. Wilkes, Maurice Vincent, 201, 226. Wilkinson, James Hardy, 241, 499. Williams, Hugh Cowie, 380, 390, 394,  401, 412, 415, 661, 664. Williams, John Hayden, 541. Williams, Virginia Panayotova Vassilevska  Virgini cid:31  Pana cid:26 otova Vasilevska , 717.  Williamson, Dorothy, 115. Wilson, Edwin Bidwell, 134. Winograd, Shmuel, 280, 316, 500, 501,  507, 509, 512–514, 520, 523, 705–707, 710, 712, 714.  Wirsing, Eduard, 363, 366, 376. WM1  word size minus one , 252, 267, 613. Wolf, Thomas Howard, 192. Wolff von Gudenberg, Jürgen Freiherr, 242. Wolfowitz, Jacob, 69, 74. Woltman, George Frederick, 409. Wood, William Wayne, 115. Word length: Logarithm of word size. Word size, 12–16, 265, 276. Wrench, John William, Jr., 280, 379,  627, 728.  Wright, Edward Maitland, 384, 653. Wunderlich, Charles Marvin, 390,  394, 399–400.  Wynn, Peter, 356, 613. Wynn-Williams, Charles Eryl, 202.  Yagati, see Lakshman. Yaglom, Akiva Moiseevich   cid:23 glom, Akiva  Moiseeviq , 622.  Yaglom, Isaak Moiseevich   cid:23 glom, Isaak  Moiseeviq , 622.  Yao, Andrew Chi-Chih     , 138, 170,  179, 316, 378, 484, 485, 540.   , 484.  Yao, Frances Foong Chu   Yates, Frank, 145, 173, 501–502. Yee, Alexander Jih-Hing   Yohe, James Michael, 612. Young, Jeffery Stagg, 664. Younis, Saed Ghalib   cid:149 ˇ cid:216  cid:218  r¿nﬂ  cid:139 jn cid:147  , 311. Yuditsky, Davit Islam Gireevich   cid:16 dicki cid:26 ,   , 280.  Davit Islam Gireeviq , 292.  Yun, David Yuan-Yee    460, 686, 688, 689, 721.   , 454–455,  Yuriev, Sergei Petrovich   cid:16 r~ev, Serge cid:26   Petroviq , 366.  Z-independent vectors, 524. Zacher, Hans-Joachim, 200. Zaman, Arif  ˛n¸ cid:144  † cid:142 n« , 72, 75, 546, Zantema, Hantsje, 696. Zaremba, Stanisław Krystyn, 108, 115,  547, 549.  117, 332, 584.  Zaring, Wilson Miles, 653. Zassenhaus, Hans Julius, 446, 448, 449,  455, 456, 681, 685.  Zeilberger, Doron  XBXALIIV OEXEC , 536, 683. Zero, 196, 336.  leading, 222, 238–240, 327. minus, 202, 244–245, 249, 268, 274. order of magnitude, 239. polynomial, 418. Zero divisors, 671. Zeta function, 362, 382, 414, 644. Zhang, Linbo   Ziegler Hunts, Julian James, 617. Zierler, Neal, 29. Zippel, Richard Eliot, 455, 675. Zuckerman, Herbert Samuel, 155–156. Zuse, Konrad, 202, 225, 227. Zvonkin, Alexander Kalmanovich  Zvonkin,   , 764.  Aleksandr Kalmanoviq , 170.  the TEX and cid:111  cid:112  cid:113  cid:114  cid:115  cid:116  cid:117  cid:113  software as described in the author’s books Computers & Typesetting  THIS BOOK was composed on a Sun SPARCstation with Computer Modern typefaces, using   Reading, Mass.: Addison–Wesley, 1986 , Volumes A–E. The illustrations were produced with John Hobby’s METAPOST system. Some names in the index were typeset with additional fonts developed by Yannis Haralambous  Greek, Hebrew, Arabic , Olga G. Lapko  Cyrillic , Frans J. Velthuis  Devanagari , Masatoshi Watanabe  Japanese , and Linbo Zhang  Chinese .   This page intentionally left blank    Character code:  00 ␣  01 A  02 B  03 C  04 D  05 E  06 F  07 G  08 H  09 I  10 ´  11 J  12 K  13 L  14 M  15 N  16 O  17 P  18 Q  19 R  20 ˚  21 ˝  22 S  23 T  24 U  00 1 No operation  NOP 0   2  2  08 rA ← V LDA 0:5   16 rA ← −V LDAN 0:5   24 M F  ← rA  2  STA 0:5   32 M F  ← rJ  2  STJ 0:2   40 rA : 0, jump  1  48  1 rA ← [rA]? ± M INCA 0  DECA 1  ENTA 2  ENNA 3  2 CI ← rA F  : V  56  CMPA 0:5  FCMP 6   General form: C t Description  OP F   01 2 rA ← rA + V ADD 0:5  FADD 6  09 rI1 ← V LD1 0:5   2  2  17 rI1 ← −V LD1N 0:5   25 M F  ← rI1  2  ST1 0:5   33 M F  ← 0  2  STZ 0:5   41 1 rI1 : 0, jump  02 2 rA ← rA − V SUB 0:5  FSUB 6  10 rI2 ← V LD2 0:5   2  2  18 rI2 ← −V LD2N 0:5   26 M F  ← rI2  2  ST2 0:5   34 1 Unit F busy?  JBUS 0   42 1 rI2 : 0, jump  03 10 rAX ← rA × V  MUL 0:5  FMUL 6  11 rI3 ← V LD3 0:5   19 rI3 ← −V LD3N 0:5   2  2  27 M F  ← rI3  2  ST3 0:5   35  1 + T  Control, unit F  IOC 0   43 1 rI3 : 0, jump  J3[+]  51  1 rI3 ← [rI3]? ± M INC3 0  DEC3 1  ENT3 2  ENN3 3  2 CI ← rI3 F  : V  59  JA[+]  J1[+]  J2[+]  49  1 rI1 ← [rI1]? ± M INC1 0  DEC1 1  ENT1 2  ENN1 3  2 CI ← rI1 F  : V  57  50  1 rI2 ← [rI2]? ± M INC2 0  DEC2 1  ENT2 2  ENN2 3  2 CI ← rI2 F  : V  58  CMP1 0:5   CMP2 0:5   CMP3 0:5   C = operation code,  5 : 5  field of instruction F = op variant,  4 : 4  field of instruction M = address of instruction after indexing V = M F  = contents of F field of location M OP = symbolic name for operation  F  = normal F setting  t = execution time; T = interlock time   25 V  26 W  27 X  28 Y  29 Z  30 0  31 1  32 2  33 3  34 4  35 5  36 6  37 7  38 8  39 9  40 .  41 ,  42    43    44 +  45 -  46 *  47    48 =  49 $  50 <  51 >  52 @  53 ;  54 :  55 ‚  04 12 rA ← rAX V rX ← remainder  DIV 0:5  FDIV 6   12 rI4 ← V LD4 0:5   20 rI4 ← −V LD4N 0:5   2  2  28 M F  ← rI4  2  ST4 0:5   10  2  2  05  Special NUM 0  CHAR 1  HLT 2   13 rI5 ← V LD5 0:5   21 rI5 ← −V LD5N 0:5   29 M F  ← rI5  2  ST5 0:5   06 2 Shift M bytes SLA 0  SRA 1  SLAX 2  SRAX 3  SLC 4  SRC 5  2  14 rI6 ← V LD6 0:5   2  22 rI6 ← −V LD6N 0:5   30 M F  ← rI6  2  ST6 0:5   36 Input, unit F  1 + T  37  1 + T  Output, unit F  38 1 Unit F ready?  IN 0   44 1 rI4 : 0, jump  J4[+]  OUT 0   45 1 rI5 : 0, jump  J5[+]  JRED 0   46 1 rI6 : 0, jump  J6[+]  52  1 rI4 ← [rI4]? ± M INC4 0  DEC4 1  ENT4 2  ENN4 3  2 CI ← rI4 F  : V  60  53  1 rI5 ← [rI5]? ± M INC5 0  DEC5 1  ENT5 2  ENN5 3  2 CI ← rI5 F  : V  61  54  1 rI6 ← [rI6]? ± M INC6 0  DEC6 1  ENT6 2  ENN6 3  2 CI ← rI6 F  : V  62  1 + 2F  07 Move F words from M to rI1  MOVE 1   15 rX ← V LDX 0:5   23 rX ← −V LDXN 0:5   31 M F  ← rX  2  STX 0:5   2  2  1  39  Jumps  JMP 0  JSJ 1  JOV 2  JNOV 3  also [*] below 1  47 rX : 0, jump  JX[+]  55  1 rX ← [rX]? ± M INCX 0  DECX 1  ENTX 2  ENNX 3  2 CI ← rX F  : V  63  CMP4 0:5   CMP5 0:5   CMP6 0:5   CMPX 0:5   rA = register A rX = register X rAX = registers A and X as one rIi = index register i, 1 ≤ i ≤ 6 rJ = register J CI = comparison indicator  [*]: [+]: JL 4  < N 0  JE 5  = Z 1  JG 6  > P 2  JGE 7  ≥ NN 3  ̸= NZ 4  JNE 8  JLE 9  ≤ NP 5
