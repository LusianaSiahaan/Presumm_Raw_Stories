Neural Network Design  2nd Edition  Hagan Demuth Beale De Jesús   Neural Network Design  2nd Edtion  Martin T. Hagan  Oklahoma State University  Stillwater, Oklahoma  Howard B. Demuth  University of Colorado  Boulder, Colorado  Mark Hudson Beale  MHB Inc.  Hayden, Idaho  Orlando De Jesús  Consultant Frisco, Texas   Copyright by Martin T. Hagan and Howard B. Demuth. All rights reserved. No part of the book  may be reproduced, stored in a retrieval system, or transcribed in any form or by any means -  electronic, mechanical, photocopying, recording or otherwise - without the prior permission of  Hagan and Demuth.  MTH To Janet, Thomas, Daniel, Mom and Dad  HBD To Hal, Katherine, Kimberly and Mary  MHB To Leah, Valerie, Asia, Drake, Coral and Morgan  ODJ To: Marisela, María Victoria, Manuel, Mamá y Papá.  Neural Network Design, 2nd Edition, eBook  OVERHEADS and DEMONSTRATION PROGRAMS can be found at the following website:  hagan.okstate.edu nnd.html  A somewhat condensed paperback version of this text can be ordered from Amazon.   Contents  2  Neuron Model and Network Architectures  Preface  Introduction  Objectives  History  Applications  Biological Inspiration  Further Reading   Objectives  Theory and Examples   Notation  Neuron Model   Single-Input Neuron  Transfer Functions  Multiple-Input Neuron   Network Architectures   A Layer of Neurons  Multiple Layers of Neurons  Recurrent Networks   Summary of Results  Solved Problems  Epilogue  Exercises   i  1-1 1-2 1-5 1-8 1-10  2-1 2-2 2-2 2-2 2-2 2-3 2-7 2-9 2-9 2-10 2-13 2-16 2-20 2-22 2-23   3  4  An Illustrative Example  Objectives  Theory and Examples   Problem Statement  Perceptron   Two-Input Case  Pattern Recognition Example   Hamming Network   Feedforward Layer  Recurrent Layer   Hopfield Network   Epilogue  Exercises   Perceptron Learning Rule  Objectives  Theory and Examples   Learning Rules  Perceptron Architecture   Single-Neuron Perceptron  Multiple-Neuron Perceptron   Perceptron Learning Rule   Test Problem  Constructing Learning Rules  Unified Learning Rule  Training Multiple-Neuron Perceptrons   Proof of Convergence   Notation  Proof  Limitations   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   ii  3-1 3-2 3-2 3-3 3-4 3-5 3-8 3-8 3-9 3-12 3-15 3-16  4-1 4-2 4-2 4-3 4-5 4-8 4-8 4-9 4-10 4-12 4-13 4-15 4-15 4-16 4-18 4-20 4-21 4-33 4-34 4-36   5  6  Signal and Weight Vector Spaces  Objectives  Theory and Examples   Linear Vector Spaces  Linear Independence  Spanning a Space  Inner Product  Norm  Orthogonality   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   Gram-Schmidt Orthogonalization   Vector Expansions   Reciprocal Basis Vectors   Linear Transformations for Neural Networks  5-1 5-2 5-2 5-4 5-5 5-6 5-7 5-7 5-8 5-9 5-10 5-14 5-17 5-26 5-27 5-28  6-1 6-2 6-2 6-3 6-6 6-10 6-13 6-15 6-17 6-28 6-29 6-30  Objectives  Theory and Examples   Linear Transformations  Matrix Representations  Change of Basis  Eigenvalues and Eigenvectors   Diagonalization   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   iii   7  8  Supervised Hebbian Learning  Objectives  Theory and Examples   Linear Associator  The Hebb Rule   Performance Analysis   Pseudoinverse Rule  Application  Variations of Hebbian Learning   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   Performance Surfaces and Optimum Points  Objectives  Theory and Examples  Taylor Series   Vector Case   Directional Derivatives  Minima  Necessary Conditions for Optimality   First-Order Conditions  Second-Order Conditions   Quadratic Functions   Eigensystem of the Hessian   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   iv  7-1 7-2 7-3 7-4 7-5 7-7 7-10 7-12 17-4 7-16 7-29 7-30 7-31  8-1 8-2 8-2 8-4 8-5 8-7 8-9 8-10 8-11 8-12 8-13 8-20 8-22 8-34 8-35 8-36   9  10  Performance Optimization  Objectives  Theory and Examples   Steepest Descent   Stable Learning Rates  Minimizing Along a Line   Newton’s Method  Conjugate Gradient   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   Widrow-Hoff Learning  Objectives  Theory and Examples   ADALINE Network   Single ADALINE   Mean Square Error  LMS Algorithm  Analysis of Convergence  Adaptive Filtering   Adaptive Noise Cancellation  Echo Cancellation   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   9-1 9-2 9-2 9-6 9-8 9-10 9-15 9-21 9-23 9-37 9-38 9-39  10-1 10-2 10-2 10-3 10-4 10-7 10-9 10-13 10-15 10-21 10-22 10-24 10-40 10-41 10-42  v   11  Backpropagation  Objectives  Theory and Examples   Multilayer Perceptrons   Pattern Classification  Function Approximation  The Backpropagation Algorithm   Performance Index  Chain Rule  Backpropagating the Sensitivities  Summary   Example  Batch vs. Incremental Training  Using Backpropagation   Choice of Network Architecture  Convergence  Generalization   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   Objectives  Theory and Examples   Drawbacks of Backpropagation   Performance Surface Example  Convergence Example   Heuristic Modifications of Backpropagation   Momentum  Variable Learning Rate   Numerical Optimization Techniques   Conjugate Gradient  Levenberg-Marquardt Algorithm   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   vi  11-1 11-2 11-2 11-3 11-4 11-7 11-8 11-9 11-11 11-13 11-14 11-17 11-18 11-18 11-20 11-22 11-25 11-27 11-41 11-42 11-44  12-1 12-2 12-3 12-3 12-7 12-9 12-9 12-12 12-14 12-14 12-19 12-28 12-32 12-46 12-47 12-50  12  Variations on Backpropagation   13  14  Dynamic Networks  D  Generalization  Objectives  Theory and Examples   Problem Statement  Methods for Improving Generalization  Estimating Generalization Error Early Stopping  Regularization  Bayesian Analysis  Bayesian Regularization  Relationship Between Early Stopping   and Regularization   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   Objectives  Theory and Examples   Layered Digital Dynamic Networks   Example Dynamic Networks   Principles of Dynamic Learning  Dynamic Backpropagation   Preliminary Definitions  Real Time Recurrent Learning  Backpropagation-Through-Time  Summary and Comments on   Dynamic Training   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   vii  13-1 13-2 13-2 13-5 13-6 13-6 13-8 13-10 13-12  13-19 13-29 13-32 13-44 13-45 13-47  14-1 14-2 14-3 14-5 14-8 14-12 14-12 14-12 14-22  14-30 14-34 14-37 14-46 14-47 14-48   15  16  Associative Learning  Objectives  Theory and Examples   Simple Associative Network  Unsupervised Hebb Rule   Hebb Rule with Decay   Simple Recognition Network  Instar Rule   Kohonen Rule  Simple Recall Network  Outstar Rule  Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   Competitive Networks  Objectives  Theory and Examples   Hamming Network   Layer 1  Layer 2   Competitive Layer   Competitive Learning  Problems with Competitive Layers   Competitive Layers in Biology  Self-Organizing Feature Maps   Improving Feature Maps   Learning Vector Quantization   LVQ Learning  Improving LVQ Networks  LVQ2    Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   viii  15-1 15-2 15-3 15-5 15-7 15-9 15-11 15-15 15-16 15-17 15-21 15-23 15-34 15-35 15-37  16-1 16-2 16-3 16-3 16-4 16-5 16-7 16-9 16-10 16-12 16-15 16-16 16-18 16-21 16-22 16-24 16-37 16-38 16-39   17  Objectives  Theory and Examples   Radial Basis Networks  18  Grossberg Network  Radial Basis Network   Function Approximation  Pattern Classification  Global vs. Local  Training RBF Networks   Linear Least Squares  Orthogonal Least Squares  Clustering  Nonlinear Optimization  Other Training Techniques   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   Objectives  Theory and Examples   Biological Motivation: Vision   Illusions  Vision Normalization   Basic Nonlinear Model  Two-Layer Competitive Network   Layer 1  Layer 2  Choice of Transfer Function  Learning Law   Relation to Kohonen Law   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   ix  17-1 17-2 17-2 17-4 17-6 17-9 17-10 17-11 17-18 17-23 17-25 17-26 17-27 17-30 17-35 17-36 17-38  18-1 18-2 18-3 18-4 18-8 18-9 18-12 18-13 18-17 18-20 18-22 18-24 18-26 18-30 18-42 18-43 18-45   19  Objectives  Theory and Examples   Adaptive Resonance Theory  20  Stability  Objectives  Theory and Examples   Overview of Adaptive Resonance  Layer 1   Steady State Analysis   Layer 2  Orienting Subsystem  Learning Law: L1-L2   Subset Superset Dilemma  Learning Law  Learning Law: L2-L1  ART1 Algorithm Summary   Initialization  Algorithm   Other ART Architectures   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   Recurrent Networks  Stability Concepts  Definitions   Lyapunov Stability Theorem  Pendulum Example  LaSalle’s Invariance Theorem   Definitions  Theorem  Example  Comments   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises 30  x  19-1 19-2 19-2 19-4 19-6 19-10 19-13 19-17 19-17 19-18 19-20 19-21 19-21 19-21 19-23 19-25 19-30 19-45 19-46 19-48  20-1 20-2 20-2 20-3 20-4 20-5 20-6 20-12 20-12 20-13 20-14 20-18 20-19 20-21 20-28 20-29   21  Hopfield Network  22  Practical Training Issues  Objectives  Theory and Examples  Hopfield Model  Lyapunov Function   Invariant Sets  Example  Hopfield Attractors   Effect of Gain  Hopfield Design   Content-Addressable Memory  Hebb Rule  Lyapunov Surface   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   Objectives  Theory and Examples  Pre-Training Steps   Training the Network   Selection of Data  Data Preprocessing  Choice of Network Architecture   Weight Initialization  Choice of Training Algorithm  Stopping Criteria  Choice of Performance Function  Committees of Networks   Post-Training Analysis   Fitting  Pattern Recognition  Clustering  Prediction  Overfitting and Extrapolation  Sensitivity Analysis   Epilogue  Further Reading   xi  21-1 21-2 21-3 21-5 21-7 21-7 21-11 21-12 21-16 21-16 21-18 21-22 21-24 21-26 21-36 21-37 21-40  22-1 22-2 22-3 22-3 22-5 22-8 22-13 22-13 22-14 22-14 22-16 22-18 22-18 22-18 22-21 22-23 22-24 22-27 22-28 22-30 22-31   23  24  25  Case Study 1:Function Approximation  Objectives  Theory and Examples   Description of the Smart Sensor System  Data Collection and Preprocessing  Selecting the Architecture  Training the Network  Validation  Data Sets   Epilogue  Further Reading   Case Study 2:Probability Estimation  Objectives  Theory and Examples   Description of the CVD Process  Data Collection and Preprocessing  Selecting the Architecture  Training the Network  Validation  Data Sets   Epilogue  Further Reading   23-1 23-2 23-2 23-3 23-4 23-5 23-7 23-10 23-11 23-12  24-1 24-2 24-2 24-3 24-5 24-7 24-9 24-12 24-13 24-14  Case Study 3:Pattern Recognition  Objectives  Theory and Examples   25-1 25-2 Description of Myocardial Infarction Recognition  25-2 25-3 Data Collection and Preprocessing  25-6 Selecting the Architecture  25-7 Training the Network  Validation  25-7 25-10 Data Sets  25-11 25-12  Epilogue  Further Reading   xii   26  27  Case Study 4: Clustering  Objectives  Theory and Examples   Description of the Forest Cover Problem  Data Collection and Preprocessing  Selecting the Architecture  Training the Network  Validation  Data Sets   Epilogue  Further Reading   Case Study 5: Prediction  Objectives  Theory and Examples   Description of the Magnetic Levitation System  Data Collection and Preprocessing  Selecting the Architecture  Training the Network  Validation  Data Sets   Epilogue  Further Reading   26-1 26-2 26-2 26-4 26-5 26-6 26-7 26-11 26-12 26-13  27-1 27-2 27-2 27-3 27-4 27-6 27-8 27-13 27-14 27-15  xiii   Appendices  Bibliography  Notation  Software  Index  A  B  C  I  xiv   Preface  This book gives an introduction to basic neural network architectures and  learning rules. Emphasis is placed on the mathematical analysis of these  networks, on methods of training them and on their application to practical  engineering problems in such areas as nonlinear regression, pattern recog- nition, signal processing, data mining and control systems.  Every effort has been made to present material in a clear and consistent  manner so that it can be read and applied with ease. We have included  many solved problems to illustrate each topic of discussion. We have also  included a number of case studies in the final chapters to demonstrate  practical issues that arise when using neural networks on real world prob- lems.  Since this is a book on the design of neural networks, our choice of topics  was guided by two principles. First, we wanted to present the most useful  and practical neural network architectures, learning rules and training  techniques. Second, we wanted the book to be complete in itself and to flow  easily from one chapter to the next. For this reason, various introductory  materials and chapters on applied mathematics are included just before  they are needed for a particular subject. In summary, we have chosen some  topics because of their practical importance in the application of neural  networks, and other topics because of their importance in explaining how  neural networks operate.  We have omitted many topics that might have been included. We have not,  for instance, made this book a catalog or compendium of all known neural  network architectures and learning rules, but have instead concentrated  on the fundamental concepts. Second, we have not discussed neural net- work implementation technologies, such as VLSI, optical devices and par- allel computers. Finally, we do not present the biological and psychological  foundations of neural networks in any depth. These are all important top- ics, but we hope that we have done the reader a service by focusing on those  topics that we consider to be most useful in the design of neural networks  and by treating those topics in some depth.  This book has been organized for a one-semester introductory course in  neural networks at the senior or first-year graduate level.  It is also suit- able for short courses, self-study and reference.  The reader is expected to  have some background in linear algebra, probability and differential equa- tions.   P-1   Preface  2 2+  Each chapter of the book is divided into the following sections: Objectives,  Theory and Examples, Summary of Results, Solved Problems, Epilogue,  Further Reading and Exercises. The Theory and Examples section compris- es the main body of each chapter. It includes the development of fundamen- tal ideas as well as worked examples  indicated by the icon shown here in  the left margin . The Summary of Results section provides a convenient  listing of important equations and concepts and facilitates the use of the  book as an industrial reference. About a third of each chapter is devoted to  the Solved Problems section, which provides detailed examples for all key  concepts.   The following figure illustrates the dependencies among the chapters.  1  2  3  4  5  Introduction  Architectures  Illustrative  Example     Perceptron  Learning Rule  Signal and   Weight Vector   Spaces  Linear   6 Transformations   for Neural  Networks  Performance   8  Surfaces  Peformance  Optimization  Widrow-Hoff  9  10  11 Backpropagation  12 Variations on  Backpropagation  Generalization  13  14  Dynamic  Networks  Radial Basis   17  Networks  Supervised   7  Hebb  15  Associative  Learning  Competitive   16  Learning  Grossberg  18  19  20  21  ART  Stability  Hopfield  22 Practical Training  Case Study   23  Function   Approximation  24  Case Study  Probability  Estimation  25  Case Study   Pattern   Recognition  Case Study  Prediction  27  26  Case Study  Clustering  Chapters 1 through 6 cover basic concepts that are required for all of the  remaining chapters. Chapter 1 is an introduction to the text, with a brief  historical background and some basic biology. Chapter 2 describes the ba-  P-2   sic neural network architectures. The notation that is introduced in this  chapter is used throughout the book. In Chapter 3 we present a simple pat- tern recognition problem and show how it can be solved using three differ- ent types of neural networks. These three networks are representative of  the types of networks that are presented in the remainder of the text. In  addition, the pattern recognition problem presented here provides a com- mon thread of experience throughout the book.  Much of the focus of this book will be on methods for training neural net- works to perform various tasks. In Chapter 4 we introduce learning algo- rithms and present the first practical algorithm: the perceptron learning  rule. The perceptron network has fundamental limitations, but it is impor- tant for historical reasons and is also a useful tool for introducing key con- cepts that will be applied to more powerful networks in later chapters.  One of the main objectives of this book is to explain how neural networks  operate. For this reason we will weave together neural network topics with  important introductory material. For example, linear algebra, which is the  core of the mathematics required for understanding neural networks, is re- viewed in Chapters 5 and 6. The concepts discussed in these chapters will  be used extensively throughout the remainder of the book.  Chapters 7, and 15–19 describe networks and learning rules that are  heavily inspired by biology and psychology. They fall into two categories:  associative networks and competitive networks. Chapters 7 and 15 intro- duce basic concepts, while Chapters 16–19 describe more advanced net- works.  Chapters 8–14 and 17 develop a class of learning called performance learn- ing, in which a network is trained to optimize its performance. Chapters 8  and 9 introduce the basic concepts of performance learning. Chapters 10– 13 apply these concepts to feedforward neural networks of increasing pow- er and complexity, Chapter 14 applies them to dynamic networks and  Chapter 17 applies them to radial basis networks, which also use concepts  from competitive learning.  Chapters 20 and 21 discuss recurrent associative memory networks. These  networks, which have feedback connections, are dynamical systems. Chap- ter 20 investigates the stability of these systems. Chapter 21 presents the  Hopfield network, which has been one of the most influential recurrent net- works.  Chapters 22–27 are different than the preceding chapters. Previous chap- ters focus on the fundamentals of each type of network and their learning  rules. The focus is on understanding the key concepts. In Chapters 22–27,  we discuss some practical issues in applying neural networks to real world  problems. Chapter 22 describes many practical training tips, and Chapters  23–27 present a series of case studies, in which neural networks are ap- plied to practical problems in function approximation, probability estima- tion, pattern recognition, clustering and prediction.  P-3   Preface  Software  » 2 + 2 ans =       4  MATLAB is not essential for using this book. The computer exercises can  be performed with any available programming language, and the Neural  Network Design Demonstrations, while helpful, are not critical to under- standing the material covered in this book.  However, we have made use of the MATLAB software package to supple- ment the textbook. This software is widely available and, because of its ma- trix vector notation and graphics, is a convenient environment in which to  experiment with neural networks. We use MATLAB in two different ways.  First, we have included a number of exercises for the reader to perform in  MATLAB. Many of the important features of neural networks become ap- parent only for large-scale problems, which are computationally intensive  and not feasible for hand calculations. With MATLAB, neural network al- gorithms can be quickly implemented, and large-scale problems can be  tested conveniently. These MATLAB exercises are identified by the icon  shown here to the left.  If MATLAB is not available, any other program- ming language can be used to perform the exercises.   The second way in which we use MATLAB is through the Neural Network  Design Demonstrations, which can be downloaded from the website        hagan.okstate.edu nnd.html. These interactive demonstrations illustrate  important concepts in each chapter. After the software has been loaded into  the MATLAB directory on your computer  or placed on the MATLAB path ,  it can be invoked by typing nnd at the MATLAB prompt. All  demonstrations are easily accessible from a master menu. The icon shown  here to the left identifies references to these demonstrations in the text.  The demonstrations require MATLAB or the student edition of MATLAB,  version 2010a or later. See Appendix C for specific information on using the  demonstration software.  Overheads As an aid to instructors who are using this text, we have prepared a  companion set of overheads. Transparency masters  in Microsoft  Powerpoint format or PDF  for each chapter are available on the web at  hagan.okstate.edu nnd.html.   P-4   Acknowledgments  Acknowledgments  We are deeply indebted to the reviewers who have given freely of their time  to read all or parts of the drafts of this book and to test various versions of  the software. In particular we are most grateful to Professor John Andreae,  University of Canterbury; Dan Foresee, AT&T; Dr. Carl Latino, Oklahoma  State University; Jack Hagan, MCI; Dr. Gerry Andeen, SRI; and Joan Mill- er and Margie Jenks, University of Idaho. We also had constructive inputs  from our graduate students in ECEN 5733 at Oklahoma State University,  ENEL 621 at the University of Canterbury, INSA 0506 at the Institut Na- tional des Sciences Appliquées and ECE 5120 at the University of Colo- rado, who read many drafts, tested the software and provided helpful  suggestions for improving the book over the years. We are also grateful to  the anonymous reviewers who provided several useful recommendations.  We wish to thank Dr. Peter Gough for inviting us to join the staff in the  Electrical and Electronic Engineering Department at the University of  Canterbury, Christchurch, New Zealand, and Dr. Andre Titli for inviting  us to join the staff at the Laboratoire d'Analyse et d'Architecture des  Systèms, Centre National de la Recherche Scientifique, Toulouse, France.  Sabbaticals from Oklahoma State University and a year’s leave from the  University of Idaho gave us the time to write this book. Thanks to Texas  Instruments, Halliburton, Cummins, Amgen and NSF, for their support of  our neural network research. Thanks to The Mathworks for permission to  use material from the Neural Network Toolbox.  P-5   Objectives  1 Introduction  1  Objectives History Applications Biological Inspiration Further Reading  1-1 1-2 1-5 1-8 1-10  Objectives  As you read these words you are using a complex biological neural network.  You have a highly interconnected set of some 1011 neurons to facilitate your  reading, breathing, motion and thinking. Each of your biological neurons,  a rich assembly of tissue and chemistry, has the complexity, if not the  speed, of a microprocessor. Some of your neural structure was with you at  birth. Other parts have been established by experience.   Scientists have only just begun to understand how biological neural net- works operate. It is generally understood that all biological neural func- tions, including memory, are stored in the neurons and in the connections  between them. Learning is viewed as the establishment of new connections  between neurons or the modification of existing connections. This leads to  the following question: Although we have only a rudimentary understand- ing of biological neural networks, is it possible to construct a small set of  simple artificial “neurons” and perhaps train them to serve a useful func- tion? The answer is “yes.” This book, then, is about artificial neural net- works.   The neurons that we consider here are not biological. They are extremely  simple abstractions of biological neurons, realized as elements in a pro- gram or perhaps as circuits made of silicon. Networks of these artificial  neurons do not have a fraction of the power of the human brain, but they  can be trained to perform useful functions. This book is about such neu- rons, the networks that contain them and their training.  1-1   1 Introduction  History  The history of artificial neural networks is filled with colorful, creative in- dividuals from a variety of fields, many of whom struggled for decades to  develop concepts that we now take for granted. This history has been doc- umented by various authors. One particularly interesting book is Neuro- computing: Foundations of Research by John Anderson and Edward  Rosenfeld. They have collected and edited a set of some 43 papers of special  historical interest. Each paper is preceded by an introduction that puts the  paper in historical perspective.  Histories of some of the main neural network contributors are included at  the beginning of various chapters throughout this text and will not be re- peated here. However, it seems appropriate to give a brief overview, a sam- ple of the major developments.  At least two ingredients are necessary for the advancement of a technology:  concept and implementation. First, one must have a concept, a way of  thinking about a topic, some view of it that gives a clarity not there before.  This may involve a simple idea, or it may be more specific and include a  mathematical description. To illustrate this point, consider the history of  the heart. It was thought to be, at various times, the center of the soul or a  source of heat. In the 17th century medical practitioners finally began to  view the heart as a pump, and they designed experiments to study its  pumping action. These experiments revolutionized our view of the circula- tory system. Without the pump concept, an understanding of the heart was  out of grasp.  Concepts and their accompanying mathematics are not sufficient for a  technology to mature unless there is some way to implement the system.  For instance, the mathematics necessary for the reconstruction of images  from computer-aided tomography  CAT  scans was known many years be- fore the availability of high-speed computers and efficient algorithms final- ly made it practical to implement a useful CAT system.  The history of neural networks has progressed through both conceptual in- novations and implementation developments. These advancements, how- ever, seem to have occurred in fits and starts rather than by steady  evolution.  Some of the background work for the field of neural networks occurred in  the late 19th and early 20th centuries. This consisted primarily of interdis- ciplinary work in physics, psychology and neurophysiology by such scien- tists as Hermann von Helmholtz, Ernst Mach and Ivan Pavlov. This early  work emphasized general theories of learning, vision, conditioning, etc.,  and did not include specific mathematical models of neuron operation.  1-2   History  1  The modern view of neural networks began in the 1940s with the work of  Warren McCulloch and Walter Pitts [McPi43], who showed that networks  of artificial neurons could, in principle, compute any arithmetic or logical  function. Their work is often acknowledged as the origin of the neural net- work field.   McCulloch and Pitts were followed by Donald Hebb [Hebb49], who pro- posed that classical conditioning  as discovered by Pavlov  is present be- cause of the properties of individual neurons. He proposed a mechanism for  learning in biological neurons  see Chapter 7 .  The first practical application of artificial neural networks came in the late  1950s, with the invention of the perceptron network and associated learn- ing rule by Frank Rosenblatt [Rose58]. Rosenblatt and his colleagues built  a perceptron network and demonstrated its ability to perform pattern rec- ognition. This early success generated a great deal of interest in neural net- work research. Unfortunately, it was later shown that the basic perceptron  network could solve only a limited class of problems.  See Chapter 4 for  more on Rosenblatt and the perceptron learning rule.   At about the same time, Bernard Widrow and Ted Hoff [WiHo60] intro- duced a new learning algorithm and used it to train adaptive linear neural  networks, which were similar in structure and capability to Rosenblatt’s  perceptron. The Widrow-Hoff learning rule is still in use today.  See Chap- ter 10 for more on Widrow-Hoff learning.   Unfortunately, both Rosenblatt’s and Widrow’s networks suffered from the  same inherent limitations, which were widely publicized in a book by Mar- vin Minsky and Seymour Papert [MiPa69]. Rosenblatt and Widrow were  aware of these limitations and proposed new networks that would over- come them. However, they were not able to successfully modify their learn- ing algorithms to train the more complex networks.   Many people, influenced by Minsky and Papert, believed that further re- search on neural networks was a dead end. This, combined with the fact  that there were no powerful digital computers on which to experiment,  caused many researchers to leave the field. For a decade neural network re- search was largely suspended.  Some important work, however, did continue during the 1970s. In 1972  Teuvo Kohonen [Koho72] and James Anderson [Ande72] independently  and separately developed new neural networks that could act as memories.   See Chapter 15 and Chapter 16 for more on Kohonen networks.  Stephen  Grossberg [Gros76] was also very active during this period in the investi- gation of self-organizing networks.  See Chapter 18 and Chapter 19.   Interest in neural networks had faltered during the late 1960s because of  the lack of new ideas and powerful computers with which to experiment.  During the 1980s both of these impediments were overcome, and research  in neural networks increased dramatically. New personal computers and   1-3   1 Introduction  workstations, which rapidly grew in capability, became widely available. In  addition, important new concepts were introduced.   Two new concepts were most responsible for the rebirth of neural networks.  The first was the use of statistical mechanics to explain the operation of a  certain class of recurrent network, which could be used as an associative  memory. This was described in a seminal paper by physicist John Hopfield  [Hopf82].  Chapter 20 and Chapter 21 discuss these Hopfield networks.   The second key development of the 1980s was the backpropagation algo- rithm for training multilayer perceptron networks, which was discovered  independently by several different researchers. The most influential publi- cation of the backpropagation algorithm was by David Rumelhart and  James McClelland [RuMc86]. This algorithm was the answer to the criti- cisms Minsky and Papert had made in the 1960s.  See Chapter 11 for a de- velopment of the backpropagation algorithm.   These new developments reinvigorated the field of neural networks. Since  the 1980s, thousands of papers have been written, neural networks have  found countless applications, and the field has been buzzing with new the- oretical and practical work.  The brief historical account given above is not intended to identify all of the  major contributors, but is simply to give the reader some feel for how  knowledge in the neural network field has progressed. As one might note,  the progress has not always been “slow but sure.” There have been periods  of dramatic progress and periods when relatively little has been accom- plished.   Many of the advances in neural networks have had to do with new con- cepts, such as innovative architectures and training rules. Just as impor- tant has been the availability of powerful new computers on which to test  these new concepts.  Well, so much for the history of neural networks to this date. The real ques- tion is, “What will happen in the future?” Neural networks have clearly  taken a permanent place as important mathematical engineering tools.  They don’t provide solutions to every problem, but they are essential tools  to be used in appropriate situations. In addition, remember that we still  know very little about how the brain works. The most important advances  in neural networks almost certainly lie in the future.  The large number and wide variety of applications of this technology are  very encouraging. The next section describes some of these applications.  1-4   Applications  Applications  1  A newspaper article described the use of neural networks in literature re- search by Aston University. It stated that “the network can be taught to  recognize individual writing styles, and the researchers used it to compare  works attributed to Shakespeare and his contemporaries.” A popular sci- ence television program documented the use of neural networks by an Ital- ian research institute to test the purity of olive oil. Google uses neural  networks for image tagging  automatically identifying an image and as- signing keywords , and Microsoft has developed neural networks that can  help convert spoken English speech into spoken Chinese speech. Research- ers at Lund University and Skåne University Hospital in Sweden have  used neural networks to improve long-term survival rates for heart trans- plant recipients by identifying optimal recipient and donor matches. These  examples are indicative of the broad range of applications that can be found  for neural networks. The applications are expanding because neural net- works are good at solving problems, not just in engineering, science and  mathematics, but in medicine, business, finance and literature as well.  Their application to a wide variety of problems in many fields makes them  very attractive. Also, faster computers and faster algorithms have made it  possible to use neural networks to solve complex industrial problems that  formerly required too much computation.  The following note and Table of Neural Network Applications are repro- duced here from the Neural Network Toolbox for MATLAB with the per- mission of the MathWorks, Inc.  A 1988 DARPA Neural Network Study [DARP88] lists various neural net- work applications, beginning with the adaptive channel equalizer in about  1984. This device, which is an outstanding commercial success, is a single- neuron network used in long distance telephone systems to stabilize voice  signals. The DARPA report goes on to list other commercial applications,  including a small word recognizer, a process monitor, a sonar classifier and  a risk analysis system. Thousands of neural networks have been applied in hundreds of fields in  the many years since the DARPA report was written. A list of some of those  applications follows.  Aerospace  High performance aircraft autopilots, flight path simulations,  aircraft control systems, autopilot enhancements, aircraft com- ponent simulations, aircraft component fault detectors  1-5   1 Introduction  Automotive  Banking  Defense  Electronics  Entertainment  Financial  Insurance  Manufacturing  Automobile automatic guidance systems, fuel injector control,  automatic braking systems, misfire detection, virtual emission  sensors, warranty activity analyzers  Check and other document readers, credit application evalua- tors, cash forecasting, firm classification, exchange rate fore- casting, predicting loan recovery rates, measuring credit risk  Weapon steering, target tracking, object discrimination, facial  recognition, new kinds of sensors, sonar, radar and image sig- nal processing including data compression, feature extraction  and noise suppression, signal image identification  Code sequence prediction, integrated circuit chip layout, pro- cess control, chip failure analysis, machine vision, voice syn- thesis, nonlinear modeling  Animation, special effects, market forecasting  Real estate appraisal, loan advisor, mortgage screening, corpo- rate bond rating, credit line use analysis, portfolio trading pro- gram, corporate financial analysis, currency price prediction   Policy application evaluation, product optimization  Manufacturing process control, product design and analysis,  process and machine diagnosis, real-time particle identifica- tion, visual quality inspection systems, beer testing, welding  quality analysis, paper quality prediction, computer chip qual- ity analysis, analysis of grinding operations, chemical product  design analysis, machine maintenance analysis, project bid- ding, planning and management, dynamic modeling of chemi- cal process systems  1-6   1  Applications  Medical  Oil and Gas  Robotics  Speech  Securities  Breast cancer cell analysis, EEG and ECG analysis, prosthesis  design, optimization of transplant times, hospital expense re- duction, hospital quality improvement, emergency room test  advisement  Exploration, smart sensors, reservoir modeling, well treatment  decisions, seismic interpretation  Trajectory control, forklift robot, manipulator controllers, vi- sion systems, autonomous vehicles  Speech recognition, speech compression, vowel classification,  text to speech synthesis  Market analysis, automatic bond rating, stock trading advisory  systems  Telecommunications  Image and data compression, automated information services,  real-time translation of spoken language, customer payment  processing systems  Transportation  Truck brake diagnosis systems, vehicle scheduling, routing  systems  Conclusion The number of neural network applications, the money that has been in- vested in neural network software and hardware, and the depth and  breadth of interest in these devices is enormous.  1-7   1 Introduction  Biological Inspiration  The artificial neural networks discussed in this text are only remotely re- lated to their biological counterparts. In this section we will briefly describe  those characteristics of brain function that have inspired the development  of artificial neural networks.  The brain consists of a large number  approximately 1011  of highly con- nected elements  approximately 104 connections per element  called neu- rons. For our purposes these neurons have three principal components: the  dendrites, the cell body and the axon. The dendrites are tree-like receptive  networks of nerve fibers that carry electrical signals into the cell body. The  cell body effectively sums and thresholds these incoming signals. The axon  is a single long fiber that carries the signal from the cell body out to other  neurons. The point of contact between an axon of one cell and a dendrite of  another cell is called a synapse. It is the arrangement of neurons and the  strengths of the individual synapses, determined by a complex chemical  process, that establishes the function of the neural network. Figure 1.1 is  a simplified schematic diagram of two biological neurons.  Dendrites  Synapse  Axon  Cell Body  Figure 1.1  Schematic Drawing of Biological Neurons  Some of the neural structure is defined at birth. Other parts are developed  through learning, as new connections are made and others waste away.  This development is most noticeable in the early stages of life. For example,   1-8   1  Biological Inspiration  it has been shown that if a young cat is denied use of one eye during a crit- ical window of time, it will never develop normal vision in that eye. Lin- guists have discovered that infants over six months of age can no longer  discriminate certain speech sounds, unless they were exposed to them ear- lier in life [WeTe84].  Neural structures continue to change throughout life. These later changes  tend to consist mainly of strengthening or weakening of synaptic junctions.  For instance, it is believed that new memories are formed by modification  of these synaptic strengths. Thus, the process of learning a new friend’s  face consists of altering various synapses. Neuroscientists have discovered  [MaGa2000], for example, that the hippocampi of London taxi drivers are  significantly larger than average. This is because they must memorize a  large amount of navigational information—a process that takes more than  two years.  Artificial neural networks do not approach the complexity of the brain.  There are, however, two key similarities between biological and artificial  neural networks. First, the building blocks of both networks are simple  computational devices  although artificial neurons are much simpler than  biological neurons  that are highly interconnected. Second, the connections  between neurons determine the function of the network. The primary ob- jective of this book will be to determine the appropriate connections to solve  particular problems.  It is worth noting that even though biological neurons are very slow when  compared to electrical circuits  10-3 s compared to 10-10 s , the brain is  able to perform many tasks much faster than any conventional computer.  This is in part because of the massively parallel structure of biological neu- ral networks; all of the neurons are operating at the same time. Artificial  neural networks share this parallel structure. Even though most artificial  neural networks are currently implemented on conventional digital com- puters, their parallel structure makes them ideally suited to implementa- tion using VLSI, optical devices and parallel processors.  In the following chapter we will introduce our basic artificial neuron and  will explain how we can combine such neurons to form networks. This will  provide a background for Chapter 3, where we take our first look at neural  networks in action.  1-9   1 Introduction  Further Reading  [Ande72]   J. A. Anderson, “A simple neural network generating an in- teractive memory,” Mathematical Biosciences, Vol. 14, pp.  197–220, 1972.  Anderson proposed a “linear associator” model for associa- tive memory. The model was trained, using a generaliza- tion of the Hebb postulate, to learn an association between  input and output vectors. The physiological plausibility of  the network was emphasized. Kohonen published a closely  related paper at the same time [Koho72], although the two  researchers were working independently.  [AnRo88]  J. A. Anderson and E. Rosenfeld, Neurocomputing: Foun- dations of Research, Cambridge, MA: MIT Press, 1989.  Neurocomputing is a fundamental reference book. It con- tains over forty of the most important neurocomputing  writings. Each paper is accompanied by an introduction  that summarizes its results and gives a perspective on the  position of the paper in the history of the field.  [DARP88]  DARPA Neural Network Study, Lexington, MA: MIT Lin- coln Laboratory, 1988.  [Gros76]  This study is a compendium of knowledge of neural net- works as they were known to 1988. It presents the theoret- ical foundations of neural networks and discusses their  current applications. It contains sections on associative  memories, recurrent networks, vision, speech recognition,  and robotics. Finally, it discusses simulation tools and im- plementation technology.  S. Grossberg, “Adaptive pattern classification and univer- sal recoding: I. Parallel development and coding of neural  feature detectors,” Biological Cybernetics, Vol. 23, pp. 121– 134, 1976.  Grossberg describes a self-organizing neural network  based on the visual system. The network, which consists of  short-term and long-term memory mechanisms, is a contin- uous-time competitive network. It forms a basis for the  adaptive resonance theory  ART  networks.  1-10   Further Reading  [Gros80]  S. Grossberg, “How does the brain build a cognitive code?”  Psychological Review, Vol. 88, pp. 375–407, 1980.  Grossberg’s 1980 paper proposes neural structures and  mechanisms that can explain many physiological behaviors  including spatial frequency adaptation, binocular rivalry,  etc. His systems perform error correction by themselves,  without outside help.   [Hebb 49]  D. O. Hebb, The Organization of Behavior. New York:  Wiley, 1949.  1  [Koho72]   T. Kohonen, “Correlation matrix memories,” IEEE Trans- actions on Computers, vol. 21, pp. 353–359, 1972.  [Hopf82]  [MaGa00]  The main premise of this seminal book is that behavior can  be explained by the action of neurons. In it, Hebb proposed  one of the first learning laws, which postulated a mecha- nism for learning at the cellular level.  Hebb proposes that classical conditioning in biology is  present because of the properties of individual neurons.  J. J. Hopfield, “Neural networks and physical systems with  emergent collective computational abilities,” Proceedings  of the National Academy of Sciences, Vol. 79, pp. 2554– 2558, 1982.  Hopfield describes a content-addressable neural network.  He also presents a clear picture of how his neural network  operates, and of what it can do.  Kohonen proposed a correlation matrix model for associa- tive memory. The model was trained, using the outer prod- uct rule  also known as the Hebb rule , to learn an  association between input and output vectors. The mathe- matical structure of the network was emphasized. Ander- son published a closely related paper at the same time  [Ande72], although the two researchers were working inde- pendently.  E. A. Maguire, D. G. Gadian, I. S. Johnsrude, C. D. Good, J.  Ashburner, R. S. J. Frackowiak, and C. D. Frith, “Naviga- tion-related structural change in the hippocampi of taxi  drivers,” Proceedings of the National Academy of Sciences,  Vol. 97, No. 8, pp. 4398-4403, 2000.  Taxi drivers in London must undergo extensive training,  learning how to navigate between thousands of places in  the city. This training is colloquially known as ‘‘being on  The Knowledge’’ and takes about 2 years to acquire on av-  1-11   1 Introduction  erage. This study demonstrated that the posterior hippoc- ampi of London taxi drivers were significantly larger  relative to those of control subjects.   [McPi43]  W. McCulloch and W. Pitts, “A logical calculus of the ideas  immanent in nervous activity,” Bulletin of Mathematical  Biophysics., Vol. 5, pp. 115–133, 1943.  This article introduces the first mathematical model of a  neuron, in which a weighted sum of input signals is com- pared to a threshold to determine whether or not the neu- ron fires. This was the first attempt to describe what the  brain does, based on computing elements known at the  time. It shows that simple neural networks can compute  any arithmetic or logical function.  [MiPa69]  M. Minsky and S. Papert, Perceptrons, Cambridge, MA:  MIT Press, 1969.  A landmark book that contains the first rigorous study de- voted to determining what a perceptron network is capable  of learning. A formal treatment of the perceptron was need- ed both to explain the perceptron’s limitations and to indi- cate directions for overcoming them. Unfortunately, the  book pessimistically predicted that the limitations of per- ceptrons indicated that the field of neural networks was a  dead end. Although this was not true it temporarily cooled  research and funding for research for several years.  F. Rosenblatt, “The perceptron: A probabilistic model for  information storage and organization in the brain,” Psycho- logical Review, Vol. 65, pp. 386–408, 1958.  Rosenblatt presents the first practical artificial neural net- work — the perceptron.  [Rose58]   [RuMc86]  D. E. Rumelhart and J. L. McClelland, eds., Parallel Dis- tributed Processing: Explorations in the Microstructure of  Cognition, Vol. 1, Cambridge, MA: MIT Press, 1986.  [WeTe84]  One of the two key influences in the resurgence of interest  in the neural network field during the 1980s. Among other  topics, it presents the backpropagation algorithm for train- ing multilayer networks.  J. F. Werker and R. C. Tees, “Cross-language speech per- ception: Evidence for perceptual reorganization during the  first year of life,” Infant Behavior and Development, Vol. 7,  pp. 49-63, 1984.  1-12   Further Reading  This work describes an experiment in which infants from  the Interior Salish ethnic group in British Columbia, and  other infants outside that group, were tested on their abil- ity to discriminate two different sounds from the Thompson  language, which is spoken by the Interior Salish. The re- searchers discovered that infants less than 6 or 8 months of  age were generally able to distinguish the sounds, whether  or not they were Interior Salish. By 10 to 12 months of age,  only the Interior Salish children were able to distinguish  the two sounds.  1  [WiHo60]  B. Widrow and M. E. Hoff, “Adaptive switching cir- cuits,”1960 IRE WESCON Convention Record, New York:  IRE Part 4, pp. 96–104, 1960.  This seminal paper describes an adaptive perceptron-like  network that can learn quickly and accurately. The authors  assume that the system has inputs and a desired output  classification for each input, and that the system can calcu- late the error between the actual and desired output. The  weights are adjusted, using a gradient descent method, so  as to minimize the mean square error.  Least Mean Square  error or LMS algorithm.   This paper is reprinted in [AnRo88].  1-13   Objectives  2 Neuron Model and Network  Architectures  2  Objectives  Objectives Theory and Examples  Notation Neuron Model  Single-Input Neuron Transfer Functions Multiple-Input Neuron  Network Architectures  A Layer of Neurons Multiple Layers of Neurons Recurrent Networks  Summary of Results Solved Problems Epilogue Exercises  2-1 2-2 2-2 2-2 2-2 2-3 2-7 2-9 2-9 2-10 2-13 2-16 2-20 2-22 2-23  In Chapter 1 we presented a simplified description of biological neurons  and neural networks. Now we will introduce our simplified mathematical  model of the neuron and will explain how these artificial neurons can be in- terconnected to form a variety of network architectures. We will also illus- trate the basic operation of these networks through some simple examples.  The concepts and notation introduced in this chapter will be used through- out this book.  This chapter does not cover all of the architectures that will be used in this  book, but it does present the basic building blocks. More complex architec- tures will be introduced and discussed as they are needed in later chapters.  Even so, a lot of detail is presented here. Please note that it is not necessary  for the reader to memorize all of the material in this chapter on a first read- ing. Instead, treat it as a sample to get you started and a resource to which  you can return.  2-1   2 Neuron Model and Network Architectures  Theory and Examples  Notation  Unfortunately, there is no single neural network notation that is universal- ly accepted. Papers and books on neural networks have come from many di- verse fields, including engineering, physics, psychology and mathematics,  and many authors tend to use vocabulary peculiar to their specialty. As a  result, many books and papers in this field are difficult to read, and con- cepts are made to seem more complex than they actually are. This is a  shame, as it has prevented the spread of important new ideas. It has also  led to more than one “reinvention of the wheel.”  In this book we have tried to use standard notation where possible, to be  clear and to keep matters simple without sacrificing rigor. In particular, we  have tried to define practical conventions and use them consistently.   Figures, mathematical equations and text discussing both figures and  mathematical equations will use the following notation:  Scalars — small italic letters: a,b,c  Vectors — small bold nonitalic letters: a,b,c  Matrices — capital BOLD nonitalic letters: A,B,C   Additional notation concerning the network architectures will be intro- duced as you read this chapter. A complete list of the notation that we use  throughout the book is given in Appendix B, so you can look there if you  have a question.   Neuron Model  Weight Bias Net Input Transfer Function  Single-Input Neuron A single-input neuron is shown in Figure 2.1. The scalar input  plied by the scalar weight   to form  w summer. The other input,  , is multiplied by a bias  1 the summer. The summer output  into a transfer function   Some authors use the term “activation function” rather than transfer func- tion and “offset” rather than bias.     is multi- , one of the terms that is sent to the   and then passed to  , often referred to as the net input, goes   , which produces the scalar neuron output   wp  .   n  b  p  a  f  If we relate this simple model back to the biological neuron that we dis- cussed in Chapter 1, the weight   corresponds to the strength of a synapse,  the cell body is represented by the summation and the transfer function,  and the neuron output    represents the signal on the axon.  w  a  2-2   Neuron Model  Inputs  General Neuron  p   cid:0  cid:0 Σ w n  cid:0  cid:0   b  f   cid:0  cid:0  a  cid:0  cid:0   1 a = f  wp + b   Figure 2.1  Single-Input Neuron  2  The neuron output is calculated as  a  =   f wp  b+    .  If, for instance,   w  3=  ,   p  2=   and   b  1.5–=  , then   a  =  f 3 2    –  1.5    =  f 4.5    The actual output depends on the particular transfer function that is cho- sen. We will discuss transfer functions in the next section.  The bias is much like a weight, except that it has a constant input of 1.  However, if you do not want to have a bias in a particular neuron, it can be  omitted. We will see examples of this in Chapters 3, 7 and 16.  b  w  Note that   and  are both adjustable scalar parameters of the neuron.  Typically the transfer function is chosen by the designer and then the pa- rameters   will be adjusted by some learning rule so that the neu- ron input output relationship meets some specific goal  see Chapter 4 for  an introduction to learning rules . As described in the following section, we  have different transfer functions for different purposes.   and   w  b  Transfer Functions  The transfer function in Figure 2.1 may be a linear or a nonlinear function  of  . A particular transfer function is chosen to satisfy some specification  of the problem that the neuron is attempting to solve.  n  A variety of transfer functions have been included in this book. Three of the  most commonly used functions are discussed below.  The hard limit transfer function, shown on the left side of Figure 2.2, sets  the output of the neuron to 0 if the function argument is less than 0, or 1 if  its argument is greater than or equal to 0. We will use this function to cre- ate neurons that classify inputs into two distinct categories. It will be used  extensively in Chapter 4.  2-3  Hard Limit  Transfer Function   2 Neuron Model and Network Architectures  a  +1  0 -1 a = hardlim  n   n   cid:0  cid:0   -b w  p  a  +1  0 -1  Hard Limit Transfer Function  a = hardlim  wp + b   Single-Input hardlim Neuron  Figure 2.2  Hard Limit Transfer Function  The graph on the right side of Figure 2.2 illustrates the input output char- acteristic of a single-input neuron that uses a hard limit transfer function.  Here we can see the effect of the weight and the bias. Note that an icon for  the hard limit transfer function is shown between the two figures. Such  icons will replace the general   in network diagrams to show the particular  transfer function that is being used.  f  The output of a linear transfer function is equal to its input:  a  n=  ,   2.1   as illustrated in Figure 2.3.  Neurons with this transfer function are used in the ADALINE networks,  which are discussed in Chapter 10.  a  +b  0  n   cid:0   -b w  p  Linear  Transfer Function  a = purelin  n   Linear Transfer Function  a = purelin  wp + b   Single-Input purelin Neuron  Figure 2.3  Linear Transfer Function  The output   ron with a bias is shown on the right of Figure 2.3.    versus input    p  a    characteristic of a single-input linear neu-  Log-Sigmoid  Transfer Function  The log-sigmoid transfer function is shown in Figure 2.4.  a  +1  0 -1  2-4   Neuron Model  a   +1  0  -1 a = logsig  n   a   +1  p  2  n   cid:0  cid:0   -b w  0 -1 a = logsig  wp + b   Log-Sigmoid Transfer Function  Single-Input logsig Neuron  Figure 2.4  Log-Sigmoid Transfer Function  This transfer function takes the input  which may have any value between  plus and minus infinity  and squashes the output into the range 0 to 1, ac- cording to the expression:  a  =  1 ---------------- e n–+ 1  .   2.2   The log-sigmoid transfer function is commonly used in multilayer networks  that are trained using the backpropagation algorithm, in part because this  function is differentiable  see Chapter 11 .  Most of the transfer functions used in this book are summarized in Table  2.1. Of course, you can define other transfer functions in addition to those  shown in Table 2.1 if you wish.  To experiment with a single-input neuron, use the Neural Network Design  Demonstration One-Input Neuron nnd2n1.  2-5   2 Neuron Model and Network Architectures  Name  Input Output Relation  Icon  Hard Limit  Symmetrical Hard Limit  a a  = =  0 1  n n  0 0  a a  1 –= = +1  n n  0 0  Linear  a  n=  Saturating Linear  Symmetric Saturating   Linear  Log-Sigmoid  Hyperbolic Tangent   Sigmoid  a a a  a a a  = = =  0 n 1  n 0 n  0   n 1  1  1  –= = n = 1  n 1– n  1–   n 1  1  a  =  1 ---------------- e n–+ 1  a  =  e n–– en ------------------ e n–+ en  Positive Linear  a  = a  0 =  n  n  0 0  n  Competitive  a a  = =  1    neuron with max n 0    all other neurons  Table 2.1 Transfer Functions  MATLAB Function   cid:0  cid:0   hardlim   cid:0  cid:0   hardlims   cid:0  cid:0   purelin   cid:0  cid:0   satlin   cid:0  cid:0   satlins   cid:0  cid:0   logsig   cid:0  cid:0   tansig   cid:0  cid:0   cid:0  C  cid:0   poslin  compet  2-6   Neuron Model  Multiple-Input Neuron Typically, a neuron has more than one input. A neuron with  shown in Figure 2.5. The individual inputs  corresponding elements    of the weight matrix   p1 p2 ... pR,  ,  ,  w1 1 w1 2  ,  ... w1 R ,  ,  W  .  R   inputs is    are each weighted by   Weight Matrix  2  Inputs  Multiple-Input Neuron  p1 p2p3  pR  w1, 1  cid:0  cid:0 Σ n  cid:0  cid:0  w1, R  b   cid:0  cid:0 f a  cid:0  cid:0   1 a = f  Wp + b   Figure 2.5  Multiple-Input Neuron  The neuron has a bias  form the net input   n  :  b  , which is summed with the weighted inputs to   n  =  w1 1 p1 w1 2 p2  +  +  ... w1 R pR  +  +  b  .  This expression can be written in matrix form:  where the matrix   W   for the single neuron case has only one row.  Now the neuron output can be written as  n Wp b+  =  ,  a  =   f Wp b+    .   2.3    2.4    2.5   Fortunately, neural networks can often be described with matrices. This  kind of matrix expression will be used throughout the book. Don’t be con- cerned if you are rusty with matrix and vector operations. We will review  these topics in Chapters 5 and 6, and we will provide many examples and  solved problems that will spell out the procedures.  We have adopted a particular convention in assigning the indices of the el- ements of the weight matrix. The first index indicates the particular neu- ron destination for that weight. The second index indicates the source of  the signal fed to the neuron. Thus, the indices in  represents the connection to the first  and only  neuron from the second  source. Of course, this convention is more useful if there is more than one  neuron, as will be the case later in this chapter.   say that this weight   w1 2  2-7  Weight Indices   2 Neuron Model and Network Architectures  We would like to draw networks with several neurons, each having several  inputs. Further, we would like to have more than one layer of neurons. You  can imagine how complex such a network might appear if all the lines were  drawn. It would take a lot of ink, could hardly be read, and the mass of de- tail might obscure the main features. Thus, we will use an abbreviated no- tation. A multiple-input neuron using this notation is shown in Figure 2.6.   Abbreviated Notation  Input  Multiple-Input Neuron   cid:0  cid:0  p  cid:0  cid:0  W R x 1 1 x R  cid:0  cid:0 b 1  cid:0  cid:0   1 x 1  R  n 1 x 1   cid:0  cid:0  a  cid:0  cid:0  1 x 1  cid:0  cid:0   cid:0  cid:0   f  1  a = f  Wp + b   Figure 2.6  Neuron with   R   Inputs, Abbreviated Notation  R 1  , indicating that the input is a single vector of   As shown in Figure 2.6, the input vector   is represented by the solid ver- p  are displayed below the variable  tical bar at the left. The dimensions of  p as   elements. These  inputs go to the weight matrix   columns but only one row  in this single neuron case. A constant 1 enters the neuron as an input and  is multiplied by a scalar bias   is  b , which is the sum of the bias  . The neuron’s output  n  is a scalar in this case. If we had more than one neuron, the network out- a put would be a vector.  . The net input to the transfer function   and the product  b  , which has   Wp  W  R  R  f  The dimensions of the variables in these abbreviated notation figures will  always be included, so that you can tell immediately if we are talking about  a scalar, a vector or a matrix. You will not have to guess the kind of variable  or its dimensions.  Note that the number of inputs to a network is set by the external specifi- cations of the problem. If, for instance, you want to design a neural network  that is to predict kite-flying conditions and the inputs are air temperature,  wind velocity and humidity, then there would be three inputs to the net- work.  To experiment with a two-input neuron, use the Neural Network Design  Demonstration Two-Input Neuron  nnd2n2 .  2-8   Network Architectures  Network Architectures  Commonly one neuron, even with many inputs, may not be sufficient. We  might need five or ten, operating in parallel, in what we will call a “layer.”  This concept of a layer is discussed below.  2  Layer   neurons is shown in Figure 2.7. Note that each   inputs is connected to each of the neurons and that the weight ma-  S  A Layer of Neurons A single-layer network of  of the  trix now has    rows.  R  S  Inputs  Layer of S Neurons  p1  p2  p3  pR  w1,1  wS, R  a1  a2  aS  1  b1   cid:0  cid:0 Σ n1  cid:0  cid:0   cid:0  cid:0 Σ n2  cid:0  cid:0   cid:0  cid:0 Σ nS  cid:0  cid:0   b2  bS  1   cid:0 f  cid:0   cid:0  f  cid:0   cid:0  f  cid:0   1  a = f Wp + b   Figure 2.7  Layer of S Neurons  The layer includes the weight matrix, the summers, the bias vector  transfer function boxes and the output vector  inputs as another layer, but we will not do that here.   , the  . Some authors refer to the   b  a  Each element of the input vector  weight matrix  tion   and an output  a  . Each neuron has a bias    is connected to each neuron through the  , a summer, a transfer func- . Taken together, the outputs form the output vector   W  ai  bi  .   p  f  It is common for the number of inputs to a layer to be different from the  number of neurons  i.e.,   R S   .   You might ask if all the neurons in a layer must have the same transfer  function. The answer is no; you can define a single  composite  layer of neu- rons having different transfer functions by combining two of the networks   2-9   2 Neuron Model and Network Architectures  shown above in parallel. Both networks would have the same inputs, and  each network would create some of the outputs.  The input vector elements enter the network through the weight matrix  W  :  W  =  w1 1 w1 2  w1 R w2 1 w2 2  w2 R    wS 1 wS 2  wS R  .   2.6   As noted previously, the row indices of the elements of matrix   indicate  the destination neuron associated with that weight, while the column indi- ces indicate the source of the input for that weight. Thus, the indices in   say that this weight represents the connection to the third neuron  w3 2 from the second source.   W  Fortunately, the S-neuron, R-input, one-layer network also can be drawn in  abbreviated notation, as shown in Figure 2.8.  Input  Layer of S Neurons  p R x 1  1  S x R  n cid:0 W  cid:0 b  cid:0   S x 1  S x 1  f  a  cid:0  cid:0  S x 1  cid:0  cid:0   cid:0  cid:0   S  R  a = f Wp + b   Figure 2.8  Layer of   S   Neurons, Abbreviated Notation  Here again, the symbols below the variables tell you that for this layer,  is a vector of length  length  summation and multiplication operations, the bias vector  function boxes and the output vector.    p  are vectors of  . As defined previously, the layer includes the weight matrix, the  , the transfer    matrix, and    is an   R W   and   S R  ,   b  b  a  S  Multiple Layers of Neurons Now consider a network with several layers. Each layer has its own weight  matrix   and an output vector  . We need to introduce some additional notation to distinguish between  a  , its own bias vector   , a net input vector   W  b  n  2-10   Network Architectures  Layer Superscript  these layers. We will use superscripts to identify the layers. Specifically, we  append the number of the layer as a superscript to the names for each of  W1 these variables. Thus, the weight matrix for the first layer is written as and the weight matrix for the second layer is written as  is used in the three-layer network shown in Figure 2.9.  ,  . This notation   W2  2  w 11,1  a11  w 21,1  a21  w 31,1  Inputs  First Layer  p1  p2  p3  pR  1  b11   cid:0  cid:0 Σ n11  cid:0  cid:0   cid:0  cid:0 Σ n12  cid:0  cid:0   cid:0  cid:0 Σ n1S 1  cid:0  cid:0  b1S 1  b12  1   cid:0 f 1  cid:0   cid:0 f 1  cid:0   cid:0  f 1  cid:0   1  a12  a1S 1  w 1S 1, R  Second Layer  1  b21   cid:0  cid:0 Σ n21  cid:0  cid:0   cid:0  cid:0 Σ n22  cid:0  cid:0   cid:0  cid:0 Σ n2S 2  cid:0  cid:0  b2S 2  b22  1   cid:0 f 2  cid:0   cid:0  f 2  cid:0   cid:0  f 2  cid:0   1  a22  a2S 2  w 2S 2, S 1  w 3S 3, S 2  Third Layer  1  b31   cid:0  cid:0 Σ n31  cid:0  cid:0   cid:0  cid:0 Σ n32  cid:0  cid:0   cid:0  cid:0 Σ n3S 3  cid:0  cid:0   b3S 3  b32  1   cid:0 f 3  cid:0   cid:0 f 3  cid:0   cid:0  f 3  cid:0   1  a31  a32  a3S 3  a1 = f 1  W1p + b1   a2 = f 2  W2a1 + b2   a3 = f 3  W3a2 + b3   a3 = f 3  W3f 2  W2f 1  W1p + b1  + b2  + b3   Figure 2.9  Three-Layer Network  As shown, there are   neurons in  the second layer, etc. As noted, different layers can have different numbers  of neurons.    neurons in the first layer,    inputs,   R  S1  S2  The outputs of layers one and two are the inputs for layers two and three.  Thus layer 2 can be viewed as a one-layer network with  S a1  S2  neurons, and an  a2 .   S2= , and the output is   . The input to layer 2 is    weight matrix    inputs,   S1  W2   =   S1  R  Output Layer Hidden Layers  A layer whose output is the network output is called an output layer. The  other layers are called hidden layers. The network shown above has an out- put layer  layer 3  and two hidden layers  layers 1 and 2 .  The same three-layer network discussed previously also can be drawn us- ing our abbreviated notation, as shown in Figure 2.10.  2-11   2 Neuron Model and Network Architectures  Input  First Layer  Second Layer  Third Layer  p R x 1  1  R   cid:0  W1  cid:0  S1 x R  cid:0 b1  cid:0   S1 x 1  n1 S1 x 1  f 1   cid:0  cid:0  a1  cid:0  cid:0  S1 x 1  cid:0  cid:0  1  cid:0  cid:0   S1   cid:0  W2  cid:0  S2 x S1  cid:0 b2  cid:0   S2 x 1   cid:0  cid:0  a2  cid:0  cid:0  S2 x 1 n2 S2 x 1  cid:0  cid:0  1  cid:0  cid:0   f 2  W3 S3 x S2   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0 b3  cid:0  cid:0   S3 x 1  S2   cid:0  cid:0  a3  cid:0  cid:0  S3 x 1 n3 S3 x 1  cid:0  cid:0   cid:0  cid:0   f 3  S3  a1 = f 1  W1p + b1   a2 = f 2  W2a1 + b2   a3 = f 3  W3a2 + b3   a3 = f 3  W3 f 2  W2f 1  W1p + b1  + b2   + b3   Figure 2.10  Three-Layer Network, Abbreviated Notation  Multilayer networks are more powerful than single-layer networks. For in- stance, a two-layer network having a sigmoid first layer and a linear second  layer can be trained to approximate most functions arbitrarily well. Single- layer networks cannot do this.  At this point the number of choices to be made in specifying a network may  look overwhelming, so let us consider this topic. The problem is not as bad  as it looks. First, recall that the number of inputs to the network and the  number of outputs from the network are defined by external problem spec- ifications. So if there are four external variables to be used as inputs, there  are four inputs to the network. Similarly, if there are to be seven outputs  from the network, there must be seven neurons in the output layer. Finally,  the desired characteristics of the output signal also help to select the trans- fer function for the output layer. If an output is to be either  , then  a symmetrical hard limit transfer function should be used. Thus, the archi- tecture of a single-layer network is almost completely determined by prob- lem specifications, including the specific number of inputs and outputs and  the particular output signal characteristic.   or   1–  1  Now, what if we have more than two layers? Here the external problem  does not tell you directly the number of neurons required in the hidden lay- ers. In fact, there are few problems for which one can predict the optimal  number of neurons needed in a hidden layer. This problem is an active area  of research. We will develop some feeling on this matter as we proceed to  Chapter 11, Backpropagation.  As for the number of layers, most practical neural networks have just two  or three layers. Four or more layers are used rarely.  We should say something about the use of biases. One can choose neurons  with or without biases. The bias gives the network an extra variable, and  so you might expect that networks with biases would be more powerful   2-12   Network Architectures  than those without, and that is true. Note, for instance, that a neuron with- out a bias will always have a net input   of zero when the network inputs   are zero. This may not be desirable and can be avoided by the use of a  p bias. The effect of the bias is discussed more fully in Chapters 3, 4 and 5.   n  In later chapters we will omit a bias in some examples or demonstrations.  In some cases this is done simply to reduce the number of network param- eters. With just two variables, we can plot system convergence in a two-di- mensional plane. Three or more variables are difficult to display.  2  Recurrent Networks Before we discuss recurrent networks, we need to introduce some simple  building blocks. The first is the delay block, which is illustrated in Figure  2.11.  Delay  Figure 2.11  Delay Block  The delay output   a t    is computed from its input   u t    according to  a t   =  u t  1–    .   2.7   Thus the output is the input delayed by one time step.  This assumes that  time is updated in discrete steps and takes on only integer values.  Eq.  2.7   requires that the output be initialized at time  . This initial condition  is indicated in Figure 2.11 by the arrow coming into the bottom of the delay  block.  0=  t  Integrator  Another related building block, which we will use for the continuous-time  recurrent networks in Chapters 18–21, is the integrator, which is shown in  Figure 2.12.  Delay  D   cid:0  cid:0  u t  a t   cid:0  cid:0  a 0   a t  = u t - 1   2-13   2 Neuron Model and Network Architectures  Integrator  u t   a t   a 0   t  0  t    0  a t  =   u τ  dτ + a 0   Figure 2.12  Integrator Block  Recurrent Network  The integrator output   a t    is computed from its input   u t    according to  a t   =  u   d  +  a 0   .   2.8   The initial condition  of the integrator block.  a 0    is indicated by the arrow coming into the bottom   We are now ready to introduce recurrent networks. A recurrent network is  a network with feedback; some of its outputs are connected to its inputs.  This is quite different from the networks that we have studied thus far,  which were strictly feedforward with no backward connections. One type of  discrete-time recurrent network is shown in Figure 2.13.   Initial  Condition  Recurrent Layer  p S x 1  1  S   cid:0  W S x S  cid:0  b  cid:0  S x 1   cid:0  cid:0   cid:0  cid:0   D  a t  S x 1  n t + 1  S x 1   cid:0  cid:0  a t + 1   cid:0  cid:0  S x 1  cid:0  cid:0   S  a 0  = p      a t + 1  = satlins  Wa t  + b   Figure 2.13  Recurrent Network  2-14   Network Architectures  In this particular network the vector  a 0  p= outputs:   supplies the initial conditions  i.e.,   . Then future outputs of the network are computed from previous   p  a 1   =  satlins Wa 0  b+      ,   a 2   =  satlins Wa 1  b+      , . . .  Recurrent networks are potentially more powerful than feedforward net- works and can exhibit temporal behavior. These types of networks are dis- cussed in Chapters 3, 14 and 18–21.  2  2-15   2 Neuron Model and Network Architectures  Summary of Results  Single-Input Neuron  Multiple-Input Neuron  Inputs  General Neuron  p   cid:0  cid:0 Σ w n  cid:0  cid:0   b  f   cid:0  cid:0  a  cid:0  cid:0   1 a = f  wp + b   Inputs  Multiple-Input Neuron  p1 p2p3  pR  w1, 1  cid:0  cid:0 Σ n  cid:0  cid:0  w1, R  b   cid:0  cid:0 f a  cid:0  cid:0   1 a = f  Wp + b   Input  Multiple-Input Neuron   cid:0  cid:0  p  cid:0  cid:0  W R x 1 1 x R  cid:0  cid:0 b 1  cid:0  cid:0   1 x 1  R  n 1 x 1   cid:0  cid:0  a  cid:0  cid:0  1 x 1  cid:0  cid:0   cid:0  cid:0   f  1  a = f  Wp + b   2-16   Summary of Results  Transfer Functions  Name  Input Output Relation  Icon  MATLAB Function  2  Hard Limit  Symmetrical Hard Limit  a a  = =  0 1  n n  0 0  a a  –= 1 = +1  n n  0 0  Linear  a  n=  Saturating Linear  Symmetric Saturating   Linear  Log-Sigmoid  Hyperbolic Tangent   Sigmoid  a a a  a a a  = = =  0 n 1  n 0 n  0   n 1  1  1  –= = n = 1  n 1– n  1–   n 1  1  a  =  1 ---------------- e n–+ 1  a  =  e n–– en ------------------ e n–+ en  Positive Linear  a  = a  0 =  n  n  0 0  n  Competitive  a a  = =  1    neuron with max n 0    all other neurons   cid:0  cid:0   hardlim   cid:0  cid:0   hardlims   cid:0  cid:0   purelin   cid:0  cid:0   satlin   cid:0  cid:0   satlins   cid:0  cid:0   logsig   cid:0  cid:0   tansig   cid:0  cid:0   cid:0  C  cid:0   poslin  compet  2-17   2 Neuron Model and Network Architectures  Layer of Neurons  Input  Layer of S Neurons  p R x 1  1  S x R  n cid:0 W  cid:0 b  cid:0   S x 1  S x 1  f  a  cid:0  cid:0  S x 1  cid:0  cid:0   cid:0  cid:0   S  R  a = f Wp + b   Three Layers of Neurons  Input  First Layer  Second Layer  Third Layer  p R x 1  1  R   cid:0  W1  cid:0  S1 x R  cid:0 b1  cid:0   S1 x 1  n1 S1 x 1  f 1   cid:0  cid:0  a1  cid:0  cid:0  S1 x 1  cid:0  cid:0  1  cid:0  cid:0   S1   cid:0  W2  cid:0  S2 x S1  cid:0 b2  cid:0   S2 x 1   cid:0  cid:0  a2  cid:0  cid:0  S2 x 1 n2 S2 x 1  cid:0  cid:0  1  cid:0  cid:0   f 2  W3 S3 x S2   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0 b3  cid:0  cid:0   S3 x 1  S2   cid:0  cid:0  a3  cid:0  cid:0  S3 x 1 n3 S3 x 1  cid:0  cid:0   cid:0  cid:0   f 3  S3  a1 = f 1  W1p + b1   a2 = f 2  W2a1 + b2   a3 = f 3  W3a2 + b3   a3 = f 3  W3 f 2  W2f 1  W1p + b1  + b2   + b3   Delay  Delay  D   cid:0  cid:0  u t  a t   cid:0  cid:0  a 0   a t  = u t - 1   2-18   Summary of Results  Integrator  Integrator  u t   a t   a 0   a t  =   u τ  dτ + a 0   t  0  2  Recurrent Network  Initial  Condition  Recurrent Layer  p S x 1  1  S   cid:0  W S x S  cid:0  b  cid:0  S x 1   cid:0  cid:0   cid:0  cid:0   D  a t  S x 1  n t + 1  S x 1   cid:0  cid:0  a t + 1   cid:0  cid:0  S x 1  cid:0  cid:0   S  a 0  = p      a t + 1  = satlins  Wa t  + b   How to Pick an Architecture  Problem specifications help define the network in the following ways:  1. Number of network inputs = number of problem inputs  2. Number of neurons in output layer = number of problem outputs  3. Output layer transfer function choice at least partly determined by   problem specification of the outputs  2-19   2 Neuron Model and Network Architectures  Solved Problems  P2.1 The input to a single-input neuron is 2.0, its weight is 2.3 and its   bias is -3.   i. What is the net input to the transfer function?  ii. What is the neuron output?  i. The net input is given by:  n  =  wp  b+  =  2.3   2   +  3–    =  1.6  ii. The output cannot be determined because the transfer function is not  specified.  P2.2 What is the output of the neuron of P2.1 if it has the following   transfer functions?  i. Hard limit   ii. Linear  iii. Log-sigmoid  i. For the hard limit transfer function:  ii. For the linear transfer function:  a  =  hardlim 1.6    =  1.0  a  =  purelin 1.6    =  1.6  iii. For the log-sigmoid transfer function:  a  =  logsig 1.6    =  =  0.8320  1 ------------------- e 1.6–+ 1  » 2 + 2 ans =       4  Verify this result using MATLAB and the function logsig, which is in the  MININNET directory  see Appendix B .  P2.3 Given a two-input neuron with the following parameters:   b  =  1.2  ,   =   and   W lowing transfer functions:  5– 6  3 2  p  =  T  , calculate the neuron output for the fol-  i. A symmetrical hard limit transfer function  ii. A saturating linear transfer function  2-20   Solved Problems  iii. A hyperbolic tangent sigmoid  tansig  transfer function  First calculate the net input   n  :  n Wp b+  =  =  3 2  +  1.2    =  1.8–  .  5– 6  Now find the outputs for each of the transfer functions.  i.   a  =  hardlims  1.8–    1–=  ii.   a  =  satlin  1.8–    =  0  iii.   a  =  tansig  1.8–    =  –  0.9468  2  P2.4 A single-layer neural network is to have six inputs and two out- puts. The outputs are to be limited to and continuous over the  range 0 to 1. What can you tell about the network architecture?  Specifically:  i. How many neurons are required?  ii. What are the dimensions of the weight matrix?  iii. What kind of transfer functions could be used?  iv. Is a bias required?  The problem specifications allow you to say the following about the net- work.  i. Two neurons, one for each output, are required.  ii. The weight matrix has two rows corresponding to the two neurons and  six columns corresponding to the six inputs.  The product   is a two-el- ement vector.   Wp  iii. Of the transfer functions we have discussed, the  tion would be most appropriate.   logsig   transfer func-  iv. Not enough information is given to determine if a bias is required.  2-21   2 Neuron Model and Network Architectures  Epilogue  This chapter has introduced a simple artificial neuron and has illustrated  how different neural networks can be created by connecting groups of neu- rons in various ways. One of the main objectives of this chapter has been to  introduce our basic notation. As the networks are discussed in more detail  in later chapters, you may wish to return to Chapter 2 to refresh your mem- ory of the appropriate notation.  This chapter was not meant to be a complete presentation of the networks  we have discussed here. That will be done in the chapters that follow. We  will begin in Chapter 3, which will present a simple example that uses  some of the networks described in this chapter, and will give you an oppor- tunity to see these networks in action. The networks demonstrated in  Chapter 3 are representative of the types of networks that are covered in  the remainder of this text.  2-22   Exercises  Exercises  E2.1 A single input neuron has a weight of 1.3 and a bias of 3.0. What possible   kinds of transfer functions, from Table 2.1, could this neuron have, if its  output is given below. In each case, give the value of the input that would  produce these outputs.  2  i. 1.6  ii. 1.0  iii. 0.9963  iv. -1.0  E2.2 Consider a single-input neuron with a bias. We would like the output to be   -1 for inputs less than 3 and +1 for inputs greater than or equal to 3.  i. What kind of a transfer function is required?  ii. What bias would you suggest? Is your bias in any way related to the   input weight? If yes, how?  iii. Summarize your network by naming the transfer function and stat- ing the bias and the weight. Draw a diagram of the network. Verify  the network performance using MATLAB.  » 2 + 2 ans =       4  E2.3 Given a two-input neuron with the following weight matrix and input vec- , we would like to have an output of 0.5. Do  tor:  you suppose that there is a combination of bias and transfer function that  might allow this?   and   5– 7  3 2  W  p  =  =  T  i. Is there a transfer function from Table 2.1 that will do the job if the   bias is zero?  ii. Is there a bias that will do the job if the linear transfer function is   used? If yes, what is it?  iii. Is there a bias that will do the job if a log-sigmoid transfer function   is used? Again, if yes, what is it?  iv. Is there a bias that will do the job if a symmetrical hard limit trans-  fer function is used? Again, if yes, what is it?  E2.4 A two-layer neural network is to have four inputs and six outputs. The   range of the outputs is to be continuous between 0 and 1. What can you tell  about the network architecture? Specifically:  2-23   2 Neuron Model and Network Architectures  i. How many neurons are required in each layer?  ii. What are the dimensions of the first-layer and second-layer weight   matrices?  iii. What kinds of transfer functions can be used in each layer?  iv. Are biases required in either layer?  E2.5 Consider the following neuron.  Input  General Neuron  p  w  n  a  f   cid:2   b  1  a  =    f  wp  +   b  Figure P15.1  General Neuron  Sketch the neuron response  plot a versus p for -2<p<2  for the following  cases.  i.  ii.  iii.  iv.  v.  w  w  w  w  w  1=  ,   b  1=  ,   f  =  hardlims  .  1–=  ,   b  1=  ,   f  =  hardlims  .  2=  ,   b  3=  ,   f  purelin  .  2=  ,   b  3=  ,   f  satlins  .  =  =  2–=  ,   b  1–=  ,   f  =  poslin  .  2-24   Exercises  E2.6 Consider the following neural network.  Input  Sat. Linear Layer  Linear Layer  w1  1,1  p  w1  2,1  1  n 1  1  n 2  1  b 1  1  b 2   cid:2   1  cid:2   1  1  a 1  1  a 2  w2  1,1  w2  1,2   cid:2   1  2  b 1  2  n 1  2  a 1  1 a  =  satlin W b  +    p     1  1  2 a  =  2 purelin W a b  +       1  2  1  w1 1  2=  ,   1  w2 1  1=  ,   1 b1  2=  ,   1 b2  1–=  ,   2  w1 1  1=  ,   2  w1 2  1–=  ,   2 b1  0=  Sketch the following responses  plot the indicated variable versus p for  3–      .  p  2  3 1 n1 1 a1 1 n2 1 a2 2 n1 2 a1  .  .  .  .  .  i.  ii.  iii.  iv.  v.  vi.  2-25   Objectives  3 An Illustrative Example  Objectives  Two-Input Case Pattern Recognition Example  Objectives Theory and Examples  Problem Statement Perceptron  Hamming Network  Feedforward Layer Recurrent Layer  Hopfield Network  Epilogue Exercises  3-1 3-2 3-2 3-3 3-4 3-5 3-8 3-9 3-10 3-12 3-15 3-16  3  Think of this chapter as a preview of coming attractions. We will take a  simple pattern recognition problem and show how it can be solved using  three different neural network architectures. It will be an opportunity to  see how the architectures described in the previous chapter can be used to  solve a practical  although extremely oversimplified  problem. Do not ex- pect to completely understand these three networks after reading this  chapter. We present them simply to give you a taste of what can be done  with neural networks, and to demonstrate that there are many different  types of networks that can be used to solve a given problem.  The three networks presented in this chapter are representative of the  types of networks discussed in the remaining chapters: feedforward net- works  represented here by the perceptron , competitive networks  repre- sented here by the Hamming network  and recurrent associative memory  networks  represented here by the Hopfield network .  3-1   3 An Illustrative Example  Theory and Examples  Problem Statement  A produce dealer has a warehouse that stores a variety of fruits and vege- tables. When fruit is brought to the warehouse, various types of fruit may  be mixed together. The dealer wants a machine that will sort the fruit ac- cording to type. There is a conveyer belt on which the fruit is loaded. This  conveyer passes through a set of sensors, which measure three properties  of the fruit: shape, texture and weight. These sensors are somewhat primi- tive. The shape sensor will output a 1 if the fruit is approximately round  and a   if it is more elliptical. The texture sensor will output a 1 if the sur- face of the fruit is smooth and a  output a 1 if the fruit is more than one pound and a  pound.   if it is rough. The weight sensor will    if it is less than one   1–  1–  1–  The three sensor outputs will then be input to a neural network. The pur- pose of the network is to decide which kind of fruit is on the conveyor, so  that the fruit can be directed to the correct storage bin. To make the prob- lem even simpler, let’s assume that there are only two kinds of fruit on the  conveyor: apples and oranges.   Neural Network  Sensors  Sorter  Apples Oranges  As each fruit passes through the sensors it can be represented by a three- dimensional vector. The first element of the vector will represent shape,  the second element will represent texture and the third element will repre- sent weight:  3-2   Perceptron  p  =  shape texture weight  .  p1  =  p2  =  ,  .  1 1– 1–  1 1 1–  Therefore, a prototype orange would be represented by  and a prototype apple would be represented by   3.2   3   3.1    3.3   The neural network will receive one three-dimensional input vector for  each fruit on the conveyer and must make a decision as to whether the fruit  is an orange    or an apple   .       p1  p2  Now that we have defined this simple  trivial?  pattern recognition prob- lem, let’s look briefly at three different neural networks that could be used  to solve it. The simplicity of our problem will facilitate our understanding  of the operation of the networks.  Perceptron  The first network we will discuss is the perceptron. Figure 3.1 illustrates a  single-layer perceptron with a symmetric hard limit transfer function hard- lims.  Inputs  - Title -   cid:0  cid:0  p  cid:0  cid:0  W R x 1 S x R  cid:0  cid:0 b 1  cid:0  cid:0   Sym. Hard Limit Layer  cid:0   cid:0   cid:0   cid:0  S a = hardlims  Wp + b   - Exp -  n S x 1  S x 1  a S x 1  Figure 3.1   Single-Layer Perceptron  R  3-3   3 An Illustrative Example  Two-Input Case Before we use the perceptron to solve the orange and apple recognition  problem  which will require a three-input perceptron, i.e.,   , it is use- ful to investigate the capabilities of a two-input single-neuron perceptron     , which can be easily analyzed graphically. The two-input percep- R tron is shown in Figure 3.2.  2=  3=  R  Inputs  Two-Input Neuron  p1  p2   cid:0  cid:0 Σ w1,1 n  cid:0  cid:0  w1,2  b   cid:0  cid:0  a  cid:0  cid:0   1  a = hardlims  Wp + b   Figure 3.2  Two-Input Single-Neuron Perceptron  Single-neuron perceptrons can classify input vectors into two categories.  For example, for a two-input perceptron, if   then   and   1–=  1=  w1 1  w1 2  a  =  hardlims n   =  hardlims    1– 1 p b+    .   3.4   Therefore, if the inner product of the weight matrix  a single row vector in  this case  with the input vector is greater than or equal to  , the output  will be 1. If the inner product of the weight vector and the input is less than  . This divides the input space into two parts. Fig- b– . The blue line in the fig- ure 3.3 illustrates this for the case where  1–= ure represents all points for which the net input  n  , the output will be    is equal to 0:  1–  b–  b     n  =  1– 1 p 1–  =  0  .   3.5   Notice that this decision boundary will always be orthogonal to the weight  matrix, and the position of the boundary can be shifted by changing  .  In  the general case,   is a matrix consisting of a number of row vectors, each  of which will be used in an equation like Eq.  3.5 . There will be one bound- ary for each row of  . See Chapter 4 for more on this topic.  The shaded  region contains all input vectors for which the output of the network will  be 1. The output will be    for all other input vectors.  W  W  1–  b  3-4   Perceptron  W  n > 0  -1  p2  1  n < 0  3  p1  1  Figure 3.3   Perceptron Decision Boundary  The key property of the single-neuron perceptron, therefore, is that it can  separate input vectors into two categories. The decision boundary between  the categories is determined by the equation  Wp b+  0=  .   3.6   Because the boundary must be linear, the single-layer perceptron can only  be used to recognize patterns that are linearly separable  can be separated  by a linear boundary . These concepts will be discussed in more detail in  Chapter 4.  Pattern Recognition Example Now consider the apple and orange pattern recognition problem. Because  there are only two categories, we can use a single-neuron perceptron. The  vector inputs are three-dimensional   equation will be   , therefore the perceptron   3=  R  a  =  hardlims w1 1 w1 2 w1 3   3.7         p1 p2 p3     b+    .  b   and the elements of the weight matrix so that  We want to choose the bias  the perceptron will be able to distinguish between apples and oranges. For  example, we may want the output of the perceptron to be 1 when an apple  is input and   when an orange is input. Using the concept illustrated in  Figure 3.3, let’s find a linear boundary that can separate oranges and ap-  1–  3-5   3 An Illustrative Example  ples. The two prototype vectors  recall Eq.  3.2  and Eq.  3.3   are shown in  Figure 3.4. From this figure we can see that the linear boundary that di- vides these two vectors symmetrically is the    plane.    p1 p3  p3  p2    p1 p3  The  the equation  or  p1  p1   orange   p2  apple   Figure 3.4   Prototype Vectors   plane, which will be our decision boundary, can be described by   p2  0=  ,  0 1 0  0+  0=  .  p1 p2 p3   3.8    3.9   Therefore the weight matrix and bias will be  W  =  0 1 0  ,   b  0=  .   3.10   The weight matrix is orthogonal to the decision boundary and points to- ward the region that contains the prototype pattern  want the perceptron to produce an output of 1. The bias is 0 because the  decision boundary passes through the origin.    apple  for which we   p2  Now let’s test the operation of our perceptron pattern classifier. It classifies  perfect apples and oranges correctly since  3-6   Perceptron  Orange:  Apple:  a  =  hardlims 0 1 0  =  –   1 orange    ,   3.11   a  =  hardlims 0 1 0  =   1 apple    .   3.12   But what happens if we put a not-so-perfect orange into the classifier? Let’s  say that an orange with an elliptical shape is passed through the sensors.  The input vector would then be  3   3.13                     1 1– 1–     0+    1 1 1–     0+    p  =  .  1– 1– 1–  1– 1– 1–     0+    The response of the network would be  a  =  hardlims 0 1 0  =  –   1 orange    .   3.14   In fact, any input vector that is closer to the orange prototype vector than  to the apple prototype vector  in Euclidean distance  will be classified as an  orange  and vice versa .  To experiment with the perceptron network and the apple orange classifica- tion problem, use the Neural Network Design Demonstration Perceptron  Classification  nnd3pc .  This example has demonstrated some of the features of the perceptron net- work, but by no means have we exhausted our investigation of perceptrons.  This network, and variations on it, will be examined in Chapters 4 through  13. Let’s consider some of these future topics.  In the apple orange example we were able to design a network graphically,  by choosing a decision boundary that clearly separated the patterns. What  about practical problems, with high dimensional input spaces? In Chapters  4, 7, 10 and 11 we will introduce learning algorithms that can be used to   3-7   3 An Illustrative Example  train networks to solve complex problems by using a set of examples of  proper network behavior.  The key characteristic of the single-layer perceptron is that it creates linear  decision boundaries to separate categories of input vector. What if we have  categories that cannot be separated by linear boundaries? This question  will be addressed in Chapter 11, where we will introduce the multilayer  perceptron. The multilayer networks are able to solve classification prob- lems of arbitrary complexity.  Hamming Network  The next network we will consider is the Hamming network [Lipp87]. It  was designed explicitly to solve binary pattern recognition problems   where each element of the input vector has only two possible values — in  our example 1 or   . This is an interesting network, because it uses both  feedforward and recurrent  feedback  layers, which were both described in  Chapter 2. Figure 3.5 shows the standard Hamming network. Note that  the number of neurons in the first layer is the same as the number of neu- rons in the second layer.  1–  The objective of the Hamming network is to decide which prototype vector  is closest to the input vector. This decision is indicated by the output of the  recurrent layer. There is one neuron in the recurrent layer for each proto- type pattern. When the recurrent layer converges, there will be only one  neuron with nonzero output. This neuron indicates the prototype pattern  that is closest to the input vector. Now let’s investigate the two layers of the  Hamming network in detail.  Feedforward Layer  Recurrent Layer   cid:0  cid:0  p W1  cid:0  cid:0  R x 1 S x R  cid:0  cid:0 b1 1  cid:0  cid:0   S x 1  R  n1 S x 1  a1 S x 1   cid:0   cid:0   cid:0   cid:0  S   cid:0  cid:0   cid:0  cid:0   W2 S x S  n2 t + 1   S x 1   cid:0  cid:0   cid:0  cid:0  a2 t + 1   cid:0  cid:0   cid:0  cid:0   S x 1  S   cid:0  cid:0 D  cid:0  cid:0   a2 t   S x 1  a1 = purelin  W1p + b1   - Exp 1 -  a2 0  = a1      a2 t + 1  = poslin  W2a2 t    Figure 3.5   Hamming Network  3-8   Hamming Network  Feedforward Layer The feedforward layer performs a correlation, or inner product, between  each of the prototype patterns and the input pattern  as we will see in Eq.   3.17  . In order for the feedforward layer to perform this correlation, the  rows of the weight matrix in the feedforward layer, represented by the con- nection matrix  , are set to the prototype patterns. For our apple and or- ange example this would mean  W1  W1  =  p1 T p2 T  =  1 1– 1 1  1– 1–  .  3   3.15   The feedforward layer uses a linear transfer function, and each element of  the bias vector is equal to   is the number of elements in the in- put vector. For our example the bias vector would be  , where   R  R  b1  =  .  3 3   3.16    3.17   With these choices for the weight matrix and bias vector, the output of the  feedforward layer is  a1 W1p b1+  =  =  T p1 T p2  p  +  =  3 3  Tp 3+ p1 Tp 3+ p2  .  Note that the outputs of the feedforward layer are equal to the inner prod- ucts of each prototype pattern with the input, plus  . For two vectors of  equal length  norm , their inner product will be largest when the vectors  point in the same direction, and will be smallest when they point in oppo- site directions.  We will discuss this concept in more depth in Chapters 5,  8 and 9.  By adding   to the inner product we guarantee that the outputs  of the feedforward layer can never be negative. This is required for proper  operation of the recurrent layer.  R  R  This network is called the Hamming network because the neuron in the  feedforward layer with the largest output will correspond to the prototype  pattern that is closest in Hamming distance to the input pattern.  The  Hamming distance between two vectors is equal to the number of elements  that are different. It is defined only for binary vectors.  We leave it to the  reader to show that the outputs of the feedforward layer are equal to    minus twice the Hamming distances from the prototype patterns to the in- put pattern.  2R  3-9   3 An Illustrative Example  Recurrent Layer The recurrent layer of the Hamming network is what is known as a “com- petitive” layer. The neurons in this layer are initialized with the outputs of  the feedforward layer, which indicate the correlation between the proto- type patterns and the input vector. Then the neurons compete with each  other to determine a winner. After the competition, only one neuron will  have a nonzero output. The winning neuron indicates which category of in- put was presented to the network  for our example the two categories are  apples and oranges . The equations that describe the competition are:  and  a2 0   a1=        Initial Condition ,  a2 t  1+    =  poslin W2a2 t       .   Don’t forget that the superscripts here indicate the layer number, not a  power of 2.  The   transfer function is linear for positive values and  zero for negative values. The weight matrix W2 has the form  poslin   3.18    3.19    3.20   W2  =  1 –  – 1  ,     is some number less than   where  rons in the recurrent layer.  Can you show why  1  1– S  1– S  , and    ?   1           is the number of neu-  S  must be less than   An iteration of the recurrent layer proceeds as follows:  a2 t  1+    =  poslin  =  poslin   3.21       1 –  – 1   a2 t          2 t  a1 2 t  a2  –  –  2 t  2 t   a2 a1       .  Each element is reduced by the same fraction of the other. The larger ele- ment will be reduced by less, and the smaller element will be reduced by  more, therefore the difference between large and small will be increased.  The effect of the recurrent layer is to zero out all neuron outputs, except the  one with the largest initial value  which corresponds to the prototype pat- tern that is closest in Hamming distance to the input .  To illustrate the operation of the Hamming network, consider again the ob- long orange that we used to test the perceptron:  3-10   Hamming Network  p  =  .  1– 1– 1–   3.22   The output of the feedforward layer will be  a1  =  1 1– 1 1  1– 1–  +  =  3 3  3+  1 3+ 1–      =  ,  4 2  1– 1– 1–   3.23   3  which will then become the initial condition for the recurrent layer.  The weight matrix for the recurrent layer will be given by Eq.  3.20  with    any number less than 1 would work . The first iteration of the re-  current layer produces  1 2  =  a2 1   =  poslin W2a2 0       =   3.24   poslin      1  0.5–  0.5–      4 2  1  =   poslin 3  0      3 0  poslin      1  0.5–  0.5–  1      3 0  poslin  3    1.5–      =  3 0  .  .                      The second iteration produces  a2 2   =  poslin W2a2 1       =   3.25   Since the outputs of successive iterations produce the same result, the net- work has converged. Prototype pattern number one, the orange, is chosen  as the correct match, since neuron number one has the only nonzero out- put.  Recall that the first element of  .  This is the correct  choice, since the Hamming distance from the orange prototype to the input  pattern is 1, and the Hamming distance from the apple prototype to the in- put pattern is 2.  Tp 3+ p1   was   a1      To experiment with the Hamming network and the apple orange classifica- tion problem, use the Neural Network Design Demonstration Hamming  Classification  nnd3hamc .  3-11   Hopfield Network  3 An Illustrative Example  There are a number of networks whose operation is based on the same prin- ciples as the Hamming network; that is, where an inner product operation   feedforward layer  is followed by a competitive dynamic layer. These com- petitive networks will be discussed in Chapters 15, 16, 18 and 19. They are  self-organizing networks, which can learn to adjust their prototype vectors  based on the inputs that have been presented.  The final network we will discuss in this brief preview is the Hopfield net- work. This is a recurrent network that is similar in some respects to the re- current layer of the Hamming network, but which can effectively perform  the operations of both layers of the Hamming network. A diagram of the  Hopfield network is shown in Figure 3.6.  This figure is actually a slight  variation of the standard Hopfield network. We use this variation because  it is somewhat simpler to describe and yet demonstrates the basic con- cepts.   The neurons in this network are initialized with the input vector, then the  network iterates until the output converges. When the network is operat- ing correctly, the resulting output should be one of the prototype vectors.  Therefore, whereas in the Hamming network the nonzero neuron indicates  which prototype pattern is chosen, the Hopfield network actually produces  the selected prototype pattern at its output.  Initial  Condition  Recurrent Layer  p S x 1  1  S   cid:0  W S x S  cid:0  b  cid:0  S x 1   cid:0  cid:0   cid:0  cid:0   D  a t  S x 1  n t + 1  S x 1   cid:0  cid:0  a t + 1   cid:0  cid:0  S x 1  cid:0  cid:0   S  a 0  = p      a t + 1  = satlins  Wa t  + b   Figure 3.6   Hopfield Network  The equations that describe the network operation are  a 0   p=   3.26   and  3-12   Hopfield Network  1+ a t    =  satlins Wa t  b+      ,   3.27   where   is the transfer function that is linear in the range [-1, 1] and  saturates at 1 for inputs greater than 1 and at -1 for inputs less than -1.  satlins  The design of the weight matrix and the bias vector for the Hopfield net- work is a more complex procedure than it is for the Hamming network,  where the weights in the feedforward layer are the prototype patterns.  Hopfield design procedures will be discussed in detail in Chapter 21.   To illustrate the operation of the network, we have determined a weight  matrix and a bias vector that can solve our orange and apple pattern rec- ognition problem. They are given in Eq.  3.28 .  3  W  =  0.2 0 0 0 1.2 0 0 0.2 0  b  =  0.9 0  0.9–   3.28   Although the procedure for computing the weights and biases for the  Hopfield network is beyond the scope of this chapter, we can say a few  things about why the parameters in Eq.  3.28  work for the apple and or- ange example.  p2  . In both patterns, the first element is   ,  We want the network output to converge to either the orange pattern,  p1 , and the  or the apple pattern,  third element is  . The difference between the patterns occurs in the sec- ond element. Therefore, no matter what pattern is input to the network, we  want the first element of the output pattern to converge to  , the third el- ement to converge to  ,  whichever is closer to the second element of the input vector.  , and the second element to go to either    or   1–  1–  1–  1  1  1  The equations of operation of the Hopfield network, using the parameters  given in Eq.  3.28 , are  a1 t  1+    satlins 0.2a1 t     0.9+    a2 t  1+    satlins 1.2a2 t       a3 t  1+    satlins 0.2a3 t     0.9–    =  =  =  Regardless of the initial values,  until it saturates at  urates at  Therefore, if it is initially negative, it will eventually saturate at  initially positive it will saturate at   , the first element will be increased  , and the third element will be decreased until it sat- .  ; if it is   . The second element is multiplied by a number larger than   ai 0   1–  1–  .   1  1  1   3.29   3-13   3 An Illustrative Example   It should be noted that this is not the only  You might want to try some others. See if you can discover what makes  these work.    pair that could be used.   W  b       ,  Let’s again take our oblong orange to test the Hopfield network. The out- puts of the Hopfield network for the first three iterations would be  a 0   =  ,   a 1   =  ,   a 2   =  ,   a 3   =   3.30   1– 1– 1–  0.7 1– 1–  1 1– 1–  1 1– 1–  The network has converged to the orange pattern, as did both the Hamming  network and the perceptron, although each network operated in a different  way. The perceptron had a single output, which could take on values of -1   orange  or 1  apple . In the Hamming network the single nonzero neuron in- dicated which prototype pattern had the closest match. If the first neuron  was nonzero, that indicated orange, and if the second neuron was nonzero,  that indicated apple. In the Hopfield network the prototype pattern itself  appears at the output of the network.  To experiment with the Hopfield network and the apple orange classifica- tion problem, use the Neural Network Design Demonstration Hopfield Clas- sification  nnd3hopc .  As with the other networks demonstrated in this chapter, do not expect to  feel completely comfortable with the Hopfield network at this point. There  are a number of questions that we have not discussed. For example, “How  do we know that the network will eventually converge?” It is possible for  recurrent networks to oscillate or to have chaotic behavior. In addition, we  have not discussed general procedures for designing the weight matrix and  the bias vector. These topics will be discussed in detail in Chapters 20 and  21.  3-14   Epilogue  Epilogue  3  The three networks that we have introduced in this chapter demonstrate  many of the characteristics that are found in the architectures which are  discussed throughout this book.   Feedforward networks, of which the perceptron is one example, are pre- sented in Chapters 4, 7, 11, 12, 13 and 17. In these networks, the output is  computed directly from the input in one pass; no feedback is involved.  Feedforward networks are used for pattern recognition, as in the apple and  orange example, and also for function approximation  see Chapter 11 .  Function approximation applications are found in such areas as adaptive  filtering  see Chapter 10  and automatic control.  Competitive networks, represented here by the Hamming network, are  characterized by two properties. First, they compute some measure of dis- tance between stored prototype patterns and the input pattern. Second,  they perform a competition to determine which neuron represents the pro- totype pattern closest to the input. In the competitive networks that are  discussed in Chapters 16, 18 and 19, the prototype patterns are adjusted  as new inputs are applied to the network. These adaptive networks learn  to cluster the inputs into different categories.  Recurrent networks, like the Hopfield network, were originally inspired by  statistical mechanics. They have been used as associative memories, in  which stored data is recalled by association with input data, rather than by  an address. They have also been used to solve a variety of optimization  problems. We will discuss these recurrent networks in Chapters 20 and 21.  We hope this chapter has piqued your curiosity about the capabilities of  neural networks and has raised some questions. A few of the questions we  will answer in later chapters are:  1. How do we determine the weight matrix and bias for perceptron net-  works with many inputs, where it is impossible to visualize the decision  boundary?  Chapters 4 and 10   2.  If the categories to be recognized are not linearly separable, can we ex- tend the standard perceptron to solve the problem?  Chapters 11, 12  and 13   3. Can we learn the weights and biases of the Hamming network when we   don’t know the prototype patterns?  Chapters 16, 18 and 19   4. How do we determine the weight matrix and bias vector for the   Hopfield network?  Chapter 21   5. How do we know that the Hopfield network will eventually converge?    Chapters 20 and 21   3-15   3 An Illustrative Example  Exercises  E3.1 In this chapter we have designed three different neural networks to distin- guish between apples and oranges, based on three sensor measurements   shape, texture and weight . Suppose that we want to distinguish between  bananas and pineapples:  p1  =    Banana   p2  =    Pineapple   1– 1 1–  1– 1– 1  i. Design a perceptron to recognize these patterns.  ii. Design a Hamming network to recognize these patterns.  iii. Design a Hopfield network to recognize these patterns.  iv. Test the operation of your networks by applying several different in-  put patterns. Discuss the advantages and disadvantages of each  network.  E3.2 Consider the following prototype patterns.  p1  =  ,   p2  =  1 0.5  2 1  i. Find and sketch a decision boundary for a perceptron network that   will recognize these two vectors.  ii. Find weights and bias which will produce the decision boundary you   found in part i, and sketch the network diagram.  iii. Calculate the network output for the following input. Is the network   response  decision  reasonable? Explain.  iv. Design a Hamming network to recognize the two prototype vectors   p  =  1 0  above.  3-16   Exercises  v. Calculate the network output for the Hamming network with the in- put vector given in part iii, showing all steps. Does the Hamming  network produce the same decision as the perceptron? Explain why  or why not. Which network is better suited to this problem? Explain.  E3.3 Consider a Hopfield network, with the following weight and bias.  W  =  1 1–  1– 1  ,   b  =  0 0  3  i. The following input  initial condition  is applied to the network.   Find the network response  show the network output at each itera- tion until the network converges .  p  =  0.9 1  ii. Draw a sketch indicating what region of the input space will con- verge to the same final output that you found in part i.  In other  words, for what other p vectors will the network converge to the  same final output?  Explain how you obtained your answer.  iii. What other prototypes will this network converge to, and what re- gions of the input space correspond to each prototype  sketch the re- gions . Explain how you obtained your answer.  E3.4 Consider the following perceptron network.  Inputs  - Title -   cid:0  cid:0  p  cid:0  cid:0  W R x 1 S x R  cid:0  cid:0 b 1  cid:0  cid:0   Sym. Hard Limit Layer  cid:0   cid:0   cid:0   cid:0  S a = hardlims  Wp + b   - Exp -  n S x 1  S x 1  R  a S x 1  W  =  b  =  1 1 1– 1  2– 0  i. How many different classes can this network classify?  ii. Draw a diagram illustrating the regions corresponding to each  class. Label each region with the corresponding network output.  iii. Calculate the network output for the following input.  3-17   3 An Illustrative Example  iv. Plot the input from part iii in your diagram from part ii, and verify   that it falls in the correctly labeled region.  E3.5 We want to design a perceptron network to output a 1 when either of these   two vectors are input to the network:  and to output a -1 when either of the following vectors are input to the net- work:  p  =  1 1–        1– 0  1 2  ,            1– 1  0 2  .      i. Find and sketch a decision boundary for a network that will solve   this problem.  ii. Find weights and biases that will produce the decision boundary   you found in part i.  Show all work.  iii. Draw the network diagram using abreviated notation.  iv. For each of the four vectors given above, calculate the net input, n,  and the network output, a, for the network you have designed.  Ver- ify that your network solves the problem.  v. Are there other weights and biases that would solve the problem?  If   so, would you consider your weights best?  Explain.  E3.6 We have the folowing two prototype vectors:  .        1– 1  1 1      i. Find and sketch a decision boundary for a perceptron network that   will recognize these two vectors.  ii. Find weights and bias that will produce the decision boundary you   found in part i.  iii. Draw the network diagram using abreviated notation.  3-18   iv. For the vector given below, calculate the net input, n, and the net- work output, a, for the network you have designed.  Does the net- work produce a good output? Explain.  Exercises  0.5 0.5–  v. Design a Hamming network to recognize the two vectors used in   vi. Calculate the network output for the Hamming network for the in- put vector given in part iv. Does the network produce a good output?  Explain.  vii. Design a Hopfield network to recognize the two vectors used in part   part i.  i.  viii. Calculate the network output for the Hopfield network for the input  vector given in part iv. Does the network produce a good output? Ex- plain.  3  E3.7 We want to design a Hamming network to recognize the following proto-  type vectors:  i. Find the weight matrices and bias vectors for the Hamming net-  work.  ii. Draw the network diagram.  iii. Apply the following input vector and calculate the total network re-  sponse  iterating the second layer to convergence . Explain the  meaning of the final network output.        1 1    1– 1–  1– 1  .      p  =  1 0  iv. Sketch the decision boundaries for this network. Explain how you   determined the boundaries.  3-19   Objectives  4 Perceptron Learning Rule  Objectives Theory and Examples  Learning Rules Perceptron Architecture  Single-Neuron Perceptron Multiple-Neuron Perceptron  Perceptron Learning Rule  Test Problem Constructing Learning Rules Unified Learning Rule Training Multiple-Neuron Perceptrons  Proof of Convergence  Notation Proof Limitations  Summary of Results Solved Problems Epilogue Further Reading Exercises  4-1 4-2 4-2 4-3 4-5 4-8 4-8 4-9 4-10 4-12 4-13 4-15 4-15 4-16 4-18 4-20 4-21 4-34 4-35 4-37  4  Objectives  One of the questions we raised in Chapter 3 was: “How do we determine the  weight matrix and bias for perceptron networks with many inputs, where  it is impossible to visualize the decision boundaries?” In this chapter we  will describe an algorithm for training perceptron networks, so that they  can learn to solve classification problems. We will begin by explaining what  a learning rule is and will then develop the perceptron learning rule. We  will conclude by discussing the advantages and limitations of the single- layer perceptron network. This discussion will lead us into future chapters.  4-1   4 Perceptron Learning Rule  Theory and Examples  In 1943, Warren McCulloch and Walter Pitts introduced one of the first ar- tificial neurons [McPi43]. The main feature of their neuron model is that a  weighted sum of input signals is compared to a threshold to determine the  neuron output. When the sum is greater than or equal to the threshold, the  output is 1. When the sum is less than the threshold, the output is 0. They  went on to show that networks of these neurons could, in principle, com- pute any arithmetic or logical function. Unlike biological networks, the pa- rameters of their networks had to be designed, as no training method was  available. However, the perceived connection between biology and digital  computers generated a great deal of interest.  In the late 1950s, Frank Rosenblatt and several other researchers devel- oped a class of neural networks called perceptrons. The neurons in these  networks were similar to those of McCulloch and Pitts. Rosenblatt’s key  contribution was the introduction of a learning rule for training perceptron  networks to solve pattern recognition problems [Rose58]. He proved that  his learning rule will always converge to the correct network weights, if  weights exist that solve the problem. Learning was simple and automatic.  Examples of proper behavior were presented to the network, which learned  from its mistakes. The perceptron could even learn when initialized with  random values for its weights and biases.  Unfortunately, the perceptron network is inherently limited. These limita- tions were widely publicized in the book Perceptrons [MiPa69] by Marvin  Minsky and Seymour Papert. They demonstrated that the perceptron net- works were incapable of implementing certain elementary functions. It  was not until the 1980s that these limitations were overcome with im- proved  multilayer  perceptron networks and associated learning rules. We  will discuss these improvements in Chapters 11 and 12.  Today the perceptron is still viewed as an important network. It remains a  fast and reliable network for the class of problems that it can solve. In ad- dition, an understanding of the operations of the perceptron provides a  good basis for understanding more complex networks. Thus, the perceptron  network, and its associated learning rule, are well worth discussing here.  In the remainder of this chapter we will define what we mean by a learning  rule, explain the perceptron network and learning rule, and discuss the  limitations of the perceptron network.  Learning Rules  Learning Rule  As we begin our presentation of the perceptron learning rule, we want to  discuss learning rules in general. By learning rule we mean a procedure for  modifying the weights and biases of a network.  This procedure may also   4-2   Perceptron Architecture  be referred to as a training algorithm.  The purpose of the learning rule is  to train the network to perform some task. There are many types of neural  network learning rules. They fall into three broad categories: supervised  learning, unsupervised learning and reinforcement  or graded  learning.   Supervised Learning Training Set  In supervised learning, the learning rule is provided with a set of examples   the training set  of proper network behavior:  Target  Reinforcement Learning  Unsupervised Learning  4  p1 t 1 { , }    { , }   pQ tQ p2 t2 { , }      ,   4.1   pq   is an input to the network and   where   target  output. As the inputs are applied to the network, the network out- puts are compared to the targets. The learning rule is then used to adjust  the weights and biases of the network in order to move the network outputs  closer to the targets. The perceptron learning rule falls in this supervised  learning category. We will also investigate supervised learning algorithms  in Chapters 7–14.   is the corresponding correct   tq  Reinforcement learning is similar to supervised learning, except that, in- stead of being provided with the correct output for each network input, the  algorithm is only given a grade. The grade  or score  is a measure of the net- work performance over some sequence of inputs. This type of learning is  currently much less common than supervised learning. It appears to be  most suited to control system applications  see [BaSu83], [WhSo92] .  In unsupervised learning, the weights and biases are modified in response  to network inputs only. There are no target outputs available. At first  glance this might seem to be impractical. How can you train a network if  you don’t know what it is supposed to do? Most of these algorithms perform  some kind of clustering operation. They learn to categorize the input pat- terns into a finite number of classes. This is especially useful in such appli- cations as vector quantization. We will see in Chapters 15–19 that there  are a number of unsupervised learning algorithms.  Perceptron Architecture  Before we present the perceptron learning rule, let’s expand our investiga- tion of the perceptron network, which we began in Chapter 3. The general  perceptron network is shown in Figure 4.1.  The output of the network is given by  a  =  hardlim Wp b+      .   4.2    Note that in Chapter 3 we used the   transfer function, instead of  hardlim. This does not affect the capabilities of the network. See Exercise  E4.10.   hardlims  4-3   4 Perceptron Learning Rule  Input  Hard Limit Layer  p R x 1  1  S x R  n cid:0 W  cid:0 b  cid:0   S x 1  S x 1  a  cid:0  cid:0  S x 1  cid:0  cid:0  S cid:0  cid:0   R  a = hardlim  Wp + b   Figure 4.1  Perceptron Network  W  =  w1 1 w1 2  w1 R w2 1 w2 2  w2 R    wS 1 wS 2  wS R  .  It will be useful in our development of the perceptron learning rule to be  able to conveniently reference individual elements of the network output.  Let’s see how this can be done. First, consider the network weight matrix:   4.3    4.4    4.5   We will define a vector composed of the elements of the ith row of   W  :  Now we can partition the weight matrix:  wi  =  W  =  .  .  wi 1 wi 2  wi R  wT 1 wT 2  wT S  This allows us to write the ith element of the network output vector as  ai  =  hardlim ni    =  hardlim wT    i  p bi+    .   4.6   4-4   Perceptron Architecture  a = hardlim  n   Recall that the   hardlim   transfer function  shown at left  is defined as:  a  =  hardlim n   =      0  1 if n 0 otherwise.   4.7   n = Wp + b  Therefore, if the inner product of the ith row of the weight matrix with the  input vector is greater than or equal to  , the output will be 1, otherwise  the output will be 0. Thus each neuron in the network divides the input  space into two regions. It is useful to investigate the boundaries between  these regions. We will begin with the simple case of a single-neuron percep- tron with two inputs.  bi–  Single-Neuron Perceptron Let’s consider a two-input perceptron with one neuron, as shown in Figure  4.2.   4  Inputs  Two-Input Neuron  p1  p2   cid:0  cid:0 Σ w1,1 n  cid:0  cid:0  w1,2  b   cid:0  cid:0  a  cid:0  cid:0   1  a = hardlim  Wp + b   Figure 4.2  Two-Input Single-Output Perceptron  The output of this network is determined by  a  =  hardlim n   =  hardlim Wp b+      =  hardlim wT    p b+    =  1  hardlim w1 1 p1 w1 2 p2  +    +  b    Decision Boundary  The decision boundary is determined by the input vectors for which the net  input    is zero:  n  n  =  wT  1  p b+  =  w1 1 p1 w1 2 p2  +  +  b  =  0  .  To make the example more concrete, let’s assign the following values for  the weights and bias:  w1 1  1=  ,   w1 2  1=  ,   b  1–=  .   4.10    4.8    4.9   4-5   4 Perceptron Learning Rule  The decision boundary is then  n  =  wT  1  p b+  =  w1 1 p1 w1 2 p2  +  +  b  =  p1  +  p2  1–  =  0  .   4.11   This defines a line in the input space. On one side of the line the network  output will be 0; on the line and on the other side of the line the output will  be 1. To draw the line, we can find the points where it intersects the   and  p2   axes. To find the    intercept set   0=  p1  p2  p1  :  p2  =  –  b ---------- w1 2  =  1– ------– 1  =  1  if p1  0=  To find the   p1   intercept, set   p2  0=  :  p1  =  –  b ---------- w1 1  =  1– ------– 1  =  1  if p2  0=  .  .   4.12    4.13   The resulting decision boundary is illustrated in Figure 4.3.  To find out which side of the boundary corresponds to an output of 1, we  just need to test one point. For the input  , the network output  will be  2 0  p  =  T  a  =  hardlim wT    p b+    1  =  hardlim 1 1  =  1  .   4.14         1–   2 0  Therefore, the network output will be 1 for the region above and to the right  of the decision boundary. This region is indicated by the shaded area in Fig- ure 4.3.  p2  a = 1  1w  1wTp + b = 0  1  a = 0  1  p1  4-6  Figure 4.3  Decision Boundary for Two-Input Perceptron   Perceptron Architecture  1w  1w  We can also find the decision boundary graphically. The first step is to note  that the boundary is always orthogonal to  , as illustrated in the adjacent  figures. The boundary is defined by  w1  wT 1  p b+  0=  .   4.15   For all points on the boundary, the inner product of the input vector with  the weight vector is the same. This implies that these input vectors will all  have the same projection onto the weight vector, so they must lie on a line  orthogonal to the weight vector.  These concepts will be covered in more de- tail in Chapter 5.  In addition, any vector in the shaded region of Figure 4.3  will have an inner product greater than  , and vectors in the unshaded  b– region will have inner products less than  . Therefore the weight vector  w1 After we have selected a weight vector with the correct angular orientation,  the bias value can be computed by selecting a point on the boundary and  satisfying Eq.  4.15 .   will always point toward the region where the neuron output is 1.   b–  4  2 2+  Let’s apply some of these concepts to the design of a perceptron network to  implement a simple logic function: the AND gate. The input target pairs for  the AND gate are      0 0  p1  =    t1  0=  p2  =    t2  0=  p3  =    t3  0=  p4  =    t4  1=          0 1          1 0          1 1  .      The figure to the left illustrates the problem graphically. It displays the in- put space, with each input vector labeled according to its target. The dark  circles   indicate that  the target is 0.   indicate that the target is 1, and the light circles   The first step of the design is to select a decision boundary. We want to  have a line that separates the dark circles and the light circles. There are  an infinite number of solutions to this problem. It seems reasonable to  choose the line that falls “halfway” between the two categories of inputs, as  shown in the adjacent figure.  Next we want to choose a weight vector that is orthogonal to the decision  boundary. The weight vector can be any length, so there are infinite possi- bilities. One choice is   1w  AND  w1  =  ,  2 2   4.16   as displayed in the figure to the left.  4-7   4 Perceptron Learning Rule  Finally, we need to find the bias,  the decision boundary and satisfying Eq.  4.15 . If we use  find  . We can do this by picking a point on   we   1.5 0  p  =  b  T  wT 1  p b+  =  2 2  1.5 0  b+  =  3  b+  =  0  3–=  b  .   4.17   We can now test the network on one of the input target pairs. If we apply  p2   to the network, the output will be  a  =  hardlim wT    1  p2  b+    =  hardlim 2 2        3–   0 1  a  =  hardlim 1–    =  0,   4.18   which is equal to the target output  correctly classified.  t2  . Verify for yourself that all inputs are   To experiment with decision boundaries, use the Neural Network Design  Demonstration Decision Boundaries  nnd4db .  Multiple-Neuron Perceptron Note that for perceptrons with multiple neurons, as in Figure 4.1, there  will be one decision boundary for each neuron. The decision boundary for  neuron    will be defined by  i  wT i  p bi+  0=  .   4.19   A single-neuron perceptron can classify input vectors into two categories,  since its output can be either 0 or 1. A multiple-neuron perceptron can clas- sify inputs into many categories. Each category is represented by a differ- ent output vector. Since each element of the output vector can be either 0  or 1, there are a total of   is the number of  neurons.   possible categories, where   2S  S  Perceptron Learning Rule  Now that we have examined the performance of perceptron networks, we  are in a position to introduce the perceptron learning rule. This learning  rule is an example of supervised training, in which the learning rule is pro- vided with a set of examples of proper network behavior:  p1 t1 { , }    {  p2 t2  , }  pQ tQ  {      ,  }  ,   4.20   4-8   Perceptron Learning Rule  pq   is an input to the network and   where  put. As each input is applied to the network, the network output is com- pared to the target. The learning rule then adjusts the weights and biases  of the network in order to move the network output closer to the target.    is the corresponding target out-  tq  Test Problem In our presentation of the perceptron learning rule we will begin with a  simple test problem and will experiment with possible rules to develop  some intuition about how the rule should work. The input target pairs for  our test problem are      1 2  p1  =    t1  1=  p2  =    t2  0=  p3  =    t3  0=          1– 2          0 1–  .      4  The problem is displayed graphically in the adjacent figure, where the two  input vectors whose target is 0 are represented with a light circle  , and  the vector whose target is 1 is represented with a dark circle  . This is a  very simple problem, and we could almost obtain a solution by inspection.  This simplicity will help us gain some intuitive understanding of the basic  concepts of the perceptron learning rule.  The network for this problem should have two-inputs and one output. To  simplify our development of the learning rule, we will begin with a network  without a bias. The network will then have just two parameters,   and  w1 2  , as shown in Figure 4.4.  w1 1  Inputs  No-Bias Neuron  p1  p2   cid:0  cid:0 Σ w1,1 n  cid:0  cid:0  w1,2   cid:0  cid:0  a  cid:0  cid:0   a = hardlim Wp   Figure 4.4  Test Problem Network  By removing the bias we are left with a network whose decision boundary  must pass through the origin. We need to be sure that this network is still  able to solve the test problem. There must be an allowable decision bound- ary that can separate the vectors  . The figure  to the left illustrates that there are indeed an infinite number of such  boundaries.    from the vector    and   p1  p3  p2  4-9  1  2  3  1  2   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   3   1  2   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   3  1  2  1w  3  4 Perceptron Learning Rule  The adjacent figure shows the weight vectors that correspond to the allow- able decision boundaries.  Recall that the weight vector is orthogonal to the  decision boundary.  We would like a learning rule that will find a weight  vector that points in one of these directions. Remember that the length of  the weight vector does not matter; only its direction is important.  Constructing Learning Rules Training begins by assigning some initial values for the network parame- ters. In this case we are training a two-input single-output network with- out a bias, so we only have to initialize its two weights. Here we set the  elements of the weight vector,  ues:  , to the following randomly generated val-  w1  wT 1  =  1.0 0.8–  .   4.21   We will now begin presenting the input vectors to the network. We begin  with   :  p1  a  =  hardlim wT    1  p1    =  hardlim 1.0 0.8–          1 2  a  =  hardlim 0.6–    =  0 .   4.22   The network has not returned the correct value. The network output is 0,  while the target response,   , is 1.  t1  p1  , so that in the future it has a better chance of classifying it correctly.   We can see what happened by looking at the adjacent diagram. The initial  weight vector results in a decision boundary that incorrectly classifies the  vector  . We need to alter the weight vector so that it points more toward  p1 One approach would be to set  . This is simple and would en- sure that   was classified properly in the future. Unfortunately, it is easy  to construct a problem for which this rule cannot find a solution. The dia- gram to the lower left shows a problem that cannot be solved with the  weight vector pointing directly at either of the two class 1 vectors. If we ap- ply the rule   every time one of these vectors is misclassified, the net- work’s weights will simply oscillate back and forth and will never find a  solution.   equal to   p=  w1  w1  p1  p1   point more in the direction of   Another possibility would be to add  w1 p1 cause the direction of  w1 This rule can be stated:   to   p1 . Repeated presentations of   . Adding    to   w1  w1  p1   would make   would  .   p1  p1   to asymptotically approach the direction of   If t  =  1 and a  =  0, then  w1  new  =  w1  old p+  .   4.23   4-10   Perceptron Learning Rule  Applying this rule to our test problem results in new values for   w1  :  new  w1  =  w1  old p1+  =  1.0 0.8–  +  =  1 2  .  2.0 1.2   4.24   This operation is illustrated in the adjacent figure.  We now move on to the next input vector and will continue making changes  to the weights and cycling through the inputs until they are all classified  correctly.  The next input vector is   . When it is presented to the network we find:  p2  2  1  1w  3  a  =  hardlim wT    1  p2    =  hardlim 2.0 1.2      1– 2      =  hardlim 0.4    =  1 .  4   4.25   1  2  3  1w  The target  was misclassified as a 1.   associated with   t2  p2   is 0 and the output a is 1. A class 0 vector   Since we would now like to move the weight vector  we can simply change the addition in Eq.  4.23  to subtraction:  w1   away from the input,   If t  =  0 and a  =  1, then  w1  new  =  w1  old p–  .   4.26   If we apply this to the test problem we find:  new  w1  =  w1  old p2–  =  2.0 1.2  –  1– 2  =  3.0 0.8–  ,   4.27   which is illustrated in the adjacent figure.  Now we present the third vector   p3  :  a  =  hardlim wT    1  p3    =  hardlim 3.0 0.8–      0 1–      =  hardlim 0.8    =  1 .  The current  is a situation for which we already have a rule, so  according to Eq.  4.26 :   results in a decision boundary that misclassifies   . This   will be updated again,   w1  w1  p3   4.28    4.29   new  w1  =  w1  old p3–  =  3.0 0.8–  –  0 1–  =  .  3.0 0.2  4-11   4 Perceptron Learning Rule  1  1w  2  3  The diagram to the left shows that the perceptron has finally learned to  classify the three vectors properly. If we present any of the input vectors to  the neuron, it will output the correct class for that input vector.  This brings us to our third and final rule: if it works, don’t fix it.  If t  =  a, then   w1  new  =  old.  w1   4.30   Here are the three rules, which cover all possible combinations of output  and target values:  If t  =  1 and a  =  If t  =  0 and a  =  new  new  =  =  0, then  w1 1, then  w1 new  w1 w1 old.  old p.+ old p.–  If t  =  a, then   w1  =  w1  Unified Learning Rule The three rules in Eq.  4.31  can be rewritten as a single expression. First  we will define a new variable, the perceptron error e:  We can now rewrite the three rules of Eq.  4.31  as:  e  a–=  t  .   4.31    4.32    4.33   If e  =  If e  =  If e  new  1, then  w1 1,–  then  w1 0, then  w1 =  = new  w1 w1 = new w1 =  old p . + old p– old  .      .  p   is the same as the sign on the error, e. Furthermore, the absence of   Looking carefully at the first two rules in Eq.  4.33  we can see that the sign  of    p in the third rule corresponds to an e of 0. Thus, we can unify the three rules  into a single expression:  new  w1  =  w1  old  ep+  =  old  w1  +  a– t  p  .   4.34   This rule can be extended to train the bias by noting that a bias is simply  a weight whose input is always 1. We can thus replace the input   in Eq.   4.34  with the input to the bias, which is 1. The result is the perceptron  rule for a bias:  p  bnew  =  bold  e+  .   4.35   4-12   Perceptron Learning Rule  Training Multiple-Neuron Perceptrons The perceptron rule, as given by Eq.  4.34  and Eq.  4.35 , updates the  weight vector of a single neuron perceptron. We can generalize this rule for  the multiple-neuron perceptron of Figure 4.1 as follows. To update the ith  row of the weight matrix use:  To update the ith element of the bias vector use:  new  wi  =  wi  old  eip+  .  new  bi  =  bi  old  ei+  .  Wnew Wold  =  +  epT  ,  bnew  =  bold  e+  .   4.36    4.37    4.38    4.39   4  and  Perceptron Rule  The perceptron rule can be written conveniently in matrix notation:  2 2+  To test the perceptron learning rule, consider again the apple orange rec- ognition problem of Chapter 3. The input output prototype vectors will be        1 1– 1–              1 1 1–  .        p1  =  t1  =  0  p2  =  t2  =  1   4.40    Note that we are using 0 as the target output for the orange pattern,  ,  p1 instead of -1, as was used in Chapter 3. This is because we are using the  hardlim   transfer function, instead of   hardlims  .   Typically the weights and biases are initialized to small random numbers.  Suppose that here we start with the initial weight matrix and bias:  W  =  0.5 1–  0.5–  ,   b  =  0.5  .   4.41   The first step is to apply the first input vector,   , to the network:  p1  a  =  hardlim Wp1    b+    =  hardlim 0.5 1–  0.5–        1 1– 1–     0.5+    =  hardlim 2.5    =  1   4.42   4-13   4 Perceptron Learning Rule  Then we calculate the error:  The weight update is  e  =  t1  a–  =  0  1–  =  1–  .  Wnew Wold  =  +  epT  =  0.5 1–  0.5–  +  1–   1 1–  1–  =  0.5–  0 0.5  .  The bias update is  bnew  =  bold  e+  =  0.5  +  1–    =  0.5–  .   4.45   This completes the first iteration.  The second iteration of the perceptron rule is:  a  =  hardlim  Wp2     b+    =  hardlim      0.5–  0 0.5  +  0.5–      =  hardlim  0.5–      =  0  e  =  t2  a–  =  1  0–  =  1  Wnew Wold  =  +  epT  =  0.5–  0 0.5  +  1 1 1 1–  =  0.5 1 0.5–  bnew  =  bold  e+  =  –  0.5  1+  =  0.5  The third iteration begins again with the first input vector:  1 1 1–  1 1– 1–  a  =  hardlim  Wp1     b+     =  hardlim  0.5 1 0.5–     0.5+     =  hardlim  0.5      =  1  e  =  t1  a–  =  0  1–  =  1–  Wnew Wold  =  +  epT  =  0.5 1 0.5–  +  1–   1 1–  1–  =  0.5–  2 0.5  4-14   4.43    4.44    4.46    4.47    4.48    4.49    4.50    4.51    4.52    Proof of Convergence  bnew  =  bold  e+  =  0.5  +  1–    =  0.5–  .   4.53   If you continue with the iterations you will find that both input vectors will  now be correctly classified. The algorithm has converged to a solution. Note  that the final decision boundary is not the same as the one we developed in  Chapter 3, although both boundaries correctly classify the two input vec- tors.   To experiment with the perceptron learning rule, use the Neural Network  Design Demonstration Perceptron Rule  nnd4pr .  Proof of Convergence  Although the perceptron learning rule is simple, it is quite powerful. In  fact, it can be shown that the rule will always converge to weights that ac- complish the desired classification  assuming that such weights exist . In  this section we will present a proof of convergence for the perceptron learn- ing rule for the single-neuron perceptron shown in Figure 4.5.   4  Inputs  Hard Limit Neuron  p1 p2p3  pR  w1,1  cid:0 Σ  cid:0  w1,R b 1  n  a   cid:0   cid:0   a = hardlim  1wTp + b   Figure 4.5  Single-Neuron Perceptron  The output of this perceptron is obtained from  a  =  hardlim wT    p b+    .  1  The network is provided with the following examples of proper network be- havior:  p1 t1 { , }    { , }  pQ tQ p2 t2 { , }      .  where each target output,   , is either   0   or   1  .  tq  Notation To conveniently present the proof we will first introduce some new nota- tion. We will combine the weight matrix and the bias into a single vector:   4.54    4.55   4-15   4 Perceptron Learning Rule  x  =  .  w1 b  zq  =  .  pq 1  n  =  wT  1  p b+  =  xTz  .  xnew  =  xold  ez+  .  We will also augment the input vectors with a 1, corresponding to the bias  input:  Now we can express the net input to the neuron as follows:  The perceptron learning rule for a single-neuron perceptron  Eq.  4.34  and  Eq.  4.35   can now be written  e  1  ,    can be either   , then no change is made to  The error  , then the input vector is added to the weight vector.  the weights. If  If  , then the negative of the input vector is added to the weight vec- tor. If we count only those iterations for which the weight vector is changed,  the learning rule becomes  1–=  . If    or   0=  1=  1–  0  e  e  e  x k   =  1– x k    +  z' k  1–    ,  where   z' k  1–     is the appropriate member of the set    z1 z2  zQ          z– 1    z– 2  z– Q        .  We will assume that a weight vector exists that can correctly categorize all  . For this weight vector  Q we will assume that   input vectors. This solution will be denoted   x  and  xTzq       0   if   tq  1=  ,  xTzq    –    0   if   tq  0=  .  Proof We are now ready to begin the proof of the perceptron convergence theo- rem. The objective of the proof is to find upper and lower bounds on the  length of the weight vector at each stage of the algorithm.  4-16   4.56    4.57    4.58    4.59    4.60    4.61    4.62    4.63    Proof of Convergence  Assume that the algorithm is initialized with the zero weight vector:  x 0  k   iterations  changes to the weight vector , we find from Eq.  4.60 :  .  This does not affect the generality of our argument.  Then, after   0=  x k   =  z' 0   +  z' 1   z' k  1–  +  +    .   4.64   If we take the inner product of the solution weight vector with the weight  vector at iteration    we obtain  k  xTx k   =  xTz' 0   +  xTz' 1   xTz' k  1–  +  +    .   4.65   From Eq.  4.61 –Eq.  4.63  we can show that  From the Cauchy-Schwartz inequality  see [Brog91]   Therefore  where  xTz' i     .  xTx k   k  .  xTx k     2    x 2 x k  2  ,  x 2  =  xTx  .  x k  2    2 xTx k   -------------------------- x 2    2 k ------------- x 2  .  If we combine Eq.  4.67  and Eq.  4.68  we can put a lower bound on the  squared length of the weight vector at iteration   k  :  Next we want to find an upper bound for the length of the weight vector.  We begin by finding the change in the length at iteration   k  :  x k  2  =  xT k x k   =    1– x k    +  z' k  1–    T x k  1–      +  z' k  1–      =  xT k  1–  x k  1–    +  2xT k  1–  z' k  1–     4.71   z'T k  1–  +  z' k  1–    xT k  1–  z' k  1–    0  ,   4.72   Note that   4-17  4   4.66    4.67    4.68    4.69    4.70    4 Perceptron Learning Rule  since the weights would not be updated unless the previous input vector  had been misclassified. Now Eq.  4.71  can be simplified to  We can repeat this process for   1– x k  ,   2– x k  , etc., to obtain  x k  2    1– x k   2  z' k  1–  +  2   2 .  2  x k  2    z' 0  2  z' k  1–  +  +   2  .  If     =  max    z' i  2    , this upper bound can be simplified to  x k  2  k  .  We now have an upper bound  Eq.  4.75   and a lower bound  Eq.  4.70   on  the squared length of the weight vector at iteration  two inequalities we find  . If we combine the   k   4.73    4.74    4.75    4.76   k x k  2       or   k    2 k ------------- x 2   x 2 -----------------  .  2  k  Because   has an upper bound, this means that the weights will only be  changed a finite number of times. Therefore, the perceptron learning rule  will converge in a finite number of iterations.   The maximum number of iterations  changes to the weight vector  is in- versely related to the square of  . This parameter is a measure of how close  the solution decision boundary is to the input patterns. This means that if  the input classes are difficult to separate  are close to the decision bound- ary  it will take many iterations for the algorithm to converge.    Note that there are only three key assumptions required for the proof:  1. A solution to the problem exists, so that Eq.  4.66  is satisfied.   2. The weights are only updated when the input vector is misclassified,   therefore Eq.  4.72  is satisfied.   3. An upper bound,     , exists for the length of the input vectors.   Because of the generality of the proof, there are many variations of the per- ceptron learning rule that can also be shown to converge.  See Exercise  E4.13.   Limitations The perceptron learning rule is guaranteed to converge to a solution in a  finite number of steps, so long as a solution exists. This brings us to an im- portant question. What problems can a perceptron solve? Recall that a sin-  4-18   Linear Separability  Proof of Convergence  gle-neuron perceptron is able to divide the input space into two regions.  The boundary between the regions is defined by the equation  wT 1  p b+  0=  .   4.77   This is a linear boundary  hyperplane . The perceptron can be used to clas- sify input vectors that can be separated by a linear boundary. We call such  vectors linearly separable. The logical AND gate example on page 4-7 illus- trates a two-dimensional example of a linearly separable problem. The ap- ple orange recognition problem of Chapter 3 was a three-dimensional  example.  Unfortunately, many problems are not linearly separable. The classic ex- ample is the XOR gate. The input target pairs for the XOR gate are      0 0  p1  =    t1  0=  p2  =    t2  1=  p3  =    t3  1=  p4  =    t4  0=          0 1          1 0          1 1  .      This problem is illustrated graphically on the left side of Figure 4.6, which  also shows two other linearly inseparable problems. Try drawing a straight  line between the vectors with targets of 1 and those with targets of 0 in any  of the diagrams of Figure 4.6.  4  Figure 4.6  Linearly Inseparable Problems  It was the inability of the basic perceptron to solve such simple problems  that led, in part, to a reduction in interest in neural network research dur- ing the 1970s. Rosenblatt had investigated more complex networks, which  he felt would overcome the limitations of the basic perceptron, but he was  never able to effectively extend the perceptron rule to such networks. In  Chapter 11 we will introduce multilayer perceptrons, which can solve arbi- trary classification problems, and will describe the backpropagation algo- rithm, which can be used to train them.  4-19   4 Perceptron Learning Rule  Summary of Results  Perceptron Architecture  Input  Hard Limit Layer  p R x 1  1  S x R  n cid:0 W  cid:0 b  cid:0   S x 1  S x 1  a  cid:0  cid:0  S x 1  cid:0  cid:0  S cid:0  cid:0   R  a = hardlim  Wp + b   W  =  wT 1 wT 2  wT S  a  =  hardlim Wp b+                    ai  =  hardlim ni    =  hardlim wT    i  p bi+    Decision Boundary  wT i  p bi+  0=  .  The decision boundary is always orthogonal to the weight vector.  Single-layer perceptrons can only classify linearly separable vectors.  Perceptron Learning Rule  Wnew Wold  =  +  epT  bnew  =  bold  e+  where   e  a–=  t  .  4-20   Solved Problems  Solved Problems  P4.1 Solve the three simple classification problems shown in Figure   P4.1 by drawing a decision boundary. Find weight and bias values  that result in single-neuron perceptrons with the chosen decision  boundaries.   a    b    c   Figure P4.1   Simple Classification Problems  First we draw a line between each set of dark and light data points.  4   a    b    c   The next step is to find the weights and biases. The weight vectors must be  orthogonal to the decision boundaries, and pointing in the direction of  points to be classified as 1  the dark points . The weight vectors can have  any length we like.  1w   a   1w   b   1w   c   Here is one set of choices for the weight vectors:   a    wT 1  =  2– 1  ,    b    wT 1  =  0 2–  ,    c    wT 1  =  2 2–  .  4-21   4 Perceptron Learning Rule  Now we find the bias values for each perceptron by picking a point on the  decision boundary and satisfying Eq.  4.15 .  wT p b+ 1 wT = b 1  –  = p  0  This gives us the following three biases:   a    b  =  –  2– 1  =  0  ,  b    b  =  –  0 2–  =  2–  ,  c    b  =  –  2 2–  0 0  0 1–  2– 1  =  6  We can now check our solution against the original points. Here we test the  first network on the input vector   p  .  T  =  2– 2  a  =  hardlim wT    p b+    1  =  hardlim  2– 1      2– 2    0+   =  hardlim 6   =  1  » 2 + 2 ans =       4  We can use MATLAB to automate the testing process and to try new points.  Here the first network is used to classify a point that was not in the original  problem.  w=[-2 1]; b = 0; a = hardlim w*[1;1]+b  a =  0  P4.2 Convert the classification problem defined below into an equiva-  lent problem definition consisting of inequalities constraining  weight and bias values.      0 2          1 0  p1  =    t1  1=  p2  =    t2  1=  p3  =    t3  0=  p4  =    t4  0=          0 2–          2 0      4-22   Solved Problems  ti  Each target  be less than 0, or greater than or equal to 0. For example, since  know that the net input corresponding to  p1 to 0. Thus we get the following inequality:   indicates whether or not the net input in response to  t1   must  pi  is 1, we   must be greater than or equal   Applying the same procedure to the input target pairs for  and    results in the following set of inequalities.      p4 t4    p2 t2    ,     p3 t3       4  0w1 1  Wp1 + 2w1 2 2w1 2   b+ 0 0 + b 0. b+  2w1 2 w1 1 2w1 2 – 2w1 1  0+ b 0+ b 0+ b 0+ b  i   ii   iii  iv  Solving a set of inequalities is more difficult than solving a set of equalities.  One added complexity is that there are often an infinite number of solu- tions  just as there are often an infinite number of linear decision bound- aries that can solve a linearly separable classification problem .  However, because of the simplicity of this problem, we can solve it by  graphing the solution spaces defined by the inequalities. Note that  only appears in inequalities  ii  and  iv , and  ities  i  and  iii . We can plot each pair of inequalities with two graphs.   only appears in inequal-  w1 2  w1 1     w1,1  w1,2  ii  iv  b  iii  b  i  Any weight and bias values that fall in both dark gray regions will solve the  classification problem.  Here is one such solution:  W  =  2– 3  b  3=  .  4-23   4 Perceptron Learning Rule  P4.3 We have a classification problem with four classes of input vector.   The four classes are  class 1:  p1  =    p2  =  , class 2:   p3  =    p4  =          1 1  1– 2  1 2      2– 1              2 1–  1– 1–  2 0  ,      2– 2–  .      class 3:  p5  =    p6  =  , class 4:   p7  =    p8  =  Design a perceptron network to solve this problem.  To solve a problem with four classes of input vector we will need a percep- tron with at least two neurons, since an  -neuron perceptron can catego- rize    classes. The two-neuron perceptron is shown in Figure P4.2.  2S  S  Input  Hard Limit Layer  p 2 x 1  1  2 x 2  n cid:0 W  cid:0 b  cid:0   2 x 1  2 x 1  a  cid:0  cid:0  2 x 1  cid:0  cid:0  2 cid:0  cid:0   2  a = hardlim  Wp + b   Figure P4.2  Two-Neuron Perceptron   indicate class 1 vectors, the light squares   Let’s begin by displaying the input vectors, as in Figure P4.3. The light cir-  indicate class 2 vectors,  cles  the dark circles   indicate  class 4 vectors.   indicate class 3 vectors, and the dark squares   A two-neuron perceptron creates two decision boundaries. Therefore, to di- vide the input space into the four categories, we need to have one decision  boundary divide the four classes into two sets of two. The remaining bound- ary must then isolate each class. Two such boundaries are illustrated in  Figure P4.4. We now know that our patterns are linearly separable.  4-24   Figure P4.3  Input Vectors for Problem P4.3  4  Solved Problems  3  4  3  4  1  2  1  2          Figure P4.4  Tentative Decision Boundaries for Problem P4.3  The weight vectors should be orthogonal to the decision boundaries and  should point toward the regions where the neuron outputs are 1. The next  step is to decide which side of each boundary should produce a 1. One choice  is illustrated in Figure P4.5, where the shaded areas represent outputs of  1. The darkest shading indicates that both neuron outputs are 1. Note that  this solution corresponds to target values of  class 1:  t1  =    t2  =  , class 2:   t3  =    t4  =  class 3:  t5  =    t6  =  , class 4:   t7  =    t8  =  We can now select the weight vectors:  0 1  1 1  0 1  1 1  ,  .          0 0  1 0                  0 0  1 0  4-25   4 Perceptron Learning Rule  w1  =   and   w2  =  3– 1–  .  1 2–  Note that the lengths of the weight vectors is not important, only their di- rections. They must be orthogonal to the decision boundaries. Now we can  calculate the bias by picking a point on a boundary and satisfying Eq.   4.15 :  b1  =  wT  p  1–  =  –  3–  1–  =  1  ,  b2  =  wT  p  2–  =  –  1 2–  =  0  .  0 1  0 0  1  2  3  4  4-26  Figure P4.5  Decision Regions for Problem P4.3  In matrix form we have  W  =  wT 1 wT 2  =  3– 1  1– 2–   and   b  =  ,  1 0  which completes our design.  P4.4 Solve the following classification problem with the perceptron   rule. Apply each input vector in order, for as many repetitions as  it takes to ensure that the problem is solved. Draw a graph of the  problem only after you have found a solution.   Solved Problems      2 2  p1  =    t1  0=  p2  =    t2  1=  p3  =    t3  0=  p4  =    t4  1=          2– 2          1– 1              1 2–  Use the initial weights and bias:  W 0   =  0 0  b 0   0=  .  We start by calculating the perceptron’s output  p1  , using the initial weights and bias.  a   for the first input vector   a  =  hardlim W 0 p1    +  b 0     =  hardlim 0 0        0+   2 2  =  hardlim 0   =  1  4  The output  rule to find new weights and biases based on the error.   does not equal the target value   t1  a  , so we use the perceptron   1–  0  =  =  t1  a–  e W 1  W 0  b 1   b 0   =  =  e+  +  1– = T ep1 = 0  =  0 0   +  1–  =  1–  +  1–   2 2  =  2–  2–  We now apply the second input vector  bias.  p2  , using the updated weights and   a  =  hardlim W 1 p2    +  b 1     =  hardlim  2–  2–  =  hardlim 1   =  1      1 2–    1–   This time the output  tron rule will not result in any changes.   is equal to the target   a  t2  . Application of the percep-  W 2  W 1  b 2  b 1   = =  We now apply the third input vector.  4-27   4 Perceptron Learning Rule          a  =  hardlim W 2 p3    +  b 2     =  hardlim  2–  2–  =  hardlim 1–    =  0  2– 2    1–   The output in response to input vector  will be no changes.  p3   is equal to the target   , so there   t3  W 3  W 2  b 2  b 3   = =  We now move on to the last input vector   p4  .  a  =  hardlim W 3 p4    +  b 3     =  hardlim  2–  2–  =  hardlim 1–    =  0  1– 1    1–   This time the output  ceptron rule will result in a new set of values for    does not equal the appropriate target    and   W  b  a  .  t4  . The per-  0–  1  =  =  t4 a– =  e W 4  W 3  b 4   b 3   =  e+  +  1 = T ep4 =  =  2– =  2– 0  1–  1+  +  1   1– 1  =  3–  1–  We now must check the first vector  equal to the associated target   .  p1  t1   again. This time the output   a   is   a  =  hardlim W 4 p1    +  b 4     =  hardlim  3–  1–  =  hardlim 8–    =  0        0+   2 2  Therefore there are no changes.  W 5  W 4  b 5  b 4   = =  4-28   Solved Problems  The second presentation of  of weight and bias values.  p2   results in an error and therefore a new set   a  =  hardlim W 5 p2    +  b 5     =  hardlim  3–  1–  =  hardlim 1–    =  0      1 2–    0+   Here are those new values:  0–  1  =  =  t2 a– =  e W 6  W 5  b 6   b 5   =  e+  +  1 = T ep2 0 =  =  1+  3– =  1– 1.  +  1  1 2–  =  2–  3–  4  Cycling through each input vector once more results in no errors.  a  =  hardlim W 6 p3    +  b 6     =  hardlim  2–  3–  =  0  =  t3  a  =  hardlim W 6 p4    +  b 6     =  hardlim  2–  3–  =  1  =  t4  a  =  hardlim W 6 p1    +  b 6     =  hardlim  2–  3–  =  0  =  t1  a  =  hardlim W 6 p2    +  b 6     =  hardlim  2–  3–  =  1  =  t2               2– 2  1– 1    1+    1+     1+   2 2  1 2–    1+   Therefore the algorithm has converged. The final solution is:  W  =  2–  3–  b  1=  .  Now we can graph the training data and the decision boundary of the solu- tion. The decision boundary is given by  n Wp b+  =  =  w1 1 p1 w1 2 p2  +  +  b  =  2p–  – 1 3p2  1+  =  0  .  To find the   p2   intercept of the decision boundary, set   p1  0=  :  p2  =  –  b ---------- w1 2  =  1 ------– 3–  =  1 --- 3  if p1  0=  .  4-29   4 Perceptron Learning Rule  To find the   p1   intercept, set   p2  0=  :  p1  =  –  b ---------- w1 1  =  1 ------– 2–  =  1 --- 2  if p2  0=  .  The resulting decision boundary is illustrated in Figure P4.6.  W  Figure P4.6  Decision Boundary for Problem P4.4  Note that the decision boundary falls across one of the training vectors.  This is acceptable, given the problem definition, since the hard limit func- tion returns 1 when given an input of 0, and the target for the vector in  question is indeed 1.  P4.5 Consider again the four-class decision problem that we introduced  in Problem P4.3. Train a perceptron network to solve this problem  using the perceptron learning rule.  If we use the same target vectors that we introduced in Problem P4.3, the  training set will be:  p1  =    t1  =  p2  =    t2  =          1 1  2 0  0 0          0 1          p5  =  1 2  1– 2  p4  =    t4  =    t5  =  0 0          p3  =  1 0          p6  =  2 1–  2– 1    t3  =    t6  =  0 1      1 0          p7  =  1– 1–    t7  =  1 1          p8  =  2– 2–    t8  =  1 1  .      Let’s begin the algorithm with the following initial weights and biases:  W 0   =  ,   b 0   =  1 0 0 1  .  1 1  The first iteration is  4-30   Solved Problems  a  =  hardlim  W 0 p1 b 0   +        =  hardlim  1 0 0 1     1 1  +     =  1 1  ,  1 1  e  =  t1  a–  =  –  0 0  1 1  =  ,  1– 1–  W 1  W 0   =  +  T ep1  =  1 0 0 1  +  1– 1–  1 1  =  0 1–  1– 0  ,  b 1   =  b 0   e+  =  +  1 1  1– 1–  =  .  0 0  4  The second iteration is  a  =  hardlim  W 1 p2 b 1   +        =  hardlim      0 1–  1– 0  1 2  +     =  0 0  ,  0 0  W 2  W 1   =  +  T ep2  =  1 2  =  0 1–  1– 0  ,  e  =  t2  a–  =  ,  0 0  –  0 0  0 1–  1– 0  0 0  +  =  0 0  0 0  b 2   =  b 1   e+  =  +  =  0 0  .  0 0  The third iteration is  a  =  hardlim  W 2 p3 b 2   +        =  hardlim      0 1–  1– 0  2 1–  +     =  0 0  ,  1 0  e  =  t3  a–  =  –  0 1  1 0  =  ,  1– 1  4-31   4 Perceptron Learning Rule  W 3  W 2   =  +  T ep3  =  0 1–  1– 0  +  1– 1  2 1–  =  2– 1  0 1–  ,  b 3   =  b 2   e+  =  +  0 0  1– 1  =  .  1– 1  Iterations four through eight produce no changes in the weights.  W 8  W 7  W 6  W 5  W 4  W 3   =  =  =  =  =  b 8   =  b 7   =  b 6   =  b 5   =  b 4   =  b 3   The ninth iteration produces  a  =  hardlim  W 8 p1 b 8   +        =  hardlim      2– 1  0 1–  1 1  +  1– 1     =  ,  0 1  e  =  t1  a–  =  –  0 0  =  ,  0 1–  0 1  +  W 9  W 8   =  +  T ep1  =  2– 1  0 1–  0 1–  1 1  =  2– 0  0 2–  ,  b 9   =  b 8   e+  =  1– 1  +  0 1–  =  .  1– 0  At this point the algorithm has converged, since all input patterns will be  correctly classified. The final decision boundaries are displayed in Figure  P4.7. Compare this result with the network we designed in Problem P4.3.  4-32   Solved Problems  1  2  3  4  Figure P4.7  Final Decision Boundaries for Problem P4.5  4  4-33   4 Perceptron Learning Rule  Epilogue  In this chapter we have introduced our first learning rule — the perceptron  learning rule. It is a type of learning called supervised learning, in which  the learning rule is provided with a set of examples of proper network be- havior. As each input is applied to the network, the learning rule adjusts  the network parameters so that the network output will move closer to the  target.  The perceptron learning rule is very simple, but it is also quite powerful.  We have shown that the rule will always converge to a correct solution, if  such a solution exists. The weakness of the perceptron network lies not  with the learning rule, but with the structure of the network. The standard  perceptron is only able to classify vectors that are linearly separable. We  will see in Chapter 11 that the perceptron architecture can be generalized  to multilayer perceptrons, which can solve arbitrary classification prob- lems. The backpropagation learning rule, which is introduced in Chapter  11, can be used to train these networks.  In Chapters 3 and 4 we have used many concepts from the field of linear  algebra, such as inner product, projection, distance  norm , etc. We will find  in later chapters that a good foundation in linear algebra is essential to our  understanding of all neural networks. In Chapters 5 and 6 we will review  some of the key concepts from linear algebra that will be most important in  our study of neural networks. Our objective will be to obtain a fundamental  understanding of how neural networks work.  4-34   Further Reading  Further Reading  [BaSu83]  [Brog91]   W. L. Brogan, Modern Control Theory, 3rd Ed., Englewood  Cliffs, NJ: Prentice-Hall, 1991.  4  A. Barto, R. Sutton and C. Anderson, “Neuron-like adap- tive elements can solve difficult learning control problems,”  IEEE Transactions on Systems, Man and Cybernetics, Vol.  13, No. 5, pp. 834–846, 1983.  A classic paper in which a reinforcement learning algo- rithm is used to train a neural network to balance an in- verted pendulum.  A well-written book on the subject of linear systems. The  first half of the book is devoted to linear algebra. It also has  good sections on the solution of linear differential equa- tions and the stability of linear and nonlinear systems. It  has many worked problems.  W. McCulloch and W. Pitts, “A logical calculus of the ideas  immanent in nervous activity,” Bulletin of Mathematical  Biophysics, Vol. 5, pp. 115–133, 1943.  This article introduces the first mathematical model of a  neuron, in which a weighted sum of input signals is com- pared to a threshold to determine whether or not the neu- ron fires.  A landmark book that contains the first rigorous study de- voted to determining what a perceptron network is capable  of learning. A formal treatment of the perceptron was need- ed both to explain the perceptron’s limitations and to indi- cate directions for overcoming them. Unfortunately, the  book pessimistically predicted that the limitations of per- ceptrons indicated that the field of neural networks was a  dead end. Although this was not true, it temporarily cooled  research and funding for research for several years.  F. Rosenblatt, “The perceptron: A probabilistic model for  information storage and organization in the brain,” Psycho- logical Review, Vol. 65, pp. 386–408, 1958.  This paper presents the first practical artificial neural net- work — the perceptron.  4-35  [McPi43]  [Rose58]   [MiPa69]  M. Minsky and S. Papert, Perceptrons, Cambridge, MA:  MIT Press, 1969.   4 Perceptron Learning Rule  [Rose61]   F. Rosenblatt, Principles of Neurodynamics, Washington  DC: Spartan Press, 1961.  One of the first books on neurocomputing.  [WhSo92]  D. White and D. Sofge  Eds. , Handbook of Intelligent Con- trol, New York: Van Nostrand Reinhold, 1992.  Collection of articles describing current research and appli- cations of neural networks and fuzzy logic to control sys- tems.  4-36   Exercises  Exercises  E4.1 Consider the classification problem defined below:      p1  =  1– 1            t1  1=  p2  =    t2  1=  p3  =    t3  1=  p4  =    t4  0=          1 0      0 0              0 1  1 1–  .      p5  =    t5  0=  i. Draw a diagram of the single-neuron perceptron you would use to   solve this problem. How many inputs are required?  ii. Draw a graph of the data points, labeled according to their targets.   Is this problem solvable with the network you defined in part  i ?  Why or why not?  4  E4.2 Consider the classification problem defined below.      p1  =  1– 1    t1  1=  p2  =    t2  1=  p3  =    t3  0=  p4  =    t4  0=          0 0          1 0  .              1– 1–  i. Design a single-neuron perceptron to solve this problem. Design the  network graphically, by choosing weight vectors that are orthogonal  to the decision boundaries.  ii. Test your solution with all four input vectors.  » 2 + 2 ans =       4  iii. Classify the following input vectors with your solution. You can ei-  ther perform the calculations manually or with MATLAB.   p5  =  2– 0  p6  =  1 1  p7  =  0 1  p8  =  1– 2–  iv. Which of the vectors in part  iii  will always be classified the same   way, regardless of the solution values for  vary depending on the solution? Why?  W   and  ? Which may   b  E4.3 Solve the classification problem in Exercise E4.2 by solving inequalities  as  in Problem P4.2 , and repeat parts  ii  and  iii  with the new solution.  The  solution is more difficult than Problem P4.2, since you can’t isolate the  weights and biases in a pairwise manner.   4-37   4 Perceptron Learning Rule  E4.4 Solve the classification problem in Exercise E4.2 by applying the percep-  tron rule to the following initial parameters, and repeat parts  ii  and  iii   with the new solution.  W 0   =  0 0  b 0   0=  E4.5 Prove mathematically  not graphically  that the following problem is un-  solvable for a two-input single-neuron perceptron.      p1  =  1– 1    t1  1=  p2  =    t2  0=  p3  =    t3  1=  p4  =    t4  0=          1– 1–          1 1–          1 1       Hint: start by rewriting the input target requirements as inequalities that  constrain the weight and bias values.   E4.6 We have four categories of vectors.   Category I:   , Category II:         1– 1  1– 0            2 0  2 1                0 2  1 2        1 1–  0 1–      Category III:   , Category IV:   i. Design a two-neuron perceptron network  single layer  to recognize   these four categories of vectors. Sketch the decision boundaries.  ii. Draw the network diagram.  iii. Suppose the following vector is to be added to Category I.  Perform one iteration of the perceptron learning rule with this vec- tor.  Start with the weights you determined in part i.  Draw the new  decision boundaries.  E4.7 We have two categories of vectors. Category I consists of  1– 3–        0 0    1– 0  0 1  .      4-38   Exercises  Category II consists of        1– 1    0 2  2– 0  .      i. Design a single-neuron perceptron network to recognize these two   categories of vectors.  ii. Draw the network diagram.  iii. Sketch the decision boundary.  iv. If we add the following vector to Category I, will your network clas- sify it correctly? Demonstrate by computing the network response.  4  3– 0  v. Can your weight matrix and bias be modified so your network can  classify this new vector correctly  while continuing to classify the  other vectors correctly ? Explain.  E4.8 We want to train a perceptron network with the following training set:      p1  =  1– 1–          0 0    t1  0=  p2  =    t2  0=  p3  =    t3  1=          1– 1  .      The initial weight matrix and bias are  W 0   =  1 0  ,   b 0   =  0.5  .  i. Plot the initial decision boundary, weight vector and input patterns.  Which patterns are correctly classified using the initial weight and  bias?  ii. Train the network with the perceptron rule. Present each input vec-  tor once, in the order shown.  iii. Plot the final decision boundary, and demonstrate graphically   which patterns are correctly classified.  iv. Will the perceptron rule  given enough iterations  always learn to  correctly classify the patterns in this training set, no matter what  initial weights we use? Explain.  4-39   4 Perceptron Learning Rule  E4.9 We want to train a perceptron network using the following training set:      1 0  p1  =    t1  0=  p2  =    t2  0=  p3  =    t3  1=          1– 2          1 2  ,      starting from the initial conditions  W 0   =  0 1  ,   b 0   1=  .  i. Sketch the initial decision boundary, and show the weight vector   and the three training input vectors,  . Indicate the class of  each input vector, and show which ones are correctly classified by  the initial decision boundary.  p1 p2 p3      p1  p2  ii. Present the input    to the network, and perform one iteration of   the perceptron learning rule.  iii. Sketch the new decision boundary and weight vector, and again in-  dicate which of the three input vectors are correctly classified.  iv. Present the input    to the network, and perform one more itera-  tion of the perceptron learning rule.  v. Sketch the new decision boundary and weight vector, and again in-  dicate which of the three input vectors are correctly classified.  vi. If you continued to use the perceptron learning rule, and presented  all of the patterns many times, would the network eventually learn  to correctly classify the patterns? Explain your answer.  This part  does not require any calculations.   E4.10 The symmetric hard limit function is sometimes used in perceptron net- works, instead of the hard limit function. Target values are then taken  from the set [-1, 1] instead of [0, 1].  a = hardlims  n   n = Wp + b  i. Write a simple expression that maps numbers in the ordered set [0,  1] into the ordered set [-1, 1]. Write the expression that performs  the inverse mapping.  ii. Consider two single-neuron perceptrons with the same weight and   bias values. The first network uses the hard limit function  [0, 1]  values , and the second network uses the symmetric hard limit func- tion. If the two networks are given the same input  , and updated  with the perceptron learning rule, will their weights continue to  have the same value?  p  iii. If the changes to the weights of the two neurons are different, how   do they differ? Why?  4-40   Exercises  iv. Given initial weight and bias values for a standard hard limit per- ceptron, create a method for initializing a symmetric hard limit per- ceptron so that the two neurons will always respond identically  when trained on identical data.  E4.11 The vectors in the ordered set defined below were obtained by measuring  the weight and ear lengths of toy rabbits and bears in the Fuzzy Wuzzy An- imal Factory. The target values indicate whether the respective input vec- tor was taken from a rabbit  0  or a bear  1 . The first element of the input  vector is the weight of the toy, and the second element is the ear length.  » 2 + 2 ans =       4  p1  =    t1  0=  p2  =    t2  0=  p3  =    t3  0=  p4  =    t4  0=          1 4  3 1                  1 5  3 2                  2 4  4 1                  2 5  4 2  p5  =    t5  1=  p6  =    t6  1=  p7  =    t7  1=  p8  =    t8  1=          4  i. Use MATLAB to initialize and train a network to solve this “practi-  cal” problem.  the input vectors.  ii. Use MATLAB to test the resulting weight and bias values against   iii. Add input vectors to the training set to ensure that the decision   boundary of any solution will not intersect one of the original input  vectors  i.e., to ensure only robust solutions are found . Then retrain  the network. Your method for adding the input vectors should be  general purpose  not designed specifically for this problem .  E4.12 Consider again the four-category classification problem described in Prob-  lems P4.3 and P4.5. Suppose that we change the input vector   p3   to  i. Is the problem still linearly separable? Demonstrate your answer   graphically.  ii. Use MATLAB to initialize and train a network to solve this prob-  » 2 + 2 ans =       4  lem. Explain your results.  iii. If   p3   is changed to  p3  =  .  2 2  p3  =  2 1.5  4-41   4 Perceptron Learning Rule  is the problem linearly separable?  iv. With the   p3   from  iii , use MATLAB to initialize and train a net-  work to solve this problem. Explain your results.  E4.13 One variation of the perceptron learning rule is  Wnew Wold epT  =  +  bnew  =  bold e+    where  Does the proof require a limit on the learning rate? Explain.   is called the learning rate. Prove convergence of this algorithm.   4-42   Objectives  5 Signal and Weight   Vector Spaces  Objectives Theory and Examples  Linear Vector Spaces Linear Independence Spanning a Space Inner Product Norm Orthogonality  Summary of Results Solved Problems Epilogue Further Reading Exercises  Gram-Schmidt Orthogonalization  Vector Expansions  Reciprocal Basis Vectors  5-1 5-2 5-2 5-4 5-5 5-6 5-7 5-7 5-8 5-9 5-10 5-14 5-17 5-26 5-27 5-28  5  It is clear from Chapters 3 and 4 that it is very useful to think of the inputs  and outputs of a neural network, and the rows of a weight matrix, as vec- tors. In this chapter we want to examine these vector spaces in detail and  to review those properties of vector spaces that are most helpful when an- alyzing neural networks. We will begin with general definitions and then  apply these definitions to specific neural network problems. The concepts  that are discussed in this chapter and in Chapter 6 will be used extensively  throughout the remaining chapters of this book. They are critical to our un- derstanding of why neural networks work.  5-1  Objectives   5 Signal and Weight Vector Spaces  Theory and Examples  Linear algebra is the core of the mathematics required for understanding  neural networks. In Chapters 3 and 4 we saw the utility of representing the  inputs and outputs of neural networks as vectors. In addition, we saw that  it is often useful to think of the rows of a weight matrix as vectors in the  same vector space as the input vectors.   Recall from Chapter 3 that in the Hamming network the rows of the weight  matrix of the feedforward layer were equal to the prototype vectors. In fact,  the purpose of the feedforward layer was to calculate the inner products be- tween the prototype vectors and the input vector.  In the single neuron perceptron network we noted that the decision bound- ary was always orthogonal to the weight matrix  a row vector .  In this chapter we want to review the basic concepts of vector spaces  e.g.,  inner products, orthogonality  in the context of neural networks. We will  begin with a general definition of vector spaces. Then we will present the  basic properties of vectors that are most useful for neural network applica- tions.   One comment about notation before we begin. All of the vectors we have  discussed so far have been ordered n-tuples  columns  of real numbers and  are represented by bold small letters, e.g.,   x  =  x1 x2  xn  T  .   5.1   n  , the standard n-dimensional Euclidean space. In  These are vectors in  this chapter we will also be talking about more general vector spaces than  n . These more general vectors will be represented with a script typeface,  as in  . We will show in this chapter how these general vectors can often  be represented by columns of numbers.  x  Linear Vector Spaces  What do we mean by a vector space? We will begin with a very general def- inition. While this definition may seem abstract, we will provide many con- crete examples. By using a general definition we can solve a larger class of  problems, and we can impart a deeper understanding of the concepts.  Vector Space  Definition. A linear vector space,  over a scalar field,   F  , that satisfies the following conditions:  X  , is a set of elements  vectors  defined   1. An operation called vector addition is defined such that if   x X      x   is   an element of   X    and   y X  , then   x  y X+  .  5-2   Linear Vector Spaces  x  y+  =  y  x+  .  2.  3.  y+ x    z+  =  x  +  z+ y    .  x  0+  x=   for all   x X  .  5. For each vector  0=  that   x–  x  +    .  x X  4. There is a unique vector   0 X  , called the zero vector, such that    there is a unique vector in X, to be called   x–  , such   6. An operation, called multiplication, is defined such that for all scalars   a  F  , and all vectors   x X  ,   ax X  .  7. For any   x X  ,   1x  x=    for scalar   1   .  8. For any two scalars   a  F   and   b  F  , and any   x X  ,   a bx    =  ab  x  .  5  x1 x1  9.  b+ a  x  ax  bx+  .  10.  y+ a x    ax  ay+  .  =  =  To illustrate these conditions, let’s investigate a few sample sets and deter- mine whether or not they are vector spaces. First consider the standard  two-dimensional Euclidean space,  is clearly a vector space, and all ten conditions are satisfied for the stan- dard definitions of vector addition and scalar multiplication.   , shown in the upper left figure. This   2  2  ? What subsets of    are also vector spaces  sub- What about subsets of  spaces ? Consider the boxed area     in the center left figure. Does it sat- isfy all ten conditions? No. Clearly even condition 1 is not satisfied. The  vectors   is not. From this  example it is clear that no bounded sets can be vector spaces.   shown in the figure are in   , but    and   y+  x  x  X  X  y  2  2   that are vector spaces? Consider the line    Are there any subsets of     shown in the bottom left figure.  Assume that the line extends to infinity in  both directions.  Is this line a vector space? We leave it to you to show that  indeed all ten conditions are satisfied. Will any such infinite line satisfy the  ten conditions? Well, any line that passes through the origin will work. If  it does not pass through the origin then condition 4, for instance, would not  be satisfied.  X  In addition to the standard Euclidean spaces, there are other sets that also  satisfy the ten conditions of a vector space. Consider, for example, the set  P2  of all polynomials of degree less than or equal to 2. Two members of this  set would be  5-3  x2 x2  ℜ2 ℜ2  x + y x + y  x x  X X  x2 x2  x1 x1  y y  X X   5 Signal and Weight Vector Spaces  x  y     =  2  + +  t  4t2  =  1  +  5t .  If you are used to thinking of vectors only as columns of numbers, these  may seem to be strange vectors indeed. However, recall that to be a vector  space, a set need only satisfy the ten conditions we presented. Are these  conditions satisfied for the set  ? If we add two polynomials of degree less  than or equal to 2, the result will also be a polynomial of degree less than  or equal to 2. Therefore condition 1 is satisfied. We can also multiply a poly- nomial by a scalar without changing the order of the polynomial. Therefore  condition 6 is satisfied. It is not difficult to show that all ten conditions are  satisfied, showing that    is a vector space.  P2  P2  Consider the set  [0, 1]. Two members of this set would be  C 0 1     of all continuous functions defined on the interval   f t   1  t  x  y  =  sin  t   =  e 2t–  .   5.2    5.3   Another member of the set is shown in the figure to the left.  The sum of two continuous functions is also a continuous function, and a  scalar times a continuous function is a continuous function. The set    C 0 1  is also a vector space. This set is different than the other vector spaces we  have discussed; it is infinite dimensional. We will define what we mean by  dimension later in this chapter.  Linear Independence  Now that we have defined what we mean by a vector space, we will inves- tigate some of the properties of vectors. The first properties are linear de- pendence and linear independence.  Consider n vectors  least one of which is nonzero, such that  x1 x2   xn            . If there exist n scalars   a1 a2   an        , at   a1x1  +  a2x2  anxn  +  +  0=  ,   5.4   then the   xi    are linearly dependent.  Linear Independence  The converse statement would be: If  that each   , then   0=  xi   ai  a1x1  +  a2x2  anxn  +  +  0=   implies    is a set of linearly independent vectors.  5-4   Spanning a Space  Note that these definitions are equivalent to saying that if a set of vectors  is independent then no vector in the set can be written as a linear combi- nation of the other vectors.  2 2+  As an example of independence, consider the pattern recognition problem  of Chapter 3. The two prototype patterns  orange and apple  were given by:   5.5    5.6    5.7    5.8   Let   a1p1  +  a2p2  0=  , then  p1  =  ,   p2  =  1 1– 1–  .  1 1 1–  a1 a– 1 +  a2+ a2+ a2–  a– 1    =  ,  0 0 0  but this can only be true if  independent. Consider vectors from the space P2 of polynomials of degree less than or  equal to 2. Three vectors from this space would be  . Therefore    are linearly    and   p1  p2  a2  a1  =  =  0  2 2+  5  x1  =  1  + +  t  t2  ,   x2  =  2  +  2t  +  t2  ,   x3  =  1  t+  .  Note that if we let   a1  1=  ,   a2  1–=   and   a3  1=  , then  a1x1  +  a2x2  +  a3x3  0=  .  Therefore these three vectors are linearly dependent.  Spanning a Space  Next we want to define what we mean by the dimension  size  of a vector  space. To do so we must first define the concept of a spanning set.   Let X be a linear vector space and let   vectors in X. This subset spans X if and only if for every vector  x2u2  xmum exist scalars  words, a subset spans a space if every vector in the space can be written as  a linear combination of the vectors in the subset.   be a subset of general   there   x X . In other   u1 u2   um  x1 x2   xn   such that   x1u1  x  +  =  +  +                Basis Set  The dimension of a vector space is determined by the minimum number of  vectors it takes to span the space. This leads to the definition of a basis set.  A basis set for X is a set of linearly independent vectors that spans X. Any  basis set contains the minimum number of vectors required to span the   5-5   Inner Product  Inner Product  5 Signal and Weight Vector Spaces  space. The dimension of X is therefore equal to the number of elements in  the basis set. Any vector space can have many basis sets, but each one must  contain the same number of elements.  See [Stra80] for a proof of this fact.  Take, for example, the linear vector space P2. One possible basis for this  space is  u1  1=  ,   u2  t=  ,   u3  t2=  .   5.9   Clearly any polynomial of degree two or less can be created by taking a lin- ear combination of these three vectors. Note, however, that any three inde- pendent vectors from P2 would form a basis for this space. One such  alternate basis is:  u1  1=  ,   u2  =  1  t+  ,   u3  =  + + 2 t  t  .  1   5.10   From our brief encounter with neural networks in Chapters 3 and 4, it is  clear that the inner product is fundamental to the operation of many neural  networks. Here we will introduce a general definition for inner products  and then give several examples. Any scalar function of x and y can be defined as an inner product,  vided that the following properties are satisfied:  , pro-  x y  ,    1.  2.  x y  ,   = x ay1   , x x  ,    +  .  y x  ,   by2   , where equality holds if and only if x is the zero vector.  b x y2   ,    a x y1   ,  =  +     .  0  3. The standard inner product for vectors in Rn is  xTy  =  x1y1  +  x2y2  +   xnyn  +  ,   5.11   but this is not the only possible inner product. Consider again the set C[0, 1]  of all continuous functions defined on the interval [0, 1]. Show that the fol- lowing scalar function is man inner product  see Problem P5.6 .  x y  ,    =  x t y t dt  1  0   5.12   5-6   Norm  Norm  The next operation we need to define is the norm, which is based on the con- cept of vector length.  Norm  A scalar function    is called a norm if it satisfies the following properties:  x  0  .  1.  2.  3.  4.  x x ax x  0=   if and only if   x  0=  .   for scalar a.  = y+  a x x    y+  .  There are many functions that would satisfy these conditions. One common  norm is based on the inner product:  x  =  x x  ,  1 2  .  For Euclidean spaces,  miliar:  n  , this yields the norm with which we are most fa-  5  x  =  xTx    1 2  =  2 x1  +  2  xn + x2  +  2  .  In neural network applications it is often useful to normalize the input vec- tors. This means that    for each input vector.  1=  pi  Angle  Using the norm and the inner product we can generalize the concept of an- gle for vector spaces of dimension greater than two. The angle  between  two vectors x and y is defined by   5.13    5.14    5.15   cos  =  x y  ,   --------------- x y  .  Orthogonality  Now that we have defined the inner product operation, we can introduce  the important concept of orthogonality.  Orthogonality  Two vectors   x y  X   are said to be orthogonal if   x y  ,    0=  .   Orthogonality is an important concept in neural networks. We will see in  Chapter 7 that when the prototype vectors of a pattern recognition problem  are orthogonal and normalized, a linear associator neural network can be  trained, using the Hebb rule, to achieve perfect recognition.  In addition to orthogonal vectors, we can also have orthogonal spaces. A  vector    is orthogonal to a subspace    is orthogonal to every vec-   if   x X  X1  x  5-7   5 Signal and Weight Vector Spaces  p3  p1  X1  X2  X1 X2  . A subspace    if every vector in   . This is represented by    is orthog- x X1  is orthogonal to every vector in   . This is typically represented as  X1  tor in  onal to a subspace  X2 The figure to the left illustrates the two orthogonal spaces that were used  in the perceptron example of Chapter 3.  See Figure 3.4.  The   plane  is a subspace of   axis  which is another  3  plane was the decision boundary of a percep- subspace of  tron network. In Solved Problem P5.1 we will show that the perceptron de- cision boundary will be a vector space whenever the bias value is zero.  , which is orthogonal to the   3  . The   p1 p3  p1 p3  X1  p2  .      p2  Gram-Schmidt Orthogonalization There is a relationship between orthogonality and independence. It is pos- sible to convert a set of independent vectors into a set of orthogonal vectors  that spans the same vector space. The standard procedure to accomplish  this is called Gram-Schmidt orthogonalization.   independent vectors  Assume that we have  y1 y2 yn n tors we want to obtain   orthogonal vectors  v1 v2   vn   n onal vector is chosen to be the first independent vector:           . From these vec- . The first orthog-  To obtain the second orthogonal vector we use  tion of    that is in the direction of   y2  y2  v1  . This leads to the equation  , but subtract off the por-  where   a   is chosen so that    is orthogonal to   . This requires that  v2  v1     v1 v2  ,     =     v1 y2  ,  –  av1    =  v1 y2    ,     –  a v1 v1    ,     =  0  ,   5.18   or  v1  y1=  .  v2  =  y2  –  av1  ,  a  =  v1 y2     ---------------- v1 v1      , ,  .  Therefore to find the component of  to find the inner product between the two vectors. We call  tion of    in the direction of    on the vector   y2  .  v1  y2  v1 av1  ,  av1  , we need   the projec-  Projection  If we continue this process, the kth step will be   5.16    5.17    5.19    5.20   vk  =  yk  –  k  1–  i  1=  vi yk     ---------------vi vi vi      , ,  .  5-8   Vector Expansions  2 2+  To illustrate this process, we consider the following independent vectors in  2  :  The first orthogonal vector would be  y1  =  ,   y2  =  2 1  .  1 2  v1  =  y1  =  .  2 1   5.21    5.22   The second orthogonal vector is calculated as follows:  v2  =  y2  –  Ty2 v1 -----------v1 Tv1 v1  =  2 1 --------------------- 2 1  –  1 2  2 1  1 2  2 1  =  –  1 2  1.6 0.8  =  0.6– 1.2  .   5.23   5  See Figure 5.1 for a graphical representation of this process.  y2  y1, v1  y2  v2  av1  Figure 5.1  Gram-Schmidt Orthogonalization Example  Orthonormal  We could convert  malized  vectors by dividing each vector by its norm.   to a set of orthonormal  orthogonal and nor-   and   v1  v2  To experiment with this orthogonalization process, use the Neural Network  Design Demonstration Gram-Schmidt  nnd5gs .  Vector Expansions  Note that we have been using a script font     to represent general vectors  x n and bold type   , which can be written as col- umns of numbers. In this section we will show that general vectors in finite     to represent vectors in   x  5-9   5 Signal and Weight Vector Spaces  dimensional vector spaces can also be written as columns of numbers and  therefore are in some ways equivalent to vectors in   n  .  Vector Expansion  If a vector space  unique vector expansion:  X   has a basis set     v1 v2   vn          , then any   x X   has a   x  =  xivi  =  x1v1  +  x2v2  xnvn  +  +  .   5.24   n    i  1=  Therefore any vector in a finite dimensional vector space can be represent- ed by a column of numbers:  x  =  x1 x2  xn  T  .   5.25   x  This  terpret the meaning of  changes,  vector  x   is a representation of the general vector   . Of course in order to in-   we need to know the basis set. If the basis set   will change, even though it still represents the same general   x . We will discuss this in more detail in the next subsection.  x  x  If the vectors in the basis set are orthogonal   easy to compute the coefficients in the expansion. We simply take the inner  product of    with both sides of Eq.  5.24 :    it is very   vi vj    0=  j  ,      i  ,  vj     vj x ,    =    vj ,  xivi     =  xi vj vi       ,  =  xj vj vj       ,  .   5.26   n  i  1=  n    i  1=  Therefore the coefficients of the expansion are given by  xj  =  vj x ,     -------------- vj vj ,      .  When the vectors in the basis set are not orthogonal, the computation of the  coefficients in the vector expansion is more complex. This case is covered in  the following subsection.  Reciprocal Basis Vectors If a vector expansion is required and the basis set is not orthogonal, the re- ciprocal basis vectors are introduced. These are defined by the following  equations:   5.27    5.28   ri vj   ,     =  0  i  j  =  1  i  j ,=  5-10   Vector Expansions  Reciprocal Basis Vectors  where the basis vectors are  are   r1 r2   rn      .          v1 v2   vn           and the reciprocal basis vectors   If the vectors have been represented by columns of numbers  through vec- tor expansion , and the standard inner product is used  then Eq.  5.28  can be represented in matrix form as  where  ri vj   ,     =  Tvj ri  ,  RTB  I=  ,  B  =  v1 v2  vn  R  =  r1 r2  rn  ,  .  RT  =  B 1–  ,  Therefore R can be found from  5  and the reciprocal basis vectors can be obtained from the columns of R.  Now consider again the vector expansion  x  =  x1v1  +  x2v2  xnvn  +  +  .  Taking the inner product of r1 with both sides of Eq.  5.34  we obtain  r1 x  ,    =  x1 r1 v1     ,     +  x2 r1 v2     ,     xn r1 vn +    +     ,  .  By definition     r1 v2    ,  =     r1 v3  ,     r1 vn    =  =     ,  =  0     r1 v1  ,     =  1 .  Therefore the first coefficient of the expansion is   5.29    5.30    5.31    5.32    5.33    5.34    5.35    5.36    5.37    5.38   and in general  x1  =  r1 x  ,    ,  xj  =  rj x  ,    .  5-11   5 Signal and Weight Vector Spaces  As an example, consider the two basis vectors  s v1  =  ,   s v2  =  2 1  .  1 2  Suppose that we want to expand the vector  xs  =  0 3 --- 2  2 2+  v2  s2  v1  s1  in terms of the two basis vectors.  We are using the superscript   to indi- cate that these columns of numbers represent expansions of the vectors in  terms of the standard basis in  . The elements of the standard basis are  indicated in the adjacent figure as the vectors  . We need to use  this explicit notation in this example because we will be expanding the vec- tors in terms of two different basis sets.    and   2  s2  s1  s  The first step in the vector expansion is to find the reciprocal basis vectors.  RT  =  1–  =  2 1 1 2  2 --- 3 1 ---– 3  1 ---– 3 2 --- 3  r1  =  r2  =  .   5.41   2 --- 3 1 ---– 3  1 ---– 3 2 --- 3  Now we can find the coefficients in the expansion.  v x1  =  Txs r1  =  2 --- 3  1 ---– 3  =  1 ---– 2  v x2  =  Txs r2  =  1 ---– 3  2 --- 3  =  1  0 3 --- 2  0 3 --- 2  xv  =  RTxs  =  B 1– xs  =  2 --- 3 1 ---– 3  1 ---– 3 2 --- 3  =  0 3 --- 2  .  1 ---– 2 1  or, in matrix form,  So that  5-12   5.39    5.40    5.42    5.43    Vector Expansions  x  =  –  +  1v2  ,  1 ---v1 2   5.44   as indicated in Figure 5.2.  v2  x  v1  v2  - v1 2     Figure 5.2   Vector Expansion  Note that we now have two different vector expansions for  by   . In other words,   and   xv  xs  x  , represented   5  x  =  0s1  +  3 ---s2 2  =  –  1 ---v1 2  +  1v2  .   5.45   When we represent a general vector as a column of numbers we need to  know what basis set was used for the expansion. In this text, unless other- wise stated, assume the standard basis set was used.  x xv ,   Eq.  5.43  shows the relationship between the two different representations  . This operation, called a change of basis, will become very  of  important in later chapters for the performance analysis of certain neural  networks.  B 1– xs  =  To experiment with the vector expansion process, use the Neural Network  Design Demonstration Reciprocal Basis  nnd5rb .  5-13   5 Signal and Weight Vector Spaces  Summary of Results  Linear Vector Spaces Definition. A linear vector space, X, is a set of elements  vectors  defined  over a scalar field, F, that satisfies the following conditions:  1. An operation called vector addition is defined such that if   x X   and   y X  , then   x  y X+  .  x  y+  =  y  x+  .  y+ x    z+  =  x  +  z+ y    .  2.  3.  x  0+  x=   for all   x X  .  5. For each vector  0=  that   x–  x  +    .  x X  4. There is a unique vector   0 X  , called the zero vector, such that    there is a unique vector in X, to be called   x–  , such   6. An operation, called multiplication, is defined such that for all scalars   a  F  , and all vectors   x X  ,   ax X  .  7. For any   x X  ,   1x  x=    for scalar   1   .  8. For any two scalars   a  F   and   b  F  , and any   x X  ,   a bx    =  ab  x  .  9.  b+ a  x  ax  bx+  .  10.  y+ a x    ax  ay+  .  =  =  Linear Independence Consider n vectors  x1 x2   xn  least one of which is nonzero, such that          . If there exist n scalars   a1 a2   an        , at   a1x1  +  a2x2  anxn  +  +  0=  ,  then the   xi    are linearly dependent.  5-14   Summary of Results  Spanning a Space  Let X be a linear vector space and let  in X. This subset spans X if and only if for every vector  scalars   u1 u2   um   such that   x1 x2   xn  x2u2  xmum  x1u1  x  =  +  +  +                  .   be a subset of vectors   x X   there exist   Inner Product Any scalar function of x and y can be defined as an inner product,  x,y , pro- vided that the following properties are satisfied.  x y  ,   = x ay1   , x x  ,    0  +  .  y x  ,   by2   , where equality holds if and only if x is the zero vector.  b x y2   ,    a x y1   ,  =  +     .  1.  2.  3.  1.  2.  3.  4.  Norm A scalar function   x  0  .  x x ax x  0=   if and only if   x  0=  .   for scalar a.  = y+  a x x    y+  .  Angle The angle      between two vectors   x   and   y   is defined by  cos  =  x y  ,   --------------- x y  .  Orthogonality Two vectors  X  x y   are said to be orthogonal if   x y  ,    0=  .   Gram-Schmidt Orthogonalization Assume that we have n independent vectors  tors we will obtain n orthogonal vectors       . v1 v2   vn   y1 y2      yn  . From these vec-   is called a norm if it satisfies the following properties:  5  v1  y1=  5-15   5 Signal and Weight Vector Spaces  where  vk  =  yk  –  k  1–  i  1=  vi yk     ---------------vi vi vi      , ,  ,  vi yk     ---------------vi vi vi      , ,  is the projection of   yk   on   vi  .  Vector Expansions  x  =  xivi  =  x1v1  +  x2v2  xnvn  +  +  .  n    i  1=  For orthogonal vectors,  Reciprocal Basis Vectors  To compute the reciprocal basis vectors:  xj  =  vj x  ,   -------------- vj vj ,      ri vj   ,    0=  1=  i  j  i  j=  xj  =  rj x  ,    .  B  =  v1 v2  vn  R  =  r1 r2  rn  ,  ,  RT  =  B 1–  .  xv  =  B 1– xs  .  In matrix form:  5-16   Solved Problems  Solved Problems  P5.1 Consider the single-neuron perceptron network shown in Figure  P5.1. Recall from Chapter 3  see Eq.  3.6   that the decision bound- ary for this network is given by  . Show that the decision  boundary is a vector space if  b  Wp b+ 0=  0=  .  Inputs  Sym. Hard Limit Layer  - Title -  1xR  p  cid:0  cid:0 W Rx1  cid:0  cid:0 b  cid:0  cid:0   1x1  1  R  a  cid:0  cid:0  1x1 n  cid:0  cid:0  1x1  cid:0  cid:0   1  a = hardlims  Wp + b   - Exp -  5  Figure P5.1  Single-Neuron Perceptron  To be a vector space the boundary must satisfy the ten conditions given at  the beginning of this chapter. Condition 1 requires that when we add two  vectors together the sum remains in the vector space. Let   be two  vectors on the decision boundary. To be on the boundary they must satisfy   and   p1  p2  If we add these two equations together we find  Wp1  0=  Wp2  0=  .  Therefore the sum is also on the decision boundary.  W p1 p2+      0=  .  Conditions 2 and 3 are clearly satisfied. Condition 4 requires that the zero  , the zero vector is on the decision  vector be on the boundary. Since  boundary. Condition 5 implies that if   must  also be on the boundary. If    is on the boundary, then    is on the boundary, then  0= p  W0  p–  p  If we multiply both sides of this equation by -1 we find  Wp  0=  .  W p–    0=  .  Therefore condition 5 is satisfied.  5-17   5 Signal and Weight Vector Spaces  Condition 6 will be satisfied if for any   is also on the  boundary. This can be shown in the same way as condition 5. Just multiply  both sides of the equation by    on the boundary    instead of by 1.  ap  p  a  W ap    0=  Conditions 7 through 10 are clearly satisfied. Therefore the perceptron de- cision boundary is a vector space.  P5.2 Show that the set   Y   of nonnegative    f t   0    continuous functions   is not a vector space.  This set violates several of the conditions required of a vector space. For ex- ample, there are no negative vectors, so condition 5 cannot be satisfied. Al- so, consider condition 6. The function  a   is a member of   . Then  . Let   f t   2–=  t=  Y  Therefore   af t    is not a member of   Y  , and condition 6 is not satisfied.  af 2   =  2 2–  =  0– 4  .  P5.3 Which of the following sets of vectors are independent? Find the   dimension of the vector space spanned by each set.  ii.  sin  t  cos  t  2  cos     ---+ t  4  i.  iii.  1 1 1  1 1 1 1  1 0 1  1 0 1 1  1 2 1  1 2 1 1  5-18  i. We can solve this problem several ways. First, let’s assume that the  vectors are dependent. Then we can write  a1  +  a2  +  a3  =  .  1 0 1  1 2 1  0 0 0  1 1 1   Solved Problems  If we can solve for the coefficients and they are not all zero, then the vectors  are dependent. By inspection we can see that if we let   and  a3 dent.  , then the equation is satisfied. Therefore the vectors are depen-  1–=  1–=  2=  a1  a2  ,   Another approach, when we have  equation in matrix form:  n   vectors in   n  , is to write the above   1 1 1 1 0 2 1 1 1  a1 a2 a3  =  0 0 0  If the matrix in this equation has an inverse, then the solution will require  that all coefficients be zero; therefore the vectors are independent. If the  matrix is singular  has no inverse , then a nonzero set of coefficients will  work, and the vectors are dependent. The test, then, is to create a matrix  using the vectors as columns. If the determinant of the matrix is zero  sin- gular matrix , then the vectors are dependent; otherwise they are indepen- dent. Using the Laplace expansion [Brog91] on the first column, the  determinant of this matrix is  5  1 1 1 1 0 2 1 1 1  =  1 0 2 1 1  +  1–   1 1 1 1  +  1 1 1 0 2  =  2–  + +  0  2  =  0  Therefore the vectors are dependent.  The dimension of the space spanned by the vectors is two, since any two of  the vectors can be shown to be independent.  ii. By using some trigonometric identities we can write  cos     ---+ t  4  1– ------- 2  =  sin  t  +  cos  t  .  1 ------- 2  Therefore the vectors are dependent. The dimension of the space spanned  by the vectors is two, since no linear combination of   is iden- tically zero.   and   cos  sin  t  t  iii. This is similar to part  i , except that the number of vectors is less than  the size of the vector space they are drawn from  three vectors in   . In  this case the matrix made up of the vectors will not be square, so we will  not be able to compute a determinant. However, we can use something  called the Gramian [Brog91]. It is the determinant of a matrix whose i, j  element is the inner product of vector i and vector j. The vectors are depen- dent if and only if the Gramian is zero.  4  5-19   5 Signal and Weight Vector Spaces  For our problem the Gramian would be  G  =         x1 x1 x2 x1 x3 x1  , , ,    x1 x2   x2 x2   x3 x2         , , ,    x1 x3   x2 x3   x3 x3         , , ,         ,  x1  =  x2  =  x3  =  .  1 0 1 1  1 2 1 1  1 1 1 1  where  Therefore  G  =  4 3 5 3 3 3 5 3 7  =  4 3 3 3 7  +  3–   3 5 3 7  +  5 3 5 3 3  =  48  18–  30–  =  0  .  We can also show that these vectors are dependent by noting  2  –  1  –  1  =  .  1 1 1 1  1 0 1 1  1 2 1 1  0 0 0 0  The dimension of the space must therefore be less than 3. We can show that  x1   are independent, since   and   x2  G  =  =  4  0  .  4 3 3 3  Therefore the dimension of the space is 2.  P5.4 Recall from Chapters 3 and 4 that one-layer perceptrons can only  be used to recognize patterns that are linearly separable  can be  separated by a linear boundary — see Figure 3.3 . If two patterns  are linearly separable, are they always linearly independent?  No, these are two unrelated concepts. Take the following simple example.  Consider the two input perceptron shown in Figure P5.2.  5-20   Solved Problems  Suppose that we want to separate the two vectors  p1  =  0.5 0.5  p2  =  .  1.5 1.5  If we choose the weights and offsets to be  then the decision boundary     is shown in the figure to the left.  Clearly these two vectors are linearly separable. However, they are not lin- early independent since   Wp b+   and   2–=  w11  w12  0=  1=  1=  ,   ,   b  .  =  p2  3p1  Wp + b = 0  p2  p1  Inputs  Sym. Hard Limit Neuron  - Title -  p1  p2  SxR  w11  cid:0  cid:0 Σ n  cid:0  cid:0  w12  b   cid:0  cid:0  a  cid:0  cid:0   1  a = hardlims  Wp + b   - Exp -  Figure P5.2  Two-Input Perceptron  5  P5.5 Using the following basis vectors, find an orthogonal set using   Gram-Schmidt orthogonalization.  y1  =  y2  =  y3  =  1 0 0  0 1 0  1 1 1  Step 1.  v1  =  y1  =  1 1 1  5-21   5 Signal and Weight Vector Spaces  Step 2.  Step 3.  v2  =  y2  –  =  –  Ty2 v1 -----------v1 Tv1 v1  1 0 0  =  –  =  1 0 0  1 3 1 3 1 3  2 3 1 3– 3 1–  1 1 1  1 0 0 -------------------------- 1 1 1  1 1 1  1 1 1  v3  =  y3  –  Ty3 v1 -----------v1 Tv1 v1  –  Ty3 v2 -----------v2 Tv2 v2  v3  =  –  0 1 0  1 1 1  0 1 0 -------------------------- 1 1 1  1 1 1  1 1 1  2 3  1 3–  1–  3  –  ----------------------------------------------------------------  2 3  1 3–  1–  3  0 1 0  2 3 1 3– 3 1–  2 3 1 3– 3 1–  v3  =  –  0 1 0  1 3 1 3 1 3  –  3 1– 1 6 1 6  =  0 1 2 2 1–  P5.6 Consider the vector space of all polynomials defined on the inter-  val [-1, 1]. Show that   x y  ,    x t   y t dt   is a valid inner product.  1  =  1–  An inner product must satisfy the following properties.  1.  x y  ,    =  y x  ,    x y  ,    =  x t   y t dt  =  y t x t   dt  =  y x  ,    1    1–  2.  x ay1   ,  +  by2     =  a x y1   ,     +  b x y2   ,    1    1–  5-22   Solved Problems  1    1– a x y1   ,    =  +  b x y2   ,     x ay1   ,  +  by2     =  x t    ay1 t     +  by2 t   dt  =  1  1   a x t   y1 t dt 1–  + b x t   y2 t dt 1–  3.  x x  ,    0  , where equality holds if and only if x is the zero vector.  x x  ,    =  x t x t  td  =  x2 t    dt  0  1    1–  1    1–  Equality holds here only if   x t     0=   for   1–     t  1  , which is the zero vector.  P5.7 Two vectors from the vector space described in the previous prob- .   lem  polynomials defined on the interval [-1, 1]  are   and  Find an orthogonal set of vectors based on these two vectors.  t+  t–  1  1  5  Step 1.  Step 2.  where  Therefore  v1  =  y1  =  1  t+  v2  =  y2  –  v1 y2     ----------------v1 v1 v1      , ,  v1 y2    ,     =  t+ 1   1 t–  dt  =  t3   ---– t  3  1  1–  =  2   ---  3  –  2   ---–  3  =  4 --- 3  v1 v1    ,     =  t+ 1  2dt  =  3 t+ 1 ------------------ 3  1  1–  =  8   ---  3  –  0   =  8 --- 3  .  1    1–  1    1–  v2  =  t– 1    –  4 3 8 3---------- 1  t+    =  1 --- 2  3 ---t– 2  .  5-23   5 Signal and Weight Vector Spaces  P5.8 Expand   x  =   in terms of the following basis set.  T  6 9 9  The first step is to calculate the reciprocal basis vectors.   v1  =  v2  =  v3  =  1 2 3  1 3 2  1 1 1  B  =  1 1 1 1 2 3 1 3 2  B 1–  =  5 --- 3 1 ---– 3 1 ---– 3  1 ---– 3 1 ---– 3 2 --- 3  1 ---– 3 2 --- 3 1 ---– 3  Therefore taking the rows of B-1,   r1  =  r2  =  r3  =  3 1– 3 1– 2 3  3 1– 2 3 3 1–  .  5 3 3 1– 3 1–  The coefficients in the expansion are calculated  v x1  =  Tx r1  =  5 --- 3  1– ------ 3  1– ------ 3  v x2  =  Tx r2  =  1– ------ 3  1– ------ 3  2 --- 3  v x3  =  Tx r3  =  1– ------ 3  2 --- 3  1– ------ 3  6 9 9  6 9 9  6 9 9  =  4  =  1  =  1  ,  and the expansion is written  5-24   Solved Problems  x  =  vv1 x1  +  vv2 x2  +  vv3 x3  =  4  +  1  +  1  .  1 1 1  1 2 3  1 3 2  We can represent the process in matrix form:  xv  =  B 1– x  =  5 --- 3 1 ---– 3 1 ---– 3  1 ---– 3 1 ---– 3 2 --- 3  1 ---– 3 2 --- 3 1 ---– 3  6 9 9  =  .  4 1 1  Recall that both  expanded in terms of different basis sets.  It is assumed that  standard basis set, unless otherwise indicated.    are representations of the same vector, but are    uses the    and   x  x  xv  5  5-25   5 Signal and Weight Vector Spaces  Epilogue  This chapter has presented a few of the basic concepts of vector spaces, ma- terial that is critical to the understanding of how neural networks work.  This subject of vector spaces is very large, and we have made no attempt to  cover all its aspects. Instead, we have presented those concepts that we feel  are most relevant to neural networks. The topics covered here will be revis- ited in almost every chapter that follows.  The next chapter will continue our investigation of the topics of linear al- gebra most relevant to neural networks. There we will concentrate on lin- ear transformations and matrices.  5-26   Further Reading  Further Reading  [Brog91]   W. L. Brogan, Modern Control Theory, 3rd Ed., Englewood  Cliffs, NJ: Prentice-Hall, 1991.  This is a well-written book on the subject of linear systems.  The first half of the book is devoted to linear algebra. It also  has good sections on the solution of linear differential equa- tions and the stability of linear and nonlinear systems. It  has many worked problems.  [Stra76]   G. Strang, Linear Algebra and Its Applications, New York:  Academic Press, 1980.  Strang has written a good basic text on linear algebra.  Many applications of linear algebra are integrated into the  text.  5  5-27   5 Signal and Weight Vector Spaces  Exercises  E5.1 Consider again the perceptron described in Problem P5.1. If   b  0  , show   that the decision boundary is not a vector space.  E5.2 What is the dimension of the vector space described in Problem P5.1?  E5.3 Consider the set of all continuous functions that satisfy the condition   f 0   0=  . Show that this is a vector space.  E5.4 Show that the set of   2  2   matrices is a vector space.  E5.5 Consider a perceptron network, with the following weights and bias.  W  =  1 0 1–  ,   b  0=  .  i. Write out the equation for the decision boundary.  ii. Show that the decision boundary is a vector space.  Demonstrate   that the 10 criteria are satisfied for any point on the boundary.   iii. What is the dimension of the vector space?  iv. Find a basis set for the vector space.  E5.6 The three parts to this question refer to subsets of the set of real-valued   continuous functions defined on the interval [0,1]. Tell which of these sub- sets are vector spaces. If the subset is not a vector space, identify which of  the 10 criteria are not satisfied.  i. All functions such that   f 0.5    2=  .  ii. All functions such that    f 0.75    0=  .  iii. All functions such that   f 0.5    =  –   f 0.75    3–  .  E5.7 The next three questions refer to subsets of the set of real polynomials de- fined over the real line  e.g.,   . Tell which of these subsets are  vector spaces. If the subset is not a vector space, identify which of the 10  criteria are not satisfied.  6t2  2t  +  +  3  i. Polynomials of degree 5 or less.  ii. Polynomials that are positive for positive t.  iii. Polynomials that go to zero as t goes to zero.  5-28   E5.8 Which of the following sets of vectors are independent? Find the dimension  of the vector space spanned by each set.  Verify your answers to parts  i   and  iv  using the MATLAB function rank.   » 2 + 2 ans =       4  ii.  sin  t  iii.  1  t+  cos  t  1  t–  cos  2t    i.  iv.  1 2 3  1 2 2 1  Exercises  1 0 1  1 0 0 1  1 1– 1–  1 2 1  3 4 4 3  1 0 0  5-29  E5.9 Recall the apple and orange pattern recognition problem of Chapter 3. Find  the angles between each of the prototype patterns  orange and apple  and  the test input pattern  oblong orange . Verify that the angles make intui- tive sense.  5  1 1 1–  1 1 0  p1  =    orange    p2  =    apple    p  =  1– 1– 1–  E5.10 Using the following basis vectors, find an orthogonal set using Gram-  Schmidt orthogonalization.  Check your answer using MATLAB.   » 2 + 2 ans =       4  y1  =  y2  =  y3  =  1 1 1  E5.11 Consider the vector space of all piecewise continuous functions on the in- , which is defined in Figure E15.1, contains   terval [0, 1]. The set  three vectors from this vector space.  f1 f2 f3          i. Show that this set is linearly independent. ii. Generate an orthogonal set using the Gram-Schmidt procedure. The   inner product is defined to be   5 Signal and Weight Vector Spaces  f g  ,    =  f t g t dt  .  1  0  f2 t   1  -1  1  1  1  -1  f1 t   1  -1  f3 t   1  Figure E15.1  Basis Set for Exercise E5.11  E5.12 Consider the vector space of all piece wise continuous functions on the in- , which is defined in Figure E15.2, contains two   terval [0,1]. The set  vectors from this vector space.  f1 f2    f1 t   1  -1  1  1  f2 t   2 1  -1 -  Figure E15.2  Basis Set for Exercise E5.12  i. Generate an orthogonal set using the Gram-Schmidt procedure. The   inner product is defined to be  f g  ,    =  f t g t dt  .  1  0  5-30   Exercises   g  t   3 2 1  -1 -2 -3  h  t   1  1  1  Figure E15.3  Vectors vectors   ii. Expand the vectors   g   and   h  h   and    for Exercise E5.12 part ii.  g  in Figure E15.3 in terms of the orthog-  onal set you created in Part 1. Explain any problems you find.  E5.13 Consider the set of polynomials of degree 1 or less. This is a linear vector   space. One basis set for this space is   5  Using this basis set, the polynomial y = 2 + 4t can be represented as  Consider the new basis set  {  v1  t+=  1  ,  v2  t–=  1  }  Use reciprocal basis vectors to find the representation of y in terms of this  new basis set.  E5.14 A vector x can be expanded in terms of the basis vectors   v1 v2 { , }   as  The vectors  as  v1   and   v2   can be expanded in terms of the basis vectors   s1 s2 { , }     {  u1  1= u2  ,  t=  }  yu  =  2 4  x  =  1v1  +  1v2  v1 v2  =  =  1s1 1s1  –  +  1s2 1s2  5-31   5 Signal and Weight Vector Spaces  i. Find the expansion for x in terms of the basis vectors  ii. A vector y can be expanded in terms of the basis vectors   s1 s2 { , }  .  s1 s2 { , }   as  y  =  1s1  +  1s2  .  Find the expansion of y in terms of the basis vectors   v1 v2 { , }  .  E5.15 Consider the vector space of all continuous functions on the interval [0,1].  , which is defined in the figure below, contains two vectors   The set  from this vector space.  f1 f2    f1 t   =  1     0     t  1  f2 t   =  1 t–       0     t  1  1  1  t  1  t  1  Figure E15.4  Independent Vectors for Exercise E5.15  i. From these two vectors, generate an orthogonal set    g1 g2    using   the Gram-Schmidt procedure. The inner product is defined to be  f g  ,    =  f t g t dt  .  Plot the two orthogonal vectors   g1   and   g2   as functions of time.  ii. Expand the following vector    in terms of the orthogonal set you   created in part i., using Eq.  5.27 . Demonstrate that the expansion  is correct by reproducing h as a combination of    and   .  h  g1  g2  h t   =  1– t       0     t  1  t  1  0  1  Figure E15.5  Vector    for Exercise E5.15  h  E5.16 Consider the set of all complex numbers. This can be considered a vector  space, because it satisfies the ten defining properties. We can also define   -1  5-32   Exercises  an inner product for this vector space  where  , and  This leads to the following definition for norm:    is the real part of   x y  Im x   Re x   x  Re x Re y   Im x Im y  =  is the imaginary part of  x  +  ,  .   x  =  x y    .  i. Consider the following basis set for the vector space described   . Using the Gram-Schmidt method,   above:  j+ find an orthogonal basis set.  2j+  v1  v2  ,   =  =  1  2  ii. Using your orthogonal basis set from part i., find vector expansions  . This will allow you to write  x u1  ,  u2 j–  as a columns of numbers  u2  for  u1 ,  x u1  1 = , and   , and    and   u2  j+  j+  x  ,   =  =  3  1  .  iii. We now want to represent the vector    using the basis set   x  Use reciprocal basis vectors to find the expansion for  x . This will allow you to write  the basis vectors   column of numbers    u1 u2 xu .          u1 u2  .   in terms of   as a new  x  iv. Show that the representations for   iii. are equivalent  the two columns of numbers  resent the same vector    .  x  x   that you found in parts ii. and   both rep-   and   xu  x  5  E5.17 Consider the vectors defined in Figure E15.6. The set   dard basis set. The set   is an alternate basis set. The vector  vector that we wish to represent with respect to the two basis sets.  u1 u2        s1 s2     is the stan-  is a   x  s2  s1  u1  Figure E15.6  Vector Definitions for Exercise E5.17  i. Write the expansion for   x ii. Write the expansions for  u1  s1 s2     in terms of the standard basis   s1 s2    .   and   u2   in terms of the standard basis   x  u2  5-33   5 Signal and Weight Vector Spaces  iii. Using reciprocal basis vectors, write the expansion for    in terms of   x  the basis     u1 u2      .  iv. Draw sketches, similar to Figure 5.2, that demonstrate that the ex-  pansions of part i. and part iii. are equivalent.  E5.18 Consider the set of all functions that can be written in the form   .  This set can be considered a vector space, because it satisfies the ten defin- ing properties.   + t  sin  A    i. Consider the following basis set for the vector space described   v1 sin  above:  x 2 pansion , using this basis set.  ,  v2  as a column of numbers  t   sin + 4  t  cos  = t   cos  t   =  =  . Represent the vector   xv    find the vector ex-  ii. Using your basis set from part i., find vector expansions for   u1  =  2  sin  t   +  cos  t   ,   u2  =  3  sin  t   .  iii. We now want to represent the vector    u1 u2 terms of the basis vectors   u1 u2 xu a new column of numbers   . Use reciprocal basis vectors to find the expansion for  . This will allow you to write    of part i., using the basis set   in   as   x x   .  x        iv. Show that the representations for   iii. are equivalent  the two columns of numbers  resent the same vector    .  x  x   that you found in parts i. and   both rep-   and   xu  xv  E5.19 Suppose that we have three vectors:   x y z X     . We want to add some mul-  tiple of y to x, so that the resulting vector is orthogonal to z.  i. How would you determine the appropriate multiple of y to add to x? ii. Verify your results in part i. using the following vectors.  x  =  1 0  y  =  1 0.5  z  =  0.5 1  iii. Use a sketch to illustrate your results from part ii.  E5.20 Expand   x  =  T  1 2 2  swer using MATLAB.    in terms of the following basis set.  Verify your an-  » 2 + 2 ans =       4  v1  =  v2  =  v3  =  1 1 2–  1 1 0  1– 1 0  5-34   Exercises  E5.21 Find the value of a that makes   x ay–  Show that for this value of a the vector  that   a minimum.  Use  z  x ay–  =   is orthogonal to   y  x  =  x x  ,  1 2  .    and   x ay–  2  +  ay 2  =  x 2  .   The vector ay is the projection of x on y.  Draw a diagram for the  case where x and y are two-dimensional. Explain how this concept  is related to Gram-Schmidt orthogonalization.  5  5-35   Objectives  6 Linear Transformations for   Neural Networks  Objectives  Objectives Theory and Examples  Linear Transformations Matrix Representations Change of Basis Eigenvalues and Eigenvectors  Diagonalization  Summary of Results Solved Problems Epilogue Further Reading Exercises  6-1 6-2 6-2 6-3 6-6 6-10 6-13 6-15 6-17 6-29 6-30 6-31  6  This chapter will continue the work of Chapter 5 in laying out the mathe- matical foundations for our analysis of neural networks. In Chapter 5 we  reviewed vector spaces; in this chapter we investigate linear transforma- tions as they apply to neural networks.  As we have seen in previous chapters, the multiplication of an input vector  by a weight matrix is one of the key operations that is performed by neural  networks. This operation is an example of a linear transformation. We  want to investigate general linear transformations and determine their  fundamental characteristics. The concepts covered in this chapter, such as  eigenvalues, eigenvectors and change of basis, will be critical to our under- standing of such key neural network topics as performance learning  in- cluding the Widrow-Hoff rule and backpropagation  and Hopfield network  convergence.  6-1   6 Linear Transformations for Neural Networks  Theory and Examples  Recall the Hopfield network that was discussed in Chapter 3.  See Figure  6.1.  The output of the network is updated synchronously according to the  equation  1+ a t    =  satlin Wa t  b+      .   6.1   Notice that at each iteration the output of the network is again multiplied  by the weight matrix W. What is the effect of this repeated operation? Can  we determine whether or not the output of the network will converge to  some steady state value, go to infinity, or oscillate? In this chapter we will  lay the foundation for answering these questions, along with many other  questions about neural networks discussed in this book.  Initial  Condition  Recurrent Layer  p S x 1  1  S   cid:0  W S x S  cid:0  b  cid:0  S x 1   cid:0  cid:0   cid:0  cid:0   D  a t  S x 1  n t + 1  S x 1   cid:0  cid:0  a t + 1   cid:0  cid:0  S x 1  cid:0  cid:0   S  a 0  = p      a t + 1  = satlins  Wa t  + b   Figure 6.1  Hopfield Network  Linear Transformations  We begin with some general definitions.  Transformation  A transformation consists of three parts:  1. a set of elements   X  , called the domain,  2. a set of elements   Y  , called the range, and  3. a rule relating each    to an element   yi  Y  .  =  xi  yi  = xi X  6-2   Matrix Representations  Linear Transformation  A transformation A is linear if: 1.  x2+  A ax  x1 x2 x X  for all   for all   A x1  R  X  2.  ,   ,   ,   a      =  =  A x1 aA x    A x2 + .    ,  A x    x  θ  Consider, for example, the transformation obtained by rotating vectors in  2  by an angle , as shown in the figure to the left. The next two figures  illustrate that property 1 is satisfied for rotation. They show that if you  want to rotate a sum of two vectors, you can rotate each vector first and  then sum them. The fourth figure illustrates property 2. If you want to ro- tate a scaled vector, you can rotate it first and then scale it. Therefore ro- tation is a linear operation.  Matrix Representations  x 1 + x 2  x 2  x 1  As we mentioned at the beginning of this chapter, matrix multiplication is  an example of a linear transformation. We can also show that any linear  transformation between two finite-dimensional vector spaces can be repre- sented by a matrix  just as in the last chapter we showed that any general  vector in a finite-dimensional vector space can be represented by a column  of numbers . To show this we will use most of the concepts covered in the  previous chapter.          v1 v2   vn  Let  a basis for vector space  y  Y     be a basis for vector space    . This means that for any two vectors   , and let   u1 u2   um x X  X  Y         be    and   6  A x 1 + x 2   A x 1   A x 2   A ax    = aA x    ax  A x    x  Let  A Then   be a linear transformation with domain   X   and range   Y A:X      Y   .   can be written  Since A is a linear operator, Eq.  6.4  can be written  x  =  xivi   and   y  =  yiui  .  m  i  1=  n  i  1=  A x   y=  m  m  A  n      j  1=   xjvj    =  yiui  .  i  1=  n    j  1=  xjA vj    =  yiui  .  i  1=  6-3   6.2    6.3    6.4    6.5    6 Linear Transformations for Neural Networks  Since the vectors  combinations of the basis vectors for    are elements of  :  A vj  Y    Y  , they can be written as linear    Note that the notation used for the coefficients of this expansion,  , was  not chosen by accident.  If we substitute Eq.  6.6  into Eq.  6.5  we obtain  aij  A vj    =  aijui  .  m  i  1=  n    m    xj  j  1=  i  1=  aijui  =  yiui  .  i  1=  m  m  n  m    ui    aij xj  =  yiui  .  i  1=  j  1=  i  1=  m    n    ui      i  1=  j  1=  aij xj    yi–   0=  .  n    j  1=  aij xj  yi=  .  x1 x2  a11 a12  a1n y1 a21 a22  a2n y2      am1 am2  amn ym  xn  =  The order of the summations can be reversed, to produce  This equation can be rearranged, to obtain  Recall that since the  means that each coefficient that multiplies  cally zero  see Eq.  5.4  , therefore   form a basis set they must be independent. This   in Eq.  6.9  must be identi-  ui  ui  This is just matrix multiplication, as in   6.6    6.7    6.8    6.9    6.10    6.11   We can summarize these results: For any linear transformation between  two finite-dimensional vector spaces there is a matrix representation. When  we multiply the matrix times the vector expansion for the domain vector  ,  we obtain the vector expansion for the transformed vector   x  .   y  6-4   2 2+  A x   θ  s2  x  s1  A s1   sin θ   s1  cos θ   θ  A s2   -sin θ   s2  cos θ   θ  Matrix Representations  Keep in mind that the matrix representation is not unique  just as the rep- resentation of a general vector by a column of numbers is not unique — see  Chapter 5 . If we change the basis set for the domain or for the range, the  matrix representation will also change. We will use this fact to our advan- tage in later chapters.   As an example of a matrix representation, consider the rotation transfor- mation. Let’s find a matrix representation for that transformation. The key  step is given in Eq.  6.6 . We must transform each basis vector for the do- main and then expand it in terms of the basis vectors of the range. In this   , so to keep  example the domain and the range are the same   = things simple we will use the standard basis for both   = shown in the adjacent figure.   Y 2 vi ui   , as   = =  si  X  The first step is to transform the first basis vector and expand the resulting  transformed vector in terms of the basis vectors. If we rotate  clockwise by the angle    we obtain   counter-  s1    A s1    =  cos      s1  +  sin      s2  =  ai1si  =  a11s1  +  a21s2  ,   6.12   2    i  1=  2    i  1=  as can be seen in the middle left figure. The two coefficients in this expan- sion make up the first column of the matrix representation.  The next step is to transform the second basis vector. If we rotate  terclockwise by the angle    we obtain    s2   coun-  6  A s2    =  sin–      s1  +  cos      s2  =  ai2si  =  a12s1  +  a22s2  ,   6.13   as can be seen in the lower left figure. From this expansion we obtain the  second column of the matrix representation. The complete matrix repre- sentation is thus given by  A  =  cos sin       sin– cos       .   6.14    Verify for yourself that when you multiply a vector by the matrix of Eq.   6.14 , the vector is rotated by an angle .  In summary, to obtain the matrix representation of a transformation we  use Eq.  6.6 . We transform each basis vector for the domain and expand it  in terms of the basis vectors of the range. The coefficients of each expansion  produce one column of the matrix.  6-5   6 Linear Transformations for Neural Networks  To graphically investigate the process of creating a matrix representation,  use the Neural Network Design Demonstration Linear Transformations   nnd6lt .  Change of Basis  We notice from the previous section that the matrix representation of a lin- ear transformation is not unique. The representation will depend on what  basis sets are used for the domain and the range of the transformation. In  this section we will illustrate exactly how a matrix representation changes  as the basis sets are changed.  Consider a linear transformation  A:X for vector space    Therefore, any vector   Y , and let  u1 u2   um    can be written x X  X      . Let   be a basis for vector space Y.   v1 v2   vn   be a basis           and any vector   Y   can be written  y  the matrix representation will be  So if   or  n  m  x  =  xivi  ,  i  1=  y  =  yiui  .  i  1=  A x   y=  x1 x2  a11 a12  a1n y1 a21 a22  a2n y2      am1 am2  amn ym  xn  =  ,  Ax  y=  .  6-6  Now suppose that we use different basis sets for   t1 t2   tn basis for    . With the new basis sets, the vector    be the new basis for   , and let    Y  X         and   X w1 w2   wm    is written x X  . Let    Y    be the new    6.15    6.16    6.17    6.18    6.19    Change of Basis  and the vector   Y   is written  y  This produces a new matrix representation:  x'1 x'2  a'11 a'12  a'1n y'1 a'21 a'22  a'2n y'2      a'm1 a'm2  a'mn y'm  x'n  =  ,  or  x  =  x'iti  ,  n  i  1=  y  =  y'iwi  .  m  i  1=  A'x'  y'=  .  ti  =  tjivj  .  j  1=  n  m  wi  =  wjiuj  .  j  1=  What is the relationship between  the relationship between the two basis sets. First, since each  ment of   , they can be expanded in terms of the original basis for   ? To find out, we need to find   is an ele-   and   A'  ti  A  X  X  :  6  Next, since each  the original basis for   wi  Y  :   is an element of   Y  , they can be expanded in terms of    6.20    6.21    6.22    6.23    6.24    6.25    6.26   Therefore, the basis vectors can be written as columns of numbers:  ti  =  wi  =  t1i t2i  tni  .  w1i w2i  wmi  Define a matrix whose columns are the ti:  6-7   6 Linear Transformations for Neural Networks  Bt  =  t1 t2  tn  .  Then we can write Eq.  6.20  in matrix form:  x  =  x'1t1  +  x'2t2  +   x'ntn  +  =  Btx'  .  This equation demonstrates the relationships between the two different  representations for the vector  .  Note that this is effectively the same as  Eq.  5.43 . You may want to revisit our discussion of reciprocal basis vec- tors in Chapter 5.  Now define a matrix whose columns are the wi:  x  This allows us to write Eq.  6.21  in matrix form,  Bw  =  w1 w2  wm  .  y  =  Bwy'  ,  which then demonstrates the relationships between the two different rep- resentations for the vector y. Now substitute Eq.  6.28  and Eq.  6.30  into Eq.  6.19 :  If we multiply both sides of this equation by   1– Bw   we obtain   6.27    6.28    6.29    6.30    6.31    6.32    6.33   ABtx'  =  Bwy'  .    1– ABt Bw  x'  y'=  .  A'  =    1– ABt Bw    .  Change of Basis  A comparison of Eq.  6.32  and Eq.  6.23  yields the following operation for  a change of basis:  Similarity Transform  This key result, which describes the relationship between any two matrix  representations of a given linear transformation, is called a similarity  transform [Brog91]. It will be of great use to us in later chapters. It turns  out that with the right choice of basis vectors we can obtain a matrix rep- resentation that reveals the key characteristics of the linear transforma- tion it represents. This will be discussed in the next section.  As an example of changing basis sets, let’s revisit the vector rotation exam- ple of the previous section. In that section a matrix representation was de- veloped using the standard basis set  representation using the basis  t1 t2 { , }  s1 s2 { , } , which is shown in the adjacent fig-  . Now let’s find a new   2 2+  6-8   t2  s2  t1  s1  Change of Basis  ure.  Note that in this example the same basis set is used for both the do- main and the range.   The first step is to expand  in Eq.  6.24  and Eq.  6.25 . By inspection of the adjacent figure we find:   in terms of the standard basis set, as    and   t1  t2  Therefore we can write  Now we can form the matrix  t1  =  s1  +  0.5s2  ,  t2  =  s– 1  s2+  .  t1  =  1 0.5  t2  =  .  1– 1  Bt  =  t1 t2  =  1 1– 0.5 1  ,  Bw  =  Bt  =  1 1– 0.5 1  .  and, because we are using the same basis set for both the domain and the  range of the transformation,  6  We can now compute the new matrix representation from Eq.  6.33 :  A'  =    1– ABt Bw    =  1 3  =  +  sin 5 --- 6  sin  2 3 3 1– cos  2 3 2 3  –  4 3  cos sin sin  sin– cos  1 1– 0.5 1  –  1 3  sin  +  cos  .  Take, for example, the case where = 30 .  A'  =  –  1.033 0.667 0.417 0.699  ,  and   6-9   6.34    6.35    6.36    6.37    6.38    6.39    6.40    6 Linear Transformations for Neural Networks  A  =  0.866 0.5– 0.5 0.866  .   6.41   To check that these matrices are correct, let’s try a test vector  x  =  , which corresponds to   x'  =   6.42   .  1 0  1 0.5   Note that the vector represented by   is  t1 basis set.  The transformed test vector would be   and   x'  x  , a member of the second   y  =  Ax  =  0.866 0.5– 0.5 0.866  1 0.5  =  0.616 0.933  ,  which should correspond to  y'  =  A'x'  =  –  1.033 0.667 0.416 0.699  1 0  =  1.033 0.416  .   6.43    6.44    does correspond to  ? Both should be represen- How can we test to see if  y' tations of the same vector,   uses  y . In Chapter 5 we used the re- the basis  ciprocal basis vectors to transform from one representation to another  see  Eq.  5.43  . Using that concept we have  , in terms of two different basis sets;    uses the basis   t1 t2 { , }  s1 s2 { , }   and   y'  y  y  y'  =  B 1– y  =  1–  1– 1 0.5 1  0.616 0.933  =  2 3 3 1–  2 3 2 3  0.616 0.933  =  1.033 0.416  ,   6.45   which verifies our previous result. The vectors are displayed in the figure  to the left. Verify graphically that the two representations,  , given  by Eq.  6.43  and Eq.  6.44 , are reasonable.   and   y'  y  Eigenvalues and Eigenvectors  In this final section we want to discuss two key properties of linear trans- formations: eigenvalues and eigenvectors. Knowledge of these properties  will allow us to answer some key questions about neural network perfor- mance, such as the question we posed at the beginning of this chapter, con- cerning the stability of Hopfield networks.  Eigenvalues Eigenvectors  Let’s first define what we mean by eigenvalues and eigenvectors. Consider  .  The domain is the same as the range.   a linear transformation  Those vectors   that sat- isfy   that are not equal to zero and those scalars   z X  A:X  X    6-10  t2  s2  y = A  x   t1 = x  s1   Eigenvalues and Eigenvectors  A z    z=   6.46   are called eigenvectors    , respectively. Notice that  the term eigenvector is a little misleading, since it is not really a vector but  a vector space, since if    satisfies Eq.  6.46 , then    will also satisfy it.    and eigenvalues    z    az  z   Therefore an eigenvector of a given transformation represents a direction,  such that any vector in that direction, when transformed, will continue to  point in the same direction, but will be scaled by the eigenvalue. As an ex- ample, consider again the rotation example used in the previous sections.  Is there any vector that, when rotated by 30 , continues to point in the  same direction? No; this is a case where there are no real eigenvalues.  If  we allow complex scalars, then two eigenvalues exist, as we will see later.   How can we compute the eigenvalues and eigenvectors? Suppose that a ba- sis has been chosen for the n-dimensional vector space  . Then the matrix  representation for Eq.  6.46  can be written  X  s2  A x   θ  x  s1  or  or  Az  z=  ,    A I–  z  0=  .    A I–    0=  .  This means that the columns of   determinant of this matrix must be zero:  A I–     are dependent, and therefore the   6  This determinant is an nth-order polynomial. Therefore Eq.  6.49  always  has   roots, some of which may be complex and some of which may be re- peated.  n  2 2+  As an example, let’s revisit the rotation example. If we use the standard ba- sis set, the matrix of the transformation is  We can then write Eq.  6.49  as  A  =  cos sin  sin– cos  .  cos  –  sin  sin– cos –  0=  ,  2  –  2  cos  +      cos  2  +  sin  2    =  2  –  2  cos  1+  =  0  .   6.52   6-11   6.47    6.48    6.49    6.50    6.51    6 Linear Transformations for Neural Networks  The roots of this equation are  1  =  cos  +  j  sin  2  =  cos  –  j  sin  .   6.53   Therefore, as we predicted, this transformation has no real eigenvalues  if  sin point in a new direction.   . This means that when any real vector is transformed, it will   0  2 2+  Consider another matrix:  To find the eigenvalues we must solve  A  =  1– 0  1 2–  .  – 1– 0  1 – 2–  0=  ,   6.54    6.55    6.56    6.57    6.58    6.59    6.60   or  or  2  +  3 2  +  =   1+    2+    =  0  ,  and the eigenvalues are  1  1–=  2  2–=  .  To find the eigenvectors we must solve Eq.  6.48 , which in this example be- comes  We will solve this equation twice, once using  ning with    we have  1  1   and once using   2  . Begin-  – 1– 0  1 – 2–  z  =  .  0 0  0 1 0 1–  z1  =  0 1 0 1–  z11 z21  =  0 0  z21  0=  , no constraint on   z11  .  Therefore the first eigenvector will be  6-12    6.61    6.62    6.63    6.64    6.65    6.66   Eigenvalues and Eigenvectors  or any scalar multiple. For the second eigenvector we use   2  :  1 1 0 0  z2  =  1 1 0 0  z12 z22  =  ,  0 0  or  Therefore the second eigenvector will be  z1  =  ,  1 0  z22  z12–=  .  z2  =  ,  1 1–  =  =  1– 0  2– 2  or any scalar multiple.  To verify our results we consider the following:  Az1  =  1– 0  1 2–  1 0  =  1–   1 0  =  1z1  ,  6  Az2  =  1– 0  1 2–  1 1–  =  2–   1 1–  =  2z2  .  To test your understanding of eigenvectors, use the Neural Network Design  Demonstration Eigenvector Game  nnd6eg .  Diagonalization Whenever we have  find  make up a basis set for the vector space of the transformation. Let’s find  the matrix of the previous transformation  Eq.  6.54   using the eigenvec- tors as the basis vectors. From Eq.  6.33  we have   independent eigenvectors [Brog91]. Therefore the eigenvectors    distinct eigenvalues we are guaranteed that we can   n  n  A'  =  B 1– AB      =  1 1 0 1–  1– 0  1 2–  1 1 0 1–  =  1– 0  0 2–  .   6.67   Note that this is a diagonal matrix, with the eigenvalues on the diagonal.  This is not a coincidence. Whenever we have distinct eigenvalues we can  diagonalize the matrix representation by using the eigenvectors as the ba-  6-13   6 Linear Transformations for Neural Networks  Diagonalization  sis vectors. This diagonalization process is summarized in the following.  Let   B  =  z1 z2  zn  ,   6.68   where   {  z1 z2        zn , }   are the eigenvectors of a matrix A. Then  B 1– AB      =  1 0  0 0 2  0    0 0  n  ,  where   {  1 2        n , }   are the eigenvalues of the matrix A.  This result will be very helpful as we analyze the performance of several  neural networks in later chapters.   6.69   6-14   Summary of Results  Summary of Results  Transformations  A transformation consists of three parts:  1. a set of elements   X  , called the domain,  2. a set of elements   Y  , called the range, and  3. a rule relating each    to an element   yi  Y  .  =  xi  yi  = xi X  Linear Transformations  A transformation    is linear if:  A X  1.  2.  for all   for all     x1 x2 x X  ,   A x1    x2+ A ax     =  =    ,  A x1 aA x    A x2 + .  ,   a  R  ,   Matrix Representations          v1 v2   vn  Let  a basis for vector space  X   and range   Y    :   be a basis for vector space    be   be a linear transformation with domain   u1 u2   um  , and let   X            . Let   Y  A  6  The coefficients of the matrix representation are obtained from  Change of Basis  A x   y=  .  A vj    =  aijui  .  m  i  1=  Bt  =  t1 t2  tn  Bw  =  w1 w2  wm  A'  =    1– ABt Bw    6-15   6 Linear Transformations for Neural Networks  Eigenvalues and Eigenvectors Az  z=  Diagonalization    A I–    0=  B  =  z1 z2  zn  ,  B 1– AB      =  1 0  0 0 2  0   0 0  n  where   {  z1 z2        zn , }   are the eigenvectors of a square matrix   A  .  6-16   Solved Problems  Solved Problems  P6.1 Consider the single-layer network shown in Figure P6.1, which has   a linear transfer function. Is the transformation from the input  vector to the output vector a linear transformation?  Inputs  Linear Layer  - Title -  S x R  p  cid:0  cid:0 W R x 1  cid:0  cid:0 b  cid:0  cid:0   S x 1  1  R  a S x 1   cid:0  cid:0  n  cid:0  cid:0  S x 1  cid:0  cid:0   S  a = purelin  Wp + b   Figure P6.1   Single-Neuron Perceptron  The network equation is  In order for this transformation to be linear it must satisfy  a  =  A p   Wp b+  =  .  6  1.  2.    A p1 p2+ A ap =    =   aA p  A p1   .   A p2 +    ,  Let’s test condition 1 first.  A p1 p2+     W p1 p2+  =     b+  =  Wp1 Wp2 b  +  +  .  Compare this with  A p1   A p2 +    =  Wp1 b+ Wp2 b+  +  =  Wp1 Wp2  +  +  2b  .  Clearly these two expressions will be equal only if  network performs a nonlinear transformation, even though it has a linear  transfer function. This particular type of nonlinearity is called an affine  transformation.  . Therefore this   0=  b  6-17   6 Linear Transformations for Neural Networks  P6.2 We discussed projections in Chapter 5. Is a projection a linear   transformation?  The projection of a vector    onto a vector    is computed as  v  x  where   x v  ,     is the inner product of   x   with   y  =  A x   =  ,  x v  ,   -----------v v v  ,   .  v  We need to check to see if this transformation satisfies the two conditions  for linearity. Let’s start with condition 1:  A x1    x2+    =  x1 v   ,   -------------------------v  x2+ v v  ,    =  +  x1 v x2 v ,       ,   ----------------------------------v v v  ,    =  x1 v   ,   --------------v v v  ,    +  x2 v   ,   --------------v v v  ,    =  A x1   A x2 +  .   Here we used linearity properties of inner products.  Checking condition 2:  A ax    =  ax v   ,   ---------------v v v  ,    =  a x v  ,   ---------------v v v  ,    =  aA x   .  Therefore projection is a linear operation.  P6.3 Consider the transformation    about the line   2 trix of this transformation relative to the standard basis in    created by reflecting a vector    in  , as shown in Figure P6.2. Find the ma-  x2+  0=  2  x1  A  x  .  s2  x  s1  A x   6-18  Figure P6.2  Reflection Transformation  The key to finding the matrix of a transformation is given in Eq.  6.6 :   Solved Problems  A vj    =  aijui  .  m  i  1=  We need to transform each basis vector of the domain and then expand the  result in terms of the basis vectors for the range. Each time we do the ex- pansion we get one column of the matrix representation. In this case the  basis set for both the domain and the range is  s1 first. If we reflect  0=  . So let’s transform   s1 s2 { , } , we find   about the line      x1  x2+  s1  A s1  = -s2  A s1    =  s2–  =  ai1si  =  a11s1  +  a21s2  =  0s1  +  1–  s2   as shown in the top left figure , which gives us the first column of the ma- trix. Next we transform   :  s2  A s2    =  s1–  =  ai2si  =  a12s1  +  a22s2  =  1–  s1  +  0s2  s1  s2  A s2  = -s1   as shown in the second figure on the left , which gives us the second col- umn of the matrix. The final result is  6  Let’s test our result by transforming the vector   x  =  T  :  1 1  0 1–  1– 0  .  Ax  =  0 1–  1– 0  1 1  =  .  1– 1–  This is indeed the reflection of  in Figure P6.3.  x   about the line   x1  x2+  0=  , as we can see   2    i  1=  2    i  1=  6-19   6 Linear Transformations for Neural Networks  s2  x  A x    s1  Figure P6.3  Test of Reflection Operation   Can you guess the eigenvalues and eigenvectors of this transformation?  Use the Neural Network Design Demonstration Linear Transformations   nnd6lt  to investigate this graphically. Compute the eigenvalues and  eigenvectors, using the MATLAB function eig, and check your guess.   P6.4 Consider the space of complex numbers. Let this be the vector   space  conjugation operator  i.e.,   , and let the basis for   X  X A x    be  { x=  ,  1 j+ 1 j– }  .  . Let   A:X  X   be the   i. Find the matrix of the transformation    relative to the basis   A  set given above.  ii. Find the eigenvalues and eigenvectors of the transforma-  tion.  iii. Find the matrix representation for    relative to the eigen-  A  vectors as the basis vectors.  i. To find the matrix of the transformation, transform each of the basis  vectors  by finding their conjugate :  A v1    =  A 1  j+    =  1  j–  =  v2  =  a11v1  +  a21v2  =  0v1  +  1v2  ,  A v2    =  A 1  j–    =  1  j+  =  v1  =  a12v1  +  a22v2  =  1v1  +  0v2  .  This gives us the matrix representation  A  =  0 1 1 0  .  ii. To find the eigenvalues, we need to use Eq.  6.49 :  6-20   Solved Problems    A I–    =  =  2  1–  =   1–    1+    =  0  .  – 1  1 –  So the eigenvalues are:   6.48 :  1  1=  ,   2  1–=  . To find the eigenvectors, use Eq.   For     =  1  1=   this gives us    A I–  z  =  – 1  1 –  z  =  .  0 0  1– 1  1 1–  z1  =  1– 1  1 1–  z11 z21  =  ,  0 0  Therefore the first eigenvector will be  z11  =  z21  .  z1  =  ,  1 1  or  or  or any scalar multiple. For the second eigenvector we use     =  2  1–=  :  1 1 1 1  z1  =  1 1 1 1  z12 z22  =  ,  0 0  z12 Therefore the second eigenvector is  =  z– 22  .  z2  =  ,  1 1–  or any scalar multiple.  Note that while these eigenvectors can be represented as columns of num- bers, in reality they are complex numbers. For example:  z1  =  1v1  +  1v2  =  j+ 1    +  j– 1    =  2  ,  6-21  6   6 Linear Transformations for Neural Networks  z2  =  1v1  +  1–  v2  =  j+ 1    –  j– 1    =  2j  .  Checking that these are indeed eigenvectors:  iii. To perform a change of basis we need to use Eq.  6.33 :  where  A z1    =  2   =  2  =  1z1  ,  A z2    =  2j    =  2j–  =  2z2  .  A'  =    1– ABt Bw    =  B 1– AB      ,  B  =  z1 z2  =  1 1 1 1–  .   We are using the same basis set for the range and the domain.  Therefore  we have  A'  =  0.5 0.5 0.5 0.5–  0 1 1 0  1 1 1 1–  =  1 0 0 1–  =  1 0 0 2  .  As expected from Eq.  6.69 , we have diagonalized the matrix representa- tion.  P6.5 Diagonalize the following matrix:  A  =  2 1–  2– 3  .  The first step is to find the eigenvalues:    A I–    =  =  2 5–  4+  =   1–    4–    =  0  ,  2 – 1–  2– 3 –  so the eigenvalues are   1  1=  ,   2  4=  . To find the eigenvectors,    A I–  z  =  2 – 1–  2– 3 –  z  =  .  0 0  For     =  1  =  1  6-22   Solved Problems  1 1–  2– 2  z1  =  1 1–  2– 2  z11 z21  =  ,  0 0  or  or  Therefore the first eigenvector will be  or any scalar multiple.  For     =  2  =  4  2– 1–  2– 1–  z1  =  2– 1–  2– 1–  z12 z22  =  ,  0 0  z11  =  2z21  .  z1  =  ,  2 1  z12  =  z– 22  .  z2  =  ,  1 1–  Therefore the second eigenvector will be  or any scalar multiple.  To diagonalize the matrix we use Eq.  6.69 :  A'  =  B 1– AB      ,  B  =  z1 z2  =  2 1 1 1–  .  where  Therefore we have  6-23  6   6 Linear Transformations for Neural Networks  A'  =  1 --- 3 1 --- 3  1 --- 3 2 ---– 3  2 1–  2– 3  2 1 1 1–  =  1 0 0 4  =  1 0 0 2  .  P6.6 Consider a transformation   R2 relative to the standard basis sets is   A:R3   whose matrix representation   A  =  3 1– 0 0 0 1  .  Find the matrix for this transformation relative to the basis sets:  T  =          2 0 1    0 1– 0  0 2– 3        W  =        1 0  0 2–  .      The first step is to form the matrices  Now we use Eq.  6.33  to form the new matrix representation:  Bt  =  2 0 0 0 1– 2– 1 0 3  Bw  =  1 0 0 2–  .  A'  =    1– ABt Bw    ,  A'  =  1 0 1 ---– 2  0  3 1– 0 0 0 1  2 0 0 0 1– 2– 1 0 3  =  6 1 2 3 1 ---– 2 2  ---– 0  .  Therefore this is the matrix of the transformation with respect to the basis  sets    and   W  T  .  P6.7 Consider a transformation   A:2 2  . One basis set for   2   is given   as   V  =    v1 v2      .  i. Find the matrix of the transformation    relative to the basis   A  set   V   if it is given that  6-24   Solved Problems  A v1 A v2      =  =  v1 v1  +  +  2v2, v2.  w1  w2  =  =  v1  +  v2,  v1 v2.  –  A  =  1 1 2 1  .  ii. Consider a new basis set   W  the transformation  that  A  =    w1 w2      . Find the matrix of    relative to the basis set   W   if it is given   i. Each of the two equations gives us one column of the matrix, as defined  in Eq.  6.6 . Therefore the matrix is  ii. We can represent the  of the    basis vectors:  V  W   basis vectors as columns of numbers in terms   w1  =  1 1  w2  =  .  1 1–  6  We can now form the basis matrix that we need to perform the similarity  transform:  The new matrix representation can then be obtained from Eq.  6.33 :  Bw  =  1 1 1 1–  .  A'  =    1– ABw Bw    ,  A'  =  1 1 2 1  1 1 1 1–  =  1 --- 2 1 --- 2  1 --- 2 1 ---– 2  5 --- 2 1 ---– 2  1 --- 2 1 ---– 2  .  6-25   6 Linear Transformations for Neural Networks  P6.8 Consider the vector space   P2  or equal to 2. One basis for this vector space is  er the differentiation transformation   .   D   of all polynomials of degree less than  . Consid-  1 t t2   V  =        i. Find the matrix of this transformation relative to the basis   set   V  .  tion.  ii. Find the eigenvalues and eigenvectors of the transforma-  i. The first step is to transform each of the basis vectors:  D 1   =  0  =  0 1  +  0 t  +  0 t2  ,  D t   =  1  =  1 1  +  0 t  +  0 t2  ,  D t2    =  2t  =  0 1  +  2 t  +  0 t2  .  The matrix of the transformation is then given by  D  =  0 1 0 0 0 2 0 0 0  .  ii. To find the eigenvalues we must solve    D I–    =  =  3–  =  0  .  – 1 0 – 2 0 – 0 0  Therefore all three eigenvalues are zero. To find the eigenvectors we need  to solve    D I–  z  =  z  =  .  – 1 0 – 2 0 – 0 0  0 0 0  For     0=   we have  0 1 0 0 0 2 0 0 0  z1 z2 z3  =  .  0 0 0  6-26   Solved Problems  This means that  Therefore we have a single eigenvector:     z2  =  z3  =  0  .  z  =  .  1 0 0  Therefore the only polynomial whose derivative is a scaled version of itself  is a constant  a zeroth-order polynomial .  P6.9 Consider a transformation   A:R2  R2  . Two examples of trans-  formed vectors are given in Figure P6.4. Find the matrix represen- tation of this transformation relative to the standard basis set.  x 1  x 2  A x 1   s2  A x 2   s1  6  Figure P6.4  Transformation for Problem P6.9  For this problem we do not know how the basis vectors are transformed, so  we cannot use Eq.  6.6  to find the matrix representation. However, we do  know how two vectors are transformed, and we do know how those vectors  can be represented in terms of the standard basis set. From Figure P6.4 we  can write the following equations:  A 2 2  =  1– 0  ,   A 1– 1  =  .  2– 1–  We then put these two equations together to form  A 2 1– 2 1  =  1– 0  2– 1–  .  6-27   6 Linear Transformations for Neural Networks  So that  A  =  1– 0  2– 1–  2 1– 2 1  1–  =  1– 0  2– 1–  1 --- 4 1 ---– 2  1 --- 4 1 --- 2  =  3 --- 4 1 --- 2  5 ---– 4 1 ---– 2  .  This is the matrix representation of the transformation with respect to the  standard basis set.   This procedure is used in the Neural Network Design Demonstration Linear  Transformations  nnd6lt .  6-28   Epilogue  Epilogue  In this chapter we have reviewed those properties of linear transforma- tions and matrices that are most important to our study of neural net- works. The concepts of eigenvalues, eigenvectors, change of basis   similarity transformation  and diagonalization will be used again and  again throughout the remainder of this text. Without this linear algebra  background our study of neural networks could only be superficial.  In the next chapter we will use linear algebra to analyze the operation of  one of the first neural network training algorithms — the Hebb rule.  6  6-29   6 Linear Transformations for Neural Networks  Further Reading  [Brog91]   W. L. Brogan, Modern Control Theory, 3rd Ed., Englewood  Cliffs, NJ: Prentice-Hall, 1991.  This is a well-written book on the subject of linear systems.  The first half of the book is devoted to linear algebra. It also  has good sections on the solution of linear differential equa- tions and the stability of linear and nonlinear systems. It  has many worked problems.  [Stra76]   G. Strang, Linear Algebra and Its Applications, New York:  Academic Press, 1980.  Strang has written a good basic text on linear algebra.  Many applications of linear algebra are integrated into the  text.  6-30   Exercises  Exercises  E6.1 Is the operation of transposing a matrix a linear transformation?  E6.2 Consider again the neural network shown in Figure P6.1. Show that if the   is equal to zero then the network performs a linear operation.  bias vector   b  E6.3 Consider the linear transformation illustrated in Figure E6.1.  i. Find the matrix representation of this transformation relative to   the standard basis set.  ii. Find the matrix of this transformation relative to the basis set     v1 v2      .  v2  A v1   s2  v1  s1  A v2   6  Figure E6.1  Example Transformation for Exercise E6.3  E6.4 Consider the space of complex numbers. Let this be the vector space   X  , and    be  let the basis for  X plication by    i.e.,  j+  1  {  ,  1 j+ 1 j– } =  A x   . Let  A:X  . j+ x 1  X   be the operation of multi-  i. Find the matrix of the transformation    relative to the basis set   A  given above.  ii. Find the eigenvalues and eigenvectors of the transformation.  iii. Find the matrix representation for    relative to the eigenvectors as   A  the basis vectors.  iv. Check your answers to parts  ii  and  iii  using MATLAB.  » 2 + 2 ans =       4  6-31   6 Linear Transformations for Neural Networks  E6.5 Consider a transformation   , from the space of second-order poly- nomials to the space of third-order polynomials, which is defined by the fol- lowing:  A:P2  P3  x  =  a0  +  a1t  +  a2t2  ,  A x   =  a0 t  1+    +  a1 t  1+  2  +  a2 t  1+  3  .  Find the matrix representation of this transformation relative to the basis  sets   1 t t2 t3   1 t t2   V3  V2  =  =          .          E6.6 Consider the vector space of polynomials of degree two or less. These poly- . Now consider the transforma- 1+ t   6  nomials have the form  f t  tion in which the variable  t t2 1+ 2 t  a2t2 =  is replaced by   + +  .  for example,   a1t t2  1+ t    a0  2  4t  2t  +  +  +  =  +  +  +  3  3  i. Find the matrix of this transformation with respect to the basis set      1 t  1–    t2    .  ii. Find the eigenvalues and eigenvectors of the transformation. Show   the eigenvectors as columns of numbers and as functions of time   polynomials .  E6.7 Consider the space of functions of the form     sin  + t    . One basis set for   V  =    sin   t  cos  t    . Consider the differentiation transformation   this space is  D  .  i. Find the matrix of the transformation    relative to the basis set   V  .  D  ii. Find the eigenvalues and eigenvectors of the transformation. Show   the eigenvectors as columns of numbers and as functions of   t  .  iii. Find the matrix of the transformation relative to the eigenvectors as   basis vectors.  E6.8 Consider the vector space of functions of the form   V  =    1  +  e2t    1  –  e2t     e2t  . One basis set  . Consider the differentiation   +  for this vector space is  transformation   .  D  i. Find the matrix of the transformation    relative to the basis set   V  ,   D  using Eq.  6.6 .  ii. Verify the operation of the matrix on the function   2e2t  .  iii. Find the eigenvalues and eigenvectors of the transformation. Show  the eigenvectors as columns of numbers  with respect to the basis  set     and as functions of   V  .  t  6-32   Exercises  iv. Find the matrix of the transformation relative to the eigenvectors as   basis vectors.  E6.9 Consider the set of all 2x2 matrices. This set is a vector space, which we will  call X  yes, matrices can be vectors . If M is an element of this vector space,  define the transformation  . Consider  the following basis set for the vector space X.   M MT  , such that   A M  A:X  X  +  =  v1  =  ,   v2  =  ,   v3  =  ,   v4  =  0 0 1 0  0 0 0 1  1 0 0 0  0 1 0 0  i. Find the matrix representation of the transformation    relative to    for both domain and range   using Eq.   A            v1 v2 v3 v4  the basis set   6.6  .  ii. Verify the operation of the matrix representation from part i. on the  element of X given below.  Verify that the matrix multiplication pro- duces the same result as the transformation.   1 2 0 1  iii. Find the eigenvalues and eigenvectors of the transformation. You do  not need to use the matrix representation that you found in part i.  You can find the eigenvalues and eigenvectors directly from the def- inition of the transformation. Your eigenvectors should be 2x2 ma- trices  elements of the vector space X . This does not require much  computation. Use the definition of eigenvector in Eq.  6.46 .  6  E6.10 Consider a transformation   , from the space of first degree poly- nomials into the space of second degree polynomials. The transformation is  defined as follows  A:P1  P2  A a  bt+    =  at  +  b ---t2 2 P1   e.g.,  P2  is   6t+  A 2 = 1 t t2  = V      2t .   3t2  +   . One basis set for    is   U  =  1 t    . One basis for   i. Find the matrix representation of the transformation   A   relative to   the basis sets   U   and   V  , using Eq.  6.6 .  ii. Verify the operation of the matrix on the polynomial   6  8t+  .  Verify   that the matrix multiplication produces the same result as the  transformation.   6-33   6 Linear Transformations for Neural Networks  iii. Using a similarity transform, find the matrix of the transformation   with respect to the basis sets   S  =    1  t+ 1    t–     and   V  .  E6.11 Let D be the differentiation operator    D f   =  df dt     , and use the basis set    u1 u2      =    e5t te5t      for both the domain and the range of the transformation D.  i. Show that the transformation D is linear., ii. Find the matrix of this transformation relative to the basis shown   above.  iii. Find the eigenvalues and eigenvectors of the transformation D.   E6.12 A certain linear transformation has the following eigenvalues and eigen-  vectors  represented in terms of the standard basis set :      1 2  z1  =    1  1=  ,   z2  =    2  2=          1– 2      i. Find the matrix representation of the transformation, relative to   the standard basis set.  ii. Find the matrix representation of the transformation relative to the   eigenvectors as the basis vectors.  E6.13 Consider a transformation  v1 v2  of basis vectors   V  =      A:2 2  and the transformed basis vectors.   . In the figure below, we show a set   A v  1   v1  v2  Figure E6.2  Definition of Transformation for Exercise E6.13  A v  2   6-34   Exercises  i. Find the matrix representation of this transformation with respect   to the basis vectors   V  =    v1 v2      .  ii. Find the matrix representation of this transformation with respect   to the standard basis vectors.  iii. Find the eigenvalues and eigenvectors of this transformation.   Sketch the eigenvectors and their transformations.  iv. Find the matrix representation of this transformation with respect   to the eigenvectors as the basis vectors.  E6.14 Consider the vector spaces    of second-order and third-order poly- nomials. Find the matrix representation of the integration transformation  I:P2  , relative to the basis sets   1 t t2 t3   1 t t2   P3   and   V3  V2  .   =  =                  P2  P3  E6.15 A certain linear transformation   relative to the standard basis set of  A:2 2   has a matrix representation   Find the matrix representation of this transformation relative to the new  basis set:  6  A  =  1 2 3 4  .  V  =        1 3  2 5  .      E6.16 We know that a certain linear transformation   A:R2  R2   has eigenvalues   and eigenvectors given by  1  1=  z1  =  2  2=  z2  =  1 1  .  1 2   The eigenvectors are represented relative to the standard basis  set.   i. Find the matrix representation of the transformation   A   relative to   the standard basis set.  ii. Find the matrix representation relative to the new basis  6-35   6 Linear Transformations for Neural Networks  V  =        1 1  1– 1  .      E6.17 Consider the transformation A created by projecting a vector x onto the line  shown in Figure E6.3. An example of the transformation is shown in the  figure.  i. Using Eq.  6.6 , find the matrix representation of this transforma-  tion relative to the standard basis set   s1 s2    .  ii. Using your answer to part i, find the matrix representation of this   transformation relative to the basis set  E6.3.   v1 v2     shown in Figure   iii. What are the eigenvalues and eigenvectors of this transformation?   Sketch the eigenvectors and their transformations.  v  1  x  s  2  y =A x    s  1  Figure E6.3  Definition of Transformation for Exercise E6.17  E6.18 Consider the following basis set for   2  :  v  2  6-36   Exercises  V  =    v1 v2      =        1 1–  1 2–  .       The basis vectors are represented relative to the standard basis set.   i. Find the reciprocal basis vectors for this basis set.  ii. Consider a transformation   A:2 2  relative to the standard basis in   for   . The matrix representation  2   is  A  A  =  0 1 3– 2–  .  Find the expansion of  ciprocal basis vectors.   Av1   in terms of the basis set   V  .  Use the re-  iii. Find the expansion of    in terms of the basis set   V  .  Av2  iv. Find the matrix representation for    relative to the basis   V  .  This   A  step should require no further computation.   6  6-37   Objectives  7 Supervised Hebbian Learning  Objectives Theory and Examples Linear Associator The Hebb Rule  Performance Analysis  Pseudoinverse Rule Application Variations of Hebbian Learning  Summary of Results Solved Problems Epilogue Further Reading Exercises  7-1 7-2 7-3 7-4 7-5 7-7 7-10 7-12 7-14 7-16 7-29 7-30 7-31  Objectives  7  The Hebb rule was one of the first neural network learning laws. It was  proposed by Donald Hebb in 1949 as a possible mechanism for synaptic  modification in the brain and since then has been used to train artificial  neural networks.  In this chapter we will use the linear algebra concepts of the previous two  chapters to explain why Hebbian learning works. We will also show how  the Hebb rule can be used to train neural networks for pattern recognition.  7-1   7 Supervised Hebbian Learning  Theory and Examples  Donald O. Hebb was born in Chester, Nova Scotia, just after the turn of the  century. He originally planned to become a novelist, and obtained a degree  in English from Dalhousie University in Halifax in 1925. Since every first- rate novelist needs to have a good understanding of human nature, he be- gan to study Freud after graduation and became interested in psychology.  He then pursued a master’s degree in psychology at McGill University,  where he wrote a thesis on Pavlovian conditioning. He received his Ph.D.  from Harvard in 1936, where his dissertation investigated the effects of  early experience on the vision of rats. Later he joined the Montreal Neuro- logical Institute, where he studied the extent of intellectual changes in  brain surgery patients. In 1942 he moved to the Yerkes Laboratories of Pri- mate Biology in Florida, where he studied chimpanzee behavior.  In 1949 Hebb summarized his two decades of research in The Organization  of Behavior [Hebb49]. The main premise of this book was that behavior  could be explained by the action of neurons. This was in marked contrast  to the behaviorist school of psychology  with proponents such as B. F. Skin- ner , which emphasized the correlation between stimulus and response and  discouraged the use of any physiological hypotheses. It was a confrontation  between a top-down philosophy and a bottom-up philosophy. Hebb stated  his approach: “The method then calls for learning as much as one can about  what the parts of the brain do  primarily the physiologist’s field , and relat- ing the behavior as far as possible to this knowledge  primarily for the psy- chologist ; then seeing what further information is to be had about how the  total brain works, from the discrepancy between  1  actual behavior and  2   the behavior that would be predicted from adding up what is known about  the action of the various parts.”  The most famous idea contained in The Organization of Behavior was the  postulate that came to be known as Hebbian learning:  “When an axon of cell A is near enough to excite a cell B and repeatedly or  persistently takes part in firing it, some growth process or metabolic change  takes place in one or both cells such that A’s efficiency, as one of the cells fir- ing B, is increased.”  This postulate suggested a physical mechanism for learning at the cellular  level. Although Hebb never claimed to have firm physiological evidence for  his theory, subsequent research has shown that some cells do exhibit Heb- bian learning. Hebb’s theories continue to influence current research in  neuroscience.  As with most historic ideas, Hebb’s postulate was not completely new, as  he himself emphasized. It had been foreshadowed by several others, includ- ing Freud. Consider, for example, the following principle of association  stated by psychologist and philosopher William James in 1890: “When two   7-2  Hebb’s Postulate   Linear Associator  brain processes are active together or in immediate succession, one of  them, on reoccurring tends to propagate its excitement into the other.”  Linear Associator  Linear Associator  Hebb’s learning law can be used in combination with a variety of neural  network architectures. We will use a very simple architecture for our initial  presentation of Hebbian learning. In this way we can concentrate on the  learning law rather than the architecture. The network we will use is the  linear associator, which is shown in Figure 7.1.  This network was intro- duced independently by James Anderson [Ande72] and Teuvo Kohonen  [Koho72].   Inputs  p R x 1  R  n cid:0 W  Linear Layer  cid:0  cid:0   cid:0  cid:0   cid:0  cid:0  a = purelin  Wp   S x R  S x 1  S  a S x 1  Figure 7.1  Linear Associator  a Wp  =  ,  ai  =  wijpj  .  R  j  1=  The output vector   a   is determined from the input vector   p   according to:  or  7   7.1    7.2   Associative Memory  The linear associator is an example of a type of neural network called an  associative memory. The task of an associative memory is to learn   pairs  of prototype input output vectors:  Q  p1 t1 { , }    {  p2 t2  , }   pQ tQ , }  {      .   7.3   In other words, if the network receives an input  , for  duce an output  tq= changed slightly  i.e.,  p =  . slightly  i.e.,  +  = +  q pq  =  a  a    tq  p  pq=   then it should pro-    1 2   Q    then the output should only be changed   . In addition, if the input is   7-3   The Hebb Rule  new  wij  =  wij  old  fi aiq  +    gj pjq      ,   7.4   7 Supervised Hebbian Learning  The Hebb Rule  How can we interpret Hebb’s postulate mathematically, so that we can use  it to train the weight matrix of the linear associator? First, let’s rephrase  the postulate: If two neurons on either side of a synapse are activated si- multaneously, the strength of the synapse will increase. Notice from Eq.   7.2  that the connection  synapse  between input   is the  weight  produces a positive  mathematical interpretation of the postulate could be  pj  should increase. This suggests that one   . Therefore Hebb’s postulate would imply that if a positive    and output    then   wij  wij  ai  ai  pj     jth  pjq   is the    element of the    ele- where  qth ment of the network output when the   input vector is presented to the   is a positive constant, called the learning rate. This equa- network; and  tion says that the change in the weight   is proportional to a product of  functions of the activities on either side of the synapse. For this chapter we  will simplify Eq.  7.4  to the following form   input vector  qth  pq aiq   is the   wij  ith    ;   new  wij  =  wij  old aiqpjq  +  .   7.5   Note that this expression actually extends Hebb’s postulate beyond its  strict interpretation. The change in the weight is proportional to a product  of the activity on either side of the synapse. Therefore, not only do we in- crease the weight when both   are positive, but we also increase  the weight when they are both negative. In addition, this implementation  of the Hebb rule will decrease the weight whenever   have opposite  sign.   and    and   ai  ai  pj  pj  The Hebb rule defined in Eq.  7.5  is an unsupervised learning rule. It does  not require any information concerning the target output. In this chapter  we are interested in using the Hebb rule for supervised learning, in which  the target output is known for each input vector.  We will revisit the unsu- pervised Hebb rule in Chapter 13.  For the supervised Hebb rule we substi- tute the target output for the actual output. In this way, we are telling the  algorithm what the network should do, rather than what it is currently do- ing. The resulting equation is  new  wij  =  old wij  +  tiqpjq  ,  where  learning rate    is the    tiq   element of the   ith qth  to one, for simplicity.     target vector   .  We have set the   tq  Notice that Eq.  7.6  can be written in vector notation:   7.6    7.7   Wnew Wold  =  +  T tqpq  .  7-4   The Hebb Rule  If we assume that the weight matrix is initialized to zero and then each of  the    input output pairs are applied once to Eq.  7.7 , we can write  Q  T W t1p1  =  +  t2p2  T  tQpQ +  +  T  =  T tqpq  .   7.8   Q    q  1=  This can be represented in matrix form:  W  =  t1 t2  tQ  =  TPT  ,   7.9   T p1 T p2  T pQ  where  T  =  t1 t2  tQ        P  =  p1 p2  pQ  .   7.10   Performance Analysis Let’s analyze the performance of Hebbian learning for the linear associa- tor. First consider the case where the   vectors are orthonormal  orthog- onal and unit length . If  output can be computed   is input to the network, then the network   pq  pk  a Wpk  =  =  Q      q  1=   T tqpq    pk  =   pq  tq    Tpk    .  Q  q  1=  Since the   pq   are orthonormal,  Therefore Eq.  7.11  can be rewritten    Tpk pq    =  1  =  0  q  k=  q  k.  a Wpk  =  =  tk  .  7   7.11    7.12    7.13   The output of the network is equal to the target output. This shows that, if  the input prototype vectors are orthonormal, the Hebb rule will produce the  correct output for each input.  7-5   7 Supervised Hebbian Learning  But what about non-orthogonal prototype vectors? Let’s assume that each   vector is unit length, but that they are not orthogonal. Then Eq.  7.11   pq becomes  Error  a Wpk  =  =  tk  +  q  k  tq pq    Tpk    .   7.14   Because the vectors are not orthogonal, the network will not produce the  correct output. The magnitude of the error will depend on the amount of  correlation between the prototype input patterns.  2 2+  As an example, suppose that the prototype input output vectors are          0.5 0.5– 0.5 0.5–  1 1–                  0.5 0.5 0.5– 0.5–  1 1  .          p1  =  t1  =  p2  =  t2  =   7.15    Check that the two input vectors are orthonormal.   The weight matrix would be  W TPT  =  =  1 1 1– 1  0.5 0.5– 0.5 0.5  0.5 0.5–  0.5– 0.5–  =  1 0 0 0 1 1–  1– 0  .   7.16   If we test this weight matrix on the two prototype inputs we find  and   7.17    7.18   Wp1  =  1 0 0 0 1 1–  1– 0  =  ,  1 1–  Wp2  =  1 0 0 0 1 1–  1– 0  =  .  1 1  0.5 0.5– 0.5 0.5–  0.5 0.5 0.5– 0.5–  7-6  Success!! The outputs of the network are equal to the targets.   Pseudoinverse Rule  2 2+  Now let’s revisit the apple and orange recognition problem described in  Chapter 3. Recall that the prototype inputs were  p1  =    orange    p2  =    apple    .   7.19   1 1– 1–  1 1 1–   Note that they are not orthogonal.  If we normalize these inputs and  choose as desired outputs -1 and 1, we obtain        p1  =  0.5774 – 0.5774 0.5774 –  t1  =  1–  p2  =  t2  =  1   7.20               0.5774 0.5774 0.5774 –  .        Our weight matrix becomes  W TPT  =  =  1– 1  –  0.5774 0.5774 0.5774 0.5774  – –  0.5774 0.5774  =  0 1.1548 0  .   7.21   So, if we use our two prototype patterns,  Wp1  =  0 1.1548 0  =  –  0.6668  ,   7.22   0.5774 – 0.5774 0.5774 –  0.5774 0.5774 0.5774 –  7  Wp2  =  0 1.1548 0  =  0.6668  .   7.23   The outputs are close, but do not quite match the target outputs.  Pseudoinverse Rule  When the prototype input patterns are not orthogonal, the Hebb rule pro- duces some errors. There are several procedures that can be used to reduce  these errors. In this section we will discuss one of those procedures, the  pseudoinverse rule.   Recall that the task of the linear associator was to produce an output of  for an input of   . In other words,  tq     pq  Wpq  tq=  q  =  1 2   Q       .   7.24   7-7   7 Supervised Hebbian Learning  If it is not possible to choose a weight matrix so that these equations are  exactly satisfied, then we want them to be approximately satisfied. One ap- proach would be to choose the weight matrix to minimize the following per- formance index:  F W    tq Wpq  –  2  .  Q  =  q  1=  pq  If the prototype input vectors   are orthonormal and we use the Hebb rule  to find W, then F W  will be zero. When the input vectors are not orthogo- nal and we use the Hebb rule, then F W  will be not be zero, and it is not  clear that F W  will be minimized. It turns out that the weight matrix that  will minimize F W  is obtained by using the pseudoinverse matrix, which  we will define next.  First, let’s rewrite Eq.  7.24  in matrix form:  WP  T=  ,  T  =  t1 t2  tQ  P  =  p1 p2  pQ  .  Then Eq.  7.25  can be written  F W    =  T WP  –  2  =  E 2  ,  where  where  and  Note that F W  can be made zero if we can solve Eq.  7.26 . If the P matrix  has an inverse, the solution is  However, this is rarely possible. Normally the  P  will be independent, but R  the dimension of   the number of  no exact inverse will exist.    vectors  the columns of    will be larger than Q   vectors . Therefore, P will not be a square matrix, and   pq pq  pq   7.25    7.26    7.27    7.28    7.29    7.30    7.31   E  =  T WP  –  ,  E 2  =    2 eij  .  i  j  W TP 1–  =  .  7-8   Pseudoinverse Rule  Pseudoinverse Rule  It has been shown [Albe72] that the weight matrix that minimizes Eq.   7.25  is given by the pseudoinverse rule:  W TP+  =  ,   7.32   where  matrix   P+ P   is the Moore-Penrose pseudoinverse. The pseudoinverse of a real   is the unique matrix that satisfies  PP+P P+PP+  =  =  P, P+,  P+P  PP+  =  =  P+P  T  ,  PP+    T  .   7.33   When the number, R, of rows of  Q, of  be computed by  , and the columns of   P  P  P   is greater than the number of columns,   are independent, then the pseudoinverse can   P+  =  PTP   1– PT  .   7.34   2 2+  To test the pseudoinverse rule  Eq.  7.32  , consider again the apple and or- ange recognition problem. Recall that the input output prototype vectors  are        1 1– 1–              1 1 1–  .        p1  =  t1  =  1–  p2  =  t2  =  1   7.35   7   Note that we do not need to normalize the input vectors when using the  pseudoinverse rule.   The weight matrix is calculated from Eq.  7.32 :  W TP+  =  =  1– 1   7.36         1 1 1– 1 1– 1–   +      ,  where the pseudoinverse is computed from Eq.  7.34 :  P+  =  PTP   1– PT  =  1–  3 1 1 3  1 1– 1 1  1– 1–  =  0.25 0.5– 0.25 0.5  – –  0.25 0.25  .   7.37   7-9   7 Supervised Hebbian Learning  This produces the following weight matrix:  W TP+  =  =  1– 1  0.25 0.5– 0.25 0.5  – –  0.25 0.25  =  0 1 0  .   7.38   Let’s try this matrix on our two prototype patterns.  Wp1  =  0 1 0  =  1–  Wp2  =  0 1 0  =  1  1 1– 1–  1 1 1–   7.39    7.40   The network outputs exactly match the desired outputs. Compare this re- sult with the performance of the Hebb rule. As you can see from Eq.  7.22   and Eq.  7.23 , the Hebbian outputs are only close, while the pseudoinverse  rule produces exact results.  Now let’s see how we might use the Hebb rule on a practical, although  greatly oversimplified, pattern recognition problem. For this problem we  will use a special type of associative memory — the autoassociative memo- ry. In an autoassociative memory the desired output vector is equal to the  input vector  i.e.,   . We will use an autoassociative memory to store  a set of patterns and then to recall these patterns, even when corrupted  patterns are provided as input.  pq=  tq  The patterns we want to store are shown to the left.  Since we are designing  an autoassociative memory, these patterns represent the input vectors and  the targets.  They represent the digits {0, 1, 2} displayed in a 6X5 grid. We  need to convert these digits to vectors, which will become the prototype pat- terns for our network. Each white square will be represented by a “-1”, and  each dark square will be represented by a “1”. Then, to create the input vec- tors, we will scan each 6X5 grid one column at a time. For example, the first  prototype pattern will be  p1  =  1– 1 1 1 1 1– 1 1–  1–  1–  1– 1 1 1–  1 1–  T  .   7.41   The vector  digit “2”. Using the Hebb rule, the weight matrix is computed   corresponds to the digit “0”,    to the digit “1”, and   p2  p1  p3   to the   7-10  Application  Autoassociative Memory  p1,t1 p2,t2 p3,t3   Application  W p1p1  =  T p2p2 +  T p3p3 +  T  .   7.42    Note that   pq   replaces   tq   in Eq.  7.8 , since this is autoassociative memory.   Because there are only two allowable values for the elements of the proto- type vectors, we will modify the linear associator so that its output ele- ments can only take on values of “-1” or “1”. We can do this by replacing  the linear transfer function with a symmetrical hard limit transfer func- tion. The resulting network is displayed in Figure 7.2.  Inputs  p 30x1  Sym. Hard Limit Layer  cid:0  cid:0   cid:0  cid:0  a  cid:0  cid:0  30x1  cid:0  cid:0   30 a = hardlims  Wp    cid:0 W  cid:0   n 30x1  30x30  30  Figure 7.2  Autoassociative Network for Digit Recognition  Now let’s investigate the operation of this network. We will provide the net- work with corrupted versions of the prototype patterns and then check the  network output. In the first test, which is shown in Figure 7.3, the network  is presented with a prototype pattern in which the lower half of the pattern  is occluded. In each case the correct pattern is produced by the network.  7  Figure 7.3  Recovery of 50% Occluded Patterns  In the next test we remove even more of the prototype patterns. Figure 7.4  illustrates the result of removing the lower two-thirds of each pattern. In  this case only the digit “1” is recovered correctly. The other two patterns  produce results that do not correspond to any of the prototype patterns.  This is a common problem in associative memories. We would like to design  networks so that the number of such spurious patterns would be mini- mized. We will come back to this topic again in Chapter 18, when we dis- cuss recurrent associative memories.  7-11   7 Supervised Hebbian Learning  Figure 7.4  Recovery of 67% Occluded Patterns  In our final test we will present the autoassociative network with noisy ver- sions of the prototype pattern. To create the noisy patterns we will random- ly change seven elements of each pattern. The results are shown in Figure  7.5. For these examples all of the patterns were correctly recovered.  Figure 7.5  Recovery of Noisy Patterns  To experiment with this type of pattern recognition problem, use the Neural  Network Design Demonstration Supervised Hebb  nnd7sh .  Variations of Hebbian Learning  There have been a number of variations on the basic Hebb rule. In fact,  many of the learning laws that will be discussed in the remainder of this  text have some relationship to the Hebb rule.  One of the problems of the Hebb rule is that it can lead to weight matrices  having very large elements if there are many prototype patterns in the  training set. Consider again the basic rule:  A positive parameter  , called the learning rate, can be used to limit the  amount of increase in the weight matrix elements, if the learning rate is  less than one, as in:    Wnew Wold  =  +  T tqpq  .  Wnew Wold tqpq  +  =  T  .   7.43    7.44   We can also add a decay term, so that the learning rule behaves like a  smoothing filter, remembering the most recent inputs more clearly:  Wnew Wold tqpq  +  =  T  Wold  –  =  – 1  Wold tqpq  +  T  ,   7.45     where  learning law becomes the standard rule. As    is a positive constant less than one. As        approaches zero, the    approaches one, the learning   7-12   Variations of Hebbian Learning  law quickly forgets old inputs and remembers only the most recent pat- terns. This keeps the weight matrix from growing without bound.  The idea of filtering the weight changes and of having an adjustable learn- ing rate are important ones, and we will discuss them again in Chapters  10, 12, 15, 16, 18 and 19.  If we modify Eq.  7.44  by replacing the desired output with the difference  between the desired output and the actual output, we get another impor- tant learning rule:  Wnew Wold  tq  +  =    aq–  T pq  .   7.46   This is sometimes known as the delta rule, since it uses the difference be- tween desired and actual output. It is also known as the Widrow-Hoff algo- rithm, after the researchers who introduced it. The delta rule adjusts the  weights so as to minimize the mean square error  see Chapter 10 . For this  reason it will produce the same results as the pseudoinverse rule, which  minimizes the sum of squares of errors  see Eq.  7.25  . The advantage of  the delta rule is that it can update the weights after each new input pattern  is presented, whereas the pseudoinverse rule computes the weights in one  step, after all of the input target pairs are known. This sequential updating  allows the delta rule to adapt to a changing environment. The delta rule  will be discussed in detail in Chapter 10.   The basic Hebb rule will be discussed again, in a different context, in Chap- ter 13. In the present chapter we have used a supervised form of the Hebb  rule. We have assumed that the desired outputs of the network,  , are  known, and can be used in the learning rule. In the unsupervised Hebb  rule, which is discussed in Chapter 13, the actual network output is used  instead of the desired network output, as in:  tq  Wnew Wold aqpq  =  +  T  ,   7.47   aq   is the output of the network when    is given as the input  see  where  also Eq.  7.5  . This unsupervised form of the Hebb rule, which does not re- quire knowledge of the desired output, is actually a more direct interpreta- tion of Hebb’s postulate than is the supervised form discussed in this  chapter.  pq  7  7-13   7 Supervised Hebbian Learning  Summary of Results  Hebb’s Postulate  “When an axon of cell A is near enough to excite a cell B and repeatedly or  persistently takes part in firing it, some growth process or metabolic change  takes place in one or both cells such that A’s efficiency, as one of the cells fir- ing B, is increased.”  Linear Associator  Inputs  p R x 1  R  n cid:0 W  Linear Layer  cid:0  cid:0   cid:0  cid:0   cid:0  cid:0  a = purelin  Wp   S x R  S x 1  S  a S x 1  new  wij  =  old wij  +  tqipqj  T W t1p1  =  +  t2p2  T  tQpQ +  +  T  W  =  t1 t2  tQ  =  TPT  T p1 T p2  T pQ  W TP+  =  7-14  The Hebb Rule  Pseudoinverse Rule   Summary of Results  When the number,  Q be computed by  , of   P   and the columns of   P  R  , of rows of   P   is greater than the number of columns,   are independent, then the pseudoinverse can   P+  =  PTP   1– PT  .  Variations of Hebbian Learning  Filtered Learning  Delta Rule  Unsupervised Hebb   See Chapter 14   Wnew  =  – 1  Wold tqpq  +  T   See Chapter 10   Wnew Wold  tq  =  +    aq–  T pq   See Chapter 13   Wnew Wold aqpq  =  +  T  7  7-15   7 Supervised Hebbian Learning  Solved Problems  P7.1 Consider the linear associator shown in Figure P7.1.  Inputs  p 4x1  4  n cid:0 W  Linear Layer  cid:0  cid:0   cid:0  cid:0   cid:0  cid:0  a = purelin  Wp   2x4  2x1  2  a 2x1  Figure P7.1   Single-Neuron Perceptron  Let the input output prototype vectors be          1 1– 1 1–  1 1–                  1 1 1– 1–  p1  =  t1  =  p2  =  t2  =  1 1  .          i. Use the Hebb rule to find the appropriate weight matrix for   this linear associator.  ii. Repeat part  i  using the pseudoinverse rule.  iii. Apply the input    to the linear associator using the weight  matrix of part  i , then using the weight matrix of part  ii .   p1  i. The first step is to create the P and T matrices of Eq.  7.10 :  P  =  ,  T  =  1 1 1– 1  .  1 1 1 1– 1– 1 1– 1–  Then the weight matrix can be computed using Eq.  7.9 :  7-16   Solved Problems  Wh  =  TPT  =  1 1 1– 1  1 1– 1 1  1 1–  1– 1–  =  2 0 0 0 2 2–  2– 0  .  ii. For the pseudoinverse rule we use Eq.  7.32 :  , four, is greater than the number of columns  Since the number of rows of  P , two, and the columns of  of   are independent, then the pseudoinverse  P can be computed by Eq.  7.34 :  P  W TP+  =  .  P+  =  PTP   1– PT  .  P+  =  1 1– 1 1  1 1–  1– 1–  1 1– 1 1  1 1–  1– 1–  =       1–    4 0 0 4  1 1– 1 1  1 1–  1– 1–         1 1 1 1– 1– 1 1– 1–   1–       =  1 --- 0 4  0  1 --- 4  1 1– 1 1  1 1–  1– 1–  =  1 --- 4 1 --- 4  1 ---– 4 1 --- 4  1 --- 4 1 ---– 4  1 ---– 4 1 ---– 4  The weight matrix can now be computed:  Wp  =  TP+  =  1 1 1– 1  1 --- 4 1 --- 4  1 ---– 4 1 --- 4  1 --- 4 1 ---– 4  1 ---– 4 1 ---– 4  =  1 --- 0 0 2  0  1 --- 2  1 ---– 2  1 ---– 2  0  .  iii. We now test the two weight matrices.  Whp1  =  2 0 0 0 2 2–  2– 0  =  4 4–  t1  1 1– 1 1–  7  7-17   7 Supervised Hebbian Learning  Wpp1  =  1 --- 0 0 2  0  1 --- 2  1 ---– 2  1 ---– 2  0  1 1– 1 1–  =  1 1–  =  t1  Why didn’t the Hebb rule produce the correct results? Well, consider again  Eq.  7.11 . Since   are orthogonal  check that they are  this equa- p1 tion can be written   and   p2  but the  the network will not be   p1   vector is not normalized, so     t1  .    1  . Therefore the output of   The pseudoinverse rule, on the other hand, is guaranteed to minimize  Whp1  =  t1 p1      ,  Tp1 Tp1 p1  2       q  1=  tq Wpq  –  2  ,  which in this case can be made equal to zero.  P7.2 Consider the prototype patterns shown to the left.  i. Are these patterns orthogonal?  ii. Design an autoassociator for these patterns. Use the Hebb   rule.  iii. What response does the network give to the test input pat-  tern,   pt  , shown to the left?   The first thing we need to do is to convert the patterns into vectors.   i. Let’s assign any solid square the value 1 and any open square the value -1.  Then to convert from the two-dimensional pattern to a vector we will scan  the pattern column by column.  We could use rows if we wished.  The two  prototype vectors then become:  p1  =  1 1 1– 1 1–  1–  T  p2  =  1– 1 1 1 1 1–  T  .  To test orthogonality we take the inner product of   p1   and   p2  :  7-18  p1  p2  pt   Solved Problems  Tp2 p1  =  1 1 1– 1 1–  1–  =  .  0  1– 1 1 1 1 1–     Tp1 p1  =  Tp2 p2  =  6  .   W TPT  =  ,  P  =  T  =  .  1 1– 1 1 1– 1 1 1 1 1– 1– 1–  Therefore they are orthogonal.  Although they are not normalized since  ii. We will use an autoassociator like the one in Figure 7.2, except that the  number of inputs and outputs to the network will be six. To find the weight  matrix we use the Hebb rule:  where  Therefore the weight matrix is  W TPT  =  =  1 1 1– 1 1– 1– 1 1 1 1  1– 1–  =  0  2–  2 0 2– 0 2 0 2 0 2– 0 2 0 2 0 2– 0  0 2– 0 2 0 2 0 2– 0 2 0 2 0 0 2 2–  2–  0  .  iii. To apply the test pattern to the network we convert it to a vector:  pt  =  1 1 1 1 1 1–  T  .  The network response is then  1 1– 1 1 1– 1 1 1 1– 1 1– 1–  7-19  7             0  2–  2 0 2– 0 2 0 2 0 2– 0 2 0 2 0 2– 0  0 2– 0 2 0 2 0 2– 0 2 0 2 0 0 2 2–  2–  0  1 1 1 1 1 1–            7 Supervised Hebbian Learning  a  =  hardlims Wpt      =  hardlims  a  =  hardlims  =  =  p2 .   2–  6   2   6  2  6–            1– 1 1 1 1 1–  Is this a satisfactory response? How would we want the network to respond  to this input pattern? The network should produce the prototype pattern  ,  that is closest to the input pattern. In this case the test input pattern,  pt has a Hamming distance of 1 from  . There- fore the network did produce the correct response.  See Chapter 3 for a dis- cussion of Hamming distance.   , and a distance of 2 from   p1  p2  Note that in this example the prototype vectors were not normalized. This  did not cause the same problem with network performance that we saw in  Problem P7.1, because of the hardlims nonlinearity. It forces the network  output to be either 1 or -1. In fact, most of the interesting and useful prop- erties of neural networks are due to the effects of nonlinearities.  P7.3 Consider an autoassociation problem in which there are three pro-  . Design autoassociative   totype patterns  shown below as  networks to recognize these patterns, using both the Hebb rule  and the pseudoinverse rule. Check their performance on the test  pattern    shown below.  p1 p2 p3  ,   ,   pt  p1  =  p2  =  p3  =  pt  =  1 1 1 1– 1 1– 1  1 1 1– 1– 1 1 1  7-20  1– 1 1– 1 1 1– 1  1– 1 1– 1– 1 1– 1   This problem is a little tedious to work out by hand, so let’s use MATLAB.  First we create the prototype vectors.  » 2 + 2 ans =       4  Solved Problems  p1=[ 1  1 -1 -1  1  1  1]';  p2=[ 1  1  1 -1  1  -1  1]';  p3=[-1  1 -1  1  1 -1  1]';  P=[p1 p2 p3];  Now we can compute the weight matrix using the Hebb rule.  wh=P*P';  To check the network we create the test vector.  pt=[-1  1 -1 -1  1 -1  1]';  The network response is then calculated.  ah=hardlims wh*pt ;  ah'  ans =       1     1    -1    -1     1    -1     1  Notice that this response does not match any of the prototype vectors. This  is not surprising since the prototype patterns are not orthogonal. Let’s try  the pseudoinverse rule.  7  pseu=inv P'*P *P';  wp=P*pseu;  ap=hardlims wp*pt ;  ap'  ans =  7-21      -1     1    -1     1     1    -1     1  Note that the network response is equal to  . Is this the correct response?  As usual, we want the response to be the prototype pattern closest to the  input pattern. In this case   and  . Therefore the pseudoinverse rule pro- p2 duces the correct response.   is a Hamming distance of 2 from both   , but only a distance of 1 from   p1  p3  p3  pt  Try other test inputs to see if there are additional cases where the pseudo- inverse rule produces better results than the Hebb rule.   7 Supervised Hebbian Learning  P7.4 Consider the three prototype patterns shown to the left.  p1  p2  p3  recognize these three patterns.  i. Use the Hebb rule to design a perceptron network that will   pt  ii. Find the response of the network to the pattern    shown to   pt  the left. Is the response correct?  i. We can convert the patterns to vectors, as we did in previous problems,  to obtain:  p1  =  p2  =  p3  =  pt  =  .  1 1– 1 1  1 1 1– 1  1– 1– 1– 1  1 1– 1 1–  We now need to choose the desired output vectors for each prototype input  vector. Since there are three prototype vectors that we need to distinguish,  we will need two elements in the output vector. We can choose the three de- sired outputs to be:  t1  =  1– 1–  t2  =  1– 1  t3  =  .  1 1–   Note that this choice was arbitrary. Any distinct combination of 1 and -1  could have been chosen for each vector.    The resulting network is shown in Figure P7.2.  Inputs  Sym. Hard Limit Layer  n 2x1  W 2x4   cid:0  cid:0  p  cid:0  cid:0  4x1   cid:0   cid:0   cid:0  2 a = hardlims  Wp   a 2x1  4  7-22  Figure P7.2  Perceptron Network for Problem P7.4  The next step is to determine the weight matrix using the Hebb rule.   Solved Problems  W TPT  =  =  1– 1–  1– 1  1 1–  1 1– 1 1 1– 1–  1 1 1– 1 1– 1  =  1– 3– 1 3  1– 1–  1– 1–  ii. The response of the network to the test input pattern is calculated as  follows.         3– 1– 1 3  1– 1–  1– 1–  1 1– 1 1–         a  =  hardlims Wpt      =  hardlims  =  hardlims   2–  2–      =  1– 1–  p1 .  So the response of the network indicates that the test input pattern is clos- est to   is 1, while the  distance to   . Is this correct? Yes, the Hamming distance to    and    is 3.  p1  p1  p2  p3  P7.5 Suppose that we have a linear autoassociator that has been de-  signed for  Hebb rule. The vector elements are either 1 or -1.   orthogonal prototype vectors of length   Q  R   using the   Q   prototype patterns are eigenvectors of the   ii. What are the other    R   -   Q    eigenvectors of the weight ma-  i. Show that the  weight matrix.  trix?  7  i. Suppose the prototype vectors are:  p1 p2   pQ        .  Since this is an autoassociator, these are both the input vectors and the de- sired output vectors. Therefore  T  =  p1 p2  pQ  P  =  p1 p2  pQ  .  If we then use the Hebb rule to calculate the weight matrix we find  W TPT  =  =  T pqpq  ,  Q    q  1=  7-23   7 Supervised Hebbian Learning  from Eq.  7.8 . Now, if we apply one of the prototype vectors as input to the  network we obtain  a Wpk  =  =  Q      q  1=   T pqpq    Q  q  1=  pk  =   pq  pq    Tpk    .  Because the patterns are orthogonal, this reduces to  And since every element of    must be either -1 or 1, we find that  pk  To summarize the results:  a  =  pk pk    Tpk    .  a  =  pkR  .  Wpk  =  Rpk  ,  which implies that   is the corresponding  eigenvalue. Each prototype vector is an eigenvector with the same eigen- value.   is an eigenvector of    and   pk  W  R  ii. Note that the repeated eigenvalue  -dimensional eigenspace   has a  Q associated with it: the subspace spanned by the   prototype vectors. Now  Q consider the subspace that is orthogonal to this eigenspace. Every vector in  this subspace should be orthogonal to each prototype vector. The dimension  of the orthogonal subspace will be  - . Consider the following arbitrary  basis set for this orthogonal space:  R Q  R  If we apply any one of these basis vectors to the network we obtain:  z1 z2   zR Q–        .  a Wzk  =  =  Q      q  1=   T pqpq    Q  q  1=  zk  =   pq  pq    Tzk    =  0  ,  since each  eigenvector of    is orthogonal to every   with eigenvalue 0.  W  zk  pq  . This implies that each   zk   is an   To summarize, the weight matrix   and 0. This  means that any vector in the space spanned by the prototype vectors will  be amplified by  , whereas any vector that is orthogonal to the prototype  vectors will be set to 0. We will revisit this concept when we discuss the per- formance of the Hopfield network in Chapter 18.   has two eigenvalues,   W  R  R  7-24   Solved Problems  P7.6 The networks we have used so far in this chapter have not includ- ed a bias vector. Consider the problem of designing a perceptron  network  Figure P7.3  to recognize the following patterns:  p1  =  1 1  p2  =  .  2 2  Inputs  - Title -   cid:0  cid:0  p  cid:0  cid:0  W 2x1 1x2  cid:0  cid:0 b 1  cid:0  cid:0   Sym. Hard Limit Layer  cid:0   cid:0   cid:0   cid:0  1 a = hardlims  Wp + b   - Exp -  n 1x1  1x1  2  a 1x1  Figure P7.3  Single-Neuron Perceptron  i. Why is a bias required to solve this problem?  ii. Use the pseudoinverse rule to design a network with bias to   solve this problem.  i. Recall from Chapters 3 and 4 that the decision boundary for the percep- tron network is the line defined by:  7  If there is no bias, then   b  0=   and the boundary is defined by:  Wp b+  0=  .  Wp  0=  ,  Wp = 0  p2  p1  p1  p2   and   , which are given in this problem. They are shown graph-  which is a line that must pass through the origin. Now consider the two  vectors,  ically in the figure to the left, along with an arbitrary decision boundary  that passes through the origin. It is clear that no decision boundary that  passes through the origin could separate these two vectors. Therefore a  bias is required to solve this problem.  ii. To use the pseudoinverse rule  or the Hebb rule  when there is a bias  term, we should treat the bias as another weight, with an input of 1  as is  shown in all of the network figures . We then augment the input vectors  with a 1 as the last element:  7-25   7 Supervised Hebbian Learning  Let’s choose the desired outputs to be  so that  p'1  =  p'2  =  .  2 2 1  t1  1=  t2  1–=  ,  P  =  ,   T  =  1 1–  .  1 1 1  1 2 1 2 1 1  We now form the pseudoinverse matrix:  P+  =  1 1 1 2 2 1         1–      1 2 1 2 1 1  1–  1 1 1 2 2 1  =  3 5 5 9  1 1 1 2 2 1  =  0.5– 0.5– 0.5 0.5  2 1–  .  The augmented weight matrix is then computed:  W'  =  TP+  =  1 1–  0.5– 0.5– 0.5 0.5  2 1–  =  1–  1– 3  .  We can then pull out the standard weight matrix and bias:  W  =  1–  1–  b  3=  .  The decision boundary for this weight and bias is shown in the Figure P7.4.  This boundary does separate the two prototype vectors.  7-26   Solved Problems  Wp + b = 0  p2  p1  Figure P7.4  Decision Boundary for Solved Problem P7.6  P7.7 In all of our pattern recognition examples thus far, we have repre- sented patterns as vectors by using “1” and “-1” to represent dark  and light pixels  picture elements , respectively. What if we were  to use “1” and “0” instead? How should the Hebb rule be changed?  First, let’s introduce some notation to distinguish the two different repre- sentations  usually referred to as the bipolar {-1, 1} representation and the  binary {0, 1} representation . The bipolar representation of the prototype  input output vectors will be denoted  p1 t1 { , }    { , }   pQ tQ p2 t2 }  {      ,  ,  and the binary representation will be denoted  {  p'1 t'1 , }    {  p'2 t'2  ,   }   p'Q t'Q  {    ,  }  .  The relationship between the two representations is given by:  p'q  =  1 ---pq 2  ---1+  1 2  pq  =  2p'q  1–  ,  where   1   is a vector of ones.  Next, we determine the form of the binary associative network. We will use  the network shown in Figure P7.5. It is different than the bipolar associa- tive network, as shown in Figure 7.2, in two ways. First, it uses the hardlim  nonlinearity rather than hardlims, since the output should be either 0 or  1. Secondly, it uses a bias vector. It requires a bias vector because all binary  vectors will fall into one quadrant of the vector space, so a boundary that  passes through the origin will not always be able to divide the patterns.   See Problem P7.6.   The next step is to determine the weight matrix and the bias vector for this  network. If we want the binary network of Figure P7.5 to have the same   7  7-27   7 Supervised Hebbian Learning  effective response as a bipolar network  as in Figure 7.2 , then the net in- put,   , should be the same for both networks:  n  W'p' b+  =  Wp  .  Inputs  Hard Limit Layer  p' R x 1  1  R  S x R  n cid:0  cid:0 W'  cid:0  cid:0 b  cid:0  cid:0    cid:0  cid:0  a  cid:0  cid:0  S x 1  cid:0  cid:0   S a = hardlim  Wp'+ b   S x 1  S x 1  Figure P7.5  Binary Associative Network  This will guarantee that whenever the bipolar network produces a “1” the  binary network will produce a “1”, and whenever the bipolar network pro- duces a “-1” the binary network will produce a “0”.  If we then substitute for   p'   as a function of   p   we find:  W'     ---p 1 1  ---1+  2 2  b+  =  1 ---W'p 1 ---W'1 + 2 2  b+  =  Wp  .  Therefore, to produce the same results as the bipolar network, we should  choose  W'  2W=  b  –=  W1  ,  where   W   is the bipolar weight matrix.  7-28   Epilogue  Epilogue  We had two main objectives for this chapter. First, we wanted to introduce  one of the most influential neural network learning rules: the Hebb rule.  This was one of the first neural learning rules ever proposed, and yet it con- tinues to influence even the most recent developments in network learning  theory. Second, we wanted to show how the performance of this learning  rule could be explained using the linear algebra concepts discussed in the  two preceding chapters. This is one of the key objectives of this text. We  want to show how certain important mathematical concepts underlie the  operation of all artificial neural networks. We plan to continue to weave to- gether the mathematical ideas with the neural network applications, and  hope in the process to increase our understanding of both.  We will again revisit the Hebb rule in Chapters 15 and 21. In Chapter 21  we will use the Hebb rule in the design of a recurrent associative memory  network — the Hopfield network.  The next two chapters introduce some mathematics that are critical to our  understanding of the two learning laws covered in Chapters 10 and 11.  Those learning laws fall under a subheading called performance learning,  because they attempt to optimize the performance of the network. In order  to understand these performance learning laws, we need to introduce some  basic concepts in optimization. As with the material on the Hebb rule, our  understanding of these topics in optimization will be greatly aided by our  previous work in linear algebra.  7  7-29   7 Supervised Hebbian Learning  Further Reading  [Albe72]   A. Albert, Regression and the Moore-Penrose Pseudoin- verse, New York: Academic Press, 1972.  [Ande72]   Albert’s text is the major reference for the theory and basic  properties of the pseudoinverse. Proofs are included for all  major pseudoinverse theorems.  J. Anderson, “A simple neural network generating an inter- active memory,” Mathematical Biosciences, vol. 14, pp.  197–220, 1972.  Anderson proposed a “linear associator” model for associa- tive memory. The model was trained, using a generaliza- tion of the Hebb postulate, to learn an association between  input and output vectors. The physiological plausibility of  the network was emphasized. Kohonen published a closely  related paper at the same time [Koho72], although the two  researchers were working independently.  [Hebb49]  D. O. Hebb, The Organization of Behavior, New York:  Wiley, 1949.  The main premise of this seminal book is that behavior can  be explained by the action of neurons. In it, Hebb proposes  one of the first learning laws, which postulated a mecha- nism for learning at the cellular level.  [Koho72]   T. Kohonen, “Correlation matrix memories,” IEEE Trans- actions on Computers, vol. 21, pp. 353–359, 1972.  Kohonen proposed a correlation matrix model for associa- tive memory. The model was trained, using the outer prod- uct rule  also known as the Hebb rule , to learn an  association between input and output vectors. The mathe- matical structure of the network was emphasized. Ander- son published a closely related paper at the same time  [Ande72], although the two researchers were working inde- pendently.  7-30   Exercises  Exercises  pt  p1  p2  E7.1 Consider the prototype patterns given to the left.   p1  p2  i. Are   p1   and   p2   orthogonal?  ii. Use the Hebb rule to design an autoassociator network for these   patterns.  iii. Test the operation of the network using the test input pattern     pt shown to the left. Does the network perform as you expected? Ex- plain.  E7.2 Repeat Exercise E7.1 using the pseudoinverse rule.  E7.3 Use the Hebb rule to determine the weight matrix for a perceptron network    shown in Figure E7.1  to recognize the patterns shown to the left.  Inputs  Sym. Hard Limit Layer  n 1x1  W 1x6   cid:0  cid:0  p  cid:0  cid:0  6x1   cid:0   cid:0   cid:0  1 a = hardlims  Wp   a 1x1  6  7  Figure E7.1  Perceptron Network for Exercise E7.3  E7.4 In Problem P7.7 we demonstrated how networks can be trained using the  Hebb rule when the prototype vectors are given in binary  as opposed to bi- polar  form. Repeat Exercise E7.1 using the binary representation for the  prototype vectors. Show that the response of this binary network is equiv- alent to the response of the original bipolar network.  E7.5 Show that an autoassociator network will continue to perform if we zero   the diagonal elements of a weight matrix that has been determined by the  Hebb rule. In other words, suppose that the weight matrix is determined  from:  W PPT QI  =  –  ,  7-31   7 Supervised Hebbian Learning  where  vectors continue to be eigenvectors of the new weight matrix.    is the number of prototype vectors.  Hint: show that the prototype   Q  E7.6 We have three input output prototype vector pairs:      1 0           1 1  p1  =  t1  =  1  p2  =  t2  =  1–  p3  =  t3  =  1           0 1  .      i. Show that this problem cannot be solved unless the network uses a   bias.  ii. Use the pseudoinverse rule to design a network for these prototype  vectors. Verify that the network correctly transforms the prototype  vectors.  E7.7 Consider the reference patterns and targets given below. We want to use   these data to train a linear associator network.      2 4  p1  =  t1  =  26  p2  =  t2  =  26          4 2          p3  =  2– 2–  t3  =  26–      i. Use the Hebb rule to find the weights of the network.  ii. Find and sketch the decision boundary for the network with the   Hebb rule weights.  iii. Use the pseudo-inverse rule to find the weights of the network. Be-  cause the number, R, of rows of  , the pseudoinverse can be computed by  umns, Q, of  P  1– P+ PT PPT  P  =    .  is less than the number of col-  iv. Find and sketch the decision boundary for the network with the   pseudo-inverse rule weights.  v. Compare  discuss  the decision boundaries and weights for each of   the methods  Hebb and pseudo-inverse .   E7.8 Consider the three prototype patterns shown in Figure E7.2.  i. Are these patterns orthogonal? Demonstrate.  ii. Use the Hebb rule to determine the weight matrix for a linear au-  toassociator to recognize these patterns.  iii. Draw the network diagram.  7-32   Exercises  iv. Find the eigenvalues and eigenvectors of the weight matrix.  Do not   W I–  0=  . Use an analysis of the Hebb   solve the equation  rule.   p1  p2  p3  -1 1  Figure E7.2  Prototype Patterns for Exercise E7.8  E7.9 Suppose that we have the following three reference patterns and their tar-  gets.      p1  =  t1  =  75  p2  =  t2  =  75          6 3  3 6          p3  =  6– 3  t3  =  75–      i. Draw the network diagram for a linear associator network that   could be trained on these patterns.  ii. Use the Hebb rule to find the weights of the network.  iii. Find and sketch the decision boundary for the network with the   Hebb rule weights. Does the boundary separate the patterns? Dem- onstrate.  iv. Use the pseudo-inverse rule to find the weights of the network. De-  scribe the difference between this boundary and the Hebb rule  boundary.  E7.10 We have the following input output pairs:      p1  =  t1  =  1  1 1          p2  =  1 1–  t2  =  1–      i. Use the Hebb rule to determine the weight matrix for the percep-  tron network shown in Figure E7.3.  ii. Plot the resulting decision boundary. Is this a “good” decision   boundary? Explain.  iii. Repeat part i. using the Pseudoinverse rule.  iv. Will there be any difference in the operation of the network if the   Pseudoinverse weight matrix is used? Explain.  7  7-33   7 Supervised Hebbian Learning  Inputs  Sym. Hard Limit Layer  a 1 x 1  p 2 x 1  W  1 2x  n 1 x 1  2  1  a =  hardlims Wp        Figure E7.3  Network for Exercise E7.10  » 2 + 2 ans =       4  E7.11 One question we might ask about the Hebb and pseudoinverse rules is:   How many prototype patterns can be stored in one weight matrix? Test this  experimentally using the digit recognition problem that was discussed on  page 7-10. Begin with the digits “0” and “1”. Add one digit at a time up to  “6”, and test how often the correct digit is reconstructed after randomly  changing 2, 4 and 6 pixels.  i. First use the Hebb rule to create the weight matrix for the digits “0”  and “1”. Then randomly change 2 pixels of each digit and apply the  noisy digits to the network. Repeat this process 10 times, and record  the percentage of times in which the correct pattern  without noise   is produced at the output of the network. Repeat as 4 and 6 pixels of  each digit are modified. The entire process is then repeated when  the digits “0”, “1” and “2” are used. This continues, one digit at a  time, until you test the network when all of the digits “0” through  “6” are used. When you have completed all of the tests, you will be  able to plot three curves showing percentage error versus number of  digits stored, one curve each for 2, 4 and 6 pixel errors.  ii. Repeat part  i  using the pseudoinverse rule, and compare the re-  sults of the two rules.  7-34   Objectives  8 Performance Surfaces and  Optimum Points  Objectives Theory and Examples  Taylor Series  Vector Case  Directional Derivatives Minima Necessary Conditions for Optimality  First-Order Conditions Second-Order Conditions  Quadratic Functions  Eigensystem of the Hessian  Summary of Results Solved Problems Epilogue Further Reading Exercises  8-1 8-2 8-2 8-4 8-5 8-7 8-9 8-10 8-11 8-12 8-13 8-20 8-22 8-34 8-35 8-36  Objectives  8  This chapter lays the foundation for a type of neural network training tech- nique called performance learning. There are several different classes of  network learning laws, including associative learning  as in the Hebbian  learning of Chapter 7  and competitive learning  which we will discuss in  Chapter 16 . Performance learning is another important class of learning  law, in which the network parameters are adjusted to optimize the perfor- mance of the network. In the next two chapters we will lay the groundwork  for the development of performance learning, which will then be presented  in detail in Chapters 10–14. The main objective of the present chapter is to  investigate performance surfaces and to determine conditions for the exist- ence of minima and maxima of the performance surface. Chapter 9 will fol- low this up with a discussion of procedures to locate the minima or maxima.  8-1   Theory and Examples  Performance Learning  Performance Index  8 Performance Surfaces and Optimum Points  There are several different learning laws that fall under the category of  performance learning. Two of these will be presented in this text. These  learning laws are distinguished by the fact that during training the net- work parameters  weights and biases  are adjusted in an effort to optimize  the “performance” of the network.  There are two steps involved in this optimization process. The first step is  to define what we mean by “performance.” In other words, we must find a  quantitative measure of network performance, called the performance in- dex, which is small when the network performs well and large when the  network performs poorly. In this chapter, and in Chapter 9, we will assume  that the performance index is given. In Chapters 10, 11 and 13 we will dis- cuss the choice of performance index.  The second step of the optimization process is to search the parameter  space  adjust the network weights and biases  in order to reduce the per- formance index. In this chapter we will investigate the characteristics of  performance surfaces and set some conditions that will guarantee that a  surface does have a minimum point  the optimum we are searching for .  Thus, in this chapter we will obtain some understanding of what perfor- mance surfaces look like. Then, in Chapter 9 we will develop procedures for  locating the optimum points.  Taylor Series  Taylor Series Expansion  F x   Let us say that the performance index that we want to minimize is repre- sented by   is the scalar parameter we are adjusting. We will  assume that the performance index is an analytic function, so that all of its  derivatives exist. Then it can be represented by its Taylor series expansion  about some nominal point   , where   x  x  :  F x   =  F x    +  F x   x– x    x  x=  d xd  +  1 --- 2  2  d x2 d  F x   x  x=  x– x  2   +  +  1 ----- n!  n  d xn d  F x   x  x=  x– x  n   +   8.1   2 2+  We will use the Taylor series expansion to approximate the performance  index, by limiting the expansion to a finite number of terms. For example,  let   8-2   Taylor Series  F x   =  cos  x   .   8.2   The Taylor series expansion for   F x    about the point   x  0=   is  F x   =  cos  x   =  cos  0   –  sin  0  x  0–    –  cos  0  x  0–  2  1 --- 2  +  1 --- 6  sin  0  x  0–  3  +    =  1  –  1 ---x2 2  +  1 ------x4  24  +  The zeroth-order approximation of  is  F x     using only the zeroth power of   x      The second-order approximation is  F x  F0 x     1=  .  F x  F2 x     =  1  –  1 ---x2 2  .   Note that in this case the first-order approximation is the same as the ze- roth-order approximation, since the first derivative is zero.   The fourth-order approximation is   F x  F4 x     =  1  –  1 ---x2 2  +  1 ------x4 24  .  A graph showing  8.1.  F x   and these three approximations is shown in Figure    8.3    8.4    8.5    8.6   8  F0 x   F4 x   F x   F2 x   1.5  0.5  1  0  -0.5  -1  -1.5 -6  8-3  -4  -2  0  2  4  6  Figure 8.1  Cosine Function and Taylor Series Approximations   8 Performance Surfaces and Optimum Points  x  x  0=  . However, as    moves farther away from   From the figure we can see that all three approximations are accurate if  is very close to  higher-order approximations are accurate. The second-order approxima- tion is accurate over a wider range than the zeroth-order approximation,  and the fourth-order approximation is accurate over a wider range than the  second-order approximation. An investigation of Eq.  8.1  explains this be- havior. Each succeeding term in the series involves a higher power of  x– , these terms will become geometrically   x smaller.    x  only the    gets closer to   . As   x  x  x  We will use the Taylor series approximations of the performance index to  investigate the shape of the performance index in the neighborhood of pos- sible optimum points.   To experiment with Taylor series expansions of the cosine function, use the  MATLAB® Neural Network Design Demonstration Taylor Series  nnd8ts1 .  Vector Case Of course the neural network performance index will not be a function of a  scalar  . It will be a function of all of the network parameters  weights and  biases , of which there may be a very large number. Therefore, we need to  extend the Taylor series expansion to functions of many variables. Consid- er the following function of    variables:  x  n  F x   =   F x1 x2   xn          .   8.7   The Taylor series expansion for this function, about the point   x  , will be  F x   =  F x    +  F x    x1    x1  –  x1    +  x x=   x2  F x   x x=    x2  –  x2      +  +   xn  F x   x x=    xn  –  xn    +  1 --- 2  2   2  x1  F x   x x=    x1  –  x1  2  +  1 --- 2    2  x1 x2  F x   x x=    x1  –  x1   x2   –  x2    +  This notation is a bit cumbersome. It is more convenient to write it in ma- trix form, as in:  F x   =  F x    +    F x   T  x x–    x x= T F x   2  +  1 x– --- x 2  x– x    +    x x=   8.8    8.9   Gradient  where     F x    is the gradient, and is defined as  8-4   Directional Derivatives    F x   =  F x   F x    F x    x1   x2   xn  T  ,   8.10   Hessian  and   2  F x    is the Hessian, and is defined as:  2  F x   =    F x   F x    F x   .   8.11   F x   F x    F x     2  x1 x2 2   2  x2    2   2  x1 2  x2 x1  2  xn x1    F x   2  xn x2    F x    2  x1 xn 2  x2 xn        F x   2   2  xn  The gradient and the Hessian are very important to our understanding of  performance surfaces. In the next section we discuss the practical meaning  of these two concepts.  To experiment with Taylor series expansions of a function of two variables,  use the MATLAB® Neural Network Design Demonstration Vector Taylor  Series nnd8ts2 .  Directional Derivatives    F x   xi  , is the first derivative of the per- The ith element of the gradient,   axis. The ith element of the diagonal of the  formance index   along the  xi F 2F x  xi 2 Hessian matrix,  , is the second derivative of the performance in-  axis. What if we want to know the derivative of the func- dex   along the  xi  be a vector in the direction along  tion in an arbitrary direction? We let  which we wish to know the derivative. This directional derivative can be  computed from  p  F    8  Directional Derivative  The second derivative along    can also be computed:  p  pT F x  ----------------------   p  .  2  pT F x  p ----------------------------  .  p 2  8-5   8.12    8.13    8 Performance Surfaces and Optimum Points  To illustrate these concepts, consider the function  2 2+  F x   =  2 x1  +  2 2x2  .   8.14   Suppose that we want to know the derivative of the function at the point  x . First we evaluate the gradient  at    in the direction   0.5 0.5  2 1–  p  =  :  T  T  = x    F x   =  x x=   x1  x2  F x   F x   =  2x1 4x2  =  .  1 2  x x=   8.15   The derivative in the direction    can then be computed:  p  x x=  1 2 1– 2 ------------------------  2 1–  pT F x  ----------------------   p  =  =  0 ------- 5  =  0  .   8.16   x Therefore the function has zero slope in the direction  Why did this happen? What can we say about those directions that have  zero slope? If we consider the definition of directional derivative in Eq.   8.12 , we can see that the numerator is an inner product between the di- rection vector and the gradient. Therefore any direction that is orthogonal  to the gradient will have zero slope.   from the point   p  .   Which direction has the greatest slope? The maximum slope will occur  when the inner product of the direction vector and the gradient is a maxi- mum. This happens when the direction vector is the same as the gradient.   Notice that the magnitude of the direction vector has no effect, since we  normalize by its magnitude.  This effect is illustrated in Figure 8.2, which  . On the contour plot we see five  shows a contour plot and a 3-D plot of  vectors starting from our nominal point   and pointing in different direc- tions. At the end of each vector the first directional derivative is displayed.  The maximum derivative occurs in the direction of the gradient. The zero  derivative is in the direction orthogonal to the gradient  tangent to the con- tour line .  F x  x  To experiment with directional derivatives, use the MATLAB® Neural Net- work Design Demonstration Directional Derivatives  nnd8dd .  8-6   Minima  2.2  2.0  1.6  0.8 0.0  6  4  2  0 2  x2  0  2  1  -1  -2 -2  -1  1  2  0 x1  1  0  x2  -1  -1  0  x1  -2  -2  2  1  Figure 8.2  Quadratic Function and Directional Derivatives  Minima  Recall that the objective of performance learning will be to optimize the  network performance index. In this section we want to define what we  mean by an optimum point. We will assume that the optimum point is a  minimum of the performance index. The definitions can be easily modified  for maximization problems.  Strong Minimum  Strong Minimum  Global Minimum  Global Minimum   F x x+    is a strong minimum of   x The point  F x  In other words, if we move away from a strong minimum a small distance  in any direction the function will increase.  F x    such that    if a scalar   . 0   exists, such that    for all   0  x  x        .  x  F x    if   0   is a unique global minimum of   The point  x For a simple strong minimum,     . Therefore this is some- at some points outside a small neighborhood of  times called a local minimum. For a global minimum the function will be  larger than the minimum point at every other point in the parameter  space.  , the function may be smaller than    for all   F x  x  x   F x x+   F x      8  Weak Minimum  Weak Minimum  The point  lar   0     is a weak minimum of   x  exists, such that   F x  F x     F x x+    if it is not a strong minimum, and a sca- . 0   such that    for all           x  x  8-7   8 Performance Surfaces and Optimum Points  No matter which direction we move away from a weak minimum, the func- tion cannot decrease, although there may be some directions in which the  function does not change.  2 2+  As an example of local and global minimum points, consider the following  scalar function:  F x   =  3x4  7x2  –  1 ---x– 2  6+  .   8.17   This function is displayed in Figure 8.3. Notice that it has two strong min- imum points: at approximately -1.1 and 1.1. For both of these points the  function increases in a local neighborhood. The minimum at 1.1 is a global  minimum, since there is no other point for which the function is as small.  There is no weak minimum for this function. We will show a two-dimen- sional example of a weak minimum later.  F x   =  3x4  7x2  –  1 ---x– 2  6+  Local Minimum  Global Minimum  -1  0  1  2  8  6  4  2  0 -2  8-8  Figure 8.3  Scalar Example of Local and Global Minima  2 2+  Now let’s consider some vector cases. First, consider the following function:  Contour Plot  Saddle Point  F x   =    x2  x1–  4  +  8x1x2  x1–  +  x2  +  3  .   8.18   In Figure 8.4 we have a contour plot  a series of curves along which the  function value remains constant  and a 3-D surface plot for this function   for function values less than 12 . We can see that the function has two  strong local minimum points: one at  -0.42, 0.42 , and the other at  0.55,  -0.55 . The global minimum point is at  0.55, -0.55 .  There is also another interesting feature of this function at  -0.13, 0.13 . It  is called a saddle point because of the shape of the surface in the neighbor- hood of the point. It is characterized by the fact that along the line    x2–= the saddle point is a local maximum, but along a line orthogonal to that line  it is a local minimum. We will investigate this example in more detail in  Problems P8.2 and P8.5.  x1   Necessary Conditions for Optimality  This function is used in the MATLAB® Neural Network Design Demonstra- tion Vector Taylor Series  nnd8ts2 .  2  1  0  -1  -2 -2  2  1  0  -1  -2 -2  12  8  4  0 2  8  6  4  2  0 2  1  0  -1  0  -1  -2  -2  2  1  -1  0  1  2  Figure 8.4  Vector Example of Minima and Saddle Point  2 2+  As a final example, consider the function defined in Eq.  8.19 :  F x   =    2 x1  –  1.5x1x2  +  2 2x2  2 x1   8.19   The contour and 3-D plots of this function are given in Figure 8.5. Here we  can see that any point along the line    is a weak minimum.  0=  x1  1  0  -1  0  -1  -2  -2  2  1  -1  0  1  2  Figure 8.5  Weak Minimum Example  8  Necessary Conditions for Optimality  Now that we have defined what we mean by an optimum  minimum  point,  let’s identify some conditions that would have to be satisfied by such a  point. We will again use the Taylor series expansion to derive these condi- tions:  8-9   8 Performance Surfaces and Optimum Points  F x   =  F x x+     =  F x    +    F x   T  x  x x=  +  1 ---xT F x  2  2  x x=  x  +   ,  where  x  =  x  x–  .  First-Order Conditions If  ligible and we can approximate the function as  x   is very small then the higher order terms in Eq.  8.20  will be neg-  F x x+    F x     +    F x   T  x  .  x x=  x  The point  should go up  or at least not go down  if  the second term in Eq.  8.22  should not be negative. In other words   is a candidate minimum point, which means that the function   is not zero. For this to happen   x  However, if this term is positive,    F x   T  x  0  .  x x=    F x   T  x  0  ,  x x=  then this would imply that   F x x–    F x     –    F x   T  x     F x    .   8.25   x x=  But this is a contradiction, since   should be a minimum point. Therefore,  since Eq.  8.23  must be true, and Eq.  8.24  must be false, the only alter- native must be that  x  Since this must be true for any   x  , we have    F x   T  x  0=  .  x x=    F x   0=  .  x x=   8.20    8.21    8.22    8.23    8.24    8.26    8.27   Stationary Points  Therefore the gradient must be zero at a minimum point. This is a first-or- der, necessary  but not sufficient  condition for   to be a local minimum  point. Any points that satisfy Eq.  8.27  are called stationary points.  x  8-10   Necessary Conditions for Optimality  Second-Order Conditions Assume that we have a stationary point  zero at all stationary points, the Taylor series expansion will be  . Since the gradient of   x  F x    is   F x x+     =  F x    +  1 ---xT F x  2  2  x x=  x  +    .   8.28   As before, we will consider only those points in a small neighborhood of  so that  in Eq.  8.28 . Therefore a strong minimum will exist at   x ,   can be approximated by the first two terms    is small and   F x   x   if  x  Positive Definite Matrix  For this to be true for arbitrary  positive definite.  By definition, a matrix   0  x   requires that the Hessian matrix be   A   is positive definite if  Positive Semidefinite  for any vector   z  0  . It is positive semidefinite if  xT F x   2  x  0  .  x x=  zTAz  0  zTAz  0  Sufficient Condition  z  for any vector  . We can check these conditions by testing the eigenvalues  of the matrix. If all eigenvalues are positive, then the matrix is positive def- inite. If all eigenvalues are nonnegative, then the matrix is positive  semidefinite.   A positive definite Hessian matrix is a second-order, sufficient condition for  a strong minimum to exist. It is not a necessary condition. A minimum can  still be strong if the second-order term of the Taylor series is zero, but the  third-order term is positive. Therefore the second-order, necessary condi- tion for a strong minimum is that the Hessian matrix be positive semi-def- inite.  8  2 2+  To illustrate these conditions, consider the following function of two vari- ables:  First, we want to locate any stationary points, so we need to evaluate the  gradient:   8.29    8.30    8.31    8.32    8.33   F x   =  4 x1  2+ x2  .    F x   =  =  0  .  3 4x1 2x2  8-11   8 Performance Surfaces and Optimum Points  Therefore the only stationary point is the point  test the second-order condition, which requires the Hessian matrix:  . We now need to   0=  x  2  F x   =  x  0=  12x1  2 0 2  0  x  0=  =  0 0 0 2  .   8.34   0=  This matrix is positive semidefinite, which is a necessary condition for  x  to be a strong minimum point. We cannot guarantee from first-or- der and second-order conditions that it is a minimum point, but we have  not eliminated it as a possibility. Actually, even though the Hessian matrix  is only positive semidefinite,  cannot prove it from the conditions we have discussed.   is a strong minimum point, but we   0=  x  Just to summarize, the necessary conditions for  strong or weak, of    are:  F x   x   to be a minimum,     F x   0=   and   2  F x    positive semidefinite.  x x=  x x=  The sufficient conditions for    to be a strong minimum point of   F x    are:  x    F x   0=   and   2  F x    positive definite.  x x=  x x=  Quadratic Functions  We will find throughout this text that one type of performance index is uni- versal — the quadratic function. This is true because there are many appli- cations in which the quadratic function appears, but also because many  functions can be approximated by quadratic functions in small neighbor- hoods, especially near local minimum points. For this reason we want to  spend a little time investigating the characteristics of the quadratic func- tion.  Quadratic Function  The general form of a quadratic function is  F x   =  1 ---xTAx dTx 2  +  +  c  ,   8.35   where the matrix  replaced by a symmetric matrix that produces the same    is symmetric.  If the matrix is not symmetric it can be   . Try it!   A  F x   To find the gradient for this function, we will use the following useful prop- erties of the gradient:      hTx    =  xTh     =  h  ,   8.36   where   h   is a constant vector, and  8-12   Quadratic Functions    xTQx  =  Qx QTx  +  =  2Qx   for symmetric Q   .   8.37   We can now compute the gradient of   F x   :  and in a similar way we can find the Hessian:    F x   =  Ax d+  ,  2  F x   A=  .  All higher derivatives of the quadratic function are zero. Therefore the first  three terms of the Taylor series expansion  as in Eq.  8.20   give an exact  representation of the function. We can also say that all analytic functions  behave like quadratics over a small neighborhood  i.e., when   is small .  x  Eigensystem of the Hessian We now want to investigate the general shape of the quadratic function. It  turns out that we can tell a lot about the shape by looking at the eigenval- ues and eigenvectors of the Hessian matrix. Consider a quadratic function  that has a stationary point at the origin, and whose value there is zero:  The shape of this function can be seen more clearly if we perform a change  of basis  see Chapter 6 . We want to use the eigenvectors of the Hessian  matrix,   is symmetric, its eigenvectors  will be mutually orthogonal.  See [Brog91].  This means that if we make up  a matrix with the eigenvectors as the columns, as in Eq.  6.68 :  , as the new basis vectors. Since   A  A  F x   =  1 ---xTAx 2  .  B  =  z1 z2  zn  ,  B 1–  =  BT  .  the inverse of the matrix will be the same as the transpose:  8   This assumes that we have normalized the eigenvectors.   If we now perform a change of basis, so that the eigenvectors are the basis  vectors  as in Eq.  6.69  , the new    matrix will be  A   8.38    8.39    8.40    8.41    8.42    8.43   A'  =    BTAB    =  1 0  0 0 2  0    0 0  n  =    ,  8-13   8 Performance Surfaces and Optimum Points  where the   i   are the eigenvalues of   . We can also write this equation as  A  A  =  BBT  .   8.44   We will now use the concept of the directional derivative to explain the  physical meaning of the eigenvalues and eigenvectors of  how they determine the shape of the surface of the quadratic function.  , and to explain   A  Recall from Eq.  8.13  that the second derivative of a function  direction of a vector    is given by  p  F x    in the   Now define   2  pT F x  p ----------------------------  =  p 2  pTAp -------------- p 2  .  p  =  Bc  ,  where  tors of  tion, and Eq.  8.44 , we can rewrite Eq.  8.45 :  c  is the representation of the vector   with respect to the eigenvec- A .  See Eq.  6.28  and the discussion that follows.  With this defini-  p  pTAp -------------- p 2  =  cTBT BBT Bc  ---------------------------------------- cTBTBc  =  cTc ------------ cTc  =  n  2  ici 1= i ------------------- n 2  ci  i  1=  .  This result tells us several useful things. First, note that this second deriv- ative is just a weighted average of the eigenvalues. Therefore it can never  be larger than the largest eigenvalue, or smaller than the smallest eigen- value. In other words,  min    pTAp -------------- max p 2    .  p  =  zmax  ,  Under what condition, if any, will this second derivative be equal to the  largest eigenvalue? What if we choose  where  For this case the   zmax  c   vector will be   is the eigenvector associated with the largest eigenvalue,   c  =  BTp  =  BTzmax  =  0 0  0 1 0  0  T  ,   8.45    8.46    8.47    8.48    8.49   max  ?    8.50   8-14   z2  λmax    z1  λmin   Quadratic Functions  where the one occurs only in the position that corresponds to the largest  eigenvalue  i.e.,   . This is because the eigenvectors are orthonor- mal.  cmax  1=  If we now substitute   zmax   for   p   in Eq.  8.47  we obtain  TAzmax zmax ---------------------------- zmax  2  =  n  2  ici 1= i ------------------- n 2  ci  i  1=  =  max  .   8.51   So the maximum second derivative occurs in the direction of the eigenvec- tor that corresponds to the largest eigenvalue. In fact, in each of the eigen- vector directions the second derivatives will be equal to the corresponding  eigenvalue. In other directions the second derivative will be a weighted av- erage of the eigenvalues. The eigenvalues are the second derivatives in the  directions of the eigenvectors.  The eigenvectors define a new coordinate system in which the quadratic  cross terms vanish. The eigenvectors are known as the principal axes of the  function contours. The figure to the left illustrates these concepts in two di- mensions. This figure illustrates the case where the first eigenvalue is  smaller than the second eigenvalue. Therefore the minimum curvature   second derivative  will occur in the direction of the first eigenvector. This  means that we will cross contour lines more slowly in this direction. The  maximum curvature will occur in the direction of the second eigenvector,  therefore we will cross contour lines more quickly in that direction.   One caveat about this figure: it is only valid when both eigenvalues have  the same sign, so that we have either a strong minimum or a strong maxi- mum. For these cases the contour lines are always elliptical. We will pro- vide examples later where the eigenvalues have opposite signs and where  one of the eigenvalues is zero.  8  2 2+  For our first example, consider the following function:  F x   =  2 x1  2+ x2  =  1 ---xT 2 0 2 0 2  x  .   8.52   The Hessian matrix and its eigenvalues and eigenvectors are  2  F x   =  ,   1  2=  ,   z1  =  ,   2  2=  ,   z2  =   8.53   1 0  .  0 1  2 0 0 2  8-15   8 Performance Surfaces and Optimum Points   Actually, any two independent vectors could be the eigenvectors in this  case. There is a repeated eigenvalue, and its eigenvector is the plane.   Since all the eigenvalues are equal, the curvature should be the same in all  directions, and therefore the function should have circular contours. Figure  8.6 shows the contour and 3-D plots for this function, a circular hollow.  2  1  0  -1  -2 -2  4  2  0 2  1  0  -1  0  -1  -2  -2  2  1  -1  0  1  2  Figure 8.6  Circular Hollow  2 2+  Let’s try an example with distinct eigenvalues. Consider the following qua- dratic function:  F x   =  2 x1  +  x1x2  +  2 x2  =  1 ---xT 2 1 2 1 2  x   8.54   The Hessian matrix and its eigenvalues and eigenvectors are  2  F x   =  ,   1  1=  ,   z1  =  ,   2  3=  ,   z2  =   8.55   2 1 1 2  1 1–  .  1 1   As we discussed in Chapter 6, the eigenvectors are not unique, they can be  multiplied by any scalar.  In this case the maximum curvature is in the di- rection of   so we should cross contour lines more quickly in that direction.  Figure 8.7 shows the contour and 3-D plots for this function, an elliptical  hollow.  z2  8-16   Quadratic Functions  3  2  1  0 2  2  1  0  -1  -2 -2  1  0  -1  0  -1  -2  -2  2  1  -1  0  1  2  Figure 8.7  Elliptical Hollow  2 2+  What happens when the eigenvalues have opposite signs? Consider the fol- lowing function:  F x   =  –  1 2 ---x1 4  –  3 ---x1x2 2  –  1 2 ---x2 4  =  1 ---xT 2  0.5– 1.5–  1.5– 0.5–  x  .   8.56   The Hessian matrix and its eigenvalues and eigenvectors are  2  F x   =  0.5– 1.5–  1.5– 0.5–  1– 1  ,   1  1=  ,   z1  =  ,   2  2–=  ,   z2  =  .   8.57   1– 1–  z1  The first eigenvalue is positive, so there is positive curvature in the direc- tion of  . The second eigenvalue is negative, so there is negative curvature  in the direction of  . Also, since the magnitude of the second eigenvalue is  greater than the magnitude of the first eigenvalue, we will cross contour  lines faster in the direction of   z2  .  z2  Figure 8.8 shows the contour and 3-D plots for this function, an elongated  saddle. Note that the stationary point,  8  x  =  ,  0 0   8.58   is no longer a strong minimum point, since the Hessian matrix is not posi- tive definite. Since the eigenvalues are of opposite sign, we know that the  Hessian is indefinite  see [Brog91] . The stationary point is therefore a sad- dle point. It is a minimum of the function along the first eigenvector  posi- tive eigenvalue , but it is a maximum of the function along the second  eigenvector  negative eigenvalue .  8-17   8 Performance Surfaces and Optimum Points  2  1  0  -1  -2 -2  4  0  -4  -8 2  1  0  -1  0  -1  -2  -2  2  1  -1  0  1  2  Figure 8.8  Elongated Saddle  2 2+  As a final example, let’s try a case where one of the eigenvalues is zero. An  example of this is given by the following function:  F x   =  –  x1x2  +  1 2 ---x1 2  1 2 ---x2 2  =  1 ---xT 1 2 1–  1– 1  x  .   8.59   The Hessian matrix and its eigenvalues and eigenvectors are  2  F x   =  ,   1  2=  ,   z1  =  ,   2  0=  ,   z2  =   8.60   1 1–  1– 1  1– 1  .  1– 1–  The second eigenvalue is zero, so we would expect to have zero curvature  along  . Figure 8.9 shows the contour and 3-D plots for this function, a sta- tionary valley. In this case the Hessian matrix is positive semidefinite, and  we have a weak minimum along the line  z2  x1  x2=  ,   8.61   corresponding to the second eigenvector.  For quadratic functions the Hessian matrix must be positive definite in or- der for a strong minimum to exist. For higher-order functions it is possible  to have a strong minimum with a positive semidefinite Hessian matrix, as  we discussed previously in the section on minima.  8-18   Quadratic Functions  3  2  1  0 2  2  1  0  -1  -2 -2  1.  2.  3.  4.  5.  1  0  -1  0  -1  -2  -2  2  1  -1  0  1  2  Figure 8.9  Stationary Valley  To experiment with other quadratic functions, use the MATLAB® Neural  Network Design Demonstration Quadratic Function  nnd8qf .  At this point we can summarize some characteristics of the quadratic func- tion.   If the eigenvalues of the Hessian matrix are all positive, the function  will have a single strong minimum.  If the eigenvalues are all negative, the function will have a single  strong maximum.  If some eigenvalues are positive and other eigenvalues are negative,  the function will have a single saddle point.  If the eigenvalues are all nonnegative, but some eigenvalues are zero,  then the function will either have a weak minimum  as in Figure 8.9   or will have no stationary point  see Solved Problem P8.7 .  If the eigenvalues are all nonpositive, but some eigenvalues are zero,  then the function will either have a weak maximum or will have no sta- tionary point.  8  We should note that in this discussion we have assumed, for simplicity,  that the stationary point of the quadratic function was at the origin, and  that it had a zero value there. This requires that the terms   in Eq.   8.35  both be zero. If   is nonzero then the function is simply increased in   at every point. The shape of the contours do not change.  magnitude by  When   is invertible, the shape of the contours are not  changed, but the stationary point of the function moves to   is nonzero, and    and   A  d  d  c  c  c  x  =  A–  1– d  .   8.62   A   is not invertible  has some zero eigenvalues  and   If  stationary points may not exist  see Solved Problem P8.7 .  d   is nonzero then   8-19   8 Performance Surfaces and Optimum Points  Summary of Results  Taylor Series  F x   =  F x    +    F x   T  x x–    x x= T F x   2  +  1 x– --- x 2  x– x    +    x x=    F x   =  F x   F x    F x    x1   x2   xn  T  Gradient  Hessian Matrix  2  F x   =    F x   F x    2   2  x1 2  x2 x1  2  xn x1    F x   F x      2  x1 x2 2   2  x2    F x   2  xn x2    F x    2  x1 xn 2  x2 xn      F x   F x     F x   2   2  xn  Directional Derivatives  First Directional Derivative  Second Directional Derivative  pT F x  ----------------------   p  2  pT F x  p ----------------------------  p 2  8-20   Summary of Results  Minima  The point  F x  F x x+   is a strong minimum of   such that    for all   x      F x     x   if a scalar  x    0  .    0   exists, such that   The point  x  0  .  x   is a unique global minimum of   F x    if   F x  F x x+       for all   Strong Minimum  Global Minimum  Weak Minimum  The point  lar   0     is a weak minimum of   x  exists, such that   F x  F x x+  F x      if it is not a strong minimum, and a sca-    such that    for all         0  .  x  x  Necessary Conditions for Optimality  First-Order Condition    F x   0=    Stationary Points   x x=  Second-Order Condition  2  F x   x x=  0    Positive Semidefinite Hessian Matrix   Quadratic Functions  Gradient  Hessian  Directional Derivatives  F x   =  1 ---xTAx dTx 2  +  +  c    F x   =  Ax d+  2  F x   A=  min    pTAp -------------- max p 2    8-21  8   8 Performance Surfaces and Optimum Points  Solved Problems  P8.1 In Figure 8.1 we illustrated 3 approximations to the cosine func-  0=  . Repeat that procedure about the point   tion about the point  x   2  =  .  x  The function we want to approximate is  F x   =  cos  x   .  The Taylor series expansion for   F x    about the point   x  =   2   is  F x   =  cos  x   =  cos  –  sin     ---  2  =  –     ---– x  2  +   3  1  --- x ---–  6 2  –  cos   2     x  ---  ---–  2 2   +  +  –    1   x   ---  --- ---–  2 2 2  3   1   x  ---– ---  ---  2 6 2  5  1  --------- x ---–  120 2  sin    +  The zeroth-order approximation of   F x    is  F x  F0 x     0=  .  The first-order approximation is  F x  F1 x     =  –     ---– x  2  =  x–  .   --- 2   Note that in this case the second-order approximation is the same as the  first-order approximation, since the second derivative is zero.   The third-order approximation is   F x  F3 x     =  –     ---– x  2  +   3  1  --- x ---–  6 2  .  F x   A graph showing  and these three approximations is shown in Figure  P8.1. Note that in this case the zeroth-order approximation is very poor,  while the first-order approximation is accurate over a reasonably wide  range. Compare this result with Figure 8.1. In that case we were expanding  about a local maximum point,   , so the first derivative was zero.  0=  x  Check the Taylor series expansions at other points using the Neural Net- work Design Demonstration Taylor Series  nnd8ts1 .  8-22   Solved Problems  F1 x   F0 x   F x   F3 x   1.5  0.5  1  0  -0.5  -1  -1.5 -6  -4  -2  0  2  4  6  Figure P8.1  Cosine Approximation About   x  =   2  P8.2 Recall the function that is displayed in Figure 8.4, on page 8-9. We  know that this function has two strong minima. Find the second- order Taylor series expansions for this function about the two min- ima.  The equation for this function is  F x   =    x2  x1–  4  +  8x1x2  x1–  +  x2  +  3  .  To find the second-order Taylor series expansion, we need to find the gra- dient and the Hessian for   . For the gradient we have  F x     F x   =   x1  x2  F x   F x   =  –   4 x2  4 x2  3 x1– 3 x1–  +  8x2 1– 1 8x1  +  +  ,  and the Hessian matrix is  8  2  F x   =  2   2  x1 2  x2 x1    F x   F x     2  x1 x2 2   2  x2  F x   F x   =    12 x2  12 x2  2 x1– 2 x1–  –  8+  –    12 x2  12 x2  x1– x1–  2 8+ 2  8-23   8 Performance Surfaces and Optimum Points  One strong minimum occurs at  x2 of   0.55 0.55  about these two points we obtain:  = F x   =  –  –  T  x1  . If we perform the second-order Taylor series expansion   0.42  0.42  T  , and the other at   F1 x   =  F x1    +    F x   T  x1– x    +  1 --- x x1– 2  T  2  F x   x1– x    x x1  =  =  2.93  +   1 --- x  2   –  8.42 – 0.42  0.42 – 8.42      x  –  0.42 – 0.42       .  x x1 =  T    0.42 – 0.42  If we simplify this expression we find  F1 x   =  4.49  –  –  3.7128  3.7128 x  +  1 ---xT 8.42 2 0.42  –  0.42 – 8.42  x  .  Repeating this process for    results in  x2  F2 x   =  7.41  –  11.781 11.781  –  x  +  1 – ---xT 14.71 6.71 2 14.71  6.71  –  x  .  The original function and the two approximations are plotted in the follow- ing figures.  Check the Taylor series expansions at other points using the Neural Net- work Design Demonstration Vector Taylor Series  nnd8ts2 .  2  1  0  -1  -2 -2  12  8  4  0 2  8-24  1  0  -1  0  -1  -2  -2  2  1  -1  0  1  2  Figure P8.2  Function   F x    for Problem P8.2   Solved Problems  12  8  4  0 2  12  8  4  0 2  1  1  -1  -2 -2  2  1  0  2  1  0  -1  -2 -2  -1  0  1  2  0  -1  0  -1  -2  -2  2  1  Figure P8.3  Function   F1 x    for Problem P8.2  -1  0  1  2  0  -1  0  -1  -2  -2  2  1  Figure P8.4  Function   F2 x    for Problem P8.2  P8.3 For the function   F x    given below, find the equation for the line   that is tangent to the contour line at   x  =  T  .  0 0  8  F x   =  x1+ 2  2  +   5 1  x1–  2– x2  2  F x   To solve this problem we can use the directional derivative. What is the de- rivative of   along a line that is tangent to a contour line? Since the con- tour is a line along which the function does not change, the derivative of  F x   should be zero in the direction of the contour. So we can get the equa- tion for the tangent to the contour line by setting the directional derivative  equal to zero.  First we need to find the gradient:  8-25   8 Performance Surfaces and Optimum Points    F x   =    x1+ 2 2  10 1  +   x1– 10 1 2– –  x2  x1–  2– x2 2x2      1–    =  6–  –  20x2  +  +  12x1 + 20x1x2  2 10x2 +  3 20x2  .  If we evaluate this at   x  =  T  0 0  , we obtain    F x    =  .  6– 0  pT F x  ----------------------   p  .  Now recall that the equation for the derivative of  vector    is  p  F x    in the direction of a   Therefore if we want the equation for the line that passes through  x ator of the directional derivative in the direction of    and along which the derivative is zero, we can set the numer-   to zero:  0 0  =  T  x  where   x  =  x  x–  . For this case we have  x T F x      0=  ,  xT  6– 0  0=  , or   x1  0=  .  This result is illustrated in Figure P8.5.  2  1  0  -1  -2 -2  12  8  4  0 2  8-26  1  0  -1  0  -1  -2  -2  2  1  -1  0  1  2  Figure P8.5  Plot of   F x    for Problem P8.3   Solved Problems  P8.4 Consider the following fourth-order polynomial:  F x   =  x4  –  2x2  –  +  2x  +  4  .  2 ---x3 3  Find any stationary points and test them to see if they are minima.  To find the stationary points we set the derivative of   F x    to zero:  F x   =  4x3  2x2  –  4x–  2+  =  0  .  d xd  We can use MATLAB to find the roots of this polynomial:  coef=[4 -2 -4 2];  stapoints=roots coef ;  stapoints’  ans =  1.0000   -1.0000    0.5000  Now we need to check the second derivative at each of these points. The sec- ond derivative of   F x    is  2  d x2 d  F x   =  12x2  4x–  4–  .  If we evaluate this at each of the stationary points we find      2  d x2 d  F 1     4=         2  d x2 d  F 1–      12=         2  d x2 d  F 0.5      3–=   .  Therefore we should have strong local minima at 1 and -1  since the second  derivatives were positive , and a strong local maximum at 0.5  since the  second derivative was negative . To find the global minimum we would  have to evaluate the function at the two local minima:    F 1   =  4.333   F 1–       =  1.667    .  Therefore the global minimum occurs at -1. But are we sure that this is a  global minimum? What happens to the function as  ? In  this case, because the highest power of   has a positive coefficient and is  x  , the function goes to  an even power    at both limits. So we can safely   say that the global minimum occurs at -1. The function is plotted in Figure  P8.6.  –     or   x4  x  x  8  8-27   8 Performance Surfaces and Optimum Points  F x     =  x4  –  2x2  –  +  2x  +  4  2 ---x3 3  8  6  4  2  0 -2  -1  0  1  2  Figure P8.6  Graph of   F x    for Problem P8.4  P8.5 Look back to the function of Problem P8.2. This function has three   stationary points:  x1  =  – 0.41878 0.41878  ,   x2  =  – 0.134797 0.134797  ,   x3  =  0.55358 – 0.55358  .  Test whether or not any of these points could be local minima.  From Problem P8.2 we know that the Hessian matrix for the function is  2  F x   =    12 x2 12 x2    2 x1– 2 x1–  –  8+  –    12 x2  12 x2  x1– x1–  2 8+ 2  .  To test the definiteness of this matrix we can check the eigenvalues. If the  eigenvalues are all positive, the Hessian is positive definite, which guaran- tees a strong minimum. If the eigenvalues are nonnegative, the Hessian is  positive semidefinite, which is consistent with either a strong or a weak  minimum. If one eigenvalue is positive and the other eigenvalue is nega- tive, the Hessian is indefinite, which would signal a saddle point.  If we evaluate the Hessian at   x1  , we find  2  F x1    =  8.42 0.42 –  0.42 – 8.42  .  The eigenvalues of this matrix are  therefore   x1   must be a strong minimum point.  1  =  8.84  ,   2  =  8.0  ,  8-28   Solved Problems  If we evaluate the Hessian at   x2  , we find  The eigenvalues of this matrix are  2  F x2    =  0.87 7.13 7.13 0.87  .  1  –=  6.26  ,   2  =  8.0  ,  x2  therefore   must be a saddle point. In one direction the curvature is neg- ative, and in another direction the curvature is positive. The negative cur- vature is in the direction of the first eigenvector, and the positive curvature  is in the direction of the second eigenvector. The eigenvectors are  z1  =   and   z2  =  1 1–  .  1 1   Note that this is consistent with our previous discussion of this function  on page 8-8.   If we evaluate the Hessian at   x3  , we find  2  F x3    =  14.7 6.71 –  6.71 – 14.7  .  The eigenvalues of this matrix are  1  =  21.42  ,   2  =  8.0  ,  therefore   x3   must be a strong minimum point.  Check these results using the Neural Network Design Demonstration Vector  Taylor Series  nnd8ts2 .  8  P8.6 Let’s apply the concepts in this chapter to a neural network prob- lem. Consider the linear network shown in Figure P8.7. Suppose  that the desired inputs outputs for the network are      p1  2=      0.5= t1            p2  1–=      0= t2      .  Sketch the following performance index for this network:  F x   =  – t1  a1 x   2  +  – t2  a2 x   2  .  8-29   8 Performance Surfaces and Optimum Points  Input  w  p  Linear Neuron  cid:0   cid:0  cid:0  Σ  cid:0  cid:0  n  cid:0  b  a  1  a = purelin  wp + b   Figure P8.7  Linear Network for Problem P8.6  The parameters of this network are  ter vector  w   and   b  , which make up the parame-  We want to sketch the performance index  . First we will show that the  performance index is a quadratic function. Then we will find the eigenvec- tors and eigenvalues of the Hessian matrix and use them to sketch the con- tour plot of the function.  F x   Begin by writing   F x    as an explicit function of the parameter vector   x  :  x  =  .  w b  F x   =  2 e1  2+ e2  ,    e1  =  t1  –    wp1  b+          e2  =  t2  –    wp2  b+      .  This can be written in matrix form:  where  where  F x   =  eTe  ,  e  =  t  –  x  =  t Gx –  .  p1 1 p2 1  The performance index can now be rewritten:  F x   =  t Gx –  T t Gx  –    =  tTt  –  2tTGx  +  xTGTGx  .  8-30   Solved Problems  If we compare this with Eq.  8.35 :  F x   =  1 ---xTAx dTx 2  +  +  c  ,  we can see that the performance index for this linear network is a quadrat- ic function, with  c  =  tTt  ,   d  =  2– GTt  , and   A  =  2GTG  .  The gradient of the quadratic function is given in Eq.  8.38 :    F x   =  Ax d+  =  2GTGx 2– GTt  .  The stationary point  also the center of the function contours  will occur  where the gradient is equal to zero:  x  =  A 1– d  –  =  GTG   1– GTt  .  G  =  p1 1 p2 1  =  2 1 1– 1   and   t  =  0.5 0  For   we have  x  =  GTG   1– GTt  =  1–  5 1 1 2  1 0.5  =  0.167 0.167  .   Therefore the optimal network parameters are   w  =  0.167   and   b  =  0.167  .    The Hessian matrix of the quadratic function is given by Eq.  8.39 :  8  2  F x   =  A  =  2GTG  =  10 2 2 4  .  To sketch the contour plot we need the eigenvectors and eigenvalues of the  Hessian. For this case we find        1  =  10.6      z1  =  ,     2  3.4=     z2  =  1 0.3                  0.3 1–          .  Therefore we know that   is a strong minimum. Also, since the first eigen- value is larger than the second, we know that the contours will be elliptical  and that the long axis of the ellipses will be in the direction of the second       x  8-31   8 Performance Surfaces and Optimum Points  eigenvector. The contours will be centered at  Figure P8.8.  x  . This is demonstrated in   2  1  0  -1  -2 -2  6  3  0 2  1  0  -1  0  -1  -2  -2  2  1  -1  0  1  2  Figure P8.8  Graph of Function for Problem P8.6  P8.7 There are quadratic functions that do not have stationary points.   This problem illustrates one such case. Consider the following  function:  F x   =  1 1– x  +  1 ---xT 1 1 2 1 1  x  .  Sketch the contour plot of this function.  As with Problem P8.6, we need to find the eigenvalues and eigenvectors of  the Hessian matrix. By inspection of the quadratic function we see that the  Hessian matrix is  2  F x   =  A  =  1 1 1 1  .   8.63   The eigenvalues and eigenvectors are        1  0=     z1  =  ,     2  2=      z2  =      1 1–                          .  1 1  Notice that the first eigenvalue is zero, so there is no curvature along the  first eigenvector. The second eigenvalue is positive, so there is positive cur- vature along the second eigenvector. If we had no linear term in  , the  plot of the function would show a stationary valley, as in Figure 8.9. In this  case we must find out if the linear term creates a slope in the direction of  the valley  the direction of the first eigenvector .  F x   8-32   Solved Problems  The linear term is  From Eq.  8.36  we know that the gradient of this term is  Flin x   =  1 1– x  .    Flin x   =  ,  1 1–  which means that the linear term is increasing most rapidly in the direc- tion of this gradient. Since the quadratic term has no curvature in this di- rection, the overall function will have a linear slope in this direction.  Therefore   will have positive curvature in the direction of the second  eigenvector and a linear slope in the direction of the first eigenvector. The  contour plot and the 3-D plot for this function are given in Figure P8.9.  F x   2  1  0  -1  -2 -2  3  0  -3  2  1  0  -1  0  -1  -2  -2  2  1  -1  0  1  2  Figure P8.9  Falling Valley Function for Problem P8.7  Whenever any of the eigenvalues of the Hessian matrix are zero it is impos- sible to solve for the stationary point of the quadratic function using  8  x  –=  A 1– d  ,  since the Hessian matrix does not have an inverse. This lack of an inverse  could mean that we have a weak minimum point, as illustrated in Figure  8.9, or that there is no stationary point, as this example shows.  8-33   8 Performance Surfaces and Optimum Points  Epilogue  Performance learning is one of the most important classes of neural net- work learning rules. With performance learning, network parameters are  adjusted to optimize network performance. In this chapter we have intro- duced tools that we will need to understand performance learning rules. Af- ter reading this chapter and solving the exercises, you should be able to:  i. Perform a Taylor series expansion and use it to approximate a func-  tion.  ii. Calculate a directional derivative.  iii. Find stationary points and test whether they could be minima.  iv. Sketch contour plots of quadratic functions.  We will be using these concepts in a number of succeeding chapters, includ- ing the chapters on performance learning  9–14 , the radial basis network  chapter  17  and the chapters on stability and Hopfield networks  20–21 .  In the next chapter we will build on the concepts we have covered here, to  design algorithms that will optimize performance functions. Then, in suc- ceeding chapters, we will apply these algorithms to the training of neural  networks.  8-34   Further Reading  Further Reading  [Brog91]   W. L. Brogan, Modern Control Theory, 3rd Ed., Englewood  Cliffs, NJ: Prentice-Hall, 1991.  This is a well-written book on the subject of linear systems.  The first half of the book is devoted to linear algebra. It also  has good sections on the solution of linear differential equa- tions and the stability of linear and nonlinear systems. It  has many worked problems.  [Gill81]  P. E. Gill, W. Murray, and M. H. Wright, Practical Optimi- zation, New York: Academic Press, 1981.  As the title implies, this text emphasizes the practical im- plementation of optimization algorithms. It provides moti- vation for the optimization methods, as well as details of  implementation that affect algorithm performance.  [Himm72]  D. M. Himmelblau, Applied Nonlinear Programming, New  York: McGraw-Hill, 1972.  This is a comprehensive text on nonlinear optimization. It  covers both constrained and unconstrained optimization  problems. The text is very complete, with many examples  worked out in detail.  [Scal85]   L. E. Scales, Introduction to Non-Linear Optimization, New  York: Springer-Verlag, 1985.  A very readable text describing the major optimization al- gorithms, this text emphasizes methods of optimization  rather than existence theorems and proofs of convergence.  Algorithms are presented with intuitive explanations,  along with illustrative figures and examples. Pseudo-code  is presented for most algorithms.  8  8-35   8 Performance Surfaces and Optimum Points  Exercises  E8.1 Consider the following scalar function:  F x   =  1 3 ---x– 4  -------------------------- 1 x3 ---– 2  .  i. Find the second-order Taylor series approximation for   F x    about   ii. Find the second-order Taylor series approximation for   F x    about   the point   x  0.5–=  .  the point   x  =  1.1  .  iii. Plot   F x    and the two approximations and discuss their accuracy.  E8.2 Consider the following function of two variables:  F x   =  e    2 2x1  +  2 2x2  +  x1  –  5x2  +  10    .  i. Find the second-order Taylor series approximation for   F x    about   the point   x  =  T  .  0 0  ii. Find the stationary point for this approximation.  iii. Find the stationary point for   is simply a quadratic function.   F x   .  Note that the exponent of   F x      » 2 + 2 ans =       4  iv. Explain the difference between the two stationary points.  Use   MATLAB to plot the two functions.   E8.3 For the following functions find the first and second directional derivatives   from the point   T  1 1   in the direction   p  =  T  .  1– 1  =  x 7 2 ---x1 2  2 5x1  9 2 ---x1 2  1 2  --- 7x1 2 2 x1  +  i.  F x   =  –  6x1x2  2– x2  ii.  F x   =  –  6x1x2  +  2 5x2  +  4x1  +  4x2  iii.  F x   =  –  2x1x2  +  2 3x2  +  2x1  x2–  iv.  F x   –=  +  12x1x2  –  2 2x2    v.  F x   =  x1x2  +  2 x2  +  3x1  +  3x2  8-36   Exercises  vi.  F x   =  –  3x1x2  +  –  4x1  +  4x2  vii.  F x   =  –  2x1x2  +  +  x1  –  2x2  viii.  F x   =  +  2x1x2  +  4x1  +  4x2  ix.  F x   =  +  4x1x2  +  5x1  x.  F x   =  2x1x2  +  x1  +  x2  1 2 ---x1 2 1 2 ---x1 2 3 2 ---x1 2  3 2 ---– x1 2 2 2x1  –  1 2 ---x2 2  2 2x2  +  3 2 ---x2 2 1 2 ---x2 2  +  » 2 + 2 ans =       4  » 2 + 2 ans =       4  E8.4 For the following function,  F x   =  x4  –  1 ---x2 2  1+  ,  i.  find the stationary points,  ii. test the stationary points to find minimum and maximum points,   and  iii. plot the function using MATLAB to verify your answers.  E8.5 Consider the following function of two variables:  F x   =    x1  x2+  4  –  12x1x2  +  x1  +  x2  +  1  .  i. Verify that the function has three stationary points at   x1  =  – –  0.6504 0.6504  ,   x2  =  0.085 0.085  ,   x3  =  0.5655 0.5655  .  ii. Test the stationary points to find any minima, maxima or saddle   points.  iii. Find the second-order Taylor series approximations for the function   at each of the stationary points.  iv. Plot the function and the approximations using MATLAB.  8  E8.6 For the functions of Exercise E8.3:  i.  find the stationary points,  ii. test the stationary points to find minima, maxima or saddle points,  iii. provide rough sketches of the contour plots, using the eigenvalues   8-37   8 Performance Surfaces and Optimum Points  » 2 + 2 ans =       4  and eigenvectors of the Hessian matrices, and  iv. plot the functions using MATLAB to verify your answers.  E8.7 Consider the following quadratic function:  F x   =  1 ---xT 1 2 3–  3– 1  x  +  4 4– x  +  2  .  i. Find the gradient and Hessian matrix for   F x   .  ii. Sketch the contour plot for   F x   .   iii. Find the directional derivative of   F x    at the point   x0  =  0 0  T   in   the direction   p  =  T  .  1 1  iv. Is your answer to part iii. consistent with your contour plot of part   ii.? Explain.  E8.8 Repeat Exercise E8.7 with the following quadratic function:  E8.9 Consider the following function:  F x   =  1 ---xT 3 2 2–  2– 0  x  +  4 4 x  +  2  .  F x   =    1  +  x1  +  x2  2  +  1 ---x 4  4  1  .  i. Find the quadratic approximation to   F x    about the point   x0  =  1 0  T  ii. Sketch the contour plot of the quadratic approximation in part i.  E8.10 Consider the following function:  F x   =  +  2x1x2  +  3 x2  +  4x1  +  4x2  .  3 ---x 2  2  1  i. Find the quadratic approximation to   F x    about the point   ii. Locate the stationary point of the quadratic approximation you   x0  =  1 0  T  .  found in part i.  8-38   Exercises  iii. Is the answer to part ii a minimum of   F x   ?  E8.11 Consider the following function:  F x   =  x1x2  x1–  +  2x2  .  i. Locate any stationary points.  ii. For each answer to part i., determine, if possible, whether the sta- tionary point is a minimum point, a maximum point, or a saddle  point.  iii. Find the directional derivative of the function at the point   x0  =  1– 1  T   in the direction   p  =  T  .  1– 1  E8.12 Consider the following function:  F x   =  2 x1  +  2x1x2  +  2 x2  +    x1  x2–  3  .  i. Find the quadratic approximation to   F x    about the point   x0  =  2 1  T  .  ii. Sketch the contour plot of the quadratic approximation.  E8.13 Recall the function in Problem P8.7. For that function there was no station-  vec-  vector that   ary point. It is possible to modify the function, by changing only the  tor, so that a stationary point will exist. Find a new nonzero  will create a weak minimum.  d  d  8  8-39   Objectives  9 Performance Optimization  Objectives Theory and Examples Steepest Descent  Stable Learning Rates Minimizing Along a Line  Newton’s Method Conjugate Gradient  Summary of Results Solved Problems Epilogue Further Reading Exercises  9-1 9-2 9-2 9-6 9-8 9-10 9-15 9-21 9-23 9-37 9-38 9-39  Objectives  We initiated our discussion of performance optimization in Chapter 8.  There we introduced the Taylor series expansion as a tool for analyzing the  performance surface, and then used it to determine conditions that must be  satisfied by optimum points. In this chapter we will again use the Taylor  series expansion, in this case to develop algorithms to locate the optimum  points. We will discuss three different categories of optimization algorithm:  steepest descent, Newton’s method and conjugate gradient. In Chapters  10–14 we will apply all of these algorithms to the training of neural net- works.  9  9-1   9 Performance Optimization  Theory and Examples  In the previous chapter we began our investigation of performance surfac- es. Now we are in a position to develop algorithms to search the parameter  space and locate minimum points of the surface  find the optimum weights  and biases for a given neural network .  It is interesting to note that most of the algorithms presented in this chap- ter were developed hundreds of years ago. The basic principles of optimiza- tion were discovered during the 17th century, by such scientists and  mathematicians as Kepler, Fermat, Newton and Leibniz. From 1950 on,  these principles were rediscovered to be implemented on “high speed”  in  comparison to the pen and paper available to Newton  digital computers.  The success of these efforts stimulated significant research on new algo- rithms, and the field of optimization theory became recognized as a major  branch of mathematics. Now neural network researchers have access to a  vast storehouse of optimization theory and practice that can be applied to  the training of neural networks.  The objective of this chapter, then, is to develop algorithms to optimize a  performance index  . For our purposes the word “optimize” will mean  . All of the optimization algo- to find the value of  x0 rithms we will discuss are iterative. We begin from some initial guess,  and then update our guess in stages according to an equation of the form   that minimizes   F x  x  F x   ,   or  xk  1+  =  xk kpk  +  ,  x k  =    xk  1+  xk–    =  kpk  ,   9.1    9.2   pk   represents a search direction, and the positive scalar    is the learning rate, which determines the length of the step.  where the vector  k The algorithms we will discuss in this chapter are distinguished by the  choice of the search direction,  ties. There are also a variety of ways to select the learning rate,  will discuss several of these.  . We will discuss three different possibili- , and we   k  pk  Steepest Descent  When we update our guess of the optimum  minimum  point using Eq.   9.1 , we would like to have the function decrease at each iteration. In other  words,  F xk    F xk     .  1+   9.3   9-2   Steepest Descent  How can we choose a direction,  rate,  lor series expansion  see Eq.  8.9   of   , so that for sufficiently small learning  , we will move “downhill” in this way? Consider the first-order Tay-   about the old guess   k  pk  :  F x   xk  F xk   1+    =  F xk   +  x k   F xk     +  where   gk   is the gradient evaluated at the old guess   ,  T x k gk xk  :  F xk   For  of Eq.  9.4  must be negative:   to be less than   1+    F xk    , the second term on the right-hand side   We will select an    that is small, but greater than zero. This implies:  k  gk      F x   .  x xk=  T x k gk  =  kgk  Tpk  0  .  Tpk gk  0  .  Tpk gk  pk  =  g– k  .  xk  1+  =  xk kgk  –  .  xk kgk  –  .  9-3  Descent Direction  pk   that satisfies this equation is called a descent direction. The  Any vector  function must go down if we take a small enough step in this direction. This  brings up another question. What is the direction of steepest descent?  In  what direction will the function decrease most rapidly?  This will occur  when  is most negative.  We assume that the length of   does not change, only  the direction.  This is an inner product between the gradient and the direc- tion vector. It will be most negative when the direction vector is the nega- tive of the gradient.  Review our discussion of directional derivatives on  page 8-6.  Therefore a vector that points in the steepest descent direction is  pk  Steepest Descent  Using this in the iteration of Eq.  9.1  produces the method of steepest de- scent:  9  Learning Rate  For steepest descent there are two general methods for determining the  F x  learning rate,    k with respect to   at each iteration. In this case we are minimizing along  the line  . One approach is to minimize the performance index  k   9.4    9.5    9.6    9.7    9.8    9.9    9.10    9.11    9 Performance Optimization  The other method for selecting  or to use variable, but predetermined, values  e.g.,  cuss the choice of    in more detail in the following examples.   is to use a fixed value  e.g.,   1 k  k  k  =  k  =  k   ,   . We will dis-  0.02  2 2+  Let’s apply the steepest descent algorithm to the following function,  starting from the initial guess  F x   =  2 x1  +  2 25x2  ,  x0  =  .  0.5 0.5  The first step is to find the gradient:  If we evaluate the gradient at the initial guess we find    F x   =   x1  x2  F x   F x   =  2x1 50x2  .  g0  =    F x   =  x x0=  .  1 25  Assume that we use a fixed learning rate of  the steepest descent algorithm would be    =  0.01  . The first iteration of   x1  =  x0 g0  –  =  0.5 0.5  –  0.01 1 25  =  0.49 0.25  .  The second iteration of steepest descent produces  x2  =  x1 g1  –  =  0.49 0.25  –  0.01 0.98 12.5  =  0.4802 0.125  .  If we continue the iterations we obtain the trajectory illustrated in Figure  9.1.   9.12    9.13    9.14    9.15    9.16    9.17   9-4   Steepest Descent  0.5  1  0  -0.5  -1 -1  0.5  1  0  -0.5  -1 -1  9-5  -0.5  0  0.5  1  Figure 9.1  Trajectory for Steepest Descent with     =  0.01  Note that the steepest descent trajectory, for small learning rate, follows a  path that is always orthogonal to the contour lines. This is because the gra- dient is orthogonal to the contour lines.  See the discussion on page 8-6.   How would a change in the learning rate change the performance of the al- gorithm? If we increase the learning rate to  , we obtain the tra- jectory illustrated in Figure 9.2. Note that the trajectory now oscillates. If  we make the learning rate too large the algorithm will become unstable;  the oscillations will increase instead of decaying.  0.035    =  -0.5  0  0.5  1  Figure 9.2  Trajectory for Steepest Descent with     =  0.035  We would like to make the learning rate large, since then we will be taking  large steps and would expect to converge faster. However, as we can see  from this example, if we make the learning rate too large the algorithm will  become unstable. Is there some way to predict the maximum allowable  learning rate? This is not possible for arbitrary functions, but for quadratic  functions we can set an upper limit.  9   9 Performance Optimization  Stable Learning Rates Suppose that the performance index is a quadratic function:  From Eq.  8.38  the gradient of the quadratic function is  F x   =  1 ---xTAx dTx 2  +  +  c  .    F x   =  Ax d+  .  If we now insert this expression into our expression for the steepest descent  algorithm  assuming a constant learning rate , we obtain  or  xk  1+  =  xk gk  –  =  xk  Axk d+  –      xk  1+  =  I A–  xk d–  .  I A–  This is a linear dynamic system, which will be stable if the eigenvalues of  the matrix   are less than one in magnitude  see [Brog91] . We can  express the eigenvalues of this matrix in terms of the eigenvalues of the  Hessian matrix   ues and eigenvectors of the Hessian matrix. Then   be the eigenval-  1 2   n  z1 z2   zn  . Let    and   A                      I A–  zi  =  zi Azi  –  =  zi izi  –  =  1 i –  zi  .   9.22    are the same as the eigenvectors of  Therefore the eigenvectors of  A . Our condition for the sta- 1 i – bility of the steepest descent algorithm is then  , and the eigenvalues of   I A–  I A–   are         If we assume that the quadratic function has a strong minimum point, then  its eigenvalues must be positive numbers. Eq.  9.23  then reduces to  Since this must be true for all the eigenvalues of the Hessian matrix we  have  The maximum stable learning rate is inversely proportional to the maxi- mum curvature of the quadratic function. The curvature tells us how fast  the gradient is changing. If the gradient is changing too fast we may jump   9-6   9.18    9.19    9.20    9.21    9.23    9.24    9.25   1 i –    1  .   2 ---- i  .      2 ----------- max  .   Steepest Descent  past the minimum point so far that the gradient at the new location will be  larger in magnitude  but opposite direction  than the gradient at the old lo- cation. This will cause the steps to increase in size at each iteration.  2 2+  Let’s apply this result to our previous example. The Hessian matrix for that  quadratic function is  A  =  2 0 0 50  .  The eigenvalues and eigenvectors of   A   are        1  2=      z1  =    2  50=      z2  =                   1 0              .  0 1  Therefore the maximum allowable learning rate is      2 ----------- max  =  2 ------ 50  =  0.04  .   9.26    9.27    9.28   This result is illustrated experimentally in Figure 9.3, which shows the  steepest descent trajectories when the learning rate is just below       , the maximum stable value.    and just above    0.041  0.039    =  =  0.5  1  0  -0.5  -1 -1  0.5  1  0  -0.5  -1 -1  -0.5  0  0.5  1  -0.5  0  0.5  1  Figure 9.3  Trajectories for     =  0.039    left  and     =  0.041    right .  This example has illustrated several points. The learning rate is limited by  the largest eigenvalue  second derivative  of the Hessian matrix. The algo- rithm tends to converge most quickly in the direction of the eigenvector cor- responding to this largest eigenvalue, and we don’t want to overshoot the  minimum point by too far in that direction.  Note that in our examples the  initial step is almost parallel to the  .  However, the al- gorithm will tend to converge most slowly in the direction of the eigenvec-   axis, which is   z2  x2  9  9-7   9 Performance Optimization  tor that corresponds to the smallest eigenvalue   end it is the smallest eigenvalue, in combination with the learning rate,  that determines how quickly the algorithm will converge. When there is a  great difference in magnitude between the largest and smallest eigenval- ues, the steepest descent algorithm will converge slowly.   for our example . In the   z1  To experiment with steepest descent on this quadratic function, use the Neu- ral Network Design Demonstration Steepest Descent for a Quadratic   nnd9sdq .  Minimizing Along a Line Another approach for selecting the learning rate is to minimize the perfor- mance index with respect to    k to minimize   at each iteration. In other words, choose   k  F xk kpk   +    .   9.29   To do this for arbitrary functions requires a line search, which we will dis- cuss in Chapter 12. For quadratic functions it is possible to perform the lin- ear minimization analytically. The derivative of Eq.  9.29  with respect to  k  , can be shown to be  , for quadratic   F x   d kd  F xk kpk   +    =    F x   T  pk kpk  +  T F x  2  x xk=  pk  .  x xk=   9.30   If we set this derivative equal to zero and solve for   k  , we obtain  k  =  –     T  F x  pk  --------------------------------------------- T F x  pk pk 2  x xk=  x xk=  =  –     Tpk gk ------------------ TAkpk pk  ,  where   Ak   is the Hessian matrix evaluated at the old guess   xk  :   For quadratic functions the Hessian matrix is not a function of   k  .   2 2+  Let’s apply steepest descent with line minimization to the following qua- dratic function:  Ak    2  F x   .  x xk=  F x   =  1 ---xT 2 1 2 1 2  x  ,   9.31    9.32    9.33   starting from the initial guess  9-8    9.34    9.35    9.36   Steepest Descent  The gradient of this function is  x0  =  0.8 0.25  –  .    F x   =  2x1 x2+ + 2x2 x1  .  The search direction for steepest descent is the negative of the gradient.  For the first iteration this will be  p0  =  g– 0  =  –  F x   =  x x0=  1.35 – 0.3–  .  From Eq.  9.31 , the learning rate for the first iteration will be  0  =  –  1.35 0.3  – 1.35 0.3–  ---------------------------------------------------------------- 1.35 – 0.3–  2 1 1 2  0.3–  1.35  –  =  0.413  .   9.37   The first step of steepest descent will then produce  x1  =  x0 0g0  –  =  0.8 0.25  –  –  0.413 1.35 0.3  =  0.24 0.37 –  .   9.38   The first five iterations of the algorithm are illustrated in Figure 9.4.  Note that the successive steps of the algorithm are orthogonal. Why does  this happen? First, when we minimize along a line we will always stop at  a point that is tangent to a contour line. Then, since the gradient is orthog- onal to the contour line, the next step, which is along the negative of the  gradient, will be orthogonal to the previous step.   We can show this analytically by using the chain rule on Eq.  9.30 :  9  d kd  F xk kpk   +    =  F xk   1+    =    F x   T  d kd  x xk  =  1+  d kd    xk kpk  +     9.39   =    F x   T  pk  =  T pk . gk  1+  x xk  =  1+  9-9   9 Performance Optimization  Therefore at the minimum point, where this derivative is zero, the gradient  is orthogonal to the previous search direction. Since the next search direc- tion is the negative of this gradient, the consecutive search directions must  be orthogonal.  Note that this result implies that when minimizing in any  direction, the gradient at the minimum point will be orthogonal to the  search direction, even if we are not using steepest descent. We will use this  result in our discussion of conjugate directions.   Contour Plot  1  0.5  2 x  0  -0.5  9-10  -1 -1  -0.5  0 x1  0.5  1  Figure 9.4  Steepest Descent with Minimization Along a Line  To experiment with steepest descent with minimization along a line, use the  Neural Network Design Demonstration Method Comparison  nnd9mc .  Later in this chapter we will find that we can improve performance if we  adjust the search directions, so that instead of being orthogonal they are  conjugate.  We will define this term later.  If conjugate directions are used  the function can be exactly minimized in at most   is the  dimension of  .  There are certain types of quadratic functions that are  minimized in one step by the steepest descent algorithm. Can you think of  such a function? How is its Hessian matrix characterized?    steps, where   x  n  n  Newton’s Method  The derivation of the steepest descent algorithm was based on the first-or- der Taylor series expansion  Eq.  9.4  . Newton’s method is based on the  second-order Taylor series:  F xk   1+    =  F xk xk   +   F xk     +  Txk gk  +  1 ---xk 2  TAkxk  .   9.40   The principle behind Newton’s method is to locate the stationary point of  . If we use Eq.  8.38  to take the gra- this quadratic approximation to  dient of this quadratic function with respect to   and set it equal to zero,  we find  F x   xk    9.41    9.42    9.43    9.44    9.45    9.46    9.47   Newton’s Method  Solving for   xk   produces  Newton’s Method  Newton’s method is then defined:  gk Akxk  +  0=  .  xk  =  1–– Ak  gk  .  xk  1+  =  xk Ak  –  1– gk  .  F x   =  2 x1  +  2 25x2  .  2 2+  To illustrate the operation of Newton’s method, let’s apply it to our previous  example function of Eq.  9.12 :  The gradient and Hessian matrices are    F x   =   x1  x2  F x   F x   =  2x1 50x2  ,   2  F x   =  2 0 0 50  .  If we start from the same initial guess  x0  =  ,  0.5 0.5  the first step of Newton’s method would be  1–  x1  =  0.5 0.5  –  2 0 0 50  1 25  =  0.5 0.5  –  0.5 0.5  =  .  0 0  This method will always find the minimum of a quadratic function in one  step. This is because Newton’s method is designed to approximate a func- tion as quadratic and then locate the stationary point of the quadratic ap- proximation. If the original function is quadratic  with a strong minimum   it will be minimized in one step. The trajectory of Newton’s method for this  problem is given in Figure 9.5.  If the function   is not quadratic, then Newton’s method will not gener- ally converge in one step. In fact, we cannot be sure that it will converge at  all, since this will depend on the function and the initial guess.  F x   9  9-11   9 Performance Optimization  0.5  1  0  -0.5  -1 -1  9-12  -0.5  0  0.5  1  Figure 9.5  Trajectory for Newton’s Method  2 2+  Recall the function given by Eq.  8.18 :  F x   =    x2  x1–  4  +  8x1x2  x1–  +  x2  +  3  .   9.48   We know from Chapter 8  see Problem P8.5  that this function has three  stationary points:  x1  =  0.41878 – 0.41878  ,   x2  =  0.134797 – 0.134797  ,   x3  =  0.55358 0.55358 –  .   9.49   The first point is a strong local minimum, the second point is a saddle point,  and the third point is a strong global minimum.  T  =  x0  1.5 0  If we apply Newton’s method to this problem, starting from the initial  guess  graph on the left-hand side of the figure is a contour plot of the original  function. On the right we see the quadratic approximation to the function  at the initial guess.  , our first iteration will be as shown in Figure 9.6. The   The function is not minimized in one step, which is not surprising since the  function is not quadratic. However, we do take a step toward the global  minimum, and if we continue for two more iterations the algorithm will  converge to within 0.01 of the global minimum. Newton’s method converges  quickly in many applications because analytic functions can be accurately  approximated by quadratic functions in a small neighborhood of a strong  minimum. So as we move closer to the minimum point, Newton’s method  will more accurately predict its location. In this case we can see that the  contour plot of the quadratic approximation is similar to the contour plot of  the original function near the initial guess.   Newton’s Method  2  1  0  -1  -2 -2  2  1  0  -1  -2 -2  2  1  0  -1  -2 -2  2  1  0  -1  -2 -2  -1  0  1  2  -1  0  1  2  Figure 9.6  One Iteration of Newton’s Method from   x0  =  1.5 0  T  T  0  =  1.5–  In Figure 9.7 we see one iteration of Newton’s method from the initial guess  x0 . In this case we are converging to the local minimum. Clear- ly Newton’s method cannot distinguish between a local minimum and a glo- bal minimum, since it approximates the function as a quadratic, and the  quadratic function can have only one minimum. Newton’s method, like  steepest descent, relies on the local features of the surface  the first and  second derivatives . It cannot know the global character of the function.  -1  0  1  2  -1  0  1  2  Figure 9.7  One Iteration of Newton’s Method from   x0  =  1.5–  0  T  T  =  0.75 0.75  In Figure 9.8 we see one iteration of Newton’s method from the initial guess  x0 . Now we are converging toward the saddle point of the  function. Note that Newton’s method locates the stationary point of the  quadratic approximation to the function at the current guess. It does not  distinguish between minima, maxima and saddle points. For this problem  the quadratic approximation has a saddle point  indefinite Hessian ma-  9  9-13   9 Performance Optimization  trix , which is near the saddle point of the original function. If we continue  the iterations, the algorithm does converge to the saddle point of   .  F x   -1  0  1  2  -1  0  1  2  Figure 9.8  One Iteration of Newton’s Method from   x0  =  0.75 0.75  T  In each of the cases we have looked at so far the stationary point of the qua- dratic approximation has been close to a corresponding stationary point of  F x  very unpredictable results.   . This is not always the case. In fact, Newton’s method can produce   T  =  1.15 0.75  In Figure 9.9 we see one iteration of Newton’s method from the initial guess  x0 . In this case the quadratic approximation predicts a sad- dle point, however, the saddle point is located very close to the local mini- mum of  . If we continue the iterations, the algorithm will converge to  the local minimum. Notice that the initial guess was actually farther away  from the local minimum than it was for the previous case, in which the al- gorithm converged to the saddle point.  F x   2  1  0  -1  -2 -2  2  1  0  -1  -2 -2  2  1  0  -1  -2 -2  2  1  0  -1  -2 -2  -1  0  1  2  -1  0  1  2  Figure 9.9  One Iteration of Newton’s Method from   x0  =  1.15 0.75  T  9-14   Conjugate Gradient  To experiment with Newton’s method and steepest descent on this function,  use the Neural Network Design Demonstrations Newton’s Method  nnd9nm   and Steepest Descent  nnd9sd .  This is a good place to summarize some of the properties of Newton’s meth- od that we have observed.   While Newton’s method usually produces faster convergence than steepest  descent, the behavior of Newton’s method can be quite complex. In addition  to the problem of convergence to saddle points  which is very unlikely with  steepest descent , it is possible for the algorithm to oscillate or diverge.  Steepest descent is guaranteed to converge, if the learning rate is not too  large or if we perform a linear minimization at each stage.  In Chapter 12 we will discuss a variation of Newton’s method that is well  suited to neural network training. It eliminates the divergence problem by  using steepest descent steps whenever divergence begins to occur.  Another problem with Newton’s method is that it requires the computation  and storage of the Hessian matrix, as well as its inverse. If we compare  steepest descent, Eq.  9.10 , with Newton’s method, Eq.  9.43 , we see that  their search directions will be identical when  Ak  =  1– Ak  =  I  .   9.50   This observation has lead to a class of optimization algorithms know as  quasi-Newton or one-step-secant methods. These methods replace    with a positive definite matrix,  ,which is updated at each iteration with- out matrix inversion. The algorithms are typically designed so that for qua- dratic functions  quadratic functions.  See [Gill81], [Scal85] or [Batt92] for a discussion of  these methods.  .  The Hessian is constant for    will converge to   1– Ak  A 1–  Hk  Hk  Conjugate Gradient  Quadratic Termination  Newton’s method has a property called quadratic termination, which  means that it minimizes a quadratic function exactly in a finite number of  iterations. Unfortunately, it requires calculation and storage of the second  derivatives. When the number of parameters,  , is large, it may be imprac- tical to compute all of the second derivatives.  Note that the gradient has   elements.  This is especially true  n with neural networks, where practical applications can require several  hundred to many thousand weights. For these cases we would like to have  methods that require only first derivatives but still have quadratic termi- nation.   elements, while the Hessian has   n2  n  Recall the performance of the steepest descent algorithm, with linear  searches at each iteration. The search directions at consecutive iterations  were orthogonal  see Figure 9.4 . For quadratic functions with elliptical   9  9-15   9 Performance Optimization  contours this produces a zig-zag trajectory of short steps. Perhaps quadrat- ic search directions are not the best choice. Is there a set of search direc- tions that will guarantee quadratic termination? One possibility is  conjugate directions.  Suppose that we wish to locate the minimum of the following quadratic  function:  Conjugate  pk A set of vectors   nite Hessian matrix    is mutually conjugate with respect to a positive defi- A   if and only if  F x   =  1 ---xTAx dTx 2  +  +  c  .  TApj pk  0=  .  k  j   9.51    9.52   As with orthogonal vectors, there are an infinite number of mutually con- jugate sets of vectors that span a given  conjugate vectors consists of the eigenvectors of  z1 z2   zn  To see that the eigenvectors are conjugate, replace    and   be the eigenvalues and eigenvectors of the Hessian matrix.   in Eq.  9.52 :  -dimensional space. One set of   1 2   n   with   . Let   A        n              pk  zk  TAzj zk  =  jzk  Tzj  =  0      k  j  ,   9.53   where the last equality holds because the eigenvectors of a symmetric ma- trix are mutually orthogonal. Therefore the eigenvectors are both conju- gate and orthogonal.  Can you find a quadratic function where all  orthogonal vectors are also conjugate?   It is not surprising that we can minimize a quadratic function exactly by  searching along the eigenvectors of the Hessian matrix, since they form the  principal axes of the function contours.  See the discussion on pages 8-13  through 8-19.  Unfortunately this is not of much practical help, since to  find the eigenvectors we must first find the Hessian matrix. We want to  find an algorithm that does not require the computation of second deriva- tives.  It can be shown  see [Scal85] or [Gill81]  that if we make a sequence of ex- p1 p2   pn act linear searches along any set of conjugate directions  ,   then the exact minimum of any quadratic function, with   parameters, will  n be reached in at most   searches. The question is “How can we construct  these conjugate search directions?” First, we want to restate the conjugacy  condition, which is given in Eq.  9.52 , without use of the Hessian matrix.  Recall that for quadratic functions    n          F x   =  Ax d+  ,  2  F x   A=  .   9.54    9.55   9-16   Conjugate Gradient  By combining these equations we find that the change in the gradient at  iteration    is  1+  k  gk  =  gk  1+  gk–  =    Axk  1+  d+   Axk d+ –      =  A xk  ,   9.56   where, from Eq.  9.2 , we have   xk  =    xk  1+  xk–    =  kpk  ,   9.57   and   k   is chosen to minimize   F x    in the direction   pk  .  We can now restate the conjugacy conditions  Eq.  9.52  :  kpk  TApj  =  T Apj xk  =  T pj gk  =  0  k  j  .   9.58   Note that we no longer need to know the Hessian matrix. We have restated  the conjugacy conditions in terms of the changes in the gradient at succes- sive iterations of the algorithm. The search directions will be conjugate if  they are orthogonal to the changes in the gradient.  Note that the first search direction,  tor that is orthogonal to  of conjugate vectors. It is common to begin the search in the steepest de- scent direction:   can be any vec- . Therefore there are an infinite number of sets   , is arbitrary, and   g0  p0  p1  p0  g0–=  .   9.59     g0  Then, at each iteration we need to construct a vector  to  onalization, which we discussed in Chapter 5. It can be simplified  see  [Scal85]  to iterations of the form   that is orthogonal  . It is a procedure similar to Gram-Schmidt orthog-  g1     gk 1–  pk        pk  =  gk–  +  kpk  1–  .   9.60   k  The scalars   can be chosen by several different methods, which produce  equivalent results for quadratic functions. The most common choices  see  [Scal85]  are  due to Hestenes and Stiefel,  k  =  T gk gk 1– --------------------------- T pk gk 1–  1–  ,  k  =  Tgk gk gk  ----------------------- T gk 1–  1–  9-17  9   9.61    9.62    9 Performance Optimization  due to Fletcher and Reeves, and  k  =  T gk gk ----------------------- T gk 1–  1– gk  1–   9.63   due to Polak and Ribiére.  Conjugate Gradient  To summarize our discussion, the conjugate gradient method consists of  the following steps:  1. Select the first search direction to be the negative of the gradient, as in   Eq.  9.59 .  2. Take a step according to Eq.  9.57 , selecting the learning rate   k   to   minimize the function along the search direction. We will discuss gen- eral linear minimization techniques in Chapter 12. For quadratic func- tions we can use Eq.  9.31 .  3. Select the next search direction according to Eq.  9.60 , using Eq.    9.61 , Eq.  9.62 , or Eq.  9.63  to calculate   k  .  4.  If the algorithm has not converged, return to step 2.  2 2+  To illustrate the performance of the algorithm, recall the example we used  to demonstrate steepest descent with linear minimization:  with initial guess  The gradient of this function is  F x   =  1 ---xT 2 1 2 1 2  x  ,  x0  =  0.8 0.25  –  .    F x   =  2x1 x2+ 2x2 + x1  .  As with steepest descent, the first search direction is the negative of the  gradient:  p0  =  g0–  =  –  F x   T  =  x x0=  1.35 – 0.3–  .  9-18   9.64    9.65    9.66    9.67    Conjugate Gradient  From Eq.  9.31 , the learning rate for the first iteration will be  0  =  –  1.35 0.3  1.35 – 0.3–  ---------------------------------------------------------------- 1.35 – 0.3–  2 1 1 2  0.3–  1.35  –  =  0.413  .   9.68   The first step of conjugate gradient is therefore:  x1  =  x0 0p0  +  =  0.8 0.25  –  +  0.413  1.35 – 0.3–  =  0.24 – 0.37  ,   9.69   which is equivalent to the first step of steepest descent with minimization  along a line.  Now we need to find the second search direction from Eq.  9.60 . This re- quires the gradient at   :  x1  g1  =    F x   =  x x1=  2 1 1 2  0.24 0.37 –  =  0.11 0.5–  .   9.70   We can now find   1  :  1  =  Tg1 g1 ----------- Tg0 g0  0.11 0.11 0.5– 0.5– --------------------------------------------  =  1.35 0.3  1.35 0.3  =  0.2621 ---------------- 1.9125  =  0.137  ,   9.71   using the method of Fletcher and Reeves  Eq.  9.62  . The second search di- rection is then computed from Eq.  9.60 :  p1  =  g1–  +  1p0  =  –  0.11 0.5  +  0.137  1.35 – 0.3–  =  0.295 – 0.459  .   9.72   9  From Eq.  9.31 , the learning rate for the second iteration will be  1  =  –  0.11 0.5–  – 0.295 0.459  ------------------------------------------------------------------------ 0.295 – 0.459  2 1 1 2  0.295  0.459  –  =  0.262 ------------- 0.325  =  0.807  .   9.73   9-19   9 Performance Optimization  The second step of conjugate gradient is therefore   x2  =  x1 1p1  +  =  0.24 0.37 –  +  0.807  0.295 – 0.459  =  .  0 0   9.74   As predicted, the algorithm converges exactly to the minimum in two iter- ations  since this is a two-dimensional quadratic function , as illustrated in  Figure 9.10. Compare this result with the steepest descent algorithm, as  shown in Figure 9.4. The conjugate gradient algorithm adjusts the second  search direction so that it will pass through the minimum of the function   center of the function contours , instead of using an orthogonal search di- rection, as in steepest descent.  -0.5  0  0.5  1  Figure 9.10  Conjugate Gradient Algorithm  We will return to the conjugate gradient algorithm in Chapter 12. In that  chapter we will discuss how the algorithm should be adjusted for non-qua- dratic functions.  To experiment with the conjugate gradient algorithm and compare it with  steepest descent, use the Neural Network Design Demonstration Method  Comparison  nnd9mc .  0.5  1  0  -0.5  -1 -1  9-20   Summary of Results  Summary of Results  General Minimization Algorithm  xk  1+  =  xk kpk  +  or  x k  =    xk  1+  xk–    =  kpk  Steepest Descent Algorithm xk  1+  =  xk kgk  –  Where   gk      F x   x xk=  Stable Learning Rate    k  =  , constant       2 ----------- max    1 2   n           Eigenvalues of Hessian matrix   A  Learning Rate to Minimize Along the Line   xk  1+  =  xk kpk  +  k  –=  Tpk gk ---------------- TApk pk         For quadratic functions   After Minimizing Along the Line   xk  1+  =  xk kpk  +  T pk gk  1+  0=  Newton’s Method  9  xk  1+  =  xk Ak  –  1– gk  Where   Ak    2  F x   x xk=  9-21   9 Performance Optimization  Conjugate Gradient Algorithm x k  =  kpk  Learning rate   k   is chosen to minimize along the line   xk  1+  =  xk kpk  +  .  p0  g0–=  pk  =  gk–  +  kpk  1–  k  =  T gk gk 1– --------------------------- T pk gk 1–  1–   or   k  =   or   k  =  Tgk gk gk  ----------------------- T gk 1–  1–  T gk gk ----------------------- T gk 1–  1– gk  1–  Where   gk      F x    and   gk  =  gk  1+  gk–  .  x xk=  9-22   Solved Problems  Solved Problems  P9.1 We want to find the minimum of the following function:  F x   =  5x1  2 6x1x2 –  +  2 5x2  +  4x1  +  4x2  .  i. Sketch a contour plot of this function.  ii. Sketch the trajectory of the steepest descent algorithm on   the contour plot of part  i  if the initial guess is  x0  . Assume a very small learning rate is used.  =  2.5–  1–  T  iii. What is the maximum stable learning rate?  i. To sketch the contour plot we first need to find the Hessian matrix. For  quadratic functions we can do this by putting the function into the stan- dard form  see Eq.  8.35  :  F x   =  1 ---xTAx dTx 2  +  +  c  =  1 ---xT 10 6– 2 6– 10  x  +  4 4 x  .  From Eq.  8.39  the Hessian matrix is  2  F x   =  A  =  10 6– 6– 10  .  The eigenvalues and eigenvectors of this matrix are  1  4=  ,   z1  =  ,   2  16=  ,   z2  =  1 1  .  1 1–  From the discussion on quadratic functions in Chapter 8  see page 8-15  we  know that the function contours are elliptical. The maximum curvature of  F x  , and the minimum  curvature is in the direction of   2   the long axis of the ellipses .   is in the direction of    is larger than   , since   1  z2  z1  Next we need to find the center of the contours  the stationary point . This  occurs when the gradient is equal to zero. From Eq.  8.38  we find  9    F x   =  Ax d+  =  10 6– 6– 10  x  +  4 4  =  .  0 0  Therefore  9-23   9 Performance Optimization  x  =  –  1–  10 6– 6– 10  =  4 4  .  1– 1–  The contours will be elliptical, centered at  tion of   . The contour plot is shown in Figure P9.1.  x  z1  , with long axis in the direc-  ii. We know that the gradient is always orthogonal to the contour line,  therefore the steepest descent trajectory, if we take small enough steps,  will follow a path that is orthogonal to each contour line it intersects. We  can therefore trace the trajectory without performing any computations.  The result is shown in Figure P9.1.  -2  -1  0  Figure P9.1  Contour Plot and Steep. Desc. Trajectory for Problem P9.1  iii. From Eq.  9.25  we know that the maximum stable learning rate for a  quadratic function is determined by the maximum eigenvalue of the Hes- sian matrix:      2 ----------- max  .    2 ------ 16  =  0.125  .  The maximum eigenvalue for this problem is  ity  2  16=  , therefore for stabil-  This result is verified experimentally in Figure P9.2, which shows the  steepest descent trajectories when the learning rate is just below    the maximum stable value.       and just above    0.13  0.12    =  =  0  -1  -2  -3 -3  9-24   Solved Problems  0  -1  -2  0  -1  -2  -3 -3  -2  -1  0  -3 -3  -2  -1  0  Figure P9.2  Trajectories for     =  0.12    left  and     =  0.13    right   P9.2 Consider again the quadratic function of Problem P9.1. Take two  steps of the steepest descent algorithm, minimizing along a line at  each step. Use the following initial condition:  x0  =  0 2–  T  .  In Problem P9.1 we found the gradient of the function to be    F x   =  Ax d+  =  10 6– 6– 10  x  +  .  4 4  If we evaluate this at   x0  , we find  g0  =    F x0    =  Ax0 d+  =  10 6– 6– 10  0 2–  +  =  4 4  16 16–  .  Therefore the first search direction is  p0  =  g– 0  =  16– 16  .  9  To minimize along a line, for a quadratic function, we can use Eq.  9.31 :  0  =  –  Tp0 g0 ---------------- TAp0 p0  16 16–  16– 16  =  –  -------------------------------------------------------- 16– 16  10 6– 6– 10  16–  16  =  –  512 – ------------ 8192  =  0.0625  .  9-25   9 Performance Optimization  Therefore the first iteration of steepest descent will be  x1  =  x0 0g0  –  =  0 2–  –  0.0625 16 16–  =  .  1– 1–  To begin the second iteration we need to find the gradient at   x1  :  g1  =  F x1     =  Ax1 d+  =  10 6– 6– 10  1– 1–  +  =  4 4  .  0 0  Therefore we have reached a stationary point; the algorithm has con- verged. From Problem P9.1 we know that  of this quadratic function. The trajectory is shown in Figure P9.3.   is indeed the minimum point   x1  0  -1  -2  -3 -3  -2  -1  0  Figure P9.3  Steepest Descent with Linear Minimization for Problem P9.2  This is an unusual case, where the steepest descent algorithm located the  minimum in one iteration. Notice that this occurred because the initial  guess was located in the direction of one of the eigenvectors of the Hessian  matrix, with respect to the minimum point. For those cases where every di- rection is an eigenvector, the steepest descent algorithm will always locate  the minimum in one iteration. What would this imply about the eigenval- ues of the Hessian matrix?  P9.3 Recall Problem P8.6, in which we derived a performance index for  a linear neural network. The network, which is displayed again in  Figure P9.4, was to be trained for the following input output pairs:      p1  2=      0.5= t1           p2  1–=      0= t2      The performance index for the network was defined to be  F x   =  – t1  a1 x   2  +  – t2  a2 x   2  ,  9-26   Solved Problems  which was displayed in Figure P8.8.   i. Use the steepest descent algorithm to locate the optimal pa-  rameters for this network  recall that  from the initial guess    0.05  1 1  x0  =  =  .  T  x  =  T  w b   , starting   . Use a learning rate of   ii. What is the maximum stable learning rate?  Input  w  p  Linear Neuron  cid:0   cid:0  cid:0  Σ  cid:0  cid:0  n  cid:0  b  a  1  a = purelin  wp + b   Figure P9.4  Linear Network for Problems P9.3 and P8.6  In Problem P8.6 we found that the performance index could be written   i. in quadratic form:  F x   =  1 ---xTAx dTx 2  +  +  c  ,  where  c  =  tTt  =  0.5 0  =  0.25  ,  0.5 0  d  =  2– GTt  =  2–  2 1– 1 1  0.5 0  =  ,  2– 1–  A  =  2GTG  =  10 2 2 4  .  9  The gradient at   x0   is  g0  =    F x0    =  Ax0 d+  =  10 2 2 4  1 1  +  2– 1–  =  .  10 5  9-27   9 Performance Optimization  The first iteration of steepest descent will be  x1  =  x0 g0  –  =  1 1  –  0.05 10 5  =  0.5 0.75  .  The second iteration will be  x2  =  x1 g1  –  =  0.5 0.75  –  0.05 4.5 3  =  0.275 0.6  .  The remaining iterations are displayed in Figure P9.5. The algorithm con- verges to the minimum point  value for both the weight and the bias of this network is   . Therefore the optimal   0.167 0.167  0.167  x  =  .  T  -1  0  1  2  Figure P9.5  Steepest Descent Trajectory for Problem P9.3 with     =  0.05  Note that in order to train this network we needed to know all of the input  output pairs. We then performed iterations of the steepest descent algo- rithm until convergence was achieved. In Chapter 10 we will introduce an  adaptive algorithm, based on steepest descent, for training linear net- works. With this adaptive algorithm the network parameters are updated  after each input output pair is presented. We will show how this allows the  network to adapt to a changing environment.  ii. The maximum eigenvalue of the Hessian matrix for this problem is  1    see Problem P8.6 , therefore for stability  10.6  =      2  ---------- 10.6  =  0.1887  .  2  1  0  -1  -2 -2  9-28   Solved Problems  P9.4 Consider the function  F x   =  e    2 x1  x1–  +  2 2x2  +  4    .  Take one iteration of Newton’s method from the initial guess  . How close is this result to the minimum point of  x0 1 2– Explain.  =  T  F x   ?   The first step is to find the gradient and the Hessian matrix. The gradient  is given by    F x   =   x1  x2  F x   F x     2 x1  x1–  +  2 2x2  +  4     2x1  =  e    1–     4x2  ,  and the Hessian matrix is given by  2  F x   =  2   2  x1 2  x2 x1    F x   F x     2  x1 x2 2   2  x2  F x   F x     2 x1  x1–  +  2 2x2  +  4  2  4x1  –  =  e  4x1 3+  4x2       2x1  1– 2 16x2     4x2  4+    2x1 1–  If we evaluate these at the initial guess we find  g0  =    F x   =  x x0=  0.163  –  1.302  610 610  ,  and  A0  =  2  F x   =  x x0=  0.049  –  0.130  710 710  –  0.130  1.107  710 710  .  Therefore the first iteration of Newton’s method, from Eq.  9.43 , will be  9  9-29   9 Performance Optimization  x1  =  x0 A0  –  1– g0  =  1 2–  –  0.049  –  0.130  710 710  –  0.130  1.107  710 710  0.163  –  1.302  610 610  =  0.971 1.886 –  1–  How close is this to the true minimum point of  exponent of    is a quadratic function:  F x   F x   ? First, note that the   2 x1  x1–  +  2 2x2  +  4  =  1 ---xTAx dTx 2  +  +  c  =  1 ---xT 2 0 2 0 4  x  +  1– 0 x  +  4  .  The minimum point of  exponent, which is  F x    will be the same as the minimum point of the   x  =  A–  1– d  =  –  1–  2 0 0 4  1– 0  =  .  0.5 0  Therefore Newton’s method has taken only a very small step toward the  true minimum point. This is because   cannot be accurately approxi- mated by a quadratic function in the neighborhood of   F x   .   T  =  x0  1 2–  For this problem Newton’s method will converge to the true minimum  point, but it will take many iterations. The trajectory for Newton’s method  is illustrated in Figure P9.6.  -1  0  1  2  Figure P9.6  Newton’s Method Trajectory for Problem P9.4  2  1  0  -1  -2 -2  9-30   Solved Problems  P9.5 Compare the performance of Newton’s method and steepest de-  scent on the following function:  Recall that this function is an example of a stationary valley  see Eq.  8.59   and Figure 8.9 . The gradient is  Start from the initial guess  F x   =  1 ---xT 1 2 1–  1– 1  x  .  x0  =  .  1 0    F x   =  Ax d+  =  1 1–  1– 1  x  and the Hessian matrix is  Newton’s method is given by  2  F x   =  A  =  1 1–  1– 1  .  xk  1+  =  xk Ak  –  1– gk  .  Note, however, that we cannot actually perform this algorithm, because the  Hessian matrix is singular. We know from our discussion of this function  in Chapter 8 that this function does not have a strong minimum, but it does  have a weak minimum along the line   .  x1  x2=  What about steepest descent? If we start from the initial guess, with learn- ing rate   , the first two iterations will be  0.1    =  9  x1  =  x0 g0  –  =  1 0  –  0.1 1 1–  =  ,  0.9 0.1  x2  =  x1 g1  –  =  0.9 0.1  –  0.1 0.8 0.8–  =  0.82 0.18  .  9-31   9 Performance Optimization  The complete trajectory is shown in Figure P9.7. This is a case where the  steepest descent algorithm performs better than Newton’s method. Steep- est descent converges to a minimum point  weak minimum , while New- ton’s method fails to converge. In Chapter 12 we will discuss a technique  that combines steepest descent with Newton’s method, to overcome the  problem of singular  or almost singular  Hessian matrices.  Figure P9.7  Steepest Descent Trajectory for Problem P9.5 with     =  0.1  -0.5  0  0.5  1  P9.6 Consider the following function:  F x   =  3 x1  +  x1x2  –  2  2x2 x1  i. Perform one iteration of Newton’s method from the initial   guess   x0  =  T  .  1 1  ii. Find the second-order Taylor series expansion of   F x  . Is this quadratic function minimized at the point   x0 found in part  i ? Explain.   about  x1     i. The gradient of   F x    is    F x   =   x1  x2  F x   F x   =  2 3x1  +  x2 –  –  2 2x1x2 2x2  x1 2x1  ,  and the Hessian matrix is  0.5  1  0  -0.5  -1 -1  9-32   If we evaluate these at the initial guess we find  Solved Problems  2  F x   =  –  2 2x2 6x1 – 1 4x1x2  1  –  4x1x2 2 2x1  –  .  g0  =    F x   =  x x0=  ,  2 1–  A0  =  2  F x   =  x x0=  4 3–  3– 2–  .  The first iteration of Newton’s method is then  x1  =  x0 A0  –  1– g0  =  1–  –  1 1  4 3–  3– 2–  2 1–  =  0.5882 1.1176  .  ii. From Eq.  9.40 , the second-order Taylor series expansion of  about    is  F x      x0  F x   =  F x0 x0   +   F x0     +  Tx0 g0  +  1 ---x0 2  TA0x0  .  If we substitute the values for   x0 g0  ,    and   A0  , we find  F x     1  +  2 1–  x  –      1 1  +       1 --- x  2   –  T    1 1  4 3–  3– 2–  x  –      1 1  .      This can be reduced to  F x     2–  +  1 4 x  +  1 ---xT 4 2 3–  3– 2–  x  .  . The question is whether or not  This function has a stationary point at  the stationary point is a strong minimum. This can be determined from the  eigenvalues of the Hessian matrix. If both eigenvalues are positive, it is a  strong minimum. If both eigenvalues are negative, it is a strong maximum.  If the two eigenvalues have opposite signs, it is a saddle point. In this case  the eigenvalues of    are  x1  A0  9  1  =  5.24   and   2  –=  3.24  .  9-33   9 Performance Optimization  Therefore the quadratic approximation to  x1 and its quadratic approximation.   , since it is a saddle point. Figure P9.8 displays the contour plots of    is not minimized at  F x      F x    at   x0  This sort of problem was also illustrated in Figure 9.8 and Figure 9.9. New- ton’s method does locate the stationary point of the quadratic approxima- tion of the function at the current guess. It does not distinguish between  minima, maxima and saddle points.  2  1  0  -1  -2 -2  2  1  0  -1  -2 -2  -1  0  1  2  -1  0  1  2  Figure P9.8  One Iteration of Newton’s Method from   x0  =  1 1  T  P9.7 Repeat Problem P9.3  i  using the conjugate gradient algorithm.  Recall that the function to be minimized was  F x   =  0.25  +  2–  1– x  +  1 ---xT 10 2 2 2 4  x  .  The gradient at   x0   is  g0  =    F x0    =  Ax0 d+  =  10 2 2 4  1 1  +  2– 1–  =  .  10 5  The first search direction is then  p0  =  g– 0  =  10– 5–  .  To minimize along a line, for a quadratic function, we can use Eq.  9.31 :  9-34   Solved Problems  0  =  –  Tp0 g0 ---------------- TAp0 p0  10 5  10– 5–  =  –  ----------------------------------------------------- 10– 5–  10 2 2 4  10–  5–  =  –  125 – ------------ 1300  =  0.0962  .  Therefore the first iteration of conjugate gradient will be  x1  =  x0 0p0  +  =  +  0.0962  1 1  10– 5–  =  0.038 0.519  .  Now we need to find the second search direction from Eq.  9.60 . This re- quires the gradient at   :  x1  g1  =    F x   =  x x1=  10 2 2 4  0.038 0.519  +  2– 1–  =  0.577 – 1.154  .  We can now find   1  :  –  10.577  – -----------------------------------------------------------------  3.846  0.577 – 1.154  =  1  =  T g1 g0 -------------- Tg0 g0  10 5  10 5  =  1.665 ------------- 125  =  0.0133  ,  using the method of Polak and Ribiére  Eq.  9.63  .  The other two methods  for computing  You may want to try them.  The second search direction is then computed  from Eq.  9.60 :   will produce the same results for a quadratic function.   1  p1  =  g1–  +  1p0  =  0.577 – 1.154  +  0.0133  10– 5–  =  0.444 – 1.220  .  From Eq.  9.31 , the learning rate for the second iteration will be  1  =  –  ---------------------------------------------------------------------------  =  –  =  0.2889  .  1.664 – ---------------- 5.758  –  0.577  1.154  0.444 1.220 –  0.444 1.220  –  10 2 2 4  0.444 – 1.220  The second step of conjugate gradient is therefore   9-35  9   9 Performance Optimization  x2  =  x1 1p1  +  =  0.038 0.519  +  0.2889 0.444 1.220  –  =  0.1667 0.1667  .  As expected, the minimum is reached in two iterations. The trajectory is il- lustrated in Figure P9.9.  -1  0  1  2  Figure P9.9  Conjugate Gradient Trajectory for Problem P9.7  P9.8 Show that conjugate vectors are independent.  p0 p1   pn 1– Suppose that we have a set of vectors,   A gate with respect to the Hessian matrix  then, from Eq.  5.4 , it must be true that  , which are conju-  . If these vectors are dependent,         ajpj  0=  ,  n  1–    j  0=  n  1–    j  0=  for some set of constants  If we multiply both sides of this equation by   a0 a1   an  1–        , at least one of which is nonzero.   TA pk  , we obtain  TA pk  ajpj  =  ajpk  TApj  =  akpk  TApk  =  0  ,  where the second equality comes from the definition of conjugate vectors in  Eq.  9.52 . If   is positive definite  a unique strong minimum exists , then  TApk pk .  Therefore conjugate directions must be independent.   must be strictly positive. This implies that    must be zero for all   ak  A  k  2  1  0  -1  -2 -2  n  1–    j  0=  9-36   Epilogue  Epilogue  In this chapter we have introduced three different optimization algorithms:  steepest descent, Newton’s method and conjugate gradient. The basis for  these algorithms is the Taylor series expansion. Steepest descent is derived  by using a first-order expansion, whereas Newton’s method and conjugate  gradient are designed for second-order  quadratic  functions.   Steepest descent has the advantage that it is very simple, requiring calcu- lation only of the gradient. It is also guaranteed to converge to a stationary  point if the learning rate is small enough. The disadvantage of steepest de- scent is that training times are generally longer than for other algorithms.  This is especially true when the eigenvalues of the Hessian matrix, for qua- dratic functions, have a wide range of magnitudes.  Newton’s method is generally much faster than steepest descent. For qua- dratic functions it will locate a stationary point in one iteration. One disad- vantage is that it requires calculation and storage of the Hessian matrix,  as well as its inverse. In addition, the convergence properties of Newton’s  method are quite complex. In Chapter 12 we will introduce a modification  of Newton’s method that overcomes some of the disadvantages of the stan- dard algorithm.  The conjugate gradient algorithm is something of a compromise between  steepest descent and Newton’s method. It will locate the minimum of a qua- dratic function in a finite number of iterations, but it does not require cal- culation and storage of the Hessian matrix. It is well suited to problems  with large numbers of parameters, where it is impractical to compute and  store the Hessian.  In later chapters we will apply each of these optimization algorithms to the  training of neural networks. In Chapter 10 we will demonstrate how an ap- proximate steepest descent algorithm, Widrow-Hoff learning, can be used  to train linear networks. In Chapter 11 we generalize Widrow-Hoff learn- ing to train multilayer networks. In Chapter 12 the conjugate gradient al- gorithm, and a variation of Newton’s method, are used to speed up the  training of multilayer networks.  9  9-37   9 Performance Optimization  Further Reading  [Batt92]  R. Battiti, “First and Second Order Methods for Learning:  Between Steepest Descent and Newton’s Method,” Neural  Computation, Vol. 4, No. 2, pp. 141-166, 1992.  This article reviews the latest developments in uncon- strained optimization using first and second derivatives.  The techniques discussed are those that are most suitable  for neural network applications.  [Brog91]   W. L. Brogan, Modern Control Theory, 3rd Ed., Englewood  Cliffs, NJ: Prentice-Hall, 1991.  This is a well-written book on the subject of linear systems.  The first half of the book is devoted to linear algebra. It also  has good sections on the solution of linear differential equa- tions and the stability of linear and nonlinear systems. It  has many worked problems.  [Gill81]  P. E. Gill, W. Murray and M. H. Wright, Practical Optimi- zation, New York: Academic Press, 1981.  As the title implies, this text emphasizes the practical im- plementation of optimization algorithms. It provides moti- vation for the optimization methods, as well as details of  implementation that affect algorithm performance.  [Himm72]  D. M. Himmelblau, Applied Nonlinear Programming, New  York: McGraw-Hill, 1972.  This is a comprehensive text on nonlinear optimization. It  covers both constrained and unconstrained optimization  problems. The text is very complete, with many examples  worked out in detail.  [Scal85]   L. E. Scales, Introduction to Non-Linear Optimization, New  York: Springer-Verlag, 1985.  A very readable text describing the major optimization al- gorithms, this text emphasizes methods of optimization  rather than existence theorems and proofs of convergence.  Algorithms are presented with intuitive explanations,  along with illustrative figures and examples. Pseudo-code  is presented for most algorithms.  9-38   Exercises  Exercises  E9.1 In Problem P9.1 we found the maximum stable learning rate for the steep- est descent algorithm when applied to a particular quadratic function. Will  the algorithm always diverge when a larger learning rate is used, or are  there any conditions for which the algorithm will still converge?  E9.2 We want to find the minimum of the following function:  F x   =  1 ---xT 6 2 2–  2– 6  x  +  1–  1– x  .  i. Sketch a contour plot of this function.  ii. Sketch the trajectory of the steepest descent algorithm on the con- . Assume a very   tour plot of part  i , if the initial guess is  small learning rate is used.  0 0  x0  =  T  iii. Perform two iterations of steepest descent with learning rate     =  0.1  .  iv. What is the maximum stable learning rate?  v. What is the maximum stable learning rate for the initial guess giv-  en in part  ii ?  See Exercise E9.1.   vi. Write a MATLAB M-file to implement the steepest descent algo-  rithm for this problem, and use it to check your answers to parts  i .  through  v .  » 2 + 2 ans =       4  E9.3 For the quadratic function  i. Find the minimum of the function along the line  9  F x   =  2 x1  +  2 2x2  ,  x  =  1 1  +   1– 2–  .  ii. Verify that the gradient of   F x    at the minimum point from part  i    is orthogonal to the line along which the minimization occurred.  9-39   9 Performance Optimization  E9.4 For the functions given in Exercise E8.3 perform two iterations of the   steepest descent algorithm with linear minimization, starting from the ini- tial guess   . Write MATLAB M-files to check your answer.  T  =  x0  1 1  » 2 + 2 ans =       4  E9.5 Consider the following function:  F x   =  + 1    x1  +  x2 5–  2   1 +    3x1  –  2x2  2    .  i. Perform one iteration of Newton’s method, starting from the initial   guess   x0  =  10 10  T  .  ii. Repeat part  i , starting from the initial guess   x0  =  T  .  2 2  iii. Find the minimum of the function, and compare with your results   from the previous two parts.  E9.6 Consider the following quadratic function  F x   =  1 ---xT 3 2 2 2 0  x  +  4 4 x  i. Sketch the contour plot for   F x   . Show all work.  ii. Take one iteration of Newton’s method from the initial guess   x0  =  T  .  0 0  iii. In part  ii , did you reach the minimum of   F x   ? Explain.  E9.7 Consider the function  F x   =    x1  x2+  4  +   2 x2  1–  2  i. Find the second-order Taylor series approximation of this function   about the point   x0  =  T  .  1– 1  ii. Is this point a minimum point? Does it satisfy the first and second   iii. Perform one iteration of Newton's method from the initial guess   order conditions?   x0  =  0.5 0  T  .  9-40   Exercises  E9.8 Consider the following quadratic function:  F x   =  1 ---xT 7 2 9–  9– 17–  x  +  16 8 x  i. Sketch the contour plot for this function.  ii. Take one step of Newton’s method from the initial guess   x0  =  T  .  2 2  part  ii ? Explain.  iii. Did you reach the minimum of the function after the Newton step of   iv. From the initial guess in part ii, trace the path of steepest descent,  with very small learning rate, on your contour plot from part  i . Ex- plain how you determined the path. Will steepest descent eventual- ly converge to the same result you found in part  ii ? Explain.  E9.9 Consider the following function:  F x   =    1  +  x1  +  x2  2  +  1 ---x 4  4  1  .  i. Find the quadratic approximation to   F x    about the point   x0  =  T  .  2 2  ii. Sketch the contour plot of the quadratic approximation in part i.  iii. Perform one iteration of Newton’s method on the function   F x   given in part  i . Sketch the path from   the initial condition  x1   on your contour plot from part  ii .  x0   from   to  x0  iv. Is the   tion? Is it a strong minimum of the original function    in part iii. a strong minimum of the quadratic approxima- ? Explain.  x1  F x   v. Will Newton’s method always converge to a strong minimum of   , given enough iterations? Will it always converge to a strong  ? Explain your an-  F x  minimum of the quadratic approximation of  swers in detail.  F x   9  » 2 + 2 ans =       4  E9.10 Recall the function presented in Exercise E8.5. Write MATLAB M-files to  implement the steepest descent algorithm and Newton’s method for that  function. Test the performance of the algorithms for various initial guesses.  E9.11 Repeat Exercise E9.4 using the conjugate gradient algorithm. Use each of   the three methods  Eq.  9.61 –Eq.  9.63   at least once.  9-41   9 Performance Optimization  E9.12 Prove or disprove the following statement: p2   is conjugate to   p1  If   then   p1   and   p2  is conjugate to   p3  .   is conjugate to   p3  ,   9-42   Objectives  10 Widrow-Hoff Learning  Objectives Theory and Examples ADALINE Network  Single ADALINE  Mean Square Error LMS Algorithm Analysis of Convergence Adaptive Filtering  Adaptive Noise Cancellation Echo Cancellation  Summary of Results Solved Problems Epilogue Further Reading Exercises  10-1 10-2 10-2 10-3 10-4 10-7 10-9 10-13 10-15 10-21 10-22 10-24 10-40 10-41 10-42  Objectives  In the previous two chapters we laid the foundation for performance learn- ing, in which a network is trained to optimize its performance. In this chap- ter we apply the principles of performance learning to a single-layer linear  neural network.   Widrow-Hoff learning is an approximate steepest descent algorithm, in  which the performance index is mean square error. This algorithm is im- portant to our discussion for two reasons. First, it is widely used today in  many signal processing applications, several of which we will discuss in  this chapter. In addition, it is the precursor to the backpropagation algo- rithm for multilayer networks, which is presented in Chapter 11  10  10-1   10 Widrow-Hoff Learning  Theory and Examples  Bernard Widrow began working in neural networks in the late 1950s, at  about the same time that Frank Rosenblatt developed the perceptron  learning rule. In 1960 Widrow, and his graduate student Marcian Hoff, in- troduced the ADALINE  ADAptive LInear NEuron  network, and a learn- ing rule which they called the LMS  Least Mean Square  algorithm  [WiHo60].   Their ADALINE network is very similar to the perceptron, except that its  transfer function is linear, instead of hard-limiting. Both the ADALINE  and the perceptron suffer from the same inherent limitation: they can only  solve linearly separable problems  recall our discussion in Chapters 3 and  4 . The LMS algorithm, however, is more powerful than the perceptron  learning rule. While the perceptron rule is guaranteed to converge to a so- lution that correctly categorizes the training patterns, the resulting net- work can be sensitive to noise, since patterns often lie close to the decision  boundaries. The LMS algorithm minimizes mean square error, and there- fore tries to move the decision boundaries as far from the training patterns  as possible.  The LMS algorithm has found many more practical uses than the percep- tron learning rule. This is especially true in the area of digital signal pro- cessing. For example, most long distance phone lines use ADALINE  networks for echo cancellation. We will discuss these applications in detail  later in the chapter.   Because of the great success of the LMS algorithm in signal processing ap- plications, and because of the lack of success in adapting the algorithm to  multilayer networks, Widrow stopped work on neural networks in the early  1960s and began to work full time on adaptive signal processing. He re- turned to the neural network field in the 1980s and began research on the  use of neural networks in adaptive control, using temporal backpropaga- tion, a descendant of his original LMS algorithm.  ADALINE Network  The ADALINE network is shown in Figure 10.1. Notice that it has the  same basic structure as the perceptron network we discussed in Chapter 4.  The only difference is that it has a linear transfer function.  10-2   ADALINE Network  Input  Linear Neuron  p R x 1  1   cid:0 W n cid:0   cid:0 b  S x R  S x 1   cid:0  cid:0  a S x 1  cid:0  cid:0   cid:0  cid:0   S x 1  S  R  a = purelin  Wp + b   Figure 10.1  ADALINE Network  The output of the network is given by  a  =  purelin Wp b+     Wp b+  =  .   10.1   Recall from our discussion of the perceptron network that the ith element  of the network output vector can be written  ai  =  purelin ni    =  purelin wT    i  p bi+    =  wT  i  p bi+  ,   10.2   where   wi   is made up of the elements of the ith row of   W  :  wi  =  wi 1 wi 2  wi R  .  Single ADALINE To simplify our discussion, let’s consider a single ADALINE with two in- puts. The diagram for this network is shown in Figure 10.2.  The output of the network is given by   10.3   a  =  purelin n   =  purelin wT    1  p b+    =  wT  p b+  1  =  wT 1  p b+  =  w1 1 p1 w1 2 p2  +  +  b .   10.4   10  10-3   10 Widrow-Hoff Learning  Inputs  Two-Input Neuron  p1  p2   cid:0  cid:0 Σ w1,1 n  cid:0  cid:0  w1,2  b   cid:0  cid:0  a  cid:0  cid:0   1  a = purelin  Wp + b   Figure 10.2  Two-Input Linear Neuron  You may recall from Chapter 4 that the perceptron has a decision bound- ary, which is determined by the input vectors for which the net input   is  zero. Now, does the ADALINE also have such a boundary? Clearly it does.  If we set   specifies such a line, as shown in Figure  10.3.   then   p b+  wT 1  0=  0=  n  n  p2  a < 0  -b w1,2  1wTp + b = 0  a > 0  1w  p1  -b w1,1  Figure 10.3  Decision Boundary for Two-Input ADALINE  The neuron output is greater than 0 in the gray area. In the white area the  output is less than zero. Now, what does this imply about the ADALINE?  It says that the ADALINE can be used to classify objects into two catego- ries. However, it can do so only if the objects are linearly separable. Thus,  in this respect, the ADALINE has the same limitation as the perceptron.  Mean Square Error  Now that we have examined the characteristics of the ADALINE network,  we are ready to begin our development of the LMS algorithm. As with the  perceptron rule, the LMS algorithm is an example of supervised training,  in which the learning rule is provided with a set of examples of proper  network behavior:  10-4   Mean Square Error  p1 t1 { , }    {  p2 t2  , }   pQ tQ  {      ,  }  ,   10.5   pq   is an input to the network, and   where  output. As each input is applied to the network, the network output is com- pared to the target.    is the corresponding target   tq  The LMS algorithm will adjust the weights and biases of the ADALINE in  order to minimize the mean square error, where the error is the difference  between the target output and the network output. In this section we want  to discuss this performance index. We will consider first the single-neuron  case.  To simply our development, we will lump all of the parameters we are  adjusting, including the bias, into one vector:  Similarly, we include the bias input “1” as a component of the input vector  Now the network output, which we usually write in the form  can be written as  x  =  w1 b  .  z  =  .  p 1  a  =  wT 1  p  b+  ,  a  =  xTz  .   10.6    10.7    10.8    10.9   Mean Square Error  This allows us to conveniently write out an expression for the ADALINE  network mean square error:  F x  E e2   =    =  E t  a–    2   =  E t  –    xTz  2   ,   10.10   E     where the expectation is taken over all sets of input target pairs.  Here we  use   to denote expected value. We use a generalized definition of ex- pectation, which becomes a time-average for deterministic signals. See  [WiSt85].  We can expand this expression as follows:  F x  E t2   =  –  2txTz  +  xTzzTx    =  E t2    –  2xTE tz    +  xTE zzT    x .   10.11   10  10-5   Correlation Matrix  10 Widrow-Hoff Learning  This can be written in the following convenient form:  F x   =  c  –  2xTh  +  xTRx  ,   10.12   where  c  =  E t2    ,   h  =  E tz     and   R  =  E zzT     .   10.13   h   gives the cross-correlation between the input vector and  Here the vector  its associated target, while   is the input correlation matrix. The diagonal  elements of this matrix are equal to the mean square values of the elements  of the input vectors.  R  Take a close look at Eq.  10.12 , and compare it with the general form of the  quadratic function given in Eq.  8.35  and repeated here:  F x   =  c dTx +  +  1 ---xTAx 2  .   10.14   We can see that the mean square error performance index for the ADA- LINE network is a quadratic function, where  d  2h–=   and   A  2R=  .   10.15   This is a very important result, because we know from Chapter 8 that the  characteristics of the quadratic function depend primarily on the Hessian  matrix  . For example, if the eigenvalues of the Hessian are all positive,  then the function will have one unique global minimum.   A  R  In this case the Hessian matrix is twice the correlation matrix  , and it  can be shown that all correlation matrices are either positive definite or  positive semidefinite, which means that they can never have negative  eigenvalues. We are left with two possibilities. If the correlation matrix has  only positive eigenvalues, the performance index will have one unique glo- bal minimum  see Figure 8.7 . If the correlation matrix has some zero  eigenvalues, the performance index will either have a weak minimum  see  Figure 8.9  or no minimum  see Problem P8.7 , depending on the vector  d Now let’s locate the stationary point of the performance index. From our  previous discussion of quadratic functions we know that the gradient is  2h–=  .  F x   =   c dTx  +  +     1  ---xTAx  2  =  d Ax+  =  2h–  +  2Rx  .   10.16   The stationary point of  zero:  F x    can be found by setting the gradient equal to   2h–  +  2Rx  0=  .   10.17   10-6   LMS Algorithm  LMS Algorithm  Therefore, if the correlation matrix is positive definite there will be a  unique stationary point, which will be a strong minimum:  x  =  R 1– h  .   10.18   It is worth noting here that the existence of a unique solution depends only  on the correlation matrix  . Therefore the characteristics of the input vec- tors determine whether or not a unique solution exists.  R  Now that we have analyzed our performance index, the next step is to de- sign an algorithm to locate the minimum point. If we could calculate the  , we could find the minimum point directly  statistical quantities  from Eq.  10.18 . If we did not want to calculate the inverse of  , we could  use the steepest descent algorithm, with the gradient calculated from Eq.  h  10.16 . In general, however, it is not desirable or convenient to calculate    . For this reason we will use an approximate steepest descent algo- and  rithm, in which we use an estimated gradient.    and   R  R  R  h  The key insight of Widrow and Hoff was that they could estimate the mean  square error    by  F x   Fˆ x   =    t k   –  a k   2  =  e2 k   ,   10.19   where the expectation of the squared error has been replaced by the  squared error at iteration  timate of the form:  . Then, at each iteration we have a gradient es-  k  ˆ F x   =  e2 k   .   10.20   This is sometimes referred to as the stochastic gradient. When this is used  in a gradient descent algorithm, it is referred to as “on-line” or incremental  learning, since the weights are updated as each input is presented to the  network.   R  The first  weights, while the  as. Thus we have  e2 k   elements of   R 1+ st   are derivatives with respect to the network   element is the derivative with respect to the bi-    e2 k   j  =  e2 k   --------------- w1 j  =  2e k  e k   ------------- w1 j   for   j  =  1 2   R       ,   10.21   and    e2 k   R 1+  =  e2 k   b---------------  =  2e k  e k   b-------------  .   10.22   10-7  10  Stochastic Gradient   10 Widrow-Hoff Learning  Now consider the partial derivative terms at the ends of these equations.  First evaluate the partial derivative of  : w1 j  with respect to the weight   e k    e k  ------------- w1 j  =     a k  ----------------------------------  t k  –  w1 j  =   w1 j      t k   –    wT 1  p k   b+      =   w1 j    t k   –  w1 i pi k     b+       R    i  1=  where  pi k  simplifies to   is the   ith   element of the input vector at the   kth   iteration. This   e k   ------------- w1 j  –=  pj k   .  e k   b-------------  1–=  .  In a similar way we can obtain the final element of the gradient:  Note that  ent of the squared error at iteration   pj k    and   1  k   can be written   are the elements of the input vector   , so the gradi-  z  ˆ F x   =    e2 k   =  –  2e k z k   .   10.26   Now we can see the beauty of approximating the mean square error by the  single error at iteration  , as in Eq.  10.19 . To calculate this approximate  gradient we need only multiply the error times the input.  k  This approximation to   can now be used in the steepest descent algo- rithm. From Eq.  9.10  the steepest descent algorithm, with constant learn- ing rate, is  F x   If we substitute   ˆ F x   , from Eq.  10.26 , for   F x    we find  xk  1+  =  xk  F x     –  .  x xk=  xk  1+  =  xk  +  2e k z k   ,  or  and  w1  1+ k    =  w1  k   +  2e k p k   ,  1+ b k    =  b k   +  2e k   .  10-8   10.23    10.24    10.25    10.27    10.28    10.29    10.30    Analysis of Convergence  These last two equations make up the least mean square  LMS  algorithm.  This is also referred to as the delta rule or the Widrow-Hoff learning algo- rithm.   The preceding results can be modified to handle the case where we have  multiple outputs, and therefore multiple neurons, as in Figure 10.1. To up- date the ith row of the weight matrix use  wi  1+ k    =  wi  k   +  2ei k p k   ,   10.31   where  element of the bias we use  ei k    is the ith element of the error at iteration   k  . To update the ith   1+  bi k    =  bi k   +  2ei k   .  W k  1+   W k   =  +  2e k pT k   ,  and  b k 1+    =  b k   +  2e k   .  Note that the error    and the bias  are now vectors.  b  e   10.32    10.33    10.34   LMS Algorithm  The LMS algorithm can be written conveniently in matrix notation:  Analysis of Convergence  The stability of the steepest descent algorithm was investigated in Chapter  9. There we found that the maximum stable learning rate for quadratic   is the largest eigenvalue of the Hessian  functions is  matrix. Now we want to investigate the convergence of the LMS algorithm,  which is approximate steepest descent. We will find that the result is the  same.   2 max  , where   max    2–   z k   z k 1–   is a function only  To begin, note that in the LMS algorithm, Eq.  10.28 ,  . If we assume that successive input vectors are  of  statistically independent, then  . We will show in  the following development that for stationary input processes meeting this  condition, the expected value of the weight vector will converge to   is independent of      z 0    z k   xk  xk    R 1– h This is the minimum mean square error    10.18 .   x  =  .  Recall the LMS algorithm  Eq.  10.28  :   10.35   2 E ek       solution, as we saw in Eq.   10  xk  1+  =  xk  +  2e k z k   .   10.36   10-9   10 Widrow-Hoff Learning  Now take the expectation of both sides:  E xk   1+    =  E xk    +  2E e k z k       .   10.37   Substitute   t k   –  xk Tz k    for the error to give  E xk   1+    =  +  2 E t k z k        E xk –      Tz k   z k       .   10.38   Finally, substitute    for   Tz k  xk   and rearrange terms to give    E xk zT k xk  E xk   1+    =  E xk    +  2 E tkz k        E z k zT k  –      xk      .   10.39   Since   xk   is independent of   z k   :  E xk   1+    =  E xk    +  2 h RE xk  –      .  This can be written as  E xk   1+    =  I –  2R  E xk    +  2h  .  This dynamic system will be stable if all of the eigenvalues of   fall  inside the unit circle  see [Brog91] . Recall from Chapter 9 that the eigen- R values of  .  Therefore, the system will be stable if   are the eigenvalues of   , where the    will be   2i  2R  2R  I –  I –  i  1  –      Since  i therefore  0  ,   1  –  2i   is always less than 1. The condition on stability is   1  –  2i  1–  .   1     i     for all i  ,  0  1 max        .  Note that this condition is equivalent to the condition we derived in Chap- ter 9 for the steepest descent algorithm, although in that case we were us- A ing the eigenvalues of the Hessian matrix  . Now we are using the  R 2R= eigenvalues of the input correlation matrix  .  .  Recall that  If this condition on stability is satisfied, the steady state solution is  A   10.40    10.41    10.42    10.43    10.44    10.45    10.46   E xss     =  I –  2R  E xss      +  2h  ,  E xss     =  R 1– h  =  x  .  10-10  or  or   Analysis of Convergence  Thus the LMS solution, obtained by applying one input vector at a time, is  the same as the minimum mean square error solution of Eq.  10.18 .   2 2+  To test the ADALINE network and the LMS algorithm consider again the  apple orange recognition problem originally discussed in Chapter 3. For  simplicity we will assume that the ADALINE network has a zero bias.  The LMS weight update algorithm of Eq.  10.29  will be used to calculate  the new weights at each step in the network training:  W k  1+   W k   =  +  2e k pT k   .   10.47   First let’s compute the maximum stable learning rate  . We can get such  a value by finding the eigenvalues of the input correlation matrix. Recall  that the orange and apple vectors and their associated targets are          1 1– 1–              1 1 1–  .        p1  =  t1  =  1–  p2  =  t2  =  1   10.48   If we assume that the input vectors are generated randomly with equal  probability, we can compute the input correlation matrix:  R  =  E ppT     =  1 T ---p1p1 2  +  1 T ---p2p2 2  =  1 --- 2  1 1– 1–  +  1 --- 2  1 1 1–  1 0 1– 0 1 0 1– 0 1   .  1 1–  1–  1 1 1–  =   10.49   The eigenvalues of   R   are  1  =   1.0  2  =   0.0  3  2.0=  .   10.50   Thus, the maximum stable learning rate is      1 ----------- max  =  1 ------- 2.0  =  0.5  .   10.51   To be conservative we will pick  tions it might not be practical to calculate  trial and error. Other techniques for choosing   0.2  R    =  .  Note that in practical applica-  , and      could be selected by      are given in [WiSt85].   We will start, arbitrarily, with all the weights set to zero, and then will ap- ply inputs  , etc., in that order, calculating the new weights  after each input is presented.  The presentation of the weights in alternat-  p1 p2 p1 p2  ,   ,   ,   10  10-11   10 Widrow-Hoff Learning  ing order is not necessary. A random sequence would be fine.  Presenting  p1  , the orange, and using its target of -1 we get  a 0  W 0 p 0  W 0 p1  =  =  =  0 0 0  0=  ,   10.52   and  e 0   =  t 0   –  a 0   =  –  a 0   =  1–  1–=–  0  .  t1   10.53   Now we can calculate the new weight matrix:  W 1  W 0   =  +  2e 0 pT 0   =  0 0 0  +  2 0.2    1–    =  0.4–  0.4 0.4  .   10.54   T  1 1– 1–  According to plan, we will next present the apple,   , and its target of 1:  a 1  W 1 p 1  W 1 p2  =  =  =  0.4–  0.4 0.4  0.4–=  ,   10.55   e 1   =  t 1   –  a 1   =  –  a 1   =  1  –  0.4–    =  1.4  .  t2   10.56   and so the error is  Now we calculate the new weights:  W 2  W 1   =  +  2e 1 pT 1   =  0.4–  0.4 0.4  +  2 0.2   1.4    0.16 0.96 0.16  –   .   10.57   T  =  1 1 1–  Next we present the orange again:  a 2  W 2 p 2  W 2 p1  =  =  =  0.16 0.96 0.16  –  –=  0.64  .   10.58   1 1– 1–  p2  1 1 1–  1 1– 1–  10-12   Adaptive Filtering  The error is  The new weights are  e 2   =  t 2   –  a 2   =  t1 a 2   –  =  1–  –  –  0.64    –=  0.36  .   10.59   W 3  W 2   =  +  2e 2 pT 2   =  0.016 1.1040 0.0160  –  .   10.60   If we continue this procedure, the algorithm converges to  W     =  0 1 0  .   10.61   Compare this result with the result of the perceptron learning rule in  Chapter 4. You will notice that the ADALINE has produced the same deci- sion boundary that we designed in Chapter 3 for the apple orange problem.  This boundary falls halfway between the two reference patterns. The per- ceptron rule did not produce such a boundary. This is because the percep- tron rule stops as soon as the patterns are correctly classified, even though  some patterns may be close to the boundaries. The LMS algorithm mini- mizes the mean square error. Therefore it tries to move the decision bound- aries as far from the reference patterns as possible.  Adaptive Filtering  As we mentioned at the beginning of this chapter, the ADALINE network  has the same major limitation as the perceptron network; it can only solve  linearly separable problems. In spite of this, the ADALINE has been much  more widely used than the perceptron network. In fact, it is safe to say that  it is one of the most widely used neural networks in practical applications.  One of the major application areas of the ADALINE has been adaptive fil- tering, where it is still used extensively. In this section we will demonstrate  an adaptive filtering example.  Tapped Delay Line  In order to use the ADALINE network as an adaptive filter, we need to in- troduce a new building block, the tapped delay line. A tapped delay line  with    outputs is shown in Figure 10.4.  R  The input signal enters from the left. At the output of the tapped delay line  we have an  -dimensional vector, consisting of the input signal at the cur- rent time and at delays of from    time steps.  R 1–   to   R  1  10  10-13   Adaptive Filter  If we combine a tapped delay line with an ADALINE network, we can cre- ate an adaptive filter, as is shown in Figure 10.5. The output of the filter is  given by  a k   =  purelin Wp b+      =   w1 i y k  i–  1+    b+  .   10.62   10 Widrow-Hoff Learning  y k   p1 k  = y k   p2 k  = y k - 1    cid:0 D   cid:0 D  cid:0  D  cid:0   pR k  = y k - R + 1   Figure 10.4  Tapped Delay Line  y k   R    i  1=  Inputs  ADALINE   cid:0 D  cid:0   cid:0 D  cid:0   cid:0 D  w1,1  w1,2  w1,R  SxR   cid:0 Σ  n k  a k   cid:0  cid:0   b  1  a k  = purelin  Wp k  + b   Figure 10.5  Adaptive Filter ADALINE  10-14   2 2+  Adaptive Filtering  If you are familiar with digital signal processing, you will recognize the net- work of Figure 10.5 as a finite impulse response  FIR  filter [WiSt85]. It is  beyond the scope of this text to review the field of digital signal processing,  but we can demonstrate the usefulness of this adaptive filter through a  simple, but practical, example.  Adaptive Noise Cancellation An adaptive filter can be used in a variety of novel ways. In the following  example we will use it for noise cancellation. Take some time to look at this  example, for it is a little different from what you might expect. For in- stance, the output “error” that the network tries to minimize is actually an  approximation to the signal we are trying to recover!  Let’s suppose that a doctor, in trying to review the electroencephalogram   EEG  of a distracted graduate student, finds that the signal he would like  to see has been contaminated by a 60-Hz noise source. He is examining the  patient on-line and wants to view the best signal that can be obtained. Fig- ure 10.6 shows how an adaptive filter can be used to remove the contami- nating signal.  EEG Signal  random   s  Contaminated       Signal  t  Restored Signal  e  +  - Adaptively Filtered  Noise to Cancel  Contamination  "Error"  Contaminating        Noise  m  Noise Path     Filter  Graduate  Student  v  60-Hz  Noise Source  a  Adaptive    Filter   Adaptive Filter Adjusts to Minimize Error  and in doing    this removes 60-Hz noise from contaminated signal   Figure 10.6  Noise Cancellation System  As shown, a sample of the original 60-Hz signal is fed to an adaptive filter,  whose elements are adjusted so as to minimize the “error”  . The desired  output of the filter is the contaminated EEG signal  . The adaptive filter  will do its best to reproduce this contaminated signal, but it only knows  about the original noise source,  . Thus, it can only reproduce the part of    v t that is linearly correlated with  , which is  . In effect, the adaptive filter  v will attempt to mimic the noise path filter, so that the output of the filter   m  e  t  10  10-15   10 Widrow-Hoff Learning   will be close to the contaminating noise   a be close to the original uncontaminated EEG signal   m  s  .  . In this way the error   e   will   In this simple case of a single sine wave noise source, a neuron with two  weights and no bias is sufficient to implement the filter. The inputs to the  filter are the current and previous values of the noise source. Such a two- input filter can attenuate and phase-shift the noise   in the desired way.  The filter is shown in Figure 10.7.  v  Inputs  ADALINE  v k   w1,1   cid:0 D w1,2  cid:0   SxR   cid:0 Σ  n k  a k   cid:0  cid:0   a k  = w1,1 v k  + w1,2 v k - 1   Figure 10.7  Adaptive Filter for Noise Cancellation  We can apply the mathematical relationships developed in the previous  sections of this chapter to analyze this system. In order to do so, we will  first need to find the input correlation matrix  correlation vector    and the input target cross-  R  :  h  R  =    zzT     and   h  =  E tz    .   10.63   In our case the input vector is given by the current and previous values of  the noise source:  z k   =  v k  v k 1–    ,   10.64   while the target is the sum of the current signal and filtered noise:  t k   =  s k  m k   +  .   10.65   Now expand the expressions for   R   and   h   to give  R  =    E v2 k   1–  E v k    E v k v k      1–     ,  v k    E v2 k  1–     10.66   10-16   Adaptive Filtering  and  h  =      E s k  m k     E s k  m k   +  +  v k   1–  v k      .   10.67   v  , the EEG signal   To obtain specific values for these two quantities we must define the noise  signal  . For this exercise we  will assume: the EEG signal is a white  uncorrelated from one time step to  the next  random signal uniformly distributed between the values -0.2 and  +0.2, the noise source  60-Hz sine wave sampled at 180 Hz  is given by    and the filtered noise   m  s  and the filtered noise that contaminates the EEG is the noise source atten- uated by a factor of 10 and shifted in phase by    2  :  v k   =  1.2   sin     2k  ---------  3  ,  m k   =  0.12   sin     2k --------- 3    ---+  2  .   10.68    10.69   Now calculate the elements of the input correlation matrix   R  :  E v2 k      =  1.2  =  1.2  20.5  =  0.72  ,   10.70   21 --- 3  3    k  1=   sin     2k  ---------  3   2   E v2 k   1–      =  E v2 k      =  0.72  ,   10.71   E v k v k  1–        3  =  1 --- 3     k  1=  1.2   sin  2k  1.2   ---------   3  sin   2 k 1–  -----------------------  3  =  1.2  20.5  cos  =  –  0.36     2  ------  3   where we have used some trigonometric identities .  Thus   R   is  R  =  0.72 – 0.36  0.36 – 0.72  .  The terms of  term in Eq.  10.67  first:  h   can be found in a similar manner. We will consider the top   E s k  m k   +      v k     =  E s k v k      E m k v k  +      .   10.74   10-17   10.72    10.73   10   10 Widrow-Hoff Learning  Here the first term on the right is zero because  dent and zero mean. The second term is also zero:  s k    and   v k    are indepen-  E m k v k       =  0.12   sin     2k --------- 3    ---+  2   1.2     sin  2k  ---------  3  =  0   10.75   1 --- 3  3       k  1=  Thus, the first element of   h   is zero.  Next consider the second element of   h  :  E s k  m k   +      v k  1–      =  E s k v k  1–        As with the first element of  s k  v k 1–  uated as follows:   and   , the first term on the right is zero because   are independent and zero mean. The second term is eval-  h  +  E m k v k  1–       .   10.76   E m k v k  1–        0.12   sin     2k --------- 3    ---+  2   1.2      sin   2 k 1–  -----------------------  3  3  =  1 --- 3     k  1=  –=  0.0624 .  Thus,   h   is   10.77    10.78   h  =  0  .  –  0.0624  The minimum mean square error solution for the weights is given by Eq.   10.18 :  x  =  R 1– h  =  0.72 – 0.36  0.36 – 0.72  0  –  0.0624  =  – –  0.0578 0.1156  .   10.79   1–  Now, what kind of error will we have at the minimum solution? To find this  error recall Eq.  10.12 :  F x   =  c  –  2xTh  +  xTRx  .   10.80   We have just found   x h ,    and   R  , so we only need to find   c  :  c  =  E t2 k     E s k  m k  =  +      2    =  E s2 k      +  2E s k m k      E m2 k  +     .   10.81   10-18   Adaptive Filtering  The middle term is zero because   are independent and zero  mean. The first term, the mean squared value of the random signal, can be  calculated as follows:   and   m k   s k   1 ------- 0.4  0.2    0.2–  3    1 --- 3  k  1=      E s2 k      =  s2 sd  =  1  ---------------s3 3 0.4    0.2  0.2–  =  0.0133  .   10.82   The mean square value of the filtered noise is  E m2 k      =  0.12   sin  =  0.0072  ,   10.83      2 ------ 3    ---+  2  2    so that  Substituting  square error is  x h ,    and   R  c  =  0.0133  +  0.0072  =  0.0205  .   10.84    into Eq.  10.80 , we find that the minimum mean   F x    =  0.0205 2 0.0072  –      +  0.0072  =  0.0133  .   10.85   The minimum mean square error is the same as the mean square value of  the EEG signal. This is what we expected, since the “error” of this adaptive  noise canceller is in fact the reconstructed EEG signal.  Figure 10.8 illustrates the trajectory of the LMS algorithm in the weight  space with learning rate   in this  simulation were initialized arbitrarily to 0 and -2, respectively. You can  see from this figure that the LMS trajectory looks like a noisy version of  steepest descent.  . The system weights   and   w1 1  w1 2  0.1    =  w1 2  2  1  0  -1  -2 -2  10-19  -1  1  2  0  w1 1  Figure 10.8  LMS Trajectory for     =  0.1  10   10 Widrow-Hoff Learning  Note that the contours in this figure reflect the fact that the eigenvalues  and eigenvectors of the Hessian matrix      are  2R=  A  1  =  2.16  ,   z1  =  ,   2  =  0.72  ,   z2  =  –  0.7071 0.7071  – –  0.7071 0.7071  .   10.86    Refer back to our discussion in Chapter 8 on the eigensystem of the Hes- sian matrix.   If the learning rate is decreased, the LMS trajectory is smoother than that  shown in Figure 10.8, but the learning proceeds more slowly. If the learn- ing rate is increased, the trajectory is more jagged and oscillatory. In fact,  as noted earlier in this chapter, if the learning rate is increased too much  the system does not converge at all. The maximum stable learning rate is   2 2.16  0.926    =  .  In order to judge the performance of our noise canceller, consider Figure  10.9. This figure illustrates how the filter adapts to cancel the noise. The  top graph shows the restored and original EEG signals. At first the re- stored signal is a poor approximation of the original EEG signal. It takes  about 0.2 second  with    for the filter to adjust to give a reasonable  restored signal. The mean square difference between the original and re- stored signal over the last half of the experiment was 0.002. This compares  favorably with the signal mean square value of 0.0133. The difference be- tween the original and restored signal is shown in the lower graph.  0.1    =  Original and Restored EEG Signals   Original and Restored EEG Signals  -1  -2 0  2  1  0  2  1  0  -1  -2 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  EEG Signal Minus Restored Signal  0.05  0.1  0.15  0.2  0.3  0.35  0.4  0.45  0.5  0.25 Time  Figure 10.9  Adaptive Filter Cancellation of Contaminating Noise  10-20   Adaptive Filtering  You might wonder why the error does not go to zero. This is because the  LMS algorithm is approximate steepest descent; it uses an estimate of the  gradient, not the true gradient, to update the weights. The estimate of the  gradient is a noisy version of the true gradient. This will cause the weights  to continue to change slightly, even after the mean square error is at the  minimum point. You can see this effect in Figure 10.8.  To experiment with the use of this adaptive noise cancellation filter, use the  MATLAB® Neural Network Design Demonstration Adaptive Noise Cancel- lation  nnd10nc . A more complex noise source and actual EEG data are used  in the Demonstration Electroencephalogram Noise Cancellation  nnd10eeg .  Echo Cancellation Another very important practical application of adaptive noise cancellation  is echo cancellation. Echoes are common in long distance telephone lines  because of impedance mismatch at the “hybrid” device that forms the junc- tion between the long distance line and the customer’s local line. You may  have experienced this effect on international telephone calls.  Figure 10.10 illustrates how an adaptive noise cancellation filter can be  used to reduce these echoes [WiWi85]. At the end of the long distance line  the incoming signal is sent to an adaptive filter, as well as to the hybrid de- vice. The target output of the filter is the output of the hybrid. The filter  thus tries to cancel the part of the hybrid output that is correlated with the  input signal — the echo.  +   cid:0  cid:0  cid:0  cid:0 Transmission  -  cid:0  cid:0  cid:0  cid:0   Line   cid:0  cid:0 Phone   cid:0  cid:0  cid:0 Hybrid  cid:0  cid:0  cid:0   Filter   cid:0  cid:0  cid:0 Adaptive  cid:0  cid:0  cid:0   Adaptive   cid:0  cid:0  cid:0  cid:0  cid:0  cid:0   cid:0  cid:0  cid:0   Hybrid  Filter   cid:0  cid:0  Phone   cid:0  cid:0  cid:0  cid:0 Transmission   cid:0  cid:0  cid:0  cid:0   Line  -  +  Figure 10.10  Echo Cancellation System  10  10-21   10 Widrow-Hoff Learning  Summary of Results  ADALINE  Mean Square Error  Input  Linear Neuron  p R x 1  1   cid:0 W n cid:0   cid:0 b  S x R  S x 1   cid:0  cid:0  a S x 1  cid:0  cid:0   cid:0  cid:0   S x 1  S  R  a = purelin  Wp + b   F x  E e2   =    =  E t  a–    2   =  E t  –    xTz  2   F x   =  c  –  2xTh  +  xTRx  ,  c  =  E t2    ,   h  =  E tz     and   R  =  E zzT     Unique minimum, if it exists, is    x  =  R 1– h  .  Where   x  =   and   z  =  w1 b  .  p 1  LMS Algorithm  W k  1+   W k   =  +  2e k pT k   b k 1+    =  b k   +  2e k   Convergence Point  x  =  R 1– h  10-22   Summary of Results  Stable Learning Rate  0  1 max           where max is the maximum eigenvalue of R  Tapped Delay Line  Adaptive Filter ADALINE  Inputs  ADALINE  y k   y k   p1 k  = y k    cid:0 D   cid:0 D  cid:0  D  cid:0   p2 k  = y k - 1   pR k  = y k - R + 1   SxR   cid:0 Σ  n k  a k   cid:0  cid:0    cid:0 D  cid:0   cid:0 D  cid:0   cid:0 D  w1,1  w1,2  w1,R  b  1  R    i  1=  10-23  a k  = purelin  Wp k  + b   10  a k   =  purelin Wp b+      =   w1 i y k  i–  1+    b+   10 Widrow-Hoff Learning  Solved Problems  P10.1 Consider the ADALINE filter in Figure P10.1.  Inputs  ADALINE  y k    cid:0 D  cid:0   cid:0 D  w1,1  w1,2  w1,3  SxR   cid:0 Σ  n k  a k   cid:0  cid:0   a k  = purelin  Wp k  + b   Figure P10.1  ADALINE Filter  Suppose that  w1 1  =   2      w1 2  =  1–         w1 3  =  3  ,  and the input sequence is     y k     =      0 0 0 5            4–    0 0 0          where   y 0   5=  ,   y 1   4–=  , etc.  i. What is the filter output just prior to   k  0=  ?  ii. What is the filter output from   k  0=   to   k  5=  ?  iii. How long does   y 0    contribute to the output?  k  0=   three zeros have entered the filter, and the output is   k  0=  , which has the value 2, so that    the digit “5” has entered the filter, and it will be multiplied by  . This can be viewed as the   10=  a 0   i. Just prior to  zero.  ii. At  w1 1 matrix operation:  10-24   Solved Problems  a 0  Wp 0     =  =  w1 1 w1 2 w1 3  =  2 1– 3  =  10  .  y 0  1–  y  2– y  5 0 0  Similarly, one can calculate the next outputs as  a 1  Wp 1     =  =  2 1– 3  =  13–  a 2  Wp 2     =  =  2 1– 3  =  19  4– 5 0  0 4– 5  a 3  Wp 3     =  =  2 1– 3  12–=  ,   a 4  Wp 4     =  =  2 1– 3  0=  .  0 0 4–  0 0 0  All remaining outputs will be zero.  , so it will have an in- iii. The effects of  fluence for three time intervals. This corresponds to the length of the im- pulse response of this filter.   last from    through   y 0   0=  2=  k  k  P10.2 Suppose that we want to design an ADALINE network to distin-  guish between various categories of input vectors. Let us first try  the categories listed below:  Category I:        p1  =  T  1 1   and   p2  =  T  1–  1–  Category II:       p3  =  T  .  2 2  i. Can an ADALINE network be designed to make such a dis-  ii. If the answer to part  i  is yes, what set of weights and bias   tinction?   might be used?  Next consider a different set of categories.  Category III:        p1  =  T  1 1   and   p2  =  T  1 1–  10  10-25   10 Widrow-Hoff Learning  Category IV:       p3  =  T  .  1 0  iii. Can an ADALINE network be designed to make such a dis-  iv. If the answer to part  iii  is yes, what set of weights and bias   tinction?  might be used?  i. The input vectors are plotted in Figure P10.2.  p3  I  p1  II  p2  Figure P10.2  Input Vectors for Problem P10.1  i   The blue line in this figure is a decision boundary that separates the two  categories successfully. Since they are linearly separable, an ADALINE  network will do the job.  ii. The decision boundary passes through the points  know these points to be the intercepts   and  tion  – b w1 1    3 0 – b w1 2   and   0 3  . We  . Thus, a solu-    b  3=  ,   w1 1  1–=  ,   w1 2  1–=  ,   is satisfactory. Note that if the output of the ADALINE is positive or zero  the input vector is classified as Category I, and if the output is negative the  input vector is classified as Category II. This solution also provides for er- ror, since the decision boundary bisects the line between   and   .  p1  p3  iii. The input vectors to be distinguished are shown in Figure P10.3. The  vectors in the figure are not linearly separable, so an ADALINE network  cannot distinguish between them.  iv. As noted in part  iii , an ADALINE cannot do the job, so there are no  values for the weights and bias that are satisfactory.  10-26   Solved Problems  p1  p3  p2  Figure P10.3  Input Vectors for Problem P10.1  iii   P10.3 Suppose that we have the following input target pairs:      1 1  p1  =    t1  1=  ,   p2  =    t2  1–=          1 1–  .      These patterns occur with equal probability, and they are used to  train an ADALINE network with no bias. What does the mean  square error performance surface look like?  First we need to calculate the various terms of the quadratic function. Re- call from Eq.  10.11  that the performance index can be written as  Therefore we need to calculate   c h ,   F x   =  c  –  .  2xTh  and   +  R  xTRx .   The probability of each input occurring is 0.5, so the probability of each tar- get is also 0.5. Thus, the expected value of the square of the targets is  c  =  E t2    =  1 2 0.5    +  1–  2 0.5    1=  .  In a similar way, the cross-correlation between the input and the target can  be calculated:  h  =  E tz    =  0.5  +  0.5    1–   1  1 1   1 1–  =  .  0 1  Finally, the input correlation matrix   R   is  10  10-27   10 Widrow-Hoff Learning  R  =  E zzT     =  p1p1  T 0.5   p2p2 +  T 0.5    =  0.5   1 1  1 1  +  1 1–  =  1 1–  1 0 0 1  Therefore the mean square error performance index is  F x   =  c  –  2xTh  +  xTRx  =  1  –  2 w1 1 w1 2  +  w1 1 w1 2  0 1  1 0 0 1  w1 1 w1 2  =  1  –  2w1 2  +  2  w1 1  +  2  w1 2  The Hessian matrix of  2. Therefore the contours of the performance surface will be circular. To  find the center of the contours  the minimum point , we need to solve Eq.   10.18 :  , has both eigenvalues at   , which is equal to   F x   2R  x  =  R 1– h  =  1–  1 0 0 1  =  0 1  .  0 1  Thus we have a minimum at  square error performance surface is shown in Figure P10.4.  w1 1  w1 2  0=  1=  . The resulting mean   ,   w1 2  4  3  2  1  0  -1  -2 -3  10-28  -2  -1  1  2  3  0  w1 1  Figure P10.4  Contour Plot of   F x    for Problem P10.3   Solved Problems  P10.4 Consider the system of Problem P10.3 again. Train the network us-  ing the LMS algorithm, with the initial guess set to zero and a  learning rate  . Apply each reference pattern only once  during training. Draw the decision boundary at each stage.  0.25    =  Assume the input vector  weights are calculated as follows:  p1   is presented first. The output, error and new   a > 0  1w  W 1  W 0   =  +  2e 0 p 0 T  =  0 0  +  2  1   1  1 1 ---  4  =  1 --- 2  1 --- 2  .  The decision boundary associated with these weights is shown to the left.  Now apply the second input vector:  a 0   =  purelin 0 0  0=  ,  1 1  e 0   =  t 0   –  a 0   =  1  1=–  0  ,  a 1   =  purelin      1 --- 2  1 --- 2  1 1–      0=  ,  e 1   =  t 1   –  a 1   =  1–  1–=–  0  ,  W 2  W 1   =  +  2e 1 p 1 T  =  1 --- 2  1 --- 2  +  2  1   ---  4  1–   1 1–  =  0 1  .  The decision boundary associated with these weights is shown to the left.  This boundary shows real promise. It is exactly halfway between the input  vectors. You might verify for yourself that each input vector, when applied,  yields its correct associated target.  What set of weights would be optimal  if the targets associated with the two input vectors were exchanged?   P10.5 Now consider the convergence of the system of Problems P10.3 and  P10.4. What is the maximum stable learning rate for the LMS algo- rithm?  The LMS convergence is determined by the learning rate  R not exceed the reciprocal of the largest eigenvalue of  this limit by finding these eigenvalues using MATLAB.    , which should  . We can determine   10  » 2 + 2 ans =       4  a < 0  1w  a > 0  a < 0  10-29   10 Widrow-Hoff Learning  [V,D] = eig  R  V =  D=  1     0 0     1  1 0   0 1  The diagonal terms of matrix D give the eigenvalues, 1 and 1, while the col- umns of V show the eigenvectors. Note, incidentally, that the eigenvectors  have the same direction as those shown in Figure P10.4.   The largest eigenvalue,  at  max  1=  , sets the upper limit on the learning rate    1 max    =  1 1  =  1  .  The suggested learning rate in the previous problem was 0.25, and you  found  perhaps  that the LMS algorithm converged quickly. What do you  suppose happens when the learning rate is 1.0 or larger?   P10.6 Consider the adaptive filter ADALINE shown in Figure P10.5. The  purpose of this filter is to predict the next value of the input signal  from the two previous values. Suppose that the input signal is a  stationary random process, with autocorrelation function given by  Cy n   =  E y k y k  n+        Cy 0   3=  ,   Cy 1   1–=  ,   Cy 2   1–=  .  i. Sketch the contour plot of the performance index  mean   square error .   ii. What is the maximum stable value of the learning rate          for the LMS algorithm?  iii. Assume that a very small value is used for     . Sketch the path   of the weights for the LMS algorithm, starting with initial  guess  . Explain your procedure for sketch- = ing the path.  W 0   0.75 0  T  10-30   Solved Problems  Inputs  ADALINE  y k    cid:0  cid:0 D  t k  = y k   +  SxR   cid:0 Σ  a k  n k   cid:0  cid:0  -  e k   w1,1   cid:0 D w1,2  cid:0   a k  = w1,1 y k - 1  + w1,2 y k - 2   Figure P10.5  Adaptive Predictor  i. To sketch the contour plot we first need to find the performance index  and the eigenvalues and eigenvectors of the Hessian matrix. First note that  the input vector is given by  Now consider the performance index. Recall from Eq.  10.12  that   z k   =  p k   =  1– y k 2– y k     .  F x   =  c  –  2xTh  +  xTRx  .  We can calculate the constants in the performance index as shown below:  c  =  E t2 k     E y2 k  =     Cy 0  =  3=  ,  R  =  E zzT     =  E  y2 k 1– y k  1– y k    2–    2–    y k 1– y2 k  y k 2–    =  Cy 0  Cy 1  Cy 1  Cy 0   =  3 1–  1– 3  h  =  E t z  =  E y k y k y k y k  1– 2–     =  Cy 1  Cy 2   =  .  1– 1–  10  The optimal weights are  10-31   10 Widrow-Hoff Learning  x  =  R 1– h  =  1–  3 1–  1– 3  1– 1–  =  3 8 4 8  1 8 3 8  1– 1–  =  1 2– 1 2–  .  The Hessian matrix is  2F x   =  A  =  2R  =  6 2–  2– 6  .  Now we can get the eigenvalues:  A I–  =  6 – 2–  2– 6 –  =  2  –  12  +  32  =   8–    4–    .  Thus,  To find the eigenvectors we use  1  =  4           8=  .  2  A I–  v  0=  .  For   1  4=  ,  and for   2  8=  ,  2 2–  2– 2  v1  =  0        v1  =  2– 2–  2– 2–  v2  =  0        v2  =  ,  .  1– 1–  1– 1  Therefore the contours of   will be elliptical, with the long axis of each  ellipse along the first eigenvector, since the first eigenvalue has the small- est magnitude. The ellipses will be centered at  shown in Figure P10.6.  . The contour plot is   F x   x  10-32   Solved Problems  w1 2  2  1  0  -1  -2 -2  1  0.5  -0.5  w1 2  2 x  0  10-33  -1  1  2  0  w1 1  Figure P10.6  Error Contour for Problem P10.6  You might check your sketch by writing a MATLAB M-file to plot the con- tours.  ii. The maximum stable learning rate is the reciprocal of the maximum  eigenvalue of  , which is the same as twice the reciprocal of the largest  eigenvalue of the Hessian matrix   2F x   A=  R  :   2 max    =  2 8  =  0.25  .  iii. The LMS algorithm is approximate steepest descent, so the trajectory  for small learning rates will move perpendicular to the contour lines, as  shown in Figure P10.7.  Contour Plot  -1 -1  -0.5  0.5  1  0 x1  w1 1  Figure P10.7  LMS Weight Trajectory  10   10 Widrow-Hoff Learning  P10.7 The pilot of an airplane is talking into a microphone in his cockpit.  The sound received by the air traffic controller in the tower is gar- bled because the pilot’s voice signal has been contaminated by en- gine noise that reaches his microphone. Can you suggest an  adaptive ADALINE filter that might help reduce the noise in the  signal received by the control tower? Explain your system.  The engine noise that has been inadvertently added to the microphone in- put can be minimized by using the adaptive filtering system shown in Fig- ure P10.8. A sample of the engine noise is supplied to an adaptive filter  through a microphone in the cockpit. The desired output of the filter is the  contaminated signal coming from the pilot’s microphone. The filter at- tempts to reduce the “error” signal to a minimum. It can do this only by  subtracting the component of the contaminated signal that is linearly cor- related with the engine noise  and presumably uncorrelated with the pilot’s  voice . The result is that a clear voice signal is sent to the control tower, in  spite of the fact that the engine noise got into the pilot’s microphone along  with his voice signal.  See [WiSt85] for discussion of similar noise cancella- tion systems.   Restored Signal  e  "Error"  Contaminated       Signal  t  +  - Adaptively Filtered  Noise to Cancel  Contamination  Pilot  s  Voice  Signal  Contaminating        Noise  m  Noise Path     Filter  v  Airplane Engine Noise Source  a  Adaptive    Filter   Figure P10.8  Filtering Engine Noise from Pilot’s Voice Signal  P10.8 This is a classification problem like that described in Problems   P4.3 and P4.5, except that here we will use an ADALINE network  and the LMS learning rule rather than the perceptron learning  rule. First we will describe the problem.  We have a classification problem with four classes of input vector.  The four classes are  10-34   Solved Problems          1 1  1– 2  1 2      2– 1      class 1:  p1  =    p2  =  , class 2:   p3  =    p4  =  2 1–  1– 1–  2 0  ,      2– 2–  .      class 3:  p5  =    p6  =  , class 4:   p7  =    p8  =  Train an ADALINE network to solve this problem using the LMS  learning rule. Assume that each pattern occurs with probability  1 8  .   indicate class 1 vectors, the light squares   Let’s begin by displaying the input vectors, as in Figure P10.9. The light   indicate class 2 vec- circles  tors, the dark circles   indicate class 3 vectors, and the dark squares   in- dicate class 4 vectors. These input vectors can be plotted as shown in  Figure P10.9.          1  2  Figure P10.9  Input Vectors for Problem P10.8  We will use target vectors similar to the ones we introduced in Problem  P4.3, except that we will replace any targets of 0 by targets of -1.  The per- ceptron could only output 0 or 1.  Thus, the training set will be:          1 1  2 0  p1  =    t1  =  p2  =    t2  =  1– 1–          1– 1–          p3  =  p4  =    t4  =  1– 1          p5  =    t5  =  1 1–          p6  =  1 2  1– 2    t3  =    t6  =  1– 1      1 1–          p7  =  1– 1–    t7  =  1 1          p8  =  2– 2–    t8  =  2 1–  2– 1  1 1      10  3  4  10-35   10 Widrow-Hoff Learning  Also, we will begin as in Problem P4.5 with the following initial weights  and biases:  W 0   =  ,   b 0   =  1 0 0 1  .  1 1  Now we are almost ready to train an ADALINE network using the LMS  rule. We will use a learning rate of  vectors in order according to their subscripts. The first iteration is  , and we will present the input   0.04    =  a 0   =  purelin  W 0 p 0  b 0     +     =  purelin  1 0 0 1      +  1 1      1 1  =  2 2  e 0   =  t 0   –  a 0   =  1– 1–  –  2 2  =  3– 3–  W 1  W 0   =  +  2e 0 pT 0   =  1 0 0 1  +   2 0.04    1 1  =  –  0.76 0.24 0.24 0.76  –  3– 3–  1 1  b 1   =  b 0   +  2e 0   =  +   2 0.04    3– 3–  =  0.76 0.76  .  The second iteration is  a 1   =  purelin  W 1 p 1  b 1     +     =  purelin      –  0.76 0.24 0.76 0.24  –  1 2  +  0.76 0.76      =  1.04 2.04  e 1   =  t 1   –  a 1   =  1– 1–  –  1.04 2.04  =  – –  2.04 3.04  W 2  W 1   =  +  2e 1 pT 1   =  –  0.76 0.24 0.24 0.76  –  +   2 0.04    – –  2.04 3.04  1 2  =  0.5968 0.4832 –  0.5664 – 0.2736  10-36   Solved Problems  b 2   =  b 1   +  2e 1   =  +   2 0.04    0.76 0.76  – –  2.04 3.04  =  0.5968 0.5168  .  If we continue until the weights converge we find  W     =  0.5948 – 0.1667  – –  0.0523 0.6667  ,   b     =  0.0131 0.1667  .  The resulting decision boundaries are shown in Figure P10.10. Compare  this result with the final decision boundaries created by the perceptron  learning rule in Problem P4.5  Figure P4.7 . The perceptron rule stops  training when all the patterns are classified correctly. The LMS algorithm  moves the boundaries as far from the patterns as possible.  1  2  3  4  Figure P10.10  Final Decision Boundaries for Problem P10.8  P10.9 Repeat the work of Widrow and Hoff on a pattern recognition   problem from their classic 1960 paper [WiHo60]. They wanted to  design a recognition system that would classify the six patterns  shown in Figure P10.11.  Figure P10.11  Patterns and Their Classification Targets  10  Patterns  Targets  T  60  G  0  F  -60  10-37   10 Widrow-Hoff Learning  These patterns represent the letters T, G and F, in an original form  on the top and in a shifted form on the bottom. The targets for  these letters  in their original and shifted forms  are +60, 0 and -60,  respectively.  The values of 60, 0 and -60 were nice for use on the  face of a meter that Widrow and Hoff used to display their network  output.  The objective is to train a network so that it will classify  the six patterns into the appropriate T, G or F groups.  The blue squares in the letters will be assigned the value +1, and the white  squares will be assigned the value -1. First we convert each of the letters  into a single 16-element vector. We choose to do this by starting at the up- per left corner, going down the left column, then going down the second col- umn, etc. For example, the vector corresponding to the unshifted letter T is  p1  =  1 1–  1–  1– 1 1 1 1 1 1–  1–  1–  1–  1–  1–  1–  T  We have such an input vector for each of the six letters.  The ADALINE network that we will use is shown in Figure P10.12.  Input  Linear Neuron   1 x 16  p  cid:0  cid:0 W 16 x 1  cid:0  cid:0   cid:0  cid:0   b 1 x 1  1  16  a 1 x 1   cid:0  cid:0  n  cid:0  cid:0  1 x 1  cid:0  cid:0   1  a = purelin  Wp + b   Figure P10.12  Adaptive Pattern Classifier   Widrow and Hoff built their own machine to realize this ADALINE. Ac- cording to them, it was “about the size of a lunch pail.”    Now we will present the six vectors to the network in a random sequence  and adjust the weights of the network after each presentation using the  LMS algorithm with a learning rate of  . After each adjustment of  weights, all six vectors will be presented to the network to generate their  outputs and corresponding errors. The sum of the squares of the errors will  be examined as a measure of the quality of the network.  0.03    =  Figure P10.13 illustrates the convergence of the network. The network is  trained to recognize these six characters in about 60 presentations, or  roughly 10 for each of the possible input vectors.  10-38   Solved Problems  The results shown in Figure P10.13 are quite like those obtained and pub- lished by Widrow and Hoff some 35 years ago. Widrow and Hoff did good  science. One can indeed duplicate their work, even decades later  without  a lunch pail .  x 104  2.5  Convergence of Error  10  20  30  40  50  60  70  80  90  100  Time Steps  Figure P10.13  Error Convergence with Learning Rate of 0.03  To experiment with this character recognition problem, use the MATLAB®  Neural Network Design Demonstration Linear Pattern Classification   nnd10lc . Notice the sensitivity of the network to noise in the input pattern.  2  1  1.5  r o r r  E    .     q S m m u S  0.5  0 0  10-39  10   10 Widrow-Hoff Learning  Epilogue  In this chapter we have presented the ADALINE neural network and the  LMS learning rule. The ADALINE network is very similar to the percep- tron network of Chapter 4, and it has the same fundamental limitation: it  can only classify linearly separable patterns. In spite of this limitation on  the network, the LMS algorithm is in fact more powerful than the percep- tron learning rule. Because it minimizes mean square error, the algorithm  is able to create decision boundaries that are more robust to noise than  those of the perceptron learning rule.  The ADALINE network and the LMS algorithm have found many practical  applications. Even though they were first presented in the late 1950s, they  are still very much in use in adaptive filtering applications. For example,  echo cancellers using the LMS algorithm are currently employed on many  long distance telephone lines.  Chapter 14 provides more extensive cover- age of dynamic networks, which are widely used for filtering, prediction  and control.   In addition to its importance as a practical solution to many adaptive fil- tering problems, the LMS algorithm is also important because it is the fore- runner of the backpropagation algorithm, which we will discuss in  Chapters 11 through 14. Like the LMS algorithm, backpropagation is an  approximate steepest descent algorithm that minimizes mean square er- ror. The only difference between the two algorithms is in the manner in  which the derivatives are calculated. Backpropagation is a generalization  of the LMS algorithm that can be used for multilayer networks. These more  complex networks are not limited to linearly separable problems. They can  solve arbitrary classification problems.  10-40   Further Reading  Further Reading  [AnRo89]  J. A. Anderson, E. Rosenfeld, Neurocomputing: Founda- tions of Research, Cambridge, MA: MIT Press, 1989.  [StDo84]  [WiHo60]  Neurocomputing is a fundamental reference book. It con- tains over forty of the most important neurocomputing  writings. Each paper is accompanied by an introduction  that summarizes its results and gives a perspective on the  position of the paper in the history of the field.  W. D. Stanley, G. R. Dougherty, R. Dougherty, Digital Sig- nal Processing, Reston VA: Reston, 1984  B. Widrow, M. E. Hoff, “Adaptive switching circuits,” 1960  IRE WESCON Convention Record, New York: IRE Part 4,  pp. 96–104.  This seminal paper describes an adaptive perceptron-like  network that can learn quickly and accurately. The authors  assumed that the system had inputs, a desired output clas- sification for each input, and that the system could calcu- late the error between the actual and desired output. The  weights are adjusted, using a gradient descent method, so  as to minimize the mean square error.  Least mean square  error or LMS algorithm.   This paper is reprinted in [AnRo88].  [WiSt 85]  B. Widrow and S. D. Stearns, Adaptive Signal Processing,  Englewood Cliffs, NJ: Prentice-Hall, 1985.  This informative book describes the theory and application  of adaptive signal processing. The authors include a review  of the mathematical background that is needed, give de- tails on their adaptive algorithms, and then discuss practi- cal information about many applications.  [WiWi 88]  B. Widrow and R. Winter, “Neural nets for adaptive filter- ing and adaptive pattern recognition,” IEEE Computer  Magazine, March 1988, pp. 25–39.  This is a particularly readable paper that summarizes ap- plications of adaptive multilayer neural networks. The net- works are applied to system modeling, statistical  prediction, echo cancellation, inverse modeling and pattern  recognition.   10  10-41   10 Widrow-Hoff Learning  Exercises  E10.1 An adaptive filter ADALINE is shown in Figure E10.1. Suppose that the   weights of the network are given by  w1 1  1=  ,   w1 2  4–=  ,   w1 3  2=  ,  and the input to the filter is    y k     =      0 0 0 1 1 2 0 0                       .  Find the response     a k      of the filter.   Inputs  ADALINE  y k    cid:0 D  cid:0   cid:0 D  w1,1  w1,2  w1,3  SxR   cid:0 Σ  n k  a k   cid:0  cid:0   a k  = purelin  Wp k  + b   Figure E10.1  Adaptive Filter ADALINE for Exercise E10.1  E10.2 In Figure E10.2 two classes of patterns are given.   i. Use the LMS algorithm to train an ADALINE network to distin-  guish between class I and class II patterns  we want the network to  identify horizontal and vertical lines .   ii. Can you explain why the ADALINE network might have difficulty   with this problem?  Class I  Class II  Figure E10.2  Pattern Classification Problem for Exercise E10.2  10-42   E10.3 Suppose that we have the following two reference patterns and their tar-  gets:  P10.3:  p1  =    t1  1=  ,   p2  =    t2  1–=          1 1–  .      In Problem P10.3 these input vectors to an ADALINE were assumed to oc- cur with equal probability. Now suppose that the probability of vector    is 0.75 and that the probability of vector  probabilities change the mean square error surface? If yes, what does the  surface look like now? What is the maximum stable learning rate?  p1  is 0.25. Does this change of   p2  E10.4 In this exercise we will modify the reference pattern    from Problem   p1  =    t1  1=  ,   p2  =    t2  1–=          1– 1–  p2  .      Exercises          1 1  1 1  i. Assume that the patterns occur with equal probability. Find the   mean square error and sketch the contour plot.  ii. Find the maximum stable learning rate.  iii. Write a MATLAB M-file to implement the LMS algorithm for this  problem. Take 40 steps of the algorithm for a stable learning rate.  Use the zero vector as the initial guess. Sketch the trajectory on the  contour plot.  iv. Take 40 steps of the algorithm after setting the initial values of both   parameters to 1. Sketch the final decision boundary.  v. Compare the final parameters from parts  iii  and  iv . Explain your   results.  E10.5 We again use the reference patterns and targets from Problem P10.3, and  assume that they occur with equal probability. This time we want to train  an ADALINE network with a bias. We now have three parameters to find:  w1 1   and   w1 2  ,   b  .  i. Find the mean square error and the maximum stable learning rate.  ii. Write a MATLAB M-file to implement the LMS algorithm for this  problem. Take 40 steps of the algorithm for a stable learning rate.  Use the zero vector as the initial guess. Sketch the final decision  boundary.  iii. Take 40 steps of the algorithm after setting the initial values of all   parameters to 1. Sketch the final decision boundary.  iv. Compare the final parameters and the decision boundaries from   parts  iii  and  iv . Explain your results.  10  10-43  » 2 + 2 ans =       4  » 2 + 2 ans =       4   10 Widrow-Hoff Learning  E10.6 We have two categories of vectors. Category I consists of  Category II consists of        1 1  1– 2  .            0 1–  4– 1  .      We want to train a single-neuron ADALINE network without a bias to rec- ognize these categories  t = 1 for Category I and t = -1 for Category II . As- sume that each pattern occurs with equal probability.  i. Draw the network diagram.  ii. Take four steps of the LMS algorithm, using the zero vector as the  initial guess.  one pass through the four vectors above - present each  vector once . Use a learning rate of 0.1.  iii. What are the optimal weights?  iv. Sketch the optimal decision boundary.   v. How do you think the boundary would change if the network were  allowed to have a bias? If the boundary would change, indicate the  approximate new position on your sketch of part iv. You do not need  to perform any calculations here - just explain your reasoning.  E10.7 Suppose that we have the following three reference patterns and their tar-  gets:      3 6          6 3  p1  =  t1  =  75  ,   p2  =  t2  =  75  ,   p3  =  t3  =  75–          6– 3  .      Each pattern is equally likely.  i. Draw the network diagram for an ADALINE network with no bias   that could be trained on these patterns.  ii. We want to train the ADALINE network with no bias using these  patterns. Sketch the contour plot of the mean square error perfor- mance index.  iii. Find the maximum stable learning rate for the LMS algorithm.  10-44   Exercises  iv. Sketch the trajectory of the LMS algorithm on your contour plot. As- sume a very small learning rate, and start with all weights equal to  zero. This does not require any calculations.  E10.8 Suppose that we have the following two reference patterns and their tar-  gets:      1 2  p1  =  t1  =  1–  ,   p2  =  t2  =  1          2– 1  .      The probability of vector  want to train an ADALINE network without a bias on this data set.   is 0.5 and the probability of vector   p1  p2   is 0.5.We   i. Sketch the contour plot of the mean square error performance in-  dex.  ii. Sketch the optimal decision boundary.  iii. Find the maximum stable learning rate.  iv. Sketch the trajectory of the LMS algorithm on your contour plot. As-  sume a very small learning rate, and start with initial weights  W 0   =  .  0 1  E10.9 We have the following input target pairs:      4 2          2 4–  p1  =    t1  5=  ,   p2  =    t2  2–=  ,   p3  =    t3  9=          4– 4  .      The first two pair each occurs with probability of 0.25, and the third pair  occurs with probability 0.5. We want to train a single-neuron ADALINE  network without a bias to perform the desired mapping.  i. Draw the network diagram.  ii. What is the maximum stable learning rate?  iii. Perform one iteration of the LMS algorithm. Apply the input    and     =  0.1  p1 . Start from the initial weights   use a learning rate of  x0  0 0  =  .  T  10  10-45   10 Widrow-Hoff Learning  E10.10 Repeat E10.9 for the following input target pairs:      p1  =  2 4–    t1  1=  ,   p2  =    t2  1–=  ,   p3  =    t3  1=          4 2  .              4– 4  The first two pair each occurs with probability of 0.25, and the third pair  occurs with probability 0.5. We want to train a single-neuron ADALINE  network without a bias to perform the desired mapping.  E10.11 We want to train a single-neuron ADALINE network without a bias, using  the following training set, which categorizes vectors into two classes. Each  pattern occurs with equal probability.      p1  =  1– 2    t1  1–=  p2  =    t2  1–=  p3  =    t3  1=  p4  =    t4  1=          0 1–          1– 0              2 1–  i. Draw the network diagram.  ii. Take one step of the LMS algorithm  present    only  starting from   p1  the initial weight   W 0   =  . Use a learning rate of 0.1.  0 0  iii. What are the optimal weights? Show all calculations.  iv. Sketch the optimal decision boundary.  v. How do you think the boundary would change if the network were  allowed to have a bias? Indicate the approximate new position on  your sketch of part iv.  vi. What is the maximum stable learning rate for the LMS algorithm?  vii. Sketch the contour plot of the mean square error performance sur-  face.  viii. On your contour plot of part vii, sketch the path of the LMS algo- rithm for a very small learning rate  e.g., 0.001  starting from the  initial condition  . This does not require any calcula- tions, but explain how you obtained your answer.  W 0   2 0  =  E10.12 Suppose that we have the following three reference patterns and their tar-  gets:      2 4          4 2  p1  =  t1  =  26  ,   p2  =  t2  =  26  ,   p3  =  t3  =  26–          2– 2–  .      10-46   Exercises  The probability of vector  the probability of vector   p1 p3   is 0.25, the probability of vector   is 0.5.  p2   is 0.25 and   i. Draw the network diagram for an ADALINE network with no bias   that could be trained on these patterns.  ii. Sketch the contour plot of the mean square error performance in-  dex.  iii. Show the optimal decision boundary  for the weights that minimize  mean square error  and verify that it separates the patterns into the  appropriate categories.  iv. Find the maximum stable learning rate for the LMS algorithm. If   the target values are changed from 26 and -26 to 2 and -2, how  would this change the maximum stable learning rate?  v. Perform one iteration of the LMS algorithm, starting with all   weights equal to zero, and presenting input vector  ing rate of   0.5    =  .  p1  . Use a learn-  vi. Sketch the trajectory of the LMS algorithm on your contour plot. As- sume a very small learning rate, and start with all weights equal to  zero.  E10.13 Consider the adaptive predictor in Figure E10.3.  y k    cid:0  cid:0 D  Inputs  ADALINE  t k  = y k   +  SxR   cid:0 Σ  a k  n k   cid:0  cid:0  -  e k   w1,1   cid:0 D w1,2  cid:0   a k  = w1,1 y k - 1  + w1,2 y k - 2   Figure E10.3  Adaptive Predictor for Exercise E10.13  Assume that   y k    is a stationary process with autocorrelation function  Cy n   =  E y k  y k  n+            .  i. Write an expression for the mean square error in terms of   Cy n   .  ii. Give a specific expression for the mean square error when   10  10-47   10 Widrow-Hoff Learning  y k   =  sin     k  ------  5  .  iii. Find the eigenvalues and eigenvectors of the Hessian matrix for the  mean square error. Locate the minimum point and sketch a rough  contour plot.  iv. Find the maximum stable learning rate for the LMS algorithm.  v. Take three steps of the LMS algorithm by hand, using a stable   learning rate. Use the zero vector as the initial guess.  vi. Write a MATLAB M-file to implement the LMS algorithm for this  problem. Take 40 steps of the algorithm for a stable learning rate  and sketch the trajectory on the contour plot. Use the zero vector as  the initial guess. Verify that the algorithm is converging to the op- timal point.  vii. Verify experimentally that the algorithm is unstable for learning   rates greater than that found in part  iv .  E10.14 Repeat Problem P10.9, but use the numerals “1”, “2” and “4”, instead of the  letters “T”, “G” and “F”. Test the trained network on each reference pattern  and on noisy patterns. Discuss the sensitivity of the network.  Use the Neu- ral Network Design Demonstration Linear Pattern Classification  nnd10lc .   » 2 + 2 ans =       4  10-48   Objectives  11 Backpropagation  Objectives Theory and Examples  Multilayer Perceptrons  Pattern Classification Function Approximation  The Backpropagation Algorithm  Performance Index Chain Rule Backpropagating the Sensitivities Summary  Example Batch vs. Incremental Training Using Backpropagation  Choice of Network Architecture Convergence Generalization  Summary of Results Solved Problems Epilogue Further Reading Exercises  11-1 11-2 11-2 11-3 11-4 11-7 11-8 11-9 11-11 11-13 11-14 11-17 11-18 11-18 11-20 11-22 11-25 11-27 11-41 11-42 11-44  Objectives  In this chapter we continue our discussion of performance learning, which  we began in Chapter 8, by presenting a generalization of the LMS algo- rithm of Chapter 10. This generalization, called backpropagation, can be  used to train multilayer networks. As with the LMS learning law, back- propagation is an approximate steepest descent algorithm, in which the  performance index is mean square error. The difference between the LMS  algorithm and backpropagation is only in the way in which the derivatives  are calculated. For a single-layer linear network the error is an explicit lin- ear function of the network weights, and its derivatives with respect to the  weights can be easily computed. In multilayer networks with nonlinear  transfer functions, the relationship between the network weights and the  error is more complex. In order to calculate the derivatives, we need to use  the chain rule of calculus. In fact, this chapter is in large part a demonstra- tion of how to use the chain rule.  11-1  11   11 Backpropagation  Theory and Examples  The perceptron learning rule of Frank Rosenblatt and the LMS algorithm  of Bernard Widrow and Marcian Hoff were designed to train single-layer  perceptron-like networks. As we have discussed in previous chapters, these  single-layer networks suffer from the disadvantage that they are only able  to solve linearly separable classification problems. Both Rosenblatt and  Widrow were aware of these limitations and proposed multilayer networks  that could overcome them, but they were not able to generalize their algo- rithms to train these more powerful networks.   Apparently the first description of an algorithm to train multilayer net- works was contained in the thesis of Paul Werbos in 1974 [Werbo74]. This  thesis presented the algorithm in the context of general networks, with  neural networks as a special case, and was not disseminated in the neural  network community. It was not until the mid 1980s that the backpropaga- tion algorithm was rediscovered and widely publicized. It was rediscovered  independently by David Rumelhart, Geoffrey Hinton and Ronald Williams  [RuHi86], David Parker [Park85], and Yann Le Cun [LeCu85]. The algo- rithm was popularized by its inclusion in the book Parallel Distributed Pro- cessing [RuMc86], which described the work of the Parallel Distributed  Processing Group led by psychologists David Rumelhart and James Mc- Clelland. The publication of this book spurred a torrent of research in neu- ral networks. The multilayer perceptron, trained by the backpropagation  algorithm, is currently the most widely used neural network.  In this chapter we will first investigate the capabilities of multilayer net- works and then present the backpropagation algorithm.  We first introduced the notation for multilayer networks in Chapter 2. For  ease of reference we have reproduced the diagram of the three-layer per- ceptron in Figure 11.1. Note that we have simply cascaded three percep- tron networks. The output of the first network is the input to the second  network, and the output of the second network is the input to the third net- work. Each layer may have a different number of neurons, and even a dif- ferent transfer function. Recall from Chapter 2 that we are using  superscripts to identify the layer number. Thus, the weight matrix for the  first layer is written as   and the weight matrix for the second layer is  written   W1  .   W2  To identify the structure of a multilayer network, we will sometimes use  the following shorthand notation, where the number of inputs is followed  by the number of neurons in each layer:  R S1–  S2–  S3–  .   11.1   11-2  Multilayer Perceptrons   Multilayer Perceptrons  w 11,1  a11  w 21,1  a21  w 31,1  Inputs  First Layer  p1  p2  p3  pR  1  b11   cid:0  cid:0 Σ n11  cid:0  cid:0   cid:0  cid:0 Σ n12  cid:0  cid:0   cid:0  cid:0 Σ n1S 1  cid:0  cid:0  b1S 1  b12  1   cid:0 f 1  cid:0   cid:0 f 1  cid:0   cid:0  f 1  cid:0   1  a12  a1S 1  w 1S 1, R  Second Layer  1  b21   cid:0  cid:0 Σ n21  cid:0  cid:0   cid:0  cid:0 Σ n22  cid:0  cid:0   cid:0  cid:0 Σ n2S 2  cid:0  cid:0  b2S 2  b22  1   cid:0 f 2  cid:0   cid:0 f 2  cid:0   cid:0  f 2  cid:0   1  a22  a2S 2  w 2S 2, S 1  w 3S 3, S 2  Third Layer  1  b31   cid:0  cid:0 Σ n31  cid:0  cid:0   cid:0  cid:0 Σ n32  cid:0  cid:0   cid:0  cid:0 Σ n3S 3  cid:0  cid:0   b3S 3  b32  1   cid:0 f 3  cid:0   cid:0 f 3  cid:0   cid:0  f 3  cid:0   1  a31  a32  a3S 3  a1 = f 1  W1p + b1   a2 = f 2  W2a1 + b2   a3 = f 3  W3a2 + b3   2 2+  p4  p3  p2  p1  a3 = f 3  W3f 2  W2f 1  W1p + b1  + b2  + b3   Figure 11.1  Three-Layer Network  Let’s now investigate the capabilities of these multilayer perceptron net- works. First we will look at the use of multilayer networks for pattern clas- sification, and then we will discuss their application to function  approximation.  Pattern Classification To illustrate the capabilities of the multilayer perceptron for pattern clas- sification, consider the classic exclusive-or  XOR  problem. The input tar- get pairs for the XOR gate are      0 0  p1  =    t1  0=  p2  =    t2  1=  p3  =    t3  1=  p4  =    t4  0=          0 1          1 0          1 1  .      This problem, which is illustrated graphically in the figure to the left, was  used by Minsky and Papert in 1969 to demonstrate the limitations of the  single-layer perceptron. Because the two categories are not linearly sepa- rable, a single-layer perceptron cannot perform the classification.  A two-layer network can solve the XOR problem. In fact, there are many  different multilayer solutions. One solution is to use two neurons in the  first layer to create two decision boundaries. The first boundary separates  p1 . Then  the second layer is used to combine the two boundaries together using an    from the other patterns, and the second boundary separates   p4  11-3  11   11 Backpropagation  AND operation. The decision boundaries for each first-layer neuron are  shown in Figure 11.2.  1w1  2w1  Layer 1 Neuron 1  Layer 1 Neuron 2  Figure 11.2  Decision Boundaries for XOR Network  The resulting two-layer, 2-2-1 network is shown in Figure 11.3. The overall  decision regions for this network are shown in the figure in the left margin.  The shaded region indicates those inputs that will produce a network out- put of 1.  Inputs  p1  p2  Individual Decisions  cid:0  cid:0   cid:0  cid:0  Σ a11 n11 2  cid:0  cid:0   cid:0  cid:0  2 -1  cid:0  cid:0 Σ  a12  cid:0  cid:0   n12  1  -1 -1  1.5  1  AND Operation   cid:0  cid:0   cid:0  cid:0  1 n21 a21 Σ  cid:0  cid:0   cid:0  cid:0  -1.5 1  1  Figure 11.3  Two-Layer XOR Network  See Problems P11.1 and P11.2 for more on the use of multilayer networks  for pattern classification.  Function Approximation Up to this point in the text we have viewed neural networks mainly in the  context of pattern classification. It is also instructive to view networks as  function approximators. In control systems, for example, the objective is to  find an appropriate feedback function that maps from measured outputs to  control inputs. In adaptive filtering  Chapter 10  the objective is to find a  function that maps from delayed values of an input signal to an appropri- ate output signal. The following example will illustrate the flexibility of the  multilayer perceptron for implementing functions.  11-4   Multilayer Perceptrons  2 2+  Consider the two-layer, 1-2-1 network shown in Figure 11.4. For this exam- ple the transfer function for the first layer is log-sigmoid and the transfer  function for the second layer is linear. In other words,  f1 n   =   and   f2 n   n=  .  1 ---------------- e n–+ 1   11.2   Input  Log-Sigmoid Layer  Linear Layer  p  w11,1  w12,1  b11   cid:0  cid:0 Σ n11  cid:0  cid:0   cid:0  cid:0 Σ n12  cid:0  cid:0   b12  1   cid:0  cid:0  a11  cid:0  cid:0   cid:0  cid:0  a12  cid:0  cid:0   1  w21,1   cid:0  cid:0 Σ n2  cid:0  cid:0  w21,2  b2   cid:0   cid:0   1  a2  a1 = logsig  W1p + b1   a2 = purelin  W2a1 + b2   Figure 11.4  Example Function Approximation Network  Suppose that the nominal values of the weights and biases for this network  are  1  w1 1  10=  ,   1  w2 1  10=  ,   1 b1  10–=  ,   1 b2  10=  ,  2  w1 1  1=  ,   2  w1 2  1=  ,   b2  0=  .  The network response for these parameters is shown in Figure 11.5, which  plots the network output  .    is varied over the range    as the input   a2  2  2–  p      Notice that the response consists of two steps, one for each of the log-sig- moid neurons in the first layer. By adjusting the network parameters we  can change the shape and location of each step, as we will see in the follow- ing discussion.  The centers of the steps occur where the net input to a neuron in the first  layer is zero:  1 w1 1 1 p = n1  1+ b1  =  0    p  =  –   11.3   1 b1 1---------- w1 1  =  –  10– --------- 10  =  1  ,  11-5  11   11 Backpropagation  1 w2 1 1 p = n2  1+ b2  =  0    p  =  –   11.4   1 b2 1---------- w2 1  =  10 ------– 10  =  1–  .  The steepness of each step can be adjusted by changing the network  weights.   3  2  1  0  a2  11-6  -1 -2  -1  1  2  0 p  Figure 11.5  Nominal Response of Network of Figure 11.4  Figure 11.6 illustrates the effects of parameter changes on the network re- sponse. The blue curve is the nominal response. The other curves corre- spond to the network response when one parameter at a time is varied over  the following ranges:  1–    2  w1 1    1  ,   1–    2  w1 2    1  ,   0    1 b2    20  ,   1–    b2    1  .   11.5   Figure 11.6  a  shows how the network biases in the first  hidden  layer can  be used to locate the position of the steps. Figure 11.6  b  illustrates how  the weights determine the slope of the steps. The bias in the second  out- put  layer shifts the entire network response up or down, as can be seen in  Figure 11.6  d .  From this example we can see how flexible the multilayer network is. It  would appear that we could use such networks to approximate almost any  function, if we had a sufficient number of neurons in the hidden layer. In  fact, it has been shown that two-layer networks, with sigmoid transfer  functions in the hidden layer and linear transfer functions in the output  layer, can approximate virtually any function of interest to any degree of  accuracy, provided sufficiently many hidden units are available  see  [HoSt89] .  To experiment with the response of this two-layer network, use the MAT- LAB® Neural Network Design Demonstration Network Function  nnd11nf .   The Backpropagation Algorithm  1 b2  2  w1 2  3  2  1  0  3  2  1  0  2  w1 1  b2  3  2  1  0  3  2  1  0  -1 -2  -1  0  a   1  2  -1 -2  -1  0  b   1  2  -1 -2  -1  1  2  -1 -2  -1  0  c   1  2  0  d   Figure 11.6  Effect of Parameter Changes on Network Response  Now that we have some idea of the power of multilayer perceptron net- works for pattern recognition and function approximation, the next step is  to develop an algorithm to train such networks.  The Backpropagation Algorithm  It will simplify our development of the backpropagation algorithm if we use  the abbreviated notation for the multilayer network, which we introduced  in Chapter 2. The three-layer network in abbreviated notation is shown in  Figure 11.7.  As we discussed earlier, for multilayer networks the output of one layer be- comes the input to the following layer. The equations that describe this op- eration are  =  fm 1+ Wm 1+ am bm 1+  am 1+  is the number of layers in the network. The neurons in the first   0 1   M 1–    11.6    for   m  +  =      ,      where  layer receive external inputs:  M  which provides the starting point for Eq.  11.6 . The outputs of the neurons  in the last layer are considered the network  outputs:  a0  p=  ,  a  aM=  .  11-7   11.7    11.8   11   11 Backpropagation  Input  First Layer  Second Layer  Third Layer  p R x 1  1  R   cid:0  W1  cid:0  S1 x R  cid:0 b1  cid:0   S1 x 1  n1 S1 x 1  f 1   cid:0  cid:0  a1  cid:0  cid:0  S1 x 1  cid:0  cid:0  1  cid:0  cid:0   S1   cid:0  W2  cid:0  S2 x S1  cid:0 b2  cid:0   S2 x 1   cid:0  cid:0  a2  cid:0  cid:0  S2 x 1 n2 S2 x 1  cid:0  cid:0  1  cid:0  cid:0   f 2  W3 S3 x S2   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0 b3  cid:0  cid:0   S3 x 1  S2   cid:0  cid:0  a3  cid:0  cid:0  S3 x 1 n3 S3 x 1  cid:0  cid:0   cid:0  cid:0   f 3  S3  a1 = f 1  W1p + b1   a2 = f 2  W2a1 + b2   a3 = f 3  W3a2 + b3   a3 = f 3  W3 f 2  W2f 1  W1p + b1  + b2   + b3   Figure 11.7  Three-Layer Network, Abbreviated Notation  Performance Index The backpropagation algorithm for multilayer networks is a generalization  of the LMS algorithm of Chapter 10, and both algorithms use the same per- formance index: mean square error. The algorithm is provided with a set of  examples of proper network behavior:  p1 t1 { , }    {  p2 t2  , }   pQ tQ  {  ,      }  ,   11.9   pq   is an input to the network, and   where  put. As each input is applied to the network, the network output is com- pared to the target. The algorithm should adjust the network parameters  in order to minimize the mean square error:   is the corresponding target out-  tq  F x  E e2   =    =  E t  a–    2   .   11.10   where  the network has multiple outputs this generalizes to   is the vector of network weights and biases  as in Chapter 10 . If   x  F x  E eTe   =    =  E t  a–    T t  a–     .   11.11   As with the LMS algorithm, we will approximate the mean square error by   Fˆ x   =    t k   –  a k   T t k     –  a k     =  eT k e k   ,   11.12   where the expectation of the squared error has been replaced by the  squared error at iteration   .   k  The steepest descent algorithm for the approximate mean square error   stochastic gradient descent  is  11-8   The Backpropagation Algorithm  m k  1+  wi j    =  m k 1+ bi    =  ,  –  m k   Fˆ ----------- wi j m wi j m k   Fˆ --------- bi m bi  –  ,   11.13    11.14   where      is the learning rate.  So far, this development is identical to that for the LMS algorithm. Now we  come to the difficult part – the computation of the partial derivatives.  Chain Rule For a single-layer linear network  the ADALINE  these partial derivatives  are conveniently computed using Eq.  10.33  and Eq.  10.34 . For the mul- tilayer network the error is not an explicit function of the weights in the  hidden layers, therefore these derivatives are not computed so easily.   Because the error is an indirect function of the weights in the hidden lay- ers, we will use the chain rule of calculus to calculate the derivatives. To  review the chain rule, suppose that we have a function   that is an explicit  function only of the variable  respect to a third variable  w  n . The chain rule is then:  . We want to take the derivative of    with   f  f  f n w    d ---------------------- wd  =  f n  d ------------ nd    n w  d --------------- wd  .   11.15   f n   en=   and   n  2w=  , so that   f n w       =  e2w  ,   11.16   f n w    d ---------------------- wd  =  f n  d ------------ nd    n w  d --------------- wd  =  en   2   .  We will use this concept to find the derivatives in Eq.  11.13  and Eq.   11.14 :  2 2+  For example, if  then   11.17    11.18    11.19   11  Fˆ ----------- m wi j Fˆ --------- m bi  =  =  Fˆ --------- m ni Fˆ --------- m ni      m ni ----------- m wi j  ,  m ni --------- m bi  .  11-9   11 Backpropagation  The second term in each of these equations can be easily computed, since  the net input to layer   is an explicit function of the weights and bias in  that layer:  m  m ni  =  m 1–  m aj wi j  m+ bi  .   11.20   Sm 1–   j  1=  Therefore  If we now define  m ni ----------- m wi j  =  m 1– aj  ,   1=  .  m ni --------- m bi  Sensitivity   the sensitivity of  m  Fˆ   , then Eq.  11.18  and Eq.  11.19  can be simplified to   to changes in the ith element of the net input at layer   m si    Fˆ --------- m ni  ,  =  Fˆ ----------- m wi j Fˆ --------- m bi  m 1–  maj si  ,  m= si  .  We can now express the approximate steepest descent algorithm as  In matrix form this becomes:  m k  1+  wi j    =  m k  si wi j  –  maj  m 1–  ,  m k 1+ bi    =  m k  si bi  –  m  .  Wm k  1+   Wm k  sm am 1–  =  –    T  ,  bm k  1+    =  bm k  sm  –  ,  where  11-10   11.21    11.22    11.23    11.24    11.25    11.26    11.27    11.28    The Backpropagation Algorithm  sm    Fˆ --------- nm  =  Fˆ --------- m n1 Fˆ --------- m n2  Fˆ ---------- m n Sm  .   11.29    Note the close relationship between this algorithm and the LMS algorithm  of Eq.  10.33  and Eq.  10.34  .  Backpropagating the Sensitivities It now remains for us to compute the sensitivities  other application of the chain rule. It is this process that gives us the term  backpropagation, because it describes a recurrence relationship in which  the sensitivity at layer  . m 1+   is computed from the sensitivity at layer   , which requires an-  sm  m  To derive the recurrence relationship for the sensitivities, we will use the  following Jacobian matrix:  nm 1+  ---------------- nm    m 1+ m 1+   n1 n1 ---------------  --------------- m m n2 n1 m 1+ m 1+   n2 n2 ---------------  --------------- m m n2 n1   m 1+ m 1+   n n Sm 1+ Sm 1+ ---------------  --------------- m m n2 n1  m 1+  n1 --------------- m n Sm m 1+  n2 --------------- m n Sm  m 1+  n Sm 1+ --------------- m n Sm  .   11.30   Next we want to find an expression for this matrix. Consider the i,    j ele- ment of the matrix:  11-11  11   11 Backpropagation    Sm   m 1+ al wi l      m 1+ bi     ---------------------------------------------------------  1=  +  m  l  m 1+  ni --------------- m nj  =  =  m 1+  wi j  m aj --------- m nj  m nj  fm nj m   ------------------- m nj  =  m 1+  wi j  m 1+ f· m wi j  m nj   ,  =   11.31   where  where  Therefore the Jacobian matrix can be written  f·m  m nj    =  m fm nj   ------------------- m nj  .  nm 1+  ---------------- Wm 1+ F· m nm nm  =    ,  F· m nm    =  f·m  m n1    0   0  f·m    0  0  0 m n2  0  f·m  .    m n Sm     11.32    11.33    11.34   We can now write out the recurrence relation for the sensitivity by using  the chain rule in matrix form:  sm  =  Fˆ --------- nm  =       T nm 1+  ----------------  nm   Fˆ ---------------- nm 1+   F· m nm   Wm 1+   T  =  Fˆ ---------------- nm 1+   F· m nm     Wm 1+   Tsm 1+  .  =   11.35   Now we can see where the backpropagation algorithm derives its name.  The sensitivities are propagated backward through the network from the  last layer to the first layer:  sM    sM 1–       s2  s1  .   11.36   11-12   The Backpropagation Algorithm  At this point it is worth emphasizing that the backpropagation algorithm  uses the same approximate steepest descent technique that we used in the  LMS algorithm. The only complication is that in order to compute the gra- dient we need to first backpropagate the sensitivities. The beauty of back- propagation is that we have a very efficient implementation of the chain  rule.  We still have one more step to make in order to complete the backpropaga- tion algorithm. We need the starting point,  , for the recurrence relation  of Eq.  11.35 . This is obtained at the final layer:  sM  M si  =  Fˆ --------- M ni  a– a– t   -------------------------------------  =  =  T t M ni  =  –  ai– 2 ti     11.37   ai --------- M ni  .  SM 2  -------------------------------- j  aj– tj  1=  M ni  Now, since  we can write  ai --------- M ni  =  M ai --------- M ni  =  M fM ni   -------------------- M ni  f·M  M ni    ,  =  M si  =  –  ai– 2 ti    f·M  M ni    .  This can be expressed in matrix form as 2F· M nM   –=  sM    t a–    .  Summary Let’s summarize the backpropagation algorithm. The first step is to propa- gate the input forward through the network:  am 1+  =  fm 1+ Wm 1+ am bm 1+  +       for   m  =  0 1   M 1–       ,   11.42   a0  p=  ,  a  aM=  .  The next step is to propagate the sensitivities backward through the net- work:  sM  –=  2F· M nM     t a–    ,  11-13  11   11.38    11.39    11.40    11.41    11.43    11.44    Example  2 2+  11 Backpropagation  sm  =  F· m nm     Wm 1+   Tsm 1+  , for   m M 1–   2 1  =        .   11.45   Finally, the weights and biases are updated using the approximate steep- est descent rule:  Wm k  1+   Wm k  sm am 1–  =  –    T  ,  bm k  1+    =  bm k  sm  –  .   11.46    11.47   To illustrate the backpropagation algorithm, let’s choose a network and ap- ply it to a particular problem. To begin, we will use the 1-2-1 network that  we discussed earlier in this chapter. For convenience we have reproduced  the network in Figure 11.8.   Next we want to define a problem for the network to solve. Suppose that we  want to use the network to approximate the function  g p   =  1  sin+   for   2–     p  2  .   11.48      ---p  4  To obtain our training set we will evaluate this function at several values  of   .   p  Input  Log-Sigmoid Layer  Linear Layer  p  w11,1  w12,1  b11   cid:0  cid:0 Σ n11  cid:0  cid:0   cid:0  cid:0 Σ n12  cid:0  cid:0   b12  1   cid:0  cid:0  a11  cid:0  cid:0   cid:0  cid:0  a12  cid:0  cid:0   1  w21,1   cid:0  cid:0 Σ n2  cid:0  cid:0  w21,2  b2   cid:0   cid:0   1  a2  a1 = logsig  W1p + b1   a2 = purelin  W2a1 + b2   Figure 11.8  Example Function Approximation Network  Before we begin the backpropagation algorithm we need to choose some ini- tial values for the network weights and biases. Generally these are chosen  to be small random values. In the next chapter we will discuss some rea- sons for this. For now let’s choose the values  11-14   W1 0   =  – –  0.27 0.41  ,   b1 0   =  – –  0.48 0.13  ,   W2 0   =  0.09 0.17  –  ,   b2 0   =  0.48  .  The response of the network for these initial values is illustrated in Figure  11.9, along with the sine function we wish to approximate.  Example  a2  2.5  1.5  3  2  1  0  0.5  −0.5  −1 −2  −1.5  −1  −0.5  0  0.5  1  1.5  2  p  Figure 11.9  Initial Network Response  Next, we need to select a training set  . In this  case, we will sample the function at 21 points in the range [-2,2] at equally  spaced intervals of 0.2. The training points are indicated by the circles in  Figure 11.9.  { , }   pQ tQ , } p2 t2  { , } p1 t1  {        Now we are ready to start the algorithm. The training points can be pre- sented in any order, but they are often chosen randomly. For our initial in- put we will choose   , which is the 16th training point:  1=  p  The output of the first layer is then  a0  =  p  =  1  .  a1  =  f1 W1a0 b1+      =  logsig      – –  0.27 0.41  +  1  – –  0.48 0.13      =  logsig   –  –  0.75 0.54      =  +  1 ------------------- e0.75 1 1 ------------------- e0.54 1  +  =  0.321 0.368   .  The second layer output is  11-15  11   11 Backpropagation  a2  =  f2 W2a1 b2+     =  purelin  0.09 0.17  –     +  0.48     =  0.446  .  0.321 0.368  The error would then be  e  =  t  a–  =  1  sin+  a2–  =  1  sin+  –  0.446  =  1.261  .         ---p  4             ---1  4      The next stage of the algorithm is to backpropagate the sensitivities. Be- fore we begin the backpropagation, recall that we will need the derivatives  of the transfer functions,   . For the first layer   and   f·1 n     f·2 n     f·1 n   =  d nd     1  ----------------  e n–+ 1  =  e n– ----------------------- 2 e n–+ 1  =   1 –  1  ----------------  e n–+ 1     1  ----------------  e n–+ 1  =  a1– 1   a1    .  For the second layer we have  f·2 n   =  d n  nd  =  1  .  We can now perform the backpropagation. The starting point is found at  the second layer, using Eq.  11.44 :  s2  =  2F· 2 n2   –    t a–    =  2 f·2 n2  –     1.261    =  –  2 1 1.261      =  –  2.522  .  The first layer sensitivity is then computed by backpropagating the sensi- tivity from the second layer, using Eq.  11.45 :  s1  =  F· 1 n1     W2   Ts2  =  1  a1    1– 1 a1 0  0 1– 1 a2  1  a2    0.09 – 0.17  –  2.522  – 1  =  0.321 0   0.321     0 – 1 0.368   0.368     0.09 0.17 –  –  2.522  =  0.218  0  0  0.233  0.227 – 0.429  =  0.0495 – 0.0997   .  The final stage of the algorithm is to update the weights. For simplicity, we  will use a learning rate  .  In Chapter 12 the choice of learning rate  will be discussed in more detail.  From Eq.  11.46  and Eq.  11.47  we have  0.1    =  11-16   Batch vs. Incremental Training  W2 1  W2 0  s2 a1  =  –  T  =  0.09 0.17  –  –  0.1  –  2.522  0.321 0.368  =  0.171 0.0772  –   ,  b2 1   =  b2 0  s2  –  =  0.48  –  0.1  –  2.522  =  0.732  ,  W1 1  W1 0  s1 a0  =  –  T  =  – –  0.27 0.41  –  0.1  0.0495 – 0.0997  =  1  – –  0.265 0.420  ,  b1 1   =  b1 0  s1  –  =  – –  0.48 0.13  –  0.1  0.0495 – 0.0997  =  – –  0.475 0.140  .  This completes the first iteration of the backpropagation algorithm. We  next proceed to randomly choose another input from the training set and  perform another iteration of the algorithm. We continue to iterate until the  difference between the network response and the target function reaches  some acceptable level.  Note that this will generally take many passes  through the entire training set.  We will discuss convergence criteria in  more detail in Chapter 12.  To experiment with the backpropagation calculation for this two-layer net- work, use the MATLAB® Neural Network Design Demonstration Backprop- agation Calculation  nnd11bc .  Incremental Training  Batch Training  Batch vs. Incremental Training  The algorithm described above is the stochastic gradient descent algo- rithm, which involves “on-line” or incremental training, in which the net- work weights and biases are updated after each input is presented  as with  the LMS algorithm of Chapter 10 . It is also possible to perform batch train- ing, in which the complete gradient is computed  after all inputs are ap- plied to the network  before the weights and biases are updated. For  example, if each input occurs with equal probability, the mean square error  performance index can be written  F x  E eTe   =    =  E t  a–    T t  a–     =    tq  aq–  T tq    aq–    .   11.49   Q    1 ---- Q  q  1=  The total gradient of this performance index is  11-17  11   11 Backpropagation  F x   =    tq  aq–  T tq    aq–    =   tq aq–      T tq aq–        .  11.50   Q    1 ----   Q  q  1=      Q    1 ---- Q  q  1=  Therefore, the total gradient of the mean square error is the mean of the  gradients of the individual squared errors. Therefore, to implement a batch  version of the backpropagation algorithm, we would step through Eq.   11.41  through Eq.  11.45  for all of the inputs in the training set. Then,  the individual gradients would be averaged to get the total gradient. The  update equations for the batch steepest descent algorithm would then be  Wm k  1+  Q   Wm k   ---- – = Q  q  1=  m 1–  m aq sq   T  ,  bm k  1+    =  Q  bm k   ---- – Q  m sq  .  q  1=   11.51    11.52   Using Backpropagation  In this section we will present some issues relating to the practical imple- mentation of backpropagation. We will discuss the choice of network archi- tecture, and problems with network convergence and generalization.  We  will discuss implementation issues again in Chapter 12, which investigates  procedures for improving the algorithm.   Choice of Network Architecture As we discussed earlier in this chapter, multilayer networks can be used to  approximate almost any function, if we have enough neurons in the hidden  layers. However, we cannot say, in general, how many layers or how many  neurons are necessary for adequate performance. In this section we want  to use a few examples to provide some insight into this problem.  2 2+  For our first example let’s assume that we want to approximate the follow- ing functions:  g p   =  1  sin+   for   2–     p  2  ,   11.53      i  -----p  4  i   takes on the values 1, 2, 4 and 8. As   where  becomes more complex, because we will have more periods of the sine wave  . It will be more difficult for a neural network  over the interval  with a fixed number of neurons in the hidden layers to approximate  g p  as    is increased, the function    is increased.     2–  2  p     i  i  11-18   Using Backpropagation  For this first example we will use a 1-3-1 network, where the transfer func- tion for the first layer is log-sigmoid and the transfer function for the sec- ond layer is linear. Recall from our example on page 11-5 that this type of  two-layer network can produce a response that is a sum of three log-sig- moid functions  or as many log-sigmoids as there are neurons in the hidden  layer . Clearly there is a limit to how complex a function this network can  implement. Figure 11.10 illustrates the response of the network after it has  been trained to approximate  sponses are shown by the blue lines.  . The final network re-   1 2 4 8  g p    for   =      i  i  1=  i  2=  -1 -2  3  2  1  0  3  2  1  0  -1 -2  -1 -2  3  2  1  0  3  2  1  0  -1 -2  -1  0  1  2  -1  0  1  2  i  4=  i  8=  -1  0  1  2  -1  0  1  2  Figure 11.10  Function Approximation Using a 1-3-1 Network  i  4=   the 1-3-1 network reaches its maximum capabil-  the network is not capable of producing an accurate approx- . In the bottom right graph of Figure 11.10 we can see how   We can see that for  ity. When  4 i imation of  g p  the 1-3-1 network attempts to approximate  g p  square error between the network response and  network response is only able to match a small part of the function.  i  is minimized, but the   . The mean    for  g p   8=  2 2+  In the next example we will approach the problem from a slightly different  perspective. This time we will pick one function   and then use larger  and larger networks until we are able to accurately represent the function.  For    we will use  g p   g p   g p   =  1  sin+   for   2–     p  2  .   11.54      6  ------p  4  To approximate this function we will use two-layer networks, where the  transfer function for the first layer is log-sigmoid and the transfer function  for the second layer is linear  1- -1 networks . As we discussed earlier in   S1  11-19  11   11 Backpropagation  this chapter, the response of this network is a superposition of  functions.  S1   sigmoid   Figure 11.11 illustrates the network response as the number of neurons in  the first layer  hidden layer  is increased. Unless there are at least five neu- rons in the hidden layer the network cannot accurately represent   g p   .  1-2-1  1-3-1  -1 -2  3  2  1  0  3  2  1  0  -1 -2  -1 -2  3  2  1  0  3  2  1  0  -1 -2  -1  0  1  2  -1  0  1  2  1-4-1  1-5-1  -1  0  1  2  -1  0  1  2  Figure 11.11  Effect of Increasing the Number of Hidden Neurons  To summarize these results, a 1- hidden layer and linear neurons in the output layer, can produce a re- sponse that is a superposition of  proximate a function that has a large number of inflection points, we will  need to have a large number of neurons in the hidden layer.  -1 network, with sigmoid neurons in the    sigmoid functions. If we want to ap-  S1  S1  Use the MATLAB® Neural Network Design Demonstration Function Ap- proximation  nnd11fa  to develop more insight into the capability of a two- layer network.  Convergence In the previous section we presented some examples in which the network  response did not give an accurate approximation to the desired function,  even though the backpropagation algorithm produced network parameters  that minimized mean square error. This occurred because the capabilities  of the network were inherently limited by the number of hidden neurons it  contained. In this section we will provide an example in which the network  is capable of approximating the function, but the learning algorithm does  not produce network parameters that produce an accurate approximation.  In the next chapter we will discuss this problem in more detail and explain  why it occurs. For now we simply want to illustrate the problem.  11-20   Using Backpropagation  2 2+  The function that we want the network to approximate is  g p   =  1  sin+  p     for   2–     p  2  .   11.55   To approximate this function we will use a 1-3-1 network, where the trans- fer function for the first layer is log-sigmoid and the transfer function for  the second layer is linear.  Figure 11.12 illustrates a case where the learning algorithm converges to  a solution that minimizes mean square error. The thin blue lines represent  intermediate iterations, and the thick blue line represents the final solu- tion, when the algorithm has converged.  The numbers next to each curve  indicate the sequence of iterations, where 0 represents the initial condition  and 5 represents the final solution. The numbers do not correspond to the  iteration number. There were many iterations for which no curve is repre- sented. The numbers simply indicate an ordering.   5  0  1  2  3  4  3  2  1  0  -1 -2  -1  0  1  2  Figure 11.12  Convergence to a Global Minimum  Figure 11.13 illustrates a case where the learning algorithm converges to  a solution that does not minimize mean square error. The thick blue line   marked with a 5  represents the network response at the final iteration.  The gradient of the mean square error is zero at the final iteration, there- fore we have a local minima, but we know that a better solution exists, as  evidenced by Figure 11.12. The only difference between this result and the  result shown in Figure 11.12 is the initial condition. From one initial con- dition the algorithm converged to a global minimum point, while from an- other initial condition the algorithm converged to a local minimum point.  11-21  11   11 Backpropagation  3  2  1  0  -1 -2  3  2  1  5  4  0  -1  0  1  2  Figure 11.13  Convergence to a Local Minimum  Note that this result could not have occurred with the LMS algorithm. The  mean square error performance index for the ADALINE network is a qua- dratic function with a single minimum point  under most conditions .  Therefore the LMS algorithm is guaranteed to converge to the global min- imum as long as the learning rate is small enough. The mean square error  for the multilayer network is generally much more complex and has many  local minima  as we will see in the next chapter . When the backpropaga- tion algorithm converges we cannot be sure that we have an optimum so- lution. It is best to try several different initial conditions in order to ensure  that an optimum solution has been obtained.  Generalization In most cases the multilayer network is trained with a finite number of ex- amples of proper network behavior:  p1 t1 { , }    {  p2 t2  , }   pQ tQ  {  ,      }  .   11.56   This training set is normally representative of a much larger class of pos- sible input output pairs. It is important that the network successfully gen- eralize what it has learned to the total population.  2 2+  For example, suppose that the training set is obtained by sampling the fol- lowing function:  g p   =  1  sin+     ---p  4  ,   11.57   at the points  .  There are a total of 11 input tar- get pairs.  In Figure 11.14 we see the response of a 1-2-1 network that has   1.2–   1.6 2   1.6–  2–  =  p          11-22   Using Backpropagation  been trained on this data. The black line represents  , the blue line rep- resents the network response, and the ‘+’ symbols indicate the training set.   g p   -1 -2  -1  0  1  2  Figure 11.14  1-2-1 Network Approximation of   g p   . If we were to find the response of the network at a value of   We can see that the network response is an accurate representation of  g p  not contained in the training set  e.g.,  produce an output close to    that was   , the network would still   . This network generalizes well.  0.2–=  g p   p  p  Now consider Figure 11.15, which shows the response of a 1-9-1 network  that has been trained on the same data set. Note that the network response  accurately models  pute the network response at a value of   e.g.,  p sponse    not contained in the training set    the network might produce an output far from the true re-   at all of the training points. However, if we com-  . This network does not generalize well.  0.2–= g p   g p   p  3  2  1  0  3  2  1  0  -1 -2  -1  0  1  2  Figure 11.15  1-9-1 Network Approximation of   g p   The 1-9-1 network has too much flexibility for this problem; it has a total  of 28 adjustable parameters  18 weights and 10 biases , and yet there are  only 11 data points in the training set. The 1-2-1 network has only 7 param-  11  11-23   11 Backpropagation  eters and is therefore much more restricted in the types of functions that it  can implement.   For a network to be able to generalize, it should have fewer parameters than  there are data points in the training set. In neural networks, as in all mod- eling problems, we want to use the simplest network that can adequately  represent the training set. Don’t use a bigger network when a smaller net- work will work  a concept often referred to as Ockham’s Razor .   An alternative to using the simplest network is to stop the training before  the network overfits. A reference to this procedure and other techniques to  improve generalization are given in Chapter 13.  To experiment with generalization in neural networks, use the MATLAB®  Neural Network Design Demonstration Generalization  nnd11gn .  11-24   Summary of Results  Summary of Results  Multilayer Network  Input  First Layer  Second Layer  Third Layer  p R x 1  1  R   cid:0  W1  cid:0  S1 x R  cid:0 b1  cid:0   S1 x 1  n1 S1 x 1  f 1   cid:0  cid:0  a1  cid:0  cid:0  S1 x 1  cid:0  cid:0  1  cid:0  cid:0   S1   cid:0  W2  cid:0  S2 x S1  cid:0 b2  cid:0   S2 x 1   cid:0  cid:0  a2  cid:0  cid:0  S2 x 1 n2 S2 x 1  cid:0  cid:0  1  cid:0  cid:0   f 2  W3 S3 x S2   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0 b3  cid:0  cid:0   S3 x 1  S2   cid:0  cid:0  a3  cid:0  cid:0  S3 x 1 n3 S3 x 1  cid:0  cid:0   cid:0  cid:0   f 3  S3  a1 = f 1  W1p + b1   a2 = f 2  W2a1 + b2   a3 = f 3  W3a2 + b3   a3 = f 3  W3 f 2  W2f 1  W1p + b1  + b2   + b3   Backpropagation Algorithm  Performance Index  F x  E eTe   =    =  E t  a–    T t  a–     Approximate Performance Index  Fˆ x   =  eT k e k   =    t k   –  a k   T t k     –  a k     Sensitivity  sm    Fˆ --------- nm  =  Fˆ --------- m n1 Fˆ --------- m n2  Fˆ ---------- m n Sm  11-25  11   11 Backpropagation  Forward Propagation  am 1+  =  fm 1+ Wm 1+ am bm 1+  +       for   m  =  0 1   M 1–       ,  a0  p=  ,  a  aM=  .  Backward Propagation  sM  –=  2F· M nM     t a–    ,  sm  =  F· m nm     Wm 1+   Tsm 1+  , for   m M 1–   2 1  =        ,  where  F· m nm    =    f·m n1 m 0   f·m  0  0 m n2     0  0  0  ,    f·m n m Sm    f·m  m nj    =  m fm nj   ------------------- m nj  .  Wm k  1+   Wm k  sm am 1–  =  –    T  ,  bm k  1+    =  bm k  sm  –  .  Weight Update  Approximate Steepest Descent   11-26   Solved Problems  Solved Problems  P11.1 Consider the two classes of patterns that are shown in Figure   P11.1. Class I represents vertical lines and Class II represents hor- izontal lines.  Class I  Class II  Figure P11.1  Pattern Classes for Problem P11.1  i. Are these categories linearly separable?  ii. Design a multilayer network to distinguish these categories.  i. Let’s begin by converting the patterns to vectors by scanning each 2X2  grid one column at a time. Each white square will be represented by a “-1”  and each blue square by a “1”. The vertical lines  Class I patterns  then be- come  and the horizontal lines  Class II patterns  become  p1  =   and   p2  =  p3  =   and   p4  =  1 1 1– 1–  1 1– 1 1–  ,  .  1– 1– 1 1  1– 1 1– 1  In order for these categories to be linearly separable we must be able to  place a hyperplane between the two categories. This means there must be  a weight matrix    and a bias    such that  b  W  Wp1  b+  0 Wp2  ,   b+  0 Wp3  ,   b+  0 Wp4  ,   b+  0  .  These conditions can be converted to  11-27  11   11 Backpropagation  1 1 1– 1–  w1 1 w1 2 w1 3 w1 4  =  w1 1  +  w1 2  –  w1 3  –  w1 4  0  ,  –  w1 1  –  w1 2  +  w1 3  +  w1 4  0  ,  w1 1  –  w1 2  +  w1 3  –  w1 4  0  ,  –  w1 1  +  w1 2  –  w1 3  +  w1 4  0  .  The first two conditions reduce to  w1 1  +  w1 2    w1 3  +  w1 4   and   w1 3  +  w1 4    w1 1  +  w1 2  ,  which are contradictory. The final two conditions reduce to  w1 1  +  w1 3    w1 2  +  w1 4   and   w1 2  +  w1 4    w1 1  +  w1 3  ,  which are also contradictory. Therefore there is no hyperplane that can  separate these two categories.  ii. There are many different multilayer networks that could solve this prob- lem. We will design a network by first noting that for the Class I vectors  either the first two elements or the last two elements will be “1”. The Class  II vectors have alternating “1” and “-1” patterns. This leads to the network  shown in Figure P11.2.  Inputs  OR Operation   cid:0  cid:0   cid:0  cid:0  a11  cid:0  cid:0   cid:0  cid:0   AND Operations  cid:0  cid:0  2 Σ n11  cid:0  cid:0  2 -1  cid:0  cid:0 Σ  n12  a12  cid:0  cid:0   cid:0  cid:0   2  1  2  -1  1  p1  p2  p3  p4  11-28   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0  2 n21 a21 Σ  cid:0  cid:0   cid:0  cid:0   cid:0  cid:0  2  1  1  Figure P11.2  Network to Categorize Horizontal and Vertical Lines   Solved Problems  The first neuron in the first layer tests the first two elements of the input  vector. If they are both “1” it outputs a “1”, otherwise it outputs a “-1”. The  second neuron in the first layer tests the last two elements of the input vec- tor in the same way. Both of the neurons in the first layer perform AND op- erations. The second layer of the network tests whether either of the  outputs of the first layer are “1”. It performs an OR operation. In this way,  the network will output a “1” if either the first two elements or the last two  elements of the input vector are both “1”.  P11.2 Figure P11.3 illustrates a classification problem, where Class I vec- tors are represented by light circles, and Class II vectors are rep- resented by dark circles. These categories are not linearly  separable. Design a multilayer network to correctly classify these  categories.  Figure P11.3  Classification Problem  We will solve this problem with a procedure that can be used for arbitrary  classification problems. It requires a three-layer network, with hard-limit- ing neurons in each layer. In the first layer we create a set of linear decision  boundaries that separate every Class I vector from every Class II vector.  For this problem we used 11 such boundaries. They are shown in Figure  P11.4.  2  1  3  4  5  6  7  10  8  9  11  Figure P11.4  First Layer Decision Boundaries  Each row of the weight matrix in the first layer corresponds to one decision  boundary. The weight matrix and bias vector for the first layer are    W1  T  =  1 1– 1 1–  1 1– 1–  1 1–  1 1–  1 1–  1 1–  1– 1 1 1 1– 1 1  ,  11-29  11   11 Backpropagation  b1  T  =  2– 3 0.5 0.5 1.75  –  2.25 3.25  –  3.75 6.25 5.75  –  –  4.75  .   Review Chapters 3, 4 and 10 for procedures for calculating the appropriate  weight matrix and bias for a given decision boundary.  Now we can com- bine the outputs of the 11 first layer neurons into groups with a second lay- er of AND neurons, such as those we used in the first layer of the network  in Problem P11.1. The second layer weight matrix and bias are  W2  =  1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1  ,   bT  =  .  3– 3– 3– 3–  The four decision boundaries for the second layer are shown in Figure  P11.5. For example, the neuron 2 decision boundary is obtained by combin- ing the boundaries 5, 6, 9 and 11 from layer 1. This can be seen by looking  at row 2 of   .  W2  2  1  3  4  Figure P11.5  Second Layer Decision Regions  In the third layer of the network we will combine together the four decision  regions of the second layer into one decision region using an OR operation,  just as in the last layer of the network in Problem P11.1. The weight matrix  and bias for the third layer are  W3  =  1 1 1 1  ,   b3  3=  .  The complete network is shown in Figure P11.6.  The procedure that we used to develop this network can be used to solve  classification problems with arbitrary decision boundaries as long as we  have enough neurons in the hidden layers. The idea is to use the first layer  to create a number of linear boundaries, which can be combined by using  AND neurons in the second layer and OR neurons in the third layer. The  decision regions of the second layer are convex, but the final decision  boundaries created by the third layer can have arbitrary shapes.  11-30   Solved Problems  Input  Initial Decisions  AND Operations  OR Operation  p 2 x 1  1  11 x 2   cid:0 W1  cid:0   cid:0  b1  cid:0  11 x 1  2   cid:0  cid:0  a1 11 x 1 n1  cid:0  cid:0  11 x 1 1  cid:0  cid:0   4 x 11   cid:0  cid:0 W2  cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   b2 4 x 1  11   cid:0  cid:0 W3  cid:0  cid:0  a2 4 x 1 n2  cid:0  cid:0   cid:0  cid:0   cid:0  cid:0  4 x 1 1  cid:0  cid:0   cid:0  cid:0   b3 1 x 1  1 x 4  4   cid:0  cid:0  a3 1 x 1 n3  cid:0  cid:0  1 x 1  cid:0  cid:0   1  a1 = hardlims  W1p + b1   a2 = hardlims  W2a1 + b2   a3 = hardlims  W3a2 + b3   Figure P11.6  Network for Problem P11.2  The final network decision regions are given in Figure P11.7. Any vector in  the shaded areas will produce a network output of 1, which corresponds to  Class II. All other vectors will produce a network output of -1, which cor- responds to Class I.  Figure P11.7  Final Decision Regions  P11.3 Show that a multilayer network with linear transfer functions is   equivalent to a single-layer linear network.  For a multilayer linear network the forward equations would be  a1 W1p b1+  =  a2 W2a1 b2+  =  =  W2W1p W2b1 b2+  +      ,  a3 W3a2 b3+  =  =  W3W2W1p W3W2b1 W3b2 b3  +  +  +      .  If we continue this process we can see that for an M-layer linear network,  the equivalent single-layer linear network would have the following weight  matrix and bias vector  W WMWM 1–  W2W1  =  ,  11-31  11   11 Backpropagation  b WMWM 1–  W2  =    b1 WMWM 1–  W3  +    b2  bM  +  +  .  P11.4 The purpose of this problem is to illustrate the use of the chain   rule. Consider the following dynamic system:  We want to choose the initial condition  time  some target output    the system output   K=  y K  k    t  y 0    so that at some final   will be as close as possible to   . We will minimize the performance index  using steepest descent, so we need the gradient  1+ y k    =  f y k      .  F y 0       =  – t  y K    2   F y 0  y 0         .   y 0     y K    ,  r k      y 0     y k   .  Find a procedure for computing this using the chain rule.  The gradient is  The key term is   F y 0  y 0         =   t – y 0    y K    2  =  – 2 t  y K      –  y K    .   y 0     which cannot be computed directly, since  y 0   . Let’s define an intermediate term  y K     is not an explicit function of   Then we can use the chain rule:  1+ r k    =  1+ y k    =   y 0     1+   y k ----------------------  y k     y k   -------------  y 0   =  1+   y k ----------------------  y k     r k   .  From the system dynamics we know  1+   y k ----------------------  y k   =  f y k     -------------------- y k    f· y k      .  =  Therefore the recursive equation for the computation of   r k    is  11-32   Solved Problems  This is initialized at   k  0=  :  1+ r k    =  f· y k    r k   .  r 0   =  y 0   -------------  y 0   =  1  .  r 0   1=  ,  The total procedure for computing the gradient is then  1+ r k    =  f· y k    r k   , for   k  =  0 1   K 1–       ,   F y 0  y 0         =  – 2 t  y K      –  r K      .  P11.5 Consider the two-layer network shown in Figure P11.8. The initial   weights and biases are set to  w1  1=  ,   b1  1=  ,   w2  2–=  ,   b2  1=  .  An input target pair is given to be  i. Find the squared error   weights and biases.    1= p       .  ,  t 1= e 2   as an explicit function of all   ii. Using part  i  find   e 2   w1   at the initial weights and biases.  iii. Repeat part  ii  using backpropagation and compare results.  Inputs  Log-Sigmoid Layer  Linear Layer  p  w 1  n 1   cid:0 Σ  a 1   cid:0   w 2  a 2   cid:0 Σ  n 2  cid:0  cid:0  b 2  1  a 1 = logsig  w 1 p + b 1   a 2 = purelin  w 2 a 1 + b 2   Figure P11.8  Two-Layer Network for Problem P11.5  i. The squared error is given by  b 1  1  11-33  11   11 Backpropagation  e 2  =  a2– t  2  =  t  –  w2          ----------------------------------------------------------   + 1  b1+  exp  1 w1p   –    b2+   2       .  ii. The derivative is  e 2  ------------ w1  =  2e  =  2e w2  e --------- w1      ------------------------------------------------------------ 2 + 1  b1+  1 w1p   exp  –      exp  –    w1p  b1+      p–    To evaluate this at the initial weights and biases we find  a1  =  ----------------------------------------------------------   + 1  b1+  exp  1 w1p   –    =  ---------------------------------------------------------   + 1  exp  1 1 1    1+  –    =  0.8808  a2  =  w2a1  b2+  =  2–  0.8808  1+  =  –  0.7616  e  =  a2– t    =  – 1  –  0.7616      =  1.7616  e 2  ------------ w1  =  2e w2      ------------------------------------------------------------ 2 + 1  b1+  1 w1p   exp  –      exp  –    w1p  b1+      p–        =   2 1.7616    2–        ----------------------------------------------------------- 2 + 1  1  1 1   exp  1+  –      exp  –    1 1   1+      1–    =  3.5232 0.2707  =  0.7398 .     1   --------------------  2  1.289  iii. To backpropagate the sensitivities we use Eq.  11.44  and Eq.  11.45 :          s2  =  –    t a–    =  –  2 1  1  –  –  0.7616      =  –  3.5232  ,  s1    W2   Ts2  =  a1 1  a1–        2–  s2  From Eq.  11.23  we can compute   0.8808 1  –  0.8808  3.5232    =  0.7398 .    2– e 2    – w1  :  =  s1a0  =  s1p  =    0.7398   1   =  0.7398  .  This agrees with our result from part  ii .  2F· 2 n2  F· 1 n1    =  =  e 2  ------------ w1  11-34   Solved Problems  P11.6 Earlier in this chapter we showed that if the neuron transfer func-  tion is log-sigmoid,  then the derivative can be conveniently computed by  a  =  f n   =  1 ---------------- e n–+ 1  ,  f· n   =  a– a 1    .  Find a convenient way to compute the derivative for the hyperbol- ic tangent sigmoid:  a  =  f n   =  tansig n   =  e n–– en ------------------ e n–+ en  .  Computing the derivative directly we find  f· n   =  f n  d ------------ nd  =   e n–– d en ------------------  e n–+ en nd       =  –  en ------------------------- en en   e n–– 2 e n–+    e n––    +  e n–+ en ------------------ e n–+ en  =  1  –  2 en  ------------------------- 2 en   e n–– e n–+  =  1  –  a 2 .  P11.7 For the network shown in Figure P11.9 the initial weights and bi-  ases are chosen to be   w1 0   1–=  ,   b1 0   1=  ,   w2 0   2–=  ,   b2 0   1=  .  An input target pair is given to be    1–= p  ,  t 1=      .  Perform one iteration of backpropagation with     1=  .  11-35  11   11 Backpropagation  Inputs  Tan-Sigmoid Layer  Tan-Sigmoid Layer  p  w 1  n 1   cid:0 Σ  a 1   cid:0   b 1  1  w 2  a 2   cid:0 Σ  n 2  cid:0  cid:0   cid:0  cid:0  b 2  1  a 1 = tansig  w 1 p + b 1   a 2 = tansig  w 2 a 1 + b 2   Figure P11.9  Two-Layer Tan-Sigmoid Network  The first step is to propagate the input through the network.  n1  =  w1p  b1+  =  1–    1–    1+  =  2  a1  =  tansig n1    =  n– 1  exp ------------------------------------------------- n– 1  exp  exp exp  n1 n1  – +        =   2– exp --------------------------------------------  2– exp  exp exp  2  2   – +  =  0.964  n2  =  w2a1  b2+  =  2–   0.964     1+  =  –  0.928  a2  =  tansig n2    =  n– 2  exp ------------------------------------------------- n– 2  exp  exp exp  n2 n2  – +        =   exp 0.928 -----------------------------------------------------------------  0.928 exp  0.928 0.928  exp exp  – –  – +        –=  0.7297  –=  1.6175  =  0.2285  e  =  a2– t    =  – 1  –  0.7297      =  1.7297  Now we backpropagate the sensitivities using Eq.  11.44  and Eq.  11.45 .  s2  =  2F· 2 n2   –    t a–    =  –  – 2 1  a2  2   e   =  –  – 2 1  –  0.7297  2  1.7297  s1  =  F· 1 n1     W2   Ts2  =  – 1  a1  2  w2s2  =  – 1    0.964  2    2–    –  1.6175       Finally, the weights and biases are updated using Eq.  11.46  and Eq.   11.47 :  w2 1   =  w2 0  s2 a1  –  T  =  2–    –  1  –  1.6175   0.964     =  –  0.4407  ,  11-36   Solved Problems  w1 1   =  w1 0  s1 a0  –  T  =  1–    –   1 0.2285    1–    =  –  0.7715  ,  b2 1   =  b2 0  s2  –  =  1 1  –  –  1.6175    =  2.6175  ,  b1 1   =  b1 0  s1  –  =  1 1 0.2285  –      =  0.7715  .  P11.8 In Figure P11.10 we have a network that is a slight modification to  the standard two-layer feedforward network. It has a connection  from the input directly to the second layer. Derive the backpropa- gation algorithm for this network.  Input  Layer 1  Layer 2  p  R x 1   cid:0  cid:0 W1  cid:0  cid:0   cid:0  cid:0 b1 1  S1 x R  S1 x 1  R   cid:0  cid:0 W2,1  cid:0  cid:0   cid:0  cid:0 W2 a1 S1 x 1  cid:0  cid:0   cid:0  cid:0  n1 S1 x 1  cid:0  cid:0   cid:0  cid:0 b2 1 S1  S2 x S1  S2 x 1  f 1  n2 S2 x 1  a2 S2 x 1   cid:0   cid:0  f 2  cid:0  S2  Figure P11.10  Network with Bypass Connection  We begin with the forward equations:  n1 W1p b1+  =  ,  a1  =  f1 n1      =  f1 W1p b1+       ,  n2 W2a1 W2 1 p b2  +  =  +  ,  a2  =  f2 n2      =  f2 W2a1 W2 1 p b2      +  +  .  The backpropagation equations for the sensitivities will not change from  those for a standard two-layer network. The sensitivities are the deriva- tives of the squared error with respect to the net inputs; these derivatives  don’t change, since we are simply adding a term to the net input.   Next we need the elements of the gradient for the weight update equations.  For the standard weights and biases we have  11-37  11   11 Backpropagation  =  Fˆ ----------- m wi j Fˆ --------- m bi  Fˆ --------- m ni    m ni ----------- m wi j  =  m 1–  maj si  ,  =  .  m si  =    m ni --------- m bi  Fˆ --------- m ni W1 b1 W2 ,  W2 1  ,   :  Fˆ ------------- 2 1  wi j  =  Fˆ -------- 2 ni    2 ni ------------- 2 1  wi j  =  2 si    2 ni ------------- 2 1  wi j  .  2 ni  =  1  2 aj wi j  +  2 1 pj wi j  +  2 bi  .  S1   j  1=  2 ni ------------- 2 1  wi j  pj=   and   =  2pj si  .  R    j  1=  Fˆ ------------- 2 1  wi j  Therefore  Therefore the update equations for  do need an additional equation for    and   b2   do not change. We   To find the derivative on the right-hand side of this equation note that  The update equations can thus be written in matrix form as:  Wm k  1+   Wm k  sm am 1–  =  –    T  ,   m  1 2=  ,  bm k  1+    =  bm k  sm  –  ,   m  1 2=  .  W2 1  1+ k   W2 1  =  k  s2 a0  –  T  =  W2 1  k  s2 p  –  T  .  The main point of this problem is that the backpropagation concept can be  used on networks more general than the standard multilayer feedforward  network.  P11.9 Find an algorithm, based on the backpropagation concept, that   can be used to update the weights  work shown in Figure P11.11.  w1   and   w2   in the recurrent net-  11-38   Solved Problems  Inputs  Linear Recurrent Layer  w2  w1  p k   n k    cid:0  cid:0 Σ  cid:0  cid:0   a k + 1    cid:0  cid:0 D  a k   a 0   a k + 1  = purelin  w1 p k  + w2 a k    Figure P11.11  Linear Recurrent Network  The first step is to define our performance index. As with the multilayer  networks, we will use squared error:  Fˆ x   =    t k   –  a k   2  =    e k   2  .  For our weight updates we will use the steepest descent algorithm:  wi  –=     Fˆ x  wi  .  These derivatives can be computed as follows:   Fˆ x  wi  =   wi    t k   –  a k   2  =  2 t k    –  a k     –      a k   ------------- wi  .      Therefore, the key terms we need to compute are   a k  ------------- wi  .  To compute these terms we first need to write out the network equation:  1+ a k    =  purelin w1p k  w2a k   +      =  w1p k  w2a k   +  .  Next we take the derivative of both sides of this equation with respect to  the network weights:  1+   a k ----------------------- w1   1+ a k ----------------------- w2  =  p k  w2  +  =  a k  w2  +  a k   ------------- w1 a k   ------------- w2  ,  .  11-39  11   11 Backpropagation  w1   and    is itself a function   Note that we had to take account of the fact that  of  .  These two recursive equations are then used to compute the  derivatives needed for the steepest descent weight update. The equations  are initialized with  a k   w2  a 0   -------------- w1  0=  ,   a 0   -------------- w2  0=  ,  since the initial condition is not a function of the weight.  To illustrate the process, let’s say that  would be  a 0   0=  . The first network update   a 1   =  w1p 0  w2a 0   +  =  w1p 0   .  The first derivatives would be computed:  a 1   -------------- w1  =  p 0  w2  +  =  p 0   ,   =  a 0  w2  +  a 0   -------------- w1  a 1   -------------- w2  a 0   -------------- w2  =  0  .  The first weight updates would be  wi  =  –     Fˆ x  wi  =  –   2 t 1     –  a 1     –      a 1   -------------- wi      w1  w2  –=  2 t 1     –  a 1     –  p 0     =  –  2 t 1     –  a 1    0   =  0  .  This algorithm is a type of dynamic backpropagation, in which the gradient  is computed by means of a difference equation.  P11.10 Show that backpropagation reduces to the LMS algorithm for a   single-layer linear network  ADALINE .  The sensitivity calculation for a single-layer linear network would be:  s1  =  2F· 1 n1   –    t a–    =  2– I t  a–    =  2e–  ,  The weight update  Eq.  11.46  and Eq.  11.47   would be  W1 k  1+   W1 k  s1 a0  =  –  T  =  W1 k   2e–  –  pT  =  W1 k   +  2epT  b1 k  1+    =  b1 k  s1  –  =  b1 k   2e–  –    =  b1 k   +  2e  .  This is identical to the LMS algorithm of Chapter 10.  11-40   Epilogue  Epilogue  In this chapter we have presented the multilayer perceptron network and  the backpropagation learning rule. The multilayer network is a powerful  extension of the single-layer perceptron network. Whereas the single-layer  network is only able to classify linearly separable patterns, the multilayer  network can be used for arbitrary classification problems. In addition, mul- tilayer networks can be used as universal function approximators. It has  been shown that a two-layer network, with sigmoid-type transfer functions  in the hidden layer, can approximate any practical function, given enough  neurons in the hidden layer.  The backpropagation algorithm is an extension of the LMS algorithm that  can be used to train multilayer networks. Both LMS and backpropagation  are approximate steepest descent algorithms that minimize squared error.  The only difference between them is in the way in which the gradient is cal- culated. The backpropagation algorithm uses the chain rule in order to  compute the derivatives of the squared error with respect to the weights  and biases in the hidden layers. It is called backpropagation because the  derivatives are computed first at the last layer of the network, and then  propagated backward through the network, using the chain rule, to com- pute the derivatives in the hidden layers.   One of the major problems with backpropagation has been the long train- ing times. It is not feasible to use the basic backpropagation algorithm on  practical problems, because it can take weeks to train a network, even on  a large computer. Since backpropagation was first popularized, there has  been considerable work on methods to accelerate the convergence of the al- gorithm. In Chapter 12 we will discuss the reasons for the slow conver- gence of backpropagation and will present several techniques for  improving the performance of the algorithm.  Another key problem in training multilayer networks is overfitting. The  network may memorize the data in the training set, but fail to generalize  to new situations. In Chapter 13 we will describe in detail training proce- dures that can be used to produce networks with excellent generalization.  This chapter has focused mainly on the theoretical development of the  backpropagation learning rule for training multilayer networks. Practical  aspects of training networks with this method are discussed in Chapter 22.  Real-world case studies that demonstrate how to train and validate multi- layer networks are provided in Chapter 23  function approximation , Chap- ter 24  probability estimation  and Chapter 25  pattern recognition .  11-41  11   11 Backpropagation  Further Reading  [HoSt89]  K. M. Hornik, M. Stinchcombe and H. White, “Multilayer  feedforward networks are universal approximators,” Neu- ral Networks, vol. 2, no. 5, pp. 359–366, 1989.  This paper proves that multilayer feedforward networks  with arbitrary squashing functions can approximate any  Borel integrable function from one finite dimensional space  to another finite dimensional space.  [LeCu85]  Y. Le Cun, “Une procedure d’apprentissage pour reseau a  seuil assymetrique,” Cognitiva, vol. 85, pp. 599–604, 1985.  Yann Le Cun discovered the backpropagation algorithm at  about the same time as Parker and Rumelhart, Hinton and  Williams. This paper describes his algorithm.  D. B. Parker, “Learning-logic: Casting the cortex of the hu- man brain in silicon,” Technical Report TR-47, Center for  Computational Research in Economics and Management  Science, MIT, Cambridge, MA, 1985.  David Parker independently derived the backpropagation  algorithm at about the same time as Le Cun and Rumel- hart, Hinton and Williams. This report describes his algo- rithm.  D. E. Rumelhart, G. E. Hinton and R. J. Williams, “Learn- ing representations by back-propagating errors,” Nature,  vol. 323, pp. 533–536, 1986.  This paper contains the most widely publicized description  of the backpropagation algorithm.   [RuMc86]  D. E. Rumelhart and J. L. McClelland, eds., Parallel Dis- tributed Processing: Explorations in the Microstructure of  Cognition, vol. 1, Cambridge, MA: MIT Press, 1986.  This book was one of the two key influences in the resur- gence of interest in the neural networks field during the  1980s. Among other topics, it presents the backpropagation  algorithm for training multilayer neural networks.  P. J. Werbos, “Beyond regression: New tools for prediction  and analysis in the behavioral sciences,” Ph.D. Thesis,  Harvard University, Cambridge, MA, 1974.  This Ph.D. thesis contains what appears to be the first de- scription of the backpropagation algorithm  although that   11-42  [Park85]  [RuHi86]  [Werbo74]   Further Reading  name is not used . The algorithm is described here in the  context of general networks, with neural networks as a spe- cial case. Backpropagation did not become widely known  until it was rediscovered in the mid 1980s by Rumelhart,  Hinton and Williams [RuHi86], David Parker [Park85] and  Yann Le Cun [LeCu85].  11-43  11   11 Backpropagation  Exercises  E11.1 Design multilayer networks to perform the classifications illustrated in   Figure E11.1. The network should output a 1 whenever the input vector is  in the shaded region  or on the boundary  and a -1 otherwise. Draw the net- work diagram in abbreviated notation and show the weight matrices and  bias vectors.  i.  iii.  ii.  iv.  Figure E11.1  Pattern Classification Tasks  E11.2 Choose the weights and biases for the 1-2-1 network shown in Figure 11.4   so that the network response passes through the points indicated by the  blue circles in Figure E11.2.   Use the MATLAB® Neural Network Design Demonstration Two-Layer Net- work Function  nnd11nf  to check your result.  11-44   Exercises  i.  iii.  t  t  4  3  2  1  0  4  3  2  1  0  −1  −2 −2  −1  −2 −2  0 p  0 p  ii.  iv.  t  t  4  3  2  1  0  4  3  2  1  0  −1  −2 −2  −1  −2 −2  0 p  0 p  −1  1  2  −1  1  2  −1  1  2  −1  1  2  Figure E11.2  Function Approximation Tasks  E11.3 Find a single-layer network that has the same input output characteristic   as the network in Figure E11.3.  Inputs  Linear Layer 1  Linear Layer 2  p1  p2   cid:0  cid:0  Σ n 11 -2  cid:0  cid:0  -1 -0.5  cid:0  cid:0 Σ  n 12  1 3  1  -0.5   cid:0  cid:0  a 11  cid:0  cid:0   a 12  cid:0  cid:0   1   cid:0  cid:0  1 n 21 Σ  cid:0  cid:0  1 0.5   cid:0  cid:0  a 21  cid:0  cid:0   1  Figure E11.3  Two-Layer Linear Network  11-45  11   E11.4 Use the chain rule to find the derivative   f  w   in the following cases:  11 Backpropagation  i.  ii.  iii.  iv.  f n   f n   f n   f n   =  =  =  =  sin  n   ,   n w    =  w2  .  tanh n   ,   n w    5w=  .  exp  n   ,   n w    =  cos  w    .  logsig n   ,   n w    =  exp  w    .  E11.5 Consider again the backpropagation example that begins on page 11-14.  i. Find the squared error   e 2   as an explicit function of all weights and   biases.  ii. Using part  i , find   e 2   1  w1 1   at the initial weights and biases.  iii. Compare the results of part  ii  with the backpropagation results de-  scribed in the text.  E11.6 For the network shown in Figure E11.4 the initial weights and biases are   chosen to be   w1 0   1=  ,   b1 0   2–=  ,   w2 0   1=  ,   b2 0   1=  .  The network transfer functions are  and an input target pair is given to be  f1 n   =  n 2  ,   f2 n   1 ---= n  ,    p  1=  1= t    .  Perform one iteration of backpropagation with     1=  .  Inputs  Layer 1  Layer 2  p  w 1  n 1   cid:0 Σ  a 1   cid:0 f 1  w 2  a 2   cid:0 Σ  n 2  cid:0  cid:0 f 2 b 2  1  a 1 = f 1 w 1 p + b 1   a 2 = f 2  w 2 a 1 + b 2   Figure E11.4  Two-Layer Network for Exercise E11.6  b 1  1  11-46   Exercises  E11.7 Consider the two-layer network in Figure E11.5.  Inputs  Linear Layer  Linear Layer  p  w1  1,1  w1  2,1  1  n 1  1  n 2  1  b 1  1  b 2   cid:2   1  cid:2   1  1  a 1  1  a 2  w2  1,1  1  n 1   cid:2   1  b 1  w2  1,2  1  a2  1 a  =  purelin W p b  +       1  1  2 a  =  2 purelin W b  +    a1     2  Figure E11.5  Two-Layer Network for Exercise E11.7  with the following input and target:  and biases are given by    p1   1=  t1  2=    . The initial weights   W1 0   =  ,   W2 0   =  1– 1  ,   b1 0   =  ,   b2 0   3=  1 1–  2 1  i. Apply the input to the network and make one pass forward through   the network to compute the output and the error.  ii. Compute the sensitivities by backpropagating through the network.  iii. Compute the derivative    e 2 w1 1  1     Very little calculation is required here.    using the results of part ii.   E11.8 For the network shown in Figure E11.6 the neuron transfer function is  and an input target pair is given to be  f1 n   =  n 2  ,      p  =  t =  1 1  8 2  .      Perform one iteration of backpropagation with     1=  .  11-47  11   11 Backpropagation  Inputs  p1  p2  Σ 1  Layer 1  cid:0  cid:0  n 11 1  cid:0  cid:0  -1  cid:0  cid:0 Σ  n 12  1 1  1   cid:0  cid:0  a 11  cid:0  cid:0   f 1  a 12  cid:0  cid:0 f 1  1  1  Figure E11.6  Single-Layer Network for Exercise E11.8  E11.9 We want to train the network in Figure E11.7 using the standard back-  propagation algorithm  approximate steepest descent .  Inputs Square Law Neuron  p1  w1,1  p2  w1,2  b   cid:2   1  n  a      = 2 f n n  a  =    +  Wp b  2  Figure E11.7  Square Law Neuron  The following input and target are given:  p  =      1 1  0= t      The initial weights and bias are  W 0   =  1 1–  ,   b 0   1=  .  i. Propagate the input forward through the network.  ii. Compute the error.  iii. Propagate the sensitivities backward through the network.  11-48   Exercises  iv. Compute the gradient of the squared error with respect to the   weights and bias.  v. Update the weights and bias  assume a learning rate of  = 0.1 .  E11.10 Consider the following multilayer perceptron network.  The transfer func-  tion of the hidden layer is   f n   n2=  .   Inputs  Square Law Layer  - Title 1 -  Linear Layer  - Title 2 -   cid:0  cid:0 W1 p 2x1  cid:0  cid:0   cid:0  cid:0 b1 1  2x2  2x1  n1 2x1   cid:0  cid:0  a1 2x1  cid:0  cid:0   cid:0  cid:0  1   cid:0 W2  cid:0   cid:0 b2  1x2  1x1   cid:0  cid:0  a2 1x1  cid:0  cid:0   cid:0  cid:0   n2 1x1  a1 = square W1p+b1   - Exp 1 -  a2 = purelin W2a1+b2   - Exp 2 -  Figure E11.8  Two-Layer Square Law Network  The initial weights and biases are:  W1 0   =  ,   W2 0   =  ,   b1 0   =  2 1  ,   b2 0   1–=  .  1 1– 1 0  1 1–  Perform one iteration of the standard steepest descent backpropagation   use matrix operations  with learning rate = 0.5 for the following input  target pair:  p  =      1 1  2= t      E11.11 Consider the network shown in Figure E11.9.  11-49  11   11 Backpropagation  Inputs  Layer 1  Layer  2  p   cid:0  cid:0   cid:0  cid:0  n1 w1 a1 Σ  cid:0  cid:0   cid:0  cid:0  b1  SxR   cid:0  cid:0   cid:0  cid:0  n2 w2 a2 SxR cid:0  cid:0   cid:0  cid:0  b2  Σ  1  1  a1 =  w1p+b1  3  a2 =  w2a1+b2  2  Figure E11.9  Two-Layer Network for Exercise E11.11  The initial weights and biases are chosen to be  w1 0    2–=  b1 0    1=  w2 0   1=    b2 0   2–=  .  An input target pair is given to be    p1   1=  t1  0=    ,  Perform one iteration of backpropagation  steepest descent  with = 1.  E11.12 Consider the multilayer perceptron network in Figure E11.10.  The trans-  fer function of the hidden layer is   f n   n3=  .   Inputs  Cubic Law Layer  Linear Layer  p1  2 x 1  1  2  W1  2 x 2  b1  2 x 1  n1  2 x 1  a1  2 x 1  1  S 1  W2  1 x 2  b2  1 x 1  a2  1 x 1  n2  1 x 1  1  1 a  =  cube W p b  +       1  1  2  2 a W a b  =  +  2  1  Figure E11.10  Cubic Law Neural Network  The initial weights and biases are:  11-50   Exercises  W1 0   =  ,   b1 0   =  ,   W2 0   =  1 1  ,   b2 0   1=  .  1 1– 1 0  1 2  Perform one iteration of the standard steepest descent backpropagation   use matrix operations  with learning rate  = 0.5 for the following input  target pair:  p  =      1– 1  1–= t  .      nm  =  m Wmam 1–    bm+    ,  E11.13 Someone has proposed that the standard multilayer network should be   modified to include a scalar gain at each layer. This means that the net in- put at layer m would be computed as  m   is the scalar gain at layer m. This gain would be trained like the  where  weights and biases of the network. Modify the backpropagation algorithm   Eq.  11.41  to Eq.  11.47   for this new network.  There will need to be a  new equation added to update  , but some of the other equations may  have to be modified as well.   m  E11.14 Consider the two-layer network shown in Figure E11.11.  Inputs  Log-Sigmoid Layer  Linear Layer  p  2  1  1  a 1  1  a 2  2  3   cid:2   1  -1  n2  a2  1  n 1  1  n 2   cid:2   1  cid:2   1  -1  1  11-51  Figure E11.11  Two-Layer Network for Exercise E11.14  i. If p = 1, use a  slightly  modified form of backpropagation  as devel-  oped in Eq.  11.41  through Eq.  11.47   to find  11   11 Backpropagation  a2 n2  ,   a2 1 n1  ,   a2 1 n2  .  ii. Use the results of i. and the chain rule to find   a2 p Your answers to both parts should be numerical.  .  E11.15 Consider the network shown in Figure E11.12, where the inputs to the neu-  ron involve both the original inputs and their product. This is a type of  higher-order network.  Inputs  Log-Sigmoid Layer  p1  p2  n  a  w1  w1,2  w2   cid:2   b  1  a  =  logsig w p w p w p p  +  +     1,2  1  2  1  1  2  2  +   b  Figure E11.12  Higher-Order Network  i. Find a learning rule for the network parameters, using the approx-  imate steepest descent algorithm  as was done for backpropaga- tion .  ii. For the following initial parameter values, inputs and target, per-  form one iteration of your learning rule with learning rate = 1:  w1   1=  w2  1–=    w1 2  0.5=    b1  1=    p1   0=  p2  1=    t  =  0.75  E11.16 In Figure E11.13 we have a two-layer network that has an additional con- nection from the input directly to the second layer. Derive the backpropa- gation algorithm for this network.  11-52   Exercises  Inputs  Layer 1  Layer 2  p  w 1   cid:0 Σ  n 1  a 1   cid:0 f 1  b 1  1  w 2,1 w 2  a 2   cid:0 Σ  n 2  cid:0  cid:0 f 2 b 2  1  a 1 = f 1 w 1 p + b 1   a 2 = f 2  w 2 a 1 + w 2,1 p + b 2   Figure E11.13  Two-Layer Network with Bypass Connection  E11.17 In the multilayer network, the net input is computed as follows  nm 1+  =  Wm 1+ am bm 1+  +   or   m 1+ ni  m 1+ aj wi j  m  +  m 1+ bi  .  Sm =  j  1=  If the net input calculation is changed to the following equation  squared  distance calculation , how will the sensitivity backpropagation  Eq.   11.35   change?  E11.18 Consider again the net input calculation, as described in Exercise E11.17.  If the net input calculation is changed to the following equation  multiply  by the bias, instead of add , how will the sensitivity backpropagation  Eq.   11.35   change?  m 1+ ni  m 1+    wi j  m– aj  2  Sm =  j  1=  m 1+ ni  =  m 1+ aj wi j    m 1+ bi  .       Sm   j  1=    m    E11.19 Consider the system shown in Figure E11.14. There are a series of stages,  with different transfer functions in each stage.  There are no weights or bi- ases.  We want to take the derivative of the output of this system     with  respect to the input of the system    . Derive a recursive algorithm that  you can use to compute this derivative. Use the concepts that we used to  derive the backpropagation algorithm, and use the following intermediate  variable in your algorithm:  aM  p  11  11-53   11 Backpropagation  qi  =  aM ---------- ai  .  p  f 1  f 2  f 3  a 1  a 2  a 3  a M  f M  Figure E11.14  Cascade System  E11.20 The backpropagation algorithm is used to compute the gradient of the   squared error with respect to the weights and biases of a multilayer net- work. How would the algorithm be changed if you wanted to compute the  gradient with respect to the inputs of the network  i.e., with respect to the  elements of the input vector  p write out the final algorithm.   ? Carefully explain all of your steps, and   E11.21 With the standard backpropagation algorithm, we want to compute the de-  rivative  To calculate this derivative, we use the chain rule in the form  Suppose that we want to use Newton's method. We would need to find the  second derivative  F ------- w  =  ------ n F ------- n w    .  F ------- w  .  2F --------- w2  .  What form will the chain rule take in this case?  E11.22 The standard steepest descent backpropagation algorithm, which is sum- marized in Eq.  11.41  through Eq.  11.47 , was designed to minimize the  performance function that was the sum of squares of the network errors, as  given in Eq.  11.12 . Suppose that we want to change the performance func- tion to the sum of the fourth powers of the errors  e4  plus the sum of the  squares of the weights and biases in the network. Show how Eq.  11.41    11-54   Exercises  through Eq.  11.47  will change for this new performance function.  You  don't need to rederive any steps which are already given in this chapter and  do not change.   E11.23 Repeat Problem P11.4 using the “backward” method described below.  In Problem P11.4. we had the dynamic system  1+ y k    =  f y k      .  We had to choose the initial condition  the system output  . We minimized the performance index t  y K      K=  would be as close as possible to some target output    so that at some final time   y 0   k  F y 0       =  – t  y K    2  =  e2 K    using steepest descent, so we needed the gradient  We developed a procedure for computing this gradient using the chain rule.  The procedure involved a recursive equation for the term  which evolved forward in time. The gradient can also be computed in a dif- ferent way by evolving the term   F y 0  y 0         .  r k      y 0     y k   ,  q k      y k     e2 K    backward through time.  E11.24 Consider the recurrent neural network in Figure E11.15.  p k     a k  -1   1  w   cid:2   n   k  a   k  D      a k  =  p k      +  w a k    -1   Figure E11.15  Recurrent Network  11-55  11   11 Backpropagation  We want to find the weight value  system output  will minimize the performance index  scent, so we need the gradient  F w   the   will be as close as possible to some target output  . We   using steepest de-   so that at some final time   F w  .   w  t a K –  K= t  a K  2  w  =  k      i. Find a general procedure to compute this gradient using the chain   rule. Develop an equation to evolve the following term forward  through time:  s k      a k  w  .  Show each step of your entire procedure carefully. This will involve  updating    and also computing the gradient   F w   w  s k   .  ii. Assume that  a function of  derivative of this expression with respect to  equals   K p 1  p 2  p 3     assuming  w  3= ,    and   s 3   w  ,   .  . Write out the complete expression for   a 3    as   . Take the   0=  a 0  , and show that it   1  S1–  E11.25 Write a MATLAB program to implement the backpropagation algorithm  for a   network. Write the program using matrix operations, as in  Eq.  11.41  to Eq.  11.47 . Choose the initial weights and biases to be ran- dom numbers uniformly distributed between -0.5 and 0.5  using the MAT- LAB function rand , and train the network to approximate the function  1–  » 2 + 2 ans =       4  g p   =  1  sin+   for   2–     p  2  .     ---p  2  S1  2=  Use  learning rate  convergence properties of the algorithm as the learning rate changes.  S1 , and use several different initial conditions. Discuss the   . Experiment with several different values for the    and    10=  11-56   Objectives  12 Variations on   Backpropagation  12  Objectives Theory and Examples  Drawbacks of Backpropagation  Performance Surface Example Convergence Example  Heuristic Modifications to Backpropagation  Momentum Variable Learning Rate  Numerical Optimization Techniques  Conjugate Gradient Levenberg-Marquardt Algorithm  Summary of Results Solved Problems Epilogue Further Reading Exercises  12-1 12-2 12-3 12-3 12-7 12-9 12-9 12-12 12-14 12-14 12-19 12-28 12-32 12-46 12-47 12-50  The backpropagation algorithm introduced in Chapter 11 was a major  breakthrough in neural network research. However, the basic algorithm is  too slow for most practical applications. In this chapter we present several  variations of backpropagation that provide significant speedup and make  the algorithm more practical.  We will begin by using a function approximation example to illustrate why  the backpropagation algorithm is slow in converging. Then we will present  several modifications to the algorithm. Recall that backpropagation is an  approximate steepest descent algorithm. In Chapter 9 we saw that steepest  descent is the simplest, and often the slowest, minimization method. The  conjugate gradient algorithm and Newton’s method generally provide fast- er convergence. In this chapter we will explain how these faster procedures  can be used to speed up the convergence of backpropagation.  12-1  Objectives   12 Variations on Backpropagation  Theory and Examples  When the basic backpropagation algorithm is applied to a practical prob- lem the training may take days or weeks of computer time. This has en- couraged considerable research on methods to accelerate the convergence  of the algorithm.  The research on faster algorithms falls roughly into two categories. The  first category involves the development of heuristic techniques, which arise  out of a study of the distinctive performance of the standard backpropaga- tion algorithm. These heuristic techniques include such ideas as varying  the learning rate, using momentum and rescaling variables  e.g.,  [VoMa88], [Jacob88], [Toll90] and [RiIr90] . In this chapter we will discuss  the use of momentum and variable learning rates.  Another category of research has focused on standard numerical optimiza- tion techniques  e.g., [Shan90], [Barn92], [Batt92] and [Char92] . As we  have discussed in Chapters 10 and 11, training feedforward neural net- works to minimize squared error is simply a numerical optimization prob- lem. Because numerical optimization has been an important research  subject for 30 or 40 years  see Chapter 9 , it seems reasonable to look for  fast training algorithms in the large number of existing numerical optimi- zation techniques. There is no need to “reinvent the wheel” unless absolute- ly necessary. In this chapter we will present two existing numerical  optimization techniques that have been very successfully applied to the  training of multilayer perceptrons: the conjugate gradient algorithm and  the Levenberg-Marquardt algorithm  a variation of Newton’s method .  We should emphasize that all of the algorithms that we will describe in this  chapter use the backpropagation procedure, in which derivatives are pro- cessed from the last layer of the network to the first. For this reason they  could all be called “backpropagation” algorithms. The differences between  the algorithms occur in the way in which the resulting derivatives are used  to update the weights. In some ways it is unfortunate that the algorithm  we usually refer to as backpropagation is in fact a steepest descent algo- rithm. In order to clarify our discussion, for the remainder of this chapter  we will refer to the basic backpropagation algorithm as steepest descent  backpropagation  SDBP .  In the next section we will use a simple example to explain why SDBP has  problems with convergence. Then, in the following sections, we will present  various procedures to improve the convergence of the algorithm.  SDBP  12-2   Drawbacks of Backpropagation  Drawbacks of Backpropagation  12  Recall from Chapter 10 that the LMS algorithm is guaranteed to converge  to a solution that minimizes the mean squared error, so long as the learn- ing rate is not too large. This is true because the mean squared error for a  single-layer linear network is a quadratic function. The quadratic function  has only a single stationary point. In addition, the Hessian matrix of a qua- dratic function is constant, therefore the curvature of the function in a giv- en direction does not change, and the function contours are elliptical.  SDBP is a generalization of the LMS algorithm. Like LMS, it is also an ap- proximate steepest descent algorithm for minimizing the mean squared er- ror. In fact, SDBP is equivalent to the LMS algorithm when used on a  single-layer linear network.  See Problem P11.10.  When applied to multi- layer networks, however, the characteristics of SDBP are quite different.  This has to do with the differences between the mean squared error perfor- mance surfaces of single-layer linear networks and multilayer nonlinear  networks. While the performance surface for a single-layer linear network  has a single minimum point and constant curvature, the performance sur- face for a multilayer network may have many local minimum points, and  the curvature can vary widely in different regions of the parameter space.  This will become clear in the example that follows.  Performance Surface Example To investigate the mean squared error performance surface for multilayer  networks we will employ a simple function approximation example. We will  use the 1-2-1 network shown in Figure 12.1, with log-sigmoid transfer func- tions in both layers.  Input  Log-Sigmoid Layer  Log-Sigmoid Layer  p  w11,1  w12,1  b11   cid:0  cid:0 Σ n11  cid:0  cid:0   cid:0  cid:0 Σ n12  cid:0  cid:0   b12  1   cid:0  cid:0  a11  cid:0  cid:0   cid:0  cid:0  a12  cid:0  cid:0   1  w21,1   cid:0  cid:0 Σ n2  cid:0  cid:0  w21,2  b2   cid:0   cid:0   1  a2  a1 = logsig  W1p + b1   a2 = logsig  W2a1 + b2   Figure 12.1  1-2-1 Function Approximation Network  In order to simplify our analysis, we will give the network a problem for  which we know the optimal solution. The function we will approximate is   12-3   12 Variations on Backpropagation  the response of the same 1-2-1 network, with the following values for the  weights and biases:  1  w1 1  10=  ,   1  w2 1  10=  ,   1 b1  5–=  ,   1 b2  5=  ,  2  w1 1  1=  ,   2  w1 2  1=  ,   b2  1–=  .   12.1    12.2   The network response for these parameters is shown in Figure 12.2, which  plots the network output  .    is varied over the range    as the input   a2  2  2–  p      a2  0.5  1  0.75  0.25  0 -2  12-4  -1  1  2  0 p  Figure 12.2  Nominal Function  We want to train the network of Figure 12.1 to approximate the function  displayed in Figure 12.2. The approximation will be exact when the net- work parameters are set to the values given in Eq.  12.1  and Eq.  12.2 .  This is, of course, a very contrived problem, but it is simple and it illus- trates some important concepts.  Let’s now consider the performance index for our problem. We will assume  that the function is sampled at the values  p  =  2–    1.9–    1.8–   1.9 2        ,   12.3   and that each occurs with equal probability. The performance index will be  the sum of the squared errors at these 41 points.  We won’t bother to find  the mean squared error, which just requires dividing by 41.   2  1   and   In order to be able to graph the performance index, we will vary only two  parameters at a time. Figure 12.3 illustrates the squared error when only  w1 1 their optimal values given in Eq.  12.1  and Eq.  12.2 . Note that the mini- mum error will be zero, and it will occur when  , as  indicated by the open blue circle in the figure.   are being adjusted, while the other parameters are set to    and   w1 1  10=  w1 1  w1 1  1=  1  2   12  Drawbacks of Backpropagation  There are several features to notice about this error surface. First, it is  clearly not a quadratic function. The curvature varies drastically over the  parameter space. For this reason it will be difficult to choose an appropri- ate learning rate for the steepest descent algorithm. In some regions the  surface is very flat, which would allow a large learning rate, while in other  regions the curvature is high, which would require a small learning rate.   Refer to discussions in Chapters 9 and 10 on the choice of learning rate for  the steepest descent algorithm.   It should be noted that the flat regions of the performance surface should  not be unexpected, given the sigmoid transfer functions used by the net- work. The sigmoid is very flat for large inputs.  2  1=  , along the valley that runs parallel to the   A second feature of this error surface is the existence of more than one local  minimum point. The global minimum point is located at  w1 1 w1 1 there is also a local minimum, which is located in the valley that runs par- allel to the  0.88 w1 1 formance of backpropagation on this surface.   axis.  This local minimum is actually off the graph at   .  In the next section we will investigate the per-   and  w1 1  axis. However,   w1 1 2 ,   10=  w1 1  38.6  =  =  2  1  1  1  2 w1 1  5  15  10  0  -5 -5  10  5  0 -5  0  10  15  5 1 w1 1  0  2 w1 1  5  10  10  15  15  -5  0  5  1 w1 1  Figure 12.3  Squared Error Surface Versus   1  w1 1   and   2  w1 1  1  w1 1  5–=   and   Figure 12.4 illustrates the squared error when   are being ad- justed, while the other parameters are set to their optimal values. Note  that the minimum error will be zero, and it will occur when  1 , as indicated by the open blue circle in the figure. b1 Again we find that the surface has a very contorted shape, steep in some  regions and very flat in others. Surely the standard steepest descent algo- rithm will have some trouble with this surface. For example, if we have an  initial guess of  , the gradient will be very close to zero,    and   10–=  10=  w1 1  0=  ,   1  1  1 b1  w1 1  1 b1  12-5   12 Variations on Backpropagation  and the steepest descent algorithm would effectively stop, even though it is  not close to a local minimum point.  15  5  -15  1 b1  -5  2.5  1.5  2  1  0.5  0 -10  -25  -10  0  20  30  10 1 w1 1  0  10  1 w1 1  20  30  -30  -10  -20  0  1 b1  20  10   and   1 b1  1  w1 1 1 b2  Figure 12.4  Squared Error Surface Versus   Figure 12.5 illustrates the squared error when  ed, while the other parameters are set to their optimal values. The mini- mum error is located at  circle in the figure.  , as indicated by the open blue    are being adjust-   and    and   5–=  5=  1 b2  1 b1  1 b1  This surface illustrates an important property of multilayer networks: they  have a symmetry to them. Here we see that there are two local minimum  points and they both have the same value of squared error. The second so- lution corresponds to the same network being turned upside down  i.e., the  top neuron in the first layer is exchanged with the bottom neuron . It is be- cause of this characteristic of neural networks that we do not set the initial  weights and biases to zero. The symmetry causes zero to be a saddle point  of the performance surface.  This brief study of the performance surfaces for multilayer networks gives  us some hints as to how to set the initial guess for the SDBP algorithm.  First, we do not want to set the initial parameters to zero. This is because  the origin of the parameter space tends to be a saddle point for the perfor- mance surface. Second, we do not want to set the initial parameters to large  values. This is because the performance surface tends to have very flat re- gions as we move far away from the optimum point.  Typically we choose the initial weights and biases to be small random val- ues. In this way we stay away from a possible saddle point at the origin  without moving out to the very flat regions of the performance surface.  An- other procedure for choosing the initial parameters is described in  [NgWi90].  As we will see in the next section, it is also useful to try several  different initial guesses, in order to be sure that the algorithm converges to  a global minimum point.  12-6   Batching  Drawbacks of Backpropagation  12  10  5  -5  2 b1  0  1.4  0.7  0 -10  -10  -10  -5  5  10  0 1 b1  -5  0  1 b1  5  5  10  10  -10  -5  0  1 b2  Figure 12.5  Squared Error Surface Versus   1 b1   and   1 b2  Convergence Example Now that we have examined the performance surface, let’s investigate the  performance of SDBP. For this section we will use a variation of the stan- dard algorithm, called batching, in which the parameters are updated only  after the entire training set has been presented. The gradients calculated  at each training example are averaged together to produce a more accurate  estimate of the gradient.  If the training set is complete, i.e., covers all pos- sible input output pairs, then the gradient estimate will be exact.   1  2  w1 1  w1 1   and   In Figure 12.6 we see two trajectories of SDBP  batch mode  when only two  parameters,   are adjusted. For the initial condition labeled  “a” the algorithm does eventually converge to the optimal solution, but the  convergence is slow. The reason for the slow convergence is the change in  curvature of the surface over the path of the trajectory. After an initial  moderate slope, the trajectory passes over a very flat surface, until it falls  into a very gently sloping valley. If we were to increase the learning rate,  the algorithm would converge faster while passing over the initial flat sur- face, but would become unstable when falling into the valley, as we will see  in a moment.  1  2  =  =  ,   0.88  38.6  w1 1  Trajectory “b” illustrates how the algorithm can converge to a local mini- mum point. The trajectory is trapped in a valley and diverges from the op- timal solution. If allowed to continue the trajectory converges to  . The existence of multiple local minimum points  w1 1 is typical of the performance surface of multilayer networks. For this rea- son it is best to try several different initial guesses in order to ensure that  a global minimum has been obtained.  Some of the local minimum points  may have the same value of squared error, as we saw in Figure 12.5, so we  would not expect the algorithm to converge to the same parameter values  for each initial guess. We just want to be sure that the same minimum error  is obtained.   12-7   12 Variations on Backpropagation  2 w1 1  5  15  b  10  0  -5 -5  a  12-8  0  10  15  5 1 w1 1  Figure 12.6  Two SDBP  Batch Mode  Trajectories   The progress of the algorithm can also be seen in Figure 12.7, which shows  the squared error versus the iteration number. The curve on the left corre- sponds to trajectory “a” and the curve on the right corresponds to trajectory  “b.” These curves are typical of SDBP, with long periods of little progress  and then short periods of rapid advance.  a  b  1.5  2  1  0.5  0 100  6  4  2  0 100  102  Iteration Number  104  101 102 Iteration Number  103  Figure 12.7  Squared Error Convergence Patterns  We can see that the flat sections in Figure 12.7 correspond to times when  the algorithm is traversing a flat section of the performance surface, as  shown in Figure 12.6. During these periods we would like to increase the  learning rate, in order to speed up convergence. However, if we increase the  learning rate the algorithm will become unstable when it reaches steeper  portions of the performance surface.  This effect is illustrated in Figure 12.8. The trajectory shown here corre- sponds to trajectory “a” in Figure 12.6, except that a larger learning rate  was used. The algorithm converges faster at first, but when the trajectory  reaches the narrow valley that contains the minimum point the algorithm  begins to diverge. This suggests that it would be useful to vary the learning  rate. We could increase the learning rate on flat surfaces and then decrease  the learning rate as the slope increased. The question is: “How will the al-   Heuristic Modifications of Backpropagation  gorithm know when it is on a flat surface?” We will discuss this in a later  section.  12  2 w1 1  5  15  10  0  -5 -5  Squared Error  1.5  1  0.5  0 100  0  10  15  5 1 w1 1  101 102 Iteration Number  103  Figure 12.8  Trajectory with Learning Rate Too Large  Another way to improve convergence would be to smooth out the trajectory.  Note in Figure 12.8 that when the algorithm begins to diverge it is oscillat- ing back and forth across a narrow valley. If we could filter the trajectory,  by averaging the updates to the parameters, this might smooth out the os- cillations and produce a stable trajectory. We will discuss this procedure in  the next section.  To experiment with this backpropagation example, use the MATLAB® Neu- ral Network Design Demonstration Steepest Descent Backpropagation   nnd12sd .  Heuristic Modifications of Backpropagation  Now that we have investigated some of the drawbacks of backpropagation   steepest descent , let’s consider some procedures for improving the algo- rithm. In this section we will discuss two heuristic methods. In a later sec- tion we will present two methods based on standard numerical optimiza- tion algorithms.  Momentum The first method we will discuss is the use of momentum. This is a modifi- cation based on our observation in the last section that convergence might  be improved if we could smooth out the oscillations in the trajectory. We  can do this with a low-pass filter.  Before we apply momentum to a neural network application, let’s investi- gate a simple example to illustrate the smoothing effect. Consider the fol- lowing first-order filter:  12-9   12 Variations on Backpropagation  y k   =  y k  1–    +  – 1  w k   ,   12.4   w k   where  is the momentum coefficient that must satisfy   is the input to the filter,   y k    is the output of the filter and        The effect of this filter is shown in Figure 12.9. For these examples the in- put to the filter was taken to be the sine wave:  0    1  .  w k   =  1  sin+     2k  ---------  16  ,   12.5    12.6     and the momentum coefficient was set to    0.98  right graph . Here we can see that the oscillation of the filter output is less  than the oscillation in the filter input  as we would expect for a low-pass  filter . In addition, as   is increased the oscillation in the filter output is  reduced. Notice also that the average filter output is the same as the aver- age filter input, although as   is increased the filter output is slower to re- spond.     left graph  and   0.9  =  =        1.5  2  1  0.5  0 0  1.5  2  1  0.5  50  a    100 =  0.9  150  200  0 0  50  b    100 =  0.98  150  200  Figure 12.9  Smoothing Effect of Momentum  To summarize, the filter tends to reduce the amount of oscillation, while  still tracking the average value. Now let’s see how this works on the neural  network problem. First, recall that the parameter updates for SDBP  Eq.   11.46  and Eq.  11.47   are  Wm k   –=  sm am 1–    T  ,  bm k   –=  sm  .   12.7    12.8   12-10   Heuristic Modifications of Backpropagation  Momentum  MOBP  When the momentum filter is added to the parameter changes, we obtain  the following equations for the momentum modification to backpropaga- tion  MOBP :  12  Wm k   =  Wm k  1–    –  – 1  sm am 1–    T  ,  bm k   =  bm k  1–    –  – 1  sm  .   12.9    12.10   If we now apply these modified equations to the example in the preceding  section, we obtain the results shown in Figure 12.10.  For this example we  have used a batching form of MOBP, in which the parameters are updated  only after the entire training set has been presented. The gradients calcu- lated at each training example are averaged together to produce a more ac- curate estimate of the gradient.  This trajectory corresponds to the same  initial condition and learning rate shown in Figure 12.8, but with a momen- tum coefficient of  . We can see that the algorithm is now stable. By  the use of momentum we have been able to use a larger learning rate, while  maintaining the stability of the algorithm. Another feature of momentum  is that it tends to accelerate convergence when the trajectory is moving in  a consistent direction.  0.8  =    2 w1 1  5  15  10  0  -5 -5  Squared Error  1.5  1  0.5  0 100  101 102 Iteration Number  103  0  10  15  5 1 w1 1  Figure 12.10  Trajectory with Momentum  If you look carefully at the trajectory in Figure 12.10, you can see why the  procedure is given the name momentum. It tends to make the trajectory  continue in the same direction. The larger the value of  , the more “mo- mentum” the trajectory has.    To experiment with momentum, use the MATLAB® Neural Network Design  Demonstration Momentum Backpropagation  nnd12mo .  12-11   Variable Learning Rate VLBP  12 Variations on Backpropagation  Variable Learning Rate We suggested earlier in this chapter that we might be able to speed up con- vergence if we increase the learning rate on flat surfaces and then decrease  the learning rate when the slope increases. In this section we want to ex- plore this concept.  Recall that the mean squared error performance surface for single-layer  linear networks is always a quadratic function, and the Hessian matrix is  therefore constant. The maximum stable learning rate for the steepest de- scent algorithm is two divided by the maximum eigenvalue of the Hessian  matrix.  See Eq.  9.25 .   As we have seen, the error surface for the multilayer network is not a qua- dratic function. The shape of the surface can be very different in different  regions of the parameter space. Perhaps we can speed up convergence by  adjusting the learning rate during the course of training. The trick will be  to determine when to change the learning rate and by how much.  There are many different approaches for varying the learning rate. We will  describe a very straightforward batching procedure [VoMa88], where the  learning rate is varied according to the performance of the algorithm. The  rules of the variable learning rate backpropagation algorithm  VLBP  are:  1.  2.  3.  If the squared error  over the entire training set  increases by more  than some set percentage  weight update, then the weight update is discarded, the learning rate  is multiplied by some factor      if it is used  is set to zero.     typically one to five percent  after a   , and the momentum coefficient   0  1       If the squared error decreases after a weight update, then the weight  update is accepted and the learning rate is multiplied by some factor   has been previously set to zero, it is reset to its original val-  1 ue.  . If     If the squared error increases by less than  is accepted but the learning rate is unchanged. If  set to zero, it is reset to its original value.    , then the weight update   has been previously      See Problem P12.3 for a numerical example of VLBP.   To illustrate VLBP, let’s apply it to the function approximation problem of  the previous section. Figure 12.11 displays the trajectory for the algorithm  using the same initial guess, initial learning rate and momentum coeffi- cient as was used in Figure 12.10. The new parameters were assigned the  values    =  1.05  ,     =  0.7   and     4%=  .   12.11   12-12   Heuristic Modifications of Backpropagation  12  2 w1 1  5  15  10  0  -5 -5  12-13  0  10  15  5 1 w1 1  Figure 12.11  Variable Learning Rate Trajectory  Notice how the learning rate, and therefore the step size, tends to increase  when the trajectory is traveling in a straight line with constantly decreas- ing error. This effect can also be seen in Figure 12.12, which shows the  squared error and the learning rate versus iteration number.   When the trajectory reaches a narrow valley, the learning rate is rapidly  decreased. Otherwise the trajectory would have become oscillatory, and the  error would have increased dramatically. For each potential step where the  error would have increased by more than 4% the learning rate is reduced  and the momentum is eliminated, which allows the trajectory to make the  quick turn to follow the valley toward the minimum point. The learning  rate then increases again, which accelerates the convergence. The learning  rate is reduced again when the trajectory overshoots the minimum point  when the algorithm has almost converged. This process is typical of a  VLBP trajectory.  Squared Error  Learning Rate  1.5  1  0.5  0 100  60  40  20  0 100  101 102 Iteration Number  103  101 102 Iteration Number  103  Figure 12.12  Convergence Characteristics of Variable Learning Rate  There are many variations on this variable learning rate algorithm. Jacobs  [Jaco88] proposed the delta-bar-delta learning rule, in which each network  parameter  weight or bias  has its own learning rate. The algorithm in- creases the learning rate for a network parameter if the parameter change    12 Variations on Backpropagation  has been in the same direction for several iterations. If the direction of the  parameter change alternates, then the learning rate is reduced. The Su- perSAB algorithm of Tollenaere [Toll90] is similar to the delta-bar-delta  rule, but it has more complex rules for adjusting the learning rates.  Another heuristic modification to SDBP is the Quickprop algorithm of  Fahlman [Fahl88]. It assumes that the error surface is parabolic and con- cave upward around the minimum point and that the effect of each weight  can be considered independently.    ,       and   The heuristic modifications to SDBP can often provide much faster conver- gence for some problems. However, there are two main drawbacks to these  methods. The first is that the modifications require that several parame- ters be set  e.g.,   , while the only parameter required for SDBP  is the learning rate. Some of the more complex heuristic modifications can  have five or six parameters to be selected. Often the performance of the al- gorithm is sensitive to changes in these parameters. The choice of param- eters is also problem dependent. The second drawback to these  modifications to SDBP is that they can sometimes fail to converge on prob- lems for which SDBP will eventually find a solution. Both of these draw- backs tend to occur more often when using the more complex algorithms.  To experiment with VLBP, use the MATLAB® Neural Network Design Dem- onstration Variable Learning Rate Backpropagation  nnd12vl .  Now that we have investigated some of the heuristic modifications to  SDBP, let’s consider those methods that are based on standard numerical  optimization techniques. We will investigate two techniques: conjugate  gradient and Levenberg-Marquardt. The conjugate gradient algorithm for  quadratic functions was presented in Chapter 9. We need to add two proce- dures to this algorithm in order to apply it to more general functions.  The second numerical optimization method we will discuss in this chapter  is the Levenberg-Marquardt algorithm, which is a modification to New- ton’s method that is well-suited to neural network training.  Conjugate Gradient In Chapter 9 we presented three numerical optimization techniques: steep- est descent, conjugate gradient and Newton’s method. Steepest descent is  the simplest algorithm, but is often slow in converging. Newton’s method  is much faster, but requires that the Hessian matrix and its inverse be cal- culated. The conjugate gradient algorithm is something of a compromise; it  does not require the calculation of second derivatives, and yet it still has  the quadratic convergence property.  It converges to the minimum of a qua- dratic function in a finite number of iterations.  In this section we will de- scribe how the conjugate gradient algorithm can be used to train   12-14  Numerical Optimization Techniques   Numerical Optimization Techniques  CGBP  multilayer networks. We will call this algorithm conjugate gradient back- propagation  CGBP .  Let’s begin by reviewing the conjugate gradient algorithm. For ease of ref- erence, we will repeat the algorithm steps from Chapter 9  page 9-18 :  1. Select the first search direction    to be the negative of the gradient,   p0  as in Eq.  9.59 :  12  p0  g0–=  ,  gk      F x   .  x xk=  xk  1+  =  xk kpk  +  .  pk  =  gk–  +  kpk  1–  ,  2. Take a step according to Eq.  9.57 , selecting the learning rate   minimize the function along the search direction:  3. Select the next search direction according to Eq.  9.60 , using Eq.    9.61 , Eq.  9.62 , or Eq.  9.63  to calculate   k  :  where  with   12.12    12.13   k   to    12.14    12.15    12.16   k  =  T gk gk 1– --------------------------- T pk gk 1–  1–   or   k  =   or   k  =  Tgk gk gk  ----------------------- T gk 1–  1–  T gk gk ----------------------- T gk 1–  1– gk  1–  .  4.  If the algorithm has not converged, continue from step 2.  This conjugate gradient algorithm cannot be applied directly to the neural  network training task, because the performance index is not quadratic.  This affects the algorithm in two ways. First, we will not be able to use Eq.   9.31  to minimize the function along a line, as required in step 2. Second,  the exact minimum will not normally be reached in a finite number of  steps, and therefore the algorithm will need to be reset after some set num- ber of iterations.  Let’s address the linear search first. We need to have a general procedure  for locating the minimum of a function in a specified direction. This will in- volve two steps: interval location and interval reduction. The purpose of the  interval location step is to find some initial interval that contains a local  minimum. The interval reduction step then reduces the size of the initial  interval until the minimum is located to the desired accuracy.  12-15   12 Variations on Backpropagation  Interval Location  We will use a function comparison method [Scal85] to perform the interval  location step. This procedure is illustrated in Figure 12.13. We begin by  evaluating the performance index at an initial point, represented by  a1 the figure. This point corresponds to the current values of the network  weights and biases. In other words, we are evaluating   in   F x0    .  The next step is to evaluate the function at a second point, represented by   from the initial point, along the first  b1 search direction   . In other words, we are evaluating   in the figure, which is a distance     p0   12.17    12.18   F x   F x0   +  p0    .  4ε  8ε  2ε  ε  a1  b1 a2  b2 a3  a5  b3 a4  x  b4 b5  Interval Reduction  Figure 12.13  Interval Location  We then continue to evaluate the performance index at new points  cessively doubling the distance between points. This process stops when  the function increases between two consecutive evaluations. In Figure  . At this point we know that the mini- 12.13 this is represented by b4 mum is bracketed by the two points  . We cannot narrow the in- a5 terval any further, because the minimum may occur either in the interval  . These two possibilities are illustrated in  [ a4  b4 Figure 12.14  a .   or in the interval   a3  b3 [  , suc-   and    to   b3  b5  bi  ]  ]  ,  ,  Now that we have located an interval containing the minimum, the next  step in the linear search is interval reduction. This will involve evaluating  the function at points inside the interval  , which was selected in the  interval location step. From Figure 12.14 we can see that we will need to  evaluate the function at two internal points  at least  in order to reduce the  size of the interval of uncertainty. Figure 12.14  a  shows that one internal  function evaluation does not provide us with any information on the loca- tion of the minimum. However, if we evaluate the function at two points    c , as in Figure 12.14  b , we can reduce the interval of uncertainty. If  and   a5  b5 ]  d  [  ,  12-16   Numerical Optimization Techniques    F c  F d  the interval  ] cur in the interval  minimum located in the initial interval. More about that later.   , as shown in Figure 12.14  b , then the minimum must occur in  , then the minimum must oc- c  b [ , .  Note that we are assuming that there is a single   . Conversely, if  a  d [ ,  F c  F d     ]  F x   F x   12  Golden Section Search  a  c  b   a   Interval is not reduced.  a  c  d  b   b  Minimum must occur   between c and b.  Figure 12.14  Reducing the Size of the Interval of Uncertainty  c  d   and   . There are several ways to do this   The procedure described above suggests a method for reducing the size of  the interval of uncertainty. We now need to decide how to determine the lo- cations of the internal points   see [Scal85] . We will use a method called the Golden Section search,  which is designed to reduce the number of function evaluations required.  At each iteration one new function evaluation is required. For example, in  the case illustrated in Figure 12.14  b , point  a point   would become the new point  . Then point  d . The  c trick is to place the new point so that the interval of uncertainty will be re- duced as quickly as possible.   would be placed between the original points    would be discarded and    would become the new   , and a new    and   b  d  a  d  c  The algorithm for the Golden Section search is as follows [Scal85]:  +  – 1  –  – 1   b1   b1   a1– a1–      ,   ,   Fc Fd  =  =  F c1  F d1    .  .    =  0.618  Set  For   k  c1 d1 =  If   =  =  a1 b1   1 2  Fc Fd   repeat   then  Set  ak ck Fd  1+  1+  =  ak= = ak ;   Fc  ;   bk +  1+ – 1  ;  dk=  bk    1+  =    F ck  1+ Fc  dk  1+  –  1+  ck=  ak  1+  else  12-17   12 Variations on Backpropagation  Set  ak dk Fc  1+  1+  =  ck= = bk ;   Fd  ;   bk –  1+ – 1  ;  bk= ck  bk      1+  =  F dk  1+ Fd  1+  –  1+  dk=  ak  1+  end  end until   bk  1+  –  ak  1+    tol  Where   tol   is the accuracy tolerance set by the user.   See Problem P12.4 for a numerical example of the interval location and in- terval reduction procedures.   n  There is one more modification to the conjugate gradient algorithm that  needs to be made before we apply it to neural network training. For qua- dratic functions the algorithm will converge to the minimum in at most  n iterations, where   is the number of parameters being optimized. The  mean squared error performance index for multilayer networks is not qua- dratic, therefore the algorithm would not normally converge in  tions. The development of the conjugate gradient algorithm does not  indicate what search direction to use once a cycle of   iterations has been  completed. There have been many procedures suggested, but the simplest  method is to reset the search direction to the steepest descent direction   negative of the gradient  after  od.   iterations [Scal85]. We will use this meth-   itera-  n  n  n     Let’s now apply the conjugate gradient algorithm to the function approxi- mation example that we have been using to demonstrate the other neural  network training algorithms. We will use the backpropagation algorithm to  compute the gradient  using Eq.  11.23  and Eq.  11.24   and the conjugate  gradient algorithm to determine the weight updates. This is a batch mode  algorithm, as the gradient is computed after the entire training set has  been presented to the network.  Figure 12.15 shows the intermediate steps of the CGBP algorithm for the  first three iterations. The interval location process is illustrated by the  open blue circles; each one represents one evaluation of the function. The  final interval is indicated by the larger open black circles. The black dots in  Figure 12.15 indicate the location of the new interior points during the  Golden Section search, one for each iteration of the procedure. The final  point is indicated by a blue dot.  Figure 12.16 shows the total trajectory to convergence. Notice that the  CGBP algorithm converges in many fewer iterations than the other algo- rithms that we have tested. This is a little deceiving, since each iteration of  CGBP requires more computations than the other methods; there are  many function evaluations involved in each iteration of CGBP. Even so,  CGBP has been shown to be one of the fastest batch training algorithms for  multilayer networks [Char92].  12-18   Numerical Optimization Techniques  12  2 w1 1  5  15  10  0  -5 -5  12-19  0  10  15  Figure 12.15  Intermediate Steps of CGBP  Squared Error  5 1 w1 1  1.5  1  0.5  2 w1 1  5  15  10  0  -5 -5  0 100  101  Iteration Number  102  0  10  15  5 1 w1 1  Figure 12.16  Conjugate Gradient Trajectory  To experiment with CGBP, use the MATLAB® Neural Network Design  Demonstrations Conjugate Gradient Line Search  nnd12ls  and Conjugate  Gradient Backpropagation  nnd12cg .  Levenberg-Marquardt Algorithm The Levenberg-Marquardt algorithm is a variation of Newton’s method  that was designed for minimizing functions that are sums of squares of oth- er nonlinear functions. This is very well suited to neural network training  where the performance index is the mean squared error.  Basic Algorithm  Let’s begin by considering the form of Newton’s method where the perfor- mance index is a sum of squares. Recall from Chapter 9 that Newton’s  method for optimizing a performance index    is  F x    12 Variations on Backpropagation  xk  1+  =  xk Ak  –  1– gk  ,   12.19   where   Ak    2  F x    and   gk      F x   x xk=  .  x xk=  If we assume that   F x    is a sum of squares function:  F x   =  2 x  vi  =  vT x v x   ,   12.20   N    i  1=  then the jth element of the gradient would be    F x   j  =  =  2  vi x   F x   xj--------------  vi x   xj---------------  .  N    i  1=  The gradient can therefore be written in matrix form:    F x   =  2JT x v x   ,  where   12.21    12.22   J x   =  v1 x   x2----------------  v2 x   x2----------------   v1 x   x1---------------- v2 x   x1----------------   vN x  vN x    x2----------------  x1----------------  v1 x   xn---------------- v2 x   xn----------------  vN x   xn----------------  Jacobian Matrix  is the Jacobian matrix.   .   12.23   Next we want to find the Hessian matrix. The  matrix would be  k j   element of the Hessian     2  F x   k j  =  2F x  ----------------- xk xj  =  2  N    i  1=  vi x   xk---------------  vi x   xj---------------      +  vi x   2vi x  ----------------- xk xj  .       12.24   The Hessian matrix can then be expressed in matrix form:  2  F x   =  2JT x J x   +  2S x   ,   12.25   where  12-20   Numerical Optimization Techniques  S x   =  vi x   2  vi x   .  N  i  1=  2  F x     2JT x J x   .  12   12.26    12.27   If we assume that   S x    is small, we can approximate the Hessian matrix as  Gauss-Newton  If we then substitute Eq.  12.27  and Eq.  12.22  into Eq.  12.19 , we obtain  the Gauss-Newton method:  xk  1+  =  xk  –    2JT xk  J xk     1–  2JT xk  v xk    =  xk  –    JT xk  J xk     1– JT xk  v xk   .   12.28   Note that the advantage of Gauss-Newton over the standard Newton’s  method is that it does not require calculation of second derivatives.   JTJ One problem with the Gauss-Newton method is that the matrix    may not be invertible. This can be overcome by using the following modifi- cation to the approximate Hessian matrix:  H  =  G  =  H I+  .   12.29   To see how this matrix can be made invertible, suppose that the eigenval- ues and eigenvectors of   . Then   and    are   1 2   n  H                  Gzi  =  H I+  zi  =  Hzi zi  +  =  izi zi  +  =   12.30       z1 z2   zn i +  zi  .  Therefore the eigenvectors of  the eigenvalues of  creasing  vertible.  G i + 0   are    i +   until   G     are the same as the eigenvectors of   G .   for all  i   can be made positive definite by in- , and therefore the matrix will be in-  , and   H  xk  1+  =  xk  –    JT xk  J xk   kI +   1– JT xk  v xk    .   12.31   or  xk  –=    JT xk  J xk   kI +   1– JT xk  v xk    .   12.32   This algorithm has the very useful feature that as  proaches the steepest descent algorithm with small learning rate:   is increased it ap-  k  12-21  Levenberg-Marquardt  This leads to the Levenberg-Marquardt algorithm [Scal85]:   12 Variations on Backpropagation  xk  1+    xk  –  v xk    =  xk  –  1 -----JT xk k  1 -------- F x  2k    , for large   k  ,   12.33   while as   k   is decreased to zero the algorithm becomes Gauss-Newton.  k   multiplied by some factor    set to some small value  e.g.,    . If a  The algorithm begins with  , then the step is repeated with  step does not yield a smaller value for   should  k 10= decrease, since we would be taking a small step in the direction of steepest  descent. If a step does produce a smaller value for   is divided  by   for the next step, so that the algorithm will approach Gauss-Newton,  which should provide faster convergence. The algorithm provides a nice  compromise between the speed of Newton’s method and the guaranteed  convergence of steepest descent.   . Eventually   F x    , then     e.g.,    1  F x   F x   0.01  k  k    =  Now let’s see how we can apply the Levenberg-Marquardt algorithm to the  multilayer network training problem. The performance index for multilay- er network training is the mean squared error  see Eq.  11.11  . If each tar- get occurs with equal probability, the mean squared error is proportional  to the sum of squared errors over the    targets in the training set:  Q  F x   =    tq  aq–  T tq    aq–    Q  q  1= Q    q  1=  =  Teq eq  =    ej q  2  =  vi  2 ,  Q    SM   q  1=  j  1=  N    i  1=   12.34   where   ej q   is the j th element of the error for the q th input target pair.  Eq.  12.34  is equivalent to the performance index, Eq.  12.20 , for which  Levenberg-Marquardt was designed. Therefore it should be a straightfor- ward matter to adapt the algorithm for network training. It turns out that  this is true in concept, but it does require some care in working out the de- tails.  Jacobian Calculation  The key step in the Levenberg-Marquardt algorithm is the computation of  the Jacobian matrix. To perform this computation we will use a variation  of the backpropagation algorithm. Recall that in the standard backpropa- gation procedure we compute the derivatives of the squared errors, with re- spect to the weights and biases of the network. To create the Jacobian  matrix we need to compute the derivatives of the errors, instead of the de- rivatives of the squared errors.  It is a simple matter conceptually to modify the backpropagation algorithm  to compute the elements of the Jacobian matrix. Unfortunately, although   12-22   12  Numerical Optimization Techniques  the basic concept is simple, the details of the implementation can be a little  tricky. For that reason you may want to skim through the rest of this sec- tion on your first reading, in order to obtain an overview of the general flow  of the presentation, and return later to pick up the details. It may also be  helpful to review the development of the backpropagation algorithm in  Chapter 11 before proceeding.  Before we present the procedure for computing the Jacobian, let’s take a  closer look at its form  Eq.  12.23  . Note that the error vector is  vT  =  v1 v2  vN  =  e1 1  e2 1  e  e1 2  e  SM Q  ,  SM 1   12.35   the parameter vector is  xT  =  x1 x2  xn  =  1 w1 2 w1 1  1  w  1 S1 R  1  b b1  1 w1 1 S1  2  b  M SM  ,  12.36   N  =  Q SM   and   n  =  S1 R 1+    S2 S1    +  1+    SM SM 1– +  +    1+    .   Therefore, if we make these substitutions into Eq.  12.23 , the Jacobian  matrix for multilayer network training can be written  J x   =  .   12.37   e1 1 e1 1 -------------  ------------- 1 1 w1 2 w1 1 e2 1 e2 1 -------------  ------------- 1 1 w1 2 w1 1      e SM 1 -------------- 1 w1 1 e1 2 ------------- 1 w1 1   e SM 1 --------------  1 w1 2 e1 2 -------------  1 w1 2  e1 1 e1 1 ------------  --------------- 1 1 b1 w S1 R e2 1 e2 1 ------------  --------------- 1 1 b1 w S1 R      e SM 1 --------------- 1 w S1 R e1 2 --------------- 1 w S1 R   e SM 1 --------------  1 b1 e1 2 ------------  1 b1            The terms in this Jacobian matrix can be computed by a simple modifica- tion to the backpropagation algorithm.  Standard backpropagation calculates terms like  Fˆ x   xl--------------  =  Teq eq  xl---------------  .   12.38   12-23   Marquardt Sensitivity  The backpropagation process computed the sensitivities through a recur- rence relationship from the last layer backward to the first layer. We can  use the same concept to compute the terms needed for the Jacobian matrix   Eq.  12.37   if we define a new Marquardt sensitivity:  12 Variations on Backpropagation  For the elements of the Jacobian matrix that are needed for the Levenberg- Marquardt algorithm we need to calculate terms like  Recall from Eq.  11.18  in our derivation of backpropagation that  where the first term on the right-hand side was defined as the sensitivity:  J h l  =  vh xl-------  =  ek q xl-----------  .  Fˆ ----------- m wi j  =  Fˆ --------- m ni    m ni ----------- m wi j  ,  m si    Fˆ --------- m ni  .   12.39    12.40    12.41    12.42   m s˜i h    vh ----------- m ni q  =  ek q ----------- m ni q  ,  where, from Eq.  12.35 ,   h  =  1– q  SM k+  .  Now we can compute elements of the Jacobian by  J h l  =  vh xl-------  =  ek q ----------- m wi j  =  ek q ----------- m ni q    m ni q ----------- m wi j  m ni q ----------- m wi j  =  m s˜i h    =  m s˜i h    m 1– aj q  ,   12.43   or if   xl   is a bias,  J h l  =  vh xl-------  =  ek q ----------- m bi  =  ek q ----------- m ni q    m ni q ----------- m bi  m ni q ----------- m bi  =  m s˜i h    =  m s˜i h  .   12.44   The Marquardt sensitivities can be computed through the same recurrence  relations as the standard sensitivities  Eq.  11.35   with one modification  at the final layer, which for standard backpropagation is computed with  Eq.  11.40 . For the Marquardt sensitivities at the final layer we have  12-24   12  Numerical Optimization Techniques  M s˜i h  =  vh ----------- M ni q  =  ek q ----------- M ni q  =  tk q  M–    ak q ------------------------------- M ni q  =  –  M ak q ------------ M ni q  =        f· M ni q M    –  0  for i  k=  for i  k   .  pq  has been applied to the network and the cor- Therefore when the input  M aq responding network output  quardt backpropagation is initialized with   has been computed, the Levenberg-Mar-  M S˜ q  –=  F· M nq M    ,  F· M nM     is defined in Eq.  11.34 . Each column of the matrix   where  must be backpropagated through the network using Eq.  11.35  to produce  one row of the Jacobian matrix. The columns can also be backpropagated  together using     The total Marquardt sensitivity matrices for each layer are then created by  augmenting the matrices computed for each input:  m S˜ q  =  F· m nq m     Wm 1+   TS˜ q  m 1+  .  S˜ m  =  m S˜ 2 S˜ 1  m  S˜ Q  m  .   12.45    12.46   M S˜ q   12.47    12.48   SM  Note that for each input that is presented to the network we will backprop- agate   sensitivity vectors. This is because we are computing the deriva- tives of each individual error, rather than the derivative of the sum of  squares of the errors. For every input applied to the network there will be  SM there will be one row of the Jacobian matrix.   errors  one for each element of the network output . For each error   After the sensitivities have been backpropagated, the Jacobian matrix is  computed using Eq.  12.43  and Eq.  12.44 . See Problem P12.5 for a nu- merical illustration of the Jacobian computation.  LMBP  The iterations of the Levenberg-Marquardt backpropagation algorithm   LMBP  can be summarized as follows:  1. Present all inputs to the network and compute the corresponding net-  work outputs  using Eq.  11.41  and Eq.  11.42   and the errors  eq . Compute the sum of squared errors over all inputs,   M– aq  tq  =  F x   ,   12-25   12 Variations on Backpropagation  using Eq.  12.34 .  2. Compute the Jacobian matrix, Eq.  12.37 . Calculate the sensitivities   with the recurrence relations Eq.  12.47 , after initializing with Eq.   12.46 . Augment the individual matrices into the Marquardt sensitiv- ities using Eq.  12.48 . Compute the elements of the Jacobian matrix  with Eq.  12.43  and Eq.  12.44 .  3. Solve Eq.  12.32  to obtain   xk  .  4. Recompute the sum of squared errors using   of squares is smaller than that computed in step 1, then divide  let  reduced, then multiply   . If this new sum  ,    and go back to step 1. If the sum of squares is not    and go back to step 3.  xk xk  xk xk   by    by   xk  1+    =  +  +      The algorithm is assumed to have converged when the norm of the gradi- ent, Eq.  12.22 , is less than some predetermined value, or when the sum  of squares has been reduced to some error goal.  To illustrate LMBP, let’s apply it to the function approximation problem in- troduced at the beginning of this chapter. We will begin by looking at the  basic Levenberg-Marquardt step. Figure 12.17 illustrates the possible  steps the LMBP algorithm could take on the first iteration.   0  10  15  5 1 w1 1  Figure 12.17  Levenberg-Marquardt Step  k  , which corresponds to the steepest descent   The black arrow represents the direction taken for small  , which corre- sponds to the Gauss-Newton direction. The blue arrow represents the di- rection taken for large  direction.  This was the initial direction taken by all of the previous algo- rithms discussed.  The blue curve represents the Levenberg-Marquardt  step for all intermediate values of   is increased the al- gorithm moves toward a small step in the direction of steepest descent.  This guarantees that the algorithm will always be able to reduce the sum  of squares at each iteration.  . Note that as   k  k  k  2 w1 1  5  15  10  0  -5 -5  12-26   Numerical Optimization Techniques  =    5=  0.01   and   . Note that the algorithm converges in fewer itera-  Figure 12.18 shows the path of the LMBP trajectory to convergence, with  0 tions than any of the methods we have discussed so far. Of course this al- gorithm also requires more computation per iteration than any of the other  algorithms, since it involves a matrix inversion. Even given the large num- ber of computations, however, the LMBP algorithm appears to be the fast- est neural network training algorithm for moderate numbers of network  parameters [HaMe94].   12  2 w1 1  5  15  10  0  -5 -5  1.5  1  0.5  0 100  Squared Error  101  Iteration Number  102  0  10  15  5 1 w1 1  Figure 12.18  LMBP Trajectory  To experiment with the LMBP algorithm, use the MATLAB® Neural Net- work Design Demonstrations Marquardt Step  nnd12ms  and Marquardt  Backpropagation  nnd12m .  n  The key drawback of the LMBP algorithm is the storage requirement. The  algorithm must store the approximate Hessian matrix    n  is the number of parameters  weights and biases  in the  matrix, where  network. Recall that the other methods discussed need only store the gra- dient, which is an n-dimensional vector. When the number of parameters  is very large, it may be impractical to use the Levenberg-Marquardt algo- rithm.  What constitutes “very large” depends on the available memory on  your computer, but typically a few thousand parameters is an upper limit.   . This is an   JTJ  n  12-27   12 Variations on Backpropagation  Summary of Results  Heuristic Variations of Backpropagation  Batching The parameters are updated only after the entire training set has been pre- sented. The gradients calculated for each training example are averaged  together to produce a more accurate estimate of the gradient.  If the train- ing set is complete, i.e., covers all possible input output pairs, then the gra- dient estimate will be exact.   Backpropagation with Momentum  MOBP   Wm k   =  Wm k  1–    –  – 1  sm am 1–    T  bm k   =  bm k  1–    –  – 1  sm  Variable Learning Rate Backpropagation  VLBP  1.  If the squared error  over the entire training set  increases by more  than some set percentage  weight update, then the weight update is discarded, the learning rate  is multiplied by some factor    if  it is used  is set to zero.     typically one to five percent  after a   , and the momentum coefficient    1      2.  3.  If the squared error decreases after a weight update, then the weight  update is accepted and the learning rate is multiplied by some factor   has been previously set to zero, it is reset to its original val-  1 ue.  . If     If the squared error increases by less than  , then the weight update  is accepted but the learning rate and the momentum coefficient are un- changed.    12-28   Summary of Results  Numerical Optimization Techniques  Conjugate Gradient  12  F x   Interval Location  4ε  8ε  2ε  ε  a1  b1 a2  b2 a3  a5  b3 a4  x  b4 b5  Interval Reduction  Golden Section Search   +  – 1  –  – 1   b1   b1   a1– a1–      ,   ,   Fc Fd  =  =  F c1  F d1    .  .    =  0.618  Set  For   k  c1 d1 =  If   =  =  a1 b1   1 2  Fc Fd   repeat   then  Set  ak ck Fd  1+  1+  =  ak= = ak ;   Fc  ;   bk +  1+ – 1  ;  dk=  bk    1+  =    F ck  1+ Fc  dk  1+  –  1+  ck=  ak  1+  Set  ak dk Fc  1+  1+  =  ck= = bk ;   Fd  ;   bk –  1+ – 1  ;  bk= ck  bk      1+  =  F dk  1+ Fd  1+  –  1+  dk=  ak  1+  else  end  end until   bk  1+  –  ak  1+    tol  12-29   12 Variations on Backpropagation  Levenberg-Marquardt Backpropagation  LMBP   xk  –=    JT xk  J xk   kI +   1– JT xk  v xk    vT  =  v1 v2  vN  =  e1 1  e2 1  e  e1 2  e  SM Q  SM 1  xT  =  x1 x2  xn  =  1 w1 2 w1 1  1  w  1 S1 R  1  b b1  1 w1 1 S1  2  b  M SM  N  =  Q SM   and   n  =  S1 R 1+    S2 S1    1+    SM SM 1– +  +    +  1+    J x   =  e1 1 e1 1 -------------  ------------- 1 1 w1 2 w1 1 e2 1 e2 1 -------------  ------------- 1 1 w1 2 w1 1     e SM 1 -------------- 1 w1 1 e1 2 ------------- 1 w1 1   e SM 1 --------------  1 w1 2 e1 2 -------------  1 w1 2  e1 1 e1 1 ------------  --------------- 1 1 b1 w S1 R e2 1 e2 1 ------------  --------------- 1 1 b1 w S1 R    e SM 1 --------------- 1 w S1 R e1 2 --------------- 1 w S1 R   e SM 1 --------------  1 b1 e1 2 ------------  1 b1     J h l  =  vh xl-------  =  ek q ----------- m wi j  =  ek q ----------- m ni q    m ni q ----------- m wi j  =  m s˜i h      m ni q ----------- m wi j  m s˜i h  =    m 1– aj q   for weight   xl  J h l  =  vh xl-------  =  ek q ----------- m bi  =  ek q ----------- m ni q    m ni q ----------- m bi  m ni q ----------- m bi  =  m s˜i h    =  m s˜i h   for bias      xl  m s˜i h    vh ----------- m ni q  =  ek q ----------- m ni q    Marquardt Sensitivity  where   h  =  1– q  SM k+  M S˜ q  –=  F· M nq M    m S˜ q  =  F· m nq m     Wm 1+   TS˜ q  m 1+  S˜ m  =  m S˜ 2 S˜ 1  m  S˜ Q  m  12-30   12  Summary of Results  Levenberg-Marquardt Iterations  1. Present all inputs to the network and compute the corresponding net-  work outputs  using Eq.  11.41  and Eq.  11.42   and the errors  eq . Compute the sum of squared errors over all inputs,  using Eq.  12.34 .  M– aq  tq  =  F x   ,   2. Compute the Jacobian matrix, Eq.  12.37 . Calculate the sensitivities   with the recurrence relations Eq.  12.47 , after initializing with Eq.   12.46 . Augment the individual matrices into the Marquardt sensitiv- ities using Eq.  12.48 . Compute the elements of the Jacobian matrix  with Eq.  12.43  and Eq.  12.44 .  3. Solve Eq.  12.32  to obtain   xk  .  4. Recompute the sum of squared errors using   of squares is smaller than that computed in step 1, then divide  let  reduced, then multiply   . If this new sum  ,    and go back to step 1. If the sum of squares is not    and go back to step 3.  xk xk  xk xk   by    by   xk  1+    +  =  +      12-31   12 Variations on Backpropagation  Solved Problems  P12.1 We want to train the network shown in Figure P12.1 on the train-  ing set        p1  3–=        t1  =  0.5  ,     p2  2=        t2  1=              ,      starting from the initial guess  w 0   =  0.4  ,   b 0   =  0.15  .  Demonstrate the effect of batching by computing the direction of  the initial step for SDBP with and without batching.  Input  Log-Sigmoid Layer  p  w   cid:0  cid:0 Σ n  cid:0  cid:0   b   cid:0   cid:0   a  1  a = logsig  w p + b   Figure P12.1  Network for Problem P12.1  Let’s begin by computing the direction of the initial step if batching is not  used. In this case the first step is computed from the first input target pair.  The forward and backpropagation steps are  a  =  logsig wp    b+    =  -------------------------------------------------------------------   1  0.15  exp  –  +  +      1 3– 0.4  =  0.2592  e  =  t  a–  =  0.5 0.2592  –  =  0.2408  s  =  2f· n e  –  =  –  2a 1  a–  e  =  –   2 0.2592   1 –  0.2592  0.2408  =  –  0.0925  .  The direction of the initial step is the negative of the gradient. For the  weight this will be  For the bias we have  sp–  =  –  –  0.0925    3–    =  –  0.2774  .  s–  =  –  –  0.0925    =  0.0925  .  12-32   Solved Problems  Therefore the direction of the initial step in the     w b     plane would be  12  –  0.2774 0.0925  .  Now let’s consider the initial direction for the batch mode algorithm. In this  case the gradient is found by adding together the individual gradients  found from the two sets of input target pairs. For this we need to apply the  second input to the network and perform the forward and backpropagation  steps:  a  =  logsig wp    b+    =  1  ----------------------------------------------------------------   1  0.4 2   0.15  exp  –  +  +    =  0.7211  e  =  t  a–  =  1  –  0.7211  =  0.2789  s  =  2f· n e  –  =  –  2a 1  a–  e  =  –   2 0.7211   1 –  0.7211  0.2789  =  –  0.1122  .  The direction of the step is the negative of the gradient. For the weight this  will be  For the bias we have  sp–  =  –  –  0.1122   2   =  0.2243  .  s–  =  –  –  0.1122    =  0.1122  .  The partial gradient for the second input target pair is therefore  0.2243 0.1122  .  If we now add the results from the two input target pairs we find the direc- tion of the first step of the batch mode SDBP to be  1 --- 2      –  0.2774 0.0925  +  0.2243 0.1122      =  1 --- 2  –  0.0531 0.2047  =  –  0.0265 0.1023  .  The results are illustrated in Figure P12.2. The blue circle indicates the ini- tial guess. The two blue arrows represent the directions of the partial gra- dients for each of the two input target pairs, and the black arrow  represents the direction of the total gradient. The function that is plotted  is the sum of squared errors for the entire training set. Note that the indi- vidual partial gradients can point in quite different directions than the true  gradient. However, on the average, over several iterations, the path will  generally follow the steepest descent trajectory.  12-33   12 Variations on Backpropagation  The relative effectiveness of the batch mode over the incremental approach  depends very much on the particular problem. The incremental approach  requires less storage, and, if the inputs are presented randomly to the net- work, the trajectory is stochastic, which makes the algorithm somewhat  less likely to be trapped in a local minimum. It may also take longer to con- verge than the batch mode algorithm.  1.5  1  0.5  0 -4  b  4  2  0  -2  -4 -4  -2  0  b  2  -4  -2  2  0  w  4  4  Figure P12.2  Effect of Batching in Problem P12.1  -2  2  4  0 w  P12.2 In Chapter 9 we proved that the steepest descent algorithm, when   applied to a quadratic function, would be stable if the learning  rate was less than 2 divided by the maximum eigenvalue of the  Hessian matrix. Show that if a momentum term is added to the  steepest descent algorithm there will always be a momentum coef- ficient that will make the algorithm stable, regardless of the learn- ing rate. Follow the format of the proof on page 9-6.  The standard steepest descent algorithm is  If we add momentum this becomes  Recall from Chapter 8 that the quadratic function has the form  and the gradient of the quadratic function is  xk  =   F xk –    =  –  gk  ,  xk  =  xk  1–  –  1 –  gk  .  F x   =  1 ---xTAx dTx 2  +  +  c  ,    F x   =  Ax d+  .  12-34   Solved Problems  If we now insert this expression into our expression for the steepest descent  algorithm with momentum we obtain  12  Using the definition    this can be rewritten  xk  1–  –  – 1   Axk d+      .  xk  xk xk–  =  =  =  xk  1+  xk 1+  xk   xk– xk  –    –  – 1   Axk d+      1–  or  Now define a new vector  xk  1+  =    + 1  I  –  – 1  A  xk  –  xk  1–  –  – 1  d  .  x˜ k  =  xk 1– xk  .  The momentum variation of steepest descent can then be written  x˜ k  1+  =  0 I–  I – 1    + 1  I  –  A    x˜ k  +  0 – 1  –    d  =  Wx˜ k  v+  .  This is a linear dynamic system that will be stable if the eigenvalues of  are less than one in magnitude. We will find the eigenvalues of  es. First, rewrite   W    in stag-   as  W  W  W  =   where   T  =    + 1  I  –  – 1  A    .  0 I I– T  The eigenvalues and eigenvectors of   W   should satisfy  Wzw  =  wzw  , or   0 I I– T  w z1 w z2  =  w  w z1 w z2  .  This means that  w  =  w z2  wz1 w z2 At this point we will choose  , with  t .  If this choice is not appropriate it will lead  corresponding eigenvalue  to a contradiction.  Therefore the previous equations become   to be an eigenvector of the matrix   wz2   and   w Tz2  w z1  T  =  +  –  w  .  12-35   12 Variations on Backpropagation  w z2  =  w  wz1   and   –  w z1  +  w  tz2  =  w  wz2  .  If we substitute the first equation into the second equation we find  –  w   w------z2  +  w  tz2  =  w  wz2   or     w  2 t w  –    +  w z2  0=  .  Therefore for each eigenvalue  W   that are roots of the quadratic equation   of   T  t   there will be two eigenvalues   w   of   From the quadratic formula we have  w  2 t w  –    +  0=  .  w  =    t 4– --------------------------------------  2  .  t 2  For the algorithm to be stable the magnitude of each eigenvalue must be  less than 1. We will show that there always exists some range of  which this is true.    for     Note that if the eigenvalues    are complex then their magnitude will be   w    :  w  =  2 t ----------- 4  +  –  2 4 -----------------------  t 4  =    .   This is true only for real   is  between 0 and 1, the magnitude of the eigenvalue must be less than 1. It  remains to show that there exists some range of   for which all of the eigen- values are complex.  . We will show later that    is real.  Since       t  t  In order for   w   to be complex we must have  t  2    4–   or  0 t Let’s now consider the eigenvalues  pressed in terms of the eigenvalues of  z1 z2   zn  Then   of  A           be the eigenvalues and eigenvectors of the Hessian matrix.   T . Let   . These eigenvalues can be ex-    1 2   n           and   t    2   .  Tzi    + 1  I  –  – 1  A  zi  =  + 1  zi  –  – 1  Azi  + 1  zi  –  – 1  izi  =    + 1    –  – 1  i  zi  =  tzi i  .  =  =  12-36   12  Solved Problems  Therefore the eigenvectors of  the eigenvalues of    are  T  T   are the same as the eigenvectors of   A  , and   t i  =    + 1    –  – 1  i    .   Note that  fore, in order for   t i  w   is real, since      ,    and   i   for symmetric   A   are real.  There-   to be complex we must have  t    2    or   + 1    –  – 1  i    2   .    1=   both sides of the inequality will equal 2. The function on the  . The   For  right of the inequality, as a function of  function on the left of the inequality has a slope of  values of the Hessian will be positive real numbers if the function has a  strong minimum, and the learning rate is a positive number, this slope  must be greater than 1. This shows that the inequality will always hold for    , has a slope of 1 at    close enough to 1.  . Since the eigen-  1 i  1=  +      To summarize the results, we have shown that if a momentum term is add- ed to the steepest descent algorithm on a quadratic function, then there  will always be a momentum coefficient that will make the algorithm stable,   is close  regardless of the learning rate. In addition we have shown that if   enough to 1, then the magnitudes of the eigenvalues of   will be  . It can   be shown  see [Brog91]  that the magnitudes of the eigenvalues determine  how fast the algorithm will converge. The smaller the magnitude, the fast- er the convergence. As the magnitude approaches 1, the convergence time  increases.  W  =  We can demonstrate these results using the example on page 9-7. There we  showed that the steepest descent algorithm, when applied to the function  F x  . In Figure P12.3  we see the steepest descent trajectory  with momentum  with    and  learning rate but no momentum.  . Compare this trajectory with Figure 9.3, which uses the same   , was unstable for a learning rate    0.4  2 25x2  0.041  0.2  2 x1    +  =  =    12-37   12 Variations on Backpropagation  0.5  1  0  -0.5  -1 -1  12-38  -0.5  0  0.5  1  Figure P12.3  Trajectory for     =  0.041   and     =  0.2  P12.3 Execute three iterations of the variable learning rate algorithm on   the following function  from the Chapter 9 example on page 9-7 :  starting from the initial guess  F x   =  2 x1  +  2 25x2  ,  x0  =  ,  0.5 0.5  and use the following values for the algorithm parameters:    =  0.05  ,     =  0.2  ,     =  1.5  ,     =  0.5  ,     5%=  .  The first step is to evaluate the function at the initial guess:  F x0    =  1 ---x0 2  T 2 0 0 50  x0  =  1 --- 0.5 0.5 2  2 0 0 50  0.5 0.5  =  6.5  .  The next step is to find the gradient:    F x   =   x1  x2  F x   F x   =  2x1 50x2  .  If we evaluate the gradient at the initial guess we find:   Solved Problems  g0  =    F x   =  x x0=  .  1 25  12  With the initial learning rate of  gorithm is    =  0.05  , the tentative first step of the al-  x0  =  x 1–  –  – 1  g0  =  0.2 0 0  –  0.8 0.05     1 25  =  –  0.04 1–  t x1  =  x0 x0  +  =  0.5 0.5  +  –  0.04 1–  =  0.46 0.5–  .  To verify that this is a valid step we must test the value of the function at  this new point:  t F x1    =  1 t --- x1 2  T 2 0 0 50  t x1  =  1 --- 0.46 0.5– 2  2 0 0 50  0.46 0.5–  =  6.4616  .  This is less than  learning rate is increased:  F x0    . Therefore this tentative step is accepted and the   x1  =  t x1  =  0.46 0.5–  ,   F x1    =  6.4616   and     =  1.5 0.05  =    =  0.075  .  The tentative second step of the algorithm is  x1  =  x0  –  – 1  g1  =  0.2  –  0.8 0.075    –  0.04 1–   0.92 25–  =  –  0.0632 1.3  t x2  =  x1 x1  +  =  0.46 0.5–  +  –  0.0632 1.3  =  0.3968  .  0.8  We evaluate the function at this point:  t F x2    =  1 t --- x2 2  T 2 0 0 50  t x2  =  1 --- 0.3968 0.8 2  2 0 0 50  0.3968  0.8  =  16.157  .  Since this is more than  the learning rate and set the momentum coefficient to zero.   larger than   F x1  5%    , we reject this step, reduce   x2  x1=  ,   F x2    =  F x1    =  6.4616  ,     =    =  0.5 0.075      =  0.0375  ,     0=  12-39   12 Variations on Backpropagation  Now a new tentative step is computed  momentum is zero .  x2  =  –  g2  =  –    0.0375   0.92 25–  =  –  0.0345 0.9375  t x3  =  x2 x2  +  =  0.46 0.5–  +  –  0.0345 0.9375  =  0.4255 0.4375  t F x3    =  1 t --- x3 2  T 2 0 0 50  t x3  =  1 --- 0.4255 0.4375 2  2 0 0 50  0.4255 0.4375  =  4.966  This is less than  reset to its original value, and the learning rate is increased.  . Therefore this step is accepted, the momentum is   F x2    x3  t= x3  ,     =  0.2  ,     =  1.5 0.0375  =    =  0.05625  This completes the third iteration.  P12.4 Recall the example from Chapter 9 that we used to demonstrate   the conjugate gradient algorithm  page 9-18 :  with initial guess  F x   =  1 ---xT 2 1 2 1 2  x  ,  x0  =  0.8 0.25  –  .    F x   =  2x1 x2+ 2x2 + x1  .  12-40  Perform one iteration of the conjugate gradient algorithm. For the  linear minimization use interval location by function evaluation  and interval reduction by the Golden Section search.  The gradient of this function is  As with steepest descent, the first search direction for the conjugate gradi- ent algorithm is the negative of the gradient:   .  .          1.35 – 0.3–      1.35 – 0.3–      Solved Problems  12  For the first iteration we need to minimize   F x    along the line  p0  =  g0–  =  –  F x   T  =  x x0=  1.35 – 0.3–  x1  =  x0 0p0  +  =  0.8 0.25  –  +  0  1.35 – 0.3–  The first step is interval location. Assume that the initial step size is    . Then the interval location would proceed as follows:  0.075  =  F a1    =  F    –  0.8 0.25      =  0.5025  ,  b1  =    =  0.075  ,   F b1    =  F  +  0.075  =  0.3721      0.8 0.25  –  0.8 0.25  –              b2  =  2  =  0.15  ,   F b2    =  F  +  0.15  =  0.2678  b3  =  4  =  0.3  ,   F b3    =  F  0.8 0.25  –  +  0.3  1.35 – 0.3–  =  0.1373  b4  =  8  =  0.6  ,   F b4    =  F  0.8 0.25  –  +  0.6  1.35 – 0.3–  =  0.1893  .  Since the function increases between two consecutive evaluations we know  that the minimum must occur in the interval  . This process is il- lustrated by the open blue circles in Figure P12.4, and the final interval is  indicated by the large open black circles.  0.15  0.6  [  ]  ,  The next step in the linear minimization is interval reduction using the  Golden Section search. This proceeds as follows:  c1  =  a1  +  – 1   b1   a1–    =  0.15  +    0.382   0.6   –  0.15    =  0.3219  ,  d1  =  b1  –  – 1   b1   a1–    =  0.6  –    0.382   0.6   –  0.15    =  0.4281  ,  Fa  =  0.2678  ,   Fb  =  0.1893  ,   Fc  =  0.1270  ,   Fd  =  0.1085  .  12-41   12 Variations on Backpropagation  Since   Fc Fd  , we have  a2  =  c1  =  0.3219  ,   b2  =  b1  =  0.6  ,   c2  =  d1  =  0.4281  d2  =  b2  –  – 1   b2   a2–    =  0.6  –    0.382   0.6   –  0.3219    =  0.4938  ,  Fa  =  Fc  =  0.1270  ,   Fc  =  Fd  =  0.1085  ,   Fd  =  F d2    =  0.1232  .  This time   Fc Fd  , therefore  a3  =  a2  =  0.3219  ,   b3  =  d2  =  0.4938  ,   d3  =  c2  =  0.4281  ,  c3  =  a3  +  – 1   b3   a3–    =  0.3219  +    0.382   0.4938   –  0.3219    =  0.3876  ,  Fb  =  Fd  =  0.1232  ,   Fd  =  Fc  =  0.1085  ,   Fc  =  F c3    =  0.1094  .  This routine continues until  P12.4 indicate the location of the new interior points, one for each iteration  of the procedure. The final point is indicated by a blue dot. Compare this  result with the first iteration shown in Figure 9.10.  . The black dots in Figure   tol  bk  ak  1+  1+    –  0.5  1  0  -0.5  -1 -1  12-42  -0.5  0  0.5  1  Figure P12.4  Linear Minimization Example  P12.5 To illustrate the computation of the Jacobian matrix for the Lev- enberg-Marquardt method, consider using the network of Figure  P12.5 for function approximation. The network transfer functions  are chosen to be  Therefore their derivatives are  f1 n   =  n 2  ,   f2 n   n=  .  f·1 n   2n=  f·2 n   ,   1=  .   Solved Problems  Assume that the training set consists of   {    p1  1=   t1  ,  1=   }  ,   {    p2  2=   t2  ,  2=   }  ,  and that the parameters are initialized to  W1  1=  ,   b1  0=  ,   W2  2=  ,   b1  1=  .  Find the Jacobian matrix for the first step of the Levenberg-Mar- quardt method.  12  Input  Layer 1  Layer 2  p  w1  1,1   cid:0  cid:0 Σ n11  cid:0  cid:0   b1  1   cid:0  cid:0 f 1 a11  cid:0  cid:0   w2  1,1  1   cid:0  cid:0 Σ n2  cid:0  cid:0   b2  1   cid:0 f 2  cid:0   a2  1  1  1  a1 = f 1  w1 p + b1   a2 = f 2  w2 a1 + b2   Figure P12.5  Two-Layer Network for LMBP Demonstration  The first step is to propagate the inputs through the network and compute  the errors.  0 a1  =  p1  =  1  1 W1a1 n1  =  0 b1+  =  1 1  0+  =  1  ,   1 a1  =  1 f1 n1    =  1  2  =  1  2 W2a1 n1  =  1 b2+  =    2 1  1+    =  ,   2 a1  =  2 f2 n1    3  =  3    =  3  e1  =    t1  2– a1    =    1  3–    =  2–  0 a2  =  p2  =  2  1 W1a2 n2  =  0 b1+  =  1 2  0+  =  2  ,   1 a2  =  1 f1 n2    =  2  2  =  2 W2a2 n2  =  1 b2+  =    2 4  1+    =  ,   2 a2  =  2 f2 n2    9  =  9    =  4  9  12-43   12 Variations on Backpropagation  e2  =    t2  2– a2    =    2  9–    =  7–  The next step is to initialize and backpropagate the Marquardt sensitivi- ties using Eq.  12.46  and Eq.  12.47 .  1 S˜ 1  =  F· 1 n1 1     W2   2  TS˜ 1  =  1  2n1 1  2  1–  =  2 1  2  1–  =  4–  2 S˜ 1  =  F· 2 n1 2    –  =  1–  2 S˜ 2  =  F· 2 n2 2    –  =  1–  1 S˜ 2  =  F· 1 n2 1     W2   2  TS˜ 2  =  2  2n1 2  2  1–  =  2 2  2  1–  =  8–  S˜ 1  =  1  1 S˜ 2 S˜ 1  =  4–  8–  ,   S˜ 2  =  2  2 S˜ 2 S˜ 1  =  1–  1–  We can now compute the Jacobian matrix using Eq.  12.43 , Eq.  12.44  and  Eq.  12.37 .  J x   =  v1 x1------- v2 x1-------  v1 x2------- v2 x2-------  v1 x3------- v2 x3-------  v1 x4------- v2 x4-------  =  e1 1 ------------- 1 w1 1 e1 2 ------------- 1 w1 1  e1 1 ------------ 1 b1 e1 2 ------------ 1 b1  e1 1 ------------- 2 w1 1 e1 2 ------------- 2 w1 1  e1 1 ------------ 2 b1 e1 2 ------------ 2 b1  J 1 1  =  v1 x1-------  =  e1 1 ------------- 1 w1 1  =  e1 1 ------------ 1 n1 1    1 n1 1 ------------- 1 w1 1  =  1 s˜1 1    =  1 s˜1 1  0  a1 1  =  4–   1   =  4–  J 1 2  =  v1 x2-------  =  e1 1 ------------ 1 b1  =  e1 1 ------------ 1 n1 1    1 n1 1 ------------ 1 b1  =  1 s˜1 1    =  1 s˜1 1  =  4–  J 1 3  =  v1 x3-------  =  e1 1 ------------ 2 n1 1    2 n1 1 ------------- 2 w1 1  2 n1 1 ------------- 2 w1 1  =  2 s˜1 1    =  2 s˜1 1  1  a1 1  =  1–   1   =  1–  1 n1 1 ------------- 1 w1 1  1 n1 1 ------------ 1 b1  12-44   Solved Problems  12  J 1 4  =  v1 x4-------  =  e1 1 ------------ 2 n1 1    1 n1 1 ------------ 2 b1  =  2 s˜1 1    =  2 s˜1 1  =  1–  2 n1 1 ------------ 2 b1  J 2 1  =  v2 x1-------  =  e1 2 ------------ 1 n1 2    1 n1 2 ------------- 1 w1 1  1 n1 2 ------------- 1 w1 1  =  1 s˜1 2    =  1 s˜1 2  0  a1 2  =  8–   2   =  16–  J 2 2  =  v2 x2-------  =  e1 2 ------------ 1 b1  =  e1 2 ------------ 1 n1 2    1 n1 2 ------------ 1 b1  =  1 s˜1 2    =  1 s˜1 2  =  8–  J 2 3  =  v2 x3-------  =  e1 2 ------------ 2 n1 2    2 n1 2 ------------- 2 w1 1  2 n1 2 ------------- 2 w1 1  =  2 s˜1 2    =  2 s˜1 2  1  a1 2  =  1–   4   =  4–  J 2 4  =  v2 x4-------  =  e1 2 ------------ 2 b1  =  e1 2 ------------ 2 n1 2    2 n1 2 ------------ 2 b1  =  2 s˜1 2    =  2 s˜1 2  =  1–  1 n1 2 ------------ 1 b1  2 n1 2 ------------ 2 b1  Therefore the Jacobian matrix is  J x   =  4– 16–  4– 8–  1– 4–  1– 1–  .  12-45   12 Variations on Backpropagation  Epilogue  One of the major problems with the basic backpropagation algorithm   steepest descent backpropagation — SDBP  has been the long training  times. It is not feasible to use SDBP on practical problems, because it can  take weeks to train a network, even on a large computer. Since backpropa- gation was first popularized, there has been considerable work on methods  to accelerate the convergence of the algorithm. In this chapter we have dis- cussed the reasons for the slow convergence of SDBP and have presented  several techniques for improving the performance of the algorithm.  The techniques for speeding up convergence have fallen into two main cat- egories: heuristic methods and standard numerical optimization methods.  We have discussed two heuristic methods: momentum  MOBP  and vari- able learning rate  VLBP . MOBP is simple to implement, can be used in  batch mode or incremental mode and is significantly faster than SDBP. It  does require the selection of the momentum coefficient, but   is limited to   and the algorithm is not extremely sensitive to this choice. the range   0  1 [ ,    ]  The VLBP algorithm is faster than MOBP but must be used in batch mode.  For this reason it requires more storage. VLBP also requires the selection  of a total of five parameters. The algorithm is reasonably robust, but the  choice of the parameters can affect the convergence speed and is problem  dependent.  We also presented two standard numerical optimization techniques: conju- gate gradient  CGBP  and Levenberg-Marquardt  LMBP . CGBP is gener- ally faster than VLBP. It is a batch mode algorithm, which requires a linear  search at each iteration, but its storage requirements are not significantly  different than VLBP. There are many variations of the conjugate gradient  algorithm proposed for neural network applications. We have presented  only one.  The LMBP algorithm is the fastest algorithm that we have tested for train- ing multilayer networks of moderate size, even though it requires a matrix  inversion at each iteration. It requires that two parameters be selected, but  the algorithm does not appear to be sensitive to this selection. The main  drawback of LMBP is the storage requirement. The   is the total number of weights and bi- must be inverted, is  ases in the network. If the network has more than a few thousand param- eters, the LMBP algorithm becomes impractical on current machines.   matrix, which   , where   JTJ  n  n  n  There are many other variations on backpropagation that have not been  discussed in this chapter. Some references to other techniques are given in  Chapter 19.  12-46   Further Reading  Further Reading  12  [Barn92]  E. Barnard, “Optimization for training neural nets,” IEEE  Trans. on Neural Networks, vol. 3, no. 2, pp. 232–240, 1992.  A number of optimization algorithms that have promise for  neural network training are discussed in this paper.  [Batt92]  R. Battiti, “First- and second-order methods for learning:  Between steepest descent and Newton's method,” Neural  Computation, vol. 4, no. 2, pp. 141–166, 1992.  This paper is an excellent survey of the current optimiza- tion algorithms that are suitable for neural network train- ing.  [Char92]  C. Charalambous, “Conjugate gradient algorithm for effi- cient training of artificial neural networks,” IEE Proceed- ings, vol. 139, no. 3, pp. 301–310, 1992.  [Fahl88]  [HaMe94]  This paper explains how the conjugate gradient algorithm  can be used to train multilayer networks. Comparisons are  made to other training algorithms.  S. E. Fahlman, “Faster-learning variations on back-propa- gation: An empirical study,” In D. Touretsky, G. Hinton &  T. Sejnowski, eds., Proceedings of the 1988 Connectionist  Models Summer School, San Mateo, CA: Morgan Kauf- mann, pp. 38–51, 1988.  The QuickProp algorithm, which is described in this paper,  is one of the more popular heuristic modifications to back- propagation. It assumes that the error curve can be approx- imated by a parabola, and that the effect of each weight can  be considered independently. QuickProp provides signifi- cant speedup over standard backpropagation on many  problems.  M. T. Hagan and M. Menhaj, “Training feedforward net- works with the Marquardt algorithm,” IEEE Transactions  on Neural Networks, vol. 5, no. 6, 1994.  This paper describes the use of the Levenberg-Marquardt  algorithm for training multilayer networks and compares  the performance of the algorithm with variable learning  rate backpropagation and conjugate gradient. The Leven- berg-Marquardt algorithm is faster, but requires more  storage.  12-47   12 Variations on Backpropagation  [Jaco88]  R. A. Jacobs, “Increased rates of convergence through  learning rate adaptation,” Neural Networks, vol. 1, no. 4,  pp. 295–308, 1988.  [NgWi90]  [RiIr90]  This is another early paper discussing the use of variable  learning rate backpropagation. The procedure described  here is called the delta-bar-delta learning rule, in which  each network parameter has its own learning rate that var- ies at each iteration.  D. Nguyen and B. Widrow, “Improving the learning speed  of 2-layer neural networks by choosing initial values of the  adaptive weights,” Proceedings of the IJCNN, vol. 3, pp.  21–26, July 1990.  This paper describes a procedure for setting the initial  weights and biases for the backpropagation algorithm. It  uses the shape of the sigmoid transfer function and the  range of the input variables to determine how large the  weights should be, and then uses the biases to center the  sigmoids in the operating region. The convergence of back- propagation is improved significantly by this procedure.  A. K. Rigler, J. M. Irvine and T. P. Vogl, “Rescaling of vari- ables in back propagation learning,” Neural Networks, vol.  4, no. 2, pp. 225–230, 1991.  This paper notes that the derivative of a sigmoid function  is very small on the tails. This means that the elements of  the gradient associated with the first few layers will gener- ally be smaller that those associated with the last layer.  The terms in the gradient are then scaled to equalize them.  [Scal85]   L. E. Scales, Introduction to Non-Linear Optimization. New  York: Springer-Verlag, 1985.  Scales has written a very readable text describing the ma- jor optimization algorithms. The book emphasizes methods  of optimization rather than existence theorems and proofs  of convergence. Algorithms are presented with intuitive ex- planations, along with illustrative figures and examples.  Pseudocode is presented for most algorithms.  12-48   12  [Shan90]  [Toll90]  [VoMa88]  Further Reading  D. F. Shanno, “Recent advances in numerical techniques  for large-scale optimization,” Neural Networks for Control,  Miller, Sutton and Werbos, eds., Cambridge MA: MIT  Press, 1990.  This paper discusses some conjugate gradient and quasi- Newton optimization algorithms that could be used for  neural network training.  T. Tollenaere, “SuperSAB: Fast adaptive back propagation  with good scaling properties,” Neural Networks, vol. 3, no.  5, pp. 561–573, 1990.  This paper presents a variable learning rate backpropaga- tion algorithm in which different learning rates are used  for each weight.  T. P. Vogl, J. K. Mangis, A. K. Zigler, W. T. Zink and D. L.  Alkon, “Accelerating the convergence of the backpropaga- tion method,” Biological Cybernetics., vol. 59, pp. 256–264,  Sept. 1988.  This was one of the first papers to introduce several heuris- tic techniques for accelerating the convergence of back- propagation. It included batching, momentum and variable  learning rate.  12-49   12 Variations on Backpropagation  Exercises  E12.1 We want to train the network shown in Figure E12.1 on the training set        p1  2–=        t1  =  0.8  ,     p2  2=        t2  1=              ,      where each pair is equally likely to occur.  » 2 + 2 ans =       4  Write a MATLAB M-file to create a contour plot for the mean squared error  performance index.  Input  Log-Sigmoid Layer  p  w   cid:0  cid:0 Σ n  cid:0  cid:0   b   cid:0   cid:0   a  1  a = logsig  w p + b   Figure E12.1  Network for Exercise E12.1  E12.2 Demonstrate the effect of batching by computing the direction of the initial  step for SDBP with and without batching for the problem described in Ex- ercise E12.1, starting from the initial guess  E12.3 Recall the quadratic function used in Problem P9.1:  w 0   0=  ,   b 0   =  0.5  .  F x   =  1 ---xT 10 6– 2 6– 10  x  +  4 4 x  .  We want to use the steepest descent algorithm with momentum to mini- mize this function.   i. Suppose that the learning rate is   mentum coefficient  ideas presented in Problem P12.2.    . Find a value for the mo-  for which the algorithm will be stable. Use the   0.2    =  ii. Suppose that the learning rate is     20=  . Find a value for the mo-  mentum coefficient      for which the algorithm will be stable.  12-50   Exercises  » 2 + 2 ans =       4  iii. Write a MATLAB program to plot the trajectories of the algorithm   values of both part  i  and part  ii  on the contour    and     for the  plot of    F x   , starting from the initial guess  12  x0  =  1– 2.5–  .  E12.4 Consider the following quadratic function.  F x   =  1 ---xT 3 2 1–  1– 3  x  +  4 4– x  .  We want to use the steepest descent algorithm with momentum to mini- mize this function.   i. Perform two iterations  finding   x1   and   x2  momentum, starting from the initial condition  learning rate of    and a momentum coefficient of   1=      of steepest descent with  . Use a  0.75  0 0   x0  =  =  .  T  ii. Is the algorithm stable with this learning rate and this momentum?   Use the ideas presented in Problem P12.2.  iii. Would the algorithm be stable with this learning rate, if the momen-  tum were zero?  E12.5 Consider the following quadratic function.  F x   =  1 ---xT 3 1 2 1 3  x  +  1 2 x  +  2  .  We want to use the steepest descent algorithm with momentum to mini- mize this function.   i. Suppose the learning rate is   0=  momentum coefficient is  P12.2.    . Is the algorithm stable, if the  1= ? Use the ideas presented in Problem   ii. Suppose the learning rate is  =  momentum coefficient is      0.6  1= ?  . Is the algorithm stable, if the   12-51   12 Variations on Backpropagation  E12.6 Consider the following quadratic function.  F x   =  1 ---xT 2 1 2 1 2  x  +  1 2 x  +  2  .  We want to use the steepest descent algorithm with momentum to mini- mize this function. Suppose the learning rate is  momentum coefficient  presented in Problem Eq. P12.2.  . Find a value for the   so that the algorithm will be stable. Use the ideas   1=      E12.7 For the function of Exercise E12.3, perform three iterations of the variable   learning rate algorithm, with initial guess  Plot the algorithm trajectory on a contour plot of  parameters  F x   . Use the algorithm     =  0.4  ,     =  0.1  ,     =  1.5  ,     =  0.5  ,     5%=  .  E12.8 Consider the following quadratic function:  Perform three iterations of the variable learning rate algorithm, with ini- tial guess  x0  =  1– 2.5–  .  F x   =  2 x1  +  2 2x2  .  x0  =  .  0 1–  Use the algorithm parameters    1=  ,     =  0.2  ,     =  1.5  ,     =  0.5  ,     5%=  .   Count an iteration each time the function is evaluated after the initial  guess.   E12.9 For the function of Exercise E12.3, perform one iteration of the conjugate   gradient algorithm, with initial guess  x0  =  1– 2.5–  .  12-52   Exercises  For the linear minimization use interval location by function evaluation  and interval reduction by the Golden Section search. Plot the path of the  search on a contour plot of   .  F x   12  E12.10 Consider the following quadratic function.  F x   =  1 ---xT 4 0 2 0 2  x  +  2–  1– x  .  We want to minimize this function along the line  x  =  0 0  +   1 1  .  i. Sketch this line in the   x1 x2  ,    plane.    ii. The learning rate    must fall somewhere between 0 and 3. Perform  ,    a2 b2 c2 , and indicate these points along the line that you drew in   one iteration of the golden section search. You should find  and  d2 part i.  ,   E12.11 Consider the following quadratic function.  F x   =  1 ---xT 1 1 2 1 1  x  +  1 1 x  .  We want to minimize this function along the line  x  =  0 0  +   1– 0  .  i. Use the method described on page 12-16 to determine an initial in-  terval containing the minimum. Use     =  0.5  .  ii. Take one iteration of the golden section search to reduce the inter-  val you obtained in part i.  E12.12 Consider the following quadratic function.  F x   =  1 ---xT 1 0 2 0 2  x  .  We want to minimize this function along the line  12-53   12 Variations on Backpropagation  x  =  1 1  +   1 1–  .      a3 b3    to find the  Perform two iterations of the Golden Section search   interval   and  a1 , draw the search  b1 line in the same figure and indicate your search points  points where you  evaluated   . Assume that the initial interval is defined by   . Make a rough sketch of the contour plot of     on the line.  1 2=  F x   F x   1=  0=  k    E12.13 Consider the following quadratic function.  We want to minimize this function along the line  F x   =  1 ---xT 2 0 2 0 1  x  .  x  =  0 1  +   1 1–  .      a3 b3    to find the  Perform two iterations of the Golden Section search   interval   and  a1 , draw the search  b1 line in the same figure and indicate your search points  points where you  evaluated   . Assume that the initial interval is defined by   . Make a rough sketch of the contour plot of     on the line.  1 2=  F x   F x   0=  1=  k    E12.14 We want to use the network of Figure E12.2 to approximate the function  g p   =  1  sin+   for   2–     p  2  .     ---p  4  The initial network parameters are chosen to be  w1 0   =  – –  0.27 0.41  ,   b1 0   =  – –  0.48 0.13  ,   w2 0   =  0.09 0.17  –  ,   b2 0   =  0.48  .  p  1= . Find the Jacobian matrix for the first step of the LMBP algo-  To create the training set we sample the function  and  rithm.  Some of the information you will need has been computed in the ex- ample starting on page 11-14.    at the points   g p   0=  p     12-54   12  Exercises  Input  Log-Sigmoid Layer  Linear Layer  p  w11,1  w12,1  b11   cid:0  cid:0 Σ n11  cid:0  cid:0   cid:0  cid:0 Σ n12  cid:0  cid:0   b12  1   cid:0  cid:0  a11  cid:0  cid:0   cid:0  cid:0  a12  cid:0  cid:0   1  w21,1   cid:0  cid:0 Σ n2  cid:0  cid:0  w22,1  b2   cid:0   cid:0   1  a2  a1 = logsig  W1p + b1   a2 = purelin  W2a1 + b2   Figure E12.2  Network for Exercise E12.14  E12.15 Show that for a linear network the LMBP algorithm will converge to an op-  timum solution in one iteration if     0=  .  E12.16 In Exercise E11.25 you wrote a MATLAB program to implement the SDBP   network, and trained the network to approximate   S1–  1–  1  algorithm for a  the function  g p   =  1  sin+   for   2–     p  2  .     ---p  4  » 2 + 2 ans =       4  Repeat this exercise, modifying your program to use the training proce- dures discussed in this chapter: batch mode SDBP, MOBP, VLBP, CGBP  and LMBP. Compare the convergence results of the various methods.  12-55   Objectives  13 Generalization  Objectives  Theory and Examples   Problem Statement  Methods for Improving Generalization   Estimating Generalization Error - The Test Set  Early Stopping Regularization  Bayesian Analysis  Bayesian Regularization  Relationship Between Early Stopping and Regularization   13  13-1 13-2 13-2 13-5 13-6 13-6 13-8 13-10 13-12 13-19 13-29 13-32 13-44 13-45 13-47  Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   Objectives  One of the key issues in designing a multilayer network is determining the  number of neurons to use. In effect, that is the objective of this chapter.  In Chapter 11 we showed that if the number of neurons is too large, the net- work will overfit the training data. This means that the error on the train- ing data will be very small, but the network will fail to perform as well  when presented with new data. A network that generalizes well will per- form as well on new data as it does on the training data.  The complexity of a neural network is determined by the number of free pa- rameters that it has  weights and biases , which in turn is determined by  the number of neurons. If a network is too complex for a given data set,  then it is likely to overfit and to have poor generalization.  In this chapter we will see that we can adjust the complexity of a network  to fit the complexity of the data. In addition, this can be done without  changing the number of neurons. We can adjust the effective number of  free parameters without changing the actual number of free parameters.  13-1   13 Generalization  Theory and Examples  Generalization  Ockham’s Razor  Mark Twain once said “We should be careful to get out of an experience  only the wisdom that is in it-and stop there; lest we be like the cat that sits  down on a hot stove-lid. She will never sit down on a hot stove-lid again- and that is well; but also she will never sit down on a cold one any more.”   From Following the Equator, 1897.   That is the objective of this chapter. We want to train neural networks to  get out of the data only the wisdom that is in it. This concept is called gen- eralization. A network trained to generalize will perform as well in new sit- uations as it does on the data on which it was trained.  The key strategy we will use for obtaining good generalization is to find the  simplest model that explains the data. This is a variation of a principle  called Ockham’s razor, which is named after the English logician William  of Ockham, who worked in the 14th Century. The idea is that the more  complexity you have in your model, the greater the possibility for errors.  In terms of neural networks, the simplest model is the one that contains  the smallest number of free parameters  weights and biases , or, equiva- lently, the smallest number of neurons. To find a network that generalizes  well, we need to find the simplest network that fits the data.  There are at least five different approaches that people have used to pro- duce simple networks: growing, pruning, global searches, regularization,  and early stopping. Growing methods start with no neurons in the network  and then add neurons until the performance is adequate. Pruning methods  start with large networks, which likely overfit, and then remove neurons   or weights  one at a time until the performance degrades significantly.  Global searches, such as genetic algorithms, search the space of all possible  network architectures to locate the simplest model that explains the data.  The final two approaches, regularization and early stopping, keep the net- work small by constraining the magnitude of the network weights, rather  than by constraining the number of network weights. In this chapter we  will concentrate on these two approaches. We will begin by defining the  problem of generalization and by showing examples of both good and poor  generalization. We will then describe the regularization and early stopping  methods for training neural networks. Finally, we will demonstrate how  these two methods are, in effect, performing the same operation.  Problem Statement  Let’s begin our discussion of generalization by defining the problem. We  start with a training set of example network inputs and corresponding tar- get outputs:  13-2   Problem Statement  p1 t1 { , }    {  p2 t2  , }   pQ tQ  {      ,  }  .  For our development of the concept of generalization, we will assume that  the target outputs are generated by   13.1    13.2   13  tq  =  g pq    q+  ,  g .    is some unknown function, and   where  and zero mean noise source. Our training objective will be to produce a neu- ral network that approximates    is a random, independent   , while ignoring the noise.   g .   q  The standard performance index for neural network training is the sum  squared error on the training set:  F x   =  ED  =    tq  aq–  T tq    aq–    ,   13.3   Q    q  1=  aq   is the network output for input   where    ED to represent the sum squared error on the training data, because later we  will modify the performance index to include an additional term.  . We are using the variable   pq  g .   The problem of overfitting is illustrated in Figure 13.1. The blue curve rep- resents the function  . The large open circles represent the noisy target  points. The black curve represents the trained network response, and the  smaller circles filled with crosses represent the network response at the  training points. In this figure we can see that the network response exactly  matches the training points. However, it does a very poor job of matching  the underlying function. It overfits.  There are actually two kinds of errors that occur in Figure 13.1. The first  type of error, which is caused by overfitting, occurs for input values be- tween -3 and 0. This is the region where all of the training data points oc- ccur. The network response in this region overfits the training data and  will fail to perform well for input values that are not in the training set. The  network does a poor job of interpolation; it fails to accurately approximate  the function near the training points.  The second type of error occurs for inputs in the region between 0 and 3.  The network fails to perform well in this region, not because it is overfit- ting, but because there is no training data there. The network is extrapo- lating beyond the range of the input data.  In this chapter we will discuss methods for preventing errors of interpola- tion  overfitting . There is no way to prevent errors of extrapolation, unless  the data that is used to train the network covers all regions of the input  space where the network will be used. The network has no way of knowing  what the true function looks like in regions where there is no data.  13-3  Overfitting  Interpolation  Extrapolation   13 Generalization  25  20  15  10  5  0  −5  −10  −15  −20  −25  −30 −3  25  20  15  10  5  0  −5  −10  −15  −20  −25  −30 −3  −2  −1  0  1  2  3  Figure 13.1  Example of Overfitting and Poor Extrapolation  In Figure 13.2 we have an example of a network that has been trained to  generalize well. The network has the same number of weights as the net- work of Figure 13.1, and it was trained using the same data set, but it has  been trained in such a way that it does not fully use all of the weights that  are available. It only uses as many weights as necessary to fit the data. The  network response does not fit the function perfectly, but it does the best job  it can, based on limited and noisy data.  −2  −1  0  1  2  3  Figure 13.2  Example of Good Interpolation and Poor Extrapolation  In both Figure 13.1 and Figure 13.2 we can see that the network fails to ex- trapolate accurately. This is understandable, since the network has been  provided with no information about the characteristics of the function out-  13-4   Methods for Improving Generalization  13  p  0  3–     side of the range  . The network response outside this range will  be unpredictable. This is why it is important to have training data for all  regions of the input space where the network will be used. It is usually not  difficult to determine the required input range when the network has a sin- gle input, as in this example. However, when the network has many inputs,  it becomes more difficult to determine when the network is interpolating  and when it is extrapolating.  3        3–  3–  p1  p1   and    and   This problem is illustrated in a simple way in Figure 13.3. On the left side  of this figure we see the function that is to be approximated. The range for  . The neural network was  the input variables is  trained over these ranges of the two variables, but only for  . There-  cover their individual ranges, but only half of the total  fore, both  input space is covered. When  , the network is extrapolating, and we  can see on the right side of Figure 13.3 that the network performs poorly in  this region.  See Problem P13.4 for another example of extrapolation.  If  there are many input variables, it will be quite difficult to determine when  the network is interpolating and when it is extrapolating. We will discuss  some practical ways of dealing with this problem in Chapter 22.  p2  p2  p2  p2  p1  p1    3  a   b   t  8  6  4  2  0  −2  −4  −6 3  a  8  6  4  2  0  −2  −4  −6 3  2  1  p2  0  −1  −2  −3  −3  −2  0  −1  1  p1  3  2  2  1  p2  0  −1  −2  −3  −3  −2  0  −1  1  p1  3  2  Figure 13.3  Function  a  and Neural Network Approximation  b   Methods for Improving Generalization  The remainder of this chapter will discuss methods for improving the gen- eralization capability of neural networks. As we discussed earlier, there are  a number of approaches to this problem - all of which try to find the sim- plest network that will fit the data. These approaches fit into two general  categories: restricting the number of weights  or, equivalently, the number  of neurons  in the network, or restricting the magnitude of the weights. We  will concentrate on two methods that we have found to be particularly use- ful: early stopping and regularization. Both of these approaches attempt to  restrict the magnitude of the weights, although they do so in very different   13-5   Test Set  13 Generalization  ways. At the end of this chapter, we will demonstrate the approximate  equivalence of the two methods.  We should note that in this chapter we are assuming that there is a limited  amount of data with which to train the network. If the amount of data is  unlimited, which in practical terms means that the number of data points  is significantly larger than the number of network parameters, then there  will not be a problem of overfitting.  Estimating Generalization Error - The Test Set Before we discuss methods for improving the generalization capability of  neural networks, we should first discuss how we can estimate this error for  a specific neural network. Given a limited amount of available data, it is  important to hold aside a certain subset during the training process. After  the network has been trained, we will compute the errors that the trained  network makes on this test set. The test set errors will then give us an in- dication of how the network will perform in the future; they are a measure  of the generalization capability of the network.  In order for the test set to be a valid indicator of generalization capability,  there are two important things to keep in mind. First, the test set must  never be used in any way to train the neural network, or even to select one  network from a group of candidate networks. The test set should only be  used after all training and selection is complete. Second, the test set must  be representative of all situations for which the network will be used. This  can sometimes be difficult to guarantee, especially when the input space is  high-dimensional or has a complex shape. We will discuss this problem in  more detail in Chapter 22, Practical Training Issues.  In the remaining sections of this chapter, we will assume that a test set has  been removed from the data set before training begins, and that this set  will be used at the completion of training to measure generalization capa- bility.  Early Stopping The first method we will discuss for improving generalization is also the  simplest method. It is called early stopping [WaVe94]. The idea behind this  method is that as training progresses the network uses more and more of  its weights, until all weights are fully used when training reaches a mini- mum of the error surface. By increasing the number of iterations of train- ing, we are increasing the complexity of the resulting network. If training  is stopped before the minimum is reached, then the network will effectively  be using fewer parameters and will be less likely to overfit. In a later sec- tion of this chapter we will demonstrate how the number of parameters  changes as the number of iterations increases.  Cross-Validation  In order to use early stopping effectively, we need to know when to stop the  training. We will describe a method, called cross-validation, that uses a   13-6   Methods for Improving Generalization  Validation Set  13  validation set to decide when to stop [Sarl95]. The available data  after re- moving the test set, as described above  is divided into two parts: a training  set and a validation set. The training set is used to compute gradients or  Jacobians and to determine the weight update at each iteration. The vali- dation set is an indicator of what is happening to the network function “in  between” the training points, and its error is monitored during the training  process. When the error on the validation set goes up for several iterations,  the training is stopped, and the weights that produced the minimum error  on the validation set are used as the final trained network weights.  F  This process is illustrated in Figure 13.4. The graph at the bottom of this  figure shows the progress of the training and validation performance indi- ces,    the sum squared errors , during training. Although the training er- ror continues to go down throughout the training process, a minimum of  the validation error occurs at the point labeled “a,” which corresponds to  training iteration 14. The graph at the upper left shows the network re- sponse at this early stopping point. The resulting network provides a good  fit to the true function. The graph at the upper right demonstrates the net- work response if we continue to train to point “b,” where the validation er- ror has increased and the network is overfitting.  a  b  −0.8  −0.6  −0.4  −0.2  0.2  0.4  0.6  0.8  1  −0.8  −0.6  −0.4  −0.2  0.2  0.4  0.6  0.8  1  a2  1.5  0.5  1  0  −0.5  −1  −1.5 −1  a2  1.5  0.5  1  0  −0.5  −1  −1.5 −1  a  0  p  b  Validation  Training  0  p  103  102  F  101  100  10−1  100  13-7  101  Iteration  102  Figure 13.4  Illustration of Early Stopping   13 Generalization  The basic concept for early stopping is simple, but there are several practi- cal issues to be addressed. First, the validation set must be chosen so that  it is representative of all situations for which the network will be used. This  is also true for the test and training sets, as we mentioned earlier. Each set  must be roughly equivalent in its coverage of the input space, although the  size of each set may be different.   When we divide the data, approximately 70% is typically used for training,  with 15% for validation and 15% for testing. These are only approximate  numbers. A complete discussion of how to select the amount of data for the  validation set is given in [AmMu97].  Another practical point to be made about early stopping is that we should  use a relatively slow training method. During training, the network will  use more and more of the available network parameters  as we will explain  in the last section of this chapter . If the training method is too fast, it will  likely jump past the point at which the validation error is minimized.  To experiment with the effect of early stopping, use the MATLAB® Neural  Network Design Demonstration Early Stopping  nnd13es .  Regularization The second method we will discuss for improving generalization is called  regularization. For this method, we modify the sum squared error perfor- mance index of Eq.  13.3  to include a term that penalizes network com- plexity. This concept was introduced by Tikhonov [Tikh63]. He added a  penalty, or regularization, term that involved the derivatives of the approx- imating function  neural network in our case , which forced the resulting  function to be smooth. Under certain conditions, this regularization term  can be written as the sum of squares of the network weights, as in  F x   =  ED EW  +  =      tq  aq–  T tq    aq–    +   2 xi  ,   13.4   Q    q  1=  n  i  1=     where the ratio   controls the effective complexity of the network solu- tion. The larger this ratio is, the smoother the network response.  Note that  we could have used a single parameter here, but developments in later sec- tions will require two parameters.   Why do we want to penalize the sum squared weights, and how is this sim- ilar to reducing the number of neurons? Consider again the example mul- tilayer network shown in Figure 11.4. Recall how increasing a weight  increased the slope of the network function. You can see this effect again in  Figure 13.5, where we have changed the weight   from 0 to 2. When the  weights are large, the function created by the network can have large  slopes, and is therefore more likely to overfit the training data. If we re- strict the weights to be small, then the network function will create a   w1 1  2  13-8   Methods for Improving Generalization  smooth interpolation through the training data - just as if the network had  a small number of neurons.  13  3  2  1  0  a2  2  w1 1  2=  2  w1 1  0=  −1 −2  −1  1  2  0  p  Figure 13.5  Effect of Weight on Network Response  To experiment with the effect of weight changes on the network function, use  the MATLAB® Neural Network Design Demonstration Network Function   nnd11nf .  The key to the success of the regularization method in producing a network  that generalizes well is the correct choice of the regularization ratio  .  Figure 13.6 illustrates the effect of changing this ratio. Here we have  trained a 1-20-1 network on 21 noisy samples of a sine wave.     In the figure, the blue line represents the true function, and the large open  circles represent the noisy data. The black curve represents the trained  network response, and the smaller circles filled with crosses represent the  network response at the training points. From the figure, we can see that  the ratio   produces the best fit to the true function. For ratios  larger than this, the network response is too smooth, and for ratios smaller  than this, the network overfits.     0.01  =  There are several techniques for setting the regularization parameter. One  approach is to use a validation set, such as we described in the section on  early stopping; the regularization parameter is set to minimize the squared  error on the validation set [GoLa98]. In the next two sections we will de- scribe a different technique for automatically setting the regularization pa- rameter. It is called Bayesian regularization.  13-9   13 Generalization     0=     =  0.01  −0.8  −0.6  −0.4  −0.2  0  0.2  0.4  0.6  0.8  1  −0.8  −0.6  −0.4  −0.2  0  0.2  0.4  0.6  0.8  1     =  0.25     1=  1.5  0.5  1  0  −0.5  −1  −1.5 −1  1.5  0.5  1  0  −0.5  −1  −1.5 −1  1.5  0.5  1  0  −0.5  −1  −1.5 −1  1.5  0.5  1  0  −0.5  −1  −1.5 −1  −0.8  −0.6  −0.4  −0.2  0  0.2  0.4  0.6  0.8  1  −0.8  −0.6  −0.4  −0.2  0  0.2  0.4  0.6  0.8  1  Figure 13.6  Effect of Regularization Ratio  To experiment with the effect of regularization, use the MATLAB® Neural  Network Design Demonstration Regularization  nnd13reg .  Bayesian Analysis Thomas Bayes was a Presbyterian minister who lived in England during  the 1700’s. He was also an amateur mathematician. His most important  work was published after his death. In it, he presented what is now known  as Bayes’ Theorem. The theorem states that if you have two random  events,  , then the conditional probability of the occurrence of  given the occurrence of    can be computed as   and   ,   B  A  A  B  P A B    =  P B A P A  -------------------------------  P B  .   13.5   Eq.  13.5  is called Bayes’ rule. Each of the terms in this expression has a   is called the prior probability.  name by which it is commonly referred.    It tells us what we know about  .   is called the posterior probability. This tells us what we know about   after  we learn about  . Nor-  given  mally this term is given by our knowledge of the system that describes the  relationship between  event   , and it acts as a normalization factor in Bayes’ rule.   is the marginal probability of the    is the conditional probability of    before we know the outcome of   B P B A  B P A B  A P B   and   P A  A A  .   .   A  B  B  B        13-10   2 2+  13  Methods for Improving Generalization  To illustrate how Bayes’ rule can be used, consider the following medical  situation. Assume that 1% of the population have a certain disease. There  is a test that can be performed to detect the presence of this disease. The  test is 80% accurate in detecting the disease in people who have it. Howev- er, 10% of the time, someone without the disease will register a positive  test. If you take the test and register positive, your question would be:  What is the probability that I actually have the disease? Most of us  includ- ing most physicians, as has been shown in many studies , would guess that  the probability is very high, considering that the test is 80% accurate in de- tecting the disease in a sick person. However, this turns out not to be the  case, and Bayes’ rule can help us overcome this lack of intuition, when it  comes to probability.    B  A  P A B   represent the event that you have the disease. Let   Let   represent the  event that you have a positive test result. We can then use Bayes’ rule to  , which is the probability that you have the disease, given that  find  you have a positive test. We know that the prior probability   would be  P A  is 0.8, because  0.01, because 1% of the population have the disease.   the test is 80% accurate in detecting the disease in people who have it.  No- tice that this conditional probability is based on our knowledge of the test  procedure and its accuracy.  In order to use Bayes’ rule, we need one more  term, which is  whether or not you have the disease. This can be obtained by adding the  probability of having a positive test when you have the disease to the prob- ability of having a positive test when you don’t have the disease:  . This is the probability of getting a positive test,   P B A  P B      P B    =  P A  B   P A +  B    =  P B A  P A   P B A +  P A    ,   13.6   where we have used the definition of conditional probability:  P B A    =  , or   P A  B    =  P B A  P A    .   13.7   B  P A ----------------------- P A   If we plug in our known probabilities into Eq.  13.6 , we find  P B    =  0.8    0.01  +  0.1    0.99  =  0.107  ,   13.8   P B A  where  We can now use Bayes’ rule to find the posterior probability    is 0.1, because 10% of healthy people register a positive test.   P A B      :  P A B    =  P B A P A  -------------------------------  P B  =  0.8 0.01 ------------------------   0.107  =  0.0748  .   13.9   This tells us that even if you get a positive test, you only have a 7.5% chance  of having the disease. For most of us, this result is not intuitive.  The key to Bayes’ rule is the prior probability  odds of having the disease were only 1 in 100. If this number had been  much higher, then our posterior probability   would have also in-  . In this case, the prior   P A B  P A      13-11   13 Generalization  creased significantly. It is important when using Bayes’ rule to have the  prior probability    accurately reflect our prior knowledge.  P A    For another example of using Bayes’ rule and the effect of the prior density,  see Solved Problem P13.2 and its associated demonstration.  In the next section, we will apply Bayesian analysis to the training of mul- tilayer networks. The advantage of Bayesian methods is that we can insert  prior knowledge through the selection of the prior probability. For neural  network training, we will make the prior assumption that the function we  are approximating is smooth. This means that the weights cannot be too  large, as was demonstrated in Figure 13.5. The trick will be to incorporate  this prior knowledge into an appropriate choice for the prior probability.  Bayesian Regularization Although there have been many approaches to the automatic selection of  the regularization parameter, we will concentrate on one developed by  David MacKay [MacK92]. This approach puts the training of neural net- works into a Bayesian statistical framework. This framework is useful for  many aspects of training, in addition to the selection of the regularization  parameter, so it is an important concept to become familiar with. There are  two levels to this Bayesian analysis. We will begin with Level I.  Level I Bayesian Framework  The Bayesian framework begins with the assumption that the network  weights are random variables. We then choose the weights that maximize  the conditional probability of the weights given the data. Bayes’ rule is  used to find this probability function:  P x D   M            =    P D x  M  P x  M -----------------------------------------------------------    P D   M         ,   13.10    is the vector containing all of the weights and biases in the net-  represents the training data set,    P D x  M  where  x  are parameters associ- work,  D ated with the density functions   is the   selected model - the architecture of the network we have chosen  i.e., how  many layers and how may neurons in each layer .   and    and  P x  M  , and   M          P D x  M  It is worth taking some time to investigate each of the terms in Eq.  13.10 .   is the probability density for the data, given a certain  First,   set of weights    which we will explain shortly , and the  x choice of model  . If we assume that the noise terms in Eq.  13.2  are in- dependent and have a Gaussian distribution, then  , the parameter  M      P D x  M        =  exp  –  ED    ,  1 --------------- ZD     13.11   13-12   Methods for Improving Generalization    where  squared error  as defined in Eq.  13.3  , and   is the variance of each element of   ,   =  1        2 2  2   q ED  ,    is the   ZD     =    2 2  N 2  =       N 2  ,   13.12   13  where   N   is   Q SM  , as in Eq.  12.34 .  Likelihood Function  Maximum Likelihood  Prior Density  Evidence Posterior Density  x  Eq.  13.11  is called the likelihood function. It is a function of the network  weights  , and it describes how likely a given data set is to occur, given a  specific set of weights. The maximum likelihood method selects the weights  so as to maximize the likelihood function, which in this Gaussian case is  the same as minimizing the squared error  sum squared error performance index can be derived statistically with the  assumption of Gaussian noise in the training set, and our standard choice  for the weights is the maximum likelihood estimate.  . Therefore, our standard   ED  Now consider the second term on the right side of Eq.  13.10 :  .   This is called the prior density. It embodies our knowledge about the net- work weights before we collect any data. Bayesian statistics allows us to in- corporate prior knowledge through the prior density. For example, if we  assume that the weights are small values centered around zero, we might  select a zero-mean Gaussian prior density:  P x  M    P x  M      =  exp  –  EW    1 ---------------- ZW     13.13     where  sum squared weights  as defined in Eq.  13.4  , and   is the variance of each of the weights,   ,   =  1        2 2w  2 w  EW   is the   ZW     =    2 2w  n 2       n 2  ,  =   13.14   where  n  12.35 .   is the number of weights and biases in the network, as in Eq.   The final term on the right side of Eq.  13.10  is  the evidence, and it is a normalizing term that is not a function of  objective is to find the weights  P x D   M  However, it will be important later for estimating    that maximize the posterior density  , then we do not need to be concerned with  P D   M  .   and    P D   M  . This is called  . If our     .   x  x                      With the Gaussian assumptions that we made earlier, we can rewrite the  posterior density, using Eq.  13.10 , in the following form:  13-13   Most Probable  13 Generalization  P x D   M            =  1 1   --------------- ---------------- ZW  ZD    ----------------------------------------------------------------------------------- Normalization Factor  ED EW  exp  –  +     13.15   =  1  --------------------- ZF       exp  –  F x         ZF     is a function of   where  F x  is our regularized performance index, which we defined in Eq.  13.4 . To  find the most probable value for the weights, we should maximize the pos- terior density  . This is equivalent to minimizing the regu-  larized performance index     but not a function of   P x D   M   , and    and       x      .         =  ED EW  +  F x   Therefore, our regularized performance index can be derived using Baye- sian statistics, with the assumption of Gaussian noise in the training set  and a Gaussian prior density for the network weights. We will identify the  weights that maximize the posterior density as  , or most probable. This  is to be contrasted with the weights that maximize the likelihood function:  xML  xMP  .         and   . The parameter  q  Note how this statistical framework provides a physical meaning for the  parameters   is inversely proportional to the   . Therefore, if the noise variance is  variance in the measurement noise  large,   will be large. This  will force the resulting weights to be small and the network function to be  smooth  as seen in Figure 13.6 . The larger the measurement noise, the  more we will smooth the network function, in order to average out the af- fects of the noise.   will be small, and the regularization ratio        The parameter   is inversely proportional to the variance in the prior dis- tribution for the network weights. If this variance is large, it means that  we have very little certainty about the values of the network weights, and,  therefore, they might be very large. The parameter   will then be small,  and the regularization ratio   will also be small. This will allow the net- work weights to be large, and the network function will be allowed to have  more variation  as seen in Figure 13.6 . The larger the variance in the prior  density for the network weights, the more variation the network function  will be allowed to have.       Level II Bayesian Framework     and   So far we have an interesting statistical derivation of the regularized per- formance index and some new insight into the meanings of the parameters  , but what we really want to find is a way to estimate these param-  eters from the data. In order to do this, we need to take the Bayesian anal- ysis to another level. If we want to estimate  analysis, we need the probability density   this can written   using Bayesian  . Using Bayes’ rule     P   D M   and      13-14   Methods for Improving Generalization  P   D M        =    P D   M  P   M ------------------------------------------------------------    P D M   .   13.16        and   This has the same format as Eq.  13.10 , with the likelihood function and  the prior density in the numerator of the right hand side. If we assume a  uniform  constant  prior density   for the regularization parame- , then maximizing the posterior is achieved by maximizing the  ters  likelihood function  . However, note that this likelihood func- tion is the normalization factor  evidence  from Eq.  13.10 . Since we have  assumed that all probabilities have a Gaussian form, we know the form for  the posterior density of Eq.  13.10 . It is shown in Eq.  13.15 . Now we can  solve Eq.  13.10  for the normalization factor  evidence .  P D   M  P   M            13  P D   M        =    P D x  M  P x  M ------------------------------------------------------------      P x D   M            1  --------------- ZD   -----------------------------------------------------------------------------------------------------  EW  ED  exp  exp  –  –    1 ---------------- ZW   –  1    F x   exp  --------------------- ZF    ED EW –    exp ------------------------------- --------------------------------------------- ZD   ZW  – exp  ZF    F x   –        =  =  =  ZF      ------------------------------- ZD   ZW    13.17   Note that we know the constants   and  Eq.  13.14 . The only part we do not know is  timate it by using a Taylor series expansion.   ZD     ZW   ZF       from Eq.  13.12  and    . However, we can es-  Since the objective function has the shape of a quadratic in a small area  surrounding a minimum point, we can expand  F x  lor series  see Eq.  8.9   around its minimum point,  ent is zero:   in a second order Tay- , where the gradi- xMP  F x  F xMP        +  xMP  THMP x  –  xMP    ,   13.18   1 – --- x 2  =  H   ED  EW2 2  where  Hessian evaluated at  the expression for the posterior density, Eq.  13.15 :   is the  . We can now substitute this approximation into    is the Hessian matrix of   , and   F x   xMP  +  HMP  P x D   M              exp  –  F xMP      –  xMP  THMP x xMP  –    ,   13.19   1 ------ ZF  1 – --- x 2  which can be rewritten as  13-15   Effective  of Parameters  13 Generalization  P x D   M                  1 ------exp F xMP ZF  –            exp  –  1 – --- x 2  xMP  THMP x  –  xMP    .  13.20   The standard form of the Gaussian density is  P x   =  ------------------------------------------exp  1  2  n HMP     1–   –  1 – --- x xMP 2  THMP x  –  xMP      .   13.21   Therefore, equating Eq.  13.21  with Eq.  13.20 , we can solve for   ZF          2  n 2 det HMP         1–    1 2  exp  –  F xMP        .  ZF        :   13.22    and   Placing this result into Eq.  13.17 , we can solve for the optimal values for   at the minimum point. We do this by taking the derivative with   respect to each of the log of Eq.  13.17  and set them equal to zero. This  yields  see Solved Problem P13.3 :       1–  MP  =  ------------------------- 2EW xMP    and   MP  =  N – ------------------------- 2ED xMP     ,   13.23     n  –  =  2MPtr HMP    is the total number of parameters in the network. The term   where   is called the effective number of parameters,  and   is a  n measure of how many parameters  weights and biases  in the neural net- work are effectively used in reducing the error function. It can range from  zero to   .  See the example on page 13-23 for more analysis of   .     n    Bayesian Regularization Algorithm  The Bayesian optimization of the regularization parameters requires the  computation of the Hessian matrix of  . We  propose using the Gauss-Newton approximation to the Hessian matrix  [FoHa97], which is readily available if the Levenberg-Marquardt optimiza- tion algorithm is used to locate the minimum point  see Eq.  12.31  . The  additional computation required for optimization of the regularization is  minimal.   at the minimum point   F x   xMP  Here are the steps required for Bayesian optimization of the regularization  parameters, with the Gauss-Newton approximation to the Hessian matrix:  0.  ,   Initialize    and then   and  ED using Eq.  13.23 .  EW   and the weights. The weights are initialized randomly,       are computed. Set   , and compute    and   n=      1. Take one step of the Levenberg-Marquardt algorithm toward minimiz-  ing the objective function   ED EW 2. Compute the effective number of parameters   F x   =  +  .    =  n  –  2tr H   1–  , mak-  13-16   Methods for Improving Generalization  ing use of the Gauss-Newton approximation to the Hessian available in  the Levenberg-Marquardt training algorithm:  H J , where  training set errors  see Eq.  12.37  .   is the Jacobian matrix of the   2JTJ  2In  F x   2    =  +  3. Compute new estimates for the regularization parameters     =  13    ------------------ 2EW x      and     =  N – ------------------ 2ED x   .  4. Now iterate steps 1 through 3 until convergence.     and    the objective function   Bear in mind that with each reestimate of the regularization parameters   point is moving. If traversing the performance surface generally moves to- ward the next minimum point, then the new estimates for the regulariza- tion parameters will be more precise. Eventually, the precision will be good  enough that the objective function will not significantly change in subse- quent iterations. Thus, we will obtain convergence.   changes; therefore, the minimum   F x   GNBR  When this Gauss-Newton approximation to Bayesian regularization   GNBR  algorithm is used, the best results are obtained if the training data  is first mapped into the range [-1,1]  or some similar region . We will dis- cuss this preprocessing of the training data in Chapter 22.  In Figure 13.7 you can see the results of training a 1-20-1 network with  GNBR on the same data set represented in Figure 13.4 and Figure 13.6.  The network has fit the underlying function, without overfitting to the  noise. The fit looks similar to that obtained in Figure 13.6, with the regu- larization ratio set to  with GNBR, the final regularization ratio for this example was     . In fact, at the completion of training   0.0137     0.01  =  =  .  The training process for this example is illustrated in Figure 13.8. In the  upper left of this figure, you see the squared error on the training set. No- tice that it does not necessarily go down at each iteration. In the upper  right of the figure, you see the squared testing error. This was obtained by  comparing the network function to the true function at a number of points  between -1 and 1. It is a measure of the generalization capability of the net- work.  This would not be possible in a practical case, where the true func- tion was unknown.  Note that the testing error is at its minimum at the  completion of training.   13-17   13 Generalization  1.5  0.5  1  0  −0.5  −1  −1.5 −1  102  100  ED  10−2  100  10−1  10−2  10−3  10−4  100     −0.8  −0.6  −0.4  −0.2  0  0.2  0.4  0.6  0.8  1  Figure 13.7  Bayesian Regularization Fit        and the effective num- Figure 13.8 also shows the regularization ratio   during training. These parameters have no particular  ber of parameters  meaning during the training process, but at the completion of training they  are significant. As we mentioned earlier, the final regularization ratio was  , which is consistent with our earlier investigation of regular-   ization—illustrated in Figure 13.6. The final effective number of parame- ters was  . This is out of a total of 61 total weights and biases in the  network.  0.0137  5.2  =  =    Training  Testing  101 102 Iteration  103  101 102 Iteration  103  ED  103  102  101  100  100  102    101  101 102 Iteration  103  100  100  101 102 Iteration  103  Figure 13.8  Bayesian Regularization Training Process  The fact that in this example the effective number of parameters is much  less than the total number of parameters  6 versus 61  means that we   13-18   13  Methods for Improving Generalization  might well have been able to use a smaller network to fit this data. There  are two disadvantages of a large network: 1  it may overfit the data, and 2   it requires more computation to calculate the network output. We have  overcome the first disadvantage by training with GNBR; although the net- work has 61 parameters, it is equivalent to a network with only 6 parame- ters. The second disadvantage is only important if the calculation time for  the network response is critical to the application. This is not usually the  case, since the time to calculate a network response to a particular input is  measured in milliseconds. In those cases where the calculation time is sig- nificant, you can train a smaller network on the data.  On the other hand, when the effective number of parameters is close to the  total number of parameters, this can mean that the network is not large  enough to fit the data. In this case, you should increase the size of the net- work and retrain on the data set.  To experiment with Bayesian Regularization, use the MATLAB® Neural  Network Design Demonstration Bayesian Regularization  nnd17breg .  Relationship Between Early Stopping and Regularization We have discussed two techniques for improving network generalization:  early stopping and regularization. These two methods were developed in  very different ways, but they both improve generalization by restricting the  network weights and, therefore, producing a network with fewer effective  parameters. Early stopping restricts the network weights by stopping the  training before the weights have converged to the minimum of the squared  error. Regularization restricts the weights by adding a term to the squared  error that penalizes large weights. In this section we want to demonstrate,  using a linear example, an approximate equivalence of these two methods.  During the process, we will also shed some light on the meaning of the ef- fective number of parameters,  . This development is based on the more  general procedures described in [SjLj94].    Early Stopping Analysis  Consider the single layer linear network shown in Figure 10.1. We have  shown in Eq.  10.12  and Eq.  10.14  that the mean square error perfor- mance function for this linear network is quadratic, of the form  F x   =  c dTx +  +  1 ---xTAx 2  ,   13.24   A   is the Hessian matrix. In order to study the performance of early  where  stopping, we will analyze the evolution of the steepest descent algorithm on  this linear network. From Eq.  10.16 , we know that the gradient of the per- formance index is  F x   =  Ax d+  .   13.25   13-19   13 Generalization  Therefore, the steepest descent algorithm  see Eq.  9.10   will be  xk  1+  =  xk gk  –  =  xk  Axk d+  –      .   13.26   We want to know how close we come to the minimum of the squared error  at each iteration. For quadratic performance indices, we know that the  minimum will occur at the following point  see Eq.  8.62  :  xML  –=  A 1– d  ,   13.27   where the superscript   indicates that this result maximizes the likeli- hood function, in addition to minimizing the squared error, as we saw in  Eq.  13.11 .  ML  We can now rewrite Eq.  13.26  as  xk  1+  =  xk A xk A 1– d  +  –      =  xk A xk  –    –  xML    .   13.28   With some additional algebra we can find  xk  1+  =  I A–  xk AxML  +  =  Mxk  +  I M–  xML  ,   13.29   where  Starting at the first iteration, using Eq.  13.29 , we have  . The next step is to relate   I A–  M  xk  1+  =     to the initial guess   x0  .   x1 Mx0  =  +  I M–  xML  ,   13.30   where the initial guess  Continuing to the second iteration:  x0   usually consists of random values near zero.   +  =  =  I M–  x2 Mx1  xML M2x0 M I M– + M2x0 MxML M2xML – M2x0 =  xML I M– xML MxML + – M2x0 I M– Following similar steps, at the kth iteration we have  xML M2xML  xML  =  +  +  +  +  =  –    2  xML  .   13.31   xk Mkx0  =  +  I Mk –  xML  ,   13.32   This key result shows how far we progress from the initial guess to the  maximum likelihood weights in k iterations. We will use this result later to  compare with regularization.  13-20   Methods for Improving Generalization  Recall from Eq.  13.4  that the regularized performance index adds a pen- alty term to the sum squared error, as in  Regularization Analysis  F x   =  ED EW  +  .   13.33   13  For the following analysis, it will more convenient to consider the following  equivalent  because the minimum occurs at the same place  performance  index  F x   =  F x  -----------  =  ED  +   ---E  W  =  ED EW  +  ,   13.34   which has only one regularization parameter.  The sum squared weight penalty term   EW   can be written  EW  =  x x0–  T x  x0–    ,   13.35   where the nominal value    is normally taken to be the zero vector.  x0  In order to locate the minimum of the regularized performance index,  which is also the most probable value  to zero:  , we will set the gradient equal   xMP  F  x   =  E D  EW+  =  0  .  The gradient of the penalty term, Eq.  13.35 , is  EW  =  2 x x0–    .   13.36    13.37   From Eq.  13.25  and Eq.  13.28 , the gradient of the sum squared error is  E D  =  Ax d+  =  A x A 1– d  +    =  A x  –  xML    .   13.38   We can now set the total gradient to zero:  F  x   =  A x  –  xML    +  2 x  x0–    =  0  .   13.39   The solution of Eq.  13.39  is the most probable value for the weights,  We can make that substitution and perform some algebra to obtain  xMP  .   A xMP    –  xML    =  =    2 xMP 2 xMP    –  –  x0– –   xML  =    –  –  xML   x0–    2 xMP – 2 xML xML  –      :    xMP  Now combine the terms multiplying   +  xML  x0–     13.40   13-21   13 Generalization  Solving for     xMP  –  A 2I + xML    , we find   xMP   –  xML    =  2 x0    –  xML    .   13.41     xMP  –  xML    =  2 A 2I  +   1– x0    –  xML   M x0  =    –  xML    ,   13.42   where   M  =  2 A 2I  +   1–  .   We want to know the relationship between the regularized solution  and the minimum of the squared error  xMP  , so we can solve Eq.  13.42  for   xMP  xML  :     xMP Mx0  =  +  I M –  xML  .   13.43   This is the key result that describes the relationship between the regular- ized solution and the minimum of the squared error. By comparing Eq.   13.43  with Eq.  13.32 , we can investigate the relationship between early  stopping and regularization. We will do that in the next section.  Connection Between Early Stopping and Regularization  To compare early stopping and regularization, we need to compare Eq.   13.43  and Eq.  13.32 . They are summarized in Figure 13.9. We would like  to find out when these two solutions are equal. In other words, when do ear- ly stopping and regularization produce the same weights?  Early Stopping  Regularization  xk Mkx0  =  +  I Mk –  xML  xMP Mx0  =  +  I M –  xML  M  =  I A–    M  =  2 A 2I  +   1–  Figure 13.9  Early Stopping and Regularization Solutions  M  Mk The key matrix for early stopping is   1– . If these two matrices are equal, then  regularization is  the weights for early stopping will be the same as the weights for regular-  are the same  ization. In Eq.  9.22  we showed that the eigenvectors of  as the eigenvectors of  1 i –  are then where the eigenvalues of    and that the eigenvalues of  A  M . The eigenvalues of   . The key matrix for   M  are  Mk  2 A 2I  I A–   are   +  A  k  ,   =  =    i  eig Mk      =  1 i –  k  .   13.44   Now let’s consider the matrix  led to Eq.  9.22 , we can show that the eigenvectors of  same as the eigenvectors of  , and the eigenvalues of   . First, using the same procedures that   are the   are   A 2I + A 2I +  M  A     13-22   Methods for Improving Generalization    . Also, the eigenvectors of the inverse of a matrix are the same as   2 i+ the eigenvectors of the original matrix, and the eigenvalues of the inverse  are the reciprocals of the original eigenvalues. Therefore, the eigenvectors   are of    are the same as the eigenvectors of   , and the eigenvalues of   A  M  M  13  Therefore, in order for  values:  Mk   to equal   M  , they just need to have equal eigen-  eig M      =  2  ----------------------  2+ i  .  2  ----------------------  2+ i  =  1 i –  k  .  Take the logarithm of both sides:  i   2------+ 1  These expressions are equal at  i derivatives are equal. Taking derivatives of both sides, we have  1 i –  log–  0=  log  =  k    .  , so they will always be equal if their    13.47   or  1 ---  1  –  --------------------- i   2------+ 1   k  =  ----------------- – 1 i  –    ,  k  =  1 2------     – 1 i ----------------------------------   2 + 1 i    .  k  1 2------  .   is small  slow, stable learning  and   i  If  the approximate result  i    2     is small, then we have    13.45    13.46    13.48    13.49    13.50   Therefore, early stopping is approximately equivalent to regularization. In- creasing the number of iterations   is approximately the same as decreas- k ing the regularization parameter  . This makes intuitive sense, because   increasing the number of iterations, or decreasing the regularization pa- rameter, can lead to overfitting.  Example, Interpretation of Effective Number of Parameters  2 2+  We will illustrate this result with a simple example. Suppose that we have  a single layer, linear network with no bias. The input target pairs are given  by   13-23   13 Generalization      1 1  p1  =    t1  1=  ,   p2  =    t2  1–=          1– 1  ,      where the probability of the first pair is 0.75, and the probability of the sec- ond pair is 0.25. Following Eq.  10.13  and Eq.  10.15 , we can find the qua- dratic mean square error performance index as  c  =  E t2    =  1 2 0.75      1–  2 0.25      +  =  1  ,  h  =  E tz    =    0.75  +    0.25    1–     1  1 1  1– 1  =  ,  1 0.5  d  =  2h–  =  2–   1 0.5  =  ,  2– 1–  A  =  2R  =  2 E zzT         =    2 0.75      1 1  1 1  +  0.25  1– 1  =  1– 1      2 1 1 2  ,  ED  =  c  +  T  xTd 1 ---x + 2  Ax  .  The minimum of the mean squared error occurs at  xML  =  A–  1– d  =  R 1– h  =  1–  1 0.5 0.5 1  1 0.5  =  .  1 0  Now let’s investigate the eigensystem of the Hessian matrix of   ED  :  2ED x   =  A  =  2R  =  2 1 1 2  .  To find the eigenvalues:  A I–  =  2 –  1  1  2 –  =  2 4–  =+  3   1–    3–    ,  To find the eigenvectors:  1  =  1           3=  .  2  13-24   Methods for Improving Generalization  For   1  1=  ,  and for   2  3=  ,  A I–  v  0=  .  1 1 1 1  v1  =  0        v1  =  ,  1 1–  1– 1 1 1–  v2  =  0        v2  =  .  1 1  The contour plot for    is shown in Figure 13.10  ED  13  0.5  1  0  −0.5  x2  13-25  −1 −0.5  0  1  1.5  0.5  x1  Figure 13.10  Contour Plot for   ED  Now consider the regularized performance index of Eq.  13.34 . Its Hessian  matrix will be  2F x   =  E2  D  EW2  +  =  E2  D  +  2I  =  2 1 1 2  +   2 0 0 2  =  2  2+ 1  1 2+  2  .  In Figure 13.11 we have contour plots for   F   as      is equal to 0, 1 and     .   13 Generalization    =  0.5    1=    0=  1  0  1  0  x2  −0.5  x2  −0.5  In Figure 13.12 the blue curve represents the movement of  ied.   xMP   as      is var-  −1 −0.5  0  1  1.5  0.5  x1  Figure 13.11  Contour Plot for   F  xMP    =  0.5    0=  −1 −0.5  0  1  1.5  0.5  x1  Figure 13.12    xMP   as      is Varied  Now let’s compare this regularization result with early stopping. Figure  13.13 shows the steepest descent trajectory for minimizing  , starting  from very small values for the weights. If we stop early, the result will fall  along the blue curve. Notice that this curve is very close to the regulariza- tion curve in Figure 13.12. If the number of iterations is very small, this is  equivalent to a very large value for  . As the number of iterations increas- es, it is equivalent to reducing   ED    .    13-26   Methods for Improving Generalization  13  0.5  1  0  −0.5  x2  13-27  −1 −0.5  0  1  1.5  0.5  x1  Figure 13.13  Steepest Descent Trajectory  To experiment with the relationship between Early Stopping and Regular- ization, use the MATLAB® Neural Network Design Demonstration Early  Stopping Regularization  nnd17esr .  v2  2ED x  2   and the results of regularization   is larger than   It is useful to consider the relationship between the eigenvalues and eigen- vectors of the Hessian matrix   has higher  and early stopping. In this example,  curvature in the   direction. This means that we will get a quicker reduc- tion in the squared error if we move in that direction first. This is shown in  Figure 13.13, as the initial steepest descent movement is almost in the di- . Note also that in regularization, as shown in Figure 13.12, as  rection of   direction.   For a given change in the weights, this direction provides the largest reduc- tion in the squared error.   decreases from a large value, the weights move first in the   , so   ED  1  v2  v2  v1  v1  1   in the    is smaller than   , we only move in the  ED   direc-  direction. This  1   direction at all. We would only need to move in the   Since the eigenvalue  2 tion after achieving significant reduction in  v2 would be even more pronounced if the difference between   were  , we would not have to move in  greater. In the limiting case, where  the   direction to  get the complete reduction in the squared error.  This would be the case of  the stationary valley, as in Figure 8.9.  Note that in this case we would only  be effectively using one parameter, even though the network has two  weights.  Of course, this one effective parameter is some combination of the  two weights.  Therefore, the effective number of parameters is related to  the number of eigenvalues of   that are significantly different than  zero. We will analyze this in detail in the next section.  2ED x    and   0=  1  2  v2   13 Generalization  Recall the previous definition for the effective number of parameters:  Effective Number of Parameters    =  n 2MPtr HMP    –     1–     13.51   We can express this in terms of the eigenvalues of  write the Hessian matrix as  2ED x   . First, we can   H x   =  2F x   =   E2  D  EW2  +  =   E2  D  +  2I  .   13.52   Using arguments similar to those leading to Eq.  13.44 , we can show that  H x   are  . We can then use two properties of  the eigenvalues of  i  tr H 1– eigenvalues to compute   are the re-  ciprocals of the eigenvalues of  H to the sum of its eigenvalues. Using these two properties, we can write  2+ . First, the eigenvalues of  , and, second, the trace of a matrix is equal   H 1–    tr H 1–    n  =  i  1=  1 --------------------- 2+ i  .  We can now write the effective number of parameters as    =  n  –  2MPtr HMP       1–    =  n  n  –  i  1=  2 --------------------- 2+ i  =  n    i  1=  i --------------------- i 2+  ,   13.54   or  where    =  n    i  1=  i --------------------- i 2+  =  n    i  1=  i  ,  i  =  i --------------------- i 2+  .   13.53    13.55    13.56   0    , so the effective number of parameters   1 . If all of the eigenvalues of    must fall be- Note that   i tween zero and   are large, then the  n effective number of parameters will equal the total number of parameters.  If some of the eigenvalues are very small, then the effective number of pa- rameters will equal the number of large eigenvalues, as was also demon- strated by our example in the previous section. Large eigenvalues mean  large curvature, which means that the performance index changes rapidly  along those eigenvectors. Every large eigenvector represents a productive  direction for optimizing performance.  2ED x     13-28   Summary of Results  Summary of Results  Problem Statement  A network trained to generalize will perform as well in new situations as it  does on the data on which it was trained.  13  ED  =    tq  aq–  T tq    aq–    Q  q  1=  Methods for Improving Generalization  Estimating Generalization Error - The Test Set Given a limited amount of available data, it is important to hold aside a cer- tain subset during the training process. After the network has been  trained, we will compute the errors that the trained network makes on this  test set. The test set errors will then give us an indication of how the net- work will perform in the future; they are a measure of the generalization  capability of the network.  Early Stopping The available data  after removing the test set  is divided into two parts: a  training set and a validation set. The training set is used to compute gra- dients or Jacobians and to determine the weight update at each iteration.  When the error on the validation set goes up for several iterations, the  training is stopped, and the weights that produced the minimum error on  the validation set are used as the final trained network weights.  Regularization  F x   =  ED EW  +  =      tq  aq–  T tq    aq–    +   2 xi  Q    q  1=  n  i  1=  Bayesian Regularization  Level I Bayesian Framework    P D x  M  P x  M -----------------------------------------------------------  P D   M  =              P x D   M        13-29   13 Generalization  P D x  M        =  exp  –  ED    ,     =  1      2 2    1 --------------- ZD    ZD     =    2 2  N 2  =       N 2  P x  M      =  exp  –  EW    ,     =  1      2 2w    1 ---------------- ZW    ZW     =    2 2w  n 2  =       n 2  P x D   M            =  exp  –  F x     1  --------------------- ZF       Level II Bayesian Framework  P   D M        =    P D   M  P   M ------------------------------------------------------------    P D M   MP  =     ------------------------- 2EW xMP    and   MP  =  N – ------------------------- 2ED xMP       =  n  –  2MPtr HMP     1–  Bayesian Regularization Algorithm  0.  ,  Initialize     and  and then  ED using Eq.  13.23 .  EW   and the weights. The weights are initialized randomly,       are computed. Set   , and compute    and   n=      1. Take one step of the Levenberg-Marquardt algorithm toward minimiz-  ing the objective function   ED EW 2. Compute the effective number of parameters   F x   =  +  .  , mak- ing use of the Gauss-Newton approximation to the Hessian available in  the Levenberg-Marquardt training algorithm:  H J , where  training set errors  see Eq.  12.37  .   is the Jacobian matrix of the   N 2tr H  2JTJ  2In  F x   2    =  +  =  –     1–  3. Compute new estimates for the regularization parameters     =    ------------------ 2EW x      and     =  N – ------------------ 2ED x   .  4. Now iterate steps 1 through 3 until convergence.  13-30   Summary of Results  Relationship Between Early Stopping and Regularization  Early Stopping  Regularization  xk Mkx0  =  +  I Mk –  xML  xMP Mx0  =  +  I M –  xML  13  M  =  I A–    M  =  2 A 2I  +   1–  eig Mk      =  1 i –  k  eig M      =  2  ----------------------  2+ i  k  1 2------  n    =  i  1=  i --------------------- i 2+  0       n  Effective Number of Parameters  13-31   13 Generalization  Solved Problems  P13.1 In this problem and in the following one we want to investigate the  relationship between maximum likelihood methods and Bayesian  methods. Suppose that we have a random variable that is uniform- ly distributed between 0 and x. We take a series of Q independent  samples of the random variable. Find the maximum likelihood es- timate of x. Before we begin this problem, let’s review the Level I Bayesian formulation  of Eq.  13.10 . We will not need the Level II formulation for this simple  problem, so we do not need the regularization parameters. Also, we only  have a single parameter to estimate, so x is a scalar. Eq.  13.10  can then  be simplified to  P x D    =    P x  P D x ------------------------------  P D  .  We are interested in the maximum likelihood estimate for this problem, so  we need to find the value of x that maximizes the likelihood term  .  The data is the Q independent samples from the uniformly distributed ran- dom variable. A graph of the uniform density function is given in Figure  P13.1.  P D x      f t x    1 --- x  t  x  Figure P13.1  Uniform Density Function  The definition can be written  f t x    =  t  x      1   --- 0 x    0 elsewhere  .  13-32   Solved Problems  If we have Q independent samples of the random variable, then we can  multiply each of the individual probabilities to get the joint probability of  all samples:  P D x      =   f ti x    =  Q    i  1=  x    for all i  ti      1  xQ-----  0     0 elsewhere  =     1 xQ-----  x max ti      0 x max ti   The plot of the resulting likelihood function is shown in Figure P13.1.  13  P D x      max ti   x  Figure P13.2  Likelihood Function for Solved Problem P13.1  From this plot, we can see that the value of  function is   x   that maximizes the likelihood   xML  =  max ti   .   is the maximum value  Therefore, the maximum likelihood estimate of  obtained from the Q independent samples of the random variable. This  seems like a reasonable estimate of  dom variable.  , which is the upper limit of the ran-  x  x  P13.2 In this problem we will compare the maximum likelihood and  Bayesian estimators. Assume that we have a series of measure- ments of a random signal in noise:  Assume that the noise has a Gaussian density, with zero mean:  ti  =  x  i+  .  13-33   13 Generalization  f i    =  1 -------------- 2  exp  –      2  i  --------- 22   i. Find the maximum likelihood estimate of   x  .  ii. Find the most probable estimate of    is a  zero-mean random variable, with Gaussian prior density:  . Assume that   x  x  f x   =  1 ---------------- 2x  exp  –       x2  ----------- 2  2x  =  1 ---------------- ZW    exp  –  EW      P D x  i. To find the maximum likelihood estimate, we need to find the likelihood  function  . The first  step is to use the noise density to find the density of the measurement.  Since, with  the density for the noise, but with a mean of    given, the density for the measurement would be the same as   . This represents the density of the data, given   , we have  x  x    x   f ti x    =  1 -------------- 2  exp  –      2  x– ti  ------------------ 22   .  Assuming that the measurement noises are independent, we can multiply  the probability densities:  P D x      =  f t1 t2  tQ x           =   f t1 x  f t2 x    f tQ x      =  P D x      =  1 -------------------------- Q 2 Q 2  exp  –  =  1 ----------- Z    exp  –  ED    Q  x– ti   2    1= i  ---------------------------     22          where    =  1 --------- 22  Q    i  1=  Q    i  1=  ,   ED  =  x– ti  2  =  2  ei  ,   Z     =       Q 2  .  To maximize the likelihood, we should minimize  to zero, we find  ED  . Setting the derivative   dED xd  =  d xd  Q    i  1=  Q    i  1=  x– ti  2  =  2–  x– ti    =  –  2  –  Qx  =  0  .  Q      i  1=    ti   13-34   Solved Problems  Solving for   x  , we find the maximum likelihood estimate:  ii. To find the most probable estimate, we need to use Bayes’ rule  Eq.   13.10   to find the posterior density:  13  xML  Q  =  1 ---- Q  ti  i  1=  P x D    =    P x  P D x ------------------------------  P D  .  P D x      =  exp  –  ED    1 ----------- Z    The likelihood function   P D x       was found above to be  The prior density is  where  P x   =  f x   =  1 ---------------- 2x  exp  –       x2  ----------- 2  2x  =  1 ---------------- ZW    exp  –  EW    ,  The posterior density can then be computed as    =  1 ----------- 2 2x  P x D    =  ,   ZW     =       1 2  ,   EW  x2=  .            f x t1 t2  tQ   f t1 t2  tQ f x    x -------------------------------------------------   f t1 t2  tQ  1   --------------- ZD   -----------------------------------------------------------------------------------   1 ---------------- ZW   Normalization Factor  ED EW  exp  –  +    =  =  To find the most probable value for x, we maximize the posterior density.  This is equivalent to minimizing  To find the minimum, we take the derivative with respect to  equal to zero:  x   and set it   ED EW  +  =    x– ti  2  x2  .  +  Q    i  1=  13-35   13 Generalization  d ED EW  xd  +    =  Q     d   xd   x– ti  2  +  =  2–  x– ti    +  2x   x2    Q    i  1=  =  –  2  –  Qx  +  2x  =  2–   –   Q+  x  =  0  i  1= Q        1= Q  i  i  1=    ti     ti   Solving for     , we obtain  Q         ti  ---------------------  Q+  1=  i  xMP  =            xML   goes to zero  variance   Notice that as   approach- es  . Increasing the variance of the prior density represents increased  uncertainty in our prior knowledge about x. With large prior uncertainty,  we rely on the data for our estimate of x, which leads to the maximum like- lihood estimate.   goes to infinity ,   x  xMP  2  2    ,   2  2=  1=  Figure P13.3 illustrates  . Here the variance associated with the  x 1= measurement is smaller than the variance associated with our prior densi- xML ty for   than it is to the maximum of the  = prior density, which occurs at 0.   for the case where   ,   P x  1=  P D x  and  t1   is closer to   P x D   and   , so   xMP  t1  Q  ,   =  1  x    To experiment with this signal in noise example, use the MATLAB® Neural  Network Design Demonstration Signal Plus Noise  nnd13spn .  13-36   P x D    P D x      13  xMP  −2  −1  0  1  2  3  Figure P13.3  Prior and Posterior Density Functions  P13.3 Derive Eq.  13.23 .     and   MP To solve for  , given in Eq.  13.17 , with respect to  , and set the de-  P D   M rivatives to zero. Taking the log of Eq.  13.17 , and substituting Eq.  13.12 ,  Eq.  13.14  and Eq.  13.22 , we obtain  , we will take the derivatives of the log of    and   MP        log  P D   M        =  –  log ZD         –  log ZW           log ZF n ---log 2 2  =    –  1 ---logdet HMP 2      –  F xMP      –  log     ---   –  log  n --- 2     ---   =  –  F xMP      –  log  det HMP    log      +  log      +  log  2  –  log    1 --- 2   N + ---- 2  n --- 2  N ---- 2  N ---- 2  n --- 2    =  F  2   in Eq.  13.4 , we can write it as  F 2=   We will consider first the second term in this expression. Since  Hessian of    ED  H be an eigenvalue of    for all corresponding eigenvalues. Now we take the derivative of the second  term in the above equation with respect to  . Since the determinant of a  matrix can be expressed as the product of its eigenvalues, we can reduce it  as shown below, where  .   , where  EW  = b  be an eigenvalue of    is the trace of the inverse of the Hessian   . If we let  h  ED2 B = , then  B  h 2+  B 2I  2+ H   is the   tr H 1–    and   b  H  H    =  +    Solved Problems  0.6  0.4  0.2  0  −3  P x   13-37   13 Generalization   1 ---  2  log H  det  =  1   ----------------  2detH   h    n  k         1= n  1=  i  1   ----------------  2detH    b i  2+       =  =  =  1  ---------------- 2detH  n    i  1=     j  i    b j         1= i ----------------------------------------------  2+  i  j    b i  2+    n  n  n    i  1=  =  i  1=  1 ------------------- b i 2+  tr H 1–     =    b j  2+      b   i    2+    Next, we will take the derivative of the same term with respect to  define the parameter  step. The parameter    . First,  , as shown below, and expand it for use in our next    is referred to as the effective number of parameters.      –  n  2tr H 1–     N  =  n  – 2  i  1=  1 ------------------- b i 2+  =  n    1=  i 1 --- 2   – 1  2  -------------------  b 2+ i  =  n    i  1=      b  i  ------------------- b  2+ i  =  n    i  1=  b i h----- i  Now take the derivative of   log  det HMP       with respect to     .  13-38   13  Solved Problems   1 ---  2  log  detH  =  1   ----------------  2detH   h k    n  k         1= n  1=  i  1   ----------------  2detH    b i  2+       =  =  1  ---------------- 2detH  n    i  1=     j  i    b j  2+      b   i    2+    n      b  i     -----   1= i ---------------------------------------------------------  2+  b j      j  i n  =  1 --- 2      b i  i  1= b i ------------------- b i 2+  2+    =   2------  n  =  1 2------  i  1=  where the fourth step is derived from the fact that  B value of   , and therefore the derivative of   b i  with respect to    which is   b i  B  .  b  i   is an eigenvalue of   is just the eigen-   Now we are finally ready to take the derivatives of all terms in  log    and set them equal to zero. The derivative with respect to   P D   M  will be           log  P D   M        –=      1 F wMP ---logdet HMP  –   2  EW wMP    tr HMP    1–  –          +    +   n ---log  2 n  –=  =  –  EW wMP      tr HMP    1–  –  +  ------------- 2MP  0=  n  ------------- 2MP  Rearranging terms, and using our definition of     , we have  EW wMP          2MPEW wMP MP  =  =  =  tr HMP    1–  –  n  ------------- 2MP    n  –  2MPtr HMP    --------------------------- 2EW wMP    1–  =  13-39   13 Generalization  We now repeat the process for     .     log  P D   M        –=    +   N ----log  2      1 F wMP ---logdet HMP  –   2  ED wMP    N  +  –            –=  ------------ 2MP N  +  ------------ 2MP  ------------ 2MP  0=    ------------ 2MP  =  –  ED wMP      –  Rearranging terms,  ED wMP      =  MP  =  N  ------------ 2MP  –    ------------ 2MP  N – -------------------------- 2ED wMP    P13.4 Demonstrate that extrapolation can occur in a region that is sur-  rounded by training data.  Consider the function displayed in Figure 13.3. In that example, extrapo- lation occurred in the upper left region of the input space, because all of the  training data was in the lower right. Let’s provide training data around the  outside of the input space, but without data in the region  1.5–    p1    1.5  1.5–    p2    1.5  .  The training data is distributed as shown in Figure P13.4.  3  2  1  0  −1  −2  −3 −3  13-40  −2  −1  0  1  2  3  Figure P13.4  Training Data Locations   Solved Problems  The result of the training is shown in Figure P13.5. The neural network ap- proximation significantly overestimates the true function in the region  without training data, even though surrounded by regions with training  data. In addition, this result is random. With a different set of initial ran- dom weights, the network might underestimate the true function in this re- gion. Extrapolation occurs because there is a significantly large region  without training data. When the input space is of high dimension, it can be  very difficult to tell when a network is extrapolating. It cannot be done by  simply checking the individual ranges of each input variable.  13  a   b   t  8  6  4  2  0  −2  −4  −6 3  a  8  6  4  2  0  −2  −4  −6 3  2  1  0  p2  −1  −2  −3  −3  −2  0  −1  1  p1  3  2  2  1  0  p2  −1  −2  −3  −3  −2  0  −1  1  p1  3  2  Figure P13.5  Function  a  and Neural Network Approximation  b   P13.5 Consider the example starting on page 13-23. Find the effective   number of parameters if     1=  .  To find the effective number of parameters, we can use Eq.  13.55 :  We earlier found the eigenvalues to be  parameter is   1  1=  ,   2  3=  . The regularization   n    =  i  1=  i --------------------- i 2+  .    =  =  1  .   ---  We can rewrite      in terms of      as follows    =  n    i  1=  i --------------------- i 2+  =  n    i  1=  i + 2  ------------------  i ---  =  n    i  1=  i ----------------- i 2+  .  13-41   13 Generalization  Substituting our numbers, we find    =  n    i  1=  i ----------------- i 2+  =  1 ------------ 1 2+  +  3 ------------ 3 2+  =  1 --- 3  3 ---+ 5  =  14 ------ 15  .  Therefore, we are using approximately one of the two available parame- ters. The network has two parameters:  . The parameter we  are using is not one of these two, but rather a combination. As we can see  from Figure 13.11, we move in the direction of the second eigenvector:   and   w1 2  w1 1  v2  =  ,  1 1  which means that we are changing   by the same amount. Al- though there are two parameters, we are effectively using only one. Since  v2  is the eigenvector with the largest eigenvalue, we move in that direction  to obtain the greatest reduction in the squared error.    and   w1 2  w1 1  P13.6 Demonstrate overfitting with polynomials. Consider fitting a poly-  nomial  gk p   =  x0  +  x1p  +  x2p2  xkpk  +  +  to a set of data  lowing squared error performance function.  { , }   pQ tQ , } p2 t2  { , } p1 t1  {         so as to minimize the fol-  First, we want to express the problem in matrix form. Define the following  vectors.  F x   =  – tq  gk pq  2    Q  q  1=  t  =  G  =  t1 t2  tQ     k 1 p1  p1 k 1 p2  p2   k 1 pQ  pQ     x  =  x0 x1  xk  We can then write the performance index as follows.  F x   =  – t Gx  T t Gx  –    =  tTt  2xTGTt  –  +  xTGTGx  13-42   Solved Problems  13  To locate the minimum, we take the gradient and set it equal to zero.    F x   =  2GTt  –  +  2GTGx  =  0  Solving for the weights, we obtain the least squares solution  maximum  likelihood for the Gaussian noise case .  GTG  xML  =  GTt    xML  =  GTG   1– GTt  To demonstrate the operation of the polynomial fitting, we will use the sim- ple linear function  . To create the data set, we will sample the func- tion at five different points and will add noise as follows  p=  t  ti  =  pi  i+  ,   p  =    1–     0.5–   0 0.5 1      ,  i   has a uniform density with range   where  shows how to generate the data and fit a 4th order polynomial. The results  of fitting 2nd and 4th order polynomials are shown in Figure P13.6. The 4th  order polynomial has five parameters, which allow it to exactly fit the five  noisy data points, but it doesn’t produce an accurate approximation of the  true function.  . The code below    0.25  0.25  –      » 2 + 2 ans =       4  p = -1:.5:1; t = p + 0.5* rand size p  -0.5 ; Q = length p ; ord = 4; G = ones Q,1 ; for i=1:ord,     G = [G  p' .^i]; end x =  G'*G \G'*t'; % Could also use x = G\t’;  Function Data 2nd Order 4th Order     1.5  0.5  1  0  −0.5  −1   −1  13-43  −0.8  −0.6  −0.4  −0.2  0  0.2  0.4  0.6  0.8  1  Figure P13.6  Polynomial Approximations to a Straight Line   13 Generalization  Epilogue  The focus of this chapter has been the development of algorithms for  training multilayer neural networks so that they generalize well. A  network that generalizes well will perform as well in new situations as it  performs on the data for which it was trained.   The basic approach to producing networks that generalize well is to find  the simplest network that can represent the data. A simple network is one  that has a small number of weights and biases.  The two methods that we presented in this chapter, early stopping and reg- ularization, produce simple networks by constraining the weights, rather  than by reducing the number of weights. We showed in this chapter that  constraining the weights is equivalent to reducing the number of weights.  Chapter 23 presents a case study that uses Bayesian regularization to pre- vent overfitting in a practical function approximation problem. Chapter 25  presents a case study that uses early stopping to prevent overfitting in a  practical pattern recognition problem.  13-44   Further Reading  Further Reading  [AmMu97]  S. Amari, N. Murata, K.-R. Muller, M. Finke, and H. H.  Yang, “Asymptotic Statistical Theory of Overtraining and  Cross-Validation,” IEEE Transactions on Neural Net- works, vol. 8, no. 5, 1997.  13  [FoHa97]  [GoLa98]  [Sarle95]   When using early stopping, it is important to decide on the  number of data points to place in the validation set. This  paper provides a theoretical basis for the choice of valida- tion set size.  D. Foresee and M. Hagan, “Gauss-Newton Approximation  to Bayesian Learning,” Proceedings of the 1997 Interna- tional Joint Conference on Neural Networks, vol. 3, pp.  1930 - 1935, 1997.  This paper describes a method for implementing Bayesian  regularization by using the Gauss-Newton approximation  to the Hessian matrix.  C. Goutte and J. Larsen, “Adaptive Regularization of Neu- ral Networks Using Conjugate Gradient,” Proceedings of  the IEEE International Conference on Acoustics, Speech  and Signal Processing, vol. 2, pp. 1201-1204, 1998.  When using regularization, the important step is setting  the regularization parameter. This paper describes a proce- dure for setting the regularization parameter to minimize  the validation set error.  Bayesian approaches have been used for many years in sta- tistics. This paper presents one of the first developments of  a Bayesian framework for training neural networks.  MacKay followed this paper with many others describing  refinements of the approach.  W. S. Sarle, “Stopped training and other remedies for over- fitting,” In Proceedings of the 27th Symposium on Interface,  1995.  This is one of the early papers on the use of early stopping  with a validation set to prevent overfitting. The paper de- scribes simulation results comparing early stopping with  other methods for improving generalization.  13-45  [MacK92]  D. J. C. MacKay, “Bayesian Interpolation,” Neural Compu- tation, vol. 4, pp. 415-447, 1992.   [SjLj94]  [Tikh63]  [WaVe94]  13 Generalization  J. Sjoberg and L. Ljung, “Overtraining, regularization and  searching for minimum with application to neural net- works,” Linkoping University, Sweden, Tech. Rep. LiTH- ISY-R-1567, 1994.  This report explains how early stopping and regularization  are approximately equivalent processes. It demonstrates  that the number of iterations of training is inversely pro- portional to the regularization parameter.  A. N. Tikhonov, “The solution of ill-posed problems and the  regularization method,” Dokl. Acad. Nauk USSR, vol. 151,  no. 3, pp. 501-504, 1963.  Regularization is a method by which a squared error per- formance index is augmented by a penalty on the complex- ity of the approximating function. This is the original paper  that introduced the concept of regularization. The penalty  involved the derivatives of the approximating function.  C. Wang, S. S. Venkatesh, and J. S. Judd, “Optimal Stop- ping and Effective Machine Complexity in Learning,” Ad- vances in Neural Information Processing Systems, J. D.  Cowan, G. Tesauro, and J. Alspector, Eds., vol. 6, pp. 303- 310, 1994.  This paper describes how the effective number of network  parameters changes during the training process and how  the generalization capability of the network can be im- proved by stopping the training early.  13-46   Exercises  Exercises  E13.1 Consider fitting a polynomial  kth order   gk p   =  x0  +  x1p  +  x2 p2  xk pk  +  +  13  { , } p1 t1  to a set of data  . It has been proposed that min- imizing a performance index that penalizes the derivatives of the polyno- mial will provide improved generalization. Investigate the relationship  between this technique and regularization using squared weights.  { , }   pQ tQ , } p2 t2  {        i. Derive the least squares solution for the weights   , which minimiz- es the following squared error performance index.  See Solved Prob- lem P13.6.   xi  ii. Derive the regularized least squares solution, with a squared   weight penalty.  F x   =  – tq  gk pq  2    Q  q  1=  F x   =  – tq  gk pq  2    +   2 xi  Q    q  1=  k  i  0=  iii. Derive a solution for the weights that minimizes a sum of the   squared error plus a sum of squared derivatives.  F x   =  – tq  gk pq  2    Q  +   i  1=  2  d pd  gk pq    iv. Derive a solution for the weights that minimizes a sum of the   squared error plus a sum of squared second derivatives.  F x   =  – tq  gk pq  2    Q  +   i  1=  2  d p2  d  2  gk pq    Q    q  1=  Q    q  1=  13-47  » 2 + 2 ans =       4  E13.2 Write a MATLAB program to implement the solutions you found in E13.1   values to obtain   for all cases. Plot the data points, the noise-free    and the polynomial approximation in each case. Compare   i. through iv. Using the following data points, adjust the  the best results. Use  function   the four approximations. Which do you think produces the best results?  Which cases produce similar results?  8=  p=    k  t   13 Generalization  ti  =  pi  i+  ,   p  =    1–     0.5–   0 0.5 1      ,  where  mand in MATLAB .  i   has a uniform density with range      0.1–  0.1      use the rand com-  E13.3 Consider fitting a polynomial  1st order ,   g1 p   =  x0  +  x1p  , to the following   data set:      p1  1=    t1  4=  ,   p2  2=    t2  6=          .      i. Find the least squares solutions for the weights   x1 imize the following sum squared error performance index:   and   x0   that min-  ii. Find the regularized least squares solution for the weights    when the following squared weight penalty is used:  x0   and   x1  F x   =  – tq  g1 pq  2    .  2  q  1=  F x   =  – tq  g1 pq  2    +  2 xi  .  2    q  1=  1  i  0=  » 2 + 2 ans =       4  E13.4 Investigate the extrapolation characteristics of neural networks and poly- nomials. Consider the problem described in E11.25, where a sine wave is  fit over the range  . Select 11 training points evenly spaced over  this interval.     2–  2  p  i. After fitting the 1-2-1 neural network over this range, plot the actu-  al sine function and the neural network approximation over the  range      4–  .   4  p  ii. Fit a fifth-order polynomial  which has the same number of free pa-  rameters as the 1-2-1 network  to the sine wave over the range  2– and the polynomial approximation over the range     using your results from E13.1 i. . Plot the actual function         4–  2  4  p  p  .  iii. Discuss the extrapolation characteristics of the neural network and   the polynomial.  E13.5 Suppose that we have a random variable    that is distributed according to  the following density function. We take a series of Q independent samples  of the random variable. Find the maximum likelihood estimate of   .  xML      x  t  13-48   Exercises  f t x    =  exp  t x2----  t   --–  x  t  0  E13.6 For the random variable   given in E13.5, suppose that    is a random vari- able with the following prior density function. Find the most probable esti- mate of   xMP   .       x  x  t  f x   =  exp  x–    x  0  E13.7 Repeat E13.6 for the following prior density function. Under what condi-  tions will   xMP  =  xML  ?  13  f x   =  1 ---------------- 2x  exp  –      2  x x–  --------------------- 2  2x  E13.8 In the signal plus noise example given in Solved Problem P13.2, find   xMP     for the following prior density functions.  i.  ii.  f x   f x   =  =  exp 1 --- 2  x–    x  0  exp  x ––    E13.9 Suppose that we have a random variable   t  the following density function. We take a series of  ples of the random variable.   that is distributed according to   independent sam-  2=  Q  f t x    =   exp  –  x– t    0  t t  x x  i. Find the likelihood function   f t1 t2   x    , and sketch versus   x  .  ii. Suppose that the two measurements are   the maximum likelihood estimate of   x      t1 xML  1=  .   and   t2  1=  . Find   For the random variable   above, suppose that  the following prior density function.  t  x   is a random variable with   f x   =  exp  –  2x2    1 --------------  2  iii. Sketch the posterior density   f x t1 t2     .  You do not need to compute   the denominator, just find the general shape. Assume the same  measurements from part ii.   13-49   13 Generalization  iv. Find the most probable estimate of   x      xMP   .  E13.10 We have a coin that is not fair.  The probability of heads is not the same as   .  the probability of tails.  We want to estimate the probability of heads    x  i. If we flip the coin 10 times, the probability of getting exactly   t     heads, given that the probability of heads is  xML the maximum likelihood estimate of  ral log of  plain.  , is given below. Find  x  .  Hint: Take the natu-  before finding the maximum.  Is it reasonable? Ex-  p t x      x    p t x    =  10   xt 1  t  x–  t–      10  , where   10    t  =  10!  ----------------------  t! 10 t–    ii. Assume that the probability of heads,   x  , is a random variable with   the following prior density function. Find the most probable esti-  before  mate of  xMP finding the maximum.  Explain why    .  Hint: Take the natural log of    is different than   p x   p t x  xMP  xML      x  .  p x   =  12x2 1  x–    ,   0     x  1  .  E13.11 Suppose that the prior density in the level I Bayesian analysis  see page 13-  12  has nonzero mean,   . Find the new performance index.  x  E13.12 Suppose that we have the following inputs and targets:      2 1  p1  =  t1  1=  p2  =  t2  3=           2– 1–      We want to train a single-layer linear network without a bias on this train- ing set. Assume that each input vector occurs with equal probability. We  will train the network using the regularized performance index of Eq.   13.34 .  ii. Find   i. Find    and the effective number of parameters   xMP xMP  and  .  Your answer should be specific to this problem—not a general   . Explain the difference between   . 1= xMP   and      as   , if           xML discussion of the difference between   xMP   and   xML  .   E13.13 Suppose we have the following input pattern and target:      p1  =    t1  1=  2 1  .      13-50   Exercises  This pattern is used to train a single layer, linear network with no bias.  i. The network is to be trained with a regularized performance index,  . Find an   with the regularization parameter set to    expression for the regularized performance index.  1 2    =  =  ii. Find   xMP   and the effective number of parameters     .  13  iii. Sketch the contour plot of the regularized performance index.  iv. Find the maximum stable learning rate if steepest descent is used   to train the network.  v. Find the initial direction for the steepest descent algorithm trajec-  tory, if both initial weights are set to zero.  vi. Sketch an approximate complete trajectory  on your contour plot  from part iii.  for the steepest algorithm, with very small learning  rate, from the initial conditions where both weights are set to zero.  Explain your procedure for sketching the trajectory.  E13.14 Suppose that we have a single layer, linear network with no bias. The in-  put target pairs of the training set are given by      p1  =  1– 1  t1  2–=  ,   p2  =  t2  2=  ,   p3  =  t3  4=          2 1  ,              1 2  where each pair occurs with equal probability. We want to minimize the  regularized performance index of Eq.  13.34 .  i. Find the effective number of parameters     , for     1=  .  ii. Starting with zero initial weights, approximately how many itera- tions of the steepest descent algorithm would need to be made on the  mean square performance index   to produce results that would  be equivalent to minimizing the regularized performance index  with   ? Assume a learning rate of   ED  0.01  1=      =  .  » 2 + 2 ans =       4  E13.15 Repeat E11.25, but modify your program to use early stopping and to use  30 neurons. Select 10 training points and 5 validation points. Add noise to  the validation and testing points that is uniformly distributed between    using the MATLAB function rand . Measure the mean square  0.1– error of the trained network on a testing set consisting of 20 equally-spaced  points of the noise-free function. Try 10 different random sets of training  and validation data. Compare the results with early-stopping with the re- sults without early stopping.  and   0.1  13-51   13 Generalization  » 2 + 2 ans =       4  E13.16 Repeat E13.15, but use regularization instead of early stopping. This will  require modifying your program to compute the gradient of the regularized  performance index. Add the standard gradient of the squared error, which  is computed by the standard backpropagation algorithm, to the gradient of   these results with the early stopping results.   times the squared weights. Try three different values of   . Compare     E13.17 Consider again the problem described in E10.4  i. Find the regularized performance index for     =  0 1   . Sketch the   contour plot in each case. Indicate the location of the optimal  weights in each case.  ii. Find the effective number of parameters for     =  0 1     .  iii. Starting with zero initial weights, approximately how many itera- tions of the steepest descent algorithm would need to be made on the  mean square performance index to produce results that would be  equivalent to minimizing the regularized performance index with    ? Assume a learning rate of   0.01  1=    =  .  iv. Write a MATLAB M-file to implement the steepest descent algo-  =  0.01  rithm to minimize the mean square error performance index that  you found in part i.  This is a quadratic function.  Start the algo- rithm with zero initial conditions, and use a learning rate of  . Sketch the trajectory on a contour plot of the mean square   error  the contour plot was found in E10.4 . Verify that at the itera- tion you computed in part iii., the weights are close to the same val- ues you found to minimize the regularized performance index with     in part i.  1=  » 2 + 2 ans =       4  13-52   Objectives  14 Dynamic Networks  Objectives  Layered Digital Dynamic Networks   Objectives Theory and Examples  Example Dynamic Networks Principles of Dynamic Learning  Dynamic Backpropagation  Preliminary Definitions  Real Time Recurrent Learning  Backpropagation-Through-Time   14-1 14-2 14-3 14-5 14-8 14-12 14-12 14-12 14-22 Summary and Comments on Dynamic Training  14-30 14-34 14-36 14-45 14-46 14-47  Summary of Results Solved Problems Epilogue Further Reading Exercises  14  Neural networks can be classified into static and dynamic categories. The  multilayer network that we have discussed in the last three chapters is a  static network. This means that the output can be calculated directly from  the input through feedforward connections. In dynamic networks, the out- put depends not only on the current input to the network, but also on the  current or previous inputs, outputs or states of the network. For example,  the adaptive filter networks we discussed in Chapter 10 are dynamic net- works, since the output is computed from a tapped delay line of previous  inputs. The Hopfield network we discussed in Chapter 3 is also a dynamic  network. It has recurrent  feedback  connections, which means that the  current output is a function of outputs at previous times.  We will begin this chapter with a brief introduction to the operation of dy- namic networks, and then we will describe how these types of networks can  be trained. The training will be based on optimization algorithms that use  gradients  as in steepest descent and conjugate gradient algorithms  or Ja- cobians  as in Gauss-Newton and Levenberg-Marquardt algorithms  These  algorithms were described in Chapters 10, 11 and 12 for static networks.  The difference between the training of static and dynamic networks is in  the manner in which the gradient or Jacobian is computed. In this chapter,  we will present methods for computing gradients for dynamic networks.  14-1   14 Dynamic Networks  Theory and Examples  Dynamic Networks  D  Recurrent  Dynamic networks are networks that contain delays  or integrators, for  continuous-time networks  and that operate on a sequence of inputs.  In  other words, the ordering of the inputs is important to the operation of the  network.  These dynamic networks can have purely feedforward connec- tions, like the adaptive filters of Chapter 10, or they can also have some  feedback  recurrent  connections, like the Hopfield network of Chapter 3.  Dynamic networks have memory. Their response at any given time will de- pend not only on the current input, but on the history of the input sequence.  Because dynamic networks have memory, they can be trained to learn se- quential or time-varying patterns. Instead of approximating functions, like  the static multilayer perceptron network of Chapter 11, a dynmic network  can approximate a dynamic system. This has applications in such diverse  areas as control of dynamic systems, prediction in financial markets, chan- nel equalization in communication systems, phase detection in power sys- tems, sorting, fault detection, speech recognition, learning of grammars in  natural languages, and even the prediction of protein structure in genetics.  Dynamic networks can be trained using the standard optimization meth- ods that we have discussed in Chapters 9 through 12. However, the gradi- ents and Jacobians that are required for these methods cannot be  computed using the standard backpropagation algorithm. In this chapter  we will present the dynamic backpropagation algorithms that are required  for computing the gradients for dynamic networks.  There are two general approaches  with many variations  to gradient and  Jacobian calculations in dynamic networks: backpropagation-through- time  BPTT  [Werb90] and real-time recurrent learning  RTRL  [WiZi89].  In the BPTT algorithm, the network response is computed for all time  points, and then the gradient is computed by starting at the last time point  and working backward in time. This algorithm is efficient for the gradient  calculation, but it is difficult to implement on-line, because the algorithm  works backward in time from the last time step.   In the RTRL algorithm, the gradient can be computed at the same time as  the network response, since it is computed by starting at the first time  point, and then working forward through time. RTRL requires more calcu- lations than BPTT for calculating the gradient, but RTRL allows a conve- nient framework for on-line implementation. For Jacobian calculations,  the RTRL algorithm is generally more efficient than the BPTT algorithm.   In order to more easily present general BPTT and RTRL algorithms, it will  be helpful to introduce modified notation for networks that can have recur- rent connections. In the next section we will introduce this notation, and  then the remainder of the chapter will present general BPTT and RTRL al- gorithms for dynamic networks.  14-2   Layered Digital Dynamic Networks  Layered Digital Dynamic Networks  LDDN  In this section we want to introduce the neural network framework that we  will use to represent general dynamic networks. We call this framework  Layered Digital Dynamic Networks  LDDN . It is an extension of the nota- tion that we have used to represent static multilayer networks. With this  new notation, we can conveniently represent networks with multiple recur- rent  feedback  connections and tapped delay lines.  To help us introduce the LDDN notation, consider the example dynamic  network given in Figure 14.1.   14  Inputs  Layer 1  Layer 2  Layer 3  p1   t  R x 11  1  R 1  IW1,1  S 1 x R  b1  S 1 x 1  LW1,1  LW1,3  T D L  T D L  a1   t  T D L  1  S 1 x 1  n1   t  S 1 x 1  f 1  S 1  LW2,1  S 2 x S 1  b2  S 2 x 1  T D L  LW2,3  n2   t  S 2 x 1  a2   t  S 2 x 1  1  f 2  S 2  LW3,2  S 3 x S 2  b3  S 3 x 1  a3   t  S 3 x 1  n3   t  S 3 x 1  f 3  S 3  Figure 14.1  Example Dynamic Network  The general equations for the computation of the net input  m of an LDDN are  nm t    for layer   nm t   LWm l d al t  d–    l  = f Lm  Im  +  l      d DLm l     d DIm l  IWm l d pl t  d–    +  bm   14.1   Input Weight Layer Weight  pl t    is the l th input vector to the network at time t,   where  input weight between input l and layer m,  tween layer l and layer m,   is the set  of all delays in the tapped delay line between Layer l and Layer m,   is  the set of all delays in the tapped delay line between Input l and Layer m,    is the   is the layer weight be-   is the bias vector for layer m,   LWm l  DLm l  DIm l  IWm l  bm  14-3   D  Simulation Order  Backpropagation Order  14 Dynamic Networks   is the set of indices of input vectors that connect to layer m, and    is  Im the set of indices of layers that directly connect forward to layer m. The out- put of layer m is then computed as  f Lm  am t   =  fm nm t       .   14.2   Compare this with the static multilayer network of Eq.  11.6 . LDDN net- works can have several layers connecting to layer m. Some of the connec- tions can be recurrent through tapped delay lines. An LDDN can also have  multiple input vectors, and the input vectors can be connected to any layer  in the network; for static multilayer networks, we assumed that the single  input vector connected only to Layer 1.  With static multilayer networks, the layers were connected to each other in  numerical order. In other words, Layer 1 was connected to Layer 2, which  was connected to Layer 3, etc. Within the LDDN framework, any layer can  connect to any other layer, even to itself. However, in order to use Eq.   14.1 , we need to compute the layer outputs in a specific order. The order  in which the layer outputs must be computed to obtain the correct network  output is called the simulation order.  This order need not be unique; there  may be several valid simulation orders.  In order to backpropagate the de- rivatives for the gradient calculations, we must proceed in the opposite or- der, which is called the backpropagation order. In Figure 14.1, the  standard numerical order, 1-2-3, is the simulation order, and the backprop- agation order is 3-2-1.  As with the multilayer network, the fundamental unit of the LDDN is the  layer. Each layer in the LDDN is made up of five components:  1. a set of weight matrices that come into that layer  which may connect   from other layers or from external inputs ,  2. any tapped delay lines  represented by     that appear at  the input of a set of weight matrices  Any set of weight matrices can be  preceded by a TDL. For example, Layer 1 of Figure 14.1 contains the  weights    and the corresponding TDL. ,  DLm l  LW1 3 d   DIm l   or   3. a bias vector,  4. a summing junction, and  5. a transfer function.  The output of the LDDN is a function not only of the weights, biases, and  current network inputs, but also of some layer outputs at previous points  in time. For this reason, it is not a simple matter to calculate the gradient  of the network output with respect to the weights and biases. The weights  and biases have two different effects on the network output. The first is the  direct effect, which can be calculated using the standard backpropagation  algorithm from Chapter 11. The second is an indirect effect, since some of   14-4   Layered Digital Dynamic Networks  the inputs to the network are previous outputs, which are also functions of  the weights and biases. The main development of the next two sections is  a general gradient calculation for arbitrary LDDNs.  2 2+  Example Dynamic Networks Before we introduce dynamic training, let’s get a feeling for the types of re- sponses we can expect to see from dynamic networks. Consider first the  feedforward dynamic network shown in Figure 14.2.  14  Inputs  Linear Neuron  p t     iw1,1 0   D  D  iw1,1 2   iw1,1 1    cid:2   n t     a t     a t    =  iw   0   t +  p  iw   1   t-1  p   +  iw   2   t-2   p  1,1  1,1  1,1  Figure 14.2  Example Feedforward Dynamic Network  This is an ADALINE filter, which we discussed in Chapter 10  see Figure  10.5 . Here we are representing it in the LDDN framework. The network  has a TDL on the input, with  . To demonstrate the opera- tion of this network, we will apply a square wave as input, and we will set  all of the weight values equal to 1 3:   0 1 2  DI1 1  =        iw1 1 0   ,   iw1 1 1   ,   iw1 1 2   1 ---= 3  1 ---= 3  1 ---= 3  .  The network response is calculated from:  a t   =  n t   =  IW d p t  d–    2    =  n1 t   =  0= d iw1 1 0 p t   +  iw1 1 1 p t  1–    +  iw1 1 2 p t  2–    where we have left off the superscripts on the weight and the input, since  there is only one input and only one layer.   14.3    14.4   14-5   14 Dynamic Networks  The response of the network is shown in Figure 14.3. The open circles rep- resent the square-wave input signal  . The dots represent the network  response  . For this dynamic network, the response at any time point  depends on the previous three input values. If the input is constant, the  output will become constant after three time steps. This type of linear net- work is called a Finite Impulse Response  FIR  filter.  a t   p t   FIR  D  1.5  0.5  1  0  −0.5  −1  −1.5  0  5  10  15  20  25  Figure 14.3  Response of ADALINE Filter Network  This dynamic network has memory. Its response at any given time will de- pend not only on the current input, but on the history of the input sequence.  If the network does not have any feedback connections, then only a finite  amount of history will affect the response. In the next example, we will con- sider a network that has an infinite memory.  To experiment with the finite impulse response example, use the Neural Net- work Design Demonstration Finite Impulse Response Network  nnd14fir .  2 2+  Now consider another simple linear dynamic network, but one that has a  recurrent connection. The network in Figure 14.4 is a recurrent dynamic  network. The equation of operation of the network is  a1 t   = =  LW1 1 1 a1 t  =  n1 t  lw1 1 1 a t 1–  1–  iw1 1 p t     +  +  IW1 1 0 p1 t    14.5   where, in the last line, we have left off the superscripts, since there is only  one neuron and one layer in the network. To demonstrate the operation of  this network, we will set the weight values to  lw1 1 1    and   iw1 1  1 ---= 2  1 ---= 2  .   14.6   14-6   Layered Digital Dynamic Networks  Inputs  Linear Neuron  n t     a t     p t     iw1,1  cid:2  lw1,1 1   D  14      a t  =      iw p t  1,1  +  lw  1,1 1   a t    -1   Figure 14.4  Recurrent Linear Neuron  The response of this network to the square wave input is shown in Figure  14.5. The network responds exponentially to a change in the input se- quence. Unlike the FIR filter network of Figure 14.2, the exact response of  the network at any given time is a function of the infinite history of inputs  to the network.  1.5  0.5  1  0  −0.5  −1  −1.5  0  14-7  5  10  15  20  25  Figure 14.5  Recurrent Neuron Response  To experiment with this infinite impulse response example, use the Neural  Network Design Demonstration Infinite Impulse Response Network   nnd14iir .  Compare the dynamic networks of the previous two examples with the stat- ic, two-layer perceptron of Figure 11.4. Static networks can be trained to  approximate static functions, like  , where the output can be comput- ed directly from the current input. Dynamic networks, on the other hand,   p   sin   D  14 Dynamic Networks  can be trained to approximate dynamic systems, such as robot arms, air- craft, biological processes and economic systems, where the current system  output depends on a history of previous inputs and outputs. Because dy- namic systems are more complex than static functions, we expect that the  training process for dynamic networks will be more challenging than static  network training.  In the following section, we will discuss the computation of gradients for  the training of dynamic networks. For static networks, these gradients  were computed using the standard backpropagation algorithm. For dy- namic networks, the backpropagation algorithm must be modified.  Principles of Dynamic Learning  2 2+  Before we get into the details of training dynamic networks, let’s first in- vestigate a simple example. Consider again the recurrent network of Fig- ure 14.4. Suppose that we want to train the network using steepest  descent. The first step is to compute the gradient of the performance func- tion. For this example we will use sum squared error:  F x   =  e2 t   =    t t   –  a t   2  .   14.7   Q    t  1=  Q    t  1=  The two elements of the gradient will be   14.8    14.9    14.10   F x   ------------------------ lw1 1 1   =  Q    t  1=  e2 t   ------------------------ lw1 1 1   =  – 2  e t   a t   ------------------------ lw1 1 1   ,  Q  t  1=  Q  F x  --------------- iw1 1  =  Q    t  1=  e2 t  --------------- iw1 1  =  – 2  t  1=  e t  a t  --------------- iw1 1  The key terms in these equations are the derivatives of the network output  with respect to the weights:  a t   ------------------------ lw1 1 1    and   a t  --------------- iw1 1  .  1– a t  If we had a static network, then these terms would be very easy to compute.  They would correspond to  , respectively. However, for re- current networks, the weights have two effects on the network output. The  first is the direct effect, which is also seen in the corresponding static net- work. The second is an indirect effect, caused by the fact that one of the net- work inputs is a previous network output. Let’s compute the derivatives of  the network output, in order to demonstrate these two effects.   and   p t     14-8   Principles of Dynamic Learning  The equation of operation of the network is  a t   =  lw1 1 1 a t  1–    +  iw1 1 p t   .   14.11   We can compute the terms in Eq.  14.10  by taking the derivatives of Eq.   14.11 :  a t   ------------------------ lw1 1 1   =  1– a t    +  lw1 1 1  a t 1–  ------------------------ lw1 1 1   ,  a t  --------------- iw1 1  =  p t   +  lw1 1 1 a t   1– ---------------------- iw1 1  .   14.12    14.13   14  The first term in each of these equations represents the direct effect that  each weight has on the network output. The second term represents the in- direct effect. Note that unlike the gradient computation for static networks,  the derivative at each time point depends on the derivative at previous  time points  or at future time points, as we will see later .    a t  iw1 1  The following figures illustrate the dynamic derivatives. In Figure 14.6 a   we see the total derivatives   and also the static portions of the  derivatives. Note that if we consider only the static portion, we will under- estimate the effect of a change in the weight. In Figure 14.6 b  we see the  original response of the network  which was also shown in Figure 14.5  and  a new response, in which   is increased from 0.5 to 0.6. By comparing  the two parts of Figure 14.6, we can see how the derivative indicates the  effect on the network response of a change in the weight   iw1 1  .  iw1 1     b      Total Derivative Static Derivative  a   1.5  0.5  2  1  0  −0.5  −1  −1.5  −2    0  Original Response Incremental Response  1.5  0.5  1  0  −0.5  −1  −1.5    0  5  10  15  20  25  5  10  15  20  25  Figure 14.6  Derivatives for    and Response of Network in Figure 14.4  iw1 1  In Figure 14.7 we see similar results for the weight  to get from this example are: 1  the derivatives have static and dynamic  components, and 2  the dynamic component depends on other time points.  . The key ideas   lw1 1 1   14-9   D  14 Dynamic Networks  To experiment with dynamic derivatives, use the Neural Network Design  Demonstration Dynamic Derivatives  nnd14dynd .  a   1.5  0.5  2  1  0  −0.5  −1  −1.5  −2    0     b      Original Response Incremental Response  1.5  0.5  1  0  −0.5  −1  −1.5    0  Total Derivative Static Derivative  5  10  15  20  25  5  10  15  20  25  Figure 14.7  Derivatives for   lw1 1 1    and Response of Network in Figure 14.4  Having made this initial investigation of a single-neuron network, let’s con- sider the slightly more complex dynamic network that is shown in Figure  14.8. It consists of a static multilayer network with a single feedback loop  added from the output of the network to the input of the network through   represents all of the network pa- a single delay. In this figure, the vector  rameters  weights and biases , and the vector   represents the output of  the multilayer network at time step t. This network will help us demon- strate the key steps of dynamic training.  a t   x  p   t  D  a   t  Multilayer Network  a    = NN     ,   -1 ,   x  p  a  t  t  t  Figure 14.8  Simple Dynamic Network  As with a standard multilayer network, we want to adjust the weights and  biases of the network to minimize the performance index,  , which is  normally chosen to be the mean squared error. In Chapter 11, we derived  the backpropagation algorithm for computing the gradient of  , which  we could then use with any of the optimization methods from Chapter 12  to minimize  . With dynamic networks, we need to modify the standard  backpropagation algorithm. There are two different approaches to this  problem. They both use the chain rule, but are implemented in different  ways:  F x   F x   F x   14-10   Principles of Dynamic Learning  F x------  Q  =  t  1=  a t   ------------ xT  T    eF ------------ a t   ,  F x------  Q  =  t  1=  ea t  --------------- xT  T    F ------------ a t   .  where the superscript e indicates an explicit derivative, not accounting for  indirect effects through time. The explicit derivatives can be obtained with  the standard backpropagation algorithm of Chapter 11. To find the com- plete derivatives that are required in Eq.  14.14  and Eq.  14.15 , we need  the additional equations:  14  or  and  a t   ------------ xT  =  ea t  --------------- xT  +  ea t  ------------------------ aT t   1–    a t 1–   --------------------- xT  F ------------ a t   =  eF ------------ a t   +  ea t 1+  ------------------------ aT t      F ---------------------- a t  1+  .  RTRL  Eq.  14.14  and Eq.  14.16  make up the real-time recurrent learning   RTRL  algorithm. Note that the key term is  BPTT  which must be propagated forward through time. Eq.  14.15  and Eq.   14.17  make up the backpropagation-through-time  BPTT  algorithm.  Here the key term is  a t   ------------ xT  ,  F ------------ a t   which must be propagated backward through time.  In general, the RTRL algorithm requires somewhat more computation  than the BPTT algorithm to compute the gradient. However, the BPTT al- gorithm cannot be conveniently implemented in real time, since the out- puts must be computed for all time steps, and then the derivatives must be  backpropagated back to the initial time point. The RTRL algorithm is well  suited for real time implementation, since the derivatives can be calculated  at each time step.  For Jacobian calculations, which are needed for Leven- berg-Marquardt algorithms, the RTRL algorithm is often more efficient  than the BPTT algorithm. See [DeHa07].   14-11   14.14    14.15    14.16    14.17    14.18    14.19    D  Input Layer  Output Layer  14 Dynamic Networks  Dynamic Backpropagation  In this section, we will develop general RTRL and BPTT algorithms for dy- namic networks represented in the LDDN framework. This development  will involve generalizing Eq.  14.14  through Eq.  14.17 .  Preliminary Definitions In order to simplify the description of the training algorithm, some layers  of the LDDN will be assigned as network outputs, and some will be as- signed as network inputs. A layer is an input layer if it has an input weight,  or if it contains any delays with any of its weight matrices. A layer is an  output layer if its output will be compared to a target during training, or if  it is connected to an input layer through a matrix that has any delays as- sociated with it.  For example, the LDDN shown in Figure 14.1 has two output layers  1 and  3  and two input layers  1 and 2 . For this network the simulation order is  1-2-3, and the backpropagation order is 3-2-1. As an aid in later deriva- tions, we will define U as the set of all output layer numbers and X as the  set of all input layer numbers. For the LDDN in Figure 14.1, U={1,3} and  X={1,2}.  The general equations for simulating an arbitrary LDDN network are giv- en in Eq.  14.1  and Eq.  14.2 . At each time point, these equations are it- erated forward through the layers, as  simulation order. Time is then incremented from    is incremented through the    to   Q=  1=  m  .  t  t  Real Time Recurrent Learning In this subsection we will generalize the RTRL algorithm, given in Eq.   14.14  and Eq.  14.16 , for LDDN networks. This development will follow  in many respects the development of the backpropagation algorithm for  static multilayer networks in Chapter 11. You may want to quickly review  that material before proceeding.  Eq.  14.14   The first step in developing the RTRL algorithm is to generalize Eq.   14.14 . For the general LDDN network, we can calculate the terms of the  gradient by using the chain rule, as in  F x------  Q  =    t  1=  u U  au t  --------------- xT  T    eF --------------- au t    .   14.20   If we compare this equation with Eq.  14.14 , we notice that in addition to  each time step, we also have a term in the sum for each output layer. How- ever, if the performance index   is not explicitly a function of a specific  output   , then that explicit derivative will be zero.  F x   au t   14-12   Dynamic Backpropagation  Eq.  14.16   The next step of the development of the RTRL algorithm is the generaliza- tion of Eq.  14.16 . Again, we use the chain rule:  au t  --------------- xT  =  eau t  -----------------  xT  +      u' U  x X    d DLx u'    eau t  ----------------- nx t T     enx t  --------------------------- T au' t d–     au' t  d– ------------------------- xT  .  14.21   In Eq.  14.16  we only had one delay in the system. Now we need to account  for each output and also for the number of times each output is delayed be- fore it is input to another layer. That is the reason for the first two summa- tions in Eq.  14.21 . These equations must be updated forward in time, as  t is varied from 1 to Q. The terms   14  are generally set to zero for   t  0  .   To implement Eq.  14.21 , we need to compute the terms  eau t  ----------------- nx t T     enx t  --------------------------- T au' t  d–  .  To find the second term on the right, we can use  x t  nk  = f Lx  l      d' DLx l  i  1=  x l d'  lwk i  ai  l t d'–    +   Ix  l      d' DIx l  i  1=  x l d'  iwk i  pi  l t d'–    +  x bk  au t  --------------- xT  Sl   Rl    Compare with Eq.  11.20 .  We can now write  x t  enk ------------------------ u' t   d– aj  =   x u' lwk j  d   .  If we define the following sensitivity term u t  eak ----------------- m t   ni  u m sk i  t     ,  which can be used to make up the following matrix  14-13   14.22    14.23    14.24    14.25    14.26    D  14 Dynamic Networks  Su m  t   =  eau t  ------------------- nm t T   =  u m s1 1 u m s2 1  t   u m t  s1 2 u m t  s2 2   u m t  sSu 2  u m t   s1 Sm u m t  t   s1 Sm  u m t   sSu Sm  t   u m sSu 1  ,   14.27   then we can write Eq.  14.23  as  eau t  ----------------- nx t T  or in matrix form    enx t  --------------------------- T au' t d–   Sx =  i j  k  1=  u x si k  d+ t       x u' lwk j  d   ,   14.28   eau t  ----------------- nx t T     enx t  --------------------------- T au' t  d–  =  Su x   t  LWx u'    d   .   14.29   Therefore Eq.  14.21  can be written   au t  --------------- xT  =  eau t  -----------------  xT  +      u' U  x X    d DLx u'    Su x   t  LWx u'    d     au' t  d– ------------------------- xT   14.30   Many of the terms in the summation on the right hand side of Eq.  14.30   will be zero and will not have to be computed. To take advantage of these  efficiencies, we introduce some indicator sets. They are sets that tell us for  which layers the weights and the sensitivities are nonzero.  The first type of indicator set contains all of the output layers that connect  to a specified layer  x some nonzero delay:    which will always be an input layer  with at least   U x  ELW  =    u U      LWx u d   0  0 d      ,   14.31   where    means “such that,” and  means “there exists.”    The second type of indicator set contains the input layers that have a non- zero sensitivity with a specified layer   u  :  X u  ES  =    x X     Su x  0      .   14.32    is nonzero, there is a static connection from layer   Su x  to ouput  .  The third type of indicator set contains the layers that have a non-   When  layer  u zero sensitivity with a specified layer   x  u  :  ES u   =    x  Su x  0      .   14.33   14-14   .   14.34   14  Dynamic Backpropagation  X u  ES   and   ES u    contains only input   will not be needed in the simplification of Eq.  14.30 , but it   The difference between  layers.  will be used for the calculation of sensitivities in Eq.  14.38 . Using Eq.  14.31  and Eq.  14.32 , we can rearrange the order of the sum- mations in Eq.  14.30  and sum only over nonzero terms:   is that   ES u   X u  ES  au t  --------------- xT  =  eau t  -----------------  xT  Su x  t   + x ES    X u    U x  u' ELW        d DLx u'     LWx u'  d     au' t  d– ------------------------- xT  Eq.  14.34  makes up the generalization of Eq.  14.16  for the LDDN net- work. It remains to compute the sensitivity matrices  it derivatives    and the explic- , which are described in the next two subsections.  eau t  w  Su m  t   Sensitivities  In order to compute the elements of the sensitivity matrix, we use a form  of standard static backpropagation. The sensitivities at the outputs of the  network can be computed as  u u sk i  t   =  eak u t  ----------------- u t   ni  =  f· u   u t  ni   for i  k=  0  for i  k  ,   u U  ,   14.35   or, in matrix form,  where   F· u nu t       Su u  is defined as   t   =  F· u nu t       ,  F· u nu t       =  u t     f· u n1  0   f· u   0     0  u t  n2  0  f· u n   0  0   t     u Su   14.36    14.37    see also Eq.  11.34  . The matrices  gating through the network, from each network output, using   can be computed by backpropa-  t   Su m  Su m  t   =  Su l  t LWl m  0   F· m nm t       ,   u U  ,   14.38    l ES u    b Lm  14-15   D  We also need to compute the explicit derivatives  Explicit Derivatives  14 Dynamic Networks  where m is decremented from u through the backpropagation order, and  b  is the set of indices of layers that are directly connected backwards to  Lm layer m  or to which layer m connects forward  and that contain no delays  in the connection. The backpropagation step given in Eq.  14.38  is essen- tially the same as that given in Eq.  11.45 , but it is generalized to allow for  arbitrary connections between layers.  eau t  -----------------  .  xT   14.39   Using the chain rule of calculus, we can derive the following expansion of  Eq.  14.39  for input weights:  u t  eak ----------------------- m l d   iwi j  =  eak u t  ----------------- m t   ni    eni m t  ----------------------- m l d   iwi j  =  u m sk i  t     l t d– pj    .   14.40   In vector form we can write  eau t  ----------------------- m l d   iwi j  =  u m si  t     l t d– pj    .   14.41   In matrix form we have  eau t    ------------------------------------------ T vec IWm l d    =    pl t  d–    T    Su m  t   ,   14.42   and in a similar way we can derive the derivatives for layer weights and  biases:  eau t    -------------------------------------------- T vec LWm l d    =    al t  d–    T    Su m  t   ,  eau t  ----------------- T bm   =  Su m  t   ,   14.43    14.44   where the vec operator transforms a matrix into a vector by stacking the  columns of the matrix one underneath the other, and   is the Kroneck- er product of    [MaNe99].  A B   and   A  B  The total RTRL algorithm for the LDDN network is summarized in the fol- lowing pseudo code.  14-16   Dynamic Backpropagation  Real-Time Recurrent Learning Gradient  Initialize:  au t  --------------- xT  For t = 1 to Q,  0=  0 t  , for all   u U  ,  ,   U =  ES u  =  X u  = ES For m decremented through the BP order   and    for all   u U  .  t LWl m  0   F· m nm t       14    X u  ES  For all   b Lm Su l  =  t   , if   Su m  u U  ES u   l ES u   add m to the set  if   m X  b Lm ES u   , add m to the set   EndFor u m U If  Sm m t   add m to the sets  if   F· m nm t  U    and  , add m to the set   m X  =  ES m  X m ES    EndIf m EndFor m u U For   =    pl t  d–    T    Su m  t   =    al t  d–    T    Su m  t   ------------------------------------------ T vec IWm l d    eau t   eau t  -------------------------------------------- T vec LWm l d    eau t  ----------------- T bm   Su m  t   =  EndFor weights and biases au t  --------------- xT  eau t  -----------------  xT  + x ES  =    X u   EndFor u  EndFor t Compute Gradients  F x------  Q  =    t  1=  u U  au t  --------------- xT  T    eF --------------- au t    14-17   incremented through the simulation order  For all weights and biases  x is a vector containing all weights and biases   Su x  t    U x  ELW  u'        d DLx u'     LWx u'  d     au' t  d– ------------------------- xT   D  14 Dynamic Networks  Example RTRL Implementations  FIR and IIR   2 2+  To demonstrate the RTRL algorithm, consider again the feedforward dy- namic network of Figure 14.2. The equation of operation of this network is  a t   =  n t   =  iw1 1 0 p t   +  iw1 1 1 p t 1–    +  iw1 1 2 p t 2–    .  The architecture of the network is defined by  U  =  1   ,   X  =  1   ,   I1  =  1   ,   DI1 1  =     0 1 2      ,   f L1  =  ,   U 1  = ELW  .  We will choose the following standard performance function with three  time points:  F  =    t t   –  a t   2  =  e2 t   =  e2 1   +  e2 2   +  e2 3   ,  Q    t  1=  3    t  1=  with the following inputs and targets:    p 1  t 1           p 2  t 2           p 3  t 3       .  The RTRL algorithm begins with some initialization:  U =  ,   ES 1  =  ,   X 1  = ES  .  In addition, the initial conditions for the delays,  vided.  p 0  p    1–    , must be pro-  The network response is then computed for the first time step:  a 1   =  n 1   =  iw1 1 0 p 1   +  iw1 1 1 p 0   +  iw1 1 2 p  1–    Because the RTRL algorithm proceeds forward through time, we can im- mediately compute the derivatives for the first time step. We will see in the  next section that the BPTT algorithm, which proceeds backward through  time, will require that we proceed through all of the time points before we  can compute the derivatives.  From the preceding pseudo-code, the first step in the derivative calculation  will be  since the transfer function is linear. We also update the following sets:  S1 1 1   =  F· 1 n1 1       =  1  ,  X 1  ES  =  1   ,   ES 1   =  1   .  14-18   14  Dynamic Backpropagation  The next step is the computation of the explicit derivatives from Eq.   14.42 :  ea1 1    ------------------------------------------ T vec IW1 1 0    ea1 1    ------------------------------------------ T vec IW1 1 1    =  =  ea 1  ----------------------- iw1 1 0   ea 1  ----------------------- iw1 1 1   ea1 1    ------------------------------------------ T vec IW1 1 2    =  ea 1  ----------------------- iw1 1 2   =    p1 1   T    S1 1  t   =  p 1   ,  =    p1 0   T    S1 1  t   =  p 0   ,  =    p1  1–    T    S1 1  t   =  p  1–    .  The next step would be to compute the total derivative, using Eq.  14.34 .  However, since  , the total derivatives are equal to the explicit  derivatives.  U 1  = ELW  All of the above steps would be repeated for each time point, and then the  final step is to compute the derivatives of the performance index with re- spect to the weights, using Eq.  14.20 :  F x------  =  Q      t  1=  u U  au t  --------------- xT  T    eF --------------- au t    =  3    t  1=  a1 t  --------------- xT  T    eF --------------- a1 t    .  If we break this down for each weight, we have  F  ----------------------- iw1 1 0   F  ----------------------- iw1 1 1   F  ----------------------- iw1 1 2   =  p 1   –  2e 1     +  p 2   –  2e 2     +  p 3  2e 3   –    ,  =  p 0   –  2e 1     +  p 1   –  2e 2     +  p 2  2e 3   –    ,  =  p  1–    –  2e 1     +  p 0   –  2e 2     +  p 1   –  2e 3     .  We can then use this gradient in any of our standard optimization algo- rithms from Chapters 9 and 12. Note that if we use steepest descent, this  result is a batch form of the LMS algorithm  see Eq.  10.33  .  2 2+  Let’s now do an example using a recurrent neural network. Consider again  the simple recurrent network in Figure 14.4. From Eq.  14.5 , the equation  of operation of this network is  a t   =  lw1 1 1 a t  1–    +  iw1 1 p t   The architecture of the network is defined by  14-19   D  14 Dynamic Networks  U  =  1   ,   X  =  1   ,   I1  =  1   ,   DI1 1  =  0   ,   DL1 1  =  1   ,   f L1  =  1   ,   U 1  ELW  =  1   .  We will choose the same performance function as the previous example:  F  =    t t   –  a t   2  =  e2 t   =  e2 1   +  e2 2   +  e2 3   ,  Q    t  1=  3    t  1=  with the following inputs and targets:    p 1  t 1           p 2  t 2           p 3  t 3       .  We initialize with  U =  ,   ES 1  =  ,   X 1  = ES  .  In addition, the initial condition for the delay,  tives   a 0   , and the initial deriva-  a 0  --------------- iw1 1   and   a 0  ----------------------- lw1 1 1   must be provided.  The initial derivatives are usually set to zero.   The network response is then computed for the first time step:  a 1   =  lw1 1 1 a 0   +  iw1 1 p 1   The derivative calculation begins with  since the transfer function is linear. We also update the following sets:  S1 1 1   =  F· 1 n1 1       =  1  ,  X 1  ES  =  1   ,   ES 1   =  1   .  The next step is the computation of the explicit derivatives:  ea1 1    ------------------------------------------ T vec IW1 1 0    =  ea 1  ---------------- iw1 1  ea1 1    -------------------------------------------- T vec LW1 1 1    =  ea 1  ----------------------- lw1 1 1   =    p1 1   T    S1 1 1   =  p 1   ,  =    a1 0   T    S1 1 1   =  a 0   .  14-20   Dynamic Backpropagation  The next step is to compute the total derivative, using Eq.  14.34 :  a1 t  --------------- xT  =  ea1 t  ----------------- S1 1 xT  +  t LW1 1 1 a1 t 1–  ------------------------ xT  .   14.45   Replicating this formula for each of our weights for this network, for  we have  t  1=  ,   a 1  --------------- iw1 1  =  p 1   +  a 1  ----------------------- lw1 1 1   =  a 0   +  =  lw1 1 1  a 0  --------------- iw1 1 lw1 1 1  a 0  ----------------------- lw1 1 1   p 1   ,  =  a 0   .  14  Note that unlike the corresponding equation in the previous example, these  equations are recursive. The derivative at the current time depends on the  derivative at the previous time.  Note that the two initial derivatives on the  right side of this equation would normally be set to zero, but at the next  time step they would be nonzero.  As we mentioned earlier, the weights in  a recurrent network have two different effects on the network output. The  first is the direct effect, which is represented by the explicit derivative in  Eq.  14.45 . The second is an indirect effect, since one of the inputs to the  network is a previous output, which is also a function of the weights. This  effect causes the second term in Eq.  14.45 .  All of the above steps would be repeated for each time point:  ea 2  ---------------- iw1 1  =  p 2   ,   =  a 1   ,  ea 2  ----------------------- lw1 1 1   a 2  --------------- iw1 1  =  p 2   +  a 2  ----------------------- lw1 1 1   =  a 1   +  =  lw1 1 1  a 1  --------------- iw1 1 lw1 1 1  a 1  ----------------------- lw1 1 1   p 2   +  lw1 1 1 p 1   ,  =  a 1   +  lw1 1 1 a 0   ,  ea 3  ---------------- iw1 1  =  p 3   ,   =  a 2   ,  ea 3  ----------------------- lw1 1 1   a 3  --------------- iw1 1  =  p 3   a 3  ----------------------- lw1 1 1   =  a 2   +  +  lw1 1 1  a 2  --------------- iw1 1 lw1 1 1  a 2  ----------------------- lw1 1 1   =  p 3   +  lw1 1 1 p 2   +    lw1 1 1   2p 1   ,  =  a 2   +  lw1 1 1 a 1   +    lw1 1 1   2a 0   .  14-21   D  14 Dynamic Networks  The final step is to compute the derivatives of the performance index with  respect to the weights, using Eq.  14.20 :  F x------  =  Q      t  1=  u U  au t  --------------- xT  T    eF --------------- au t    =  3    t  1=  a1 t  --------------- xT  T    eF --------------- a1 t    .  If we break this down for each weight, we have  F --------------- iw1 1  2e 1   –  a 1  --------------- iw1 1 2e 1  p 1  –      +  –  a 2  --------------- iw1 1 2e 2  p 2      –  =  =  +  2e 2   –  2e 3  p 3     +  lw1 1 1 p 2   +  –  2e 3       +  a 3  --------------- iw1 1 lw1 1 1 p 1  lw1 1 1      2p 1     2e 1     +  –  2e 2     +  –  2e 3     F  ----------------------- lw1 1 1   –  a 1  ----------------------- lw1 1 1  2e 1  a 0   –  =  =  a 2  ----------------------- lw1 1 1   +    –  2e 2  a 1   –  2e 3  a 2     +  lw1 1 1 a 1   +  a 3  ----------------------- lw1 1 1    lw1 1 1 a 0  lw1 1 1    2a 0     The expansions that we show in the final two lines of the above equations   and also in some of the previous equations  would not be necessary in prac- tice, since the results would be numerical. We have included them here so  that we can compare this result with the BPTT algorithm, which we  present next.  Backpropagation-Through-Time In this section we will generalize the Backpropagation-Through-Time   BPTT  algorithm, given in Eq.  14.15  and Eq.  14.17 , for LDDN net- works.  The first step is to generalize Eq.  14.15 . For the general LDDN network,  we can calculate the terms of the gradient by using the chain rule, as in  Eq.  14.15   F ----------------------- m l d   lwi j  Q  =  Su     t  1=  u U  k  1=  F --------------- u t   ak    u t  eak ----------------- m t   ni  m t  eni ----------------------- m l d   lwi j   14.46    for the layer weights , where  layers, and   Su   is the number of neurons in layer   u  .  u   is an output layer, U is the set of all output   From Eq.  14.24  we can write  14-22   Dynamic Backpropagation  We will also define  The terms of the gradient for the layer weights can then be written   14.48   14  m t  eni ----------------------- m l d   lwi j  =  l t d– aj    .  m t  di  =  Su   F --------------- u t   ak    u t  eak ----------------- m t   ni  .  u U  k  1=  F ----------------------- m l d   lwi j  Q  =  t  1=  l t m t aj d– di    ,  If we use the sensitivity term defined in Eq.  14.26 ,  u m sk i  t     u t  eak ----------------- m t   ni  ,  then the elements   m t  di   can be written  In matrix form this becomes  where  m t  di  =  Su   F --------------- u t   ak  u U  k  1=    u m sk i  t   .  dm t   =  u U  Su m    t   T    F --------------- au t    F --------------- au t    =  F --------------- u t   a1  ---------------  F F ---------------- u u t   t   aSu a2  T  Now the gradient can be written in matrix form.  F  ---------------------------- LWm l d    Q  =  t  1=  dm t       al t  d–    T  ,   14.54   and by similar steps we can find the derivatives for the biases and input  weights:  14-23   14.47    14.49    14.50    14.51    14.52    14.53    D  Eq.  14.54  through Eq.  14.56  make up the generalization of Eq.  14.15   for the LDDN network.   14 Dynamic Networks  F  -------------------------- IWm l d    Q  =  t  1=  dm t       pl t  d–    T  ,  F --------- bm  Q  =  t  1=  dm t   .  Eq.  14.17    14.55    14.56   The next step in the development of the BPTT algorithm is the generaliza- tion of Eq.  14.17 . Again, we use the chain rule:  F --------------- au t    =  eF --------------- au t    +      u' U  x X    d DLx u  eau' t  d+ --------------------------- T nx t d+     enx t  d+ --------------------------- au t T    T    F ------------------------- au' t   d+   14.57    Many of the terms in these summations will be zero. We will provide a  more efficient representation later in this section.  In Eq.  14.17  we only  had one delay in the system. Now we need to account for each network out- put, how that network output is connected back through a network input,  and also for the number of times each network output is delayed before it  is applied to a network input. That is the reason for the three summations  in Eq.  14.57 . This equation must be updated backward in time, as t is var- ied from Q to 1. The terms   F --------------- au' t     14.58   are generally set to zero for t > Q.   If we consider the matrix in the brackets on the right side of Eq.  14.57 ,  from Eq.  14.29  we can write  eau' t d+  --------------------------- T nx t d+     enx t d+  --------------------------- au t T    =  Su' x  d+ t   LWx u d    .   14.59   This allows us to write Eq.  14.57  as   14-24   Dynamic Backpropagation  F --------------- au t    =  eF --------------- au t   +      u' U  x X    d DLx u  Su' x    d+ t   LWx u d    T     14.60   F ------------------------- au' t   d+  Many of the terms in the summation on the right hand side of Eq.  14.60   will be zero and will not have to be computed. In order to provide a more  efficient implementation of Eq.  14.60 , we define the following indicator  sets:  14  X ELW  u   =    x X      LWx u d   0  0 d      ,  U x  ES  =    u U     Su x  0      .   14.61    14.62   u  The first set contains all of the input layers that have a connection from   with at least some nonzero delay. The second set contains  output layer  . When the  output layers that have a nonzero sensitivity with input layer  Su x sensitivity    to output layer    is nonzero, there is a static connection from input layer  u  x  x  .  We can now rearrange the order of the summation in Eq.  14.60  and sum  only over the existing terms:  F --------------- au t    =  eF --------------- au t   + X x ELW      LWx u d T  u     d DLx u  Su' x  d+ t  T    F ------------------------- au' t   d+   U x  u' ES     14.63   The total BPTT algorithm is summarized in the following pseudo code.  Summary  14-25   Backpropagation-Through-Time Gradient  D  14 Dynamic Networks  Initialize:  F --------------- au t    For t = Q to 1,  0 t Q=  , for all   u U  ,  U u  = U = ES For m decremented through the BP order  ES u  =  , and   ,    for all   u U  .    b Lm Su l  t LWl m  0   F· m nm t       For all   =  t   , if   Su m  u U  ES u   l ES u   add m to the set  add u to the set   b Lm ES u  U m  ES  EndFor u m U If  Sm m t   add m to the sets   F· m nm t   ,   =  U ES m     and   U m ES     decremented through the BP order  EndIf m EndFor m u U For   =  eF --------------- au t    F --------------- au t   EndFor u For all layers m  = U m u ES  dm t       EndFor m  EndFor t Compute Gradients  F  ---------------------------- LWm l d    F  -------------------------- IWm l d    Q  =  1= t Q  =  t  1=  Q  F --------- bm  =  dm t   t  1=  Su m    t   T    F --------------- au t    dm t       al t  d–    T  dm t       pl t  d–    T  14-26  + X x ELW    u     d DLx u    LWx u d T   U x  u' ES    Su' x  d+ t  T    F ------------------------- au' t   d+   Dynamic Backpropagation  Example BPTT Implementations  FIR and IIR   2 2+  To demonstrate the BPTT algorithm, we will use the same example net- works that we used for the RTRL algorithm. First, we use the feedforward  dynamic network of Figure 14.2. We defined the network architecture on  page 14-18.  Before the gradient can be computed using BPTT, the network response  must be computed for all time steps:  14  a 1   =  n 1   =  iw1 1 0 p 1   +  iw1 1 1 p 0   +  iw1 1 2 p  1–    ,  a 2   =  n 2   =  iw1 1 0 p 2   +  iw1 1 1 p 1   +  iw1 1 2 p 0   a 3   =  n 3   =  iw1 1 0 p 3   +  iw1 1 2 p 0   +  iw1 1 2 p 1   ,  .  The BPTT algorithm begins with some initialization:  U =  ,   ES 1  =  ,   U 1  = ES  .  The first step in the derivative calculation will be the sensitivity calcula- tion. For BPTT, we start at the last time point    3=   :  t  since the transfer function is linear. We also update the following sets:  The next step is the calculation of the following derivative using Eq.   14.63 :  S1 1 3   =  F· 1 n1 3       =  1  ,  U 1  ES  =  1   ,   ES 1   =  1   .  F ---------------- a1 3    =  eF ---------------- a1 3    =  –  2e 3   .  The final step for   t  3=   is Eq.  14.52 :  d1 3   =  S1 1 3     T    =  –  2e 3   .  We repeat the previous steps for   t  2=  1=  , to obtain  F ---------------- a1 3    and   t  F ---------------- a1 2    F ---------------- a1 1    d1 2   =  S1 1 2     T    =  –  2e 2   ,  d1 1   =  S1 1 1     T    =  –  2e 1   .  14-27   D  14 Dynamic Networks  Now, all time steps are combined in Eq.  14.55 :  F  -------------------------- IW1 1 0    =  F  ----------------------- iw1 1 0   =  3    t  1=  d1 t       p1 t   T  =  –  2e t     p t   ,  F  -------------------------- IW1 1 1    F  ----------------------- iw1 1 1   F  -------------------------- IW1 1 2    F  ----------------------- iw1 1 2   =  =  =  =  3    t  1=  3    t  1=  d1 t       p1 t  1–    T  =  –  2e t     p t 1–    ,  d1 t       p1 t  2–    T  =  –  2e t     p t 2–    .  3    t  1=  3    t  1=  3    t  1=  Note that this is the same result we obtained for the RTRL algorithm ex- ample on page 14-19. RTRL and BPTT should always produce the same  gradient. The only difference is in the implementation.  2 2+  Let’s now use our previous recurrent neural network example of Figure  14.4. We defined the architecture of this network on page 14-19.  Unlike the RTRL algorithm, where initial conditions for the derivatives  must be provided, the BPTT algorithm requires final conditions for the de- rivatives:  a 4  --------------- iw1 1   and   a 4  ----------------------- lw1 1 1   ,  which are normally set to zero.   The network response is then computed for all time steps:  a 1   =  lw1 1 1 a 0   +  iw1 1 p 1   a 2   =  lw1 1 1 a 1   +  iw1 1 p 2   a 3   =  lw1 1 1 a 2   +  iw1 1 p 3   S1 1 3   =  F· 1 n1 3       =  1  ,  X 1  ES  =  1   ,   ES 1   =  1   .  14-28  The derivative calculation begins with  since the transfer function is linear. We also update the following sets:  Next we compute the following derivative using Eq.  14.63 :   14  Dynamic Backpropagation  F --------------- a1 t   , we find  For   t  3=  =  eF --------------- LW1 1 1 TS1 1 a1 t    +  1+ t  T    F ------------------------ a1 t   1+  F ---------------- a1 3    =  eF ---------------- a1 3    +  lw1 1 1 S1 1 4 T    0  =  F ---------------- a1 4    eF ---------------- a1 3    =  –  2e 3   and  and  and  Continuing to   t  2=  ,  d1 3   =  S1 1 3     T    =  –  2e 3   F ---------------- a1 3    S1 1 2   =  F· 1 n1 2       =  1  ,  F ---------------- a1 2    eF ---------------- a1 2   2e 2  –  =  =  +  lw1 1 1 S1 1 3 T    +  lw1 1 1   –  2e 3     F ---------------- a1 3    d1 2   =  S1 1 2     T    =  –  2e 2   +  lw1 1 1   –  2e 3     F ---------------- a1 2    Finally, for   t  1=  ,  S1 1 1   =  F· 1 n1 1       =  1  ,  +  lw1 1 1 S1 1 2 T    F ---------------- a1 2   lw1 1 1    +  lw1 1 1   –  2e 2     +  2  –  2e 3     F ---------------- a1 1    eF ---------------- a1 1   2e 1   –  =  =  d1 1   =  S1 1 1     T    =  –  2e 1   +  lw1 1 1   –  2e 2     +    lw1 1 1   2  –  2e 3     Now we can compute the total gradient, using Eq.  14.54  and Eq.  14.55 :  F ---------------- a1 1    14-29   D  14 Dynamic Networks  F  ---------------------------- LW1 1 1    F  ----------------------- lw1 1 1   =  3    t  d1 t       a1 t  1–    T   a 0  –  a 1  +  2e 1  + 2e 2  –  1= lw1 1 1  2e 2  –  2e 3  lw1 1 1  – +  +    lw1 1 1   a 0  – +  2 – 2e 3   2e 3        d1 t       p1 t   T  F  -------------------------- IW1 1 0    F --------------- iw1 1  =  3    t  1= 2e 1  + 2e 2  –   p 1  –  p 2  +  lw1 1 1  2e 2   – lw1 1 1  2e 3  – +  +    lw1 1 1   p 3  – +  2 – 2e 3   2e 3        =  =  =  =  This is the same result that we obtained with the RTRL algorithm on page  14-22.  Summary and Comments on Dynamic Training The RTRL and BPTT algorithms represent two methods for computing the  gradients for dynamic networks. Both algorithms compute the exact gradi- ent, and therefore they produce the same final results. The RTRL algo- rithm performs the calculations from the first time point forward, which is  suitable for on-line  real-time  implementation. The BPTT algorithm starts  from the last time point and works backward in time. The BPTT algorithm  generally requires fewer computations for the gradient calculation than  RTRL, but BPTT usually requires more memory storage.   In addition to the gradient, versions of BPTT and RTRL can be used to com- pute Jacobian matrices, as are needed in the Levenberg-Marquardt de- scribed in Chapter 12. For Jacobian calculations, the RTRL algorithm is  generally more efficient that the BPTT algorithm. See [DeHa07] for details.  Once the gradients or Jacobians are computed, many standard optimiza- tion algorithms can be used to train the networks. However, training dy- namic networks is generally more difficult than training feedforward  networks—for a number of reasons. First, a recurrent net can be thought  of as a feedforward network, in which the recurrent network is unfolded in  time. For example, consider the simple single-layer recurrent network of  Figure 14.4. If this network were to be trained over five time steps, we  could unfold the network to create 5 layers - one for each time step. If a sig- moid transfer function is used, then if the output of the network is near the  saturation point for any time point, the resulting gradient could be quite  small.  Another problem in training dynamic networks is the shape of the error  surface. It has been shown  see [PhHa13]  that the error surfaces of recur- rent networks can have spurious valleys that are not related to the dynam-  14-30   Dynamic Backpropagation  ic system that is being approximated. The underlying cause of these valleys  is the fact that recurrent networks have the potential for instabilities. For  example, the network of Figure 14.4 will be unstable if   is greater  than one in magnitude. However, it is possible, for a particular input se- quence, that the network output can be small for a particular value of  lw1 1 1  ues for    greater than one in magnitude, or for certain combinations of val- lw1 1 1   lw1 1 1    and   iw1 1  .   Finally, it is sometimes difficult to get adequate training data for dynamic  networks. This is because the inputs to some layers will come from tapped  delay lines. This means that the elements of the input vector cannot be se- lected independently, since the time sequence from which they are sampled  is generally correlated in time. Unlike static networks, in which the net- work response depends only on the input to the network at the current  time, dynamic network responses depend on the history of the input se- quence. The data used to train the network must be representative of all  situations for which the network will be used, both in terms of the ranges  for each input, but also in terms of the variation of the inputs over time.  14  2 2+  To illustrate the training of dynamic networks, consider again the simple  recurrent network of Figure 14.4, but let’s use a nonlinear sigmoid transfer  function, as shown in Figure 14.9.   Inputs  Tan-Sigmoid Neuron  n t     a t     p t     iw1,1  cid:2  lw1,1 1   D      a t  =    tansig      iw p t  1,1  +  lw  1,1   1    -1     a t  Figure 14.9  Nonlinear Recurrent Network  Recall from Chapter 11 that static multilayer networks can be used to ap- proximate functions. Dynamic networks can be used to approximate dy- namic systems. A function maps from one vector space  the domain  to  another vector space  the range . A dynamic system maps from one set of  time sequences  the input sequences    to another set of time sequences   . For example, the network of Figure 14.9 is a   the output sequences  dynamic system. It maps from input sequences to output sequences.  p t   a t   In order to simplify our analysis, we will give the network a problem for  which we know the optimal solution. The dynamic system we will approxi- mate is the same network, with the following values for the weights:  14-31   14 Dynamic Networks  lw1 1 1   =  0.5  ,   iw1 1  =  0.5  ,   14.64   The input sequence that we use to train a dynamic network must be repre- sentative of all possible input sequences. Because this network is so simple,  it is not difficult to find an appropriate input sequence, but for many prac- tical networks it can be difficult. We will use a standard form of input se- quence  called the skyline function , which consists of a series of pulses of  varying height and width. The input and target sequences are shown in  Figure 14.10. The circles represent the input sequence and the dots repre- sent the target sequence. The targets were created by applying the given  input sequence to the network of Figure 14.9, with the weights given by Eq.   14.64 .  D  1  0.8  0.6  0.4  0.2  0  −0.2  −0.4  −0.6  −0.8  −1  0  14-32  2  4  6  8  10  12  14  16  18  20  Figure 14.10  Input and Target Sequences  Figure 14.11 shows the squared error performance surface for this prob- lem. Note that as the weight  tude, the squared error grows steeply. This effect would be even more  prominent, if the length of the training sequence were longer. However, we  can also see some narrow valleys in the surface in the regions where  lw1 1 1  [PhHa13]. See Exercise E14.18 to investigate the cause of these valleys.    is greater than one.  This is a very common result, as discussed in    becomes greater than one in magni-  lw1 1 1   The narrow valleys can have an effect on training, since the trajectory can  be trapped or misdirected by the spurious valleys. On the left of Figure  14.11 we see a steepest descent path. The path is misdirected at the begin- ning of the trajectory, because of the narrow valley seen near the bottom of  the contour plot.   Dynamic Backpropagation  lw1 1 1   1.5  0.5  2  1  0  −0.5  −1  −1.5  −2 −2  F  1.4  1.2  1  0.8  0.6  0.4  0.2  0 −2  −1.5  −1  −0.5  0  0.5  1  1.5  2  iw1 1  −1  lw1 1 1   0  1  1  2  2  −2  −1  0  iw1 1  14  Figure 14.11  Performance Surface and Steepest Descent Trajectory  To experiment with the training of this recurrent network, use the Neural  Network Design Demonstration Recurrent Network Training  nnd14rnt .  14-33   14 Dynamic Networks  Summary of Results  D  Initialize:  au t  --------------- xT  For t = 1 to Q,  0=  0 t  , for all   u U  ,  X u  = U = ES For m decremented through the BP order  ES u  =   and   ,    for all   u U  .  Real-Time Recurrent Learning Gradient  t LWl m  0   F· m nm t         For all   b Lm Su l  =  t   , if   Su m  u U  ES u   l ES u   add m to the set  if   m X  b Lm ES u   , add m to the set   EndFor u m U If  Sm m t   add m to the sets  if   F· m nm t  U    and  , add m to the set   m X  =  X u  ES   ES m X m ES    EndIf m EndFor m u U For   ------------------------------------------ T vec IWm l d    eau t   eau t  -------------------------------------------- T vec LWm l d    eau t  Su m ----------------- T bm   t   =  =    pl t  d–    T    Su m  t   =    al t  d–    T    Su m  t    incremented through the simulation order  For all weights and biases  x is a vector containing all weights and biases   EndFor weights and biases au t  --------------- xT  eau t  -----------------  xT  + x ES  =    X u   Su x  t    U x  ELW  u'        d DLx u'     LWx u'  d     au' t  d– ------------------------- xT  EndFor u  EndFor t Compute Gradients  F x------  Q  =    t  1=  u U  au t  --------------- xT  T    eF --------------- au t    14-34   Backpropagation-Through-Time Gradient  14  Summary of Results  Initialize:  F --------------- au t    For t = Q to 1,  0 t Q=  , for all   u U  ,  U u  = U = ES For m decremented through the BP order  ES u  =  , and   ,    for all   u U  .    b Lm Su l  t LWl m  0   F· m nm t       For all   =  t   , if   Su m  u U  ES u   l ES u   add m to the set  add u to the set   b Lm ES u  U m  ES  EndFor u m U If  Sm m t   add m to the sets   F· m nm t   ,   =  U ES m     and   U m ES     decremented through the BP order  EndIf m EndFor m u U For   =  eF --------------- au t    F --------------- au t   EndFor u For all layers m  = U m u ES  dm t       EndFor m  EndFor t Compute Gradients  F  ---------------------------- LWm l d    F  -------------------------- IWm l d    Q  =  1= t Q  =  t  1=  Q  F --------- bm  =  dm t   t  1=  Su m    t   T    F --------------- au t    dm t       al t  d–    T  dm t       pl t  d–    T  14-35  + X x ELW    u     d DLx u    LWx u d T   U x  u' ES    Su' x  d+ t  T    F ------------------------- au' t   d+   D  14 Dynamic Networks  Definitions Notation   is the lth input vector to the network at time t.  is the net input for layer m.   is the transfer function for layer m.   is the output for layer m.   is the input weight between input l and layer m.   is the layer weight between layer l and layer m.   is the bias vector for layer m.  pl t  nm t  fm  am t  IWm l LWm l bm DLm l Layer m.  DIm l Layer m.   is the set of all delays in the tapped delay line between Layer l and    is the set of all delays in the tapped delay line between Input l and    is the set of indices of input vectors that connect to layer m.   is the set of indices of layers that directly connect forward to layer m.  Im f Lm b  is the set of indices of layers that are directly connected backwards to  Lm layer m  or to which layer m connects forward  and that contain no delays  in the connection.  A layer is an input layer if it has an input weight, or if it contains any de- lays with any of its weight matrices. The set of input layers is X.  A layer is an output layer if its output will be compared to a target during  training, or if it is connected to an input layer through a matrix that has  any delays associated with it. The set of output layers is U.  The sensitivity is defined   u m sk i  t     u t  eak ----------------- m t   ni  .  U x  ELW  =    u U      LWx u d   0  0 d      X u  ES  =    x X      Su x  0      ES u   =   x    Su x  0      X ELW  u   =    x X     LWx u d   0  0 d      U x  ES  =    u U     Su x  0      14-36   Solved Problems  Solved Problems  P14.1 Before stating the problem, let’s first introduce some notation that   will allow us to efficiently represent dynamic networks:  Layer 1 with 2 tansig neurons.  Input vector with 3 elements.  14  1  2  9  Tapped delay from 1 to 9.  Tapped delay from 0 to 9.  Figure P14.1  Blocks for Dynamic Network Schematics  Using this notation, consider the following network  3  9  1  2  3  1  2  3  1 1  4  1  1  5  2  6  4  1  8  3  9  2  10  1  Figure P14.2  Example Dynamic Network for Problem P14.1  Define the network architecture, by showing  f Lm the dimension of each weight matrix.  ,  . Also, select a simulation order and indicate   U X Im DIm 1 DLm l  U x  ELW  b ELW ,  Lm  u   ,   ,   ,   ,   ,   ,   X  The input layers have input weights, or have delays with their weight ma- trices. Therefore  . The output layers are compared to tar- gets, or connect to an input layer through a matrix that has any delays. If    1 2 4 6 9  X  =            14-37  3  2  7  2   D  14 Dynamic Networks          =   2 3 4 5 8 10  we assume that only Layer 10 is compared to a target, then  U 1, the only nonempty set of inputs will be  there will only be one nonempty set of input delays:  nections between layers are defined by  . Since the single input vector connects only to Layer  . For the same reason,  . The con- DI1 1  1   0   I1  =  =      f L1  =  ,   f L2  =  1 2    ,   f L3  =  2   ,   f L4  =     3 4 7      ,   =  4   ,  f L5  f L6  =  2 5    ,   =  2   ,   f L8  =  7   ,   f L9  =  4 8    ,   f L10  =  6 9    .  b L1  =  2   ,   =     3 6 7      ,   b = L3  ,   b L4  =  5 9    ,   b = L5  ,  b L6  =  10    ,   b L7  =  4 8    ,   b = L8  ,   b L9  =  10    ,   b L10  =  .  f L7  b L2  Associated with these connections are the following layer delays  DL2 1  =  0   ,   DL2 2  =  1   ,   DL3 2  =  0   ,   DL4 3  =  1   ,   DL4 4  =  1   ,  DL4 7  =  0   ,   DL5 4  =  0   ,   DL6 2  =  0   ,   DL6 5  =  0 1    ,   DL7 2  =  0   ,  DL8 7  =  0   ,   DL9 4  =  0   ,   DL9 8  =  1   ,   DL10 6  =  0   ,   DL10 9  =  0   .  The layers that have connections from output layers are  U 2  ELW  =  2   ,   U 4  ELW  =  3 4    ,  U 6  ELW  =  5   ,   U 9  ELW  =  8   .  The layers that connect to input layers are  X ELW  2   =  2   ,   X ELW  3   =  4   ,   X ELW  4   =  4   ,   X ELW  5   =  6   ,   X ELW  8   =  9   .  The simulation order can be chosen to be  mensions of the weight matrices are     1 2 3 7 4 5 6 8 9 10                    . The di-  IW1 1 0   3  2  ,   LW2 1 0   2  3  ,   LW2 2 1   3  3  ,   LW3 2 0   3  2  ,  LW4 3 1   2  1  ,   LW4 4 1   1  1  ,   LW4 7 0   2  1  ,   LW5 4 0   1  2  ,  LW6 2 0   3  4  ,   LW6 5 1   2  4  ,   LW7 2 0   3  2  ,   LW8 7 0   2  3  ,  LW9 4 0   1  2  ,   LW9 8 1   3  2  ,   LW10 6 0   4  1  ,   LW10 9 d   2  1  .  14-38   Solved Problems  P14.2 Write out the BPTT equations for the network presented in Prob-  lem P14.1.  We will assume that the network response has been computed for all time  points, and we will demonstrate the process for one time step, since they all  will be similar. We proceed through the layers according to the backpropa- gation order, which is the reverse of the simulation order:    10 9 8 6 5 4 7 3 2 1    .                    14  F --------------- a8 t    =  eF --------------- LW9 8 1 TS10 9 a8 t    +  1+ t  T    F -------------------------- a10 t   1+  S10 10  t   =  F· 10 n10 t       F ----------------- a10 t    =  eF ----------------- a10 t    d10 t   =  S10 10    t   T    F ----------------- a10 t    S10 9  t   =  S10 10  t LW10 9 0 F· 9 n9 t       d9 t   =  S10 9    t   T    F ----------------- a10 t    S8 8  t   =  F· 8 n8 t       d8 t   =  S8 8    t   T    S10 6  t   =  S10 10  t LW10 6 0 F· 6 n6 t       F --------------- a8 t    F ----------------- a10 t    d6 t   =  S10 6    t   T    S5 5  t   =  F· 5 n5 t       d5 t   =  S5 5    t   T    F --------------- a5 t    14-39  F --------------- a5 t    =  eF --------------- LW6 5 1 TS10 6 a5 t    +  1+ t  T    F -------------------------- a10 t  1+   LW6 5 0 TS10 6  t T    +  F ----------------- a10 t     D  F --------------- a4 t    =  eF --------------- LW4 4 1 T S4 4 a4 t    +  1+ t  T    F ------------------------ a4 t   1+  +  S5 4  1+ t  T    +  S10 4  1+ t  T    F ------------------------ a5 t   1+  d4 t   =  S4 4    t   T    S5 4    t   T    +  S10 4    t   T    +  F --------------- a4 t    F --------------- a5 t    F -------------------------- a10 t  1+   F ----------------- a10 t    d7 t   =  S10 7    t   T    S8 7    t   T    +  S5 7    t   T    +  S4 7    t   T    +  F --------------- a8 t    F --------------- a5 t    F ----------------- a10 t    F --------------- a4 t    F --------------- a3 t    =  eF --------------- LW4 3 1 T S4 4 a3 t    +  1+ t  T    F ------------------------ a4 t   1+  +  S5 4  1+ t  T    +  S10 4  1+ t  T    F ------------------------ a5 t   1+  F -------------------------- a10 t  1+   S10 2  t   =  S10 6  t LW6 2 0 F· 2 n2 t      S10 7 +  t LW7 2 0 F· 2 n2 t       14 Dynamic Networks  S10 4  t   S10 9  t LW9 4 0 F· 4 n4 t       S5 4  t   S5 5  t LW5 4 0 F· 4 n4 t       S4 4  t   =  F· 4 n4 t       =  =  =  =  =  =  S10 7  t   S10 4  t LW4 7 0 F· 7 n7 t       S8 7  t   S5 7  t   S4 7  t   S8 8  t LW8 7 0 F· 7 n7 t       S5 4  t LW4 7 0 F· 7 n7 t       S4 4  t LW4 7 0 F· 7 n7 t       S3 3  t   =  F· 3 n3 t       d3 t   =  S3 3    t   T    F --------------- a3 t    S8 7  t LW7 2 0 F· 2 n2 t       S5 7  t LW7 2 0 F· 2 n2 t       S4 7  t LW7 2 0 F· 2 n2 t       S3 3  t LW3 2 0 F· 2 n2 t       S8 2  t   S5 2  t   S4 2  t   S3 2  t   =  =  =  =  14-40   Solved Problems  S2 2  t   =  F· 2 n2 t       F --------------- a2 t    =  +  eF --------------- LW2 2 1 T S2 2 a2 t   S4 2  1+ t  T    +  F ------------------------ a2 t   1+ T    +  S5 2  1+ t  1+ t  T    +  S3 2  1+ t  T    F ------------------------ a3 t   1+  +  S8 2  1+ t  T    +  S10 2  1+ t  T    14  F ------------------------ a5 t 1+   F -------------------------- a10 t   1+  d2 t   =  S10 2    t   T  S8 2    t   T  +  S5 2    t   T  +  S4 2    t   T    +  S3 2  t   T    S2 2  t   T      F --------------- a8 t   F --------------- a3 t    +      F --------------- a5 t   F --------------- a2 t    S10 1  t   S10 2  t LW2 1 0 F· 1 n1 t       S8 2  t LW2 1 0 F· 1 n1 t       S5 2  t LW2 1 0 F· 1 n1 t       S4 2  t LW2 1 0 F· 1 n1 t       S3 2  t LW2 1 0 F· 1 n1 t       S2 2  t LW2 1 0 F· 1 n1 t       d1 t   =  S10 1    t   T  S8 1    t   T  +  S5 1    t   T  +  S4 1    t   T    +  S3 1  t   T    S2 1  t   T      F --------------- a8 t   F --------------- a3 t    +      F --------------- a5 t   F --------------- a2 t    After the preceding steps have been repeated for all time points, from the  last time point back to the first, then the gradient can be computed as fol- lows:  F  ---------------------------- LWm l d    Q  =  t  1=  dm t       al t  d–    T  F ------------------------ a4 t   1+ F ------------------------ a8 t   1+    F ----------------- a10 t   F --------------- a4 t    +    =  =  =  =  =  =  S8 1  t   S5 1  t   S4 1  t   S3 1  t   S2 1  t     F ----------------- a10 t   F --------------- a4 t    +    14-41   D  14 Dynamic Networks  F  -------------------------- IWm l d    Q  =  t  1=  dm t       pl t  d–    T  Q  F --------- bm  =  dm t   t  1=  P14.3 Write out the RTRL equations for the network presented in Prob-  lem P14.1.  As in the previous problem, we will demonstrate the process for one time  step, since each step is similar. We will proceed through the layers accord- ing to the backpropagation order. The sensitivity matrices   are com- puted in the same way for the RTRL algorithm as for the BPTT algorithm,  so we won’t repeat those steps from Problem P14.2.  Su m  t   The explicit derivative calculations for the input weight will be  For the layer weights and the biases, the explicit derivatives are calculated  by  eau t    ------------------------------------------ T vec IW1 1 0    =    p1 t   T    Su 1  t   eau t    -------------------------------------------- T vec LWm l d    =    al t  d–    T    Su m  t   eau t  ----------------- T bm  For the total derivatives, we have  =  Su m  t   a2 t  --------------- xT  a3 t  --------------- xT  =  =  ea2 t  ----------------- S2 2 xT  +  t  LW2 2 1     ea3 t  ----------------- S3 2 xT  +  t  LW2 2 1     a2 t  1– ------------------------ xT  a2 t  1– ------------------------ xT  14-42   Solved Problems  a4 t  --------------- xT  =  ea4 t  ----------------- S4 4 xT  +  t  LW4 4 1     +  LW4 3 1     a4 t  1– ------------------------ xT  a3 t  1– ------------------------ xT  +  S4 2  t  LW2 2 1     a2 t  1– ------------------------ xT  a5 t  --------------- xT  =  ea5 t  ----------------- S5 4 xT  +  t  LW4 4 1     +  LW4 3 1     a4 t  1– ------------------------ xT  a3 t  1– ------------------------ xT  14  +  S5 2  t  LW2 2 1     a2 t  1– ------------------------ xT  a8 t  --------------- xT  =  ea8 t  ----------------- S8 2 xT  +  t  LW2 2 1     a2 t  1– ------------------------ xT  a10 t  -----------------  xT  xT  ea10 t  ------------------- S10 9  +  =  t  LW9 8 1     a8 t  1– ------------------------ xT  +  S10 6  t  LW6 5 0     +  LW6 5 1     a5 t  --------------- xT  a4 t  1– ------------------------ xT  a5 t  1– ------------------------ xT  a3 t  1– ------------------------ xT  +  S10 4  t  LW4 4 1     +  LW4 3 1     +  S10 2  t  LW2 2 1     a2 t  1– ------------------------ xT  After the above steps are iterated through all time points, we can compute  the gradient with  F -------- xT  Q  =  t  1=  T    a2 t  --------------- xT  +  eF --------------- a2 t   T  T    a3 t  --------------- xT  +  eF --------------- a3 t   T  T    a4 t  --------------- xT  eF --------------- a4 t   T  +  eF --------------- a5 t      a5 t  --------------- xT  +  eF --------------- a8 t      a8 t  --------------- xT  +  eF ----------------- a10 t      a10 t  -----------------  xT  P14.4 From the previous problem, show the detail of the calculations in-  volved in the explicit derivative term  ea2 t    ------------------------------------------ T vec IW1 1 0    =    p1 t   T    S2 1  t   14-43   D  vec IW1 1 0     T  =  1 1 iw1 1  1 1 iw2 1  1 1 iw1 2  1 1 iw2 2  1 1 iw1 3  1 1 iw2 3  14 Dynamic Networks  First, let’s display the details of the individual vectors and matrices in this  expression:  IW1 1 0   =  1 1 iw1 1 1 1 iw2 1  1 1 iw1 2 1 1 iw2 2  1 1 iw1 3 1 1 iw2 3  p1 t   =  S2 1     t   =  p1 p2 p3  ea2 t  ------------------ n1 t T   =  2 1 s1 1 2 1 s2 1 2 1 s3 1  2 1 s1 2 2 1 s2 2 2 1 s3 2  ea2 t    ------------------------------------------ T vec IW1 1 0    =  2 a1 1 1  iw1 1 2 a2 1 1  iw1 1 2 a3 1 1  iw1 1  2 a1 1 1  iw2 1 2 a2 1 1  iw2 1 2 a3 1 1  iw2 1  2 a1 1 1  iw1 2 2 a2 1 1  iw1 2 2 a3 1 1  iw1 2  2 a1 1 1  iw2 2 2 a2 1 1  iw2 2 2 a3 1 1  iw2 2  2 a1 1 1  iw1 3 2 a2 1 1  iw1 3 2 a3 1 1  iw1 3  2 a1 1 1  iw2 3 2 a2 1 1  iw2 3 2 a3 1 1  iw2 3  The Kronecker product is defined as  A B  =   a1 1 B  a1 m B  an 1 B  an m B  ,  therefore    p1 t   T    S2 1  t   =  p1s1 1 p1s2 1 p1s3 1  2 1 p1s1 2 2 1 p1s2 2 2 1 p1s3 2  2 1 p2s1 1 2 1 p2s2 1 2 1 p2s3 1  2 1 p2s1 2 2 1 p2s2 2 2 1 p2s3 2  2 1 p3s1 1 2 1 p3s2 1 2 1 p3s3 1  2 1 p3s1 2 2 1 p3s2 2 2 1 p3s3 2  2 1  2 1  2 1  .  14-44   Solved Problems  P14.5 Find the computational complexity for the BPTT and RTRL algo- rithms applied to the sample network in Figure P14.3 as a function  of the number of neurons in Layer 1    , the number of delays in  the tapped delay line     and the length of the training sequence    Q  S1   .  D  D  R  1  S1  14  Figure P14.3  Sample Network for Exercise E14.1  The complexity of the BPTT gradient calculation is generally determined  by Eq.  14.54 . For this network, the most important weights will be  LW1 1 d   :  F  ---------------------------- LW1 1 d    Q  =  t  1=  d1 t       a1 t  d–    T  ,  The outer product calculation involves  done for Q time steps and for D delays, so the BPTT gradient calculation is  O S1   operations, which must be   DQ  2      .  S1  2  The complexity of the RTRL gradient is based generally on Eq.  14.34 . For  this sample network, we can consider the equation for   2=  u  :  a2 t  --------------- xT  =  ea2 t  ----------------- S2 1 xT  +  t   LW1 1 d     a1 t  d– ------------------------ xT  D    d  1=  S1  Inside the summation we have a matrix multiplication involving an  matrix times an  O S1 dient calculations are  matrix does not change the order of the complexity.  S1    matrix. This multiplication will be  . It must be done for every d and every t, therefore the RTRL gra- . The multiplication by the sensitivity   O S1  D2Q  DS1  S1  3+  1+  S1  4  4  D                  14-45   Epilogue  D  14 Dynamic Networks  Dynamic networks can be trained using the same optimization procedures  that we described in Chapter 12 for static multilayer networks. However,  the calculation of the gradient for dynamic networks is more complex than  for static networks. There are two basic approaches to the gradient calcu- lation in dynamic networks. One approach, backpropagation through time   BPTT , starts at the last time point, and works backward in time to com- pute the gradient. The second approach, real-time recurrent learning   RTRL , starts at the first time point, and then works forward through  time.  RTRL requires more calculations than BPTT for calculating the gradient,  but RTRL allows a convenient framework for on-line implementation. Also,  RTRL generally requires less storage than BPTT. For Jacobian calcula- tions, RTRL is often more efficient than the BPTT algorithm.  Chapter 27 presents a real-world case study for using dynamic networks to  solve a prediction problem.  14-46   Further Reading  Further Reading  [DeHa07]  O. De Jesús and M. Hagan, “Backpropagation Algorithms  for a Broad Class of Dynamic Networks,” IEEE Transac- tions on Neural Networks, vol. 18, no. 1, pp., 2007.  This paper presents a general development of BPTT and  RTRL algorithms for gradient and Jacobian calculations.  Experimental results are presented that compare the com- putational complexities of the two algorithms for a variety  of network architectures.  14  [MaNe99]  J.R. Magnus and H. Neudecker, Matrix Differential Calcu- lus, John Wiley & Sons, Ltd., Chichester, 1999.  This textbook provides a very clear and complete descrip- tion of matrix theory and matrix differential calculus.  M. Phan and M. Hagan, “Error Surface of Recurrent Net- works,” IEEE Transactions on Neural Networks and  Learning Systems, Vol. 24, No. 11, pp. 1709 - 1721, October,  2013.  This paper describes spurious valleys that appear in the er- ror surfaces of recurrent networks. It also describes some  procedures that can be used to improve training of recur- rent networks.  [Werb90]  P. J. Werbos, “Backpropagation through time: What it is  and how to do it,” Proceedings of the IEEE, vol. 78, pp.  1550–1560, 1990.  [PhHa13]  [WiZi89]  The backpropagation through time algorithm is one of the  two principal approaches to computing the gradients in re- current neural network. This paper describes the general  framework for backpropagation through time.  R. J. Williams and D. Zipser, “A learning algorithm for con- tinually running fully recurrent neural networks,” Neural  Computation, vol. 1, pp. 270–280, 1989.  This paper introduced the real-time, recurrent learning al- gorithm for computing the gradients of dynamic networks.  Using this method, the gradients are computed by starting  at the first time point, and then working forward through  time. The algorithm is suitable for on-line, or real-time, im- plementation.  14-47   14 Dynamic Networks  Exercises  E14.1 Put the network of Figure 14.1 into the schematic form, which we intro-  duced in Problem P14.1.  D  E14.2 Consider the network in Figure 14.4, with weight values  p 3   , and   p 2   a 0   p 1   . If   2=  3=  4=  ,   ,   iw1 1 2=  2= , find    and  a 1   ,   lw1 1 1  = a 2  a 3   ,   0.5 .  E14.3 Consider the network in Figure P14.3, with  =  0.5 = ,  1   ,   p1 2   LW1 1 2  2   =  IW1 1 a1 1–  a1 2   and   LW1 1 1  ,  1  p1 1  . If  1  = a1 3  .  = =  S1 ,  2= D a1 0  ,   0.2 p1 3  =  1= = 1–   ,  1=  and   ,  R 2  , find    and   a1 1   ,   E14.4 Consider the network in Figure E14.1. Define the network architecture, by   showing  simulation order and indicate the dimension of each weight matrix.  U X Im DIm 1 DLm l  . Also, select a   U x  ELW  b ELW ,  Lm  f Lm  u   ,   ,   ,   ,   ,   ,   ,   X  2  2  1  2  3  2  4  3  5  2  4  3  Figure E14.1  Dynamic Network for Exercise E14.4  E14.5 Write out the RTRL equations for the network in Figure E14.2  1  3  1  2  1 2  2  2  3  2  Figure E14.2  Dynamic Network for Exercise E14.5  E14.6 Write out the BPTT equations for the network in Figure E14.2.  14-48   Exercises  E14.7 Write out the equations of operation for the network in Figure E14.3. As- sume that all weights have a value of 0.5, and that all biases have a value  of 0.  i. Assume that the initial network output is   initial network input is   p 1   1=  . Solve for   a 0   = . a 1   0.5  , and that the   ii. Describe any problems that you would have in simulating this net- work. Will you be able to apply the BPTT and RTRL algorithms to  compute the gradients for this network? What test should you apply  to recurrent networks to determine if they can be simulated and  trained?  14  Figure E14.3  Dynamic Network for Exercise E14.7  E14.8 Consider the network in Figure E14.4.  1  1  1  1  1  3  1  2  1 1  2  5  Figure E14.4  Dynamic Network for Exercise E14.8  i. Write out the equations for computing the network response.  ii. Write out the BPTT equations for the network.  iii. Write out the RTRL equations for the network.  E14.9 Repeat E14.8 for the following networks.  14-49   D  14 Dynamic Networks  i.  ii.  3  3  1  1  1  1  2  1  2  2  5  2  5  E14.10 Consider the network in Figure E14.5.  4  3  2  5  1  3  1  1  1  2  3  4  Figure E14.5  Recurrent Network for Exercise E14.10  i. Define the network architecture, by showing   U X Im DIm 1  ,   ,   ,   ,   DLm l  ,   f Lm  ,   b ELW ,  Lm  U x  ELW  ,   X  u   .  ii. Select a simulation order and write out the equations needed to de-  termine the network response.  iii. Which   Su x  t    will need to be calculated  i.e., for which   u   and which   x   ?  iv. Write out Eq.  14.63  specifically for the term   xT  14.34  specifically for the term  to show exactly which terms are included.   a3 t   F a3 t      and Eq.   .  Expand the summation   14-50   Exercises  E14.11 Repeat E14.10 for the following networks, except, in part iv., change   a3   to   the indicated layer.  4  5  ,   a4  .  14  ii.  3  1  2  1  2  2  3  5  5  1  4  ,   a4  .  4  3  ,   a2  .  2  1  2  1  1  1  1  3  2  6  1  2  i.  3  iii.  3  3  2  5  3  2  5  3  4  14-51   D  4  5  ,   a2  .  14 Dynamic Networks  3  2  5  3  1  3  2  6  2  1  1  3  iv.  3  3  v.  6  5  ,   a1  .  2  1  2  3  2  2  6  4  2  5  3  E14.12 One of the advantages of the RTRL algorithm is that the gradient can be  computed at the same time as the network response, since it is computed  by starting at the first time point, and then working forward through time.  Therefore, RTRL allows a convenient framework for on-line implementa- tion. Suppose that you are implementing the RTRL algorithm, and you up- date the weights of the network at each time step.  i. Discuss the accuracy of the gradient calculation if the weights are   updated at each time step of a steepest descent algorithm.  ii. Implement the RTRL algorithm in MATLAB for the network in Fig- ure 14.4. Generate training data by using the network with the two  weights set equal to 0.5. Use the same input sequence shown in Fig- ure 14.10, and use the network responses   as the targets. Using  this data set, train the network using the steepest descent algo- rithm, with a learning rate of  . Set the initial weights to ze- ro. Update the weights at each time step. Save the gradient at the  end of the eighth time step.  a t   0.1    =  » 2 + 2 ans =       4  14-52   Exercises  iii. Repeat part ii., but do not update the weights. Compute the gradi- ent at the end of the eighth time step. Compare this gradient with  the one you obtained in part ii. Explain any differences  E14.13 Consider again the recurrent network of Figure 14.4. Assume that   F x    is   the sum squared error for the first two time points.  i. Let   a 0   0=  . Find   a 1    and   a 2    as a functions of   p 1  p 2   ,   , and   the weights in the network.  ii. Find the sum squared error for the first two time steps as an explicit   function of   p 1  p 2   ,   t 2   , and the network weights.  14  iii. Using part  ii , find   t 1  F  ,  ,  ----------------------- lw1 1 1   .  iv. Compare the results of part  iii  with the results determined by   RTRL on page 14-22 and by BPTT on page 14-30.  E14.14 In the process of deriving the RTRL algorithm in Exercise E14.5, you   should have produced the following expression  ea3 t    -------------------------------------------- T vec LW2 1 1    =    a1 t 1–    T    S3 2  t   If  a1 1   =   and   S3 2 2   =  1 1–  2 3 4 5–  ,  Find   ea3 2    -------------------------------------------- T vec LW2 1 1     and indicate   3 2  2 1 1   ---------------------------------------- T  vec lw1 2  ea1   .  E14.15 Each layer of a standard LDDN network has a summing junction, which  combines contributions from inputs, other layers, and the bias, as in Eq.   14.1 , which is repeated here:  nm t   =   f Lm  l      d DLm l  LWm l d al t  d–    +  IWm l d pl t d–    +  bm  .   Im  l      d DIm l  If, instead of summing, the net input was computed as a product of the con- tributions, how would the RTRL and BPTT algorithms change?  E14.16 As discussed in Exercise E14.15, the contribution to the net input from oth- er layers is computed as product of a layer matrix with a layer output, as in  LWm l d al t  d–    .  14-53   14 Dynamic Networks  If, instead of multiplying the layer matrix times the layer output, we were  to compute the distance between each row of the weight matrix and the lay- er output, as in  How would the RTRL and BPTT algorithms change?  ni  –=  wi  a–  .  D  E14.17 Find and compare the computational complexity for the BPTT and RTRL  algorithms applied to the sample network in Figure E14.6 as a function of  the number of neurons in Layer 2    , the number of delays in the tapped  delay line      and the length of the training sequence    S2   .  Q  D  3  1  2  D  2  S2  Figure E14.6  Recurrent Network for Exercise E14.17  E14.18 Consider again the network of Figure 14.4. Let the input weight of the net-  work   iw1 1  1=  . Assume that the initial network output is   a 0   0=  .   i. Write the network output at time   t   as a function only of the layer   weight  lw1 1 1  polynomial in   , and the input sequence.  The result should be a  lw1 1 1   .   ii. Find the network output at time   t  8=  , using   lw1 1 1   1.4–=   and   the following input sequence:  » 2 + 2 ans =       4  p t   =     3 1 1 6 3 5 1 6                .  iii. With   lw1 1 1   1.4–=  , the network should be unstable, since this   weight in the feedback loop is greater than one in magnitude. The  output would generally grow with time for an unstable network.   This applies to linear networks.  In ii., you should have found a  small value for  the roots of the polynomial you found in part i. You can use the  MATLAB command roots.  How might this result be related to the  spurious valleys in the error surface discussed on page 14-32?  . Can you explain this result?  Hint: Investigate   a 8   14-54   Objectives  15 Associative Learning  Objectives  Objectives Theory and Examples  Simple Associative Network Unsupervised Hebb Rule Hebb Rule with Decay  Simple Recognition Network Instar Rule  Kohonen Rule  Simple Recall Network Outstar Rule  Summary of Results Solved Problems Epilogue Further Reading Exercises  15-1 15-2 15-3 15-5 15-7 15-9 15-11 15-15 15-16 15-17 15-21 15-23 15-34 15-35 15-37  15  The neural networks we have discussed so far  in Chapters 4, 7, 10–14   have all been trained in a supervised manner. Each network required a tar- get signal to define correct network behavior.  In contrast, this chapter introduces a collection of simple rules that allow  unsupervised learning. These rules give networks the ability to learn asso- ciations between patterns that occur together frequently. Once learned, as- sociations allow networks to perform useful tasks such as pattern  recognition and recall.  Despite the simplicity of the rules in this chapter, they will form the foun- dation for powerful networks in Chapters 16, 18, 19.  15-1   15 Associative Learning  Theory and Examples  This chapter is all about associations: how associations can be represented  by a network, how a network can learn new associations.  What is an association? An association is any link between a system’s input  and output such that when a pattern A is presented to the system it will  respond with pattern B. When two patterns are linked by an association,  the input pattern is often referred to as the stimulus. Likewise, the output  pattern is referred to as the response.  Stimulus Response  Associations are so fundamental that they formed the foundation of the be- haviorist school of psychology. This branch of psychology attempted to ex- plain much of animal and human behavior by using associations and rules  for learning associations.  This approach has since been largely discredit- ed.   One of the earliest influences on the behaviorist school of psychology was  the classic experiment of Ivan Pavlov, in which he trained a dog to salivate  at the sound of a bell, by ringing the bell whenever food was presented. This  is an example of what is now called classical conditioning. B. F. Skinner  was one of the most influential proponents of the behaviorist school. His  classic experiment involved training a rat to press a bar in order to obtain  a food pellet. This is an example of instrumental conditioning.  It was to provide a biological explanation for some of this behavior that led  Donald Hebb to his postulate, previously quoted in Chapter 7 [Hebb49]:  “When an axon of cell A is near enough to excite a cell B and repeatedly or  persistently takes part in firing it, some growth process or metabolic change  takes place in one or both cells such that A’s efficiency, as one of the cells fir- ing B, is increased.”  In Chapter 7 we analyzed the performance of a supervised learning rule  based on Hebb’s postulate. In this chapter we will discuss unsupervised  forms of Hebbian learning, as well as other related associative learning  rules.  A number of researchers have contributed to the development of associa- tive learning. In particular, Tuevo Kohonen, James Anderson and Stephen  Grossberg have been very influential. Anderson and Kohonen indepen- dently developed the linear associator network in the late 1960s and early  1970s  [Ande72], [Koho72] . Grossberg introduced nonlinear continuous- time associative networks during the same time period  e.g., [Gross68] . All  of these researchers, in addition to many others, have continued the devel- opment of associative learning up to the present time.  In this chapter we will discuss the elemental associative learning rules.  Then, in Chapters 14–16 we will present more complex networks that use   15-2   Simple Associative Network  associative learning as a primary component. Chapter 16 will describe Ko- honen networks, and Chapters 18 and 19 will discuss Grossberg networks.  Simple Associative Network  Let’s take a look at the simplest network capable of implementing an asso- ciation. An example is the single-input hard limit neuron shown in Figure  15.1.  Inputs  p  Hard Limit Neuron  cid:0  cid:0  n a  cid:0  cid:0    cid:0  cid:0 Σ w  cid:0  cid:0   b = -0.5  1  a = hardlim  wp + b   15  Figure 15.1  Single-Input Hard Limit Associator  The neuron’s output   a   is determined from its input   p   according to  a  =  hardlim wp    b+    =  hardlim wp    –  0.5    .   15.1   For simplicity, we will restrict the value of   to be either 0 or 1, indicating  whether a stimulus is absent or present. Note that   is limited to the same  values by the hard limit transfer function. It indicates the presence or ab- sence of the network’s response.  a  p  p  =       1  stimulus  0  no stimulus            a  =       1  response  0  no response   15.2   The presence of an association between the stimulus  = 1, and the re- sponse  = 1 is dictated by the value of  stimulus only if   . The network will respond to the     in this case 0.5 .   is greater than   w  b–  w  p  a  The learning rules discussed in this chapter are normally used in the  framework of a larger network, such as the competitive networks of Chap- ters 16, 18 and 19. In order to demonstrate the operation of the associative  learning rules, without using complex networks, we will use simple net- works that have two types of inputs.  Unconditioned Stimulus  Conditioned Stimulus  One set of inputs will represent the unconditioned stimulus. This is analo- gous to the food presented to the dog in Pavlov’s experiment. Another set  of inputs will represent the conditioned stimulus. This is analogous to the  bell in Pavlov’s experiment. Initially the dog salivates only when food is   15-3   15 Associative Learning  2 2+  presented. This is an innate characteristic that does not have to be learned.  However, when the bell is repeatedly paired with the food, the dog is con- ditioned to salivate at the sound of the bell, even when no food is present.  We will represent the unconditioned stimulus as  stimulus simply as  associated with  justed according to the relevant learning rule.   and the conditioned  . For our purposes we will assume that the weights   are ad-  p  are fixed, but that the weights associated with   p0  p0  p  Figure 15.2 shows a network for recognizing bananas. The network has  both an unconditioned stimulus  banana shape  and a conditioned stimulus   banana smell . We don’t mean to imply here that smell is more condition- able than sight. In our examples in this chapter the choices of conditioned  and unconditioned stimuli are arbitrary and are used simply to demon- strate the performance of the learning rules. We will use this network to  demonstrate the operation of the Hebb rule in the following section.  Inputs  Hard Limit Neuron  Sight of banana    p0  Smell of banana    p   cid:0  cid:0 Σ w0 = 1 n  cid:0  cid:0  w = 0 b = -0.5   cid:0  cid:0   cid:0  cid:0   a    Banana?  1  a = hardlim  w0p0 + w p + b   Figure 15.2  Banana Associator  Fruit  Network  Banana?  The definitions of the unconditioned and conditioned inputs for this net- work are  Shape  Smell  p0  =       1  shape detected  0  shape not detected       p  =       1  smell detected  0  smell not detected  .   15.3   At this time we would like the network to associate the shape of a banana,  but the not the smell, with a response indicating the fruit is a banana. The  problem is solved by assigning a value greater than   and assigning  . The following values satisfy these requirements: a value less than    to    to   w0  b–  b–  w  The banana associator’s input output function now simplifies to  w0  1=  ,   w  0=  .   15.4   15-4   15  Unsupervised Hebb Rule  Unsupervised Hebb Rule  Thus, the network will only respond if a banana is sighted   er a banana is smelled      or not    1=  0=   .   p  p  a  =  hardlim p0    –  0.5    .   15.5   p0  1=   , wheth-  We will use this network in later sections to illustrate the performance of  several associative learning rules.  For simple problems it is not difficult to design a network with a fixed set  of associations. On the other hand, a more useful network would be able to  learn associations.  When should an association be learned? It is generally accepted that both  animals and humans tend to associate things that occur simultaneously.  To paraphrase Hebb: if a banana smell stimulus occurs simultaneously  with a banana concept response  activated by some other stimulus such as  the sight of a banana shape , the network should strengthen the connection  between them so that later it can activate its banana concept in response  to the banana smell alone.  The unsupervised Hebb rule does just that by increasing the weight  tween a neuron’s input   wij  in proportion to their product:   and output    be-  pj wij q   ai  ai q pj q  +  .  =  wij q  1–   15.6    See also Eq.  7.5 .  The learning rate   dictates how many times a stimu- lus and response must occur together before an association is made. In the  network in Figure 15.2, an association will be made when  since then  of    will produce the response   , regardless of the value   b–  1=  1=  0.5  p0    w  ,   =  a  p  .  Note that Eq.  15.6  uses only signals available within the layer containing  the weights being updated. Rules that satisfy this condition are called local  learning rules. This is in contrast to the backpropagation rule, for example,  in which the sensitivity must be propagated back from the final layer. The  rules introduced in this chapter will all be local learning rules.  The unsupervised Hebb rule can also be written in vector form:  W q  W q  1–  =   a q pT q  +  .   15.7   Local Learning  Training Sequence  As with all unsupervised rules, learning is performed in response to a se- ries of inputs presented in time  the training sequence :  p 1  p 2    p Q          .   15.8   15-5   15 Associative Learning   Note that we are using the notation  , in order to empha- size the time-sequence nature of the inputs.  At each iteration, the output   are up- a dated with the Hebb rule.   is calculated in response to the input   , and then the weights   , instead of   p q   pq  W  p  2 2+  Let’s apply the unsupervised Hebb rule to the banana associator. The asso- ciator will start with the weight values determined in our previous exam- ple, so that it will initially respond to the sight, but not the smell, of a  banana.  w0  =  1 w 0    =  0   15.9   The associator will be repeatedly exposed to a banana. However, while the  network’s smell sensor will work reliably, the shape sensor will operate  only intermittently  on even time steps . Thus the training sequence will  consist of repetitions of the following two sets of inputs:    p0 1  w0  0=    p 1   1=        p0 2   1=    p 2   1=      .   15.10   The first weight  lus  ing the unsupervised Hebb rule with a learning rate of 1:  , representing the weight for the unconditioned stimu-  will be updated at each iteration, us-  , will remain constant, while   p0  w  w q   =  w q  1–    +  a q p q   .  The output for the first iteration    q  1=    is  a 1   = =  hardlim w0p0 1  w 0 p 1  hardlim 1 0 =  + 0 1  0.5     +  –      0.5  – 0     no response  .  The smell alone did not generate a response. Without a response, the Hebb  rule does not alter   w  .  w 1   =  w 0   +  a 1 p 1   =  0  0 1+  =  0   15.13   In the second iteration, both the banana’s shape and smell are detected and  the network responds accordingly:  a 2   = =  hardlim w0p0 2  w 1 p 2  hardlim 1 1 =  + 0 1  0.5     +  –      0.5  – 1     banana  .   15.14   Because the smell stimulus and the response have occurred simultaneous- ly, the Hebb rule increases the weight between them.  w 2   =  w 1   +  a 2 p 2   =  0  1 1+  =  1   15.15   When the sight detector fails again, in the third iteration, the network re- sponds anyway. It has made a useful association between the smell of a ba- nana and its response.   15.11    15.12   15-6   15  Unsupervised Hebb Rule  a 3   = =  hardlim w0p0 3  w 2 p 3  hardlim 1 0 =  + 1 1  0.5     +  –      0.5  – 1     banana   w 3   =  w 2   +  a 3 p 3   =  1  1 1+  =  2   15.16    15.17   From now on, the network is capable of responding to bananas that are de- tected either by sight or smell. Even if both detection systems suffer inter- mittent faults, the network will be correct most of the time.  To experiment with the unsupervised Hebb rule, use the Neural Network De- sign Demonstration Unsupervised Hebb Rule  nnd13uh .  We have seen that the unsupervised Hebb rule can learn useful associa- tions. However, the Hebb rule, as defined in Eq.  15.6 , has some practical  shortcomings. The first problem becomes evident if we continue to present  inputs and update   will become ar- bitrarily large. This is at odds with the biological systems that inspired the  Hebb rule. Synapses cannot grow without bound.   in the example above. The weight   w  w  The second problem is that there is no mechanism for weights to decrease.  If the inputs or outputs of a Hebb network experience any noise, every  weight will grow  however slowly  until the network responds to any stim- ulus.  Hebb Rule with Decay One way to improve the Hebb rule is by adding a weight decay term  Eq.   7.45  ,  W q  W q  1–  =   a q pT q  +  –  W q  1–    =  – 1  W q  1–   a q pT q  , +   15.18     , the decay rate, is a positive constant less than one. As    approach- where  es zero, the learning law becomes the standard rule. As   approaches one,  the learning law quickly forgets old inputs and remembers only the most  recent patterns. This keeps the weight matrix from growing without  bound.  The idea of filtering the weight changes was also discussed in  Chapter 12, where we called it momentum.       . This value is found  The maximum weight value  by setting both    to maximize learning  in  the scalar version of Eq.  15.18  and solving for the steady state weight  i.e.  when both new and old weights are equal .   to a value of 1 for all    is determined by    and   wij  pj  ai  q    max  15-7  Decay Rate   15 Associative Learning  wij  = wij  – 1 1 – =  wij aipj  +  wij +  ---=  wij   15.19   2 2+  Let’s examine the operation of the Hebb rule with decay on our previous ba- nana associator problem. We will use a decay rate   of 0.1. The first itera- tion, where only the smell stimulus is presented, is the same:    a 1   =   0     no response       w  1   0=  .   15.20   The next iteration also produces identical results. Here both stimuli are  presented, and the network responds to the shape. Coincidence of the smell  stimulus and response create a new association:  a 2   =   1     banana       w 2   1=  .   15.21   The results of the third iteration are not the same. The network has  learned to respond to the smell, and the weight continues to increase. How- ever, this time the weight increases by only 0.9, instead of 1.0.  w 3   =  w 2   +  a 3 p 3   –  0.1w 2   =  1  +  1 1  –  0.1 1  =  1.9   15.22   The decay term limits the weight’s value, so that no matter how often the  association is reinforced,    will never increase beyond   max  w  .  wij  max  wij  =   ---  =  1 ------- 0.1  =  10   15.23   The new rule also ensures that associations learned by the network will not  be artifacts of noise. Any small random increases will soon decay away.  Figure 15.3 displays the response of the Hebb rule, with and without decay,  for the banana recognition example. Without decay, the weight continues  to increase by the same amount each time the neuron is activated. When  decay is added, the weight exponentially approaches its maximum value    wij To experiment with the Hebb rule with decay, use the Neural Network De- sign Demonstrations Hebb with Decay nnd13hd  and Effect of Decay Rate   nnd13edr .  10=  max   .  15-8   Simple Recognition Network  10  8  6  4  2  w  30  20  10  0 0  Hebb Rule  Hebb with Decay  15  10  20  q  30  0 0  10  20  30  q  Figure 15.3  Response of the Hebb Rule, With and Without Decay  The Hebb rule with decay does solve the problem of large weights. Howev- er, it does so at a price. The environment must be counted on to occasionally  present all stimuli that have associations. Without reinforcement, associa- tions will decay away.  To illustrate this fact, consider Eq.  15.18  if   ai  0=  :  If     =  0.1  , this reduces to  wij q   =  – 1  wij q  1–    .  wij q   =  0.9  wij q  1–    .   15.24    15.25   wij   will be decreased by 10% at each presentation for which   Therefore  ai We will discuss a solution to this problem in a later section.  . Any association that was previously learned will eventually be lost.   0=  Simple Recognition Network  Instar  So far we have considered only associations between scalar inputs and out- puts. We will now examine a neuron that has a vector input.  See Figure  15.4.  This neuron, which is sometimes referred to as an instar, is the sim- plest network that is capable of pattern recognition, as we will demonstrate  shortly.   15-9   15 Associative Learning  Inputs  Hard Limit Neuron  p1  p2  pR  w1,1 w1,2  w1,R   cid:0  cid:0 Σ  cid:0  cid:0   n  a  b  1  a = hardlim  Wp + b   Figure 15.4  Instar  You will notice the similarities between the instar of Figure 15.4 and the  perceptron of Figure 4.2  also the ADALINE of Figure 10.2 and the linear  associator of Figure 7.1 . We give these networks different names, in part  for historical reasons  since they arose at different times and out of differ- ent environments , and because they perform different functions and are  analyzed in different ways. For example, we will not directly consider the  decision boundary of the instar, although this was an important concept for  the perceptron. Instead, we will analyze the ability of the instar to recog- nize a pattern, as with the neurons in the first layer of the Hamming net- work.  See page 3-10.   The input output expression for the instar is  a  =  hardlim Wp b+      =  hardlim wT    p b+    .  1   15.26   The instar will be active whenever the inner product between the weight  vector  row of the weight matrix  and the input is greater than or equal to  b–  :  wT 1  p  b–  .   15.27   From our discussion of the Hamming network on page 3-10, we know that  for two vectors of constant length, the inner product will be largest when  they point in the same direction. We can also show this using Eq.  5.15 :  wT 1  p  =  w1  p  cos  b–  ,   15.28    is the angle between the two vectors. Clearly the inner product is     where  maximized when the angle     w1=  p   , then the inner product will be largest when   p  w1=  .     is 0. If   p   and   w1   have the same length   Based on these arguments, the instar of Figure 15.4 will be active when    p  appropriately, we can select how close  is “close” to  the input vector must be to the weight vector in order to activate the instar.  . By setting the bias   w1  b  15-10   Instar Rule  If we set  b  –=  w1  p  ,   15.29   then the instar will only be active when  tion as  pattern    points in exactly the same direc-  . Thus, we will have a neuron that recognizes only the   0=  p       w1 . w1  If we would like the instar to respond to any pattern near  then we can increase  p value of  making it the less discriminatory.  w1 . The larger the  , the more patterns there will be that can activate the instar, thus    to some value larger than    small ,   w1        –  b  b  We should note that this analysis assumes that all input vectors have the  same length  norm . We will revisit the question of normalization in Chap- ters 16, 18 and 19.  15  We can now design a vector recognition network if we know which vector  we want to recognize. However, if the network is to learn a vector without  supervision, we need a new rule, since neither version of the Hebb rule pro- duces normalized weights.  Instar Rule  One problem of the Hebb rule with decay was that it required stimuli to be  repeated or associations would be lost. A better rule might allow weight de- cay only when the instar is active    . Weight values would still be lim- ited, but forgetting would be minimized. Consider again the original Hebb  rule:  0  a  wij q   =  wij q 1–   ai q pj q  +  .   15.30   To get the benefits of weight decay, while limiting the forgetting problem,  a decay term can be added that is proportional to   :  wij q   =  wij q  1–   ai q pj q  +  –   15.31   We can simplify Eq.  15.31  by setting  are learned at the same rate old values decay  and gathering terms.    so new weight values    equal to       ai q  ai q wij q  1–    wij q   =  wij q  1–   ai q  pj q  wij q +  1–  –         15.32   Instar Rule  This equation, called the instar rule, can also be rewritten in vector form:  w q   i  =  i  w q  1–    +  ai q  p q  w q  i–  1–        .   15.33   The performance of the instar rule can be best understood if we consider  the case where the instar is active    . Eq.  15.33  can then be written  1=  ai  15-11   15 Associative Learning  w q   i  w q  1–    +   p q  w q  1–        i–  i  =  =  1 –   w q i  1–    +  p q  .   15.34   This operation is displayed graphically in Figure 15.5.  p q   iw q   iw q - 1   Figure 15.5  Graphical Representation of the Instar Rule  When the instar is active, the weight vector is moved toward the input vec- tor along a line between the old weight vector and the input vector. The dis- tance the weight vector moves depends on the value of  , the  new weight vector is equal to the old weight vector  no movement . When  , the new weight vector is equal to the input vector  maximum move-  ment . If  , the new weight vector will be halfway between the old  weight vector and the input vector.  . When   1=  0=  0.5        =  One useful feature of the instar rule is that if the input vectors are normal- ized, then   will also be normalized once it has learned a particular vector  . We have found a rule that not only minimizes forgetting, but results in  p normalized weight vectors, if the input vectors are normalized.  wi  Let’s apply the instar rule to the network in Figure 15.6. It has two inputs:  one indicating whether a fruit has been visually identified as an orange   unconditioned stimulus  and another consisting of the three measure- ments taken of the fruit  conditioned stimulus .  The output of this network is  a  =  hardlim w0p0 Wp b  +  +      .   15.35   2 2+  Fruit  Network  Orange?  Sight  Measure  The elements of input  Chapter 3  Eq.  3.2  . This constraint ensures that  with a length of   . The definitions of   p0  p  p  =  3   will be constrained to ±1 values, as defined in   p   is a normalized vector    and   p   are  p0  =       1    orange detected visually  0    orange not detected         p  =  shape texture weight  .   15.36   15-12   Instar Rule  The bias   15.29 .   b   is   2–  , a value slightly more positive than   p–  3–=  .  See Eq.   2  Inputs  Hard Limit Neuron  Sight of orange   p0  w0  =  3  Measured shape   p1 Measured texture   p2 Measured weight   p3  w1,1   cid:0  cid:0 Σ  cid:0  cid:0   n  b  =  -2  a   Orange?  w1,3  1  a = hardlim  w0 p0 + W p + b   Figure 15.6  Orange Recognizer  15  We would like the network to have a constant association between the sight  of an orange and its response, so  tially, the network should not respond to any combination of fruit measure- ments, so the measurement weights will start with values of 0.   will be set greater than   . But ini-  w0  b–  w0  3=  ,     W 0   =  wT 1  0   =  0 0 0   15.37   The measurement weights will be updated with the instar rule, using a  learning rate of   1=    .  w q   1  =  w1  1– q    +  a q  p q  w1  –    1– q       15.38   The training sequence will consist of repeated presentations of an orange.  The measurements will be given every time. However, in order to demon- strate the operation of the instar rule, we will assume that the visual sys- tem only operates correctly on even time steps, due to a fault in its  construction.  p0 1   0=    p 1   =  p0 2   1=    p 2   =     15.39         1 1– 1–               1 1– 1–         Because  measurements of an orange in the first iteration.   initially contains all zeros, the instar does not respond to the   W  15-13   15 Associative Learning  =             a 1   hardlim w0p0 1  Wp 1   +    2–    a 1   =  hardlim 3 0  +  0 0 0  =  0     no response    15.40   1 1– 1–     2–    Since the neuron did not respond, its weights  star rule.  w1   are not altered by the in-     w 1   1  =  w 0   +  1  a 1  p 1  w 0       1–   15.41   =  +  0  –  =  0 0 0        1 1– 1–        0 0 0  0 0 0  However, the neuron does respond when the orange is identified visually,  in addition to being measured, in the second iteration.  a 2   =  hardlim w0p0 2  Wp 2   +    2–     15.42   =  hardlim 3 1  +  0 0 0  =  1     orange   1 1– 1–     2–    The result is that the neuron learns to associate the orange’s measurement  vector with its response. The weight vector   becomes a copy of the orange  measurement vector.  w1     w 2   1  =  w 1   +  1  a 2  p 2  w 1       1–   15.43   The network can now recognize the orange by its measurements. The neu- ron responds in the third iteration, even though the visual detection system  failed again.  =  +  1  –  =  0 0 0        1 1– 1–        0 0 0  1 1– 1–  15-14   Instar Rule  a 3   =  2–      +  hardlim w0p0 3  Wp 3        2–      1 1– 1–  1 1–  1–  +  a 3   =  hardlim 3 0  =  1     orange    15.44   Having completely learned the measurements, the weights stop changing.   A lower learning rate would have required more iterations.      w 3   1  =  w 2   +  1  a 3  p 3  w 2       1–   15.45   =  +  1  –  =  1 1– 1–        1 1– 1–  1 1– 1–        1 1– 1–  15  The network has learned to recognize an orange by its measurements, even  when its visual detection system fails.  To experiment with the instar rule, use the Neural Network Design Demon- strations Instar  nnd13is  and Graphical Instar  nnd13gis .  Kohonen Rule At this point it is appropriate to introduce another associative learning  rule, which is related to the instar rule. It is the Kohonen rule:  Kohonen Rule  w q   =  i  w q i  1–    +   p q  w q  1–             for i    X q   .  i–   15.46   Like the instar rule, the Kohonen rule allows the weights of a neuron to  learn an input vector and is therefore suitable for recognition applications.  Unlike the instar rule, learning is not proportional to the neuron’s output   is a member of  ai q  the set   . Instead, learning occurs when the neuron’s index   X q   .  i  If the instar rule is applied to a layer of neurons whose transfer function  only returns values of 0 or 1  such as hardlim , then the Kohonen rule can  be made equivalent to the instar rule by defining  such that  . The advantage of the Kohonen rule is that it can also  be used with other definitions. It is useful for training networks such as the  self-organizing feature map, which will be introduced in Chapter 16.   as the set of all   ai q   X q   1=     i  15-15   15 Associative Learning  Simple Recall Network  Outstar  We have seen that the instar network  with a vector input and a scalar out- put  can perform pattern recognition by associating a particular vector  stimulus with a response. The outstar network, shown in Figure 15.7, has  a scalar input and a vector output. It can perform pattern recall by associ- ating a stimulus with a vector response.  The input-output expression for this network is  a  =  satlins Wp      .   15.47   The symmetric saturating function  work will be used to recall a vector containing values of -1 or 1.   was chosen because this net-  satlins  Symmetric Saturating  Linear Layer  Input  p  a1   a2   aS  w1,1  n1 cid:0  cid:0 Σ  cid:0   cid:0   cid:0  cid:0 Σ w2,1 n2  cid:0   cid:0  cid:0  nS  cid:0  cid:0 Σ  cid:0  wS,1  cid:0   cid:0  cid:0  a = satlins  Wp   Figure 15.7  Outstar Network  If we would like the network to associate a stimulus  an input of 1  with a  particular output vector    which contains only a  single column vector  equal to    is 1, the output will be   , we can simply set   . Then, if   a  a  a  W  p  :  a  =  satlins Wp      =    =  a  .    satlins a 1 a   are less than or equal to 1 in mag-   15.48    This assumes that the elements of  nitude.   Note that we have created a recall network by setting a column of the  weight matrix to the desired vector. Earlier we designed a recognition net- work by setting a row of the weight matrix to the desired vector. a  We can now design a network that can recall a known vector  need a learning rule if the network is to learn a vector without supervision.  We will describe such a learning rule in the next section.  , but we   15-16   Outstar Rule  Outstar Rule  To derive the instar rule, forgetting was limited by making the weight de- cay term of the Hebb rule proportional to the output of the network,  Conversely, to obtain the outstar learning rule, we make the weight decay  term proportional to the input of the network,   ai  .   :  pj  wij q   =  wij q  1–   ai q pj q  +  –  pj q wij q  1–    .   15.49   If we set the decay rate  we get     equal to the learning rate      and collect terms,   wij q   =  wij q 1–    ai q  wij q +  1–  –      pj q   .   15.50   The outstar rule has properties complimentary to the instar rule. Learning  occurs whenever  umn    is nonzero  instead of   moves toward the output vector.   . When learning occurs, col-  ai  pj  wj  15  Outstar Rule  As with the instar rule, the outstar rule can be written in vector form    q   wj  =    q 1–  wj    a q  wj +  –      q 1–    pj q   ,   15.51   where   wj   is the jth column of the matrix   W  .  2 2+  To test the outstar rule we will train the network shown in Figure 15.8.   Inputs  0  Measured shape  p1 Measured texture   p2 Measured weight   p3  0  0  w1,1 0 w2,2 0 w3,3 0  = 1  = 1  = 1  Identified Pineapple   p  w1,1  w3,1  Symmetric Saturating  Linear Layer  cid:0   cid:0   cid:0   cid:0   cid:0   cid:0    cid:0  cid:0 Σ n1  cid:0  cid:0   cid:0  cid:0 Σ n2  cid:0  cid:0   cid:0  cid:0 Σ n3  cid:0  cid:0  a = satlins  W0p0 + Wp   a1   Recalled shape  a2   Recalled texture  a3   Recalled weight  Figure 15.8  Pineapple Recaller  15-17   Sight  Measure  Fruit  Network  Measurements?  15 Associative Learning  The outputs of the network are calculated as follows:  a  =  satlins W0p0 Wp  +      ,   15.52   where  W0  =  1 0 0 0 1 0 0 0 1  .   15.53   The network’s two inputs provide it with measurements   unconditioned stimulus , as well as a signal  p been identified visually  conditioned stimulus .   taken on a fruit   indicating a pineapple has   p0  p0  =  shape texture weight         p  =       1    if a pineapple can be seen  0    otherwise   15.54   The network’s output is to reflect the measurements of the fruit currently  being examined, using whatever inputs are available. W0  The weight matrix for the unconditioned stimulus,  ty matrix, so that any set of measurements  ied to the output  set to zero initially, so that a 1 on  be updated with the outstar rule using a learning rate of 1:  . The weight matrix for the conditioned stimulus,  W  , is set to the identi-   with ±1 values  will be cop- , is  W  will    will not generate a response.   p0  a  p    q   wj  =  wj    q 1–    +    a q  wj  –    q 1–    p q   .   15.55   The training sequence will consist of repeated presentations of the sight  and measurements of a pineapple. The pineapple measurements are  ppineapple  =  .  1– 1– 1   15.56   However, due to a fault in the measuring system, measured values will  only be available on even iterations.        p0 1   =    p 1   1=  p0 2   =    p 2   1=     15.57                1– 1– 1         0 0 0  15-18   15  Outstar Rule  In the first iteration, the pineapple is seen, but the measurements are un- available.  a 1   =  satlins W0p0 1  Wp 1   +      ,   15.58   0 0 0  0 0 0  a 1   =  satlins  +  =       no response    15.59         0 0 0     1    0 0 0  The network sees the pineapple but cannot output proper measurements,  because it has not learned them and the measurement system is not work- ing. The weights remain unchanged after being updated.  w1 1   =  w1 0   +    a 1  w1 0   –  p 1   =  +  –  1  =   15.60         0 0 0        0 0 0  0 0 0  In the second iteration the pineapple is seen, and the measurements are  taken properly.  a 2   =  satlins  +  =       measurements given    15.61         1– 1– 1     1    0 0 0  1– 1– 1  The measurements are available, so the network outputs them correctly.  The weights are then updated as follows:  w1 2   =  w1 1   +    a 2  w1 1   –  p 2   =  +  –  1  =   .  0 0 0        1– 1– 1        0 0 0  1– 1– 1  Since the sight of the pineapple and the measurements were both avail- able, the network forms an association between them. The weight matrix  is now a copy of the measurements, so they can be recalled later.   15.62   15-19   15 Associative Learning  In iteration three, measurements are unavailable once again, but the out- put is  a 3   =  satlins  +  =       measurements recalled   .   15.63         0 0 0  1– 1– 1     1    1– 1– 1  The network is now able to recall the measurements of the pineapple when  it sees it, even though the measurement system fails. From now on, the  weights will no longer change values unless a pineapple is seen with differ- ent measurements.  w1 3   =  w1 2   +    a 2  w1 2   –  p 2   =  +  –  1  =  1– 1– 1        1– 1– 1  1– 1– 1        1– 1– 1  To experiment with the outstar rule with decay, use the Neural Network De- sign Demonstration Outstar Rule  nnd13os .  In Chapter 19 we will investigate the ART networks, which use both the  instar and the outstar rules.   15.64   15-20   Summary of Results  Summary of Results  Association  An association is a link between the inputs and outputs of a network so  that when a stimulus A is presented to the network, it will output a re- sponse B.  Associative Learning Rules  Unsupervised Hebb Rule  W q  W q 1–  =   a q pT q  +  Hebb Rule with Decay  W q   =  – 1  W q  1–   a q pT q  +  Instar  15  Inputs  Hard Limit Neuron  p1  p2  pR  w1,1 w1,2  w1,R   cid:0  cid:0 Σ  cid:0  cid:0   n  a  b  1  a = hardlim  Wp + b   a  =  hardlim wT    p b+    1  The instar is activated for   wT 1  p  =  w1  p  cos  b–  ,  where      is the angle between   p   and   w1  .  Instar Rule  w q   i  =  i  w q  1–    +  ai q  p q  w q  i–  1–        w q  i  =  1 –  w q  1–   p q  +       if ai q     1=    15-21   15 Associative Learning  p q   iw q   iw q - 1   Graphical Representation of the Instar Rule    ai q   1=     =  wi  1– q    p q  wi +  –    1– q           for i    X q   Kohonen Rule w q   i  Outstar  Symmetric Saturating  Linear Layer  Input  p  a1   a2   aS  w1,1  n1 cid:0  cid:0 Σ  cid:0   cid:0   cid:0  cid:0 Σ w2,1 n2  cid:0   cid:0  cid:0  nS  cid:0  cid:0 Σ  cid:0  wS,1  cid:0   cid:0  cid:0  a = satlins  Wp   15-22  Outstar Rule  wj q    =  wj q  1–    a q  wj q +  1–  –      pj q    Solved Problems  Solved Problems  P15.1 In Eq.  15.19  the maximum weight for the Hebb rule with decay   was calculated, assuming that  Calculate the maximum weight resulting if  gether between values of 0 and 1.   and   ai  pj   were 1 at every time step.   alternate to-   and   ai  pj  We begin with the scalar version of the Hebb rule with decay:  wij q   =  – 1  wij q  1–   ai q pj q  +  .  We can rewrite this expression twice using  the weight is updated over two time steps.  q   to index the weight values as   wij q  1+    =  – 1  wij q  ai q pj q   +  wij q  2+    =  – 1  wij q  1+   ai q +  1+  pj q  1+    By substituting the first equation into the second, we get a single expres- sion showing how    is updated over two time steps.  wij  wij q  2+    =  – 1   1 –   wij q  ai q pj q   +   ai q +  1+  pj q  1+    At this point we can substitute values for  for a maximum weight, we will set  pj q  ai q step, and increases in the second, ensuring that  of the two weights. If we solve for   pj  and   1+  2+   and       wij q  wij q  2+ , we obtain   to 1. This will mean that the weight decreases in the first time   ai ai q   . Because we are looking   to 0, and   and   1+    pj q     is the maximum   wij q  2+    =  1 –  2wij q  +  .  Assuming that  wij it by setting both    will eventually reach a steady state value, we can find  wij q   and solving   equal to   wij q   2+   and   wij  max    15  max  wij  =  – 1  2wij  max +  ,  max  wij  =   ---------------- 2– 2  .  » 2 + 2 ans =       4  We can use MATLAB to make a plot of this relationship. The plot will show  learning rates and decay rates at intervals of 0.025.  Here are the commands for creating a mesh plot of the maximum weight,  as a function of the learning and decay rate values.  lr = 0:0.025:1; dr = 0.025:0.025:1;  15-23   15 Associative Learning  [LR,DR] = meshgrid dr,lr ; MW = LR .   DR .*  2 - DR  ; mesh DR,LR,MW ;  The plot shows that  small with respect to the learning rate   wij  max   approaches infinity as the decay rate      becomes       see Figure P15.1 .  t  i     h g e w m u m x a M  i  20  15  10  5  0 0        0.2  0.4  0.6  0.8  Decay Rate  0.4  0.2  1  0  Learning Rate  Figure P15.1  Maximum Weight   max  wij  1  0.8  0.6  P15.2 Retrain the orange recognition network on page 13-13 using the in-  star rule with a learning rate of 0.4. Use the same training se- quence. How many time steps are required for the network to  learn to recognize an orange by its measurements?  Here is the training sequence. It is to be repeated until the network can re- spond to the orange measurements    , even when the visual  detection system fails    1 1–  0=  p0  1–   .  p  =  T  p0 1   0=    p 1   =  p0 2   1=    p 2   =  1 1– 1–               1 1– 1–           » 2 + 2 ans =       4  We will use MATLAB to make the calculations. These two lines of code set  the weights to their initial values.  w0 = 3; W = [0 0 0];  15-24   We can then simulate the first time step of the network.  The neuron does not yet recognize the orange, so its output is 0. The  weights do not change when updated with the instar rule.  The neuron begins learning the measurements in the second iteration.  15  Solved Problems  p0 = 0; p = [1; -1; -1]; a = hardlim w0*p0+W*p-2  a =      0  W = W + 0.4*a* p’-W  W =      0 0 0  p0 = 1; p = [1; -1; -1]; a = hardlim w0*p0+W*p-2  a =      1  W = W + 0.4*a* p’-W  W =      0.4000 -0.4000 -0.4000  p0 = 0; p = [1; -1; -1]; a = hardlim w0*p0+W*p-2  a =      0  W = W + 0.4*a* p’-W  W =      0.4000 -0.4000 -0.4000  Here are the results of the fourth iteration:  a =      1 W =      0.6400 -0.6400 -0.6400  the fifth iteration:  15-25  But the association is still not strong enough for a response in the third it- eration.   15 Associative Learning  a =      0 W =      0.6400 -0.6400 -0.6400  and the sixth iteration:  a =      1 W =      0.7840 -0.7840 -0.7840 .  p0 = 0; p = [1; -1; -1]; a = hardlim w0*p0+W*p-2  a =      1  W = W + 0.4*a* p’-W  W =      0.8704 -0.8704 -0.8704  By the seventh iteration the network is able to recognize the orange by its  measurements alone.  Due to the lower learning rate, the network had to experience the measure- ments paired with its response three times  the even numbered iterations   before it developed a strong association between them.  P15.3 Both the recognition and recall networks used in this chapter’s ex- amples could only learn a single vector. Draw the diagram and de- termine the parameters of a network capable of recognizing and  responding to the following two vectors:  p1  =                p2  =  5 5– 5  .  5– 5 5  The network should only respond when an input vector is identi- cal to one of these vectors.  We know the network must have three inputs, because it must recognize  three-element vectors. We also know that it will have two outputs, one out- put for each response.  Such a network can be obtained by combining two instars into a single lay- er, as in Figure P15.2.  15-26   Solved Problems  Inputs  Hard Limit Layer  p1  p2  p3  w1,1   cid:0  cid:0 Σ  cid:0  cid:0   n1  a1  1  b1  cid:0  cid:0 Σ w2,3  cid:0  cid:0  b2  1   cid:0  cid:0  n2 a2  cid:0  cid:0   a = hardlim  Wp + b   15  Figure P15.2  Two-Vector Recognition Network  We now set the weights  , so that its net  input will be at a maximum when an input vector points in the same direc- tion as   so that the second neuron is most   to  . sensitive to vectors in the direction of    of the first neuron equal to   . Likewise, we will set   w2  w1  p1  p1  p2 p2  Combining the weight vectors gives us the weight matrix  W  =  =  =  wT 1 wT 2  T p1 T p2  5 5–  5– 5 5 5  .   Note that this is the same manner in which we determined the weight ma- trix for the first layer of the Hamming network. In fact, the first layer of  the Hamming network is a layer of instars. More about that in the next  chapter.   The lengths of   p1   and   p2   are the same:  p1  =  p2  =  5 2  +  5–  2  5 2  +  =  75  .  To ensure that only an exact match between an input vector and a stored  vector results in a response, both biases are set as follows  Eq.  15.29  :  » 2 + 2 ans =       4  We can use MATLAB to test that the network does indeed respond to   p1  .  b1  =  b2  =  –  p1  2  =  75–  .  W = [5 -5 5; -5 5 5]; b = [-75; -75];  15-27   15 Associative Learning  p1 = [5; -5; 5]; a = hardlim W*p1+b  a =      1      0  p3 = [-5; 5; -5]; a = hardlim W*p3+b  a =      0       0  The first neuron responded, indicating that the input vector was  second neuron did not respond, because the input was not   .  p1  . The   p2  We can also check that the network does not respond to a third vector  that is not equal to either of the stored vectors.  p3     Neither neuron recognizes this new vector, so they both output 0.  P15.4 A single instar is being used for pattern recognition. Its weights   and bias have the following values:  W wT  =  1  =  1 1–  1–              b  2–=  .  How close must an input vector  with a magnitude of    be to the  weight vector for the neuron to output a 1? Find a vector that oc- curs on the border between those vectors that are recognized and  those vectors that are not.  3  We begin by writing the expression for the neuron’s output.  According to the definition of  product between    and   p  wT 1  ,  hardlim a   will be 1 if and only if the inner    is greater than or equal to   b–    Eq.  15.28  :  a  =  hardlim wT    p b+    1  wT  p  1  =  w1  p  cos  b–  .  We can find the maximum angle between  w1 tion by substituting for the norms and solving   and   p   that meets this condi-  3    3    cos  2      cos 1– 2   ---  3  =  48.19  .  15-28   Solved Problems  To find a borderline vector with magnitude  meets the following conditions:  3  , we need a vector   p   that   p1  =  2 p1  +  2 p2  +  2 p3  =  3  ,  wT 1  p  =  w1p1 w2p2 w3p3  +  +  b–  =  p1 p2–  –  p3  2–  =  0  .  Since we have three variables and only two constraints, we can set the  third variable    to 0 and solve  p1  2 p1  +  2 p2  +  2 p3  =  3        2     p2  2+ p3  3=  ,  p1  p2–  –  p3  –  2            p2  p3+  2–=  ,    p2  p3+  2  =  2 p2  +  2 p3  +  2p2p3  =  2–  2  =  4  ,  3  +  2p2p3  =  4            p2p3  0.5=  ,  15  p2 p2    p3+    =  2 p2  +  p2p3  =  2 p2  0.5+  =  p2  2–    =  –  2p2  .  After a little work we find that there are two possible values for   p2  :  It turns out that if we pick  on the other value.  p2   to be one of these values, then   p3   will take   2 p2  +  2p2  +  0.5  0=  ,  p2  =  1–    0.5  .  p2  p3+  =  1–    0.5  p3+  =  2–  p3  =  1–    0.5  Therefore, the following vector  ognized.  p   is just the right distance from   w   to be rec-  p  =  0 + –  1– 1–  0.5 0.5  We can test it by presenting it to the network.  15-29   15 Associative Learning  a  =  hardlim wT    p b+    1  a  =  hardlim 1 1–  1–        0 + –  1– 1–  0.5 0.5     2–    a  =  hardlim 0   =  1  The vector  boundary of the instar’s active region.  p   does indeed result in a net input of 0 and is therefore on the   P15.5 Consider the instar network shown in Figure P15.3. The training   sequence for this network will consist of the following inputs:      p0 1   0=    p 1   =  p0 2   1=    p 2   =  1– 1           1– 1         .  These two sets of inputs are repeatedly presented to the network  until the weight matrix    converges.   W  i. Perform the first four iterations of the instar rule, with   =  0.5  . Assume that the initial   W   matrix is set   learning rate  to all zeros.    ii. Display the results of each iteration of the instar rule in   graphical form  as in Figure 15.5 .  Inputs  Hard Limit Neuron  p0  p1  p2  w0  =  3  w1,1   cid:0  cid:0 Σ  cid:0  cid:0   n  a  b  =  -2  w1,2  1  a = hardlim  w0 p0  + W p + b   15-30  Figure P15.3  Instar Network for Problem P15.5  i. Because  the measurements in the first iteration.  W   initially contains all zeros, the instar does not respond to    Solved Problems  a 1   =  hardlim w0p0 1  Wp 1   +    a 1   =  hardlim 3 0  +  0 0    2–   2–   =  0  1– 1          The neuron did not respond. Therefore its weights  instar rule.  w1   are not altered by the   1    w1  =  w1  0   +  0.5a 1  p 1  w1  –    0     =  +  0  0 0      1– 1  –      0 0  =  0 0  15  Because the unconditioned stimulus appears on the second iteration, the  instar does respond.  The neuron did respond, and its weights    are updated by the instar rule.  a 2   =  hardlim w0p0 2  Wp 2   +    a 2   =  hardlim 3 1  +    2–   2–   =  1  1– 1  0 0  w1  2    w1  =  w1  1   +  0.5a 2  p 2  w1  –    1     =  +  0.5  0 0      1– 1  –      0 0  =  0.5– 0.5  On the third iteration, the unconditioned stimulus is not presented, and  the weights have not yet converged close enough to the input pattern.  Therefore, the instar does not respond.  a 3   =  hardlim w0p0 3  Wp 3   +    a 3   =  hardlim 3 0  +  0.5–  0.5       2–   2–   1– 1  =  0  Since the neuron did not respond, its weights are not updated.  15-31   15 Associative Learning  3    w1  =  w1  2   +  0.5a 3  p 3  w1  –    2     =  0.5– 0.5  +  0      1– 1  –  0.5– 0.5      =  0.5– 0.5  Because the unconditioned stimulus again appears on the fourth iteration,  the instar does respond.  a 4   =  hardlim w0p0 4  Wp 4   +    a 4   =  hardlim 3 1  +  0.5–  0.5       2–   2–   1– 1  =  1  Since the instar was activated, its weights are updated.  4    w1  =  w1  3   +  0.5a 4  p 4  w1  –    3     =  0.5– 0.5  +  0.5      1– 1  –  0.5– 0.5      =  –  0.75 0.75  This completes the fourth iteration. If we continue this process,  converge to   p  .  w1   will   ii.  Note that the weights are only updated  instar active  on iterations 2  and 4. Recall from Eq.  15.34  that when the instar is active, the learning  rule can be written  w1  q   =  w1  1– q    p q  w1 +  –    1– q      =  1 –   w1  1– q   p q  +  .  When the instar is active, the weight vector is moved toward the input vec- tor along a line between the old weight vector and the input vector. Figure  P15.4 displays the movement of the weight vector for this problem. The  weights were updated on iterations 2 and 4. Because  , whenever  the instar is active the weight vector moves halfway from its current posi- tion toward the input vector.  0.5    =  w1  q   =  0.5   w1  1– q    +  0.5  p q   15-32   Solved Problems  p  1w 4   1w 2   1w 0   Figure P15.4  Instar Rule Example  15  15-33   15 Associative Learning  Epilogue  In this chapter we introduced some simple networks capable of forming  associations. We also developed and studied several learning rules that  allowed networks to create new associations. Each rule operated by  strengthening an association between any stimulus and response that  occurred simultaneously.  The simple associative networks and learning rules developed in this chap- ter are useful in themselves, but they are also important building blocks for  more powerful networks. In this chapter we introduced two networks, and  associated learning rules, that will be fundamental for the development of  important networks in the next three chapters: the instar and the outstar.  The instar is a network that is trained to recognize a pattern. The outstar  is a network that is trained to recall a pattern. We will use layers of instars  in Chapters 16 and 18 to perform pattern recognition. These networks are  very similar to the Hamming network of Chapter 3, whose first layer was,  in fact, a layer of instars. In Chapter 19 we will introduce a more complex  network, which combines both instars and outstars in order to produce sta- ble learning.  15-34   Further Reading  Further Reading  [Ande72]   J. Anderson, “A simple neural network generating an inter- active memory,” Mathematical Biosciences, vol. 14, pp.  197–220, 1972.  Anderson has proposed a “linear associator” model for asso- ciative memory. The model was trained, using a generali- zation of the Hebb postulate, to learn an association  between input and output vectors. The physiological plau- sibility of the network was emphasized. Kohonen published  a closely related paper at the same time [Koho72], although  the two researchers were working independently.  S. Grossberg, “Some physiological and biochemical conse- quences of psychological postulates,” Proceedings of the Na- tional Academy of Sciences, vol. 60, pp. 758–765, 1968.  This article describes early mathematical models  nonlin- ear differential equations  of associative learning. It syn- thesizes psychological, mathematical and physiological  ideas.  15  [Gros68]   [Gros82]  S. Grossberg, Studies of Mind and Brain, Boston: D. Reidel  Publishing Co., 1982.  This book is a collection of Stephen Grossberg papers from  the period 1968 through 1980. It covers many of the funda- mental concepts which are used in later Grossberg net- works, such as the Adaptive Resonance Theory networks.  [Hebb49]  D. O. Hebb, The Organization of Behavior, New York:  Wiley, 1949.  The main premise of this seminal book was that behavior  could be explained by the action of neurons. In it, Hebb pro- posed one of the first learning laws, which postulated a  mechanism for learning at the cellular level.  [Koho72]   T. Kohonen, “Correlation matrix memories,” IEEE Trans- actions on Computers, vol. 21, pp. 353–359, 1972.  Kohonen proposed a correlation matrix model for associa- tive memory. The model was trained, using the outer prod- uct rule  also known as the Hebb rule , to learn an  association between input and output vectors. The mathe- matical structure of the network was emphasized. Ander- son published a closely related paper at the same time  [Ande72], although the two researchers were working inde- pendently.  15-35   15 Associative Learning  [Koho87]  T. Kohonen, Self-Organization and Associative Memory,  2nd Ed., Berlin: Springer-Verlag, 1987.  This book introduces the Kohonen rule and several net- works that use it. It provides a complete analysis of linear  associative models and gives many extensions and exam- ples.  [Leib90]   D. Lieberman, Learning, Behavior and Cognition, Bel- mont, CA: Wadsworth, 1990.  Leiberman’s text forms an excellent introduction to behav- ioral psychology. This field is of interest to anyone looking  to model human  or animal  learning with neural net- works.  15-36   Exercises  Exercises  E15.1 The network shown in Figure E15.1 is to be trained using the Hebb rule   with decay, using a learning rate      of 0.3 and a decay rate      of 0.1.  Inputs  Hard Limit Neuron  p0  p  w0  = 1 n a  cid:0  cid:0 Σ  cid:0  cid:0   cid:0  cid:0   cid:0  cid:0  w  b = -0.8  1  a = hardlim  w0p0  + wp  + b   15  Figure E15.1  Associative Network  i. If   w   is initially set to 0, and    remain constant  with the val- ues shown in Figure E15.1 , how many consecutive presentations of  the following training set are required before the neuron will re- spond to the test set? Make a plot of   versus iteration number.   and   w  b  w0  Training set:     p0  1=  1= p        Test set:     p0  0=  1= p    w  ii. Assume that    has an initial value of 1. How many consecutive pre- sentations of the following training set are required before the neu- ron will no longer be able to respond to the test set? Make a plot of  w   versus iteration number.  Training set:   p0    0=  0= p        Test set:   p0    0=  1= p    E15.2 For Exercise E15.1 part  i , use Eq.  15.19  to determine the steady state  . Verify that this answer agrees with your plot from Exercise   value of  E15.1 part  i .  w  E15.3 Repeat Exercise E15.1, but this time use the Hebb rule without decay   E15.4 The following rule looks similar to the instar rule, but it behaves quite dif-       0=   .  ferently:  wij  –=  old ai pj wij  +    15-37   15 Associative Learning  i. Determine the conditions under which the    is nonzero.  ii. What value does the weight approach when    is nonzero?  wij wij  iii. Can you think of a use for this rule?   E15.5 The instar shown in Figure E15.2 is to be used to recognize a vector.  Inputs  Hard Limit Neuron  w0  = 1  w1,1   cid:0  cid:0 Σ  cid:0  cid:0   n  a  w1,2  b  =  -0.5  p0  p1  p2  1  a = hardlim  w0p0  + Wp  + b   Figure E15.2  Vector Recognizer  » 2 + 2 ans =       4  i. Train the network with the instar rule on the following training se-  quence. Apply the instar rule to the second input’s weights only   which should be initialized to zeros , using a learning rate of 0.6.  The other weight and the bias are to remain constant at the values  in the figure.  You may wish to use MATLAB to perform the calcu- lations.               p0 1   1=    p 1   =  p0 2   0=    p 2   =  p0 3   1=    p 3   =  p0 4   0=    p 4   =  p0 5   1=    p 5   =  p0 6   0=    p 6   =  0.174 0.985  0.174 0.985  0.174 0.985                          0.174 – 0.985  0.174 – 0.985  – 0.174 0.985              ii. What were your final values for   W  ?  iii. How do these final values compare with the vectors in the training   sequence?  15-38   Exercises  iv. What magnitude would you expect the weights to have after train-  ing, if the network were trained for many more iterations of the  same training sequence?  E15.6 Consider the instar network shown in Figure E15.3. The training sequence   for this network will consist of the following inputs:      p0 1   0=    p 1   =  p0 2   1=    p 2   =  1 1–           1 1–         .  These two sets of inputs are repeatedly presented to the network until the  weight matrix    converges.   W  i. Perform the first eight iterations of the instar rule, with learning   rate     =  0.25  . Assume that the initial   W   matrix is set to  15  ii. Display the results of each iteration of the instar rule in graphical   form  as in Figure 15.5 .  W  =  1 0  .  Inputs  Hard Limit Neuron  p0  p1  p2  w0  =  3  w1,1   cid:0  cid:0 Σ  cid:0  cid:0   n  a  b  =  -2  w1,2  1  a = hardlim  w0 p0  + W p + b   Figure E15.3  Instar Network for Exercise E15.6  E15.7 Draw a diagram of a network capable of recognizing three different four-  element vectors  of ±1 values  when given different stimuli  of value 1 .  i. How many inputs does your network have? How many outputs?   What transfer function did you use?  ii. Choose values for the network’s weights so that it can recognize   each of the following vectors:  15-39   15 Associative Learning  p1  =            p2  =            p3  =  1 1– 1 1–  1 1– 1– 1  iii. Choose an appropriate value for the biases. Explain your choice.  iv. Test the network with one of the vectors above. Was its response   correct?  v. Test the network with the following vector.  1– 1– 1 1–  1– 1– 1 1  p1  =  Why did it respond the way it did?  E15.8 This chapter included an example of a recognition network that initially  used a visual system to identify oranges. At first the network needed the  visual system to tell it when an orange was present, but eventually it  learned to recognize oranges from sensor measurements.  i. Let us replace the visual system with a person. Initially, the net-  work would depend on a person to tell it when an orange was  present. Would you consider the network to be learning in a super- vised or unsupervised manner?  ii. In what ways would the input from a person resemble the targets   used to train supervised networks in earlier chapters?  iii. In what ways would it differ?  G  Scan  Push 1 2 43  E15.9 The network shown in Figure E15.4 is installed in an elevator used by   three senior executives in a plush high-security corporate building. It has  buttons marked ‘1’ through ‘4’ for four floors above the ground floor. When  an executive enters the elevator on the ground floor, it determines which  person it is with a retinal scan, and then uses the network to select the floor  where that person is most likely to go to. If the guess is incorrect, the per- son can push a different button at any time, but if the network is correct, it  will save an important executive the effort of pushing a button.  15-40   Exercises  Floor Code  Retinal Scan  Floor?  Input  Sym. Hard Limit Layer  2  3  3 x 1  p0 2 x 1  2 x 2   cid:0 W0  cid:0  p  cid:0 W  cid:0 b  cid:0  a = hardlims  W0 p0  + W p  +  b    cid:0  cid:0  a 2 x 1  cid:0  cid:0  2 cid:0  cid:0   n 2 x 1  2 x 1  2 x 3  1  Figure E15.4  Elevator Network  15  Scan  Push 1 2 43  Network  Floor?  The network’s input output function is  a  =  hardlims W0p0 Wp b  +  +      .  The first input  been pushed.  p0   provides the network with a floor code, if a button has   0 p1  =     1st floor         0 p2  =     2nd floor   0 p3  =     3rd floor         0 p4  =     4th floor   1– 1–  1– 1  1 1–  1 1  If no button is pushed, then no code is given.  The first input is weighted with an identity matrix, and the biases are set  to -0.5, so that if a button is pushed the network will respond with its code.  The second input is always available. It consists of three elements that rep- resent the three executives:  0 p0  =  0 0     no button pushed   W0  I=  ,   b  =  0.5– 0.5–  15-41   15 Associative Learning  p1  =     President   ,      p2  =     Vice-President   ,   p3  =     Chairman   .  1 0 0  0 1 0  0 0 1  The network learns to recall the executives’ favorite floors by updating the  second set of weights using the outstar rule  using a learning rate of 0.6 .  Initially those weights are set to zero:  » 2 + 2 ans =       4  events:  i. Use MATLAB to simulate the network for the following sequence of   W  =  0 0 0 0 0 0  .  President pushes ‘4’, Vice-President pushes ‘3’, Chairman pushes ‘1’, Vice-President pushes ‘3’,  Chairman pushes ‘2’, President pushes ‘4’.  In other words, train the network on the following sequence:    p0  0=  p4  p  p1=    ,    p0  0=  p3  p  p2=    ,    p0  0=  p1  p  p3=    ,  p0    0=  p3  p  p2=    ,    p0  0=  p2  p  p3=    ,    p0  0=  p4  p  p1=    .  ii. What are the final weights?  iii. Now continue simulating the network on these events:  President does not push a button,  Vice-President does not push a button,  Chairman does not push a button.  iv. Which floors did the network take each executive to?  v. If the executives were to push the following buttons many times,   what would you expect the resulting weight matrix to look like?  President pushes ‘3’,  Vice-President pushes ‘2’,  Chairman pushes ‘4’.  15-42   Objectives  16 Competitive Networks  Objectives Theory and Examples Hamming Network  Layer 1 Layer 2  Competitive Layer  Competitive Learning Problems with Competitive Layers  Competitive Layers in Biology Self-Organizing Feature Maps  Improving Feature Maps Learning Vector Quantization  LVQ Learning Improving LVQ Networks  LVQ2   Summary of Results Solved Problems Epilogue Further Reading Exercises  16-1 16-2 16-3 16-3 16-4 16-5 16-7 16-9 16-10 16-12 16-15 16-16 16-18 16-21 16-22 16-24 16-37 16-38 16-39  16  Objectives  The Hamming network, introduced in Chapter 3, demonstrated one tech- nique for using a neural network for pattern recognition. It required that  the prototype patterns be known beforehand and incorporated into the net- work as rows of a weight matrix.  In this chapter we will discuss networks that are very similar in structure  and operation to the Hamming network. Unlike the Hamming network,  however, they use the associative learning rules of Chapter 15 to adaptive- ly learn to classify patterns. Three such networks are introduced in this  chapter: the competitive network, the feature map and the learning vector  quantization  LVQ  network.  16-1   16 Competitive Networks  Theory and Examples  The Hamming network is one of the simplest examples of a competitive  network. The neurons in the output layer of the Hamming network com- pete with each other to determine a winner. The winner indicates which  prototype pattern is most representative of the input pattern. The compe- tition is implemented by lateral inhibition — a set of negative connections  between the neurons in the output layer. In this chapter we will illustrate  how this competition can be combined with the associative learning rules  of Chapter 15 to produce powerful self-organizing  unsupervised  net- works.  As early as 1959, Frank Rosenblatt created a simple “spontaneous” classi- fier, an unsupervised network based on the perceptron, which learned to  classify input vectors into two classes with roughly equal members.  In the late 1960s and early 1970s, Stephen Grossberg introduced many  competitive networks that used lateral inhibition to good effect. Some of  the useful behaviors he obtained were noise suppression, contrast-en- hancement and vector normalization. His networks will be examined in  Chapters 18 and 19.  In 1973, Christoph von der Malsburg introduced a self-organizing learning  rule that allowed a network to classify inputs in such a way that neighbor- ing neurons responded to similar inputs. The topology of his network mim- icked, in some ways, the structures previously found in the visual cortex of  cats by David Hubel and Torten Wiesel. His learning rule generated a great  deal of interest, but it used a nonlocal calculation to ensure that weights  were normalized. This made it less biologically plausible.  Grossberg extended von der Malsburg's work by rediscovering the instar  rule, examined in Chapter 15.  The instar rule had previously been intro- duced by Nils Nilsson in his 1965 book Learning Machines.  Grossberg  showed that the instar rule removed the necessity of re-normalizing  weights, since weight vectors that learn to recognize normalized input vec- tors will automatically be normalized themselves.  The work of Grossberg and von der Malsburg emphasizes the biological  plausibility of their networks. Another influential researcher, Teuvo Ko- honen, has also been a strong proponent of competitive networks. However,  his emphasis has been on engineering applications and efficient mathe- matical descriptions of the networks. During the 1970s he developed a sim- plified version of the instar rule and also, inspired by the work of von der  Malsburg and Grossberg, found an efficient way to incorporate topology  into a competitive network.  In this chapter we will concentrate on the Kohonen framework for compet- itive networks. His models illustrate the major features of competitive net-  16-2   Hamming Network  works, and yet they are mathematically more tractable than the Grossberg  networks. They provide a good introduction to competitive learning.  We will begin with the simple competitive network. Next we will present  the self-organizing feature map, which incorporates a network topology. Fi- nally, we will discuss learning vector quantization, which incorporates  competition within a supervised learning framework.  Hamming Network  Since the competitive networks discussed in this chapter are closely related  to the Hamming network  shown in Figure 16.1 , it is worth reviewing the  key concepts of that network first.  Feedforward Layer  Recurrent Layer   cid:0  cid:0  p W1  cid:0  cid:0  R x 1 S x R  cid:0  cid:0 b1 1  cid:0  cid:0   S x 1  R  n1 S x 1  a1 S x 1   cid:0   cid:0   cid:0   cid:0  S   cid:0  cid:0   cid:0  cid:0   W2 S x S  n2 t + 1   S x 1   cid:0  cid:0   cid:0  cid:0  a2 t + 1   cid:0  cid:0   cid:0  cid:0   S x 1  S   cid:0  cid:0 D  cid:0  cid:0   a2 t   S x 1  16  a1 = purelin  W1p + b1   - Exp 1 -  a2 0  = a1      a2 t + 1  = poslin  W2a2 t    Figure 16.1  Hamming Network  The Hamming network consists of two layers. The first layer  which is a  layer of instars  performs a correlation between the input vector and the  prototype vectors. The second layer performs a competition to determine  which of the prototype vectors is closest to the input vector.  Layer 1 Recall from Chapter 15  see page 15-9 and following  that a single instar is  able to recognize only one pattern. In order to allow multiple patterns to be  classified, we need to have multiple instars. This is accomplished in the  Hamming network.  Suppose that we want the network to recognize the following prototype vec- tors:    p1 p2   pQ          .   16.1   16-3   16 Competitive Networks  Then the weight matrix,   W1  , and the bias vector,   b1  , for Layer 1 will be:  W1  =  =  ,   b1  =  ,   16.2   1  wT wT 2  wT  S  p1 T p2 T  pQ T  R R  R  W1  where each row of  ognize, and each element of  each input vector    .  The number of neurons,  of prototype vectors which are to be recognized,    represents a prototype vector which we want to rec-  is set equal to the number of elements in  , is equal to the number  .    S Q  b1  R  Thus, the output of the first layer is  a1 W1p b1+  =  =   16.3   p1 Tp R+ Tp R+ p2  pQ T p R+  .  Note that the outputs of Layer 1 are equal to the inner products of the pro- totype vectors with the input, plus  . As we discussed in Chapter 3  page  3-9 , these inner products indicate how close each of the prototype patterns  is to the input vector.  This was also discussed in our presentation of the  instar on page 15-10.   R  Layer 2 In the instar of Chapter 15, a   transfer function was used to decide  if the input vector was close enough to the prototype vector. In Layer 2 of  the Hamming network we have multiple instars, therefore we want to de- cide which prototype vector is closest to the input. Instead of the    hardlim transfer function, we will use a competitive layer to choose the closest pro- totype.  hardlim  Layer 2 is a competitive layer. The neurons in this layer are initialized with  the outputs of the feedforward layer, which indicate the correlation be- tween the prototype patterns and the input vector. Then the neurons com- pete with each other to determine a winner. After the competition, only one  neuron will have a nonzero output. The winning neuron indicates which  category of input was presented to the network  each prototype vector rep- resents a category .  The first-layer output    is used to initialize the second layer.  a1  a2 0   a1=   16.4   16-4   Competitive Layer  Then the second-layer output is updated according to the following recur- rence relation:  a2 t The second-layer weights  and the off-diagonal elements have a small negative value.  poslin W2a2 t   1+  W2  =        .   are set so that the diagonal elements are 1,    16.5   2 wij  =      j=    1    if i –    otherwise     ,  where   0       1 ------------ 1– S   16.6   Lateral Inhibition  This matrix produces lateral inhibition, in which the output of each neuron  has an inhibitory effect on all of the other neurons. To illustrate this effect,  substitute weight values of 1 and  ,  and rewrite Eq.  16.5  for a single neuron.   for the appropriate elements of   W2  –  2 t 1+ ai    =  poslin ai  2 t       2 t  aj   –   j  i   16.7   16  At each iteration, each neuron’s output will decrease in proportion to the  sum of the other neurons’ outputs  with a minimum output of 0 . The out- put of the neuron with the largest initial condition will decrease more slow- ly than the outputs of the other neurons. Eventually that neuron will be the  only one with a positive output. At this point the network has reached  steady state. The index of the second-layer neuron with a stable positive  output is the index of the prototype vector that best matched the input.  Winner-Take-All  This is called a winner-take-all competition, since only one neuron will have  a nonzero output. In Chapter 18 we will discuss other types of competition.  You may wish to experiment with the Hamming network and the apple or- ange classification problem. The Neural Network Design Demonstration  Hamming Classification  nnd3hamc  was previously introduced in Chapter 3.  Competitive Layer  Competition  The second-layer neurons in the Hamming network are said to be in com- petition because each neuron excites itself and inhibits all the other neu- rons. To simplify our discussions in the remainder of this chapter, we will  define a transfer function that does the job of a recurrent competitive layer:  It works by finding the index   of the neuron with the largest net input,  and setting its output to 1  with ties going to the neuron with the lowest  index . All other outputs are set to 0.  compet n    .  a  = i   16.8   16-5   16 Competitive Networks  ai  =      i= 1 i i 0 i  , where    ni  ni  i  , and    i  i  ni=  ni   16.9   Replacing the recurrent layer of the Hamming network with a competitive  transfer function on the first layer will simplify our presentations in this  chapter.  We will study the competition process in more detail in Chapter  18.  A competitive layer is displayed in Figure 16.2.  Input  p R x 1  R  Competitive Layer  cid:0  cid:0   cid:0  cid:0  n C S x 1  cid:0  cid:0  S cid:0  cid:0  a = compet  Wp    cid:0 W  cid:0   S x R  a S x 1  Figure 16.2  Competitive Layer  W  As with the Hamming network, the prototype vectors are stored in the rows  p n  calculates the distance between the input vector  of    . The net input  wi  .  and each prototype    assuming vectors have normalized lengths of  L The net input   between  p   is proportional to the angle    and the prototype vector    of each neuron   i  ni  :  i  wi  n Wp  =  =  p  =  =   16.10   T  T  T  w1 w2  wS  w1 w2  wS  Tp Tp  Tp  L2 L2  cos  cos  1 2    L2  cos  S  .  The competitive transfer function assigns an output of 1 to the neuron  whose weight vector points in the direction closest to the input vector:  a  =  compet Wp      .   16.11   To experiment with the competitive network and the apple orange classifi- cation problem, use the Neural Network Design Demonstration Competitive  Classification  nnd14cc .  16-6   Competitive Layer  Competitive Learning We can now design a competitive network classifier by setting the rows of  W learning rule that could be used to train the weights in a competitive net- work, without knowing the prototype vectors. One such learning rule is the  instar rule from Chapter 15:   to the desired prototype vectors. However, we would like to have a   wi  q   =  wi  1– q   ai q  p q  wi +  –    1– q      .   16.12   For the competitive network,    i   is only nonzero for the winning neuron   . Therefore, we can get the same results using the Kohonen rule.  i=  a  wi  q   wi  1– q    p q  wi +  –    1– q      1 –   wi  q 1–   p q  +  =  =   16.13   and  wi  q   =  wi  1– q    i  i   16.14   16  Thus, the row of the weight matrix that is closest to the input vector  or has  the largest inner product with the input vector  moves toward the input  vector. It moves along a line between the old row of the weight matrix and  the input vector, as shown in Figure 16.3.  p q   iw q   iw q - 1   Figure 16.3  Graphical Representation of the Kohonen Rule  2 2+  Let’s use the six vectors in Figure 16.4 to demonstrate how a competitive  layer learns to classify vectors. Here are the six vectors:  p1  =  p4  =  0.1961 – 0.9806    p2  =  0.1961 0.9806    p3  =  0.9806 0.1961  0.9806 0.1961 –    p5  =  – –  0.5812 0.8137    p6  =  – –  0.8137 0.5812   .   16.15   16-7   16 Competitive Networks  Figure 16.4  Sample Input Vectors  Our competitive network will have three neurons, and therefore it can clas- sify vectors into three classes. Here are the “randomly” chosen normalized  initial weights:  p1 p2  w1  =  0.7071 – 0.7071    w2  =  0.7071 0.7071    w3  =  – 1.0000 0.0000  ,   W  =  .   16.16   1  wT wT 2 w3 T  The data vectors are shown at left, with the weight vectors displayed as ar- rows. Let’s present the vector    to the network:  p2  a  =  compet Wp2      =  compet   16.17         –  0.7071 0.7071 0.7071 0.7071 0.0000 – 1.0000  0.1961 0.9806        2w  1w  p3 p4  3w  p6  p5  =  compet      –  0.5547 – 0.8321 0.1961        =   .  0 1 0  The second neuron’s weight vector was closest to  tition   the winning neuron with a learning rate of   , so it won the compe-   and output a 1. We now apply the Kohonen learning rule to    = 0.5.  2=  i  p2    p1 p2  2w  3w  p6  p5  p3 p4  1w  wnew 2  =  wold  2  +   p2 wold   2–     16.18   =  0.7071 0.7071  +      0.5 0.1961 0.9806  –  0.7071 0.7071      =  0.4516 0.8438  The Kohonen rule moves  , as can be seen in the diagram at  left. If we continue choosing input vectors at random and presenting them  to the network, then at each iteration the weight vector closest to the input  vector will move toward that vector. Eventually, each weight vector will    closer to   w2  p2  16-8   Competitive Layer  point at a different cluster of input vectors. Each weight vector becomes a  prototype for a different cluster.  This problem is simple enough that we can predict which weight vector will  point at which cluster. The final weights will look something like those  shown in Figure 16.5.  p1 p2  2w  1w  p3 p4  3w  p6  p5  Figure 16.5  Final Weights  2w  1w  3w  stable  slow  α ≈ 0  unstable  fast  α ≈ 1  16  Once the network has learned to cluster the input vectors, it will classify  new vectors accordingly.The diagram in the left margin uses shading to  show which region each neuron will respond to. The competitive layer as- signs each input vector   to one of these classes by producing an output of  1 for the neuron whose weight vector is closest to   p  .  p  To experiment with the competitive learning use the Neural Network Design  Demonstration Competitive Learning  nnd14cl .  Problems with Competitive Layers Competitive layers make efficient adaptive classifiers, but they do suffer  from a few problems. The first problem is that the choice of learning rate  forces a trade-off between the speed of learning and the stability of the final  weight vectors. A learning rate near zero results in slow learning. However,  once a weight vector reaches the center of a cluster it will tend to stay close  to the center.  In contrast, a learning rate near 1.0 results in fast learning. However, once  the weight vector has reached a cluster, it will continue to oscillate as dif- ferent vectors in the cluster are presented.  Sometimes this trade-off between fast learning and stability can be used to  advantage. Initial training can be done with a large learning rate for fast  learning. Then the learning rate can be decreased as training progresses,  to achieve stable prototype vectors. Unfortunately, this technique will not  work if the network needs to continuously adapt to new arrangements of  input vectors.  A more serious stability problem occurs when clusters are close together.  In certain cases, a weight vector forming a prototype of one cluster may “in-  16-9   16 Competitive Networks  vade” the territory of another weight vector, and therefore upset the cur- rent classification scheme.  The series of four diagrams in Figure 16.6 illustrate this problem. Two in- put vectors  shown with blue circles in diagram  a   are presented several  times. The result is that the weight vectors representing the middle and  right clusters shift to the right. Eventually one of the right cluster vectors  is reclassified by the center weight vector. Further presentations move the  middle vector over to the right until it “loses” some of its vectors, which  then become part of the class associated with the left weight vector.   a    b    c    d   Figure 16.6  Example of Unstable Learning  A third problem with competitive learning is that occasionally a neuron’s  initial weight vector is located so far from any input vectors that it never  wins the competition, and therefore never learns. The result is a “dead”  neuron, which does nothing useful. For example, the downward-pointing  weight vector in the diagram to the left will never learn, regardless of the  order in which vectors are presented. One solution to this problem consists  of adding a negative bias to the net input of each neuron and then decreas- ing the bias each time the neuron wins. This will make it harder for a neu- ron to win the competition if it has won often. This mechanism is  sometimes called a “conscience.”  See Exercise E16.4.   Finally, a competitive layer always has as many classes as it has neurons.  This may not be acceptable for some applications, especially when the num- ber of clusters is not known in advance. In addition, for competitive layers,  each class consists of a convex region of the input space. Competitive layers  cannot form classes with nonconvex regions or classes that are the union of  unconnected regions.  Some of the problems discussed in this section are solved by the feature  map and LVQ networks, which are introduced in later sections of this chap- ter, and the ART networks, which are presented in Chapter 19.  Competitive Layers in Biology  In previous chapters we have made no mention of how neurons are physi- cally organized within a layer  the topology of the network . In biological  neural networks, neurons are typically arranged in two-dimensional lay- ers, in which they are densely interconnected through lateral feedback. The  diagram to the left shows a layer of twenty-five neurons arranged in a two- dimensional grid.  16-10   neuron j -e -e  -e  -e  -e  -e  -e  -e  +1  -e  -e  -e  -e  -e  -e  -e  -e  -e  -e  -e  -e  -e  -e  -e  -e  On-center off-surround  Mexican-Hat Function   16.19    16.20   16  Competitive Layers in Biology  Often weights vary as a function of the distance between the neurons they  connect. For example, the weights for Layer 2 of the Hamming network are  assigned as follows:  Eq.  16.20  assigns the same values as Eq.  16.19 , but in terms of the dis- tances    between neurons:  dij  wij  =       1  if i –  if i    j= j  .  wij  =       1  if dij –  if dij    0= 0  .  Either Eq.  16.19  or Eq.  16.20  will assign the weight values shown in the  diagram at left. Each neuron  is labeled with the value of the weight  ,  which comes from it to the neuron marked   wij  .  i  j  The term on-center off-surround is often used to describe such a connection  pattern between neurons. Each neuron reinforces itself  center , while in- hibiting all other neurons  surround .  It turns out that this is a crude approximation of biological competitive lay- ers. In biology, a neuron reinforces not only itself, but also those neurons  close to it. Typically, the transition from reinforcement to inhibition occurs  smoothly as the distance between neurons increases.  This transition is illustrated on the left side of Figure 16.7. This is a func- tion that relates the distance between neurons to the weight connecting  them. Those neurons that are close provide excitatory  reinforcing  connec- tions, and the magnitude of the excitation decreases as the distance in- creases. Beyond a certain distance, the neurons begin to have inhibitory  connections, and the inhibition increases as the distance increases. Be- cause of its shape, the function is referred to as the Mexican-hat function.  On the right side of Figure 16.7 is a two-dimensional illustration of the  Mexican-hat  on-center off-surround  function. Each neuron   is marked to  show the sign and relative strength of its weight    going to neuron   .  i  j  wij  neuron j - - - + - + - + - -  - - + + + + + - -  - - + - -  -  wij  +  -  dij  16-11  Figure 16.7  On-Center Off-Surround Layer in Biology   16 Competitive Networks  Biological competitive systems, in addition to having a gradual transition  between excitatory and inhibitory regions of the on-center off-surround  connection pattern, also have a weaker form of competition than the win- ner-take-all competition of the Hamming network. Instead of a single ac- tive neuron  winner , biological networks generally have “bubbles” of  activity that are centered around the most active neuron. This is caused in  part by the form of the on-center off-surround connectivity pattern and also  by nonlinear feedback connections.  See the discussion on contour enhance- ment in Chapter 18.   Self-Organizing Feature Maps  SOFM  In order to emulate the activity bubbles of biological systems, without hav- ing to implement the nonlinear on-center off-surround feedback connec- tions, Kohonen designed the following simplification. His self-organizing  feature map  SOFM  network first determines the winning neuron   us- ing the same procedure as the competitive layer. Next, the weight vectors  for all neurons within a certain neighborhood of the winning neuron are up- dated using the Kohonen rule,  i  wi  q   wi  1– q    p q  wi +  –    q 1–      1 –   wi  1– q   p q  +  =  =  i Ni d    ,   16.21   Neighborhood  where the neighborhood  Ni d   of the winning neuron  that lie within a radius  d  i  :   contains the indices for all of the neurons   Ni d   =   j dij  d    .   16.22   When a vector  neighbors will move toward  tions, neighboring neurons will have learned vectors similar to each other.   is presented, the weights of the winning neuron and its   . The result is that, after many presenta-  p  p  2 2+  To demonstrate the concept of a neighborhood, consider the two diagrams  shown in Figure 16.8. The left diagram illustrates a two-dimensional  neighborhood of radius  d a neighborhood of radius   . The right diagram shows    around neuron   1= d  2=  13  .   The definition of these neighborhoods would be  N13 1   =     8 12 13 14 18          ,  N13 2   =     3 7 8 9 11 12 13 14 15 17 18 19 23                          .   16.23    16.24   16-12   Self-Organizing Feature Maps  5  10  15  20  25  1  6  11  16  21  2  78  12  17  22  3  13  18  23  4  9  14  19  24  N13 1   1  6  11  16  21  23  7  12  17  22  8  13  18  23  4  9  14  19  24  5  10  15  20  25  N13 2   Figure 16.8  Neighborhoods  We should mention that the neurons in an SOFM do not have to be ar- ranged in a two-dimensional pattern. It is possible to use a one-dimension- al arrangement, or even three or more dimensions. For a one-dimensional  SOFM, a neuron will only have two neighbors within a radius of 1  or a sin- gle neighbor if the neuron is at the end of the line . It is also possible to de- fine distance in different ways. For instance, Kohonen has suggested  rectangular and hexagonal neighborhoods for efficient implementation.  The performance of the network is not sensitive to the exact shape of the  neighborhoods.  16  2 2+  Now let’s demonstrate the performance of an SOFM network. Figure 16.9  shows a feature map and the two-dimensional topology of its neurons.  Input  p 3 x 1  3   cid:0 W  cid:0   Feature Map  cid:0  cid:0   cid:0  cid:0  n C 25 x 1  cid:0  cid:0  25 cid:0  cid:0  a = compet  Wp   25 x 3  a 25 x 1  Feature Map  1  6  11  16  21  2  7  12  17  22  3  8  13  18  23  4  9  14  19  24  5  10  15  20  25  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  -1 -1  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  1  Figure 16.9  Self-Organizing Feature Map  The diagram in the left margin shows the initial weight vectors for the fea- ture map. Each three-element weight vector is represented by a dot on the  sphere.  The weights are normalized, therefore they will fall on the surface  of a sphere.  Dots of neighboring neurons are connected by lines so you can  see how the physical topology of the network is arranged in the input space.  16-13   1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  -1 -1  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  -1 -1  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  -1 -1  16 Competitive Networks  The diagram to the left shows a square region on the surface of the sphere.  We will randomly pick vectors in this region and present them to the fea- ture map.  Each time a vector is presented, the neuron with the closest weight vector  will win the competition. The winning neuron and its neighbors move their  weight vectors closer to the input vector  and therefore to each other . For  this example we are using a neighborhood with a radius of 1.  The weight vectors have two tendencies: first, they spread out over the in- put space as more vectors are presented; second, they move toward the  weight vectors of neighboring neurons. These two tendencies work together  to rearrange the neurons in the layer so that they evenly classify the input  space.  The series of diagrams in Figure 16.10 shows how the weights of the twen- ty-five neurons spread out over the active input space and organize them- selves to match its topology.  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  1  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  1     1  -1 -1     1  -1 -1     1  -1 -1  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  16-14  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  1     1  -1 -1     1  -1 -1     1  -1 -1  Figure 16.10  Self-Organization, 250 Iterations per Diagram  In this example, the input vectors were generated with equal probability  from any point in the input space. Therefore, the neurons classify roughly  equal areas of the input space.  Figure 16.11 provides more examples of input regions and the resulting  feature maps after self-organization.   Self-Organizing Feature Maps  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8     1  -1 -1  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8     1  -1 -1  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8     1  -1 -1  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  -1 -1  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  1  Figure 16.11  Other Examples of Feature Map Training  Occasionally feature maps can fail to properly fit the topology of their input  space. This usually occurs when two parts of the net fit the topology of sep- arate parts of the input space, but the net forms a twist between them. An  example is given in Figure 16.12.  16  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  -1 -1  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8     1  -1 -1  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  1  Figure 16.12  Feature Map with a Twist  It is unlikely that this twist will ever be removed, because the two ends of  the net have formed stable classifications of different regions.  Improving Feature Maps So far, we have described only the most basic algorithm for training feature  maps. Now let’s consider several techniques that can be used to speed up  the self-organizing process and to make it more reliable.  One method to improve the performance of the feature map is to vary the  size of the neighborhoods during training. Initially, the neighborhood size,   is gradually reduced, until it only  d includes the winning neuron. This speeds up self-organizing and makes  twists in the map very unlikely.  , is set large. As training progresses,   d  The learning rate can also be varied over time. An initial rate of 1 allows  neurons to quickly learn presented vectors. During training, the learning  rate is decreased asymptotically toward 0, so that learning becomes stable.   16-15   16 Competitive Networks   We discussed the use of this technique for competitive layers earlier in the  chapter.  Another alteration that speeds self-organization is to have the winning  neuron use a larger learning rate than the neighboring neurons. Finally, both competitive layers and feature maps often use an alternative  expression for net input. Instead of using the inner product, they can di- rectly compute the distance between the input vector and the prototype  vectors. The advantage of using the distance is that input vectors do not  need to be normalized. This alternative net input expression is introduced  in the next section on LVQ networks. Other enhancements to the SOFM are described in Chapter 26, including  a batch version of the SOFM learning rule. That chapter is a case study of  using the SOFM for clustering.  To experiment with feature maps use the Neural Network Design Demon- strations 1-D Feature Maps  nnd14fm1  and 2-D Feature Maps  nnd14fm2 .  Learning Vector Quantization  The final network we will introduce in this chapter is the learning vector  quantization  LVQ  network, which is shown in Figure 16.13. The LVQ net- work is a hybrid network. It uses both unsupervised and supervised learn- ing to form classifications.  In the LVQ network, each neuron in the first layer is assigned to a class,  with several neurons often assigned to the same class. Each class is then  assigned to one neuron in the second layer. The number of neurons in the  first layer,  , will therefore always be at least as large as the number of  neurons in the second layer,   , and will usually be larger.  S1  S2  Inputs  Competitive Layer  Linear Layer  W1  S 1 x R  p  R x 1  dist  n1  S 1 x 1  a1 C S 1 x 1  n2  S 2 x 1  W2  S 2 x S 1  R  S 1  S 2  a2  S 2 x 1  n1  i  1  i=   - w p  1 a  =  compet n     1     2  1 a W a  =  2  Figure 16.13  LVQ Network  16-16   Learning Vector Quantization  As with the competitive network, each neuron in the first layer of the LVQ  network learns a prototype vector, which allows it to classify a region of the  input space. However, instead of computing the proximity of the input and  weight vectors by using the inner product, we will simulate the LVQ net- works by calculating the distance directly. One advantage of calculating  the distance directly is that vectors need not be normalized. When the vec- tors are normalized, the response of the network will be the same, whether  the inner product is used or the distance is directly calculated.  The net input of the first layer of the LVQ will be  1 ni  –=  w1 i  p–  ,   16.25   or, in vector form,  n1  –=  w1 1 w1 2  w1 S1  p– p–  p–  .  a1  =  compet n1    .  The output of the first layer of the LVQ is   16   16.26    16.27   Subclass  Therefore the neuron whose weight vector is closest to the input vector will  output a 1, and the other neurons will output 0.   Thus far, the LVQ network behaves exactly like the competitive network   at least for normalized vectors . There is a difference in interpretation,  however. In the competitive network, the neuron with the nonzero output  indicates which class the input vector belongs to. For the LVQ network, the  winning neuron indicates a subclass, rather than a class. There may be sev- eral different neurons  subclasses  that make up each class.  The second layer of the LVQ network is used to combine subclasses into a  single class. This is done with the  sent subclasses, and the rows represent classes.   has a single 1 in each  column, with the other elements set to zero. The row in which the 1 occurs  indicates which class the appropriate subclass belongs to.   matrix. The columns of    repre-  W2  W2  W2    2 wki  1=      subclass i is a part of class   k   16.28   The process of combining subclasses to form a class allows the LVQ net- work to create complex class boundaries. A standard competitive layer has   16-17   16 Competitive Networks  the limitation that it can only create decision regions that are convex. The  LVQ network overcomes this limitation.  LVQ Learning The learning in the LVQ network combines competitive learning with su- pervision. As with all supervised learning algorithms, it requires a set of  examples of proper network behavior:    p1 t1   p2 t2        pQ tQ         .  Each target vector must contain only zeros, except for a single 1. The row  in which the 1 appears indicates the class to which the input vector be- longs. For example, if we have a problem where we would like to classify a  particular three-element vector into the second of four classes, we can ex- press this as          p1  =    t1  =  1 2 0 1 2  0 1 0 0  .           16.29   Before learning can occur, each neuron in the first layer is assigned to an  output neuron. This generates the matrix  . Typically, equal numbers of  hidden neurons are connected to each output neuron, so that each class can  be made up of the same number of convex regions. All elements of   are  set to zero, except for the following:  W2  W2  2 If hidden neuron i is to be assigned to class k, then set wki  1=  .   16.30   W2  Once  trained with a variation of the Kohonen rule.   is defined, it will never be altered. The hidden weights   W1   are   p   is presented to the network, and the distance from   The LVQ learning rule proceeds as follows. At each iteration, an input vec-  to each proto- tor  type vector is computed. The hidden neurons compete, neuron   wins the  competition, and the   is multiplied  by  , which also has only one nonzero element,  k  W2 , indicating that    is being assigned to class    to get the final output    is set to 1. Next,   th element of   a1  a2  a1  k  i  i  p  .  p  The Kohonen rule is used to improve the hidden layer of the LVQ network  in two ways. First, if  w1 i   is classified correctly, then we move the weights    of the winning hidden neuron toward   p  p  .  w1 i  q   =  w1  i  q 1–    p q  +    –  w1 i  1– q      , if   2 ak  =  tk  =  1   16.31   16-18   Learning Vector Quantization  p  Second, if  neuron won the competition, and therefore we move its weights  from    was classified incorrectly, then we know that the wrong hidden   away   w1  i  .  p  w1  q   i  =  w1 i  1– q    p q  –    –  w1  i  1– q      , if   2 ak  =  1  tk  =  0   16.32   The result will be that each hidden neuron moves toward vectors that fall  into the class for which it forms a subclass and away from vectors that fall  into other classes.  2 2+  Let’s take a look at an example of LVQ training. We would like to train an  LVQ network to solve the following classification problem:  p4  p1  Class 1  Class 2  p2  p3  class 1:  p1  =    p2  =  , class 2:  p3  =    p4  =   16.33       1– 1–  1 1      1 1–  1– 1  ,      as illustrated by the figure in the left margin. We begin by assigning target  vectors to each input:  16      1 1          p1  =    t1  =  ,   p2  =    t2  =  1– 1–  1 0          1 0  ,      p3  =    t3  =  1 1–  0 1          ,   p4  =  1– 1    t4  =  0 1  .      We now must choose how many subclasses will make up each of the two  classes. If we let each class be the union of two subclasses, we will end up  with four neurons in the hidden layer. The output layer weight matrix will  be   16.34    16.35    16.36   W2  =  1 1 0 0 0 0 1 1  .  W2  connects hidden neurons 1 and 2 to output neuron 1. It connects hidden  neurons 3 and 4 to output neuron 2. Each class will be made up of two con- vex regions.  W1  The row vectors in  in the diagram at left. The weights belonging to the two hidden neurons  that define class 1 are marked with hollow circles. The weights defining  class 2 are marked with solid circles. The values for these weights are   are initially set to random values. They can be seen   w1  1  =  0.543 – 0.840  ,   w1 2  =  – –  0.969 0.249  ,   w1 3  =  0.997 0.094  ,   w1 4  =  0.456 0.954  .   16.37   p4  1w1  4w1  2w1  p1  p2  3w1  p3  16-19   16 Competitive Networks  At each iteration of the training process, we present an input vector, find  its response, and then adjust the weights. In this case we will begin by pre- senting   .  p3  –  –          –  –  w1 1 w1 2 w1 3 w1 4  p3– p3– p3– p3–            T  T  a1  =  compet n1    =  compet   16.38               0 0 1 0  =  compet              –  –  –  –  –  0.543  0.840  1 1–  –  0.969  –  0.249 T  0.997 0.094  0.456 0.954  1 1– T  1 1–  T  1 1–  T  –  T  –  –  –  T  =  compet   –   –  –  –  2.40 2.11 1.09 2.03         =  0 0 1 0  The third hidden neuron has the closest weight vector to  termine which class this neuron belongs to, we multiply   p3 a1 W2  . In order to de-  by   .  a2 W2a1  =  =  1 1 0 0 0 0 1 1  =  0 1   16.39   This output indicates that  is updated by moving it toward   p3  p3  .   is a member of class 2. This is correct, so   w1 3     w1  3  1   =  w1  0   p3 w1  +  –    3  3  0      16.40   =  0.997 0.094  +  0.5      1 1–  –  0.997 0.094      =  0.998 0.453 –  The diagram on the left side of Figure 16.14 shows the weights after    was updated on the first iteration. The diagram on the right side of Figure  16.14 shows the weights after the algorithm has converged.  w1 3  16-20   Learning Vector Quantization  The diagram on the right side of Figure 16.14 also indicates how the re- gions of the input space will be classified. The regions that will be classified  as class 1 are shown in gray, and the regions that will be classified as class  2 are shown in blue.  p4  1w1  4w1  p2  2w1  p1  3w1 p3  p4  p1  4w1  1w1  p2  p3             2w1  3w1  Figure 16.14  After First and Many Iterations  16  Improving LVQ Networks  LVQ2  The LVQ network described above works well for many problems, but it  does suffer from a couple of limitations. First, as with competitive layers,  occasionally a hidden neuron in an LVQ network can have initial weight  values that stop it from ever winning the competition. The result is a dead  neuron that never does anything useful. This problem is solved with the  use of a “conscience” mechanism, a technique discussed earlier for compet- itive layers, and also presented in Exercise E16.4.  Secondly, depending on how the initial weight vectors are arranged, a neu- ron’s weight vector may have to travel through a region of a class that it  doesn’t represent, to get to a region that it does represent. Because the  weights of such a neuron will be repulsed by vectors in the region it must  cross, it may not be able to cross, and so it may never properly classify the  region it is being attracted to. This is usually solved by applying the follow- ing modification to the Kohonen rule.  If the winning neuron in the hidden layer incorrectly classifies the current  input, we move its weight vector away from the input vector, as before.  However, we also adjust the weights of the closest neuron to the input vec- tor that does classify it properly. The weights for this second neuron should  be moved toward the input vector.  When the network correctly classifies an input vector, the weights of only  one neuron are moved toward the input vector. However, if the input vector  is incorrectly classified, the weights of two neurons are updated, one weight  vector is moved away from the input vector, and the other one is moved to- ward the input vector. The resulting algorithm is called LVQ2.  LVQ2  To experiment with LVQ networks use the Neural Network Design Demon- strations LVQ1 Networks  nnd14lv1  and LVQ2 Networks  nnd14lv2 .  16-21   16 Competitive Networks  Summary of Results  Competitive Layer  Competitive Learning with the Kohonen Rule  wi    p q  +  1 –  1– q  1– q  wi  wi  wi  q   =  =  –        1– q   p q  +  wi  q   =  wi  1– q    i  ,  i  where   i   is the winning neuron.  Input  p R x 1  R  Competitive Layer  cid:0  cid:0   cid:0  cid:0  n C S x 1  cid:0  cid:0  S cid:0  cid:0  a = compet  Wp    cid:0 W  cid:0   S x R  a S x 1  p q   iw q   iw q - 1   16-22   Summary of Results  Input  Self-Organizing Feature Map Feature Map  cid:0  cid:0   cid:0  cid:0  n C 25 x 1  cid:0  cid:0  25 cid:0  cid:0  a = compet  Wp    cid:0 W  cid:0   p 3 x 1  25 x 3  3  a 25 x 1  Feature Map  1  6  11  16  21  2  7  12  17  22  3  8  13  18  23  4  9  14  19  24  5  10  15  20  25  Self-Organizing with the Kohonen Rule     p q  wi +  1– q  1– q  q   wi  wi  =  –      =  1 –   wi  1– q   p q  +  Ni d   =   j dij  d    LVQ Network Inputs  Competitive Layer  Linear Layer  i Ni d    16  W1  S 1 x R  p  R x 1  dist  n1  S 1 x 1  a1 C S 1 x 1  n2  S 2 x 1  W2  S 2 x S 1  R  S 1  S 2  a2  S 2 x 1  n1  i  1  i=   - w p  1 a  =  compet n     1     2  1 a W a  =  2    2 wki  1=      subclass i is a part of class   k  LVQ Network Learning with the Kohonen Rule  w1 i  q   =  w1  i  1– q    p q  +    –  w1 i  q 1–      , if   2 ak  =  tk  =  1  w1  q   i  =  w1  i  1– q    p q  –    –  w1  i  1– q      , if   2 ak  =  1  tk  =  0  16-23   16 Competitive Networks  Solved Problems  P16.1 Figure P16.1 shows several clusters of normalized vectors.   Figure P16.1  Clusters of Input Vectors for Problem P16.1  Design the weights of the competitive network shown in Figure  P16.2, so that it classifies the vectors according to the classes indi- cated in the diagram and with the minimum number of neurons.  class 1  class 2  class 4  class 3  Input  p R x 1  R  Competitive Layer  cid:0  cid:0   cid:0  cid:0  n C S x 1  cid:0  cid:0  S cid:0  cid:0  a = compet  Wp    cid:0 W  cid:0   S x R  a S x 1  Figure P16.2  Competitive Network for Problem P16.1  Redraw the diagram showing the weights you chose and the deci- sion boundaries that separate the region of each class.  Since there are four classes to be defined, the competitive layer will need  four neurons. The weights of each neuron act as prototypes for the class  that neuron represents. Therefore, for each neuron we will choose a proto- type vector that appears to be approximately at the center of a cluster.  Classes 1, 2 and 3 each appear to be roughly centered at a multiple of  45 Given this, the following three vectors are normalized  as is required for  the competitive layer  and point in the proper directions.  .   w1  =  1– 1  2 2  ,   w2  =  1 1  2 2  ,   w3  =  2 1 2– 1  16-24   Solved Problems  The center of the fourth cluster appears to be about twice as far from the  vertical axis as it is from the horizontal axis. The resulting normalized  weight vector is  The weight matrix  transposed prototype vectors:  W   for the competitive layer is simply the matrix of the   w4  =  5– 2 5 1–  .  W  =  =  wT 1 wT 2 wT 3 wT 4  1– 1 1 2–  2 2 2 5  1 1 1– 1–  2 2 2 5  .  We get Figure P16.3 by drawing these weight vectors with arrows and bi- secting the circle between each adjacent weight vector to get the class re- gions.  class 1  class 2  16  class 4  class 3  Figure P16.3  Final Classifications for Problem P16.1  P16.2 Figure P16.4 shows three input vectors and three initial weight   vectors for a three-neuron competitive layer. Here are the values  of the input vectors:  p1  =  ,   p2  =  ,   p3  =  1– 0  0 1  1 1  2 2  .  The initial values of the three weight vectors are  w1  =  ,   w2  =  0 1–  5– 2 5 1  ,   w3  =  5– 1 5 2  .  16-25   16 Competitive Networks  Calculate the resulting weights found after training the competi- tive layer with the Kohonen rule and a learning rate   of 0.5, on  the following series of inputs:    p1 p2 p3 p1 p2 p3  ,   ,   ,   ,   ,   .  p2  3w  p3  2w p1  1w  W  =  0  5– 2 5– 1  1 2  1– 5 5  Figure P16.4  Input Vectors and Initial Weights for Problem P16.2  First we combine the weight vectors into the weight matrix   W  .  Then we present the first vector   p1  .  a  =  compet Wp1      =  compet  =  compet        0  5– 2 5– 1  1 2  1– 5 5  1– 0              0  0.894 0.447        =  0 1 0  The second neuron responded, since  will update   w2  with the Kohonen rule.  w2   was closest to   . Therefore, we   p1  p2  3w  p3  wnew  2  =  wold 2  +   p1 wold   2–    =  5– 2 5 1  +  0.5  1– 0  –  5– 2 5 1  =  0.947 – 0.224            The diagram at left shows that the new    moved closer to   w2  p1  .  We will now repeat this process for   p2  .  2w p1  1w  16-26   Solved Problems  a  =  compet Wp2      =  compet  =  compet        0  1–  0.947 – 5– 1  0.224 5 2        0 1        1–  0.224 0.894        =  0 0 1  The third neuron won, so its weights move closer to   p2  .  wnew 3  =  wold 3  +   p2 wold   3–    =  5– 1 5 2  +  0.5 0 1  –  5– 1 5 2  =  0.224 – 0.947            We now present   p3  .  a  =  compet Wp3      =  compet  1 1  2 2                   0  1–  – –  0.947 0.224  0.224 0.947  – 0.707 – 0.512 0.512        =  0 0 1  =  compet  16  The third neuron wins again.  wnew  3  =  wold 3  +   p2 wold   3–    =  – 0.224 0.947  +  0.5 1 1  2 2  –  0.224 – 0.947  =  0.2417 0.8272            After presenting  neuron 3 twice. The final weights are   through   p3  p1   again, neuron 2 will again win once and   p2  3w  p3  2w p1  W  =  0  1–  0.974 0.118 – 0.414 0.8103  .  The final weights are also shown in the diagram at left.  1w  w2   has almost learned   Note that  The other weight vector,  never won the competition, is a dead neuron.  p3 .  , was never updated. The first neuron, which    is directly between   , and    and   w1  w3  p1  p2  P16.3 Consider the configuration of input vectors and initial weights  shown in Figure P16.5. Train a competitive network to cluster   16-27   16 Competitive Networks  these vectors using the Kohonen rule with learning rate  .  Find graphically the position of the weights after all of the input  vectors  in the order shown  have been presented once.  0.5    =  Figure P16.5  Input Vectors and Initial Weights for Problem P16.3  This problem can be solved graphically, without any computations. The re- sults are displayed in Figure P16.6.  1w 0   p1  p4  p2  p4  p2  2w 0   2w 3   2w 4   1w 0   1w 1   p1  1w 2   2w 0   p3  p3  16-28  Figure P16.6  Solution for Problem P16.3 p1   is presented first. The weight vector   The input vector  p1  , therefore neuron 1 wins the competition and   w1   is closest to   is moved halfway to   w1   Solved Problems    p2  , since   . Next,    is not changed.   is moved halfway to   p1 0.5 = w1 petition and  w2 p3 On the third iteration,  and is moved halfway to  neuron 2 again wins. The weight vector   p2   is presented, and again neuron 1 wins the com- . During these first two iterations,    is presented. This time  p3  w2 . On the fourth iteration,    wins the competition  p4  is presented, and   w2   is moved halfway to   p4  .   and   If we continue to train the network, neuron 1 will classify the input vectors  p1 . If the  input vectors were presented in a different order, would the final classifi- cation be different?  , and neuron 2 will classify the input vectors    and   p3  p4  p2  P16.4 So far in this chapter we have only talked about feature maps   whose neurons are arranged in two dimensions. The feature map  shown in Figure P16.7 contains nine neurons arranged in one di- mension.  Inputs  Feature Map  Feature Map  16  p  3 x 1  W  9 x 3  n C9 x 1  a  9 x 1  3  9  1  2  3  4  5  6  7  8  9  a  =     Wp compet  Figure P16.7  Nine-Neuron Feature Map  Given the following initial weights, draw a diagram of the weight  vectors, with lines connecting weight vectors of neighboring neu- rons.  W  =  0  0.41 0.45 0.41 0.41 0.41 0.41 0.41 0.82 0.89 0.82 0.89 1 0.89 0.82 0.89 0.82  0.45 0 0.45  0.41 – 0.41  0.45 0  – –  0  0  –  0  –  –  T  Train the feature map for one iteration, on the vector below, using  a learning rate of 0.1 and a neighborhood of radius 1. Redraw the  diagram for the new weight matrix.  16-29   The feature map diagram for the initial weights is given in Figure P16.8.  16 Competitive Networks  p  =  0.67 0.07 0.74  7  8  9  4  5  6  1  2  3  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  -1 -1  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  1  Figure P16.8  Original Feature Map  We start updating the network by presenting    to the network.  p  a  =  compet Wp      =  compet        0  0.41 0.41 0.45 0.41 0.41 0.41 0.41 0.82 0.89 0.82 0.89 1 0.89 0.82 0.89 0.82  0.45 0 0.45  0.41 – 0.41  0.45 0  – –  0  –  –  0  –  0  T  0.67 0.07 0.74        =  compet 0.91 0.96 0.85 0.70 0.74 0.63 0.36 0.36 0.3    T    The second neuron won the competition. Looking at the network diagram,  we see that the second neuron’s neighbors, at a radius of 1, include neurons  1 and 3. We must update each of these neurons’ weights with the Kohonen  rule.  =  0 1 0 0 0 0 0 0 0  T  16-30   Solved Problems  w1  1   =  w1  0   p w1  –  +  0     =  +  0.1  w2  1   =  w2  0   p w2  –  +  0     =  +  0.1  0.41 0.41 0.82  0.45  0  0.89              0.67 0.07 0.74  0.67 0.07 0.74  –  –  0.41 0.41 0.82  0.45  0  0.89              =  =  0.43 0.37 0.81  0.47 0.01 0.88  w3  1   =  w3  0   p w3  –  +  0     =  +  0.1  –  0.41 0.41 – 0.82        0.67 0.07 0.74  0.41 0.41 – 0.82        =  0.43 0.36 – 0.81  Figure P16.9 shows the feature map after the weights were updated.  16  P16.5 Given the LVQ network shown in Figure P16.10 and the weight val- ues shown below, draw the regions of the input space that make up  each class.  7  8  9  4  5  6  1  2  3  1  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  -1 -1  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  1  Figure P16.9  Feature Map after Update  W1  =  ,   W2  =  1 0 0 0 0 0 1 0 0 0 0 0 1 1 1  0 0 1 1– 1 1 1 1– 1– 1–  16-31   16 Competitive Networks  Inputs  Competitive Layer  Linear Layer  W1  5 x 2  p  2 x 1  -dist  n1 C5 x 1  a1  5 x 1  n1  i  1  i= -  - w p  1 a  =  compet n     1     a2  3 x 1  n2  3 x 1  W2  3 x 5  2  1 a W a  =  2  2  5  3  Figure P16.10  LVQ Network for Problem P16.5  We create the diagram shown in Figure P16.11 by marking each vector  in  ith column of   wi    of the corresponding nonzero element in the   , which indicates the class.   according to the index   W1  k  W2  Figure P16.11  Prototype Vectors Marked by Class  The decision boundaries separating each class are found by drawing lines  between each pair of prototype vectors, perpendicular to an imaginary line  connecting them and equidistant from each vector.  In Figure P16.12, each convex region is colored according to the weight vec- tor it is closest to.  Figure P16.12  Class Regions and Decision Boundaries  16-32   Solved Problems  P16.6 Design an LVQ network to solve the classification problem shown   in Figure P16.13. The vectors in the diagram are to be classified  into one of three classes, according to their color.   Figure P16.13  Classification Problem  When the design is complete, draw a diagram showing the region  for each class.  We will begin by noting that since LVQ networks calculate the distance be- tween vectors directly, instead of using the inner product, they can classify  vectors that are not normalized, such as those above.  16  Next we will identify each color with a class:    Class 1 will include all white dots.    Class 2 will include all black dots.    Class 3 will include all blue dots.  Now we can choose the dimensions of the LVQ network. Since there are  three classes, the network must have three neurons in its output layer.  There are nine subclasses  i.e., clusters . Therefore the hidden layer must  have nine neurons. This gives us the network shown in Figure P16.14.  Inputs  Competitive Layer  Linear Layer  W1  9 x 2  p  2 x 1  -dist  n1 C9 x 1  a1  9 x 1  a2  3 x 1  n2  3 x 1  W2  3 x 9  2  1 a W a  =  2  2  9  3  Figure P16.14  LVQ Network for Problem P16.6  n1  i  1  i= -  - w p  1 a  =  compet n     1     16-33   16 Competitive Networks  We can now design the weight matrix  row equal to a transposed prototype vector for one cluster. Picking proto- type vectors at the center of each cluster gives us the following values:   of the first layer by setting each   W1  W1  =  1– 0 1 1– 0 1 1– 1 1 1 0 0 0 1–  0 1 1– 1–  T  .  Now each neuron in the first layer will respond to a different cluster.  Next we choose  class. To do this we use the following rule:  W2   so that each subclass is connected to the appropriate   2 If subclass i is to be assigned to class  k, then set wki  1=  .  For example, the first subclass is the top-left cluster in the vector diagram.  The vectors in this cluster are white, so they belong in the first class. There- fore we should set    to one.  2  w1 1  Once we have done this for all nine subclasses we end up with these values:  W2  =  1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1  .  We can test the network by presenting a vector to it. Here we calculate the  output of the first layer for   :  T  =  p  1 0  a1  =  compet n1    =  compet  =  .  5– 2– 1– 2– 1– 0              5–   2–  1–                   0 0 0 0 0 1 0 0 0  16-34  The network says that the vector we presented is in the sixth subclass.  Let’s see what the second layer says.   Solved Problems  a2 W2a1  =  =  1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1  =  1 0 0  0 0 0 0 0 1 0 0 0  The second layer indicates that the vector is in class 1, as indeed it is. The  diagram of class regions and decision boundaries is shown in Figure  P16.15.  16  Figure P16.15  Class Regions and Decision Boundaries  P16.7 Competitive layers and feature maps require that input vectors be   normalized. But what if the available data is not normalized?  One way to handle such data is simply to normalize it before giving  it to the network. This has the disadvantage that the vector magni- tude information, which may be important, is lost.  Another solution is to replace the inner product expression usual- ly used to calculate net input,  with a direct calculation of distance,  a  =  compet Wp      ,  ni  –=  wi  p–   and   a  =  compet n    ,  as is done with the LVQ network. This works and saves the magni- tude information.  However, a third solution is to append a constant of 1 to each input  vector before normalizing it. Now the change in the added element  will preserve the vector magnitude information.  16-35   16 Competitive Networks  Normalize the following vectors using this last method:  First we add an extra element with value 1 to each vector.  Then we normalize each vector.  p1  =  ,   p2  =  ,   p3  =  1 1  1 1 1  0 1  0 1 1  .  0 0  0 0 1  p'1  =  ,   p'2  =  ,   p'3  =  p''1  =  p''2  =      1 1 1  0 1 1  1 1 1  0 1 1  =  =  1 1 1  3 3 3  0 2 2  1 1  p''3  =    0 0 1  0 0 1  =  0 0 1  Now the third element of each vector contains magnitude information,  since it is equal to the inverse of the magnitude of the extended vectors.  16-36   Epilogue  Epilogue  In this chapter we have demonstrated how the associative instar learning  rule of Chapter 15 can be combined with competitive networks, similar to  the Hamming network of Chapter 3, to produce powerful self-organizing  networks. By combining competition with the instar rule, each of the pro- totype vectors that are learned by the network become representative of a  particular class of input vector. Thus the competitive networks learn to di- vide their input space into distinct classes. Each class is represented by one  of the prototype vectors  rows of the weight matrix .  Three types of networks, all developed by Tuevo Kohonen, were discussed  in this chapter. The first is the standard competitive layer. Its simple oper- ation makes it a practical network for many problems.  The self-organizing feature map is very similar to the competitive layer,  but more closely models biological on-center off-surround networks. The re- sult is a network that not only learns to classify input vectors, but also  learns the topology of the input space.  The third network, the LVQ network, uses both unsupervised and super- vised learning to recognize clusters. It uses a second layer to combine mul- tiple convex regions into classes that can have any shape. LVQ networks  can even be trained to recognize classes made up of multiple unconnected  regions.  Chapters 18 and 19 will build on the networks presented in this chapter.  For example, Chapter 18 will carry out a more detailed examination of lat- eral inhibition, on-center off-surround networks and the biology that in- spired them. In Chapter 19 we discuss a modification to the standard  competitive network  called adaptive resonance theory , which solves the  weight stability problem that we discussed in this chapter.  Chapter 22 presents practical tips for training competitive networks, and  Chapter 26 is a case study of using self organizing feature maps on a real- world clustering problem.  16  16-37   16 Competitive Networks  Further Reading  [FrSk91]  J. Freeman and D. Skapura, Neural Networks: Algorithms,  Applications, and Programming Techniques, Reading, MA:  Addison-Wesley, 1991.  This text contains code fragments for network algorithms,  making the details of the networks clear.  [Koho87]  T. Kohonen, Self-Organization and Associative Memory,  2nd Ed., Berlin: Springer-Verlag, 1987.  Kohonen introduces the Kohonen rule and several net- works that use it. It provides a complete analysis of linear  associative models and gives many extensions and exam- ples.  [Hech90]  R. Hecht-Nielsen, Neurocomputing, Reading, MA: Addi- son-Wesley, 1990.  This book contains a section on the history and mathemat- ics of competitive learning.  [RuMc86]  D. Rumelhart, J. McClelland et al., Parallel Distributed  Processing, vol. 1, Cambridge, MA: MIT Press, 1986.  Both volumes of this set are classics in neural network lit- erature. The first volume contains a chapter describing  competitive layers and how they learn to detect features.  16-38   Exercises  Exercises  E16.1 Suppose that the weight matrix for layer 2 of the Hamming network is giv-  en by  W2  =  3 ---– 4  1  3 ---– 4  1  3 ---– 4 3 ---– 4  3 ---– 4 3 ---– 4  1  .    3 ---= 4    1 ------------ 1– S  1 ---= 2  .  p4  p1  This matrix violates Eq.  16.6 , since  Give an example of an output from Layer 1 for which Layer 2 will fail to  operate correctly.  E16.2 Consider the input vectors and initial weights shown in Figure E16.1.  16  p2  1w  p3  3w 2w  Figure E16.1  Cluster Data Vectors  i. Draw the diagram of a competitive network that could classify the  data above so that each of the three clusters of vectors would have  its own class.  ii. Train the network graphically  using the initial weights shown  by   presenting the labeled vectors in the following order:  p1 p2 p3 p4  ,   ,   ,   .  Recall that the competitive transfer function chooses the neuron  with the lowest index to win if more than one neuron has the same   16-39   16 Competitive Networks  net input. The Kohonen rule is introduced graphically in Figure  16.3.  iii. Redraw the diagram in Figure E16.1, showing your final weight vec-  tors and the decision boundaries between each region that repre- sents a class.  E16.3 Train a competitive network using the following input patterns:  p1  =  ,   p2  =  ,   p3  =  1 1–  1 1  .  1– 1–  i. Use the Kohonen learning law with   , and train for one pass  through the input patterns.  Present each input once, in the order  given.  Display the results graphically. Assume the initial weight  matrix is  0.5    =  W  =  2 0 0 2  .  ii. After one pass through the input patterns, how are the patterns   clustered?  In other words, which patterns are grouped together in  the same class?  Would this change if the input patterns were pre- sented in a different order? Explain.  iii. Repeat part  i  using     =  0.25  . How does this change affect the   training?  E16.4 Earlier in this chapter the term “conscience” was used to refer to a tech- nique for avoiding the dead neuron problem plaguing competitive layers  and LVQ networks.  Neurons that are too far from input vectors to ever win the competition can  be given a chance by using adaptive biases that get more negative each  time a neuron wins the competition. The result is that neurons that win  very often start to feel “guilty” until other neurons have a chance to win.  Figure E16.2 shows a competitive network with biases. A typical learning  rule for the bias    of neuron    is  i  bi  new bi  =  0.9bi old – bi  old, if i  i  0.2  , if i  i=  .        16-40   Exercises  Input  Competitive Layer  cid:0  W n cid:0  3 x 2  cid:0 b   cid:0  cid:0  a  cid:0  cid:0  3 x 1 C  cid:0  cid:0   3 x 1  p 2 x 1  1  2  3 x 1  3  a = compet  Wp + b   Figure E16.2  Competitive Layer with Biases  i. Examine the vectors in Figure E16.3. Is there any order in which  the vectors can be presented that will cause   to win the competi- tion and move closer to one of the vectors?  Note: assume that adap- tive biases are not being used.   w1  16  p2  3w  p3  2w p1  Figure E16.3  Input Vectors and Dead Neuron  ii. Given the input vectors and the initial weights and biases defined  below, calculate the weights  using the Kohonen rule  and the bias- es  using the above bias rule . Repeat the sequence shown below un- til neuron 1 wins the competition.  p1  =  ,   p2  =  ,   p3  =  1– 0  1 1  2 2  1w  0 1  w1  =  ,   w2  =  0 1–  5– 2 5– 1  ,   w3  =  5– 1 5– 2  ,   b1 0   =  b2 0   =  b3 0   =  0  Sequence of input vectors:   p1 p2 p3 p1 p2 p3   ,   ,   ,   ,   ,   ,   16-41   16 Competitive Networks  iii. How many presentations occur before    wins the competition?  w1  E16.5 The net input expression for LVQ networks calculates the distance be-  tween the input and each weight vector directly, instead of using the inner  product. The result is that the LVQ network does not require normalized  input vectors. This technique can also be used to allow a competitive layer  to classify nonnormalized vectors. Such a network is shown in Figure  E16.4.  Inputs  Competitive Layer  W  2 x 2  p  2 x 1  -dist  n1 C2 x 1  a1  2 x 1  2  2  n1  i  i= -  - w p  1 a  =  compet n     1     Figure E16.4  Competitive Layer with Alternate Net Input Expression  Use this technique to train a two-neuron competitive layer on the  nonnor- malized  vectors below, using a learning rate,   , of 0.5.    p1  =  ,   p2  =  ,   p3  =  1– 2  1 1  2– 2–  Present the vectors in the following order:  p1 p2 p3 p2 p3 p1  ,   ,   ,   ,   ,   .  Here are the initial weights of the network:  w1  =  ,   w2  =  0 1  .  1 0  E16.6 Repeat E16.5 for the following inputs and initial weights. Show the move- ments of the weights graphically for each step. If the network is trained for  a large number of iterations, how will the three vectors be clustered in the  final configuration?  16-42   Exercises  p1  =  ,   p2  =  ,   p3  =  2 0  0 1  2 2  w1  =  ,   w2  =  .  1– 0  1 0  0 2  E16.7 We have a competitive learning problem, where the input vectors are  p1  =  ,   p2  =  ,   p3  =  ,   p4  =  1 1  ,  2 2  0 1  and the initial weight matrix is  W  =  1 1–  1– 1  .  16  i. Use the Kohonen learning law to train a competitive network using  .  Present each vector once, in the order   a learning rate of  shown.  Use the modified competitive network of Figure E16.4,  which uses negative distance, instead of inner product.  0.5    =  ii. Display the results of part i graphically, as in Figure 16.3.  Show all   four iterations.   iii. Where will the weights eventually converge  approximately ? Ex-  plain. Sketch the approximate final decision boundaries.  E16.8 Show that the modified competitive network of Figure E16.4, which com- putes distance directly, will produce the same results as the standard com- petitive network, which uses the inner product, when the input vectors are  normalized.   E16.9 We would like a classifier that divides the interval of the input space de-  fined below into five classes.  0    p1    1  » 2 + 2 ans =       4  i. Use MATLAB to randomly generate 100 values in the interval   shown above with a uniform distribution.  ii. Square each number so that the distribution is no longer uniform.  iii. Write a MATLAB M-file to implement a competitive layer. Use the  M-file to train a five-neuron competitive layer on the squared values   16-43   16 Competitive Networks  until its weights are fairly stable.  iv. How are the weight values of the competitive layer distributed? Is  there some relationship between how the weights are distributed  and how the squared input values are distributed?  E16.10 We would like a classifier that divides the square region defined below into   sixteen classes of roughly equal size.  » 2 + 2 ans =       4  above.  i. Use MATLAB to randomly generate 200 vectors in the region shown   0    p1    1  ,   2    p2    3  ii. Write a MATLAB M-file to implement a competitive layer with Ko- honen learning. Calculate the net input by finding the distance be- tween the input and weight vectors directly, as is done by the LVQ  network, so the vectors do not need to be normalized. Use the M-file  to train a competitive layer to classify the 200 vectors. Try different  learning rates and compare performance.  iii. Write a MATLAB M-file to implement a four-neuron by four-neuron   two-dimensional  feature map. Use the feature map to classify the  same vectors. Use different learning rates and neighborhood sizes,  then compare performance.  E16.11 We want to train the following 1-D feature map  which uses distance in-  stead of inner product to compute the net input :  Inputs  Feature Map  Feature Map  W  4 x 2  p 2 x 1  -dist  n 4 x 1  a C 4 x 1  2  4  1  2  3  4  iw p  -   n = - i a  =      compet n  Figure E16.5  1-D Feature Map for Exercise E16.11  The initial weight matrix is   W 0   =  2 1– 2 1  1– 1 2– 0  T  .  16-44   Exercises  i. Plot the initial weight vectors as dots, and connect the neighboring  weight vectors as lines  as in Figure 16.10, except that this is a 1-D  feature map .  ii. The following input vector is applied to the network. Perform one it- eration of the feature map learning rule.  You can do this graphical- ly.  Use a neighborhood size of 1 and a learning rate of   0.5    =  .  p1  =  2– 0  T  iii. Plot the new weight vectors as dots, and connect the neighboring   weight vectors as lines.  E16.12 Consider the following feature map, where distance is used instead of inner   product to compute the net input.  Inputs  Feature Map  Feature Map  W  4 x 2  p 2 x 1  -dist  n 4 x 1  2  ni  i= -   -w p    a  =      compet n  a 4 x 1  C  4  1  3  2  4  Figure E16.6  2-D Feature Map for Exercise E16.12  The initial weight matrix is   W  =  T  0 1 1 0 0 0 1 1–  i. Plot the initial weights, and show their topological connections, as   in Figure 16.10.  ii. Apply the input   p  =  T  1– 1  ture map learning rule, with learning rate of  hood radius of 1.  , and perform one iteration of the fea- , and neighbor-  0.5    =  iii. Plot the weights after the first iteration, and show their topological   connections.  16-45  16   16 Competitive Networks  E16.13 An LVQ network has the following weights:  W1  =  ,   W2  =  1 0 0 0 0 0 1 1 0 0 0 0 0 1 1  .  0 0 1 0 1– 0 0 1 0 1–  i. How many classes does this LVQ network have? How many sub-  classes?  ii. Draw a diagram showing the first-layer weight vectors and the de-  cision boundaries that separate the input space into subclasses.  iii. Label each subclass region to indicate which class it belongs to.  E16.14 We would like an LVQ network that classifies the following vectors accord-  ing to the classes indicated:  class 1:   , class 2:   , class 3:           1– 1 1–  1 1– 1–                1– 1– 1    1 1– 1  1 1 1–                1– 1– 1–  1– 1 1  .        i. How many neurons are required in each layer of the LVQ network?  ii. Define the weights for the first layer.  iii. Define the weights for the second layer.  iv. Test your network for at least one vector from each class.  E16.15 We would like an LVQ network that classifies the following vectors accord-  ing to the classes indicated:  class 1:   p1  =    p2  =  , class 2:   p3  =    p4  =      1 1  0 2          1– 1  1 2      i. Could this classification problem be solved by a perceptron? Explain   your answer.  ii. How many neurons must be in each layer of an LVQ network that  can classify the above data, given that each class is made up of two  convex-shaped subclasses?  iii. Define the second-layer weights for such a network.  16-46   Exercises  iv. Initialize the first-layer weights of the network to all zeros and cal- culate the changes made to the weights by the Kohonen rule  with  a learning rate    of 0.5  for the following series of vectors:    p4 p2 p3 p1 p2  ,   ,   ,   ,   .  v. Draw a diagram showing the input vectors, the final weight vectors   and the decision boundaries between the two classes.  E16.16 An LVQ network has the following weights and training data.      p1  =  2– 2  1 0      t1  =  ,   p2  =  t2  =  0 1          ,   p3  =  2 2–  t3  =  1 0  ,       16  W1  =  ,   W2  =  1 1 0 0 0 1  ,  1 0 0 1 0 0          2 0  2– 0  p4  =  t4  =  0 1      i. Plot the training data input vectors and weight vectors  as in Figure   16.14 .  ii. Perform four iterations of the LVQ learning rule, with learning rate  p1 ,    one iteration for each input . Do this graphically, on a   , as you present the following sequence of input vectors:    p2 p3 p4 separate diagram from part i.  0.5 ,   = ,   iii. After completing the iterations in part ii, on a new diagram, sketch  the regions of the input space that make up each subclass and each  class. Label each region to indicate which class it belongs to.  E16.17 An LVQ network has the following weights:  W1  =  0 1 1– 0 0 0 0 0 1 1–  1– 1–  1– 1  T  ,   W2  =  1 0 1 0 1 1 0 0 1 0 1 0 0 1  .  i. How many classes does this LVQ network have? How many sub-  classes?  ii. Draw a diagram showing the first-layer weight vectors and the de-  cision boundaries that separate the input space into subclasses.  iii. Label each subclass region to indicate which class it belongs to.  16-47   16 Competitive Networks  iv. Suppose that an input   p  =  network. Perform one iteration of the LVQ algorithm, with    from Class 1 is presented to the  . 0.5    =  T  1 0.5  E16.18 An LVQ network has the following weights:  W1  =  0 0 2 1 1 0 2 2 1 1–  1– 1–  T  ,   W2  =  1 1 1 0 0 0 0 0 0 1 1 1  .  i. How many classes does this LVQ network have? How many sub-  classes?  ii. Draw a diagram showing the first-layer weight vectors and the de-  cision boundaries that separate the input space into subclasses.  iii. Label each subclass region to indicate which class it belongs to.  iv. Perform one iteration of the LVQ algorithm, with the following in-  1–  2–  T  ,   t  =  T  1 0  . Use learning rate   put target pair:    0.5  =  .  p  =  16-48   Objectives  17 Radial Basis Networks  Objectives  Objectives  Theory and Examples   Radial Basis Network   Function Approximation  Pattern Classification  Global vs. Local   Training RBF Networks   Linear Least Squares  Orthogonal Least Squares  Clustering  Nonlinear Optimization  Other Training Techniques   Summary of Results  Solved Problems  Epilogue  Further Reading  Exercises   17-1 17-2 17-2 17-4 17-6 17-9 17-9 17-11 17-17 17-22 17-23 17-25 17-26 17-29 17-33 17-34 17-36  17  The multilayer networks discussed in Chapters 11 and 12 represent one  type of neural network structure for function approximation and pattern  recognition. As we saw in Chapter 11, multilayer networks with sigmoid  transfer functions in the hidden layers and linear transfer functions in the  output layer are universal function approximators. In this chapter we will  discuss another type of universal approximation network, the radial basis  function network. This network can be used for many of the same applica- tions as multilayer networks.  This chapter will follow the structure of Chapter 11. We will begin by dem- onstrating, in an intuitive way, the universal approximation capabilities of  the radial basis function network. Then we will describe three different  techniques for training these networks. They can be trained by the same  gradient-based algorithms discussed in Chapters 11 and 12, with deriva- tives computed using a form of backpropagation. However, they can also be  trained using a two-stage process, in which the first layer weights are com- puted independently from the weights in the second layer. Finally, these  networks can be built in an incremental way - one neuron at a time.  17-1   17 Radial Basis Networks  Theory and Examples  The radial basis function network is related to the multilayer perceptron  network of Chapter 11. It is also a universal approximator and can be used  for function approximation or pattern recognition. We will begin this chap- ter with a description of the network and a demonstration of its abilities for  function approximation and pattern recognition.  The original work in radial basis functions was performed by Powell and  others during the 1980’s [Powe87]. In this original work, radial basis func- tions were used for exact interpolation in a multidimensional space. In oth- er words, the function created by the radial basis interpolation was  required to pass exactly through all targets in the training set. The use of  radial basis functions for exact interpolation continues to be an important  application area, and it is also an active area of research.   For our purposes, however, we will not be considering exact interpolation.  Neural networks are often used on noisy data, and exact interpolation often  results in overfitting when the training data is noisy, as we discussed in  Chapter 13. Our interest is in the use of radial basis functions to provide  robust approximations to unknown functions based on generally limited  and noisy measurements. Broomhead and Lowe [BrLo88] were the first to  develop the radial basis function neural network model, which produces a  smooth interpolating function. No attempt is made to force the network re- sponse to exactly match target outputs. The emphasis is on producing net- works that will generalize well to new situations.  In the next section we will demonstrate the capabilities of the radial basis  function neural network. In the following sections we will describe proce- dures for training these networks.   Radial Basis Network  RBF  The radial basis network is a two-layer network. There are two major dis- tinctions between the radial basis function  RBF  network and a two layer  perceptron network. First, in layer 1 of the RBF network, instead of per- forming an inner product operation between the weights and the input   matrix multiplication , we calculate the distance between the input vector  and the rows of the weight matrix.  This is similar to the LVQ network  shown in Figure 16.13.  Second, instead of adding the bias, we multiply by  the bias. Therefore, the net input for neuron i in the first layer is calculated  as follows:  1 ni  =  p w1  i–  1 bi  .   17.1   17-2   Radial Basis Network  Each row of the weight matrix acts as a center point - a point where the net  input value will be zero. The bias performs a scaling operation on the trans- fer  basis  function, causing it to stretch or compress.   We should note that most papers and texts on RBF networks use the terms  standard deviation, variance or spread constant, rather than bias. We have  used the bias in order to maintain a consistency with other networks in this  text. This is simply a matter of notation and pedagogy. The operation of the  network is not affected. When a Gaussian transfer function is used, the  bias is related to the standard deviation as follows:   1  2  =  b      .    The transfer functions used in the first layer of the RBF network are dif- ferent than the sigmoid functions generally used in the hidden layers of  multilayer perceptrons  MLP . There are several different types of transfer  function that can be used  see [BrLo88] , but for clarity of presentation we  will consider only the Gaussian function, which is the one most commonly  used in the neural network community. It is defined as follows  e n2–=  ,  a   17.2   and it is plotted in Figure 17.1.   17  e n2–=  a  a  1.0  0.8   0.6   0.4   0.2   0     −3  17-3  −2  −1  1  2  3  0  n  Figure 17.1  Gaussian Basis Function  Local Function  Global Function  A key property of this function is that it is local. This means that the output  is close to zero if you move very far in either direction from the center point.  This is in contrast to the global sigmoid functions, whose output remains  close to 1 as the net input goes to infinity.   The second layer of the RBF network is a standard linear layer:  a2 W2a1 b2+  =   17.3    17 Radial Basis Networks  Figure 17.2 shows the complete RBF network.  Inputs  Radial Basis Layer  Linear Layer  W1  S 1 x R  p1  R x 11  dist  1  R 1  b1  S 1 x 1  n1  S 1 x 1  .*  a1  S 1 x 1  1  S 1  W2  S 2 x S 1  b2  S 2 x 1  a2  S 2 x 1  n2  S 2 x 1  S 2  1  a  i  =  radbas     1  -  w p i  b  1     i  2  a W a b  =  +  2  1  2  Figure 17.2  Radial Basis Network  Function Approximation This RBF network has been shown to be a universal approximator  [PaSa93], just like the MLP network. To illustrate the capability of this  network, consider a network with two neurons in the hidden layer, one out- put neuron, and with the following default parameters:  1  w1 1  1–=  ,   1  w2 1  1=  ,   1 b1  2=  ,   1 b2  2=  ,  2  w1 1  1=  ,   2  w1 2  1=  ,   b2  0=  .  The response of the network with the default parameters is shown in Fig- ure 17.3, which plots the network output   is varied over  the range    as the input   a2  2  2–  p      .  Notice that the response consists of two hills, one for each of the Gaussian  neurons  basis functions  in the first layer. By adjusting the network pa- rameters, we can change the shape and location of each hill, as we will see  in the following discussion.  As you proceed through this example, it may  be helpful to compare the response of this sample RBF network with the  response of the sample MLP network in Figure 11.5.   17-4   Radial Basis Network  a2  2  1  0  2  1  0  2  1  0  −1 −2  −1  1  2  0  p  Figure 17.3  Default Network Response  Figure 17.4 illustrates the effects of parameter changes on the network re- sponse. The blue curve is the nominal response. The other curves corre- spond to the network response when one parameter at a time is varied over  the following ranges:  1    0 w2 1    2  ,   1–    2  w1 1    1  ,   0.5    1 b2    8  ,   1–    b2    1  .   17.4   17  −1 −2  −1  1  2  −1 −2  −1  1  2  2  1  0  2  1  0  1  w2 1  0   b  b2  0   d   1 b2  0   a   2  w1 1  0   c   17-5  −1 −2  −1  1  2  −1 −2  −1  1  2  Figure 17.4  Effect of Parameter Changes on Network Response  Figure 17.4  a  shows how the network biases in the first layer can be used  to change the width of the hills - the larger the bias, the narrower the hill.  Figure 17.4  b  illustrates how the weights in the first layer determine the    17 Radial Basis Networks  location of the hills; there will be a hill centered at each first layer weight.  For multidimensional inputs there will be a hill centered at each row of the  weight matrix. For this reason, each row of the first layer weight matrix is  often called the center for the corresponding neuron  basis function .   Center  Notice that the effects of the weight and the bias in first layer of the RBF  network are much different than for the MLP network, which was shown  in Figure 11.6. In the MLP network, the sigmoid functions create steps.  The weights change the slopes of the steps, and the biases change the loca- tions of the steps.  Figure 17.4  c  illustrates how the weights in the second layer scale the  height of the hills. The bias in the second layer shifts the entire network  response up or down, as can be seen in Figure 17.4  d . The second layer of  the RBF network is the same type of linear layer used in the MLP network  of Figure 11.6, and it performs a similar function, which is to create a  weighted sum of the outputs of the layer 1 neurons.  This example demonstrates the flexibility of the RBF network for function  approximation. As with the MLP, it seems clear that if we have enough  neurons in the first layer of the RBF network, we can approximate virtual- ly any function of interest, and [PaSa93] provides a mathematical proof  that this is the case. However, although both MLP and RBF networks are  universal approximators, they perform their approximations in different  ways. For the RBF network, each transfer function is only active over a  small region of the input space - the response is local. If the input moves  far from a given center, the output of the corresponding neuron will be close  to zero. This has consequences for the design of RBF networks. We must  have centers adequately distributed throughout the range of the network  inputs, and we must select biases in such a way that all of the basis func- tions overlap in a significant way.  Recall that the biases change the width  of each basis function.  We will discuss these design considerations in more  detail in later sections.  To experiment with the response of this RBF network, use the MATLAB®  Neural Network Design Demonstration RBF Network Function  nnd17nf .  2 2+  Pattern Classification To illustrate the capabilities of the RBF network for pattern classification,  consider again the classic exclusive-or  XOR  problem. The categories for  the XOR gate are  Category 1:   p2  =    p3  =  , Category 2:   p1  =    p4  =      1– 1  1 1–          1– 1–  1 1  .      17-6   p2  p1  p4  p3  Radial Basis Network  The problem is illustrated graphically in the figure to the left. Because the  two categories are not linearly separable, a single-layer network cannot  perform the classification.  RBF networks can classify these patterns. In fact, there are many different  RBF solutions. We will consider one solution that demonstrates in a simple  way how to use RBF networks for pattern classification. The idea will be to  have the network produce outputs greater than zero when the input is near  patterns  , and outputs less than zero for all other inputs.  Note  that the procedures we will use to design this example network are not  suitable for complex problems, but they will help us illustrate the capabil- ities of the RBF network.    or   p3  p2  From the problem statement, we know that the network will need to have  two inputs and one output. For simplicity, we will use only two neurons in  the first layer  two basis functions , since this will be sufficient to solve the  XOR problem. As we discussed earlier, the rows of the first-layer weight  matrix will create centers for the two basis functions. We will choose the  centers to be equal to the patterns  . By centering a basis function  at each pattern, we can produce maximum network outputs there. The first  layer weight matrix is then   and   p2  p3  W1  =  T p2 T p3  =  1– 1 1 1–  .   17.5   17  The choice of the bias in the first layer depends on the width that we want  for each basis function. For this problem, we would like the network func- tion to have two distinct peaks at  . Therefore, we don’t want the  basis functions to overlap too much. The centers of the basis functions are  each a distance of   from the origin. We want the basis function to drop  significantly from its peak in this distance. If we use a bias of 1, we would  get the following reduction in that distance:   and   p2  p3  2  a  =  e n2–   – e 1  =  2  2  =  e 2–  =  0.1353  .   17.6   Therefore, each basis function will have a peak of 1 at the centers, and will  drop to 0.1353 at the origin. This will work for our problem, so we select the  first layer bias vector to be  b1  =  .  1 1   17.7   The original basis function response ranges from 0 to 1  see Figure 17.1 .  We want the output to be negative for inputs much different than   and  p3 , so we will use a bias of -1 for the second layer, and we will use a value   p2  17-7   17 Radial Basis Networks  of 2 for the second layer weights, in order to bring the peaks back up to 1.  The second layer weights and biases then become  W2  =  2 2  ,   b2  1–=  .   17.8   For the network parameter values given in  17.5 ,  17.7  and  17.8 , the net- work response is shown in Figure 17.5. This figure also shows where the  surface intersects the plane at  , which is where the decision takes  place. This is also indicated by the contours shown underneath the surface.  These are the function contours where  . They are almost circles that   vectors. This means that the network output will  surround the  be greater than 0 only when the input vector is near the   vectors.   and    and   0=  0=  p3  p2  a2  a2  p2  p3  2  1  p2  0  −1  −2  −3  −3  −2  0  −1  1  p1  3  2  Figure 17.5  Example 2-Input RBF Function Surface  Figure 17.6 illustrates more clearly the decision boundaries. Whenever the  input falls in the blue regions, the output of the network will be greater  than zero. Whenever the network input falls outside the blue regions, the  network output will be less than zero.  a2  0.5  1  0  −0.5  −1  −1.5  −2 3  17-8   Radial Basis Network  p2  3  2  1  0  −1  −2  −3 −3  17  −2  −1  1  2  3  0  p1  Figure 17.6  RBF Example Decision Regions  This network, therefore, classifies the patterns correctly. It is not the best  solution, in the sense that it does not always assign input patterns to the  closest prototype vector, unlike the MLP solution shown in Figure 11.2.  You will notice that the decision regions for this RBF network are circles,  unlike the linear boundaries that we see in single layer perceptrons. The  MLP can put linear boundaries together to create arbitrary decision re- gions. The RBF network can put circular boundaries together to create ar- bitrary decision regions. In this problem, the linear boundaries are more  efficient. Of course, when many neurons are used, and the centers are close  together, the elementary RBF boundaries are no longer purely circular,  and the elementary MLP boundaries are no longer purely linear. However,  associating circular boundaries with RBF networks and linear boundaries  with MLP networks can be helpful in understanding their operation as pat- tern classifiers.  To experiment with the RBF network for pattern classification, use the  MATLAB® Neural Network Design Demonstration RBF Pattern Classifica- tion  nnd17pc .  Now that we see the power of RBF networks for function approximation  and pattern recognition, the next step is to develop general training algo- rithms for these networks.  Global vs. Local Before we discuss the training algorithms, we should say a final word about  the advantages and disadvantages of the global transfer functions used by  the MLP networks and the local transfer functions used by the RBF net- works. The MLP creates a distributed representation, because all of the  transfer functions overlap in their activity. At any given input value, many   17-9   17 Radial Basis Networks  sigmoid functions in the first layer will have significant outputs. They must  sum or cancel in the second layer in order to produce the appropriate re- sponse at each point. In the RBF network, each basis function is only active  over a small range of the input. For any given input, only a few basis func- tions will be active.  There are advantages and disadvantages to each approach. The global ap- proach tends to require fewer neurons in the hidden layer, since each neu- ron contributes to the response over a large part of the input space. For the  RBF network, however, basis centers must be spread throughout the range  of the input space in order to provide an accurate approximation. This leads  to the problem of the “curse of dimensionality,” which we will discuss in the  next section. Also, if more neurons, and therefore more parameters, are  used, then there is a greater likelihood that the network will overfit the  training data and fail to generalize well to new situations.  On the other hand, the local approach generally leads to faster training, es- pecially when the two-stage algorithms, which will be discussed in the next  section, are used. Also, the local approach can be very useful for adaptive  training, in which the network continues to be incrementally trained while  it is being used, as in adaptive filters  nonlinear versions of the filters in  Chapter 10  or controllers. If, for a period of time, training data only ap- pears in a certain region of the input space, then a global representation  will tend to improve its accuracy in those regions at the expense of its rep- resentation in other regions. Local representations will not have this prob- lem to the same extent. Because each neuron is only active in a small region  of the input space, its weights will not be adjusted if the input falls outside  that region.  Unlike the MLP network, which is almost always trained by some gradi- ent-based algorithm  steepest descent, conjugate gradient, Levenberg- Marquardt, etc. , the RBF network can be trained by a variety of approach- es.   RBF networks can be trained using gradient-based algorithms. However,  because of the local nature of the transfer function and the way in which  the first layer weights and biases operate, there tend to be many more un- satisfactory local minima in the error surfaces of RBF networks than in  those of MLP networks. For this reason, gradient-based algorithms are of- ten unsatisfactory for the complete training of RBF networks. They are  used on occasion, however, for fine-tuning of the network after it has been  initially trained using some other method. Later in this chapter we will dis- cuss the modifications to the backpropagation equations in Chapter 11 that  are needed to compute the gradients for RBF networks.  The most commonly used RBF training algorithms have two stages, which  treat the two layers of the RBF network separately. The algorithms differ   17-10  Training RBF Networks   Curse of Dimensionality  Training RBF Networks  mainly in how the first layer weights and biases are selected. Once the first  layer weights and biases have been selected, the second layer weights can  be computed in one step, using a linear least-squares algorithm. We will  discuss linear least squares in the next section.  The simplest of the two-stage algorithms arranges the centers  first layer  weights  in a grid pattern throughout the input range and then chooses a  constant bias so that the basis functions have some degree of overlap. This  procedure is not optimal, because the most efficient approximation would  place more basis functions in regions of the input space where the function  to be approximated is most complex. Also, for many practical cases the full  range of the input space is not used, and therefore many basis functions  could be wasted. One of the drawbacks of the RBF network, especially  when the centers are selected on a grid, is that they suffer from the curse  of dimensionality. This means that as the dimension of the input space in- creases, the number of basis functions required increases geometrically.  For example, suppose that we have one input variable, and we specify a  grid of 10 basis functions evenly spaced across the range of that input vari- able. Now, increase the number of input variables to 2. To maintain the  same grid coverage for both input variables, we would need 102, or 100 ba- sis functions.  Another method for selecting the centers is to select some random subset  of the input vectors in the training set. This ensures that basis centers will  be placed in areas where they will be useful to the network. However, due  to the randomness of the selection, this procedure is not optimal. A more  efficient approach is to use a method such as the Kohonen competitive lay- er or the feature map, described in Chapter 16, to cluster the input space.  The cluster centers then become basis function centers. This ensures that  the basis functions are placed in regions with significant activity. We will  discuss this method in a later section.  A final procedure that we will discuss for RBF training is called orthogonal  least squares. It is based on a general method for building linear models  called subset selection. This method starts with a large number of possible  centers—typically all of the input vectors from the training data. At each  stage of the procedure, it selects one center to add to the first layer weight.  The selection is based on how much the new neuron will reduce the sum  squared error. Neurons are added until some criteria is met. The criteria is  typically chosen to maximize the generalization capability of the network.  Linear Least Squares In this section we will assume that the first layer weights and biases of the  RBF network are fixed. This can be done by fixing the centers on a grid, or  by randomly selecting the centers from the input vectors in the training  data set  or by using the clustering method which is described in a later sec- tion . When the centers are randomly selected, all of the biases can be com- puted using the following formula [Lowe89]:  17-11  17   17 Radial Basis Networks  1 bi  =  S1 ---------- dmax  ,   17.9   dmax   is the maximum distance between neighboring centers. This is   where  designed to ensure an appropriate degree of overlap between the basis  functions. Using this method, all of the biases have the same value. There  are other methods which use different values for each bias. We will discuss  one such method later, in the section on clustering.  Once the first layer parameters have been set, the training of the second  layer weights and biases is equivalent to training a linear network, as in  Chapter 10. For example, consider that we have the following training  points  p1 t1 { , }    {  p2 t2  , }   pQ tQ  {      ,  }  ,   17.10   pq   is an input to the network, and   where  output. The output of the first layer for each input  can be computed as  tq   is the corresponding target   in the training set   pq  1 ni q  =  pq w1  i–  1 bi  ,  1 aq  =  1 radbas nq    .   17.11    17.12    17.13    17.14    17.15   Since the first layer weights and biases will not be adjusted, the training  data set for the second layer then becomes  1 t2 a2 The second layer response is linear:  1 t1 a1            aQ       1 tQ    .  a2 W2a1 b2+  =  .  We want to select the weights and biases in this layer to minimize the sum  square error performance index over the training set:  F x   =    2– tq aq  T tq aq 2–     Q  q  1=  Our derivation of the solution to this linear least squares problem will fol- low the linear network derivation starting with Eq.  10.6 . To simplify the  discussion, we will assume a scalar target, and we will lump all of the pa- rameters we are adjusting, including the bias, into one vector:  17-12   Similarly, we include the bias input “1” as a component of the input vector  Training RBF Networks  x  =  w2 1 b2  .  zq  =  .  1 aq 1  2 aq  =  w2  1  1  Taq  b2+  ,  aq  =  xTzq  .  Now the network output, which we usually write in the form  can be written as  This allows us to conveniently write out an expression for the sum square  error:  F x   =  eq  2  =  aq– tq  2  =  – tq  xTzq  2  .   17.20   Q    q  1=  Q    q  1=  Q    q  1=  17  To express this in matrix form, we define the following matrices:  t  =  ,   U  =  ,   e  =   17.21   t1 t2  tQ  uT T z1 1 uT T z2 2   T uT zQ  =  Q  .  e1 e2  eQ  The error can now be written  and the performance index become  e  –=  t Ux  ,  F x   =  t Ux –  T t Ux  –    .  If we use regularization, as we discussed in Chapter 13, to help in prevent- ing overfitting, we obtain the following form for the performance index:   17.16    17.17    17.18    17.19    17.22    17.23   17-13   17 Radial Basis Networks  F x   =  t Ux –  T t Ux  –  =  t Ux –  T t Ux  –   xTx +  ,   17.24   n  +    2 xi  i  1=  where     =      from Eq.  13.4 . Let’s expand this expression to obtain  F x   =  =  t Ux – tTt  – 2tTUx  T t Ux  xTx +   tTt = xT UTU I+ x  +  –  –  2tTUx  +  xTUTUx xTx  +   17.25   Take a close look at Eq.  17.25 , and compare it with the general form of the  quadratic function, given in Eq.  8.35  and repeated here:  F x   =  c dTx +  +  1 ---xTAx 2  .   17.26   Our performance function is a quadratic function, where  c  =  tTt  ,   d  –=  2UTt   and   A  =  2 UTU I+     .   17.27   From Chapter 8 we know that the characteristics of the quadratic function  depend primarily on the Hessian matrix  . For example, if the eigenvalues  of the Hessian are all positive, then the function will have one unique glo- bal minimum.   A    2 UTU I+   In this case the Hessian matrix is  , and it can be shown that  this matrix is either positive definite or positive semidefinite  see Exercise  E17.4 , which means that it can never have negative eigenvalues. We are  left with two possibilities. If the Hessian matrix has only positive eigenval- ues, the performance index will have one unique global minimum  see Fig- ure 8.7 . If the Hessian matrix has some zero eigenvalues, the performance  index will either have a weak minimum  see Figure 8.9  or no minimum   see Problem P8.7 , depending on the vector  minimum, since   . In this case, it must have a   is a sum square function, which cannot be negative.  d  F x   Now let’s locate the stationary point of the performance index. From our  previous discussion of quadratic functions in Chapter 8, we know that the  gradient is  F x   =   c dTx  +  +     1  ---xTAx  2  =  d Ax+  =  –  2UTt  +  2 UTU I+   x  .  17.28   The stationary point of  zero:  F x    can be found by setting the gradient equal to   –  2ZTt  +  2 UTU I+   x   UTU I+       x  =  UTt  .   17.29   Therefore, the optimum weights    can be computed from     0= x  17-14   Training RBF Networks  If the Hessian matrix is positive definite, there will be a unique stationary  point, which will be a strong minimum:    UTU I+  x  =  UTt  .  x  =    UTU I+   1– UTt   17.30    17.31   Let’s demonstrate this procedure with a simple problem.   2 2+  To illustrate the least squares algorithm, let’s choose a network and apply  it to a particular problem. We will use an RBF network with three neurons  in the first layer to approximate the following function  Example     ---p  4  g p   =  1  sin+   for   2–     p  2  .   17.32   To obtain our training set we will evaluate this function at six values of   p  :  This produces the targets  p  =     2–   1.2–  0.4–    0.4 1.2 2        .  t  =     0 0.19 0.69 1.3 1.8 2            .   17.33    17.34   17  We will choose the basis function centers to be spaced equally throughout  the input range: -2, 0 and 2. For simplicity, we will choose the bias to be the  reciprocal of the spacing between points. This produces the following first  layer weight and bias.  W1  =  ,   b1  =  2– 0 2  .  0.5 0.5 0.5  The next step is to compute the output of the first layer, using the following  equations.  1 ni q  =  pq w1  i–  1 bi  ,  =  1 radbas nq    .  1 aq a1  This produces the following    vectors   17.35    17.36    17.37   17-15   17 Radial Basis Networks  a1  =        1    0.368 0.018  0.852 0.698 0.077    0.527 0.961 0.237    0.237 0.961 0.527    0.077 0.698 0.852  0.018 0.368    1         17.38   We can use Eq.  17.17  and Eq.  17.21  to create the U and t matrices  The next step is to solve for the weights and biases in the second layer us- ing Eq.  17.30 . We will begin with the regularization parameter set to zero.  UT  =  1  0.852 0.527 0.237 0.077 0.018 0.368 0.698 0.961 0.961 0.698 0.368 0.018 0.077 0.237 0.527 0.852  ,  1  1  1  1  1  1 1  tT  =  0 0.19 0.69 1.3 1.8 2  .  x  =    UTU I+   1– UTt  1–  =  2.07 1.76 0.42 2.71 1.76 3.09 1.76 4.05 0.42 1.76 2.07 2.71 2.71 4.05 2.71  6  1.01 4.05 4.41  6  =  –  1.03 0  1.03  1   17.39    17.40    17.41   The second layer weight and bias are therefore  W2  =  –  1.03  0 1.03  ,   b2  1=  .   17.42   Figure 17.7 illustrates the operation of this RBF network. The blue line  represents the RBF approximation, and the circles represent the six data  points. The dotted lines in the upper axis represent the individual basis  functions scaled by the corresponding weights in the second layer  includ- ing the constant bias term . The sum of the dotted lines will produce the  blue line. In the small axis at the bottom, you can see the unscaled basis  functions, which are the outputs of the first layer.  17-16   −0.8  −0.6  −0.4  −0.2  0  0.2  0.4  0.6  0.8  −0.8  −0.6  −0.4  −0.2  0.2  0.4  0.6  0.8  0  p  Figure 17.7  RBF Sine Approximation  The RBF network design process can be sensitive to the choice of the center  locations and the bias. For example, if we select six basis functions and six  data points, and if we choose the first layer biases to be 8, instead of 0.5,  then the network response will be as shown in Figure 17.8. The spread of  the basis function decreases as the inverse of the bias. When the bias is this  large, there is not sufficient overlap in the basis functions to provide a  smooth approximation. We match each data point exactly. However, be- cause of the local nature of the basis function, the approximation to the  true function is not accurate between the training data points.  17  Training RBF Networks  2.5  1.5  2  1  0  0.5  −0.5  −1  −1.5 −1  1  0.5  0 −1  a2  a1  1.5  0.5  2  1  0  −0.5  −1 −2  1  0.5  0 −2  a2  a1  1  1  2  2  −1.5  −1  −0.5  0  0.5  −1.5  −1  −0.5  0.5  0  p  1  1  1.5  1.5  Figure 17.8  RBF Response with Bias Too Large  17-17   17 Radial Basis Networks  To experiment with the linear least squares fitting, use the MATLAB® Neu- ral Network Design Demonstration RBF Linear Least Squares  nnd17lls .  Orthogonal Least Squares In the previous section we assumed that the weights and biases in the first  layer were fixed.  For example, the centers could be fixed on a grid, or ran- domly selected from the input vectors in the training set.  In this section  we consider a different approach for selecting the centers. We will assume  that there exists a number of potential centers. These centers could include  the entire set of input vectors in the training set, vectors chosen in a grid  pattern, or vectors chosen by any other procedure one might think of. We  will then select vectors one at a time from this set of potential centers, until  the network performance is satisfactory. We will build up the network one  neuron at a time.  The basic idea behind this method comes from statistics, and it is called  subset selection [Mill90]. The general objective of subset selection is to  choose an appropriate subset of independent variables to provide the most  efficient prediction of a target dependent variable. For example, suppose  that we have 10 independent variables, and we want to use them to predict  our target dependent variable. We want to create the simplest predictor  possible, so we want to use the minimum number of independent variables  for the prediction. Which subset of the 10 independent variables should we  use? The optimal approach, called an exhaustive search, tries all combina- tions of subsets and finds the smallest one that provides satisfactory per- formance.  We will define later what we mean by satisfactory  performance.   Unfortunately, this strategy is not practical. If we have Q variables in our  original set, the following expression gives the number of distinct subsets:  Q    q  1=  Q!  ------------------------- ! q! Q q–    .   17.43   Q  Q  10=  20=  , this number is 1023. If   If  , the number is more than 1 mil- lion. We need to have a less expensive strategy than the exhaustive search.  There are several suboptimal procedures. They are not guaranteed to find  the optimal subset, but they require significantly less computation. One  procedure is called forward selection. This method begins with an empty  model and then adds variables one at a time. At each stage, we add the in- dependent variable that provides the largest reduction in squared error.  We stop adding variables when the performance is adequate. Another ap- proach, called backward elimination, starts with all independent variables  selected for the model. At each stage we eliminate the variable that would  cause the least increase in the squared error. The process continues until  the performance is inadequate. There are other approaches which combine   17-18  Subset Selection  Forward Selection  Backward Elimination   Training RBF Networks  forward selection and backward elimination, so that variables can be added  and deleted at each iteration.  Any of the standard subset selection techniques can be used for selecting  RBF centers. For purposes of illustration, we will consider one specific form  of forward selection, called orthogonal least squares [ChCo91]. Its main  feature is that it efficiently calculates the error reduction provided by the  addition of each potential center to the RBF network.  To develop the orthogonal least squares algorithm, we begin with Eq.   17.22 , repeated here in slightly different form:  t  =  Ux  e+  .   17.44   We will use our standard notation for matrix rows and columns to individ- ually identify both the rows and the columns of the matrix U:  U  =  =  u1 u2  un   17.45   1  uT T z1 uT T z2 2   T uT zQ  =  Q  U  Here each row of the matrix   represents the output of layer 1 of the RBF  network for one input vector from the training set. There will be a column  of the matrix U for each neuron  basis function  in layer 1 plus the bias term     . Note that for the OLS algorithm, the potential centers for the  n basis functions are often chosen to be all of the input vectors in the training  set. In this case,  , since the constant “1” for the bias term  z is included in   n , as shown in Eq.  17.17 .   will equal   Q 1+  1+  S1  =  Eq.  17.44  is in the form of a standard linear regression model. The matrix  U  are called the re- gressor vectors.   is called the regression matrix, and the columns of   U  The objective of OLS is to determine how many columns of  neurons or basis functions  should be used. The first step is to calculate  how much each potential column would reduce the squared error. The prob- lem is that the columns are generally correlated with each other, and so it  is difficult to determine how much each individual column would reduce  the error. For this reason, we need to first orthogonalize the columns. Or- thogonalizing the columns means that we can decompose     numbers of    as follows:  U  U  U MR  =  ,   17.46   where   R   is an upper triangular matrix, with ones on the diagonal:  17-19  17  Regression Matrix   17 Radial Basis Networks  R  =  r1 3  r1 n 1 r1 2 0 1 r2 3  r2 n   0 0 0 0  0  rn 1– 0  1    n  ,   17.47    17.49    17.50    17.51    17.52   M  and  the following properties:   is a matrix with orthogonal columns   mi  . This means that   M   has   MTM V  =  =  0  0 v1 1 0 v2 2  0  0  vn n 0  =  m1  Tm1 0 m2  0  0  0 Tm2  0  Tmn  0  mn  .   17.48   Now Eq.  17.44  can be written  t MRx  =  e+  =  Mh e+  ,  where  The least squares solution for Eq.  17.49  is  h  =  Rx  .  and because   V   is diagonal, the elements of    can be computed  h  =  MTM   1– MTt  =  V  ,   1– MTt h  Tt hi mi --------- vi i  =  =  Tt mi -------------- Tmi mi  .  h   we can compute   From   is upper-triangu- lar, Eq.  17.50  can be solved by back-substitution and does not require a  matrix inversion.   using Eq.  17.50 . Since   R  x  There are a number of ways to obtain the orthogonal vectors  will use the Gram-Schmidt orthogonalization process of Eq.  5.20 , starting  with the original columns of   , but we   mi  .  U  m1  u1=  ,   17.53   17-20   Training RBF Networks  mk  =  uk  –  ri k mi  ,  k  1–  i  1=   17.54   where  ri k  =  ,   i  =  1  k     1–  .   17.55   Tuk mi -------------- Tmi mi  Now let’s see how orthogonalizing the columns of   enables us to efficient- ly calculate the squared error contribution of each basis vector. Using Eq.   17.49 , the total sum square value of the targets is given by  U  tTt  =    Mh e+  T Mh e+      =  hTMTMh eTMh hTMTe  +  +  +  eTe  .   17.56   Consider the second term in the sum:  eTMh If we use the optimal   = h   from Eq.  17.51 , we find  t Mh –  TMh  =  tTMh hTMTMh  –  .   17.57   eTMh  =  tTMh  –  tTMV 1– MTMh  =  tTMh  –  tTMh  =  0  .   17.58   Therefore the total sum square value from Eq.  17.56  becomes  17  tTt  =  hTMTMh eTe  +  =  hTVh eTe  +  =  2mi hi  Tmi  +  eTe  .   17.59   n    i  1=  The first term on the right of Eq.  17.59  is the contribution to the sum  squared value explained by the regressors, and the second term is the re- maining sum squared value that is not explained by the regressors. There- fore, regressor  basis function     contributes   i  2mi hi  Tmi   17.60   to the squared value. This also represents how much the squared error can  be reduced by including the corresponding basis function in the network.  We will use this number, after normalizing by the total squared value, to  determine the next basis function to include at each iteration:  oi  =  2mi Tmi hi -------------------- tTt  .   17.61   This number always falls between zero and one.  17-21   17 Radial Basis Networks  Now let’s put all these ideas together into an algorithm for selecting cen- ters.  The OLS Algorithm  To begin the algorithm, we start with all potential basis functions included  in the regression matrix  .  As we explained below Eq.  17.45 , if all input  vectors in the training set are to be considered potential basis function cen- ters, then the  .  This matrix represents only  potential basis functions, since we start with no basis functions included in  the network.   matrix will be   Q 1+   by   U  U  Q  The first stage of the OLS algorithm consists of the following three steps,  for   1  Q   =  :    i  We then select the basis function that creates the largest reduction in er- ror:  The remaining iterations of the algorithm continue as follows  for iteration  k :  For   i  =  1  Q     ,   i  i1  , ...,   i  1–  ik  ,   j  =  1  k     1–  ,   17.67   i  m1  ui=  ,  i  h1  =  i Tt m1 ---------------------- i Tm1 i  m1  ,  i  o1  =  2m1 i Tm1 i  i   h1 ------------------------------------- tTt  .  o1  =    i1 o1  =  i  max o1      ,  i1 m1 m1  =    =  ui1  .  i  rj k  =  Tui mj -------------- Tmj mj  i  mk  =  ui  –  i  mj rj k  ,  k  1–  j  1=  17-22   17.62    17.63    17.64    17.65    17.66    17.68    Training RBF Networks  i  hk  =  i Tt mk ---------------------- i Tmk i  mk  ,  i  ok  =  2mk i Tmk i  i   hk ------------------------------------- tTt  ,  ok  =    ik ok  =  i  max ok      ,  rj k  =   ik rj k  ,   j  =  1  k     1–  .  ik mk mk  =    .  1  –  oj    ,  k  j  1=   17.69    17.70    17.71    17.72    17.73    17.74   The iterations continue until some stopping criterion is met. One choice of  stopping criterion is     is some small number. However, if   where   is chosen too small, we can  have overfitting, since the network will become too complex. An alternative  is to use a validation set, as we discussed in the chapter on generalization.  We would stop when the error on the validation set increased.    After the algorithm has converged, the original weights  from the transformed weights  back substitution,  h   by using Eq.  17.50 . This produces, by   x   can be computed   17  xn  hn=  ,   xk  =  hk  –  rj k xj  ,   17.75   n  j  1+=  k  where  justable parameters .  n   is the final number of weights and biases in the second layer  ad-  To experiment with orthogonal least squares learning, use the MATLAB®  Neural Network Design Demonstration RBF Orthogonal Least Squares   nnd17ols .  Clustering There is another approach [MoDa89] for selecting the weights and biases  in the first layer of the RBF network. This method uses the competitive net- works described in Chapter 16. Recall that the competitive layer of Ko- honen  see Figure 16.2  and the Self Organizing Feature Map  see Figure   17-23   17 Radial Basis Networks  16.9  perform a clustering operation on the input vectors of the training set.  After training, the rows of the competitive networks contain prototypes, or  cluster centers. This provides an approach for locating centers and select- ing biases for the first layer of the RBF network. If we take the input vec- tors from the training set and perform a clustering operation on them, the  resulting prototypes  cluster centers  could be used as centers for the RBF  network. In addition, we could compute the variance of each individual  cluster and use that number to calculate an appropriate bias to be used for  the corresponding neuron.  Consider again the following training set:  p1 t1 { , }    {  p2 t2  , }   pQ tQ  {      ,  }  .  We want to perform a clustering of the input vectors from this training set:   17.76    17.77     p1 p2   pQ          .  We will train the first layer weights of the RBF network to perform a clus- tering of these vectors, using the Kohonen learning rule of Eq.  16.13 , and  repeated here:  w1  q   i  =  w1 i  1– q    p q  +    –  w1  i  1– q      ,   17.78   p q    is one of the input vectors in the training set, and   where   is  the weight vector that was closest to  .  We could also use other cluster- ing algorithms, such as the Self Organizing Feature Map, or the k-means  clustering algorithm, which was suggested in [MoDa89].  As described in  Chapter 16, Eq.  17.78  is repeated until the weights have converged. The  resulting converged weights will represent cluster centers of the training  set input vectors. This will insure that we will have basis functions located  in areas where input vectors are most likely to occur.  1– q  p q   i    w1  In addition to selecting the first layer weights, the clustering process can  provide us with a method for determining the first layer biases. For each  neuron  basis function , locate the   input vectors from the training set  that are closest to the corresponding weight vector  center . Then compute  the average distance between the center and its neighbors.  nc  disti  =  i w1 pj i–  1 ----- nc  nc      j  1=  1 --- 2   2     17.79   i p1   is the input vector that closest to   where   the next closest  input vector. From these distances, [MoDa89] recommends setting the first  layer biases as follows:  , and is   w1 i  i p2  17-24   Training RBF Networks  1 bi  =  1 ------------------ 2disti  .   17.80   Therefore, when a cluster is wide, the corresponding basis function will be  wide as well. Notice that in this case each bias in the first layer will be dif- ferent. This should provide a network that is more efficient in its use of ba- sis functions than a network with equal biases.  After the weights and biases of the first layer are determined, linear least  squares is used to find the second layer weights and biases.  There is a potential drawback to the clustering method for designing the  first layer of the RBF network. The method only takes into account the dis- tribution of the input vectors; it does not consider the targets. It is possible  that the function we are trying to approximate is more complex in regions  for which there are fewer inputs. For this case, the clustering method will  not distribute the centers appropriately. On the other hand, one would  hope that the training data is located in regions where the network will be  most used, and therefore the function approximation will be most accurate  in those areas.  Nonlinear Optimization It is also possible to train RBF networks in the same manner as MLP net- works—using nonlinear optimization techniques, in which all weights and  biases in the network are adjusted at the same time. These methods are not  generally used for the full training of RBF networks, because these net- works tend to have many more unsatisfactory local minima in their error  surfaces. However, nonlinear optimization can be used for the fine-tuning  of the network parameters, after initial training by one of the two-stage  methods we presented in earlier sections.  We will not present the nonlinear optimization methods in their entirety  here, since they were treated extensively in Chapters 11 and 12. Instead,  we will simply indicate how the basic backpropagation algorithm for com- puting the gradient in MLP networks can be modified for RBF networks.  The derivation of the gradient for RBF networks follows the same pattern  as the gradient development for MLP networks, starting with Eq.  11.9 ,  which you may wish to review at this time. Here we will only discuss the  one step where the two derivations differ. The difference occurs with Eq.   11.20 . The net input for the second layer of the RBF network has the same  form as its counterpart in the MLP network, but the first layer net input  has a different form  as given in Eq.  17.1  and repeated here :  1 ni  =  p w1  i–  1 bi  =  1 bi    1– pj wi j  2  .   17.81   S1   j  1=  17-25  17   17 Radial Basis Networks  If we take the derivative of this function with respect to the weights and  biases, we obtain  1 ni ----------- 1 wi j  =  1 bi  1– ---------------------------------------2 pj wi j      1–    =  1 2  1  1 wi j   pj– bi ----------------------------- p w1 i–  ,    17.82   This produces the modified gradient equations  compare with Eq.  11.23   and Eq.  11.24   for Layer 1 of the RBF network  S1   j  1=    1– pj wi j  2  1 ni -------- 1 bi  =  p w1  i–  .  Fˆ ----------- 1 wi j  =  1  1 wi j   pj– 1bi ----------------------------- si p w1 i–  ,  Fˆ -------- 1 bi  =  1 p w1 si  i–  .   17.83    17.84    17.85   Therefore, if we look at the summary of the gradient descent backpropaga- tion algorithm for MLP networks, from Eq.  11.44  to Eq.  11.47 , we find  that the only difference for RBF networks is that we substitute Eq.  17.84   and Eq.  17.85  for Eq.  11.46  and Eq.  11.47  when    2= the original equations remain the same.  . When   1=  m  m  To experiment with nonlinear optimization learning, use the MATLAB®  Neural Network Design Demonstration RBF Nonlinear Optimization   nnd17no .  Other Training Techniques In this chapter we have only touched the surface of the variety of training  techniques that have been proposed for RBF networks. We have attempted  to present the principal concepts, but there are many variations. For exam- ple, the OLS algorithm has been extended to handle multiple outputs  [ChCo92] and regularized performance indices [ChCh96]. It has also been  used in combination with a genetic algorithm [ChCo99], which was used to  select the first layer biases and the regularization parameter. The expecta- tion maximization algorithm has also been suggested by several authors  for optimizing the center locations, starting with [Bish91]. [OrHa00] used  a regression tree approach for center selection. There have also been many  variations on the use of clustering and on the combination of clustering for  initialization and nonlinear optimization for fine-tuning. The architecture  of the RBF network lends itself to many training approaches.   17-26   Summary of Results  Summary of Results  Radial Basis Network  Inputs  Radial Basis Layer  Linear Layer  W1  S 1 x R  p1  R x 11  dist  1  R 1  b1  S 1 x 1  n1  S 1 x 1  .*  a1  S 1 x 1  1  S 1  W2  S 2 x S 1  b2  S 2 x 1  a2  S 2 x 1  n2  S 2 x 1  S 2  1  a  i  =  radbas     1  -  w p i  b  1     i  2  a W a b  =  +  2  1  2  Training RBF Networks  Linear Least Squares  17  x  =  ,   zq  =  w2 1 b2  1 aq 1  t  =  ,   U  =  ,   e  =  t1 t2  tQ  uT T z1 1 uT T z2 2   T uT zQ  =  Q  e1 e2  eQ  F x   =  t Ux –  T t Ux  –   xTx +    UTU I+  x  =  UTt  17-27   17 Radial Basis Networks  Orthogonal Least Squares  Step 1  i  m1  ui=  ,  i  h1  =  i Tt m1 ---------------------- i Tm1 i  m1  ,  i  o1  =  2m1 i Tm1 i  i   h1 ------------------------------------- tTt  .  o1  =    i1 o1  =  i  max o1      i1 m1 m1  =    =  ui1  .  Step k  i  mk  =  ui  –  i  mj rj k  ,  k  1–  j  1=  i  hk  =  i Tt mk ---------------------- i Tmk i  mk  ,  i  ok  =  2mk i Tmk i  i   hk ------------------------------------- tTt  ,  ok  =    ik ok  =  i  max ok      ,  ik mk mk  =    .  For   i  =  1  Q     ,   i  i1  , ...,   i  1–  ik  i  rj k  =  Tuk mj -------------- Tmj mj  ,   j  =  1  k     ,  17-28   Summary of Results  Clustering  Training the weights w1 i    p q  +  1– q  –    w1  q   i  =  w1  i  1– q      Selecting the bias  disti  =  i w1 pj i–  1 ----- nc  nc      j  1=  1 --- 2   2    1 bi  =  1 ------------------ 2disti  Fˆ ----------- 1 wi j  =  1  1 wi j   pj– 1bi ----------------------------- si p w1 i–  ,  Fˆ -------- 1 bi  =  1 p w1 si  i–  .  Nonlinear Optimization  Replace Eq.  11.46  and Eq.  11.47  in standard backpropagation with  17  17-29   17 Radial Basis Networks  Solved Problems  P17.1 Use the OLS algorithm, to approximate the following function:  g p   =  cos  p     for   1–     p  1  .  To obtain our training set we will evaluate this function at five val- ues of   p  :  This produces the targets  p  =     1–   0.5–   0 0.5 1      .  t  =    1–      0 1 0    1–    .  Perform one iteration of the OLS algorithm. Assume that the in- puts in the training set are the potential centers and that the bias- es are all equal to 1.  First, we compute the outputs of the first layer:  We can use Eq.  17.17  and Eq.  17.21  to create the U and t matrices:  1 ni q  =  pq w1  i–  1 bi  ,  1 aq  =  1 radbas nq    ,  a1  =            1.000 0.779 0.368 0.105 0.018    0.779 1.000 0.779 0.368 0.105    0.368 0.779 1.000 0.779 0.368    0.105 0.368 0.779 1.000 0.779    0.018 0.105 0.368 0.779 1.000  .            UT  =  1.000 0.779 0.368 0.105 0.018 0.779 1.000 0.779 0.368 0.105 0.368 0.779 1.000 0.779 0.368 0.105 0.368 0.779 1.000 0.779 0.018 0.105 0.368 0.779 1.000 1.000 1.000 1.000 1.000 1.000  ,  tT  =  1– 0 1 0 1–  .  17-30   Solved Problems  Now we perform step one of the algorithm:  i  m1  ui=  ,  1  m1  =  ,   2  m1  =  ,   3  m1  =  ,   4  m1  =  ,   0.105 0.368 0.779 1.000 0.779  1.000 0.779 0.368 0.105 0.018  0.779 1.000 0.779 0.368 0.105  0.018 0.105 0.368 0.779 1.000  0.368 0.779 1.000 0.779 0.368  1.000 1.000 1.000 1.000 1.000  5  m1  =  ,   6  m1  =  ,  1  h1  –=  0.371  ,   2  h1  –=  0.045  ,   =  0.106  ,   4  h1  –=  0.045  ,   5  h1  –=  0.371  ,   17  i  h1  =  i Tt m1 ---------------------- i Tm1 i  m1  ,  3  h1 6  h1  –=  0.200  ,  i  o1  =  2m1 i Tm1 i  i   h1 ------------------------------------- tTt  ,  3  o1 6  o1  =  =  0.0667  .  1  o1  =  0.0804  ,   2  o1  =  0.0016  ,   0.0094  ,   4  o1  =  0.0016  ,   5  o1  =  0.0804  ,   We see that the first and fifth centers would produce a 0.0804 reduction in  the error. This means that the error would be reduced by 8.04%, if the first  or fifth center were used in a single-neuron first layer. We would typically  select the first center, since it has the smallest index.  0=  , since the bias center,   If we stop at this point, we would add the first center to the hidden layer.  Using Eq.  17.75 , we would find that  . Also,  , was not selected on the first iteration.  b2 Note that if we continue to add neurons in the hidden layer, the first weight  will change. This can be seen from Eq.  17.75 . This equation to find   is  only used after all of the  . hn   will exactly equal    are found. Only   6  m1  0.371  1  h1  w1 1  h1  x1  hk  xn  xk  =  =  =  =  –  2  17-31   17 Radial Basis Networks  U  U  .   with respect to  , which was chosen on the first iteration, using Eq.  17.54 . It is inter-  If we continue the algorithm, the first column would be removed from  We would then orthogonalize all remaining columns of  m1 esting to note that the error reduction on the second iteration would be  much higher than the reduction on the first iteration. The sequence of re- ductions would be 0.0804, 0.3526, 0.5074, 0.0448, 0.0147, 0, and the centers  would be chosen in the following order: 1, 2, 5, 3, 4, 6. The reason that re- ductions in later iterations are higher is that it takes a combination of basis  functions to produce the best approximation. This is why forward selection  is not guaranteed to produce the optimal combination, which can be found  with an exhaustive search. Also, notice that the bias is selected last, and it  produces no reduction in the error.   P17.2 Figure P17.1 illustrates a classification problem, where Class I vec- tors are represented by dark circles, and Class II vectors are rep- resented by light circles. These categories are not linearly  separable. Design a radial basis function network to correctly clas- sify these categories.  Figure P17.1  Classification Problem for Problem P17.2  From the problem statement, we know that the network will need to have  two inputs, and we can use one output to distinguish the two classes. We  will choose a positive output for Class I vectors, and a negative output for  Class II vectors. The Class I region is made up of two simple subregions,  and it appears that two neurons should be sufficient to perform the classi- fication. The rows of the first-layer weight matrix will create centers for the  two basis functions, and we will choose each center to be located in the mid- dle of one subregion. By centering a basis function in each subregion, we  can produce maximum network outputs there. The first layer weight ma- trix is then  W1  =  1  1 2.5 2.5  .  The choice of the biases in the first layer depends on the width that we  want for each basis function. For this problem, the first basis function  should be wider than the second. Therefore, the first bias will be smaller  than the second bias. The boundary formed by the first basis function   17-32   Solved Problems  should have a radius of approximately 1, while the second basis function  boundary should have a radius of approximately  . We want the basis  functions to drop significantly from their peaks in these distances. If we use  a bias of 1 for the first neuron and a bias of 2 for the second neuron, we get  the following reductions within one radius of the centers:  1 2  a  =  e n2–  =    e 1 1 –  2  =  e 1–  =  0.3679  ,   a  =  e n2–  =      – e 2 0.5  2  =  e 1–  =  0.3679  This will work for our problem, so we select the first layer bias vector to be  b1  =  .  1 2  The original basis function response ranges from 0 to 1  see Figure 17.1 .  We want the output to be negative for inputs outside the decision regions,  so we will use a bias of -1 for the second layer, and we will use a value of 2  for the second layer weights, in order to bring the peaks back up to 1. The  second layer weights and biases then become  W2  =  2 2  ,   b2  1–=  .  For these network parameter values, the network response is shown on the  right side of Figure P17.2. This figure also shows where the surface inter- sects the plane at  , which is where the decision takes place. This is  also indicated by the contours shown underneath the surface. These are the  function contours where  clearly on the left side of Figure P17.2.  . These decision regions are shown more   0=  0=  a2  a2  17  p2  3.5  2.5  4  3  2  1  1.5  0.5  0  0  a2  0.5  1  0  −0.5  −1  −1.5  −2 0  17-33  0.5  1  1.5  2.5  3  3.5  4  2  p1  Figure P17.2  Decision Regions for Problem P17.2  1  2  p1  3  1  4  0  4  3  2  p2   17 Radial Basis Networks  P17.3 For an RBF network with one input and one neuron in the hidden   layer, the initial weights and biases are chosen to be   w1 0   0=  ,   b1 0   1=  ,   w2 0   2–=  ,   b2 0   1=  .  An input target pair is given to be    p  1–=  1= t    .  Perform one iteration of steepest descent backpropagation with    1=  .  The first step is to propagate the input through the network.  n1  =  p w1–  b1  =  1    1–  0–  2  =  1  a1  =  radbas n1    =  2  –  e n1  =  e 1–  =  0.3679  n2  =  w2a1  b2+  =  2–   0.3679     1+  =  0.2642  a2  =  purelin n2    =  n2  =  0.2642  e  =  a2– t    =  – 1    0.2642      =  0.7358  Now we backpropagate the sensitivities using Eq.  11.44  and Eq.  11.45 .  s2  =  2F· 2 n2   –    t a–    =  –  2 1  e   =  –  2 1 0.7358  =  –  1.4716  s1  =  F· 1 n1     W2   Ts2  =  2– n1e n1  –    2  w2s2  =    2–  e 1–  1    2–    –  1.4716    =  –  2.1655     Finally, the weights and biases are updated using Eq.  11.46  and Eq.   11.47  for Layer 2, and Eq.  17.84  and Eq.  17.85  for Layer 1:  w2 1   =  w2 0  s2 a1  –  T  =  2–    –  1  –  1.4716   0.3679     =  –  1.4586  ,  w1 1   =  w1 0  s1 b1 w1  –         p–  ------------------------- p w1–   =  0   –  1  –  2.1655   –   1 0   ----------------------------   1–  1– 0–  =  2.1655  ,  b2 1   =  b2 0  s2  –  =  1 1  –  –  1.4716    =  2.4716  ,  b1 1   =  b1 0  s1 p w1–  –  =  1  –  1  –  2.1655    1–  0–  =  3.1655  .  17-34   Epilogue  Epilogue  The radial basis function network is an alternative to the multilayer per- ceptron network for problems of function approximation and pattern recog- nition. In this chapter we have demonstrated the operation of the RBF  network, and we have described several techniques for training the net- work. Unlike MLP training, RBF training usually consists of two stages. In  the first stage, the weights and biases in the first layer are found. In the  second stage, which typically involves linear least squares, the second layer  weights and biases are calculated.  17  17-35   17 Radial Basis Networks  Further Reading  [Bish91]  [BrLo88]  [ChCo91]  [ChCo92]  [ChCh96]  [ChCo99]  C. M. Bishop, “Improving the generalization properties of  radial basis function neural networks,” Neural Computa- tion, Vol. 3, No. 4, pp. 579-588, 1991.  First published use of the expectation-maximization algo- rithm for optimizing cluster centers for the radial basis net- work.  D.S. Broomhead and D. Lowe, “Multivariable function in- terpolation and adaptive networks,” Complex Systems,  vol.2, pp. 321-355, 1988.  This seminal paper describes the first use of radial basis  functions in the context of neural networks.  S. Chen, C.F.N. Cowan, and P.M. Grant, “Orthogonal least  squares learning algorithm for radial basis function net- works,” IEEE Transactions on Neural Networks, Vol.2,  No.2, pp.302-309, 1991.  The first description of the use of the subset selection tech- nique for selecting centers for radial basis function net- works.  S. Chen, P. M. Grant, and C. F. N. Cowan, “Orthogonal  least squares algorithm for training multioutput radial ba- sis function networks,” Proceedings of the Institute of Elec- trical Engineers, Vol. 139, Pt. F, No. 6, pp. 378–384, 1992.  This paper extends the orthogonal least squares algorithm  to the case of multiple outputs.  S. Chen, E. S. Chng, and K. Alkadhimi, “Regularised or- thogonal least squares algorithm for constructing radial  basis function networks,” International Journal of Control,  Vol. 64, No. 5, pp. 829–837, 1996.  Modifies the orthogonal least squares algorithm to handle  regularized performance indices.  S. Chen, C.F.N. Cowan, and P.M. Grant, “Combined Genet- ic Algorithm Optimization and Regularized Orthogonal  Least Squares Learning for Radial Basis Function Net- works,” IEEE Transactions on Neural Networks, Vol.10,  No.5, pp.302-309, 1999.  Combines a genetic algorithm with orthogonal least  squares to compute the regularization parameter and basis   17-36   [Lowe89]  [Mill90]  [MoDa89]  [PaSa93]  [Powe87]  Further Reading  function spread, while also selecting centers and solving for  the optimal second layer weights of radial basis function  networks.  D. Lowe, “Adaptive radial basis function nonlinearities,  and the problem of generalization,” Proceedings of the First  IEE International Conference on Artificial Neural Net- works, pp. 171 - 175, 1989.  This paper describes the use of gradient-based algorithms  for training all of the parameters of an RBF network, in- cluding basis function centers and widths. It also provides  a formula for setting the basis function widths, if the cen- ters are randomly chosen from the training data set.  A.J. Miller, Subset Selection in Regression. Chapman and  Hall, N.Y., 1990. This book provides a very complete and clear discussion of  the general problem of subset selection. This involves  choosing an appropriate subset from a large set of indepen- dent input variables, in order to provide the most efficient  prediction of some dependent variable. J. Moody and C.J. Darken, “Fast Learning in Networks of  Locally-Tuned Processing Units,” Neural Computation,  Vol. 1, pp. 281–294, 1989.  The first published use of clustering methods to find radial  basis function centers and variances.  This paper compares a variety of methods for training radi- al basis function networks. The methods include forward  selection with regularization and also regression trees.  J. Park and I.W. Sandberg, “Universal approximation us- ing radial-basis-function networks,” Neural Computation,  vol. 5, pp. 305-316, 1993.  This paper proves the universal approximation capability  of radial basis function networks.  M.J.D. Powell, “Radial basis functions for multivariable in- terpolation: a review,” Algorithms for Approximation, pp.  143-167, Oxford, 1987.  This paper provides the definitive survey of the original  work on radial basis functions. The original use of radial  basis functions was in exact multivariable interpolation.  17-37  [OrHa00]  M. J. Orr, J. Hallam, A. Murray, and T. Leonard, “Assess- ing rbf networks using delve,” IJNS, 2000.  17   17 Radial Basis Networks  Exercises  E17.1 Design an RBF network to perform the classification illustrated in Figure  E17.1. The network should produce a positive output whenever the input  vector is in the shaded region and a negative output otherwise.  Figure E17.1  Pattern Classification Regions  E17.2 Choose the weights and biases for an RBF network with two neurons in the  hidden layer and one output neuron, so that the network response passes  through the points indicated by the blue circles in Figure E17.2.   Use the MATLAB® Neural Network Design Demonstration RBF Network  Function  nnd17nf  to check your result.  3  2  1  0  −1 −2  17-38  −1  0  1  2  Figure E17.2  Function Approximation Exercise   Exercises  E17.3 Consider a 1-2-1 RBF network  two neurons in the hidden layer and one   output neuron . The first layer weights and biases are fixed as follows:  W1  =  ,   b1  =  1– 1  .  0.5 0.5  Assume that the bias in the second layer is fixed at 0   set has the following input target pairs:  b2  0=   . The training     p1   1=  t1  1–=    ,     p2   0=  t2  0=    ,     p3  1–=    t3  1=    .  i. Use linear least squares to solve for the second layer weights, as-  suming that the regularization parameter     0=  .  ii. Plot the contour plot for the sum squared error. Recall that it will be   a quadratic function.  See Chapter 8.   iii. Write a MATLAB® M-file to check your answers to parts i. and ii.  iv. Repeat parts i. to iii., with     4=  . Plot regularized squared error.  E17.4 The Hessian matrix for the performance index of the RBF network, given   in Eq.  17.25 , is  2 UTU I+     .  17  Show that this matrix is at least positive semidefinite for  that it is positive definite if    0  .   0  , and show   E17.5 Consider an RBF network with the weights and biases in the first layer  fixed. Show how the LMS algorithm of Chapter 10 could be modified for  learning the second layer weights and biases.  E17.6 Suppose that a Gaussian transfer function in the first layer of the RBF net-  work is replaced with a linear transfer function.  i. In Solved Problem P11.8, we showed that a multilayer perceptron  with linear transfer functions in each layer is equivalent to a single- layer perceptron. If we use a linear transfer function in each layer  of an RBF network, is that equivalent to a single-layer network? Ex- plain.  ii. Work out an example, equivalent to Figure 17.4, to demonstrate the  operation of the RBF network with linear transfer function in the  first layer. Use MATLAB® to plot your figures. Do you think that  the RBF network will be a universal approximator, if the first layer  transfer function is linear? Explain your answer.   17-39  » 2 + 2 ans =       4  » 2 + 2 ans =       4   17 Radial Basis Networks  E17.7 Consider a Radial Basis Network, as in Figure 17.2, but assume that there  is no bias in the second layer. There are two neurons in the first layer  two  basis functions . The first layer weights  centers  and biases are fixed, and  we have three inputs and targets. The outputs of the first layer and the tar- gets are given by  a1  =        2 1    1 2  0 1      ,   t  =     0 1 2      .  i. Use linear least squares to find the second layer weights of the net-  work.  ii. Assume now that the basis function centers in the first layer are   only potential centers. If orthogonal least squares is used to select  potential centers, which center will be selected first, what will be its  corresponding weight in the second layer, and how much will it re- duce the squared error? Show all calculations clearly and in order.  iii. Is there a relationship between the two weights that you computed   in part i. and the single weight that you computed in part ii? Ex- plain.  E17.8 Repeat E17.7 for the following data:  i.  a1  =  ,   t  =     1 2    1–    .              1 2  2 1      2 1  1 1  1– 1      0 1      ii.  a1  =  ,   t  =     3 1 2      .  E17.9 Consider the variation of the radial basis network shown in Figure E17.3.   The inputs and targets in the training set are   p2  t2  1=  1=    .    p1  1–=  t1  1–=    ,   i. Find the linear least squares solution for the weight matrix   W2  .  ii. For the weight matrix    that you found in part i., sketch the net-  W2  work response as the input varies from -2 to 2.  17-40   Exercises  Inputs  Linear Radial Basis Layer  Linear Layer  W1  2 x 1  dist  b1  2 x 1  p  1 x 1  1  1  n1  2 x 1  .*  a1  2 x 1  W2  1 x 2  a2  1 x 1  n2  1 x 1  2  1  1  a  i  1  i=   - w p  b  1  i  2  1 a W a  =  2  Figure E17.3  Radial Basis Network for Exercise E17.9  E17.10 Write a MATLAB® program to implement the linear least squares algo-  RBF network with first layer weights and biases   rithm for the  fixed. Train the network to approximate the function  S1–  1–  1  17  g p   =  1  sin+   for   2–     p  2  .     ---p  8  i. Select 10 training points at random from the interval   2–     p  2  .  ii. Select four basis function centers evenly spaced on the interval   p  2     . Then, use Eq.  17.9  to set the bias. Finally, use linear   2– least squares to find the second layer weights and biases, assuming  that there is no regularization. Plot the network response for  2– the sum squared error over the training set.  , and show the training points on the same plot. Compute      2  p  iii. Double the bias from part ii and repeat.  iv. Decrease the bias by half from part ii, and repeat.  v. Compare the final sum squared errors for all cases and explain your   results.  E17.11 Use the function described in Exercise E17.10, and use an RBF network   with 10 neurons in the hidden layer.  i. Repeat Exercise E17.10 ii. with regularization parameter     =  0.2  .   Describe the changes in the RBF network response.   ii. Add uniform random noise in the range    to the training  targets. Repeat Exercise E17.10 ii. with no regularization and with  regularization parameter  best results. Explain.  . Which case produces the   0.2 2 20   0.1–  0.1    =          17-41  » 2 + 2 ans =       4  » 2 + 2 ans =       4   17 Radial Basis Networks  E17.12 Write a MATLAB® program to implement the orthogonal least squares al- gorithm. Repeat Exercise E17.10 using the orthogonal least squares algo- rithm. Use the 10 random training point inputs as the potential centers,  and use Eq.  17.9  to set the bias. Use only the first four selected centers.  Compare your final sum squared errors with the result from E17.10 part ii.  » 2 + 2 ans =       4  E17.13 Write a MATLAB® program to implement the steepest descent algorithm   RBF network. Train the network to approximate the func-  S1–  1–  1  for the  tion  g p   =  1  sin+   for   2–     p  2  .     ---p  8  » 2 + 2 ans =       4  You should be able to use a slightly modified version of the program you  wrote for Exercise E11.25.  i. Select 10 data points at random from the interval   2–     p  2  .  ii. Initialize all parameters  weights and biases in both layers  as   small random numbers, and then train the network to convergence.   Experiment with the learning rate  , to determine a stable value.    Plot the network response for    2 p points on the same plot. Compute the sum squared error over the  training set. Use 2, 4 and 8 centers. Try different sets of initial  weights.  , and show the training   2–  iii. Repeat part ii., but use a different method for initializing the pa- rameters. Start by setting the parameters as follows. First, select  basis function centers evenly spaced on the interval  Then, use Eq.  17.9  to set the bias. Finally, use linear least squares  to find the second layer weights and biases. Compute the squared  error for these initial weights and biases. Starting from these initial  conditions, train all parameters with steepest descent.     2–  .   2  p  iv. Compare the final sum squared errors for all cases and explain your   results.  E17.14 Suppose that a radial basis function layer  Layer 1 of the RBF network    were used in the second or third layer of a multilayer network. How could  you modify the backpropagation equation, Eq.  11.35 , to accommodate this  change.  Recall that the weight update equations would be modified from  Eq.  11.23  and Eq.  11.24  to Eq.  17.84  and Eq.  17.85 .   E17.15 Consider again Exercise E16.10, in which you trained a feature map to   cluster the input space  0    p1    1  ,   2    p2    3  .  17-42   » 2 + 2 ans =       4  Exercises  Assume that over this input space, we wish to use an RBF network to ap- proximate the following function:  t  =  sin    2p1    cos    2p2    .  i. Use MATLAB to randomly generate 200 input vectors in the region   shown above.  ii. Write a MATLAB M-file to implement a four-neuron by four-neuron   two-dimensional  feature map. Calculate the net input by finding  the distance between the input and weight vectors directly, as is  done by the LVQ network, so the vectors do not need to be normal- ized. Use the feature map to cluster the input vectors.   iii. Use the trained feature map weight matrix from part ii as the   weight matrix of the first layer of an RBF network. Use Eq.  17.79   to determine the average distance between each cluster and its cen- ter, and then use Eq.  17.80  to set the bias for each neuron in the  first layer of the RBF network.  iv. For each of the 200 input vectors in part i, compute the target re- sponse for the function above. Then use the resulting input target  pairs to determine the second-layer weights and bias for the RBF  network.  v. Repeat parts ii to iv, using a two by two feature map. Compare your   results.  17  17-43   Objectives  18 Grossberg Network  Objectives Theory and Examples  Biological Motivation: Vision  Illusions Vision Normalization  Basic Nonlinear Model Two-Layer Competitive Network  Layer 1 Layer 2 Choice of Transfer Function Learning Law  Relation to Kohonen Law  Summary of Results Solved Problems Epilogue Further Reading Exercises  18-1 18-2 18-3 18-4 18-8 18-9 18-12 18-13 18-17 18-20 18-22 18-24 18-26 18-30 18-42 18-43 18-45  Objectives  18  This chapter is a continuation of our discussion of associative and compet- itive learning algorithms in Chapters 15 and 16. The Grossberg network  described in this chapter is a self-organizing continuous-time competitive  network. This will be the first time we have considered continuous-time re- current networks, and we will introduce concepts here that will be further  explored in Chapters 20 and 21. This Grossberg network is also the foun- dation for the adaptive resonance theory  ART  networks that we will  present in Chapter 19.  We will begin with a discussion of the biological motivation for the Gross- berg network: the human visual system. Although we will not cover this  material in any depth, the Grossberg networks are so heavily influenced by  biology that it is difficult to discuss his networks without putting them in  their biological context. It is also important to note that biology provided  the original inspirations for the field of artificial neural networks, and we  should continue to look for inspiration there, as scientists continue to de- velop new understanding of brain function.  18-1   18 Grossberg Network  Theory and Examples  During the late 1960s and the 1970s the number of researchers in the field  of neural networks dropped dramatically. There were, however, a number  of researchers who continued to work during this period, including Tuevo  Kohonen, James Anderson, Kunihiko Fukushima and Shun-ichi Amari,  among others. One of the most prolific was Stephen Grossberg.   Grossberg has been continuously active, and highly productive, in neural  network research since the early 1960s. His work is characterized by the  use of nonlinear mathematics to model specific functions of mind and  brain, and his volume of output has been consistent with the magnitude of  the task of understanding the brain. The topics of his papers have ranged  from such specific areas as how competitive networks can provide contrast  enhancement in vision, to such general subjects as a universal theory for  human memory.   In part because of the scale of his efforts, his work has a reputation for dif- ficulty. Each new paper is built on a foundation of 30 years of previous re- sults, and is therefore difficult to assess on its own merits. In addition, his  terminology is self-consistent, but not in standard use by other researchers.  His work is also characterized by a high level of mathematical and neuro- physiological sophistication. He is inspired by the interdisciplinary re- search into brain function by Helmholtz, Maxwell and Mach, and he brings  this viewpoint to his work. His research lies at the intersection of mathe- matics, psychology and neurophysiology. A lack of background in these ar- eas can make his work difficult to approach on a first reading.  This chapter will take a rudimentary look at one of the seminal networks  developed by Grossberg. In order to obtain the maximum understanding of  his ideas, we will begin with a brief introduction to the biological motiva- tion for the network: the visual system. Then we will present the mathe- matical building block for many of Grossberg’s networks: the shunting  model. After understanding the function of this simple model, we will dem- onstrate how it can be used to build a neural network for adaptive pattern  recognition. This network will then form the basis for the adaptive reso- nance theory networks that are discussed in Chapter 19. By building up  gradually to the more complex networks, we hope to make them more eas- ily understandable.  There is an important lesson we should take from the work described in  this chapter. Although the original inspiration for the field of artificial neu- ral networks came from biology, at times we forget to look back to biology  for new ideas. It will be the blending of biology, mathematics, psychology  and other disciplines that will provide the maximum growth in our under- standing of neural networks.  18-2   Biological Motivation: Vision  Biological Motivation: Vision  The neural network described in this chapter was inspired by the develop- mental physiology of the human visual system. In this section we want to  provide a brief introduction to vision, so that the function of the network  will be more understandable.   In Figure 18.1 we have a schematic representation of the first stages of the  visual system. Light passes through the cornea  the transparent front part  of the eye  and the lens, which bends the light to focus objects on the retina   the interior layer of the external wall of the eye . It is after the light falls  on the retina that the immense job of translating this information into an  understandable image begins. As we will see later in this chapter, much of  what we “see” is not actually present in the image projected onto the retina.  Bipolar Cell  Amacrine   Cell  Ganglion Cell  Optic Nerve  Cone  Rod  Horizontal  Cell  Optic Nerve Fiber  Light  Lens  Retina  18  Figure 18.1  Eyeball and Retina  Retina  Rods Cones  The retina is actually a part of the brain. It becomes separated from the  brain during fetal development, but remains connected to it through the  optic nerve. The retina consists of three layers of nerve cells. The outer lay- er consists of the photoreceptors  rods and cones , which convert light into  electrical signals. The rods allow us to see in dim light, whereas the cones  allow us to see fine detail and color. For reasons not completely understood,  light must pass through the other two layers of the retina in order to stim- ulate the rods and cones. As we will see later, this obstruction must be com- pensated for in neural processing, in order to reconstruct recognizable  images.  Bipolar Cells  The middle layer of the retina consists of three types of cells: bipolar cells,  horizontal cells and amacrine cells. Bipolar cells receive input from the re- ceptors and feed into the third layer of the retina, containing the ganglion   18-3   18 Grossberg Network  Horizontal Cells Amacrine Cells  cells. Horizontal cells link the receptors and the bipolar cells, and amacrine  cells link bipolar cells with the ganglion cells.  Ganglion Cells  Visual Cortex  The final layer of the retina contains the ganglion cells. The axons of the  ganglion cells pass across the surface of the retina and collect in a bundle  to form the optic nerve. It is interesting to note that each eye contains  roughly 125 million receptors, but only 1 million ganglion cells. Clearly  there is significant processing done in the retina to perform data reduction.  The axons of the ganglion cells, bundled into the optic nerve, connect to an  area of the brain called the lateral geniculate nucleus, as illustrated in Fig- ure 18.2. From this point the fibers fan out into the primary visual cortex,  located at the back of the brain. The axons of the ganglion cells make syn- apses with lateral geniculate cells, and the axons of the lateral geniculate  cells make synapses with cells in the visual cortex. The visual cortex is the  region of the brain devoted to visual function and consists of many layers  of cells.  Lateral Geniculate Nucleus  Primary Visual Cortex  Retina  Figure 18.2  Visual Pathway  The connections along the visual pathway are far from random. The map- ping from each layer to the next is highly organized. The axons from the  ganglion cells in a certain part of the retina go to cells in a particular part  of the lateral geniculate, which in turn go to a particular part of the visual  cortex.  This topographic mapping was one of the inspirations for the self- organizing feature map described in Chapter 14.  In addition, as we can see  in Figure 18.2, each hemisphere of the brain receives input from both eyes,  since half of the optic nerve fibers cross and the other half stay uncrossed.  It turns out that the left half of each visual field ends up in the right half  of the brain, and the right half of each visual field ends up in the left half  of the brain.  Illusions We now have some idea of the general structure of the visual pathway, but  how does it function? What is the purpose of the three layers of the retina?  What operations are performed by the lateral geniculate? Some hints to the   18-4   Biological Motivation: Vision  answers to these questions can be obtained by investigating visual illu- sions.  Why are there so many visual illusions? Mechanisms that overcome imper- fections of the retinal uptake process imply illusions. Grossberg and others  have used the vast store of known illusions to probe adaptive perceptual  mechanisms [GrMi89]. If we can develop mathematical models that pro- duce the same illusions the biological system does, then we may have a  mechanism that describes how this part of the brain works. To help us un- derstand why illusions exist, we will first consider some of the imperfec- tions of the retinal uptake process.  Figure 18.3 is the view of the retina that an ophthalmologist has when  looking into the eye through the cornea. The large pale circle is the optic  disk, where the optic nerve leaves the retina on its way to the lateral gen- iculate. This is also where arteries enter and veins leave the retina. The op- tic disk causes a blind spot in our vision, as we will discuss in a moment.  The darker disk to the right of the optic disk is the fovea, which constitutes  the center of our field of vision. This is a region of the retina, about half a  millimeter in diameter, that contains only cones. Although cones are dis- tributed throughout the retina, they are most densely packed in the fovea.  In addition, in this area of the retina the other layers are displaced to the  side, so that the cones lie at the front. The densely packed photoreceptors,  and the lack of obstruction, give us our best fine-detail vision at the fovea,  which allows us to precisely focus the lens.  Optic Disk  Fovea  Optic Disk  Blind Spot   Vein  18  Fovea  Figure 18.3  Back of the Eye  from [John01]   From Figure 18.3 we can see that there are a number of imperfections in  retinal uptake. First, there are no rods and cones in the optic disk, which  leaves a blind spot in our field of vision. We are not normally aware of the  blind spot because of processing done in the visual pathway, but we can  identify it with a simple test. Look at the blue circle on the left side of Fig- ure 18.4, while covering your left eye. As you move your head closer to the  page, then farther away, you will notice a point  about nine inches away    18-5   18 Grossberg Network  at which the circle on the right will disappear from your field of vision.  You  are still looking at the circle on the left.  If you haven’t tried this before, it  can be a little disconcerting. The interesting thing is that we don’t see our  blind spot as a black hole. Somehow our brains fill in the missing region.  Figure 18.4  Test for the Blind Spot  Other imperfections in the retinal uptake are the arteries and veins that  cross in front of the photoreceptors at the back of the retina. These obstruct  the rods and cones from receiving all of the light in the visual field. In ad- dition, because the photoreceptors are at the back of the retina, light must  pass through the other two layers to reach them.  Figure 18.5 illustrates the effect of these imperfections. Here we see an  edge displayed on the retina. The drawing on the right illustrates the im- age initially perceived by the photoreceptors. The regions covered by the  blind spot and the veins are not observed by the rods and cones.  The rea- son we do not “see” the arteries, veins, etc., is that the vision pathway does  not respond to stabilized images. The eyeballs are constantly jerking, in  what are called saccadic movements, so that even fixed objects in our field  of vision are moving relative to the eye. The veins are fixed relative to the  eyeball, so they fade from our vision.   Edge  Retina  Vein  Blind Spot  Stabilized Images Fade  Emergent Segmentation Featural Filling-in  Figure 18.5  Perception of an Edge on the Retina  after [Gros90]   Because we do not see edges as displayed on the right side of Figure 18.5,  the neural systems in our visual pathway must be performing some opera- tion that compensates for the distortions and completes the image. Gross- berg suggests [GrMi89] that there are two primary types of compensatory  processing involved. The first, which he calls emergent segmentation, com- pletes missing boundaries. The second, which he calls featural filling-in,  fills in the color and brightness inside the resulting boundaries. These two  processes are illustrated in Figure 18.6. In the top figure we see an edge as  it is originally perceived by the rods and cones, with missing sections. In   18-6   Biological Motivation: Vision  the lower figure we see the completed edge, after the emergent segmenta- tion and featural filling-in.  Before Processing  After Processing  Emergent Segmentation  Featural Filling-in  Figure 18.6  Compensatory Processing  after [Gros90]   If the processing along the visual pathway is recreating missing parts of  the images we see, there must be times when it makes mistakes, since it  cannot know exactly those parts of a scene from which it receives no light.  These mistakes are illustrated by visual illusions. Consider, for example,  the two figures in the left margin. In the top figure you should be able to  see a bright white triangle lying on top of several other black objects. In  fact, no such triangle exists in the figure. It is purely a creation of the emer- gent segmentation and featural filling-in process of your visual system.  The same is true of the bright white circle which appears to lie on top of the  lines in the lower-left figure.  The featural filling-in process is also demonstrated in Figure 18.7. This il- lusion is called neon color spreading [vanT75]. In the diagram on the right  you may be able to see light blue diamonds, or even wide light blue lines  criss-crossing the figure. In the diagram on the left you may be able to see  a light blue ring. The blue you see filling in the diamonds and the ring is  not a result of the color having been smeared during the printing process,  nor is it caused by the scattering of light. This effect does not appear on the  retina at all. It does not exist, except in your brain.  The perception of neon  color spreading can vary from individual to individual, and the strength of  the perception is dependent on the colors used. If you do not notice the ef- fect in Figure 18.7, look at the cover of any issue of the journal Neural Net- works, Pergamon Press.   Later in this chapter we will discuss some neural network models that can  help to explain the processes that implement emergent segmentation, as  well as other visual phenomena.  18  18-7   18 Grossberg Network  Brightness Constancy Brightness Contrast  Figure 18.7  Neon Color Spreading  Featural Filling In   Vision Normalization In addition to emergent segmentation and featural filling-in, there are two  other phenomena that give us an indication of what operations are being  performed in the early vision system: brightness constancy and brightness  contrast. The brightness constancy effect is evidenced by the test illustrat- ed in Figure 18.8. In this test a subject is shown a small grey disk inside a  darker grey annulus, which is illuminated by white light of a certain inten- sity. The subject is asked to indicate the brightness of the central disk by  looking at a series of grey disks, separately illuminated, and selecting the  disk with the same brightness. Next, the brightness of the light illuminat- ing the grey disk and dark annulus is increased, and the subject is again  asked to select the disk with the same brightness. This process is repeated  for several different levels of illumination. It turns out that in each case the  subject will choose the same disk as matching the original central disk.  Even though the total light entering the subject’s eye is 10 to 100 times  brighter, it is only the relative brightness that registers.  Variable  Illumination  Separate  Constant Illumination  Figure 18.8  Test of Brightness Constancy  after [Gros90]   18-8   Basic Nonlinear Model  Another phenomenon of the vision system, which is closely related to  brightness constancy, is brightness contrast. This effect is illustrated by  the two figures in the left margin. At the centers of the two figures we have  two small disks with equivalent grey scale. The small disk in the top figure  is surrounded by a darker annulus, while the small disk in the lower figure  is surrounded by a lighter annulus. Even though both disks have the same  grey scale, the one inside the darker annulus appears brighter. This is be- cause our vision system is sensitive to relative intensities. It would seem  that the total activity across the image is held constant.   The properties of brightness constancy and brightness contrast are very  important to our vision system. Since we see things in so many different  lighting conditions, if we were not able to compensate for the absolute in- tensity of a scene, we would never learn to recognize things. Grossberg calls  this process of normalization “discounting the illuminant.”   In the rest of this chapter we want to present a neural network architecture  that is consistent with the physical phenomena discussed in this section.  Basic Nonlinear Model  Leaky Integrator  Before we introduce the Grossberg network, we will begin by looking at  some of the building blocks that make up the network. The first building  block is the “leaky” integrator, which is shown in Figure 18.9. The basic  equation for this system is  Time Constant  where      is the system time constant.   18.1   18  dn t    ------------ dt  =  –  n t     +  p t     ,  p  +  -  Leaky Integrator  n  . n   cid:0 1 ε  cid:0  ε dn dt = - n + p  Figure 18.9  Leaky Integrator  The response of the leaky integrator to an arbitrary input   p t    is  n t     =  e t –  n 0     –     – e t  p     d  .  t  1 + ---  0   18.2   18-9   18 Grossberg Network  For example, if the input  zero, Eq.  18.2  will produce  p t    is constant and the initial condition   n 0    is   n t     =  e t –– p 1    .   18.3   A graph of this response, for  response exponentially approaches a steady state value of 1.   and   1=  1=  , is given in Figure 18.10. The   p    1  0.75  n  0.5  0.25  0 0  1  2  3  4  5  t  Figure 18.10  Leaky Integrator Response  n t    will be scaled by the same amount. For example, if the input   There are two important properties of the leaky integrator that we want to  note. First, because Eq.  18.1  is linear, if the input  response  is doubled, then the response will also be doubled, but will maintain the  same shape. This is evident in Eq.  18.3 . Second, the speed of response of  the leaky integrator is determined by the time constant   decreas-  increases, the response becomes  es, the response becomes faster; when  slower.  See Problem P18.1.    is scaled, then the   . When   p        To experiment with the leaky integrator, use the Neural Network Design  Demonstration Leaky Integrator  nnd15li .  Shunting Model  The leaky integrator forms the nucleus of one of Grossberg’s fundamental  neural models: the shunting model, which is shown in Figure 18.11. The  equation of operation of this network is  dn t    ------------ dt  =  –  n t     +  b+    –  n t     p+  –    n t     b-+  p-  ,   18.4   Excitatory  Inhibitory  p+  where   is a nonnegative value representing the excitatory input to the  network  the input that causes the response to increase , and   is a non- negative value representing the inhibitory input  the input that causes the  response to decrease . The biases   are nonnegative constants that  determine the upper and lower limits on the neuron response, as we will  explain next.   and   b+  p-  b-  18-10   Basic Nonlinear Model  Input  Basic Shunting Model  p+  p-  +  -  +  -   cid:0  cid:0 1 ε . n  cid:0  cid:0   b+  +  -  n  +  +  b-  ε dn dt = -n +  b+ - n  p+ -  n + b-  p-  Figure 18.11  Shunting Model  There are three terms on the right-hand side of Eq.  18.4 . When the net  sign of these three terms is positive,   will go up. When the net sign is  negative,   will go down. To understand the performance of the network,  let’s investigate the three terms individually.   n t     n t     –  n t     The first term,  integrator. It is negative whenever  n t    b+  is negative. The second term,   – n t    control. This term will be positive while  zero when  third term,  er limit on   , is a linear decay term, which is also found in the leaky   is positive, and positive whenever  n t    n t     , but will become  . The  , also provides nonlinear gain control. It sets a low-  . This effectively sets an upper limit on  p-  .  , provides nonlinear gain   p+  is less than   b+= b-+ n t    b– -  of   n t     – n t      of   n t     b+  b+    ,   b-  1=  0=   and   Figure 18.12 illustrates the performance of the shunting network when  b+ 1= when the excitatory input  p- right graph  put is increased by a factor of 5, the steady state network response is in- creased by less than a factor of 2. If we were to continue to increase the  excitatory input, we would find that the steady state network response  would increase, but would always be less than   . In the left graph we see the network response  p+ . For the  . Notice that even though the excitatory in- 0=   and the inhibitory input    and   5=  0=  1=  p+  1=  b+  p-  .   If we apply an inhibitory input to the shunting network, the steady state  network response will decrease, but will remain greater than  . To sum- b+ marize the operation of the shunting model, if   and  b– -  will remain between these limits, as shown in the figure in  the left margin.  b– -  falls between   , then   n 0     n t     18  b+  0  -b-  n t   18-11   18 Grossberg Network  p+  1=  p+  5=  1  0.75  0.5  0.25  n t   0.5  1  0.75  0.25  0 0  1  2  3  4  1  2  3  4  5  5  0 0  t  t  Figure 18.12  Shunting Network Response  The shunting model will form the basis for the Grossberg competitive net- work, which we will discuss in the next section. The nonlinear gain control  will be used to normalize input patterns and to maintain relative intensi- ties over a wide range of total intensity.  To experiment with the shunting network, use the Neural Network Design  Demonstration Shunting Network  nnd15sn .  Two-Layer Competitive Network  We are now ready to present the Grossberg competitive network. This net- work was inspired by the operation of the mammalian visual system, which  we discussed in the opening section of this chapter.  Grossberg was influ- enced by the work of Chistoph von der Malsburg [vond73], which was influ- enced in turn by the Nobel-prize-winning experimental work of David  Hubel and Torsten Wiesel [HuWi62].  A block diagram of the network is  shown in Figure 18.13.  There are three components to the Grossberg network: Layer 1, Layer 2  and the adaptive weights. Layer 1 is a rough model of the operation of the  retina, while Layer 2 represents the visual cortex. These models do not ful- ly explain the complexity of the human visual system, but they do illustrate  a number of its characteristics. The network includes short-term memory   STM  and long-term memory  LTM  mechanisms, and performs adapta- tion, filtering, normalization and contrast enhancement. In the following  subsections we will discuss the operation of each of the components of the  network.  As we analyze the various elements of the Grossberg network, you will no- tice the similarity to the Kohonen competitive network of the previous  chapter.  18-12  Short-Term Memory Long-Term Memory   Two-Layer Competitive Network  Layer 1  Retina   Layer 2   Visual Cortex   Input  STM  LTM   Adaptive Weights   Normalization  Contrast  Enhancement  Figure 18.13  Grossberg Competitive Network  Layer 1 Layer 1 of the Grossberg network receives external inputs and normalizes  the intensity of the input pattern.  Recall from Chapter 14 that the Ko- honen network performs best when the input patterns are normalized. For  the Grossberg network the normalization is accomplished by the first layer  of the network.  A block diagram of this layer is given in Figure 18.14. Note  that it uses the shunting model, with the excitatory and inhibitory inputs  computed from the input vector   p  .  18  Input  Layer 1   cid:0  cid:0   cid:0  cid:0  +W1 S1 x S1 +  cid:0  cid:0   cid:0  cid:0   -W1 S1 x S1  -  p S1 x 1  S1  +  -  n1.   cid:0 1 ε  cid:0   +b1  +  +  +  -b1  - n1   cid:0  cid:0   cid:0  cid:0  a1  cid:0  cid:0   S1  εdn1 dt = - n1 +  +b1 - n1  [+W1] p -  n1 + -b1  [-W1] p  Figure 18.14  Layer 1 of the Grossberg Network  The equation of operation of Layer 1 is   dn1 t    -------------- dt  =  n1 t     –  +  b+ 1 n1 t     –   W+     1  p  –  n1 t       b- 1+   W-   1  p  .   18.5   18-13   18 Grossberg Network  As we mentioned earlier, the parameter  sponse. It is chosen so that the neuron responses will be much faster than  the changes in the adaptive weights, which we will discuss in a later sec- tion.   determines the speed of re-    Eq.  18.5  is a shunting model with excitatory input   1  W+    p  , where  Therefore the excitatory input to neuron  vector.  i   is the ith element of the input   The inhibitory input to Layer 1 is   1  W-    p  , where  1  W+  =  1 0  0 0 1  0 .   0 0  1  1  W-  =  0 1  1 1 0  1 .   1 1  0   18.6    18.7   Thus the inhibitory input to neuron   is the sum of all elements of the input  vector, except the ith element.  i  i  The connection pattern defined by the matrices  on-center off-surround pattern. This is because the excitatory input for  neuron    which turns the neuron on  comes from the element of the input  vector centered at the same location  element   , while the inhibitory input   which turns the neuron off  comes from surrounding locations. This type  of connection pattern produces a normalization of the input pattern, as we  will show in the following discussion.   is called an    and   i  1  W+  1  W-  For simplicity, we will set the inhibitory bias  lower limit of the shunting model to zero, and we will set all elements of the  excitatory bias    to zero, which sets the    to the same value, i.e.,  b+ 1  b- 1  b+ 1 i  =  b+ 1  ,   i  =  1 2   S1       ,   18.8   so that the upper limit for all neurons will be the same.  To investigate the normalization effect of Layer 1, consider the response of  neuron   :  i  18-14  On-Center Off-Surround   Two-Layer Competitive Network  1 t    dni -------------- dt    =  –  1 t    ni  b+ 1    –  +  1 t    ni  pi  –  1 t    ni   18.9   .  pj    j  i  In the steady state    1 t    dt    dni  0=    we have  0  =  1– ni  b+ 1    +  1– ni  pi  1 – ni  .  pj   18.10   If we solve for the steady state neuron output    we find  j  i  1 ni  We now define the relative intensity of input   i   to be  Then the steady state neuron activity can be written  1 ni  =  .  b+ 1pi ---------------------- S1 +  pj  1  j  1=  pi  pi ----= P   where   P  S1 =  j  1=  .  pj  1 ni  =  b+ 1P  pi -------------  1 P+     .  1 ni  Therefore  the magnitude of the total input  bounded:   will be proportional to the relative intensity   , regardless of  . In addition, the total neuron activity is   pi  P  18  S1   j  1=  1 nj  =  S1   j  1=  b+ 1P  pj -------------  1 P+     =  b+ 1P  -------------  1 P+       b+ 1  .  The input vector is normalized so that the total activity is less than  while the relative intensities of the individual elements of the input vector  are maintained. Therefore, the outputs of Layer 1,  , code the relative in- put intensities,  , rather than the instantaneous fluctuations in the total  pi . This result is produced by the on-center off-surround con- input activity,  P nection pattern of the inputs and the nonlinear gain control of the shunting  model.  1 ni  ,   b+ 1  Note that Layer 1 of the Grossberg network explains the brightness con- stancy and brightness contrast characteristics of the human visual system,  which we discussed on page 15-8. The network is sensitive to the relative   18-15   18.11    18.12    18.13    18.14    18 Grossberg Network  -  -  +  -  -  intensities of an image, rather than absolute intensities. In addition, exper- imental evidence has shown that the on-center off-surround connection  pattern is a characteristic feature of the receptive fields of retinal ganglion  cells [Hube88].  The receptive field is an area of the retina in which the pho- toreceptors feed into a given cell. The figure in the left margin illustrates  the on-center off-surround receptive field of a typical retinal ganglion cell.  A “+” indicates an excitatory region, and a “-” indicates an inhibitory re- gion. It is a two-dimensional pattern, as opposed to the one-dimensional  connections of Eq.  18.6  and Eq.  18.7 .   2 2+  To illustrate the performance of Layer 1, consider the case of two neurons,  with   b+ 1  1=  0.1  ,   =    :  0.1    =  –  1 t    n1  +  – 1  1 t    n1  p1  –  1 t   p2 n1  ,  0.1    =  –  1 t    n2  +  – 1  1 t    n2  p2  –  1 t   p1 n2  .  1 t    dn1 -------------- dt  1 t    dn2 -------------- dt   18.15    18.16   The response of this network, for two different input vectors, is shown in  Figure 18.15. For both input vectors, the second element is four times as  large as the first element, although the total intensity of the second input  vector is five times as large as that of the first input vector  50 vs. 10 . From  Figure 18.15 we can see that the response of the network maintains the rel- ative intensities of the inputs, while limiting the total response. The total  response      will always be less than 1.  +  1 t    n1  1 t    n2  1  0.75  0.5  0.25  1 n2  1 n1  1  0.75  0.5  0.25  1 n2  1 n1  p1  =  2 8  p2  =  10 40  0 0  0.05  0.15  0.2  0 0  0.05  0.15  0.2  0.1 t  0.1 t  Figure 18.15  Layer 1 Response  To experiment with Layer 1 of the Grossberg network, use the Neural Net- work Design Demonstration Grossberg Layer 1  nnd15gl1 .  18-16   Short-Term Memory  Two-Layer Competitive Network  Layer 2 Layer 2 of the Grossberg network, which is a layer of continuous-time in- stars, performs several functions. First, like Layer 1, it normalizes total ac- tivity in the layer. Second, it contrast enhances its pattern, so that the  neuron that receives the largest input will dominate the response.  This is  closely related to the winner-take-all competition in the Hamming network  and the Kohonen network.  Finally, it operates as a short-term memory   STM  by storing the contrast-enhanced pattern.  Figure 18.16 is a diagram of Layer 2. As with Layer 1, the shunting model  forms the basis for Layer 2. The main difference between Layer 2 and Lay- er 1 is that Layer 2 uses feedback connections. The feedback enables the  network to store a pattern, even after the input has been removed. The  feedback also performs the competition that causes the contrast enhance- ment of the pattern. We will demonstrate these properties in the following  discussion.  Layer 2  ++  On-Center   cid:0  cid:0   W2 S2 x S1   cid:0  cid:0  +W2 + S2 x S2  +b2  a1  S1   cid:0  cid:0 1 ε n2.  cid:0  cid:0   +  -  +  -  - n2  +  f 2   cid:0  cid:0  a2  cid:0  cid:0   cid:0  cid:0   -b2  S2 +  cid:0  cid:0  -W2  cid:0  cid:0  S2 x S2  Off-Surround  εdn2 dt = - n2 +  +b2 - n2  {[+W2] f 2 n2  + W2 a1}  -  n2 + -b2  [-W2] f 2 n2   Figure 18.16  Layer 2 of the Grossberg Network  18  The equation of operation of Layer 2 is   dn2 t    -------------- dt  =  n2 t     –  +    b+ 2 n2 t    – b- 2+ n2 t        W+ 2   W- 2   –    f2 n2 t          f2 n2 t          W2a1 +     18.17   18-17   18 Grossberg Network  1  =  W+  2 W+   provides on-center feedback connections, and   This is a shunting model with excitatory input  where  sists of adaptive weights, analogous to the weights in the Kohonen net- work. The rows of  The inhibitory input to the shunting model is  W-   provides off-surround feedback connections.  , after training, will represent the prototype patterns.     con-  f2 n2 t       1 2 W-  , where     W2a1 + W2  f2 n2 t     2 W-  W2  W+  ,               2  =  2 2+  To illustrate the performance of Layer 2, consider a two-neuron layer with    =  0.1  ,   b+ 2  =  ,   b- 2  =  ,   W2  =  1 1  0 0  =  0.9 0.45 0.45 0.9  ,   18.18   w2 w2  1 2  T T  and  0.1    2 t    dn1 -------------- dt  0.1    2 t    dn2 -------------- dt  f2 n     =  10 n 2 ------------------- n 2 1 +  .   18.19   The equations of operation of the layer will be  =  –  2 t    n1  +  – 1  2 t    n1   f2 n1      2 t       +  1  w2  Ta1     18.20   =  –  2 t    n2  +  – 1  2 t    n2   f2 n2      2 t       +  2  w2  Ta1     18.21   –  2 t   f2 n2 n1     2 t        –  2 t   f2 n1 n2     2 t       .  W2  Note the relationship between these equations and the Hamming and Ko- honen networks. The inputs to Layer 2 are the inner products between the  prototype patterns  rows of the weight matrix    and the output of Layer  1  normalized input pattern . The largest inner product will correspond to  the prototype pattern closest to the input pattern. Layer 2 then performs a  competition between the neurons, which tends to contrast enhance the out- put pattern — maintaining large outputs while attenuating small outputs.  This contrast enhancement is generally milder than the winner-take-all  competition of the Hamming and Kohonen networks. In the Hamming and  Kohonen networks, the competition drives all but one of the neuron out- puts to zero. The exception is the one with the largest input. In the Gross- berg network, the competition maintains large values and attenuates small  values, but does not necessarily drive all small values to zero. The amount   18-18  Contrast Enhance   Two-Layer Competitive Network  of contrast enhancement is determined by the transfer function  will see in the next section.  f2  , as we   Figure 18.17 illustrates the response of Layer 2 when the input vector  a1 0.2 0.8 is applied for     the steady state result obtained from our Layer 1 example   0.25   seconds and then removed.  =  T  w2  Ta1  2  2 t    n2  1  0.75  0.5  0.25  0 0  w2  Ta1  1  2 t    n1  t  0.1  0.2  0.3  0.4  0.5  Figure 18.17  Layer 2 Response  There are two important characteristics of this response. First, even before  the input is removed, some contrast enhancement is performed. The inputs  to Layer 2 are  w2  Ta1  1  =  0.9 0.45  =  0.54  ,  w2  Ta1  2  =  0.45 0.9  =  0.81  .  0.2 0.8  0.2 0.8  18   18.22    18.23    times as much input as the first neu- Therefore the second neuron has  ron. However, after    times the output of the first neuron. The contrast between high and low has  been increased dramatically.   seconds the output of the second neuron is   0.25  6.34  1.5  The second characteristic of the response is that after the input has been  set to zero, the network further enhances the contrast and stores the pat- tern. In Figure 18.17 we can see that after the input is removed  at    seconds  the output of the first neuron decays to zero, while the output of  the second neuron reaches a steady state value of  maintained, even after the input is removed.  Grossberg calls this behavior  reverberation [Gross76].  It is the nonlinear feedback that enables the net-  . This output is   0.79  0.25  18-19   Oriented Receptive Field  18 Grossberg Network  work to store the pattern, and the on-center off-surround connection pat- tern  determined by    that causes the contrast enhancement.   and   W+  W-  2  2  As an aside, note that we have used the on-center off-surround structure in  both layers of the Grossberg network. There are other connection patterns  that could be used for different applications. Recall, for instance, the emer- gent segmentation problem discussed earlier in this chapter. A structure  that has been proposed to implement this mechanism is the oriented recep- tive field [GrMi89], which is shown in the left margin. For this structure,  the “on”  excitatory  connections come from one side of the field  indicated  by the blue region , and the “off”  inhibitory  connections come from the  other side of the field  indicated by the white region .   The operation of the oriented receptive field is illustrated in Figure 18.18.  When the field is aligned with an edge, the corresponding neuron is acti- vated  large response . When the field is not aligned with an edge, then the  neuron is inactive  small response . This explains why we might perceive  an edge where none exists, as can be seen in the right-most receptive field  shown in Figure 18.18.  For a complete discussion of oriented receptive fields and how they can be  incorporated into a neural network architecture for preattentive vision, see  [GrMi89]. This paper also discusses a mechanism for featural filling-in.  Active  Active  Inactive  Figure 18.18  Operation of Oriented Receptive Field  n2i 0   i  Choice of Transfer Function The behavior of Layer 2 of the Grossberg network depends very much on  the transfer function  . For example, suppose that an input has been  applied for some length of time, so that the output has stabilized to the pat- tern shown in the left margin.  Each point represents the output of an in- dividual neuron.  If the input is then removed, Figure 18.19 demonstrates  how the choice of   will affect the steady state response of the network.   See [Gross82].   f2 n     f2 n     18-20   Two-Layer Competitive Network  f 2 n   Linear  Stored Pattern  n2 ∞   Comments  Perfect storage of any pattern, but amplifies noise.  Amplifies noise, reduces contrast.  Winner-take-all, suppresses noise, quantizes total activity.  Supresses noise, contrast enhances, not quantized.  18  Figure 18.19  Effect of Transfer Function   f2 n       after [Gross82]   If the transfer function is linear, the pattern is perfectly stored. Unfortu- nately, the noise in the pattern will be amplified and stored as easily as the  significant inputs.  See Problem P18.6.  If the transfer function is slower- than-linear  e.g.,   , the steady state response is independent  of the initial conditions; all neurons that begin with nonzero values will  come to the same level in the steady state. All contrast is eliminated, and  noise is amplified.   e n––  f2 n     =  1  Faster-than-linear transfer functions  e.g.,    produce a winner- take-all competition. Only those neurons with the largest initial values are  stored; all others are driven to zero. This minimizes the effect of noise, but  quantizes the response into an all-or-nothing signal  as in the Hamming  and Kohonen networks .  =  f2 n     n 2  A sigmoid function is faster-than-linear for small signals, approximately  linear for intermediate signals and slower-than-linear for large signals.  When a sigmoid transfer function is used in Layer 2, the pattern is contrast  enhanced; larger values are amplified, and smaller values are attenuated.  All initial neuron outputs that are less than a certain level  called the  quenching threshold by Grossberg [Gros76]  decay to zero. This merges the  noise suppression of the faster-than-linear transfer functions with the per- fect storage produced by linear transfer functions.  Slower than  Linear  Faster than  Linear  Sigmoid  18-21   Long-Term Memory  18 Grossberg Network  To experiment with Layer 2 of the Grossberg network, use the Neural Net- work Design Demonstration Grossberg Layer 2  nnd15gl2 .  W2  Learning Law The third component of the Grossberg network is the learning law for the  adaptive weights  . Grossberg calls these adaptive weights the long-term  memory  LTM . This is because the rows of   will represent patterns that  have been stored and that the network will be able to recognize. As in the  Kohonen and Hamming networks, the stored pattern that is closest to an  input pattern will produce the largest output in Layer 2. In the next sub- section we will look more closely at the relationship between the Grossberg  network and the Kohonen network.  W2  One learning law for   W2   is given by  2 t    dwi j ----------------- dt  =      –  2 wi j  t     +  2 t   nj ni  1 t       .   18.24   The first term in the bracket on the right-hand side of Eq.  18.24  is a pas- sive decay term, which we have seen in the Layer 1 and Layer 2 equations,  while the second term implements a Hebbian-type learning. Together,  these terms implement the Hebb rule with decay, which was discussed in  Chapter 13.   Recall from Chapter 13 that it is often useful to turn off learning  and for- getting  when   is not active. This can be accomplished by the following  learning law:  2 t    ni  =  ni  2 t       –  2 wi j  t     +  1 t    nj    ,   18.25   or, in vector form,  =  ni  2 t       –    w2 i  t       +  n1 t       ,   18.26   w2 where  i Eq.  4.4  .  t      is a vector composed of the elements of the ith row of   W2    see   , which allows learning  and forgetting  to occur only when   The terms on the right-hand side of Eq.  18.25  are multiplied  gated  by  2 t     is not  ni zero. This is the continuous-time implementation of the instar learning  rule, which we introduced in Chapter 13  Eq.  15.32  . In the following sub- section we will demonstrate the equivalence of Eq.  18.25  and Eq.  15.32 .  2 t    ni  2 t    dwi j ----------------- dt  i  d w2   t    --------------------- dt  18-22   Two-Layer Competitive Network  2 2+  To illustrate the performance of the Grossberg learning law, consider a net- work with two neurons in each layer. The weight update equations would  be  2  t    dw1 1 ------------------- dt  2  t    dw1 2 ------------------- dt  2  t    dw2 1 ------------------- dt  2  t    dw2 2 ------------------- dt  =  2 t    n1    –  2  w1 1  t     +  1 t    n1    ,  =  2 t    n1    –  2  w1 2  t     +  1 t    n2    ,  =  2 t    n2    –  2  w2 1  t     +  1 t    n1    ,  =  2 t    n2    –  2  w2 2  t     +  1 t    n2    ,   18.27    18.28    18.29    18.30      has been set to   where the learning rate coefficient  ample, we will assume that two different input patterns are alternately  presented to the network for periods of 0.2 seconds at a time. We will also  assume that Layer 1 and Layer 2 converge very quickly, in comparison  with the convergence of the weights, so that the neuron outputs are effec- tively constant over the 0.2 seconds. The Layer 1 and Layer 2 outputs for  the two different input patterns will be  . To simplify our ex-  1  for pattern 1:   n1  =  ,   n2  =  for pattern 2:   n1  =  ,   n2  =  0.9 0.45  0.45 0.9  ,  .  1 0  0 1   18.31    18.32   18  Pattern 1 is coded by the first neuron in Layer 2, and pattern 2 is coded by  the second neuron in Layer 2.  2  2  =  t     t     0.45  w1 2   and     is only adjusted during those periods when  1 t    n1  Figure 18.20 illustrates the response of the adaptive weights, beginning  with all weights set to zero. Note that the first row of the weight matrix   is non-   w1 1 zero, and that it converges to the corresponding   and  0.9 1 t     .  The elements in the first row of the weight matrix are indi- n2 cated by the blue lines in Figure 18.20.  Also, the second row of the weight  2 t      matrix   n2 is nonzero, and it converges to the corresponding    0.45 and   .  The elements in the second row of the weight matrix are  indicated by the black lines in Figure 18.20.     is only adjusted during those periods when  =   pattern     pattern    2 t    n1 =  1 t    n2  1 t    n1   and   w2 1  w2 2  0.9  n1  n1  t     t     =  2  2  18-23   18 Grossberg Network  1  0.75  0.5  0.25  0 0  2  w1 1  t     2  w2 2  t     2  w1 2  t     2  w2 1  t     0.5  1  1.5  2  2.5  3  Figure 18.20  Response of the Adaptive Weights  To experiment with the adaptive weights, use the Neural Network Design  Demonstration Adaptive Weights  nnd15aw .  Relation to Kohonen Law  In the previous section we indicated that the Grossberg learning law was a  continuous-time version of the instar learning law, which we discussed in  Chapter 13. Now we want to demonstrate this fact. We will also show that  the Grossberg network is, in its simplest form, a continuous-time version  of the Kohonen competitive network of Chapter 14.  To begin, let’s repeat the Grossberg learning law of Eq.  18.25 :  i  d w2   t    --------------------- dt  =  ni  2 t       –    w2 i  t       +  n1 t       .   18.33   If we approximate the derivative by   i  d w2   t    --------------------- dt  w2 t    i --------------------------------------------    w2 t t+  – t  i  ,     18.34   then we can rewrite Eq.  18.33  as  w2 i  t t+      =  w2 i  t     t  +    2 t     ni    –  w2  i  t     +  n1 t       .   18.35    Compare this equation with the instar rule that was presented in Chapter  13 in Eq.  15.33 .  If we rearrange terms, this can be reduced to  w2 i  t t+     =  1  t –    2 t     ni   w2 i  t     t  +    2 t    n1 t         .  ni   18.36   To simplify the analysis further, assume that a faster-than-linear transfer  function is used in Layer 2, so that only one neuron in that layer will have   18-24   Relation to Kohonen Law  a nonzero output; call it neuron  will be updated:  i  . Then only row   i   of the weight matrix   w2  t t+     =  1 '–   w2 i  t     +  ' n1 t     ,  i   18.37   where   '  =   t    2 t     ni  .  This is almost identical to the Kohonen rule for the competitive network  that we introduced in Chapter 14 in Eq.  16.13 . The weight vector for the  winning neuron  with nonzero output  will be moved toward  , which is a  normalized version of the current input pattern.  n1  There are three major differences between the Grossberg network that we  have presented in this chapter and the basic Kohonen competitive network.  First, the Grossberg network is a continuous-time network  satisfies a set  of nonlinear differential equations . Second, Layer 1 of the Grossberg net- work automatically normalizes the input vectors. Third, Layer 2 of the  Grossberg network can perform a “soft” competition, rather than the win- ner-take-all competition of the Kohonen network. This soft competition al- lows more than one neuron in Layer 2 to learn. This causes the Grossberg  network to operate as a feature map.  18  18-25   18 Grossberg Network  Summary of Results  Basic Nonlinear Model  Leaky Integrator  dn t    ------------ dt  =  –  n t     +  p t     p  +  -  Leaky Integrator  n  . n   cid:0 1 ε  cid:0  ε dn dt = - n + p  Shunting Model  dn t    ------------ dt  =  –  n t     +  b+    –  n t     p+  –    n t     b-+  p-  Input  Basic Shunting Model  p+  p-  +  -  +  -   cid:0  cid:0 1 ε . n  cid:0  cid:0   b+  +  -  n  +  +  b-  ε dn dt = -n +  b+ - n  p+ -  n + b-  p-  b+  0  -b-  n t   18-26   Summary of Results  Two-Layer Competitive Network  Layer 1  Retina   Layer 2   Visual Cortex   Input  STM  LTM   Adaptive Weights   Normalization  Contrast  Enhancement  Layer 1  Input  Layer 1   cid:0  cid:0   cid:0  cid:0  +W1 S1 x S1 +  cid:0  cid:0   cid:0  cid:0   -W1 S1 x S1  -  p S1 x 1  S1  +  -  n1.   cid:0 1 ε  cid:0   +b1  +  +  +  -b1  - n1   cid:0  cid:0   cid:0  cid:0  a1  cid:0  cid:0   S1  εdn1 dt = - n1 +  +b1 - n1  [+W1] p -  n1 + -b1  [-W1] p  dn1 t    -------------- dt  =  n1 t     –  +  b+ 1 n1 t     –   W+     1  p  –  n1 t       b- 1+   W-   1  p  1  W+  =  1  W-  =  1 0  0 0 1  0   0 0  1  0 1  1 1 0  1   1 1  0  On-Center  Off-Surround     18-27  18   Layer 2  18 Grossberg Network  Steady State Neuron Activity  1 ni  =  b+ 1P  pi -------------  1 P+     , where   pi   and   P  pi ----= P  S1 =  j  1=  pj  Layer 2  ++  On-Center   cid:0  cid:0   W2 S2 x S1   cid:0  cid:0  +W2 + S2 x S2  +b2  a1  S1   cid:0  cid:0 1 ε n2.  cid:0  cid:0   +  -  +  -  - n2  +  f 2   cid:0  cid:0  a2  cid:0  cid:0   cid:0  cid:0   -b2  S2 +  cid:0  cid:0  -W2  cid:0  cid:0  S2 x S2  Off-Surround  εdn2 dt = - n2 +  +b2 - n2  {[+W2] f 2 n2  + W2 a1}  -  n2 + -b2  [-W2] f 2 n2   dn2 t    -------------- dt  =  n2 t     –  +  b+ 2 n2 t     –   W+ 2       f2 n2 t          W2a1 +    n2 t       b- 2+   W-   2  f2 n2 t          –  18-28   Summary of Results  Choice of Transfer Function  f 2 n   Linear  Stored Pattern  n2 ∞   Comments  Perfect storage of any pattern, but amplifies noise.  Amplifies noise, reduces contrast.  Winner-take-all, suppresses noise, quantizes total activity.  Supresses noise, contrast enhances, not quantized.  Learning Law  i  d w2   t    --------------------- dt  =  ni  2 t       –    w2 i  t       +  n1 t        Continuous-Time Instar Learning   18  Slower than  Linear  Faster than  Linear  Sigmoid  18-29   18 Grossberg Network  Solved Problems  P18.1 Demonstrate the effect of the coefficient    on the performance of  the leaky integrator, which is shown in Figure P18.1, with the in- put   1=  p    .  p  +  -  Leaky Integrator  n  . n   cid:0 1 ε  cid:0  ε dn dt = - n + p  dn t    ------------ dt  =  –  n t     +  p t     .  Figure P18.1  Leaky Integrator  The equation of operation for the leaky integrator is  The solution to this differential equation, for an arbitrary input   p t   , is  n t     =  e t –  n 0     –     – e t  –  p t    d  .  t  1 + ---  0  If   p t   1=  , the solution will be  n t     =  e t –  n 0     –     – e t  d  .  t  1 + ---  0  We want to show how this response changes as a function of  sponse will be    . The re-  n t     =  e t –  n 0     +  e t –– 1    =  e t –    n 0     1–    1+  .  n 0     n 0     , approaching the steady state response of   , and then grows exponentially  or decays ex-  is greater than or less than   is decreased,  n    This response begins at  ponentially, depending on whether or not  1 the response becomes faster  since  steady state value remains constant. Figure P18.2 illustrates the responses  for  . Notice that the steady state value  remains 1 for each case. Only the speed of response changes.   decays more quickly , while the    1 0.5 0.25 0.125  , with   e t –  . As   n 0     1=  0=  =             18-30   Solved Problems    =  0.125  1.25  1  0.75  0.5  0.25    1=  0 0  0.5  1.5  2  1 t  Figure P18.2  Effect of      on Leaky Integrator Response  P18.2 Again using the leaky integrator of Figure P18.1, set      1=  .  i. Find a difference equation approximation to the leaky inte-  grator differential equation by approximating the deriva- tive using  dn t    ------------  dt    n t t+  n t      ----------------------------------- t  –  .  ii. Using   t  =  0.1  , compare the response of this difference   equation with the response of the differential equation for  p t  0 t  . Compare the two over the range   1= . 1   and      n 0     0=  18  iii. Using the difference equation model for the leaky integra- tor, show that the response is a weighted average of previ- ous inputs.  i.  If we make the approximation to the derivative, we find  n t t+  n t      ----------------------------------- t  –  =  –  n t     +  p t     or  n t t+     =  n t    t  +    –  n t     +  p t       =  1 t–  n t     +    t  p t     .  ii. If we let   t  =  0.1   we obtain the difference equation  0.1+  n t     =  0.9n t     +  0.1p t     .  If we let   p t   1=   and   n 0     0=  , then we can solve for   n t   :  18-31   18 Grossberg Network  n 0.1      =  0.9n 0     +  0.1p 0     =  0.1  n 0.2      =  0.9n 0.1      +  0.1p 0.1      =  0.9 0.1    +  0.1 1   0.19  ,  n 0.3      =  0.9n 0.2      +  0.1p 0.2     =  0.9 0.19      +  0.1 1   0.271  ,  =  =  n 0.4     =  0.9n 0.3     +  0.1p 0.3     =  0.9 0.271      +  0.1 1   =  0.3439  ,  n 0.5      =  0.9n 0.4      +  0.1p 0.4     =  0.9 0.3439      +  0.1 1   =  0.4095  ,  n 0.6      =  0.4686  ,   n 0.7     =  0.5217  ,   n 0.8      =  0.5695  ,  n 0.9     =  0.6126  ,   n 1.0     =  0.6513  .  From Problem P18.1, the solution to the differential equation is  n t     =  e t –  n 0     +  e t –– 1    =  e t–– 1    .  Figure P18.3 illustrates the relationship between the difference equation  solution and the differential equation solution. The black line represents  the differential equation solution, and the blue circles represent the differ- ence equation solution. The two solutions are very close, and can be made  arbitrarily close by decreasing the interval   t  .  1  0.75  0.5  0.25  0 0  18-32  0.25  0.5  0.75  1  Figure P18.3  Comparison of Difference and Differential Equations  iii. Consider again the difference equation model of the leaky integrator,  which we developed in part  ii :  0.1+  n t     =  0.9n t     +  0.1p t     .  If we start from a zero initial condition we find  n 0.1     =  0.9n 0     +  0.1p 0     =  0.1p 0     ,  n 0.2      =  0.9n 0.1      +  0.1p 0.1      =  0.9 0.1p 0         +  0.1p 0.1      =  0.09p 0     +  0.1p 0.1       Solved Problems  n 0.3    =  0.9n 0.2      +  0.1p 0.2      =  0.081p 0     +  0.09p 0.1      +  0.1p 0.2         n k0.1    =  0.1    0.9  k  1– p 0     +  0.9  k  2– p 0.1      p k +  1–  +     0.1      .  Therefore the response of the leaky integrator is a weighted average of pre- vious inputs,  . Note that the recent inputs con- tribute more to the response than the early inputs.      p k   p 0    p 0.1   1–  0.1           P18.3 Find the response of the shunting network shown in Figure P18.4   for     1=  ,   b+  1=  ,   b-  1=  ,   p+  0=  ,   p-  10=   and   n 0     =  0.5  .  Input  Basic Shunting Model  +  -  +  -   cid:0  cid:0 1 ε . n  cid:0  cid:0   b+  +  -  n  +  +  b-  ε dn dt = -n +  b+ - n  p+ -  n + b-  p-  Figure P18.4  Shunting Network  The equation of operation of the shunting network is  =  –  n t     +  b+    –  n t     p+  –    n t     b-+  p-  .  For the given parameter values this becomes  =  –  n t     –    n t     1+  10  =  –  11n t     10–  .  The solution to this equation is  n t     =  – e 11t  n 0     –    – e 11 t  10–   d  ,  +  t  0  p+  p-  dn t    ------------ dt  dn t    ------------  dt  18-33  18   18 Grossberg Network  or  n t     =  – e 11t  0.5  +  10  1  –– ------–  11  e 11t    .  The response is plotted in Figure P18.5.  0.5  1  0  -0.5  -1 0  18-34  0.25  0.5  0.75  1  Figure P18.5  Shunting Network Response  There are two things to note about this response. First, as with all shunting  networks, the response will never drop below  .  1–  is increased, the steady state response will de- As the inhibitory input  crease, but it can never be less than  . The second characteristic of the  response is that the speed of the response will increase as the input is in- creased. For instance, if the input were changed from  ,  100 the response would be  , which in this case is   10=  b-–  b-–   to   p-  p-  p-  =  n t     =  – e 101t  0.5  +   –  100  1 e 101t –– ---------  101    .  Since   – e 101t   decays more rapidly than   – e 11t  , the response will be faster.  P18.4 Find the response of Layer 1 of the Grossberg network for the case   of two neurons, with  p onstrate the effect of   c 2c  =  T  b+ 1  1=  ,   b- 1  0=  ,     1=   and input vector   . Assume that the initial conditions are set to zero. Dem-  c   on the response.  The Layer 1 differential equations for this case are  1 t    dn1 -------------- dt  =  –  1 t    n1  +  – 1  1 t    n1   c   –  1 t    2c n1    =  –  3c+ 1  1 t     n1  c+  ,   Solved Problems  1 t    dn2 -------------- dt  =  –  1 t    n2  +  – 1  1 t    n2   2c    –  1 t    c  n2  =  –  3c+ 1  1 t     n2  2c+  .  The solutions to these equations would be  1 t    n1  3c+  t  – e 1  =  1 0    n1  3c+   t –    – e 1  c  d  ,  1 t    n2  =  3c+  t  – e 1  1 0    n2  3c+   t –    – e 1  2c   d  .  +  t  0  +  t  0  If the initial conditions are set to zero, these equations reduce to  1 t    n1  =     c  1 e 1 –– ---------------  3c+ 1  3c+  t    ,  1 t    n2  =     2c  1 e 1 –– ---------------  3c+ 1  3c+  t    .  Note that the outputs of Layer 1 retain the same relative intensities as the  inputs; the output of neuron 2 is always twice the output of neuron 1. This  behavior is consistent with Eq.  18.13 . In addition, the total output inten- sity   , as predicted in Eq.  18.14 .  1 t    n1  is increased, it has two effects on the response. First, the steady state   As  values increase slightly. Second, the response becomes faster, since  – e 1   decays more rapidly as     is never larger than    increases.  1 t    n2  3c+  1=  b+ 1  +  c  c  t  P18.5 Consider Layer 2 of the Grossberg network. Assume that the input  to Layer 2 is applied for some length of time and then removed  set  to zero .  i. Find a differential equation that describes the variation in   the total output of Layer 2,  18  after the input to Layer 2 has been removed.  ii. Find a differential equation that describes the variation in   N2 t     2 t    nk  ,  S2 =  k  1=  the relative outputs of Layer 2, 2 t    ni ------------ N2 t     2 t    ni  =  ,  18-35   18 Grossberg Network  after the input to Layer 2 has been removed.  i. The operation of Layer 2 is described by Eq.  18.17 :  dn2 t    -------------- dt  =  n2 t     –  +  b+ 2 n2 t     – b- 2+ n2 t      W+ 2    W- 2     –  f2 n2 t          .  f2 n2 t          W2a1 +    If the input is removed, then  inhibitory bias  b+ 2  . The response of neuron    to   b+ 2  b- 2  W2a1   is zero. For simplicity, we will set the   to zero, and we will set all elements of the excitatory bias   i   is then given by  2 t    dni -------------- dt    =  –  2 t    ni  b+ 2    –  +  2 t    ni   f2 ni     2 t          –  2 t    ni  f2 nk    2 t            i  k  .      This can be rearranged to produce  2 t    dni -------------- dt    =  –  2 t    ni  +  b+ 2 f2 ni      2 t         –  2 t    ni  f2 nk    2 t       .  S2   k  1=              If we then make the definition  F2 t     f2 nk    2 t        ,  S2 =  k  1=  we can simplify the equation to  =  1 F2 t    +  –  2 t     ni  +  b+ 2 f2 ni      2 t          .  To get the total activity, sum this equation over   i   to produce  =  1 F2 t    +  N2 t     –  +  b+ 2 F2 t         .  This equation describes the variation in the total activity in Layer 2 over  time.  ii. The derivative of the relative activity is  2 t    dni -------------- dt    dN2 t    --------------- dt  18-36   Solved Problems  2 t    ni      =  d td  d ni td  2 t    ------------ N2 t     =  1  ------------ N2 t     d td    2 t    ni    –  2 t    ni ------------------- 2 N2 t      d td  N2 t         .  If we then substitute our previous equations for these derivatives, we find    d td  2 t    ni      =  1  ------------ N2 t       –  1 F2 t    +  2 t     ni  +  b+ 2 f2 ni      2 t            –  2 t    ni ------------ N2 t       –  1 F2 t    +  N2 t     +  b+ 2 F2 t            .  Two terms on the right-hand side will cancel to produce    d td  2 t    ni      =  1  ------------ N2 t       b+ 2 f2 ni      2 t           –    b+ 2 F2 t           ,  2 t    ni ------------ N2 t       d td  2 t    ni      =  b+ 2F2 t    ------------------- N2 t     2 t    f2 ni     ------------------ F2 t     –  2 t    ni ------------ N2 t     .  We can put this in a more useful form if we expand the terms in the brack- ets:  f2 ni 2 t        ------------------ F2 t     –  2 t    ni ------------ N2 t     =  1  ------------------------ f2 ni   F2 t   N2 t       2 t      N2 t     –  2 t   F2 t    ni    18  =  1  ------------------------ g2 ni   F2 t   N2 t     =  2 t    ni  ------------------------ F2 t   N2 t     S2   k  1=  2 t     2 t      ni  2 t    nk  –  2 t    ni  g2 nk     2 t     2 t      nk  S2   k  1=  S2   k  1=  2 t    g2 ni   nk    2 t       –  g2 nk     2 t           ,  or  where  Combining this expression with our previous equation, we obtain  g2 ni    2 t        =  f2 ni 2 t        ------------------ 2 t    ni  .  18-37   18 Grossberg Network    d td  2 t    ni      =  b+ 2ni  2 t     2 t    g2 ni   nk    2 t       –  g2 nk     2 t          .  This form of the differential equation describing the evolution of the rela- tive outputs is very useful in demonstrating the characteristics of Layer 2,  as we will see in the next solved problem.  P18.6 Suppose that the transfer function in Layer 2 of the Grossberg net-  work is linear.  i. Show that the relative outputs of Layer 2 will not change af-  ter the input has been removed.  ii. Under what conditions will the total output of Layer 2 decay   to zero after the input has been removed?  i. From Problem P18.5 we know that the relative outputs of Layer 2, after  the input has been removed, evolve according to  S2   k  1=  S2   k  1=    d td  2 t    ni      =  b+ 2ni  2 t     2 t    g2 ni   nk    2 t       –  g2 nk     2 t          .  If the transfer function for Layer 2,   f2 n     , is linear, then  Therefore  f2 n     =  c n  .  g2 n     =  f2 n    ----------- n  =  c n ------- n  =  c  .  If we substitute this expression into our differential equation, we find    d td  2 t    ni      =  b+ 2ni  2 t     2 t    c nk  c–    =  0  .  S2   k  1=  Therefore the relative outputs do not change.  ii. From Problem P18.5, the total output of Layer 2, after the input has  been removed, evolves according to  =  1 F2 t    +  N2 t     –  +  b+ 2 F2 t         .  dN2 t    --------------- dt  18-38   Solved Problems  If   f2 n      is linear, then  F2 t     =  f2 nk    2 t        =  2 t     c nk  =  c  2 t    nk  =  c N2 t     .  S2   k  1=  S2   k  1=  S2   k  1=  Therefore the differential equation can be written  dN2 t    --------------- dt  =  –  + 1  c N2 t     N2 t     +  b+ 2 c N2 t         =  –    1  –  b+ 2c  c N2 t     N2 t     .  +  To find the equilibrium solutions of this equation, we set the derivative to  zero:  0  –=    1  –  b+ 2c  c N2 t     N2 t     .  +  Therefore there are two equilibrium solutions:  N2 t     0=   or   N2 t     =  b+ 2c 1– ------------------- c  .  We want to know the conditions under which the total output will converge  to each of these possible solutions. Consider two cases:  1.  1    b+ 2c For this case, the derivative of the total output,  dN2 t    --------------- dt  –=    1  –  b+ 2c  c N2 t     N2 t     ,  +  18  will always be negative for positive  .  Recall that the outputs of  Layer 2 are never negative.  Therefore, the total output will decay  to zero.  N2 t     N2 t     0=  lim  t  2.  1    b+ 2c b+ 2c  a  If  be negative until  zero. Therefore,  N2 0         1– N2 t     , then the derivative of the total output will   c , when the derivative will be   =  b+ 2c   c  1–  N2 t     =  lim  t  b+ 2c   1– ------------------------ c  .  18-39   18 Grossberg Network      N2 0     b+ 2c  b  If  be positive until  ro. Therefore,  1– N2 t      c =  , then the derivative of the total output will  b+ 2c , when the derivative will be ze-    c  1–  N2 t     =  lim  t  b+ 2c   1– ------------------------ c  .  Therefore, if the transfer function of Layer 2 is linear, the total output will  decay to zero if  , then the total output will converge to  b+ 2c  As an example of these results, consider the following Layer 2 equations:  . In any case, the relative outputs will remain constant.  b+ 2c  b+ 2c  . If    c  1–      1  1  2 t    dn1 -------------- dt  2 t    dn2 -------------- dt  =  –  2 t    n1  +    1.5 n1  –  2 t      n1   2 t       –  2 t    n2 n1    2 t       ,  =  –  2 t    n2  +    1.5 n2  –  2 t      n2   2 t       –  2 t    n1 n2    2 t       .  For this case,  ,  1= put will converge to     b+ 2  =  1.5   and   c  1=  , therefore   1    b+ 2c  . The total out-  N2 t     =  lim  t  b+ 2c   1– ------------------------ c  =    1.5 1– --------------------- 1  =  0.5  .  In Figure P18.6 we can see the response of Layer 2 for two different sets of  initial conditions:  n2 0     =   and   n2 0     =  0.75 0.5  0.15 0.1  .  As expected, the total output converges to   for both initial conditions. In  addition, since the relative values of the initial conditions are the same for  the two cases, the outputs converge to the same values in both cases.  0.5  18-40   Solved Problems  1.5  1  0.5  0 0  N2 t    2 t    n1  2 t    n2  5  10  Figure P18.6  Response of Layer 2 for Linear   f2 n     P18.7 Show that the continuous-time Hebb rule with decay, given by Eq.    18.24 , is equivalent to the discrete-time version given by Eq.   15.18 .  The continuous-time Hebb rule with decay is  2 t    dwi j ----------------- dt  =      –  2 wi j  t     +  2 t   nj ni  1 t       .  If we approximate the derivative by  2 t    dwi j ----------------- dt    2 t    wi j ----------------------------------------------  2 t t+  –   wi j t  ,  18  the Hebb rule becomes  2 wi j  t t+      =  2 wi j  t    t  +    –  2 wi j  t     +  2 t   nj ni  1 t       .  This can be rearranged to obtain  2 wi j  t t+     =  1 t –  2 wi j  t    t ni  +    2 t   nj  1 t       .  In vector form this would be  W2 t t+      =  1 t –  W2 t    t n2 t    n1 t     +      T    .  If we compare this with Eq.  15.18 ,  W q   =  1 –  W q  1–   a q pT q  +  ,  we can see that they have the identical form.  18-41   18 Grossberg Network  Epilogue  The Grossberg network presented in this chapter was inspired by the visu- al system of higher vertebrates. To motivate the network, we presented a  brief description of the primary visual pathway. We also discussed some vi- sual illusions, which help us to understand the mechanisms underlying the  visual system.  The Grossberg network is a two-layer, continuous-time competitive net- work, which is very similar in structure and operation to the Kohonen com- petitive network presented in Chapter 14. The first layer of the Grossberg  network normalizes the input pattern. It demonstrates how the visual sys- tem can use on-center off-surround connection patterns and a shunting  model to implement an automatic gain control, which normalizes total ac- tivity.   The second layer of the Grossberg network performs a competition, which  contrast enhances the output pattern and stores it in short-term memory.  It uses nonlinear feedback and the on-center off-surround connection pat- tern to produce the competition and the storage. The choice of the transfer  function and the feedback connection pattern determines the degree of  competition  e.g., winner-take-all, mild contrast enhancement, or no  change in the pattern .  The adaptive weights in the Grossberg network use an instar learning rule,  which stores prototype patterns in long-term memory. When a winner- take-all competition is performed in the second layer, this learning rule is  equivalent to the Kohonen learning rule used in Chapter 14.   As with the Kohonen network, one key problem of the Grossberg network  is the stability of learning; as more inputs are applied to the network, the  weight matrix may never converge. This problem was discussed extensive- ly in Chapter 14. In Chapter 16 we will present a class of networks that is  designed to overcome this difficulty: the Adaptive Resonance Theory  ART   networks. The ART networks are direct descendents of the Grossberg net- work presented in this chapter.  Another problem with the Grossberg network, which we have not discussed  in this chapter, is the stability of the differential equations that implement  the network. In Layer 2, for example, we have a set of differential equations  with nonlinear feedback. Can we make some general statement about the  stability of such systems? Chapter 17 will present a comprehensive discus- sion of this problem.  18-42   Further Reading  Further Reading  [GrMi89]  [Gros76]  S. Grossberg, E. Mingolla and D. Todorovic, “A neural net- work architecture for preattentive vision,” IEEE Transac- tions on Biomedical Engineering, vol. 36, no. 1, pp. 65–84,  1989.  The objective of this paper is to develop a neural network  for general purpose preattentive vision. The network con- sists of two main subsystems: a boundary contour system  and a feature contour system.  S. Grossberg, “Adaptive pattern classification and univer- sal recoding: I. Parallel development and coding of neural  feature detectors,” Biological Cybernetics, vol. 23, pp. 121– 134, 1976.  Grossberg describes a continuous-time competitive net- work, inspired by the developmental physiology of the visu- al cortex. The structure of this network forms the  foundation for other important networks.  [Gros82]  S. Grossberg, Studies of Mind and Brain, Boston: D. Reidel  Publishing Co., 1982.  This book is a collection of Stephen Grossberg papers from  the period 1968 through 1980. It covers many of the funda- mental concepts that are used in later Grossberg networks,  such as the adaptive resonance theory networks.  18  [Hube88]  D.H. Hubel, Eye, Brain, and Vision, New York: Scientific  American Library, 1988.  [vanT75]  David Hubel has been at the center of research in this area  for 30 years, and his book provides an excellent introduc- tion to the human visual system. He explains the current  view of the visual system in a way that is easily accessible  to anyone with some scientific training.  H. F. J. M. van Tuijl, “A new visual illusion: Neonlike color  spreading and complementary color induction between  subjective contours,” Acta Psychologica, vol. 39, pp. 441– 445, 1975.  This paper describes the original discovery of the illusion in  which crosses of certain colors, when placed inside Ehren- stein figures, appear to spread into solid shapes.  18-43   18 Grossberg Network  [vond73]  C. von der Malsburg, “Self-organization of orientation sen- sitive cells in the striate cortex,” Kybernetic, vol. 14, pp. 85– 100, 1973.  Malsberg’s is one of the first papers to present a self-orga- nizing feature map neural network. The network is a model  for the visual cortex of higher vertebrates. This paper influ- enced the work of Kohonen and Grossberg on feature maps.  18-44   Exercises  Exercises  E18.1 Consider the leaky integrator shown in Figure E18.1.  i. Find the response   n t    if     1=  ,   n 0   1=   and   p t   =  0.5  .  ii. Find the response   n t    if     1=  ,   n 0   1=   and   p t   2=  iii. Find the response   n t    if     4=  ,   n 0   1=   and   p t   2=  .  .  » 2 + 2 ans =       4  iv. Check your answers to the previous parts by writing a MATLAB   M-file to simulate the leaky integrator. Use the ode45 routine. Plot  the response for each case.  p  +  -  Leaky Integrator  n  . n   cid:0 1 ε  cid:0  ε dn dt = - n + p  Figure E18.1  Leaky Integrator  E18.2 Consider the shunting network shown in Figure E18.2.  18  i. Find and sketch the response of the shunting network if     2=  ,   b+  3=  ,   b-  1=  ,   p+  0=  ,   p-  5=   and   n 0     1=  .  ii. Find and sketch the response of the shunting network if     2=  ,   b+  3=  ,   b-  1=  ,   p+  0=  ,   p-  50=   and   n 0     1=  .  iii. Find and sketch the response of the shunting network if     2=  ,   b+  3=  ,   b-  1=  ,   p+  50=  ,   p-  0=   and   n 0     1=  .  iv. Find and sketch the response of the shunting network if     5=  ,   b+  2=  ,   b-  6=  ,   p+  5=  ,   p-  0=   and   n 0     0=  .  v. Find and sketch the response of the shunting network if     5=  ,   b+  2=  ,   b-  6=  ,   p+  0=  ,   p-  5=   and   n 0     0=  .  vi. Find and sketch the response of the shunting network if     =  0.25  ,   b+  4=  ,   b-  2=  ,   p+  2=  ,   p-  2=   and   n 0     0=  .  vii. Find and sketch the response of the shunting network if     =  0.25  ,   b+  4=  ,   b-  2=  ,   p+  2=  ,   p-  4=   and   n 0     0=  .  18-45   18 Grossberg Network  » 2 + 2 ans =       4  viii. Check your answers to the previous parts by writing a MATLAB  M-file to simulate the shunting network. Use the ode45 routine.  Plot the response for each case. Verify that your responses agree  with the known characteristics of the shunting model.  ix. Explain the differences in operation of the leaky integrator and the   shunting network.  Input  Basic Shunting Model  p+  p-  +  -  +  -   cid:0  cid:0 1 ε . n  cid:0  cid:0   b+  +  -  n  +  +  b-  ε dn dt = -n +  b+ - n  p+ -  n + b-  p-  Figure E18.2  Shunting Network  E18.3 Suppose that Layer 1 of the Grossberg network has two neurons, with    and input vector   p  =  . Assume that the initial   T  2 1  =  0.5  b+ 1 conditions are set to zero.  0.5  ,   =    i. Find the steady state response of Layer 1, using Eq.  18.13 .  ii. Find the solution to the differential equation for Layer 1. Verify that   the steady state response agrees with your answer to part  i .  iii. Check your answer by writing a MATLAB M-file to simulate Layer   1 of the Grossberg network. Use the ode45 routine. Plot the re- sponse.  » 2 + 2 ans =       4  E18.4 Repeat Exercise E18.3 for input vector   p  =  T  .  20 10  E18.5 Consider the first layer of the Grossberg network. The parameters are set  . Find   . The input to the network is   to be  the first layer outputs and sketch them versus time.  0=  2=  2=  b+ 1  4 1  b- 1  ,   ,   p  =    T  18-46   Exercises  E18.6 Find the differential equation that describes the variation in the total out-  put of Layer 1,  N1 t     1 t    ni  .  S1 =  i  1=   Use the technique presented in Problem P18.5.   E18.7 Assume that Layer 2 of the Grossberg network has two neurons, with   0=  . The inputs have been applied for   b- 2 f2 n  some length of time, then removed.   and   2n=  1=  1=  b+ 2  ,   ,     i. What will be the steady state total output,   N2 t     ?  lim  t  ii. Repeat part  i  if   b+ 2  =  0.25  .  » 2 + 2 ans =       4  iii. Check your answers to the previous parts by writing a MATLAB   M-file to simulate Layer 2 of the Grossberg network. Use the ode45  routine. Plot the responses for the following initial conditions:  n2 0     =   and   n2 0     =  2 1  .  0.2 0.1  E18.8 Suppose that the transfer function for Layer 2 of the Grossberg network is   f2 n   =  c    n   , and     2  1=  ,   b+ 2  1=  .   18  i. Using the results of Problem P18.5, show that, after the inputs have  been removed, all of the relative outputs of Layer 2 will decay to ze- ro, except the one with the largest initial condition  winner-take-all  competition .   ii. For what values of   c   will the total output   N2 t      have a nonzero sta-  ble point  steady state value ?  iii. If the condition of part  ii  is satisfied, what will be the steady state   value of   N2 t     ? Will this depend on the initial condition   N2 0     ?  iv. Check your answers to the previous parts by writing a MATLAB   and   M-file and simulating the total response of Layer 2 for  N2 0     4=  3=  c  .  E18.9 Simulate the response of the adaptive weights for the Grossberg network.   Assume that the coefficient  terns are alternately presented to the network for periods of 0.2 seconds at  a time. Also, assume that Layer 1 and Layer 2 converge very quickly, in  comparison with the convergence of the weights, so that the neuron out-  . Assume that two different input pat-   is   1    » 2 + 2 ans =       4  » 2 + 2 ans =       4  18-47   18 Grossberg Network  puts are effectively constant over the 0.2 seconds. The Layer 2 and Layer 1  outputs for the two different input patterns will be:  for pattern 1:   n1  =  ,   n2  =  for pattern 2:   n1  =  ,   n2  =  0.8 0.2  0.5 0.5  ,  .  1 0  0 1  E18.10 Repeat Exercise E18.9, but use the Hebb rule with decay, Eq.  18.24 , in-  stead of the instar learning of Eq.  18.25 . Explain the differences between  the two responses.  » 2 + 2 ans =       4  18-48   Objectives  19 Adaptive Resonance Theory  Objectives Theory and Examples  Overview of Adaptive Resonance Layer 1  Steady State Analysis  Layer 2 Orienting Subsystem Learning Law: L1-L2  Subset Superset Dilemma Learning Law  Learning Law: L2-L1 ART1 Algorithm Summary  Initialization Algorithm  Other ART Architectures  Summary of Results Solved Problems Epilogue Further Reading Exercises  19-1 19-2 19-2 19-4 19-6 19-10 19-13 19-16 19-17 19-18 19-20 19-21 19-21 19-21 19-23 19-24 19-29 19-44 19-45 19-47  Objectives  19  In Chapter 16 and Chapter 18 we learned that one key problem of compet- itive networks is the stability of learning. There is no guarantee that, as  more inputs are applied to the network, the weight matrix will eventually  converge. In this chapter we will present a modified type of competitive  learning, called adaptive resonance theory  ART , which is designed to  overcome the problem of learning stability.  19-1   19 Adaptive Resonance Theory  Theory and Examples  Stability Plasticity  A key problem of the Grossberg network presented in Chapter 18 and the  competitive networks of Chapter 16, is that they do not always form stable  clusters  or categories . Grossberg did show [Gros76] that if the number of  input patterns is not too large, or if the input patterns do not form too many  clusters relative to the number of neurons in Layer 2, then the learning  eventually stabilizes. However, he also showed that the standard competi- tive networks do not have stable learning in response to arbitrary input  patterns. The learning instability occurs because of the network’s adapt- ability  or plasticity , which causes prior learning to be eroded by more re- cent learning.  Grossberg refers to this problem as the “stability plasticity dilemma.” How  can a system be receptive to significant new patterns and yet remain stable  in response to irrelevant patterns? We know that biological systems are  very good at this. For example, you can easily recognize your mother’s face,  even if you have not seen her for a long time and have met many new people  in the mean time.  Grossberg and Gail Carpenter developed a theory, called adaptive reso- nance theory  ART , to address the stability plasticity dilemma  see  [CaGr87a], [CaGr87b], [CaGr90], [CaGrRe91] and [CaGrMa92] . The ART  networks are based on the Grossberg network of Chapter 18. The key inno- vation of ART is the use of “expectations.” As each input pattern is present- ed to the network, it is compared with the prototype vector that it most  closely matches  the expectation . If the match between the prototype and  the input vector is not adequate, a new prototype is selected. In this way,  previously learned memories  prototypes  are not eroded by new learning.  It is beyond the scope of this text to discuss all of the variations of adaptive  resonance theory. Instead, we will present one of the ART networks in de- tail — ART1  see [CaGr87a] . This particular network is designed for bina- ry input vectors only. However, from this one architecture, the key features  of adaptive resonance theory can be understood.  Overview of Adaptive Resonance  The basic ART architecture is shown in Figure 19.1. It is a modification of  the Grossberg network of Chapter 18  compare with Figure 18.13 , which  is designed to stabilize the learning process. The innovations of the ART ar- chitecture consist of three parts: Layer 2  L2  to Layer 1  L1  expectations,  the orienting subsystem and gain control. In this section we will describe  the general operation of the ART system; then, in later sections, we will dis- cuss each subsystem in detail.  19-2   Overview of Adaptive Resonance  Layer 1  Layer 2  Input  Gain Control  Expectation  Reset  Orienting Subsystem  Figure 19.1  Basic ART Architecture  Recall from Chapter 18 that the L1-L2 connections of the Grossberg net- work are instars, which perform a clustering  or categorization  operation.  When an input pattern is presented to the network, it is multiplied  after  normalization  by the L1-L2 weight matrix. Then, a competition is per- formed at Layer 2 to determine which row of the weight matrix is closest to  the input vector. That row is then moved toward the input vector. After  learning is complete, each row of the L1-L2 weight matrix is a prototype  pattern, which represents a cluster  or category  of input vectors.  In the ART networks, learning also occurs in a set of feedback connections  from Layer 2 to Layer 1. These connections are outstars  see Chapter 15 ,  which perform pattern recall. When a node in Layer 2 is activated, this re- produces a prototype pattern  the expectation  at Layer 1. Layer 1 then  performs a comparison between the expectation and the input pattern.  When the expectation and the input pattern are not closely matched, the  orienting subsystem causes a reset in Layer 2. This reset disables the cur- rent winning neuron, and the current expectation is removed. A new com- petition is then performed in Layer 2, while the previous winning neuron  is disabled. The new winning neuron in Layer 2 projects a new expectation  to Layer 1, through the L2-L1 connections. This process continues until the  L2-L1 expectation provides a close enough match to the input pattern.  In the following sections we will investigate each of the subsystems of the  ART system, as they apply to one particular ART network — ART1   [CaGr87a] . We will first describe the differential equations that describe  the subsystem operations. Then we will derive the steady state responses  of each subsystem. Finally, we will summarize the overall operation of the  ART1 system.  19-3  19   19 Adaptive Resonance Theory  Layer 1  The main purpose of Layer 1 is to compare the input pattern with the ex- pectation pattern from Layer 2.  Both patterns are binary in ART1.  If the  patterns are not closely matched, the orienting subsystem will cause a re- set in Layer 2. If the patterns are close enough, Layer 1 combines the ex- pectation and the input to form a new prototype pattern.  Layer 1 of the ART1 network, which is displayed in Figure 19.2, is very  similar to Layer 1 of the Grossberg network  see Figure 18.14 . The differ- ences occur at the excitatory and inhibitory inputs to the shunting model.  For the ART1 network, no normalization is performed at Layer 1; therefore  we don’t have the on-center off-surround connections from the input vector.  The excitatory input to Layer 1 of ART1 consists of a combination of the in- put pattern and the L1-L2 expectation. The inhibitory input consists of the  gain control signal from Layer 2. In the following we will explain how these  inputs work together.  Input  Layer 1  p S1 x 1  S1  +  +  Expectation  +b1  +  -  +  -  n1.   cid:0 1 ε  cid:0   a2   cid:0  cid:0  W2:1  cid:0  cid:0  S1 x S2  cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0  a1  cid:0  cid:0   cid:0  cid:0   f 1  S1  + - n1  +  +  -b1  Gain Control   cid:0  cid:0  -W1 S1 x S2  a2  ε dn1 dt = - n1 +  +b1 - n1   { p + W2:1 a2} -  n1 + -b1  [-W1] a2  Figure 19.2  Layer 1 of the ART1 Network  The equation of operation of Layer 1 is  dn1 t    -------------- dt  =  n1 t     –  +  b+ 1 n1 t     –     p W2:1a2 t  +     19.1   n1 t       b- 1+   W-   1  a2 t   –  and the output of Layer 1 is computed  19-4   where  Layer 1  a1  =  hardlim+ n1    ,  hardlim+ n   =      1, n 0, n  0 0  .   19.2    19.3   Eq.  19.1  is a shunting model with excitatory input  the sum of the input vector and the L2-L1 expectation. For example, as- sume that the jth neuron in Layer 2 has won the competition, so that its  output is 1, and the other neurons have zero output. For this case we have  , which is   p W2:1a2 t   +  W2:1a2  =  2:1 w2 w1  2:1  wj  2:1  w  2:1 S2  =  2:1  wj  ,   19.4   0 0  1   2:1  wj   is the jth column of the matrix   where  trained using an outstar rule, as we will show in a later section.  Now we  can see that   matrix is   .  The   W2:1  W2:1  p W2:1a2  +  =  p wj  +  2:1  .   19.5   Therefore the excitatory input to Layer 1 is the sum of the input pattern  and the L2-L1 expectation. Each column of the L2-L1 matrix represents a  different expectation  prototype pattern . Layer 1 combines the input pat- tern with the expectation using an AND operation, as we will see later.  The inhibitory input to Layer 1 is the gain control term   1  W-    a2 t   , where  1  W-  =  1 1  1 1 1  1 .   1 1  1  19   19.6   Therefore, the inhibitory input to each neuron in Layer 1 is the sum of all  of the outputs of Layer 2. Since we will be using a winner-take-all compe- tition in Layer 2, whenever Layer 2 is active there will be one, and only one,  nonzero element of   after the competition. Therefore the gain control in- put to Layer 1 will be one when Layer 2 is active, and zero when Layer 2 is  inactive  all neurons having zero output . The purpose of this gain control  will become apparent as we analyze the steady state behavior of Layer 1.  a2  19-5   19 Adaptive Resonance Theory  Steady State Analysis The response of neuron   i   in Layer 1 is described by  1 dni -------- dt    =  1– ni  b+ 1    +  1– ni  2  2:1aj wi j  –    1 ni  b- 1+    2 aj  ,   19.7      pi     S2 +  j  1=        S2   j  1=    1«   so that the short-term memory traces  the neuron outputs    where  change much faster than the long-term memory traces  the weight matri- ces .  We want to investigate the steady state response of this system for two dif- ferent cases. In the first case Layer 2 is inactive, therefore   for all  .  j In the second case Layer 2 is active, and therefore one neuron has an out- put of 1, and all other neurons output 0.   0=  2 aj  Consider first the case where Layer 2 is inactive. Since each   19.7  simplifies to  2 aj  0=  , Eq.   1 dni -------- dt    =  1– ni  b+ 1    +  1– ni   pi   .  In the steady state    1 t    dt    dni  0=    we have  0  =  1– ni  b+ 1    +  1– ni  pi  =  –  pi+ 1  If we solve for the steady state neuron output   .  +  1 ni 1 ni  b+ 1pi  we find  1 ni  =  b+ 1pi ------------- 1 pi+  .   19.8    19.9    19.10   Therefore, if  we chose the transfer function for Layer 1 to be the  we have   , and if    then   1=  0=  0=  pi  pi   then   1 ni  1 ni  =  hardlim+  0  b+ 1 2 . Since   function, then   a1  p=  .   19.11   Therefore, when Layer 2 is inactive, the output of Layer 1 is the same as  the input pattern.  Now let’s consider the second case, where Layer 2 is active. Assume that  neuron   for  k   is the winning neuron in Layer 2. Then   . For this case Eq.  19.7  simplifies to   and   1=  0=  2 aj  2 ak  j  j  19-6   Layer 1  1 dni -------- dt    =  1– ni  +    b+ 1 ni 1–  2:1  pi wi j +    –    1 ni  b- 1+    .   19.12    19.13    19.14    19.15    19.16    19.17   In the steady state    1 t    dt    dni  0=    we have  0  = =  1– ni  1 –  + +  b+ 1 ni 1–  2:1 + pi wi j  2:1  pi wi j + 1  ni + + 1  b- 1+  b+ 1 pi wi j 2:1  1  – ni +     b- 1–   .  If we solve for the steady state neuron output    we find  1 ni  1 ni  =  2:1  +  b+ 1 pi wi j b- 1–  --------------------------------------------- 2:1 + pi wi j  +  2  .  Recall that Layer 1 should combine the input vector with the expectation  from Layer 2  represented by   . Since we are dealing with binary pat- terns  both the input and the expectation , we will use a logical AND oper- 1  to be less than  ation to combine the two vectors. In other words, we want  ni 1  is equal to zero, and we want  zero when either   to be greater  ni than zero when both    are equal to one.   2:1 wi j  and   wj  pi  2:1   or  pi  2:1 wi j  If we apply these conditions to Eq.  19.14 , we obtain the following equa- tions:  which we can combine to produce  b+ 1 2   b- 1–  0  ,  b+ 1  b- 1–  0  ,  b+ 1 2     b- 1  b+ 1  .   b- 1  For example, we can use   b+ 1  1=   and   =  1.5   to satisfy these conditions.  Therefore, if Eq.  19.17  is satisfied, and neuron   of Layer 2 is active, then  the output of Layer 1 will be  j  a1  =  p wj  2:1  ,   19.18   where  represents the logical AND operation.  Notice that we needed the gain control in order to implement the AND op- eration. Consider the numerator of Eq.  19.14 :  b+ 1 pi wi j  +  2:1    b- 1–  .   19.19   19-7  19   2 2+  To demonstrate the operation of Layer 1, assume the following network pa- rameters:  19 Adaptive Resonance Theory  b- 1  The term   is multiplied by the gain control term, which in this case is 1.  If this term did not exist, then Eq.  19.19  would be greater than zero  and  therefore   was  greater than zero. This would represent an OR operation, rather than an  AND operation. As we will see when we discuss the orienting subsystem, it  is critical that Layer 1 perform an AND operation.   would be greater than zero  whenever either   2:1 wi j   or   1 ni  pi  When Layer 2 is inactive, the gain control term is zero. This is necessary  because in that case we want Layer 1 to respond to the input pattern alone,  since no expectation will be activated by Layer 2.  To summarize the steady state operation of Layer 1:  If Layer 2 is not active  i.e., each   2 aj  0=   ,  If Layer 2 is active  i.e., one   2 aj  1=   ,  a1  p=  .  a1  =  p wj  2:1  .   19.20    19.21     =  0.1  ,   b+ 1  1=   and   b- 1  =  1.5  .   19.22   Assume also that we have two neurons in Layer 2, two elements in the in- put vector and the following weight matrix and input:  W2:1  =   and   p  =  1 1 0 1  .  0 1   19.23   If we take the case where Layer 2 is active, and neuron 2 of Layer 2 wins  the competition, the equations of operation of Layer 1 are  0.1    1 dn1 -------- dt  0.1    1 dn2 -------- dt  =  =  =  =  1– n1  +  1– 1 n1  2:1  p1 w1 2   +    –    1 n1  1.5+     19.24   1– n1  +  1– 1 n1   0 1+    –    1 n1  1.5+    =  –  1 3n1  –  0.5  1– n2  +  1– 1 n2  2:1  p2 w2 2   +    –    1 n2  1.5+     19.25   1– n2  +  1– 1 n2   1 1+    –    1 n2  1.5+    =  –  1 4n2  +  0.5 .  These can be simplified to obtain  19-8   In this simple case we can find closed-form solutions for these equations. If  we assume that both neurons start with zero initial conditions, the solu- tions are  These are displayed in Figure 19.3.   19.26    19.27    19.28    19.29   0.05  0.15  0.2  0.1 t  1 t  n1  Figure 19.3  Response of Layer 1 1 t   converges to a negative value, and   converges to a pos- n2 1 t   converges to 1  recall  a2 hardlim+  . This agrees with our   Note that  itive value. Therefore,  that the transfer function for Layer 1 is  steady state analysis  see Eq.  19.21  , since   converges to 0, and   1 t  a1  19  2:1 p w2    =    0 1  1 1  =  0 1  =  a1  .   19.30   To experiment with Layer 1 of the ART1 network, use the Neural Network  Design Demonstration ART1 Layer 1  nnd16al1 .  Layer 1  1 dn1 -------- dt  1 dn2 -------- dt  =  –  30n  1 5– 1  ,  =  –  1 40n2  5+  .  1 t  n1  =  1 ---– 6  –– 1  e 30t    ,  1 t  n2  =  1 –– --- 1 8  e 40t    .  1 t  n2  1 t  n1  0.2  0.1  0  -0.1  -0.2 0  19-9   a0  n  . n  19 Adaptive Resonance Theory  Layer 2  Layer 2 of the ART1 network is almost identical to Layer 2 of the Grossberg  network of Chapter 18. Its main purpose is to contrast enhance its output  pattern. For our implementation of the ART1 network, the contrast en- hancement will be a winner-take-all competition, so only the neuron that  receives the largest input will have a nonzero output.  There is one major difference between the second layers of the Grossberg  and the ART1 networks. Layer 2 of the ART1 network uses an integrator  that can be reset. In this type of integrator, whose symbol is shown in the  left margin, any positive outputs are reset to zero whenever the   signal  becomes positive. The outputs that are reset remain inhibited for a long pe- riod of time, so that they cannot be driven above zero.  By a “long” period  of time we mean until an adequate match has occurred and the weights  have been updated.   a0  In the original ART1 paper, Carpenter and Grossberg suggested that the  reset mechanism could be implemented using a gated dipole field  [CaGr87]. They later suggested a more sophisticated biological model, us- ing chemical neurotransmitters, in their ART3 architecture [CaGr90]. For  our purposes we will not be concerned with the specific biological imple- mentation.  Figure 19.4 displays the complete Layer 2 of the ART1 network. Again, it  is almost identical to Layer 2 of the Grossberg network  see Figure 18.16 ,  with the primary exception of the resetable integrator. The reset signal,  ,  is the output of the orienting subsystem, which we will discuss in the next  section. It generates a reset whenever there is a mismatch at Layer 1 be- tween the input signal and the L2-L1 expectation.  a0  One other small difference between Layer 2 of the ART1 network and Lay- er 2 of the Grossberg network is that two transfer functions are used in  ART1. The transfer function  feedback connections, while the output of Layer 2 is computed as  a2 want the output of Layer 2 to be a binary signal.  . The reason for the second transfer function is that we    is used for the on-center off-surround   hardlim+ n2  f2 n2  =      19-10   Layer 2  Layer 2  +  On-Center   cid:0  cid:0  +  cid:0  cid:0   W1:2 S2 x S1  +  -  +  -  n2.   cid:0 1 ε  cid:0    cid:0  cid:0  +W2  cid:0  cid:0   cid:0  cid:0  S2 x S2  cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   f 2  +b2  -b2  +  -  n2  +  +   cid:0  cid:0  a2  cid:0  cid:0   cid:0  cid:0   S2   cid:0  cid:0 S2 x S2 -W2 ε dn2 dt = - n2 +  +b2 - n2  {[+W2] f 2 n2  + W1:2 a1}  Off-Surround  -  n2 + -b2  [-W2] f 2 n2   Figure 19.4  Layer 2 of the ART1 Network  The equation of operation of Layer 2 is   a1  a0 Reset  dn2 t    -------------- dt  =  n2 t     –  +  b+ 2 n2 t     –   W+ 2       f2 n2 t          W1:2a1 +     19.31   n2 t       b- 2+   W-   2  f2 n2 t          .  –  2  2  W+      W+   provides on-center feedback connections  identical to Layers 1   This is a shunting model with excitatory input  where   con- and 2 of the Grossberg network of Chapter 18, Eq.  18.6  , and  sists of adaptive weights, analogous to the weights in the Kohonen net- work. They are trained according to an instar rule, as we will see in a later  section. The rows of  , after training, will represent the prototype pat- terns.   W1:2  W1:2  ,       W1:2a1 +  f2 n2 t        The inhibitory input to the shunting model is   pro- vides off-surround feedback connections  identical to Layers 1 and 2 of the  Grossberg network — Eq.  18.7  .  , where        2 W-  f2 n2 t        W-  2  19  19-11   19 Adaptive Resonance Theory  To illustrate the performance of Layer 2, consider a two-neuron layer with  2 2+    =  0.1  ,   b+ 2  =  ,   b- 2  =  ,   W1:2  =  1 1  1 1  =  0.5 0.5 1 0  ,   19.32   w1:2 w1:2  T T  1 2  and  0.1    2 t    dn1 -------------- dt  0.1    2 t    dn2 -------------- dt  f2 n     =      10 n 2,  0 ,  0 n 0 n  .   19.33   The equations of operation of the layer will be  =  –  2 t    n1  +  – 1  2 t    n1   f2 n1      2 t       +  1  w1:2  Ta1     19.34   =  –  2 t    n2  +  – 1  2 t    n2   f2 n2      2 t       +  2  w1:2  Ta1     19.35   –    2 t    n1  1+  f2 n2     2 t       –    2 t    n2  1+  f2 n1     2 t       .  This is identical in form to the Grossberg Layer 2 example in Chapter 18  2 t     see Eq.  18.20  and Eq.  18.21  , except that  n1 and    to range between -1 and +1.   . This will allow   1=  b- 2     2 t    n2  W1:2  The inputs to Layer 2 are the inner products of the prototype patterns   rows of the weight matrix    with the output of Layer 1.  The rows of  this weight matrix are normalized, as will be explained in a later section.   The largest inner product will correspond to the prototype pattern that is  closest to the output of Layer 1. Layer 2 then performs a competition be- tween the neurons. The transfer function   is chosen to be a faster-than- linear transfer function  see Chapter 18, page 18-20, for a discussion of the   . This choice will force the neuron with largest input to have  effect of  a positive  , and the other neuron to have a negative    with appropriate  choice of network parameters . After the competition, one neuron output  will be 1, and the other neuron output will be zero, since we are using the  hardlim+   transfer function to compute the layer output.  f2 n    n  f2 n     n  Figure 19.5 illustrates the response of Layer 2 when the input vector is  a1 a1     has a larger inner product with   . The second row of   W1:2  T  =  1 0  19-12   Orienting Subsystem  than the second row, therefore neuron 2 wins the competition. At steady  state,  state Layer 2 output will then be   has a negative value. The steady    has a positive value, and   2 t    n1  2 t    n2  a2  =  .  0 1   19.36   w1:2  Ta1  2  w1:2  Ta1  1  2 t  n2  2 t  n1  0.5  1  0  -0.5  -1  0  0.05  0.15  0.2  0.1 t  Figure 19.5  Response of Layer 2  We can summarize the steady state operation of Layer 2 as follows:  2 ai  =  1 , 0 ,        if w1:2   i  =  Ta1 max w1:2 otherwise  j    Ta1      .   19.37   To experiment with Layer 2 of the ART1 network, use the Neural Network  Design Demonstration ART1 Layer 2  nnd16al2 .  19  Orienting Subsystem  One of the key elements of the ART architecture is the Orienting Sub- system. Its purpose is to determine if there is a sufficient match between  the L2-L1 expectation and the input pattern. When there is not enough of  a match, the Orienting Subsystem should send a reset signal to Layer 2.  The reset signal will cause a long-lasting inhibition of the previous winning  neuron, and thus allow another neuron to win the competition.  Figure 19.6 displays the Orienting Subsystem.   19-13   19 Adaptive Resonance Theory  Orienting Subsystem  +W0 1 x S1   cid:0  cid:0   cid:0  cid:0  +  cid:0  cid:0   cid:0  cid:0   -W0 1 x S1  -  p  a1  +  -  n0.   cid:0 1 ε  cid:0   +b0  -b0  +  -  n0  +  +   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0  a0  cid:0  cid:0  Reset cid:0  cid:0   f  0  1  ε dn0 dt = -n0 +  +b0 - n0  [+W0] p -  n0 + -b0  [-W0] a 1  Figure 19.6  Orienting Subsystem of the ART1 Network  The equation of operation of the Orienting Subsystem is  dn0 t    -------------- dt  =  n0 t     –  +  b+ 0 n0 t     –   W+   0p      –  n0 t       b- 0+   W-   0a1    .   19.38   This is a shunting model, with excitatory input   W+  0p  , where  0  W+  =       .   19.39   Therefore, the excitatory input can be written  W+  0p  =      p  =    pj  =   p 2  ,   19.40   where the last equality holds because   p  The inhibitory input to the Orienting Subsystem is   , where   is a binary vector. 0a1  W-  0  W-  =       .   19.41   Therefore, the inhibitory input can be written  W-  0a1  =      a1  =    1 t  aj   a1 2  .  =   19.42   S1   j  1=  S1   j  1=  19-14   Orienting Subsystem  Whenever the excitatory input is larger than the inhibitory input, the Ori- enting Subsystem will be driven on. Consider the following steady state op- eration:  0  = =    b+ 0  n0– +  n0– + 1  p 2  a1 2  – + n0  , we find    p 2   n0  If we solve for     a1 2 n0   b+ 0  p 2    b- 0+   –  – +    b- 0  a1 2   .   19.43   Let   b+ 0  =  b- 0  =  1  , then   n0  , or in other words:  n0  =      –  b- 0  a1 2 b+ 0  p 2  ------------------------------------------------------------  +   1  p 2  a1 2 +  p 2  a1 2   if   0  0  –    .  n0  0   if   a1 2 ------------ p 2   ---  =  .  Vigilance  =  This is the condition that will cause a reset of Layer 2, since  a0 hardlim+ n0  is called the vigilance parameter, and must  . If the vigilance is close to 1, a reset will occur un- fall in the range  less   is close to    p to prevent a reset. The vigilance parameter determines the coarseness of  the categorization  or clustering  created by the prototype vectors.  . The term    p  . If the vigilance is close to 0,    need not be close to    0  1  a1  a1    2:1  a1  p 2  =  p wj   whenever Layer 2 is active.    will always be greater than or equal to   Recall from Eq.  19.21  that  Therefore,  equal when the expectation  Therefore, the orienting subsystem will cause a reset when there is enough  of a mismatch between  . The amount of mismatch required for  a reset is determined by the vigilance parameter   a1 2  has a 1 wherever the input   . They will be  p   has a 1.    and   wj  wj  2:1  2:1  p  .    2 2+  To demonstrate the operation of the Orienting Subsystem, suppose that    0.75  4=  3=  0.1   ,         ,   ,       =  =  19   19.44    19.45    19.46    19.47   p  =   and   a1  =  1 1  .  1 0  The equation of operation becomes  0.1  dn0 t    -------------- dt  =  n0 t     –  +  – 1  n0 t      3 p1     p2+      n0 t       –  1+  1  4 a1     1+ a2      19-15   19 Adaptive Resonance Theory  or  dn0 t    --------------  dt  =  110n0 t     –  20+  .   19.48   The response is plotted in Figure 19.7. In this case a reset signal will be  n0 t  sent to Layer 2, since   is positive. In this example, because the vigi- lance parameter is set to   has only two elements, we will   =  are not identical.  If the vigilance param-  and  have a reset whenever  p a1 , we would not have had a reset for the  eter were set to    0.25 = a1 2 p 2 of Eq.  19.46 , since    0.75 a1  , and    and   1 2  .     p  p  =  n0 t   0.2  0.1  0  -0.1  -0.2 0  0.05  0.15  0.2  0.1 t  Figure 19.7  Response of the Orienting Subsystem  The steady state operation of the Orienting Subsystem can be summarized  as follows:  a0  =  1 , 0 ,    if a1 2 p 2  otherwise      .         19.49   To experiment with the Orienting Subsystem, use the Neural Network De- sign Demonstration Orienting Subsystem  nnd16os .  Learning Law: L1-L2  The ART1 network has two separate learning laws: one for the L1-L2 con- nections, and another for the L2-L1 connections. The L1-L2 connections  use a type of instar learning to learn to recognize a set of prototype pat- terns. The L2-L1 connections use outstar learning in order to reproduce  or  recall  a set of prototype patterns. In this section we will describe the L1-  19-16   Resonance  Learning Law: L1-L2  L2 instar learning law, and in the following section we will present the L2- L1 outstar learning law.  We should note that the L1-L2 connections and the L2-L1 connections are  updated at the same time. Whenever the input pattern and the expectation  have an adequate match, as determined by the Orienting Subsystem, both  W1:2  are adapted. This process of matching, and subsequent ad- aptation, is referred to as resonance, hence the name adaptive resonance  theory.   and   W2:1  Subset Superset Dilemma The learning in the L1-L2 connections of the ART1 network is very close to  the learning in the Grossberg network of Chapter 18, with one major dif- ference. In the Grossberg network, the input patterns are normalized in  Layer 1, and therefore all of the prototype patterns will have the same  length. In the ART1 network no normalization takes place in Layer 1.  Therefore a problem can occur when one prototype pattern is a subset of  another. For example, suppose that the L1-L2 connection matrix is  W1:2  =  1 1 0 1 1 1  ,   19.50   so that the prototype patterns are  w1:2 1  =   and   w1:2 2  =   19.51   1 1 0  .  1 1 1  w1:2 1   is a subset of   w1:2 2  , since   w1:2  2   has a 1 wherever   w1:2 1   has   We say that  a 1.  If the output of Layer 1 is  a1  =  ,  1 1 0  then the input to Layer 2 will be  W1:2a1  =  1 1 0 1 1 1  =  .  2 2  1 1 0  19-17  19   19.52    19.53    19 Adaptive Resonance Theory  Both prototype vectors have the same inner product with  the first prototype is identical to  called the subset superset dilemma.  , even though   and the second prototype is not. This is   a1  a1  One solution to the subset superset dilemma is to normalize the prototype  patterns. That is, when a prototype pattern has a large number of nonzero  entries, the magnitude of each entry should be reduced. For example, using  our preceding problem, we could modify the L1-L2 matrix as follows:  The input to Layer 2 will then be  W1:2  =  1 --- 2 1 --- 3  1 --- 0 2 1 --- 3  1 --- 3  .  W1:2a1  =  1 --- 2 1 --- 3  1 --- 0 2 1 --- 3  1 --- 3  1 1 0  =  .  1 2 --- 3   19.54    19.55   Now we have the desired result: the first prototype has the largest inner  product with   . The first neuron in Layer 2 will be activated.  a1  In the Grossberg network of Chapter 18 we obtained normalized prototype  patterns by normalizing the input patterns in Layer 1. In the ART1 net- work we will normalize the prototype patterns by using an on-center off- surround competition in the L1-L2 learning law.  Learning Law The learning law for   W1:2   is  i  d w1:2  t   ------------------------- dt  =  2 t  ai      b+  –  i  w1:2  t    W+  a1  t    19.56   –    w1:2 i  t   b-+   W-  a1  t    ,  where  b+  =  ,   b-  =  ,   W+  =   and   W-  =   19.57   1 0  0 0 1  0   0 0  1  0 1  1 1 0  1 .   1 1  0  1 1  1  0 0  0  19-18   Learning Law: L1-L2  i  ,   w1:2 i  W1:2 w1:2  i , is moved in the direction of   This is a modified form of instar learning. When neuron  tive, the ith row of  ence between Eq.  19.56  and the standard instar learning is that the  elements of   is normalized. In the bracket  on the right side of Eq.  19.56  we can see that it has the form of a shunting  a1 . The excita- model, with on-center off-surround input connections from  b-   a vector of 1’s , and the inhibitory bias is  , which  tory bias is  0= ensures that the elements of   remain between 0 and 1.  Recall our dis- cussion of the shunting model in Chapter 18.    of Layer 2 is ac- a1 . The differ-   compete, and therefore   w1:2 i  w1:2  1=  b+  i  To verify that Eq.  19.56  causes normalization of the prototype patterns,  let’s investigate the steady state operation. For this analysis we will as- sume that the outputs of Layer 1 and Layer 2 remain constant until the  weights reach steady state. This is called fast learning.  Fast Learning  The equation for element   1:2 wi j   is  1:2 t  dwi j ------------------- dt  =  2 t  1 wi j ai  –  1:2 t   aj  1 t  wi j  1:2 t   –    1 t  ak  .   19.58   j k 2 t  ai  1=    and set the de-  If we assume that neuron  rivative to zero in Eq.  19.58 , we see that   is active in Layer 2    i  0  =  1:2 – 1 wi j  aj  1:2  1 wi j –  1 ak  .    k  j   19.59   , we will consider two cases. First, as-  To find the steady state value of  sume that  . Then we have  1=  1 aj  1:2 wi j  0  =  1:2 – 1 wi j   wi j  –  1:2 a1 2    1–    =  –   +  a1 2  1–  1:2 wi j  +  ,   19.60   1:2 wi j  =   a1 2  ------------------------------  1–  +  .  19   19.61   S1   k  1=   Note that   1 ak  =  a1 2  , since   a1   is a binary vector.   On the other hand, if   0=  , then Eq.  19.59  reduces to  1 aj  0  –=  1:2 a1 2 wi j  ,   19.62   or  or  19-19   19 Adaptive Resonance Theory  To summarize Eq.  19.61  and Eq.  19.63 :  1:2 wi j  0=  .  w1:2  i  =  a1 a1 2  ------------------------------  1–  +  ,   19.63    19.64   Learning Law: L2-L1  where     1   to ensure that the denominator will never equal zero.   Therefore the prototype patterns will be normalized, and this will solve the  subset superset dilemma.  By “normalized” here we do not mean that all  prototype vectors will have unit length in Euclidean distance, but simply  that the rows of   that have more nonzero entries will have elements  with smaller magnitudes. In this case, vectors with more nonzero entries  may actually have a smaller length than vectors with fewer nonzero en- tries.   W1:2  W2:1  The L2-L1 connections,  , in the ART1 architecture are trained using  an outstar learning rule. The purpose of the L2-L1 connections is to recall  an appropriate prototype pattern  the expectation , so that it can be com- pared and combined, in Layer 1, with the input pattern. When the expec- tation and the input pattern do not match, a reset is sent to Layer 2, so that  a new prototype pattern can be chosen  as we have discussed in previous  sections .  The learning law for   W2:1   is a typical outstar equation:  2:1 t    d wj ------------------------- dt  =  2 t  wj aj  –    2:1 t   a1 t     .  +   19.65   Therefore, if neuron  column  investigate the steady state operation of Eq.  19.65 .   in Layer 2 is active  has won the competition , then   pattern. To illustrate this, let’s    is moved toward the   j W2:1  of   a1  j  For this analysis we will assume the fast learning scenario, where the out- puts of Layer 1 and Layer 2 remain constant until the weights reach steady  state. Assume that neuron  . Setting  the derivative in Eq.  19.65  to zero, we find   in Layer 2 is active, so that   1=  2 aj  j  0  =  2:1  –  wj  a1+  , or   wj  2:1  a1=  .   19.66   . Recall  Therefore column  from Eq.  19.20  and Eq.  19.21  that   is a combination of the input pat- tern and the appropriate prototype pattern. Therefore the prototype pat-   converges to the output of Layer 1,   a1  j W2:1  of   a1  19-20   ART1 Algorithm Summary  tern is modified to incorporate the current input pattern  if there is a close  enough match .  W1:2  W2:1  j   and   Keep in mind that   are updated at the same time. When neu- ron   of Layer 2 is active and there is a sufficient match between the expec- tation and the input pattern  which indicates a resonance condition , then  j W1:2  of   are adapted. In fast learning, column  row  j W2:1  of   is set to a normalized version of  a1 .  j W2:1  of  j W1:2  of    and column   , while row    is set to   a1  ART1 Algorithm Summary  Now that we have investigated each of the subsystems of the ART1 archi- tecture, we can gain some insight into its overall operation if we summarize  the key steady state equations and organize them into an algorithm.  W2:1   and   . The initial   Initialization The ART1 algorithm begins with an initialization of the weight matrices  W1:2  matrix is set to all 1’s. Thus, the first time  a new neuron in Layer 2 wins a competition, resonance will occur, since  a1 . This means that any  untrained column in  match with any input pattern.   is effectively a blank slate and will cause a    and therefore   a1 2 p 2  p wj  W2:1  W2:1    1=    2:1  p  =  =    Since the rows of the  umns of  S1   W2:1  1–   +    , every element of the initial  .  W1:2   matrix should be normalized versions of the col-  W1:2   matrix is set to   Algorithm After initialization, the ART1 algorithm proceeds as follows:  1. First, we present an input pattern to the network. Since Layer 2 is not   , the output of Layer 1 is  Eq.   0=  active on initialization  i.e., each   19.20    2 aj  19  2. Next, we compute the input to Layer 2,  a1  p=  .  W1:2a1  ,   19.67    19.68   and activate the neuron in Layer 2 with the largest input  Eq.  19.37  :  19-21   19 Adaptive Resonance Theory  2 ai  =  1 , 0 ,        if w1:2   i  =  Ta1 max w1:2 otherwise  k    Ta1      .   19.69   In case of a tie, the neuron with the smallest index is declared the win- ner.  3. We then compute the L2-L1 expectation  where we assume neuron   of   j  Layer 2 is activated :  4. Now that Layer 2 is active, we adjust the Layer 1 output to include the   L2-L1 expectation  Eq.  19.21  :  W2:1a2  =  2:1  wj  .  a1  =  p wj  2:1  .  5. Next, the Orienting Subsystem determines the degree of match be-  tween the expectation and the input pattern  Eq.  19.49  :  a0  =  1 , 0 ,    if a1 2 p 2  otherwise      .        6.  a0  1=  If   resonance , and return to step 1. If   , then we set   0=  a0  2 aj  , inhibit it until an adequate match occurs   0=  , we continue with step 7.  7. Resonance has occurred. Therefore we update row   j W1:2  of     Eq.    19.61  :  a1 a1 2  ------------------------------  1–  .  w1:2  j  =  + j W2:1  of   8. We now update column     Eq.  19.66  :   19.74  9. We remove the input pattern, restore all inhibited neurons in Layer 2,   wj  .  2:1  a1=  and return to step 1 with a new input pattern.  The input patterns continue to be applied to the network until the weights  stabilize  do not change . Carpenter and Grossberg have shown [CaGr87a]  that the ART1 algorithm will always form stable clusters for any set of in- put patterns.  See Problems P19.5, P19.6 and P19.7 for detailed examples of the ART1 al- gorithm.  19-22   19.70    19.71    19.72    19.73    Other ART Architectures  To experiment with the ART1 algorithm, use the Neural Network Design  Demonstration ART1  nnd16a1 .  Other ART Architectures  The ART1 network is just one example of adaptive resonance theory. Car- penter and Grossberg, and others in their research group, have developed  many variations on this theme.  One disadvantage of the ART1 network is that it can only be used for bina- ry input patterns. Carpenter and Grossberg developed a variation of ART1,  called ART2, to handle either analog or binary patterns [CaGr87b]. The ba- sic structure of ART2 is very similar to ART1, with the exception of Layer  1. In ART2 several sublayers take the place of Layer 1. These sublayers are  needed because analog vectors, unlike binary vectors, can be arbitrarily  close together. The sublayers perform a combination of normalization and  noise suppression, in addition to the comparison of the input vector and the  expectation that is needed by the orienting subsystem.  Carpenter and Grossberg later developed the ART3 network [CaGr90],  which introduced a more sophisticated biological model for the reset mech- anism required for ART. Up to the present time, this network has not been  widely applied.  In 1991 Carpenter, Grossberg and Reynolds introduced the ARTMAP net- work [CaGrRe91]. In contrast with all of the previous ART networks, it is  a supervised network. The ARTMAP architecture consists of two ART mod- ules that are connected by an “inter-ART” associative memory. One ART  module receives the input vector, while the other ART module receives the  desired output vector. The network learns to predict the correct output vec- tor whenever the input vector is presented.  More recently, Carpenter, Grossberg, Markuzon, Reynolds and Rosen have  modified the ARTMAP architecture to incorporate fuzzy logic. The result is  referred to as Fuzzy ARTMAP [CaGrMa92]. It seems to improve perfor- mance, especially with noisy input patterns.  19  All of these ART architectures incorporate the key modules discussed in  this chapter, including:    L1-L2 instars for pattern recognition.    L2-L1 outstars for pattern recall.    Layer 2 for contrast enhancement  competition .    Layer 1 for comparison of input and expectation.    Orienting Subsystem for resetting when a pattern mismatch occurs.  19-23   19 Adaptive Resonance Theory  Summary of Results  Basic ART Architecture  Layer 1  Layer 2  Input  Gain Control  Expectation  Reset  Orienting Subsystem  ART1 Network  Binary Patterns   ART1 Layer 1  Input  Layer 1  +  +  Expectation  +b1  +  -  +  -  n1.   cid:0 1 ε  cid:0   a2   cid:0  cid:0  W2:1  cid:0  cid:0  S1 x S2  cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0  a1  cid:0  cid:0   cid:0  cid:0   f 1  S1  + - n1  +  +  -b1  Gain Control   cid:0  cid:0  -W1 S1 x S2  a2  ε dn1 dt = - n1 +  +b1 - n1   { p + W2:1 a2} -  n1 + -b1  [-W1] a2  p S1 x 1  S1  19-24   Summary of Results  Layer 1 Equation  dn1 t    -------------- dt  =  n1 t     –  +  b+ 1 n1 t     –     p W2:1a2 t  +    –  n1 t       b- 1+   W-   1  a2 t   If Layer 2 is not active  i.e., each   If Layer 2 is active  i.e., one   Steady State Operation  ,   a1  2 aj 1=  0= a1   ,   p=  . 2:1  =  p wj  .  2 aj  ART1 Layer 2  Layer 2  +  On-Center   cid:0  cid:0  +  cid:0  cid:0   W1:2 S2 x S1  +  -  +  -  n2.   cid:0 1 ε  cid:0   a1  a0 Reset   cid:0  cid:0  +W2  cid:0  cid:0   cid:0  cid:0  S2 x S2  cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   f 2  +b2  -b2  +  -  n2  +  +   cid:0  cid:0  a2  cid:0  cid:0   cid:0  cid:0   S2   cid:0  cid:0 S2 x S2 -W2 ε dn2 dt = - n2 +  +b2 - n2  {[+W2] f 2 n2  + W1:2 a1}  Off-Surround  -  n2 + -b2  [-W2] f 2 n2   Layer 2 Equation  dn2 t    -------------- dt  =  n2 t     –  +  b+ 2 n2 t     –   W+       2  f2 n2 t          W1:2a1 +    n2 t       b- 2+   W-   2  f2 n2 t           –  Steady State Operation  if w1:2   i  =  Ta1 max w1:2 otherwise  j    Ta1      2 ai  =  1 , 0 ,        19-25  19   19 Adaptive Resonance Theory  Orienting Subsystem  Orienting Subsystem  +W0 1 x S1   cid:0  cid:0   cid:0  cid:0  +  cid:0  cid:0   cid:0  cid:0   -W0 1 x S1  -  p  a1  +  -  n0.   cid:0 1 ε  cid:0   +b0  -b0  +  -  n0  +  +   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0  a0  cid:0  cid:0  Reset cid:0  cid:0   f  0  1  ε dn0 dt = -n0 +  +b0 - n0  [+W0] p -  n0 + -b0  [-W0] a 1  Orienting Subsystem Equation  dn0 t    -------------- dt  =  n0 t     –  b+ 0 n0 t     –   W+   0p      –  n0 t       b- 0+   W-   0a1    +  where   0  W+  =       ,   0  W-  =       ,   b+ 0  =  b- 0  =  1  Steady State Operation  a0  =  1 , 0 ,    if a1 2 p 2  otherwise            L1-L2 Learning Law  i  d w1:2  t   ------------------------- dt  =  2 t  ai      b+  –  w1:2 i  t    W+  a1  t   –  w1:2  t   b-+   W-  a1  t       i  b+  =       b-  =       W+  =       W-  =  1 0  0 0 1  0   0 0  1  0 1  1 1 0  1   1 1  0  1 1  1  0 0  0  19-26   Summary of Results  Steady State Operation  Fast Learning   w1:2  i  =  a1 a1 2  ------------------------------  1–  +    Neuron   i   in Layer 2 Active   L2-L1 Learning Law  2:1 t    d wj ------------------------- dt  =  2 t  wj aj  –    2:1 t   a1 t     +  Steady State Operation  Fast Learning   2:1  wj  a1=    Neuron   j   in Layer 2 Active   ART1 Algorithm  Fast Learning  Summary  Initialization  The initial   W2:1   matrix is set to all 1’s.  Every element of the initial   W1:2   matrix is set to        +  S1  1–    .  Algorithm  a1  p=  .  W1:2a1  ,  1. First, we present an input pattern to the network. Since Layer 2 is not   active on initialization  i.e., each   0=   , the output of Layer 1 is  2 aj  2. Next, we compute the input to Layer 2,  and activate the neuron in Layer 2 with the largest input:  19  2 ai  =  1 , 0 ,        if w1:2   i  =  Ta1 max w1:2 otherwise  k    Ta1      .  In case of a tie, the neuron with the smallest index is declared the win- ner.  3. We then compute the L2-L1 expectation  where we assume neuron   of   j  Layer 2 is activated :  W2:1a2  =  2:1  wj  .  19-27   19 Adaptive Resonance Theory  4. Now that Layer 2 is active, we adjust the Layer 1 output to include the   L2-L1 expectation:  a1  =  p wj  2:1  .  5. Next, the Orienting Subsystem determines the degree of match be-  tween the expectation and the input pattern:  a0  =  1 , 0 ,    if a1 2 p 2  otherwise      .        6.  a0  1=  If   resonance , and return to step 1. If   , then we set   0=  a0  2 aj  , inhibit it until an adequate match occurs   0=  , we continue with step 7.  7. Resonance has occurred, therefore we update row   j W1:2  of   :  8. We now update column   a1 a1 2  w1:2  j  =  ------------------------------  1– + j W2:1  of   :  .  2:1  wj  a1=  .  9. We remove the input pattern, restore all inhibited neurons in Layer 2,   and return to step 1 with a new input pattern.  19-28   Solved Problems  Solved Problems  P19.1 Consider Layer 1 of the ART1 network with the following parame-  ters:    =  0.01       b+ 1  2=       b- 1  3=  .  Assume two neurons in Layer 2, two elements in the input vector  and the following weight matrix and input:  W2:1  =       p  =  0 1 1 1  .  1 1  Also, assume that neuron 1 of Layer 2 is active.  i. Find and plot the response   n1  .  ii. Check to see that the answer to part  i  satisfies the steady   state response predicted by Eq.  19.21 .  i. Since Layer 2 is active, and neuron 1 of Layer 2 wins the competition,  the equations of operation of Layer 1 are    0.01    1 dn1 -------- dt    0.01    1 dn2 -------- dt  =  =  =  =  1– n1  +  1– 2 n1  2:1  p1 w1 1   +    –    1 n1  3+    1– n1  +  1– 2 n1   1 0+    –    1 n1  3+    =  –  1 3n1  1–  1– n2  +  1– 2 n2  2:1  p2 w2 1   +    –    1 n2  3+    1– n2  +  1– 2 n2   1 1+    –    1 n2  3+    =  –  1 4n2  +  1  .  These can be simplified to obtain  19  If we assume that both neurons start with zero initial condition, the solu- tions are  1 dn1 -------- dt  1 dn2 -------- dt  =  –  300n  –  100  ,  1 1  =  –  1 400n2  +  100  .  19-29   19 Adaptive Resonance Theory  1 t  n1  =  1 ---– 3  –– 1  e 300t    ,  1 t  n2  =  1 --- 1 e 400t –– 4    .  These are displayed in Figure P19.1.  0.4  0.2  0  -0.2  -0.4 0  1 t  n2  1 t  n1  0.005  0.015  0.02  0.01 t  Figure P19.1  Response of Layer 1  1 t  n1  ii. Note that  positive value. Therefore,  call that the transfer function for Layer 1 is  our steady state analysis  see Eq.  19.21  , since  1 t   converges to a negative value, and   converges to a  n2 1 t   converges to 1  re- a2 hardlim+   converges to 0, and    . This agrees with   1 t  a1  2:1 p w1    =    1 1  0 1  =  0 1  =  a1  .   19.75   P19.2 Consider Layer 2 of the ART1 network with the following parame-    =  0.1       b+ 2  =       b- 2  =       W1:2  =  2 2  2 2  w1:2 w1:2  T T  1 2  =  0.5 0.5 0 1  ters:  and  f2 n     =      10 n 2  0  n n  0 0  .  19-30   Solved Problems  Assume that the output of Layer 1 is  a1  =  .  1 0  This is equivalent to the Layer 2 example in the text  page 19-12 ,  with the exception of the bias values.  i. Write the equations of operation of Layer 2 and simulate and  plot the response. Explain the effect of increasing the bias  values.  ii. Verify that the steady state operation of Layer 2 is correct.  i. The equations of operation of the layer will be  0.1    2 t    dn1 -------------- dt  0.1    2 t    dn2 -------------- dt  =  –  2 t    n1  +  – 2  2 t    n1   f2 n1      2 t       +  1  w1:2  Ta1    –    2 t    n1  2+  f2 n2     2 t       ,  =  –  2 t    n2  +  – 2  2 t    n2   f2 n2      2 t       +  2  w1:2  Ta1    –    2 t    n2  2+  f2 n1     2 t       .  Figure P19.2 illustrates the response of Layer 2 when the input vector is  a1   than the first row, therefore neuron 2 wins the competition.    has a larger inner product with   . The second row of   W1:2  1 0  a1  =  T  19  w1:2  Ta1  2  w1:2  Ta1  1  2 t  n2  2 t  n1  0.05  0.15  0.2  0.1 t  Figure P19.2  Response of Layer 2  2  1  0  -1  -2 0  19-31   19 Adaptive Resonance Theory  If we compare Figure P19.2 with Figure 19.5, we can see that the bias value  has three effects. First, the speed of response is increased; the neuron out- puts move more quickly to their steady state values. Second, the range of  the response is increased from  .  Recall from Chapter 18  b+ that for the shunting model the upper limit will be the excitatory bias  .  b- The lower limit will be the inhibitory bias  .  Third, the neuron responses  move closer to the upper and lower limits.  2–  2 [  1–  1   to   ]  ]  [  ,  ,  ii. At steady state,  ue. The steady state Layer 2 output will then be   has a positive value, and   2 t    n1  2 t    n2   has a negative val-  a2  =  .  1 0  This agrees with the desired steady state response characteristics of Layer  2:   2 ai  =  1 , 0 ,        if w1:2   i  =  Ta1 max w1:2 otherwise  j    Ta1      .  P19.3 Consider the Orienting Subsystem of the ART1 network with the   following parameters:    =  0.1         =  0.5         2=          =  0.25        b+ 0  =  b- 0  =  0.5  .  The inputs to the Orienting Subsystem are  p  =       a1  =  .  1 1 1  1 0 1  i. Find and plot the response of the Orienting Subsystem   n0 t   .  ii. Verify that the steady state conditions are satisfied.  The equation of operation of the Orienting Subsystem is  0.1  dn0 t    -------------- dt  =  n0 t     –  +    0.5  –  n0 t      0.5 p1     +  p2  +  p3      n0 t       –  0.5+  1  2 a1     +  1 a2  +  1 a3      or  19-32   Solved Problems  The response is then  dn0 t    --------------  dt  =  65n0 t     –  –  12.5  .  n0 t   =  –  0.1923  –– 1  e 65t    This response is plotted in Figure P19.3. In this case, since  tive,  = Layer 2.   hardlim+ n0  a0  =  0    , and therefore a reset signal will not be sent to    is nega-  n0 t   n0 t   0.1 t  -0.3 0  0.05  0.15  0.2  Figure P19.3  Response of the Orienting Subsystem  ii. The steady state operation of the Orienting Subsystem can be summa- rized as follows:  a0  =  1 , 0 ,    if a1 2 p 2  otherwise      .        19  For this problem   a1 2 p 2    =  2    1 0 1  2  1 1 1  =    =  0.25  .  2 --- 3  Therefore   a0  0=  , which agrees with the results of part  i .  0.3  0.15  0  -0.15  19-33   19 Adaptive Resonance Theory  P19.4 Show that the learning equation for the L2-L1 connections is   equivalent to the outstar equation described in Chapter 15.  The L2-L1 learning law  Eq.  19.65   is  If we approximate the derivative by   2:1 t    d wj ------------------------- dt  =  2 t  wj aj  –    2:1 t   a1 t     .  +  2:1 t      d wj ----------------------- dt    2:1 t    wj -------------------------------------------------  2:1 t t+    wj – t  ,  then we can rewrite Eq.  19.65  as  2:1 t t+     wj  =  2:1 t     wj  +    t  aj  2 t    wj    –  2:1 t     a1 t       .  +  This is equivalent to the outstar rule of Chapter 15  Eq.  15.51  . Here the  input to the L2-L1 connections is  , and the output of the L2-L1 connec- tions is   2 t    aj  a1  .  P19.5 Train an ART1 network using the following input vectors:  p1  =  ,   p2  =  ,   p3  =  .  1 0 0  1 1 0  0 1 0    2=  , and     =  0.4  , and choose   S2  3=    3 cate-  Use the parameters  gories .  Our initial weights will be  W2:1  =  ,   W1:2  =  1 1 1 1 1 1 1 1 1  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5  .  We now begin the algorithm.  1. Compute the Layer 1 response:  a1  =  p1  =  .  0 1 0  19-34   Solved Problems  2. Next, compute the input to Layer 2:  W1:2a1  =  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5  0 1 0  =  .  0.5 0.5 0.5  Since all neurons have the same input, pick the first neuron as the win- ner.  In case of a tie, pick the neuron with the smallest index.   a2  =  3. Now compute the L2-L1 expectation:  4. Adjust the Layer 1 output to include the L2-L1 expectation:  W2:1a2  =  =  =  2:1 w1  .  1 1 1 1 1 1 1 1 1  1 0 0  a1  =  2:1 p1 w1    =    =  .  1 1 1  1 1 1  0 1 0  5. Next, the Orienting Subsystem determines the degree of match be-  tween the expectation and the input pattern:  a1 2 p1    2  1 ---= 1    =  0.4  , therefore   a0  0=    no reset .  6. Since   a0  0=  , continue with step 7.  7. Resonance has occurred, therefore update row   1 W1:2   of   :  19  w1:2 1  =  2a1 a1 2  ------------------------------ 2 1–  +  =  a1  =  ,   W1:2  =  1  0 0 0.5 0.5 0.5 0.5 0.5 0.5  .  8. Update column   1 W2:1   of   :  1 0 0  0 1 0  0 1 0  19-35   19 Adaptive Resonance Theory  2:1 w1  =  a1  =  ,   W2:1  =  0 1 0  0 1 1 1 1 1 0 1 1  .  9. Remove   p2 1. Compute the new Layer 1 response  Layer 2 inactive :  , and return to step 1 with input pattern   p1  .  a1  =  p2  =  .  1 0 0  2. Next, compute the input to Layer 2:  W1:2a1  =  1  0 0 0.5 0.5 0.5 0.5 0.5 0.5  1 0 0  =  .  0 0.5 0.5  Since neurons 2 and 3 have the same input, pick the second neuron as  the winner:  a2  =  .  0 1 0   19.76   3. Now compute the L2-L1 expectation:  4. Adjust the Layer 1 output to include the L2-L1 expectation:  W2:1a2  =  =  2:1 w2  =  0 1 1 1 1 1 0 1 1  0 1 0  .  1 1 1  a1  =  2:1 p2 w2    =    =  .  1 0 0  1 1 1  1 0 0  5. Next, the Orienting Subsystem determines the degree of match be-  tween the expectation and the input pattern:  a1 2 p2    2  1 ---= 1    =  0.4  , therefore   a0  0=    no reset .  19-36   Solved Problems  6. Since   a0  0=  , continue with step 7.  7. Resonance has occurred, therefore update row   2 W1:2   of   :  w1:2 2  =  2a1 a1 2  ------------------------------ 2 1–  +  =  a1  =  ,   W1:2  =  1 0 0  1 0  0 0 1 0 0.5 0.5 0.5  .  8. Update column   2 W2:1   of   :  2:1 w2  =  a1  =  ,   W2:1  =  1 0 0  0 1 1 1 0 1 0 0 1  .  9. Remove   p2  , and return to step 1 with input pattern   p3  .  1. Compute the Layer 1 response with the new input vector:  a1  =  p3  =  .  1 1 0  2. Next, compute the input to Layer 2:  W1:2a1  =  1 0  0 0 1 0 0.5 0.5 0.5  1 1 0  =  .  1 1 1  a2  =  .  1 0 0  3. Now compute the L2-L1 expectation:  W2:1a2  =  =  2:1 w1  =  0 1 1 1 0 1 0 0 1  1 0 0  .  0 1 0  4. Adjust the Layer 1 output to include the L2-L1 expectation:  19-37  Since all neurons have the same input, pick the first neuron as the win- ner:  19   19 Adaptive Resonance Theory  a1  =  2:1 p3 w1    =    =  .  1 1 0  0 1 0  0 1 0  5. Next, the Orienting Subsystem determines the degree of match be-  tween the expectation and the input pattern:  a1 2 p3    2  1 ---= 2    =  0.4  , therefore   a0  0=    no reset .  6. Since   a0  0=  , continue with step 7.  7. Resonance has occurred, therefore update row   1 W1:2   of   :  w1:2 1  =  2a1 a1 2  ------------------------------ 2 1–  +  =  a1  =  ,   W1:2  =  0 1 0  1 0  0 0 1 0 0.5 0.5 0.5  .  8. Update column   1 W2:1   of   :  2:1 w2  =  a1  =  ,   W2:1  =  0 1 0  0 1 1 1 0 1 0 0 1  .  This completes the training, since if you apply any of the three patterns  again they will not change the weights. These patterns have been success- fully clustered. This type of result  stable learning  is guaranteed for the  ART1 algorithm, since it has been proven to always produce stable clus- ters.  P19.6 Repeat Problem P19.5, but change the vigilance parameter to     =  0.6  .  The training will proceed exactly as in Problem P19.5, until pattern  presented, so let’s pick up the algorithm at that point.  p3   is   1. Compute the Layer 1 response:  a1  =  p3  =  .  1 1 0  2. Next, compute the input to Layer 2:  19-38   Since all neurons have the same input, pick the first neuron as the win- ner:  Solved Problems  W1:2a1  =  1 0  0 0 1 0 0.5 0.5 0.5  1 1 0  =  .  1 1 1  a2  =  .  1 0 0  1 1 0  W2:1a2  =  =  2:1 w1  =  0 1 1 1 0 1 0 0 1  1 0 0  .  0 1 0  a1  =  2:1 p3 w1    =    =  .  0 1 0  0 1 0  3. Now compute the L2-L1 expectation:  4. Adjust the Layer 1 output to include the L2-L1 expectation:  5. Next, the Orienting Subsystem determines the degree of match be-  tween the expectation and the input pattern:  a1 2 p3    2  1 ---= 2    =  0.6  , therefore   a0  1=    reset .  6. Since   a0  1=  , set   2 a1   resonance , and return to step 1.  0=  , inhibit it until an adequate match occurs   19  1. Recompute the Layer 1 response  Layer 2 inactive :  a1  =  p3  =  .  1 1 0  2. Next, compute the input to Layer 2:  19-39   19 Adaptive Resonance Theory  W1:2a1  =  1 0  0 0 1 0 0.5 0.5 0.5  1 1 0  =  .  1 1 1  Since neuron 1 is inhibited, neuron 2 is the winner:  3. Now compute the L2-L1 expectation:  a2  =  .  0 1 0  1 1 0  W2:1a2  =  =  2:1 w2  =  0 1 1 1 0 1 0 0 1  0 1 0  .  1 0 0  a1  =  2:1 p3 w2    =    =  .  1 0 0  1 0 0  4. Adjust the Layer 1 output to include the L2-L1 expectation:  5. Next, the Orienting Subsystem determines the degree of match be-  tween the expectation and the input pattern:  a1 2 p3    2  1 ---= 2    =  0.6  , therefore   a0  1=    reset .  0=  , inhibit it until an adequate match occurs   6. Since   a0  1=  , set   2 a2   resonance , and return to step 1.  1. Recompute the Layer 1 response:  a1  =  p3  =  .  1 1 0  2. Next, compute the input to Layer 2:  W1:2a1  =  1 0  0 0 1 0 0.5 0.5 0.5  1 1 0  =  .  1 1 1  19-40   Solved Problems  Since neurons 1 and 2 are inhibited, neuron 3 is the winner:  3. Now compute the L2-L1 expectation:  a2  =  .  0 0 1  1 1 0  W2:1a2  =  =  2:1 w3  =  0 1 1 1 0 1 0 0 1  0 0 1  .  1 1 1  a1  =  2:1 p3 w3    =    =  .  1 1 1  1 1 0  4. Adjust the Layer 1 output to include the L2-L1 expectation:  5. Next, the Orienting Subsystem determines the degree of match be-  tween the expectation and the input pattern:  a1 2 p3    2  2 ---= 2    =  0.6  , therefore   a0  0=    no reset .  6. Since   a0  0=  , continue with step 7.  7. Resonance has occurred, therefore update row   3 W1:2   of   :  w1:2 3  =  2a1 a1 2  ------------------------------ 2 1–  +  =  1  2 ---a 3  =  ,   W1:2  =  2 --- 3 2 --- 3 0  0 1 0 1 0 0 2 --- 3  2 --- 0 3  .  19  8. Update column   3 W2:1   of   :  2:1 w3  =  a1  =  ,   W2:1  =  1 1 0  0 1 1 1 0 1 0 0 0  .  This completes the training, since if you apply any of the three patterns  again they will not change the weights.  Verify this for yourself by applying   19-41   19 Adaptive Resonance Theory  each input pattern to the network.  These patterns have been successfully  clustered.   Note that in Problem P19.5, where the vigilance was  , the patterns  were clustered into two categories. In this problem, with vigilance  ,  the patterns were clustered into three categories. The closer the vigilance  is to 1, the more categories will be used. This is because an input pattern  must be closer to a prototype in order to be incorporated into that proto- type. When the vigilance is close to zero, many different input patterns can  be incorporated into one prototype. The vigilance parameter adjusts the  coarseness of the categorization.  0.4  0.6      =  =  P19.7 Train an ART1 network using the following input vectors  see   [CaGr87a] :  p1  p2  p3  p4  Present the vectors in the order    i.e.,  p1 p2 p3 p1 p4 sented twice in each epoch . Use the parameters   2= and choose  weights have converged.  =   3 categories . Train the network until the   p1  and   3=  S2  –  –  –  –     is pre-  0.6  ,   We begin by initializing the weight matrices. The initial  S1 25 fore it is an    matrix of 1’s. The initial  S1   matrix, with each element equal to    matrix is an   matrix is normalized, there-  3 S2  W1:2  S2  25  =  =  3  W2:1   S1  ----------------------------   + 1–  =  -----------------------------  + 2 1–  2 25  =  0.0769  .  To create the input vectors we will scan each pattern row-by-row, where  each blue square will be represented by a 1 and each white square will be  represented by a 0. Since the input patterns are   grids, this will create  25-dimensional input vectors.  5  5  We now begin the training. Since it is not practical to display all of the cal- culations when the vectors are so large, we have summarized the results of  the algorithm in Figure P19.4. Each row represents one iteration of the  ART1 algorithm  presentation of one input vector . The left-most pattern  in each row is the input vector. The remainder of the patterns represent the  three columns of the  resonance point — the column of   that matched with the input pattern.  Whenever a reset occurred, it is represented by a check mark. When more  than one reset occurred in a given iteration, the number beside the check  mark indicates the order in which the reset occurred.   matrix. At each iteration, a star indicates the   W2:1  W2:1  19-42   Solved Problems  p  w1  2:1 w2  2:1 w3  2:1  p  w1  2:1 w2  2:1 w3  2:1  1  2  3  4  5  1  2  1  6  7  8  9  10   a  First Epoch   b  Second Epoch  Figure P19.4  ART1 Iterations for Problem   A total of 10 iterations of the algorithm were performed  two epochs of the  sequence   . The weights are now stable.  You may want  to check this by presenting each input pattern.   p1 p2 p3 p1 p4  –  –  –  –  p1  is presented,   There are several interesting points to notice in this example. First, notice  that at iteration 4 both  . However, on iteration  5, when   no  p4 longer provides an adequate match with  , as we can see at itera- tions 6 and 8. This requires them to take over neuron  , which was unused  during the first epoch.   is modified to include  p3   are coded by    and  2:1 w2  . This new    and   2:1 w2  2:1 w2  p3  p4  p1  3  The results of the algorithm could be modified by changing the vigilance  parameter. How small would you have to make the vigilance, so that only  two neurons in Layer 2 would be required to code all 4 input vectors? How  large would the vigilance have to be before a fourth Layer 2 neuron was  needed?  19  19-43   19 Adaptive Resonance Theory  Epilogue  Competitive learning, and many other types of neural network training al- gorithms, suffer from a problem called the stability plasticity dilemma. If a  learning algorithm is sensitive to new inputs  plastic , then it runs the risk  of forgetting prior learning  unstable . The ART networks were designed to  achieve learning stability while maintaining sensitivity to novel inputs.   In this chapter, the ART1 network was used to illustrate the key concepts  of adaptive resonance theory. The ART1 network is based on the Grossberg  competitive network of Chapter 18, with a few modifications. The key inno- vation of ART is the use of “expectations.” As each input pattern is present- ed to the network, it is compared with the prototype vector that it most  closely matches  the expectation . If the match between the prototype and  the input vector is not adequate, a new prototype is selected. In this way,  previously learned memories  prototypes  are not eroded by new learning.  One important point to keep in mind when analyzing ART networks, is  that they were designed to be biologically plausible mechanisms for learn- ing. They have as much to do with understanding how the brain works as  they do with inspiring practical pattern recognition systems. For this rea- son, the learning mechanisms are required to use only local information at  each neuron. This is not true of all of the learning rules discussed in this  text.  Although the ART networks solve the problem of learning instability, in  which the network weights never stabilize, there is another stability prob- lem that we have not yet discussed. This is the stability of the differential  equations that implement the short-term memory equations of the net- work. In Layer 2, for example, we have a set of differential equations with  nonlinear feedback. Can we make some general statement about the sta- bility of such systems? Chapter 20 will present a comprehensive discussion  of this problem.  19-44   Further Reading  Further Reading  [CaGr87a]  G. A. Carpenter and S. Grossberg, “A massively parallel ar- chitecture for a self-organizing neural pattern recognition  machine,” Computer Vision, Graphics, and Image Process- ing, vol. 37, pp. 54–115, 1987.  In this original presentation of the ART1 architecture, Car- penter and Grossberg demonstrate that the architecture  self-organizes and self-stabilizes in response to an arbi- trary number of binary input patterns. The key feature of  ART is a top-down matching mechanism.  G. A. Carpenter and S. Grossberg, “ART2: Self-organiza- tion of stable category recognition codes for analog input  patterns,” Applied Optics, vol. 26, no. 23, pp. 4919–4930,  1987.  This article describes an extension of the ART1 architec- ture that is designed to handle analog input patterns.  G. A. Carpenter and S. Grossberg, “ART3: Hierarchical  search using chemical transmitters in self-organizing pat- tern recognition architectures,” Neural Networks, vol. 3,  no. 23, pp. 129–152, 1990.  This article demonstrates how the Orienting Subsystem of  the ART networks could be implemented in biological neu- rons through the use of chemical transmitters.  [CaGr87b]  [CaGr90]  [CaGrMa92] G. A. Carpenter, S. Grossberg, N. Markuzon, J. Reynolds  and D. Rosen, “Fuzzy ARTMAP: An adaptive resonance ar- chitecture for incremental learning of analog maps,” Pro- ceedings of the International Joint Conference on Neural  Networks, Baltimore, MD, vol. 3, no. 5, pp. 309–314, 1992. The authors present a modification of the ARTMAP archi- tecture to include fuzzy logic that enables better perfor- mance in a noisy environment.  19  [CaGrRe91]  G. A. Carpenter, S. Grossberg and J. Reynolds, “ARTMAP:  Supervised real-time learning and classification of nonsta- tionary data by a self-organizing neural network,” Neural  Networks, vol. 4, no. 5, pp. 169–181, 1991. This article presents an adaptive resonance theory net- work for supervised learning. The network consists of two  interconnected ART modules. One module receives the in- put vector, and the other module receives the desired out- put vector.   19-45   19 Adaptive Resonance Theory  [Gros76]  S. Grossberg, “Adaptive pattern classification and univer- sal recoding: I. Parallel development and coding of neural  feature detectors,” Biological Cybernetics, vol. 23, pp. 121– 134, 1976.  Grossberg describes a continuous-time competitive net- work, inspired by the developmental physiology of the visu- al cortex. The structure of this network forms the  foundation for other important networks.  [Gros82]  S. Grossberg, Studies of Mind and Brain, Boston: D. Reidel  Publishing Co., 1982.  This book is a collection of Stephen Grossberg papers from  the period 1968 through 1980. It covers many of the funda- mental concepts used in later Grossberg networks, such as  the adaptive resonance theory networks.  19-46   Exercises  Exercises  E19.1 Consider Layer 1 of the ART1 network with   . Assume two neurons  in Layer 2, two elements in the input vector and the following weight ma- trix and input:  0.02  =    W2:1  =       p  =  0 1 1 1  .  0 1  Also assume that neuron 2 of Layer 2 is active.  i. Find and plot the response   ii. Find and plot the response   iii. Find and plot the response   n1 n1 n1   if    if    if   b+ 1 b+ 1 b+ 1  2=   and   4=   and   4=   and   b- 1 b- 1 b- 1  3=  5=  4=  .  .  .  » 2 + 2 ans =       4  iv. Check to see that the answers to parts  i – iii  satisfy the steady  state response predicted by Eq.  19.21 . Explain any inconsisten- cies.  v. Check your answers to parts  i – iii  by writing a MATLAB M-file to  simulate Layer 1 of the ART1 network. Use the ode45 routine. Plot  the response for each case.  E19.2 Consider Layer 2 of the ART1 network with the following parameters:  and    =  0.1       W1:2  =  w1:2 w1:2  T T  1 2  =  2 2 --- --- 3 3 1 0  f2 n     =      10 n 2 ,  0 ,  n n  0 0  .  Assume that the output of Layer 1 is  a1  =  .  1 1  19  » 2 + 2 ans =       4  i. Write the equations of operation of Layer 2, and simulate and plot   the response if the following bias vectors are used:  19-47   19 Adaptive Resonance Theory  ii. Repeat part  i  for the following bias vectors:  iii. Repeat part  i  for the following bias vectors:  b+ 2  =       b- 2  =  b+ 2  =       b- 2  =  b+ 2  =       b- 2  =  .  .  .  2 2  3 3  0 0  iv. Do the results of all of the previous parts satisfy the desired steady   state response described in Eq.  19.37 ? If not, explain why.  E19.3 Consider the Orienting Subsystem of the ART1 network with the following   parameters:    =  0.1       b+ 0  =  b- 0  =  2  .  The inputs to the Orienting Subsystem are  p  =       a1  =  .  0 0 1  i. Find and plot the response of the Orienting Subsystem   n0 t   , for     =  0.5         4=          =  0.125   .  ii. Find and plot the response of the Orienting Subsystem   n0 t   , for     =  0.5         2=          =  0.25   .  iii. Verify that the steady state conditions are satisfied in parts  i  and    ii .  » 2 + 2 ans =       4  iv. Check your answers to parts  i  and  ii  by writing a MATLAB M-file   to simulate the Orienting Subsystem.  2 2  3 3  3 3  1 1 1  19-48   Exercises  E19.4 To derive the steady state conditions for the L1-L2 and L2-L1 learning   rules, we have made the assumption that the input pattern and the neuron  outputs remain constant until the weight matrices converge. This is called  “fast learning.” Show that this fast learning assumption is equivalent to  setting the learning rate   to 1 in the instar and outstar learning rules pre- sented in Chapter 15 and the Kohonen competitive learning rule in Chap- ter 16.    E19.5 Train an ART1 network using the following input vectors:  p1  =  ,   p2  =  ,   p3  =  ,   p3  =  .  0 1 0 1  1 0 0 1  1 1 0 0  1 1 1 1  Use the parameter     2=  , and choose   3=    3 categories .  S2  i. Train the network to convergence using     =  0.3  .  ii. Repeat part  i  using     =  0.6  .  iii. Repeat part  ii  using     =  0.9  .  E19.6 The ART1 algorithm can be modified to add a new neuron in Layer 2 when- ever there is no adequate match between the existing prototypes and the  input pattern. This involves creating a new row of the   matrix and a  new column of the    matrix. Describe how this would be done.  W1:2  W2:1  E19.7 Write a Matlab M-file to implement the ART1 algorithm  with the modifi- cation described in Exercise E19.6 . Use this M-file to train an ART1 net- work using the following input vectors  see Problem P19.7 :  » 2 + 2 ans =       4  19  p1  p2  p3  p4  Present the vectors in the order  twice in each epoch . Use the parameters  S2 Compare your results with Problem P19.7.  , and choose    3 categories . Train the network until the weights have converged.    is presented    i.e.,  p1 = 0.9  p1 p2 p3 p1 p4  –  and  2=  –   3=  –  –    19-49   19 Adaptive Resonance Theory  E19.8 Recall the digit recognition problem described in Chapter 7  page 7-10 .   Train an ART1 network using the digits  – , as displayed below:  0 9  » 2 + 2 ans =       4  p1  p2  p3  p4  p5  p7  p8  p9  p10  Use the parameter  lab M-file from Exercise E19.7.  2=    , and choose   5=    5 categories . Use the Mat-  p6  S2  i. Train the network to convergence using     ii. Train the network to convergence using     iii. Train the network to convergence using     =  =  =  0.3  0.6  0.9  .  .  .  iv. Discuss the results of parts  i – iii . Explain the effect of the vigi-  lance parameter.  19-50   Objectives  20 Stability  Objectives Theory and Examples  Recurrent Networks Stability Concepts  Definitions  Lyapunov Stability Theorem Pendulum Example LaSalle’s Invariance Theorem  Definitions Theorem Example Comments Summary of Results Solved Problems Epilogue Further Reading Exercises  20-1 20-2 20-2 20-3 20-4 20-5 20-6 20-12 20-12 20-13 20-14 20-18 20-19 20-21 20-28 20-29 20-30  Objectives  The problem of “convergence” in a recurrent network was first raised in our  discussion of the Hopfield network, in Chapter 3. It was noted there that  the output of a recurrent network could converge to a stable point, oscillate,  or perhaps even diverge. The “stability” of the steepest descent process and  of the LMS algorithm were discussed in Chapter 9 and Chapter 10, respec- tively. The stability of Grossberg’s continuous-time recurrent networks  was discussed in Chapter 18.   In this chapter we will define stability more carefully. Our objective is to  determine whether a particular set of nonlinear equations has points  or  trajectories  to which its output might converge. To help us study this topic  we will introduce Lyapunov’s Stability Theorem and apply it to a simple,  but instructive, problem. Then, we will present a generalization of the  Lyapunov Theory: LaSalle’s Invariance Theorem. This will set the stage for  Chapter 21, where LaSalle’s theorem is used to prove the stability of  Hopfield networks.  20  20-1   20 Stability  Theory and Examples  Recurrent Networks  We first discussed recurrent neural networks, which have feedback connec- tions from their outputs to their inputs, when we introduced the Hamming  and Hopfield networks in Chapter 3. The Grossberg networks of Chapter  18 and Chapter 19 also contain recurrent connections. Recurrent networks  are potentially more powerful than feedforward networks, since they are  able to recognize and recall temporal, as well as spatial, patterns. However,  the behavior of these recurrent networks is much more complex than that  of feedforward networks.   For feedforward networks, the output is constant  for a fixed input  and is  a function only of the network input. For recurrent networks, however, the  output of the network is a function of time. For a given input and a given  initial network output, the response of the network may converge to a sta- ble output. However, it may also oscillate, explode to infinity, or follow a  chaotic pattern. In the remainder of this chapter we want to investigate  general nonlinear recurrent networks, in order to determine their long- term behavior.   We will consider recurrent networks that can be described by nonlinear dif- ferential equations of the form:  d a t  td  =  g a t ,p t ,t     .   20.1   p t   Here   See Figure 20.1.     is the input to the network, and   a t    is the output of the network.   Nonlinear Recurrent Network  p  a  g   cid:0  cid:0  . a  cid:0  cid:0   cid:0  cid:0   a 0   da t  dt =   g  a t , p t , t    20-2  Figure 20.1  Nonlinear, Continuous-Time, Recurrent Network   Stability Concepts  Stability Concepts  We want to know how these systems perform in the steady state. We will  be most interested in those cases where the network converges to a con- stant output — a stable equilibrium point. A nonlinear system can have  many stable points. For some neural networks these stable points repre- sent stored prototype patterns. When possible, we would like to know  where the stable points are, and which initial conditions   converge to  a given stable point  i.e., what is the basin of attraction for a given stable  point? .   a 0   To begin our discussion, let’s introduce some basic stability concepts with  a simple, intuitive example. Consider the motion of a ball bearing, with dis- sipative friction, in a gravity field. In the adjacent figure, we have a ball  bearing at the bottom of a trough  point   . If we move the bearing to a  different position, it will oscillate back and forth in the trough, but, because  of friction, it will eventually settle back to the bottom of the trough. We will  call this position an asymptotically stable point, which we will define more  precisely in the next section.  a  Consider now the second figure in the left margin. Here we have a ball  bearing positioned at the center of a flat surface. If we place the bearing in  a different position, it will not move. The position at the center of the sur- face is not asymptotically stable, since the bearing does not move back to  the center if it is moved away. However, it is stable in a certain sense, be- cause at least the ball does not roll farther away from the center point. We  call this kind of point stable in the sense of Lyapunov, which we will define  in the next section.  Now consider the third figure in the left margin. The ball bearing is posi- tioned at the top of a hill. This is an equilibrium point, since the ball will  remain at the top of the hill, if we position it carefully. However, if the bear- ing is given the slightest disturbance, it will roll down the hill. This is an  unstable equilibrium point.  In the next chapter we will try to design Hopfield neural networks, in  which the stored prototype patterns will be asymptotically stable equilibri- um points. We would also like the basins of attraction for these stable  points to be as large as possible.  For example, consider Figure 20.2. We would like to design neural net- works with large basins of attraction such as those of Case A. One can cer- tainly imagine that if a ball that rolls with high friction is placed  with zero  velocity  in any one of the basins of Case A, it will remain in that basin and  will eventually find its way to the bottom  stable point . However, Case B  is more complicated. If, for instance, one places a ball with friction at point  P, it is not clear which stable point will eventually capture the ball. The ball  may not come to rest at the stable point closest to P. It is also difficult to  tell how large the basin of attraction is for a specific stable point.  20-3  20   20 Stability  Case A  Large Basin of Attraction  Equilibrium Point  Stability  Definition 1: Stability  in the sense of Lyapunov   Case B  P  Complex Region of Attraction  Figure 20.2  Basins of Attraction  Now that we have presented some intuitive notions of stability, we will pur- sue them with mathematical rigor in the remainder of this chapter.  Definitions We will begin with specific mathematical definitions of the different types  of stability discussed in the previous section. In these definitions we will be  talking about the stability of an equilibrium point; a point   where the de- rivative in Eq.  20.1  is zero. For simplicity, we will talk specifically about  the point  , which is referred to as the origin. This restriction does  not affect the generality of our discussion.  0=  a  a  The origin is a stable equilibrium point if for any given value  exists a number  a t    there  , then the resulting motion    such that if    satisfies   a 0    for       a t   0  0  0        .  t  This definition says that the system output is not going to move too far  away from a given stable point, so long as it is initially close to the stable  point. Let’s say that you want the system output to remain within a dis-  of the origin. If the origin is stable, then you can always find a dis- tance     which may be a function of   , such that if the system output is  tance   within   of   of the origin at time   the origin. The position of the ball  with zero velocity  in the figure to the  left is stable in the sense of Lyapunov, so long as the ball bearing has fric- tion. If the ball bearing did not have friction, then any initial velocity would  produce a trajectory  vector  ball.    in this case would consist of the position and the velocity of the    in which the position would go to infinity.  The   , then it will always remain within   a t   a t   0=      t  Next, let’s consider the stronger concept of asymptotic stability.  20-4   Lyapunov Stability Theorem  Asymptotic Stability  Definition 2: Asymptotic Stability  The origin is an asymptotically stable equilibrium point if there exists a  number  a t    such that whenever  t   the resulting motion satisfies    0  0  as     a 0     .  This is a stronger definition of stability. It says that as long as the output  of the system is initially within some distance   of the stable point, the out- put will eventually converge to the stable point. The position of the ball   with zero velocity  in the diagram in the left margin is asymptotically sta- ble, so long as the ball bearing has friction. If there is no friction, the posi- tion is only stable in the sense of Lyapunov.    We would like to build neural networks that have many specified asymp- totically stable points, each of which represents a prototype pattern. This  is the design objective we will use for building Hopfield networks in Chap- ter 21.  In addition to the stability definitions, there is another concept we will use  in analyzing stability. It is the concept of a definite function. The next two  definitions will clarify this concept.  Positive Definite  Definition 3: Positive Definite  A scalar function  a  0  .  V a    is positive definite if   V 0   0=   and   V a   0   for   Positive Semidefinite  Definition 4: Positive Semidefinite  A scalar function   V a    is positive semidefinite if   V a   0   for all   a  .   These definitions can be modified appropriately to define the concepts neg- ative definite and negative semidefinite.  Now that we have defined stabili- ty, let’s consider a method for testing stability.  Lyapunov Stability Theorem  One of the most important approaches for investigating the stability of  nonlinear systems is the theory introduced by Alexandr Mikhailovich  Lyapunov, a Russian mathematician. Although his major work was first  published in 1892, it received little attention outside Russia until much lat- er. In this section we will discuss one of Lyapunov’s most powerful tech- niques for stability analysis — the so-called direct method.  Consider the autonomous  unforced, no explicit time dependence  system:  20  da td  =  g a   .   20.2   20-5   20 Stability  The Lyapunov stability theorem can then be stated as follows.  Theorem 1: Lyapunov Stability Theorem  If a positive definite function  ative semidefinite, then the origin   a 0=  20.2 . If a positive definite function  V a  is negative definite, then the origin   a each case,   V a   V   can be found such that   dV a  dt   is neg-   is stable for the system of Eq.   can be found such that  dV a  dt     is asymptotically stable. In        0=   is called a Lyapunov function of the system.    V a    as a generalized energy function. The concept of the   You can think of  theorem is that if the energy of a system is continually decreasing    dV a  dt mum energy state. Lyapunov’s insight was to generalize the concept of en- ergy, so that the theorem could be applied to systems where the energy is  difficult to express or has no meaning.   negative definite , then it will eventually settle at some mini-  We should note that the theorem only states that if a suitable Lyapunov  function   can be found, the system is stable. It gives us no information  about the stability of the system in those situations where we are unable to  find such a function.  V a   Pendulum Example  We can gain some insight into Lyapunov’s stability theorem by applying it  to a simple mechanical system. This system is very simple, and its opera- tion is easy to visualize, and yet it illustrates important concepts that we  will apply to neural network design in the next chapter. The example sys- tem we will use is the pendulum shown in Figure 20.3.  l  θ  m  mg  Figure 20.3  Pendulum  Using Newton’s second law   tion of the pendulum as  F  =  ma   , we can write the equation of opera-  ml   2  d     t2 d  =  –  c  –  mg  sin      ,  d td   20.3   20-6   Pendulum Example  or  ml   +  c  +  mg  sin      0=  ,   20.4   2 d  t2 d  d td   is the angle of the pendulum,     where  the length of the pendulum,  itational constant.  c   is the damping coefficient, and    is the mass of the pendulum,   is   is the grav-  m  g  l  The first term on the right side of Eq.  20.3  is the damping force, which is  proportional to the velocity of the pendulum. It is this term that represents  the energy dissipation in the system. The second term on the right side of  Eq.  20.3  is the gravitational force, which is proportional to the sine of the  angle of the pendulum. It is equal to zero when the pendulum is straight  down and has its maximum value when the pendulum is horizontal.    0=  , but more generally it is   If the damping coefficient is not zero, the pendulum will eventually come to  rest hanging down in the vertical position. This solution might be viewed  as  That is, given the appropriate initial conditions, the pendulum might sim- ply settle to   or it might rotate once to give a solution of  , etc.   There are many possible equilibrium solutions.  The positions  , for   odd values of   2= n= , are also equilibrium points, but they are not stable.   , where   3 ...  2n  0=  1   2  .       =  =  0  n  n        To analyze the stability of this system, we will write the pendulum equa- tion in state variable form, where it will appear as a pair of first-order dif- ferential equations. Let’s choose the following state variables:  We can write equations for the pendulum in terms of these state variables  as follows:  a1  =   and   a2  =  d td  .  da1 td  a2=  ,  da2 td  =  –  sin  a1    –  g --- l  c ------a2 ml  .  da1 td  =  a2  =  0  ,  20-7   20.5    20.6    20.7    20.8   20  Now we want to investigate the stability of the origin     for this pen- dulum system.  The origin corresponds to a pendulum angle of zero and a  pendulum velocity of zero.  We first want to check that the origin is an equi- librium point. We do this by substituting    into the state equations.  0=  0=  a  a   20 Stability  da2 td  =  –  sin  a1    –  g --- l  c ------a2 ml  =  –  sin  0   –  g --- l  c ------ 0  ml  =  0   20.9   Since the derivatives are zero, the origin is an equilibrium point.   Next we need to find a Lyapunov function for the pendulum. For this ex- ample we will use the energy of the system as the Lyapunov function  . To  obtain the total energy of the pendulum, we add the kinetic and potential  energies.  V  V a   =  1 ---ml2 a2 2  2 mgl 1  –  +  cos  a1       20.10   In order to test the stability of the system, we need to evaluate the deriva- tive of    with respect to time.  V  V a   =    V a   Tg a   =  d td  V a1  da1   td     +  V a2  da2   td      20.11   The partial derivatives of   can be obtained from Eq.  20.10 , and the  derivatives of the two state variables are given by Eq.  20.6  and Eq.  20.7 .  Thus we have  V a   d td  V a   =    mgl  sin  a1    a2  +    ml2a2    sin  a1    –     –  g --- l  c  ------a2  ml  .   20.12   The     mgl  sin  a1    a2   terms cancel, which leaves only  d td  V a   –=  cl a2  2  0  .   20.13   a  0=  In order to prove that the origin     is asymptotically stable, we must  show that this derivative is negative definite. The derivative is zero at the  origin, but it also is zero for any value of  dV a  dt Lyapunov’s theorem, then, we know that the origin is a stable point. How- ever, we cannot say, from the theorem and this Lyapunov function, that the  origin is asymptotically stable.   is negative semidefinite, rather than negative definite. From   , as long as   . Thus,   0=  a1  a2    In this case we know that as long as the pendulum has friction, it will even- tually settle in a vertical position, and, therefore, that the origin is asymp- totically stable. However, Lyapunov’s theorem, using our Lyapunov  function, can only tell us that the origin is stable. To prove that the origin  is asymptotically stable, we will need a refinement of Lyapunov’s theorem,  LaSalle’s Invariance Theorem. We will discuss LaSalle’s theorem in the  next section.  20-8   Pendulum Example  2 2+  First, let’s investigate the pendulum further, by taking a specific numerical  example. Let  . Now we can rewrite the  state equations for the pendulum as  9.8 m  1.96  9.8  =  =  =  =  1  g  c      l  Expressions for   V   and its derivative follow:  da1 td  a2=  ,  da2 td  sin–=  a1    –  0.2a2  .  V  =  9.8  2 1 --- a2 2  2  +  – 1  cos  a1      ,  dV td  –=    19.208   a2  2  .   20.14    20.15    20.16    20.17   Note that   dV dt     is zero for any value of   a1   as long as   a2  0=  .  Figure 20.4 displays the 3-D and contour plots of the energy surface,  , as  the angle varies between -10 and +10 radians and the angular velocity var- ies between -2 and 2 radians per second. Note that in this range there are  three possible minimum points of the energy surface, at 0 and   2  V  .  400  300  200  100  0 2  a2  2  1  0  -1  -2  1  0  a2  -1  -5  0  a1  -2  -10  10  5  -10  -5  5  10  0 a1  20  Figure 20.4  Pendulum Energy Surface   We will find in Chapter 21 that the minimum points of the Lyapunov func- tion can correspond to prototype patterns in an autoassociative neural net- work. The pendulum system, like recurrent neural networks, has many  minimum points.   Of course, the energy plots shown in Figure 20.4 do not tell us in what way,  or by what route, the pendulum finds a particular energy minimum. To   20-9   20 Stability  show this, we have plotted the energy contours, and one particular path for  the pendulum, in Figure 20.5. The response trajectory, shown by the blue  line, starts from an initial position,  , of 1.3 radians  74   and an initial  velocity,  , of 1.3 radians per second. The trajectory converges to the  equilibrium point   a2 0   a1 0   0=  a  .  Contour Plot  -10  -5  5  10  0 x1 a1  Figure 20.5  Pendulum Response on State Variable Plane  A time response plot of the two state variables is shown in Figure 20.6. No- tice that, because the initial velocity is positive, the pendulum continues to  move up initially.  Check to see if this agrees with Figure 20.5.  It reaches  a maximum angle of about 2 radians before falling back down. The oscilla- tions continue to decay as both state variables converge to zero.  In this case, both state variables converge to zero. However, this is not the  only possible equilibrium point, as we will show later.  It is also interesting to plot the pendulum energy,  , as in Figure 20.7. Re- call from Eq.  20.17  that the energy should never increase; this is consis- tent with Figure 20.7. Eq.  20.17  also predicts that the derivative of the  energy curve should only be zero when the velocity,  , is zero. This is also  verified if we compare Figure 20.7 with Figure 20.6. At those times where  the    graph crosses the zero axis, the slope of the energy curve is zero.   a2  V  a2  a2  2 x  0  2  1  -1  -2  20-10   Pendulum Example  a1  a2  2  1  0  -1  -2  0  160  120  80  40  0  0  20-11  V t   10  30  40  20 t  Figure 20.6  State Variables   a1    blue  and   a2   vs. Time  Notice that, although there are points where the derivative of the energy  curve is zero, the derivative does not remain zero until the energy is also  zero. This observation will lead to LaSalle’s Invariance Theorem, which we  will discuss in the next section. The key idea of that theorem is to identify  those points where the derivative of the Lyapunov function is zero, and  then to determine if the system will be trapped at those points.  Those plac- es where a trajectory can be trapped are called invariant sets.  If the only  point that can trap the trajectory, and that has zero derivative, is the ori- gin, then the origin is asymptotically stable.  10  30  40  20 t  20  Figure 20.7  Pendulum Lyapunov Function  Energy  vs. Time  The particular pendulum behavior shown in the graphs in this section de- pends on the initial conditions of the two state variables. The choice of a dif- ferent set of initial conditions may give results entirely different from those  shown in these plots. We will expand on this in the next section.   Lyapunov Function  Definitions Definition 5: Lyapunov Function  20 Stability  To experiment with the pendulum, use the Neural Network Design Demon- stration Dynamic System  nnd17ds .  LaSalle’s Invariance Theorem  The pendulum example demonstrated a problem with Lyapunov’s theorem.  We found a Lyapunov function whose derivative was only negative  semidefinite  not negative definite , and yet we know that the origin is as- ymptotically stable for the pendulum system. In this section we will intro- duce a theorem that clarifies this uncertainty in Lyapunov’s theorem. It  does so by defining those regions of the state space where the derivative of  the Lyapunov function is zero, and then identifying those parts of that re- gion that can trap the trajectory.   Before we discuss LaSalle’s Invariance Theorem, we first need to introduce  the following definitions.   be a continuously differentiable function from   n  to   is a Lyapunov function on  G  V    . If   G   is any    for the system   V  Let  subset of  da dt  =    n g a   , we say that   if   dV a  ---------------  dt  =    V a   Tg a    20.18   does not change sign on   G  .  This is a generalization of our previous definition of the Lyapunov function,  which we used in Theorem 1. Here we do not require that the function be  positive definite. In fact, there is no direct requirement on the function it- self  except that it be continuously differentiable . The only requirement is  on the derivative of  . The derivative cannot change sign anywhere on the  set  semidefinite or if it is positive semidefinite.  . Note that the derivative will not change sign if it is negative   G  V  We should note here that we have not yet explained how to choose the set  . We will use the following definitions and theorems to help us select the  G best    for a given system.  G  Set Z  Definition 6: Set   Z  Z  =    a: dV a  dt    0=     a in the closure of G    .   20.19   20-12   LaSalle’s Invariance Theorem  G  Here “the closure of  is a key set. It contains all of those points where the derivative of the  Lyapunov function is zero. Later we will want to determine where in this  set the system trajectory can be trapped.   ” includes the interior and the boundary of   . This   G  Invariant Set  Definition 7: Invariant Set  A set of points in  lution of  =  da dt    n g a    is invariant with respect to    if every so-  starting in that set remains in the set for all time.  da dt  g a   =    If the system gets into an invariant set, then it can’t get out.  Set L  Definition 8: Set   L  L   is defined as the largest invariant set in   Z  .  This set includes all possible points at which the solution might converge.  The Lyapunov function does not change in    because its derivative is ze- L   because it is an invariant set .  ro , and the trajectory will be trapped in  L Now, if this set has only one stable point, then that point is asymptotically  stable. This is, in essence, what LaSalle’s theorem will say.  Theorem LaSalle’s Invariance Theorem extends the Lyapunov Stability Theorem.  We will use it to design Hopfield networks in the next chapter. The theorem  proceeds as follows [Lasa67].  Theorem 2: LaSalle’s Invariance Theorem  V   is a Lyapunov function on  0  If    a t  that remains in   is a  basin of attraction for  , which has all of the stable points.  If all trajecto- L ries are bounded, then  a t   , then each solution  G  da dt G  approaches   g a  = L   for all       = L     L   for    as    as   .    G  .  t  t  t    If a trajectory stays in  , or it will go to  infinity. If all trajectories are bounded, then all trajectories will converge  to   , then it will either converge to   G  L  L  .  20  There is a corollary to LaSalle’s theorem that we will use extensively. It in- volves choosing the set    in a special way.  G  20-13   20 Stability  Corollary 1: LaSalle’s Corollary  Let   G   be a component  one connected subset  of    =    a:V a      .   20.20   Assume that  L its region of attraction.  closure L G  G  =     is bounded,    dV a  dt   be a subset of  G  0 . Then    on the set   G  , and let the set   L   is an attractor, and   G   is in   LaSalle’s theorem, and its corollary, are very powerful. Not only can they  tell us which points are stable    , but they can also provide us with a par- tial region of attraction    is constructed differently in the  corollary than in the theorem.    .  Note that   L  L  G  To clarify LaSalle’s Invariance Theorem, let’s return to the pendulum ex- ample we discussed earlier.  Example Let’s apply Corollary 1 to the pendulum example. The first step in using the  corollary will be to choose the set  . This set will then be used to select  the set     a component of      .   G    For this example we will use the value  set of points where the energy is less than or equal to   , therefore  100 . 100  100    =   will be the   100  =    a:V a     100     20.21   This set is displayed in blue in Figure 20.8.  2 2+  a2  2  1  0  -1  -2  20-14  -10  -5  5  10  0 a1  Figure 20.8  Illustration of the Set   100  The next step in our analysis is to choose a component  connected subset   of  . Since we have been investigating the stability of the    for the set   G  100   LaSalle’s Invariance Theorem  origin, let’s choose the component of  ing set is shown in Figure 20.9.  100   that contains   a  0=  . The result-  a2  2  1  0  -1  -2  -10  -5  5  10  0 a1  Figure 20.9  Illustration of the Set   G  Now that we have chosen  Lyapunov function is less than or equal to zero on  know that  less than or equal to zero on   . From Eq.  20.17  we   is negative semidefinite. Therefore it will certainly be   , we need to check that the derivative of the   dV a  dt  G  G  G  .    We are now ready to determine the attractor set  L  , which is the largest invariant set in   Z  .  L  . We begin with the set   Z  =    a: dV a  dt     0=   a in the closure of G    =    a: a2  =  0, a in the closure of G   .  This can also be written as  Z  =    a:a2  =  0,  1.6–    a1    1.6    .   20.22    20.23   We know from Eq.  20.17  that the derivative of  velocity is zero, which corresponds to the  the segment of the  Figure 20.10.  a1  axis that falls within   a1  V a    axis. Therefore  G  . The set    is only zero when the   consists of   is displayed in   Z  Z  L  Z  1.6  . To find    is the largest invariant set in    we need to answer the     radians, with zero initial velocity, will the velocity of the pendulum   The set  question: If we start the pendulum from an initial position between  and  remain zero? Clearly the only such initial condition would be   straight down . If we start the pendulum from any other position in  , the  pendulum will start to fall, so the velocity will not remain zero and the tra- jectory will move out of   consists only of the origin:  . Therefore, the set    radians   1.6–  Z  L  0  L  Z  L  =    a: a  0=    .   20.24   20-15  20   Z  Z  20 Stability  a2  2  1  0  -1  -2  a2  2  1  0  -1  -2  20-16  -10  -5  5  10  0 a1  Figure 20.10  Illustration of the Set   Z   is the closure of the intersection of  The set  L is simply  : L  L   and   G  , which in this case   L  =  closure L G    =  L  =    a: a  0=    .   20.25   Therefore, based on LaSalle’s corollary,  stable point  and  jectory that starts in    is an attractor  asymptotically   is in its region of attraction. This means that any tra-   will decay to the origin.  L  G  G  2 2+  Now suppose that we had taken a bigger region for     , such as  300  =    a: V a       300      .   20.26   This set is shown in gray in Figure 20.11.  -10  -5  5  10  0 a1  Figure 20.11  Illustration of   G  =  300    Gray  and   Z  We let   G  =  300  , since   300   has only one component. The set   Z   is given by    LaSalle’s Invariance Theorem  Z  =    a: a2  0=    ,   20.27   which is shown by the blue bar on the horizontal axis of Figure 20.11. Thus,  it follows that   L  =  L  =    a: a1  =  n, a2  0=    .   20.28   This is because there are now several different positions within the set    where we can place the pendulum, without causing the velocity to become  nonzero. The pendulum can be pointing directly up or directly down. This  corresponds to the positions  . If we place the pendu- lum in any of these positions, with zero velocity, then the pendulum will re- main stationary. We can show this by setting the derivatives equal to zero  in Eq.  20.14  and Eq.  20.15 .   for any integer   n  Z  n  da1 td  =  a2  =  0  ,   20.29   da2 td     =  sin–  a1    –  0.2a2  =  sin–  a1    =      a1  n=     20.30    0   =  G  300  For this choice of   we can say very little about where the trajecto- ry will converge. We tried to increase the size of our known region of attrac- tion for the origin, but this  G equilibrium points. We made  blue dots in Figure 20.12.   is a region of attraction for all of the  G   is illustrated by the    too large. The set   L  a2  2  1  0  -1  -2  20-17  -10  -5  5  10  0 a1  Figure 20.12  The Set   L  We cannot tell which of the equilibrium points  blue dots  will attract the  trajectory. All we can say is that if we start somewhere in  , one of the  equilibrium points will attract the system solution, but we cannot say for  sure which one it will be. Consider, for instance, the trajectory shown in   300  20   20 Stability  Figure 20.13. This shows the pendulum response for an initial position of 2  radians and an initial velocity of 1.5 radians per second. This time the pen- dulum had enough velocity to go over the top, and it converged to the equi- librium point at    radians.  2  Now that we have discussed LaSalle’s Invariance Theorem, you might want  to experiment some more with the pendulum, in order to investigate the re- gions of attraction for the various stable points. To experiment with the pen- dulum, use the Neural Network Design Demonstration Dynamic System   nnd17ds .  Contour Plot  -10  -5  5  10  0 a1 x1  Figure 20.13  Pendulum Trajectory for Different Starting Conditions  Comments The keys to LaSalle’s theorem are the choices of the Lyapunov function  and the set  dicate the region of attraction. However, we want to choose  set     V  to be as large as possible, because that will in-  so that the   , which will contain the attractor set, is as small as possible.  . We want   G  G  V  Z  For instance, we could try  space  erywhere. However, it gives us no information since   . This is a Lyapunov function for the entire  , since its derivative is zero  and therefore doesn’t change sign  ev-  Z n  0=  n  V  =  .    Z  =  =  V  G  Z1  V2  V1  . If   Z2   and   , and   V1 V2+   are both Lyapunov functions on    have the same sign, then  Z  Notice that if    and   is also a Lyapunov func- dV2 dt tion, where   is a  Z1  is always at least as  “better” Lyapunov function than either    good as either  Z1 and  . Therefore, if you have found two different Lyapunov functions and  their derivatives have the same sign, then add them together and you may  have a better function. The best Lyapunov function for a given system is  the one that has the smallest attractor set and the largest region of attrac- tion.   is smaller than both  V2 V   can never be larger than the smaller of   dV1 dt  , since   , then    and    or    or  V1  V2  V1  Z2  Z2  .   Z  Z    2a2  x  0  2  1  -1  -2  20-18   Summary of Results  Summary of Results  Stability Concepts  Definitions Definition 1: Stability  in the sense of Lyapunov   The origin is a stable equilibrium point if for any given value  exists a number  a t    there  , then the resulting motion    such that if    satisfies   a 0    for       a t   0  0  0        .  t  Definition 2: Asymptotic Stability  The origin is an asymptotically stable equilibrium point if there exists a  number  a 0    such that whenever  t   the resulting motion satisfies   0  as    0    a 0     .  Definition 3: Positive Definite  A scalar function  a  0  .  Definition 4: Positive Semidefinite  V a    is positive definite if   V 0   0=   and   V a   0   for   A scalar function   V a    is positive semidefinite if   V a   0   for all   a  .  Lyapunov Stability Theorem  Consider the autonomous  unforced, no explicit time dependence  system  da td  =  g a   .  The Lyapunov stability theorem can then be stated as follows.  Theorem 1: Lyapunov Stability Theorem  20  If a positive definite function  V a  ative semidefinite, then the origin   itive definite function  definite, then the origin   called a Lyapunov function of the system.  V a  a  0=   can be found such that  a   is neg-   is stable for this system. If a pos-  dV a  dt     can be found such that  0=    is asymptotically stable. In each case,   dV a  dt   is negative   V     is   20-19   20 Stability  LaSalle’s Invariance Theorem  Definitions Definition 5: Lyapunov Function   be a continuously differentiable function from   n  to   is a Lyapunov function on  G  V    . If   G   is any    for the system   V  Let  subset of  da dt  =    n g a   , we say that   if   dV a  ---------------  dt  =    V a   Tg a   does not change sign on   G  .  Definition 6: Set   Z  Z  =    a: dV a  dt    0=     a in the closure of G    .   20.31   Definition 7: Invariant Set  A set of points  solution of    G da dt   in  =  n g a    is invariant with respect to  g a  da dt  for all time.  starting in   remains in  G  G  =     if every   Definition 8: Set   L  L   is defined as the largest invariant set in   Z  .  Theorem  Theorem 2: LaSalle’s Invariance Theorem  V   is a Lyapunov function on  0  If    a t  that remains in   is a  , which has all of the stable points.  If all trajecto- basin of attraction for  L ries are bounded, then  a t   , then each solution  G  da dt G  approaches   g a  = L   for all       = L     L   for    as    as   .    G  .  t  t  t    Corollary 1: LaSalle’s Corollary  Let   G   be a component  one connected subset  of    =    a:V a      .   20.32   G  Assume that  L its region of attraction.  closure L G  =     is bounded,    dV a  dt   be a subset of  G  0 . Then    on the set   G  , and let the set   L   is an attractor, and   G   is in   20-20   Solved Problems  Solved Problems  P20.1 Test the stability of the origin for the following system.  da1 dt    =  a1–  +  a2  2  da2 dt    –=  a2 a1    1+    The basic job here is to find a Lyapunov   that is positive definite and  has a derivative that is negative semidefinite or, better yet, negative defi- nite.  The latter is a stronger condition.   V a   Let us try   V a   =  a1  2  +  a2  2  . The derivative of   V a    is  dV a  ---------------  dt  =  V  T da   ------   dt  =  V da1  --------  a1 dt     +  V da2  --------  a2 dt     ,  or  dV a  ---------------  dt  =  2a1    a1–  +  a2  2    +  2a2  –  a2 a1    1+      =  –  2 a1  2  –  2 a2  2  .  The derivative  ymptotically stable.  dV a  dt     is negative definite. Therefore, the origin is as-  P20.2 Test the stability of the origin for the following system.  da1 dt    –=  a1  5  –=  5 a2  7    da2 dt 2  Let us try   V a   =  a1  2  +  a2  . Then we have  dV a  ---------------  dt  =  2a1  –  a1  5    +  2a2  –  5 a2  7    =  –  2 a1  6  –  10 a2  8  .  Here again,   ymptotically stable.  dV a  dt   is negative definite, and therefore the origin is as-  20  P20.3 Consider the mechanical system shown in Figure P20.1. This is a  spring-mass-damper system, with a nonlinear spring. We will de- fine   . Then the equations of motion are   and   dx dt  x=  =    a2  a1  da1 dt    a2=  ,  20-21   20 Stability  da2 dt    =  –  a1  3  –  a2       nonlinear spring   .  Consider the candidate Lyapunov function  V a   =  1 --- a1 4  4  +  1 --- a2 2  2  .  Use the corollary to LaSalle’s invariance theorem to provide as  much information as possible about the equilibrium points and ba- sins of attraction.  x   cid:0  cid:0  cid:0   Figure P20.1  Mechanical System  First calculate the derivative of   V a    as  dV a  ---------------  dt  =  V da1  --------  a1 dt     +  V da2  --------  a2 dt     =  a1  3a2  +  a2    –  a1    a2–    =  3  –  a2  2  .  Thus,   dV dt     does not change sign on   2  .  Now let us define   G  =    =    a: V a      and consider the case for  P20.2. The set   . A contour plot of   is indicated in blue on the plot.  1=    1  V a    is shown in Figure   a2  0  4  2  -2  -4 -4  20-22  -2  2  4  0 a1  Figure P20.2  Contour Plot of   V a    and   1   Solved Problems  Now we need to determine the set   Z  .  Z  =    a: dV dt    =  0, a    in the closure of G    =    a: a2  0= , a  in the closure of G    or  Next we find the set   L  . Since   a  0=   is the only invariant set,   Z  =    a: a2  =  0,    2–    a1    2    L  =    a:   a1  =  0,  a2  0=    .  Therefore, the origin,  is an attractor and    is in its region of attraction.  1  Further, we can increase  traction for the origin.     to show that the entire   2   is the basin of at-  2   and an initial velocity of   Figure P20.3 shows the response of the spring-mass-damper from an initial  position of  . Note that the trajectory is parallel  to the contour lines when the trajectory crosses the  with our earlier result, which showed that the derivative of the Lyapunov  function was zero whenever   axis is not an in- variant set  except for the origin ; therefore the trajectory is only attracted  to the origin.  . Fortunately, the    axis. This agrees   0=  a2  a2  a2  2  a  =  ,  0 0  Contour Plot  a2  2 x  0  4  2  -2  -4 -4  20-23  20  -2  2  4  0 x1 a1  Figure P20.3  Spring-Mass-Damper Response   20 Stability  P20.4 Consider the following nonlinear system:  da1 dt    =  a1 a1    2  +  a2  2  4–    a2–  da2 dt    =  a1  +  a2 a1    2  +  a2  2  4–    .  This system has two invariant sets, the origin  and the circle  Assuming the candidate Lyapunov function    a:  a  =  0      ,    a:  a1  2  +  a2  2  4 =    .  V a   =  a1  2  +  a2  2  ,  use LaSalle’s Invariance Theorem to find out as much as you can  about the region of attraction for the origin.  Our job, then, is to determine whether or not the given invariant sets rep- resent a stable point or a stable trajectory. Let’s first take a look at  .  We recall that   dV dt    dV a  ---------------  dt  =  V da1  --------  a1 dt     +  V da2  --------  a2 dt     ,   and substitute for the various terms to give  dV a  ---------------  dt  =  2a1 a1 a1      2  +  a2  2 4–    a2–    +  2a2 a1    a2+    a1  2  +  a2  2  4–      .  This can be simplified to   dV a  ---------------  dt  =  2 a1   2  +  a2  2   a1   2  +  a2  2  4–    .  Thus,   dV dt     is zero at   a  0=   and on the circle   a1  2  +  a2  2  4=  .  G  2  , a region of attraction. Is there a change of sign of  ? Yes, there is. As we go from outside the circle of radius     We now pick   to its  over all   is  interior, the sign of   negative semidefinite inside the circle   in- . Let’s pick a  side this circle, so that the circle will not be included. The following set will  do.   changes from positive to negative. So   dV dt 2  dV dt G  dV dt  a1  a2  4=  2  2  +      20-24   Now we consider  only point inside   1 1  . There are just two places that   is   . Therefore,  0=  a  dV dt    0=  , and the   Solved Problems  G  =  1  =    a: V a   1    Z  =    a:   a1  =  0, a2  0=       and  L  =  L  =  Z  .   is in its region of attraction.We can use  The origin is the attractor, and  the same arguments to show that the region of attraction for the origin in- cludes all points inside the circle   1  4=  2  2  .   +  a1  a2  Figure P20.4 displays two trajectories for this system, one that begins in- side the circle  , and one that begins outside the circle. Al- though the circle is an invariant set, it is not an attractor. The only  attractor for this system is the origin.  a1  a2  4=  2  2  +  Contour Plot  a2  2 x  0  3  2  1  -1  -2  -3 -3  20-25  -2  -1  1  2  3  0 x1 a1  Figure P20.4  Sample Trajectories for Problem P20.4  P20.5 Consider the following nonlinear system.  da t     dt    –=    a t  1–   a t    2–    i. Find any equilibrium points for this system.  ii. Use the following candidate Lyapunov function to obtain  whatever information you can about the regions of attrac- tion for the equilibrium points found in part  i .  Hint: Use  the corollary to LaSalle’s Invariance Theorem.   20   20 Stability  V a   =  2– a  2  i. To find the equilibrium points, we set   da t  dt    0=  .  0  =  –  1– a   a 2–           a  =  1, a   2=    are equilibrium points  ii. To use LaSalle’s corollary, we need to find   dV dt    .  dV ------- dt  =  V a     da   td  =  2– 2 a    –  1– a   a 2–      –=  1– 2 a   a 2–  2  Now we let   G  =    =    a:  V a      .  For example, try    0.5  =  . This gives  Note that a solution of   2–   a  0.5   yields  G  =  =    a:  a  2–  2  0.5    .  0.5 2    2– a      0.5  or  1.3     a  2.7  .  Thus,   dV dt    is negative definite on   G  .   Next we need to find the set  dV dt Only one of these falls within    is zero. There are two points where  . Therefore  G  Z    , which contains those points within   dV dt     is zero,   a  1=  G  and    where  .  a  2=  Now we need to find  point in   Z  , and it is an equilibrium point. Thus  L  , the largest invariant set in   Z  . There is only one   This means that   G   is in the region of attraction for 2.  We can use the same arguments with values of  that the region for attraction for   2=  a   must include at least     up to 1.0. So we can say   What if we consider those regions where  2, and   will change sign on  G about the region of attraction for  a the corollary to LaSalle’s Invariance Theorem.  dV dt   1    ? Then   Z   includes both 1 and   . Therefore we cannot say anything  1=  , using this Lyapunov function and   Z  =    a:   a  2=    .  L  =  L  =  Z  .    a:   1     a  3    .  20-26   Solved Problems  Figure P20.5 displays some typical responses for this system. Here we can   is actually unstable. Any initial con- see that the equilibrium point  dition above   goes to  a minus infinity.  a  converges to   . Anything less than   1= a  2=  1=  1=  a  1  3  4  2 t  Figure P20.5  Stable and Unstable Responses for Problem P20.5  20  4  3  2  1  0 0  20-27   20 Stability  Epilogue  In this chapter we have presented the concept of stability, as applied to dy- namic systems. For nonlinear dynamic systems, like recurrent neural net- works, we do not talk about the stability of the system. Rather, we discuss  the stability of certain system trajectories and, in particular, equilibrium  points.  There were two main stability theorems discussed in this chapter. The first  is the Lyapunov Stability Theorem, which introduces the concept of gener- alized energy — the Lyapunov function. The concept behind this theorem  is that if a system’s “energy” is always decreasing, then it will eventually  stabilize at a point of minimum “energy.”  The second theorem presented was LaSalle’s Invariance Theorem, which is  an enhancement of the Lyapunov Stability Theorem. There are two key im- provements made by LaSalle. The first is a clarification of the cases in  which the Lyapunov function does not decrease throughout the state space,  but stays constant in some regions. LaSalle’s theorem introduced the con- cept of an invariant set to identify those regions that can trap the system  trajectory. The second improvement made by LaSalle’s theorem is that, in  addition to indicating the stability of equilibrium points, it also gave infor- mation about the regions of attraction of each stable point.  The ideas presented in this chapter are important tools for the analysis of  recurrent neural networks, like the Grossberg networks of Chapters 18 and  19.  See [CoGr83] for an application of LaSalle’s Invariance Theorem to re- current neural networks.  In Chapter 21 we will use LaSalle’s theorem to  explain the operation of the Hopfield network.  20-28   Further Reading  Further Reading  [Brog91]   W. L. Brogan, Modern Control Theory, 3rd Ed., Englewood  Cliffs, NJ: Prentice-Hall, 1991.  [CoGr83]  [Lasa67]  This is a well-written book on the subject of linear systems.  The first half of the book is devoted to linear algebra. It also  has good sections on the solution of linear differential equa- tions and the stability of linear and nonlinear systems. It  has many worked problems.  M. A. Cohen and S. Grossberg, “Absolute stability of global  pattern formation and parallel memory storage by compet- itive neural networks,” IEEE Transactions on Systems,  Man and Cybernetics, vol. 13, no. 5, pp. 815–826, 1983.  Cohen and Grossberg apply LaSalle’s Invariance Theorem  to the analysis of the stability of competitive neural net- works. The network description is very general, and the au- thors show how their analysis can be applied to many  different types of recurrent neural networks.  J. P. LaSalle, “An invariance principle in the theory of sta- bility,” in Differential Equations and Dynamic Systems, J.  K. Hale and J. P. LaSalle, eds., New York: Academic Press,  pp. 277–286, 1967.  This article provides a unified presentation of Lyapunov’s  stability theory, including several extensions. It introduces  LaSalle’s Invariance Theorem and various corollaries.   [SlLi91]  J.-J. E. Slotine and W. Li, Applied Nonlinear Control, En- glewood Cliffs, NJ: Prentice-Hall, 1991.  This text is an introduction to nonlinear control systems. A  significant portion of the book is devoted to the analysis of  nonlinear dynamic systems. A number of stability theo- rems are presented and demonstrated.  20  20-29   20 Stability  Exercises  E20.1 Use Lyapunov’s Stability Theorem to test the stability of the origin for the   following systems.  i.  ii.  da1 dt    =  –  a1  3  a2+  da2 dt    =  a1–  a2– 2  a2  a1–  +  a2–    a1  1+    da1 dt    da2 dt    =  =  E20.2 Consider the following nonlinear system:  da1 dt    =  a2 2a1 a1  –    2  +  a2  2    ,  da2 dt    =  a– 1  –  2a2 a1    2  +  a2  2    .  i. Use Lyapunov’s Stability Theorem and the candidate Lyapunov   function shown below to investigate the stability of the origin.  V a   =   a1  2  a2  +  2  » 2 + 2 ans =       4  ii. Check your stability result from part  i  by writing a MATLAB M- file to simulate the response of this system for several different ini- tial conditions. Use the ode45 routine. Plot the responses.  E20.3 Consider the following nonlinear system:  da dt    =  1+ a a    ,  i. Find any equilibrium points.  ii. The following Lyapunov function is proposed. Show that this is a  valid Lyapunov function for use in Lasalle’s invariance theorem.  V a   –=  2a3    +  3a2    .  iii. Use the corollary to Lasalle’s theorem and the proposed Lyapunov  function to provide as much information as you can about the stable  equilibrium points and their basins of attraction. Identify the sets  Z, G and L. Use graphs wherever possible.  20-30   Exercises  E20.4 Repeat E20.3 for the following systems and Lyapunov functions.  In some   cases, it may be useful to sketch the Lyapunov functions.   i.  ii.  iii.  iv.  v.  vi.  da dt  da dt  da dt  da dt  da dt  da dt              =  =  =  =  =  =  2– a   a 1+    ,   V a   1+ a a    ,   V a   –=  2+ a a    ,   V a   =  a–  1– a    ,   V a   =  2 1+ = a 3a2 2a3   + a3 3 a2– – 3a2 2a3  –  cos  a   , find a   V a   sin  a   , find a   V a   E20.5 Consider the following nonlinear system:  We want to use the corollary to Lasalle’s invariance theorem to locate at- tractors and find out as much as we can about the basins of attraction, us- ing the following Lyapunov function.  da1 dt    a2=  ,  da2 dt    =  a– 2 1  a2–  2  a1–  .  V a   =  a1  2  +  a2  2  .  i. Find any equilibrium points.  ii. Find   dV a  dt    .  iii. Choose a set G.  iv. Find the corresponding set Z.  v. Find the set L.  » 2 + 2 ans =       4  vi. What have you learned about the attractors of this system and the   basins of attraction? Can you learn more by modifying the set G?  Explain.  vii. Check your results by writing a MATLAB M-file to simulate the re- sponse of this system for several different initial conditions. Use the  ode45 routine. Plot the responses.  20  E20.6 Consider the following nonlinear system:  da1 dt    a2=  ,  20-31   20 Stability  da2 dt    =  a– 1  –  a2  3  .  i. Find any equilibrium points.  ii. Find as much information about the stability of the equilibrium   points as possible, using the corollary to LaSalle’s theorem and the  candidate Lyapunov function  » 2 + 2 ans =       4  iii. Check your results from parts  i  and  ii  by writing a MATLAB M- file to simulate the response of this system for several different ini- tial conditions. Use the ode45 routine. Plot the responses.  V a   =  a1  2  +  a2  2  .  E20.7 Consider the following nonlinear system:  da dt    =  a– 1   1 a+    =  a2–  .  1  i. Find any equilibrium points.  ii. Find a suitable Lyapunov function.  Hint: Start with a form for   dV dt     and work backward to find   V  .   iii. Sketch the Lyapunov function.  iv. Use the corollary to LaSalle’s theorem and the Lyapunov function of  part  ii  to find as much information as possible about regions of at- traction. Use graphs wherever possible.   Hint: The graph shown in Figure E20.1 may be helpful.   f  a  = 1 -  a   β  2  -β  β  Figure E20.1  Helpful Function for Exercise E20.7  20-32   Exercises  E20.8 Consider the following nonlinear system:  da1 dt    =  a2  –  a1 a1    4  +  2 a2  2  10–    ,  da2 dt    =  –  a1  3  –  3 a2  5 a1   4  +  2 a2  2  10–    .  » 2 + 2 ans =       4  i. Find any invariant sets.  You may want to simulate this system us-  ing MATLAB in order to help identify the invariant sets.   ii. Using the candidate Lyapunov function shown below and the corol- lary to LaSalle’s theorem, investigate the stability of the invariant  sets you found in part  i .  E20.9 Consider the following system:  V a   =    a1  4  +  2 a2  2  10–  2  da td  =  1– 0  0 2–  a  +  1 2  i. Find any equilibrium points.  ii. Find a Lyapunov function and identify attractors and basins of at- traction. Use the corollary to Lasalle’s theorem and carefully iden- tify and graph the sets   , G, Z and L.    E20.10 For the nonlinear system  da1 dt    =  a2  +   a1 1  –  a1  2  –  a2  2    ,  da2 dt    =  a1–  +   a2 1  –  a1  2  –  a2  2    ,  we know that the following sets are invariant:    a a  0=    ,    a a1  2  +  a2  2  1=    .  20  The following Lyapunov function is proposed:  V a   =    a1  2  +  a2  2  1–  2  .  20-33   20 Stability  Use the corollary to Lasalle’s theorem to find out as much as possible about  the basins of attraction for the two invariant sets given above, and graph  the sets   , G, Z and L.    E20.11 Consider the system  i. Find any equilibrium points.  ii. The following Lyapunov function is proposed. Show that this is a  valid Lyapunov function for use in Lasalle’s invariance theorem.  da1 dt    =  –  cos  a2    a1–  ,  da2 dt    a1=  .  V a   =  sin+  a2    2 a1 ------------ 2  iii. Use Lasalle’s theorem to find out as much information as you can  about the stable equilibrium points and their basins of attraction.   Make a rough sketch of the contour plot for    to assist you.   V a   20-34   Objectives  21 Hopfield Network  Objectives Theory and Examples  Hopfield Model Lyapunov Function Invariant Sets Example Hopfield Attractors  Effect of Gain Hopfield Design  Content-Addressable Memory Hebb Rule Lyapunov Surface  Summary of Results Solved Problems Epilogue Further Reading Exercises  21-1 21-2 21-3 21-5 21-7 21-8 21-11 21-13 21-16 21-16 21-19 21-23 21-25 21-27 21-37 21-38 21-41  Objectives  This chapter will discuss the Hopfield recurrent neural network — a net- work that was highly influential in bringing about the resurgence of neural  network research in the early 1980s. We will begin with a description of the  network, and then we will show how Lyapunov stability theory can be used  to analyze the network operation. Finally, we will demonstrate how the  network can be designed to behave as an associative memory.  This chapter brings together many topics discussed in previous chapters:  the discrete-time Hopfield network  Chapter 3 , eigenvalues and eigenvec- tors  Chapter 6 ; associative memory and the Hebb rule  Chapter 7 ; Hes- sian matrices, conditions for optimality, quadratic functions and surface  and contour plots  Chapter 8 ; steepest descent and phase plane trajecto- ries  Chapter 9 ; continuous-time recurrent networks  Chapter 18 ; and  Lyapunov’s Stability Theorem and LaSalle’s Invariance Theorem  Chapter  20 . This chapter is, in some ways, a culmination of all our previous efforts.  21  21-1   21 Hopfield Network  Theory and Examples  Much of the resurgence of interest in neural networks during the early  1980s can be attributed to the work of John Hopfield. As a well-known Cal.  Tech. physicist, Hopfield’s visibility and scientific credentials lent renewed  credibility to the neural network field, which had been tarnished by the  hype of the mid-1960s. Early in his career he studied the interaction be- tween light and solids. Later he focused on the mechanism of electron  transfer between biological molecules. One can imagine that his academic  study in physics and mathematics, combined with his later experiences in  biology, prepared him uniquely for the conception and presentation of his  neural network contribution.  Hopfield wrote two highly influential papers in 1982 [Hopf82] and 1984  [Hopf84]. Many of the ideas in these papers were based on the previous  work of other researchers, such as the neuron model of McCulloch and Pitts  [McPi43], the additive model of Grossberg [Gros67], the linear associator of  Anderson [Ande72] and Kohonen [Koho72] and the Brain-State-in-a-Box  network of Anderson, Silverstein, Ritz and Jones [AnSi77]. However,  Hopfield’s papers are very readable, and they bring together a number of  important ideas and present them with a clear mathematical analysis  in- cluding the application of Lyapunov stability theory .  There are several other reasons why Hopfield’s papers have had such an  impact. First, he identified a close analogy between his neural network and  the Ising model of magnetic materials, which is used in statistical physics.  This brought a significant amount of existing theory to bear on the analysis  of neural networks, and it encouraged many physicists, as well as other sci- entists and engineers, to turn their attention to neural network research.  Hopfield also had close contacts with VLSI chip designers, because of his  long association with AT&T Bell Laboratories. As early as 1987, Bell Labs  had successfully developed neural network chips based on the Hopfield net- work. One of the main promises of neural networks is their suitability for  parallel implementation in VLSI and optical devices. The fact that Hopfield  addressed the implementation issues of his networks distinguished him  from most previous neural network researchers.  Hopfield emphasized practicality, both in the implementation of his net- works and in the types of problems they solved. Some of the applications  that he described in his early papers include content-addressable memory   which we will discuss later in this chapter , analog-to-digital conversion  [TaHo86], and optimization [HoTa85]  as in the traveling salesman prob- lem .  In the next section we will present the Hopfield model. We will use the con- tinuous-time model from the 1984 paper [Hopf84]. Then we will apply  Lyapunov stability theory and LaSalle’s Invariance Theorem to the analy-  21-2   Hopfield Model  sis of the Hopfield model. In the final section we will demonstrate how the  Hebb rule can be used to design Hopfield networks as content-addressable  memories.  Hopfield Model  Hopfield Model  In keeping with his practical viewpoint, Hopfield presented his model as an  electrical circuit. The basic Hopfield model  see [Hopf84]  is shown in Fig- ure 21.1.  Amplifier  Inverting Output  Resistor  I1  R1,S  I2  ρ  C  n1  R2,1  n2  ρ  C  IS  nS  RS,2  ρ  C  a1  a2  aS  Figure 21.1  Hopfield Model  Each neuron is represented by an operational amplifier and its associated  resistor capacitor network. There are two sets of inputs to the neurons. The  first set, represented by the currents  , are constant external inputs.  The other set consists of feedback connections from other op-amps. For in- stance, the second output,  , which is connected, in  a2 turn, to the input of amplifier  . Resistors are, of course, only positive, but  a negative input to a neuron can be obtained by selecting the inverted out- put of a particular amplifier.  In Figure 21.1, the inverting output of the  first amplifier is connected to the input of the second amplifier through re- sistor   , is fed to resistor  S  I1 I2 ...  RS 2  .        R2 1  The equation of operation for the Hopfield model, derived using Kirchhoff’s  current law, is  C  dni t    -------------  dt  =  Ti j aj t     –  ni t    ---------- Ri  Ii+  ,  S    j  1=  21-3  21   21.1    21 Hopfield Network  ni   is the input voltage to the ith amplifier,   where  of the ith amplifier,  input current to the ith amplifier. Also,   is the amplifier input capacitance and    is the output voltage   is a fixed   ai  Ii  C  Ti j  =  1 -------- Ri j  ,   1 ----- Ri  =  1 ---  S  1 -------- Ri j  +  j  1=  ,   ni  =  f 1– ai      or   ai  =  f ni     ,   21.2   f n   where  assume that the circuit is symmetric, so that    is the amplifier characteristic. Here and in what follows we will   .  =  Ti j  Tj i  The amplifier transfer function,  Both this sigmoid function and its inverse are assumed to be increasing  functions. We will provide a specific example of a suitable transfer function  later in this chapter.  , is ordinarily a sigmoid function.   f ni  ai  =    If we multiply both sides of Eq.  21.1  by   Ri  , we obtain  RiC  dni t    -------------  dt  =  S    j  1=  RiTi j aj t     –  ni t     +  RiIi  .   21.3   This can be transformed into our standard neural network notation if we  define    =  RiC  ,   wi j  =  RiTi j   and   bi  =  RiIi  .   21.4   Now Eq.  21.3  can be rewritten as  dni t    -------------    dt  S    j  1=  =  –  ni t     +  wi j aj t     +  .  bi   21.5   In vector form we have  and  dn t    ------------ dt  =  –  n t     +  Wa t    b  +  .  a t     =   f n t       .   21.6    21.7   The resulting Hopfield network is displayed in Figure 21.2.  Thus, Hopfield’s original network of   operational amplifier circuits can be  represented conveniently in our standard network notation. Note that the  input vector  Hopfield network is used for associative memory networks, as will be dis- cussed at the end of this chapter.   determines the initial network output. This form of the   p  S  21-4   Lyapunov Function  Input  Recurrent Layer  W S x S   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   b S x 1  1  p S x 1  S  . n   cid:0 1 ε  cid:0   - +  cid:0   cid:0   f -1  cid:0    cid:0  cid:0   cid:0  cid:0  n  cid:0  cid:0   f   S x 1  a  S x 1  S  n 0  = f -1 p ,   a 0  = p      ε dn dt = - n + W f n  + b  Figure 21.2  Hopfield Network  Lyapunov Function  The application of Lyapunov stability theory to the analysis of recurrent  networks was one of the key contributions of Hopfield.  Cohen and Gross- berg also used Lyapunov theory for the analysis of competitive networks at  about the same time [CoGr83].  In this section we will demonstrate how  LaSalle’s Invariance Theorem, which was presented in Chapter 20, can be  used with the Hopfield network. The first step in using LaSalle’s theorem  is to choose a Lyapunov function. Hopfield suggested the following func- tion:  V a   =  –  1 ---aTWa 2  +  S    i  1=  ai   0              f 1– u  ud  bTa  .  –   21.8   Hopfield’s choice of this particular Lyapunov candidate is one of his key  contributions. Notice that the first and third terms make up a quadratic  function. In a later section of this chapter we will use our previous results  on quadratic functions to help develop some insight into this Lyapunov  function.  To use LaSalle’s theorem, we will need to evaluate the derivative of  For clarity, we will consider each of the three terms of  ing Eq.  8.37 , the derivative of the first term is  .   separately. Us-  V a   V a   21  21-5   21 Hopfield Network  d td      –  1 ---aTWa 2      1 --- 2  =  –      aTWa    =  –    Wa  Tda ------ dt  Tda ------ dt  =  –  aTWda ------ dt  .   21.9   The second term in  of these integrals, we find  V a    consists of a sum of integrals. If we consider one   d td  ai   0              d aid  ai   0        f 1– u  ud  =  f 1– u  ud  dai td  =  f 1– ai    dai td  =  ni  dai td  .   21.10   The total derivative of the second term in   V a    is then              d ----- dt  S    i  1=  ai   0        f 1– u  ud  =  nTda ------ dt  .  Using Eq.  8.36 , we can find the derivative of the third term in   V a   .  –  bTa    =  –      bTa    d td  Tda ------ dt  =  –  bTda ------ dt  Therefore, the total derivative of   V a    is  d td  V a   =  –  aTWda ------ dt  +  nTda ------ bTda ------ dt dt  –  =  aTW–    +  nT bT–   21.13   da ------ dt  .  From Eq.  21.6  we know that   aTW–    nT bT–    +  =  –  dn t    ------------  dt  T  .  This allows us to rewrite Eq.  21.13  as  d td  V a   =  –  dn t    ------------  dt  Tda ------ dt  =  –  dni   td        dai   td  .  S    i  1=  Since   ni  =  f 1– ai    , we can expand the derivative of    as follows:  ni  dni td  =  d td    f 1– ai      =    f 1– ai      d aid  dai td  .  Now Eq.  21.15  can be rewritten  21-6   21.11    21.12    21.14    21.15    21.16    Lyapunov Function  d td  V a   =  –  dni   td     dai   td     =  –  S    i  1=  S    i  1=     d aid    f 1– ai           dai   td  2  .   21.17   f 1– ai If we assume that   erational amplifier, then   is an increasing function, as it would be for an op-  From Eq.  21.17 , this implies that  d aid    f 1– ai      0  .  d td  V a   0  .   21.18    21.19   Thus, if  nite function. Therefore,   f 1– ai     is an increasing function,   dV a  dt     is a negative semidefi-  V a    is a valid Lyapunov function.  Invariant Sets Now we want to apply LaSalle’s Invariance Theorem to determine equilib- rium points for the Hopfield network. The first step is to find the set    Eq.   20.19  .  Z  Z  =    a: dV a  dt     0=   a in the closure of G     21.20   This set includes all points at which the derivative of the Lyapunov func- tion is zero. For now, let’s assume that    is all of   S  G  .  We can see from Eq.  21.17  that such derivatives will be zero if the deriv- atives of all of the neuron outputs are zero.  da ------ dt  0=   21.21   However, when the derivatives of the outputs are zero, the circuit is at  equilibrium. Thus, those points where the system “energy” is not changing  are also points where the circuit is at equilibrium.  This means that the set  to   Z  .  L  , the largest invariant set in   Z  , is exactly equal   L  Z=   21.22   Thus, all points in   Z   are potential attractors.  Some of these features will be illustrated in the following example.  21  21-7   21 Hopfield Network  2 2+  Example Consider the following example from Hopfield’s original paper [Hopf84].  We will examine a system having an amplifier characteristic    21.23    21.24    21.26    21.27    21.28   We can also write this expression as  a  =  f n   =  2 ---tan 1–     n  ---------  2  .  n  =  2 ------tan     ---a  2  .  Assume two amplifiers, with the output of each connected to the input of  the other through a unit resistor, so that  R1 2  =  R2 1  =  1   and   T1 2  =  T2 1  =  1  .   21.25   Thus we have a weight matrix   If the amplifier input capacitance is also set to 1, we have  Let us also take     =  1.4   and   I1  =  I2  =  0  . Therefore  W  =  0 1 1 0  .    =  RiC  =  1  .  b  =  .  0 0  Recall from Eq.  21.8  that the Lyapunov function is  V a   =  –  1 ---aTWa 2  +  S    i  1=        ai   0        f 1– u  ud  bTa  .  –   21.29   The first term of the Lyapunov function, for this example, is  –  1 ---aTWa 2  =  –  1 --- a1 a2 2  0 1 1 0  a1 a2  =  –  a1a2  .   21.30   21-8   Lyapunov Function  The third term is zero, because the biases are zero. The ith part of the sec- ond term is  ai   0  f 1– u  ud  =  2 ------  ai   0  tan     ---u  2  ud  =  –  log  cos  2 ------     ---u  2  2 ---  ai  0  .   21.31   This expression can be simplified to  ai   0  f 1– u  ud  –=  4 --------log 2  cos       ---ai  2  .   21.32   Finally, substituting all three terms into Eq.  21.29 , we have our  Lyapunov function:  V a   =  –  a1a2  –  4  ------------- log 1.42      cos       ---a1  2      +  log  cos           ---a2  2      .   21.33   Now let’s write out the network equation  Eq.  21.6  . With  b  , it is  0=    1=   and   dn ------ dt  =  n– Wf n  +    =  n– Wa  +  .   21.34   If we substitute the weight matrix of Eq.  21.26 , this expression can be  written as the following pair of equations:  The neuron outputs are  dn1 dt    a2  n1–  dn2 dt    a1  n2–  =  =  ,  .  a1  =  2  ---tan 1– 1.4  -----------n1      2  a2  =  2  ---tan 1– 1.4  -----------n2      2  ,  .   21.35    21.36    21.37    21.38   Now that we have found expressions for the system Lyapunov function and  the network equation of operation, let’s investigate the network behavior.  The Lyapunov function contour and a sample trajectory are shown in Fig- ure 21.3.  21  21-9   21 Hopfield Network  2  1  0  1  1  0.5  a2  0  -0.5  -1 -1  -0.5  0.5  1  0  a1  0.5  0  a2  -0.5  1  0.5  -0.5  0  a1  -1  -1  Figure 21.3  Hopfield Example Lyapunov Function and Trajectory  The contour lines in this figure represent constant values of the Lyapunov  function. The system has two attractors, one in the lower left and one in the  upper right of Figure 21.3. Starting from the upper left, the system con- verges, as shown by the blue line, to the stable point at the lower left.  Figure 21.4 displays the time response of the two neuron outputs.  2  4  6  8  10  t  Figure 21.4  Hopfield Example Time Response  Figure 21.5 displays the time response of the Lyapunov function. As ex- pected, it decreases continuously as the equilibrium point is approached.  The system also has an equilibrium point at the origin. If the network is  initialized anywhere on a diagonal line drawn from the upper-left corner to  the lower-right corner, the solution converges to the origin. Any initial con- ditions that do not fall on this line, however, will converge to one of the so- lutions in the lower-left or upper-right corner. The solution at the origin is  a saddle point of the Lyapunov function, not a local minimum. We will dis-  0.5  1  0  -0.5  -1 0  a2  a1  21-10   Lyapunov Function  cuss this problem in a later section. Figure 21.6 displays a trajectory that  converges to the saddle point.  2  4  6  8  10  t  Figure 21.5  Lyapunov Function Response  -1 -1  -0.5  0.5  1  0  a1  Figure 21.6  Hopfield Convergence to a Saddle Point  To experiment with the Hopfield network, use the Neural Network Design  Demonstration Hopfield Network  nnd18hn .  This example has provided some insight into the Hopfield attractors. In the  next section we will analyze them more carefully.  Hopfield Attractors In the example network in the previous section we found that the Hopfield  network attractors were stationary points of the Lyapunov function. Now  we want to show that this is true in the general case. Recall from Eq.   21.21  that the potential attractors of the Hopfield network satisfy   21  V a   1  2  1.5  0.5  0  0  1  0.5  a2  0  -0.5  21-11   21 Hopfield Network  da ------ dt  0=  .  How are these points related to the minima of the Lyapunov function  ?  In Chapter 8  Eq.  8.27   we showed that the minima of a function must be  stationary points  i.e., gradient equal to zero . The stationary points of  V a    will satisfy   V a   V  =  V a1  V a2   ...   V aS  T  =  0  ,  where  V a   =  –  1 ---aTWa 2  +  S    i  1=        ai   0        f 1– u  ud  bTa  .  –   21.41   If we follow steps similar to those we used to derive Eq.  21.13 , we can find  the following expression for the gradient:    V a   =    W–  a n b–+    =  –   21.42   dn t    ------------  dt  .  The ith element of the gradient is therefore   ai  V a   =  –  =  –      f 1– ai         =  –    f 1– ai      dni td  d td  dai td  .  d aid   21.43   Notice, incidentally, that if   f 1– a    is linear, Eq.  21.43  implies that  Therefore, the response of the Hopfield network is steepest descent. Thus,  if you are in a region where   is approximately linear, the network so- lution approximates steepest descent.  f 1– a   We have assumed that the transfer function and its inverse are monotonic  increasing functions. Therefore,    21.39    21.40    21.44    21.45   da ------ dt  –=  V a   .  d aid    f 1– ai      0  .  21-12  From Eq.  21.43 , this implies that those points for which    Effect of Gain  will also be points where  da t    ------------  dt  0=  ,    V a   0=  .   21.46    21.47   Therefore, the attractors, which are members of the set   21.39 , will also be stationary points of the Lyapunov function   L   and satisfy Eq.   V a   .  Effect of Gain  The Hopfield Lyapunov function can be simplified if we consider those cas- es where the amplifier gain   is large. Recall that the nonlinear amplifier  characteristic for our previous example was     a  =  f n   =  2 ---tan 1–     n  ---------  2  .   21.48   This function is displayed in Figure 21.7 for four different gain values.    14=    =  1.4    =  0.14  -2.5  2.5  5  0 n  Figure 21.7  Inverse Tangent Amplifier Characteristic     determines the steepness of the curve at   The gain  the slope of the curve at the origin increases. As  proaches a signum  step  function.    n  0=  . As      goes to infinity,    increases,   ap-  f n   Now recall from Eq.  21.8  that the general Lyapunov function is  V a   =  –  1 ---aTWa 2  +  S    i  1=        ai   0        f 1– u  ud  bTa  .  –   21.49   21  1  0.5  a  0  -0.5  -1 -5  21-13   21 Hopfield Network  For our previous example,   f 1– u   =  2 ------  tan     u  ------  2  .   21.50   Therefore, the second term in the Lyapunov function takes the form  ai   0  f 1– u  ud  =  2 ------  2 ---  log  cos     ai  --------  2        =  –  log  cos  4 -------- 2  ai  --------  2     .   21.51   A graph of this function is shown in Figure 21.8 for three different values   increases the function flattens and is close to  of the gain. Note that as    0 most of the time. Thus, as the gain   goes to infinity, the integral in the  second term of the Lyapunov function will be close to zero in the range  – Lyapunov function then reduces to   . This allows us to eliminate that term, and the high-gain   ai      1  1      V a   =  –  1 ---aTWa 2  bTa  .  –   21.52   High-Gain Lyapunov Function    =  0.14    =  1.4  1.5  0.5  1  0    14=  -1  -0.5  0.5  1  0 a  Figure 21.8  Second Term in the Lyapunov Function  By comparing Eq.  21.52  with Eq.  8.35 , we can see that the high-gain  Lyapunov function is, in fact, a quadratic function:  V a   =  –  1 ---aTWa 2  bTa  –  =  1 ---aTAa dTa 2  +  +  c  ,   21.53   where  2  V a   =  A  =  W–  ,   d  b–=   and   c  0=  .   21.54   21-14   Effect of Gain  This is an important development, for now we can apply our results from  Chapter 8 on quadratic functions to the understanding of the operation of  Hopfield networks.  Recall that the shape of the surface of a quadratic function is determined  by the eigenvalues and eigenvectors of its Hessian matrix. The Hessian  matrix for our example Lyapunov function is  2  V a   =  W–  =  0 1–  1– 0  .   21.55   The eigenvalues of this Hessian matrix are computed as follows:  2  V a   I–  =     =  2 1–  =   1+    1–    .   21.56   – 1–  1– –  Thus, the eigenvalues are  vectors are  1  1–=   and   2  1=  . It follows that the eigen-  z1  =   and   z2  =  .  1 1–   21.57   What does the surface of the high-gain Lyapunov function look like? We  know, since the Hessian matrix has one positive and one negative eigenval- ue, that we have a saddle point condition. The surface will have a negative  curvature along the first eigenvector and a positive curvature along the  second eigenvector. The surface is shown in Figure 21.9.  The function does not have a minimum. However, the network is con- strained to the hypercube  tion. Therefore, there will be constrained minima at the two corners of the  hypercube   by the amplifier transfer func-  a:  1–  ai          1  a  =   and   a  =  .  1– 1–   21.58   When the gain is very small, there is a single minimum at the origin  see  Exercise E21.1 . As the gain is increased, two minima move out from the  origin toward the two corners given by Eq.  21.58 . Figure 21.3 displays an  intermediate case, where the gain is  . The minima in that figure oc- cur at  1.4  =    1 1  1 1  a  =   and   a  =  0.57 0.57  – –  0.57 0.57  .   21.59   21-15  21   21 Hopfield Network  In the general case, where there are more than two neurons in the network,  the high-gain minima will fall in certain corners of the hypercube   tions, after we describe the Hopfield design process.  . We will discuss the general case in more detail in later sec-  a:  1–  ai        1  0.5  1  0  -0.5  -1 -1  0.5  1  0  -0.5  -1 1  -0.5  0  0.5  1  Figure 21.9  Example High Gain Lyapunov Function  0.5  0  -0.5  0  -0.5  -1  -1  1  0.5  Hopfield Design  The Hopfield network does not have a learning law associated with it. It is  not trained, nor does it learn on its own. Instead, a design procedure based  on the Lyapunov function is used to determine the weight matrix.  Consider again the high-gain Lyapunov function  V a   =  –  1 ---aTWa 2  bTa  .  –   21.60   b   so that    and the  The Hopfield design technique is to choose the weight matrix  bias vector   takes on the form of a function that you want to  minimize. Convert whatever problem you want to solve into a quadratic  minimization problem. Since the Hopfield network will minimize  also solve the original problem. The trick, of course, is in the conversion,  which is generally not straightforward.  , it will   W  V  V  Content-Addressable Memory In this section we will describe how a Hopfield network can be designed to  work as an associative memory. The type of associative memory we will de- sign is called a content-addressable memory, because it retrieves stored  memories on the basis of part of the contents. This is in contrast to stan- dard computer memories, where items are retrieved based on their ad- dresses. For example, if we have a content-addressable data base that  contains names, addresses and phone numbers of employees, we can re- trieve a complete data entry simply by providing the employee name  or   21-16  Content-Addressable Memory   Hopfield Design  perhaps a partial name . The content-addressable memory is effectively  the same as the autoassociative memory described in Chapter 7  see page  7-10 , except that in this chapter we will use the recurrent Hopfield net- work instead of the linear associator.  Suppose that we want to store a set of prototype patterns in a Hopfield net- work. When an input pattern is presented to the network, the network  should produce the stored pattern that most closely resembles the input  pattern. The initial network output is assigned to the input pattern. The  network output should then converge to the prototype pattern closest to the  input pattern. For this to happen, the prototype patterns must be minima  of the Lyapunov function.  Let’s assume that the prototype patterns are    p1 p2   pQ          .   21.61   . As- Each of these vectors consists of  sume further that  , so that the state space is large, and that the pro- totype patterns are well distributed in this space, and so will not be close  to each other.   elements, having the values   Q S«   or   1–  1  S  In order for a Hopfield network to be able to recall the prototype patterns,  the patterns must be minima of the Lyapunov function. Since the high-gain  Lyapunov function is quadratic, we need the prototype patterns to be  con- strained  minima of an appropriate quadratic function. We propose the fol- lowing quadratic performance index:  J a   Q  –=  1 --- 2  q  1=    pq  Ta  2  .   21.62   If the elements of the vectors  imized at the prototype patterns, as we will now show.   are restricted to be   1  a  , this function is min-  Assume that the prototype patterns are orthogonal. If we evaluate the per- formance index at one of the prototype patterns, we find   J pj    =  Q  1 – --- 2  q  1=    pq  Tpj  2  =  –  1  --- pj 2    Tpj  2  =  S ---– 2  .   21.63   The second equality follows from the orthogonality of the prototype pat- terns. The last equality follows because all elements of    are   1  .  pj  Next, evaluate the performance index at a random input pattern  , which  is presumably not close to any prototype pattern. Each element in the sum  in Eq.  21.62  is an inner product between a prototype pattern and the in- put. The inner product will increase as the input moves closer to a proto-  a  21  21-17   21 Hopfield Network  type pattern. However, if the input is not close to any prototype pattern,  then all terms of the sum in Eq.  21.62  will be small. Therefore,  be largest  least negative  when  will be smallest  most negative  when  patterns.   will   is not close to any prototype pattern, and   is equal to any one of the prototype   J a   a  a  We have now found a quadratic function that accurately indicates the per- formance of the content-addressable memory. The next step is to choose the  weight matrix   will  be equivalent to the quadratic performance index    so that the Hopfield Lyapunov function    and bias   W  V  b  J  .  If we use the supervised Hebb rule to compute the weight matrix  with tar- get patterns being the same as input patterns  as  W  =   pq  pq  T  ,  Q  q  1=  b  0=  ,  and set the bias to zero  then the Lyapunov function is   21.64    21.65   V a   =  –  1 ---aTWa 2  =  –  1 ---aT 2   pq  pq  T a  =  –  aTpq  pq  Ta  .   21.66   Q    1 --- 2  q  1=  This can be rewritten  V a   =    pq  Ta  2  =  J a   .   21.67   Q  q  1=  1 ---– 2  Q    q  1=  Therefore, the Lyapunov function is indeed equal to the quadratic perfor- mance index for the content-addressable memory problem. The Hopfield  network output will tend to converge to the stored prototype patterns   among other possible equilibrium points, as we will discuss later .   As noted in Chapter 7, the supervised Hebb rule does not work well if there  is significant correlation between the prototype patterns. In that case the  pseudoinverse technique has been suggested. Another design technique,  which is beyond the scope of this text, is given in [LiMi89].  In the best situation, where the prototype patterns are orthogonal, every  prototype pattern will be an equilibrium point of the network. However,  there will be many other equilibrium points as well. The network may well  converge to a pattern that is not one of the prototype patterns. A general  rule is that, when using the Hebb rule, the number of stored patterns can   21-18   Hopfield Design  be no more than 15% of the number of neurons.The reference [LiMi89] dis- cusses more complex design procedures, which minimize the number of  spurious equilibrium points.  In the next section we will analyze the location of the equilibrium points  more closely.  Hebb Rule Let’s take a closer look at the operation of the Hopfield network when the  Hebb rule is used to compute the weight matrix and the prototype patterns  are orthogonal.  The following analysis follows the discussion in the Chap- ter 7, Problem P7.5.  The supervised Hebb rule is given by  Q  q  1=  pj  Q    q  1=  W  =   pq  pq  T  .   21.68   If we apply the prototype vector    to the network, then  Wpj  =  pq pq  Tpj  =  pj pj    Tpj  =  Spj  ,   21.69   where the second equality holds because the prototype patterns are orthog- onal, and the third equality holds because each element of   or  1–  . Eq.  21.69  is of the form   is either   pj  1  Wpj  =  pj  .   21.70   Therefore, each prototype vector is an eigenvector of the weight matrix and  they have a common eigenvalue of   for the eigen- value   . The eigenspace    is therefore  S=  S=      X  X  =  span p1, p2,  ... ,pQ      .   21.71   This space contains all vectors that can be written as linear combinations  of the prototype vectors. That is, any vector   that is a linear combination  of the prototype vectors is an eigenvector.  a  Wa W 1p1 2p2  =  +    +   QpQ  +      1Wp1 2Wp2  +  +   QWpQ  +      1Sp1 2Sp2  +  +   QSpQ  +    =  =  =  21-19  S 1p1 2p2   +  +   QpQ  +    =  Sa   21.72   21   21 Hopfield Network  The eigenspace for the eigenvalue  S= the prototype vectors are independent .     is Q-dimensional  assuming that   The entire space   RS   can be divided into two disjoint sets [Brog85],  RS  X=  X  ,   21.73   X   is the orthogonal complement of   where  not just the one we are considering here.  Every vector in  to every vector in   . This means that for any vector   X X ,  X  X  a  .  This is true for any set   ,   is orthogonal   X  pq  Ta  =  0,  q  =  1 2   Q       .   21.74   Therefore, if   a X  ,  Wa  =   pq  pq  Ta  =    pq 0    =  0  =  0 a  .   21.75   Q  q  1=  Q    q  1=  So   X   defines an eigenspace for the repeated eigenvalue     0=  .  To summarize, the weight matrix has two eigenvalues,  eigenspace for the eigenvalue  tors. The eigenspace for the eigenvalue  the space spanned by the prototype vectors.   is the space spanned by the prototype vec-  is the orthogonal complement of   . The    and   0  0  S  S  Since  from Eq.  21.54   the Hessian matrix for the high-gain Lyapunov  function    is  V  V2  W–=  ,   21.76   the eigenvalues for   V2   will be   S–   and   0  .   The high-gain Lyapunov function is a quadratic function. Therefore, the  eigenvalues of the Hessian matrix determine its shape. Because the first  eigenvalue is negative,  . Because the  .  second eigenvalue is zero,    will have negative curvature in  X X V   will have zero curvature in   V  What do these results say about the response of the Hopfield network? Be- cause  , the trajectories of the Hopfield net- work will tend to fall into the corners of the hypercube   that  are contained in    has negative curvature in   a:  1–  ai      V  X  X      1  .  Note that if we compute the weight matrix using the Hebb rule, there will  be at least two minima of the Lyapunov function for each prototype vector.  If   will also be in the space spanned by  the prototype vectors,  . Therefore, the negative of each prototype vector  will be one of the corners of the hypercube    is a prototype vector, then    that are con-  a:  1–  p– q  pq      X      1  ai  21-20   Hopfield Design  Spurious Patterns  tained in  function that do not correspond to prototype patterns.   . There will also be a number of other minima of the Lyapunov   X    a:  1–   are in the corners of the hypercube  X   that  The minima of  V are contained in  . These corners will include the prototype patterns, but  they will also include some linear combinations of the prototype patterns.  Those minima that are not prototype patterns are often referred to as spu- rious patterns. The objective of Hopfield network design is to minimize the  number of spurious patterns and to make the basins of attraction for each  of the prototype patterns as large as possible. A design method that is guar- anteed to minimize the number of spurious patterns is described in  [LiMi89].  ai        1  2 2+  To illustrate these principles, consider again the second-order example we  have been discussing, where the connection matrix is  Suppose that this had been designed using the Hebb rule with one proto- type pattern  obviously not an interesting practical case :  W  =  0 1 1 0  .  p1  =  .  1 1  Then  Notice that   W p1 p1  =  T  =  1 1  =  1 1  1 1 1 1  .  W' W I–  =  =  0 1 1 0   21.77    21.78    21.79    21.80   corresponds to our original connection matrix. More about this in the next  section.  The high-gain Lyapunov function is  V a   =  –  1 ---aTWa 2  =  –  1 ---aT 1 1 2 1 1  a  .   21.81   21  The Hessian matrix for   V a    is  21-21   21 Hopfield Network  2  V a   =  W–  =  1– 1–  1– 1–  .   21.82   Its eigenvalues are  1  =  S–  =  2–  , and   2  0=  ,   21.83   and the corresponding eigenvectors are  The first eigenvector, corresponding to the eigenvalue  space spanned by the prototype vector:  S–  , represents the   The second eigenvector, corresponding to the eigenvalue  orthogonal complement of the first eigenvector:  0  , represents the   The Lyapunov function is displayed in Figure 21.10.  This surface has a straight ridge from the upper-left to the lower-right cor- ner. This represents the zero curvature region of  . Initial conditions to  the left or to the right of the ridge will converge to the points  X  z1  =   and   z2  =  1 1  .  1 1–  X  =    a: a1  a2=    .  X  =    a: a1  =  a– 2    .  a  =   or   a  =  1 1  ,  1– 1–   21.84    21.85    21.86    21.87   respectively. Initial conditions exactly on this ridge will stabilize where  they start. This situation is the same as that for our original example  see  Figure 21.9 , except that in that case, initial points on the sloping ridge con- verged to the origin, instead of remaining where they started  see Figure  21.6 . Initial points to the right or to the left of the ridge, in both systems,  converge to the prototype design points. Thus, the convergence of our orig- inal system, and the convergence of the system with zero diagonal ele- ments, are identical in every important aspect. We will investigate this  further in the next section.  21-22   Hopfield Design  0  -0.5  -1  -1.5  -2 1  0.5  0  -0.5  0.5  1  0  -0.5  -1 -1  -0.5  0  0.5  1  -1  -1  -0.5  0  0.5  1  Figure 21.10  Example Lyapunov Function  Lyapunov Surface In many discussions of the Hopfield network the diagonal elements of the  weight matrix are set to zero. In this section we will analyze the effect of  this operation on the Lyapunov surface.  See also Chapter 7, Exercise  E7.5.   For the content-addressable memory network, all of the diagonal elements  of the weight matrix will be equal to    the number of prototype patterns ,  Q . Therefore, we can zero the diagonal  since the elements of each  1 by subtracting    times the identity matrix:   are   pq  Q  W' W QI  =  –  .   21.88   Let’s investigate how this change affects the form of the Lyapunov func- tion. If we multiply this new weight matrix times one of the prototype vec- tors we find  W'pq W QI  =  –    pq  =  Spq Qpq  –  =  S Q–  pq  .   21.89   Therefore,  pace is   X  S Q–     is an eigenvalue of   W'  , and the corresponding eigens-  , the space spanned by the prototype vectors.  If we multiply the new weight matrix times a vector from the orthogonal  complement space,   , we find  a X  .  Q–   is an eigenvalue of   Therefore,  X To summarize, the eigenvectors of  W  , but the eigenvalues are now   W'a W QI  =  –    a  =  0 Qa  –  =  Qa–  .   21.90   W'  , and the corresponding eigenspace is   21  W' S Q–   are the same as the eigenvectors of   and  . There-   , instead of    and   Q–  0  S  21-23   21 Hopfield Network  fore, the eigenvalues of the Hessian matrix of the modified Lyapunov func- tion,  , are    and   S Q–  V' a   –=  W'  2  Q  –    .  This implies that the energy surface will have negative curvature in  positive curvature in  which had negative curvature in    and  , in contrast with the original Lyapunov function,    and zero curvature in   X  X  .   X  X  A comparison of Figure 21.9 and Figure 21.10 demonstrates the effect on  the Lyapunov function of setting the diagonal elements of the weight ma- trix to zero. In terms of system performance, the change has little effect. If  the initial condition of the Hopfield network falls anywhere off of the line  , then, in either case, the output of the network will converge to  a1 one of the corners of the hypercube  , which consists of the  two points  1–  a:  1– .  a2–=   1–   and   1 1  ai      =  =    a  a  1  T  T  W  a1  a2–=  , and the weight   If the initial condition falls exactly on the line   is used, then the network output will remain constant. If the ini- matrix  tial condition falls exactly on the line    W' is used, then the network output will converge to the saddle point at the  origin  as in Figure 21.6 . Neither of these results is desirable, since the  network output does not converge to a minimum of the Lyapunov function.  Of course, the only case in which the network converges to a saddle point  is when the initial condition falls exactly on the line  , which would  be highly unlikely in practice.  , and the weight matrix   a2–=  a2–=  a1  a1  21-24   Summary of Results  Summary of Results  Hopfield Model  dn t    ------------ dt  =  –  n t     +  Wa t    b  +  a t     =   f n t       Recurrent Layer  Input  p S x 1  S  W S x S   cid:0  cid:0   cid:0  cid:0   cid:0  cid:0   b S x 1  1   cid:0  cid:0   cid:0  cid:0  n  cid:0  cid:0   f   S x 1  a  S x 1  S  . n   cid:0 1 ε  cid:0   - +  cid:0   cid:0   f -1  cid:0   n 0  = f -1 p ,   a 0  = p      ε dn dt = - n + W f n  + b  Lyapunov Function  V a   =  –  1 ---aTWa 2  +  f 1– u  ud  bTa  –  S    i  1=        ai   0        d td  V a   –  =     d aid    f 1– ai    2         dai   td  S  i  1=  If   d aid    f 1– ai      0  , then   V a   0  .  d td  Invariant Sets  The Invariant Set Consists of the Equilibrium Points.  L  =  Z  =    a:da dt     0=   a in the closure of G    21-25  21   21 Hopfield Network  Hopfield Attractors  The Equilibrium Points Are Stationary Points.  If   da t    ------------  dt  0=  , then     V a   0=  .    V a   =    –  Wa  n b–+    =  –  dn t    ------------  dt  High-Gain Lyapunov Function 1 ---aTWa 2  V a   =  –  bTa  –  2  V a   W–=  Content-Addressable Memory  W  =   pq  pq  T   and   b  0=  Q  q  1=  Energy Surface  Orthogonal Prototype Patterns   Are  Eigenvalues Eigenvectors of   W–=  V a   2  1  S–=  , with eigenspace   X  =  span p1, p2,  ... ,pQ      .  2  0=  , with eigenspace   X  .  X   is defined such that for any vector   a X  ,   pq  Ta  =  0,  q  =  1 2   Q       Trajectories  Orthogonal Prototype Patterns  Because the first eigenvalue is negative,  in  in  Hopfield network will tend to fall into the corners of the hypercube    V a  . Because the second eigenvalue is zero,  . Because    will have negative curvature   will have zero curvature  V a   has negative curvature in  , the trajectories of the  X   that are contained in   a:  1–  X X  V a       X    1  .  ai  21-26   Solved Problems  Solved Problems  P21.1 Assume the binary prototype vectors  p1  =           p2  =  .  1 1 1– 1–  1 1– 1 1–  i. Design a continuous-time Hopfield network  specify connec-  tion weights  to recognize these patterns, using the Hebb  rule.  ii. Find the Hessian matrix of the high-gain Lyapunov function  for this network. What are the eigenvalues and eigenvectors  of the Hessian matrix?  iii. Assuming large gain, what are the stable equilibrium points   for this Hopfield network?  i. First calculate the weight matrix from the reference vectors, using the  supervised Hebb rule.  W p1 p1  =  T p2 p2  +  T  =  1 1 1– 1 1 1– 1– 1–  1– 1– 1 1 1 1  1– 1–  +  1 1– 1– 1 1– 1–  1 1– 1 1 1– 1  1 1–  1 1–  ,  which simplifies to  ii. The Hessian of the high-gain Lyapunov function, from Eq.  21.54 , is  the negative of the weight matrix:  21  W  =  2 0 0 2– 0 2 2– 0 2 0 0 2– 2– 0 0 2  .  2  V a   =  0 0 2 2– 2 0 0 2– 0 2 2– 0 2 0 0 2–  .  21-27   21 Hopfield Network  The prototype patterns are orthogonal   ues are    and   0=  4–  S–  =  =  2  1  p1  Tp2  0=   . Thus, the eigenval-  . The eigenspace for   1  4–=   is   The eigenspace for   0=   is the orthogonal complement of   X  :  2  X  =  span p1, p2      .  X  =  span  ,            1 1 1 1  1 1– 1– 1  ,          where we have chosen two vectors that are orthogonal to both   p2 iii.  The stable points will be   since the negative of the pro- totype patterns will also be equilibrium points. There may be other equilib- rium points, if other corners of the hypercube lie in the span  There are a total of  and four will fall in   .   corners of the hypercube. Four will fall in   and partly in   16= . The other corners are partly in   p1, p2 p– 1 p2–  p1, p2   and   24 X    X XT  p1  X      .  .      P21.2 Consider a high-gain Hopfield network with a weight matrix and   bias given by  W  =    and  b  =  1– 1–  1– 1–  .  1 1–  i. Sketch a contour plot of the high-gain Lyapunov function   for this network.  ii. If the network is given the initial condition   will the network converge?   T  , where   1 1  i. First consider the high-gain Lyapunov function  The Hessian matrix is  V a   =  –  1 ---aTWa 2  bTa  .  –  2  V a   =  W–  =  1 1 1 1  .  Next, we need to compute the eigenvalues and eigenvectors:  21-28   Solved Problems  2  V a   I–  =  =  2  2–  1–+  1  =    2–    .  1 –  1  1  1 –  The eigenvalues are   1  0=   and   2  Now we can find the eigenvectors. For   0=  ,  .  2= 1    2  V a   –  1I  z1  0=  ,  and therefore  1 1 1 1  z1  0=   or   z1  =  .  1 1–    2  V a   –  2I  z2  0=  1– 1 1 1–  z2  0=   or   z2  =  .  1 1  Similarly, for   2  2=  ,  and therefore  So the term   has zero curvature in the direction  tion   .  z1  z2   and positive curvature in the direc-  Now we have to account for the linear term. First plot the contour without  the linear term, as in Figure P21.1.  The linear term will cause a negative slope in the direction of   –  1 ---aTWa 2  b  =  .  1 1–  Therefore everything will curve down toward  ure P21.2.  T  1 1–  , as is shown in Fig-  21  21-29   21 Hopfield Network  0.5  0  -0.5  1.5  2  1  0.5  0 1  2  1  0  -1  -2 1  0.5  1  0  -0.5  -1 -1  0.5  1  0  -0.5  -1 -1  -0.5  0  0.5  1  -1  -1  -0.5  0  0.5  1  Figure P21.1  Contour Without Linear Term  ii. All trajectories will converge to  , regardless of the initial condi- tions. As we can see in Figure P21.2, the energy function has only one min- imum, which is located at  network is constrained to fall within the hypercube   .  Keep in mind that the output of the   1 1–  1 1–  a:  1–  .           1  T  ai  T  0.5  0  -0.5  0  -0.5  -1  -1  1  0.5  -0.5  0  0.5  1  Figure P21.2  Contour Including Linear Term  P21.3 Consider the following prototype vectors.  p1  =        p2  =  1 1  1– 1  i. Design a Hopfield network to recognize these patterns.  ii. Find the Hessian matrix of the high-gain Lyapunov function  for this network. What are the eigenvalues and eigenvectors  of the Hessian matrix?  21-30   Solved Problems  iii. What are the stable points for this Hopfield network  as-  sume large gain ? What are the basins of attraction?  iv. How well does this network perform the pattern recognition   problem?  i. We will use the Hebb rule to find the weight matrix.  W p1 p1  =  T p2 p2  +  T  =  1 1 1 1  +  1 1– 1– 1  =  2 0 0 2  The bias is set to zero.  ii. The Hessian matrix of the high-gain Lyapunov function is the negative  of the weight matrix.  b  =  0 0  2  V a   =  W–  =  2– 0 0 2–  1  =  2  =  S–  =  2–  z1  =   and   z2  =  1 0  ,  0 1  By inspection, we can see that there is a repeated eigenvalue.  The eigenvectors will then be  or any linear combination.  The entire  value   2–=  .     2   is the eigenspace for the eigen-  iii. From Chapter 8 we know that when the eigenvalues of the Hessian are  equal, the contours will be circular. Because the eigenvalues are negative,  the function will have a single maximum at the origin. There will be four  minima at the four corners of the hypercube  . There are also  four saddle points. The high-gain Lyapunov function is displayed in Figure  P21.3.  a:  1–  ai          1  21  21-31   21 Hopfield Network  0.5  1  0  -0.5  -1 -1  0.5  1  0  -0.5  -1 -1  0  -0.5  -1  -1.5  -2 1  0  -0.25  -0.5  -0.75  -1 1  0.5  0  -0.5  0  -0.5  -1  -1  1  0.5  -0.5  0  0.5  1  Figure P21.3  High-Gain Lyapunov Function for Problem P21.3  There are a total of nine stationary points. We could use the corollary to La- Salle’s Invariance Theorem to show that the maximum point at the origin  has a basin of attraction that only includes the origin itself. Therefore it is  not a stable equilibrium point. The saddle points have regions of attraction  that are lines.  For example, the saddle point at   has a region of at- traction along the negative   axis.  The four corners of the hypercube are  the only attractors that have two-dimensional regions of attraction. The re- gion of attraction for each corner is the corresponding quadrant of the hy- percube. Figure P21.4 shows the low-gain Lyapunov function  with gain      and illustrates convergence to a saddle point and to a minimum.   1– 0  1.4  a1  =  T  -0.5  0  0.5  1  Figure P21.4  Lyapunov Function for Problem P21.3  0.5  0  -0.5  0  -0.5  -1  -1  1  0.5  iv. The network does not do a good job on the pattern recognition problem.  Not only does it recognize the two prototype patterns, but it also “recogniz- es” the other two corners of the hypercube as well. The network will con- verge to whichever corner is closest to the input pattern, even though we  only wanted it to store the two prototype patterns. Since every possible   21-32   Solved Problems  two-bit pattern has been stored, the network is not very useful. This is not  unexpected, since the number of patterns that the Hebb rule is expected to  store is only 15% of the number of neurons. Since we only have two neu- rons, we can’t expect to successfully store many patterns. Try Exercise  E21.2 for a better network.  P21.4 A Hopfield network has the following high-gain Lyapunov func-  tion:  V a   –=  1 --- 7 a1  2  2  +  12a1a2  –  2 a2  2    .  i. Find the weight matrix.  ii. Find the gradient vector of the Lyapunov function.  iii. Find the Hessian matrix of the Lyapunov function.  iv. Sketch a contour plot of the Lyapunov function.  v. Sketch the path that a steepest descent algorithm would fol-  low for   V a    with an initial condition of   0.25 0.25  T  .  i.  V a    is a quadratic function, which can be rewritten as  V a   =  –  1 --- 7 a1  2  2  +  12a1a2 2 a2  –  2    =  –  1 ---aT 7 6 2 6 2–  a  .  Therefore the weight matrix is  ii. S ince  dient.  V a    is a quadratic function, we can use Eq.  8.38  to find the gra-  W  =  7 6 6 2–  .    V a   –=  7 6 6 2–  a  21-33  iii. From Eq.  8.39 , the Hessian is  2  V a   =  –  7 6 6 2–  =  7– 6–  6– 2  .  21   21 Hopfield Network  iv. To compute the eigenvalues,  2  V a   I–  =  =  2  +  5 50–  =   10+    5–    .  7–  –  6–  6– 2 –  The eigenvalues are   2 Now we can find the eigenvectors. For   10–=   and   1  .   5= 1  10–=  ,    2  V a   –  1I  z1  0=  ,  and therefore  Similarly, for   2  5=  ,  and therefore  3 6– 6– 12  z1  0=   or   z1  =  .  2 1    2  V a   –  2I  z2  0=  12– 6–  6– 3–  z2  0=   or   z2  =  .  1 2–  Note that this is a saddle point case, since  tive curvature along  of the high-gain Lyapunov function is shown in Figure P21.5.  0 2  and positive curvature along   . There will be nega- . The contour plot  z2     1  z1  0.5  1  0  -0.5  -1 -1  4  0  -4  -8 1  -0.5  0  0.5  1  0.5  0  -0.5  0  -0.5  -1  -1  1  0.5  Figure P21.5  High-Gain Lyapunov Func. & Steepest Descent Trajectory  v. The steepest descent path will follow the negative of the gradient and  will be perpendicular to the contour lines, as we saw in Chapter 9. When   21-34   Solved Problems  the trajectory hits the edge of the hypercube, it follows the edge down to the  minimum point. The resulting trajectory is shown in Figure P21.5.  The high-gain Lyapunov function is only an approximation, since it as- sumes infinite gain. As a comparison, Figure P21.6 illustrates the  Lyapunov function, and the Hopfield trajectory, for a gain of   0.5  .  0.5  1  0  -0.5  -1 -1  8  6  4  2  0  -2  -4  -6 1  0.5  0  -0.5  0  -0.5  -1  -1  1  0.5  -0.5  0  0.5  1  Figure P21.6  Lyapunov Function & Hopfield Trajectory  P21.5 The Hopfield network has been used for applications other than  content-addressable memory. One of these other applications is  analog-to-digital  A D  conversion [HoTa86]. The function of the    A D converter is to take an analog signal  , and convert it into a se- ries of bits  zeros and ones . For example, a two-bit A D converter  would try to approximate the signal    as follows:  y  y  y    1–    ai2 i  =  a1  +  a22  ,  2  i  1=  a2  a1   and    are allowed values of   where  would approximate analog values in the range from  resolution of  mance index for the A D conversion process:   or   .  Tank and Hopfield suggest the following perfor-  .  This A D converter  , with a    to   3  1  1  0  0  J a   =   1  --- y 2   2  –  i  1=  1–  ai2 i   2     –  1 --- 2  2      i  1=  1–  22 i  ai ai 1–         ,  where the first term represents the A D conversion error, and the  second term forces    to take on values of    and    or   .  0  1  a1  a2  21  Show that this performance index can be written as the Lyapunov  function of a Hopfield network and define the appropriate weight  matrix and bias vector.  21-35   21 Hopfield Network  The first step is to expand the terms of the performance index.      2  i  1=   2     y  –  1–  ai2 i  =  y2  – 2y  1–    ai2 i  +  1–    +  1– j    aiaj2 i  ,  2      i  1=  1–  22 i  ai ai   1–  =  1–    222 i  ai  1–    ai22 i  2  i  1=       2    i  1=  If we substitute these terms back into the performance index we find  J a   =  aiaj2 i 1–    +  1– j    +  1–  ai 22 i     2iy –  .    1 --- y2  2    2    +  j  1=  i  2    1= j  i         2  2    j  1=  i  1=  2  –  i  1=  2    i  1=  The first term is not a function of  minima will occur, and we can ignore it.  a  . Therefore, it does not affect where the   We now want to show that this performance index takes the form of a high- gain Lyapunov function:  V a   =  –  1 ---aTWa 2  bTa  .  –  This will be the case if  W  =   and   b  =  0 2–  2– 0  1 ---– 2 2–  y  2y  .  In this Hopfield network, unlike the content-addressable memory, the in- put to the network is the scalar  , which is then used to compute the bias  vector. In the content-addressable memory, the inputs to the network were  vector patterns, which became the initial conditions on the network out- puts.  y  Note that in this network the transfer function must limit the output to the  range   . One transfer function that could be used is     1  0  a  f n   =  1  ----------------------- e n––  1  .  21-36   Epilogue  Epilogue  In this chapter we have introduced the Hopfield model, one of the most in- fluential neural network architectures. One of the reasons that Hopfield  was so influential was that he emphasized the practical considerations of  the network. He described how the network could be implemented as an  electrical circuit, and VLSI implementations of Hopfield-type networks  were built at an early stage.   Hopfield also explained how the network could be used to solve practical  problems in pattern recognition and optimization. Some of the applications  that Hopfield proposed for his networks were: content-addressable memory  [Hopf82], A D conversion [TaHo86] and linear programming and optimiza- tion tasks, such as the traveling salesman problem [HoTa85].  One of Hopfield’s key contributions was the application of Lyapunov stabil- ity theory to the analysis of his recurrent networks. He also showed that,  for high-gain amplifiers, the Lyapunov function for his network was a qua- dratic function, which was minimized by the network. This led to several  design procedures. The idea behind the development of the design tech- niques was to convert a given task into a quadratic minimization problem,  which the network could then solve.  The Hopfield network is the last topic we will cover in any detail in this  text. However, we have certainly not exhausted all of the important neural  network architectures. In the next chapter we will give you some ideas  about where to go next to explore the subject further.  21  21-37   21 Hopfield Network  Further Reading  [Ande72]   J. Anderson, “A simple neural network generating an inter- active memory,” Mathematical Biosciences, vol. 14, pp.  197–220, 1972.  Anderson proposed a “linear associator” model for associa- tive memory. The model was trained, using a generaliza- tion of the Hebb postulate, to learn an association between  input and output vectors. The physiological plausibility of  the network was emphasized. Kohonen published a closely  related paper at the same time [Koho72], although the two  researchers were working independently.  J. A. Anderson, J. W. Silverstein, S. A. Ritz and R. S. Jones,  “Distinctive features, categorical perception, and probabil- ity learning: Some applications of a neural model,” Psycho- logical Review, vol. 84, pp. 413–451, 1977.  This article describes the brain-state-in-a-box neural net- work model. It combines the linear associator network with  recurrent connections to form a more powerful autoassocia- tive system. It uses a nonlinear transfer function to contain  the network output within a hypercube.  M. A. Cohen and S. Grossberg, “Absolute stability of global  pattern formation and parallel memory storage by compet- itive neural networks,” IEEE Transactions on Systems,  Man and Cybernetics, vol. 13, no. 5, pp. 815–826, 1983.  Cohen and Grossberg apply LaSalle’s Invariance Theorem  to the analysis of the stability of competitive neural net- works. The network description is very general, and the au- thors show how their analysis can be applied to many  different types of recurrent neural networks.  S. Grossberg, “Nonlinear difference-differential equations  in prediction and learning theory,” Proceedings of the Na- tional Academy of Sciences, vol. 58, pp. 1329–1334, 1967.  This early work of Grossberg’s discusses the storage of in- formation in dynamically stable configurations.  [AnSi77]  [CoGr83]  [Gros67]  21-38   [Hopf82]  [Hopf84]  [HoTa85]  Further Reading  J. J. Hopfield, “Neural networks and physical systems with  emergent collective computational properties,” Proceedings  of the National Academy of Sciences, vol. 79, pp. 2554– 2558, 1982.  This is the original Hopfield neural network paper, which  signaled the resurgence of the field of neural networks. It  describes a discrete-time network that behaves as a con- tent-addressable memory. Hopfield demonstrates that the  network evolves so as to minimize a specific Lyapunov  function.  J. J. Hopfield, “Neurons with graded response have collec- tive computational properties like those of two-state neu- rons,” Proceedings of the National Academy of Sciences, vol.  81, pp. 3088–3092, 1984.  Hopfield demonstrates how an analog electrical circuit can  function as a model for a large network of neurons with a  graded response. The Lyapunov function for this network  is derived and is used to design a network for use as a con- tent-addressable memory.  J. J. Hopfield and D. W. Tank, “ ‘Neural’ computation of de- cisions in optimization problems,” Biological Cybernetics,  vol. 52, pp. 141–154, 1985.  This article describes the application of Hopfield networks  to the solution of optimization problems. The traveling  salesman problem, in which the length of a trip through a  number of cities with only one visit to each city is mini- mized, is mapped onto a Hopfield network.  [Koho72]   T. Kohonen, “Correlation matrix memories,” IEEE Trans- actions on Computers, vol. 21, pp. 353–359, 1972.  Kohonen proposed a correlation matrix model for associa- tive memory. The model was trained, using the outer prod- uct rule  also known as the Hebb rule , to learn an  association between input and output vectors. The mathe- matical structure of the network was emphasized. Ander- son published a closely related paper at the same time  [Ande72], although the two researchers were working inde- pendently.  21  21-39   [LiMi89]  [McPi43]  [TaHo86]  21 Hopfield Network  J. Li, A. N. Michel and W. Porod, “Analysis and synthesis  of a class of neural networks: Linear systems operating on  a closed hypercube,” IEEE Transactions on Circuits and  Systems, vol. 36, no. 11, pp. 1405–1422, November 1989.  This article investigates a class of neural networks de- scribed by first-order linear differential equations defined  on a closed hypercube  Hopfield-like networks . Wanted  and unwanted equilibrium points fall at the corners of the  cube. The authors discuss design procedures that minimize  the number of spurious equilibrium points.  W. McCulloch and W. Pitts, “A logical calculus of the ideas  immanent in nervous activity,” Bulletin of Mathematical  Biophysics., vol. 5, pp. 115–133, 1943.  This article introduces the first mathematical model of a  neuron in which a weighted sum of input signals is com- pared to a threshold to determine whether or not the neu- ron fires.  D. W. Tank and J. J. Hopfield, “Simple ‘neural’ optimiza- tion networks: An A D converter, signal decision circuit  and a linear programming circuit,” IEEE Transactions on  Circuits and Systems, vol. 33, no. 5, pp. 533–541, 1986.  The authors describe how Hopfield neural networks can be  designed to solve certain optimization problems. In one ex- ample the Hopfield network implements an analog-to-digi- tal conversion.  21-40   Exercises  Exercises  E21.1 In the Hopfield network example starting on page 18-8 we used a gain of  . Figure 21.3 displays the Lyapunov function for that example. The    high-gain Lyapunov function for the example is shown in Figure 21.9.   1.4  =  i. Show that the minima of the Lyapunov function for this example  .  Use Eq.   =      will be located at points where   21.42  and set the gradient of   = n1 V a   =  f n1 n2  to zero.   f n2  ii. Investigate the change in location of the minima as the gain is var-  ied from     =  0.1   to     10=  .  iii. Sketch the contour plot for several different values of gain in this in-  terval. You will probably need to use MATLAB for this.  E21.2 In Problem P21.3 we used the supervised Hebb rule to design a Hopfield   network to recognize the following patterns:  If we use another design rule [LiMi89], we find the following weight matrix  and bias  p1  =        p2  =  1 1  .  1– 1  W  =        b  =  1 0 0 10–  .  0 11  i. Graph the contour plot for the high-gain Lyapunov function, if this   weight matrix and bias are used.   ii. Discuss the difference between the performance of this Hopfield   network and the one designed in Problem P21.3.  iii. Write a MATLAB M-file to simulate the Hopfield network. Use the  ode45 routine. Plot the responses of this network for several initial  conditions.  » 2 + 2 ans =       4  » 2 + 2 ans =       4  E21.3 A Hopfield network has the following high-gain Lyapunov function:  V a   –=  1 --- a1  2  2  +  2a1a2  +  4 a2  2  +  6a1  +  10a2    .  21  i. Find the weight matrix and bias vector for this network.  ii. Find the gradient and Hessian for   V a   .  21-41   21 Hopfield Network  iii. Sketch a contour plot of   V a   .  iv. Find the stationary point s  for   . Use the corollary to LaSalle’s  Invariance Theorem to find as much information as you can about  basins of attraction for any stable equilibrium points.  V a   E21.4 In Problem P21.5 we demonstrated how a Hopfield network could be de-  signed to operate as an A D converter.   i. Sketch the contour plot of the high-gain Lyapunov function for the  . Lo-  two-bit A D converter network using an input value of  cate the minimum points.  0.5  =  y  ii. Repeat part  i  for an input value of   y  =  2.5  .  iii. Use the answers to parts  i  and  ii  to explain how the network will   operate. Will the network perform the A D conversion correctly?  E21.5 Assume the binary prototype vectors  p1  =  1– 1 1 1–  T  ,   p2  =  1 1– 1 1–  T  .  i. Design a continuous-time Hopfield network  specify connection   weights and biases only  to recognize these patterns, using the Hebb  rule.  ii. Find the Hessian matrix of the high-gain Lyapunov function for this  network. What are the eigenvalues and eigenvectors of the Hessian  matrix?  This requires very little computation.   iii. Assuming large gain, what are the stable equilibrium points for this   Hopfield network?  E21.6 Repeat Exercise E21.5 for the following prototype vectors.  i.  ii.  iii.  p1 p1 p1  =  =  =  1 1– 1 1  1– 1 1 1–  T  =  ,  T  p2 ,  p2  T  .  T  .  1–  1 1 1 1–  = T  1 1 1– p2  =  ,   1–  1– 1 1 1–  1–  1–  1–  1– 1 1 1  T  .  E21.7 Consider a high-gain Hopfield network with weight matrix and bias given   by:  W  =   and   b  =  1 3 3 9  .  3 1–  21-42   Exercises  network.  network converge?  i. Sketch a contour plot of the high-gain Lyapunov function for this   ii. If the network is given the following initial condition, where will the   a 0   =  0.5 0.5–  1 1–  E21.8 Design a high-gain Hopfield network  give the weights and the biases  with   only one stable equilibrium point:  Explain your procedure, and show all steps.  Do not use the Hebb rule.   E21.9 Consider a high-gain Hopfield network with weight matrix and bias given   by:  W  =   and   b  =  1– 1–  1– 1–  .  1 1–  i. Sketch a contour plot of the high-gain Lyapunov function for this   network.  ii. Assuming a large gain, what are the stable equilibrium points for  this Hopfield network? What can you say about the basins of attrac- tion for these stable equilibrium points? Explain your answers.  E21.10 Repeat E21.9 for the following weight and bias:  W  =   and   b  =  1– 0  0 1–  .  1 1  » 2 + 2 ans =       4  E21.11 In Exercise E7.11 we asked the question: How many prototype patterns   can be stored in one weight matrix? Repeat that problem using the  Hopfield network. Begin with the digits “0” and “1”.  The digits are shown  at the end of this problem.  Add one digit at a time up to “6”, and test how  often the correct digit is reconstructed after randomly changing 2, 4 and 6  pixels.  21  i. First use the Hebb rule to create the weight matrix for the digits “0”   21-43   21 Hopfield Network  and “1”. Then randomly change 2 pixels of each digit and apply the  noisy digits to the network. Repeat this process 10 times, and record  the percentage of times in which the correct pattern  without noise   is produced at the output of the network. Repeat as 4 and 6 pixels of  each digit are modified. The entire process is then repeated when  the digits “0”, “1” and “2” are used. This continues, one digit at a  time, until you test the network when all of the digits “0” through  “6” are used. When you have completed all of the tests, you will be  able to plot three curves showing percentage error versus number of  digits stored, one curve each for 2, 4 and 6 pixel errors.  ii. Repeat part  i  using the pseudoinverse rule  see Chapter 7 , and   compare the results of the two rules.  iii. For extra credit, repeat part  i  using the method described in   [LiMi89]. In that paper it is called Synthesis Procedure 5.1.  p1  p2  p3  p4  p5  p6  p7  p8  p9  p10  21-44   Objectives  22 Practical Training Issues  Objectives  Theory and Examples  Pre-Training Steps   Training the Network   Selection of Data  Data Preprocessing  Choice of Network Architecture   22-1 22-2 22-3 22-3 22-5 22-8 22-13 22-13 Weight Initialization  22-14 Choice of Training Algorithm  22-14 Stopping Criteria  Choice of Performance Function  22-16 Multiple Training Runs and Committees of Networks 22-17 22-18 22-18 22-21 22-23 22-24 22-27 22-28 22-30 22-31  Fitting  Pattern Recognition  Clustering  Prediction  Overfitting and Extrapolation  Sensitivity Analysis   Post-Training Analysis   Epilogue  Further Reading   Objectives  Previous chapters have focused on particular neural network architectures  and training rules, with an emphasis on fundamental understanding. In  this chapter, we will discuss some practical training tips that apply to a va- riety of networks. No derivations are provided for the techniques that are  presented here, but we have found these methods to be useful in practice.  There will be three basic sections in this chapter. The first section describes  things that need to be done prior to training a network, such as collecting  and preprocessing data and selecting the network architecture. The second  section addresses network training itself. The final section considers post- training analysis.  22-1  22   22 Practical Training Issues  Theory and Examples  In previous chapters, we have discussed a variety of neural network archi- tectures and learning rules. Those chapters have placed special emphasis  on the fundamental concepts behind each network. In this chapter, we will  concentrate on practical aspects of training neural networks. Theoretical  aspects and practical aspects are not mutually exclusive. It is only by com- bining a deep knowledge of network fundamentals with practical experi- ence in using neural networks that we can get the most out of this  technology.  Figure 22.1 illustrates the neural network training process. It is an itera- tive procedure that begins by collecting data and preprocessing it to make  training more efficient. At this stage, the data also needs to be divided into  training validation testing sets  see Chapter 13 . After the data is selected,  we need to choose the appropriate network type  multilayer, competitive,  dynamic, etc.  and architecture  e.g., number of layers, number of neurons .  Then we select a training algorithm that is appropriate for the network and  the problem we are trying to solve. After the network is trained, we want  to analyze the performance of the network. This analysis may lead us to  discover problems with the data, the network architecture, or the training  algorithm. The entire process is then iterated until the network perfor- mance is satisfactory.  Collect Preprocess  Data  Select Network Type Architecture  Select Training  Algorithm  Analyze Network  Performance  Initialize Weights  &  Train Network  Use Network  Figure 22.1  Flowchart of Neural Network Training Process  In the remainder of this chapter, we will discuss each part of the training  process in some detail. We have divided this material into three major sec- tions: Pre-Training Steps, Training the Network, and Post-Training Anal- ysis.  22-2   Pre-Training Steps  Pre-Training Steps  Before we dig into the details of training, it is worth making a preliminary  comment. Before beginning the neural network training process, you  should first determine if a neural network is needed to solve your problem,  or if some simpler linear technique might be adequate. For example, there  is no need to use a neural network for a fitting problem, if standard linear  regression will produce a satisfactory result. The neural network tech- niques provide additional power, but at the expense of more challenging  training requirements. When linear methods will work, they are the first  choice.  There are a number of steps that need to be performed before the network  is trained. They can be grouped into three categories: Selection of Data,  Data Preprocessing, and Choice of Network Type and Architecture.  Selection of Data It is generally difficult to incorporate prior knowledge into a neural net- work, therefore the network will only be as good as the data that is used to  train it. Neural networks represent a technology that is at the mercy of the  data. The training data must span the full range of the input space for  which the network will be used. As we discussed in Chapter 13, there are  training methods we can use to insure that the network interpolates accu- rately throughout the range of the data provided  generalizes well . How- ever, it is not possible to guarantee network performance when the inputs  to the network are outside the range of the training set. Neural networks,  like other nonlinear “black box” methods, do not extrapolate well.  It is not always easy to be sure that the input space is adequately sampled  by the training data. For simple problems, in which the dimension of the  input vector is small, and each element of the input vector can be chosen  independently, we can sample the input space using a grid. However, these  conditions are not often satisfied. For many problems, the dimension of the  input space is large, which precludes the use of grid sampling. In addition,  it is often the case that the input variables are dependent. For example,  consider Figure 22.2. The shaded area represents the range over which the  two inputs can vary. Even though each variable can range from -1 to 1, we  would not need to create a grid in which both variables vary throughout  their range, as shown by the dots in Figure 22.2. The network only needs  to fit the function in the shaded area, since this is where the network will  be used. It would be inefficient to fit the network outside the range of its  use. This is especially true when the input dimension is large.  22-3  22   22 Practical Training Issues  p2  p1  Figure 22.2  Input Range With Dependent Input Variables  It may not be possible to precisely define the active region of the input  space. However, we can often collect data during standard operation of the  system we are trying to model. In some cases, we have complete control  over the design of the experiment during which data is collected. In these  cases, we must be sure that the experimental setup drives the system  through all conditions for which we plan to use the network.  How can we be sure that the input space has been adequately sampled by  the training data? This is difficult to do prior to training, and there are  many cases in which we have no control over the data collection process  and must use whatever data is available. We will come back to this subject  in the Post-Training Analysis section on page 22-18. By analyzing the  trained network, we can often tell if the training data was sufficient. In ad- dition, we can use techniques that indicate when a network is being used  outside the range of the data with which it was trained. This will not im- prove the network performance, but it will prevent us from using a network  in situations where it is not reliable.  After collecting the data, we generally divide it into three sets: training,  validation and testing. As we discussed in Chapter 13, the training set will  generally make up approximately 70% of the full data set, with validation  and testing making up approximately 15% each. It is important that each  of these sets be representative of the full data set — that the validation and  test sets cover the same region of the input space as the training set. The  simplest method for dividing the data is to select each set at random from  the full data set. This usually produces a good result, but it is best to review  the division to check for major differences between the sets. It is also pos- sible in the post-training analysis to detect problems in the data division.  We will have more to say about that later.  A final question we must ask about the selection of data is “Do we have  enough data?” This is difficult to answer, especially before we train the net-  22-4   Pre-Training Steps  work. The amount of data that is required depends on the complexity of the  underlying function that we are trying to approximate  or the complexity  of the decision boundaries that we are trying to implement . If the function  to be approximated is very complex, with many inflection points, then this  requires a large amount of data. If the function is very smooth, then the  data requirements are significantly reduced  unless the data is very noisy .  The choice of the data set size is closely related to the choice of the number  of neurons in the neural network. This is discussed in the Choice of Net- work Architecture section on page 22-8. Of course, we generally don’t know  how complex the underlying function is before we begin training the net- work. For this reason, as we discussed earlier, the entire neural network  training process is iterative. At the completion of training we will analyze  the performance of the network. The results of that analysis can help us de- cide if we have enough data or not.  Data Preprocessing The main purpose of the data preprocessing stage is to facilitate network  training. Data preprocessing consists of such steps as normalization, non- linear transformations, feature extraction, coding of discrete inputs tar- gets, handling of missing data, etc. The idea is to perform preliminary  processing of the data to make it easier for the neural network training to  extract the relevant information.      exp  3–  0.05  For example, in multilayer networks, sigmoid transfer functions are often  used in the hidden layers. These functions become essentially saturated  when the net input is greater than three    . We don’t want  this to happen at the beginning of the training process, because the gradi- ent will then be very small. In the first layer, the net input is a product of  the input times the weight plus the bias. If the input is very large, then the  weight must be small in order to prevent the transfer function from becom- ing saturated. It is standard practice to normalize the inputs before apply- ing them to the network. In this way, initializing the network weights to  small random values guarantees that the weight-input product will be  small. Also, when the input values are normalized, the magnitudes of the  weights have a consistent meaning. This is especially important when us- ing regularization, as described in Chapter 13. Regularization requires the  weight values to be small. However, “small” is a relative term; if the input  values are very small, we need large weights to produce a significant net  input. Normalizing the inputs clarifies the meaning of “small” weights.  Normalization  There are two standard methods for normalization. The first method nor- malizes the data so that they fall into a standard range — typically -1 to 1.  This can be done with  pn  =  2 p pmin –  .  pmax pmin  –     1–  ,   22.1   pmin  where  the input vectors in the data set,   pmax   is the vector containing the minimum values of each element of    contains the maximum values,   .      22  22-5   22 Practical Training Issues  represents an element-by-element division of two vectors, and  sulting normalized input vector.  pn   is the re-  An alternative normalization procedure is to adjust the data so that they  have a specified mean and variance — typically 0 and 1. This can be done  with the transformation  pn  =  p pmean –  . pstd  ,   22.2  pstd  pmean   is the average of the input vectors in the data set, and   where   is  the vector containing the standard deviations of each element of the input  vectors.  Generally, the normalization step is applied to both the input vectors and  the target vectors in the data set.  In addition to normalization, which involves a linear transformation, non- linear transformations are sometimes also performed as part of the pre- processing stage. Unlike normalization, which is a standard process that  can be applied to any data set, these nonlinear transformations are case- specific. For example, many economic variables show a logarithmic depen- dence [BoJe94]. In that case, it might be appropriate to take the logarithm  of the input values before applying them to the neural network. Another  example is molecular dynamics simulation [RaMa05], in which atomic forc- es are calculated as functions of distances between atoms. Since it is known  that the forces are inversely related to the distances, we might perform the  reciprocal transformation on the inputs, before applying them to the net- work. This represents one way of incorporating prior knowledge into neural  network training. If the nonlinear transformation is cleverly chosen, it can  make the network training more efficient. The preprocessing will off-load  some of the work required of the neural network in finding the underlying  transformation between inputs and targets.  Another data preprocessing step is called feature extraction. This generally  applies to situations in which the dimension of the raw input vectors is very  large and the components of the input vector are redundant. The idea of  feature extraction is to reduce the dimension of the input space by calculat- ing a small set of features from each input vector, and using the features  as the input to the neural network. For example, neural networks can be  used to analyze EKG  electrocardiogram  signals to identify heart prob- lems [HeOh97]. The EKG might involve 12 or 15 signals  leads  measured  over several minutes at a high sampling rate. This is too much data to ap- ply directly to the neural network. Instead, we would extract certain fea- tures from the EKG signal, such as average time intervals between certain  waveforms, average amplitudes of certain waves, etc.  See Chapter 25.   There are also certain general-purpose feature extraction methods. One of  these is the method of principal component analysis  PCA  [Joll02]. This  method transforms the original input vectors so that the components of the   22-6  Nonlinear Transformation  Feature Extraction  Principal Components   Coding the Targets  Pre-Training Steps  transformed vectors are uncorrelated. In addition, the components of the  transformed vector are ordered such that the first component has the  greatest variance, the second component has the next greatest variance,  etc. We generally keep only the first few components of the transformed  vector, which account for most of the variance in the original vector. This  results in a large reduction in the dimension of the input vector, if the orig- inal components are highly correlated. The drawback of using PCA is that  it only considers linear relationships between the components of the input  vector. When reducing the dimension using a linear transformation, we  might lose some nonlinear information. Since the main purpose of using  neural networks is to gain the power of their nonlinear mapping capabili- ties, we should be careful when using principal components to reduce the  input dimension before applying the inputs to the neural network. There is  a nonlinear version of PCA, called kernel PCA [ScSm99].  Another important preprocessing step is needed whenever the inputs or  targets take on only discrete values. For example, in pattern recognition  problems, each target will represent one of a finite number of classes. In  these cases we need to have a procedure for coding the inputs or targets. If  we have a pattern recognition problem in which there are four classes,  there are at least three common ways in which we could code the targets.  First, we can have scalar targets that take on four possible values  e.g., 1,  2, 3, 4 . Second, we can have two-dimensional targets, which represent a  binary code of the four classes  e.g.,  0,0 ,  0,1 ,  1,0 ,  1,1  . Third, we can  have four-dimensional targets, in which only one neuron at a time is active   e.g.,  1,0,0,0 ,  0,1,0,0 ,  0,0,1,0 ,  0,0,0,1  . The third method tends to yield  the best results in our experience.  Note that discrete inputs can be coded  in the same ways as discrete targets.   When coding the target values, we also need to consider the transfer func- tion that is used in the output layer of the network. For pattern recognition  problems, we would typically use sigmoid functions: log-sigmoid or tan- gent-sigmoid. If we use the tangent-sigmoid in the last layer, which is more  common, then we might consider assigning target values to -1 or 1, which  represent the asymptotes of the function. However, this tends to cause dif- ficulties for the training algorithm, which tries to saturate the sigmoid  function to meet the target value. It is better to assign target values at the  point where the second derivative of the sigmoid function is maximum  see  [LeCu98] . For the tangent-sigmoid function, this occurs when the net in- put is -1 and 1, which corresponds to output values of -0.76 and +0.76.  Softmax  Another transfer function that is used in the output layer of multilayer pat- tern recognition networks is the softmax function. This transfer function  has the form  ai  =  f ni    =  exp  ni    exp  nj    .   22.3   S    j  1=  22  22-7   Missing Data  22 Practical Training Issues  The outputs of the softmax transfer function can be interpreted as the prob- abilities associated with each class. Each output will fall between 0 and 1,  and the sum of the outputs will equal 1. See Chapter 24 for an example ap- plication of the softmax transfer function.  Another practical issue to consider is missing data. It is often the case, es- pecially when dealing with economic data, for example, that some of the  data is missing. For instance, we might have an input vector containing 20  economic variables that are collected at monthly intervals. There may be  some months in which one or two of the 20 variables were not collected  properly. The simplest solution to this problem would be to throw out the  data for any month in which any of the variables were missing. However,  we might be very limited in the amount of data available, and it might be  very expensive to collect additional data. In these cases, we would like to  make full use of any data that we have, even if it is incomplete.  There are several strategies for dealing with missing data. If the missing  data occurs in an input variable, one possibility is to replace the missing  value with the average value for that particular input variable. At the same  time, we could add an additional flag element to the input vector that  would indicate that missing data for that input variable had been replaced  with the average. This additional element of the input vector could be as- signed the value 1 when the input variable was available, and 0 when the  input variable was missing for that case. This would provide the neural  network with information about which variables were missing. An addi- tional flag element would be added to the input vector for every input vari- able that contained missing points.  If the missing data occurs in an element of the target, then the performance  index can be modified so that errors associated with the missing target val- ues are not included. All known target values will contribute to the perfor- mance index, but missing target values will not.  Choice of Network Architecture The next step in the network training process is the choice of network ar- chitecture. The basic type of network architecture is determined by the  type of problem we wish to solve. Once the basic architecture is chosen, we  need to decide such specific details as how many neurons and layers we  want to use, how many outputs the network should have, and what type of  performance function we want to use for training.  Choice of Basic Architecture  The first step in choosing the architecture is to define the problem that we  are trying to solve. For this chapter, we will limit our discussion to four  types of problems: fitting, pattern recognition, clustering and prediction.  Fitting  Fitting is also referred to as function approximation or regression. In fitting  problems, you want a neural network to map between a set of inputs and a   22-8   Pre-Training Steps  corresponding set of targets. For example, a realtor might want to estimate  home prices from such input variables as tax rate, pupil teacher ratio in lo- cal schools and crime rate. An automotive engineer might want to estimate  engine emission levels based on measurements of fuel consumption and  speed. A physician might want to predict a patient's body fat level based on  body measurements. For fitting problems, the target variable takes on con- tinuous values.  For an example of training a neural network for a fitting  problem, see Chapter 23.   The standard neural network architecture for fitting problems is the mul- tilayer perceptron, with tansig neurons in the hidden layers, and linear  neurons in the output layer. The tansig transfer function is generally pre- ferred over the logsig transfer function in the hidden layers for the same  reason that inputs are normalized. It produces outputs  which are inputs  to the next layer  that are centered near zero, whereas the logsig transfer  function always produces positive outputs. For most fitting problems, a sin- gle hidden layer is sufficient. If the results with one hidden layer are not  satisfactory, two layers are sometimes used. It would be rare in a standard  fitting problem to use more than two hidden layers, although, for very dif- ficult problems, deep networks, with many layers, have been used. Linear  transfer functions are used in the output layer for fitting problems, because  the target output is a continuous variable. As we saw in Chapter 11, a two  layer network with sigmoid transfer functions in the hidden layer and lin- ear transfer functions in the output layer is a universal approximator.  Radial basis networks can also be used for fitting problems. The Gaussian  transfer function is most commonly used in the hidden layer for these net- works, with linear transfer functions in the output layer.  Pattern recognition is also referred to as pattern classification. In pattern  recognition problems, you want a neural network to classify inputs into a  set of target categories. For example, a wine dealer might want to recognize  the vineyard that a particular bottle of wine came from, based on a chemi- cal analysis of the wine. A physician might want to classify a tumor as be- nign or malignant, based on uniformity of cell size, clump thickness and  mitosis.  In addition to fitting problems, multilayer perceptrons can be used for pat- tern recognition. The main difference between a fitting network and a pat- tern recognition network is the transfer function used in the output layer.  For pattern recognition problems, we generally use a sigmoid function in  the output layer. The radial basis function network can also be used for pat- tern recognition.   For an example of training a neural network for a pattern recognition prob- lem, see Chapter 25.  Pattern Recognition  Clustering  Clustering, or segmentation, is another use for neural networks. In cluster- ing problems, you want a neural network to group data by similarity. For  example, businesses may wish to perform market segmentation, which is   22  22-9   Prediction  22 Practical Training Issues  done by grouping people according to their buying patterns. Computer sci- entists may want to perform data mining by partitioning data into related  subsets. Biologists may wish to perform bioinformatic analyses, such as  grouping genes with related expression patterns.  Any of the competitive networks described in Chapter 16 could be used for  clustering. The self-organizing feature map  SOFM  is the most popular  network for clustering. The main advantage of the SOFM is that it allows  visualization of high-dimensional spaces.  For an example of training a neural network for a clustering problem, see  Chapter 26.  Prediction also falls under the categories of time series analysis, system  identification, filtering or dynamic modeling. The idea is that we wish to  predict the future value of some time series. An equities trader might want  to predict the future value of some security. A control engineer might want  to predict a future value of the concentration of some chemical, which is the  output of a processing plant. A power systems engineer might want to pre- dict outages on the electric grid.  Prediction requires the use of dynamic neural networks, as discussed in  Chapter 14. The specific form of the network will depend on the particular  application. The simplest network for nonlinear prediction is the focused  time-delay neural network, which is shown in Figure 22.3. This is part of a  general class of dynamic networks, called focused networks, in which the  dynamics appear only at the input layer of a static multilayer feedforward  network. This network has the advantage that it can be trained using static  backpropagation algorithms, since the tapped-delay-line at the input of the  network can be replaced with an extended vector of delayed values of the  input.  Inputs  Layer 1  Layer 2  p1   t  R 1  T D L  1  IW1,1  S 1 x     R d1  b1  S 1 x 1  n1   t  a1   t  S 1 x 1  S 1 x 1  1  S 1  LW2,1  S 2 x S 1  b2  S 2 x 1  a2   t  S 2 x 1  n2   t  S 2 x 1  S 2  Figure 22.3  Focused Time-Delay Neural Network  For problems of dynamic modeling and control, the NARX network  Non- linear AutoRegressive model with eXogenous input  is popular. This net- work is shown in Figure 22.4. The input signal could represent, for   22-10   Pre-Training Steps  example, the voltage applied to a motor, and the output could represent the  angular position of a robot arm. As with the focused time-delay neural net- work, the NARX network can be trained with static backpropagation. The  two tapped-delay-lines can be replaced with extended vectors of delayed in- puts and targets. We can use targets, instead of feeding back the network  outputs  which would require dynamic backpropagation for training , be- cause the output of the network should match the targets when training is  complete.This is discussed in more detail in Chapter 27.  Inputs  Layer 1  Layer 2  p1   t  n1   t  a1   t  S 1 x 1  LW2,1  S 2 x S 1  b2  S 2 x 1  S 1 x 1  1  S 1  a2   t  S 2 x 1  n2   t  S 2 x 1  S 2  R x 11  R 1  T D L  1  T D L  IW1,1  S 1 x R  b1  S 1 x 1  LW1,3  Figure 22.4  NARX Neural Network  There are many other types of dynamic networks that could be used for pre- diction, but the focused time delay network and the NARX network are the  simplest of their type.  For an example of training a neural network for a prediction problem, see  Chapter 27.  Selection of Architecture Specifics  After the basic network structure is chosen, we want to select the specifics  of the architecture  e.g., the number of layers, the number of neurons, etc. .  In some cases, the basic architecture choice will automatically determine  the number of layers. For example, if the SOFM is used for clustering, then  the network will have one layer. In the case of the multilayer network,  which can be used for fitting or pattern recognition, the number of hidden  layers is not determined by the problem, since any number of hidden layers  is possible. The standard procedure is to begin with a network with one hid- den layer. If the performance of the two-layer network is not satisfactory,  then a three-layer network can be used. It would be unusual to use more  than two hidden layers. The training becomes more difficult when multiple  hidden layers are used. This is because each layer performs a squashing op- eration, as sigmoid functions are used in the hidden layers. This causes the   22-11  22   22 Practical Training Issues  derivatives of the performance function with respect to weights in the early  layers to be quite small, which can cause slow convergence for steepest de- scent optimization. For very difficult problems, however, deep networks,  with several hidden layers, can be used. Typically, parallel or GPU comput- ing is required to train deep multilayer networks within a reasonable  amount of time.  We also need to select the number of neurons in each layer. The number of  neurons in the output layer is the same as the size of the target vector. The  numbers of neurons in the hidden layers are determined by the complexi- ties of the function that is being approximated or the decision boundaries  that are being implemented. Unfortunately, we don’t normally know how  complex the problem is until we try to train the network. The standard pro- cedure is to begin with more neurons than necessary and then to use early  stopping or Bayesian regularization to prevent overfitting, as was de- scribed in Chapter 13.   The principal drawback to having too many neurons is that the network  may overfit the data. If we use early stopping or Bayesian regularization,  then we can prevent overfitting. However, there may be some situations in  which we are concerned with the computation time or space required by the  network  e.g., for real-time implementation on microcontrollers, VLSI or  FPGAs . In these cases we want to find the simplest network that will fit  the data. If you use Bayesian regularization, the effective number of pa- rameters can be used to determine how many neurons to use. If, after train- ing, the effective number of parameters is much less than the total number  of parameters in the network, then the number of neurons can be reduced,  and the network retrained. It is also possible to use “pruning” methods to  eliminate neurons or weights in the network.  The number of neurons in the last layer is equal to the number of elements  in the target vector. However, when there are multiple targets, we have a  choice to make. We can have one network with multiple outputs, or we can  have multiple networks, each with one output. For example, neural net- works have been used to estimate LDL, VLDL and HDL cholesterol levels,  based on a spectral analysis of the blood. It is possible to have one neural  network with three neurons in the output layer to estimate all three cho- lesterol levels, or we could have three neural networks, with each one esti- mating only one of the three components. Theoretically, both methods  should work, but in practice one method may work better than another. We  generally start with one multi-output network, and then use multiple sin- gle-output networks if the original results are not satisfactory.  Another architectural choice is the size of the input vector. This is often a  simple choice, which is determined by the training data. However, there  are times when input vectors in the training data have redundant or irrel- evant elements. When the dimension of the potential input vector is very  large, it is sometimes advantageous to eliminate redundant or irrelevant  elements. This can reduce the required computation and can assist in pre-  22-12   Training the Network  Input Selection  Training the Network  venting overfitting during training. The input selection process for nonlin- ear networks can be quite difficult, and there is no perfect solution. The  Bayesian regularization method  Eq.  13.23   can be modified to assist in  input selection. It is possible to have different   parameters for different  sets of weights. For example, we can let each column of the weight matrix  in the first layer of a multilayer network have its own  given element of the input vector is irrelevant, then the corresponding    parameter would become large and force all elements of that column of the  weight matrix to be small. That element could then be eliminated from the  input vector.   parameter. If a        Another technique that can assist in pruning the input vector is a sensitiv- ity analysis of the trained network. In the Sensitivity Analysis section on  page 22-28 we discuss this technique.  After the data has been prepared, and the network architecture has been  selected, we are ready to train the network. In this section, we will discuss  some of the decisions that need to be made as part of the training process.  This includes the method for initializing the weights, the training algo- rithm, the performance index, and the criterion for stopping training.  Weight Initialization Before training the network, we need to initialize the weights and biases.  The method we use will depend on the type of network. For multilayer net- works, the weights and biases are generally set to small random values   e.g., uniformly distributed between -0.5 and 0.5, if the inputs are normal- ized to fall between -1 and 1 . As we discussed in Chapter 12, if we set the  weights and biases to zero, the initial condition may fall on a saddle point  of the performance surface. If we make the initial weights large, the initial  condition can fall on a flat part of the performance surface, caused by sat- uration of the sigmoid transfer functions.  There is another approach to setting the initial weights and biases for a  two-layer network. It was introduced by Widrow and Nguyen [WiNg90].  The idea is to set the magnitude of the weights in the first layer so that the  linear region of each sigmoid function covers   of the range of the input.  The biases are then randomly set, so that the center of each sigmoid func- tion falls randomly in the input space. The details of the method are as fol- lows  assuming the inputs to the network have been normalized to values  between -1 and 1 . W1 w1  , to have a random direction and a magnitude of  Set row i of   1 S1  ,     i  w1 i  0.7 S1  1 R  .  =  22-13  22   22 Practical Training Issues  Set   bi   to a uniform random value between   w1  i–   and   w1  i  .  For competitive networks, the weights can also be set as small random  numbers. Another possibility is to randomly select some of the input vec- tors in the training set to become initial rows of the weight matrix. In this  way, we can be sure that the initial weights will fall within the range of the  input vectors, so we will be less likely to have dead units, as described in  Chapter 16. For the SOM, dead units are not a problem. The initial neigh- borhood size is set large enough so that all neurons will have the opportu- nity to learn during the initial stages of training. This will move all weight  vectors into the appropriate region of the input space. Training can con- verge faster, however, if rows of the weight matrix are initially placed in  the active input region.  Choice of Training Algorithm For multilayer networks, we generally use gradient- or Jacobian-based al- gorithms, as described in Chapter 12. These algorithms can be implement- ed in either batch mode or sequential  also known as incremental, pattern  or stochastic  mode. For example, in the sequential form of steepest descent   see Eq.  11.13   we update the weights after each input is presented to the  network. In batch mode  see page 12-7 , all of the inputs are presented to  the network, and the total gradient is computed by summing the gradients  for each input, before the weights are updated. In some situations, the se- quential form is preferred — for example, when on-line or adaptive opera- tion is required. However, many of the more efficient optimization  algorithms  e.g., conjugate gradient and Newton’s methods  are inherently  batch algorithms.   For multilayer networks with up to a few hundred weights and biases that  are being used for function approximation, the Levenberg-Marquardt algo- rithm  see Eq.  12.31   is usually the fastest training method. When the  number of weights reaches a thousand or more, the Levenberg-Marquardt  algorithm is not as efficient as some of the conjugate gradient algorithms.  This is mainly because the matrix inverse calculation scales geometrically  with the number of weights. For large networks, the Scaled Conjugate Gra- dient algorithm of [Moll93] is very efficient. This method is also attractive  for pattern recognition problems. The Levenberg-Marquardt algorithm  does not work as well for pattern recognition, in which the sigmoid transfer  functions in the final layer are operating well outside the linear region.  Of the algorithms that can be implemented in sequential mode, the fastest  are the extended Kalman filter algorithms. These algorithms are closely re- lated to sequential implementations of the Gauss-Newton algorithm. Un- like the batch version of Gauss-Newton, they do not require an inversion of  the approximate Hessian matrix. The decoupled extended Kalman filter  implementation of [PuFe97] appears to be the most efficient of these types  of algorithms.  22-14   Training the Network  Stopping Criteria For most applications of neural networks, the training error never converg- es identically to zero. The error can reach zero for the perceptron network,  assuming a linearly separable problem, as we showed in Chapter 4. How- ever, it is unlikely to happen for multilayer networks. For this reason, we  need to have other criteria for deciding when to stop the training.  We can stop the training when the error reaches some specified limit. How- ever, it is usually difficult to know what an acceptable error level is. The  simplest criterion is to stop the training after a fixed number of iterations.  Because it is also difficult to know how many iterations will be required,  the maximum iteration number is generally set reasonably high. If the  weights have not converged after the maximum number of iterations has  been reached, we can restart training, using the final weights from the first  run as initial conditions for the restart.  We will talk more about how to tell  if a network has converged in the Post-Training Analysis section on page  22-18.   Another stopping criterion is the norm of the gradient of the performance  index. If this norm reaches a sufficiently small threshold, then the training  can be stopped. Since the gradient should be zero at a minimum of the per- formance index, this criterion will stop the algorithm when it gets close to  the minimum. Unfortunately, as we have seen in Chapter 12, the perfor- mance surface for multilayer networks can have many flat regions, where  the norm of the gradient will be small. For this reason, the threshold for  the minimum norm should be set to a very small value  e.g.,  square error performance indices, with normalized targets , so that the  training does not end prematurely.   for mean   10 6–  We can also stop the training when the reduction in the performance index  per iteration becomes small. As with the norm of the gradient, this criterion  can stop the training too early. During the training of multilayer networks,  the performance can remain almost constant for a number of iterations be- fore dropping suddenly. When training is complete, it is useful to view the  training performance curve on a log-log scale, as in Figure 22.5, to verify  convergence.  If we are using early stopping, as discussed in Chapter 13, to prevent over- fitting, then we will stop the training when the performance on the valida- tion set increases for a set number of iterations. In addition to preventing  overfitting, this stopping procedure also provides a significant reduction in  computation; for most practical problems, the validation error will increase  before any of the other stopping criteria are reached.  As shown in Figure 22.1, neural network training is an iterative process.  Even after the training algorithm has converged, post-training analysis  may suggest that the network be modified and retrained. In addition, sev- eral training runs should be made for each potential network to ensure that  a global minimum has been reached.  22-15  22   22 Practical Training Issues     r o r r E   e r a u q S m u S     103  102  101  100  10−1  10−2  10−3    100  101  102  Epochs  103  Figure 22.5  Typical Training Performance Curve  The previous stopping criteria apply mainly to gradient-based training.  When training competitive networks, like the SOFM, there is no explicit  performance index or gradient to monitor for convergence. The training  stops only when the maximum number of iterations has been reached. For  SOFMs, the learning rate and the neighborhood size are decreased over  time. Typically, the neighborhood size is reduced to zero by the completion  of training, so the maximum iteration number determines the end of train- ing, as well as the rate of decrease in neighborhood size and learning rate.  This is therefore a very important parameter. It is generally chosen to be  more than ten times the number of neurons in the network. This is an ap- proximate number, and the network needs to be analyzed at the completion  of training to determine if the performance is satisfactory.  This will be dis- cussed in the Post-Training Analysis section on page 22-18.  The network  may need to be trained several times with different training lengths to  achieve a satisfactory result.  Choice of Performance Function For multilayer networks, the standard performance index is mean square  error. When all inputs in the training set are equally likely to occur, then  this can be written  F x   =    tq  aq–  T tq    aq–    ,   22.4   1 QSM-----------  Q    q  1=  or  22-16   Training the Network  F x   =    ti q  ai q–  2  .  1 QSM-----------  Q    SM   q  1=  i  1=   22.5   The scale factor that occurs outside the sum has no effect on the location of  the optimum weights. Therefore, the sum square error performance index  will produce the same weights as the mean square error performance in- dex. However, the appropriate scaling can be useful when comparing errors  on data sets of different size.  While mean square error is the most common performance index, there are  others that have been used. For example, we could use mean absolute er- ror. This would be similar to Eq.  22.5 , except that the absolute value of  the error would be used, instead of the square of the error. This perfor- mance index is generally less sensitive to one or two large errors in the data  set, and is therefore somewhat more robust to outliers than is the mean  square error algorithm. This concept can be extended to any power of the  absolute error, as follows  F x   =  1 QSM-----------  Q    SM   q  1=  i  1=  ti q  ai q–  K  ,   22.6   K  2=   corresponds to mean square error and   where   corresponds to  mean absolute error. The general error given by Eq.  22.6  is referred to as  the Minkowski error.  1=  K  As we saw in Chapter 13, the mean square performance index can be aug- mented with the mean square weights, to produce a regularized perfor- mance index, which is used to prevent overfitting. The Bayesian  regularization algorithm is an excellent training method for preventing  overfitting. It uses a regularized performance index, and uses Bayesian  methods to select the regularization parameter. See Chapter 13 for details.  Mean square error works well for function approximation problems, in  which the target values are continuous. However, in pattern recognition  problems, where the targets take on discrete values, other performance in- dices might be more appropriate. One performance index that has been  proposed for classification problems is cross-entropy [Bish95]. Cross-entro- py is defined as  F x   –=  Q  SM   q  1=  i  1=  ti q  ln  ai q -------- ti q  .   22.7   Here we assume that the target values are 0 and 1, and they identify which  of the two classes the input vector belongs to. The softmax transfer function   22-17  22  Cross-Entropy   22 Practical Training Issues  is generally used in the last layer of the neural network, if the cross-entro- py performance index is used.  As a closing note on the choice of performance index, recall from Chapter  11 that the backpropagation algorithm for computing training gradients  will work for any differentiable performance index. If you change the per- formance index, you only need to change the initialization of the sensitivi- ties in the last layer  see Eq.  11.37  .  Multiple Training Runs and Committees of Networks A single training run may not produce optimal performance, because of the  possibility of reaching a local minimum of the performance surface. It is  best to restart the training at several different initial conditions and select  the network that produces the best performance. Five to ten restarts will  almost always produce a global optimum [HaBo07].  There is another way to perform multiple training runs and make use of all  of the networks that have been trained. This is called the committee of net- works [PeCo93]. For each training session, the validation set is randomly  selected from the training data, and a random set of initial weights and bi- ases is chosen. After N networks have been trained, all of the networks are  used together to form a joint output. For function approximation networks,  the joint output can be a simple average of the outputs of each network. For  classification networks, the joint output can be the result of a vote, in which  the class that is chosen by the majority of the networks is selected as the  output of the committee. The performance of the committee will usually be  better than even the best of the individual networks. In addition, the vari- ation in the outputs of the individual networks can be used to provide error  bars, or confidence levels, for the committee output.  Post-Training Analysis  Before using a trained neural network, we need to analyze it to determine  if the training was successful. There are many techniques for post-training  analysis. We will discuss some of the more common ones. Since these tech- niques vary, depending on the application, we will organize them according  to these four application areas: fitting, pattern recognition, clustering and  prediction.  Fitting One useful tool for analyzing neural networks trained for fitting problems  is a regression between the trained network outputs and the corresponding  targets. We fit a linear function of the form  aq  =  mtq  + +  c  q  ,   22.8   22-18   Post-Training Analysis  m   and   where  tq aq error of the regression.    is a target value,   c   are the slope and offset, respectively, of the linear function,   is the residual    is a trained network output, and   q  The terms in the regression can be computed as follows:  Q   aq   t– tq   q ----------------------------------------------  a–  1=    ,  mˆ  =  Q    q  1=  t– tq  2  cˆ  =  a mˆ t  –  ,  Q  Q  a  =  1 ---- Q  q  1=  aq  ,   t  =  1 ---- Q  q  1=  tq  .   22.9    22.10    22.11   Outliers  tq=  Figure 22.6 shows an example regression analysis. The blue line represents  the linear regression, the thin black line represents the perfect match  , and the circles represent the data points. In this example, we can  aq see that the match is pretty good, although not perfect. The next step would  be to investigate data points that fall far from the regression line. For ex- ample, there are two points around   that seem to be out- liers. We would investigate these points to see if there was a problem with  the data. This could be a bad data point, or it could be located far from other  training points. In the latter case, we would need to collect more data in  that region.   and   17=  27=  a  t  In addition to computing the regression coefficients, we often also compute  the correlation coefficient between the  , which is also known as  the    value:   and   aq  tq  R  Q   aq   t– tq   q ----------------------------------------------  a–    Q 1–  stsa  1=   ,  R  =   22.12   where  where  st  =  t– tq     and   sa  =    aq a–    .   22.13   1  ------------- Q 1–  Q    q  1=  1  ------------- Q 1–  Q    q  1=  22-19  22   22 Practical Training Issues     a  50  40  30  20  10  0    0  5  10  15  20  25  30  35  40  45  50  t  Figure 22.6  Regression Between Trained Network Outputs and Targets  R   value can generally range from -1 to 1, but we would expect it to be  The  close to 1 for our neural network application. If  , then all of the data  R points will fall exactly on the regression line. If  , then the data will  R not be concentrated around the regression line, but will be randomly scat- tered. For the data of Figure 22.6,  . We can see that the data does  not fall exactly on the regression line, but the variation is relatively small.  1= 0=  0.965  R  =  .   The square of the correlation coefficient,  , is sometimes used instead of  R R2  represents the proportion of the variability in a data set that is ac- counted for by the linear regression, and is also referred to as the coefficient  of determination. For the data of Figure 22.6,   0.931  R2  =  .  R2  R2  R   or    values are significantly less than 1, then the neural net-  When the  work has not done a good job of fitting the underlying function. A close  analysis of the scatter plot may be helpful in determining problems in the  fit. For example, we might find that when the targets are large there is  more spread in the scatter plot.  This is not the case in Figure 22.6.  We  might also notice that there are fewer data points with large targets. This  would indicate that we need to have more data points in the training set for  these target values.  Recall that the original data set was divided into training, validation  if  early stopping is used  and testing subsets. The regression analysis should  be performed on each subset individually, as well as the full data set. Dif- ferences between the subsets would indicate overfitting or extrapolation.  For example, if the training set shows accurate fitting, but the validation  and test results are poor, then this would indicate overfitting  which can  sometimes happen, even when early-stopping is used . In this case, we  might reduce the size of the neural network and retrain. If both the train- ing and validation results are good, but the testing results are poor, then   22-20   Post-Training Analysis  this could indicate extrapolation  where the testing data falls outside the  training and validation data . In this case, we need to provide more data  for training and validation. If the results for all three data sets are poor,  then it might be necessary to increase the number of neurons in the net- work. Another choice is to increase the number of layers in the network. If  you start with a single hidden layer, and the results are poor, then a second  hidden layer could be helpful. First, try more neurons in the single hidden  layer, and then increase the number of layers.  In addition to the regression scatter plot, another tool that can identify out- liers is a histogram of the errors, as shown in Figure 22.7. The y-axis rep- resents the number of errors that falls within each interval on the x-axis.  Here we can see that two errors are greater than 8. These represent the  same two errors that we identified as outliers in Figure 22.6.  140  120  100  80  60  40  20  0 −8  Confusion Matrix  −6  −4  0  2  4  6  8  10  12  −2  e  a–=  t  Figure 22.7  Histogram of Network Errors  Pattern Recognition For pattern recognition problems, the regression analysis is not as useful  as it is for fitting problems, since the target values are discrete. However,  there is an analogous tool - the confusion  or misclassification  matrix. The  confusion matrix is a table whose columns represent the target class and  whose rows represent the output class. For example, Figure 22.8 shows a  sample confusion matrix in which there were 214 data points. There were  41 input vectors that belonged to Class 1 and were correctly classified as  Class 1. There were 162 input vectors that belonged to Class 2 and were  correctly classified as Class 2. The correctly classified inputs show in the  diagonal cells of the confusion matrix. The off-diagonal cells show misclas- sified inputs. The lower left cell shows that four inputs from Class 1 were  misclassified by the network as Class 2. If Class 1 is considered a positive  outcome, then the lower left cell represents false negatives, which are also   False Negative  22-21  22   22 Practical Training Issues  False Positive  called Type II errors. The upper right cell shows that one input from Class  2 was misclassified by the network as Class 1. This would be considered a  false positive or a Type I error.  ROC Curve  Figure 22.8  Sample Confusion Matrix  Another useful tool for analyzing a pattern recognition network is called  the Receiver Operating Characteristic  ROC  curve. To create this curve, we  take the output of the trained network and compare it against a threshold  which ranges from -1 to +1  assuming a tansig transfer function in the last  layer . Inputs that produce values above the threshold are considered to be- long to Class 1, and those with values below the threshold are considered  to belong to Class 2. For each threshold value, we count the fraction of true  positives and false positives in the data set. This pair of numbers produces  one point on the ROC curve. As the threshold is varied, we trace the com- plete curve, as shown in Figure 22.9.  The ideal point for the ROC curve to pass through would be  0,1 , which  would correspond to no false positives and all true positives. A poor ROC  curve would represent a random guess, which is represented by the diago- nal line in Figure 22.9, which passes through the point  0.5,0.5 .   Confusion Matrix  47  22.0%  1  0.5%  97.9% 2.1%  4  1.9%  162 75.7%  97.6% 2.4%  1  2  s s a l C    t u p t u O  92.2% 7.8%  1  99.4% 0.6%  97.7% 2.3%  2  Target Class  22-22   Post-Training Analysis  e t a R   e v i t i s o P   e u r T  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0 0  Quantization Error  Topographic Error  0.2  0.4  0.6  0.8  False Positive Rate  1  Figure 22.9  Receiver Operating Characteristic Curve  Clustering The SOM is the most commonly used network for clustering. There are sev- eral measures of SOM performance. One is quantization error. This is the  average distance between each input vector and the closest prototype vec- tor. It measures the map resolution. This can be made artificially small, if  we use a large number of neurons. If there are as many neurons as input  vectors in the data set, then the quantization error could be zero. This  would represent overfitting. If the number of neurons is not significantly  smaller than the number of input vectors, then the quantization error is  not meaningful.  Another measure of SOM performance is topographic error. This is the pro- portion of all input vectors for which the closest prototype vector and the  next closest prototype vector are not neighbors in the feature map topology.  Topographic error measures the preservation of the topology. In a well- trained SOM, prototypes that are neighbors in the topology should also be  neighbors in the input space. In this case, the topographic error should be  zero.  Distortion Measure  The performance of the SOM can also be assessed by the distortion mea- sure:  Q  S    Ed  =  q  1=  i  1=  hicq wi  pq–  2  ,   22.14   where  that is closest to the input vector    is the neighborhood function, and   hij  :  cq  pq   is the index of the prototype   22  22-23   22 Practical Training Issues  cq  =  arg  minj wj    pq–    .   22.15   For the simplest neighborhood function,  within some pre-specified neighborhood radius of prototype j, and equal to  zero otherwise. It is also possible to have neighborhood functions that de- crease continuously, such as the Gaussian function:   is equal to 1 if prototype i is   hij  hij  =  exp      2  –  ---------------------------   wi wj– 2d2  ,   22.16   where   d   is the neighborhood radius.  Prediction As we discussed earlier, one application of neural networks is the predic- tion of the future values of some time series. For prediction problems, we  use dynamic networks, such as the focused time-delay neural network  shown in Figure 22.3. There are two important concepts that are used  when analyzing a trained prediction network:  the prediction errors should not be correlated in time, and  the prediction errors should not be correlated with the input sequence.  If the prediction errors were correlated in time, then we would be able to  predict the prediction errors and, therefore, improve our original predic- tion. Also, if the prediction errors were correlated with the input sequence,  then we would also be able to use this correlation to predict the errors.   1.  2.  Autocorrelation Function  In order to test the correlation of the prediction errors in time, we can use  the sample autocorrelation function:  Re    =  e t e t  +    .   22.17   1  ------------ Q –  Q –    t  1=  White Noise  Re    If the prediction errors are uncorrelated  white noise , then we would ex- pect   is  close to zero, we can set an approximate 95% confidence interval [BoJe96]  using the range   to be close to zero, except when   . To determine if   Re    0=    2Re 0  -----------------  –  Q    Re      2Re 0  -----------------  .  Q   22.18   e t    is white, if   We can say that  . This  concept is illustrated in Figure 22.10 and Figure 22.11. Figure 22.10 shows  a sample autocorrelation function for the prediction errors of a network  that has not been adequately trained. We can see that the autocorrelation    satisfies Eq.  22.18  for   Re    0    22-24   Post-Training Analysis  function does not fall totally within the bounds defined by Eq.  22.18 ,  which are indicated by the dashed lines in the figure. Figure 22.11 shows  the corresponding autocorrelation function when a network has been suc- cessfully trained.    falls within the bounds, except at   0=    .  Re    Correlation in the prediction errors can indicate that the length of the  tapped delay lines in the network should be increased.  −0.1  −40  −20  0  20  40  Figure 22.10    Re     for Inadequately Trained Network  0.2  0.1  0  0.03  0.02  0.01  0  −40  −20  0  20  40  Cross-correlation Function  To test the correlation between the prediction errors and the input se- quence, we can use the sample cross-correlation function:  Figure 22.11    Re     for Successfully Trained Network  22-25  22   22 Practical Training Issues  Rpe    =  p t e t  +    .   22.19   1  ------------ Q –  Q –    t  1=  If there is no correlation between the prediction errors and the input se- quence, then we would expect  mine if  interval [BoJe96] using the range  . To deter-  is close to zero, we can set an approximate 95% confidence    to be close to zero for all   Rpe    Rpe      2 Re 0  Rp 0  ----------------------------------------  –    Rpe      2 Re 0  Rp 0  ----------------------------------------  .  Q  Q   22.20   This concept is illustrated in Figure 22.12 and Figure 22.13. Figure 22.12  shows a sample cross-correlation function for the prediction errors of a net- work that has not been adequately trained. We can see that the cross-cor- relation function does not fall totally within the bounds defined by Eq.   22.20 , which are indicated by the dashed lines. Figure 22.13 shows the  corresponding cross-correlation function when a network has been success- fully trained.    falls within the bounds for all   .    Rpe    −0.1  −40  −20  0  20  40  Figure 22.12    Rpe     for Inadequately Trained Network  When using a NARX network, correlation between the prediction error and  the input can suggest that the lengths of the tapped delay lines in the input  and feedback paths should be increased.  0.2  0.1  0  22-26   Post-Training Analysis  0.02  0.01  0  −0.01  −0.02  −40  −20  0  20  40  Figure 22.13    Rpe     for Successfully Trained Network  Overfitting and Extrapolation Recall from Chapter 13 that the total data set is divided into three parts:  training, validation and testing. The training set is used to calculate gradi- ents and to determine weight updates. The validation set is used to stop  training before overfitting occurs.  If Bayesian regularization is used, then  the validation set may be merged with the training set.  The test set is used  to predict future performance of the network. The test set performance is  the measure of network quality. If, after a network has been trained, the  test set performance is not adequate, then there are usually four possible  causes:     the network has reached a local minimum,    the network does not have enough neurons to fit the data,    the network is overfitting, or    the network is extrapolating.  The local minimum problem can almost always be overcome by retraining  the network with five to ten random sets of initial weights. The network  with minimum training error will generally represent a global minimum.  The other three problems can generally be distinguished by analyzing the  training, validation and test set errors. For example, if the validation error  is much larger than the training error, then overfitting has probably oc- curred. Even though early stopping is used, it is possible to have some over- fitting, if the training occurs too quickly. In this case, we can use a slower  training algorithm to retrain the network.  22-27  22   22 Practical Training Issues  If the validation, training and test errors are all similar in size, but the er- rors are too large, then it is likely that the network is not powerful enough  to fit the data. In this case, we should increase the number of neurons in  the hidden layer and retrain the network. If Bayesian regularization is  used, this situation is indicated by the effective number of parameters be- coming equal to the total number of parameters. When the network is large  enough, the effective number of parameters should remain below the total  number of parameters.  If the validation and training errors are similar in size, but the test errors  are significantly larger, then the network may be extrapolating. This indi- cates that the test data fall outside the range of the training and validation  data. In this case, we need to get more data. You can merge the test data  into the training validation data and then collect new test data. You should  continue to add data until the results on all three data sets are similar.  If training, validation and test errors are similar, and the errors are small  enough, then we can put the multilayer network to use. However, we still  need to be careful about the possibility of extrapolation. If the multilayer  network inputs are outside the range of the data with which it was trained,  then extrapolation will occur. It is difficult to guarantee that training data  will encompass all future uses of a neural network.   One method for detecting extrapolation is to train a companion competitive  network to cluster the input vectors in the multilayer network training set.  Then, when an input is applied to the multilayer network, the same input  is applied to the companion competitive network. When the distance of the  input vector to the nearest prototype vector of the competitive network is  larger than the distance from the prototype to the most distant member of  its cluster of inputs in the training set, we can suspect extrapolation. This  technique is referred to as novelty detection.  Sensitivity Analysis After a multilayer network has been trained, it is often useful to assess the  importance of each element of the input vector. If we can determine that a  given element of the input vector is unimportant, then we can eliminate it.  This can simplify the network, reduce the amount of computation and help  prevent overfitting. There is no one method that can absolutely determine  the importance of each input, but a sensitivity analysis can be helpful in  this regard. A sensitivity analysis computes the derivatives of the network  response with respect to each element of the input vector. If the derivative  with respect to a certain input element is small, then that element can be  eliminated from the input vector.  Because the multilayer network is nonlinear, the derivative of the network  output with respect to an input element will not be constant. For each input  vector in the training set, the derivatives will be different. For this reason,  we can’t use a single derivative to determine sensitivity. One option would   22-28  Novelty Detection   Post-Training Analysis  be to take the average of the absolute derivatives, or else the rms deriva- tives, over the entire training set. Another option would be to compute the  derivative of the sum square error with respect to each element of the input  vector. Each of these will compute a single derivative for each element of  the input vector. The last computation can be performed with a simple vari- ation of the backpropagation algorithm  see Eq.  11.44  to Eq.  11.47  . Re- call from Eq.  11.32  that  m si    Fˆ --------- m ni  ,  Fˆ  where  with respect to an element of the input vector, using the chain rule:   is a single error squared. We want to convert this to a derivative   We know that  therefore, Eq.  22.22  becomes  Fˆ pj-------  =  S1   i  1=  Fˆ -------- 1 ni    1 ni pj--------  =  S1   i  1=  1 si    1 ni pj--------  .  1 ni  =  1 pj wi j  1+ bi  ,  R    j  1=  Fˆ pj-------  =  S1   i  1=  Fˆ -------- 1 ni    1 ni pj--------  =  S1   i  1=  1 wi j 1 si  .  In matrix form, we can write this as  Fˆ p------  W1    Ts1  .  =   22.21    22.22    22.23    22.24    22.25   This will be the derivative for a single squared error. To get the derivative  of the sum square error, we sum the individual derivatives for each single  squared error. The resulting vector will contain the derivatives of the sum  square error for each element of the input vector. If we find that some of  these derivatives are much smaller than the maximum derivative, then we  can consider removing those inputs. After removing the potentially irrele- vant inputs, we retrain the network and compare the performance with the  original network. If the performance is similar, then we accept the simpli- fied network.  22-29  22   22 Practical Training Issues  Epilogue  While previous chapters have focused on the fundamentals of particular  network architectures and training rules, this chapter has discussed some  practical aspects of neural network training. Neural network training is an  iterative process involving data collection and preprocessing, network ar- chitecture selection, network training and post-training analysis.  The next five chapters will demonstrate some of these practical aspects, as  we present some real-world case studies. The case studies will cover a va- riety of applications, including function fitting, density estimation, pattern  recognition, clustering and prediction.  22-30   Further Reading  Further Reading  [Bish95]   [BoJe94]   [HaBo07]  [HeOh97]  C.M. Bishop, Neural Networks for Pattern Recognition, Ox- ford University Press,1995. This well-written and well-organized textbook presents  neural networks from a statistical perspective.  G.E.P. Box, G.M. Jenkins, and G.C. Reinsel, Time Series  Analysis: Forecasting and Control, 4th Edition, John Wiley  & Sons, 2008.  This is a classic text on time series analysis. It focuses on  practical aspects, rather than theoretical derivations.  L. Hamm, B. W. Brorsen and M. T. Hagan, “Comparison of  Stochastic Global Optimization Methods to Estimate Neu- ral Network Weights,” Neural Processing Letters, Vol. 26,  No. 3, December 2007.  This paper demonstrates that using multiple restarts of a  local optimization procedure, like steepest descent or con- jugate gradient, produces results that are comparable to  global optimization methods, but with less computation.  B. Hedén, H. Öhlin, R. Rittner, L. Edenbrandt, “Acute My- ocardial Infarction Detected in the 12-Lead ECG by Artifi- cial Neural Networks,” Circulation, vol. 96, pp. 1798–1802,  1997.  Describes the use of neural networks in detecting myocar- dial infarctions, using the electrocardiogram.  [Joll02]  I.T. Jolliffe, Principal Component Analysis, Springer Series  in Statistics, 2nd ed., Springer, NY, 2002.  The most popular text on principal component analysis.  [LeCu98]   Y. LeCun, L. Bottou, G. B. Orr, K.-R. Mueller, “Efficient  BackProp,” Lecture Notes in Comp. Sci., vol. 1524, 1998.  [Moll93]  This paper presents practical tips that improve the train- ing of multilayer networks.  M. Moller, “A scaled conjugate gradient algorithm for fast  supervised learning,” Neural Networks, vol. 6, pp. 525-533,  1993.  The scaled conjugate gradient algorithm presented in this  paper converges quickly, and with a minimum amount of  memory requirements.  22-31  22   22 Practical Training Issues  [NgWi90]  [PeCo93]  [PuFe97]  [RaMa05]   [ScSm99]  D. Nguyen and B. Widrow, “Improving the learning speed  of 2-layer neural networks by choosing initial values of the  adaptive weights,” Proceedings of the IJCNN, vol. 3, pp.  21–26, July 1990.  This paper describes a procedure for setting the initial  weights and biases for the backpropagation algorithm. It  uses the shape of the sigmoid transfer function and the  range of the input variables to determine how large the  weights should be, and then uses the biases to center the  sigmoids in the operating region. The convergence of back- propagation is improved significantly by this procedure.  M. P. Perrone and L. N. Cooper, “When networks disagree:  Ensemble methods for hybrid neural networks,” in Neural  Networks for Speech and Image Processing, R. J. Mam- mone, Ed., Chapman-Hall, pp. 126-142, 1993.  This paper describes how you can combine the outputs of a  committee of networks to produce results that are more ac- curate than any of the individual networks.  G.V. Puskorius and L.A. Feldkamp, “Extensions and en- hancements of decoupled extended Kalman filter training,”  Proceedings of the 1997 International Conference on Neural  Networks, vol. 3, pp. 1879-1883, 1997.  The extended Kalman filter algorithm described in this pa- per is one of the faster sequential algorithms for neural net- work training.  L.M. Raff, M. Malshe, M. Hagan, D.I. Doughan, M.G. Rock- ley, and R. Komanduri, “Ab initio potential-energy surfaces  for complex, multi-channel systems using modified novelty  sampling and feedforward neural networks,” The Journal  of Chemical Physics, vol. 122, 2005.  This paper describes how neural networks can be used to  model molecular interactions.  B. Schölkopf, A. Smola, K.-R. Muller, “Kernel Principal  Component Analysis,” in B. Schölkopf, C. J. C. Burges, A.  J. Smola  Eds. , Advances in Kernel Methods-Support Vec- tor Learning, MIT Press Cambridge, MA, USA, pp. 327- 352, 1999.  This paper introduces a nonlinear version of principal com- ponent analysis using a kernel method.  22-32   Objectives  23 Case Study 1:  Function Approximation  23  Objectives Theory and Examples  Description of the Smart Sensor System Data Collection and Preprocessing Selecting the Architecture Training the Network Validation Data Sets  Epilogue Further Reading  23-1 23-2 23-2 23-3 23-4 23-5 23-7 23-10 23-11 23-12  Objectives  This chapter represents the first of a series of case studies with neural net- works. Neural networks can be used for a wide variety of applications, and  it would be impossible to provide case studies for each application. We will  limit our presentations to five important application areas: function ap- proximation  aka, nonlinear regression , density function estimation, pat- tern recognition  aka, pattern classification , clustering and prediction   aka, time series analysis, system identification, or dynamic modeling . For  each case study, we will step through the neural network design training  process.  In this chapter, we present a function approximation problem. For function  approximation problems, the training set consists of a set of dependent  variables  response variables  and one or more independent variables  ex- planatory variables . The neural network learns to create a mapping be- tween the explanatory variables and the response variables. In the case  study we consider in this chapter, the system in question is a smart sensor.  A smart sensor consists of one or more standard sensors that are coupled  with a neural network to produce a calibrated measurement of a single pa- rameter. In this chapter, we will consider a smart position sensor, which  uses the voltages coming from two solar cells to produce an estimate of the  location of an object in one dimension.  23-1   23 Case Study 1: Function Approximation  Theory and Examples  This chapter presents a case study in using neural networks for function  approximation. Function approximation consists of defining a mapping be- tween a set of input variables and a corresponding set of output variables.  For example, we might want to estimate the price of a home, based on char- acteristics of the neighborhood, such as tax rate, pupil teacher ratio in local  schools and crime rate. Another example would be estimating octane num- ber of a gasoline product at an oil refinery, based on measurements of re- actor temperatures and pressures [FoGi07]. In the case study presented in  this chapter, we will consider a smart position sensor system.  Description of the Smart Sensor System  Figure 23.1 illustrates the sensor arrangements for this case study. An ob- ject is suspended between a light source and two solar cells. The object  casts a shadow on the solar cells, which causes the voltage out of the solar  cells to decrease.   Shadow  v1  v2  Figure 23.1  Position Sensor Arrangement  y  v2   decreases, then    increases, first the voltage  v1  increases, and finally  v2  As the object position  voltage  demonstrated in Figure 23.2. Our objective is to determine the object posi- tion from measurements of the two voltages. Clearly this is a very nonlin- ear relationship, so a multilayer network will be needed to learn the  mapping. This is a classic type of function approximation problem, in which  we are trying to learn the inverse of a function. The forward function is the    decreases, then the   increases. This is   v1  y  23-2   Data Collection and Preprocessing  mapping from  v2   to   .   y  y   to   v1   and   v2  . We want to learn the mapping from   v1   and   23  v2  v1  y  Figure 23.2  Example Solar Cell Outputs vs. Object Position  Data Collection and Preprocessing  In order to collect data for this process, we took measurements of the two  solar cell voltages at a number of calibrated positions of the object. The ob- ject we used for these experiments was a table tennis ball. The data is dis- played in Figure 23.3. There are a total of 67 sets of measurements. Each  circle represents a voltage measurement at a calibrated position. The units  of position are inches, and the units of voltage are volts. The flat regions at  0 volts for each curve occur where the shadow of the ball completely covers  a sensor. If the shadow were large enough to cover both cells at the same  time, we would not be able to recover ball position from cell voltages.  7  6  5  4  3  2  1  0 0  v2  v1  23-3  1  2  3  y  Figure 23.3  Data Collected from Solar Cells   23 Case Study 1: Function Approximation  The next step is to divide the data into training, validation and test sets. In  this case, because we will be using the Bayesian regularization training  technique, we do not need to have a validation set. We did set aside 15% of  the data for testing purposes. To perform the division, we arranged the  data in order, according to object position, and then selected every sixth or  seventh point for testing. This resulted in 10 testing points. The testing  points are not used in any way for training the network, but after the net- work has been completely trained, we will use the testing data as an indi- cator of future network performance.  The input vector for network training will consist of the solar cell voltages  and the target will be ball position  p  =  ,  v1 v2  t  y=  .   23.1    23.2   The data were scaled using Eq.  22.1 , so that both the inputs and the tar- gets were in the range [-1,1]. The resulting scaled data is shown in Figure  23.4  S v2  0.5  1  0  −0.5  S v1  −1  −1  23-4  −0.5  0.5  1  0  yS  Figure 23.4  Scaled Data  Selecting the Architecture  Because the mapping between the solar cell voltages and the ball position  is highly nonlinear, we will use a multilayer network architecture to learn  the mapping. We know that there will be two elements in the input vector,    23  Training the Network  which is defined in Eq.  23.1 . The single target for the network is the ball  position, given in Eq.  23.2 .   Figure 23.5 shows the network architecture. We are using the tan-sigmoid  transfer function in the hidden layer, and a linear output layer. This is the  standard network for function approximation. As we discussed in Chapter  11, this network has been shown to be a universal approximator. There are  cases in which two hidden layers are used, but we normally try first with  one hidden layer. The number of neurons in the hidden layer,  , will de- pend on the function to be approximated. This is something that cannot  generally be known before training. We will have more to say about this in  the next section.  S1  Inputs  Tan-Sigmoid Layer  Linear Layer  p1  2 x 1  1  2  W1  S 1 x 2  b1  S 1 x 1  n1  S 1 x 1  a1  S 1 x 1  1  W2  1 S 1x  b2  1 x 1  S 1  a2  1 x 1  n2  1 x 1  1  1 a  =  tansig W p b  +       1  1  2 a  =  2 purelin W a b  +       2  1  Figure 23.5  Network Architecture  Training the Network  Before beginning the training, we initialized the network weights using the  method of Widrow and Nguyen described in Chapter 22. Then we used  Bayesian regularization to train the network. Bayesian regularization,  which we discussed in Chapter 13, is a very effective algorithm for training  multilayer networks to perform function approximation. This algorithm is  designed to train networks so that they generalize well, without the need  for a validation set. Because the validation set can then be added to the  training set, the performance is often better than that obtained with early  stopping.  In the next chapter, we will give an example of using early stop- ping, with a validation set.    Figure 23.6 illustrates the sum square error versus iteration number,  while using the Bayesian regularization training algorithm. We used a net- work with 10 neurons in the hidden layer     for this case. The net- work was trained for 100 iterations, at which time the performance was  changing very little.  10=  S1  23-5   23 Case Study 1: Function Approximation  E S S  102  101  100  10−1  10−2  10−3  100  23-6  101  Iterations  102  Figure 23.6  Sum Squared Error vs. Iteration Number    S1  10=     Training has converged after 100 iterations, but we want to ensure that we  have not fallen into a local minimum. For this reason, we want to retrain  the network several times, using different initial weights and biases.  We  use the Nguyen-Widrow initialization method described in Chapter 22.   Table 23.1. shows the final validation SSE for each of five different training  runs. We can see that all of the errors are similar, although the errors are  slightly smaller for runs 2, 4 and 5. Any of the weights from these five cases  would produce a satisfactory network. We will discuss this in more detail  in the next section.  1.121e-003  8.313e-004  1.068e-003  8.672e-004  8.271e-004  Table 23.1.  Final Training SSE for Five Different Initial Conditions      Recall from Chapter 13 that the Bayesian regularization algorithm com- putes a parameter  , which indicates the effective number of parameters  that are being used by the network. In Figure 23.7, we can see the variation  of   during training. It eventually converges to 17.4. There are a total of 41  parameters in this 2-10-1 network, so we are only using about 40% of the  weights and biases. For each of the five training runs discussed above,     converged to values between 17 and 20. This indicates that we might be  able to use a smaller network, if we are concerned about the amount of com- putation required to compute a network response.   23  Validation    20  15  10  5  0 0  20  40  60  80  100  Iterations  Figure 23.7  Effective Number of Parameters    S1  10=     To see whether or not a smaller network would be satisfactory, we trained  several networks with different numbers of hidden neurons. Since the ef- fective number of parameters is near 20, we would expect that a network  with five hidden neurons  21 weights and biases  might provide an ade- quate fit. Our experiments produced the results shown in Table 23.2. We  can see that the performances of all five networks are roughly equivalent,  except for the case where  , where the total number of parameters in  the network is only 13.   3=  S1  S1  3=  S1  5=  S1  8=  S1     10=     S1  20=  4.406e-003  9.227e-004  8.088e-004  8.672e-004  8.096e-004  Table 23.2.  Final Training SSE for Five Different Hidden Layer Sizes  The Bayesian regularization method allows us to train a network of almost  arbitrary size, and yet insure that only the required number of parameters  is effectively used. If we were concerned about the amount of time required  to compute the network output  e.g., for real-time applications , then we  would want to use the network with  . Otherwise, the original net- work with   is satisfactory. We don’t need to spend a lot of time find- ing the optimal number of neurons. The training algorithm will insure that  we do not overfit.  10=  5=  S1  S1  Validation  An important tool for network validation is a scatter plot of network out- puts versus targets, as shown in Figure 23.8  in normalized units . We ex-  23-7   23 Case Study 1: Function Approximation  pect that for a well trained network the points in the scatter plot will fall  close to the 45  output=target line. In this case, the fit is excellent. The fig- ure on the left shows the training data, while the figure on the right shows  the testing data. Because the testing data fit is as good as the training data  fit, we can be confident that the network did not overfit.  Training  Testing  1  0.5  a  0  −0.5  0.5  1  0  −0.5  a  −1 −1  −0.5  0.5  1  −1 −1  −0.5  0  t  0  t  0.5  1  Figure 23.8  Scatter Plots of Network Outputs vs. Targets - Training and Testing Sets  Another useful plot is a histogram of network error, as shown in Figure  23.9. This gives us an idea of the accuracy of the network. For this histo- gram, we have converted the network output back into units of inches. This  is done by applying the reverse of the target preprocessing function to the  network output. The reverse of Eq.  22.1 , for the targets, is given by  a  =  an    1+  .*  tmax tmin   -----------------------------  tmin  +  – 2   23.3   an  .*   represents an element-by-element multiplica-   is the original network output, which was trained to match the   where  normalized target, and  tion of two vectors. After the postprocessing operation of Eq.  23.3 , the re- sulting un-normalized output is subtracted from the raw targets, to  produce an error in inches. Figure 23.9 shows the distribution of these er- rors for both training and testing sets. We can see that almost all errors are  within one hundredth of an inch. This is within the accuracy of the original  measurements, so we cannot expect to do better.  23-8   Validation  23  15  10  5  y  2.5  3  2  1  1.5  0.5  0 0  0  −0.02  −0.01  0  0.01  0.02  Figure 23.9  Histogram of Position Errors  in Inches   Because this network has only two inputs, we can plot the trained network  response, which is shown in Figure 23.10.  This figure shows the response  from original unscaled inputs, in volts, to original unscaled output, in inch- es.  The blue circles indicate the path that is taken by the voltages as the  ball is moved. Notice that the Bayesian regularization training has pro- duced a smooth network response, even though the response is highly non- linear.   2  v2  4  0  2  6  4  v1  Figure 23.10  Network Response  Original Units   Another thing to notice about Figure 23.10 is that training data only falls  along the blue circles. The form of the network response in other regions is  of no importance for the operation of the smart sensor system, since the   23-9   23 Case Study 1: Function Approximation  network will never be used there. If the network were retrained, the shape  of the response away from the blue circles might be very different, even  though the response near the blue circles will always be the same. This con- cept is very important for many neural network applications. Often, during  normal network operation, only a small portion of the input space will be  accessed. The network only has to fit the underlying function in these re- gions where the network will be used. This means that the size of the data  set can be modest, even when the input dimension is large. Of course, in  these cases, it is critical that the training data span the full range of poten- tial network operation.  Data Sets  There are two data files associated with this case study:    ball_p.txt — contains the input vectors in the original data set    ball_t.txt — contains the target vectors in the original data set  They can be found with the demonstration software, which is described in  Appendix C.  23-10   Epilogue  Epilogue  23  This chapter has demonstrated the use of multilayer neural networks for  function approximation. This case study is representative of a large class  of neural network applications that could be termed “soft sensors” or  “smart sensors.” The idea is to use a neural network to fuse several raw  sensor outputs into a calibrated measurement of some key variable of in- terest.  A multilayer network with sigmoid transfer functions in the hidden layers  and linear transfer functions in the output layer is well suited to this type  of application, and Bayesian regularization is an excellent training algo- rithm to use in this situation.  In the next chapter, we will look at another neural network application —  probability estimation. We will also use multilayer neural networks for  that application, but we will change the transfer function in the output lay- er.  23-11   23 Case Study 1: Function Approximation  Further Reading  [FoGi07]  L. Fortuna, P. Giannone, S. Graziani, M. G. Xibilia, “Virtu- al Instruments Based on Stacked Neural Networks to Im- prove Product Quality Monitoring in a Refinery,” IEEE  Transactions on Instrumentation and Measurement, vol.  56, no. 1, pp. 95–101, 2007.  This paper describes the use of neural networks as soft sen- sors in a refinery. Measurements of reactor temperatures  and pressures are used to predict octane number in a gaso- line product.  23-12   Objectives  24 Case Study 2:  Probability Estimation  24  Objectives Theory and Examples  Description of the CVD Process Data Collection and Preprocessing Selecting the Architecture Training the Network Validation Data Sets  Epilogue Further Reading  24-1 24-2 24-2 24-3 24-5 24-7 24-9 24-12 24-13 24-14  Objectives  This chapter represents the second of a series of case studies with neural  networks. The previous chapter demonstrated the use of neural networks  for function approximation. In this chapter we use a neural network to es- timate a probability function.   Probability estimation is a special case of function approximation. In func- tion approximation we want the neural network to map between a set of in- put variables and a set of response variables. However, in the case of  probability estimation the response variables correspond to a set of proba- bilities. Since probabilities have certain special properties — they must al- ways be positive, and they must sum to 1 — we want the neural network  to enforce these conditions.   In the case study we consider in this chapter, the system in question is  chemical vapor deposition of diamond. A carbon dimer  a bound pair of car- bon atoms  is projected toward a diamond surface. We want to determine  the probabilities for various reactions based on characteristics of the pro- jected dimer. The input variables consist of such properties as translational  energy and incidence angle, and the response variables consist of the prob- abilities of the potential reactions, such as chemisorption and scattering.  24-1   24 Case Study 2: Probability Estimation  Theory and Examples  CVD  This chapter presents a case study in using neural networks for probability  estimation. Probability estimation consists of determining the probabilities  of certain events, based on a set of input variables. For example, we might  want to know the probabilities associated with a patient having a certain  disease, based on a set of laboratory tests. Another example would be de- termining the probability of a financial instrument going up in price, based  on a set of market conditions.  For this probability estimation case study, we will train a neural network  to estimate reaction rates in a chemical process. Chemical vapor deposition   CVD  of diamond is a process for making synthetic diamond. The idea is to  cause carbon atoms in a gas to settle on a substrate in crystalline form. In  order to study this process, scientists are often interested in reaction rates,  which will determine how quickly the diamond can be created. In this case  study, we will train a neural network to compute reaction rates as a carbon  dimer  a bound pair of carbon atoms  interacts with the crystalline dia- mond substrate.  We will begin by describing the CVD process and how simulated data can  be collected for this process. Then, we will show how a neural network can  be trained to learn the reaction probabilities. The details of the procedure  are described in [AgSa05].  Description of the CVD Process  During the CVD process, a carbon dimer is projected toward a diamond  substrate. For the purpose of this study, we will assume that the dimer can  react with the substrate in one of three ways: chemisorption  the atoms in  the dimer become bound to the substrate , scattering  the atoms bounce off  the substrate , or desorption  the atoms become bound to the substrate for  a period of time, but are then released . There is another possible reaction  that occurs with very small probability, but we will ignore it for this study.   See [AgSa05] for a full discussion.  We will train a neural network to esti- mate the probabilities of each of the reactions, based on various character- istics of the carbon dimer, which will be described below.  The notation we will use to define this interaction is illustrated in Figure  24.1. The black circle represents the carbon dimer, and the corresponding  directed line represents the direction of the initial velocity vector. The blue  star represents the location of the central carbon atom in the diamond sub- strate. The angle   denotes the angle of incidence, i.e., the angle between  the direction of the initial velocity vector of the carbon dimer and the per- pendicular on the surface  the z direction . The impact parameter b is de- fined as the distance between the location of the central atom and the point  of intersection of the initial velocity vector and the diamond surface  indi-    24-2   Data Collection and Preprocessing  cated by the origin of the axes in Figure 24.1 . The angle   represents the  angle between the x axis and the line from the origin to the central atom.    z   cid:4   b   cid:3   24  y  Figure 24.1  Notation for Carbon Dimer Diamond Substrate Interaction  Data Collection and Preprocessing  Data for training the neural network are obtained by molecular dynamics   MD  simulations. In MD, the motion of atoms and molecules in a material  under a given force are simulated, using known laws of physics to calculate  the forces on individual atoms [RaMa05]. For this case study, we use a total  of 324 atoms to model the CVD system. Out of these, 282 atoms of diamond  substrate are used to model the crystalline face with 40 atoms of hydrogen  on the top layer of the diamond surface, and 2 atoms in the C2 dimer. In  Figure 24.1, the  x,y  plane represents the location of the diamond sub- strate. Each of the carbon atoms on the top layer of the substrate, except  the central atom and boundary atoms, is capped by a hydrogen atom. Any  reactions will occur near the central atom.  For this study, we want to determine the dependence of probabilities for  chemisorption, scattering, and desorption on b,  , rotational velocity      of the C2 dimer. The initial C2 vibra- vrot tional energy is set equal to the zero-point energy and the temperature of  the lattice is maintained constant at 600 K [RaMa05].   , and translational velocity    vtrans     ,   This is an interesting function approximation problem, in that we don’t  have access to the true underlying reaction probabilities, which are un-  x  24-3   24 Case Study 2: Probability Estimation  known. We will obtain estimates of these probabilities by running Monte  Carlo simulation experiments. We will need some notation to help us keep  track of the various probabilities that we will work with. First, we will in- dicate the true underlying reaction probabilities by   refers   is the vector that characterizes the C2 dimer: to the reaction process, and   , where   PX p  X  p    p  =  .   24.1     b  vtrans vrot  The reaction process can be chemisorption   desorption   work will be indicated by  the Monte Carlo simulations will be indicated by    , or   . The probability estimates produced by the neural net- . The probability estimates obtained from    , scattering    NN p PX  D=  C=  S=  X  X  X    .    MC p PX  The Monte Carlo estimates are obtained by  MC p PX    =  NX ------ NT  ,   24.2   p  X  NX   is the number of MD trajectories that resulted in reaction    and  where   is the total number of trajectories computed. The results of a given tra- NT jectory depend upon a multitude of input variables. These include the pa- rameters included in  , as well as the initial orientation of the C2 dimer,  the angle defining the C2 rotational plane, the initial C2 vibrational energy  and its phase, the temperature of the system, and all of the variables that  define the vibrational phases of the diamond surface. Because we are only  interested in the effect of  ables are randomly set for each MD simulation, except that the initial C2  vibrational energy is set equal to the zero-point energy and the tempera- ture of the lattice is maintained constant at 600 K. Eq.  24.2  averages over  the trajectories to estimate the underlying true probabilities  .  As a  note of clarification here, we use the term Monte Carlo to refer to the set of  simulations that are obtained by setting a number of the variables to ran- dom values for each trajectory. We refer to the simulation of a single tra- jectory as an MD simulation, since the principles of molecular dynamics  are used to perform the computations.    on the reaction probabilities, the other vari-  PX p  p    This is a standard method used by chemists to estimate reaction probabil- ities. If they want to determine the effect of  , for example, on the proba- bilities, they must run a series of Monte Carlo simulations at each value of   that is of interest. This can be extremely time consuming. The required   number of Monte Carlo trials can be quite large, if an accurate reaction  probability is required. Our objective in this case study is to train a neural     24-4   24  Selecting the Architecture  network to learn the true reaction probabilities as a function of the param- eters in   p  .  To train a neural network, we need a set of target outputs. Since we do not  know the true underlying probabilities  , we will use the estimates ob- tained from the Monte Carlo simulations  Monte Carlo probabilities as being noisy versions of the true probabilities.  The neural network will need to interpolate these noisy values to produce  an accurate estimate of   without overfitting. This is a good applica- tion for our generalization procedures, which were discussed in Chapter 13.  . We can think of these    PX p MC p PX  PX p      MC p   p PX  The data set consists of 2000 different   input target pairs. Out  of these 2000 data points, 1400  70%  were randomly selected for training,  300  15%  for validation, and 300 for testing. For each trajectory, the    were generated randomly, using physically-appropriate distributions for  each variable [RaMa05]. A total of   different trajectories were run  to obtain each  . This means that 2000x50 trajectories were run to  create the entire data set.  MC p PX  50=  NT  p            vtrans   and    and radians per picosecond for   The original units of the inputs are radians for  , angstroms for b,  angstroms per picosecond for  . Be- fore presenting the input data to the network for training, they are scaled  using Eq.  22.1 , so that each element of the input vector ranges from -1 to  1. The targets have values that are always in the range 0 to 1, since they  represent probabilities. In the next section, we will describe a network ar- chitecture, in which the softmax transfer function of Eq.  22.3  is used in  the final layer. This transfer function produces outputs that range from 0  to 1, so the original unscaled targets will work fine.  vrot  Selecting the Architecture  We will use a multilayer network for this application. We know that there  will be five elements in the input vector, which is defined in Eq.  24.1 . The  target for the network can be a vector with three elements:  t  =  MC p PC MC p PS MC p PD        ,   24.3   MC p or we can use three different networks, each with a different  PX target. We have tried both possibilities, and the results are similar.     as a   In this case, there is an advantage to using the single network, with three  elements in the output vector. The three targets represent probabilities.  Therefore, they are always in the range 0 to 1, and they always sum to 1.   24-5   24 Case Study 2: Probability Estimation  This is an ideal situation for using the softmax transfer function of Eq.   22.3 , which is repeated here:  ai  =  f ni    =  exp  ni    exp  nj    .   24.4   S    j  1=  ai   is affected by all of the net inputs   This transfer function is different from others we have used, in that each  neuron output  .  In the other trans- fer functions, the net input  does not cause any substantial difficulties in network training. The back- propagation algorithm of Eq.  11.44  and Eq.  11.45  can still be used to  compute the gradient. However, the derivative of the transfer function is  no longer a diagonal matrix. The derivative of the softmax function has the  following form:   affected only the neuron output   .  This   nj  ai  ni  m a1  m ai       Sm   i  1=    m– a1    –  m ma2 a1  –  m maSm a1      F· m nm    =  –  m ma1 a2  m a2       Sm   i  1=    m– a2    m ai      –  m a1 m aSm  –  m a2 m aSm   aSm  m  m ai  m– aSm   24.5   –  m maSm a2         Sm   i  1=       The complete network architecture is shown in Figure 24.2. The input vec- tor of Eq.  24.1  has 5 elements. The output vector has 3 elements, which is  consistent with the target vector of Eq.  24.3 . The transfer function in the  hidden layer is the hyperbolic tangent sigmoid, and the softmax transfer  function is used in the output layer. The number of neurons in the hidden  , is yet to be determined. It depends on the complexity of the func- layer,  tion that we are trying to approximate, but we do not know at this point  how complex the function is. In general, the size of the hidden layer must  be determined as part of the training process. We must choose   so that  the network provides an accurate fit to the training data, without overfit- ting. We will discuss this selection in the next section.  S1  S1  24-6   Training the Network  Inputs  Tan-Sigmoid Layer  Softmax Layer  p1  5 x 1  1  5  W1  S 1 x 5  b1  S 1 x 1  n1  S 1 x 1  a1  S 1 x 1  1  W2  3 x S 1  b2  3 x 1  S 1  n2  3 x 1  3  1 a  =  tansig W p b  +       1  1  2 a  =  2 softmax W a b  +    2 1     Figure 24.2  Network Architecture  a2  3 x 1  24  Training the Network  We trained the network using the scaled conjugate gradient algorithm of  [Mill93]. Many other conjugate gradient or Levenberg-Marquardt algo- rithms, such as those discussed in Chapter 12, would have also worked  well. The targets for this problem have a significant amount of noise, so we  are not expecting extreme accuracy in the final fit. We used early stopping,  as described in Chapter 13, to prevent overfitting. We stopped the training  if the error on the validation set failed to improve over 25 iterations. A typ- ical training session is illustrated in Figure 24.3, which shows training and  validation MSE. The minimum of the validation performance was reached  at iteration 69. The algorithm continued for 25 more iterations, until iter- ation 94. Since the validation error was not reduced during those 25 itera- tions, the weights from iteration 69 were saved as the final trained values.     Training Validation  10−1  E S M  10−2  10−3    100  101  Iterations  102  Figure 24.3  Training and Validation Mean Square Error    S1  10=     24-7   24 Case Study 2: Probability Estimation  S1  10=  The results shown in Figure 24.3 represent a network with 10 neurons in  the hidden layer    . We need to verify that this is a reasonable num- ber. One indicator is a comparison of training and validation performance.  Table 24.1. shows the training and validation root mean square error   RMSE  for the trained network. We can see that training and validation  errors are roughly the same. The validation data was randomly selected  and was selected independently of the training set. Because the errors were  approximately the same on both sets, it appears that the network fit is con- sistent throughout the relevant input range, and no overfitting occurs.   Training RMSE  Validation RMSE  0.0496  0.0634  0.0586  0.0439  0.0659  0.0604  Table 24.1.  Comparison of Training and Validation RMSE for   S1  10=  It is also important to determine if the errors are as small as possible and  if the fit is adequate. We will have more to say about that in the next sec- tion, but at this point we can try fitting networks with different numbers  of hidden neurons. Table 24.2. shows the results of fitting a network with  two hidden neurons. Again, the training and validation errors are consis- tent, which indicates lack of overfitting, but the errors are higher than  those for   10=  S1  .  Training RMSE  Validation RMSE  0.0634  0.0669  0.0617  0.0627  0.0704  0.0618  Table 24.2.  Comparison of Training and Validation RMSE for   S1  2=  S1  Table 24.3. shows the results for  . The validation error is slightly  higher than the training error, which might indicate some overfitting. The  main point is that neither training nor validation errors are significantly  smaller for  . This indicates that ten hidden neu- rons are sufficient for this problem. We will investigate this further in the  next section.   than for   20=  10=  20=  S1  S1  PC p PS p PD p        PC p PS p PD p        24-8   Validation  Training RMSE  Validation RMSE  PC p PS p PD p        0.0432  0.0603  0.0569  0.0444  0.0643  0.0595  24  Table 24.3.  Comparison of Training and Validation RMSE for   S1  20=  There is one further step that we want to make as part of the training pro- cess. We want to ensure that we have not fallen into a local minimum. For  this reason, we want to retrain the network several times, using different  initial weights and biases.  We use the Nguyen-Widrow initialization  method described in Chapter 22.  Table 24.4. shows the final validation  MSE for each of five different training runs. We can see that all of the er- rors are similar, so we have reached a global minimum at each run. If one  error was significantly lower than the others, then we would use the  weights that obtained the lowest error.  3.074e-003  2.953e-003  3.031e-003  3.105e-003  3.050e-003  Table 24.4.  Final Validation MSE for Five Different Initial Conditions  We have determined that a neural network with ten neurons in the hidden  layer produces a reasonable response without overfitting. The next step is  to analyze the performance of the network. Depending on the results of  that analysis, we might adjust the network architecture or training data  and retrain the network.  Validation  An important tool for network validation is a scatter plot of network out- puts versus targets, as shown in Figure 24.4. Here we can see that there is  a strong linear relationship between the targets and the network outputs,  but there appears to be quite a bit of variation. We might expect that for a  well trained network the points in the scatter plot would fall exactly on the  outputs=target line. Why do we have so much variation in this plot? The  reason is that the targets of the network are not the true reaction probabil- ities,  . There is noise in the  targets.   , but the Monte Carlo estimates   MC p PX  PX p      24-9   24 Case Study 2: Probability Estimation  NN PS  1  0.8  0.6  0.4  0.2  0 0  NN PD  1  0.8  0.6  0.4  0.2  0 0  NN PC  1  0.8  0.6  0.4  0.2  0 0  0.2  0.4  0.6  0.8  1  0.2  0.4  0.6  0.8  1  0.2  0.4  0.6  0.8  1  MC PC  MC  PS  MC PD  Figure 24.4  Scatter Plot of Network Outputs vs. Targets    NT  50=     The relationship between  pect to have  MC  PX   and   PX   is such that ~95% of the time we ex-  where  PX  2–    MC PX PX    2+  ,    =  PX 1 PX–  ----------------------------  .  NT   24.6    24.7   This relationship is illustrated in Figure 24.5 for   NT  50=  .  MC  PX  1  0.8  0.6  0.4  0.2  0 0  24-10  0.2  0.4  0.6  0.8  1  PX  Figure 24.5  Expected Statistical Spread of   MC PX   for   NT  50=   Validation  By comparing Figure 24.4 with Figure 24.5, we can see that the spread in  the data is explained by the statistical variations in  . To further verify  this observation, we generated additional testing data, in which 500 Monte  Carlo trials were run to obtain each   . We applied  this testing data to the network that was trained on the original data set  with  . The resulting scatter plots are shown in Figure 24.6. Here  we can see that the spread has decreased dramatically from Figure 24.4,  even though the network has not changed. This means that the neural net- work is fitting the true probabilities   and not the statistical fluctua- tions in   MC p PX    i.e.,   PX p  MC PX  50=  500  NT  NT  =      .    MC p PX  24  NN PC  1  0.8  0.6  0.4  0.2  0 0  NN PS  1  0.8  0.6  0.4  0.2  0 0  NN PD  1  0.8  0.6  0.4  0.2  0 0  0.2  0.4  0.6  0.8  1  0.2  0.4  0.6  0.8  1  0.2  0.4  0.6  0.8  1  MC PC  MC  PS  MC PD  Figure 24.6  Scatter Plot of Network Outputs vs. Targets    NT  =  500     After the neural network has been trained, it becomes a simple matter to  investigate the effect of the input parameters on the reaction probabilities.  In Figure 24.7, we see the effect of the impact parameter b on the reaction  probabilities, as determined by the neural network. As the impact param- eter is increased, the probability of chemisorption decreases, while the  probabilities of scattering and desorption increase.  For this study, we have  set   to 0.004 radians per femtosecond,   to 5.4 radians,   and  vtrans   to 0.004 angstroms per femtosecond.    to 0.3 radians,   vrot    With standard methods, a study such as that shown in Figure 24.7 would  take thousands of simulations. The trained neural network has fully cap- tured the relationships between the parameters in   and the reaction prob- abilities. Therefore, we can perform arbitrary studies by simply computing  the network responses at a varying set of input points. Note that the net- work interpolated smoothly through a noisy set of data points to capture  the true underlying function. By using the early stopping technique, we  prevented the network from overfitting the noise in the data.  p  24-11   24 Case Study 2: Probability Estimation  1  0.5  NN PC  0  0  1  0  0  1  0  0  NN PS  0.5  NN PD  0.5  0.5  1.5  2.5  3.5  0.5  1.5  2.5  3.5  1  1  1  2  2  2  3  3  3  0.5  2.5  3.5  1.5  b  Figure 24.7  Reaction Probabilities vs. Impact Parameter  Data Sets  There are four data files associated with this case study:    cvd_p.txt — contains the input vectors in the original data set    cvd_t.txt — contains the target vectors in the original data set    cvd_p500.txt — contains the input vectors in the   =  500   test set    cvd_t500.txt — contains the target vectors in the   =  500   test set  NT NT  They can be found with the demonstration software, which is described in  Appendix C.  24-12   Epilogue  Epilogue  This chapter has illustrated the use of neural networks for probability es- timation on a chemical vapor deposition problem. Monte Carlo simulations  were used to provide estimates of the reaction probabilities. These esti- mates were used as targets for the neural network. The network was able  to capture the true underlying probability function without overfitting to  the errors in the Monte Carlo estimates. This was accomplished by using  the early stopping procedure, which stops network training if the error on  an independent validation set increases.  In the next chapter, we apply neural networks to a pattern recognition  problem. We will also use multilayer neural networks for that application.  24  24-13   24 Case Study 2: Probability Estimation  Further Reading  [AgSa05]  P.M. Agrawal, A.N.A. Samadh, L.M. Raff, M. Hagan, S. T.  Bukkapatnam, and R. Komanduri, “Prediction of molecu- lar-dynamics simulation results using feedforward neural  networks: Reaction of a C2 dimer with an activated dia- mond  100  surface,” The Journal of Chemical Physics 123,  224711, 2005.  This paper describes the details of training a neural net- work to predict the reaction probabilities for chemical va- por deposition of diamond.  [Mill93]  M.F. Miller, “A scaled conjugate gradient algorithm for fast  supervised learning,” Neural Networks, vol. 6, pp. 525-533,  1993.  [RaMa05]  The scaled conjugate gradient algorithm is a fast batch  training algorithm for neural networks that requires a  minimum of memory and computation at each iteration.  L.M. Raff, M. Malshe, M. Hagan, D.I. Doughan, M.G. Rock- ley, and R. Komanduri, “Ab initio potential-energy surfaces  for complex, multi-channel systems using modified novelty  sampling and feedforward neural networks,” The Journal  of Chemical Physics, 122, 084104, 2005.  This paper explains how neural networks can be used for  molecular dynamics simulations.  24-14   Objectives  25 Case Study 3:   Pattern Recognition  Objectives  25  Objectives Theory and Examples  25-1 25-2 Description of Myocardial Infarction Recognition 25-2 25-3 Data Collection and Preprocessing 25-6 Selecting the Architecture Training the Network 25-7 25-7 Validation 25-10 Data Sets 25-11 25-12  Epilogue Further Reading  This chapter presents a case study in using neural networks for pattern  recognition. In pattern recognition problems, you want a neural network to  classify inputs into a set of target categories, e.g., recognize the vineyard  that a particular bottle of wine came from, based on a chemical analysis, or  classify a tumor as benign or malignant, based on uniformity of cell size,  clump thickness, mitosis.  In this chapter we will demonstrate the application of multilayer neural  networks to the recognition of heart disease from a reading of the electro- cardiogram. We will show each of the steps in the pattern recognition pro- cess: data collection, feature extraction, architecture selection, network  training and network validation.  25-1   25 Case Study 3: Pattern Recognition  Theory and Examples  In pattern recognition  pattern classification  problems we are trying to  categorize network inputs into their corresponding classes. Here are a few  examples of pattern recognition problems:    recognition of handwritten zip codes    spoken word recognition    disease recognition from a list of symptoms    fingerprint recognition    white blood cell classification  In the case study presented in this chapter, we will be looking for patterns  in electrocardiogram signals that indicate the presence of a myocardial in- farction  heart attack .   Description of Myocardial Infarction Recognition  An electrocardiogram  EKG  is a recording of the electrical activity of the  heart over time. It generally consists of an array of different signals record- ed at the same time. An EKG can consist of a single signal  also called a  lead , although the standard EKG that is used for detailed interpretation  consists of 12 leads. EKG’s with as many as 15 leads are sometimes used.  Each lead represents the electrical activity across two points on the body.  The 12-lead EKG is determined from 10 electrodes that are placed on spe- cific locations on the body. The calculation of the 12-lead potentials from  the 10 electrodes is a somewhat complex calculation, and beyond the scope  of this case study. The interested reader is referred to [Dubi00] for a more  complete discussion of the EKG.  Through a careful analysis of the EKG, a physician can often determine the  health of the heart. The shapes of the signals indicate the path of electrical  flow in the heart as various muscles are contracted in a coordinated way to  pump blood in and out. If a part of the heart muscle has been damaged be- cause of a lack of blood flow through the coronary arteries  called a myocar- dial infarction  MI , or heart attack , then the path of electrical flow  changes. A well-trained physician can discern from the changes in the  EKG, if the heart has been damaged, and where the damage has occurred.  For this case study, we will train a neural network to recognize MI’s, using  information obtained from a 15 lead EKG.  25-2   Data Collection and Preprocessing  Data Collection and Preprocessing  The EKG signals used for this case study were obtained from the Physio- Net database [MoMa01]. Data were extracted from the QT data set for  healthy patients and patients with MI’s. Each EKG consists of 15 leads.  The leads are labeled I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6, VX,  VY, VZ. Figure 25.1 shows a small portion of the lead I signal for one of the  healthy patients.  25  0.6  0.4  0.2  0  −0.2  0  0.5  1  Time  sec   1.5  2  Figure 25.1  Example EKG Signal  Our data set has a total of 447 EKG records. Of these, 79 represent healthy  patients, and the remaining 368 have an MI diagnosis. A diagnosis for each  record was provided by a physician, but it is possible for the diagnosis to be  in error. We will have more to say about this when we come to the valida- tion of the network.  Each EKG consists of 15 leads measured at a rate of 1000 Hz for a period  of several minutes. This is an enormous amount of data, and it would be  impractical to use the entire EKG as an input to the neural network. As  with many pattern recognition problems, we need to perform a feature ex- traction step before using the neural network to execute the pattern recog- nition step. Feature extraction involves mapping the high-dimensional  input space into a space with fewer dimensions, in order to simplify and  make more robust the pattern recognition step.  There are a number of general methods for dimensionality reduction. This  includes linear methods, like the principal components method that we  mentioned in Chapter 22, and nonlinear methods, like manifold learning  [TeSi00]. For this case study, instead of using general methods to generate  the low-dimensional feature space, we will extract features that are com-  25-3   25 Case Study 3: Pattern Recognition  monly used by physicians to detect abnormalities in the EKG. The first step  is to consider a typical cycle of an EKG signal, as shown in Figure 25.2.  PR  Segment  R  ST  Segment  P  T  PR  Interval  Q  S  QRS  Duration  QT Interval  Figure 25.2  Prototype Cycle of an Electrocardiogram Signal  William Einthoven, in the early 1900’s, was the first to carefully measure  and analyze the EKG. He assigned the letters P, Q, R, S and T to the vari- ous deflections shown in the prototype cycle of Figure 25.2. He also de- scribed the electrocardiographic features of a number of cardiovascular  disorders. He won the Nobel Prize in Medicine in 1924 for his discoveries.  His features are still used to this day.  For this study, we have used some of the features that are in standard use  by physicians, as well as other features that are related to the prototype cy- cle [Raff06]. The descriptions of the 47 features that we used are listed be- low.  If a description refers to the “amplitude” lead, this refers to the square  root of the sum of squares of the VX, VY and VZ leads.   Input Features to the Neural Network  age in years gender, -1=female, 1=male  average time between heart beats in sec rms deviation of the mean heart rate in beats sec full width at half maximum for the heart rate distribution average qt interval for lead with max t wave average qt interval for all leads  1. 2. 3. maximum heart rate in beats min 4. minimum heart rate in beats min 5. 6. 7. 8. 9. 10. average corrected qt interval for lead with max t wave 11. average corrected qt interval for all leads 12. average qrs interval for all leads 13. average pr interval for lead with maximum p wave 14.  rms deviation of pr intervals from average-max p lead  25-4   25  Data Collection and Preprocessing  rms deviation of st segment lengths  rms deviation for pr interval from average-all leads  rms deviation of qt intervals rms deviation of corrected qt intervals  15. average pr interval for all leads 16. 17. percentage of negative p waves-max p lead 18. average percentage of negative p waves for all leads 19. maximum amplitude of any t wave 20. 21. 22. average st segment length 23. 24. average heart rate in beats min 25. rms deviation of heart rate distribution in beats min 26. average rt angle averaged over all amplitude beats 27. number of missed r waves  beats  28. % total qt intervals not analyzed or missing 29. % total pr intervals not analyzed or missing 30. % total st intervals not analyzed or missing 31. average number of maxima between t wave end and q 32. 33. ave qrs from amplitude lead 34. 35. ave st segment from amplitude lead 36. 37. ave qt interval from amplitude lead 38. 39. ave bazetts corrected qt interval from amplitude lead 40. 41. ave r-r interval from amplitude lead 42. 43. average area under qrs complexes 44. average area under s-t wave end 45. average ratio of qrs area to s-t wave area 46. 47. st elevation at the start of the st interval for amplitude signal  rms deviation of corrected qt interval from amplitude lead  rms deviation of st segment from amplitude lead  rms deviation of r-r interval from amplitude lead  rms deviation of qt interval from amplitude lead  rms deviation of qrs from amplitude lead  rms deviation of rt angle for all beats  rms deviation of rt angle within each beat averaged over all beats in amplitude signal  To summarize, the data set contains 447 records. Each record has 47 input  variables, and one target value. The target is 1 for a healthy diagnosis and  -1 for an MI diagnosis.  One of the problems with the data set is that there are only 79 records for  the healthy diagnosis, while there are 368 records for the MI diagnosis. If  we train the network using the sum square error performance index, where  all of the errors are weighted equally, the network will be biased to indicate   25-5   25 Case Study 3: Pattern Recognition  the MI diagnosis. The ideal solution to this problem would be to collect  more data from healthy patients. Let’s say that this is not possible in this  case, and we need to do what we can with the data available. One possibil- ity is to use a weighted sum square error as the performance index, where  errors for healthy patients would be weighted higher than errors for MI pa- tients, so that overall healthy and MI contributions would be equal if each  error were equal. Another simple approach is to repeat the healthy records  in the data set, so that the total number of healthy records is equal to the  number of MI records. This requires extra computation, but that is not a  problem in this case study. Since it is the simplest solution, we will use it  here.  After the data has been collected, the next step is to divide the data into  training, validation and test sets. In this case, we randomly set aside 15%  of the data for validation and 15% for testing. For the validation and test- ing sets, we did not include multiple entries of the healthy records. This  was done only for the training set.  The data were normalized using Eq.  22.1 , so that the inputs were in the  range [-1,1]. Since the tangent-sigmoid transfer function will be used in the  output layer of the neural network, the targets were set to values of -0.76  and +0.76, instead of -1 and 1, to prevent training difficulties caused by sat- uration of the sigmoid functions, as discussed in Chapter 22.  Selecting the Architecture  Figure 25.3 shows the network architecture. We are using the tangent-sig- moid transfer function in both layers. This is the standard network for pat- tern recognition. There are cases in which two hidden layers are used, but  we normally try first with one hidden layer. The number of neurons in the  hidden layer,  , will depend on the complexity of the decision boundaries  needed for the pattern recognition task. This is something that cannot gen- erally be known before training. We will start with 10 neurons in the hid- den layer, and then test the network performance after training.  S1  Inputs  Tan-Sigmoid Layer  Tan-Sigmoid Layer  47 x 1  p  1  47  W1  S 1 x 47  b1  S 1 x 1  n1  S 1 x 1  a1  S 1 x 1  1  W2  1 S 1x  b2  1 x 1  S 1  a2  1 x 1  n2  1 x 1  1  1 a  =  tansig W p b  +       1  1  2 a  =  tansig     2 1  2 W a b  +    Figure 25.3  Network Architecture  25-6   Training the Network  Training the Network  We trained the network using the scaled conjugate gradient algorithm of  [Mill93]. This algorithm is very efficient for pattern recognition problems.  We used early stopping to prevent network overfitting.  10=  Figure 25.4 illustrates the mean squared error versus iteration number.  The blue line shows the validation error, and the black line shows the train- ing error. We used a network with 10 neurons in the hidden layer  S1    . The minimum validation error occurred at iteration 16, as indi- cated by the circle in Figure 25.4, and the network parameters were saved  at this point. Note that the validation error curve does not always fall at  each iteration, and it may rise before falling to a lower value. We tested  that the validation error was not reduced over 40 iterations before we final- ly stopped the training.  25  101  100  10−1  E S M     Validation  Training  10−2   0  10  20 30 Iterations  40  50  Figure 25.4  Mean Squared Error vs. Iteration Number    S1  10=     Validation  As we discussed in previous chapters, an important tool for network vali- dation in function approximation problems is a scatter plot of network out- puts versus targets. For pattern recognition problems, the network outputs  and targets are discrete variables, so a scatter plot is not particularly use- ful. Instead of the scatter plot, we use the confusion matrix, which was dis- cussed in Chapter 22. Figure 25.5 shows the confusion matrix for our  trained network on the test data. The upper left cell shows that 13 of the  14 healthy EKG’s in the test set were classified correctly, while the 2,2 cell  shows that 66 of the 71 MI EKG’s were classified correctly. A total of 92.9%  of the test data were classified correctly. The largest number of mistakes  were for MI records that were classified as healthy  5 , as shown in cell 1,2.  25-7   25 Case Study 3: Pattern Recognition   Confusion Matrix  13  15.3%  5  5.9%  72.2% 27.8%  1  1.2%  66  77.6%  98.5% 1.5%  1  2  s s a l C    t u p t u O  92.9% 7.1%  1  93.0% 7.0%  92.9% 7.1%  2  Target Class  Figure 25.5  Confusion Matrix for Test Data  One Data Division   Another useful validation tool for pattern recognition problems is the Re- ceive Operating Characteristic  ROC  curve, described in Chapter 22. Fig- ure 25.6 shows the ROC curve  blue line  for the test data. The ideal curve  would follow the path from 0,0 to 0,1 and then to 1,1. The curve for this test  set is close to the ideal path.  e t a R   e v i t i s o P   e u r T  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0 0  25-8  0.2  0.4  0.6  0.8  False Positive Rate  1  Figure 25.6  Receiver Operating Characteristic Curve  Test Set   The results shown in Figure 25.5 and Figure 25.6 represent one division of  the data into training validation testing sets. Because the data set is fairly  small, especially in terms of healthy diagnoses, we might wonder how sen- sitive the results are to the data division. To investigate this sensitivity, we  performed a Monte Carlo simulation. The data were divided 1,000 different    Validation  times. For each division of the data, a neural network was trained with dif- ferent random initial weights. The results of the 1,000 trials were averaged  together, and the results are shown in Figure 25.7.   Confusion Matrix  9.11 14.0%  3.69 5.7%  71.2% 28.8%  2.51 3.9%  49.83 76.5%  95.2% 4.8%  1  2  s s a l C    t u p t u O  78.4% 21.6%  1  93.1% 6.9%  90.5% 9.5%  2  Target Class  25  Figure 25.7  Average Test Confusion Matrix for 1,000 Monte Carlo Runs  Figure 25.7 represents the average results over the 1,000 different net- works and data divisions. There were approximately 12 healthy patients   on average  in each test set. Of these, more than 9 were correctly diag- nosed. There were approximately 54 sick patients in each test set, and ap- proximately 50 were correctly diagnosed. The average testing error was  approximately 9.5%. Note that none of the patients in the test set were  used to train the neural network, so these numbers should be conservative  estimates of how the network should perform on new patients.  The average test results for the Monte Carlo simulation are similar to our  original test results. However, in addition to knowing the average results,  it is also helpful to look at the distribution of errors. Figure 25.8 shows a  histogram of the percentage errors. The average percent error is 9.5%, but  there is a significant spread in the distribution of errors. The standard de- viation of the percent error is 3.5.  25-9   25 Case Study 3: Pattern Recognition  160  120  80  40  0  0  5  10  15  20  25  Percent Error  Figure 25.8  Percent Error Histogram  1000 Monte Carlo Trials   This Monte Carlo process can be helpful in validating the data and the  training process. For example, it is possible to identify which patients are  misclassified in each Monte Carlo run. Those patients who are consistently  misclassified  regardless of the division of the data  can be carefully inves- tigated. These cases can be helpful in two areas. First, they can enable us  to refine the data base. If, after reevaluation by clinicians, it is determined  that a particular patient was mislabeled in the original data set, the data  can be corrected. Second, if we find upon review that the patient was cor- rectly labeled in the original data set, then we can use that patient to help  improve the operation of the neural network classification. This may in- volve identifying new features which capture the key characteristics of the  EKG, or it may involve obtaining more data with similar characteristics, in  order to reinforce the training of the neural network.   The Monte Carlo process can be also used to help to improve the network  performance. By combining the individual networks obtained from the  Monte Carlo trials, we can often obtain a more accurate classification. The  same input is applied to all of the networks, and the outputs can be com- bined through a “voting” procedure. We choose the class that is selected by  the largest number of networks.  Data Sets  There are two data files associated with this case study:   ekg_p.txt — contains the input vectors in the ekg data set   ekg_t.txt — contains the targets in the ekg data set  They can be found with the demonstration software, which is described in  Appendix C.  25-10   Epilogue  Epilogue  This chapter has demonstrated the use of multilayer neural networks for  pattern recognition. In this case study, the pattern recognition network  was used to classify EKG records into healthy and myocardial infarction di- agnoses.  Most pattern recognition tasks involve a feature extraction step, in which  the original data set is reduced in dimension. The features that were ex- tracted from the EKG data consisted of characteristics of the prototype  EKG cycle.  25  A Monte Carlo procedure was used as part of the network validation. The  data were randomly divided a number of times into training validation test  sets, and for each division a neural network was trained with random ini- tial weights. The performances of all of the networks were analyzed to de- termine expected future performance. In addition, those records that were  misclassified by most of the networks, regardless of the data division, were  analyzed to assist in refining the data set and improving the pattern recog- nition.  In the next chapter, we apply neural networks to a clustering problem. We  will use a self-organizing feature map network for that application.  25-11   25 Case Study 3: Pattern Recognition  Further Reading  [Dubi00]  D. Dubin, Rapid Interpretation of EKG's, Sixth Edition,  Tampa, FL: COVER, 2000.  [MoMa01]  [Raff06]  [TeSi00]  This book describes the EKG in very clear terms, and leads  you through the interpretation in a step-by-step way.  G.B. Moody, R.G. Mark, and A.L. Goldberger, “PhysioNet:  a Web-based resource for the study of physiologic signals,”  IEEE Transactions on Engineering in Medicine and Biolo- gy, vol. 20, no. 3, pp: 70-75, 2001.  This paper describes the PhysioNet data base that contains  a large variety of recorded physiologic signals. The data  base can be found at http:  www.physionet.org .  The features in the data set described in this chapter were  designed and extracted by Dr. Lionel Raff, Regents Profes- sor of Chemistry at Oklahoma State University.  J. B. Tenenbaum, V. de Silva, J. C. Langford, “A Global  Geometric Framework for Nonlinear Dimensionality Re- duction,” Science, vol. 290, pp. 2319-2323, 2000.  There are several different approaches to manifold learn- ing, in which data is mapped from a high-dimensional  space to a lower-dimensional manifold. This paper intro- duces a method called Isomap.  25-12   Objectives  26 Case Study 4: Clustering  Objectives  Objectives Theory and Examples  Description of the Forest Cover Problem Data Collection and Preprocessing Selecting the Architecture Training the Network Validation Data Sets  Epilogue Further Reading  26-1 26-2 26-2 26-4 26-5 26-6 26-7 26-10 26-11 26-12  26  This chapter presents a case study in using neural networks for clustering.  In clustering problems, you want a neural network to group data by simi- larity. For example, market segmentation can be done by grouping people  according to their buying patterns, data mining can be done by partitioning  data into related subsets, and bioinformatic analysis can be done by group- ing genes with related expression patterns.  In this chapter, we will apply clustering to a problem in forestry, in which  we would like to analyze forest cover types. We will use the self-organizing  feature map network of Chapter 16 to perform the clustering, and we will  demonstrate a variety of visualization tools that can be used in conjunction  with the SOFM.  26-1   26 Case Study 4: Clustering  Theory and Examples  This chapter presents a case study in using neural networks for clustering.  In clustering problems, we generally don’t have a set of network targets  available, so clustering networks are trained by unsupervised training al- gorithms. Instead of training a network to produce a desired response, we  want to analyze a data set to look for hidden patterns. There are many ap- plication areas for clustering. It is widely used in data mining, in which we  analyze large data sets to identify similarities within subsets of the data.  It is used in city planning, when town councils apportion regions of the city  into areas of similar home type and land usage. It is used in image compres- sion, in which a small set of prototype sub-images are identified and com- bined to represent a large collection of images. It is used in speech  recognition systems, in which speakers are clustered into categories in or- der to simplify the problem of speaker-independent recognition. Clustering  is used by marketers to identify distinct groups in their customer bases. It  has also been used to organize large bibliographic data bases so that relat- ed material can be quickly accessed.  The neural network that we will use in this application is the self-organiz- ing feature map  SOFM , which we introduced in Chapter 16. This cluster- ing network has a unique attribute that enables us to visualize large data  sets in many dimensions. We will focus on that visualization capability in  this case study.  Description of the Forest Cover Problem  An important job of the forest service is to maintain accurate natural re- source inventory information. One key characteristic that is recorded is the  type of forest cover found in wilderness areas. This type of data can be ex- pensive to collect, since it generally requires on-site inspection or estima- tion from remotely sensed data. [BlDe99] describes how forest cover type  can be predicted from independent variables that can be more easily ob- tained. In this chapter, we will use the data described in that paper to per- form a clustering analysis. We will demonstrate how an analysis of the  data using an SOFM can allow us to visualize the high-dimensional space  of independent variables and identify relationships between the forest cov- er types.  Ten independent variables that can indicate forest cover type are shown in  Table 26.1.  There were 12 variables used in [BlDe99], but for ease of pre- sentation, we selected only the first 10 for this case study.  These variables  can be measured or estimated much more easily than forest cover type. We  want to use the SOFM to find out whether these variables can be used to  cluster the data in such a way as to separate regions with different forest  cover types.  26-2   1  2  3  4  5  6  7  8  9  Description of the Forest Cover Problem  Variable Number Description  Elevation in meters  Aspect in degrees azimuth  Slope in degrees  Horz Dist to nearest surface water  Vert Dist to nearest surface water  Horz Dist to nearest roadway  Units  meters  azimuth  degrees  meters  meters  meters  26  Hillshade index at 9am, summer solstice  0 to 255 index  Hillshade index at noon, summer solstice  0 to 255 index  Hillshade index at 3pm, summer solstice  0 to 255 index  10  Horz Dist to nearest wildfire ignition points meters  Table 26.1 Description of Independent Variables  The forest cover types of interest in [BlDe99] are shown in Table 26.1. The  data set we will use for this case study contains information on cover type,  but we will not use this as part of the training process. We will use it to test  the clustering ability of the SOM.  Label  Name  Krummholz  Spruce Fir  Lodgepole Pine  Ponderosa Pine  Cottonwood Willow  Aspen  Douglas-fir  Table 26.2 Forest Cover Types  0  1  2  3  4  5  6  26-3   26 Case Study 4: Clustering  Data Collection and Preprocessing  The data used in this study came originally from [HeBa99]. It contains the  forest cover type for 30 x 30 meter cells obtained from US Forest Service   USFS  Region 2 Resource Information System  RIS  data. The original  data set contained 581,012 observations of 12 independent variables and  the forest cover type. We have used the first 20,000 observations, and we  have used only the first 10 independent variables, which are described in  Table 26.1. The forest cover types are given in Table 26.2. As mentioned  previously, we did not use these for training.  For supervised learning, as demonstrated in the previous three chapters,  after the data is collected, the next step is to divide the data into training,  validation and test sets. For unsupervised learning, we don’t generally di- vide the data in this way, because there is no need for a validation set to  stop the training. Competitive training is typically performed for a fixed  number of iterations. We use the entire data set for training.  The next step is to normalize the data. The data were scaled using Eq.   22.1 , so that the inputs were in the range [-1,1].  For the SOFM, data can  also be scaled using Eq.  22.2 , so that the input variables have a mean of  0 and a variance of 1.   Before proceeding to train the network, it is often useful to view the input  data. One convenient format for this is the scatter plot. Figure 26.1 illus- trates a set of scatter plots among the input variables 7, 8 and 9. The diag- onal plots in this figure are histograms for these three input variables, and  the off-diagonal plots are the scatter plots.  We only show three of the vari- ables in this figure because of the limits of the page size.   8  9  1  0.5  1  0  −0.5  −1 −1  8000  6000  4000  2000  0 −1  0 7 8  0  1  1  9  0.5  1  0  −0.5  −1 −1  0.5  1  0  −0.5  −1 −1  6000  4000  2000  0 −1  0 7  0 8 9  0  1  1  1  8000  6000  4000  2000  0 −1  7  0  26-4  Figure 26.1  Scatter Plots for Input Variables 7, 8, 9.   Selecting the Architecture  There are several things we can look for in Figure 26.1. First, we want to  see how well the data is scattered throughout the range. If some variables  show little or no variation, then we would remove them from the analysis.  We also look for correlation between the variables. For example, if the  points in the scatter plot fell exactly along a line, then we would know that  the two variables were linearly related. There would be no need to use both  variables in the analysis. From Figure 26.1 we can see that there is some  correlation between the variables, but they are not linearly dependent.  Selecting the Architecture  We will use the SOFM network, described in Chapter 16, to perform the  clustering for this case study. The specific architecture is often selected  based, in part, on the number of data points, so that there will be a reason- able amount of data associated with each prototype vector.  Recall that  each row of the weight matrix represents a prototype vector. An input is as- sociated with the prototype vector to which it is closest.  As the data set size  increases, the number of neurons should increase as well, although not as  rapidly. A rule of thumb is to have the number of neurons increase as the  square root of the number of data points.  Figure 26.2 shows the architecture of the network we selected. We have 10  input variables  defined in Table 26.1 , and we are using 150 neurons. The  feature map is 15x10, and it uses an hexagonal arrangement of neurons.  This means that each internal neuron will have six neighbors.  Hexagonal Feature Map  Inputs  Competitive Layer  26  p  10 x 1  10  W  150 x 10  dist  ni =  -  w p  i=      compet n  a  n  a  150 x 1  150 x 1  C  150  Figure 26.2  SOM Network Architecture  15 10x  After the network has been trained, we will analyze the results to deter- mine if the network architecture is satisfactory. In practical cases, we often  try several different architectures. Unlike with supervised training, in  which we have a clear performance measure  normally sum square error ,  there is no firm criterion for best performance in SOFM networks. Often   26-5   Training the Network  26 Case Study 4: Clustering  what we are looking for is insight into the data set. It is somewhat of an art  to selecting the best architecture and training regime for SOFM networks.  This will become more clear when we analyze the results of the trained net- work in a later section.  Before beginning the training, the weight vectors  rows of W  were initial- ized using what is called linear initialization [Koho95]. First, a covariance  matrix of the input vectors was computed. Then, the two eigenvectors of  this matrix having the two largest eigenvalues were found. The rows of W  were then assigned by taking the average of the input vectors and adding  linear combinations of the two eigenvectors. This places all of the initial  weight vectors in the space spanned by the two eigenvectors. This initial- ization process produces quicker training convergence than when using a  purely random weight initialization.  It is also possible to randomly select  input vectors from the training set to be the initial weight vectors.  Recall from Eq.  16.21  the SOFM learning rule, which we repeat here:  wi  q   wi  1– q    p q  wi +  –    1– q      1 –   wi  1– q   p q  +  =  =  where   i   is the index of the winning neuron, and  i Ni d     26.1   Ni d    26.2  defines the neuron neighborhood. For this case study, we have used a batch  form of the algorithm, in which all of the inputs in the training set are ap- plied to the network before the weights are updated. To develop the batch  form, we can first modify the sequential form of Eq.  26.1  to   j dij  d  =    wi  q   =  wi  1– q    +  hi i p q  wi  –    1– q      ,   26.3   where  would produce Eq.  26.1  is  hi i   is the neighborhood function. The neighborhood function that   Using this definition of neighborhood function, we can define a batch ver- sion of Eq.  26.1 :   26.4    26.5   hi i  =    0      i Ni d   i Ni d    wi  k   =  Q   hi q  i p q  q -------------------------------------  1=  ,  Q    q  1=  hi q  i  26-6   Validation  i q   k   is the iteration number and   where   is the winning neuron for input  p q  . Note that for the batch algorithm we have to distinguish between the  iteration number and the input number, since all inputs are applied to the  network at each iteration. This is in contrast to the sequential algorithm of  Eq.  26.1 , where there is one iteration for each input. Also, notice that the  learning rate does not affect the batch algorithm, since it would appear in  both the numerator and the denominator of Eq.  26.5 .  For the neighborhood function of Eq.  26.4 , this batch algorithm has the  effect of assigning each weight to the average of the input vectors for which  it is in the neighborhood of the winner. As with the sequential algorithm,  the neighborhood size is decreased during training. The neighborhood size  is set large at the beginning of training until all weights move into the re- gion of the input space where the data lies. Then the neighborhood size is  reduced, to fine-tune the position of the weights.  The batch algorithm requires many fewer iterations than the sequential al- gorithm, although each iteration requires much more computation. For  this case study, we used two iterations of the batch algorithm. During the  first iteration the neighborhood size was 4, and during the second iteration  the neighborhood size was reduced to 1.  26  Validation  We will consider two numerical measures of the quality of a trained SOM:  resolution and topology preservation  see page 22-23 . One measure of  SOM resolution is the quantization error, which is the average distance be- tween each data vector and its winning neuron. If the average distance is  too large, then there are many input vectors that are not adequately repre- sented by any of the prototypes.  A measure of SOM topology preservation is the topographic error. This is  the proportion of all input vectors for which the closest  winning  neuron  and the next closest neuron are not adjacent to each other in the feature  map topology. When this number is small, it means that the neurons that  are neighbors in the topology are also neighbors in the input space. It is im- portant that this topology be preserved, so that the visualization tools we  will discuss later can provide valid insight into the data set.  For our trained SOM, the final quantization error was 0.535, and the final  topographic error was 0.037. This means that for less than 4% of all input  vectors, the winning neuron and the next closest neuron were not adjacent  to each other. It appears that the SOM has achieved the correct topology by  the completion of the training.  There are a number of visualization methods that can be used to assess the  trained SOM network. One of the key tools is called the unified distance  matrix, or u-matrix. This is a figure that shows the distance between neigh- boring neurons in the feature map. The u-matrix has a cell for each neuron   26-7   26 Case Study 4: Clustering  in the feature map and an additional cell between each pair of neurons. The  cells between neurons are color-coded with the distance between the corre- sponding weight vectors. The cells that represent the neurons are coded  with the mean of the surrounding values. Figure 26.3 shows the u-matrix  for our trained SOM.     0.6  0.5  0.4  0.3  0.2  0.1     Figure 26.3  U-Matrix for Trained SOM  In Figure 26.3, the light-colored cells represent large distances between  neurons. We can see that there is a string of light colored cells on the left  side of the feature map. This indicates that the clusters associated with the  neurons on the left side of the map are significantly different than those in  the middle and right sides of the map. For this data set we actually know  the forest cover types for each data point. We can label the feature map  cells with the cover type that is associated with the closest input vector to  that cluster center. The resulting labeled map is shown in Figure 26.4.  By comparing Figure 26.4 with Figure 26.3, we can see that the forest cover  type 2  see Table 26.2  is associated with the left edge of the feature map.  As we move from left to right across the map, we see type 0 and 1 coded into  the center section, followed by types 5, 3 and 4, with type 6 located mainly  in the upper right section of the map. It is clear that the SOM has learned  to cluster the data according to forest cover type.  To get more insight into how the SOM has clustered the data, we can pro- duce a “hit histogram.” For this graph, we count how many times each neu- ron was the winning neuron for the entire data set. Since our data is  labeled with forest cover type, we can also see where each type falls on the  feature map. Such a graph is displayed in Figure 26.5. In each cell you can  see a hexagram with a certain gray-scale. The size of the hexagrams indi- cate how many times the corresponding neuron was the winning neuron.  The gray level of the hexagram indicates the forest cover type. The darkest  hexagons correspond to type 0 forest covers, and the lightest hexagons cor- respond to type 6 forest covers. We can see that the various regions of the   26-8   Validation  map have consistent colors. The left side has medium gray levels, corre- sponding to type 2 cover. The darkest levels are in the center-left region of  the map, which corresponds to cover types 0 and 1. The lighter levels,  which correspond to types 5 and 6, are in the center-right region, and the  median levels of gray, corresponding to types 3 and 4, are on the right edge.  2  2  2  2  2  2  2  2  2  0  2  2  2  2  1  2  0  2  2  2  2  2  2  0  0  0  2  2  2  2  1  1  0  0  1  0  2  2  1  1  0  1  1  0  2  1  1  1  0  0  0  0  2  0  1  0  0  0  0  1  1  0  0  0  0  0  1  0  1  0  0  1  0  0  0  1  1  2  0  0  0  0  0  5  5  5  0  0  1  2  6  5  5  5  5  2  1  1  6  5  5  5  5  5  2  6  6  3  5  5  5  5  5  6  3  3  5  5  6  5  6  6  3  3  3  4  6  5  6  3  3  3  4  4  6  3  4  4  4  4  4  4  5  3  4  4  4  4  4  4  26  Figure 26.4  Labeled SOM  For many problems, we would not be able to label each input vector. The  point here is that the SOM has been able to cluster the data into similar  cover types, without knowing what the actual cover types were. This means  that the 10 variables making up the input vectors have enough correlation  with cover type to allow the SOM to make a useful clustering of the data.  Another tool that is useful in analyzing the trained SOM is the component  plane. A component plane is a figure that represents a column of the weight  matrix of the SOM. Each column corresponds to one element of the input  vector; the jth element of column i represents the connection from input i  to neuron j. In a component plane, each element of the weight is represent- ed by a cell in the feature map topology at the location of the neuron to  which it is connected. The gray level of the cell represents the magnitude  of that element of the weight vector.  The ten component planes  one for each column of the weight matrix - each  element of the input vector  for the trained SOM are shown in Figure 26.6.  The first thing that we notice is that each of the columns is distinct. There  are no two columns that have the same pattern. We can also see that input  variables 1, 4, 5, 6 and 10 seem to be important in separating type 2 cover  types from the rest of the data. They show patterns in which a boundary  appears on the left edge of the feature map, where the type 2 cover types  are clustered. By going back to Table 26.1, we can then locate the appropri- ate variables to see if we can deduce their connection to type 2 cover.  26-9   26 Case Study 4: Clustering  Figure 26.5  Hit Histogram for Trained SOM  3  7              −0.2  −0.4  −0.6  0.8 0.6 0.4        4  8        −0.4  −0.6  −0.8  0.8  0.4  1  5  9                    2  6  10           0  −0.4  −0.4 −0.45 −0.5  0.4  0  −0.4           0.5 0 −0.5  0.5  0  −0.5  0.5 0 −0.5  Figure 26.6  Component Planes for the Trained SOM  Data Sets  There are two data files associated with this case study:    cover_p.txt — contains the input vectors in the data set    cover_t.txt — contains the targets  labels  in the data set  They can be found with the demonstration software, which is described in  Appendix C.  26-10   Epilogue  Epilogue  This chapter has demonstrated the use of SOM networks for clustering, in  which input vectors in a data set are arranged so that similar vectors are  placed in the same cluster. In this case study, the SOM was used to cluster  forestry data. The idea was to cluster land into similar forest cover types.  One of the principal advantages of the SOM network, in addition to its abil- ity to efficiently cluster a data set, is its ability to enable visualization of  high dimensional data sets.  In the next chapter, we apply neural networks to a prediction problem. We  will use a Nonlinear Autoregressive model with eXogenous inputs  NARX   network for that application.  26  26-11   26 Case Study 4: Clustering  Further Reading  [BlDe99]  J. A. Blackard and D. J. Dean, “Comparative Accuracies of  Artificial Neural Networks and Discriminant Analysis in  Predicting Forest Cover Types from Cartographic Vari- ables,” Computers and Electronics in Agriculture, vol. 24,  pp. 131-151, 1999.  This study compared neural networks and discriminant  analysis for predicting forest cover types from cartographic  variables. The study evaluated four wilderness areas in the  Roosevelt National Forest, located in the Front Range of  northern Colorado.  [HeBa99]  S. Hettich and S. D. Bay, The UCI KDD Archive [http:   kdd.ics.uci.edu], Irvine, CA: University of California, De- partment of Information and Computer Science, 1999.  [Koho93]  The UCI Knowledge Discovery in Databases Archive. This  is an online repository of large data sets which encompass- es a wide variety of data types, analysis tasks, and applica- tion areas. It is maintained by the University of California,  Irvine.  T. Kohonen, “Things you haven't heard about the Self-Or- ganizing Map,” Proceedings of the International Conference  on Neural Networks  ICNN , San Francisco, pp. 1147-1156,  1993.  This paper describes the batch form of the SOM learning  rule, as well as other variations on the SOM.  [Koho95]  T. Kohonen, Self-Organizing Map, 2nd ed., Springer-Ver- lag, Berlin, 1995.  This text describes the theory and practical operation of the  Self-Organizing Map in detail. It also has a chapter on the  Learning Vector Quantization algorithms.  26-12   Objectives  27 Case Study 5: Prediction  Objectives  27  Objectives Theory and Examples  27-1 27-2 Description of the Magnetic Levitation System 27-2 27-3 Data Collection and Preprocessing Selecting the Architecture 27-4 27-7 Training the Network 27-8 Validation Data Sets 27-13 27-14 27-15  Epilogue Further Reading  This chapter presents a case study in using neural networks for prediction.  Prediction is a kind of dynamic filtering, in which past values of one or  more time series are used to predict future values. Dynamic networks, such  as those described in Chapter 10 and Chapter 14, are used for filtering and  prediction. Unlike the previous case studies, the input to these dynamic  networks is a time sequence.  There are many applications for prediction. For example, a financial ana- lyst might want to predict the future value of a stock, bond, or other finan- cial instrument. An engineer might want to predict the impending failure  of a jet engine. Predictive models are also used for system identification  or  dynamic modeling , in which we build dynamic models of physical systems.  These dynamic models are important for analysis, simulation, monitoring  and control of a variety of systems, including manufacturing systems,  chemical processes, robotics and aerospace systems. In this chapter we will  demonstrate the development of predictive models for a magnetic levita- tion system.  27-1   27 Case Study 5: Prediction  Theory and Examples  This chapter presents a case study in using neural networks for prediction.  In this case study, the predictor neural network is used to model a dynamic  system. This dynamic modeling from data is referred to as system identifi- cation. System identification can be applied to a variety of systems: eco- nomic, aerospace, biological, transportation, communication,  manufacturing, chemical process, etc. For this case study, we will consider  a simple magnetic levitation system. Magnetic levitation has been used for  many years in transportation systems. In our simple maglev system, we  will suspend a magnet above an electromagnet. A maglev train works on a  similar principle.  Description of the Magnetic Levitation System  The objective of this maglev system is to control the position of a magnet  suspended above an electromagnet, where the magnet is constrained so  that it can only move in the vertical direction, as shown in Figure 27.1.  N  S  y t     i t     +  -  Figure 27.1  Magnetic Levitation System  The equation of motion for this system is  d2y t  --------------- dt2  =  g–  +   ----- M  i2 t   i t  ---------------------------------   sgn y t   –   ----- M  dy t  ------------  dt   27.1   y t    is the gravitational constant. The parameter    is the distance of the magnet above the electromagnet,    is  where   is the mass of the magnet, and  the current flowing in the electromagnet,   is a viscous friction coef- g ficient that is determined by the material in which the magnet moves, and   is a field strength constant that is determined by the number of turns of   wire on the electromagnet and the strength of the magnet. For our case  study, the parameter values are set to  M 3=  12=  15=  i t   9.8  M    ,   ,   ,       =  g  .  27-2   Data Collection and Preprocessing  The objective of the case study will be to develop a dynamic neural network  model that can predict the next value of the magnet position, based on pre- vious values of the magnet position and the input current. Once the model  has been developed, it can be used to find a controller that can determine  the correct current to apply to the electromagnet, so as to move the magnet  to some desired position. We won’t go in to the control design in this case  study, but the reader is referred to [HaDe02] and [NaMu97].  Data Collection and Preprocessing  For this case study, we did not build the maglev system of Figure 27.1. In- stead, we created a computer simulation to implement Eq.  27.1 . We used  Simulink® to implement the simulation, but any simulation tool could be  used. For our simulations, the current was allowed to range from -1 to 4  amps. The data were collected every 0.01 seconds.  To develop an accurate model, we need to be sure that the system inputs  and outputs cover the operating range for which the system will be used.  For system identification problems, we often collect training data while ap- plying random inputs which consist of a series of pulses of random ampli- tude and duration.  This form of input is sometimes called a skyline  function, because of its resemblance to a city skyline.  The duration and  amplitude of the pulses must be chosen carefully to produce accurate iden- tification. Figure 27.2 shows the input current, and the corresponding mag- net position for our data set. A total of 4000 data points were collected.  27     A      t n e r r u C     m      n o i t i s o P  4  2  0  8  6  4  2  0 0  27-3  0  10  20  30  40  10  20  Time  Sec   30  40  Figure 27.2  Magnetic Levitation Data  The skyline form of input function has the advantage that it can explore  both transient and steady state operation of the system. Because some of  the pulses are long, the system will approach steady state at the end of    27 Case Study 5: Prediction  those pulses. Shorter width pulses explore transient operation of the sys- tem.  When steady state performance is poor, it is useful to increase the duration  of the input pulses. Unfortunately, within a training data set, if we have  too much data in steady state conditions, the training data may not be rep- resentative of typical plant behavior. This is due to the fact that the input  and output signals do not adequately cover the region that is going to be  controlled. This will result in poor transient performance. We need to  choose the training data so that we produce adequate transient and steady  state performance. This can be done by using an input sequence with a  range of pulse widths and amplitudes.  After the data has been collected, the next step is to divide the data into  training, validation and test sets. In this case, because we will be using the  Bayesian regularization training technique, we do not need to have a vali- dation set. We did set aside 15% of the data for testing purposes. When the  input is a time sequence, it is useful to have the testing sequence consist of  a contiguous segment of the original data set. For our tests, we used the  last 15% of the data as the testing set.  The data were scaled using Eq.  22.1 , so that both the inputs and the tar- gets were in the range [-1,1]. The resulting scaled data is shown in Figure  27.3  10  20  30  40  10  20  Time  Sec   30  40  Figure 27.3  Scaled Data  t n e r r u C  n o i t i s o P  0.5  1  0  −0.5  −1 0  0.5  1  0  −0.5  −1 0  27-4  Selecting the Architecture  There are many dynamic network architectures that can be used for pre- diction. A popular architecture is the nonlinear autoregressive network    Selecting the Architecture  with exogenous inputs  NARX  network, which was discussed in Chapter  22. The NARX network is a recurrent dynamic network, with feedback con- nections enclosing several static layers of the network. The NARX model is  based on the linear ARX model, which is commonly used in time series  modeling.  The defining equation for the NARX model is  y t   =   f y t  1–   y t   2–    y t     ny–   u t   1–   u t   2–    u t     nu–      ,  27.2   y t   u t   u t   .  For our application,   where the next value of the dependent output signal   is regressed on  previous values of the output signal and previous values of an independent   exogenous  input signal   is the position of  the magnet, and   is the current going into the electromagnet.  We can  implement the NARX model by using a feedforward neural network to ap- proximate the function  . A diagram of the resulting network is shown  in the Figure 27.4, where a two-layer feedforward network is used for the  approximation. The output of the last layer of the network is the prediction  of the next value of the magnet position. The network input is the current  into the electromagnet.  f   y t   We are using the tan-sigmoid transfer function in the hidden layer, and a  linear output layer. As with the standard multilayer network, the number  of neurons in the hidden layer,  , will depend on the complexity of the sys- tem being approximated. We will discuss this choice in the next section.  S1  Inputs  Layer 1  Layer 2  p1   t  n1   t  a1   t  S 1 x 1  LW2,1  1 xS 1  S 1 x 1  1  S 1  b2  1 x 1  a2   t  1 x 1  n2   t  S 2 x 1  1  1 x 1  1  T D L  1  T D L  IW1,1  S 1 x 1  b1  S 1 x 1  LW1,3  Figure 27.4  NARX Network Architecture  To define the architecture, we also need to set the length of the tapped-de- lay lines. The TDL for the inputs will contain the variables  u t 1– y t 1–  , and the TDL for the outputs will contain the variables  . The TDL lengths    need to be defined. Be-    u t    y t    and   nu–   ny–  nu  ny     27-5  27   27 Case Study 5: Prediction  cause the defining differential equation in Eq.  27.1  is second order, we  will start with  . Later, we will investigate other possibilities.  =  =  2  ny  nu  Before demonstrating the training of the NARX network, we need to  present an important configuration that is useful in training. We can con- sider the output of the NARX network to be an estimate of the output of the  nonlinear dynamic system that we are trying to model. The output is fed  back to the input of the feedforward neural network, as part of the stan- dard NARX architecture, as shown on the left side of Figure 27.5. Since the  true output is available during the training of the network, we could create  a series-parallel architecture  see [NaPa90] , in which the true output is  used instead of feeding back the estimated output, as shown on the right  side of Figure 27.5. This has two advantages. The first is that the input to  the feedforward network will be more accurate. The second is that the re- sulting network has a purely feedforward architecture, and static back- propagation can be used for training.  u   t  T D L  T D L  Feed  Forward Network     y t^  u   t     y t  T D L  T D L  Feed  Forward Network     y t^  Parallel Architecture  Series-Parallel Architecture  Figure 27.5  Parallel and Series-Parallel Forms  Using the series-parallel form, we can actually use a standard multilayer  network to implement the NARX model. We can create an input vector that  consists of previous system inputs and outputs:  The target is then the next value of the output:  p  =  1– u t 2– u t 1– y t 2– y t       .  t  =  y t   .  27-6   27.3    27.4    Training the Network  Training the Network  We used the Bayesian regularization training algorithm, described in  Chapter 13, to train the NARX network, after the weights were initialized  using the Widrow Nguyen method  see page 22-13 . The prediction problem  is similar to the function approximation problem, which was demonstrated  in Chapter 23, and the Bayesian regularization method is effective for both  applications.  Because we have 4000 data points, and the number of network weights and  biases will be less than 100  as we will see later , the chances of overfitting  are very small. We do not need to use Bayesian regularization  or early  stopping  in this case. However, because it can tell us the effective number  of parameters, we like to use it whenever it is appropriate.  Figure 27.6 illustrates the sum square error versus iteration number,  while using Bayesian regularization. We used a network with 10 neurons  in the hidden layer     for this case. The network was trained for  1000 iterations, at which time the performance was changing very little.  Several different networks were trained with different initial conditions,  and the final SSE was similar for each, so we can be confident that we did  not reach a local minimum.  10=  S1  27  E S S  105  104  103  102  101  100  10−1  10−2  10−3  10−4  100  27-7  101  Iterations  102  103  Figure 27.6  Sum Squared Error vs. Iteration Number    S1  10=       In Figure 27.7, we can see the variation of the effective number of param- eters   during training. It eventually converges to 39. There are a total of  61 parameters in this 4-10-1 network, so we are effectively using less than  2 3 of the weights and biases. If the effective number of parameters was  close to the total number of parameters, then we would increase the num- ber of hidden neurons and retrain the network. That is not the case here.   27 Case Study 5: Prediction  There is no need to decrease the number of neurons, since the network com- putation time is not critical for this application. The only other reason for  reducing the number of neurons would be to prevent overfitting. In terms  of preventing overfitting, having 39 effective parameters is equivalent to  having 39 total parameters. That is the beauty of using the Bayesian reg- ularization technique. This method chooses the correct number of parame- ters for each problem, as long as we have a sufficient number of potential  parameters in the network.  40  30  20  10    0 0  200  400 Iterations  600  800  1000  Figure 27.7  Effective Number of Parameters    S1  10=     Validation  As we discussed in previous chapters, an important tool for network vali- dation is a scatter plot of network outputs versus targets, as shown in Fig- ure 27.8  in normalized units . The figure on the left shows the training  data, while the figure on the right shows the testing data. Because the test- ing data fit is as good as the training data fit, we can be confident that the  network did not overfit.  For prediction problems, there is another set of tools for model validation.  These tools are based on two basic properties of accurate prediction models.  The first property is that the prediction errors,  e t   =  y t   –  yˆ t   =  y t   –  a2 t   ,   27.5   should be uncorrelated with each other from one time step to the next. The  second property is that prediction errors should be uncorrelated with the  input sequence   .  See [BoJe86].   u t   27-8   Validation  If the prediction errors were correlated with each other, then we could use  that correlation to improve the predictions. For example, if prediction er- rors one time step apart had a positive correlation, then a large positive  prediction error at the current time point would suggest that the prediction  error at the next time point would also be positive. By lowering our next  prediction, we could then reduce the next prediction error.  Training  Testing  1  0.5  a  0  −0.5  1  0.5  a  0  −0.5  −1 −1  −0.5  0.5  1  −1 −1  −0.5  0.5  1  0 t  0 t  Figure 27.8  Scatter Plots of Network Outputs vs. Targets - Training and Testing Sets  The same argument holds for correlation between the input sequence and  the prediction error. For accurate prediction models, there should be no cor- relation between the input and the prediction error. If there was correla- tion, then we could use this correlation to improve the predictor.  To measure the correlation in a time sequence, we use the autocorrelation  function, which can be estimated by  Re    =  e t e t  +    .   27.6   1  ------------ Q –  Q –    t  1=  The autocorrelation function of the prediction error  in normalized units   of our trained network for the maglev problem is shown in Figure 27.9.  For the prediction error to be uncorrelated  termed “white” noise , the au- tocorrelation function should be an impulse at  , with all other values  equal to zero. Because Eq.  27.6  provides only an estimate of the true au- tocorrelation function, the values at   will never be exactly equal to ze- ro. If the error sequence is white noise, then we can find confidence  intervals for the     see [BoJe86]  defined by  0=  0      Re    27-9  27   27 Case Study 5: Prediction    2  Re 0  ------------- Q  .   27.7   Re    x 10−8  6  4  2  0  −50  0   50  Figure 27.9    Re   ny      =  nu  =  ,   S1  2  10=     The dashed blue lines in Figure 27.9 indicate these confidence bounds. We  can see that the estimated autocorrelation function for the prediction er- rors falls outside these bounds at a number of points. This indicates that  we may need to increase    and   .  ny  nu  To measure correlation between the input sequence  error    and the prediction  , we use the cross-correlation function, which can be estimated by  u t   e t   Q –    1=  1  ------------ Q –  t Rue    Rue    =  u t e t  +    .   27.8   The cross-correlation function  network for the maglev problem is shown in Figure 27.9.    in normalized units  of our trained   As with the estimated autocorrelation function, we can define confidence  intervals to determine if the cross-correlation function is near zero  Re 0  Ru 0  ------------------------------------  .    2  Q   27.9   The dashed blue lines in Figure 27.10 represent these confidence bounds.  The cross-correlation function remains within these bounds, so it does not  indicate a problem.  27-10   Validation  Rue    0  x 10−5  0.8  0.4  −0.4  −0.8  −50  Re    x 10−8  3  2  1  0  −50  0   50  Figure 27.10    Rue   ny      =  nu  =  ,   S1  2  10=     Because the autocorrelation function of the prediction errors in Figure 27.9  indicated correlation in the errors, we increased the   values from  2 to 4 and retrained our neural network predictor. The resulting estimated  autocorrelation function is shown in Figure 27.11. Here we can see that  Re   that our model is performing correctly.   falls within the confidence bounds, except at   , which indicates    and   0=  nu  ny    27  0   50  Figure 27.11    Re   ny      =  nu  =  ,   S1  4  10=     The estimated cross-correlation function, with the increased delay order, is  shown in Figure 27.12. All of the points are well within the zero confidence   27-11   27 Case Study 5: Prediction  interval. There is no significant correlation between the errors and the in- put.  0   50  Figure 27.12    Rue   ny      =  nu  =  ,   S1  4  10=     =  ny  nu  With  , we have white prediction errors, and there is no signif- icant correlation between the prediction errors and the model input. It ap- pears that we have an accurate prediction model.   =  4  The errors for the final prediction model are shown in Figure 27.13. We can  see that the errors are very small. However, because of the series-parallel  configuration, these are errors for only a one-step-ahead prediction. A more  stringent test would be to rearrange the network into the original parallel  form and then to perform an iterated prediction over many time steps. We  will now demonstrate the parallel operation.  Figure 27.14 illustrates the iterated prediction. The solid line is the actual  position of the magnet, and the dashed line is the position predicted by the  NARX neural network. The network prediction is very accurate - even 600  time steps ahead.  Rue    x 10−6  8  4  0  −4  −8  −50  27-12   Data Sets  x 10−4  4  2  0  r o r r E   n o i t c i d e r P   d e z i l a m r o N  −2  −4  0  n o i t i s o P   d e z i l a m r o N  0.5  1  0  −0.5  −1 0  10  20  Time  Sec   30  40  Figure 27.13  Prediction Errors vs. Time  27  2  Time  Sec   4  6  Figure 27.14  Iterated Prediction for the Maglev NARX Network  Data Sets  There are two data files associated with this case study:    maglev_u.txt — contains the input sequence in the original data set    maglev_y.txt — contains the output sequence in the original data set  They can be found with the demonstration software, which is described in  Appendix C.  27-13   27 Case Study 5: Prediction  Epilogue  This chapter has demonstrated the use of multilayer neural networks for  prediction, in which a future value of a time series is predicted from past  values of that series and potentially other series. In this case study, the  prediction network was used as a model of a magnetic levitation system.  This modeling of dynamic systems is referred to as system identification.  A Nonlinear Autoregressive model with eXogenous inputs  NARX  network  is well suited to this problem, and Bayesian regularization is an excellent  training algorithm to use in this situation.  27-14   Further Reading  Further Reading  [BoJe94]  [HaDe02]  [NaMu97]  [NaPa90]  G. E. P. Box, G. M. Jenkins, and G. C. Reinsel, Time Series  Analysis: Forecasting and Control, Fourth Edition, Wiley,  2008.  A classic textbook on time series analysis and the develop- ment of prediction models.  M. Hagan, H. Demuth, O. De Jesus, “An Introduction to the  Use of Neural Networks in Control Systems,” International  Journal of Robust and Nonlinear Control, vol. 12, no. 11,  pp. 959-985, 2002.  This survey paper describes some practical aspects of using  neural networks for control systems. Three neural network  controllers are demonstrated: model predictive control,  NARMA-L2 control, and model reference control.  Narendra, K.S.; Mukhopadhyay, S., “Adaptive control us- ing neural networks and approximate models,” IEEE  Transactions on Neural Networks, vol. 8, no. 3, pp. 475 -  485, 1997.  This paper introduced the NARMA-L2 model and control- ler. Once the NARMA-L2 model is trained, it can be easily  inverted to form a controller for the identified system.  K. S. Narendra and K. Parthasarathy, “Identification and  control of dynamical systems using neural networks,”  IEEE Transactions on Neural Networks, vol. 1, no. 1, pp. 4– 27, 1990.  Classic early paper on the use of neural networks for the  identification and control of dynamical systems.  27  27-15   A   Bibliography  A Bibliography  [AgSa05]  [Albe72]   [AmMu97]  [Ande72]  [AnRo88]  [AnSi77]  [Barn92]  [BaSu83]  [Batt92]  [Bish91]  P.M. Agrawal, A.N.A. Samadh, L.M. Raff, M. Hagan, S. T.  Bukkapatnam, and R. Komanduri, “Prediction of molecu- lar-dynamics simulation results using feedforward neural  networks: Reaction of a C2 dimer with an activated dia- mond  100  surface,” The Journal of Chemical Physics 123,  224711, 2005.  Chapter 24   A. Albert, Regression and the Moore-Penrose Pseudoin- verse, New York: Academic Press, 1972.  Chapter 7   S. Amari, N. Murata, K.-R. Muller, M. Finke, and H. H.  Yang, “Asymptotic Statistical Theory of Overtraining and  Cross-Validation,” IEEE Transactions on Neural Net- works, vol. 8, no. 5, 1997.  Chapter 13   J. A. Anderson, “A simple neural network generating an in- teractive memory,” Mathematical Biosciences, vol. 14, pp.  197–220, 1972.  Chapter 1, 15, 21   J. A. Anderson and E. Rosenfeld, Neurocomputing: Foun- dations of Research, Cambridge, MA: MIT Press, 1989.   Chapter 1, 10   J. A. Anderson, J. W. Silverstein, S. A. Ritz and R. S. Jones,  “Distinctive features, categorical perception, and probabil- ity learning: Some applications of a neural model,” Psycho- logical Review, vol. 84, pp. 413–451, 1977.  Chapter 21   E. Barnard, “Optimization for training neural nets,” IEEE  Transactions on Neural Networks, vol. 3, no. 2, pp. 232– 240, 1992.  Chapter 12   A. R. Barto, R. S. Sutton and C. W. Anderson, “Neuronlike  adaptive elements that can solve difficult learning control  problems,” IEEE Transactions on Systems, Man, and Cy- bernetics, vol. 13, pp. 834–846, 1983.  Chapter 4   R. Battiti, “First and second order methods for learning:  Between steepest descent and Newton’s method,” Neural  Computation, vol. 4, no. 2, pp. 141–166, 1992.  Chapter 9,  12   C. M. Bishop, “Improving the generalization properties of  radial basis function neural networks,” Neural Computa- tion, vol. 3, no. 4, pp. 579-588, 1991.  Chapter 17   A-1  A   [Bish95]   [BlDe99]  [BoJe94]   [BrLo88]  [Brog91]   [CaGr87a]  [CaGr87b]  [CaGr90]  [CaGrRe91]  [Char92]  A   Bibliography  C.M. Bishop, Neural Networks for Pattern Recognition, Ox- ford University Press,1995.  Chapter 22   J. A. Blackard and D. J. Dean, “Comparative Accuracies of  Artificial Neural Networks and Discriminant Analysis in  Predicting Forest Cover Types from Cartographic Vari- ables,” Computers and Electronics in Agriculture, vol. 24,  pp. 131-151, 1999.  Chapter 26   G.E.P. Box, G.M. Jenkins, and G.C. Reinsel, Time Series  Analysis: Forecasting and Control, 4th Edition, John Wiley  & Sons, 2008.  Chapter 22, 27   D.S. Broomhead and D. Lowe, “Multivariable function in- terpolation and adaptive networks,” Complex Systems,  vol.2, pp. 321-355, 1988.  Chapter 17   W. L. Brogan, Modern Control Theory, 3rd Ed., Englewood  Cliffs, NJ: Prentice-Hall, 1991.  Chapter 4, 5, 6, 8, 9, 20   G. A. Carpenter and S. Grossberg, “A massively parallel ar- chitecture for a self-organizing neural pattern recognition  machine,” Computer Vision, Graphics, and Image Process- ing, vol. 37, pp. 54–115, 1987.  Chapter 19   G. A. Carpenter and S. Grossberg, “ART2: Self-organiza- tion of stable category recognition codes for analog input  patterns,” Applied Optics, vol. 26, no. 23, pp. 4919–4930,  1987.  Chapter 19   G. A. Carpenter and S. Grossberg, “ART3: Hierarchical  search using chemical transmitters in self-organizing pat- tern recognition architectures,” Neural Networks, vol. 3,  no. 23, pp. 129–152, 1990.  Chapter 19   nolds and D. B. Rosen, “Fuzzy ARTMAP: A neural network  architecture for incremental learning of analog multidi- mensional maps,” IEEE Transactions on Neural Networks,  vol. 3, pp. 698–713, 1992.  Chapter 19   G.A. Carpenter, S. Grossberg and J. Reynolds, “ARTMAP:  Supervised real-time learning and classification of nonsta- tionary data by a self-organizing neural network,” Neural  Networks, vol. 4, pp. 565–588, 1991.  Chapter 19   C. Charalambous, “Conjugate gradient algorithm for effi- cient training of artificial neural networks,” IEEE Proceed- ings, vol. 139, no. 3, pp. 301–310, 1992.  Chapter 12   [CaGrMa92] G. A. Carpenter, S. Grossberg, N. Markuzon, J. H. Rey-  A-2   [ChCo91]  [ChCo92]  [ChCh96]  [ChCo99]  [CoGr83]  [DARP88]  [DeHa07]  [Dubi00]  [Fahl89]  [FoGi07]  A   Bibliography  S. Chen, C.F.N. Cowan, and P.M. Grant, “Orthogonal least  squares learning algorithm for radial basis function net- works,” IEEE Transactions on Neural Networks, vol.2,  no.2, pp.302-309, 1991.  Chapter 17   S. Chen, P. M. Grant, and C. F. N. Cowan, “Orthogonal  least squares algorithm for training multioutput radial ba- sis function networks,” Proceedings of the Institute of Elec- trical Engineers, vol. 139, Pt. F, no. 6, pp. 378–384, 1992.   Chapter 17   S. Chen, E. S. Chng, and K. Alkadhimi, “Regularised or- thogonal least squares algorithm for constructing radial  basis function networks,” International Journal of Control,  vol. 64, no. 5, pp. 829–837, 1996.  Chapter 17   S. Chen, C.F.N. Cowan, and P.M. Grant, “Combined Genet- ic Algorithm Optimization and Regularized Orthogonal  Least Squares Learning for Radial Basis Function Net- works,” IEEE Transactions on Neural Networks, vol.10,  no.5, pp.302-309, 1999.  Chapter 17   M. A. Cohen and S. Grossberg, “Absolute stability of global  pattern formation and parallel memory storage by compet- itive neural networks,” IEEE Transactions on Systems,  Man, and Cybernetics, vol. 13, no. 5, pp. 815–826, 1983.   Chapter 20, 21   DARPA Neural Network Study, Lexington, MA: MIT Lin- coln Laboratory, 1988.  Chapter 1   O. De Jesús and M. Hagan, “Backpropagation Algorithms  for a Broad Class of Dynamic Networks,” IEEE Transac- tions on Neural Networks, vol. 18, no. 1, pp., 2007.  Chapter  14   D. Dubin, Rapid Interpretation of EKG's, Sixth Edition,  Tampa, FL: COVER, 2000.  Chapter 25   S. E. Fahlman, “Fast learning variations on back-propaga- tion: An empirical study,” in Proceedings of the 1988 Con- nectionist Models Summer School, D. Touretzky, G. Hinton  and T. Sejnowski, eds., San Mateo, CA: Morgan Kaufmann,  pp. 38–51, 1989.  Chapter 12   L. Fortuna, P. Giannone, S. Graziani, M. G. Xibilia, “Virtu- al Instruments Based on Stacked Neural Networks to Im- prove Product Quality Monitoring in a Refinery,” IEEE  Transactions on Instrumentation and Measurement, vol.  56, no. 1, pp. 95–101, 2007.  Chapter 23   A-3  A   [FoHa97]  [FrSk91]  [Gill81]  [GoLa98]  [GrMi89]  [Gros67]  [Gros68]   [Gros76]  [Gros80]  [Gros82]  [Gros90]  A   Bibliography  D. Foresee and M. Hagan, “Gauss-Newton Approximation  to Bayesian Learning,” Proceedings of the 1997 Interna- tional Joint Conference on Neural Networks, vol. 3, pp.  1930 - 1935, 1997.  Chapter 13   J. Freeman and D. Skapura, Neural Networks: Algorithms,  Applications, and Programming Techniques, Reading, MA:  Addison-Wesley, 1991.  Chapter 16   P. E. Gill, W. Murray and M. H. Wright, Practical Optimi- zation, New York: Academic Press, 1981.  Chapter 8, 9   C. Goutte and J. Larsen, “Adaptive Regularization of Neu- ral Networks Using Conjugate Gradient,” Proceedings of  the IEEE International Conference on Acoustics, Speech  and Signal Processing, vol. 2, pp. 1201-1204, 1998.  Chap- ter 13   S. Grossberg, E. Mingolla and D. Todorovic, “A neural net- work architecture for preattentive vision,” IEEE Transac- tions on Biomedical Engineering, vol. 36, no. 1, pp. 65–84,  Jan. 1989.  Chapter 18   S. Grossberg, “Nonlinear difference-differential equations  in prediction and learning theory,” Proceedings of the Na- tional Academy of Sciences, vol. 58, pp. 1329–1334, 1967.   Chapter 21   S. Grossberg, “Some physiological and biochemical conse- quences of psychological postulates,” Proceedings of the Na- tional Academy of Sciences, vol. 60, pp. 758–765, 1968.   Chapter 15   S. Grossberg, “Adaptive pattern classification and univer- sal recoding: I. Parallel development and coding of neural  feature detectors,” Biological Cybernetics, vol. 23, pp. 121– 134, 1976.  Chapter 1, 18, 19   S. Grossberg, “How does the brain build a cognitive code?”  Psychological Review, vol. 88, pp. 375–407, 1980.  Chapter  1   S. Grossberg, Studies of Mind and Brain, Boston: D. Reidel  Publishing Co., 1982.  Chapter 15, 18, 19   S. Grossberg, “Neural networks: From foundations to ap- plications,” Short-Course Notes, Boston University, Bos- ton, MA, May 6–11, 1990.  Chapter 18   [HaBo07]  L. Hamm, B. W. Brorsen and M. T. Hagan, “Comparison of  Stochastic Global Optimization Methods to Estimate Neu-  A-4   [HaDe02]  [HaMe94]  [HeBa99]  [Hebb 49]  [Hech90]  [HeOh97]  [Himm72]  [Hopf82]  [Hopf84]  [HoTa85]  [HoSt89]  A   Bibliography  ral Network Weights,” Neural Processing Letters, vol. 26,  no. 3, December 2007.  Chapter 22   M. Hagan, H. Demuth, O. De Jesus, “An Introduction to the  Use of Neural Networks in Control Systems,” International  Journal of Robust and Nonlinear Control, vol. 12, no. 11,  pp. 959-985, 2002.  Chapter 27   M. T. Hagan and M. Menhaj, “Training feedforward net- works with the Marquardt algorithm,” IEEE Transactions  on Neural Networks, vol. 5, no. 6, pp. 989–993, 1994.  Chap- ter 12   S. Hettich and S. D. Bay, The UCI KDD Archive [http:   kdd.ics.uci.edu], Irvine, CA: University of California, De- partment of Information and Computer Science, 1999.   Chapter 26   D. O. Hebb, The Organization of Behavior, New York:  Wiley, 1949.  Chapter 1, 7, 15   R. Hecht-Nielsen, Neurocomputing, Reading, MA: Addi- son-Wesley, 1990.  Chapter 16   B. Hedén, H. Öhlin, R. Rittner, L. Edenbrandt, “Acute My- ocardial Infarction Detected in the 12-Lead ECG by Artifi- cial Neural Networks,” Circulation, vol. 96, pp. 1798–1802,  1997.  Chapter 22   D. M. Himmelblau, Applied Nonlinear Programming, New  York: McGraw-Hill, 1972.  Chapter 8, 9   J. J. Hopfield, “Neural networks and physical systems with  emergent collective computational properties,” Proceedings  of the National Academy of Sciences, vol. 79, pp. 2554– 2558, 1982.  Chapter 1, 21   J. J. Hopfield, “Neurons with graded response have collec- tive computational properties like those of two-state neu- rons,” Proceedings of the National Academy of Sciences, vol.  81, pp. 3088–3092, 1984.  Chapter 21   J. J. Hopfield and D. W. Tank, “ ‘Neural’ computation of de- cisions in optimization problems,” Biological Cybernetics,  vol. 52, pp. 141–154, 1985.  Chapter 21   K. M. Hornik, M. Stinchcombe and H. White, “Multilayer  feedforward networks are universal approximators,” Neu- ral Networks, vol. 2, no. 5, pp. 359–366, 1989.  Chapter 11   A-5  A   [Hube88]  [Jaco88]  [John01]  [Joll02]  [Koho72]   [Koho87]  [Koho93]  [Koho95]  [LaSa67]  [LeCu85]  [LeCu98]   A   Bibliography  D. H. Hubel, Eye, Brain, and Vision, New York: Scientific  American Library, 1988.  Chapter 18   R. A. Jacobs, “Increased rates of convergence through  learning rate adaptation,” Neural Networks, vol. 1, no. 4, pp  295–308, 1988.  Chapter 12   G. L. Johnson, “Contributions to the comparative anatomy  of the mammalian eye, chiefly based on ophthalmoscopic  examination,” Philosophical Transactions of the Royal So- ciety of London, Series B., vol. 194, pp. 1–82, Plate 1, 1901.   Chapter 18   I.T. Jolliffe, Principal Component Analysis, Springer Series  in Statistics, 2nd ed., Springer, NY, 2002.  Chapter 22   T. Kohonen, “Correlation matrix memories,” IEEE Trans- actions on Computers, vol. 21, pp. 353–359, 1972.  Chapter  1, 15, 21   T. Kohonen, Self-Organization and Associative Memory,  2nd Ed., Berlin: Springer-Verlag, 1987.  Chapter 15, 16   T. Kohonen, “Things you haven't heard about the Self-Or- ganizing Map,” Proceedings of the International Conference  on Neural Networks  ICNN , San Francisco, pp. 1147-1156,  1993.  Chapter 26   T. Kohonen, Self-Organizing Map, 2nd ed., Springer-Ver- lag, Berlin, 1995.  Chapter 26   J. P. LaSalle, “An invariance principle in the theory of sta- bility,” in Differential Equations and Dynamic Systems, J.  K. Hale and J. P. Lasalle, eds., New York: Academic Press,  pp. 277–286, 1967.  Chapter 20   Y. Le Cun, “Une procedure d’apprentissage pour reseau a  seuil assymetrique,” Cognitiva, vol. 85, pp. 599–604, 1985.   Chapter 11   Y. LeCun, L. Bottou, G. B. Orr, K.-R. Mueller, “Efficient  BackProp,” Lecture Notes in Computer Science, vol. 1524,  1998.  Chapter 22   [Leib90]   D. Lieberman, Learning, Behavior and Cognition, Bel- mont, CA: Wadsworth, 1990.  Chapter 15   A-6   [LiMi89]  [Lowe89]  [MacK92]  [MaNe99]  [MaGa00]  [McPi43]  [Mill90]  [Mill93]  [MoDa89]  [Moll93]  [MoMa01]  A   Bibliography  J. Li, A. N. Michel and W. Porod, “Analysis and synthesis  of a class of neural networks: Linear systems operating on  a closed hypercube,” IEEE Transactions on Circuits and  Systems, vol. 36, no. 11, pp. 1405–1422, November 1989.   Chapter 21   D. Lowe, “Adaptive radial basis function nonlinearities,  and the problem of generalization,” Proceedings of the First  IEE International Conference on Artificial Neural Net- works, pp. 171 - 175, 1989.  Chapter 17   D. J. C. MacKay, “Bayesian Interpolation,” Neural Compu- tation, vol. 4, pp. 415-447, 1992.  Chapter 13   J.R. Magnu and H. Neudecker, Matrix Differential Calcu- lus, John Wiley & Sons, Ltd., Chichester, 1999.  Chapter  14   E. A. Maguire, D. G. Gadian, I. S. Johnsrude, C. D. Good, J.  Ashburner, R. S. J. Frackowiak, and C. D. Frith, “Naviga- tion-related structural change in the hippocampi of taxi  drivers,” Proceedings of the National Academy of Sciences,  vol. 97, no. 8, pp. 4398-4403, 2000.  Chapter 1   W. McCulloch and W. Pitts, “A logical calculus of the ideas  immanent in nervous activity,” Bulletin of Mathematical  Biophysics, vol. 5, pp. 115–133, 1943.  Chapter 1, 4, 21   A.J. Miller, Subset Selection in Regression. Chapman and  Hall, N.Y., 1990.  Chapter 17  M.F. Miller, “A scaled conjugate gradient algorithm for fast  supervised learning,” Neural Networks, vol. 6, pp. 525-533,  1993.  Chapter 24   J. Moody and C.J. Darken, “Fast Learning in Networks of  Locally-Tuned Processing Units,” Neural Computation,  vol. 1, pp. 281–294, 1989.  Chapter 17   M. Moller, “A scaled conjugate gradient algorithm for fast  supervised learning,” Neural Networks, vol. 6, pp. 525-533,  1993.  Chapter 22   G.B. Moody, R.G. Mark, and A.L. Goldberger, “PhysioNet:  a Web-based resource for the study of physiologic signals,”  IEEE Transactions on Engineering in Medicine and Biolo- gy, vol. 20, no. 3, pp: 70-75, 2001.  Chapter 25   [MiPa69]  M. Minsky and S. Papert, Perceptrons, Cambridge, MA:  MIT Press, 1969.  Chapter 1, 4   A-7  A   [NaMu97]  [NaPa90]  [NgWi90]  [OrHa00]  [Park85]  [PaSa93]  [PeCo93]  [PhHa13]  [Powe87]  [PuFe97]  [RaMa05]   A   Bibliography  Narendra, K.S.; Mukhopadhyay, S., “Adaptive control us- ing neural networks and approximate models,” IEEE  Transactions on Neural Networks, vol. 8, no. 3, pp. 475 -  485, 1997.  Chapter 27   K. S. Narendra and K. Parthasarathy, “Identification and  control of dynamical systems using neural networks,”  IEEE Transactions on Neural Networks, vol. 1, no. 1, pp. 4– 27, 1990.  Chapter 27   D. Nguyen and B. Widrow, “Improving the learning speed  of 2-layer neural networks by choosing initial values of the  adaptive weights,” Proceedings of the IJCNN, vol. 3, pp.  21–26, July 1990.  Chapter 12, 22   M. J. Orr, J. Hallam, A. Murray, and T. Leonard, “Assess- ing rbf networks using delve,” IJNS, 2000.  Chapter 17   D. B. Parker, “Learning-logic: Casting the cortex of the hu- man brain in silicon,” Technical Report TR-47, Center for  Computational Research in Economics and Management  Science, MIT, Cambridge, MA, 1985.  Chapter 11   J. Park and I.W. Sandberg, “Universal approximation us- ing radial-basis-function networks,” Neural Computation,  vol. 5, pp. 305-316, 1993.  Chapter 17   M. P. Perrone and L. N. Cooper, “When networks disagree:  Ensemble methods for hybrid neural networks,” in Neural  Networks for Speech and Image Processing, R. J. Mam- mone, Ed., Chapman-Hall, pp. 126-142, 1993.  Chapter 22   M. Phan and M. Hagan, “Error Surface of Recurrent Net- works,” IEEE Transactions on Neural Networks and  Learning Systems, vol. 24, no. 11, pp. 1709 - 1721, October,  2013.  Chapter 14   M.J.D. Powell, “Radial basis functions for multivariable in- terpolation: a review,” Algorithms for Approximation, pp.  143-167, Oxford, 1987.  Chapter 17   G.V. Puskorius and L.A. Feldkamp, “Extensions and en- hancements of decoupled extended Kalman filter training,”  Proceedings of the 1997 International Conference on Neural  Networks, vol. 3, pp. 1879-1883, 1997.  Chapter 22   L.M. Raff, M. Malshe, M. Hagan, D.I. Doughan, M.G. Rock- ley, and R. Komanduri, “Ab initio potential-energy surfaces  for complex, multi-channel systems using modified novelty  sampling and feedforward neural networks,” The Journal  of Chemical Physics, vol. 122, 2005.  Chapter 22, 24   A-8   [RiIr90]  [Rose58]   [Rose61]   [RuHi86]  [RuMc86]  [Sarle95]   [Scal85]   [ScSm99]  [Shan90]  [SjLj94]  [SlLi91]  [StDo84]  A   Bibliography  A. K. Rigler, J. M. Irvine and T. P. Vogl, “Rescaling of vari- ables in back propagation learning,” Neural Networks, vol.  3, no. 5, pp 561–573, 1990.  Chapter 12   F. Rosenblatt, “The perceptron: A probabilistic model for  information storage and organization in the brain,” Psycho- logical Review, vol. 65, pp. 386–408, 1958.  Chapter 1, 4   F. Rosenblatt, Principles of Neurodynamics, Washington  DC: Spartan Press, 1961.  Chapter 4   D. E. Rumelhart, G. E. Hinton and R. J. Williams, “Learn- ing representations by back-propagating errors,” Nature,  vol. 323, pp. 533–536, 1986.  Chapter 11   D. E. Rumelhart and J. L. McClelland, eds., Parallel Dis- tributed Processing: Explorations in the Microstructure of  Cognition, vol. 1, Cambridge, MA: MIT Press, 1986.  Chap- ter 1, 11, 16   W. S. Sarle, “Stopped training and other remedies for over- fitting,” In Proceedings of the 27th Symposium on Interface,  1995.  Chapter 13   L. E. Scales, Introduction to Non-Linear Optimization, New  York: Springer-Verlag, 1985.  Chapter 8, 9, 12   B. Schölkopf, A. Smola, K.-R. Muller, “Kernel Principal  Component Analysis,” in B. Schölkopf, C. J. C. Burges, A.  J. Smola  Eds. , Advances in Kernel Methods-Support Vec- tor Learning, MIT Press Cambridge, MA, USA, pp. 327- 352, 1999.  Chapter 22   D. F. Shanno, “Recent advances in numerical techniques  for large-scale optimization,” in Neural Networks for Con- trol, Miller, Sutton and Werbos, eds., Cambridge, MA: MIT  Press, 1990.  Chapter 12   J. Sjoberg and L. Ljung, “Overtraining, regularization and  searching for minimum with application to neural net- works,” Linkoping University, Sweden, Tech. Rep. LiTH- ISY-R-1567, 1994.  Chapter 13   J.-J. E. Slotine and W. Li, Applied Nonlinear Control, En- glewood Cliffs, NJ: Prentice-Hall, 1991.  Chapter 20   W. D. Stanley, G. R. Dougherty and R. Dougherty, Digital  Signal Processing, Reston VA: Reston Publishing Co.,  1984.  Chapter 10   A-9  A   [Stra76]  [TaHo86]  [TeSi00]  [Tikh63]  [Toll90]  [vanT75]  [VoMa88]  [vond73]  [WaVe94]  [Werbo74]  A   Bibliography   G. Strang, Linear Algebra and Its Applications, New York:  Academic Press, 1980.  Chapter 5, 6   D. W. Tank and J. J. Hopfield, “Simple ‘neural’ optimiza- tion networks: An A D converter, signal decision circuit  and a linear programming circuit,” IEEE Transactions on  Circuits and Systems, vol. 33, no. 5, pp. 533–541, 1986.   Chapter 21   J. B. Tenenbaum, V. de Silva, J. C. Langford, “A Global  Geometric Framework for Nonlinear Dimensionality Re- duction,” Science, vol. 290, pp. 2319-2323, 2000.  Chapter  25   A. N. Tikhonov, “The solution of ill-posed problems and the  regularization method,” Dokl. Acad. Nauk USSR, vol. 151,  no. 3, pp. 501-504, 1963.  Chapter 13   T. Tollenaere, “SuperSAB: Fast adaptive back propagation  with good scaling properties,” Neural Networks, vol. 3, no.  5, pp. 561–573, 1990.  Chapter 12   H. F. J. M. van Tuijl, “A new visual illusion: Neonlike color  spreading and complementary color induction between  subjective contours,” Acta Psychologica, vol. 39, pp. 441– 445, 1975.  Chapter 18   T. P. Vogl, J. K. Mangis, A. K. Zigler, W. T. Zink and D. L.  Alkon, “Accelerating the convergence of the backpropaga- tion method,” Biological Cybernetics, vol. 59, pp. 256–264,  Sept. 1988.  Chapter 12   C. von der Malsburg, “Self-organization of orientation sen- sitive cells in the striate cortex,” Kybernetic, vol. 14, pp. 85– 100, 1973.  Chapter 18   C. Wang, S. S. Venkatesh, and J. S. Judd, “Optimal Stop- ping and Effective Machine Complexity in Learning,” Ad- vances in Neural Information Processing Systems, J. D.  Cowan, G. Tesauro, and J. Alspector, Eds., vol. 6, pp. 303- 310, 1994.  Chapter 13   P. J. Werbos, “Beyond regression: New tools for prediction  and analysis in the behavioral sciences,” Ph.D. Thesis,  Harvard University, Cambridge, MA, 1974. Also published  as The Roots of Backpropagation, New York: John Wiley &  Sons, 1994.  Chapter 11   [Werb90]  P. J. Werbos, “Backpropagation through time: What it is  and how to do it,” Proceedings of the IEEE, vol. 78, pp.  1550–1560, 1990.  Chapter 14   A-10   A   Bibliography  [WeTe84]  [WhSo92]  [WiHo60]  [WiSt 85]  [WiWi 88]  [WiZi89]  J. F. Werker and R. C. Tees, “Cross-language speech per- ception: Evidence for perceptual reorganization during the  first year of life,” Infant Behavior and Development, vol. 7,  pp. 49-63, 1984.  Chapter 1   D. White and D. Sofge, eds., Handbook of Intelligent Con- trol, New York:Van Nostrand Reinhold, 1992.  Chapter 4   B. Widrow, M. E. Hoff, “Adaptive switching circuits,”1960  IRE WESCON Convention Record, New York: IRE Part 4,  pp. 96–104, 1960.  Chapter 1, 10   B. Widrow and S. D. Stearns, Adaptive Signal Processing,  Englewood Cliffs, NJ: Prentice-Hall, 1985.  Chapter 10   B. Widrow and R. Winter, “Neural nets for adaptive filter- ing and adaptive pattern recognition,” IEEE Computer  Magazine, March 1988, pp. 25–39.  Chapter 10   R. J. Williams and D. Zipser, “A learning algorithm for con- tinually running fully recurrent neural networks,” Neural  Computation, vol. 1, pp. 270–280, 1989.  Chapter 14   A  A-11   B   Notation  B Notation  Basic Concepts  Scalars: small italic letters.....a,b,c  Vectors: small bold nonitalic letters.....a,b,c  Matrices: capital BOLD nonitalic letters.....A,B,C   Language  Vector means a column of numbers.  Row vector means a row of a matrix used as a vector  column .  General Vectors and Transformations  Chapters 5 and 6    - row,   j   - column,   k   - layer,   t   - time or iteration  x  =  A y    Weight Matrices  Scalar Element  t   k wi j i  Matrix  Wk t   Column Vector  k t   wj  wk  i  t   Row Vector  Bias Vector  Scalar Element  k t  bi  Vector bk t   B-1  B   B   Notation  Input Vector  Scalar Element  pi t   p t   pq  As One of a Sequence of Input Vectors  As One of a Set of Input Vectors  Net Input Vector  Scalar Element k ni q  k t  ni   or   Vector nk t    or   k nq  Output Vector  Scalar Element k ai q  k t  ai   or   Vector ak t    or   k aq  Transfer Function  Scalar Element  k ai  =  fk ni k    Vector =  ak  fk nk      Target Vector  Scalar Element  ti t    or   ti q  B-2   Number of Layers, Number of Neurons per Layer  Number of Input Vectors  and Targets , Dimension of Input Vector  Parameter Vector  includes all weights and biases   B   Notation  Vector t t    or   tq  Set of Prototype Input Target Vectors  p1 t1 { , }    {  p2 t2  , }   pQ tQ , }  {      Error Vector  Scalar Element ti t  –  ei t   =  Vector e t    or   eq  Sizes and Dimensions  ai t    or   ei q  =  ti q  ai q–  M Sk  ,   Q R  ,   Vector  x  At Iteration  k xk  or   x k   Norm  x  Performance Index  F x   Gradient and Hessian 2   and       F xk  gk=  F xk    =  Ak  B-3  B   B   Notation  Parameter Vector Change  xk  =  xk  1+  xk–  Eigenvalue and Eigenvector  i   and   zi  Fˆ x   Scalar f· n   Matrix  =  d nd  f n   Approximate Performance Index  single time step   Transfer Function Derivative  F· m nm    =    f· m n1 m 0   0  f· m    0  0  0 m n2  0  f· m    m n Sm    Approximate Hessian Matrix  Jacobian Matrix  J x   H  =  JTJ  Sensitivity Vector  Scalar Element  m si    Fˆ --------- m ni  Vector  sm    Fˆ --------- nm  B-4   Partial Matrix  single input vector     and Full Matrix  all inputs   pq  B   Notation  Marquardt Sensitivity Matrix  Scalar Element  m s˜i h    vh ----------- m ni q  =  ek q ----------- m ni q  m S˜ q   and   S˜ m  =  m S˜ 2 S˜ 1  m  S˜ Q  m  Dynamic Networks  Sensitivity  u m sk i  t     eak u t  ----------------- m t   ni  Weight Matrices  IWm l d  LWm l d   Index Sets   - input weight between input   l   and layer   m   at delay   d   - layer weight between layer   l   and layer   m   at delay   d  DLm l DIm l Im f Lm b Lm   - delays in the tapped delay line between Layer l and Layer m.   - delays in the tapped delay line between Input l and Layer m.   - indices of input vectors that connect to layer m.   - indices of layers that directly connect forward to layer m.   - indices of layers that are directly connected backwards to layer  m  or to which layer m connects forward  and that contain no de- lays in the connection.  U x  ELW X u  ES ES u  X u  ELW U x  ES  =    u U      0  LWx u d   Su x    Su x 0  LWx u d    Su x      0      =    x X  =   x    =    x X  =    u U  0  0 d      0  0 d      B-5  B   B   Notation  Definitions  Input Layer  X  - has an input weight, or contains any delays with   any of its weight matrices  Output Layer  U  - its output will be compared to a target during   training, or it is connected to an input layer through a matrix that  has delays associated with it.  Parameters for Backpropagation and Variations  Learning Rate and Momentum     and        ,    and     Learning Rate Increase, Decrease and Percentage Change  Conjugate Gradient Direction Adjustment Parameter  Marquardt Parameters     and     Generalization  Regularization Parameters     ,    and      ---=  Effective Number of Parameters  k    M  Selected Model  ED EW  ,   xML xMP  ,   Sum Squared Error and Sum Squared Weights  Maximum Likelihood and Most Probable Weights  B-6   B   Notation  Feature Map Terms  Distance Between Neurons  dij   - distance between neuron   i   and neuron   j  Neighborhood  j dij  Ni d   =  d    Grossberg and ART Networks  On-Center and Off-Surround Connection Matrices  1  W+  =   and   1 W-  =  1 0  0 0 1  0   0 0  1  0 1  1 1 0  1   1 1  0  Excitatory and Inhibitory Biases  b+   and   b-  Time Constant  Relative Intensity  pi  pi ----= P   where   P  S1 =  j  1=  pj  Instar and Outstar Weight Matrices  W1:2   and   W2:1  Orienting Subsystem Parameters     ,    and       vigilance    ---=  ART1 Learning Law Parameter      B-7  B   Zero Derivative Set, Largest Invariant Set and Closure  Z L  ,    and   L  B   Notation  Lyapunov Stability  Lyapunov Function  V a   Bounded Lyapunov Function Set    =    a:V a      Hopfield Network Parameters  Circuit Parameters ,  Ii   Ti j C Ri  ,   ,   ,   Amplifier Gain    B-8   Introduction  C Software  Introduction  We have used MATLAB, a numeric computation and visualization soft- ware package, in this text. However, MATLAB is not essential for using  this book. The computer exercises can performed with any available pro- gramming language, and the Neural Network Design Demonstrations,  while helpful, are not critical to understanding the material covered in this  book.  MATLAB is widely available and, because of its matrix vector notation and  graphics, is a convenient environment in which to experiment with neural  networks. We use MATLAB in two different ways. First, we have included  a number of exercises for the reader to perform in MATLAB. Many of the  important features of neural networks become apparent only for large scale  problems, which are computationally intensive and not feasible for hand  calculations. With MATLAB, neural network algorithms can be quickly im- plemented, and large scale problems can be tested conveniently. If MAT- LAB is not available, any other programming language can be used to  perform the exercises.  The second way in which we use MATLAB is through the Neural Network  Design Demonstrations, which can be downloaded from the website      hagan.okstate.edu nnd.html. These interactive demonstrations illustrate  important concepts in each chapter. The icon to the left identifies referenc- es to these demonstrations in the text.  MATLAB, or the student edition of MATLAB, version 2010a or later,  should be installed on your computer in a a folder named MATLAB. To cre- ate this directory or folder and complete the MATLAB installation process,  follow the instructions given in the MATLAB documentation. Take care to  follow the guidelines given for setting the path.   After the Neural Network Design Demonstration software has been loaded  into the MATLAB directory on your computer  or if the MATLAB path has  been set to include the directory containing thedemonstration software ,  the demonstrations can be invoked by typing nnd at the MATLAB prompt.  All demonstrations are easily accessible from a master menu.   C-1  C   C   Software  Overview of Demonstration Files  Running the Demonstrations You can run the demonstrations directly by typing their names at the  MATLAB prompt. Typing help nndesign brings up a list of all the demos you  can choose from.   Alternatively, you can run the Neural Network Design splash window  nnd   and then click the Contents button. This will take you to a graphical Table  of Contents. From there you can select chapters with buttons at the bottom  of the window and individual demonstrations with popup menus.   Sound Many of the demonstrations use sound. In many cases the sound adds to  the understanding of a demonstration. In other cases it is there simply for  fun. If you need to turn the sound off you can give MATLAB the following  command and all demonstrations will run quietly:   To turn sound back on:    nnsound off    nnsound on   List of Demonstrations  General  You may note that demonstrations that utilize sound often run faster when  sound is off. In addition, on some machines which do not support sound er- rors can occur unless the sound is turned off.  nnd - Splash screen. nndtoc - Table of contents. nnsound - Turn Neural Network Design sounds on and off.  Chapter 2, Neuron Model and Network Architectures  nnd2n1 - One-input neuron. nnd2n2 - Two-input neuron.  Chapter 3, An Illustrative Example  nnd3pc - Perceptron classification. nnd3hamc - Hamming classification. nnd3hopc - Hopfield classification.  C-2   Overview of Demonstration Files  Chapter 4, Perceptron Learning Rule nnd4db - Decision boundaries. nnd4pr - Perceptron rule.  Chapter 5, Signal and Weight Vector Spaces  nnd5gs - Gram-Schmidt. nnd5rb - Reciprocal basis.  Chapter 6, Linear Transformations for Neural Networks  nnd6lt - Linear transformations. nnd6eg - Eigenvector game.  Chapter 7, Supervised Hebbian Learning  nnd7sh - Supervised Hebb.  Chapter 8, Performance Surfaces and Optimum Points  nnd8ts1 - Taylor series 1. nnd8ts2 - Taylor series 2. nnd8dd - Directional derivatives. nnd8qf - Quadratic function.  Chapter 9, Performance Optimization  nnd9sdq - Steepest descent for quadratic function. nnd9mc - Method comparison. nnd9nm - Newton's method. nnd9sd - Steepest descent.  Chapter 10, Widrow-Hoff Learning  nnd10nc - Adaptive noise cancellation. nnd10eeg - Electroencephalogram noise cancellation. nnd10lc - Linear pattern classification.  Chapter 11, Backpropagation  nnd11nf - Network function. nnd11bc - Backpropagation calculation. nnd11fa - Function approximation. nnd11gn - Generalization.  C-3  C   C   Software  Chapter 12, Variations on Backpropagation  nnd12sd1- Steepest descent backpropagation 1. nnd12sd2 - Steepest descent backpropagation 2. nnd12mo - Momentum backpropagation. nnd12vl - Variable learning rate backpropagation. nnd12ls - Conjugate gradient line search. nnd12cg - Conjugate gradient backpropagation. nnd12ms - Maquardt step. nnd12m - Marquardt backpropagation.  Chapter 13, Generalization  nnd13es - Early stoppinng. nnd13reg - Regularization. nnd13breg - Bayesian regularization. nnd13esr - Early stopping regularization.  Chapter 14, Dynamic Networks  nnd14fir - Finite impulse response network. nnd14iir - Infinite impulse response network. nnd14dynd - Dynamic derivatives. nnd14rnt - Recurrent network training.  Chapter 15, Associative Learning  nnd15uh - Unsupervised Hebb. nnd15edr - Effect of decay rate. nnd15hd - Hebb with decay. nnd15gis - Graphical instar. nnd15is - Instar. nnd15os - Outstar.  Chapter 16, Competitive Networks  nnd16cc - Competitive classification. nnd16cl - Competitive learning. nnd16fm1 - 1-D feature map. nnd16fm2 - 2-D feature map. nnd16lv1 - LVQ1. nnd16lv2 - LVQ2.  Chapter 17, Radial Basis Networks nnd17nf - Network function. nnd17pc - Pattern classification. nnd17lls - Linear least squares. nnd17ols - Orthogonal least squares. nnd17no - Nonlinear optimization.  C-4   Overview of Demonstration Files  Chapter 18, Grossberg Network nnd18li - Leaky integrator. nnd18sn - Shunting network. nnd18gl1 - Grossberg layer 1. nnd18gl2 - Grossberg layer 2. nnd18aw - Adaptive weights.  Chapter 19, Adaptive Resonance Theory  nnd19al1 - ART1 layer 1. nnd19al2 - ART1 layer 2. nnd19os - Orienting subsystem. nnd19a1 - ART1 algorithm.  Chapter 20, Stability  nnd20ds - Dynamical system.  Chapter 21, Hopfield Network  nnd21hn - Hopfield network.  C-5  C   Index  I  Index  A Abbreviated notation 2-8 ADALINE network 10-2  decision boundary 10-4 mean square error 10-4  Adaptive filtering 10-13 Adaptive noise cancellation 10-15 Adaptive resonance theory  ART  19-2 Amacrine cells 18-4 Amari, S. 18-2 AND gate 4-7 Anderson, J.A. 1-2, 1-3, 15-2, 18-2 Angle 5-7 Apple and Orange Example 3-2  Hamming network solution 3-8 Hopfield solution 3-12 perceptron 3-3 perceptron solution 3-5 problem statement 3-2  Applications of neural networks 1-5  aerospace 1-5 automotive 1-5 banking 1-5 defense 1-6 electronics 1-6 entertainment 1-6 financial 1-6 insurance 1-6 manufacturing 1-6 medical 1-6 oil and gas 1-6 robotics 1-7 securities 1-7 speech 1-7 telecommunications 1-7 transportation 1-7  ART1  fast learning 19-19  Layer 1 19-4 Layer 2 19-10 learning law  L1 - L2 19-17 L2 - L1 19-20  orienting subsystem 19-13 resonance 19-17 subset superset dilemma 19-17 summary 19-21 vigilance 19-15  ART2 19-23 ART3 19-23 ARTMAP 19-23 Associative learning  Hebb rule 7-4 instar rule 15-11 Kohonen rule 15-15 outstar rule 15-17 pseudoinverse rule 7-7 unsupervised Hebb rule 15-5  Associative memory 7-3  autoassociative memory 7-10 Hopfield network 21-5 linear associator 7-3  Associative networks 15-3  instar 15-9 outstar 15-16 Attractors 21-11 Autoassociative memory 7-10 Autocorrelation function 22-24  B Backpropagation 11-7  batching 12-7 CGBP 12-15 choice of network architecture 11-18 conjugate gradient 12-14 convergence 11-20  Index-1   delta-bar-delta 12-13 drawbacks 12-3, 14-3 example 11-14 generalization 11-22 initial weights 12-6 Jacobian matrix 12-23 Levenberg-Marquardt 12-19, 12-21  Jacobian calculation 12-22 Marquardt sensitivity 12-24  LMBP 12-25 MOBP 12-11 performance index 11-8 performance surface 12-3 Quickprop 12-14 SDBP 12-2 sensitivity 11-10 summary 11-13 SuperSAB 12-14 VLBP 12-12  Backpropagation order 14-4 Backpropagation-through-time  BPTT  14-2, 14-  11, 14-22  Backward elimination 17-18 Basis set 5-5 Batch SOFM learning algorithm 26-6 Batch training 11-17 Batching 12-7 Bayes’ rule 13-10 Bayes’ Theorem 13-10 Bayesian analysis 13-12  effective number of parameters 13-16 evidence 13-13 Gauss-Newton  approximation  to  Bayesian   regularization 13-17  likelihood function 13-13 posterior density 13-13 prior density 13-13  Bayesian regularization 13-12, 23-6  effective number of parameters 23-6  Baysian analysis  most probable 13-14  Biological inspiration of neural networks 1-8 Bipolar cells 18-3 Brightness constancy 18-8  Index-2  Index  C Carpenter, G. 19-2 Center 17-6 CGBP 12-15 Chain rule 11-9 Change of basis 6-6  similarity transformation 6-8  Chemical vapor deposition 24-2 Choice of network architecture 11-18 Circular hollow 8-16 Clustering 22-9 Coding the targets 22-7 Committee of networks 22-18, 25-10 Competitive learning 16-7  adaptive resonance theory 19-2 ART1 19-4 ART2 19-23 ART3 19-23 ARTMAP 19-23 Fuzzy ARTMAP 19-23 instar rule 16-7 Kohonen rule 16-7 learning rate 16-9 LVQ2 16-21 problems 16-9  Competitive networks 16-5  ART1 19-4 Grossberg 18-13 Hamming network 16-3 Lateral inhibition 16-5 learning vector quantization 16-16 self-organizing feature map 16-12 winner-take-all 16-5  Conditioned stimulus 15-3 Cones 18-3 Confusion matrix 22-21, 25-7 Conjugate directions 9-16 Conjugate gradient 9-15, 12-14 golden section search 12-17 interval location 12-16 interval reduction 12-16  Content-addressable memory 21-16 Contour plot 8-8 Contrast enhance 18-18   Index  I  control  electromagnet 27-2 Correlation matrix 10-6 Cross-correlation function 22-25 Cross-entropy 22-17 Cross-validation 13-6 Curse of dimensionality 17-11  D Decay rate 15-7 Decision boundary 4-5, 10-4, 11-4 Delay 2-13 Delta rule 7-13, 10-7 Delta-bar-delta 12-13 Descent direction 9-3 Diagonalization 6-13 Directional derivative 8-5 Distortion measure 22-23 Domain 6-2 Dynamic networks 14-2  E Early stopping 13-7, 13-19, 25-7 Echo cancellation 10-21 EEG 10-15 Effective number of parameters 13-16, 13-23, 23-  6  Eigenvalues 6-10 Eigenvectors 6-10 Electrocardiagram 25-2 electromagnet 27-2 Elliptical hollow 8-17 Emergent segmentation 18-6 Equilibrium point 20-4 Euclidean space 5-3 evidence 13-13 Excitatory 18-10 Extended Kalman filter algorithm 22-14 Extrapolation 13-3, 22-21, 22-27  F Fahlman, A.E. 12-14 False negative 22-21 False positive 22-22  Index-3  Gauss-Newton approximation to Bayesian regu-  Featural filling-in 18-6 Feature extraction 22-6, 25-3 Finite impulse response 14-6 Fitting 22-8 Forest cover 26-2 Forward selection 17-18 Fovea 18-5 Fukushima, K. 18-2 Function approximation 11-4, 22-8 Fuzzy ARTMAP 19-23  G Ganglion cells 18-4 Gauss-Newton algorithm 12-21  Jacobian matrix 12-20  larization 13-17  Generalization 11-22, 13-2 Golden Section search 12-17 Gradient 8-5 Gradient descent 9-2 Gram-Schmidt orthogonalization 5-8 Grossberg competitive network 18-13 choice of transfer function 18-20 Layer 1 18-13 Layer 2 18-17 learning law 18-22  relation to Kohonen law 18-24  Grossberg, S. 1-3, 15-2, 18-2, 19-2  H Hamming network 3-8, 16-3  feedforward layer 3-8, 16-3 recurrent layer 3-9, 16-4  Hebb rule 7-4, 21-18 decay rate 15-7 performance analysis 7-5 supervised 7-4 unsupervised 7-4, 15-5 with decay 7-12 Hebb, D.O. 1-3, 7-2 Hebb’s postulate 7-2 Hebbian learning 7-2  variations 7-12   Hessian 8-5  eigensystem 8-13  Hidden layer 2-11 High-gain Lyapunov function 21-13 Hinton, G.E. 11-2 Histogram of errors 22-21 History of neural networks 1-2 Hoff, M.E. 1-3, 10-2, 11-2 Hopfield model 21-3 Hopfield network 3-12, 6-2, 21-5  attractors 21-11 design 21-16  content-addressable memory 21-16  effect of gain 21-12 example 21-7 Hebb rule 21-18 high-gain Lyapunov function 21-13 Lasalle’s invariance theorem 21-7 Lyapunov function 21-5 Lyapunov surface 21-22 spurious patterns 21-20  Hopfield, J.J. 1-4 Horizontal cells 18-4 Hubel, D.H. 16-2, 18-12  I Illusions 18-4 Incremental training 11-17 Infinite impulse response 14-7 Inhibitory 18-10 Inner product 5-6 Input selection 22-12 Input weight 14-3 Instar 15-9 Instar rule 15-11, 16-7 Integrator 2-13 Interpolation 13-3 Interval location 12-16 Interval reduction 12-16 Invariant set 20-13  J Jacobian matrix 12-20 Jacobs, R.A. 12-13  Index  K Kohonen rule 15-15, 16-7  graphical representation 16-7  Kohonen, T. 1-3, 15-2, 18-2  L LaSalle’s corollary 20-14 LaSalle’s invariance theorem 20-13  invariant set 20-13 Set  L 20-13 Z 20-12  Lateral inhibition 16-5 Layer 2-9  competitive 16-5  problems 16-9  hidden 2-11 output layer 2-11 superscript 2-11  Layer weight 14-3 Layered Digital Dynamic Network  LDDN  14-3 Le Cun, Y. 11-2 Leaky integrator 18-9 Learning rate 9-3, 10-8  competitive learning 16-9 stable 9-6, 10-10  Learning rules 4-2  ART1 19-21 backpropagation 11-7 competitive learning 16-7 delta rule 7-13 Grossberg competitive network 18-22 Hebb rule 7-4 Hebbian learning 7-2 learning vector quantization 16-16 LMS algorithm 10-7 local learning 15-5 perceptron 4-8, 4-13  proof of convergence 4-15  performance learning 8-2 pseudoinverse rule 7-7 reinforcement learning 4-3 supervised learning 4-3 unsupervised learning 4-3  Index-4   Index  Widrow-Hoff 7-13  Learning vector quantization  LVQ  16-16  Levenberg-Marquardt  algorithm  12-19,  12-21,   I  subclass 16-17  22-14, 24-7  Jacobian calculation 12-22 Jacobian matrix 12-20 Likelihood function 13-13 Linear associator 7-3 Linear independence 5-4 Linear initialization 26-6 Linear least squares 17-11 Linear separability 4-19 Linear transformation 6-2  change of basis 6-6 domain 6-2 matrix representation 6-3  change of basis 6-6  range 6-2  Linear vector spaces 5-2 LMBP 12-25 LMS algorithm 10-2, 10-7  adaptive filtering 10-13 adaptive noise cancellation 10-15 analysis of convergence 10-9 learning rate 10-8 stable learning rate 10-10  Local learning 15-5 Long term memory  LTM  18-12, 18-22 LVQ2 16-21 Lyapunov function 20-12 Lyapunov stability theorem 20-6  M Mach, E. 1-2 magnet 27-2 Marquardt algorithm 12-19 Marquardt sensitivity 12-24 Matrix representation 6-3  change of basis 6-6 diagonalization 6-13  McClelland, J.L. 1-4, 11-2 McCulloch, W.S. 1-3, 4-2 Mean square error 10-4, 11-8  Mean squared error 22-16 Memory  associative 7-3 autoassociative 7-10  Mexican-hat function 16-11 Minima 8-7  first-order conditions 8-10 global minimum 8-7 necessary conditions 8-9 second-order conditions 8-11 strong minimum 8-7 sufficient condition 8-11 weak minimum 8-7 Minkowski error 22-17 Minsky, M. 1-3, 4-2 Missing data 22-8 MOBP 12-11 Molecular dynamics 24-3 Momentum 12-9, 15-7 Monte Carlo simulation 25-9 Most probable 13-14 Multilayer perceptron 11-2 Myocardial infarction 25-2  N NARX network 22-10 Negative definite matrix 8-11 Negative semidefinite 8-11 Neighborhood 16-12 Network architectures 2-9  layer 2-9 multiple layers 2-10  Neural Network Toolbox for MATLAB 1-5 Neuron model 2-2  multiple input neuron 2-7 single input neuron 2-2 transfer functions 2-3  Newton’s method 9-10 Nguyen-Widrow weight initialization 22-13, 24-9 Nilsson, N. 16-2 Noise cancellation adaptive 10-15 echo cancellation 10-21  Norm 5-7  Index-5   Normalization 22-5, 25-6, 26-4 Novelty detection 22-28  O Ockham’s razor 13-2 On-center off-surround 16-11, 18-14 Optic disk 18-5 Optimality  first-order conditions 8-10 necessary conditions 8-9 second-order conditions 8-11 sufficient condition 8-11  Optimization  conjugate gradient 9-15, 12-14 descent direction 9-3 Gauss-Newton 12-21 Levenberg-Marquardt 12-19, 12-21 Newton’s method 9-10 quadratic termination 9-15 steepest descent 9-2  stable learning rates 9-6  Oriented receptive field 18-20 Orienting subsystem 19-13 Orthogonal least squares 17-18 Orthogonality 5-7 Orthonormal 5-9 Outliers 22-19 Outstar 15-16 Outstar rule 15-17 Overfitting 13-3, 22-27  P Papert, S. 1-3, 4-2 Parker, D.B. 11-2 Pattern classification 11-3, 22-9, 25-2 Pavlov, I. 1-2 Perceptron 3-3  architecture 4-3 constructing learning rules 4-10 decision boundary 4-5 learning rule 4-8, 4-13  proof of convergence 4-15  multilayer 11-2 multiple-neuron 4-8  Index  single-neuron 4-5 test problem 4-9 training multiple-neuron perceptrons 4-13 two-input case 3-4 unified learning rule 4-12  Performance Index 11-8 Performance index 8-2, 22-16  cross-entropy 22-17 mean squared error 22-16 Minkowski error 22-17 quadratic function 8-12  Performance learning 8-2 Pitts, W.H. 1-3, 4-2 Positive definite 20-5 Positive definite matrix 8-11 Positive semidefinite 8-11, 20-5 posterior density 13-13 Post-training analysis 22-18 Prediction 22-10, 22-24 Preprocessing 22-5  coding the targets 22-7 feature extraction 22-6 normalization 22-5, 25-6, 26-4 principal component analysis 22-6  Principal component analysis 22-6 Prior density 13-13 Probability estimation 24-2 Projection 5-8 Prototype patterns 21-16 Pseudoinverse rule 7-7  Q Quadratic function 8-12 circular hollow 8-16 elliptical hollow 8-17 Hessian  eigensystem 8-13  saddle point 8-18 stationary valley 8-19  Quadratic termination 9-15 Quantization error 22-23 Quickprop 12-14  Index-6   Index  I  R R value 22-20 Radial basis network 17-2 backpropagation 17-25 center 17-6 pattern classification 17-6  Range 6-2 RBF 17-2 Real-time recurrent learning  RTRL  14-2, 14-11,   Receiver  operating  characteristic   ROC   curve   14-12  22-22, 25-8  Reciprocal basis vectors 5-10 Recurrent 14-2 Recurrent network 2-13, 2-14, 20-2 regression 22-8 Regression scatter plot 22-20 Regularization 13-8, 13-19, 13-21 Reinforcement learning 4-3 Resonance 19-17 Retina 18-3 Rods 18-3 Rosenblatt, F. 1-3, 4-2, 10-2, 11-2, 16-2 Rosenfeld, E. 1-2 Rumelhart, D.E. 1-4, 11-2  S Saddle point 8-8, 8-18 Scaled conjugate gradient algorithm 22-14 SDBP 12-2 Segmentation 22-9 Self-organizing  feature  map   SOFM   16-12,  22-  16, 26-2  distortion measure 22-23 neighborhood 16-12 quantization error 22-23 topographic error 22-23  Sensitivity 11-10  backpropagation 11-11 Sensitivity analysis 22-28 Set  L 20-13 Z 20-12  Shakespeare, W. 1-5  Short term memory  STM  18-12, 18-17 Shunting model 18-10 Similarity transform 6-8 Simulation order 14-4 Smart sensor 23-2 Softmax 22-7, 24-6 Spanning a space 5-5 Spurious patterns 21-20 Stability  asymptotically stable 20-3, 20-5 concepts 20-3 equilibrium point 20-4 in the sense of Lyapunov 20-3, 20-4 LaSalle’s corollary 20-14 LaSalle’s invariance theorem 20-13 Lyapunov function 20-12 Lyapunov stability theorem 20-6 pendulum example 20-6  Stability-plasticity dilemma 19-2 Stationary point 8-10  minima 8-7 saddle point 8-8  Stationary valley 8-19 Steepest descent 9-2 learning rate 9-3 minimizing along a line 9-8 stable learning rates 9-6  Stimulus-response 15-2  conditioned stimulus 15-3 unconditioned stimulus 15-3  Stopping criteria 22-15 Subclass 16-17 Subset selection 17-18 Subset superset dilemma 19-17 SuperSAB 12-14 Supervised learning 4-3  Hebb rule 7-4 performance learning 8-2 target 4-3 training set 4-3  T Tapped delay line 10-13 Target 4-3  Index-7   Index  Vision normalization 18-8 Visual cortex 18-4 VLBP 12-12 von der Malsburg, C. 16-2, 18-12  W Weight indices 2-7 Weight initialization 22-13, 24-9 Weight matrix 2-7 Werbos, P.J. 11-2 White noise 22-24 Widrow, B. 1-3, 10-2, 11-2 Widrow-Hoff algorithm 7-13, 10-7  adaptive filtering 10-13  Wiesel, T. 16-2, 18-12 Williams, R.J. 11-2 Winner-take-all 16-5  Taylor series expansion 8-2  vector case 8-4  Test set 13-6 Thomas Bayes 13-10 Tikhonov 13-8 Time constant 18-9 Tollenaere, T. 12-14 Topographic error 22-23 Training process 22-2 Training set 4-3  sequence 15-5  Transfer functions 2-3, 2-6  competitive 2-6 global vs local 17-9 hard limit 2-3, 2-6 hyperbolic tangent sigmoid 2-6 linear 2-4, 2-6 log-sigmoid 2-4, 2-6 positive linear 2-6 saturating linear 2-6 softmax 22-7, 24-6 symmetric saturating linear 2-6 symmetrical hard limit 2-6 table 2-6  Type I error 22-22  U Unconditioned stimulus 15-3 Unsupervised learning 4-3  Hebb rule 7-4, 15-5  V Validation set 13-7 Vector expansion 5-9  reciprocal basis vectors 5-10  Vector space 5-2  angle 5-7 basis set 5-5 orthonormal 5-9 projection 5-8 spanning 5-5 vector expansion 5-9  Vigilance 19-15 Vision 18-3  Index-8   This book provides a clear and detailed coverage of fundamental neural network architectures and learning rules. In it, the authors emphasize a coherent presentation of the principal neural networks, methods for training them and their applications to practical problems.  Features  cid:2  Extensive coverage of training methods for both feedforward networks  including  multilayer and radial basis networks  and recurrent networks. In addition to conjugate gradient and Levenberg-Marquardt variations of the backpropagation algorithm, the text also covers Bayesian regularization and early stopping, which ensure the generalization ability of trained networks.  Associative and competitive networks, including feature maps and learning vector  quantization, are explained with simple building blocks.  A chapter of practical training tips for function approximation, pattern recognition,  clustering and prediction, along with five chapters presenting detailed real-world case studies.  Detailed examples and numerous solved problems. Slides and comprehensive  demonstration software can be downloaded from hagan.okstate.edu nnd.html.   cid:2    cid:2    cid:2   About the Authors  Martin T. Hagan  Ph.D. Electrical Engineering, University of Kansas  has taught and conducted research in the areas of control systems and signal processing for the last 35 years. For the last 25 years his research has focused on the use of neural networks for control, filtering and prediction. He is a Professor in the School of Electrical and Computer Engineering at Oklahoma State University and a co-author of the Neural Network Toolbox for MATLAB.  Howard B. Demuth  Ph.D. Electrical Engineering, Stanford University  has twenty-three years of industrial experience, primarily at Los Alamos National Laboratory, where he helped design and build one of the world's first electronic computers, the "MANIAC." Demuth has fifteen years teaching experience as well. He is co-author of the Neural Network Toolbox for MATLAB and currently teaches a Neural Network course for the University of Colorado at Boulder.  Mark Hudson Beale  B.S. Computer Engineering, University of Idaho  is a software engineer with a focus on artificial intelligence algorithms and software development technology. Mark is co-author of the Neural Network Toolbox for MATLAB and provides related consulting through his company, MHB Inc., located in Hayden, Idaho.  Orlando De Jesús  Ph.D. Electrical Engineering, Oklahoma State University  has twenty- four years of industrial experience, with AETI C.A. in Caracas, Venezuela, Halliburton in Carrollton, Texas and is currently working as Engineering Consultant in Frisco, Texas. Orlando’s dissertation was a basis for the dynamic network training algorithms in the Neural Network Toolbox for MATLAB.
