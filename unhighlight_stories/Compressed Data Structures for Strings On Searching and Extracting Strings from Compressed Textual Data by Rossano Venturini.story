Atlantis Studies in Computing 4 Series Editors: J. A. Bergstra · M. W. Mislove  Rossano Venturini   Compressed Data  Structures  for Strings  On Searching and Extracting  Strings from Compressed  Textual Data    Atlantis Studies in Computing  Volume 4  Series Editors  Jan A. Bergstra, Amsterdam, The Netherlands Michael W. Mislove, New Orleans, USA  For further volumes: www.atlantis-press.com   Aims and Scope of the Series  The series aims at publishing books in the areas of computer science, computer and network technology, IT management, information technology and informatics from the technological, managerial, theoretical fundamental, social or historical perspective.  We welcome books in the following categories:  Technical monographs: relevance, completeness and clarity of presentation. Textbooks.  these will be reviewed as to timeliness, usefulness,  Books of a more speculative nature: these will be reviewed as to relevance and clarity of presentation.  For more information on this series and our other book series, please visit our  website at:  www.atlantis-press.com publications books Atlantis Press 29, avenue Laumière 75019 Paris, France   Rossano Venturini  Compressed Data Structures for Strings  On Searching and Extracting Strings from Compressed Textual Data   Rossano Venturini Department of Computer Science University of Pisa Pisa Italy  ISSN 2212-8557 ISBN 978-94-6239-032-4 DOI 10.2991 978-94-6239-033-1  ISSN 2212-8565  electronic  ISBN 978-94-6239-033-1   eBook   Library of Congress Control Number: 2013951770   cid:2  Atlantis Press and the authors 2014 This book, or any parts thereof, may not be reproduced for commercial purposes in any form or by any means, electronic or mechanical, including photocopying, recording or any information storage and retrieval system known or to be invented, without prior permission from the Publisher.  Printed on acid-free paper   Foreword  The Italian chapter of the EATCS  European Association for Theoretical Com- puter Science  was founded in 1988, and aims at facilitating the exchange of ideas and results among Italian theoretical computer scientists, and at stimulating cooperation between the theoretical and the applied communities in Italy.  One of the major activities of this Chapter is to promote research in theoretical computer science, stimulating scientiﬁc excellence by supporting, and encouraging the very best and creative young Italian theoretical computer scientists. This is done also by sponsoring a prize for the best Ph.D. thesis. This award has been usually presented every 2 years and an interdisciplinary committee selects the best two Ph.D. theses, among those defended in the previous 2-year period, one on the themes of Algorithms, Automata, Complexity, and Game Theory, and the other one on the themes of Logics, Semantics, and Programming Theory.  In 2012 we started a cooperation with Atlantis Press so that the selected Ph.D. theses will be published as volumes in the Atlantis Studies in Computing. The present volume contains one of the two theses selected for publication in 2012: Logics in Computer Science by Fabio Mogavero  supervisor: Prof. Aniello  Murano, University of Naples ‘‘Federico II’’, Italy   and On Searching and Extracting Strings from Compressed Textual Data by Rossano  Venturini  supervisor: Prof. Paolo Ferragina, University of Pisa, Italy .  The scientiﬁc committee which selected these theses was composed of Professors Marcella Anselmo  University of Salerno , Pierluigi Crescenzi  Uni- versity of Florence , and Gianluigi Zavattaro  University of Bologna .  They gave the following reasons to justify the assignment of the award to the  thesis by  Rossano Venturini:  The thesis by Rossano Venturini deals with problems connected to data com- pression and to their access without incurring in their whole decompression. In both cases this work contributes signiﬁcantly, both from a theoretical and an applicative point of view.  In particular, the ﬁrst part of this thesis deals with data compressors. On the one hand it analyzes the performance of a given compressor, in relation to the compression either of the whole input text or partitioned into sub-texts; on the  v   vi  Foreword  other hand it studies the performance of compressors based on the well known Lempel and Ziv technique. The theoretical results are sustained by some experiments.  The second part deals with random access to the compressed data. The thesis includes a compressed storage scheme optimizing the access time and an impressive collection of experiments. The ﬁrst result appeared in 2007 and it has already been cited more than 60 times. The second one was published on Journal of Experimental Algorithms in 2009 and has already been cited almost 70 times. Finally, the thesis proposes a compressed full-text index  whose primitives are hence different from those used for a text search  that is fast and with good performance in terms of used space. This scheme has been presented at SIGIR in 2007, appeared in 2010 on ACM Transactions on Algorithms and it has already been cited 21 times.  This year the Italian Chapter of EATCS decided to dedicate the two prizes to the memory of Prof. Stefano Varricchio  the one on themes of Algorithms, Automata, Complexity, and Game Theory , and Prof. Nadia Busi  the one on themes on Logics, Semantics, and Programming Theory , both excellent researchers who prematurely passed away.  I hope that this initiative will further contribute to strengthen the sense of belonging to the same community of all the young researchers who have accepted the challenges posed by any branch of theoretical computer science.  Tiziana Calamoneri   Preface  Data volumes are exploding as organizations and users collect and store increasing amounts of information for their own use and for sharing it with others. To cope with these large datasets, software developers typically take advantage of faster and faster I O-subsystems and multicore processors, and or they exploit the virtual memory to make the caching and delivering of data requested by their algorithms simple and effective whenever their working set is small. Sometimes, they gain an additional speed-up by reducing the storage usage of their algorithms because this impacts favorably on the number of machines disks required for a given compu- tation, and on the amount of data that is transferred cached to in the faster memory levels closer to the CPUs. However it is well-known, both in algorithm and software engineering communities, that the principled exploitation of all these issues via a proper arrangement of data and a properly structured algorithmic computation can abundantly surpass the best expected technology advancements and the help coming from operating systems or heuristics. As a result, data compression and indexing nowadays play a key role in the design of modern algorithms for applications that manage massive datasets. Their effective combi- nation is, however, not easy because of three main reasons:   each memory level  cache, DRAM, mechanical disk, SSD,...  has its own cost, capacity, latency, and bandwidth, and thus accessing data in the memory levels closer to the CPU is orders of magnitude faster than accessing data at the last levels. Therefore, space-efﬁcient algorithms and data structures should be space and I O-conscious and thus deploy  temporal and spatial  locality of reference as a key principle in their design.    compressed space typically comes at a cost—namely, compression and or decompression time—so that compression should be plugged into algorithms and data structures without impairing the efﬁciency of their operations.    data compression and indexing seem ‘‘opposite approaches’’ because the former aims at removing data redundancies, whereas the latter introduces extra-data in the index to support faster operations.  It is, thus, not surprising that, until recently, algorithm and software designers were faced with a dilemma: achieve either efﬁcient compression at the cost of slow operations over the compressed data, or vice versa.  vii   viii  Preface  This dichotomy was successfully addressed starting from the year 2000 [Ferragina and Manzini  2005 ], due to various scientiﬁc achievements which showed how to relate Information Theory and String-Matching concepts, in a way that index regularities that show up when data is compressible are discovered and exploited to reduce index occupancy without impairing query efﬁciency  see the surveys [Navarro and Mäkinen  2007 ; Ferragina et al.  2008a ] and references therein . The net result has been the design of compressed data structures for indexing data  aka compressed indexes, or compressed and searchable data for- mats  that take space close to the kth order entropy of the input data, and support the powerful substring queries and the extraction of arbitrary portions of data in time close to the one required by  optimal  uncompressed indexes. Given this latter feature, these data structures are sometime called self-indexes.  Originally designed for raw data  i.e., strings , compressed indexes have been recently extended to deal with other data types, such as sequences  see e.g., [Ferragina et al.  2007 ; Ferragina and Venturini  2007b ; Pa˘tras cid:2 cu  2008 ; Grossi et al.  2013 ] , dictionaries  see e.g., [Ferragina and Venturini  2010, 2013 ; Hon et al.  2008 ] , unlabeled trees  see e.g., [Benoit et al.  2005 ; Jansson et al.  2007 ; Farzan et al.  2009 ; Sadakane and Navarro  2010 ] , labeled trees  see e.g., [Ferragina et al.  2009b ; Ferragina and Rao  2008 ] , graphs  see e.g., [Claude and Navarro  2010 ] , binary relations and permutations  see e.g., [Barbay and Navarro  2009 ; Barbay et al.  2010 ] , and many others. The consequence of this impressive ﬂow of results is that, nowadays, it is known how to index almost any  complex  data-type in compressed space and support various kinds of queries fast over it. From a theoretical point of view, and as far as the RAM model is con- cerned, the above dichotomy may be considered successfully addressed.  The thesis of Rossano Venturini provides signiﬁcant contributions to several of those issues, by achieving deep results in four main scenarios which combine in different ways compression and indexing goals, looking at them from the theo- retical and the engineering perspective. Here I summarize the main achievements and refer the reader for more details to the cited Chapters and the numerous publications that spurred from this research.  Lossless data compression. The problem of lossless data compression consists of compactly representing data in a format that allows their faithful recovery. A recent challenging trend has been the one which asks to improve the performance of a given compressor C by partitioning the input data in a way that compressing each individual part by C is better than applying C to the whole input. Chapter 3 investigates this problem by taking as compressor C one whose compression performance can be bounded in terms of the zero-th or the kth order empirical entropy of the input data. Previous results offered poor or sub-optimal solutions [Buchsbaum et al.  2003 ; Giancarlo and Sciortino  2003 ; Buchsbaum et al.  2000, 2003 ; Ferragina et al.  2005a ]. In Chap. 3 it is described the ﬁrst algorithm  cid:3  time and O n  space, a partition of the input data which computes in O n log1þe n whose compressed output is guaranteed to be no more than 1 þ e Þ-worse the optimal one, where e [ 0 is ﬁxed in advance and n is the input length.  ð   cid:2    Preface  ix In the next Chap. 4 is addressed the design of the compressor C, taking into consideration the very famous class of dictionary-based compressors introduced by Lempel and Ziv in the late 70s [Ziv and Lempel  1977 ], currently at the core of many softwares like gzip, zip, pkzip, lzma2, LZ4, just to cite a few. This compression scheme squeezes an input text by replacing some of its substrings with  shorter  codewords which are actually pointers to phrases in a dictionary. Surprisingly enough, although many fundamental results were known about the speed and effectiveness of this compression process, there was no parsing scheme which guaranteed to achieve the minimum number of bits for a ﬁxed class of codewords. Chapter 4 presents an algorithm which achieves bit-optimality in the compressed output-size of LZ77 by taking efﬁcient optimal time and optimal space. The experimental results show also that this theoretical achievement is effective in practice, by beating renowned LZ77-implementations such as lzma2 and LZ4 an interesting example of a win-win situation of a theoretical idea with a practical impact.  Random access to compressed data. This is a key problem in modern data- storage solutions, whenever data have to be compressed and still provided to their underlying applications. Classical compression algorithms fail in offering this facility, because they support only the decompression of the whole compressed ﬁle. Conversely, more and more applications nowadays need to fast access portions of those compressed data: think, e.g., to the collection of Web pages and the search engine that needs to access them at each user query in order to return pertinent snippets for the query results to be shown to the search engine users. Chapter 5 addresses this problem by presenting an elegant scheme that represents data into a compressed form, with provable space bounds in terms of its kth order entropy, still providing access to any of its substrings in optimal time. It is remarkable that this result improves known achievements via a very simple approach.  Engineering compressed indexes. The theory of compressed full-text indexes was mature enough to ask ourselves if and how this could be a breakthrough for compressed data storage. The Pizza and Chili’s effort described in Chap. 6, joint between the University of Pisa and the University of Chile, contributes in this direction by offering a set of tuned implementations of the most successful compressed text indexes, together with effective test-beds and scripts for their automatic validation and test. These indexes, all adhering a standardized API, were extensively tested on various datasets showing that they take roughly the same space used by traditional compressors  like gzip and bzip2 , with the additional feature of being able to extract arbitrary portions of compressed data at the speed of 2–4 MB sec, and to search for arbitrary substrings  hence, not necessarily words  at few lsec per occurrence. Compared to the approach described in the previous point, this adds to the compressed data-storage scheme the capability to search for arbitrary substrings in an efﬁcient way. It is remarkable that the Pizza and Chili site is currently one of the most used software libraries in the Compu- tational Biology community, as witnessed by the numerous papers and interna- tional projects which mention it.   x  Preface  Compressed indexes for string dictionaries. The ﬁnal problem addressed in this the sis concerns with the compressed indexing of a large dictionary of strings having variable length, and supporting tolerant retrieval queries [Manning et al.  2008 ] such as membership, preﬁx sufﬁx, and substring searches. As strings are getting longer and longer, and dictionaries of strings are getting larger and larger, it becomes crucial to devise implementations for the above primitive which are fast and work in compressed space. This is the topic of Chap. 7 that introduces the so-called compressed permuterm index to solve the problem above in efﬁcient time and kth order entropy space. This result is so simple and elegant that it has been mentioned in the Manning-Raghavan-Schülze’s book [Manning et al.  2008 ], and constitutes the core of a submitted US Patent owned by Yahoo!  Overall, I think that this Thesis is a nice balance of signiﬁcant theoretical achievements and precious algorithm-engineering results that provide both a strong contribution to the theory of Data Structures and another witness, if it were further needed, of the fact mentioned at the beginning of my Preface that the proper ‘‘arrangement of data and structuring of computation’’ can introduce improvements which abundantly surpass the best expected technology advance- ments and the cute heuristics devised by software engineers!  Paolo Ferragina University of Pisa  References  Barbay, J. and Navarro, G.  2009 . Compressed representations of permutations, and applications, in Proceedings of the Symposium on Theoretical Aspects of Computer Science  STACS , LIPIcs,Vol. 3  Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, Germany , pp. 111–122. Barbay, J., Claude, F. and Navarro, G.  2010 . Compact rich-functional binary relation representations, in Proceedings of the Latin American Theoretical Informatics  LATIN , Lecture Notes in Computer Science, Vol. 6034  Springer , pp. 170–183.  Benoit, D., Demaine, E., Munro, I., Raman, R., Raman, V. and Rao, S.  2005 . Representing trees  of higher degree, Algorithmica 43, pp. 275–292.  Buchsbaum, A. L., Caldwell, D. F., Church, K. W., Fowler, G. S. and Muthukrishnan, S.  2000 . Engineering the compression of massive tables: an experimental approach, in Proceedings 11th Annual ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 175–184.  Buchsbaum, A. L., Fowler, G. S. and Giancarlo, R.  2003 . Improving table compression with  combinatorial optimization, Journal of the ACM 50, 6, pp. 825–851.  Claude, F. and Navarro, G.  2010 . Extended compact web graph representations, in Algorithms  and Applications, Lecture Notes in Computer Science, Vol. 6060  Springer , pp. 77–91.  Farzan, A., Raman, R. and Rao, S.  2009 . Universal succinct representations of trees? in Proceedings of the 36th International Colloquium on Automata, Languages and Programming  ICALP , Lecture Notes in Computer Science, Vol. 5555  Springer , pp. 451–462.  Ferragina, P. and Manzini, G.  2005 . Indexing compressed text, Journal of the ACM 52, 4,  pp. 552–581.  Ferragina, P. and Venturini, R.  2007b . A simple storage scheme for strings achieving entropy  bounds, Theoretical Computer Science 372, 1, pp. 115–121.   Preface  xi  Ferragina, P. and Rao, S.  2008 . Tree compression and indexing,  in M.-Y. Kao  ed. ,  Ferragina, P. and Venturini, R.  2010 . The compressed permuterm index, ACM Transactions on  Encyclopedia of Algorithms  Springer .  Algorithms 7, 1, p. 10.  Ferragina, P. and Venturini, R.  2013 . Compressed cache-oblivious String B-tree, in ESA 2013:  Proceedings of 21th Annual European Symposium on Algorithms  ESA , pp. 469–480.  Ferragina, P., Giancarlo, R., Manzini, G. and Sciortino, M.   2005a . Boosting textual  compression in optimal linear time, Journal of the ACM 52, pp. 688–713.  Ferragina, P., Manzini, G., Mäkinen, V. and Navarro, G.  2007 . Compressed representations of sequences and full-text indexes, ACM Transactions on Algorithms  TALG  3, 2, p. article 20. Ferragina, P., Gonzáez, R., Navarro, G. and Venturini, R.  2008a . Compressed text indexes:  From theory to practice, ACM Journal of Experimental Algorithmics 13.  Ferragina, P., Luccio, F., Manzini, G. and Muthukrishnan, S.  2009b . Compressing and indexing  labeled trees, with applications, Journal of the ACM 57, 1.  Giancarlo, R. and Sciortino, M.  2003 . Optimal partitions of strings: A new class of Burrows- in Proceedings of the 14th Annual Symposium on  Wheeler compression algorithms, Combinatorial Pattern Matching  CPM   LNCS vol. 2676 , pp. 129–143.  Grossi, R., Raman, R., Rao, S. S. and Venturini, R.  2013 . Dynamic compressed strings with random access, in Proceedings of the 40th International Colloquium on Automata, Languages and Programming  ICALP , pp. 504–515.  Hon, W.-K., Lam, T. W., Shah, R., Tam, S.-L. and Vitter, J. S.  2008 . Compressed index for IEEE Data Compression Conference  DCC ,  in Proceedings of  dictionary matching, pp. 23–32.  Proceedings of pp. 575–584   Cambridge University Press .  1, p. article 2.  Jansson, J., Sadakane, K. and Sung, W.  2007 . Ultra-succinct representation of ordered trees, in the 18th ACM-SIAM Symposium on Discrete Algorithms  SODA ,  Manning, C. D., Raghavan, P. and Schülze, H.  2008 . Introduction to Information Retrieval  Navarro, G. and M¨ akinen, V.  2007 . Compressed full text indexes, ACM Computing Surveys 39,  Pa˘trasùcu, M.  2008 . Succincter, in Proceedings of the 49th Annual IEEE Symposium on  Foundations of Computer Science  FOCS , pp. 305–313.  Sadakane, K. and Navarro, G.  2010 . Fully-functional succinct trees, in Proceedings of the  ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 134–149.  Ziv, J. and Lempel, A.  1977 . A universal algorithm for sequential data compression, IEEE  Transaction on Information Theory 23, pp. 337–343.   Contents  1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  2 Basic Concepts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 Models of Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.1 RAM Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.2 External-Memory Model . . . . . . . . . . . . . . . . . . . . . . . 2.2 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Classical Full-Text Indexes. . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.1 Suffix Tree. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.2 Suffix Array . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.1 Empirical Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.2 Burrows-Wheeler Transform . . . . . . . . . . . . . . . . . . . .  3 Optimally Partitioning a Text to Improve Its Compression . . . . . . 3.1 The PPC-Paradigm and Motivations for Optimal Partitioning . . . 3.2 The Problem and our Solution . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 A Pruning Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Space and Time Efficient Algorithms for Generating Ge T  . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 On Zero-th Order Compressors . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 On Optimal Partition and Booster . . . . . . . . . . . . . . . . . 3.4 On k-th Order Compressors . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 On BWT-Based Compressors . . . . . . . . . . . . . . . . . . . . . . . . .  4 Bit-Complexity of Lempel-Ziv Compression . . . . . . . . . . . . . . . . . 4.1 Notation and Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 An Efficient and Bit-Optimal Greedy Parsing . . . . . . . . . . . . . . 4.3 On the Bit-Efficiency of the Greedy LZ77-Parsing . . . . . . . . . . 4.4 On Bit-Optimal Parsings and Shortest-Path Problems. . . . . . . . . 4.4.1 An Useful and Small Subgraph of GðTÞ. . . . . . . . . . . . . 4.4.2 An Efficient Bit-Optimal Parser . . . . . . . . . . . . . . . . . . 4.4.3 On the Optimal Construction of T B . . . . . . . . . . . . . . . . 4.5 An Experimental Support to our Theoretical Findings . . . . . . . .  1  5 5 5 6 6 7 7 9 10 11 12  15 16 19 20  21 23 26 27 28  33 36 37 40 42 42 45 51 52  xiii   xiv  Contents  5 Fast Random Access on Compressed Data . . . . . . . . . . . . . . . . . . 5.1 Our Storage Scheme for Strings . . . . . . . . . . . . . . . . . . . . . . . 5.2 Huffman on Blocks of Symbols . . . . . . . . . . . . . . . . . . . . . . . 5.3 BWT Compression and Access . . . . . . . . . . . . . . . . . . . . . . . .  6 Experiments on Compressed Full-Text Indexing . . . . . . . . . . . . . . 6.1 Basics and Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1.1 Backward Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1.2 Rank Query . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Compressed Indexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.1 The FM-index Family . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.2 Implementing the FM-index . . . . . . . . . . . . . . . . . . . . . 6.2.3 The Compressed Suffix Array . . . . . . . . . . . . . . . . . . . 6.2.4 The Lempel-Ziv Index. . . . . . . . . . . . . . . . . . . . . . . . . 6.2.5 Novel Implementations . . . . . . . . . . . . . . . . . . . . . . . . 6.3 The Pizza and Chili Site. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3.1 Indexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3.2 Texts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.1 Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.2 Counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.3 Locate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.4 Extract. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.5 Final Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . .  55 56 58 59  61 64 64 65 66 66 68 69 71 72 74 75 76 78 79 80 81 84 87  7 Dictionary Indexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Compressed Permuterm Index. . . . . . . . . . . . . . . . . . . . . . . . . 7.2.1 A Simple, but Inefficient Solution. . . . . . . . . . . . . . . . . 7.2.2 A Simple and Efficient Solution . . . . . . . . . . . . . . . . . . 7.3 Dynamic Compressed Permuterm Index . . . . . . . . . . . . . . . . . . 7.3.1 Deleting One Dictionary String. . . . . . . . . . . . . . . . . . . 7.3.2 Inserting One Dictionary String . . . . . . . . . . . . . . . . . . 7.4 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.5 Further Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  89 91 92 93 93 98 99 100 102 106  8 Future Directions of Research . . . . . . . . . . . . . . . . . . . . . . . . . . .  107  Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  111   Chapter 1 Introduction  A large fraction of the data we process every day consists of a sequence of symbols over an alphabet, and hence is a text. Unformatted natural language documents, XML and HTML ﬁle collections, program codes, music sequences, DNA and pro- tein sequences, are the typical examples that come to our mind when thinking of text incarnations. Although the scientiﬁc literature offers many solutions to the stor- age, access and search of textual data, the current growth and availability of mas- sive amounts of texts gathered and processed by applications—Web search engines, textual and biological databases, just to cite a few—has changed the algorithmic requirements of these basic processing and mining tools and provides ample moti- vation for a great deal of new theoretical research on algorithms and data structures. Indeed, the memory hierarchies on current PCs and workstations are very complex because they consist of multiple levels: L1 and L2 caches, internal memory, one or more disks, other external storage devices  like CD-ROMs and DVDs , and mem- ories of multiple hosts over a network. Although the virtualization of the memory permits the address space to be larger than the internal memory, it is well-known that not all memory references are equal: each of these memory levels has its own cost, capacity, latency and bandwidth, and thus the memory accesses at the highest levels of the hierarchy are orders of magnitude faster than the accesses at the lowest levels. Therefore, applications working on large data sets should carefully organize their data in order to help the underlying Virtual Memory to guarantee efﬁcient per- formance to the underlying applications. Knuth, in his famous The Art of Computer Programming, observed indeed that the “space optimization is closely related to time optimization in a disk memory”. Obviously this consideration is valid for all levels of the memory hierarchy.  In this scenario data compression seems mandatory because it may induce a twofold advantage: ﬁtting more data into high and faster memory levels reduces the transfer time from the slow levels, and may speed up the execution of algorithms. It goes without saying that storing data in compressed format is beneﬁcial when- ever the cost of accessing them out-weights their decompression time. Until recently data compression had its most important applications in areas of data transmission  R. Venturini, Compressed Data Structures for Strings, Atlantis Studies in Computing 4, DOI: 10.2991 978-94-6239-033-1_1,   Atlantis Press and the authors 2014  1   2  1 Introduction  and storage: texts were compressed only for reducing their size and, except for some heuristic approaches, any access or search to their contents required their whole decompression [Witten et al.  1999 ]. Thus, the focus was primarily posed on designing compression schemes that achieve the maximum level of compression and that are usually very slow both in compression and decompression. Recently, a bunch of algorithmic tools and theoretical machineries have been made available to access compressed information without incurring in their whole decompression  e.g. see [Navarro and Mäkinen  2007 ] and references therein . The exact meaning of access to compressed information deeply varies by application to application. Dif- ferent applications usually ask to provide different operations on compressed texts that range from allowing fast decompression of the whole text, to permitting random accesses to its substrings, up to answering sophisticated pattern matching queries on it. In literature is known a plethora of solutions: some of them support most of these operations while other are more specialized and provide just few of them. At a ﬁrst glance, tools with fewer supported operations seems to be unnecessary since, for example, by providing efﬁcient random access to substrings we also derive fast decompression of whole text. However, by designing specialized schemes we can obtain faster and or more compressed solutions. It is the case of compressed full-text indexes which are compressed tools that answer efﬁciently pattern matching queries on the indexed text Navarro and Mäkinen  2007 . Most of them also allow random accesses to substrings of the compressed text. However, these tools are slower than ad hoc solutions  e.g. [Sadakane and Grossi  2006 ; González and Navarro  2006 ; Ferragina and Venturini  2007b ]  in solving the latter operation primarily due to inefﬁciencies induced by their compression strategies. Observation above forced the research community to design different and specialized solutions that better ﬁt in different applicative scenarios. These scenarios can be identiﬁed by observing the behavior of applications on the compressed data they manage. Sometimes data are accessed by applications only few times in their lifetime. We can, thus, admit some time inefﬁciencies on compression decompression speed and focus our efforts on achieving best possible compression. Consider, as an example, applications that manage huge ﬁles that have to be stored on a disk for backup purposes or transmitted from an host to an other through a network with small bandwidth. Other applications, instead, access data very often once it has been compressed. There are two possible pattern of accesses: the application performs its computation by scanning the whole text, even more times, from left to right, or by random accessing its substrings. For example, the ﬁrst type of accesses may sufﬁce in applications that compute some statistics on the text for mining purposes while the task of generating snippets in a search engine requires random accesses on compressed text. Finally, applications that manage texts require to perform some relative simple pattern matching queries on them. Such queries require to efﬁciently report or count the number of the occur- rences of a given pattern on the underlying text. These queries usually sufﬁce in many contexts but may be also used as building blocks whenever we require more sophisticated queries such as approximate pattern matching, searching with regular expressions, and so on.   1 Introduction  3  We can summarize the contributions presented in this book with the following points that introduce solutions for the different applicative scenarios described above.  Maximizing achieved compression. As we said before, in some scenarios compres- sion decompression speed is not the main concern since we are more interested in maximizing the achieved compression. In Chap. 3 we describe a way to optimize the performance of a given compressor over an input text. More precisely, we investigate the problem of partitioning an input string T in such a way that compressing individ- ually its parts via the given compressor gets a compressed output that is shorter than applying the compressor over the entire T at once. Our solution is the ﬁrst known algorithm which is guaranteed to compute in O n log1+ε n  time a partition of T whose compressed output is guaranteed to be no more than  1 + ε  times worse in size than the optimal one, where ε is an arbitrary positive value. The content of this chapter is based on results in [Ferragina et al.  2009c, 2011a ].  Fast whole decompression. LZ77 is a compression scheme that has been introduced by Lempel and Ziv about 30 years ago [Ziv and Lempel  1977 ]. It has gained a lot of popularity due to its good compression performance and its very fast decompression algorithm which makes it the default choice in any scenario in which we often decompress the original text. In Chap. 4 we provide algorithms that permit to optimize the compression performance of LZ-based compressors. These theoretical results are sustained by some experiments that compare our novel LZ-based compressors against the most popular compression tools  like gzip, bzip2  and state-of-the-art compressors  like the booster of [Ferragina et al.  2005a, 2006a ] . These results show that our solutions signiﬁcantly improve compression performance of other LZ-based compressors still retaining very fast decompression speed. The content of this chapter is based on papers [Ferragina et al.  2009d, 2013a ].  Random access to the compressed data. The task of providing efﬁcient random accesses to the compressed data is a key problem in many applications. For example, search engines and textual databases often need to extract pieces of data  e.g. web pages or records  from a compressed collection of ﬁles, without incurring in their whole decompression. Many solutions for lossless data compression  e.g. the ones discussed in the previous two points  fail in providing efﬁcient random access to the compressed data and in achieving provable good compression ratio. In Chap. 5 we describe a compressed storage scheme for strings which improves known solutions in compression ratio and supports the retrieval of any of its substrings in optimal time. The importance of the problem is conﬁrmed by the fact that our scheme has been used as building block in many subsequent papers to reduce space requirements of data structures and algorithms  e.g. see [Barbay and Munro  2008 ; Belazzougui et al.  2009 ; Fischer  2009 ; Fischer et al.  2008 ; Ferragina and Fischer  2007 ; Chien et al.  2008 ; Hon et al.  2008 ; Nitto and Venturini  2008 ] . The content of this chapter is based on papers Ferragina and Venturini  2007b, 2007c . The content of this chapter is based on papers Ferragina and Venturini  2007b, 2007c .  Compressed full-text indexes. Sufﬁx trees and sufﬁx arrays [Gusﬁeld  1997 ] are well-known classical full-text indexes that solve the so-called text searching prob-   4  1 Introduction  lem efﬁciently in time, but they are greedy of space requiring  cid:3  n log n  bits to index a text of n symbols. In practice this means that such indexes are from 4 to 20 times larger than the input text, and thus their use is unfeasible for large data sets. Recent results [Navarro and Mäkinen  2007 ] have shown astonishing and strong connections between indexing data structures and compressor design. This connec- tion, at a ﬁrst glance, might appear paradoxical because these tools have antithetical goals. On one hand, index design aims at augmenting data with routing information  i.e. data structures  that allow the efﬁcient retrieval of patterns or the extraction of some information. On the other hand, compressors aim at removing the redundancy present in the data to squeeze them in a reduced space occupancy. The resulting compressed full-text indexes carefully combine ideas born in both ﬁelds to obtain the search power of sufﬁx arrays trees and a space occupancy close to the one achievable by the best known compressors, like gzip and bzip2. These studies were mainly at a theoretical stage. The content of Chap. 6 complemented those theoretical achieve- ments with a signiﬁcant experimental and algorithmic engineering effort. This is the ﬁrst extensive experimental comparison among the most important compressed indexes known in the literature. This result has led to the design and develop of the Pizza & Chili site  http:  pizzachili.di.unipi.it , that offers publicly available implementations of the best known compressed indexes, and a collection of texts and tools for experimenting and validating these indexes. The content of this chapter is based on paper Ferragina et al.  2008a .  Compressed indexes for dictionaries of strings. Many applications require to index dictionary of strings instead of texts, like terms and URLs in web search engines. The main difference between these indexes and full-text ones relies in the basic operations to be supported: membership of a string, ranking of a string in the sorted dictionary, or selection of the ith string from it. Tries are classical data structures providing those primitives but they require a large amount of extra space Knuth  1998 . Since dictionaries of strings are getting larger and larger, it becomes crucial to devise implementations for the above primitives which are fast and work in compressed space. In Chap. 7 we describe a compressed index having these characteristics. In addition, we also devise a dynamic compressed index that is able to maintain the dictionary under insertions and deletions of an individual string. This theoretical study has been complemented with a rich set of experiments which have shown that our compressed index is also of practical relevance. In fact, it improves known approaches based on front-coding [Manning et al.  2008 ; Witten et al.  1999 ] by more than 50 % in absolute space occupancy, still guaranteeing comparable query time. The content of this chapter is based on papers Ferragina and Venturini  2007a, 2010 . We conclude in Chap. 8 by presenting some of the most important and challenging  problems of this fascinating area of research.   Chapter 2 Basic Concepts  In the last decade more than before researchers have shown that combinatorial pattern matching and data compression are strongly related. On one hand, pattern matching data structures and techniques are used to develop time efﬁcient implementations of almost any data compression algorithm. On the other hand, many classical pattern matching data structures suffer of space inefﬁciencies which could be mitigated by reducing the redundancy present in the indexed text via data compression.  The aim of the present chapter is that of introducing the most important models of computation and useful notation as well as some well-known results on combinatorial pattern matching and data compression that will be often referred in subsequent chapters.  The content of this chapter can be safely skipped by readers who are familiar with  the ﬁelds of textual data compression and combinatorial pattern matching.  2.1 Models of Computation  In order to reason about algorithms and data structures, we need models of compu- tation that grasp the essence of technological aspects of real computers so that algo- rithms that are proved efﬁcient in the model are also efﬁcient in practice. The next subsections present two of the most important models: RAM Model and External- Memory Model.  2.1.1 RAM Model  The RAM model tries to model a realistic computer. RAM stands for Random Access Machine, which differentiates the model from classic but unrealistic computation models such as a tape-based Turing Machine. The machine is formed of a CPU,  R. Venturini, Compressed Data Structures for Strings, Atlantis Studies in Computing 4, DOI: 10.2991 978-94-6239-033-1_2,   Atlantis Press and the authors 2014  5   6  2 Basic Concepts  which executes primitive operations, and a memory, which stores the program and the data. The memory is divided in cells, called words, each having size w bits. It is usually assumed that log n ≤ w = ε log n , where n is the size of the problem.1 This assumption  usually referred as trans-dichotomous assumption  Fredman and Willard 1994   is actually very realistic: a word must be large enough to store pointers and indices into the data, since otherwise we cannot even address the input. We can operate on words using at least a basic instruction set consisting of: direct and indirect addressing, and a number of computational instructions, including addition, subtraction, multiplication, division, bitwise boolean operations and left and right shifts. Each of these operations requires a constant amount of time and can only manipulate O 1  words at a time.  2.1.2 External-Memory Model  The RAM model does not reﬂect the memory hierarchy of real computers that have more than one layer of memory, with different access characteristics. The external- memory model  or I O model  was introduced by Aggarwal and Vitter in  1988  to capture memory hierarchy with two layers. The model abstracts a computer which consists of two memory levels: a fast and small  internal  memory of size M, and a slow but potentially unbounded disk. Actually, this model can be used to abstract any two different layers in the memory hierarchy  e.g., disk and network . Both the memory and disk are divided into blocks of size B. The CPU can only operate directly on the data stored in memory, which consists of M B blocks. Algorithms can make memory transfer operations, that is they can either read one block from disk to memory, or write one block from memory to disk. The cost of an algorithm is the number of memory transfers required to complete the task. In this model, thus, the cost of performing any other operation on data in memory is considered of secondary importance. Clearly any algorithm that has running time T  N   in the RAM model can be trivially converted into an external-memory algorithm that requires no more than T  N   memory transfers. However, faster algorithms can be often designed by carefully organizing and orchestrating accesses to data on the disk.  2.2 Notation It is convenient to ﬁx a common notation regarding strings. Let T[1, n] be a string drawn from an totally ordered alphabet  cid:3  = [σ].2 We will refer to the ith symbol of T as T[i]. Given any two positions i, j ∈ [n] such that i ≤ j, we will use T[i : j]  1 In the following we will assume that all logarithms are taken to the base 2, whenever not explicitly indicated, and we assume 0 log 0 = 0. 2 The notation [σ] denotes the set {1, 2, . . . , σ}. Unless otherwise stated, we will assume that σ = O poly n  .   2.2 Notation  7  to denote the substring T[i]T[i + 1] . . . T[ j]. We will use Ti = T[i : n] to denote the i-th sufﬁx of T . We will often compare strings resorting to their lexicographic order. The lexico- graphic ordering, denoted by≤, is a total ordering induced by an ordering of symbols √ drawn from in the alphabet  cid:3  and it is deﬁned as follows. Given two strings S and S √  S < S   if and only alphabet  cid:3 , we say that S is lexicographically smaller than S if S[1] < S √ 1. The empty string is considered lexicographically smaller than any non-empty string.  √[1] or both S[1] = S  √[1] and S1 < S  √  2.3 Classical Full-Text Indexes  The aim of this section is that of introducing the Text Searching Problem and clas- sical full-text indexes that will be intensively referred in the next chapters. The Text Searching Problem is stated as follows. Problem 2.1 Given a text T[1, n] and a pattern P[1, p], we wish to answer the following queries:  1  Count P  that returns the number of occurrences  occ  of P in T ;  2  Locate P  that reports the occ positions in T where P occurs.  In literature are known several algorithms to solve this problem in linear time via a sequential scan of T  Gusﬁeld 1997 . Despite the increase in processing speeds, sequential text searching long ago ceased to be a viable alternative for many applica- tions, and indexed text searching has became mandatory. A  full- text index is a data structure built over a text T which signiﬁcantly speeds up subsequential searches for arbitrary pattern strings. This speed up came at the cost of additional space con- sumption  namely, the space required to store the index . Many different indexing data structures have been proposed in the literature, most notably sufﬁx trees and sufﬁx arrays  e.g., see  Aluru and Ko 2008; Gusﬁeld 1997  and references therein .  2.3.1 Sufﬁx Tree Let S be a sorted set of strings drawn from an alphabet  cid:3  such that no string is a proper preﬁx of any other string. The  compact  trie of S is the rooted ordered tree deﬁned as follows:   each edge is labeled with a non-empty string;   the labels of any two edges leaving the same node begin with different symbols;   all internal nodes, except possibly the root, are branching;   the tree has S leaves;   8  2 Basic Concepts    the i-th leaf is associated with the i-th lexicographic smaller string S of S and the concatenation of the labels on the path from the root to this leaf is exactly equal to S. The sufﬁx tree of a text T[1, n] is the compacted trie, denoted as ST T   or simply ST, built on all the n sufﬁxes of T . We can ensure that no sufﬁx is a proper preﬁx of another sufﬁx by simply assuming that a special symbol, say $, terminates the text T . The symbol $ does not appear anywhere else in T and is lexicographically smaller than any other symbol in  cid:3 . This constraint immediately implies that each sufﬁx of T has its own unique leaf in the sufﬁx tree, since any two sufﬁxes of T will eventually follow separate branches in the tree.  For a given edge, the edge label is simply the substring in T corresponding to the edge. For edge between nodes u and v in ST, the edge label  denoted label u, v   is always a non-empty substring of T . For a given node u in the sufﬁx tree, its path label  denoted pathlabel u   is deﬁned as the concatenation of edge labels on the path from the root to u. The string depth of node u is simply  pathlabel u . For any two sufﬁxes Ti and T j , if w is their longest common preﬁx, then there exists a node u in ST whose path label is equal to w. This node u is the lowest common ancestor of the two leaves labeled i and j. In the following, we will use Lcp Ti , T j   to denote the length of w. Figure 2.1 shows the sufﬁx tree of the example text T = abracadabra$. In order to allow a linear space representation of the tree, each edge label is repre- sented by a pair of integers denoting, respectively, the starting and ending positions in the T of the substring describing the edge label. If the edge label corresponds to a repeat substring, the indices corresponding to any these occurrences could be used. In this way, the sufﬁx tree can be stored in ε n log n  bits of space which may be, however, much more than the n cid:5 log σ∼ bits needed to represent text itself. We notice that this space bound is not optimal for any σ , since there are just σ n different possible strings while n log n bits are sufﬁce to represent nn different elements.  An optimal algorithm for building the sufﬁx tree of a text T is the elegant solution  by Farach-Colton  1997  which requires O n  time.3  In order to solve the Text Searching Problem with the sufﬁx tree we observe that if a pattern P occurs in T starting from position i, then P is a preﬁx of Ti . This implies that the searching algorithm should identify the highest node u in ST such that P is preﬁx of pathlabel u . From this observation we can derive the following algorithm for Count P : Start from the root of ST and follow the path matching symbols of P, until a mismatch occurs or P is completely matched. In the former case P does not occur in T . In the latter case, each leaf in the subtree below the matching position gives an occurrence of P. This algorithm counts the occ occurrences of any pattern P[1, p] in time O  p log σ  . These positions can be located by traversing the subtree in time proportional to its size. It is easy to see that this size is O occ . The complexity of counting can be reduced to O  p  by placing a  minimal  perfect hashing function  Hagerup and Tholey 2001  in each node to speed up percolation. This will increase the space just by a constant factor.  3 Recall the assumption σ = O poly n  .   2.3 Classical Full-Text Indexes  Fig. 2.1 The sufﬁx tree of the example text T = abracadabra$  9  For further details, the reader can consult the books by Gusﬁeld  1997  and, Crochemore and Rytter  2003  that provide a comprehensive treatment of sufﬁx trees, their construction, and their applications.  2.3.2 Sufﬁx Array  The sufﬁx array  Manber and Myers 1993  is a compact version of the sufﬁx tree, obtained by storing in a array S A[1, n] the starting positions of the sufﬁxes of T listed in lexicographic order. As the sufﬁx tree, this array requires ε n log n  bits in the worst case. The main practical advantage is given by the fact that the constant hidden in the big-Oh notation is smaller  namely, it is less than 4 . S A can be obtained by traversing the leaves of the sufﬁx tree, or it can be built directly in optimal linear time via ad-hoc sorting methods  Gusﬁeld 1997; Puglisi et al. 2007 . Since any substring   10  2 Basic Concepts  of T is the preﬁx of a text sufﬁx, the solution to the Text Searching Problem consists in ﬁnding the interval of positions in S A corresponding all text sufﬁxes that start with P. Once this interval S A[sp, ep] has been identiﬁed, Count P  is solved by returning the value occ = ep − sp + 1, and Locate P  is solved by retrieving the entries S A[sp], S A[sp + 1], . . . S A[ep]. The interval S A[sp, ep] can be binary searched in O  p log n  time, since each binary search step requires to compare up to p symbols of a text sufﬁx and the pattern. This time can be reduced to O  p+log n  by using an auxiliary array called LC P that doubles the space requirement of the sufﬁx array. The array LC P[1, n] essentially captures information on the heights of the internal nodes in the sufﬁx tree of T . It is deﬁned such that its entry LC P[i] is equal to the length of the longest common preﬁx of the  i −1 -st and i-th lexicographically smallest sufﬁxes in T  namely, LC P[i] = Lcp TS A[i−1], TS A[i]  . The LC P array can be computed in linear time starting from the sufﬁx array and, in conjunction with an auxiliary data structure to solve Range Minimum Queries  RMQ , it sufﬁces to compute the length of longest common preﬁx between any pair of sufﬁxes of T .  We conclude this section by introducing the inverse sufﬁx array. Even if it is not directly related to the Text Searching Problem, it will be often referred in the next −1, is an array of n elements deﬁned chapters. The inverse sufﬁx array, denoted as S A as  −1[i] = j ⊆⇒ S A[ j] = i. In other words, since S A is just a permutation of[n], S A Therefore, S A order. We ﬁnally notice that S A and S A other in linear time. Figure 2.2 shows S A, S A text T = abracadabra$.  −1 is deﬁned to be its inverse. −1[i] = j tells us that sufﬁx Ti is the j-th sufﬁx in lexicographic −1 can be easily computed one from the −1 and LC P arrays for the example  S A  2.4 Compression  In this section we present some concepts related to compression that will be very useful in the next pages. In particular, in Sect. 2.4.1 we introduce the notion of empir- ical entropy which is a well-known measure of the compressibility of a text. This measure will be used in almost all the chapters either to optimize the performance of different compressors  Chap. 3  or to establish upper bounds to performance of com- pression schemes or compressed indexes  Chaps. 5– 7 . In Sect. 2.4.2 we introduce the Burrows-Wheeler Transform which is an amazing algorithm for data compression. Even if we will introduce some other compression algorithms in the next chapters, we decided to introduce this tool earlier since it plays a central rule in all the thesis.  Fig. 2.2 The table shows −1 and LC P arrays S A, S A for the example text T = abracadabra$  SA SA− 1  LCP  12  11  4  −  8  0  8  12  1  1  5  4  4  9  1  6  6  1  9  10  0  2  3  3  5  7  0  7  11  0  10  2  0  3  1  2   11   2.1   2.4 Compression  2.4.1 Empirical Entropy  The empirical entropy resembles the entropy deﬁned in the probabilistic setting  for example, when the input comes from a Markov source , but now it is deﬁned for any ﬁnite individual string and can be used to measure the performance of compression algorithms without any assumption on the input distribution [Manzini  2001 ].  The 0-th order empirical entropy is currently a well-established measure of com- pressibility for a single string  Manzini 2001 , and it is deﬁned as follows. For each c ∈  cid:3 , we let nc be the number of occurrences of c in T . The zero-th order empirical entropy of T is deﬁned as  H0 T   = 1T   cid:2  c∈ cid:3   nc log  n nc  .  Note that TH0 T   provides an information-theoretic lower bound to the output size of any compressor that encodes each symbol of T with a ﬁxed code  Witten et al. 1999 . The so-called zero-th order statistical compressors  such as Huffman or Arithmetic  Witten et al. 1999   achieve an output size which is very close to this bound. However, they require to know information about frequencies of input symbols  called the model of the source . Those frequencies can be either known in advance  static model  or computed by scanning the input text  semistatic model . In both cases the model must be stored in the compressed ﬁle to be used by the decompressor. The compressed size achieved by zero-th order compressors over T is bounded byC0 T   ≤ λn H0 T  + f0 n, σ   bits, where λ is a positive constant and f0 n, σ   is a function including the extra costs of encoding the source model and or other inefﬁciencies ofC. As an example, for Huffman f0 n, σ   = σ log σ + O σ  +n bits and λ = 1, and for Arithmetic f0 n, σ   = O σ log n  bits and λ = 1.  In order to evict the cost of the model, we can resort to zero-th order adaptive compressors that do not require to know the symbols’ frequencies in advance, since they are computed incrementally during the compression. The zero-th order adaptive empirical entropy of T  Howard and Vitter 1992   is then deﬁned as  H a 0   T   = 1T   cid:2  c∈ cid:3   log  n! nc! .   2.2   The compress size achieved by zero-th order adaptive compressors over T is  T  + f0 n, σ   bits where f0 n, σ   is a function including   T   ≤ n H a  0  bounded byCa inefﬁciencies of C.  0  Let us now come to more powerful compressors. For any string u of length k, we denote by uT the string of single symbols following the occurrences of u in T , taken from left to right. For example, if T = abracadabra$ and u = ab, we have uT = rr since the two occurrences of ab in T are both followed by symbol r. The k-th order empirical entropy of T is deﬁned as   12  2 Basic Concepts  Analogously, the k-th order adaptive empirical entropy of T is deﬁned as  Hk  T   = 1T  uT H0 uT  .  H a k   T   = 1T  uT H a  0   uT  .   cid:2   u∈ cid:3 k   cid:2   u∈ cid:3 k   2.3    2.4   We have Hk  T   ≥ Hk+1 T   for any k ≥ 0. As usual in data compression  Manzini 2001 , the value n Hk  T   is an information-theoretic lower bound to the output size of any compressor that encodes each symbol of T with a ﬁxed code that depends on the symbol itself and on the k immediately preceding symbols. Recently  see e.g.  Kosaraju and Manzini 1999; Manzini 2001; Ferragina et al. 2005a, 2009a; Mäkinen and Navarro 2007; Ferragina et al 2005b  and references therein  authors have provided upper bounds in terms of Hk  T  for sophisticated data compres- sion algorithms, such as dictionary based  Kosaraju and Manzini 1999 , Bwt-based  Manzini 2001; Ferragina et al 2005a; Kaplan et al. 2007 , and PPM-like. These bounds have the form C T   ≤ λT Hk  T   + fk  T, σ  , where λ is a positive constant and fk  T, σ   is a function including the extra-cost of encoding the source model and or other inefﬁciencies of C. The smaller are λ and fk   , the better is the compressor C. As an example, the bound of the compressor in  Mäkinen and Navarro 2007  has λ = 1 and f  T, σ   = O σ k+1 logT + T log σ log logT  logT . Similar bounds that involve the adaptive k-th order entropy are known  Manzini 2001; Ferragina et al. 2005a, 2009a  for many compressors. In these cases the bound takes  T   + fk  T, σ   bits where fk  T, σ   is a function the form Ca including the inefﬁciencies of C.   T   ≤ λTH  ∗ k  k  2.4.2 Burrows-Wheeler Transform  In Burrows and Wheeler  1994  introduced a new compression algorithm based on a reversible transformation, now called the Burrows-Wheeler Transform  BWT from now on . The BWT transforms the input string T into a new string that is easier to compress. The BWT of T , hereafter denoted by Bwt T   or simply Bwt, is built with three basic steps  see Fig. 2.3 :   1  append at the end of T a special symbol $ smaller than any other symbol of  cid:3 ;  2  form a conceptual matrix MT whose rows are the cyclic rotations of string T $ in lexicographic order;  3  construct string L by taking the last column of the sorted matrix MT . We set Bwt T   = L. Every column of MT , hence also the transformed string L, is a permutation of T $. In particular the ﬁrst column of MT , call it F, is obtained by lexicographically   2.4 Compression  Fig. 2.3 Example of Burrows-Wheeler trans- form for the string T = abracadabra$. The matrix on the right has the rows sorted in lexico- graphic order. The output of the BWT is the column L = ard$rcaaaabb  F  13  L  sorting the symbols of T $  or, equally, the symbols of L . Note that the sorting of the rows of MT is essentially equal to the sorting of the sufﬁxes of T , because of the presence of the special symbol $. This shows that:  1  symbols following the same substring  context  in T are grouped together in L, and thus give raise to clusters of nearly identical symbols;  2  there is an obvious relation between MT and S A. Property 1 is the key for devising modern data compressors  see e.g.  Manzini 2001  , Property 2 is crucial for designing compressed indexes  see e.g.  Navarro and Mäkinen 2007   and, additionally, suggests a way to compute the Bwt through the construction of the sufﬁx array of T : L[0] = T[n] and, for any 1 ≤ i ≤ n, set L[i] = T[S A[i] − 1].  Burrows and Wheeler  1994  devised two properties for the invertibility of the Bwt:  a  Since the rows in MT are cyclically rotated, L[i] precedes F[i] in the original  b  For any c ∈  cid:3 , the  cid:6 -th occurrence of c in F and the  cid:6 -th occurrence of c in L  string T .  correspond to the same character of the string T .  As a result, the original text T can be obtained backwards from L by resorting to function L F  also called Last-to-First column mapping or LF-mapping  that maps row indexes to row indexes, and is deﬁned as:  L F  i   = C[L[i]] + RankL[i] L , i  ,  where C[L[i]] counts the number of occurrences in T of symbols smaller than L[i] and RankL[i] L , i   is a function that returns the number of times symbol L[i] occurs   14  2 Basic Concepts  in the preﬁx L[1 : i]. We talk about LF-mapping because the symbol c = L[i] is located in the ﬁrst column of MT at position L F  i  . The LF-mapping allows one to navigate T backwards: if T[k] = L[i], then T[k − 1] = L[L F  i  ] because row L F  i   of MT starts with T[k] and thus ends with T[k − 1]. In this way, we can reconstruct T backwards by starting at the ﬁrst row, equal to $T , and repeatedly applying L F for n steps. As an example, see Fig. 2.3 in which the 3rd a in L lies onto the row which starts with bracadabra$ and, correctly, the 3rd a in F lies onto the row which starts with abracadabra$. That symbol a is T[1].   Chapter 3 Optimally Partitioning a Text to Improve Its Compression  ≤  which is then partitioned into substrings T  Reorganizing data in order to improve the performance of a given compressor C is a recent and important paradigm in data compression  see e.g. Buchsbaum et al. 2003; Ferragina et al. 2005a . The basic idea consists of permuting the input data T to form ≤ k that are a new string T ﬁnally compressed individually by the base compressor C. The goal is to ﬁnd the best instantiation of the two steps Permuting + Partitioning so that the compression ≤ i minimizes the total length of the compressed output. of the individual substrings T This approach  hereafter abbreviated as PPC  is clearly at least as powerful as the classic data compression approach that applies C to the entire T : just take the identity permutation and set k = 1. The question is whether it can be more powerful than that!  ≤ = T ≤ ≤ 1T 2  ··· T  Intuition leads to think favorably about it: by grouping together objects that are “related”, one can hope to obtain better compression even using a very weak compressor C. Surprisingly enough, this intuition has been sustained by convincing theoretical and experimental results only recently. These results have investigated the PPC-paradigm under various angles by considering: different data formats  strings Ferragina et al. 2005a, trees Ferragina et al. 2005b, tables Buchsbaum et al. 2003, etc. , different granularities for the items of T to be permuted  chars, node labels, columns, blocks Bentley and McIlroy 2001; Kulkarni et al. 2004, ﬁles Chang et al. 2008; Suel and Memon 2002; Trendaﬁlov et al. 2004 etc. , different permutations  see e.g. Giancarlo et al. 2007; Vo and Vo 2007; Trendaﬁlov et al. 2004; Chang et al. 2008 , different base compressors to be boosted  0-th order compressors, gzip, bzip2, etc. . Among these plethora of proposals, we survey below the most notable examples which are useful to introduce the problem we attack in this chapter, and refer the reader to the cited bibliography for other interesting results.  R. Venturini, Compressed Data Structures for Strings, Atlantis Studies in Computing 4, DOI: 10.2991 978-94-6239-033-1_3,   Atlantis Press and the authors 2014  15   16  3 Design and Safety Analysis of a Drive-by-Wire Vehicle  3.1 The PPC-Paradigm and Motivations for Optimal  Partitioning  The PPC-paradigm was introduced in Buchsbaum et al.  2000 , and further elabo- rated upon in Buchsbaum et al.  2003 . In these papers T is a table formed by ﬁxed size columns, and the goal is to permute the columns in such a way that individually compressing contiguous groups of them gives the shortest compressed output. The authors of Buchsbaum et al.  2003  showed that the PPC-problem in its full general- ity is MAX-SNP hard, devised a link between PPC and the classical asymmetric TSP problem, and then resorted known heuristics to ﬁnd approximate solutions based on several measures of correlations between the table’s columns. For the grouping they proposed either an optimal but very slow approach, based on Dynamic Programming  see below , or some very simple and fast algorithms which however did not have any guaranteed bounds in terms of efﬁcacy of their grouping process. Experiments showed that these heuristics achieve signiﬁcant improvements over the classic gzip, when it is applied on the serialized original T  row- or column-wise . Furthermore, they showed that the combination of the TSP-heuristic with the DP-optimal parti- tioning is even better, but it is too slow to be used in practice even on small ﬁle sizes because of the DP-cubic time complexity.1  When T is a text string, the most famous instantiation of the PPC-paradigm has been obtained by combining the Burrows and Wheeler Transform  Burrows and Wheeler 1994  with a context-based grouping of the input symbols, which are ﬁnally compressed via proper 0-th order-entropy compressors  like Mtf, Rle, Huffman, Arithmetic, or their combinations, see e.g. Witten et al. 1999 . Here the PPC-paradigm takes the name of compression booster  Ferragina et al. 2005a  because the net result it produces is to boost the performance of the base compres- sor C from 0-th order-entropy bounds to k-th order entropy bounds, simultaneously over all k ∈ 0. In this scenario the permutation acts on single symbols, and the partitioning permuting steps deploy the context  substring  following each symbol in the original string in order to identify “related” symbols which must be therefore compressed together. Recently Giancarlo et al.  2007  investigated whether there exist other permutations of the symbols of T which admit effective compression and can be computed inverted fast. Unfortunately they found a connection between table compression and the Bwt, so that many natural similarity-functions between con- texts turned out to induce MAX-SNP hard permuting problems! Interesting enough, the Bwt seems to be the unique highly compressible permutation which is fast to be computed and achieves effective compression bounds. Several other papers have given an analytic account of this phenomenon  Manzini 2001; Ferragina et al. 2009a; Kaplan et al. 2007; Mäkinen and Navarro 2007  and have shown, also experimentally  Ferragina et al. 2006a , that the partitioning of the BW-transformed data is a key step for achieving effective compression ratios. Optimal partitioning is actually even  1 Page 836 of Buchsbaum et al.  2003  says: “computing a good approximation to the TSP reordering before partitioning contributes signiﬁcant compression improvement at minimal time cost. [...] This time is negligible compared to the time to compute the optimal, contiguous partition via DP”.   3.1 The PPC-Paradigm and Motivations for Optimal Partitioning  17  more mandatory in the context of labeled-tree compression where a BWT-inspired transform, called XBW-transform in Ferragina et al.  2005b, 2006b , allows to pro- duce permuted strings with a strong clustering effect. Starting from these premises  Giancarlo and Sciortino 2003  attacked the computation of the optimal partitioning of T via a DP-approach, which turned to be very costly; then  Ferragina et al. 2005a   and subsequently many other authors, see e.g. Ferragina et al. 2009a; MäNakinen and Navarro 2007; Ferragina et al. 2005b  proposed solutions which are not opti- mal but, nonetheless, achieve interesting k-th order-entropy bounds. This is indeed a subtle point which is frequently neglected when dealing with compression boosters, especially in practice, and for this reason we detail it more clearly in Sect. 3.3.1 in √ which we show an inﬁnite class of strings for which the compression achieved by the booster is far from the optimal-partitioning by a multiplicative factor ε   log n .  Finally, there is another scenario in which the computation of the optimal partition of an input string for compression boosting can be successful and occurs when T is a single  possibly long  ﬁle on which we wish to apply classic data compressors, such as gzip, bzip2, PPM, etc.  Witten et al. 1999 . Note that how much redundancy can be detected and exploited by these compressors depends on their ability to “look back” at the previously seen data. However, such ability has a cost in terms of memory usage and running time, and thus most compression systems provide a facility that controls the amount of data that may be processed at once—usually called the block size. For example the classic tools gzip and bzip2 have been designed to have a small memory footprint, up to few hundreds KBs. More recent and sophisticated compressors, like PPM  Witten et al. 1999  and the family of BWT-based compressors  Ferragina et al. 2006a , have been designed to use block sizes of up to a few hundreds MBs. But using larger blocks to be compressed at once does not necessarily induce a better compression ratio! As an example, let us take C as the simple Huffman or Arithmetic coders and use them to compress the text T = 0n 21n 2: There is a clear difference whether we compress individually the two halves of T  achieving an output size of about O log n  bits  or we compress T as a whole  achieving n + O log n  bits . The impact of the block size is even more signiﬁcant as we use more powerful compressors, such as the k-th order entropy encoder PPM which compresses each symbol according to its preceding k-long context. In this case take T =  2k0 n  2 k+1   2k1 n  2 k+1   and observe that if we divide T in two halves and compress them individually, the output size is about O log n  bits, but if we compress the entire T at once then the output size turns + O log n  bits. Therefore the choice of the block size to be much longer, i.e. cannot be underestimated and, additionally, it is made even more problematic by the fact that it is not necessarily the same along the whole ﬁle we are compressing because it depends on the distribution of the repetitions within it. This problem is even more challenging when T is obtained by concatenating a collection of ﬁles via any permutation of them: think to the serialization induced by the Unix tar command, or other more sophisticated heuristics like the ones discussed in Suel and Memon  2002 , Chang et al.  2008 , Ouyang et al.  2002 , Trendaﬁlov et al.  2004 . In these cases, the partitioning step looks for homogeneous groups of contiguous ﬁles which can be effectively compressed together by the base-compressor C. More  n k+1   18  3 Design and Safety Analysis of a Drive-by-Wire Vehicle  than before, taking the largest memory-footprint offered by C to group the ﬁles and compress them at once is not necessarily the best choice because real collections are typically formed by homogeneous groups of dramatically different sizes  e.g. think to a Web collection and its different kinds of pages . Again, in all those cases we could apply the optimal DP-based partitioning approach of Giancarlo and Sciortino  2003 , Buchsbaum et al.  2003 , but this would take more than cubic time  in the overall input size T  thus resulting unusable even on small input data of few MBs. In summary the efﬁcient computation of an optimal partitioning of the input text for compression boosting is an important and still open problem of data compression  see Buchsbaum and Giancarlo 2008 . The goal of our solution is to make a step forward by providing the ﬁrst efﬁcient approximation algorithm for this problem, formally stated as follows. Let C be the base compressor we wish to boost, and let T[1, n] be the input string we wish to partition and then compress by C. So, we are assuming that T has been  possibly  permuted in advance, and we are concentrating on the last two steps of the PPC-paradigm. Now, given a partition P of the input string into contiguous substrings, say T = T1T2 ··· Tk, we denote by Cost P  the cost of this partition C Ti  , where C α  is the length in bit of the string α and measure it as compressed by C. The problem of optimally partitioning T according to the base- compressor C consists then of computing the partition Popt achieving the minimum cost, namely Popt = minP Cost P , and thus the shortest compressed output.2 As we mentioned above Popt might be computed via a Dynamic-Programming approach  Buchsbaum et al. 2003; Giancarlo and Sciortino 2003 . Deﬁne E[i] as the cost of the optimum partitioning of T[1, i], and set E[0] = 0. Then, for each i ∈ 1, we can compute E[i] using the recurrence min0≤ j≤i−1 E[ j] + C T[ j + 1 : i] . At the end E[n] gives the cost of Popt, which can be explicitly determined by standard back-tracking over the DP-array. Unfortunately, this solution requires to run C over  cid:3  n2  substrings of average length  cid:3  n , for an overall  cid:3  n3  time cost in the worst case which is clearly unfeasible even on small input sizes n.   cid:2  l i=1  In order to overcome this computational bottleneck we make two crucial obser- vations:  1  instead of applying C over each substring of T , we use an entropy-based estimation of C’s compressed output that can be computed efﬁciently and incremen- tally by suitable dynamic data structures;  2  we relax the requirement for an exact solution to the optimal partitioning problem, and aim at ﬁnding a partition whose cost is no more than  1+ ε  worse than Popt, where ε may be any positive constant. Item  1  takes inspiration from the heuristics proposed in Buchsbaum et al.  2000, 2003 , but it is executed in a more principled way because our entropy-based cost functions reﬂect the real behavior of modern compressors, and our dynamic data structures al- low the efﬁcient estimation of those costs without their re-computation from scratch at each substring  as instead occurred in Buchs-baum et al. 2000, 2003 . For item  2  it is convenient to resort to a well-known reduction from solutions of dynamic  2 We are assuming that C α  is a preﬁx-free encoding of α, so that we can concatenate the compressed output of many substrings and still be able to recover them via a sequential scan.   3.1 The PPC-Paradigm and Motivations for Optimal Partitioning  19  programming recurrences to Single Source Shortest path  SSSP  computation over weighted DAGs  Dasgupta et al. 2006 . In our case, the solution for the optimal par- titioning problem can be rephrased as a SSSP-computation over a weighted DAG consisting of n nodes and O n2  edges whose costs are derived from item  1 . By exploiting some interesting structural properties of this graph, we are able to restrict the computation of that SSSP to a subgraph consisting of O n log1+ε n  edges only. The technical part of our solution  see Sect. 3.2  will show that we can build this graph on-the-ﬂy as the SSSP-computation proceeds over the DAG via the proper use of time-space efﬁcient dynamic data structures. The ﬁnal result will be to show that we can  1 + ε -approximate Popt in O n log1+ε n  time and O n  space, for both 0-th order compressors  like Huffman and Arithmetic Witten et al. 1999  and k-th order compressors  like PPM Witten et al. 1999 . We will also extend these results to the class of BWT-based compressors, when T is a collection of texts.  We point out that the result on 0-th order compressors is interesting in its own from both the experimental side, since Huffword compressor is the standard choice for the storage of Web pages  Witten et al. 1999 , and from the theoretical side since it can be applied to the compression booster of Ferragina et al.  2005a  to fast obtain an approximation of the optimal partition of Bwt T   in O n log1+ε n  time. This may be better than the algorithm of Ferragina et al.  2005a  both in time complexity, since that takes O nσ  time where σ is the alphabet of T , and in compression ratio  as we have shown above, see Sect. 3.3.1 . The case of a large alphabet  namely, σ = ε polylog n    is particularly interesting whenever we consider either a word-based Bwt  Moffat and Isal 2005  or the Xbw-transform over labeled trees  Ferragina et al. 2005a . Finally, we mention that our results apply also to the practical case in which the base compressor C has a maximum  block  size B of data it can process at once  see above the case of gzip, bzip2, etc. . In this situation the time performance of our solution reduces to O n log1+ε B log σ  .  The map of the chapter is as follows. Section 3.2 describes reduction from the optimal partitioning problem of T to a SSSP problem over a weighted DAG in which edges represent substrings of T and edge costs are entropy-based estimations of the compression of these substrings via C. After that, we show some properties of this DAG that permit our fast solution to the SSSP problem. The subsequent sections will address the problem of incrementally and efﬁciently computing those edge costs as they are needed by the SSSP-computation, distinguishing the two cases of 0-th order estimators  Sect. 3.3  and k-th order estimators  Sect. 3.4 , and the situation in which C is a BWT-based compressor and T is a collection of ﬁles  Sect. 3.5 .  3.2 The Problem and Our Solution In our solution we will use entropy-based upper bounds for the estimation ofC T[i : j]  described in Sect. 2.4.1. In what follows we will assume that the function f0 n, σ  can be computed in constant time given n and σ. Even if we use these entropy-based bounds for the estimation of C T[i : j]  instead of the real compress size, this   20  3 Design and Safety Analysis of a Drive-by-Wire Vehicle  will not be enough to achieve a fast DP-based algorithm for our optimal-partitioning problem. We cannot recompute from scratch those estimates for every substring T[i : j] of T , being them  cid:3  n2  in number. So we will show below some structural properties of our problem and introduce few novel technicalities  Sects. 3.3 and 3.4  that will allow us to compute Hk  T[i : j]  only on a reduced subset of T ’s substrings, having size O n log1+ε n , by taking O polylog n   time per substring and O n  space overall.  The optimal partitioning problem, stated in Sect. 3.1 can be reduced to a single source shortest path computation  SSSP  over a directed acyclic graph G T   deﬁned as follows. The graph G T   has a vertex vi for each text position i of T , plus an additional vertex vn+1 marking the end of the text, and an edge connecting vertex vi to vertex v j for any pair of indices i and j such that i < j. Each edge  vi , v j   has associated the cost c vi , v j   = C T[i : j − 1]  that corresponds to the size in bits of the substring T[i : j − 1] compressed by C. We remark the following crucial, but easy to prove, property of the cost function deﬁned on G T  : Fact 1. For any vertex vi , it is 0 < c vi , vi+1  ≤ c vi , vi+2  ≤ . . . ≤ c vi , vn+1  There is a one-to-one correspondence between paths from v1 to vn+1 in G T   and partitions of T : every edge  vi , v j   in the path identiﬁes a contiguous substring T[i : j − 1] of the corresponding partition. Therefore the cost of a path is equal to the  compression- cost of the corresponding partition. Thus, we can ﬁnd the optimal partition of T by computing the shortest path in G T   from v1 to vn+1. Unfortunately this simple approach has two main drawbacks:  1  the number of edges in G T   is  cid:3  n2 , thus making the SSSP computation  inefﬁcient  i.e. ε n2  time  if executed directly over G T  ;   2  the computation of the each edge cost might take  cid:3  n  time over most T ’s  substrings, if C is run on each of them from scratch.  In the following sections we will successfully address both these two drawbacks. First, we sensibly reduce the number of edges in the graph G T   to be examined during the SSSP computation and show that we can obtain a  1+ ε  approximation using only O n log1+ε n  edges, where ε > 0 is a user-deﬁned parameter  Sect. 3.2.1 . Second, we show some sufﬁcient properties that C needs to satisfy in order to compute efﬁciently every edge’s cost. These properties hold for some well-known compressors— e.g. 0-order compressors, PPM-like and bzip-like compressors— and for them we show how to compute each edge cost in constant or polylogarithmic time  Sects. 3.3–3.5 .  3.2.1 A Pruning Strategy  The aim of this section is to design a pruning strategy that produces a subgraph Gε T   of the original DAG G T   in which the shortest path distance between its leftmost   3.2 The Problem and Our Solution  21  and rightmost nodes, v1 and vn+1, increases by no more than a factor  1 + ε . We deﬁne Gε T   to contain all edges  vi , v j   of G T  , recall i < j, such that at least one of the following two conditions holds:  1  there exists a positive integer k such that c vi , v j   ≤  1 + ε k < c vi , v j+1 ;  2  j = n + 1. In other words, by Fact 1, we are keeping for each integer k the edge of G T   that approximates at the best the value  1 + ε k from below. Given this, we will call ε-maximal the edges of Gε T  . Clearly, each vertex of Gε T   has at most log1+ε n = O  1 ε log n  outgoing edges, which are ε-maximal by deﬁnition. Therefore the total ε log n . Hereafter, we will denote with dG  −,−  the size of Gε T   is at most O  n shortest path distance between any two nodes in a graph G. The following lemma states a basic property of shortest path distances over our special DAG G T  : Lemma 3.1. For any triple of indices 1 ≤ i ≤ j ≤ q ≤ n + 1 we have:  1  dG T   v j , vq   ≤ dG T   vi , vq    2  dG T   vi , v j   ≤ dG T   vi , vq   Proof. We prove just 1, since 2 is symmetric. It sufﬁces by induction to prove the case j = i + 1. Let  vi , w1  w1, w2 ... wh−1, wh  , with wh = vq, be a shortest path in G T   from vi to vq. By fact 1, c v j , w1  ≤ c vi , w1  since i ≤ j. Therefore the cost of the path  v j , w1  w1, w2 ... wh−1, wh   is at most dG T   vi , vq  , which proves  cid:2  the claim.  The correctness of our pruning strategy relies on the following theorem: Theorem 3.1. For any text T , the shortest path in Gε T   from v1 to vn+1 has a total cost of at most  1 + ε  dG T   v1, vn+1 . Proof. We prove a stronger assertion: dGε T   vi , vn+1  ≤  1 + ε  dG T   vi , vn+1  for any index 1 ≤ i ≤ n + 1. This is clearly true for i = n + 1, because in that case the distance is 0. Now let us inductively consider the shortest path π in G T   from vi to vn+1 and let  vk , vt1   . . .  vth vn+1  be its edges. By the deﬁnition of ε-   vt1 maximal edge, it is possible to ﬁnd an ε-maximal edge  vk , vr   with t1 ≤ r, such that c vk , vr   ≤  1 + ε  c vk , vt1 , vn+1  while, by induction, dGε T   vr , vn+1  ≤  1 + ε  dG T   vr , vn+1 . Combining this  cid:2  with the triangle inequality we get the thesis.  , vt2  . By Lemma 3.1, dG T   vr , vn+1  ≤ dG T   vt1  3.2.2 Space and Time Efﬁcient Algorithms for Generating Gε T  Theorem 3.1 ensures that, in order to compute a  1+ ε  approximation of the optimal partition of T , it sufﬁces to compute the SSSP in Gε T   from v1 to vn+1. This can be easily computed in O Gε T    = O n logε n  time since Gε T   is a DAG  Cormen et al. 2001 , by making a single pass over its vertices and relaxing all edges going out from the current one.   22  3 Design and Safety Analysis of a Drive-by-Wire Vehicle  However, generating Gε T   in efﬁcient time is a non-trivial task for three main reasons. First, the original graph G T   contains ε n2  edges, so that we cannot check each of them to determine whether it is ε-maximal or not, because this would take ε n2  time. Second, we cannot compute the cost of an edge  vi , v j   by executing C T[i : j − 1]  from scratch, since this would require time linear in the substring length, and thus ε n3  time over all T ’s substrings. Third, we cannot materialize Gε T    e.g. its adjacency lists  because it consists of  cid:3  n polylog n   edges, and thus its space occupancy would be super-linear in the input size.  The rest of this section is devoted to design an algorithm which overcomes the three limitations above. The specialty of our algorithm consists of materializing Gε T   on- the-ﬂy, as its vertices are examined during the SSSP-computation, by spending only polylogarithmic time per edge. The actual time complexity per edge will depend on the entropy-based cost function we will use to estimate C T[i : j − 1]  and on the dynamic data structure we will deploy to compute that estimation efﬁciently. The key tool we use to make a fast estimation of the edge costs is a dynamic data structure built over the input text T and requiring O T  space. We state the main properties of this data structure in an abstract form, in order to design a gen- eral framework for solving our problem; in the next sections we will then provide implementations of this data structure and thus obtain real time space bounds for our problem. So, let us assume to have a dynamic data structure that maintains a set of sliding windows over T denoted by w1, w2, . . . , wlog1+ε n. The sliding windows are substrings of T which start at the same text position l but have different lengths: namely, wi = T[l : ri] and r1 ≤ r2 ≤ . . . ≤ rlog1+ε n. The data structure must support  the following three operations:  1  Remove   moves the starting position l of all windows one position to the right   i.e. l + 1 ; right  i.e. ri + 1 ;   2  Append wi   moves the ending position of the window wi one position to the  3  Size wi   computes and returns the value C T[l : ri] . This data structure is enough to generate ε-maximal edges via a single pass over T , using O T  space. More precisely, let vl be the vertex of G T   currently examined by our SSSP computation, and thus l is the current position reached by our scan of T . We maintain the following invariant: the sliding windows correspond to all ε-maximal edges going out from vl, that is, the edge  vl , v1+rt   is the ε-maximal   ≤  1 + ε t < c vl , v1+ rt+1  . Initially all indices are edge satisfying c vl , v1+rt set to 0. To maintain the invariant, when the text scan advances to the next position l + 1, we call operation Remove   once to increment index l and, for each t = 1, . . . , log1+ε n , we call operation Append wt   until we ﬁnd the largest rt such that Size wt   = c vl , v1+rt   ≤  1 + ε t . The key issue here is that Append and Size are paired so that our data structure should take advantage of the rightward sliding of rt for computing c vl , v1+rt   efﬁciently. Just one symbol is entering wt to its right, so we need to deploy this fact for making the computation of Size wt   fast  given its previous value . Here comes into play the second contribution of our solution that consists of adopting the entropy-bounded estimates for the compressibility of a string   3.2 The Problem and Our Solution  23  to estimate indeed the edge costs Size wt   = C wt  . This idea is crucial because we will be able to show that these functions do satisfy some structural properties that admit a fast incremental computation, as the one required by Append + Size. These issues will be discussed in the following sections, here we just state that, overall, the SSSP computation over Gε T   takes O n  calls to operation Remove, and O n log1+ε n  calls to operations Append and Size. Theorem 3.2. If we have a dynamic data structure occupying O n  space and supporting operation Remove in time L n , and operations Append and Size in time R n , we can compute the shortest path in Gε T   from v1 to vn+1 taking O n L n  +  n log1+ε n  R n   time and O n  space.  3.3 On Zero-th Order Compressors  In this section we explain how to implement the data structure above whenever C is a 0-th order compressor, and thus H0 is used to provide a bound to the compression Size wi   as the sum of T[l : ri]H0 T[l : ri]  =  cid:2  cost of G T  ’s edges. The key point is actually to show how to efﬁciently compute c∼σ nc log  ri − l + 1  nc   see its deﬁnition in Sect. 2.4.1  plus f0 ri − l + 1,σT[l:ri] , where nc is the number of occurrences of symbol c in T[l : ri] and σT[l:ri] denotes the number of different symbols in T[l : ri]. The ﬁrst solution we are going to present is very simple and uses O σ  space per window. The idea is the following: for each window wi we keep in memory an array of counters Ai[c] indexed by symbol c in σ. At any step of our algorithm, the counter Ai[c] stores the number of occurrences of symbol c in T[l : ri]. For any  cid:2  c∼σ Ai[c] log Ai[c]. window wi , we also use a variable Ei that stores the value of It is easy to notice that:  T[l : ri] H0 T[l : ri]  =  ri − l + 1  log ri − l + 1  − Ei .   3.1   Therefore, if we know the value of Ei , we can answer to a query Size wi   in constant time. So, we are left with showing how to implement efﬁciently the two operations that modify l or any rs value and, thus, modify appropriately the E’s value. This can be done as follows:  1  Remove  : For each window wi , we subtract from the appropriate counter and from variable Ei the contribution of the symbol T[l] which has been evicted from the window. That is, we decrease Ai[T[l]] by one, and update Ei by subtracting  Ai[T[l]]+1  log Ai[T[l]]+1  and then summing Ai[T[l]] log Ai[T[l]]. Finally we set l = l + 1.  2  Append wi  : We add to the appropriate counter and variable Ei the contri- bution of the symbol T[ri + 1] which has been appended to window wi . That is, we increase Ai[T[r + 1]] by one, then we update Ei by subtracting   24  3 Design and Safety Analysis of a Drive-by-Wire Vehicle   A[T[ri+1]]−1  log A[T[ri +1]]−1  and summing A[T[ri +1]] log A[T[ri + 1]]. Finally we set ri = ri + 1. In this way, operation Remove requires constant time per window, hence O log1+ε n  time overall. Append wi   takes constant time. The space required by the counters Ai is O σ log1+ε n  words. Unfortunately, the space complexity of this solution can be too much when it is used as the basic-block for computing the k-th order entropy of T  see Sect. 2.4.1  as we will do in Sect. 3.4. In fact, we would achieve min σk+1 log1+ε n, n log1+ε n  space, which may be superlinear in n depending on σ and k. The rest of this section is therefore devoted to provide an implementation of our dynamic data structure that takes the same query time above for these three operations, but within O n  space, which is independent of σ and k. The new solution still uses E’s value but the counters Ai are computed on-the-ﬂy by exploiting the fact that all windows share the same value of l. We keep an array B indexed by symbols whose entry B[c] stores the number of occurrences of c in T[1 : l]. We can keep these counters updated after a Remove by simply decreasing B[T[l]] by one. We also maintain an array R with an entry for each text position. The entry R[ j] stores the number of occurrences of symbol T[ j] in T[1 : j]. The number of elements in both B and R is no more than n, hence they take O n  space.  These two arrays are enough to correctly update the value Ei after Append wi  , which is in turn enough to estimate H0  see Eq. 3.1 . In fact, we can compute the value Ai[T[ri + 1]] by computing R[ri + 1]− B[T[ri + 1]] which correctly reports the number of occurrences of T[ri + 1] in T[l : ri + 1]. Once we have the value of Ai[T[ri + 1]], we can update Ei as explained in the above item 2. We are left with showing how to support Remove   whose computation requires to evaluate the value of Ai[T[l]] for each window wi . Each of these values can be computed as R[t]− B[T[l]] where t is the last occurrence of symbol T[l] in T[l : ri]. The problem here is given by the fact that we do not know the position t. We solve this issue by resorting to a doubly linked list Lc for each symbol c. The list Lc links together the last occurrences of c in all those windows, ordered by increasing position. Notice that a position j may be the last occurrence of symbol T[ j] for different  but consecutive  windows. In this case we force that position to occur in L T[ j] just once. These lists are sufﬁcient to compute values Ai[T[l]] for all the windows together. In fact, since any position in L T[l] is the last occurrence of at least one sliding window, each of them can be used to compute Ai[T[l]] for the appropriate indices i. Once we have all values Ai[T[l]], we can update all Ei ’s as explained in the above item 1. Since list L T[l] contains no more than log1+ε n elements, all Es can be updated in O log1+ε n  time. Notice that the number of elements in all the lists L is bounded by the text length. Thus, they are stored using O n  space.  It remains to explain how to keep lists L correctly updated. Notice that only one list may change after a Remove   or an Append wi  . In the former case we have possibly to remove position l from list L T[l]. This operation is simple because, if that position is in the list, then T[l] is the last occurrence of that symbol in w1  recall that all the windows start at position l, and are kept ordered by increasing ending position    3.3 On Zero-th Order Compressors  25  and, thus, it must be the head of L T[l]. The case of Append wi   is more involved. Since the ending position of wi is moved to the right, position ri + 1 becomes the last occurrence of symbol T[ri + 1] in wi . Recall that Append wi   inserts symbol T[ri + 1] in wi . Thus, it must be inserted in L T[ri+1] in its correct  sorted  position, if it is not present yet. Obviously, we can do that in O log1+ε n  time by scanning the whole list. This is too much, so we show how to spend only constant time. Let p the rightmost occurrence of the symbol T[ri + 1] in T[0 : ri].3 If p < l, then ri + 1 must be inserted in the front of L T[ri+1] and we have done. In fact, p < l implies that there is no occurrence of T[ri + 1] in T[l : ri] and, thus, no position can precede ri + 1 in L T[ri+1]. Otherwise  i.e. p ∈ l , we have that p is in L T[ri+1], because it is the last occurrence of symbol T[ri + 1] for some window w j with j ≤ i. We observe that if w j = wi , then p must be replaced by ri + 1 which is now the last occurrence of T[ri + 1] in wi ; otherwise ri + 1 must be inserted after p in L T[ri+1] because p is still the last occurrence of this symbol in the window w j . We can decide which one is the correct case by comparing p and ri−1  i.e. the ending position of the preceding window wri−1 . In any case, the list is kept updated in constant time. Lemma 3.2. Let T[1, n] be a text drawn from an alphabet of size σ = poly n . If we estimate Size   via 0-th order entropy, then we can design a dynamic data structure that takes O n  space and supports the operations Remove in R n  = O log1+ε n  time, and Append and Size in L n  = O 1  time.  The following Lemma derives by the discussion above:  In order to evict the cost of the model from the compressed output  see Sect. 2.4.1 , authors typically resort to 0-th order adaptive compressors which do not store the symbols’ frequencies, since they are computed incrementally during the compression  Howard and Vitter 1992 . A similar approach can be used in this case to achieve the same time and space bounds of Lemma 3.2. Here, we require that Size wi   =  T[l : ri] . Recall that with these type of compressors Ca the model must not be stored. We use the same tools above but we change the values stored in variables Ei and the way in which they are updated after a Remove or an Append.   T[l : ri]  = T[l : ri]H a  0  0  Observe that in this case we have that Ca   T[l : ri]  = T[l : ri]H a  0  0   T[l : ri]  = log  ri − l + 1 !  −  cid:3  c∼σ  log nc!   where nc is the number of occurrences of symbol c in T[l : ri]. Therefore, if the  cid:2   T[l : variable Ei stores the value ri]  = log  ri − l + 1 !  − Ei .4  c∼σ log Ai[c]! , then we have that T[l : ri]H a  0  3 Notice that we can precompute and store the last occurrence of symbol T[ j + 1] in T[1 : j] for all js in linear time and space. 4 Notice that the value log  ri − l + 1 !  can be stored in a variable and updated in constant time since the size of the value ri − l + 1 changes just by one after a Remove or an Append.   26  3 Design and Safety Analysis of a Drive-by-Wire Vehicle  After the two operations, we change E’s value in the following way:   1  Remove  : For any window wi we update Ei by subtracting log Ai[T[l]] . We  2  Append wi  : We update Ei by summing log A[T[ri + 1]] and we increase ri by  also increase l by one.  one.  By the discussion above and Theorem 3.2 we obtain:  Theorem 3.3. Given a text T[1, n] drawn from an alphabet of size σ = poly n , we can ﬁnd an  1 + ε -optimal partition of T with respect to a 0-th order  adaptive  compressor in O n log1+ε n  time and O n  space, where ε is any positive constant. We point out that these results can be applied to the compression booster of Ferragina et al.  2005a  to fast obtain an approximation of the optimal partition of Bwt T  . This may be better than the algorithm of Ferragina et al.  2005a  both in time complexity, since that algorithm took O nσ  time, and in compression ratio by log n   see discussion and class of strings in Sect. 3.3.1 . The a factor up to ε  case of a large alphabet  namely, σ = ε polylog n    is particularly interesting whenever we consider either a word-based Bwt  Moffat and Isal 2005  or the Xbw- transform over labeled trees  Ferragina et al. 2005a . We notice that our result is interesting also for the Huffword compressor which is the standard choice for the storage of Web pages  Witten et al. 1999 ; here σ consists of the distinct words constituting the Web-page collection.  √  3.3.1 On Optimal Partition and Booster  An O nσ -exitime algorithm to partition Bwt T   has been presented in Ferragina et al.  2005a . Even if it achieves interesting k-th order-entropy bounds, there are cases in which their greedy algorithm does not ﬁnd the optimal solution. This is indeed a subtle point which is frequently neglected when dealing with compression boosters, especially in practice. In this section we prove that there exists an inﬁnite class of √ strings for which the partition selected by booster  Ferragina et al. 2005a  is far from log n . Consider an alphabet σ = {c1, c2, . . . , cσ} the optimal one by a factor ε  and assume that c1 < c2 < . . . < cσ. We divide it into l = σ α groups of α consecutive symbols each, where α > 0 will be deﬁned later. Let σ1, σ2, . . . , σl denote these sub-alphabets. For each σi , we build a De Bruijn sequence Ti in which each pair of symbols of σi occurs exactly once. By construction each sequence Ti has length α2. Then, we deﬁne T = T1, T2 . . . Tl, so that T = σα and each symbol of σ occurs exactly α times in T . Therefore, the ﬁrst column of Bwt matrix is equal to  c1 α c2 α . . .  cσ α. We denote with Lc the portion of Bwt T   that has symbol c as preﬁx in the Bwt matrix. By construction, if c ∼ σi , we have that any Lc has either one occurrence of each symbol of σi or one occurrence of these symbols of σi minus one plus one occurrence of some symbol of σi−1  or σl if i = 1 . In both   3.3 On Zero-th Order Compressors  27  cases, each Lc has α symbols, which are all distinct. Notice that by construction, the longest common preﬁx among any two sufﬁxes of T is at most 1. Therefore, since the booster can partition only using preﬁx-close contexts  see Ferragina et al. 2005a , there are just three possible partitions:  1  one substring containing all symbols of L,  2  one substring per Lc, or  3  as many substrings as symbols of L. Assuming that the cost of each model is at least log σ bits, 5 then the costs of all possible booster’s partitions are:  1  Compressing the whole L at once has cost at least σα log σ bits. In fact, all the  2  Compressing each string Lc costs at least α log α + log σ bits, since each Lc contains α distinct symbols. Thus, the overall cost for this partition is at least σα log α + σ log σ bits.  symbols in σ have the same frequency in L.   3  Compressing each symbol separately has overall cost at least σα log σ bits.  We consider the alternative partition which is not achievable by the booster that subdivides L into σ α2 substrings denoted S1, S2, . . . , Sσ α2 of size α3 symbols each  recall that T = σα . Notice that each Si is drawn from an alphabet of size smaller than α3. The strings Si are compressed separately. The cost of compressing each string Si is O α3 log α3 + log σ  = O α3 log α + log σ . Since there are σ α2 strings Si s, the cost of this partition is P = O σα log α +  σ α2  log σ . Therefore, by setting √ α = O  log σ  bits. As far as the booster √ is concerned, the best compression is achieved by its second partition whose cost is partition. Since σ ∈ √ log σ  times larger than our proposed O σ log σ  bits. Therefore, the latter is ε   √ n, the ratio among the two partitions is ε   log σ  log log σ , we have that P = O σ  log n .  √  3.4 On k-th Order Compressors  In this section we make one step further and consider the more powerful k-th order compressors, for which there exist Hk bounds for estimating the size of their com- pressed output  see Sect. 2.4.1 . Here Size wi   must compute C T[l : ri]  which is estimated by   ri − l + 1 Hk  T[l : ri]  + fk  ri − l + 1,σT[l:ri] , where σT[l,ri] denotes the number of different symbols in T[l : ri]. Let us denote with Tq[1 : n − q] the text whose i-th symbol T[i] is equal to the q-gram T[i : i + q − 1]. Actually, we can remap the symbols of Tq to integers in [n] without modifying its 0-th order entropy. In fact the number of distinct q-grams occurring in Tq is less than n, the length of T . Thus Tq’s symbols take O log n  bits  5 Here we assume that it contains at least one symbol. Nevertheless, as we will see, the compression gap between booster’s partition and the optimal one grows as the cost of the model becomes bigger.   28  3 Design and Safety Analysis of a Drive-by-Wire Vehicle  and Tq can be stored in O n  space. This remapping takes linear time and space, whenever σ is polynomial in n.  A simple calculation shows that the k-th order  adaptive  entropy of a string  see deﬁnition Sect. 2.4.1  can be expressed as the difference between the 0-th order  adaptive  entropy of its k + 1-grams and its k-grams. This suggests that we can use the solution of the previous section in order to compute the 0-th order entropy of the appropriate substrings of Tk+1 and Tk. More precisely, we use two instances of the data structure of Theorem 3.3  one for Tk+1 and one for Tk , which are kept synchronized in the sense that, when operations are performed on one data structure, then they are also executed on the other. Lemma 3.3. Let T[1, n] be a text drawn from an alphabet of size σ = poly n . If we estimate Size   via k-th order entropy, then we can design a dynamic data structure that takes O n  space and supports the operations Remove in R n  = O log1+ε n  time, and Append and Size in L n  = O 1  time.  Essentially the same technique is applicable to the case of k-th order adaptive compressor C, in this case we keep up-to-date the 0-th order adaptive entropies of the strings Tk+1 and Tk. Theorem 3.4. Given a text T[1, n] drawn from an alphabet of size σ = poly n , we can ﬁnd an  1 + ε -optimal partition of T with respect to a k-th order  adaptive  compressor in O n log1+ε n  time and O n  space, where ε is any positive constant. We point out that this result applies also to the practical case in which the base compressor C has a maximum  block  size B of data it can process at once  this is the typical scenario for gzip, bzip2, etc. . In this situation the time performance of our solution reduces to O n log1+ε B log σ  .  3.5 On BWT-Based Compressors  In literature we know entropy-bounded estimates for the output size of BWT-based compressors  Manzini 2001 . So we could apply Theorem 3.4 to compute the opti- mal partitioning of T for such a type of compressors. Nevertheless, it is also known  Ferragina et al. 2006a  that such compression-estimates are rough in practice because of the features of the compressors that are applied to the Bwt T  -string. Typically, Bwt is encoded via a sequence of simple compressors such as Mtf encoding, Rle encoding  which is optional , and ﬁnally a 0-order encoder like Huffman or Arith- metic  Witten et al. 1999 . For each of these compression steps, a 0-th entropy bound is known  Manzini 2001 , but the combination of these bounds may result much far from the ﬁnal compressed size produced by the overall sequence of compressors in practice  Ferragina et al. 2006a .  In this section, we propose a solution to the optimal partitioning problem for BWT- based compressors that introduces a  cid:3  σ log n  slowdown in the time complexity   3.5 On BWT-Based Compressors  29  of Theorem 3.4, but with the advantage of computing the  1 + ε -optimal solution with respect to the real compressed size, thus without any estimation by any entropy- cost functions. Since in practice it is σ = polylog n , this slowdown should be negligible. In order to achieve this result, we need to address a slightly different  but yet interesting in practice  problem which is deﬁned as follows. The input string T has the form S[1]1S[2]2 . . . S[m]n where each S[i] is a text  called page  drawn from an alphabet σ, and 1, 2, . . . , n are special symbols greater than any symbol of σ. A partition of T must be page-aligned, that is it must form groups of contiguous pages S[i]i . . . S[ j] j , denoted also S[i : j]. Our aim is to ﬁnd a page-aligned partition whose cost is at most  1 + ε  the minimum possible cost, for any ﬁxed ε > 0. We notice that this problem generalizes the table partitioning problem  Buchsbaum et al. 2003 , since we can assume that S[i] is a column of the table.  To simplify things we will drop the Rle encoding step of a Bwt-based algorithm. We start by noticing that a close analog of Theorem 3.2 holds for this variant of the optimal partitioning problem, which implies that a  1 + ε -approximation of the optimum cost  and the corresponding partition  can be computed using a data structure supporting operations Append, Remove, and Size; with the only difference that the windows w1, w2, . . . , wm subject to the operations are groups of contiguous pages of the form wi = S[l, ri].  It goes without saying that there exist data structures designed to dynamically maintain a dynamic text compressed with a Bwt-based compressor under insertions and deletions of symbols  see Chap. 7 . But they do not ﬁt our context for two reasons:  1  their underlying compressor is signiﬁcantly different from the scheme above;  2  in the worst case, they would spend linear space per window yielding a super-linear overall space complexity.  Instead of keeping a given window w in compressed form, our approach only ≤ = Mtf Bwt w   since store the frequency distribution of the integers in the string w this is enough to compute the compressed output size produced by the ﬁnal step of the Bwt-based algorithm, which is usually implemented via Huffman or Arithmetic  Witten et al. 1999 . Indeed, since Mtf produces a sequence of integers from 0 to σ, we can store their number of occurrences for each window wi into an array Fwi of size σ. The update of Fwi due to the insertion or the removal of a page in wi incurs ≤ i as pages are added removed from the two main difﬁculties:  1  how to update w extremes of the window wi ,  2  perform this update implicitly over Fwi , because of the space reasons mentioned above. Our solution relies on two key facts about Bwt and Mtf:  ≤   1  Since the pages are separated in T by distinct separators, inserting or removing one page into a window w does not alter the relative lexicographic order of the original sufﬁxes of w  see Ferragina and Venturini 2010 .   2  If a string s  is obtained from string s by inserting or removing a char c into an ≤  differs from Mtf s  in at most σ symbols. More arbitrary position, then Mtf s ≤ is the next occurrence in s of the newly inserted  or removed  precisely, if c symbol c, then the Mtf has to be updated only in the ﬁrst occurrence of each symbol of σ among c and c  ≤  .   30  3 Design and Safety Analysis of a Drive-by-Wire Vehicle  We can now describe a data structure that supports operations Append w  and Remove  . We assume that the separator symbols in the Bwt T   are ignored by the Mtf step, which means that when the Mtf encoder ﬁnds a separator in Bwt T  , this is replaced with the corresponding integer without altering the Mtf-list. This variant does not introduce any compression penalty  because every separator occurs just once  but simpliﬁes the discussion that follows. Given a range I = [a, b] of positions of T , an occurrence of a symbol of Bwt T   is called active[a,b] if it corresponds to a symbol in T[a : b]. For any range [a, b] ⊆ [n] of positions in T , we deﬁne rBwt T[a : b]  as the string obtained by concatenating the active[a,b] symbols of Bwt T   by preserving their relative order. In the following, we will not indicate the interval when it will be clear from the context. Notice that, due to the presence of separators, rBwt T[a : b]  coincides with Bwt T[a : b]  when T[a : b] spans a group of contiguous pages  see Chap. 7 . Moreover, Mtf rBwt T[a : b]   is the string obtained by performing the Mtf algorithm on rBwt T[a : b] . We will call the symbol Mtf rBwt T[a : b]  [i] as the Mtf-encoding of the symbol rBwt T[a : b] [i]. For each window w, our solution will not explicitly store neither rBwt w  or Mtf rBwt T[a : b]   since this might require a superlinear amount of space. Instead, we maintain only an array Fw of size σ whose entry Fw[e] keeps the number of occurrences of the encoding e in Mtf rBwt w  . The array Fw is enough to com- pute the 0-order entropy of Mtf rBwt w   in σ time  or eventually the exact cost of compressing it with Huffman in σ log σ time .  We are left with showing how to correctly keep updated Fw after a Remove   or an Append w . In the following we will concentrate only on Append w  since Remove   is symmetrical. The idea underlying the implementation of Append w , where w = S[l, r], is to conceptually insert the symbols of the next page S[r+1] into rBwt w  one at time from left to right. Since the relative order among the symbols of rBwt w  is preserved in Bwt T  , it is more convenient to work with active symbols of Bwt T   by resorting to a data structure, whose details are given later, which is able to efﬁciently answer the following two queries with parameters c, I and h, where c ∼ σ, I = [a, b] is a range of positions in T and h is a position in Bwt T  : 0   Prevc I, h : locate the last active[a,b] occurrence in Bwt T  [0, h − 1] of symbol c;  Nextc I, h : locate the ﬁrst active[a,b] occurrence in Bwt T  [h + 1, n] of symbol c. This data structure is built over the whole text T and requires O T  space. Let c be the symbol of S[ri + 1] we have to conceptually insert in rBwt T[a : b] . We can compute the position  say, h  of this symbol in Bwt T   by resorting to the inverse sufﬁx array of T . Once we know position h, we have to determine what changes in Mtf rBwt w   the insertion of c has produced and update Fw accordingly. It is not hard to convince ourselves that the insertion of symbol c changes no more than σ encodings in Mtf rBwt w  . In fact, only the ﬁrst active occurrence of each symbol in σ after position h may change its Mtf encoding. More precisely, let h p and hn be respectively the last active occurrence of c before h and the ﬁrst active occurrence of c after h in Bwt w , then the ﬁrst active occurrence of a symbol after h   3.5 On BWT-Based Compressors  31  changes its Mtf encoding if and only if it occurs active both in Bwt w [h p, h] and in Bwt w [h, hn]. Otherwise, the new occurrence of c has no effect on its Mtf encoding. Notice that h p and hn can be computed via proper queries Prevc and Nextc. In order to correctly update Fw, we need to recover for each of the above symbols their old and new encodings. The ﬁrst step consists of ﬁnding the last active occurrence before h of each symbols in σ using Prev queries. Once we have these positions, we can recover the status of the Mtf list, denoted λ, before encoding c at position h. This is simply obtained by sorting the symbols ordered by decreasing position. In the second step, for each distinct symbol that occurs active in Bwt w [h p, h], we ﬁnd its ﬁrst active occurrence in Bwt w [h, hn]. Knowing λ and these occurrences sorted by increasing position, we can simulate the Mtf algorithm to ﬁnd the old and new encodings of each of those symbols.  This provides an algorithm to perform Append w  by making O σ  queries of types Prev and Next for each symbol of the page to append in w. To complete the proof of the time bounds in Theorem 3.5 we have to show how to support queries of type Prev and Next in O log n  time and O n  space. This is achieved by a straightforward reduction to a classic geometric range-searching problem. Given a set of points P = { x1, y1 ,  x2, y2 , . . . ,  x p, yp } from the set [n]×[n]  notice that n can be larger than p , such that no pair of points shares the same x- or y-coordinate, there exists a data structure  Mäkinen and Navarro 2006  requiring O  p  space and supporting the following two queries in O log p  time:   RangeMax [l, r], h : return among the points of P contained in [l, r]×[−⇒, h] the one with maximum y-value   RangeMin [l, r], h : return among the points of P contained in [l, r]×[h,+⇒] the one with minimum y-value −1 of T in O n log σ  time then, for each symbol c ∼ σ, we deﬁne Pc as the set of points { i, S A −1[i + 1]  T[i] = c} and build the above geometric range-searching structure on Pc. It is easy to see that Prevc I, h  can −1[h + 1]  on be computed in O log n  time by answering query RangeMax I, S A the set Pc, and the same holds for Nextc by using RangeMin instead of RangeMax, this completes the reduction and the proof of the following theorem. Theorem 3.5. Given a sequence of texts of total length n and alphabet size σ = poly n , we can compute an  1+ε -approximate solution to the optimal partitioning problem for a BWT-based compressor, in O n log1+ε n  σ log n  time and O n + σ log1+ε n  space.  Initially we compute S A and S A   Chapter 4 Bit-Complexity of Lempel-Ziv Compression  One of the most famous lossless data-compression schemes is the one introduced by Lempel and Ziv in the late 1970s, and indeed many  non- commercial programs are currently based on it—like gzip, zip, pkzip, arj, rar, just to cite a few. This compression scheme is known as dictionary-based compressor, and consists of squeezing an input text T[1, n] by replacing some of its substrings with  shorter  codewords which are actually pointers to a dictionary of phrases. The dictionary can be either static  in that it has been constructed before the compression starts  or dynamic  in that it is built as the input text is compressed . The well-known LZ77 and LZ78 compressors, proposed by Lempel and Ziv in Ziv and Lempel  1977, 1978 , and all their variants  Salomon 2004 , are interesting examples of dynamic dictionary-based compressors.  Many theoretical and experimental results have been dedicated to LZ-compressors in these 30 years and, although today there are alternative solutions to the loss- less data-compression problem [e.g., Burrows-Wheeler compression and Predic- tion by Partial Matching  Witten et al. 1999 ], dictionary-based compression is still widely used for its unique combination of compression power and compres- sion decompression speed. Over the years dictionary-based compression has also gained importance as a general algorithmic tool, being employed in the design of compressed text indexes  Navarro and Mäkinen 2007 , in universal clustering Cilibrasi and Vitányi 2005  or classiﬁcation tools  Ziv 2007 , in designing optimal pre-fetching mechanisms  Vitter and Krishnan 1996 , and in streaming or on-the-ﬂy compression applications  Cormode and Muthukrishnan 2005; Gagie and Manzini 2007 .  In this chapter we address some key issues which arise when dealing with the output-size in bits of the so called LZ77-parsing scheme, namely the one in which the dictionary consists of all substrings starting in the last M scanned positions of the text, where M is called the window size  and possibly depends on the text length , and phrase-codewords consist of triples≤d, ε, c∈ where d is the relative offset of the copied phrase  d √ M , ε is the length of the phrase and c is the single  new  character fol- lowing it. Classically, the LZ77-parser adopts a greedy rule, namely one that at each  R. Venturini, Compressed Data Structures for Strings, Atlantis Studies in Computing 4, DOI: 10.2991 978-94-6239-033-1_4,   Atlantis Press and the authors 2014  33   34  4 Bit-Complexity of Lempel-Ziv Compression  step takes the longest dictionary phrase which is a preﬁx of the currently unparsed sufﬁx of the input text. This greedy parsing can be computed in O n log σ  time and O M  space  Fiala and Greene 1989 .1 The greedy parsing is optimal with respect to the number of phrases in which T can be parsed by any sufﬁx-complete dictionary  like the LZ77-dictionary . Of course, the number of parsed phrases inﬂuences the compression ratio and, indeed, various authors  Ziv and Lempel 1977; Kosaraju and Manzini 1999  proved that greedy parsing achieves asymptotically the  empirical  entropy of the source generating the input text T. But these fundamental results have not closed the problem of optimally compressing T because the optimality in the number of parsed phrases is not necessarily equal to the optimality in the number of bits output by the ﬁnal compressor on each individual input string T. In fact, if we assume that the phrases are compressed via an equal-length encoder, like in Kosaraju and Manzini  1999 , Salomon  2004 , Ziv and Lempel  1977 , then the output produced by the greedy parsing scheme is bit optimal. But if one aims for higher compression, variable-length encoders should be taken into account  see e.g. Witten et al.  1999 , Salomon  2007 , and the software gzip2 , and in this situation the greedy-parsing scheme is no longer optimal in terms of the number of bits output by the ﬁnal compressor.  Starting from these premises we address in this chapter four main problems, both on the theoretical and the experimental side, which pertain with the bit-optimal compression of the input string T via parsers that deploy the LZ77-dictionary built on an unbounded window  namely, it is M = n . Our results extend easily to windows of arbitrary size M < n. Problem 1 Let us consider the greedy LZ77-parser, and assume that we encode every parsed phrase wi with a variable-length encoder. The value of εi = wi is in some sense ﬁxed by the greedy choice, being the length of the longest phrase occurring in the current LZ77-dictionary. Conversely, the value of di depends on the position of the copy of wi in T. In order to minimize the number of bits output by the ﬁnal compressor, the greedy parser should obviously select the closest copy of each phrase wi in T, and thus the smallest possible di. Surprisingly enough, known implementations of greedy parsers are time optimal but not bit-optimal, because they select an arbitrary or the leftmost occurrence of the longest copied phrase  see Crochemore et al.  2008  and references therein , or they select the closest copy but take O n log n  suboptimal time  Amir et al. 2002 . In Sect. 4.2 we provide an elegant, yet simple, algorithm which computes at each parsing step the closest copy overall time and O n  space of the longest dictionary phrase in O  Theorem 4.1 . Problem 2 How good is the greedy LZ77-parsing of T whenever the compression cost is measured in terms of number of bits produced in output? We show that the greedy selection of the longest dictionary phrase at each parsing step is not   cid:2  1 + log σ   cid:2  n  log log n   cid:3  cid:3   1 Recently, Crochemore et al.  2008  showed how to achieve the optimal O n  time and space when the alphabet has size O n  and the window is unbounded, i.e., M = n. 2 Gzip home page http:  www.gzip.org.   4 Bit-Complexity of Lempel-Ziv Compression  35  optimal, and this may be larger than the bit-optimal parsing by a multiplicative factor  cid:3  log n  log log n , which is unbounded asymptotically  Sect. 4.3 . Additionally, we show that this lower-bound is tight up to a factor σ log log n , and we support these theoretical ﬁgures with some experimental results which stress the practical importance of ﬁnding the bit-optimal parsing of T.  Problem 3 How much efﬁciently  in time and space  can we compute the bit-optimal  LZ77- parsing of T? Several solutions are indeed known for this problem but they are either inefﬁcient  Schuegraf and Heaps 1974 , in that they take σ n2  worst-case time and space, or they are approximate  Katajainen and Raita 1989 , or they rely on heuristics  Klein 1997; Smith and Storer 1985; Bekesi et al. 1997; Cohn and Khazan 1996  which do not provide any guarantee on the time space performance of the compression process. This is the reason why Rajpoot and Sahinalp stated in Rajpoot and Sahinalp  2002  at page 159 that “We are not aware of any on-line or off-line parsing scheme that achieves optimality when the LZ77-dictionary is in use under any constraint on the codewords other than being of equal length”. In this chapter we investigate this question by considering a general class of variable-length codeword encodings which are typically used in data compression  e.g. gzip  and in the design of search engines and compressed indexes  Navarro and Makinen 2007; Salomon 2004; Witten et al. 1999 . Our ﬁnal result is a time efﬁcient  possibly, optimal  and space optimal for the problem above  Theorem 4.3 .  Technically speaking, we follow  Schuegraf and Heaps 1974  and model the search for a bit-optimal parsing of an input string T as a single-source shortest path problem  shortly, SSSP  on a weighted DAG G T   consisting of n nodes, one per character of T, and e edges, one per possible parsing step. Every edge is weighted according to the length in bits of the codeword adopted to compress the correspond- ing phrase. Since these codewords are tuples of integers  see above , we consider a natural class of codeword encoders which satisfy the so called increasing cost property: the greater is the integer to be encoded, the longer is the codeword. This class encompasses most of the encoders used in the literature to design data com- pressors  see Salomon  2007  and gzip , compressed full-text indexes  Navarro and Mäkinen 2007  and search engines  Witten et al. 1999 . We prove new com- binatorial properties for this SSSP-problem and show that the computation of the SSSP in G T   can be restricted onto a subgraph cid:4 G T   whose structure depends on the integer-encoding functions adopted to compress the LZ77-phrases, and whose size is provably smaller than the complete graph generated by Schuegraf and Heaps  1974   see Theorem 4.2 . Additionally, we design an algorithm that solves the SSSP on the subgraph  cid:4 G T   without materializing it all at once, but creating and exploring its edges on-the-ﬂy in optimal O 1  amortized time per edge and O n  optimal space overall. As a result, our novel LZ77-compressor achieves bit-optimality in O n  opti- mal working space and in time proportional to  cid:4 G T  . This way, the compressor is optimal in the size of the sub-graph which is O n log n  for a large class of integer encoders, like Elias, Rice, and Fibonacci codes  Witten et al. 1999; Salomon 2007 , and it is optimal O n  for  most of  the encodings used by gzip. This is the ﬁrst result providing a positive answer to Rajpoot-Sahinalp’s question above.   36  4 Bit-Complexity of Lempel-Ziv Compression  Problem 4 How much efﬁcient is in practice our bit-optimal LZ77-compressor? To establish this, we have taken several freely available text collections, and compared our compressor against the classic gzip and bzip2,3 as well as against the state- of-the-art boosting compressor of Ferragina et al.  2005a . Section 4.5 reports some experimental ﬁgures, and comments on our theoretical ﬁndings as well as on possible algorithm-engineering research directions which deserve further attention.  4.1 Notation and Terminology Let T[1, n] be a string drawn from an alphabet λ = [σ]. In the following we will assume that σ is at most n.4 We use T[i] to denote the ith symbol of T; T[i : j] to denote the substring  also called the phrase  extending from the ith to the jth symbol in T  extremes included ; and Ti = T[i : n] to denote the i-th sufﬁx of T.  In the rest of the chapter we concentrate on LZ77-compression with an unbounded window size, so we will drop the speciﬁcation “LZ77” unless this will be required to make things unambiguous. The compressor, as any dictionary-based compressor, will work in two intermingled phases: parsing and encoding. Let w1, w2, . . . , wi−1 be the phrases in which a preﬁx of T has been already parsed. At this step, the dictionary consists of all substrings of T starting in the last M positions of w1, w2 . . . wi−1, where M is called the window size  hereafter assumed unbounded, for simplicity . The classic parsing-rule adopted by most LZ77-compressors selects the next phrase according to the so called longest match heuristic: that is, this phrase is taken as the longest phrase in the current dictionary which preﬁxes the remaining sufﬁx of T. This is usually called greedy parsing. After such a phrase is selected, the parser adds one further symbol to it and thus forms the next phrase wi of T’s parsing. In the rest of the chapter, and for simplicity of exposition, we will restrict to the LZ77-variant which avoids the additional symbol per phrase. This means that wi is represented by the integer pair ≤di, εi∈, where di is the relative offset of the copied phrase wi within the preﬁx w1 . . . wi−1 and εi is its length wi. Every ﬁrst occurrence of a new symbol c is encoded as ≤0, c∈. We allow self-referencing phrases, i.e., a phrase’s source could overlap the phrase itself.  Once phrases are identiﬁed and represented via pairs of integers, their components are compressed via variable-length integer encoders which eventually produce the compressed output of T as a sequence of bits. In order to study and design bit-optimal parsing schemes, we therefore need to deal with such integer encoders. Let f be an integer-encoding function that maps any integer x ∈ [n] into a  bit- codeword f  x  whose length is denoted by f  x  bits. In this chapter we consider variable-length encodings which use longer codewords for greater integers:  3 Bzip2 home page http:  www.bzip.org . 4 In case of a larger alphabet, our algorithms are still correct but we need to add the term Tsort  n, σ  to their time complexities, which denotes the time required to sort remap all distinct symbols of T into the range [n].   4.1 Notation and Terminology  37  Property 1  Increasing Cost Property . For any x, y ∈ [n], x √ y iff f  x  √ f  y .  This property is satisﬁed by most known integer encoders—like equal-length codewords, Elias codes  Witten et al. 1999 , Rice’s codes  Salomon 2004 , Fibonacci’s codes  Salomon 2007 —which are used to design data compressors  Salomon 2004 , compressed full-text indexes  Navarro and Mäkinen 2007  and search engines  Witten et al. 1999 .  4.2 An Efﬁcient and Bit-Optimal Greedy Parsing  In this section we describe how to compute efﬁciently the greedy parsing that mini- mizes the ﬁnal compressed size. We remark that the minimization here is done with respect to all the LZ77-parsings that follow the greedy strategy for selecting their phrases; this means that these parsings are constrained to take at every step the longest possible phrase matching the current sufﬁx, but are free to choose which previous copy of this phrase to select. Let f and g be two integer encoders which satisfy the Increasing Cost Property  possibly f = g . We denote by LZf ,g T   the compressed output produced by the greedy-parsing strategy in which we have used f to compress the distance di, and g to compress the length εi of parsed phrase wi. Thus, in LZf ,g T   any phrase wi is encoded in f  di  + g εi  bits. Given that the parsing is the greedy one, εi is ﬁxed  being the length of the longest copy , so we minimize LZf ,g T   by minimizing the distance di of wi’s copy in T. If pi is the starting position of wi in T  namely T[pi, pi + εi−1] = wi , many copies of the phrase wi could be present in T[1, pi−1]. To minimize LZf ,g T   we should choose the copy which is the closest one to pi, and thus requires the minimum number of bits to encode its distance di  recall the assumption M = n . In this section we propose an elegant, yet simple, algorithm that selects the right- most copy of each phrase wi in O n 1 + log σ  log log n   time. This algorithm is the fastest known in the literature  Crochemore et al. 2008 . It requires the sufﬁx tree ST of T and the parsing of T which consists of, say, k √ n phrases. It is well known that all these machineries can be computed in linear time and space. We say that a node u of ST is marked iff the string spelled out by the root-to-u path in ST is equal to some phrase wi. In this case we use the notation upi to denote the node marked by phrase wi which starts at position pi of T. Since the same node may be marked by different phrases, but any phrase marks just one node, the total number of marked nodes is bounded by the number of phrases, hence k. Furthermore, if a node is assigned with many phrases, since the greedy LZ77-parsing takes the longest one, it must be the case that every such occurrences of wi is followed by a distinct character. So the number of phrases assigned to the same marked node is bounded by σ.   38  4 Bit-Complexity of Lempel-Ziv Compression  All marked nodes can be computed in O n  time by searching each phrase in the sufﬁx tree ST. Let us now deﬁne STC as the contracted version of ST, namely a tree whose internal nodes are the marked nodes of ST and whose leaves are the leaves of ST. The parent of any node in STC is its lowest marked ancestor in ST. It is easy to see that STC consists of O k  internal nodes and n leaves, and that it can be built in O n  time via a top-down visit of ST.  Given the properties of sufﬁx trees, we can now rephrase our problem as follows: for each position pi, we need to compute the largest position xi which is smaller than pi and whose leaf in STC lies within the subtree rooted at upi. Our algorithm processes the input string T from left to right and, at each position j, it maintains the following invariant: the parent v of any leaf in STC stores the maximum position h < j such that the leaf labeled h is attached to v. Maintaining this invariant is trivial: after that position j is processed, j is assigned to the parent of the leaf labeled j in STC. The key point now is how to compute the position xi of the rightmost-copy of wi whenever we discover that j is the starting position of a phrase  i.e. j = pi for some i . In this case, the algorithm visits the subtree of STC rooted at uj and computes the maximum position stored in its internal nodes. By the invariant, this position is the rightmost copy of the phrase wi. This process takes O n + σ    time,   is the number of internal nodes in the subtree rooted at upi in STC. In where  upi fact, by construction, there can be at most σ repetitions of the same phrase in the parsing of T, and for each of them the algorithm performs a visit of the corresponding subtree.   = O n . By properties of sufﬁx trees, the depth of upi is smaller than εi = wi, and each  marked  node of STC is visited as many times as the number of its  marked  ancestors in STC  with their multiplicities . For each  marked  node upi, this number can be bounded by εi = O wi . Summing i=1 O wi  = O n . Thus, the above algorithm requires up on all nodes, we get O σ × n  time, which is linear whenever σ = O 1 .  cid:3  cid:3   As a ﬁnal step we prove that   cid:5  k i=1  upi  k i=1  upi   cid:5    cid:5    cid:2    cid:2   k  Now we will show how to further reduce the time complexity to O  n  1 + log σ  log log n  by properly combining a slightly modiﬁed variant of the tree covering procedure of Geary et al.  2006  with a dynamic Range Maximum Query data structure  McCreight 1985; Willard 2000  applied on properly composed arrays of integers. Notice that this improvement leads to an algorithm requiring O n  time for alphabets of size poly-logarithmic in n. Given STC and an integer parameter P ∼ 2  in our case P = σ  this procedure covers the k internal nodes of STC in a number of connected subtrees, all of which have size σ P , except the one which contains the root of STC that has size O P . Any two of these subtrees are either disjoint or intersect at their common root.  We refer to Sect. 2 of Geary et al.  2006  for more details.  In our modiﬁcation we impose that there is no node in common to two subtrees, because we move their common root to the subtree that contains its parent. None of the above properties change, except for the fact that each cover could now be a subforest instead of subtree of STC. Let F1, F2, . . . Ft be the subforests obtained by the above covering, where we clearly have that t = O k P .   4.2 An Efﬁcient and Bit-Optimal Greedy Parsing  39  We deﬁne the tree STSC whose leaves are the leaves of STC and whose internal nodes are the above subforests. With a little abuse of notation, let us refer with Fi to the node in STSC corresponding to the subforest Fi. The leaf l having u as parent in STC, is thus connected to the node Fi in STSC, where Fi is the forest that contains the node u. Notice that roots of subtrees in any subforest Fi have common parent in STC. The computation of the rightmost copy for a phrase pi is now divided in two phases. Let Fi be the subforest that contains upi, the node spelled out by the phrase starting at T[pi]. In the ﬁrst phase, we compute the rightmost copy for the phrase starting at pi among the descendants of upi in STSC that belong to subforests different from Fi. In the second phase, we compute its rightmost copy among the descendants of upi in Fi. The maximum between these two values will give the rightmost copy for pi, of course. To solve the former problem, we execute our previous algorithm on STSC. It simply visits all subforests descendant from Fi in STSC, each of them maintaining the rightmost position among its already scanned leaves, and returns the maximum of these value. Since groups of P = σ nodes of STC have single nodes in STSC, in this case our previous algorithm requires O n  time.  The latter problem is solved with a new algorithm exploiting the fact that the number of nodes in Fi is O σ  and resorting to dynamic Range Maximum Queries  RMQ  on properly deﬁned arrays  McCreight 1985 . We assign to each node of Fi an unique integer in the range [Fi] that corresponds to the time of its visit in a depth-ﬁrst traversal of Fi. This way the nodes in the subtree rooted at some node u are identiﬁed by integers spanning the whole range from the starting time to the ending time of the DFS-visit of u. We use an array AFi that has an entry for each node of Fi at the position speciﬁed by its starting-time of the DFS-visit. Initially, all entries are set to −⊆; as algorithm proceeds, nodes entries of Fi AFi will get values that denote  the rightmost position of the copies of their corresponding phrases in T. Precisely, as done before T is scanned rightward and when a position j of T is processed, we identify the subforest Fi containing the father of the leaf labeled j in STC and we change to j the entry in AFi corresponding to that node. If j is also the starting position of a phrase, we identify the subforest Fx containing the node uj which spells out that phrase  it is an ancestor of the leaf labeled j 5 and compute its rightmost copy in Fx by executing a RMQ on AFx . The left and right indexes for the range query are, respectively, the starting and ending time of the visit of uj in Fx.  Since AFi is changed as T is processed, we need to solve a dynamic Range Maxi- mum Queries. To do that we build on each array AFi a binary balanced tree in which leaves correspond to entries of the array while each internal node stores the maxi- mum value in its subtree  hence, sub-range of AFi . In this way, Range-Max queries and updates on AFi take O log σ  time in the worst case. Indeed, an update may possibly change the values stored in the nodes of a leaf-to-root path having O log σ  length; a Range-Max query has to compute the maximum among the values stored in  at most  O log σ  nodes, these nodes are the ones covering the queried interval.  5 Notice that the node uj can be identiﬁed during the rightward scanning of T as usually done in LZ77-parsing, taking O n  time for all identiﬁed phrases.   40  4 Bit-Complexity of Lempel-Ziv Compression  We notice that the overall complexity of the algorithm is dominated by the O n  updates to the RMQ data structures  one for each text position  and the O k  RMQ queries  one for each phrase . Then the algorithm takes O n log σ + k log σ  = O n log σ  time and O n  space. A further improvement can be obtained by adopting an idea similar to the one in Willard  2000, Sect. 5  to reduce the height of each balanced tree and, consequently, our time complexity by a factor O log log n . The idea is to replace each of the above binary balanced trees with a balanced tree with  1 2  log n  log log n branching factor. The children of each node u in this tree are associated with the maximum value stored in their subtrees. Together with these maxima, we store in u an array with an entry of log log n bits for each child. The i-th entry stores the rank of the maximum associated with the i-th child with respect to the maxima of the other children. The above array ﬁts in  1 2  log n bits. Given a range of children, a query asks to compute in constant time the position of the maximum associated with the children in this range. The query is solved with a lookup in a table n log2 n log log n  bits which tabulates the result of any possible query of size O  on any possible array of ranks. Instead, an update asks to increase the maximum ⇒ associated with a child and adjusts accordingly the array of ranks. For this aim we n log2 n  bits which tabulates the result of any possible resort to a table of size O  update on any possible array of ranks. Theorem 4.1 Given a string T[1, n] drawn from an alphabet λ = [σ], there exists an algorithm that computes the greedy parsing of T and reports the rightmost copy of each phrase in the LZ77-dictionary taking O time and O n  space.   cid:2  1 + log σ   cid:2  n  log log n  ⇒   cid:3  cid:3   4.3 On the Bit-Efﬁciency of the Greedy LZ77-Parsing  We have already noticed in the Introduction that the greedy strategy used by LZf ,g T   is not necessarily bit-optimal, so we will hereafter use OPTf ,g T   to denote the Bit-Optimal LZ77-parsing of T relative to the integer encoding functions f and g. OPTf ,g T   computes a parsing of T which uses phrases extracted from the LZ77- dictionary, encodes these phrases by using f and g, and minimizes the total number of bits in output. Of course LZf ,g T   ∼ OPTf ,g T  , but we aim at establishing how much worse the greedy parsing can be with respect to the bit-optimal one. In LZf ,g T   OPTf ,g T   = what follows we identify an inﬁnite family of strings T for which   cid:3  , so the gap may be asymptotically unbounded thus stressing the need   cid:3  for an  f , g -optimal parser, as requested by Rajpoot and Sahinalp  2002 .  log log n  log n   cid:2   Our argument holds for any choice of f and g from the family of encoding functions that represent an integer x with a bit string of size σ log x  bits  thus the well-known Elias’, Rice’s, and Fibonacci’s coders belong to this family . These encodings satisfy the increasing cost property stated in Sect. 4.1. So taking inspiration from the proof of Lemma 4.2 in Kosaraju and Manzini  1999 , we consider the inﬁnite family of   4.3 On the Bit-Efﬁciency of the Greedy LZ77-Parsing  41  strings Tl = bal c2l ba ba2 ba3 . . . bal, parameterized in the positive integer l. The greedy LZ77-parser partitions Tl as6:   b   a   al−1   c   c2l−1   ba   ba2   ba3  . . .  bal ,  where the symbols forming a parsed phrase have been delimited within a pair of brackets. Thus it copies the latest l phrases from the beginning of Tl and takes at least l × f  2l  = σ l2  bits, since we are counting only the cost for encoding the distances. A more parsimonious parser selects the copy of bai−1  with i > 1  from its  immediately previous occurrence thus parsing Tl as:   b   a   al−1   c   c2l−1   b   a   ba   a   ba2   a  . . .  bal−1   a .  l i=2  Hence the encoding of this parsing, called rOPT Tl , takes g 2l − 1  +  [f  i  + g i  + f  0 ] + O l  = O l log l  bits.  g l − 1  +  cid:5  Lemma 4.1 There exists an inﬁnite family of strings such that, for any of its elements T, it is LZf ,g T   ∼ σ logT  log logT  OPTf ,g T  . Proof 1 Since OPT Tl  is a parsimonious parser, not necessarily the optimal one, it is OPT Tl  √ rOPT Tl . Then we can write: rOPT Tl  ∼  cid:2  . Since Tl = 2l + l2 − O l , we have that l = σ logTl  for sufﬁciently  cid:2   OPTf ,g Tl  ∼ LZf ,g Tl  LZf ,g Tl    σ long strings.  log l   cid:3   l  negligible in practice too.  LZf ,g T   OPTf ,g T   √ f  T +g T  f  0 +g 0   The experimental results reported in Table 4.1 will show that this gap is not Additionally we can prove that this lower bound is tight up to a log logT mul- tiplicative factor, by easily extending to the case of the LZ77-dictionary  which is dynamic , a result proved in Katajainen and Raita  1992  for static dictionaries. Pre- , which is upper bounded by O logT  cisely, it holds that becausef  T  = g T  = σ logT  andf  0  = g 0  = O 1 . To see this, let us assume that LZf ,g T   and OPTf ,g T   are formed by εlz and εopt phrases respec- tively. Of course, εlz √ εopt because the greedy parsing is optimal with respect to the number of parsed phrases for T, whereas the other parser is optimal with respect to the number of bits in output. We then assume the worst-case scenario in which every phrase is encoded by LZf ,g T   with the longest encoding  namely, f  T  andg T  bits each  while OPTf ,g T   uses the shortest one  namely,f  0  andg 0  bits each . f  0 +g 0  = σ logT . Therefore, we have  LZf ,g T   OPTf ,g T   √ εlz f  T +g T   εopt  f  0 +g 0    √ f  T +g T   6 Recall the variant of LZ77 we are considering in this chapter, which uses just a pair of integers per phrase, and thus drops the char following that phrase in T.   42  4 Bit-Complexity of Lempel-Ziv Compression  4.4 On Bit-Optimal Parsings and Shortest-Path Problems  In this section we will describe how to compute efﬁciently the parsing that minimizes the ﬁnal compressed size of T with respect to all possible LZ77-parsings. Following Schuegraf and Heaps  1974 , we model the design of a bit-optimal LZ77-parsing strategy for a string T as a Single-Source Shortest Path problem  shortly, SSSP- problem  on a weighted DAG G T   deﬁned as follows. Graph G T   =  V , E  has one vertex per symbol of T plus a dummy vertex vn+1, and its edge set E is deﬁned so that  vi, vj  ∈ E iff  1  j = i + 1 or  2  the substring T[i : j − 1] occurs in T starting from a  previous  position p < i. Clearly i < j and thus G T   is a DAG. Every edge  vi, vj  is labeled with the pair ≤di,j, εi,j∈ which is set to ≤0, T[i]∈ in case  1 , or it is set to ≤i − p, j − i∈ in case  2 . The second case corresponds to copying a phrase longer than one single character.7  It is easy to see that the edges outgoing from vi denote all possible parsing steps that can be taken by any parsing strategy which uses a LZ77-dictionary. Hence, there exists a correspondence between paths from v1 to vn+1 in G T   and LZ77- parsings of the whole string T. If we weight every edge  vi, vj  ∈ E with an integer c vi, vj  = f  di,j  + g εi,j , which accounts for the cost of encoding its label  phrase  via the integer encoding functions f and g, then the length in bits of the encoded parsing is equal to the cost of the corresponding weighted path in G T  . The problem of determining OPTf ,g T   is thus reduced to computing the shortest path from v1 to vn+1 in G T  . Given that G T   is a DAG, its shortest path from v1 to vn+1 can be computed in O E  time and space. However, this is σ n2  in the worst case  take e.g. T = an  thus resulting inefﬁcient and actually unfeasible in practice even for strings of few Megabytes. In what follows we show that the computation of the SSSP can be restricted to a subgraph of G T   whose size depends on the choice of f and g, provided that they satisfy Property 1  see Sect. 4.1 , and is O n log n  for most known integer- encoding functions used in practice. Then we will design efﬁcient algorithms and data structures that will allow us to generate this subgraph on-the-ﬂy by taking O 1  amortized time per edge and O n  space overall. These algorithms will be therefore time-and-space optimal for the subgraph in hand, and will provide the ﬁrst positive answer to Rajpoot-Sahinalp’s question we mentioned at the beginning of this chapter.  4.4.1 An Useful and Small Subgraph of G T   We use FS v  to denote the forward star of a vertex v, namely the set of vertices pointed by v in G T  ; and we use BS v  to denote the backward star of v, namely the  7 Notice that there may be several different candidate positions p from which we can copy the substring T[i : j − 1]. We can arbitrarily choose any position among the ones whose distance from i is encodable with the smallest number of bits  namely, f  di,j  bits is minimized .   4.4 On Bit-Optimal Parsings and Shortest-Path Problems  43  set of vertices pointing to v in G T  . We can prove that the indices of the vertices in FS v  and BS v . Fact 2 Given a vertex vi and let vi+x and vi−y be respectively the vertex with greatest index in FS vi  and the smallest index in BS vi , it holds   FS vi  = {vi+1 . . . , vi+x−1, vi+x} and   BS vi  = {vi−y . . . , vi−2, vi−1}. Furthermore, x and y are smaller than the length of the longest repeated substring in T. Proof 2 By deﬁnition of  vi, vi+x , string T[i : i + x − 1] occurs at some position p < i in T. Any preﬁx T[i : k − 1] of T[i : i + x − 1] also occurs at that position p, thus vk ∈ FS vi . The bound on x derives from the deﬁnition of  vi, vi+x  which  cid:2  represented a repeated substring in T. A similar argument holds for BS vi .  This means that if an edge  vi, vj  does exist in G T  , then there exist also all edges which are nested within it and are incident into one of its extremes, namely either vi or vj. The following property relates the indices of the vertices vj ∈ FS vi  with the cost of their connecting edge  vi, vj , and not surprisingly shows that the smaller is j  i.e. the shorter is the edge , the smaller is the cost of encoding the phrase T[i : j − 1].8 Fact 3 Given a vertex vi, for any pair of vertices vj≥ , vj≥≥ ∈ FS vi  such that j ≥ < j we have that c vi, vj≥   √ c vi, vj≥≥  . The same property holds for vj≥ , vj≥≥ ∈ BS vi . ≥≥ − 1] and thus the Proof 3 We observe that T[i : j ﬁrst substring occurs wherever the latter occurs. Therefore, we can always copy ≥≥− 1]. By the Increasing T[i : j Cost Property satisﬁed by f and g, we have that f  di,j≥   √ f di,j≥≥   and g εi,j≥   √ g εi,j≥≥  .  cid:2   ≥− 1] from the same position at which we copy T[i : j  ≥ − 1] is a preﬁx of T[i : j  ≥≥  ,  Given these monotonicity properties, we are ready to characterize a special subset  of the vertices in FS vi , and their connecting edges. Deﬁnition 4.1 An edge  vi, vj  ∈ E is called   d—maximal iff the next edge from vi takes more bits to encode its distance: f  di,j  < f  di,j+1 ;   ε—maximal iff the next edge from vi takes more bits to encode its length:g li,j  < g li,j+1 . Edge  vi, vj  is called maximal if it is either d-maximal or ε-maximal: thus c vi, vj  < c vi, vj+1 .  The number of maximal edges depends on the functions f and g  which satisfy Property 1 . Let Q f , n   resp. Q g, n   be the number of different codeword lengths  8 Recall that c vi, vj  = f  di,j +g εi,j , if the edge does exist, otherwise we set c vi, vj  = +⊆.   44  4 Bit-Complexity of Lempel-Ziv Compression  generated by f  resp. g  when applied to integers in the range[n]. We can partition[n] into contiguous sub-ranges I1, I2, . . . , IQ f ,n  such that the integers in Ii are mapped to codewords  strictly  shorter than the codewords for the integers in Ii+1. by f Similarly, g partitions the range [n] in Q g, n  contiguous sub-ranges. Lemma 4.2 There are at most Q f , n + Q g, n  maximal edges outgoing from any vertex vi.  Proof 4 By Fact 2, vertices in FS vi  have indices in a range R, and by Fact 3, c vi, vj  is monotonically non-decreasing as j increases in R. Moreover we know that f  resp. g   cid:2  cannot change more than Q f , n   resp. Q g, n   times. To speed up the computation of a SSSP from v1 to vn+1, we construct a subgraph  cid:4 G T   of G T   which is formed by maximal edges only, it is smaller than G T   and contains one of those SSSP. Theorem 4.2 There exists a shortest path in G T   from v1 to vn+1 that traverses  cid:2  maximal edges only.  , vir+1  Proof 5 By contradiction assume that every such shortest path contains at least one  , vih+1    by Fact 1 , and by Fact 2 on BS vih+1 , vih+1  . . . vik , with i1 = 1 and ik = n + 1, be one of non-maximal edge. Let π = vi1 , vi2 these shortest paths, and let γ = vi1 . . . vir be the longest initial subpath of π which traverses maximal edges only. Assume w.l.o.g. that π is the shortest path maximizing the value of γ. We know that  vir   is a non-maximal edge, and thus we can , vj  that has the same cost. By deﬁnition of maximal take the maximal edge  vir edge, it is j > ir+1. Now, since G T   is a DAG and indices in π are increasing, it must exist an index ih ∼ ir+1 such that the index of that maximal edge j lies in [ih, ih+1]. Since  vih   is an edge of π and thus of the graph G T  , it does   we can conclude exist the edge  vj, vih+1   √ c vih . . . vik is also that c vj, vih+1 a shortest path but its longest initial subpath of maximal edges consists of γ + 1  cid:2  vertices, which is a contradiction. Theorem 4.2 implies that the distance between v1 and vn+1 is the same in G T   and  cid:4 G T  , with the advantage that computing SSSP in  cid:4 G T   can be done faster and in reduced space, because the subgraph  cid:4 G T   consists of n + 1 vertices and at most n Q f , n  + Q g, n   edges.9 For Elias’ codes  Elias 1975 , Fibonacci’s codes  Salomon 2007 , Rice’s codes  Salomon 2004 , and most practical integer encoders used for search engines and data compressors  Salomon 2004; Witten et al. 1999 , it is Q f , n  = Q g, n  = O log n . Therefore  cid:4 G T   = O n log n , so it is smaller than the complete graph built and used by previous chapters  Schuegraf and Heaps 1974; Katajainen and Raita 1989 . For the encoders used in gzip, it is Q f , n  = Q g, n  = O 1  and thus, in these practical settings, we have  cid:4 G T   = O n .   . Consequently, the path vi1  . . . vir vjvih+1  9 Observe that FS v  √ Q f , n  + Q g, n , for any vertex v in  cid:4 G T    Lemma 4.2 .   4.4 On Bit-Optimal Parsings and Shortest-Path Problems  45  4.4.2 An Efﬁcient Bit-Optimal Parser  From a high level, our solution is a variant of a classic linear-time algorithm for SSSP over a DAG [see Sect. 24.2 in Cormen et al.  2001 ], here applied to work on the subgraph  cid:4 G T  . Therefore, its correctness follows directly from Theorem 24.5 of Cormen et al.  2001  and our Theorem 4.2. However, it is clear that we cannot generate  cid:4 G T   by pruning G T  , so the key difﬁculty in computing the SSSP is how to generate on-the-ﬂy and efﬁciently  in time and space  the maximal edges outgoing from vertex vi. We will refer to this problem as the forward-star generation problem, and use FSG for brevity. In what follows we show that FSG takes O 1  amortized time per edge and O n  space in total  although the size of G T   may be ω n  . Combining this result with Lemma 4.2, we will obtain the following theorem: Theorem 4.3 Given a string T[1, n] drawn from an alphabet λ = [σ], and two integer-encoding functions f and g that satisfy Property 1, there exists a compressor that computes the  f , g -optimal parsing of T based on a LZ77-dictionary by taking O n Q f , n  + Q g, n    time and O n  space in the worst case. Most of the integer-encoding functions used in practice are such that Q e, n  = O log n   Salomon 2007 . This holds also for the following codes: Elias Gamma code, Elias Delta code, Elias Omega code, Fibonacci code, Variable-Byte code, Even-Rodeh codes, Nibbles code Rice codes or Boldi-Vigna Zeta codes  Witten et al. 1999; Salomon 2007; Boldi and Vigna 2005 . Thus, by Theorem 4.3 we derive the following corollary that instantiates our result for many of such integers-encoding functions. Corollary 4.1 Given a string T[1, n] drawn from an alphabet λ = [σ], and let f and g be chosen among the class of integer codes indicated above, the  f , g -optimal parsing of T based on a LZ77-dictionary can be computed in O n log n  time and O n  working space in the worst case.  To the best of our knowledge, this result is the ﬁrst one that answers positively to the question posed by Rajpoot and Sahinalp in  2002  at page 159. The rest of this section will be devoted to prove Theorem 4.3, which is indeed the main contribution of this chapter. From Lemma 4.2 we know that the edges outgoing from vi can be partitioned into no more than Q f , n  groups, according to the distance from T[i] of the copied string they represent. Let I1, I2, . . . , IQ f ,n  be the intervals of distances such that all distances in Ik are encoded with the same number of bits by f . Take now the   for the interval Ik. We can infer that substring T[i : hk − 1] d-maximal edge  vi, vhk is the longest substring having a copy at distance within Ik because, by Deﬁnition   in FS vi  denotes a longer substring 4.1 and Fact 3, any edge following  vi, vhk   , and thus must which must lie in a farther interval  by d-maximality of  vi, vhk have longer distance from T[i]. Once d-maximal edges are known, the computation of the ε-maximal edges is then easy because it sufﬁces to further decompose the edges between successive d-maximal edges, say between  vi, vhk−1+1  and  vi, vhk  ,   46  4 Bit-Complexity of Lempel-Ziv Compression  according to the distinct values assumed by the encoding function g on the lengths in the range [hk−1, . . . , hk − 1]. This takes O 1  time per ε-maximal edge, because it needs some algebraic calculations, and the corresponding copied substring can then be inferred as a preﬁx of T[i : hk − 1].  So, let us concentrate on the computation of d-maximal edges outgoing from vertex vi. We remark that we could use the solution proposed in Fiala and Greene  1989  on each of the Q f , n  ranges of distances in which a phrase copy can be found. Unfortunately, this approach would pay another multiplicative factor log σ per symbol and its space complexity would be super-linear in n. Conversely, our solution overcomes these drawbacks by deploying two key ideas:  1  The ﬁrst idea aims at achieving the optimal O n  working-space bound. It con- sists of proceeding in Q f , n  passes, one per interval Ik of possible d-costs for the edges in cid:4 G T  . During the kth pass, we logically partition the vertices of cid:4 G T   in blocks of Ik contiguous vertices, say vi, vi+1, . . . , vi+Ik−1, and compute all d-maximal edges which spread out from that block of vertices and have copy- distance within Ik  thus they all have the same d-cost, say c Ik  . These edges are kept in memory until they are used by our bit-optimal parser, and discarded as soon as the ﬁrst vertex of the next block, i.e. vi+Ik, needs to be processed. The next block of Ik vertices is then fetched and the process repeats. All passes are executed in parallel over the Ik in order to guarantee that all d-maximal edges of vi are available when processing this vertex. This means that, at any time, we have available d-maximal edges for all Q f , n  distinct blocks of vertices, one Ik = O n . block for each interval Ik. Thus, the space occupancy is Observe that there exist other choices for the size of the blocks that allow to retain O n  working space. However, our choice has the additional advantage of making it possible an efﬁcient computation of the d-maximal edges of vertices within each block.  2  The second key idea aims at computing the d-maximal edges for that block of Ik contiguous vertices in O Ik  time and space. This is what we address below, being the most sophisticated technicality of our solution. As a result, we show  n Ik O Ik  = O n Q f , n  , and that the time complexity of FSG is thus we will go to pay O 1  amortized time per d-maximal edge. Combining this fact with the previous observation on the computation of the ε-maximal edges, we get Theorem 4.3 above. Consider the kth pass of FSG in which we assume that Ik = [l, r]. Recall that all distances in Ik can be f -encoded in the same number of, say, c Ik  bits. Let B = [i, i + Ik − 1] be the block of  indices of  vertices for which we wish to compute on-the-ﬂy the d-maximal edges of cost c Ik . This means that the d-maximal edge from vertex vh, h ∈ B, represents a phrase that starts at T[h] and has a copy starting in the window  of indices  Wh = [h − r, h − l]. Thus, the distance of that copy can be f -encoded in c Ik  bits, and so we will say that the edge has d- cost c Ik . Since this computation must be done for all vertices in B, it is useful to consider the window WB = Wi∗Wi+Ik−1 which merges the ﬁrst and last window of   cid:5 Q f ,n  k=1   cid:5 Q f ,n  k=1  positions that can be the  copy- reference of any d-maximal edge outgoing from B.   4.4 On Bit-Optimal Parsings and Shortest-Path Problems  T  1  B  B  B  Wh  i − r  j − l  B  i  h  j  47  n  Fig. 4.1 Interval B = [i, j] with j = i + Ik − 1, window WB and its two halves W≥ W≥≥  = Wj. It is also shown the window Wh of position h ∈ B  B  B  = Wi and  Note that WB = 2Ik − 1  see Fig. 4.1  and it spans all positions where the copy of a d-maximal edge outgoing from a vertex in B can occur.  The next fact is crucial to fast compute all these d-maximal edges via an indexing  data structures built over T: Fact 4 If there exists a d-maximal edge outgoing from vh and having d-cost c Ik , then this edge can be found by determining a position s ∈ Wh whose sufﬁx Ts shares the longest common preﬁx  shortly, Lcp  with Th which is the maximum Lcp between Th and all other sufﬁxes in Wh. Proof 6 Let the edge  vh, vh+q+1  be a d-maximal edge of d-cost c Ik  . The reference copy of T[h, h + q] must start in Wh, having distance in Ik. Among all positions s in Wh take one whose sufﬁx Ts shares the Lcp with Th, clearly q √ Lcp. We have to show that q = Lcp, so this is maximal. Of course, there may exist many such positions, we take just one of them. By deﬁnition of d-maximality, any other edge longer edge  vh, vh+q≥≥  , with ≥≥ > q, represents a substring whose reference-copy occurs in a farther window q ≥ ∈ Wh must be the starting position of a before Wh in T. So any other position s  cid:2  reference-copy that is shorter than q.  Notice that a vertex vh may not have a d-maximal edge of d-cost c Ik . Indeed, sufﬁx Ts may share the maximum Lcp with sufﬁx Th in the window Wh, but a longer Lcp can be shared with a sufﬁx in a window closer to Th. Thus, from vh we can reach a farther vertex at a lower cost implying that vh has no d-maximal edge of cost c Ik . Hereafter we call the position s of Fact 4 maximal position for vertex vh whenever it induces a d-maximal edge for vh.  Our algorithm will compute the maximal positions of every vertex vh in B and every cost c Ik . If no maximal position does exist, vh will be assigned an arbitrary position. The net result is that we will generate a supergraph of  cid:4 G T   which is still guaranteed to have the size stated in Lemma 4.2 and can be created efﬁciently in O Ik  time and space, for each block of distances Ik, as we required above.  Fact 4 relates the computation of maximal positions for the vertices in B to Lcp- computations between sufﬁxes in B and sufﬁxes in WB. Therefore it is natural to resort to some string-matching data structure, like the compact trie TB, built over the sufﬁxes of T which start in the range of positions B ∗ WB. Compacted trie TB takes O B + WB  = O Ik  space  namely, proportional to the number of indexed   48  4 Bit-Complexity of Lempel-Ziv Compression  strings , and this bound is within our required space complexity. It is not easy to build TB in O Ik  time and space, because this time complexity is independent of the length of the indexed sufﬁxes and the alphabet size. The proof of this result may be of independent interest, and it is deferred to Sect. 4.4.3. The key idea is to exploit the fact that the algorithm deploying this trie  and that we detail below  does not make any assumptions on the edge-ordering of TB, because it just computes  sort of  Lca-queries on its structure.  So, let us assume that we are given the trie TB. We notice that maximal position s for a vertex vh in B having d-cost c Ik  can be computed by ﬁnding a leaf of TB which is labeled with an index s that belongs to the range Wh and shares the maximum Lcp with the sufﬁx spelled out by the leaf labeled h.10 This actually corresponds to ﬁnding a leaf whose label s belongs to the range Wh and has the deepest Lca with the leaf labeled h. We need to answer this query in O 1  amortized time per vertex vh, since we aim at achieving an O Ik  time complexity over all vertices in B. This is not easy because this is not the classic Lca-query since we do not know s, which is actually the position we are searching for. Furthermore, one could think to resort to proper predecessor successor queries on a suitable dynamic set of sufﬁxes in Wh. The idea is to consider the sufﬁxes of Wh and to identify the predecessor and the successor of h in the lexicographically order. The answer is the one among these two sufﬁxes that shares the longest common preﬁx with Th. Unfortunately, this would take ω 1  time per query because of well-known lower bounds  Beame and Fich 1999 . Therefore, in order to answer this query in constant  amortized  time per vertex of B, we deploy proper structural properties of the trie TB and the problem at hand.  Let u be the Lca of the leaves labeled h and s in TB. For simplicity, we assume that the window Wh strictly precedes B and that s is the unique maximal position for vh  our algorithm deals with these cases too, see the proof of Lemma 4.3 . We observe that h must be the smallest index that lies in B and labels a leaf descending ≥ < h does exist. from u in TB. In fact assume, by contradiction, that a smaller index h ≥ ∈ B and thus vh would not have a d-maximal edge of d-cost c Ik  By deﬁnition h a possibly longer phrase, instead of copying because it could copy from the closer h from the farther set of positions in Wh. This observation implies that we have to search only for one maximal position for each node u of TB. For each node u, we denote by a u  the smallest position belonging to B and labeling a leaf in the subtree rooted at u. Value of a u  is undeﬁned  ⊥  whenever such a smallest position does not exist. Computing the value a    for all nodes u in TB takes O TB  = O Ik  time and space via a traversal of the trie TB. By discussion above, it follows that, for each node u, vertex va u  is exactly the solely vertex for which we have to identify its maximal position. Now we need to compute the maximal position for va u , for each node u ∈ TB. We cannot traverse the subtree of u searching for the maximal position for va u , because this would take quadratic time complexity overall. Conversely, we deﬁne  ≥  10 Observe that there may be several leaves having these characteristics. We can arbitrarily choose one of them because they denote copies of the same phrase that can be encoded with the same number of bits for the length and for the distance  i.e., c Ik   bits .   49  This suggests to compute for each node u the rightmost position in W≥  B and its right extreme in W≥≥ B. If s does exist for va u , then s belongs to either W≥ B  resp. W≥≥  4.4 On Bit-Optimal Parsings and Shortest-Path Problems W≥ B and W≥≥ B to be the ﬁrst and the second half of WB, respectively, and observe that any window Wh has its left extreme in W≥ B  see Fig. 4.1 . Therefore the window Wa u  containing the maximal position s for va u  B and W≥≥ overlaps both W≥ B or to W≥≥ B, and the leaf labeled s descends from u. Hence the maximum  resp. minimum  among the positions in W≥ B  that label leaves descending from u must belong to Wa u . B and the leftmost position in W≥≥ B that label a leaf descending from u, denoted respectively by max u  and min u . This computation takes O Ik  time with a post-order traversal of TB. We can now efﬁciently compute mp[h] as a maximal position for vh, if it exists, or otherwise set mp[h] arbitrarily. We initially set all mp’s entries to nil; then we visit TB in post-order and perform, at each node u, the following two checks whenever mp[a u ] = nil: If min u  ∈ Wa u , we set mp[a u ] = min u ; if max u  ∈ Wa u , we set mp[a u ] = max u .11 We ﬁnally check if mp[a u ] is still nil and we set mp[a u ] = a parent u   whenever a u   cid:12 = a parent u  . This last check is needed  see proof of Lemma 4.3  to manage the case in which we can copy the phrase starting at position a u  from position a parent u   and, additionally, we have that B overlaps WB  which may occur depending on f  . Since TB has size O Ik , the overall algorithm requires O Ik  time and space in the worst case, and hence Theorem 4.3 follows. Figure 4.2 shows a running example of our algorithm in which Ik = 4, B = [8, 11] and the trie TB is the one shown  it is an imaginary trie, just for illustrative B = [4, 7]. Notice that some a u -values are ⊥ purposes . It is W≥ because the leaves descending from that node are labeled with positions which lie  B = [1, 4] and W≥≥  Fig. 4.2 The picture shows a running example for a n imaginary  trie TB and our algorithm. We are assuming that Ik = [4, 7] and B = [8, 11]. Thus, Ik = 4, W≥ B = [1, 4] and W≥≥ B = [4, 7]. For each node of the tree, we report the values of a  , max   and min    a = 8 = 3 =  3  8  a = 8 = 4 = 4  a = 9 = 4 = 4  a = 10 = 1 = 6  a =  = 4 = 4  9  a = 11 = 1 =  a = 10  = = 6  2  4  5  1  11  6  7  10  11 The value of mp[a u ] can be arbitrarily set to min u  or max u  whenever both min u  and max u  belong to Wa u .   50  4 Bit-Complexity of Lempel-Ziv Compression  ≥ B  outside B; otherwise a u  is the smallest position descending from u and belonging to B. Now take some node u, say the rightmost one at the last level of the trie, it has a u  = 10 and its reference window W10 = [10 − 7, 10 − 4] = [3, 6] for which the copies have cost c Ik . We can compute min u  = 6 because it is the minimum of = [4, 7]. Instead, we ≥≥ the leaves descending from u and whose value belongs to W = [1, 4]. In this case, mp[10] have max u  = ⊥ because no leaf-value belongs W B is correctly set to 6 which is the maximal position for v10. The other results of our algorithm are mp[8] = 3, mp[9] = 4 and mp[11] = 10.12 Observe that 3 and 4 are indeed maximal positions for, respectively, v8 and v9. Moreover, we notice that v9 has three possible maximal positions  namely, 2, 4 and 5 . Choosing arbitrarily one of them does not change neither the length of the copied substring nor the cost of its encoding. We ﬁnally notice that v11 has no maximal position with cost c Ik , in fact values of min and max on the node with a = 11 are not in W11. Observe that we can indeed copy the same substring from positions 7 and 10, the latter copy is less expensive since it is closer to position 11. Lemma 4.3 For each position h ∈ B, if there exists a d-maximal edge outgoing from vh and having d-cost c Ik , then mp[h] is equal to its maximal position. Proof 7 Take B = [i, i + Ik − 1] and consider the longest path π = u1, u2 . . . uz in TB that starts from the leaf u1 labeled with h ∈ B and goes upward until the traversed nodes satisfy the condition a uj  = h, here j = 1, . . . , z. By deﬁnition of a-value  see above , we know that all leaves descending from uz and occurring in B are labeled with an index which is larger than h. Clearly, if parent uz  does exist, then it is a parent uz   < h. There are two cases for the ﬁnal value stored in mp[h]. Case 1. Suppose that mp[h] ∈ Wh. We want to prove that mp[h] is the index of the leaf which has the deepest Lca with h among all the other leaves labeled with an index in Wh  hence it has maximal length . Let ux ∈ π be the node in which the value of mp[h] is assigned, so a ux  = h and the ancestors of ux on π will not change mp[h] because the algorithm will ﬁnd mp[h]  cid:12 = nil. Assume now that there exists at least another position in Wh whose leaf has a deeper Lca with leaf h. This Lca must lie on u1 . . . ux−1, say ul. Since Wh is a window having its left extreme in W≥ and its right extreme in W≥≥ B B, the value max ul  or min ul  must lie in Wh and thus the algorithm has set mp[h] to one of these positions, because of the post-order visit of TB and the check on mp[a ul ] = nil. Therefore mp[h] must be the index of the leaf having the deepest Lca with h, and thus by Fact 4 it is its maximal position. Case 2. Suppose that mp[h]  ∈ Wh and, thus, it cannot be a maximal position for vh. We have to prove that it does not exist a d-maximal edge outgoing from the vertex vh with cost c Ik . Let Ts be the sufﬁx in Wh having the maximum Lcp with Th, and let l be the Lcp-length. Values min ui  and max ui  do not belong to Wh, for any node ui ∈ π with a ui  = h, otherwise mp[h] would have been assigned with an index in Wh  contradicting the hypothesis . Thus the value of mp[h] remains nil up to node uz where it is set to a parent uz  . This implies that no sufﬁx descending  12 Observe that we obtain mp[11] = 10 by setting mp[a u ] = a parent u   at the node with a = 11.   4.4 On Bit-Optimal Parsings and Shortest-Path Problems  51  from uz starts in Wh and, in particular, Ts does not descend from uz. Therefore, the Lca between leaves h and s is a node in the path from parent uz  to the root of TB, and, as a result, it is Lcp Ta parent uz  , Th  ∼ Lcp Ts, Th  = l. By observing that a parent uz   < a uz  = h, a parent uz   belongs to B  both by deﬁnition of a-value , and mp[h] = a parent uz    ∈ Wh  by assumption , it follows that we can copy from position a parent uz   at a cost smaller than c Ik  a substring longer than the one we can copy from s. So we found an edge from vh with smaller d-cost and at least the same length. This way vh has no d-maximal edge of cost c Ik  in  cid:4 G T  .  cid:2   4.4.3 On the Optimal Construction of TB In the discussion above we left out the explanation on how to build TB in O Ik  time and space, thus within a time complexity which is independent of the length of the Ik indexed sufﬁxes and the alphabet size σ. To achieve this result we deploy the crucial fact that the algorithm of the previous section does not make any assumption on the ordering of the children of TB’s nodes, because it just computes  sort of  Lca-queries on its structure.  First of all, we build the sufﬁx array of the whole string T and a data structure that answers constant-time Lcp-queries between pair of sufﬁxes [see e.g. Puglisi et al.  2007 ]. This takes O n  time and space. Let us ﬁrst assume that B and WB are contiguous and form the range[i, i+3Ik−1]. If we had the sorted sequence of sufﬁxes starting in T[i, i + 3Ik − 1], we could easily build TB in O Ik  time and space by deploying the above Lcp-data structure. Unfortunately, it is unclear how to obtain from the sufﬁx array of the whole T, the sorted sub-sequence of sufﬁxes starting in the range [i, i + 3Ik − 1] by taking O B + WB  = O Ik  time  notice that these sufﬁxes have length σ n − i  . We cannot perform a sequence of predecessor successor queries because they would take ω 1  time each  Beame and Fich 1999 . Conversely, we resort the key observation above that TB does not need to be ordered, and thus devise a solution which builds an unordered TB in O Ik  time and space, passing through the construction of the sufﬁx array of a transformed string. The transformation is simple. We ﬁrst map the distinct symbols of T[i, i+ 3Ik− 1] to the ﬁrst O Ik  integers. This mapping does not need to reﬂect their lexicographic order, and thus can be computed in O Ik  time by a simple scan of those symbols and the use of a table T of size σ. Then, we deﬁne ˆT as the string T which has been transformed by re-mapping symbols according to table T  namely, those occurring in S[i, i + 3Ik − 1] . We can prove the following Lemma. Lemma 4.4 Let Ti, . . . , Tj be a contiguous sequence of sufﬁxes in T. The re-mapped sufﬁxes ˆTi . . . ˆTj can be lexicographically sorted in O j − i + 1  time.   52  4 Bit-Complexity of Lempel-Ziv Compression  component-wise, and we assume that $ is a special “pair” larger than any other pair  Proof 8 Consider the string of pairs w = ≤ˆT[i], bi∈ . . .≤ˆT[j], bj∈ $ , where bh is 1 if ˆTh+1 > ˆTj+1, −1 if ˆTh+1 < ˆTj+1, or 0 if h = j. The ordering of the pairs is deﬁned in w. For any pair of indices p, q ∈ [1 . . . j−i], it is ˆTp+i > ˆTq+i iff wp > wq and thus comparing the corresponding sufﬁxes of w. In fact, suppose that wp > wq and set r = Lcp wp, wq . We have that w[p + r] = ≤ˆT[p + i + r], bp+i+r∈ > ≤ˆT[q + i+ r], bq+i+r∈ = w[q+ r]. Hence we have that either ˆT[p+ i+ r] > ˆT[q+ i+ r] or bp+i+r > bq+i+r, which means bp+i+r = 1 and bq+i+r = 0. The latter actually means that ˆTp+i+r+1 > ˆTj+1 ∼ ˆTq+i+r+1. In any case, it follows that ˆTp+i+r > ˆTq+i+r and thus ˆTp+i > ˆTq+i, since their ﬁrst r symbols are equal. This implies that sorting the sufﬁxes ˆTi, . . . , ˆTj reduces to computing the sufﬁx array of w, and this takes O w  time given that the alphabet size is O w   Puglisi et al. 2007 . Clearly, w can be constructed in that time bound because comparing ˆTz with ˆTj+1 takes O 1  time via an Lcp-query on T  using the proper data structure  cid:2  Lemma 4.4 allows us to generate the compact trie of ˆTi, . . . , ˆTi+3Ik−1, which is equal to the  unordered  compacted trie of Ti, . . . , Ti+3Ik−1 after replacing every ID assigned by table T with its original symbol in T. We ﬁnally notice that if B and WB are not contiguous  as instead we assumed above , we can use a similar strategy to sort separately the sufﬁxes in B and the sufﬁxes in WB, and then merge these two sequences together by deploying the Lcp-data structure mentioned at the beginning of this section.  above  and a check at their ﬁrst mismatch.  4.5 An Experimental Support to our Theoretical Findings  In this section we provide an experimental support to our ﬁndings of Sect. 4.3, and compare our proposals of Sects. 4.2 and 4.4 with some state-of-the-art compressors over few freely available text collections. Table 4.1 reports our experimental results. Let us ﬁrst consider algorithm Fixed-LZ77, which uses an unbounded window and equal-length encoders for the distance of the copied phrases. Its compression performance shows that an unbounded window may introduce a signiﬁcant compres- sion gain wrt to a bounded one, as used by gzip and bzip2  see e.g. HTML , thus witnessing the presence in current  Web text  collections of surprisingly many long repetitions at large distances. This motivates our main study for M = n, even if our results extend to the bounded-window case too.  Then consider Rightmost-LZ77  Sect. 4.2 , which uses an unbounded win- dow and selects the rightmost copy of the currently longest phrase. As expected, this parsing combined with the use of variable-length integer encoders improves Fixed-LZ77, thus sustaining in practice the starting point of our theoretical inves- tigation.   37.52 28.40 20.62 19.95 26.19 25.02 22.11 HTML  Boldi et al. 2004   compression %  11.99 8.10 4.45 4.62 6.69 6.16 5.68  4.5 An Experimental Support to our Theoretical Findings  Table 4.1 Each text collection consists of 50 Mbytes of data Compressor  English  Ferragina et al. 2008a   %   gzip -9 bzip2 -9 boosterOpt lzma2 -9 Fixed-LZ77 Rightmost-LZ77 BitOptimal-LZ77 Compressor  53  src al.  C C++ Java  Ferragina et 2008a   %  23.29 19.78 17.36 16.06 24.63 21.21 18.97 Avg Dec. time  sec   gzip -9 bzip2 -9 boosterOpt lzma2 -9 Fixed-LZ77 Rightmost-LZ77 BitOptimal-LZ77 All the experiments were executed on a 2.6 GHz Pentium 4, with 1.5 GB of main memory, and running Fedora Linux  0.7 6.3 20.2 2.1 0.8 0.9 0.9  Finally, we tested our bit-optimal compressor BitOptimal-LZ7713 ﬁnding that it improves Rightmost-LZ77, as theoretically predicted in Lemma 4.1. Surprisingly, BitOptimal-LZ77 signiﬁcantly improves bzip2  which uses a bounded window  and comes close to the booster tool [which uses an unbounded window  Ferragina et al. 2005a ] and lzma214  which is an LZ77-based compressor using a sophisticated encoding algorithm . Additionally, since BitOptimal-LZ77 adopts the same decompression algorithm of gzip, it retains its fast decompression speed which is at least one order of magnitude faster than decompressing bzip2’s or booster’s compressed ﬁles, and three times faster than lzma2. This is a nice com- bination which makes BitOptimal-LZ77 practically relevant for a wide range of applications in which the paradigm is “compress once and decompress many times”  like in Web search engines and IR systems , or where the decompression system is less powerful than the compressor one  like a server that distributes data to clients, possibly mobile phones .  As far as construction time is concerned, BitOptimal-LZ77 is slower than other LZ77-based compressors by factors that range from 2  w.r.t. lzma2  to 20  w.r.t. gzip . Our preliminary implementation does not follow the computation of  13 Algorithms Rightmost-LZ77 and BitOptimal-LZ77 encode copy-distances and lengths by using a variant of Rice codes in which we have not just one bucketing of size 2k, rather we have a series of buckets of increasing size, ﬁxed in advance. 14 Lzma2 home page http:  7-zip.org .   54  4 Bit-Complexity of Lempel-Ziv Compression  maximal edges as described in Sect 4.4.2. Instead, we use the less efﬁcient solution mentioned in the previous section which resorts to binary balanced search trees to solve predecessor successor queries on a suitable dynamic set of sufﬁxes in Wh. This approach gives an easier solution to implement which, however, loses a factor log n in time. We believe that a more engineered implementation could sensibly reduce the construction time of BitOptimal-LZ77.   Chapter 5 Fast Random Access on Compressed Data  Starting from Ferragina and Manzini  2005  and  Grossi et al.  2003 ; Sadakane  2003  , the design of compressed  self indexes for strings became an active ﬁeld of research  some of these results will be presented in Chaps. 6 and 7 . The key problem addressed in these papers consists of representing a string T[1, n] within compressed space, and still be able to solve the Text Searching Problem in efﬁcient time, without incurring in the whole decompression of the compressed data. In these results, compressed space usually means space close to the k-th order empirical entropy of T , and efﬁcient time means something depending on the length of the searched string.  logσ n  Recently, Sadakane and Grossi  2006  addressed the foundational problem of designing a compressed storage scheme for a string T in which the query operation to be supported is the retrieval of any ε-long substring of T in optimal O 1+ ε   time. The previous known solutions  Navarro and Mäkinen  2007   were based on com- pressed indexes. The main drawback of these solutions is given by the fact that they incur in an additional sub-logarithmic time overhead. Instead, the Sadakane-Grossi’s storage scheme achieves the optimal time bound and occupies a number of bits upper bounded by the following function1: n Hk  T  + O    k + 1  log σ + log log n , where, as usual, Hk  T   is the k-th order entropy of string T  recall Deﬁnition 2.3 . This storage scheme is based on a sophisticated combination of various tech- niques: Ziv-Lempel’s string encoding  Ziv and Lempel  1978  , succinct dictionaries  Raman et al.  2007  , and some novel succinct data structures for supporting navi- gation and path-decoding in Lz-tries. Since storing T by means of a plain array of symbols takes σ n log σ   bits, the scheme in Sadakane and Grossi  2006  is effective when k = o logσ n .  logσ n  n  González and Navarro  2006  proposed a simpler storage scheme achieving the  same query time and a slightly improved space bound:  1 As stated in González and Navarro  2006 , the term  k + 1  log σ appears erroneously as k in Sadakane and Grossi  2006 . We therefore use the correct bound in this chapter.  R. Venturini, Compressed Data Structures for Strings, Atlantis Studies in Computing 4, DOI: 10.2991 978-94-6239-033-1_5,   Atlantis Press and the authors 2014  55   56  5 Fast Random Access on Compressed Data  n Hk  T   + O   n   k log σ + log log n    logσ n   5.1   This storage scheme exploits a statistical encoder  namely, Arithmetic  on most of T ’s substrings but, unlike  Sadakane and Grossi  2006  , requires to ﬁx the order k of the entropy bound in advance.  In what follows we propose a very simple storage scheme that:  1  drops the use of any compressor  either statistical or Lz- like , and deploys only binary encodings and tables;  2  matches the space bound of Eq.  5.1  simultaneously over all k = o logσ n . We then exploit this storage scheme to achieve two corollary results. The ﬁrst one provides a novel bound in terms of Hk  T   on the compression ratio achievable by any 0-th order compressor applied on blocks of l contiguous symbols of T , with k ≤ l  see Theorem 5.3 . The second result shows that our storage scheme can be used upon the string Bwt T   in order to achieve an interesting compressed- space bound which depends on the k-th order entropy of both the strings T and Bwt T    see Theorem 5.4 .  Recently, Theorem 5.3 has been used in Golynski et al.  2008  to design a scheme which achieves the same query time and has a slightly improved space bound. Essentially, they use our construction combined with recent techniques in a way logσ n log log n  in Eq.  5.1 , so that its space that allows them to remove the term O  bound becomes n Hk  T   + O  logσ n k log σ   bits. This implies that their solution is better than ours only for very small value of k  namely, k = o  log log n    .  n  n  log σ  5.1 Our Storage Scheme for Strings Let T[1, n] be a string drawn from an alphabet λ, and assume that n is a multiple of 2 logσ n√. If this is not the case, we append to T the missing symbols taking b = ∈ 1 them as the special null symbol.2 We partition T into blocks T i of size b each. Let S be the set of distinct blocks of T . The number of all blocks is n b ; the number of distinct blocks is S = O σ b  = O n1 2 . We deﬁne B = [ε, 0, 1, 00, 01, 10, 11, 000, . . .] to denote the inﬁnite sequence of binary strings ordered ﬁrst by length and then lexicographically by their content, with ε denotes the empty string. The encoding scheme. We sort the elements of S per decreasing frequency of occurrence in T ’s partition. Let r  T i   be the rank of the block T i in this ordering, −1  j   be its inverse function  namely, the one that returns the block having and let r the given rank j . The storage scheme for T consists of the following information.   Each block T i is assigned a codeword enc i   consisting of the binary string that has rank r  T i   in B. It is simple to see that enc i   ≤ log i ≤ 1 2 log n. Of course,  2 This will add to the entropy estimation a negligible additive term equal to O log σ logσ n  = O log n  bits.   5.1 Our Storage Scheme for Strings  57  b  b  bc   .  logσ n  enc i   is not a uniquely decodable code, but the additional tables built below will allow us to decode it in constant time and within a space bounded by Eq.  5.1 .   We build a bit sequence V obtained by juxtaposing the binary encodings of all T ’s blocks in the order of their appearance in T . Namely V = enc 1 ··· enc  n   We store r −1 as a table of O σ b  entries, taking O σ b log n  = o n  bits.   To guarantee constant-time access to the encodings of T ’s blocks and to ensure their decodings, we use a two-level storage scheme for the starting positions of encs  see Munro  1996  . Speciﬁcally, we logically group every c = σ log n  contiguous blocks into one superblock, having thus size bc log σ = σ log2 n  bits. Table ] stores the starting position of the encoding of every super-block in V , DSblk[1, n and table Dblk[1, n ] stores the starting position in V of the encoding of every block relative to the beginning of its enclosing super-block. Note that the starting position b log n  = O n log σ  , whereas the of each super-block is no more than V = O  n relative position of each block within its super-block is O log2 n . Consequently, bc logV + n b log log n  = O  n log log n tables DSblk and Dblk occupy O  n   bits overall, and guarantee a constant-time access to every codeword enc i   and its length.3 Theorem 5.1 Our storage scheme encodes T[1, n] inV+ O  n log log n is upper bounded by Eq.  5.1 , simultaneously over all k = o logσ n . Proof For every position i, k ≤ i ≤ n, let us denote by fi denotes the empirical probability of seeing T[i] after the k-order context T[i − k, i − 1]. According to Deﬁnition 2.3, this can be rephrased by saying that fi is the frequency of occurrence of symbol T[i] within uT , where u = T[i − k, i − 1]. It is easy to see that a  semi- static  k-order modeler can compute all the frequencies fi via two passes over T , hence in O n  time. Arithmetic encoding is the most effective statistical encoder  Witten et al.  1999  . Given the fi s, it represents the string T with a range of size F = f1 × f2 ×···× fn. It is well known  Witten et al.  1999   that 2+ log 1 F   = 2+  cid:2  n i=1 log 1  fi   bits  cid:2  are enough to distinguish a number within that range. The binary representation of n i=k+1 log 1  fi  , this number is the Arithmetic compression of T . If we compute and then group all the terms referring to the same k-th order context, we obtain a summation upper bounded by n Hk  T    see Eq.  2.3  . Additionally, since fi ≥ 1 n,  cid:2  i=1 log 1  fi   = O k log n . As a result, a  semi-static  k-th order we have that Arithmetic encoder compresses the whole T within n Hk  T   + 2 + O k log n  bits. Let us introduce a compressor E that encodes each block T i of T individually: the ﬁrst k symbols of T i are represented explicitly, the remaining b−k symbols of T i are compressed via the above k-order Arithmetic encoder  hence using their k-th order frequencies f s . It is easy to observe that the codeword so assigned to T i uniquely identiﬁes it. This blocking approach increases the above Arithmetic encoding of the whole T by O  n b k log σ   bits, which accounts for the cost of explicitly storing the ﬁrst k symbols of each T i .    bits, which  logσ n  k  3 It sufﬁces to compute the starting position of enc i   and enc i + 1 , if any.   58  5 Fast Random Access on Compressed Data  To show that the string V produced by our storage scheme enc is shorter than the compressed string produced by E, it sufﬁces to note that the codewords assigned by E are a subset of B, whereas the codewords assigned by enc are the ﬁrst S binary strings of B. Given that B is the set of the shortest codewords assignable to S’s strings, our encoding enc is better than E because it follows the golden rule of data compression: it assigns shorter codewords to more frequent symbols. Summing up the cost of the block’s encodings and the space occupancy of the decoding table, we get the space bound of Eq.  5.1 , whenever k = o logσ n  and independently of  cid:2  it.  We now show how to decode in constant time a generic block T k. This will be enough to prove the result for any ε-long substring of T . We ﬁrst derive the starting position p k  of the string enc k  that encodes T k in V . Namely, we compute the super-block number h = ∼k c⊆ containing enc k , and its starting bit-position y = DSblk[h] within V . Then, we compute x = Dblk[k] as the relative bit-position of enc k  within its enclosing super-block. Thus p k  = x + y. Similarly, we derive the starting position p k + 1  of enc k + 1  in V  if any, otherwise we set p k + 1  = V+ 1 . We can thus fetch enc k  = V[ p k , p k + 1 − 1] in constant time since enc k  = p k + 1  − p k  = O log n  bits. We ﬁnally decode enc k  as follows. Let v be the integer value represented by the binary string enc k , where v = 0 if enc k  = ε. Because of the canonical ordering of S, T k is computed as the block having rank z = 2 −1 z . Theorem 5.2 Our storage scheme retrieves any substring of T of length ε in optimal O 1 + ε  enc k +v. That is, T k = r    time.  logσ n  Proof The algorithm described above allows to retrieve any block T k in constant time. The theorem follows by observing that any ε-long substring T[ j, j + l − 1] spans O 1 + l  cid:2     blocks of T .  logσ n  5.2 Huffman on Blocks of Symbols  It is well-known that the Huffman compressor cannot represent a symbol with less than one bit. To circumvent this, the string T is usually partitioned into n l blocks of length l each, and then Huffman is applied onto the alphabet λl of these new symbols, i.e. l-long blocks. This blocking strategy spreads the per-symbol inefﬁciency over the entire block, thus reducing it to 1 l bits. It is natural to ask what is the compression ratio of this block-Huffman algorithm. The following theorem bounds the 0-th order entropy of Tl by the k-th order empirical entropy of original string T .   5.2 Huffman on Blocks of Symbols  59  Theorem 5.3 H0 Tl   ≤ l Hk  T   + O k log σ  , simultaneously over all k ≤ l. Proof Consider the compressor E in the proof of Theorem 5.1. E does not depend on the size of the blocks in which T has been decomposed. Hence, we can set b = l, apply E onto the blocked T and thus assign a distinct preﬁx-free codeword to each distinct block in S  i.e. symbol of λl . As seen in that proof, the space necessary to represent the whole Tl is bounded by n Hk  T   + O  n l k log σ   bits. The stated theorem follows by the classical Information-Theory lower bound, since every preﬁx-free encoder needs on Tl at least TlH0 Tl   = n  cid:2  We note that the case k = 0 of Theorem 5.3 has been proved in Sadakane  2003 . Given Theorem 5.3, the output produced by Huffman over Tl is bounded l H0 Tl   + n by n  ≤ n Hk  T   + O  n   k log σ + 1  .  l H0 Tl   bits.  l  l  5.3 BWT Compression and Access  logσ n    time.  In this section we show that our storage scheme can be used upon the string Bwt T   in order to achieve an interesting compressed-space bound which depends on both Hk  T   and Hk  Bwt T   . The relation between these two entropies will be com- mented below. Theorem 5.4 Our storage scheme applied on the string L = Bwt T   takes no more than min{n Hk  L , n Hk  T  }+o n log σ   bits, simultaneously over all k = o logσ n . Any ε-long substring of L can be retrieved in optimal O 1 + ε Proof Let Ck   j   be the k-long preﬁx of the j-th row of MT . By the properties of Bwt T    see Sect. 2.4.2 , Ck   j   follows L[ j] in T and thus Ck   j   is the following k-long context of L[ j]. The Deﬁnition 2.3 of Hk  T   can changed by replacing the notion of preceding k-long contexts with the one of following k-long contexts. The difference between these two quantities results negligible Ferragina and Manzini  2005 . Therefore, to ease our discussion we consider the following k-long contexts and work with Hk  T   as it were deﬁned over them. We partition L into substrings of length b = ∈ 1  2 logσ n√, say L1L2 . . . Ln b  called hereafter blocks . Note that each block Li corresponds to a range of b rows in the BWT-matrix MT . We say that the block Li is k-preﬁx-equal if all rows in MT[ i − 1 b + 1, ib] have the same preﬁx of length k. Otherwise, Li is said to be k-preﬁx- different. The total number of k-preﬁx-different blocks is O σ k  , because that is the number of distinct strings of length k over λ. Moreover, we note that the ﬁrst k symbols of T belong to k-preﬁx-different blocks because of the special symbol $.  To prove the Theorem we consider a preliminary encoding scheme for L’s blocks. A k-preﬁx-different block Li is written without any compression as k occurrences of a special null symbol plus Li itself. This takes O  b + k  log σ   = O log n + k log σ   bits. Since there are O σ k   k-preﬁx-different blocks and we assumed k = o logσ n , the  plain  encoding of the k-preﬁx-different blocks takes O σ k log n  = o n  bits. As far as k-preﬁx-equal blocks are concerned, we encode a block Li as follows: we write explicitly the k-long following context shared by all Li ’s symbols, using   60  5 Fast Random Access on Compressed Data  k log σ bits; and then use a k-th order Arithmetic encoder on the individual symbols of Li . This encoder computes for any symbol L[ j] the empirical probability f j of seeing this symbol followed by the context Ck   j   in T . In the proof of Theorem 5.1, we considered f as the preceding contexts and showed that a  semi-static  k-th order Arithmetic encoder that uses the f s, compresses T within Hk  T  +  n b k log σ  + O k log n + 2 bits. Here we are compressing L, which is a permutation of string T , and we are considering the following contexts of T ’s symbols. Given our comment above on the deﬁnition of Hk  T  , we can conclude that this bound still holds for the Arithmetic encoder applied on L. Summing up, the space required by this encoding scheme over all blocks of L is n Hk  T   + O  n b k log σ   bits.  Let us now take our storage scheme enc of Sect. 5.1, apply it onto string L, and compare the length of the resulting compressed string against the previous encoding. By Theorem 5.1, we know that enc encodes L within n Hk  L  + O  n b k log σ   bits, and this proves one part of the theorem. As far as the other term Hk  T   is concerned, we observe that any block Li may occur many times in the partition of L and each occurrence may have associated a different k-long following context. As a result, the above scheme encodes all occurrences of Li with at most O σ k   different codewords, because it has at most σ k distinct k-contexts  as a k-preﬁx- equal block  and at most σ k plain encodings  as a k-preﬁx-different block . If we re-assign to all these codewords the shortest one, we have that each distinct block of L gets one codeword in B. Following an argument similar to the one used in the proof of Theorem 5.1, this encoding of L’s blocks is worse than enc because this latter assigns the shortest codewords of B to the distinct blocks of L. Therefore enc takes no more than n Hk  T   + O  n b k log σ   bits.  cid:2  The relation between Hk  T   and Hk  L  is not fully known. In González and Navarro  2006  they proved that H1 L  ≤ 1 + Hk  T   log σ + o 1  for any k <  1− ε  logσ n and any constant 0 < ε < 1. Actually the gap may be quite large. For example, let us consider the string T =  bba m and set k = 1. By Eq. 2.3 we have  n H1 T   =  m − 1 H0 bm−1  + 2m H0  ba m   = 2m = 2 3 n  On the other hand, since L = Bwt T   = b2mam, we have  n H1 L  =  m − 1 H0 am−1  + 2m H0 b2m−1a  = − 2m − 1  log 2m−1 = 2m log 2m −  2m − 1  log 2m − 1  = O log n   − log 1  2m  2m  which is exponentially smaller than n H1 T  , for any m > 1. On the other side, we show an example in which n H1 L  > n H1 T  . Let T =  a1a2 . . . am  m and k = 1. We have n H1 T   = 0. Since L = Bwt T   = am n H1 L  = − m − 1   m − 1  log m−1  m−1, we have   = σ   m am 1 + log 1  ⇒ n log  . . . am  ⇒  n   m  m   Chapter 6 Experiments on Compressed Full-Text Indexing  Most of the manipulations required over texts involve, sooner or later, searching those  usually long  sequences for  usually short  pattern sequences. Not surprisingly, text searching and processing has been a central issue in Computer Science research since its beginnings.  Despite the increase in processing speeds, sequential text searching long ago ceased to be a viable alternative for many applications, and indexed text searching has become mandatory. A text index is a data structure built over a text which sig- niﬁcantly speeds up searches for arbitrary patterns, at the cost of some additional space. The inverted list structure  see e.g. Witten et al. 1999  is an extremely popular index to handle so-called “natural language” texts, due to its simplicity, low space requirements, and fast query execution. An inverted list is essentially a table record- ing the positions of the occurrences of every distinct word in the indexed text. Thus every word-based query is already pre-computed, and phrase queries are carried out via list intersections. These design features made inverted lists the de facto choice for the implementation of Web search engines and IR systems  see e.g. Witten et al. 1999; Zobel and Moffat 2006, and references therein .  There are contexts, however, in which either the texts or the queries cannot be factored out into sequences of words, or it is the case that the number of distinct words is so large that indexing all of them in a table would be too much space con- suming. Typical examples include bio-informatics, computational linguistics, mul- timedia databases, and search engines for agglutinating and Far East languages. In these cases texts and queries must be modeled as arbitrarily long sequences of sym- bols and an index for these types of texts, in order to be efﬁcient, must be able to search and retrieve any substring of any length. These are nowadays the so called full-text indexes.  As we altready discussed in Sect. 2.3, classical full-text indexes wasted a lot of space: Data structures like sufﬁx trees and sufﬁx arrays require at the very least four times the text size  plus text  to achieve reasonable efﬁciency  Aluru and Ko 2008; Gusﬁeld 1997 . Several engineered versions achieved relevant, yet not spectacular, reductions in space  Andersson and Nilsson 1995; Kärkkäinen 1995; Kärkkäinen  R. Venturini, Compressed Data Structures for Strings, Atlantis Studies in Computing 4, DOI: 10.2991 978-94-6239-033-1_6,   Atlantis Press and the authors 2014  61   62  6 Experiments on Compressed Full-Text Indexing  and Ukkonen 1996a, b; Giegerich et al. 2003 . For example, space consumptions like 2.5 times the text size, plus text, were reported  see survey by Navarro et al. 2001 .  Although space consumption by itself is not usually a problem today given the availability of cheap massive storage, the access speed of that storage has not improved much, while CPU speeds have been doubling every 24 months, as well the sizes of the various  internal  memory levels. Given that nowadays an access to the disk can be up to one million times slower than main memory, it is often mandatory to ﬁt the index in internal memory and leave as few data as possible onto disk.  A folklore alternative way to further reduce the space of full-text indexes are the so-called q-gram indexes  Ullman 1977; Jokinen and Ukkonen 1991; Sutinen and Tarhio 1996; Lehtinen et al. 1996; Navarro and Baeza-Yates 1998; Williams and Zobel 2002; Puglisi et al. 2006   more references in Navarro et al.  2001  . This can be seen as an adaptation of the inverted-list scheme that takes as a “word” any q-gram occurring in the indexed text  i.e., any substring of length q , and stores all occur- rences of these q-grams within a table. Queries are solved by intersecting joining lists of q-gram occurrences depending on whether the query pattern is longer shorter than q. In principle this index can take as much as four times the text size, as each text position starts a q-gram and thus spends one integer in some list. The space can be alleviated by several means, already explored in the cited papers:  1  compressing the inverted lists, since they contain increasing numbers, as done for classical inverted indexes  Witten et al. 1999 ;  2  indexing spaced text q-grams, while still ﬁnding all the occurrences, e.g. Sutinen and Tarhio  1996 ;  3  using block-addressing, ﬁrst introduced to reduce the space of natural-language indexes  Manber and Wu 1994  and later extended to q-gram indexes, e.g. Lehtinen et al.  1996 , Puglisi et al.  2006 . In block-addressing the text is divided into blocks and the index only points to the blocks where the q-grams appear. This permits achieving very little index space at the price of having to scan the candidate text blocks. Hence the text must be separately available in a form that permits fast scanning. Block-addressing has been successfully combined with the compression of the text in the case of natural language  Navarro et al. 2000 , but not in general text as far as we know. Besides needing the text in plain form, these indexes do not offer relevant worst-case search time guarantees.  This situation is drastically changed in the last decade  Navarro and Mäkinen 2007 . Starting in the year 2000, a rapid sequence of achievements showed how to relate information theory with string matching concepts. The regularities in com- pressible texts were exploited to reduce index occupancy without impairing the query efﬁciency. The overall result has been the design of full-text indexes whose size is proportional to that of the compressed text. Moreover, those indexes are able to reproduce any text portion without accessing the original text, and thus they replace the text—hence the name self-indexes. This way compressed full-text self-indexes  compressed indexes, for short  allow one to add search and random access function- alities to compressed data with a negligible penalty in time and space performance. For example, it is feasible today to index the 3 GB Human genome on a 1 GB RAM desktop PC.   6 Experiments on Compressed Full-Text Indexing  63  The content of this chapter is primarly devoted to a practical study of this novel technology. Although a comprehensive survey of theoretical aspects has recently appeared  Navarro and Mäkinen 2007 , the algorithmics underlying these com- pressed indexes require for their implementation a signiﬁcant programming skill, a deep engineering effort, and a strong algorithmic background. To date only iso- lated implementations and focused comparisons of compressed indexes have been reported, and they missed a common API, which prevented their re-use or deploy within other applications. The present work has therefore a threefold purpose: Algorithmic Engineering. We review the most successful compressed indexes that have been implemented so far, and present them in a way that may be useful for software developers, by focusing on implementation choices as well as on their lim- itations. We think that this point of view complements  Navarro and Mäkinen 2007  and ﬁxes the state-of-the-art for this technology, possibly stimulating improvements in the design of such sophisticated algorithmic tools. In addition, we introduce two novel implementations of compressed indexes. These correspond to new versions of the FM-Index data structure, one of which combines the best existing theoretical guarantees with a competitive space time tradeoff in practice. Experimental. We experimentally compare a selected subset of implementations. This serves not only to help programmers in choosing the best index for their needs, but also gives a grasp of the practical relevance of this novel algorithmic technology. Technology Transfer. We introduce the Pizza and Chili site,1 which was developed with the aim of providing publicly available implementations of compressed indexes. Each implementation is well-tuned and adheres to a suitable API of functions which should, in our intention, allow any programmer to easily plug the provided com- pressed indexes within his her own software. The site also offers a collection of texts and tools for experimenting and validating the proposed compressed indexes. We hope that this simple API and the good performance of those indexes will spread their use in several applications.  The use of compressed indexes is obviously not limited to plain text searching. Every time one needs to store a set of strings which must be subsequently accessed for query-driven or id-driven string retrieval, one can use a compressed index with the goal of squeezing the dictionary space without slowing down the query performance. This is the subtle need that any programmer faces when implementing hash tables, tries or other indexing data structures. Actually, the use of compressed indexes has been successfully extended to handle several other more sophisticated data structures, such as dictionary indexes  Ferragina and Venturini 2007a, 2010   see Chap. 7 , labeled trees  Ferragina et al. 2005b, 2006c , graphs  Farzan and Munro 2008 , etc. Dealing with all those applications is out of the scope of this chapter, whose main goal is to address the above three issues, and comment on the experimental behavior of this new algorithmic technology.  1 Available at unipi.it.  two mirrors: pizzachili.dcc.uchile.cl and pizzachili.di.   64  6 Experiments on Compressed Full-Text Indexing  This chapter is organized as follows. Section 6.1 explains the key conceptual ideas underlying the most relevant compressed indexes. Section 6.2 describes how the indexes implement those basic ideas. Section 6.3 presents the Pizza and Chili site, and the next Sect. 6.4 comments on a large suite of experiments aimed at comparing the most successful implementations of the compressed indexes present in this web site.  6.1 Basics and Background  We recall from Sect. 2.3 that the text searching problem is then stated as follows. Given a text string T[1, n] and a pattern P[1, p], we wish to answer the following queries:  1  count the number of occurrences  occ  of P in T ;  2  locate the occ positions in T where P occurs. In this chapter we assume that T can be preprocessed, and an index is built on it, in order to speed up the execution of subsequent queries. We assume that the cost of index construction is amortized over sufﬁciently many searches, as otherwise sequential searching is preferable.  3  extract the substring T[l : r], given positions l and r in T .  In the case of self-indexes, which replace the text, a third operation of interest is  In Sect. 2.3 we described sufﬁx trees and sufﬁx arrays which are the most important classical full-text indexes whose main drawback is their space occupancy  namely, ε n log n  bits .  6.1.1 Backward Search  In Sect. 2.3.2 we described the classical binary-search method over sufﬁx arrays. Here we review an alternative approach which has been recently proposed in Ferragina and Manzini  2005 , hereafter named backward search. For any i = p, p−1, . . . , 1, this search algorithm keeps the interval S A[Firsti : Lasti] storing all text sufﬁxes which are preﬁxed by P[i : p]. This is done via two main steps: Initial step. We have i = p, so that it sufﬁces to access a precomputed table that stores the pair [First p, Last p] for all possible symbols P[ p] ≤  cid:3 . Inductive step. Let us assume to have computed the interval S A[Firsti+1 : Lasti+1], whose sufﬁxes are preﬁxed by P[i + 1 : p]. The present step determines the next interval S A[Firsti : Lasti] for P[i : p] from the previous interval and the next pattern symbol P[i]. The implementation is not obvious, and leads to different realizations of backward searching in several compressed indexes, with various time performances. The backward-search algorithm is executed by decreasing i until either an empty interval is found  i.e. Firsti > Lasti  , or S A[First1 : Last1] contains all pattern occurrences. In the former case no pattern occurrences are found; in the latter case the algorithm has found occ = Last1 − First1 + 1 pattern occurrences.   6.1 Basics and Background  65  6.1.2 Rank Query Given a string S[1, n], function Rankx  S, i   returns the number of times symbol x appears in the preﬁx S[1 : i]. Rank queries are central to compressed indexing, so it is important to understand how they are implemented and how much space time they need. We have two cases depending on the alphabet of S. For the aims of this chapter it sufﬁces to pose our attention on practical implementations and defer the discussion on theoretical results to next chapter which is more focused on theoretical aspects.  Rank over Binary Sequences. In this case there exist simple and practical constant- time solutions using o n  bits of space in addition to S  Munro 1996 . We cover only Rank1 as Rank0 S, i   = i − Rank1 S, i  . The solution partitions S into blocks of size s, and stores explicit answers for rank-queries done at block beginnings. One of the best practical implementations of the idea  González et al. 2005  solves Rank1 S, i   by summing two quantities:  1  the pre-computed answer for the preﬁx of S which ends at the beginning of the block enclosing S[i], plus  2  the relative rank of S[i] within its block. The latter is computed via a byte-wise scanning of the block, using small precomputed tables. This solution involves a space time tradeoff related to s, but nonetheless its query-time performance is rather satisfactory already with 5 % space overhead on top of S. Rank over General Sequences. Given a sequence S[1, n] over an alphabet of size σ , the wavelet tree  Grossi et al. 2003; Foschini et al. 2006  is a perfect binary tree of height ε log σ  , built on the alphabet symbols, such that the root represents the whole alphabet and each leaf represents a distinct alphabet symbol. If a node v represents alphabet symbols in the range  cid:3 v = [i, j], then its left child vl represents  cid:3 vl = [i, i+ j + 1, j]. We associate to each node v the subsequence Sv of S formed by the symbols in  cid:3 v. Sequence Sv is not really stored at the node, but it is replaced by a bit sequence Bv such that Bv[i] = 0 iff Sv[i] is a symbol whose leaf resides in the left subtree of v. Otherwise, Bv[i] is set to 1.  ] and its right child vr represents  cid:3 vr = [ i+ j  2  2  The power of the wavelet tree is to reduce rank operations over general alphabets to rank operations over a binary alphabet, so that the rank-machinery above can be used in each wavelet-tree node. Precisely, let us answer the query Rankc S, i  . We start from the root v of the wavelet tree  with associated vector Bv , and check which subtree encloses the queried symbol c. If c descends into the right subtree, we set i = Rank1 Bv, i   and move to the right child of v. Similarly, if c belongs to the left subtree, we set i = Rank0 Bv, i   and go to the left child of v. We repeat this until we reach the leaf that represents c, where the current i value is the answer to Rankc S, i  . Since any binary-rank takes O 1  time, the overall rank operation takes O log σ   time. We note that the wavelet tree can replace S as well: to obtain S[i], we start from the root v of the wavelet tree. If Bv[i] = 0, then we set i = Rank0 Bv, i   and go to the left child. Similarly, if Bv[i] = 1, then we set i = Rank1 Bv, i   and go to the   66  6 Experiments on Compressed Full-Text Indexing  right child. We repeat this until we reach a leaf, where the symbol associated to the leaf is the answer. Again, this takes O log σ   time. The wavelet tree requires comparable space to the original sequence, as it requires n log σ  1 + o 1   bits of space. A practical way to reduce the space occupancy to the zero-order entropy of S is to replace the balanced tree structure by the Huffman tree of S. Now we have to follow the binary Huffman code of a symbol to ﬁnd its place in the tree. It is not hard to see that the total number of bits required by such a tree is at most n H0 S  + 1  + o n log σ   and the average time taken by rank and access operations is O H0 S  , where H0 is the zero-th order empirical entropy of S  see Sect. 6.2 . This structure is the key tool in our implementation of SSA or AF-index  Sect. 6.4 .  6.2 Compressed Indexes  Compressed indexes provide a viable alternative to classical indexes that are par- simonious in space and efﬁcient in query time. They have undergone signiﬁcant development in the last years, so that we count now in the literature many solutions that offer a plethora of space-time tradeoffs  Navarro and Mäkinen 2007 . In theoret- ical terms, the most succinct indexes achieve n Hk  T  + o n log σ   bits of space, and for a ﬁxed λ > 0, require O  p log σ   counting time, O log1+λ n  time per located occurrence, and O  cid:6  log σ + log1+λ n  time to extract a substring of T of length  cid:6 .2 This is a surprising result because it shows that whenever T[1, n] is compressible it can be indexed into smaller space than its plain form and still offer search capabilities in efﬁcient time.  In the following we review the most competitive compressed indexes for which there is an implementation we are aware of. We will review the FM-index family, which builds on the BWT and backward searching; Sadakane’s Compressed Sufﬁx Array  CSA , which is based on compressing the sufﬁx array via a so-called  cid:7  function that captures text regularities; and the LZ-index, which is based on Lempel- Ziv compression. All of them are self-indexes in that they include the indexed text, which therefore may be discarded.  6.2.1 The FM-index Family  The FM-index is composed of a compressed representation of Bwt T   plus auxiliary structures for efﬁciently computing generalized rank queries on it. The main idea  Ferragina and Manzini 2005  is to obtain a text index from the BWT and then use backward searching for identifying the pattern occurrences  Sects. 6.1.1 and 2.4.2 .  2 These locating and extracting complexities are better than those reported in Ferragina et al.  2007 , and can be obtained by setting the sampling step to log1+λ n log σ .   6.2 Compressed Indexes  67  Fig. 6.1 Algorithm to get the interval S A[First : Last] of text sufﬁxes preﬁxed by P, using an FM-index  Several variants of this algorithmic scheme do exist  Ferragina and Manzini 2001, 2005; Mäkinen and Navarro 2005; Ferragina et al. 2007  which induce several time space tradeoffs for the counting, locating, and extracting operations. Counting. The counting procedure takes a pattern P and obtains the interval S A[First : Last] of text sufﬁxes preﬁxed by it  or, which is equivalent, the inter- val of rows of the matrix MT preﬁxed by P, see Sect. 2.4.2 . Figure 6.1 gives the pseudocode to compute First and Last. The algorithm is correct: Let [Firsti+1, Lasti+1] be the range of rows in MT that start with P[i+1 : p], and we wish to know which of those rows are preceded by P[i]. These correspond precisely to the occurrences of P[i] in Bwt T  [Firsti+1 : Lasti+1]. Those occurrences, mapped to the ﬁrst column of MT , form a  contiguous  range that is computed with a rationale similar to that for L F  ·  in Sect. 2.4.2, and thus via a just two rank operations on Bwt T  .  Locating. Algorithm in Fig. 6.2 obtains the position of the sufﬁx that preﬁxes the i-th row of MT . The basic idea is to logically mark a suitable set of rows of MT , and keep for each of them their position in T  that is, we store the corresponding S A values . Then, FM-locate i  scans the text T backwards using the LF-mapping ∈] + t, where t is the number until a marked row i  is found, and then it reports S A[i  ∈  Fig. 6.2 Algorithm to obtain S A[i] using an FM-index   68  6 Experiments on Compressed Full-Text Indexing  ∈  + n + o n  bits.  of backward steps used to ﬁnd such i . To compute the positions of all occurrences of a pattern P, it is thus enough to call FM-locate i  for every First √ i √ Last. The sampling rate of MT ’s rows, hereafter denoted by sS A, is a crucial parameter that trades space for query time. Most FM-index implementations mark all the S A[i] that are a multiple of sS A, via a bitmap B[1, n]. All the marked S A[i]s are stored contiguously in sufﬁx array order, so that if B[i] = 1 then one ﬁnds the corresponding S A[i] at position Rank1 B, i   in that contiguous storage. This guarantees that at most sS A LF-steps are necessary for locating the text position of any occurrence. The extra space is n log n sS A A way to avoid the need of bitmap B is to choose a symbol c having some suitable frequency in T , and then store S A[i] if Bwt T  [i] = c  Ferragina and Manzini 2001 . Then the position of S A[i] in the contiguous storage is Rankc Bwt T  , i  , so no extra space is needed other than Bwt T  . In exchange, there is no guarantee of ﬁnding a marked cell after a given number of steps. Extracting. The same text sampling mechanism used for locating permits extracting text substrings. Given sS A, we store the positions i such that S A[i] is a multiple of sS A now in the text order  previously we followed the S A-driven order . To extract T[l : r], we start from the ﬁrst sample that follows the area of interest, that is, sample number d =  cid:5  r + 1  sS A∼. From it we obtain the desired text backwards with the same mechanism for inverting the BWT  see Sect. 2.4.2 , here starting with the value i stored for the d-th sample. We need at most sS A + r − l + 1 applications of the LF-step.  6.2.2 Implementing the FM-index All the query complexities are governed by the time required to obtain C[c], Bwt T  [i], and Rankc Bwt T  , i    all of them implicit in L F as well . While C is a small table of σ log n bits, the other two are problematic. Counting requires up to 2 p calls to Rankc, locating requires sS A calls to Rankc and Bwt T  , and extracting  cid:6  symbols requires sS A +  cid:6  calls to Rankc and Bwt T  . In what follows we brieﬂy comment on the solutions adopted to implement those basic operations.  The original FM-index implementation  FM-index  Ferragina and Manzini 2001   compressed Bwt T   by splitting it into blocks and using independent zero-order com- pression on each block. Values of Rankc are precomputed for all block beginnings, and the rest of the occurrences of c from the beginning of the block to any posi- tion i are obtained by sequentially decompressing the block. The same traversal ﬁnds Bwt T  [i]. This is very space-effective: It approaches in practice the k-th order entropy because the partition into blocks takes advantage of the local compressibility of Bwt T  . On the other hand, the time to decompress the block makes computation of Rankc relatively expensive. For locating, this implementation marks the BWT positions where some chosen symbol c occurs, as explained above.   6.2 Compressed Indexes  69  A very simple and effective alternative to represent Bwt T   has been proposed with the Succinct Sufﬁx Array  SSA   Ferragina et al. 2007; Mäkinen and Navarro 2005 . It uses a Huffman-shaped wavelet tree, plus the marking of one out-of sS A text positions for locating and extracting. The space is n H0 T  +1 +o n log σ   bits, and the average time to determine Rankc Bwt T  , i   and Bwt T  [i] is O H0 T   + 1 . The space bound is not appealing because of the zero-order compression, but the relative simplicity of this index makes it rather fast in practice. In particular, it is an excellent option for DNA text, where the k-th order compression is not much better than the zero-th order one, and the small alphabet makes H0 T   √ log σ small too. The Run-Length FM-index  RLFM   Mäkinen and Navarro 2005  has been intro- duced to achieve k-th order compression by applying run-length compression to Bwt T   prior to building a wavelet tree on it. The BWT generates long runs of iden- tical symbols on compressible texts, which makes the RLFM an interesting alterna- tive in practice. The price is that the mappings from the original to the run-length compressed positions slow down the query operations a bit, in comparison to the SSA.  6.2.3 The Compressed Sufﬁx Array  The compressed sufﬁx array  CSA  was not originally a self-index, and required O n log σ   bits of space  Grossi and Vitter 2005 . Sadakane  2003, 2002  then pro- posed a variant which is a self-index and achieves space bound in term of 0-order entropy. The result in Grossi et al.  2003  described the ﬁrst index that achieves high-order compression. The CSA represents the sufﬁx array S A[1, n] by a sequence of numbers  cid:7  i  , such that S A[ cid:7  i  ] = S A[i] + 1. It is not hard to see  Grossi and Vitter 2005  that  cid:7  is piecewise monotone increasing in the areas of S A where the sufﬁxes start with the same symbol. In addition, there are long runs where  cid:7  i + 1  =  cid:7  i   + 1, and these runs can be mapped one-to-one to the runs in Bwt T    Navarro and Mäkinen 2007 . These properties permit a compact representation of  cid:7  and its fast access. Essentially, we differentially encode  cid:7  i   −  cid:7  i − 1 , run-length encode the long runs of 1’s occurring over those differences, and for the rest use an encoding favoring small numbers. Absolute samples are stored at regular intervals to permit the efﬁcient decoding of any  cid:7  i  . The sampling rate  hereafter denoted by s cid:7   gives a space time tradeoff for accessing and storing  cid:7 . In Sadakane  2003  it is shown that the index requires O n H0 T  +n log log σ   bits of space. The analysis has been then improved in Navarro and Mäkinen  2007  to n Hk  T   + O n log log σ   for any k √ α logσ n and constant 0 < α < 1.  Counting. The CSA  Sadakane 2003  used the classical binary searching to count the number of pattern occurrences in T . The actual implementation, proposed in Sadakane  2002 , uses backward searching  Sect. 6.1.1 :  cid:7  is used to obtain [Firsti , Lasti] from [Firsti+1, Lasti+1] in O log n  time, for a total of O  p log n    70  6 Experiments on Compressed Full-Text Indexing  Fig. 6.3 Algorithm to get the interval S A[First, Last] preﬁxed by P, using the CSA. The [min, max] interval is obtained via binary search  counting time. Precisely, let S A[Firsti : Lasti] be the range of sufﬁxes S A[ j] that start with P[i] and such that S A[ j]+ 1  = S A[ cid:7   j  ]  starts with P[i + 1 : p]. The former is equivalent to the condition [Firsti , Lasti] ⊆ [C[P[i]] + 1, C[P][i] + 1]]. The latter is equivalent to saying that Firsti+1 √  cid:7   j   √ Lasti+1. Since  cid:7  i   is monotonically increasing in the range C[P[i]] < j √ C[P[i] + 1]  since the ﬁrst symbols of sufﬁxes in S A[Firsti : Lasti] are the same , we can binary search this interval to ﬁnd the range [Firsti , Lasti]. Figure 6.3 shows the pseudocode for counting using the CSA.  Locating. Locating is similar to the FM-index, in that the sufﬁx array is sampled at regular intervals of size sS A. However, instead of using the LF-mapping to traverse the text backwards, this time we use  cid:7  to traverse the text forward, given that S A[ cid:7  i  ] = S A[i] + 1. This points out an interesting duality between the FM-index and the CSA. Yet, there is a fundamental difference: function L F  ·  is implicitly stored and calculated on the ﬂy over Bwt T  , while function  cid:7  ·  is explicitly stored. The way these functions are calculated stored makes the CSA a better alternative for large alphabets. Extracting. Given C and  cid:7 , we can obtain T[S A[i] : n] symbolwise from i, as follows. The ﬁrst symbol of the sufﬁx pointed to by S A[i], namely, T[S A[i]], is the symbol c such that C[c] < i √ C[c + 1], because all the sufﬁxes S A[C[c] + 1], . . . , S A[C[c + 1]] start with symbol c. Now, ir order to obtain the next symbol, T[S A[i] + 1], we compute i ∈ =  cid:7  i   and use the same procedure above to obtain ∈]] = T[S A[i] + 1], and so on. The binary search in C can be avoided by T[S A[i representing it as a bit vector D[1, n] such that D[C[c]] = 1, thus c = Rank1 D, i  . Now, given a text substring T[l : r] to extract, we must ﬁrst ﬁnd the i such that l = S A[i] and then we can apply the procedure above. Again, we sample the text at regular intervals by storing the i values such that S A[i] is a multiple of sS A. To extract T[l : r] we actually extract T[⇒l sS A≥ · sS A : r], so as to start from the preceding sampled position. This takes sS A + r − l + 1 applications of  cid:7 .   6.2 Compressed Indexes  6.2.4 The Lempel-Ziv Index  71  The Lempel-Ziv index  LZ-index  is a compressed self-index based on a Lempel-Ziv partitioning of the text. There are several members of this family  e.g., Navarro 2004; Arroyuelo et al. 2006; Ferragina and Manzini 2005 , we focus on the versions described in Navarro  2004 , Arroyuelo et al.  2006  and available in the Pizza and Chili site. This index uses LZ78 parsing  Ziv and Lempel 1978  to generate a phrases, T = Z1, . . . , Zn∈. These phrases are all partitioning of T[1 : n] into n different, and each phrase Zi is formed by appending a single symbol to a previous phrase Z j , j < i  except for a virtual empty phrase Z0 . Since it holds Zi = Z j · c, for some j < i and c ≤  cid:3 , the set is preﬁx-closed. We can then build a trie on these phrases, called LZ78-trie, which consists of n  nodes, one per phrase.  ∈  ∈  ∈  ∈  ∈  ∈  log n  log n  The original LZ-index  Navarro 2004  is formed by  1  the LZ78 trie;  2  a trie formed with the reverse phrases Z r i , called the reverse trie;  3  a mapping from phrase identiﬁers i to the LZ78 trie node that represents Zi ; and  4  a similar mapping to Z r i in the reverse phrases. The tree shapes in  1  and  2  are represented using parentheses ∈  bits and the encoding proposed in Munro and Raman  1997  so that they take O n and constant time to support various tree navigation operations. Yet, we must also store the phrase identiﬁer in each trie node, which accounts for the bulk of the space bits of space, which can be bounded by for the tries. Overall, we have 4n 4n Hk  T   + o n log σ   for k = o logσ n   Navarro and Mäkinen 2007 . This can be reduced to  2+λ n Hk  T  +o n log σ   by noticing that the mapping  3  is essentially the inverse permutation of the sequence of phrase identiﬁers in  1 , and similarly  4  with  2   Arroyuelo and Navarro 2008 . It is possible to represent a permutation and its inverse using  1 + λ n bits of space and access the inverse permutation in O 1 λ  time  Munro et al. 2003  . An occurrence of P in T can be found according to one of the following situations: 1. P lies within a phrase Zi . Unless the occurrence is a sufﬁx of Zi , since Zi = Z j·c, P also appears within Z j , which is the parent of Zi in the LZ78 trie. A search for Pr in the reverse trie ﬁnds all the phrases that have P as a sufﬁx. Then the node mapping permits, from the phrase identiﬁers stored in the reverse trie, to reach their corresponding LZ78 nodes. All the subtrees of those nodes are occurrences. 2. P spans two consecutive phrases. This means that, for some j, P[1 : j] is a sufﬁx of some Zi and P[ j + 1 : p] is a preﬁx of Zi+1. For each j, we search for Pr[1 : j] in the reverse trie and P[ j + 1 : p] in the LZ78 trie, choosing the smaller subtree of the two nodes we arrived at. If we choose the descendants of the reverse trie node for Pr[1 : j], then for each phrase identiﬁer i that descends from the node, we check whether i + 1 descends from the node that corresponds to P[ j + 1 : p] in the LZ78 trie. This can be done in constant time by comparing preorder numbers.  3. P spans three or more nodes. This implies that some phrase is completely con- tained in P, and since all phrases are different, there are only O  p2  different phrases to check, one per substring of P. Those are essentially veriﬁed one by one.   72  6 Experiments on Compressed Full-Text Indexing  Notice that the LZ-index carries out counting and locating simultaneously, which renders the LZ-index not competitive for counting alone. Extracting text is done by traversing the LZ78 paths upwards from the desired phrases, and then using mapping  3  to continue with the previous or next phrases. The LZ-index is very competitive for locating and extracting.  6.2.5 Novel Implementations  We introduce two novel compressed index implementations in this chapter. Both are variants of the FM-index family. The ﬁrst one is interesting because it is a re-engineering of the ﬁrst reported implementation of a self-index  Ferragina and Manzini 2001 . The second is relevant because it implements the self-index offering the best current theoretical space time guarantees. It is fortunate, as it does not always happen, that theory and practice marry well and this second index is also relevant in the practical space time tradeoff map.  6.2.5.1 The FMI-2  As the original FM-index  Ferragina and Manzini 2001 , the FMI-2 adopts a two- level bucketing scheme for implementing efﬁcient rank and access operations onto Bwt T  . In detail, string Bwt T   is partitioned into buckets and superbuckets: a bucket consists of lb symbols, a superbucket consists of lsb buckets. Additionally, the FMI-2 maintains two tables: Table Tsb stores, for each superbucket and for each symbol c, the number of occurrences of c before that superbucket in Bwt T  ; table Tb stores, for each bucket and for each symbol c, the number of occurrences of c before that bucket and up to the beginning of its superbucket. In other words, Tsb stores the value of the ranking function up to the beginning of superbuckets; whereas Tb stores the ranking function up to the beginning of buckets and relative to their enclosing superbuckets. Finally, every bucket is individually compressed using the sequence of zero-order compressors: MTF, RLE, Huffman  as in bzip2 . This compression strategy does not guarantee that the space of FMI-2 is bounded by the kth order entropy of T . Nevertheless, the practical performance is close to the one achievable by the best known compressors, and can be traded by tuning parameters lb and lsb. The main difference between the original FM-index and the novel FMI-2 lies in the strategy adopted to select the rows positions of T which are explicitly stored. The FMI-2 marks logically and uniformly the text T by adding a special symbol every sS A symbols of the original text. This way, all of the MT ’s rows that start with that special symbol are contiguous, and thus their positions can be stored and accessed easily.  The count algorithm is essentially a backward search  Algorithm 6.1 , modiﬁed to take into account the presence of special symbols added to the indexed text. To search for a pattern P[1 : p], the FMI-2 actually searches for min{ p−1, sS A} patterns   6.2 Compressed Indexes  73  obtained by inserting the special symbols in P at each sS A-th position, and searches for the pattern P itself. This search is implemented in parallel over all patterns above by exploiting the fact that, at any step i, we have to search either for P[ p − i] or for the special symbol. As a result, the overall search cost is quadratic in the pattern length, and the output is now a set of at most p ranges of rows.  Therefore, the FMI-2 is slower in counting than the original FM-index, but locating is faster, and this is crucial because this latter operation is usually the bottleneck of compressed indexes. Indeed the locate algorithm proceeds for at most sS A phases. Let S0 be the range of rows to be located, eventually identiﬁed via a count operation. At a generic phase k, Sk contains the rows that may be reached in k backward steps from the rows in S0. Sk consists of a set of ranges of rows, rather than a single range. To maintain the invariant, the algorithm picks up a range of Sk, say [a, b], and determines the z √ σ distinct symbols that occur in the substring Bwt T  [a : b] via two bucket scans and some accesses to tables Tsb and Tb. Then it executes z backward steps, one per such symbols, thus determining z new ranges of rows  to be inserted in Sk+1  which are at distance k + 1 from the rows in S0. The algorithm cycles over all ranges of Sk to form the new set Sk+1. Notice that if the rows of a range start with the special symbol, their positions in the indexed text are explicitly stored, and can be accessed in constant time. Then, the position of the corresponding rows in S0 can be inferred by summing k to those values. Notice that this range can be dropped from Sk. After no more than sS A phases the set Sk will be empty.  6.2.5.2 The Alphabet-Friendly FM-index  The Alphabet-Friendly FM-index  AF-index   Ferragina et al. 2007  resorts to the deﬁnition of k-th order entropy  see Sect. 2.4.1  by encoding each substring wT up to its zero-order entropy. Since all the wT are contiguous in Bwt T    regardless of which k value we are considering , it sufﬁces to split Bwt T   into blocks given by the k-th order contexts, for any desired k, and to use a Huffman-shaped wavelet tree  see Sect. 6.1.2  to represent each such block. In addition, we need all Rankc values precomputed for every block beginning, as the local wavelet trees can only answer Rankc within their blocks. In total, this achieves n Hk  T   + o n log σ   bits, for moderate and ﬁxed k √ α logσ n and 0 < α < 1. Actually the AF-index does better, by splitting Bwt T   in an optimal way, thus guaranteeing that the space bound above holds simultaneously for every k. This is done by resorting to the idea of compression boosting  Ferragina et al. 2005a .  The compression booster ﬁnds the optimal partitioning of Bwt T   into t non- empty blocks, s1, . . . , st , assuming that each block s j will be represented using s jH0 s j   + f  s j  bits of space, where f  ·  is a nondecreasing concave function supplied as a parameter. Given that the partition is optimal, it can be shown that the resulting space is upper bounded by n Hk + σ k f  n σ k   bits simultaneously for every k. That is, the index is not built for any speciﬁc k. As explained, the AF-index represents each block s j by means of a Huffman- shaped wavelet tree wt j , which will take at most s j H0 s j   + 1  + σ log n bits.   74  6 Experiments on Compressed Full-Text Indexing  ∈∈ = i − i  ∈∈], where i  The last term accounts for the storage of the Huffman code. In addition, for each block j we store an array C j[c], which tells the Rankc values up to block j. This accounts for other σ log n bits per block. Finally, we need a bitmap R[1, n] indicating the starting positions of the t blocks in Bwt T  . Overall, the formula giving the excess of storage over the entropy for block j is f  s j  = 2s j + 2σ log n. To carry out any operation at position i, we start by computing the block where position i lies, j = Rank1 R, i  , and the starting position of that block, ∈ = Select1 R, j  .  This tells the position of the j-th 1 in R. As it is a sort i of inverse of Rank, it is computed by binary search over Rank values.  Hence Bwt T  [i] = s j[i ∈ + 1 is the offset of i within block j. Then, the different operations are carried out as follows.   For counting, we use the algorithm of Fig. 6.1. In this case, we have Rankc Bwt T  , i   = C j[c] + Rankc s j , i ∈∈ , where the latter is computed using the wavelet tree wt j of s j .   For locating, we use the algorithm of Fig. 6.2. In this case, we have c = Bwt T  [i] = s j[i ∈∈], we also use the wavelet tree wt j of s j .   For extracting, we proceed similarly as for locating, as explained in Sect. 6.2.1. ∗ ∗ nt rather than n bits. We cut R into As a ﬁnal twist, R is actually stored using 2 ∗ n t. There are at most t chunks which are not all zeros. nt chunks of length Concatenating them all requires only nt indicates whether each chunk is all-zero or not. It is easy to translate rank select operations into this representation.  ∗ nt bits. A second bitmap of length  ∈∈]. To compute s j[i  ∗  6.3 The Pizza and Chili Site  The Pizza and Chili site has two mirrors: one in Chile and one in Italy.3 Its ultimate goal is to push towards the technology transfer of this fascinating algorithmic tech- nology lying at the crossing point of data compression and data structure design. In order to achieve this goal, the Pizza and Chili site offers publicly available and highly tuned implementations of various compressed indexes. The implementations follow a suitable C C++ API of functions which should, in our intention, allow any programmer to plug easily the provided compressed indexes within his her own soft- ware. The site also offers a collection of texts for experimenting with and validating the compressed indexes. In detail, it offers three kinds of material:   A set of compressed indexes which are able to support the search functionalities of classical full-text indexes  e.g., substring searches , but requiring succinct space occupancy and offering, in addition, some text access operations that make them useful within text retrieval and data mining software systems.   A set of text collections of various types and sizes useful to test experimentally the available  or new  compressed indexes. The text collections have been selected  3 http:  pizzachili.dcc.uchile.cl and http:  pizzachili.di.unipi.it.   6.3 The Pizza and Chili Site  75  to form a representative sample of different applications where indexed text searching might be useful. The size of these texts is large enough to stress the impact of data compression over memory usage and CPU performance. The goal of experimenting with this testbed is to conclude whether, or not, compressed index- ing is beneﬁcial over uncompressed indexing approaches, like sufﬁx trees and sufﬁx arrays. And, in case it is beneﬁcial, which compressed index is preferable according to the various applicative scenarios represented by the testbed.   Additional material useful to experiment with compressed indexes, such as scripts for their automatic validation and efﬁciency test over the available text collections.  The Pizza and Chili site hopes to mimic the success and impact of other initiatives, such as data-compression.info and the Calgary and Canterbury corpora, just to cite a few. Actually, the Pizza and Chili site is a mix, as it offers both software and testbeds. Several people have already contributed to make this site work and, hopefully, many more will contribute to turn it into a reference for all researchers and software develop- ers interested in experimenting and developing the compressed-indexing technology. The API we propose is thus intended to ease the deployment of this technology in real software systems, and to provide a reference for any researcher who wishes to contribute to the Pizza and Chili repository with his her new compressed index.  6.3.1 Indexes  The Pizza and Chili site provides several index implementations, all adhering to a common API. All indexes, except CSA and LZ-index, are built through the deep- shallow algorithm of Manzini and Ferragina  Manzini and Ferragina 2004  which constructs the Sufﬁx Array data structure using little extra space and is fast in practice.   The Sufﬁx Array  Manber and Myers 1993  is a plain implementation of the classical index  see Sect. 2.3.2 , using either n log n bits of space or simply n computer integers, depending on the version. This was implemented by Rodrigo González.   The SSA  Ferragina et al. 2007; Mäkinen and Navarro 2005  uses a Huffman-based wavelet tree over the string Bwt T    Sect. 6.2.1 . It achieves zero-order entropy in space with little extra overhead and striking simplicity. It was implemented by Veli Mäkinen and Rodrigo González.   The AF-index  Ferragina et al. 2007  combines compression boosting  Ferragina et al. 2005a  with the above wavelet tree data structure  Sect. 6.2.5.2 . It achieves high-order compression, at the cost of being more complex than SSA. It was implemented by Rodrigo González.   The RLFM  Mäkinen and Navarro 2005  is an improvement over the SSA  Sect. 6.2.1 , which exploits the equal-letter runs of the BWT to achieve k-th order compression, and in addition uses a Huffman-shaped wavelet tree. It is slightly larger than the AF-index. It was implemented by Veli Mäkinen and Rodrigo González.   76  6 Experiments on Compressed Full-Text Indexing    The FMI-2  Sect. 6.2.5.1  is an engineered implementation of the original FM-index  Ferragina and Manzini 2001 , where a different sampling strategy is designed in order to improve the performance of the locating operation. It was implemented by Paolo Ferragina and Rossano Venturini.   The CSA  Sadakane 2003, 2002  is the variant using backward search  Sect. 6.2.3 . It achieves high-order compression and is robust for large alphabets. It was imple- mented by Kunihiko Sadakane and adapted by Rodrigo González to adhere the API of the Pizza and Chili site. To construct the sufﬁx array, it uses the qsufsort by Jesper Larsson and Kunihiko Sadakane  2007 .   The LZ-index  Navarro 2004; Arroyuelo et al. 2006  is a compressed index based on LZ78 compression  Sect. 6.2.4 , implemented by Diego Arroyuelo and Gonzalo Navarro. It achieves high-order compression, yet with relatively large constants. It is slow for counting but very competitive for locating and extracting.  These implementations support any byte-based alphabet of size up to 255 symbols:  one symbol is automatically reserved by the indexes as the terminator “$”.  6.3.2 Texts  We have chosen the texts forming the Pizza and Chili collection by following three basic considerations. First, we wished to cover a representative set of application areas where the problem of full-text indexing might be relevant, and for each of them we selected texts freely available on the Web. Second, we aimed at having one ﬁle per text type in order to avoid unreadable tables of many results. Third, we have chosen the size of the texts to be large enough in order to make indexing relevant and compression apparent. These are the current collections provided in the repository:   dna  DNA sequences . This ﬁle contains bare DNA sequences without descrip- tions, separated by newline, obtained from ﬁles available at the Gutenberg Project site: namely, from 01hgp10 to 21hgp10, plus 0xhgp10 and 0yhgp10. Each of the four DNA bases is coded as an uppercase letter A,G,C,T, and there are a few occurrences of other special symbols.   english  English texts . This ﬁle is the concatenation of English texts selected from the collections etext02—etext05 available at the Gutenberg Project site. We deleted the headers related to the project so as to leave just the real text.   pitches  MIDI pitch values . This ﬁle is a sequence of pitch values  bytes whose values are in the range 0–127, plus a few extra special values  obtained from a myriad of MIDI ﬁles freely available on the Internet. The MIDI ﬁles were converted into the IRP format by using the semex tool by Kjell Lemstrom  Lemström and Perttu 2000 . This is a human-readable tuple format, where the 5th column is the pitch value. The pitch values were coded in one byte each and concatenated all together.   proteins  protein sequences . This ﬁle contains bare protein sequences with- out descriptions, separated by newline, obtained from the Swissprot database   6.3 The Pizza and Chili Site  Table 6.1 General statistics for our indexed texts Text dna english pitches proteins sources xml  Size  MB  200 200 50 200 200 200  Alphabet size 16 225 133 25 230 96  77  Inv. match prob. 3.86 15.12 40.07 16.90 24.81 28.65   ftp.ebi.ac.uk  pub databases swissprot  . Each of the 20 amino acids is coded as an uppercase letter.   sources  source program code . This ﬁle is formed by C Java source codes obtained by concatenating all the .c, .h, .C and .java ﬁles of the linux-2.6.11.6  ftp.kernel.org  and gcc-4.0.0  ftp.gnu.org  distributions.   xml  structured text . This ﬁle is in XML format and provides bibliographic infor- mation on major computer science journals and proceedings. It was downloaded from the DBLP archive at dblp.uni-trier.de.  For the experiments we have limited the short ﬁle pitches to its initial 50 MB, whereas all the other long ﬁles have been cut down to their initial 200 MB. We show now some statistics on those ﬁles. These statistics and the tools used to compute them are also available at the Pizza and Chili site.  Table 6.1 summarizes some general characteristics of the selected ﬁles. The last column, inverse match probability, is the reciprocal of the probability of matching between two randomly chosen text symbols. This may be considered as a measure of the effective alphabet size—indeed, on a uniformly distributed text, it would be precisely the alphabet size. Table 6.2 provides some information about the compressibility of the texts by reporting the value of Hk for 0 √ k √ 4, measured as number of bits per input sym- bol. As a comparison on the real compressibility of these texts, Table 6.3 shows the performance of three well-known compressors  sources available in the site : gzip  Lempel-Ziv-based compressor , bzip2  BWT-based compressor , and ppmdi  k-th order modeling compressor . Notice that, as k grows, the value of Hk decreases  Table 6.2 Ideal compressibility of our indexed texts Text  log σ  H0  4th order  4.000 2222 dna 7.814 589230 english 7.055 3845792 pitches proteins 4.644 224132 7.845 1719387 sources 6.585 907678 xml For every k-th order model, with 0 √ k √ 4, we report the number of distinct contexts of length k, and the empirical entropy Hk, measured as number of bits per input symbol  2nd order  H2 152 1.920 10829 2.948 10946 4.139 4.156 607 9525 3.102 2.170 7049  3rd order  683 102666 345078 11607 253831 141736  1st order  H1 16 1.930 225 3.620 133 4.734 4.178 25 230 4.077 3.480 96  H3 1.916 2.422 3.457 4.066 2.337 1.434  H4 1.910 2.063 2.334 3.826 1.852 1.045  1.974 4.525 5.633 4.201 5.465 5.257   78  6 Experiments on Compressed Full-Text Indexing  Table 6.3 Real compressibility of our indexed texts, as achieved by the best-known compressors: gzip  option -9 , bzip2  option -9 , and ppmdi  option -l 9  Text dna english pitches proteins sources xml  H4 1.910 2.063 2.334 3.826 1.852 1.045  2.076 2.246 2.890 3.584 1.493 0.908  2.162 3.011 2.448 3.721 1.790 1.369  1.943 1.957 2.439 3.276 1.016 0.745  bzip2  ppmdi  gzip  but the size of the dictionary of length-k contexts grows signiﬁcantly, eventually approaching the size of the text to be compressed. Typical values of k for ppmdi are around 5 or 6. It is interesting to note in Table 6.3 that the compression ratios achiev- able by the tested compressors may be superior to H4, because they use  explicitly or implicitly  longer contexts.  6.4 Experimental Results  In this section we report experimental results from a subset of the compressed indexes available at the Pizza and Chili site. We restricted our experiments to a few indexes: Succinct Sufﬁx Array  version SSA_v2 in Pizza and Chili , Alphabet-Friendly FM-index  version AF-index_v2 in Pizza and Chili , Compressed Sufﬁx Array  CSA in Pizza and Chili , and LZ-index  version LZ-index4 in Pizza and Chili , because they are the best representatives of the three classes of compressed indexes we discussed in Sect. 6.2. This small number will provide us with a succinct, yet signiﬁcant, picture of the performance of all known compressed indexes  Navarro and Mäkinen 2007 .  There is no need to say that further algorithmic engineering of the indexes exper- imented in this chapter, as well of the other indexes available in the Pizza and Chili site, could possibly change the charts and tables shown below. However, we believe that the overall conclusions drawn from our experiments should not change signiﬁ- cantly, unless new algorithmic ideas are devised for them. Indeed, the following list of experimental results has a twofold goal: on one hand, to quantify the space and time performance of compressed indexes over real datasets, and on the other hand, to motivate further algorithmic research by highlighting the limitations of the present indexes and their implementations.  Table 6.4 shows the parameters used to construct the indexes in our experiments. The SSA and AF-index have a sampling rate parameter sS A that trades locating and extracting time for space. More precisely, they need O sS A  accesses to the wavelet tree for locating, and O sS A+r −l+ 1  accesses to extract Tl,r , in exchange for n log n additional bits of space. We can remove those structures if we are only sS A interested in counting.   6.4 Experimental Results  79  Table 6.4 Parameters used for the different indexes in our experiments Index AF-index CSA LZ-index SSA The cases of multiple values correspond to space time tradeoff curves  Count − s cid:7  = {128} λ = { 1 −  Locate   extract sS A = {4, 16, 32, 64, 128, 256} sS A = {4, 16, 32, 64, 128, 256}; s cid:7  = {128} λ = {1, 1 sS A = {4, 16, 32, 64, 128, 256}  , 1 20  , 1 10  , 1 4  , 1 3  , 1 5  }  }  4  2  The CSA has two space time tradeoffs. A ﬁrst one, s cid:7 , governs the access time to  cid:7 , which is O s cid:7    in exchange for n log n bits of space required by the samples. The s cid:7  second, sS A, affects locating and extracting time just as above. For pure counting we can remove the sampling related to sS A, whereas for locating the best is to use the default value  given by Sadakane  of s cid:7  = 128. The best choice for extracting is less clear, as it depends on the length of the substring to extract.  Finally, the LZ-index has one parameter λ which trades counting locating time for space occupancy: The cost per candidate occurrence is multiplied by 1 λ , and the additional space is 2λn Hk  T   bits. No structure can be removed in the case of counting, but space can be halved if the extract operation is the only one needed  just remove the reverse trie .  All the experiments were executed on a 2.6 GHz Pentium 4, with 1.5 GB of main memory, and running Fedora Linux. The searching and building algorithms for all compressed indexes were coded in C C++ and compiled with gcc or g++ version 4.0.2.  6.4.1 Construction  Table 6.5 shows construction time and peak of memory usage during construction for one collection, namely english, as all the others give roughly similar results. In order to fairly evaluate the time and space consumption of the algorithm needed to construct the sufﬁx array underlying the CSA implementation, we replaced the construction algorithm proposed by Larsson and Sadakane  2007  and used in the original implementation by Sadakane  see Sect. 6.3.1 , with the faster algorithm proposed by Manzini and Ferragina  2004  and used by all other compressed SA-based indexes.4 The bulk of the time of SSA and CSA is that of sufﬁx array construction  prior to its compression . The AF-index takes much more time because it needs to run the compression boosting algorithm over the sufﬁx array. The LZ-index spends most of the time in parsing the text and creating the LZ78 and reverse tries. In all cases construction times are practical, 1–4 sec MB with our machine.  4 We note that this change was done just for timing measurements, the code available on the Pizza and Chili site still uses Larsson-Sadakane’s algorithm because this was the choice of the CSA’s implementor.   80  6 Experiments on Compressed Full-Text Indexing  Build time  sec   Table 6.5 Time and peak of main memory usage required to build the various indexes over the 200 MB ﬁle english Index AF-index CSA LZ-index SSA  The indexes are built using the default value for the locate tradeoff  that is, sS A = 64 for AF-index and SSA; sS A = 64 and s cid:7  = 128 for CSA; and λ = 1  Main memory usage  MB   1,751 1,801 1,037 1,251  772 233 198 217  4 for the LZ-index   The memory usage might be problematic, as it is 5–9 times the text size. Albeit the ﬁnal index is small, one needs much memory to build it ﬁrst.5 This is a problem of compressed indexes, which is attracting a lot of practical and theoretical research  Lam et al. 2002; Arroyuelo and Navarro 2005; Hon et al. 2003; Mäkinen and Navarro 2008 .  Note we have given construction time and space for just one parameter setting per index. The reason is that time and space for construction is mostly insensitive to these parameters. They imply sparser or denser sampling of sufﬁx arrays and permutations, but those sampling times are negligible compared to sufﬁx array and trie construction times, and sampling does not affect peak memory consumption either.  6.4.2 Counting We searched for 50,000 patterns of length p = 20, randomly chosen from the indexed texts. The average counting time was then divided by p to display counting time per symbol. This is appropriate because the counting time of the indexes is linear in m, and 20 is sufﬁciently large to blur small constant overheads. The exception is the LZ-index, whose counting time is superlinear in p, and not competitive at all for this task.  Table 6.6 shows the results on this test. The space of the SSA, AF-index, and CSA does not include what is necessary for locating and extracting. We can see that, as expected, the AF-index is always smaller than the SSA, yet they are rather close on dna and proteins  where the zero-order entropy is not much larger than higher-order entropies . The space usages of the AF-index and the CSA are similar and usually the best, albeit the CSA predictably loses in counting time on smaller alphabets  dna, proteins , due to its O  p log n  rather than O  p log σ   com- plexity. The CSA takes advantage of larger alphabets with good high-order entropies  sources, xml , a combination where the AF-index, despite of its name, proﬁts less. Note that the space performance of the CSA on those texts conﬁrms that its space occupancy is related to the high-order entropy.  5 In particular, this limited us to indexing up to 200 MB of text in our machine.   6.4 Experimental Results  81  plain SA  LZ-index  AF-index  Table 6.6 Experiments on the counting of pattern occurrences Text  Space Time  Space Time  CSA Space Time  SSA Time 0.956 0.29 dna 2.147 0.60 english 2.195 0.74 pitches proteins 1.905 0.56 2.635 0.72 sources 2.764 0.69 xml Time is measured in microseconds per pattern symbol. The space usage is expressed as a fraction of the original text size. We put in boldface those results that lie within 10 % of the best space time tradeoffs  Space Time 0.542 0.512 0.363 0.479 0.499 0.605  43.896 0.93 68.774 1.27 55.314 1.95 47.030 1.81 162.444 1.27 306.711 0.71  1.914 0.28 2.694 0.42 2.921 0.66 3.082 0.56 2.946 0.49 2.256 0.34  5.220 0.46 4.758 0.44 3.423 0.63 6.477 0.67 4.345 0.38 4.321 0.29  5 5 5 5 5 5  Space  With respect to time, the SSA is usually the fastest thanks to its simplicity. Some- times the AF-index gets close and it is actually faster on xml. The CSA is rarely competitive for counting, and the LZ-index is well out of bounds for this experi- ment. Notice that the plain sufﬁx array  last column in Table 6.6  is 2–6 times faster than any compressed index, but its space occupancy can be up to 18 times larger.  6.4.3 Locate  We locate sufﬁcient random patterns of length 5 to obtain a total of 2–3 million occurrences per text  see Table 6.7 . This way we are able to evaluate the average cost of a single locate operation, by making the impact of the counting cost negligible. Figures 6.4 and 6.5 report the time space tradeoffs achieved by the different indexes for the locate operation.  We remark that the implemented indexes include the sampling mechanism for locate and extract as a single module, and therefore the space for both operations is included in these plots. Therefore, the space could be reduced if we only wished to locate. However, as extracting snippets of pattern occurrences is an essential func- tionality of a self-index, we consider that the space for efﬁcient extraction should always be included.6  Table 6.7 Number of searched patterns of length 5 and total number of located occurrences  Text dna english pitches proteins sources xml   patterns 10 100 200 3,500 50 20   occurrences 2,491,410 2,969,876 2,117,347 2,259,125 2,130,626 2,831,462  6 Of course, we could have a sparser sampling for extraction, but we did not want to complicate the evaluation more than necessary.   82  6 Experiments on Compressed Full-Text Indexing  Fig. 6.4 Space-time trade- offs for locating occurrences of patterns of length 5  dna    e c n e r r u c c o   r e p   s c e s o r c m  i      e m  i t    e c n e r r u c c o    r e p    s c e s o r c m  i      e m  i t    e c n e r r u c c o    r e p    s c e s o r c m  i      e m  i t   30   25   20   15   10   5   0   30   25   20   15   10   5   0   30   25   20   15   10   5   0   0   0.5   1   1.5   2   2.5  Space usage  fraction of text   english   0   0.5   1   1.5   2   2.5  Space usage  fraction of text   pitches   0   0.5   1   1.5   2   2.5  Space usage  fraction of text    6.4 Experimental Results  Fig. 6.5 Space-time trade- offs for locating occurrences of patterns of length 5  83  proteins    e c n e r r u c c o   r e p   s c e s o r c m  i      e m  i t    e c n e r r u c c o    r e p    s c e s o r c m  i      e m  i t    e c n e r r u c c o    r e p    s c e s o r c m  i       e m  i t   30   25   20   15   10   5   0   30   25   20   15   10   5   0   30   25   20   15   10   5   0   0   0.5   1   1.5   2   2.5  Space usage  fraction of text   sources   0   0.5   1   1.5   2   2.5  Space usage  fraction of text   xml   0   0.5   1   1.5   2   2.5  Space usage  fraction of text    84 Table 6.8 Locate time required by plain SA in microseconds per occurrence, with p = 5  6 Experiments on Compressed Full-Text Indexing  DNA  English  Pitches  Proteins  Sources  plain SA We recall that this implementation requires 5 bytes per indexed symbol  0.007  0.005  0.005  0.006  0.007  XML  0.006  The comparison shows that usually CSA can achieve the best results with minimum space, except on dna where the SSA performs better as expected  given its query time complexity,  see Sect. 6.2.2 , and on proteins for which the sufﬁx-array- based indexes perform similarly  and the LZ-index does much worse . The CSA is also the most attractive alternative if we ﬁx that the space of the index should be equal to that of the text  recall that it includes the text , dna and xml being the exceptions, where the LZ-index is superior.  The LZ-index can be much faster than the others if one is willing to pay for some extra space. The exceptions are pitches, where the CSA is superior, and proteins, where the LZ-index performs poorly. This may be caused by the large number of patterns that were searched to collect the 2–3 million occurrences  see Table 6.7 , as the counting is expensive on the LZ-index.  Table 6.8 shows the locate time required by an implementation of the classical sufﬁx array: it is between 100 and 1,000 times faster than any compressed index, but always ﬁve times larger than the indexed text. Unlike counting, where compressed indexes are comparable in time with classical ones, locating is much slower on compressed indexes. This comes from the fact that each locate operation  except on the LZ-index  requires to perform several random memory accesses, depending on the sampling step. In contrast, all the occurrences are contiguous in a classical sufﬁx array. As a result, the compressed indexes are currently very efﬁcient in case of selective queries, but traditional indexes become more effective when locating many occurrences. This fact has triggered recent research activity on this subject but a deeper understanding on index performance on hierarchical memories is still needed.  6.4.4 Extract  We extracted substrings of length 512 from random text positions, for a total of 5 MB of extracted text. Figures 6.6 and 6.7 report the time space tradeoffs achieved by the tested indexes. We still include both space to locate and extract, but we note that the sampling step affects only the time to reach the text segment to extract from the closest sample, and afterwards the time is independent of the sampling. We chose length 512 to smooth out the effect of this sampling.  The comparison shows that, for extraction purposes, the CSA is better for sources and xml, whereas the SSA is better on dna and proteins. On english and pitches both are rather similar, albeit the CSA is able to oper- ate on reduced space. On the other hand, the LZ-index is much faster than the   6.4 Experimental Results  Fig. 6.6 Space-time tradeoffs for extracting text symbols  85  dna    r a h c   r e p   s c e s o r c m  i      e m  i t    r a h c   r e p    s c e s o r c m  i       e m  i t    r a h c   r e p    s c e s o r c m  i       e m  i t   5   4   3   2   1   0   5   4   3   2   1   0   5   4   3   2   1   0   0   0.2   0.6   0.4 Space usage  fraction of text    0.8   1   1.2   1.4  english   0   0.2   0.6   0.4 Space usage  fraction of text    0.8   1   1.2   1.4  pitches   0   0.2   0.6   0.4 Space usage  fraction of text    0.8   1   1.2   1.4   86  6 Experiments on Compressed Full-Text Indexing  Fig. 6.7 Space-time tradeoffs for extracting text symbols  proteins    r a h c   r e p   s c e s o r c m  i      e m  i t    r a h c   r e p    s c e s o r c m  i      e m  i t    r a h c   r e p    s c e s o r c m  i      e m  i t   5   4   3   2   1   0   5   4   3   2   1   0   5   4   3   2   1   0   0   0.2   0.6   0.4 Space usage  fraction of text    0.8   1   1.2   1.4  sources   0   0.2   0.6   0.4 Space usage  fraction of text    0.8   1   1.2   1.4  xml   0   0.2   0.6   0.4 Space usage  fraction of text    0.8   1   1.2   1.4   6.4 Experimental Results  87  others on xml, english and sources, if one is willing to pay some additional space.7  It is difﬁcult to compare these times with those of a classical index, because the latter has the text readily available. Nevertheless, we note that the times are rather good: using the same space as the text  and some times up to half the space  for all the functionalities implemented, the compressed indexes are able to extract around 1 MB sec, from arbitrary positions. This shows that self-indexes are appealing as compressed-storage schemes with the support of random accesses for snippet extraction.  6.4.5 Final Comparison  In Table 6.9 we summarize our experimental results by showing the most promising compressed index es  depending on the text type and task.  For counting, the best indexes are SSA and AF-index. This stems from the fact that they achieve very good zero- or high-order compression of the indexed text, while their average counting complexity is O  p H0 T   + 1  . The SSA has the advantage of a simpler search mechanism, but the AF-index is superior for texts with small high-order entropy  i.e. xml, sources, english . The CSA usually loses because of its O  p log n  counting complexity.  For locating and extracting, which are LF-computation intensive, AF-index is hardly better than simpler SSA because the beneﬁt of a denser sampling does not compensate for the presence of many wavelet trees. The SSA wins for small-alphabet data, like dna and proteins. Conversely, for all other high-order compressible texts the CSA is better than the other approaches. We also notice that the LZ-index is a very competitive choice when extra space is allowed and the texts are highly compressible.  The ultimate moral is that there is not a clear winner for all text collections, and that this is not to be taken as a static, ﬁnal result, because the area is developing fast. Nonetheless, our current results provide an upper bound on what these compressed indexes can achieve in practice: Count We can compress the text within 30–50 % of its original size, and search for  20,000–50,000 patterns of 20 chars each within a second.  Locate We can compress the text within 40–80 % of its original size, and locate  about 100,000 pattern occurrences per second.  Extract We can compress the text within 40–80 % of its original size, and decompress  its symbols at a rate of about 1 MB second.  The above ﬁgures show that the compressed full-text indexes are from one  count  to three  locate  orders of magnitudes slower than what one can achieve with a plain  7 Actually the LZ-index is not plotted for pitches and proteins because it needs more than 1.5 times the text size.   6 Experiments on Compressed Full-Text Indexing  88  Count  Locate  Extract  Table 6.9 The most promising indexes given the size and time they obtain for each operation text  DNA  English  Pitches  Proteins  Sources  XML  SSA - LZ-index SSA SSA -  SSA AF-index CSA LZ-index CSA LZ-index  AF-index SSA CSA - CSA -  SSA - SSA - SSA -  CSA AF-index CSA LZ-index CSA LZ-index  AF-index - CSA LZ-index CSA LZ-index  sufﬁx array, at the beneﬁt of using up to 18 times less space. This slowdown is due to the fact that search operations in compressed indexes access the memory in a non- local way thus eliciting many cache IO misses, with a consequent degradation of the overall time performance. Nonetheless compressed indexes achieve a  search extract  throughput which is signiﬁcant and may match the efﬁciency speciﬁcations of most software tools which run on a commodity PC.   Chapter 7 Dictionary Indexes  String processing and searching tasks are at the core of modern Web search, information retrieval and data mining applications. Most of such tasks boil down to some basic algorithmic primitives which involve a large dictionary of strings hav- ing variable length. Typical examples include: pattern matching  exact, approximate, with wild-cards, . . . , the ranking of a string in a sorted dictionary, or the selection of the i-th string from it. While it is easy to imagine uses of pattern matching prim- itives in real applications, such as search engines and text mining tools, rank select operations appear uncommon. However they are quite often used  probably, uncon- sciously!  by programmers to replace long strings with unique IDs which are easier and faster to be processed and compressed. In this context ranking a string means mapping it to its unique ID, whereas selecting the i-th string means retrieving it from its ID  i.e. its ranked position i .  As strings are getting longer and longer, and dictionaries of strings are getting larger and larger, it becomes crucial to devise implementations for the above prim- itives which are fast and work in compressed space. This is the topic of the present chapter that actually addresses the design of compressed data structures for the so called tolerant retrieval problem, deﬁned as follows  Manning et al. 2008 . Let S be a sorted dictionary of m strings having total length n and drawn from an arbitrary alphabet ε of size σ. The tolerant retrieval problem consists of preprocessing S in order to efﬁciently support the following WildCard P  query operation: search . Symbol √ is the so for the strings in S which match the pattern P ≤  ε ∈ {√} + called wild-card symbol, and matches any substring of ε√ . In principle, the pattern P might contain several occurrences of √; however, for practical reasons, it is common to restrict the attention to the following signiﬁcant cases:   Membership query determines whether a pattern P ≤ ε+ occurs in S. Here P does not include wild-cards.   Prefix query determines all strings in S which are preﬁxed by string α. Here P = α√ with α ≤ ε+   Suffix query determines all strings in S which are sufﬁxed by string β. Here P = √β with β ≤ ε+  .  .  R. Venturini, Compressed Data Structures for Strings, Atlantis Studies in Computing 4, DOI: 10.2991 978-94-6239-033-1_7,   Atlantis Press and the authors 2014  89   90  7 Dictionary Indexes  .    Substring query determines all strings in S which have γ as a substring. Here P = √γ√ with γ ≤ ε+   PrefixSuffix query is the most sophisticated one and asks for all strings in S that are preﬁxed by α and sufﬁxed by β. Here P = α √ β with α, β ≤ ε+ We extend the tolerant retrieval problem to include the following two basic primitives:   RankString P  computes the rank of string P ≤ ε+ nary S.   SelectString i  retrieves the i-th string of the  sorted  dictionary S.  within the  sorted  dictio-  .  There are two classical approaches to string searching: Hashing and Tries  Baeza-Yates and Ribeiro-Neto 1999 . Hashing supports only the exact Membership query; its more sophisticated variant called minimal ordered perfect hashing  Witten et al. 1999  supports also the RankString operation but only on strings of S. All other queries need however the inefﬁcient scan of the whole dictionary!  Tries are more powerful in searching than hashing, but they introduce extra space and fail to provide an efﬁcient solution to the PrefixSuffix query. In fact, the search for P = α √ β needs to visit the subtrie descending from the trie-path labeled α, in order to ﬁnd the strings that are sufﬁxed by β. Such a brute-force visit may cost  cid:3  S  time independently of the number of query answers  cfr. Baeza-Yates and Gonnet 1996 . We can circumvent this limitation by using the sophisticated approach proposed in Ferragina et al.  2003  which builds two tries, one storing the strings of S and the other storing their reversals, and then reduce the PrefixSuffix query to a geometric 2D-range query, which is eventually solved via a proper efﬁcient geometric data structure in O α + β + polylog n   time. The overall space occupancy would be  cid:3  n log n  bits, with a large constant hidden in the big-O notation due to the presence of the two tries and the geometric data structure.  Recently Manning et al.  2008  resorted the Permuterm index of Garﬁeld  1976  as a time-efﬁcient and elegant solution to the tolerant retrieval problem above. The idea is to take every string s ≤ S, append a special symbol $, and then consider all the cyclic rotations of s$. The dictionary of all rotated strings is called the permuterm dictionary, and is then indexed via any data structure that supports preﬁx searches, e.g. the trie. The key to solve the PrefixSuffix query is to rotate the query string α √ β$ so that the wild-card symbol appears at the end, namely β$α√. Finally, it sufﬁces to perform a preﬁx-query for β$α over the permuterm dictionary. As a result, the Permuterm index allows to reduce any query of the Tolerant Retrieval problem on the dictionary S to a preﬁx query over its permuterm dictionary. The limitation of this elegant approach relies in its space occupancy, as “its dictionary becomes quite large, including as it does all rotations of each term”  Manning et al. 2008 . In practice, one memory word per rotated string  and thus 4 bytes per symbol  is needed to index it, for a total of σ n log n  bits.  In this chapter we propose the Compressed Permuterm Index which solves the tolerant retrieval problem in time proportional to the length of the queried string P, and space close to the k-th order empirical entropy of the dictionary S. The time complexity matches the one achieved by the  uncompressed  Permuterm index. The   7 Dictionary Indexes  91  space complexity is close to the k-th order empirical entropy of the dictionary S. In addition, we devise a dynamic Compressed Permuterm Index that is able to maintain the dictionary S under insertions and deletions of an individual string s in O s 1 + log σ  log log n  log n  time. All query operations are slowed down by a multiplicative factor of at most O  1 + log σ  log log n  log n . The space occupancy is still close to the k-th order empirical entropy of the dictionary S.  Our result is based on a variant of the Burrows-Wheeler Transform here extended to work on a dictionary of strings of variable length. We prove new properties of such BWT, and show that known  dynamic  compressed indexes may be easily adapted to solve efﬁciently the  dynamic  Tolerant Retrieval problem.  We ﬁnally complement our theoretical study with a signiﬁcant set of experiments over large dictionaries of URLs, hosts and terms, and compare our Compressed Permuterm index against some classical approaches to the Tolerant Retrieval problem mentioned in Manning et al.  2008 , Witten et al.  1999  such as tries and front-coded dictionaries. Experiments will show that tries are fast but much space consuming; conversely our compressed permuterm index allows to trade query time by space occupancy, resulting as fast as Front-Coding in searching the dictionary but more than 50 % smaller in space occupancy—thus being close to gzip, bzip2 and ppmdi. This way the compressed permuterm index offers a plethora of solutions for the Tolerant Retrieval problem which may well adapt to different applicative scenarios.  7.1 Background  The Backward Search algorithm of the FM-index family has been already intensiv- elly discussed in Sect. 6.2.1 . Since our solution is based on it, we report here the pseudocode of this algorithm  Fig. 7.1 . Chapter 6 was primarly focused on practical solutions, thus, we presented only practical solutions to provide Rank and Select queries. The literature offers also many theoretical solutions for this problem having often better time space bounds  see e.g. Navarro and Mäkinen  2007 , Barbay et al.  2007  and references therein . We do not want to enter into details on this topic and, thus, we just summarize below the ones having best bounds. Lemma 7.1 Let T[1, n] be a string over alphabet ε of size σ and let L = Bwt T   be its BW-transform.  1  For σ = O polylog n  , there exists a data structure which supports Rank queries and the retrieval of any symbol of L in constant time, by using nHk T  + o n  bits of space, for any k ≤ α logσ n and 0 < α < 1  Ferragina et al. 2007  [Theorem 5].  2  For general ε, there exists a data structure which supports Rank queries and the retrieval of any symbol of L in O log log σ  time, by using nHk T   + n · o log σ  bits of space, for any k ≤ α logσ n and 0 < α < 1 [Barbay et al.  2007 ][Theorem 4.2].   92  7 Dictionary Indexes  Fig. 7.1 The algorithm to ﬁnd the range [First, Last] of rows of MT preﬁxed by Q[1, q]  By plugging Lemma 7.1 into Backward_search, the authors of Ferragina et al.  2007 , Barbay et al.  2007  obtained: Theorem 7.1 Given a text T[1, n] drawn from an alphabet ε of size σ, there exists a compressed index that takes q× trank time to support Backward_search Q[1, q] , where trank is the time cost of a single Rank operation over L = Bwt T  . The space usage is bounded by nHk T   + lspace bits, for any k ≤ α logσ n and 0 < α < 1, where lspace is o n  when σ = O polylog n   and n · o log σ  otherwise.  As we have already seen in Chap. 6, compressed indexes support also other oper- ations, like locate and display of pattern occurrences, which are slower than Back- ward_search in that they require polylog n  time per occurrence. One positive feature of our compressed permuterm index is that it will not need these  sophisti- cated  data structures, and thus it will not incur in this polylog-slowdown.  7.2 Compressed Permuterm Index  The way in which the Permuterm dictionary is computed, immediately suggests that there should be a relation between the BWT and the Permuterm dictionary of the string set S. In both cases we talk about cyclic rotations of strings, but in the former we refer to just one string, whereas in the latter we refer to a dictionary of strings of possibly different lengths. The notion of BWT for a set of strings has been considered in Mantaci et al.  2005  for the purpose of string compression and comparison. Here, we are interested in the compressed indexing of the string dictionary S, which introduces more challenges. Surprisingly enough the solution we propose is novel, simple, and efﬁcient in time and space; furthermore, it admits an effective dynamization.   7.2 Compressed Permuterm Index  93  7.2.1 A Simple, but Inefﬁcient Solution Let S = {s1, s2, . . . , sm} be the lexicographically sorted dictionary of strings to be indexed. Let $ resp.   be a symbol smaller  resp. larger  than any other symbol of ε. We consider the doubled strings cid:2 si = si$si. It is easy to note that any pattern searched by PrefixSuffix P  matches si if, and only if, the rotation of P mentioned in the Introduction is a substring of cid:2 si. For example, the query PrefixSuffix α √ β  matches si iff the rotated string β$α occurs as a substring of cid:2 si. Consequently, the simplest approach to solve the Tolerant Retrieval problem with compressed indexes seems to boil down to the indexing of the string  cid:2 SD =  cid:2 s1 cid:2 s2 ···  cid:2 sm by means of the data structure of Theorem 7.1. Unfortunately, this approach suffers of subtle inefﬁciencies in the indexing and searching steps. To see them, let us “compare” string  cid:2 SD against string SD = $s1$s2$ . . . $sm−1$sm$, which is a serialization of the dictionary S  and it will be at the core of our approach, see below . We note that the “duplication” of si within  cid:2 si:  1  doubles the string to be indexed, because  cid:2 SD = 2SD − 1; and  2  doubles the space bound of compressed indexes evaluated in Theorem 7.1, because  cid:2 SDHk  cid:2 SD  ∼= 2SDHk SD  ± m k log σ + 2 , where the second term comes from the presence of symbol  which introduces new k-long substrings in the computation of Hk  cid:2 SD . Point  1  is a limitation for building large static compressed indexes in practice, being their construction space a primary concern  Puglisi et al. 2007 ; point  2  will be experimentally investigated in Sect. 7.4 where we show that a compressed index built on  cid:2 SD may be up to 1.9 times larger than a compressed index built on SD.  7.2.2 A Simple and Efﬁcient Solution  Unlike the previous solution, our Compressed Permuterm index works on the plain string SD, and is built in three steps  see Fig. 7.2 :  1  Build the string SD = $s1$s2$ . . . $sm−1$sm$. Recall that the dictionary strings are lexicographically ordered, and that symbol $ resp.   is assumed to be smaller  resp. larger  than any other symbol of ε.   2  Compute L = Bwt SD .  3  Build a compressed data structure to support Rank queries over the string L   Lemma 7.1 .  Our goal is to turn every wild-card search over the dictionary S into a substring search over the string SD. Some of the required queries are immediately imple- mentable as substring searches over SD  and thus they can be supported supported by procedure Backward_search and the RankString data structure built on L . But the sophisticated PrefixSuffix query needs a different approach because it requires to simultaneously match a preﬁx and a sufﬁx of a dictionary string, which are possibly far apart from each other in SD. In order to circumvent this limitation,   7 Dictionary Indexes  94  Fig. 7.2 Given the dictionary S = {hat, hip, hope, hot}, we build the string SD = $hat$hip$hope$hot$, and then compute its BW- transform. Arrows denote the positions incremented by the function jump2end  we prove a novel property of Bwt SD  and deploy it to design a function, called jump2end, that allows to modify the procedure Backward_search of Fig. 7.1 in a way that is suitable to support efﬁciently the PrefixSuffix query. The main idea is that when Backward_search reaches the beginning of some dictionary string, say si, then it “jumps” to the last symbol of si rather than continuing onto the last symbol of its previous string in S, i.e. the last symbol of si−1. Surprisingly enough, function jump2end i  consists of one line of code:  if 1 ≤ i ≤ m then return i+1  else return i   and its correctness derives from the following two Lemmas.  Refer to Fig. 7.2 for an illustrative example.   Lemma 7.2 Given the sorted dictionary S, and the way string SD is built, matrix MSD satisﬁes the following properties:   The ﬁrst row of MSD is preﬁxed by $s1$, thus it ends with symbol L[1] = .   For any 2 ≤ i ≤ m, the i-th row of MSD is preﬁxed by $si$ and thus it ends with the last symbol of si−1, i.e. L[i] = si−1[si−1].   The  m + 1 -th row of MSD is preﬁxed by $$s1$, and thus it ends with the last symbol of sm, i.e. L[m + 1] = sm[sm].   7.2 Compressed Permuterm Index  95  Proof The three properties come from the sorted ordering of the dictionary strings in SD, from the fact that symbol $  resp.   is the smallest  resp. largest  alphabet symbol, from the cyclic rotation of the rows in MSD , and from their lexicographic  cid:2  ordering.  The Lemma 7.1 immediately implies the “locality” property deployed by function jump2end i : Lemma 7.3 Any row i ≤ [1, m] is preﬁxed by $si$ and the next row  i + 1  ends with the last symbol of si.  We are now ready to design the procedures for pattern searching and for dis- playing the strings of S. As we anticipated above the main search procedure, called BackPerm_search, is derived from the original Backward_searchof Figure 7.1 by adding one step which makes proper use of jump2end: : First = jump2end First ; Last = jump2end Last ; ⊆ 3 It is remarkable that the change is minimal  just one line of code!  and takes constant time, because jump2end takes O 1  time. Let us now comment on the correctness of the new procedure BackPerm_search β$α  in solving the sophisti- cated query PrefixSuffix α√ β . We note that BackPerm_search proceeds as the standard Backward_search for all symbols Q[i] ⇒= $. In fact, the rows involved in these search steps do not belong to the range [1, m], and thus jump2end is ineffec- tive. When Q[i] = $, the range [First, Last] is formed by rows which are preﬁxed by $α. By Lemma 7.3 we know that these rows are actually preﬁxed by strings $sj, with j ≤ [First, Last], and thus these strings are in turn preﬁxed by $α. Given that moves this range of rows to [First + 1, Last + 1], [First, Last] ≥ [1, m], Step 3 ⊆ and thus identiﬁes the new block of rows which are ended by the last symbols of those strings sj  Lemma 7.3 . After that, BackPerm_search continues by scanning backward the symbols of β  no other $ symbol is involved , thus eventually ﬁnding the rows preﬁxed by β$α.  Figure 7.3 shows the pseudo-code of two other basic procedures: Back_step i  and Display_string i . The former procedure is a slight variation of the backward step implemented by any current compressed index based on BWT  see Chap. 6 , here modiﬁed to support a leftward cyclic scan of every dictionary string. Precisely, if F[i] is the j-th symbol of some dictionary string s, then Back_step i  returns the row preﬁxed by the  j − 1 -th symbol of that string if j > 1  this is a standard backward step , otherwise it returns the row preﬁxed by the last symbol of s  by means of jump2end . Procedure Display_string i  builds upon Back_step i  and retrieves the string s, namely the dictionary string that contains the symbol F[i]. string L = Bwt SD , we obtain: Theorem 7.2 Let SD be the string built upon a dictionary S of m strings having total length n and drawn from an alphabet ε of size σ, such that σ = polylog n . We can design a Compressed Permuterm index such that:  Using the data structures of Lemma 7.1 for supporting Rank queries over the   96  7 Dictionary Indexes  Fig. 7.3 Algorithm Back_step is the one devised in Ferragina and Manzini  2005  for standard compressed indexes. Algorithm Display_string i  retrieves the string containing the symbol F[i]    Procedure Back_step i  takes O 1  time.   Procedure BackPerm_search Q[1, q]  takes O q  time.   Procedure Display_string i  takes O s  time, if s is the string containing symbol F[i]. Space occupancy is bounded by nHk SD  + o n  bits, for any k ≤ α logσ n and 0 < α < 1.  Proof For the time complexity, we observe that function jump2end takes constant time, and it is invoked O 1  times at each possible iteration of procedures Back- Perm_search and Display_string. Moreover, Back_step takes constant time, by Lemma 7.1. For the space complexity, we use the data structure of Lemma 7.1  case 1  to support Rank queries on the string L = Bwt SD .  cid:2  If σ = σ polylog n  , the above time bounds must be multiplied by a factor O log log σ  and the space bound has an additive term of n · o log σ  bits  Lemma 7.1, case 2 .  We are left with detailing the implementation of WildCard, RankString and SelectString queries for the Tolerant Retrieval problem. As it is standard in the Compressed Indexing literature we distinguish between two sub-problems: count- ing the number of dictionary strings that match the given wild-card query P, and retrieving these strings.  Based on the Compressed Permuterm index of Theorem 7.2 we have:    Membership query invokes procedure BackPerm_search $P$ , then simply checks if First < Last.   Prefix query invokes procedure BackPerm_search $α  and returns the value Last − First + 1 as the number of dictionary strings preﬁxed by α. These strings can be retrieved by applying Display_string i , for each i ≤ [First, Last].   7.2 Compressed Permuterm Index  97    Suffix query invokes procedure BackPerm_search β$  and returns the value Last − First + 1 as the number of dictionary strings sufﬁxed by β. These strings can be retrieved by applying Display_string i , for each i ≤ [First, Last].   Substring query invokes procedure BackPerm_search γ  and returns the value Last − First + 1 as the number of occurrences of γ as a substring of S’s strings.1 Unfortunately, the efﬁcient retrieval of these strings cannot be through the exe- cution of Display_string, as we did for the queries above. A dictionary string s may now be retrieved multiple times if γ occurs many times as a substring of s. To circumvent this problem we design a simple time-optimal retrieval, as follows. We use a bit vector V of size Last − First + 1, initialized to 0. The execution of Display_string is modiﬁed so that V[j − First] is set to 1 when a row j within the range [First, Last] is visited during its execution. In order to retrieve once all dictionary strings that contain γ, we scan through i ≤ [First, Last] and invoke the modiﬁed Display_string i  only if V[i − First] = 0. It is easy to see that if i1, i2, . . . , ik ≤ [First, Last] are the rows of MSD denoting the occurrences of γ in some dictionary string s  i.e. F[ij] is a symbol of s , only Display_string i1  is fully executed, thus taking O s  time. For all the other rows ij, with j > 1, we ﬁnd V[ij − First] = 1 and thus Display_string ij  is not invoked.   PrefixSuffix query invokes BackPerm_search β$α  and returns the value Last − First + 1 as the number of dictionary strings which are preﬁxed by α and sufﬁxed β. These strings can be retrieved by applying Display_string i , for each i ≤ [First, Last].   RankString  P  invokes BackPerm_search $P$  and returns the value First, if First < Last, otherwise P ⇒≤ S  see Lemma 7.2  and thus the lexicographic position of P in S can be discovered by means of a slight variant of Backward_search whose details are given in Fig. 7.5  see Sect. 7.3.2 for further comments .   SelectString  i  invokes Display_string i  provided that 1 ≤ i ≤ m  see Lemma 7.2 . Theorem 7.3 Let S be a dictionary of m strings having total length n, drawn from an alphabet ε of size σ such that σ = polylog n . Our Compressed Permuterm index ensures that:   If P[1, p] is a pattern with one-single wild-card, the query WildCard P  takes O p  time to count the number of occurrences of P in S, and O Locc  time to retrieve the dictionary strings matching P, where Locc is their total length.   Substring γ  takes O γ  time to count the number of occurrences of γ as a substring of S’s strings, and O Locc  time to retrieve the dictionary strings having γ as a substring, where Locc is their total length.   RankString P[1, p]  takes O p  time.   SelectString i  takes O si  time. The space occupancy is bounded by nHk SD  + o n  bits, for any k ≤ α logσ n and 0 < α < 1.  1 This is different from the problem of efﬁciently counting the number of strings containing γ. Our index does not solve this interesting problem  cfr. Sadakane  2007  and references therein .   98  7 Dictionary Indexes  According to Lemma 7.1  case 2 , if σ = σ polylog n   the above time bounds must be multiplied by O log log σ  and the space bound has an additive term of n · o log σ  bits. We remark that our Compressed Permuterm index can support all wild-card searches without using any locate-data structure, which is known to be the main bottleneck of current compressed indexes  Navarro and Mäkinen 2007 : it implies the polylog-term in their query bounds and most of the o n log σ  term of their space cost. The net result is that our Compressed Permuterm index achieves in practice space occupancy much closer to known compressors and very fast queries, as we will experimentally show in Sect. 7.4. A comment is in order at this point. Instead of introducing function jump2end and then modify the Backward_search procedure, we could have modiﬁed L = Bwt SD  just as follows: cyclically rotate the preﬁx L[1, m + 1] of one single step  i.e. move L[1] =  to position L[m + 1] . This way, we are actually plugging Lemma 7.3 directly into the string L. It is thus possible to show that the compressed index of Theorem 7.1 applied on the rotated L, is equivalent to the compressed permuterm index introduced above. The performance in practice of this variation are slightly better since the computation of jump2end is no longer required. This is the implementation we used in the experiments of Sect. 7.4.  7.3 Dynamic Compressed Permuterm Index  In this section we deal with the dynamic Tolerant Retrieval problem in which the dictionary S changes over the time under two update operations:   InsertString W   inserts the string W in S.   DeleteString j  removes the j-th lexicographically smallest string sj from S. The problem of maintaining a compressed index over a dynamically changing collection of strings, has been addressed in e.g. Ferragina and Manzini  2005 , Chan et al.  2007 , Mäkinen and Navarro  2007 . In those papers the design of dynamic Compressed Indexes boils down to the design of dynamic compressed data structures for supporting Rank Select operations. Here we adapt those solutions to the design of our dynamic Compressed Permuterm Index by showing that the inser- tion deletion of an individual string s in from S can be implemented via an optimal number O s  of basic insert delete operations of single symbols in the compressed Rank  Select data structure built on L = Bwt SD . Precisely, we will consider the following two basic update operations:   Insert L, i, c  inserts symbol c between symbols L[i] and L[i + 1].   Delete L, i  removes the ith symbol L[i]. The literature provides several dynamic data structures for supporting Rank queries and the above two update operations, with various time space trade-offs. The best known results are currently due to González and Navarro  2008 :   7.3 Dynamic Compressed Permuterm Index  99  Lemma 7.4 Let S[1, s] be a string drawn from an alphabet ε of size σ and let L = Bwt S  be its BW-Transform. There exists a dynamic data structure that supports Rank, Select and Access operations in L taking O  1+log σ  log log s  log s  time, and maintains L under insert and delete operations of single symbols in O  1 + log σ  log log s  log s  time. The space required by this data structure is nHk S  + o n log σ  bits, for any k < α logσ s and constant 0 < α < 1.  Our dynamic Compressed Permuterm Index is designed upon the above dynamic data structures, in a way that any improvement to Lemma 7.4 will positively reﬂect onto an improvement to our bounds. Therefore we will indicate the time complexities of our algorithms as a function of the number of Insert and Delete operations executed onto the changing string L = Bwt SD . We also notice that these operations will change not only L but also the string F  which is the lexicographically sorted version of L, see Sect. 2.4.2 . The maintenance of L will be discussed in the next subsections; while for F we will make use of the solution proposed in Mäkinen and Navarro  2007  [Section 7] that takes σ log s + o σ log s  bits and implements in O log s  time the following query and update operations: C[c] returns the number of symbols in F smaller than c; deleteF c  removes from F an occurrence of symbol c; and insertF c  adds an occurrence of symbol c in F.  The next two subsections detail our implementations of InsertString and DeleteString. The former is a slight modiﬁed version of the algorithm introduced in Chan et al.  2007 , here adapted to deal with the specialties of our dictionary prob- lem: namely, the dictionary strings forming SD must be kept in lexicographic order. The latter coincides with the algorithm presented in Mäkinen and Navarro  2007  for which we prove an additional property  Lemma 7.5  which is a key for using this result as is in our context.  7.3.1 Deleting One Dictionary String  The operation DeleteString j  requires to delete the string sj from the dic- ⊆ = tionary S, and thus recompute the BW-transform L $s1$ . . . $sj−1$sj+1$ . . . $sm$. The key property we deploy next is that this removal does not impact on the ordering of the rows of MSD which do not refer to sufﬁxes of $sj.  of the new string SD  ⊆  Lemma 7.5 The removal from L of the symbols of $sj gives the correct string ⊆ . Bwt SD It is enough to prove that the removal of $sj will not inﬂuence the order Proof as two rows which are not between any pair of rows i deleted from MSD , and thus do not start end with symbols of $sj. We compare the -th row, say Si⊆, and the sufﬁx of SD corresponding sufﬁx of SD corresponding to the i -th row, say Si⊆⊆. We recall that these are increasing strings, in that they are to the i composed by the dictionary strings which are arranged in increasing lexicographic  in MSD . Take i  ⊆ < i  ⊆, i  ⊆⊆  ⊆⊆  ⊆⊆  ⊆   100  7 Dictionary Indexes  Fig. 7.4 Algorithm to delete the string $sj from SD  ⊆  ⊆⊆  $s  $s  $ and α⊆⊆  $, respectively, where α⊆, α⊆⊆ ⊆, s  order and they are separated by the special symbol $  see Sect. 7.2.2 . Since all dictionary strings are distinct, the mismatch between Si⊆ and Si⊆⊆ occurs before the second occurrence of $ in them. Let us denote the preﬁx of Si⊆ and Si⊆⊆ preceding ⊆⊆ the second occurrence of $ with α⊆ are are dictionary strings. If the  possibly empty  sufﬁxes of dictionary strings, and s mismatch occurs in α⊆ we are done, because they are not sufﬁxes of $sj  by the assumption , and therefore they are not interested by the deletion process. If the ⊆ and they are both different of sj, we are also done. The mismatch occurs in s ⊆⊆ = sj. We consider the ﬁrst case, because the second trouble is when s is similar. This case occurs when α⊆ = α⊆⊆, so that the order between Si⊆ and Si⊆⊆ ⊆ = sj, then the order of the two rows is ⊆⊆ . If s is given by the order of s versus s ⊆ < s ⊆⊆  because Si⊆ < Si⊆⊆  and sj+1 is then given by comparing sj+1 and s . Since s , we have that sj+1 ≤ s ⊆ , and the thesis the smallest dictionary string greater than s  cid:2  follows.  or α⊆⊆ ⊆⊆ or s ⊆ = sj or s ⊆  ⊆⊆  ⊆⊆  Given this property, we can use the same string-deletion algorithm of Mäkinen and Navarro  2007  to remove all symbols of $sj from L and F.  Fig. 7.4 reports the pseudo-code of this algorithm, for the sake of completeness .  7.3.2 Inserting One Dictionary String  An implementation of InsertString W   for standard compressed indexes was described in Chan et al.  2007 . Here we present a slightly modiﬁed version of that algorithm which correctly deals with the maintenance of the lexicographic ordering of the dictionary strings in SD, and the re-computation of its BW-transform. We recall that this order is crucial for the correctness of most of our query operations. Let j be the lexicographic position of the string W in S. InsertString W   ⊆ = $s1$ . . . $sj−1$W$ requires to recompute the BWT L sj$sj+1$ . . . $sm$. For this purpose, we can use the reverse of Lemma 7.5 in order to infer that this insertion does not affect the ordering of the rows already in MSD . Thus InsertString W   boils down to insert just the symbols of W in their correct  of the new string SD  ⊆   7.3 Dynamic Compressed Permuterm Index  101  Fig. 7.5 Algorithm LexOrder W[1, w]  returns the lexicographic position of W in S  positions within L  and, accordingly, in F . This is implemented in two main steps: ﬁrst, we ﬁnd the lexicographic position of W in S  Algorithm LexOrder W   ; and then, we deploy this position to infer the positions in L where all symbols of W have to be inserted  Algorithm InsertString .  ⊆  The pseudo-code in Fig. 7.5 details algorithm LexOrder W   which assumes that any symbol of W already occurs in the dictionary strings. If this is not the case, we set c = W[x] as the leftmost symbol of W which does not occur in any string of S, as the smallest symbol which is lexicographically greater than c and occurs and set c in S. If LexOrder is correct, then LexOrder W[1, x−1]c ⊆  returns the lexicographic position of W in S. Lemma 7.6 Given a string W[1, w] whose symbols occurs in S, LexOrder W   returns the lexicographic position of W among the strings in S.  Its correctness derives from the correctness of Backward_search. At any Proof step i, First points to the ﬁrst row of MSD which is preﬁxed by the sufﬁx W[w − i, w]$. If such a row does not exist, First points to the ﬁrst row of MSD which is lexicographically greater than W[w − i, w]$.  cid:2   ⊆ = Bwt SD  Now we have all the ingredients to describe algorithm InsertString W  . Sup- pose that j is the value returned by LexOrder W[1, w] . We have to insert the symbol W[i] preceding any sufﬁx W[i + 1, w] in its correct position of L ⊆  and update the string F too. The algorithm in Fig. 7.6 starts from the last symbol W[w], and inserts it at the  j + 1 -th position of Bwt SD   by Lemma 7.3 . It also inserts the symbol $ in F, since it is the ﬁrst symbol of the  j + 1 -th row. After that, the algorithm performs a backward step from the  j + 1 -th row with the symbol W[w] in order to ﬁnd the position in L where W[w − 1] should be inserted. Accordingly, the symbol W[w] is inserted in F too. These insertions are executed in L and F until all positions of W are processed. Step 7 completes the process by inserting the special symbol $. Overall, InsertString executes an optimal number of inserts of single symbols in L and F. We then use the dynamic data structures of Lemma 7.4 to dynamically maintain L, and the solution of Mäkinen and Navarro  2007  [Section 7] to maintain F, thus obtaining:   102  7 Dictionary Indexes  Fig. 7.6 Algorithm to insert string $W[1, w] by knowing its lexicographically order j among the strings in S  Theorem 7.4 Let S be a dynamic dictionary of m strings having total length n, drawn from an alphabet ε of size σ. The Dynamic Compressed Permuterm index supports all queries of the Tolerant Retrieval problem with a slowdown factor of O  1 + log σ  log log n  log n  with respect to its static counterpart  see Theorem 7.3 . Addi- tionally, it can support InsertString W   in O W 1+log σ  log log n  log n  time; and DeleteString j  in O sj 1 + log σ  log log n  log n   time. The space occupancy is bounded by nHk SD  + o n log σ  bits, for any k ≤  We point out again that any improvement to Lemma 7.4 will positively affect the  α logσ n and 0 < α < 1.  dynamic bounds above.  7.4 Experimental Results  We downloaded from http:  law.dsi.unimi.it  various crawls of the web—namely, arabic-2005, indocina-2004, it-2004, uk-2005, webbase-2001  Boldi et al. 2004 . We extracted from uk-2005 about 190 Mb of distinct urls, and we derived from all crawls about 34 Mb of distinct host-names. The dictionary of urls and hosts have been lexicographically sorted by reversed host names in order to maximize the longest common-preﬁx  shortly, lcp  shared by strings adjacent in the lexicographic order. We have also built a dictionary of  alphanumeric  terms by parsing the TREC collection WT10G and by dropping  spurious  terms longer than 50 symbols. These three dictionaries are representatives of string sets usually manipulated in Web search and mining engines.  Table 7.1 reports some statistics on these three dictionaries: DictUrl  the dictio- nary of urls , DictHost  the dictionary of hosts , and DictTerm  the dictionary of terms . In particular lines 3–5 describe the composition of the dictionaries at the string level, lines 6–8 account for the repetitiveness in the dictionaries at the string- preﬁx level  which affects the performance of front-coding and trie, see below , and   7.4 Experimental Results  103  Table 7.1 Statistics on our three dictionaries Statistics DictUrl Size  Mb  σ  strings Avg_len strings Max_len strings Avg_lcp Max_lcp Total_lcp gzip -9 bzip2 -9 ppmdi -l 9  190 95 3,034,144 64.92 1,138 45.85 720 68.81 % 11.49 % 10.86 % 8.32 %  DictHost  DictTerm  34 52 1,778,927 18.91 180 11.25 69 55.27 % 23.77 % 24.03 % 19.08 %  118 36 10,707,681 10.64 50 6.81 49 58.50 % 29.50 % 32.58 % 29.06 %  Table 7.2 Space occupancy is reported as a percentage of the original dictionary size Method Trie FC-32 FC-128 FC-1024 CPI-AFI CPI-CSA-64 CPI-CSA-128 CPI-CSA-256 CPI-FMI-256 CPI-FMI-512 CPI-FMI-1024 Recall that Trie and Fc are built on both the dictionary strings and their reversals, in order to support PrefixSuffix queries  DictHost  %  1793.19 113.22 109.91 108.94 47.48 56.36 50.11 46.99 40.68 34.58 31.45  DictTerm  %  1727.93 106.45 102.10 100.84 52.24 73.98 67.73 64.61 55.41 47.80 44.13  DictUrl  %  1374.29 109.95 107.41 106.67 49.72 37.82 31.57 28.45 24.27 18.94 16.12  the last three lines account for the repetitiveness in the dictionaries at the sub-string level  which affects the performance of compressed indexes . It is interesting to note that the Total_lcp varies between 55–69 % of the dictionary size, whereas the amount of compression achieved by gzip, bzip2 and ppmdi is superior and reaches 67–92 %. This proves that there is much repetitiveness in these dictionaries not only at the string-preﬁx level but also within the strings. The net consequence is that compressed indexes, which are based on the Burrows-Wheeler Transform  and thus have the same bzip2-core , should achieve on these dictionaries signiﬁcant compression, much better than the one achieved by front-coding based schemes!  In Tables 7.2 and 7.3 we test the time and space performance of three  compressed   solutions to the Tolerant Retrieval problem:  CPI is our Compressed Permuterm Index of Sect. 7.2.2. In order to compress the string SD and implement procedures BackPerm_search and Display_string,   104  7 Dictionary Indexes  DictTerm 5  Table 7.3 Timings are given in μ secs char averaged over one million of searched patterns, whose length is reported at the top of each column Method  DictUrl 10 0.1 1.3 3.2 26.6 1.8 4.9 7.3 11.8 11.9 16.2 24.1  Trie FC-32 FC-128 FC-1024 CPI-AFI CPI-CSA-64 CPI-CSA-128 CPI-CSA-256 CPI-FMI-256 CPI-FMI-512 CPI-FMI-1024 Value b denotes in CPI-FMI-b the bucket size of the FM-index, in CPI-CSA-b the sample rate of the function λ  Ferragina et al. 2008a , and in FC-b the bucket size of the front-coding scheme. We recall that b allows in all these solutions to trade space occupancy per query time  1.2 2.5 4.6 25.0 2.9 5.4 7.6 12.8 22.5 34.2 57.6  DictHost 5 0.4 1.5 3.4 24.6 1.6 4.3 6.9 11.8 19.3 28.4 46.4  15 0.5 1 1.8 11.0 2.5 5.2 7.6 12.5 15.5 23.1 38.4  10 0.9 1.7 2.8 14.6 3.0 5.7 8.3 13.2 20.1 30.3 50.1  60 0.2 0.4 1.0 5.2 2.9 5.6 8.0 14.1 9.8 13.4 20.7  we modiﬁed three types of compressed indexes available under the Pizza and Chili site  Ferragina et al. 2008a  and discussed in Chap. 6, which represent the best choices in this setting. Namely CSA, FM-index v2  shortly FMI , and the alphabet-friendly FM-index  shortly AFI . We tested three variants of CSA and FMI by properly setting their parameter which allows to trade space occupancy by query performance.  FC data structure applies front-coding to groups of b adjacent strings in the sorted dictionary, and then keeps explicit pointers to the beginners of every group  Witten et al. 1999 .  Trie is the ternary search tree of Bentley and Sedgewick which “combines the time efﬁciency of digital tries with the space efﬁciency of binary search trees”  Bentley and Sedgewick 1997 .2  Theorem 7.3 showed that Cpi supports efﬁciently all queries of the Tolerant Retrieval problem. The same positive feature does not hold for the other two data structures. In fact Fc and Trie support only preﬁx searches over the indexed strings. Therefore, in order to implement the PrefixSuffix query, we need to build these data structures twice—one on the strings of S and the other on their reversals. This doubles the space occupancy, and slows down the search performance because we need to ﬁrst make two preﬁx-searches, one for P’s preﬁx α and the other for P’s sufﬁx β, and then we need to intersect the two candidate lists of answers. If we wish to also support the rank select primitives, we need to add some auxiliary data that keep information about the left-to-right numbering of trie leaves, thus further increasing the space occupancy of the trie-based solution. In Table 7.2 we account for such  2 Code at http:  www.cs.princeton.edu ~rs strings .   7.4 Experimental Results  105  “space doubling”, but not for the auxiliary data, thus giving an advantage in space to these data structures wrt Cpi. It is evident the large space occupancy of ternary search trees because of the use of pointers and the explicit storage of the dictionary strings  without any compression . As predicted from the statistics of Table 7.1, Fc achieves a compression ratio of about 40 % on the original dictionaries, but more than 60 % on their reversal. Further, we note that Fc space improves negligibly if we vary the bucket size b from 32 to 1,024 strings, and achieves the best space time trade- off when b = 32.3 In summary, the space occupancy of the Fc solution is more than the original dictionary size, if we wish to support all queries of the Tolerant Retrieval problem! As far as the variants of Cpi are concerned, we note that their space improvement is signiﬁcant: a multiplicative factor from 2 to 7 wrt Fc, and from 40 to 86 wrt Trie. In Sect. 7.2.1 we mentioned another simple solution to the Tolerant Retrieval problem which was based on the compressed indexing of the string  cid:2 SD, built by juxtaposing twice every string of S. In that section we argued that this solution is inefﬁcient in indexing time and compressed-space occupancy because of this “string duplication” process. Here we investigate experimentally our conjecture by comput- ing and comparing the k-th order empirical entropy of the two strings  cid:2 SD and SD. As predicted theoretically, the two entropy values are close for all three dictionaries, thus implying that the compressed indexing of  cid:2 SD should require about twice the compressed indexing of SD  recall that  cid:2 SD = 2SD − 1 . To check this, we have then built two FM-indexes: one on  cid:2 SD and the other on SD, by varying S over the three dictionaries. We found that the space occupancy of the FM-index built on  cid:2 SD is a factor 1.6–1.9 worse than our Cpi-Fmi built on SD. So we were right when in Sect. 7.2.1 we conjectured the inefﬁciency of the compressed indexing of  cid:2 SD.  We have ﬁnally tested the time efﬁciency of the above indexing data structures over a P4 2.6 GHz machine, with 1.5 Gb of internal memory and running Linux kernel 2.4.20. We executed a large set of experiments by varying the searched- pattern length, and by searching one million patterns per length. Since the results were stable over all these timings, we report in Table 7.3 only the most signiﬁcant ones by using the notation microsecs per searched symbol  shortly ¯s char : this is obtained by dividing the overall time of an experiment by the total length of the searched patterns. We remark that the timings in Table 7.3 account for the cost of searching a pattern preﬁx and a pattern sufﬁx of the speciﬁed length. While this is the total time taken by our Cpi to solve a PrefixSuffix query, the timings for Fc and Trie are optimistic evaluations because they should also take into account the time needed to intersect the candidate list of answers returned by the preﬁx sufﬁx queries! Keeping this in mind, we look at Table 7.3 and note that Cpi allows to trade space occupancy per query time: we can go from a space close to gzip–ppmdi and access time of 20–57 µs char  i.e. CPI-FMI-1024 , to an access time similar to Fc of few µs char but using less than half of its space  i.e. CPI-AFI . Which  3 A smaller b would enlarge the extra-space dedicated to pointers, a larger b would impact seriously on the time efﬁciency of the preﬁx searches.   106  7 Dictionary Indexes  variant of Cpi to choose depends on the application for which the Tolerant Retrieval problem must be solved.  We ﬁnally notice that, of course, any improvement to compressed indexes  Navarro and Mäkinen 2007  will immediately and positively impact onto our Cpi, both in theory and in practice. Overall our experiments show that Cpi is a novel compressed storage scheme for string dictionaries which is fast in supporting the sophisticated searches of the Tolerant Retrieval problem, and is as compact as the best known compressors!  7.5 Further Considerations In Manning et al.  2008  the more sophisticated wild-card query P = α√ β√ γ is also considered and implemented by intersecting the set of strings containing γ$α with the set of strings containing β. Our compressed permuterm index allows to avoid the materialization of these two sets by working only on the compressed index built on the string SD. The basic idea consists of the following steps:   Compute [First   Compute [First   For each r ≤ [First ⊆⊆] or to [1, m]  i.e. starts with $ . a row which either belongs to [First   In the former case r is an answer to WildCard P , in the latter case it is not.  ⊆] repeatedly apply Back_step of Fig. 7.2 until it ﬁnds  ⊆] = BackPerm_search γ$α ; ⊆⊆] = BackPerm_search β ;  ⊆, Last ⊆⊆, Last  ⊆⊆, Last  ⊆, Last  The number of Back_step’s invocations depends on the length of the dictionary strings which match the query PrefixSuffix α √ γ . In practice, it is possible to engineer this paradigm to reduce the total number of Back_steps  see Chap. 6, FM- indexV2 . The above scheme can be also used to answer more complex queries as P = α√ β1 √ β2 √ . . .√ βk √ γ, with possibly empty α and γ. The efﬁciency depends on the selectivity of the individual queries PrefixSuffix α√ γ  and Substring βi , for i = 1, . . . , k.  It would be then interesting to extend our results in two directions, either by proving guaranteed and efﬁcient worst-case bounds for queries with multiple wild- card symbols, or by turning our Compressed Permuterm index in a I O-conscious or, even better, cache-oblivious compressed data structure. This latter issue actually falls in the key challenge of current data structural design: does it exist a cache-oblivious compressed index?   Chapter 8 Future Directions of Research  We conclude by presenting some of the most important open problems of this fasci- nating ﬁeld of research. Exact optimal partitioning. In Chap. 3 we have investigated the problem of parti- tioning an input string T in such a way that compressing individually its parts via a base-compressor C gets a compressed output that is shorter than applying C over the entire T at once. We provided an algorithm which is guaranteed to compute in O n log1+ε n  time a partition of T whose compressed output is guaranteed to be no more than  1 + ε -worse the optimal one, where ε may be any positive constant. An interesting open question consists in understanding if it is possible to design an o n2  time solution for computing the exact optimal partition. Speeding up solutions of dynamic programming recurrences. Many applications require to solve efﬁciently dynamic programming recurrences  see Giancarlo  1997  and references therein . The simplest type of recurrence, called 1D 1D, have the form E[ j] = mini < j  E[i] + c i, j    and is used as a building block to solve more sophisticated recurrences. A trivial algorithm solves such recurrences in quadratic time and, by simple argumentations, it has been shown that this algorithm is optimal for general cost functions c  . Nevertheless, subquadratic solutions can be obtained whenever the cost function c   has particular properties that can be exploited. For example, in literature are known efﬁcient algorithms that permit to solve these re- currences even in linear time provided that the cost function c   satisﬁes a property called quadrangle inequality or some of its variants. However, properties related to quadrangle inequality may not hold in many contexts. As an example consider the recurrences that come out in the two problems addressed in Chaps. 3 and 4. Our efﬁcient algorithms have been obtained by showing that dynamic programming recurrences can be solved approximated in subquadratic time even when function c   is increasing without requiring properties related to quadrangle inequality. We believe that this research can be reﬁned and integrated in order to better understand which are necessary and or sufﬁcient properties that function c   must be satisﬁed in order to guarantee subquadratic solutions.  R. Venturini, Compressed Data Structures for Strings, Atlantis Studies in Computing 4, DOI: 10.2991 978-94-6239-033-1_8,   Atlantis Press and the authors 2014  107   108  8 Future Directions of Research  Random Access in compressed strings in RAM model. The scheme presented in Chap. 5 is very simple and, as deeply illustrated in Sadakane and Grossi  2006 , may ﬁnd successful applications into many other interesting contexts  see Jansson et al.  2007 ; Sadakane and Grossi  2006 ; Barbay and Munro  2008 ; Belazzougui et al.  2009 ; Fischer  2009 ; Fischer et al.  2008 ; Ferragina and Fischer  2007 ; Chien et al.  2008 ; Hon et al.  2008 ; Nitto and Venturini  2008  and references therein . However, all known solutions are far from being usable in practice because of the additive term o n log σ  which usually dominates the kth order entropy term. More research is still needed to either to reduce the lower order term as much as possible n logσ n k log σ  would be a valuable  e.g. removing the factor k log σ in the term O  improvement , or to show a lower bound that relates access time and achievable space bound in terms of the kth order entropy. Since the storage scheme in Chap. 5 , unlike Sadakane and Grossi  2006 ; González and Navarro  2006 , does not use any sophisticated data compression machinery, we are led to think that there is room for improvement. The recent result in Grossi et al.  2013  moves a step forward in this direction. Indeed, the chapter presents a dynamic version of the scheme in Chap. 5 and proves that the factor k log σ can be removed at the cost of increasingly the access time by a term O log n  log log n . Random Access in compressed string on External-Memory model. This problem extends the one stated in the previous point to the External-Memory model, for which the complexity of retrieving a substring from T is measured in terms of I Os. In this scenario a time optimal solution is one that retrieves any B logσ n consecutive symbols of T with O 1  disk accesses. The scheme we presented in Chap. 5 might work well in the external-memory model too, except for three main inefﬁciencies:  1  its blocking approach reduces the overall compression ratio by posing a limit to the value k which is too small for common alphabet sizes, since it must be o logσ n ;  2  it does not completely exploit the fact that internal-memory operations have no cost;  3  the table used for block-compression may not ﬁt in the internal memory, and thus may force us to reduce furthermore the block size. Following the deﬁnition of kth order empirical entropy  see Sect. 2.4.1, we can obtain an I O conscious and more space-efﬁcient scheme by storing in internal memory the model corresponding to all contexts occurring in T , and then compressing any symbol of T according to its preceding context and its number of occurrences. Clearly, this model could be very large and thus not ﬁt in internal memory. An interesting open problem is therefore how to prune the whole model in order to ﬁt it in internal memory and achieve the maximum compression among all models which satisfy that space-bound. As proved in Ferragina et al.  2005a , a further difﬁculty arises from the fact that the best possible compression ratio is not necessarily achieved by considering contexts of the same length. Faster Count queries. The time complexity of Count procedure of  compressed  full-text indexes is non optimal in the RAM model. An interesting open problem concerns the possibility of designing a compressed full-text index that supports Count P[1, p]  in  close to  optimal time  namely, O    . The ﬁrst result in this direction appeared in Grossi and Vitter  2005  in which it has been shown how  logσ n  p   8 Future Directions of Research  109  p  p  logσ n  logσ n  + logε  to build a non compressed full-text index on T that requires  ε−1 + O 1  n log σ bits of space and performs counting operations in O 1  time if p = o logσ n  or σ n  time otherwise, for any ﬁxed value of 0 < ε < 1. A step forward O  has been done in Grossi et al.  2003  that proposes a compressed full-text index that occupies ε−1n Hk + O n log log n   bits of space keeping a similar time complexity, for logε σ n any ﬁxed value of 0 < ε < 1 3 . Faster Locate queries. Another open challenge concerning compressed indexes is to fasten their locate queries in order to achieve the optimal O occ  time bound. The best known results are two indexes due to He et al.  2005 ; Ferragina and Manzini  2005 . The former considers only binary texts and locates the occ occurrences of a pattern P[1, p] in O  p + occ  time for large enough p and 2n + o n  bits of space  so it is not compressed ; while the latter has no restriction on p but requires space O n Hk  T   logε n  + o n log σ  bits  which has the extra log-factor in front of the optimal n Hk  T   term . Is it possible to achieve O  p + occ   or even better + occ   searching time and O n Hk  T    + o n log σ  bits of space in the O  worst case? This result would be provably better than classical data structures. Faster Locate and Extract operation depending on the distribution of queries. As we pointed out the efﬁciency of Locate and Extract queries is the major drawback of compressed indexes, both in theory and in practice.The common strategy to solve these operations consists in marking one position every t positions of sufﬁx array and its inverse, where t is an user deﬁned parameter.This strategy costs roughly O  n log n   bits of space and guarantees to solve a Locate or an Extract query in no more than O C·t   time, where C is a cost that depends on the particular index  e.g. the time required to perform an L F step in FM-index-like indexes .In some scenarios we can assume to known the distribution of the queries that must be solved by the index  e.g. the query log of a search engine provides a good estimation of the frequency of a particular query . Results in [Ferragina et al.  2011b  and  2013b ] provide a way to design distribution-aware compressed full-text indexes when the distribution of subsequent queries is known beforehand. Indeed, given the distribution of the subsequent queries and a bound on the space occupancy, it is shown how to ﬁnd a sampling strategy that induces the ﬁxed space bound and minimizes the expected time required for solving Locate Extract queries drawn accordingly to the input distribution. Experiments show that the advantage at query time is between 4–36 times better than the classical approach to Locate and 2 times for Extract. An interesting open problem asks for designing distribution-aware compressed indexes that are able to adapt themselves to an unknown distribution of queries. Compressed indexes on External-Memory model. The memory of current PCs is hierarchical so that, in order to achieve effective algorithmic performance, cache- aware or even cache-oblivious solutions should be designed. Although their small space requirements might permit compressed indexes to ﬁt in main memory, there will always be cases where they have to operate on external memory. The most attractive full-text indexes for secondary memory are the String B-tree [Ferragina and Grossi  1999 ] , the Self-adjusting Sufﬁx Tree [Ko and Aluru  2007 ] and the two cache-oblivious String B-trees [Bender et al.  2006 ; Ferragina et al.  2008b ].  t   110  8 Future Directions of Research  log log n  Unfortunately these data structures do not achieve higher order entropy. A recent result [Hon et al.  2009 ] shows that a text T can be indexed in O n Hk  T   + 1   + o n log σ  bits and such that all occurrences of a pattern P[1, p] in T can be reported in O  p  B logσ n + log4 n + occ logB n  I Os where occ is the number of occurrences of P in T . Thus, the index achieves optimal query I O performance with respect to the length P  namely, O  p B  I Os but not with respect to the number of its occurrences. Unfortunately, [Chien et al.  2008 ] noted that lower bounds in range searching data structures suggest that the last term O occ logB n  cannot be improved to O occ B  by occupying O n log σ  bits of space. This implies that there are only two ways to improve the above index:  1  reduce the middle polylogarithmic term, and  2  reduce the space term from O n Hk  T   + 1   to exactly n Hk  T  . Space efﬁcient Bwt construction. An interesting problem of great practical sig- niﬁcance concerns the auxiliary space needed to build the compressed indexes. We require a space efﬁcient computation of the Bwt, since many such indexes are based on it. The Bwt of a string T can be stored using n log σ bits but the linear time algo- rithms used to construct it make use of auxiliary arrays  i.e. sufﬁx array  whose stor- age takes ε n log n  bits. This poses a serious limitation to the size of the largest Bwt that can be computed efﬁciently in internal memory. The problem of space and time efﬁcient computation of large Bwt is still open even if interesting preliminary results are proposed in [Hon et al.  2003 ; Kärkkäinen  2007 ]. Other approaches  e.g. the one described in Chap. 7  process the text from left to right by adding one symbol at a time to the partial Bwt. The space required by such solutions is n Hk  T  + o n log σ  bits but their time complexity is not linear  namely, O n 1+log σ  log log n  log n  . We also mention the result in [Ferragina et al.  2010 ] that proposes an algorithm that requires just o n  bits of auxiliary space  here original text is replaced with its Bwt  and computes the Bwt in O n log1+ε n  time, for any ε > 0. A natural question arises: Is it possible to build the Bwt of a string T in linear time using O n log σ   or better O n Hk  T     bits of auxiliary space?   Bibliography  Aggarwal, A. and Vitter, J. S.  1988 . The input output complexity of sorting and related problems,  Communications of the ACM 31, 9, pp. 1116–1127.  Aluru, S. and Ko, P.  2008 . Encyclopedia of Algorithms, chap. on “Lookup Tables, Sufﬁx Trees  Amir, A., Landau, G. M. and Ukkonen, E.  2002 . Online timestamped text indexing, Information  and Sufﬁx Arrays”  Springer .  Processing Letters 82, 5, pp. 253–259.  & Experience 25 , 2, pp. 129–141.  Andersson, A. and Nilsson, S.  1995 . Efﬁcient implementation of sufﬁx trees, Software Practice  Arroyuelo, D. and Navarro, G.  2005 . Space-efﬁcient construction of LZ-index, in Proceedings of the 16th Annual International Symposium on Algorithms and Computation  ISAAC , LNCS vol. 3827  Springer , pp. 1143–1152.  Arroyuelo, D. and Navarro, G.  2008 . Practical approaches to reduce the space requirement of Lempel-Ziv-based compressed text indices, Tech. Rep. TR DCC-2008-9, Dept. of Computer Science, Univ. of Chile.  Arroyuelo, D., Navarro, G. and Sadakane, K.  2006 . Reducing the space requirement of LZ-index, in Proceedings of the 17th Annual Symposium on Combinatorial Pattern Matching  CPM , LNCS vol. 4009  Springer , pp. 319–330.  Baeza-Yates, R. and Gonnet, G.  1996 . Fast text searching for regular expressions or automaton  searching on tries, Journal of the ACM 43, 6, pp. 915–936.  Baeza-Yates, R. and Ribeiro-Neto, B.  1999 . Modern, Information Retrieval  ACM Addison-  Wesley .  Barbay, J., Claude, F. and Navarro, G.  2010 . Compact rich-functional binary relation representa- tions, in Proceedings of the Latin American Theoretical Informatics  LATIN , Lecture Notes in Computer Science, Vol. 6034  Springer , pp. 170–183.  Barbay, J., He, M., Munro, I. J. and Rao, S. S.  2007 . Succinct indexes for string, binary relations and multi-labeled trees, in Proceedings of the 18th ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 680–689.  Barbay, J. and Munro, J. I.  2008 . Succinct encoding of permutations: Applications to text indexing,  in Encyclopedia of Algorithms.  Barbay, J. and Navarro, G.  2009 . Compressed representations of permutations, and applications, in Proceedings of the Symposium on Theoretical Aspects of Computer Science  STACS , LIPIcs, Vol. 3  Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, Germany , pp. 111–122.  Beame, P. and Fich, F. E.  1999 . Optimal bounds for the predecessor problem, in Proceedings of  the 31st ACM Symposium on Theory of Computing  STOC , pp. 295–304.  Békési, J., Galambos, G., Pferschy, U. and Woeginger, G.  1997 . Greedy algorithms for on-line  data compression, Journal of Algorithms 25, 2, pp. 274–289.  R. Venturini, Compressed Data Structures for Strings, Atlantis Studies in Computing 4, DOI: 10.2991 978-94-6239-033-1,   Atlantis Press and the authors 2014  111   112  Bibliography  Belazzougui, D., Botelho, F. C. and Dietzfelbinger, M.  2009 . Hash, displace, and compress, in  Proceedings of the 17th Annual European Symposium on Algorithms  ESA , pp. 682–693.  Bender, M. A., Farach-Colton, M. and Kuszmaul, B. C.  2006 . Cache-oblivious string b-trees, in Proceedings of the 25th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems  PODS , pp. 233–242.  Benoit, D., Demaine, E., Munro, I., Raman, R., Raman, V. and Rao, S.  2005 . Representing trees  of higher degree, Algorithmica 43, pp. 275–292.  Bentley, J. and McIlroy, M.  2001 . Data compression with long repeated strings, Information  Sciences 135, 1–2, pp. 1–11.  Bentley, J. L. and Sedgewick, R.  1997 . Fast algorithms for sorting and searching strings, in  Proceedings of the 8th ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 360–369.  Boldi, P., Codenotti, B., Santini, M. and Vigna, S.  2004 . UbiCrawler: A scalable fully distributed  web crawler, Software: Practice & Experience 34, 8, pp. 711–726.  Boldi, P. and Vigna, S.  2005 . Codes for the world wide web, Internet Mathematics 2, 4. Buchsbaum, A. and Giancarlo, R.  2008 . Encyclopedia of Algorithms, chap. Table Compression   Springer , pp. 939–942.  Buchsbaum, A. L., Caldwell, D. F., Church, K. W., Fowler, G. S. and Muthukrishnan, S.  2000 . Engineering the compression of massive tables: an experimental approach, in Proceedings 11th Annual ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 175–184.  Buchsbaum, A. L., Fowler, G. S. and Giancarlo, R.  2003 . Improving table compression with  combinatorial optimization, Journal of the ACM 50, 6, pp. 825–851.  Burrows, M. and Wheeler, D.  1994 . A block sorting lossless data compression algorithm, Tech.  Rep. 124, Digital Equipment Corporation.  Chan, H., Hon, W., Lam, T. and Sadakane, K.  2007 . Compressed indexes for dynamic text col-  lections, ACM Transactions on Algorithms 3, 2.  Chang, F., Dean, J., Ghemawat, S., Hsieh, W., Wallach, D., Burrows, M., Chandra, T., Fikes, A. and Gruber, R.  2008 . Bigtable: A distributed storage system for structured data, ACM Transactions on Computer Systems 26, 2.  Chien, Y.-F., Hon, W.-K., Shah, R. and Vitter, J. S.  2008 . Geometric burrows-wheeler transform: Linking range searching and text indexing, in Proceedings of IEEE Data Compression Conference  DCC , pp. 252–261.  Cilibrasi, R. and Vitányi, P. M. B.  2005 . Clustering by compression, IEEE Transactions on Infor-  mation Theory 51, 4, pp. 1523–1545.  Claude, F. and Navarro, G.  2010 . Extended compact web graph representations, in Algorithms  and Applications, Lecture Notes in Computer Science, Vol. 6060  Springer , pp. 77–91.  Cohn, M. and Khazan, R.  1996 . Parsing with preﬁx and sufﬁx dictionaries, in Proceedings of  IEEE Data Compression Conference  DCC , pp. 180–189.  Cormen, T. H., Leiserson, C. E., Rivest, R. L. and Stein, C.  2001 . Introduction to Algorithms,  Second Edition  The MIT Press and McGraw-Hill Book Company .  Cormode, G. and Muthukrishnan, S.  2005 . Substring compression problems, in Proceedings of  the 16th Annual ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 321–330.  Crochemore, M., Ilie, L. and Smyth, W. F.  2008 . A simple algorithm for computing the Lempel-Ziv factorization, in Proceedings of the IEEE Data Compression Conference  DCC , pp. 482–488. Crochemore, M. and Rytter, W.  2003 . Jewels of Stringology  World Scientiﬁc Publishing Com-  pany , ISBN 9810248970.  Dasgupta, S., Papadimitriou, C. and Vazirani, U.  2006 . Algorithms  McGraw-Hill Sci-  Elias, P.  1975 . Universal codeword sets and representations of the integers, IEEE Transactions on  ence Engineering Math , ISBN 0073523402.  Information Theory 21, 2, pp. 194–203.  Farach-Colton, M.  1997 . Optimal sufﬁx tree construction with large alphabets, in Proceedings of  the 38th Annual Symposium Foundation Compututer Science  FOCS , pp. 137–143.  Farzan, A. and Munro, I. J.  2008 . Succinct representations of arbitrary graphs, in Proceedings of  the 16th Annual European Symposium on Algorithms  ESA , pp. 393–404.   Bibliography  113  Farzan, A., Raman, R. and Rao, S.  2009 . Universal succinct representations of trees? in Proceed- ings of the 36th International Colloquium on Automata, Languages and Programming  ICALP , Lecture Notes in Computer Science, Vol. 5555  Springer , pp. 451–462.  Ferragina, P. and Fischer, J.  2007 . Sufﬁx arrays on words, in Proceedings of the 18th Annual  Symposium on Combinatorial Pattern Matching  CPM , pp. 328–339.  Ferragina, P., Gagie, T. and Manzini, G.  2010 . Lightweight data indexing and compression in exter- nal memory, in Proceedings of the 10th Latin American Symposium on Theoretical Informatics  LATIN , pp. 697–710.  Ferragina, P., Giancarlo, R. and Manzini, G.  2006a . The engineering of a compression boosting library: Theory vs practice in BWT compression, in Proceedings of the 14th European Symposium on Algorithms  ESA   LNCS vol. 4168 , pp. 756–767.  Ferragina, P., Giancarlo, R. and Manzini, G.  2009a . The myriad virtues of wavelet trees, Infor-  mation and Computation 207, pp. 849–866.  Ferragina, P., Giancarlo, R., Manzini, G. and Sciortino, M.  2005a . Boosting textual compression  in optimal linear time, Journal of the ACM 52, pp. 688–713.  Ferragina, P., González, R., Navarro, G. and Venturini, R.  2008a . Compressed text indexes: From  theory to practice, ACM Journal of Experimental Algorithmics 13.  Ferragina, P. and Grossi, R.  1999 . The string B-tree: A new data structure for string search in  external memory and its applications, Journal of ACM 46, 2, pp. 236–280.  Ferragina, P., Grossi, R., Gupta, A., Shah, R. and Vitter, J. S.  2008b . On searching compressed string collections cache-obliviously, in Proceedings of the 27-th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  PODS , pp. 181–190.  Ferragina, P., Koudas, N., Muthukrishnan, S. and Srivastava, D.  2003 . Two-dimensional substring  indexing, Journal of Computer System Science 66, 4, pp. 763–774.  Ferragina, P., Luccio, F., Manzini, G. and Muthukrishnan, S.  2005b . Structuring labeled trees for optimal succinctness, and beyond, in Proceedings of the 46th IEEE Symposium on Foundations of Computer Science  FOCS , pp. 184–193.  Ferragina, P., Luccio, F., Manzini, G. and Muthukrishnan, S.  2006b . Compressing and searching XML data via two zips, in Proceedings of the 15th International World Wide Web Conference  WWW , pp. 751–760.  Ferragina, P., Luccio, F., Manzini, G. and Muthukrishnan, S.  2006c . Compressing and searching XML data via two zips. in Proceedings of the 15th World Wide Web Conference  WWW , pp. 751–760.  Ferragina, P., Luccio, F., Manzini, G. and Muthukrishnan, S.  2009b . Compressing and indexing  labeled trees, with applications, Journal of the ACM 57, 1.  Ferragina, P. and Manzini, G.  2001 . An experimental study of an opportunistic index, in Proceed- ings of the 12th Annual ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 269–278. Ferragina, P. and Manzini, G.  2005 . Indexing compressed text, Journal of the ACM 52, 4, pp.  552–581.  Ferragina, P., Manzini, G., Mäkinen, V. and Navarro, G.  2007 . Compressed representations of  sequences and full-text indexes, ACM Transactions on Algorithms  TALG 3, 2, p. article 20.  Ferragina, P., Nitto, I. and Venturini, R.  2009c . On optimally partitioning a text to improve its compression, in Proceedings of the 17th Annual European Symposium on Algorithms  ESA , pp. 420–431.  Ferragina, P., Nitto, I. and Venturini, R.  2009d . On the bit-complexity of Lempel-Ziv compression, in Proceedings of the 20th ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 768–777. Ferragina, P., Nitto, I. and Venturini, R.  2011a . On optimally partitioning a text to improve its  compression, Algorithmica 61, 1, pp. 51–74.  Ferragina, P., Nitto, I. and Venturini, R.  2013a . On the bit-complexity of Lempel-Ziv compression,  SIAM Journal on Computing  SICOMP  42, pp. 1521–1541.  Ferragina, P. and Rao, S.  2008 . Tree compression and indexing, in M.-Y. Kao  ed. , Encyclopedia  of Algorithms  Springer .   114  Bibliography  Ferragina, P., Sirén, J. and Venturini, R.  2011b . Distribution-aware compressed full-text indexes,  in Proceedings of 19th Annual European Symposium on Algorithms  ESA , pp. 760–771.  Ferragina, P., Sirén, J. and Venturini, R.  2013b . Distribution-aware compressed full-text indexes,  Algorithmica.  Ferragina, P. and Venturini, R.  2007a . Compressed permuterm index, in Proceedings of the 30th Annual International Conference on Research and Development in Information Retrieval  SIGIR , pp. 535–542.  Ferragina, P. and Venturini, R.  2007b . A simple storage scheme for strings achieving entropy  bounds, Theoretical Computer Science 372, 1, pp. 115–121.  Ferragina, P. and Venturini, R.  2007c . A simple storage scheme for strings achieving entropy bounds, in Proceedings of the 18th ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 690–696.  Ferragina, P. and Venturini, R.  2010 . The compressed permuterm index, ACM Transactions on  Algorithms 7, 1, p. 10.  Ferragina, P. and Venturini, R.  2013 . Compressed cache-oblivious String B-tree, in ESA 2013:  Proceedings of 21th Annual European Symposium on Algorithms  ESA , pp. 469–480.  Fiala, E. R. and Greene, D. H.  1989 . Data compression with ﬁnite windows, Communications of  the ACM 32, 4, pp. 490–505.  Fischer, J.  2009 . Short labels for lowest common ancestors in trees, in Proceedings of the 17th  Annual European Symposium on Algorithms  ESA , pp. 752–763.  Fischer, J., Heun, V. and Stühler, H. M.  2008 . Practical entropy-bounded schemes for o 1 -range  minimum queries, in Proceedings of IEEE Data Compression Conference  DCC .  Foschini, L., Grossi, R., Gupta, A. and Vitter, J.  2006 . When indexing equals compression: Exper- iments with compressing sufﬁx arrays and applications, ACM Transactions on Algorithms 2, 4, pp. 611–639.  Fredman, M. L. and Willard, D. E.  1994 . Trans-dichotomous algorithms for minimum spanning  trees and shortest paths, Journal of Computer and System Sciences 48, 3, pp. 533–551.  Gagie, T. and Manzini, G.  2007 . Space-conscious compression, in Proceedings of the 32th Inter- national Symposium on Mathematical Foundations of Computer Science  MFCS , pp. 206–217. Garﬁeld, E.  1976 . The permuterm subject index: An autobiographical review, Journal of the ACM  27, pp. 288–291.  Geary, R. F., Raman, R. and Raman, V.  2006 . Succinct ordinal trees with level-ancestor queries,  ACM Transactions on Algorithms 2, 4, pp. 510–534.  Giancarlo, R.  1997 . Dynamic programming: special cases, in A. Apostolico and Z. Galil  eds. ,  Pattern Matching Algorithms, 2nd edn.  Oxford Univ. Press , pp. 201–236.  Giancarlo, R., Restivo, A. and Sciortino, M.  2007 . From ﬁrst principles to the Burrows and Wheeler transform and beyond, via combinatorial optimization, Theoretical Computer Science 387, 3, pp. 236–248.  Giancarlo, R. and Sciortino, M.  2003 . Optimal partitions of strings: A new class of Burrows- Wheeler compression algorithms, in Proceedings of the 14th Annual Symposium on Combinato- rial Pattern Matching  CPM   LNCS vol. 2676 , pp. 129–143.  Giegerich, R., Kurtz, S. and Stoye, J.  2003 . Efﬁcient implementation of lazy sufﬁx trees, Software  Practice & Experience 33, 11, pp. 1035–1049.  Golynski, A., Raman, R. and Rao, S. S.  2008 . On the redundancy of succinct data structures, in Proceedings of the 11th Scandinavian Workshop on Algorithm Theory  SWAT   LNCS vol. 5124 , pp. 148–159.  González, R., Grabowski, S., Mäkinen, V. and Navarro, G.  2005 . Practical implementation of rank and select queries, in Poster Proceedings Volume of 4th Workshop on Efﬁcient and Experimental Algorithms  WEA , pp. 27–38.  González, R. and Navarro, G.  2006 . Statistical encoding of succinct data structures, in Proceedings  of the 17th Annual Symposium on Combinatorial Pattern Matching  CPM , pp. 294–305.   Bibliography  115  González, R. and Navarro, G.  2008 . Improved dynamic rank-select entropy-bound structures, in Proceedings of the 8th Latin American Symposium on Theoretical Informatics  LATIN , LNCS 4957, pp. 374–386.  Grossi, R., Gupta, A. and Vitter, J.  2003 . High-order entropy-compressed text indexes, in Pro-  ceedings of the 14th ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 841–850.  Grossi, R., Raman, R., Rao, S. S. and Venturini, R.  2013 . Dynamic compressed strings with random access, in Proceedings of the 40th International Colloquium on Automata, Languages and Programming  ICALP , pp. 504–515.  Grossi, R. and Vitter, J. S.  2005 . Compressed sufﬁx arrays and sufﬁx trees with applications to  text indexing and string matching, SIAM Journal on Computing 35, 2, pp. 378–407.  Gusﬁeld, D.  1997 . Algorithms on Strings, Trees, and Sequences: Computer Science and Compu-  tational Biology  Cambridge University Press .  Hagerup, T. and Tholey, T.  2001 . Efﬁcient minimal perfect hashing in nearly minimal space, in Proceedings of the 18th Annual Symposium on Theoretical Aspects of, Computer Science STACS , pp. 317–326.  He, M., Munro, I. J. and Rao, S. S.  2005 . A categorization theorem on sufﬁx arrays with appli- cations to space efﬁcient text indexes, in Proceedings of the 16th ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 23–32.  Hon, W., Sadakane, K. and Sung, W.  2003 . Breaking a time-and-space barrier in constructing full- text indices, in Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science  FOCS   IEEE Computer Society , pp. 251–260.  Hon, W.-K., Lam, T. W., Shah, R., Tam, S.-L. and Vitter, J. S.  2008 . Compressed index for dictionary matching, in Proceedings of IEEE Data Compression Conference  DCC , pp. 23–32. Hon, W.-K., Shah, R., Thankachan, S. V. and Vitter, J. S.  2009 . On entropy-compressed text indexing in external memory, in Proceedings of the 16th International Symposium on String Processing and Information Retrieval  SPIRE , pp. 75–89.  Howard, P. G. and Vitter, J. S.  1992 . Analysis of arithmetic coding for data compression, Infor-  mation Processing Management 28, 6, pp. 749–764.  Jansson, J., Sadakane, K. and Sung, W.  2007 . Ultra-succinct representation of ordered trees, in Proceedings of the 18th ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 575–584. Jokinen, P. and Ukkonen, E.  1991 . Two algorithms for approximate string matching in static texts, in Proceedings of the 16th Annual Symposium on Mathematical Foundations of Computer Science  MFCS , Vol. 16, pp. 240–248.  Kaplan, H., Landau, S. and Verbin, E.  2007 . A simpler analysis of Burrows-Wheeler-based com-  pression, Theoretical Computer Science 387, 3, pp. 220–235.  Kärkkäinen, J.  1995 . Sufﬁx cactus: a cross between sufﬁx tree and sufﬁx array, in Proceedings of the 6th Annual Symposium on Combinatorial Pattern Matching  CPM , LNCS vol. 937  Springer , pp. 191–204.  Kärkkäinen, J.  2007 . Fast BWT in small space by blockwise sufﬁx sorting, Theoretical Computer  Science 387, 3, pp. 249–257.  Kärkkäinen, J. and Ukkonen, E.  1996a . Lempel-Ziv parsing and sublinear-size index structures for string matching, in Proceedings of the 3rd South American Workshop on String Processing  WSP   Carleton University Press , pp. 141–155.  Kärkkäinen, J. and Ukkonen, E.  1996b . Sparse sufﬁx trees, in Proceedings of the 2nd Annual Inter- national Conference on Computing and Combinatorics  COCOON , LNCS vol. 1090  Springer , pp. 219–230.  Katajainen, J. and Raita, T.  1989 . An approximation algorithm for space-optimal encoding of a  text, Computer Journal 32, 3, pp. 228–237.  Katajainen, J. and Raita, T.  1992 . An analysis of the longest match and the greedy heuristics in  text encoding, Journal of the ACM39, 2, pp. 281–294.  Klein, S. T.  1997 . Efﬁcient optimal recompression, Computer Journal 40, 2 3, pp. 117–126. Knuth, D. E.  1998 . The art of computer programming, volume 3:  2nd ed.  sorting and searching  Addison Wesley Longman Publishing Co. Inc., Redwood City, CA, USA , ISBN 0-201-89685-0.   116  Bibliography  Ko, P. and Aluru, S.  2007 . Optimal self-adjusting trees for dynamic string data in secondary stor- age, in Proceedings of the 14th International Symposium on String Processing and Information Retrieval  SPIRE , pp. 184–194.  Kosaraju, R. and Manzini, G.  1999 . Compression of low entropy strings with Lempel-Ziv algo-  rithms, SIAM Journal on Computing 29, 3, pp. 893–911.  Kulkarni, P., Douglis, F., LaVoie, J. and Tracey, J.  2004 . Redundancy elimination within large  collections of ﬁles, in USENIX Annual Technical Conference, pp. 59–72.  Lam, T.-W., Sadakane, K., Sung, W.-K. and Yiu, S.-M.  2002 . A space and time efﬁcient algorithm for constructing compressed sufﬁx arrays, in Proceedings of the 8th Conference on Computing and Combinatorics  COCOON , LNCS vol. 2387  Springer , pp. 401–410.  Larsson, N. J. and Sadakane, K.  2007 . Faster sufﬁx sorting, Theoretical Computer Science 387,  3, pp. 258–272.  Lehtinen, O., Sutinen, E. and Tarhio, J.  1996 . Experiments on block indexing, in Proceedings of the 3rd South American Workshop on String Processing  WSP   Carleton University Press , pp. 183–193.  Lemström, K. and Perttu, S.  2000 . SEMEX - an efﬁcient music retrieval prototype, in Proceedings  of the 1st International Symposium on Music, Information Retrieval  ISMIR .  Mäkinen, V. and Navarro, G.  2005 . Succinct sufﬁx arrays based on run-length encoding, Nordic  Journal of Computing 12, 1, pp. 40–66.  Mäkinen, V. and Navarro, G.  2006 . Position-restricted substring searching, in Proceedings of the 7th Latin American Symposium on Theoretical Informatics  LATIN   LNCS vol. 3887 , pp. 703–714.  Mäkinen, V. and Navarro, G.  2007 . Implicit compression boosting with applications to self- indexing, in Proceedings of the 14th International Symposium on String Processing and Infor- mation Retrieval  SPIRE   LNCS col. 4726 , pp. 229–241.  Mäkinen, V. and Navarro, G.  2008 . Dynamic entropy-compressed sequences and full-text indexes,  ACM Transactions on Algorithms 4, 3, p. article 32.  Manber, U. and Myers, G.  1993 . Sufﬁx arrays: A new method for on-line string searches, SIAM  Journal of Computing 22, pp. 935–948.  Manber, U. and Wu, S.  1994 . Glimpse: a tool to search through entire ﬁle systems, in Proceedings  of the USENIX Winter 1994 Technical Conference  USENIX Association , pp. 4–4.  Manning, C. D., Raghavan, P. and Schülze, H.  2008 . Introduction to, Information Retrieval  Cam-  bridge University Press .  Mantaci, S., Restivo, A., Rosone, G. and Sciortino, M.  2005 . An extension of the Burrows-Wheeler transform and applications to sequence comparison and data compression, in Proceedings of the 16th Annual Symposium on Combinatorial Pattern Matching  CPM , pp. 178–189.  Manzini, G.  2001 . An analysis of the Burrows-Wheeler transform, Journal of the ACM 48, 3, pp.  407–430.  Algorithmica 40, 1, pp. 33–50.  Manzini, G. and Ferragina, P.  2004 . Engineering a lightweight sufﬁx array construction algorithm,  McCreight, E. M.  1985 . Priority search trees, SIAM Journal on Computing14, 2, pp. 257–276. Moffat, A. and Isal, R.  2005 . Word-based text compression using the Burrows-Wheeler transform,  Information Processing Management 41, 5, pp. 1175–1192.  Munro, I. J.  1996 . Tables, in Proceedings of the 16th Conference on Foundations of Software Technology and Theoretical Computer Science  FSTTCS , LNCS vol. 1180  Springer , pp. 37– 42.  Munro, I. J., Raman, R., Raman, V. and Rao, S.  2003 . Succinct representations of permutations, in Proceedings of the 30th International Colloquium on Automata, Languages and Programming  ICALP , LNCS vol. 2719  Springer , pp. 345–356.  Munro, I. J. and Raman, V.  1997 . Succinct representation of balanced parentheses, static trees and planar graphs, in Proceedings of the 38th Annual IEEE Symposium on Foundations of Computer Science  FOCS   IEEE Computer Society , pp. 118–126.   Bibliography  117  Navarro, G.  2004 . Indexing text using the Ziv-Lempel trie, Journal of Discrete Algorithms  JDA 2,  Navarro, G. and Baeza-Yates, R.  1998 . A practical q-gram index for text retrieval allowing errors,  1, pp. 87–114.  CLEI Electronic Journal 1, 2.  Navarro, G., Baeza-Yates, R., Sutinen, E. and Tarhio, J.  2001 . Indexing methods for approximate  string matching, IEEE Data Engineering Bulletin 24, 4, pp. 19–27.  Navarro, G. and Mäkinen, V.  2007 . Compressed full text indexes, ACM Computing Surveys 39,  1, p. article 2.  Navarro, G., Moura, E., Neubert, M., Ziviani, N. and Baeza-Yates, R.  2000 . Adding compression  to block addressing inverted indexes, Information Retrieval 3, 1, pp. 49–77.  Nitto, I. and Venturini, R.  2008 . On compact representations of all-pairs-shortest-path-distance matrices, in Proceedings of the 19th Annual Symposium on Combinatorial Pattern Matching  CPM , pp. 166–177.  Ouyang, Z., Memon, N., Suel, T. and Trendaﬁlov, D.  2002 . Cluster-based delta compression of a collection of ﬁles, in Proceedings of the 3rd Conference on Web Information Systems Engineering  WISE   IEEE Computer Society , pp. 257–268.  Pˇatra¸scu, M.  2008 . Succincter, in Proceedings of the 49th Annual IEEE Symposium on Foundations  Puglisi, S., Smyth, W. and Turpin, A.  2007 . A taxonomy of sufﬁx array construction algorithms,  of Computer Science  FOCS , pp. 305–313.  ACM Computing Surveys 39, 2.  Puglisi, S. J., Smyth, W. F. and Turpin, A.  2006 . Inverted ﬁles versus sufﬁx arrays for locating patterns in primary memory, in Proceedings of the 13th International Symposium on String Processing and Information Retrieval  SPIRE , LNCS vol. 4209  Springer , pp. 122–133.  Rajpoot, N. and Sahinalp, C.  2002 . Handbook of Lossless Data Compression, chap. Dictionary-  based data compression  Academic Press , pp. 153–167.  Raman, R., Raman, V. and Rao, S. S.  2007 . Succinct indexable dictionaries with applications to  encoding -ary trees, preﬁx sums and multisets, ACM Transactions on Algorithms 3, 4.  Sadakane, K.  2002 . Succinct representations of lcp information and improvements in the com- pressed sufﬁx arrays, in Proceedings of the 13th Annual ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 225–232.  Sadakane, K.  2003 . New text indexing functionalities of the compressed sufﬁx arrays, Journal of  Algorithms 48, 2, pp. 294–313.  Algorithms 5, 1, pp. 12–22.  Sadakane, K.  2007 . Succinct data structures for ﬂexible text retrieval systems, Journal of Discrete  Sadakane, K. and Grossi, R.  2006 . Squeezing succinct data structures into entropy bounds, in Proceedings of the 17th ACM-SIAM Symposium on Discrete Algorithms  SODA , pp. 1230–1239. Sadakane, K. and Navarro, G.  2010 . Fully-functional succinct trees, in Proceedings of the ACM-  SIAM Symposium on Discrete Algorithms  SODA , pp. 134–149.  Salomon, D.  2004 . Data Compression: the Complete Reference, 3rd Edition  Springer Verlag . Salomon, D.  2007 . Variable-length Codes for Data Compression  Springer . Schuegraf, E. J. and Heaps, H. S.  1974 . A comparison of algorithms for data base compression by use of fragments as language elements, Information Storage and Retrieval 10, 9–10, p. 309319. Smith, M. E. G. and Storer, J. A.  1985 . Parallel algorithms for data compression, Journal of the  ACM 32, 2, pp. 344–373.  Suel, T. and Memon, N.  2002 . Algorithms for delta compression and remote ﬁle synchronization,  in K. Sayood  ed. , Lossless Compression Handbook  Academic Press .  Sutinen, E. and Tarhio, J.  1996 . Filtration with q-samples in approximate string matching, in Proceedings of the 7th Annual Symposium on Combinatorial Pattern Matching  CPM , LNCS vol. 1075  Springer , pp. 50–61.  Trendaﬁlov, D., Memon, N. and Suel, T.  2004 . Compressing ﬁle collections with a TSP-based  approach, Tech. rep., Technical Report TR-CIS-2004-02, Polytechnic University.  Ullman, J.  1977 . A binary n-gram technique for automatic correction of substitution, deletion,  insertion and reversal errors in words, The Computer Journal 10, pp. 141–147.   118  Bibliography  Vitter, J. S. and Krishnan, P.  1996 . Optimal prefetching via data compression, Journal of the ACM  43, 5, pp. 771–793.  Science 387, 3, pp. 273–283.  Vo, B. and Vo, K.-P.  2007 . Compressing table data with column dependency, Theoretical Computer  Willard, D. E.  2000 . Examining computational geometry, Van Emde Boas trees, and hashing from  the perspective of the fusion tree, SIAM Journal on Computing 29, 3.  Williams, H. E. and Zobel, J.  2002 . Indexing and retrieval for genomic databases, IEEE Transaction  on Knowledge and Data Engineering 14, 1, pp. 63–78.  Witten, I. H., Moffat, A. and Bell, T. C.  1999 . Managing Gigabytes: Compressing and Indexing Documents and Images, 2nd edn.  Morgan Kaufmann Publishers, Los Altos, CA 94022, USA . Ziv, J.  2007 . Classiﬁcation with ﬁnite memory revisited, IEEE Transactions on Information Theory  53, 12, pp. 4413–4421.  Ziv, J. and Lempel, A.  1977 . A universal algorithm for sequential data compression, IEEE Trans-  action on Information Theory 23, pp. 337–343.  Ziv, J. and Lempel, A.  1978 . Compression of individual sequences via variable length coding,  IEEE Transaction on Information Theory 24, pp. 530–536.  Zobel, J. and Moffat, A.  2006 . Inverted ﬁles for text search engines, ACM Computing Surveys 38,  2, p. 6.
