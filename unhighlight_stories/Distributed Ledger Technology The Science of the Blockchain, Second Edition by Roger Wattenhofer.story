Distributed Ledger Technology  The Science of the Blockchain  Roger Wattenhofer   Copyright c cid:13  2016–2017 by Roger Wattenhofer Second Revised Edition, 2017 First Original Edition, 2016 Inverted Forest Publishing ISBN-13 978-1544232102 ISBN-10 1544232101   Preface  When hanging out with colleagues from ﬁnancial technology  Fin- Tech , you cannot help but notice that the so-called blockchain is all the rage, almost as important as the Internet. Some FinTech colleagues seem to understand the blockchain as a magic piece of code that allows the participants of a distributed system to agree on a common view of the system, to track changes in the system. In the distributed systems community, agreement techniques have been known long before cryptocurrencies such as Bitcoin  where the term blockchain is borrowed  emerged. Various concepts and protocols exist, each with its own advantages and disadvantages.  The idea of this book is to give a scientiﬁcally precise overview of the most interesting approaches that have emerged in the past few decades. If you are a developer  in FinTech or not , this book will help you to get a better understanding what is right and what is wrong for your distributed system, what is possible and what is not.  This Book  This book introduces the basic techniques when building fault- tolerant distributed systems. We will present diﬀerent protocols and algorithms that allow for fault-tolerant operation, and we will discuss practical systems that implement these techniques.  The book presents the main ideas independently from each other, each topic has its own chapter. Each chapter starts with an introductory story that motivates the content of the chapter. Al- gorithms, protocols and deﬁnitions are presented formally, so that you know how to implement the ideas. Some insights are proved in theorems, so that you learn why the concepts or algorithms are correct, and what they can guarantee. Most of the other text is  iii   iv  presented as so-called remarks. These remarks discuss various in- formal thoughts, and often set the stage for the next idea. However, one will get the essence of each chapter even without reading any of these remarks. Each chapter also discusses the history of the ideas presented in the chapter, so that you can ﬁnd the original research.  Apart from minor improvements, this second edition of the book contains two new chapters. Chapter 5 presents an introduction to cryptography, Chapter 9 discusses Bitcoin beyond the blockchain. In the book, we will see diﬀerent models  and combinations of models  that may make sense in diﬀerent scenarios. The focus of the book is on protocols and systems that matter in practice. In other words, we do not discuss concepts because they are fun, but because they are practically relevant.  Nevertheless, have fun!   Contents  1 Introduction  1.1 What are Distributed Systems? . . . . . . . . . . . . 1.2 Book Overview . . . . . . . . . . . . . . . . . . . . .  1 1 2  2 Fault-Tolerance & Paxos  2.1 Client Server . . . . . . . . . . . . . . . . . . . . . . 2.2 Paxos  7 7 . . . . . . . . . . . . . . . . . . . . . . . . . . 12  3 Consensus  19 3.1 Two Friends . . . . . . . . . . . . . . . . . . . . . . . 19 3.2 Consensus . . . . . . . . . . . . . . . . . . . . . . . . 20 Impossibility of Consensus . . . . . . . . . . . . . . . 20 3.3 3.4 Randomized Consensus . . . . . . . . . . . . . . . . 27 3.5 Shared Coin . . . . . . . . . . . . . . . . . . . . . . . 31  4 Byzantine Agreement  35 4.1 Validity . . . . . . . . . . . . . . . . . . . . . . . . . 36 4.2 How Many Byzantine Nodes? . . . . . . . . . . . . . 37 4.3 The King Algorithm . . . . . . . . . . . . . . . . . . 40 . . . . . . . . . 42 4.4 Lower Bound on Number of Rounds 4.5 Asynchronous Byzantine Agreement . . . . . . . . . 43  5 Cryptography Basics  49 5.1 Key Exchange . . . . . . . . . . . . . . . . . . . . . . 49 5.2 Public Key Cryptography . . . . . . . . . . . . . . . 54 5.3 Secret Sharing & Bulk Encryption . . . . . . . . . . 59 5.4 Message Authentication & Passwords . . . . . . . . . 62 5.5 Transport Layer Security . . . . . . . . . . . . . . . 65  v   vi  CONTENTS  6 Authenticated Agreement  71 6.1 Agreement with Authentication . . . . . . . . . . . . 71 6.2 Zyzzyva . . . . . . . . . . . . . . . . . . . . . . . . . 73  7 Quorum Systems  87 7.1 Load and Work . . . . . . . . . . . . . . . . . . . . . 88 7.2 Grid Quorum Systems . . . . . . . . . . . . . . . . . 90 7.3 Fault Tolerance . . . . . . . . . . . . . . . . . . . . . 92 7.4 Byzantine Quorum Systems . . . . . . . . . . . . . . 95  8 Eventual Consistency & Bitcoin  105 8.1 Consistency, Availability and Partitions . . . . . . . 106 8.2 Bitcoin . . . . . . . . . . . . . . . . . . . . . . . . . . 107 8.3 Smart Contracts . . . . . . . . . . . . . . . . . . . . 115 8.4 Weak Consistency . . . . . . . . . . . . . . . . . . . 119  9 Inside Bitcoin  123 9.1 Cryptographic Tools . . . . . . . . . . . . . . . . . . 123 9.2 Script & Message Formats . . . . . . . . . . . . . . . 124 9.3 Players . . . . . . . . . . . . . . . . . . . . . . . . . . 129  10 Distributed Storage  141 10.1 Consistent Hashing . . . . . . . . . . . . . . . . . . . 141 10.2 Hypercubic Networks . . . . . . . . . . . . . . . . . . 143 10.3 DHT & Churn . . . . . . . . . . . . . . . . . . . . . 151   Chapter 1  Introduction  1.1 What are Distributed Systems?  Today’s computing and information systems are inherently distrib- uted. Many companies are operating on a global scale, with thou- sands or even millions of machines on all the continents. Data is stored in various data centers, computing tasks are performed on multiple machines.  At the other end of the spectrum, also your mobile phone is a distributed system. Not only does it probably share some of your data with the cloud, the phone itself contains multiple processing and storage units.  Moreover, computers have come a long way. In the early 1970s, microchips featured a clock rate of roughly 1 MHz. Ten years later, in the early 1980s, you could get a computer with a clock rate of roughly 10 MHz. In the early 1990s, clock speed was around 100 MHz. In the early 2000s, the ﬁrst 1 GHz processor was shipped to customers. Just a few years later, in the mid 2000s, one could already buy processors with clock rates between 3 and 4 GHz. If you buy a new computer today, chances are that the clock rate is still between 3 and 4 GHz, since clock rates basically stopped grow- ing after about 2004. Clock speed can apparently not be increased without running into physical issues such as overheating.  In summary, today almost all computer systems are distributed,  for diﬀerent reasons:    Geography: Large organizations and companies are inher-  ently geographically distributed.  1   2  CHAPTER 1.  INTRODUCTION    Parallelism: In order to speed up computation, we employ  multicore processors or computing clusters.    Reliability: Data is replicated on diﬀerent machines in order  to prevent loss.    Availability: Data is replicated on diﬀerent machines in or- der to allow for fast access, without bottlenecks, minimizing latency.  Even though distributed systems have many beneﬁts, such as increased storage, computational power, or even the possibility to connect spatially separated locations, they also introduce challeng- ing coordination problems. Some say that going from one computer to two is a bit like having a second child. When you have one child and all cookies are gone from the cookie jar, you know who did it! Coordination problems are so prevalent, they come with various ﬂavors and names: blockchain, consistency, agreement, consensus, ledger, event sourcing, etc.  Coordination problems will happen quite often in a distributed system. Even though every single node  computer, core, network switch, etc.  of a distributed system will only fail once every few years, with millions of nodes, you can expect a failure every minute. On the bright side, one may hope that a distributed system with multiple nodes may tolerate some failures and continue to work correctly.  1.2 Book Overview  The central concept of this book will be introduced in Chapter 2 and is generally known as state replication, see Deﬁnition 2.8. We achieve state replication if all nodes of a distributed system agree on a sequence of transactions, the same set of transactions in the same order. In the ﬁnancial tech industry, state replication is often called blockchain or ledger. State replication can be achieved using various algorithms, depending on the failures the system must be able to tolerate.  In Chapter 2 we will motivate and introduce the basic deﬁni- tions, and present Paxos, an algorithm that achieves state replica- tion even though a minority of nodes in the system may crash.  In Chapter 3 we will understand that Paxos may not make progress, and that indeed no deterministic protocol can solve state replication if we are unlucky. However, on the positive side, we will   1.2. BOOK OVERVIEW  3  also introduce a fast randomized consensus protocol that can solve state replication despite crash failures.  In Chapter 4 we look beyond simple crash failures, and intro- duce protocols that work even in the presence of malicious behav- ior, in synchronous and asynchronous systems. In addition, we will explore diﬀerent deﬁnitions for correct behavior.  In Chapter 5 we introduce some basic cryptographic princi- ples that will help us to implement various eﬃcient protocols in the remaining chapters of the book. We do not tailor our descrip- tion towards any speciﬁc cryptographic tool, but describe the basic mathematical foundations.  In Chapter 6 we use a cryptographic concept called message au- thentication. We ﬁrst present a simple synchronous protocol, and then Zyzzyva, a state of the art asynchronous protocol for imple- menting state replication if message authentication is available.  In Chapter 7 we investigate scalability issues by studying so- called quorum systems. If a set of servers is no longer powerful enough, and adding more servers does not help, quorum systems may be an elegant solution.  In Chapter 8 we introduce weaker consistency concepts, and provide a introductory description of Bitcoin as a prime example of such a protocol.  In Chapter 9 we dig a bit deeper and explain some of the fasci- nating details of Bitcoin, such as how the Bitcoin script is deﬁned. Finally, in Chapter 10 we explore even weaker consistency con-  cepts, and present highly scalable distributed storage solutions.  Chapter Notes  Many good textbooks have been written on the subject, e.g. [AW04, CGR11, CDKB11, Lyn96, Mul93, Ray13, TS01]. James Aspnes has written an excellent freely available script on distributed systems [Asp14]. Similarly to our book, these focus on large-scale distrib- uted systems, and hence there is some overlap with our book. There are also some excellent textbooks focusing on small-scale multicore systems, e.g. [HS08].  Some colleagues have helped in writing and improving this book. Thanks go to Pascal Bissig, Philipp Brandes, Christian Decker, Klaus-Tycho F¨orster, Arthur Gervais, Barbara Keller, Rik Melis, Darya Melnyk, Peter Robinson, David Stolz, and Saravanan Vijayakumaran. Jinchuan Chen, Qiang Lin, Yunzhi Xue, and Qing   4  CHAPTER 1.  INTRODUCTION  Zhu translated this book into Simpliﬁed Chinese, and along the way found improvements to the English version as well. Thanks!  Bibliography  [Asp14] James Aspnes. Notes on Theory of Distributed Sys-  tems, 2014.  [AW04] Hagit Attiya and Jennifer Welch. Distributed Comput- ing: Fundamentals, Simulations and Advanced Topics  2nd edition . John Wiley Interscience, March 2004.  [CDKB11] George Coulouris, Jean Dollimore, Tim Kindberg, and Gordon Blair. Distributed Systems: Concepts and De- sign. Addison-Wesley Publishing Company, USA, 5th edition, 2011.  [CGR11] Christian Cachin, Rachid Guerraoui, and Lus Ro- drigues. Introduction to Reliable and Secure Distributed Programming. Springer Publishing Company, Incorpo- rated, 2nd edition, 2011.  [HS08] Maurice Herlihy and Nir Shavit. The Art of Multi- processor Programming. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2008.  [Lyn96] Nancy A. Lynch. Distributed Algorithms. Morgan Kauf- mann Publishers Inc., San Francisco, CA, USA, 1996.  [Mul93] Sape Mullender, editor. Distributed Systems  2nd Ed. . ACM Press Addison-Wesley Publishing Co., New York, NY, USA, 1993.  [Ray13] Michel Raynal. Distributed Algorithms for Message- Passing Systems. Springer Publishing Company, Incor- porated, 2013.  [TS01] Andrew S. Tanenbaum and Maarten Van Steen. Dis- tributed Systems: Principles and Paradigms. Prentice Hall PTR, Upper Saddle River, NJ, USA, 1st edition, 2001.   BIBLIOGRAPHY  Bibliography  5  [Asp14] James Aspnes. Notes on Theory of Distributed Sys-  tems, 2014.  [AW04] Hagit Attiya and Jennifer Welch. Distributed Comput- ing: Fundamentals, Simulations and Advanced Topics  2nd edition . John Wiley Interscience, March 2004.  [CDKB11] George Coulouris, Jean Dollimore, Tim Kindberg, and Gordon Blair. Distributed Systems: Concepts and De- sign. Addison-Wesley Publishing Company, USA, 5th edition, 2011.  [CGR11] Christian Cachin, Rachid Guerraoui, and Lus Ro- drigues. Introduction to Reliable and Secure Distributed Programming. Springer Publishing Company, Incorpo- rated, 2nd edition, 2011.  [HS08] Maurice Herlihy and Nir Shavit. The Art of Multi- processor Programming. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2008.  [Lyn96] Nancy A. Lynch. Distributed Algorithms. Morgan Kauf- mann Publishers Inc., San Francisco, CA, USA, 1996.  [Mul93] Sape Mullender, editor. Distributed Systems  2nd Ed. . ACM Press Addison-Wesley Publishing Co., New York, NY, USA, 1993.  [Ray13] Michel Raynal. Distributed Algorithms for Message- Passing Systems. Springer Publishing Company, Incor- porated, 2013.  [TS01] Andrew S. Tanenbaum and Maarten Van Steen. Dis- tributed Systems: Principles and Paradigms. Prentice Hall PTR, Upper Saddle River, NJ, USA, 1st edition, 2001.   6  CHAPTER 1.  INTRODUCTION   Chapter 2  Fault-Tolerance & Paxos  How do you create a fault-tolerant distributed system? In this chapter we start out with simple questions, and, step by step, im- prove our solutions until we arrive at a system that works even under adverse circumstances, Paxos.  2.1 Client Server  Deﬁnition 2.1  node . We call a single actor in the system node. In a computer network the computers are the nodes, in the classical client-server model both the server and the client are nodes, and so on. If not stated otherwise, the total number of nodes in the system is n.  Model 2.2  message passing . In the message passing model we study distributed systems that consist of a set of nodes. Each node can perform local computations, and can send messages to every other node.  Remarks:    We start with two nodes, the smallest number of nodes in a distributed system. We have a client node that wants to “manipulate” data  e.g., store, update, . . .   on a remote server node.  7   8  CHAPTER 2. FAULT-TOLERANCE & PAXOS  Algorithm 2.3 Na¨ıve Client-Server Algorithm  1: Client sends commands one at a time to server  Model 2.4  message loss . In the message passing model with message loss, for any speciﬁc message, it is not guaranteed that it will arrive safely at the receiver.  Remarks:    A related problem is message corruption, i.e., a message is received but the content of the message is corrupted. In practice, in contrast to message loss, message corruption can be handled quite well, e.g. by including additional information in the message, such as a checksum.    Algorithm 2.3 does not work correctly if there is message  loss, so we need a little improvement.  Algorithm 2.5 Client-Server Algorithm with Acknowledgments  1: Client sends commands one at a time to server 2: Server acknowledges every command 3: If the client does not receive an acknowledgment within a rea-  sonable time, the client resends the command  Remarks:    Sending commands “one at a time” means that when the client sent command c, the client does not send any new command c cid:48  until it received an acknowledgment for c.   Since not only messages sent by the client can be lost, but also acknowledgments, the client might resend a message that was already received and executed on the server. To prevent multiple executions of the same command, one can add a sequence number to each message, allowing the receiver to identify duplicates.    This simple algorithm is the basis of many reliable pro-  tocols, e.g. TCP.    The algorithm can easily be extended to work with mul- tiple servers: The client sends each command to every   2.1. CLIENT SERVER  9  server, and once the client received an acknowledgment from each server, the command is considered to be exe- cuted successfully.    What about multiple clients?  Model 2.6  variable message delay . In practice, messages might experience diﬀerent transmission times, even if they are being sent between the same two nodes.  Remarks:    Throughout this chapter, we assume the variable message  delay model.  Theorem 2.7. If Algorithm 2.5 is used with multiple clients and multiple servers, the servers might see the commands in diﬀerent order, leading to an inconsistent state.  Proof. Assume we have two clients u1 and u2, and two servers s1 and s2. Both clients issue a command to update a variable x on the servers, initially x = 0. Client u1 sends command x = x + 1 and client u2 sends x = 2 · x.  Let both clients send their message at the same time. With variable message delay, it can happen that s1 receives the message from u1 ﬁrst, and s2 receives the message from u2 ﬁrst.1 Hence, s1 computes x =  0 + 1  · 2 = 2 and s2 computes x =  0 · 2  + 1 = 1.  Deﬁnition 2.8  state replication . A set of nodes achieves state replication, if all nodes execute a  potentially inﬁnite  sequence of commands c1, c2, c3, . . . , in the same order.  Remarks:    State replication is a fundamental property for distrib-  uted systems.    For people working in the ﬁnancial tech industry, state replication is often synonymous with the term blockchain. The Bitcoin blockchain we will discuss in Chapter 8 is in- deed one way to implement state replication. However, as we will see in all the other chapters, there are many al- ternative concepts that are worth knowing, with diﬀerent properties.  1For example, u1 and s1 are  geographically  located close to each other,  and so are u2 and s2.   10  CHAPTER 2. FAULT-TOLERANCE & PAXOS    Since state replication is trivial with a single server, we can designate a single server as a serializer. By letting the serializer distribute the commands, we automatically order the requests and achieve state replication!  Algorithm 2.9 State Replication with a Serializer  1: Clients send commands one at a time to the serializer 2: Serializer forwards commands one at a time to all other servers  3: Once the serializer received all acknowledgments, it notiﬁes the  client about the success  Remarks:    This idea is sometimes also referred to as master-slave  replication.  of failure!    What about node failures? Our serializer is a single point    Can we have a more distributed approach of solving state replication? Instead of directly establishing a consistent order of commands, we can use a diﬀerent approach: We make sure that there is always at most one client sending a command; i.e., we use mutual exclusion, respectively locking.  Algorithm 2.10 Two-Phase Protocol  1: Client asks all servers for the lock  2: if client receives lock from every server then 3:  Client sends command reliably to each server, and gives the lock back  Clients gives the received locks back Client waits, and then starts with Phase 1 again  Phase 1  Phase 2  4: else 5: 6: 7: end if   2.1. CLIENT SERVER  11  Remarks:    This idea appears in many contexts and with diﬀerent names, usually with slight variations, e.g. two-phase lock- ing  2PL .    Another example is the two-phase commit  2PC  proto- col, typically presented in a database environment. The ﬁrst phase is called the preparation of a transaction, and in the second phase the transaction is either committed or aborted. The 2PC process is not started at the client but at a designated server node that is called the coordinator.   It is often claimed that 2PL and 2PC provide better con- sistency guarantees than a simple serializer if nodes can recover after crashing. In particular, alive nodes might be kept consistent with crashed nodes, for transactions that started while the crashed node was still running. This beneﬁt was even improved in a protocol that uses an additional phase  3PC .    The problem with 2PC or 3PC is that they are not well-  deﬁned if exceptions happen.    Does Algorithm 2.10 really handle node crashes well? No! In fact, it is even worse than the simple serializer ap- proach  Algorithm 2.9 : Instead of needing one available node, Algorithm 2.10 requires all servers to be respon- sive!    Does Algorithm 2.10 also work if we only get the lock from a subset of servers? Is a majority of servers enough?   What if two or more clients concurrently try to acquire a majority of locks? Do clients have to abandon their al- ready acquired locks, in order not to run into a deadlock? How? And what if they crash before they can release the locks?    Bad news: It seems we need a slightly more complicated  concept.    Good news: We postpone the complexity of achieving state replication and ﬁrst show how to execute a single command only.   12  CHAPTER 2. FAULT-TOLERANCE & PAXOS  2.2 Paxos  Deﬁnition 2.11  ticket . A ticket is a weaker form of a lock, with the following properties:    Reissuable: A server can issue a ticket, even if previously  issued tickets have not yet been returned.    Ticket expiration: If a client sends a message to a server using a previously acquired ticket t, the server will only accept t, if t is the most recently issued ticket.  Remarks:    There is no problem with crashes: If a client crashes while holding a ticket, the remaining clients are not aﬀected, as servers can simply issue new tickets.    Tickets can be implemented with a counter: Each time a ticket is requested, the counter is increased. When a client tries to use a ticket, the server can determine if the ticket is expired.    What can we do with tickets? Can we simply replace the locks in Algorithm 2.10 with tickets? We need to add at least one additional phase, as only the client knows if a majority of the tickets have been valid in Phase 2.   2.2. PAXOS  13  Algorithm 2.12 Na¨ıve Ticket Protocol  1: Client asks all servers for a ticket  2: if a majority of the servers replied then 3: 4:  Client sends command together with ticket to each server Server stores command only if ticket is still valid, and replies to client  Client waits, and then starts with Phase 1 again  8: if client hears a positive answer from a majority of the servers  Client tells servers to execute the stored command  Client waits, and then starts with Phase 1 again  Phase 1  Phase 2  5: else 6: 7: end if  Phase 3  then  9: 10: else 11: 12: end if  Remarks:    There are problems with this algorithm: Let u1 be the ﬁrst client that successfully stores its command c1 on a majority of the servers. Assume that u1 becomes very slow just before it can notify the servers  Line 9 , and a client u2 updates the stored command in some servers to c2. Afterwards, u1 tells the servers to execute the command. Now some servers will execute c1 and others c2!    How can this problem be ﬁxed? We know that every client u2 that updates the stored command after u1 must have used a newer ticket than u1. As u1’s ticket was accepted in Phase 2, it follows that u2 must have acquired its ticket after u1 already stored its value in the respective server.    Idea: What if a server, instead of only handing out tickets in Phase 1, also notiﬁes clients about its currently stored   14  CHAPTER 2. FAULT-TOLERANCE & PAXOS  command? Then, u2 learns that u1 already stored c1 and instead of trying to store c2, u2 could support u1 by also storing c1. As both clients try to store and execute the same command, the order in which they proceed is no longer a problem.    But what if not all servers have the same command stored, and u2 learns multiple stored commands in Phase 1. What command should u2 support?    Observe that it is always safe to support the most recently stored command. As long as there is no majority, clients can support any command. However, once there is a majority, clients need to support this value.    So, in order to determine which command was stored most recently, servers can remember the ticket number that was used to store the command, and afterwards tell this number to clients in Phase 1.    If every server uses its own ticket numbers, the newest ticket does not necessarily have the largest number. This problem can be solved if clients suggest the ticket num- bers themselves!   2.2. PAXOS  15  Algorithm 2.13 Paxos  Client  Proposer   Server  Acceptor   Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  c t = 0   cid:47  command to execute  cid:47  ticket number to try  Tmax = 0  cid:47  largest issued ticket C = ⊥ Tstore = 0  cid:47  ticket used to store C Phase 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   cid:47  stored command  1: t = t + 1 2: Ask all servers for ticket t  3: if t > Tmax then 4: 5: 6: end if  Tmax = t Answer with ok Tstore, C   Phase 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  7: if a majority answers ok then 8:  Pick  Tstore, C  with largest Tstore if Tstore > 0 then  c = C  end if Send propose t, c  to same majority  9: 10: 11: 12:  13: end if  14: if t = Tmax then 15: 16: 17: 18: end if  C = c Tstore = t Answer success  Phase 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19: if a majority answers success  Send execute c  to every server  then  20: 21: end if  Remarks:    Unlike previously mentioned algorithms, there is no step where a client explicitly decides to start a new attempt and jumps back to Phase 1. Note that this is not neces- sary, as a client can decide to abort the current attempt and start a new one at any point in the algorithm. This has the advantage that we do not need to be careful about selecting “good” values for timeouts, as correctness is in- dependent of the decisions when to start new attempts.   The performance can be improved by letting the servers   16  CHAPTER 2. FAULT-TOLERANCE & PAXOS  send negative replies in phases 1 and 2 if the ticket ex- pired.    The contention between diﬀerent clients can be alleviated by randomizing the waiting times between consecutive attempts.  Lemma 2.14. We call a message propose t,c  sent by clients on Line 12 a proposal for  t,c . A proposal for  t,c  is chosen, if it is stored by a majority of servers  Line 15 . For every issued propose t cid:48 ,c cid:48   with t cid:48  > t holds that c cid:48  = c, if there was a chosen propose t,c .  Proof. Observe that there can be at most one proposal for every ticket number τ since clients only send a proposal if they received a majority of the tickets for τ  Line 7 . Hence, every proposal is uniquely identiﬁed by its ticket number τ . Assume that there is at least one propose t cid:48 ,c cid:48   with t cid:48  > t and c cid:48   cid:54 = c; of such proposals, consider the proposal with the smallest ticket number t cid:48 . Since both this proposal and also the propose t,c  have been sent to a majority of the servers, we can denote by S the non-empty intersection of servers that have been involved in both proposals. Recall that since propose t,c  has been chosen, this means that that at least one server s ∈ S must have stored com- mand c; thus, when the command was stored, the ticket number t was still valid. Hence, s must have received the request for ticket t cid:48  after it already stored propose t,c , as the request for ticket t cid:48  invalidates ticket t. Therefore, the client that sent propose t cid:48 ,c cid:48   must have learned from s that a client already stored propose t,c . Since a client adapts its proposal to the command that is stored with the highest ticket number so far  Line 8 , the client must have proposed c as well. There is only one possibility that would lead to the client not adapting c: If the client received the information from a server that some client stored propose t∗,c∗ , with c∗  cid:54 = c and t∗ > t. But in that case, a client must have sent propose t∗,c∗  with t < t∗ < t cid:48 , but this contradicts the assumption that t cid:48  is the smallest ticket number of a proposal issued after t.  Theorem 2.15. If a command c is executed by some servers, all servers  eventually  execute c.  Proof. From Lemma 2.14 we know that once a proposal for c is chosen, every subsequent proposal is for c. As there is exactly   2.2. PAXOS  17  one ﬁrst propose t,c  that is chosen, it follows that all successful proposals will be for the command c. Thus, only proposals for a single command c can be chosen, and since clients only tell servers to execute a command, when it is chosen  Line 20 , each client will eventually tell every server to execute c.  Remarks:    If the client with the ﬁrst successful proposal does not  crash, it will directly tell every server to execute c.    However, if the client crashes before notifying any of the servers, the servers will execute the command only once the next client is successful. Once a server received a re- quest to execute c, it can inform every client that arrives later that there is already a chosen command, so that the client does not waste time with the proposal process.    Note that Paxos cannot make progress if half  or more  of the servers crash, as clients cannot achieve a majority anymore.    The original description of Paxos uses three roles: Pro- posers, acceptors and learners. Learners have a trivial role: They do nothing, they just learn from other nodes which command was chosen.    We assigned every node only one role. In some scenarios, it might be useful to allow a node to have multiple roles. For example in a peer-to-peer scenario nodes need to act as both client and server.    Clients  Proposers  must be trusted to follow the pro- tocol strictly. However, this is in many scenarios not a reasonable assumption. In such scenarios, the role of the proposer can be executed by a set of servers, and clients need to contact proposers, to propose values in their name.    So far, we only discussed how a set of nodes can reach decision for a single command with the help of Paxos. We call such a single decision an instance of Paxos.    For state replication as in Deﬁnition 2.8, we need to be able to execute multiple commands, we can extend each   18  CHAPTER 2. FAULT-TOLERANCE & PAXOS  instance with an instance number, that is sent around with every message. Once the 1st command is chosen, any client can decide to start a new instance and compete for the 2nd command. If a server did not realize that the 1st instance already came to a decision, the server can ask other servers about the decisions to catch up.  Chapter Notes  Two-phase protocols have been around for a long time, and it is unclear if there is a single source of this idea. One of the earlier descriptions of this concept can found in the book of Gray [Gra78]. Leslie Lamport introduced Paxos in 1989. But why is it called Paxos? Lamport described the algorithm as the solution to a prob- lem of the parliament of a ﬁctitious Greek society on the island Paxos. He even liked this idea so much, that he gave some lec- tures in the persona of an Indiana-Jones-style archaeologist! When the paper was submitted, many readers were so distracted by the descriptions of the activities of the legislators, they did not under- stand the meaning and purpose of the algorithm. The paper was rejected. But Lamport refused to rewrite the paper, and he later wrote that he “was quite annoyed at how humorless everyone work- ing in the ﬁeld seemed to be”. A few years later, when the need for a protocol like Paxos arose again, Lamport simply took the paper out of the drawer and gave it to his colleagues. They liked it. So Lamport decided to submit the paper  in basically unaltered form!  again, 8 years after he wrote it – and it got accepted! But as this paper [Lam98] is admittedly hard to read, he had mercy, and later wrote a simpler description of Paxos [Lam01].  Bibliography  [Gra78] James N Gray. Notes on data base operating systems.  Springer, 1978.  [Lam98] Leslie Lamport. The part-time parliament. ACM Trans- actions on Computer Systems  TOCS , 16 2 :133–169, 1998.  [Lam01] Leslie Lamport. Paxos made simple. ACM Sigact News,  32 4 :18–25, 2001.   Chapter 3  Consensus  3.1 Two Friends  Alice wants to arrange dinner with Bob, and since both of them are very reluctant to use the “call” functionality of their phones, she sends a text message suggesting to meet for dinner at 6pm. However, texting is unreliable, and Alice cannot be sure that the message arrives at Bob’s phone, hence she will only go to the meet- ing point if she receives a conﬁrmation message from Bob. But Bob cannot be sure that his conﬁrmation message is received; if the con- ﬁrmation is lost, Alice cannot determine if Bob did not even receive her suggestion, or if Bob’s conﬁrmation was lost. Therefore, Bob demands a conﬁrmation message from Alice, to be sure that she will be there. But as this message can also be lost. . .  You can see that such a message exchange continues forever, if both Alice and Bob want to be sure that the other person will come to the meeting point!  Remarks:    Such a protocol cannot terminate: Assume that there are protocols which lead to agreement, and P is one of the protocols which require the least number of messages. As the last conﬁrmation might be lost and the protocol still needs to guarantee agreement, we can simply decide to always omit the last message. This gives us a new proto- col P  cid:48  which requires less messages than P , contradicting the assumption that P required the minimal amount of messages.  19   20  CHAPTER 3. CONSENSUS    Can Alice and Bob use Paxos?  3.2 Consensus  In Chapter 2 we studied a problem that we vaguely called agree- ment. We will now introduce a formally speciﬁed variant of this problem, called consensus.  Deﬁnition 3.1  consensus . There are n nodes, of which at most f might crash, i.e., at least n − f nodes are correct. Node i starts with an input value vi. The nodes must decide for one of those values, satisfying the following properties:    Agreement All correct nodes decide for the same value.   Termination All correct nodes terminate in ﬁnite time.   Validity The decision value must be the input value of a  node.  Remarks:    We assume that every node can send messages to every other node, and that we have reliable links, i.e., a message that is sent will be received.    There is no broadcast medium. If a node wants to send a message to multiple nodes, it needs to send multiple individual messages.    Does Paxos satisfy all three criteria? If you study Paxos carefully, you will notice that Paxos does not guaran- tee termination. For example, the system can be stuck forever if two clients continuously request tickets, and neither of them ever manages to acquire a majority.  3.3 Impossibility of Consensus  Model 3.2  asynchronous . In the asynchronous model, algo- rithms are event based  “upon receiving message . . . , do . . . ” . Nodes do not have access to a synchronized wall-clock. A message sent from one node to another will arrive in a ﬁnite but unbounded time.   3.3.  IMPOSSIBILITY OF CONSENSUS  21  Remarks:    The asynchronous time model is a widely used formaliza-  tion of the variable message delay model  Model 2.6 .  Deﬁnition 3.3  asynchronous runtime . For algorithms in the asynchronous model, the runtime is the number of time units from the start of the execution to its completion in the worst case  every legal input, every execution scenario , assuming that each message has a delay of at most one time unit.  Remarks:    The maximum delay cannot be used in the algorithm design, i.e., the algorithm must work independent of the actual delay.    Asynchronous algorithms can be thought of as systems, where local computation is signiﬁcantly faster than mes- sage delays, and thus can be done in no time. Nodes are only active once an event occurs  a message arrives , and then they perform their actions “immediately”.    We will show now that crash failures in the asynchro- nous model can be quite harsh. In particular there is no deterministic fault-tolerant consensus algorithm in the asynchronous model, not even for binary input.  Deﬁnition 3.4  conﬁguration . We say that a system is fully de- ﬁned  at any point during the execution  by its conﬁguration C. The conﬁguration includes the state of every node, and all messages that are in transit  sent but not yet received .  Deﬁnition 3.5  univalent . We call a conﬁguration C univalent, if the decision value is determined independently of what happens afterwards.  Remarks:  valent.    We call a conﬁguration that is univalent for value v v-    Note that a conﬁguration can be univalent, even though no single node is aware of this. For example, the conﬁg- uration in which all nodes start with value 0 is 0-valent  due to the validity requirement .   22  CHAPTER 3. CONSENSUS    As we restricted the input values to be binary, the deci- sion value of any consensus algorithm will also be binary  due to the validity requirement .  Deﬁnition 3.6  bivalent . A conﬁguration C is called bivalent if the nodes might decide for 0 or 1.  Remarks:    The decision value depends on the order in which mes- sages are received or on crash events. I.e., the decision is not yet made.    We call the initial conﬁguration of an algorithm C0. When nodes are in C0, all of them executed their initialization code and possibly, based on their input values, sent some messages. These initial messages are also included in C0. In other words, in C0 the nodes are now waiting for the ﬁrst message to arrive.  Lemma 3.7. There is at least one selection of input values V such that the according initial conﬁguration C0 is bivalent, if f ≥ 1. Proof. As explained in the previous remark, C0 only depends on the input values of the nodes. Let V = [v0, v1, . . . , vn−1] denote the array of input values, where vi is the input value of node i.  We construct n + 1 arrays V0, V1, . . . , Vn, where the index i in Vi denotes the position in the array up to which all input values are 1. So, V0 = [0, 0, 0, . . . , 0], V1 = [1, 0, 0, . . . , 0], and so on, up to Vn = [1, 1, 1, . . . , 1].  Note that the conﬁguration corresponding to V0 must be 0- valent so that the validity requirement is satisﬁed. Analogously, the conﬁguration corresponding to Vn must be 1-valent. Assume that all initial conﬁgurations with starting values Vi are univalent. Therefore, there must be at least one index b, such that the con- ﬁguration corresponding to Vb is 0-valent, and conﬁguration corre- sponding to Vb+1 is 1-valent. Observe that only the input value of the bth node diﬀers from Vb to Vb+1. Since we assumed that the algorithm can tolerate at least one failure, i.e., f ≥ 1, we look at the following execution: All nodes except b start with their initial value according to Vb respectively Vb+1. Node b is “extremely slow”; i.e., all messages sent by b are scheduled in such a way, that all other nodes must assume that b crashed, in order to satisfy the termination requirement. Since the   3.3.  IMPOSSIBILITY OF CONSENSUS  23  nodes cannot determine the value of b, and we assumed that all initial conﬁgurations are univalent, they will decide for a value v independent of the initial value of b. Since Vb is 0-valent, v must be 0. However we know that Vb+1 is 1-valent, thus v must be 1. Since v cannot be both 0 and 1, we have a contradiction.  Deﬁnition 3.8  transition . A transition from conﬁguration C to a following conﬁguration Cτ is characterized by an event τ =  u, m , i.e., node u receiving message m.  Remarks:    Transitions are the formally deﬁned version of the “events”  in the asynchronous model we described before.    A transition τ =  u, m  is only applicable to C, if m was  still in transit in C.    Cτ diﬀers from C as follows: m is no longer in transit, u has possibly a diﬀerent state  as u can update its state based on m , and there are  potentially  new messages in transit, sent by u.  Deﬁnition 3.9  conﬁguration tree . The conﬁguration tree is a directed tree of conﬁgurations. Its root is the conﬁguration C0 which is fully characterized by the input values V . The edges of the tree are the transitions; every conﬁguration has all applicable transitions as outgoing edges.  Remarks:    For any algorithm, there is exactly one conﬁguration tree  for every selection of input values.    Leaves are conﬁgurations where the execution of the al- gorithm terminated. Note that we use termination in the sense that the system as a whole terminated, i.e., there will not be any transition anymore.    Every path from the root to a leaf is one possible asyn-  chronous execution of the algorithm.    Leaves must be univalent, or the algorithm terminates  without agreement.   24  CHAPTER 3. CONSENSUS    If a node u crashes when the system is in C, all transitions   u,∗  are removed from C in the conﬁguration tree.  Lemma 3.10. Assume two transitions τ1 =  u1, m1  and τ2 =  u2, m2  for u1  cid:54 = u2 are both applicable to C. Let Cτ1τ2 be the conﬁguration that follows C by ﬁrst applying transition τ1 and then τ2, and let Cτ2τ1 be deﬁned analogously. It holds that Cτ1τ2 = Cτ2τ1 .  Proof. Observe that τ2 is applicable to Cτ1 , since m2 is still in tran- sit and τ1 cannot change the state of u2. With the same argument τ1 is applicable to Cτ2, and therefore both Cτ1τ2 and Cτ2τ1 are well- deﬁned. Since the two transitions are completely independent of each other, meaning that they consume the same messages, lead to the same state transitions and to the same messages being sent, it follows that Cτ1τ2 = Cτ2τ1.  Deﬁnition 3.11  critical conﬁguration . We say that a conﬁgura- tion C is critical, if C is bivalent, but all conﬁgurations that are direct children of C in the conﬁguration tree are univalent.  Remarks:    Informally, C is critical, if it is the last moment in the execution where the decision is not yet clear. As soon as the next message is processed by any node, the decision will be determined.  Lemma 3.12. If a system is in a bivalent conﬁguration, it must reach a critical conﬁguration within ﬁnite time, or it does not al- ways solve consensus.  Proof. Recall that there is at least one bivalent initial conﬁguration  Lemma 3.7 . Assuming that this conﬁguration is not critical, there must be at least one bivalent following conﬁguration; hence, the system may enter this conﬁguration. But if this conﬁguration is not critical as well, the system may afterwards progress into another bivalent conﬁguration. As long as there is no critical conﬁguration, an unfortunate scheduling  selection of transitions  can always lead the system into another bivalent conﬁguration. The only way how an algorithm can enforce to arrive in a univalent conﬁguration is by reaching a critical conﬁguration.  Therefore we can conclude that a system which does not reach a critical conﬁguration has at least one possible execution where   3.3.  IMPOSSIBILITY OF CONSENSUS  25  it will terminate in a bivalent conﬁguration  hence it terminates without agreement , or it will not terminate at all.  Lemma 3.13. If a conﬁguration tree contains a critical conﬁgura- tion, crashing a single node can create a bivalent leaf; i.e., a crash prevents the algorithm from reaching agreement.  Proof. Let C denote critical conﬁguration in a conﬁguration tree, and let T be the set of transitions applicable to C. Let τ0 =  u0, m0  ∈ T and τ1 =  u1, m1  ∈ T be two transitions, and let Cτ0 be 0-valent and Cτ1 be 1-valent. Note that T must contain these transitions, as C is a critical conﬁguration. Assume that u0  cid:54 = u1. Using Lemma 3.10 we know that C has a following conﬁguration Cτ0τ1 = Cτ1τ0. Since this conﬁguration follows Cτ0 it must be 0-valent. However, this conﬁguration also follows Cτ1 and must hence be 1-valent. This is a contradiction and therefore u0 = u1 must hold. Therefore we can pick one particular node u for which there is a transition τ =  u, m  ∈ T which leads to a 0-valent conﬁgura- tion. As shown before, all transitions in T which lead to a 1-valent conﬁguration must also take place on u. Since C is critical, there must be at least one such transition. Applying the same argument again, it follows that all transitions in T that lead to a 0-valent conﬁguration must take place on u as well, and since C is critical, there is no transition in T that leads to a bivalent conﬁguration. Therefore all transitions applicable to C take place on the same node u!  If this node u crashes while the system is in C, all transitions are removed, and therefore the system is stuck in C, i.e., it terminates in C. But as C is critical, and therefore bivalent, the algorithm fails to reach an agreement.  Theorem 3.14. There is no deterministic algorithm which always achieves consensus in the asynchronous model, with f > 0.  Proof. We assume that the input values are binary, as this is the easiest non-trivial possibility. From Lemma 3.7 we know that there must be at least one bivalent initial conﬁguration C. Using Lemma 3.12 we know that if an algorithm solves consensus, all executions starting from the bivalent conﬁguration C must reach a critical conﬁguration. But if the algorithm reaches a critical conﬁguration, a single crash can prevent agreement  Lemma 3.13 .   26  Remarks:  CHAPTER 3. CONSENSUS    If f = 0, then each node can simply send its value to all  others, wait for all values, and choose the minimum.    But if a single node may crash, there is no deterministic  solution to consensus in the asynchronous model.    How can the situation be improved? For example by giving each node access to randomness, i.e., we allow each node to toss a coin.   3.4. RANDOMIZED CONSENSUS  27  3.4 Randomized Consensus  Algorithm 3.15 Randomized Consensus  Ben-Or  1: vi ∈ {0, 1} 2: round = 1 3: decided = false   cid:47  input bit  4: Broadcast myValue vi, round   5: while true do  Propose  6: Wait until a majority of myValue messages of current round  arrived if all messages contain the same value v then  else  Broadcast propose v, round  Broadcast propose ⊥, round   end if  if decided then  Broadcast myValue vi, round+1  Decide for vi and terminate  end if  Adapt  16: Wait until a majority of propose messages of current round  arrived if all messages propose the same value v then  vi = v decide = true  else if there is at least one proposal for v then  Choose vi randomly, with P r[vi = 0] = P r[vi = 1] = 1 2  vi = v  else  22: 23: 24: 25: 26: 27: end while  end if round = round + 1 Broadcast myValue vi, round   7: 8:  9: 10: 11:  12: 13:  14: 15:  17: 18: 19: 20: 21:   28  Remarks:  CHAPTER 3. CONSENSUS    The idea of Algorithm 3.15 is very simple: Either all nodes start with the same input bit, which makes con- sensus easy. Otherwise, nodes toss a coin until a large number of nodes get – by chance – the same outcome.  Lemma 3.16. As long as no node sets decided to true, Algorithm 3.15 always makes progress, independent of which nodes crash.  Proof. The only two steps in the algorithm when a node waits are in Lines 6 and 15. Since a node only waits for a majority of the nodes to send a message, and since f < n 2, the node will always receive enough messages to continue, as long as no correct node set its value decided to true and terminates.  Lemma 3.17. Algorithm 3.15 satisﬁes the validity requirement.  Proof. Observe that the validity requirement of consensus, when restricted to binary input values, corresponds to: If all nodes start with v, then v must be chosen; otherwise, either 0 or 1 is acceptable, and the validity requirement is automatically satisﬁed.  Assume that all nodes start with v.  In this case, all nodes propose v in the ﬁrst round. As all nodes only hear proposals for v, all nodes decide for v  Line 17  and exit the loop in the following round.  Lemma 3.18. Algorithm 3.15 satisﬁes the agreement requirement.  Proof. Observe that proposals for both 0 and 1 cannot occur in the same round, as nodes only send a proposal for v, if they hear a majority for v in Line 8.  Let u be the ﬁrst node that decides for a value v in round r. Hence, it received a majority of proposals for v in r  Line 17 . Note that once a node receives a majority of proposals for a value, it will adapt this value and terminate in the next round. Since there cannot be a proposal for any other value in r, it follows that no node decides for a diﬀerent value in r.  In Lemma 3.16 we only showed that nodes make progress as long as no node decides, thus we need to be careful that no node gets stuck if u terminates. Any node u cid:48   cid:54 = u can experience one of two scenarios: Either it also receives a majority for v in round r and decides, or it does not receive a majority. In the ﬁrst case, the agreement requirement   3.4. RANDOMIZED CONSENSUS  29  is directly satisﬁed, and also the node cannot get stuck. Let us study the latter case. Since u heard a majority of proposals for v, it follows that every node hears at least one proposal for v. Hence, all nodes set their value vi to v in round r. Therefore, all nodes will broadcast v at the end of round r, and thus all nodes will propose v in round r + 1. The nodes that already decided in round r will terminate in r + 1 and send one additional myValue message  Line 13 . All other nodes will receive a majority of proposals for v in r + 1, and will set decided to true in round r + 1, and also send a myValue message in round r + 1. Thus, in round r + 2 some nodes have already terminated, and others hear enough myValue messages to make progress in Line 6. They send another propose and a myValue message and terminate in r + 2, deciding for the same value v.  Lemma 3.19. Algorithm 3.15 satisﬁes the termination require- ment, i.e., all nodes terminate in expected time O 2n .  Proof. We know from the proof of Lemma 3.18 that once a node hears a majority of proposals for a value, all nodes will terminate at most two rounds later. Hence, we only need to show that a node receives a majority of proposals for the same value within expected time O 2n .  Assume that no node receives a majority of proposals for the same value. In such a round, some nodes may update their value to v based on a proposal  Line 20 . As shown before, all nodes that update the value based on a proposal, adapt the same value v. The rest of the nodes choses 0 or 1 randomly. The probability that all nodes choose the same value v in one round is hence at least 1 2n. Therefore, the expected number of rounds is bounded by O 2n . As every round consists of two message exchanges, the asymptotic runtime of the algorithm is equal to the number of rounds.  Theorem 3.20. Algorithm 3.15 achieves binary consensus with expected runtime O 2n  if up to f < n 2 nodes crash.  Remarks:    How good is a fault tolerance of f < n 2?  Theorem 3.21. There is no consensus algorithm for the asyn- chronous model that tolerates f ≥ n 2 many failures.   30  CHAPTER 3. CONSENSUS  Proof. Assume that there is an algorithm that can handle f = n 2 many failures. We partition the set of all nodes into two sets N, N cid:48  both containing n 2 many nodes. Let us look at three diﬀerent selection of input values: In V0 all nodes start with 0. In V1 all nodes start with 1. In Vhalf all nodes in N start with 0, and all nodes in N cid:48  start with 1.  Assume that nodes start with Vhalf. Since the algorithm must solve consensus independent of the scheduling of the messages, we study the scenario where all messages sent from nodes in N to nodes in N cid:48   or vice versa  are heavily delayed. Note that the nodes in N cannot determine if they started with V0 or Vhalf. Analogously, the nodes in N cid:48  cannot determine if they started in V1 or Vhalf. Hence, if the algorithm terminates before any message from the other set is received, N must decide for 0 and N cid:48  must decide for 1  to satisfy the validity requirement, as they could have started with V0 respectively V1 . Therefore, the algorithm would fail to reach agreement.  The only possibility to overcome this problem is to wait for at least one message sent from a node of the other set. However, as f = n 2 many nodes can crash, the entire other set could have crashed before they sent any message. In that case, the algorithm would wait forever and therefore not satisfy the termination re- quirement.  Remarks:    Algorithm 3.15 solves consensus with optimal fault-to- lerance – but it is awfully slow. The problem is rooted in the individual coin tossing: If all nodes toss the same coin, they could terminate in a constant number of rounds.   Can this problem be ﬁxed by simply always choosing 1  at Line 22?!    This cannot work: Such a change makes the algorithm deterministic, and therefore it cannot achieve consensus  Theorem 3.14 . Simulating what happens by always choosing 1, one can see that it might happen that there is a majority for 0, but a minority with value 1 prevents the nodes from reaching agreement.    Nevertheless, the algorithm can be improved by tossing a so-called shared coin. A shared coin is a random variable   3.5. SHARED COIN  31  that is 0 for all nodes with constant probability, and 1 with constant probability. Of course, such a coin is not a magic device, but it is simply an algorithm. To improve the expected runtime of Algorithm 3.15, we replace Line 22 with a function call to the shared coin algorithm.  3.5 Shared Coin  Algorithm 3.22 Shared Coin  code for node u  1: Choose local coin cu = 0 with probability 1 n, else cu = 1 2: Broadcast myCoin cu  3: Wait for n − f coins and store them in the local coin set Cu 4: Broadcast mySet Cu  5: Wait for n − f coin sets 6: if at least one coin is 0 among all coins in the coin sets then 7: 8: else 9: 10: end if  return 1  return 0  Remarks:    Since at most f nodes crash, all nodes will always re- ceive n − f coins respectively coin sets in Lines 3 and 5. Therefore, all nodes make progress and termination is guaranteed.    We show the correctness of the algorithm for f < n 3. To simplify the proof we assume that n = 3f + 1, i.e., we assume the worst case.  Lemma 3.23. Let u be a node, and let W be the set of coins that u received in at least f + 1 diﬀerent coin sets. It holds that W ≥ f + 1.  Proof. Let C be the multiset of coins received by u. Observe that u receives exactly C =  n − f  2 many coins, as u waits for n − f coin sets each containing n − f coins. Assume that the lemma does not hold. Then, at most f coins are in all n− f coin sets, and all other coins  n− f   are in at most   32  CHAPTER 3. CONSENSUS  f coin sets. In other words, the number of total of coins that u received is bounded by  C ≤ f ·  n − f   +  n − f   · f = 2f  n − f  .  Our assumption was that n > 3f , i.e., n − f > 2f . Therefore C ≤ 2f  n − f   <  n − f  2 = C, which is a contradiction.  Lemma 3.24. All coins in W are seen by all correct nodes. Proof. Let w ∈ W be such a coin. By deﬁnition of W we know that w is in at least f +1 sets received by u. Since every other node also waits for n − f sets before terminating, each node will receive at least one of these sets, and hence w must be seen by every node that terminates.  Theorem 3.25. If f < n 3 nodes crash, Algorithm 3.22 imple- ments a shared coin.  Proof. Let us ﬁrst bound the probability that the algorithm returns 1 for all nodes. With probability  1− 1 n n ≈ 1 e ≈ 0.37 all nodes chose their local coin equal to 1  Line 1 , and in that case 1 will be decided. This is only a lower bound on the probability that all nodes return 1, as there are also other scenarios based on message scheduling and crashes which lead to a global decision for 1. But a probability of 0.37 is good enough, so we do not need to consider these scenarios. With probability 1 −  1 − 1 n W there is at least one 0 in W . Using Lemma 3.23 we know that W ≥ f + 1 ≈ n 3, hence the probability is about 1 −  1 − 1 n n 3 ≈ 1 −  1 e 1 3 ≈ 0.28. We know that this 0 is seen by all nodes  Lemma 3.24 , and hence everybody will decide 0. Thus Algorithm 3.22 implements a shared coin.  Remarks:    We only proved the worst case. By choosing f fairly small, it is clear that f + 1  cid:54 ≈ n 3. However, Lemma 3.23 can be proved for W ≥ n − 2f . To prove this claim you need to substitute the expressions in the contradictory statement: At most n−2f−1 coins can be in all n−f coin sets, and n− n−2f −1  = 2f +1 coins can be in at most f coin sets. The remainder of the proof is analogous, the only diﬀerence is that the math is not as neat. Using the modiﬁed Lemma we know that W ≥ n 3, and therefore Theorem 3.25 also holds for any f < n 3.   BIBLIOGRAPHY  33    We implicitly assumed that message scheduling was ran- dom; if we need a 0 but the nodes that want to propose 0 are “slow”, nobody is going to see these 0’s, and we do not have progress. There exist more complicated proto- cols that solve this problem.  Theorem 3.26. Plugging Algorithm 3.22 into Algorithm 3.15 we get a randomized consensus algorithm which terminates in a con- stant expected number of rounds tolerating up to f < n 3 crash failures.  Chapter Notes  The problem of two friends arranging a meeting was presented and studied under many diﬀerent names; nowadays, it is usually referred to as the Two Generals Problem. The impossibility proof was established in 1975 by Akkoyunlu et al. [AEH75].  The proof that there is no deterministic algorithm that always solves consensus is based on the proof of Fischer, Lynch and Pater- son [FLP85], known as FLP, which they established in 1985. This result was awarded the 2001 PODC Inﬂuential Paper Award  now called Dijkstra Prize . The idea for the randomized consensus al- gorithm was originally presented by Ben-Or [Ben83]. The concept of a shared coin was introduced by Bracha [Bra87]. A shared coin that can withstand worst-case scheduling has been developed by Alistarh et al. [AAKS14]; this shared coin was inspired by earlier shared coin solutions in the shared memory model [Cha96].  Apart from randomization, there are other techniques to still get consensus. One possibility is to drop asynchrony and rely on time more, e.g. by assuming partial synchrony [DLS88] or timed asynchrony [CF98]. Another possibility is to add failure detectors [CT96].  Bibliography  [AAKS14] Dan Alistarh, James Aspnes, Valerie King, and Jared Saia. Communication-eﬃcient randomized consensus. In 28th International Symposium of Distributed Com- puting  DISC , Austin, TX, USA, October 12-15, 2014, pages 61–75, 2014.   34  CHAPTER 3. CONSENSUS  [AEH75] EA Akkoyunlu, K Ekanadham, and RV Huber. Some constraints and tradeoﬀs in the design of network com- munications. In ACM SIGOPS Operating Systems Re- view, volume 9, pages 67–74. ACM, 1975.  [Ben83] Michael Ben-Or. Another advantage of free choice  extended abstract : Completely asynchronous agree- ment protocols. In Proceedings of the second annual ACM symposium on Principles of distributed comput- ing, pages 27–30. ACM, 1983.  [Bra87] Gabriel Bracha. Asynchronous byzantine agreement Information and Computation, 75 2 :130–  protocols. 143, 1987.  [CF98] Flaviu Cristian and Christof Fetzer. The timed asyn- chronous distributed system model. In Digest of Pa- pers: FTCS-28, The Twenty-Eigth Annual Interna- tional Symposium on Fault-Tolerant Computing, Mu- nich, Germany, June 23-25, 1998, pages 140–149, 1998.  [Cha96] Tushar Deepak Chandra. Polylog randomized wait- free consensus. In Proceedings of the Fifteenth Annual ACM Symposium on Principles of Distributed Comput- ing, Philadelphia, Pennsylvania, USA, pages 166–175, 1996.  [CT96] Tushar Deepak Chandra and Sam Toueg. Unreliable failure detectors for reliable distributed systems. J. ACM, 43 2 :225–267, 1996.  [DLS88] Cynthia Dwork, Nancy A. Lynch, and Larry J. Stock- meyer. Consensus in the presence of partial synchrony. J. ACM, 35 2 :288–323, 1988.  [FLP85] Michael J. Fischer, Nancy A. Lynch, and Mike Pater- son. Impossibility of Distributed Consensus with One Faulty Process. J. ACM, 32 2 :374–382, 1985.   Chapter 4  Byzantine Agreement  In order to make ﬂying safer, researchers studied possible failures of various sensors and machines used in airplanes. While trying to model the failures, they were confronted with the following prob- lem: Failing machines did not just crash, instead they sometimes showed arbitrary behavior before stopping completely. With these insights researchers modeled failures as arbitrary failures, not re- stricted to any patterns.  Deﬁnition 4.1  Byzantine . A node which can have arbitrary be- havior is called byzantine. This includes “anything imaginable”, e.g., not sending any messages at all, or sending diﬀerent and wrong messages to diﬀerent neighbors, or lying about the input value.  Remarks:    Byzantine behavior also includes collusion, i.e., all byzan- tine nodes are being controlled by the same adversary.   We assume that any two nodes communicate directly, and that no node can forge an incorrect sender address. This is a requirement, such that a single byzantine node cannot simply impersonate all nodes!    We call non-byzantine nodes correct nodes.  Deﬁnition 4.2  Byzantine Agreement . Finding consensus as in Deﬁnition 3.1 in a system with byzantine nodes is called byzantine agreement. An algorithm is f -resilient if it still works correctly with f byzantine nodes.  35   36  CHAPTER 4. BYZANTINE AGREEMENT  Remarks:    As for consensus  Deﬁnition 3.1  we also need agreement, termination and validity. Agreement and termination are straight-forward, but what about validity?  4.1 Validity  Remarks:  Deﬁnition 4.3  Any-Input Validity . The decision value must be the input value of any node.    This is the validity deﬁnition we used for consensus, in  Deﬁnition 3.1.    Does this deﬁnition still make sense in the presence of byzantine nodes? What if byzantine nodes lie about their inputs?    We would wish for a validity deﬁnition which diﬀerenti-  ates between byzantine and correct inputs.  Deﬁnition 4.4  Correct-Input Validity . The decision value must be the input value of a correct node.  Remarks:    Unfortunately, implementing correct-input validity does not seem to be easy, as a byzantine node following the protocol but lying about its input value is indistinguish- able from a correct node. Here is an alternative.  Deﬁnition 4.5  All-Same Validity . If all correct nodes start with the same input v, the decision value must be v.  Remarks:    If the decision values are binary, then correct-input va-  lidity is induced by all-same validity.    If the input values are not binary, but for example from sensors that deliever values in R, all-same validity is in most scenarios not really useful.   4.2. HOW MANY BYZANTINE NODES?  37  Deﬁnition 4.6  Median Validity . If the input values are order- able, e.g. v ∈ R, byzantine outliers can be prevented by agreeing on a value close to the median of the correct input values – how close depends on the number of byzantine nodes f .  Remarks:    Is byzantine agreement possible? If yes, with what valid-  ity condition?    Let us try to ﬁnd an algorithm which tolerates 1 sin- gle byzantine node, ﬁrst restricting to the so-called syn- chronous model.  Model 4.7  synchronous . In the synchronous model, nodes operate in synchronous rounds. In each round, each node may send a message to the other nodes, receive the messages sent by the other nodes, and do some local computation.  Deﬁnition 4.8  synchronous runtime . For algorithms in the syn- chronous model, the runtime is simply the number of rounds from the start of the execution to its completion in the worst case  every legal input, every execution scenario .  4.2 How Many Byzantine Nodes?  Algorithm 4.9 Byzantine Agreement with f = 1.  1: Code for node u, with input value x:  2: Send tuple u, x  to all other nodes 3: Receive tuple v, y  from all other nodes v 4: Store all received tuple v, y  in a set Su  Round 1  Round 2  5: Send set Su to all other nodes 6: Receive sets Sv from all nodes v 7: T = set of tuple v, y  seen in at least two sets Sv, including 8: Let tuple v, y  ∈ T be the tuple with the smallest value y 9: Decide on value y  own Su   38  CHAPTER 4. BYZANTINE AGREEMENT  Remarks:    Byzantine nodes may not follow the protocol and send syntactically incorrect messages. Such messages can eas- ily be deteced and discarded. It is worse if byzantine nodes send syntactically correct messages, but with a bo- gus content, e.g., they send diﬀerent messages to diﬀerent nodes.    Some of these mistakes cannot easily be detected: For example, if a byzantine node sends diﬀerent values to diﬀerent nodes in the ﬁrst round; such values will be put into Su. However, some mistakes can and must be de- tected: Observe that all nodes only relay information in Round 2, and do not say anything about their own value. So, if a byzantine node sends a set Sv which contains a tuple v, y , this tuple must be removed by u from Sv upon receiving it  Line 6 .    Recall that we assumed that nodes cannot forge their source address; thus, if a node receives tuple v, y  in Round 1, it is guaranteed that this message was sent by v.  Lemma 4.10. If n ≥ 4, all correct nodes have the same set T . Proof. With f = 1 and n ≥ 4 we have at least 3 correct nodes. A correct node will see every correct value at least twice, once directly from another correct node, and once through the third correct node. So all correct values are in T . If the byzantine node sends the same value to at least 2 other  correct  nodes, all correct nodes will see the value twice, so all add it to set T . If the byzantine node sends all diﬀerent values to the correct nodes, none of these values will end up in any set T . Theorem 4.11. Algorithm 4.9 reaches byzantine agreement if n ≥ 4.  Proof. We need to show agreement, any-input validity and termi- nation. With Lemma 4.10 we know that all correct nodes have the same set T , and therefore agree on the same minimum value. The nodes agree on a value proposed by any node, so any-input validity holds. Moreover, the algorithm terminates after two rounds.   4.2. HOW MANY BYZANTINE NODES?  39  Remarks:  T .    If n > 4 the byzantine node can put multiple values into    Algorithm 4.9 only provides any-input agreement, which is questionable in the byzantine context. One can achieve all-same validity by choosing the smallest value that oc- curs at least twice, if a value appears at least twice.    The idea of this algorithm can be generalized for any f and n > 3f . In the generalization, every node sends in every of f + 1 rounds all information it learned so far to all other nodes. In other words, message size increases exponentially with f .    Does Algorithm 4.9 also work with n = 3?  Theorem 4.12. Three nodes cannot reach byzantine agreement with all-same validity if one node among them is byzantine.  Proof. We have three nodes u, v, w. In order to achieve all-same validity, a correct node must decide on its own value if another node supports that value. The third node might disagree, but that node could be byzantine. If correct node u has input 0 and correct node v has input 1, the byzantine node w can fool them by telling u that its value is 0 and simultaneously telling v that its value is 1. This leads to u and v deciding on their own values, which results in violating the agreement condition. Even if u talks to v, and they ﬁgure out that they have diﬀerent assumptions about w’s value, u cannot distinguish whether w or v is byzantine.  Theorem 4.13. A network with n nodes cannot reach byzantine agreement with f ≥ n 3 byzantine nodes.  Proof. Let us assume  for the sake of contradiction  that there exists an algorithm A that reaches byzantine agreement for n nodes with f ≥ n 3 byzantine nodes. With A, we can solve byzantine agreement with 3 nodes. For simplicity, we call the 3 nodes u, v, w supernodes. Each supernode simulates n 3 nodes, either  cid:98 n 3 cid:99  or  cid:100 n 3 cid:101 , if n is not divisible by 3. Each simulated node starts with the input of its supernode. Now the three supernodes simulate algorithm A. The single byzantine supernode simulates  cid:100 n 3 cid:101  byzantine nodes. As algorithm A promises to solve byzantine agreement for f ≥ n 3,   40  CHAPTER 4. BYZANTINE AGREEMENT  A has to be able to handle  cid:100 n 3 cid:101  byzantine nodes. Algorithm A guarantees that the correct nodes simulated by the correct two supernodes will achieve byzantine agreement. So the two correct supernodes can just take the value of their simulated nodes  these values have to be the same by the agreement property , and we have achieved agreement for three supernodes, one of them byzantine. This contradicts Lemma 4.12, hence algorithm A cannot exist.  4.3 The King Algorithm  Algorithm 4.14 King Algorithm  for f < n 3   1: x = my input value 2: for phase = 1 to f + 1 do  Round 1  3:  Broadcast value x   Round 2 if some value y  at least n − f times then  Broadcast propose y   end if if some propose z  received more than f times then  Let node vi be the predeﬁned king of this phase i The king vi broadcasts its current value w if received strictly less than n − f propose x  then  4: 5: 6:  7: 8: 9:  x = z  end if  Round 3  10: 11: 12: 13: end if 14: 15: end for  x = w  Lemma 4.15. Algorithm 4.14 fulﬁlls the all-same validity.  Proof. If all correct nodes start with the same value, all correct nodes propose it in Round 2. All correct nodes will receive at least n−f proposals, i.e., all correct nodes will stick with this value, and never change it to the king’s value. This holds for all phases.   4.3. THE KING ALGORITHM  41  Lemma 4.16. If a correct node proposes x, no other correct node proposes y, with y  cid:54 = x, if n > 3f .  Proof. Assume  for the sake of contradiction  that a correct node proposes value x and another correct node proposes value y. Since a good node only proposes a value if it heard at least n − f value messages, we know that both nodes must have received their value from at least n− 2f distinct correct nodes  as at most f nodes can behave byzantine and send x to one node and y to the other one . Hence, there must be a total of at least 2 n − 2f   + f = 2n − 3f nodes in the system. Using 3f   n nodes, a contradiction.  Lemma 4.17. There is at least one phase with a correct king.  Proof. There are f + 1 phases, each with a diﬀerent king. As there are only f byzantine nodes, one king must be correct.  Lemma 4.18. After a round with a correct king, the correct nodes will not change their values v anymore, if n > 3f .  Proof. If all correct nodes change their values to the king’s value, all correct nodes have the same value. If some correct node does not change its value to the king’s value, it received a proposal at least n−f times, therefore at least n−2f correct nodes broadcasted this proposal. Thus, all correct nodes received it at least n − 2f > f times  using n > 3f  , therefore all correct nodes set their value to the proposed value, including the correct king. Note that only one value can be proposed more than f times, which follows from Lemma 4.16. With Lemma 4.15, no node will change its value after this round.  Theorem 4.19. Algorithm 4.14 solves byzantine agreement.  Proof. The king algorithm reaches agreement as either all correct nodes start with the same value, or they agree on the same value latest after the phase where a correct node was king according to Lemmas 4.17 and 4.18. Because of Lemma 4.15 we know that they will stick with this value. Termination is guaranteed after 3 f + 1  rounds, and all-same validity is proved in Lemma 4.15.   42  CHAPTER 4. BYZANTINE AGREEMENT  Remarks:    Algorithm 4.14 requires f + 1 predeﬁned kings. We as- sume that the kings  and their order  are given. Finding the kings indeed would be a byzantine agreement task by itself, so this must be done before the execution of the King algorithm.    Do algorithms exist which do not need predeﬁned kings?  Yes, see Section 4.5.    Can we solve byzantine agreement  or at least consensus   in less than f + 1 rounds?  4.4 Lower Bound on Number of Rounds  Theorem 4.20. A synchronous algorithm solving consensus in the presence of f crashing nodes needs at least f + 1 rounds, if nodes decide for the minimum seen value.  Proof. Let us assume  for the sake of contradiction  that some al- gorithm A solves consensus in f rounds. Some node u1 has the smallest input value x, but in the ﬁrst round u1 can send its infor- mation  including information about its value x  to only some other node u2 before u1 crashes. Unfortunately, in the second round, the only witness u2 of x also sends x to exactly one other node u3 before u2 crashes. This will be repeated, so in round f only node uf +1 knows about the smallest value x. As the algorithm terminates in round f , node uf +1 will decide on value x, all other surviving  correct  nodes will decide on values larger than x.  Remarks:    A general proof without the restriction to decide for the  minimum value exists as well.    Since byzantine nodes can also just crash, this lower bound also holds for byzantine agreement, so Algorithm 4.14 has an asymptotically optimal runtime.    So far all our byzantine agreement algorithms assume the synchronous model. Can byzantine agreement be solved in the asynchronous model?   4.5. ASYNCHRONOUS BYZANTINE AGREEMENT  43  4.5 Asynchronous Byzantine Agreement  Algorithm 4.21 Asynchronous Byzantine Agreement  Ben-Or, for f < n 10  1: xi ∈ {0, 1} 2: r = 1 3: decided = false 4: Broadcast propose xi,r  5: repeat 6: Wait until n−f propose messages of current round r arrived   cid:47  input bit  cid:47  round  if at least n 2 +3f + 1 propose messages contain same value x then  xi = x, decided = true  else if at least n 2 + f + 1 propose messages contain same value x then  7:  8:  9:  10: 11: 12: 13:  xi = x  else  end if r = r + 1 14: Broadcast propose xi,r  15: 16: until decided  see Line 8  17: decision = xi  choose xi randomly, with P r[xi = 0] = P r[xi = 1] = 1 2  Lemma 4.22. Let a correct node choose value x in Line 10, then no other correct node chooses value y  cid:54 = x in Line 10.  Proof. For the sake of contradiction, assume that both 0 and 1 are chosen in Line 10. This means that both 0 and 1 had been proposed by at least n 2 + 1 out of n − f correct nodes. In other words, we have a total of at least 2· n 2 + 2 = n + 2 > n− f correct nodes. Contradiction!  Theorem 4.23. Algorithm 4.21 solves binary byzantine agreement as in Deﬁnition 4.2 for up to f < n 10 byzantine nodes. Proof. First note that it is not a problem to wait for n− f propose messages in Line 6, since at most f nodes are byzantine. If all correct nodes have the same input value x, then all  except the f byzantine nodes  will propose the same value x. Thus, every node   44  CHAPTER 4. BYZANTINE AGREEMENT  receives at least n − 2f propose messages containing x. Observe that for f   n 2 + 3f and the nodes will decide on x in the ﬁrst round already. We have established all-same validity! If the correct nodes have diﬀerent  binary  input values, the validity condition becomes trivial as any result is ﬁne.  What about agreement? Let u be the ﬁrst node to decide on value x  in Line 8 . Due to asynchrony another node v received messages from a diﬀerent subset of the nodes, however, at most f senders may be diﬀerent. Taking into account that byzantine nodes may lie  send diﬀerent propose messages to diﬀerent nodes , f additional propose messages received by v may diﬀer from those received by u. Since node u had at least n 2 + 3f + 1 propose messages with value x, node v has at least n 2 + f + 1 propose messages with value x. Hence every correct node will propose x in the next round, and then decide on x.  So we only need to worry about termination: We have already seen that as soon as one correct node terminates  Line 8  everybody terminates in the next round. So what are the chances that some node u terminates in Line 8? Well, we can hope that all correct nodes randomly propose the same value  in Line 12 . Maybe there are some nodes not choosing randomly  entering Line 10 instead of 12 , but according to Lemma 4.22 they will all propose the same. Thus, at worst all n− f correct nodes need to randomly choose the same bit, which happens with probability 2− n−f  +1. If so, all correct nodes will send the same propose message, and the algo- rithm terminates. So the expected running time is exponential in the number of nodes n in the worst case.  Remarks:    This Algorithm is a proof of concept that asynchronous byzantine agreement can be achieved. Unfortunately this algorithm is not useful in practice, because of its runtime.   Note that for f ∈ O n , the probability for some node to terminate in Line 8 is greater than some positive con- stant. Thus, the Ben-Or algorithm terminates within expected constant number of rounds for small values of f .    For a long time, there was no algorithm with subexponen- tial runtime. The currently fastest algorithm has an ex- pected runtime of O n2.5  but only tolerates f ≤ 1 500n   4.5. ASYNCHRONOUS BYZANTINE AGREEMENT  45  many byzantine nodes. This algorithm works along the lines of the shared coin algorithm; additionally nodes try to detect which nodes are byzantine.  Chapter Notes  The project which started the study of byzantine failures was called SIFT and was founded by NASA [WLG+78], and the research re- garding byzantine agreement started to get signiﬁcant attention with the results by Pease, Shostak, and Lamport [PSL80, LSP82]. In [PSL80] they presented the generalized version of Algorithm 4.9 and also showed that byzantine agreement is unsolvable for n ≤ 3f . The algorithm presented in that paper is nowadays called Exponen- tial Information Gathering  EIG , due to the exponential size of the messages.  There are many algorithms for the byzantine agreement prob- lem. For example the Queen Algorithm [BG89] which has a bet- ter runtime than the King algorithm [BGP89], but tolerates less failures. That byzantine agreement requires at least f + 1 many rounds was shown by Dolev and Strong [DS83], based on a more complicated proof from Fischer and Lynch [FL82].  While many algorithms for the synchronous model have been around for a long time, the asynchronous model is a lot harder. The only results were by Ben-Or and Bracha. Ben-Or [Ben83] was able to tolerate f < n 5. Bracha [BT85] improved this toler- ance to f < n 3. The ﬁrst algorithm with a polynomial expected runtime was found by King and Saia [KS13]. However, their ap- proach required exponential computation time for each node. In the follow-up paper [KS14], same authors improved the local com- putation time to be polynomial in expectation at the expense of a smaller tolerance f < 0.000028n.  Nearly all developed algorithms only satisfy all-same validity. There are a few exceptions, e.g., correct-input validity [FG03], available if the initial values are from a ﬁnite domain, or median validity [SW15] if the input values are orderable.  Before the term byzantine was coined, the terms Albanian Gen- erals or Chinese Generals were used in order to describe malicious behavior. When the involved researchers met people from these countries they moved – for obvious reasons – to the historic term byzantine [LSP82].   46  CHAPTER 4. BYZANTINE AGREEMENT  Bibliography  [Ben83] Michael Ben-Or. Another advantage of free choice  extended abstract : Completely asynchronous agree- ment protocols. In Proceedings of the second annual ACM symposium on Principles of distributed comput- ing, pages 27–30. ACM, 1983.  [BG89] Piotr Berman and Juan A Garay. Asymptotically opti-  mal distributed consensus. Springer, 1989.  [BGP89] Piotr Berman, Juan A. Garay, and Kenneth J. Perry. Towards optimal distributed consensus  extended ab- stract . In 30th Annual Symposium on Foundations of Computer Science, Research Triangle Park, North Carolina, USA, 30 October - 1 November 1989, pages 410–415, 1989.  [BT85] Gabriel Bracha and Sam Toueg. Asynchronous con- sensus and broadcast protocols. Journal of the ACM  JACM , 32 4 :824–840, 1985.  [DS83] Danny Dolev and H. Raymond Strong. Authenticated algorithms for byzantine agreement. SIAM Journal on Computing, 12 4 :656–666, 1983.  [FG03] Matthias Fitzi and Juan A Garay. Eﬃcient player- optimal protocols for strong and diﬀerential consensus. In Proceedings of the twenty-second annual symposium on Principles of distributed computing, pages 211–220. ACM, 2003.  [FL82] Michael J. Fischer and Nancy A. Lynch. A lower bound for the time to assure interactive consistency. 14 4 :183–186, June 1982.  [KS13] Valerie King and Jared Saia. Byzantine agreement in polynomial expected time:[extended abstract]. In Pro- ceedings of the forty-ﬁfth annual ACM symposium on Theory of computing, pages 401–410. ACM, 2013.  [KS14] Valerie King and Jared Saia. Faster agreement via a spectral method for detecting malicious behavior. In Proceedings of the Twenty-ﬁfth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’14, pages   BIBLIOGRAPHY  47  785–800, Philadelphia, PA, USA, 2014. Society for In- dustrial and Applied Mathematics.  [LSP82] Leslie Lamport, Robert E. Shostak, and Marshall C. Pease. The byzantine generals problem. ACM Trans. Program. Lang. Syst., 4 3 :382–401, 1982.  [PSL80] Marshall C. Pease, Robert E. Shostak, and Leslie Lam- port. Reaching agreement in the presence of faults. J. ACM, 27 2 :228–234, 1980.  [SW15] David Stolz and Roger Wattenhofer.  Byzantine Agreement with Median Validity. In 19th Interna- tional Conference on Priniciples of Distributed Systems  OPODIS , Rennes, France, 2015.  [WLG+78] John H. Wensley, Leslie Lamport, Jack Goldberg, Mil- ton W. Green, Karl N. Levitt, P. M. Melliar-Smith, Robert E. Shostak, and Charles B. Weinstock. Sift: Design and analysis of a fault-tolerant computer for air- craft control. In Proceedings of the IEEE, pages 1240– 1255, 1978.   48  CHAPTER 4. BYZANTINE AGREEMENT   Chapter 5  Cryptography Basics  As we have seen in Section 4.5, state replication as introduced in Deﬁnition 2.8 can be solved by using randomization. It turns out, however, that cryptographic tools may simplify protocols consider- ably. Before we present these crypto-based protocols in the remain- der of this book, we will brieﬂy give an introduction to public-key cryptography in this chapter. We discuss some of the most impor- tant concepts such as encryption or digital signatures.  Public-key cryptography is one of the biggest scientiﬁc achieve- ments of the last century. Two people that never met before can establish a common secret in plain sight? Sounds like pure magic! The idea of this chapter is to reveal some of the tricks of this “crypto magic”. This chapter is not tailored towards any particu- lar application, rather we present the foundations in a vanilla way.  5.1 Key Exchange  How to agree on a common secret key in public, if you never met before?  Deﬁnition 5.1  Primitive Root . Let p ∈ N be a prime. g ∈ N is a primitive root of p if the following holds: For every h ∈ N, with 1 ≤ h < p, there is a k ∈ N s.t. gk = h mod p.  49   50  CHAPTER 5. CRYPTOGRAPHY BASICS  Algorithm 5.2 Diﬃe-Hellman Key Exchange Input: Publicly known prime p and a primitive root g of p. Result: Alice and Bob agree on a common secret key. 1: Alice picks kA, with 1 ≤ kA ≤ p − 2 and sends gkA mod p to 2: Bob picks kB, with 1 ≤ kB ≤ p − 2 and sends gkB mod p to  3: Alice calculates cid:0 gkB cid:1 kA mod p = gkB kA mod p 4: Bob calculates cid:0 gkA cid:1 kB mod p = gkAkB mod p  Alice  Bob  5: Alice & Bob have a common secret key gkAkB mod p = gkB kA  mod p  Remarks:    Also, we will use k for keys, m for messages, p for primes, g for primitive roots, and c for ciphertext  encrypted mes- sages . Generally speaking, an encryption algorithm en- crypts a plain message m by applying a key k, resulting in ciphertext c.    Small  not so secure  example for prime p = 5 and prim- itive root g = 2: 21 = 2 mod 5, 22 = 4 mod 5, 23 = 3 mod 5, 24 = 1 mod 5. One more primitive root for p = 5 exists. There are sophisticated methods to quickly ﬁnd primitive roots, but they are beyond the material covered in this chapter.    Algorithm 5.2 with p = 5 and g = 2: Alice picks kA = 2 with 22 = 4 mod 5, and Bob picks kB = 3 with 23 = 3 mod 5. Thus, Bob receives 4 and Alice receives 3. Then, Bob calculates 43 = 4 mod 5, and Alice calculates 32 = 4 mod 5. Hence, Alice and Bob have agreed on the common secret key of 4.    How secure is Algorithm 5.2?  Deﬁnition 5.3  Discrete Logarithm Problem . Let p ∈ N be a prime, and let g, a ∈ N with 1 ≤ g, a < p. The discrete logarithm problem is deﬁned as ﬁnding an x ∈ N with gx = a mod p.   5.1. KEY EXCHANGE  51  Remarks:    Intuitively, the best approach to calculate the common secret key of Algorithm 5.2 from the publicly known p, g, gkA , gkB is to solve the discrete logarithm problem. This is also the best known attack.    However, for some classes of primes there are better at- tacks, which is why one often resorts to so-called safe primes p, where p cid:48  =  p − 1  2 is also a prime.    How to ﬁnd big enough primes though? Deterministic methods are still too slow in practice. Thus, let’s go probabilistic with the following primality test.  Algorithm 5.4 Probabilistic Primality Testing Input: An odd number p ∈ N. Result: Is p a prime? 1: Let j, r ∈ N and j odd with p − 1 = 2rj 2: Select x ∈ N uniformly at random, 1 ≤ x < p 3: Set x0 = xj mod p 4: if x0 = 1 or x0 = p − 1 then 5: Output “p is probably prime” and stop 6: end if 7: for i = 1, . . . r − 1 do Set xi = x2 if xi = p − 1 then  i−1 mod p  8: 9:  Output “p is probably prime” and stop  10: end if 11: 12: end for 13: Output “p is not prime”  Lemma 5.5. Algorithm 5.4 is correct with probability 75% if it outputs “p is probably prime”, and 100% correct if it outputs “p is not prime”. Corollary 5.6. The runtime of Algorithm 5.4 is O r  ∈ O log p    52  CHAPTER 5. CRYPTOGRAPHY BASICS  Remarks:    The proof for the probabilistic correctness of the primal- ity test in Algorithm 5.4 goes beyond the material covered in this chapter.    Algorithm 5.4 is a Monte Carlo algorithm as its  fast  runtime is deterministic, but the output can be wrong with bounded probability. However, running the algo- rithm again on the same p, but with diﬀerent x, pro- duces an independent result, allowing to bound the error probability by 1  4r in r runs.    A simple method to ﬁnd big primes is thus as follows: Pick a big random number p, with p being odd. Run Algorithm 5.4 until p is prime with the desired probability of 1 − ε. If p is not prime, pick another p. According to the prime number theorem, the average distance between two primes of size at most n is just ln n, i.e., there is a good chance to ﬁnd a big prime.    While it is easy to ﬁnd big primes, the problem of factor- ization is believed to be hard: I.e., given some integer x, ﬁnd the prime factors of x. Many cryptographic proto- cols rely on the  perceived  hardness of the factorization problem, most famously RSA.  Deﬁnition 5.7  Man in the Middle Attack . A man in the middle attack is deﬁned as an attacker Eve deciphering or changing the messages between Alice and Bob, while Alice and Bob believe they are communicating directly with each other.  Theorem 5.8. The Diﬃe-Hellman Key Exchange from Algorithm 5.2 is vulnerable to a man in the middle attack.  Proof. Assume that Eve can intercept and relay all messages be- tween Alice and Bob. That alone does not make it a man in the middle attack, Eve needs to be able to decipher or change mes- sages without Alice or Bob noticing. However, Eve can emulate Alice’s and Bob’s behavior to each other, by picking her own k cid:48  A, k cid:48  B, and then agreeing on common keys gkAk cid:48  A with Alice and Bob, respectively. Thus, Eve can relay all messages between Alice and Bob while deciphering and  possibly  changing them, while Alice and Bob believe they are securely communicating with each other.  B , gkB k cid:48    5.1. KEY EXCHANGE  53  Remarks:    It is a bit like concurrently playing chess with two grand- masters: If you play white and black respectively, you can essentially let them play against each other by relaying their moves.    How do we ﬁx this? One idea is to personally meet in private ﬁrst, exchange a common secret key kA,B, and then use this key for secure communication. Now a man in the middle cannot change the key.  Deﬁnition 5.9  Forward Secrecy . A sequence of secured commu- nication rounds has the property of forward secrecy, if discovering the secret key s  of a single communication round does not reveal the content of past communication rounds.  Remarks:  times.    So Alice and Bob cannot use the same secret key multiple  Algorithm 5.10 Diﬃe-Hellman Key Exchange with Forward Se- crecy Input: Alice’s and Bob’s common secret key kA,B, and furthermore a prime p with a primitive root g for p. Result: A Diﬃe-Hellman key exchange not vulnerable to a man in the middle attack, and with forward secrecy. 1: Bob picks a random number 1 ≤ kB ≤ p − 2 and sends Alice 2: Alice picks a random number 1 ≤ kA ≤ p − 2 and sends gkA  gkB mod p encrypted with kA,B as cB as a challenge  mod p encrypted with kA,B as cA to Bob as a challenge  3: Alice and Bob decrypt the respective messages, and Alice sends gkB + 1 encrypted with kA,B to Bob as a response  and Bob as well with gkA + 1   4: If decryption yields gkA + 1 for Alice, and gkB + 1 for Bob,  respectively, they accept the round key gkAkB mod p  Lemma 5.11. Algorithm 5.10 has the property of forward secrecy and is not vulnerable to a man in the middle attack, if encryption with kA,B is secure.   54  CHAPTER 5. CRYPTOGRAPHY BASICS  Proof. For a man in the middle attack, Eve needs to be able to decrypt and encrypt with kA,B to convince Alice and Bob that they directly communicated with each other, which is a contradiction to the security assumption.  Regarding forward secrecy, if the attacker Eve gathers the secret key gkAkB of a communication round, she can decrypt the messages of this communication round. Even if Eve gains access to kA,B, she cannot gain access to the keys generated in past communication rounds.  Remarks:    Observe that forward secrecy only applies to communica- tion rounds in the past. If Eve gains access to kA,B, she can perform man in the middle attacks in future commu- nication rounds.    However, we have a new inconvenience: Alice and Bob need to agree on a secret key kA,B beforehand. Further- more, with n participants, everyone needs n− 1 diﬀerent keys.  5.2 Public Key Cryptography  “Love all, trust a few.” – William Shakespeare  Deﬁnition 5.12  Public Key Cryptography . A public key cryp- tography system uses two keys: A public key kp, to be disseminated to everyone, and a secret  private  key ks, only known to the owner. A message encrypted with the secret key can be decrypted with the corresponding public key. Analogously, a message encrypted with the public key can be decrypted with the corresponding secret key.  Remarks:    Popular public key cryptosystem include RSA and elliptic  curve cryptography.    With public key cryptography, we have reduced the num- ber of keys – everyone just needs a secret and a public key.   5.2. PUBLIC KEY CRYPTOGRAPHY  55    A conceptual way to think of public key cryptography is as follows: The secret key is a physical  secret  key that opens a speciﬁc type of padlock, and this type of padlock is freely available. The public key is a physical key too, freely available, but it opens only a  secret  speciﬁc type of padlock. If Alice wants to send Bob an encrypted message, she applies his public padlock to the message container, and only Bob can open it. Similarly, if Alice wants to authenticate her message to Bob, she locks the container with her secret padlock, and only Alice’s public key can unlock it. Lastly, if Alice wants to ensure both encryption and authentication, she applies both her own secret padlock and Bob’s public padlock to the message container.    We will now extend the Diﬃe-Hellman algorithm to pub-  lic key cryptography.  Algorithm 5.13 Elgamal Public Secret Key Generation Input: Publicly known prime p and a primitive root g of p. Result: Alice generates a public and a secret key 1: Alice picks random ks with 1 ≤ ks ≤ p − 2 as her secret key 2: Alice calculates kp = gks mod p as her public key  Remarks:    Alice can publish p, g, kp, but should keep ks to her own.    We will now start with encryption, before covering au-  thentication.  Theorem 5.15  Fermat’s little theorem . Let p be a prime num- ber. Then, for any a ∈ N holds: ap = a mod p. If a is not divisible by p, then ap−1 = 1 mod p.  Lemma 5.16. Algorithm 5.14 is correct.   56  CHAPTER 5. CRYPTOGRAPHY BASICS  Algorithm 5.14 Elgamal Public Key Encryption and Decryption Input: Alice and Bob know p, g, kp, Alice knows ks. Result: Bob sends Alice an encrypted message, which she can decrypt. 1: Bob picks a message 1 ≤ m ≤ p−2 and a random 1 ≤ x ≤ p−2 2: Bob sends gx mod p and c = m · kx 3: Alice ﬁrst calculates y =  gx p−ks−1 mod p 4: Alice then obtains m = y · c mod p  p mod p to Alice  Proof.  y · c =  gx p−ks−1 cid:0 m · kx =  gx p−ks−1 cid:16    cid:1  mod p m · cid:0 gks cid:1 x cid:17   p  =  gx p−ks−1 m ·  gx ks mod p = m  gx p−1 mod p = m mod p  using Theorem 5.15 .  mod p  using kp = gks mod p   Remarks:    We can now send someone an encrypted message using public key cryptography, but what about authentication?   Again, we ﬁrst need some number theoretic preliminaries.  Deﬁnition 5.17  Greatest Common Divisor, gcd . The greatest common divisor  gcd  of two integers i1, i2 is the largest integer that divides i1 and i2 without a remainder.  Theorem 5.18. Let p be a prime and i be an integer with gcd i, p  gc= 1. Let a1, a2 ∈ N. If a1 = a2 mod  p − 1 , then ia1 = ia2 mod p.   5.2. PUBLIC KEY CRYPTOGRAPHY  57  Algorithm 5.19 Elgamal Authentication Input: Alice and Bob know p, g, kp, Alice knows ks. Result: Alice signs a message 1 ≤ m ≤ p − 2, which Bob authenticates. 1: Alice picks a random 1 ≤ x ≤ p − 1, with gcd x, p − 1  = 1 2: Alice calculates a = gx mod p and b = x−1 mod  p − 1  3: Alice calculates d =  m − aks b mod  p − 1  4: Alice sends the message m and the signature  a, d  to Bob 5: Bob checks if 1 ≤ a ≤ p − 1, else he rejects 6: Bob accepts Alice’s signature for m if ka  p ad = gm mod p  Remarks:    A multiplicative inverse modulo p  in this algorithm: x−1 mod p , can be calculated using, e.g., the extended Eu- clidean algorithm.  Lemma 5.20. Algorithm 5.19 is correct. Proof. With d =  m − aks x−1 mod  p − 1 , it follows that:  dx = m − aks mod  p − 1  ⇒ m = dx + aks mod  p − 1 .  Using Theorem 5.18, we now obtain gdx+aks = gm mod p. Hence,  p ad mod p = cid:0 gks cid:1 a  ka   gx d = gaksgdx mod p = gm mod p.  Remarks:    The security of the Elgamal public key cryptography again depends on the hardness of the discrete logarithm prob- lem.    We can now authenticate a message using public key cryptography, e.g., we can check that the public key of Alice corresponds to Alice’s secret key.    However, we are back still at our old problem: How do I know that Alice’s public key really belongs to Alice? Maybe Eve pretended to be Alice? To use a famous say- ing by Peter Steiner: “On the Internet, nobody knows you’re a dog”.   58  CHAPTER 5. CRYPTOGRAPHY BASICS    What can we do, unless we personally meet with everyone to exchange secret keys? The answer lies in trusting a few, in order to trust many: Let’s say that you don’t know Alice, but both Alice and you know Doris. If you trust Doris, then Doris can verify Alice’s public key for you. In the future, you can ask Alice to vouch for her friends as well, etc.    Trust is not limited to real persons though, especially since Alice and Doris are represented by their keys. Take a website like PayPal for example. How do you know that you give them your credit card information, and not some infamous Nigerian princess Eve? You probably don’t know anybody who personally knows PayPal...  Deﬁnition 5.21  Web of Trust . Let G =  V, E  be a graph, where an edge between two nodes u, v represents trust between u, v. For any two nodes u, w, we say u trusts w if there is a path from u to w in G.  Remarks:    Hence, if you want someone to authenticate themselves,  you need to ﬁnd a path in the Web of Trust to them.    In practice, the Web of Trust is a bit more sophisticated, as you can assign various levels of trust – and you might only trust someone in short distance.    The whole situation is a bit of a chicken and egg dilemma though. In the beginning, you don’t trust anyone, and nobody trusts you. You may want to ﬁnd some well- connected nodes and gain their trust. This is the moti- vation for certiﬁcate authorities.  Deﬁnition 5.22  Certiﬁcate Authority, CA . A certiﬁcate author- ity is a node in a web of trust that is trusted by many other nodes.  Remarks:    A main distinction between a CA  or nodes in general  and your real-life friends is that trust is not needed to be mutual, edges in the web of trust can also be directed. As such a node u might trust v, but v does not necessarily need to trust u.   5.3. SECRET SHARING & BULK ENCRYPTION  59    You will ﬁnd trust for some certiﬁcate authorities pre- installed on your system browser, known as root certiﬁ- cates. When you want to know if you can trust a node, the node can supply you with a path  chain of trust  from the CA. More speciﬁcally, you will be supplied with signatures which you can check  as you trust the CA .    Again, one can implement various levels of trust, e.g.,  you might only trust short paths.    Moreover, a CA might get compromised. This leads to the idea of key revocation, where one can check if a key for a signature has been compromised – the correspond- ing certiﬁcate can be generated by anyone holding the respective secret key. Another idea is to also generate expiration dates for keys.    A totally diﬀerent problem is that your own set of root certiﬁcates might be compromised, e.g., if malicious soft- ware adds new root certiﬁcates to one’s device.  5.3 Secret Sharing & Bulk Encryption  “Three may keep a secret, if two of them are dead.” – Benjamin Franklin  Deﬁnition 5.23  Perfect Secrecy . An encryption algorithm has perfect secrecy, if the encrypted message reveals no information to an attacker, except for the possible maximum length of the message.  Deﬁnition 5.24  Threshold Secret Sharing . Let t, n ∈ N with 1 ≤ t ≤ n. An algorithm that distributes a secret among n participants such that t participants need to collaborate to recover the secret is called a  t,n -threshold secret sharing scheme.   60  CHAPTER 5. CRYPTOGRAPHY BASICS  Algorithm 5.25  n, n -Threshold Secret Sharing Input: A secret k, encoded in binary representation of length l k .  Secret distribution 1: Generate n − 1 random binary numbers ki of length l k  and  distribute them among n − 1 participants k1, . . . , kn−1, i.e., kn = k ⊕ k1 ⊕ k2 ⊕ ··· ⊕ kn−1  2: Give participant n the value kn as the result of XOR of k and  Secret recovery 1: Collect all n values k1, . . . , kn and obtain k = k1 ⊕ k2 ⊕ ··· ⊕ kn−1 ⊕ kn  Theorem 5.26. Algorithm 5.25 has perfect secrecy even if n − 1 participants collaborate. Proof. The theorem holds as applying the XOR operation ⊕ to a random bitstring and k results in a random bitstring.  Remarks:    How can we achieve a  t, n -threshold secret sharing pro-  tocol with perfect secrecy?  Algorithm 5.27  t, n -Threshold Secret Sharing Input: A secret k, represented as a real number.  Secret distribution 1: Generate t − 1 random a1, . . . , at−1 ∈ R 2: Obtain a polynomial f of degree t − 1 with f  x  = k + a1x + 3: Generate n distinct x1, . . . , xn ∈ R \ 0 4: Distribute  x1, f  x1   to participant P1, . . . ,  xn, f  xn   to Pn  ··· + at−1xt−1  Secret recovery 1: Collect t pairs  xi, f  xi   from at least t participants 2: Use Lagrange’s interpolation formula to obtain f  0  = k   5.3. SECRET SHARING & BULK ENCRYPTION  61  Remarks:    With at most t − 1 pairs  xi, f  xi  , there are inﬁnitely many possible polynomials with diﬀerent values for f  0 .   There are many other  t, n -threshold secret sharing pro-  tocols, e.g., with intersecting hyperplanes.    Note that in practice, a ﬁnite ﬁeld of prime order instead  of real numbers is used.    We can now use the ideas in this section so far to develop  a bulk encryption algorithm with perfect secrecy.  Deﬁnition 5.28  Bulk Encryption Algorithm . A bulk encryption algorithm can securely encrypt a message of any size.  Algorithm 5.29 One-Time Pad Input: A message m known to Alice, and a symmetric key k  as a random bitstring  of length l k  known by both Alice and Bob.  Encryption 1: Alice sends c = m ⊕ k to Bob  Decryption 1: Bob obtains m by m = c ⊕ k  Corollary 5.30. Algorithm 5.29 has perfect secrecy.  Remarks:    Note that Algorithm 5.29 has one big disadvantage – Al- ice and Bob need to agree on a large random number ﬁrst! While this is feasible for, e.g., secret agents, it is quite impractical for everyday usage.    One can use padding to also remove information about the length of the message, e.g., by adding random bits to the secret.  Deﬁnition 5.31  Electronic Code Book, ECB . Given a method to encrypt a block of x bits, ECB encrypts a message of length rx by splitting the message into r blocks of length x, encrypting each block separately.   62  CHAPTER 5. CRYPTOGRAPHY BASICS  Remarks:    Do we now have a secure method to easily encrypt a large message, if we can encrypt small blocks, each using the same one-time pad?    Suppose you have two message blocks m1, m2 of the same length, encrypted with k, resulting in c1, c2. However, you can obtain m1⊕m2 = c1⊕c2, giving you information about m1 and m2.  Deﬁnition 5.32  Cipher Block Chaining, CBC . Given a method f to encrypt a block of x bits, CBC encrypts a message of length rx by splitting the message into r blocks of length x, m1, m2, . . . , mr, encrypting  the plaintext of   each block XORed with the previous encrypted block, i.e., ci = f  mi ⊕ ci−1 . The ﬁrst block c0 is ini- tialized randomly.  Remarks:    Are we secure now? Using the same technique as in the  last remark, you can again get, e.g., m4 ⊕ m5.    CBC is still one of the standard techniques though when encrypting blocks successively, as more advanced algo- rithms are not susceptible to this simple attack for one- time pads. An example would be the advanced encryp- tion standard  AES . Using AES with CBC is an example of a bulk encryption algorithm. The operation of AES is beyond the scope of this short chapter however.  5.4 Message Authentication & Passwords  I’ve been imitated so well I’ve heard people copy my mistakes. – Jimi Hendrix  Deﬁnition 5.33  Replay Attack . In a replay attack a previously valid message from Alice to Bob is sent again from an eavesdropper Eve to Bob.   5.4. MESSAGE AUTHENTICATION & PASSWORDS  63  Remarks:    An easy way to prevent replay attacks is to include time stamps in messages. Bob can detect a replay attack, if the time stamp is too old or multiple messages with the same time stamp arrive. Another idea is to use nonces  numbers only used once , with the sender and receiver keeping track of the nonces used so far.    Another issue is that an attacker could change an en-  crypted message without knowing the content  Deﬁnition 5.34  Malleability . If ciphertext c can be changed to c cid:48  such that the receiver decrypts it into a diﬀerent message m cid:48  without noticing, the encryption algorithm is malleable.  Remarks:    The Elgamal encryption Algorithm 5.14 is malleable: An p mod p as z · c, resulting in  attacker can relay c = m · kx a valid decryption of zm.    Thus, we need a way to ensure that the messages can- not be changed by an attacker. A natural solution are hash functions. However, not that some well-known hash functions are not secure.  Deﬁnition 5.35  One-Way Hash Function . A hash function h : U → S is called one-way, if for a given z ∈ S it is computationally hard to ﬁnd an element x ∈ U with h x  = z.  Deﬁnition 5.36  Collision Resistant Hash Function . A hash func- tion h : U → S is called collision resistant, if it is computationally hard to ﬁnd elements x  cid:54 = y, x, y ∈ U , with h x  = h y  ∈ S.  Remarks:    It can be shown that a collision resistant hash function  is also a one-way hash function.  Theorem 5.37  Example for a Collision Resistant Hash Function . Let p = 2q + 1 be a safe prime, with primitive roots g1  cid:54 = g2 of p. The hash function h : {0, . . . , q − 1}×{0, . . . , q − 1} → Z\{0} with h x1, x2  = gx1  2 mod p is a collision resistant hash function.  1 gx2   64  CHAPTER 5. CRYPTOGRAPHY BASICS  Remarks:    For a small example, let us pick p = 5 with primitive roots g1 = 2 and g2 = 3. We choose x1 = 3 and x2 = 4, obtaining the hash h 3, 4  = 2334 mod 5 = 3 mod 5.    Popular hash functions used in cryptography include the Secure Hash Algorithm  SHA  and the Message-Digest Algorithm  MD .    It can be shown that ﬁnding a collision for the hash func- tion described in Theorem 5.37 is equivalent to solving the discrete logarithm problem for logg1 g2. Thus, the hash function is a collision resistant hash function, as we assume the discrete logarithm problem to be computa- tionally hard.    One might think that using a collision resistant hash function is good enough to store passwords for a service. E.g., store the hash of each password, and then compare it to the input of the user. Even if the hashes are leaked, an attacker Eve cannot recover the passwords – or can she?    In practice, many users use short passwords, trading se- curity for convenience. Eve can sample the hashes of common passwords such as “password ”, revealing the passwords of all users using these simple passwords. To counter this attack, one uses a technique called salting: The service adds a random bitstring  the salt  to each password before storing the hash  or, less secure, but simpler, the username . Even if the salt is known for each user, Eve needs to attack the hash of each user in- dividually.    To make life for Eve even harder, it is good practice to use hash functions that provably need a lot of computation and memory to execute. However, there is still a trade oﬀ as the real user wants to log in fast as well.    Many web services already oﬀer secure two-factor authen- tication  e.g., via mobile phones  instead of just pass- words or challenge-response systems. However, there is a trade-oﬀ between security and convenience.   5.5. TRANSPORT LAYER SECURITY  65    Are we resistant against malleability now, if we include a hash of the encrypted message? No: An attacker chang- ing the message can change the hash as well, as the hash function is not assumed to be secret. How do we prevent the hash from being modiﬁed without being noticed? The answer are HMACs.  Deﬁnition 5.38  Message Authentication Code, MAC . A mes- sage authentication code is a bitstring that veriﬁes that a message comes from the desired sender and was not changed until reaching the receiver.  Deﬁnition 5.39  Hash-Based Message Authentication Code, HMAC . A hash-based message authentication code is a MAC that uses a collision resistant hash function in combination with a secret key.  Algorithm 5.40 Hash-Based Message Authentication Code Gen- eration Input: An encrypted message c, to be sent from Alice to Bob, the publicly known hash function h from Theorem 5.37, and a secret key 1 ≤ k ≤ c known to Alice and Bob. Result: An HMAC for c, checkable by Bob.  1: Alice computes hA = h k, h k, c  , and sends c, hA to Bob 2: Bob computes hB = h k, h k, c  , and checks if hA = hB  Remarks:    In practice, if k > c, then k will be hashed to have a smaller size. Also, the key will be padded for extra secu- rity.    If an attacker wants to change the message, he needs to change the HMAC too. To change the HMAC, he needs to know the secret key k    Algorithm 5.40 can be also used with any other collision  resistant hash function.  5.5 Transport Layer Security  Now we have all the key ingredients to understand network security.   66  CHAPTER 5. CRYPTOGRAPHY BASICS  Protocol 5.41  Transport Layer Security, TLS . TLS is a network protocol in which a client and a server exchange information in order to communicate in a secure way. Common features include a key exchange protocol  Section 5.1 , the authentication of the server to the client  5.2 , a bulk encryption algorithm  5.3 , and a message authentication algorithm  5.4 .  Remarks:    TLS is the successor of Secure Sockets Layer  SSL . How- ever, sometimes in practice the term SSL includes  the newer  TLS as well.    HTTPS  Hypertext Transfer Protocol Secure  is not a protocol on its own, but rather denotes the usage of HTTP via TLS or SSL.    SSH  Secure Shell , even though close in name to SSL, is something diﬀerent: It is a protocol to allow a client to remotely access a server, e.g., for a command-line inter- face.  Chapter Notes  The concept of one-way functions is surprisingly old. In 1874, William Stanley Jevons wrote: “Can the reader say what two num- bers multiplied together will produce the number 8616460799? I think it unlikely that anyone but myself will ever know.” [Jev74]. To spill the beans: 89681 · 96079.  The Diﬃe-Hellman Key Exchange was published in the seminal paper [DH76], parallel unpublished work also existed from Ellis et al. at the British intelligence service GCHQ. For some works show- ing the hardness of breaking the Diﬃe-Hellman key exchange, we refer to, e.g., [dB88], [Mau94], [Sho97]. For some more recommen- dations on how to choose the parameters of the Diﬃe-Hellman key exchange see RFC 3526 at http:  tools.ietf.org html  rfc3526. The currently fastest algorithms to solve the discrete logarithm problem still have non-practical runtime, e.g., [Adl79]. The idea of challenging the other party to return an encrypted version of one’s random number incremented by one in Algorithm 5.10 is taken from the Kerberos protocol. The Elgamal cryptosys- tem was published by Elgamal in 1984 [Gam84], some years after RSA [RSA78].   5.5. TRANSPORT LAYER SECURITY  67  The ﬁrst deterministic polynomial primality test, by Agrawal, Kayal, and Saxena, was published in [MA04], with an improved runtime of ˜O log6 p  available at https:  math.dartmouth.edu  ~carlp aks041411.pdf. The Miller-Rabin primality test is from Rabin [Rab80] and Miller [Mil76]. For an introduction to number theory, we recommend, e.g., [SO85].  The idea for the web of trust was proposed by Zimmermann in 1992. For certiﬁcate chains and key revocation, we refer to RFC 5280 at http:  tools.ietf.org html rfc5280.  The Chaum-van-Heijst-Pﬁtzmann hash function described in Theorem 5.37 was published in [CvHP91] by Chaum et al., for the reduction to the discrete logarithm problem see, e.g., [Sti95]. How- ever, the runtime of the Chaum-van-Heijst-Pﬁtzmann hash func- tion is too high in practice, it is chosen in this chapter as it is easier to understand compared to other related work. The subsequently described HMAC Algorithm 5.40 is from RFC 2104 at https:   tools.ietf.org html rfc2104, with further security updates in RFC 6151, cf. https:  tools.ietf.org html rfc6151.  The secret sharing variant discussed in this chapter is from Shamir [Sha79], Blakley developed similar work in parallel [Bla79], and also discussed its relation to one-time pads [Bla80].  While CBC seems superior to ECB, there is one downside: De- cryption of ECB can be parallelized, but the decryption of CBC has to be sequential. The in this context mentioned AES encryption is a symmetric key algorithm, based on the Rijndael cipher of Daemen and Rijmen. Details of the Advanced Encryption Standard can be found in http:  csrc.nist.gov publications fips fips197  fips-197.pdf. AES, with a key length of 128,192, or 256 bits, re- placed DES  Data Encryption Standard , as its key length of just 56 was no longer secure enough against brute-force attacks.  While this chapter was based on the discrete logarithm problem, other problems with similar characteristics exist. A simple alter- native is the factorization problem. Many current systems such as Bitcoin use cryptography based on so-called elliptic curves.  For a general overview of the topic of computer security, we recommend [PHS03] and [FS03]. Lastly, as a very general recom- mendation, we urge you not to implement your own cryptosystem unless you really know what you are doing – there is just too much that can easily be missed.   68  CHAPTER 5. CRYPTOGRAPHY BASICS  Bibliography  [Adl79] Leonard Adleman. A subexponential algorithm for the discrete logarithm problem with applications to cryp- tography. In Proceedings of the 20th Annual Symposium on Foundations of Computer Science, SFCS ’79, pages 55–60, Washington, DC, USA, 1979. IEEE Computer Society.  [Bla79] G.R. Blakley. Safeguarding cryptographic keys. In Pro- ceedings of the 1979 AFIPS National Computer Con- ference, pages 313–317, Monval, NJ, USA, 1979. AFIPS Press.  [Bla80] G. R. Blakley. One time pads are key safeguard- ing schemes, not cryptosystems fast key safeguarding schemes  threshold schemes  exist. In Proceedings of the 1980 IEEE Symposium on Security and Privacy, Oak- land, California, USA, April 14-16, 1980, pages 108– 113. IEEE Computer Society, 1980.  [CvHP91] David Chaum, Eug`ene van Heijst, and Birgit Pﬁtzmann. Cryptographically strong undeniable signatures, uncon- ditionally secure for the signer. In Joan Feigenbaum, ed- itor, Advances in Cryptology - CRYPTO ’91, 11th An- nual International Cryptology Conference, Santa Bar- bara, California, USA, August 11-15, 1991, Proceedings, volume 576 of Lecture Notes in Computer Science, pages 470–484. Springer, 1991.  [dB88] Bert den Boer. Diﬃe-hillman is as strong as discrete log for certain primes. In Shaﬁ Goldwasser, editor, Ad- vances in Cryptology - CRYPTO ’88, 8th Annual In- ternational Cryptology Conference, Santa Barbara, Cal- ifornia, USA, August 21-25, 1988, Proceedings, volume 403 of Lecture Notes in Computer Science, pages 530– 539. Springer, 1988.  [DH76] Whitﬁeld Diﬃe and Martin E. Hellman. New direc- tions in cryptography. IEEE Trans. Information The- ory, 22 6 :644–654, 1976.  [FS03] Niels Ferguson and Bruce Schneier. Practical cryptogra-  phy. Wiley, 2003.   BIBLIOGRAPHY  69  [Gam84] Taher El Gamal. A public key cryptosystem and a signa- ture scheme based on discrete logarithms. In G. R. Blak- ley and David Chaum, editors, Advances in Cryptol- ogy, Proceedings of CRYPTO ’84, Santa Barbara, Cal- ifornia, USA, August 19-22, 1984, Proceedings, volume 196 of Lecture Notes in Computer Science, pages 10–18. Springer, 1984.  [Jev74] William Stanley Jevons. The Principles of Science: A Treatise on Logic and Scientiﬁc Method. Macmillan & Co., 1874.  [MA04] Nitin Saxena Manindra Agrawal, Neeraj Kayal. PRIMES Is in P. Annals of Mathematics, 160 2 :781– 793, 2004.  [Mau94] Ueli M. Maurer. Towards the equivalence of breaking the diﬃe-hellman protocol and computing discrete algo- rithms. In Yvo Desmedt, editor, Advances in Cryptology - CRYPTO ’94, 14th Annual International Cryptology Conference, Santa Barbara, California, USA, August 21-25, 1994, Proceedings, volume 839 of Lecture Notes in Computer Science, pages 271–281. Springer, 1994.  [Mil76] Gary L. Miller. Riemann’s hypothesis and tests for pri- mality. J. Comput. Syst. Sci., 13 3 :300–317, December 1976.  [PHS03] Josef Pieprzyk, Thomas Hardjono, and Jennifer Seberry.  Fundamentals of computer security. Springer, 2003.  [Rab80] M.O. Rabin. Probabilistic algorithms for testing primal-  ity. J. Number Theory, 12:128 – 138, 1980.  [RSA78] R. L. Rivest, A. Shamir, and L. Adleman. A method for obtaining digital signatures and public-key cryptosys- tems. Commun. ACM, 21 2 :120–126, February 1978.  [Sha79] Adi Shamir. How to share a secret. Commun. ACM,  22 11 :612–613, 1979.  [Sho97] Victor Shoup. Lower bounds for discrete logarithms In Walter Fumy, editor, Ad- and related problems. vances in Cryptology - EUROCRYPT ’97, International Conference on the Theory and Application of Crypto- graphic Techniques, Konstanz, Germany, May 11-15,   70  CHAPTER 5. CRYPTOGRAPHY BASICS  1997, Proceeding, volume 1233 of Lecture Notes in Com- puter Science, pages 256–266. Springer, 1997.  [SO85] Winfried Scharlau and Hans Opolka. From Fermat to Minkowski: lectures on the theory of numbers and its historical development. Undergraduate Texts in Mathe- matics. Springer, New York, 1985.  [Sti95] Douglas R. Stinson. Cryptography - theory and practice. Discrete mathematics and its applications series. CRC Press, 1995.   Chapter 6  Authenticated Agreement  Byzantine nodes are able to lie about their inputs as well as re- ceived messages. Can we detect certain lies and limit the power of byzantine nodes? Possibly, the authenticity of messages may be validated using signatures?  6.1 Agreement with Authentication  Deﬁnition 6.1  Signature . If a node never signs a message, then no correct node ever accepts that message. We denote a message msg x  signed by node u with msg x u.  71   72  CHAPTER 6. AUTHENTICATED AGREEMENT  Remarks:    Algorithm 6.2 shows an agreement protocol for binary inputs relying on signatures. We assume there is a des- ignated “primary” node p. The goal is to decide on p’s value.  Algorithm 6.2 Byzantine Agreement with Authentication  Code for primary p:  1: if input is 1 then 2:  broadcast value 1 p decide 1 and terminate  decide 0 and terminate  3: 4: else 5: 6: end if  Code for all other nodes v:  7: for all rounds i ∈ 1, . . . , f + 1 do  8: 9: 10: 11:  S is the set of accepted messages value 1 u. if S ≥ i and value 1 p ∈ S then  broadcast S ∪ {value 1 v} decide 1 and terminate  end if 12: 13: end for 14: decide 0 and terminate  Theorem 6.3. Algorithm 6.2 can tolerate f < n byzantine failures while terminating in f + 1 rounds.  Proof. Assuming that the primary p is not byzantine and its input is 1, then p broadcasts value 1 p in the ﬁrst round, which will trigger all correct nodes to decide for 1. If p’s input is 0, there is no signed message value 1 p, and no node can decide for 1.  If primary p is byzantine, we need all correct nodes to decide for the same value for the algorithm to be correct. Let us assume that p convinces a correct node v that its value is 1 in round i with i < f + 1. We know that v received i signed messages for value 1. Then, v will broadcast i + 1 signed messages for value 1, which will trigger all correct nodes to also decide for 1. If p tries to convince some node v late  in round i = f + 1 , v must receive f + 1 signed messages. Since at most f nodes are byzantine, at least one correct   6.2. ZYZZYVA  73  node u signed a message value 1 u in some round i < f + 1, which puts us back to the previous case.  Remarks:    The algorithm only takes f + 1 rounds, which is optimal  as described in Theorem 4.20.    Using signatures, Algorithm 6.2 solves consensus for any number of failures! Does this contradict Theorem 4.12? Recall that in the proof of Theorem 4.12 we assumed that a byzantine node can distribute contradictory in- formation about its own input. If messages are signed, correct nodes can detect such behavior – a node u signing two contradicting messages proves to all nodes that node u is byzantine.    Does Algorithm 6.2 satisfy any of the validity conditions introduced in Section 4.1? No! A byzantine primary can dictate the decision value. Can we modify the algorithm such that the correct-input validity condition is satisﬁed? Yes! We can run the algorithm in parallel for 2f + 1 primary nodes. Either 0 or 1 will occur at least f + 1 times, which means that one correct process had to have this value in the ﬁrst place. In this case, we can only handle f < n  2 byzantine nodes.    In reality, a primary will usually be correct. If so, Algo- rithm 6.2 only needs two rounds! Can we make it work with arbitrary inputs? Also, relying on synchrony limits the practicality of the protocol. What if messages can be lost or the system is asynchronous?    Zyzzyva uses authenticated messages to achieve state re- plication, as in Deﬁnition 2.8. It is designed to run fast when nodes run correctly, and it will slow down to ﬁx failures!  6.2 Zyzzyva  Deﬁnition 6.4  View . A view V describes the current state of a replicated system, enumerating the 3f + 1 replicas. The view V also marks one of the replicas as the primary p.   74  CHAPTER 6. AUTHENTICATED AGREEMENT  Deﬁnition 6.5  Command . If a client wants to update  or read  data, it sends a suitable command c in a Request message to the primary p. Apart from the command c itself, the Request mes- sage also includes a timestamp t. The client signs the message to guarantee authenticity.  Deﬁnition 6.6  History . The history h is a sequence of com- mands c1, c2, . . . in the order they are executed by Zyzzyva. We denote the history up to ck with hk.    In Zyzzyva, the primary p is used to order commands  submitted by clients to create a history h.    Apart from the globally accepted history, node u may also have a local history, which we denote as hu or hu k.  Deﬁnition 6.7  Complete command . If a command completes, it will remain in its place in the history h even in the presence of failures.  Remarks:  Remarks:    As long as clients wait for the completion of their com- mands, clients can treat Zyzzyva like one single computer even if there are up to f failures.   6.2. ZYZZYVA  75  In the Absence of Failures  Algorithm 6.8 Zyzzyva: No failures  1: At time t client u wants to execute command c 2: Client u sends request R = Request c,t u to primary p 3: Primary p appends c to its local history, i.e., hp =  hp, c  4: Primary p sends OR = OrderedRequest hp, c, R p to all replicas  5: Each replica r appends command c to local history hr =  hr, c   and checks whether hr = hp  6: Each replica r runs command ck and obtains result a 7: Each replica r sends Response a,OR r to client u 8: Client u collects the set S of received Response a,OR r messages  9: Client u checks if all histories hr are consistent 10: if S = 3f + 1 then  Client u considers command c to be complete  11: 12: end if  Remarks:    Since the client receives 3f + 1 consistent responses, all  correct replicas have to be in the same state.    Only three communication rounds are required for the  command c to complete.    Note that replicas have no idea which commands are con- sidered complete by clients! How can we make sure that commands that are considered complete by a client are actually executed? We will see in Theorem 6.23.    Commands received from clients should be ordered ac- cording to timestamps to preserve the causal order of commands.    There is a lot of optimization potential. For example, in- cluding the entire command history in most messages in- troduces prohibitively large overhead. Rather, old parts of the history that are agreed upon can be truncated. Also, sending a hash value of the remainder of the his- tory is enough to check its consistency across replicas.   76  CHAPTER 6. AUTHENTICATED AGREEMENT    What if a client does not receive 3f + 1 Response a,OR r messages? A byzantine replica may omit sending any- thing at all! In practice, clients set a timeout for the collection of Response messages. Does this mean that Zyzzyva only works in the synchronous model? Yes and no. We will discuss this in Lemma 6.26 and Lemma 6.27.  Byzantine Replicas  Algorithm 6.9 Zyzzyva: Byzantine Replicas  append to Al- gorithm 6.8  1: if 2f + 1 ≤ S < 3f + 1 then  Client u sends Commit S u to all replicas Each replica r replies with a LocalCommit S r message to u  Client u collects at least 2f + 1 LocalCommit S r messages and considers c to be complete    If replicas fail, a client u may receive less than 3f + 1 consistent responses from the replicas. Client u can only assume command c to be complete if all correct replicas r eventually append command c to their local history hr.  Deﬁnition 6.10  Commit Certiﬁcate . A commit certiﬁcate S contains 2f + 1 consistent and signed Response a,OR r messages from 2f + 1 diﬀerent replicas r.    The set S is a commit certiﬁcate which proves the execu- tion of the command on 2f + 1 replicas, of which at least f + 1 are correct. This commit certiﬁcate S must be ac- knowledged by 2f + 1 replicas before the client considers the command to be complete.    Why do clients have to distribute this commit certiﬁcate to 2f + 1 replicas? We will discuss this in Theorem 6.21.  2: 3:  4:  5: end if  Remarks:  Remarks:   6.2. ZYZZYVA  77    What if S < 2f + 1, or what if the client receives 2f + 1 messages but some have inconsistent histories? Since at most f replicas are byzantine, the primary itself must be byzantine! Can we resolve this?  Byzantine Primary  Deﬁnition 6.11  Proof of Misbehavior . Proof of misbehavior of some node can be established by a set of contradicting signed mes- sages.  Remarks:    For example, if a client u receives two Response a,OR r messages that contain inconsistent OR messages signed by the primary, client u can prove that the primary misbe- haved. Client u broadcasts this proof of misbehavior to all replicas r which initiate a view change by broadcasting a IHatePrimaryr message to all replicas.  Algorithm 6.12 Zyzzyva: Byzantine Primary  append to Al- gorithm 6.9  1: if S < 2f + 1 then  Client u sends the original R = Request c,t u to all replicas Each replica r sends a ConfirmRequest R r message to p if primary p replies with OR then  Replica r forwards OR to all replicas Continue as in Algorithm 6.8, Line 5  Replica IHatePrimaryr to all replicas  initiates  r  view change by broadcasting  2:  3: 4: 5: 6: 7:  8:  else  end if  9: 10: end if  Remarks:    A faulty primary can slow down Zyzzyva by not send- ing out the OrderedRequest messages in Algorithm 6.8, repeatedly escalating to Algorithm 6.12.    Line 5 in the Algorithm is necessary to ensure liveness.  We will discuss this in Theorem 6.27.   78  CHAPTER 6. AUTHENTICATED AGREEMENT    Again, there is potential for optimization. For example, a replica might already know about a command that is requested by a client. In that case, it can answer without asking the primary. Furthermore, the primary might al- ready know the message R requested by the replicas. In that case, it sends the old OR message to the requesting replica.  Safety  Remarks:  Deﬁnition 6.13  Safety . We call a system safe if the following condition holds: If a command with sequence number j and a his- tory hj completes, then for any command that completed earlier  with a smaller sequence number i < j , the history hi is a preﬁx of history hj.    In Zyzzyva a command can only complete in two ways,  either in Algorithm 6.8 or in Algorithm 6.9.    If a system is safe, complete commands cannot be re-  ordered or dropped. So is Zyzzyva so far safe?  Lemma 6.14. Let ci and cj be two diﬀerent complete commands. Then ci and cj must have diﬀerent sequence numbers.  Proof. If a command c completes in Algorithm 6.8, 3f + 1 replicas sent a Response a,OR r to the client. If the command c completed in Algorithm 6.9, at least 2f + 1 replicas sent a Response a,OR r message to the client. Hence, a client has to receive at least 2f + 1 Response a,OR r messages.  Both ci and cj are complete. Therefore there must be at least 2f + 1 replicas that responded to ci with a Response a,OR r mes- sage. But there are also at least 2f + 1 replicas that responded to cj with a Response a,OR r message. Because there are only 3f + 1 replicas, there is at least one correct replica that sent a Response a,OR r message for both ci and cj. A correct replica only sends one Response a,OR r message for each sequence num- ber, hence the two commands must have diﬀerent sequence num- bers.  Lemma 6.15. Let ci and cj be two complete commands with se- quence numbers i < j. The history hi is a preﬁx of hj.   6.2. ZYZZYVA  79  Proof. As in the proof of Lemma 6.14, there has to be at least one correct replica that sent a Response a,OR r message for both ci and cj.  A correct replica r that sent a Response a,OR r message for ci will only accept cj if the history for cj provided by the primary is consistent with the local history of replica r, including ci.  Remarks:    A byzantine primary can cause the system to never com- plete any command. Either by never sending any mes- sages or by inconsistently ordering client requests. In this case, replicas have to replace the primary.  View Changes  Deﬁnition 6.16  View Change . In Zyzzyva, a view change is used to replace a byzantine primary with another  hopefully correct  replica. View changes are initiated by replicas sending IHatePrima- ryr to all other replicas. This only happens if a replica obtains a valid proof of misbehavior from a client or after a replica fails to obtain an OR message from the primary in Algorithm 6.12.  Remarks:    How can we safely decide to initiate a view change, i.e. demote a byzantine primary? Note that byzantine nodes should not be able to trigger a view change!  Algorithm 6.17 Zyzzyva: View Change Agreement 1: All replicas continuously collect the set H of IHatePrimaryr 2: if a replica r received H > f messages or a valid ViewChange  messages  message then  3:  4: 5: 6: end if  Replica r broadcasts ViewChange H r,hr,Sr Replica r stops participating in the current view Replica r switches to the next primary “p = p + 1”  l  r   80  CHAPTER 6. AUTHENTICATED AGREEMENT  Remarks:    The f + 1 IHatePrimaryr messages in set H prove that at least one correct replica initiated a view change. This proof is broadcast to all replicas to make sure that once the ﬁrst correct replica stopped acting in the current view, all other replicas will do so as well.    Sr  l  is the most recent commit certiﬁcate that the replica obtained in the ending view as described in Algorithm 6.9. Sr l will be used to recover the correct history before the new view starts. The local histories hr are included in the ViewChange H r,hr,Sr l  r message such that com- mands that completed after a correct client received 3f + 1 responses from replicas can be recovered as well.    In Zyzzyva, a byzantine primary starts acting as a nor- mal replica after a view change. In practice, all machines eventually break and rarely ﬁx themselves after that. In- stead, one could consider to replace a byzantine primary with a fresh replica that was not in the previous view.  Algorithm 6.18 Zyzzyva: View Change Execution  p  new  primary ViewChange H r,hr,Sr  1: The C 2: if new primary p collected C ≥ 2f + 1 messages then 3: New primary p sends NewView C p to all replicas 4: end if  l  r messages  collects  the  set  of  5: if a replica r received a NewView C p message then 6:  Replica r recovers new history hnew as shown in Algo- rithm 6.20 Replica r broadcasts ViewConfirm hnew r message to all replicas  7:  8: end if  then  10:  11: 12: end if  9: if a replica r received 2f + 1 ViewConfirm hnew r messages  Replica r accepts hr = hnew as the history of the new view Replica r starts participating in the new view   6.2. ZYZZYVA  Remarks:  81    Analogously to Lemma 6.15, commit certiﬁcates are or- dered. For two commit certiﬁcates Si and Sj with se- quence numbers i < j, the history hi certiﬁed by Si is a preﬁx of the history hj certiﬁed by Sj.    Zyzzyva collects the most recent commit certiﬁcate and the local history of 2f + 1 replicas. This information is distributed to all replicas, and used to recover the history for the new view hnew.    If a replica does not receive the NewView C p or the View- Confirm hnew r message in time, it triggers another view change by broadcasting IHatePrimaryr to all other repli- cas.    How is the history recovered exactly? It seems that the set of histories included in C can be messy. How can we be sure that complete commands are not reordered or dropped?  Figure 6.19: The structure of the data reported by diﬀerent repli- cas in C. Commands up to the last commit certiﬁcate Sl were completed in either Algorithm 6.8 or Algorithm 6.9. After the last commit certiﬁcate Sl there may be commands that completed at a correct client in Algorithm 6.8. Algorithm 6.20 shows how the new history hnew is recovered such that no complete commands are lost.  1 + f  f  t c e r r o c  r e h t o  s a c i l  p e r  s a c i l  p e r  { { {     } } }  z z z  {  } z  z    commands up to Sl  ≥ f + 1 consistent histories  < f + 1 consistent histories  }  {  z  }  {  }  z    }  {z  {  }  discarded commands  hnew  {z  Inconsistent or missing commands  Consistent commands  Consistent commands with commit certiﬁcate   82  CHAPTER 6. AUTHENTICATED AGREEMENT  Algorithm 6.20 Zyzzyva: History Recovery 1: C = set of 2f + 1 ViewChange H r,hr,Sr r messages in  NewView C p  2: R = set of replicas included in C 3: Sl = most recent commit certiﬁcate Sr 4: hnew = history hl contained in Sl 5: k = l + 1, next sequence number 6: while command ck exists in C do 7: 8: 9:  if ck is reported by at least f + 1 replicas in R then  Remove replicas from R that do not support ck hnew =  hnew, ck   l reported in C  end if 10: k = k + 1 11: 12: end while 13: return hnew  Remarks:  hnew.    Commands up to Sl are included into the new history    If at least f + 1 replicas share a consistent history after the last commit certiﬁcate Sl, also the commands after that are included.    Even if f + 1 correct replicas consistently report a com- mand c after the last commit certiﬁcate Sl, c may not be considered complete by a client, e.g., because one of the responses to the client was lost. Such a command is included in the new history hnew. When the client retries executing c, the replicas will be able to identify the same command c using the timestamp included in the client’s request, and avoid duplicate execution of the command.   Can we be sure that all commands that completed at a  correct client are carried over into the new view?  Lemma 6.21. The globally most recent commit certiﬁcate Sl is included in C.  Proof. Any two sets of 2f + 1 replicas share at least one correct replica. Hence, at least one correct replica which acknowledged the most recent commit certiﬁcate Sl also sent a LocalCommit Sl r message that is in C.   6.2. ZYZZYVA  83  Lemma 6.22. Any command and its history that completes after Sl has to be reported in C at least f + 1 times.  Proof. A command c can only complete in Algorithm 6.8 after Sl. Hence, 3f + 1 replicas sent a Response a,OR r message for c. C includes the local histories of 2f + 1 replicas of which at most f are byzantine. As a result, c and its history is consistently found in at least f + 1 local histories in C.  Lemma 6.23. If a command c is considered complete by a client, command c remains in its place in the history during view changes.  Proof. We have shown in Lemma 6.21 that the most recent commit certiﬁcate is contained in C, and hence any command that termi- nated in Algorithm 6.9 is included in the new history after a view change. Every command that completed before the last commit certiﬁcate Sl is included in the history as a result. Commands that completed in Algorithm 6.8 after the last commit certiﬁcate are supported by at least f +1 correct replicas as shown in Lemma 6.22. Such commands are added to the new history as described in Algo- rithm 6.20. Algorithm 6.20 adds commands sequentially until the histories become inconsistent. Hence, complete commands are not lost or reordered during a view change.  Theorem 6.24. Zyzzyva is safe even during view changes.  Proof. Complete commands are not reordered within a view as described in Lemma 6.15. Also, no complete command is lost or reordered during a view change as shown in Lemma 6.23. Hence, Zyzzyva is safe.  Remarks:    So Zyzzyva correctly handles complete commands even in the presence of failures. We also want Zyzzyva to make progress, i.e., commands issued by correct clients should complete eventually.    If the network is broken or introduces arbitrarily large  delays, commands may never complete.    Can we be sure commands complete in periods in which  delays are bounded?  Deﬁnition 6.25  Liveness . We call a system live if every com- mand eventually completes.   84  CHAPTER 6. AUTHENTICATED AGREEMENT  Lemma 6.26. Zyzzyva is live during periods of synchrony if the primary is correct and a command is requested by a correct client.  Proof. The client receives a Response a,OR r message from all cor- rect replicas. If it receives 3f + 1 messages, the command com- pletes immediately in Algorithm 6.8. If the client receives fewer than 3f + 1 messages, it will at least receive 2f + 1, since there are at most f byzantine replicas. All correct replicas will answer the client’s Commit S u message with a correct LocalCommit S r message after which the command completes in Algorithm 6.9.  Lemma 6.27. If, during a period of synchrony, a request does not complete in Algorithm 6.8 or Algorithm 6.9, a view change occurs.  Proof. If a command does not complete for a suﬃciently long time, the client will resend the R = Request c,t u message to all repli- cas. After that, if a replica’s ConfirmRequest R r message is not answered in time by the primary, it broadcasts an IHatePrimaryr message. If a correct replica gathers f +1 IHatePrimaryr messages, the view change is initiated. If no correct replica collects more than f IHatePrimaryr messages, at least one correct replica received a valid OrderedRequest hp, c, R p message from the primary which it forwards to all other replicas. In that case, the client is guaran- teed to receive at least 2f + 1 Response a,OR r messages from the correct replicas and can complete the command by assembling a commit certiﬁcate.  Remarks:    If the newly elected primary is byzantine, the view change may never terminate. However, we can detect if the new primary does not assemble C correctly as all contained messages are signed. If the primary refuses to assemble C, replicas initiate another view change after a timeout.  Chapter Notes  Algorithm 6.2 was introduced by Dolev et al. [DFF+82] in 1982. Byzantine fault tolerant state machine replication  BFT  is a prob- lem that gave rise to various protocols. Castro and Liskov [MC99] introduced the Practical Byzantine Fault Tolerance  PBFT  proto- col in 1999, applications such as Farsite [ABC+02] followed. This triggered the development of, e.g., Q U [AEMGG+05] and HQ   BIBLIOGRAPHY  85  [CML+06]. Zyzzyva [KAD+07] improved on performance espe- cially in the case of no failures, while Aardvark [CWA+09] improved performance in the presence of failures. Guerraoui at al. [GKQV10] introduced a modular system which allows to more easily develop BFT protocols that match speciﬁc applications in terms of robust- ness or best case performance.  Bibliography  [ABC+02] Atul Adya, William J. Bolosky, Miguel Castro, Ger- ald Cermak, Ronnie Chaiken, John R. Douceur, Jon Howell, Jacob R. Lorch, Marvin Theimer, and Roger P. Wattenhofer. Farsite: Federated, available, and reliable storage for an incompletely trusted environment. SIGOPS Oper. Syst. Rev., 36 SI :1–14, December 2002.  [AEMGG+05] Michael Abd-El-Malek, Gregory R Ganger, Garth R Goodson, Michael K Reiter, and Jay J Wylie. Fault-scalable byzantine fault-tolerant services. ACM SIGOPS Operating Systems Review, 39 5 :59–74, 2005.  [CML+06] James Cowling, Daniel Myers, Barbara Liskov, Ro- drigo Rodrigues, and Liuba Shrira. Hq replication: A hybrid quorum protocol for byzantine fault toler- ance. In Proceedings of the 7th Symposium on Op- erating Systems Design and Implementation, OSDI ’06, pages 177–190, Berkeley, CA, USA, 2006.  [CWA+09] Allen Clement, Edmund L Wong, Lorenzo Alvisi, Michael Dahlin, and Mirco Marchetti. Making byzantine fault tolerant systems tolerate byzantine faults. In NSDI, volume 9, pages 153–168, 2009.  [DFF+82] Danny Dolev, Michael J Fischer, Rob Fowler, Nancy A Lynch, and H Raymond Strong. An eﬃcient algorithm for byzantine agreement with- out authentication. Information and Control, 52 3 :257–274, 1982.  [GKQV10] Rachid Guerraoui, Nikola Kneˇzevi´c, Vivien Qu´ema, and Marko Vukoli´c. The next 700 bft protocols.   86  CHAPTER 6. AUTHENTICATED AGREEMENT  In Proceedings of the 5th European conference on Computer systems, pages 363–376. ACM, 2010.  [KAD+07] Ramakrishna Kotla, Lorenzo Alvisi, Mike Dahlin, Allen Clement, and Edmund Wong. Zyzzyva: spec- ulative byzantine fault tolerance. In ACM SIGOPS Operating Systems Review, volume 41, pages 45–58. ACM, 2007.  [MC99] Barbara Liskov Miguel Castro. Practical byzantine fault tolerance. In OSDI, volume 99, pages 173–186, 1999.   Chapter 7  Quorum Systems  What happens if a single server is no longer powerful enough to service all your customers? The obvious choice is to add more servers and to use the majority approach  e.g. Paxos, Chapter 2  to guarantee consistency. However, even if you buy one million servers, a client still has to access more than half of them per request! While you gain fault-tolerance, your eﬃciency can at most be doubled. Do we have to give up on consistency?  Let us take a step back: We used majorities because major- ity sets always overlap. But are majority sets the only sets that guarantee overlap? In this chapter we study the theory behind overlapping sets, known as quorum systems. Deﬁnition 7.1  quorum, quorum system . Let V = {v1, . . . , vn} be a set of nodes. A quorum Q ⊆ V is a subset of these nodes. A quorum system S ⊂ 2V is a set of quorums s.t. every two quorums intersect, i.e., Q1 ∩ Q2  cid:54 = ∅ for all Q1, Q2 ∈ S.  Remarks:    When a quorum system is being used, a client selects a quorum, acquires a lock  or ticket  on all nodes of the quorum, and when done releases all locks again. The idea is that no matter which quorum is chosen, its nodes will intersect with the nodes of every other quorum.    What can happen if two quorums try to lock their nodes    A quorum system S is called minimal if ∀Q1, Q2 ∈ S :  at the same time?  Q1  cid:54 ⊂ Q2.  87   88  CHAPTER 7. QUORUM SYSTEMS    The simplest quorum system imaginable consists of just one quorum, which in turn just consists of one server. It is known as Singleton.    In the Majority quorum system, every quorum has  cid:98  n  2 cid:99 +  1 nodes.    Can you think of other simple quorum systems?  7.1 Load and Work  the probability PZ Q  of accessing a quorum Q ∈ S s.t. cid:80   Deﬁnition 7.2  access strategy . An access strategy Z deﬁnes Q∈S PZ Q   Deﬁnition 7.3  load .  = 1.   cid:80     The load of access strategy Z on a node vi is LZ vi  =  Q∈S;vi∈Q PZ Q .    The load induced by access strategy Z on a quorum system S is the maximal load induced by Z on any node in S, i.e., LZ S  = maxvi∈S LZ vi .    The load of a quorum system S is L S  = minZ LZ S .  Deﬁnition 7.4  work .    The work of a quorum Q ∈ S is the number of nodes in Q,  W  Q  = Q.    The work induced by access strategy Z on a quorum system S is the expected number of nodes accessed, i.e., WZ S  =   cid:80  Q∈S PZ Q  · W  Q .    The work of a quorum system S is W  S  = minZ WZ S .  Remarks:    Note that you cannot choose diﬀerent access strategies Z for work and load, you have to pick a single Z for both.   We illustrate the above concepts with a small example. Let V = {v1, v2, v3, v4, v5} and S = {Q1, Q2, Q3, Q4}, with Q1 = {v1, v2}, Q2 = {v1, v3, v4}, Q3 = {v2, v3, v5}, Q4 = {v2, v4, v5}. If we choose the access strategy Z   7.1. LOAD AND WORK  89  s.t. PZ Q1  = 1 2 and PZ Q2  = PZ Q3  = PZ Q4  = 1 6, then the node with the highest load is v2 with LZ v2  = 1 2 + 1 6 + 1 6 = 5 6, i.e., LZ S  = 5 6. Regarding work, we have WZ S  = 1 2· 2 + 1 6· 3 + 1 6· 3 + 1 6· 3 = 15 6.    Can you come up with a better access strategy for S?   If every quorum Q in a quorum system S has the same  number of elements, S is called uniform.    What is the minimum load a quorum system can have?  Primary Copy vs. Majority How many nodes need to be accessed? What is the load of the busiest node?  Singleton Majority   Work   Load   1 1  > n 2 > 1 2  n  Table 7.5: First comparison of the Singleton and Majority quorum systems. Note that the Singleton quorum system can be a good choice when the failure probability of every single node is > 1 2. √ Theorem 7.6. Let S be a quorum system. Then L S  ≥ 1  holds. Proof. Let Q = {v1, . . . , vq} be a quorum of minimal size in S, with sizes Q = q and S = s. Let Z be an access strategy for S. Every other quorum in S intersects in at least one element with this quorum Q. Each time a quorum is accessed, at least one node in Q is accessed as well, yielding a lower bound of LZ vi  ≥ 1 q for some vi ∈ Q. Furthermore, as Q is minimal, at least q nodes need to be ac- cessed, yielding W  S  ≥ q. Thus, LZ vi  ≥ q n for some vi ∈ Q, as each time q nodes are accessed, the load of the most accessed node is at least q n. Combining both ideas leads to LZ S  ≥ max  1 q, q n  ⇒ √ LZ S  ≥ 1  n, as Z can be any access strategy.  n. Thus, L S  ≥ 1   √  Remarks:    Can we achieve this load?   90  CHAPTER 7. QUORUM SYSTEMS  7.2 Grid Quorum Systems n ∈ N, Deﬁnition 7.7  Basic Grid quorum system . Assume √ √ and arrange the n nodes in a square matrix with side length of n for 1 ≤ i ≤ √ quorums, with each containing the full row i and the full column i,  n, i.e., in a grid. The basic Grid quorum system consists of  √  n.  each quorum Qi with 1 ≤ i ≤ √ Figure 7.8: The basic version of the Grid quorum system, where √ n uses row i and column i. The n− 1 and two quorums overlap in exactly size of each quorum is 2 √ two nodes. Thus, when the access strategy Z is uniform  i.e., the n− 1, and the √ probability of each quorum is 1  load of every node is in Θ 1   √ n , the work is 2  n .  Remarks:    Consider the right picture in Figure 7.8: The two quo- rums intersect in two nodes. If both quorums were to be accessed at the same time, it is not guaranteed that at least one quorum will lock all of its nodes, as they could enter a deadlock!    In the case of just two quorums, one could solve this by letting the quorums just intersect in one node, see Fig- ure 7.9. However, already with three quorums the same situation could occur again, progress is not guaranteed!   However, by deviating from the “access all at once” strat- egy, we can guarantee progress if the nodes are totally ordered!  Theorem 7.11. If each quorum is accessed by Algorithm 7.10, at least one quorum will obtain a lock for all of its nodes.  Proof. We prove the theorem by contradiction. Assume no quorum can make progress, i.e., for every quorum we have: At least one of   7.2. GRID QUORUM SYSTEMS  91  Figure 7.9: There are other ways to choose quorums in the grid s.t. pairwise diﬀerent quorums only intersect in one node. The size n − 1, i.e., the work is in √ of each quorum is between √ n . When the access strategy Z is uniform, the load of every Θ  node is in Θ 1   n and 2  √  √  n .  Algorithm 7.10 Sequential Locking Strategy for a Quorum Q  1: Attempt to lock the nodes one by one, ordered by their identi-  ﬁers  2: Should a node be already locked, release all locks and start over  its nodes is locked by another quorum. Let v be the node with the highest identiﬁer that is locked by some quorum Q. Observe that Q already locked all of its nodes with a smaller identiﬁer than v, otherwise Q would have restarted. As all nodes with a higher identiﬁer than v are not locked, Q either has locked all of its nodes or can make progress – a contradiction. As the set of nodes is ﬁnite, one quorum will eventually be able to lock all of its nodes.  Remarks:    But now we are back to sequential accesses in a distrib- uted system? Let’s do it concurrently with the same idea, i.e., resolving conﬂicts by the ordering of the nodes. Then, a quorum that locked the highest identiﬁer so far can always make progress!   92  CHAPTER 7. QUORUM SYSTEMS  Algorithm 7.12 Concurrent Locking Strategy for a Quorum Q Invariant: Let vQ ∈ Q be the highest identiﬁer of a node locked by Q s.t. all nodes vi ∈ Q with vi < vQ are locked by Q as well. Should Q not have any lock, then vQ is set to 0.  1: repeat 2: Attempt to lock all nodes of the quorum Q 3:  for each node v ∈ Q that was not able to be locked by Q do  exchange vQ and vQ cid:48  with the quorum Q cid:48  that locked v if vQ > vQ cid:48  then  Q cid:48  releases lock on v and Q acquires lock on v  4: 5:  6: 7: 8: 9: until all nodes of the quorum Q are locked  end if end for  Theorem 7.13. If the nodes and quorums use Algorithm 7.12, at least one quorum will obtain a lock for all of its nodes.  Proof. The proof is analogous to the proof of Theorem 7.11: As- sume for contradiction that no quorum can make progress. How- ever, at least the quorum with the highest vQ can always make progress – a contradiction! As the set of nodes is ﬁnite, at least one quorum will eventually be able to acquire a lock on all of its nodes.  Remarks:    What if a quorum locks all of its nodes and then crashes? Is the quorum system dead now? This issue can be pre- vented by, e.g., using leases instead of locks: leases have a timeout, i.e., a lock is released eventually.  7.3 Fault Tolerance  Deﬁnition 7.14  resilience . If any f nodes from a quorum system S can fail s.t. there is still a quorum Q ∈ S without failed nodes, then S is f -resilient. The largest such f is the resilience R S . Theorem 7.15. Let S be a Grid quorum system where each of the n quorums consists of a full row and a full column. S has a resilience of  n − 1.  √   7.3. FAULT TOLERANCE  93  √  √ Proof. If all n nodes on the diagonal of the grid fail, then every quorum will have at least one failed node. Should less than n nodes fail, then there is a row and a column without failed nodes.  Remarks:    The Grid quorum system in Theorem 7.15 is diﬀerent from the Basic Grid quorum system described in Deﬁ- nition 7.7. In each quorum in the Basic Grid quorum system the row and column index are identical, while in the Grid quorum system of Theorem 7.15 this is not the case.  Deﬁnition 7.16  failure probability . Assume that every node works with a ﬁxed probability p  in the following we assume con- crete values, e.g. p > 1 2 . The failure probability Fp S  of a quorum system S is the probability that at least one node of every quorum fails.  Remarks:    The asymptotic failure probability is Fp S  for n → ∞. Facts 7.17. A version of a Chernoﬀ bound states the following: Let x1, . . . , xn be independent Bernoulli-distributed random vari- ables with P r[xi = 1] = pi and P r[xi = 0] = 1 − pi = qi, then for X :=  i=1 xi and µ := E[X] = cid:80 n  cid:80 n  i=1 pi the following holds:  for all 0 < δ < 1: P r[X ≤  1 − δ µ] ≤ e−µδ2 2 .  Theorem 7.18. The asymptotic failure probability of the Majority quorum system is 0.  Proof. In a Majority quorum system each quorum contains exactly  cid:98  n 2 cid:99  + 1 nodes and each subset of nodes with cardinality  cid:98  n 2 cid:99  + 1 2 cid:99  forms a quorum. The Majority quorum system fails, if only  cid:98  n nodes work. Otherwise there is at least one quorum available. In order to calculate the failure probability we deﬁne the following random variables:  1, 0,  xi =  and X := cid:80 n  if node i works, happens with probability p if node i fails, happens with probability q = 1 − p i=1 xi, with µ = np,   cid:40    94  CHAPTER 7. QUORUM SYSTEMS  whereas X corresponds to the number of working nodes. To estimate the probability that the number of working nodes is less than  cid:98  n above. By setting δ = 1 − 1 P r[X ≤ n  2 cid:99  + 1 we will make use of the Chernoﬀ inequality from 2 cid:99 ] ≤ 2 ] = P r[X ≤  1 − δ µ]. 2p we have 0 < δ ≤ 1 2 due to 1 2 < p ≤ 1. Thus, we can use the Chernoﬀ bound and get FP  S  ≤ e−µδ2 2 ∈ e−Ω n .  2p we obtain FP  S  = P r[X ≤  cid:98  n  With δ = 1 − 1  Theorem 7.19. The asymptotic failure probability of the Grid quo- rum system is 1. Proof. Consider the n = d · d nodes to be arranged in a d × d grid. A quorum always contains one full row. In this estimation we will make use of the Bernoulli inequality which states that for all n ∈ N, x ≥ −1 :  1 + x n ≥ 1 + nx. The system fails, if in each row at least one node fails  which happens with probability 1 − pd for a particular row, as all nodes work with probability pd . Therefore we can bound the failure probability from below with: Fp S  ≥ P r[at least one failure per row] =  1 − pd d ≥ 1 − dpd −→ n→∞ 1.  Remarks:    Now we have a quorum system with optimal load  the Grid  and one with fault-tolerance  Majority , but what if we want both?  Deﬁnition 7.20  B-Grid quorum system . Consider n = dhr nodes, arranged in a rectangular grid with h·r rows and d columns. Each group of r rows is a band, and r elements in a column re- stricted to a band are called a mini-column. A quorum consists of one mini-column in every band and one element from each mini- column of one band; thus every quorum has d + hr − 1 elements. The B-Grid quorum system consists of all such quorums.  Theorem 7.22. The asymptotic failure probability of the B-Grid quorum system is 0.  Proof. Suppose n = dhr and the elements are arranged in a grid with d columns and h · r rows. The B-Grid quorum system does fail if in each band a complete mini-column fails, because then it   7.4. BYZANTINE QUORUM SYSTEMS  95  Figure 7.21: A B-Grid quorum system with n = 100 nodes, d = 10 columns, h · r = 10 rows, h = 5 bands, and r = 2. The depicted quorum has a d + hr − 1 = 10 + 5 · 2 − 1 = 19 nodes. If the access strategy Z is chosen uniformly, then we have a work of d + hr − 1 and a load of d+hr−1 √ n and r = log n, we obtain a work of Θ   n .  . By setting d = n  and a load of Θ  1   √ √  n  is not possible to choose a band where in each mini-column an element is still working. It also fails if in a band an element in each mini-column fails. Those events may not be independent of each other, but with the help of the union bound, we can upper bound the failure probability with the following equation: Fp S  ≤ P r[in every band a complete mini-column fails]  + P r[in a band at least one element of every m.-col. fails] ≤  d 1 − p r h + h 1 − pr d  √  We use d =  n, r = ln d, and 0 ≤  1 − p  ≤ 1 3. Using nln x = xln n, we have d 1 − p r ≤ d · dln 1 3 ≈ d−0.1, and hence for large enough d the whole ﬁrst term is bounded from above by d−0.1h  cid:28  1 d2 = 1 n. Regarding the second term, we have p ≥ 2 3, and h = d  ln d < d. Hence we can bound the term from above by d 1 − dln 2 3 d ≈ d 1−d−0.4 d. Using  1+t n n ≤ et, we get  again, for large enough d  an upper bound of d 1 − d−0.4 d = d 1 − d0.6 d d ≤ d · e−d0.6 = d −d0.6  ln d +1  cid:28  d−2 = 1 n. In total, we have Fp S  ∈ O 1 n .  7.4 Byzantine Quorum Systems  While failed nodes are bad, they are still easy to deal with: just access another quorum where all nodes can respond! Byzantine nodes make life more diﬃcult however, as they can pretend to be   96  CHAPTER 7. QUORUM SYSTEMS  Singleton Majority  Work Load Resilience F. Prob.∗∗  1 1 0 1 − p  > n 2 > 1 2 < n 2 → 0  Θ cid:0 1   Grid √ √ Θ   n  √ Θ   n  → 1  n cid:1  Θ cid:0 1   n cid:1   B-Grid∗ √ √ n  Θ   √ Θ   n  → 0  Table 7.23: Overview of the diﬀerent quorum systems regarding resilience, work, load, and their asymptotic failure probability. The best entries in each row are set in bold. ∗ Setting d = ∗∗Assuming prob. q =  1 − p  is constant but signiﬁcantly less than 1 2  n and r = log n  √  a regular node, i.e., one needs more sophisticated methods to deal with them. We need to ensure that the intersection of two quorums always contains a non-byzantine  correct  node and furthermore, the byzantine nodes should not be allowed to inﬁltrate every quo- rum. In this section we study three counter-measures of increasing strength, and their implications on the load of quorum systems. Deﬁnition 7.24  f -disseminating . A quorum system S is f - disseminating if  1  the intersection of two diﬀerent quorums always contains f + 1 nodes, and  2  for any set of f byzantine nodes, there is at least one quorum without byzantine nodes.  Remarks:    Thanks to  2 , even with f byzantine nodes, the byzan- tine nodes cannot stop all quorums by just pretending to have crashed. At least one quorum will survive. We will also keep this assumption for the upcoming more advanced byzantine quorum systems.    Byzantine nodes can also do something worse than crash- ing - they could falsify data! Nonetheless, due to  1 , there is at least one non-byzantine node in every quorum intersection. If the data is self-verifying by, e.g., authen- tication, then this one node is enough.    If the data is not self-verifying, then we need another  mechanism.  Deﬁnition 7.25  f -masking . A quorum system S is f -masking if  1  the intersection of two diﬀerent quorums always contains   7.4. BYZANTINE QUORUM SYSTEMS  97  2f + 1 nodes, and  2  for any set of f byzantine nodes, there is at least one quorum without byzantine nodes.  Remarks:    Note that except for the second condition, an f -masking quorum system is the same as a 2f -disseminating system. The idea is that the non-byzantine nodes  at least f + 1 can outvote the byzantine ones  at most f  , but only if all non-byzantine nodes are up-to-date!    This raises an issue not covered yet in this chapter. If we access some quorum and update its values, this change still has to be disseminated to the other nodes in the byzantine quorum system. Opaque quorum systems deal with this issue, which are discussed at the end of this section.    f -disseminating quorum systems need more than 3f nodes and f -masking quorum systems need more than 4f nodes. Essentially, the quorums may not contain too many nodes, and the diﬀerent intersection properties lead to the dif- ferent bounds.  L S  ≥ cid:112  f + 1  n holds. L S  ≥ cid:112  2f + 1  n holds.  Theorem 7.26. Let S be a f -disseminating quorum system. Then  Theorem 7.27. Let S be a f -masking quorum system. Then  Proofs of Theorems 7.26 and 7.27. The proofs follow the proof of Theorem 7.6, by observing that now not just one element is ac- cessed from a minimal quorum, but f +1 or 2f +1, respectively.  Deﬁnition 7.28  f -masking Grid quorum system . A f -masking Grid quorum system is constructed as the grid quorum system, but each quorum contains one full column and f +1 rows of nodes, with  2f + 1 ≤ √  n.  Remarks:    The f -masking Grid nearly hits the lower bound for the load of f -masking quorum systems, but not quite. A small change and we will be optimal asymptotically.   98  CHAPTER 7. QUORUM SYSTEMS  √ Figure 7.29: An example how to choose a quorum in the f -masking Grid with f = 2, i.e., 2 + 1 = 3 rows. The load is in Θ f   n  when the access strategy is chosen to be uniform. Two quorums overlap by their columns intersecting each other’s rows, i.e., they overlap in at least 2f + 2 nodes.  Deﬁnition 7.30  M -Grid quorum system . The M -Grid quo- rum system is constructed as the grid quorum as well, but each quorum contains f + 1 columns of nodes, with  f + 1 rows and  √  √  f ≤ √  n−1 2  .  Figure 7.31: An example how to choose a quorum in the M - Grid with f = 3, i.e., 2 rows and 2 columns. The load is in  Θ  cid:112 f  n  when the access strategy is chosen to be uniform. Two  √  quorums overlap with each row intersecting each other’s column, i.e., 2  f + 12 = 2f + 2 nodes.  Corollary 7.32. The f -masking Grid quorum system and the M - Grid quorum system are f -masking quorum systems.  Remarks:    We achieved nearly the same load as without byzantine nodes! However, as mentioned earlier, what happens if we access a quorum that is not up-to-date, except for the intersection with an up-to-date quorum? Surely we can ﬁx that as well without too much loss?   7.4. BYZANTINE QUORUM SYSTEMS  99    This property will be handled in the last part of this chapter by opaque quorum systems. It will ensure that the number of correct up-to-date nodes accessed will be larger than the number of out-of-date nodes combined with the byzantine nodes in the quorum  cf.  7.33.1  .  Deﬁnition 7.33  f -opaque quorum system . A quorum system S is f -opaque if the following two properties hold for any set of f byzantine nodes F and any two diﬀerent quorums Q1, Q2:   Q1 ∩ Q2  \ F >  Q2 ∩ F   ∪  Q2 \ Q1    F ∩ Q  = ∅ for some Q ∈ S   7.33.1    7.33.2   Figure 7.34: Intersection properties of an opaque quorum system. Equation  7.33.1  ensures that the set of non-byzantine nodes in the intersection of Q1, Q2 is larger than the set of out of date nodes, even if the byzantine nodes “team up” with those nodes. Thus, the correct up to date value can always be recognized by a majority voting.  Theorem 7.35. Let S be a f -opaque quorum system. Then, n > 5f .  Proof. Due to  7.33.2 , there exists a quorum Q1 with size at most n − f . With  7.33.1 , Q1 > f holds. Let F1 be a set of f  byzantine  nodes F1 ⊂ Q1, and with  7.33.2 , there ex- ists a Q2 ⊂ V \ F1. Thus, Q1 ∩ Q2 ≤ n − 2f . With  7.33.1 , Q1 ∩ Q2 > f holds. Thus, one could choose f  byzantine  nodes F2 with F2 ⊂  Q1∩ Q2 . Using  7.33.1  one can bound n− 3f from below: n − 3f >  Q2 ∩ Q1  − F2 ≥  Q2 ∩ Q1  ∪  Q1 ∩ F2  ≥ F1 + F2 = 2f.   100  CHAPTER 7. QUORUM SYSTEMS  Remarks:    One can extend the Majority quorum system to be f - opaque by setting the size of each quorum to contain  cid:100  2n + 2f   3 cid:101  nodes. Then its load is 1 n cid:100  2n + 2f   3 cid:101  ≈ 2 3 + 2f  3n ≥ 2 3.    Can we do much better? Sadly, no...  Theorem 7.36. Let S be a f -opaque quorum system. Then L S  ≥ 1 2 holds. Proof. Equation  7.33.1  implies that for Q1, Q2 ∈ S, the intersec- tion of both Q1, Q2 is at least half their size, i.e.,  Q1 ∩ Q2  ≥ Q1 2. Let S consist of quorums Q1, Q2, . . . . The load induced by an access strategy Z on Q1 is:   cid:88    cid:88   v∈Q1  v∈Qi  LZ Qi  ≥ cid:88   Qi   Q1 2  LZ Qi  = Q1 2 .  Using the pigeonhole principle, there must be at least one node in Q1 with load of at least 1 2.  Chapter Notes  Historically, a quorum is the minimum number of members of a deliberative body necessary to conduct the business of that group. Their use has inspired the introduction of quorum systems in com- puter science since the late 1970s early 1980s. Early work focused on Majority quorum systems [Lam78, Gif79, Tho79], with the no- tion of minimality introduced shortly after [GB85]. The Grid quo- rum system was ﬁrst considered in [Mae85], with the B-Grid being introduced in [NW94]. The latter article and [PW95] also initiated the study of load and resilience.  The f -masking Grid quorum system and opaque quorum sys- tems are from [MR98], and the M -Grid quorum system was in- troduced in [MRW97]. Both papers also mark the start of the formal study of Byzantine quorum systems. The f -masking and the M -Grid have asymptotic failure probabilities of 1, more com- plex systems with better values can be found in these papers as well.  Quorum systems have also been extended to cope with nodes dynamically leaving and joining, see, e.g., the dynamic paths quo- rum system in [NW05].   BIBLIOGRAPHY  101  For a further overview on quorum systems, we refer to the book by Vukoli´c [Vuk12] and the article by Merideth and Reiter [MR10].  Bibliography  [GB85] Hector Garcia-Molina and Daniel Barbar´a. How to as- sign votes in a distributed system. J. ACM, 32 4 :841– 860, 1985.  [Gif79] David K. Giﬀord. Weighted voting for replicated data. In Michael D. Schroeder and Anita K. Jones, editors, Proceedings of the Seventh Symposium on Operating System Principles, SOSP 1979, Asilomar Conference Grounds, Paciﬁc Grove, California, USA, 10-12, De- cember 1979, pages 150–162. ACM, 1979.  [Lam78] Leslie Lamport. The implementation of reliable distrib- uted multiprocess systems. Computer Networks, 2:95– 114, 1978.  [Mae85] Mamoru Maekawa. A square root N algorithm for mu- tual exclusion in decentralized systems. ACM Trans. Comput. Syst., 3 2 :145–159, 1985.  [MR98] Dahlia Malkhi and Michael K. Reiter. Byzantine quorum  systems. Distributed Computing, 11 4 :203–213, 1998.  [MR10] Michael G. Merideth and Michael K. Reiter. Selected results from the latest decade of quorum systems re- search. In Bernadette Charron-Bost, Fernando Pedone, and Andr´e Schiper, editors, Replication: Theory and Practice, volume 5959 of Lecture Notes in Computer Sci- ence, pages 185–206. Springer, 2010.  [MRW97] Dahlia Malkhi, Michael K. Reiter, and Avishai Wool. The load and availability of byzantine quorum systems. In James E. Burns and Hagit Attiya, editors, Proceed- ings of the Sixteenth Annual ACM Symposium on Prin- ciples of Distributed Computing, Santa Barbara, Cali- fornia, USA, August 21-24, 1997, pages 249–257. ACM, 1997.  [NW94] Moni Naor and Avishai Wool. The load, capacity and availability of quorum systems. In 35th Annual Sym- posium on Foundations of Computer Science, Santa Fe,   102  CHAPTER 7. QUORUM SYSTEMS  New Mexico, USA, 20-22 November 1994, pages 214– 225. IEEE Computer Society, 1994.  [NW05] Moni Naor and Udi Wieder. Scalable and dynamic quo- rum systems. Distributed Computing, 17 4 :311–322, 2005.  [PW95] David Peleg and Avishai Wool. The availability of quo-  rum systems. Inf. Comput., 123 2 :210–223, 1995.  [Tho79] Robert H. Thomas. A majority consensus approach to concurrency control for multiple copy databases. ACM Trans. Database Syst., 4 2 :180–209, 1979.  [Vuk12] Marko Vukolic. Quorum Systems: With Applications to Storage and Consensus. Synthesis Lectures on Distrib- uted Computing Theory. Morgan & Claypool Publish- ers, 2012.  Bibliography  [GB85] Hector Garcia-Molina and Daniel Barbar´a. How to as- sign votes in a distributed system. J. ACM, 32 4 :841– 860, 1985.  [Gif79] David K. Giﬀord. Weighted voting for replicated data. In Michael D. Schroeder and Anita K. Jones, editors, Proceedings of the Seventh Symposium on Operating System Principles, SOSP 1979, Asilomar Conference Grounds, Paciﬁc Grove, California, USA, 10-12, De- cember 1979, pages 150–162. ACM, 1979.  [Lam78] Leslie Lamport. The implementation of reliable distrib- uted multiprocess systems. Computer Networks, 2:95– 114, 1978.  [Mae85] Mamoru Maekawa. A square root N algorithm for mu- tual exclusion in decentralized systems. ACM Trans. Comput. Syst., 3 2 :145–159, 1985.  [MR98] Dahlia Malkhi and Michael K. Reiter. Byzantine quorum  systems. Distributed Computing, 11 4 :203–213, 1998.   BIBLIOGRAPHY  103  [MR10] Michael G. Merideth and Michael K. Reiter. Selected results from the latest decade of quorum systems re- search. In Bernadette Charron-Bost, Fernando Pedone, and Andr´e Schiper, editors, Replication: Theory and Practice, volume 5959 of Lecture Notes in Computer Sci- ence, pages 185–206. Springer, 2010.  [MRW97] Dahlia Malkhi, Michael K. Reiter, and Avishai Wool. The load and availability of byzantine quorum systems. In James E. Burns and Hagit Attiya, editors, Proceed- ings of the Sixteenth Annual ACM Symposium on Prin- ciples of Distributed Computing, Santa Barbara, Cali- fornia, USA, August 21-24, 1997, pages 249–257. ACM, 1997.  [NW94] Moni Naor and Avishai Wool. The load, capacity and availability of quorum systems. In 35th Annual Sym- posium on Foundations of Computer Science, Santa Fe, New Mexico, USA, 20-22 November 1994, pages 214– 225. IEEE Computer Society, 1994.  [NW05] Moni Naor and Udi Wieder. Scalable and dynamic quo- rum systems. Distributed Computing, 17 4 :311–322, 2005.  [PW95] David Peleg and Avishai Wool. The availability of quo-  rum systems. Inf. Comput., 123 2 :210–223, 1995.  [Tho79] Robert H. Thomas. A majority consensus approach to concurrency control for multiple copy databases. ACM Trans. Database Syst., 4 2 :180–209, 1979.  [Vuk12] Marko Vukolic. Quorum Systems: With Applications to Storage and Consensus. Synthesis Lectures on Distrib- uted Computing Theory. Morgan & Claypool Publish- ers, 2012.   104  CHAPTER 7. QUORUM SYSTEMS   Chapter 8  Eventual Consistency & Bitcoin  How would you implement an ATM? Does the following implemen- tation work satisfactorily?  Algorithm 8.1 Na¨ıve ATM  1: ATM makes withdrawal request to bank 2: ATM waits for response from bank 3: if balance of customer suﬃcient then 4: ATM dispenses cash 5: else 6: ATM displays error 7: end if  Remarks:    A connection problem between the bank and the ATM  may block Algorithm 8.1 in Line 2.    A network partition is a failure where a network splits into at least two parts that cannot communicate with each other. Intuitively any non-trivial distributed system cannot proceed during a partition and maintain consis- tency. In the following we introduce the tradeoﬀ between consistency, availability and partition tolerance.    There are numerous causes for partitions to occur, e.g., physical disconnections, software errors, or incompatible  105   106 CHAPTER 8. EVENTUAL CONSISTENCY & BITCOIN  protocol versions. From the point of view of a node in the system, a partition is similar to a period of sustained message loss.  8.1 Consistency, Availability and Parti-  tions  Deﬁnition 8.2  Consistency . All nodes in the system agree on the current state of the system.  Deﬁnition 8.3  Availability . The system is operational and in- stantly processing incoming requests.  Deﬁnition 8.4  Partition Tolerance . Partition tolerance is the ability of a distributed system to continue operating correctly even in the presence of a network partition.  Theorem 8.5  CAP Theorem . It is impossible for a distrib- uted system to simultaneously provide Consistency, Availability and Partition Tolerance. A distributed system can satisfy any two of these but not all three.  Proof. Assume two nodes, sharing some state. The nodes are in diﬀerent partitions, i.e., they cannot communicate. Assume a re- quest wants to update the state and contacts a node. The node may either: 1  update its local state, resulting in inconsistent states, or 2  not update its local state, i.e., the system is no longer available for updates.  Algorithm 8.6 Partition tolerant and available ATM  Synchronize local view of balances between ATM and bank if balance of customer insuﬃcient then  ATM displays error and aborts user interaction  1: if bank reachable then 2: 3: 4: 5: 6: end if 7: ATM dispenses cash 8: ATM logs withdrawal for synchronization  end if   8.2. BITCOIN  Remarks:  107    Algorithm 8.6 is partition tolerant and available since it continues to process requests even when the bank is not reachable.    The ATM’s local view of the balances may diverge from the balances as seen by the bank, therefore consistency is no longer guaranteed.    The algorithm will synchronize any changes it made to the local balances back to the bank once connectivity is re-established. This is known as eventual consistency.  Deﬁnition 8.7  Eventual Consistency . If no new updates to the shared state are issued, then eventually the system is in a quiescent state, i.e., no more messages need to be exchanged between nodes, and the shared state is consistent.  Remarks:    Eventual consistency is a form of weak consistency.   Eventual consistency guarantees that the state is eventu- ally agreed upon, but the nodes may disagree temporar- ily.    During a partition, diﬀerent updates may semantically conﬂict with each other. A conﬂict resolution mechanism is required to resolve the conﬂicts and allow the nodes to eventually agree on a common state.    One example of eventual consistency is the Bitcoin cryp-  tocurrency system.  8.2 Bitcoin  Deﬁnition 8.8  Bitcoin Network . The Bitcoin network is a ran- domly connected overlay network of a few thousand nodes, con- trolled by a variety of owners. All nodes perform the same opera- tions, i.e., it is a homogenous network and without central control.   108 CHAPTER 8. EVENTUAL CONSISTENCY & BITCOIN  Remarks:    The lack of structure is intentional:  it ensures that an attacker cannot strategically position itself in the network and manipulate the information exchange. Information is exchanged via a simple broadcasting protocol.  Deﬁnition 8.9  Address . Users may generate any number of pri- vate keys, from which a public key is then derived. An address is derived from a public key and may be used to identify the recipient of funds in Bitcoin. The private public key pair is used to uniquely identify the owner of funds of an address.  Remarks:    The terms public key and address are often used inter- changeably, since both are public information. The ad- vantage of using an address is that its representation is shorter than the public key.    It is hard to link addresses to the user that controls them, hence Bitcoin is often referred to as being pseudonymous.   Not every user needs to run a fully validating node, and end-users will likely use a lightweight client that only temporarily connects to the network.    The Bitcoin network collaboratively tracks the balance  in bitcoins of each address.    The address is composed of a network identiﬁer byte, the hash of the public key and a checksum. It is commonly stored in base 58 encoding, a custom encoding similar to base 64 with some ambiguous symbols removed, e.g., lowercase letter “l” since it is similar to the number “1”.   The hashing algorithm produces addresses of size 20 bytes. This means that there are 2160 distinct addresses. It might be tempting to brute force a target address, how- ever at one billion trials per second one still requires ap- proximately 245 years in expectation to ﬁnd a matching private public key pair. Due to the birthday paradox the odds improve if instead of brute forcing a single address we attempt to brute force any address. While the odds of a successful trial increase with the number of addresses, lookups become more costly.   8.2. BITCOIN  109  Deﬁnition 8.10  Output . An output is a tuple consisting of an amount of bitcoins and a spending condition. Most commonly the spending condition requires a valid signature associated with the private key of an address.  Remarks:    Spending conditions are scripts that oﬀer a variety of op- tions. Apart from a single signature, they may include conditions that require the result of a simple computa- tion, or the solution to a cryptographic puzzle.    Outputs exist in two states: unspent and spent. Any output can be spent at most once. The address balance is the sum of bitcoin amounts in unspent outputs that are associated with the address.    The set of unspent transaction outputs  UTXOs  and some additional global parameters are the shared state of Bitcoin. Every node in the Bitcoin network holds a com- plete replica of that state. Local replicas may temporarily diverge, but consistency is eventually re-established.  Deﬁnition 8.11  Input . An input is a tuple consisting of a ref- erence to a previously created output and arguments  signature  to the spending condition, proving that the transaction creator has the permission to spend the referenced output.  Deﬁnition 8.12  Transaction . A transaction is a data structure that describes the transfer of bitcoins from spenders to recipients. The transaction consists of a number of inputs and new outputs. The inputs result in the referenced outputs spent  removed from the UTXO , and the new outputs being added to the UTXO.  Remarks:    Inputs reference the output that is being spent by a  h, i - tuple, where h is the hash of the transaction that created the output, and i speciﬁes the index of the output in that transaction.    Transactions are broadcast in the Bitcoin network and  processed by every node that receives them.   110 CHAPTER 8. EVENTUAL CONSISTENCY & BITCOIN  Algorithm 8.13 Node Receives Transaction  1: Receive transaction t 2: for each input  h, i  in t do 3:  if output  h, i  is not in local UTXO or signature invalid then  Drop t and stop  4: end if 5: 6: end for 7: if sum of values of inputs < sum of values of new outputs then 8: Drop t and stop 9: end if 10: for each input  h, i  in t do 11: 12: end for 13: Append t to local history 14: Forward t to neighbors in the Bitcoin network  Remove  h, i  from local UTXO  Remarks:    Note that the eﬀect of a transaction on the state is deter- ministic. In other words if all nodes receive the same set of transactions in the same order  Deﬁnition 2.8 , then the state across nodes is consistent.    The outputs of a transaction may assign less than the sum of inputs, in which case the diﬀerence is called the transaction fee. The fee is used to incentivize other par- ticipants in the system  see Deﬁnition 8.19     Notice that so far we only described a local acceptance policy. Nothing prevents nodes to locally accept diﬀerent transactions that spend the same output.    Transactions are in one of two states: unconﬁrmed or conﬁrmed. Incoming transactions from the broadcast are unconﬁrmed and added to a pool of transactions called the memory pool.  Deﬁnition 8.14  Doublespend . A doublespend is a situation in which multiple transactions attempt to spend the same output. Only one transaction can be valid since outputs can only be spent once. When nodes accept diﬀerent transactions in a doublespend, the shared state becomes inconsistent.   8.2. BITCOIN  Remarks:  111    Doublespends may occur naturally, e.g., if outputs are co- owned by multiple users. However, often doublespends are intentional – we call these doublespend-attacks: In a transaction, an attacker pretends to transfer an output to a victim, only to doublespend the same output in another transaction back to itself.    Doublespends can result in an inconsistent state since the validity of transactions depends on the order in which they arrive. If two conﬂicting transactions are seen by a node, the node considers the ﬁrst to be valid, see Al- gorithm 8.13. The second transaction is invalid since it tries to spend an output that is already spent. The order in which transactions are seen, may not be the same for all nodes, hence the inconsistent state.    If doublespends are not resolved, the shared state di- verges. Therefore a conﬂict resolution mechanism is need- ed to decide which of the conﬂicting transactions is to be conﬁrmed  accepted by everybody , to achieve eventual consistency.  Deﬁnition 8.15  Proof-of-Work . Proof-of-Work  PoW  is a me- chanism that allows a party to prove to another party that a certain amount of computational resources has been utilized for a period of time. A function Fd c, x  → {true, f alse}, where diﬃculty d is a positive number, while challenge c and nonce x are usually bit-strings, is called a Proof-of-Work function if it has following properties:  1. Fd c, x  is fast to compute if d, c, and x are given. 2. For ﬁxed parameters d and c, ﬁnding x such that Fd c, x  = true is computationally diﬃcult but feasible. The diﬃculty d is used to adjust the time to ﬁnd such an x.  Deﬁnition 8.16  Bitcoin PoW function . The Bitcoin PoW func- tion is given by  Fd c, x  → SHA256 SHA256 cx   <  2224  .  d   112 CHAPTER 8. EVENTUAL CONSISTENCY & BITCOIN  Remarks:    This function concatenates the challenge c and nonce x, and hashes them twice using SHA256. The output of SHA256 is a cryptographic hash with a numeric value in {0, . . . , 2256−1} which is compared to a target value 2224 d , which gets smaller with increasing diﬃculty.    SHA256 is a cryptographic hash function with pseudo- random output. No better algorithm is known to ﬁnd a nonce x such that the function Fd c, x  returns true than simply iterating over possible inputs. This is by design to make it diﬃcult to ﬁnd such an input, but simple to verify the validity once it has been found.    If the PoW functions of all nodes had the same challenge, the fastest node would always win. However, as we will see in Deﬁnition 8.19, each node attempts to ﬁnd a valid nonce for a node-speciﬁc challenge.  Deﬁnition 8.17  Block . A block is a data structure used to com- municate incremental changes to the local state of a node. A block consists of a list of transactions, a reference to a previous block and a nonce. A block lists some transactions the block creator  “miner”  has accepted to its memory pool since the previous block. A node ﬁnds and broadcasts a block when it ﬁnds a valid nonce for its PoW function.  Algorithm 8.18 Node Finds Block 1: Nonce x = 0, challenge c, diﬃculty d, previous block bt−1 2: repeat 3: 4: until Fd c, x  = true 5: Broadcast block bt =  memory pool, bt−1, x   x = x + 1  Remarks:    With their reference to a previous block, the blocks build  a tree, rooted in the so called genesis block.    The primary goal for using the PoW mechanism is to adjust the rate at which blocks are found in the net- work, giving the network time to synchronize on the lat-   8.2. BITCOIN  113  est block. Bitcoin sets the diﬃculty so that globally a block is created about every 10 minutes in expectation.   Finding a block allows the ﬁnder to impose the transac- tions in its local memory pool to all other nodes. Upon receiving a block, all nodes roll back any local changes since the previous block and apply the new block’s trans- actions.    Transactions contained in a block are said to be conﬁrmed  by that block.  Deﬁnition 8.19  Reward Transaction . The ﬁrst transaction in a block is called the reward transaction. The block’s miner is re- warded for conﬁrming transactions by allowing it to mint new coins. The reward transaction has a dummy input, and the sum of out- puts is determined by a ﬁxed subsidy plus the sum of the fees of transactions conﬁrmed in the block.  Remarks:    A reward transaction is the sole exception to the rule that the sum of inputs must be at least the sum of outputs.   The number of bitcoins that are minted by the reward transaction and assigned to the miner is determined by a subsidy schedule that is part of the protocol. Initially the subsidy was 50 bitcoins for every block, and it is being halved every 210,000 blocks, or 4 years in expecta- tion. Due to the halving of the block reward, the total amount of bitcoins in circulation never exceeds 21 million bitcoins.    It is expected that the cost of performing the PoW to ﬁnd a block, in terms of energy and infrastructure, is close to the value of the reward the miner receives from the reward transaction in the block.  Deﬁnition 8.20  Blockchain . The longest path from the genesis block, i.e., root of the tree, to a leaf is called the blockchain. The blockchain acts as a consistent transaction history on which all nodes eventually agree.   114 CHAPTER 8. EVENTUAL CONSISTENCY & BITCOIN  Remarks:    The path length from the genesis block to block b is the  height hb.    Only the longest path from the genesis block to a leaf is a valid transaction history, since branches may contradict each other because of doublespends.    Since only transactions in the longest path are agreed upon, miners have an incentive to append their blocks to the longest chain, thus agreeing on the current state.    The mining incentives quickly increased the diﬃculty of the PoW mechanism: initially miners used CPUs to mine blocks, but CPUs were quickly replaced by GPUs, FP- GAs and even application speciﬁc integrated circuits  AS- ICs  as bitcoins appreciated. This results in an equilib- rium today in which only the most cost eﬃcient miners, in terms of hardware supply and electricity, make a proﬁt in expectation.    If multiple blocks are mined more or less concurrently, the system is said to have forked. Forks happen naturally because mining is a distributed random process and two new blocks may be found at roughly the same time.  Algorithm 8.21 Node Receives Block  1: Receive block b 2: For this node the current head is block bmax at height hmax 3: Connect block b in the tree as child of its parent p at height  hb = hp + 1  4: if hb > hmax then 5: 6:  hmax = hb bmax = b Compute UTXO for the path leading to bmax Cleanup memory pool  7: 8: 9: end if   8.3. SMART CONTRACTS  115  Remarks:    Algorithm 8.21 describes how a node updates its local state upon receiving a block. Notice that, like Algo- rithm 8.13, this describes the local policy and may also result in node states diverging, i.e., by accepting diﬀerent blocks at the same height as current head.    Unlike extending the current path, switching paths may result in conﬁrmed transactions no longer being conﬁr- med, because the blocks in the new path do not include them. Switching paths is referred to as a reorg.    Cleaning up the memory pool involves 1  removing trans- actions that were conﬁrmed in a block in the current path, 2  removing transactions that conﬂict with con- ﬁrmed transactions, and 3  adding transactions that were conﬁrmed in the previous path, but are no longer conﬁrm- ed in the current path.    In order to avoid having to recompute the entire UTXO at every new block being added to the blockchain, all cur- rent implementations use data structures that store undo information about the operations applied by a block. This allows eﬃcient switching of paths and updates of the head by moving along the path.  Theorem 8.22. Forks are eventually resolved and all nodes even- tually agree on which is the longest blockchain. The system there- fore guarantees eventual consistency.  Proof. In order for the fork to continue to exist, pairs of blocks need to be found in close succession, extending distinct branches, otherwise the nodes on the shorter branch would switch to the longer one. The probability of branches being extended almost simultaneously decreases exponentially with the length of the fork, hence there will eventually be a time when only one branch is being extended, becoming the longest branch.  8.3 Smart Contracts  Deﬁnition 8.23  Smart Contract . A smart contract is an agree- ment between two or more parties, encoded in such a way that the correct execution is guaranteed by the blockchain.   116 CHAPTER 8. EVENTUAL CONSISTENCY & BITCOIN  Remarks:    Contracts allow business logic to be encoded in Bitcoin transactions which mutually guarantee that an agreed upon action is performed. The blockchain acts as conﬂict mediator, should a party fail to honor an agreement.    The use of scripts as spending conditions for outputs en- ables smart contracts. Scripts, together with some addi- tional features such as timelocks, allow encoding complex conditions, specifying who may spend the funds associ- ated with an output and when.  Deﬁnition 8.24  Timelock . Bitcoin provides a mechanism to make transactions invalid until some time in the future: time- locks. A transaction may specify a locktime: the earliest time, expressed in either a Unix timestamp or a blockchain height, at which it may be included in a block and therefore be conﬁrmed.  Remarks:    Transactions with a timelock are not released into the network until the timelock expires. It is the responsibility of the node receiving the transaction to store it locally until the timelock expires and then release it into the network.    Transactions with future timelocks are invalid. Blocks may not include transactions with timelocks that have not yet expired, i.e., they are mined before their expiry timestamp or in a lower block than speciﬁed. If a block includes an unexpired transaction it is invalid. Upon re- ceiving invalid transactions or blocks, nodes discard them immediately and do not forward them to their peers.    Timelocks can be used to replace or supersede transac- tions: a timelocked transaction t1 can be replaced by another transaction t0, spending some of the same out- puts, if the replacing transaction t0 has an earlier time- lock and can be broadcast in the network before the re- placed transaction t1 becomes valid.  Deﬁnition 8.25  Singlesig and Multisig Outputs . When an out- put can be claimed by providing a single signature it is called a singlesig output. In contrast the script of multisig outputs   8.3. SMART CONTRACTS  117  speciﬁes a set of m public keys and requires k-of-m  with k ≤ m  valid signatures from distinct matching public keys from that set in order to be valid.  Remarks:    Most smart contracts begin with the creation of a 2-of-2 multisig output, requiring a signature from both parties. Once the transaction creating the multisig output is con- ﬁrmed in the blockchain, both parties are guaranteed that the funds of that output cannot be spent unilaterally.  Algorithm 8.26 Parties A and B create a 2-of-2 multisig output o  1: B sends a list IB of inputs with cB coins to A 2: A selects its own inputs IA with cA coins 3: A creates transaction ts{[IA, IB], [o = cA + cB →  A, B ]} 4: A creates timelocked transaction tr{[o], [cA → A, cB → B]}  and signs it  5: A sends ts and tr to B 6: B signs both ts and tr and sends them to A 7: A signs ts and broadcasts it to the Bitcoin network  Remarks:    ts is called a setup transaction and is used to lock in funds into a shared account. If ts is signed and broadcast imme- diately, one of the parties could not collaborate to spend the multisig output, and the funds become unspendable. To avoid a situation where the funds cannot be spent, the protocol also creates a timelocked refund transaction tr which guarantees that, should the funds not be spent before the timelock expires, the funds are returned to the respective party. At no point in time one of the parties holds a fully signed setup transaction without the other party holding a fully signed refund transaction, guaran- teeing that funds are eventually returned.    Both transactions require the signature of both parties. In the case of the setup transaction because it has two inputs from A and B respectively which require individ- ual signatures. In the case of the refund transaction the   118 CHAPTER 8. EVENTUAL CONSISTENCY & BITCOIN  single input spending the multisig output requires both signatures being a 2-of-2 multisig output.  from S  Algorithm 8.27 Simple Micropayment Channel from S to R with capacity c 1: cS = c, cR = 0 2: S and R use Algorithm 8.26 to set up output o with value c 3: Create settlement transaction tf{[o], [cS → S, cR → R]} 4: while channel open and cR < c do In exchange for good with value δ 5: cR = cR + δ 6: cS = cS − δ 7:  8: Update tf with outputs [cR → R, cS → S]  S signs and sends tf to R  9: 10: end while 11: R signs last tf and broadcasts it  Remarks:    Algorithm 8.27 implements a simple micropayment chan- nel, a smart contract that is used for rapidly adjust- ing micropayments from a spender to a recipient. Only two transactions are ever broadcast and inserted into the blockchain: the setup transaction ts and the last settle- ment transaction tf . There may have been any number of updates to the settlement transaction, transferring ever more of the shared output to the recipient.    The number of bitcoins c used to fund the channel is also the maximum total that may be transferred over the simple micropayment channel.    At any time the recipient R is guaranteed to eventually receive the bitcoins, since she holds a fully signed set- tlement transaction, while the spender only has partially signed ones.    The simple micropayment channel is intrinsically unidi- rectional. Since the recipient may choose any of the set- tlement transactions in the protocol, she will use the one with maximum payout for her. If we were to transfer bit- coins back, we would be reducing the amount paid out   8.4. WEAK CONSISTENCY  119  to the recipient, hence she would choose not to broadcast that transaction.  8.4 Weak Consistency  Eventual consistency is only one form of weak consistency. A num- ber of diﬀerent tradeoﬀs between partition tolerance and consis- tency exist in literature.  Deﬁnition 8.28  Monotonic Read Consistency . If a node u has seen a particular value of an object, any subsequent accesses of u will never return any older values.  Remarks:    Users are annoyed if they receive a notiﬁcation about a comment on an online social network, but are unable to reply because the web interface does not show the same notiﬁcation yet. In this case the notiﬁcation acts as the ﬁrst read operation, while looking up the comment on the web interface is the second read operation.  Deﬁnition 8.29  Monotonic Write Consistency . A write opera- tion by a node on a data item is completed before any successive write operation by the same node  i.e., system guarantees to seri- alize writes by the same node .  Remarks:    The ATM must replay all operations in order, otherwise it might happen that an earlier operation overwrites the result of a later operation, resulting in an inconsistent ﬁnal state.  Deﬁnition 8.30  Read-Your-Write Consistency . After a node u has updated a data item, any later reads from node u will never see an older value.  Deﬁnition 8.31  Causal Relation . The following pairs of opera- tions are said to be causally related:    Two writes by the same node to diﬀerent variables.   A read followed by a write of the same node.   120 CHAPTER 8. EVENTUAL CONSISTENCY & BITCOIN    A read that returns the value of a write from any node.   Two operations that are transitively related according to the  above conditions.  Remarks:    The ﬁrst rule ensures that writes by a single node are seen in the same order. For example if a node writes a value in one variable and then signals that it has written the value by writing in another variable. Another node could then read the signalling variable but still read the old value from the ﬁrst variable, if the two writes were not causally related.  Deﬁnition 8.32  Causal Consistency . A system provides causal consistency if operations that potentially are causally related are seen by every node of the system in the same order. Concurrent writes are not causally related, and may be seen in diﬀerent orders by diﬀerent nodes.  Chapter Notes  The CAP theorem was ﬁrst introduced by Fox and Brewer [FB99], although it is commonly attributed to a talk by Eric Brewer [Bre00]. It was later proven by Gilbert and Lynch [GL02] for the asynchro- nous model. Gilbert and Lynch also showed how to relax the con- sistency requirement in a partially synchronous system to achieve availability and partition tolerance.  Bitcoin was introduced in 2008 by Satoshi Nakamoto [Nak08]. Nakamoto is thought to be a pseudonym used by either a single person or a group of people; it is still unknown who invented Bit- coin, giving rise to speculation and conspiracy theories. Among the plausible theories are noted cryptographers Nick Szabo [Big13] and Hal Finney [Gre14]. The ﬁrst Bitcoin client was published shortly after the paper and the ﬁrst block was mined on January 3, 2009. The genesis block contained the headline of the release date’s The Times issue “The Times 03 Jan 2009 Chancellor on brink of second bailout for banks”, which serves as proof that the genesis block has been indeed mined on that date, and that no one had mined before that date. The quote in the genesis block is also thought to be an ideological hint: Bitcoin was created in a climate   BIBLIOGRAPHY  121  of ﬁnancial crisis, induced by rampant manipulation by the bank- ing sector, and Bitcoin quickly grew in popularity in anarchic and libertarian circles. The original client is nowadays maintained by a group of independent core developers and remains the most used client in the Bitcoin network.  Central to Bitcoin is the resolution of conﬂicts due to double- spends, which is solved by waiting for transactions to be included in the blockchain. This however introduces large delays for the conﬁrmation of payments which are undesirable in some scenar- ios in which an immediate conﬁrmation is required. Karame et al. [KAC12] show that accepting unconﬁrmed transactions leads to a non-negligible probability of being defrauded as a result of a doublespending attack. This is facilitated by information eclips- ing [DW13], i.e., that nodes do not forward conﬂicting transactions, hence the victim does not see both transactions of the doublespend. Bamert et al. [BDE+13] showed that the odds of detecting a dou- blespending attack in real-time can be improved by connecting to a large sample of nodes and tracing the propagation of transactions in the network.  Bitcoin does not scale very well due to its reliance on conﬁr- mations in the blockchain. A copy of the entire transaction his- tory is stored on every node in order to bootstrap joining nodes, which have to reconstruct the transaction history from the genesis block. Simple micropayment channels were introduced by Hearn and Spilman [HS12] and may be used to bundle multiple trans- fers between two parties but they are limited to transferring the funds locked into the channel once. Recently Duplex Micropayment Channels [DW15] and the Lightning Network [PD15] have been proposed to build bidirectional micropayment channels in which the funds can be transferred back and forth an arbitrary number of times, greatly increasing the ﬂexibility of Bitcoin transfers and enabling a number of features, such as micropayments and routing payments between any two endpoints.  Bibliography  [BDE+13] Tobias Bamert, Christian Decker, Lennart Elsen, Samuel Welten, and Roger Wattenhofer. Have a snack, pay with bitcoin. In IEEE Internation Conference on Peer-to-Peer Computing  P2P , Trento, Italy, 2013.  [Big13] John Biggs. Who is the real satoshi nakamoto?   122 CHAPTER 8. EVENTUAL CONSISTENCY & BITCOIN  researcher may  one http:  on.tcrn.ch l R0vA, 2013.  have  found  the  answer.  [Bre00] Eric A. Brewer. Towards robust distributed systems. In Symposium on Principles of Distributed Computing  PODC . ACM, 2000.  [DW13] Christian Decker and Roger Wattenhofer. Information propagation in the bitcoin network. In IEEE Interna- tional Conference on Peer-to-Peer Computing  P2P , Trento, Italy, September 2013.  [DW15] Christian Decker and Roger Wattenhofer. A Fast and Scalable Payment Network with Bitcoin Duplex Mi- cropayment Channels. In Symposium on Stabilization, Safety, and Security of Distributed Systems  SSS , 2015.  [FB99] Armando Fox and Eric Brewer. Harvest, yield, and scalable tolerant systems. In Hot Topics in Operating Systems. IEEE, 1999.  [GL02] Seth Gilbert and Nancy Lynch. Brewer’s conjecture and the feasibility of consistent, available, partition-tolerant web services. SIGACT News, 2002.  [Gre14] Andy Greenberg. Nakamoto’s neighbor: My hunt for bitcoin’s creator led to a paralyzed crypto genius. http:  onforb.es 1rvyecq, 2014.  [HS12] Mike Hearn  Jeremy  Spilman.  and Rapidly  tract: https:  en.bitcoin.it wiki Contract, accessed on November 11, 2015.  adjusting  Con- micro-payments. 2012. Last  [KAC12] G.O. Karame, E. Androulaki, and S. Capkun. Two Bit- coins at the Price of One? Double-Spending Attacks on Fast Payments in Bitcoin. In Conference on Computer and Communication Security  CCS , 2012.  [Nak08] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic  cash system. https:  bitcoin.org bitcoin.pdf, 2008.  [PD15] Joseph Poon and Thaddeus Dryja. The bitcoin light-  ning network. 2015.   Chapter 9  Inside Bitcoin  Bitcoin features various concepts that go beyond the blockchain centric discussion of Chapter 8. In this chapter we discuss some of Bitcoin’s other interesting features.  9.1 Cryptographic Tools  The Bitcoin protocol limits its use of cryptographic tools to crypto- graphic hash functions such as SHA256 and RIPEMD160, Merkle trees, and the Elliptic Curve Digital Signature Algorithm.  Deﬁnition 9.1  Cryptographic hash functions . Cryptographic hash functions map an arbitrarily long input byte sequence to a ﬁxed size output, commonly called digest, eﬀectively ﬁngerprinting the input.  Remarks:    Cryptographic hashes are designed such that it is com- putationally infeasible to recreate a valid input sequence.   Ideal cryptographic hash functions are computationally inexpensive, and the output hash always changes when the input sequence is altered.    Cryptographic hash functions are widely used in Bitcoin, e.g., the id of a transaction corresponds to the crypto- graphic hash of the transaction.  123   124  CHAPTER 9.  INSIDE BITCOIN    Hash functions are a base component of other data struc-  tures used in Bitcoin, e.g. Merkle trees.  Deﬁnition 9.2  Merkle Tree . Merkle trees allow to combine mul- tiple cryptographic hash input sequences in a hash tree.  Remarks:    A Merkle tree allows for a compact representation of a set of transactions, e.g., when the tree is built up from the transaction hashes, see Figure 9.3.  Figure 9.3: Merkle tree of transactions. The topmost hash is re- ferred to as merkle root. It is a compact representation of the involved transactions.  9.2 Script & Message Formats  Deﬁnition 9.4  Script . Bitcoin uses a custom stack-based script- ing language simply called Script in an attempt to allow diﬀerent types of transactions.  Remarks:    Script’s ﬂexibility allows extending the functionality of  transactions beyond the simple transfer of funds.    Script is stack based, supports many functions  com- monly referred to as opcodes  and either evaluates to true or false. Script supports dozens of diﬀerent op- codes, ranging from simple comparison opcodes to cryp- tographic hash functions and signature veriﬁcation.  Hash  Hash  Hash  Hash  Hash  Hash  Hash  Transaction  Transaction  Transaction  Transaction   9.2. SCRIPT & MESSAGE FORMATS  125    Because Script is supposed to be executed on any vali- dating Bitcoin node, execution time is critical to prevent denial of service attacks. Therefore, many opcodes have been temporarily disabled. Consequently, Script is kept simple on purpose, and therefore does not support the same complexity as general purpose programming lan- guages.    An example Script program contains two constants  de- noted by    and one opcode  execution goes from the left to the right :      OP CHECKSIG.  Constants are pushed by default on the stack, and upon execution, the stack contains     .  Then, OP CHECKSIG is executed which veriﬁes the <sig- nature> under the provided  . If the sig- nature matches the provided public key, OP CHECKSIG re- turns true, and in return, the script returns true.  Deﬁnition 9.5  Transaction Format . Transactions as introduced in Deﬁnition 8.12 must obey a certain format.  Remarks:    Table 9.6 summarizes the general format of a Bitcoin  transaction.    Tables 9.7 and 9.8 respectively describe the transaction  input and output format.  Field  Description  Version number Version, currently 1 Input counter List of inputs Output counter List of outputs locktime  positive integer see Table regarding transaction inputs positive integer see Table regarding transaction inputs Block height or time when transaction is valid  Size  4 bytes 1 - 9 bytes Variable 1 - 9 bytes Variable 4 bytes  Table 9.6: Transaction format inside a Bitcoin block.   126  Field  CHAPTER 9.  INSIDE BITCOIN  Description  Size  Previous transaction hash Previous transaction output index Dependency index Script length ScriptSig Sequence number  32 bytes 4 bytes 1 - 9 bytes Input script Variable generally 0xFFFFFFFF 4 bytes  Dependency  Table 9.7: Transaction input format inside a Bitcoin block.  Field  Description  value Script length ScriptSig  Output script  positive integer of Satoshis to be transferred  Size  4 bytes 1 - 9 bytes Variable  Table 9.8: Transaction output format inside a Bitcoin block.  Deﬁnition 9.9  Standard Transaction Types . Bitcoin supports by default several standard transaction types.  Remarks:    Only standard transaction types are broadcasted and val- idated within the network. Transactions that do not match standard transaction types are generally discarded.   Because transactions can have multiple outputs, the dif- ferent output types can be combined within one transac- tion.    Here are the standard transaction types:   Pay To Public Key Hash  P2PKH : A P2PKH transac-  tion output contains the following opcodes:  OP DUP OP HASH160   OP EQUALVERIFY OP CHECKSIG  The corresponding input that would be eligible to spend the output, speciﬁes the required signature and the full public key:        Pay To Script Hash  P2SH : A P2SH transaction out- put can only be redeemed by an input that provides a script, that hashes to the hash of the corresponding out- put. A P2SH output for example contains the following transaction output:   9.2. SCRIPT & MESSAGE FORMATS  127  OP HASH160   OP EQUALVERIFY  The redeeming input consequently needs to provide a re- deemScript, that hashes to the input’s hash. Every stan- dard Script can be used for this purpose:       P2SH allows to create a transaction where the respon- sibility for providing the redeem conditions of a trans- action is pushed from the sender to the redeemer of the funds. Consequently the sender is not required to pay an excess in transaction fees, if the redeem script happens to be complex and thus big in terms of bytes. In prac- tice P2SH outputs are heavily used for multi-signature  multisig  transactions, but multi-signatures can both be accomplished with m-of-n output scripts as well as P2SH.   Multisig: A multi-signature  or commonly referred to as multisig  transaction, requires multiple signatures in order to be redeemable, c.g. Deﬁnition 8.25. Multisig transaction outputs are usually denoted as m-of-n, m be- ing the minimum number of signatures that are required for the transaction output to be redeemable, out of the n possible signatures that correspond to the public keys de- ﬁned in the transaction output. An example transaction output corresponds to:      [B pubkey] [C pubkey..]   OP CHECKMULTISIG  while the redeeming input follows this structure:  OP 0   [B signature] [C signature..]  Deﬁnition 9.10  Script Execution . To validate a new transac- tion, the input  signature script  and the output of the previous transaction  pubkey script  are concatenated. Once concatenated, the script is executed according to the Script language. Constants, denoted by   are pushed on the stack, and opcodes execute their respective actions by taking into account the topmost stack value.   128  CHAPTER 9.  INSIDE BITCOIN  Figure 9.11: Script execution for a P2PKH transaction called Transaction 2: The output and input script are concatenated  Sig- nature script ﬁrst and then the PubKey script . In a ﬁrst step, the two constants     are pushed onto the stack. Then OP DUP duplicates the top-most stack value,   in this case. The next opcode OP HASH160 hashes the   and saves it as   on the stack. The constant   is pushed onto the stack and OP EQUALVERIFY veriﬁes if the two top- most stack elements are equal. If they are equal, they are removed from the stack, and the last opcode OP CHECKSIG veriﬁes if the pub- lic key on the stack     matches the signature    . If the signature is valid, the script returns true, meaning that the input of transaction 2 is allowed to spent the output 1 of Transac- tion 1.  Remarks:  tion.    In Figure 9.11, we visualize the validation of a transac-  PubKey Script  @Z  @A @B  Transaction 1 Script:  OP_DUP OP_HASH160   OP_EQUALVERIFY OP_CHECKSIG           OP_DUP OP_HASH160   OP_EQUALVERIFY OP_CHECKSIG  Signature Script  @A  @B @C  Transaction 2      Execution stack Execution code           Execution stack Execution code             Execution stack Execution code         Execution stack Execution code     True   9.3. PLAYERS  129  Deﬁnition 9.12  Block Format . Blocks as introduced in Deﬁni- tion 8.17 must obey a certain format.  Remarks:    Each block header has a speciﬁc set of ﬁelds that we are  listed in Table 9.13.    The block itself contains the actual transactions.  Field  Description  Block version number  Version Hash of previous block Hash of previous Block header Transaction Merkle root hash Merkle root hash Unix timestamp Time Diﬃculty as in Algorithm 8.18 Bits Nonce PoW as in Algorithm 8.18  Size  4 bytes 32 bytes 32 bytes 4 bytes 4 bytes 4 bytes  Table 9.13: Bitcoin block header format.  9.3 Players  The Bitcoin ecosystem emerged over the need to provide diﬀerent services to diﬀerent participating players depending on their avail- able resources. We describe in the following the various players  also known as node types  and how they interoperate.  Deﬁnition 9.14  Miner . Miners perform the Proof-of-Work  Def- inition 8.15  to ﬁnd and broadcast blocks in the Bitcoin network.  Remarks:    Miners must quickly retrieve information about the new- est blocks, validate transactions that are included in new blocks, operate dedicated mining hardware to perform as many hash operations as possible, and eﬃciently spread found blocks to the whole network.    As we discussed in Deﬁnition 8.19, every block provides  a monetary reward.   130  CHAPTER 9.  INSIDE BITCOIN    Because a block is found on average only every 10 min- utes, it may take a long time until a miner receives a payout. Miners therefore typically organize themselves into groups of miners, commonly referred to as mining pools. Because of its higher overall hashing power, a mining pool has a higher chance to ﬁnd a block, and mining pool members can consequently receive payouts more periodically than an independent miner.  Deﬁnition 9.15  Full Node . A full node is a node that  i  main- tains the full copy of the blockchain,  ii  validates incoming trans- actions and blocks, and  iii  forwards transactions and blocks to its peer nodes.  Remarks:    In addition to providing validation services to the Bitcoin network, a full node might provide an open TCP port  Bitcoin uses the TCP port 8333 , where other Bitcoin nodes can connect to.  Deﬁnition 9.16  Lightweight Client . A lightweight client does not maintain the full Bitcoin blockchain, but rather follows the co- called Simple Payment Veriﬁcation  SPV  scheme.  Remarks:    The SPV scheme allows the lightweight client to verify that a transaction has been included in the blockchain, by only receiving the block headers. In order to ﬁnd such transactions quickly, they often use so-called Bloom ﬁlters.    Lightweight clients do not receive transactions that are ir- relevant to their operation, do not need to perform trans- action or block validation and consequently require sig- niﬁcantly fewer resources to operate than full nodes or miners.  Deﬁnition 9.17  Web Wallet . A web wallet is an online wallet, hosted on a remote server and accessible through a website.   9.3. PLAYERS  Remarks:  131    A full node installation takes a serious amount of disk space and requires several hours in order to download and index the current blockchain. Therefore, users started relying on centralised services that host the main Bitcoin functionalities, called web wallets.    A web wallet is instantly functional, does not occupy hard disk space, is accessible anywhere and is consequently more convenient than a local Bitcoin client.    However, a malicious web wallet operator could poten- tially have access to the funds of a user, and  i  steal or forward stolen Bitcoins,  ii  trade with non-used funds, or  iii  proﬁle users.  Chapter Notes  While Bitcoin mostly focuses on the ability to provide a decentral- ized cryptocurrency, alternative PoW blockchains such as Ethereum [But14] allow executing smart contracts described in a Turing- complete programming language that run on top of the blockchain. For a comprehensive overview on cryptocurrencies, we refer to Bon- neau et al. [BMC+15].  Several papers analyze double-spending attacks in Bitcoin, e.g., [Ros14]. Bamert et al. [BDE+13] compile countermeasures to de- tect double spending attacks. In [KAC12], Karame et al. inves- tigate double-spending attacks of fast payments  payments that have not been conﬁrmed in the blockchain  in Bitcoin, and show that double-spending of fast payments can be performed in spite of the measures recommended by Bitcoin developers. Barber et al. [BBSU12] analyze possible ways to enhance the resilience of Bitcoin against security threats.  Eyal and Sirer [ES14] show that a selﬁsh miner can increase its relative mining revenue by not directly publishing his blocks. Similarly, Courtois and Bahack [CB14] study subversive mining strategies. In [NKMS15], Nayak et al. combine selﬁsh mining and eclipse attacks, without considering optimal adversarial strategies. Sapirshtein et al. [SSZ15] devise optimal adversarial strategies for selﬁsh mining in Bitcoin.  Croman et al. [CDE+16] discuss the scalability limitations of Bitcoin, but do not quantify the security implications of smaller   132  CHAPTER 9.  INSIDE BITCOIN  block intervals or bigger blocks on the security of the system. Sev- eral works [GKL15, CDE+16] analyse the security of Bitcoin’s pro- tocol in the synchronous network model.  Bitcoin’s blockchain forms as a chain, i.e., each block  except the genesis block  has a parent block. The longest chain with the highest diﬃculty is considered the main chain. GHOST [SZ15] is an alternative to the longest chain rule for establishing consensus in PoW based blockchains and aims to alleviate adverse impacts of stale blocks.  Most security-related studies assume that nodes directly receive the information disseminated in the Bitcoin network. In [DW13], Decker et al. connect to a subset of the Bitcoin network, and measure the propagation delay of blocks. Miller et al. [MLP+15] try to discover Bitcoin’s topology by exploiting, for instance, a 2-minute request timeout for transactions.  Open decentralized PoW blockchain’s security relies on the as- sumption that most participating nodes receive all transaction and block information nearly simultaneously, i.e., there is a tight syn- chronization. Eclipse attacks aim to partition the network into 2 or more clusters, such that the synchronization is no longer pos- sible. Heilman’s et al. were the ﬁrst to show eclipse attacks on Bitcoin [HKZG15]. The authors showed that by monopolizing the connections of nodes in the system  an expensive strategy , an ad- versary could perform selﬁsh mining and abuse Bitcoin’s consensus protocol.  A number of contributions focus on the privacy aspects of PoW blockchains. Although nodes transfer funds among pseudonyms  addresses , payments are linkable such that the origin of a pay- ment is traceable at any time. Transaction prices and time of pay- ment are also published and stored persistently in the blockchain. Miers et al. introduced in [MGGR13] ZeroCoin, a cryptographic extension to Bitcoin that augments the protocol to prevent the tracing of coin expenditure. In [AK14], Androulaki and Karame proposed an extension of ZeroCoin to hide the transaction values and address balances in the system. In [Eli11], Elias investigates the legal aspects of privacy in Bitcoin. Reid and Harrigan [RH13] analyze the ﬂow of Bitcoin transactions in a small part of Bitcoin log. In [AKC13], Androulaki et al. evaluate user privacy in Bit- coin and show that Bitcoin leaks considerable information about the proﬁles of users. More speciﬁcally, the authors show that in a typical university setting, even when users adopt privacy measures, proﬁles of almost 40% of the users are recoverable. In [RS13], Ron   9.3. PLAYERS  133  and Shamir analyze the behavior of Bitcoin users. In [OKH13], Ober et al. study the time-evolution properties of Bitcoin by an- alyzing its transaction graph. Decker et al. [DW14] investigate transaction malleability and the Mt. Gox incident. In [MPJ+16], Meiklejohn et al. identify big players in the Bitcoin system by leveraging Heuristics I and II adapted from [KAR+15, AKC13]. The authors perform transactions with big vendors such as Mt. Gox. and use our heuristics to identify clusters of addresses of such merchants. In [KKM14], the authors investigated the possibility of linking addresses of the same user by utilizing the network address information  IPs .  As far as we are aware, Mullin et al. [Mul83] were the ﬁrst to propose an estimate of the false positive rate of Bloom ﬁlters. In [CRJ10], Christensen et al. propose a novel technique for com- puting the false positive rate, which results in tighter estimates when compared to [Mul83].  In [BBL12], Bianchi et al. quantify the privacy properties of Bloom ﬁlters; their analysis, however, does not address the privacy provisions when the adversary has access to multiple Bloom ﬁlters originating from the same entity. In [NK09], Nojima et al., pro- pose a cryptographically secure privacy-preserving Bloom-ﬁltering protocol based on blind signatures; this proposal, however, incurs additional computational load on SPV nodes. Private Information Retrievals  e.g., [Ker12]   PIRs  can also be used as an alternative to Bloom ﬁlters; PIR schemes, however, result in the non-negligible computational overhead on the nodes. For example, Bloom ﬁlters are used, e.g., to enhance the privacy of document search [BC07], design privacy-preserving record linkage [SBR09], or obfuscate the schemes of database tables from curious administrators [WA09].  In [BDOZ12], Babaioﬀ et al. address the lack of incentives for Bitcoin users to include recently announced transactions in a block. Furthermore, in [SS11], Syed et al. propose a user-friendly technique for managing Bitcoin wallets. In [MC13], Moore and Christin study the economic risks that investors face due to Bitcoin exchanges. Clark et al. [CE12] propose the use of the Bitcoin PoW to construct veriﬁable commitment schemes.  Many PoW alternatives have been proposed. In Proof of Stake  PoS  [KN12], the voting power of nodes is based on the amount of “stake” they own in the respective blockchain system. Proof of Burn  PoB  is a proposal to replace PoW by burning trans- action outputs, such that they can no longer be spent. Existing PoB-based blockchains, however, rely on PoW to create blocks and   134  CHAPTER 9.  INSIDE BITCOIN  therefore ultimately rely on PoW for coin creation. Proof of Capac- ity  PoC , aims to use the available hard-disk space to replace PoW. Bitcoin-NG [EGSvR15] performs leader election of PoW—allowing the leader to sign micro-blocks until a new leader is elected. The literature features many additional proposals [KKJG+16, Vuk15] that rely on traditional Byzantine fault tolerant consensus proto- cols in the hope to increase the consensus eﬃciency and achieve high transactional throughput. Recent studies propose to combine the use of PoW with BFT protocols to realize highly-performant open consensus protocols  Byzcoin [KKJG+16] .  The Bitcoin protocol uses the following cryptographic concepts: SHA256 [NIS13] and RIPEMD160 [DBP96], Merkle trees [Mer82] and the Elliptic Curve Digital Signature Algorithm  ECDSA  [Kob87].  This chapter is based on a text by Arthur Gervais.  Bibliography  [AK14] Elli Androulaki and Ghassan Karame. Hiding transac- tion amounts and balances in bitcoin. In Proceedings of TRUST, 2014.  [AKC13] Elli Androulaki, Ghassan Karame, and Srdjan Cap- kun. Evaluating user privacy in bitcoin. In Financial Cryptography, 2013.  [BBL12] Giuseppe Bianchi, Lorenzo Bracciale, and Pierpaolo Loreti. Better than nothing privacy with bloom ﬁlters: To what extent? In Privacy in Statistical Databases, pages 348–363. Springer, 2012.  [BBSU12] S. Barber, X. Boyen, E. Shi, and E. Uzun. Bitter to Better - How to Make Bitcoin a Better Currency. In Financial Cryptography and Data Security, 2012.  [BC07] Steven Michael Bellovin and William R Cheswick. Privacy-enhanced searches using encrypted bloom ﬁl- ters. 2007.  [BDE+13] Tobias Bamert, Christian Decker, Lennart Elsen, Samuel Welten, and Roger Wattenhofer. Have a snack, pay with bitcoin. In IEEE Internation Conference on Peer-to-Peer Computing  P2P , Trento, Italy, 2013.   BIBLIOGRAPHY  135  [BDOZ12] M. Babaioﬀ, S. Dobzinski, S. Oren, and A. Zohar. On Bitcoin and Red Balloons. 2012. In Proceedings of the ACM Conference on Electronic Commerce  EC .  [BMC+15] Joseph Bonneau, Andrew Miller, Jeremy Clark, Arvind Narayanan, Joshua A. Kroll, and Edward W. Felten. SoK: Research Perspectives and Challenges for Bitcoin and Cryptocurrencies. In 2015 IEEE Sympo- sium on Security and Privacy, May 2015.  [But14] V. Buterin. A next-generation smart contract and  decentralized application platform, 2014.  [CB14] Nicolas T. Courtois and Lear Bahack. On subversive miner strategies and block withholding attack in bit- coin digital currency. CoRR, abs 1402.1718, 2014.  [CDE+16] Kyle Croman, Christian Decker, Ittay Eyal, Adem Efe Gencer, Ari Juels, Ahmed E. Kosba, Andrew Miller, Prateek Saxena, Elaine Shi, Emin G¨un Sirer, Dawn Song, and Roger Wattenhofer. On scaling decentral- ized blockchains. In Financial Cryptography and Data Security, 2016.  [CE12] J. Clark and A. Essex.  Short Paper  CommitCoin: Carbon Dating Commitments with Bitcoin. In Finan- cial Cryptography and Data Security, 2012.  [CRJ10] Ken Christensen, Allen Roginsky, and Miguel Jimeno. A new analysis of the false positive rate of a bloom ﬁlter. Information Processing Letters, 110 21 :944– 949, 2010.  [DBP96] Hans Dobbertin, Antoon Bosselaers, and Bart Pre- neel. RIPEMD-160: A strengthened version of RIPEMD. In Fast Software Encryption, Third Inter- national Workshop, Cambridge, UK, February 21-23, 1996, Proceedings, pages 71–82, 1996.  [DW13] Christian Decker and Roger Wattenhofer. Information propagation in the bitcoin network. In IEEE Interna- tional Conference on Peer-to-Peer Computing  P2P , Trento, Italy, September 2013.   136  CHAPTER 9.  INSIDE BITCOIN  [DW14] Christian Decker and Roger Wattenhofer. Bitcoin transaction malleability and mtgox. In Computer Se- curity - ESORICS 2014 - 19th European Symposium on Research in Computer Security, Wroclaw, Poland, September 7-11, 2014. Proceedings, Part II, pages 313–326, 2014.  [EGSvR15] Ittay Eyal, Adem Efe Gencer, Emin Gun Sirer, and Robbert van Renesse. Bitcoin-ng: A scalable blockchain protocol. arXiv preprint arXiv:1510.02037, 2015.  [Eli11] M. Elias. Bitcoin: Tempering the Digital Ring of  Gyges or Implausible Pecuniary Privacy, 2011.  [ES14] Ittay Eyal and Emin G¨un Sirer. Majority is not enough: Bitcoin mining is vulnerable. In Finan- cial Cryptography and Data Security, pages 436–454. Springer, 2014.  [GKL15] Juan Garay, Aggelos Kiayias, and Nikos Leonardos. The bitcoin backbone protocol: Analysis and appli- cations. In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pages 281–310. Springer, 2015.  [HKZG15] E. Heilman, A. Kendler, A. Zohar, and S. Gold- berg. Eclipse attacks on bitcoin’s peer-to-peer net- work. 2015.  [KAC12] G.O. Karame, E. Androulaki, and S. Capkun. Two Bitcoins at the Price of One? Double-Spending At- tacks on Fast Payments in Bitcoin. In Conference on Computer and Communication Security  CCS , 2012.  [KAR+15] Ghassan O Karame, Elli Androulaki, Marc Roeschlin, Arthur Gervais, and Srdjan ˇCapkun. Misbehavior in bitcoin: a study of double-spending and accountabil- ity. ACM Transactions on Information and System Security  TISSEC , 18 1 :2, 2015.  [Ker12] Florian Kerschbaum. Outsourced private set intersec- tion using homomorphic encryption. In Proceedings of the 7th ACM Symposium on Information, Computer and Communications Security, pages 85–86. ACM, 2012.   BIBLIOGRAPHY  137  [KKJG+16] Eleftherios Kokoris-Kogias, Philipp Jovanovic, Nico- las Gailly, Ismail Khoﬃ, Linus Gasser, and Bryan Ford. Enhancing bitcoin security and performance with strong consistency via collective signing. arXiv preprint arXiv:1602.06997, 2016.  [KKM14] Philip Koshy, Diana Koshy, and Patrick McDaniel. An analysis of anonymity in bitcoin using p2p network traﬃc. In Financial Cryptography, 2014.  [KN12] Sunny King and Scott Nadal. Ppcoin: Peer-to-peer crypto-currency with proof-of-stake. self-published pa- per, August, 19, 2012.  [Kob87] Neal Koblitz. Elliptic curve cryptosystems. Mathe-  matics of computation, 48 177 :203–209, 1987.  [MC13] Tyler Moore and Nicolas Christin. Beware the mid- dleman: Empirical analysis of bitcoin-exchange risk. In Financial Cryptography and Data Security, pages 25–33, 2013.  [Mer82] R.C. Merkle. Method of providing digital signatures,  January 5 1982. US Patent 4,309,569.  [MGGR13] Ian Miers, Christina Garman, Matthew Green, and Aviel D Rubin. Zerocoin: Anonymous distributed e- cash from bitcoin. In Security and Privacy  SP , 2013 IEEE Symposium on, pages 397–411. IEEE, 2013.  [MLP+15] Andrew Miller, James Litton, Andrew Pachulski, Neal Gupta, Dave Levin, Neil Spring, and Bobby Bhat- tacharjee. Discovering bitcoin’s public topology and inﬂuential nodes, 2015.  [MPJ+16] Sarah Meiklejohn, Marjori Pomarole, Grant Jordan, Kirill Levchenko, Damon McCoy, Geoﬀrey M. Voelker, and Stefan Savage. A ﬁstful of bitcoins: characteriz- ing payments among men with no names. Commun. ACM, 59 4 :86–93, 2016.  [Mul83] James K Mullin. A second look at bloom ﬁlters. Com-  munications of the ACM, 26 8 :570–571, 1983.   138  CHAPTER 9.  INSIDE BITCOIN  [NIS13] NIST.  Sha 256, 2013. Available from: https:    web.archive.org web 20130526224224 http:   csrc.nist.gov groups STM cavp documents  shs sha256-384-512.pdf.  [NK09] Ryo Nojima and Youki Kadobayashi. Cryptographi- cally secure bloom-ﬁlters. Transactions on Data Pri- vacy, 2 2 :131–139, 2009.  [NKMS15] Kartik Nayak, Srijan Kumar, Andrew Miller, and Elaine Shi. Stubborn mining: Generalizing selﬁsh mining and combining with an eclipse attack. Tech- nical report, IACR Cryptology ePrint Archive 2015, 2015.  [OKH13] Micha Ober,  Stefan Katzenbeisser,  and Kay Hamacher. Structure and anonymity of the bitcoin transaction graph. Future Internet, 5 2 :237–250, 2013.  [RH13] Fergal Reid and Martin Harrigan. An analysis of anonymity in the bitcoin system. In Yaniv Altshuler, Yuval Elovici, Armin B. Cremers, Nadav Aharony, and Alex Pentland, editors, Security and Privacy in Social Networks, pages 197–223. Springer New York, 2013.  [Ros14] Meni Rosenfeld. Analysis of hashrate-based double  spending. arXiv preprint arXiv:1402.2009, 2014.  [RS13] Dorit Ron and Adi Shamir. Quantitative analysis of the full bitcoin transaction graph. In Financial Cryp- tography and Data Security, pages 6–24, 2013.  [SBR09] Rainer Schnell, Tobias Bachteler, and J¨org Reiher. Privacy-preserving record linkage using bloom ﬁl- ters. BMC Medical Informatics and Decision Making, 9 1 :41, 2009.  [SS11] O. Syed and A. Syed. Bitcoin Gateway, A Peer-to-peer  Bitcoin Vault and Payment Network, 2011.  [SSZ15] Ayelet Sapirshtein, Yonatan Sompolinsky, and Aviv Zohar. Optimal selﬁsh mining strategies in bitcoin. arXiv preprint arXiv:1507.06183, 2015.   BIBLIOGRAPHY  139  [SZ15] Yonatan Sompolinsky and Aviv Zohar. Secure high- rate transaction processing in bitcoin. In Finan- cial Cryptography and Data Security, pages 507–527. Springer, 2015.  [Vuk15] Marko Vukolic. The quest for scalable blockchain fab- ric: Proof-of-work vs. bft replication. In Proceedings of the IFIP WG 11.4 Workshop iNetSec 2015. 2015.  [WA09] Chiemi Watanabe and Yuko Arai. Privacy-preserving queries for a das model using encrypted bloom ﬁlter. In Database systems for advanced applications, pages 491–495. Springer, 2009.   140  CHAPTER 9.  INSIDE BITCOIN   Chapter 10  Distributed Storage  How do you store 1M movies, each with a size of about 1GB, on 1M nodes, each equipped with a 1TB disk? Simply store the movies on the nodes, arbitrarily, and memorize  with a global index  which movie is stored on which node. What if the set of movies or nodes changes over time, and you do not want to change your global index too often?  10.1 Consistent Hashing  Several variants of hashing will do the job, e.g. consistent hashing:  Algorithm 10.1 Consistent Hashing  1: Hash the unique ﬁle name of each movie x with a known set of  hash functions hi x  → [0, 1 , for i = 1, . . . , k  2: Hash the unique name  e.g., IP address and port number  for 3: Store a copy of movie x on node u if hi x  ≈ hi u , for any i.  of each node with the same set of hash functions hi, i = 1, . . . , k  More formally, store movie x on node u if  hi x  − hi u  = min  {hi m  − hi u }, for any pair i, u  m  141   142  CHAPTER 10. DISTRIBUTED STORAGE  Theorem 10.2  Consistent Hashing . In expectation, each node in Algorithm 10.1 stores km n movies.  Proof. For a speciﬁc movie  out of m  and a speciﬁc hash function  out of k , all n nodes have the same probability 1 n to hash closest to the movie hash. By linearity of expectation, each node stores km n movies, in expectation.  Remarks:    Let us do a back-of-the-envelope calculation. We have m = 1M movies, n = 1M nodes, each node has storage for 1TB 1GB = 1K movies, i.e., we use k = 1K hash functions. Theorem 10.2 shows each node stores about 1K movies.    Using the Chernoﬀ bound below with µ = km n = 1K, the probability that a node uses 10% more memory than expected is less than 1%.  Facts 10.3. A version of a Chernoﬀ bound states the following: Let x1, . . . , xn be independent Bernoulli-distributed random vari- ables with P r[xi = 1] = pi and P r[xi = 0] = 1 − pi = qi, then for X :=   cid:80 n i=1 xi and µ := E[X] = cid:80 n  i=1 pi the following holds:   cid:18    cid:19 µ  for any δ > 0: P r[X ≥  1 + δ µ] <  eδ   1 + δ  1+δ   Remarks:    Instead of storing movies directly on nodes as in Algo- rithm 10.1, we can also store the movies on any nodes we like. The nodes of Algorithm 10.1 then simply store forward pointers to the actual movie locations.    In this chapter we want to push unreliability to the ex- treme. What if the nodes are so unreliable that on aver- age a node is only available for 1 hour? In other words, nodes exhibit a high churn, they constantly join and leave the distributed system.    With such a high churn, hundreds or thousands of nodes will change every second. No single node can have an accurate picture of what other nodes are currently in the   10.2. HYPERCUBIC NETWORKS  143  system. This is remarkably diﬀerent to classic distributed systems, where a single unavailable node may already be a minor disaster: all the other nodes have to get a consistent view  Deﬁnition 6.4  of the system again. In high churn systems it is impossible to have a consistent view at any time.    Instead, each node will just know about a small subset of 100 or less other nodes  “neighbors” . This way, nodes can withstand high churn situations.    On the downside, nodes will not directly know which node is responsible for what movie. Instead, a node searching for a movie might have to ask a neighbor node, which in turn will recursively ask another neighbor node, until the correct node storing the movie  or a forward pointer to the movie  is found. The nodes of our distrib- uted storage system form a virtual network, also called an overlay network.  10.2 Hypercubic Networks  In this section we present a few overlay topologies of general inter- est.  Deﬁnition 10.4  Topology Properties . Our virtual network should have the following properties:    The network should be  somewhat  homogeneous: no node should play a dominant role, no node should be a single point of failure.    The nodes should have IDs, and the IDs should span the universe [0, 1 , such that we can store data with hashing, as in Algorithm 10.1.    Every node should have a small degree, if possible polylog- arithmic in n, the number of nodes. This will allow every node to maintain a persistent connection with each neighbor, which will help us to deal with churn.    The network should have a small diameter, and routing should be easy. If a node does not have the information about a data item, then it should know which neighbor to ask.   144  CHAPTER 10. DISTRIBUTED STORAGE  Within a few  polylogarithmic in n  hops, one should ﬁnd the node that has the correct information.  Figure 10.5: The structure of a fat tree.  Remarks:    Some basic network topologies used in practice are trees, rings, grids or tori. Many other suggested networks are simply combinations or derivatives of these.    The advantage of trees is that the routing is very easy: for every source-destination pair there is only one path. However, since the root of a tree is a bottleneck, trees are not homogeneous. Instead, so-called fat trees should be used. Fat trees have the property that every edge connecting a node v to its parent u has a capacity that is proportional to the number of leaves of the subtree rooted at v. See Figure 10.5 for a picture.    Fat trees belong to a family of networks that require edges of non-uniform capacity to be eﬃcient. Networks with edges of uniform capacity are easier to build. This is usually the case for grids and tori. Unless explicitly mentioned, we will treat all edges in the following to be of capacity 1.  Deﬁnition 10.6  Torus, Mesh . Let m, d ∈ N. The  m, d -mesh M  m, d  is a graph with node set V = [m]d and edge set  E =  { a1, . . . , ad ,  b1, . . . , bd }  ai, bi ∈ [m],  ai − bi = 1  ,   cid:40    cid:41   d cid:88   i=1  4  2  1   10.2. HYPERCUBIC NETWORKS  145  where [m] means the set {0, . . . , m− 1}. The  m, d -torus T  m, d  is a graph that consists of an  m, d -mesh and additionally wrap- around edges from nodes  a1, . . . , ai−1, m−1, ai+1, . . . , ad  to nodes  a1, . . . , ai−1, 0, ai+1, . . . , ad  for all i ∈ {1, . . . , d} and all aj ∈ [m] with j  cid:54 = i. In other words, we take the expression ai − bi in the sum modulo m prior to computing the absolute value. M  m, 1  is also called a path, T  m, 1  a cycle, and M  2, d  = T  2, d  a d- dimensional hypercube. Figure 10.7 presents a linear array, a torus, and a hypercube.  Figure 10.7: The structure of M  m, 1 , T  4, 2 , and M  2, 3 .  Remarks:    Routing on a mesh, torus, or hypercube is trivial. On a d-dimensional hypercube, to get from a source bitstring s to a target bitstring t one only needs to ﬁx each “wrong” bit, one at a time; in other words, if the source and the target diﬀer by k bits, there are k! routes with k hops.   As required by Deﬁnition 10.4, the d-bit IDs of the nodes need to be mapped to the universe [0, 1 . One way to do this is by interpreting an ID as the binary representation of the fractional part of a decimal number. For example, the ID 101 is mapped to 0.1012 which has a decimal value of 0 · 20 + 1 · 2−1 + 0 · 2−2 + 1 · 2−3 = 5 8 .    The Chord architecture is a close relative of the hyper- cube, basically a less rigid hypercube. The hypercube connects every node with an ID in [0, 1  with every node in exactly distance 2−i, i = 1, 2, . . . , d in [0, 1 . Chord instead connect nodes with approximately distance 2−i.  0  1  2  −1m  00  10  20  30  110  111  01  11  21  31  100  101  02  12  22  32  010  011  03  13  23  33  000  001  M   ,1 m  T   4,2   M 2,3    146  CHAPTER 10. DISTRIBUTED STORAGE    The hypercube has many derivatives, the so-called hy- percubic networks. Among these are the butterﬂy, cube- connected-cycles, shuﬄe-exchange, and de Bruijn graph. We start with the butterﬂy, which is basically a “rolled out” hypercube.  Deﬁnition 10.8  Butterﬂy . Let d ∈ N. The d-dimensional butterﬂy BF  d  is a graph with node set V = [d + 1] × [2]d and an edge set E = E1 ∪ E2 with  E1 = {{ i, α ,  i + 1, α }  i ∈ [d], α ∈ [2]d}  and E2 = {{ i, α ,  i + 1, β }  i ∈ [d], α, β ∈ [2]d,α − β = 2i}. A node set { i, α   α ∈ [2]d} is said to form level i of the butterﬂy. The d-dimensional wrap-around butterﬂy W-BF d  is deﬁned by taking the BF  d  and having  d, α  =  0, α  for all α ∈ [2]d.  Remarks:    Figure 10.9 shows the 3-dimensional butterﬂy BF  3 . The BF  d  has  d + 1 2d nodes, 2d · 2d edges and de- It is not diﬃcult to check that combining the gree 4. node sets { i, α   i ∈ [d]} for all α ∈ [2]d into a single node results in the hypercube.    Butterﬂies have the advantage of a constant node degree over hypercubes, whereas hypercubes feature more fault- tolerant routing.    You may have seen butterﬂy-like structures before, e.g. sorting networks, communication switches, data center networks, fast fourier transform  FFT . The Benes net- work  telecommunication  is nothing but two back-to- back butterﬂies. The Clos network  data centers  is a close relative to Butterﬂies too. Actually, merging the 2i nodes on level i that share the ﬁrst d− i bits into a single node, the Butterﬂy becomes a fat tree. Every year there are new applications for which hypercubic networks are the perfect solution!    Next we deﬁne the cube-connected-cycles network.  It only has a degree of 3 and it results from the hypercube by replacing the corners by cycles.   10.2. HYPERCUBIC NETWORKS  147  Figure 10.9: The structure of BF 3 .  Deﬁnition 10.10  Cube-Connected-Cycles . Let d ∈ N. The cube-connected-cycles network CCC d  is a graph with node set V = { a, p   a ∈ [2]d, p ∈ [d]} and edge set  E =  cid:8 { a, p ,  a,  p + 1  mod d }  a ∈ [2]d, p ∈ [d] cid:9   ∪ cid:8 { a, p ,  b, p }  a, b ∈ [2]d, p ∈ [d], a = b except for ap   cid:9  .  Figure 10.11: The structure of CCC 3 .  Remarks:    Two possible representations of a CCC can be found in  Figure 10.11.    The shuﬄe-exchange is yet another way of transforming the hypercubic interconnection structure into a constant degree network.  000  001  010  011  100  101  110  111  0  1  2  3   110,0    111,0    110,1    111,1    100,1    110,2    101,1    111,2    100,0    101,0    100,2    000,2    010,2    011,2    101,2    011,0    010,0    010,1    000,1    001,2    011,1    001,1   2  1  0   000,0    001,0   000  001  010  011  100  101  110  111   148  CHAPTER 10. DISTRIBUTED STORAGE  Deﬁnition 10.12  Shuﬄe-Exchange . Let d ∈ N. The d- dimensional shuﬄe-exchange SE d  is deﬁned as an undi- rected graph with node set V = [2]d and an edge set E = E1 ∪ E2 with E1 = {{ a1, . . . , ad ,  a1, . . . , ¯ad }   a1, . . . , ad  ∈ [2]d, ¯ad = 1−ad}  and  graph.  E2 = {{ a1, . . . , ad ,  ad, a1, . . . , ad−1 }   a1, . . . , ad  ∈ [2]d} .  Figure 10.13 shows the 3- and 4-dimensional shuﬄe-exchange  Figure 10.13: The structure of SE 3  and SE 4 .  Deﬁnition 10.14  DeBruijn . The b-ary DeBruijn graph of dimension d DB b, d  is an undirected graph G =  V, E  with node set V = {v ∈ [b]d} and edge set E that contains all edges {v, w} with the property that w ∈ { x, v1, . . . , vd−1  : x ∈ [b]}, where v =  v1, . . . , vd .  Figure 10.15: The structure of DB 2, 2  and DB 2, 3 .  SE 3   100  101  SE 4   1000  1001  1100  1101  000  001  110  111  0000  0001  0100  0101  1010  1011  1110  1111  010  011  0010  0011  0110  0111  E E  1  2  01  001  011  00  11  000  111  010  101  10  100  110   10.2. HYPERCUBIC NETWORKS  149  Remarks:    Two examples of a DeBruijn graph can be found in Fig-  ure 10.15.    There are some data structures which also qualify as hy- percubic networks. An example of a hypercubic network is the skip list, the balanced binary search tree for the lazy programmer:  Deﬁnition 10.16  Skip List . The skip list is an ordinary ordered linked list of objects, augmented with additional forward links. The ordinary linked list is the level 0 of the skip list. In addition, every object is promoted to level 1 with probability 1 2. As for level 0, all level 1 objects are connected by a linked list. In general, every object on level i is promoted to the next level with probability 1 2. A special start-object points to the smallest ﬁrst object on each level.  Remarks:    Search, insert, and delete can be implemented in O log n  expected time in a skip list, simply by jumping from higher levels to lower ones when overshooting the searched position. Also, the amortized memory cost of each object is constant, as on average an object only has two forward links.    The randomization can easily be discarded, by determin- istically promoting a constant fraction of objects of level i to level i + 1, for all i. When inserting or deleting, object o simply checks whether its left and right level i neighbors are being promoted to level i + 1. If none of them is, promote object o itself. Essentially we establish a maximal independent set  MIS  on each level, hence at least every third and at most every second object is promoted.    There are obvious variants of the skip list, e.g., the skip graph. Instead of promoting only half of the nodes to the next level, we always promote all the nodes, similarly to a balanced binary tree: All nodes are part of the root level of the binary tree. Half the nodes are promoted left, and half the nodes are promoted right, on each level. Hence on level i we have have 2i lists  or, if we connect the last   150  CHAPTER 10. DISTRIBUTED STORAGE  element again with the ﬁrst: rings  of about n 2i objects. The skip graph features all the properties of Deﬁnition 10.4.    More generally, how are degree and diameter of Deﬁnition 10.4 related? The following theorem gives a general lower bound.  Theorem 10.17. Every graph of maximum degree d > 2 and size n must have a diameter of at least  cid:100  log n   log d − 1   cid:101  − 2.  Proof. Suppose we have a graph G =  V, E  of maximum degree d and size n. Start from any node v ∈ V . In a ﬁrst step at most In two steps at most d ·  d − 1  d other nodes can be reached. additional nodes can be reached. Thus, in general, in at most r steps at most  d ·  d − 1 i = 1 + d ·  d − 1 r − 1  d − 1  − 1  ≤ d ·  d − 1 r d − 2  r−1 cid:88   i=0  1 +  nodes  including v  can be reached. This has to be at least n to ensure that v can reach all other nodes in V within r steps. Hence,   d − 1 r ≥  d − 2  · n  d  ⇔ r ≥ logd−1  d − 2  · n d  .  Since logd−1  d − 2  d  > −2 for all d > 2, this is true only if r ≥  cid:100  log n   log d − 1   cid:101  − 2.  Remarks:    In other words, constant-degree hypercubic networks fea-  ture an asymptotically optimal diameter D.    Other hypercubic graphs manage to have a diﬀerent tradeoﬀ between node degree d and diameter D. The pancake graph, for instance, minimizes the maximum of these with max d, D  = Θ log n  log log n . The ID of a node u in the pancake graph of dimension d is an arbi- trary permutation of the numbers 1, 2, . . . , d. Two nodes u, v are connected by an edge if one can get the ID of node v by taking the ID of node u, and reversing  ﬂipping  the ﬁrst i numbers of u’s ID. For example, in dimension d = 4, nodes u = 2314 and v = 1324 are neighbors.   10.3. DHT & CHURN  151    There are a few other interesting graph classes which are not hypercubic networks, but nevertheless seem to relate to the properties of Deﬁnition 10.4. Small-world graphs  a popular representations for social networks  also have small diameter, however, in contrast to hypercubic net- works, they are not homogeneous and feature nodes with large degrees.    Expander graphs  an expander graph is a sparse graph which has good connectivity properties, that is, from ev- ery not too large subset of nodes you are connected to an even larger set of nodes  are homogeneous, have a low degree and small diameter. However, expanders are often not routable.  10.3 DHT & Churn  Deﬁnition 10.18  Distributed Hash Table  DHT  . A distrib- uted hash table  DHT  is a distributed data structure that imple- ments a distributed storage. A DHT should support at least  i  a search  for a key  and  ii  an insert  key, object  operation, possibly also  iii  a delete  key  operation.  Remarks:    A DHT has many applications beyond storing movies, e.g., the Internet domain name system  DNS  is essen- tially a DHT.    A DHT can be implemented as a hypercubic overlay net- work with nodes having identiﬁers such that they span the ID space [0, 1 .    A hypercube can directly be used for a DHT. Just use a globally known set of hash functions hi, mapping movies to bit strings with d bits.    Other hypercubic structures may be a bit more intricate when using it as a DHT: The butterﬂy network, for in- stance, may directly use the d + 1 layers for replication, i.e., all the d + 1 nodes are responsible for the same ID.    Other hypercubic networks, e.g.  the pancake graph,  might need a bit of twisting to ﬁnd appropriate IDs.   152  CHAPTER 10. DISTRIBUTED STORAGE    We assume that a joining node knows a node which al- ready belongs to the system. This is known as the boot- strap problem. Typical solutions are: If a node has been connected with the DHT previously, just try some of these previous nodes. Or the node may ask some au- thority for a list of IP addresses  and ports  of nodes that are regularly part of the DHT.    Many DHTs in the literature are analyzed against an ad- versary that can crash a fraction of random nodes. After crashing a few nodes the system is given suﬃcient time to recover again. However, this seems unrealistic. The scheme sketched in this section signiﬁcantly diﬀers from this in two major aspects.    First, we assume that joins and leaves occur in a worst- case manner. We think of an adversary that can remove and add a bounded number of nodes; the adversary can choose which nodes to crash and how nodes join.    Second, the adversary does not have to wait until the sys- tem is recovered before it crashes the next batch of nodes. Instead, the adversary can constantly crash nodes, while the system is trying to stay alive. Indeed, the system is never fully repaired but always fully functional. In par- ticular, the system is resilient against an adversary that continuously attacks the “weakest part” of the system. The adversary could for example insert a crawler into the DHT, learn the topology of the system, and then re- peatedly crash selected nodes, in an attempt to partition the DHT. The system counters such an adversary by con- tinuously moving the remaining or newly joining nodes towards the areas under attack.    Clearly, we cannot allow the adversary to have un- bounded capabilities. In particular, in any constant time interval, the adversary can at most add and or remove O log n  nodes, n being the total number of nodes cur- rently in the system. This model covers an adversary which repeatedly takes down nodes by a distributed de- nial of service attack, however only a logarithmic num- ber of nodes at each point in time. The algorithm relies on messages being delivered timely, in at most constant   10.3. DHT & CHURN  153  time between any pair of operational nodes, i.e., the syn- chronous model. Using the trivial synchronizer this is not a problem. We only need bounded message delays in order to have a notion of time which is needed for the adversarial model. The duration of a round is then proportional to the propagation delay of the slowest mes- sage.  Algorithm 10.19 DHT 1: Given: a globally known set of hash functions hi, and a hyper-  cube  or any other hypercubic network   2: Each hypercube virtual node  “hypernode”  consists of  Θ log n  nodes.  3: Nodes have connections to all other nodes of their hypernode  and to nodes of their neighboring hypernodes.  4: Because of churn, some of the nodes have to change to another hypernode such that up to constant factors, all hypernodes own the same number of nodes at all times.  5: If the total number of nodes n grows or shrinks above or below a certain threshold, the dimension of the hypercube is increased or decreased by one, respectively.  Remarks:    Having a logarithmic number of hypercube neighbors, each with a logarithmic number of nodes, means that each node has Θ log2 n  neighbors. However, with some additional bells and whistles one can achieve Θ log n  neighbor nodes.    The balancing of nodes among the hypernodes can be seen as a dynamic token distribution problem on the hy- percube. Each hypernode has a certain number of to- kens, the goal is to distribute the tokens along the edges of the graph such that all hypernodes end up with the same or almost the same number of tokens. While to- kens are moved around, an adversary constantly inserts and deletes tokens. See also Figure 10.20.    In summary, the storage system builds on two basic com- ponents:  i  an algorithm which performs the described   154  CHAPTER 10. DISTRIBUTED STORAGE  Figure 10.20: A simulated 2-dimensional hypercube with four hy- pernodes, each consisting of several nodes. Also, all the nodes are either in the core or in the periphery of a node. All nodes within the same hypernode are completely connected to each other, and additionally, all nodes of a hypernode are connected to the core nodes of the neighboring nodes. Only the core nodes store data items, while the peripheral nodes move between the nodes to bal- ance biased adversarial churn.  dynamic token distribution and  ii  an information aggre- gation algorithm which is used to estimate the number of nodes in the system and to adapt the dimension of the hypercube accordingly:  Theorem 10.21  DHT with Churn . We have a fully scalable, eﬃcient distributed storage system which tolerates O log n  worst- case joins and or crashes per constant time interval. As in other storage systems, nodes have O log n  overlay neighbors, and the usual operations  e.g., search, insert  take time O log n .  Remarks:    Indeed, handling churn is only a minimal requirement to make a distributed storage system work. Advanced stud- ies proposed more elaborate architectures which can also handle other security issues, e.g., privacy or Byzantine attacks.  Chapter Notes  The ideas behind distributed storage were laid during the peer- to-peer  P2P  ﬁle sharing hype around the year 2000, so a lot   BIBLIOGRAPHY  155  of the seminal research in this area is labeled P2P. The paper of Plaxton, Rajaraman, and Richa [PRR97] laid out a blueprint for many so-called structured P2P architecture proposals, such as Chord [SMK+01], CAN [RFH+01], Pastry [RD01], Viceroy [MNR02], Kademlia [MM02], Koorde [KK03], SkipGraph [AS03], SkipNet [HJS+03], or Tapestry [ZHS+04]. Also the paper of Plax- ton et. Some of linear and consistent hashing [KLL+97], locating shared objects [AP90, AP91], compact rout- ing [SK85, PU88], and even earlier: hypercubic networks, e.g. [AJ75, Wit81, GS81, BA84].  al. was standing on the shoulders of giants.  its eminent precursors are:  Furthermore, the techniques in use for preﬁx-based overlay structures are related to a proposal called LAND, a locality-aware distributed hash table proposed by Abraham et al. [AMD04].  More recently, a lot of P2P research focussed on security as- pects, describing for instance attacks [LMSW06, SENB07, Lar07], and provable countermeasures [KSW05, AS09, BSS09]. Another topic currently garnering interest is using P2P to help distribute live streams of video content on a large scale [LMSW07]. There are several recommendable introductory books on P2P computing, e.g. [SW05, SG05, MS07, KW08, BYL08].  Bibliography  [AJ75] George A. Anderson and E. Douglas Jensen. Computer Interconnection Structures: Taxonomy, Characteris- tics, and Examples. ACM Comput. Surv., 7 4 :197– 213, December 1975.  [AMD04] Ittai Abraham, Dahlia Malkhi, and Oren Dobzinski. LAND: stretch  1 + epsilon  locality-aware networks for DHTs. In Proceedings of the ﬁfteenth annual ACM- SIAM symposium on Discrete algorithms, SODA ’04, pages 550–559, Philadelphia, PA, USA, 2004.  [AP90] Baruch Awerbuch and David Peleg. Eﬃcient Distrib- uted Construction of Sparse Covers. Technical report, The Weizmann Institute of Science, 1990.  [AP91] Baruch Awerbuch and David Peleg. Concurrent Online Tracking of Mobile Users. In SIGCOMM, pages 221– 233, 1991.   156  CHAPTER 10. DISTRIBUTED STORAGE  [AS03] James Aspnes and Gauri Shah. Skip Graphs. In SODA,  pages 384–393. ACM SIAM, 2003.  [AS09] Baruch Awerbuch and Christian Scheideler. Towards a Scalable and Robust DHT. Theory Comput. Syst., 45 2 :234–260, 2009.  [BA84] L. N. Bhuyan and D. P. Agrawal. Generalized Hyper- cube and Hyperbus Structures for a Computer Net- work. IEEE Trans. Comput., 33 4 :323–333, April 1984.  [BSS09] Matthias Baumgart, Christian Scheideler, and Stefan Schmid. A DoS-resilient information system for dy- namic data management. In Proceedings of the twenty- ﬁrst annual symposium on Parallelism in algorithms and architectures, SPAA ’09, pages 300–309, New York, NY, USA, 2009.  [BYL08] John Buford, Heather Yu, and Eng Keong Lua. P2P Networking and Applications. Morgan Kaufmann Pub- lishers Inc., San Francisco, CA, USA, 2008.  [GS81] J.R. Goodman and C.H. Sequin. Hypertree: A Multi- processor Interconnection Topology. Computers, IEEE Transactions on, C-30 12 :923–933, December 1981.  [HJS+03] Nicholas J. A. Harvey, Michael B. Jones, Stefan Saroiu, Marvin Theimer, and Alec Wolman. SkipNet: a scal- able overlay network with practical locality properties. In Proceedings of the 4th conference on USENIX Sym- posium on Internet Technologies and Systems - Vol- ume 4, USITS’03, pages 9–9, Berkeley, CA, USA, 2003. USENIX Association.  [KK03] M. Frans Kaashoek and David R. Karger. Koorde: A Simple Degree-Optimal Distributed Hash Table. In M. Frans Kaashoek and Ion Stoica, editors, IPTPS, volume 2735 of Lecture Notes in Computer Science, pages 98–107. Springer, 2003.  [KLL+97] David R. Karger, Eric Lehman, Frank Thomson Leighton, Rina Panigrahy, Matthew S. Levine, and Daniel Lewin. Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots   BIBLIOGRAPHY  157  on the World Wide Web. In Frank Thomson Leighton and Peter W. Shor, editors, STOC, pages 654–663. ACM, 1997.  [KSW05] Fabian Kuhn, Stefan Schmid, and Roger Wattenhofer. A Self-Repairing Peer-to-Peer System Resilient to Dy- namic Adversarial Churn. In 4th International Work- shop on Peer-To-Peer Systems  IPTPS , Cornell Uni- versity, Ithaca, New York, USA, Springer LNCS 3640, February 2005.  [KW08] Javed I. Khan and Adam Wierzbicki.  Introduction: Guest editors’ introduction: Foundation of peer-to- peer computing. Comput. Commun., 31 2 :187–189, February 2008.  [Lar07] Erik Larkin.  Storm Worm’s virulence may change tactics. http:  www.networkworld.com news 2007  080207-black-hat-storm-worms-virulence.html, August 2007.  [LMSW06] Thomas Locher, Patrick Moor, Stefan Schmid, and Free Riding in BitTorrent is Roger Wattenhofer. Cheap. In 5th Workshop on Hot Topics in Networks  HotNets , Irvine, California, USA, November 2006.  [LMSW07] Thomas Locher, Remo Meier, Stefan Schmid, and Roger Wattenhofer. Push-to-Pull Peer-to-Peer Live Streaming. In 21st International Symposium on Distributed Computing  DISC , Lemesos, Cyprus, September 2007.  [MM02] Petar Maymounkov and David Mazi`eres. Kademlia: A Peer-to-Peer Information System Based on the XOR Metric. In Revised Papers from the First International Workshop on Peer-to-Peer Systems, IPTPS ’01, pages 53–65, London, UK, UK, 2002.  [MNR02] Dahlia Malkhi, Moni Naor, and David Ratajczak. Viceroy: a scalable and dynamic emulation of the but- terﬂy. In Proceedings of the twenty-ﬁrst annual sym- posium on Principles of distributed computing, PODC ’02, pages 183–192, New York, NY, USA, 2002.   158  CHAPTER 10. DISTRIBUTED STORAGE  [MS07] Peter Mahlmann and Christian Schindelhauer. Peer-  to-Peer Networks. Springer, 2007.  [PRR97] C. Greg Plaxton, Rajmohan Rajaraman,  and Andr´ea W. Richa. Accessing Nearby Copies of Repli- cated Objects in a Distributed Environment. In SPAA, pages 311–320, 1997.  [PU88] David Peleg and Eli Upfal. A tradeoﬀ between space and eﬃciency for routing tables. In Proceedings of the twentieth annual ACM symposium on Theory of com- puting, STOC ’88, pages 43–52, New York, NY, USA, 1988.  [RD01] Antony Rowstron and Peter Druschel. Pastry: Scal- able, decentralized object location and routing for large-scale peer-to-peer systems. In IFIP ACM Inter- national Conference on Distributed Systems Platforms  Middleware , pages 329–350, November 2001.  [RFH+01] Sylvia Ratnasamy, Paul Francis, Mark Handley, Richard Karp, and Scott Shenker. A scalable content- addressable network. SIGCOMM Comput. Commun. Rev., 31 4 :161–172, August 2001.  [SENB07] Moritz Steiner, Taouﬁk En-Najjary, and Ernst W. Bier- sack. Exploiting KAD: possible uses and misuses. SIG- COMM Comput. Commun. Rev., 37 5 :65–70, October 2007.  [SG05] Ramesh Subramanian and Brian D. Goodman. Peer to Peer Computing: The Evolution of a Disruptive Tech- nology. IGI Publishing, Hershey, PA, USA, 2005.  [SK85] Nicola Santoro and Ramez Khatib. Labelling and Im- plicit Routing in Networks. Comput. J., 28 1 :5–8, 1985.  [SMK+01] Ion Stoica, Robert Morris, David Karger, M. Frans Kaashoek, and Hari Balakrishnan. Chord: A scal- able peer-to-peer lookup service for internet applica- tions. SIGCOMM Comput. Commun. Rev., 31 4 :149– 160, August 2001.   BIBLIOGRAPHY  159  [SW05] Ralf Steinmetz and Klaus Wehrle, editors. Peer-to- Peer Systems and Applications, volume 3485 of Lecture Notes in Computer Science. Springer, 2005.  [Wit81] L. D. Wittie. Communication Structures for Large Networks of Microcomputers. IEEE Trans. Comput., 30 4 :264–273, April 1981.  [ZHS+04] Ben Y. Zhao, Ling Huang, Jeremy Stribling, Sean C. Rhea, Anthony D. Joseph, and John Kubiatowicz. Tapestry: a resilient global-scale overlay for service de- ployment. IEEE Journal on Selected Areas in Commu- nications, 22 1 :41–53, 2004.   Index  access strategy, 88 agreement, 20 all-same validity, 36 any-input validity, 36 asynchronous byzantine agree-  ment, 43  tem, 20  asynchronous distributed sys-  asynchronous runtime, 21 authenticated byzantine agree-  ment, 72 authentication, 71 availability, 106  B-grid quorum system, 94 bitcoin address, 108 bitcoin block, 112 bitcoin network, 107 bivalent conﬁguration, 22 block algorithm, 114 block format, 128 blockchain, 9, 113 bulk encryption, 61 butterﬂy topology, 146 byzantine agreement, 35 byzantine agreement with one  fault, 37  byzantine behavior, 35  CAP, 106 causal consistency, 120 causal relation, 119 certiﬁcate authority CA, 58  churn, 154 cipher block chaining CBC, 62 client-server algorithm, 8 collision resistant hash function,  63  command, 74 commit certiﬁcate, 76 complete command, 74 concurrent locking strategy, 92 conﬁguration of a system, 21 conﬁguration transition, 23 conﬁguration tree, 23 consensus, 20 consistency, 106 consistent hashing, 141 correct-input validity, 36 critical conﬁguration, 24 cryptographic hash functions,  cube-connected-cycles topology,  123  147  DeBruijn topology, 148 DHT, 151 DHT algorithm, 153 Diﬃe-Hellman key exchange, 50 discrete logarithm, 50 distributed hash table, 151 doublespend, 110  electronic code book ECB, 61 Elgamal public key encryption  and decryption, 56  160   INDEX  161  eventual consistency, 107  f-disseminating quorum system,  f-masking grid quorum system,  96  97  f-masking quorum system, 96 f-opaque quorum system, 99 failure probability, 93 Fermat’s little theorem, 55 forward secrecy, 53 full node, 130  grid quorum system, 90  hash-based message authentica-  tion code HMAC, 65  history of commands, 74 homogeneous system, 143 hypercube topology, 144 hypercubic network, 143  king algorithm, 40  lightweight client, 130 live system, 83 liveness, 83 load of a quorum system, 88  Miller-Rabin algorithm, 51 miner, 129 mining algorithm, 112 monotonic read consistency, 119 monotonic write consistency,  119  multisig output, 116  node, 7  one-time pad algorithm, 61 one-way hash function, 63 operation, 74  P2PKH, 126 P2SH, 126 partition tolerance, 106 paxos algorithm, 15 perfect secrecy, 59 primitive root, 49 proof of misbehavior, 77 proof of work, 111 pseudonymous, 108 public key cryptography, 54 public secret key generation, 55  quorum, 87 quorum system, 87  M-grid quorum system, 98 majority quorum system, 88 malleability, 63 man in the middle attack, 52 median validity, 37 merkle tree, 124 mesh topology, 144 message  authentication code  randomized  consensus  algo-  rithm, 27  read-your-write consistency, 119 refund transaction, 117 replay attack, 62 request, 74 resilience of a quorum system,  92  MAC, 65  reward transaction, 113  message loss model, 8 message passing model, 7 message-digest algorithm MD,  63  micropayment channel, 118  safe system, 78 safety, 78 script, 124 script execution, 127   162  INDEX  variable message delay model, 9 view change, 79 view of a distributed system, 73  weak consistency, 119 web of trust, 58 web wallet, 130 work of a quorum system, 88  zyzzyva algorithm, 75  secure hash algorithm SHA, 63 secure shell SSH, 66 secure sockets layer SSL, 66 sequential locking strategy, 91 serializer, 10 setup transaction, 117 SHA256, 112 shared coin algorithm, 31 shuﬄe-exchange network, 148 signature, 71 simple payment veriﬁcation, 130 singlesig output, 116 singleton quorum system, 88 skip list topology, 149 smart contract, 115 SPV, 130 standard transaction types, 126 state replication, 9 state replication with serializer,  synchronous distributed system,  10  37  synchronous runtime, 37  termination, 20 threshold secret sharing, 59 ticket, 12 timelock, 116 torus topology, 144 transaction, 109 transaction algorithm, 110 transaction fee, 110 transaction format, 125 transaction input, 109 transaction output, 109 transport layer security TLS, 66 two-phase commit, 11 two-phase locking, 11 two-phase protocol, 10  univalent conﬁguration, 21  validity, 20
