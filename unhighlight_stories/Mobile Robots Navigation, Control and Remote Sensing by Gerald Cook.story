MOBILE ROBOTS     IEEE Press    445 Hoes Lane    Piscataway, NJ 08854   IEEE Press Editorial Board  Lajos Hanzo,  Editor in Chief    R. Abhari     J. Anderson     G. W. Arnold     F. Canavero      M. El - Hawary     B - M. Haemmerli     M. Lanzerotti     D. Jacobson      O. P. Malik     S. Nahavandi     T. Samad     G. Zobrist     Kenneth Moore,  Director of IEEE Book and Information Services  BIS   Technical Reviewers   Dr. Atalay Barkana    Anadolu University, Turkey    Dr. Osman Parlaktuna, Dr. Metin Ozkan, and Dr. Ahmet Yazici    Electrical and Electronics Engineering Department    Eskisehir Osmangazi University, Turkey    H. W. Mergler, Ph.D.    Leonard Case Professor of Electrical Engineering    School of Engineering    Case Western Reserve University         MOBILE ROBOTS  Navigation, Control and  Remote Sensing  GERALD COOK  A JOHN WILEY & SONS, INC., PUBLICATION   Copyright   2011 by Institute of Electrical and Electronics Engineers. All rights reserved.  Published by John Wiley & Sons, Inc., Hoboken, New Jersey. Published simultaneously in Canada.  No part of this publication may be reproduced, stored in a retrieval system, or transmitted in  any form or by any means, electronic, mechanical, photocopying, recording, scanning, or  otherwise, except as permitted under Section 107 or 108 of the 1976 United States Copyright  Act, without either the prior written permission of the Publisher, or authorization through  payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc.,  222 Rosewood Drive, Danvers, MA 01923,  978  750-8400, fax  978  750-4470, or on the web at  www.copyright.com. Requests to the Publisher for permission should be addressed to the  Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030,   201  748-6011, fax  201  748-6008, or online at http:  www.wiley.com go permissions.  Limit of Liability Disclaimer of Warranty: While the publisher and author have used their best  efforts in preparing this book, they make no representations or warranties with respect to the  accuracy or completeness of the contents of this book and speciﬁ cally disclaim any implied  warranties of merchantability or ﬁ tness for a particular purpose. No warranty may be created  or extended by sales representatives or written sales materials. The advice and strategies  contained herein may not be suitable for your situation. You should consult with a professional  where appropriate. Neither the publisher nor author shall be liable for any loss of proﬁ t or  any other commercial damages, including but not limited to special, incidental, consequential,  or other damages.  For general information on our other products and services or for technical support, please  contact our Customer Care Department within the United States at  800  762-2974, outside the  United States at  317  572-3993 or fax  317  572-4002.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in  print may not be available in electronic formats. For more information about Wiley products,  visit our web site at www.wiley.com.  Library of Congress Cataloging-in-Publication Data:    p. cm.      ISBN 978-0-470-63021-1  hardback   Cook, Gerald, 1937–   Mobile robots : navigation, control and remote sensing   Gerald Cook.       1.  Mobile robots.  I.  Title.   TJ211.415.C68 2011   629.8'932–dc22    2010046371  oBook ISBN: 978-1-118-02640-3 ePDF ISBN: 978-1-118-02719-6 ePub ISBN: 978-1-118-02904-6  Printed in the United States of America.  10  9  8  7  6  5  4  3  2  1      To my heavenly Father for leading me to a vocation that has brought me  a lifetime of joy and happiness.      To my wife, Nancy Anne, for her encouragement and support of all my  endeavors throughout my career.      To my two adult sons, Bo and Ford, for their continued encouragement  and interest in my work.      To my mother, Rose Boyer Cook, who as a single parent of four provided  so abundantly for our needs.          CONTENTS  Preface  Introduction     1 Kinematic Models for Mobile Robots  1.0  Introduction,  1 1.1  Vehicles with Front-Wheel Steering,  1 1.2  Vehicles with Differential-Drive Steering,  5      Exercises,  8 References,  9    2 Mobile Robot Control 2.0  Introduction,  11 2.1  Front-Wheel Steered Vehicle, Heading Control,  11 2.2  Front-Wheel Steered Vehicle, Speed Control,  22 2.3  Heading and Speed Control for the Differential-  Drive Robot,  23  2.4  Reference Trajectory and Incremental Control,   Front-Wheel Steered Robot,  26  2.5  Heading Control of Front-Wheel Steered Robot Using the   2.6  Computed Control for Heading and Velocity, Front-Wheel   Nonlinear Model,  32  Steered Robot,  36  xi xiii  1  11  vii   viii  CONTENTS  2.7  Heading Control of Differential Drive Robot Using   the Nonlinear Model,  38  2.8  Computed Control for Heading and Velocity,   Differential-Drive Robot,  39  2.9  Steering Control Along a Path Using a Local   Coordinate Frame,  41  2.10  Optimal Steering of Front-Wheel Steered Vehicle,  54 2.11  Optimal Steering of Front-Wheel Steered Vehicle,   Free Final Heading Angle,  75 Exercises,  77 References,  78         3 Robot Attitude  3.0  Introduction,  79 3.1  Deﬁ nition of Yaw, Pitch and Roll,  79 3.2  Rotation Matrix for Yaw,  80 3.3  Rotation Matrix for Pitch,  82 3.4  Rotation Matrix for Roll,  84 3.5  General Rotation Matrix,  86 3.6  Homogeneous Transformation,  88 3.7  Rotating a Vector,  92      Exercises,  93 References,  94    4 Robot Navigation  79  95  4.0  Introduction,  95 4.1  Coordinate Systems,  95 4.2  Earth-Centered Earth-Fixed Coordinate System,  96 4.3  Associated Coordinate Systems,  98 4.4  Universal Transverse Mercator  UTM    Coordinate System,  102  4.5  Global Positioning System,  104 4.6  Computing Receiver Location Using GPS,   Numerical Methods,  108 4.6.1  Computing Receiver Location Using GPS via   Newton’s Method,  108  4.6.2  Computing Receiver Location Using GPS via   Minimization of a Performance Index,  116  4.7  Array of GPS Antennas,  123 4.8  Gimbaled Inertial Navigation Systems,  126 4.9  Strap-Down Inertial Navigation Systems,  131   CONTENTS  ix  4.10  Dead Reckoning or Deduced Reckoning,  137 4.11  Inclinometer Compass,  138      Exercises,  142 References,  147    5 Application of Kalman Filtering  149  5.0  Introduction,  149 5.1  Estimating a Fixed Quantity Using Batch Processing,  149 5.2  Estimating a Fixed Quantity Using   Recursive Processing,  151  5.3  Estimating the State of a Dynamic System Recursively,  156 5.4  Estimating the State of a Nonlinear System via the   Extended Kalman Filter,  169 Exercises,  185 References,  189         6 Remote Sensing  6.0  Introduction,  191 6.1  Camera Type Sensors,  191 6.2  Stereo Vision,  202 6.3  Radar Sensing: Synthetic Aperture Radar  SAR ,  206 6.4  Pointing of Range Sensor at Detected Object,  212 6.5  Detection Sensor in Scanning Mode,  217      Exercises,  222 References,  223  191  225    7 Target Tracking Including Multiple Targets   with Multiple Sensors 7.0  Introduction,  225 7.1  Regions of Conﬁ dence for Sensors,  225 7.2  Model of Target Location,  232 7.3  Inventory of Detected Targets,  239      Exercises,  244 References,  245    8 Obstacle Mapping and its Application   to Robot Navigation 8.0  Introduction,  247 8.1  Sensors for Obstacle Detection and Geo-Registration,  248 8.2  Dead Reckoning Navigation,  249  247   x  CONTENTS  8.3  Use of Previously Detected Obstacles for Navigation,  252 8.4  Simultaneous Corrections of Coordinates of Detected   Obstacles and of the Robot,  258 Exercises,  262 References,  263         9 Operating a Robotic Manipulator  265  9.0  Introduction,  265 9.1  Forward Kinematic Equations,  265 9.2  Path Speciﬁ cation in Joint Space,  269 9.3  Inverse Kinematic Equations,  271 9.4  Path Speciﬁ cation in Cartesian Space,  276 9.5  Velocity Relationships,  284 9.6  Forces and Torques,  289      Exercises,  292 References,  293  10 Remote Sensing via UAVS  10.0  Introduction,  295 10.1  Mounting of Sensors,  295 10.2  Resolution of Sensors,  296 10.3  Precision of Vehicle Instrumentation,  297 10.4  Overall Geo-Registration Precision,  298      Exercises,  300 References,  300  Appendix A  Demonstrations of Undergraduate   Student Robotic Projects A.0  Introduction,  301 A.1  Demonstration of the GEONAVOD Robot,  301 A.2  Demonstration of the Automatic Balancing   Robotic Bicycle  ABRB ,  302  See demonstration videos at http:  www.wiley.com WileyCDA   WileyTitle productCd-0470630213.html  Index   295  301  305    PREFACE         A number of experiences and acquaintances have contributed to this  project. The Countermine Branch of the Science Division of the Night  Vision Electronic Sensors Directorate  NVESD , United States Army  played a particularly important role through its sponsorship of related  research.  This  research  effort  had  as  its  objective  the  detection  and  geo - registration  of  landmines  through  the  use  of  vehicular  mounted  sensors. The nature of the problem required that a broad set of tools  be  brought  to  bear.  These  required  tools  included  a  vehicle  model,  sensor  models,  coordinate  transformations,  navigation,  state  estima- tion, probabilistic decision making, and others. Much of the required  technology had previously existed. The contribution here was to bring  together  these  particular  bodies  of  knowledge  and  combine  them  so  as to meet the objectives. This led to several interesting years of inter- action  with  NVESD  and  other  researchers  in  this  area  of  applied  research.    Afterwards,  it  was  realized  that  the  work  could  be  cast  in  a  more  general framework, leading to a set of notes for a second - year graduate  course  in  Mobile  Robots.  A  course  on  modern  control  and  one  on  random processes are the required prerequisites. This course was taught  several times at George Mason University, and numerous revisions and  additions  resulted  as  well  as  a  set  of  problems  at  the  end  of  each  chapter. Finally, the notes were organized more formally with the result  being this book. The following is a suggested schedule for teaching a  one semester course from this book.  xi   xii     PREFACE     1.       Kinematic Models for Mobile Robots  0.5 weeks.     2.       Mobile Robot Control  2.0 weeks.     3.       Robot Attitude  1.0 week.     4.       Robot Navigation  2.0 weeks.     5.       Application of Kalman Filtering  2.0 weeks.     6.       Remote Sensing : 3.0 weeks.     7.       Target Tracking Including Multiple Targets with Multiple Sensors      8.       Obstacle Mapping and its Application to Robot Navigation  1.0   1.0 week.    week.      9.       Operating a Robotic Manipulator  1.0 week.     10.       Remote Sensing via UAVs  0.5 weeks.       It is hoped that this book will also serve as a useful reference to those  working in related areas. Because of the overriding objective described  in the title of the book, the topics cut across traditional curricular boun- daries to bring together material from several engineering disciplines.  As a result of this the book could be used for a course taught within  electrical engineering, mechanical engineering, aerospace engineering  or possibly others. I would like to acknowledge here that MATLAB  ®    is  a  registered  trademark  of The  MathWorks,  Inc. Also,  please  note,  two of the videos referred to in Appendix  A  can be viewed at  http:   www.wiley.com WileyCDA WileyTitle productCd - 0470630213.html .    I would like to express my appreciation to some of the individuals  who have inﬂ uenced and encouraged me in the writing of this book.  These include Kelly Sherbondy, my research sponsor at NVESD, former  colleague  Guy  Beale,  department  chairman  Andre  Manitius,  former  student Patrick Kreidl, industrial associate Bill Pettus, collaborator at  the  Naval  Research  Laboratory  Jay  Oaks,  former  students  Smriti  Kansal and Shwetha Jakkidi who were part of the NVESD project, and  the many other students who have attended my classes and provided  me with inspiration over the years.      Gerald Cook            INTRODUCTION         Mobile robots, as the name implies, have the ability to move around.  They may travel on the ground, on the surface of bodies of water, under  water, and in the air. This is in contrast with ﬁ xed - base robotic manipu- lators that are more commonplace in manufacturing operations such  as  automobile  assembly,  aircraft  assembly,  electronic  parts  assembly,  welding, spray painting and others. Fixed - base robotic manipulators are  typically programmed to perform repetitive tasks with perhaps limited  use  of  sensors,  whereas  mobile  robots  are  typically  less  structured  in  their operation and likely to use more sensors.    As a mobile robot performs its tasks, it is important for its supervisor  to maintain knowledge of its location and orientation. Only then can  the sensed information be accurately reported and fully exploited. Thus  navigation  is  essential.  Navigation  is  also  required  in  the  process  of  directing the mobile robot to a speciﬁ ed destination. Along with naviga- tion is the need for stable and efﬁ cient control strategies. The naviga- tion  and  control  operations  must  work  together  hand - in - hand.  Once  the mobile robot has reached its destination, the sensors can acquire  the  needed  data  and  either  store  it  for  future  transfer  or  report  it  immediately to the next level up. Thus there is a whole system of func- tions required for effective use of mobile robots.    Mobile robots may be operated in a variety of different modes. One  of these is the tele - operated mode in which a supervisor provides some  of  the  instructions.  Here  sensors  including  cameras  provide  informa- tion from the robot to the supervisor that enables him or her to assess   xiii   xiv  INTRODUCTION  the situation and decide on the next course of action. The supervision  may be very complete, leaving no decision making to the robot, or it  may be at a high level only, leaving details to be worked out by algo- rithms residing on the robot. Some examples of this type of operation  are the Mars rovers and the walking robots that descended down into  the  volcano  on  Mount  Saint  Helens  in  the  state  of  Washington.  Additional  applications  include  the  handling  of  hazardous  materials  such as nuclear waste or explosives and the search in war operations  for  explosives  such  as  landmines.  Other  examples  are  unmanned  air  vehicles  UAV ’ s  and autonomous underwater vehicles  AUV ’ s  that  can be used for reconnaissance operations. The trajectory may be pre -  speciﬁ ed  with  the  provision  for  intervention  and  re - direction  as  the  circumstances dictate.    One of the more interesting stories involving a tele - operated mobile  robot took place in Prince William County, Virginia in the nineties. The  police had a suspect cornered in an apartment house and decided that  since  he  was  armed  they  would  send  in  their  mobile  robot.  It  was  a  tracked vehicle with a camera, an articulated manipulator and a stun  gun. Under the direction of a supervisor the robot was able to climb  the  stairs,  open  the  apartment  door,  open  a  closet  door,  lift  a  pile  of  clothes off the suspect and then stun him so that he could be appre- hended. This served a very useful purpose and alleviated the need for  the police ofﬁ cers to subject themselves to risk of injury or death.    Another  possible  mode  is  autonomous  operation.  Here  the  robot  operates without external inputs except those inputs obtained through  its sensors. Often there is a random element to the motion with sensors  for collision avoidance and or signal seeking. One example of this type  of operation was the miniature solar - powered lawn mowers at the CIA  in  Langley,  Virginia.  These  mobile  robots  were  the  size  of  a  dinner  plate and had razor sharp blades. The courtyard in which they worked  was  quite  smooth  with  well  deﬁ ned  boundaries.  Each  robot  could  move in a random direction until hitting an obstacle at which time it  switched  to  a  new  direction.  Another  example  of  this  autonomous  robotic behavior is a swimming - pool cleaner. This device moves about  the pool sucking up any debris on the bottom of the pool and causing  it to be pumped into the ﬁ ltration system. The motion of the mobile  robot seems to be somewhat random with the walls of the pool pro- viding a natural boundary. Similar devices exist for vacuuming homes  or ofﬁ ces.    A very exciting and recent example of an underwater semi - autono- mous vehicle was the crossing of the Atlantic Ocean, from the coast of  New Jersey to the coast of Spain, by the deep - sea glider Scarlet. This    INTRODUCTION  xv  eight - foot  long,  135  pound,  unmanned  vehicle  was  the  product  of  a  research team at Rutgers University and Teledyne Webb Research. The  voyage took 221 days, extended over 4,600 miles and provided data on  the water temperature and salinity as a function of depth. The glider  was  powered  by  a  battery  that  alternately  pumped  water  out  of  the  front  portion  of  the  vehicle  to  cause  it  to  rise  and  took  on  water  to  cause it to dive. The battery could also be shifted forward or backward  to  modify  the  weight  distribution  and  thereby  adjust  the  glide  angle.  As the glider dove or climbed, its hydrodynamic wings gave it forward  motion  in  much  the  same  manner  as  that  of  a  toy  airplane  glider  dropped from a second ﬂ oor window. It was equipped with a rudder  for  steering.  Normally  it  traveled  down  to  a  depth  of  600  feet  below  the surface of the ocean and then up to within 60 feet of the surface.  A few times per day it would surface to get a GPS ﬁ x on its position,  make radio contact with its supervisor and obtain a new way - point to  head  toward.  Apparently  the  vehicle  was  equipped  with  an  inertial  measurement  device  that  would  provide  heading  information  while  underwater.  Washington Post, Tuesday Dec 15, 2009, health and science  Section pages E1 and E6     Examples  of  mobile  robots  in  manufacturing  facilities  include  wheeled vehicles used for material transfer from one work station to  another. Here a line painted on the ﬂ oor may designate the path for  the mobile robot to follow. Optical sensors sense the boundaries of the  line  and  give  commands  to  the  steering  system  to  cause  the  mobile  robot to follow along the track. Schemes such as this can also be used  for mobile robots whose assignment is to perform inventory checks or  security checks in a large facility such as a warehouse. Here the path  for the mobile robot is speciﬁ ed and the sensors acquire and store the  required information as the robot makes its rounds.    There are two basic types of steering used by mobile robots operat- ing on the ground. For both of these types of steering, the mobile robot  may  have  one  or  two  front  wheels.  One  type  is  front - wheel  steering  much like that of an automobile. This type of steering presents interest- ing challenges to the controller because it has a nonzero turning radius  limited by the length of the robot and the maximum steering angle.    The other type of steering involves independent wheel control for  each side. By rotating the left and right wheels in opposite directions  at the same speed the robot can be made to turn while in place, i.e., at  a zero turning radius. Tracked vehicles use this same type differential -  drive steering strategy, often referred to as skid steering.    The  objectives  of  this  book  are  to  serve  as  a  textbook  for  a  one -  semester graduate course on the same subject area and also to provide    xvi  INTRODUCTION  a useful reference for one interested in this ﬁ eld. The book presumes  knowledge  of  modern  control  and  random  processes.  Exercises  are  included  with  each  chapter.  Prior  facility  with  digital  simulation  of  dynamic systems is very helpful but may be developed as one takes the  course. The material lends itself well to the inclusion of a course project  if one desires to do so.           1   KINEMATIC MODELS  FOR MOBILE ROBOTS           1.0      INTRODUCTION    This  chapter  is  devoted  to  the  development  of  kinematic  models  for  two types of wheeled robots. The kinematic equations are developed  along with the basic geometrical properties of achievable motion. The  two conﬁ gurations considered here do not exhaust the myriad of pos- sible  conﬁ gurations  for  wheeled  robots;  however,  they  serve  as  an  adequate test bed for the development and discussion of the principals  involved.       1.1      VEHICLES WITH FRONT - WHEEL STEERING    The ﬁ rst type of mobile robot to be considered is the one with front -  wheel steering. Here the vehicle is usually powered via the rear wheels,  and the steering is achieved by way of an actuator for turning the front  wheels.    In Figure  1.1  we have a diagram for a four - wheel front - wheel - steered  robot. The  equations  would  also  apply  for  the  case  of  a  single  front  wheel. The angle the front wheels make with respect to the longitudinal   Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  1   2     KINEMATIC MODELS FOR MOBILE ROBOTS  y  ground  L   x  robot  y  robot  y  a  d   a  R y  x  ground       Figure 1.1       Schematic Diagram of the Front - Wheel Steered Robot    axis of the robot,   yrobot, is deﬁ ned as   α, measured in the counter - clockwise  direction . The angle that the longitudinal axis,   yrobot, makes with respect  to the   yground axis is deﬁ ned as   ψ, also measured in the counter - clockwise  direction. The instantaneous center about which the robot is turning is  the  point  of  intersection  of  the  two  lines  passing  through  the  wheel  axes.      From geometry we have  which may be solved to yield the instantaneous radius of curvature for  the path of the midpoint of the rear axle of the robot.    1.1      = tanα   L     R     R  L= tanα      From geometry we also have  rear wheel =    v  R     ψ ψ cid:1     R  =  d dt            VEHICLES WITH FRONT-WHEEL STEERING     3       cid:1 ψ= v  rear wheel    R  which can be written as       cid:1 ψ  =  v rear wheel α   tan L  =  v rear wheel  α tan      L    1.2       If one held the steering angle   α constant, the trajectory would result  in a circle whose radius is dictated by the robot length and the actual  steering angle used per equation   1.1  .    Now  the  instantaneous  curvature  itself  is  deﬁ ned  as  the  ratio  of  change in angle divided by change in distance or change in angle per  distance traveled. It is given by  which is the inverse of the instantaneous radius of curvature. Thus the  radius of curvature may be interpreted as  ∆ ∆    κ ψ ψ   t = ∆ ∆   s t  ∆ ∆ s  =  =   cid:1  ψ     vrear wheel  =     R  =  1 κ  v rear wheel   cid:1  ψ  =  ds ψ d     i.e., the change in distance traveled per radian change in heading angle.   The  complete  set  of  kinematic  equations  for  the  motion  in  robot   coordinates are  =     vx = 0     v    v rear wheel = v       cid:1 ψ  y       α rear wheel tan      L      cid:1 x vrear wheel     cid:1 y vrear wheel  = − =  sinψ    cosψ       1.3a       1.3b       1.3c        1.4a       1.4b      Converted to earth coordinates these become  or                              4     KINEMATIC MODELS FOR MOBILE ROBOTS  = v       cid:1 ψ  α rear wheel tan      L    1.4c       This  form  of  the  equations  is  quite  simple:  however,  it  should  be  noted that these equations are nonlinear. Also see Dudek and Jenkin.   Now if we wish to take into account the fact that steering angle and  velocity cannot change instantaneously, we may deﬁ ne as control signals  the derivatives or rates of these variables, i.e.      cid:1 α= u1      rear wheel = 2         cid:1 v u    1.5a       1.5b       The system of equations for this model is now ﬁ fth order. The equa- tions provide the correct kinematic relationships among the variables  for motion and rotation in the  xy  plane but do not include the complex- ity of suspension or motor dynamics. Also not included in this model  are robot pitch and roll.    It may be desirable to form a discrete - time model from these equa- tions. This would be useful for discrete - time simulation as well as other  applications.  Clearly  these  equations  are  nonlinear.  Therefore,  the  methods  used  for  converting  a  linear  continuous - time  system  to  a  discrete - time representation are not applicable. One approach is to use  the Euler integration method. This method is a ﬁ rst - order, Taylor - series  approximation  to  integration  and  says  that  the  derivative  may  be  approximated by a ﬁ nite difference      cid:1 x t      ≈  +    x t  −∆   t ∆ t      x t       This can be re - arranged to yield       x t  +  ∆ t     ≈      x t  +  ∆ cid:1      x t t       Setting   t kT=   and the sampling interval   ∆t T=  and applying this to   the above equations we have        x k  +      1 T  =    x kT Tv     −  rear wheel     kT  ψ  sin    kT            1.6a     and                                 and  VEHICLES WITH DIFFERENTIAL-DRIVE STEERING     5        y k  +      1 T  =    y kT Tv     +     kT   cos  ψ    kT             ψ     k  +      1 T  =  ψ    kT T     +     kT     α tan    kT          rear wheel  v rear wheel L     α     k  +      1 T  =  α    +  kT Tu kT           1         1.6b       1.6c       1.6d       1.6e            v k  +      1 T  =    v kT Tu kT           2       +   Here  the  sampling  interval    T  must  be  chosen  sufﬁ ciently  small  depending on the dynamics of the original differential equations, i.e.,  the behavior of the discrete - time model must match up with that of the  original system. For a linear system, this corresponds to selecting the  sampling  interval  to  be  approximately  one - ﬁ fth  of  the  smallest  time  constant of the system or smaller depending on the degree of precision  required. For nonlinear systems, it may be necessary to determine this  limiting  size  empirically.  This  discrete - time  model  may  be  used  for  analysis, control design, estimator design and simulation.    It should be noted that more sophisticated and more robust methods  exist for converting continuous - time dynamic system models to discrete -  time models. For more information on this topic the reader is referred  to   “ Digital  Simulation  of  Dynamic  Systems ”   by  Hartley,  Beale  and  Chicatelli.    From time to time, it will be convenient to interpret speed expressed  in  various  units.  For  this  reason  the  following  equalities  are  presented.        10  kilometers hr  2 778 .  meters   sec  9 1134 .  feet   sec  6 2137 .  mph       =  =  =     1.2      VEHICLES WITH DIFFERENTIAL - DRIVE STEERING    Another common type of steering used for mobile robots is differential -  drive steering illustrated in Figure  1.2 . Here the wheels on one side of  the robot are controlled independently of the wheels on the other side.  By coordinating the two different speeds, one can cause the robot to  spin in place, move in a straight line, move in a circular path, or follow  any prescribed trajectory.      The equations of motion for the robot steered via differential wheel  speeds are now derived. Let R represent the instantaneous radius of    6     KINEMATIC MODELS FOR MOBILE ROBOTS  y  robot  y  y  ground  R  y  x  robot  W  x  ground       Figure 1.2       Schematic Diagram of Differential - Drive Robot    curvature of the robot trajectory. The width of the vehicle, i.e., spacing  between  the  wheels,  is  designated  as W.  From  geometrical  consider- ations we have:  and                 left =    v   cid:1 ψ   −  R W     2      right =    v   cid:1 ψ   +  R W     2         v  right  −  v left  =  cid:1 ψ   W  v right       cid:1 ψ=  v left      − W  =     R  v Wleft  cid:1 ψ 2    +   Now subtracting the two above equations yields  so we obtain for the angular rate of the robot   Solving for the instantaneous radius of curvature, we have:    1.7a       1.7b        1.8       VEHICLES WITH DIFFERENTIAL-DRIVE STEERING     7   This results in the expression for velocity along the robot ’ s longitu-  dinal axis:  =   cid:1 ψ R  =     v  y  v right  v left  − W  W v right 2 v right  + −  v left v left  v right  =  + 2  v tt lef       In summary, the equations of motion in robot coordinates are:  or  or ﬁ nally                             and  and  =  R      v right  v left  +  W 2    v left − W  =     R  W v right 2 v right  + −  v left v left          vx = 0     v right  =     v  y  + 2  v left       v right       cid:1 ψ=  v left      − W  v right  v left      cid:1 x  = −  sinψ     v right      cid:1 y  =  cosψ     + 2 + v left 2  v right       cid:1 ψ=  v left      − W   If we convert to earth coordinates these become:    1.9        1.10a       1.10b       1.10c        1.11a       1.11b       1.11c                            and  and  8     KINEMATIC MODELS FOR MOBILE ROBOTS   As we did in the case for the robot with front - wheel steering, we may  wish  to  account  for  the  fact  that  velocities  cannot  change  instanta- neously. Thus, we would introduce as the control variables the velocity  rates:  right = 1         cid:1 v u  left = 2         cid:1 v u    1.12a       1.12b       The system of equations for this kinematic model is now ﬁ fth order.   Again  we  can  use  the  Euler  integration  method  for  obtaining  a   discrete - time model for this system of nonlinear equations,        x k    1 T x kT T        +  =  −  v right     kT  v left     kT     ψ sin    kT            1.13a           y k  +      1 T  =    y kT T     +  v right     kT  v left     kT     cos  ψ    kT            1.13b        ψ     k  +      1 T  =  ψ    kT T     +  v right     kT  v left     kT             v  right      k  +      1 T  =  v right     kT Tu kT           1       +    1.13c       1.13d     +   2 +   2  −   W     v  left      k  +      1 T  =  v left     kT Tu kT           2       +    1.13e       More  sophisticated  and  more  accurate  methods  for  obtaining  discrete - time  models  exist;  however,  this  Euler  model  may  be  quite  useful if the sampling interval is set sufﬁ ciently small. These discrete -  time models may be used for system analysis, for controller design, for  estimator design and for system simulation. More complex models for  mobile robots could also include pitch, roll and vertical motion.     EXERCISES          1.     A  front - wheel  steered  robot  is  to  turn  to  the  left  with  a  radius  of  curvature equal to 20 meters. The robot is 1 meter wide and 2 meters  long. What should the steering angle be?      REFERENCES     9      2.     A differential wheel steered robot is to turn to the left with a radius  of curvature equal to 20 meters and is to travel at 1 meter per second.  The width is 1 meter and the length is 2 meters. What should be the  velocities of the right side and the left side?         3.     Using  the  discrete - time  model  presented,  perform  a  digital  simulation  of  the  front - wheel  steered  robot  using  a  steering  angle of forty ﬁ ve degrees, a length of 1.5 meters and a speed of 2.778  meters per second. Experiment with the sample interval, T and ﬁ nd  the maximum allowable value that yields consistent results.         4.     Develop a digital simulation for the steered wheel robot modeled  in Chapter  1 . Assume that the width from wheel to wheel is 1 meter  and that the length, axle to axle is 2 meters. A sequence of speeds  and steering angles will be inputs. Include limits in your model so  that steering angle will not exceed  +  or  − 45 degrees regardless of the  command. Simulate the robot for straight line motion and for motion  when  the  steering  angle  is  held  constant  at  45  degrees  and  then  constant at  − 45 degrees. Simulate several seconds of motion. Use the  Euler  formula  for  integration  and  experiment  with  the  sampling  interval. Then use a sampling interval of 0.1 second and see if this  sampling interval yields correct results. Plot x vs t, y vs t, heading vs  t, and y vs x.         5.     Develop a digital simulation for the differential drive robot, modeled  in Chapter  1 . Assume that the width from wheel to wheel is 1 meter  and that the length, axle to axle is 2 meters. A sequence of right side  speeds and left side speeds will be the inputs. Simulate for straight  line motion and for motion when the right side speed is 10% above  the  average  speed,   right  speed    +    left  speed  2,  and  the  left  side  speed is 10% below the average speed. Simulate several seconds of  motion. Use the Euler formula for integration and experiment with  the sampling interval. Then use a sampling interval of 0.1 second and  see if this sampling interval yields correct results. Plot x vs t, y vs t,  heading vs t, and y vs x.          REFERENCES       Carlos  Canudas    de    Wit  ,    Bruno    Siciliano  ,    Georges    Bastin     eds ,   “  Theory  of   Robot Control  ” , Springer  1996 .        Corke ,   P.I.  ;    Ridley ,   P.  ,   “  Steering  kinematics  for  a  center - articulated  mobile  robot  ” , Robotics and Automation, IEEE Transactions on vol. 17, no. 2,  2001 ,  pp.  215  –  218 .     10     KINEMATIC MODELS FOR MOBILE ROBOTS      Farbod   Fahimi  ,  “  Autonomous Robots: Modeling, Path Planning, and Control  ” ,   Springer.        Gregory    Dudek  ,    Michael    Jenkin  ,   “  Computational  Principles  of  Mobile   Robotics  ” ,  Cambridge University Press ,  2000 .        Hartley ,  Tom  ,   Guy O.   Beale   and   Stephen P.   Chicatelli  ,  “  Digital Simulation of   Dynamic Systems  ” , Prentice Hall,  1994 .        Kansal ,   S.      Jakkidi ,   S.    and    Cook ,   G.    “  The  Use  of  Mobile  Robots  for  Remote  Sensing  and  Object  Localization  ”    Proceedings  of  IECON  2003 ,   pp   279  –   284 , Roanoke, Va, USA, Nov. 2 – 6,  2003  .        Indiveri ,   G.     An  Introduction  to  Wheeled  Mobile  Robot  Kinematics  and   Dynamics . In  Robocamp? Padeborn  Germany  , April 8  2002 .                2   MOBILE ROBOT CONTROL           2.0      INTRODUCTION    This  chapter  is  devoted  primarily  to  the  steering  control  of  wheeled  mobile  robots,  with  minor  attention  also  devoted  to  speed  control.  Different tools of control theory are applied here with attention given  to various measures of performance including stability. A local coordi- nate system with quite general applicability is introduced. The chapter  concludes with a section on minimal path length trajectories.      FRONT - WHEEL STEERED VEHICLE,      2.1   HEADING CONTROL    Now  that  mathematical  models  have  been  developed  for  the  mobile  robot,  several  controllers  for  the  speed  and  direction  of  the  mobile  robot will be proposed and analyzed. Performance, including stability  and robustness is of greatest interest. First, heading control of the front -  wheel steered robot will be addressed. In the following, for simplicity  of notation we set:  Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  11   12     MOBILE ROBOT CONTROL   The desired heading may be given directly as a command:  rear wheel =    v  V         ψdes  =  specified heading       This direction could arise from a predetermined trajectory or it could  be  designated  by  a  sensor  that  detected  something  of  interest.  The  desired direction could also be computed in terms of the current loca- tion  and  the  coordinates  of  a  destination  if  one  does  not  have  to  be  concerned for obstacles. The direction from the current robot position  to the destination may be expressed as:     ψdes  = −  − tan 1     x des y des  − −     x y       Initially  it  will  be  assumed  that  by  some  means  a  desired  heading  has been established and the goal will be to aim the robot in that direc- tion. Several steering algorithms will now be presented.    First assuming that the steering angle may be commanded directly   we could choose as our expression for it:     α  =  ψ ψ          K des  −    2.1     which is illustrated in Figure  2.1 .    The steering angle here is proportional to the error in heading, i.e.,  linear  control.  Making  the  approximation    tan   α α≈   yields  for  the  robot closed - loop heading equation:         cid:1 ψ  =  tan  α α    ≈  V L  V L  V L       cid:1 ψ  =  ψ ψ    K des  −           2.2                        or   Assuming a ﬁ xed velocity, the error in heading goes to zero expo- nentially. The speed of convergence is determined by the time constant,  c =     T L KV  .    FRONT-WHEEL STEERED VEHICLE, HEADING CONTROL     13  Example of Linear Control for K = 0.5  l  e g n A   g n i r e e t S  0.5  0.4  0.3  0.2  0.1  0  -0.1  -0.2  -0.3  -0.4  -0.5  -1  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  1  Error in Heading Angle       Figure 2.1         α versus      ψ ψdes −  for   α    =  ψ ψ    K des  −    and   K = 0 5.      Another control algorithm for consideration is     α π ψ ψ =    sgn   −  des       4    2.3     which is illustrated in Figure  2.2 .    Here the steering angle steers in the proper direction with a ﬁ xed  steering angle of 45 degrees. Again using the approximation   tan   α α≈   yields         cid:1 ψ α= V    L       cid:1 ψ π ψ ψ        sgn   =  −  des  V L  4    2.4               or   14     MOBILE ROBOT CONTROL  Example of fixed Steering Angle Control with proper direction  l     e g n A g n i r e e S  t  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  -1  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  1  Error in Heading Angle       Figure 2.2         α versus       for   α π ψ ψ ψ ψdes −         sgn   −  =  des  4   The error in heading angle goes to zero as a ramp with slope   V  Lπ  4    as is shown in Figure  2.3 .      Figure  2.4  shows the behavior of the steering angle versus time.     While this rapid convergence of the heading error to zero is desir- able, when the error becomes very small, the slightest bit of noise or  dynamic  lag  will  cause  the  steering  angle  to  switch  back  and  forth  between   ±π  4 in what is called chattering. This is an undesirable feature  as it causes wear on the steering mechanism and results in inefﬁ cient  longitudinal motion of the robot.    As an alternative aimed at preserving the best features of the two   algorithms above, one may combine them into a single algorithm     α π  =       sgn   4  ψ ψ    −  des  whenever K  ψ ψ π 4  −  >  des         2.5a        α  =  ψ ψ  ,    −  des  K  whenever K  ψ ψ π 4  <  −  des         2.5b            and   Plot of Error in Heading Angle vs Time  0.2  0.4  0.6  0.8  1.2  1.4  1.6  1.8  2  1  Time       Figure 2.3     versus Time for   α π ψ ψ ψ ψdes −                sgn   −  =  des  4  Plot of Steering Angle vs Time  l  e g n A   g n d a e H n     i  i   r o r r  E  l     e g n A g n i r e e S  t  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  0  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  0  0.2  0.4  0.6  0.8  1.2  1.4  1.6  1.8  2  1  Time       Figure 2.4         α versus Time for   α π ψ ψ       sgn   =  −  des  4  15   16     MOBILE ROBOT CONTROL  Plot of Steering Angle vs Error in Heading Angle using Eqn 3.5, K = 2  l     e g n A g n i r e e S  t  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  -1  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  0.8  1  Error in Heading Angle       Figure 2.5         α versus      ψ ψdes −  for the Control Algorithm of Equation   2.5   with   K = 2       This control algorithm is illustrated in Figure  2.5 .     Inserting  this  steering  algorithm  into  the  linearized  equation  for   heading rate yields       cid:1 ψ π ψ ψ    sgn   =  −  des  V L  4  whenever K  ψ ψ π 4  −  >  des         2.6a          cid:1 ψ  =  ψ ψ      −  des  K  whenever K  V L  ψ ψ π 4  −  <  des         2.6b       The steering angle steers in the proper direction with a ﬁ xed steering  angle  and  the  error  in  heading  initially  diminishes  as  a  ramp.  Then  as the gain times the error in heading becomes less than   π  4 radians,  the algorithm reverts to linear control with steering angle proportional  to error in heading, i.e., proportional to   ψ ψdes − . The error in heading  then has exponential decay and does not chatter. Figures  2.6  and  2.7          and   Plot of Steering Angle versus Time  0  0  1  2  3  4  6  7  8  9  10  5  Time       Figure 2.6         α versus Time for   ψ   0  0=  and   ψ πdes =   2, K    =    2.0    Plot of Heading Angle versus Time  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  1.6  1.4  1.2  1  0.8  0.6  0.4  0.2  l     e g n A g n i r e e S  t  l     e g n A g n d a e H  i  0  0  1  2  3  4  6  7  8  9  10  5  Time       Figure 2.7         ψ versus Time for   ψ   0  0=  and   ψ πdes =   2, K    =    2.0    17   18     MOBILE ROBOT CONTROL  illustrate  this.  Note  that  the  steering  control  is  saturated  for  the  ﬁ rst  three seconds and then becomes proportional control. Note also that  the  heading  angle  is  a  ramp  for  the  ﬁ rst  three  seconds  and  then  approaches  the  desired  heading  with  exponential  error  decay.  Here    V = 1 and   L = 2.      Care must be used when basing the control on an angle to ensure  that the control action for a given angle is consistent even if the expres- sion  for  the  angle  exceeds    2π  in  magnitude.  For  the  above  steering  −ψ ψ.  If  angle  algorithm  one  may  create  a  variable,    angle error − 2π. On the other hand if    angle error + 2π. The control action    angle error . is then based on this new expression for   angle error  > 2π ,  set   angle error = < −2π, set   angle error  angle error =  angle error  =  . .  des  .   .  .  .  .  .     EXAMPLE 1     Demonstrate the steering strategy just described for the case of driving  π the mobile robot to a destination in the x - y space. Let    ψ 2 0=  ,  and     y   0   x   0 gain of K     =    2.0.    ,  0=  . The  destination  is     xdest = 10 ,  and     ydest = 10 .  Use  a   = −     0    SOLUTION 1     Here  the  desired  heading  varies  as  it  is  computed  based  on  the  i.e.,   current    ψdes  . The steering control described  above is then implemented and the motion of the robot simulated. The  results are shown in Figures    2.8a  ,   2.8b    and    2.8c  .       the  mobile  robot  and         x t  location  of  − tan [   the  destination,       ] y t  y dest  = −      t  −  dest  −1  x   This  steering  strategy  works  ﬁ ne  as  long  as  all  disturbances  occur  before the mobile robot gets within a distance of   L   tan maxα  from the  destination. Disturbances after that cannot be accommodated because  of the ﬁ nite turning radius of the robot.    We  now  assume  that  the  steering  angle  alpha  may  not  be  com- manded  directly  but  rather  that  its  rate  is  the  controlled  variable.  Setting the rate of the steering angle proportional to error in heading  yields         cid:1 α  =  ψ ψ          K des  −    2.7      l     e g n A g n i r e e S  t  l     e g n A g n d a e H  i  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  -0.7  -0.8  -0.9  -1  -1.1  -1.2  -1.3  -1.4  -1.5  -1.6    0  1  2  3  4  6  7  8  9  10  5  Time       Figure 2.8a       Steering Angle   α versus Time       Desired Heading Angle Actual Heading Angle  1  2  3  4  6  7  8  9  10  5  Time       Figure 2.8b       Desired Heading and Actual Heading versus Time    19   20     MOBILE ROBOT CONTROL  e  t  i  a n d r o o c   y  7  6  5  4  3  2  1  0 0  or           1  2  3  4  5  6  7  8  x coordinate       Figure 2.8c       Resulting Trajectory, y versus x    which  again  is  linear  control.  Taking  the  equation  for  heading  rate  under the assumption of small steering angle and differentiating yields  ψ α= V     cid:1  cid:1   cid:1       L      cid:1  cid:1 ψ  =  K  ψ ψ    V L des  −           2.8       2.9       Unfortunately this control algorithm yields imaginary poles leading  to  sustained  oscillations.  This  may  be  seen  by  applying  the  Laplace  Transform to the above equation yielding     ψ      s  =     KV L +2   s KV L  ψ      s      des                  FRONT-WHEEL STEERED VEHICLE, HEADING CONTROL     21   Introducing an additional term into the algorithm, this time a feed- back  term  in  the  steering  angle  itself,  we  consider  as  a  steering  algorithm      cid:1 α  =  ψ ψ      −  des  −  K  1  K  2  α      V L    2.10       The steering angle rate is proportional to heading error and steering   angle. Using the approximate expression for heading rate,       cid:1 ψ α= V ,    L  the above equation for the steering angle rate may be re - expressed as      cid:1  α  =  ψ ψ      −  des  −   cid:1  ψ     2  K  K  1    2.11       The  ﬁ nal  expression  for  the  steering  algorithm,  equation    2.11  ,  reveals that in fact the additional term in the control algorithm adds  damping  through  rate  feedback.  For  implementation  purposes  it  is  preferable  to  use  the  former  expression,  equation    2.10    which  is  in  terms of steering angle since the latter expression would require dif- ferentiation of heading angle.  for    cid:1  cid:1 ψ above yields   Using this control algorithm which contains damping in the equation       cid:1  cid:1  ψ  =  K  1  ψ ψ      −  des  −  K  2  V L   cid:1  ψ      V L    2.12     which corresponds to     ψ      s  =     1       K V L +              K V L s K V L     2  1  +  2  s  ψ      s      des   By  proper  choice  of    K1  and    K2  one  can  achieve  any  desired  pole  locations. In implementing the above, one must use care to ensure that  neither  the  maximum  steering  angle  rate  nor  the  maximum  steering  angle is exceeded. Also it should be kept in mind that the performance  analysis has been done under the assumption of sufﬁ ciently small steer- ing angles allowing one to make the approximation,   tan   α α≈ .     22     MOBILE ROBOT CONTROL     2.2      FRONT - WHEEL STEERED VEHICLE, SPEED CONTROL    So far the focus has been on controlling the heading of the mobile robot  for steering purposes. Additionally one must control the robot speed.  One way to select the desired speed for the robot would be in terms  of distance to the destination and time remaining, i.e.,     V  des  =     x  des  −  2     + x time  to go     y des  −  2    y       Now it is possible that the desired velocity would exceed the velocity  achievable by the robot. In that case one could modify the algorithm  for commanded velocity to become     V  des  =  min          x des     y des  −  2    y  −  2     + x time  to go  , max V              2.13       Here velocity is commanded to be  distance remaining   time - to - go   or   Vmax, whichever is less. i.e., the control strategy is saturating command.   The above expressions assumed that velocity could jump to the com- manded value instantaneously. A more realistic approach would be to  command a rate to achieve the desired velocity or to have the velocity  approach the desired velocity with a time constant. The expression for  the latter method    2.14     can be re - arranged to     τ cid:1 V V Vdes  =  +            cid:1 V  =  V  − des τ  V     min          x des  −  2     + x time     y des  −  2    y  , max V       −  V            cid:1 V  =  to go τ         2.15    Here desired velocity has been expressed as the saturating command,   distance remaining   time - to - go  or   Vmax, whichever is less. The veloc- ity approaches the desired value with a time constant of   τ.                  or   HEADING AND SPEED CONTROL FOR THE DIFFERENTIAL-DRIVE ROBOT     23   In all of these speed control possibilities, it may be of interest to also  compute the energy consumed. If one can determine the motor torque  required  to  achieve  the  motion,  then  since  torque  is  proportional  to  armature current, one would be able to compute the electrical power  consumed   current  times  voltage   and  integrate  this  to  ﬁ nd  energy  consumed. Energy management is especially important when operating  from a ﬁ nite energy supply such as that of a battery.    To  integrate  turn  and  velocity  control  for  the  front - wheel  steered  robot, one can simply have the robot turn as it travels longitudinally,  and the somewhat separate controls jointly affect the vehicle trajectory.  The actual path mapped out on the ground for a ﬁ xed steering angle  is independent of the robot speed. Therefore for many problems it is  reasonable to separate the two control problems. If there are obstacles  or if high speeds could cause the robot to skid during steep turns, then  steering and velocity controls must be coordinated.      HEADING AND SPEED CONTROL FOR THE      2.3   DIFFERENTIAL - DRIVE ROBOT    The robot with differential wheel control turns by using different wheel  speeds.  One  approach  to  steering  toward  a  particular  heading  would  be to rotate in place making use of the equation for heading rate  and picking  and              v right       cid:1 ψ=  v left    − W  right = max      v  V  left = − max      v  V  =     V  v right  v left     + 2  until  achieving  the  desired  heading.  Then  the  robot  could  proceed  forward toward the destination at the desired speed making use of the  fact that   24     MOBILE ROBOT CONTROL  and using  and also     v  right  =  V  desired        v  left  =  V  desired                           and  and   For  a  commanded  turn  in  place  at  a  rate  that  is  lower  than  the  maximum turn rate, one may determine the appropriate wheel veloci- ties using       cid:1 ψdesired  =  v right desired  v left desired       − W  and solving to obtain    2.16       2.17a       2.17b         v  right  W  =   cid:1 ψ desired 2       W  = −     v  left   cid:1 ψ desired 2        This combination achieves the desired turn rate and also causes the  longitudinal  velocity  to  be  zero  during  the  turn. After  achieving  the  desired heading, one may use an algorithm of the type used before     v  right  =  min     x des     y des  −  2    y  −  2     + x time  to go  , max V         2.18a        v  left  =  min     x des     y des  −  2    y  −  2     + x time  to go  , max V    2.18b       These wheel velocities yield the correct robot velocity and also main- tain  straight - line  motion.  Upon  reaching  the  destination,  one  could  then rotate in place to obtain the desired ﬁ nal heading. If one wishes                             HEADING AND SPEED CONTROL FOR THE DIFFERENTIAL-DRIVE ROBOT     25  to combine turning of the robot with longitudinal motion, then some  strategy must be ﬁ rst used to determine the desired turn rate and the  desired velocity. Once these have been obtained, the equations for turn  rate and longitudinal velocity can be combined to yield as solutions     v  right  =  V  desired  W  +   cid:1 ψ desired 2          v  left  =  V  desired  W  −   cid:1 ψ desired 2         2.19a       2.19b       It is easy to show that this combination satisﬁ es both the desired turn   rate and desired longitudinal velocity.    The  following  compares  two  contrasting  strategies  for  reaching  a  destination.  One  is  to  spread  the  rotation  uniformly  over  the  entire  trajectory and turn while moving at the maximum speed   turn - while -  traveling  . The other is to turn in place at maximum rotational speed  and then travel at maximum speed to the destination   turn - then - travel  .  Let the initial heading offset be given by   ∆ψ  2 and the distance between  the initial and ﬁ nal points be given by   D. This is shown in the construc- tion presented in Figure  2.9 .      For the  turn - while - traveling  strategy the path is a circle. From the law   of sines one can determine the relationship between   R and   D to be     R D=     sin   2  ∆ψ         2   The travel time can be found by realizing that the outer wheels travel  2 ∆ψ. Thus the travel time is      at speed   Vmax and travel a distance      R W+  and           ∆y  2   Initial     point   Final point  D  ∆y  R       Figure 2.9       Construction for Evaluating Travel Time     26     MOBILE ROBOT CONTROL  turn while traveling =    T  ∆ ψ  2         +  R W V  max  =  ∆ ψ W 2 V  max  +  ∆ ψ R V  max       Note that the ﬁ nal heading of the mobile robot is   −∆ψ  2.   For  the   turn - then - travel   strategy,  including  turning  to  the  same  heading at the end of the trajectory as the  turn - while - traveling  strategy,  the total time required is  turn then travel =    T  ∆ ψ   W 2 V  max  2  +  D V  max  +  ∆ ψ   W 2 V  max  2  =  ∆ ψ W 2 V  max  2  R  +  sin    V  max  ∆ψ 2           The ﬁ rst terms in each expression are identical, but the last term is  smaller for the case of the  turn - then - travel  strategy. The greater is   ∆ψ  2,  the greater this difference will be.      REFERENCE TRAJECTORY AND INCREMENTAL      2.4   CONTROL, FRONT - WHEEL STEERED ROBOT    At this point, we shall examine the mobile robot control problem using  a  method  called  incremental  control  about  a  reference  trajectory.  Rather than robot heading control as was just addressed, here the goal  will be to steer the robot so as to cause it to move along a speciﬁ ed  reference  trajectory.  The  equations  of  motion  for  the  front - wheel  steered  robot  with  rear - wheel  drive  are  repeated  below  for  convenience.                 and      cid:1 x     cid:1 y V=  V= − sinψ  cosψ        cid:1 ψ  = V L  α tan      Here we are using the simpliﬁ ed third order model which assumes  that the velocity and steering angle can be directly controlled. As was  already noted, the dynamic equations are seen to be nonlinear, whereas  most of the theory for control design is based on linear systems. One  approach to controlling a nonlinear system is to ﬁ rst deﬁ ne a reference    REFERENCE TRAJECTORY AND INCREMENTAL CONTROL     27  trajectory satisfying the overall objectives of the problem at hand. This  trajectory is comprised of values of the system state and control at all  points  between  the  initial  and  ﬁ nal  conditions.  These  values  for  the  reference trajectory will be denoted with the subscript r.   Next we subtract the equations describing the reference trajectory   from the original equations.  r  = − sinψ         cid:1 x V r = cosψ         cid:1 y Vr  r  r  r       cid:1 ψ  r  =  tan  α r       V r L  r      cid:1   cid:1  x x     cid:1   cid:1  y y V  = − =  − −  r  V  ψ + sin ψ −  sin  V r cos  r  ψ     ψ      r  V r  cos       cid:1   cid:1 ψ ψ r  −  =  α −  tan  tan  α r       V L  V r L    2.20a       2.20b       2.20c        2.21a       2.21b       2.21c       Next we expand each term in the equations above in a Taylor Series  about the reference values and drop all the terms of second order and  higher. Deﬁ ning the deviations from the reference values as   δ we obtain  an incremental model      δ  cid:1 x  = −  V  sin  ψ  +  V r  sin  ψ r  ≈ −  V r  sin  ψ δ V  −  r  sin  ψ r  −  V r  cos  ψδψ  +  r  V r  sin  ψ r     ≈ − δ     δ  cid:1 x V      δ =  cid:1 y V  cos  sin ψ  ψ −  − Vr r cos V r  cos ψ r  ψδψ ≈  r V r      cos  ψ r  −  V r  sin  ψδψ δ V  +  r  cos  ψ r  −  V r    2.22a    cos  ψ r        δ δ  cid:1 y V  ≈  cos  ψ  − Vr r  sin  ψδψ  r         2.22b                       and  and  or  or      28     MOBILE ROBOT CONTROL  and  or     as         cid:1  =     δψ  α −  tan  tan  α r  ≈  tan  α r  +  V L  V r L  V r L  V r cos  2  δα δ V α L  +  L  tan  α r  −  tan  α r     V r L   cid:1  ≈    δψ  V r cos  2  δα δ V α L  +  L  tan  α r         2.22c       Having retained only the ﬁ rst order terms, these can be re - arranged   δ  cid:1    x   δ  cid:1      y    cid:1    δψ    =        0 0 0 0 0 0  − V r − V r  ψ r ψ r  cos sin 0        δ   x   δ y     δ ψψ    +          − ψ sin r ψ cos r α tan r L          0 0 V r 2 cos  L  α r  δ   V       αα δ    2.23       The  terms  within  the  coefﬁ cient  matrices  in  the  above  equations  would  be  evaluated  along  the  reference  trajectory.  This  incremental  model is equivalent to a time - varying linear system.  δ  cid:1    x   δ  cid:1  y        cid:1    δψ    = [  ]      A t  δ   x   δ y     δψ    +[      B t  δ  ] V      δα    If the coefﬁ cient matrices remain constant, then it becomes a time -   invariant linear system. δ  cid:1    x   δ  cid:1  y        cid:1    δψ       = [  ]  A  δ   x   δ y     δψ    +[  B  δ ]  V      δα   In this case one can utilize the theory of linear control to design a  controller to maintain the mobile robot near the reference trajectory.  Note that for all coefﬁ cients of   A and   B to be constant   Vr,   ψr and   αr  must be constant implying that   αr is zero. A very simple linear control  algorithm for the above would be     δ V  δ = − 1 K y      REFERENCE TRAJECTORY AND INCREMENTAL CONTROL     29  and                    or     δα  =  δ 2  K x K  −  δψ 3       Here the positive dependence of   δα on   δx is selected because the   x   axis is positive to the right and positive   α steers to the left.    The above equations then become in closed loop  δ  cid:1    x   δ  cid:1      y    cid:1    δψ    =          0 0 V r L  K  2  K 1 − K  1  ψ sin r ψ cos r  00  ψ − cos V r r ψ − sin V r r V 3− r K L          δ   x   δ y     δψ           2.24       One  may  now  determine  the  eigenvalue  equation  for  the  above   matrix  λ 3      + +  1    cos K 2   K V r  2  ψ r cos  + ψ r     λ 2   K V L    3 L K K V r  r +  1  3  cos  ψ λ +    L     r     K K V  1  2  rr L2       =   0  and compare it to that corresponding to the desired eigenvalues  −  λ λ 1        des  −  λ λ 2      des  −  λ λ 3      des  =     0     λ 3      −  λ 1  + − λ   2 λ λ λ − 3 1  des  des  des  2  des  des  2     +  λ λ λ λ − 2 3 = 0  des  des     1  +  λ λ 3 1  des  des  +  λ λ 3 2  des  dess  des   λ      By equating the coefﬁ cients of these two equations one can solve for  the   Ki ’ s. It should be clear that by the proper choice of the   Ki ’ s one can  achieve any desirable eigenvalues. The equations for the incremental  model  simplify  considerably  for  the  case  where  the  trajectory  is  a  straight line along the y axis. Here both   ψr = 0 and   αr = 0 causing the  equations to become  δ  cid:1    x   δ  cid:1      y    cid:1    δψ    =        0 0 0 0 0 0  − V r 0 0        δ   x   δ y     δψ    +          00 1 0          0 0 V r L  δ   V δα          2.25       30     MOBILE ROBOT CONTROL   It should be noted that for any straight - line reference path, one could  translate and rotate the axes so that the reference path would be along  the y axes and the assumptions of   ψr = 0 and   αr = 0 would be satisﬁ ed.  We use the same simple linear control algorithm as was used above   The  coefﬁ cient  matrix  for  the  closed - loop  incremental  system     2.26a       2.26b        2.27         δ V  δ = − 1 K y          δα  =  δ 2  K x K  −  δψ 3       δ  cid:1    x   δ  cid:1      y    cid:1    δψ    =          0 0 K V 2 r L  0 K  1  −  0  −          − V r 0 K V 3 r L  δ   x   δδ y     δψ         λ +        K  1     λ 2  +     K V 3 r L  λ +  2 K V 2 r L    =  0      The  associated  eigenvalue  equation  for  the  closed - loop  coefﬁ cient   matrix becomes   It is easy to show that the solutions will all lie in the left - half plane  as  long  as  all  the  gains,    K1,    K2,  and    K3  are  positive  and  that  one  can  choose these coefﬁ cients to place the eigenvalues wherever one pleases.   Alternatively, one can analyze the stability of this control algorithm  by utilizing Lyapunov Stability Theory. The linearized equations under  the  stated  conditions  for  the  simpliﬁ ed  case  are  repeated  for  convenience  and  becomes                       and  δψ        δ ≈ −  cid:1 x Vr V≈    δ δ cid:1 y        δψ   cid:1  ≈ V r L  δα        2.28a       2.28b       2.28c      REFERENCE TRAJECTORY AND INCREMENTAL CONTROL     31   Now using the control algorithm previously introduced,     δ V  δ = − 1 K y        δα  =  δ 2  K x K  −  δψ ,   3     δ  cid:1 x    δ  cid:1 y  δψ ≈ − Vr ≈ − 1 δ K y            we have for the closed – loop system equations   cid:1  ≈    δψ  K V 2 r L  δ x  −  K V 3 r L  δψ        Now taking as a Lyapunov function     LF  =  δ a x  [  2  +  δ b y  2  +  δψ     ] c  2    2.29     1 2  with a, b and c all being positive, we have        d LF dt         =  δ a x V r  −     δψ δ  δ   b y K y 1  −  +        +   δψ c   K V 2 r L  δ x  −  K V 3 r L   δψ         d LF dt         = −  δ δψ  aV x  r  −  δ bK y 1  2  +  cK V r  2 L  δ δψ x  −  c  K V 3 r L  δψ  2       d LF dt         = −  δ bK y 1  2  +     cK L  2  −   a V x   r  δ δψ  −  c  K V 3 r L  δψ      2    2.30      and  and                          or  or   32     MOBILE ROBOT CONTROL   By taking  =     a  2    cK L                 and  which implies that   K2 is positive, this reduces to     d LF dt         = −  δ bK y 1  2  −  c  K V 3 r L  δψ      2    2.31       which is negative semi - deﬁ nite for positive values for   b,   c,   K1 and   K3.   is zero would be when   δx  The only nonzero states for which   d LF dt is nonzero and both   δy and   δψ are zero. The control action guarantees  that for the closed loop system this is not an equilibrium state. Thus the  system is proven to be asymptotically stable within the accuracy of the  linearized model.             HEADING CONTROL OF FRONT - WHEEL STEERED      2.5   ROBOT USING THE NONLINEAR MODEL    It is of interest to study the stability of steering control laws without  using any linear approximations of the equations of motion. Here we  will again assume that one may manipulate the steering angle   α directly.  The desired heading is taken to be the direction from the current robot  position to a desired ﬁ xed location. It is measured with respect to the  y  axis  in  the  counter - clockwise  direction.  In  addition  to  retaining  the nonlinearities of the robot model, we also incorporate the fact that  the desired heading angle changes with robot motion. The expression  for the desired heading is     ψdes  =  − 1  tan     − −     x x des −     y y des          Here it is seen that the   x term enters as a negative since   x is deﬁ ned  positive to the right and   ψ is deﬁ ned positive in the counter - clockwise  direction or to the left. Note that the coordinates may be shifted so that  the desired location is at the origin of x - y space. Deﬁ ne  = −     X x xdes       = −     Y y ydes         2.32a       2.32b       HEADING CONTROL OF FRONT-WHEEL STEERED ROBOT     33   Noting that for a ﬁ xed desired location  we obtain in the new coordinates the same model as before      cid:1 xdes = 0        cid:1 ydes = 0         cid:1 X      cid:1 Y V=  V= − sinψ  cosψ        cid:1 ψ  = V L  α tan       ψdes  =  − tan 1     X − Y             LF  =  ψ ψ        −     des  2  1 2  LF < 0     d     dt  where the desired location in these coordinates is now the origin. The  expression for the desired heading angle as given above reduces to   We now choose as a Lyapunov - like function   We say  “ Lyapunov - like ”  because the function is not positive deﬁ nite  but rather positive semi - deﬁ nite. We now seek to determine a control  to guarantee that    2.33        2.34       Thus we examine  {  1 2  } =     d dt     ψ ψ    −  2     des  ψ ψ    −  des        d dt  ψ  −  d dt     ψ         des  and  and                              34     MOBILE ROBOT CONTROL   which is a measure of the rate at which the robot converges to the   1 desired heading. Since any error in heading causes    2  ψ ψ− des    2      to be   positive,  a  large  negative  value  for  its  derivative  would  indicate  fast  convergence to the desired heading.    Noting that  ψ     d  des     dt  =  2  cos  ψ  des   cid:1    cid:1  − YX XY  + 2 Y           a series of manipulations results in  ψ     d  des     dt  =  − V     cos  ψ  des  sin  ψ  +  V  sin  ψ  des  cos  ψ       1 + 2  X Y  2  ψ     d  des     dt  =  ψ ψ  −  sin           des    2.35      − V + 2  X Y  2   Using this result yields  {  1 2  } =        d dt     ψ ψ    −  2     des  ψ ψ    −     des  ψ    d  dt  +     V + 2  X Y  2  ψ ψ  −  sin   des         } =  ψ ψ    −  2     des  {  1 2     d dt      ψ ψ    −  des   sin   ψ ψ  −     des  2  V + 2 ψ ψ    X Y V − L  +   tan  des  α               2.36    It is clear that the ﬁ rst term is positive for heading errors less than    π in magnitude. Thus for stability the second term must be made nega- tive and larger in magnitude than the ﬁ rst. One solution is to select     α π = 4  ψ ψ         sign  −  des    2.37      or  or                  HEADING CONTROL OF FRONT-WHEEL STEERED ROBOT     35   Since   tan  π 4 1=  and  this results in  ψ ψ         −  des  ψ ψ ψ ψ   sign  =  −  −     des  des        d dt     ψ ψ    −  2     des  =  1 2  V + 2  X Y  2  ψ ψ    −  des   sin   ψ ψ  −  −     des  ψ ψ  −      des  V L    2.38     which is clearly negative as long as the robot is at a distance of more  than   L from the destination. Thus disturbances that occur before the  robot gets within L from the ﬁ nal destination can be accommodated.  Those that occur later cannot be accommodated. The controller here  is a bang - bang controller, which does a good job of getting the robot  headed in the proper direction quickly; however, as mentioned earlier,  it could cause chattering, or rapid switching of the control, after reach- ing the desired heading. Another solution is to take     tan  α  =  sin   ψ ψ    −des         2.39       This strategy results in  ψ ψ    −  2     des  =  ψ ψ    −  des   sin   ψ ψ  −     des  1 2     d dt       sin   des  ψ  −ψψdes               2.40   2  V + 2 ψ ψ    X Y V − L  −  which also is negative as long as the robot is at a distance of more than    L from the destination. Note that since  one will have       sin   ψ ψdes −     ≤ 1       α π≤ 4                      which  is  a  very  reasonable  range  for  allowable  steering  angles. This  controller outputs a smaller control signal as the heading error becomes  smaller.  It  thus  would  not  chatter  as  the  previous  controller  did,  but    36     MOBILE ROBOT CONTROL  neither  does  it  converge  to  the  correct  heading  as  quickly.  Another  solution would be to combine these algorithms and use     α π = 4  ψ ψ         sign  −  des    2.41a     whenever the heading error exceeds some given threshold and to use     tan  α  =  sin   ψ ψ    −des         2.41b       whenever the heading error is equal to or less than the threshold. This  combines the beneﬁ ts of the two previous controllers. It provides rapid  convergence toward the proper heading when the heading error is large  and also eliminates the chattering that would result from application  of the ﬁ rst algorithm alone.    These solutions steer the robot toward the destination. Disturbances  that  occur  at  distances  greater  than  L  from  the  destination  can  be  accommodated. The  ﬁ nal  orientation  of  the  robot  has  been  left  free  for  this  problem  with  the  objective  being  simply  to  keep  the  robot  headed  toward  the  destination.  Final  orientation  will  depend  on  the  initial  conditions  as  well  as  the  disturbances  that  occur  during  the  transition.      COMPUTED CONTROL FOR HEADING AND      2.6   VELOCITY, FRONT - WHEEL STEERED ROBOT    The  model  for  the  front - wheel  steered  robot  is  repeated  for  convenience.                 and       cid:1 X      cid:1 Y V=  V= − sinψ  cosψ        cid:1 ψ  = V L  α tan      Let us assume for this analysis that the steering angle and velocity  cannot be changed instantaneously and therefore we take as the control  variables   COMPUTED CONTROL FOR HEADING AND VELOCITY     37      cid:1 α= u1         cid:1 V u= 2     and  or                          or   i.e., we use the 5 th  order model for the robot. Here the steering system  may be made to behave as a second - order system with speciﬁ ed natural  frequency and damping ratio.    For this purpose   u1 is selected so as to achieve  ψ ξωψ ωψ ωψ      ′′ +  ′ +  =  2  2 n  2 n  n  des        2.42a     ψ ω ψ ψ ζωψ ′      ′′ =  −  −  2        des  2 n  n         2.42b     where the prime denotes differentiation with respect to distance trav- eled, s. The dynamic behavior of heading is speciﬁ ed in this way rather  than in terms of time because the actual turning motion for the front -  wheel steered robot is in fact dependent on distance traveled. Noting  that  ψ ψ   ′ =        d dt ds dt  =  α     tan L  ′′ =ψ      1   cid:1    α  α  LV cos  2  and that  the solution becomes   cid:1 α cos      LV  2  α  2= − [  ζω α ω ψ ψ  ]  tan    L  +  −     des  2 n  n     =     u LV  1  2  cos  α ζω α ω ψ ψ     [  ]  tan    − 2  L  +  −     des  2 n  n    2.43       Note  that  here  the  system  has  not  been  approximated  as  a  linear  system, but rather the nonlinearities have been retained in the model.    38     MOBILE ROBOT CONTROL  This type of controller is sometimes referred to as  “ computed control ” .  It  cancels  out  the  existing  dynamics  and  replaces  it  with  the  desired  dynamics. It assumes that one has a perfect model of the robot. This  equation for the control must be subjected to the respective constraints  on maximum steering angle rate and maximum steering angle, i.e.,   For speed control   u2 is selected so that the speed converges to the  desired  speed  according  to  the  solution  of  the  ﬁ rst  order  differential  equation,  and              or     u 1  1≤ u MAX        α α≤ MAX        τ cid:1 V V Vdes  =  +       2 =    u  V  des  − V τ        2.44a       2.44b       If the values for   ψdes and   Vdes are constant, then the control algorithms  just described guarantees a stable system as long as saturation does not  occur. When in the tele - operated mode, the expression for   ψdes may be  an  input  from  the  operator,  possibly  by  use  of  a  joystick. The  speed  command may be also be an input from the operator. Further analysis  would be required in the case of time varying   ψdes and   Vdes or if there  is saturation of the control variables.      HEADING CONTROL OF DIFFERENTIAL DRIVE      2.7   ROBOT USING THE NONLINEAR MODEL    It is of interest to analyze the stability of steering control laws for the  differential - steered  robot  also  without  linearizing  the  equations  of  motion. Here we will assume that one may directly manipulate the right  and left velocities. The analysis proceeds exactly as in Section  2.5  except  that one replaces the equation for heading angle rate and the equation  for velocity with those for the differential - drive model, i.e.,    Proceeding in a parallel fashion it can be shown that one stable solu-  tion with favorable properties would be to use  COMPUTED CONTROL FOR HEADING AND VELOCITY     39  tanα←  v right  v left     − W  V     L  ←     V  v right  v left      + 2  v left  v right      − W  =  ψ ψ         sign  −  des  v right      + 2  v left  =  V  desired       v left  v right      − W  =  sin   ψ ψ        −  des  v right      + 2  v left  =  V  desired         2.45a       2.45b       2.46a       2.46b      and  and  and                    whenever the heading error exceeds some given threshold and to use   whenever the heading error is equal to or less than the threshold. The  solutions to the two equations would yield the required velocities for  each  side.  This  control  strategy  does  not  induce  chattering  and  yet  provides rapid convergence when heading error is large. Simultaneously  it provides the speciﬁ ed velocity.      COMPUTED CONTROL FOR HEADING AND      2.8   VELOCITY, DIFFERENTIAL - DRIVE ROBOT    For  convenience  we  repeat  here  the  model  for  the  differential - drive  robot for the case where we take the rate change of wheel velocities as  the control inputs.   40     MOBILE ROBOT CONTROL  v left  sinψ   v left  cosψ   v left    =      cid:1 x      cid:1 y  = −  v right  v right  + 2 + 2 − W right = 1       cid:1 v u       cid:1 ψ=  v right  left = 2        cid:1 v u      cid:1  cid:1  ψ ζωψ ωψ ωψ  =  +  +  2   cid:1   2 n  2 n  n  des       τ cid:1 V V Vdes  =  +       The steering system here may also be made to behave as a second -  order system with speciﬁ ed natural frequency and damping ratio. Here    u1 and   u2 are selected so as to achieve   Proceeding  in  a  parallel  fashion  as  in  the  case  of  the  front - wheel   steered robot we arrive at the conclusions that  u 1      − u 2 W  ζωψ ω ψ ψ = − 2  ] [  +  −      cid:1   des  2 n  n         2.47a     u 1      + 2  u 2  =  1 τ  V  −     v right  +  des  v left            2    2.47b     =     u 1  V  des  −     +  v left      2  v right τ  −  ζω n     v right  −  v left     +  W ω ψ ψ     2  −  des        2 n    2.48a                                   and  and  and  or   STEERING CONTROL ALONG A PATH     41  and     =     u 2  v des  −     +  v left      2  v right τ  +  ζω n     v right  −  v left     −  W ω ψ ψ     2  −  des        2 n    2.48b       This illustrates the application of computed control to the differential -  drive robot. It assumes a perfect model for the robot, and provides a  control to cancel the nonlinearities. It then further includes the neces- sary terms to give the speciﬁ ed behavior.      STEERING CONTROL ALONG A PATH USING A LOCAL      2.9   COORDINATE FRAME    The objective of the approach to be discussed here is tracking along a  curved  path  without  prior  knowledge  of  the  path. This  is  in  contrast  with the reference trajectory approach where prior knowledge of the  path is presumed and the equations were linearized. It also differs from  the  situation  where  one  heads  toward  a  particular  destination  and  is  unconstrained by the need to stay on a path. The local coordinate frame  to be used here is deﬁ ned such that the   x axis of the local frame passes  through the robot and is normal to the path. The origin is on the path  with coordinates designated as   xpath and   ypath, and the   y axis is tangent  to the path at the origin and is pointed in the direction of the desired  travel. See Figure  2.10 . As the vehicle moves along near the path, the  origin of the coordinate system moves along the path maintaining the  relationship described between the robot position and the position and  orientation of the local coordinates.      The coordinates of the path itself are given as functions of the dis- tance which the origin of the local coordinate system has traveled along  the path as the robot moves, i.e.,           and  where     x  path  =  x     s  path  path            y  path  =  y     s  path  path            ds  path     dt  =   cid:1  x  2 path  +   cid:1  y  2 path         2.49a       2.49b       2.49c      42     MOBILE ROBOT CONTROL  y-local   y  Path  y-earth  x-local  *  Robot Position  Path  x-earth      Path Being Followed by Mobile Robot and Deﬁ nition of Local Coordinate        Figure 2.10   Frame    and  mation as                    or   and     tan  ψpath  = −  cid:1  x  path   cid:1  y          path    2.49d       The local coordinates of the robot are given by a rotation transfor-          x  y Local Coords  =     ψ path ψ  cos − sin  path  sin cos  path  ψ  ψ    path     − −  x x y y  path     path Earth Coords         2.50        x     y  local  local  [  − = x x = − −  [  x x  ]cos  path  ]sin  path  ψ path ψ  path  + [ +  − y y −  y y  [  ]sin  path  ]cos  path  path  ψ    ψ     path     ψ  local  =  ψ ψ  −      path   Now by the deﬁ nition of the coordinate system,   ylocal  is zero, i.e.     − − [ x x  ]sin  path  ψ  path  +  −  y y  [  ]cos  path  ψ  path  =  0      STEERING CONTROL ALONG A PATH     43   Further, since it is always zero, its derivative with respect to time is   also zero, i.e.,     d dt     − − { [  x x  ]sin  path  ψ  path  +  −  y y  [  ]cos  path  ψ  path  =  }  0      Evaluating the above differential and using the expressions for    cid:1 x and  tanψ       cid:1 y as well as the fact that    cid:1 xpath may be written as    cid:1  x permits one to solve for    cid:1 ypath.  = −  path  path  path   cid:1  y      cid:1  y  path  =    V  cos  ψ  −  x   cid:1  ψ  local  local  path   cos  ψ      path    2.51a       From this result one can then obtain    cid:1 xpath as      cid:1  x  path  = −   V  ψ  cos  −  x   cid:1  ψ  local  local  path   sin  ψ      path    2.51b       Now differentiating the equation for   xlocal yields   cid:1  x      local  =  −  cid:1   cid:1  [ x x path − − [ x x  ]cos ]s  +  −  ψ  cid:1   cid:1  [ y y path path  cid:1  ψ ψ + iin [  ]sin − y y  path  path  path  ψ  path ]cos   cid:1  ψ ψ  path  path     path  which is recognized as      cid:1  x  local  =  −   cid:1   cid:1  x x  [  ]cos  path  ψ  path  +  −   cid:1   cid:1  y y  [  ]sin  path  ψ  path  −  y local   cid:1  ψ tth    pa      cid:1  x  local  =  −   cid:1   cid:1  x x  [  ]cos  path  ψ  path  +  −   cid:1   cid:1  y y  [  path  ]sin  ψ    path  since the deﬁ nition of the coordinate system guarantees   ylocal = 0. Now  again using the deﬁ nitions for    cid:1 x and    cid:1 y the above can be expressed as      cid:1  x local  = −  ψ ψ  sin cos  V  path  −   cid:1  x  path  ψ  cos  path  +  V  cos  ψ ψ  sin  path  −   cid:1  y  path  ssinψpath        cid:1  x local  = −  V  sin  ψ  local  −   cid:1  x  path  ψ  cos  path  −   cid:1  y  path  sin  ψ    path  and since      cid:1  x  path  = −   cid:1  y  path  tanψ    path                          or  or   44     MOBILE ROBOT CONTROL  this reduces to simply   For the equation regarding the angle of the path we have      cid:1 x  local  = − sinψ      V  local       cid:1  ψ  local  =  tan   cid:1  α ψ  −       path  V L    2.52        2.53       Here  we  may  express     cid:1 ψpath  in  terms  of  its  dependence  on  its  loca- tion along the path and the motion of the coordinate origin along the  path, i.e.,       cid:1 ψ  path  =  ψ ∂ ∂ s  path  path  ds path dt       It is recognized that   ∂   is really the deﬁ nition of curvature  spath .  The  equation  for    may  be  evaluated  from  its  deﬁ nition    2.49c    and  is  found   of  the  path,  which  may  be  designated  as    κ    ds through the use of   2.51   to be  ∂ψpath s   path    dt  path         cid:1  s  path  =  cosψ  V  −  x  local  local   cid:1  ψ     path   Thus       cid:1  ψ  =  κ          which may be rearranged and solved for    cid:1 ψpath yielding  cos      local  local  V  path  path  path  x  s  ψ  −   cid:1  ψ       cid:1 ψ  path  =  κ   s κ   s    path   x local  path  +  1  V  cos  ψ  local       The equations for this model ﬁ nally become  1      cid:1 s  path  =      cid:1 x  local  s  + 1 κ   V  x local = − sinψ      path  local     V  cos  ψ  local         2.54a       2.54b                           STEERING CONTROL ALONG A PATH     45  and          cid:1 ψ  local  =  V  α −  tan     1 L  κ   s κ   s    path   x local  path  +  1  ψ  cos  local            2.54c       These  three  variables — displacement  of  the  robot  from  the  path,  distance  of  the  coordinate  origin  along  the  path  and  heading  of  the  robot with respect to the path — completely describe the kinematic state  of the robot. From equation   2.54a   it is seen that    cid:1 spath may either be  greater than or less than the robot velocity   V depending on whether  the robot takes the inside or the outside when negotiating a curve. If  the robot stays exactly on the path, then    cid:1 spath and   V are equal.    On the right - hand side of the last equation, the ﬁ rst term inside the  parentheses represents the curvature of the robot trajectory. The allow- able  range  of  this  term  represents  the  robot  steerability. The  second  term represents the effective curvature of the path and may be thought  of  as  a  disturbance.  It  will  be  assumed  that  the  robot  does  have  the  steerability to negotiate the curves along the path when it starts on the  path and aligned with it; otherwise, the control task would be impos- sible. Thus  the  allowable  range  of  the  steerability  term  must  always  exceed the allowable range of the disturbance term under the condi- tions that   xlocal is zero and that   ψlocal is zero. The assumption regarding  steerability then becomes  tan  maxα  ≥  max{  abs  κ      spath    }         2.55a     1     L     tan   α   max  κ≥ L  max         2.55b       It is instructive to also express these differential equations in terms   of distance traveled by the robot,   s. Since for any   w     dw ds     =       dw dt ds dt  dw dt  =       V  or              we obtain     ds  path     ds  = ′ s  path  =  1  + 1 κ    s     x local  path  cos  ψ      local    2.56a      46     MOBILE ROBOT CONTROL  and similarly     dx  local     ds  local  = ′ x local  = − ψ      sin  local    2.56b     ψ     d  local     ds  ψ = ′  local  =  α −  tan  1 L  κ   s κ   s    path   x local  path  +  1  ψ  cos  ll      loca    2.56c       These equations may be used in the study of steering control algo- rithms. One major accomplishment of this conversion to local coordi- nates is that the control problem has been transformed from a tracking  problem into a regulator problem with a disturbance. For either formu- lation,  a  perfect  solution  requires  future  knowledge  of  the  path  curvature.    For the case of zero path curvature the system of kinematic equa-  tions take on the familiar form  and   spath becomes the local   y axis. In terms of distance traveled these  equations are      cid:1 s     cid:1 x  path  local  cosψ     = V = − sinψ     V  local  local       cid:1 ψ  local  =  α tan      V L      ′ spath     ′ xlocal  = cosψ     = − sinψ      local  local  =ψ      1 ′ local L  α tan        2.57a       2.57b       2.57c       2.58a       2.58b       2.58c                             and  and    EXAMPLE 2     Explore the possibility of using the    xlocal  and     ′xlocal  coordinates for control  design.                  STEERING CONTROL ALONG A PATH     47    SOLUTION 2     As a ﬁ rst step toward designing a controller for this system consider the  case  of  zero  path  curvature  above.  By  differentiating  the  equation  for      ′xlocal  one has       ′′ = − x  local  cos  ψ ψ ′ local  local  = −  local  tan  α    cos  ψ L    A control is selected which depends on heading error and displace-  ment from the path,   α  =     tan  L ψ  cos  local  −     K  1  sin  ψ  local  +  K x  2  losal           Here it appears that positive feedback has been used, but it is really  negative. Recall that    x  is deﬁ ned positive to the right while    α  and    ψ  are  deﬁ ned  positive  counter - clockwise. This  expression  written  in  terms  of     xlocal  becomes   α  =     tan  L ψ  cos  local     K x 1  ′ local  +  K x  2  losal         leading to       ′′ + x local  K x 1  ′ local  +  K x  2  local  =  0     max  α α≤    Clearly one could choose    K1  and    K2  to achieve any desired response  in    xlocal  as long as the equation for    α  yields a value within the achievable  limits, i.e.,    abs     . The effect of going to a second order equation  in    xlocal  has been to map    ψlocal  into     ′xlocal . Unfortunately, while things look  ﬁ ne in terms of    xlocal  and     ′xlocal , the equilibrium point    xlocal = 0  and     ′ 0  π  maps into    xlocal = 0  and    ψ  . For n odd this means the robot is in  stable equilibrium but headed in the wrong direction. Thus working in  the     xlocal ,    ′xlocal   space  is  not  advisable  for  the  case  of  large  initial  errors  because of the one - to - many - mapping when going back to the    xlocal ,   ψlocal   space.      xlocal  n=  local  =   A heuristic approach is now presented as a candidate for maintain- ing the mobile robot on path. This approach is to specify a proﬁ le for  desired behavior for driving the error in displacement from the path    48     MOBILE ROBOT CONTROL     xlocal   to  zero,  and  then  to  allocate  the  remaining  allowable  velocity  toward motion along the path     cid:1 spath . It is arbitrarily speciﬁ ed that the  velocity perpendicular to the path be      cid:1 x  local desired  −  = −  β x  local  for  β x  local  ≤  V         2.59a         cid:1 x  local desired  −  = −  V  sgn   x     local  for β x  local  >  V         2.59b       Using the remaining available velocity,    V  2  2−  cid:1  xlocal desired  −  , for motion   along the path implies     tan   ψlocal desired  −     =  −  −  cid:1  x local desired −  cid:1  2 x V  2 local desired  −     ψlocal desired  −  =  − tan 1  −  −  cid:1  x local desired −  cid:1  2 x V  2 local desired  −               ψlocal desired  −  = − sin 1 −   cid:1  x  local desired       − V    2.60a       2.60b       2.60c       It  may  be  seen  from  the  ﬁ rst  form  that  this  equation  for  desired   heading guarantees     − π ψ  ≤  2     local desired  −  ≤  π    2     and thus ensures that the desired component of velocity along the path  will be in the proper direction. One then steers the robot to the desired  heading   ψlocal desired   as follows  −       cid:1 ψ  local  =  γ ψ    local desired  −  −  ψ         local    2.61a     tan  α γ ψ  =     local desired  −  −  ψ  local  +     V  ψ  cos       local    2.61b        κ κ x  +  1  local     and  or  or                          or  V     L           {  L V  {  L V  STEERING CONTROL ALONG A PATH     49     α  =  − 1  tan  γ ψ    local desired  −  −  ψ  local  +     L  κ κ x  +  1  local     ψ  cos  locaal  } for    γ ψ    local desired  −  −  ψ  local  +     L  ψ  cos  local  ≤  tan  α  mmax        2.61c        κ κ x  +  1  local     α α  =  max sgn  γ ψ    local desired  −  −  ψ  local  +     L     κ κ x  +  1  local     cos  ψ l oocal  } for    γ ψ    local desired  −  −  ψ  local  +     L  ψ  cos  local  >  tan  α  mmax        2.61d         κ κ x  +  1  local   In implementing such a control one must always check to see whether     ψ  local desired  −  −  ψ  local  π <    and if not, then add or subtract   2π as many times as is needed for this  to be true.    When the controller is operating in the linear region, i.e., not satu-  rated, the equation   2.61a   may be manipulated to become       cid:1  ψ  local  =  −  − sin 1   γ    cid:1  x       −  ψ  local        local desired  V       cid:1  ψ  local  =   γ   − 1  sin     β x local V    −  − 1  sin  −    cid:1  x local V               cid:1 x  local  = − sin   V   ψ     local   Linearizing the sine and inverse sine functions yields       cid:1  ψ  local  =  γ β  x local  V  +   cid:1  x local V           2.62a     or  and       L V       L V  or  and                              50     MOBILE ROBOT CONTROL  and         cid:1 x  local  = − ψ      V  local    2.62b     which may be combined to yield γ  cid:1  x      cid:1  cid:1  x  +  +  γβ x  =  0         local  local  local    2.63      The above is guaranteed to be stable for all positive values of   β and    γ. A crucial difference in this control strategy and the one presented in  the preceding example is that in the preceding example the tangent of    α depended on the sine of the heading error, while here the tangent of    α depends on the heading error itself.    Equation    2.61    is  another  instance  of  applying  computed  control.  The  ﬁ rst  term  in  equation    2.61b    provides  linear  feedback  and  the  second  term  cancels  out  the  nonlinear  disturbance  term   a  curving  path . Here one could select the parameters   β and   γ corresponding to  the desired closed - loop system behavior. To implement such a control- ler requires that the commanded steering angle,   α not exceed its allow- able range. It is expected that the left side of equation   2.61b   could be  made equal to the second term on the right side, i.e., steerability must  exceed the disturbance. However, care must be exercised in selecting    β and   γ so that the total of all the terms on the right side can still be  satisﬁ ed by the left side. Otherwise the controller operates in the satu- rated  mode  and  the  closed - loop  dynamics  are  not  the  same  as  those  predicted by the linear equations.    Implementation  also  requires  that  one  be  able  to  sense    ylocal,    ψlocal  and    κ.  Sensing  the  displacement  from  the  path,    ylocal  and  the  current  heading of the robot relative to the path,   ψlocal would be required even  for a straight path. What is required additionally here is   κ, the curvature  of the path at the current robot location. Sensing of portions of the path  not yet traversed and estimating the curvature from these observations  would be required.    If one chooses to use the computed control without knowledge of   the path curvature, i.e., according to     α  =  tan  γ ψ    local desired  −  −  ψ        local  }  {  1 L − V        where     ψ  local desired  −  =  − 1  sin   cid:1  x  local desired  − V  =  − 1  sin  −  β x V  local      STEERING CONTROL ALONG A PATH     51  when, in fact, the path does have curvature, the linearized closed - loop  equations become         cid:1  cid:1  x  local  +  γ  cid:1  x  local  +  γβ x  local  = −  2  V  κ   s κ   s    path   x local  path  +  1  ψ  cos  llocal      The solution is now inﬂ uenced by the disturbance on the right side  of the equation which can cause transient as well as steady - state errors.   A simulation of the system using this control strategy is presented  in Figure  2.11 . Here the initial condition of the robot is   x = 10,   y = 0 and    ψ π= −   2  with    β= 2 0.   and    γ= 0 5. .  The  path  is  a  circle  of  radius  8.  Observe that the robot initially turns as steeply as possible and then  brieﬂ y heads to the closest point on the path. It then merges with the  path according to equation   2.60  . Because of the path curvature and  the  fact  that  the  control  assumed  zero  curvature,  there  is  a  tracking  error. This error would remain as long as the path has this curvature.     A second simulation is shown in Figure  2.12 . Here   x = 11,   y = 0 and    ψ π=   2 Note the ﬁ rst portion shows motion directly toward the path  followed by a gradual turn to move along the path.        Reference Path Robot Path  e  t  i  a n d r o o c   y  6  5  4  3  2  1  0    4  5  6  7  8  9  10  11  12  x coordinate       Figure 2.11       Simulation A Illustrating the Heuristic Steering Control Strategy     52     MOBILE ROBOT CONTROL     Reference Path Robot Path  e  t  i  a n d r o o c   y  6  5  4  3  2  1  0    4  and                 5  6  7  8  9  10  11  12  x coordinate       Figure 2.12       Simulation B Illustrating the Heuristic Steering Control Strategy     Classical  linear  control  techniques  may  be  used  here  if  one  uses  linearized equations and assumes that   V will be constant. Here we shall  use  the  equations  in  the  time  domain.  By  making  the  following  approximations     tanα α≈   ψ ≈    sinψ    cosψlocal ≈ 1    local     local  κ     s κ     s xlocal      1+  ψ  cos  local  κ ≈         cid:1 ψ  local  =  α κ    −  V  V L  the differential equations of interest become   STEERING CONTROL ALONG A PATH     53  and           or  x             cid:1 x  local  = − ψ     V  local   Now  feeding  back    −xlocal  with  unity  gain  plus    K1  times  the  local  heading  angle,    ψlocal  and  then  setting  the  steering  angle    α  equal  to  a  combination of proportional plus integral error terms yields     α     s  =  +     K K s        ref  p  i  +  x local  − 1     s K  ψ       s     local     α    s  =  +     K K s ref     [  p  i  +  +1      K s V x     1  local     ] s       The above may appear to be positive feedback, but it is not. As was  discussed in a previous example, recall that   xlocal is positive to the right,  and  that    ψ  is  positive  in  the  counter - clockwise  direction. The  corre- sponding block diagram in local coordinates with the feedback control  is shown in Figure  2.13 .      The closed - loop transfer function then becomes  =      s  local        VK K L s    K V L s V K L 2         V K L VK K L  p +  +                 2  i  2  2  1  i  p  p  1  +  3  s  + s V K L     2  i  ref  +  +  3  s      VK K L s     p  1  2  2    V K L VK K L s V K L           2  1  i  i  +  +  +      κ  +   Vs    p   The steady - state gain of the ﬁ rst portion is seen to be unity guaran- teeing good steady - state reference tracking. The s in the numerator of   0  +  −  +  K  p  i  K s  a  V L  y  L  1 s  V  Lx  1 s  k  V − +  1K  +  +       Figure 2.13       Block Diagram of the Closed - Loop Steering System in Local Coordinates     54     MOBILE ROBOT CONTROL  x  y  Path ion Specificat k &, x  y  ,  p  p versus  s  px − py y  p  + −  x y  cosy  p  siny  p  + −  + y  k  + + y  Lx  local  a  Controller  siny X· = − V cosy · = VY V y tana · = L  x y y       Figure  2.14   a Path       Block  Diagram  in  Original  Coordinates  for  Mobile  Robot  Following   the second portion guarantees zero steady - state error from the distur- bance. Finally the choice of the three gain parameters,   K Kp i and   K1 will  dictate the locations of the closed - loop poles.    In  Figure   2.14   is  a  block  diagram  in  terms  of  original  coordinates  showing how the local coordinates are obtained. Here   x,   y and   κ are  assumed to be functions of the distance traveled along the path, s. The  operations represented here have application for any steering control  algorithm which is computed based on the local coordinates.      The  nonlinear  robot  equations  for  the  robot  have  been  simulated  utilizing the control algorithm based on classical linear control theory.  The behavior was quite good as long as the initial deviation from the  path did not exceed several meters and the curvature of the path did  L. Some illustrative  not exceed that of the robot ’ s capability,   tan  examples follow and are shown in Figures  2.15 ,  2.16  and  2.17 . Varying  responses may be obtained by adjusting the gain parameters. Here the    and    Ki = 0 4. .  The  resulting  gains  were  set  at    K closed - loop  poles  for  the  linearized  system  were  at   − 1,   − 1  and   − 0.2.  Note that there is a ﬁ nite zero in the closed - loop transfer function that  could  cause  overshoot  in  some  cases  even  though  the  roots  of  the  denominator correspond to an overdamped system.      4 618 . ,  maxα  9527  K p      =  =  .  1    OPTIMAL STEERING OF FRONT - WHEEL      2.10   STEERED VEHICLE    The objective of the previous control algorithms was to provide control  action that would stabilize the overall behavior and yield good perfor- mance. In this section, a different approach is taken. It will be assumed  that  the  robot  will  operate  at  a  ﬁ xed  velocity  and  the  objective  is  to    OPTIMAL STEERING OF FRONT-WHEEL STEERED VEHICLE     55     Reference Path Robot Path  e  t  i  a n d r o o c   y  20  18  16  14  12  10  8  6  4  2  0  -2    -5  -4  -3  -2  -1  0  1  2  3  4  5  x coordinate       Figure 2.15       Mobile Robot Recovering from 2   m Error While Tracking a Straight Line    steer it in such a way that it will reach the destination in minimum time.  It  is  further  assumed  that  there  are  no  obstacles  so  that  the  robot  is  free to travel anywhere without being conﬁ ned to a roadway. Note that  the minimum - time trajectory would also be the minimum - distance tra- jectory  since  speed  is  ﬁ xed. The  proceeding  builds  on  the  theory  of  optimal control. The interested reader may consult appropriate refer- ences for more background on this body of knowledge. The equations  of motion are repeated once more for convenience.           and      cid:1 x     cid:1 y V=  V= − sinψ  cosψ        cid:1 ψ  = V L  α tan      We now form the Hamiltonian   56     MOBILE ROBOT CONTROL     Reference Path Robot Path  e  t  i  a n d r o o c   y  6  4  2  0  -2  -4    -6                 or  or  -4  -2  4  6  8  0 2 x coordinate       Figure 2.16   a Circle of Radius 5   m       Mobile Robot Recovering from 2   m Error While Tracking a Segment of      H  = − 1 λ  x  V  sin  ψ λ V y  +  cos  ψ λ ψ  +  α tan        2.64      V L   The equations for the co - states become      cid:1 λx  = − ∂ H ∂ x  = 0       λx C= 1     = − ∂  cid:1 λy H     ∂ y  = 0      2.65a        λy C= 2,        2.65b      OPTIMAL STEERING OF FRONT-WHEEL STEERED VEHICLE     57  Reference Path Robot Path     18  16  14  12  10  8  6  e  t  i  a n d r o o c   y  and  or              choosing  whenever  4   -25  -24  -23  -22 -21 x coordinate  -20  -19  -18       Figure  2.17   Sloping Curve       Mobile  Robot  Recovering  from  2   m  Error  While  Tracking  a  Gently   ψ = − ∂  cid:1  H λ     ψ ∂  = −  λ x  V  cos  ψ λ V y  +  sin  ψ λ λ y  = −   cid:1  y  −  x   cid:1  x        λψ     =  {     t C C y t  3  1  −  −     } 0 y  −  {     C x t  2     } 0     x  −    2.65c       Examining the Hamiltonian, it is seen that it will be minimized by      α α  = − max    sign  λψ           λψ ≠ 0      =     R  L  tan maxα         cid:1 λψ = 0       −  λ λx   cid:1  y  −  y   cid:1  x  =  0       −  cid:1  C y C x  −   cid:1   1  2  =  0          m  C = − 2 C 1       58     MOBILE ROBOT CONTROL   As was shown earlier this control action results in segments of circles   with radius   Thus portions of the optimal trajectory are segments of circles. It is  of  interest  to  examine  the  possibility  of  singular  control,  i.e.,  what  if    λψ = 0? To answer this we note that for   λψ = 0 over a non - zero interval  we must also have  which implies that the robot is moving in a straight line with slope    2.66a       2.66b       This straight - line motion can happen if and only if   α= 0. Thus the   optimal control obeys the following     α α  = −  max    sign  λ λψ ψ   ;  ≠  0        2.67a        α λψ  0 ;  =  =  0         2.67b       At this point it can be concluded that the optimal trajectory is seen  to be a series of segments that are either segments of clockwise circles,  segments  of  counter - clockwise  circles  or  straight  lines. The  equation  developed for   λψ, i.e.,     λψ     =  {     t C C y t  1  3  −  −     } 0 y  −  {     C x t  2     } 0     x  −    2.68a     or  or                          and   OPTIMAL STEERING OF FRONT-WHEEL STEERED VEHICLE     59  can also be expressed in terms of the ﬁ nal conditions as     λψ     =  t D C y t  {     f  3  1  +  −     } y t  +  C x t  {     f  2     } x t       −    2.68b       By solving for   λψ   t f   using each of the expressions it is easy to show   that  =  −     D C C y t  {     f  3  3  1  −  0    } y  −  C x t  {     f  2  0    }     x  −    2.69       The ﬁ rst form is useful in working forward from the initial conditions   to determine where the trajectory switches from a circular arc                       to a singular arc  to a circular arc     α α= ± max       α= 0        α= 0    α α= ± max      The second form is useful for working backward from the ﬁ nal condi-  tions to determine where the trajectory switches from a singular arc   Note  that  the  problem  is  a  two - point  boundary  value  problem  as  would be expected for an optimal control problem. The three speciﬁ ed  ﬁ nal conditions on   x,   y and   ψ provide the constraining equations that  determine the unknown initial conditions,   C1,   C2 and   C3 for the co - state  variables. These in turn determine   D3 per the above equation. The test  for  determining  whether  the  values  for    C1,    C2  and    C3  are  the  correct  ones for the particular problem at hand is to integrate the equations of  motion  and  the  co - state  equations  forward  in  time  from  the  initial  conditions until the beginning of the singular arc, i.e.,   λψ becomes zero.  Now using these same values for   C1,   C2 and   C3, and the boundary condi- tions on   x,   y and   ψ, one determines   D3 and then integrates the equa- tions of motion backward from the speciﬁ ed ﬁ nal conditions until the  end of the singular arc, i.e., again   λϕ becomes zero. The conditions at  the  beginning   arrive   and  end   depart   of  the  singular  arc  should  satisfy the equation   60     MOBILE ROBOT CONTROL     −   C y  1  arrive  −  y depart       C x  2  arrive  −  −  x  depart  =     0         2.70       This is equivalent to the earlier equation     −  cid:1  C y C x  −   cid:1   2  1  =  0     along the singular arc.    Several  examples  will  now  be  presented  to  illustrate  how  the  unknown coefﬁ cients may be determined. First the radius of curvature  when using maximum steering angle will be evaluated. Assume that the  length of the mobile robot is 2 and that the maximum steering angle is    π  4. Then evaluating the equation for the radius of curvature we obtain  =     R  L  tan maxα  =  2      EXAMPLE 3     Take as the initial conditions   [     x      0      0 y  ψ      0  ] = − [  T  20  − 4 0  ]    T   and as the ﬁ nal conditions   [     x t      f      y t f     ψ  t  f  ] = [  T  T 0 0 0     ]    As  will  be  seen  the  minimum - time  trajectory  is  comprised  of  a  90  degree section of a clockwise circle followed by a straight line of length  16 followed by a 90 degree section of a counter - clockwise circle.       SOLUTION 3     Utilizing  the  boundary  conditions  for  the  problem  coupled  with  the  conditions    and along the singular arc      H t f    = 0   dH     dt  = 0       λψ = 0                                            OPTIMAL STEERING OF FRONT-WHEEL STEERED VEHICLE     61   yields as solutions for the co - state parameters   [     C C C D 3  2  1  3  ] =  T     L 2 V  0  L V  −   T      L V    Thus    is used at the beginning of the trajectory to determine where the singular  arc begins, and      λψ =  −  L V  L 2 V  {     y t  −     }   0 y     λψ = −  +  L V  L 2 V  {     y t f  −     }   y t   is used at the end of the trajectory to determine where the singular arc  ends.      The fact that    −  C C2     1  =  0   is consistent with the fact that the slope of   the singular arc of the trajectory is zero,      Plots of y vs x,    λψ  vs t and    α  vs t follow in Figures    2.18a  ,   2.18b    and      2.18c  .        EXAMPLE 4     Take as the initial conditions   [     x      0      0 y  ψ      0  ] = − [  T  20 0  ]   π −  T   and as the ﬁ nal conditions   [     x t      f      y t f     ψ  t  f  ] = [  T  T 0 0 0      ]    SOLUTION 4     The minimum - time trajectory is comprised of a 90 degree section of a  counter - clockwise circle followed by a straight line of length 16 followed  by a 90 degree section of a counter - clockwise circle. The solutions for the  co - states yield    10  8  6  4  2  0  -2  -4  -6  -8  i  e t a n d r o o c   y  a d m a L  1.5  0.5  2  1  0  -0.5  -1  -1.5  -2  0  -10  -20  -18  -16  -14  -12  -10  -8  -6  -4  -2  0  x-coordinate       Figure 2.18a       Trajectory in x - y Space    5  10  15  20  25  Time       Figure 2.18b         λψ  versus Time    62   OPTIMAL STEERING OF FRONT-WHEEL STEERED VEHICLE     63  l     e g n A g n i r e e S  t  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  0    Thus            5  10  15  20  25  Time       Figure 2.18c         α versus Time    [     C C C D 3  3  1  2  ] =  T     L 2 V  0  −  L V  −   T      L V   is used at the beginning of the trajectory, and      λψ = −  −  L V  L 2 V  {     y t  −     }   0 y     λψ = −  +  L V  L 2 V  {     y t f  −     }   y t   is used at the end of the trajectory.      The fact that    −  C C2  0   is consistent with the fact that the slope of  the singular arc of the trajectory is zero. Plots of y vs x,    λψ  vs t and    α  vs  t follow in Figures    2.19a  ,   2.19b    and    2.19c  .         1  =   10  8  6  4  2  0  -2  -4  -6  -8  i  e t a n d r o o c   y  a d m a L  0  -0.2  -0.4  -0.6  -0.8  -1  -1.2  -1.4  -1.6  -1.8  -2  0  -10  -20  -18  -16  -14  -12  -10  -8  -6  -4  -2  0  x coordinate       Figure 2.19a       Trajectory in x - y Space    5  10  15  20  25  Time       Figure 2.19b         λψ  versus Time    64   OPTIMAL STEERING OF FRONT-WHEEL STEERED VEHICLE     65  l     e g n A g n i r e e S  t  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  5  10  15  20  25  Time       Figure 2.19c         α versus Time      EXAMPLE 5     Take as the initial conditions   [     x      0      0 y  ψ      0  T   ] = − 2   − 20  −  π  T    2   and as the ﬁ nal conditions   [     x t      f      y t f     ψ  t  f  ] = [  T  T 0 0 0      ]    SOLUTION 5     The minimum - time trajectory is comprised of a 90 degree section of a  counter - clockwise  circle  followed  by  a  straight  line  of  length  18.  The  solutions for the co - states yield   [     C C C D 3  2  1  3  ] =  T     0  −  L V  −  2 L V   T      0            66     MOBILE ROBOT CONTROL  e  t  i  a n d r o o c   y  0  -2  -4  -6  -8  -10  -12  -14  -16  -18  -20  -10    Thus         -8  -6  -4  -2  0  2  4  6  8  10  x-coordinate       Figure 2.20a       Trajectory in x - y Space       λψ = −  2 L V  +  L V  {     x t  −  x     }    0    is  used  at  the  beginning  of  the  trajectory.  Here  it  is  not  necessary  to  describe the equation for    λψ  in terms of the terminal conditions since the  ﬁ nal portion of the trajectory is a singular arc. The fact that    − 0  is consistent with the fact that the slope of the singular arc of the trajec- tory is inﬁ nite. Plots of y vs x,    λψ  vs t and    α  vs t follow in Figures    2.20a  ,    2.20b    and    2.20c  .      C C1  =     2    EXAMPLE 6     Take as the initial conditions   [     x      0      0 y  ψ      0  T   ] = − 4   − 20  −  π  T    2   0  -0.5  -1  -1.5  -2  -2.5  -3  -3.5  -4  0  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  a d m a L  l     e g n A g n i r e e S  t  5  10  15  20  25  Time       Figure 2.20b         λψ versus Time    5  10  15  20  25  Time       Figure 2.20c         α versus Time    67   68     MOBILE ROBOT CONTROL   and as the ﬁ nal conditions   [     x t      f      y t f     ψ  t  f  ] =  T     0 0  −  π  T  2         SOLUTION 6     The minimum - time trajectory is comprised of a 90 degree section of a  counter - clockwise circle followed by a straight line of length 16 followed  by a 90 degree section of a clockwise circle. The solutions for the co - states  yield   [     C C C D 3  1  2  3  ] =  T     0  −  L 2 V  −  L V   T      L V    Thus    is used at the beginning of the trajectory, and      λψ = −  +  L V  L 2 V  {     x t  −     }   0 x     λψ =  −  L V  L 2 V  {     x t f  −     }    x t    is used at the end of the trajectory.      The fact that    −    is consistent with the fact that the slope of  0 the singular arc of the trajectory is inﬁ nite. Plots of y vs x,    λψ  vs t and    α   vs t follow in Figures    2.21a  ,   2.21b    and    2.21c  .         2  C C1  =    EXAMPLE 7     Take as the initial conditions   [     x      0      0 y  ψ      0  T   ] = −   . 24 0 5  ,  − − 4 16 0 5 .  ,  −  π  T    2                  i  e t a n d r o o c   y  a d m a L  0  -2  -4  -6  -8  -10  -12  -14  -16  -18  -20  -10  1.5  0.5  2  1  0  -0.5  -1  -1.5  -2  0  -8  -6  -4  -2  0  2  4  6  8  10  x-coordinate       Figure 2.21a       Trajectory in x - y Space    5  10  15  20  25  Time       Figure 2.21b         λψ  versus Time    69   70     MOBILE ROBOT CONTROL  l     e g n A g n i r e e S  t  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  0              5  10  15  20  25  Time       Figure 2.21c         α versus Time     and as the ﬁ nal conditions   [     x t      f      y t f     ψ  t  f  ] =  T     0 0  −  π  T  2         SOLUTION 7     The minimum - time trajectory is comprised of a 45 degree section of a  counter - clockwise circle followed by a straight line of length 20 followed  by a 45 degree section of a clockwise circle. The solutions for the co - states  yield      C  1     C  2    2  =  L V = − L V  = −     C  3  L V  1 −   2 1     +  L  1 − 2 1   2 L V  2    2    +     +  L  1 − 2 1       +  L   OPTIMAL STEERING OF FRONT-WHEEL STEERED VEHICLE     71   and     For     Thus                  =     D 3  −  L V  2 L V  2    1 − 2 1        +  L     L = 2     which is what was assumed here, these reduce to   [     C C C D 3  1  2  3  ] =  T      1 2 V  −  1 2 V  − + 2 V  2  2  − V  2   T          λψ = − + 2 V  2  −  1 2 V        y t  −       0 y  +  {     x t  −  x     }   0   is used at the beginning of the trajectory, and      λψ = − 2 V  2  +  1 2 V        y t  f  −       y t  −  {     x t f  −     }   x t  1 2 V  1 2 V   is used at the end of the trajectory.      The fact that    −    is consistent with the fact that the slope of the  1 singular arc of the trajectory is unity. Plots of y vs x,    λψ  vs t and    α  vs t  follow in Figures    2.22a  ,   2.22b    and    2.22c  .       C C2     =   It would be desirable to obtain a control law which would provide  the  optimal  steering  control  as  a  function  of  the  present  state.  For  a  ﬁ xed  set  of  ﬁ nal  conditions,  it  is  conceivable  that  such  a  control  law  does exist. However, the fact that the state space is of dimension three  makes  this  a  difﬁ cult  problem,  i.e.,  the  surface  for  switching  from  maximum steering angle to zero steering angle and vice versa would  be a surface described as a function of   x,   y, and   ψ. Nevertheless, one  can take advantage of the nature of the candidate segments of optimal  trajectories as dictated by the necessary conditions and use geometrical  reasoning to solve for the complete optimal trajectory given the bound- ary  conditions.  Optimal  trajectories  were  seen  to  be  a  series  of  seg- ments that are either arcs of clockwise circles, arcs of counter - clockwise  circles or straight lines.    i  e t a n d r o o c   y  a d m a L  0  -2  -4  -6  -8  -10  -12  -14  -16  0  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  0  5  10  15  20  25  Time       Figure 2.22a       Trajectory in x - y Space    5  10  15  20  25  Time       Figure 2.22b         λψ  versus Time    72   OPTIMAL STEERING OF FRONT-WHEEL STEERED VEHICLE     73  l     e g n A g n i r e e S  t  0.8  0.6  0.4  0.2  0  -0.2  -0.4  -0.6  -0.8  0  5  10  15  20  25  Time       Figure 2.22c         α versus Time     An example will be presented to illustrate this approach.      EXAMPLE 8     Take as the initial conditions   [     x      0      0 y  ψ      0  T   ] = −   . 24 0 5  ,  − − 4 16 0 5 .  ,  −  π  T    2   and as the ﬁ nal conditions   [     x t      f      y t f     ψ  t  f  ] =  T     0 0  −  π  T  2              Next construct the possible trajectories emanating from the initial state  as well as those terminating at the ﬁ nal state. These are sections of circles  in the clockwise direction and sections of circles in the counter - clockwise    74     MOBILE ROBOT CONTROL       Figure 2.23       Trajectory in x - y Space    direction, all at the minimum radius of curvature of the mobile robot, or  sections of straight lines.      Now connect one circle from the initial state with one from the ﬁ nal  state circles with a straight line that is tangent to each. By inspection one  can decide which circles to connect so that the path is of minimum length.  Figure    2.23    illustrates the optimal solution for this example. It is seen that  the  optimal  trajectory  consists  of  a  brief  turn  in  the  counter  clockwise  direction where the steering angle is maximum to the left followed by a  straight section where the steering angle is zero followed by a brief turn  in  the  clockwise  direction  where  the  steering  angle  is  maximum  to  the  right.         EXAMPLE 9     Take as the initial conditions   [     x      0      0 y  ψ      0  T  ] = −[ 5 ,  , 0  π   3  4  ]    T   and as the ﬁ nal conditions   [     x t      f      y t f     ψ  t  f  ] = [  T  T 0 0 0     ]          The same procedure as above is followed in this example. Figure    2.24     illustrates the optimal solution for this example. It is seen that the optimal  trajectory consists of a segment of a turn in the counter clockwise direc- tion where the steering angle is maximum to the left followed by a straight  section where the steering angle is zero followed by another turn in the    OPTIMAL STEERING OF FRONT-WHEEL STEERED VEHICLE     75       Figure 2.24       Trajectory in x - y Space    counter clockwise direction where the steering angle is again maximum  to the left           OPTIMAL STEERING OF FRONT - WHEEL STEERED      2.11   VEHICLE, FREE FINAL HEADING ANGLE    A simpler problem results for the case where the ﬁ nal heading angle  is left free. For this case we have that the ﬁ nal value of the associated  co - state variable is zero, i.e.,     λψ   t f = 0      The net result of this is that the last segment of the optimal trajectory  is singular, i.e. , the steering angle for this segment is zero. The optimal  solution  is  then  an  arc  of  the  minimum - radius  circle  followed  by  a  straight - line segment. The robot is steered at maximum steering angle  until it is pointed at the ﬁ nal destination. Then it travels in a straight  line.   For     ψ    t  >  −1  tan     − − {        } x t x t f −    } {     y t y t f             α α= − max,         ψ    t  <  −1  tan     − − {        } x t x t f −    } {     y t y t f            2.71a       2.71b       2.72a                 one uses  for   76     MOBILE ROBOT CONTROL  one uses  and for  one uses              ψ    t  =  −1  tan     − −    } {     x t x t f −    } {     y t y t f             α α= max         α= 0        2.72b       2.73a       2.73b       An  illustration  of  an  optimal  trajectory  with  free  ﬁ nal  heading  is  shown in Figure  2.25 . The robot travels in a circle of minimum radius  using   α α= ± max until it is headed toward the destination. Then it travels  in a straight line with   α= 0.      In summary, the necessary conditions for the optimal control of the  mobile robot have been derived. For the case where the ﬁ nal heading  and ﬁ nal position are speciﬁ ed, the ﬁ nal segment of the trajectory and  the initial segment of the trajectory are sections of circles. These seg- ments are connected by a straight line corresponding to singular control.  Through  a  proper  choice  of  the  unknown  parameters  of  the  co - state  variables a solution for a particular example may be obtained. For the  switches  that  must  take  between  singular  segments  and  non - singular  segments, one must always go from the non - singular segments to the  singular segments. An alternative and simpler approach uses geometri- cal  reasoning  coupled  with  the  nature  of  the  solution.  Both  methods        Figure 2.25       Trajectory in x - y Space     are illustrated via examples. A closed - form solution has been obtained  for the case where the ﬁ nal orientation is free.   EXERCISES     77    EXERCISES          1.     Use the linearized model for the front - wheel steered robot and take  ,   xref = 0 and   ψref = 0. Use  the  y  axis as the reference path, i.e.,   y as control algorithms,   δ . Determine  K x K V the solutions for the closed - loop eigenvalues and ﬁ nd the conditions  on the   K s’  for stable behavior.     ref =  and   δα  = − 1  δ K y  δψ 3  Vt =  δ 2  −      2.     Develop an algorithm for speed control and one for heading control.  Do this for both type robots. Assume that you have measurements  of  position,  heading  and  velocity.  Simulate  for  a  step  change  in  heading. Also simulate for a step change in speed. Modify your algo- rithms as needed for desirable behavior. Since the system is nonlin- ear,  it  is  more  difﬁ cult  to  determine  whether  it  is  stable  than  if  it  were linear. Simulate any conditions you wish to test for stability.         3.     Repeat the above control design for the case where there is no direct  measurement  of  either  heading  or  speed.  Use  ﬁ nite  differences  of  the   x   and   y   measurements  to  approximate  heading  and  speed.  Simulate as before.         4.     Assume the robot is located at   x = 3 and   y = 5. The desired location  is   x = 8 and   y = 35. What is the instantaneous desired heading angle  for the robot to move toward the target?         5.     Consider the deﬁ nition of a local coordinate system as described in  the  previous  chapter.  What  are  the  measurements  that  one  must  have  available  in  order  to  implement  a  control  strategy  based  on  such a coordinate system? What kind of approximations could one  use to simplify these requirements? Can you think of any coordinate  system  and  accompanying  control  strategy  that  would  not  require  knowledge of displacement of the robot from the center of the lane?        6.     Using  computed  control  one  can  theoretically  cause  a  system  to  behave in any desired manner. Discuss the practical limitations when  using  this  method.  What  information  is  required  in  forming  the  control signal? What if one asks the system to perform beyond its  physical capabilities, e.g., excessively fast or with an excessively small  radius of curvature?         7.     Solve the optimal robot steering problem when the ﬁ nal heading is  free. The initial heading angle is   −π  2 radians and the initial position    78     MOBILE ROBOT CONTROL  m= 20 . The minimal  is   x = 0 and   y = 0. The ﬁ nal position is   x m= 2  and   y radius of curvature for the robot is 2 m . The performance measure is  distance traveled.         8.     Use the results for the minimum - time solutions for the front - wheel  steered robot and ﬁ nd the solution for the optimal trajectory when      0   y  ,     0 x t f  ,     0 0 x  ,     y t f  and      t 0      t  = −  π    ψ  ψ  12  0         , 0  , 2  =  =  =  =  =  f    REFERENCES      Athans  and  Falb ,   “  Optimal  Control:  An  Introduction  to  the  Theory  and  Its   Application  ” ,  McGraw - Hill ,  NY ,  1966 .        Kansal ,  S.  ,   Jakkidi ,  S.   and   Cook ,  G.    “  The Use of Mobile Robots for Remote  Sensing  and  Object  Localization  ”    Proceedings  of  IECON  2003 ,   pp   279  –   284 ,  Roanoke, Va, USA , Nov. 2 – 6,  2003  .        Brogan ,  W. L.    “  Modern Control Theory  ” ,  Prentice Hall ,  Upper Saddle River,   NJ ,  1991 .        Bryson ,  A. E.   and   Ho ,  Y - C  ,  “  Applied Optimal Control: Optimization, Estimation   and Control  ” ,  Blaisdell ,  Waltham, MA   1969 .       Carlos  Canudas  de  Wit,  Bruno  Siciliano ,    Georges    Bastin     eds ,   “  Theory  of       De   Luca ,  A.  ,   Oriolo  ,   G.  and  Vendittelli  ,   M.  ,  Control of Wheeled Mobile Robots:   Robot Control  ” ,  Springer   1996 .    An Experimental Overview .        Dixon ,   Dawson  ,   Zergeroglu  and  Behal ,   “   Nonlinear  Control  of  Wheeled   Robots   ” ,  Springer - Verlag London Limited ,  2001 .        Dixon ,   Dawson  ,   Zergeroglu  and  Zhang .   Robust  tracking  and  regulation  control for mobile robots . In  International Journal of Robust and Nonlinear  Control, Int. J. Robust Nonlinear Control 2000 ,  10 : 199  –  216 .        Farbod   Fahimi  ,  “  Autonomous Robots: Modeling, Path Planning, and Control  ” ,       Lavalle ,   Steven  ,    M.     “  Planning  Algorithms     Motion  Planning  ” ,   Cambridge    Springer .    University Press ,  2006 .        Moon Kim ,  B.   and   Tsiotras ,  P.  ,  Controllers for Unicycle - Type Wheeled Robots:  Theoretical Results and Experimental Validation . In  IEEE Transactions on  Robotics and Automation  , vol.  18  , no.  3 , June  2002 .       Radu Bogdan Rusu and Marius Borodi ,  On Computing Robust Controllers  for Mobile Robot Trajectory Calculus: Lyapunov   unpublished paper series:   http:  ﬁ les.rbrusu.com publications Rusu05RobotuxLyapunov.pdf  .                3   ROBOT ATTITUDE           3.0      INTRODUCTION    This chapter is devoted to the introduction of coordinate frames and  rotations.  This  is  important  in  the  study  of  the  motion  of  any  type  vehicle  such  as  airplanes,  ships  and  automobiles  as  well  as  mobile  robots. It will be seen that frames provide an efﬁ cient means of keeping  track of vehicle orientation and also enable simple conversion of dis- placement with respect to an intermediate frame to displacement with  respect to a ﬁ xed frame.       3.1      DEFINITION OF YAW, PITCH AND ROLL    Shown in Figure  3.1  is a mobile robot with a coordinate frame attached.  This frame moves with the robot and is called the robot frame. The Y  axis is aligned with the longitudinal axis of the robot, and the X axis  points  out  the  right  side. The  Z  axis  points  upward  to  form  a  right -  handed system. This type frame deﬁ nition is commonly used in the ﬁ eld  of robotics. It differs from the convention used by those in aerospace  where the X axis is aligned with the longitudinal axis, the Y axis is to  the right, and the Z axis points down, still a right - handed system.     Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  79   80     ROBOT ATTITUDE  EZ  Earth Frame   VX  VZ  Robot Frame   VY  EY  EX       Figure 3.1       Mobile Robot with Earth and Robot Coordinate Frames     As  the  robot  moves  about,  it  experiences  translation  or  change  in  position. In addition to this, it also may experience rotation or change  in attitude. The various rotations of the robot are now deﬁ ned. Yaw is  rotation about the Z axis in the counter - clockwise direction as viewed  looking into the Z axis. Pitch is rotation about the new  after the yaw  motion   X  axis,  in  the  counter - clockwise  direction  as  viewed  looking  into the X axis, i.e., front end up is positive pitch. Roll is rotation about  the  new   after  both  yaw  and  pitch   Y  axis  in  the  counter - clockwise  direction as viewed looking into the Y axis, i.e., left side of vehicle up  is positive. In the system used by those in the aerospace ﬁ eld pitch is  counter - clockwise  rotation  about  the  Y  axis  while  roll  is  counter -  clockwise rotation about the X axis, i.e., the roles of the X and Y axes  are reversed with respect to these two rotations.       3.2      ROTATION MATRIX FOR YAW    The rotation matrices for basic rotations are now derived. For yaw we  have the diagram shown in Figure  3.2 . Axes 1 represent the robot coor- dinate frame before rotation and axes 2 represent the robot coordinate  frame after positive yaw rotation by the amount   ψ. The z axes come  out  of  the  paper.  It  bears  repeating  that  counter - clockwise  rotation  about the z axis is taken as positive yaw.      We wish to express in the original coordinate frame 1 the location  of a point whose coordinates are given in the new frame 2. For x and  y we have        = =     x 1    y 1  x  2  cos  x  2  sin  ψ − + ψ  y 2  sin  y 2  cos  ψ  ψ    ROTATION MATRIX FOR YAW     81  1y  2y  +   y  2x  1x       Figure 3.2       Frame 2 Yawed with Respect to Frame 1    and for z  or                 z 1  2= z                     x y z  1  =        ψ cos ψ sin 0  ψ ψ  − sin cos 0        0 0 1              x y z      2   Thus the rotation matrix for yaw is  ψ    Ryaw     =        ψ cos ψ sin 0  ψ ψ  − sin cos 0        0 0 1         3.1       EXAMPLE 1     A  vector  expressed  in  the  rotated  coordinate  system  with     ψ   of     π  2   is  given by                   x y z  2  =        1 0 0              Express this vector in the original coordinate system.      82     ROBOT ATTITUDE    SOLUTION 1     The expression of this vector in the original coordinate system becomes                   x y z  1  =  2 2        π   cos π sin   0  − π sin   2 0 π 2 0 cos   0 1              1 0 0        =              0 1 0         Note that the Euclidean norm of each column of the rotation matrix  is one and that each column is orthogonal to each of the others. This is  the deﬁ nition of an orthonormal matrix. A convenient property of such  matrices is that the inverse is simply the transpose, i.e.,     R  yaw  ψ      − =1  R  yaw  ψ T             3.2      This  property  can  be  proved  by  pre - multiplying  an  orthonormal  matrix by its transpose and then using the properties which it possesses,  i.e.,     R  yaw  ψ T      R  yaw  ψ =        I  col col  ,  i  j  〈      〉 = =  ,1 0  = ≠  i i  j j          3.3      ROTATION MATRIX FOR PITCH    For pitch we have the situation depicted in Figure  3.3 . The  x  axes come  out of the paper. Note again that front end up corresponds to positive  pitch      Again we wish to express in the original coordinate frame the loca- tion of a point whose coordinates have been given in the new frame.  For  x  and  z  we have  or                    = =     y 1    z 1  y 2  cos  y 2  sin  θ − + θ  z 2  sin  z 2  cos  θ  θ      x 1  2= x     and for  x      ROTATION MATRIX FOR PITCH     83  1z  2z  2y  1y  q       Figure 3.3       Frame 2 Pitched with Respect to Frame 1    or                           x y z  1  =        1 0 0  0 cos sin  θ θ   0  − θ sin   θ  cos              x y z      2   Thus the rotation matrix for pitch is  θ    Rpitch     =        1 0 0  0 cos sin  θ θ   0  θ − sin   θ  cos         3.3       EXAMPLE 2     A vector expressed in the rotated coordinate system with    θ  of    π  2   i.e.,  pitched up by the angle    π  2   is given by                   x y z  2  =        0 1 0              Express this vector in the original coordinate system.       SOLUTION 2     The expression of this vector in the original coordinate system becomes    84     ROBOT ATTITUDE                  x y z  1  =        1 0 0  0 π cos   π sin    2 2  0 − π sin   2 π 2 cos                      0 1 0  =              0 0 1         One  may  easily  verify  that  the  rotation  matrix  for  pitch  is  also   orthonormal.       3.4      ROTATION MATRIX FOR ROLL    Finally we treat roll. This is counter - clockwise rotation about the  y  axis  which results in left side up being deﬁ ned as positive roll. The  y  axes  come out of the paper as is shown in Figure  3.4 .      Once more we wish to express in the original coordinate frame the  location of a point whose coordinates are given in the new frame. For   x  and  z  we have  = x = −  2 x     x 1     z 1  cos  sin  2  + φ z 2 + φ z 2  sin  φ  φ  cos  and for  y                  y 1  2= y     2z  +  1z  f  1x  2x       Figure 3.4       Frame 2 Rolled with Respect to Frame 1     or              ROTATION MATRIX FOR ROLL     85                  x y z  1  =  φ        cos 0 sin  −  φ  0 1 0  sin 0 cos  φ     φ               x y z      2   Thus the rotation matrix for roll is  φ    Rroll      =  φ        cos 0 sin  −  φ  0 1 0  sin 0 cos  φ     φ          3.4     which is also orthonormal.    EXAMPLE 3    A  vector  expressed  in  the  rotated  coordinate  system  with     φ   of     π  2  is  given by                   x y z  2  =        1 0 0              Express this vector in the original coordinate system.       SOLUTION 3     In the original coordinate system the expression of this vector becomes                   x y z  1  =        π   cos 0 π sin    −  2  0 1 2 0  π sin   0 π   cos  2  2              1 0 0        =         0  0   −  1         Another way to think about the deﬁ nitions of these different rota- tions is to reference them to the longitudinal axis of the vehicle, starting  with  the  vehicle  level  and  pointing  along  the   y   axis  of  the  reference  frame. Yaw is the rotation of the longitudinal axis of the robot in the    86     ROBOT ATTITUDE  horizontal  plane.  CCW  rotation  as  viewed  from  above  is  taken  as  positive. Pitch is the rotation of the longitudinal axis of the robot in a  plane perpendicular to the horizontal plane. Front end up is taken as  positive.  Roll  is  the  rotation  of  the  robot  about  its  longitudinal  axis.  Left side up is taken as positive.    It is worth reiterating that each of these rotation matrices is ortho- normal,  i.e.,  the  columns  are  all  orthogonal  to  each  other,  and  each  column  has  Euclidean  norm  of  one,  making  the  inverse  equal  to  the  transpose.       3.5      GENERAL ROTATION MATRIX    We  now  deﬁ ne  the  general  rotation  matrix. After  a  frame  has  been  yawed,  pitched  and  rolled,  in  this  speciﬁ c  order,  a  point  with  coordi- nates given in this new frame may be converted into its coordinates in  the original frame by the following operation     =  R  yaw  ψ      R  pitch  θ      R  roll  φ        3.5                      x y z  1              x y z  2        Note  that  the  conversion  back  into  the  original  coordinates  is  in  the  reverse  order  of  the  rotations;  i.e.,  roll  was  the  last  rotation.  Therefore,  it  is  the  ﬁ rst  matrix  to  operate  on  the  coordinates  of  the  point  in  question. Yaw  was  the  ﬁ rst  rotation;  therefore,  it  is  the  last  matrix to operate on the point in question. By multiplying these three  rotation  matrices  together  in  the  order  shown  above  we  have  the  general rotation matrix:  = ψθφ ,   R  , ψ φ −  cos cos  ψ φ + sin cos       −    cos  sin sin sin sin sin cos θ φ  ψ θ φ ψ θ φ ssin  − ψ θ sin cos ψ θ cos cos θ sin  ψ φ + ψ φ −  sin cos sin sin  sin sin co ss cos sin cos θ φ  φ ψ θ   ψ θ φ          cos cos   3.6       It is easy to show that this product of orthonormal matrices is also   orthonormal. Thus the general rotation matrix is also orthonormal.    GENERAL ROTATION MATRIX     87   As  was  the  case  for  the  individual  rotation  matrices,  this  general  rotation matrix can be used to express a vector in an original coordinate  frame when it has ﬁ rst been expressed in a frame that has been rotated  with  respect  to  the  original  frame.  No  matter  what  the  attitude  of  a  vehicle or how it arrived at this attitude, there exists a set of rotations  in the order prescribed, yaw, pitch and roll, which will yield this very  same attitude.    A more generic expression of attitude that does not depend on ones  choice of rotation order is the matrix comprised of direction cosines of  the axes of frame 2 with the axes of frame 1. The components of the  ﬁ rst column are successively the inner product of the  x  unit vector of  frame 2 with the  x  unit vector of frame 1, the inner product of the  x   unit vector of frame 2 with the  y  unit vector of frame 1 and the inner  produce of the  x  unit vector of frame 2 with the  z  unit vector of frame  1. Likewise the components of the second column are successively the  inner product of the  y  unit vector of frame 2 with the  x  unit vector of  frame 1, the inner product of the  y  unit vector of frame 2 with the  y   unit  vector  of  frame  1  and  the  inner  produce  of  the   y   unit  vector  of  frame 2 with the  z  unit vector of frame 1. Finally the components of  the third column are successively the inner product of the  z  unit vector  of frame 2 with the  x  unit vector of frame 1, the inner product of the  z   unit vector of frame 2 with the  y  unit vector of frame 1 and the inner  produce of the  z  unit vector of frame 2 with the  z  unit vector of frame  1. In other words:  =     R 21   U U  U U    U U  T 2 x T 2 x T 2 x  x  1  y  1  z  1  U U U U U U  T 2 y T 2 y T 2 y  x  1  y  1  zz  1  x  U U U U y zU U  T 2 z T 2 z T 2 z  1  1  1               3.7       The entries of the matrix   R  ,  ,  ψθφ given in equation   3.6   may be  equated to this matrix yielding the values for yaw, pitch and roll which  when executed in that order would yield the given orientation. Equating  terms it may be readily seen that  ψ= −U U U U     T 2 y  T 2 y  1  1    y     tan    sinθ= U Uy  T 2  x 1    z     tan  φ= −U U U U  T 2 x  T 2 z     1  z  1      z              and   88     ROBOT ATTITUDE     3.6      HOMOGENEOUS TRANSFORMATION    There are situations where one frame is not only rotated with respect  to another, but is also displaced. Suppose frame 2 is both rotated and  displaced with respect to frame 1. Then a vector initially expressed with  respect to frame 2 can be expressed with respect to frame 1 as below.                  x y z  1  =  ,  ψθφ   ,  R              x y z  2  +         x  y    z origin o     ff frame  2  exp  ressed in frame coords  1  or in shorthand notation                  x y z  1  =  R 21              x y z  2  +        x o y o z o             21    3.8       If  one  goes  through  a  series  of  transformations,  the  operations  become even more cumbersome. For the case of two transformations  the equations are                  x y z  2  =  R 32              x y z  3  +        x o y o z o           32                  x y z  1  =  R 21              x y z  2  +        x o y o z o           21                  x y z  1  =  R R 32  21  +  R 21              x y z  3        x o y o z o        32  +        xx o y o z o            21                 and  or   HOMOGENEOUS TRANSFORMATION     89   This can be written more concisely as a single operation using the   homogeneous  transformation.   For  a  single  transformation  containing  translation and rotation  where for   A21 we have                    x y z 1  1  =  A 21                x y z 1  2              =     A 21  R 21  0  0  0  x o y o z o 1                3.9       3.10       Note  that  the  upper  left  three - by - three  matrix  is  the  rotation  matrix  while  the  upper  portion  of  the  right  column  is  comprised  of  the origin of frame 2 in frame 1 coordinates. Here   xo,   yo and   zo could  represent, for example, the origin of the sensor frame in vehicle coor- dinates. One can use this transformation to convert a vector speciﬁ ed  in one set of coordinates to its expression in another set of coordinates  in  a  single  operation. When  using  this  homogeneous  transformation,  the position vectors are converted to dimension four by appending a  1  as  the  fourth  entry. This  is  necessary  not  only  to  make  the  matrix  operations conformal, but also to couple in the location of the origin  of the second coordinate system with respect to the original coordinate  system.      EXAMPLE 4     Let frame 2 be both rotated and displaced with respect to frame 1. The  rotation is a yaw of 90 degrees   =     R21        0 1 0  − 1 0 0 0 0 1                     90     ROBOT ATTITUDE   and the displacement of the origin of frame 2 with respect to frame 1 is     Now let the point of interest be given by             x 0 y 0 z 0        =        10 5 0                            x y z  exp  ressedi in frame  2  =        1 0 0              Express this vector in frame 1.       SOLUTION 4     In frame 1 the expression of this vector becomes                   x y z  exp  ressed in frame  1  =        0 1 0  − 1 0 0 0 0 1         1  0  0         +        10 5 0             Now solving this problem by using the homogeneous transformation   matrix we have                   x y z                    x y z 1  =  +        0 1 0               10  5  0    =        10 6 0            exp  ressed in frame  1  =         0 1 0 0  − 1 0 10 0 5 0 0 0 1  0 1 0                1 0 0 1               exp  ressed in frame  1                  or    HOMOGENEOUS TRANSFORMATION     91   or  or                 where   The homogeneous transformation matrices can be multiplied just as  the  rotation  matrices  can.  Thus  the  homogeneous  transformation  to  take a vector from its expression in frame 3 coordinates to its expres- sion  in  frame  2  coordinates  and  ﬁ nally  to  its  expression  in  frame  1  coordinates can be written  = [  ][  A A 32  21  ]    3.11                   x y z 1  3       =         10 6 0 1             exp  ressed in frame  1                    x y z 1                    x y z 1  1                    x y z 1  1  = [  ]  A 31                x y z 1  3     [     A 31  ] = [  ][  A A 32  21  ]     Another interesting and useful property of the homogeneous trans-  formation matrix is that its inverse can be expressed as             − 1  x o y o z o 1         =         R 21  T R 21  0  0  0  0  0  a b c 0 1                3.12      92     ROBOT ATTITUDE  where the entries in the upper part of the last column are deﬁ ned by    3.13       In  all  of  these,  use  has  been  made  of  the  fact  that  for  a  rotation                   a b c  = −  T R 21        x o y 0 z o             − =1     R  RT     since the rotation matrix is orthonormal.    This  homogeneous  transformation  provides  a  concise  means  of  expressing a vector in an original frame when the second frame is both  rotated  and  translated  with  respect  to  the  original  frame.  Its  conve- nience becomes even more pronounced when there is a series of trans- formations, e.g., sensor frame to vehicle frame and then vehicle frame  to earth frame.       3.7      ROTATING A VECTOR    Another important application of the rotation matrix is to express the  new  coordinates  of  a  vector  after  the  vector  itself  has  been  yawed,  pitched and rolled. Here the same coordinate frame is used before and  after the rotation. To illustrate, consider an initial vector expressed in  frame 1. This vector is now rotated about the z axis. The expression for  this rotated vector, again in frame 1, is given by the following             x  y    z after rotation  =        ψ cos ψ sin 0  ψ ψ  − sin cos 0   0  0  1          x  y    z before rotation           matrix        or                  x y z  =  R  yaw     ψ              x y z  after rotation  before rotat  iion         3.14        This same process holds for each of the rotations. Thus if a vector is  rotated ﬁ rst about the  y  axis, then about the  x  axis and ﬁ nally about  the  z  axis, the new vector in the original frame is given by  EXERCISES     93  ψ θ φ ψ θ φ  sin sin sin cos sin sin θ φ  − ψ θ sin cos      ψ θ cos cos θ sin  φ ψ nn + cos si ψ φ − sin sin  ψ θ φ  sin sin cos  ψ θ φ cos ssin cos   θ φ  cos cos     =   x  y    z after rot − ψ φ cos cos + ψ φ sin cos − cos sin   x  y    z beforee rot                       or                  x y z  =  ,  ψθφ   ,  R              x y z  after rotation  before rota  ttion         3.15       To reiterate, in this second application of rotation matrices, the vector  on the right - hand side of the equation is the vector before its rotation  and the result on the left - hand side is the vector after its rotation. Both  vectors are expressed in the same frame.    We shall ﬁ nd important uses for these rotation matrices and homo-  geneous transformation matrices in the chapters that follow.     EXERCISES      =  = −         1.     Evaluate  2 ,  the    ψ π θ π   the  2 ,    ψ θ π        2.     Evaluate  =  0 ,  =  rotation  matrix  and  φ =  0 .     2 ,  rotation  matrix  φ π =    2 .     and  for   the   case  where   for   the   case  where       3.     Evaluate the homogeneous transformation for the case where the  second  frame  has  orientation  with  respect  to  the  ﬁ rst  frame  of    ψ π θ π = φ 0     and  location  with  respect  to  the  ﬁ rst  frame of  x     =    3,  y     =    2,  z     =    1.     and  = −  2 ,  2 ,  =      94     ROBOT ATTITUDE      4.     Evaluate the homogeneous transformation for the case where the  second  frame  has  orientation  with  respect  to  the  ﬁ rst  frame  of  φ π   ψ θ π = 2 ,    and location with respect to the ﬁ rst frame  of  x     =    1,  y     =    3,  z     =    2.     and  0 ,  =  =      5.     Given a target whose location is expressed in frame 2 as  x     =    1,  y     =    2,   z     =    0, ﬁ nd its location with respect to frame 1. The origin of frame 2  is at  x     =    20,  y     =    10 and  z     =    1 with respect to frame 1. The orientation  of  frame  2  with  respect  to  frame  1  is    ψ π θ π 0   .  Solve  using  a  rotation  plus  a  translation  and  also  solve  using  the  homogeneous transformation     and  φ =  , 2  , 4  =  =        =  4 ,  , 4  φ =  = −  and      6.     A target is located in frame 3 at coordinates  x     =    3,  y     =    2,  z     =    1. The  origin of frame 3 is at coordinates  x     =    10,  y     =    0,  z     =    0 with respect to  frame  2.  The  orientation  of  frame  3  with  respect  to  frame  2  is    ψ π θ π . The origin of frame 2 is at coordinates  0    x     =    0,  y     =    5,  z     =    0 with respect to frame 1. The orientation of frame 2  with respect to frame 1 is   ψ π θ 2 .  A  Compute the  homogeneous  transformation  describing  frame  3  with  respect  to  frame 2 and determine the location of the target in frame 2.  B  Next  compute the homogeneous transformation describing frame 2 with  respect to frame 1 and determine the location of the target in frame  1.   C   Finally  multiply  the  homogeneous  transformations  together   in  the  proper  order   and  determine  the  location  of  the  target  in  frame 1 in a single step.     φ π    and  , 0  =  =  =  ,      7.     A vector has coordinates   [  1 0 0 ′. This vector is to be rotated about  the y axis by the amount   π  2. Use the rotation matrix to determine  the resulting vector?     ]      8.     A vector has coordinates   [  1 0 0 ′. This vector is to be rotated about  the y axis by the amount   π  2 and then about the z axis by the amount    −π  2. Use the rotation matrix to determine the resulting vector?        ]    REFERENCES   NJ ,  1991 .    Press ,  Princeton, NJ   1999 .    University Press ,  2006 .              Brogan ,  W. L.    “  Modern Control Theory  ” ,  Prentice Hall ,  Upper Saddle River,       Kuipers ,   J.  B.  ,   “  Quaternions  and  Rotation  Sequences  ” ,   Princeton  University       Lavalle ,   Steven  ,    M.     “  Planning  Algorithms     Motion  Planning  ” ,   Cambridge      4   ROBOT NAVIGATION           4.0      INTRODUCTION    This chapter introduces the topic of navigation and the various means  of accomplishing this. The focus is on inertial navigation systems,  gim- baled and strap - down  and the Global Positioning System  GPS . Also,  brieﬂ y  discussed  is  deduced  reckoning  utilizing  less  sophisticated  methodology.       4.1      COORDINATE SYSTEMS    One deﬁ nition of navigation is the process of accurately determining  position and velocity relative to a known reference or the process of  planning  and  executing  the  maneuvers  necessary  to  move  between  desired locations. One important factor in navigation is an understand- ing of the different coordinate systems. Figure  4.1  shows a sphere rep- resenting the earth along with several coordinate frames. To minimize  confusion only the  x  and  z  axes are shown. The  y  axes in each case are  such as to form right - handed coordinate systems.      Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  95   96     ROBOT NAVIGATION  IIIZ  IX  North   I ZZ ,  II  IVY  lat  long  X ,  III Z  IV  IVX  IIX       Figure 4.1       Earth and Several Different Coordinate Frames      EARTH - CENTERED EARTH - FIXED      4.2   COORDINATE SYSTEM    In Coordinate System I, the  z  axis points out the North pole, the  x  axis  points through the equator at the prime meridian, and the  y  axis  not  shown  completes the right - handed coordinate frame. This set of axes  is  called  the  earth - centered  earth - ﬁ xed  axes   ECEF .  As  the  name  implies,  this  set  of  axes  has  its  origin  at  the  center  of  the  earth  and  rotates with the earth. There is a unique relation between the ECEF  coordinates  of  a  point  on  the  surface  of  the  earth  and  its  longitude,  which is measured positively Eastward from the prime meridian running  through Greenwich, England and its latitude, which is measured posi- tively Northward from the equator. Starting with latitude and longitude  the   X ,   Y   and   Z   in  Earth  Centered  Earth  Fixed  coordinates  can  be  determined approximately assuming a spherical model of the earth of  radius   R and using the equations = cos  = cos  = sin     4.1b        X R    4.1a        Y R    4.1c         Z R                cos    sin   long  long  lat  lat  lat                   EARTH-CENTERED EARTH-FIXED COORDINATE SYSTEM     97   It should be pointed out that the earth is not a perfect sphere and  that  more  precise  models  of  its  shape  do  exist.  These  more  precise  models account for the ﬂ atness of the earth, i.e., the fact that the radius  at the poles, 6,356.7   km, is slightly less than the radius at the equator,  6,378.1   km. The spherical model is used in examples here for its simplic- ity in application.     For Long      =     85    deg W and Lat     =    42    deg N ﬁ nd X,Y,Z in ECEF coordi- nates. Use dimensions of meters and assume the point is on the earth ’ s  surface.        EXAMPLE 1     SOLUTION 1     Since  the  longitude  is  85  degrees  to  the  West,  it  is  expressed  as    −  85  degrees.      X R  cos   Lat   cos   Long  6378137  cos   42     Y R  cos   Lat  Long     6378137  cos   42      Z R  sin   Lat     6378137  sin   42     , 4 267 806 678  ,  .  m        =   sin  =  =   =  = = =  − =   85  cos  − = −   85   sin   413 107 719  .  ,  m     , 4 721 842 835mm    ,  .   In the example above we have used the equatorial radius of the earth  for  R . The process can be reversed if the ECEF coordinates are given  and the latitude and longitude have to be determined.     lat  =  − tan 1        long  =  − tan 1          X Y  2  Z + 2            Y X    4.2a       4.2b               EXAMPLE 2     For the point at ECEF coordinates, X     =    3,000,000    m, Y      =      − 5,000,000    m,  and  Z      =       −  2,638,181    m,  ﬁ nd  Lat  and  Long   Note:  The  point  may  be  slightly off the earth ’ s surface .      98     ROBOT NAVIGATION    SOLUTION 2   ,  ,     X = 3 000 000      Y = −5 000 000    Z = −2 638 181  ,  ,  ,  ,                                    Long  =    180  π    * tan  − 1    = −     6  − × 5 10 × 6 3 10   6  =  π    * tan  5 83095 10 . − 2 638181 10 . 6 5 83095 10 .        6 400 000 ,  × ×  ×   =  m  Z  +  − 1  ,  2  2      X Y2  +  2     Lat  =    180  =     R  +  2  X Y  59 03  .  deg or  59 03  .  deg W     6    = −  . 24 4  deg or  224 4. deg S       4.3      ASSOCIATED COORDINATE SYSTEMS    Other coordinates are useful in describing motion on the surface of the  earth, and some of these shown on the ﬁ gure above are now described.  The relationships between the variables of coordinate systems II and I  are given below. Here the angles referred to as   lat and   long are assumed  to be expressed in radians while angles referred to as  Lat  and  Long  are  assumed to be expressed in degrees.            X Y Z        II    = −     cos long sin long 0  sin cos  long long 0        0 0 1         X  Y    Z I        Coordinate frame II has been rotated counter clockwise about the    ZI axis by an amount   long. This corresponds to a new frame with the    X II axis now pointing through the equator at longitude   long. Note that  this matrix is given by   R    or   R  long  long  −1     .   T   yaw  yaw   The  relationships  between  the  variables  of  coordinate  systems  III   and II are given below.       X Y Z               III  =        cos lat 0 sin  lat  −  0 1 0  sin lat 0 cos lat               X  YY    Z II        Coordinate frame III has been rotated clockwise about the   YII axis  by  an  amount    lat. The    X III  axis  now  points  through  the  meridian  of     4.3        4.4       ASSOCIATED COORDINATE SYSTEMS     99  longitude   long and the parallel of latitude   lat. Note that this matrix is  given by   R     or   R lat    or   R  − −1    lat  lat   .   −  T   roll  roll    roll   The  relationships  between  the  variables  of  coordinate  systems  IV   and III are given below.       X Y Z               IV  =        0 1 0 0 0 1 1 0 0              X Y Z        III  +        0 0 − R               4.5       For coordinate frame IV, the origin has been moved from the center  of the earth to the surface of the earth. The   YIV axis is parallel to the    ZIII  axis,  the    ZIV   axis  is  parallel  to  the    X III  axis,  and  the    X IV  axis  is  parallel to the   YIII axis. One can think of the orientation of frame IV  as one obtained by rotating frame III about its z axis by 90 degrees ccw  and then rotating about the new x axis by 90 degrees ccw. The rotation  matrix is   [  π T      ]  π      ]  π       2   where  R  R  R  R  2  2  pitch  pitch  yaw  1−  or   [       =  0 1 0  π 2       yaw −  1 0  0 0    1 0        Ryaw     π 2     Rpitch     π 2  =        1 0 0 0 0 1   0  − 1    0       This coordinate frame attached to the surface of the earth with the   y  axis pointing North, the   X IV axis pointing East and the   ZIV axis point- ing outward from the earth ’ s surface is a useful local coordinate system.  One can describe x - y locations with respect to this frame in terms of  longitude and latitude given the longitude and latitude of the origin of  the coordinate system.    By assuming a spherical earth and performing the above operations   in succession, starting with an initial point            X Y Z        I  =        R R  cos long sin long sin R  cos cos lat  lat lat           and deﬁ ning the latitude and longitude of the origin of the ﬁ nal frame  to be   long0 and   lat0 we get:  and            100     ROBOT NAVIGATION  R  IV = −    X IV = − R Y     − R  IV =     Z  IV =    X R IV = −    Y  R  which reduce to  cos  long  sin  long  0  cos  lat R  sin  long  cos  long  cos  lat     0  +  cos sin  long long  0  long long  cos cos  lat lat  sin iin s  0  lat lat R  0  sin  lat  cos  lat  cos sin −  0  cos  R  cos   long  0  long  lat  cos  lat R  0  sin  lat  0  sin  lat R        0 −  cos  lat  sin   long long  −   0     −  cos  lat  sin  lat  cos   long long     0  0  R  sin  lat  cos  lat  0        4.6b       4.6a     + +  +  R  cos  lat  cos  lat R  0  sin  lat  0  sin  lat R R  cos   lat  −  =  +  −  lat  0     −  R        4.6c       For points on the surface of the earth in the vicinity of the origin of  the ﬁ nal frame these equations for x and y may be approximated quite  accurately as     X  IV ≈ IV ≈    Y  −  R  cos  lat −  lat   [ long long ]0       R lat  [  ]0         ZIV ≈ 0        4.7a       4.7b       4.7c       EXAMPLE 3     A  local  coordinate  system  is  set  up  at  Long      =     70    deg W      =      − 70    deg  and  Lat      =     38    deg N. A mobile robot is at Long      =     69.998    deg W      =      − 69.998    deg  and  Lat      =     38.001    deg  N.  Find  the  X,Y  coordinates  for  the  robot. Take  X - East and Y - North.       SOLUTION 3   X      local = = local =    Y  −  .    R Long Long 0   . , 6 378 137 002 π       R Lat Lat  , −     0   cos  788    180     0  π      180   Lat π 180 =      . 175 4   = , 6 378 137 001   .  ,  m     π       180     =  111 3 .  m        and  IV ≈     Z  and                                  ASSOCIATED COORDINATE SYSTEMS     101  IVY  VY  +   a  VX  IVX       Figure 4.2       Local Coordinate System with X Axis Rotated Relative to East     If we now go to a ﬁ nal local coordinate system rotated such that the   x  axis of frame V is at an angle   α with respect to the  x  axis of frame  IV i.e., East, then we have the frame shown in Figure  4.2 .      The appropriate rotation matrix is given by              = −     cos sin 0  sin cos 0  α α  X Y Z  α α            0 0 1        V  X Y Z             IV       4.8       Note that this transformation matrix is given by   Ryaw   α −1 or   Ryaw     α .  T For all of these transformations in the preceding, the inverse  or trans- pose   is  required  because  the  coordinates  are  being  converted  from  their expression in the old frame to their expression in the new frame  rather than vice versa as was the case considered as the rotation matri- ces were derived.    Applying  the  above  transformation  to  equation    4.7    yields  as  the   expression for the coordinates in frame V +  long long  cos cos  lat  α  V =     X  R  −  [  ]     0  0  α sin [  lat  R  −  lat  0  ]         4.9a     V = −    Y  R  α  sin cos  lat  0  [  long long  ]  0  −  +  α cos [  lat  R  −  lat  ] 0        4.9b     and       EXAMPLE 4     Re - do the previous example but with X - Y axes now rotated by 30 degrees  clockwise.      102     ROBOT NAVIGATION    SOLUTION 4                local =  X       cos  0 − Lat Lat  α cos     Long Long R + π 180       R − 30  175 4  − α sin     + 111 3 − α cos     +     . sin   . cos  α sin      cos  Long Long 0 − + 180   tt0 Lat La R − − 30 30  . cos   . sin   111 3     00 − 30    175 4  local =    X local = − R Y     π     local = −    Y    Lat 0 π      180 =  96 24  .        m       180        Lat 0 π      =     184 09  .  m           UNIVERSAL TRANSVERSE MERCATOR   UTM        4.4   COORDINATE SYSTEM    A  commonly  used  coordinate  system  is  the  Universal  Transverse  Mercator   UTM   Coordinate  System.  The  more  common  Mercator  projections result from projecting the sphere onto a cylinder tangent  to the equator. Regions near the poles are greatly distorted appearing  larger than they really are. Regions near the equator are most accurate.  The purpose of such a projection is to convert the spherical shape of  the earth to a ﬂ at map.    Transverse  Mercator  Projections  result  from  projecting  the  sphere  onto a cylinder tangent to a central meridian. Regions near the central  meridian  are  projected  most  accurately.  Distortion  of  scale,  distance,  direction and area increase as one moves away from the central merid- ian.  In  the  UTM  coordinate  system,  longitudinal  zones  are  only  six  degrees of longitude wide, extending three degrees to either side of the  central meridian. Transverse Mercator maps are often used to portray  areas with larger north - south than east - west extent. In the UTM coor- dinate system, these 6 degree longitudinal zones extend from 80 degrees  South  latitude  to  84  degrees  North  latitude. There  are  sixty  of  these  longitudinal zones covering the entire earth labeled with the numbers  1 through 60. Each longitudinal zone is further divided into zones of  latitude, beginning with zone C at 80 degrees south up to M just below  the equator. To the North the zones run from N just above the equator  to X at 84 degrees north. All the zones span eight degrees in the north -  south direction except zone X, which spans 12 degrees.    Within  each  longitudinal  zone,  the  easting  coordinate  is  measured  from  the  central  meridian  with  500   km  false  easting  added  to  ensure  positive  coordinates.  The  northing  coordinate  is  measured  from  the  equator with a 10,000   km false northing added for positions south of    UNIVERSAL TRANSVERSE MERCATOR  UTM  COORDINATE SYSTEM     103  the  equator. The  coordinates  thus  derived  deﬁ ne  a  location  within  a  UTM  longitudinal  zone  either  north  or  south  of  the  equator,  but  because  the  same  co - ordinate  system  is  repeated  for  each  zone  and  hemisphere, it is necessary to not only state the northing and easting  but to also state the UTM longitudinal zone and either the hemisphere  or latitudinal zone to deﬁ ne the location uniquely world - wide.    The following are formulas relating latitude and longitude to UTM.  Given  a  latitude  and  longitude  the  UTM  coordinates  can  be  deter- mined by ﬁ rst computing the longitudinal zone number  =     i  int  180     + Long 6    +  1        4.10       Here longitude in the westward direction would be taken as negative.   The Central Meridian for the longitudinal zone is then given by:     Long  0  = − [  177  + −     i  1 6   ]        4.11       Using the spherical earth approximation and also ignoring the dis- tortion  in  projecting  the  sphere  onto  the  cylinder,  one  may  roughly  compute northern and eastern as  ≈  π 180        Northing R Lat −    Easting R Long Long  ≈     π       0  180   cos   Lat     500 000  ,         4.12b      +    4.12a      The  correct  calculation  of  northing  and  easting  uses  ellipsoids  to  account  for  the  true  shape  of  the  earth  and  the  distortion  caused  by  the projection. Among other effects this model yields a larger northern  for  points  lying  off  the  central  meridian  than  for  points  of  the  same  latitude lying on the central meridian.                 Find the UTM coordinates for the point Long      =     10    deg East, Lat      =     43    deg  North.        EXAMPLE 5     SOLUTION 5     Again a spherical model for the earth is used with    R  . Also,  the distortion in projecting the strip about the central meridian onto the   m  = 6 378 137  ,  ,   104     ROBOT NAVIGATION  cylinder  is  ignored.  Using  these  approximations  yields  the  following  rough values for the solution   deg N     ,     i  =  =  1    10  43  Int  180     Long  deg E Lat +  Long  + 6  =      i = 32      Central Meridian Calculation   = − [ 9    Long0 −    Long Long  177  + =     0  =  − 32 1 6   ] − = 10 9 1 deg   π 180     −  =     ≈  deg       Northing RLat      Easting R Long Long  ≈     .  ,  4 786 938 104 , π        cos   180  0  m     0  43     +  500 000  ,  581413 92  .  m        =   The UTM system is sometimes preferable to longitude and latitude  in specifying relative locations because of the linear scale and because  most persons are familiar with the unit of meters.       4.5      GLOBAL POSITIONING SYSTEM    The Global Positioning System  GPS  provides a means for a receiver  or  user  to  determine  its  location  anywhere  on  or  slightly  above  the  surface of the earth. This is sometimes referred to as geolocation. The  GPS system includes a constellation of twenty - four earth - orbiting sat- ellites which are situated in such a way as to maximize coverage of the  earth. Their orbital radii are approximately 20,200 kilometers, and they  are spaced in six orbits with four satellites per orbit. The orbits have  inclination angles of 55 degrees with respect to the equator, and their  orbital period is twelve hours. Each satellite is equipped with an atomic  clock and a radio transmitter and receiver.    The status and operational capability of the satellites is monitored  by  a  series  of  ground  stations  with  antennas  stationed  in  different  parts of the world as well as a master control station. The entire opera- tion  depends  on  the  use  of  encoded  radio  signals.  The  Standard  Positioning Service  SPS  utilizes a 1.023   MHz repeating pseudo random    GLOBAL POSITIONING SYSTEM     105  code called Coarse Acquisition  C A  code and is available for public  use. The Precise Positioning Service  PPS  utilizes a 10.23   MHz repeat- ing pseudo random code called Precise Acquisition  P  code. It can be  encrypted to make it available for use by the Department of Defense  only.    Geolocation is based on the use of modulated radio signals transmit- ted from the satellites and received by the user. Based on signal travel  time one can determine distance. Distance calculations from the user  to the visible satellites combined with knowledge of the satellite posi- tions at the time of the signal transmission allow one to use triangula- tion and thereby determine the user location.    By  computing  the  time  duration  of  one  bit  in  the  pseudo  random  code and multiplying this by the speed of light, one can determine the  potential  resolution  of  distance  calculations  from  the  satellite  to  the  user. The C A Code provides the potential for distance resolution of  30 meters or better while the P code provides the potential for distance  resolution  of  3  meters  or  better.  GPS  systems  used  in  the  surveying  mode,  where  the  receiver  is  stationary  for  hours,  have  the  capability  for distance resolution in the centimeter range.    The GPS ground stations as well as the GPS satellites utilize atomic  clocks. Time is measured starting at 24:00:00 January 5, 1980. No leap  seconds are included in GPS Time. Receiver clocks are not as accurate  as the atomic clocks and normally exhibit bias. This bias creates errors  in the determination of travel time of the signals and therefore causes  errors in calculation of the distances to the satellites. These distances  to all satellites will be in error by the amount of the local clock error  multiplied by the speed of light.    In addition to distance errors caused by the local clock error, atmo- spheric  effects  can  also  cause  errors.  There  are  ionospheric  delays  caused by the layer of the atmosphere containing ionized air, and there  are  also  tropospheric  delays  caused  by  changes  in  the  temperature,  pressure and humidity of the lowest part of the atmosphere.    Apart from geolocation errors caused by errors in the distance cal- culations to the satellites, there are also geolocation errors caused by  incorrect satellite ephemeris data. The ephemeris errors may be decom- posed into tangential, radial and cross track components. Radial ephem- eris errors have the greatest impact on geolocation errors.    Another source of error is caused by multipath transmission. Here  reﬂ ected signals near the receiver may interfere or be mistaken for the  original signal. Because multipath signals have a longer route, the com- puted distance from the satellite will be greater than the actual distance.    106     ROBOT NAVIGATION  Multi - path  transmission  is  difﬁ cult  to  detect  and  sometimes  hard  to  avoid.    GPS receivers receive and process signals from the in - view satellites.  In the case of the P code which provides most precise geo - location, a  unique segment of the 10.23   MHz PRN code is generated at each satel- lite and is known ahead of time by the receiver. This P code operating  at 10.23 chips per microsecond repeats only once per week. It is com- bined  with  50   bps  data  sequences  via  the  exclusive - or  operation. The  data sequence, which consists of time - tagged data bits marking the time  they  are  transmitted,  is  sent  every  30  seconds  and  contains  the  Navigation Message, which is information regarding the GPS satellite  orbits, clock correction and other system parameters. The carrier signal  is either the L1 sinusoid operating at 1,575.42   MHZ or the L2 sinusoid  operating at 1,227.6   MHz modulated by the data - modulated 10.23   MHz  PRN using binary phase shift keying  BPSK .    In the case of the C A code which provides less precise geo - location,  a  1.023   MHz  PRN  code  is  generated  at  each  satellite,  and  it  is  also  known ahead of time by the receiver. This C A code operating at 1.023  chips per microsecond repeats once per millisecond. It too is combined  with  50   bps  data  sequences  via  the  exclusive - or  operation.  The  data  sequence is sent every 30 seconds and contains the Navigation Message  The carrier signal here is the L1 sinusoid operating at 1,575.42   MHZ.   Basically,  the  system  operates  by  the  receiver  noting  on  its  local  clock the time at which signals are received from satellites. This time  of  arrival  is  computed  by  shifting  within  the  receiver  the  known  segment  of  the  P  code  for  the  particular  satellite  and  correlating  it  with the received signal. The correlation will be maximized when the  shift corresponds to the time of arrival. Correlators have been devel- oped that permit simultaneous correlations of the received signal with  thousands of different time shifted signals providing rapid determina- tion  of  the  time  of  arrival.  Under  normal  operation  the  50   bps  data  sequence is not known beforehand. This limits the duration of the time  sequences used for the correlation to 1 50 th  of a second. Usually this  duration of signal is sufﬁ cient, but under noisy environments this limi- tation  can  cause  a  problem.  Longer  correlation  durations  have  the  effect  of  reducing  the  noise  impact  inversely  to  correlation  duration,  thereby  making  the  determination  of  the  correlation  peak  more  accurate.    Knowing the time the signal was transmitted from the time - tagged  data,  and  having  determined  the  time  of  arrival  with  respect  to  the  receiver  clock,  i.e.,  the  local  clock,  the  travel  time  for  each  received  signal may be computed and converted to the distance from the receiver    GLOBAL POSITIONING SYSTEM     107  to the respective satellites. This travel time will be accurate within local  clock error plus the error in correlator alignment. If correlator align- ment  is  correct  within  one  chip  of  the  P  code,  this  corresponds  to   3    ×    10 ^ 8  meters  per  sec.   10.23    ×    10 ^ 6   Hz   or  approximately  30  meters. If the correlator alignment is correct within one hundredth of  a chip width, the error in distance would be less than 0.3 meters. These  ﬁ gures assume that the local clock error has been perfectly accounted  for.  In  the  case  of  the  C A  code,  the  error  in  distance  caused  by  an  alignment error of one chip is approximately 300 meters. If the correla- tor alignment is correct within one hundredth of a chip width, the error  in distance would be less than 3 meters. The result of performing these  correlations on signals received from all the visible satellites is a set of  pseudo ranges from the receiver to these visible GPS satellites.    Knowing the distance to a single satellite, along with the knowledge  of that satellite ’ s position at the time the signal was transmitted, places  the receiver on a sphere centered about that point. With signals from  two  satellites,  the  receiver  is  placed  on  a  sphere  about  each  of  two  points with their intersection being a circle. Using a measurement from  a third satellite places the receiver on a sphere about this third point.  The  intersection  of  this  sphere  with  the  previously  described  circle  yields two points, and only one of these is near the surface of the earth.  Thus  in  principle,  three  GPS  satellites  are  sufﬁ cient  to  locate  the  receiver if there is no local clock error. In practice, an error does exist  between the local clock and the GPS clock of the ﬂ eet of satellites. In  order to correct for this local clock error, signals must be received from  a fourth satellite. This extra equation allows one to determine the three -  dimensional position as well as the local clock error. If more than four  satellites are visible, the redundancy can be used to reduce other types  of errors.    Geometric  Dilution  of  Precision   GDOP   is  computed  from  the  geometric relationships between the receiver position and the positions  of  the  satellites  the  receiver  is  using  for  navigation.  If  there  is  not  a  good spread among the visible satellites, GDOP will be high and the  computed position of the receiver is more sensitive to small errors in  distance calculations and satellite positions. Imagine that two satellites  are  close  together. The  distance  from  each  of  these  satellites  to  the  receiver  yields  a  sphere  about  the  respective  satellite.  Since  the  two  spheres are approximately the same size and have approximately the  same center, then their intersection will be very sensitive to any kind  of  error.  GDOP  components  include  position  dilution  of  precision   PDOP , horizontal dilution of precision  HDOP , vertical dilution of  precision  VDOP  and time dilution of precision  TDOP .    108     ROBOT NAVIGATION   Differential GPS  DGPS  provides improved precision in the com- puted  location  of  the  receiver.  Here  one  receiver,  a  base  station,  is  placed at a surveyed location, and the other receivers are free to rove.  The  difference  between  the  computed  position  and  the  known  true  position for the base station is evaluated. These errors and information  about the different factors contributing to the errors are broadcast to  all the roving receivers for their use. By this means the GPS accuracy  can  be  substantially  improved  through  canceling  the  effect  of  the  common — mode errors. The effectiveness of DGPS degrades when the  rovers are separated from the base station by as much as tens of miles.  For this system to be successful, the base station must, at a minimum,  broadcast the following set of information for each satellite:              Satellite identiﬁ cation number              Range correction              Ephemeris set identiﬁ er              Reference time         COMPUTING RECEIVER LOCATION USING  GPS ,      4.6   NUMERICAL METHODS    Having  performed  the  correlations  of  the  signals  received  from  the  visible satellites with the shifted signals generated within the receiver,  the times of arrival of the signals from the satellites may be extracted.  Then the travel times are determined and the pseudo distances from  the receiver to the visible satellites are computed. Once this has been  accomplished, one can proceed to an iterative process for the determi- nation of the receiver location.     Computing Receiver Location Using  GPS  via      4.6.1   Newton ’ s Method   The following is the system of nonlinear equations that are based on  the measurements of distances from four or more different satellites to  the receiver. Here the   di are computed from travel time of the signals  multiplied by the speed of light,    xi,   yi,   zi  are the ECEF coordinates of  satellite i,   x y ,  z   are the assumed ECEF position coordinates of the  receiver antenna,   tb is the receiver clock bias, and c is the speed of light.  The unknowns are the position coordinates of the receiver,   x y ,  z   and  the  local  clock  error,    tb.  Given  the  actual  receiver  location  and  the    COMPUTING RECEIVER LOCATION USING GPS, NUMERICAL METHODS     109  actual  local  clock  bias  correction  the  equations  below  would  be  satisﬁ ed.     [     [     [     [   1  x  2  x  3  x  4  x  − − − −  2  x     x  x  2    2     2  x     + + + +  1     y  2     y  3     y  4     y  − − − −  2    y     1 z  + + + +  − − − −    ] . 2 0 5 z   ] . 2 0 5 z   ] . 2 0 5 z   ] . 2 0 5 z  = = = =  1  d  2  d  3  d  4  d  + ctb + ctb + ctb + ctb                      2     z  4     z  4     z  2    y 2    y  2    y    4.13a       4.13b       4.13c       4.13d       The  distance  calculations  on  the  left - hand  side  are  based  on  the  known locations of the satellites at the time the signals were transmit- ted  and  the  current  estimate  of  the  receiver  location. The    di  on  the  right - hand side are computed from travel time of the signals multiplied  by the speed of light. This pseudo distance from each satellite to the  receiver  is  then  corrected  for  the  local  clock  bias. The  correct  values  for   x,   y,   z and   tb should cause all these equations to be satisﬁ ed. Here  equations are shown for four satellites, the minimum required to yield  the location of the receiver and the correction for the local clock bias.  If  more  satellites  are  visible,  more  equations  may  be  added  to  those  below providing an even more accurate solution.    As a matter of fact the locations of the GPS satellites are not per- fectly  known. Also  there  are  errors  in  the  determination  of  time  of  arrival of the signals from the satellites causing the pseudo distances to  have errors. This problem including the random errors is an estimation  problem which will be treated in a later chapter following the introduc- tion  of  the  Kalman  Filter.  For  now  the  problem  will  be  treated  as  a  deterministic problem that assumes perfect knowledge of the satellite  positions and the pseudo distances.    These equations may be re - arranged to express the error between  the ranges from the assumed receiver location to the respective satel- lites and the corrected pseudo ranges as determined from signal time  of travel.  1  2  3  4     E    E    E    E  = = = =  1  [   x  2  [   x  3  [   x  4  [   x  − − − −  2  x     x  2    2  x     2  x     + + + +  1     y  2     y  3     y  4     y  − − − −  2    y  2    y 2    y  2    y  1  d  + + + +     1 z  2     z  4     z  4     z  − − − −  . 2 0 5   ] z . 2 0 5   ] z . 2 0 5   ] z . 2 0 5   ] z  −   − − −  2     d  3     d  4     d  + ctb + ctb + ctb + ctb                                    4.14a       4.14b       4.14c       4.14d       Since the equations are nonlinear, the solution is not straightforward  but rather requires an iterative process. First one makes an initial guess                            110     ROBOT NAVIGATION  at  the  receiver  location  and  the  local  clock  error.  Zero  would  be  a  reasonable ﬁ rst guess for the local clock error. Now unless one made  a perfect guess of the receiver location and the local clock error, the  above nonlinear equations would not be satisﬁ ed. Thus the actual posi- tion of the receiver must be determined by a series of corrections to  the assumed location. One approach is to use Newton ’ s method to force  the above error vector to zero.    The equations for the errors may be written more concisely as  =  i     E  [   X X  i T      X X  i   ]  .0 5  −  −  −  i     d  +  ct  b   ,  i  =  1 cid:1      N    4.15     where the receiver location is  and the i th  satellite location is  =     X              x y z     =  i     X        i  i  x y i z             Here  N  is the number of visible satellites.   Deﬁ ning  i     r  =  −  x x  i  {   2     +  −  y y  i     2     +  −     i z z  2 5     }.  the above equations simplify to  =    E r  i  i  −  i     d  +  ct  b   ,  i  =  1 cid:1      N    4.16       One  can  now  expand  the  error  equation  using  the  Taylor  series   through the linear terms to obtain  +  ∆ E  =     E  [ + ∂  ∂  E X     ∂    ∂ EE ct  b  ]         4.17             1  2  r r  − −       + +  ct b ct  b       N  r  −     +  ct  b     1  d 2 d  cid:1  N d                ∆ x ∆ y ∆ z ∆ ct  b                                  and  COMPUTING RECEIVER LOCATION USING GPS, NUMERICAL METHODS     111   Differentiating the equation for the error yields     ∂  ∂  i  E X     =  [   X X  i T      X X  i  −  −  − .0 5  ]  −     X X  i T    =  −     X X  i T       1 i r     ∂  ∂  i  E ct     b  = −  1      Note that this last derivative is with respect to   ctb, which has dimen- sions of distance rather than with respect to   tb, which has dimension of  time. The reason for this is that the other derivatives of the error were  taken with respect to distance. Doing this removes one potential source  of  numerical  errors,  large  differences  in  scale.  In  expanded  form  this  becomes  +  ∆ E  =  E                  1  d 2 d  cid:1  N d  + +  ct b ct  b       +  ct  b            −     x  1  xx     2        −  x x  cid:1          r  +  − −  1  2  r r  N            − 1 1 r 1 2 r  1 N rr  −  y y     1     2        −  y y  cid:1   1 1 r 1 2 r  1 N r  −     1 z z     2        −  z z  cid:1   1 1 r 1 2 r  1 N r  − 1  − 1  cid:1                     ∆ x ∆ y ∆ z ∆ ttb c         −  x x     N     −  y y     N     −  z z     N     − 1  ‘           4.18       One seeks the appropriate values for   ∆E to force this error to zero.  0  and  solving  for  the  changes  in  estimated  receiver   E+  =∆  Setting    E location and receiver clock bias yields     [ = − ∂  E X E ct        b  ∂  ∂  ∂  − 1  ]    4.19                ∆ x ∆ y ∆ z ∆ ct  b                1  2  r r  − −       + +  ct b cct  b       N  r  −     +  ct  b     1  d 2 d  cid:1  N d               112     ROBOT NAVIGATION  or  −  x x     1     2  x       −− x  cid:1   1 1 r 1 2 r  −  y y     1     2        −  y y  cid:1   ∆ x ∆ y ∆ z ∆ ct  b                    = −            r r  1 1 r 1 2 r  1 N r − 1 −  2  N     ct b ct  b               −  x x + +  1  d 2 d  cid:1  N d  +  ct  b     1 N r               N  r  −     −     1 z z     2        −  z z  cid:1   1 1 r 1 2 r  1 N r  −1  − 1  − 1  cid:1              −  y y     N     −  z z     N     − 1  ‘‘                4.20    In practice, a scale factor less than one is often introduced to aid in  the convergence of the solution. For cases where the number of visible  satellites  is  greater  than  four,  there  will  be  more  equations  than  unknowns. To  proceed,  one  may  utilize  the  least - squares  solution  in  which case the matrix inverse becomes a generalized inverse.  ∆ x ∆ y ∆ z ∆ ct  b                    { [ = ∂  E X E ct        b  ∂  ∂  ∂  ] ∂ [  T  E X E ct        b  ∂  ∂  ∂  } ]  − 1  [  ∂  ∂  E      X E ct     b  ∂  ∂  ]  T         1  2  r r  − −       + +  ct b ct  b       N  r  −     +  ct  b     1  d 2 d  cid:2  N d                       4.21    Fortunately,  these  equations  are  all  consistent,  which  means  that  there exists an exact solution  in the absence of noise  even though the  number of equations exceeds the number of unknowns. After solving  equation   4.21  , one updates the receiver location and the local clock  bias correction according to             x y z ct  b         =  +         x y z ct  b                ∆ x ∆ y ∆ z ∆ ct  b                4.22                COMPUTING RECEIVER LOCATION USING GPS, NUMERICAL METHODS     113   This correction process is repeated until it reaches steady state, i.e.,  the  correction  approaches  zero. At  this  time  the  distances  computed  from the equations involving receiver location should agree with the  pseudo distances based on the measurements of signal travel time with  local clock bias correction.      EXAMPLE 6     At a given point signals are received from different four satellites. At the  time the signals left the satellites their positions expressed in ECEF coor- dinates were as follows:                                                   .  .  .  31864 37     x1 7766188 44 ;   21960535 34    y1 . ;      z1 12522838 56 ;   25922679 66    x2 . ;      y2 ;   6629461 28    z2    x3    y3 ;      z3 1692757 72    x4 2786005 69 . ;      y4 . ;   15900725 8    z4 ;     = = − = = − = − = = − = − = = − = − =  ;   5743774 02 ;   . 25828319 92  21302003 49  ;   .  .  .  .  .     d1 1022228206 42    d2 1024096139 11    d3 1021729070 63    d4 1021259581 09  = = = =  .  .  .                 Based  on  the  times  the  signals  were  received  and  the  information  regarding when they were transmitted from the respective satellites, their  pseudo distances from the receiver are computed to be     What is the receiver location?      114     ROBOT NAVIGATION    SOLUTION 6     A program was developed for solving this problem on the computer. The  initial guess for the receiver location is the origin of the ECEF coordinate  system and the initial guess for local clock error is zero.      N  =  10;   C  =  3  *  10 ^ 8;      d1  =  1022228206.42; x1  =  7766188.44;  y1  =   - 21960535.34; z1  =  12522838.56;   X1  =  [x1 y1 z1] ′ ;      d2  =  1024096139.11; x2  =   - 25922679.66;  y2  =   - 6629461.28; z2  =  31864.37;   X2  =  [x2 y2 z2] ′ ;      d3  =  1021729070.63; x3  =   - 5743774.02;  y3  =   - 25828319.92; z3  =  1692757.72;   X3  =  [x3      y3 z3] ′ ;      d4  =  1021259581.09; x4  =   - 2786005.69;  y4  =   - 15900725.8; z4  =  21302003.49;   X4  =  [x4 y4 z4] ′ ;      x  =  0; y  =  0; z  =  0;   X  =  [x y z] ′ ;      ctb  =  0; delX  =  0; delctb  =  0;      for j  =  1:N                              X  =  X  +  delX;               ctb  =  ctb  +  delctb;               r1  =    X  -  X1  ′   *   X  -  X1   ^ .5;               r2  =    X  -  X2  ′   *   X  -  X2   ^ .5;               r3  =    X  -  X3  ′   *   X  -  X3   ^ .5;               r4  =    X  -  X4  ′   *   X  -  X4   ^ .5;                  error  =   - [r1  -   d1  +  ctb ; r2  -   d2  +  ctb ;                             r3  -   d3  +  ctb ; r4  -   d4  +  ctb ];      115 COMPUTING RECEIVER LOCATION USING GPS, NUMERICAL METHODS      %These are the errors between distances computed  based on respective satellite positions and    %current estimated receiver position versus  distances computed based on the times the signals  were    %received from the respective satellites                   L j   =   r1  -   d1  +  ctb   ^ 2  +   r2  -   d2  +  ctb   ^ 2                                 +   r3  -   d3  +  ctb   ^ 2  +   r4  -   d4  +  ctb   ^ 2;    %positive deﬁ nite measure of error                              grad1  =  [ -  1   r1   *   X1  -  X  ′   - 1];               grad2  =  [ -  1   r2   *   X2  -  X  ′   - 1];               grad3  =  [ -  1   r3   *   X3  -  X  ′   - 1];               grad4  =  [ -  1   r4   *   X4  -  X  ′   - 1];               Grad  =  [grad1; grad2; grad3; grad4];               Gradinv  =   inv Grad ′   *  Grad    *  Grad ′ ;      % A generalized inverse is used in order to be  able to handle cases where more than 4 satellites  are visible.                  delX  =  K  *  [1 0 0 0; 0 1 0 0; 0 0 1 0]  *                delctb  =  K  *  [0 0 0 1]  *  Gradinv  *  error;       %This is the correction desired at each step,  K      is often set to be less than one, maybe 0.5 or  so,      to    %improve convergence and reduce the probability  of overshoot      in      the solution process.      end   ﬁ gure   plot L     Gradinv  *  error;     The output of the program was the receiver location which is            , ,     x = −2 430 745    y = −4 702 345    z = 3 546 569    , ,  ,  ,         116     ROBOT NAVIGATION     x coordinate y coordinate z coordinate  x 106  6  4  2  0  -2  -4  -6    1  1.5  2  2.5  3  3.5  4  4.5  5  5.5  6  Number of Iterations       Figure 4.3a       Convergence of the Coordinates of the Receiver Position       and the local clock error         tb = −3 3342156  .        The extremely rapid convergence of x, y, z and total error can be seen  from Figure    4.3a    and    4.3b    as well as from Table    4.1  .  It is seen that after  5  iterations,  the  solution  has  been  reached  within  half  a  meter  in  each  coordinate. The value used for K was 1.0           If the receiver is mounted on a moving vehicle, its position may be  tracked  by  successive  GPS  measurements.  Once  its  ﬁ rst  location  has  been  determined,  its  next  location  can  be  determined  quite  rapidly  since the previous location serves as a good initial guess for the iterative  process. Probably only one iteration would be required.      Computing Receiver Location Using  GPS  via Minimization      4.6.2   of a Performance Index   An alternative approach to determining the coordinates of the receiver  is to formulate a positive deﬁ nite performance index based on the sum    COMPUTING RECEIVER LOCATION USING GPS, NUMERICAL METHODS     117  x 1017  14  12  10  8  6  4  2  0  1  1.5  2  2.5 3.5 Number of Iterations  3  4  4.5  5       Figure 4.3b       Convergence of the Norm of GPS Error      TABLE 4.1      Convergence of coordinates as a function of iteration number      Iteration       1       2       3       4       5       50       Performance  Index       x       y       z       Ctb       3.9658   e + 018      0       0       0       0       1.3970   e + 012      −  2,977,571       −  5,635,278       4,304,234       −  3.338750       2.1109   e + 009      2,451,728       −  4,730,878       3,573,997       −  3.3343802       3.1469   e + 003      −  2,430,772       −  4,702,376       3,546,603       −  3.334215       7.3558   e - 009      −  2,430,745       −  4,702,345       3,546,569       −  3.334215        1.3878     e - 015       −  2.430,745       −  4,702,345       3,546,569       −  3.3342156     of squares of the distance errors described in equation   4.16  . The per- formance measure used to represent total error in geo - location will be  taken as     =     L  N  ∑{[   =  i  1  −  x x  i  2     +  −  y y  i     2     +  −  z z  i     . 2 0 5   ]  −  i  [  d  +  ct  b  2  ]}         4.23     where,  as  was  the  case  before,  the  coordinates  of  the  i th   satellite  at  i T  and  the  ] time  of  transmission  are  given  by  the  coordinates    [  i x y z  ,  ,  i   118     ROBOT NAVIGATION  coordinates of the receiver are initially estimated to be   [ , the number of visible satellites. Utilizing   X the above can be written in shorthand as  i x y z  = [  ,  ,  i  i  x y z T.   N is  i T ]  and   X x y z T , ]    , ] = [ ,  =     L  N  ∑{[   =  i  1  −  X X  i T      X X  i   ]  .0 5  −  −  i  [  d  +  ct  b  2  ]}     =     L  N  ∑{  i  r  =  i  1  −  i  [  d  +  ct  b  ]}2         4.24       It is seen that the individual error terms are squared before being  added  together.  Otherwise  negative  and  positive  errors  could  cancel  each  other,  leading  to  a  small  value  of   L   even  though  there  were  signiﬁ cant  errors.  Note  that  this  is  the  same  expression  used  in  the  Matlab  program  of  the  previous  example  to  monitor  the  progress  of  the  Newton  iterative  method.  Here  this  performance  index  will  be  used to actually deﬁ ne the iterative procedure. The goal is to determine  the coordinates of the receiver which cause this performance index to  be minimized. Starting with an initial guess, one successively perturbs  the coordinates of the receiver and the local clock error to move the  solution  toward  this  minimum. As  part  of  the  numerical  search  pro- cedure, the gradient of the performance index must be determined in  order  to  know  in  what  direction  to  perturb  the  coordinates  of  the  receiver.  Differentiating  the  above  expression  of  the  performance  index, one obtains  ∂      ∂  L X     =  {[  2  X X  i T      X X  i   ]  . 0 5  −  −  −  i     d  +  ct  b   }  −  X X  i T      X X  i  −  −− 0 5 2.  ]    −  X X i T        N  ∑ = i 1 2  1  [    Using the deﬁ nition for   ri the above derivative simpliﬁ es to     ∂  ∂  L X     =  2  i  { r  −  i     d  +  ct  b   }   X X  i T      r  i       −    4.25      N  ∑  =  i  1   This vector points in the direction of increasing  L.  The goal here is   to minimize  L , so one is interested in the direction of decrease, i.e.,        or         COMPUTING RECEIVER LOCATION USING GPS, NUMERICAL METHODS     119     −∂  ∂  L X     =  2  i  { r  −  i     d  +  ct  b   }   i  T     X X r  i      −  N  ∑  =  i  1  −  i        ∂     X X r   Note that the portion      i  is a unit vector pointing from the  current  estimate  of  the  receiver  location  to  the  location  of  the  ith  satellite.  The  weighting  for  this  i th   component  of    − ∂   L X   is  the  error  in  distances  as  calculated  from  the  current  estimate  of  the  receiver  position  and  the  i th   satellite  position  and  the  time - corrected  pseudo - distance from the receiver to the i th  satellite. If the distance as  calculated from the current estimate of the receiver position and the  i th  satellite position is greater than the time - corrected pseudo distance  from the receiver to the i th  satellite, then this component of   − ∂     L X   would indicate that one should perturb the estimated receiver location  toward  the  i th   satellite.  For  the  opposite  situation,  the  perturbation  would be the move the estimated receiver location away from the i th   satellite There  will  be   N   such  vectors  added  together,  each  weighted  in proportion to the respective errors and each moving toward or away  from  the  respective  satellites.  The  perturbations  will  be  conﬁ ned  to  the space spanned by the vectors from the receiver to the visible satel- lites. A  set  of  satellites  having  a  low  value  for  GDOP,  i.e.,  having  a  good  geometric  distribution,  will  result  in  a  more  robust  iterative  process. If the visible satellites were all in the same plane, corrections  in  receiver  position  using  this  iterative  process  would  be  limited  to  this plane.   ∂      One also needs to adjust the local clock bias. The result of this dif-  ferentiation becomes     ∂  ∂  L ct     b  =  i  { 2 r  −  i     d  +  ct  b   }   −   1         4.26     N  ∑  =  i  1  and  again  the  negative  value  is  of  interest  since  the  goal  is  to  mini- mize  L ,     −∂  ∂  L ct     b  =  i  { 2 r  −  i     d  +  ct  b   }      N  ∑  =  i  1            This direction indicated by the objective of decreasing  L  is seen to  be that of an increase in  ctb  if the computed range is greater than the  corrected pseudo range. One uses these derivative functions as a means    120     ROBOT NAVIGATION  T .  of determining the perturbation of the 4 dimensional vector   [ , ] One must also know what step size to use for the perturbation. Often  times the function to be minimized is very complex and a step size too  large may overshoot the solution or a step size too small may take very  long  to  converge.  Also,  one  step  size  may  not  apply  everywhere.  A  means of determining not only the direction but also the step size for  the iterative process is to utilize a more complete Taylor series includ- ing the second derivative of the function to be minimized, i.e.,  x y z ctb  ,  ,     f w     +  ∆   w  ≈      f w  + ∂  f w w     ∆  ∂  +  ∆ T w  ∂ ∂ [    ∂  f w     T      ∂  ∆ ]  w w      1 2   Then minimizing  with respect to  Δ w yields     f w     + ∆    w        ∆w  = − ∂ ∂ w     ∂  {   ∂  f w     T   }  ∂  ∂  f w     T         − 1         4.27       This may be interpreted as using the Newton method to determine  the point where the ﬁ rst derivative is zero. Performing these operations  on  the  performance  index  of  interest,  one  obtains  as  the  matrix  of  second derivatives  ∂  {   ∂  L X     T   }  =  2  −     i  r  +  ct  b     i  d r  i  +  I     b     i  + ct d 3 i     r      X X X X      i  i T    −  −             i  1  =  N  N  ∑ ∑ ∑  N  =  1  i  =  i  1  1 i r  1 i r  ∂  ∂  L ct           b  = − 2  −     X X  i T       ∂     ∂  L X     T    = − 2  −     X X  i                   ∂     ∂ X ∂     ∂ X ∂ ct      ∂  b  and        ∂  ∂  2  L ct     2 b  =  2  N       The changes in  X  and  ctb  then become   COMPUTING RECEIVER LOCATION USING GPS, NUMERICAL METHODS     121            ∆ X ∆ ctb    = −         ∂ ∂ X ∂ ∂ X  ∂     ∂  L X     T    ∂     ∂  L X     T    ∂  ∂  L ct           b  ∂     L      ∂  ct  b     b  ∂ ct ∂ ct  b  ∂  ∂  − 1              ∂ T   L X ∂ L ctb  ∂ ∂                 4.28       Solve  the  previous  geo - location  problem,  this  time  via  minimizing  a  performance index.       EXAMPLE 7     SOLUTION 7     Below is shown the computer code for using the above described mini- mization procedure to determine the receiver coordinates. The data on  satellite positions and distances are the same as before and are omitted  here to save space.                                                                                                   N  =  50;   delX  =  0; delctb  =  0;    ctb  =  0;   x  =  0;    y  =  0;    z  =  4000000;   X  =  [x y z] ′ ;      for j  =  1:N                     X  =  X  +  delX;                  ctb  =  ctb  +  delctb;                  r1  =    X  -  X1  ′   *   X  -  X1   ^ .5;                  r2  =    X  -  X2  ′   *   X  -  X2   ^ .5;                  r3  =    X  -  X3  ′   *   X  -  X3   ^ .5;                  r4  =    X  -  X4  ′   *   X  -  X4   ^ .5;                     L j   =   r1  -  d1  +  ctb   ^ 2  +   r2  -   d2  +     ctb   ^ 2  +   r3  -  d3  +  ctb   ^ 2  +   r4   -   d4  +  ctb   ^ 2;                    delLx  =   2   r1   *   r1  -   d1  +  ctb    *   X     -  X1   +   2   r2   *   r2  -   d2  +     ctb    *   X  -  X2                                            +   2   r3   *   r3  -   d3  +  ctb    *   X  -  X3    +   2   r4   *   r4  -   d4  +  ctb    *   X  -  X4 ;                 delLctb  =   - 2  *    r1  -   d1  +  ctb    +   r2  -   d2     +  ctb    +   r3  -   d3  +  ctb    +   r4   -   d4  +  ctb   ;        ROBOT NAVIGATION  122               delLxdelLx  =  2  *    r1  -   d1  +  ctb     r1   +   r2  -   d2  +  ctb     r2  +   r3       -   d3  +  ctb     r3    +   r4  -   d4  +  ctb     r4   *  eye 3 ;                                                            delLxdelLx  =  delLxdelLx  +  2  *     d1  +  ctb    r1 ^ 3      *   X  -  X1   *   X  -  X1  ′        +    d2  +  ctb    r2 ^ 3   *   X  -  X2    *   X  -  X2  ′          +    d3  +  ctb    r3 ^ 3   *   X  -  X3      *   X  -  X3  ′        +    d4  +  ctb    r4 ^ 3   *   X  -  X4      *   X  -  X4  ′  ;               delLctbdelLx  =   -  2   r1   *   X  -  X1   -   2   r2   *    X  -  X2   -   2   r3   *   X  -  X3   -   2   r4        *   X  -  X4 ;               delLxdelLctb  =   -  2   r1   *   X  -  X1  ′   -   2   r2    *   X  -  X2  ′   -   2   r3   *   X  -  X3  ′        -   2   r4   *   X  -  X4  ′ ;               delLctbdelLctb  =  2  *  4;                              D2  =  [delLxdelLx delLctbdelLx; delLxdelLctb                 delX  =   - [1 0 0 0; 0 1 0 0; 0 0 1 0]  *                  delctb  =   - [0 0 0 1]  *  inv D2   *  [delLx; delLctb];               xx j   =  [1 0 0]  *  X;               yy j   =  [0 1 0]  *  X;               zz j   =  [0 0 1]  *  X;               ctbb j   =  ctb;   end      ﬁ gure   plot L    axis [1 10 0 5 * 10 ^ 18]       ﬁ gure   plot xx ; hold on; plot yy ; hold on; plot zz    axis [1 10  - 1.5 * 10 ^ 7 1 * 10 ^ 7]     inv D2   *  [delLx; delLctb];   delLctbdelLctb];     From  the  code  it  is  seen  that  the  procedure  was  started  with  initial  guess of x     =     0, y     =     0 and z     =     4,000,000. It had been found that the itera- tive process converged to a wrong solution unless one placed the initial    ARRAY OF GPS ANTENNAS     123     x coordinate y coordinate z coordinate  x 107  1  0.5  0  -0.5  -1  -1.5    1  2  3  4  5  6  7  8  9  10  Number of Iterations       Figure 4.4a       Convergence of the Coordinates of the Receiver Position    value of z in the correct hemisphere. The rapid convergence of x, y, z and  total error can be seen from Figure    4.4a    and    4.4b    as well as from Table     4.2  .  It is seen that after 10 iterations the solution has been reached within  half a meter in each coordinate. This was not as fast as the convergence  of the ﬁ rst method, but nevertheless quite fast.           Either of the two methods presented would be sufﬁ cient for initial- ization, and either method would probably converge in one step while  tracking a moving vehicle using the previous location as the ﬁ rst guess  for the new location. The ﬁ rst method is considerably simpler in terms  of  analytical  and  computational  complexity  as  well  as  having  faster  convergence.        4.7      ARRAY OF  GPS  ANTENNAS    An array of four GPS antennas such as is shown in Figure  4.5  may be  used to compute vehicle attitude. These are attached to a frame in the  shape of a cross with antennas labeled 1  front , 2  left side , 3  rear   and 4  right side . The  x  axis runs across to the right side of the frame.    124     ROBOT NAVIGATION  x 1018  4.5  3.5  2.5  1.5  5  4  3  2  1  0.5  0  1  2  3  4  5  6  7  8  9  10  Number of Iterations       Figure 4.4b       Convergence of the Norm of GPS Error      TABLE 4.2      Convergence of coordinates as a function of iteration number      Iteration       1       3       6       8       10       50       Performance  Index       x       y       z       ctb       3.9745   e + 018      0       0       4e + 6       0       7.271354   e + 013      −  8,150,229       −  1,381,163       5,136,466       −  3.338750       1.5179206   e + 011      −  3,247,551       −  5,823,740        4,015,697       −  3.3372523       5.2192730   e + 006      −  2,435,857       −  4,708,767       3,550,007       −  3.334235       7.0935216   e - 011      −  2.430745       −  4,702,345       3,546,568       −  3.334215        1.3877787     e - 015       −  2,430,745.1       −  4,702,345.1       3,546,568.7       −  3.33421563     The  y  axis runs from the rear to the front of the frame. The  z  axis com- pletes the right - handed set.      Before any rotation of the vehicle, i.e., zero yaw, pitch and roll, the   coordinates are              x 1     x  2    x  3  ,  veh W  = x = − =  x veh  ,     y L 1 2  = + x veh = − L  y 3  ,     +  2  y 2 2     ,  z 1 ,  y veh = y veh + ,  y veh  =  z 2  z 3       z veh = z veh =      z veh         4.29a       4.29b       4.29c      ARRAY OF GPS ANTENNAS     125  2   X   4  Y   1   3        Figure 4.5   Four GPS Antennas       Convergence of Coordinates as a Function of Iteration Number Array of   and                                   and    =     x W  4     2  +  x veh  ,  y 4  =  y veh  ,  z 4  =  z veh         4.29d       Now after rotation of the vehicle through a yaw angle of   ψ, a pitch  angle of   θ, and a roll angle of   φ in that order, the coordinates become  1  1     x  L     x 1  2    y 2     y L  = − = =    z L = − = − = =    x L = − = −  3    y 3    z 3  zveh cos  2    cos    sin  θ 2 2    cos W  + ψ θ    sin cos   2 ψ θ + cos   +     − ψ φ + ψ φ    sin cos 2 + θ φ    cos sin   2 + ψ θ    sin cos   2 ψ θ cos   +  2    cos    sin  θ 2     z W  zveh  W  L  L       2       xveh     yveh            zveh xveh +  yveh       ψ θ φ sin sin sin   ψ θ φ sin sin    cos  + +  xveh yveh            =     x W  4  2    cos  ψ φ −  cos  =     y W  4  2    sin cos  ψ φ +  ψ θ φ sin sin sin   ψ θ φ sin sin    cos  +  +  xveh  yveh              4.30a       4.30b       4.30c       4.31a       4.31b       4.31c       4.32a       4.32b       4.32c       4.33a       4.33b                  126     ROBOT NAVIGATION  = −  W  θ φ    cos sin   2  +     z 4  zveh         4.33c       By manipulating these equations it is possible to isolate the attitude   angles in terms of the measured variables. For pitch we have     θ=  − 1  tan      −   z z 3 1 + 2     y 1  3    x  −     x 1  −  2  y 3               Yaw is given by   For roll     ψ=  − 1  tan     − −   x x 1 −   y y 1 3    3              4.34        4.35         φ =  − 1  sin     −    z z  = 2 4 θ cos W  − 1  sin        z 2   x  3  − z W 4 + −        y 1  2  −     x 1  y 3  2      L            4.36       Notice  that  in  all  these  expressions  the  coordinates  of  the  vehicle  location subtract off and do not affect the expressions for vehicle atti- tude. The major errors in the measurements at antenna locations will  be common to all four antennas. Thus, even though absolute positioning  of  the  antennas  may  not  be  possible,  the  computed  attitude  may  be  extremely  accurate.  As  an  example,  one  GPS  array  unit  advertises  accuracy in position of only 40   cm but accuracy in attitude of less than  six - tenths of a degree.       4.8      GIMBALED INERTIAL NAVIGATION SYSTEMS    The  two  distinct  implementation  approaches  for  Inertial  Navigation  Systems  INS  will be discussed. These are gimbaled and strap - down  gyro systems. For the gimbaled system there is an actuated platform on  which  are  mounted  three  orthogonal  gyros.  A  gimbaled  platform  is  shown in Figure  4.6 .      Here  the  gyros  on  the  platform  in  conjunction  with  the  gimbal  motors  maintain  the  platform  at  a  ﬁ xed  attitude  in  an  inertial  frame  even though the mounting frame, which is attached to the vehicle of  interest, may rotate. Thus the platform is called a stable platform. In    GIMBALED INERTIAL NAVIGATION SYSTEMS     127  Accelerometers  Azimuth Motor  Gyros  Pitch Motor  Roll Motor  Mounting Frame       Figure 4.6       Schematic Diagram of a Gimbaled Platform    studying the behavior of the gyros, we use one of the equations that  governs the behavior of a rotating body, i.e.,        τ= ΩxL        4.37       Here   L is the angular momentum of the gyro and it points along the  spin axis. The vector   Ω is the angular precession velocity of this momen- tum  vector. The  associated  torque  is  represented  by  the  vector    τ. As  the attitude of the vehicle changes, a small angular velocity,   Ω of the  platform and thus an equal angular rotation of the gyro angular momen- tum  vector  may  occur  as  a  result  of  slight  friction  in  the  imperfect  gimbal bearings. The torque,   τ resulting from this rotation according to  equation   4.37   is sensed at the gyro bearing supports. Through feed- back control the gimbal motors react to negate this torque maintaining  the  platform  at  the  ﬁ xed  attitude. The  torques  will  be  sensed  by  the  gyros  in  platform  coordinates;  however,  the  gimbal  motors  will  be  aligned  according  to  the  attitude  of  the  vehicle.  Thus  a  coordinate  transformation is necessary to determine the required gimbal rotations.  Through this arrangement the platform is maintained at a ﬁ xed attitude  even though the vehicle may rotate. Shaft encoders or similar sensors  are  used  to  measure  the  gimbal  angles  and  thus  provide  the  current  attitude  of  the  vehicle  with  respect  to  the  stable  platform.  From  the  ﬁ gure it can be seen that there is a gimbal and a gimbal motor each for  yaw  or azimuth , pitch and roll.    128     ROBOT NAVIGATION   In addition to the gyros, three accelerometers are also mounted on  the stable platform. They read the three components of acceleration in  inertial coordinates. By integrating these signals twice with respect to  time, one obtains change in position in all three coordinates. Combining  this information with the original position gives one the current posi- tion of the vehicle in inertial space. If there is any offset or bias in the  accelerometers, the process of double integration creates an error that  grows as the square of time, eventually becoming unacceptable. Some  auxiliary  means  of  determining  position  is  required  for  periodic  re -  calibration of the inertial unit. The time between re - calibration is dic- tated by the accuracy requirements and the extent of the accelerometer  bias.  Biases  may  range  from  a  few  hundredths  of  a  milli - g  to  a  few  milli - g ’ s.    In  addition  to  these  position  errors  caused  by  accelerometer  bias,  there  are  attitude  errors  caused  by  drift,  i.e.,  the  system  thinks  it  is  rotating when it is not. The drift rates may be as high as a few degrees  per hour and as low as a few milli - degrees per hour.    Errors  in  attitude  do  not  grow  as  fast  as  errors  in  position  since  attitude estimates do not suffer from the double integration of biases  as the position estimates do.      EXAMPLE 8   =  + +        2 ,  where  acc    An  accelerometer  yields     acc indicated represents the true acceleration, n represents random errors of zero mean  − mg . Compute posi- and b represents a bias. Let    b tion from this signal. If the maximum position error one can tolerate is  1     m, at what time intervals must the computations be corrected with an  independent, correct position measurement?       or    1 02 10 3  acc n bm  2  m s  −10 5    sec  =  *  .    SOLUTION 8     The integration of the accelerometer output yields as     vel t    , the indicated velocity,      b  2t 2               vel t  + +  acc n b dt v            0            vel t     acc dt v      0  +  +  +  bdt  ndt     + ∫  t  0  t  ∫  0  =  =  t  ∫ ∫  0 t  0  t    GIMBALED INERTIAL NAVIGATION SYSTEMS     129         vel t  =      v t  +  + 0     bt    where    v t      represents the true velocity and    bt   represents the error caused  by the bias. The error causes by the random noise term integrates to zero  since it has zero mean.          Position t     vel t dt P            0    +         Position t  =        v t dt  +  bt dt P      0    +     Position t      =      P t  +  b    Now setting the error term to its maximum allowable value   =  t  ∫  0  t  ∫  0  t  ∫  0      2  t 2     b  2  t 2  1=  m     2     t  2= b        t  = 2 b     =     t  2 10    5     447 213  .  sec  =  =  7 45 .  min         Next  the  question  of  obtaining  vehicle  attitude  and  position  from  discrete - time outputs of the gimbaled gyroscope with stable platform  is  addressed. When  using  the  gimbaled  gyroscope,  the  attitude  equa- tions are simply:   for yaw     ψ    t  =1    ψ  +  k  measured     t  +  1  k         or    yields    or                                                       130     ROBOT NAVIGATION  for pitch  and for roll     θ    t  =1    θ measured  +  k     t  +  1  k           φ    t  =1    φ measured  +  k     t  +  1  k          In other words, the measurements of the gimbal angles are the same  as the attitude measurements of the vehicle with respect to the stable  platform.    For position the equations are:    for  x       cid:1    x t  +  1  k     =   cid:1      x t k  +  a t x       k measured     t  +  1  k  −  t  k             x t    +  1  k     =      x t k  +   cid:3    x t  +  1  k      t  +  1  k  −  +  t  k     a t x       k measured    4.38a            4.38b        t  +  1  k  t  k  2     − 2  +  1  k     =   cid:1      y t k  +  a t y       k measured     t  +  1  k  −  t  k            4.38c        y t    +  1  k     =      y t k  +   cid:1       y t  k  t  +  1  k  −  t  k     +  a t y       k measured     t  +  1  k  t  k  2            4.38d     for  y      cid:1    y t     and for  z       cid:1    z t  +  1  k     =   cid:1      z t k  +  a t z       k measured     t  +  1  k  −  t  k            4.38e        z t    +  1  k     =      z t k  +   cid:1       z t  k  t  +  1  k  −  t  k     +  a t z       k measured     t  +  1  k  t  k  2            4.38f       Attitude  and  position  are  expressed  in  inertial  space,  i.e.,  a  non -  rotating, non - moving coordinate system. Here the assumption of con- stant acceleration over the sampling interval has been assumed, i.e.,         a t  =      a t  k measured  for  ≤ < +  t  t  k  1     t  k   A  combination  of    a tk measured    could  be  used  for  better accuracy if one is willing to wait for the measurement at time    tk+1.      and    a tk  measured        +1     − 2  − 2   STRAP-DOWN INERTIAL NAVIGATION SYSTEMS     131     4.9      STRAP - DOWN INERTIAL NAVIGATION SYSTEMS    In addition to gimbaled inertial navigation systems, there are also strap -  down systems. Here the spin axes for the gyros are rigidly attached to  the  vehicle,  which  means  that  they  change  in  attitude  as  the  vehicle  changes attitude. Since the sensors experience the full dynamic motion  of the vehicle, higher bandwidth rate gyros with higher dynamic range  are required. The equation relating precession rate to torque   τ= ΩxL  can be broken down into components  τ  x  τ      y  τ  z        =        0 − L z L y  L z 0 L x  −  − L y L x 0        Ω Ω Ω        x  y  z              There will be an equation of this type for each of the three orthogo-  nal gyros.    For the gyro aligned with the  x  axis of the platform, only   Lx is non   zero, and   τx is zero leading to   For the gyro aligned with the  y  axis of the platform, only   Ly is non   zero, and   τy is zero leading to  τ  y      τ z     − gyro x  =     −  0 L x  L x 0        y  Ω   Ω      z  τ  x      τ z     − gyro y  =     0 L y  −  L y 0        x  Ω   Ω      z  τ  x      τ y     − gyro z  =     −  0 L z  L z 0        x  Ω   Ω      y   For the gyro aligned with the  z  axis of the platform, only   Lz is non   zero, and   τz is zero leading to   These over - determined equations  six equations and three unknowns   involve the angular momentum of each gyro  known quantities , and  the  six  torques,  which  are  measured  at  the  individual  gyro  bearing  supports.    4.39        4.40a        4.40b        4.40c                   132     ROBOT NAVIGATION  τ  − y gyro x  τ  − z gyro x τ  − x gyro y      τ  − z gyro y  τ −  x gyro z τ  − y gyro z            =            0 0 0 L y 0 L z  −  −  0 L x 0 0 L z 0  −  L x 0 L y 0 0 0                  xΩ Ωyy zΩ               4.41       The  solutions,    Ωx,    Ωy,  and    Ωz  which  are  the  instantaneous  angular  body rates: pitch rate, roll rate and yaw rate may be obtained via the  least - squares method as     Ωx  =     Ωy  =  1 +  1 +  L L z  2 y  2    L L z  2 x  2    L  τ y z gyro y  −−  ,  τ − z y gyro z  ,  L          −  τ x z gyro x  +−  ,  L  τ − z x gyro z  ,  L            4.42a       4.42b        Ωz  =  1 +  L L y  2 x  2    L  τ x y gyro x  −−  ,  τ − y x gyro y  ,  L            4.42c       Although this is a least squares solution, the equations are consistent  and there is no error in the solution for the body rates. These rates are  related to the attitude rates according to the following equations:  +  sin cos   cid:1   cid:1     Ωx = θ φ ψ φ θ     cos    Ωy = − cid:1   cid:1 φ ψ θsin      cid:1     Ωz = ψ φ θ θ φ sin     cos cos  −   cid:1    In matrix form these become  Ω Ω Ω            x  y  z        =  φ        cos 0 sin  −  φ  0 1 0  sin cos θ cos cos  φ θ   sin   φ θ   −   cid:1  θ      cid:1  φ      cid:1  ψ           4.43a       4.43b       4.43c        4.44     which may be inverted to yield the angular rates for yaw, pitch and roll.  and                           STRAP-DOWN INERTIAL NAVIGATION SYSTEMS     133   cid:1  θ      cid:1  φ          cid:1  ψ    =        φ cos φ θ sin tan φ θ sin   cos  0 1 0  −  φ cos tan cos   sin  φ θ   φ    cosθ   Ω Ω Ω        x  y  z               4.45       Once  these  angular  rates  for  attitude  have  been  determined,  the  attitude  angles  themselves  can  be  updated  via  numerical  integration.  Using  the  Euler  integration  method,  i.e.,  derivative  approximated  by  forward difference, yields     t     ψ      θ    t  + +  ∆ t ∆ t      cid:1  ψ ψ ∆ + = t t t         ∆ cid:1  θ + = θ         t t t           φ    t  +  ∆ t  =  φ t      +     ∆ cid:1  φ t t           Another approach to the problem of obtaining vehicle orientation  from body angular rates involves the use of quaternions. The quater- nion vector is deﬁ ned by  =     q         q 0 q 1 q 2 q 3                4.46     where  the  components  of  the  quaternion  are  related  to  the  vehicle  attitude by     q0    q1    q2  = = =  cos   cos   cos   ψ ψ ψ  2  + 2 2    cos     cos      − +  2 2    cos     sin       2 2    sin     cos       φ φ φ  θ θ θ  2  2  sin   sin   sin   2  ψ ψ ψ  φ     2 2    sin     sin      φ        sin     cos      2 2 φ     2 2    cos     sin       θ θ θ  2  2    4.47a       4.47b       4.47c     and  equation     q3  =  ψ  sin   2 2    cos     cos       2  θ  φ  −  cos   ψ  φ        sin     sin      2 2  θ  2    4.47d       It  can  be  shown  that  the  quaternion  vector  obeys  the  differential   =             cid:1 q A t q    4.48     and                     134     ROBOT NAVIGATION  where      =     A t         0 Ω Ω Ω  z  y  x  1 2  −  Ω 0 Ω x − Ω  z  y  y  x  − −  Ω Ω 0 Ω  z  x  z  Ω − Ω y − Ω 0                4.49       The  entries  in  the  matrix    A t   ,    Ωx,    Ωy,  and    Ωz  are  seen  to  be  the  computed body angular rates. The rates,   Ωx and   Ωy, are interchanged  here as compared to where they would appear with the frame deﬁ nition  used by those in the aerospace ﬁ eld. This is because pitch is about the   x  axis here versus about the  y  axis there and vice versa for roll.    This differential equation for   q can be integrated numerically to yield  its updated values. If one uses the approximation that the matrix   A t     T+ 1   with  value  is  constant  during  the  time  interval  from    kT  to        A kT    , then the solution would be  k        q k     +      1 T  =  e  A kT T          q kT          If  one  were  to  integrate  the  yaw,  pitch  and  roll  equations  directly,  the expressions for their derivatives in terms of body rates are more  complex  as  was  illustrated  by  equation    4.45  .  After  integrating  the  quaternions  the  current  values  for  yaw,  pitch  and  roll  angles  can  be  determined from the entries of   q via     ψ=     θ= −  − tan 1    − 1 sin [  q q 2 1 2 − q 1 2 2 q q 2 1 3  + 2  2 −  −  2        q q 0 3  q 2 2 3 q q     ] 0 2     φ=  − 1  tan     q q 2 2 3 − q 1 2 1  + 2  2 −  q q 0 1 q 2 2 2     .         4.50a       4.50b       4.50c     where use has been made of the fact that     q 2 0  +  q 2 1  +  q 2 2  +  q 2 3  =    1   These new values can then be used for updating the rotation matrix.  The  quaternion  representation  has  a  number  of  desirable  properties.  One interesting feature is that the   A matrix is skew symmetric, i.e.,                    and   STRAP-DOWN INERTIAL NAVIGATION SYSTEMS     135     A  = −  AT  .      As a result of this the square of the norm of the quaternion is seen   to be constant  or                 T q q    =   cid:1   cid:1    T q q q q  +  T  d     dt     T q q    =  T q A q q Aq  T  T     +     T   q q  T   q A A q     T  =  +  = 0     d     dt  d     dt  which is seen to be   This  result  is  consistent  with  the  earlier  mentioned  fact  that  the  square  of  the  Euclidean  norm  is  ﬁ xed  at  unity. This  property  can  be  used as a means to check and correct some of the numerical errors that  accompany the numerical integration.    Another beneﬁ t from using the quaternions is that the singularity at  pitch of 90 degrees is avoided. Consider a rotation which consists of a  yaw  of  amount    ψ,  followed  by  a  pitch  of    π  2,  followed  by  a  roll  of  amount    φ.  This  same  ﬁ nal  attitude  could  be  attained  by  an  inﬁ nite  number of other combinations of yaw and roll as long as the sum of  yaw  plus  roll  remains  the  same. Thus  it  is  impossible  to  determine  a  unique set of rotations corresponding to the ﬁ nal attitude. This situation  is referred to as a singularity. This may also be detected from equation    4.36   by noting that there is division by   cosθ whose result is undeﬁ ned  for    θ π= ±   2.  The  four  components  of  the  quaternion  provide  the  minimum  dimension  necessary  to  avoid  the  singularity  and  yield  a  unique attitude even when pitch is 90 degrees.    Regarding position calculation, the fact that the platform is rigidly  attached to the vehicle means that the accelerometers measure accel- eration  in  vehicle  coordinates.  These  must  be  converted  to  inertial  coordinates in order to be useful for determining the vehicle position.  Thus  one  uses  the  rotation  matrices  to  perform  the  transformation.  Obviously the rotation matrices utilize the most recent computations  of the vehicle orientation angles,   ψ,   θ and   φ.   136     ROBOT NAVIGATION      a x  a  y   a z inertial coords            =  Rot  ψ      Rot     φ                        Rot  θ                   a x    a    y       a z vehicle coords           4.51    One  then  performs  a  double  integration  of  these  accelerations  as  before  to  determine  the  new  vehicle  position.  The  equations  above  must all be executed at each sample instant. It is clear that the equa- tions  are  much  more  complex  for  the  case  of  strap - down  gyros  than  for  gimbaled  gyros.  However,  construction  costs  for  gimbaled  gyros  can  be  very  high.  Complex  mechanical  hardware  for  the  gimbaled  gyro  is  replaced  by  complex  software  operations  for  the  strap - down  gyro.    The ring laser gyro is another type of strap - down gyroscope. It mea- sures the time it takes for light to make a complete circuit through a  glass enclosure or through a coil of optical ﬁ ber. By sending two light  beams in opposite directions, the difference in arrival time of the two  beams can be used to compute the angular motion and thus the angular  body rate about that axis. Again, quaternions may used as a means to  keep track of the attitude of the vehicle given the angular body rates.  Since no gimbals are involved, translational accelerations are measured  in  body  coordinates  and  must  be  converted  to  inertial  coordinates  before  integration  to  obtain  velocity  and  position  calculation.  Ring  laser  gyros  are  quite  accurate  for  attitude. Translational  position  and  velocity  computations  are  still  at  the  mercy  of  accelerometer  bias.  Computations and software requirements for these two types of strap -  down gyros are comparable.    Low - cost micro - electromechanical - systems  MEMS  devices provide  another  means  of  measuring  attitude.  These  devices  use  vibrational  motion of solid - state devices, and the sensed Coriolis forces are used  to compute attitude change. While the simplicity of construction and  low  cost  make  these  very  attractive  for  certain  situations,  limits  on  accuracy and precision may preclude their use in some applications.    Some of the navigation errors that may result from inertial systems   are listed below.     Instrumentation errors:  The sensed quantities may not exactly equal  the  physical  quantities  because  of  imperfections  in  the  sensors   e.g.,  bias, scale factor, non - linearity, random noise .    DEAD RECKONING OR DEDUCED RECKONING     137    Computational errors:  The navigation equations are typically imple- mented by a digital computer. Imperfect solutions of differential equa- tions as a result of having approximated them as difference equations  comprise another source of error.     Alignment  errors:   Errors  caused  by  the  fact  that  the  sensors  and  their  platforms  may  not  be  aligned  perfectly  with  their  assumed  directions.       4.10      DEAD RECKONING OR DEDUCED RECKONING    Dead  Reckoning  typically  uses  shaft  encoders  or  similar  devices  to  measure  the  angular  rotation  of  the  wheels. The  simple  formula  that  follows is then used to convert this measurement to distance traveled.     Α  S r= θ     ∆    4.52       In  the  above,    r  is  the  wheel  radius. This  yields  length  of  the  path  traveled by the wheel, but it does not result in the new position since  it  does  not  contain  any  information  about  the  curvature  of  the  path  traveled. To determine this, direction must also be recorded. One way  to accomplish this is to use two encoders. Placing an encoder on each  rear wheel works for front wheel steered vehicles or for independent  wheel drive vehicles. Using a proﬁ le of each encoder reading, one can  track  vehicle  motion  in  terms  of  direction  and  distance  traveled  and  can  compute  its  new  position  given  its  initial  position. The  following  equations  give  the  incremental  changes  in   x   position,   y   position,  and  heading.  r        ∆  ψ  =  ∆ θ L          r     − ∆ θ R W θ ∆ + R 2 θ θ ∆ + ∆ R l 2  θ ψ ∆    sin  l          cos  ψ          ∆ x  = −  r        ∆ y  =    4.53a       4.53b       4.53c       Here   W   is  the  lateral  distance  between  the  wheels,    r  is  the  wheel  radius and the   ∆θ  ‘ s are the incremental encoder readings expressed in  radians. These can be expressed as difference equations               138     ROBOT NAVIGATION     ψ    k  +    1  =  ψ      k  +  θ r   [     k  +    1  −  θ r     ] k  −  θ [ l     k  +    1  −  θ l     ]  k         4.54a        x k    +    1  =      x k  −  θ r   [     k  +    1  −  θ r     ] k  +  θ [ l    k  +    1  −  θ l  ψ    ] sin    k  k  +    1          and     y k    +    1  =      y k  +  θ r   [     k  +    1  −  θ r     ] k  +  θ [ l     k  +    1  −  θ l  ψ    ] cos    k  k  +    1         r W  r 2  r 2   4.54b      4.54c       It  should  be  apparent  that  a  little  wheel  slippage  can  cause  large  error buildups. For example a slight error in heading can cause a large  error in calculated location if the distance traveled is great. Thus this  method of deduced reckoning, sometimes called  “ dead reckoning ” , can  only  be  used  for  short  distances  and  needs  frequent  re - calibration.  Potential applications are in situations where contact with GPS satel- lites has been temporarily lost.     Let the wheel radius be 0.15     m and the lateral distance between tires 1     m.  ≤ Suppose  that     ψ π = − ,    x   0 15 ,      0   2   θl k − = < < 15 16        , and    and   k k   θr k < ≤     30 31 , 40.  Compute the trajectory and plot    y k     versus    x k   .   k  0=  .  Let     θl k     < ≤   θr k     k  0=  ,  and     y   0 40 .  Let    ,= 0 30   ,= k  =  k  0     EXAMPLE 9     SOLUTION 9     The trajectory is generated using the difference equations for x, y, and  heading, equation     4.45    . The result is shown in Figure    4.7  .           4.11      INCLINOMETER COMPASS    The  Inclinometer - Compass  measures  the  rotation  of  the  longitudinal  axis about the original  z  axis, i.e., yaw, via a digital compass. It measures  the angle of the longitudinal axis with respect to the original  xy  plane,    INCLINOMETER COMPASS     139  e  t  i  a n d r o o c   y  3.5  2.5  3  2  1  1.5  0.5  0 -1.4  y  x  =   g  g    g z 1 ψ φ − cos cos ψ φ + sin cos − cos sin                              g g g  x  y  z  2  -1.2  -1  -0.8  -0.6  -0.4  -0.2  0  0.2  0.4  0.6  x coordinate       Figure 4.7       Robot Trajectory Based on Dead Reckoning, y versus x    i.e., pitch, via the gravity vector. It measures the rotation of the body  about its longitudinal axis, i.e., roll, also via the gravity vector.    For the determination of pitch and roll from the sensed gravitational   force, one may use the general rotation matrix to write  ψ θ φ ψ θ φ  sin sin sin cos sin sin θ φ  ψ θ − sin cos ψ θ cos cos θ sin  cos sin sin sin  ψ φ iin sin cos + ψ φ − ccos  s cos sin θ φ  ψ θ φ   ψ θ φ     cos cos      21   Here the force vector is measured along the respective axes of the  rotated frame. The rotation matrix converts these forces to the original    140     ROBOT NAVIGATION  frame for which the  z  axis is vertical. In the original frame the gravita- tional force is zero except along the  z  axis, permitting one to write  ψ θ φ ψ θ φ  sin sin sin cos sin sin θ φ  ψ θ − sin cos ψ θ cos cos θ sin  ψ φ + ψ φ −  cos sin sin sin  sin cos θ φ  sin cos sin co ss  ψψ θ φ   ψ θ φ     cos cos      21  =   0  0   −  g 1 ψ φ − cos cos ψ φ + sin cos − cos sin        g g g  x  y  z  2   Now since the components of gravitational force along the various  axes are independent of yaw, one may simplify the above relationships  by  considering  the  case  where  yaw  is  zero.  Under  this  condition  the  above equation simpliﬁ es to            0 0 − g        1  =        φ cos θ φ sin sin θ φ − cos sin  0 cos sin  φ  sin  θ θ φ sin cos   θ ccos cosθ φ   −        g g g  x  y  z        21  2         4.55       From the ﬁ rst equation one obtains for roll     φ=  −− 1 tan    g    gx  z            4.56a       Now in the second equation one can make use of the ﬁ rst result and   replace   gx by     g  x  = − sin   cosφ  g  φ    z   After some algebra one then obtains for pitch     θ  =  − 1 g tan    y  φ cos    g  z            4.56b       Now  the  magnetic  compass  is  used  to  determine  yaw  through  the  detection  of  the  magnetic  ﬁ eld  along  each  axis.  Consider  a  vehicle  pointing  North  with  zero  pitch  and  roll. The  magnetic  ﬁ eld  detected                                      INCLINOMETER COMPASS     141  would be exclusively along the  y  axis. Now consider the vehicle yawed  but still with zero pitch and roll. The magnetic ﬁ eld detected would be  along both the  x  and  y  axes. Using the rotation matrix for yaw                 ψ    Ryaw     =        ψ cos ψ sin 0  ψ ψ  − sin cos 0        0 0 1     coupled with the above information yields            0 m y 0  1        =        ψ cos ψ sin 0  ψ ψ  − sin cos 0        0 0 1        m x m y m z  2  2  2           or from the ﬁ rst equation  with solution  =     0  m x  2  cos  ψ  −  m y  2  sin  ψ      tan  ψ= −m mx     2  . 2     y   This equation enables one to determine yaw from the components  of the magnetic ﬁ eld detected along each axis. It is important to keep  in  mind  that  this  result  was  derived  under  the  assumption  of  zero  pitch and roll. Next include pitch and roll as well as yaw. The relation  between the vehicle with yaw alone and the one with yaw, pitch and  roll would be  ψ cos ψ sin 0            − sin cos 0  =        ψ cos ψ sin 0  ψ ψ   0  0    1 ψ − s iin ψ cos 0        2  2   m x  m  y   m 2 z   1 0   0 0       0 1  0 cos sin  θ θ    0   − θ sin    −  θ   cos  cos 0 sin  φ  φ  0 1 0     sin 0 cos  φ     φ         m x m y m z  3  3  3         142     ROBOT NAVIGATION            m x m y m z  2  2  2        =        1 0 0  0 cos sin  θ θ   0  − θ sin   θ  cos        cos 0 sin  −  φ  φ  0 1 0  s  iin 0 cos  φ     φ         m x m y m z  3  3  3                     m x m y m z  2  2  2        =        φ cos θ φ sin sin − θ φφ cos sin  0 cos sin  θ θ  φ  sin  − θ φ sin cos   θ φ  cos cos        m x m y m z  3  3  3           with solutions     m x  2  =  cos  φ m x  3  +  sin  φ   m z  3  or  or              and     m y  2  =  sin sin  θ φ m x  3  +  cos  θ m y  3  −  sin cos  θ φ     m z  3   Finally one has  tan       ψ  = − = −  x    m m 2 y φ  cos m x  2  3  +  sin  φ m z  3     sin sin  θ φ m x  3  +  cos  θ m y  3  −  sin  θ  ccos         φmz3  4.56c       It is important to realize that the inclinometer responds to accelera- tion.  If  the  vehicle  is  stationary  or  is  moving  in  a  straight  line  at  a  constant rate, the only acceleration is gravity, and the instrument pro- vides a correct indication of pitch and roll. However, for any other case  the indicated pitch and roll will be erroneous. Thus, some other means  must be sought for dynamic situations. The inclinometer could be used  in situations where the robot comes to a stop and then performs some  action such as acquiring a radar image or infrared image. The attitude  measurement  from  the  Inclinometer Compass  could  then  be  used  to  convert the image from robot coordinates to earth coordinates.     EXERCISES              1.     A local coordinate system is set up at longitude of 77 degrees West  and latitude of 38 degrees North. The  x  axis points East and the  y     EXERCISES     143  axis points North. A mobile robot is determined to be located at  76.95 degrees West and 38.02 degrees North. Find the robot coor- dinates in the local frame in meters. Assume a spherical earth with  radius of 6,378,137 meters.             2.     For a given Longitude    =    85   W and Lattitude    =    42   N, ﬁ nd X,Y,Z in  ECEF  coordinates.  Use  dimensions  of  meters  and  assume  the  point  is  on  earth ’ s  surface.  Take  as  the  earth ’ s  radius  6,378,137  meters.             3.     Consider  the  point  for  which  the  ECEF  coordinates  are:  X    =    3,000,000 Y    =     − 5,000,000  and  Z    =     − 2,638,181.  Find  Long  and  Lat  Note: The point may be slightly off the earth ’ s surface .            4.     Find UTM coordinates for the point Long    =    10   E, Lat    =    43   N.            5.     A  robot ’ s  GPS  reads  long    =    85  degrees  and  lat    =    28  degrees.  Determine the longitudinal strip for UTM coordinates. Determine  the  central  meridian  for  this  strip.  Using  the  spherical  earth  approximation  determine  the  UTM  coordinates,  i.e.,  Easting  and  Northing.             6.     A  robot  travels  from  the  origin  of  a  set  of  local  coordinates   long0    =     − 77 degrees and lat0    =    38 degrees  to a point long    =     − 77.01  degrees  and  lat    =    38.03  degrees.  Find  the  distance  between  the  robot and the origin. Ignore the  z  component.             7.     A robot on an exploration ﬁ nds an object of interest. With respect  to the robot, the object is 15 meters in front and 3 meters to the  left. The robot ’ s position in UTM coordinates is Easting    =    246,315.2  and  Northing    =    1,432,765.4.  Its  heading  is  60  degrees  measured  counter - clockwise from East. Both pitch and roll are zero. What are  the UTM coordinates of the object of interest?             8.     A DGPS system is being used for more precise location determina- tion.  The  base  receiver  is  placed  at  a  surveyed  location  with  Easting    =    276,453.12  and  Northing    =    1,235,462.76. After  traveling  some distance the robot reads as its location Easting    =    276,602.32  and Northing    =    1,235,813.76 while the base station reads as its loca- tion Easting    =    276,454.62 and Northing    =    1,235,464.15. What is the  corrected robot location?             9.     An array of 4 GPS antennas has the following respective position  vectors in the order of Easting  x , Northing  y  and elevation  z .  Both L and W are one meter.   144     ROBOT NAVIGATION  v      v      v      v      =  1 1 0 e . e 1 0 . e 1 0 . 2 1 0 e . 1 0 e . e 1 0 .  =  =  3 1 0 . 1 0 . 1 0 .  e e e  =  4 1 0 . 1 0 . 1 0 .  e e e  + + + + + + + + + + + +  006 0 65342746650635 006 1 34527600549365 0006 0 00003969640952 006 0 65342745390796 006 1 34527671161197 0006 0 00003966122139  * . * . * . * . * . * .  006 0 65342678349365 006 1 34527668850635 0006 0 00003943759048  * . * . * .  006 0 65342679609204 006 1 34527598238804 0006 0 00003947277861  * . * . * .                Find the robot attitude in terms of yaw, pitch and roll.  See conven- tion for receiver placements in Chapter  5 .          10.     At  a  given  time,  a  set  of  satellite  positions  in  ECEF  coordinates   were given as  .  .  .  31864 37  = = − = = − = − = = − = − = = − = − =     x1 7766188 44 ;   ;      y1 . 21960535 34 ;      z1 12522838 56 25922679 66    x2 ;   .    y2 6629461 28 ;      z2    x3    y3 ;      z3 1692757 72 2786005 69    x4 . ;   . ;      y4 15900725 8 ;       z4  ;   5743774 02 25828319 92 . ;    21302003 49  ;   .  .  .  .   At the corresponding time, the ranges from a receiver are measured  to be                                                                                 EXERCISES     145  .  = = = =     d1 1022228206 42    d2 1024096139 11    d3 1021729070 63    d4 1021259581 09  .  .  .                Compute the location of the receiver. Work in ECEF coordinates.  Since data from four satellites are provided, you have enough equa- tions to solve not only for the location, but also determine the local  clock error. You may use the following equations  = = = =  1     d    d 2    d    d  3  4  [   1  X x     2  [   2  X x  [   3  X x  2    2     [   4  X x     2  − − − −  + + + +  1    Y  2    Y  3    Y  4    Y  − − − −  2    y  2    y 2    y  2    y  + + + +  1  2  Z z  − Z z − − Z z −  Z z    ] . 2 0 5 2 0 5   ] .   ] . 2 0 5 2 0 5   ] .  4  4              + + + +  ctr     ctr  ctr  ctr           where  the    di  are  the  measured  pseudoranges,     X i,  Y i,  Z i   are  the  ECEF coordinates of iteratively for  x ,  y , and  z . Several iterations  may be required to determine the values of  x , satellite i,   x y, z   are  the ECEF position coordinates of the receiver antenna, where  tr  is  the receiver clock bias and  c  is the speed of light. Solve these non- linear equations  y , and  z  that are consistent with the measured   dis.  Also determine the local clock error.         11.     A differential - wheel steered robot uses deduced reckoning to keep  track of its position. Each wheel has radius of 0.2 meters. The width  of the robot is 1 meter. The initial heading is zero, and the initial  position is 0, 0. For the right side the rotation of the wheel is mea- sured via an encoder and is given by,     θright k      = 0 5 . *  k       For the left side it is     θleft k      =  . 0 0005  *  k  3  . 0 01  *  k  2  . * 0 5  k      +  −   Plot x vs k, y vs k and y vs x for   0  ≤ ≤k  20          12.     At a given point signals are received from different four satellites.  At the time the signal left the satellites their positions expressed in  ECEF coordinates were as follows:   146     ROBOT NAVIGATION                                                  .  .  .  31864 37  = = − = = − = − = = − = − = = − = − =     x1 7766188 44 ;   ;   21960535 34 .    y1 ;      z1 12522838 56 25922679 66    x2 . ;      y2 6629461 28 ;      z2    x3    y3    z3 1692757 72    x4 .    y4    z4  ;   2786005 69 15900725 8  ;   5743774 02 ;   . 25828319 92  ;   . ;   ;     21302003 49  ;   .  .  .  .  .  = = = =     d1 1022228210 42    d2 1024096127 11    d3 1021729065 63    d4 1021259591 09  .  .  .                Based on the times the signals were received and the information  regarding  when  they  were  transmitted  from  the  respective  satel- lites, their distances from the receiver are computed to be   What is the receiver location? Do not worry if the point is not on  the earth ’ s surface. What is the local clock error?         13.     Consider the four antenna GPS array of a different conﬁ guration  than  the  one  analyzed  in  the  text.  Let  the  coordinates  of  each  antenna in the vehicle frame be given as follows: x1    =    W 2, y1    =    L 2,  z1    =    0, x2    =    W 2, y2    =     − L 2, z2    =    0, x3    =     − W 2, y3    =     − L 2, z3    =    0, and  x4    =     − W 2,  y4    =    L 2,  z4    =    0. The  position  of  each  antenna  is  pro- vided via the GPS receiver. Use the transformation matrix for yaw,  pitch and roll in that order to compute the position of each corner  of the array for arbitrary attitude. Decide how to combine x, y, or  z measurements of the different corners to isolate some convenient  trigonometric function of yaw and solve for yaw in terms of these    REFERENCES     147  measurements  x1    =    W 2, Y1    =    L 2,  Z1    =    0.  Do  the  same  for  pitch  and  roll.  Use  your  own  judgment  in  deciding  what  combinations  are less susceptible to error. Your ﬁ nal answer should be a set of  expressions for yaw, pitch and roll in terms of the measured posi- tions of the array corners.          REFERENCES     Johnny Appleseed  G  P  S — The Theory  and  Practice  of  GPS.   http:  www.ja -   gps.com.au what - is - gps.aspx .      Aviation  Theory:  Global  Positioning  System.   http:  www.ﬂ ightsimaviation.  com aviation_theory_9_Global_Positioning_System_GPS.html .        Brogan ,  W. L.    “  Modern Control Theory  ” ,  Prentice Hall ,  Upper Saddle River,  NJ ,  1991 .  http:  www.colorado.edu geography gcraft notes coordsys coordsys. html .        P. H.   Dana  , The Global Positioning System   2000  .       El - Rabbany ,  A.  ,  Introduction to GPS The Global Positioning System ,  Artech   House ,  2002 .        Jay. A.   Farrel  , and   Mathew   Barth  ,  “  The Global Positioning System  &  Inertial   Navigation  ” ,  McGraw - Hill Professional ,  1998 .      GPS  Primer  Elements  of  GPS.   http:  www.aero.org education primers gps   elements.html .    Press ,  Princeton, NJ ,  1999 .    2002 - 05 - 29.        Kuipers ,   J.  B.  ,   “  Quaternions  and  Rotation  Sequences  ” ,   Princeton  University      “ Military Map Reading 201 ”   PDF . National Geospatial - Intelligence Agency.       Widrow   and   Stearns  ,  Adaptive Signal Processing ,  Prentice Hall ,  1985 .       Yen ,   Kenneth    and    Gerald    Cook  ,  “  Improved  Local  Linearization Algorithm  for  Solving  the  Quaternion  Equations  ” ,  AIAA  Journal  of  Guidance  and  Control , Vol.  3 , No.  5 , pp  468  –  471 .                5   APPLICATION OF KALMAN  FILTERING           5.0      INTRODUCTION    This  chapter  is  devoted  to  estimation  of  non - deterministic  quantities  with special focus being on estimation of the state of a dynamic system,  e.g., the location and orientation of a mobile robot, and also estimation  of the coordinates of a detected object of interest. The Kalman Filter  is  presented  and  utilized  to  a  large  extent.  Simulations  are  used  to  illustrate the capability of this methodology.      ESTIMATING A FIXED QUANTITY USING      5.1   BATCH PROCESSING    The accuracy of the geo - location of objects of interest depends on many  factors, not the least of which is the accuracy of the estimates of the  robot ’ s  position  and  attitude.  In  order  to  enhance  this  accuracy,  a  Kalman Filter may be utilized in conjunction with the measurements  and  the  kinematic  model. The  development  of  the  Kalman  ﬁ lter  will  begin with a well - known estimation problem, that of estimating a ﬁ xed  quantity using batch processing.   Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  149   150     APPLICATION OF KALMAN FILTERING   First deﬁ ne the vectors  =     Y k  =     V k        y 1 y 2 y k              v 1 v 2 v k                  =     Y Hx V k  k       +     cov   k =  V  R         5.1       5.2        5.3        5.4       We have a set of measurement equations with error   Vk relating the   quantity to be estimated,   x to the measurements,   Yk.   The  errors  are  assumed  to  have  zero  mean  and  covariance   described by   We  form  a  function  to  be  minimized  utilizing  the  inverse  of  the   covariance as the weighting matrix  =  −  − 1  −  k  k  k     ˆ        T    ˆ    J Y Hx R Y Hx k    5.5      We seek to ﬁ nd that   ˆxk which minimizes this performance measure.  By  using  the  inverse  of  the  covariance  as  the  weighting  matrix  one  causes  those  measurements  with  smallest  measurement  error  covari- ance to be emphasized most heavily. The solution to this least - squares  problem is well known to be          =     ˆ x k  [  T  H R H H R Y k  T       − 1 ]  − 1  − 1    5.6       The covariance of the error in this estimate may be determined by  ﬁ rst replacing   Yk in the above equation utilizing equation   5.3  . Doing  this yields  −  ˆ    x x k  = −  x H R H H R Hx V k     [  T  T        − 1 ]  − 1  − 1  +  −  ˆ    x x k  = − [  T  H R H H R V k  T      − 1 ]  − 1  − 1  and                       or   ESTIMATING A FIXED QUANTITY USING RECURSIVE PROCESSING     151   Then forming the expectation of the outer product, recognizing that  the expected value of the error itself is zero, yields for the covariance  ˆ    E x x  {   −  k  −  ˆ x x      T   }  k  =  T  [ H R H  − 1  − 1   ]  which is denoted as   Pk . Thus     P H R H  [  T  k  − 1  =  − 1     ]    5.7         ESTIMATING A FIXED QUANTITY USING      5.2   RECURSIVE PROCESSING    The above result is a weighted generalized inverse. Now, if additional  data become available, it is desirable to update the estimate by incor- porating  the  new  data,  but  without  re - processing  the  data  already  accounted for, i.e., we do not wish to repeat the processing of the entire  batch. Thus a recursive scheme is sought which will incorporate the new  data by using it to bring about a correction to the previous estimate.  The following development follows very closely that in Sage and Melsa,  chapter 6, section 7, and also in Brogan, chapter 6, section 8. The addi- tional measurement obtained is given by     y k  +  1  =  H x v k  +  k  1  1      +  +   As before we form a function to be minimized  =  [ ˆ   Y Hx k  −  k  T    +  1     y k  +  1  −  ˆ H x 1  +  k  +  1  k  T    − 1  ]   R 0  0 − 1 R + k  1        k  ˆ Y Hx  − + 11 k − ˆ H x 1  +  +  k  1  y k  with solution given by   = [    +  1  ˆx   k      T  H H  T + k  1  − 1  ]   R 0  0 − 1 R + k  1        H H  k  +  1         − 1  [[  T  H H  T + k  1  − 1  ]   R 0  0 − 1 R + k  1    5.8        1 k   5.9          +  Y k y k        1   5.10     +                J        or     {  =     ˆx k  +  1  T  H R H H R H  T + k  1  1  +  1  k  − 1 + k  − 1  +  T  H R Y H R y k  T + k  k  1  1  +  − 1 + k  − 1  +  − 1  }  [  ] 1      152     APPLICATION OF KALMAN FILTERING   Now  using  the  previously  derived  result  given  by  equation    5.7     rearranged  then  =1 −    P k  T  H R H  − 1     {  =     ˆx k  +  1  − 1 P k  +  H R H  T + k  1  1  − 1 + k  +  1  k  T  H R Y H R y 1 k  T + k  k  1  +  1  − 1 + k  − 1  +  − 1  }  [  ]     or, through use of the Matrix Inversion Lemma, [ }  P P H H P H k  − 1 ]  R k  T + k  T + k     ˆ x  {  −  =  +  [  +  +  +  k  k  k  k  k  1  1  1  1  1  1  − 1 ky+ H P H R Y H R 1 kk  T + k  − 1  T  +  k  k  1  +  +  ] 1    which can be written as  =  ˆ x k      +  1  − 1  −  T  [ ] P H R Y k k + − 1 P H R y + 1 k k k  T + k  1  1  [  P H H P H k  T + k P H H P H k  + 1 [  T + k  T + k  −  +  +  k  k  k  k  1  1  1  1  + R k T + k  1  + 1 +  − 1 ] R k  − 1  T  k  ] H P H R Y kk − 1 R y+ k k  [ + 1 k − 1 ] H P H  T + k  +  +  k  k  11  1  1  1     +  1  =  ˆ x k      +  1  −  ˆ x k +  P H H P H k  + k P H I H k  T + k  T + k k P H k  [ 1 −  T + k {  [  ++  k  1  1  1  1 T + k  1  1  R + k + R k  +  1  +  ˆ H x 1 k − 1 ]  k H P H R y 1 k  − 1 + k  T + k  }  +  k  k  1  1      +  1  − 1 ]  +   Now expressing  we have  =     I H P H  [  +  k  k  1  T + k  1  +  R k  +  1  − 1 ] [  H P H  +  k  k  1  T + k  1  +  R k  1   ]  +  =  +  1  ˆ x k      −  ˆ x k + −  1  T T [ P H H P H + + 1 1 k k k + T {[ P H H + 1 k k − 1 T [ ] [ H P H + k  + k k 1PP H k + R k  T + k  +  +  +  k  k  k  1  1  1  1  +  R k  R k H  k  +  1  ++  1  k  +  − 1 ] + 1 − 1 ] [ P H k  ˆ H x 1 k H P H − 1 T + + k k  T + k k R y 1 k  + 1 ]}  k  1  1  +  +  1  R k  +  1  ]     =  +  1  ˆ x k      −  ˆ x k + −  1  [  T + k [  P H H P H k  + 1 k k T P H H P H + 1 kk k k − 1 ] H P H R y + k k  T + k  T + k  +  +  k  k  k  1  1  1  1  1  +  1  +  T + 1 k + R k  − ˆ 1 ] H x R + 1 1 k k − 1 ] [ H P H  +  +  +  k  k  k  1  1  +  T + k  1  R k  +  1       This reduces to =  ˆ x k      +  1  −  ˆ x k +  [  P H H P H k  + k P H H P 1 kk k  1 k H  T + k [  T + k  T + k  +  k  1  1  T + k  +1  +  1 R k  R + 1 k − 1 ]  +  1  k  − ˆ 1 ] H x + 1 k − 1 R R y + 1 k k  +  k  1     +  1                       or  or   ESTIMATING A FIXED QUANTITY USING RECURSIVE PROCESSING     153  or           or           ˆ x  +  1  k  =  ˆ x  k  +  P H H P H k  T + k  [  +  k  k  1  1  T + k  1  +  R k  +  1  − 1 ] [  y k  +  1  −  ˆ ] H x 1 k  +  k     or ﬁ nally     ˆ x k  +  1  =  +  ˆ [ x K y k k  +  k  1  +  1  −  ˆ ] H x 1 k  +  k       where the deﬁ nition has been made     K  +  1  k  =  P H H P H k  T + k  [  +  k  k  1  1  T + k  1  +  R k  +  1  − 1 ]         5.11       5.12       We see that the new estimate is given by the old estimate plus a cor- rection. The correction consists of a residual, i.e., the difference in the  new measurement and what the new measurement would be expected  to be based on the old estimate, multiplied by the optimal gain. This  form is intuitively appealing and provides the efﬁ ciency of being recur- sive, i.e., it builds on the estimate up to this point in time and does not  re - process measurements that have already been processed. For addi- tional recursive steps, recalling the deﬁ nition for   Pk one replaces   H by          and   R−1 by       to obtain  H Hk+  R 0  0 − 1 Rk +        − 1  1  1     P k  +  1  T  H H  T + k  1   = [    − 1  ]   R 0  − 1  0 − 1 R + k  1        H H  k  +  1         =  [  T  H R H  − 1  ++  H R H  T + k  1  1  − 1 + k  +  1  k  − 1]        P k  +  1  =  − 1 P k  [  +  H H H  T + k  1  1  − 1 + k  +  1  k  − 1 ]         5.13       Using the matrix inversion lemma this can be expressed as     P k  +  =  −  P P H H P H k  T + k  [  +  k  k  k  1  1  T + k  +  R k  +  − 1 ]  H P k  +  k  1       1  1 −1 . Note from equation   5.14     which does not require the existence of   Pk that the norm of   Pk+1  will be less than the norm of   Pk , indicating the  improvement in the estimate as a result of the new measurement. The  next value for the gain matrix can now be found using equation   5.12    with the appropriate subscript updates.   1    5.14         EXAMPLE 1     Estimate the y coordinate of a stationary object with n equally reliable  unbiased measurements.      154     APPLICATION OF KALMAN FILTERING    SOLUTION 1     Using  batch  processing  one  would  compute  the  expected  value  of  this  coordinate to be     Upon receiving an additional measurement one could repeat the batch   processing and obtain      ˆx  n  n  = ∑1  n  1  y i         ˆx  +  1  n  =  1  n  +  1  + ∑  1  1  n  y i        However, the above is seen to be  ∑     ˆx  =  +  n  n  1  1 +  n  1  1  +  y i  1 +  n  1  y n  +  1        ˆx  +  1  n  =  n +  n  1      1 n  n  ∑  1    +   y i  1 +  n  1  y n  +  1        ˆ x  +  1  n  =  −  1     1 +     1  n  +  ˆ x  n  1 +  n  1  y n  +  1        ˆ x  +  1  n  =  ˆ x  n  +     y n  +  1  −  ˆ   x n      1 +  n  1    This can be viewed as the old estimate plus a gain times the residual  where the residual is the difference between the new measurement and  the expected value of the new measurement based on the old estimate.       The actual value of a quantity is 5.0. Use only noisy measurements of  this quantity to estimate its value. The noise, which is additive, is Gaussian                      or    or    and ﬁ nally     EXAMPLE 2    ESTIMATING A FIXED QUANTITY USING RECURSIVE PROCESSING     155  with  mean  of  zero  and  variance  of  0.04.  Use  a  sequence  of  50  measurements.       SOLUTION 2     The software code for solving this problem is given below and the results  are shown in Figure    5.1  .       %Estimation Example   R  =  0.04;   P  =  R;   y 1   =  5  +  sqrt R   *  randn;   xest 1   =  y 1 ;      for i  =  1:49                  y i + 1   =  5  +  sqrt R   *  randn;                  P  =  1     1   P   +   1   R  ;                  K  =  P    P  +  R ;      Measured Signal Estimated Signal Actual Mean  5.5  5.4  5.3  5.2  5.1  5  4.9  4.8  4.7  4.6  4.5    0  5  10  15  20  25  30  35  40  45  50       Figure 5.1       Sequence of Measurements and Estimates of the Quantity, 5.0       APPLICATION OF KALMAN FILTERING  156                  xest i + 1   =  xest i   +  K  *   y i + 1  - xest i  ;      end         ﬁ gure      plot y       hold on      plot xest          ESTIMATING THE STATE OF A DYNAMIC      5.3   SYSTEM RECURSIVELY   +  +   The  problem  next  considered  differs  from  the  problem  addressed  in the previous section, i.e., estimating a ﬁ xed quantity, in that it is seeks  the estimate of the state of a dynamic system. Thus the state is changing  from sample instant to sample instant. This problem is more complex  than that of estimating a ﬁ xed quantity, and yet there is a relationship  obeyed by this dynamic state that can be exploited in the estimation  process. This relationship is the system equations.    Here we assume a linear time invariant system operating in discrete   time with the following description. The state equations are           X k      1 T AX kT Bu kT Gw kT                         +  =  +    5.15     with output equation +  =  +  +             v k      1 T      1 T        Y k         1 T HX k    5.16      Here   w represents a Gaussian, zero - mean, independent process dis- turbance of covariance   Q and   v represents Gaussian, zero - mean, inde- pendent measurement noise of covariance   R. For shorthand notation,  we shall represent the output at time      as   Yk+1.  Denote  the  estimate  of  the  state  at  time    kT  given  measurements  through time   kT as    ˆXk k and the expected value of the state at time      k+1 .  This  latter  k quantity is a prediction and is obtained by computing the conditional  expectation of the next state taking advantage of the state equations.  The  result  from  simply  using  the  right - hand  side  of  the  above  equations is +      1 T Y kT  T+ 1   given  measurements  through  time    kT  as     ˆXk  T+ 1 , i.e.,   Y k        E X k  + 1      T   }  =  k                 {   } { E AX kT Y kT +  } E Bu kT Y kT               {  +         E Gw kT     {           } Y kT       5.17       ESTIMATING THE STATE OF A DYNAMIC SYSTEM RECURSIVELY     157   Using  the  shorthand  notation  coupled  with  the  fact  that  the  input  signal is known and the expected value of the disturbance is zero, this  reduces to      ˆ X  +  k  1     k  =  ˆ AX  k k     +  Bu k         5.18       The one - step prediction equation is seen to be simply a propagation  of  the  discrete - time  model,  building  on  the  estimate  at  the  previous  sample. With no additional measurements available, this would be the  best estimate of the new system state. It is the estimate of the state at  time       given measurements through time   kT.   T+ 1    k   The covariance of the error in the prediction may be determined by  ﬁ rst recognizing that the expected value of the prediction error is zero.  Thus this covariance reduces to     E X k  {        E AX  {   +       1      ˆ T Y k X      ˆ    kT Y kT Bu kT Gw kT AX  1     −     X k +  ˆ T Y k X −  − +  1             +  −        +  k  k  +  k  1  k    Bu kT          =  T   }  ˆ       kkT Gw kT AX A X kT Y kT Bu                        Bu kT     T    }    −    k k −  k k       +    +   Now since the present value of the state is independent of the present   value of the process disturbance, this reduces to       P k  +  1     ˆ k AE X kT X  {         =  −  ˆ X kT X  −            k k     T   }  T  T A GQG     k k     +  or  where     P k    +  1  T     k AP k k A GQG         T     +  =     P k k E X kT X         {             X kT X        T   }     k k     k k     =  −  −  the covariance of the error in the estimate at the  k  th  step. Here use was  made of the fact that the expected value of the error in the estimate is  zero.    Now when a new measurement becomes available, the information  contained within it can be incorporated to improve the estimate. After  T+ 1   the  new  estimate  is  as  obtaining  the  measurement  at  time      follows  k        E X k  {  1        T Y k  +  +  =  1 }       ˆ X −  +  +  k  1   k  X  +  k  1   k  {      E X k −       Y k  +  1  +  1    T ˆ HX  kk  k+1  }        5.19                         158     APPLICATION OF KALMAN FILTERING   Note that the equation above has been arranged in such a way that  the quantities in the conditional expectation on the right side are each  of zero mean. This becomes      ˆ X  +  k  1  k  +  1  =  ˆ X  +  k  1  k  +  P P Y k xy     − 1 yy  +  1  −  ˆ HX  +  k  1  k            5.20     where the covariances in the above equation are deﬁ ned as  =     P xy  E X  {   +  1  k  −  ˆ X  +  k  1   k      Y k  +  1  −  ˆ HX  +  k  1   k  T   }         5.21     and  variables  =     P yy  E Y k  {   +  1  −  ˆ HX  +  k  1   k      Y k  +  1  −  ˆ HX  +  k  1   k  T   }         5.22       The  preceding  is  based  on  the  known  basic  result  for  Gaussian            E x y  = +  x P P y y     xy          −1 yy  −    5.23       For the covariance   Pxy it may be seen from its deﬁ nition that  E X      {  =  +  1 k {   E X  −  ˆ X  +  1  k  + k −  1   k ˆ X      Y k  −  ˆ HX  k  +  k  1   k  HX  +  1  k  +  1     k  + 1  +  v kk  T   } −1  +  ˆ HX  T 1    } k  +  k       Now since   vk+1 is of zero mean, and also since   Xk+1 and    ˆ   Xk k+1   are   independent of   vk+1, this reduces to +  P k  =     P xy     1      k H  T       Likewise for the covariance   Pyy it may be noted that  {  E Y k     =  +  1 {   −  ˆ HX  k      k  + 1  +  Y k −  +  1  − ˆ HX  ˆ HX  +  k  1   k  +  1  k       T   }  k  E HX  +  1  k  v k  +  1  HX  +  1  k  +  ˆ v HX + 1  −  +  k  1   k  T   }  or   Thus  P HE X yy k     +  {    HP k  1  k  +     + 11   k H RT +  1   k  −  ˆ X  = =  X  +  1  k  −  ˆ X  +  k  1   k  T   }  T  { H E v k  T v + k  1  }  +  1  +  − 1    P P yy  xy  =    P k  +  1        k H HP k  [  T  +  − 1     ] k H R  T  +  1                                      ESTIMATING THE STATE OF A DYNAMIC SYSTEM RECURSIVELY     159   The estimation equation may be re - written in the form         ˆ X  +  k  1  k  +  1  =  ˆ X  +  k  1  k  +    K k  +     1  Y k  +  1  −  ˆ HX  +  k  1  k            5.24       The estimate is comprised of the prediction plus a correction. This   + 1 .  correction is a residual term pre - multiplied by the gain matrix   K k  The residual term is comprised of the actual measurement minus the  predicted value of this measurement. It is seen to depend on the pre- dicted state through the output matrix and is an  mx 1 vector, which is  usually of lower dimension than the state which is  nx 1.    The covariance of the error in the estimate may be evaluated. First   the error is     X  +  1  k  −  ˆ X  +  k  1  k  +  1  =  X  +  1  k  −  ˆ X  +  k  1  k  −    K k  +  1     Y k  +  1  −  ˆ HX  +  k  1  k          X  +  1  k  −  ˆ X  +  k  1  k  +  1  =  X  +  1  k  −  ˆ X  +  k  1  k  −    K k  +  1     HX  +  1  k  +  v k  +  1  −  ˆ HX  +  k  1  k         Forming the covariance as the expected value of this times its trans-  pose yields +    P k         1     k  +    1  =  1        k P k +   K HP k  +    − +   Utilizing  the  expression  above  for    P Pxy yy  performing some manipulations yields    P k  +  1     k  +    1  =  T  T  −    KHP k  +   1     P k k H K + T RR KT   1     k H −1  as  the  gain  matrix  and       k  +  1      +   P k −   P k      1 k +     1   k H HP k     T  +      1 k       5.25      + 1 , which will be of dimension nxm, may be obtained         k H R HP k  1     T  +  − 1  +   The gain   K k   sequentially by solving the equations     P k    +  1  T     k AP k k A GQG         T       +  =    5.26     which is the covariance of the error in the prediction, and       K k  +    1  =  − 1 P P yy xy  =    P k  +  1        k H HP k  [  T  +  − 1         ] k H R  T  +  1    5.27       The ﬁ lter gain may be used in turn for the following equation       P k  +  1     k  +  =  −  [    I K k  +    1  1    ]   H P k  1      k       +    5.28            or                   160     APPLICATION OF KALMAN FILTERING   which is the covariance of the error in the estimate. The last equation  may be shown to be equivalent to equation   5.25  . This ﬁ lter is optimal  in terms of providing the expected value of the state when the system  equations and the output equations are linear, when the process dis- turbance  and  measurement  noise  have  Gaussian  distributions  and  when they are both independent sequences.      EXAMPLE 3     Consider the discrete - time dynamic system           x k 1       x k 2  + +  1   1    = =      x k Tx k 1 2 +         x k Tu k w k 2               + +       y k  +    1  =    x k 1  +  +    1    v k  +    1        where    x1  is position and    x2  is velocity. The measurement equation is     Let the system initially be at rest and let the input be a pulse of unit  amplitude  and  duration  10T  where T  is  taken  to  be  0.2  seconds. Take  variances    R    =    0.006    and    Q    =    0.00005  .  Simulate  this  system  and  imple- ment a Kalman Filter to estimate the state.       SOLUTION 3     The software code for solving this problem is given below and the results  are shown in Figures    5.2a    and    5.2b  .       T  =  0.2;   A  =  [1 T; 0 1];   B  =  [T ^ 2   2 T] ′ ;   H  =  [1 0];   G  =  [0 1] ′ ;   Q  =  0.00005;   R  =  0.006   x1 1   =  0;   x2 1   =  0;   x1e 1   =  0;   x2e 1   =  0;   xest  =  [x1e 1  x2e 1 ] ′ ;   x1p 1   =  0;    ESTIMATING THE STATE OF A DYNAMIC SYSTEM RECURSIVELY     161   x2p 1   =  0;   PE  =  [R 0; 0 0];   PP  =  A  *  PE 1   *  A ′   +  Q;      for i =  1:25               if i    <    10                           u  =  0.25;               else                        u  =  0;               end               x1 i + 1   =  x1 i   +  T  *  x2 i   +   T ^ 2   2                  x2 i + 1   =  x2 i   +  T  *  u  +  sqrt Q   *  randn;               y i + 1   =  x1 i + 1   +  sqrt R   *  randn;                  PP  =  A  *  PE  *  A ′   +  G  *  Q  *  G ′ ;               K  =  PP  *  H ′   *  inv H  *  PP  *  H ′   +  R ;               PE  =  [eye 2   -  K  *  H]  *  PP;                  xpredict  =  A  *  xest  +  B  *  u;               xest  =  xpredict  +  K  *   y i + 1   - H  *  xpredict ;               x1e i + 1   =  [1 0]  *  xest;               x2e i + 1   =  [0 1]  *  xest;   end     *  u;     The beneﬁ t of the ﬁ ltering is quite apparent for the estimate of position  when compared to the noisy measurement and especially apparent for  the estimate of velocity that was not even measured.       It is interesting to further analyze the contributions of the two terms  that make up the estimate of state. The prediction is the propagation  of the previous estimate utilizing the system model. The reliability of  this model is manifested through one ’ s choice of   Q, the covariance of  the disturbance   w. If one has conﬁ dence in the ﬁ delity of the model,  then the   Q matrix is chosen to represent only the process disturbance.  If one knows that the model is only a rough approximation, then the    Q matrix is chosen to be larger to account for this uncertainty as well  as  the  process  disturbance.  When  this  covariance  is  small,  then  the  model  can  do  a  good  job  of  predicting  the  next  value  of  the  state.  However, if this covariance is large, then the prediction is not very reli- able,  and  the  output  measurement  needs  to  play  a  greater  role  in  determining the next value of the state.    Actual value Estimate Noisy Measurement     5  10  15  20  25  30       Figure 5.2a       Plot of Actual Value, Noisy Measurement and Estimate of x 1        Actual value Estimate  2.5  1.5  0.5  2  1  0  -0.5    0  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0    0  5  10  15  20  25  30       Figure 5.2b       Plot of Actual Value and Estimate of x 2 . No Measurement Available    162   ESTIMATING THE STATE OF A DYNAMIC SYSTEM RECURSIVELY     163  + 1   The  reliability  of  the  output  measurement  is  manifested  through  one ’ s choice of   R, the covariance of the measurement noise,   v. A small  measurement noise covariance corresponds to an accurate output mea- surement with the result being that the residual correction is incorpo- rated strongly into the estimate. Conversely, a large measurement noise  covariance corresponds to an unreliable output measurement and the  residual  correction  is  given  a  lesser  role  in  the  estimate.  These  two  covariances,   Q and   R, interact together to yield the appropriate weight- ing to the model propagation and to the residual correction.       k   The   P k     equation shows that the covariance of the error in  prediction  depends  on  the  system  equations  through    A  and  on  the  process  disturbance.  Compared  to  the  covariance  of  the  error  in  the  estimate, this covariance of the error in prediction is always increased  by  the  positive  semi - deﬁ nite  disturbance  term,    GQGT  and  may  be  increased  or  decreased  by  the    AP k k AT   term  depending  on  the  eigenvalues of   A.           To analyze the behavior of the covariance of the estimate it is instruc- tive  to  utilize  the  expression  from  equation    5.25    which  is  repeated  here for convenience +   P k −   P k      1 k +     1   k H HP k        k H R HP k    P k  − 1 ]    1  +  +  =  +  +  +  k  1  1  1  [     T  T                k   5.29       The above reveals that the covariance of the error after the measure- ment has been taken, i.e., the covariance of the estimate, will always be  smaller than the covariance of the error before the measurement was  taken. This  can  be  seen  from  the  fact  that  any  covariance  matrix  is  positive  semi - deﬁ nite  and  that  the  term  being  subtracted  off  from  1  will be smaller    P k     in norm than   P k . This seems intuitively correct since the addi-   tion of the measurement can only provide more information as to the  true value of the state.    is positive semi - deﬁ nite. Thus   P k    + 1  + 1      k      k  +  +  k  1        EXAMPLE 4      with        Analyze the optimal ﬁ lter for the simple scalar system        x k  +  1 α    =      x k  +          u k w k     +         y k  =      x k  +      v k        164     APPLICATION OF KALMAN FILTERING    SOLUTION 4     The ﬁ lter equations are    the prediction, and    the estimate.       ˆ x  +  k  1 α ˆ x  =  k     k k     +     , u k        ˆ x  +  k  1     k  +  1  =  ˆ x  +  k  1     k  +    K k  +  1     y k  +  1  −  ˆ x  +  k  1     k   ,       Combining the equations for the covariance of the prediction and the  covariance  of  the  estimate  yields  a  single  equation  for  the  estimate  covariance      P k    +  1     k  +  =  −  [    I K k  +  1      1  T H AP k k A GQG         ][  T  ]      +    Setting the steady state of the estimation covariance to    p  and using the   model parameters for the example yields the equation      p  =  −  K      2α  +    p q        1    Using the equation for the covariance of the prediction in the equation   for the ﬁ lter gain yields   +    1  =    K k      T  T [ AP k k A GQG H H AP k k A + − 1 ] GQG H R                     ]  [  T  T  T  T     + +   or, for this example     It is seen from the above that for the special case    r = 0,   K  is unity. The   =     K  +  α 2 p q + + α 2 p q r         ˆ x  +  k  1     k  +  1  =  ˆ x  +  k  1     k  +    K y k  +  1  −  ˆ x  +  k  1     k           ˆ x  +  k  1     k  +  1  =  ˆ x  +  k  1     k  +     y k  +  1  −  ˆ x  +  k  1     k           ˆ x  +  k  1     k  +  1  =  y k  1     +  equation    becomes    or                               ESTIMATING THE STATE OF A DYNAMIC SYSTEM RECURSIVELY     165    That is, the best estimate is the output measurement itself since it is noise  free. Here the ratio of measurement noise to process disturbance is     Using the expression for    K  in the equation for    p  gives the equation      r q  = 0     =     p  + α 2     p q r + + α 2 p q r        Evaluating this for    r = 0  reveals that    p  is zero which would be expected   with zero measurement noise.      When  we  now  consider  the  opposite  case,  i.e.,  a  measurement  noise  that  is  very  large,  it  is  seen  that     p   takes  on  a  nonzero  but  ﬁ nite  value.  Dividing numerator and denominator of the expression for    p  above by     r  yields      α 2     p q +     p r q r  whose solution for large    r  becomes   α 2     p  =  +     +  1    The gain becomes   =     p  q 2α    −1  =     K  q −  1  +  q  2α     r     ˆ x  +  k  1     k  +  1  =  ˆ x  +  k  1     k         q r  = 0        which approaches zero for large    r . In this case the estimator approaches      ˆ x  +  k  1     k  +  1  =  ˆ x  +  k  1     k  +  0    y k  +  1  −  ˆ x  +  k  1     k          That  is,  the  best  estimate  is  the  propagation  of  the  model  since  the  measurement is very noisy. Here the ratio of process disturbance to mea- surement noise is    These  two  extreme  cases  in  the  example  above  illustrate  the  way  the  ﬁ lter  allocates  weighting  on  the  measurement  versus  weighting                         or    166     APPLICATION OF KALMAN FILTERING  on  the  propagation  of  the  model.  The  conﬁ dence  in  each  depends  on  the  respective  measurement  noise  and  process  disturbance  covariances.    By using the Kalman ﬁ lter, it is possible to obtain estimates having  lower error variance than the measurement itself. For the example just  considered, we set alpha to be .707 and   r to be 1.0. The parameter   q  was varied from 0.1 to 5.0 and the covariance of the prediction error  and the covariance of the estimation error were computed and plotted.  These  are  shown  in  Figure   5.3 . The  covariance  of  the  estimate  is  the  smaller  one.  Note  that  its  value  is  very  small  for  small    q  and  that  it  increases  with    q  but  is  always  lower  than    r.  In  fact  it  asymptotically  approaches   r as   q gets large.      It is also interesting to examine the value of gain under these same  conditions. In Figure  5.4  it is seen that the gain is low when the ratio  of process disturbance to measurement noise is small, indicating that  the ﬁ lter relies more on the model than on the relatively noisy measure-     Estimated Covariance Predicted Covariance  6  5  4  3  2  1  0    0  0.5  1  1.5 3.5 Process Disturbance Covariance, q  2.5  3  2  4  4.5  5       Figure 5.3   Disturbance Covariance,   q. Measurement Noise Covariance,   r, Is 1.0       Plot of Estimation Covariance and Prediction Covariance versus Process    ESTIMATING THE STATE OF A DYNAMIC SYSTEM RECURSIVELY     167  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  i     n a G n a m a K  l  0.1  0  0.5  1  1.5 3.5 Process Disturbance Covariance, q  2.5  2  3  4  4.5  5       Figure 5.4   ment Noise Covariance,   r, Is 1.0       Plot of Kalman Gain versus Process Disturbance Covariance,   q. Measure-  ment. When this ratio is large, the gain increases causing the ﬁ lter to  give more weight to the measurement. As   q gets larger the gain asymp- totically approaches unity.      Conversely, it is interesting to observe the behavior of the prediction  and  estimate  covariances  when  the  covariance  of  the  process  distur- bance is held constant and the covariance of the measurement noise is  varied. As  the  measurement  noise  covariance  get  large,  the  estimate  covariance  and  the  prediction  covariance  both  approach  the  same  2− α ; i.e., there is little or no additional improvement from  value,   q    1 using the very noisy measurement in the estimate versus the prediction.  This  is  evident  from  the  plot  of  estimate  covariance  and  prediction  covariance in Figure  5.5 .         The behavior of the ﬁ lter is also reﬂ ected in the plot of gain shown in  Figure  5.6 . When the covariance of the measurement noise is very large,  the gain diminishes. The ﬁ lter places its conﬁ dence in the propagation  of the model and ignores the new measurement. It is important to keep  in mind that these results shown in Figures  5.3 – 5.6  and in the preceding  example all apply only to the steady-state behavior of the ﬁ lter.          Estimated Covariance Predicted Covariance  0    0  10  20  40  30 70 Measurement Noise Covariance, r  50  60  80  90  100       Figure 5.5   ment Noise Covariance,   r. Process Disturbance Covariance,   q, Is 1.0       Plot of Estimation Covariance and Prediction Covariance versus Measure-  2  1.8  1.6  1.4  1.2  1  0.8  0.6  0.4  0.2  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  i     n a G n a m a K  l  0  0  10  20  40  30 70 Measurement Noise Covariance, r  50  60  80  90  100     Plot  of  Kalman  Gain  versus  Measurement  Noise  Covariance,    r.  Process        Figure  5.6   Disturbance Covariance,   q, Is 1.0   168   ESTIMATING THE STATE OF A NONLINEAR SYSTEM     169    ESTIMATING THE STATE OF A NONLINEAR SYSTEM      5.4   VIA THE EXTENDED KALMAN FILTER    When the system is nonlinear and described in terms of a continuous -  time model       cid:1 X f X U Gw  +  =        ,         5.30     where   w represents a Gaussian zero - mean white - noise process distur- bance, one can use Euler ’ s method of integration to yield as a discrete -  time model        X k      1 T X kT Tf X kT U kT   ,                     TGw kT               5.31      +  =  +  +   The sampling interval, T, must be taken sufﬁ ciently small in accor- dance with the system dynamics in order to provide an accurate repre- sentation of the system. The output equations are +  +  +  =  +       h X k       1 T     v k  1     T             Y k      1 T    5.32          Here again   v represents a random, zero - mean, white measurement  noise. Again it is helpful to use shorthand notation and represent the  T+ 1  as   yk+1 and the estimate of the state at time   kT  output at time     given measurements through time   kT as    ˆXk k. The one - step prediction  equation  is  simply  a  propagation  of  the  discrete - time  model  given  above and for this case is  k     {         E X k  +       1 T Y k  }  =  E X kT Tf X kT U kT  {    ,                     TGw kT Y k     +  +              5.33         ˆ X  +  k  1  k  =  ˆ X  k k  +  TE f X kT U kT  {     ,          }         5.34       It is well known that in general     E f X kT U kT Y k  {          ,        }    f X U kT     k k  ,             ≠  ˆ   To compute the solution to equation   5.34   would require knowledge   . In order to obtain a practi- of the probability density function   p X kT     via  the  cal  estimation  algorithm,  one  may  expand    f X kT U kT Taylor series about the current estimate of the state vector through the  linear term to obtain   ,              ,        or         170     APPLICATION OF KALMAN FILTERING    f X kT U kT   ,            ≈  ˆ    f X U kT     k k  ,         + ∂    ∂  f X        ˆ   X kT X  −        =       x x k k  ˆ     k k      +…   5.35a          and  or  and                                 }     E f X kT U kT Y k + ∂      f X    f X U kT  {   ≈        , ,    ˆ  ∂     k k          E X k  {   ˆ TT X  −         Y k  }  k k        +  …  = x x k k       ˆ           5.35b        E f X kT U kT Y k  {          ,        }    f X U kT     k k  ,     ≈  ˆ  + +     0  …        5.35c       Note that this approximation is correct through the linear term. For  more discussion on this see Gelb, A., ed., .Applied Optimal Estimation.  Cambridge, Massachusetts: MIT Press, 1974 pp183 – 184. Thus the pre- diction may be expressed using the shorthand notation as      ˆ X  +  k  1  k  =  ˆ X  k k  +  Tf X U  k k  ,    ˆ     k         T+ 1  the new estimate     5.36          After obtaining the measurement at time      k  is as follows      ˆ X  +  k  1  k  +  1  =  ˆ X  +  k  1  k  +  K Y k     +  1  −    ˆ h X  +  k  1  k             5.37     i.e., the prediction plus a correction. The gain   K is obtained by sequen- tially solving the same equations as before     P k    +  =  1  T         k A k P k k A k               GT Q GT       T         +    5.38a        K k    +    1  =    P k  +  1            k H k H k P k  T     [  1  T         k H k  +  +  − 1     ]  R    5.38b          P k  +  1     k  +  =  −  [    I K k  +    1    1     ]   H k P k  1      k       +    5.38c       Here   A k    is the linearization of the nonlinear discrete - time system   equations at the present estimate of the state,         A k  = +  I T  ∂          f X U ∂  , X          ˆ X  Uk k  ,     k    5.39a      ESTIMATING THE STATE OF A NONLINEAR SYSTEM     171  and   H k    is the linearization of the nonlinear output equation         H k  = ∂         h X ∂ X Xk k     ˆ            5.39b       Also note from equation   5.38a   that the discrete - time equivalent of  the disturbance coefﬁ cient matrix is   GT. The matrix   Q is the covariance   , and the matrix   R is the covariance  of the process disturbance   w kT  of the measurement noise   v kT   . Here, as in the case of a linear system,  we see the interaction between the covariance of the error in prediction  i.e., the error before the measurement is made, and the covariance of  the error in the estimate, i.e., the error after the measurement is made.  It is worth mentioning again that the covariance of the estimate error  is always less than or equal to the covariance of the prediction error  because more information has been made available.    If the nonlinear system is described initially in discrete time as     X  k  + =  1    f X U  ,  k  k     Gw k       +   Then the prediction step becomes simply      ˆ X  =1  k  +  k    ˆ f X U  k k  ,     k          and the matrix   A k    is deﬁ ned by = ∂    f X U          A k  ∂     , k X     k          ˆ X  Uk k  ,     k    5.40        5.41       5.42       Further there is no   T factor multiplying the disturbance coefﬁ cient  matrix  in  the  equation  for  the  covariance  of  the  prediction  error.  Everything else remains the same.      EXAMPLE 5     Apply the Kalman Filter to a front - wheel steered mobile robot. The ﬁ fth -  order model is to be used with state deﬁ ned as               =     X    x   y   ψ     v     α         172     APPLICATION OF KALMAN FILTERING    The control algorithm for the steering is computed control. The desired  closed - loop behavior is to have a natural frequency of 0.2 and a damping  ratio of 1.0. Implementation of this steering algorithm requires steering  angle,  velocity,  actual  heading  and  desired  heading. The  length  of  the  robot is 1.0 meter and the sample interval is taken to be 0.1 second. Only  the x and y coordinates of the robot location are measured.       SOLUTION 5     For the output, since it was assumed that only    x   and    y   were measured  we have     Process disturbance was assumed in the heading and velocity equa-  tions. Thus      H =    1 0 0 0 0 0 1 0 0 0            G =           0 0 0 0 0 0 1 0 0 1                  R =    0 04 . 0  0 0 04 .            Q =    0 01 . 0   0  0 01 .        The covariance matrix for the measurement noises is     For the disturbance the covariance matrix is     The matrix exponential for the discrete - time model is computed via  the inﬁ nite series truncated after the fourth term. The other terms in the  model follow accordingly.      delta  =  +  IT A  +  2  A  T 2  2  !  T 3  3  !  +  3  A  T 4  4  !        A  disc = +  I A delta  *                        ESTIMATING THE STATE OF A NONLINEAR SYSTEM     173  Actual Value Noisy Measurement Kalman Filter Estimate     10  8  6  4  2  0  -2    0  5  10  15  20  25  Time       Figure 5.7a       X Coordinate versus Time for Front - Wheel Steered Robot     and         G  disc =  delta G  * .       The  desired  heading  was  taken  to  be  the  direction  from  the  current  location to the speciﬁ ed destination,    x  , and the desired veloc- ity was taken to be    v = 0 5.   meters per second. The simulation was halted  before the ﬁ nal state had been reached.    25 ,  25  =  =  y    In the Figure    5.7a    the displacement in the x coordinate is the variable  of  interest. The  robot  motion  has  been  simulated;  therefore,  the  actual  value of x is available for the plot. Also plotted are the noisy measure- ment  of  x  and  the  estimate  of  x.  It  is  apparent  that  the  estimate  is  not  perfect;  however  it  is  considerably  better  in  accuracy  than  the  measurement.        Figure    5.7b    shows the actual, measured and estimated motion in the   y coordinate.        In Figure    5.7c    the path of the robot is shown in the x - y space.       It was assumed that there was no measurement of heading. However,  the robot model used in the Kalman Filter contained heading as a state  variable and thus leads to an estimate of it. This result points out that the    Actual Value Noisy Measurement Kalman Filter Estimate        5  10  15  20  25  Time       Figure 5.7b       Y Coordinate versus Time for Front - Wheel Steered Robot    Actual Value Kalman Filter Estimate  2.5  3  2  1  0  1.5  0.5  -0.5  -1    0  2.5  1.5  0.5  2  1  0  -0.5    -2  e  t  i  a n d r o o c   y  174  0  2  4  6  8  10  x coordinate       Figure 5.7c       Y Coordinate versus X Coordinate for Front - Wheel Steered Robot     ESTIMATING THE STATE OF A NONLINEAR SYSTEM     175     Actual Value Kalman Filter Estimate Desired Heading  1.2  1  0.8  0.6  0.4  0.2  0  -0.2    0  5  10  15  20  25  Time       Figure 5.7d      Actual, Estimated and Desired Robot Heading Angle for Front - Wheel  Steered Robot. No Heading Angle Measurement Was Available. Estimates Based on  Kalman Filter with Only  x  and  y  Measurements    ﬁ lter not only improves the estimates of measured variables, but it also  provides estimates of unmeasured variables. Figure    5.7d    shows the plots  of heading.        It  was  also  assumed  that  there  was  no  measurement  of  velocity.  Nevertheless the Kalman ﬁ lter provides an estimate of this quantity since  it  is  an  observable  state.  Plots  of  the  actual  and  estimated  velocity  are  shown in Figure    5.7e  .       Steering angle was another component of state that was not measured.  Here again the Kalman ﬁ lter provides an estimate of this quantity since  it is an observable state. Plots of the actual and estimated steering angle  are shown in Figure    5.7f  .       In  this  example,  the  system  was  of  order  ﬁ ve  and  the  number  of  outputs was two. Through the use of the Kalman ﬁ lter, estimates of all  states  were  obtained.  The  steering  and  velocity  control  algorithms  required knowledge of all the states; therefore, the estimates were used  instead  of  the  actual  values  The  preceding  plots  show  that  the  robot  is  being  steered  toward  the  destination  and  that  the  velocity  is  being  maintained  near  its  desired  value.  This  particular  control  algorithm,    Actual Value Kalman Filter Estimate        5  10  15  20  25  Time       Figure 5.7e       Actual and Estimated Robot Velocity for Front - Wheel Steered Robot    Plot of Steering of a Front-Wheel Steered Mobile Robot  Actual Value Kalman Filter Estimate  0.6  0.5  0.4  0.3  0.2  0.1  0  -0.1    0  0.12  0.1  0.08  0.06  0.04  0.02  0  -0.02  -0.04    0  5  10  15  20  25  Time       Figure 5.7f       Actual and Estimated Steering Angle for Front - Wheel Steered Robot    176   ESTIMATING THE STATE OF A NONLINEAR SYSTEM     177  computed control for a speciﬁ ed natural frequency and damping ratio,  stretches the limits of use for these non - perfect estimates of the states. A  simpler control algorithm would be more robust. An alternative would  be to add a digital compass to measure the heading.       EXAMPLE 6     Apply the Kalman Filter to the GPS initialization problem, i.e., the deter- mination of the location of the receiver given the pseudo distances from  the visible satellites to the receiver and given the locations of these satel- lites. The data on the satellite positions in ECEF coordinates at time of  transmit are as follows   X1    =    [7766188.44,  − 21960535.34, 12522838.56],   − 6629461.28,  31864.37],  X3    =    [ − 5743774.02,  X2    =    [ − 25922679.66,   − 25828319.92,  1692757.72],    − 15900725.8,  21302003.49]   and the pseudo distances are given as   d1    =    1022228206.42,  d2    =    1024096139.11, d3    =    1021729070.63   and   d4    =    1021259581.09   all in  meters.      X4    =    [ − 2786005.69,    and      SOLUTION 6     The  problem  must  be  formulated  to  suit  the  format  of  an  estimation  problem. For the initialization, one essentially freezes time and processes  the single set of measurements recursively until the estimate of receiver  position converges. Thus the positions of the satellites are treated as con- stant  during  this  iterative  process. The  position  of  the  receiver  is  also  modeled as being constant but with the inclusion of a disturbance term  to allow for its adjustment as the estimation process takes place. Recall  that one has N equations of the form   X X X X      i  −  −−    } .5 i T  ct  b  where the receiver coordinates aree      i  d        {   = − given by       X x y z T ] .    = [    Now deﬁ ne the N dimensional output vector as    YY d d d  = [  and  the  4  dimensional  state  vector  as     XX x y z ctb T variables the output equation may be written as   1 dN T ] ]  .  Using  these   2  3  cid:1   = [            YY k    h XX k            v k      +  =    Here  the  vector     v   represents  the  measurement  noise,  i.e.,  the  uncer- tainty in the pseudo distances. These errors are caused by uncertainty in     and                  178     APPLICATION OF KALMAN FILTERING  the correlation process of determining the time of arrival of signals from  the  satellites  to  the  receiver  and  also  by  uncertainty  in  the  satellites ’   positions.      Under the assumption that the receiver position is ﬁ xed and the local   clock bias is constant the process model is simply        XX k  +    1  =      XX k  +  Iw        Here  the  4x1  process  disturbance  vector     X   is  included  to  allow  for  changes  in     X    and     ctb   as  the  estimation  process  takes  place.  Thus  we have      A XX k          =   I      =    I     G k    The output matrix is given by      H XX k           = ∂  ∂  h XX         which becomes the Nx4 matrix      H XX        =  1  2  N              −     x x k 1     r k −     x x k 1     r k  cid:1  −     x x k 1     r k  1  2  N  −     x y k 1     r k −     x kk y 1     r k  cid:1  −     x y k 1     r k  1  2  N  −     x z k 1     r k −     x z k 1     r k  cid:1  −     xx z k 1     r k   − 1    −  1     − 1     cid:1         R, the covariance matrix for the measurement noise is NxN and Q, the   covariance matrix for the process disturbance is 4x4.      The computer code for accomplishing this estimation process follows   below.      N  =  10;   c  =  3 * 10 ^ 8;                                          %speed of light      x1  =  7766188.44; y1  =   - 21960535.34;  z1  =  12522838.56;   X1  =  [x1 y1 z1] ′ ;       ESTIMATING THE STATE OF A NONLINEAR SYSTEM     179   x2  =   - 25922679.66; y2  =   - 6629461.28;  z2  =  31864.37;   X2  =  [x2 y2 z2] ′ ;      x3  =   - 5743774.02; y3  =   - 25828319.92;  z3  =  1692757.72;   X3  =  [x3 y3 z3] ′ ;      x4  =   - 2786005.69; y4  =   - 15900725.8;  z4  =  21302003.49;   X4  =  [x4 y4 z4] ′ ;                           % locations of visible  satellites      d1  =  1022228206.42; d2  =  1024096139.11;  d3  =  1021729070.63; d4  =  1021259581.09;   d  =  [d1 d2 d3 d4] ′ ;                  %pseudo distances, visible  satellites to receiver      x  =  0;         y  =  0;         z  =  0;         ctb  =  0;               %initial  guess for receiver location and local clock error      X  =  [x y z] ′ ;                                                      % receiver position  vector   XX  =  [X ′  ctb] ′ ;                                                   % state vector   A  =  eye 4 ;                                                            %propogation of X and  ctb   P  =  100000  *  eye 4 ;                           %cov of X and ctb   Q  =  10000  *  eye 4 ;                              %process disturbance  on X and ctb   R  =  10  *  eye 4 ;                                          % measurement noise in  di ′ s      NxN      for j  =  1:N                  xx j   =  [1 0 0]  *  X;               yy j   =  [0 1 0]  *  X;               zz j   =  [0 0 1]  *  X;               ctbb j   =  ctb;                                 %for plotting                  r1  =    X  -  X1  ′   *   X  -  X1   ^ .5;               r2  =    X  -  X2  ′   *   X  -  X2   ^ .5;               r3  =    X  -  X3  ′   *   X  -  X3   ^ .5;      APPLICATION OF KALMAN FILTERING  180               r4  =    X  -  X4  ′   *   X  -  X4   ^ .5;                  h  =  [r1  -  ctb r2  -  ctb r3  -  ctb r4  -  ctb] ′ ;                            %h x , di is the ith output                  L j   =   r1    −    ctb  -  d1  ^ 2  +   r2    −    ctb  - d2  ^ 2  +      r3    −    ctb  -  d3  ^ 2  +   r4    −    ctb  -  d4  ^ 2;      % indication of convergence of the process               H1  =  [ 1   r1   *   X  -  X1  ′ ,       -  1];                                  %partial of h wrt x, N rows, 4 columns               H2  =  [ 1   r2   *   X  -  X2  ′ ,       -  1];               H3  =  [ 1   r3   *   X  -  X3  ′ ,       -  1];               H4  =  [ 1   r4   *   X  -  X4  ′ ,       -  1];               H  =  [H1; H2; H3; H4];                                                                         %linearized output matrix               A  =  eye 4 ;            %state transition matrix from  linearization                  P  =  A  *  P  *  A ′   +  Q;               K  =  P  *  H ′   *  inv H  *  P  *  H ′   +  R ;               P  =   eye 4     −    K  *  H   *  P;               residual  =  [d1  -  h 1  d2  -  h 2  d3  -  h 3  d4                 XX  =  [X;      ctb];               XX  =  XX  +  K  *   residual ;               X  =  [eye 3 ; 0 0 0] ′   *  XX;               ctb  =  [0 0 0 1]  *  XX;      end      ﬁ gure   plot L       ﬁ gure   plot xx    hold on   plot yy    hold on   plot zz      -  h 4 ] ′ ;     In Figures    5.8a    and    5.8b    are shown plots demonstrating the conver- gence of the estimates to the correct values. Note the values used for the       x coordinate y coordinate z coordinate  2  4  6  8  10  12  14  16  18  20  Number of Iterations       Figure  5.8a   P    =    100,000 * eye 4 , Q    =    10,000 * eye 4 , R    =    10 * eye 4        Plot  Demonstrating  the  Convergence  of  the  Coordinate  Estimates   x 106  x 1016  6  4  2  0  -2  -4  -6    0  3.5  4  3  2  1  2.5  1.5  0.5  0  0  2  4  6  8  10  12  14  16  18  20  Number of Iterations       Figure  5.8b   P    =    100,000 * eye 4 , Q    =    10,000 * eye 4 , R    =    10 * eye 4        Plot  Demonstrating   the  Convergence  of   the  Error  Function   181   182     APPLICATION OF KALMAN FILTERING  initial covariance of state error and the covariances for the process dis- turbance and measurement noise.        Case 1 .   P      =      100,000   * eye   4 ,   Q      =      10,000   * eye   4 ,   R      =      10   * eye   4    , , 2 430 745 09594  .  −  ′ 4 702 345 11359 3 546 568 70600   , ]  ,  ,  .  ,  ,  .           X = − , [    tb = −3 33421  .        Note the slight overshoot in each of the coordinate estimates.      Case 2     This example can be used to illustrate the impact of the values used for  the  covariances. To  illustrate  this  point,  the  process  is  re - run  with  the  initial  covariance,  P,  reduced  by  a  factor  of  100,  i.e.,    P     =    1,000  * eye  4 ,   Q     =    10,000  * eye  4 ,  R     =    10  * eye  4  . In Figures    5.9a    and    5.9b    are shown  plots  demonstrating  convergence  of  the  estimates  and  the  error  function.        x coordinate y coordinate z coordinate  x 106  4  3  2  1  0  -1  -2  -3  -4  -5    0  2  4  6  8  10  12  14  16  18  20  Number of Iterations       Figure  5.9a   P    =    1,000 * eye 4 , Q    =    10,000 * eye 4 , R    =    10 * eye 4        Plot  Demonstrating  the  Convergence  of  the  Coordinate  Estimates    ESTIMATING THE STATE OF A NONLINEAR SYSTEM     183  x 1016  3.5  4  3  2  1  2.5  1.5  0.5  0  0  2  4  6  8  10  12  14  16  18  20  Number of Iterations       Figure  5.9b   P    =    1,000 * eye 4 , Q    =    10,000 * eye 4 , R    =    10 * eye 4        Plot  Demonstrating   the  Convergence  of   the  Error  Function            , , 2 430 745 09594  .  ,  −  ′ 4 702 345 11360 3 546 568 70600   , ]  ,  ,  ,  .  ,  .     X = − [    tb = −3 334215  .        Note  the  absence  of  the  overshoot  in  the  coordinate  estimates. Thus  the transient portion of the result is quite different from Case 1, but the  ﬁ nal values are the same.      Case 3     This example can be used to illustrate the importance of selecting appro- priate  values  of  R.  Here  it  is  increased  by   1000:  P    =    1,000  * eye  4 ,  Q    =    100  * eye  4 ,  R    =    10,000  * eye  4  .  In  Figures     5.10a     and     5.10b     are  shown plots of the results.        X =  [ , 173 007 421 998 391 056 363 126  ,  .  ,  ,  .  ,  ,  −  ′ 501 754 522 949    ]  ,  ,  .    The transient portion of the solutions is different from Case 1 or Case  2 and also after 10 iterations the solution has not converged to the proper  solution.        x coordinate y coordinate z coordinate  2  4  6  8  10  12  14  16  18  20  Number of Iterations       Figure 5.10a   P    =    1,000 * eye 4 , Q    =    100 * eye 4 , R    =    10,000 * eye 4        Plot Demonstrating Lack of Convergence of the Coordinate Estimates   x 107  x 1016  4  3  2  1  0  -1  -2    0  3.5  4  3  2  1  2.5  1.5  0.5  0  0  2  4  6  8  10  12  14  16  18  20  Number of Iterations     Plot  Demonstrating  Lack  of  Convergence  of  Error  Function        Figure  5.10b   P    =    1,000 * eye 4 , Q    =    100 * eye 4 , R    =    10,000 * eye 4    184   EXERCISES     185    These three executions of the Kalman Filter using different values for  the various covariance matrix demonstrate how important these matrices  are to the solutions. The proper adjustment of these covariance matrices  is sometimes referred to as tuning.      General conclusions are that if Q is too small, there is slow conver- gence; however, large values for Q are okay. Since Q represents process  disturbances, if it is made small, the estimate places its trust in the propa- gation equation, which here is the identity matrix and the state estimate  tends to stay near its initial value longer.      Large values of R cause divergence, while setting R to zero is okay.  The main source of information for solving the geo - location problem is  the set of pseudo - distances to the visible satellites and the coordinates of  these satellites. A large value of R corresponds to these pseudo - distance  measurements and the satellite coordinates being unreliable, causing the  estimation process to fail.      Regarding the covariance of the initial estimate, P, the ﬁ nal value of  the state estimate is independent of the initial value of P, but using a value  for P that is very small causes slower convergence. Starting with a small  value for P is equivalent to telling the ﬁ lter that the initial estimate of the  state is a good one. This causes it to be slow to change even in the pres- ence  of  measurements  that  yield  large  residuals.  A  large  value  for  P  works better.      For tracking a moving target, the initialization would provide a very  good  starting  point  for  the  next  solution.  Also,  one  would  model  the  receiver with a different state equation to take into account its motion.       The Kalman Filter which was developed approximately half a century  ago  continues  to  play  an  important  role  in  estimating  the  state  of  dynamic systems with limited and noisy measurements. Further exam- ples of its application will appear in the chapters ahead.           1 .         A        A simple scalar dynamic system has the following discrete - time    EXERCISES   model:  with output             x k  +    1  =  0 5 .      x k          u k w k     +  +         y k  =      x k  +      v k       186     APPLICATION OF KALMAN FILTERING   Let the initial value of the state be     x   0  0=    and let the input signal be         u k  =  ; 1  k  =  , 0 1  20…      The standard deviation for the process disturbance is 0.1 and for the  measurement noise it is also 0.1.    Develop a simulation for this system and plot the state and the   output versus  k .                B         Next  develop  a  Kalman  Filter  for  estimating  the  state  of  this  system. Run the ﬁ lter in parallel with the simulation. Plot the  state, the output and the estimate of state all versus  k . Comment  on  the  accuracy  of  the  state  estimate  as  compared  with  the  output measurement.     Repeat  part   B   but  now  let  the  standard  deviation  for  the  measurement noise be 0.5. Again comment on the accuracy of  the state estimate as compared with the output measurement.           2 .         A        A simple scalar dynamic system with feedback has the follow-              C       ing discrete - time model:       x k  +    1  =  . 1 1      x k          u k w k     +  +  with output   Let the initial value of the state be         y k  =      x k  +      v k         x   0  3=      The objective is to use feedback to stabilize the system utilizing a  control signal given by         u k  = −  . 0 6     ; y k k  =  0 1 ,  20…      The standard deviation for the process disturbance is 0.1 and for the  measurement noise it is also 0.1.    Develop a simulation for this system and plot the state versus  k .       B        Next  develop  a  Kalman  Filter  to  operate  in  parallel  with  the  system simulation and change the control algorithm to         u k  = −  0.6      ;  ˆ x k k k  =  0,1  …    20   Compare the behavior here with that obtained in part A.                              EXERCISES     187      3 .         A        A second - order dynamic system with feedback has the follow-  ing discrete - time model:      T x k 1  1     x k 2    x k 1   x k 2   =    1   1    + +         1 0       with output     +  T   2    T  2         u k  +  T   2 2   T         w k     = [  ]         y k  1 0      x k      v k      +   Take  T  to be 0.1 sec. The standard deviation for the process distur- bance  is  0.1  and  for  the  measurement  noise  it  is  also  0.1.  Let  the  initial value of the state be         =          x 1 x  0     0      0 0    2   The  objective  is  to  control  the  speed  of  the  system  using  simple  velocity feedback, One example of an idealized control signal would  be given by         u k  =    5 10  −  x k k 2        =  , 0 1  …    20  −  −  T  1  ]     x k 1      x k 1   Since   x k2    is not measurable, this scheme is not feasible. Approximate    x k2    by the expression   [ . Develop a simulation  for  this  system  and  plot  the  second  component  of  state   velocity   versus  k .       B        Now develop a Kalman Filter to run in parallel with the system.  Simulate  and  this  time  use  the  estimated  value  for  velocity  in  the  … .  Plot  the  control  algorithm,  i.e.,    u k second  component  of  state  versus   k   and  compare  the  results  with  those of part A.        ˆ x k k k 2      10 5         0,1  20  =  −  =      4 .         A        Assume that for the front - wheel steered robot positions x and  y and also heading are all assumed measurable. Show the behavior  in  going  from  the  point  0,0  to  the  point  10,10. The  initial  heading  angle is zero. There is no measurement noise and no process distur- bance. Take as the desired heading     ψdes   and use as the control algorithm  − tan 1  = −  for abs Gain     *   ψ ψ π  −  ≤      des    4       x des y des *  Gain     x y  − − ψ ψ    −  des     α  =  and     α π ψ ψ =    sgn   −  des  4  for abs Gain     *   ψ ψ π        −  >  des  4                         188     APPLICATION OF KALMAN FILTERING   Use the Euler method for simulating the system and take the sample  interval to be 0.1 sec. The length of the robot is 2 meters. Take 0.5  as the Gain. Let the velocity be one meter per second.       B        Now assume there is no heading measurement. Use ﬁ nite dif- ferencing of x and y at each sampling instant to approximate heading,  i.e.,   ψ≈ −   and control the robot with the same  tan  − 1         x k     y k  − −    x k   y k  − −       1   1  algorithm as in the 1 st  part. Assume perfect measurements on x and  y and assume zero process disturbance.       C        Repeat the second part except now include a random process  disturbance  in  the  steering  angle  for  the  simulation  and  include  random noise in the x and y measurements. See what type of per- formance the ﬁ nite differencing method for heading determination  provides in the presence of noise. Let the standard deviation of the  disturbance  be  0.05  radians  and  let  the  standard  deviation  of  the  measurement noise be 0.2 meters for x and y.       D        Now re - solve the problem of control with no heading measure- ment, this time by implementing a Kalman Filter. Your simulation  should include the process disturbance and the measurement noise  as in the third part. Since the process disturbance is not measurable  directly,  do  not  include  it  in  the  prediction  equation  of  the  ﬁ lter.  Compare the behavior obtained in part three and part four, and see  if either approaches the ideal behavior obtained in part one.   Discuss your results.            5 .    A  plant  model  for  a  steel - bending  operation   is  given  by            cid:1  x 1  cid:1  x 2  cid:1  x  3          = −     0 1 8 . 0   0 1  − 7 1 1 . .    0 0        x 1 x 2 x  3        +        0 0 0 9 .        +  u        0 0 0        w with output            y 1 y 2     =    1 0  0 0     0 1        x 1 x 2 x  3        +    v 1 v 2         Here  the    x1  represents  curvature  of  the  bent  steel,    x2  represents  curvature  rate  and    x3  represents  the  transverse  roller  position.  Develop a discrete - time state model with a sampling interval of 0.1  second. Next develop a Kalman Filter for estimating all the states.  This is an example where the Kalman Filter can be used to estimate  the components of state that are not directly measurable and also    REFERENCES     189  to  give  improved  estimates  of  those  that  are  measurable.  Let  the  covariances for   w,   v1 and   v2 all be taken as 0.1. Let the input signal  be a unit step. Simulate the system along with the Kalman Filter and  plot  the  actual  states,  the  measurements  of  states  1  and  3  and  all  three state estimates.         6 .    For  the  exercise  described  above,  repeat  everything  except  now  reference curvature x .  Let  the  reference  curvature  be  a  step  3  implement  the  control  according  to    u   − − . 28 5 function.        = 26 9.  . 9 5  23  x 1  −  x  2    REFERENCES   NJ ,  1991 .        Brogan ,  W. L.    “  Modern Control Theory  ” ,  Prentice Hall ,  Upper Saddle River,       Robert    Grover    Brown  and    Patrick    Y.C.     Hwang:   “ Introduction  to  Random  Signals and Applied Kalman Filtering ” , J .  Wiley and Sons ,  New York, N.Y .,   3rd edition ,  1996 .        Bryson ,  A. E.   and   Ho ,  Y - C  ,   “  Applied Optimal Control: Optimization, Estimation   and Control  ” , Blaisdell, Waltham ,  MA   1969 .        Cook ,   G.  ,  and    Dawson ,   D.  E.  ,   “  Optimum  Constant - Gain  Filters , ”    IEEE  Transactions on Industrial Electronics and Control Instrumentation , Vol  21 ,  No.  3   August  1974  , pp.  159  –  63 .        Cook ,   G.    and    Lee ,   Y.  C.     “  Use  of  Kalman  Filtering  for  Data  Resolution  Enhancement  ” ,  IEEE Transactions on Industrial Electronics and Control  Instrumentation ”  , Vol  22 , No.  4   November  1975  , pp  497  –  500 .        Gelb ,  A.  ,  ed.,   Applied  Optimal  Estimation .   Cambridge,  Massachusetts :   MIT       E.    Kiriy    and    M.    Buehler  ,   Three - state  Extended  Kalman  Filter  for  Mobile   Press , : 1974 .    Robot Localization    2002  .     Hill ,  NY, NY ,  1969 .     J. Wiley  &  Sons ,  2008 .        Meditch ,  J. S.  ,  “  Stochastic Optimal Linear Estimation and Control  ” ,  McGraw -      Mohinder S.   Grewal   and   Angus   P.    Andrews  “ Kalman Filtering  ” ,  Third Edition ,       S.   Han  ,   Q.   Zhang  ,   H.   Noh  ,  “  Kalman Filtering of DGPS Positions for a Parallel  Tracking Application  ” ,  Transactions of the ASAE.  Vol.  45   3  :  553  –  559 ,  2002 .        Haykin ,  S.     2002  .  Adaptive Filter Theory .  Prentice Hall .       Julier ,  S.J.  ,   Uhlmann ,  J.K.  ,  “  A new extension of the Kalman ﬁ lter to nonlinear  systems  ” .   Int.  Symp.  Aerospace Defense  Sensing,  Simul.  and  Controls  3 .   1997 .        S.    Roumeliotis    and    G.    Bekey  ,  “  Bayesian  estimation  and  Kalman  ﬁ ltering: A  uniﬁ ed  framework  for  mobile  robot  localization  ” .   Proceedings  of  IEEE    190     APPLICATION OF KALMAN FILTERING  International  Conference  on  Robotics  and  Automation .   San  Francisco ,   California    2000  .        Sage ,   A.  P.    and    Melsa ,   J.  L.     “  Estimation  Theory  with  Applications  to   Communication and Control  ” , McGraw  –  Hill Inc,  1971 .        Trostmann ,  E.  ,   Hansen ,  N. E.  , and   Cook ,  G.  ,   “ General Scheme for Automatic  Control  of  Continuous  Bending  of  Beams, ”   Transactions  of  the  ASME,  Journal  of  Dynamic  Systems ,   Measurement  and  Control , Vol.   104 ,  No.   2    June  1982  , pp.  173  –  179 .                6   REMOTE SENSING           6.0      INTRODUCTION    This chapter is devoted to the acquisition of images via sensors mounted  on a mobile robot. Particular attention is given to projecting the sensor  ﬁ eld of view onto a surface  such as the ground  and then converting  a speciﬁ c pixel coordinate  for example the coordinates of a detected  object  of  interest   of  this  ﬁ eld  into  the  actual  ground  coordinates.  Pointing at a detected target for ranging is also treated.       6.1      CAMERA TYPE SENSORS    In this section, we analyze camera type sensors and address the task of  converting sensor frame coordinates of objects to their representation  in  other  frames  such  as  vehicle  coordinates  and  earth  coordinates.  Figure   6.1   illustrates  these  multiple  frames.  The  analysis  applies  to  infrared  cameras  and  also  to  visible  wavelength  cameras.  The  only  requirement is that it be a pin - hole type digital camera with the ﬁ eld  of  view  divided  into  pixels.  In  order  to  proceed  we  ﬁ rst  deﬁ ne  some  coordinate frames. The sensor coordinate frame has   y aligned with the   Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  191   VZ  Vehicle Frame set  at DGPS Antenna   SZ  VX  Pan & Tilt Unit,  Sensor Frame  SY       Figure 6.1       Sensor Bearing Mobile Robot    192     REMOTE SENSING  EZ  Earth Frame   EY  EX  Focal Plane   x   L   Pin Hole   L  VY  z  SX  x  Target Point  y  z   Virtual Focal Plane        Figure 6.2       Diagram of Pinhole Camera    longitudinal axis of the camera, i.e., the camera boresight,   x pointing to  the right and   z pointing upward. We seek to compute the direction of  the  vector  pointing  at  the  target,  which  appears  in  the  i th   row  and  j th   column of the pixel array. See Figure  6.2 .      By expressing the pixel coordinates in the virtual focal plane rather  than in the actual focal plane, one eliminates the need to think in terms  of reversed coordinate directions.    The coordinates shown in Figure  6.3  are the coordinates of the center  of the pixel in question. The rows,   i, and the columns,   j, are numbered  in the same way as rows and columns of a matrix, i.e., beginning at the  upper left corner. We can compute   y and   z as follows. Assume that the  pixel array has N rows and M columns. The pixel size is   ∆w wide by   ∆h  high. The focal length is   L. The dimensions of the ﬁ eld of view are given  by   VFOV and   HFOV. Here   VFOV   2 and   HFOV   2 are each half of    CAMERA TYPE SENSORS     193  z   2  − Mj 2  ∆− 1 w  j   i   *  N  ∆−+ 21 i h 2 x        Figure 6.3   Virtual Focal Plane       Computing X - Z Coordinates for ijth Pixel in a Digital Camera Using the   the respective sensor vertical ﬁ eld of view and horizontal ﬁ eld of view.  By setting   j = 1 and using trigonometric arguments we have   Now  noting  that  one  may  express    x  ∆   and    y L=   we  w  −  M     [           1 2 L  ]  ∆ w  =     tan  HFOV     2        =  1     L    tan [   M  HFOV ]   − 1 2       2   ∆      w − j M 2  =  2     x y     =  x L     =  [     − j M  2  ]   1 2 ∆ w     − L  −  1     or              have  or  2  =     x L     1  − j M − M  − 1  tan   HFOV      2  =  xNRO         6.1     where   xNRO is deﬁ ned as the normalized horizontal readout. Likewise  in the  z  coordinate setting   j = 1 yields   or  or                    194     REMOTE SENSING  −  N     [           1 2 L  ]  ∆ h  =     tan  VFOV     2        =  1     L    tan [   N  VFOV ]   − 1 2    2   ∆    h      Here noting that   z  ∆  and again using   y L= h  N  =  i  + −1 2 2     z y z L        =  =  [     N  + − 1 2 L     i     ] 2 ∆ h     N  =     z L     i  + − 1 2 − 1 N  tan   VFOV      2  =  zNRO         6.2     where   zNRO is deﬁ ned as the normalized vertical readout. We then have  as azimuth     ψray  =  − 1  tan  − 1  tan  x       NRO  − = − x L    6.3     and for elevation we have     θray  =  − 1  tan  z + 2  L x  2  =  − 1  tan    z L + 2 2   x L  1  =  − 1  tan  z NRO + 2 1 x NRO         6.4       The steps are to ﬁ rst determine the pixel coordinates of the object  of  interest.  Then    xNRO  is  calculated  from  equation    6.1    and    zNRO  is  calculated  from  equation    6.2  .  Equations    6.3    and    6.4    then  yield  azimuth and elevation of the ray in camera coordinates.    Two samples of IR images are shown in Figure  6.4 . The vertical ﬁ eld  of view of the camera was approximately 12 degrees and the horizontal  ﬁ eld  of  view  was  approximately  16  degrees. The  corresponding  pixel  dimensions were 240 by 320. The image on the left is a section of earth  that  has  been  recently  disturbed  and  is  still  loose. The  image  on  the  right is a road with markers placed along the right side. Through the  use of a software tool, one can click on an object of interest within an  image such as these and obtain its pixel coordinates,  i  and  j . From these,  one can use the equations just developed and determine the azimuth    CAMERA TYPE SENSORS     195       Figure 6.4       Sample IR Images    and  elevation   i.e.,  the  direction  in  camera  coordinates   of  the  ray  pointing at this object.      It is now desired to express the ground location of the tip of this ray  that passes through the  ij th pixel in sensor coordinates, i.e., the point  where this ray intersects the ground. The coordinates are given by                  x y z  snsor coords  =        θ − cos sin r ij ray θ cos cos r ray ij r sinθ  ray  ij  ray  ψ ψ aay  r               6.5       It is seen that the range to the pixel in question,   rij, is required for  this calculation. If the terrain in front of the vehicle is planar and this  plane extends underneath the vehicle, and the boresight of the camera  is parallel to the earth  zero pitch for the camera , then the range   rij  may be calculated by solving the equation  sinθ = −  ray     r ij  H        r ij  = −   sinθ      H  ray    6.6     where   H is the height of the camera above the ground. Knowing   rij, one  can now compute the  x  and  y  coordinates of the object in camera coor- dinates. These can then be converted to vehicle coordinates and ﬁ nally  to earth coordinates.    The above is the simplest case possible and is presented to introduce  the idea of computing the intersection of the ray with the ground as a            or   196     REMOTE SENSING  means of determining its length. The more complex case of arbitrary  vehicle  orientation  is  now  addressed.  Here  the  vehicle  may  have  nonzero pitch and roll as well as yaw, and the camera may be mounted  with arbitrary orientation as well. In this case we must use a rotation  matrix and express the ray in vehicle axes and ﬁ nally earth axes before  solving for  r .    The vector expressed in camera coordinates is given as        6.7a          6.7b                  X Y Z                  X Y Z           RayCamCoords  Ray Cam Coords  Ray inCam Coords  where  r  is yet to be determined. In vehicle coordinates this becomes  =  − cos sin r  cos cos r  θ    θ ψ   θ ψ    sin  r  =  rR  Cam Veh  −  ˆ X ˆ Y ˆ Z                =  r         ˆ X ˆ Y ˆ Z         +        X Y Z         ˆ X  ˆ  Y  ˆ Z          RayinVeh Coords  Ray  iinCam Coords  CamOrigin Veh Coords   Finally converting this to earth coordinates we have  X Y Z                  Ray Earth Coords  +  R  −  Veh Earth        X Y Z        =  rR  Veh Earth Cam Veh  −  −  R  Ray Cam Coords                6.7c   +        X Y Z        CamOrigin Veh Co  oords  VehOrigin EarthCoords  ,   Setting the z - component of the ray in earth coordinates equal to the  z - component of the vehicle origin in earth coordinates minus the height  of the vehicle origin above the ground expressed in earth coordinates,  one can solve for r, the length of the vector.  Note that the placement  of the GPS antenna on the vehicle is deﬁ ned as the vehicle origin. Thus  the height of the vehicle origin is the same as the height of the GPS  antenna.    CAMERA TYPE SENSORS     197  [  ] 0 0 1  r  R  Veh Earth Cam Veh  −  −  R         ˆ X ˆ Y ˆ Z         RayCanCoords                  X Y Z        +  [[  ] 0 0 1  −R  Veh Earth  +[  ] 0 0 1  = [  ] 0 0 1        X Y Z        VehOrigin E  ,  aarthCoords  CamOrigin VehCoords  ,  −[  ] 0 0 1  R  −  Veh Earth   X  Y    Z            VehOrigin EarthCoords 0 0 H         6.8a   ,  ant             This simpliﬁ es to  [  ] 0 0 1  r  R  −  Veh Earth Cam Veh  −  R         ˆ X ˆ Y ˆ Z                     X Y Z             RayCanCoords              = −  [  0 0  ] 11  −R  Veh Earth  +        0 0 H  ant               6.8b   CamOrigin VehCoords  ,  which is a scalar equation in r that one can solve by division. Using this  value for r, one then can express the location of an object of interest  in  any  of  the  coordinate  systems  using  the  equations  above. This  all  assumes  that  the  elevation  of  the  earth  at  the  point  where  the  ray  intersects the earth is the same as that under the vehicle or robot.      EXAMPLE 1     Consider a camera with pixel dimensions    N = 360  and    M = 480 . Take as  the ﬁ eld of view,    HFOV = π  4  and    VFOV = π  6  The camera longitudi- nal axis or boresight is at a pitch angle of    θ π= −   8  and the height of the  camera is 2     m. Determine the footprint, i.e., the intersection of the ﬁ eld  of view with the ground.       198     REMOTE SENSING     SOLUTION 1    % Every 30th pixel is plotted via the computer  program with code given below.   N  =  480; M  =  360;   HFOV  =  pi   4; VFOV  =  pi   6;   H  =  2;                              % height of camera      thetac  =   - pi   8;   %camera boresight pitch angle  with respect to robot   % camera yaw and roll wrt robot assumed to be zero.   sir  =  0;                     % yaw of robot wrt earth   thetar  =  0;            % pitch of robot wrt earth   phir  =  0;                  % roll of robot wrt earth      %camera position wrt robot coordinate center  which coincided with gps antenna   xc  =  0; yc  =  0.5; zc  =   - 0.25;   Xc  =  [xc yc zc] ′ ;      %robot position in earth coordinates, for  simplicity taken to be zero here   xe  =  0; ye  =  0; ze  =  0;   Xr  =  [xe ye ze] ′ ;      %rotation camera wrt robot   Royc  =  [1 0 0; 0 1 0; 0 0 1];   Ropc  =  [1 0 0; 0 cos thetac        - sin thetac ;     Rorc  =  [1 0 0; 0 1 0; 0 0 1];   Roc  =  Royc  *  Ropc  *  Rorc;      %rotation robot wrt earth   Royv  =  [cos sir   - sin sir  0; sin sir  cos sir  0;     Ropv  =  [1 0 0; 0 cos thetar   - sin thetar ;     Rorv  =  [ cos phir  0 sin phir ; 0 1 0;  -  sin phir      Rov  =  Royv  *  Ropv  *  Rorv;   ﬁ gure   for ii  =  1:16   0 0 1];  0 sin thetar  cos thetar ];  0 cos phir ];   0 sin thetac  cos thetac ];    CAMERA TYPE SENSORS     199  tan HFOV   2 ;  tan VFOV   2 ;  corresponding to pixel                for jj  =  1:12                           i  =  ii  *  30; j  =  jj  *  30;                           xn  =    2  *  j  –  M  -  1     M  -  1    *                              zn  =    N  +  1  -  2  *  i     N  -  1    *                              si  =   - atan xn ;                                    %azimuth of ray                             theta  =  atan zn   sqrt 1  +  yn ^ 2  ;                         %elevation of ray corresponding to pixel                           xhat  =   - cos theta   *  sin si ;                        %unit    vector in sensor frame                           yhat  =  cos theta   *  cos si ;                           zhat  =  sin theta ;                           Xhat  =  [xhat yhat zhat] ′ ;                           num  =   - [0 0 1]  *  Rov  *   Xc  +  [0 0 H] ′  ;                         %range computation                           denom  =  [0 0 1]  *  Rov  *  Roc  *  Xhat;                           range  =  num   denom;                           Xte  =  range  *  Rov  *  Roc  *  Xhat  +  Rov  *  Xc  +  Xr;                         %target in earth coords                           xx  =  Xte 1 ;                           yy  =  Xte 2 ;                           plot xx, yy,  ′  *  ′                 % plots ground    coordinates for each pixel computed                           hold on               end   end      The projection of the set of pixels onto the earth is shown in Figure    6.5  .          EXAMPLE 2      SOLUTION 2     If the vehicle is tilted, this will be reﬂ ected in the vehicle to earth rotation.  Repeat the previous example, but now suppose the vehicle itself is pitched  downward by an amount    π  20  radians or 9 degrees. Determine the foot- print for this case.        The ﬁ eld of view is computed using the code of the previous example.  Figure     6.6     shows  that  the  farthest  extremity  of  the  ﬁ eld  of  view  has    6  4  2  0  -2  -4  -6  2  3  2  1  0  -1  -2  4  6  8  10  12  14       Figure 6.5       Camera Footprint with Vehicle at Zero Yaw, Pitch and Roll    -3 1.5  2  2.5  3  3.5  4  4.5  5  5.5  6     Camera Footprint with Vehicle at Zero Yaw and Roll, but Pitched Down        Figure 6.6     π  20 Radians    200   shortened from 11.5     m to 5.5     m as a result of the vehicle being pitched  downward.        CAMERA TYPE SENSORS     201    If  the  vehicle  is  rolled,  then  the  transformation  performed  on  the  ray  before computing range must also include the roll. Consider the previous  example but now with the vehicle rolled positively  left side up  by an  amount    π 18  radians or 10 degrees.         EXAMPLE 3      SOLUTION 3     In Figure    6.7    is shown the camera footprint for the case where the vehicle  is rolled in the positive direction by    π 18  radians.         By  observing  how  the  ﬁ elds  of  view  for  the  camera - type  sensors  change so drastically with vehicle attitude, it is apparent that accurate   6  5  4  3  2  1  0  -1  -2  -3  -4  2  4  6  8  10  12  14  16  18  20       Figure 6.7   Radians       Camera Footprint with Vehicle at Zero Yaw and Pitch, but Rolled by   π 18    202     REMOTE SENSING       Figure 6.8   to Target       Illustration of Single - Camera Algorithm for Determining Length of Ray   Target   instrumentation for this purpose is absolutely necessary. Yaw, pitch and  roll all have a major impact on accurate geo - location. Possible instru- mentation includes an array of GPS antennas or an inertial measure- ment unit. An inclinometer could be used subject to the limitation that  there be no vehicle acceleration during sensing. Terrain elevation also  impacts geo - location accuracy. Figure  6.8  illustrates the geometry for  determining major errors.      Since the intersection of ray with the ground is computed based on  the angle of the ray, the height of the antenna and the elevation of the  earth, this intersection depends on vehicle attitude  measured , vehicle  position   measured   and  terrain  elevation   either  assumed  ﬂ at  or  obtained from range scan .    The  sensitivity  functions  when  using  the  algorithm  based  on  the   intersection of the ray with the ground are           σ  2 dnrng  =    2  r  H  antenna  2     σ 2 pitch  +    r  H  antenna  2     σ 2 elevatio  nn          σ  2 crsrng  =  σ 2 2 heading  r         6.9a       6.9b      2  2   While the coefﬁ cient of   σpitch   is typically larger than the coefﬁ cient  of   σelevation , the error in elevation itself is much larger than the error in  pitch causing both terms to contribute signiﬁ cantly. Thus very precise  position and orientation information is required for accurate determi- nation of the location of a detected object.       6.2      STEREO VISION    Because  of  the  potential  errors  when  using  a  single  camera,  stereo  vision is also considered as a means of ground registration of objects  of interest. Here we use a pair of cameras in a coordinated way. The    STEREO VISION     203  Pan and Tilt  Platform  Camera 1  w  y  r1  r2  ψ  1  ψ  2  Target  Camera 2 x        Figure 6.9       Setup for Using Stereo Vision for Geo - Locating Objects of Interest    diagram in Figure  6.9  illustrates the setup. Both cameras are mounted  rigidly on a single platform and spaced apart by a known quantity. This  platform could be attached to a pan and tilt unit as a means of scanning  with the cameras.      Once an object of interest is identiﬁ ed, one determines the angle of  the  respective  rays  from  each  camera  to  the  object.  These  include  angles of the rays in camera coordinates and also the ﬁ xed angles of  the  cameras  with  respect  to  the  coordinate  frame  of  the  mounting  platform. Both azimuth and elevation are needed. Next, one uses the  law of sines to determine the distances from each camera to the point  of intersection of the rays. The equations are   With this information, one can compute the coordinates of the object  in the frame of the camera mounting platform. In terms of   r1 these are  =     r W1  ψ cos 2 + ψ ψ  cos 2  1  θ      1  sin   =     r W2  ψ cos 1 ψ ψ +  cos 2  1  θ      2  sin   = −     x  +  r 1  sin  cosψ θ      1  1  W 2     y r= 1  cos  cosψ θ      1  1     z r= 1  sinθ      1    6.10a       6.10b        6.11a       6.11b       6.11c                     and  and   204     REMOTE SENSING   In terms of   r2 these become                       and  and  and  =     x  W 2    y r= 2  −  r 2  sin  cosψ θ      2  2  cos  cosψ θ      2  2     z r= 2  sinθ      2    6.12a       6.12b       6.12c       Clearly, both solutions should place the object at the same point. The   error variances when using stereo vision are     σ  2 downrange  =    2  r W  2       σ σ 2 2 ψ ψ 1 2  +    +    r W  2     σ 2 W         6.13a        σ  2 crossrange  =  σ 2 2 heading  r         6.13b       Since W is precisely known, this reduces to     σ  2 downrange  =    2  r W  2       σ σψ 2 ψ 2  +  2  1           6.14a        σ  2 crossrange  =  σ 2 2 heading  r         6.14b       Here   σψ is comprised primarily of the uncertainty in camera mount- ing angles and is expected to be extremely small. Further, the effect of  terrain is eliminated, signiﬁ cantly reducing the downrange errors.      EXAMPLE 4     Two  cameras  for  stereo  imaging  are  mounted  on  a  platform  2  meters  apart. An  object  is  detected  in  each  camera  and  determined  to  be  the  same object. For camera 1 based on the pixel coordinates and the camera  mounting angles, the azimuth angle of the ray to the target is determined    STEREO VISION     205  to be 0.3 radians to the right. The pitch angle of the ray is  − 0.2383 radians.  For camera 2 based on the pixel coordinates and the camera mounting  angles, the azimuth angle for the ray to the target is 0.2 radians to the left  and the pitch angle is  − 0.2466. Determine the location of the object with  respect to the camera mounting platform.         SOLUTION 4     Using the formula for    r1  given above,   =     r W1  ψ cos 2     ψ ψ + 2  1  sin  θ    1  cos     r1  =  2    0 2 . cos   0 3 0 2 cos .  +    −      sin .    =  4 21 .      0 2383 .    Thus in the camera mounting platform coordinates we obtain   = −     x  +  r 1  sin  ψ θ 1  cos  1  =  0 21 .     W 2     y r=  1  cos  ψ θ 1  cos  1  =  3 91 .       Similarly solving for    r2  via the formula for it      z r=  1  sinθ  1  = − 1      =     r W2  ψ cos 1     + ψ ψ 2  1  sin  θ    2  cos     r2  =  2    cos  cos    0 3 . −     0 2466 .    =     0 5 sin .  4 11 .        Now using the expressions involving    r2  gives                         yields    and    yields    206     REMOTE SENSING            and   =     x  W 2  −  r 2  sin  ψ θ 2  cos  2  =  0 21 .        y r=  2  cos  ψ θ 2  cos  2  =  3 91 .        z r=  2  sinθ  2  = − 1      which are in agreement with the other results. This location may now be  transformed to the robot coordinates and then to earth coordinates.       One  important  consideration  when  using  stereo  vision  is  making  sure that one has detected the same target in both images. Whatever  image recognition method is used for the detection in one frame would  be used in the other, and the same features would be sought in each  frame.  However,  there  is  another  factor  which  greatly  alleviates  the  problem of locating the target in the second frame. If the two cameras  are mounted on the mounting frame at the same elevation angle and  if their longitudinal axes are almost parallel, then the target will appear  at almost the same pixel row in each camera. Thus from the pixel coor- dinates of the detection in the ﬁ rst image frame, one has a very good  starting point for the search in the second image frame.      RADAR SENSING: SYNTHETIC APERTURE      6.3   RADAR   SAR      Radar sensing relies on the transmission of electromagnetic energy and  the measurement of the returned signal. The synthetic aperture method  of sensing utilizes multiple signal returns from each pixel to be imaged.  These multiple returns may be obtained from a single antenna, which  moves along and is in a different location for each return, or they may  be obtained from an array of sensors with each sensor at a different  location. The actual radar signal may be a pulse or sinusoids of stepped  frequency. If a stepped frequency signal is employed, the received signal  is converted to the time domain by use of the inverse Fourier Transform.  For a moving receiver antenna, this conversion is done for each antenna  location. If an array of receiver antennas is used, this is done for each  antenna. Here we consider a horizontal array of M receiver antennas  mounted on the vehicle. At the m th  receiver, the received signal is con- verted to the time domain via the inverse discrete Fourier Transform   RADAR SENSING: SYNTHETIC APERTURE RADAR  SAR      207  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  i  1   r e v e c e R    t  a     i  d e v e c e r   l  a n g S  i  0  0  1  2  3  5  4 6 Time of Travel  7  8  9  10       Figure 6.10       Plot of Hypothetical Signal Received at Receiver 1 versus Time of Travel          t             g t  m  =    G f m p   exp   i  π       2 f t p    6.15      N f  ∑1  =  1  p  N  f   A possible graph of such a signal is shown in Figure  6.10 .     Now  to  compute  the  intensity  of  signals  returned  from  the  pixel    X YF F, one ﬁ rst computes the travel time from the single transmitter,  to the pixel in question and back to each receiver element. For the m th   receiver element this is  m ground     x y F F  ,     [   x  , s m  −  x  F  2     +     y , s m  −  y F  2     +  2 z , s m  1 2   ]  [   x r  ,  mm  −  x  F  2     +     y , r m  −  y F  2     +     z , r m  −    1 2  z F   ]               6.16   =  1 c +  1 c   That is, the distance is computed from the transmitter to the pixel of  interest to each receiver. This distance is converted to travel time by  dividing by the speed of light.    The  time - domain  waveform  received  at  each  receiver  element  is  then evaluated at the time computed for the pixel of interest and for    208     REMOTE SENSING  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  i  1   r e v e c e R    t  a     i  d e v e c e r   l  a n g S  i  0  0  1  2  3  5  4 6 Time of Travel  7  8  9  10       Figure 6.11   Pixel of Interest Noted       Plot of Hypothetical Signal Received at Receiver 1, Travel Time for the   the respective receiver element. In Figures  6.11 ,  6.12  and  6.13  are illus- trated  hypothetical  samples  of  waveforms  at  the  different  receiver  antennas and travel times associated with a particular pixel.      These signal values are then summed over all receiver elements        S x y F     ,  F  n     [ g t m system  t  m ground     x y F F  ,   ]       +    6.17      =  M  ∑1  =  1  m  M   The  result  is  the  signal  intensity  corresponding  to  the  pixel  being  imaged.  Clearly  there  is  other  signal  content  included  in  each  of  the  terms of the summation above. Each term includes all reﬂ ection from  an ellipse passing through the pixel of interest. These ellipses, one cor- responding to each receiver element, all coincide at the pixel of interest.  Therefore, these components of the signals add coherently whereas the  other components are incoherent and tend to cancel each other. This  point is illustrated in the Figure  6.14 .      As an aside, one can see that the angle at which the ellipses intersect  each other depends on the width of the array. The narrower the array,  the more difﬁ cult it is to determine the intersection point in the cross    1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  i  2   r e v e c e R    t a     i  d e v e c e r   l  a n g S  i  3  i    r e v e c e R    t  a     i  d e v e c e r   l  a n g S  i  0  0  1  2  3  5  4 6 Time of Travel  7  8  9  10       Figure 6.12   Pixel of Interest Noted       Plot of Hypothetical Signal Received at Receiver 2, Travel Time for the   0  0  1  2  3  5  4 6 Time of Travel  7  8  9  10       Figure 6.13   Pixel of Interest Noted       Plot of Hypothetical Signal Received at Receiver 3, Travel Time for the   209   210     REMOTE SENSING  Locus of equidistant way points,  Transmitter to Receiver 1   Locus of equidistant way points,  Transmitter to Receiver 2   Receiver 1   Transmitter   Receiver 2        Figure 6.14      Illustration of Sources of Received Signals Using Time Gating. Only the  point  where  the  ellipses  intersect  is  common  to  both  receivers.  Signals  reaching  the  receivers via other points on the ellipses are incoherent.    track direction. This is the reason that the ﬁ nite width of the antenna  array limits the resolution in the cross track coordinate.    This process is continued for each pixel in the frame. Note that ﬁ rst  the  signals  are  transmitted  by  the  transmitter  and  collected  by  all  receivers over the entire frequency range. The inverse DFT of the signal  received at each receiver is then computed. Then the images are formed  afterward for the entire frame, pixel - by - pixel. Parallel processing may  be used here. Signals for the next frame may be transmitted and col- lected  while  the  image  is  being  formed  for  the  previous  frame. The  required time interval between frames would be the maximum of these  two operations.    Because of the way the SAR images are formed, the image initially  obtained is in vehicle coordinates. If the vehicle is yawed, pitched or  rolled with respect to the ﬁ eld of view to be imaged, this needs to be  taken  into  account  for  exact  computation  of  the  distances  from  the  transmitting antenna to the pixel in question and back to each receiver  element. Likewise, if the terrain in front of the vehicle is uneven, this  also needs to be incorporated for exact distance calculations. Fortunately  for radar, the effects of vehicle pitch and roll are minimal in the range  calculation as are the effects of uneven terrain. The two most important  pieces of information are vehicle position and heading or yaw.    Effects of uneven terrain on ground registration may be analyzed in  more detail utilizing Figure  6.15 . With the receiver elements arranged    RADAR SENSING: SYNTHETIC APERTURE RADAR  SAR      211  XA = crossrange  α  YA =downrange    zA  ψ  R  -θ       Figure 6.15       Geometrical Considerations, Radar Type Sensor    in a horizontal line with the transmitter centered on this array, the locus  of points lying at the intersection of the ellipses is as shown below. Here  the receiver elements are located along the  x  axis of the antenna and  symmetric about the  z  axis. The transmitter is located at the origin of  these axes. The ray to the pixel of interest is labeled with length R. Note  that this ray makes angle alpha with respect to the  yz  plane. The equa- tions for signal time - of - travel are such that one can rotate this ray about  the  x  axis, maintaining the angle alpha with respect to the  yz  plane, and  deﬁ ne a circle such that reﬂ ections from any point lying on this circle  will  be  imaged.  If  the  height  of  the  antenna  above  the  terrain  being  imaged is H, as was expected, then the pixel ’ s  y  and  z  coordinates will  be as was expected. However, if the terrain is higher or lower than was  expected, the  y  and  z  coordinates of the pixel imaged will change. The   x   coordinate  is  unchanged  by  elevation  changes.  Knowledge  of  the  terrain  elevation  can  be  used  to  prevent  this  type  of  error  in  image  formation. Fortunately the  y  coordinate changes only very slightly with  elevation causing errors due to unknown terrain to be minimal.      The equations for the point imaged are as follows.           x  R  A = − sinα    A = −    z  H         6.18a       6.18b      212     REMOTE SENSING  and        A =     y  2  R  − cos α  2  2  H         6.18c       For geo - location of targets the orientation of the ﬁ eld of view with  respect  to  the  vehicle  must  be  known. Typically  for  forward - looking  sensing it would be pre - speciﬁ ed as a rectangle in front of the vehicle.  The actual design of the radar would be dictated by this speciﬁ cation.  Thus each pixel has a known pair of  x  and  y  coordinates with respect  to  vehicle  coordinates.  The   z   coordinate  will  be  obtained  either  by  assumption of a ﬂ at earth or from a terrain model based on an array  of  range  measurements.  To  convert  from  pixel  location  in  vehicle  coordinates to pixel location in earth coordinates, one uses the equa- tion below            X Y Z  PE  PE  PE        =        X Y Z  VE  VE  VE        +  R  VE        X Y Z  PV  PV  PV               6.19       For the rotation matrix   RVE one needs to know the vehicle attitude.  Of the three components,  yaw, pitch and roll  yaw is the most crucial  for accurate geo - location when using radar. As stated earlier, the geo -  location  accuracy  for  radar  is  fairly  insensitive  to  errors  in  pitch  and  roll. This is in contrast to camera type sensors, which are quite sensitive  to  all  three  components  of  attitude.  Candidate  sensors  for  attitude  measurement include a digital compass, an array of GPS antennas or  an inertial measurement unit. An inclinometer could be a candidate if  there is no acceleration during the imaging, e.g., imaging is done with  the  vehicle  stopped.  Clearly,  one  also  needs  vehicle  position,  which  would most likely be obtained via DGPS or from an inertial navigation  system. See Kositsky and Milanfar for an example of this type of radar.       6.4      POINTING OF RANGE SENSOR AT DETECTED OBJECT    If an object of interest has been detected with only an IR camera, one  may know quite precisely the direction to the target but not the dis- tance  range . A combination of an IR camera and radar could be used  for improving the precision of the geo - registration process. A combina- tion of the IR camera and a ranging device such as a ranging laser may    POINTING OF RANGE SENSOR AT DETECTED OBJECT     213  Object  Yve  YPn       Figure 6.16   the Camera Boresight       P & T Unit at Zero Yaw and Zero Pitch, Object of Interest Detected Off   XPn  Xve  also  be  used  to  complete  the  required  measurements. Assuming  that  an object has been detected via a camera and the direction to it has  been determined, we now turn to the task of pointing a ranging laser  in that direction. See Figure  6.16 .      We assume that the ranging laser and the camera are both mounted  on a pan and tilt device and that they are co - aligned with each other.  We further assume at this point that the pan and tilt angles are both  zero at the time of target detection, i.e., the camera is aimed straight  ahead. Later we will handle the more general case.    From  Section   6.1   we  may  compute  the  direction  of  the  ray  to  the  target corresponding to the location of the target in the image, i.e., cor- responding to the ij th  camera pixel coordinate. Using these angles com- puted as has been illustrated, one gets as an expression for the unit ray  pointing at the target  and                 x  = −sin  cosψ  ray  θ      ray     y  = cos  cosψ  ray  θ      ray  = sin  .θ      ray     z     pan  =ψ      ray    6.20       6.21       6.22        6.23      One ﬁ nds that the required pan or yaw angle is simply  and the required tilt or pitch angle is   214     REMOTE SENSING  Object  Yveh  YPnT  XPnT  Xveh       Figure  6.17   Interest to Appear at Camera Boresight       Pan  and  Tilt  Unit  Rotated  to  Required  Yaw  and  Pitch  for  Object  of               tilt  =θ      ray    6.24       Angular motion of this amount will bring the camera boresight onto  the detected target. Once the pan and tilt unit bring the boresight of  the camera in alignment with the object of interest, a co - aligned ranging  laser will similarly now have its axis aligned with the object of interest.  See Figure  6.17 .      The distance to the object of interest will now be measured with the  ranging laser. With the boresight or the  x  axis of the sensor aimed at  the object of interest, the position of this target expressed in the sensor  frame will be given by            X Y Z  TS  TS  TS        pointed  =        0  0  Range               6.25        6.26       Converting this back to the coordinates of the sensor before the pan   and tilt motion, we have            X Y Z  TS  TS  TS        original  =        − Range Range  cos cos Range  θ ray θaay r sin  sin cos θ ray  ψ ψ  ray  ray              Several  coordinate  transformations  are  required  to  convert  this  vector  to  earth  coordinates.  The  general  rotation  matrix  presented  earlier is repeated here   POINTING OF RANGE SENSOR AT DETECTED OBJECT     215      = R ψ θφ , , ψ φ ψ θ φ −  cos cos      ψ φ ψ θ φ + sin cos    −   cos ssin  sin sin sin cos sin sin θ φ  ψ θ − sin cos ψ θ cos cos θ sin  + ψ φ ψ θ ψ φ −  cos sin sin sin  sin sin co ss sin cos cos θ φ  φ   ψ θ φ     6.27           cos cos   The rotation matrix for the sensor with respect to vehicle is given by     =     RSV        ψ cos SV ψ sin SV 0  − sin cos  ψ SV ψ SV sin  θ cos SV θ cos SV θ SV  ψ θ sin sin SV SV − θ ψSSV cos sin SV θ cos SV               6.28       These  represent  the  orientation  of  the  platform  of  the  pan  and  tilt  unit  with  respect  to  the  vehicle  under  zero  pan  and  tilt.  It  is  simpler  than  equation    6.27    because  roll  of  the  pan  and  tilt  unit  is  zero.  These  also  correspond  to  the  camera  orientation  with  respect  to  the  vehicle  because  it  has  been  assumed  that  the  camera  is  mounted with zero yaw, pitch and roll with respect to the pan and tilt  platform.    For the vehicle with respect to earth the rotation matrix is  = cos sin sin cos  RVE                  VE  VE  ψ ψ ψ VE ψ VE − cos  φ cos VE θ sin VE φ cos VE θ sin VE θ sin VE  − sin + sin φ  φ VE  φ VE  −  ψ  sin  VE  cos  θ VE  cos  θ VE  V  cos  ψ EE θ VE  sin  cos sin sin cos  VE  VE  ψ ψ ψ VE ψ VE co ss  sin sin sin sin θ VE  φVVE θ VE φ VE θ VE cos  + cos − cos φ VE  φ VE  φ VE                    6.29       The yaw angle,   ψVE, pitch angle,   θVE, and roll angle,   φVE, all vary and   are read out from the robot attitude sensor.    Now  the  target  location  is  converted  to  vehicle  coordinates.  The   required equation is               T  X Y T Z        = [  R  SV  ]        T  X Y T Z        +        S  X Y S Z       S Vehicle Coords         6.30      T Vehicleo Coords  T Sensor  CCoords   216     REMOTE SENSING   This can be written more concisely as a single operation using the    homogeneous transformation  introduced earlier.             T  X Y T Z T 1         = [  ]  A  SV         T  X Y T Z T 1         Vehicle Coords  S  eensor Coords         6.31     where for   ASV we have                   A  SV  =  R  SV  X Y Z  S VehCoords  S VehCoords  S VehCoords              0  0  0  1    6.32        6.33      The fourth column represents the position of the origin of the sensor  frame  in  vehicle  coordinates,  i.e.,  the  dimensions  associated  with  the  location of the sensor on the vehicle. They are ﬁ xed quantities and are  measured with respect to the position of the DGPS receiver, since this  is where the origin of the vehicle frame is deﬁ ned to be.    For   AVE  we have     A  VE  =  R  VE  X Y Z  V Earthoords  V Earthoords  V Earthoords              0  0  0  1  where the fourth column represents the position of the origin of the  vehicle frame in earth coordinates, i.e., the coordinates obtained from  DGPS receiver. Using these homogeneous transformation matrices the  position of the object of interest in earth axes will be given by  T  X Y T Z T 1                    =  R  VE  Earth Coords  0  0  0               R  SV  0  0  0  X Y Z  X Y S Z  Veh EarthCoords  Veh Earth  CCoords  Veh EarthCoords  1  1  S Vehicle Coords  VVehicle Coords  S Vehicle Coords                     T  X Y T Z T 1                      6.34   Sensor Coords               DETECTION SENSOR IN SCANNING MODE     217  or                   T  X Y T Z T 1                T  X Y T Z T 1         = [  ][  A  VE  A  SV  ]         6.35      Earth coords  Sensor Coords   In terms of the non - homogeneous representation, the above equa-  tions for the target in earth coordinates are equivalent to  T  X Y T Z                  T EarthCoords  =        V  X Y V Z       V EarthCoords  +  S  EE  R V    X   Y   S     Z S VehicleCoords sinθ  cos  θ cos cos  ray  θ  sin Range ray  ψ ray ψ  ray  ray  Range  − RRange        +  R R  VE SV                6.36    Although less concise than when using the homogeneous transfor- mation matrices, this last form is intuitively appealing since it is the sum  of three recognizable terms:  a  position of the vehicle with respect to  the  earth  in  earth  coordinates  plus   b   position  of  the  sensor  with  respect to the vehicle converted to earth coordinates plus  c  position  of the target with respect to the sensor converted to vehicle coordinates  and then to earth coordinates. In the above the rotation matrix,   RSV is  evaluated at zero pan and tilt, the conditions that existed at the time  of the detection.    The above analysis has been based on the assumption that the target  has been detected by the camera with the pan and tilt platform at zero  pan  and  zero  tilt. The  required  pan  and  tilt  for  aligning  the  camera  boresight and the ranging device with the target were then computed  based on the camera pixel coordinates of the target. After measuring  the range to the target, the target location was then determined in earth  coordinates.       6.5      DETECTION SENSOR IN SCANNING MODE    A more likely scenario would be that the pan and tilt unit is used in a  scanning mode and that the target was detected when the pan and tilt  coordinates  were  not  zero.  In  Figures   6.18a   and   6.18b   are  shown  IR  images with different pan angles.      218     REMOTE SENSING   a    b        Figure 6.18        a  Camera Pointed Straight Ahead,  b  Camera Panned to the Right    Object  YPnT  Yveh  XPnT  Xveh       Figure 6.19   and Pitch. Object detected off camera boresight       Pan and Tilt Unit in Scanning Mode. Pan and Tilt Unit at Non - zero Yaw    The question then is what should the new pan and tilt coordinates  be in order to align the camera boresight  and thus the laser boresight   with the target for measuring the range. See Figure  6.19 .      The  unit  vector  at  detection  in  camera  coordinates  would  be   given by        XT SensoraCoords  =    6.37      ray  − sin  cos     ψ ψ ray sin  θ cos ray θ cos ray θ ray              The  rotation  of  the  camera  with  respect  to  the  vehicle  is  given  by  the current pan and tilt angles at the time of target detection. This rota- tion matrix is   DETECTION SENSOR IN SCANNING MODE     219     R  pan tilt&  =        cos  sin     pan 1   pan 1 0  − sin  cos     pan 1   pan 1 0   0  0   1        1 0 0  0 cos  tilt sin  tilt 1    1    0 − sin  tilt cos  tilt 1    1               6.38       The unit ray to the target in vehicle coordinates is given by  T Vehicle Coords =  X       1  0    0  cos  sin     pan 1   pan 1 0  − sin  cos     pan 1   pan 1 0  0 cos  tilt sin  tilt 1    1    0 − sin  tilt cos  tilt 1    1           0  0    1 − sin  cos                  6.39   ray  ψ ψ ray sin  θ cos ray θ cos ray θ ray        T Vehicle Coords =  X  − sin  cos              cos  sin     pan 1   pan 1 0 − ψ θ cos sin ray ψ θ cos cos ray ssinθray  ray  ray        tilt tilt  1    1     cos  pan 1  cos  pan 1 si nn    tilt  1  sin  − cos     ttilt 1 tilt  1      sin  pan 1  sin  pan 1 cos    tilt  1         6.40              In order to measure the range to the target, we desire the pan and  tilt motion that will place the target at the boresight of the camera. See  Figure  6.20 . When this is accomplished, the expression for the unit ray  in camera coordinates will be     XT SensoraCoords =    6.41                    0 1 0        The equation for the unit ray in vehicle coordinates will then become  T Vehicle Coords =  X            1  0    0  cos  sin        pan 2 pan 2 0  − sin  cos     pan 2   pan 2 0  0 cos  tilt sin  tilt       2  2  0 − sin  tilt cos  tilt  2    2           0  0    1  0  1    0                     6.42           or                220     REMOTE SENSING  Object  Yveh  YPnT  XPnT  Xveh       Figure 6.20   Object to Camera Boresight       Pan and Tilt Unit Rotated by Required Yaw and Pitch to Bring Detected   or  X       T VehicleCoords =    cos   sin        pan 2 pan 2 0  equation  − sin  cos   tilt tilt  2    2     cos  pan 2  cos  pan 2   nn  si tilt  2  sin  − cos      ttilt 2 tilt     2   sin  pan 2  sin  pan 2   cos  tilt  2                   0 1 0           6.43         Equating  the  two  expressions  for    XT Vehicle Coords  yields  the  vector   2  2  =    2    − sin   cos                  cos  sin  sin  cos   − + − −  tilt tilt    cos  pan 2   cos  pan  2   sin    tilt θ ψ cos  sin pan 1 ray ray θ  sin  sin  tilt pan 1 1 ray θ + ψ cos  sin pan 1 ray ray θ  sin  sin  tilt pan 1 1 ra yy ψ  cos sin  cos tilt  −  ray  1  sin   pan 1   cos   tilt  1   cos  ψ  ray  cos  θrray               co  ss   pan 1   cos   tilt  1   cos  ψ  ray  cos  θ ray   6.44   θ ray  +  cos   tilt   sin  1  θ ray            One can solve for   pan2 by equating the ratio of the negative of the   ﬁ rst component to the second component on each side, i.e.,           or  or        DETECTION SENSOR IN SCANNING MODE     221      = tan pan 2     ψ  cos sin pan 1 ray         sin sin pan tilt  1 1     ψ −  sin sin pan 1        −  sin cos pan 1  +  θ cos ray   θ sin ray θ + cos rray   ssinθray  1  ray tilt  sin  pan 1  cos  tilt  1              cos  ψ aay  r  cos  θ ray  −  cos  pan 1  cos  tilt  1              ψ  cos  ray  cos  θ ray             pan 2        − 1  tan  =         cos  sin  − −  sin  cos   +  ψ  sin pan 1 ray  sin  tilt pan 1 1 ψ  sin pan 1 ray  sin  tilt pan 1  θ cos ray θ  sin ray θ + cos rray ssinθray    1  sin   pan 1   cos   tilt  1   cos  ψ aay  r  cos  θ ray  −  cos   pan 1   cos   tilt  1   cos  ψ  ray  cos  θ ray                 6.45       One ﬁ nds   tilt2 by equating the third components on each side, i.e.,        sin  tilt  2    =     sin  tilt  1     ψ  cos  ray  cos  θ ray  +     cos  tilt  1     sin  θ    ray     tilt  2  =  − 1  sin {sin   tilt   cos  1  ψ  ray  cos  θ ray  +  cos   tilt  1   sin  θ     }  ray    6.46       After setting the pan and tilt unit to these angles, the longitudinal  axis of the camera and range ﬁ nder will be aligned with the target. Once  the range has been measured, one proceeds as before to convert the  measurement to earth coordinates.      EXAMPLE 5     For determining the required pan and tilt angles for bringing the bore- sight to the target, one might be tempted to simply add the azimuth of  the ray pointing at the target to    pan1  and to add the elevation of the ray  pointing at the target to    tilt1 . Demonstrate numerically that this is not the  correct approach.       222     REMOTE SENSING     SOLUTION 5   20=    and    ψray = 7 . Also let    tilt1    Let    pan1  and     ψray   one  would  conclude  that     pan2 28 4962 correct procedure yields    pan2  one would conclude that    tilt2 dure yields    tilt2  = 35=  34 7407 .     =  .  30=    and    θray = 5 .By adding    pan1  27=    when  in  fact  use  of  the   . Similarly by adding    tilt1  and    θray  .   when in fact use of the correct proce-    EXERCISES          1.      A       A  mobile  robot  is  at  location  x    =    102   m,  y    =    59   m  and  z    =    1   m  when speciﬁ ed in a local coordinate frame. The attitude of the robot  is yaw    =    37 degrees, pitch    =    5 degrees and roll    =    4 degrees. An object  of interest is sensed by a camera and a range sensor mounted on the  robot. The  object  is  determined  to  be  located  at  x    =    3   m,  y    =    15   m,  and z    =     − 2.2   m in the robot frame. Compute the location of the object  in the local coordinate frame.      B       A  second  object  is  identiﬁ ed.  Now  the  robot  position  is  x    =    120   m,  y    =    40   m  and  z    =    0  and  its  attitude  is  yaw    =    40  degrees,  pitch    =    zero and roll    =    zero. The object is determined to be located  at x    =     − 7   m, y    =    10   m, and z    =    2   m in the robot frame. Assume that the  robot has differential wheel steering as well as an adjustable suspen- sion system giving it the ability to vary its pitch by elevating the front  or back with respect to the wheels. It is desired to aim the longitu- dinal axis of the robot at the object of interest. What are the required  rotation angles for the robot?            2.      A       The robot has an IR camera mounted at its center with respect  to width and length. The camera is 1 meter above the ground and is  pitched down at 12 degrees below horizontal. The ﬁ eld of view is 14  degrees in height and 18 degrees in width. There are 320 pixels across  and 240 pixels down. A target is detected in the 85 th  row and 203 rd   column. What  are  the  pan  and  tilt  commands  to  align  the  camera  boresight with the target?      B       Assume  that  a  ranging  laser  that  is  aligned  with  the  camera  boresight measures the range to the target as 15 meters. Determine  the location of the target. The robot GPS readings give its location  as Easting    =    755,295 and Northing    =    3,685,240. The vehicle heading  measured  in  the  counter - clockwise  direction  from  the  Y  axis,  i.e.,  from North, is  − 135 degrees.         REFERENCES     223      3.     An IR camera has a vertical ﬁ eld of view of 12 degrees and a hori- zontal ﬁ eld of view of 16 degrees. It has 320 columns and 240 rows  of pixels. It is mounted on a robot at a height of 2 meters and a pitch  of  − 6 degrees. The x and y coordinates of the camera with respect to  the vehicle are zero. Also yaw and roll of the camera with respect to  the robot are zero. An object of interest is detected at the 30 th  row  and  the  48 th   column.  Find  the  location  of  the  object  in  vehicle  coordinates.         4.     A radar sensor mounted on a mobile robot shows an object of inter- est in the pixel of the 100 th  row and the 25 th  column. The ﬁ eld of view  is laid out to cover the area beginning 10 meters in front of the robot  up  to  50  meters  away.  It  extends  over  a  width  of  10  meters.  The  dimension of this ﬁ eld of view in pixels is 200 rows and 50 columns.  What is the location of the object of interest in robot coordinates?        5.     The radar sensor described in Exercise 4 shows an object of interest  in the 85 th  row and the 35 th  column. What is the location of the object  of  interest  in  robot  coordinates?  If  the  robot  is  located  at  x    =    300  and y    =    200 with a heading of 45 degrees all in a local coordinated  system,  what  is  the  location  of  the  object  of  interest  in  this  local  coordinate system?         6.     A  robot  with  a  camera  mounted  on  a  pan  and  tilt  unit  detects  an  object  when  the  pan  angle  is  25  degrees  and  the  tilt  angle  is   − 7  degrees. The camera has a vertical ﬁ eld of view of 12 degrees and a  horizontal  ﬁ eld  of  view  of  16  degrees.  It  has  320  columns  and  240  rows  of  pixels.  The  object  appears  in  the  112 th   row  and  the  143 rd   column. What  should  the  pan  and  tilt  angles  be  set  to  in  order  to  bring  the  boresight  of  the  camera  onto  the  target  for  a  range  measurement?         7.     Assume that the range measurement described in Exercise 6 turned  out to be 75 meters. What would be the location of the object in robot  coordinates?          REFERENCES   Press ,  1988 .        Bar - Shalom ,  Y.    and   T.    Fortmann  ,   Tracking  and  Data Association ,  Academic       Cook ,  G.  ,   Sherbondy ,  K.  , and   Jakkidi ,  S.  ,  “  Geo - Location of Mines Detected via  Vehicular - Mounted  Forward - Looking  Sensors  ” ,   Proceedings  of  SPIE  Conference  on  Detection  and  Remediation  Technologies  for  Mine  and  Minelike Targets VI , Vol. 4394,  pp  922  –  933 ,  Orlando, FL , April 16 – 20,  2001  .     224     REMOTE SENSING      Jakkidi ,   S.    and    Cook ,   G.    “  Geo - location  of  Detected  Landmines  via  Mobile  Robots Equipped with Multiple Sensors  ” ,  Proceedings of IECON 2002 ,  pp   1972  –  1977   Seville, Spain , Nov. 4 – 8,  2002  .        Jakkidi ,   S.    and    Cook ,   G.     “  Landmine  Geo - location;  Dynamic  Modeling  and  Target Tracking  ”   IEEE Trans on Aerospace and Electronics , Vol.  41  No.  1    Jan.  ’ 05  pp  51  –  59 .        Johnson ,  P. G.   and   P.   Howard  ,  “  Performance Results of the EG & G Vehicle -  Mounted Mine Detector  ” ,  Proceedings of SPIE, Detection and Remediation  Technologies for Mines and Mine - like Targets IV ,  Orlando, FL , April 5 – 9,   1999 , pp  1149  –  1159 .        Kansal ,  S.  ,   Jakkidi ,  S.   and   Cook ,  G.    “  The Use of Mobile Robots for Remote  Sensing  and  Object  Localization  ”    Proceedings  of  IECON  2003 ,   pp   279  –   284 ,  Roanoke, Va, USA , Nov. 2 – 6,  2003  .        Kositsky ,  J.  , and   P.   Milanfar  ,  “  Forward - Looking High Resolution GPR System  ” ,   Proceedings  of  SPIE,  Detection  and  Remediation Technologies  for  Mines  and Mine - like Targets IV ,  Orlando, FL , April 5 – 9,  1999 , pp  1052  –  1062 .        Lloyd ,  J. Michael  ,  Thermal Imaging Systems .  Springer ,  1975 .       Skolink ,  I. M.  ,  Introduction to Radar Systems .  McGraw Hill ,  1980 .       Wu ,  Y. A.  ,  “  EO Target Geolocation Determination , ”   Proc. Of IEEE  Conference   on Decision and Control, pp.  2766  –  2771 , December  1995 .                                                        7   TARGET TRACKING INCLUDING  MULTIPLE TARGETS WITH  MULTIPLE SENSORS           7.0      INTRODUCTION    This chapter is devoted to tracking the coordinates of detected objects  of interest as the mobile robot moves along in its search. There can be  multiple detections of a given sensor as well as detections of a given  target  by  more  than  one  sensor. All  these  detections  can  be  used  to  improve the estimate of the coordinates of the objects of interest. In  some  cases,  multiple  targets  may  be  detected  and  tracked.  Means  of  associating measurements with the proper targets are also discussed.       7.1      REGIONS OF CONFIDENCE FOR SENSORS    Every  measurement  is  accompanied  by  some  uncertainty,  which  depends  on  the  variance  of  the  measurement  error. Thus,  when  one  computes the ground coordinates of an object of interest, there is asso- ciated with this value a region of conﬁ dence. The smaller the errors, the  tighter will be the region of conﬁ dence. In the principal axes, the bound- ary of the region of conﬁ dence is described by the equation  Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  225   226     TARGET TRACKING INCLUDING MULTIPLE TARGETS    TABLE 7.1   Chi - square Tables     Typical values of Gamma from     P      γ       0.5     1.39      0.90     4.6      0.95     5.99      0.99     9.2          − x x est σ 2 x  2        +  − y y est σ 2 y  2     =  γ       7.1     where   σx and   σy are the standard deviations of the measurement error  in the x and y directions respectively. It is assumed that the errors have  Gaussian distributions. This boundary, which is an ellipse, contains the  true target location with probability P given in Table  7.1 .     The user selects   γ in accordance with the certainty desired. To have  a  high  degree  of  conﬁ dence  that  the  object  of  interest  is  within  the  boundary, one selects a high value for P. The result is a large value for  the associated  γ  and therefore a large ellipse. In the original coordinates  the ellipse axes may be rotated i.e., cross variance is possible. The vari- ance of the measurement errors depends on the errors associated with  the sensor itself as well as errors in the estimates of the vehicle position  and  attitude.  In  the  original  coordinates,  the  general  equation  of  an  ellipse deﬁ nes the boundary of the conﬁ dence region and is given by         − −  x x y y  est  est   T   2 x  σ σ  xy  σ σ  2 y  yx      − 1     − −  x x y y  est  est    =  γγ            ∆ x ∆ y   T   2 x  σ σ  xy  σ σ  2 y  yx      − 1     ∆ x ∆ y    =  γ         7.2       7.3       The  following  describes  how  to  convert  this  to  the  simpler  ellipse  representation.  Let  the  eigenvectors  of  the  inverse  of  the  covariance  matrix be represented by   ξ1 and   ξ2, and deﬁ ne the matrix     M = [  ξ ξ1  ] 2 .      Then the above equation can be written as         ∆ x ∆ y   T   − 1  MM  − 1  2 x  σ σ  xy  σ σ  2 y  yx      − 1  MM     ∆ x ∆ y    =  γ                   or   REGIONS OF CONFIDENCE FOR SENSORS     227   Now  since  the  inverse  of  the  covariance  matrix  is  symmetric,  its  eigenvectors will be orthogonal. They may also be normalized yielding  the result   M MT  . The above equation may then be re - written as  − =1         ∆ x ∆ y   T   MM  T  − 1  2 x  σ σ  xy  σ σ  2 y  yx      MM  T     ∆ x ∆ y    =  γ       or  where  and                  cid:1  ∆ x  cid:1  ∆ y          T    cid:1  σ 2   x 0  1       cid:1  ∆ x  cid:1  ∆ y           =  0  cid:1  σ 2   y  1  γ       cid:1  ∆ x  cid:1  ∆ y           =  T  M     ∆ x ∆ y          cid:1  σ 2   x 0  1          0  cid:1  σ 2   y  1    =   T  M  − 1  2 x  σ σ  xy  σ σ  2 y  yx      M       2   cid:1  ∆ x  cid:1      σ 2 x  +  2   cid:1  ∆ y  cid:1  σ 2 y  γ =       It is seen that the above representation of the ellipse reduces to the   simpler form   If desired, one may compute the coordinates of the ellipse in these  normal coordinates and then map them into the original coordinates.   It has been noted that for objects sensed via cameras one can directly  determine  the  direction  to  the  object  but  not  the  range.  If  a  single  camera is used, one can make the assumptions described in the forego- ing and estimate the range. There are errors associated with this esti- mate  that  depend  on  errors  in  measurement  of  vehicle  attitude  and  unevenness of terrain. Thus, the uncertainty in knowledge of the  y   or  down - range   component  can  be  quite  large  whereas  the   x    or  cross -  range  component is more precisely known. If one constructs an ellipse  of uncertainty about the measurement corresponding to the boundary  within which the true position exists with a given probability, this ellipse     7.4       7.5       7.6       7.7        7.8       228     TARGET TRACKING INCLUDING MULTIPLE TARGETS  is elongated in the down - range direction and more slender in the cross -  range direction, reﬂ ecting the difference in error variance in the two  coordinates.    In contrast, the radar sensor just described has very good precision  in the down - range direction, limited only by the resolution of the timing  circuitry,  whereas  its  cross - range  precision  is  poorer  because  of  the  limited horizontal extent of the array causing one to have to resolve  the intersection of nearly parallel ellipses. If one constructs an ellipse  of  uncertainty  about  this  measurement,  it  would  be  elongated  in  the  cross - range direction and more slender in the down - range direction.   Combined measurements have the property that as more measure- ments are taken, the variance of the estimate monotonically decreases.  In fact, their variances combine in the same way as do resistors in paral- lel. As an example, if one optimally estimates a scalar quantity,   X from  , ,…  where the covariance of the  , n independent measurements   Y Y 2 1 error in each measurement is   σi 2 and the mean is zero then, the estimate  is given by  Yn  1     σ 2 1  +  1     …  +  1     σ 2 n  1     σ 2 1  +  1     …  +  1     σ 2 n  +  …  +  Y 1  Y n         7.9      σ 2 1   n + σ 2 2  σ 2 1   1 + σ 2 2   The covariance of the error in the ﬁ nal estimate is     σ  2  =  1     σ 2 1  +  1     +   … 1  +  σ 2 n       1 σ 2 2    7.10          ˆ X  =      Thus for estimating the value of a ﬁ xed scalar quantity, the variance  can never increase as additional measurements are taken. And as illus- trated above, the variance for the estimate will never be greater than  the  smallest  measurement  variance. This  scalar  quantity  could  repre- sent one component of a vector describing the location of a stationary  object. Applying  the  above  results  to  this  situation,  one  has  that  the  variance of the error in any direction can never increase and further  that the variance in that direction would be no greater than the smallest  measurement variance in that direction. Because radar is so precise in  the down - range coordinate, which is where IR is the worst, and the IR  is better in the cross - range direction where this particular type of radar  is  the  worst,  the  combination  of  radar  and  IR  sensing  offers  great  promise for ground registration. These two sensors may be said to have  orthogonal axes of precision. This is a fortuitous set of circumstances  that one may exploit.    REGIONS OF CONFIDENCE FOR SENSORS     229  Downrange component  Crossrange component       Figure 7.1       Illustration of Components of Measurement Error     A diagram illustrating the components of sensor measurement error    The  variances  for  the  down - range  and  cross - range  components  of   is shown in Figure  7.1 .     this error are given by     σ    σ  2 downrange  2 crossrange  −  dnrng sen  2= σ σ = 2 crsrng sen  −      +  2  σψ −  2  r       veh    7.11       7.12       The  covariance  matrix  describing  this  uncertainty  in  earth  coordi-  nates is given by         R R  XX  YX  R R  XY  YY    =  σ  2 − x veh  σ   yx veh  −  −  σ xy veh σ 2 − y veh    +   R  eearth veh  σ  2 downrange    0  0  σ 2 crossrange            7.13      and   σy veh−  where   σx veh−  are obtained from the Kalman Filter associated  with the vehicle model. This covariance matrix is processed to yield the  required coordinate transformation and the ﬁ nal   σx 2 to be used  in equation   7.8  . Greater values of   σ yield larger ellipses. The eccen- tricity of the ellipse is determined by the relative values of   σx and   σy.   Based on the algorithm used for estimating the range to the object  of interest one can determine that the covariance for the camera sensor  in the down - range coordinate is  2 and   σy     σ  2 dnrng sen  −  2  =       2  r H  2  σθ −       veh    7.14                Note  that  this  quantity  becomes  very  large  as    r   increases.  Sensor  error variance in the cross - range direction is limited only by pixel reso- lution.  Using  typical  values  for  the  various  parameters,  the  ellipse  describing the region of conﬁ dence is shown in Figures  7.2  and  7.3 .      230     TARGET TRACKING INCLUDING MULTIPLE TARGETS  1     m      g n h  i  t r o N  46  44  42  40  38  36  34  32  30  28  26  –5  0  5  Easting  m   10  15       Figure 7.2       IR Sensor Region of Conﬁ dence     In Figure  7.2  the path of the mobile robot toward the object of inter- est is also shown. The large value of error variance in the downrange  direction compared to that in the cross - range direction is apparent. In  contrast to the IR sensor, the error variance for the radar considered  here  in  the  downrange  direction  is  small  but  it  is  larger  in  the  cross  direction. Using typical values the ellipse describing the region of con- ﬁ dence for radar is shown in Figures  7.4  and  7.5 .     The  results  shown  were  generated  using  simulated  IR  and  Radar  object detections. The parameter   γ was selected to be   4 6.  corresponding  to a   90% conﬁ dence probability.    Figures  7.6  and  7.7  illustrate the successive ellipses obtained when  the two sensors are combined. In the next ﬁ gure, the ﬁ rst measurement  is  from  an  IR  sensor  and  has  a  large  covariance  in  the  downrange  coordinate. The second measurement comes from the radar, which has  a  very  tight  covariance  in  the  downrange  coordinate.  Even  though    REGIONS OF CONFIDENCE FOR SENSORS     231  46  45.5  45  44.5  44  43.5     m       i  g n h t r o N  1  43  –3.5  –3  –2.5  –2  –1.5  –1  –0.5  Easting  m        Figure 7.3       Expanded View of the Region of Conﬁ dence for IR    radar has a large covariance in the cross - range coordinate, this dimen- sion  has  already  been  tightened  by  the  preceding  IR  measurement.  Thus the cumulative conﬁ dence region after processing both the mea- surements has good behavior in both the downrange and cross - range  coordinates. The  succeeding  ﬁ gure  illustrates  the  same  phenomenon  with the order of IR and radar measurements reversed.      The use of multiple sensors with complementary precision charac- teristics for determining the ground coordinates of detected objects of  interest has been demonstrated. Here an IR sensor that has good preci- sion in the cross - range direction is fused with a radar sensor that has  good  precision  in  the  down - range  direction.  This  fusion  of  sensors  whose axes of highest precision are orthogonal to each other exploits  the best characteristics of each sensor and provides dramatic improve- ment  in  ground  registration  precision  in  both  directions,  making  it  a  very effective ground registration combination.     232     TARGET TRACKING INCLUDING MULTIPLE TARGETS  1     m       g n h  i  t r o N  46  44  42  40  38  36  34  32  30  28  26  –5  0  10  15  5  Easting  m        Figure 7.4       Region of Conﬁ dence for Linear Array Radar Sensor       7.2      MODEL OF TARGET LOCATION    The previous section addressed the problem of measuring the location  of an object of interest, or target, utilizing various sensors. As was dis- cussed, in some cases one may have multiple sensors and may detect  the same target with more than one sensor. It is also possible that the  same  target  may  be  detected  several  times  by  a  single  sensor  as  the  robot  moves  along.  The  goal  of  this  section  is  to  combine  multiple  measurements  of  the  location  of  a  target  into  an  optimal  estimate.  Having a precise estimate of the location can be useful for accomplish- ing  the  next  goal,  which  may  be  close - up  examination,  retrieval,  or  referral to another robot that would perform one of these tasks.    In order to combine multiple measurements from a single sensor or  measurements  from  multiple  sensors  into  a  single  estimate  of  the  targets location, one may use the Kalman Filter. Of course, central to  the  application  of  the  Kalman  Filter  is  a  model  of  the  process  being    MODEL OF TARGET LOCATION     233  1  46  45.5  45  44.5  44  43.5     m  i      g n h t r o N  43  –3.5  –3  –2.5  –2  –1.5  –1  –0.5  Easting  m        Figure 7.5       Expanded View of the Region of Conﬁ dence for Radar    observed. Here, the location of the target is sought, and we will make  the assumption that it is stationary. Thus a simple model for the position  coordinates of the i th  target is       7.15        x k i       y k i  + +    1   1  = =      x k i     y k i        When  the  measurement  of  the  location  of  the  detected  target  is  converted to earth coordinates, one component of the measurement is  the estimated location and attitude of the robot itself. The covariances  of  the  errors  in  these  estimates  contribute  to  the  total  measurement  error  of  the  location  of  the  target  and  they  must  be  included  in  the  analysis. While the sensor measurement noise itself is modeled as being   “ white ” , the errors in the estimates of the robot position and attitude  are in fact colored, these estimates having been obtained via a dynamic  model and the use of the Kalman Filter. Thus the errors in measure- ment of the earth coordinates of the targets have a component that is    234     TARGET TRACKING INCLUDING MULTIPLE TARGETS  46  45.5  45  44.5  44  43.5     m       g n h  i  t r o N  1  43  –3.5  –3  –2.5  –2  –1.5  –1  –0.5  Easting  m        Figure 7.6   Followed by Radar  solid  Sensors       Convergence of Region of Conﬁ dence with Combination of IR  dashed    colored. The most accurate way of dealing with this situation would be  to  develop  a  colored  noise  model  and  incorporate  it  into  the  signal  processing. See Jakkidi and Cook. Fortunately, the impact of the noise  being colored is very small here and it can be treated as white without  serious consequences.    The measurement equations for the ith target are as shown below.     x y            Object i EarthCoords  −  Vehicle EarthCoords  −  +     v 1 vv 2     =       x y  +  R ve           x y       +    n i n i  1  2              7.16     Object i VehicleCoords  −   Note that the left side of the equation represents the  x - y  location of  the target in earth coordinates. The ﬁ rst term on the right side is the   x - y  location of the vehicle in earth coordinates and the third term on    MODEL OF TARGET LOCATION     235  1  46  45.5  45  44.5  44  43.5     m       g n h  i  t r o N  43  –3.5  –3  –2.5  –2  –1.5  –1  –0.5  Easting  m        Figure 7.7    dashed  Sensors       Convergence of Region of Conﬁ dence with Radar  solid  Followed by IR   the right side is the location of the target with respect to the robot in  robot coordinates and then converted to earth coordinates.    The  Kalman  Filter  equations  for  the  objects  of  interest  are  quite   simple. For the i th  target we have for the prediction step   cid:1  x k  cid:1  y k  +  +         1     k  1     k     =     cid:1  x k k  cid:1  y k k           +    1      w k     w k  2          and for the estimation step   cid:1  x k  cid:1  y k  +  +            =    1     k  +  1  1     k  +  1   cid:1  x k  cid:1  y k  +  +    +  1     k  1     k  K     1  +  x k meas y k meas  +  1  − −   cid:1  x kk  cid:1  y k  +  +          1     k  1     k    7.17       7.18             The discrete - time model coefﬁ cient matrices used for computing the   covariances and ﬁ lter gain are   236     TARGET TRACKING INCLUDING MULTIPLE TARGETS  and                       A =      H =    1 0 0 1  1 0 0 1                    G =    1 0 0 1            7.19       7.20       7.21       The  measurement  covariance    R  would  be  similar  to  what  was  dis- cussed in Chapter  6 , depending on the sensor used. It represents the  combined covariance of the sensor measurement noise and the error  in vehicle position all expressed in earth coordinates. For simplicity, it  is approximated here as being constant.    Recall from the discussion in Chapter 5 that the steady - state gain of  the  ﬁ lter  diminishes  when   Q   is  small.  For  this  reason  one  normally  includes a disturbance in the model. Nevertheless, even in the absence  of the disturbance in the model, during the transient the gain is nonzero.  Because of this and the interesting results that accrue, in the proceeding  we shall explore the behavior of the ﬁ lter for this particular problem  with  Q  set to zero.    Multiple  detections  of  the  same  object  provide  the  opportunity  to  improve the estimate of the location of the object. This can be seen by  the reduction in the norm of the covariance matrix as more detections  are  made.  Recalling  the  equation  for  the  covariances  and  gain,  and  using the deﬁ nitions of   A,   G,   H,  Q , and  R  from above we have       P k  +  1      k  =        P k k     the covariance of the error in the prediction,     K k      =  P k k P k k R k       [           ]  −1    +  the gain of the ﬁ lter, and       P k  +  1     k  +    1  =  −  [     ]   I K k P k  +  1      k  =  −  [     ]       I K k P k k     the covariance of the next estimate Note that the possibility of a time  varying  measurement  covariance  has  been  included,  i.e.,    R  may  be  a  function of   k.    MODEL OF TARGET LOCATION     237   The above may be re - arranged to  +  1     k  +  1    =    P k      +  [        −  P k k R k P k k R k   P k k P k k R k                             +      −1 P k k     ]        +  − 1        P k    +  1     k  +  =  R k P k k R k     [        − 1    ]  +    1        P k k       Now inverting each side of this equation yields     P k    +  1     k  +  − 1 1    =  P k k        [           ]     P k k R k R k  − 1  +  − 1  =      R k  − 1  +        P k k  − 1       P k    +  1     k  +    1  =        R k  +− 1  − 1          P k k  − 1        7.22       This illustrates that the Kalman Filter algorithm reduces to a familiar  result  from  probability  and  statistics  for  the  special  case  where  both  the A matrix and the H matrix are identity matrices. It is apparent from  the above that   P k k      , the covariance of the current estimate, decreases  monotonically as new measurements are taken. Compare this equation  with scalar case of equation   7.10  .    The equations for incorporating the new measurement into the next   estimate also simplify for this case. Using     ˆ x  +  k  1     k  +  1  =  ˆ x  +  k  1     k  +  K  +  1  k     y k  +  1  −  ˆ x  +  k  1     k        and realizing that  and that  the estimate becomes     ˆ x  +  k  1     k  =  ˆ Ax  k k     =  ˆ x  k k           K k      =  P k k P k k R k       [           ]  −1    +     ˆ x k  +  1     k  +  1  =  ˆ x  k k     +  P k k P k k R k       [        +  − 1    ]    y k  +  1  −  ˆ x        k k           or  or                  238     TARGET TRACKING INCLUDING MULTIPLE TARGETS  +  k  1     k  +  1  ˆ x      =  +               P k k R k Pk k R k + y P k k P k k R k             [            − 1     ]   k  +  − ˆ 1 x k k + − ˆ x     1     k k     +       ˆ x  +  k  1     k  +  1  =  R k P k k R k     [        − 1    ]  ˆ x  +  +  P k k P k k R k       [        − 1    ]  y k  ++1    +  k k     which may be re - written as  +  k  1     k  +  1  ˆ x      =  − 1  [       P k k + [       P k k  + − 1  R k +  − − 1 1     ] − 11     R k        P k k − − 1 1     ] R k  − 1     ˆ x k k yk  +  1          7.23       i.e.,  each  term  on  the  right  is  weighted  according  to  the  inverse  of  its  associated  covariance.  Compare  this  result  with  the  scalar  case  of  equation   7.9  .    EXAMPLE 1     A constant vector X is to be estimated from a series of measurements Y  where          Y k X v k           =  +    The noise sequence is independent with zero mean and has covariance     R k    . Determine the estimate for X and the associated covariance of the  error via processing the entire batch of data simultaneously.       SOLUTION 1     Through successive use of the recursive relationships just presented, one  can obtain as the estimate   ˆ  X  N N =         [     1 R +    R  − 1  R  + −1        2   N Y N   ]  − 1  +  …  +  R N     − 1  − 1   ] [     1  R  − 1  Y      1  +  − 1  R      2  Y      2  +  …      with covariance of this estimate being      P N N           [     1 R  =  − 1  +  R      2  − 1  +  …  +  R N     − − 1 1   ]        or     or            INVENTORY OF DETECTED TARGETS     239  Target 3  50  45  40  35  30  t  s r e e m Y     Target 2  Final vehicle  position  Target 1  Initial vehicle  position  25  –10  –5  0  5  10  15  20  X meters       Figure 7.8       Estimated Vehicle Path with Targets in Field of View     Figure  7.8  presents target tracking results involving multiple detec- tions of targets. The path of the robot as it moves from the lower right  to the upper left can be seen from the ﬁ gure. Ahead of the robot and  to its left are two objects of interest and one object that was passed by  the ﬁ eld of view of the sensor after the fourth measurement was taken.  The  series  of  ellipses  illustrate  the  0.9  conﬁ dence  regions  about  the  estimated  locations  of  the  targets.  It  is  apparent  that  these  regions  shrink rapidly as successive measurements are made.         7.3      INVENTORY OF DETECTED TARGETS    With the possibility of multiple targets in the ﬁ eld of view, the amount  of information to be saved increases, resulting in the need for some sort  of bookkeeping technique. Bookkeeping includes all the information  pertaining to objects being tracked, current estimate of coordinates and  associated  covariance. To  facilitate  the  bookkeeping,  an  inventory  is  developed. The approach used here is to have the inventory comprised  of a matrix with each column representing the proﬁ le of an individual  target. The  columns  are  appended  in  the  order  of  the  appearance  of  the  target   e.g.,  column  1  corresponds  to  the  ﬁ rst  target  detected,  column 2 corresponds to the second target detected and so forth  and  are updated as successive measurements are processed.    240     TARGET TRACKING INCLUDING MULTIPLE TARGETS   When a sensor measurement for the ﬁ rst target that has been detected  is  received,  a  ground  location  based  on  the  sensor  measurement  is  computed. This measurement is stored in the inventory. As successive  sensor measurements are received, the inventory enables one to deter- mine  whether  the  measurement  has  arisen  from  a  target  previously  detected  or  from  a  new  target. This  classiﬁ cation  of  targets  in  a  new  measurement  frame,  i.e.,  data  association,  is  carried  out  by  checking  their computed locations against those of the targets that are already  in the inventory. This process is called a  “ gating ”  operation. Clearly, it  would make no sense to fuse measurement data originating from dif- ferent targets. Hence, it is crucial that the data association operation be  effectively carried out. Here two sources of uncertainty exist, the uncer- tainty of the location of the previously detected object represented by  its associated estimate covariance listed in the inventory, as well as the  uncertainty of the current measurement represented by its associated  measurement  covariance.  If  the  error  between  a  previously  detected  target and the current measurement falls within a speciﬁ ed conﬁ dence  region, then the measurement is declared to have arisen from this pre- viously detected target and is used to update the proﬁ le for that par- ticular  target.  This  includes  updating  the  position  estimate  and  the  covariance. A  measurement  can  be  checked  against  the  i th   target  by  seeing if the inequality below is satisﬁ ed. Here it is seen that the com- bined covariance discussed above is used for the calculation required  in this gating operation.            x meas y meas  − −  ˆ x T  cid:1  y T  arg  et i  arg  et i   T      R  meas  +  P T  − 1     arg  et i     aas  x me y meas  − −  ˆ x T ˆ y T  arg  et i  arg  et i    ≤  γ       7.24       Choosing the best value for the probability P  and thus the associ- ated value for   γ  may present a challenge. If one sets P as large as 0.99   which corresponds to   γ= 9 2.   , then ninety - nine percent of the detec- tions which are from a given target will get associated with that target;  however, this corresponds to such a large ellipse that detections from  other  targets  may  also  get  associated  with  that  target.  Further,  some  detections may satisfy the gating inequality for more than one target  in  the  inventory.  One  approach  here  would  be  to  determine  all  the  targets in the inventory that meet the threshold for a given detection  and from these select that target from the inventory that is closest, i.e.,  that yields the smallest value for the LHS of inequality   7.24   to associ- ate with the new detection. If there are multiple detections at a given  time instant, one could require that only one detection be associated    INVENTORY OF DETECTED TARGETS     241  with a given target from the inventory at a given time instant and use  the closest one. In contrast, one could use all of those detections satisfy- ing the threshold for a given target in the inventory and weight them  according to their closeness to the target  see Bar Shalom . A simpler  approach would be to use a smaller value for P at the risk of missing  a  few  detections  in  order  to  prevent  the  incorrect  association  of  detections from other targets. The ﬁ nal solution requires a compromise  and  depends  on  the  density  of  targets  as  well  as  the  measurement  covariances.    If  no  target  in  the  inventory  yields  an  error  that  falls  within  this  conﬁ dence region for a given detection, then the source of the measure- ment  is  declared  to  be  a  new  target  and  a  proﬁ le  for  it  is  initiated  resulting in the inventory growing by one column.    Some examples of the inventory follow. It can be seen from Table    7.2a   that when the object of interest  was ﬁ rst detected, its location  based  on  the  sensor  measurement  was  computed  and  stored  in  the  Inventory as Target 1. The target was in the ﬁ eld of view until the fourth  measurement was taken i.e., time    =    4. Therefore, the proﬁ le of Target  1 was updated three times. When the fourth measurement was taken,     TABLE 7.2A   ﬁ eld of view of sensor     Inventory with single target in the                   X Coordinate     Y Coordinate     Covariance 11     Covariance 12     Covariance 21     Covariance 22     Time      X Coordinate     Y Coordinate     Covariance 11     Covariance 12     Covariance 21     Covariance 22     Time       Target 1      12.89     31.16     0.163     0     0     0.047     4       Target 1      12.89     31.16     0.163     0     0     0.047     4       Target 2      − 6.60     47.75     0.019     0     0     0.01     35      TABLE 7.2B   of view of sensor     Inventory with two targets in the ﬁ eld    242     TARGET TRACKING INCLUDING MULTIPLE TARGETS  TARGET X POSITION  measured-blue, estimated-red   TARGET Y POSITION  measured-blue, estimated-red   t  s r e e m X     16  14  12  10  8  6  4  2  0  –2  –4  Target 1  Target 2  t  s r e e m Y     48  46  44  42  40  38  36  34  32  30  Target 2  Target 1  0  5  10  25  30  35  0  5  10  25  30  35  15 20 T2-Frames  15 20 T2-Frames       Figure 7.9   the Regions of Conﬁ dence       X and Y Position of Target in the Field of View Along with Extremities of   a second object of interest was detected in the sensor ’ s ﬁ eld of view.  The  location  of  this  target  was  computed  and  checked  against  the  proﬁ le of the only target stored in the inventory. When the previously  detected  target  did  not  fall  within  0.9  conﬁ dence  region  surrounding  the current measurement, a new proﬁ le was created in inventory under  Target 2. This is shown in Table   7.2b  . It can also be seen from Table    7.2b  , that Target 1 disappeared after the fourth sensor measurement  was taken and was not detected again and that Target 2 was still visible  at the thirty - ﬁ fth sensor measurement.      Figure   7.9   further  illustrates  the  tracking  of  these  two  targets. The  current estimates of the coordinates of the targets are shown along with  the extremities of the regions of conﬁ dence. The lack of measurements  for Target 1 after the fourth sample is apparent as is the presence of  measurements for Target 2 through 35 samples.      The inventory also has the ability to handle the disappearance and  reappearance  of  targets  in  the  sensor  ﬁ eld  of  view. Appearance  of  a  target implies that an object of interest is present in the sensor ﬁ eld of  view and disappearance of a target implies that the object of interest  is absent from the sensor ﬁ eld of view. The proﬁ le of a target that has  been detected is updated until the target disappears. The most recent  estimates of the targets ’  locations as well as the covariance of error and  other  features  are  retained  at  the  time  of  dropout. When  the  target  reappears, the estimation process is resumed by picking up from esti- mates that existed at the time the target was last seen without any loss  of information. The proﬁ le of the target is again updated accordingly.   Figure  7.10  shows such an example. It can be seen from this ﬁ gure  that Target 1 was detected in the sensor ﬁ eld of view in frame 1. The    INVENTORY OF DETECTED TARGETS     243  TARGET X POSITION  measured-blue, estimated-red   TARGET Y POSITION  measured-blue, estimated-red   –0.5  –1  –1.5  –2  –2.5  –3  –3.5  t  s r e e m X     t  s r e e m X     10  8  6  4  2  0  –2  –4  46  45.5  45  44.5  44  43.5  t  s r e e m Y     43  0  t  s r e e m Y     46  45  44  43  42  41  40  39  –4  0  5  10  15 20 T2-Frames  25  30  35  5  10  25  30  35  15 20 T2-Frames       Figure 7.10   of the Regions of Conﬁ dence, Target Disappears from Frame 10 to Frame 15       X and Y Position of Target in the Field of View Along with Extremities   TARGET X POSITION  measured-blue, estimated-red   TARGET Y POSITION  measured-blue, estimated-red   Target2  Target1  Target2  Target1  0  5  10  15  20  25  30  35  40  0  5  10  15  20  25  30  35  40  T2-Frames  T2-Frames       Figure 7.11   of the Regions of Conﬁ dence       X and Y Position of Target in the Field of View Along with Extremities   proﬁ le for that target was then initiated. When the tenth measurement  was taken by the sensor, Target 1 disappeared from the ﬁ eld of view.  When  the  ﬁ fteenth  measurement  was  taken  by  the  sensor,  Target  1  reappeared  in  the  ﬁ eld  of  view.  The  estimation  process  resumed  by  using the latest information pertaining to Target 1 in the Inventory, and  the estimate of the location of this object was further reﬁ ned.      Figure   7.11   shows  another  example  of  a  target  dropout.  It  can  be  seen  from  this  ﬁ gure  that  initially  both  Target  1  and  Target  2  were  detected  in  the  sensor  ﬁ eld  of  view. The  proﬁ les  of  the  objects  were  updated as new measurements were taken. When the ninth measure- ment of the targets was taken by the sensor, Target 2 disappeared but  Target 1 remained continuously in the ﬁ eld of view of the sensor. The    244     TARGET TRACKING INCLUDING MULTIPLE TARGETS  proﬁ le of Target 2 was retained in the Inventory at the time of drop  out. When  the  twenty - eighth  measurement  was  taken  by  the  sensor,  Target 2 reappeared in the sensor ’ s ﬁ eld of view. The estimation process  resumed by using the latest information in the Inventory pertaining to  Target  2,  and  the  estimate  of  the  location  of  this  object  was  further  reﬁ ned.      In all of the above examples, it is apparent that the regions of con- vergence shrink rapidly as successive measurements are made and the  data are fused with the previous data.     EXERCISES          1.     The location of an object of interest has been estimated to be x    =    23   and y    =    8. The associated covariance matrix is     R =  09 .  . 01  . 01 04 .          Determine the ellipse bounding the region of conﬁ dence for prob- ability p    =    0.9 Plot this ellipse in the original x - y coordinates. Note  that  the  ellipse  should  be  centered  about  the  estimate.  If  desired,  you may ﬁ rst plot the ellipse in normal coordinates and then map  each point over to the original coordinates.         2.     A  mobile  robot  detects  an  object  of  interest  at  time    =    1.  Its  local  ground measurements are x    =    12 and y    =    21. The measurement cova- riance  including  all  components  robot  position  uncertainty  and  sensor error is     Rmeas =  . 04  01 .  . 01 05 .          At time    =    2 a detection is made at x    =    16 and y    =    18. The measure- ment covariance is the same as for the ﬁ rst detection. At time    =    3 a  detection is made at x    =    12.2 and y    =    20.75. The measurement cova- riance  is  again  the  same  as  for  the  previous  detections.  Show  the  Inventory for the ﬁ les after time    =    1, after time    =    2 and after time    =    3.  The inventory should list the targets detected up until that time, the  estimates  of  their  positions  and  the  associated  covariances.  For  testing  to  determine  if  a  new  detection  is  associated  with  a  target  being tracked, use an ellipse corresponding to p    =    0.9 and add the          REFERENCES     245  covariance of the target estimate to the covariance of the detection  measurement. Then check to see if inequality   7.23   is satisﬁ ed. An  easy  way  to  compute  the  covariance  associated  with  an  estimate  based on multiple measurements is to use        P  estimate  =     R  − 1  +  R  meas  2  − 1  +  1…      −     meas  1      3.     Two  objects  of  interest  are  being  tracked.  Their  estimated  55= locations  are    ˆ argxT   et1 The covariance matrix for the estimated location of target 1 is   ,  and    ˆ argxT  ,    ˆ argyT  ,    ˆ argyT  25=  45=  35=  et 2  et 2  et1  1  2  et  et  arg  arg    PT  . 02 . 03  . 01 05 .  =  . 05  . 02  =  . 04  01 .   . For  target  2  it  is   PT    . A new  measurement is taken with a sensor for which the measurement  noise has covariance   Rmeas =  . The sensor measurement   is    xmeas = 29,    ymeas = 41.  Determine  the  target  with  to  associate  this  measurement. Use as your criterion the one which yields the small- est value for the left side of the inequality   7.24  .      0  0 1 .  0 1 . 0      4.     Compute and plot the ellipses corresponding to the target covari- ances  given  in  Exercise  3.  Use  the  value  for    γ  corresponding  to  p    =    0.9.         5.     Compute and plot the ellipses corresponding to measurement    covariances   Rmeas1 . Use the value   for   γ corresponding to p    =    0.9. Compare the two plots. Do the shapes  correspond to any sensors discussed in the previous chapter?         0  and   Rmeas2  0 1 .  0 1 . 0  0 0 5 .  0 5 . 0  =    =      REFERENCES   Press ,  1988 .        Bar - Shalom ,  Y.    and   T.    Fortmann  ,   Tracking  and  Data Association ,  Academic       Bhatia    Indar  ,    Diehl    Vince  ,    Moore    Tim  ,    Marble    Jay  ,    Tran    Ky  ,    Bishop    Steve  ;   “  Sensor Data Fusion for Mine Detection from a Vehicle - mounted System  ”    SPIE  Vol  4038 ,  2000 .        Breuers  ,    M.  G.  J. ;   Schwering  ,    P.  B.  W. ;   van  de    Brock  ,    S.  P.  ,   “  Sensor  Fusion   Algorithms for the Detection of Landmines  ”   SPIE  Vol.  3710 .        Bryson ,  A. E.   and   Ho ,  Y - C  ,  “  Applied Optimal Control: Optimization, Estimation   and Control  ” ,  Blaisdell ,  Waltham, MA   1969 .     246     TARGET TRACKING INCLUDING MULTIPLE TARGETS      Cook ,  G.   and   Kansal ,  S.    “  Exploitation of Complementary Sensors for Precise  Ground  Registration  of  Sensed  Objects  ” ,   Proceedings  of  SPIE  Fourth  International Asia - Paciﬁ c Environmental Remote Sensing Symposium , Vol  5657  pp  20  –  29   Honolulu, Hawaii, USA  Nov. 8 – 11,  2004  .        Cook ,  G.  ,   Sherbondy ,  K.  , and   Jakkidi ,  S.  ,  “  Geo - Location of Mines Detected via  Vehicular - Mounted  Forward - Looking  Sensors  ” ,   Proceedings  of  SPIE  Conference  on  Detection  and  Remediation  Technologies  for  Mine  and  Minelike Targets VI , Vol. 4394,  pp  922  –  933 ,  Orlando, FL , April  16  –  20 ,  2001  .       Dana ,   M.P.    “  Registration: A  Prerequisite  for  Multiple  Sensor Tracking  ” ,  In   Multitarget - Multisensor Tracking: Advanced Applications , edited by   Y.   Bar -  Shalom  ,  1989 ,  Artech House .        Drummond ,  Oliver E.  ;  “  Multiple sensor tracking with multiple frames, proba- bilistic data association  ” ;  Proc SPIE  Vol 2561, Signal and data Processing  of Small Targets  1995 ,  Oliver E. Drummond ; Ed.        Gelb ,  A.  ,  ed.,   Applied  Optimal  Estimation .   Cambridge,  Massachusetts :   MIT   Press , : 1974 .        Jakkidi ,   S.    and    Cook ,   G.    “  Geo - location  of  Detected  Landmines  via  Mobile  Robots Equipped with Multiple Sensors  ” ,  Proceedings of IECON 2002 ,  pp   1972  –  1977   Seville, Spain , Nov. 4 – 8,  2002  .        Jakkidi ,   S.    and    Cook ,   G.     “  Landmine  Geo - location;  Dynamic  Modeling  and  Target Tracking  ”   IEEE Trans on Aerospace and Electronics , Vol.  41  No.  1    Jan.  ’ 05  pp  51  –  59 .        McFee ,  Aitken  ,   Chesney ,  Das   and  Russel ,  “  A Multisensor, Vehicle Mounted,  Teleoperated Mine Detector with Data Fusion  ” ,  Proceedings of SPIE  Vol.  3392.        Meditch ,  J. S.  ,  “  Stochastic Optimal Linear Estimation and Control  ” ,  McGraw -   Hill, NY, NY ,  1969 .        Reid ,  D. B.  ,  “  An Algorithm for Tracking Multiple Targets  ” ,  IEEE Transactionson   Automatic Control , Vol.  AC - 24 , No.  6 , December  1979 .        Thomopoulos ,  Ramanarayana   and  Bougoulias ,  “  Optimal Decision Fusion in  Multiple - Sensor Systems  ” ,  IEEE Transactions on Aerospace and Electronic  Systems , Vol.  AES - 23 ,  1987 , No.  5  pp  644  –  652 .        Tjuatja ,  S.  ,   J. W.   Bredow  ,   A. K.   Fung  , and   O. R.   Mitchell  ,  “  Combined Sensor  Approach to the Detection and Discrimination of Anti - Personnel Mines  ” ,   Proceedings  of  SPIE,  Detection  and  Remediation Technologies  for  Mines  and Mine - like Targets IV ,  Orlando, FL  April 5 – 9,  1999 , pp  864  –  874 .                8   OBSTACLE MAPPING AND ITS  APPLICATION TO ROBOT  NAVIGATION           8.0      INTRODUCTION    In  the  process  of  accomplishing  a  given  task,  the  mobile  robot  may  encounter obstacles other than objects of interest in its path. It is impor- tant to detect such obstacles before having a collision that could result  in damage to the robot or its instrumentation. Thus, a variety of sensors  may be operated as the robot moves along in order to provide advance  warning of obstacles. The precision need not be as great as that required  for  geo - registration  of  objects  of  interest,  but  it  must  be  sufﬁ ciently  precise to permit avoidance of the obstacle by the mobile robot without  requiring  excessive  spacing  or  miss - distance.  Some  of  the  techniques  described for detection and geo - registration of objects of interest may  also be used for detection and geo - registration of obstacles. Previously  detected and geo - registered obstacles may be in turn used as an aid in  navigation. This is especially important when the robot is isolated from  external  navigation  aids.  In  the  following,  an  introduction  to  these  topics will be presented. Some simpliﬁ cations will be made, especially  in  terms  of  the  nature  of  the  obstacles. The  reader  is  encouraged  to  consult the list of references at the end of the chapter on mapping and  localization  combined,  since  an  in - depth  treatment  of  this  topic  is  outside the scope of this book.    Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  247   248     OBSTACLE MAPPING AND ITS APPLICATION TO ROBOT NAVIGATION    SENSORS FOR OBSTACLE DETECTION AND      8.1   GEO - REGISTRATION    One important characteristic of a sensor to be used for obstacle avoid- ance is its operating range. Clearly, the greater is the range, the more  advance warning that can be provided to the robot. The speed at which  the robot is moving combined with the wheels or tracks and the type  surface it is traveling on determine the distance required to bring the  robot to a stop. The range of the obstacle detection sensor must exceed  this distance, or conversely, the robot must be operated at a speed such  that its stopping distance does not exceed the sensor range.    One type of sensing device is the scanning laser, which has ranges in  excess of 30 meters. Such devices have been used in the Darpa Grand  Challenge. These lasers are capable of taking range measurements at  yaw increments of a degree or less as they scan back and forth over the  operating range, which may be as large as 180 degrees. The results of  this type scan provide the information required for constructing a map  of obstacles in the immediate vicinity of the robot. This map would be  most easily constructed in vehicle coordinates, but could be converted  to earth coordinates given robot position and heading.    Light Detection and Ranging  Lidar  is an active sensor, similar to  radar,  that  transmits  laser  pulses  to  a  target  and  records  the  time  it  takes for the pulse to return to the sensor receiver. Lidar uses much  shorter wavelengths of the electromagnetic spectrum, typically in the  ultraviolet,  visible  or  near  infrared  range.  In  general,  it  is  possible  to  image a feature or object only about the same size as the wavelength,  or  larger.  A  laser  typically  has  a  very  narrow  beam  that  allows  the  mapping of physical features with very high resolution compared with  radar. Lidar has been used in adaptive cruise control systems for auto- mobiles. Here a lidar device is mounted on a front part of the vehicle,  such as the bumper, to monitor the distance between the vehicle and  any vehicle in front of it. In the event the vehicle in front slows down  or is too close, the system applies the brakes to slow the vehicle. When  the road ahead is clear, the system allows the vehicle to accelerate to  a speed that has been preset by the driver.    Ultra sound refers to sounds or pressure waves at frequencies above  those within the human hearing range. Typically the frequencies used  are  between  20   KHz  and  500   KHz  which  places  them  above  human  hearing and below AM radio. A common use of ultrasound is in range  ﬁ nding;  this  use  is  also  called  sonar,   sound  navigation  and  ranging .  This  works  similarly  to  radar.  An  ultrasonic  pulse  is  generated  in  a  particular direction. If there is an object in the path of this pulse, part    DEAD RECKONING NAVIGATION     249  or all of the pulse will be reﬂ ected back to the transmitter as an echo  and can be detected through the receiver path. By measuring the dif- ference in time between the pulse being transmitted and the echo being  received,  it  is  possible  to  determine  how  far  away  the  object  is. The  speed of sound in the atmosphere varies with pressure and at sea level  has a nominal value of 1,065 feet per second.    The higher frequency systems provide a narrower beam which can  provide greater detail of the object being sensed and greater precision  of its range. Systems operating at lower frequencies have a wider beam  but can measure greater distances with 25   KHz sensors advertised to  measure  from  0.3  meters  up  to  60  meters  with  the  aid  of  signal  processing.    Regardless of the type sensors used, the information provided allows  one to sequentially locate the detected obstacles in the current vehicle  coordinates. Ideally one would want to then convert the obstacle loca- tions to some common coordinate system and compile a map of all the  detected obstacles. This map would be useful for future obstacle avoid- ance  as  the  robot  goes  about  its  task  of  searching  for  objects  of  interest.     A mobile robot has used its sensors to geo - register two detected obstacles.  The coordinates for these obstacles in local coordinates are x  1     =    3  and  y  1     =    10  and x  2     =     − 10  and y  2     =    25 . Plot these obstacles on a grid surround- ing  each  with  a  2     m  radius  as  an  approximation  to  the  size  of  the  obstacles.        EXAMPLE 1     SOLUTION 1     Figure     8.1     shows  the  locations  of  the  obstacles  and  the  additional  boundary.            8.2      DEAD RECKONING NAVIGATION    When  the  robot  is  isolated  from  external  navigation  aids  such  as  GPS satellites, one could still use inertial navigation or odometry for  tracking  the  robot  in  between  external  navigation  updates.  In  the  case of inertial navigation, the buildup of drift over time may limit its  application to relative motion. Wheel slippage causes the same type of    250     OBSTACLE MAPPING AND ITS APPLICATION TO ROBOT NAVIGATION  30  28  26  24  22  20  18  16  14  12  10  8  i  t  e a n d r o o c   y  -12  -10  -8  -6  0  2  4  6  -4  -2 x coordinate       Figure 8.1       Plot Showing the Coordinates of the Two Detected Obstacles    problem with odometry. The following example illustrates the buildup  of errors possible when relying totally on dead reckoning.      EXAMPLE 2     A mobile robot using dead reckoning via odometry travels along what  was intended to be a straight line. The wheels have radius of .15     m. Each  side underwent 30 simultaneous rotations; however, the wheel on the left  side spun without moving forward at the very beginning of the trajectory  while the wheel on the right side traveled forward 0.1     m. Determine the  path based on dead reckoning as well as the actual path. The width of  the robot is 1     m.       SOLUTION 2     Based on 30 simultaneous rotations of the wheels with a circumference  of  2 π R  where  R  is  .15     m,  the  distance  traveled  by  each  side  would  be  28.2743     m. The computed trajectory would be along the y axis.      DEAD RECKONING NAVIGATION     251     Computed Trajectory Actual Trajectory  i  t  e a n d r o o c   y  30  25  20  15  10  5  0    -4  -3.5  -3  -2.5  -2  -1.5  -1  -0.5  0  0.5  1  x coordinate       Figure 8.2       Plot Showing the Computed Trajectory and the Actual Trajectory      Now utilizing the knowledge that the right side advanced forward 0.1     m at  the beginning of the trajectory while the left side was stationary and recall- ing that the robot width is 1     m means that there was an undetected robot  turn to the left by the amount    ψ= . deg 5 71 .   Thus the actual path was a straight line headed off to the left by this com- puted angle. A plot of the trajectories is shown in Figure    8.2  .       1 0 1 1  or   ψ= −  tan   .  . 0997  rad       =   The above example illustrates how a small heading error at the begin- ning of a trajectory can propagate into a large displacement error when  using dead reckoning. In the absence of better navigation, this would  in  turn  create  errors  in  geo - registration  of  any  detected  obstacles.  In  addition to the geo - registration errors caused by imperfect knowledge  of the robot position, there are also errors caused by imperfect knowl- edge of the robot orientation. The next example illustrates this point  with navigation errors in both position and heading of the robot.      EXAMPLE 3     An obstacle is detected at coordinates x     =    3  and y     =    10  in robot coordi- nates. Take the robot coordinates at the time of this detection as the origin    252     OBSTACLE MAPPING AND ITS APPLICATION TO ROBOT NAVIGATION  of  a  local  coordinate  system,  i.e.,  the  robot  is  at  x l      =    0   and  y l      =    0   with  heading  ψ  l      =    0  when the obstacle is detected at x l      =    3  and y l      =    10 . Next  the robot moves to a location computed via imperfect navigation to be  x l      =    0   and  y l      =    15   with  computed  heading   ψ  l      =    0 . At  that  time  another  obstacle is detected and correctly located in robot coordinates at x r      =     − 10   and y r      =    10 . In the local coordinate system its location is computed to be  x l      =     − 10  and y l      =    25.      a        Plot  the  coordinates  of  the  detected  obstacles  in  the  x l ,  y l   plane.  Surround the coordinates of the detected obstacles with a circle of  radius 2 as an approximation of the space occupied by the obstacle.        b        Assume that the computed position and heading of the robot at  the time of the second obstacle detection are erroneous and that  in  fact  the  robot ’ s  location  was  x l      =    .5   and  y l      =    14   with  heading   ψ  l      =     − .1  π . Compute the actual location of the second obstacle and  plot  its  coordinates.  Surround  the  coordinates  of  the  detected  obstacles with a circle of radius 2 as an approximation of the space  occupied by the obstacle.          SOLUTION 3     The second obstacle was correctly located in robot coordinates at x r      =     − 10   and  y r      =    10 ,  but  the  conversion  to  local  coordinates  was  erroneous  because  the  robot  heading  and  location  were  incorrect.  The  location  of the robot in the local coordinate system was actually x l      =    .5  and y l      =    14   ψ  l      =     − .02  p .  Therefore   with  heading  is  at  the  second  obstacle  π   02 ..5 x   or x l      =     − 8.8524  and y l      =    24.6082 .  l     π 02 14 y l Figure    8.3    shows the computed coordinates of the obstacles and Figure    8.4   shows their actual coordinates.        − 10  10  cos. − sin.  sin. cos.  π   π    =    +  02 02         The above example illustrates how a map of detected obstacles may  be in error when the robot coordinates are determined by erroneous  navigation means.      USE OF PREVIOUSLY DETECTED OBSTACLES      8.3   FOR NAVIGATION    Any  planned  paths  for  the  robot  must  take  into  account  the  map  of  obstacles,  and  the  path  must  be  modiﬁ ed  as  necessary  to  avoid  the    30  28  26  24  22  20  18  16  14  12  10  8  30  28  26  24  22  20  18  16  14  12  10  8  e  t  i  a n d r o o c   y  e  t  i  a n d r o o c   y  -12  -10  -8  -6  0  2  4  6  -4  -2 x coordinate       Figure 8.3       Plot Showing Computed Coordinates of Two Detected Obstacles    -12  -10  -8  -6  0  2  4  6  -4  -2 x coordinate       Figure 8.4       Plot Showing Actual Coordinates of Two Detected Obstacles    253   254     OBSTACLE MAPPING AND ITS APPLICATION TO ROBOT NAVIGATION  obstacles. This is a primary use of the obstacle map. In addition to this  application, the following example illustrates how one might also use  this map of obstacles as an aid in navigation.     A  mobile  robot  has  measured  the  range  from  itself  to  two  previously  detected  and  geo - registered  obstacles. The  coordinates  of  the  obstacles  along  with  their  respective  ranges  from  the  robot  are  x  1     =    10 ,  y  1     =    25 ,  d  1     =    20  and x  2     =     − 4 , y  2     =    30 , d  2     =    23 . Determine the location of the robot  using these obstacle locations and the distances.        EXAMPLE 4     SOLUTION 4     The solution to the problem is the set of values for x and y which simul- taneously satisfy   −  x x 1        2     +  −  y y 1     2     =  2   d 1  −  x x        2     2  +  −  y y 2     2     =  2    d 2    By  carrying  out  the  squaring  operations  and  then  subtracting  the   second equation from the ﬁ rst equation one obtains     2    x x  2  −  +  x 1     2    y y 2  −  y 1     =  2 d 1  −  2 d 2  −  2 x 1  +  x  2 2  −  2 y 1  +  2 y 2        Solving this for y yields   = −     y       x 2 y 2  − −  x 1 y 1       +  x     2 d 1  −  2 d 2  − 2 x 1 2   y 2  + 2 x 2 − y 1  −    +  2 y 1  2 y 2           Substituting this expression into either of the ﬁ rst two equations yields  a  quadratic  equation  in  y  whose  solution  is  well  known.  Using  the  numbers  for  this  example  one  obtains  for  the  location  of  the  robot  x r      =    13.73  and y r      =    46.65  or x r      =    .44  and y r      =    7.43 . Figure    8.5    shows the  plots of the obstacle locations and the two possible robot locations.        One must use other information such as knowledge of the approxi-  mate location to decide which solution is applicable.       and                 USE OF PREVIOUSLY DETECTED OBSTACLES FOR NAVIGATION     255  Geo-Registered Obstacle Computed Robot Position     i  t  e a n d r o o c   y  50  45  40  35  30  25  20  15  10  5  0     -20  -10  20  30  0 x coordinate  10       Figure 8.5   Robot Locations       Plot Showing the Two Geo - Registered Obstacles and the Two Computed    The above example illustrates how one can combine distances from  two geo - registered obstacles and determine the location of the mobile  robot. If there are three or more obstacles, the question becomes how  to incorporate the extra information. Any combination of two obstacles  yields a set of solutions. The idea of a least squares solution comes to  mind. A bit of reﬂ ection suggests that this is really very similar to the  problem  of  determining  the  location  of  the  robot  using  GPS.  Here  the  obstacles  play  the  role  of  the  GPS  satellites,  and  the  dimension  of the search space has been reduced from three to two.      EXAMPLE 5     A mobile robot has measured in local coordinates the bearing from itself  to two previously detected and geo - registered obstacles. The coordinates  of the obstacles along with their respective bearings from the robot are  π x  1     =    10 , y  1     =    25,   ψ  . Determine the  location of the robot using these obstacle locations and the distances.       and x  2     =     − 4 , y  2     =    30,   ψ π  = −   1 8  =   1 8  2  1   256     OBSTACLE MAPPING AND ITS APPLICATION TO ROBOT NAVIGATION    SOLUTION 5     This problem is different from the previous one in that the bearing of  the ray from the robot to the obstacle is measured rather than the range.  The  type  measurement  available  is  of  course  dependent  on  the  sensor  available. As was seen in Chapter 6, one   could measure bearing with a  digital camera without need for a ranging device.      Letting  the  coordinates  of  the  robot  be  represented  by  x  and  y,  one   may write                and    or    and      tan  ψ1  − = −     x x 1 −     y y 1        tan  ψ2  = −   x   y 2  −   x 2 −   y        −    x 1  −  x     =     y 1  −   tan y  1ψ       −    x  2  −  x     =     y 2  −   tan y  2ψ       These  two  linear  equations  can  be  solved  for  x  and  y.  Using  the  numbers for this example yields x     =    4.0919  and y     =    10.4645  Figure    8.6     shows the coordinates of the two obstacles and the computed coordinates  of the robot.         The previous examples illustrate that if one has prior knowledge of  the surroundings, sensed obstacles may provide an alternative means  of navigation or a means of re - calibrating imperfect navigation to offset  drift. However, if the map is imperfect, then the resulting computations  will yield imperfect results. This is illustrated in the next example.      EXAMPLE 6     A  mobile  robot  has  measured  the  range  from  itself  to  two  previously  detected and geo - registered obstacles. The computed coordinates of the    USE OF PREVIOUSLY DETECTED OBSTACLES FOR NAVIGATION     257     Geo-Registered Obstacle Computed Robot Position  i  t  e a n d r o o c   y  30  28  26  24  22  20  18  16  14  12  10    -6  -4  -2  0  2  4  6  8  10  12  14  16  x coordinate       Figure 8.6   Location Using Bearings Only Measurements       Plot Showing the Two Geo - Registered Obstacles and the Computed Robot   obstacles  are  slightly  in  error. These  computed  coordinates  along  with  their respective ranges from the robot are x  1     =    10.8 , y  1     =    24.2 , d  1     =    20  and  x  2     =     − 4.5 , y  2     =    31.2 , d  2     =    23 . Determine the location of the robot using the  computed obstacle locations. Compare the results with those of the earlier  example where the true coordinates for the obstacles were known.       SOLUTION 6     Using the computed coordinates of the obstacles the robot position was  computed  to  be  x r      =    14.7358   and  y r      =    43.8089   or  x r      =     − 1.4638   and  y r      =    8.4013  A plot of these coordinates is shown in Figure    8.7  .      The actual location based on correct coordinates for the obstacles is  repeated  here  for  comparison  purposes  x r      =    13.73   and  y r      =    46.65   or  x r      =    .44  and y r      =    7.43.      The  predicament  should  be  apparent.  In  unfamiliar  surroundings  with  limited  navigation,  one  may  detect  and  geo - register  obstacles,  but  if  the  robot ’ s  position  and  orientation  are  in  error,  then  the    258     OBSTACLE MAPPING AND ITS APPLICATION TO ROBOT NAVIGATION  Geo-Registered Obstacle Computed Robot Position     i  t  e a n d r o o c   y  50  45  40  35  30  25  20  15  10  5  0     -20  -10  20  30  0 10 x coordinate       Figure 8.7   Robot Locations       Plot Showing the Two Geo - Registered Obstacles and the Two Computed   geo - registration will also be in error. Later when one of these previ- ously geo - registered obstacles is again within the ﬁ eld - of - view of the  robot,  it  may  be  used  as  a  source  of  navigation.  However,  if  its  geo -  registration  was  in  error,  the  aid  provided  toward  improved  robot  navigation will be in error.    The  navigation  and  mapping  problems  must  be  simultaneously  addressed. When neither the robot coordinates nor the obstacle coor- dinates are precisely known, updates must take this into account and  must weight each piece of information according to its degree of known  precision. The result of new measurements must be to not only update  the position and orientation of the robot, but also to update the coor- dinates of the previously geo - registered obstacles.      SIMULTANEOUS CORRECTIONS OF COORDINATES      8.4   OF DETECTED OBSTACLES AND OF THE ROBOT    The model for estimates in such a situation as that described at the end  of the previous section must include as its state the obstacle locations    SIMULTANEOUS CORRECTIONS OF COORDINATES     259  as well as the location and orientation of the robot. The measurements  will include terms such as the relative location of obstacles with respect  to the robot. The precision of knowledge of the state of the robot and  the  precision  of  knowledge  of  the  location  of  the  obstacle  will  be  reﬂ ected  through  the  covariance  matrices  of  each. The  covariance  of  the measurement noise will also play a role. The impact of updates as  a result of measurements will depend on these covariances and their  relative magnitudes.    Consider  a  simple  one - dimensional  problem  where  the  robot,  which  is  stationary,  and  the  obstacle  are  constrained  to  lie  on  a  line.  There  are  current  estimates  of  the  location  of  each.  Now  suppose  a  measurement of the distance from the robot to the obstacle is taken.  If  the  result  of  this  distance  measurement  is  greater  than  what  the  estimated locations would yield, then the new estimates must be such  as to have the robot and obstacle farther apart depending on the reli- ability  of  the  measurement.  Which  estimate  must  move  the  most  depends  on  the  relative  covariances  of  the  robot  location  and  the  obstacle location.    For the more general problem, one can model the composite system          X X  robot  obstacle  + 1     k + 1     k    =       f X     , k u       k  robot X  obs  ttacle  robot     k     +    w w  robot  obstacle      k     k       as        with output       y k  +    1  =    h X  robot  +     k   , 1  X  obstacle     k  +  +     1    v k  +    1      One then proceeds to develop a Kalman Filter to estimate the total  state of the system. Here this includes the state of the robot as well as  the location of the obstacle. In the above model it has been assumed  that the obstacle is stationary. Nevertheless the inclusion of the distur- bance term in the obstacle model is required in order to provide the  possibility of updates in the obstacle location as more measurements  are  taken.  Provision  has  been  made  in  the  robot  model  for  possible  robot motion between measurements. The exact form of the  h  vector  depends on what speciﬁ cally is being measured. It could be the distance  from the robot to the obstacle. It might also include the yaw angle of  the vector from the robot to the obstacle.    This approach is illustrated in the example below.    260     OBSTACLE MAPPING AND ITS APPLICATION TO ROBOT NAVIGATION     EXAMPLE 7     Consider the situation where a robot has previously computed its own  location and that of an obstacle in a local coordinate system. The robot  once  more  measures  the  x  and  y  distances  to  the  obstacle.  Use  these  distance  measurements  to  update  the  robot  location  and  the  obstacle  location. The covariance of error in the robot location is      P  robot  =    P − robot x 0  0  P − robot y         and the covariance of error in the obstacle location is      P  obstacle  =    P − robstacle x  0  0  P − obstacle y           The covariance of the distance measurements is given by    R    Assume  that  the  robot  has  been  stationary  between  measurements  so    . A similar equation would hold for   that             k k        k k      k     k  ˆ x robot ˆ y robot  =    ˆ x robot ˆ y robot     =  the stationary obstacle.       k   k  R x 0  0 R y  + +  1 1   .          SOLUTION 7      1  k  +  ˆ T  HX k  +  [    − 1   1 1       k H HP k    The  estimation  equation  is  given  by      ˆ   X k   +  +    1  where    K k Y k + = + +       1   K k P k        +   1       the  gain  − 1 .     1 ] k H R  x robot  y  robot  x obstacle   y obstacle    Take the state vector to be    X  k +  =  T  +  + = ˆ     X k is  given  by        k  1    The output measurements are  the x and y distances measured in the positive direction from the robot  1 = to  the  obstacle.     y    yielding  0    H =  1     and     y  − 1 0  x obstacle  y obstacle  0 − 1  y robot  2 =  1 0     robot  −  −  x  .     The result of applying these deﬁ nitions and operations to the estima-  tion problem yields on an elemental basis    SIMULTANEOUS CORRECTIONS OF COORDINATES     261  ˆ x  robot  +     k  1     k  +  =  ˆ x  1          k k  −  robot −  y 1       ˆ x  P − robot x −       k k obstacle −  ˆ y robot     k  +  1     k  +  1    =  ˆ y rrobot        k k  P − robot y −        k k +  P − robot x + ++ R P − obstacle x x ˆ         x k k robot P − robot y + + P R − obstacle y y ˆ         y k k robot PP − obstacle x + + P − obstacle x         k k      P − robot x − ˆ       k k x rrobot P + obstacl + + P P − − obstacle x robot x − ˆ           kk    k k y k robot  − ee y  R x  R x  −     y 2    ˆ y obstacle        k k  ˆ x obstacle −  y 1     ˆ y obstacle −  y 2       ˆ x obstacle        k k    ˆ y obstacle         ˆ x obstaclet  +     k  1     k  +    1  =  ˆ y obstaclet     k  +  1     k  +    1  =    In order to perform some numerical comparisons let    ˆ x   ˆ y robot   y1  = 0,    ˆ x obstacle 3=  .  = 50   and     ˆ y obstacle        k k   and    y2        k k        k k  37=  = 10,  = 0 .    Assume  that          k k  robot    Tables    8.1a    and    8.1b    give the resulting updated estimates for the speci-  ﬁ ed covariances.        Note that both the robot and obstacle locations are updated. The cor- rections on robot and obstacle locations are equal because of the equal  values for their covariances. The distances computed based on the new  estimates do not match the measurements perfectly because the measure- ments themselves are not totally reliable.     If the covariances are not all equal, the residuals have different affects  on the estimates as Tables    8.2a    and    8.2b    illustrate. Here    ε  is intended to  represent a very small positive number.        TABLE 8.1A       P robot - x        1.0       First set of covariances for above example      P obstacle - x          P robot - y         1.0        1.0         R x        1.0         P obstacle - y         1.0         R y        1.0       Estimates of obstacle location and robot location before and after     TABLE 8.1B   measurements            Estimate before measurement       Estimate after measurement         x robot       10     11        y robot        0      − 1        x obstacle          y obstacle        50     49      0     1     262     OBSTACLE MAPPING AND ITS APPLICATION TO ROBOT NAVIGATION    TABLE 8.2A       P robot - x         ε       1.0       Second set of covariances for above example      P obstacle - x          R x         ε        P robot - y         ε        P obstacle - y         1.0         R y         ε      Estimates of obstacle location and robot location before and after     TABLE 8.2B   measurements            Estimate before measurement       Estimate after measurement         x robot       10     10        y robot          x obstacle          y obstacle        0     0      50     47      0     3      In this second case, it is seen from the table on covariances that the  distance measurements are very reliable and that the estimated location  of  the  robot  is  also  very  reliable,  while  the  estimation  of  the  obstacle  location is relatively unreliable. Thus the update results in a new estimate  for  the  obstacle  location  but  the  same  estimate  for  the  robot  location.  Here the computed distances between the estimated location of the robot  and the estimated location of the obstacle in both dimensions match the  measurements because of their reliability.       The  foregoing  is  intended  to  introduce  the  reader  to  some  of  the  considerations one is faced with when navigation capability is limited  and obstacles are present. For situations where obstacles are not iso- lated  but  rather  are  frequent  and  distributed,  as  would  be  the  case  inside a building, this problem becomes much more difﬁ cult. Detections  of boundaries of rooms, doorways, and furnishings are all part of the  problem. Whatever usable navigation is on the robot must also be uti- lized  and  incorporated  as  is  appropriate  for  its  degree  of  accuracy.  Clearly  this  problem  area  is  complex  with  errors  in  one  part  of  the  process feeding into the other and vice versa. This is why simultaneous  localization and mapping,  SLAM , is such an important area of ongoing  research within the area of mobile robotics.     EXERCISES          1.     There are two obstacles at x, y locations of  − 10, 20 and 18, 29 in local  coordinates.  Measurements  taken  from  the  mobile  robot  to  each  obstacle produce distances of 28 and 39 respectively. Find the two  possible locations of the robot.      REFERENCES     263      2.         A       Odometer  readings  for  the  two  rear  wheels  of  a  mobile robot  are  as  follows:    θleft k 200…   and    θright k    =      + , …   all  in  radians.  The  wheel  radius  is  1 2   k k 0.15   m.  Using  dead  reckoning  determine  and  plot  the  robot  trajectory in x,y space. Also plot robot heading versus k.    2 1000   ;  = 1 2 ,  ; k k  200  =  =  k     B       At time  k     =    200 an obstacle is detected directly in front of the  robot at a distance of 10   m. Determine the x,y coordinates of  the obstacle.        m  m      3.     Consider  a  one - dimensional  location  problem  where  the  mobile  robot = 10  and an obstacle is estimated  robot is estimated to be at   ˆy obstacle = 30 . With no robot motion having occurred, a new  to be at   ˆy measurement is made of the distance from the robot to the obstacle  and it is found to be  23     m . Compute new estimates of both the robot  location and the obstacle location for the different sets of covariance  values shown in the table below. Here   ε is intended to represent a  very small number.    P robot      P obstacle      R meas       1.0     1.0       ε      1.0       ε     1.0        ε     1.0     1.0      1.0     1.0     1.0        4.     A mobile robot sees three obstacles whose coordinates have been  previously  estimated. The  distances  to  the  obstacles  are  measured  and are to be used to estimate the location of the robot. Develop  and describe a scheme for using these three pieces of information  to determine the two coordinates of the robot location. Hint: Read  the discussion following the example where distances to two obsta- cles are used to determine the location of the robot.          REFERENCES       Chieh - Chih   Wang  ;   Thorpe ,  C.  ;  “  Simultaneous localization and mapping with  detection and tracking of moving objects  ” ,  Robotics and Automation ,  2002 ,  Page s :  2918  –  2924  vol. 3 .        Durrant - Whyte ,  H.  ;   Bailey ,  T.     2006  .  “  Simultaneous Localisation and Mapping   SLAM :  Part  I  The  Essential  Algorithms  ” .   Robotics  and  Automation  Magazine   13 :  99  –  110 .        Kansal ,  S.  ,   G.   Cook  ,   K.   Sherbondy   and   C.   Amazeen    “  Use of Fiducials and Un -  surveyed Landmarks as Geo - Location Tools in Vehicular Based Landmine  Search  ” ,  IEEE Transactions on Geoscience and Remote Sensing , Vol.  43 ,  No.  6   June  ’ 05  pp  1432  –  1439 .     264     OBSTACLE MAPPING AND ITS APPLICATION TO ROBOT NAVIGATION      Leonard ,   J.J.  ;    Durrant - whyte ,   H.F.      1991  .   “  Simultaneous  map  building  and  localization  for  an  autonomous  mobile  robot  ” .   Intelligent  Robots  and  Systems ’   91. ‘ Intelligence  for  Mechanical  Systems,  Proceedings  IROS ’ 91.  IEEE RSJ International Workshop on :  1442  –  1447 .        Smith ,   R.C.  ;    Self ,   M.  ;    Cheeseman ,   P.      1986  .   “  Estimating  Uncertain  Spatial  Relationships in Robotics  ” .  Proceedings of the Second Annual Conference  on Uncertainty in Artiﬁ cial Intelligence . UAI  ’ 86. University of Pennsylvania,  Philadelphia, PA, USA: Elsevier. pp.  435  –  461 .        Wolf ,  Denis   and   Gaurav S.   Sukhatme  ,  “  Online Simultaneous Localization and  Mapping  in  Dynamic  Environments  ” ,   Proceedings  of  the  Intl.  Conf.  on  Robotics and Automation ICRA New Orleans, Louisiana , Apr,  2004 .                9   OPERATING A ROBOTIC  MANIPULATOR           9.0      INTRODUCTION    This chapter is devoted to the treatment of a simple three - degree - of -  freedom robotic manipulator. Several possible tasks for mobile robots  were discussed in the beginning of this book. Some of these, such as  retrieval of a target, material transfer, inspection of a surface, etc., could  require the use of a robotic manipulator mounted on the mobile robot.  Another application would be for positioning a sensor close to a target  for  more  detailed  analysis. The  forward  and  inverse  kinematic  equa- tions are presented and analyzed.       9.1      FORWARD KINEMATIC EQUATIONS    Figure  9.1  shows the manipulator to be studied.      The workspace for this robotic manipulator is a portion of a sphere  3+  centered   l1 units above the base of the robotic manipula- of radius   l l tor. The lower limits of the workspace will depend on practical consid- erations in the construction of the manipulator.   2  Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  265   266     OPERATING A ROBOTIC MANIPULATOR  θ  1  l2  l 3  θ  3  θ  2  l1       Figure 9.1   and Links 1, 2 and 3       Robotic Manipulator: Waist, Shoulder, Forearm with Angles  θ  1 ,  θ  2  and  θ  3     In  analyzing  this  manipulator,  ﬁ rst  we  shall  write  the  equations  for  expressing  the  end  effector  position  in  Cartesian  coordinates  in  terms of the robotic joint angles. These are called the forward kinematic  equations. A  frame  attached  to  the  base  of  the  robotic  manipulator  shall be the coordinate frame used for this speciﬁ cation. Clearly a point  speciﬁ ed in another coordinate system such as vehicle coordinates or  earth  coordinates  could  be  converted  to  these  coordinates  and  vice  versa.    First note that the horizontal component of the vector from the base   of the robotic manipulator to the end effector is given by  2      x  +  2  y  =  l  2  cos  θ 2  +  l  3  cos   θ θ        +  3  2    9.1       This can be seen from Figure  9.2  in the plane of the links.     This may be further broken down into its two components. Figure    9.3  shows the top view of the manipulator.      The   x  component is given by this horizontal component combined   with the waist angle and is given by     x  = −  { cos l  2  θ 2  +  l  3  cos   θ θ 2 3  +   }sin  θ      1   Likewise the   y component is given by  =     y  { cos l  2  θ 2  +  l  3  cos   θ θ 2 3  +   }cos  θ      1    9.2        9.3                FORWARD KINEMATIC EQUATIONS     267       Figure  9.2   Links 1, 2 and 3       Schematic  Illustration  of  the  Robotic  Manipulator:  Side View  Showing   l2  θ  2  θ  3  l3  l2 c2  + l3 c23  l1  Y  l3 c23  - 1θ  l2 c2     Schematic  Illustration  of  the  Robotic  Manipulator:  Top  View  Showing   X       Figure  9.3   Links 2 and 3    given by   Finally the vertical projection   zof the end effector from the base is   = +  θ 2  +  θ θ        +  2  3  2  3     l  l  sin  sin      z l 1    9.4      These expressions for   x,   y and   z deﬁ ne the forward kinematic equa- tions for the manipulator, i.e., they deﬁ ne the Cartesian coordinates of  the end effector in terms of the robot joint angles. For more complex  robotic  manipulator  conﬁ gurations  one  may  utilize  the  Denavit -  Hartenberg parameters to describe each link and from these form the  homogeneous  transformation  matrix  which  describes  end - effector  position and orientation as a function of joint angles.    268     OPERATING A ROBOTIC MANIPULATOR     EXAMPLE 1   0=   degrees,    θ2    Take    θ1 0=   degrees. Let L 1    =    1 , L 2    =    2   and L 3    =    2 . Use the forward kinematic equations to determine the posi- tion of the end effector.     0=   degrees and    θ3  0=    degrees,     θ2    Take     θ1 = −    degrees.  Let  L 1    =    1 ,  L 2    =    2   and  L 3    =    2 .  Use  the  forward  kinematic  equations  to  determine  the position of the end effector.        degrees  and     θ3  90=  90  0=    degrees,     θ2  = −    degrees.  Let  L 1    =    1 ,    Take     θ1 L 2    =    2   and  L 3    =    2 .  Use  the  forward  kinematic  equations  to  determine  the position of the end effector.        degrees  and     θ3  45=  90    SOLUTION 1     x     =    0 , y     =    4 , z     =    1      EXAMPLE 2     SOLUTION 2     x     =    0 , y     =    2 , z     =    3      EXAMPLE 3     SOLUTION 3     x     =    0 , y     =    2.828 , z     =    1      EXAMPLE 4     SOLUTION 4     x     =      −  2 , y     =    2 , z     =    1      45=     degrees,     θ2    Take     θ1 = −    degrees.  Let  L 1    =    1 ,  L 2    =    2   and  L 3    =    2 .  Use  the  forward  kinematic  equations  to  determine  the position of the end effector.        degrees  and     θ3  45=  90   PATH SPECIFICATION IN JOINT SPACE     269     9.2      PATH SPECIFICATION IN JOINT SPACE    Given a trajectory i.e., a sequence of points in joint space, one could  use these equations repeatedly to determine the trajectory, or sequence  of points, of the end effector in Cartesian space.     Starting  from  reaching  toward  the  front  and  swinging  around  to  the  right  and  up.  Let     θ +  4 4    for  k     =    1,2,3  . . .  20  Plot the path of the end effector.     ,    θ π3 = −    ,    θ π2 2  100  = −  π        k   k  20  =  ∗     1     EXAMPLE 5     SOLUTION 5     The  plots  showing  the  solution  to  this  example  are  shown  in  Figures     9.4a – 9.4d  .      e  t  i  a n d r o o c   x  3.5  2.5  3  2  1  1.5  0.5  0 0  2  4  6  8  12  14  16  18  20  10 Time       Figure 9.4a       Plot of x Coordinate of End Effector versus Time     3.5  2.5  3  2  1  1.5  0.5  0 0  3.2  3.1  3  2.9  2.8  2.7  2.6  2.5  i  e t a n d r o o c   y  e  t  i  a n d r o o c   z  2  4  6  8  12  14  16  18  20  10 Time       Figure 9.4b       Plot of y Coordinate of End Effector versus Time    0  2  4  6  8  12  14  16  18  20  10 Time       Figure 9.4c       Plot of z Coordinate of End Effector versus Time    270   INVERSE KINEMATIC EQUATIONS     271  e  t  i  a n d r o o c   z  3.2  3  2.8  2.6  4  3  2  1  y coordinate  0  0  2  1  x coordinate  4  3       Figure 9.4d       Path of End Effector in xyz Space      Reaching to the front and downward. Let    θ1 and     θ k     effector.     100  = −  π    +  k  4     3  100      ;  k     =    1,2,3  . . .  20   Plot  the  path  of  the  end   4     2     k = ,   θ  0  k      =  π    −  k    EXAMPLE 6     SOLUTION 6     The  plots  showing  the  solution  to  this  example  are  shown  in  Figures     9.5a – 9.5c  .           9.3      INVERSE KINEMATIC EQUATIONS    Often one needs to solve the inverse kinematic equations and obtain  expressions for the robot joint angles in terms of the desired end - effector    i  e t a n d r o o c   y  e  t  i  a n d r o o c   z  3.7  3.65  3.6  3.55  3.5  3.45  3.4  0  2.5  2.45  2.4  2.35  2.3  2.25  2.2  2.15  2.1  0  2  4  6  8  12  14  16  18  20  10 Time       Figure 9.5a       Plot of y Coordinate of End Effector versus Time    2  4  6  8  12  14  16  18  20  10 Time       Figure 9.5b       Plot of z Coordinate of End Effector versus Time    272   INVERSE KINEMATIC EQUATIONS     273  e  t  i  a n d r o o c   z  2.5  2.4  2.3  2.2  2.1 3.7  3.6  3.5  y coordinate  3.4  -1  0  -0.5  x coordinate  1  0.5       Figure 9.5c       Path of End Effector in xyz Space    l2  θ 2  l3  θ  3+  π  α  l1  2  x  +  2  y       Figure 9.6   Angles Deﬁ ned       Schematic Drawing of Side View of Robotic Manipulator with Intermediate   position in Cartesian coordinates. These solutions would permit one to  start with a desired position for the end - effector and determine the joint  angles required to accomplish this position. Figure  9.6  presents a sche- matic drawing of the side view of robotic manipulator with intermediate  angles deﬁ ned.      274     OPERATING A ROBOTIC MANIPULATOR   One  must  ensure  that  the  speciﬁ ed  end - effector  position  is  within   reach in order to guarantee a solution. This requires that      x  2  +  2  y  +  − z l 1     2     ≤ +  l  2      l  3   Given that the speciﬁ ed end - effector position is within reach, one may   proceed. From the expressions for   x and   y one obtains the equation     tan    θ1 = −x y        θ1  =  − 1  tan  − x  y           Using the law of cosines, the square of the distance from the joint 2   of the manipulator to the end effector can be determined as  2     x  +  2  y  +  − z l 1     2     =  + −  l  2 3  l  2 2  2  l l 2 3  cos   θ π      +  3    9.5      2     x  +  2  y  +  − z l 1     2     =  + +  l  2 3  l  2 2  2  l l 2 3  cos    θ    3  which yields     θ3  =  − 1  cos     2  x  +  2  y  +     2     − −  l  2 2  l  2 3  − z l 1 2 32 l l            9.6       Note that the inverse cosine operation yields a positive or negative  angle. For practical purposes   θ3 is given the negative solution, i.e., elbow  up. Thus  the  inverse  solution  is  required  to  be  between  0  and   − 180  degrees. Thus if the cosine is a positive value, the angle is taken to be  between 0 and  − 90 degrees, and if the cosine is a negative value, the  angle is taken to be between  − 90 degrees and  − 180 degrees.   Consider again the side view of links 2 and 3. The angle   α represents  the angle of the vector from joint 2 to the end - effector with respect to  horizontal. It satisﬁ es the equation     tan   α =  − z l 1 + 2 y x  2    or  or                        INVERSE KINEMATIC EQUATIONS     275     α=  − tan 1      − z l 1 + 2 y x  2             9.7       Once this has been computed, one can again use the law of cosines      l  2 3  =  l  2 2  +  2  x  +  2  y  +  − z l 1     2     −  2  l  2  2  x  +  2  y  +  − z l 1     2   cos   θ α      −  2  2  y  +  − z l 1     2   cos   θ α   3  −  =  l  2 2  − +  l  2 3  2  x  +  2  y  +  − z l 1     2           θ  3  =  − 1 cos {  x  2  +  2  y  +  − z l 1     2     + −  l  2 2  l  2 3  2 } {  l  2  2  x  +  2  y  +  − z l 1     2   }  +  α        9.8       Here  the  solution  to  the  inverse  cosine  operation  is  taken  to  be  between 0 and 180 degrees, i.e., a positive cosine value corresponds to  an inverse cosine between 0 and 90 degrees and a negative cosine value  corresponds to an inverse cosine between 90 degrees and 180 degrees.  The  three  equations,    9.5  ,    9.6  ,  and    9.8    constitute  the  required  expressions  for  obtaining  the  robotic  manipulator  angles  given  the  desired Cartesian coordinates for the end - effector.     Let the links have length given by L 1    =    1 ; L 2    =    2 ; L 3    =    2 . Let the coor- dinates of the end effector be x     =    0 ; y     =    4  and z     =    1 . Find the required  joint angles.     or        or     to write  which leads to +  2     2 2 l  x        EXAMPLE 7     SOLUTION 7     EXAMPLE 8      θ1  0=   degrees;    θ2  0=   degrees and    θ3  0=   degrees       Let the links have length given by L 1    =    1 ; L 2    =    2 ; L 3    =    2 . Let the coor- dinates of the end effector be x     =    2 ; y     =    2  and z     =    1 . Find the required  joint angles.      276     OPERATING A ROBOTIC MANIPULATOR     θ1  = −   degrees,    θ2  45  45=    degrees and    θ3  = −   degrees     90    SOLUTION 8     EXAMPLE 9     Let the links have length given by L 1    =    1 ; L 2    =    2 ; L 3    =    2 . Let the coor- dinates of the end effector be x     =    2 ; y     =    2  and z     =    2 . Find the required  joint angles.       SOLUTION 9      θ1  = −   degrees    θ2  45  =  60 9 .    degrees, and    θ3  = − . 82 8    degrees          9.4      PATH SPECIFICATION IN CARTESIAN SPACE    In  some  cases,  one  may  simply  require  that  the  end - effector  of  the  robotic  manipulator  be  placed  at  a  speciﬁ ed  location.  Here  one  can  compute  the  required  joint  angles  using  the  inverse  kinematic  equa- tions as in the examples above and then direct the controllers to drive  each  joint  to  the  proper  angle  without  regard  for  any  coordination  among the joints. In other cases, there may be a path for the motion,  speciﬁ ed as a series of points in Cartesian coordinates. In such case, one  can solve the inverse kinematic equations for each of the points and  construct the corresponding path in robot joint coordinates. The motion  in joint coordinates would then be along this path. Several examples  are given below.      EXAMPLE 10     Reaching  out  to  the  front  to  hold  a  sensor  over  an  object.  Let  x     =    0 ,  y     =      k  20   *  3  and z     =    1  for k     =    1,2,3  . . .  20 . The links have lengths L 1    =    1 ,  L 2    =    2  and L 3    =    2  Determine the required joint angles.       SOLUTION 10     Plots illustrating the solution are shown in Figures    9.7 a – e  .       i  e t a n d r o o c   z  1.5  0.5  2  1  0 3  2.5  3  2  1  e  t  i  a n d r o o c   y  1.5  0.5  0  0  2  1  y coordinate  0  -1  0  -0.5  x coordinate  1  0.5       Figure 9.7a       Speciﬁ ed Path of End Effector in xyz Space    2  4  6  8  12  14  16  18  20  10 Time       Figure 9.7b       y Coordinate of End Effector versus Time, x    =    0; z    =    L1    =    1    277   1  0.8  0.6  0.4  0  -0.2  -0.4  -0.6  -0.8  1  θ  0.2  l    , e g n a    t  i  n o J  -1  0  90  85  80  75  65  60  55  50  45  40  0  2  θ  70    ,  l  e g n a    t  i  n o J  278  2  4  6  8  12  14  16  18  20  10 Time       Figure 9.7c       Joint Angle   θ1 versus Time    2  4  6  8  12  14  16  18  20  10 Time       Figure 9.7d       Joint Angle   θ2  versus Time     PATH SPECIFICATION IN CARTESIAN SPACE     279  3  θ  -120    ,  l  e g n a    t  i  n o J  -80  -90  -100  -110  -130  -140  -150  -160  -170  -180 0    EXAMPLE 11     SOLUTION 11     EXAMPLE 12   2  4  6  8  12  14  16  18  20  10 Time       Figure 9.7e       Joint Angle   θ3 versus Time      Lifting an object located in front of the robot. With link lengths L 1    =    1 ,  L 2    =    2   and  L 3    =    2 .  let  x k      =    0 ;  y k      =    3 ;  and  z k      =    2  *  k  20  ;  for  k     =    1,2 , …  20 . Determine the required joint angles.       Plots illustrating the solution are shown in Figures    9.8 a – e  .        Reaching over an obstacle to retrieve an object located at the front right  of the robot. With link lengths L 1    =    1 , L 2    =    2  and L 3    =    2  let x k      =    2  *  k  20  ;  y k      =    2  *  k  20   and z k      =    1    +    2  * sin  π  * k  20   for k     =    1,2 , …  20 . Determine  the required joint angles.       SOLUTION 12     Plots illustrating the solution are shown in Figures    9.9 a – d  .         i  e t a n d r o o c   z  1.5  0.5  2  1  0 4  e  t  i  a n d r o o c   z  2  1.8  1.6  1.4  1.2  1  0.8  0.6  0.4  0.2  0  0  3.5  3  2.5  y coordinate  2  -1  1  0.5  0  -0.5  x coordinate       Figure 9.8a       Speciﬁ ed Path of End Effector in xyz Space    2  4  6  8  12  14  16  18  20  10 Time       Figure 9.8b       z Coordinate of End Effector versus Time    280   1  0.8  0.6  0.4  0  -0.2  -0.4  -0.6  -0.8  1  θ  0.2  l    , e g n a    t  i  n o J  -1  0  60  55  50  45  40  35  30  25  20  0  2  θ    ,  l  e g n a    t  i  n o J  2  4  6  8  12  14  16  18  20  10 Time       Figure 9.8c       Joint Angle   θ1 versus Time    2  4  6  8  12  14  16  18  20  10 Time       Figure 9.8d       Joint Angle   θ2 versus Time    281   3  θ  l    , e g n a    t  i  n o J  -75  -76  -77  -78  -79  -80  -81  -82  -83  0  e  t  i  a n d r o o c   z  2.5  1.5  3  2  1 2  282  2  4  6  8  12  14  16  18  20  10 Time       Figure 9.8e       Joint Angle   θ3 versus Time    1.5  1  0.5  y coordinate  0  0  2  1.5  1  0.5  x coordinate       Figure 9.9a       Speciﬁ ed Path of End Effector in xyz Space     1  θ  -44.8  l    , e g n a    t  i  n o J  -44  -44.2  -44.4  -44.6  -45  -45.2  -45.4  -45.6  -45.8  -46  0  2  θ    ,  l  e g n a    t  i  n o J  160  140  120  100  80  60  40  0  2  4  6  8  12  14  16  18  20  10 Time       Figure 9.9b       Joint Angle   θ1 versus Time    2  4  6  8  12  14  16  18  20  10 Time       Figure 9.9c       Joint Angle   θ2  versus Time    283   284     OPERATING A ROBOTIC MANIPULATOR  3  θ    ,  l  e g n a    t  i  n o J  -90  -100  -110  -120  -130  -140  -150  -160  -170  -180 0  2  4  6  8  12  14  16  18  20  10 Time       Figure 9.9d       Joint Angle   θ3 versus Time       9.5      VELOCITY RELATIONSHIPS    It is interesting to consider the velocities required by the robotic joints  as a function of the velocities in Cartesian coordinates that result from  the  path  speciﬁ cation.  For  this  analysis  use  is  made  of  the  Jacobian.  Recalling the equations for  x ,  y  and  z  in terms of the joint angles, one  can take partial derivatives of the Cartesian variables with respect to  the joint angles and obtain             cid:1  x  cid:1  y  cid:1  z        =  J   cid:1  θ  1   cid:1  θ  2  cid:1   θ  3               9.8a                cid:1  x  cid:1  y  cid:1  z        =        − l c c 2 1 2 − l s 2 1cc  2  l c c 3 1 23 l s c 3 1 23  − − 0  + l s s l s s 2 1 2 3 1 23 − − l c s l c s 2 1 2 3 1 23 + l c l c 2 2 3 23  l s s 3 1 23 − l c s 3 1 23 l c 3 23   cid:1  θ  1   cid:1  θ  2  cid:1   θ  3                     9.8b      or         VELOCITY RELATIONSHIPS     285   This  equation  shows  how  the  joint  velocities  affect  the  velocity  of  the end effector in Cartesian space. By formally taking the inverse of  the Jacobian matrix we obtain   cid:1  θ  1   cid:1  θ      2  cid:1   θ  3        =        − l c c 2 1 2 − s c l22 1 2  l c c 3 1 23 l s c 3 1 23  − − 0  + l s s l s s 2 1 2 3 1 23 − − l c s l c s 2 1 2 3 1 23 + l c l c 2 2 3 23  −1  l s s 3 1 23 − l c s 3 1 23 l c 3 23               cid:1  x  cid:1  y  cid:1  z               9.9       This equation shows how the velocity of the end effector in Cartesian  space affects the velocities of the robotic joints. Of particular interest  are  situations  where  J  is  nearly  singular  causing    J −1  to  be  very  large.  For such conﬁ gurations extremely large joint velocities may be required  to achieve even moderate Cartesian velocities. By taking the determi- nant of the Jacobian, after some algebra it can be seen that the Jacobian  is singular whenever     l l 2 3     l c 2 2  +  l c 3 23      −  s 3     =     0    9.10       When        l c 2 2  +  l c 3 23     =   0           the end effector is located along the axis of joint 1 and there is no pos- sible instantaneous motion of the end effector in the direction perpen- dicular to the plane of links 2 and 3. See Figure  9.10 .   0= , links 2 and 3 are aligned and there is no possible motion  radially.  See  Figure   9.11 .  One  should  avoid  specifying  paths  which  cause  the  robotic  manipulator  to  even  come  close  to  these  singular  conﬁ gurations.      When   s3     EXAMPLE 13     The  end  effector  is  moved  at  a  constant  height  from  a  point  in  front of the robot to a point behind the robot passing close to a singular  conﬁ guration.  Let  the  link  lengths  be  L 1    =    1   L 2    =    2   and  L 3    =    2   The  speciﬁ ed  path  is  x     =     − 0.01 ,  y     =    1    −      k  2000   *  2   and  z     =    2 ;  for  k     =    1,2 , … ., 2000  Determine the required joint angles.       SOLUTION 13     Plots illustrating the solution are shown in Figures    9.12 a – d  .        286     OPERATING A ROBOTIC MANIPULATOR  l2  θ 2   l3  θ  3   l1  l2  l3  l1       Figure 9.10       One of the Singular Conﬁ gurations for the Robotic Manipulator         Figure 9.11       Other Singular Conﬁ gurations for the Robotic Manipulator     In the above, we see that   θ1 changed rapidly from 0 degrees to  − 180  degrees as the robot passed near the singular conﬁ guration. The func- tion approaches a step function as the y coordinate becomes smaller  and  smaller.  The  derivative  i.e.,  the  velocity  of  the  joint  angle  thus  approaches an impulse.      EXAMPLE 14   is  moved  along  a  path  described  x k      =    0 ,    The  end  effector  y k      =    2  *   1    −      k  10    and z k      =    1  for k     =    1:9  and x k      =    0 , y k      =    0  and  z k      =    1    +    2  *   k     −    9    10   for k     =    10:20 . This path causes the conﬁ gura- tion to become nearly singular. Determine the required joint angles.      i  e t a n d r o o c   z  2.5  1.5  3  2  1 1  1  θ    ,  l  e g n a    t  i  n o J  180  160  140  120  100  80  60  40  20  0  0  0.5  0  -0.5  y coordinate  -1  -1.5  -1  0  -0.5  x coordinate  1  0.5       Figure 9.12a       Speciﬁ ed of End Effector in xyz Space    200  400  600  800  1600 1800  2000  1000 1200 1400 Time       Figure 9.12b       Joint Angle   θ1 versus Time    287   2  θ  l    , e g n a    t  i  n o J  170  160  150  140  130  120  110  0  -138  -140  -142  -146  -148  -150  -152 0  3  θ  -144    ,  l  e g n a    t  i  n o J  288  200  400  600  800  1600 1800  2000  1000 1200 1400 Time       Figure 9.12c       Joint Angle   θ2  versus Time    200  400  600  800  1600 1800  2000  1000 1200 1400 Time       Figure 9.12d       Joint Angle   θ3 versus Time     FORCES AND TORQUES     289  e  t  i  a n d r o o c   z  3.5  2.5  3  2  1 2  1.5  1.5  1  0.5  y coordinate  0  -1  1  0.5  0  -0.5  x coordinate       Figure 9.13a       Speciﬁ ed Path of End Effector in xyz Space      SOLUTION 14     Plots illustrating the solution are shown in Figures    9.13 a – d  .        In this example, it is seen that   θ2 changes rapidly with time when the   end effector is near the singular point, x    =    0, y    =    0, z    =    1.       9.6      FORCES AND TORQUES    The Jacobian was used to develop the relationship between the veloci- ties of the end effector in Cartesian space and the angular velocities in  joint space.       9.11                 cid:1  x  cid:1  y  cid:1  z        =  J   cid:1  θ  1   cid:1  θ  2  cid:1   θ  3              This  matrix  can  also  be  used  to  develop  the  relationship  between  Joint torques and end effector forces. Now the total power delivered    1  θ  0.2  l    , e g n a    t  i  n o J  1  0.8  0.6  0.4  0  -0.2  -0.4  -0.6  -0.8  -1  0  180  160  140  120  100  80  60  0  2  θ    ,  l  e g n a    t  i  n o J  290  2  4  6  8  12  14  16  18  20  10 Time       Figure 9.13b       Joint Angle   θ1 versus Time    2  4  6  8  12  14  16  18  20  10 Time       Figure 9.13c       Joint Angle   θ2 versus Time     FORCES AND TORQUES     291  3  θ  -140    ,  l  e g n a    t  i  n o J  -110  -120  -130  -150  -160  -170  -180 0           2  4  6  8  12  14  16  18  20  10 Time       Figure 9.13d       Joint Angle   θ3 versus Time    at the end effector must be equal to the total power delivered by the  joint actuators, i. e.,  or by using the relationships between velocities   For the above to be true,  T            F x F y F z               cid:1  x  cid:1  y  cid:1  z        =  T  τ  1  τ  2 τ   3         cid:1  θ  1   cid:1  θ  2  cid:1   θθ3              T            F x F y F z         cid:1  θ  1   cid:1  θ  2  cid:1   θ  3        J  =  T  τ  1  τ  2 τ   3         cid:1  θ  1   cid:1  cid:1  θ  2  cid:1   θ  3             T  τ  1  τ      2 τ   3        =         TF x  F  y   F z  J        9.12a       9.12b        9.13a      292     OPERATING A ROBOTIC MANIPULATOR  or               F x F y F z        =[  T  J  ] −1  τ  1  τ  2 τ   3               9.13b       Since the determinant of a matrix is the same as the determinant of  its transpose, the very same conﬁ gurations that caused the Jacobian to  be  nearly  singular  and  required  very  large  joint  velocities  for  given  end - effector velocities provide very large forces at the end effector for  given torques at the joints.    For the situation where   sin   0= , the robotic manipulator has great  stiffness in the radial direction along links 2 and 3. For the other sin- = , the robotic manipu- gular condition where   l 0 lator has great stiffness in the direction normal to the plane containing  links 2 and 3.    θ3 + θ 2  θ θ 2 3  cos   cos   +        l  3  2   The  relationships  just  developed  can  be  used  to  determine  the  torques  required  to  accomplish  a  particular  task  where  the  force  required at the end effector is known. This could apply for example to  the task of lifting an object of known weight.     EXERCISES          1 .    A  robotic  manipulator  has  its  joints  set  at    θ π1 =     θ π3 = −   and L3    =    3   m. Find the location of the end effector.     4   and  .  The  robot  links  are  of  length  L1    =    1   m,  L2    =    3   m  2  ,    θ π2 =   3      2 .    A  robotic  manipulator  has  its  joints  set  at    θ π1 = −    3   . The robot links are of length L1    =    1   m, L2    =    3   m and  3  and   θ π3 = −   L3    =    3   m. Find the location of the end effector.     ,    θ π2 = −   2      3 .    A robotic manipulator is to place its end effector at the point x    =     − 1,  y    =    2, and z    =    1. The robot links are of length L1    =    1   m, L2    =    3   m and  L3    =    3   m. Determine the required joint angles.         4 .    A robotic manipulator is to place its end effector at the point x    =    2,  y    =    1, and z    =    2. The robot links are of length L1    =    1   m, L2    =    3   m and  L3    =    3   m. Determine the required joint angles.         5 .    A robotic manipulator is to be used to place an instrument near an  object  of  interest.  In  terms  of  the  coordinates  of  the  base  of  the  robot, the object is at x    =    2, y    =    3, and z    =     − 1. The robot links are of  length L1    =    1   m, L2    =    3   m and L3    =    3   m. Find the required angles for    REFERENCES     293  this manipulator. If there is an ambiguity in joint 3, put the elbow  up, i.e., choose the negative value for theta 3.     = −1 2      6 .    A robotic manipulator is to move the end effector along a path speci- , … .      20 ﬁ ed  by    x k The  robot  links  are  of  length  L1    =    1   m,  L2    =    3   m  and  L3    =    3   m.  Determine  proﬁ les  for  each  of  the  required  joint  angles  and  plot  them versus  k .     .= 0 2  and    z k    = −1  for    k = 1 2 3  20,    y k     k     ,      7 .    A robotic manipulator has its joints set at   θ π1 = −    8  and    θ π3 = −   .  The  robot  links  are  of  length  L1    =    1   m,  L2    =    3   m  and  6 L3    =    3   m.  First  ﬁ nd  the  location  of  the  end  effector.  Next  ﬁ nd  the  required  joint  angle  rates  for  the  end  effector  to  be  moving  at  an  instantaneous velocity of    cid:1 x = 0,    cid:1 y = 0 and    cid:1 z = 2.        ,   θ π2 3   2  =    REFERENCES       Asada ,  H.   and   Slotine ,  J. - J.E.    “  Robot Analysis and Control  ” ,  Wiley ,  NY ,  1986 .       Craig ,  J. J.  ,  “  Introduction to Robotics: Mechanics and Control  ” ,  Addison and       Paul ,   R.  P.  ,   Robot  Manipulators :    “ Mathematics,  Programming  and  Control  ”    Wesley ,  Reading, MA ,  1989 .     Massachusetts :  MIT Press ,  1982 .        Sciavicco ,   Lorenzo    and    Siciliano ,   Bruno  ;   “  Modelling  and  Control  of  Robot   Manipulators  ” ,  McGraw Hill ,  1996 .        Yoshikawa ,   Tsuneo  ,   “  Analysis  and  Control  of  Robot  Manipulators  with  Redundancy  ” ,  Robotics Research :  The First Int. Symp .   1984   pp.  735  –  748 .       Yoshikawa ,   Tsuneo  ,   “  Foundations  of  Robotics:  Analysis  and  Control  ” ,   MIT   Press   1990 .                10   REMOTE SENSING VIA  UAV  S            10.0      INTRODUCTION    Mobile robots are not restricted to ground vehicles. There are unmanned  water  vehicles  and  also  unmanned  air  vehicles  as  well  as  unmanned  space  vehicles.  In  this  chapter,  we  discuss  the  unmanned  air  vehicle  when used in remote sensing. Requirements on sensor resolution and  precision of vehicle attitude and position are treated.    The aircraft might use radar and or IR as well as other sensors. It is  required that objects of interest be recognizable, i.e., there need to be  enough pixels on the target to permit recognition. It is also required  that the one be able to determine the ground coordinates of the object  within some level of precision. This would enable retrieval or neutral- ization of the object or whatever other action might be desired.       10.1      MOUNTING OF SENSORS    In some cases, the sensor may be mounted directly to the aerial vehicle,  in which case it experiences the same attitude changes as the vehicle.   Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  295   296     REMOTE SENSING VIA UAVs  This  could  yield  a  somewhat  jittery  sensor  footprint  or  ﬁ eld  of  view.  Another approach is to mount a gimbaled sensor platform on the aerial  vehicle with the gimbals having the ability to compensate for vehicle  attitude changes. This would permit a steadier sensor ﬁ eld of view in  the presence of attitude disturbances. In either case, it is necessary to  very precisely know the attitude of the sensors. This can be done via  attitude  instrumentation  for  the  vehicle  coupled  with  knowledge  of  the motion of the gimbaled platform with respect to the vehicle, or the  attitude  instruments  could  be  attached  directly  to  the  gimbaled  platform.       10.2      RESOLUTION OF SENSORS    An airborne vehicle is shown in Figure  10.1  along with a vehicle - based  coordinate  system. As  before,   y   is  along  the  longitudinal  axis  of  the  vehicle,  x  is to the right and  z  is up. A target is shown on the ground  with its location given in vehicle coordinates. It is desired to recognize  the object of interest and to geo - register it. For recognition, one might  assume that 25 pixels on the target would be sufﬁ cient, e.g., 5    ×    5 pixels.  If the target is of size 0.2   m    ×    0.2   m, then the pixel spacing on the ground  would have to be no greater than 0.04   m or 4   cm.     vZ  vX  vY  Ztarget, vehicle cords =  -H   H  eZ  eX  eY  Xtarget, vehicle  coords  Ytarget, vehicle  coords       Figure 10.1       Airborne Vehicle with Downward - Looking Sensor     PRECISION OF VEHICLE INSTRUMENTATION     297   A typical IR camera might have a ﬁ eld of view approximately equal  to 12 degrees    ×    16 degrees. At an altitude of 300   m, this ﬁ eld of view on  the  ground  would  be  approximately  63  meters    ×    84  meters.  Dividing  these  numbers  by  the  required  resolution  of  .04  meters  yields  1575    ×    2100  as  the  required  dimension  of  the  camera  focal  array  in  pixels. This is not unreasonable.       10.3      PRECISION OF VEHICLE INSTRUMENTATION    To  examine  the  precision  of  the  geo - registration  task,  the  effects  of  vehicle attitude and position will be considered. The uncertainty in the   x  coordinate will be given by     σ  2 − x target  =  σ 2 2 φ  h  +  σ σ 2 2 2 ψ − x sensor  +  y  +  σ 2 − x vehicle       The uncertainty in the y coordinate will be given by     σ  2 − y target  =  σ 2 2 θ  h  +  σ σ 2 2 2 ψ − y sensor  +  x  +  σ 2 − y vehicle       An advertisement for the Ashtech ADU3 GPS array with one meter  separation between antenna elements lists accuracy in pitch and roll to  be 0.8 degrees rms and accuracy in yaw to be 0.4 degrees rms. In terms  of radians, these ﬁ gures correspond to     σψ2    σθ2  = =  − 5  . x 4 9 10 1 95 10 . x     − 4        σφ2  =  1 95 10 .  x  − 4                  and   Regarding  vehicle  position,  this  same  speciﬁ cation  sheet  states  accuracy  of  40   cm  circular  error  probable   CEP   for  the  differential  mode and 3   m CEP for the autonomous mode. CEP is the radius of a  circle for which the probability of the event being inside is 0.5. It differs  from  the  elliptical  conﬁ dence  regions  unless  the  standard  deviations  are equal in both directions. It is also more difﬁ cult to calculate than  the  ellipses  when  the  standard  deviations  are  unequal.  For  the  case  where the standard deviations are equal in both directions, the radius  becomes   298     REMOTE SENSING VIA UAVs  or conversely  = 1 386  .  σ  x     r     σ= .7215 r      For the CEP ’ s stated these correspond to     σ σx  =  2  2 y  =   .  72 40  x  .     2  =  8 3 10 .  x  − 2     when operating in the differential mode  DGPS  and     σ σx  =  2  2 y  =   .  72 3   x  2  =  4 7 .     when operating in the autonomous mode. Clearly GPS in the autono- mous mode does not yield a high degree of location precision.       10.4      OVERALL GEO - REGISTRATION PRECISION    For the sensor, the precision is limited by the quantization caused by  the digital nature of the camera. Take   ∆ as the pixel size on the ground.  Assuming  that  errors  in  localizing  the  source  of  a  signal  occurring  within a given pixel range from   −∆   2 to   ∆   2 with a uniform distribu- tion, one is able to determine the corresponding variance to be     σ2  = ∆    2 12         ∆ = 0 04. m       σ  2 − x sensor  =  σ 2 − y sensor  =  1 33 10 .  ∗  − 4  2  m       Now  using  the  equations  above  with  the  numbers  just  obtained   2 −     σx target    σy target  2 −  = =  ∗ ∗  2  h  2  h  x  1 95 10 . 1 95 10 .  x  − 4  − 4  + +  2  y  2  x  ∗ ∗  4 95 10 . x − 5 4 9 10 .  x  − 5  + +  1 33 10 . − 4 1 33 10 .  ∗ ∗  − 4  + +  ∗ ∗  8 3 10 . − 2 8 3 10 .      − 2                              For  this yields  yields   OVERALL GEO-REGISTRATION PRECISION     299   It  is  clear  that  if  the  sensor  meets  the  resolution  requirements  for  target  recognition,  it  will  not  contribute  appreciably  to  the  geo -  registration error. Also, if DGPS can be used, the vehicle position error  will not add too much to the error. Now taking as a typical target loca- tion with respect to the vehicle,  m= 15    x   m= 20    y   = −300 m    z    σx target = =    σy target  2 −  2 −    . .  17 55 17 55  + +  − 4  .  + +  02 1 33 10 . . 011 1 33 10  ∗ ∗  .  − 4  − 2  + 8 3 10 . + 8 3 10 .  ∗ ∗    − 2       The greatest source of error is the uncertainty in the vehicle pitch  and roll which get magniﬁ ed by the altitude of the vehicle squared. In  order to improve the geo - registration accuracy to the centimeter range,  the variances in pitch and roll need to be reduced by almost 100.    Multiple  looks  will  assist  in  reducing  the  error  covariance  below  what it would be for a single look. If the errors from look to look are  independent,  the  variance  reduces  as  one  over  the  number  of  looks.  However  this  alone  would  not  take  care  of  the  problem  here.  The  LEICA DMC Inclinometer advertises variances of     σψ2    σθ2  = =  x 4 10 x 5 10  − 6  − 6          σφ2  =  x 5 10  − 6       Using these numbers, the calculations of geo - registration uncertainty   2 −     σx target    σy target  2 −  = =  0 45 . 0 45 .  + +  . 0016 1 33 10 0009 1 33 10 .  . .  + +  ∗ ∗  − 4  − 4  + +  ∗ ∗  8 3 10 . 8 3 10 .  − 2  − 2       which are closer to the goal. However, one must keep in mind that the  inclinometer is accurate only when there is no vehicle acceleration. This  is a serious restriction that must be considered. Another candidate for  attitude  measurement  would  be  an  inertial  navigation  system   INS .                   and  become   300     REMOTE SENSING VIA UAVs  These are much more expensive than inclinometers, of course, but are  much more accurate and can have rotational drift as low as a few milli -  degrees per hour.    The application of UAVs to the problem of remote sensing as well  as  to  other  areas  is  rapidly  expanding.  The  ever  growing  capability,  coupled with the fact that the operator is not exposed to danger, makes  this a popular choice for consideration.     EXERCISES          1 .    An airborne vehicle is at altitude 200m. Assume that the vehicle is  in  steady  ﬂ ight. An  object  on  the  ground  below  is  at  coordinates   x     =    70 and  y     =     − 60 in vehicle coordinates. Using the instrument spec- iﬁ cations cited in the discussion in the chapter, compute the preci- sion one could achieve for geo - registration of this object using DGPS  for  positioning.  Perform  the  attitude  calculations  based  on  GPS  speciﬁ cations  and  on  inclinometer  speciﬁ cations  and  compare  the  overall results.          REFERENCES       Newcome ,   Laurence  R.      2004  .   “  Unmanned  Aviation:  A  Brief  History  of  Unmanned  Aerial  Vehicles   ” ,    American  Institute  of  Aeronautics  and  Astronautics, Inc ,  2004 .        Shim ,  D. H.  ,   Kim ,  H. J.  ,   Sastry ,  S.  ,  “  Hierarchical Control System Synthesis for  Rotorcraft - based Unmanned Aerial Vehicles   ” ,  AIAA Guidance, Navigation,  and Control Conference and Exhibit,  14  –  17  August  2000 , Denver, CO.              APPENDIX A   DEMONSTRATIONS OF  UNDERGRADUATE STUDENT  ROBOTIC PROJECTS           A.0      INTRODUCTION    The appendix is devoted to describing two robotic projects conducted  by students in the Department of Electrical and Computer Engineering  at  George  Mason  University.  In  addition  to  the  descriptions,  links  to  videos of the projects are also included.       A.1      DEMONSTRATION OF THE  GEONAVOD  ROBOT    To  view  a  video  on  the  GEONAVOD  project  go  to  the  book  web- page  at   www.wiley.com   or   http:  www.video.youtube.com watch?v =  IENfst4Dcg8 . In this video a small mobile robot can be seen carrying  out a particular task. The following describes the robot and the scenario  represented in this demonstration.    A fence - like structure was built to support three ultrasound trans- mitters as well as a radio transmitter approximately 2m above the ﬂ oor  where the robot operated. These transmitters are coordinated and syn- chronized via signals from the radio transmitter. Initially, a radio signal  is  transmitted  to  synchronize  the  clock  on  the  robot  with  the  radio   Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  301   302     DEMONSTRATIONS OF UNDERGRADUATE STUDENT ROBOTIC PROJECTS  transmitter clock. Then in a predetermined sequence at predetermined  time intervals, the three ultrasound transmitters transmit. The receiver  on the robot determines the time of arrival of each transmission, and  knowing the schedule for initiation of each transmission, computes the  time of travel from each transmitter to the robot. Knowing the speed  of travel of the ultrasound signal, the distance of the robot from each  transmitter  is  determined. Then  using  an  iterative  process,  the  robot  coordinates are computed. The dimension of the problem is reduced to  two since the  z  component is known, i.e., the robot is on the ﬂ oor.    In fact the robot has two ultrasound receivers, one at each end, so  this process is carried out for each of these. With the determination of  position of each end of the robot, heading as well as location can be  determined. The task of the robot in the demonstration is to navigate  along a path with three way - points. The robot periodically re - calculates  its position and heading as it traverses this path. In the demonstration,  a team member intervenes from time to time to set the robot off course  and test its ability to right itself and get headed back toward the next  way - point.    As the robot travels it searches for signals of interest. In this case, it  is looking for infrared sources. The search is carried out via a rotating  turret with an IR detector mounted on top of the robot. Once an IR  source is detected, the robot stops and the angle of the turret at detec- tion is noted. The robot then turns the appropriate amount to face the  source of the signal. A laser sensor is then used to measure the distance  to  this  source  and  the  coordinates  of  the  source  are  computed. The  robot then navigates its way to this object of interest.    The  design  team  was  comprised  of  Edward  Smith,  Charles  Purvis,  Johnny Garcia and Ulan Bekishov. The design, construction, assembly  and testing were entirely accomplished by this team who at the time  were all undergraduate students in the Department of Electrical and  Computer Engineering of George Mason University.      DEMONSTRATION OF THE AUTOMATIC BALANCING      A.2   ROBOTIC BICYCLE   ABRB      To view a video on the ABRB project go to the book webpage at  www. wiley.com  or  http:  www.youtube.com watch?v = vczwea6iv_M & feature  = related . In this video, the automatic balancing robotic bicycle can be  seen demonstrating its capability. A potential application of this robot  would be for the transfer of materials through spaces that are too tight  for a four - wheeled vehicle. The following describes the robot.    DEMONSTRATION OF THE AUTOMATIC BALANCING ROBOTIC BICYCLE     303   The  robotic  bicycle  balances  itself  through  the  use  of  a  reaction  wheel mounted orthogonal to the longitudinal axis and powered by an  electric motor. Whenever the motor accelerates the reaction wheel, a  torque is created within the motor and transferred to the bicycle frame  on which it is mounted. Sensors measure the roll angle of the bicycle  and  compare  this  value  to  vertical. Whenever  a  deviation  in  the  roll  angle from vertical is detected, a controller directs the motor to acceler- ate the reaction wheel. The resulting torque acts to rotate the bicycle  back toward vertical at which time the motor torque can be set to zero.  The  balancing  principle  is  similar  to  that  of  controlling  an  inverted  pendulum  without  moving  the  supporting  base,  and  two  members  of  the design team had previously successfully applied a reaction wheel  to this problem.    Several variables are measured and fed into the controller as feed- back signals. These include bicycle roll angle, roll angle rate and reac- tion wheel rate. The ﬁ rst two signals are required to stabilize the bicycle  in the vertical position while the last one is needed to bring the rotation  velocity of the reaction wheel back to zero, keeping it from being driven  into saturation. The equations of motion based on ﬁ rst principals were  derived and the resulting model was considered with regard to control- ler design; however, the ﬁ nal values of the controller gains were deter- mined empirically through experimentation with the actual bicycle.    The locomotion of the bicycle is provided by another small electric  motor, connected to the rear wheel with a belt drive. This drive motor  is  controlled  externally  by  a  human  via  a  wireless  handheld  remote  controller.  Sensors  for  collision  avoidance  are  incorporated  into  the  system and can override the locomotion commands if needed. Power  for all the operations is provided by onboard batteries.    The  design  team  was  comprised  of  Aamer  Almujahed,  Jason  Deweese, Linh Duong and Joel Potter. The design, construction, assem- bly and testing were accomplished entirely by this team who at the time  were all undergraduate students in the Department of Electrical and  Computer Engineering of George Mason University.            INDEX  Associating measurements  240 Asymptotic stability  32 Azimuth  194  Bearing  255, 256 Boresight  camera   195, 217, 218  Colored noise  234 Conﬁ dence regions  225, 229, 230,   231, 232, 233, 234, 235, 242, 243  Control  bang-bang  35 chattering  14, 16, 35, 39 computed  36, 38, 50 heading  11, 23, 32, 38 incremental  26 linear  12, 16, 20, 28, 30 optimal  58, 71, 76 proportional  16  saturated  18, 49 speed  22, 23  Coordinate earth  3, 7 earth-centered earth-ﬁ xed  96 easting  102, 103 frames  79 local  11, 41, 54, 99, 100, 101 northing  102, 103 robot  3, 7 rotations  79 UTM  102  Convergence  12, 34, 36, 183, 184, 185 Coriolis force  136 Co-states  56, 61, 65, 68, 70 Covariance  150, 157, 158, 160, 162,  163, 164, 166, 167, 168, 171, 182,  185  Cross range  229, 230, 231  Mobile Robots: Navigation, Control and Remote Sensing, First Edition. Gerald Cook.   2011 Institute of Electrical and Electronics Engineers. Published 2011 by  John Wiley & Sons, Inc.  305   306      INDEX  Curvature  2, 6, 50, 74  radius of  2, 5, 6  receiver clock bias  107, 119 Standard Positioning Service    SPS   104  time  105  Dead reckoning  137, 139, 249, 250,   251  Divergence  185 Discrete Fourier Transform  DFT    Hamiltonian  55, 57 Heading angle  3, 14  206, 210  Disturbances  18, 35, 36, 51 Down range  229, 230, 231  Elevation  195 Encoder  137 Eigenvalues  29 Euclidean norm  135 Euler integration  4, 8  Filter  Kalman, see Kalman ﬁ lter optimal  160  Fusion  231  Gating  240 Geo-location  121 Geo-register  248, 255 Gimbal  296 Gyroscope  gimbaled  126, 129 MEMS  136 ring laser  136 strapdown  126, 131  GPS  104, 105, 212  array of receivers  123 C A code  105 Coordinated Universal Time    UTC   105  correlators  106 data sequence  106 differential  108, 212, 216 geometric dilution of precision   107, 119 P code  105 Precise Positioning Service  PPS    104  Inclinometer  138, 142, 212, 299 Inertial navigation system  126  accelerometer bias  128 attitude rates  132 body rates  132 drift  128 MEMS gyro  136 quaternions  133, 134, 135 ring laser  136 strapdown  126, 131  Inventory, of detected targets  239,   240, 241, 242  Jacobian matrix  284, 285, 289  Kalman ﬁ lter  109, 149, 166, 171, 173,   175, 177, 185, 259  Latitude  96, 97, 98, 99, 104 Lidar  248 Linearize  16, 30, 52 Localization  247 Longitude  96, 97, 98, 99, 104 Longitudinal axis  2, 7 Longitudinal zones  102, 103 Lyapunov  30, 31  Mapping  247, 256 Matrix  homogeneous transformation  88,   89, 90, 91, 92, 216  inversion lemma  152, 153 orthonormal  82, 84, 85, 86, 92 rotation  80, 82, 84  Measurement noise  156 Meridean  96, 103, 104   INDEX     307  Sensor fusion  231 Steerability  45 Steering  algorithm  14, 21 angle  2, 3, 4, 13, 14, 16, 21 differential drive  5, 6, 23, 38, 39 front wheel  1, 2, 8, 26, 32, 36 optimal  54, 71, 78  Singular  control  58 arc  59, 60, 63, 66, 68, 71  Synthetic aperture radar  206  Terrain  202, 211, 212 Trajectory  minimal path length  11 minimum-time  55, 61, 65 optimal  71, 76 reference  26, 27  Two-point boundary value problem   59  UAV  295 Ultrasound  248  Yaw  80, 81, 87  Model  discrete-time  4, 5, 8 incremental  27, 28 kinematic  1,4 propagation of  162  Navigation  95 Newton’s method  108, 110  Obstacle  247, 248, 252, 253, 254, 255,   258  Odometry  250  Pan and tilt  213, 214, 215, 217, 218,   219, 220, 221, 222  Pitch  82, 83, 84, 87 Pixel  191, 192, 193, 199, 207, 208,   210, 211, 212, 213  Pointing  212, 213 Process disturbance  156 Pseudo distance  109, 113, 119  Recursive processing  151, 156 Residual  153, 154, 159 Robotic manipulator  265  end effector  266 forward kinematics  265, 267 inverse kinematics  265, 271 workspace  265 Roll  84, 85, 86, 87
