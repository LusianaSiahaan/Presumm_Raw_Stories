Elements of Information Theory  Second Edition  Solutions to Problems  Thomas M. Cover  Joy A. Thomas  October 17, 2006   1  COPYRIGHT 2006  Thomas Cover Joy Thomas  All rights reserved   2   Contents  1 Introduction  2 Entropy, Relative Entropy and Mutual Information  3 The Asymptotic Equipartition Property  4 Entropy Rates of a Stochastic Process  5 Data Compression  6 Gambling and Data Compression  7 Channel Capacity  8 Di cid:11 erential Entropy  9 Gaussian channel  10 Rate Distortion Theory  11 Information Theory and Statistics  12 Maximum Entropy  13 Universal Source Coding  14 Kolmogorov Complexity  15 Network Information Theory  16 Information Theory and Portfolio Theory  17 Inequalities in Information Theory  3  7  9  49  61  97  139  163  203  217  241  273  301  309  321  331  377  391   4  CONTENTS   Preface  Here we have the solutions to all the problems in the second edition of Elements of Information Theory. First a word about how the problems and solutions were generated.  The problems arose over the many years the authors taught this course. At  cid:12 rst the homework problems and exam problems were generated each week. After a few years of this double duty, the homework problems were rolled forward from previous years and only the exam problems were fresh. So each year, the midterm and  cid:12 nal exam problems became candidates for addition to the body of homework problems that you see in the text. The exam problems are necessarily brief, with a point, and reasonable free from time consuming calculation, so the problems in the text for the most part share these properties.  The solutions to the problems were generated by the teaching assistants and graders for the weekly homework assignments and handed back with the graded homeworks in the class immediately following the date the assignment was due. Homeworks were optional and did not enter into the course grade. Nonetheless most students did the homework. A list of the many students who contributed to the solutions is given in the book acknowledgment. In particular, we would like to thank Laura Ekroot, Will Equitz, Don Kimber, Mitchell Trott, Andrew Nobel, Jim Roche, Vittorio Castelli, Mitchell Oslick, Chien-Wen Tseng, Michael Mor- rell, Marc Goldberg, George Gemelos, Navid Hassanpour, Young-Han Kim, Charles Mathis, Styrmir Sigurjonsson, Jon Yard, Michael Baer, Mung Chiang, Suhas Diggavi, Elza Erkip, Paul Fahn, Garud Iyengar, David Julian, Yiannis Kontoyiannis, Amos Lapidoth, Erik Or- dentlich, Sandeep Pombra, Arak Sutivong, Josh Sweetkind-Singer and Assaf Zeevi. We would like to thank Prof. John Gill and Prof. Abbas El Gamal for many interesting problems and solutions.  The solutions therefore show a wide range of personalities and styles, although some of them have been smoothed out over the years by the authors. The best way to look at the solutions is that they o cid:11 er more than you need to solve the problems. And the solutions in some cases may be awkward or ine cid:14 cient. We view that as a plus. An instructor can see the extent of the problem by examining the solution but can still improve his or her own version. The solution manual comes to some 400 pages. We are making electronic copies available to course instructors in PDF. We hope that all the solutions are not put up on an insecure websiteit will not be useful to use the problems in the book for homeworks and exams if the solutions can be obtained immediately with a quick Google search. Instead, we will put up a small selected subset of problem solutions on our website, http:  www.elementso cid:12 nformationtheory.com, available to all. These will be problems that have particularly elegant or long solutions that would not be suitable homework or exam problems.  5   6  CONTENTS  We have also seen some people trying to sell the solutions manual on Amazon or Ebay. Please note that the Solutions Manual for Elements of Information Theory is copyrighted and any sale or distribution without the permission of the authors is not permitted.  We would appreciate any comments, suggestions and corrections to this solutions manual.  Tom Cover  Durand 121, Information Systems Lab  Stanford University Stanford, CA 94305.  Ph. 650-723-4505 FAX: 650-723-8473  Joy Thomas  Stratify  701 N Shoreline Avenue  Mountain View, CA 94043.  Ph. 650-210-2722 FAX: 650-988-2159  Email: cover@stanford.edu  Email: joythomas@stanfordalumni.org   Chapter 1  Introduction  7   8  Introduction   Chapter 2  Entropy, Relative Entropy and Mutual Information  1. Coin  cid:13 ips. A fair coin is  cid:13 ipped until the  cid:12 rst head occurs. Let X denote the number  of  cid:13 ips required.   a  Find the entropy H X  in bits. The following expressions may be useful:  1Xn=0  rn =  1 1  cid:0  r  ;  1Xn=0  nrn =  r   1  cid:0  r 2 :   b  A random variable X is drawn according to this distribution. Find an \e cid:14 cient" sequence of yes-no questions of the form, \Is X contained in the set S ?" Compare H X  to the expected number of questions required to determine X .  Solution:   a  The number X of tosses till the  cid:12 rst head appears has the geometric distribution with parameter p = 1=2 , where P  X = n  = pqn cid:0 1 , n 2 f1; 2; : : :g . Hence the entropy of X is  H X  =  cid:0   pqn cid:0 1 log pqn cid:0 1   npqn log q  1Xn=0  pqn log p +  pq log q  1Xn=1 =  cid:0 " 1Xn=0 =  cid:0 p log p 1  cid:0  q  cid:0   p2 =  cid:0 p log p  cid:0  q log q  p  = H p =p bits.  If p = 1=2 , then H X  = 2 bits.  9   10  Entropy, Relative Entropy and Mutual Information   b  Intuitively, it seems clear that the best questions are those that have equally likely chances of receiving a yes or a no answer. Consequently, one possible guess is that the most \e cid:14 cient" series of questions is: Is X = 1 ? If not, is X = 2 ? If not, is X = 3 ? . . . with a resulting expected number of questions equal to  P1n=1 n 1=2n  = 2: This should reinforce the intuition that H X  is a mea- sure of the uncertainty of X . Indeed in this case, the entropy is exactly the same as the average number of questions needed to de cid:12 ne X , and in general E  of questions   cid:21  H X  . This problem has an interpretation as a source cod- ing problem. Let 0 = no, 1 = yes, X = Source, and Y = Encoded Source. Then the set of questions in the above procedure can be written as a collection of  X; Y   pairs:  1; 1  ,  2; 01  ,  3; 001  , etc. . In fact, this intuitively derived code is the optimal  Hu cid:11 man  code minimizing the expected number of questions.  2. Entropy of functions. Let X be a random variable taking on a  cid:12 nite number of  values. What is the  general  inequality relationship of H X  and H Y   if   a  Y = 2X ?  b  Y = cos X ?  Solution: Let y = g x  . Then  p y  = Xx: y=g x   p x :  Consider any set of x ’s that map onto a single y . For this set  Xx: y=g x   p x  log p x   cid:20  Xx: y=g x   p x  log p y  = p y  log p y ;  since log is a monotone increasing function and p x   cid:20  Px: y=g x  p x  = p y  . Ex-  tending this argument to the entire range of X  and Y  , we obtain  H X  =  cid:0 Xx  p x  log p x   =  cid:0 Xy Xx: y=g x   cid:21   cid:0 Xy  = H Y  ;  p y  log p y   p x  log p x   with equality i cid:11  g is one-to-one with probability one.   a  Y = 2X is one-to-one and hence the entropy, which is just a function of the i.e.,  probabilities  and not the values of a random variable  does not change, H X  = H Y   .   b  Y = cos X  is not necessarily one-to-one. Hence all that we can say is that  H X   cid:21  H Y   , with equality if cosine is one-to-one on the range of X .   Entropy, Relative Entropy and Mutual Information  11  3. Minimum entropy. What is the minimum value of H p1; :::; pn  = H p  as p ranges over the set of n -dimensional probability vectors? Find all p ’s which achieve this minimum.  Solution: We wish to  cid:12 nd all probability vectors p =  p1; p2; : : : ; pn  which minimize  H p  =  cid:0 Xi  pi log pi:  Now  cid:0 pi log pi  cid:21  0 , with equality i cid:11  pi = 0 or 1 . Hence the only possible probability vectors which minimize H p  are those with pi = 1 for some i and pj = 0; j 6= i . There are n such vectors, i.e.,  1; 0; : : : ; 0  ,  0; 1; 0; : : : ; 0  , . . . ,  0; : : : ; 0; 1  , and the minimum value of H p  is 0.  4. Entropy of functions of a random variable. Let X be a discrete random variable. Show that the entropy of a function of X is less than or equal to the entropy of X by justifying the following steps:  H X; g X     a   H X; g X    = H X  + H g X  j X   b  = H X ;  c    d   = H g X   + H X j g X    cid:21  H g X  :   2.1    2.2    2.3    2.4   Thus H g X    cid:20  H X : Solution: Entropy of functions of a random variable.   a  H X; g X   = H X  + H g X jX  by the chain rule for entropies.  b  H g X jX  = 0 since for any particular value of X, g X  is  cid:12 xed, and hence   c  H X; g X   = H g X   + H Xjg X   again by the chain rule.  d  H Xjg X    cid:21  0 , with equality i cid:11  X is a function of g X  , i.e., g :  is one-to-one.  H g X jX  =Px p x H g X jX = x  =Px 0 = 0 . Hence H X; g X    cid:21  H g X   .  Combining parts  b  and  d , we obtain H X   cid:21  H g X   .  5. Zero conditional entropy. Show that if H Y jX  = 0 , then Y is a function of X ,  i.e., for all x with p x  > 0 , there is only one possible value of y with p x; y  > 0 .  Solution: Zero Conditional Entropy. Assume that there exists an x , say x0 and two di cid:11 erent values of y , say y1 and y2 such that p x0; y1  > 0 and p x0; y2  > 0 . Then p x0   cid:21  p x0; y1  + p x0; y2  > 0 , and p y1jx0  and p y2jx0  are not equal to 0 or 1. Thus  H Y jX  =  cid:0 Xx  p x Xy  p yjx  log p yjx    cid:21  p x0   cid:0 p y1jx0  log p y1jx0   cid:0  p y2jx0  log p y2jx0    > > 0;   2.5    2.6    2.7    12  Entropy, Relative Entropy and Mutual Information  since  cid:0 t log t  cid:21  0 for 0  cid:20  t  cid:20  1 , and is strictly positive for t not equal to 0 or 1. Therefore the conditional entropy H Y jX  is 0 if and only if Y is a function of X . 6. Conditional mutual information vs. unconditional mutual information. Give  examples of joint random variables X , Y and Z such that   a  I X; Y j Z  < I X; Y   ,  b  I X; Y j Z  > I X; Y   . Solution: Conditional mutual information vs. unconditional mutual information.   a  The last corollary to Theorem 2.8.1 in the text states that if X ! Y ! Z that is, if p x; y j z  = p x j z p y j z  then, I X; Y    cid:21  I X; Y j Z  . Equality holds if and only if I X; Z  = 0 or X and Z are independent. A simple example of random variables satisfying the inequality conditions above is, X is a fair binary random variable and Y = X and Z = Y . In this case,  I X; Y   = H X   cid:0  H X j Y   = H X  = 1  I X; Y j Z  = H X j Z   cid:0  H X j Y; Z  = 0:  So that I X; Y   > I X; Y j Z  .   b  This example is also given in the text. Let X; Y be independent fair binary  random variables and let Z = X + Y . In this case we have that,  I X; Y   = 0  I X; Y j Z  = H X j Z  = 1=2:  So I X; Y   < I X; Y j Z  . Note that in this case X; Y; Z are not markov.  7. Coin weighing. Suppose one has n coins, among which there may or may not be one counterfeit coin. If there is a counterfeit coin, it may be either heavier or lighter than the other coins. The coins are to be weighed by a balance.   a  Find an upper bound on the number of coins n so that k weighings will  cid:12 nd the  counterfeit coin  if any  and correctly declare it to be heavier or lighter.   b   Di cid:14 cult  What is the coin weighing strategy for k = 3 weighings and 12 coins?  Solution: Coin weighing.   a  For n coins, there are 2n + 1 possible situations or \states".   cid:15  One of the n coins is heavier.  cid:15  One of the n coins is lighter.  cid:15  They are all of equal weight.  and,  and,   Entropy, Relative Entropy and Mutual Information  13  Each weighing has three possible outcomes - equal, left pan heavier or right pan heavier. Hence with k weighings, there are 3k possible outcomes and hence we can distinguish between at most 3k di cid:11 erent \states". Hence 2n + 1  cid:20  3k or n  cid:20   3k  cid:0  1 =2 . Looking at it from an information theoretic viewpoint, each weighing gives at most log2 3 bits of information. There are 2n + 1 possible \states", with a maximum entropy of log2 2n + 1  bits. Hence in this situation, one would require at least log2 2n + 1 = log2 3 weighings to extract enough information for determination of the odd coin, which gives the same result as above.   b  There are many solutions to this problem. We will give one which is based on the  ternary number system. We may express the numbers f cid:0 12; cid:0 11; : : : ; cid:0 1; 0; 1; : : : ; 12g in a ternary number system with alphabet f cid:0 1; 0; 1g . For example, the number 8 is  -1,0,1  where  cid:0 1  cid:2  30 + 0  cid:2  31 + 1  cid:2  32 = 8 . We form the matrix with the representation of the positive numbers as its columns. 7 1 -1 1  30 31 32 Note that the row sums are not all zero. We can negate some columns to make the row sums zero. For example, negating columns 7,9,11 and 12, we obtain  12 0  cid:6 1 = 0 1  cid:6 2 = 2 1  cid:6 3 = 8  11 -1 1 1  10 1 0 1  8 -1 0 1  5 -1 -1 1  6 0 -1 1  2 -1 1 0  9 0 0 1  3 0 1 0  4 1 1 0  1 1 0 0  1 1 0 0  2 -1 1 0  3 0 1 0  4 1 1 0  5 -1 -1 1  6 0 -1 1  7 -1 1 -1  8 -1 0 1  9 0 0 -1  30 31 32 Now place the coins on the balance according to the following rule: For weighing i , place coin n  12 0  cid:6 1 = 0 -1  cid:6 2 = 0 -1  cid:6 3 = 0  10 1 0 1  11 1 -1 -1   cid:15  On left pan, if ni =  cid:0 1 .  cid:15  Aside, if ni = 0 .  cid:15  On right pan, if ni = 1 .  The outcome of the three weighings will  cid:12 nd the odd coin if any and tell whether it is heavy or light. The result of each weighing is 0 if both pans are equal, -1 if the left pan is heavier, and 1 if the right pan is heavier. Then the three weighings give the ternary expansion of the index of the odd coin. If the expansion is the same as the expansion in the matrix, it indicates that the coin is heavier. If the expansion is of the opposite sign, the coin is lighter. For example,  0,-1,-1  indicates  0 30 +  cid:0 1 3+  cid:0 1 32 =  cid:0 12 , hence coin 12 is heavy,  1,0,-1  indicates 8 is light,  0,0,0  indicates no odd coin. Why does this scheme work? It is a single error correcting Hamming code for the ternary alphabet  discussed in Section 8.11 in the book . Here are some details. First note a few properties of the matrix above that was used for the scheme. All the columns are distinct and no two columns add to  0,0,0 . Also if any coin   14  Entropy, Relative Entropy and Mutual Information  is heavier, it will produce the sequence of weighings that matches its column in the matrix. If it is lighter, it produces the negative of its column as a sequence of weighings. Combining all these facts, we can see that any single odd coin will produce a unique sequence of weighings, and that the coin can be determined from the sequence. One of the questions that many of you had whether the bound derived in part  a  was actually achievable. For example, can one distinguish 13 coins in 3 weighings? No, not with a scheme like the one above. Yes, under the assumptions under which the bound was derived. The bound did not prohibit the division of coins into halves, neither did it disallow the existence of another coin known to be normal. Under both these conditions, it is possible to  cid:12 nd the odd coin of 13 coins in 3 weighings. You could try modifying the above scheme to these cases.  8. Drawing with and without replacement. An urn contains r red, w white, and b black balls. Which has higher entropy, drawing k  cid:21  2 balls from the urn with replacement or without replacement? Set it up and show why.  There is both a hard way and a relatively simple way to do this.   Solution: Drawing with and without replacement. Intuitively, it is clear that if the balls are drawn with replacement, the number of possible choices for the i -th ball is larger, and therefore the conditional entropy is larger. But computing the conditional distributions is slightly involved. It is easier to compute the unconditional entropy.   cid:15  With replacement. In this case the conditional distribution of each draw is the  same for every draw. Thus  with prob.  r red white with prob. w b black with prob.  r+w+b  r+w+b  r+w+b  and therefore  Xi =8>< >: H XijXi cid:0 1; : : : ; X1  = H Xi  r  = log r + w + b   cid:0   r + w + b  log r  cid:0   r + w + b  log w  cid:0   r + w + b  log b: 2.10   w  b   cid:15  Without replacement. The unconditional probability of the i -th ball being red is still r= r + w + b  , etc. Thus the unconditional entropy H Xi  is still the same as with replacement. The conditional entropy H XijXi cid:0 1; : : : ; X1  is less than the unconditional entropy, and therefore the entropy of drawing without replacement is lower.   2.8    2.9   9. A metric. A function  cid:26  x; y  is a metric if for all x; y ,   cid:15   cid:26  x; y   cid:21  0  cid:15   cid:26  x; y  =  cid:26  y; x    Entropy, Relative Entropy and Mutual Information  15  if and only if x = y   cid:15   cid:26  x; y  = 0  cid:15   cid:26  x; y  +  cid:26  y; z   cid:21   cid:26  x; z  .  a  Show that  cid:26  X; Y   = H XjY   + H Y jX  satis cid:12 es the  cid:12 rst, second and fourth properties above. If we say that X = Y if there is a one-to-one function mapping from X to Y , then the third property is also satis cid:12 ed, and  cid:26  X; Y   is a metric.   b  Verify that  cid:26  X; Y   can also be expressed as   2.11    2.12    2.13    2.14    2.15    2.16    2.17    2.18    cid:26  X; Y   = H X  + H Y    cid:0  2I X; Y    = H X; Y    cid:0  I X; Y   = 2H X; Y    cid:0  H X   cid:0  H Y  :   cid:26  X; Y   = H XjY   + H Y jX :  Solution: A metric   a  Let  Then   cid:15  Since conditional entropy is always  cid:21  0 ,  cid:26  X; Y    cid:21  0 .  cid:15  The symmetry of the de cid:12 nition implies that  cid:26  X; Y   =  cid:26  Y; X  .  cid:15  By problem 2.6, it follows that H Y jX  is 0 i cid:11  Y is a function of X and H XjY   is 0 i cid:11  X is a function of Y . Thus  cid:26  X; Y   is 0 i cid:11  X and Y are functions of each other - and therefore are equivalent up to a reversible transformation.   cid:15  Consider three random variables X , Y and Z . Then  H XjY   + H Y jZ   cid:21  H XjY; Z  + H Y jZ   = H X; Y jZ  = H XjZ  + H Y jX; Z   cid:21  H XjZ ;  from which it follows that   2.19  Note that the inequality is strict unless X ! Y ! Z forms a Markov Chain and Y is a function of X and Z .   cid:26  X; Y   +  cid:26  Y; Z   cid:21   cid:26  X; Z :   b  Since H XjY   = H X  cid:0  I X; Y   , the  cid:12 rst equation follows. The second relation follows from the  cid:12 rst equation and the fact that H X; Y   = H X  + H Y    cid:0  I X; Y   . The third follows on substituting I X; Y   = H X  + H Y    cid:0  H X; Y   . 10. Entropy of a disjoint mixture. Let X1 and X2 be discrete random variables drawn according to probability mass functions p1  cid:1   and p2  cid:1   over the respective alphabets X1 = f1; 2; : : : ; mg and X2 = fm + 1; : : : ; ng: Let  X =  X1; with probability  cid:11 ;  X2; with probability 1  cid:0   cid:11 :   16  Entropy, Relative Entropy and Mutual Information   a  Find H X  in terms of H X1  and H X2  and  cid:11 :  b  Maximize over  cid:11  to show that 2H X   cid:20  2H X1  + 2H X2  and interpret using the  notion that 2H X  is the e cid:11 ective alphabet size.  Solution: Entropy. We can do this problem by writing down the de cid:12 nition of entropy and expanding the various terms. Instead, we will use the algebra of entropies for a simpler proof.  Since X1 and X2 have disjoint support sets, we can write  X =  X1 with probability   cid:11  X2 with probability 1  cid:0   cid:11    cid:18  = f  X  =  1 when X = X1  2 when X = X2  De cid:12 ne a function of X ,  Then as in problem 1, we have  H X  = H X; f  X   = H  cid:18   + H Xj cid:18    = H  cid:18   + p  cid:18  = 1 H Xj cid:18  = 1  + p  cid:18  = 2 H Xj cid:18  = 2  = H  cid:11   +  cid:11 H X1  +  1  cid:0   cid:11  H X2   where H  cid:11   =  cid:0  cid:11  log  cid:11   cid:0   1  cid:0   cid:11   log 1  cid:0   cid:11   .  11. A measure of correlation. Let X1 and X2 be identically distributed, but not  necessarily independent. Let   cid:26  = 1  cid:0   H X2 j X1   :  H X1    a  Show  cid:26  = I X1;X2  H X1  :  b  Show 0  cid:20   cid:26   cid:20  1:  c  When is  cid:26  = 0 ?  d  When is  cid:26  = 1 ?   a   Solution: A measure of correlation. X1 and X2 are identically distributed and   cid:26  = 1  cid:0   H X2jX1  H X1    cid:26  =  =  =  H X1   cid:0  H X2jX1   H X1   H X2   cid:0  H X2jX1   H X1   I X1; X2   H X1   :   since H X1  = H X2     Entropy, Relative Entropy and Mutual Information  17   b  Since 0  cid:20  H X2jX1   cid:20  H X2  = H X1  , we have H X2jX1  H X1   cid:20  1 0  cid:20   cid:26   cid:20  1:  0  cid:20    c   cid:26  = 0 i cid:11  I X1; X2  = 0 i cid:11  X1 and X2 are independent.  d   cid:26  = 1 i cid:11  H X2jX1  = 0 i cid:11  X2 is a function of X1 . By symmetry, X1 is a  function of X2 , i.e., X1 and X2 have a one-to-one relationship.  12. Example of joint entropy. Let p x; y  be given by  @  Y @  @X  0  1  0  1 3  0  1  1 3  1 3  Find   a  H X ; H Y  :  b  H X j Y  ; H Y j X :  c  H X; Y  :  d  H Y    cid:0  H Y j X :  e  I X; Y   .   f  Draw a Venn diagram for the quantities in  a  through  e .  Solution: Example of joint entropy  3 log 3 = 0:918 bits = H Y   .  2 + 1  3 log 3  3 H XjY = 0  + 2   a  H X  = 2  b  H XjY   = 1  c  H X; Y   = 3  cid:2  1  d  H Y    cid:0  H Y jX  = 0:251 bits.  e  I X; Y   = H Y    cid:0  H Y jX  = 0:251 bits.  f  See Figure 1.  3 log 3 = 1:585 bits.  3 H XjY = 1  = 0:667 bits = H Y jX  .  13. Inequality. Show ln x  cid:21  1  cid:0  1  x for x > 0:  Solution: Inequality. Using the Remainder form of the Taylor expansion of ln x  about x = 1 , we have for some c between 1 and x  ln x  = ln 1  + cid:18  1  t cid:19 t=1   x  cid:0  1  + cid:18  cid:0 1  t2  cid:19 t=c   x  cid:0  1 2  2   cid:20  x  cid:0  1   18  Entropy, Relative Entropy and Mutual Information  Figure 2.1: Venn diagram to illustrate the relationships of entropy and relative entropy  H X   H XY   I X;Y   H YX   H Y   since the second term is always negative. Hence letting y = 1=x , we obtain  or  with equality i cid:11  y = 1 .   cid:0  ln y  cid:20   1 y  cid:0  1  ln y  cid:21  1  cid:0   1 y  14. Entropy of a sum. Let X and Y be random variables that take on values x1; x2; : : : ; xr  and y1; y2; : : : ; ys , respectively. Let Z = X + Y:   a  Show that H ZjX  = H Y jX : Argue that if X; Y are independent, then H Y    cid:20  H Z  and H X   cid:20  H Z : Thus the addition of independent random variables adds uncertainty.   b  Give an example of  necessarily dependent  random variables in which H X  >  H Z  and H Y   > H Z :   c  Under what conditions does H Z  = H X  + H Y   ?  Solution: Entropy of a sum.   a  Z = X + Y . Hence p Z = zjX = x  = p Y = z  cid:0  xjX = x  .  H ZjX  = X p x H ZjX = x   p x Xz p x Xy  =  cid:0 Xx = Xx = X p x H Y jX = x  = H Y jX :  p Z = zjX = x  log p Z = zjX = x   p Y = z  cid:0  xjX = x  log p Y = z  cid:0  xjX = x    Entropy, Relative Entropy and Mutual Information  19  If X and Y are independent, then H Y jX  = H Y   . Since I X; Z   cid:21  0 , we have H Z   cid:21  H ZjX  = H Y jX  = H Y   . Similarly we can show that H Z   cid:21  H X  .   b  Consider the following joint distribution for X and Y Let  X =  cid:0 Y =  1 with probability 1=2  0 with probability 1=2  Then H X  = H Y   = 1 , but Z = 0 with prob. 1 and hence H Z  = 0 .   c  We have  H Z   cid:20  H X; Y    cid:20  H X  + H Y    because Z is a function of  X; Y   and H X; Y   = H X  + H Y jX   cid:20  H X  + H Y   . We have equality i cid:11   X; Y   is a function of Z and H Y   = H Y jX  , i.e., X and Y are independent.  15. Data processing. Let X1 ! X2 ! X3 !  cid:1  cid:1  cid:1  ! Xn form a Markov chain in this  order; i.e., let  p x1; x2; : : : ; xn  = p x1 p x2jx1  cid:1  cid:1  cid:1  p xnjxn cid:0 1 :  Reduce I X1; X2; : : : ; Xn  to its simplest form.  Solution: Data Processing. By the chain rule for mutual information,  I X1; X2; : : : ; Xn  = I X1; X2  + I X1; X3jX2  + cid:1  cid:1  cid:1  + I X1; XnjX2; : : : ; Xn cid:0 2 :  2.20  By the Markov property, the past and the future are conditionally independent given the present and hence all terms except the  cid:12 rst are zero. Therefore  I X1; X2; : : : ; Xn  = I X1; X2 :   2.21   16. Bottleneck. Suppose a  non-stationary  Markov chain starts in one of n states, necks down to k   k states. Thus X1 ! X2 ! X3 , i.e., p x1; x2; x3  = p x1 p x2jx1 p x3jx2  , for all x1 2 f1; 2; : : : ; ng , x2 2 f1; 2; : : : ; kg , x3 2 f1; 2; : : : ; mg .   a  Show that the dependence of X1 and X3 is limited by the bottleneck by proving   b  Evaluate I X1; X3  for k = 1 , and conclude that no dependence can survive such  that I X1; X3   cid:20  log k:  a bottleneck.  Solution:  Bottleneck.   20  Entropy, Relative Entropy and Mutual Information   a  From the data processing inequality, and the fact that entropy is maximum for a  uniform distribution, we get  I X1; X3   cid:20  I X1; X2   = H X2   cid:0  H X2 j X1   cid:20  H X2   cid:20  log k:  Thus, the dependence between X1 and X3 is limited by the size of the bottleneck. That is I X1; X3   cid:20  log k .   b  For k = 1 , I X1; X3   cid:20  log 1 = 0 and since I X1; X3   cid:21  0 , I X1; X3  = 0 .  Thus, for k = 1 , X1 and X3 are independent.  17. Pure randomness and bent coins. Let X1; X2; : : : ; Xn denote the outcomes of independent  cid:13 ips of a bent coin. Thus Pr fXi = 1g = p; Pr fXi = 0g = 1  cid:0  p , where p is unknown. We wish to obtain a sequence Z1; Z2; : : : ; ZK of fair coin  cid:13 ips from X1; X2; : : : ; Xn . Toward this end let f : X n ! f0; 1g cid:3  ,  where f0; 1g cid:3  = f cid:3 ; 0; 1; 00; 01; : : :g is the set of all  cid:12 nite length binary sequences , be a mapping f  X1; X2; : : : ; Xn  =  Z1; Z2; : : : ; ZK  , where Zi  cid:24  Bernoulli   1 2   , and K may depend on  X1; : : : ; Xn  . In order that the sequence Z1; Z2; : : : appear to be fair coin  cid:13 ips, the map f from bent coin  cid:13 ips to fair  cid:13 ips must have the property that all 2k sequences  Z1; Z2; : : : ; Zk  of a given length k have equal probability  possibly 0 , for k = 1; 2; : : : . For example, for n = 2 , the map f  01  = 0 , f  10  = 1 , f  00  = f  11  =  cid:3   the null string , has the property that PrfZ1 = 1jK = 1g = PrfZ1 = 0jK = 1g = 1 2 . Give reasons for the following inequalities:  nH p    a  = H X1; : : : ; Xn   b    c    cid:21  H Z1; Z2; : : : ; ZK; K  = H K  + H Z1; : : : ; ZKjK   d  = H K  + E K   e    cid:21  EK:  nH p    a  = H X1; : : : ; Xn   b    cid:21  H Z1; Z2; : : : ; ZK   Thus no more than nH p  fair coin tosses can be derived from  X1; : : : ; Xn  , on the average. Exhibit a good map f on sequences of length 4.  Solution: Pure randomness and bent coins.   Entropy, Relative Entropy and Mutual Information  21   c  = H Z1; Z2; : : : ; ZK; K   d  = H K  + H Z1; : : : ; ZKjK   e  = H K  + E K   f     cid:21  EK :   a  Since X1; X2; : : : ; Xn are i.i.d. with probability of Xi = 1 being p , the entropy  H X1; X2; : : : ; Xn  is nH p  .   b  Z1; : : : ; ZK is a function of X1; X2; : : : ; Xn , and since the entropy of a function of a random variable is less than the entropy of the random variable, H Z1; : : : ; ZK   cid:20  H X1; X2; : : : ; Xn  .   c  K is a function of Z1; Z2; : : : ; ZK , so its conditional entropy given Z1; Z2; : : : ; ZK is 0. Hence H Z1; Z2; : : : ; ZK; K  = H Z1; : : : ; ZK  + H KjZ1; Z2; : : : ; ZK  = H Z1; Z2; : : : ; ZK :   d  Follows from the chain rule for entropy.   e  By assumption, Z1; Z2; : : : ; ZK are pure random bits  given K  , with entropy 1  bit per symbol. Hence  H Z1; Z2; : : : ; ZKjK  = Xk = Xk  = EK:  p k k  p K = k H Z1; Z2; : : : ; ZkjK = k    2.22    f  Follows from the non-negativity of discrete entropy.   g  Since we do not know p , the only way to generate pure random bits is to use the fact that all sequences with the same number of ones are equally likely. For example, the sequences 0001,0010,0100 and 1000 are equally likely and can be used to generate 2 pure random bits. An example of a mapping to generate random bits is  0000 !  cid:3  0001 ! 00 0010 ! 01 0100 ! 10 1000 ! 11 0011 ! 00 0110 ! 01 1100 ! 10 1001 ! 11 1010 ! 0 1110 ! 11 1101 ! 10 1011 ! 01 0111 ! 00 1111 !  cid:3   0101 ! 1  The resulting expected number of bits is  EK = 4pq3  cid:2  2 + 4p2q2  cid:2  2 + 2p2q2  cid:2  1 + 4p3q  cid:2  2  = 8pq3 + 10p2q2 + 8p3q:   2.26    2.27    2.23    2.24    2.25    22  Entropy, Relative Entropy and Mutual Information  For example, for p  cid:25  1 2 , the expected number of pure random bits is close to 1.625. This is substantially less then the 4 pure random bits that could be generated if p were exactly 1 2 . We will now analyze the e cid:14 ciency of this scheme of generating random bits for long sequences of bent coin  cid:13 ips. Let n be the number of bent coin  cid:13 ips. The algorithm that we will use is the obvious extension of the above method of generating pure bits using the fact that all sequences with the same number of ones are equally likely.  which are powers of 2. The largest set would have a size 2blog  n  k cid:1  were a power of 2, then we could generate log cid:0 n  Consider all sequences with k ones. There are  cid:0 n k cid:1  such sequences, which are k cid:1  pure all equally likely. If  cid:0 n random bits from such a set. However, in the general case,  cid:0 n k cid:1  is not a power of 2 and the best we can to is the divide the set of  cid:0 n k cid:1  elements into subset of sizes k cid:1 c random bits. We could divide the remaining elements used to generate blog cid:0 n k cid:1  = 2l+1  cid:0  1 , in which case the subsets would be of sizes 2l; 2l cid:0 1; 2l cid:0 2; : : : ; 1 .  cid:0 n k cid:1 c . Then at least of random bits generated from a set of size  cid:0 n half of the elements belong to a set of size 2l and would generate l random bits, at least 1 4 th belong to a set of size 2l cid:0 1 and generate l  cid:0  1 random bits, etc. On the average, the number of bits generated is  into the largest set which is a power of 2, etc. The worst case would occur when  Instead of analyzing the scheme exactly, we will just  cid:12 nd a lower bound on number  k cid:1  . Let l = blog cid:0 n  k c and could be  E[Kjk 1’s in sequence]  cid:21   1   l  cid:0  1  +  cid:1  cid:1  cid:1  + 4 cid:18 1 + 3 + 8  1 2  2 4  +  1 2l 1 +  cid:1  cid:1  cid:1  +  1 4  l +  1 2 = l  cid:0   cid:21  l  cid:0  1;   2.28   2l cid:0 2 cid:19   2.29  l  cid:0  1   2.30   since the in cid:12 nite series sums to 1.  Hence the fact that  cid:0 n  in the number of random bits that are produced. Hence, the expected number of pure random bits produced by this algorithm is  k cid:1  is not a power of 2 will cost at most 1 bit on the average  EK  cid:21   k!  cid:0  1c k!  cid:0  2! k!  cid:0  2  n  n  Xk=0 n k!pkqn cid:0 kblog n k!pkqn cid:0 k log n Xk=0 n Xk=0 n k!pkqn cid:0 k log n Xn p cid:0  cid:15   cid:20 k cid:20 n p+ cid:15   n  n   cid:21   =   cid:21   k!pkqn cid:0 k log n  k!  cid:0  2:   2.31    2.32    2.33    2.34    Entropy, Relative Entropy and Mutual Information  23  Now for su cid:14 ciently large n , the probability that the number of 1’s in the sequence is close to np is near 1  by the weak law of large numbers . For such sequences, k n is close to p and hence there exists a  cid:14  such that   n k!  cid:21  2n H  k  n   cid:0  cid:14    cid:21  2n H p  cid:0 2 cid:14     2.35   using Stirling’s approximation for the binomial coe cid:14 cients and the continuity of the entropy function. If we assume that n is large enough so that the probability that n p  cid:0   cid:15    cid:20  k  cid:20  n p +  cid:15   is greater than 1  cid:0   cid:15  , then we see that EK  cid:21   1 cid:0   cid:15  n H p  cid:0  2 cid:14   cid:0  2 , which is very good since nH p  is an upper bound on the number of pure random bits that can be produced from the bent coin sequence.  18. World Series. The World Series is a seven-game series that terminates as soon as either team wins four games. Let X be the random variable that represents the outcome of a World Series between teams A and B; possible values of X are AAAA, BABABAB, and BBBAAAA. Let Y be the number of games played, which ranges from 4 to 7. Assuming that A and B are equally matched and that the games are independent, calculate H X  , H Y   , H Y jX  , and H XjY   . Solution:  World Series. Two teams play until one of them has won 4 games.  There are 2  AAAA, BBBB  World Series with 4 games. Each happens with probability  1=2 4 .  There are 8 = 2 cid:0 4 There are 20 = 2 cid:0 5 There are 40 = 2 cid:0 6  3 cid:1  World Series with 5 games. Each happens with probability  1=2 5 . 3 cid:1  World Series with 6 games. Each happens with probability  1=2 6 . 3 cid:1  World Series with 7 games. Each happens with probability  1=2 7 .  The probability of a 4 game series   Y = 4   is 2 1=2 4 = 1=8 . The probability of a 5 game series   Y = 5   is 8 1=2 5 = 1=4 . The probability of a 6 game series   Y = 6   is 20 1=2 6 = 5=16 . The probability of a 7 game series   Y = 7   is 40 1=2 7 = 5=16 .  H X  = X p x log  1  p x   = 5:8125  = 2 1=16  log 16 + 8 1=32  log 32 + 20 1=64  log 64 + 40 1=128  log 128  H Y   = X p y log  1  p y   = 1:924  = 1=8 log 8 + 1=4 log 4 + 5=16 log 16=5  + 5=16 log 16=5    24  Entropy, Relative Entropy and Mutual Information  Y is a deterministic function of X, so if you know X there is no randomness in Y. Or, H Y jX  = 0 . Since H X  + H Y jX  = H X; Y   = H Y   + H XjY   , H XjY   = H X  + H Y jX   cid:0  H Y   = 3:889  it is easy to determine  19. In cid:12 nite entropy. This problem shows that the entropy of a discrete random variable  can be in cid:12 nite. Let A = P1n=2 n log2 n  cid:0 1 .  It is easy to show that A is  cid:12 nite by bounding the in cid:12 nite sum by the integral of  x log 2 x  cid:0 1 .  Show that the integer- valued random variable X de cid:12 ned by Pr X = n  =  An log 2 n  cid:0 1 for n = 2; 3; : : : , has H X  = +1 . Solution: In cid:12 nite entropy. By de cid:12 nition, pn = Pr X = n  = 1=An log2 n for n  cid:21  2 . Therefore  H X  =  cid:0   =  cid:0   =  =  p n  log p n   1Xn=2 1Xn=2 cid:16 1=An log2 n cid:17  log cid:16 1=An log2 n cid:17  1Xn=2 1Xn=2  log A + log n + 2 log log n  log An log2 n   An log2 n  An log2 n  1Xn=2  1  An log n  +  1Xn=2  2 log log n An log2 n  :  = log A +  The  cid:12 rst term is  cid:12 nite. For base 2 logarithms, all the elements in the sum in the last term are nonnegative.  For any other base, the terms of the last sum eventually all become positive.  So all we have to do is bound the middle sum, which we do by comparing with an integral.  1  1Xn=2  >Z 1 We conclude that H X  = +1 .  An log n  2  1  Ax log x  dx = K ln ln x cid:12  cid:12  cid:12   1 2  = +1 :  20. Run length coding. Let X1; X2; : : : ; Xn be  possibly dependent  binary random variables. Suppose one calculates the run lengths R =  R1; R2; : : :  of this sequence  in order as they occur . For example, the sequence X = 0001100100 yields run lengths R =  3; 2; 2; 1; 2  . Compare H X1; X2; : : : ; Xn  , H R  and H Xn; R  . Show all equalities and inequalities, and bound all the di cid:11 erences.  Solution: Run length coding. Since the run lengths are a function of X1; X2; : : : ; Xn , H R   cid:20  H X  . Any Xi together with the run lengths determine the entire sequence   Entropy, Relative Entropy and Mutual Information  X1; X2; : : : ; Xn . Hence  H X1; X2; : : : ; Xn  = H Xi; R   = H R  + H XijR   cid:20  H R  + H Xi   cid:20  H R  + 1:  21. Markov’s inequality for probabilities. Let p x  be a probability mass function.  Prove, for all d  cid:21  0 ,  Prfp X   cid:20  dg log cid:18  1  d cid:19   cid:20  H X :  Solution: Markov inequality applied to entropy.  P  p X  < d  log  p x  log  1 d  1 d  1  p x   = Xx:p x <d  cid:20  Xx:p x <d  cid:20  Xx  = H X   p x  log  p x  log  1  p x   25   2.36    2.37    2.38    2.39    2.40    2.41    2.42    2.43    2.44   22. Logical order of ideas. Ideas have been developed in order of need, and then gener- alized if necessary. Reorder the following ideas, strongest  cid:12 rst, implications following:   a  Chain rule for I X1; : : : ; Xn; Y   , chain rule for D p x1; : : : ; xn jjq x1; x2; : : : ; xn   ,  and chain rule for H X1; X2; : : : ; Xn  .   b  D fjjg   cid:21  0 , Jensen’s inequality, I X; Y    cid:21  0 .  Solution: Logical ordering of ideas.   a  The following orderings are subjective. Since I X; Y   = D p x; y jjp x p y   is a special case of relative entropy, it is possible to derive the chain rule for I from the chain rule for D . Since H X  = I X; X  , it is possible to derive the chain rule for H from the chain rule for I . It is also possible to derive the chain rule for I from the chain rule for H as was done in the notes.   b  In class, Jensen’s inequality was used to prove the non-negativity of D . The  inequality I X; Y    cid:21  0 followed as a special case of the non-negativity of D .   26  Entropy, Relative Entropy and Mutual Information  23. Conditional mutual information. Consider a sequence of n binary random vari- ables X1; X2; : : : ; Xn . Each sequence with an even number of 1’s has probability 2 cid:0  n cid:0 1  and each sequence with an odd number of 1’s has probability 0. Find the mutual informations  I X1; X2 ;  I X2; X3jX1 ; : : : ; I Xn cid:0 1; XnjX1; : : : ; Xn cid:0 2 :  Solution: Conditional mutual information.  Consider a sequence of n binary random variables X1; X2; : : : ; Xn . Each sequence of length n with an even number of 1’s is equally likely and has probability 2 cid:0  n cid:0 1  . Any n  cid:0  1 or fewer of these are independent. Thus, for k  cid:20  n  cid:0  1 ,  I Xk cid:0 1; XkjX1; X2; : : : ; Xk cid:0 2  = 0:  However, given X1; X2; : : : ; Xn cid:0 2 , we know that once we know either Xn cid:0 1 or Xn we know the other.  I Xn cid:0 1; XnjX1; X2; : : : ; Xn cid:0 2  = H XnjX1; X2; : : : ; Xn cid:0 2   cid:0  H XnjX1; X2; : : : ; Xn cid:0 1   = 1  cid:0  0 = 1 bit:  24. Average entropy. Let H p  =  cid:0 p log2 p  cid:0   1  cid:0  p  log2 1  cid:0  p  be the binary entropy  function.   a  Evaluate H 1=4  using the fact that log2 3  cid:25  1:584 . Hint: You may wish to consider an experiment with four equally likely outcomes, one of which is more interesting than the others.   b  Calculate the average entropy H p  when the probability p is chosen uniformly  in the range 0  cid:20  p  cid:20  1 .   c   Optional  Calculate the average entropy H p1; p2; p3  where  p1; p2; p3  is a uni-  formly distributed probability vector. Generalize to dimension n .  Solution: Average Entropy.   a  We can generate two bits of information by picking one of four equally likely alternatives. This selection can be made in two steps. First we decide whether the  cid:12 rst outcome occurs. Since this has probability 1=4 , the information generated is H 1=4  . If not the  cid:12 rst outcome, then we select one of the three remaining outcomes; with probability 3=4 , this produces log2 3 bits of information. Thus  H 1=4  +  3=4  log2 3 = 2  and so H 1=4  = 2  cid:0   3=4  log 2 3 = 2  cid:0   :75  1:585  = 0:811 bits.   Entropy, Relative Entropy and Mutual Information  27   b  If p is chosen uniformly in the range 0  cid:20  p  cid:20  1 , then the average entropy  in  nats  is   cid:0 Z 1  0  p ln p +  1  cid:0  p  ln 1  cid:0  p dp =  cid:0 2Z 1  0  x ln x dx =  cid:0 2  x2  2  Therefore the average entropy is 1  2 log2 e = 1= 2 ln 2  = :721 bits.  ln x +  = 1 2 :  1  0  x2  4 ! cid:12  cid:12  cid:12    c  Choosing a uniformly distributed probability vector  p1; p2; p3  is equivalent to choosing a point  p1; p2  uniformly from the triangle 0  cid:20  p1  cid:20  1 , p1  cid:20  p2  cid:20  1 . The probability density function has the constant value 2 because the area of the triangle is 1 2. So the average entropy H p1; p2; p3  is  0Z 1  cid:0 2Z 1  p1  p1 ln p1 + p2 ln p2 +  1  cid:0  p1  cid:0  p2  ln 1  cid:0  p1  cid:0  p2 dp2dp1 :  After some enjoyable calculus, we obtain the  cid:12 nal result 5= 6 ln 2  = 1:202 bits.  25. Venn diagrams. There isn’t realy a notion of mutual information common to three random variables. Here is one attempt at a de cid:12 nition: Using Venn diagrams, we can see that the mutual information common to three random variables X , Y and Z can be de cid:12 ned by  I X; Y ; Z  = I X; Y    cid:0  I X; Y jZ  :  This quantity is symmetric in X , Y and Z , despite the preceding asymmetric de cid:12 - nition. Unfortunately, I X; Y ; Z  is not necessarily nonnegative. Find X , Y and Z such that I X; Y ; Z  < 0 , and prove the following two identities:   a  I X; Y ; Z  = H X; Y; Z   cid:0  H X   cid:0  H Y    cid:0  H Z  + I X; Y   + I Y ; Z  + I Z; X   b  I X; Y ; Z  = H X; Y; Z  cid:0 H X; Y   cid:0 H Y; Z  cid:0 H Z; X +H X +H Y  +H Z  The  cid:12 rst identity can be understood using the Venn diagram analogy for entropy and mutual information. The second identity follows easily from the  cid:12 rst.  Solution: Venn Diagrams. To show the  cid:12 rst identity, I X; Y ; Z  = I X; Y    cid:0  I X; Y jZ  by de cid:12 nition  by chain rule  = I X; Y    cid:0   I X; Y; Z   cid:0  I X; Z   = I X; Y   + I X; Z   cid:0  I X; Y; Z  = I X; Y   + I X; Z   cid:0   H X  + H Y; Z   cid:0  H X; Y; Z   = I X; Y   + I X; Z   cid:0  H X  + H X; Y; Z   cid:0  H Y; Z  = I X; Y   + I X; Z   cid:0  H X  + H X; Y; Z   cid:0   H Y   + H Z   cid:0  I Y ; Z   = I X; Y   + I X; Z  + I Y ; Z  + H X; Y; Z   cid:0  H X   cid:0  H Y    cid:0  H Z : To show the second identity, simply substitute for I X; Y   , I X; Z  , and I Y ; Z  using equations like  I X; Y   = H X  + H Y    cid:0  H X; Y   :  These two identities show that I X; Y ; Z  is a symmetric  but not necessarily nonneg- ative  function of three random variables.   28  Entropy, Relative Entropy and Mutual Information  26. Another proof of non-negativity of relative entropy. In view of the fundamental  nature of the result D pjjq   cid:21  0 , we will give another proof.  a  Show that ln x  cid:20  x  cid:0  1 for 0 < x < 1 .  b  Justify the following steps:   cid:0 D pjjq  = Xx  cid:20  Xx  cid:20  0  p x  ln  q x  p x   p x  cid:18  q x   p x   cid:0  1 cid:19    c  What are the conditions for equality?  Solution: Another proof of non-negativity of relative entropy. In view of the funda- mental nature of the result D pjjq   cid:21  0 , we will give another proof.  a  Show that ln x  cid:20  x  cid:0  1 for 0 < x < 1 .  There are many ways to prove this. The easiest is using calculus. Let  f  x  = x  cid:0  1  cid:0  ln x  for 0 < x < 1 . Then f0 x  = 1  cid:0  1 x2 > 0 , and therefore f  x  is strictly convex. Therefore a local minimum of the function is also a global minimum. The function has a local minimum at the point where f 0 x  = 0 , i.e., when x = 1 . Therefore f  x   cid:21  f  1  , i.e.,  x and f00 x  = 1  x  cid:0  1  cid:0  ln x  cid:21  1  cid:0  1  cid:0  ln 1 = 0  which gives us the desired inequality. Equality occurs only if x = 1 .   b  We let A be the set of x such that p x  > 0 .   cid:0 De pjjq  = Xx2A  cid:20  Xx2A = Xx2A  cid:20  0  p x ln  q x  p x   p x   cid:0  1 cid:19   p x  cid:18  q x  q x   cid:0  Xx2A  p x   The  cid:12 rst step follows from the de cid:12 nition of D , the second step follows from the inequality ln t  cid:20  t  cid:0  1 , the third step from expanding the sum, and the last step from the fact that the q A   cid:20  1 and p A  = 1 .   2.45    2.46    2.47    2.48    2.49    2.50    2.51    2.52    2.53    Entropy, Relative Entropy and Mutual Information  29   c  What are the conditions for equality?  We have equality in the inequality ln t  cid:20  t  cid:0  1 if and only if t = 1 . Therefore we have equality in step 2 of the chain i cid:11  q x =p x  = 1 for all x 2 A . This implies that p x  = q x  for all x , and we have equality in the last step as well. Thus the condition for equality is that p x  = q x  for all x .  27. Grouping rule for entropy: Let p =  p1; p2; : : : ; pm  be a probability distribution on m elements, i.e, pi  cid:21  0 , and Pm i=1 pi = 1 . De cid:12 ne a new distribution q on m  cid:0  1 elements as q1 = p1 , q2 = p2 ,. . . , qm cid:0 2 = pm cid:0 2 , and qm cid:0 1 = pm cid:0 1 + pm , i.e., the distribution q is the same as p on f1; 2; : : : ; m  cid:0  2g , and the probability of the last element in q is the sum of the last two probabilities of p . Show that  H p  = H q  +  pm cid:0 1 + pm H cid:18   pm cid:0 1  pm cid:0 1 + pm  ;  pm  pm cid:0 1 + pm cid:19  :   2.54   Solution:  H p  =  cid:0   =  cid:0   pi log pi  cid:0  pm cid:0 1 log pm cid:0 1  cid:0  pm log pm  m  m cid:0 2  pi log pi  Xi=1 Xi=1 Xi=1 pi log pi  cid:0  pm cid:0 1 log  cid:0  pm cid:0 1 + pm  log pm cid:0 1 + pm  pm cid:0 1 + pm  cid:0  pm log  pm cid:0 1  m cid:0 2  =  cid:0   pm cid:0 1 = H q   cid:0  pm cid:0 1 log = H q   cid:0   pm cid:0 1 + pm  cid:18  = H q  +  pm cid:0 1 + pm H2 cid:18  where H2 a; b  =  cid:0 a log a  cid:0  b log b .  pm  log  pm cid:0 1  pm cid:0 1 + pm pm cid:0 1  pm cid:0 1 + pm  pm cid:0 1 + pm pm cid:0 1 pm cid:0 1 + pm  cid:0  pm cid:0 1 + pm cid:19  ;  pm  ;  pm cid:0 1 + pm  cid:0  pm log  pm cid:0 1 + pm  pm   2.55    2.56    2.57    2.58    2.59    2.61   pm  pm cid:0 1 + pm  log  pm  pm cid:0 1 + pm cid:19  2.60   28. Mixing increases entropy. Show that the entropy of the probability distribution,   p1; : : : ; pi; : : : ; pj; : : : ; pm  , is less than the entropy of the distribution  p1; : : : ; pi+pj makes the distribution more uniform increases the entropy.  ; : : : ; pi+pj  ; : : : ; pm  . Show that in general any transfer of probability that  2  2  Solution:  Mixing increases entropy.  This problem depends on the convexity of the log function. Let  P1 =  p1; : : : ; pi; : : : ; pj; : : : ; pm   P2 =  p1; : : : ;  pi + pj  pj + pi  ; : : : ;  ; : : : ; pm   2  2   30  Entropy, Relative Entropy and Mutual Information  Then, by the log sum inequality,  H P2   cid:0  H P1  =  cid:0 2   2  pi + pj  pi + pj    log     + pi log pi + pj log pj    + pi log pi + pj log pj  Thus,  2 pi + pj  2  =  cid:0  pi + pj  log   cid:21  0:  H P2   cid:21  H P1 :  29. Inequalities. Let X , Y and Z be joint random variables. Prove the following  inequalities and  cid:12 nd conditions for equality.   a  H X; Y jZ   cid:21  H XjZ  .  b  I X; Y ; Z   cid:21  I X; Z  .  c  H X; Y; Z   cid:0  H X; Y    cid:20  H X; Z   cid:0  H X  .  d  I X; ZjY    cid:21  I Z; Y jX   cid:0  I Z; Y   + I X; Z  . Solution: Inequalities.   a  Using the chain rule for conditional entropy,  H X; Y jZ  = H XjZ  + H Y jX; Z   cid:21  H XjZ ;  with equality i cid:11  H Y jX; Z  = 0 , that is, when Y is a function of X and Z .   b  Using the chain rule for mutual information,  I X; Y ; Z  = I X; Z  + I Y ; ZjX   cid:21  I X; Z ;  with equality i cid:11  I Y ; ZjX  = 0 , that is, when Y and Z are conditionally inde- pendent given X .   c  Using  cid:12 rst the chain rule for entropy and then the de cid:12 nition of conditional mutual  information,  H X; Y; Z   cid:0  H X; Y   = H ZjX; Y   = H ZjX   cid:0  I Y ; ZjX    cid:20  H ZjX  = H X; Z   cid:0  H X  ;  with equality i cid:11  I Y ; ZjX  = 0 , that is, when Y and Z are conditionally inde- pendent given X .   d  Using the chain rule for mutual information,  I X; ZjY   + I Z; Y   = I X; Y ; Z  = I Z; Y jX  + I X; Z  ;  and therefore  I X; ZjY   = I Z; Y jX   cid:0  I Z; Y   + I X; Z  : We see that this inequality is actually an equality in all cases.   Entropy, Relative Entropy and Mutual Information  31  30. Maximum entropy. Find the probability mass function p x  that maximizes the entropy H X  of a non-negative integer-valued random variable X subject to the constraint  Notice that the  cid:12 nal right hand side expression is independent of fpig , and that the inequality,  EX =  np n  = A  1Xn=0  for a  cid:12 xed value A > 0 . Evaluate this maximum H X  .  Solution: Maximum entropy  Recall that,   cid:0   1Xi=0  pi log pi  cid:20   cid:0   pi log qi:  1Xi=0  Let qi =  cid:11   cid:12  i . Then we have that,   cid:0   1Xi=0  pi log pi  cid:20   cid:0   pi log qi  1Xi=0  =  cid:0  log  cid:11   =  cid:0  log  cid:11   cid:0  A log  cid:12   1Xi=0  pi + log  cid:12    ipi!  1Xi=0  holds for all  cid:11 ;  cid:12  such that,  The constraint on the expected value also requires that,  Combining the two constraints we have,   cid:0   1Xi=0  pi log pi  cid:20   cid:0  log  cid:11   cid:0  A log  cid:12   1Xi=0   cid:11  cid:12 i = 1 =  cid:11   1 1  cid:0   cid:12   :  1Xi=0  i cid:11  cid:12 i = A =  cid:11    cid:12    1  cid:0   cid:12  2 :   cid:11    cid:12    1  cid:0   cid:12  2 =  cid:18   cid:11  1  cid:0   cid:12  cid:19  cid:18   cid:12   cid:12  1  cid:0   cid:12   =  1  cid:0   cid:12  cid:19   = A;   32  Entropy, Relative Entropy and Mutual Information  which implies that,  So the entropy maximizing distribution is,   cid:12  =   cid:11  =  A  1  A + 1  :  A + 1  pi =  1  A + 1 cid:18  A  A + 1 cid:19 i  :  Plugging these values into the expression for the maximum entropy,  cid:0  log  cid:11   cid:0  A log  cid:12  =  A + 1  log A + 1   cid:0  A log A:  The general form of the distribution,  pi =  cid:11  cid:12 i  can be obtained either by guessing or by Lagrange multipliers where,  F  pi;  cid:21 1;  cid:21 2  =  cid:0   pi log pi +  cid:21 1   pi  cid:0  1  +  cid:21 2   ipi  cid:0  A   1Xi=0  1Xi=0  1Xi=0  is the function whose gradient we set to 0.  To complete the argument with Lagrange multipliers, it is necessary to show that the local maximum is the global maximum. One possible argument is based on the fact that  cid:0 H p  is convex, it has only one local minima, no local maxima and therefore Lagrange multiplier actually gives the global maximum for H p  .  31. Conditional entropy. Under what conditions does H X j g Y    = H X j Y   ?  Solution:  Conditional Entropy . If H Xjg Y    = H XjY   , then H X  cid:0 H Xjg Y    = H X   cid:0  H XjY   , i.e., I X; g Y    = I X; Y   . This is the condition for equality in the data processing inequality. From the derivation of the inequality, we have equal- ity i cid:11  X ! g Y   ! Y forms a Markov chain. Hence H Xjg Y    = H XjY   i cid:11  X ! g Y   ! Y . This condition includes many special cases, such as g being one- to-one, and X and Y being independent. However, these two special cases do not exhaust all the possibilities.  32. Fano. We are given the following joint distribution on  X; Y    X  Y  1 2 3  a  1 6 1 12 1 12  b  1 12 1 6 1 12  c  1 12 1 12 1 6  Let ^X Y   be an estimator for X  based on Y  and let Pe = Prf ^X Y   6= Xg:   Entropy, Relative Entropy and Mutual Information  33   a  Find the minimum probability of error estimator ^X Y   and the associated Pe .  b  Evaluate Fano’s inequality for this problem and compare.  Solution:   a  From inspection we see that  Hence the associated Pe is the sum of P  1; b ; P  1; c ; P  2; a ; P  2; c ; P  3; a  and P  3; b : Therefore, Pe = 1=2:   b  From Fano’s inequality we know  ^X y  =8>< >:  1 y = a 2 y = b 3 y = c  Pe  cid:21   H XjY    cid:0  1  :  log jXj  Pe  cid:21   1:5  cid:0  1 log 3  = :316:  Pe  cid:21   H XjY    cid:0  1 log jXj-1   :  Pe  cid:21   1:5  cid:0  1 log 2  =  1 2  :  H XjY   = H XjY = a  Prfy = ag + H XjY = b  Prfy = bg + H XjY = c  Prfy = cg 4 cid:19  Prfy = cg  4 cid:19  Prfy = bg + H cid:18  1  1 4  1  2  2  1  2  1  ;  ;  ;  ;  ;  ;  = H cid:18  1 = H cid:18  1 = H cid:18  1  1 4 1 4 1 4 = 1:5 bits.  2  2  ;  ;  ;  ;  1  1 4  4 cid:19  Prfy = ag + H cid:18  1 4 cid:19   Prfy = ag + Prfy = bg + Prfy = cg  4 cid:19   1  Hence our estimator ^X Y   is not very close to Fano’s bound in this form. If ^X 2 X ; as it does here, we can use the stronger form of Fano’s inequality to get  Here,  Hence  and  Therefore our estimator ^X Y   is actually quite good.  33. Fano’s inequality. Let Pr X = i  = pi; i = 1; 2; : : : ; m and let p1  cid:21  p2  cid:21  p3  cid:21   cid:1  cid:1  cid:1   cid:21  pm: The minimal probability of error predictor of X is ^X = 1 , with resulting probability of error Pe = 1 cid:0  p1: Maximize H p  subject to the constraint 1 cid:0  p1 = Pe   34  Entropy, Relative Entropy and Mutual Information  to  cid:12 nd a bound on Pe in terms of H . This is Fano’s inequality in the absence of conditioning.  Solution:  Fano’s Inequality.  The minimal probability of error predictor when there is no information is ^X = 1 , the most probable value of X . The probability of error in this case is Pe = 1 cid:0  p1 . Hence if we  cid:12 x Pe , we  cid:12 x p1 . We maximize the entropy of X for a given Pe to obtain an upper bound on the entropy for a given Pe . The entropy,   2.62    2.63    2.64    2.65    2.66    2.67    2.68    2.69   H p  =  cid:0 p1 log p1  cid:0   m  m  pi log pi  Xi=2 pi Xi=2 =  cid:0 p1 log p1  cid:0  Pe = H Pe  + PeH cid:18  p2 p3 Pe  cid:20  H Pe  + Pe log m  cid:0  1 ;  Pe  Pe  ;  log  pi Pe  cid:0  Pe log Pe Pe cid:19   pm  ; : : : ;  since the maximum of H cid:16  p2  Pe  ; p3 Pe  ; : : : ; pm  Pe cid:17  is attained by an uniform distribution. Hence  any X that can be predicted with a probability of error Pe must satisfy  H X   cid:20  H Pe  + Pe log m  cid:0  1 ;  which is the unconditional form of Fano’s inequality. We can weaken this inequality to obtain an explicit lower bound for Pe ,  Pe  cid:21   H X   cid:0  1 log m  cid:0  1   :  34. Entropy of initial conditions. Prove that H X0jXn  is non-decreasing with n for  any Markov chain.  Solution: Entropy of initial conditions. For a Markov chain, by the data processing theorem, we have  I X0; Xn cid:0 1   cid:21  I X0; Xn :  Therefore  or H X0jXn  increases with n .  H X0   cid:0  H X0jXn cid:0 1   cid:21  H X0   cid:0  H X0jXn   35. Relative entropy is not symmetric: Let the random variable X have three possible  outcomes fa; b; cg . Consider two distributions on this random variable Symbol  p x  1 2 1 4 1 4  q x  1 3 1 3 1 3  a b c  Calculate H p  , H q  , D pjjq  and D qjjp  . Verify that in this case D pjjq  6= D qjjp  .   Entropy, Relative Entropy and Mutual Information  35  Solution:  H p  =  log 2 +  log 4 +  log 4 = 1:5 bits:   2.70   1 4  1 4  1 3  1 3  1 2  1 3  1 4  H q  =  log 3 +  log 3 +  log 3 = log 3 = 1:58496 bits:   2.71   3 4  1 2 1 3  3 2 2 3  1 4 1 3  3 4 4 3  +  +  +  log  log  log  log  D pjjq  =  D qjjp  =  = log 3   cid:0  1:5 = 1:58496  cid:0  1:5 = 0:08496  2.72  5 3 cid:0 log 3  = 1:66666 cid:0 1:58496 = 0:08170  2.73  36. Symmetric relative entropy: Though, as the previous example shows, D pjjq  6= D qjjp  in general, there could be distributions for which equality holds. Give an example of two distributions p and q on a binary alphabet such that D pjjq  = D qjjp   other than the trivial case p = q  .  log  log  1 3  4 3  =  +  Solution: A simple case for D  p; 1  cid:0  p jj q; 1  cid:0  q   = D  q; 1  cid:0  q jj p; 1  cid:0  p   , i.e., for  p log  p q  +  1  cid:0  p  log  = q log  +  1  cid:0  q  log  q p  1  cid:0  p 1  cid:0  q  1  cid:0  q 1  cid:0  p   2.74   is when q = 1  cid:0  p .  37. Relative entropy: Let X; Y; Z be three random variables with a joint probability mass function p x; y; z  . The relative entropy between the joint distribution and the product of the marginals is  D p x; y; z jjp x p y p z   = E cid:20 log  p x; y; z   p x p y p z  cid:21   Expand this in terms of entropies. When is this quantity zero?  Solution:  D p x; y; z jjp x p y p z   = E cid:20 log  p x; y; z   p x p y p z  cid:21    2.75    2.76   = E[log p x; y; z ]  cid:0  E[log p x ]  cid:0  E[log p y ]  cid:0  E[log p z ]  2.77  =  cid:0 H X; Y; Z  + H X  + H Y   + H Z   2.78   We have D p x; y; z jjp x p y p z   = 0 if and only p x; y; z  = p x p y p z  for all  x; y; z  , i.e., if X and Y and Z are independent.  38. The value of a question Let X  cid:24  p x  , x = 1; 2; : : : ; m . We are given a set  S  cid:18  f1; 2; : : : ; mg . We ask whether X 2 S and receive the answer  Y =  1;  0;  if X 2 S if X 62 S:  Suppose PrfX 2 Sg =  cid:11  . Find the decrease in uncertainty H X   cid:0  H XjY   .   36  Entropy, Relative Entropy and Mutual Information  Apparently any set S with a given  cid:11  is as good as any other.  Solution: The value of a question.  H X   cid:0  H XjY   = I X; Y    = H Y    cid:0  H Y jX  = H  cid:11    cid:0  H Y jX  = H  cid:11    since H Y jX  = 0 .  39. Entropy and pairwise independence. Let X; Y; Z be three binary Bernoulli   1 dent, that is, I X; Y   = I X; Z  = I Y ; Z  = 0 .  2   random variables that are pairwise indepen-   a  Under this constraint, what is the minimum value for H X; Y; Z  ?   b  Give an example achieving this minimum.  Solution:   a   H X; Y; Z  = H X; Y   + H ZjX; Y     cid:21  H X; Y   = 2:   2.79    2.80    2.81   So the minimum value for H X; Y; Z  is at least 2. To show that is is actually equal to 2, we show in part  b  that this bound is attainable.   b  Let X and Y be iid Bernoulli  1  2   and let Z = X  cid:8  Y , where  cid:8  denotes addition  mod 2  xor .  40. Discrete entropies  Let X and Y be two independent integer-valued random variables. Let X be uniformly distributed over f1; 2; : : : ; 8g , and let PrfY = kg = 2 cid:0 k ,  a  Find H X   k = 1; 2; 3; : : :   b  Find H Y    c  Find H X + Y; X  cid:0  Y   . Solution:   a  For a uniform distribution, H X  = log m = log 8 = 3 .   b  For a geometric distribution, H Y   = Pk k2 cid:0 k = 2 .  See solution to problem 2.1   Entropy, Relative Entropy and Mutual Information  37   c  Since  X; Y   !  X +Y; X cid:0 Y   is a one to one transformation, H X +Y; X cid:0 Y   =  H X; Y   = H X  + H Y   = 3 + 2 = 5 .  41. Random questions  One wishes to identify a random object X  cid:24  p x  . A question Q  cid:24  r q  is asked at random according to r q  . This results in a deterministic answer A = A x; q  2 fa1; a2; : : :g . Suppose X and Q are independent. Then I X; Q; A  is the uncertainty in X removed by the question-answer  Q; A  .   a  Show I X; Q; A  = H AjQ  . Interpret.  b  Now suppose that two i.i.d. questions Q1; Q2; cid:24  r q  are asked, eliciting answers A1 and A2 . Show that two questions are less valuable than twice a single question in the sense that I X; Q1; A1; Q2; A2   cid:20  2I X; Q1; A1  .  Solution: Random questions.   a   I X; Q; A  = H Q; A   cid:0  H Q; A;jX   = H Q  + H AjQ   cid:0  H QjX   cid:0  H AjQ; X  = H Q  + H AjQ   cid:0  H Q  = H AjQ   The interpretation is as follows. The uncertainty removed in X by  Q; A  is the same as the uncertainty in the answer given the question.   b  Using the result from part a and the fact that questions are independent, we can  easily obtain the desired relationship.  I X; Q1; A1; Q2; A2    a    c    b   = I X; Q1  + I X; A1jQ1  + I X; Q2jA1; Q1  + I X; A2jA1; Q1; Q2  = I X; A1jQ1  + H Q2jA1; Q1   cid:0  H Q2jX; A1; Q1  + I X; A2jA1; Q1; Q2  = I X; A1jQ1  + I X; A2jA1; Q1; Q2  = I X; A1jQ1  + H A2jA1; Q1; Q2   cid:0  H A2jX; A1; Q1; Q2  = I X; A1jQ1  + H A2jA1; Q1; Q2   cid:20  I X; A1jQ1  + H A2jQ2  = 2I X; A1jQ1    e    d    f     a  Chain rule.  b  X and Q1 are independent.   38  Entropy, Relative Entropy and Mutual Information   c  Q2 are independent of X , Q1 , and A1 .  d  A2 is completely determined given Q2 and X .  e  Conditioning decreases entropy.  f  Result from part a.  42. Inequalities. Which of the following inequalities are generally  cid:21 ; =; cid:20  ? Label each  with  cid:21 ; =; or  cid:20  .  a  H 5X  vs. H X   b  I g X ; Y   vs. I X; Y    c  H X0jX cid:0 1  vs. H X0jX cid:0 1; X1   d  H X1; X2; : : : ; Xn  vs. H c X1; X2; : : : ; Xn   , where c x1; x2; : : : ; xn  is the Hu cid:11 -  man codeword assigned to  x1; x2; : : : ; xn  .   e  H X; Y  = H X  + H Y    vs. 1  Solution:   a  X ! 5X is a one to one mapping, and hence H X  = H 5X  .  b  By data processing inequality, I g X ; Y    cid:20  I X; Y   .  c  Because conditioning reduces entropy, H X0jX cid:0 1   cid:21  H X0jX cid:0 1; X1  .  d  H X; Y    cid:20  H X  + H Y   , so H X; Y  = H X  + H Y     cid:20  1 .  43. Mutual information of heads and tails.   a  Consider a fair coin  cid:13 ip. What is the mutual information between the top side  and the bottom side of the coin?   b  A 6-sided fair die is rolled. What is the mutual information between the top side  and the front face  the side most facing you ?  Solution:  Mutual information of heads and tails.  To prove  a  observe that  I T ; B  = H B   cid:0  H BjT    = log 2 = 1  since B  cid:24  Ber 1=2  , and B = f  T   . Here B; T stand for Bottom and Top respectively. To prove  b  note that having observed a side of the cube facing us F , there are four possibilities for the top T , which are equally probable. Thus,  I T ; F   = H T    cid:0  H TjF    = log 6  cid:0  log 4 = log 3  cid:0  1 since T has uniform distribution on f1; 2; : : : ; 6g .   Entropy, Relative Entropy and Mutual Information  39  44. Pure randomness  We wish to use a 3-sided coin to generate a fair coin toss. Let the coin X have probability mass function  A; pA B; pB C; pC  X =8>< >:  where pA; pB; pC are unknown.   a  How would you use 2 independent  cid:13 ips X1; X2 to generate  if possible  a Bernoulli  1 2    random variable Z ?   b  What is the resulting maximum expected number of fair bits generated?  Solution:   a  The trick here is to notice that for any two letters Y and Z produced by two independent tosses of our bent three-sided coin, Y Z has the same probability as ZY . So we can produce B  cid:24  Bernoulli  1 2   coin  cid:13 ips by letting B = 0 when we get AB , BC or AC , and B = 1 when we get BA , CB or CA  if we get AA , BB or CC we don’t assign a value to B .    b  The expected number of bits generated by the above scheme is as follows. We get one bit, except when the two  cid:13 ips of the 3-sided coin produce the same symbol. So the expected number of fair bits generated is  0  cid:3  [P  AA  + P  BB  + P  CC ] + 1  cid:3  [1  cid:0  P  AA   cid:0  P  BB   cid:0  P  CC ];   2.82   or,  1  cid:0  p2  A  cid:0  p2  B  cid:0  p2 C:   2.83   45. Finite entropy. Show that for a discrete random variable X 2 f1; 2; : : :g , if E log X < 1 , then H X  < 1 . Solution: Let the distribution on the integers be p1; p2; : : : . Then H p  =  cid:0 P pilogpi and E log X =P pilogi = c < 1 .  We will now  cid:12 nd the maximum entropy distribution subject to the constraint on the expected logarithm. Using Lagrange multipliers or the results of Chapter 12, we have the following functional to optimize  J p  =  cid:0 X pi log pi  cid:0   cid:21 1X pi  cid:0   cid:21 2X pi log i  Di cid:11 erentiating with respect to pi and setting to zero, we  cid:12 nd that the pi that maximizes  the entropy set pi = ai cid:21  , where a = 1= P i cid:21   and  cid:21  chosed to meet the expected log  constraint, i.e.  Using this value of pi , we can see that the entropy is  cid:12 nite.  X i cid:21  log i = cX i cid:21    2.84    2.85    40  Entropy, Relative Entropy and Mutual Information  46. Axiomatic de cid:12 nition of entropy. If we assume certain axioms for our measure of information, then we will be forced to use a logarithmic measure like entropy. Shannon used this to justify his initial de cid:12 nition of entropy. In this book, we will rely more on the other properties of entropy rather than its axiomatic derivation to justify its use. The following problem is considerably more di cid:14 cult than the other problems in this section.  If a sequence of symmetric functions Hm p1; p2; : : : ; pm  satis cid:12 es the following proper- ties,  2 ; 1   cid:15  Normalization: H2 cid:16  1  cid:15  Continuity: H2 p; 1  cid:0  p  is a continuous function of p ,  cid:15  Grouping: Hm p1; p2; : : : ; pm  = Hm cid:0 1 p1+p2; p3; : : : ; pm + p1+p2 H2 cid:16  p1  2 cid:17  = 1;  p1+p2  ;  p2  p1+p2 cid:17  ,  prove that Hm must be of the form  Hm p1; p2; : : : ; pm  =  cid:0   pi log pi;  m = 2; 3; : : : :   2.86   m  Xi=1  There are various other axiomatic formulations which also result in the same de cid:12 nition of entropy. See, for example, the book by Csisz cid:19 ar and K cid:127 orner[3].  Solution: Axiomatic de cid:12 nition of entropy. This is a long solution, so we will  cid:12 rst outline what we plan to do. First we will extend the grouping axiom by induction and prove that  Hm p1; p2; : : : ; pm  = Hm cid:0 k p1 + p2 +  cid:1  cid:1  cid:1  + pk; pk+1; : : : ; pm   + p1 + p2 +  cid:1  cid:1  cid:1  + pk Hk cid:18   p1  p1 + p2 +  cid:1  cid:1  cid:1  + pk  ; : : : ;  pk  p1 + p2 +  cid:1  cid:1  cid:1  + pk cid:19  : 2.87   Let f  m  be the entropy of a uniform distribution on m symbols, i.e.,  f  m  = Hm cid:18  1  m  ;  1 m  ; : : : ;  1  m cid:19  :  We will then show that for any two integers r and s , that f  rs  = f  r  + f  s  . We use this to show that f  m  = log m . We then show for rational p = r=s , that H2 p; 1 cid:0  p  =  cid:0 p log p cid:0   1 cid:0  p  log 1 cid:0  p  . By continuity, we will extend it to irrational p and  cid:12 nally by induction and grouping, we will extend the result to Hm for m  cid:21  2 . To begin, we extend the grouping axiom. For convenience in notation, we will let  Sk =  pi  k  Xi=1  and we will denote H2 q; 1  cid:0  q  as h q  . Then we can write the grouping axiom as  Hm p1; : : : ; pm  = Hm cid:0 1 S2; p3; : : : ; pm  + S2h cid:18  p2 S2 cid:19  :   2.88    2.89    2.90    Entropy, Relative Entropy and Mutual Information  Applying the grouping axiom again, we have  S2 cid:19  Hm p1; : : : ; pm  = Hm cid:0 1 S2; p3; : : : ; pm  + S2h cid:18  p2 S3 cid:19  + S2h cid:18  p2 S2 cid:19  = Hm cid:0 2 S3; p4; : : : ; pm  + S3h cid:18  p3 ...  = Hm cid:0  k cid:0 1  Sk; pk+1; : : : ; pm  +  k  Xi=2  Sih cid:18  pi Si cid:19  :  Now, we apply the same grouping axiom repeatedly to Hk p1=Sk; : : : ; pk=Sk  , to obtain  Hk cid:18  p1  Sk  ; : : : ;  ;  pk  pk  Sk  Sk cid:19  = H2 cid:18  Sk cid:0 1 Xi=2  Sk cid:19  + Sih cid:18  pi Si cid:19  :  1 Sk  =  k  k cid:0 1  Xi=2  Si Sk  Si=Sk cid:19  h cid:18  pi=Sk  From  2.94  and  2.96 , it follows that  Hm p1; : : : ; pm  = Hm cid:0 k Sk; pk+1; : : : ; pm  + SkHk cid:18  p1  Sk  ; : : : ;  pk  Sk cid:19  ;  which is the extended grouping axiom.  Now we need to use an axiom that is not explicitly stated in the text, namely that the function Hm is symmetric with respect to its arguments. Using this, we can combine any set of arguments of Hm using the extended grouping axiom. Let f  m  denote Hm  1 Consider  m ; : : : ; 1  m ; 1  m   .  f  mn  = Hmn   1 mn  ;  1 mn  ; : : : ;  1 mn   :  By repeatedly applying the extended grouping axiom, we have     1 mn 1 mn  ; : : : ;    +  Hn   ; : : : ;  1 m  1 mn  1 n  2 m  1 n     1 n  1 n     ; : : : ;    +  Hn   ; : : : ;  1 mn  f  mn  = Hmn   ; : : : ;  ;  1 mn 1 = Hmn cid:0 n  m 1 = Hmn cid:0 2n  m ...  ;  1 mn 1 mn 1 m  ;  ;  = Hm   1 m = f  m  + f  n :  1 m  ; : : : :    + H   ; : : : ;  1 n  1 n     41   2.91    2.92    2.93    2.94    2.95    2.96    2.97    2.98    2.99    2.100    2.101    2.102    2.103    2.104    42  Entropy, Relative Entropy and Mutual Information  We can immediately use this to conclude that f  mk  = kf  m  .  Now, we will argue that H2 1; 0  = h 1  = 0 . We do this by expanding H3 p1; p2; 0    p1 + p2 = 1   in two di cid:11 erent ways using the grouping axiom  H3 p1; p2; 0  = H2 p1; p2  + p2H2 1; 0   = H2 1; 0  +  p1 + p2 H2 p1; p2    2.105    2.106   Thus p2H2 1; 0  = H2 1; 0  for all p2 , and therefore H 1; 0  = 0 . We will also need to show that f  m + 1   cid:0  f  m  ! 0 as m ! 1 . To prove this, we use the extended grouping axiom and write  f  m + 1  = Hm+1   1  m + 1  ; : : : ;  m  m    +    +  1     m + 1 1 m  Hm   f  m   m + 1  m + 1  m + 1  m + 1  = h   = h   ; : : : ;  1 m     1  1  m  f  m + 1   cid:0   m + 1  f  m  = h   1   :  m + 1  Thus lim f  m + 1   cid:0  m that the limit on the right is h 0  = 0 . Thus lim h   m+1 f  m  = lim h   1  1  m+1   = 0 .  m+1  : But by the continuity of H2 , it follows  and therefore  Let us de cid:12 ne  and  Then  and therefore  an+1 = f  n + 1   cid:0  f  n   bn = h    :  1 n  an+1 =  cid:0   n + 1  f  n  + bn+1  =  cid:0   n + 1  ai + bn+1  1  1  n  Xi=2   2.107    2.108    2.109    2.110    2.111    2.112    2.113    2.114    n + 1 bn+1 =  n + 1 an+1 +  ai:   2.115   n  Xi=2  Therefore summing over n , we have  N  Xn=2  N  Xn=2  nbn =   nan + an cid:0 1 + : : : + a2  = N   2.116   N  Xn=2  ai:   Entropy, Relative Entropy and Mutual Information  Dividing both sides by PN  n=1 n = N  N + 1 =2 , we obtain  2  N + 1  N  Xn=2  n=2 nbn n=2 n  an = PN PN  Now by continuity of H2 and the de cid:12 nition of bn , it follows that bn ! 0 as n ! 1 . Since the right hand side is essentially an average of the bn ’s, it also goes to 0  This can be proved more precisely using  cid:15  ’s and  cid:14  ’s . Thus the left hand side goes to 0. We can then see that  aN +1 = bN +1  cid:0   N + 1  1  N  Xn=2  an  f  n + 1   cid:0  f  n  ! 0  asn ! 1:  also goes to 0 as N ! 1 . Thus  We will now prove the following lemma  Lemma 2.0.1 Let the function f  m  satisfy the following assumptions:   cid:15  f  mn  = f  m  + f  n  for all integers m , n .  cid:15  limn!1 f  n + 1   cid:0  f  n   = 0  cid:15  f  2  = 1 ,  then the function f  m  = log2 m .  Proof of the lemma: Let P be an arbitrary prime number and let  g n  = f  n   cid:0   f  P   log2 n  log2 P  Then g n  satis cid:12 es the  cid:12 rst assumption of the lemma. Also g P   = 0 .  Also if we let   cid:11 n = g n + 1   cid:0  g n  = f  n + 1   cid:0  f  n  +  f  P   log2 P  log2  n  n + 1  then the second assumption in the lemma implies that lim  cid:11 n = 0 .  For an integer n , de cid:12 ne  Then it follows that n 1  < n=P , and  P cid:23  : n 1  = cid:22  n  n = n 1 P + l  43   2.117    2.118    2.119    2.120    2.121    2.122    2.123    44  Entropy, Relative Entropy and Mutual Information  where 0  cid:20  l < P . From the fact that g P   = 0 , it follows that g P n 1   = g n 1   , and  g n  = g n 1   + g n   cid:0  g P n 1   = g n 1   +   2.124   n cid:0 1  Xi=P n 1    cid:11 i  Just as we have de cid:12 ned n 1  from n , we can de cid:12 ne n 2  from n 1  . Continuing this process, we can then write  Since n k   cid:20  n=P k , after  g n  = g n k   +   cid:11 i1 A :  n i cid:0 1   Xi=P n i   k  Xj=1  0 @ log P cid:23  + 1  k = cid:22  log n   2.125    2.126   terms, we have n k  = 0 , and g 0  = 0  this follows directly from the additive property of g  . Thus we can write  the sum of bn terms, where  Since  cid:11 n ! 0 , it follows that Thus it follows that  tn   cid:11 i  g n  =   2.127   Xi=1 bn  cid:20  P  cid:18  log n log2 n ! 0 , since g n  has at most o log2 n  terms  cid:11 i .  + 1 cid:19  :   2.128   log P  g n   f  n  log2 n  =  f  P   log2 P  lim n!1   2.129   Since P was arbitrary, it follows that f  P  = log2 P = c for every prime number P . Applying the third axiom in the lemma, it follows that the constant is 1, and f  P   = log2 P . For composite numbers N = P1P2 : : : Pl , we can apply the  cid:12 rst property of f and the prime number factorization of N to show that  f  N   =X f  Pi  =X log2 Pi = log2 N:   2.130   Thus the lemma is proved.  The lemma can be simpli cid:12 ed considerably, if instead of the second assumption, we replace it by the assumption that f  n  is monotone in n . We will now argue that the only function f  m  such that f  mn  = f  m  + f  n  for all integers m; n is of the form f  m  = loga m for some base a . Let c = f  2  . Now f  4  = f  2  cid:2  2  = f  2  + f  2  = 2c . Similarly, it is easy to see that f  2k  = kc = c log2 2k . We will extend this to integers that are not powers of 2.   Entropy, Relative Entropy and Mutual Information  45  For any integer m , let r > 0 , be another integer and let 2k  cid:20  mr < 2k+1 . Then by the monotonicity assumption on f , we have  or  Now by the monotonicity of log , we have  Combining these two equations, we obtain  Since r was arbitrary, we must have  kc  cid:20  rf  m  <  k + 1 c  c  k r  cid:20  f  m  < c  k + 1  r  r   cid:12  cid:12  cid:12  cid:12   k r  cid:20  log2 m <  k + 1  f  m   cid:0   log2 m  c  <  1 r   cid:12  cid:12  cid:12  cid:12   f  m  =  log2 m  c  and we can identify c = 1 from the last assumption of the lemma.  Now we are almost done. We have shown that for any uniform distribution on m outcomes, f  m  = Hm 1=m; : : : ; 1=m  = log2 m . We will now show that  H2 p; 1  cid:0  p  =  cid:0 p log p  cid:0   1  cid:0  p  log 1  cid:0  p :   2.136   To begin, let p be a rational number, r=s , say. Consider the extended grouping axiom for Hs  f  s  = Hs   ; : : : ;    = H   ; : : : ;  1 s  1 s  1 s  ;  s  cid:0  r s    +  s  cid:0  r s  f  s  cid:0  r   1 s    r  } {z s  cid:0  r s  r s  ;  = H2     +  f  s  +  s r  s  cid:0  r s  f  s  cid:0  r   Substituting f  s  = log2 s , etc, we obtain  H2   r s  ;  s  cid:0  r s    =  cid:0   r s  log2  r  s  cid:0  cid:18 1  cid:0   s  cid:19  log2 cid:18 1  cid:0  s  cid:0  r  s  cid:19  : s  cid:0  r  Thus  2.136  is true for rational p . By the continuity assumption,  2.136  is also true at irrational p .  To complete the proof, we have to extend the de cid:12 nition from H2 to Hm , i.e., we have to show that  Hm p1; : : : ; pm  =  cid:0 X pi log pi   2.131    2.132    2.133    2.134    2.135    2.137    2.138    2.139    2.140    46  Entropy, Relative Entropy and Mutual Information  for all m . This is a straightforward induction. We have just shown that this is true for m = 2 . Now assume that it is true for m = n  cid:0  1 . By the grouping axiom,   2.141    2.142    2.143    2.144    2.145    2.146   Hn p1; : : : ; pn  = Hn cid:0 1 p1 + p2; p3; : : : ; pn  + p1 + p2 H2 cid:18  p1 =  cid:0  p1 + p2  log p1 + p2   cid:0   p1 + p2  ;  p2  p1 + p2 cid:19  Xi=3  pi log pi  n  p2  log  p1  p1  p1 + p2  log  p1 + p2  cid:0   p2  p1 + p2  p1 + p2  n   cid:0  Xi=1  =  cid:0   pi log pi:  Thus the statement is true for m = n , and by induction, it is true for all m . Thus we have  cid:12 nally proved that the only symmetric function that satis cid:12 es the axioms is  Hm p1; : : : ; pm  =  cid:0   pi log pi:  m  Xi=1  The proof above is due to R cid:19 enyi[10]  47. The entropy of a missorted  cid:12 le.  A deck of n cards in order 1; 2; : : : ; n is provided. One card is removed at random then replaced at random. What is the entropy of the resulting deck?  Solution: The entropy of a missorted  cid:12 le.  The heart of this problem is simply carefully counting the possible outcome states. There are n ways to choose which card gets mis-sorted, and, once the card is chosen, there are again n ways to choose where the card is replaced in the deck. Each of these shu cid:15 ing actions has probability 1=n2 . Unfortunately, not all of these n2 actions results in a unique mis-sorted  cid:12 le. So we need to carefully count the number of distinguishable outcome states. The resulting deck can only take on one of the following three cases.   cid:15  The selected card is at its original location after a replacement.  cid:15  The selected card is at most one location away from its original location after a   cid:15  The selected card is at least two locations away from its original location after a  replacement.  replacement.  To compute the entropy of the resulting deck, we need to know the probability of each case.  Case 1  resulting deck is the same as the original : There are n ways to achieve this outcome state, one for each of the n cards in the deck. Thus, the probability associated with case 1 is n=n2 = 1=n .   Entropy, Relative Entropy and Mutual Information  47  Case 2  adjacent pair swapping : There are n  cid:0  1 adjacent pairs, each of which will have a probability of 2=n2 , since for each pair, there are two ways to achieve the swap, either by selecting the left-hand card and moving it one to the right, or by selecting the right-hand card and moving it one to the left.  Case 3  typical situation : None of the remaining actions \collapses". They all result in unique outcome states, each with probability 1=n2 . Of the n2 possible shu cid:15 ing actions, n2  cid:0  n  cid:0  2 n  cid:0  1  of them result in this third case  we’ve simply subtracted the case 1 and case 2 situations above .  The entropy of the resulting deck can be computed as follows.  H X  =  1 log n  +  n  cid:0  1  n 2n  cid:0  1  2 n2 log  2 n  cid:0  1   log n   cid:0   n2  n  =  n2 2    +  n2  cid:0  3n + 2   1 n2 log n2   48. Sequence length.  How much information does the length of a sequence give about the content of a se- quence? Suppose we consider a Bernoulli  1=2  process fXig: Stop the process when the  cid:12 rst 1 appears. Let N designate this stopping time. Thus X N is an element of the set of all  cid:12 nite length binary sequences f0; 1g cid:3  = f0; 1; 00; 01; 10; 11; 000; : : :g:  a  Find I N ; X N  :  Let’s now consider a di cid:11 erent stopping time. For this part, again assume Xi  cid:24  Bernoulli  1=2  but stop at time N = 6 , with probability 1=3 and stop at time N = 12 with probability 2=3: Let this stopping time be independent of the sequence X1X2 : : : X12:   b  Find H X NjN  :  c  Find H X N  :   d  Find I N ; X N  :   e  Find H X NjN  :  f  Find H X N  :  Solution:   a   I X N ; N   = H N    cid:0  H NjX N    = H N    cid:0  0   48  Entropy, Relative Entropy and Mutual Information   a  = E N    I X N ; N   = 2  where  a  comes from the fact that the entropy of a geometric random variable is just the mean.   b  Since given N we know that Xi = 0 for all i < N and XN = 1;  H X NjN   = 0:   c    d    e    f   H X N   = I X N ; N   + H X NjN    = I X N ; N   + 0  H X N   = 2:  I X N ; N   = H N    cid:0  H NjX N    = H N    cid:0  0 I X N ; N   = HB 1=3   H X 12jN = 12   H X NjN   = =  1 3 1 3 1 3 H X NjN   = 10:  =  H X 6jN = 6  + H X 6  +  2 3 H X 12   2 3  6 +  12  2 3  H X N   = I X N ; N   + H X NjN    = I X N ; N   + 10  H X N   = H 1=3  + 10:   Chapter 3  The Asymptotic Equipartition Property  1. Markov’s inequality and Chebyshev’s inequality.   a   Markov’s inequality.  For any non-negative random variable X and any t > 0 ,  show that  PrfX  cid:21  tg  cid:20   EX  :  t  Exhibit a random variable that achieves this inequality with equality.   b   Chebyshev’s inequality.  Let Y be a random variable with mean  cid:22  and variance   cid:27 2 . By letting X =  Y  cid:0   cid:22  2 , show that for any  cid:15  > 0 ,  PrfjY  cid:0   cid:22 j >  cid:15 g  cid:20    cid:27 2  cid:15 2 :  nPn  cid:27 2 n cid:15 2 :  Prn cid:12  cid:12  cid:12 Zn  cid:0   cid:22  cid:12  cid:12  cid:12  >  cid:15 o  cid:20    c   The weak law of large numbers.  Let Z1; Z2; : : : ; Zn be a sequence of i.i.d. random i=1 Zi be the sample mean.  variables with mean  cid:22  and variance  cid:27 2 . Let Zn = 1 Show that  Thus Prn cid:12  cid:12  cid:12 Z n  cid:0   cid:22  cid:12  cid:12  cid:12  >  cid:15 o ! 0 as n ! 1 . This is known as the weak law of large  numbers.  Solution: Markov’s inequality and Chebyshev’s inequality.   a  If X has distribution F  x  ,   3.1    3.2    3.3   xdF  EX = Z 1 = Z  cid:14   0  0  49  xdF +Z 1   cid:14   xdF   50  The Asymptotic Equipartition Property   cid:14   xdF   cid:21  Z 1  cid:21  Z 1 =  cid:14  PrfX  cid:21   cid:14 g:   cid:14 dF   cid:14   PrfX  cid:21   cid:14 g  cid:20   EX  :   cid:14   Rearranging sides and dividing by  cid:14  we get,   3.4   One student gave a proof based on conditional expectations. It goes like  EX = E XjX  cid:20   cid:14   PrfX  cid:21   cid:14 g + E XjX <  cid:14   PrfX <  cid:14 g   cid:21  E XjX  cid:20   cid:14   PrfX  cid:21   cid:14 g  cid:21   cid:14  PrfX  cid:21   cid:14 g;  which leads to  3.4  as well. Given  cid:14  , the distribution achieving  is  PrfX  cid:21   cid:14 g =  EX  ;   cid:14   X =   cid:14  with probability  cid:22   0 with probability 1  cid:0   cid:22   cid:14  ;   cid:14   where  cid:22   cid:20   cid:14  .   b  Letting X =  Y  cid:0   cid:22  2 in Markov’s inequality,  Prf Y  cid:0   cid:22  2 >  cid:15 2g  cid:20  Prf Y  cid:0   cid:22  2  cid:21   cid:15 2g  and noticing that Prf Y  cid:0   cid:22  2 >  cid:15 2g = PrfjY  cid:0   cid:22 j >  cid:15 g , we get,   c  Letting Y in Chebyshev’s inequality from part  b  equal  cid:22 Zn , and noticing that n , each with   cid:22 Zn is the sum of n iid r.v.’s, Zi  n  ie.  E  cid:22 Zn =  cid:22  and Var   cid:22 Zn  =  cid:27 2 variance  cid:27 2  n2  , we have,   cid:20  =   cid:15 2  E Y  cid:0   cid:22  2  cid:27 2  cid:15 2 ;  PrfjY  cid:0   cid:22 j >  cid:15 g  cid:20    cid:27 2  cid:15 2 :  Prfj  cid:22 Zn  cid:0   cid:22 j >  cid:15 g  cid:20    cid:27 2 n cid:15 2 :   The Asymptotic Equipartition Property  51  2. AEP and mutual information. Let  Xi; Yi  be i.i.d.  cid:24  p x; y  . We form the log likelihood ratio of the hypothesis that X and Y are independent vs. the hypothesis that X and Y are dependent. What is the limit of p X n p Y n  p X n; Y n   1 n  log  ?  Solution:  1 n  log  p X n p Y n  p X n; Y n   =  log  1 n  n  Yi=1  p Xi p Yi  p Xi; Yi   p Xi p Yi  p Xi; Yi   p Xi p Yi  p Xi; Yi      n  1 n  =  log  Xi=i ! E log =  cid:0 I X; Y    p X n;Y n  ! 2 cid:0 nI X;Y   , which will converge to 1 if X and Y are indeed  Thus, p X n p Y n  independent.  3. Piece of cake  A cake is sliced roughly in half, the largest piece being chosen each time, the other pieces discarded. We will assume that a random cut creates pieces of proportions:  P =    2  3 ; 1 5 ; 3   2  3   w.p. 3 5   w.p. 1  4  4  Thus, for example, the  cid:12 rst cut  and choice of largest piece  may result in a piece of size 3 3   at time 2, and so on.  5 . Cutting and choosing from this piece might reduce it to size   3  5    2  How large, to  cid:12 rst order in the exponent, is the piece of cake after n cuts?  Solution: Let Ci be the fraction of the piece of cake that is cut at the i th cut, and let i=1 Ci .  Tn be the fraction of cake left after n cuts. Then we have Tn = C1C2 : : : Cn =Qn  Hence, as in Question 2 of Homework Set 3,  lim  log Tn = lim  log Ci  1 n  1 n  n  Xi=1  = E[log C1]  =  log  +  log  2 3  1 4  3 5  :  3 4   52  4. AEP  The Asymptotic Equipartition Property  n log p xn  cid:0  Hj  cid:20   cid:15 g . Let Bn = fxn 2 X n : j 1  Let Xi be iid  cid:24  p x ; x 2 f1; 2; : : : ; mg . Let  cid:22  = EX; and H =  cid:0 P p x  log p x : Let An = fxn 2 X n : j cid:0  1 i=1 Xi  cid:0   cid:22 j  cid:20   cid:15 g .  a  Does PrfX n 2 Ang  cid:0 ! 1 ?  b  Does PrfX n 2 An \ Bng  cid:0 ! 1 ?  c  Show jAn \ Bnj  cid:20  2n H+ cid:15   , for all n .  d  Show jAn \ Bnj  cid:21    1 Solution:  2  2n H cid:0  cid:15   , for n su cid:14 ciently large.  nPn   a  Yes, by the AEP for discrete random variables the probability X n is typical goes  to 1.   b  Yes, by the Strong Law of Large Numbers P r X n 2 Bn  ! 1 . So there exists  cid:15  > 0 and N1 such that P r X n 2 An  > 1  cid:0   cid:15  2 for all n > N1 , and there exists N2 such that P r X n 2 Bn  > 1  cid:0   cid:15  2 for all n > N2 . So for all n > max N1; N2  : P r X n 2 An \ Bn  = P r X n 2 An  + P r X n 2 Bn   cid:0  P r X n 2 An [ Bn   + 1  cid:0    cid:15  2  cid:0  1   cid:15  > 1  cid:0  2 = 1  cid:0   cid:15   So for any  cid:15  > 0 there exists N = max N1; N2  such that P r X n 2 An \ Bn  > 1  cid:0   cid:15  for all n > N , therefore P r X n 2 An \ Bn  ! 1 .  c  By the law of total probability Pxn2An\Bn p xn   cid:20  1 . Also, for xn 2 An , from Theorem 3.1.2 in the text, p xn   cid:21  2 cid:0 n H+ cid:15   . Combining these two equations gives 1  cid:21  Pxn2An\Bn p xn   cid:21  Pxn2An\Bn 2 cid:0 n H+ cid:15   = jAn \ Bnj2 cid:0 n H+ cid:15   . Multiplying through by 2n H+ cid:15   gives the result jAn \ Bnj  cid:20  2n H+ cid:15   .  d  Since from  b  P rfX n 2 An \ Bng ! 1 , there exists N such that P rfX n 2 An \ Bng  cid:21  1 2 for all n > N . From Theorem 3.1.2 in the text, for xn 2 An , 2  cid:20  Pxn2An\Bn p xn   cid:20  p xn   cid:20  2 cid:0 n H cid:0  cid:15   . So combining these two gives Pxn2An\Bn 2 cid:0 n H cid:0  cid:15   = jAn \ Bnj2 cid:0 n H cid:0  cid:15   . Multiplying through by 2n H cid:0  cid:15   gives the result jAn \ Bnj  cid:21    1  2  2n H cid:0  cid:15   for n su cid:14 ciently large.  1  5. Sets de cid:12 ned by probabilities.  Let X1; X2; : : : be an i.i.d. sequence of discrete random variables with entropy H X : Let  Cn t  = fxn 2 X n : p xn   cid:21  2 cid:0 ntg denote the subset of n -sequences with probabilities  cid:21  2 cid:0 nt:  a  Show jCn t j  cid:20  2nt:  b  For what values of t does P  fX n 2 Cn t g  ! 1? Solution:   The Asymptotic Equipartition Property  53   a  Since the total probability of all sequences is less than 1, jCn t j minxn2Cn t  p xn   cid:20   b  Since  cid:0  1 n log p xn  ! H , if t   2 cid:0 nt goes to 0,  1 , and hence jCn t j  cid:20  2nt .  and if t > H , the probability goes to 1.  6. An AEP-like limit. Let X1; X2; : : : be i.i.d. drawn according to probability mass  function p x : Find  [p X1; X2; : : : ; Xn ]  1 n :  lim n!1  Solution: An AEP-like limit. X1; X2; : : : ; i.i.d.  cid:24  p x  . Hence log Xi  are also i.i.d. and  lim p X1; X2; : : : ; Xn    1  n = lim 2log p X1;X2;:::;Xn    1 n  nP log p Xi  a.e.  = 2lim 1 = 2E log p X    a.e. = 2 cid:0 H X  a.e.  by the strong law of large numbers  assuming of course that H X  exists .  7. The AEP and source coding. A discrete memoryless source emits a sequence of statistically independent binary digits with probabilities p 1  = 0:005 and p 0  = 0:995 . The digits are taken 100 at a time and a binary codeword is provided for every sequence of 100 digits containing three or fewer ones.   a  Assuming that all codewords are the same length,  cid:12 nd the minimum length re-  quired to provide codewords for all sequences with three or fewer ones.   b  Calculate the probability of observing a source sequence for which no codeword  has been assigned.   c  Use Chebyshev’s inequality to bound the probability of observing a source sequence for which no codeword has been assigned. Compare this bound with the actual probability computed in part  b .  Solution: The AEP and source coding.   a  The number of 100-bit binary sequences with three or fewer ones is  0 ! + 100  100  1 ! + 100  2 ! + 100  3 ! = 1 + 100 + 4950 + 161700 = 166751 :  The required codeword length is dlog2 166751e = 18 . 0:0454 , so 18 is quite a bit larger than the 4.5 bits of entropy.    Note that H 0:005  =   b  The probability that a 100-bit sequence has three or fewer ones is  3  Xi=0 100  i ! 0:005 i 0:995 100 cid:0 i = 0:60577 + 0:30441 + 0:7572 + 0:01243 = 0:99833   54  The Asymptotic Equipartition Property  Thus the probability that the sequence that is generated cannot be encoded is 1  cid:0  0:99833 = 0:00167 .   c  In the case of a random variable Sn that is the sum of n i.i.d. random variables  X1; X2; : : : ; Xn , Chebyshev’s inequality states that  Pr jSn  cid:0  n cid:22 j  cid:21   cid:15    cid:20   n cid:27 2  cid:15 2 ;  Therefore n cid:22  and n cid:27 2 where  cid:22  and  cid:27 2 are the mean and variance of Xi . are the mean and variance of Sn .  In this problem, n = 100 ,  cid:22  = 0:005 , and  cid:27 2 =  0:005  0:995  . Note that S100  cid:21  4 if and only if jS100  cid:0  100 0:005 j  cid:21  3:5 , so we should choose  cid:15  = 3:5 . Then  Pr S100  cid:21  4   cid:20    3:5 2   cid:25  0:04061 :  100 0:005  0:995   This bound is much larger than the actual probability 0.00167.  8. Products. Let  Let X1; X2; : : : be drawn i.i.d. according to this distribution. Find the limiting behavior of the product  1; 2; 3;  1 2 1 4 1 4  X =8>< >:   X1X2  cid:1   cid:1   cid:1  Xn   1 n :  Pn =  X1X2 : : : Xn   1 n :  log Pn =  log Xi ! E log X;  1 n  n  Xi=1  Solution: Products. Let  Then   3.5    3.6   with probability 1, by the strong law of large numbers. Thus Pn ! 2E log X with prob. 1. We can easily calculate E log X = 1 4 log 6 , and therefore Pn ! 2  4 log 6 = 1:565 .  4 log 3 = 1  4 log 2 + 1  2 log 1 + 1  1  9. AEP. Let X1; X2; : : : be independent identically distributed random variables drawn  according to the probability mass function p x ; x 2 f1; 2; : : : ; mg . Thus p x1; x2; : : : ; xn  = Qn i=1 p xi  . We know that  cid:0  1 q x1; x2; : : : ; xn  =Qn i=1 q xi ; where q is another probability mass function on f1; 2; : : : ; mg .  a  Evaluate lim  cid:0  1 n log q X1; X2; : : : ; Xn  , where X1; X2; : : : are i.i.d.  cid:24  p x  .  b  Now evaluate the limit of the log likelihood ratio 1  n log p X1; X2; : : : ; Xn  ! H X  in probability. Let  p X1;:::;Xn  when X1; X2; : : : are i.i.d.  cid:24  p x  . Thus the odds favoring q are exponentially small when p is true.  n log q X1;:::;Xn    The Asymptotic Equipartition Property  Solution:  AEP .   a  Since the X1; X2; : : : ; Xn are i.i.d., so are q X1 ; q X2 ; : : : ; q Xn  , and hence we  can apply the strong law of large numbers to obtain  lim cid:0   log q X1; X2; : : : ; Xn  = lim  cid:0   1 n  1  nX log q Xi  =  cid:0 E log q X   w.p. 1 =  cid:0 X p x  log q x  = X p x  log = D pjjq  + H p :  p x   q x   cid:0 X p x  log p x   3.10    3.11    b  Again, by the strong law of large numbers,  lim cid:0   1 n  log  q X1; X2; : : : ; Xn  p X1; X2; : : : ; Xn   = lim  cid:0   1  nX log  q Xi  p Xi     w.p. 1   3.13   55   3.7    3.8    3.9    3.12    3.14    3.15    3.16   q X  p X   =  cid:0 E log =  cid:0 X p x  log = X p x  log = D pjjq :  q x  p x   p x  q x   is to be constructed. The volume is Vn = Qn  10. Random box size. An n -dimensional rectangular box with sides X1; X2; X3; : : : ; Xn i=1 Xi : The edge length l of a n -cube with the same volume as the random box is l = V 1=n : Let X1; X2; : : : be i.i.d. uniform ; and compare to random variables over the unit interval 1 n . Clearly the expected edge length does not capture the idea of the volume  EVn  of the box. The geometric mean, rather than the arithmetic mean, characterizes the behavior of products.  [0; 1]: Find limn!1 V 1=n  n  n  Solution: Random box size. The volume Vn = Qn is a random variable, since the Xi are random variables uniformly distributed on [0; 1] . Vn tends to 0 as n ! 1 . However  i=1 Xi  loge V  1 n = n  1 n  loge Vn =  1  nX loge Xi ! E loge X   a.e.  by the Strong Law of Large Numbers, since Xi and loge Xi  are i.i.d. and E loge X   < 1 . Now  Hence, since ex is a continuous function,  E loge Xi   =Z 1  0  loge x  dx =  cid:0 1  1 n  V  n = elimn!1  1  n loge Vn =  1 e  <  1 2  :  lim n!1   56  The Asymptotic Equipartition Property  variable, and 1  Thus the \e cid:11 ective" edge length of this solid is e cid:0 1 . Note that since the Xi ’s are 2 is the arithmetic mean of the random  independent, E Vn  =Q E Xi  =   1 set is about 2nH . Let X1; X2; : : : ; Xn be i.i.d.  cid:24  p x  . Let B n  Pr B n   11. Proof of Theorem 3.3.1. This problem shows that the size of the smallest \probable"  cid:14   cid:26  X n such that  e is the geometric mean.  2  n . Also 1   cid:14     > 1  cid:0   cid:14  . Fix  cid:15  < 1 2 .   a  Given any two sets A , B such that Pr A  > 1  cid:0   cid:15 1 and Pr B  > 1  cid:0   cid:15 2 , show  that Pr A \ B  > 1  cid:0   cid:15 1  cid:0   cid:15 2 . Hence Pr A n    cid:15  \ B n    cid:14      cid:21  1  cid:0   cid:15   cid:0   cid:14 :   cid:15  \ B n  p xn    cid:14       b  Justify the steps in the chain of inequalities 1  cid:0   cid:15   cid:0   cid:14   cid:20  Pr A n  = XA n   cid:15  \B n   cid:20  XA n  = jA n   cid:20  jB n    cid:14    cid:14    cid:14    cid:15  \B n   cid:15  \ B n    cid:14   j2 cid:0 n H cid:0  cid:15  :  2 cid:0 n H cid:0  cid:15     c  Complete the proof of the theorem.  j2 cid:0 n H cid:0  cid:15    Solution: Proof of Theorem 3.3.1.   a  Let Ac denote the complement of A . Then  Since P  A   cid:21  1  cid:0   cid:15 1 , P  Ac   cid:20   cid:15 1 . Similarly, P  Bc   cid:20   cid:15 2 . Hence  P  Ac [ Bc   cid:20  P  Ac  + P  Bc :   b  To complete the proof, we have the following chain of inequalities  P  A \ B  = 1  cid:0  P  Ac [ Bc    cid:21  1  cid:0  P  Ac   cid:0  P  Bc   cid:21  1  cid:0   cid:15 1  cid:0   cid:15 2:  1  cid:0   cid:15   cid:0   cid:14       cid:15  \ B n  p xn    cid:14    a    c    b    cid:14    cid:15  \B n    cid:20  Pr A n  = XA n   cid:20  XA n   cid:15  \B n   cid:15  \ B n  = jA n   cid:20  jB n    cid:14    cid:14    cid:14    e    d   j2 cid:0 n H cid:0  cid:15  :  2 cid:0 n H cid:0  cid:15    j2 cid:0 n H cid:0  cid:15     3.17    3.18    3.19    3.20    3.21    3.22    3.23    3.24    3.25    3.26    3.27    3.28    3.29    3.30    The Asymptotic Equipartition Property  57  where  a  follows from the previous part,  b  follows by de cid:12 nition of probability of a set,  c  follows from the fact that the probability of elements of the typical set are j as the cardinality  bounded by 2 cid:0 n H cid:0  cid:15   ,  d  from the de cid:12 nition of jA n  , and  e  from the fact that A n  of the set A n   .   cid:14    cid:15  \ B n   cid:15  \ B n    cid:14   cid:18  B n    cid:14    cid:15  \ B n    cid:14   12. Monotonic convergence of the empirical distribution. Let ^pn denote the empir- ical probability mass function corresponding to X1; X2; : : : ; Xn i.i.d.  cid:24  p x ; x 2 X . Speci cid:12 cally,  ^pn x  =  I Xi = x   1 n  n  Xi=1  is the proportion of times that Xi = x in the  cid:12 rst n samples, where I is the indicator function.   a  Show for X binary that  ED ^p2n k p   cid:20  ED ^pn k p :  Thus the expected relative entropy \distance" from the empirical distribution to the true distribution decreases with sample size. Hint: Write ^p2n = 1  2 ^p0n and use the convexity of D .  2 ^pn + 1   b  Show for an arbitrary discrete X that  ED ^pn k p   cid:20  ED ^pn cid:0 1 k p :  Hint: Write ^pn as the average of n empirical mass functions with each of the n samples deleted in turn.  Solution: Monotonic convergence of the empirical distribution.   a  Note that,  ^p2n x  =  I Xi = x   1 2n  2n  Xi=1 Xi=1  n  1 n  1 2  1 2  =  =  ^pn x  +  ^p0n x :  1 2  I Xi = x  +  I Xi = x   1 2  1 n  2n  Xi=n+1  Using convexity of D pjjq  we have that, 1 1 2 2 D ^pnjjp  +  D ^p2njjp  = D  1 2  ^pn +   cid:20   ^p0njj 1 2  p   p +  1 1 2 2 D ^p0njjp :  Taking expectations and using the fact the Xi ’s are identically distributed we get,  ED ^p2njjp   cid:20  ED ^pnjjp :   58  The Asymptotic Equipartition Property   b  The trick to this part is similar to part a  and involves rewriting ^pn in terms of  ^pn cid:0 1 . We see that,  or in general,  ^pn =  I Xi = x  +  I Xn = x   n  ^pn =  I Xi = x  +  I Xj = x   ;  n  1 n  n cid:0 1  Xi=0  1  nXi6=j  where j ranges from 1 to n .  Summing over j we get,  or,  where,  n^pn =  n  cid:0  1 n  n  Xj=1  ^pj n cid:0 1 + ^pn;  ^pn =  1 n  n  Xj=1  ^pj n cid:0 1  n  Xj=1  ^pj n cid:0 1 =  1  n  cid:0  1Xi6=j  I Xi = x :  Again using the convexity of D pjjq  and the fact that the D ^pj n cid:0 1jjp  are identi- cally distributed for all j and hence have the same expected value, we obtain the  cid:12 nal result.  13. Calculation of typical set To clarify the notion of a typical set A n   and the smallest set of high probability B  n  , we will calculate the set for a simple example. Consider a sequence of i.i.d. binary random variables, X1; X2; : : : ; Xn , where the probability that Xi = 1 is 0.6  and therefore the probability that Xi = 0 is 0.4 .   cid:14    cid:15    a  Calculate H X  .   b  With n = 25 and  cid:15  = 0:1 , which sequences fall in the typical set A n   ? What is the probability of the typical set? How many elements are there in the typical set?  This involves computation of a table of probabilities for sequences with k 1’s, 0  cid:20  k  cid:20  25 , and  cid:12 nding those sequences that are in the typical set.    cid:15    The Asymptotic Equipartition Property  59   cid:0 n k cid:1 pk 1  cid:0  p n cid:0 k  cid:0  1  k 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25   cid:0 n k cid:1   1 25 300 2300 12650 53130 177100 480700 1081575 2042975 3268760 4457400 5200300 5200300 4457400 3268760 2042975 1081575 480700 177100 53130 12650 2300 300 25 1  0.000000 0.000000 0.000000 0.000001 0.000007 0.000054 0.000227 0.001205 0.003121 0.013169 0.021222 0.077801 0.075967 0.267718 0.146507 0.575383 0.151086 0.846448 0.079986 0.970638 0.019891 0.997633 0.001937 0.999950 0.000047 0.000003  n log p xn  1.321928 1.298530 1.275131 1.251733 1.228334 1.204936 1.181537 1.158139 1.134740 1.111342 1.087943 1.064545 1.041146 1.017748 0.994349 0.970951 0.947552 0.924154 0.900755 0.877357 0.853958 0.830560 0.807161 0.783763 0.760364 0.736966   c  How many elements are there in the smallest set that has probability 0.9?   d  How many elements are there in the intersection of the sets in part  b  and  c ?  What is the probability of this intersection?  Solution:   a  H X  =  cid:0 0:6 log 0:6  cid:0  0:4 log 0:4 = 0:97095 bits.  b  By de cid:12 nition, A n    cid:15   for  cid:15  = 0:1 is the set of sequences such that  cid:0  1  n log p xn  lies in the range  H X  cid:0  cid:15 ; H X + cid:15   , i.e., in the range  0.87095, 1.07095 . Examining the last column of the table, it is easy to see that the typical set is the set of all sequences with the number of ones lying between 11 and 19. The probability of the typical set can be calculated from cumulative probability column. The probability that the number of 1’s lies between 11 and 19 is equal to F  19   cid:0  F  10  = 0:970638  cid:0  0:034392 = 0:936246 . Note that this is greater than 1 cid:0   cid:15  , i.e., the n is large enough for the probability of the typical set to be greater than 1  cid:0   cid:15  .   60  The Asymptotic Equipartition Property   c  To  cid:12 nd the smallest set B  n   The number of elements in the typical set can be found using the third column.  jA n    cid:15   j =  19  Xk=11 n k! =  19  Xk=0 n k!  cid:0   10  Xk=0 n  k! = 33486026  cid:0  7119516 = 26366510:   3.31  Note that the upper and lower bounds for the size of the A n  can be calculated as 2n H+ cid:15   = 225 0:97095+0:1  = 226:77 = 1:147365  cid:2  108 , and  1  cid:0   cid:15  2n H cid:0  cid:15   = 0:9  cid:2  225 0:97095 cid:0 0:1  = 0:9  cid:2  221:9875 = 3742308 . Both bounds are very loose!   cid:15    cid:14   of probability 0.9, we can imagine that we are  cid:12 lling a bag with pieces such that we want to reach a certain weight with the minimum number of pieces. To minimize the number of pieces that we use, we should use the largest possible pieces. In this case, it corresponds to using the sequences with the highest probability. Thus we keep putting the high probability sequences into this set until we reach a total probability of 0.9. Looking at the fourth column of the table, it is clear that the probability of a sequence increases monotonically with k . Thus the set consists of sequences of k = 25; 24; : : : , until we have a total probability 0.9. Using the cumulative probability column, it follows that the set B  n  consist of sequences with k  cid:21  13 and some sequences with k = 12 . The sequences with k  cid:21  13 provide a total probability of 1 cid:0 0:153768 = 0:846232 to the set B  n  . The remaining probability of 0:9  cid:0  0:846232 = 0:053768 should come from sequences with k = 12 . The number of such sequences needed to  cid:12 ll this probability is at least 0:053768=p xn  = 0:053768=1:460813 cid:2 10 cid:0 8 = 3680690:1 , which we round up to 3680691. Thus the smallest set with probability 0.9 has 33554432 cid:0  16777216 + 3680691 = 20457907 sequences. Note that the set B  n  is not uniquely de cid:12 ned - it could include any 3680691 sequences with k = 12 . However, the size of the smallest set is well de cid:12 ned.   cid:14    cid:14    cid:14    d  The intersection of the sets A n   in parts  b  and  c  consists of all sequences with k between 13 and 19, and 3680691 sequences with k = 12 . The probability of this intersection = 0:970638 cid:0  0:153768 + 0:053768 = 0:870638 , and the size of this intersection = 33486026  cid:0  16777216 + 3680691 = 20389501 .   cid:14    cid:15   and B n    Chapter 4  Entropy Rates of a Stochastic Process  1. Doubly stochastic matrices. An n  cid:2  n matrix P = [Pij] stochastic if Pij  cid:21  0 and Pj Pij = 1 for all i and Pi Pij = 1 for all j . An n  cid:2  n  matrix P is said to be a permutation matrix if it is doubly stochastic and there is precisely one Pij = 1 in each row and each column.  is said to be doubly  It can be shown that every doubly stochastic matrix can be written as the convex combination of permutation matrices.   a  Let at =  a1; a2; : : : ; an  , ai  cid:21  0 , P ai = 1 , be a probability vector. Let b = aP , where P is doubly stochastic. Show that b is a probability vector and that H b1; b2; : : : ; bn   cid:21  H a1; a2; : : : ; an  . Thus stochastic mixing increases entropy.  b  Show that a stationary distribution  cid:22  for a doubly stochastic matrix P is the  uniform distribution.   c  Conversely, prove that if the uniform distribution is a stationary distribution for  a Markov transition matrix P , then P is doubly stochastic.  Solution: Doubly Stochastic Matrices.   a   H b   cid:0  H a  =  cid:0 Xj = Xj Xi = Xi Xj  cid:21  0 @Xi;j  61  bj log bj +Xi aiPij log Xk  aiPij log  ai log ai  akPkj  +Xi  ai  aiPij1  Pk akPkj A log Pi;j ai Pi;j bj  ai log ai   4.2    4.1    4.3    4.4    62  Entropy Rates of a Stochastic Process  m m  = 1 log  = 0;  where the inequality follows from the log sum inequality.   b  If the matrix is doubly stochastic, the substituting  cid:22 i = 1  m , we can easily check  that it satis cid:12 es  cid:22  =  cid:22 P .   c  If the uniform is a stationary distribution, then  1 m  mXj or Pj Pji = 1 or that the matrix is doubly stochastic.  =  cid:22 i =Xj   cid:22 jPji =  1  Pji;  2. Time’s arrow. Let fXig1i= cid:0 1 be a stationary stochastic process. Prove that  H X0jX cid:0 1; X cid:0 2; : : : ; X cid:0 n  = H X0jX1; X2; : : : ; Xn :  In other words, the present has a conditional entropy given the past equal to the conditional entropy given the future.  This is true even though it is quite easy to concoct stationary random processes for which the  cid:13 ow into the future looks quite di cid:11 erent from the  cid:13 ow into the past. That is to say, one can determine the direction of time by looking at a sample function of the process. Nonetheless, given the present state, the conditional uncertainty of the next symbol in the future is equal to the conditional uncertainty of the previous symbol in the past.  Solution: Time’s arrow. By the chain rule for entropy,  H X0jX cid:0 1; : : : ; X cid:0 n  = H X0; X cid:0 1; : : : ; X cid:0 n   cid:0  H X cid:0 1; : : : ; X cid:0 n   = H X0; X1; X2; : : : ; Xn   cid:0  H X1; X2; : : : ; Xn  = H X0jX1; X2; : : : ; Xn ;  where  4.9  follows from stationarity.  3. Shu cid:15 es increase entropy. Argue that for any distribution on shu cid:15 es T and any  distribution on card positions X that   4.5    4.6    4.7    4.8    4.9    4.10    4.11    4.12    4.13    4.14   H T X   cid:21  H T XjT    = H T  cid:0 1T XjT   = H XjT   = H X ;  if X and T are independent.   63   4.15    4.16    4.17    4.18    4.19    4.20    4.21    4.22    4.23    4.24   Entropy Rates of a Stochastic Process  Solution: Shu cid:15 es increase entropy.  H T X   cid:21  H T XjT    = H T  cid:0 1T XjT   = H XjT   = H X :  The inequality follows from the fact that conditioning reduces entropy and the  cid:12 rst equality follows from the fact that given T , we can reverse the shu cid:15 e.  4. Second law of thermodynamics. Let X1; X2; X3 : : : be a stationary  cid:12 rst-order Markov chain. In Section 4.4, it was shown that H Xn j X1   cid:21  H Xn cid:0 1 j X1  for n = 2; 3 : : : . Thus conditional uncertainty about the future grows with time. This is true although the unconditional uncertainty H Xn  remains constant. However, show by example that H XnjX1 = x1  does not necessarily grow with n for every x1 . Solution: Second law of thermodynamics.  H XnjX1   cid:20  H XnjX1; X2    Conditioning reduces entropy   = H XnjX2  = H Xn cid:0 1jX1    by Markovity    by stationarity   Alternatively, by an application of the data processing inequality to the Markov chain X1 ! Xn cid:0 1 ! Xn , we have  Expanding the mutual informations in terms of entropies, we have  I X1; Xn cid:0 1   cid:21  I X1; Xn :  H Xn cid:0 1   cid:0  H Xn cid:0 1jX1   cid:21  H Xn   cid:0  H XnjX1 :  By stationarity, H Xn cid:0 1  = H Xn  and hence we have H Xn cid:0 1jX1   cid:20  H XnjX1 :  5. Entropy of a random tree. Consider the following method of generating a random  tree with n nodes. First expand the root node:   cid:0   @   cid:0  cid:0   @@  Then expand one of the two terminal nodes at random:   cid:0   @   cid:0  cid:0   @@  cid:0   @   cid:0   @  @@   cid:0  cid:0  @   cid:0    cid:0  cid:0   @@   cid:0  cid:0   @@   64  Entropy Rates of a Stochastic Process  At time k , choose one of the k  cid:0  1 terminal nodes according to a uniform distribution and expand it. Continue until n terminal nodes have been generated. Thus a sequence leading to a  cid:12 ve node tree might look like this:   cid:0   @   cid:0   @   cid:0  cid:0   @@   cid:0  cid:0   @@  cid:0   @   cid:0  cid:0   @@   cid:0   @   cid:0  cid:0   @@  cid:0   @   cid:0  cid:0    cid:0   @   cid:0  cid:0   @@  cid:0   @   cid:0  cid:0   @@  cid:0   @   cid:0  cid:0   @@  @@  cid:0   @   cid:0  cid:0   @@  cid:0   @   cid:0  cid:0   @@  Surprisingly, the following method of generating random trees yields the same probabil- ity distribution on trees with n terminal nodes. First choose an integer N1 uniformly distributed on f1; 2; : : : ; n  cid:0  1g . We then have the picture.  Then choose an integer N2 uniformly distributed over f1; 2; : : : ; N1  cid:0  1g , and indepen- dently choose another integer N3 uniformly over f1; 2; : : : ;  n  cid:0  N1   cid:0  1g . The picture is now:   cid:0   @   cid:0  cid:0  N1  @@ n  cid:0  N1  @   cid:8  cid:8  cid:8  cid:8  cid:8  @@ N1  cid:0  N2  HHHHH  cid:0  cid:0  N3   cid:0    cid:0    cid:0  cid:0  N2  @  @@  n  cid:0  N1  cid:0  N3  Continue the process until no further subdivision can be made.  The equivalence of these two tree generation schemes follows, for example, from Polya’s urn model.   Now let Tn denote a random n -node tree generated as described. The probability distribution on such trees seems di cid:14 cult to describe, but we can  cid:12 nd the entropy of this distribution in recursive form.  First some examples. For n = 2 , we have only one tree. Thus H T2  = 0 . For n = 3 , we have two equally probable trees:   cid:0   @   cid:0  cid:0   @@  cid:0   @   cid:0   @  @@   cid:0  cid:0  @   cid:0    cid:0  cid:0   @@   cid:0  cid:0   @@   Entropy Rates of a Stochastic Process  Thus H T3  = log 2 . For n = 4 , we have  cid:12 ve possible trees, with probabilities 1 3, 1 6, 1 6, 1 6, 1 6.  Now for the recurrence relation. Let N1 Tn  denote the number of terminal nodes of Tn in the right half of the tree. Justify each of the steps in the following:  H Tn   1   c    d    a  = H N1; Tn   b  = H N1  + H TnjN1  = log n  cid:0  1  + H TnjN1  n cid:0 1 Xk=1 = log n  cid:0  1  + Xk=1 Xk=1  = log n  cid:0  1  +  = log n  cid:0  1  +  n  cid:0  1  n  cid:0  1  n  cid:0  1  n cid:0 1  n cid:0 1   e   2  2  H Tk :  Hk:  [H Tk  + H Tn cid:0 k ]   f  Use this to show that  or   n  cid:0  1 Hn = nHn cid:0 1 +  n  cid:0  1  log n  cid:0  1   cid:0   n  cid:0  2  log n  cid:0  2 ;  Hn n  =  Hn cid:0 1 n  cid:0  1  + cn;  for appropriately de cid:12 ned cn . Since P cn = c < 1 , you have proved that 1  n H Tn  converges to a constant. Thus the expected number of bits necessary to describe the random tree Tn grows linearly with n .  Solution: Entropy of a random tree.   a  H Tn; N1  = H Tn  + H N1jTn  = H Tn  + 0 by the chain rule for entropies and  since N1 is a function of Tn .   b  H Tn; N1  = H N1  + H TnjN1  by the chain rule for entropies.  c  H N1  = log n  cid:0  1  since N1 is uniform on f1; 2; : : : ; n  cid:0  1g .  d   H TnjN1  =  n cid:0 1  Xk=1 n  cid:0  1  1  P  N1 = k H TnjN1 = k  Xk=1  H TnjN1 = k   n cid:0 1  =  by the de cid:12 nition of conditional entropy. Since conditional on N1 , the left subtree and the right subtree are chosen independently, H TnjN1 = k  = H Tk; Tn cid:0 kjN1 =  65   4.25    4.26    4.27    4.28    4.29    4.30    4.31    4.32    4.33    4.34    66  Entropy Rates of a Stochastic Process  k  = H Tk  + H Tn cid:0 k  , so  H TnjN1  =   H Tk  + H Tn cid:0 k   :   4.35   1  n  cid:0  1  n cid:0 1  Xk=1   e  By a simple change of variables,  n cid:0 1  Xk=1  H Tn cid:0 k  =  H Tk :  n cid:0 1  Xk=1   f  Hence if we let Hn = H Tn  ,   n  cid:0  1 Hn =  n  cid:0  1  log n  cid:0  1  + 2   n  cid:0  2 Hn cid:0 1 =  n  cid:0  2  log n  cid:0  2  + 2  n cid:0 1  Xk=1 Xk=1  n cid:0 2  Hk  Hk  Subtracting the second equation from the  cid:12 rst, we get   n cid:0  1 Hn  cid:0   n cid:0  2 Hn cid:0 1 =  n cid:0  1  log n cid:0  1  cid:0   n cid:0  2  log n cid:0  2  + 2Hn cid:0 1  4.40  or  where  log n  cid:0  1   +   n  cid:0  2  log n  cid:0  2   n   cid:0   n n  cid:0  1   Hn n  =  =  Hn cid:0 1 n  cid:0  1 Hn cid:0 1 n  cid:0  1  + Cn  Cn =  log n  cid:0  1    n  cid:0  2  log n  cid:0  2   log n  cid:0  1   =  n  n   cid:0    cid:0   n n  cid:0  1  +  log n  cid:0  2   n  cid:0  1   2 log n  cid:0  2  n n  cid:0  1   Substituting the equation for Hn cid:0 1 in the equation for Hn and proceeding recursively, we obtain a telescoping sum  Hn n  Cj +  H2 2  =  =  n  Xj=3 Xj=3  n  2 log j  cid:0  2  j j  cid:0  1   +  1 n  log n  cid:0  1 :   4.36    4.37    4.38    4.39    4.41    4.42    4.43    4.44    4.45    4.46    Entropy Rates of a Stochastic Process  Since limn!1  1  n log n  cid:0  1  = 0  Hn n  lim n!1  =   cid:20   =  1Xj=3 1Xj=3 1Xj=2  2  2  log j  cid:0  2   j j  cid:0  1   j  cid:0  1 2 log j  cid:0  1  2 j2 log j  67   4.47    4.48    4.49   For su cid:14 ciently large j , log j  cid:20  pj and hence the sum in  4.49  is dominated by the sum Pj j cid:0  3  2 which converges. Hence the above sum converges.  evaluation shows that  In fact, computer  lim  =  Hn n  1Xj=3  2  j j  cid:0  1   log j  cid:0  2  = 1:736 bits.   4.50   Thus the number of bits required to describe a random n -node tree grows linearly with n .  6. Monotonicity of entropy per element. For a stationary stochastic process X1; X2; : : : ; Xn ,  show that   a    b   H X1; X2; : : : ; Xn   H X1; X2; : : : ; Xn cid:0 1   :   cid:20   n  cid:0  1  H X1; X2; : : : ; Xn    cid:21  H XnjXn cid:0 1; : : : ; X1 :  n  n   4.51    4.52   Solution: Monotonicity of entropy per element.   a  By the chain rule for entropy,  H X1; X2; : : : ; Xn   n  = Pn  n  i=1 H XijX i cid:0 1  H XnjX n cid:0 1  +Pn cid:0 1 H XnjX n cid:0 1  + H X1; X2; : : : ; Xn cid:0 1   i=1 H XijX i cid:0 1   n  =  =  n   4.53    4.54   :   4.55   From stationarity it follows that for all 1  cid:20  i  cid:20  n ,  H XnjX n cid:0 1   cid:20  H XijX i cid:0 1 ;   68  Entropy Rates of a Stochastic Process  which further implies, by averaging both sides, that,  H XnjX n cid:0 1   cid:20  Pn cid:0 1  i=1 H XijX i cid:0 1   H X1; X2; : : : ; Xn cid:0 1   :  =  n  cid:0  1 n  cid:0  1 n cid:20  H X1; X2; : : : ; Xn cid:0 1  H X1; X2; : : : ; Xn cid:0 1   n  cid:0  1  1  :   cid:20   =  Combining  4.55  and  4.57  yields,  H X1; X2; : : : ; Xn   n  n  cid:0  1  b  By stationarity we have for all 1  cid:20  i  cid:20  n ,  which implies that  H XnjX n cid:0 1   cid:20  H XijX i cid:0 1 ;  H XnjX n cid:0 1  = Pn  cid:20  Pn  n  i=1 H XnjX n cid:0 1  i=1 H XijX i cid:0 1  H X1; X2; : : : ; Xn   n  =  n  :  + H X1; X2; : : : ; Xn cid:0 1  cid:21    4.56    4.57    4.58    4.59    4.60    4.61   7. Entropy rates of Markov chains.   a  Find the entropy rate of the two-state Markov chain with transition matrix   b  What values of p01; p10 maximize the rate of part  a ?  c  Find the entropy rate of the two-state Markov chain with transition matrix  P =" 1  cid:0  p01  p10  p01  1  cid:0  p10  :  P =" 1  cid:0  p p  0  :  1   d  Find the maximum value of the entropy rate of the Markov chain of part  c . We expect that the maximizing value of p should be less than 1=2 , since the 0 state permits more information to be generated than the 1 state.   e  Let N  t  be the number of allowable state sequences of length t for the Markov  chain of part  c . Find N  t  and calculate  H0 = lim t!1  1 t  log N  t  :  Hint: Find a linear recurrence that expresses N  t  in terms of N  t  cid:0  1  and N  t  cid:0  2  . Why is H0 an upper bound on the entropy rate of the Markov chain? Compare H0 with the maximum entropy found in part  d .   Entropy Rates of a Stochastic Process  69  Solution: Entropy rates of Markov chains.   a  The stationary distribution is easily calculated.  See EIT pp. 62{63.    cid:22 0 =  p10  p01 + p10  ;  cid:22 0 =  p01  :  p01 + p10  Therefore the entropy rate is  H X2jX1  =  cid:22 0H p01  +  cid:22 1H p10  =  p10H p01  + p01H p10   :  p01 + p10   b  The entropy rate is at most 1 bit because the process has only two states. This rate can be achieved if  and only if  p01 = p10 = 1=2 , in which case the process is actually i.i.d. with Pr Xi = 0  = Pr Xi = 1  = 1=2 .   c  As a special case of the general two-state Markov chain, the entropy rate is  H X2jX1  =  cid:22 0H p  +  cid:22 1H 1  =  H p  p + 1  :   d  By straightforward calculus, we  cid:12 nd that the maximum value of H X  of part  c   occurs for p =  3  cid:0  p5 =2 = 0:382 . The maximum value is  H p  = H 1  cid:0  p  = H p5  cid:0  1  2 ! = 0.694 bits :  Note that  p5  cid:0  1 =2 = 0:618 is  the reciprocal of  the Golden Ratio.   e  The Markov chain of part  c  forbids consecutive ones. Consider any allowable sequence of symbols of length t . If the  cid:12 rst symbol is 1, then the next symbol must be 0; the remaining N  t  cid:0  2  symbols can form any allowable sequence. If the  cid:12 rst symbol is 0, then the remaining N  t  cid:0  1  symbols can be any allowable sequence. So the number of allowable sequences of length t satis cid:12 es the recurrence  N  t  = N  t  cid:0  1  + N  t  cid:0  2   N  1  = 2; N  2  = 3   The initial conditions are obtained by observing that for t = 2 only the sequence 11 is not allowed. We could also choose N  0  = 1 as an initial condition, since there is exactly one allowable sequence of length 0, namely, the empty sequence.  The sequence N  t  grows exponentially, that is, N  t   cid:25  c cid:21 t , where  cid:21  is the maximum magnitude solution of the characteristic equation  1 = z cid:0 1 + z cid:0 2 :  Solving the characteristic equation yields  cid:21  =  1+p5 =2 , the Golden Ratio.  The sequence fN  t g is the sequence of Fibonacci numbers.  Therefore log N  t  = log 1 + p5 =2 = 0:694 bits :  H0 = lim n!1  1 t  Since there are only N  t  possible outcomes for X1; : : : ; Xt , an upper bound on H X1; : : : ; Xt  is log N  t  , and so the entropy rate of the Markov chain of part  c  is at most H0 . In fact, we saw in part  d  that this upper bound can be achieved.   70  Entropy Rates of a Stochastic Process  8. Maximum entropy process. A discrete memoryless source has alphabet f1; 2g where the symbol 1 has duration 1 and the symbol 2 has duration 2. The proba- bilities of 1 and 2 are p1 and p2 , respectively. Find the value of p1 that maximizes the source entropy per unit time H X =ElX . What is the maximum value H ? Solution: Maximum entropy process. The entropy per symbol of the source is  and the average symbol duration  or time per symbol  is  H p1  =  cid:0 p1 log p1  cid:0   1  cid:0  p1  log 1  cid:0  p1   T  p1  = 1  cid:1  p1 + 2  cid:1  p2 = p1 + 2 1  cid:0  p1  = 2  cid:0  p1 = 1 + p2 :  Therefore the source entropy per unit time is  f  p1  =  H p1  T  p1   =  cid:0 p1 log p1  cid:0   1  cid:0  p1  log 1  cid:0  p1   :  2  cid:0  p1  Since f  0  = f  1  = 0 , the maximum value of f  p1  must occur for some point p1 such that 0 < p1 < 1 and @f =@p1 = 0 and  @ @p1  H p1  T  p1   T  @H=@p1   cid:0  H @T =@p1   =  T 2  After some calculus, we  cid:12 nd that the numerator of the above expression  assuming natural logarithms  is  which is zero when 1 cid:0  p1 = p2 of the golden ratio, 1  T  @H=@p1   cid:0  H @T =@p1  = ln 1  cid:0  p1   cid:0  2 ln p1 ; 2  p5 cid:0  1  = 0:61803 , the reciprocal 2  p5 + 1  = 1:61803 . The corresponding entropy per unit time is  1 = p2 , that is, p1 = 1  H p1  T  p1   =  cid:0 p1 log p1  cid:0  p2 2  cid:0  p1  1 log p2 1  =  cid:0  1 + p2 1 + p2 1  1  log p1  =  cid:0  log p1 = 0:69424 bits:  Note that this result is the same as the maximum entropy rate for the Markov chain in problem 4.7 d . This is because a source in which every 1 must be followed by a 0 is equivalent to a source in which the symbol 1 has duration 2 and the symbol 0 has duration 1.  9. Initial conditions. Show, for a Markov chain, that  H X0jXn   cid:21  H X0jXn cid:0 1 :  Thus initial conditions X0 become more di cid:14 cult to recover as the future Xn unfolds. Solution: Initial conditions. For a Markov chain, by the data processing theorem, we have  I X0; Xn cid:0 1   cid:21  I X0; Xn :  Therefore  or H X0jXn  increases with n .  H X0   cid:0  H X0jXn cid:0 1   cid:21  H X0   cid:0  H X0jXn    4.62    4.63    Entropy Rates of a Stochastic Process  71  10. Pairwise independence. Let X1; X2; : : : ; Xn cid:0 1 be i.i.d. random variables taking i=1 Xi is odd and Xn = 0  values in f0; 1g , with PrfXi = 1g = 1 otherwise. Let n  cid:21  3 .  a  Show that Xi and Xj are independent, for i 6= j , i; j 2 f1; 2; : : : ; ng .  b  Find H Xi; Xj  , for i 6= j .  c  Find H X1; X2; : : : ; Xn  . Is this equal to nH X1  ?  2 . Let Xn = 1 if Pn cid:0 1  Solution:  Pairwise Independence  X1; X2; : : : ; Xn cid:0 1 are i.i.d. Bernoulli 1 2  random variables. We will  cid:12 rst prove that for any k  cid:20  n  cid:0  1 , the probability that Pk i=1 Xi is odd is 1=2 . We will prove this by induction. Clearly this is true for k = 1 . Assume that it is true for k  cid:0  1 . Let Sk =Pk  P  Sk odd  = P  Sk cid:0 1 odd P  Xk = 0  + P  Sk cid:0 1 even P  Xk = 1   i=1 Xi . Then   4.64   +  1 2  1 2  =  =  1 2  :  1 2 1 2  1 2  :  n cid:0 1  Xi=2 Xi=2  n cid:0 1  Hence for all k  cid:20  n  cid:0  1 , the probability that Sk is odd is equal to the probability that it is even. Hence,  P  Xn = 1  = P  Xn = 0  =   a  It is clear that when i and j are both less than n , Xi and Xj are independent. The only possible problem is when j = n . Taking i = 1 without loss of generality,  P  X1 = 1; Xn = 1  = P  X1 = 1;  Xi even    4.68   = P  X1 = 1 P    Xi even    4.69   =  1 2  1 2  = P  X1 = 1 P  Xn = 1    4.70    4.71   and similarly for other possible values of the pair  X1; Xn  . Hence X1 and Xn are independent.   b  Since Xi and Xj are independent and uniformly distributed on f0; 1g ,  H Xi; Xj  = H Xi  + H Xj  = 1 + 1 = 2 bits:   4.72    c  By the chain rule and the independence of X1; X2; : : : ; Xn1 , we have  H X1; X2; : : : ; Xn  = H X1; X2; : : : ; Xn cid:0 1  + H XnjXn cid:0 1; : : : ; X1  4.73   4.74   H Xi  + 0  n cid:0 1  =  Xi=1 = n  cid:0  1;   4.65    4.66    4.67    4.75    72  Entropy Rates of a Stochastic Process  since Xn is a function of the previous Xi ’s. The total entropy is not n , which is what would be obtained if the Xi ’s were all independent. This example illustrates that pairwise independence does not imply complete independence.  11. Stationary processes. Let : : : ; X cid:0 1; X0; X1; : : : be a stationary  not necessarily Markov  stochastic process. Which of the following statements are true? Prove or provide a counterexample.   a  H XnjX0  = H X cid:0 njX0  :  b  H XnjX0   cid:21  H Xn cid:0 1jX0  :  c  H XnjX1; X2; : : : ; Xn cid:0 1; Xn+1  is nonincreasing in n .  d  H XnjX1; : : : ; Xn cid:0 1; Xn+1; : : : ; X2n  is non-increasing in n . Solution: Stationary processes.   a  H XnjX0  = H X cid:0 njX0  . This statement is true, since  H XnjX0  = H Xn; X0   cid:0  H X0  H X cid:0 njX0  = H X cid:0 n; X0   cid:0  H X0    4.76    4.77   and H Xn; X0  = H X cid:0 n; X0  by stationarity.   b  H XnjX0   cid:21  H Xn cid:0 1jX0  .  This statement is not true in general, though it is true for  cid:12 rst order Markov chains. A simple counterexample is a periodic process with period n . Let X0; X1; X2; : : : ; Xn cid:0 1 be i.i.d. uniformly distributed binary random variables and let Xk = Xk cid:0 n for k  cid:21  n . In this case, H XnjX0  = 0 and H Xn cid:0 1jX0  = 1 , contradicting the statement H XnjX0   cid:21  H Xn cid:0 1jX0  .  1  ; Xn+1  is non-increasing in n .   c  H XnjX n cid:0 1 This statement is true, since by stationarity H XnjX n cid:0 1 H Xn+1jX n reduces entropy.  1  1 ; Xn+2  where the inequality follows from the fact that conditioning  ; Xn+1  = H Xn+1jX n  2 ; Xn+2   cid:21   12. The entropy rate of a dog looking for a bone. A dog walks on the integers, possibly reversing direction at each step with probability p = :1: Let X0 = 0 . The  cid:12 rst step is equally likely to be positive or negative. A typical walk might look like this:   X0; X1; : : :  =  0; cid:0 1; cid:0 2; cid:0 3; cid:0 4; cid:0 3; cid:0 2; cid:0 1; 0; 1; : : : :   a  Find H X1; X2; : : : ; Xn :  b  Find the entropy rate of this browsing dog.   c  What is the expected number of steps the dog takes before reversing direction?  Solution: The entropy rate of a dog looking for a bone.   Entropy Rates of a Stochastic Process  73   a  By the chain rule,  H X0; X1; : : : ; Xn  =  n  Xi=0  H XijX i cid:0 1   = H X0  + H X1jX0  +  H XijXi cid:0 1; Xi cid:0 2 ;  n  Xi=2  since, for i > 1 , the next position depends only on the previous two  i.e., the dog’s walk is 2nd order Markov, if the dog’s position is the state . Since X0 = 0 deterministically, H X0  = 0 and since the  cid:12 rst step is equally likely to be positive or negative, H X1jX0  = 1 . Furthermore for i > 1 ,  Therefore,   b  From a ,  H XijXi cid:0 1; Xi cid:0 2  = H :1; :9 :  H X0; X1; : : : ; Xn  = 1 +  n  cid:0  1 H :1; :9 :  H X0; X1; : : : Xn   n + 1  1 +  n  cid:0  1 H :1; :9   n + 1  = ! H :1; :9 :   c  The dog must take at least one step to establish the direction of travel from which it ultimately reverses. Letting S be the number of steps taken between reversals, we have  E S  =  s :9 s cid:0 1 :1   1Xs=1  = 10:  Starting at time 0, the expected number of steps to the  cid:12 rst reversal is 11.  13. The past has little to say about the future. For a stationary stochastic process  X1; X2; : : : ; Xn; : : : , show that  1 2n  lim n!1  I X1; X2; : : : ; Xn; Xn+1; Xn+2; : : : ; X2n  = 0:   4.78   Thus the dependence between adjacent n -blocks of a stationary process does not grow linearly with n .  Solution:  I X1; X2; : : : ; Xn; Xn+1; Xn+2; : : : ; X2n   = H X1; X2; : : : ; Xn  + H Xn+1; Xn+2; : : : ; X2n   cid:0  H X1; X2; : : : ; Xn; Xn+1; Xn+2; : : : ; X2n  = 2H X1; X2; : : : ; Xn   cid:0  H X1; X2; : : : ; Xn; Xn+1; Xn+2; : : : ; X2n   4.79    74  Entropy Rates of a Stochastic Process  since H X1; X2; : : : ; Xn  = H Xn+1; Xn+2; : : : ; X2n  by stationarity.  Thus  1 2n  lim n!1  I X1; X2; : : : ; Xn; Xn+1; Xn+2; : : : ; X2n   = lim n!1 = lim n!1  1 2H X1; X2; : : : ; Xn   cid:0  lim 2n n!1 1 1 H X1; X2; : : : ; Xn   cid:0  lim n 2n n!1  1 2n  H X1; X2; : : : ; Xn; Xn+1; Xn+2; : : : ; X2n   4.80   H X1; X2; : : : ; Xn; Xn+1; Xn+2; : : : ; X2n  4.81   Now limn!1 since both converge to the entropy rate of the process, and therefore  n H X1; X2; : : : ; Xn  = limn!1  1 2n H X1; X2; : : : ; Xn; Xn+1; Xn+2; : : : ; X2n   1  1 2n  lim n!1  I X1; X2; : : : ; Xn; Xn+1; Xn+2; : : : ; X2n  = 0:   4.82   14. Functions of a stochastic process.   a  Consider a stationary stochastic process X1; X2; : : : ; Xn , and let Y1; Y2; : : : ; Yn be  de cid:12 ned by  for some function  cid:30  . Prove that  Yi =  cid:30  Xi ;  i = 1; 2; : : :  H Y   cid:20  H X     b  What is the relationship between the entropy rates H Z  and H X   if  Zi =   Xi; Xi+1 ;  i = 1; 2; : : :   4.85   for some function   .  Solution: The key point is that functions of a random variable have lower entropy. Since  Y1; Y2; : : : ; Yn  is a function of  X1; X2; : : : ; Xn   each Yi is a function of the corresponding Xi  , we have  from Problem 2.4   H Y1; Y2; : : : ; Yn   cid:20  H X1; X2; : : : ; Xn   Dividing by n , and taking the limit as n ! 1 , we have  H Y1; Y2; : : : ; Yn   H X1; X2; : : : ; Xn   lim n!1  n   cid:20  lim n!1  n  or  H Y   cid:20  H X     4.83    4.84    4.86    4.87    4.88    Entropy Rates of a Stochastic Process  75   4.89   15. Entropy rate. Let fXig be a discrete stationary stochastic process with entropy rate  H X  : Show  for k = 1; 2; : : : .  1 n  H Xn; : : : ; X1 j X0; X cid:0 1; : : : ; X cid:0 k  ! H X  ;  Solution: Entropy rate of a stationary process. By the Ces cid:19 aro mean theorem, the running average of the terms tends to the same limit as the limit of the terms. Hence  1 n  H X1; X2; : : : ; XnjX0; X cid:0 1; : : : ; X cid:0 k  =  n  1 n  Xi=1 H XijXi cid:0 1; Xi cid:0 2; : : : ; X cid:0 k  4.90  ! lim H XnjXn cid:0 1; Xn cid:0 2; : : : ; X cid:0 k  4.91  = H;  4.92   the entropy rate of the process.  16. Entropy rate of constrained sequences. In magnetic recording, the mechanism of recording and reading the bits imposes constraints on the sequences of bits that can be recorded. For example, to ensure proper sychronization, it is often necessary to limit the length of runs of 0’s between two 1’s. Also to reduce intersymbol interference, it may be necessary to require at least one 0 between any two 1’s. We will consider a simple example of such a constraint.  Suppose that we are required to have at least one 0 and at most two 0’s between any pair of 1’s in a sequences. Thus, sequences like 101001 and 0101001 are valid sequences, but 0110010 and 0000101 are not. We wish to calculate the number of valid sequences of length n .   a  Show that the set of constrained sequences is the same as the set of allowed paths  on the following state diagram:   b  Let Xi n  be the number of valid paths of length n ending at state i . Argue that  X n  = [X1 n  X2 n  X3 n ]t satis cid:12 es the following recursion:  with initial conditions X 1  = [1 1 0]t .   c  Let  X1 n  X2 n  X3 n   2 64  0 1 1 1 0 0 0 1 0  3 75  2 64  X1 n  cid:0  1  X2 n  cid:0  1  X3 n  cid:0  1   3 75 ;  3 75 =2 64 A =2 64  0 1 1 1 0 0 0 1 0  :  3 75   4.93    4.94   Then we have by induction  X n  = AX n  cid:0  1  = A2X n  cid:0  2  =  cid:1  cid:1  cid:1  = An cid:0 1X 1 :   4.95    76  Entropy Rates of a Stochastic Process   cid:2  cid:3  cid:2   cid:2  cid:3  cid:2   Figure 4.1: Entropy rate of constrained sequence  Using the eigenvalue decomposition of A for the case of distinct eigenvalues, we can write A = U cid:0 1 cid:3 U , where  cid:3  is the diagonal matrix of eigenvalues. Then An cid:0 1 = U cid:0 1 cid:3 n cid:0 1U . Show that we can write 1 Y1 +  cid:21 n cid:0 1  2 Y2 +  cid:21 n cid:0 1  X n  =  cid:21 n cid:0 1  3 Y3;   4.96   where Y1; Y2; Y3 do not depend on n . For large n , this sum is dominated by the largest term. Therefore argue that for i = 1; 2; 3 , we have  1 n  log Xi n  ! log  cid:21 ;  where  cid:21  is the largest  positive  eigenvalue. Thus the number of sequences of length n grows as  cid:21 n for large n . Calculate  cid:21  for the matrix A above.  The case when the eigenvalues are not distinct can be handled in a similar manner.    d  We will now take a di cid:11 erent approach. Consider a Markov chain whose state diagram is the one given in part  a , but with arbitrary transition probabilities. Therefore the probability transition matrix of this Markov chain is  Show that the stationary distribution of this Markov chain is  P =2 64  1  0  0  cid:11  0 1  cid:0   cid:11  1  0  0   cid:22  = cid:20   1  ;  1  ;  3  cid:0   cid:11   3  cid:0   cid:11   3 75 : 3  cid:0   cid:11  cid:21  : 1  cid:0   cid:11    e  Maximize the entropy rate of the Markov chain over choices of  cid:11  . What is the  maximum entropy rate of the chain?   4.97    4.98    4.99    cid:0   cid:0   cid:1   cid:1   cid:4   cid:4   cid:5   cid:5   cid:6   cid:6   Entropy Rates of a Stochastic Process  77   f  Compare the maximum entropy rate in part  e  with log  cid:21  in part  c . Why are  the two answers the same?  Solution:  Entropy rate of constrained sequences.   a  The sequences are constrained to have at least one 0 and at most two 0’s between two 1’s. Let the state of the system be the number of 0’s that has been seen since the last 1. Then a sequence that ends in a 1 is in state 1, a sequence that ends in 10 in is state 2, and a sequence that ends in 100 is in state 3. From state 1, it is only possible to go to state 2, since there has to be at least one 0 before the next 1. From state 2, we can go to either state 1 or state 3. From state 3, we have to go to state 1, since there cannot be more than two 0’s in a row. Thus we can the state diagram in the problem.   b  Any valid sequence of length n that ends in a 1 must be formed by taking a valid sequence of length n cid:0  1 that ends in a 0 and adding a 1 at the end. The number of valid sequences of length n cid:0  1 that end in a 0 is equal to X2 n cid:0  1  + X3 n cid:0  1  and therefore,  By similar arguments, we get the other two equations, and we have  X1 n  = X2 n  cid:0  1  + X3 n  cid:0  1 :  X1 n  X2 n  X3 n   2 64  3 75  =2 64  0 1 1 1 0 0 0 1 0  3 75  2 64  X1 n  cid:0  1  X2 n  cid:0  1  X3 n  cid:0  1   :  3 75  The initial conditions are obvious, since both sequences of length 1 are valid and therefore X 1  = [1 1 0]T .   c  The induction step is obvious. Now using the eigenvalue decomposition of A =  U cid:0 1 cid:3 U , it follows that A2 = U cid:0 1 cid:3 U U cid:0 1 cid:3 U = U cid:0 1 cid:3 2U , etc. and therefore  X n  = An cid:0 1X 1  = U cid:0 1 cid:3 n cid:0 1U X 1    cid:21 n cid:0 1 2 0  0  =  cid:21 n cid:0 1   cid:21 n cid:0 1 1 0 0  = U cid:0 12 64 1 U cid:0 12 64 3 U cid:0 12 64  + cid:21 n cid:0 1  1 Y1 +  cid:21 n cid:0 1  =  cid:21 n cid:0 1  1 0 0 0 0 0 0 0 0  0 0   cid:21 n cid:0 1 3  3 75 U2 64  1 1 0  1 1 0  3 75 U2 3 75 64 2 U cid:0 12 3 75 +  cid:21 n cid:0 1 64 3 75 U2 3 64 75  1 1 0  0 0 0 0 0 0 0 0 1 2 Y2 +  cid:21 n cid:0 1  3 Y3;  0 0 0 0 1 0 0 0 0  3 75 U2 64  1 1 0  3 75   4.100    4.101    4.102    4.103    4.104    4.105    78  Entropy Rates of a Stochastic Process  where Y1; Y2; Y3 do not depend on n . Without loss of generality, we can assume that  cid:21 1 >  cid:21 2 >  cid:21 3 . Thus  X1 n  =  cid:21 n cid:0 1 X2 n  =  cid:21 n cid:0 1 X3 n  =  cid:21 n cid:0 1  1 Y11 +  cid:21 n cid:0 1 1 Y12 +  cid:21 n cid:0 1 1 Y13 +  cid:21 n cid:0 1  2 Y21 +  cid:21 n cid:0 1 2 Y22 +  cid:21 n cid:0 1 2 Y23 +  cid:21 n cid:0 1  3 Y31 3 Y32 3 Y33  For large n , this sum is dominated by the largest term. Thus if Y1i > 0 , we have   4.106    4.107    4.108    4.109    4.110   To be rigorous, we must also show that Y1i > 0 for i = 1; 2; 3 . It is not di cid:14 cult to prove that if one of the Y1i is positive, then the other two terms must also be positive, and therefore either  1 n  1 n  log Xi n  ! log  cid:21 1:  log Xi n  ! log  cid:21 1:  for all i = 1; 2; 3 or they all tend to some other value. The general argument is di cid:14 cult since it is possible that the initial conditions of the recursion do not have a component along the eigenvector that corresponds to the maximum eigenvalue and thus Y1i = 0 and the above argument will fail. In our example, we can simply compute the various quantities, and thus  A =2 64  0 1 1 1 0 0 0 1 0  3 75  = U cid:0 1 cid:3 U;   4.111   where  and  U =2 64  and therefore   cid:3  =2 64  1:3247  0 0   cid:0 0:6624 + 0:5623i  0  0  0 0   cid:0 0:6624  cid:0  0:5623i  3 75 ;   4.112    cid:0 0:5664   cid:0 0:7503   cid:0 0:4276  0:6508  cid:0  0:0867i  cid:0 0:3823 + 0:4234i  cid:0 0:6536  cid:0  0:4087i 0:6508 + 0:0867i  cid:0 0:3823i0:4234i  cid:0 0:6536 + 0:4087i  ;   4.113   3 75  Y1 =2 64  0:9566 0:7221 0:5451  ;  3 75   4.114   which has all positive components. Therefore,  1 n  log Xi n  ! log  cid:21 i = log 1:3247 = 0:4057 bits:   4.115    Entropy Rates of a Stochastic Process   d  To verify the that   cid:22  = cid:20   1  ;  1  ;  3  cid:0   cid:11   3  cid:0   cid:11   3  cid:0   cid:11  cid:21 T 1  cid:0   cid:11   :  is the stationary distribution, we have to verify that P  cid:22  =  cid:22  . But this is straight- forward.   e  The entropy rate of the Markov chain  in nats  is  H =  cid:0 Xi   cid:22 iXj  1  3  cid:0   cid:11   Pij ln Pij =    cid:0  cid:11  ln  cid:11   cid:0   1  cid:0   cid:11   ln 1  cid:0   cid:11    ;   4.117   and di cid:11 erentiating with respect to  cid:11  to  cid:12 nd the maximum, we  cid:12 nd that  dH d cid:11   =  1   3  cid:0   cid:11  2   cid:0  cid:11  ln  cid:11   cid:0   1  cid:0   cid:11   ln 1  cid:0   cid:11   +  3  cid:0   cid:11     cid:0 1  cid:0  ln  cid:11  + 1 + ln 1  cid:0   cid:11    = 0;  1   3  cid:0   cid:11    ln a  cid:0  ln 1  cid:0   cid:11    =   cid:0  cid:11  ln  cid:11   cid:0   1  cid:0   cid:11   ln 1  cid:0   cid:11     which reduces to  or  i.e.,  3 ln  cid:11  = 2 ln 1  cid:0   cid:11  ;   cid:11 3 =  cid:11 2  cid:0  2 cid:11  + 1;  79   4.116    4.118    4.119    4.120    4.121   which can be solved  numerically  to give  cid:11  = 0:5698 and the maximum entropy rate as 0.2812 nats = 0.4057 bits.   f  The answers in parts  c  and  f  are the same. Why? A rigorous argument is quite involved, but the essential idea is that both answers give the asymptotics of the number of sequences of length n for the state diagram in part  a . In part  c  we used a direct argument to calculate the number of sequences of length n and found that asymptotically, X n   cid:25   cid:21 n 1 : If we extend the ideas of Chapter 3  typical sequences  to the case of Markov chains, we can see that there are approximately 2nH typical sequences of length n for a Markov chain of entropy rate H . If we consider all Markov chains with state diagram given in part  a , the number of typical sequences should be less than the total number of sequences of length n that satisfy the state constraints. Thus, we see that 2nH  cid:20   cid:21 n To complete the argument, we need to show that there exists a Markov transition matrix that achieves the upper bound. This can be done by two di cid:11 erent methods. One is to derive the Markov transition matrix from the eigenvalues, etc. of parts  a { c . Instead, we will use an argument from the method of types. In Chapter 12, we show that there are at most a polynomial number of types, and that therefore, the largest type class has the same number of sequences  to the  cid:12 rst order in the exponent  as the entire set. The same arguments can be applied to Markov types. There are only a polynomial number of Markov types and therefore of all  1 or H  cid:20  log  cid:21 1 .   80  Entropy Rates of a Stochastic Process  the Markov type classes that satisfy the state constraints of part  a , at least one of them has the same exponent as the total number of sequences that satisfy the state constraint. For this Markov type, the number of sequences in the typeclass is 2nH , and therefore for this type class, H = log  cid:21 1 . This result is a very curious one that connects two apparently unrelated objects - the maximum eigenvalue of a state transition matrix, and the maximum entropy rate for a probability transition matrix with the same state diagram. We don’t know a reference for a formal proof of this result.  17. Waiting times are insensitive to distributions. Let X0; X1; X2; : : : be drawn i.i.d.  cid:24  p x ; x 2 X = f1; 2; : : : ; mg and let N be the waiting time to the next occurrence of X0 , where N = minnfXn = X0g .  a  Show that EN = m .  b  Show that E log N  cid:20  H X  .  c   Optional  Prove part  a  for fXig stationary and ergodic. Solution: Waiting times are insensitive to distributions. Since X0; X1; X2; : : : ; Xn are drawn i.i.d.  cid:24  p x  , the waiting time for the next occurrence of X0 has a geometric distribution with probability of success p x0  .   a  Given X0 = i , the expected time until we see it again is 1=p i  . Therefore,  EN = E[E NjX0 ] =X p X0 = i  cid:18  1  p i  cid:19  = m:   b  By the same argument, since given X0 = i , N has a geometric distribution with  mean p i  and  E NjX0 = i  =  :  p i   1  Then using Jensen’s inequality, we have  E log N = Xi  cid:20  Xi = Xi  = H X :  p X0 = i E log NjX0 = i  p X0 = i  log E NjX0 = i   p i  log  1  p i    c  The property that EN = m is essentially a combinatorial property rather than a statement about expectations. We prove this for stationary ergodic sources. In essence, we will calculate the empirical average of the waiting time, and show that this converges to m . Since the process is ergodic, the empirical average converges to the expected value, and thus the expected value must be m .   4.122    4.123    4.124    4.125    4.126    4.127    Entropy Rates of a Stochastic Process  81  Let X1 = a , and de cid:12 ne a sequence of random variables N1; N2; : : : , where N1 = recurrence time for X1 , etc. It is clear that the N process is also stationary and ergodic. Let Ia Xi  be the indicator that Xi = a and Ja Xi  be the indicator that Xi 6= a . Then for all i , all a 2 X , Ia Xi  + Ja Xi  = 1 . Let N1 a ; N2 a ; : : : be the recurrence times of the symbol a in the sequence. Thus X1 = a , Xi 6= a; 1 < i < N1 a  , and XN1 a  = a , etc. Thus the sum of Ja Xi  over all i is equal to the Pj Nj a   cid:0  1  . Or equivalently  Xj  Nj a  =Xi  Ja Xi  +Xi  Ia Xi  = n   4.128   Summing this over all a 2 X , we obtain Xa Xj  Nj a  = nm   4.129   There are n terms in this sum, and therefore the empirical mean of Nj Xi  is m . Thus the empirical average of N over any sample sequence is m and thus the expected value of N must also be m .  18. Stationary but not ergodic process. A bin has two biased coins, one with prob- ability of heads p and the other with probability of heads 1  cid:0  p . One of these coins is chosen at random  i.e., with probability 1 2 , and is then tossed n times. Let X denote the identity of the coin that is picked, and let Y1 and Y2 denote the results of the  cid:12 rst two tosses.   a  Calculate I Y1; Y2jX  .  b  Calculate I X; Y1; Y2  .  c  Let H Y  be the entropy rate of the Y process  the sequence of coin tosses .  Calculate H Y  .  Hint: Relate this to limn!1  1 nH X; Y1; Y2; : : : ; Yn   .  You can check the answer by considering the behavior as p ! 1=2 . Solution:   a  SInce the coin tosses are indpendent conditional on the coin chosen, I Y1; Y2jX  =  0 .   b  The key point is that if we did not know the coin being used, then Y1 and Y2 are not independent. The joint distribution of Y1 and Y2 can be easily calculated from the following table   82  Entropy Rates of a Stochastic Process  p2  X Y1 Y2 Probability 1 H H 1 H T 1 T H 1 T T 2 H H 2 H T 2 T H 2 T T  p 1  cid:0  p  p 1  cid:0  p   1  cid:0  p 2  1  cid:0  p 2 p 1  cid:0  p  p 1  cid:0  p   p2  Thus the joint distribution of  Y1; Y2  is   1  1  cid:0  p 2   , and we can now calculate  2  p2 +  1 cid:0  p 2 ; p 1 cid:0  p ; p 1 cid:0  p ; 1  2  p2 +  I X; Y1; Y2  = H Y1; Y2   cid:0  H Y1; Y2jX   = H Y1; Y2   cid:0  H Y1jX   cid:0  H Y2jX  = H Y1; Y2   cid:0  2H p  = H cid:18  1 = H p 1  cid:0  p   + 1  cid:0  2H p    p2 +  1  cid:0  p 2 ; p 1  cid:0  p ; p 1  cid:0  p ;  2   4.130    4.131    4.132   1 2   p2 +  1  cid:0  p 2  cid:19   cid:0  2H p    4.133   where the last step follows from using the grouping rule for entropy.   c   H Y1; Y2; : : : ; Yn   n  H Y  = lim = lim  H X; Y1; Y2; : : : ; Yn   cid:0  H XjY1; Y2; : : : ; Yn  H X  + H Y1; Y2; : : : ; YnjX   cid:0  H XjY1; Y2; : : : ; Yn   n  = lim  n   4.134    4.135    4.136   Since 0  cid:20  H XjY1; Y2; : : : ; Yn   cid:20  H X   cid:20  1 , we have lim 1 ilarly 1 Yi ’s are i.i.d. given X . Combining these terms, we get  n H X  = 0 and sim- nH XjY1; Y2; : : : ; Yn  = 0 . Also, H Y1; Y2; : : : ; YnjX  = nH p  , since the  nH p   H Y  = lim  n  = H p    4.137   19. Random walk on graph. Consider a random walk on the graph   Entropy Rates of a Stochastic Process  83  t  2 @  cid:10  cid:10    cid:10    cid:10    cid:10    cid:10   t  B  3 B B  @@  cid:3   cid:3    cid:3   1   cid:3   B   cid:10    cid:3   cid:3   @  B B   cid:10  @  t XXXXXXXXXX BB  cid:8  cid:8  cid:8  cid:8  cid:8  t   cid:3  @ 5  @  @   cid:3    cid:3    cid:3   t  4   a  Calculate the stationary distribution.   b  What is the entropy rate?   c  Find the mutual information I Xn+1; Xn  assuming the process is stationary.  Solution:   a  The stationary distribution for a connected graph of undirected edges with equal weight is given as  cid:22 i = Ei 2E where Ei denotes the number of edges emanating from node i and E is the total number of edges in the graph. Hence, the station- 16 ; 3 16 ; 3 ary distribution is [ 3 16 ] ; i.e., the  cid:12 rst four nodes exterior nodes have steady state probability of  16 ; 3 16 while node 5 has steady state probability of 1 4 .  16 ; 4  3   b  Thus, the entropy rate of the random walk on this graph is 4 3  16 log2 3 + 4  16 log2 4  =  3  4 log2 3  + 1  2 = log 16  cid:0  H 3=16; 3=16; 3=16; 3=16; 1=4    c  The mutual information  I Xn+1; Xn  = H Xn+1   cid:0  H Xn+1jXn    4.138  = H 3=16; 3=16; 3=16; 3=16; 1=4   cid:0   log16  cid:0  H 3=16; 3=16; 3=16; 3=16; 1=4    4.139  = 2H 3=16; 3=16; 3=16; 3=16; 1=4   cid:0  log16  4.140  = 2    4.141   log  +  16 3  1 4  log 4   cid:0  log16   4.142   3 4 = 3  cid:0   3 2  log 3  20. Random walk on chessboard. Find the entropy rate of the Markov chain associated  with a random walk of a king on the 3  cid:2  3 chessboard  1 4 7  2 5 8  3 6 9   84  Entropy Rates of a Stochastic Process  What about the entropy rate of rooks, bishops and queens? There are two types of bishops.  Solution:  Random walk on the chessboard.  Notice that the king cannot remain where it is. It has to move from one state to the next. The stationary distribution is given by  cid:22 i = Ei=E , where Ei = number of edges emanating from node i and E = P9 i=1 Ei . By inspection, E1 = E3 = E7 = E9 = 3 , E2 = E4 = E6 = E8 = 5 , E5 = 8 and E = 40 , so  cid:22 1 =  cid:22 3 =  cid:22 7 =  cid:22 9 = 3=40 ,  cid:22 2 =  cid:22 4 =  cid:22 6 =  cid:22 8 = 5=40 and  cid:22 5 = 8=40 . In a random walk the next state is chosen with equal probability among possible choices, so H X2jX1 = i  = log 3 bits for i = 1; 3; 7; 9 , H X2jX1 = i  = log 5 for i = 2; 4; 6; 8 and H X2jX1 = i  = log 8 bits for i = 5 . Therefore, we can calculate the entropy rate of the king as  H =   cid:22 iH X2jX1 = i   9  Xi=1  = 0:3 log 3 + 0:5 log 5 + 0:2 log 8  = 2:24 bits:   4.143    4.144    4.145   21. Maximal entropy graphs. Consider a random walk on a connected graph with 4  edges.   a  Which graph has the highest entropy rate?   b  Which graph has the lowest?  Solution: Graph entropy.  There are  cid:12 ve graphs with four edges.   Entropy Rates of a Stochastic Process  85  Where the entropy rates are 1=2+3=8 log 3   cid:25  1:094; 1; :844:  :75; 1 and 1=4+3=8 log 3   cid:25    a  From the above we see that the  cid:12 rst graph maximizes entropy rate with and  entropy rate of 1:094:   b  From the above we see that the third graph minimizes entropy rate with and  entropy rate of :75:  22. 3-D Maze.  A bird is lost in a 3  cid:2  3  cid:2  3 cubical maze. The bird  cid:13 ies from room to room going to adjoining rooms with equal probability through each of the walls. To be speci cid:12 c, the corner rooms have 3 exits.   86  Entropy Rates of a Stochastic Process   a  What is the stationary distribution?   b  What is the entropy rate of this random walk?  Solution: 3D Maze. The entropy rate of a random walk on a graph with equal weights is given by equation 4.41 in the text:  H X   = log 2E   cid:0  H cid:18  E1  2E  ; : : : ;  Em  2E cid:19   There are 8 corners, 12 edges, 6 faces and 1 center. Corners have 3 edges, edges have 4 edges, faces have 5 edges and centers have 6 edges. Therefore, the total number of edges E = 54 . So,  H X   = log 108  + 8 cid:18  3  108  log  3  108 cid:19  + 12 cid:18  4  108  log  4  108 cid:19  + 6 cid:18  5  108  log  5  108 cid:19  + 1 cid:18  6  108  log  6  108 cid:19   = 2:03 bits  23. Entropy rate  Let fXig be a stationary stochastic process with entropy rate H X   .  a  Argue that H X    cid:20  H X1  .  b  What are the conditions for equality?  Solution: Entropy Rate   a  From Theorem 4.2.1  H X   = H X1jX0; X cid:0 1; : : :   cid:20  H X1    4.146   since conditioning reduces entropy   b  We have equality only if X1 is independent of the past X0; X cid:0 1; : : : , i.e., if and  only if Xi is an i.i.d. process.  24. Entropy rates  Let fXig be a stationary process. Let Yi =  Xi; Xi+1  . Let Zi =  X2i; X2i+1  . Let Vi = X2i . Consider the entropy rates H X   , H Y  , H Z  , and H V  of the processes fXig ,fYig , fZig , and fVig . What is the inequality relationship  cid:20  , =, or  cid:21  between each of the pairs listed below:   a  H X    cid:21   b  H X    cid:21   c  H X    cid:21   d  H Z   cid:21    cid:20  H Y  .  cid:20  H Z  .  cid:20  H V  .  cid:20  H X   .   Entropy Rates of a Stochastic Process  87  Solution: Entropy rates fXig is a stationary process, Yi =  Xi; Xi+1  . Let Zi =  X2i; X2i+1  . Let Vi = X2i . Consider the entropy rates H X   , H Y  , H Z  , and H V  of the processes fXig , fZig , and fVig .  a  H X   = H  Y  , since H X1; X2; : : : ; Xn; Xn+1  = H Y1; Y2; : : : ; Yn  , and dividing  by n and taking the limit, we get equality.   b  H X   < H  Z  , since H X1; : : : ; X2n  = H Z1; : : : ; Zn  , and dividing by n and  taking the limit, we get 2H X   = H Z  .   c  H X   > H  V  , since H V1jV0; : : :  = H X2jX0; X cid:0 2; : : :   cid:20  H X2jX1; X0; X cid:0 1; : : :  .  d  H Z  = 2H  X   since H X1; : : : ; X2n  = H Z1; : : : ; Zn  , and dividing by n and  taking the limit, we get 2H X   = H Z  .  25. Monotonicity.   a  Show that I X; Y1; Y2; : : : ; Yn  is non-decreasing in n .  b  Under what conditions is the mutual information constant for all n ?  Solution: Monotonicity   a  Since conditioning reduces entropy,  H XjY1; Y2; : : : ; Yn   cid:21  H XjY1; Y2; : : : ; Yn; Yn+1    4.147   and hence  I X; Y1; Y2; : : : ; Yn  = H X   cid:0  H XjY1; Y2; : : : ; Yn    cid:20  H X   cid:0  H XjY1; Y2; : : : ; Yn;n+1   = I X; Y1; Y2; : : : ; Yn; Yn+1    4.148    4.149    4.150    b  We have equality if and only if H XjY1; Y2; : : : ; Yn  = H XjY1  for all n , i.e., if  X is conditionally independent of Y2; : : : given Y1 .  26. Transitions in Markov chains. Suppose fXig forms an irreducible Markov chain with transition matrix P and stationary distribution  cid:22  . Form the associated \edge- process" fYig by keeping track only of the transitions. Thus the new process fYig takes values in X  cid:2  X , and Yi =  Xi cid:0 1; Xi  . For example  X = 3; 2; 8; 5; 7; : : :  becomes  Y =  ;; 3 ;  3; 2 ;  2; 8 ;  8; 5 ;  5; 7 ; : : :  Find the entropy rate of the edge process fYig . Solution: Edge Process H X   = H  Y  , since H X1; X2; : : : ; Xn; Xn+1  = H Y1; Y2; : : : ; Yn  , and dividing by n and taking the limit, we get equality.   88  27. Entropy rate  Entropy Rates of a Stochastic Process  Let fXig be a stationary f0; 1g valued stochastic process obeying  Xk+1 = Xk  cid:8  Xk cid:0 1  cid:8  Zk+1;  where fZig is Bernoulli  p   and  cid:8  denotes mod 2 addition. What is the entropy rate H X   ? Solution: Entropy Rate  H X   = H Xk+1jXk; Xk cid:0 1; : : :  = H Xk+1jXk; Xk cid:0 1  = H Zk+1  = H p    4.151   28. Mixture of processes  Suppose we observe one of two stochastic processes but don’t know which. What is the entropy rate? Speci cid:12 cally, let X11; X12; X13; : : : be a Bernoulli process with parameter p1 and let X21; X22; X23; : : : be Bernoulli  p2  . Let  i = 1; 2; : : : , be the observed stochastic process. Thus Y observes  1; with probability 1 2 2; with probability 1 2   cid:18  =8< :  and let Yi = X cid:18 i; the process fX1ig or fX2ig . Eventually Y will know which.  a  Is fYig stationary?  b  Is fYig an i.i.d. process?  c  What is the entropy rate H of fYig ?  d  Does  1 n   cid:0   log p Y1; Y2; : : : Yn   cid:0 ! H?   e  Is there a code that achieves an expected per-symbol description length 1  n ELn  cid:0 !  H ?  Now let  cid:18 i be Bern  1  2  . Observe  Zi = X cid:18 ii;  i = 1; 2; : : : ;  Thus  cid:18  is not  cid:12 xed for all time, as it was in the  cid:12 rst part, but is chosen i.i.d. each time. Answer  a ,  b ,  c ,  d ,  e  for the process fZig , labeling the answers  a 0  ,  b 0  ,  c 0  ,  d 0  ,  e 0  . Solution: Mixture of processes.   a  Yes, fYig is stationary, since the scheme that we use to generate the Yi s doesn’t  change with time.   Entropy Rates of a Stochastic Process  89   b  No, it is not IID, since there’s dependence now { all Yi s have been generated  according to the same parameter  cid:18  . Alternatively, we can arrive at the result by examining I Yn+1; Y n  . If the process were to be IID, then the expression I Yn+1; Y n  would have to be 0 . However, if we are given Y n , then we can estimate what  cid:18  is, which in turn allows us to predict Yn+1 . Thus, I Yn+1; Y n  is nonzero.   c  The process fYig is the mixture of two Bernoulli processes with di cid:11 erent param- eters, and its entropy rate is the mixture of the two entropy rates of the two processes so it’s given by  More rigorously,  H p1  + H p2   :  2  H Y n   H = lim n!1 = lim n!1 H p1  + H p2   1 n 1 n  =  2   H  cid:18   + H Y nj cid:18    cid:0  H  cid:18 jY n    Note that only H Y nj cid:18   grows with n . The rest of the term is  cid:12 nite and will go to 0 as n goes to 1 .   d  The process fYig is not ergodic, so the AEP does not apply and the quantity  cid:0  1=n  log P  Y1; Y2; : : : ; Yn  does NOT converge to the entropy rate.  But it does converge to a random variable that equals H p1  w.p. 1 2 and H p2  w.p. 1 2.   e  Since the process is stationary, we can do Hu cid:11 man coding on longer and longer blocks of the process. These codes will have an expected per-symbol length bounded above by H X1;X2;:::;Xn +1  and this converges to H X   .  n   a’  Yes, fYig is stationary, since the scheme that we use to generate the Yi ’s doesn’t  change with time.   b’  Yes, it is IID, since there’s no dependence now { each Yi  is generated according  to an independent parameter  cid:18 i , and Yi  cid:24  Bernoulli   p1 + p2 =2  .   c’  Since the process is now IID, its entropy rate is  H cid:18  p1 + p2  2   cid:19  :   d’  Yes, the limit exists by the AEP.   e’  Yes, as in  e  above.  29. Waiting times.  Let X be the waiting time for the  cid:12 rst heads to appear in successive  cid:13 ips of a fair coin. Thus, for example, P rfX = 3g =   1  2  3 .   90  Entropy Rates of a Stochastic Process  Let Sn be the waiting time for the nth head to appear. Thus,  S0 = 0  Sn+1 = Sn + Xn+1  where X1; X2; X3; : : : are i.i.d according to the distribution above.  a  Is the process fSng stationary?  b  Calculate H S1; S2; : : : ; Sn  .  c  Does the process fSng have an entropy rate? If so, what is it? If not, why not?  d  What is the expected number of fair coin  cid:13 ips required to generate a random  variable having the same distribution as Sn ?  Solution: Waiting time process.   a  For the process to be stationary, the distribution must be time invariant. It turns  out that process fSng is not stationary. There are several ways to show this.  marginals are not time-invariant.   cid:15  S0 is always 0 while Si , i 6= 0 can take on several values. Since the marginals for S0 and S1 , for example, are not the same, the process can’t be stationary.  cid:15  It’s clear that the variance of Sn grows with n , which again implies that the  cid:15  Process fSng is an independent increment process. An independent increment process is not stationary  not even wide sense stationary , since var Sn  = var Xn  + var Sn cid:0 1  > var Sn cid:0 1  .   b  We can use chain rule and Markov properties to obtain the following results.  H S1; S2; : : : ; Sn  = H S1  +  n  n  H SijSi cid:0 1   Xi=2 Xi=2 H SijSi cid:0 1  Xi=2  H Xi   n  = H S1  +  = H X1  +  =  H Xi   n  Xi=1  = 2n   c  It follows trivially from the previous part that  H Sn   H S  = lim n!1 = lim n!1  = 2  n  2n n   Entropy Rates of a Stochastic Process  91  Note that the entropy rate can still exist even when the process is not stationary. Furthermore, the entropy rate  for this problem  is the same as the entropy of X.  d  The expected number of  cid:13 ips required can be lower-bounded by H Sn  and upper- bounded by H Sn  + 2  Theorem 5.12.3, page 115 . Sn has a negative binomial distribution; i.e., P r Sn = k  =   k  cid:0  1 2  k for k  cid:21  n .  We have the n th success at the k th trial if and only if we have exactly n  cid:0  1 successes in k  cid:0  1 trials and a suceess at the k th trial.   n  cid:0  1 !   1  Since computing the exact value of H Sn  is di cid:14 cult  and fruitless in the exam setting , it would be su cid:14 cient to show that the expected number of  cid:13 ips required is between H Sn  and H Sn  + 2 , and set up the expression of H Sn  in terms of the pmf of Sn .  Note, however, that for large n , however, the distribution of Sn will tend to Gaussian with mean n Let pk = P r Sn = k + ESn  = P r Sn = k + 2n  . Let  cid:30  x  be the normal density  p = 2n and variance n 1  cid:0  p =p2 = 2n .  function with mean zero and variance 2n , i.e.  cid:30  x  = exp   cid:0 x2=2 cid:27 2 =p2 cid:25  cid:27 2 ,  where  cid:27 2 = 2n . Then for large n , since the entropy is invariant under any constant shift of a random variable and  cid:30  x  log  cid:30  x  is Riemann integrable,  H Sn  = H Sn  cid:0  E Sn   =  cid:0 X pk log pk  cid:25   cid:0 X  cid:30  k  log  cid:30  k   cid:25   cid:0 Z  cid:30  x  log  cid:30  x dx =   cid:0  log e Z  cid:30  x  ln  cid:30  x dx =   cid:0  log e Z  cid:30  x   cid:0   =  log e    +  ln 2 cid:25  cid:27 2   1 2  1 2  =  =  1 2 1 2  log 2 cid:25 e cid:27 2  log n cid:25 e + 1:  x2  2 cid:27 2  cid:0  lnp2 cid:25  cid:27 2    Refer to Chapter 9 for a more general discussion of the entropy of a continuous random variable and its relation to discrete entropy.   Here is a speci cid:12 c example for n = 100 . Based on earlier discussion, P r S100 = 2  k . The Gaussian approximation of H Sn  is 5:8690 while  k  =   k  cid:0  1  100  cid:0  1 !   1   92  Entropy Rates of a Stochastic Process  the exact value of H Sn  is 5:8636 . The expected number of  cid:13 ips required is somewhere between 5:8636 and 7:8636 .  30. Markov chain transitions.  P = [Pij] =2 664  1 2 1 4 1 4  1 4 1 2 1 4  1 4 1 4 1 2  3 775  Let X1 be uniformly distributed over the states f0; 1; 2g: Let fXig11 be a Markov chain with transition matrix P , thus P  Xn+1 = jjXn = i  = Pij; i; j 2 f0; 1; 2g:  a  Is fXng stationary?  b  Find limn!1  1 n H X1; : : : ; Xn :  Now consider the derived process Z1; Z2; : : : ; Zn; where  Z1 = X1 Zi = Xi  cid:0  Xi cid:0 1   mod 3 ;  i = 2; : : : ; n:  Thus Z n encodes the transitions, not the states.   c  Find H Z1; Z2; :::; Zn :  d  Find H Zn  and H Xn ; for n  cid:21  2 .  e  Find H ZnjZn cid:0 1  for n  cid:21  2 .  f  Are Zn cid:0 1 and Zn independent for n  cid:21  2 ? Solution:   a  Let  cid:22 n denote the probability mass function at time n . Since  cid:22 1 =   1 3   for all n and fXng is stationary.   cid:22 2 =  cid:22 1P =  cid:22 1 ,  cid:22 n =  cid:22 1 =   1 Alternatively, the observation P is doubly stochastic will lead the same conclusion.  3   and  3 ; 1  3 ; 1  3 ; 1  3 ; 1   b  Since fXng is stationary Markov,  lim n!1  H X1; : : : ; Xn  = H X2jX1   P  X1 = k H X2jX1 = k  1 3  cid:2  H   1 4  1 4  1 2     ;  ;  2  =  Xk=0 = 3  cid:2  : =  3 2   Entropy Rates of a Stochastic Process  93   c  Since  X1; : : : ; Xn  and  Z1; : : : ; Zn  are one-to-one, by the chain rule of entropy  and the Markovity,  H Z1; : : : ; Zn  = H X1; : : : ; Xn   n  Xk=1  =  H XkjX1; : : : ; Xk cid:0 1   n  Xk=2  = H X1  +  H XkjXk cid:0 1  = H X1  +  n  cid:0  1 H X2jX1  = log 3 +  3 2   n  cid:0  1 :  Alternatively, we can use the results of parts  d ,  e , and  f . Since Z1; : : : ; Zn are independent and Z2; : : : ; Zn are identically distributed with the probability distribution   1  2 ; 1  4 ; 1  4   ,  H Z1; : : : ; Zn  = H Z1  + H Z2  + : : : + H Zn   = H Z1  +  n  cid:0  1 H Z2  = log 3 +   n  cid:0  1 :  3 2   d  Since fXng is stationary with  cid:22 n =   1  3 ; 1  3 ; 1  H Xn  = H X1  = H     = log 3:  3   , 1 3  ;  1 3  ;  1 3  For n  cid:21  2 , Zn =8>< >:  0; 1; 2; Hence, H Zn  = H  1 2 ; 1  1 2; 1 4; 1 4:  4 ; 1  4   = 3 2 :   e  Due to the symmetry of P , P  ZnjZn cid:0 1  = P  Zn  for n  cid:21  2: Hence, H ZnjZn cid:0 1  =  H Zn  = 3 2 : Alternatively, using the result of part  f , we can trivially reach the same conclu- sion.   f  Let k  cid:21  2 . First observe that by the symmetry of P , Zk+1 = Xk+1  cid:0  Xk is  independent of Xk . Now that  H Zk+1jXk; Xk cid:0 1  = H Xk+1  cid:0  XkjXk; Xk cid:0 1   = H Xk+1  cid:0  XkjXk  = H Xk+1  cid:0  Xk  = H Zk+1 ;  Zk+1 is independent of  Xk; Xk cid:0 1  and hence independent of Zk = Xk  cid:0  Xk cid:0 1 . For k = 1 , again by the symmetry of P , Z2 is independent of Z1 = X1 trivially.   94  31. Markov.  Entropy Rates of a Stochastic Process  Let fXig  cid:24  Bernoulli p  . Consider the associated Markov chain fYign Yi =  the number of 1’s in the current run of 1’s  . For example, if X n = 101110 : : : , we have Y n = 101230 : : : .  i=1 where   a  Find the entropy rate of X n .  b  Find the entropy rate of Y n .  Solution: Markov solution.   a  For an i.i.d. source, H X   = H X  = H p  .  b  Observe that X n and Y n have a one-to-one mapping. Thus, H Y  = H X   =  H p  .  32. Time symmetry.  Let fXng be a stationary Markov process. We condition on  X0; X1  and look into the past and future. For what index k is  H X cid:0 njX0; X1  = H XkjX0; X1 ?  Give the argument.  Solution: Time symmetry. The trivial solution is k =  cid:0 n: To  cid:12 nd other possible values of k we expand  H X cid:0 njX0; X1  = H X cid:0 n; X0; X1   cid:0  H X0; X1    a    b   = H X cid:0 n  + H X0; X1jX cid:0 n   cid:0  H X0; X1  = H X cid:0 n  + H X0jX cid:0 n  + H X1jX0; X cid:0 n   cid:0  H X0; X1  = H X cid:0 n  + H X0jX cid:0 n  + H X1jX0   cid:0  H X0; X1  = H X cid:0 n  + H X0jX cid:0 n   cid:0  H X0  = H X0  + H X0jX cid:0 n   cid:0  H X0  = H XnjX0  = H XnjX0; X cid:0 1  = H Xn+1jX1; X0    c    e    d   where  a  and  d  come from Markovity and  b ;  c  and  e  come from stationarity. Hence k = n + 1 is also a solution. There are no other solution since for any other k, we can construct a periodic Markov process as a counterexample. Therefore k 2 f cid:0 n; n + 1g:   95   4.152    4.153    4.154    4.159    4.160    4.161   Entropy Rates of a Stochastic Process  33. Chain inequality: Let X1 ! X2 ! X3 ! X4 form a Markov chain. Show that  I X1; X3  + I X2; X4   cid:20  I X1; X4  + I X2; X3   Solution: Chain inequality X1 ! X2 ! X3 ! X4 +I X2; X3   cid:0  I X1; X3   cid:0  I X2; X4  I X1; X4   = H X1   cid:0  H X1jX4  + H X2   cid:0  H X2jX3   cid:0   H X1   cid:0  H X1jX3     cid:0  H X2   cid:0  H X2jX4    = H X1jX3   cid:0  H X1jX4  + H X2jX4   cid:0  H X2jX3   4.155  = H X1; X2jX3   cid:0  H X2jX1; X3   cid:0  H X1; X2jX4  + H X2jX1; X4  4.156  +H X1; X2jX4   cid:0  H X1jX2; X4   cid:0  H X1; X2jX3  + H X1jX2; X3    4.157   4.158   =  cid:0 H X2jX1; X3  + H X2jX1; X4   cid:0  H X2jX1; X4   cid:0  H X2jX1; X3; X4  = I X2; X3jX1; X4   cid:21  0  where H X1jX2; X3  = H X1jX2; X4  by the Markovity of the random variables.  34. Broadcast channel. Let X ! Y !  Z; W   form a Markov chain, i.e., p x; y; z; w  =  p x p yjx p z; wjy  for all x; y; z; w . Show that  I X; Z  + I X; W    cid:20  I X; Y   + I Z; W     4.162   Solution: Broadcast Channel X ! Y !  Z; W   , hence by the data processing inequality, I X; Y    cid:21  I X;  Z; W    , and hence  I X : Y    +I Z; W    cid:0  I X; Z   cid:0  I X; W     cid:21  I X : Z; W   + I Z; W    cid:0  I X; Z   cid:0  I X; W   = H Z; W   + H X   cid:0  H X; W; Z  + H W   + H Z   cid:0  H W; Z    cid:0 H Z   cid:0  H X  + H X; Z    cid:0  H W    cid:0  H X  + H W; X  4.165   4.166   =  cid:0 H X; W; Z  + H X; Z  + H X; W    cid:0  H X  = H WjX   cid:0  H WjX; Z  = I W ; ZjX   cid:21  0   4.163    4.164    4.167    4.168    4.169   35. Concavity of second law. Let fXng1 cid:0 1 be a stationary Markov process. Show that  H XnjX0  is concave in n . Speci cid:12 cally show that H XnjX0   cid:0  H Xn cid:0 1jX0   cid:0   H Xn cid:0 1jX0   cid:0  H Xn cid:0 2jX0   =  cid:0 I X1; Xn cid:0 1jX0; Xn   4.170   4.171    cid:20  0   96  Entropy Rates of a Stochastic Process  Thus the second di cid:11 erence is negative, establishing that H XnjX0  is a concave func- tion of n .  Solution: Concavity of second law of thermodynamics Since X0 ! Xn cid:0 2 ! Xn cid:0 1 ! Xn is a Markov chain H XnjX0    cid:0 H Xn cid:0 1jX0   cid:0   H Xn cid:0 1jX0   cid:0  H Xn cid:0 2jX0    4.172  = H XnjX0   cid:0  H Xn cid:0 1jX0; X cid:0 1   cid:0   H Xn cid:0 1jX0; X cid:0 1   cid:0  H Xn cid:0 2jX0; X cid:0 1   4.173  = H XnjX0   cid:0  H XnjX1; X0   cid:0   H Xn cid:0 1jX0   cid:0  H Xn cid:0 1jX1; X0   4.174  = I X1; XnjX0   cid:0  I X1; Xn cid:0 1jX0  = H X1jX0   cid:0  H X1jXn; X0   cid:0  H X1jX0  + H X1jXn cid:0 1; X0  = H X1jXn cid:0 1; X0   cid:0  H X1jXn; X0  = H X1; Xn cid:0 1; Xn; X0   cid:0  H X1jXn; X0  =  cid:0 I X1; Xn cid:0 1jXn; X0   cid:20  0   4.178    4.180    4.176    4.179    4.175    4.177   where  4.173  and  4.178  follows from Markovity and  4.174  follows from stationarity of the Markov chain.  If we de cid:12 ne   4.181  then the above chain of inequalities implies that  cid:1 n  cid:0   cid:1 n cid:0 1  cid:20  0 , which implies that H XnjX0  is a concave function of n .   cid:1 n = H XnjX0   cid:0  H Xn cid:0 1jX0    Chapter 5  Data Compression  1. Uniquely decodable and instantaneous codes. Let L = Pm  be the ex- pected value of the 100th power of the word lengths associated with an encoding of the random variable X: Let L1 = min L over all instantaneous codes; and let L2 = min L over all uniquely decodable codes. What inequality relationship exists between L1 and L2?  i=1 pil100  i  Solution: Uniquely decodable and instantaneous codes.  L =  pin100  i  m  Xi=1  L1 =  L2 =  Instantaneous codes  min  min  L  L  Uniquely decodable codes   5.1    5.2    5.3   Since all instantaneous codes are uniquely decodable, we must have L2  cid:20  L1 . Any set of codeword lengths which achieve the minimum of L2 will satisfy the Kraft inequality and hence we can construct an instantaneous code with the same codeword lengths, and hence the same L . Hence we have L1  cid:20  L2 . From both these conditions, we must have L1 = L2 .  2. How many  cid:12 ngers has a Martian? Let  p1; : : : ; pm!: S = S1; : : : ; Sm  The Si ’s are encoded into strings from a D -symbol output alphabet in a uniquely de- codable manner. If m = 6 and the codeword lengths are  l1; l2; : : : ; l6  =  1; 1; 2; 3; 2; 3 ;  cid:12 nd a good lower bound on D: You may wish to explain the title of the problem.  Solution: How many  cid:12 ngers has a Martian?  97   98  Data Compression  Uniquely decodable codes satisfy Kraft’s inequality. Therefore  f  D  = D cid:0 1 + D cid:0 1 + D cid:0 2 + D cid:0 3 + D cid:0 2 + D cid:0 3  cid:20  1:   5.4   We have f  2  = 7=4 > 1 , hence D > 2 . We have f  3  = 26=27 < 1 . So a possible value of D is 3. Our counting system is base 10, probably because we have 10  cid:12 ngers. Perhaps the Martians were using a base 3 representation because they have 3  cid:12 ngers.  Maybe they are like Maine lobsters ?   3. Slackness in the Kraft inequality. An instantaneous code has word lengths l1; l2; : : : ; lm  which satisfy the strict inequality  D cid:0 li < 1:  m  Xi=1  The code alphabet is D = f0; 1; 2; : : : ; D  cid:0  1g: Show that there exist arbitrarily long sequences of code symbols in D cid:3  which cannot be decoded into sequences of codewords. Solution:  Slackness in the Kraft inequality. Instantaneous codes are pre cid:12 x free codes, i.e., no codeword is a pre cid:12 x of any other codeword. Let nmax = maxfn1; n2; :::; nqg: There are Dnmax sequences of length nmax . Of these sequences, Dnmax cid:0 ni start with the i -th codeword. Because of the pre cid:12 x condition no two sequences can start with the same codeword. Hence the total number of sequences which start with some codeword i=1 D cid:0 ni < Dnmax . Hence there are sequences which do not start with any codeword. These and all longer sequences with these length nmax sequences as pre cid:12 xes cannot be decoded.  This situation can be visualized with the aid of a tree.   i=1 Dnmax cid:0 ni = DnmaxPq  is Pq  Alternatively, we can map codewords onto dyadic intervals on the real line correspond- ing to real numbers whose decimal expansions start with that codeword. Since the  length of the interval for a codeword of length ni is D cid:0 ni , and P D cid:0 ni < 1 , there ex-  ists some interval s  not used by any codeword. The binary sequences in these intervals do not begin with any codeword and hence cannot be decoded.  4. Hu cid:11 man coding. Consider the random variable  X =  x1  0:49 0:26 0:12 0:04 0:04 0:03 0:02 !  x2  x3  x4  x5  x6  x7   a  Find a binary Hu cid:11 man code for X:   b  Find the expected codelength for this encoding.   c  Find a ternary Hu cid:11 man code for X:  Solution: Examples of Hu cid:11 man codes.   Data Compression  99   a  The Hu cid:11 man tree for this distribution is   b  The expected length of the codewords for the binary Hu cid:11 man code is 2.02 bits.    H X  = 2:01 bits    c  The ternary Hu cid:11 man tree is  Codeword 1 00 011 01000 01001 01010 01011  Codeword 0 1 20 22 210 211 212  x1 x2 x3 x4 x5 x6 x7  x1 x2 x3 x4 x5 x6 x7  0.49 0.26 0.12 0.04 0.04 0.03 0.02  0.49 0.26 0.12 0.04 0.04 0.03 0.02  1  0.51 0.49  0.49 0.26 0.25  0.49 0.26 0.13 0.12  0.49 0.26 0.12 0.08 0.05  0.49 0.26 0.12 0.05 0.04 0.04  1.0  0.49 0.26 0.25  0.49 0.26 0.12 0.09 0.04  This code has an expected length 1.34 ternary symbols.   H3 X  = 1:27 ternary symbols .  5. More Hu cid:11 man codes. Find the binary Hu cid:11 man code for the source with probabilities  1=3; 1=5; 1=5; 2=15; 2=15  . Argue that this code is also optimal for the source with probabilities  1=5; 1=5; 1=5; 1=5; 1=5 :  5 ; 1  5 ; 2  15 ; 2  15   has codewords f00,10,11,010,011g.  Solution: More Hu cid:11 man codes. The Hu cid:11 man code for the source with probabilities 3 ; 1   1 To show that this code  *  is also optimal for  1 5, 1 5, 1 5, 1 5, 1 5  we have to show that it has minimum expected length, that is, no shorter code can be constructed without violating H X   cid:20  EL .  Since  H X  = log 5 = 2:32 bits.  E L  cid:3    = 2  cid:2   + 3  cid:2   3 5  =  bits.  E L any code   =  =  bits  2 5  li 5  5  Xi=1  12 5  k 5   5.5    5.6    5.7   for some integer k , the next lowest possible value of E L  is 11 5 = 2.2 bits < 2.32 bits. Hence  *  is optimal.  Note that one could also prove the optimality of  *  by showing that the Hu cid:11 man code for the  1 5, 1 5, 1 5, 1 5, 1 5  source has average length 12 5 bits.  Since each   100  Data Compression  Hu cid:11 man code produced by the Hu cid:11 man encoding algorithm is optimal, they all have the same average length.   6. Bad codes. Which of these codes cannot be Hu cid:11 man codes for any probability as-  signment?   a  f0; 10; 11g:  b  f00; 01; 10; 110g:  c  f01; 10g: Solution: Bad codes   a  f0,10,11g is a Hu cid:11 man code for the distribution  1 2,1 4,1 4 .  b  The code f00,01,10, 110g can be shortened to f00,01,10, 11g without losing its instantaneous property, and therefore is not optimal, so it cannot be a Hu cid:11 man code. Alternatively, it is not a Hu cid:11 man code because there is a unique longest codeword.   c  The code f01,10g can be shortened to f0,1g without losing its instantaneous prop-  erty, and therefore is not optimal and not a Hu cid:11 man code.  7. Hu cid:11 man 20 questions. Consider a set of n objects. Let Xi = 1 or 0 accordingly as the i-th object is good or defective. Let X1; X2; : : : ; Xn be independent with PrfXi = 1g = pi ; and p1 > p2 > : : : > pn > 1=2 : We are asked to determine the set of all defective objects. Any yes-no question you can think of is admissible.   a  Give a good lower bound on the minimum average number of questions required.   b  If the longest sequence of questions is required by nature’s answers to our questions, what  in words  is the last question we should ask? And what two sets are we distinguishing with this question? Assume a compact  minimum average length  sequence of questions.   c  Give an upper bound  within 1 question  on the minimum average number of  questions required.  Solution: Hu cid:11 man 20 Questions.   a  We will be using the questions to determine the sequence X1; X2; : : : ; Xn , where Xi is 1 or 0 according to whether the i -th object is good or defective. Thus the i=1 pi , and the least likely i=1 1  cid:0  pi  . Since the optimal set of questions corresponds to a Hu cid:11 man code for the source, a good lower bound on the average number of questions is the entropy of the sequence X1; X2; : : : ; Xn . But since the Xi ’s are independent Bernoulli random variables, we have  most likely sequence is all 1’s, with a probability of Qn sequence is the all 0’s sequence with probability Qn  EQ  cid:21  H X1; X2; : : : ; Xn  =X H Xi  =X H pi :   5.8    Data Compression  101   b  The last bit in the Hu cid:11 man code distinguishes between the least likely source symbols.  By the conditions of the problem, all the probabilities are di cid:11 erent, and thus the two least likely sequences are uniquely de cid:12 ned.  In this case, the two least likely sequences are 000 : : : 00 and 000 : : : 01 , which have probabilities  1 cid:0  p1  1 cid:0  p2  : : :  1 cid:0  pn  and  1 cid:0  p1  1 cid:0  p2  : : :  1 cid:0  pn cid:0 1 pn respectively. Thus the last question will ask \Is Xn = 1 ", i.e., \Is the last item defective?".   c  By the same arguments as in Part  a , an upper bound on the minimum average number of questions is an upper bound on the average length of a Hu cid:11 man code,  namely H X1; X2; : : : ; Xn  + 1 =P H pi  + 1 .  process U1; U2; : : : ; having transition matrix  8. Simple optimum compression of a Markov source. Consider the 3-state Markov  Un cid:0 1nUn  S1 S2 S3  S1 1 2 1 4 0  S2 1 4 1 2 1 2  S3 1 4 1 4 1 2  Thus the probability that S1 follows S3 is equal to zero. Design 3 codes C1; C2; C3  one for each state 1; 2 and 3  , each code mapping elements of the set of Si ’s into sequences of 0’s and 1’s, such that this Markov process can be sent with maximal compression by the following scheme:   a  Note the present symbol Xn = i .  b  Select code Ci:  c  Note the next symbol Xn+1 = j and send the codeword in Ci corresponding to  j .   d  Repeat for the next symbol.  What is the average message length of the next symbol conditioned on the previous state Xn = i using this coding scheme? What is the unconditional average number of bits per source symbol? Relate this to the entropy rate H U  of the Markov chain.  Solution: Simple optimum compression of a Markov source.  It is easy to design an optimal code for each state. A possible solution is  Next state S1 S2 S3 Code C1 code C2 code C3  10 0 0  0 10 -  11 E LjC1  = 1.5 bits symbol 11 E LjC2  = 1.5 bits symbol 1 E LjC3  = 1 bit symbol  The average message lengths of the next symbol conditioned on the previous state being Si are just the expected lengths of the codes Ci . Note that this code assignment achieves the conditional entropy lower bound.   102  Data Compression  To  cid:12 nd the unconditional average, we have to  cid:12 nd the stationary distribution on the states. Let  cid:22  be the stationary distribution. Then  We can solve this to  cid:12 nd that  cid:22  =  2=9; 4=9; 1=3  . Thus the unconditional average number of bits per source symbol   cid:22  =  cid:22 2 64  1=2 1=4 1=4 1=4 1=2 1=4 1=2 1=2 0  3 75  EL =   cid:22 iE LjCi   3  Xi=1 2 9  cid:2  1:5 + 4 3  bits symbol:  =  =  4 9  cid:2  1:5 +  1 3  cid:2  1  H = H X2jX1   3  Xi=1  =   cid:22 iH X2jX1 = Si   = 4=3 bits symbol:   5.9    5.10    5.11    5.12    5.13    5.14    5.15   The entropy rate H of the Markov chain is  Thus the unconditional average number of bits per source symbol and the entropy rate H of the Markov chain are equal, because the expected length of each code Ci equals the entropy of the state after state i , H X2jX1 = Si  , and thus maximal compression is obtained.  9. Optimal code lengths that require one bit above entropy. The source coding theorem shows that the optimal code for a random variable X has an expected length less than H X  + 1 . Give an example of a random variable for which the expected length of the optimal code is close to H X  + 1 , i.e., for any  cid:15  > 0 , construct a distribution for which the optimal code has L > H X  + 1  cid:0   cid:15  . Solution: Optimal code lengths that require one bit above entropy. There is a trivial example that requires almost 1 bit above its entropy. Let X be a binary random variable with probability of X = 1 close to 1. Then entropy of X is close to 0 , but the length of its optimal code is 1 bit, which is almost 1 bit above its entropy.  10. Ternary codes that achieve the entropy bound. A random variable X takes on m values and has entropy H X  . An instantaneous ternary code is found for this source, with average length  L =  H X  log2 3  = H3 X :   5.16    Data Compression  103   a  Show that each symbol of X has a probability of the form 3 cid:0 i for some i .   b  Show that m is odd.  Solution: Ternary codes that achieve the entropy bound.   a  We will argue that an optimal ternary code that meets the entropy bound corre- sponds to complete ternary tree, with the probability of each leaf of the form 3 cid:0 i . To do this, we essentially repeat the arguments of Theorem 5.3.1. We achieve the ternary entropy bound only if D pjjr  = 0 and c = 1 , in  5.25 . Thus we achieve the entropy bound if and only if pi = 3 cid:0 j for all i .   b  We will show that any distribution that has pi = 3 cid:0 li  for all i must have an odd number of symbols. We know from Theorem 5.2.1, that given the set of lengths, li , we can construct a ternary tree with nodes at the depths li . Now,  since P 3 cid:0 li = 1 , the tree must be complete. A complete ternary tree has an  odd number of leaves  this can be proved by induction on the number of internal nodes . Thus the number of source symbols is odd. Another simple argument is to use basic number theory. We know that for  this distribution, P 3 cid:0 li = 1 . We can write this as 3 cid:0 lmaxP 3lmax cid:0 li = 1 or P 3lmax cid:0 li = 3lmax . Each of the terms in the sum is odd, and since their sum is  odd, the number of terms in the sum has to be odd  the sum of an even number of odd terms is even . Thus there are an odd number of source symbols for any code that meets the ternary entropy bound.  11. Su cid:14 x condition. Consider codes that satisfy the su cid:14 x condition, which says that no codeword is a su cid:14 x of any other codeword. Show that a su cid:14 x condition code is uniquely decodable, and show that the minimum average length over all codes satisfying the su cid:14 x condition is the same as the average length of the Hu cid:11 man code for that random variable.  Solution: Su cid:14 x condition. The fact that the codes are uniquely decodable can be seen easily be reversing the order of the code. For any received sequence, we work backwards from the end, and look for the reversed codewords. Since the codewords satisfy the su cid:14 x condition, the reversed codewords satisfy the pre cid:12 x condition, and the we can uniquely decode the reversed code.  The fact that we achieve the same minimum expected length then follows directly from the results of Section 5.5. But we can use the same reversal argument to argue that corresponding to every su cid:14 x code, there is a pre cid:12 x code of the same length and vice versa, and therefore we cannot achieve any lower codeword lengths with a su cid:14 x code than we can with a pre cid:12 x code.  12. Shannon codes and Hu cid:11 man codes. Consider a random variable X which takes  on four values with probabilities   1  3 ; 1  3 ; 1  4 ; 1  12   .   a  Construct a Hu cid:11 man code for this random variable.   104  Data Compression   b  Show that there exist two di cid:11 erent sets of optimal lengths for the codewords, namely, show that codeword length assignments  1; 2; 3; 3  and  2; 2; 2; 2  are both optimal.   c  Conclude that there are optimal codes with codeword lengths for some symbols  that exceed the Shannon code length dlog 1  p x e .  Solution: Shannon codes and Hu cid:11 man codes.   a  Applying the Hu cid:11 man algorithm gives us the following table  Code Symbol Probability  0 11 101 100  1 2 3 4  1=3 1=3 1=4 1=12  1  2=3 1=3  1=3 1=3 1=3  which gives codeword lengths of 1,2,3,3 for the di cid:11 erent codewords.   b  Both set of lengths 1,2,3,3 and 2,2,2,2 satisfy the Kraft inequality, and they both achieve the same expected length  2 bits  for the above distribution. Therefore they are both optimal.   c  The symbol with probability 1=4 has an Hu cid:11 man code of length 3, which is greater than dlog 1 pe . Thus the Hu cid:11 man code for a particular symbol may be longer than the Shannon code for that symbol. But on the average, the Hu cid:11 man code cannot be longer than the Shannon code.  13. Twenty questions. Player A chooses some object in the universe, and player B attempts to identify the object with a series of yes-no questions. Suppose that player B is clever enough to use the code achieving the minimal expected length with respect to player A’s distribution. We observe that player B requires an average of 38.5 questions to determine the object. Find a rough lower bound to the number of objects in the universe.  Solution: Twenty questions.  37:5 = L cid:3   cid:0  1 < H X   cid:20  log jXj   5.17   and hence number of objects in the universe > 237:5 = 1:94  cid:2  1011:  14. Hu cid:11 man code. Find the  a  binary and  b  ternary Hu cid:11 man codes for the random  variable X with probabilities  p =    1 21  ;  2 21  ;  3 21  ;  4 21  ;  5 21  ;  6 21    :   c  Calculate L =P pili in each case.  Solution: Hu cid:11 man code.   Data Compression  105   a  The Hu cid:11 man tree for this distribution is   b  The ternary Hu cid:11 man tree is  Codeword 00 10 11 010 0110 0111  Codeword 1 2 00 01 020 021 022  6=21 5=21 4=21 3=21 3=21  6=21 5=21 4=21 3=21 3=21  x1 x2 x3 x4 x5 x6  x1 x2 x3 x4 x5 x6 x7  6=21 5=21 4=21 3=21 2=21 1=21  6=21 5=21 4=21 3=21 2=21 1=21 0=21  1  12=21 9=21  9=21 6=21 6=21  6=21 6=21 5=21 4=21  1  10=21 6=21 5=21   c  The expected length of the codewords for the binary Hu cid:11 man code is 51=21 = 2:43  bits. The ternary code has an expected length of 34=21 = 1:62 ternary symbols.  15. Hu cid:11 man codes.   a  Construct a binary Hu cid:11 man code for the following distribution on 5 symbols p =   0:3; 0:3; 0:2; 0:1; 0:1  . What is the average length of this code?   b  Construct a probability distribution p0 on 5 symbols for which the code that you constructed in part  a  has an average length  under p0   equal to its entropy H p0  .  Solution: Hu cid:11 man codes   a  The code constructed by the standard Hu cid:11 man procedure  Codeword X Probability 10 11 00 010 011 The average length = 2  cid:3  0:8 + 3  cid:3  0:2 = 2:2 bits symbol.  0.3 0.3 0.2 0.1 0.1  0.3 0.3 0.2 0.2  0.4 0.3 0.3  1 2 3 4 5  0.6 0.4  1   b  The code would have a rate equal to the entropy if each of the codewords was of length 1=p X  . In this case, the code constructed above would be e cid:14 cient for the distrubution  0.25.0.25,0.25,0.125,0.125 .  16. Hu cid:11 man codes: Consider a random variable X which takes 6 values fA; B; C; D; E; Fg  with probabilities  0:5; 0:25; 0:1; 0:05; 0:05; 0:05  respectively.   106  length?  Data Compression   a  Construct a binary Hu cid:11 man code for this random variable. What is its average   b  Construct a quaternary Hu cid:11 man code for this random variable, i.e., a code over an alphabet of four symbols  call them a; b; c and d  . What is the average length of this code?   c  One way to construct a binary code for the random variable is to start with a quaternary code, and convert the symbols into binary using the mapping a ! 00 , b ! 01 , c ! 10 and d ! 11 . What is the average length of the binary code for the above random variable constructed by this process?   d  For any random variable X , let LH be the average length of the binary Hu cid:11 man code for the random variable, and let LQB be the average length code constructed by  cid:12 rst building a quaternary Hu cid:11 man code and converting it to binary. Show that  LH  cid:20  LQB < LH + 2   5.18    e  The lower bound in the previous example is tight. Give an example where the code constructed by converting an optimal quaternary code is also the optimal binary code.   f  The upper bound, i.e., LQB < LH + 2 is not tight. In fact, a better bound is LQB  cid:20  LH + 1 . Prove this bound, and provide an example where this bound is tight.  Solution: Hu cid:11 man codes: Consider a random variable X which takes 6 values fA; B; C; D; E; Fg with probabilities  0:5; 0:25; 0:1; 0:05; 0:05; 0:05  respectively.   a  Construct a binary Hu cid:11 man code for this random variable. What is its average  length? Solution:  Code Source symbol Prob. 0 10 1100 1101 1110 1111 The average length of this code is 1 cid:2 0:5+2 cid:2 0:25+4 cid:2  0:1+0:05+0:05+0:05  = 2 bits. The entropy H X  in this case is 1:98 bits.  0.5 0.25 0.1 0.05 0.05 0.05  0.5 0.25 0.1 0.1 0.05  0.5 0.25 0.15 0.1  0.5 0.25 0.25  A B C D E F  0.5 0.5  1.0   b  Construct a quaternary Hu cid:11 man code for this random variable, i.e., a code over an alphabet of four symbols  call them a; b; c and d  . What is the average length of this code? Solution:Since the number of symbols, i.e., 6 is not of the form 1 + k D  cid:0  1  , we need to add a dummy symbol of probability 0 to bring it to this form. In this case, drawing up the Hu cid:11 man tree is straightforward.   Data Compression  107  1.0  A B C D E F G  0.5 0.25 0.15 0.1  0.5 0.25 0.1 0.05 0.05 0.05 0.0  Code Symbol Prob. a b d ca cb cc cd The average length of this code is 1  cid:2  0:85 + 2  cid:2  0:15 = 1:15 quaternary symbols.  c  One way to construct a binary code for the random variable is to start with a quaternary code, and convert the symbols into binary using the mapping a ! 00 , b ! 01 , c ! 10 and d ! 11 . What is the average length of the binary code for the above random variable constructed by this process? Solution:The code constructed by the above process is A ! 00 , B ! 01 , C ! 11 , D ! 1000 , E ! 1001 , and F ! 1010 , and the average length is 2  cid:2  0:85 + 4  cid:2  0:15 = 2:3 bits.   d  For any random variable X , let LH be the average length of the binary Hu cid:11 man code for the random variable, and let LQB be the average length code constructed by  cid:12 rsting building a quaternary Hu cid:11 man code and converting it to binary. Show that  LH  cid:20  LQB < LH + 2   5.19   Solution:Since the binary code constructed from the quaternary code is also in- stantaneous, its average length cannot be better than the average length of the best instantaneous code, i.e., the Hu cid:11 man code. That gives the lower bound of the inequality above. To prove the upper bound, the LQ be the length of the optimal quaternary code. Then from the results proved in the book, we have  H4 X   cid:20  LQ < H4 X  + 1  Also, it is easy to see that LQB = 2LQ , since each symbol in the quaternary code is converted into two bits. Also, from the properties of entropy, it follows that H4 X  = H2 X =2 . Substituting these in the previous equation, we get  H2 X   cid:20  LQB < H2 X  + 2:  Combining this with the bound that H2 X   cid:20  LH , we obtain LQB < LH + 2 .   e  The lower bound in the previous example is tight. Give an example where the code constructed by converting an optimal quaternary code is also the optimal binary code? Solution:Consider a random variable that takes on four equiprobable values. Then the quaternary Hu cid:11 man code for this is 1 quaternary symbol for each source symbol, with average length 1 quaternary symbol. The average length LQB for this code is then 2 bits. The Hu cid:11 man code for this case is also easily seen to assign 2 bit codewords to each symbol, and therefore for this case, LH = LQB .   5.20    5.21    108  Data Compression   f   Optional, no credit  The upper bound, i.e., LQB < LH + 2 is not tight. In fact, a better bound is LQB  cid:20  LH + 1 . Prove this bound, and provide an example where this bound is tight. Solution:Consider a binary Hu cid:11 man code for the random variable X and consider all codewords of odd length. Append a 0 to each of these codewords, and we will obtain an instantaneous code where all the codewords have even length. Then we can use the inverse of the mapping mentioned in part  c  to construct a quaternary code for the random variable - it is easy to see that the quatnerary code is also instantaneous. Let LBQ be the average length of this quaternary code. Since the length of the quaternary codewords of BQ are half the length of the corresponding binary codewords, we have  LBQ =  1  20 @LH + Xi:li is odd  pi1 A <  LH + 1  2  and since the BQ code is at best as good as the quaternary Hu cid:11 man code, we have  LBQ  cid:21  LQ  Therefore LQB = 2LQ  cid:20  2LBQ < LH + 1 . An example where this upper bound is tight is the case when we have only two possible symbols. Then LH = 1 , and LQB = 2 .  17. Data compression. Find an optimal set of binary codeword lengths l1; l2; : : :  min-  imizing P pili   for an instantaneous code for each of the following probability mass  functions:   5.22    5.23    a  p =   10  b  p =   9  41 ; 9 10 ;   9  41 ; 8 41 ; 7 41 ; 7 41   10    1 10  ;   9 10    1  10  2;   9  10    1  10  3; : : :   Solution: Data compression   a   Code Source symbol Prob. 10 41 10 9 41 00 01 8 41 7 41 110 111 7 41  A B C D E  41 41  24 41 17 41  17 41 14 41 10 41  14 41 10 41 9 41 8 41   b  This is case of an Hu cid:11 man code on an in cid:12 nite alphabet. If we consider an initial subset of the symbols, we can see that the cumulative probability of all symbols  fx : x > ig is Pj>i 0:9  cid:3   0:1 j cid:0 1 = 0:9 0:1 i cid:0 1 1= 1  cid:0  0:1   =  0:1 i cid:0 1 . Since this is less than 0:9  cid:3   0:1 i cid:0 1 , the cumulative sum of all the remaining terms is less than the last term used. Thus Hu cid:11 man coding will always merge the last two terms. This in terms implies that the Hu cid:11 man code in this case is of the form 1,01,001,0001, etc.   Data Compression  109  18. Classes of codes. Consider the code f0; 01g   a  Is it instantaneous?   b  Is it uniquely decodable?   c  Is it nonsingular?  Solution: Codes.  second codeword, 01.   a  No, the code is not instantaneous, since the  cid:12 rst codeword, 0, is a pre cid:12 x of the   b  Yes, the code is uniquely decodable. Given a sequence of codewords,  cid:12 rst isolate  occurrences of 01  i.e.,  cid:12 nd all the ones  and then parse the rest into 0’s.   c  Yes, all uniquely decodable codes are non-singular.  19. The game of Hi-Lo.   a  A computer generates a number X according to a known probability mass function p x ; x 2 f1; 2; : : : ; 100g . The player asks a question, \Is X = i ?" and is told \Yes", \You’re too high," or \You’re too low." He continues for a total of six questions. If he is right  i.e., he receives the answer \Yes"  during this sequence, he receives a prize of value v X : How should the player proceed to maximize his expected winnings?   b  The above doesn’t have much to do with information theory. Consider the fol- lowing variation: X  cid:24  p x ; prize = v x  , p x  known, as before. But arbitrary Yes-No questions are asked sequentially until X is determined.  \Determined" doesn’t mean that a \Yes" answer is received.  Questions cost one unit each. How should the player proceed? What is the expected payo cid:11 ?   c  Continuing  b , what if v x  is  cid:12 xed, but p x  can be chosen by the computer  and then announced to the player ? The computer wishes to minimize the player’s expected return. What should p x  be? What is the expected return to the player?  Solution: The game of Hi-Lo.   a  The  cid:12 rst thing to recognize in this problem is that the player cannot cover more than 63 values of X with 6 questions. This can be easily seen by induction. With one question, there is only one value of X that can be covered. With two questions, there is one value of X that can be covered with the  cid:12 rst question, and depending on the answer to the  cid:12 rst question, there are two possible values of X that can be asked in the next question. By extending this argument, we see that we can ask at more 63 di cid:11 erent questions of the form \Is X = i ?" with 6 questions.  The fact that we have narrowed the range at the end is irrelevant, if we have not isolated the value of X .  Thus if the player seeks to maximize his return, he should choose the 63 most valuable outcomes for X , and play to isolate these values. The probabilities are   110  Data Compression  irrelevant to this procedure. He will choose the 63 most valuable outcomes, and his  cid:12 rst question will be \Is X = i ?" where i is the median of these 63 numbers. After isolating to either half, his next question will be \Is X = j ?", where j is the median of that half. Proceeding this way, he will win if X is one of the 63 most valuable outcomes, and lose otherwise. This strategy maximizes his expected winnings.   b  Now if arbitrary questions are allowed, the game reduces to a game of 20 questions  to determine the object. The return in this case to the player is Px p x  v x   cid:0   l x   , where l x  is the number of questions required to determine the object. Maximizing the return is equivalent to minimizing the expected number of ques- tions, and thus, as argued in the text, the optimal strategy is to construct a Hu cid:11 man code for the source and use that to construct a question strategy. His   c  A computer wishing to minimize the return to player will want to minimize  expected return is therefore between P p x v x   cid:0  H and P p x v x   cid:0  H  cid:0  1 . P p x v x   cid:0  H X  over choices of p x  . We can write this as a standard mini-  mization problem with constraints. Let  J p  =X pivi +X pi log pi +  cid:21 X pi  and di cid:11 erentiating and setting to 0, we obtain  vi + log pi + 1 +  cid:21  = 0  or after normalizing to ensure that the pi ’s form a probability distribution,  pi =  2 cid:0 vi  :  Pj 2 cid:0 vj Pj 2 cid:0 vj , and rewrite the return as  To complete the proof, we let ri = 2 cid:0 vi  X pivi +X pi log pi = X pi log pi  cid:0 X pi log 2 cid:0 vi = X pi log pi  cid:0 X pi log ri  cid:0  log X 2 cid:0 vj    5.28  = D pjjr   cid:0  log X 2 cid:0 vj  ;   5.29    5.27   and thus the return is minimized by choosing pi = ri . This is the distribution that the computer must choose to minimize the return to the player.   5.24    5.25    5.26   20. Hu cid:11 man codes with costs. Words like Run! Help! and Fire! are short, not because they are frequently used, but perhaps because time is precious in the situations in which these words are required. Suppose that X = i with probability pi; i = 1; 2; : : : ; m: Let li be the number of binary symbols in the codeword associated with X = i; and let ci denote the cost per letter of the codeword when X = i: Thus the average cost C of  the description of X is C =Pm  i=1 picili:   Data Compression  111   a  Minimize C over all l1; l2; : : : ; lm such that P 2 cid:0 li  cid:20  1: Ignore any implied in-  teger constraints on li: Exhibit the minimizing l cid:3 1; l cid:3 2; : : : ; l cid:3 m and the associated minimum value C cid:3 :   b  How would you use the Hu cid:11 man code procedure to minimize C over all uniquely  decodable codes? Let CHuf f man denote this minimum.   c  Can you show that  C cid:3   cid:20  CHuf f man  cid:20  C cid:3  +  pici?  m  Xi=1  Solution: Hu cid:11 man codes with costs.   a  We wish to minimize C = P picini subject to P 2 cid:0 ni  cid:20  1 . We will assume equality in the constraint and let ri = 2 cid:0 ni and let Q = Pi pici . Let qi =   pici =Q . Then q also forms a probability distribution and we can write C as  C = X picini = QX qi log = Q cid:18 X qi log = Q D qjjr  + H q  :  1 ri  qi  ri  cid:0 X qi log qi cid:19   Since the only freedom is in the choice of ri , we can minimize C by choosing r = q or  where we have ignored any integer constraints on ni . The minimum cost C cid:3  for this assignment of codewords is  n cid:3 i =  cid:0  log  ;  pici  P pjcj  C cid:3  = QH q    b  If we use q instead of p for the Hu cid:11 man procedure, we obtain a code minimizing   c  Now we can account for the integer constraints.  expected cost.  Let  Then  Multiplying by pici and summing over i , we get the relationship  ni = d cid:0  log qie   cid:0  log qi  cid:20  ni <  cid:0  log qi + 1  C cid:3   cid:20  CHuf f man < C cid:3  + Q:   5.30    5.31    5.32    5.33    5.34    5.35    5.36    5.37    5.38    112  Data Compression  21. Conditions for unique decodability. Prove that a code C is uniquely decodable if   and only if  the extension  C k x1; x2; : : : ; xk  = C x1 C x2  cid:1  cid:1  cid:1  C xk   is a one-to-one mapping from X k to D cid:3  for every k  cid:21  1 .  The only if part is obvious.  Solution: Conditions for unique decodability. If C k is not one-to-one for some k , then C is not UD, since there exist two distinct sequences,  x1; : : : ; xk  and  x01; : : : ; x0k  such that  C k x1; : : : ; xk  = C x1  cid:1  cid:1  cid:1  C xk  = C x01  cid:1  cid:1  cid:1  C x0k  = C x01; : : : ; x0k  :  Conversely, if C is not UD then by de cid:12 nition there exist distinct sequences of source symbols,  x1; : : : ; xi  and  y1; : : : ; yj  , such that  C x1 C x2  cid:1  cid:1  cid:1  C xi  = C y1 C y2  cid:1  cid:1  cid:1  C yj  :  Concatenating the input sequences  x1; : : : ; xi  and  y1; : : : ; yj  , we obtain  C x1  cid:1  cid:1  cid:1  C xi C y1  cid:1  cid:1  cid:1  C yj  = C y1  cid:1  cid:1  cid:1  C yj C x1  cid:1  cid:1  cid:1  C xi  ;  which shows that C k is not one-to-one for k = i + j .  22. Average length of an optimal code. Prove that L p1; : : : ; pm  , the average code- word length for an optimal D -ary pre cid:12 x code for probabilities fp1; : : : ; pmg , is a con- tinuous function of p1; : : : ; pm . This is true even though the optimal code changes discontinuously as the probabilities vary.  Solution: Average length of an optimal code. The longest possible codeword in an optimal code has n cid:0 1 binary digits. This corresponds to a completely unbalanced tree in which each codeword has a di cid:11 erent length. Using a D -ary alphabet for codewords can only decrease its length. Since we know the maximum possible codeword length, there are only a  cid:12 nite number of possible codes to consider. For each candidate code C , the average codeword length is determined by the probability distribution p1; p2; : : : ; pn :  L C  =  pi‘i:  n  Xi=1  This is a linear, and therefore continuous, function of p1; p2; : : : ; pn . The optimal code is the candidate code with the minimum L , and its length is the minimum of a  cid:12 nite number of continuous functions and is therefore itself a continuous function of p1; p2; : : : ; pn .  23. Unused code sequences. Let C be a variable length code that satis cid:12 es the Kraft  inequality with equality but does not satisfy the pre cid:12 x condition.   a  Prove that some  cid:12 nite sequence of code alphabet symbols is not the pre cid:12 x of any  sequence of codewords.   b   Optional  Prove or disprove: C has in cid:12 nite decoding delay.   Data Compression  113  Solution: Unused code sequences. Let C be a variable length code that satis cid:12 es the Kraft inequality with equality but does not satisfy the pre cid:12 x condition.   a  When a pre cid:12 x code satis cid:12 es the Kraft inequality with equality, every  in cid:12 nite  sequence of code alphabet symbols corresponds to a sequence of codewords, since the probability that a random generated sequence begins with a codeword is  If the code does not satisfy the pre cid:12 x condition, then at least one codeword, say C x1  , is a pre cid:12 x of another, say C xm  . Then the probability that a random generated sequence begins with a codeword is at most  D cid:0 ‘i = 1 :  m  Xi=1  m cid:0 1  Xi=1  D cid:0 ‘i  cid:20  1  cid:0  D cid:0 ‘m < 1 ;  which shows that not every sequence of code alphabet symbols is the beginning of a sequence of codewords.   b   Optional  A reference to a paper proving that C has in cid:12 nite decoding delay will be supplied later. It is easy to see by example that the decoding delay cannot be  cid:12 nite. An simple example of a code that satis cid:12 es the Kraft inequality, but not the pre cid:12 x condition is a su cid:14 x code  see problem 11 . The simplest non-trivial su cid:14 x code is one for three symbols f0; 01; 11g . For such a code, consider decoding a string 011111 : : : 1110. If the number of one’s is even, then the string must be parsed 0,11,11, : : : ,11,0, whereas if the number of 1’s is odd, the string must be parsed 01,11, : : : ,11. Thus the string cannot be decoded until the string of 1’s has ended, and therefore the decoding delay could be in cid:12 nite.  24. Optimal codes for uniform distributions. Consider a random variable with m equiprobable outcomes. The entropy of this information source is obviously log 2 m bits.   a  Describe the optimal instantaneous binary code for this source and compute the   b  For what values of m does the average codeword length Lm equal the entropy  average codeword length Lm .  H = log2 m ?   c  We know that L < H + 1 for any probability distribution. The redundancy of a variable length code is de cid:12 ned to be  cid:26  = L  cid:0  H . For what value s  of m , where 2k  cid:20  m  cid:20  2k+1 , is the redundancy of the code maximized? What is the limiting value of this worst case redundancy as m ! 1 ?  Solution: Optimal codes for uniform distributions.   a  For uniformly probable codewords, there exists an optimal binary variable length pre cid:12 x code such that the longest and shortest codewords di cid:11 er by at most one bit.   114  Data Compression  If two codes di cid:11 er by 2 bits or more, call ms the message with the shorter codeword Cs and m‘ the message with the longer codeword C‘ . Change the codewords for these two messages so that the new codeword C0s is the old Cs with a zero appended  C0s = Cs0  and C0‘ is the old Cs with a one appended  C0‘ = Cs1  . C0s and C0‘ are legitimate codewords since no other codeword contained Cs as a pre cid:12 x  by de cid:12 nition of a pre cid:12 x code , so obviously no other codeword could contain C 0s or C0‘ as a pre cid:12 x. The length of the codeword for ms increases by 1 and the length of the codeword for m‘ decreases by at least 1. Since these messages are equally likely, L0  cid:20  L . By this method we can transform any optimal code into a code in which the length of the shortest and longest codewords di cid:11 er by at most one bit.  In fact, it is easy to see that every optimal code has this property.  For a source with n messages, ‘ ms  = blog2 nc and ‘ m‘  = dlog2 ne . Let d be the di cid:11 erence between n and the next smaller power of 2:  d = n  cid:0  2blog2 nc :  Then the optimal code has 2d codewords of length dlog 2 ne and n cid:0 2d codewords of length blog2 nc . This gives 1 n 1 n   2ddlog2 ne +  n  cid:0  2d blog2 nc   nblog2 nc + 2d   L =  =  = blog2 nc +  2d n  :  Note that d = 0 is a special case in the above equation.   b  The average codeword length equals the entropy if and only if n is a power of 2.  To see this, consider the following calculation of L :  L =Xi  pi‘i =  cid:0 Xi  pi log2 2 cid:0 ‘i = H + D pkq  ;  where qi = 2 cid:0 ‘i . Therefore L = H only if pi = qi , that is, when all codewords have equal length, or when d = 0 .   c  For n = 2m + d , the redundancy r = L  cid:0  H is given by  r = L  cid:0  log2 n = blog2 nc + = m +  = m +  2d n  cid:0  log2 n 2d n  cid:0  log2 2m + d  2d 2m + d  cid:0   ln 2  ln 2m + d   :  1  @r @d  =   2m + d  2   cid:0  2d   2m + d 2  1 ln 2  cid:1    cid:0   2m + d  Therefore   Data Compression  115  Setting this equal to zero implies d cid:3  = 2m 2 ln 2  cid:0  1  . Since there is only one maximum, and since the function is convex \ , the maximizing d is one of the two integers nearest  :3862  2m  . The corresponding maximum redundancy is  r cid:3   cid:25  m + = m +  2d cid:3   ln 2m + d cid:3    2m + d cid:3   cid:0  ln 2 2m +  :3862  2m   cid:0   2 :3862  2m   = :0861 :  ln 2m +  :3862 2m   ln 2  This is achieved with arbitrary accuracy as n ! 1 .  The quantity  cid:27  = 0:0861 is one of the lesser fundamental constants of the universe. See Robert Gallager[7] .  25. Optimal codeword lengths. Although the codeword lengths of an optimal variable length code are complicated functions of the message probabilities fp1; p2; : : : ; pmg , it can be said that less probable symbols are encoded into longer codewords. Suppose that the message probabilities are given in decreasing order p1 > p2  cid:21   cid:1  cid:1  cid:1   cid:21  pm .  a  Prove that for any binary Hu cid:11 man code, if the most probable message symbol has probability p1 > 2=5 , then that symbol must be assigned a codeword of length 1.   b  Prove that for any binary Hu cid:11 man code, if the most probable message symbol has probability p1 < 1=3 , then that symbol must be assigned a codeword of length  cid:21  2 .  Solution: Optimal codeword lengths. Let fc1; c2; : : : ; cmg be codewords of respective lengths f‘1; ‘2; : : : ; ‘mg corresponding to probabilities fp1; p2; : : : ; pmg .  a  We prove that if p1 > p2 and p1 > 2=5 then ‘1 = 1 . Suppose, for the sake of contradiction, that ‘1  cid:21  2 . Then there are no codewords of length 1; otherwise c1 would not be the shortest codeword. Without loss of generality, we can assume that c1 begins with 00. For x; y 2 f0; 1g let Cxy denote the set of codewords beginning with xy . Then the sets C01 , C10 , and C11 have total probability 1  cid:0  p1 < 3=5 , so some two of these sets  without loss of generality, C10 and C11   have total probability less 2 5. We can now obtain a better code by interchanging the subtree of the decoding tree beginning with 1 with the subtree beginning with 00; that is, we replace codewords of the form 1x : : : by 00x : : : and codewords of the form 00y : : : by 1y : : : . This improvement contradicts the assumption that ‘1  cid:21  2 , and so ‘1 = 1 .  Note that p1 > p2 was a hidden assumption for this problem; otherwise, for example, the probabilities f:49; :49; :02g have the optimal code f00; 1; 01g .    b  The argument is similar to that of part  a . Suppose, for the sake of contradiction, that ‘1 = 1 . Without loss of generality, assume that c1 = 0 . The total probability of C10 and C11 is 1  cid:0  p1 > 2=3 , so at least one of these two sets  without loss of generality, C10   has probability greater than 2 3. We can now obtain a better code by interchanging the subtree of the decoding tree beginning with 0 with the   116  Data Compression  subtree beginning with 10; that is, we replace codewords of the form 10x : : : by 0x : : : and we let c1 = 10 . This improvement contradicts the assumption that ‘1 = 1 , and so ‘1  cid:21  2 .  26. Merges. Companies with values W1; W2; : : : ; Wm are merged as follows. The two least valuable companies are merged, thus forming a list of m  cid:0  1 companies. The value of the merge is the sum of the values of the two merged companies. This continues until one supercompany remains. Let V equal the sum of the values of the merges. Thus V represents the total reported dollar volume of the merges. For example, if W =  3; 3; 2; 2  , the merges yield  3; 3; 2; 2  !  4; 3; 3  !  6; 4  !  10  , and V = 4 + 6 + 10 = 20 .   a  Argue that V is the minimum volume achievable by sequences of pair-wise merges  terminating in one supercompany.  Hint: Compare to Hu cid:11 man coding.   ~Wi = Wi=W , and show that the minimum merge volume V  W H  ~W   cid:20  V  cid:20  W H  ~W  + W   5.39    b  Let W = P Wi;  satis cid:12 es  Solution: Problem: Merges   a  We  cid:12 rst normalize the values of the companies to add to one. The total volume of the merges is equal to the sum of value of each company times the number of times it takes part in a merge. This is identical to the average length of a Hu cid:11 man code, with a tree which corresponds to the merges. Since Hu cid:11 man coding minimizes average length, this scheme of merges minimizes total merge volume.   b  Just as in the case of Hu cid:11 man coding, we have  H  cid:20  EL < H + 1;  we have in this case for the corresponding merge scheme W H  ~W   cid:20  V  cid:20  W H  ~W  + W   5.40    5.41   27. The Sardinas-Patterson test for unique decodability. A code is not uniquely decodable if and only if there exists a  cid:12 nite sequence of code symbols which can be resolved in two di cid:11 erent ways into sequences of codewords. That is, a situation such as  j j B1  A1 j  j B2  j  A2 B3  j  : : :  A3  : : :  Am  Bn  j j  must occur where each Ai and each Bi is a codeword. Note that B1 must be a pre cid:12 x of A1 with some resulting \dangling su cid:14 x." Each dangling su cid:14 x must in turn be either a pre cid:12 x of a codeword or have another codeword as its pre cid:12 x, resulting in another dangling su cid:14 x. Finally, the last dangling su cid:14 x in the sequence must also be a codeword. Thus one can set up a test for unique decodability  which is essentially the Sardinas-Patterson test[11]  in the following way: Construct a set S of all possible dangling su cid:14 xes. The code is uniquely decodable if and only if S contains no codeword.   Data Compression  117   a  State the precise rules for building the set S .   b  Suppose the codeword lengths are li , i = 1; 2; : : : ; m . Find a good upper bound  on the number of elements in the set S .   c  Determine which of the following codes is uniquely decodable:  i. f0; 10; 11g . ii. f0; 01; 11g . iii. f0; 01; 10g . iv. f0; 01g . v. f00; 01; 10; 11g . vi. f110; 11; 10g . vii. f110; 11; 100; 00; 10g .   d  For each uniquely decodable code in part  c , construct, if possible, an in cid:12 nite encoded sequence with a known starting point, such that it can be resolved into codewords in two di cid:11 erent ways.  This illustrates that unique decodability does not imply  cid:12 nite decodability.  Prove that such a sequence cannot arise in a pre cid:12 x code.  Solution: Test for unique decodability.  The proof of the Sardinas-Patterson test has two parts. In the  cid:12 rst part, we will show that if there is a code string that has two di cid:11 erent interpretations, then the code will fail the test. The simplest case is when the concatenation of two codewords yields another codeword. In this case, S2 will contain a codeword, and hence the test will fail.  In general, the code is not uniquely decodeable, i cid:11  there exists a string that admits two di cid:11 erent parsings into codewords, e.g.  x1x2x3x4x5x6x7x8 = x1x2; x3x4x5; x6x7x8 = x1x2x3x4; x5x6x7x8:   5.42   In this case, S2 will contain the string x3x4 , S3 will contain x5 , S4 will contain x6x7x8 , which is a codeword. It is easy to see that this procedure will work for any string that has two di cid:11 erent parsings into codewords; a formal proof is slightly more di cid:14 cult and using induction. In the second part, we will show that if there is a codeword in one of the sets Si; i  cid:21  2 , then there exists a string with two di cid:11 erent possible interpretations, thus showing that the code is not uniquely decodeable. To do this, we essentially reverse the construction of the sets. We will not go into the details - the reader is referred to the original paper.   a  Let S1 be the original set of codewords. We construct Si+1 from Si as follows: A string y is in Si+1 i cid:11  there is a codeword x in S1 , such that xy is in Si or if there exists a z 2 Si such that zy is in S1  i.e., is a codeword . Then the code is uniquely decodable i cid:11  none of the Si , i  cid:21  2 contains a codeword. Thus the set S = [i cid:21 2Si .   118  Data Compression   c    b  A simple upper bound can be obtained from the fact that all strings in the sets Si have length less than lmax , and therefore the maximum number of elements in S is less than 2lmax . i. f0; 10; 11g . This code is instantaneous and hence uniquely decodable. ii. f0; 01; 11g . This code is a su cid:14 x code  see problem 11 . It is therefore uniquely decodable. The sets in the Sardinas-Patterson test are S1 = f0; 01; 11g , S2 = f1g = S3 = S4 = : : : .  iii. f0; 01; 10g . This code is not uniquely decodable. The sets in the test are S1 = f0; 01; 10g , S2 = f1g , S3 = f0g , . . . . Since 0 is codeword, this code fails the test. It is easy to see otherwise that the code is not UD - the string 010 has two valid parsings.  iv. f0; 01g . This code is a su cid:14 x code and is therefore UD. THe test produces  sets S1 = f0; 01g , S2 = f1g , S3 =  cid:30  .  v. f00; 01; 10; 11g . This code is instantaneous and therefore UD. vi. f110; 11; 10g . This code is uniquely decodable, by the Sardinas-Patterson  test, since S1 = f110; 11; 10g , S2 = f0g , S3 =  cid:30  .  vii. f110; 11; 100; 00; 10g . This code is UD, because by the Sardinas Patterson  test, S1 = f110; 11; 100; 00; 10g , S2 = f0g , S3 = f0g , etc.   d  We can produce in cid:12 nite strings which can be decoded in two ways only for examples where the Sardinas Patterson test produces a repeating set. For example, in part  ii , the string 011111 : : : could be parsed either as 0,11,11, : : : or as 01,11,11, : : : . Similarly for  viii , the string 10000 : : : could be parsed as 100,00,00, : : : or as 10,00,00, : : : . For the instantaneous codes, it is not possible to construct such a string, since we can decode as soon as we see a codeword string, and there is no way that we would need to wait to decode.  28. Shannon code. Consider the following method for generating a code for a random variable X which takes on m values f1; 2; : : : ; mg with probabilities p1; p2; : : : ; pm . Assume that the probabilities are ordered so that p1  cid:21  p2  cid:21   cid:1  cid:1  cid:1   cid:21  pm . De cid:12 ne  Fi =  pk;  i cid:0 1  Xk=1   5.43   the sum of the probabilities of all symbols less than i . Then the codeword for i is the number Fi 2 [0; 1] rounded o cid:11  to li bits, where li = dlog 1  a  Show that the code constructed by this process is pre cid:12 x-free and the average length  pie .  satis cid:12 es  H X   cid:20  L < H X  + 1:   5.44    b  Construct the code for the probability distribution  0:5; 0:25; 0:125; 0:125  .  Solution: Shannon code.   119   5.45    5.46    5.47   Data Compression   a  Since li = dlog 1  pie , we have  which implies that  log  1 pi  cid:20  li < log  1 pi  + 1  H X   cid:20  L =X pili < H X  + 1:  2 cid:0 li  cid:20  pi < 2 cid:0  li cid:0 1 :  The di cid:14 cult part is to prove that the code is a pre cid:12 x code. By the choice of li , we have  Thus Fj , j > i di cid:11 ers from Fi by at least 2 cid:0 li , and will therefore di cid:11 er from Fi is at least one place in the  cid:12 rst li bits of the binary expansion of Fi . Thus the codeword for Fj , j > i , which has length lj  cid:21  li , di cid:11 ers from the codeword for Fi at least once in the  cid:12 rst li places. Thus no codeword is a pre cid:12 x of any other codeword.   b  We build the following table  Symbol Probability Fi in decimal Fi in binary  1 2 3 4  0.5 0.25 0.125 0.125  0.0 0.5 0.75 0.875  0.0 0.10 0.110 0.111  li Codeword 1 2 3 3  0 10 110 111  The Shannon code in this case achieves the entropy bound  1.75 bits  and is optimal.  29. Optimal codes for dyadic distributions. For a Hu cid:11 man code tree, de cid:12 ne the probability of a node as the sum of the probabilities of all the leaves under that node. Let the random variable X be drawn from a dyadic distribution, i.e., p x  = 2 cid:0 i , for some i , for all x 2 X . Now consider a binary Hu cid:11 man code for this distribution.  a  Argue that for any node in the tree, the probability of the left child is equal to the  probability of the right child.   b  Let X1; X2; : : : ; Xn be drawn i.i.d.  cid:24  p x  . Using the Hu cid:11 man code for p x  , we map X1; X2; : : : ; Xn to a sequence of bits Y1; Y2; : : : ; Yk X1;X2;:::;Xn  .  The length of this sequence will depend on the outcome X1; X2; : : : ; Xn .  Use part  a  to argue that the sequence Y1; Y2; : : : ; forms a sequence of fair coin  cid:13 ips, i.e., that PrfYi = 0g = PrfYi = 1g = 1 2 , independent of Y1; Y2; : : : ; Yi cid:0 1 . Thus the entropy rate of the coded sequence is 1 bit per symbol.   c  Give a heuristic argument why the encoded sequence of bits for any code that achieves the entropy bound cannot be compressible and therefore should have an entropy rate of 1 bit per symbol.  Solution: Optimal codes for dyadic distributions.   120  Data Compression   a  For a dyadic distribution, the Hu cid:11 man code acheives the entropy bound. The code tree constructed be the Hu cid:11 man algorithm is a complete tree with leaves at depth li with probability pi = 2 cid:0 li . For such a complete binary tree, we can prove the following properties  cid:15  The probability of any internal node at depth k is 2 cid:0 k .  We can prove this by induction. Clearly, it is true for a tree with 2 leaves. Assume that it is true for all trees with n leaves. For any tree with n + 1 leaves, at least two of the leaves have to be siblings on the tree  else the tree would not be complete . Let the level of these siblings be j . The probability of the parent of these two siblings  at level j cid:0  1   has probability 2j + 2j = 2j cid:0 1 . We can now replace the two siblings with their parent, without changing the probability of any other internal node. But now we have a tree with n leaves which satis cid:12 es the required property. Thus, by induction, the property is true for all complete binary trees.   cid:15  From the above property, it follows immediately the the probability of the left  child is equal to the probability of the right child.   b  For a sequence X1; X2 , we can construct a code tree by  cid:12 rst constructing the optimal tree for X1 , and then attaching the optimal tree for X2 to each leaf of the optimal tree for X1 . Proceeding this way, we can construct the code tree for X1; X2; : : : ; Xn . When Xi are drawn i.i.d. according to a dyadic distribution, it is easy to see that the code tree constructed will be also be a complete binary tree with the properties in part  a . Thus the probability of the  cid:12 rst bit being 1 is 1 2, and at any internal node, the probability of the next bit produced by the code being 1 is equal to the probability of the next bit being 0. Thus the bits produced by the code are i.i.d. Bernoulli 1 2 , and the entropy rate of the coded sequence is 1 bit per symbol.   c  Assume that we have a coded sequence of bits from a code that met the entropy bound with equality. If the coded sequence were compressible, then we could used the compressed version of the coded sequence as our code, and achieve an average length less than the entropy bound, which will contradict the bound. Thus the coded sequence cannot be compressible, and thus must have an entropy rate of 1 bit symbol.  Symbol  30. Relative entropy is cost of miscoding: Let the random variable X have  cid:12 ve possible outcomes f1; 2; 3; 4; 5g . Consider two distributions p x  and q x  on this random variable p x  1 2 1 4 1 8 1 16 1 16  q x  C1 x  C2 x  1 2 1 8 1 8 1 8 1 8  0 10 110 1110 1111  0 100 101 110 111  1 2 3 4 5   a  Calculate H p  , H q  , D pjjq  and D qjjp  .   Data Compression  121   b  The last two columns above represent codes for the random variable. Verify that the average length of C1 under p is equal to the entropy H p  . Thus C1 is optimal for p . Verify that C2 is optimal for q .   c  Now assume that we use code C2 when the distribution is p . What is the average  length of the codewords. By how much does it exceed the entropy p ?   d  What is the loss if we use code C1 when the distribution is q ?  Solution: Cost of miscoding   a  H p  = 1 H q  = 1 D pjjq  = 1 D pjjq  = 1  2 log 2 + 1 2 log 2 + 1 2 log 1=2 2 log 1=2  4 log 4 + 1 8 log 8 + 1 1=2 + 1 1=2 + 1  4 log 1=4 8 log 1=8  8 log 8 + 1 8 log 8 + 1 1=8 + 1 1=4 + 1  16 log 16 + 1 8 log 8 + 1 1=8 + 1 1=8 + 1  8 log 1=8 8 log 1=8  16 log 16 = 1:875 bits.  8 log 8 = 2 bits. 16 log 1=16 8 log 1=8  1=8 + 1 1=16 + 1  16 log 1=16 8 log 1=8  1=8 = 0:125 bits.  1=16 = 0:125 bits.   b  The average length of C1 for p x  is 1.875 bits, which is the entropy of p . Thus C1 is an e cid:14 cient code for p x  . Similarly, the average length of code C2 under q x  is 2 bits, which is the entropy of q . Thus C2 is an e cid:14 cient code for q .   c  If we use code C2 for p x  , then the average length is 1  16  cid:3  16  cid:3  3 = 2 bits. It exceeds the entropy by 0.125 bits, which is the same as  8  cid:3  3 + 1  4  cid:3  3 + 1  2  cid:3  1 + 1  3 + 1 D pjjq  .   d  Similary, using code C1 for q has an average length of 2.125 bits, which exceeds  the entropy of q by 0.125 bits, which is D qjjp  .  31. Non-singular codes: The discussion in the text focused on instantaneous codes, with extensions to uniquely decodable codes. Both these are required in cases when the code is to be used repeatedly to encode a sequence of outcomes of a random variable. But if we need to encode only one outcome and we know when we have reached the end of a codeword, we do not need unique decodability - only the fact that the code is non-singular would su cid:14 ce. For example, if a random variable X takes on 3 values a, b and c, we could encode them by 0, 1, and 00. Such a code is non-singular but not uniquely decodable.  In the following, assume that we have a random variable X which takes on m values with probabilities p1; p2; : : : ; pm and that the probabilities are ordered so that p1  cid:21  p2  cid:21  : : :  cid:21  pm .  a  By viewing the non-singular binary code as a ternary code with three symbols, 0,1 and \STOP", show that the expected length of a non-singular code L1:1 for a random variable X satis cid:12 es the following inequality:  L1:1  cid:21   H2 X  log2 3  cid:0  1   5.48   where H2 X  is the entropy of X in bits. Thus the average length of a non- singular code is at least a constant fraction of the average length of an instanta- neous code.   122  Data Compression   b  Let LIN ST be the expected length of the best instantaneous code and L cid:3 1:1 be the expected length of the best non-singular code for X . Argue that L cid:3 1:1  cid:20  L cid:3 IN ST  cid:20  H X  + 1 .   c  Give a simple example where the average length of the non-singular code is less  than the entropy.  Since L1:1 = Pm dlog cid:16  i   d  The set of codewords available for an non-singular code is f0; 1; 00; 01; 10; 11; 000; : : :g .  i=1 pili , show that this is minimized if we allot the shortest code-  words to the most probable symbols. Thus l1 = l2 = 1 ,  l3 = l4 = l5 = l6 = 2 , etc. Show that in general  li =  2 + 1 cid:17 e , and therefore L cid:3 1:1 =Pm  i=1 pidlog cid:16  i  2 + 1 cid:17 e .   e  The previous part shows that it is easy to  cid:12 nd the optimal non-singular code for a distribution. However, it is a little more tricky to deal with the average length of this code. We now bound this average length. It follows from the previous part  that L cid:3 1:1  cid:21  ~L4=Pm  i=1 pi log cid:16  i  F  p  = H X   cid:0  ~L =  cid:0   2 + 1 cid:17  . Consider the di cid:11 erence pi log cid:18  i  pi log pi  cid:0   m  m  2  Xi=1  Xi=1  + 1 cid:19  :  Prove by the method of Lagrange multipliers that the maximum of F  p  occurs when pi = c= i+2  , where c = 1= Hm+2 cid:0 H2  and Hk is the sum of the harmonic series, i.e.,  Hk4=  1 i  k  Xi=1   This can also be done using the non-negativity of relative entropy.    f  Complete the arguments for  H X   cid:0  L cid:3 1:1  cid:20  H X   cid:0  ~L   cid:20  log 2 Hm+2  cid:0  H2    Now it is well known  see, e.g. Knuth, \Art of Computer Programming", Vol. 1  that Hk  cid:25  ln k  more precisely, Hk = ln k +  cid:13  + 1 120k4  cid:0   cid:15  where 0 <  cid:15  < 1=252n6 , and  cid:13  = Euler’s constant = 0:577 : : :  . Either using this or a simple approximation that Hk  cid:20  ln k + 1 , which can be proved by integration of x , it can be shown that H X   cid:0  L cid:3 1:1 < log log m + 2 . Thus we have  12k2 + 1  2k  cid:0  1  1  H X   cid:0  log log jXj  cid:0  2  cid:20  L cid:3 1:1  cid:20  H X  + 1:   5.53   A non-singular code cannot do much better than an instantaneous code!  Solution:   a  In the text, it is proved that the average length of any pre cid:12 x-free code in a D -ary alphabet was greater than HD X  , the D -ary entropy. Now if we start with any   5.49    5.50    5.51    5.52    Data Compression  123  binary non-singular code and add the additional symbol \STOP" at the end, the new code is pre cid:12 x-free in the alphabet of 0,1, and \STOP"  since \STOP" occurs only at the end of codewords, and every codeword has a \STOP" symbol, so the only way a code word can be a pre cid:12 x of another is if they were equal . Thus each code word in the new alphabet is one symbol longer than the binary codewords, and the average length is 1 symbol longer.  Thus we have L1:1 + 1  cid:21  H3 X  , or L1:1  cid:21  H2 X   log 3  cid:0  1 = 0:63H X   cid:0  1 .   b  Since an instantaneous code is also a non-singular code, the best non-singular code is at least as good as the best instantaneous code. Since the best instantaneous code has average length  cid:20  H X  + 1 , we have L cid:3 1:1  cid:20  L cid:3 IN ST  cid:20  H X  + 1 .   c  For a 2 symbol alphabet, the best non-singular code and the best instantaneous code are the same. So the simplest example where they di cid:11 er is when jXj = 3 . In this case, the simplest  and it turns out, optimal  non-singular code has three codewords 0; 1; 00 . Assume that each of the symbols is equally likely. Then H X  = log 3 = 1:58 bits, whereas the average length of the non-singular code is 1 3 :2 = 4=3 = 1:3333 < H X  . Thus a non-singular code could do better than entropy.  3 :1 + 1  3 :1 + 1   d  For a given set of codeword lengths, the fact that allotting the shortest codewords  to the most probable symbols is proved in Lemma 5.8.1, part 1 of EIT. This result is a general version of what is called the Hardy-Littlewood-Polya in- equality, which says that if a < b , c < d , then ad + bc < ac + bd . The general version of the Hardy-Littlewood-Polya inequality states that if we were given two sets of numbers A = fajg and B = fbjg each of size m , and let a[i] be the i -th largest element of A and b[i] be the i -th largest element of set B . Then  m  Xi=1  a[i]b[m+1 cid:0 i]  cid:20   aibi  cid:20   a[i]b[i]  m  Xi=1  m  Xi=1   5.54   An intuitive explanation of this inequality is that you can consider the ai ’s to the position of hooks along a rod, and bi ’s to be weights to be attached to the hooks. To maximize the moment about one end, you should attach the largest weights to the furthest hooks. The set of available codewords is the set of all possible sequences. Since the only restriction is that the code be non-singular, each source symbol could be alloted to any codeword in the set f0; 1; 00; : : :g . Thus we should allot the codewords 0 and 1 to the two most probable source symbols, i.e., to probablities p1 and p2 . Thus l1 = l2 = 1 . Similarly, l3 = l4 = l5 = l6 = 2  corresponding to the codewords 00, 01, 10 and 11 . The next 8 symbols will use codewords of length 3, etc. We will now  cid:12 nd the general form for li . We can prove it by induction, but we will j=1 2j . Then by the arguments of the previous paragraph, all source symbols of index ck +1; ck +2; : : : ; ck +2k = ck+1  derive the result from  cid:12 rst principles. Let ck =Pk cid:0 1   124  Data Compression  use codewords of length k . Now by using the formula for the sum of the geometric series, it is easy to see that  ck =X j = 1k cid:0 12j = 2X j = 0k cid:0 22j = 2  2k cid:0 1  cid:0  1 2  cid:0  1  = 2k  cid:0  2   5.55   Thus all sources with index i , where 2k  cid:0  1  cid:20  i  cid:20  2k  cid:0  2 + 2k = 2k+1  cid:0  2 use codewords of length k . This corresponds to 2k < i + 2  cid:20  2k+1 or k < log i + 2   cid:20  k + 1 or k  cid:0  1 < log i+2 2  cid:20  k . Thus the length of the codeword for the i - th symbol is k = dlog i+2 2 e . Thus the best non-singular code assigns codeword length l cid:3 i = dlog i=2+1 e to symbol i , and therefore L cid:3 1:1 =Pm i=1 pidlog i=2+1 e . 2 + 1 cid:17  . i=1 pi log cid:16  i  e  Since dlog i=2 + 1 e  cid:21  log i=2 + 1  , it follows that L cid:3 1:1  cid:21  ~L4=Pm + 1 cid:19  : pi log cid:18  i  F  p  = H X   cid:0  ~L =  cid:0   Consider the di cid:11 erence  pi log pi  cid:0    5.56   m  m  2  Xi=1  Xi=1  We want to maximize this function over all probability distributions, and therefore  we use the method of Lagrange multipliers with the constraint P pi = 1 .  Therefore let  J p  =  cid:0   pi log pi  cid:0   m  Xi=1  pi log cid:18  i  2  + 1 cid:19  +  cid:21    m  Xi=1  pi  cid:0  1   Then di cid:11 erentiating with respect to pi and setting to 0, we get  m  Xi=1  @J @pi  2  =  cid:0 1  cid:0  log pi  cid:0  log cid:18  i log pi =  cid:21   cid:0  1  cid:0  log pi = 2 cid:21  cid:0 1 2 i + 2  + 1 cid:19  +  cid:21  = 0  i + 2  2  Now substituting this in the constraint that P pi = 1 , we get  m  1  i + 2  = 1  2 cid:21   Xi=1  or 2 cid:21  = 1= Pi  1  i+2   . Now using the de cid:12 nition Hk =Pk  j=1  1 j , it is obvious that  1  i + 2  =  m  Xi=1  m+2  Xi=1  1 i  cid:0  1  cid:0   1 2  = Hm+2  cid:0  H2:  Thus 2 cid:21  =  1  Hm+2 cid:0 H2  , and  pi =  1  1  Hm+2  cid:0  H2  i + 2   5.57    5.58    5.59    5.60    5.61    5.62    5.63    125   5.64    5.65    5.66    5.68    5.69    5.70    5.71    5.72    5.74   Data Compression  Substituting this value of pi in the expression for F  p  , we obtain  F  p  =  cid:0   pi log pi  cid:0   m  Xi=1  pi log cid:18  i  2  + 1 cid:19   m  m  Xi=1 Xi=1 Xi=1  m  =  cid:0   pi log pi  i + 2  2  1  pi log  =  cid:0  = log 2 Hm+2  cid:0  H2   2 Hm+2  cid:0  H2    5.67  Thus the extremal value of F  p  is log 2 Hm+2  cid:0  H2  . We have not showed that it is a maximum - that can be shown be taking the second derivative. But as usual, it is easier to see it using relative entropy. Looking at the expressions above, we can 1 i+2 , then qi is a probability distribution  i.e., see that if we de cid:12 ne qi = , and substuting this in the expression  1  Hm+2 cid:0 H2 i+2 2=  1  2 Hm+2  cid:0 H2   1 qi  qi  cid:21  0 , P qi = 1  . Also,  for F  p  , we obtain  =  cid:0   =  cid:0   m  m  m  Xi=1 Xi=1 Xi=1 Xi=1  F  p  =  cid:0   pi log pi  cid:0   m  Xi=1  pi log cid:18  i  2  + 1 cid:19   pi log pi  i + 2  2  1 qi  1  1  pi log pi  m  m  pi log  2 Hm+2  cid:0  H2  pi Xi=1 =  cid:0  qi  cid:0  = log 2 Hm+2  cid:0  H2   cid:0  D pjjq   cid:20  log 2 Hm+2  cid:0  H2   pi log  2 Hm+2  cid:0  H2    5.73  with equality i cid:11  p = q . Thus the maximum value of F  p  is log 2 Hm+2  cid:0  H2    f   H X   cid:0  L cid:3 1:1  cid:20  H X   cid:0  ~L   cid:20  log 2 Hm+2  cid:0  H2    5.75  The  cid:12 rst inequality follows from the de cid:12 nition of ~L and the second from the result of the previous part. To complete the proof, we will use the simple inequality Hk  cid:20  ln k + 1 , which can be shown by integrating 1 x between 1 and k . Thus Hm+2  cid:20  ln m + 2  + 1 , and 2 Hm+2  cid:0  H2  = 2 Hm+2  cid:0  1  cid:0  1 2    cid:20  2 ln m + 2   = 2 log m + 2 = log e  cid:20  2 log m + 2   cid:20  2 log m2 = 4 log m where the last inequality is true for m  cid:21  2 . Therefore  2    cid:20  2 ln m + 2  + 1  cid:0  1  cid:0  1  H X   cid:0  L1:1  cid:20  log 2 Hm+2  cid:0  H2   cid:20  log 4 log m  = log log m + 2   5.76    126  Data Compression  We therefore have the following bounds on the average length of a non-singular code  H X   cid:0  log log jXj  cid:0  2  cid:20  L cid:3 1:1  cid:20  H X  + 1   5.77   A non-singular code cannot do much better than an instantaneous code!  32. Bad wine. One is given 6 bottles of wine. It is known that precisely one bottle has gone bad  tastes terrible . From inspection of the bottles it is determined that the probability pi that the ith bottle is bad is given by  p1; p2; : : : ; p6  =   8 23   . Tasting will determine the bad wine.  23 ; 6  23 ; 2  23 ; 4  23 ; 1  23 ; 2  Suppose you taste the wines one at a time. Choose the order of tasting to minimize the expected number of tastings required to determine the bad bottle. Remember, if the  cid:12 rst 5 wines pass the test you don’t have to taste the last.   a  What is the expected number of tastings required?   b  Which bottle should be tasted  cid:12 rst?  Now you get smart. For the  cid:12 rst sample, you mix some of the wines in a fresh glass and sample the mixture. You proceed, mixing and tasting, stopping when the bad bottle has been determined.   c  What is the minimum expected number of tastings required to determine the bad  wine?   d  What mixture should be tasted  cid:12 rst?  Solution: Bad Wine   a  If we taste one bottle at a time, to minimize the expected number of tastings the order of tasting should be from the most likely wine to be bad to the least. The expected number of tastings required is  8 23  + 2  cid:2   6 23  + 3  cid:2   4 23  + 4  cid:2   2 23  + 5  cid:2   2 23  + 5  cid:2   1 23   b  The  cid:12 rst bottle to be tasted should be the one with probability 8  c  The idea is to use Hu cid:11 man coding. With Hu cid:11 man coding, we get codeword lengths  23 .  as  2; 2; 2; 3; 4; 4  . The expected number of tastings required is  8 23  + 2  cid:2   6 23  + 2  cid:2   4 23  + 3  cid:2   2 23  + 4  cid:2   2 23  + 4  cid:2   1 23  6  Xi=1  pili = 1  cid:2  55 23  =  = 2:39  6  Xi=1  pili = 2  cid:2  54 23  =  = 2:35   Data Compression  127   d  The mixture of the  cid:12 rst and second bottles should be tasted  cid:12 rst.  33. Hu cid:11 man vs. Shannon. A random variable X takes on three values with probabil-  ities 0.6, 0.3, and 0.1.   a  What are the lengths of the binary Hu cid:11 man codewords for X ? What are the  lengths of the binary Shannon codewords  l x  = dlog  1  p x   e  for X ?   b  What is the smallest integer D such that the expected Shannon codeword length with a D -ary alphabet equals the expected Hu cid:11 man codeword length with a D - ary alphabet?  Solution: Hu cid:11 man vs. Shannon   a  It is obvious that an Hu cid:11 man code for the distribution  0.6,0.3,0.1  is  1,01,00 , with codeword lengths  1,2,2 . The Shannon code would use lengths dlog 1 pe , which gives lengths  1,2,4  for the three symbols.   b  For any D > 2 , the Hu cid:11 man code for the three symbols are all one character. The 1 0:1 = 1 ,  Shannon code length dlogD i.e., if D = 10 . Hence for D  cid:21  10 , the Shannon code is also optimal.  pe would be equal to 1 for all symbols if logD  1  34. Hu cid:11 man algorithm for tree construction. Consider the following problem: m binary signals S1; S2; : : : ; Sm are available at times T1  cid:20  T2  cid:20  : : :  cid:20  Tm , and we would like to  cid:12 nd their sum S1  cid:8  S2  cid:8   cid:1  cid:1  cid:1   cid:8  Sm using 2-input gates, each gate with 1 time unit delay, so that the  cid:12 nal result is available as quickly as possible. A simple greedy algorithm is to combine the earliest two results, forming the partial result at time max T1; T2  + 1 . We now have a new problem with S1  cid:8  S2; S3; : : : ; Sm , available at times max T1; T2  + 1; T3; : : : ; Tm . We can now sort this list of T’s, and apply the same merging step again, repeating this until we have the  cid:12 nal result.   a  Argue that the above procedure is optimal, in that it constructs a circuit for which  the  cid:12 nal result is available as quickly as possible.   b  Show that this procedure  cid:12 nds the tree that minimizes  where Ti is the time at which the result alloted to the i -th leaf is available, and li is the length of the path from the i -th leaf to the root.   c  Show that  C T   = max   Ti + li   i  C T    cid:21  log2 Xi  2Ti!  C T    cid:20  log2 Xi  2Ti! + 1   5.78    5.79    5.80   for any tree T .   d  Show that there exists a tree such that   128  Data Compression  Thus log2 cid:16 Pi 2Ti cid:17  is the analog of entropy for this problem.  Solution:  Tree construction:   a  The proof is identical to the proof of optimality of Hu cid:11 man coding. We  cid:12 rst show that for the optimal tree if Ti < Tj , then li  cid:21  lj . The proof of this is, as in the case of Hu cid:11 man coding, by contradiction. Assume otherwise, i.e., that if Ti < Tj and li < lj , then by exchanging the inputs, we obtain a tree with a lower total cost, since   5.81   maxfTi + li; Tj + ljg  cid:21  maxfTi + lj; Tj + lig Thus the longest branches are associated with the earliest times. The rest of the proof is identical to the Hu cid:11 man proof. We show that the longest branches correspond to the two earliest times, and that they could be taken as siblings  inputs to the same gate . Then we can reduce the problem to constructing the optimal tree for a smaller problem. By induction, we extend the optimality to the larger problem, proving the optimality of the above algorithm. Given any tree of gates, the earliest that the output corresponding to a particular signal would be available is Ti +li , since the signal undergoes li gate delays. Thus maxi Ti + li  is a lower bound on the time at which the  cid:12 nal answer is available. The fact that the tree achieves this bound can be shown by induction. For any internal node of the tree, the output is available at time equal to the maximum of the input times plus 1. Thus for the gates connected to the inputs Ti and Tj , the output is available at time max Ti; Tj  + 1 . For any node, the output is available at time equal to maximum of the times at the leaves plus the gate delays to get from the leaf to the node. This result extneds to the complete tree, and for the root, the time at which the  cid:12 nal result is available is maxi Ti + li  . The above algorithm minimizes this cost.   b  Let c1 = Pi 2Ti and c2 = Pi 2 cid:0 li . By the Kraft inequality, c2  cid:20  1 . Now let Pj 2 cid:0 lj . Clearly, pi and ri are probability mass  pi = 2Ti functions. Also, we have Ti = log pic1  and li =  cid:0  log ric2  . Then  Pj 2Tj , and let ri = 2 cid:0 li  Now the maximum of any random variable is greater than its average under any distribution, and therefore  C T   = max  i  i  = max   Ti + li   log pic1   cid:0  log ric2   pi ri  log  i  = log c1  cid:0  log c2 + max  C T    cid:21  log c1  cid:0  log c2 +Xi  pi log  pi ri   cid:21  log c1  cid:0  log c2 + D pjjr    5.82    5.83    5.84    5.85    5.86    129   5.87    5.88    5.89    5.90   Data Compression  Since  cid:0 logc2  cid:21  0 and D pjjr   cid:21  0 , we have  C T    cid:21  log c1  which is the desired result.   c  From the previous part, we achieve the lower bound if pi = ri and c2 = 1 . However, since the li ’s are constrained to be integers, we cannot achieve equality in all cases. Instead, if we let  it is easy to verify that P 2 cid:0 li  cid:20 P pi = 1 , and that thus we can construct a tree  that achieves  li = cid:24 log  1  pi cid:25  =&log Pj 2Tj 2Ti ’ ;  Ti + li  cid:20  log Xj  2Tj   + 1  for all i . Thus this tree achieves within 1 unit of the lower bound.  Clearly, log Pj 2Tj   is the equivalent of entropy for this problem!  35. Generating random variables. One wishes to generate a random variable X  X =  1; with probability p  0; with probability 1  cid:0  p  You are given fair coin  cid:13 ips Z1; Z2; : : : . Let N be the  random  number of  cid:13 ips needed to generate X . Find a good way to use Z1; Z2; : : : to generate X . Show that EN  cid:20  2 . Solution: We expand p = 0:p1p2 : : : as a binary number. Let U = 0:Z1Z2 : : : , the se- quence Z treated as a binary number. It is well known that U is uniformly distributed on [0; 1  . Thus, we generate X = 1 if U < p and 0 otherwise.  The procedure for generated X would therefore examine Z1; Z2; : : : and compare with p1; p2; : : : ; and generate a 1 at the  cid:12 rst time one of the Zi ’s is less than the correspond- ing pi and generate a 0 the  cid:12 rst time one of the Zi ’s is greater than the corresponding pi ’s. Thus the probability that X is generated after seeing the  cid:12 rst bit of Z is the probability that Z1 6= p1 , i.e., with probability 1 2. Similarly, X is generated after 2 bits of Z if Z1 = p1 and Z2 6= p2 , which occurs with probability 1 4. Thus  EN = 1:  + 2  + 3  + : : : +  1 2  1 4  1 8  = 2   5.91    5.92   36. Optimal word lengths.   2,2,3,3 ?   a  Can l =  1; 2; 2  be the word lengths of a binary Hu cid:11 man code. What about   130  Data Compression   b  What word lengths l =  l1; l2; : : :  can arise from binary Hu cid:11 man codes?  Solution: Optimal Word Lengths  We  cid:12 rst answer  b  and apply the result to  a .   b  Word lengths of a binary Hu cid:11 man code must satisfy the Kraft inequality with  equality, i.e., Pi 2 cid:0 li = 1 . An easy way to see this is the following: every node in the tree has a sibling  property of optimal binary code , and if we assign each node a ‘weight’, namely 2 cid:0 li , then 2  cid:2  2 cid:0 li is the weight of the father  mother  node. Thus, ‘collapsing’ the tree back, we have that Pi 2 cid:0 li = 1 .   a  Clearly,  1; 2; 2  satis cid:12 es Kraft with equality, while  2; 2; 3; 3  does not. Thus,  1; 2; 2  can arise from Hu cid:11 man code, while  2; 2; 3; 3  cannot.  37. Codes. Which of the following codes are   a  uniquely decodable?  b  instantaneous?  C1 = f00; 01; 0g C2 = f00; 01; 100; 101; 11g C3 = f0; 10; 110; 1110; : : :g C4 = f0; 00; 000; 0000g  Solution: Codes.   a  C1 = f00; 01; 0g is uniquely decodable  su cid:14 x free  but not instantaneous.  b  C2 = f00; 01; 100; 101; 11g is pre cid:12 x free  instantaneous .  c  C3 = f0; 10; 110; 1110; : : :g is instantaneous  d  C4 = f0; 00; 000; 0000g is neither uniquely decodable or instantaneous. 25 ; 6 25 ; 4  38. Hu cid:11 man. Find the Hu cid:11 man D -ary code for  p1; p2; p3; p4; p5; p6  =   6  and the expected word length  25 ; 4  25 ; 3  25 ; 2 25     a  for D = 2 .  b  for D = 4 .  Solution: Hu cid:11 man Codes.   a  D=2  25  14 11  11 8 6  8 6 6 5  6 6 5 4 4  6 6 4 4 3 2  6 6 4 4 2 2 1   Data Compression  131  pi li  6 25 2  6 25 2  4 25 3  4 25 3  2 25 3  2 25 4  1 25 4  E l  =  pili  Xi=1  6  cid:2  2 + 6  cid:2  2 + 4  cid:2  3 + 4  cid:2  3 + 2  cid:2  3 + 2  cid:2  4 + 1  cid:2  4  = 2:66  =  =   b  D=4  7  1 25 66 25  7  1 25 34 25  25  9 6 6 4  6 6 4 4 2 2 1  pi li  6 25 1  6 25 1  4 25 1  4 25 2  2 25 2  2 25 2  1 25 2  E l  =  pili  Xi=1  6  cid:2  1 + 6  cid:2  1 + 4  cid:2  1 + 4  cid:2  2 + 2  cid:2  2 + 2  cid:2  2 + 1  cid:2  2  = 1:36  =  =  39. Entropy of encoded bits. Let C : X  cid:0 ! f0; 1g cid:3  be a nonsingular but nonuniquely  decodable code. Let X have entropy H X :   a  Compare H C X   to H X  .  b  Compare H C X n   to H X n  .  Solution: Entropy of encoded bits   132  Data Compression   a  Since the code is non-singular, the function X ! C X  is one to one, and hence  H X  = H C X   .  Problem 2.4    b  Since the code is not uniquely decodable, the function X n ! C X n  is many to  one, and hence H X n   cid:21  H C X n   .  40. Code rate.  Let X be a random variable with alphabet f1; 2; 3g and distribution  The data compression code for X assigns codewords  1; with probability 1=2 2; with probability 1=4 3; with probability 1=4:  0; 10; 11;  if x = 1 if x = 2 if x = 3:  X =8>< >: C x  =8>< >:  Let X1; X2; : : : be independent identically distributed according to this distribution and let Z1Z2Z3 : : : = C X1 C X2  : : : be the string of binary symbols resulting from concatenating the corresponding codewords. For example, 122 becomes 01010 .   a  Find the entropy rate H X   and the entropy rate H Z  in bits per symbol. Note  that Z is not compressible further.   b  Now let the code be  and  cid:12 nd the entropy rate H Z :   c  Finally, let the code be  C x  =8>< >:  C x  =8>< >:  00; 10; 01;  if x = 1 if x = 2 if x = 3:  00; 1; 01;  if x = 1 if x = 2 if x = 3:  and  cid:12 nd the entropy rate H Z :  Solution: Code rate.  This is a slightly tricky question. There’s no straightforward rigorous way to calculate the entropy rates, so you need to do some guessing.   a  First, since the Xi ’s are independent, H X   = H X1  = 1=2 log 2+2 1=4  log 4  =  3=2: Now we observe that this is an optimal code for the given distribution on X , and since the probabilities are dyadic there is no gain in coding in blocks. So the   Data Compression  133  resulting process has to be i.i.d. Bern 1 2 ,  for otherwise we could get further compression from it . Therefore H Z  = H Bern 1 2   = 1 .   b  Here it’s easy.  H Z  = lim n!1 = lim n!1 = lim n!1 = 3=4:  H Z1; Z2; : : : ; Zn   H X1; X2; : : : ; Xn=2   n  n  H X   n  2  n   We’re being a little sloppy and ignoring the fact that n above may not be a even, but in the limit as n ! 1 this doesn’t make a di cid:11 erence .   c  This is the tricky part.  Suppose we encode the  cid:12 rst n symbols X1X2  cid:1  cid:1  cid:1  Xn into  Z1Z2  cid:1  cid:1  cid:1  Zm = C X1 C X2  cid:1  cid:1  cid:1  C Xn :  Here m = L C X1  +L C X2  + cid:1  cid:1  cid:1 +L C Xn   is the total length of the encoded sequence  in bits , and L is the  binary  length function. Since the concatenated codeword sequence is an invertible function of  X1; : : : ; Xn  , it follows that  nH X   = H X1X2  cid:1  cid:1  cid:1  Xn  = H Z1Z2  cid:1  cid:1  cid:1  ZPn  1 L C Xi     The  cid:12 rst equality above is trivial since the Xi ’s are independent. Similarly, may guess that the right-hand-side above can be written as   5.93   H Z1Z2  cid:1  cid:1  cid:1  ZPn  1 L C Xi    = E[  n  Xi=1  L C Xi  ]H Z  = nE[L C X1  ]H Z    5.94    This is not trivial to prove, but it is true.  Combining the left-hand-side of  5.93  with the right-hand-side of  5.94  yields  H Z  =  H X    E[L C X1  ] 3=2 7=4 6 7  ;  =  =  where E[L C X1  ] =P3  x=1 p x L C x   = 7=4:   134  Data Compression  41. Optimal codes. Let l1; l2; : : : ; l10 be the binary Hu cid:11 man codeword lengths for the probabilities p1  cid:21  p2  cid:21  : : :  cid:21  p10 . Suppose we get a new distribution by splitting the last probability mass. What can you say about the optimal binary codeword lengths ~l1; ~l2; : : : ; ~l11 for the probabilities p1; p2; : : : ; p9;  cid:11 p10;  1  cid:0   cid:11  p10 , where 0  cid:20   cid:11   cid:20  1 . Solution: Optimal codes.  To construct a Hu cid:11 man code, we  cid:12 rst combine the two smallest probabilities. In this case, we would combine  cid:11 p10 and  1  cid:0   cid:11  p10 . The result of the sum of these two probabilities is p10 . Note that the resulting probability distribution is now exactly the same as the original probability distribution. The key point is that an optimal code for p1; p2; : : : ; p10 yields an optimal code  when expanded  for p1; p2; : : : ; p9;  cid:11 p10;  1  cid:0   cid:11  p10 . In e cid:11 ect, the  cid:12 rst 9 codewords will be left unchanged, while the 2 new code- words will be XXX0 and XXX1 where XXX represents the last codeword of the original distribution.  In short, the lengths of the  cid:12 rst 9 codewords remain unchanged, while the lengths of the last 2 codewords  new codewords  are equal to l10 + 1 .  42. Ternary codes. Which of the following codeword lengths can be the word lengths of  a 3-ary Hu cid:11 man code and which cannot?   a   1; 2; 2; 2; 2    b   2; 2; 2; 2; 2; 2; 2; 2; 3; 3; 3   Solution: Ternary codes.   a  The word lengths  1; 2; 2; 2; 2  CANNOT be the word lengths for a 3-ary Hu cid:11 man code. This can be seen by drawing the tree implied by these lengths, and seeing that one of the codewords of length 2 can be reduced to a codeword of length 1 which is shorter. Since the Hu cid:11 man tree produces the minimum expected length tree, these codeword lengths cannot be the word lengths for a Hu cid:11 man tree.   b  The word lengths  2; 2; 2; 2; 2; 2; 2; 2; 3; 3; 3  ARE the word lengths for a 3-ary  Hu cid:11 man code. Again drawing the tree will verify this. Also, Pi 3 cid:0 li = 8  cid:2  3 cid:0 2 + 3  cid:2  3 cid:0 3 = 1 , so these word lengths satisfy the Kraft inequality with equality. Therefore the word lengths are optimal for some distribution, and are the word lengths for a 3-ary Hu cid:11 man code.  43. Piecewise Hu cid:11 man. Suppose the codeword that we use to describe a random variable X  cid:24  p x  always starts with a symbol chosen from the set fA; B; Cg , followed by binary digits f0; 1g . Thus we have a ternary code for the  cid:12 rst symbol and binary thereafter. Give the optimal uniquely decodeable code  minimum expected number of symbols  for the probability distribution  p = cid:18  16  69  ;  15 69  ;  12 69  ;  10 69  ;  8 69  ;  8  69 cid:19  :   5.95    Data Compression  135  Solution: Piecewise Hu cid:11 man.  Codeword a b1 c1 c0 b01 b00  x1 x2 x3 x4 x5 x6  16 15 12 10 8 8  22 16 16 15  16 16 15 12 10  69  31 22 16  Note that the above code is not only uniquely decodable, but it is also instantaneously decodable. Generally given a uniquely decodable code, we can construct an instan- taneous code with the same codeword lengths. This is not the case with the piece- wise Hu cid:11 man construction. There exists a code with smaller expected lengths that is uniquely decodable, but not instantaneous.  Codeword a b c a0 b0 c0  44. Hu cid:11 man. Find the word lengths of the optimal binary encoding of p =  cid:16  1  Solution: Hu cid:11 man.  100 ; 1  100 ; : : : ; 1  100 cid:17  :  Since the distribution is uniform the Hu cid:11 man tree will consist of word lengths of dlog 100 e = 7 and blog 100 c = 6 . There are 64 nodes of depth 6, of which  64- k   will be leaf nodes; and there are k nodes of depth 6 which will form 2k leaf nodes of depth 7. Since the total number of leaf nodes is 100, we have   64  cid:0  k  + 2k = 100   k = 36:  So there are 64 - 36 = 28 codewords of word length 6, and 2  cid:2  36 = 72 codewords of word length 7.  45. Random \20" questions. Let X be uniformly distributed over f1; 2; : : : ; mg . As- sume m = 2n . We ask random questions: Is X 2 S1 ? Is X 2 S2 ?...until only one integer remains. All 2m subsets of f1; 2; : : : ; mg are equally likely.  a  How many deterministic questions are needed to determine X ?   b  Without loss of generality, suppose that X = 1 is the random object. What is the probability that object 2 yields the same answers for k questions as object 1?  c  What is the expected number of objects in f2; 3; : : : ; mg that have the same  d  Suppose we ask n + pn  answers to the questions as does the correct object 1?  random questions. What is the expected number of  wrong objects agreeing with the answers?   136  Data Compression   e  Use Markov’s inequality PrfX  cid:21  t cid:22 g  cid:20  1   one or more wrong object remaining  goes to zero as n  cid:0 ! 1 .  t ; to show that the probability of error  Solution: Random \20" questions.   a  Obviously, Hu cid:11 man codewords for X are all of length n . Hence, with n deter-  ministic questions, we can identify an object out of 2n candidates.   b  Observe that the total number of subsets which include both object 1 and object 2 or neither of them is 2m cid:0 1 . Hence, the probability that object 2 yields the same answers for k questions as object 1 is  2m cid:0 1=2m k = 2 cid:0 k . More information theoretically, we can view this problem as a channel coding problem through a noiseless channel. Since all subsets are equally likely, the probability the object 1 is in a speci cid:12 c random subset is 1=2 . Hence, the question whether object 1 belongs to the k th subset or not corresponds to the k th bit of the random codeword for object 1, where codewords X k are Bern  1=2   random k -sequences.  Object Codeword 0110 : : : 1 0010 : : : 0  1 2 ...   c  Let  1j =  1;  0;  Then,  Now we observe a noiseless output Y k of X k and  cid:12 gure out which object was sent. From the same line of reasoning as in the achievability proof of the channel coding theorem, i.e. joint typicality, it is obvious the probability that object 2 has the same codeword as object 1 is 2 cid:0 k .  object j yields the same answers for k questions as object 1 otherwise  ;  for j = 2; : : : ; m.  Xj=2 E  of objects in f2; 3; : : : ; mg with the same answers  = E  Xj=2 Xj=2  =  =  m  m  2 cid:0 k  m  1j   E 1j   =  m  cid:0  1 2 cid:0 k =  2n  cid:0  1 2 cid:0 k:   d  Plugging k = n + pn into  c  we have the expected number of  2n  cid:0  1 2 cid:0 n cid:0 pn .   Data Compression  137   e  Let N by the number of wrong objects remaining. Then, by Markov’s inequality  P  N  cid:21  1   cid:20  EN =  2n  cid:0  1 2 cid:0 n cid:0 pn  cid:20  2 cid:0 pn ! 0;  where the  cid:12 rst equality follows from part  d .   138  Data Compression   Chapter 6  Gambling and Data Compression  1. Horse race. Three horses run a race. A gambler o cid:11 ers 3-for-1 odds on each of the horses. These are fair odds under the assumption that all horses are equally likely to win the race. The true win probabilities are known to be  Let b =  b1; b2; b3  , bi  cid:21  0 , P bi = 1 , be the amount invested on each of the horses.  The expected log wealth is thus  p =  p1; p2; p3  = cid:18  1  2  ;  1 4  ;  1  4 cid:19  :  W  b  =  pi log 3bi:  3  Xi=1   a  Maximize this over b to  cid:12 nd b cid:3  and W  cid:3  . Thus the wealth achieved in repeated  horse races should grow to in cid:12 nity like 2nW  cid:3   with probability one.   b  Show that if instead we put all of our money on horse 1, the most likely winner,  we will eventually go broke with probability one.  Solution: Horse race.   a  The doubling rate  pi log 3bi  pi log bioi  W  b  = Xi = Xi = X pi log 3 +X pi log pi  cid:0 X pi log = log 3  cid:0  H p   cid:0  D pjjb   cid:20  log 3  cid:0  H p ;  pi bi  139   6.1    6.2    6.3    6.4    6.5    6.6    6.7    140  with equality i cid:11  p = b . Hence b cid:3  = p =   1 2 log 9 1 By the strong law of large numbers,  8 = 0:085 .  Gambling and Data Compression  2 ; 1  4 ; 1  4   and W  cid:3  = log 3 cid:0  H  1  2 ; 1  4 ; 1  4   =  Sn = Yj  3b Xj   nPj log 3b Xj     = 2n  1 ! 2nE log 3b X  = 2nW  b    6.8    6.9    6.10    6.11    6.12   When b = b cid:3  , W  b  = W  cid:3  and Sn  :=2nW  cid:3   = 20:085n =  1:06 n .   b  If we put all the money on the  cid:12 rst horse, then the probability that we do not go broke in n races is   1 2  n . Since this probability goes to zero with n , the probability of the set of outcomes where we do not ever go broke is zero, and we will go broke with probability 1. Alternatively, if b =  1; 0; 0  , then W  b  =  cid:0 1 and Sn ! 2nW = 0 w.p.1   6.13   by the strong law of large numbers.  2. Horse race with subfair odds.  If the odds are bad  due to a track take  the gambler may wish to keep money in his pocket. Let b 0  be the amount in his pocket and let b 1 ; b 2 ; : : : ; b m  be the amount bet on horses 1; 2; : : : ; m , with odds o 1 ; o 2 ; : : : ; o m  , and win probabilities p 1 ; p 2 ; : : : ; p m  . Thus the result- ing wealth is S x  = b 0  + b x o x ; with probability p x ; x = 1; 2; : : : ; m:   a  Find b cid:3  maximizing E log S if P 1=o i  < 1:  b  Discuss b cid:3  if P 1=o i  > 1:  There isn’t an easy closed form solution in this case,  but a \water- cid:12 lling" solution results from the application of the Kuhn-Tucker conditions.   Solution:  Horse race with a cash option .  Since in this case, the gambler is allowed to keep some of the money as cash, the mathematics becomes more complicated. In class, we used two di cid:11 erent approaches to prove the optimality of proportional betting when the gambler is not allowed keep any of the money as cash. We will use both approaches for this problem. But in the case of subfair odds, the relative entropy approach breaks down, and we have to use the calculus approach.  The setup of the problem is straight-forward. We want to maximize the expected log return, i.e.,  W  b; p  = E log S X  =  pi log b0 + bioi    6.14   m  Xi=1   141   6.15    6.16    6.17    6.18    6.19    6.20   Gambling and Data Compression  over all choices b with bi  cid:21  0 and Pm  Approach 1: Relative Entropy  i=0 bi = 1 .  We try to express W  b; p  as a sum of relative entropies.  oi  + bi 1  W  b; p  = X pi log b0 + bioi  oi ! oi!  = X pi log  b0 = X pi log  b0 = X pi log pioi + log K  cid:0  D pjjr ;  + bi pi  pi 1  oi  where  and  K =X   b0 oi  + bi  = b0X 1  oi  +X bi = b0 X 1  oi  cid:0  1  + 1;  ri =  b0 oi  + bi K  is a kind of normalized portfolio. Now both K and r depend on the choice of b . To maximize W  b; p  , we must maximize log K and at the same time minimize D pjjr  . Let us consider the two cases:  a  P 1  oi  cid:20  1 . This is the case of superfair or fair odds. In these cases, it seems intu- itively clear that we should put all of our money in the race. For example, in the case of a superfair gamble, one could invest any cash using a \Dutch book"  in- vesting inversely proportional to the odds  and do strictly better with probability 1. Examining the expression for K , we see that K is maximized for b0 = 0 . In this case, setting bi = pi would imply that ri = pi and hence D pjjr  = 0 . We have succeeded in simultaneously maximizing the two variable terms in the expression for W  b; p  and this must be the optimal solution. Hence, for fair or superfair games, the gambler should invest all his money in the race using proportional gambling, and not leave anything aside as cash. 1 > 1 . In this case, sub-fair odds, the argument breaks down. Looking at the oi expression for K , we see that it is maximized for b0 = 1 . However, we cannot simultaneously minimize D pjjr  . If pioi  cid:20  1 for all horses, then the  cid:12 rst term in the expansion of W  b; p  , that is, P pi log pioi is negative. With b0 = 1 , the best we can achieve is proportional betting, which sets the last term to be 0. Hence, with b0 = 1 , we can only achieve a negative expected log return, which is strictly worse than the 0 log return achieved be setting b0 = 1 . This would indicate, but not prove, that in this case, one should leave all one’s money as cash. A more rigorous approach using calculus will prove this.   b    142  Gambling and Data Compression  We can however give a simple argument to show that in the case of sub-fair odds, the gambler should leave at least some of his money as cash and that there is at least one horse on which he does not bet any money. We will prove this by contradictionstarting with a portfolio that does not satisfy these criteria, we will generate one which does better with probability one.  Let the amount bet on each of the horses be  b1; b2; : : : ; bm  with Pm  i=1 bi = 1 , so that there is no money left aside. Arrange the horses in order of decreasing bioi , so that the m -th horse is the one with the minimum product. Consider a new portfolio with  for all i . Since bioi  cid:21  bmom for all i , b0i  cid:21  0 . We keep the remaining money, i.e.,  b0i = bi  cid:0   bmom  oi  1  cid:0   m  Xi=1  b0i = 1  cid:0  Xi=1  =  m  m  Xi=1 cid:18 bi  cid:0   bmom  oi  bmom  oi  cid:19   bmom  bmom  b0ioi =  cid:18 bi  cid:0   oi  cid:19  oi + = bioi + bmom  m Xi=1  m  oi  Xi=1 oi  cid:0  1!  1  > bioi;  as cash. The return on the new portfolio if horse i wins is  since P 1=oi > 1 . Hence irrespective of which horse wins, the new portfolio does  better than the old one and hence the old portfolio could not be optimal.  Approach 2: Calculus  We set up the functional using Lagrange multipliers as before:  J b  =  m  Xi=1  pi log b0 + bioi  +  cid:21   m Xi=0  bi!  Di cid:11 erentiating with respect to bi , we obtain  Di cid:11 erentiating with respect to b0 , we obtain  @J @bi  =  pioi  b0 + bioi  +  cid:21  = 0:  @J @b0  =  m  Xi=1  pi  b0 + bioi  +  cid:21  = 0:   6.21    6.22    6.23    6.24    6.25    6.26    6.27    6.28    6.29    Gambling and Data Compression  Di cid:11 erentiating w.r.t.  cid:21  , we get the constraint  143   6.30    6.31    6.32    6.33    6.35   X bi = 1:   cid:21 X 1  oi  =  cid:21 :  The solution to these three equations, if they exist, would give the optimal portfolio b . But substituting the  cid:12 rst equation in the second, we obtain the following equation  oi  6= 1 , the only solution to this equation is  cid:21  = 0 , which indicates that the solution is on the boundary of the region over which the maximization is being carried out. Actually, we have been quite cavalier with the  Clearly in the case when P 1 setup of the problemin addition to the constraint P bi = 1 , we have the inequality constraints bi  cid:21  0 . We should have allotted a Lagrange multiplier to each of these. Rewriting the functional with Lagrange multipliers  J b  =  m  Xi=1  pi log b0 + bioi  +  cid:21   m Xi=0  bi! +X  cid:13 ibi  Di cid:11 erentiating with respect to bi , we obtain  @J @bi  =  pioi  b0 + bioi  +  cid:21  +  cid:13 i = 0:  Di cid:11 erentiating with respect to b0 , we obtain  Di cid:11 erentiating w.r.t.  cid:21  , we get the constraint  @J @b0  =  m  Xi=1  pi  b0 + bioi  X bi = 1:  +  cid:21  +  cid:13 0 = 0:   6.34   Now, carrying out the same substitution, we get   cid:21  +  cid:13 0 =  cid:21 X 1 6= 1 , at least one of the  cid:13  ’s is non-zero, which indicates that the corresponding constraint has become active, which shows that the solution is on the boundary of the region.  which indicates that if P 1  +X  cid:13 i   6.36   oi  oi  oi  ;  In the case of solutions on the boundary, we have to use the Kuhn-Tucker conditions to  cid:12 nd the maximum. These conditions are described in Gallager[6], pg. 87. The conditions describe the behavior of the derivative at the maximum of a concave function over a convex region. For any coordinate which is in the interior of the region, the derivative should be 0. For any coordinate on the boundary, the derivative should be   144  Gambling and Data Compression  negative in the direction towards the interior of the region. More formally, for a concave function F  x1; x2; : : : ; xn  over the region xi  cid:21  0 ,  Applying the Kuhn-Tucker conditions to the present maximization, we obtain  @F @xi   cid:20  0 = 0  if xi = 0 if xi > 0  pioi  b0 + bioi  +  cid:21   cid:20  0 = 0  if bi = 0 if bi > 0  X pi  b0 + bioi  +  cid:21   cid:20  0 = 0  if b0 = 0 if b0 > 0  and  Theorem 4.4.1 in Gallager[6] proves that if we can  cid:12 nd a solution to the Kuhn-Tucker conditions, then the solution is the maximum of the function in the region. Let us consider the two cases:   a  P 1  b  P 1  oi  cid:20  1 .  In this case, we try the solution we expect, b0 = 0 , and bi = pi . Setting  cid:21  =  cid:0 1 , we  cid:12 nd that all the Kuhn-Tucker conditions are satis cid:12 ed. Hence, this is the optimal portfolio for superfair or fair odds.  oi  > 1 . In this case, we try the expected solution, b0 = 1 , and bi = 0 . We  cid:12 nd that all the Kuhn-Tucker conditions are satis cid:12 ed if all pioi  cid:20  1 . Hence under this condition, the optimum solution is to not invest anything in the race but to keep everything as cash. In the case when some pioi > 1 , the Kuhn-Tucker conditions are no longer satis cid:12 ed by b0 = 1 . We should then invest some money in the race; however, since the denominator of the expressions in the Kuhn-Tucker conditions also changes, more than one horse may now violate the Kuhn-Tucker conditions. Hence, the optimum solution may involve investing in some horses with pioi  cid:20  1 . There is no explicit form for the solution in this case. The Kuhn Tucker conditions for this case do not give rise to an explicit solution. Instead, we can formulate a procedure for  cid:12 nding the optimum distribution of capital: Order the horses according to pioi , so that  De cid:12 ne  De cid:12 ne  p1o1  cid:21  p2o2  cid:21   cid:1  cid:1  cid:1   cid:21  pmom:  i=1 pi 1 oi  i=1  1 cid:0 Pk 1 cid:0 Pk  1  if k  cid:21  1 if k = 0  Ck =8>< >:  t = minfnjpn+1on+1  cid:20  Cng:  Clearly t  cid:21  1 since p1o1 > 1 = C0 .   6.37    6.38    6.39    6.40    6.41    6.42    145   6.43    6.44    6.45    6.46    6.47    6.48   Gambling and Data Compression  Claim: The optimal strategy for the horse race when the odds are subfair and some of the pioi are greater than 1 is: set  and for i = 1; 2; : : : ; t , set  and for i = t + 1; : : : ; m , set  b0 = Ct;  bi = pi  cid:0   Ct oi  ;  bi = 0:  The above choice of b satis cid:12 es the Kuhn-Tucker conditions with  cid:21  = 1 . For b0 , the Kuhn-Tucker condition is  X pi  bo + bioi  =  t  Xi=1  1 oi  +  m  Xi=t+1  pi Ct  =  t  Xi=1  1 oi  +  1  cid:0 Pt  Ct  i=1 pi  = 1:  For 1  cid:20  i  cid:20  t , the Kuhn Tucker conditions reduce to = 1:  pioi  =  b0 + bioi  pioi pioi  For t + 1  cid:20  i  cid:20  m , the Kuhn Tucker conditions reduce to  pioi  b0 + bioi  =  pioi Ct  cid:20  1;  by the de cid:12 nition of t . Hence the Kuhn Tucker conditions are satis cid:12 ed, and this is the optimal solution.  3. Cards. An ordinary deck of cards containing 26 red cards and 26 black cards is shu cid:15 ed and dealt out one card at at time without replacement. Let Xi be the color of the ith card.   a  Determine H X1 :  b  Determine H X2 :  c  Does H Xk j X1; X2; : : : ; Xk cid:0 1  increase or decrease?  d  Determine H X1; X2; : : : ; X52 :  Solution:   a  P  cid:12 rst card red  = P  cid:12 rst card black  = 1=2 . Hence H X1  =  1=2  log 2 +   1=2  log 2 = log 2 = 1 bit.   b  P second card red  = P second card black  = 1=2 by symmetry. Hence H X2  =  1=2  log 2 +  1=2  log 2 = log 2 = 1 bit. There is no change in the probability from X1 to X2  or to Xi , 1  cid:20  i  cid:20  52   since all the permutations of red and black cards are equally likely.   146  Gambling and Data Compression   c  Since all permutations are equally likely, the joint distribution of Xk and X1; : : : ; Xk cid:0 1  is the same as the joint distribution of Xk+1 and X1; : : : ; Xk cid:0 1 . Therefore  H XkjX1; : : : ; Xk cid:0 1  = H Xk+1jX1; : : : ; Xk cid:0 1   cid:21  H Xk+1jX1; : : : ; Xk    6.49   and so the conditional entropy decreases as we proceed along the sequence. Knowledge of the past reduces uncertainty and thus means that the conditional entropy of the k -th card’s color given all the previous cards will decrease as k increases.   d  All  cid:0 52  Thus  26 cid:1  possible sequences of 26 red cards and 26 black cards are equally likely. H X1; X2; : : : ; X52  = log 52  26! = 48:8 bits  3.2 bits less than 52    6.50   4. Gambling. Suppose one gambles sequentially on the card outcomes in Problem 3. Even odds of 2-for-1 are paid. Thus the wealth Sn at time n is Sn = 2nb x1; x2; : : : ; xn ; where b x1; x2; : : : ; xn  is the proportion of wealth bet on x1; x2; : : : ; xn: Find maxb  cid:1   E log S52: Solution: Gambling on red and black cards.  E[log Sn] = E[log[2nb X1; X2; :::; Xn ]]  = n log 2 + E[log b X ]  p x  log b x   = n + Xx2X n = n + Xx2X n = n + D p x jjb x    cid:0  H X :  p x [log  b x  p x   cid:0  log p x ]  Taking p x  = b x  makes D p x jjb x   = 0 and maximizes E log S52 .  max b x   E log S52 = 52  cid:0  H X   = 52  cid:0  log = 3:2  52!  26!26!  Alternatively, as in the horse race, proportional betting is log-optimal. Thus b x  = p x  and, regardless of the outcome,  S52 =  252  26 cid:1  = 9:08:  cid:0 52  and hence  log S52 = max b x   E log S52 = log 9:08 = 3:2:   6.51    6.52    6.53    6.54    6.55    6.56    6.57    6.58    6.59    6.60    Gambling and Data Compression  147  5. Beating the public odds. Consider a 3-horse race with win probabilities  and fair odds with respect to the  false  distribution   p1; p2; p3  =    1 2  ;  1 4  ;  1 4      r1; r2; r3  =    1 4  ;  1 4  ;  1 2    :   o1; o2; o3  =  4; 4; 2  :  Thus the odds are   a  What is the entropy of the race?  will grow to in cid:12 nity.  Solution: Beating the public odds.   a  The entropy of the race is given by   b  Find the set of bets  b1; b2; b3  such that the compounded wealth in repeated plays  H p  =  log 2 +  log 4 +  log 4  1 4  1 4  1 2 3 2  :  =   b  Compounded wealth will grow to in cid:12 nity for the set of bets  b1; b2; b3  such that  W  b; p  > 0 where  W  b; p  = D pkr   cid:0  D pkb   Xi=1 Calculating D pkr  , this criterion becomes  =  3  pi log  bi ri  :  D pkb  <  1 4  :  6. Horse race: A 3 horse race has win probabilities p =  p1; p2; p3  , and odds o =   1; 1; 1  . The gambler places bets b =  b1; b2; b3  , bi  cid:21  0;P bi = 1 , where bi denotes  the proportion on wealth bet on horse i . These odds are very bad. The gambler gets his money back on the winning horse and loses the other bets. Thus the wealth Sn at time n resulting from independent gambles goes expnentially to zero.   a  Find the exponent.  b  Find the optimal gambling scheme b , i.e., the bet b cid:3  that maximizes the expo-  nent.   148  Gambling and Data Compression   c  Assuming b is chosen as in  b , what distribution p causes Sn to go to zero at  the fastest rate?  Solution: Minimizing losses.   a  Despite the bad odds, the optimal strategy is still proportional gambling. Thus  the optimal bets are b = p , and the exponent in this case is  W  cid:3  =Xi  pi log pi =  cid:0 H p :   6.61    b  The optimal gambling strategy is still proportional betting.   c  The worst distribution  the one that causes the doubling rate to be as negative as possible  is that distribution that maximizes the entropy. Thus the worst W  cid:3  is  cid:0  log 3 , and the gambler’s money goes to zero as 3 cid:0 n .  7. Horse race. Consider a horse race with 4 horses. Assume that each of the horses pays 4-for-1 if it wins. Let the probabilities of winning of the horses be f 1 8g . If you started with $100 and bet optimally to maximize your long term growth rate, what are your optimal bets on each horse? Approximately how much money would you have after 20 races with this strategy ?  2 ; 1  8 ; 1  4 ; 1  Solution: Horse race. The optimal betting strategy is proportional betting, i.e., divid- ing the investment in proportion to the probabilities of each horse winning. Thus the bets on each horse should be  50%, 25%,12.5%,12.5% , and the growth rate achieved by this strategy is equal to log 4 cid:0  H p  = log 4 cid:0  H  1 8   = 2 cid:0  1:75 = 0:25 . After 20 races with this strategy, the wealth is approximately 2nW = 25 = 32 , and hence the wealth would grow approximately 32 fold over 20 races.  4 ; 1  8 ; 1  2 ; 1  8. Lotto. The following analysis is a crude approximation to the games of Lotto conducted by various states. Assume that the player of the game is required pay $1 to play and is asked to choose 1 number from a range 1 to 8. At the end of every day, the state lottery commission picks a number uniformly over the same range. The jackpot, i.e., all the money collected that day, is split among all the people who chose the same number as the one chosen by the state. E.g., if 100 people played today, and 10 of them chose the number 2, and the drawing at the end of the day picked 2, then the $100 collected is split among the 10 people, i.e., each of persons who picked 2 will receive $10, and the others will receive nothing.  The general population does not choose numbers uniformly - numbers like 3 and 7 are supposedly lucky and are more popular than 4 or 8. Assume that the fraction of people choosing the various numbers 1; 2; : : : ; 8 is  f1; f2; : : : ; f8  , and assume that n people play every day. Also assume that n is very large, so that any single person’s choice choice does not change the proportion of people betting on any number.   a  What is the optimal strategy to divide your money among the various possible tickets so as to maximize your long term growth rate?  Ignore the fact that you cannot buy fractional tickets.    Gambling and Data Compression  149   b  What is the optimal growth rate that you can achieve in this game?  c  If  f1; f2; : : : ; f8  =  1=8; 1=8; 1=4; 1=16; 1=16; 1=16; 1=4; 1=16  , and you start with  $1, how long will it be before you become a millionaire?  Solution:   a  The probability of winning does not depend on the number you choose, and there- fore, irrespective of the proportions of the other players, the log optimal strategy is to divide your money uniformly over all the tickets.   b  If there are n people playing, and fi of them choose number i , then the number of people sharing the jackpot of n dollars is nfi , and therefore each person gets n=nfi = 1=fi dollars if i is picked at the end of the day. Thus the odds for number i is 1=fi , and does not depend on the number of people playing. Using the results of Section 6.1, the optimal growth rate is given by  W  cid:3  p  =X pi log oi  cid:0  H p  =X 1  8  log  1 fi  cid:0  log 8   c  Substituing these fraction in the previous equation we get  W  cid:3  p  =  1  1 fi  cid:0  log 8  8X log  3 + 3 + 2 + 4 + 4 + 4 + 2 + 4   cid:0  3  1 8  =  = 0:25   6.62    6.63    6.64    6.65   and therefore after N days, the amount of money you would have would be approx- imately 20:25N . The number of days before this crosses a million = log2 1; 000; 000 =0:25 = 79:7 , i.e., in 80 days, you should have a million dollars. There are many problems with the analysis, not the least of which is that the state governments take out about half the money collected, so that the jackpot is only half of the total collections. Also there are about 14 million di cid:11 erent possible tickets, and it is therefore possible to use a uniform distribution using $1 tickets only if we use capital of the order of 14 million dollars. And with such large investments, the proportions of money bet on the di cid:11 erent possibilities will change, which would further complicate the analysis. However, the fact that people’s choices are not uniform does leave a loophole that can be exploited. Under certain conditions, i.e., if the accumulated jackpot has reached a certain size, the expected return can be greater than 1, and it is worthwhile to play, despite the 50% cut taken by the state. But under normal circumstances, the 50% cut of the state makes the odds in the lottery very unfair, and it is not a worthwhile investment.  9. Horse race. Suppose one is interested in maximizing the doubling rate for a horse race. Let p1; p2; : : : ; pm denote the win probabilities of the m horses. When do the odds  o1; o2; : : : ; om  yield a higher doubling rate than the odds  o01; o02; : : : ; o0m  ?   150  Gambling and Data Compression  Solution: Horse Race Let W and W 0 denote the optimal doubling rates for the odds  o1; o2; : : : ; om  and  o01; o02; : : : ; o0m  respectively. By Theorem 6.1.2 in the book,  W = X pi log oi  cid:0  H p ; and W 0 = X pi log o0i  cid:0  H p   where p is the probability vector  p1; p2; : : : ; pm  . Then W > W 0 exactly when  P pi log oi >P pi log o0i ; that is, when  E log oi > E log o0i:  10. Horse race with probability estimates   a  Three horses race. Their probabilities of winning are   1  2 ; 1  4 ; 1  4   . The odds are   4-for-1, 3-for-1 and 3-for-1 . Let W  cid:3  be the optimal doubling rate. Suppose you believe the probabilities are   1 If you try to maximize the doubling rate, what doubling rate W will you achieve? By how much has your doubling rate decreased due to your poor estimate of the probabilities, i.e., what is  cid:1 W = W  cid:3   cid:0  W ?  4 ; 1  2 ; 1  4   .   b  Now let the horse race be among m horses, with probabilities p =  p1; p2; : : : ; pm  If you believe the true probabilities to be q =  and odds o =  o1; o2; : : : ; om  .  q1; q2; : : : ; qm  , and try to maximize the doubling rate W , what is W  cid:3   cid:0  W ?  Solution: Horse race with probability estimates   a  If you believe that the probabilities of winning are   1  4   , you would bet pro- 4 + 8 . If you bet according to the true probabilities, you  portional to this, and would achieve a growth rate P pi log bioi = 1  1 4 log 3 1 2 + 1 would bet   1 2 log 4 1 1 2 . The loss in growth rate due to incorrect es- timation of the probabilities is the di cid:11 erence between the two growth rates, which is 1  4   on the three horses, achieving a growth rate P pi log bioi =  4 log 3 1 4 ; 1 2 ; 1 4 log 3 1 4 + 1  4 log 3 1  2 log 4 1  2 log 3  4 log 9  4 = 1  4 = 1  2 + 1  4 ; 1  2 ; 1  4 log 2 = 0:25 .   b  For m horses, the growth rate with the true distribution is P pi log pioi , and with the incorrect estimate is P pi log qioi . The di cid:11 erence between the two is P pi log p1  11. The two envelope problem: One envelope contains b dollars, the other 2b dollars. The amount b is unknown. An envelope is selected at random. Let X be the amount observed in this envelope, and let Y be the amount in the other envelope.  = D pjjq  .  qi  Adopt the strategy of switching to the other envelope with probability p x  , where p x  = e cid:0 x  e cid:0 x+ex . Let Z be the amount that the player receives. Thus   X; Y   =   b; 2b ; with probability 1=2   2b; b ; with probability 1=2   6.66    Gambling and Data Compression  Z =  X; with probability 1  cid:0  p x   with probability p x   Y;  151   6.67    a  Show that E X  = E Y   = 3b 2 .   b  Show that E Y =X  = 5=4 . Since the expected ratio of the amount in the other envelope to the one in hand is 5 4, it seems that one should always switch.  This is the origin of the switching paradox.  However, observe that E Y   6= E X E Y =X  . Thus, although E Y =X  > 1 , it does not follow that E Y   > E X  .   c  Let J be the index of the envelope containing the maximum amount of money, and let J0 be the index of the envelope chosen by the algorithm. Show that for any b , I J; J0  > 0 . Thus the amount in the  cid:12 rst envelope always contains some information about which envelope to choose.   d  Show that E Z  > E X  . Thus you can do better than always staying or always switching. In fact, this is true for any monotonic decreasing switching function p x  . By randomly switching according to p x  , you are more likely to trade up than trade down.  Solution: Two envelope problem:   a  X = b or 2b with prob. 1 2, and therefore E X  = 1:5b . Y has the same  unconditional distribution.   b  Given X = x , the other envelope contains 2x with probability 1 2 and contains  x=2 with probability 1 2. Thus E Y =X  = 5=4 .   c  Without any conditioning, J = 1 or 2 with probability  1 2,1 2 . By symmetry, it is not di cid:14 cult to see that the unconditional probability distribution of J 0 is also the same. We will now show that the two random variables are not independent, and therefore I J; J0  6= 0 . To do this, we will calculate the conditional probability P  J0 = 1jJ = 1  . Conditioned on J = 1 , the probability that X = b or 2b is still  1 2,1 2 . How- ever, conditioned on  J = 1; X = 2b  , the probability that Z = X , and therefore J0 = 1 is p 2b  . Similary, conditioned on  J = 1; X = b  , the probability that J0 = 1 is 1  cid:0  p b  . Thus,  P  J0 = 1jJ = 1  = P  X = bjJ = 1 P  J0 = 1jX = b; J = 1   +P  X = 2bjJ = 1 P  J0 = 1jX = 2b; J = 1   6.68   1  cid:0  p b   +  6.69  +   6.70   p 2b   1 2  1 2   p 2b   cid:0  p b     6.71   =  =  >  1 2 1 2 1 2   152  Gambling and Data Compression  Thus the conditional distribution is not equal to the unconditional distribution and J and J0 are not independent.   d  We use the above calculation of the conditional distribution to calculate E Z  . Without loss of generality, we assume that J = 1 , i.e., the  cid:12 rst envelope contains 2b . Then  E ZjJ = 1  = P  X = bjJ = 1 E ZjX = b; J = 1   1 2  1 2 1   6.73    6.72   +P  X = 2bjJ = 1 E ZjX = 2b; J = 1  E ZjX = 2b; J = 1   E ZjX = b; J = 1  + 2 cid:0 p J0 = 1jX = b; J = 1 E ZjJ0 = 1; X = b; J = 1  +p J0 = 2jX = b; J = 1 E ZjJ0 = 2; X = b; J = 1  +p J0 = 1jX = 2b; J = 1 E ZjJ0 = 1; X = 2b; J = 1  + p J0 = 2jX = 2b; J = 1 E ZjJ0 = 2; X = 2b; J = 1  cid:1  6.74  1  [1  cid:0  p b ]2b + p b b + p 2b 2b + [1  cid:0  p 2b ]b  2 3b + 2 3b 2  b p 2b   cid:0  p b     6.76    6.75    6.77   1 2  =  =  =  =  >  as long as p 2b   cid:0  p b  > 0 . Thus E Z  > E X  .  12. Gambling. Find the horse win probabilities p1; p2; : : : ; pm   a  maximizing the doubling rate W  cid:3  for given  cid:12 xed known odds o1; o2; : : : ; om .  b  minimizing the doubling rate for given  cid:12 xed odds o1; o2; : : : ; om .  Solution: Gambling   a  From Theorem 6.1.2, W  cid:3  =P pi log oi  cid:0  H p  . We can also write this as  pi log pioi  pi log  pi 1 oi  W  cid:3  = Xi = Xi = Xi = Xi  pi log  pi log  1  oj1 A  pi  pi log0 @Xj qi  cid:0 Xi oj1 qi  cid:0  log0 @Xj A  pi  1  qi =  1 oi  1 oj  Pj   6.78    6.79    6.80    6.81    6.82   where   Gambling and Data Compression  153  Therefore the minimum value of the growth rate occurs when pi = qi . This is the distribution that minimizes the growth rate, and the minimum value is   cid:0  log cid:16 Pj  1  oj cid:17  .   b  The maximum growth rate occurs when the horse with the maximum odds wins  in all the races, i.e., pi = 1 for the horse that provides the maximum odds  13. Dutch book. Consider a horse race with m = 2 horses,  X = 1; 2  p = 1=2; 1=2  Odds  for one  = 10; 30  Bets = b; 1  cid:0  b:  The odds are super fair.   a  There is a bet b which guarantees the same payo cid:11  regardless of which horse wins. Such a bet is called a Dutch book. Find this b and the associated wealth factor S X :   b  What is the maximum growth rate of the wealth for this gamble? Compare it to  the growth rate for the Dutch book.  Solution: Solution: Dutch book.   a   Therefore,  and   b  In general,  10bD = 30 1  cid:0  bD  40bD = 30  bD = 3=4:  W  bD; P   =  1 2  log cid:18 10  3  4 cid:19  +  1 2  log cid:18 30  1  4 cid:19   = 2:91  SD X  = 2W  bD;P   = 7:5:  W  b; p  =  log 10b  +  1 2  1 2  log 30 1  cid:0  b  :  1  2 cid:18  10  10b cid:3  cid:19  +  1  30  cid:0  30b cid:3  cid:19  = 0 2 cid:18   cid:0 30  Setting the @W @b  to zero we get   154  Gambling and Data Compression  1 2b cid:3   1  + 2 b cid:3   cid:0  1   b cid:3   cid:0  1  + b cid:3  2b cid:3  b cid:3   cid:0  1  2b cid:3   cid:0  1 4b cid:3  1  cid:0  b cid:3    = 0  = 0  = 0  b cid:3  =  1 2  :  Hence  and  W  cid:3  p  =  log 5  +  log 15  = 3:11  1 2  1 2  W  bD; p  = 2:91  S cid:3  = 2W  cid:3  = 8:66 SD = 2WD = 7:5  Thus gambling  a little  with b cid:3  beats the sure win of 7.5 given by the Dutch book  14. Horse race. Suppose one is interested in maximizing the doubling rate for a horse race. Let p1; p2; : : : ; pm denote the win probabilities of the m horses. When do the odds  o1; o2; : : : ; om  yield a higher doubling rate than the odds  o01; o02; : : : ; o0m  ? Solution: Horse Race  Repeat of problem 9  Let W and W 0 denote the optimal doubling rates for the odds  o1; o2; : : : ; om  and  o01; o02; : : : ; o0m  respectively. By Theorem 6.1.2 in the book,  W = X pi log oi  cid:0  H p ; and W 0 = X pi log o0i  cid:0  H p   where p is the probability vector  p1; p2; : : : ; pm  . Then W > W 0 exactly when  P pi log oi >P pi log o0i ; that is, when  E log oi > E log o0i:  15. Entropy of a fair horse race. Let X  cid:24  p x  , x = 1; 2; : : : ; m , denote the winner of p x  . 1 b x  = 1 . Then the resulting  a horse race. Suppose the odds o x  are fair with respect to p x  , i.e., o x  = 1  Let b x  be the amount bet on horse x , b x   cid:21  0 , Pm  wealth factor is S x  = b x o x  , with probability p x  .   Gambling and Data Compression  155  If this side information is available before the bet, how much does it increase the growth rate W  cid:3  ?   a  Find the expected wealth ES X  .  b  Find W  cid:3  , the optimal growth rate of wealth.  c  Suppose  Y =  1; X = 1 or 2  0; otherwise   d  Find I X; Y   .  Solution: Entropy of a fair horse race.   a  The expected wealth ES X  is  ES X  =  S x p x    b  The optimal growth rate of wealth, W  cid:3  , is achieved when b x  = p x  for all x ,  in which case,  m  m  Xx=1 Xx=1 Xx=1  m  =  =  = 1:  b x o x p x   b x ;   since o x  = 1=p x    W  cid:3  = E log S X    p x  log b x o x    p x  log p x =p x    p x  log 1   =  =  =  m  m  Xx=1 Xx=1 Xx=1  m  = 0;   6.83    6.84    6.85    6.86    6.87    6.88    6.89    6.90    6.91    6.92    6.94    c  The increase in our growth rate due to the side information is given by I X; Y   .  so we maintain our current wealth.  Let q = Pr Y = 1  = p 1  + p 2  .  I X; Y   = H Y    cid:0  H Y jX   = H q :  = H Y     since Y is a deterministic function of X    6.93    156  Gambling and Data Compression   d  Already computed above.  16. Negative horse race Consider a horse race with m horses with win probabili- ties p1; p2; : : : pm: Here the gambler hopes a given horse will lose. He places bets i=1 bi = 1 , on the horses, loses his bet bi if horse i wins, and retains   b1; b2; : : : ; bm ;Pm the rest of his bets.  No odds.  Thus S = Pj6=i bj , with probability pi , and one wishes to maximize P pi ln 1  cid:0  bi  subject to the constraint P bi = 1:   a  Find the growth rate optimal investment strategy b cid:3  . Do not constrain the bets to be positive, but do constrain the bets to sum to 1.  This e cid:11 ectively allows short selling and margin.    b  What is the optimal growth rate?  Solution: Negative horse race   a  Let b0i = 1  cid:0  bi  cid:21  0 , and note that Pi b0i = m  cid:0  1 . Let qi = b0i=Pj b0j . Then, fqig  is a probability distribution on f1; 2; : : : ; mg . Now,  W = Xi pi log 1  cid:0  bi  = Xi pi log qi m  cid:0  1  = log m  cid:0  1  +Xi = log m  cid:0  1   cid:0  H p   cid:0  D pkq  :  pi log pi  qi pi  Thus, W  cid:3  is obtained upon setting D pkq  = 0 , which means making the bets such that pi = qi = b0i= m  cid:0  1  , or bi = 1  cid:0   m  cid:0  1 pi . Alternatively, one can use Lagrange multipliers to solve the problem.   b  From  a  we directly see that setting D pkq  = 0 implies W  cid:3  = log m cid:0 1  cid:0 H p  . 17. The St. Petersburg paradox. Many years ago in ancient St. Petersburg the following gambling proposition caused great consternation. For an entry fee of c units, a gambler receives a payo cid:11  of 2k units with probability 2 cid:0 k; k = 1; 2; : : : .   a  Show that the expected payo cid:11  for this game is in cid:12 nite. For this reason, it was argued that c = 1 was a \fair" price to pay to play this game. Most people  cid:12 nd this answer absurd.   b  Suppose that the gambler can buy a share of the game. For example, if he in- vests c=2 units in the game, he receives 1=2 a share and a return X=2 , where Pr X = 2k  = 2 cid:0 k; k = 1; 2; : : : . Suppose X1; X2; : : : are i.i.d. according to this distribution and the gambler reinvests all his wealth each time. Thus his wealth Sn at time n is given by  Sn =  Xi c  :  n  Yi=1   6.95    157   6.96    6.97    6.98    6.99   Gambling and Data Compression  Show that this limit is 1 or 0 , with probability one, accordingly as c < c cid:3  or c > c cid:3 : Identify the \fair" entry fee c cid:3 :  More realistically, the gambler should be allowed to keep a proportion b = 1  cid:0  b of his money in his pocket and invest the rest in the St. Petersburg game. His wealth at time n is then  Let  Let  We have  n  bXi  Yi=1 cid:18 b + c  cid:19  : 2 cid:0 k log 1  cid:0  b +  Sn =  1Xk=1  b2k  c ! :  W  b; c  =  :=2nW  b;c   Sn  W  cid:3  c  = max 0 cid:20 b cid:20 1  W  b; c :  Here are some questions about W  cid:3  c :   c  For what value of the entry fee c does the optimizing value b cid:3  drop below 1?  d  How does b cid:3  vary with c ?  e  How does W  cid:3  c  fall o cid:11  with c ?  Note that since W  cid:3  c  > 0 , for all c , we can conclude that any entry fee c is fair. Solution: The St. Petersburg paradox.   a  The expected return,  EX =  p X = 2k 2k =  2 cid:0 k2k =   6.100   1Xk=1  1Xk=1  1Xk=1  1 = 1:  Thus the expected return on the game is in cid:12 nite.   b  By the strong law of large numbers, we see that  1 n  log Sn =  1 n  n  Xi=1  log Xi  cid:0  log c ! E log X  cid:0  log c; w.p.1   6.101   and therefore Sn goes to in cid:12 nity or 0 according to whether E log X is greater or less than log c . Therefore  log c cid:3  = E log X =  k2 cid:0 k = 2:   6.102   1Xk=1  Therefore a fair entry fee is 2 units if the gambler is forced to invest all his money.   158  Gambling and Data Compression     "fileplot"  W b,c   2.5 2 1.5 1 0.5 0 -0.5 -1  1  2  3  4  5  c  6  7  8  9 10  0  1  0.5  b  Figure 6.1: St. Petersburg: W  b; c  as a function of b and c .   c  If the gambler is not required to invest all his money, then the growth rate is  W  b; c  =  2 cid:0 k log 1  cid:0  b +  b2k  c ! :  1Xk=1   6.103    6.104   For b = 0 , W = 1 , and for b = 1 , W = E log X cid:0 log c = 2 cid:0 log c . Di cid:11 erentiating to  cid:12 nd the optimum value of b , we obtain  @W  b; c   @b  =  2 cid:0 k  1Xk=1  1  c  cid:17   cid:0 1 +  2k  c !   cid:16 1  cid:0  b + b2k  Unfortunately, there is no explicit solution for the b that maximizes W for a given value of c , and we have to solve this numerically on the computer. We have illustrated the results with three plots. The  cid:12 rst  Figure 6.1  shows W  b; c  as a function of b and c . The second  Figure 6.2 shows b cid:3  as a function of c and the third  Figure 6.3  shows W  cid:3  as a function of c . From Figure 2, it is clear that b cid:3  is less than 1 for c > 3 . We can also see this analytically by calculating the slope @W  b;c   at b = 1 .  @b  @W  b; c   @b  =  2 cid:0 k  1Xk=1  1  c  cid:17   cid:0 1 +  2k  c !   cid:16 1  cid:0  b + b2k   6.105    Gambling and Data Compression  159  "filebstar"  * b  * b  1.5  1  0.8  0.6  0.4  0.2  0  3  2  1  0  2.5  0.5     c     c  1  2  3  4  5  6  7  8  9  10  Figure 6.2: St. Petersburg: b cid:3  as a function of c .  "filewstar"  1  2  3  4  5  6  7  8  9  10  Figure 6.3: St. Petersburg: W  cid:3  b cid:3 ; c  as a function of c .   160  Gambling and Data Compression  = Xk 1Xk=1  =  = 1  cid:0   2 cid:0 k 2k  d  cid:0  1! c   2k 1Xk=1 2 cid:0 k  cid:0  c 3  c2 cid:0 2k   6.106    6.107    6.108   which is positive for c < 3 . Thus for c < 3 , the optimal value of b lies on the boundary of the region of b ’s, and for c > 3 , the optimal value of b lies in the interior.   d  The variation of b cid:3  with c is shown in Figure 6.2. As c ! 1 , b cid:3  ! 0 . We have c2 cid:0 c as c ! 1 , but we  a conjecture  based on numerical results  that b cid:3  ! 1p2 do not have a proof.   e  The variation of W  cid:3  with c is shown in Figure 6.3.  18. Super St. Petersburg. Finally, we have the super St. Petersburg paradox, where Pr X = 22k   = 2 cid:0 k; k = 1; 2; : : : . Here the expected log wealth is in cid:12 nite for all b > 0 , for all c , and the gambler’s wealth grows to in cid:12 nity faster than exponentially for any b > 0: But that doesn’t mean all investment ratios b are equally good. To see this, we wish to maximize the relative growth rate with respect to some other portfolio, say, b =   1  2  : Show that there exists a unique b maximizing  2 ; 1  E ln   b + bX=c  2 + 1   1 2 X=c   and interpret the answer. Solution: Super St. Petersburg. With Pr X = 22k    = 2 cid:0 k; k = 1; 2; : : : , we have  E log X =Xk  2 cid:0 k log 22k  = 1;  and thus with any constant entry fee, the gambler’s money grows to in cid:12 nity faster than exponentially, since for any b > 0 ,  W  b; c  =  2 cid:0 k log 1  cid:0  b +  1Xk=1  b22k  c ! >X 2 cid:0 k log  b22k c  = 1:  But if we wish to maximize the wealth relative to the   1 maximize  1  cid:0  b  + b22k  2 cid:0 k log  c  J b; c  =Xk  1  2 + 1  2  22k c  2 ; 1  2   portfolio, we need to  As in the case of the St. Petersburg problem, we cannot solve this problem explicitly. In this case, a computer solution is fairly straightforward, although there are some   6.109    6.110    6.111    Gambling and Data Compression  161  Expected log ratio  ER b,c   0  -1  -2  -3  -4  1 0.9  0.8  0.7  0.6  0.5  b  5  c  0.4  0.3  0.2  0.1  10  Figure 6.4: Super St. Petersburg: J b; c  as a function of b and c .  complications. For example, for k = 6 , 22k is outside the normal range of numbers representable on a standard computer. However, for k  cid:21  6 , we can approximate the ratio within the log by b 0:5 without any loss of accuracy. Using this, we can do a simple numerical computation as in the previous problem.  As before, we have illustrated the results with three plots. The  cid:12 rst  Figure 6.4  shows J b; c  as a function of b and c . The second  Figure 6.5 shows b cid:3  as a function of c and the third  Figure 6.6  shows J  cid:3  as a function of c . These plots indicate that for large values of c , the optimum strategy is not to put all the money into the game, even though the money grows at an in cid:12 nite rate. There exists a unique b cid:3  which maximizes the expected ratio, which therefore causes the wealth to grow to in cid:12 nity at the fastest possible rate. Thus there exists an optimal b cid:3  even when the log optimal portfolio is unde cid:12 ned.   162  Gambling and Data Compression  0.1  0.1  1  100  1000  Figure 6.5: Super St. Petersburg: b cid:3  as a function of c .     10 c     1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  * b  * J  -0.1  0.1  1  10 c  100  1000  Figure 6.6: Super St. Petersburg: J  cid:3  b cid:3 ; c  as a function of c .   Chapter 7  Channel Capacity  1. Preprocessing the output. One is given a communication channel with transition probabilities p yjx  and channel capacity C = maxp x  I X; Y  : A helpful statistician preprocesses the output by forming ~Y = g Y  : He claims that this will strictly improve the capacity.   a  Show that he is wrong.   b  Under what conditions does he not strictly decrease the capacity?  Solution: Preprocessing the output.   a  The statistician calculates ~Y = g Y   . Since X ! Y ! ~Y forms a Markov chain, we can apply the data processing inequality. Hence for every distribution on x ,  Let ~p x  be the distribution on x that maximizes I X; ~Y   . Then  I X; Y    cid:21  I X; ~Y  :   7.1   C = max p x   I X; Y    cid:21  I X; Y  p x =~p x   cid:21  I X; ~Y  p x =~p x  = max  p x   I X; ~Y   = ~C:   7.2  Thus, the statistician is wrong and processing the output does not increase capac- ity.   b  We have equality  no decrease in capacity  in the above sequence of inequalities only if we have equality in the data processing inequality, i.e., for the distribution that maximizes I X; ~Y   , we have X ! ~Y ! Y forming a Markov chain.  2. An additive noise channel. Find the channel capacity of the following discrete  memoryless channel:  163   164  Channel Capacity  Z  X  ?   cid:19  cid:16 -  cid:18  cid:17   -  Y  where PrfZ = 0g = PrfZ = ag = 1 Z is independent of X:  2 : The alphabet for x is X = f0; 1g: Assume that  Observe that the channel capacity depends on the value of a:  Solution: A sum channel.  Y = X + Z  X 2 f0; 1g; Z 2 f0; ag   7.3   We have to distinguish various cases depending on the values of a .  a = 0 In this case, Y = X , and max I X; Y   = max H X  = 1 . Hence the capacity  is 1 bit per transmission.  a 6= 0; cid:6 1 In this case, Y has four possible values 0; 1; a and 1 + a . Knowing Y , we know the X which was sent, and hence H XjY   = 0 . Hence max I X; Y   = max H X  = 1 , achieved for an uniform distribution on the input X .  a = 1 In this case Y has three possible output values, 0; 1 and 2 , and the channel is identical to the binary erasure channel discussed in class, with a = 1=2 . As derived in class, the capacity of this channel is 1  cid:0  a = 1=2 bit per transmission. a =  cid:0 1 This is similar to the case when a = 1 and the capacity here is also 1=2 bit  per transmission.  3. Channels with memory have higher capacity. Consider a binary symmetric chan-  nel with Yi = Xi  cid:8  Zi; where  cid:8  is mod 2 addition, and Xi; Yi 2 f0; 1g: Suppose that fZig has constant marginal probabilities PrfZi = 1g = p = 1  cid:0  PrfZi = 0g; but that Z1; Z2; : : : ; Zn are not necessarily independent. Assume that Z n is inde- pendent of the input X n . Let C = 1 cid:0 H p; 1 cid:0 p : Show that maxp x1;x2;:::;xn  I X1; X2; : : : ; Xn; Y1; Y2; : : : ; Yn   cid:21  nC:  Solution: Channels with memory have a higher capacity.  where  Yi = Xi  cid:8  Zi;  Zi =  1 with probability p  0 with probability 1  cid:0  p   7.4    7.5    Channel Capacity  and Zi are not independent.  I X1; X2; : : : ; Xn; Y1; Y2; : : : ; Yn   = H X1; X2; : : : ; Xn   cid:0  H X1; X2; : : : ; XnjY1; Y2; : : : ; Yn  = H X1; X2; : : : ; Xn   cid:0  H Z1; Z2; : : : ; ZnjY1; Y2; : : : ; Yn   cid:21  H X1; X2; : : : ; Xn   cid:0  H Z1; Z2; : : : ; Zn   cid:21  H X1; X2; : : : ; Xn   cid:0 X H Zi  = H X1; X2; : : : ; Xn   cid:0  nH p  = n  cid:0  nH p ;  if X1; X2; : : : ; Xn are chosen i.i.d.  cid:24  Bern  1 memory over n uses of the channel is  2  . The capacity of the channel with  nC  n  =  max  I X1; X2; : : : ; Xn; Y1; Y2; : : : ; Yn   p x1;x2;:::;xn    cid:21  I X1; X2; : : : ; Xn; Y1; Y2; : : : ; Yn p x1;x2;:::;xn =Bern  1 2    cid:21  n 1  cid:0  H p   = nC:  Hence channels with memory have higher capacity. The intuitive explanation for this result is that the correlation between the noise decreases the e cid:11 ective noise; one could use the information from the past samples of the noise to combat the present noise.  4. Channel capacity. Consider the discrete memoryless channel Y = X + Z  mod 11 ,  where  Z =  1;  1=3; 1=3; 1=3 !  2;  3  and X 2 f0; 1; : : : ; 10g . Assume that Z is independent of X .  a  Find the capacity.  b  What is the maximizing p cid:3  x  ?  Solution: Channel capacity.  where  In this case,  Y = X + Z mod 11   1 with probability1=3 2 with probability1=3 3 with probability1=3  Z =8>< >:  H Y jX  = H ZjX  = H Z  = log 3;  165   7.6    7.7    7.8    7.9    7.10    7.11    7.12    7.13    7.14    7.15    7.16    166  Channel Capacity  independent of the distribution of X , and hence the capacity of the channel is  C = max p x  = max p x  = max p x   I X; Y    H Y    cid:0  H Y jX  H Y    cid:0  log 3  = log 11  cid:0  log 3;  which is attained when Y has a uniform distribution, which occurs  by symmetry  when X has a uniform distribution.   a  The capacity of the channel is log 11  b  The capacity is achieved by an uniform distribution on the inputs. p X = i  = 1 11  3 bits transmission.  for i = 0; 1; : : : ; 10 .  5. Using two channels at once. Consider two discrete memoryless channels  X1; p y1 j x1 ;Y1  and  X2; p y2 j x2 ;Y2  with capacities C1 and C2 respectively. A new channel  X1  cid:2 X2; p y1 j x1   cid:2  p y2 j x2 ;Y1  cid:2 Y2  is formed in which x1 2 X1 and x2 2 X2; are simultaneously sent, resulting in y1; y2: Find the capacity of this channel. Solution: Using two channels at once. Suppose we are given two channels,  X1; p y1jx1 ;Y1  and  X2; p y2jx2 ;Y2  , which we can use at the same time. We can de cid:12 ne the product channel as the channel,  X1  cid:2  X2; p y1; y2jx1; x2  = p y1jx1 p y2jx2 ;Y1  cid:2  Y2  . To  cid:12 nd the capacity of the product channel, we must  cid:12 nd the distribution p x1; x2  on the input alphabet X1  cid:2  X2 that maximizes I X1; X2; Y1; Y2  . Since the joint distribution  7.21   p x1; x2; y1; y2  = p x1; x2 p y1jx1 p y2jx2 ;  Y1 ! X1 ! X2 ! Y2 forms a Markov chain and therefore  I X1; X2; Y1; Y2  = H Y1; Y2   cid:0  H Y1; Y2jX1; X2   = H Y1; Y2   cid:0  H Y1jX1; X2   cid:0  H Y2jX1; X2  = H Y1; Y2   cid:0  H Y1jX1   cid:0  H Y2jX2   cid:20  H Y1  + H Y2   cid:0  H Y1jX1   cid:0  H Y2jX2  = I X1; Y1  + I X2; Y2 ;  where  7.23  and  7.24  follow from Markovity, and we have equality in  7.25  if Y1 and Y2 are independent. Equality occurs when X1 and X2 are independent. Hence  C = max p x1;x2   I X1; X2; Y1; Y2   I X1; Y1  + max p x1;x2   I X2; Y2   I X1; Y1  + max p x2   I X2; Y2   p x1;x2    cid:20  max = max p x1   = C1 + C2:  with equality i cid:11  p x1; x2  = p cid:3  x1 p cid:3  x2  and p cid:3  x1  and p cid:3  x2  are the distributions that maximize C1 and C2 respectively.   7.17    7.18    7.19    7.20    7.22    7.23    7.24    7.25    7.26    7.27    7.28    7.29    7.30    Channel Capacity  167  6. Noisy typewriter. Consider a 26-key typewriter.   a  If pushing a key results in printing the associated letter, what is the capacity C  in bits?   b  Now suppose that pushing a key results in printing that letter or the next  with equal probability . Thus A ! A or B; : : : ; Z ! Z or A: What is the capacity?  c  What is the highest rate code with block length one that you can  cid:12 nd that achieves  zero probability of error for the channel in part  b  .  Solution: Noisy typewriter.   a  If the typewriter prints out whatever key is struck, then the output, Y , is the  same as the input, X , and  C = max I X; Y   = max H X  = log 26;   7.31   attained by a uniform distribution over the letters.   b  In this case, the output is either equal to the input  with probability 1  2   or equal 2  . Hence H Y jX  = log 2 independent of  to the next letter   with probability 1 the distribution of X , and hence  C = max I X; Y   = max H Y    cid:0  log 2 = log 26  cid:0  log 2 = log 13;   7.32   attained for a uniform distribution over the output, which in turn is attained by a uniform distribution on the input.   c  A simple zero error block length one code is the one that uses every alternate letter, say A,C,E,. . . ,W,Y. In this case, none of the codewords will be confused, since A will produce either A or B, C will produce C or D, etc. The rate of this code,  R =  log  codewords   log 13  Block length  =  1  = log 13:   7.33   In this case, we can achieve capacity with a simple code with zero error.  7. Cascade of binary symmetric channels.  Show that a cascade of n identical  independent binary symmetric channels,  X0 ! BSC !1!  cid:1  cid:1  cid:1  ! Xn cid:0 1 ! BSC !n  each with raw error probability p , is equivalent to a single BSC with error probability 1 2  1  cid:0   1  cid:0  2p n  and hence that I X0; Xn  = 0 if p 6= 0; 1 . No encoding or decoding takes place at the intermediate terminals X1; : : : ; Xn cid:0 1 . Thus the capacity of the cascade tends to zero.  lim n!1  Solution: Cascade of binary symmetric channels. There are many ways to solve this problem. One way is to use the singular value decomposition of the transition probability matrix for a single BSC.   168  Let,  Channel Capacity  be the transition probability matrix for our BSC. Then the transition probability matrix for the cascade of n of these BSC’s is given by,  A =" 1  cid:0  p  p  p  1  cid:0  p   An = An:  A = T  cid:0 1" 1  0  0 1  cid:0  2p  T 1  cid:0 1  :  1  T =" 1  Now check that,  where,  Using this we have,  An = An  = T  cid:0 1" 1 = " 1  1  0  0  1  cid:0  2p n  T  2  1 +  1  cid:0  2p n  2  1  cid:0   1  cid:0  2p n   1  2  1 +  1  cid:0  2p n   : 2  1  cid:0   1  cid:0  2p n   1  From this we see that the cascade of n BSC’s is also a BSC with probablility of error,  pn =  1 2   1  cid:0   1  cid:0  2p n :  The matrix, T , is simply the matrix of eigenvectors of A .  This problem can also be solved by induction on n .  Probably the simplest way to solve the problem is to note that the probability of error for the cascade channel is simply the sum of the odd terms of the binomial expansion of  x + y n with x = p and y = 1  cid:0  p . But this can simply be written as 2  x + y n  cid:0  1  2  1  cid:0   1  cid:0  2p n .  2  y  cid:0  x n = 1  1  8. The Z channel. The Z-channel has binary input and output alphabets and transition  probabilities p yjx  given by the following matrix:  Q =" 1  1=2 1=2   0  x; y 2 f0; 1g  Find the capacity of the Z-channel and the maximizing input probability distribution.   Channel Capacity  169  Solution: The Z channel. First we express I X; Y   , the mutual information between the input and output of the Z-channel, as a function of x = Pr X = 1  :  H Y jX  = Pr X = 0   cid:1  0 + Pr X = 1   cid:1  1 = x  H Y   = H Pr Y = 1   = H x=2   I X; Y   = H Y    cid:0  H Y jX  = H x=2   cid:0  x  Since I X; Y   = 0 when x = 0 and x = 1 , the maximum mutual information is obtained for some value of x such that 0 < x < 1 .  Using elementary calculus, we determine that  d dx  I X; Y   =  log2  1 2  1  cid:0  x=2 x=2  cid:0  1 ;  which is equal to zero for x = 2=5 .  It is reasonable that Pr X = 1  < 1=2 because X = 1 is the noisy input to the channel.  So the capacity of the Z-channel in bits is H 1=5   cid:0  2=5 = 0:722  cid:0  0:4 = 0:322 .  9. Suboptimal codes. For the Z channel of the previous problem, assume that we choose a  2nR; n  code at random, where each codeword is a sequence of fair coin tosses. This will not achieve capacity. Find the maximum rate R such that the probability of error P  n  , averaged over the randomly generated codes, tends to zero as the block length n e tends to in cid:12 nity.  Solution: Suboptimal codes. From the proof of the channel coding theorem, it follows that using a random code with codewords generated according to probability p x  , we can send information at a rate I X; Y   corresponding to that p x  with an arbitrarily low probability of error. For the Z channel described in the previous problem, we can calculate I X; Y   for a uniform distribution on the input. The distribution on Y is  3 4, 1 4 , and therefore  I X; Y   = H Y    cid:0  H Y jX  = H   3 4  ;  1 4     cid:0   1 2  H     =  1 2  ;  1 2  3 2  cid:0   3 4  log 3:   7.34   10. Zero-error capacity. A channel with alphabet f0; 1; 2; 3; 4g has transition probabil-  ities of the form  p yjx  =  1=2 if y = x  cid:6  1 mod 5  otherwise.  0   a  Compute the capacity of this channel in bits.   b  The zero-error capacity of a channel is the number of bits per channel use that can be transmitted with zero probability of error. Clearly, the zero-error capacity of this pentagonal channel is at least 1 bit  transmit 0 or 1 with probability 1 2 . Find a block code that shows that the zero-error capacity is greater than 1 bit. Can you estimate the exact value of the zero-error capacity?  Hint: Consider codes of length 2 for this channel.  The zero-error capacity of this channel was  cid:12 nally found by Lovasz[8].   170  Channel Capacity  Solution: Zero-error capacity.   a  Since the channel is symmetric, it is easy to compute its capacity:  H Y jX  = 1 I X; Y   = H Y    cid:0  H Y jX  = H Y    cid:0  1 :  So mutual information is maximized when Y is uniformly distributed, which oc- curs when the input X is uniformly distributed. Therefore the capacity in bits is C = log2 5  cid:0  1 = log2 2:5 = 1:32 .   b  Let us construct a block code consisting of 2-tuples. We need more than 4 code- words in order to achieve capacity greater than 2 bits, so we will pick 5 codewords with distinct  cid:12 rst symbols: f0a; 1b; 2c; 3d; 4eg . We must choose a; b; c; d; e so that the receiver will be able to determine which codeword was transmitted. A sim- ple repetition code will not work, since if, say, 22 is transmitted, then 11 might be received, and the receiver could not tell whether the codeword was 00 or 22. Instead, using codewords of the form  i+1 mod 5, 2i+1 mod 5  yields the code 11,23,30,42,04.  Here is the decoding table for the pentagon channel:  0 4 0 . 4 3 . 2 3 2 0 1 0 1 . 3 4 . 3 4 . 1 2 1 2  It is amusing to note that the  cid:12 ve pairs that cannot be received are exactly the 5 codewords.  Then whenever xy is received, there is exactly one possible codeword.  Each codeword will be received as one of 4 possible 2-tuples; so there are 20 possible received 2-tuples, out of a total of 25.  Since there are 5 possible error-free messages with 2 channel uses, the zero-error capacity of this channel is at least 1 2 log2 5 = 1:161 bits.  In fact, the zero-error capacity of this channel is exactly 1 2 log2 5 . This result was obtained by L cid:19 aszl cid:19 o Lov cid:19 asz, \On the Shannon capacity of a graph," IEEE Transactions on Information Theory, Vol IT-25, pp. 1{7, January 1979. The  cid:12 rst results on zero-error capacity are due to Claude E. Shannon, \The zero- error capacity of a noisy channel," IEEE Transactions on Information Theory, Vol IT-2, pp. 8{19, September 1956, reprinted in Key Papers in the Development of Information Theory, David Slepian, editor, IEEE Press, 1974.  11. Time-varying channels. Consider a time-varying discrete memoryless channel. Let Y1; Y2; : : : ; Yn be conditionally independent given X1; X2; : : : ; Xn; with conditional dis-  tribution given by p y j x  =Qn  i=1 pi yi j xi :   Channel Capacity  171  0  Q  Q  Q  1  cid:0  pi  -  cid:17 3  0   cid:17    cid:17    cid:17   Q  Q  Q   cid:17    cid:17    cid:17    cid:17   Q  cid:17    cid:17  Q   cid:17  Q   cid:17   Q   cid:17  pi pi Q  1  cid:0  pi   cid:17    cid:17   1  Q  Q  Q  - Qs  1  Let X =  X1; X2; : : : ; Xn ; Y =  Y1; Y2; : : : ; Yn : Find maxp x  I X; Y : Solution: Time-varying channels.  We can use the same chain of inequalities as in the proof of the converse to the channel coding theorem. Hence  since by the de cid:12 nition of the channel, Yi depends only on Xi and is conditionally independent of everything else. Continuing the series of inequalities, we have  I X n; Y n  = H Y n   cid:0  H Y njX n   = H Y n   cid:0   H YijY1; : : : ; Yi cid:0 1; X n   n  n  Xi=1 Xi=1  = H Y n   cid:0   H YijXi ;  I X n; Y n  = H Y n   cid:0   n  n  Xi=1 H YijXi  Xi=1 H Yi   cid:0   1  cid:0  h pi  ;  H YijXi    cid:20    cid:20   n  n  Xi=1 Xi=1  with equality if X1; X2; : : : ; Xn is chosen i.i.d.  cid:24  Bern 1 2 . Hence  max p x   I X1; X2; : : : ; Xn; Y1; Y2; : : : ; Yn  =   1  cid:0  h pi  :   7.41   12. Unused symbols. Show that the capacity of the channel with probability transition  matrix  n  Xi=1  3 75  Pyjx =2 64  2=3 1=3 0 1=3 1=3 1=3 1=3 2=3  0   7.35    7.36    7.37    7.38    7.39    7.40    7.42    172  Channel Capacity  is achieved by a distribution that places zero probability on one of input symbols. What is the capacity of this channel? Give an intuitive reason why that letter is not used.  Solution: Unused symbols Let the probabilities of the three input symbols be p1; p2 and p3 . Then the probabilities of the three output symbols can be easily calculated to be   2  3 p1 + 1  3 p2; 1  3 ; 1  3 p2 + 2  3 p3  , and therefore  I X; Y   = H Y    cid:0  H Y jX  1 ; 3  = H   p1 +  2 3 1 3  p2;  1 1 3 3  p1  cid:0  p3 ;  1 3  p2 +  2 3 1 3  cid:0   1 3  ;  2 3  1 3  ;  p3   cid:0   p1 + p3 H  1 3   p1  cid:0  p3    cid:0   p1 + p3 H      cid:0  p2 log 3 1 3  2 3  ;  = H   +   7.43    7.44      cid:0   1  cid:0  p1  cid:0  p3  log 3  7.45   where we have substituted p2 = 1 cid:0  p1  cid:0  p3 . Now if we  cid:12 x p1 + p3 , then the second and third terms are  cid:12 xed, and the  cid:12 rst term is maximized if p1  cid:0  p3 = 0 , i.e., if p1 = p3 .  The same conclusion can be drawn from the symmetry of the problem.   Now setting p1 = p3 , we have  I X; Y   = H   1 3  ;  1 3  ;  1 3     cid:0   p1 + p3 H      cid:0   1  cid:0  p1  cid:0  p3  log 3  2 3  ;  1 3  2 = log 3  cid:0   p1 + p3 H  3 2 =  p1 + p3  log 3  cid:0  H  3  ;  1    cid:0   1  cid:0  p1  cid:0  p3  log 3 3 1    3  ;   7.46    7.47    7.48   3 ; 1  2 ; 0; 1  3 bits.  2   . The capacity of this channel = log 3 cid:0  H  2  which is maximized if p1 + p3 is as large as possible  since log 3 > H  2 3    . There- fore the maximizing distribution corresponds to p1 + p3 = 1 , p1 = p3 , and therefore  p1; p2; p3  =   1 3   = log 3 cid:0   log 3 cid:0  3   = 2 2 The intuitive reason why p2 = 0 for the maximizing distribution is that conditional on the input being 2, the output is uniformly distributed. The same uniform output distribution can be achieved without using the symbol 2  by setting p1 = p3  , and therefore the use of symbol 2 does not add any information  it does not change the entropy of the output and the conditional entropy H Y jX = 2  is the maximum possible, i.e., log 3 , so any positive probability for symbol 2 will only reduce the mutual information.  3 ; 1  Note that not using a symbol is optimal only if the uniform output distribution can be achieved without use of that symbol. For example, in the Z channel example above, both symbols are used, even though one of them gives a conditionally uniform distribution on the output.  13. Erasures and errors in a binary channel. Consider a channel with binary inputs that has both erasures and errors. Let the probability of error be  cid:15  and the probability of erasure be  cid:11  , so the the channel is as illustrated below:   Channel Capacity  173  0  Q S Q Q S S  1  cid:0   cid:11   cid:0   cid:15   Q  Q  S  S  Q  Q  S   cid:15  S  S  Q  Q   cid:19    cid:19  Q  cid:19  Q  cid:19   Q  0  -  cid:19 7   cid:19    cid:19    cid:19   S   cid:19   S  cid:19    cid:19  S   cid:19   S  Q   cid:11  Q   cid:17   cid:11    cid:17   Qs  cid:17 3  e   cid:17   S  cid:17  S  cid:17  S  S   cid:17    cid:17    cid:19   cid:19   cid:15    cid:17    cid:17    cid:19    cid:19    cid:17    cid:17   1  cid:0   cid:11   cid:0   cid:15   S  S  S Sw -  1   cid:19   cid:19   cid:17   cid:19   cid:17   cid:19   cid:17   1   a  Find the capacity of this channel.   b  Specialize to the case of the binary symmetric channel    cid:11  = 0  .   c  Specialize to the case of the binary erasure channel    cid:15  = 0  .  Solution:  to be  cid:25  and 1  cid:0   cid:25  . Then   a  As with the examples in the text, we set the input distribution for the two inputs  I X; Y    C = max p x  = max p x  = max p x    H Y    cid:0  H Y jX   H Y    cid:0  H 1  cid:0   cid:15   cid:0   cid:11 ;  cid:11 ;  cid:15  :   7.49    7.50    7.51   As in the case of the erasure channel, the maximum value for H Y   cannot be log 3 , since the probability of the erasure symbol is  cid:11  independent of the input distribution. Thus,  H Y   = H  cid:25  1  cid:0   cid:15   cid:0   cid:11   +  1  cid:0   cid:25   cid:15 ;  cid:11 ;  1  cid:0   cid:25   1  cid:0   cid:15   cid:0   cid:11   +  cid:25  cid:15    = H  cid:11   +  1  cid:0   cid:11  H cid:18   cid:25  +  cid:15   cid:0   cid:25  cid:11   cid:0  2 cid:25  cid:15   cid:20  H  cid:11   +  1  cid:0   cid:11   with equality i cid:11   cid:25 + cid:15  cid:0  cid:25  cid:11  cid:0 2 cid:25  cid:15  1 cid:0  cid:11  fact that  cid:25  = 1  cid:0   cid:25  = 1 symmetry of the problem, even though the channel is not weakly symmetric.   2 .  The 2 is the optimal distribution should be obvious from the  2 , which can be achieved by setting  cid:25  = 1  1  cid:0   cid:25   cid:0   cid:15  + 2 cid:15  cid:25   cid:0   cid:11  +  cid:11  cid:25    cid:19  7.53   1  cid:0   cid:11   1  cid:0   cid:11    7.54    7.52   = 1  ;   174  Channel Capacity  Therefore the capacity of this channel is  C = H  cid:11   + 1  cid:0   cid:11   cid:0  H 1  cid:0   cid:11   cid:0   cid:15 ;  cid:11 ;  cid:15    = H  cid:11   + 1  cid:0   cid:11   cid:0  H  cid:11    cid:0   1  cid:0   cid:11  H cid:18  1  cid:0   cid:11   cid:0   cid:15  1  cid:0   cid:11  =  1  cid:0   cid:11   cid:18 1  cid:0  H cid:18  1  cid:0   cid:11   cid:0   cid:15  1  cid:0   cid:11   1  cid:0   cid:11  cid:19  cid:19    cid:15   ;  ;   cid:15   1  cid:0   cid:11  cid:19    b  Setting  cid:11  = 0 , we get  which is the capacity of the binary symmetric channel.   c  Setting  cid:15  = 0 , we get  C = 1  cid:0  H  cid:15  ;  C = 1  cid:0   cid:11   which is the capacity of the binary erasure channel.   7.55    7.56    7.57    7.58    7.59   14. Channels with dependence between the letters. Consider the following channel over a binary alphabet that takes in two bit symbols and produces a two bit output, as determined by the following mapping: 00 ! 01 , 01 ! 10 , 10 ! 11 , and 11 ! 00 . Thus if the two bit sequence 01 is the input to the channel, the output is 10 with probability 1. Let X1; X2 denote the two input symbols and Y1; Y2 denote the corresponding output symbols.   a  Calculate the mutual information I X1; X2; Y1; Y2  as a function of the input  distribution on the four possible pairs of inputs.   b  Show that the capacity of a pair of transmissions on this channel is 2 bits.   c  Show that under the maximizing input distribution, I X1; Y1  = 0 .  Thus the distribution on the input sequences that achieves capacity does not nec- essarily maximize the mutual information between individual symbols and their corresponding outputs.  Solution:   a  If we look at pairs of inputs and pairs of outputs, this channel is a noiseless four input four output channel. Let the probabilities of the four input pairs be p00; p01; p10 and p11 respectively. Then the probability of the four pairs of output bits is p11; p00; p01 and p10 respectively, and  I X1; X2; Y1; Y2  = H Y1; Y2   cid:0  H Y1; Y2jX1; X2   = H Y1; Y2   cid:0  0 = H p11; p00; p01; p10    7.60    7.61    7.62    Channel Capacity  175   7.63    b  The capacity of the channel is achieved by a uniform distribution over the inputs,  which produces a uniform distribution on the output pairs  C = max p x1;x2   I X1; X2; Y1; Y2  = 2 bits  and the maximizing p x1; x2  puts probability 1 and 11.  4 on each of the pairs 00, 01, 10   c  To calculate I X1; Y1  , we need to calculate the joint distribution of X1 and Y1 . The joint distribution of X1X2 and Y1Y2 under an uniform input distribution is given by the following matrix X1X2nY1Y2 00 01 10 11  01 1 4 0 0 0  11 0 0 1 4 0  10 0 1 4 0 0  00 0 0 0 1 4  From this, it is easy to calculate the joint distribution of X1 and Y1 as X1nY1 0 1  0 1 4 1 4  1 1 4 1 4  and therefore we can see that the marginal distributions of X1 and Y1 are both  1=2; 1=2  and that the joint distribution is the product of the marginals, i.e., X1 is independent of Y1 , and therefore I X1; Y1  = 0 . Thus the distribution on the input sequences that achieves capacity does not nec- essarily maximize the mutual information between individual symbols and their corresponding outputs.  15. Jointly typical sequences. As we did in problem 13 of Chapter 3 for the typical set for a single random variable, we will calculate the jointly typical set for a pair of random variables connected by a binary symmetric channel, and the probability of error for jointly typical decoding for such a channel.  0  Q  Q  Q   cid:17    cid:17    cid:17   1  Q  Q   cid:17    cid:17   Q   cid:17   Q  cid:17    cid:17  Q   cid:17  Q   cid:17   Q   cid:17  0.1 0.1 Q  0.9  0.9  -  cid:17 3  0   cid:17    cid:17    cid:17   Q  Q  Q  - Qs  1   176  Channel Capacity  We will consider a binary symmetric channel with crossover probability 0.1. The input distribution that achieves capacity is the uniform distribution, i.e., p x  =  1=2; 1=2  , which yields the joint distribution p x; y  for this channel is given by XnY 0 1  0.05 0.45  0.45 0.05  0  1  The marginal distribution of Y is also  1=2; 1=2  .   a  Calculate H X  , H Y   , H X; Y   and I X; Y   for the joint distribution above.   b  Let X1; X2; : : : ; Xn be drawn i.i.d. according the Bernoulli 1 2  distribution. Of the 2n possible input sequences of length n , which of them are typical, i.e., member of A n    X  for  cid:15  = 0:2 ? Which are the typical sequences in A n    Y   ?   cid:15    cid:15    c  The jointly typical set A n    cid:15    X; Y   is de cid:12 ned as the set of sequences that satisfy equations  7.35-7.37 . The  cid:12 rst two equations correspond to the conditions that xn and yn are in A n   Y   respectively. Consider the last condition, which can be rewritten to state that  cid:0  1 n log p xn; yn  2  H X; Y   cid:0  cid:15 ; H X; Y  + cid:15   . Let k be the number of places in which the sequence xn di cid:11 ers from yn   k is a function of the two sequences . Then we can write   X  and A n    cid:15    cid:15   An alternative way at looking at this probability is to look at the binary symmetric channel as in additive channel Y = X  cid:8  Z , where Z is a binary random variable that is equal to 1 with probability p , and is independent of X . In this case,  p xn; yn  =  p xi; yi   n  Yi=1 2 cid:19 n =  cid:18  1  =  0:45 n cid:0 k 0:05 k   1  cid:0  p n cid:0 kpk  p xn; yn  = p xn p ynjxn  = p xn p znjxn  = p xn p zn  2 cid:19 n =  cid:18  1   1  cid:0  p n cid:0 kpk   7.64    7.65    7.66    7.67    7.68    7.69    7.70   Show that the condition that  xn; yn  being jointly typical is equivalent to the condition that xn is typical and zn = yn  cid:0  xn is typical.   d  We now calculate the size of A n    Z  for n = 25 and  cid:15  = 0:2 . As in problem 13 of Chapter 3, here is a table of the probabilities and numbers of sequences of with k ones   cid:15    Channel Capacity  177   cid:0 n k cid:1 pk 1  cid:0  p n cid:0 k  cid:0  1  k 0 1 2 3 4 5 6 7 8 9 10 11 12   cid:0 n k cid:1   1 25 300 2300 12650 53130 177100 480700 1081575 2042975 3268760 4457400 5200300  0.071790 0.199416 0.265888 0.226497 0.138415 0.064594 0.023924 0.007215 0.001804 0.000379 0.000067 0.000010 0.000001  n log p xn  0.152003 0.278800 0.405597 0.532394 0.659191 0.785988 0.912785 1.039582 1.166379 1.293176 1.419973 1.546770 1.673567   Sequences with more than 12 ones are omitted since their total probability is negligible  and they are not in the typical set .  What is the size of the set A n    Z  ?   cid:15    e  Now consider random coding for the channel, as in the proof of the channel coding theorem. Assume that 2nR codewords X n 1 ; X n 2 ; : : : ; X n 2nR  are chosen uni- formly over the 2n possible binary sequences of length n . One of these codewords is chosen and sent over the channel. The receiver looks at the received sequence and tries to  cid:12 nd a codeword in the code that is jointly typical with the received sequence. As argued above, this corresponds to  cid:12 nding a codeword X n i  such that Y n  cid:0  X n i  2 A n   Z  . For a  cid:12 xed codeword xn i  , what is the probability that the received sequence Y n is such that  xn i ; Y n  is jointly typical?   cid:15    f  Now consider a particular received sequence y n = 000000 : : : 0 , say. Assume that we choose a sequence X n at random, uniformly distributed among all the 2n possible binary n -sequences. What is the probability that the chosen sequence is jointly typical with this yn ?  Hint: this is the probability of all sequences xn such that yn  cid:0  xn 2 A n    Z  .    cid:15    g  Now consider a code with 29 = 512 codewords of length 12 chosen at random, uniformly distributed among all the 2n sequences of length n = 25 . One of these codewords, say the one corresponding to i = 1 , is chosen and sent over the channel. As calculated in part  e , the received sequence, with high probability, is jointly typical with the codeword that was sent. What is probability that one or more of the other codewords  which were chosen at random, independently of the sent codeword  is jointly typical with the received sequence?  Hint: You could use the union bound but you could also calculate this probability exactly, using the result of part  f  and the independence of the codewords    h  Given that a particular codeword was sent, the probability of error  averaged over the probability distribution of the channel and over the random choice of other   178  Channel Capacity  codewords  can be written as  Pr Errorjxn 1  sent  =  Xyn:yncauses error  p ynjxn 1     7.71   There are two kinds of error: the  cid:12 rst occurs if the received sequence y n is not jointly typical with the transmitted codeword, and the second occurs if there is another codeword jointly typical with the received sequence. Using the result of the previous parts, calculate this probability of error. By the symmetry of the random coding argument, this does not depend on which codeword was sent.   cid:15   The calculations above show that average probability of error for a random code with 512 codewords of length 25 over the binary symmetric channel of crossover probability 0.1 is about 0.34. This seems quite high, but the reason for this is that the value of  cid:15  that we have chosen is too large. By choosing a smaller  cid:15  , and a larger n in the de cid:12 nitions of A n  , we can get the probability of error to be as small as we want, as long as the rate of the code is less than I X; Y    cid:0  3 cid:15  . Also note that the decoding procedure described in the problem is not optimal. The optimal decoding procedure is maximum likelihood, i.e., to choose the codeword that is closest to the received sequence. It is possible to calculate the average probability of error for a random code for which the decoding is based on an approximation to maximum likelihood decoding, where we decode a received sequence to the unique codeword that di cid:11 ers from the received sequence in  cid:20  4 bits, and declare an error otherwise. The only di cid:11 erence with the jointly typical decoding described above is that in the case when the codeword is equal to the received sequence! The average probability of error for this decoding scheme can be shown to be about 0.285.  Solution: Jointly typical set   a  Calculate H X  , H Y   , H X; Y   and I X; Y   for the joint distribution above. Solution: H X  = H Y   = 1 bit, H X; Y   = H X  + H Y jX  = 1 + H p  = 1 cid:0  0:9 log 0:9 cid:0 0:1 log 0:1 = 1+0:469 = 1:469 bits, and I X; Y   = H Y   cid:0 H Y jX  = 0:531 bits.   cid:15    X  for  cid:15  = 0:2 ? Which are the typical sequences in A n    b  Let X1; X2; : : : ; Xn be drawn i.i.d. according the Bernoulli 1 2  distribution. Of the 2n possible sequences of length n , which of them are typical, i.e., member of A n  Solution:In the case for the uniform distribution, every sequence has probability  1=2 n , and therefore for every sequence,  cid:0  1 n log p xn  = 1 = H X  , and therefore every sequence is typical, i.e., 2 A n  Similarly, every sequence yn is typical, i.e., 2 A n    Y   ?   X  .   Y   .   cid:15    cid:15    cid:15    c  The jointly typical set A n    X; Y   is de cid:12 ned as the set of sequences that satisfy equations  7.35-7.37  of EIT. The  cid:12 rst two equations correspond to the conditions that xn and yn are in A n   Y   respectively. Consider the last   X  and A n    cid:15    cid:15    cid:15    Channel Capacity  condition, which can be rewritten to state that  cid:0  1 n log p xn; yn  2  H X; Y    cid:0   cid:15 ; H X; Y   +  cid:15   . Let k be the number of places in which the sequence xn di cid:11 ers from yn   k is a function of the two sequences . Then we can write  179   7.72    7.73    7.74    7.75    7.76    7.77    7.78    7.79    7.80    7.81    7.82    7.83    7.84   An alternative way at looking at this probability is to look at the binary symmetric channel as in additive channel Y = X  cid:8  Z , where Z is a binary random variable that is equal to 1 with probability p , and is independent of X . In this case,  p xn; yn  =  p xi; yi   n  Yi=1 2 cid:19 n =  cid:18  1  =  0:45 n cid:0 k 0:05 k   1  cid:0  p n cid:0 kpk  p xn; yn  = p xn p ynjxn  = p xn p znjxn  = p xn p zn  2 cid:19 n =  cid:18  1   1  cid:0  p n cid:0 kpk  Show that the condition that  xn; yn  being jointly typical is equivalent to the condition that xn is typical and zn = yn  cid:0  xn is typical. Solution:The conditions for  xn; yn  2 A n   X; Y   are   cid:15   A n    cid:15   = f xn; yn  2 X n  cid:2  Y n :  1 n 1 n 1 n   cid:12  cid:12  cid:12  cid:12  cid:0   cid:12  cid:12  cid:12  cid:12  cid:0   cid:12  cid:12  cid:12  cid:12  cid:0   <  cid:15 ;  log p xn   cid:0  H X  cid:12  cid:12  cid:12  cid:12  log p yn   cid:0  H Y   cid:12  cid:12  cid:12  cid:12  log p xn; yn   cid:0  H X; Y   cid:12  cid:12  cid:12  cid:12   <  cid:15 ;  <  cid:15 g;  1 n   cid:0   1 log p xn; yn  =  cid:0  n = 1  cid:0   2 cid:19 n pk 1  cid:0  p n cid:0 k cid:19  log cid:18  cid:18  1 n  cid:0  k log 1  cid:0  p  log p  cid:0  n  k n  But, as argued above, every sequence xn and yn satis cid:12 es the  cid:12 rst two conditions. Thereofre, the only condition that matters is the last one. As argued above,  Thus the pair  xn; yn  is jointly typical i cid:11  j1 cid:0  k n log 1 cid:0 p  cid:0 H X; Y  j <  cid:15  , i.e., i cid:11  j  cid:0  k n log 1  cid:0  p   cid:0  H p j <  cid:15  , which is exactly the condition for zn = yn  cid:8  xn to be typical. Thus the set of jointly typical pairs  xn; yn  is the set such that the number of places in which xn di cid:11 ers from yn is close to np .  n log p  cid:0  n cid:0 k  n log p cid:0  n cid:0 k   180  Channel Capacity   d  We now calculate the size of A n    Z  for n = 25 and  cid:15  = 0:2 . As in problem 7 of Homework 4, here is a table of the probabilities and numbers of sequences of with k ones   cid:15   k cid:1  Pj cid:20 k cid:0 n  cid:0 n j cid:1   p xn  = pk 1  cid:0  p n cid:0 k   cid:0 n k cid:1 pk 1  cid:0  p n cid:0 k Cumul. pr.  cid:0  1  k 0 1 2 3 4 5 6 7 8 9 10 11 12  1 25 300 2300 12650 53130 177100 480700 1081575 2042975 3268760 4457400 5200300  1 26 326 2626 15276 68406 245506 726206 1807781 3850756 7119516 11576916 16777216  7.178975e-02 7.976639e-03 8.862934e-04 9.847704e-05 1.094189e-05 1.215766e-06 1.350851e-07 1.500946e-08 1.667718e-09 1.853020e-10 2.058911e-11 2.287679e-12 2.541865e-13  0.071790 0.199416 0.265888 0.226497 0.138415 0.064594 0.023924 0.007215 0.001804 0.000379 0.000067 0.000010 0.000001  0.071790 0.271206 0.537094 0.763591 0.902006 0.966600 0.990523 0.997738 0.999542 0.999920 0.999988 0.999998 0.999999  n log p xn  0.152003 0.278800 0.405597 0.532394 0.659191 0.785988 0.912785 1.039582 1.166379 1.293176 1.419973 1.546770 1.673567   cid:15    Z  ?   Sequences with more than 12 ones are omitted since their total probability is negligible  and they are not in the typical set .  What is the size of the set A n  Solution: H Z  = H 0:1  = 0:469 . Setting  cid:15  = 0:2 , the typical set for Z is the set sequences for which  cid:0  1 n log p zn  2  H Z   cid:0   cid:15 ; H Z  +  cid:15   =  0:269; 0:669  . Looking at the table above for n = 25 , it follows that the typical Z sequences are those with 1,2,3 or 4 ones. The total probability of the set A n  the size of this set is 15276  cid:0  1 = 15275 .   Z  = 0:902006  cid:0  0:071790 = 0:830216 and   cid:15    cid:15    e  Now consider random coding for the channel, as in the proof of the channel coding theorem. Assume that 2nR codewords X n 1 ; X n 2 ; : : : ; X n 2nR  are chosen uni- formly over the 2n possible binary sequences of length n . One of these codewords is chosen and sent over the channel. The receiver looks at the received sequence and tries to  cid:12 nd a codeword in the code that is jointly typical with the received sequence. As argued above, this corresponds to  cid:12 nding a codeword X n i  such that Y n  cid:0  X n i  2 A n   Z  . For a  cid:12 xed codeword xn i  , what is the probability that the received sequence Y n is such that  xn i ; Y n  is jointly typical? Solution:The easiest way to calculate this probability is to view the BSC as an additive channel Y = X  cid:8  Z , where Z is Bernoulli  p  . Then the probability that for a given codeword, xn i  , that the output Y n is jointly typical with it is equal to the probability that the noise sequence Z n is typical, i.e., in A n   Z  . The noise sequence is drawn i.i.d. according to the distribution  1  cid:0  p; p  , and as calculated above, the probability that the sequence is typical, i.e., Pr A n   Z   = 0:830216 . Therefore the probability that the received sequence is not jointly typical with the transmitted codeword is 0.169784.   cid:15    cid:15    Channel Capacity  181   cid:15    cid:15    Z  .    Z j  cid:3    1   f  Now consider a particular received sequence y n = 000000 : : : 0 , say. Assume that we choose a sequence X n at random, uniformly distributed among all the 2n possible binary n -sequences. What is the probability that the chosen sequence is jointly typical with this yn ?  Hint: this is the probability of all sequences xn such that yn  cid:0  xn 2 A n  Solution:Since all xn sequences are chosen with the same probability    1=2 n  , the probability that the xn sequence chosen is jointly typical with the received y n is equal to the number of possible jointly typical  xn; yn  pairs times  1=2 n . The number of sequences xn that are jointly typical with a given yn is equal to number of typical zn , where zn = xn  cid:8  yn . Thus the probability that a randomly chosen xn is typical with the given yn is jA n  2  n = 15275  cid:3  2 cid:0 25 = 4:552  cid:2  10 cid:0 4 .  g  Now consider a code with 29 = 512 codewords of length 12 chosen at random, uniformly distributed among all the 2n sequences of length n = 25 . One of these codewords, say the one corresponding to i = 1 , is chosen and sent over the channel. As calculated in part  e , the received sequence, with high probability, is jointly typical with the codeword that was sent. What is probability that one or more of the other codewords  which were chosen at random, independently of the sent codeword  is jointly typical with the received sequence?  Hint: You could use the union bound but you could also calculate this probability exactly, using the result of part  f  and the independence of the codewords  Solution:Each of the other codewords is jointly typical with received sequence with probability 4:552  cid:2  10 cid:0 4 , and each of these codewords is independent. The probability that none of the 511 codewords are jointly typical with the received sequence is therefore  1  cid:0  4:552  cid:2  10 cid:0 4 511 = 0:79241 , and the probability that at least one of them is jointly typical with the received sequence is therefore 1  cid:0  0:79241 = 0:20749 . Using the simple union of events bound gives the probability of another codeword being jointly typical with the received sequence to be 4:552 cid:2 10 cid:0 4 cid:2 511 = 0:23262 . The previous calculation gives the more exact answer.   h  Given that a particular codeword was sent, the probability of error  averaged over the probability distribution of the channel and over the random choice of other codewords  can be written as  Pr Errorjxn 1  sent  =  Xyn:yncauses error  p ynjxn 1     7.85   There are two kinds of error: the  cid:12 rst occurs if the received sequence y n is not jointly typical with the transmitted codeword, and the second occurs if there is another codeword jointly typical with the received sequence. Using the result of the previous parts, calculate this probability of error. By the symmetry of the random coding argument, this does not depend on which codeword was sent. Solution:There are two error events, which are conditionally independent, given the received sequence. In the previous part, we showed that the conditional proba-   182  Channel Capacity   cid:15   bility of error of the second kind was 0.20749, irrespective of the received sequence yn . The probability of error of the  cid:12 rst kind is 0.1698, conditioned on the input code-  X; Y   , but this was conditioned on a particular input sequence. Now by the symmetry and uniformity of the random code construction, this probability does not depend  X; Y   is also equal  word. In part  e , we calculated the probability that  xn i ; Y n  =2 A n   on xn i  , and therefore the probability that  X n; Y n  =2 A n   to this probability, i.e., to 0.1698. We can therefore use a simple union of events bound to bound the total probability of error  cid:20  0:1698 + 0:2075 = 0:3773 . Thus we can send 512 codewords of length 25 over a BSC with crossover probability 0.1 with probability of error less than 0.3773. A little more accurate calculation can be made of the probability of error using the fact that conditioned on the received sequence, both kinds of error are independent. Using the symmetry of the code construction process, the probability of error of the  cid:12 rst kind conditioned on the received sequence does not depend on the received sequence, and is therefore = 0:1698 . Therefore the probability that neither type of error occurs is  using their independence  =  1  cid:0  0:1698  1  cid:0  0:2075  = 0:6579 and therefore, the probability of error is 1  cid:0  0:6579 = 0:3421   cid:15    cid:15   The calculations above show that average probability of error for a random code with 512 codewords of length 25 over the binary symmetric channel of crossover probability 0.1 is about 0.34. This seems quite high, but the reason for this is that the value of  cid:15  that we have chosen is too large. By choosing a smaller  cid:15  , and a larger n in the de cid:12 nitions of A n  , we can get the probability of error to be as small as we want, as long as the rate of the code is less than I X; Y    cid:0  3 cid:15  . Also note that the decoding procedure described in the problem is not optimal. The optimal decoding procedure is maximum likelihood, i.e., to choose the codeword that is closest to the received sequence. It is possible to calculate the average probability of error for a random code for which the decoding is based on an approximation to maximum likelihood decoding, where we decode a received sequence to the unique codeword that di cid:11 ers from the received sequence in  cid:20  4 bits, and declare an error otherwise. The only di cid:11 erence with the jointly typical decoding described above is that in the case when the codeword is equal to the received sequence! The average probability of error for this decoding scheme can be shown to be about 0.285.  16. Encoder and decoder as part of the channel: Consider a binary symmetric chan- nel with crossover probability 0.1. A possible coding scheme for this channel with two codewords of length 3 is to encode message a1 as 000 and a2 as 111. With this coding scheme, we can consider the combination of encoder, channel and decoder as forming a new BSC, with two inputs a1 and a2 and two outputs a1 and a2 .   a  Calculate the crossover probability of this channel.   Channel Capacity  nel?  183   b  What is the capacity of this channel in bits per transmission of the original chan-   c  What is the capacity of the original BSC with crossover probability 0.1?   d  Prove a general result that for any channel, considering the encoder, channel and decoder together as a new channel from messages to estimated messages will not increase the capacity in bits per transmission of the original channel.  Solution: Encoder and Decoder as part of the channel:   a  The probability of error with these 3 bits codewords was 2.8%, and thus the  crossover probability of this channel is 0.028.   b  The capacity of a BSC with crossover probability 0.028 is 1  cid:0  H 0:028  , i.e., 1- 0.18426 or 0.81574 bits for each 3 bit codeword. This corresponds to 0.27191 bits per transmission of the original channel.   c  The original channel had capacity 1  cid:0  H 0:1  , i.e., 0.531 bits transmission.  d  The general picture for the channel with encoder and decoder is shown below  W  - Message  Encoder  X n  -  Channel p yjx   Y n  -  Decoder  ^W  - Estimate  of  Message  By the data processing inequality, I W ; ^W    cid:20  I X n; Y n  , and therefore  CW =  1 n  max p w   I W ; ^W    cid:20   1 n  max p xn   I X n; Y n  = C   7.86   Thus the capacity of the channel per transmission is not increased by the addition of the encoder and decoder.  17. Codes of length 3 for a BSC and BEC: In Problem 16, the probability of error was calculated for a code with two codewords of length 3  000 and 111  sent over a binary symmetric channel with crossover probability  cid:15  . For this problem, take  cid:15  = 0:1 .   a  Find the best code of length 3 with four codewords for this channel. What is the probability of error for this code?  Note that all possible received sequences should be mapped onto possible codewords    b  What is the probability of error if we used all the 8 possible sequences of length 3  as codewords?   c  Now consider a binary erasure channel with erasure probability 0.1. Again, if we  used the two codeword code 000 and 111, then received sequences 00E,0E0,E00,0EE,E0E,EE0 would all be decoded as 0, and similarly we would decode 11E,1E1,E11,1EE,E1E,EE1   184  Channel Capacity  as 1. If we received the sequence EEE we would not know if it was a 000 or a 111 that was sent - so we choose one of these two at random, and are wrong half the time.  What is the probability of error for this code over the erasure channel?   d  What is the probability of error for the codes of parts  a  and  b  when used over  the binary erasure channel?  Solution: Codes of length 3 for a BSC and BEC:   a  To minimize the probability of confusion, the codewords should be as far apart as possible. With four codewords, the minimum distance is at most 2, and there are various sets of codewords that achieve this minimum distance. An example set is 000, 011, 110 and 101. Each of these codewords di cid:11 ers from the other codewords in at least two places.  To calculate the probability of error, we need to  cid:12 nd the best decoding rule, i.e,. we need to map all possible recieved sequences onto codewords. As argued in the previous homework, the best decoding rule assigns to each received sequence the nearest codeword, with ties being broken arbitrarily. Of the 8 possible received sequences, 4 are codewords, and each of the other 4 sequences has three codewords within distance one of them. We can assign these received sequences to any of the nearest codewords, or alternatively, for symmetry, we might toss a three sided coin on receiving the sequence, and choose one of the nearest codewords with probability  1 3, 1 3, 1 3 . All these decoding strategies will give the same average probability of error.  In the current example, there are 8 possible received sequences, and we will use the following decoder mapping 000, 001 ! 000; 011, 010 ! 011; 110, 100 ! 110; and 101, 111 ! 101. Under this symmetric mapping, the codeword and one received sequence at dis- tance 1 from the codeword are mapped on to the codeword. The probability there- fore that the codeword is decoded correctly is 0:9 cid:3 0:9 cid:3 0:9+0:9 cid:3 0:9 cid:3 0:1 = 0:81 and the probability of error  for each codeword  is 0.19. Thus the average probability of error is also 0.19.   b  If we use all possible input sequences as codewords, then we have an error if any of the bits is changed. The probability that all the three bits are received correctly is 0:9  cid:3  0:9  cid:3  0:9 = 0:729 and therefore the probability of error is 0.271.   c  There will be an error only if all three bits of the codeword are erased, and on receiveing EEE, the decoder choses the wrong codeword. The probability of re- ceiving EEE is 0.001 and conditioned on that, the probability of error is 0.5, so the probability of error for this code over the BEC is 0.0005.   d  For the code of part  a , the four codewords are 000, 011,110, and 101. We use  the following decoder mapping:   Channel Capacity  185  codeword 000 011 110 101 000 or 011 with prob. 0.5 000 or 110 with prob. 0.5  Received Sequences 000, 00E, 0E0, E00 011, 01E, 0E1, E11 110, 11E, 1E0, E10 101, 10E, 1E1, E01 0EE EE0 ... EE1 EEE  011 or 101 with prob. 0.5 000 or 011 or 110 or 101 with prob. 0.25  Essentially all received sequences with only one erasure can be decoded correctly. If there are two erasures, then there are two possible codewords that could have caused the received sequence, and the conditional probability of error is 0.5. If there are three erasures, any of the codewords could have caused it, and the conditional probability of error is 0.75. Thus the probability of error given that 000 was sent is the probability of two erasures times 0.5 plus the probability of 3 erasures times 0.75, i.e, 3  cid:3  0:9  cid:3  0:1  cid:3  0:1  cid:3  0:5 + 0:1  cid:3  0:1  cid:3  0:1  cid:3  0:75 = 0:01425 . This is also the average probability of error. If all input sequences are used as codewords, then we will be confused if there is any erasure in the received sequence. The conditional probability of error if there is one erasure is 0.5, two erasures is 0.75 and three erasures is 0.875  these corrospond to the numbers of other codewords that could have caused the received sequence . Thus the probability of error given any codeword is 3  cid:3  0:9  cid:3  0:9  cid:3  0:1  cid:3  0:5 + 3  cid:3  0:9  cid:3  0:1  cid:3  0:1  cid:3  0:75 + 0:1  cid:3  0:1  cid:3  0:1  cid:3  0:875 = 0:142625 . This is also the average probability of error.  18. Channel capacity: Calculate the capacity of the following channels with probability  transition matrices:   a  X = Y = f0; 1; 2g   b  X = Y = f0; 1; 2g   c  X = Y = f0; 1; 2; 3g  p yjx  =2 64 p yjx  =2 64  1=3 1=3 1=3 1=3 1=3 1=3 1=3 1=3 1=3  1=2 1=2  0 1=2 1=2 0 1=2  0 1=2  3 75 3 75  p yjx  =2 6664  p 1  cid:0  p 0 0  1  cid:0  p p 0 0  0 0 q 1  cid:0  q  0 0 1  cid:0  q q  3 7775   7.87    7.88    7.89    186  Channel Capacity   7.90    7.91    7.92    7.93    7.94    7.95   Solution: Channel Capacity:   a  X = Y = f0; 1; 2g  p yjx  =2 64  1=3 1=3 1=3 1=3 1=3 1=3 1=3 1=3 1=3  This is a symmetric channel and by the results of Section 8.2,  C = log jYj  cid:0  H r  = log 3  cid:0  log 3 = 0:  In this case, the output is independent of the input.   b  X = Y = f0; 1; 2g  p yjx  =2 64  1=2 1=2  0 1=2 1=2 0 1=2  0 1=2  Again the channel is symmetric, and by the results of Section 8.2,  C = log jYj  cid:0  H r  = log 3  cid:0  log = 0:58 bits   c  X = Y = f0; 1; 2; 3g  3 75  3 75  This channel consists of a sum of two BSC’s, and using the result of Problem 2 of Homework 9, the capacity of the channel is  p yjx  =2 6664  p 1  cid:0  p 0 0  1  cid:0  p p 0 0  0 0 q 1  cid:0  q  0 0 1  cid:0  q q  3 7775  C = log cid:16 21 cid:0 H p  + 21 cid:0 H q  cid:17   19. Capacity of the carrier pigeon channel. Consider a commander of an army be- sieged a fort for whom the only means of communication to his allies is a set of carrier pigeons. Assume that each carrier pigeon can carry one letter  8 bits , and assume that pigeons are released once every 5 minutes, and that each pigeon takes exactly 3 minutes to reach its destination.   a  Assuming all the pigeons reach safely, what is the capacity of this link in bits hour?   b  Now assume that the enemies try to shoot down the pigeons, and that they manage to hit a fraction  cid:11  of them. Since the pigeons are sent at a constant rate, the receiver knows when the pigeons are missing. What is the capacity of this link?   c  Now assume that the enemy is more cunning, and every time they shoot down a pigeon, they send out a dummy pigeon carrying a random letter  chosen uniformly from all 8-bit letters . What is the capacity of this link in bits hour?   Channel Capacity  187  Set up an appropriate model for the channel in each of the above cases, and indicate how to go about  cid:12 nding the capacity.  Solution: Capacity of the carrier pigeon channel.   a  The channel sends 8 bits every 5 minutes, or 96 bits hour.   b  This is the equivalent of an erasure channel with an input alphabet of 8 bit symbols, i.e., 256 di cid:11 erent symbols. For any symbols sent, a fraction  cid:11  of them are received as an erasure. We would expect that the capacity of this channel is  1  cid:0   cid:11  8 bits pigeon. We will justify it more formally by mimicking the derivation for the binary erasure channel. Consider a erasure channel with 256 symbol inputs and 257 symbol output - the extra symbol is the erasure symbol, which occurs with probability  cid:11  . Then  I X; Y   = H Y    cid:0  H Y jX  = H Y    cid:0  H  cid:11     7.96   since the probability of erasure is independent of the input. However, we cannot get H Y   to attain its maximum value, i.e., log 257 , since the probability of the erasure channel is  cid:11  independent of our input distribution. However, if we let E be the erasure event, then  H Y   = H Y; E  = H E  + H Y jE  = H  cid:11   +  cid:11  cid:2  0 +  1 cid:0   cid:11  H Y jE = 0   7.97  and we can maximize H Y   by maximizing H Y jE = 0  . However, H Y jE = 0  is just the entropy of the input distribution, and this is maximized by the uniform. Thus the maximum value of H Y   is H  cid:11   +  1  cid:0   cid:11   log 256 , and the capacity of this channel is  1  cid:0   cid:11   log 256 bits pigeon, or  1  cid:0   cid:11  96 bits hour, as we might have expected from intuitive arguments.   c  In this case, we have a symmetric channel with 256 inputs and 256 output. With probability  1  cid:0   cid:11   +  cid:11 =256 , the output symbol is equal to the input, and with probability  cid:11 =256 , it is transformed to any of the other 255 symbols. This channel is symmetric in the sense of Section 8.2, and therefore the capacity of the channel is  C = log jYj  cid:0  H r   = log 256  cid:0  H 1  cid:0   cid:11  +  cid:11 =256;  cid:11 =256;  cid:11 =256; : : : ;  cid:11 =256  = 8  cid:0  H 1  cid:0   cid:11 H 1=255; 1=255; : : : ; 1=255  = 8  cid:0  H 1  cid:0    cid:11    cid:0   cid:11    cid:0   255 256 255 256  255 256 255 256   cid:11  log 255   7.98    7.99    7.100    7.101   We have to multiply this by 12 to get the capacity in bits hour.  20. A channel with two independent looks at Y.  Let Y1 and Y2 be conditionally  independent and conditionally identically distributed given X:   a  Show I X; Y1; Y2  = 2I X; Y1   cid:0  I Y1; Y2 :   188  Channel Capacity   b  Conclude that the capacity of the channel  -  -  X  X  -   Y1; Y2   -  Y1  is less than twice the capacity of the channel  Solution: A channel with two independent looks at Y   a   I X; Y1; Y2  = H Y1; Y2   cid:0  H Y1; Y2jX    7.102   = H Y1  + H Y2   cid:0  I Y1; Y2   cid:0  H Y1jX   cid:0  H Y2jX    7.103   since Y1 and Y2 are conditionally independent given X  7.104   7.105   since Y1 and Y2 are conditionally iden- :  7.106  tically distributed   = I X; Y1  + I X; Y2   cid:0  I Y1; Y2  = 2I X; Y1   cid:0  I Y1; Y2    b  The capacity of the single look channel X ! Y1 is I X; Y1 :  C1 = max p x   The capacity of the channel X !  Y1; Y2  is  I X; Y1; Y2   2I X; Y1   cid:0  I Y1; Y2  2I X; Y1   C2 = max p x  = max p x    cid:20  max p x  = 2C1:   7.107    7.108    7.109    7.110    7.111   Hence, two independent looks cannot be more than twice as good as one look.  21. Tall, fat people  Suppose that average height of people in a room is 5 feet. Suppose the average weight is 100 lbs.   a  Argue that no more than 1  b  Find an upper bound on the fraction of 300 lb, 10 footers in the room.  3 of the population is 15 feet tall.  Solution:  Tall, fat people.   Channel Capacity   a  The average height of the individuals in the population is 5 feet. So 1  is the height of the i -th person. If more 3 of the population is at least 15 feet tall, then the average will be greater 3 15 = 5 feet since each person is at least 0 feet tall. Thus no more than 1 3  where n is the population size and hi than 1 than 1 of the population is 15 feet tall.   b  By the same reasoning as in part  a , at most 1  3 of the population weighs 300 lbs. Therefore at most 1  2 of the poplulation is 10 feet tall 3 are both  and at most 1 10 feet tall and weigh 300 lbs.  189  nP hi = 5  22. Can signal alternatives lower capacity? Show that adding a row to a channel  transition matrix does not decrease capacity.  Solution: Can signal alternatives lower capacity? Adding a row to the channel transition matrix is equivalent to adding a symbol to the input alphabet X . Suppose there were m symbols and we add an  m + 1  -st. We can always choose not to use this extra symbol.  Speci cid:12 cally, let Cm and Cm+1 denote the capacity of the original channel and the new channel, respectively. Then  Cm+1 =  max  I X; Y    p x1;:::;xm+1   max  I X; Y    p x1;:::;xm;0    cid:21  = Cm:  23. Binary multiplier channel   a  Consider the channel Y = XZ where X and Z are independent binary random variables that take on values 0 and 1. Z is Bernoulli   cid:11   , i.e. P  Z = 1  =  cid:11  . Find the capacity of this channel and the maximizing distribution on X .   b  Now suppose the receiver can observe Z as well as Y . What is the capacity?  Solution: Binary Multiplier Channel   a  Let P  X = 1  = p . Then P  Y = 1  = P  X = 1 P  Z = 1  =  cid:11 p .  I X; Y   = H Y    cid:0  H Y jX   = H Y    cid:0  P  X = 1 H Z  = H  cid:11 p   cid:0  pH  cid:11   maximizes I X; Y   . The capacity is calculated to  We  cid:12 nd that p cid:3  =  1  H  cid:11     cid:11  2   cid:11  +1   be log 2  H  cid:11     cid:11  + 1   cid:0  H  cid:11    cid:11  .   190  Channel Capacity   b  Let P  X = 1  = p . Then  I X; Y; Z  = I X; Z  + I X; Y jZ  = H Y jZ   cid:0  H Y jX; Z  = H Y jZ  =  cid:11 H p   The expression is maximized for p = 1=2 , resulting in C =  cid:11  . Intuitively, we can only get X through when Z is 1, which happens  cid:11  of the time.  24. Noise alphabets  Consider the channel  Z  X  ?   cid:19  cid:16  cid:6 -  cid:18  cid:17   -  Y  X = f0; 1; 2; 3g , where Y = X + Z , and Z is uniformly distributed over three distinct integer values Z = fz1; z2; z3g:  a  What is the maximum capacity over all choices of the Z alphabet? Give distinct  integer values z1; z2; z3 and a distribution on X achieving this.   b  What is the minimum capacity over all choices for the Z alphabet? Give distinct  integer values z1; z2; z3 and a distribution on X achieving this.  Solution: Noise alphabets  4 ; 1 4 ; 1  a  Maximum capacity is C = 2 bits. Z = f10; 20; 30g and p X  =   1  b  Minimum capacity is C = 1 bit. Z = f0; 1; 2g and p X  =   1 2 ; 0; 0; 1 2   .  4 ; 1  4   .  25. Bottleneck channel  Suppose a signal X 2 X = f1; 2; : : : ; mg goes through an intervening transition X  cid:0 ! V  cid:0 ! Y :  X  p vjx   V  p yjv   Y   Channel Capacity  191  where x = f1; 2; : : : ; mg , y = f1; 2; : : : ; mg , and v = f1; 2; : : : ; kg . Here p vjx  and p yjv  are arbitrary and the channel has transition probability p yjx  = Pv p vjx p yjv  . Show C  cid:20  log k . Solution: Bottleneck channel  The capacity of the cascade of channels is C = maxp x  I X; Y   . By the data processing inequality, I X; Y    cid:20  I V ; Y   = H V    cid:0  H V jY    cid:20  H V    cid:20  log k .  26. Noisy typewriter. Consider the channel with x; y 2 f0; 1; 2; 3g and transition prob-  abilities p yjx  given by the following matrix: 0  1 2  1 2 0 0  1 2 1 2 0  1 2  0 0 1 2  2 66664  0  0 1 2 1 2  3 77775   a  Find the capacity of this channel.   b  De cid:12 ne the random variable z = g y  where  y 2 f0; 1g y 2 f2; 3g For the following two PMFs for x , compute I X; Z   g y  =  A if  B if  :  i.  ii.  p x  =  1  2 0  if x 2 f1; 3g if x 2 f0; 2g  p x  =  0  1 2  if x 2 f1; 3g if x 2 f0; 2g   c  Find the capacity of the channel between x and z , speci cid:12 cally where x 2 f0; 1; 2; 3g ,  z 2 fA; Bg , and the transition probabilities P  zjx  are given by  p Z = zjX = x  = Xg y0 =z  P  Y = y0jX = x    d  For the X distribution of part i. of b , does X ! Z ! Y form a Markov chain? Solution: Noisy typewriter   a  This is a noisy typewriter channel with 4 inputs, and is also a symmetric channel. Capacity of the channel by Theorem 7.2.1 is log 4  cid:0  1 = 1 bit per transmission.   192  Channel Capacity   b   i. The resulting conditional distribution of Z given X is  If  then it is easy to calculate H ZjX  = 0 , and I X; Z  = 1 . If  p x  =  0 then H ZjX  = 1 and I X; Y   = 0 .  1 2  ii. Since I X; Z   cid:20  H Z   cid:20  1 , the capacity of the channel is 1, achieved by the  input distribution  0  1 2 1 1 2  3 77775  1  1 2 0 1 2  2 66664 p x  =  1  2 0  if x 2 f1; 3g if x 2 f0; 2g  if x 2 f1; 3g if x 2 f0; 2g  p x  =  1  2 0  if x 2 f1; 3g if x 2 f0; 2g   c  For the input distribution that achieves capacity, X $ Z is a one-to-one func- tion, and hence p x; z  = 1 or 0 . We can therefore see the that p x; y; z  = p z; x p yjx; z  = p z; x p yjz  , and hence X ! Z ! Y forms a Markov chain.  27. Erasure channel  Let fX ; p yjx ;Yg be a discrete memoryless channel with capacity C . Suppose this channel is immediately cascaded with an erasure channel fY; p sjy ;Sg that erases  cid:11  of its symbols.  X  p yjx   Z  Z  Y  S  Z  Z  Z  HHHHHHHH XXXXXXXX Z e  Z  Z  Speci cid:12 cally, S = fy1; y2; : : : ; ym; eg; and  PrfS = yjX = xg =  cid:22  cid:11 p yjx ; y 2 Y; PrfS = ejX = xg =  cid:11 :  Determine the capacity of this channel.  Solution: Erasure channel   193   7.112    7.113    7.114    7.115    7.116    7.117   Channel Capacity  The capacity of the channel is  C = max p x   I X; S   De cid:12 ne a new random variable Z , a function of S , where Z = 1 if S = e and Z = 0 otherwise. Note that p Z = 1  =  cid:11  independent of X . Expanding the mutual information,  I X; S  = H S   cid:0  H SjX   = H S; Z   cid:0  H S; ZjX  + H Z  + H SjZ   cid:0  H ZjX   cid:0  H SjX; Z  = I X; Z  + I S; XjZ  = 0 +  cid:11 I X; SjZ = 1  +  1  cid:0   cid:11  I X; SjZ = 0   When Z = 1 , S = e and H SjZ = 1  = H SjX; Z = 1  = 0 . When Z = 0 , S = Y , and I X; SjZ = 0  = I X; Y   . Thus  I X; S  =  1  cid:0   cid:11  I X; Y     7.118   and therefore the capacity of the cascade of a channel with an erasure channel is  1 cid:0  cid:11   times the capacity of the original channel.  28. Choice of channels.  Find the capacity C of the union of 2 channels  X1; p1 y1jx1 ;Y1  and  X2; p2 y2jx2 ;Y2  where, at each time, one can send a symbol over channel 1 or over channel 2 but not both. Assume the output alphabets are distinct and do not intersect.   a  Show 2C = 2C1 + 2C2: Thus 2C is the e cid:11 ective alphabet size of a channel with  capacity C .   b  Compare with problem 10 of Chapter 2 where 2H = 2H1 + 2H2 , and interpret  a   in terms of the e cid:11 ective number of noise-free symbols.   c  Use the above result to calculate the capacity of the following channel   194  Channel Capacity  0  Q  Q  Q  1  cid:0  p  -  cid:17 3  0   cid:17    cid:17    cid:17   Q  Q  Q   cid:17    cid:17    cid:17    cid:17   Q  cid:17    cid:17  Q   cid:17  Q   cid:17   Q   cid:17  p p Q  1  cid:0  p   cid:17    cid:17   1  Q  Q  Q  - Qs  1  2  1  - 2  Solution: Choice of Channels  a  This is solved by using the very same trick that was used to solve problem 2.10.  Consider the following communication scheme:  Let  Since the output alphabets Y1 and Y2 are disjoint,  cid:18  is a function of Y as well, i.e. X ! Y !  cid:18  .  X =  X1 Probability  cid:11   X2 Probability  1  cid:0   cid:11    cid:18  X  =  1 X = X1  2 X = X2  I X; Y;  cid:18   = I X;  cid:18   + I X; Y j cid:18   = I X; Y   + I X;  cid:18 jY    Since X ! Y !  cid:18  , I X;  cid:18 jY   = 0 . Therefore, I X; Y   = I X;  cid:18   + I X; Y j cid:18    = H  cid:18    cid:0  H  cid:18 jX  +  cid:11 I X1; Y1  +  1  cid:0   cid:11  I X2; Y2  = H  cid:11   +  cid:11 I X1; Y1  +  1  cid:0   cid:11  I X2; Y2   Thus, it follows that  C = sup   cid:11  fH  cid:11   +  cid:11 C1 +  1  cid:0   cid:11  C2g :   Channel Capacity  195  Maximizing over  cid:11  one gets the desired result. The maximum occurs for H 0  cid:11   + C1  cid:0  C2 = 0 , or  cid:11  = 2C1= 2C1 + 2C2  .  b  If one interprets M = 2C as the e cid:11 ective number of noise free symbols, then the above result follows in a rather intuitive manner: we have M1 = 2C1 noise free symbols from channel 1, and M2 = 2C2 noise free symbols from channel 2. Since at each step we get to chose which channel to use, we essentially have M1 + M2 = 2C1 + 2C2 noise free symbols for the new channel. Therefore, the capacity of this channel is C = log2 2C1 + 2C2  . This argument is very similiar to the e cid:11 ective alphabet argument given in Problem 10, Chapter 2 of the text.  29. Binary multiplier channel.   a  Consider the discrete memoryless channel Y = XZ where X and Z are inde- pendent binary random variables that take on values 0 and 1. Let P  Z = 1  =  cid:11  . Find the capacity of this channel and the maximizing distribution on X .   b  Now suppose the receiver can observe Z as well as Y . What is the capacity?  Solution: Binary Multiplier Channel  Repeat of problem 7.23    a  Let P  X = 1  = p . Then P  Y = 1  = P  X = 1 P  Z = 1  =  cid:11 p .  I X; Y   = H Y    cid:0  H Y jX   = H Y    cid:0  P  X = 1 H Z  = H  cid:11 p   cid:0  pH  cid:11   maximizes I X; Y   . The capacity is calculated to  We  cid:12 nd that p cid:3  =  1  H  cid:11     cid:11  2   cid:11  +1   H  cid:11    be log 2   cid:11  + 1   cid:0  H  cid:11    cid:11  .  b  Let P  X = 1  = p . Then  I X; Y; Z  = I X; Z  + I X; Y jZ  = H Y jZ   cid:0  H Y jX; Z  = H Y jZ  =  cid:11 H p   The expression is maximized for p = 1=2 , resulting in C =  cid:11  . Intuitively, we can only get X through when Z is 1, which happens  cid:11  of the time.  30. Noise alphabets.  Consider the channel   196  Channel Capacity  Z  X  ?   cid:19  cid:16  cid:6 -  cid:18  cid:17   -  Y  X = f0; 1; 2; 3g , where Y = X + Z , and Z is uniformly distributed over three distinct integer values Z = fz1; z2; z3g:  a  What is the maximum capacity over all choices of the Z alphabet? Give distinct  integer values z1; z2; z3 and a distribution on X achieving this.   b  What is the minimum capacity over all choices for the Z alphabet? Give distinct  integer values z1; z2; z3 and a distribution on X achieving this.  Solution: Noise alphabets  Repeat of problem 7.24   4 ; 1 4 ; 1  a  Maximum capacity is C = 2 bits. Z = f10; 20; 30g and p X  =   1 2 ; 0; 0; 1  b  Minimum capacity is C = 1 bit. Z = f0; 1; 2g and p X  =   1 2   .  4 ; 1  4   .  31. Source and channel.  We wish to encode a Bernoulli   cid:11    process V1; V2; : : : for transmission over a binary symmetric channel with crossover probability p .  V n - X n V n  -  -Y n  -^V n  Q   cid:17   -  cid:17 3  - Qs  1  cid:0  p  cid:17  Q p Q  cid:17   cid:17  Q p Q  cid:17  1  cid:0  p  Find conditions on  cid:11  and p so that the probability of error P   ^V n 6= V n  can be made to go to zero as n  cid:0 ! 1 . Solution: Source And Channel Suppose we want to send a binary i.i.d. Bernoulli   cid:11    source over a binary symmetric channel with error probability p .  By the source-channel separation theorem, in order to achieve an error rate that vanishes asymptotically, P   ^V n 6= V n  ! 0 , we need the entropy of the source to be smaller than the capacity of the channel. In this case this translates to  or, equivalently,  H  cid:11   + H p  < 1;   cid:11  cid:11  1  cid:0   cid:11  1 cid:0  cid:11 pp 1  cid:0  p 1 cid:0 p <  1 2  :   Channel Capacity  32. Random \20" questions  197  Let X be uniformly distributed over f1; 2; : : : ; mg . Assume m = 2n . We ask random questions: Is X 2 S1 ? Is X 2 S2 ?...until only one integer remains. All 2m subsets S of f1; 2; : : : ; mg are equally likely.  a  How many deterministic questions are needed to determine X ?   b  Without loss of generality, suppose that X = 1 is the random object. What is the probability that object 2 yields the same answers for k questions as object 1?  c  What is the expected number of objects in f2; 3; : : : ; mg that have the same  d  Suppose we ask n + pn  answers to the questions as does the correct object 1?  random questions. What is the expected number of  wrong objects agreeing with the answers?  e  Use Markov’s inequality PrfX  cid:21  t cid:22 g  cid:20  1   one or more wrong object remaining  goes to zero as n  cid:0 ! 1 .  t ; to show that the probability of error  Solution: Random \20" questions.  Repeat of Problem 5.45    a  Obviously, Hu cid:11 man codewords for X are all of length n . Hence, with n deter-  ministic questions, we can identify an object out of 2n candidates.   b  Observe that the total number of subsets which include both object 1 and object 2 or neither of them is 2m cid:0 1 . Hence, the probability that object 2 yields the same answers for k questions as object 1 is  2m cid:0 1=2m k = 2 cid:0 k . More information theoretically, we can view this problem as a channel coding problem through a noiseless channel. Since all subsets are equally likely, the probability the object 1 is in a speci cid:12 c random subset is 1=2 . Hence, the question whether object 1 belongs to the k th subset or not corresponds to the k th bit of the random codeword for object 1, where codewords X k are Bern  1=2   random k -sequences.  Object Codeword 0110 : : : 1 0010 : : : 0  1 2 ...  Now we observe a noiseless output Y k of X k and  cid:12 gure out which object was sent. From the same line of reasoning as in the achievability proof of the channel coding theorem, i.e. joint typicality, it is obvious the probability that object 2 has the same codeword as object 1 is 2 cid:0 k .   c  Let  1j =  1;  0;  object j yields the same answers for k questions as object 1 otherwise  ;  for j = 2; : : : ; m.   198  Then,  Channel Capacity  Xj=2 E  of objects in f2; 3; : : : ; mg with the same answers  = E  Xj=2 Xj=2  =  =  m  m  2 cid:0 k  m  1j   E 1j   =  m  cid:0  1 2 cid:0 k =  2n  cid:0  1 2 cid:0 k:   d  Plugging k = n + pn into  c  we have the expected number of  2n  cid:0  1 2 cid:0 n cid:0 pn .   e  Let N by the number of wrong objects remaining. Then, by Markov’s inequality  P  N  cid:21  1   cid:20  EN =  2n  cid:0  1 2 cid:0 n cid:0 pn  cid:20  2 cid:0 pn ! 0;  where the  cid:12 rst equality follows from part  d .  33. BSC with feedback. Suppose that feedback is used on a binary symmetric channel with parameter p . Each time a Y is received, it becomes the next transmission. Thus X1 is Bern 1 2 , X2 = Y1; X3 = Y2; : : : ; Xn = Yn cid:0 1:   a  Find limn!1  b  Show that for some values of p , this can be higher than capacity.  1 n I X n; Y n  .   c  Using this feedback transmission scheme,  X n W; Y n  =  X1 W  ; Y1; Y2; : : : ; Ym cid:0 1  , what is the asymptotic communication rate achieved; that is, what is limn!1  1 n I W ; Y n  ?  Solution: BSC with feedback solution.   a   So,  H Y njX n  =Xi H Y n  =Xi  I X n; Y n  = H Y n   cid:0  H Y njX n : H YijY i cid:0 1; X n  = H Y1jX1  +Xi H YijY i cid:0 1  = H Y1  +Xi  H YijY n  = H p  + 0:  H YijXi  = 1 +  n  cid:0  1 H p    Channel Capacity  199  I X n; Y n  = 1 +  n  cid:0  1 H p   cid:0  H p  = 1 +  n  cid:0  2 H p   and,  1 n  lim n!1  I X n; Y n  = lim n!1  1 +  n  cid:0  2 H p   = H p   n   b  For the BSC C = 1  cid:0  H p  . For p = 0:5 , C = 0 , while limn!1  1 nI X n; Y n  =  H 0:5  = 1 .   c  Using this scheme 1  nI W ; Y n  ! 0 .  34. Capacity. Find the capacity of   a  Two parallel BSC’s   b  BSC and single symbol.   c  BSC and ternary channel.  X  X  X  -  p  p  -   cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8 * HHHHHHHj  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8 * HHHHHHHj  p p  -  -  -  p   cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8 * HHHHHHHj  -  p  -  1 2  -  cid:0  cid:18   1 2  1 2  1 2   cid:0    cid:0    cid:0    cid:0   -  HHHHHHHj HHHHHHHj  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8 * HHHHHHHj  - -  -   cid:0    cid:0   1 2  1 2  1 2  1 2  1 2  1 2  1  2  3  4  1  2  3  1  2  3 4  5  1  2  3  4  1  2  3  1  2  3 4  5  Y  Y  Y   200   d  Ternary channel.  Solution: Capacity  Channel Capacity  p yjx  =" 2=3 1=3  1=3 2=3  :  0  0   7.119   Recall the parallel channels problem  problem 7.28 showed that for two channels in parallel with capacities C1 and C2 , the capacity C of the new channel satis cid:12 es   a  Here C1 = C2 = 1  cid:0  H p  , and hence 2C = 2C1+1 , or,   b  Here C1 = 1  cid:0  H p  but C2 = 0 and so 2C = 2C1 + 1 , or,  2C = 2C1 + 2C2  C = 2  cid:0  H p :  C = log cid:16 21 cid:0 H p  + 1 cid:17  :   c  The p in the  cid:12 gure is a typo. All the transition probabilities are 1 2. The capacity of the ternary channel  which is symmetric  is log 3  cid:0  H  1 2   = log 3  cid:0  1 . The capacity of the BSC is 0, and together the parallel channels have a capacity 2C = 3=2 + 1 , or C = log 5 2 .   d  The channel is weakly symmetric and hence the capacity is log3  cid:0  H  1  3 ; 2  3   =  log 3  cid:0   log 3  cid:0  2  3   = 2 3 .  35. Capacity.  Suppose channel P has capacity C; where P is an m  cid:2  n channel matrix.  a  What is the capacity of  ~P =" P 0 1   0  ^P =" P 0 Ik   0   b  What about the capacity of  where Ik if the k  cid:2  k identity matrix.  Solution: Solution: Capacity.   a  By adding the extra column and row to the transition matrix, we have two channels in parallel. You can transmit on either channel. From problem 7.28, it follows that  ~C = log 20 + 2C   ~C = log 1 + 2C    Channel Capacity  201   b  This part is also an application of the conclusion problem 7.28. Here the capacity  of the added channel is log k:  ^C = log 2log k + 2C   ^C = log k + 2C    Zi =  1;  cid:0 1;  p = 0:5 p = 0:5  ?  Z =  1;  cid:0 1;  p = 0:5 p = 0:5  ?  36. Channel with memory.  Consider the discrete memoryless channel Yi = ZiXi with input alphabet Xi 2 f cid:0 1; 1g:  a  What is the capacity of this channel when fZig is i.i.d. with   7.120    7.121   Now consider the channel with memory. Before transmission begins, Z is ran- domly chosen and  cid:12 xed for all time. Thus Yi = ZXi:   b  What is the capacity if  Solution: Channel with memory solution.   a  This is a BSC with cross over probability 0.5, so C = 1  cid:0  H p  = 0 .  b  Consider the coding scheme of sending X n =  1; b1; b2; : : : ; bn cid:0 1  where the  cid:12 rst symbol is always a zero and the rest of the n  cid:0  1 symbols are  cid:6 1 bits. For the  cid:12 rst symbol Y1 = Z , so the receiver knows Z exactly. After that the receiver can recover the remaining bits error free. So in n symbol transmissions n bits are sent, for a rate R = n cid:0 1 n ! 1 . The capacity C is bounded by log jXj = 1 , therefore the capacity is 1 bit per symbol.  37. Joint typicality.  Let  Xi; Yi; Zi  be i:i:d: according to p x; y; z : We will say that  xn; yn; zn  is jointly typical  written  xn; yn; zn  2 A n     if   cid:15    cid:15  p xn  2 2 cid:0 n H X  cid:6  cid:15    cid:15  p yn  2 2 cid:0 n H Y   cid:6  cid:15    cid:15  p zn  2 2 cid:0 n H Z  cid:6  cid:15    cid:15  p xn; yn  2 2 cid:0 n H X;Y   cid:6  cid:15    cid:15  p xn; zn  2 2 cid:0 n H X;Z  cid:6  cid:15    cid:15  p yn; zn  2 2 cid:0 n H Y;Z  cid:6  cid:15    cid:15  p xn; yn; zn  2 2 cid:0 n H X;Y;Z  cid:6  cid:15     202  Channel Capacity  Now suppose   ~X n; ~Y n; ~Z n  is drawn according to p xn p yn p zn : Thus ~X n; ~Y n; ~Z n have the same marginals as p xn; yn; zn  but are independent. Find  bounds on  P rf  ~X n; ~Y n; ~Z n  2 A n    cid:15  g in terms of the entropies H X ; H Y  ; H Z ; H X; Y  ; H X; Z ; H Y; Z   and H X; Y; Z :  Solution: Joint typicality.  P rf  ~X n; ~Y n; ~Z n  2 A n    cid:15  g =  p xn p yn p zn   X xn;yn;zn 2A n  X xn;yn;zn 2A n    cid:15    cid:15    cid:20   2 cid:0 n H X +H Y  +H Z  cid:0 3 cid:15     cid:15   j2 cid:0 n H X +H Y  +H Z  cid:0 3 cid:15     cid:20  jA n   cid:20  2n H X;Y;Z + cid:15  2 cid:0 n H X +H Y  +H Z  cid:0 3 cid:15    cid:20  2n H X;Y;Z  cid:0 H X  cid:0 H Y   cid:0 H Z +4 cid:15    P rf  ~X n; ~Y n; ~Z n  2 A n    cid:15  g =  p xn p yn p zn   X xn;yn;zn 2A n  X xn;yn;zn 2A n    cid:15    cid:15    cid:21   2 cid:0 n H X +H Y  +H Z +3 cid:15     cid:15   j2 cid:0 n H X +H Y  +H Z  cid:0 3 cid:15     cid:21  jA n   cid:21   1  cid:0   cid:15  2n H X;Y;Z  cid:0  cid:15  2 cid:0 n H X +H Y  +H Z  cid:0 3 cid:15    cid:21   1  cid:0   cid:15  2n H X;Y;Z  cid:0 H X  cid:0 H Y   cid:0 H Z  cid:0 4 cid:15    Note that the upper bound is true for all n; but the lower bound only hold for n large.   Chapter 8  Di cid:11 erential Entropy  1. Di cid:11 erential entropy. Evaluate the di cid:11 erential entropy h X  =  cid:0 R f ln f for the  following:   a  The exponential density, f  x  =  cid:21 e cid:0  cid:21 x , x  cid:21  0:  b  The Laplace density, f  x  = 1  2  cid:21 e cid:0  cid:21 jxj:   c  The sum of X1 and X2; where X1 and X2 are independent normal random  variables with means  cid:22 i and variances  cid:27 2  i ; i = 1; 2:  Solution: Di cid:11 erential Entropy.   a  Exponential distribution.   b  Laplace density.  h f   =  cid:0 Z 1  0   cid:21 e cid:0  cid:21 x[ln  cid:21   cid:0   cid:21 x]dx  =  cid:0  ln  cid:21  + 1 nats. = log  bits.  e  cid:21   1 2   cid:21 e cid:0  cid:21 jxj[ln  1 2  + ln  cid:21   cid:0   cid:21 jxj] dx  h f   =  cid:0 Z 1 =  cid:0  ln 2e = ln  cid:21  2e  cid:21   = log   cid:0 1 1 2  cid:0  ln  cid:21  + 1 nats.  bits.  203   8.1    8.2    8.3    8.4    8.5    8.6    8.7    204  Di cid:11 erential Entropy   c  Sum of two normal distributions.  The sum of two normal random variables is also normal, so applying the result derived the class for the normal distribution, since X1 +X2  cid:24  N   cid:22 1 + cid:22 2;  cid:27 2 2  ,  1 + cid:27 2  h f   =  log 2 cid:25 e  cid:27 2  1 +  cid:27 2  2  bits.  1 2   8.8   2. Concavity of determinants. Let K1 and K2 be two symmetric nonnegative de cid:12 nite  n  cid:2  n matrices. Prove the result of Ky Fan[4]:  for 0  cid:20   cid:21   cid:20  1;  cid:21  = 1  cid:0   cid:21 ;  j  cid:21 K1 +  cid:21 K2 j cid:21 j K1 j cid:21 j K2 j cid:21 ; where j K j denotes the determinant of K: Hint: Let Z = X cid:18 ; where X1  cid:24  N  0; K1 ; X2  cid:24  N  0; K2  and  cid:18  = Bernoulli   cid:21  : Then use h Z j  cid:18    cid:20  h Z : Solution: Concavity of Determinants. Let X1 and X2 be normally distributed n - vectors, Xi  cid:24   cid:30 Ki x  , i = 1; 2 . Let the random variable  cid:18  have distribution Prf cid:18  = 1g =  cid:21  , Prf cid:18  = 2g = 1  cid:0   cid:21 ; 0  cid:20   cid:21   cid:20  1 . Let  cid:18  , X1 , and X2 be independent and let Z = X cid:18  . Then Z has covariance KZ =  cid:21 K1 +  1  cid:0   cid:21  K2 . However, Z will not be multivariate normal. However, since a normal distribution maximizes the entropy for a given variance, we have  ln 2 cid:25 e nj cid:21 K1+ 1 cid:0  cid:21  K2j  cid:21  h Z   cid:21  h Zj cid:18   =  cid:21   ln 2 cid:25 e njK1j+ 1 cid:0  cid:21    1 2  1 2  ln 2 cid:25 e njK2j :  8.9    8.10   j cid:21 K1 +  1  cid:0   cid:21  K2j  cid:21  jK1j cid:21 jK2j1 cid:0  cid:21  ;  1 2  Thus  as desired.  3. Uniformly distributed noise. Let the input random variable X to a channel be uniformly distributed over the interval  cid:0 1=2  cid:20  x  cid:20  +1=2 . Let the output of the channel be Y = X + Z , where the noise random variable is uniformly distributed over the interval  cid:0 a=2  cid:20  z  cid:20  +a=2 .  a  Find I X; Y   as a function of a .  b  For a = 1  cid:12 nd the capacity of the channel when the input X is peak-limited; that is, the range of X is limited to  cid:0 1=2  cid:20  x  cid:20  +1=2 . What probability distribution on X maximizes the mutual information I X; Y   ?   c   Optional  Find the capacity of the channel for all values of a , again assuming  that the range of X is limited to  cid:0 1=2  cid:20  x  cid:20  +1=2 .  Solution: Uniformly distributed noise. The probability density function for Y = X +Z is the convolution of the densities of X and Z . Since both X and Z have rectangular densities, the density of Y is a trapezoid. For a < 1 the density for Y is  cid:0  1 + a =2  cid:20  y  cid:20   cid:0  1  cid:0  a =2  1=2a  y +  1 + a =2   cid:0  1  cid:0  a =2  cid:20  y  cid:20  + 1  cid:0  a =2 1  1=2a   cid:0 y  cid:0   1 + a =2  + 1  cid:0  a =2  cid:20  y  cid:20  + 1 + a =2  pY  y  =8>< >:   Di cid:11 erential Entropy  205  and for a > 1 the density for Y is  pY  y  =8>< >:   cid:0  a + 1 =2  cid:20  y  cid:20   cid:0  a  cid:0  1 =2 y +  a + 1 =2  cid:0  a  cid:0  1 =2  cid:20  y  cid:20  + a  cid:0  1 =2 1=a  cid:0 y  cid:0   a + 1 =2 + a  cid:0  1 =2  cid:20  y  cid:20  + a + 1 =2  When a = 1 , the density of Y is triangular over the interval [ cid:0 1; +1] .   a  We use the identity I X; Y   = h Y    cid:0  h Y jX  .  It is easy to compute h Y   directly, but it is even easier to use the grouping property of entropy. First suppose that a < 1 . With probability 1  cid:0  a , the output Y is conditionally uniformly distributed in the interval [ cid:0  1  cid:0  a =2; + 1  cid:0  a =2] ; whereas with probability a , Y has a split triangular density where the base of the triangle has width a .  h Y   = H a  +  1  cid:0  a  ln 1  cid:0  a  + a   + ln a   1 2  =  cid:0 a ln a  cid:0   1  cid:0  a  ln 1  cid:0  a  +  1  cid:0  a  ln 1  cid:0  a  +  + a ln a =  nats.  a 2  a 2  If a > 1 the trapezoidal density of Y can be scaled by a factor a , which yields h Y   = ln a+1=2a . Given any value of x , the output Y is conditionally uniformly distributed over an interval of length a , so the conditional di cid:11 erential entropy in nats is h Y jX  = h Z  = ln a for all a > 0 . Therefore the mutual information in nats is  I X; Y   =  a=2  cid:0  ln a if a  cid:20  1 if a  cid:21  0 :  1=2a  As expected, I X; Y   ! 1 as a ! 0 and I X; Y   ! 0 as a ! 1 .   b  As usual with additive noise, we can express I X; Y   in terms of h Y   and h Z  :  I X; Y   = h Y    cid:0  h Y jX  = h Y    cid:0  h Z  :  [ cid:0 1=2; +1=2] , their sum Y is Since both X and Z are limited to the interval limited to the interval [ cid:0 1; +1] . The di cid:11 erential entropy of Y is at most that of a random variable uniformly distributed on that interval; that is, h Y    cid:20  1 . This maximum entropy can be achieved if the input X takes on its extreme values x =  cid:6 1 each with probability 1 2. In this case, I X; Y   = h Y    cid:0  h Z  = 1  cid:0  0 = 1 . Decoding for this channel is quite simple:  ^X =   cid:0 1=2 if y < 0 +1=2 if y  cid:21  0 :  This coding scheme transmits one bit per channel use with zero error probability.  Only a received value y = 0 is ambiguous, and this occurs with probability 0.    c  When a is of the form 1=m for m = 2; 3; : : : , we can achieve the maximum possible value I X; Y   = log m when X is uniformly distributed over the discrete points f cid:0 1; cid:0 1+2= m cid:0 1 ; : : : ; +1 cid:0 2= m cid:0 1 ; +1g . In this case Y has a uniform probability density on the interval [ cid:0 1 cid:0  1= m cid:0  1 ; +1 + 1= m cid:0  1 ] . Other values of a are left as an exercise.   206  Di cid:11 erential Entropy  4. Quantized random variables. Roughly how many bits are required on the average to describe to 3 digit accuracy the decay time  in years  of a radium atom if the half-life of radium is 80 years? Note that half-life is the median of the distribution.  Solution: Quantized random variables. The di cid:11 erential entropy of an exponentially distributed random variable with mean 1= cid:21  is log e  cid:21  bits. If the median is 80 years, then  or  Hence  Z 80  0   cid:21 e cid:0  cid:21 x dx =  1 2   cid:21  =  = 0:00866  ln 2 80  g y  =  f  A cid:0 1y :  1 jAj  and the di cid:11 erential entropy is log e= cid:21  . To represent the random variable to 3 digits  cid:25  10 bits accuracy would need log e= cid:21  + 10 bits = 18.3 bits. 5. Scaling. Let h X  =  cid:0 R f  x  log f  x  dx . Show h AX  = log j det A  j +h X :  Solution: Scaling. Let Y = AX . Then the density of Y is  h AX  =  cid:0 Z g y  ln g y  dy  f  A cid:0 1y hln f  A cid:0 1y   cid:0  log jAji dy f  x  [ln f  x   cid:0  log jAj] jAj dx  =  cid:0 Z =  cid:0 Z = h X  + log jAj:  1 jAj 1 jAj  6. Variational inequality: Verify, for positive random variables X , that  log EP  X  = sup Q  [EQ log X   cid:0  D QjjP  ] where EP  X  = Px xP  x  and D QjjP   = Px Q x  log Q x  over all Q x   cid:21  0 , P Q x  = 1 . It is enough to extremize J Q  = EQ ln X cid:0 D QjjP  +  cid:21  P Q x   cid:0  1  .  P  x  , and the supremum is  Solution: Variational inequality   8.18   Using the calculus of variations to extremize  J Q  =Xx  q x  ln x  cid:0 Xx  q x  ln  q x  p x   +  cid:21  Xx  q x   cid:0  1   we di cid:11 erentiate with respect to q x  to obtain  @J  @q x   = ln x  cid:0  ln  q x  p x   cid:0  1 +  cid:21  = 0   8.11    8.12    8.13    8.14    8.15    8.16    8.17    8.19    8.20    Di cid:11 erential Entropy  or  where c0 has to be chosen to satisfy the constraint, Px q x  = 1 . Thus  Substituting this in the expression for J , we obtain  q x  = c0xp x   c0 =  1  Px xp x   J cid:3  = Xx  c0xp x  ln x  cid:0 Xx  c0xp x  ln  c0xp x   p x   c0xp x  ln x  cid:0 Xx  c0xp x  ln x  =  cid:0  ln c0 +Xx = lnXx  xp x   To verify this is indeed a maximum value, we use the standard technique of writing it as a relative entropy. Thus  lnXx  xp x   cid:0 Xx  q x  ln x +Xx  q x  ln  q x  p x   q x  xp x   yp y   Py  q x  ln  = Xx = D qjjp0   cid:21  0  Thus  lnXx  xp x  = sup Q   EQ ln X   cid:0  D QjjP     This is a special case of a general relationship that is a key in the theory of large deviations.  7. Di cid:11 erential entropy bound on discrete entropy: Let X be a discrete random  variable on the set X = fa1; a2; : : :g with Pr X = ai  = pi . Show that 121 A :  log 2 cid:25 e 0 @  pii2  cid:0   1Xi=1  H p1; p2; : : :   cid:20   ipi!2  1Xi=1  1 2  +  1  Moreover, for every permutation  cid:27  ,  H p1; p2; : : :   cid:20   1 2  log 2 cid:25 e 0 1Xi=1 @  p cid:27  i i2  cid:0   1Xi=1  ip cid:27  i !2  +  1  121 A :  Hint: Construct a random variable X0 such that Pr X0 = i  = pi . Let U be an uniform 0,1] random variable and let Y = X0 + U , where X0 and U are independent.  207   8.21    8.22    8.23    8.24    8.25    8.26    8.27    8.28    8.29    8.30    8.31    208  Di cid:11 erential Entropy  Use the maximum entropy bound on Y to obtain the bounds in the problem. This bound is due to Massey  unpublished  and Willems unpublished .  Solution: Di cid:11 erential entropy bound on discrete entropy  Of all distributions with the same variance, the normal maximizes the entropy. So the entropy of the normal gives a good bound on the di cid:11 erential entropy in terms of the variance of the random variable. Let X be a discrete random variable on the set X = fa1; a2; : : :g with  Pr X = ai  = pi:  Moreover, for every permutation  cid:27  ,  H p1; p2; : : :   cid:20   H p1; p2; : : :   cid:20   1 2  1 2  log 2 cid:25 e 0 @ log 2 cid:25 e 0 1Xi=1 @  1Xi=1  pii2  cid:0   1Xi=1  ipi!2  +  p cid:27  i i2  cid:0   1Xi=1  ip cid:27  i !2  1  121 A : 121 A :  1  +  De cid:12 ne two new random variables. The  cid:12 rst, X0 , is an integer-valued discrete random variable with the distribution  Let U be a random variable uniformly distributed on the range [0; 1] , independent of X0 . De cid:12 ne the continuous random variable ~X by  Pr X0 = i  = pi:  ~X = X0 + U:  The distribution of the r.v. ~X is shown in Figure 8.1.  It is clear that H X  = H X0  , since discrete entropy depends only on the probabilities and not on the values of the outcomes. Now  H X0  =  cid:0   pi log pi  1Xi=1 1Xi=1 cid:18 Z i+1 1Xi=1Z i+1  i  i  =  cid:0   =  cid:0  =  cid:0 Z 1  = h  ~X ;  1  f ~X x  dx cid:19  log cid:18 Z i+1  i  f ~X x  dx cid:19   f ~X x  log f ~X x  dx  f ~X x  log f ~X x  dx  since f ~X x  = pi for i  cid:20  x < i + 1 .   8.32    8.33    8.34    8.35    8.36    8.37    8.38    8.39    8.40    8.41    Di cid:11 erential Entropy  209  ~X  6  - f   ~X   Figure 8.1: Distribution of ~X .  Hence we have the following chain of inequalities:  H X  = H X0   = h  ~X   log 2 cid:25 e Var  ~X    cid:20  =  =  1 2 1 2  1 2  log 2 cid:25 e   Var X0  + Var U     log 2 cid:25 e 0 1Xi=1 @  pii2  cid:0   1Xi=1  ipi!2  +  1  121 A :   8.42    8.43    8.44    8.45    8.46   Since entropy is invariant with respect to permutation of p1; p2; : : : , we can also obtain a bound by a permutation of the pi ’s. We conjecture that a good bound on the variance will be achieved when the high probabilities are close together, i.e, by the assignment : : : ; p5; p3; p1; p2; p4; : : : for p1  cid:21  p2  cid:21   cid:1  cid:1  cid:1  . How good is this bound? Let X be a Bernoulli random variable with parameter 1 2 , which implies that H X  = 1 . The corresponding random variable X0 has variance 1 4 , so the bound is  H X   cid:20   1 2  log 2 cid:25 e  cid:18  1  4  +  1  12 cid:19  = 1:255 bits.   8.47   8. Channel with uniformly distributed noise: Consider a additive channel whose input alphabet X = f0; cid:6 1; cid:6 2g , and whose output Y = X + Z , where Z is uniformly distributed over the interval [ cid:0 1; 1] . Thus the input of the channel is a discrete random   210  Di cid:11 erential Entropy  variable, while the output is continuous. Calculate the capacity C = maxp x  I X; Y   of this channel.  Solution: Uniformly distributed noise  We can expand the mutual information  I X; Y   = h Y    cid:0  h Y jX  = h Y    cid:0  h Z    8.48   and h Z  = log 2 , since Z  cid:24  U   cid:0 1; 1  . The output Y is a sum a of a discrete and a continuous random variable, and if the probabilities of X are p cid:0 2; p cid:0 1; : : : ; p2 , then the output distribution of Y has a uniform distribution with weight p cid:0 2=2 for  cid:0 3  cid:20  Y  cid:20   cid:0 2 , uniform with weight  p cid:0 2 + p cid:0 1 =2 for  cid:0 2  cid:20  Y  cid:20   cid:0 1 , etc. Given that Y ranges from -3 to 3, the maximum entropy that it can have is an uniform over this range. This can be achieved if the distribution of X is  1 3, 0, 1 3,0,1 3 . Then h Y   = log 6 and the capacity of this channel is C = log 6  cid:0  log 2 = log 3 bits.  9. Gaussian mutual information. Suppose that  X; Y; Z  are jointly Gaussian and that X ! Y ! Z forms a Markov chain. Let X and Y have correlation coe cid:14 cient  cid:26 1 and let Y and Z have correlation coe cid:14 cient  cid:26 2 . Find I X; Z  . Solution: Gaussian Mutual Information First note that we may without any loss of generality assume that the means of X , Y and Z are zero. If in fact the means are not zero one can subtract the vector of means without a cid:11 ecting the mutual information or the conditional independence of X , Z given Y . Let   cid:3  =    cid:27 2 x   cid:27 x cid:27 z cid:26 xz   cid:27 x cid:27 z cid:26 xz   cid:27 2 z  ! ;  be the covariance matrix of X and Z . We can now use Eq.  8.34  to compute  I X; Z  = h X  + h Z   cid:0  h X; Z   log  2 cid:25 e cid:27 2  x  +  log  2 cid:25 e cid:27 2  1 2  x   cid:0   log  2 cid:25 ej cid:3 j   1 2  =  1 2 =  cid:0   1 2  log 1  cid:0   cid:26 2 xz   Now,   cid:26 xz =  EfXZg  cid:27 x cid:27 z EfEfXZjY gg   cid:27 x cid:27 z  =  =  =   cid:27 x cid:27 z  EfEfXjY gEfZjY gg Ef cid:16   cid:27 x cid:26 xy Y cid:17 g Y cid:17  cid:16   cid:27 z cid:26 zx   cid:27 y   cid:27 y   cid:27 x cid:27 z  =  cid:26 xy cid:26 zy   211   8.49    8.50    8.51    8.52   Di cid:11 erential Entropy  We can thus conclude that  I X; Y   =  cid:0   log 1  cid:0   cid:26 2  xy cid:26 2  zy   1 2  10. The Shape of the Typical Set  Let Xi be i.i.d.  cid:24  f  x  , where Let h =  cid:0 R f ln f . Describe the shape  or form  or the typical set A n  f  xn  2 2 cid:0 n h cid:6  cid:15  g . Solution: The Shape of the Typical Set  f  x  = ce cid:0 x4  :   cid:15  = fxn 2 Rn :  We are interested in the set f xn 2 R : f  xn  2 2 cid:0 n h cid:6  cid:15   g. This is:  Since Xi are i.i.d.,  2 cid:0 n h cid:0  cid:15    cid:20  f  xn   cid:20  2 cid:0 n h+ cid:15    f  xn  =  f  x   =  ce cid:0 x4  i  = enln c  cid:0 Pn  i=1 x4  i  n  n  Yi=1 Yi=1  n  Xi=1  Plugging this in for f  xn   in the above inequality and using algebraic manipulation gives:  n ln c  +  h  cid:0   cid:15  ln 2    cid:21   x4 i  cid:21  n ln c  +  h +  cid:15  ln 2    So the shape of the typcial set is the shell of a 4-norm ball nxn : jjxnjj4 2  n ln c  +  h  cid:6   cid:15  ln 2   1=4o  .  11. Non ergodic Gaussian process.  Consider a constant signal V in the presence of iid observational noise fZig . Thus Xi = V + Zi , where V  cid:24  N  0; S  , and Zi are iid  cid:24  N  0; N   . Assume V and fZig are independent.   a  Is fXig stationary?   212  Di cid:11 erential Entropy   b  Find limn cid:0 !1  c  What is the entropy rate h of fXig ?  d  Find the least mean squared error predictor ^Xn+1 X n  and  cid:12 nd  cid:27 2  i=1 Xi . Is the limit random?  1  nPn  Xn 2:   e  Does fXig have an AEP? That is, does  cid:0  1 Solution: Nonergodic Gaussian process  n log f  X n   cid:0 ! h ?   a  Yes. EXi = EV + Zi = 0 for all i , and  1 = limn cid:0 !1 E  ^Xn cid:0   EXiXj = E V + Zi  V + Zj  =   S;  S + N:  i = j i 6= j   8.53   Since Xi is Gaussian distributed it is completely characterized by its  cid:12 rst and second moments. Since the moments are stationary, Xi is wide sense stationary, which for a Gaussian distribution implies that Xi is stationary.   b   lim n!1  1 n  n  Xi=1  Xi = lim n!1  1 n  n  Xi=1  = V + lim n!1   Zi + V    1 n  n  Xi=1  Zi  = V + EZi by the strong law of large numbers  = V   8.56    8.57   The limit is a random variable N  0; S  .   c  Note that X n  cid:24  N  0; KX n   , where KX n has diagonal values of S + N and o cid:11  diagonal values of S . Also observe that the determinant is jKX nj = N n nS=N + 1  . We now compute the entropy rate as:  h X   = lim = lim  = lim  h X1; X2; : : : ; Xn   log  2 cid:25 e njKX nj  log cid:18  2 cid:25 e nN n cid:18  nS  N  = lim  log 2 cid:25 eN  n +  1 n 1 2n 1 2n 1 2n  =  =  1 2 1 2  log 2 cid:25 eN + lim  log 2 cid:25 eN  1 2n  + 1 cid:19  cid:19  log cid:18  nS + 1 cid:19   N  + 1 cid:19   1 2n  log cid:18  nS  N   8.54    8.55    8.58    8.59    8.60    8.61    8.62    8.63    Di cid:11 erential Entropy   d  By iterated expectation we can write  E cid:16 Xn+1  cid:0  ^Xn+1 X n  cid:17 2  = E cid:18 E cid:18  cid:16 Xn+1  cid:0  ^Xn+1 X n  cid:17 2 cid:12  cid:12  cid:12  cid:12   X n cid:19  cid:19   We note that minimizing the expression is equivalent to minimizing the inner expectation, and that for the inner expectation the predictor is a nonrandom variable. Expanding the inner expectation and taking the derivative with respect to the estimator ^Xn+1 X n  , we get  E cid:16  Xn+1  cid:0  ^Xn+1 X n  2jX n cid:17   = E cid:16  X 2  n+1  cid:0  2Xn+1 ^Xn+1 X n  + ^X 2  n+1 X n  jX n cid:17    8.65   dE cid:16  Xn+1  cid:0  ^Xn+1 X n  2jX n cid:17   d ^Xn+1 X n   = E cid:16  cid:0 2Xn+1 + 2 ^Xn+1 X n jX n cid:17   8.66  =  cid:0 2E Xn+1jX n  + 2 ^Xn+1 X n    8.67  Setting the derivative equal to 0, we see that the optimal ^Xn+1 X n  = E Xn+1jX n  . To  cid:12 nd the limiting squared error for this estimator, we use the fact that V and X n are normal random variables with known covariance matrix, and therefore the conditional distribution of V given X n is  so  Now  f  V jX n   cid:24  N    S  nS + N  Xi;  SN  nS + N!  n  Xi=1  ^Xn+1 X n  = E Xn+1jX n   = E V jX n  + E Zn+1jX n  =  Xi + 0  S  n  nS + N  Xi=1  and hence the limiting squared error  e2 = lim n!1  E  ^Xn  cid:0  Xn 2 E  S E  E   S  S   n  cid:0  1 S + N   n  cid:0  1 S + N   n  cid:0  1 S + N  n cid:0 1  n cid:0 1  Xi=1 Xi=1 Xi=1  n cid:0 1  = lim n!1  = lim n!1  = lim n!1  Xi  cid:0  Xn!2  Zi + V    cid:0  Zn  cid:0  V!2  Zi  cid:0  Zn  cid:0   N   n  cid:0  1 S + N  V!2  213   8.64    8.68    8.69    8.70    8.71    8.72    8.73    8.74    8.75    214  Di cid:11 erential Entropy  = lim  = lim  n!1 cid:18  n!1 cid:18   S  EZ 2  i + EZ 2   n  cid:0  1 S + N cid:19 2 n cid:0 1 n + cid:18  Xi=1  n  cid:0  1 S + N cid:19 2  n  cid:0  1 N + N + cid:18    n  cid:0  1 S + N cid:19 2  n  cid:0  1 S + N cid:19 2  N  N  S  S  EV 2 8.76   = 0 + N + 0  = N  because   e  Even though the process is not ergodic, it is stationary, and it does have an AEP  e cid:0 X tK  cid:0 1  Xn X=2  1 n   cid:0   1 2  1  =  ln   2 cid:25  n=2jKX nj  1 ln f  X n  =  cid:0  n 1 2n 1 ln 2 cid:25 e njKX nj  cid:0  2n 1 h X n   cid:0  n  ln 2 cid:25  n +  1 2n  1 2n  1 2  +  =  =  lnjKX nj + 1 2n  1 2  +  X tK cid:0 1  X nX  X tK cid:0 1  X nX  1 2n X tK cid:0 1  X nX  2 W , where W  cid:24  N  0; I  . Then Since X  cid:24  N  0; K  , we can write X = K 2 W = W tW = P W 2 X tK cid:0 1X = W tK i , and therefore X tK cid:0 1X has a  cid:31 2 distribution with n degrees of freedom. The density of the  cid:31 2 distribution is  2 K cid:0 1K  1  1  1  f  x  =  2  n  2  cid:0 1e cid:0  x x  cid:0   n  2  2  n 2  The moment generating function of the  cid:31 2 distribution is  M  t  = Z f  x etx dx  etx dx   cid:0 1   1  cid:0  2t x  2  2   cid:0   n  n 2  2  n  n 2  2  cid:0 1e cid:0  x  cid:0   n 2  2 1  1 cid:0 2t   n 2  = Z x = Z  =  1  n 2   1  cid:0  2t   n  2  cid:0 1e cid:0  1 cid:0 2t x=2   1  cid:0  2t  dx  By the Cherno cid:11  bound  Lemma 11.19.1   Pr cid:26  1  nX W 2  i > 1 +  cid:15  cid:27   cid:20  min  cid:20  e cid:0  n  s  e cid:0 s 1+ cid:15   1  cid:0  2s  cid:0  n 2   cid:15  cid:0 ln 1+ cid:15     2   8.77    8.78    8.79    8.80    8.81    8.82    8.83    8.84    8.85    8.86    8.87    8.88    8.89    8.90    8.91    Di cid:11 erential Entropy  setting s =  cid:15  Thus  2 1+ cid:15   .  215  1 n  Pr cid:26  cid:12  cid:12  cid:12  cid:12  cid:0   ln f  X n   cid:0  hn cid:12  cid:12  cid:12  cid:12   >  cid:15  cid:27  = Pr cid:26  cid:12  cid:12  cid:12  cid:12    cid:20  e cid:0  n  1  2 cid:18  cid:0 1 +  2   cid:15  cid:0 ln 1+ cid:15     1  i  cid:19  cid:12  cid:12  cid:12  cid:12  nX W 2  >  cid:15  cid:27   8.92    8.93   and the bound goes to 0 as n ! 1 , and therefore by the Borel Cantelli lemma,  1 n   cid:0   ln f  X n   cid:0  hn ! 0   8.94   with probability 1. So Xi satis cid:12 es the AEP even though it is not ergodic.   216  Di cid:11 erential Entropy   Chapter 9  Gaussian channel  1. A channel with two independent looks at Y. Let Y1 and Y2 be conditionally  independent and conditionally identically distributed given X:   a  Show I X; Y1; Y2  = 2I X; Y1   cid:0  I Y1; Y2 :  b  Conclude that the capacity of the channel  -  -  X  X  -   Y1; Y2   -  Y1  is less than twice the capacity of the channel  Solution: Channel with two independent looks at Y .   a   I X; Y1; Y2  = H Y1; Y2   cid:0  H Y1; Y2jX   = H Y1  + H Y2   cid:0  I Y1; Y2   cid:0  H Y1jX   cid:0  H Y2jX    9.2   since Y1 and Y2 are conditionally independent given X  9.3   9.4   since Y1 and Y2 are conditionally iden- :  9.5  tically distributed   = I X; Y1  + I X; Y2   cid:0  I Y1; Y2  = 2I X; Y1   cid:0  I Y1; Y2    9.1    9.6    b  The capacity of the single look channel X ! Y1 is I X; Y1 :  C1 = max p x   217   218  Gaussian channel  The capacity of the channel X !  Y1; Y2  is  I X; Y1; Y2   2I X; Y1   cid:0  I Y1; Y2  2I X; Y1   C2 = max p x  = max p x    cid:20  max p x  = 2C1:  -  X  -   Y1; Y2   Hence, two independent looks cannot be more than twice as good as one look.  2. The two-look Gaussian channel.  Consider the ordinary Gaussian channel with two correlated looks at X, i.e., Y =  Y1; Y2  , where  with a power constraint P on X , and  Z1; Z2   cid:24  N2 0; K  , where  Y1 = X + Z1 Y2 = X + Z2  K =" N N  cid:26  N  cid:26  N  :  Find the capacity C for   a   cid:26  = 1   b   cid:26  = 0   c   cid:26  = -1  Solution: The two look Gaussian channel. It is clear that the input distribution that maximizes the capacity is X  cid:24  N  0; P   . Evaluating the mutual information for this distribution,  C2 = max I X; Y1; Y2   = h Y1; Y2   cid:0  h Y1; Y2jX  = h Y1; Y2   cid:0  h Z1; Z2jX  = h Y1; Y2   cid:0  h Z1; Z2  N  cid:26  N ! ;  Z1; Z2   cid:24  N  0;" N N  cid:26   Now since   9.7    9.8    9.9    9.10    9.11    9.12    9.13    9.14    9.15    9.16    9.17    9.18    Gaussian channel  we have  1 log 2 cid:25 e 2jKZj = 2 Since Y1 = X + Z1 , and Y2 = X + Z2 , we have  h Z1; Z2  =  1 2  log 2 cid:25 e 2N 2 1  cid:0   cid:26 2 :   Y1; Y2   cid:24  N  0;" P + N P +  cid:26 N  P +  cid:26 N P + N ! ;  and  Hence the capacity is  h Y1; Y2  =  log 2 cid:25 e 2jKY j =  log 2 cid:25 e 2 N 2 1  cid:0   cid:26 2  + 2P N  1  cid:0   cid:26   :   9.21   1 2  1 2  C2 = h Y1; Y2   cid:0  h Z1; Z2  N  1 +  cid:26   cid:19  :  log cid:18 1 +  1 2  2P  =   a   cid:26  = 1 .  In this case, C = 1  2 log 1 + P  N   , which is the capacity of a single look  channel. This is not surprising, since in this case Y1 = Y2 .   b   cid:26  = 0 . In this case,  N  cid:19  ; which corresponds to using twice the power in a single look. The capacity is the same as the capacity of the channel X !  Y1 + Y2  .  log cid:18 1 +   9.24   C =  1 2  2P   c   cid:26  =  cid:0 1 . In this case, C = 1 , which is not surprising since if we add Y1 and Y2 ,  we can recover X exactly.  Note that the capacity of the above channel in all cases is the same as the capacity of the channel X ! Y1 + Y2 .  3. Output power constraint. Consider an additive white Gaussian noise channel with an expected output power constraint P . Thus Y = X + Z , Z  cid:24  N  0;  cid:27  2  , Z is independent of X , and EY 2  cid:20  P . Find the channel capacity. Solution: Output power constraint  C =  I X; Y    max  max  max  f  X :E X+Z 2 cid:20 P  f  X :E X+Z 2 cid:20 P  f  X :E X+Z 2 cid:20 P  =  =   h Y    cid:0  h Y jX    h Y    cid:0  h Z    Given a constraint on the output power of Y , the maximum di cid:11 erential entropy is achieved by a normal distribution, and we can achieve this by have X  cid:24  N  0; P  cid:0  N   , and in this case,  C =  log 2 cid:25 eP  cid:0   1 2  1 2  log 2 cid:25 eN =  log  1 2  P N  :  219   9.19    9.20    9.22    9.23    9.25    9.26    9.27    9.28    9.29    220  Gaussian channel  4. Exponential noise channels. Consider an additive noise channel Yi = Xi + Zi , where Zi is i.i.d. exponentially distributed noise with mean  cid:22  . Assume that we have a mean constraint on the signal, i.e., EXi  cid:20   cid:21  . Show that the capacity of such a channel is C = log 1 +  cid:21  Solution: Exponential noise channels   cid:22    .  Just as for the Gaussian channel, we can write  C =  max  I X; Y    =  =  =  =  f  X :EX cid:20  cid:21   f  X :EX cid:20  cid:21   f  X :EX cid:20  cid:21   max  max  max  max  f  X :EX cid:20  cid:21   f  X :EX cid:20  cid:21   h Y    cid:0  h Y jX  h Y    cid:0  h ZjX  h Y    cid:0  h Z  h Y    cid:0   1 + ln  cid:22    Now Y = X + Z , and EY = EX + EZ  cid:20   cid:21  +  cid:22  . Given a mean constraint, the entropy is maximized by the exponential distribution, and therefore  max  h Y   = 1 + ln  cid:21  +  cid:22     9.36   EY  cid:20  cid:21 + cid:22   Unlike normal distributions, though, the sum of two exponentially distributed variables is not exponential, so we cannot set X to be an exponential distribution to achive the right distribution of Y . Instead, we can use characterstic functions to  cid:12 nd the distribution of X . The characteristic function of an exponential distribution  The distribution of X that when added to Z will give an exponential distribution for Y is the ratio of the characterstic functions    t  =Z 1   cid:22   e cid:0  x   cid:22  e cid:0 itx dx =  1  1  cid:0  i cid:22 t   X  t  =  1  cid:0  i cid:22 t  1  cid:0  i  cid:21  +  cid:22  t  which can been seen to correpond to mixture of a point mass and an exponential distribution. If  X =  0;  with probability  cid:22   cid:21 + cid:22  Xe; with probability  cid:21   cid:21 + cid:22   where Xe has an exponential distribution with parameter  cid:22  +  cid:21  , we can verify that the characterstic function of X is correct.  Using the value of entropy for exponential distributions, we get  C = h Y    cid:0  h Z  = 1 + + ln  cid:21  +  cid:22    cid:0   1 + ln  cid:22   = ln cid:18 1 +   cid:21    cid:22  cid:19    9.41    9.30    9.31    9.32    9.33    9.34    9.35    9.37    9.38    9.39    9.40    Gaussian channel  221  5. Fading channel.  Consider an additive noise fading channel  V  Z  X  -  ?   cid:31  cid:28 s  cid:30  cid:29   -  ?   cid:31  cid:28   cid:30  cid:29   -  Y  Y = XV + Z;  where Z is additive noise, V is a random variable representing fading, and Z and V are independent of each other and of X . Argue that knowledge of the fading factor V improves capacity by showing  I X; Y jV    cid:21  I X; Y  :  Solution: Fading Channel  Expanding I X; Y; V   in two ways, we get  where  9.44  follows from the independence of X and V , and  9.45  follows from I X; V jY    cid:21  0 .  6. Parallel channels and water cid:12 lling. Consider a pair of parallel Gaussian channels,  I X; Y; V   = I X; V   + I X; Y jV   = I X; Y   + I X; V jY    I X; V   + I X; Y jV   = I X; Y   + I X; V jY   I X; Y jV   = I X; Y   + I X; V jY   I X; Y jV    cid:21  I X; Y    X2 ! +  Z1    Y1 Y2 ! =  X1   Z1 Z2 !  cid:24  N  0;"  cid:27 2  1 0  Z2 ! ; 2 ! ;  0  cid:27 2   9.42    9.43    9.44    9.45    9.46    9.47   i.e.  i.e.,  where  and there is a power constraint E X 2 power does the channel stop behaving like a single channel with noise variance  cid:27  2 begin behaving like a pair of channels?  2    cid:20  2P . Assume that  cid:27 2  1 + X 2  1 >  cid:27 2  2 . At what 2 , and   222  Gaussian channel  Solution: Parallel channels and water cid:12 lling. By the result of Section 10.4, it follows that we will put all the signal power into the channel with less noise until the total power of noise + signal in that channel equals the noise power in the other channel. After that, we will split any additional power evenly between the two channels.  Thus the combined channel begins to behave like a pair of parallel channels when the 1  cid:0   cid:27 2 signal power is equal to the di cid:11 erence of the two noise powers, i.e., when 2P =  cid:27  2 2 . 7. Multipath Gaussian channel. Consider a Gaussian noise channel of power contraint P , where the signal takes two di cid:11 erent paths and the received noisy signals are added together at the antenna.   cid:24  cid:24  cid:24  cid:24  cid:24  cid:24  cid:24  cid:24  cid:24  cid:24 : XXXXXXXXXXz  X  XXXXXXXXXXz  cid:24  cid:24  cid:24  cid:24  cid:24  cid:24  cid:24  cid:24  cid:24  cid:24 :   cid:19  cid:16   cid:18  cid:17    cid:6   -  Y  Z1 ? +   cid:19  cid:16   cid:18  cid:17   cid:19  cid:16   cid:18  cid:17   + 6 Z2  -  Y1  -  Y2   a  Find the capacity of this channel if Z1 and Z2 are jointly normal with covariance  matrix KZ ="  cid:27 2   cid:26  cid:27 2  cid:27 2  .   cid:26  cid:27 2   b  What is the capacity for  cid:26  = 0 ,  cid:26  = 1 ,  cid:26  =  cid:0 1 ? Solution: Multipath Gaussian channel.  The channel reduces to the following channel:  Z1 + Z2  ?   cid:19  cid:16 -  cid:18  cid:17   2X  -  Y  The power constraint on the input 2X is 4P . Z1 and Z2 are zero mean, and therefore so is Z1 + Z2 . Then  V ar Z1 + Z2  = E[ Z1 + Z2 2]  1 + Z 2 = E[Z 2 = 2 cid:27 2 + 2 cid:26  cid:27 2:  2 + 2Z1Z2]   Gaussian channel  223  Thus the noise distribution is N  0; 2 cid:27 2 1 +  cid:26   .   a  Plugging the noise and power values into the formula for the one-dimensional   P; N   channel capacity, C = 1  2 log 1 + P  N   , we get  C =  =  1 2 1 2  log cid:18 1 + log cid:18 1 +  4P  2 cid:27 2 1 +  cid:26   cid:19   cid:27 2 1 +  cid:26   cid:19  :  2P   b   i. When  cid:26  = 0 , C = 1 ii. When  cid:26  = 1 , C = 1 iii. When  cid:26  =  cid:0 1 , C = 1 .  2 log 1 + 2P 2 log 1 + P   cid:27 2   .  cid:27 2   .  8. Parallel Gaussian channels  Consider the following parallel Gaussian channel  Z1  cid:24  N  0; N1   ?  -   cid:19  cid:16 +-  cid:18  cid:17  Z2  cid:24  N  0; N2   Y1  ?   cid:19  cid:16 +-  cid:18  cid:17   -  Y2  X1  X2  where Z1  cid:24  N  0;N1  and Z2  cid:24  N  0;N2  are independent Gaussian random variables and Yi = Xi + Zi . We wish to allocate power to the two parallel channels. Let  cid:12 1 and  cid:12 2 be  cid:12 xed. Consider a total cost constraint  cid:12 1P1 +  cid:12 2P2  cid:20   cid:12 ; where Pi is the power allocated to the ith channel and  cid:12 i is the cost per unit power in that channel. Thus P1  cid:21  0 and P2  cid:21  0 can be chosen subject to the cost constraint  cid:12  .  a  For what value of  cid:12  does the channel stop acting like a single channel and start   b  Evaluate the capacity and  cid:12 nd P1; P2 that achieve capacity for  cid:12 1 = 1;  cid:12 2 =  acting like a pair of channels?  2; N1 = 3; N2 = 2 and  cid:12  = 10 .   224  Gaussian channel  Solution: Parallel channels  When we have cost constraints on the power, we need to optimize the total capacity of the two parallel channels  C =  1 2  log cid:18 1 +  P1  N1 cid:19  +  1 2  log cid:18 1 +  P2  N2 cid:19   subject to the constraint that  Using the methods of Section 9.4, we set   cid:12 1P1 +  cid:12 2P2  cid:20   cid:12   J P1; P2  =X 1  2  log cid:18 1 +  Pi  Ni cid:19  +  cid:21  X  cid:12 iPi   and di cid:11 erentiating with respect to Pi , we have  or  or  1 2  1  Pi + Ni  +  cid:21  cid:12 i = 0;  Pi =     cid:23   cid:12 i  cid:0  Ni +:   cid:12 iPi =   cid:23   cid:0   cid:12 iNi +:   9.48    9.49    9.50    9.51    9.52    9.53    a  It follows that we will put all the signal power into the channel with less weighted noise    cid:12 iNi   until the total weighted power of noise + signal in that channel equals the weighted noise power in the other channel. After that, we will split any additional power between the two channels according to their weights. Thus the combined channel begins to behave like a pair of parallel channels when the signal power is equal to the di cid:11 erence of the two weighted noise powers, i.e., when  cid:12 1 cid:12  =  cid:12 2N  cid:0  2  cid:0   cid:12 1N1 .   b  In this case,  cid:12 1N1 <  cid:12 2N2 , so we would put power into channel 1 until  cid:12  = 1 . After that we would put power according to their weights, i.e. we would divide remaining power of 9 in the ratio 2 is to 1. Thus we would set P1 = 6 + 1 and P2 = 3 , and so that  cid:23  = 10 in the equation above. The capacity in this case is  C =  log 1 + 7=3  +  log 1 + 3=2  = 1:53 bits:   9.54   1 2  1 2  9. Vector Gaussian channel  Consider the vector Gaussian noise channel  where X =  X1; X2; X3  , Z =  Z1; Z2; Z3 ; and Y =  Y1; Y2; Y3 ; EkXk2  cid:20  P; and  Y = X + Z;  Z  cid:24  N 0 B@  0;2 64  1 0 1 0 1 1 1 1 2  :  3 75  1 CA   Gaussian channel  225  Find the capacity. The answer may be surprising.  Solution: Vector Gaussian channel  Normally one would water- cid:12 ll over the eigenvalues of the noise covariance matrix. Here we have the degenerate case  i.e., one of the eigenvalue is zero , which we can exploit easily.  Musing upon the structure of the noise covariance matrix, one can see Z1 + Z2 = Z3 . Thus, by processing the output vector as Y1 + Y2  cid:0  Y3 =  X1 + Z1  +  X2 + Z2  cid:0   X3 + Z3  = X1 + X2  cid:0  X3; we can get rid of the noise completely. Therefore, we have in cid:12 nite capacity.  Note that we can reach the conclusion by water- cid:12 lling on the zero eigenvalue.  10. The capacity of photographic  cid:12 lm. Here is a problem with a nice answer that takes a little time. We’re interested in the capacity of photographic  cid:12 lm. The  cid:12 lm consists of silver iodide crystals, Poisson distributed, with a density of  cid:21  particles per square inch. The  cid:12 lm is illuminated without knowledge of the position of the silver iodide particles. It is then developed and the receiver sees only the silver iodide particles that have been illuminated. It is assumed that light incident on a cell exposes the grain if it is there and otherwise results in a blank response. Silver iodide particles that are not illuminated and vacant portions of the  cid:12 lm remain blank. The question is, \What is the capacity of this  cid:12 lm?"  We make the following assumptions. We grid the  cid:12 lm very  cid:12 nely into cells of area dA . It is assumed that there is at most one silver iodide particle per cell and that no silver iodide particle is intersected by the cell boundaries. Thus, the  cid:12 lm can be considered to be a large number of parallel binary asymmetric channels with crossover probability 1  cid:0   cid:21 dA . By calculating the capacity of this binary asymmetric channel to  cid:12 rst order in dA  making the necessary approximations  one can calculate the capacity of the  cid:12 lm in bits per square inch. It is, of course, proportional to  cid:21  . The question is what is the multiplicative constant?  The answer would be  cid:21  bits per unit area if both illuminator and receiver knew the positions of the crystals.  Solution: Capacity of photographic  cid:12 lm  As argued in the problem, each small cell can be modelled as a binary asymmetric Z-channel with probability transition matrix  p yjx  ="  1  1  cid:0   cid:21  dA  cid:21  dA   0  x; y 2 f0; 1g   9.55   where x = 1 corresponds to shining light on the cell. Let  cid:12  =  cid:21  dA .  First we express I X; Y   , the mutual information between the input and output of the Z-channel, as a function of  cid:11  = Pr X = 1  :  H Y jX  = Pr X = 0   cid:1  0 + Pr X = 1   cid:1  H  cid:12   =  cid:11 H  cid:12     226  Gaussian channel  H Y   = H Pr Y = 1   = H  cid:11  cid:12    I X; Y   = H Y    cid:0  H Y jX  = H  cid:11  cid:12    cid:0   cid:11 H  cid:12    Since I X; Y   = 0 when  cid:11  = 0 and  cid:11  = 1 , the maximum mutual information is obtained for some value of  cid:11  such that 0 <  cid:11  < 1 .  Using elementary calculus, we determine that  converting the equation to nats rather than bits ,  d d cid:11   I X; Y   =  cid:12  ln  1  cid:0   cid:11  cid:12   cid:11  cid:12   cid:0  He  cid:12    To  cid:12 nd the optimal value of  cid:11  , we set this equal to 0, and solve for  cid:11  as   cid:11  =  1  cid:12   1 He   cid:12    1 + e   cid:12    cid:13  =  1 He   cid:12    1 + e   cid:12    cid:13  = 1  cid:0   cid:13  =  He  cid:12     cid:12   e  He  cid:12     cid:12   1 + e  He   cid:12     cid:12   =  cid:13 e  ln  cid:13   cid:0  ln  cid:13  =  He  cid:12     cid:12   If we let  then  cid:11  cid:12  =  cid:13  , and  or  so that  I X; Y   = He  cid:11  cid:12    cid:0   cid:11 He  cid:12    1 He   cid:12    He  cid:12     cid:12    cid:12   1 + e  = He  cid:13    cid:0  =  cid:0  cid:13  ln  cid:13   cid:0   cid:13  ln  cid:13   cid:0   cid:13  ln  cid:13   cid:0  ln  cid:13   =  cid:0  ln  cid:13  = ln 1 + e cid:0  He  cid:12    cid:25  e cid:0  He   cid:12   = e cid:0   cid:0  cid:12  ln  cid:12  cid:0  1 cid:0  cid:12   ln 1 cid:0  cid:12    cid:25  e cid:0  cid:0  ln  cid:12  =  cid:12       cid:12    cid:12    cid:12    9.56    9.57    9.58    9.59    9.60    9.61    9.62    9.63    9.64    9.65    9.66    9.67    9.68   Thus the capacity of this channel is approximately  cid:12  nats when  cid:12  ! 0 .  11. Gaussian mutual information. Suppose that  X; Y; Z  are jointly Gaussian and that X ! Y ! Z forms a Markov chain. Let X and Y have correlation coe cid:14 cient  cid:26 1 and let Y and Z have correlation coe cid:14 cient  cid:26 2 . Find I X; Z  .   Gaussian channel  227  Solution: Gaussian Mutual Information  Repeat of problem 8.9  First note that we may without any loss of generality assume that the means of X , Y and Z are zero. If in fact the means are not zero one can subtract the vector of means without a cid:11 ecting the mutual information or the conditional independence of X , Z given Y . Let   cid:3  =    cid:27 2 x   cid:27 x cid:27 z cid:26 xz   cid:27 x cid:27 z cid:26 xz   cid:27 2 z  ! ;  be the covariance matrix of X and Z . We can now use Eq.  9.93  and Eq.  9.94  to compute  I X; Z  = h X  + h Z   cid:0  h X; Z   log  2 cid:25 e cid:27 2  x  +  log  2 cid:25 e cid:27 2  1 2  x   cid:0   log  2 cid:25 ej cid:3 j   1 2  =  1 2 =  cid:0   1 2  log 1  cid:0   cid:26 2 xz   Now,   cid:26 xz =  EfXZg  cid:27 x cid:27 z EfEfXZjY gg   cid:27 x cid:27 z  =  =  =   cid:27 x cid:27 z  EfEfXjY gEfZjY gg Ef cid:16   cid:27 x cid:26 xy Y cid:17 g Y cid:17  cid:16   cid:27 z cid:26 zx   cid:27 y   cid:27 y   cid:27 x cid:27 z  =  cid:26 xy cid:26 zy  I X; Y   =  cid:0   log 1  cid:0   cid:26 2  xy cid:26 2  zy   1 2  We can thus conclude that  12. Time varying channel. A train pulls out of the station at constant velocity. The received signal energy thus falls o cid:11  with time as 1=i2 . The total received signal at time i is  where Z1; Z2; : : : are i.i.d.  cid:24  N  0; N   . The transmitter constraint for block length n is  Yi = cid:18  1  i cid:19  Xi + Zi;  1 n  n  Xi=1  i  w   cid:20  P; w 2 f1; 2; : : : ; 2nRg: x2  Using Fano’s inequality, show that the capacity C is equal to zero for this channel.   228  Gaussian channel  Solution: Time Varying Channel  Just as in the proof of the converse for the Gaussian channel nR = H W   = I W ; ^W   + H Wj ^W     cid:20  I W ; ^W   + n cid:15 n  cid:20  I X n; Y n  + n cid:15 n = h Y n   cid:0  h Y njX n  + n cid:15 n = h Y n   cid:0  h Z n  + n cid:15 n  cid:20   h Yi   cid:0  h Z n  + n cid:15 n  n  h Yi   cid:0   h Zi  + n cid:15 n  n  Xi=1  I Xi; Yi  + n cid:15 n:  n  Xi=1 Xi=1 Xi=1  n  =  =  Pi =  1  2nR Xw  x2 i  w :  Now let Pi be the average power of the i th column of the codebook, i.e.,  Then, since Yi = 1 of Yi is 1  i Xi + Zi and since Xi and Zi are independent, the average power  i2 Pi + N . Hence, since entropy is maximized by the normal distribution,  1 i2 Pi + N  : Continuing with the inequalities of the converse, we obtain  h Yi   cid:20   log 2 cid:25 e   1 2  nR  cid:20  X  h Yi   cid:0  h Zi   + n cid:15 n 1 i2 Pi + N     cid:0  i2N cid:19  + n cid:15 n: Pi   cid:20  X cid:18  1 = X 1  log cid:18 1 +  log 2 cid:25 e   2  2  1 2  log 2 cid:25 eN cid:19  + n cid:15 n  Since each of the codewords satis cid:12 es the power constraint, so does their average, and hence  1  nXi  Pi  cid:20  P:  This corresponds to a set of parallel channels with increasing noise powers. Using water cid:12 lling, the optimal solution is to put power into the  cid:12 rst few channels which have the lowest noise power. Since the noise power in the channel i is Ni = i2N , we will put power into channels only where Pi + Ni  cid:20   cid:21  . The height of the water level in the water  cid:12 lling is less than N +nP , and hence the for all channels we put power, i2N < nP +N , or only o pn  channels. The average rate is less than 1 2 log 1 + nP=N   and the capacity per transmission goes to 0. Hence there capacity of this channel is 0.  npn 1   9.69    9.70    9.71    9.72    9.73    9.74    9.75    9.76    9.77    9.78    9.79    9.80    9.81    9.82    Gaussian channel  229  13. Feedback capacity for n = 2 . Let  Z1; Z2   cid:24  N  0; K ; K = " 1  cid:26    cid:26  1  : Find the  with and without feedback given a trace  power  constraint  2 log jKX+Zj jKZj  maximum of 1 tr KX    cid:20  2P: Solution: Feedback capacity  Without feedback, the solution is based on water cid:12 lling. The eigenvalues of the matrix are 1  cid:6   cid:26  , and therefore if P <  cid:26  , we would use only one of the channels, and achieve capacity C = 1 1 cid:0  cid:26    . For P  cid:21   cid:26  , we would use both eigenvalues and the waterlevel for water  cid:12 lling would be obtained by distributing the remaining power equally across both eigenvalues. Thus the water level would be  1 +  cid:26   +  2P  cid:0  2 cid:26  =2 = 1 + P , and the capacity would be C = 1  2 log 1 + 2P  2 log  1+P  1+ cid:26    + 1  2 log  1+P  1 cid:0  cid:26    .  With feedback, the solution is a a little more complex. From  9.102 , we have  Cn;F B = max  log j B + I K  n   1 2n  Z  B + I t + KV j jK  n  Z j  where the maximum is taken over all nonnegative de cid:12 nite KV and strictly lower tri- angular B such that  In the case when n = 2 ,  tr BK  n   Z Bt + KV    cid:20  nP:   B + I K  n   Z  B + I t + KV =   1 0 =   1 + P1   cid:26  + b  b 1 !  1  cid:26    cid:26  1 !  1 b 1 + P2 + 2 cid:26 b + b2 !   cid:26  + b  0 1 ! +  P1  0 P2 ! 9.85   0  subject to the constraint that  tr  P1  0 P2 + b2 !  cid:20  2P  0  Expanding this, we obtain the mutual information as  I X; Y   = 1 + P1 + P2 + P1P2 + P1b2 + 2P1b cid:26   cid:0   cid:26 2  Setting up the functional and di cid:11 erentiating with respect to the variables, we obtain the following relationships  subject to  and  P1 + P2 + b2 = 2P  P1 = P2 + b2 + 2b cid:26   b =  cid:26 P1   9.83    9.84    9.86    9.87    9.88    9.89    9.90    9.91    230  Gaussian channel  14. Additive noise channel. Consider the channel Y = X + Z , where X is the trans- mitted signal with power constraint P , Z is independent additive noise, and Y is the received signal. Let  Z =  0;  with prob. 1 10 Z cid:3 ; with prob. 9 10  ;  where Z cid:3   cid:24  N  0; N  : Thus Z has a mixture distribution which is the mixture of a Gaussian distribution and a degenerate distribution with mass 1 at 0.   a  What is the capacity of this channel? This should be a pleasant surprise.   b  How would you signal in order to achieve capacity?  Solution: Additive Noise channel  The capacity of this channel is in cid:12 nite, since at the times the noise is 0 the output is exactly equal to the input, and we can send an in cid:12 nite number of bits.  To send information through this channel, just repeat the same real number at the input. When we have three or four outputs that agree, that should correspond to the points where the noise is 0, and we can decode an in cid:12 nite number of bits.  15. Discrete input continuous output channel. Let PrfX = 1g = p , PrfX = 0g = 1  cid:0  p , and let Y = X + Z , where Z is uniform over the interval [0; a] , a > 1 , and Z is independent of X .   a  Calculate   b  Now calculate I X; Y   the other way by  I X; Y   = H X   cid:0  H XjY  :  I X; Y   = h Y    cid:0  h Y jX :   c  Calculate the capacity of this channel by maximizing over p  Solution: Discrete input Continuous Output channel   a  Since  and  Therefore,  a  a 0  f  Y jX = 0  =  1  1  cid:0  p  1 1 a p 1 a  f  Y jX = 1  =8>< >: f  y  =8><  1  cid:0  p  1 1 a >: p 1 a  a  0  cid:20  y < a otherwise  0  cid:20  y < 1 1  cid:20  y  cid:20  a a < y < 1 + a  0  cid:20  y < 1 1  cid:20  y  cid:20  a a < y < 1 + a   9.92    9.93    9.94    Gaussian channel  231   b  H X  = H p  . H XjY = y  is nonzero only for 1  cid:20  y  cid:20  a , and by Bayes rule,  conditioned on Y , the probabilty that X = 1 is  P  X = 1jY = y  =  P  X = 1 f  yjX = 1   P  X = 1 f  yjX = 1  + P  X = 0 f  yjX = 0   = p   9.95   and hence H XjY   = P  1  cid:20  Y  cid:20  a H p  = a cid:0 1 H X   cid:0  H XjY   = 1  a H p  .  a H p  . Therefore I X; Y   =   c  f  Y jX = 0   cid:24  U  0; a  , and hence h Y jX = 0  = log a , and similarly for X = 1 ,  so that h Y jX  = log a . The di cid:11 erential entropy h Y   can be calculated from  9.94  as  h Y   =  cid:0 Z 1  0  1 a  log   1  cid:0  p   1  cid:0  p a  dy  cid:0 Z a  1 a   cid:0 p log p  cid:0   1  cid:0  p  log 1  cid:0  p   + log a H p  + log a  log  1 a  1  =  =  1 a 1 a  dy  cid:0 Z 1+a  a  p a  p a  log  dy 9.96    9.97    9.98   and again I X; Y   = h Y    cid:0  h Y jX  = 1  a H p  .   d  The mutual information is maximized for p = 0:5 , and the corresponding capacity  of the channel is 1 a .  16. Gaussian mutual information  Suppose that  X; Y; Z  are jointly Gaussian and that X ! Y ! Z forms a Markov chain. Let X and Y have correlation coe cid:14 cient  cid:26 1 and let Y and Z have correlation coe cid:14 cient  cid:26 2 . Find I X; Z  .  Solution: Gaussian Mutual Information  Repeat of problem 8.9  First note that we may without any loss of generality assume that the means of X , Y and Z are zero. If in fact the means are not zero one can subtract the vector of means without a cid:11 ecting the mutual information or the conditional independence of X , Z given Y . Let   cid:3  =    cid:27 2 x   cid:27 x cid:27 z cid:26 xz   cid:27 x cid:27 z cid:26 xz   cid:27 2 z  ! ;  be the covariance matrix of X and Z . We can now use Eq.  9.93  and Eq.  9.94  to compute  I X; Z  = h X  + h Z   cid:0  h X; Z   log  2 cid:25 e cid:27 2  x  +  log  2 cid:25 e cid:27 2  1 2  x   cid:0   log  2 cid:25 ej cid:3 j   1 2  =  1 2 =  cid:0   1 2  log 1  cid:0   cid:26 2 xz    232  Now,  Gaussian channel   cid:26 xz =  EfXZg  cid:27 x cid:27 z EfEfXZjY gg   cid:27 x cid:27 z  =  =  =   cid:27 x cid:27 z  EfEfXjY gEfZjY gg Ef cid:16   cid:27 x cid:26 xy Y cid:17 g Y cid:17  cid:16   cid:27 z cid:26 zx   cid:27 y   cid:27 y   cid:27 x cid:27 z  =  cid:26 xy cid:26 zy  I X; Y   =  cid:0   log 1  cid:0   cid:26 2  xy cid:26 2  zy   1 2  Z i  X i  Y i  We can thus conclude that  17. Impulse power.  Consider the additive white Gaussian channel  where Zi  cid:24  N  0; N   , and the input signal has average power constraint P: 1 = nP and EX 2  a  Suppose we use all our power at time 1;  i.e. EX 2  i = 0; for  i = 2; 3; : : : ; n: Find  I X n; Y n   max f  xn   n  where the maximization is over all distributions f  xn  subject to the constraint EX 2  i = 0; for i = 2; 3; : : : ; n:  1 = nP and EX 2   b  Find  and compare to part  a :  Solution: Impulse power.  f  xn : E  1  i=1 X 2  i   cid:20 P  max  nPn  1 n  I X n; Y n    Gaussian channel  233   a    b   where  a  comes from the constraint that all our power, nP , be used at time 1 and  b  comes from that fact that given Gaussian noise and a power constraint nP , I X; Y    cid:20  1  2 log 1 + nP  N   .  I X n; Y n   max   a  = max  I X1; Y1   n  n  1  N  cid:17  2 log cid:16 1 + nP  n   b  =  I X n; Y n   max  n   a  = max  nI X; Y    n  = max I X; Y   P  log cid:18 1 +  N cid:19   1 2  =  where  a  comes from the fact that the channel is memoryless. Notice that the quantity in part  a  goes to zero as n ! 1 while the quantity in part  b  stays constant. Hence the impulse scheme is suboptimal.  18. Gaussian channel with time-varying mean. Find the capacity of the following  Gaussian channels.  Zi  -  ’$? &%  Xi  -  Yi  Let Z1; Z2; : : : be independent and let there be a power contraint P on xn W   . Find the capacity when   a   cid:22 i = 0 , for all i .  b   cid:22 i = ei;  i = 1; 2; : : : Assume  cid:22 i known to the transmitter and receiver.   234  Gaussian channel   c   cid:22 i unknown, but  cid:22 i i.i.d.  cid:24  N  0; N1  for all i .  Solution: Gaussian Noise with time-varying mean   a  This is the classical Gaussian channel capacity problem with  C =  1 2  log cid:18 1 +  P  N cid:19  :  C =  1 2  log cid:18 1 +  P  N cid:19  :   b  Since the transmitter and the receiver both know the means, the receiver can simply subtract the mean while decoding. Thus, we are back in case  a . Hence the capacity is   c  Let pi be the density of Zi . Clearly pi is independent of the time index i . Also  where  cid:3  represents convolution. From the distribution of Zi it is obvious that the optimal input distribution Xi is N  0; P   and the capacity is  p y  = Z  1  p2 cid:25 N 1  e cid:0   cid:22 2  2N 1  1  p2 cid:25 N  e cid:0   y cid:0  cid:22  2  2N  = N  0; N    cid:3  N  0; N1  = N  0; N + N1 ;  C =  1 2  log cid:18 1 +  P  N + N1 cid:19  :  19. A parametric form for channel capacity  Consider m parallel Gaussian channels, Yi = Xi + Zi , where Zi  cid:24  N  0;  cid:21 i  and the noises Xi are independent r.v.’s. Thus C = Pm   where  cid:21  is chosen to satisfy Pm  i=1  cid:21   cid:0   cid:21 i + = P . Show that this can be rewritten in the form  2 log 1 +   cid:21  cid:0  cid:21 i +  i=1   cid:21 i  1  P   cid:21   = Pi: cid:21 i cid:20  cid:21   cid:21   cid:0   cid:21 i  C  cid:21   = Pi: cid:21 i cid:20  cid:21   2 log  cid:21   cid:21 i  1  :  Here P   cid:21   is piecewise linear and C  cid:21   is piecewise logarithmic in  cid:21 :  Solution: Parametric form of channel capacity  The optimal strategy for parallel Gaussian channels is given by water- cid:12 lling. Here,  cid:21  represents the maximum received power in any channel which is being used; i.e. any channel i for which  cid:21 i <  cid:21  will act as a single Gaussian channel with noise Ni =  cid:21 i and will communicate a signal with power Pi =  cid:21   cid:0  Ni . The   cid:1  + notation ensures   235   9.99    9.100    9.101    9.102    9.103   Gaussian channel  that channels with  cid:21 i >  cid:21  will not be used. Thus, the the total transmitted power, as a function of  cid:21  , is given by  P   cid:21   = Xi: cid:21 i< cid:21   Pi = Xi: cid:21 i< cid:21     cid:21   cid:0   cid:21 i  =Xi    cid:21   cid:0   cid:21 i +  Now, if we consider the capacity of channel i,  Ci =  =  =  1 2 1 2 1 2  log cid:18 1 + log cid:18 1 +  Pi  Ni cid:19   cid:21 i  cid:19   cid:21   cid:0   cid:21 i  log   cid:21   cid:21 i  C  cid:21   = Xi: cid:21 i< cid:21   Ci = Xi: cid:21 i< cid:21   1 2  log   cid:21   cid:21 i  Y = X + Z;  EX 2  cid:20  P;  EZ 2 = N:  and we obtain  20. Robust decoding. Consider an additive noise channel whose output Y is given by  where the channel input X is average power limited,  and the noise process fZkg1k= cid:0 1 is iid with marginal distribution pZ z   not necessarily Gaussian  of power N ,   a  Show that the channel capacity, C = maxEX 2 cid:20 P I X; Y   , is lower bounded by  CG where  CG =  1 2  log cid:18 1 +  P  N cid:19  ;  i.e., the capacity CG corresponding to white Gaussian noise.   b  Decoding the received vector to the codeword that is closest to it in Euclidean distance is in general sub-optimal, if the noise is non-Gaussian. Show, however, that the rate CG is achievable even if one insists on performing nearest neigh- bor decoding  minimum Euclidean distance decoding  rather than the optimal maximum-likelihood or joint typicality decoding  with respect to the true noise distribution .   c  Extend the result to the case where the noise is not iid but is stationary and  ergodic with power N .   236  Gaussian channel  Hint for b and c: Consider a size 2nR random codebook whose codewords are drawn independently of each other according to a uniform distribution over the n dimensional sphere of radius pnP .   cid:15  Using a symmetry argument show that, conditioned on the noise vector, the ensem- ble average probability of error depends on the noise vector only via its Euclidean norm kzk .   cid:15  Use a geometric argument to show that this dependence is monotonic.  cid:15  Given a rate R   N such that N0 cid:19  :  log cid:18 1 +  R <  1 2  P  Compare the case where the noise is iid N  0; N 0  to the case at hand.   cid:15  Conclude the proof using the fact that the above ensemble of codebooks can achieve  the capacity of the Gaussian channel  no need to prove that .  Solution: Robust decoding   a  The fact that the worst noise is Gaussian is a consequence of the entropy power inequality, and is proved in problem 9.21. Since CG is the capacity of the Gaussian, it is the lower bound on the capacity of the channel for all noise distributions.   b  As suggested in the hint, we will draw codewords at random according to a uniform distribution on a sphere of radius pnP . We will send a codeword over the channel, and given the received sequence,  cid:12 nd the codeword that is closest  in Euclidean distance  to the received sequence. First, by the symmetry of the code construction, the probability of error does not depend on which message was sent, so without loss of generality, we can assume that message 1  i.e., codeword 1 was sent . The probability of error then depends only on whether the noise sequence Z n is such that the received vector the closer to some other codeword. However, given any transmitted codeword, all the other codewords are randomly distributed in all directions, and therefore the probability of error does not depend on the direction of the error, only on the norm jjX 1  + Zjj and jjZjj . By the spherical symmetry of the choice of X 1  , the probability of error depends only on jjZjj . To show monotonocity of the error rate with the norm of the noise, consider an error where the received sequence X 1 +Z is closer to some other codeword X 2  say. Now if increase the norm of the error a little, we have  jjX 1  + Z 1 +  cid:1    cid:0  X 2 jj = jjX 1  + Z  cid:0  X 2 jj +  cid:1 jjZjj   9.104   by the triangle inequality, and hence if the output is closer to X 2  , then increasing the norm of the noise will not reduce the probability that it is closer to X 2  . Thus the error probability is monotonically decreasing the the norm of the error.   Gaussian channel  237  Finally we consider using this code on a Gaussian channel with noise N 0 > N , such the R < 1 2 log 1 + P=N0  . Since this is a Gaussian channel, the standard results show that we can achieve arbitrarily low probability of error for this code. Now comparing the non Gaussian channel with the Gaussian channel, we can see that probability close to 1 that the norm of the error in the Gaussian channel is less than the norm of the error for the non Gaussian channel. By the monotonicity of the probability of error with respect to the norm of the noise, we can see that the probability of error for the non-Gaussian channel is less than the probability of error for the Gaussian channel, and hence goes to 0 as the block length goes to 1 .  21. A mutual information game. Consider the following channel:  Z  X  ?   cid:19  cid:16 -  cid:18  cid:17   -  Y  EX = 0;  EX 2 = P;  EZ = 0;  EZ 2 = N;  Throughout this problem we shall constrain the signal power  and the noise power  and assume that X and Z are independent. The channel capacity is given by I X; X + Z  .  Now for the game. The noise player chooses a distribution on Z to minimize I X; X + Z ; while the signal player chooses a distribution on X to maximize I X; X + Z : Letting X cid:3   cid:24  N  0; P  ; Z cid:3   cid:24  N  0; N  ; show that Gaussian X cid:3  and Z cid:3  satisfy the saddlepoint conditions  I X; X + Z cid:3    cid:20  I X cid:3 ; X cid:3  + Z cid:3    cid:20  I X cid:3 ; X cid:3  + Z :  Thus  min  max  I X; X + Z  = max  min  I X; X + Z   Z  X  =  Z  X 1 2  log cid:18 1 +  P  N cid:19  ;  and the game has a value. In particular, a deviation from normal for either player worsens the mutual information from that player’s standpoint. Can you discuss the implications of this?   9.105    9.106    9.107    9.108    9.109    238  Gaussian channel  Note: Part of the proof hinges on the entropy power inequality from Chapter 17, which states that if X and Y are independent random n -vectors with densities, then  2  2  n h X+Y   cid:21  2  2  n h X  + 2  n h Y :  2   9.110   Solution: A mutual information game. Let X and Z be random variables with EX = 0 , EX 2 = P , EZ = 0 and EZ 2 = N . Let X cid:3   cid:24  N  0; P   and Z cid:3   cid:24  N  0; N   . Then as proved in class, I X; X + Z cid:3   = h X + Z cid:3    cid:0  h X + Z cid:3 jX    9.111   = h X + Z cid:3    cid:0  h Z cid:3    cid:20  h X cid:3  + Z cid:3    cid:0  h Z cid:3   = I X cid:3 ; X cid:3  + Z cid:3  ;  where the inequality follows from the fact that given the variance, the entropy is max- imized by the normal.  To prove the other inequality, we use the entropy power inequality,  Let  Then  22h X+Z   cid:20  22h X  + 22h Z :  g Z  =  22h Z  2 cid:25 e  :  I X cid:3 ; X cid:3  + Z  = h X cid:3  + Z   cid:0  h X cid:3  + ZjX cid:3   log cid:16 22h X  cid:3   + 22h Z  cid:17   cid:0  h Z  log   2 cid:25 e P +  2 cid:25 e g Z    cid:0  log cid:18 1 +  = h X cid:3  + Z   cid:0  h Z   cid:21  =  g Z  cid:19  ;  1 2 1 2 1 2  =  P  1 2  log 2 cid:25 e g Z    9.120   where the inequality follows from the entropy power inequality. Now 1 + P is a g Z  decreasing function of g Z  , it is minimized when g Z  is maximum, which occurs when h Z  is maximized, i.e., when Z is normal. In this case, g Z  cid:3   = N and we have the following inequality,  I X cid:3 ; X cid:3  + Z   cid:21  I X cid:3 ; X cid:3  + Z cid:3  :  Combining the two inequalities, we have  I X; X + Z cid:3    cid:20  I X cid:3 ; X cid:3  + Z cid:3    cid:20  I X cid:3 ; X cid:3  + Z :   9.112    9.113    9.114    9.115    9.116    9.117    9.118    9.119    9.121    9.122    9.123    Gaussian channel  Hence, using these inequalities, it follows directly that  min  max  Z  X  I X; X + Z   cid:20  max  X  I X; X + Z cid:3    = I X cid:3 ; X cid:3  + Z cid:3   = min  cid:20  max  min  I X cid:3 ; X cid:3  + Z   X  Z  Z  I X cid:3 ; X cid:3  + Z :  We have shown an inequality relationship in one direction between minZ maxX I X; X+ Z  and maxX minZ I X; X + Z  . We will now prove the inequality in the other direc- tion is a general result for all functions of two variables.  For any function f  a; b  of two variables, for all b , for any a0 ,  Taking the minimum over a0 , we have  f  a0; b   cid:21  min  a  f  a; b :  max  b  f  a0; b   cid:21  max  b  a  min  f  a; b :  min a0  max  b  f  a0; b   cid:21  min  a0  max  min  f  a; b :  b  a  min  max  a  b  f  a; b   cid:21  max  b  a  min  f  a; b :  Hence  or  From this result,  min  max  Z  X  I X; X + Z   cid:21  max  X  Z  min  I X; X + Z :   9.132   From  9.127  and  9.132 , we have  min  max  I X; X + Z  = max  min  I X; X + Z   Z  X  =  Z  X 1 2  log cid:18 1 +  P  N cid:19  :  This inequality implies that we have a saddlepoint in the game, which is the value of If signal player chooses X cid:3  , the noise player cannot do any better than the game. choosing Z cid:3  . Similarly, any deviation by the signal player from X cid:3  will make him do worse, if the noise player has chosen Z cid:3  . Any deviation by either player will make him do worse.  Another implication of this result is that not only is the normal the best possible signal distribution, it is the worst possible noise distribution.  239   9.124    9.125    9.126    9.127    9.128    9.129    9.130    9.131    9.133    9.134    240  22. Recovering the noise  Gaussian channel  i=1 X 2  Consider a standard Gaussian channel Y n = X n + Z n; where Zi is i.i.d.  cid:24  N  0; N  ; i = 1; 2; : : : ; n; and 1 Here we are interested in recovering the noise Z n and we don’t care about the signal X n: By sending X n =  0; 0; : : : ; 0  , the receiver gets Y n = Z n and can fully determine the value of Z n . We wonder how much variability there can be in X n and still recover the Gaussian noise Z n . The use of the channel looks like  nPn  i  cid:20  P:  Z n  ?   cid:19  cid:16 -  cid:18  cid:17   X n  -  Y n  -  ^Z n Y n   Argue that, for some R > 0 , the transmitter can arbitrarily send one of 2nR di cid:11 erent sequences of xn without a cid:11 ecting the recovery of the noise in the sense that  Prf ^Z n 6= Z ng ! 0  as n ! 1 :  For what R is this possible?  Solution: Recovering the noise We prove that sup R = C = C P=N  : If R < C , from the achievability proof of the channel coding theorem, 2nR di cid:11 erent X n sequences can be decoded correctly with arbitrarily small error for n large enough. Once X n is determined, Z n can be easily computed as Y n  cid:0  X n . We show that this is optimal by using proof by contradiction. Assume that there is some R > C such that Z n can be recovered with Prf ^Z n 6= Z ng ! 0 as n ! 1 : But this implies that X n = Y n  cid:0  Z n can be determined with arbitrary precision; that is, there is a codebook X n W  ; W = 1; : : : ; 2nR with R > C and Prf ^X n 6= X ng = PrfW  cid:20  ^Wg ! 0 as n ! 1 : As we saw in the converse proof of the channel coding theorem, this is impossible. Hence, we have the contradiction and R cannot be greater than C:   Chapter 10  Rate Distortion Theory  1. One bit quantization of a single Gaussian random variable. Let X  cid:24  N  0;  cid:27  2  and let the distortion measure be squared error. Here we do not allow block descriptions. Show that the optimum reproduction points for 1 bit quantization are  cid:6 q 2  cid:25   cid:27  , and that  the expected distortion for 1 bit quantization is  cid:25  cid:0 2 Compare this with the distortion rate bound D =  cid:27 22 cid:0 2R for R = 1 . Solution: One bit quantization of a Gaussian random variable. Let X  cid:24  N  0;  cid:27  2  and let the distortion measure be squared error. With one bit quantization, the obvious reconstruction regions are the positive and negative real axes. The reconstruction point is the centroid of each region. For example, for the positive real line, the centroid a is   cid:25   cid:27 2 .  using the substitution y = x2=2 cid:27 2 . The expected distortion for one bit quantization is  e cid:0  x2  2 cid:27 2 dx  2  x  p2 cid:25  cid:27 2  cid:27 r 2   cid:25   0  a = Z 1 = Z 1 =  cid:27 r 2   cid:25   0  ;  e cid:0 y dy  e cid:0  x2  2 cid:27 2 dx  D = Z 0  1   cid:25 !2  cid:0 1 x +  cid:27 r 2 p2 cid:25  cid:27 2 0  x  cid:0   cid:27 r 2  cid:25 !2 +Z 1  cid:25  cid:19   cid:0 1 cid:18 x2 +  cid:27 2 2 = 2Z 1 p2 cid:25  cid:27 2  cid:25 ! 0   cid:0 2x cid:27 r 2  cid:0 2Z 1  e cid:0  x2  2 cid:27 2 dx  1  p2 cid:25  cid:27 2 e cid:0  x2 1  2 cid:27 2 dx  1  p2 cid:25  cid:27 2  e cid:0  x2  2 cid:27 2 dx  241   10.1    10.2    10.3    10.4    10.5    10.6    10.7    242  Rate Distortion Theory  1 p2 cid:25    cid:27 2r 2   cid:25   =  cid:27 2 +  2  cid:25  =  cid:27 2  cid:25   cid:0  2   cid:27 2  cid:0  4 :   cid:25    10.8    10.9    10.11    10.12    10.13    10.14   2. Rate distortion function with in cid:12 nite distortion. Find the rate distortion func-  tion R D  = min I X; ^X  for X  cid:24  Bernoulli   1  2   and distortion  Solution: Rate Distortion. We wish to evaluate the rate distortion function  R D  =  min  I X; ^X :   10.10   Since d 0; 1  = 1 , we must have p 0; 1  = 0 for a  cid:12 nite distortion. Thus, the distortion D = p 1; 0  , and hence we have the following joint distribution for  X; ^X   assuming D  cid:20  1 2  .  d x; ^x  =8>< >:  0; x = ^x; 1; x = 1; ^x = 0; 1; x = 0; ^x = 1:  p ^xjx :P x;^x  p x p ^xjx d x;^x  cid:20 D  p x; ^x  =" 1  2 D 1  0  2  cid:0  D   The mutual information for this joint distribution is R D  = I X; ^X  = H X   cid:0  H Xj ^X   = H   1 2  ;  1 2     cid:0     + D H  1  2  ;  D  2 + D!  1  = 1 +  log  + D log  1 2  1 2 + D  1 2 + D D  ;  1 2 + D  1 2 1 2  which is the rate distortion function for this binary source if 0  cid:20  D  cid:20  1 achieve D = 1  2 with zero rate  use p ^x = 0  = 1  , we have R D  = 0 for D  cid:21  1 2 .  2 . Since we can  3. Rate distortion for binary source with asymmetric distortion. Fix p ^xjx  and  evaluate I X; ^X  and D for   The rate distortion function cannot be expressed in closed form.  Solution: Binary source with asymmetric distortion. X  cid:24  Bern  1 measure is  2  , and the distortion   10.15   X  cid:24  Bern 1=2 ; d x; ^x  =" 0 a b 0   d x; ^x  =" 0 a b 0  :   Rate Distortion Theory  243  Proceeding with the minimization to calculate R D  as  R D  =  min  p ^xjx :P p x p ^xjx d x;^x  cid:20 D  I X; ^X ;   10.16   we must choose the conditional distribution p ^xjx  . Setting p 0j0  =  cid:11  and p 1j1  =  cid:12  , we get the joint distribution  p x; ^x  ="   cid:11  2 1 cid:0  cid:12  2  1 cid:0  cid:11  2  cid:12  2   :  Hence the distortion constraint can be written as 1  cid:0   cid:12  2  b  cid:20  D: The function to be minimized, I X; ^X  , can be written  1  cid:0   cid:11   a +  2  I X; ^X  = H  ^X   cid:0  H  ^XjX  = H    cid:11  + 1  cid:0   cid:12   2  1 2     cid:0   H  cid:11    cid:0   1 2  H  cid:12  :   10.19   Using the method of Lagrange multipliers, we have  J  cid:11 ;  cid:12 ;  cid:21   = H    cid:11  + 1  cid:0   cid:12   2  1 2     cid:0   H  cid:11    cid:0   1 2  H  cid:12   +  cid:21    1  cid:0   cid:11   2  a +  1  cid:0   cid:12  2  b    10.20   and di cid:11 erentiating to  cid:12 nd the maximum, we have the following equations:  1  2 log 2 log  cid:0   1  2  1 cid:0  cid:11 + cid:12   cid:11 +1 cid:0  cid:12  1 cid:0  cid:11 + cid:12   cid:11 +1 cid:0  cid:12   2 !  cid:0  2 !  cid:0   2  1  2 cid:18 log 2 cid:18 log 1  cid:0   cid:11   1  2   cid:11   cid:19   cid:0  1  cid:0   cid:11   cid:12   cid:19   cid:0  1  cid:0   cid:12  1  cid:0   cid:12  2  a +   cid:21 a 2   cid:21 b 2  b = D  = 0  = 0  In principle, these equations can be solved for  cid:11  ,  cid:12  , and  cid:21  and substituted back in the de cid:12 nition to  cid:12 nd the rate distortion function. This problem unfortunately does not have an explicit solution.  4. Properties of R D  . Consider a discrete source X 2 X = f1; 2; : : : ; mg with distri- bution p1; p2; : : : ; pm and a distortion measure d i; j  . Let R D  be the rate distortion function for this source and distortion measure. Let d0 i; j  = d i; j   cid:0  wi be a new distortion measure and let R0 D  be the corresponding rate distortion function. Show that R0 D  = R D +  cid:22 w  , where  cid:22 w = P piwi , and use this to show that there is no essential loss of generality in assuming that min ^x d i; ^x  = 0 , i.e., for each x 2 X , there is one symbol ^x which reproduces the source with zero distortion.  This result is due to Pinkston[9].   10.17    10.18    10.21    10.22    10.23    244  Rate Distortion Theory  Solution: Properties of the rate distortion function. By de cid:12 nition,  R0 D0  =  min  I X; ^X :   10.24   p ^xjx :P p ^xjx p x d0 x;^x  cid:20 D0 For any conditional distribution p ^xjx  , we have p x p ^xjx d0 x; ^x   D0 = Xx;^x = Xx;^x = Xx;^x = D  cid:0 Xx = D  cid:0   cid:22 w;  p x p ^xjx  d x; ^x   cid:0  wx  p x p ^xjx d x; ^x   cid:0 Xx  p x wx  p x wxX^x  p ^xjx   R0 D0  =  min  I X; ^X   =  p ^xjx :P p ^xjx p x d0 x;^x  cid:20 D0 p ^xjx :P p ^xjx p x d x;^x  cid:20 D0+  cid:22 w  min  = R D0 +  cid:22 w :  I X; ^X    10.25    10.26    10.27    10.28    10.29    10.30    10.31    10.32   or D = D0 +  cid:22 w . Hence  For any distortion matrix, we can set wi = min^x d i; ^x  , hence ensuring that min^x d0 x; ^x  = 0 for every x . This produces only a shift in the rate distortion function and does not change the essential theory. Hence, there is no essential loss of generality in assuming that for each x 2 X , there is one symbol ^x which reproduces it with zero distortion. 5. Rate distortion for uniform source with Hamming distortion. Consider a source X uniformly distributed on the set f1; 2; : : : ; mg . Find the rate distortion function for this source with Hamming distortion, i.e.,  d x; ^x  =  0 if x = ^x; 1 if x 6= ^x:  d x; ^x  =  0 if x = ^x 1 if x 6= ^x  Solution: Rate distortion for uniform source with Hamming distortion. X is uniformly distributed on the set f1; 2; : : : ; mg . The distortion measure is  Consider any joint distribution that satis cid:12 es the distortion constraint D . Since D = Pr X 6= ^X  , we have by Fano’s inequality  H Xj ^X   cid:20  H D  + D log m  cid:0  1 ;   10.33    Rate Distortion Theory  and hence  I X; ^X  = H X   cid:0  H Xj ^X    cid:21  log m  cid:0  H D   cid:0  D log m  cid:0  1 :  We can achieve this lower bound by choosing p ^x  to be the uniform distribution, and the conditional distribution of p xj^x  to be p ^xjx   = 1  cid:0  D   10.36   if ^x = x if ^x 6= x:  = D= m  cid:0  1   It is easy to verify that this gives the right distribution on X and satis cid:12 es the bound with equality for D < 1  cid:0  1  m . Hence  R D   = log m  cid:0  H D   cid:0  D log m  cid:0  1   0  if 0  cid:20  D  cid:20  1  cid:0  1 if D > 1  cid:0  1 m :  m   10.37   6. Shannon lower bound for the rate distortion function. Consider a source X with a distortion measure d x; ^x  that satis cid:12 es the following property: all columns of the distortion matrix are permutations of the set fd1; d2; : : : ; dmg . De cid:12 ne the function  10.38    cid:30  D  =  H p :  max i=1 pidi cid:20 D  p:Pm  The Shannon lower bound on the rate distortion function[13] is proved by the following steps:   a  Show that  cid:30  D  is a concave function of D .  b  Justify the following series of inequalities for I X; ^X  if Ed X; ^X   cid:20  D ,  I X; ^X  = H X   cid:0  H Xj ^X   = H X   cid:0 X^x  cid:21  H X   cid:0 X^x  cid:21  H X   cid:0   cid:30  X^x   cid:21  H X   cid:0   cid:30  D ;  p ^x H Xj ^X = ^x  p ^x  cid:30  D^x   p ^x D^x!  where D^x =Px p xj^x d x; ^x  .   c  Argue that  which is the Shannon lower bound on the rate distortion function.  R D   cid:21  H X   cid:0   cid:30  D ;  245   10.34    10.35    10.39    10.40    10.41    10.42    10.43    10.44    246  Rate Distortion Theory   d  If in addition, we assume that the source has a uniform distribution and that the rows of the distortion matrix are permutations of each other, then R D  = H X   cid:0   cid:30  D  , i.e., the lower bound is tight.  Solution: Shannon lower bound on the rate distortion function.   a  We de cid:12 ne   cid:30  D  =  max i=1 pidi cid:20 D  H p :  p:Pm  From the de cid:12 nition, if D1  cid:21  D2 , then  cid:30  D1   cid:21   cid:30  D2  since the maximization is over a larger set. Hence  cid:30  D  is a monotonic increasing function. To prove concavity of  cid:30  D  , consider two levels of distortion D1 and D2 and let p 1  and p 2  achieve the maxima in the de cid:12 nition of  cid:30  D1  and  cid:30  D2  . Let p  cid:21   be the mixture of the two distributions, i.e.,  p  cid:21   =  cid:21 p 1  +  1  cid:0   cid:21  p 2 : Then the distortion is a mixture of the two distortions  Since entropy is a concave function, we have  Hence  D cid:21  =Xi  p  cid:21   i di =  cid:21 D1 +  1  cid:0   cid:21  D2:  H p  cid:21     cid:21   cid:21 H p 1   +  1  cid:0   cid:21  H p 2  :   cid:30  D cid:21   =  max  H p   p:P pidi=D cid:21   cid:21  H p  cid:21     cid:21   cid:21 H p 1   +  1  cid:0   cid:21  H p 2   =  cid:21  cid:30  D1  +  1  cid:0   cid:21   cid:30  D2 ;  proving that  cid:30  D  is a concave function of D .   b  For any  X; ^X  that satisfy the distortion constraint, we have  I X; ^X   p ^x H Xj ^X = ^x    a    b    c   = H X   cid:0  H Xj ^X  = H X   cid:0 X^x  cid:21  H X   cid:0 X^x  cid:21  H X   cid:0   cid:30  X^x  cid:21  H X   cid:0   cid:30  D ;   e    d   p ^x  cid:30  D^x   p ^x D^x    10.45    10.46    10.47    10.48    10.49    10.50    10.51    10.52    10.53    10.54    10.55    10.56    10.57    Rate Distortion Theory  247  where  a  follows from the de cid:12 nition of mutual information,  b  from the de cid:12 nition of conditional entropy,   c  follows from the de cid:12 nition of  cid:30  D ^x  where D^x =P p xj^x d x; ^x  =P p xj^x di0 that H p xj^x    cid:20   cid:30  D^x   d  follows from Jensen’s inequality and the concavity of  cid:30  , and  e  follows from the monotonicity of  cid:30  and the fact that P p ^x D^x =P p x; ^x d x; ^x   cid:20   D . Hence, from the de cid:12 nition of the rate distortion function, we have  R D  =  min  I X; ^X   p ^xjx :P p x;^x d x;^x  cid:20 D   cid:21  H X   cid:0   cid:30  D ;   10.58    10.59   which is the Shannon lower bound on the rate distortion function.   c  Let p cid:3  =  p cid:3 1; p cid:3 2; : : : ; p cid:3 m  be the distribution that achieves the maximum in the de cid:12 nition of the  cid:30  D  . Assume that the source has a uniform distribution and that the rows of the distortion matrix are permutations of each other. Let the distortion matrix be [aij] . We can then choose p ^x  to have a uniform distribution and choose p x = ij^x = j  = p cid:3 k , if aij = dk . For this joint distribution,  p^x j pxj^x ijj  1 m  p cid:3 k  px i  = Xj = Xj  =  1 m  since the rows of the distortion matrix are permutations of each other and therefore each element p cid:3 k , k = 1; 2; : : : ; m occurs once in the above sum. Hence the distribution of x has the desired source distribution. For this joint distribution, we have  pxj^x ijj aij  p cid:3 kdk  Xi;j  px;^x i; j aij = Xj = Xj = Xj  = D;  1  mXi mXk  1  1 m  D  the desired distortion. The mutual information  I X; ^X  = H X   cid:0  H Xj ^X    10.60    10.61    10.62    10.63    10.64    10.65    10.66    10.67    248  Rate Distortion Theory   10.68    10.69    10.70    10.71    10.72    10.73    10.74    10.75   H Xj ^X = j   H p cid:3     cid:30  D   1 m  1 m  = H X   cid:0 Xj = H X   cid:0 Xj 1 = H X   cid:0 Xj m = H X   cid:0   cid:30  D :  R D  =  min  I X; ^X   p ^xjx :P p x;^x d x;^x  cid:20 D   cid:20  I X; ^X  = H X   cid:0   cid:30  D :  Hence using this joint distribution in the de cid:12 nition of the rate distortion function  Combining this with the Shannon lower bound on the rate distortion function, we must have equality in the above equation and hence we have equality in the Shannon lower bound.  7. Erasure distortion. Consider X  cid:24  Bernoulli  1  given by the matrix  2  , and let the distortion measure be  d x; ^x  =" 0  0  : 1 1  1 1  Calculate the rate distortion function for this source. Can you suggest a simple scheme to achieve any value of the rate distortion function for this source? Solution: Erasure distortion. Consider X  cid:24  Bernoulli  1 0  : 1 1  d x; ^x  =" 0  2  , and the distortion measure   10.76   1 1  The in cid:12 nite distortion constrains p 0; 1  = p 1; 0  = 0 . Hence by symmetry the joint distribution of  X; ^X  is of the form shown in Figure 10.1.  For this joint distribution, it is easy to calculate the distortion D =  cid:11  and that I X; ^X  = H X   cid:0  H Xj ^X  = 1  cid:0   cid:11  . Hence we have R D  = 1  cid:0  D for 0  cid:20  D  cid:20  1 . For D > 1 , R D  = 0 .  It is very see how we could achieve this rate distortion function. If D is rational, say k=n , then we send only the  cid:12 rst n cid:0  k of any block of n bits. We reproduce these bits exactly and reproduce the remaining bits as erasures. Hence we can send information at rate 1  cid:0  D and achieve a distortion D . If D is irrational, we can get arbitrarily close to D by using longer and longer block lengths.  8. Bounds on the rate distortion function for squared error distortion. For the case of a continuous random variable X with mean zero and variance  cid:27  2 and squared   Rate Distortion Theory  249  1  cid:0   cid:11   -  0  0  Q  Q  Q  Q  Q  Q  Q  Q  Q  Q  Q  cid:11   Q  Q   cid:17    cid:11   cid:17    cid:17   Q   cid:17   Qs  cid:17 3  e  -  1   cid:17    cid:17    cid:17    cid:17    cid:17    cid:17   1  cid:0   cid:11    cid:17    cid:17    cid:17    cid:17   1  Figure 10.1: Joint distribution for erasure rate distortion of a binary source  error distortion, show that  h X   cid:0   log 2 cid:25 eD   cid:20  R D   cid:20   1 2  1 2  log   cid:27 2 D  :   10.77   For the upper bound, consider the joint distribution shown in Figure 10.2. Are Gaussian random variables harder or easier to describe than other random variables with the same variance?  Solution: Bounds on the rate distortion function for squared error distortion. We assume that X has zero mean and variance  cid:27 2 . To prove the lower bound, we use the same techniques as used for the Guassian rate distortion function. Let  X; ^X  be random variables such that E X  cid:0  ^X 2  cid:20  D . Then I X; ^X  = h X   cid:0  h Xj ^X    10.78   = h X   cid:0  h X  cid:0  ^Xj ^X   cid:21  h X   cid:0  h X  cid:0  ^X   cid:21  h X   cid:0  h N  0; E X  cid:0  ^X 2   log 2 cid:25 e E X  cid:0  ^X 2 = h X   cid:0   cid:21  h X   cid:0  log 2 cid:25 e D:  1 2 1 2   10.79    10.80    10.81    10.82    10.83   To prove the upper bound, we consider the joint distribution as shown in Figure 10.3,   250  Rate Distortion Theory  ?   cid:27 2 cid:0 D cid:17  Z  cid:24  N  cid:16 0; D cid:27 2 ’$- &%   X + Z   ^X =  cid:27 2 cid:0 D  cid:27 2  X   cid:27 2 cid:0 D  cid:27 2  -   cid:18  cid:17  cid:19  cid:16 u?  -  ^X  Figure 10.2: Joint distribution for upper bound on rate distortion function.  ?   cid:27 2 cid:0 D cid:17  Z  cid:24  N  cid:16 0; D cid:27 2 ’$- &%   X + Z   ^X =  cid:27 2 cid:0 D  cid:27 2  X   cid:27 2 cid:0 D  cid:27 2  -   cid:18  cid:17  cid:19  cid:16 u?  -  ^X  Figure 10.3: Joint distribution for upper bound on rate distortion function   Rate Distortion Theory  and calculate the distortion and the mutual information between X and ^X . Since  since X and Z are independent and zero mean. Also the mutual information is  we have  Now  ^X =   cid:27 2  cid:0  D   cid:27 2   X + Z  ;   cid:27 2 Z!2 E X  cid:0  ^X 2 = E  D  cid:27 2  cid:0  D  cid:27 2 X  cid:0  EX 2 +   cid:27 2  cid:0  D  cid:27 2 !2  cid:27 2 cid:19 2 =  cid:18  D  cid:27 2 +   cid:27 2  cid:0  D  cid:27 2 !2  cid:27 2 cid:19 2 =  cid:18  D  EZ 2  D cid:27 2  cid:27 2  cid:0  D  = D;  I X; ^X  = h  ^X   cid:0  h  ^XjX   cid:27 2  cid:0  D  = h  ^X   cid:0  h    cid:27 2 Z :  E X + Z 2   EX 2 + EZ 2   E ^X 2 =    cid:27 2  cid:0  D  cid:27 2 !2  cid:27 2 !2 =    cid:27 2  cid:0  D  cid:27 2 !2  cid:27 2 + =    cid:27 2  cid:0  D =  cid:27 2  cid:0  D:  D cid:27 2   cid:27 2  cid:0  D!  Hence, we have  I X; ^X  = h  ^X   cid:0  h    cid:27 2  cid:0  D   cid:27 2 Z  = h  ^X   cid:0  h Z   cid:0  log  cid:20  h N  0;  cid:27 2  cid:0  D    cid:0    cid:27 2   cid:27 2  cid:0  D 1 2  log 2 cid:25 e   log 2 cid:25 e   cid:27 2  cid:0  D   cid:0   1 2  log 2 cid:25 e   =  =  1 2  1 2  log   cid:27 2 D  ;  D cid:27 2   cid:27 2  cid:0  D  cid:0  log  cid:27 2  cid:0  D  cid:0   D cid:27 2   cid:27 2   cid:27 2  cid:0  D  cid:27 2 !2 log   cid:27 2  cid:0  D  1 2  251   10.84    10.85    10.86    10.87    10.88    10.89    10.90    10.91    10.92    10.93    10.94    10.95    10.96    10.97    10.98    10.99    252  Rate Distortion Theory  which combined with the de cid:12 nition of the rate distortion function gives us the required upper bound.  For a Gaussian random variable, h X  = 1 2 log 2 cid:25 e  cid:27 2 and the lower bound is equal to the upper bound. For any other random variable, the lower bound is strictly less than the upper bound and hence non-Gaussian random variables cannot require more bits to describe to the same accuracy than the corresponding Gaussian random variables. This is not surprising, since the Gaussian random variable has the maximum entropy and we would expect that it would be the most di cid:14 cult to describe.  9. Properties of optimal rate distortion code. A good  R; D  rate distortion code with R  cid:25  R D  puts severe constraints on the relationship of the source X n and the representations ^X n . Examine the chain of inequalities  10.100{10.112  considering the conditions for equality and interpret as properties of a good code. For example, equality in  10.101  implies that ^X n is a deterministic function of X n .  Solution: Properties of optimal rate distortion code. The converse of the rate distortion theorem relies on the following chain of inequalities  H Xi   cid:0   H Xij ^X n; Xi cid:0 1; : : : ; X1    a   nR   b    cid:21  H  ^X n   cid:21  H  ^X n   cid:0  H  ^X njX n   c  = I  ^X n; X n  = H X n   cid:0  H X nj ^X n   d  =  n  H Xi   cid:0  H X nj ^X n    f     e  =  n  n  n  =   cid:21   Xi=1 Xi=1 Xi=1 Xi=1 Xi=1 Xi=1  cid:21  nR  1  = n   cid:21   n  n  1 n  n   g    h   n  n  Xi=1 Xi=1  H Xi   cid:0   H Xij ^Xi   I Xi; ^Xi   R Ed Xi; ^Xi    R Ed Xi; ^Xi    Ed Xi; ^Xi !  n  Xi=1   i  = nR Ed X n; ^X n   = nR D :   10.100    10.101    10.102    10.103    10.104    10.105    10.106    10.107    10.108    10.109    10.110    10.111    10.112    Rate Distortion Theory  253  We will have equality in  a  if ^X n is uniformly distributed over the set of codewords -i.e., if all the codewords were equally likely,  b  if ^X n is a deterministic function of X n ,  f  if each Xi depends only on the corresponding ^Xi and is conditionally independent of every other ^Xj ,  g  if the joint distribution of Xi and ^Xi de cid:12 nition of the rate distortion function, and  h  if either the rate distortion curve is a straight line or if all the distortions  at each i   are equal.  is the one achieving the minimum in the  Thus the optimal rate distortion code would be deterministic, and the joint distribu- tion between the source symbol and the codeword at each instant of time would be independent and equal to the joint distribution that achieves the minimum of the rate distortion function. The distortion would be the same for each time instant.  10. Rate distortion. Find and verify the rate distortion function R D  for X uniform  on X = f1; 2; : : : ; 2mg and  d x; ^x  =  1 where ^X is de cid:12 ned on ^X = f1; 2; : : : ; 2mg .  You may wish to use the Shannon lower bound in your argument.   for x  cid:0  ^x odd, for x  cid:0  ^x even,  0  Solution: Rate distortion  Since the columns of the distortion measure are alternate 0 and 1, they are all permu- tations of each other, and we can apply the Shannon lower bound on the rate distortion function. The Shannon lower bound says that  where  R D   cid:21  H X   cid:0   cid:30  D ;   cid:30  D  =  max i=1 pidi cid:20 D  H p :  p:Pm  In Problem 6, it was shown that if the input probability distribution is uniform, the bound is tight, and the Shannon lower bound is equal to the rate distortion function.  Therefore to calculate the R D  , we only need to compute  cid:30  D  for the distortion measure of the problem. Each row of the distortion matrix is a permutation  actually a cyclic shift  of the  cid:12 rst row [010101 : : : 01] . Let Y be random variable with distribution p1; : : : ; p2m , and let Z be the value of the d 0; Y   . Thus Z is 0 on the even values of Y and 1 on the odd values. Then   10.113    10.114    10.115    cid:30  D  =  max i=1 pidi cid:20 D  H p   p:Pm   254  Rate Distortion Theory  =  =  =  =  p:Pm p:Pm p:Pm p:Pm  H Y    H Y; Z   max i=1 pidi cid:20 D max i=1 pidi cid:20 D max i=1 pidi cid:20 D i=1 pidi cid:20 D  cid:0 p log p  cid:0   1  cid:0  p  log 1  cid:0  p  +  1  cid:0  p H Y jZ = 0  + pH Y jZ = 1  max  H Z  + H Y jZ    10.116    10.117    10.119    10.118    10.120   where p = Pr Z = 1  . Since Pi pidi =Pi:Z=1 pi = p , we have p  cid:20  D . Given Z = 0 , there are m possible values of Y , and the entropy is maximized by a uniform over these values. Similarly, conditioned on Z = 1 , H Y jZ = 1  is maximized by an uniform distribution on the m values of Y where Z = 1 . Thus   cid:30  D  = max p:p cid:20 D  H p  + p log m +  1  cid:0  p  log m = H D  + log m   10.121   R D  = H X   cid:0   cid:30  D  = log 2m  cid:0  log m  cid:0  H D  = 1  cid:0  H D    10.122   and hence  11. Lower bound  Let  e cid:0 x4 e cid:0 x4dx  and   cid:0 1  X  cid:24  R 1 R x4e cid:0 x4 R e cid:0 x4dx De cid:12 ne g a  = max h X  over all densities such that EX 4  cid:20  a . Let R D  be the rate distortion function for X with the above density and with distortion criterion d x; ^x  =  x  cid:0  ^x 4: Show R D   cid:21  g c   cid:0  g D  . Solution: Lower bound  = c:  dx  This is a continuous analog of the Shannon lower bound for the rate distortion function. By similar arguments  The maximum entropy distribution given the expected fourth power constraint is of the form  R D  =  I X; ^X   min  Ed X; ^X  cid:20 D  min  Ed X; ^X  cid:20 D  =  h X   cid:0  h Xj ^X   e cid:0 x4 e cid:0 x4dx  X  cid:24   R 1   cid:0 1   10.123    10.124    10.125    10.126    Rate Distortion Theory  255  and hence h X  = g c  . Now h Xj ^X  = h X  cid:0  ^Xj ^X   cid:20  h X  cid:0  ^X   cid:20  g D  from the de cid:12 nition of g a  = maxEX 4=a h X  . Therefore  10.127   R D   cid:21  g c   cid:0  g D   12. Adding a column to the distortion matrix. Let R D  be the rate distortion func- tion for an i.i.d. process with probability mass function p x  and distortion function d x; ^x  , x 2 X , ^x 2 ^X . Now suppose that we add a new reproduction symbol ^x0 to ^X with associated distortion d x; ^x0  , x 2 X . Does this increase or decrease R D  and why?  Solution: Adding a column Let the new rate distortion function be denoted as ~R D  , and note that we can still achieve R D  by restricting the support of p x; ^x  , i.e., by simply ignoring the new symbol. Thus, ~R D   cid:20  R D  . Finally note the duality to the problem in which we added a row to the channel tran- sition matrix to have no smaller capacity  Problem 7.22 .  13. Simpli cid:12 cation. Suppose X = f1; 2; 3; 4g , ^X = f1; 2; 3; 4g , p i  = 1  and X1; X2; : : : are i.i.d.  cid:24  p x  . The distortion matrix d x; ^x  is given by  4 , i = 1; 2; 3; 4 ,  1 2 3 4 1 0 0 1 1 2 0 0 1 1 3 1 1 0 0 4 1 1 0 0   a  Find R 0  , the rate necessary to describe the process with zero distortion.   b  Find the rate distortion function R D  . There are some irrelevant distinctions in  alphabets X and ^X , which allow the problem to be collapsed.   c  Suppose we have a nonuniform distribution p i  = pi , i = 1; 2; 3; 4 . What is  R D  ?  Solution: Simpli cid:12 cation   a  We can achieve 0 distortion if we output ^X = 1 if X = 1 or 2 , and ^X = 3 if X = 3 or 4 . Thus if we set Y = 1 if X = 1 or 2 , and Y = 2 if X = 3 or 4 , we can recover Y exactly if the rate is greater than H Y   = 1 bit. It is also not hard to see that any 0 distortion code would be able to recover Y exactly, and thus R 0  = 1 .   b  If we de cid:12 ne Y as in the previous part, and ^Y similarly from ^X , we can see that the distortion between X and ^X is equal to the Hamming distortion between Y and ^Y . Therefore if the rate is greater than the Hamming rate distortion function R D  for Y , we can recover X to distortion D . Thus R D  = 1  cid:0  H D  .   256  Rate Distortion Theory   c  If the distribution of X is not uniform, the same arguments hold and Y has a distribution  p1 + p2; p3 + p4  , and the rate distortion function is R D  = H p1 + p2   cid:0  H D  ,  14. Rate distortion for two independent sources. Can one simultaneously compress two independent sources better than by compressing the sources individually? The following problem addresses this question. Let fXig be iid  cid:24  p x  with distortion d x; ^x  and rate distortion function RX  D  . Similarly, let fYig be iid  cid:24  p y  with distortion d y; ^y  and rate distortion function RY  D  . Suppose we now wish to describe the process f Xi; Yi g subject to distortions Ed X; ^X   cid:20  D1 and Ed Y; ^Y    cid:20  D2 . Thus a rate RX;Y  D1; D2  is su cid:14 cient, where I X; Y ; ^X; ^Y    RX;Y  D1; D2  =  min  p ^x;^yjx;y :Ed X; ^X  cid:20 D1;Ed Y; ^Y   cid:20 D2  Now suppose the fXig process and the fYig process are independent of each other.  a  Show  RX;Y  D1; D2   cid:21  RX  D1  + RY  D2 :   b  Does equality hold?  Now answer the question.  Solution: Rate distortion for two independent sources   a  Given that X and Y are independent, we have  p x; y; ^x; ^y  = p x p y p ^x; ^yjx; y    10.128   Then  I X; Y ; ^X; ^Y   = H X; Y    cid:0  H X; Y j ^X; ^Y     10.129  = H X  + H Y    cid:0  H Xj ^X; ^Y    cid:0  H Y jX; ^X; ^Y   10.130   cid:21  H X  + H Y    cid:0  H Xj ^X   cid:0  H Y j ^Y    10.131  = I X; ^X  + I Y ; ^Y     10.132   where the inequality follows from the fact that conditioning reduces entropy. Therefore  RX;Y  D1; D2  =  min  I X; Y ; ^X; ^Y     10.133   p ^x;^yjx;y :Ed X; ^X  cid:20 D1;Ed Y; ^Y   cid:20 D2   cid:21  =  p ^x;^yjx;y :Ed X; ^X  cid:20 D1;Ed Y; ^Y   cid:20 D2 cid:16 I X; ^X  + I Y ; ^Y   cid:17  10.134   min  I X; ^X  +  min  I Y ; ^Y   10.135   min  p ^xjx :Ed X; ^X  cid:20 D1  = RX D1  + RY  D2   p ^yjy :Ed Y; ^Y   cid:20 D2   10.136    Rate Distortion Theory   b  If  then  p x; y; ^x; ^y  = p x p y p ^xjx p ^yjy ;  257   10.137   I X; Y ; ^X; ^Y   = H X; Y    cid:0  H X; Y j ^X; ^Y     10.138  = H X  + H Y    cid:0  H Xj ^X; ^Y    cid:0  H Y jX; ^X; ^Y   10.139  = H X  + H Y    cid:0  H Xj ^X   cid:0  H Y j ^Y    10.140  = I X; ^X  + I Y ; ^Y     10.141   Let p x; ^x  be a distribution that achieves the rate distortion RX  D1  at dis- tortion D1 and let p y; ^y  be a distribution that achieves the rate distortion RY  D2  at distortion D2 . Then for the product distribution p x; y; ^x; ^y  = p x; ^x p y; ^y  , where the component distributions achieve rates  D1; RX  D1   and  D2; RX  D2   , the mutual information corresponding to the product distribution is RX  D1  + RY  D2  . Thus  RX;Y  D1; D2  =  min  I X; Y ; ^X; ^Y   = RX  D1 +RY  D2   p ^x;^yjx;y :Ed X; ^X  cid:20 D1;Ed Y; ^Y   cid:20 D2   10.142   Thus by using the product distribution, we can achieve the sum of the rates. Therefore the total rate at which we encode two independent sources together with distortions D1 and D2 is the same as if we encoded each of them separately.  15. Distortion-rate function. Let  D R  =  min  Ed X; ^X   p ^xjx :I X; ^X  cid:20 R   10.143   be the distortion rate function.   a  Is D R  increasing or decreasing in R ?   b  Is D R  convex or concave in R ?   c  Converse for distortion rate functions: We now wish to prove the converse by focusing on D R  . Let X1; X2; : : : ; Xn be i.i.d.  cid:24  p x  . Suppose one is given a  2nR; n  rate distortion code X n ! i X n  ! ^X n i X n   , with i X n  2 2nR . And suppose that the resulting distortion is D = Ed X n; ^X n i X n    . We must show that D  cid:21  D R  . Give reasons for the following steps in the proof:  D = Ed X n; ^X n i X n      a  = E   b  =  1 n  n  1 n n  Xi=1 Xi=1  d Xi; ^Xi   Ed Xi; ^Xi    10.144    10.145    10.146    258  Rate Distortion Theory  n  D cid:16 I Xi; ^Xi  cid:17  I Xi; ^Xi ! Xi=1 I X n; ^X n  cid:19   n   d    c   n  1 n  Xi=1  cid:21   cid:21  D  1  cid:21  D cid:18  1  cid:21  D R   n   e    f     10.147    10.148    10.149    10.150   Solution: Distortion rate function.   a  Since for larger values of R , the minimization in  D R  =  min  Ed X; ^X    10.151   p ^xjx :I X; ^X  cid:20 R  is over a larger set of possible distributions, the minimum has to be at least as small as the minimum over the smaller set. Thus D R  is a nonincreasing function of R .   b  By similar arguments as in Lemma 10.4.1, we can show that D R  is a convex function of R . Consider two rate distortion pairs  R1; D1  and  R2; D2  which lie on the distortion-rate curve. Let the joint distributions that achieve these pairs be p1 x; ^x  = p x p1 ^xjx  and p2 x; ^x  = p x p2 ^xjx  . Consider the distribution p cid:21  =  cid:21 p1 +  1  cid:0   cid:21  p2 . Since the distortion is a linear function of the distribution, we have D p cid:21   =  cid:21 D1 +  1  cid:0   cid:21  D2 . Mutual information, on the other hand, is a convex function of the conditional distribution  Theorem 2.7.4  and hence  Ip cid:21  X; ^X   cid:20   cid:21 Ip1 X; ^X  +  1  cid:0   cid:21  Ip2 X; ^X  =  cid:21 R1 +  1  cid:0   cid:21  R2   10.152   Therefore we can achieve a distortion  cid:21 D1 +  1  cid:0   cid:21  D2 with a rate less than  cid:21 R1 +  1  cid:0   cid:21  R2 and hence  D R cid:21    cid:20  Dp cid:21  X; ^X   =  cid:21 D R1  +  1  cid:0   cid:21  D R2 ;  which proves that D R  is a convex function of R .   c    10.153    10.154    10.155    10.156    10.157   D = Ed X n; ^X n i X n      a  = E   b  =  1 n  n  1 n n  Xi=1 Xi=1  d Xi; ^Xi   Ed Xi; ^Xi    259   10.158    10.159    10.160    10.161    10.162    10.163    10.165    10.166   Rate Distortion Theory  n  D cid:16 I Xi; ^Xi  cid:17  I Xi; ^Xi ! Xi=1 I X n; ^X n  cid:19   n   d    c   n  1 n  Xi=1  cid:21   cid:21  D  1  cid:21  D cid:18  1  cid:21  D R   n   e    f     a  follows from the de cid:12 nition of distortion for sequences  b  from exchanging summation and expectation  c  from the de cid:12 nition of the distortion rate function based on the joint distribution p xi; ^xi  ,  d  from Jensen’s inequality and the convexity of D R   e  from the fact that  I X n; ^X n  = H X n   cid:0  H X nj ^X n   H Xi   cid:0  H X nj ^X n   =  =   cid:21   =  n  n  n  Xi=1 Xi=1 Xi=1 Xi=1  n  n  n  Xi=1 Xi=1  H Xi   cid:0   H Xij ^Xi   I Xi; ^Xi   H Xi   cid:0   H Xij ^X n; Xi cid:0 1; : : : ; X1    10.164   and  f  follows from the de cid:12 nition of the distortion rate function.  16. Probability of conditionally typical sequences. In Chapter 7, we calculated the probability that two independently drawn sequences X n and Y n are weakly jointly typical. To prove the rate distortion theorem, however, we need to calculate this prob- ability when one of the sequences is  cid:12 xed and the other is random.  The techniques of weak typicality allow us only to calculate the average set size of the conditionally typical set. Using the ideas of strong typicality on the other hand provides us with stronger bounds which work for all typical xn sequences. We will outline the proof that Prf xn; Y n  2 A cid:3  n  g  cid:25  2 cid:0 nI X;Y   for all typical xn . This approach was introduced by Berger[1] and is fully developed in the book by Csisz cid:19 ar and K cid:127 orner[3]. Let  Xi; Yi  be drawn i.i.d.  cid:24  p x; y  . Let the marginals of X and Y be p x  and p y  respectively.   cid:15    a  Let A cid:3  n    cid:15   be the strongly typical set for X . Show that  jA cid:3  n    cid:15   j :=2nH X    10.167    260  Rate Distortion Theory  Hint: Theorem 11.1.1 and 11.1.3.   b  The joint type of a pair of sequences  xn; yn  is the proportion of times  xi; yi  =   a; b  in the pair of sequences, i.e.,  pxn;yn a; b  =  1 n  N  a; bjxn; yn  =  1 n  n  Xi=1  I xi = a; yi = b :   10.168   The conditional type of a sequence yn given xn is a stochastic matrix that gives the proportion of times a particular element of Y occurred with each element of X in the pair of sequences. Speci cid:12 cally, the conditional type Vynjxn bja  is de cid:12 ned as  Vynjxn bja  =  N  a; bjxn; yn   :  N  ajxn    10.169   Show that the number of conditional types is bounded by  n + 1 jXjjYj .   c  The set of sequences yn 2 Y n with conditional type V with respect to a sequence  xn is called the conditional type class TV  xn  . Show that  1   n + 1 jXjjYj  2nH Y jX   cid:20  jTV  xn j  cid:20  2nH Y jX :   10.170    d  The sequence yn 2 Y n is said to be  cid:15  -strongly conditionally typical with the se- quence xn with respect to the conditional distribution V   cid:1 j cid:1   if the conditional type is close to V . The conditional type should satisfy the following two condi- tions: i. For all  a; b  2 X  cid:2  Y with V  bja  > 0 ,  1 n jN  a; bjxn; yn   cid:0  V  bja N  ajxn j  cid:20    cid:15   :  jYj + 1   10.171    cid:15   ii. N  a; bjxn; yn  = 0 for all  a; b  such that V  bja  = 0 . The set of such sequences is called the conditionally typical set and is denoted A cid:3  n   Y jxn  . Show that the number of sequences yn that are conditionally typical with a given xn 2 X n is bounded by 2n H Y jX  cid:0  cid:15 1   cid:20  jA cid:3  n    Y jxn j  cid:20   n + 1 jXjjYj2n H Y jX + cid:15 1 ;  10.172   1   cid:15    n + 1 jXjjYj where  cid:15 1 ! 0 as  cid:15  ! 0 .  strongly typical set A cid:3  n    cid:15   i.   e  For a pair of random variables  X; Y   with joint distribution p x; y  , the  cid:15  -  is the set of sequences  xn; yn  2 X n  cid:2  Y n satisfying 1 n  <   cid:15   N  a; bjxn; yn   cid:0  p a; b  cid:12  cid:12  cid:12  cid:12   jXjjYj   cid:12  cid:12  cid:12  cid:12    10.173   for every pair  a; b  2 X  cid:2  Y with p a; b  > 0 .   Rate Distortion Theory  261  ii. N  a; bjxn; yn  = 0 for all  a; b  2 X  cid:2  Y with p a; b  = 0 . The set of  cid:15  -strongly jointly typical sequences is called the  cid:15  -strongly jointly typical set and is denoted A cid:3  n  Let  X; Y   be drawn i.i.d.  cid:24  p x; y  . For any xn such that there exists at least  X; Y   , the set of sequences yn such that  xn; yn  2 A cid:3  n  one pair  xn; yn  2 A cid:3  n    X; Y   .   cid:15    cid:15    cid:15   satis cid:12 es  1   n + 1 jXjjYj  2n H Y jX  cid:0  cid:14   cid:15     cid:20  jfyn :  xn; yn  2 A cid:3  n    cid:15   gj  cid:20   n + 1 jXjjYj2n H Y jX + cid:14   cid:15   ;  where  cid:14   cid:15   ! 0 as  cid:15  ! 0 . In particular, we can write  2n H Y jX  cid:0  cid:15 2   cid:20  jfyn :  xn; yn  2 A cid:3  n    cid:15   gj  cid:20  2n H Y jX + cid:15 2 ;   10.174    10.175   where we can make  cid:15 2 arbitrarily small with an appropriate choice of  cid:15  and n .   f  Let Y1; Y2; : : : ; Yn be drawn i.i.d.  cid:24 Q p yi  . For xn 2 A cid:3  n   is bounded by   xn; Y n  2 A cid:3  n    cid:15    cid:15   , the probability that  2 cid:0 n I X;Y  + cid:15 3   cid:20  Pr  xn; Y n  2 A cid:3  n    cid:15      cid:20  2 cid:0 n I X;Y   cid:0  cid:15 3 ;   10.176   where  cid:15 3 goes to 0 as  cid:15  ! 0 and n ! 1 .  Solution:  Probability of conditionally typical sequences.   a  The set of strongly typical sequences is the set of sequence whose type is close the distribution p . We have two conditions - that the proportion of any symbol a in the sequence is close to p a  and that no symbol with p a  = 0 occurs in the sequence. The second condition may seem a technical one, but is essential in the proof of the strong equipartition theorem below. By the strong law of large numbers, for a sequence drawn i.i.d.  cid:24  p x  , the asymptotic proportion of any letter a is close to p a  with high probability. So for appropriately large n , the proportion of every letter is within  cid:15  of p a  with probability close to 1, i.e., the strongly typical set has a probability close to 1. We will show that  j  cid:20  2n H p + cid:15 0 ;   10.177   2n H p  cid:0  cid:15 0   cid:20  jA cid:3  n  where  cid:15 0 goes to 0 as  cid:15  ! 0 and n ! 1 . For sequences in the strongly typical set,   cid:15    cid:0 H p   cid:0   1 n  log p xn  = Xa2X  p a  log p a   cid:0   N  ajxn  log p a   1  n Xa2X  =  cid:0 Xa2X cid:18  1  n  N  ajxn   cid:0  p a  cid:19  log p a ;   10.178    262  Rate Distortion Theory  and since j 1 p a  = 0 , we have  n N  ajxn   cid:0  p a j   0 , and j 1  n N  ajxn   cid:0  p a j = 0 if   10.179   1 n  log p xn j <  cid:15 1:  j  cid:0  H p   cid:0  p a  . It follows that  cid:15 1 ! 0 as  cid:15  ! 0 .  where  cid:15 1 =  cid:15 Pa:p a >0 log 1 Recall the de cid:12 nition of weakly typical sequences in Chapter 3. A sequence was de cid:12 ned as  cid:15 1 -weakly typical if j  cid:0  log p xn   cid:0  H p j  cid:20   cid:15 1 . Hence a sequence that is  cid:15  -strongly typical is also  cid:15 1 -weakly typical. Hence the strongly typical set is a subset of the corresponding weakly typical set, i.e., A cid:3  n  Similarly, by the continuity of the entropy function, it follows that for all types in the typical set, the entropy of the type is close to H p  . Speci cid:12 cally, for all   cid:15   cid:26  A n   cid:15 1 .  xn 2 A cid:3  n    cid:15   , jpxn a   cid:0  p a j <  cid:15  and hence by Lemma 10.0.5, we have  jH pxn   cid:0  H p j <  cid:15 2;   10.180   where  cid:15 2 =  cid:0 jXj cid:15  log  cid:15  ! 0 as  cid:15  ! 0 . There are only a polynomial number of types altogether and hence there are only a polynomial number of types in the strongly typical set. The type class of any  type q 2 A cid:3  n    cid:15   , by Theorem 12.1.3, has a size bounded by  1   n + 1 jXj  2nH q   cid:20  jT  q j  cid:20  2nH q :  By the previous part of this theorem, for q 2 A cid:3  n    cid:15   ,  jH q   cid:0  H p j  cid:20   cid:15 2 , and  hence  1   n + 1 jXj  2n H p  cid:0  cid:15 2   cid:20  jT  q j  cid:20  2n H p + cid:15 2 :  Since the number of elements in the strongly typical set is the sum of the sizes of the type classes in the strongly typical set, and there are only a polynomial number of them, we have  1   n + 1 jXj  2n H p  cid:0  cid:15 2   cid:20  jA cid:3  n    cid:15   j  cid:20   n + 1 jXj2n H p + cid:15 2 ;   10.183    cid:15   n log jA cid:3  n   j  cid:0  H p j  cid:20   cid:15 0 , where  cid:15 0 =  cid:15 2 + jXjn log n + 1  which goes to 0 as  i.e., j 1  cid:15  ! 0 and n ! 1 . It is instructive to compare the proofs of the strong AEP with the AEP for weakly typical sequences. The results are similar, but there is one important di cid:11 erence. The lower bound on size of the strongly typical set does not depend on the prob- ability of the setinstead, the bound is derived directly in terms of the size of type classes. This enables the lower bound in the strong AEP to be extended to conditionally typical sequences and sets; the weak AEP cannot be extended similarly. We will consider the extensions of the AEP to conditional distributions in the next part.   10.181    10.182    Rate Distortion Theory  263   b  The concept of types for single sequences can be extended to pairs of sequences  for which we can de cid:12 ne the concept of the joint type and the conditional type. De cid:12 nition: The joint type of a pair of sequences  xn; yn  is the proportion of times a pair of symbols  a; b  occurs jointly the the pair of sequences, i.e.,  pxn;yn a; b  =  1 n  N  a; bjxn; yn :   10.184   De cid:12 nition: The conditional type of a sequence yn given xn is a stochastic matrix that gives the proportion of times a particular element of Y occurred with each element of X in the pair of sequences. Speci cid:12 cally, the conditional type Vynjxn bja  is de cid:12 ned as  Vynjxn bja  =  N  a; bjxn; yn   :  N  ajxn    10.185   The set of sequences yn 2 Y n with conditional type V with respect to a sequence xn is called the conditional type class TV  xn  .  Lemma 10.0.2 The number of conditional types for sequences of length n from the alphabet X and Y is bounded by  n + 1 jXjjYj .  Proof: By Theorem 12.1.1, the number of ways of choosing a row of the matrix V   cid:1 ja  is bounded by  n+1 jYj and there are jXj di cid:11 erent choices of rows. So the total number of di cid:11 erent conditional types is bounded by  n + 1 jXjjYj . 2   c  Since Vynjxn is a stochastic matrix, we can multiply it with pxn to  cid:12 nd the joint type of  xn; yn  . We will denote the conditional entropy of Y given X for this joint distribution as H Vynjxnjpxn  . Lemma 10.0.3 For xn 2 X n , let TV  xn  denote the set of sequences yn 2 Y n with conditional type V with respect to xn . Then  1   n + 1 jXjjYj  2nH V jpxn    cid:20  jTV  xn j  cid:20  2nH V jpxn  :   10.186   Proof: This is a direct consequence of the corresponding lemma about the size of unconditional type classes. We can consider the subsequences of the pair corresponding each element of X . For any particular element a 2 X , the number of conditionally typical sequences depends only the conditional type V   cid:1 ja  , and hence the number of conditionally typical sequences is bounded by Ya2X  2N  ajxn H V jpxn    cid:20  jTV  xn j  cid:20  Ya2X   N  ajxn  + 1 jYj  2N  ajxn H V jpxn    1   10.187   which proves the lemma. 2  The above two lemmas generalize the corresponding lemmas for unconditional types. We can use these to extend the strong AEP to conditionally typical sets.   264  Rate Distortion Theory   d  We begin with the de cid:12 nition of strongly conditionally typical sequences.  De cid:12 nition: The sequence yn 2 Y n is said to be  cid:15  -strongly conditionally typical with the sequence xn with respect to the conditional distribution V   cid:1 j cid:1   if the conditional type is close to V . The conditional type should satisfy the following two conditions: i. For all  a; b  2 X  cid:2  Y with V  bja  > 0 ,  1 n jN  a; bjxn; yn   cid:0  V  bja N  ajxn j  cid:20   cid:15 :   10.188    cid:15    Y jxn  .  ii. N  a; bjxn; yn  = 0 for all  a; b  such that V  bja  = 0 . The set of such sequences is called the conditionally typical set and is denoted A cid:3  n  Essentially, a sequence yn is conditionally typical with xn if the subsequence of yn corresponding to the occurrences of a particular symbol a in xn is typical with respect to the conditional distribution V   cid:1 ja  . Since the number of such conditionally typical sequences is just the product of the number of subsequences conditionally typically corresponding to each choice of a 2 X , we can now extend the strong AEP to derive a bound on the size of the conditionally typical set.  Lemma 10.0.4 The number of sequences yn that are conditionally typical with a given xn 2 X n is bounded by  1   n + 1 jXjjYj  2n H V jpxn   cid:0  cid:15 4   cid:20  jA cid:3  n    cid:15    Y jxn j  cid:20   n + 1 jXjjYj2n H V jpxn  + cid:15 4 ;   10.189   where  cid:15 4 =  cid:0 jXjjYj cid:15  log  cid:15  ! 0 as  cid:15  ! 0 .  Proof: Just as in the proof of the strong AEP  Theorem 12.2.1 , we will derive the bounds using purely combinatorial arguments. The size of the conditional type class is bounded in Lemma 10.0.3 in terms of the entropy of the conditional type. By Lemma 10.0.5 and the de cid:12 nition of the conditionally typical set, we have   cid:12  cid:12  cid:12 H pynjxnjpxn   cid:0  H V jpxn  cid:12  cid:12  cid:12   cid:20   cid:0 jXjjYj cid:15  log  cid:15    10.190   Combining this with the bound on the number of conditional types  Lemma 10.0.2 , we have the theorem. 2   e  We now extend the de cid:12 nition of strongly typical sequences to pairs of sequences. The joint type of a pair of sequences is the proportion of occurrences of a pair of symbols together in the pair. A pair of sequences  xn; yn  is called jointly strongly typical with respect to a distribution p x; y  if the joint type is close to p x; y  .  De cid:12 nition: For a pair of random variables  X; Y   with joint distribution p x; y  , the  cid:15  -strongly typical set A cid:3  n  is the set of sequences  xn; yn  2 X n cid:2 Y n satisfying   cid:15    Rate Distortion Theory  i.   cid:12  cid:12  cid:12  cid:12   1 n  N  a; bjxn; yn   cid:0  p a; b  cid:12  cid:12  cid:12  cid:12   <  cid:15   for every pair  a; b  2 X  cid:2  Y with p a; b  > 0 .  ii. N  a; bjxn; yn  = 0 for all  a; b  2 X  cid:2  Y with p a; b  = 0 . The set of  cid:15  -strongly jointly typical sequences is called the  cid:15  -strongly jointly typical set and is denoted A cid:3  n    X; Y   .   cid:15   Theorem 10.0.1  Joint AEP.  Let  X n; Y n  be sequences of length n drawn  i.i.d. according to p xn; yn  =Qn  i=1 p xi; yi  . Then  P  A cid:3  n    cid:15     ! 1;  as n ! 1:  Proof: Follows directly from the weak law of large numbers. 2  From the de cid:12 nition, it is clear that strongly jointly typical sequences are also  individually typical, i.e., for xn such that  xn; yn  2 A cid:3  n    cid:15    X; Y   ,  jpxn a   cid:0  p a j  cid:20  Xb2Y  cid:20   cid:15 jYj;  jpxn;yn a; b   cid:0  p a; b j  for all a 2 X :  265   10.191    10.192    10.193    10.194   . This in turn implies that the pair is also conditionally typical  Hence xn 2 A cid:3  n   cid:15 jYj for the conditional distribution p yjx  , i.e., for  xn; yn  2 A cid:3  n  jpxn;yn a; b   cid:0  p bja pxn a j <  cid:15  jYj + 1  <  cid:15 jXjjYj:   cid:15    X; Y   ,   10.195   Since conditional entropy is also a continuous function of the distribution, the conditional entropy of the type of a jointly strongly typical sequence, pxn;yn , is close to conditional entropy for p x; y  . Hence we can also extend Lemma 10.0.3 for elements of the typical set as follows:  Theorem 10.0.2  Size of conditionally typical set  Let  X; Y   be drawn i.i.d.  cid:24  p x; y  . For any xn such that there exists at least  X; Y   , the set of sequences yn such that  xn; yn  2 A cid:3  n  one pair  xn; yn  2 A cid:3  n    cid:15    cid:15   satis cid:12 es  1   n + 1 jXjjYj  2n H Y jX  cid:0  cid:14   cid:15     cid:20  jfyn :  xn; yn  2 A cid:3  n    cid:15   gj  cid:20   n + 1 jXjjYj2n H Y jX + cid:14   cid:15   ;  where  cid:14   cid:15   ! 0 as  cid:15  ! 0 . In particular, we can write  2n H Y jX  cid:0  cid:15 5   cid:20  jfyn :  xn; yn  2 A cid:3  n    cid:15   gj  cid:20  2n H Y jX + cid:15 5 ;  where we can make  cid:15 5 arbitrarily small with an appropriate choice of  cid:15  and n .   10.196    10.197    266  Rate Distortion Theory  Proof: The theorem follows from Theorem 10.0.2 and the continuity of conditional entropy as a function of the joint distribution. Now the set of sequences that are jointly typical with a given xn are also  cid:15 jXjjYj - strongly conditionally typical, and hence from the upper bound of Theo- rem 10.0.2, we have  jfyn :  xn; yn  2 A cid:3  n    cid:15   gj  cid:20   n + 1 jXjjYj2n H p bja jpxn  + cid:14   cid:15   ;   10.198   where  cid:14   cid:15   ! 0 as  cid:15  ! 0 . Now since  H Y jX  =  cid:0 Xx  p x X p yjx  log p yjx   is a linear function of the distribution p x  , we have   10.199   jH p bja jpxn   cid:0  H Y jX j  cid:20   cid:15 jYj max a2X  H Y jX = a   cid:20   cid:15 jYj log jYj;   10.200   which gives us the upper bound of the theorem.   cid:15   For the lower bound, assume that  xn; yn  2 A cid:3  n   X; Y   . Then since the joint type of a pair of sequences is determined by the type of xn and the conditional type of yn given xn , all sequences yn with this conditional type will also be in A cid:3  n   X; Y   . Hence the number of sequences jfyn :  xn; yn  2 A cid:3  n  gj is at least as much as the number of sequences of this conditional type, which by the lower bound of Lemma 10.0.4, and the continuity of conditional entropy as a function of the joint distribution  Lemma 10.0.5 and  10.200  , we have   cid:15    cid:15   jfyn :  xn; yn  2 A cid:3  n    cid:15   gj  cid:21   n + 1 jXjjYj2n H p bja jpxn   cid:0  cid:14   cid:15   ;   10.201   where  cid:14   cid:15   ! 0 as  cid:15  ! 0 . This gives us the theorem with  log n + 1  +  cid:15 jYj log jYj  cid:0   cid:15 jXj2jYj2 log  cid:15 jXjjYj:   10.202    cid:15 5 = jXjjYj  n  2   cid:15   To use this result, we have to assume that there is at least one y n such that  xn; yn  2 A cid:3  n   X; Y   . From the de cid:12 nitions of the strongly typical sets, it is clear that if jpxn a  cid:0  p a j <  cid:15  , there exists at least one conditional distribution ^p bja  such that j^p bja pxn  a   cid:0  p a; b j <  cid:15  and hence for large enough n , we have at least one conditional type such that jpxn;yn a; b   cid:0  p a; b j  cid:20   cid:15  and hence if xn is  cid:15  -strongly typical, then there exists a conditional type such the joint type is jointly typical. For such an xn sequence, we can always  cid:12 nd a yn such that  xn; yn  is jointly typical.   f  Notice that for the results of Theorems 10.0.2, we have used purely combinatorial arguments to bound the size of the conditionally type class and the conditionally typical set. These theorems illustrate the power of the method of types. We will now use the last theorem to bound the probability that a randomly chosen Y n will be conditionally typical with a given xn 2 A cid:3  n   .   cid:15    Rate Distortion Theory  267  Theorem 10.0.3 Let Y1; Y2; : : : ; Yn be drawn i.i.d.  cid:24  Q p y  . For xn 2 A cid:3  n  the probability that  xn; Y n  2 A cid:3  n   is bounded by   cid:15  3   cid:15   ,  2 cid:0 n I X;Y  + cid:15 7   cid:20  Pr  xn; Y n  2 A cid:3  n    cid:15      cid:20  2 cid:0 n I X;Y   cid:0  cid:15 7    10.203   where  cid:15 7 =  cid:15 5  cid:0   cid:15 jXjjYj log  cid:15 jXj which goes to 0 as  cid:15  ! 0 and n ! 1 .  Proof: If Y n 2 A cid:3  n    cid:15   , then p Y n  :=2 cid:0 n H Y    , and hence  P   xn; Y n  2 A cid:3  n    cid:15     =  where  cid:15 6 =  cid:0  cid:15 jXjjYj log  cid:15 jXj since jpyn  cid:0  pj  cid:20   cid:15 jXj if  xn; yn  2 A cid:3  n    cid:15   .  Also  P   xn; Y n  2 A cid:3  n    cid:15     =  Xyn: xn;yn 2A cid:3  n  Xyn: xn;yn 2A cid:3  n    cid:15    cid:15    cid:20   p yn    10.204   2 cid:0 n H Y   cid:0  cid:15 6    10.205    cid:15   = jA cid:3  n   Y jxn j2 cid:0 n H Y   cid:0  cid:15 6   cid:20  2n H Y jX + cid:15 5 2 cid:0 n H Y   cid:0  cid:15 6  = 2 cid:0 n I X;Y   cid:0  cid:15 7 :   10.206    10.207    10.208   Xyn: xn;yn 2A cid:3  n  Xyn: xn;yn 2A cid:3  n    cid:15    cid:15    cid:21   p yn    10.209   2 cid:0 n H Y  + cid:15 6    10.210    cid:15   = jA cid:3  n   Y jxn j2 cid:0 n H Y  + cid:15 6   cid:21  2n H Y jX  cid:0  cid:15 5 2 cid:0 n H Y  + cid:15 6  = 2 cid:0 n I X;Y  + cid:15 7 :   10.211    10.212    10.213   Hence  2  2 cid:0 n I X;Y  + cid:15 7   cid:20  P r  xn; Y n  2 A cid:3  n    cid:15      cid:20  2 cid:0 n I X;Y   cid:0  cid:15 7 :   10.214   The main result of this problem is the last theorem, which gives upper and lower bounds on the probability that a randomly chosen sequence y n will be jointly typical with a given xn . This was used in the proof of the rate distortion theorem. To end this solution, we will prove a theorem on the continuity of entropy:  Lemma 10.0.5 If jp x   cid:0  q x j  cid:20   cid:15  for all x , then jH p   cid:0  H q j  cid:20   cid:0  cid:15 jXj log  cid:15  .  Proof: We will use some simple properties of the function  f  x  =  cid:0 x ln x  for 0  cid:20  x  cid:20   1 e  :   10.215    268  Rate Distortion Theory  Since f0 x  =  cid:0 1  cid:0  ln x > 0 and f 00 x  =  cid:0  1 concave function. Now consider  x , f  x  is an increasing  g x  = f  x +  cid:15    cid:0  f  x  = x ln x  cid:0   x +  cid:15   ln x +  cid:15  :   10.216   Then again by di cid:11 erentiation, it is clear that g0 x  < 0 so the function is strictly decreasing. Hence g x  < g 0  =  cid:0  cid:15  ln  cid:15  for all x . For any a 2 X , assume p a  > q a  , and hence we have  p a   cid:0  q a   cid:20   cid:15    10.217   Hence by the fact that f is an increasing function, we have  j  cid:0  p a  ln p a  + q a  ln q a j =  cid:0 p a  ln p a  + q a  ln q a    10.218   cid:20   cid:0  q a  +  cid:15   ln q a  +  cid:15   + q a  ln q a   cid:20   cid:0  cid:15  ln  cid:15 :  10.219   Summing this over all a 2 X , we have the lemma. 2  17. The source-channel separation theorem with distortion: Let V1; V2; : : : ; Vn be a  cid:12 nite alphabet i.i.d. source which is encoded as a sequence of n input symbols X n of a discrete memoryless channel. The output of the channel Y n is mapped onto the i=1 Ed Vi; ^Vi  be reconstruction alphabet ^V n = g Y n  . Let D = Ed V n; ^V n  = 1 the average distortion achieved by this combined source and channel coding scheme.  nPn  V n - X n V n  - Channel Capacity C  - Y n  - ^V n   a  Show that if C > R D  , where R D  is the rate distortion function for V , then it is possible to  cid:12 nd encoders and decoders that achieve a average distortion arbitrarily close to D .   b   Converse.  Show that if the average distortion is equal to D , then the capacity  of the channel C must be greater than R D  .  Solution: Source channel separation theorem with distortion   a  To show achievability, we consider two codes at rate R , where C > R > R D  . The  cid:12 rst code is a rate distortion code that achieves distortion D at rate R . The second code is a channel code that allows transmission over the channel at rate R with probability of error going to 0. Using the rate distortion code to encode the source into one of the 2nR messages, and the channel code to send this message over the channel. Since the probability of error is exponentially small, the received   Rate Distortion Theory  269  message is the same as the transmitted message with probability close to 1. In that case, the results of the achieveability of rate distortion show that the decoded sequence is within distortion D of the input sequence with high probability.  To complete the analysis, we need to consider the case wther the channel code produces an errorhowever, even in this case, the distortion produced by the error is bounded, and hence the total distortion is essentially the same as achieved without the errors.   b  To prove the converse, we need to prove that for any encoding system that achieves distortion D , the capacity of the channel should be greater than R D  . Mimicking the steps for the converse for the rate distortion function, we can de cid:12 ne coding function fn and decoding function gn , Let ^V n = ^V n Y n  = gn Y n  be the reproduced sequence corresponding to V n . Assume that Ed V n; ^V n   cid:21  D for this code. Then we have the following chain of inequalities:  I V n; ^V n  = H V n   cid:0  H V nj ^V n   n  H Vij ^V n; Vi cid:0 1; : : : ; V1    a   n  n  n  n  n  =  =  =   cid:21   I Vi; ^Vi   H Vi   cid:0   H Vi   cid:0   H Vij ^Vi   Xi=1 Xi=1  H Vi   cid:0  H V nj ^V n   Xi=1 Xi=1 Xi=1 Xi=1 Xi=1  cid:21  R Ed Vi; ^Vi  ! = n  1 Xi=1  cid:21  nR  1 Ed Vi; ^Vi ! Xi=1  R Ed Vi; ^Vi    n  n  n  n  n   b   = nR Ed V n; ^V n   = nR D ;   10.220    10.221    10.222    10.223    10.224    10.225    10.226    10.227    10.228    10.229   where  a  follows from the fact that conditioning reduces entropy,  b  from the convexity of the rate distortion function. Also by the data processing inequality,  I V n; ^V n   cid:21  I X n; Y n   cid:20  nC   10.230   where the last inequality follows from Lemma 7.9.2.   270  18. Rate distortion.  Rate Distortion Theory  Let d x; ^x  be a distortion function. We have a source X  cid:24  p x : Let R D  be the associated rate distortion function.   a  Find ~R D  in terms of R D ; where ~R D  is the rate distortion function associ- ated with the distortion ~d x; ^x  = d x; ^x  + a for some constant a > 0:  They are not equal    b  Now suppose that d x; ^x   cid:21  0 for all x; ^x and de cid:12 ne a new distortion function d cid:3  x; ^x  = bd x; ^x ; where b is some number  cid:21  0: Find the associated rate dis- tortion function R cid:3  D  in terms of R D :   c  Let X  cid:24  N  0;  cid:27 2  and d x; ^x  = 5 x  cid:0  ^x 2 + 3: What is R D ? Solution: Rate distortion.   a    b  If b > 0  ~R D  =  inf  I X; ^X   p ^xjx : E  ~d x;^x   cid:20 D  p ^xjx : E d x;^x  +a cid:20 D  inf  inf  =  =  p ^xjx : E d x;^x   cid:20 D cid:0 a  = R D  cid:0  a   I X; ^X   I X; ^X   R cid:3  D  =  inf  inf  inf  p ^xjx : E d cid:3  x;^x   cid:20 D  p ^xjx : E bd x;^x   cid:20 D  p ^xjx : E d x;^x   cid:20  D  b  I X; ^X   I X; ^X   I X; ^X   =  =  = R cid:18  D b  cid:19  ;   c  Let Rse D  be the rate distortion function associate with the distortion dse x; ^x  =  else if b = 0 then d cid:3  = 0 and R cid:3  D  = 0:   x  cid:0  ^x 2: Then from parts  a  and  b  we have R D  = Rse cid:18  D  cid:0  3 5  cid:19  :   Rate Distortion Theory  We know that  Therefore we have  Rse D  =  1  2 log  cid:27 2  D 0  cid:20  D  cid:20   cid:27 2 D <  cid:27 2  0  R D  =  1  2 log 5 cid:27 2 D cid:0 3  0  3  cid:20  D  cid:20  5 cid:27 2 + 3 D > 5 cid:27 2 + 3  271  19. Rate distortion with two constraints  Let Xi be iid  cid:24  p x  . We are given two distortion functions d1 x; ^x  and d2 x; ^x  . We wish to describe X n at rate R and reconstruct it with distortions Ed1 X n; ^X n 1    cid:20  D1 , and Ed2 X n; ^X n  2    cid:20  D2 , as shown here:  Here i  cid:1   takes on 2nR values. What is the rate distortion function R D1; D2  ? Solution: Rate distortion with two constraints  1  i ; ^X n  2  i    X n  cid:0 ! i X n   cid:0 !   ^X n 1 ; ^X n 1   1 ; ^X n 2  :  D1 = ED X n D2 = ED X n  R D1; D2  =  min  I X; ^X1; ^X2   p ^x1;^x2jx  subject to: Ed1  ^X1; X   cid:20  D1 Ed2  ^X2; X   cid:20  D2  Some interesting things to note about R D1; D2  are the following. First, max R D1 ; R D2    cid:20  R D1; D2   cid:20  R D1 +R D2  . The upper bound occurs when the mutual information is minimized with ^X1 independent of ^X2 which is always allowed. The lower bound occurs because the best rate achieved in the more constrained problem can not be lower than the best rate acheived in either less constrained problem. Note that the optimization is over the set of distributions of the form p ^x1; ^x2jx  which is a larger set than if conditional independence p ^x1jx p ^x2jx  were required, and the minimum rate achieving distribution may not have conditional independence. As a simple example of where the optimal solution is conditionally dependent consider a Gaussian source where both distortion measures are square error and the distortion bounds are the same as well. In this case the minimum rate is achieved when ^x1 = ^x2 almost surely which gives R D1; D2  = R D1  = R D2  . So for I X; ^X1; ^X2  = I X; ^X1  + I X; ^X2j ^X1  the second term is zero and the  cid:12 rst term is minimized which is not possible of conditional independence is required.   272  20. Rate distortion  Rate Distortion Theory  Consider the standard rate distortion problem, Xi i.i.d.  cid:24  p x ; X n ! i X n  ! ^X n; ji  cid:1  j = 2nR: Consider two distortion criteria d1 x; ^x  and d2 x; ^x  . Suppose d1 x; ^x   cid:20  d2 x; ^x  for all x 2 X ; ^x 2 ^X : Let R1 D  and R2 D  be the corresponding rate distortion functions.   a  Find the inequality relationship between R1 D  and R2 D  .   b  Suppose we must describe the source fXig at the minimum rate R achieving  d1 X n; ^X n  1    cid:20  D and d2 X n; ^X n  2    cid:20  D: Thus ^X n  X n ! i X n  !8< :  1  i X n   2  i X n    ^X n  and ji  cid:1  j = 2nR: Find the minimum rate R .  Solution: Rate distortion  a  Any rate  and coding scheme  satisfying d2 X n; ^X n   cid:20  D automatically satisfy  d1 X n; ^X n   cid:20  D . Hence   b  As in Problem 10.19,  R1 D   cid:20  R2 D :  R2 D  = max R1 D ; R2 D    cid:20  R D; D ;  where R D; D  is the minimum rate distortion function achieving both distortion criteria. For the other direction of inequality, repeat the argument we used in part  a : If we use the rate R2 D  and the optimal coding scheme with ^X1 = ^X2 = ^X , we satisfy both distortion constraints since d1 X n; ^X n   cid:20  d2 X n; ^X n   cid:20  D: This implies R2 D  is achievable so that R2 D   cid:21  R D; D :   Chapter 11  Information Theory and Statistics  1. Cherno cid:11 -Stein lemma. Consider the two hypothesis test  H1 : f = f1 vs. H2 : f = f2  Find D f1 k f2  if  a  fi x  = N  0;  cid:27 2  b  fi x  =  cid:21 ie cid:0  cid:21 ix; x  cid:21  0; i = 1; 2  c  f1 x  is the uniform density over the interval [0,1] and f2 x  is the uniform density  i  ; i = 1; 2  over [a; a + 1] . Assume 0 < a < 1:   d  f1 corresponds to a fair coin and f2 corresponds to a two-headed coin.  Solution: Stein’s lemma.   a  f1 = N  0;  cid:27 2  1  , f2 = N  0;  cid:27 2 2  , D f1jjf2  = Z 1 2"ln   cid:0 1 1  =  f1 x " 1  ln   cid:27 2 2  cid:27 2  1  cid:0   x2 1  cid:0   2 cid:27 2  x2 2 cid:27 2  2! dx  2  cid:27 2 1  cid:27 2  2  cid:0  1 :   cid:27 2 2  cid:27 2 1  +   b  f1 =  cid:21 1e cid:0  cid:21 1x , f2 =  cid:21 2e cid:0  cid:21 2x ,  D f1jjf2  = Z 1  0   cid:21 1   cid:21 2  cid:0   cid:21 1x +  cid:21 2x cid:21  dx  f1 x  cid:20 ln  cid:21 2  cid:21 1  cid:0  1:  +  = ln   cid:21 1  cid:21 2 273   11.1    11.2    11.3    11.4    274  Information Theory and Statistics   c  f1 = U [0; 1] , f2 = U [a; a + 1] ,   11.5    11.6    11.7    11.8   0  D f1jjf2  = Z 1 = Z a = 1:  0  f1 ln  f1 f2  f1 ln1 +Z 1  a  f1 ln 1  In this case, the Kullback Leibler distance of 1 implies that in a hypothesis test, the two distributions will be distinguished with probability 1 for large samples.   d  f1 = Bern cid:16  1  2 cid:17  and f2 = Bern 1  , D f1jjf2  =  1 2  ln  +  ln  1 2  1 2 0  = 1:  1 2 1  The implication is the same as in part  c .  2. A relation between D P k Q  and Chi-square. Show that the  cid:31 2 statistic   cid:31 2 =  cid:6 x   P  x   cid:0  Q x  2  Q x   2  cid:31 2 + : : :  Q = 1 + P cid:0 Q  is  twice  the  cid:12 rst term in the Taylor series expansion of D P k Q  about Q: Thus D P k Q  = 1 Suggestion: Write P Solution: A relation between D P k Q  and Chi-square. There are many ways to expand D PjjQ  in a Taylor series, but when we are expanding about P = Q , we must get a series in P  cid:0  Q , whose coe cid:14 cients depend on Q only. It is easy to get misled into forming another series expansion, so we will provide two alternative proofs of this result.  Q and expand the log.   cid:15  Expanding the log.  Writing P  Q = 1 + P cid:0 Q  Q = 1 +  cid:1   Q , and P = Q +  cid:1  , we get  D PjjQ  = Z P ln  P Q  = Z  Q +  cid:1   ln cid:18 1 + = Z  Q +  cid:1     cid:1  Q  cid:0   cid:1 2  cid:1 2 = Z  cid:1  + Q  cid:0  2Q   cid:1   Q cid:19  2Q2 + : : :!   cid:1 2  + : : : :   11.9    11.10    11.11    11.12    275   11.13    11.15    11.16    11.17    11.19   Information Theory and Statistics  The integral of the  cid:12 rst term R  cid:1  = R P  cid:0 R Q = 0 , and hence the  cid:12 rst non-zero  term in the expansion is   cid:1 2 2Q  =   cid:31 2 2  ;  which shows that locally around Q , D PjjQ  behaves quadratically like  cid:31 2 .   cid:15  By di cid:11 erentiation.  If we construct the Taylor series expansion for f , we can write  f  x  = f  c  + f0 c  x  cid:0  c  + f00 c    x  cid:0  c 2  2  + : : :   11.14   Doing the same expansion for D PjjQ  around the point Q , we get  D PjjQ P =Q = 0;  P Q  D0 PjjQ P =Q =  ln D00 PjjQ P =Q = cid:18  1  + 1 P =Q = 1;  P cid:19 P =Q  =  1 Q  :  and  Hence the Taylor series is  D PjjQ  = 0 +Z 1 P  cid:0  Q  +Z 1  Q   P  cid:0  Q 2  2  + : : :   11.18   =   cid:31 2 + : : : :  1 2  and we get  cid:31 2  2 as the  cid:12 rst non-zero term in the expansion.  e  3. Error exponent for universal codes. A universal source code of rate R achieves : = e cid:0 nD P  cid:3 kQ ; where Q is the true distribution and P  cid:3   a probability of error P  n  achieves min D P k Q  over all P such that H P    cid:21  R:  a  Find P  cid:3  in terms of Q and R:  b  Now let X be binary. Find the region of source probabilities Q x ; x 2 f0; 1g , e ! 0:  for which rate R is su cid:14 cient for the universal source code to achieve P  n   Solution: Error exponent for universal codes.   a  We have to minimize D pjjq  subject to the constraint that H p   cid:21  R . Rewriting  this problem using Lagrange multipliers, we get  J p  =X p log  p q  +  cid:21 X p log p +  cid:23 X p:   11.20    276  Information Theory and Statistics  Figure 11.1: Error exponent for universal codes  U  P*  Q  Di cid:11 erentiating with respect to p x  and setting the derivative to 0, we obtain  log  + 1 +  cid:21  log p +  cid:21  +  cid:23  = 0;   11.21   p q  which implies that  p cid:3  x  =  :  q cid:22  x   Pa q cid:22  a    11.22   where  cid:22  =  cid:21  1 cid:0  cid:21  is chosen to satisfy the constraint H p cid:3   = R . We have to  cid:12 rst check that the constraint is active, i.e., that we really need equality in the constraint. For this we set  cid:21  = 0 or  cid:22  = 1 , and we get p cid:3  = q . Hence if q is such that H q   cid:21  R , then the maximizing p cid:3  is q . On the other hand, if H q  < R , then  cid:21  6= 0 , and the constraint must be satis cid:12 ed with equality. Geometrically it is clear that there will be two solutions for  cid:21  of the form  11.22  which have H p cid:3   = R , corresponding to the minimum and maximum distance to q on the manifold H p  = R . It is easy to see that for 0  cid:20   cid:22   cid:20  1 , p cid:3  cid:22  x  lies on the geodesic from q to the uniform distribution. Hence, the minimum will lie in this region of  cid:22  . The maximum will correspond to negative  cid:22  , which lies on the other side of the uniform distribution as in the  cid:12 gure.   Information Theory and Statistics  277   b  For a universal code with rate R , any source can be transmitted by the code In the binary case, this corresponds to p 2 [0; h cid:0 1 R   or p 2  if H p  < R .  1  cid:0  h cid:0 1 R ; 1] , where h is the binary entropy function.  4. Sequential projection. We wish to show that projecting Q onto P1 and then project-  ing the projection ^Q onto P1T P2 is the same as projecting Q directly onto P1T P2: Let P1 be the set of probability mass functions on X satisfying  Let P2 be the set of probability mass functions on X satisfying  p x  = 1;  Xx p x hi x   cid:21   cid:11 i; i = 1; 2; : : : ; r:  p x  = 1;  Xx p x gj x   cid:21   cid:12 j; j = 1; 2; : : : ; s:  Xx  Xx  Suppose Q 62 P1S P2: Let P  cid:3  minimize D P k Q  over all P 2 P1: Let R cid:3  minimize D R k Q  over all R 2 P1T P2: Argue that R cid:3  minimizes D R k P  cid:3   over all R 2 P1T P2: Solution: Sequential Projection. P1 is de cid:12 ned by the constraints fhig and P2 by the constraints fgig . Hence P1 \ P2 is de cid:12 ned by the union of the constraints.  We will assume that all the constraints are active. In this case, from the parametric form of the distribution that minimizes D pjjq  subject to equality constraints as derived in the  cid:12 rst homework, we have  D pjjq  i=1  cid:21 ihi x :  p cid:3  x  = arg min p2P1  = c1q x ePr D pjjq  ihi x +Ps  p2P1\P2  i=1  cid:21 0  = c2q x ePr  r cid:3  x  = arg min  j=1  cid:23  0  j gj  x :  p cid:3  cid:3  x  = arg min  D pjjp cid:3    p2P1\P2  = c3p cid:3  x eP  cid:23 igi x  = c3c1q x eP  cid:23 igi x +P  cid:21 ihi x ;  where the constants are chosen so as to satisfy the constraints. Now when we project p cid:3  onto P1 \ P2 , we get   11.23    11.24    11.25    11.26    11.27    11.28    11.29    11.30    11.31    11.32    11.33    278  Information Theory and Statistics  which is of the same form as r cid:3  . Since the constants are chosen to satisfy the constraints in both cases, we will obtain the same constants, and hence  r cid:3  x  = p cid:3  cid:3  x   for all x .   11.34   Hence sequential projection is equivalent to direct projection, and r cid:3  minimizes D rjjp cid:3   over all r 2 P1 \ P2 . An alternative proof is to use the fact  proved in the  cid:12 rst homework  that for any set E determined by constraints of the type in the problem,  D pjjp cid:3   + D p cid:3 jjq  = D pjjq ;  for all p 2 E .   11.35   where p cid:3  is the distribution in E that is closest to q . Let p cid:3  cid:3  be the projection of p cid:3  on P1 \ P2 . Then for every element of P1 \ P2 ,  D pjjp cid:3   + D p cid:3 jjq  = D pjjq :  Taking the minimum of both sides over p 2 P1 \ P2 , we see that the same p must simultaneously minimize both sides, i.e.,  p cid:3  cid:3  = r cid:3 :  5. Counting. Let X = f1; 2; : : : ; mg . Show that the number of sequences xn 2 X n , to  cid:12 rst order in the  satisfying 1 exponent, for n su cid:14 ciently large, where  i=1 g xi   cid:21   cid:11  is approximately equal to 2nH  cid:3   nPn  H cid:3  =  P :Pm  max  H P  :  i=1 P  i g i  cid:21  cid:11   Solution: Counting. We wish to count the number of sequences satisfying a certain property. Instead of directly counting the sequences, we will calculate the probability of the set under an uniform distribution. Since the uniform distribution puts a probability 1 of mn on every sequence of length n , we can count the sequences by multiplying the probability of the set by mn .  The probability of the set can be calculated easily from Sanov’s theorem. Let Q be the uniform distribution, and let E be the set of sequences of length n satisfying 1  nP g xi   cid:21   cid:11  . Then by Sanov’s theorem, we have  Qn E  :=2 cid:0 nD P  cid:3 jjQ ;  where P  cid:3  is the type in E that is closest to Q . Since Q is the uniform distribution, D PjjQ  = log m  cid:0  H P   , and therefore P  cid:3  is the type in E that has maximum entropy. Therefore, if we let   11.36    11.37    11.38    11.39    11.40   H cid:3  =  P :Pm  max  H P  ;  i=1 P  i g i  cid:21  cid:11    279   11.41    11.42    11.43    11.44   Information Theory and Statistics  we have  Multiplying this by mn to  cid:12 nd the number of sequences in this set, we obtain  Qn E  :=2 cid:0 n log m cid:0 H  cid:3  :  jEj :=2 cid:0 n log m2nH  cid:3   mn = 2nH  cid:3   :  6. Biased estimates may be better. Consider the problem of estimating  cid:22  and  cid:27  2  from n samples of data drawn i.i.d. from a N   cid:22 ;  cid:27 2  distribution.  a  Show that Xn is an unbiased estimator of  cid:22  .  b  Show that the estimator  is biased and the estimator  S2  n =  1 n  n  Xi=1   Xi  cid:0  X n 2  S2 n cid:0 1 =  1  n  cid:0  1  n  Xi=1   Xi  cid:0  X n 2  is unbiased.   c  Show that S2  n has a lower mean squared error than S 2  n cid:0 1 . This illustrates the idea that a biased estimator may be \better" than an unbiased estimator for the same parameter.  Solution: Biased estimates may be better.   a  Let Xn = 1  i=1 Xi . Then EXn = 1  estimator of  cid:22  .  nPn   b  Before we compute the expected value of S 2  nP EXi =  cid:22  . Thus Xn is an unbiased  n , we will  cid:12 rst compute the variance  of Xn . By the independence of the Xi ’s, we have  var Xn  =  var Xi  =   11.45   1  n2 Xi   cid:27 2 n  :  Also, we will need to calculate  1   Xj  cid:0   cid:22  1 E Xi  cid:0   cid:22   Xn  cid:0   cid:22   = E Xi  cid:0   cid:22  0 nXj A @ nXj6=i E Xi  cid:0   cid:22  2 +  cid:27 2 n  1 n  =  =  1  ;  E Xi  cid:0   cid:22   Xj  cid:0   cid:22    11.47    11.46    11.48   since by independence, Xi and Xj are uncorrelated and therefore E Xi cid:0  cid:22   Xj cid:0   cid:22   = 0 .   280  Information Theory and Statistics  Therefore, if we let  we have  W =Xi   Xi  cid:0  Xn 2 =Xi  cid:16  Xi  cid:0   cid:22    cid:0   Xn  cid:0   cid:22   cid:17 2 E Xi  cid:0   cid:22  2  cid:0  2Xi  ;  EW = Xi  E Xi  cid:0   cid:22   Xn  cid:0   cid:22   + nE Xn  cid:0   cid:22  2 11.50    11.49    cid:27 2 n  + n   cid:27 2 n  = n cid:27 2  cid:0  2n =  n  cid:0  1  cid:27 2  Thus,  has ES2  n = n cid:0 1  n  cid:27 2 , and it is therefore a biased estimator of  cid:27 2 , and  S2  n =  n  Xi=1  Xi  cid:0  X n 2 =  W n  1 n  1  S2 n cid:0 1 =  n  Xi=1  n  cid:0  1   Xi  cid:0  X n 2 =  W n  cid:0  1  has expected value  cid:27 2 and is therefore an unbiased estimator of  cid:27 2 .   c  This involves a lot of algebra. We will need the following properties of the Normal distribution - the third central moment is 0 and the fourth central moment is 3 cid:27  4 , and therefore  We also know that T = Xn  cid:24  N   cid:22 ;  cid:27 2  for T :  n   , and we have the corresponding results  EXi =  cid:22  EX 2  i =  cid:22 2 +  cid:27 2  E Xi  cid:0   cid:22  3 = 0  EX 3  i =  cid:22 3 + 3 cid:27 2 cid:22   E Xi  cid:0   cid:22  4 = 3 cid:27 4  EX 4  i =  cid:22 4 + 6 cid:22 2 cid:27 2 + 3 cid:27 4:  ET =  cid:22   ET 2 =  cid:22 2 +  E T  cid:0   cid:22  3 = 0  ET 3 =  cid:22 3 + 3  E T  cid:0   cid:22  4 = 3   cid:27 4 n2   cid:27 2 n   cid:27 2 n   cid:22   ET 4 =  cid:22 4 + 6 cid:22 2  cid:27 2 n  + 3   cid:27 4 n2 :   11.51    11.52    11.53    11.54    11.55    11.56    11.57    11.58    11.59    11.60    11.61    11.62    11.63    11.64    11.65    11.66    Information Theory and Statistics  Also,  EXiT =  1 n  EX 2  i T 2 = E  = E  EXiXj =  cid:22 2 +  X 2  j + 2 Xj;k:j<k   cid:27 2 n  XjXk1 A  1  EX 2  i +  nXj6=i i 0 1 @Xj n2 X 2 n2 0 i +Xj6=i @X 4 +2 Xk:k6=i  X 3  1  X 2  i X 2  j + 2 Xj;k:j6=i;k6=i;j<k  X 2  i XjXk  i Xk1 A  n2  1  n2  cid:16  cid:22 4 + 6 cid:22 2 cid:27 2 + 3 cid:27 4 +  n  cid:0  1   cid:22 2 +  cid:27 2 2   n  cid:0  1  n  cid:0  2   +2  2    cid:22 2 +  cid:27 2  cid:22 2 + 2 n  cid:0  1  cid:22   cid:22 3 + 3 cid:27 2 cid:22   cid:19   n2 cid:22 4 +  n2 + 5n  cid:22 2 cid:27 2 +  n + 2  cid:27 4  =  =  Now we are in a position to calculate EW 2 . We  cid:12 rst rewrite  W = Xi = Xi = Xi = Xi  XiT + nT 2   Xi  cid:0  T  2 i  cid:0  2Xi X 2 i  cid:0  2nT T + nT 2 X 2 X 2 i  cid:0  nT 2:  Thus  and therefore  W 2 =  Xi = Xi  X 2  i  cid:0  nT 2!2 i + 2Xi<j  X 2  X 4  i X 2  j + n2T 4  cid:0  2nXi  X 2  i T 2  EW 2 = n  cid:22 4 + 6 cid:27 2 cid:22 2 + 3 cid:27 4  + 2    cid:22 2 +  cid:27 2 2 + n2  cid:22 4 + 6   cid:22 2 + 3  n n  cid:0  1   2   cid:27 2 n   cid:0 2n2 1 =  n2  cid:0  1  cid:27 4:  n2  n2 cid:22 4 +  n2 + 5n  cid:22 2 cid:27 2 +  n + 2  cid:27 4   281   11.67    11.68    11.69    11.70    11.71    11.72    11.73    11.74    11.75    11.76    cid:27 4 n2    11.77    11.78    282  Information Theory and Statistics  Now we can calculate the mean squared error of the two estimators. The error of Sn cid:0 1 = W  n cid:0 1 is  E Sn cid:0 1  cid:0   cid:27 2 2 = E  W 2  n  cid:0  1 2  cid:0  2  n2  cid:0  1  cid:27 4  n  cid:0  1 2  cid:0  2 2 n  cid:0  1   cid:27 4:  =  =  W n  cid:0  1  n  cid:0  1  cid:27 2 n  cid:0  1   cid:27 2 +  cid:27 4!   cid:27 2 +  cid:27 4   11.80   The error of Sn cid:0 1 = W  n cid:0 1 is  E Sn  cid:0   cid:27 2 2 = E  W 2 n2  cid:0  2  n2  cid:0  1  cid:27 4 n2 2n  cid:0  1 n2  cid:27 4:  =  =  W n   cid:27 2 +  cid:27 4!  n  cid:0  1  cid:27 2  cid:0  2  n   cid:27 2 +  cid:27 4  is less than 2  Since 2n cid:0 1 n cid:0 1 for all positive n , we see that Sn has a lower expected n2 error than Sn cid:0 1 . In fact, if we let the estimator of  cid:27 2 be cW , then we can easily calculate the expected error of the estimator to be  E cW  cid:0   cid:27 2 2 =  cid:27 4 cid:16  n2  cid:0  1 c2  cid:0  2 n  cid:0  1 c + 1 cid:17  ;   11.85   1  which is minimized for c = 1 n cid:0 1   or the maximum likelihood estimator   c = 1 squared error.  n+1 . Thus neither best unbiased estimator   c = n   produces the minimum mean  7. Fisher information and relative entropy. Show for a parametric family fp cid:18  x g  1  lim  cid:18 0! cid:18     cid:18   cid:0   cid:18 0 2 D p cid:18 jjp cid:18 0  =  1 ln 4  J  cid:18  :  Solution: Fisher information and relative entropy. Let t =  cid:18 0  cid:0   cid:18  . Then :  1  1  p cid:18  x  ln  1 t2 D p cid:18 jjp cid:18 +t  =  p cid:18  x  p cid:18 +t x     cid:18   cid:0   cid:18 0 2 D p cid:18 jjp cid:18 0  =  t2 ln 2Xx  that  Let   11.79    11.81    11.82    11.83    11.84    11.86    11.87    11.88    11.89   f  t  = p cid:18  x  ln  p cid:18  x  p cid:18 +t x   :  f0 t  =  cid:0   p cid:18  p cid:18 +t  dp cid:18 +t  ;  dt  We will suppress the dependence on x and expand f  t  in a Taylor series in t . Thus   Information Theory and Statistics  and  and  Thus expanding in the Taylor series around t = 0 , we obtain  f00 t  =  p cid:18  p2  dt  cid:19 2  cid:18 +t  cid:18  dp cid:18 +t  +  p cid:18  p cid:18 +t  d2p cid:18 +t  dt2  :  f  t  = f  0  + f0 0 t + f00 0   + O t3 ;  t2 2  where f  0  = 0 ,  f0 0  =  cid:0   f00 0  =  dt  p cid:18  p cid:18   dp cid:18 +t   cid:12  cid:12  cid:12  cid:12 t=0 d cid:18   cid:19 2 p cid:18   cid:18  dp cid:18   1  +  =  dp cid:18  d cid:18   d2p cid:18  d cid:18 2  Now Px p cid:18  x  = 1 , and therefore Xx Xx  and  dp cid:18  x   d cid:18   d dt  =  1 = 0;  d2p cid:18  x   d cid:18 2 =  d d cid:18   0 = 0:  Therefore the sum of the terms of  11.92  sum to 0 and the sum of the second terms in  11.93  is 0.  Thus substituting the Taylor expansions in the sum, we obtain  d cid:18   cid:19 2 p cid:18   cid:18  dp cid:18   +  d2p cid:18   d cid:18 2 ! t2  2  + O t3 !  11.97   p cid:18  x  p cid:18 +t x   1  1  t2 ln 2Xx  p cid:18  x  ln  1  =    cid:18   cid:0   cid:18 0 2 D p cid:18 jjp cid:18 0  = t2 ln 2 0 +Xx 2 ln 2Xx  =  1  d cid:18   dp cid:18  x   t +Xx   1 d cid:18   cid:19 2 p cid:18  x  cid:18  dp cid:18  x   + O t   1  =  J  cid:18   + O t   1 ln 4  and therefore  1  lim  cid:18 0! cid:18     cid:18   cid:0   cid:18 0 2 D p cid:18 jjp cid:18 0  =  1 ln 4  J  cid:18  :  f cid:18  x ;  cid:18  2 R is de cid:12 ned by  f cid:18  X   cid:19 2 J  cid:18   = E cid:18  cid:18  @f cid:18  X =@ cid:18   0  =Z  f   cid:18  2 f cid:18   Find the Fisher information for the following families:  8. Examples of Fisher information. The Fisher information J  cid:2   for the family  283   11.90    11.91    11.92    11.93    11.94    11.95    11.96    11.98    11.99    11.100    284  Information Theory and Statistics  e cid:0  x2  2 cid:18    a  f cid:18  x  = N  0;  cid:18   = 1p2 cid:25  cid:18   b  f cid:18  x  =  cid:18 e cid:0  cid:18 x; x  cid:21  0  c  What is the Cram cid:18 er Rao lower bound on E cid:18  ^ cid:18  X  cid:0  cid:18  2; where ^ cid:18  X  is an unbiased  estimator of  cid:18  for  a  and  b ?  Solution: Examples of Fisher information.   a  f cid:18  x  = N  0;  cid:18   = 1p2 cid:25  cid:18   e cid:0  x2  2 cid:18  , and therefore  and  Therefore the Fisher information,  f0 cid:18  =  cid:0   1 2  e cid:0  x2  2 cid:18  +  e cid:0  x2 2 cid:18  ;  1  p2 cid:25  cid:18 3 =  cid:0   f0 cid:18  f cid:18   1 2 cid:18   +  x2 2 cid:18 2  1 p2 cid:25  cid:18  2 cid:18 2! :  x2  fth cid:19 2 J th  = E cid:18  cid:18  f0th = E cid:18   1 4 cid:18 2  cid:0  2 1  cid:18  4 cid:18 2  cid:0  2 2 cid:18 2 + 1 2 cid:18 2 ;  1 2 cid:18   1  cid:18   =  =  x4  4 cid:18 4!  x2 2 cid:18 2 + 3 cid:18 2 4 cid:18 4  using the \well-known" or easily veri cid:12 ed fact that for a normal N  0;  cid:18   distribu- tion, the fourth moment is 3 cid:18 2 .   b  f cid:18  x  =  cid:18 e cid:0  cid:18 x; x  cid:21  0 , and therefore ln f cid:18  = ln  cid:18   cid:0   cid:18 x , and  and therefore  d ln f cid:18   d cid:18   =  1  cid:18   cid:0  x;  d cid:18   cid:19 2 J  cid:18   = Eth cid:18  d ln f cid:18   cid:18 2  cid:0  2 1 1 +  cid:18   cid:18   = E cid:18  cid:18  1 1  cid:18 2  cid:0  2 1  cid:18   1  cid:18   1  cid:18   =  =  x + x2 cid:19   +  1  cid:18 2  using the fact that for an exponential distribution, EX = 1   cid:18  , and EX 2 = 1   cid:18  + 1  cid:18 2 .   11.101    11.102    11.103    11.104    11.105    11.106    11.107    11.108    11.109    11.110    11.111    Information Theory and Statistics  285   c  The Cramer-Rao lower bound is the reciprocal of the Fisher information, and is  therefore 2 cid:18 2 and  cid:18  for parts  a  and  b  respectively.  9. Two conditionally independent looks double the Fisher information. Let  g cid:18  x1; x2  = f cid:18  x1 f cid:18  x2  . Show Jg  cid:18   = 2Jf   cid:18   . Solution: Two conditionally independent looks double the Fisher information. We can simply use the same arguments as in Section 12.11 in the text. We de cid:12 ne the score function V  Xi  = @ @ cid:18  ln f cid:18  xi  . Then the score functions are independent mean zero random variables and since the Fisher information of g is the variance of the sum of the score functions, it is the sum of the individual variances. Thus the Fisher information of g is twice the Fisher information of f .  10. Joint distributions and product distributions. Consider a joint distribution Q x; y  with marginals Q x  and Q y  . Let E be the set of types that look jointly typical with respect to Q , i.e.,  E = fP  x; y   P  x; y  log Q x   cid:0  H X  = 0;  :  cid:0 Xx;y  cid:0 Xx;y  cid:0 Xx;y  P  x; y  log Q y   cid:0  H Y   = 0;  P  x; y  log Q x; y   cid:0  H X; Y   = 0g:   11.112    a  Let Q0 x; y  be another distribution on X  cid:2  Y . Argue that the distribution P  cid:3   in E that is closest to Q0 is of the form  P  cid:3  x; y  = Q0 x; y e cid:21 0+ cid:21 1 log Q x + cid:21 2 log Q y + cid:21 3 log Q x;y ;   11.113   where  cid:21 0;  cid:21 1;  cid:21 2 and  cid:21 3 are chosen to satisfy the constraints. Argue that this distribution is unique.   b  Now let Q0 x; y  = Q x Q y  . Verify that Q x; y  is of the form  11.113  and satis cid:12 es the constraints. Thus P  cid:3  x; y  = Q x; y  , i.e., the distribution in E closest to the product distribution is the joint distribution.  Solution: Joint distributions and product distributions.   a  This result follows directly from Problem 2 in Chapter 11. We will not repeat the  arguments.   b  If we let  cid:21 0 = 0 ,  cid:21 i =  cid:0 1 ,  cid:21 2 =  cid:0 1 , and  cid:21 3 = 1 , then  P  cid:3  x; y  = Q0 x; y e cid:21 0+ cid:21 1 log Q x + cid:21 2 log Q y + cid:21 3 log Q x;y   1  1  Q x   Q y   Q x; y   = Q x Q y   = Q x; y    11.114    11.115    11.116   and therefore Q x; y  is of the form that minimizes the relative entropy. It is easy to verify that Q x; y  trivially satis cid:12 es the constraints involved in the de cid:12 nition   286  Information Theory and Statistics  of the set E . Therefore the joint distribution is the distribution that looks jointly typical and is closest to the product distribution.  11. Cramer-Rao inequality with a bias term. Let X  cid:24  f  x;  cid:18   and let T  X  be an  estimator for  cid:18  . Let bT   cid:18   = E cid:18 T  cid:0   cid:18  be the bias of the estimator. Show that  E T  cid:0   cid:18  2  cid:21   [1 + b0T   cid:18  ]2  J  cid:18    + b2  T   cid:18  :   11.117   Solution: Cramer-Rao inequality with a bias term. The proof parallels the proof with- out the bias term  Theorem 12.11.1 . We will begin with the calculation of E cid:18  V T   , where V is the score function and T is the estimator.   11.118    11.119    11.120    11.121    11.122    11.123    11.124    11.125   @ cid:18  f  x;  cid:18   f  x;  cid:18    E V T   = Z @ = Z @  @ cid:18   T  x f  x;  cid:18   dx  f  x;  cid:18  T  x  dx  @  @ cid:18  Z f  x;  cid:18  T  x  dx  =  =  E cid:18 T  @ @ cid:18  @ @ cid:18    bT   cid:18   +  cid:18    = = b0T   cid:18   + 1  By the Cauchy-Schwarz inequality, we have   E [ V  cid:0  EV   T  cid:0  ET  ] 2  cid:20  E V  cid:0  EV  2E T  cid:0  ET  2:  Also, EV = 0 and therefore E V  cid:0  EV   T  cid:0  ET   = E V T   . Also, by de cid:12 nition, var V   = J  cid:18   . Thus we have  Now   cid:0 b0T   cid:18   + 1 cid:1 2  cid:20  J  cid:18  E T  cid:0   cid:18   cid:0  bT   cid:18   2:  E T  cid:0   cid:18   cid:0  bT   cid:18   2 = E T  cid:0   cid:18  2 + b2 = E T  cid:0   cid:18  2 + b2 = E T  cid:0   cid:18  2  cid:0  b2  T   cid:18    cid:0  2E T  cid:0   cid:18  bT   cid:18   T   cid:18    cid:0  2b2 T   cid:18  :  T   cid:18     11.126    11.127    11.128   Substituting this in the Cauchy Schwarz inequality, we have the desired result  E T  cid:0   cid:18  2  cid:21   [1 + b0T   cid:18  ]2  J  cid:18    + b2  T   cid:18  :   11.129    Information Theory and Statistics  287  12. Hypothesis testing. Let X1; X2; : : : ; Xn be i.i.d.  cid:24  p x  . Consider the hypothesis  test H1 : p = p1 versus H2 : p = p2 . Let  p1 x  =8>< >: p2 x  =8>< >:  1  2 ; x =  cid:0 1 1 4 ; x = 0 1 4 ; x = 1  1  4 ; x =  cid:0 1 1 4 ; x = 0 1 2 ; x = 1 :  and  1 n   cid:0    cid:15    a  Find the error exponent for Prf Decide H2jH1 true g in the best hypothesis test  of H1 vs. H2 subject to Prf Decide H1jH2 true g  cid:20  1 2 .  Solution: Hypothesis testing  By the Cherno cid:11 -Stein lemma, the error exponent in this hypothesis test is the exponent for probability of the acceptance region for H2 given P1 , which is  D P2jjP1  =  1 4  1 4 1 2  1 4  1 4 1 4  1 2  1 2 1 4  log  +  log  +  log  = 0:25   11.130   Thus the probability of error will go to 0 as 2 cid:0  n 4 .  13. Sanov’s theorem: Prove the simple version of Sanov’s theorem for the binary random variables, i.e., let X1; X2; : : : ; Xn be a sequence of binary random variables, drawn i.i.d. according to the distribution:  Pr X = 1  = q; Pr X = 0  = 1  cid:0  q:  Let the proportion of 1’s in the sequence X1; X2; : : : ; Xn be pX , i.e.,  pX n =  Xi:  1 n  n  Xi=1  By the law of large numbers, we would expect pX to be close to q for large n . Sanov’s theorem deals with the probability that pX n is far away from q . In particular, for concreteness, if we take p > q > 1  2 , Sanov’s theorem states that  log Prf X1; X2; : : : ; Xn  : pX n  cid:21  pg ! p log  + 1 cid:0 p  log  = D  p; 1 cid:0 p jj q; 1 cid:0 q  :  p q  1  cid:0  p 1  cid:0  q  Justify the following steps:  Prf X1; X2; : : : ; Xn  : pX  cid:21  pg  cid:20   i!qi 1  cid:0  q n cid:0 i  n  n  Xi=bnpc   11.131    11.132    11.133    11.134    288  Information Theory and Statistics  the right hand side of the last equation.   cid:15  Argue that the term corresponding to i = bnpc is the largest term in the sum on  cid:15  Show that this term is approximately 2 cid:0 nD .  cid:15  Prove an upper bound on the probability in Sanov’s theorem using the above steps. Use similar arguments to prove a lower bound and complete the proof of Sanov’s theorem.  Solution: Sanov’s theorem   cid:15  Since nX n has a binomial distribution, we have  Pr nX n = i  = n  i!qi 1  cid:0  q n cid:0 i  and therefore  n   n i!qi 1  cid:0  q n cid:0 i  =  n  cid:0  i i + 1  q 1  cid:0  q   cid:15   Pr nX n = i   Pr nX n = i + 1   Prf X1; X2; : : : ; Xn  : pX  cid:21  pg  cid:20   Xi=bnpc =  cid:0  n i+1 cid:1 qi+1 1  cid:0  q n cid:0 i cid:0 1  cid:0 n i cid:1 qi 1  cid:0  q n cid:0 i i+1 < 1 cid:0 q This ratio is less than 1 if n cid:0 i of the terms occurs when i = bnpc .   n bnpc! :=2nH p    cid:15  From Example 11.1.3,  q  and hence the largest term in the sum is  ,i.e., if i > nq  cid:0   1  cid:0  q  . Thus the maximum    n bnpc!qbnpc 1 cid:0 q n cid:0 bnpc = 2n  cid:0 p log p cid:0  1 cid:0 p  log 1 cid:0 p  +np log q+n 1 cid:0 p  log 1 cid:0 q  = 2 cid:0 nD pjjq    cid:15  From the above results, it follows that  Prf X1; X2; : : : ; Xn  : pX  cid:21  pg  cid:20   i!qi 1  cid:0  q n cid:0 i  n  n  Xi=bnpc   cid:20   n  cid:0  bnpc   n  cid:20   n 1  cid:0  p  + 1 2 cid:0 nD pjjq   bnpc!qi 1  cid:0  q n cid:0 i 11.141    11.142   where the second inequality follows from the fact that the sum is less than the largest term times the number of terms. Taking the logarithm and dividing by n and taking the limit as n ! 1 , we obtain  1 n  lim n!1  log Prf X1; X2; : : : ; Xn  : pX  cid:21  pg  cid:20   cid:0 D pjjq    11.143    11.135    11.136    11.137    11.138    11.139    11.140    Information Theory and Statistics  Similarly, using the fact the sum of the terms is larger than the largest term, we obtain  Prf X1; X2; : : : ; Xn  : pX  cid:21  pg  cid:21   n   n i!qi 1  cid:0  q n cid:0 i Xi=dnpe  cid:21    n dnpe!qi 1  cid:0  q n cid:0 i  cid:21  2 cid:0 nD pjjq   and  1 n  1 n  lim n!1  lim n!1  log Prf X1; X2; : : : ; Xn  : pX  cid:21  pg  cid:21   cid:0 D pjjq   Combining these two results, we obtain the special case of Sanov’s theorem  log Prf X1; X2; : : : ; Xn  : pX  cid:21  pg =  cid:0 D pjjq    11.148   14. Sanov. Let Xi be i.i.d.  cid:24  N  0;  cid:27 2 :  nPn i  cid:21   cid:11 2g: This can be done from  cid:12 rst principles  since the normal distribution is nice  or by using Sanov’s theorem. i  cid:21   cid:11  ? That is, what is the P  cid:3  that   a  Find the exponent in the behavior of Prf 1 nPn  b  What does the data look like if 1 i=1 X 2  minimizes D P k Q  ?  i=1 X 2  Solution: Sanov   a  From the properties of the normal distribution, we know that P X 2  distribution with n degrees of freedom, and we can directly calculate  i has a  cid:31 2  However, using Sanov’s theorem, we know that the probability of the set  Pr cid:18  1  nX X 2  i  cid:21   cid:11 2 cid:19  = Pr cid:16  cid:31 2  n  cid:21  n cid:11 2 cid:17  2 ; n cid:11 2 2    cid:0   n 2     cid:0   n  =  1 n   cid:0   log Pr cid:18  1  nX X 2  i  cid:21   cid:11 2 cid:19  = D P  cid:3 jjQ ;  where P  cid:3  is the distribution that satis cid:12 es the constraint that is closest to Q . In this case,  D PjjQ  = Z f  x  ln  f  x   1p2 cid:25 e cid:27 2 e cid:0  x2  2 cid:27 2  =  cid:0 H f   +Z f  x  lnp2 cid:25 e cid:27 2 +Z f  x  =  cid:0 H f   + ln p2 cid:25 e cid:27 2 +  E[X 2] 2 cid:27 2  x2 2 cid:27 2  289   11.144    11.145    11.146    11.147    11.149    11.150    11.151    11.152    11.153    11.154    290  Information Theory and Statistics  and hence the distribution that minimizes the relative entropy is the distribution that maximizes the entropy subject to the expected square constraint. Using the results of the maximum entropy chapter  Chapter 12 , we can see the the maximum entropy distribution is of the form f  x   cid:24  Ce cid:0  cid:12 x2 , i.e., the maximum entropy distribution is a normal distribution. Thus the P  cid:3  that minimizes relative entropy subject to the constraint 1 i  cid:21   cid:11 2 is the N  0;  cid:11 2  distribution. Substituting this distribution back into the expression for relative entropy, we obtain  nP X 2  D P  cid:3 jjQ  =  cid:0 H N  0;  cid:11 2    cid:0  lnp2 cid:25 e cid:27 2 +  E[X 2] 2 cid:27 2  cid:11 2 2 cid:27 2  log 2 cid:25 e cid:27 2  +  =  =  1 2  1 2 1 2  log 2 cid:25 e cid:11 2   cid:0   cid:11 2 log  cid:27 2   cid:11 2  cid:27 2 +  1 2   11.155    11.156    11.157    b  From the above calculation and the conditional limit theorem, the distribution of  the data conditional on the constraint is P  cid:3  , which is N  0;  cid:11 2  .  15. Counting states.  Suppose an atom is equally likely to be in each of 6 states, X 2 fs1; s2; s3; : : : ; s6g . One observes n atoms X1; X2; : : : ; Xn independently drawn according to this uniform distribution. It is observed that the frequency of occurrence of state s1 is twice the frequency of occurrence of state s2 .   a  To  cid:12 rst order in the exponent, what is the probability of observing this event?   b  Assuming n large,  cid:12 nd the conditional distribution of the state of the  cid:12 rst atom  X1 , given this observation.  Solution: Counting states   a  Using Sanov’s theorem, we need to determine the distribution P  cid:3  that is closest to the uniform with p1 = 2p2 , which is the empirical constraint. We need to minimize  D PjjQ  =X pi log 6pi  subject to the constraints P pi = 1 and p1  cid:0  2p2 = 0 . Setting up the functional  J P   =X pi log 6pi +  cid:21 1X pi +  cid:21 2 p1  cid:0  2p2   Di cid:11 erentiating with respect to pi and setting to 0, we obtain  log 6p1 + 1 +  cid:21 1 +  cid:21 2 = 0 log 6p2 + 1 +  cid:21 1  cid:0  2 cid:21 2 = 0 log 6pi + 1 +  cid:21 1 = 0;  i = 3; 4; 5; 6   11.158    11.159    11.160    11.161    11.162    Information Theory and Statistics  Thus  p1 = c12 cid:0  cid:21 2 p2 = c122 cid:21 2 pi = c1;  i = 3; 4; 5; 6  291   11.163    11.164    11.165   where c1 = 1  6 2 cid:0  1+ cid:21 1  . Since p1 = 2p2 , we obtain  cid:21 2 =  cid:0  1  chosen so that P pi = 1 , which in turn implies that c1 = 1= 2  3 log 2 . c1 should be 3 + 4  = 1=5:889 , and the corresponding distribution is  0.213,0.107,0.17,0.17,0.17,0.17 , and the relative entropy distance is 0.0175. Thus the  cid:12 rst order probability that this event happens is 2 cid:0 0:0175n .  3 + 2 cid:0  2  1   b  When the event   p1 = 2p2   happens, the conditional distribution is close to P  cid:3  =   0:213; 0:107; 0:17; 0:17; 0:17; 0:17  .  16. Hypothesis testing  Let fXig be i.i.d.  cid:24  p x  , x 2 f1; 2; : : :g . Consider two hypotheses H0 : p x  = p0 x  2 cid:17 x vs. H1 : p x  = p1 x  , where p0 x  = cid:16  1  a  Find D p0 k p1  .  b  Let PrfH0g = 1  2 . Find the minimal probability of error test for H0 vs. H1 given  , and p1 x  = qpx cid:0 1 , x = 1; 2; 3; : : :  data X1; X2; : : : ; Xn  cid:24  p x  .  Solution: Hypothesis testing   a   p0 x  p1 x   qpx cid:0 1  p0 x  log  D p0jjp1  = Xx log  cid:16  1 2 cid:17 x = Xx  cid:18  1 2 cid:19 x = Xx  cid:18  1 2p cid:19 x p q cid:19  2 cid:19 x log cid:18  cid:18  1 = Xx  cid:18  1 2 cid:19 x 2 cid:19 x 2p cid:19  +Xx  cid:18  1 x log cid:18  1 = log cid:18  1 q Xx  cid:18  1 2p cid:19 Xx  cid:18  1 2 cid:19 x 2 cid:19 x 2p cid:19  + log = 2 log cid:18  1 =  cid:0  log 4pq   x + log  log  p q  p q  p   11.166    11.167    11.168    11.169    11.170    11.171    11.172    292  Information Theory and Statistics   b  In the Bayesian setting, the minimum probability of error exponent is given by  the Cherno cid:11  information  C P1; P2 4=  cid:0  min 0 cid:20  cid:21  cid:20 1  log Xx  1  x P 1 cid:0  cid:21  P  cid:21   2   x ! :   11.173   Now  1  x P 1 cid:0  cid:21  P  cid:21   2  Xx   x  = Xx  cid:18  1 p cid:19 1 cid:0  cid:21  2 cid:19 x cid:21  cid:18  q 2 cid:21  !x Xx  p1 cid:0  cid:21  =  cid:18  q p cid:19 1 cid:0  cid:21  =  cid:18  q p cid:19 1 cid:0  cid:21  1  cid:0  p1 cid:0  cid:21   p1 cid:0  cid:21   2 cid:21   2 cid:21   =  q1 cid:0  cid:21 p cid:21  2 cid:21 p cid:21   cid:0  p  px 1 cid:0  cid:21     11.174    11.175    11.176    11.177   To  cid:12 nd the mimimum of this over  cid:21  , we di cid:11 erentiate the logarithm of this with respect to  cid:21  , and obtain   cid:0  log q + log p  cid:0   1   2p  cid:21   cid:0  p   2p  cid:21  log 2p = 0   11.178   Solving for  cid:21  from this equation and substituting this into the de cid:12 nition of Cher- no cid:11  information will provide us the answer.  17. Maximum likelihood estimation. Let ff cid:18  x g denote a parametric family of den-  sities with parameter  cid:18  cid:15 R . Let X1; X2; : : : ; Xn be i.i.d.  cid:24  f cid:18  x  . The function  l cid:18  xn  = ln  n Yi=1  f cid:18  xi !  is known as the log likelihood function. Let  cid:18 0 denote the true parameter value.   a  Let the expected log likelihood be  and show that  E cid:18 0l cid:18  X n  =Z  ln  n  Yi=1  n  Yi=1  f cid:18  xi    f cid:18 0 xi dxn ;  E cid:18 0 l X n   =   cid:0 h f cid:18 0   cid:0  D f cid:18 0jjf cid:18   n :   b  Show that the maximum over  cid:18  of the expected log likelihood is achieved by   cid:18  =  cid:18 0 .  Solution: Maximum likelihood  This problem is the continuous time analog of the cost of miscoding.   293   11.179    11.180    11.181   dxn 11.182    11.183    11.184    11.185   Information Theory and Statistics   a  Let us denote  f cid:18  xn  =  n Yi=1  f cid:18  xi !  Then if  E cid:18 0l cid:18  X n  = Z  log  n  Yi=1  n  Yi=1  f cid:18  xi    f cid:18 0 xi dxn ;   cid:0  Z f cid:18 0 xn  log f cid:18  xn  dxn = Z ln f cid:18 0 xn  log f cid:18 0 xn  dxn +Z f cid:18 0 xn  log =  cid:0 h f cid:18 0 xn    cid:0  D f cid:18 0 xn jjf cid:18  xn   =  cid:0 n h f cid:18 0 x    cid:0  D f cid:18 0 x jjf cid:18  x     f cid:18  xn  f cid:18 0 xn    b  From the non-negativity of relative entropy, it follows from the last equation that  the maximum value of the likelihood occurs when  D f cid:18 0 x jjf cid:18  x   = 0  or  cid:18  =  cid:18 0 .  geometric distribution  18. Large deviations. Let X1; X2; : : : be i.i.d. random variables drawn according to the  P rfX = kg = pk cid:0 1 1  cid:0  p ; k = 1; 2; : : :  :  Find good estimates  to  cid:12 rst order in the exponent  of  a  P rf 1  b  P rfX1 = kj 1 i=1 Xi  cid:21   cid:11 g :  c  Evaluate a  and b  for p = 1  i=1 Xi  cid:21   cid:11 g :  nPn  nPn  2 ;  cid:11  = 4 .  Solution: Large deviations  By Sanov’s theorem, the probability is determined by the relative entropy distance to the closest distribution that satis cid:12 es the constraint. Let that distribution be r1; r2; : : : ; on the integers 1,2, : : : . Then the relative entropy distance to the geometric distribution is  D rjjp  =X ri log  ri  pi cid:0 1 1  cid:0  p   We need to minimize this subject to the constraints, P ri = 1 , and P iri =  cid:11  . We  have assumed that the constraint is matched with equality without loss of generality. We set up the functional   11.186    11.187   J r  =X ri log  ri  pi cid:0 1 1  cid:0  p   +  cid:21 1X ri +  cid:21 2X iri   294  Information Theory and Statistics  Di cid:11 erentiating with respect to ri and setting to 0, we obtain  log ri  cid:0  log pi cid:0 1 1  cid:0  p   +  cid:21 1 +  cid:21 2i = 0  From the form of the equation for ri , it is clear that r too is a geometric distribution. is a geometric  Since we need to satisfy the constraint P iri =  cid:11  , it follows that ri distribution with parameter 1  cid:0  1   cid:11  . Therefore  ri = pi cid:0 1 1  cid:0  p c1ci  2  ri = cid:18 1  cid:0   1   cid:11  cid:19 i cid:0 1 1   cid:11   D rjjp  = Xi = Xi  ri log  ri log  ri   cid:11   pi cid:0 1 1  cid:0  p  1  cid:0  1  cid:11   cid:18  1  cid:11 p cid:19 i p 1  cid:0  p  cid:11   cid:0  1 p  cid:11 p  +  cid:11  log  1  = log   1  cid:0  p   cid:11   cid:0  1    11.188    11.189    11.190    11.191    11.192    11.193   1 n   cid:0   log Prf  1 n  n  Xi=1  Xi  cid:21   cid:11 g = D rjjp  = log  p   1  cid:0  p   cid:11   cid:0  1   +  cid:11  log   11.194    cid:11   cid:0  1  cid:11 p  PrfX1 = kj  1 n  n  Xi=1  Xi  cid:21   cid:11 g = rk = cid:18 1  cid:0   1   cid:11  cid:19 k cid:0 1 1   cid:11    11.195    c  For  cid:11  = 4 and p = 0:5 , we have D = log 27=16 = 0:755 , and the conditional  distribution of X1 is geometric with mean 4, i.e.  PrfX1 = kj  Xi  cid:21   cid:11 g = rk = 0:75k cid:0 10:25   11.196   1 n  n  Xi=1  19. Another expression for Fisher information. Use integration by parts to show  J  cid:18   =  cid:0 E  @2 ln f cid:18  x   :  @ cid:18 2  Solution: Another expression for Fisher information  or  and   a    b   that   Information Theory and Statistics  From  11.270 , we have  295   11.197    11.198    11.199    11.200    11.201    11.202    11.203    11.204    11.205    11.206    11.207    11.209   dx  ln f  x;  cid:18   cid:21 2 f  x;  cid:18   2  @ cid:18  @ cid:18  f  x;  cid:18    dx  0  @ cid:18   J  cid:18   = E cid:18  cid:20  @ = Z 1 = Z 1 = Z 1 = Z 1  ln f  X;  cid:18   cid:21 2 f  x;  cid:18   cid:20  @ f  x;  cid:18  " @  cid:16  @ @ cid:18  f  x;  cid:18   cid:17 2 f  x;  cid:18   cid:19 " @ 0  cid:18  @  f  x;  cid:18    dx  @ cid:18   0  0  @ cid:18  f  x;  cid:18    f  x;  cid:18    dx  Z 1  0  @ cid:18  ln f  x;  cid:18   and dv = cid:16  @ 0  cid:18  @ dv = Z 1 @ cid:18  Z 1  f  x;  cid:18   cid:19  dx  f  x;  cid:18   dx  @ cid:18   =  @  0  @ @ cid:18   1  =  = 0  Now integrating by parts, setting u = @  @ cid:18  f  x;  cid:18   cid:17  dx , we have  Therefore since  we have  Z u dv = uv  cid:0 Z v du f  x;  cid:18    dx =  cid:0 Z 1 0  cid:0   @ cid:18  f  x;  cid:18    0  cid:18  @ Z 1  @ cid:18   f  x;  cid:18   cid:19 " @  @2 @ cid:18 2 ln; f  x;  cid:18   dx   11.208   20. Stirling’s approximation: Derive a weak form of Stirling’s approximation for facto-  rials, i.e., show that  using the approximation of integrals by sums. Justify the following steps:  ln n!  =  ln x dx + ln n = ::::::   11.210   and  ln n!  =  ln x dx = ::::::   11.211    cid:18  n e cid:19 n  e cid:19 n  cid:20  n!  cid:20  n cid:18  n  n cid:0 1  Xi=2  2  ln i  + ln n   cid:20 Z n cid:0 1 ln i   cid:21 Z n  Xi=1  n  0   296  Information Theory and Statistics  Solution: Stirling’s approximation The basic idea of the proof is  cid:12 nd bounds for the i=2 ln i . If we plot the sum as a sum of rectangular areas, as shown in Figure 11.2, it is not di cid:14 cult to see that the total area of the rectangles is bounded above by the integral of the upper curve, and bounded below by the integral of the lower curve.  sum Pn  log i  log x  log x-1   4  3  2  1  0  -1  1  2  3  4  5  6  7  8  9  10  11  Figure 11.2: Upper and lower bounds on log n!  Now consider the upper bound: From the  cid:12 gure, it follows that the sum of the rectangles starting at 2,3,4, : : : ; n  cid:0  1 is less than the integral of the upper curve from 2 to n . Therefore,  ln n!  = ln n +  n cid:0 1  ln i  n cid:0 1  ln i + ln 1  = ln n +  Xi=2 Xi=2  cid:20  ln n +Z n ln x  dx = ln n + [x ln x  cid:0  x]n = ln n + n ln n  cid:0  n  cid:0   2 ln 2  cid:0  2  = ln n + n ln n=e   cid:0  ln 4=e2   2  2  n!  cid:20  n cid:18  n  e cid:19 n e2  4 !  cid:20  2n cid:18  n e cid:19 n  ;   11.212    11.213    11.214    11.215    11.216    11.217    11.218   Therefore, exponentiating, we get   Information Theory and Statistics  297  since e2=4 = 1:847 < 2 .  For the lower bound, from the  cid:12 gure, it follows that the sum of the areas of the rectanges starting at 1; 2; : : : ; n is less than the integral of the lower curve from 1 to n + 1 . Therefore  ln n!  =  n  1  ln i  ln x  cid:0  1  dx  Xi=1  cid:21  Z n+1 = Z n ln x  dx = [x ln x  cid:0  x]n = n ln n  cid:0  n  cid:0   0 ln 0  cid:0  0  = n ln n=e   0  0   11.219    11.220    11.221    11.222    11.223    11.224    11.225    11.227    11.228    11.229   Therefore, exponentiating, we get  n!  cid:21  cid:18  n e cid:19 n  :  21. Asymptotic value of  cid:0 n  k cid:1  . Use the simple approximation of the previous problem to show that, if 0  cid:20  p  cid:20  1 , and k = bnpc , i.e., k is the largest integer less than or equal to np , then  1 n  log n  k! =  cid:0 p log p  cid:0   1  cid:0  p  log 1  cid:0  p  = H p :  lim n!1   11.226   Now let pi , i = 1; : : : ; m be a probability distribution on m symbols, i.e., pi  cid:21  0 , and Pi pi = 1 . What is the limiting value of  log   1 n  n  j=0 bnpjc! =  1 n  log  bnp1c bnp2c : : : bnpm cid:0 1c n  cid:0 Pm cid:0 1 bnp1c! bnp2c! : : : bnpm cid:0 1c!  n  cid:0 Pm cid:0 1  n!  j=0 bnpjc !  Using the bounds  k cid:1  Solution: Asymptotic value of  cid:0 n  cid:18  n e cid:19 n  we obtain  e cid:19 n  cid:20  n!  cid:20  n cid:18  n  1 n  log n  k! =  1 n   log n!  cid:0  log k!  cid:0  log n  cid:0  k !    298  Information Theory and Statistics   cid:20   1  n log n cid:18 n e cid:19 n log n  cid:0   k n  1 n  log  e cid:19 k  cid:0  log cid:18  k n  cid:0  k k n  cid:0  n   cid:0  log cid:18  n  cid:0  k n  cid:0  k n  log  e  cid:19 n cid:0 k!  11.230    11.231    11.232   = ! H p  Similarly, using the same bounds  1 n  log n  k! =  cid:21   1 n   log n!  cid:0  log k!  cid:0  log n  cid:0  k !  n log cid:18  n e cid:19 n  1  e cid:19 k  cid:0  log k cid:18  k  cid:0  log n  cid:0  k  cid:18  n  cid:0  k n  cid:0  k n  cid:0  k k n  cid:0  n n  k n  log  log  log k n  cid:0  k   cid:0    11.233   e  cid:19 n cid:0 k! 11.234   1 =  cid:0  n ! H p   and therefore  lim  1 n  log n  k! = H p   By the same arguments, it is easy to see that  lim  1 n  log   n  bnp1c bnp2c : : : bnpm cid:0 1c n  cid:0 Pm cid:0 1  j=0 bnpjc! = H p1; : : : ; pm    11.238   22. The running di cid:11 erence.. Let X1; X2; : : : ; Xn be i.i.d.  cid:24  Q1 x  , and Y1; Y2; : : : ; Yn be i.i.d.  cid:24  Q2 y  . Let X n and Y n be independent. Find an expression for PrfPn i=1 Xi cid:0  Pn i=1 Yi  cid:21  ntg , good to  cid:12 rst order in the exponent. Again, this answer can be left in  parametric form.  Solution: Running di cid:11 erence  The joint distribution of X and Y is Q x; y  = Q1 x Q2 y  . The constraint that the running di cid:11 erence is greater than nt translates to a constraint on the empirical join distribution, i.e.,  By Sanov’s theorem, the probability of this large deviation is 2 cid:0 nD cid:3  to the  cid:12 rst or- der in the exponent, where D cid:3  is the minimum relative entropy distance between all distributions P that satisfy the above constraint and Q x; y  = Q1 x Q2 y  , i.e.,  Xi Xj  Pn i; j  i  cid:0  j   cid:21  t  D cid:3  =  min  PiPj Pn i;j  i cid:0 j  cid:21 t  D PjjQ    11.235    11.236    11.237    11.239    11.240    Information Theory and Statistics  299  23. Large likelihoods. Let X1; X2; : : : be i.i.d.  cid:24  Q x  , x 2 f1; 2; : : : ; mg . Let P  x  be  some other probability mass function. We form the log likelihood ratio  1 n  log  P n X1; X2; : : : ; Xn  Qn X1; X2; : : : ; Xn   =  1 n  n  Xi=1  log  P  Xi  Q Xi   of the sequence X n and ask for the probability that it exceeds a certain threshold. Speci cid:12 cally,  cid:12 nd  to  cid:12 rst order in the exponent   Qn cid:18  1  n  log  P  X1; X2; : : : ; Xn  Q X1; X2; : : : ; Xn   > 0 cid:19  :  There may be an undetermined parameter in the answer.  Solution:  24. Fisher information for mixtures. Let f1 x  and f0 x  be two given probability densities. Let Z be Bernoulli   cid:18   , where  cid:18  is unknown. Let X  cid:24  f1 x  , if Z = 1 and X  cid:24  f0 x  , if Z = 0 .  a  Find the density f cid:18  x  of the observed X .   b  Find the Fisher information J  cid:18   .   c  What is the Cram cid:19 er-Rao lower bound on the mean squared error of an unbiased  estimate of  cid:18  ?   d  Can you exhibit an unbiased estimator of  cid:18  ?  Solution:  25. Bent coins. Let fXig be iid  cid:24  Q where  Q k  = Pr Xi = k  = cid:18 m  k cid:19  qk 1  cid:0  q m cid:0 k; for k = 0; 1; 2; : : : ; m:  Thus, the Xi ’s are iid  cid:24  Binomial  m; q  . Show that, as n ! 1 ,  Pr X1 = kj  Xi  cid:21   cid:11   ! P  cid:3  k ;  1 n  n  Xi=1  where P  cid:3  is Binomial  m;  cid:21    i.e. P  cid:3  k  = cid:18 m  k cid:19   cid:21 k 1  cid:0   cid:21  m cid:0 k for some  cid:21  2 [0; 1]  .  Find  cid:21  .  Solution:  26. Conditional limiting distribution.   300  Information Theory and Statistics   a  Find the exact value of  if X1; X2; : : : ; are Bernoulli  2  3   and n is a multiple of 4.   b  Now let Xi cid:15 f cid:0 1; 0; 1g and let X1; X2 : : : be i.i.d. uniform over f cid:0 1; 0; +1g: Find  the limit of  P rfX1 = 1j  1 n  n  Xi=1  Xi =  1 4g ;  P rfX1 = +1j  1 n  n  Xi=1  X 2  i =  1 2g   11.241    11.242   for n = 2k;  k ! 1 .  Solution:  27. Variational inequality: Verify, for positive random variables X , that  log EP  X  = sup Q  [EQ log X   cid:0  D QjjP  ] where EP  X  = Px xP  x  and D QjjP   = Px Q x  log Q x  over all Q x   cid:21  0 , P Q x  = 1 . It is enough to extremize J Q  = EQ ln X cid:0 D QjjP  +  cid:21  P Q x   cid:0  1  .  P  x  , and the supremum is  Solution:   11.243   28. Type constraints.   a  Find constraints on the type PX n such that the sample variance X 2  n cid:0   X n 2  cid:20   cid:11  ,  where X 2  n = 1  i=1 X 2  i and Xn = 1  i=1 Xi .   b  Find the exponent in the probability Qn X 2  nPn  nPn  answer in parametric form.  n  cid:0   X n 2  cid:20   cid:11   . You can leave the  Solution:  29. Uniform distribution on the simplex.  Which of these methods will generate a sample from the uniform distribution on the  i=1 xi = 1g ?  simplex fx 2 Rn : xi  cid:21  0; Pn  a  Let Yi be i.i.d. uniform [0; 1] , with Xi = Yi=Pn  b  Let Yi be i.i.d. exponentially distributed  cid:24   cid:21 e cid:0  cid:21 y , y  cid:21  0; with Xi = Yi=Pn j=1 Yj:  c   Break stick into n parts.  Let Y1; Y2; : : : ; Yn cid:0 1 be i.i.d. uniform [0; 1] , and let  j=1 Yj:  Xi be the length of the ith interval.  Solution:   Chapter 12  Maximum Entropy  1. Maximum entropy. Find the maximum entropy density f , de cid:12 ned for x  cid:21  0 , satisfying EX =  cid:11 1; E ln X =  cid:11 2: That is, maximize  cid:0 R f ln f subject to R xf  x  dx =  cid:11 1;R  ln x f  x  dx =  cid:11 2 , where the integral is over 0  cid:20  x < 1 . What family of densities  is this?  Solution: Maximum entropy.  As derived in class, the maximum entropy distribution subject to constraints  and  is of the form  Z xf  x  dx =  cid:11 1 Z  ln x f  x  dx =  cid:11 2  f  x  = e cid:21 0+ cid:21 1x+ cid:21 2 ln x = cx cid:21 2e cid:21 1x;   12.1    12.2    12.3   which is of the form of a Gamma distribution. The constants should be chosen so as to satisfy the constraints.  2. Min D P k Q  under constraints on P: We wish to  cid:12 nd the  parametric form  of the probability mass function P  x ; x 2 f1; 2; : : :g that minimizes the relative entropy D P k Q  over all P such that P P  x gi x  =  cid:11 i; P  cid:3  x  = Q x eP1   a  Use Lagrange multipliers to guess that  i = 1; 2; : : : :  i=1  cid:21 igi x + cid:21 0   12.4   achieves this minimum if there exist  cid:21 i ’s satisfying the  cid:11 i constraints. This generalizes the theorem on maximum entropy distributions subject to constraints.   b  Verify that P  cid:3  minimizes D P k Q : Solution: Minimize D PjjQ  under constraints on P .  301   302  Maximum Entropy   a  We construct the functional using Lagrange multipliers  J P   =Z P  x  ln  P  x  Q x   +Xi   cid:21 iZ P  x hi x  +  cid:21 0Z P  x :   12.5   ‘Di cid:11 erentiating’ with respect to P  x  , we get  @J @P  = ln  P  x  Q x   + 1 +Xi   cid:21 ihi x  +  cid:21 0 = 0;   12.6   which indicates that the form of P  x  that minimizes the Kullback Leibler distance is  P  cid:3  x  = Q x e cid:21 0+Pi  cid:21 ihi x :   12.7    b  Though the Lagrange multiplier method correctly indicates the form of the solu- tion, it is di cid:14 cult to prove that it is a minimum using calculus. Instead we use the properties of D PjjQ  . Let P be any other distribution satisfying the constraints. Then  D PjjQ   cid:0  D P  cid:3 jjQ  = Z P  x  ln = Z P  x  ln = Z P  x  ln = Z P  x  ln = Z P  x  ln = D PjjP  cid:3    cid:21  0;  P  x   P  x   P  cid:3  x  Q x   Q x   cid:0 Z P  cid:3  x  ln Q x   cid:0 Z P  cid:3  x [ cid:21 0 +Xi Q x   cid:0 Z P  x [ cid:21 0 +Xi Q x   cid:0 Z P  x  ln  P  cid:3  x  Q x   P  x   P  x   P  x  P  cid:3  x    cid:21 ihi x ]   cid:21 ihi x ]   since both P and P  cid:3  satisfy the constraints    12.8    12.9    12.10    12.11    12.12    12.13    12.14   and hence P  cid:3  uniquely minimizes D PjjQ  . In the special case when Q is a uniform distribution over a  cid:12 nite set, minimizing D PjjQ  corresponds to maximizing the entropy of P .  3. Maximum entropy processes. Find the maximum entropy rate stochastic process  fXig1 cid:0 1 subject to the constraints:  a  EX 2  b  EX 2  i = 1; 2; : : : , i = 1; i = 1 , EXiXi+1 = 1 2 ;  i = 1; 2; : : : .   c  Find the maximum entropy spectrum for the processes in parts  a  and  b .   Maximum Entropy  303  Solution: Maximum Entropy Processes.   a  If the only constraint is EX 2  i = 1 , then by Burg’s theorem, it is clear that the maximum entropy process is a 0-th order Gauss-Markov, i.e., Xi i.i.d.  cid:24  N  0; 1  . 2 , then by Burg’s theorem, the  i = 1; EXiXi+1 = 1   b  If the constraints are EX 2  maximum entropy process is a  cid:12 rst order Gauss-Markov process of the form  Xi =  cid:0 aXi cid:0 1 + Zi; Zi  cid:24  N  0;  cid:27 2 : To determine a and  cid:27 2 , we use the Yule-Walker equations  R0 =  cid:0 aR1 +  cid:27 2 R1 =  cid:0 aR0  Substituting R0 = 1 and R1 = 1 maximum entropy process is  2 , we get a =  cid:0  1  2 and  cid:27 2 = 3  4 . Hence the  Xi =  Xi cid:0 1 + Zi; Zi  cid:24  N  0;  1 2  3 4   :  4. Maximum entropy with marginals. What is the maximum entropy distribution p x; y  that has the following marginals? Hint: You may wish to guess and verify a more general result.   12.15    12.16    12.17    12.18    12.19   xny  1 p11 p21 p31  2 p12 p22 p32  3 p13 p23 p33  1 2 3  1=2 1=4 1=4  2=3  1=6  1=6  Solution: Maximum entropy with marginals.  Given the marginal distributions of X and Y , H X  and H Y   are  cid:12 xed. Since I X; Y   = H X  + H Y    cid:0  H X; Y    cid:21  0 , we have  H X; Y    cid:20  H X  + H Y     12.20   with equality if and only if X and Y are independent. Hence the maximum value of H X; Y   is H X  + H Y   , and is attained by choosing the joint distribution to be the product distribution, i.e.,   304  Maximum Entropy  x y  1 1=3 1=6 1=6  2  1=12 1=24 1=24  3  1=12 1=24 1=24  1 2 3  1=2 1=4 1=4  2=3  1=6  1=6  5. Processes with  cid:12 xed marginals. Consider the set of all densities with  cid:12 xed pairwise marginals fX1;X2 x1; x2 ; fX2;X3 x2; x3 ; : : : ; fXn cid:0 1;Xn xn cid:0 1; xn  . Show that the max- imum entropy process with these marginals is the  cid:12 rst-order  possibly time-varying  Markov process with these marginals. Identify the maximizing f  cid:3  x1; x2; : : : ; xn  . Solution: Processes with  cid:12 xed marginals  By the chain rule,  h X1; X2; : : : ; Xn  = h X1  +  h XijXi cid:0 1; : : : ; X1   n  n  Xi=2 Xi=2   cid:20  h X1  +  h XijXi cid:0 1 ;   12.21    12.22   since conditioning reduces entropy. The quantities h X1  and h XijXi cid:0 1  depend only on the second order marginals of the process and hence the upper bound is true for all processes satisfying the second order marginal constraints.  De cid:12 ne  f cid:3  x1; x2; : : : ; xn  = f0 x1    12.23   n  Yi=2  f0 xi cid:0 1; xi  f0 xi cid:0 1   :  We will show that f  cid:3  maximizes the entropy among all processes with the same second order marginals. To prove this, we just have to show that this process satis cid:12 es has the same second order marginals and that this process achieves the upper bound  12.22 . The fact that the process satis cid:12 es the marginal constraints can be easily proved by induction. Clearly, it is true for f  cid:3  x1; x2  and if f cid:3  xi cid:0 1; xi  = f0 xi cid:0 1; xi  , then f cid:3  xi  = f0 xi  and by the de cid:12 nition of f  cid:3  , it follows that f  cid:3  xi; xi+1  = f0 xi; xi+1  . Also, since by de cid:12 nition, f  cid:3  is  cid:12 rst order Markov, h XijXi cid:0 1; : : : ; X1  = h XijXi cid:0 1  and we have equality in  12.22 . Hence f  cid:3  has the maximum entropy of all processes with the same second order marginals.  6. Every density is a maximum entropy density. Let f0 x  be a given density. Given   cid:11  . Now let r x  = ln f0 x  . Show that g cid:11  x  = f0 x  for an appropriate choice  r x  , let g cid:11  x  be the density maximizing h X  over all f satisfying R f  x r x  dx =  cid:11  =  cid:11 0 . Thus f0 x  is a maximum entropy density under the constraint R f ln f0 =  cid:11 0 .  Solution: Every density is a maximum entropy density. Given the constraints that  Z r x f  x  =  cid:11    12.24    305   12.25    12.26   Maximum Entropy  the maximum entropy density is  With r x  = log f0 x  , we have  f cid:3  x  = e cid:21 0+ cid:21 1r x   f cid:3  x  =  f  cid:21 1 0  x  0  x  dx  R f  cid:21 1  ^Xn =  max f  xn   min  b  n cid:0 1  biXn cid:0 i:  Xi=1 E Xn  cid:0  ^Xn 2;  where  cid:21 1 has to chosen to satisfy the constraint. We can chose the value of the con- straint to correspond to the value  cid:21 1 = 1 , in which case f  cid:3  = f0 . So f0 is a maximum entropy density under appropriate constraints.  i=1 satisfy EXiXi+k = Rk;  k = 0; 1; : : : ; p . Consider linear predictors for  7. Mean squared error.  Let fXign Xn , i.e.  Assume n > p . Find  where the minimum is over all linear predictors b and the maximum is over all densities f satisfying R0; : : : ; Rp . Solution: Mean squared error.  8. Maximum entropy characteristic functions.  We ask for the maximum entropy density f  x ; 0  cid:20  x  cid:20  a; satisfying a constraint on the characteristic function  cid:9  u  = R a 0 eiuxf  x dx . The answers need be given only in parametric form.  a  Find the maximum entropy f satisfying R a  b  Find the maximum entropy f satisfying R a  c  Find the maximum entropy density f  x ; 0  cid:20  x  cid:20  a; having a given value of the  0 f  x  cos u0x  dx =  cid:11  , at a speci cid:12 ed  0 f  x  sin u0x dx =  cid:12  .  point u0 .  characteristic function  cid:9  u0  at a speci cid:12 ed point u0 .   d  What problem is encountered if a = 1 ? Solution: Maximum entropy characteristic functions.  9. Maximum entropy processes.   a  Find the maximum entropy rate binary stochastic process fXig1i= cid:0 1; Xi 2  f0; 1g , satisfying PrfXi = Xi+1g = 1  b  What is the resulting entropy rate?  3 ; for all i .  Solution: Maximum entropy processes.   306  Maximum Entropy  10. Maximum entropy of sums Let Y = X1 + X2 Find the maximum entropy density  for Y under the constraint EX 2  1 = P1 , EX 2  2 = P2 ,   a  if X1 and X2 are independent.  b  if X1 and X2 are allowed to be dependent.  c  Prove part  a .  Solution: Maximum entropy of sums  11. Maximum entropy Markov chain.  Let fXig be a stationary Markov chain with Xi 2 f1; 2; 3g . Let I Xn; Xn+2  = 0 for all n .   a  What is the maximum entropy rate process satisfying this constraint?  b  What if I Xn; Xn+2  =  cid:11  , for all n for some given value of  cid:11  , 0  cid:20   cid:11   cid:20  log 3 ? Solution: Maximum entropy Markov chain.  12. An entropy bound on prediction error. Let fXng be an arbitrary real valued stochastic process. Let ^Xn+1 = EfXn+1jX ng . Thus the conditional mean ^Xn+1 is a random variable depending on the n -past X n . Here ^Xn+1 is the minimum mean squared error prediction of Xn+1 given the past.  a  Find a lower bound on the conditional variance EfEf Xn+1  cid:0  ^Xn+1 2jX ngg in  terms of the conditional di cid:11 erential entropy h Xn+1jX n  .   b  Is equality achieved when fXng is a Gaussian stochastic process? Solution: An entropy bound on prediction error.  13. Maximum entropy rate. What is the maximum entropy rate stochastic process fXig over the symbol set f0; 1g for which the probability that 00 occurs in a sequence is zero?  Solution: Maximum entropy rate  14. Maximum entropy.   a  What is the parametric form maximum entropy density f  x  satisfying the two  conditions  EX 8 = a EX 16 = b?   b  What is the maximum entropy density satisfying the condition  E X 8 + X 16  = a + b  ?   c  Which entropy is higher?  Solution: Maximum entropy.   Maximum Entropy  307  15. Maximum entropy. Find the parametric form of the maximum entropy density f  satisfying the Laplace transform condition  Z f  x e cid:0 xdx =  cid:11  ;  and give the constraints on the parameter.  Solution: Maximum entropy.  16. Maximum entropy processes  Consider the set of all stochastic processes with fXig; Xi 2 R; with  R0 = EX 2 R1 = EXiXi+1 = 1 2 :  i = 1  Find the maximum entropy rate.  Solution: Maximum entropy processes  17. Binary maximum entropy  Consider a binary process fXig; Xi 2 f cid:0 1; +1g; with R0 = EX 2 EXiXi+1 = 1 2 :  i = 1 and R1 =   a  Find the maximum entropy process with these constraints.   b  What is the entropy rate?   c  Is there a Bernoulli process satisfying these constraints?  Solution: Binary maximum entropy  18. Maximum entropy.  Maximize h Z; Vx; Vy; Vz  subject to the energy constraint E  1 Show that the resulting distribution yields  2 m k V k2 +mgZ  = E0:  E  1 2  3 5  E0  m k V k2= 2 5  EmgZ =  E0:  Thus 2  5 of the energy is stored in the potential  cid:12 eld, regardless of its strength g .  Solution: Maximum entropy.  19. Maximum entropy discrete processes.   a  Find the maximum entropy rate binary stochastic process fXig1i= cid:0 1; Xi 2 f0; 1g;  satisfying PrfXi = Xi+1g = 1  3 , for all i .   308  Maximum Entropy   b  What is the resulting entropy rate?  Solution: Maximum entropy discrete processes.  20. Maximum entropy of sums.  Let Y = X1 + X2: Find the maximum entropy of Y under the constraint EX 2 P1; EX 2  2 = P2;  1 =   a  Find the maximum entropy rate stochastic process fXig with EX 2   cid:11 ; i = 1; 2; : : : . Be careful.  i = 1; EXiXi+2 =   a  Find the minimum value of EX over all probability density functions f  x  satis-   a  if X1 and X2 are independent.  b  if X1 and X2 are allowed to be dependent.  Solution: Maximum entropy of sums.  21. Entropy rate   b  What is the maximum entropy rate?   c  What is EXiXi+1 for this process?  Solution: Entropy rate  22. Minimum expected value  fying the following three constraints:  for x  cid:20  0; f  x dx = 1; and   i  f  x  = 0   ii  R 1   cid:0 1   iii  h f   = h:   b  Solve the same problem if  i  is replaced by   i 0   f  x  = 0  for x  cid:20  a:  Solution: Minimum expected value   Chapter 13  Universal Source Coding  1. Minimax regret data compression and channel capacity. First consider uni- versal data compression with respect to two source distributions. Let the alphabet V = f1; e; 0g and let p1 v  put mass 1 cid:0   cid:11  on v = 1 and mass  cid:11  on v = e . Let p2 v  put mass 1  cid:0   cid:11  on 0 and mass  cid:11  on v = e . We assign word lengths to V according to l v  = log 1 p v  , the ideal codeword length with respect to a cleverly chosen probability mass function p v  . The worst case excess description length  above the entropy of the true distribution  is  max  i  cid:18 Epi log  1 p V    cid:0  Epi log  1  pi V   cid:19  = max  i  D pi k p :   13.1   Thus the minimax regret is R cid:3  = minp maxi D pi k p  .  a  Find R cid:3  .  b  Find the p v  achieving R cid:3  .  c  Compare R cid:3  to the capacity of the binary erasure channel  " 1  cid:0   cid:11   cid:11   0  0   cid:11  1  cid:0   cid:11    and comment.  Solution: Minimax regret data compression and channel capacity.  2. Universal data compression. Consider three possible source distributions on X ;  Pa =  :7; :2; :1 ; Pb =  :1; :7; :2 ; Pc =  :2; :1; :7 :  and  309   310  Universal Source Coding   a  Find the minimum incremental cost of compression  R cid:3  = min  max  P   cid:18   D P cid:18 kP  ;  and the associated mass function P =  p1; p2; p3 ; and ideal codeword lengths li = log 1=pi :   b  What is the channel capacity of a channel matrix with rows Pa; Pb; Pc ?  Solution: Universal data compression.  3. Arithmetic coding: Let [Xi] be a stationary binary Markov chain with transition  matrix  pij =" 3  4 1 4  1 4 3  4    13.2   Calculate the  cid:12 rst 3 bits of F  X1  = 0:F1F2 : : : when X1 = 1010111 : : : . How many bits of X1 does this specify? Solution: Arithmetic coding  2 3  1 3  4. Arithmetic coding. Let Xi be binary stationary Markov with transition matrix  3 5 :  2 4  a  Find F  01110  = Prf:X1X2X3X4X5 < :01110g:  b  How many bits :F1F2 : : : can be known for sure if it is not known how X = 01110  1 3  2 3  continues?  Solution: Arithmetic coding.  5. Lempel-Ziv. Give the LZ78 parsing and encoding of 00000011010100000110101.  Solution: Lempel-Ziv. We  cid:12 rst parse the string, looking for strings that we have not seen before. Thus, the parsing yields 0,00,000,1,10,101,0000,01,1010,1. There are 10 phrases, and therefore we need 4 bits to represent the pointer to the pre cid:12 x. Thus, using the scheme described in the text, we encode the string as  0000,0 , 0001,0 ,  0010,0 ,  0000,1 ,  0100,0 ,  0101,1 ,  0011,0 ,  0001,1 ,  0110,0 , 0000,1 .  The last phrase, though it is not really a new phrase, is handled like a new phrase .  6. Lempel Ziv 78  We are given the constant sequence xn = 11111 : : :   a  Give the LZ78 parsing for this sequence.  as n ! 1 .  Solution: Lempel Ziv 78   b  Argue that the number of encoding bits per symbol for this sequence goes to zero   Universal Source Coding  311  7. Another idealized version of Lempel-Ziv coding. An idealized version of LZ was shown to be optimal: The encoder and decoder both have available to them the \in cid:12 nite past" generated by the process, : : : ; X cid:0 1; X0 , and the encoder describes the string  X1; X2; : : : ; Xn  by telling the decoder the position Rn in the past of the  cid:12 rst recurrence of that string. This takes roughly log Rn + 2 log log Rn bits.  Now consider the following variant: Instead of describing Rn , the encoder describes Rn cid:0 1 plus the last symbol Xn . From these two the decoder can reconstruct the string  X1; X2; : : : ; Xn  .   a  What is the number of bits per symbol used in this case to encode  X1; X2; : : : ; Xn  ?   b  Modify the proof given in the text to show that this version is also asymptotically optimal, namely that the expected number of bits-per-symbol converges to the entropy rate.  Solution: Another idealized version of Lempel-Ziv coding.  In this version of LZ coding, the encoder and decoder both have available to them the in cid:12 nite past generated by the process, : : : ; X cid:0 1; X0 , and the encoder describes the string X n 1 =  X1; X2; : : : ; Xn  by telling the decoder the position Rn cid:0 1 in the past of the  cid:12 rst recurrence of the string X n cid:0 1  , plus the last symbol Xn .  1   a  Let A be the alphabet of the process, and jAj denote its size. Then the number of bits it takes to represent Rn cid:0 1 is roughly log Rn cid:0 1 + C log log Rn cid:0 1 , where C is a constant independent of n . To represent Xn , it takes dlog jAje bits, so the overall number of bits per symbol used for the whole string X n  1 is  Ln X n 1    n  log Rn cid:0 1 + C log log Rn cid:0 1 + dlog jAje  :  =  n   b  To prove that this description is asymptotically optimal it su cid:14 ces to show that  and the optimality will follow since we know that the reverse inequality also holds, by Shannon’s Noiseless Coding Theorem. For the last term in Ln it is immediate that  lim sup n!1  E cid:18  Ln  n  cid:19   cid:20  H;  dlog jAje  n ! 0;   13.3    13.4   as n ! 1 . Now notice that we always have Rn cid:0 1  cid:20  Rn , so for the  cid:12 rst two terms in Ln ,  E cid:18  log Rn cid:0 1 + C log log Rn cid:0 1  n   cid:19   cid:20  E cid:18  log Rn + C log log Rn  n   cid:19  ;   13.5    312  Universal Source Coding  and as was shown in class, the above right-hand-side is asymptotically bounded above by H ,  lim sup n!1  E cid:18  log Rn + C log log Rn  n   cid:19   cid:20  H:   13.6   Combining  13.4  with  13.5  and  13.6 , yields  13.3 , as claimed.  8. Length of pointers in LZ77. In the version of LZ77 due of the Storer and Szymanski[15],  described in Section 13.4.1, a short match can either be represented by  F; P; L    cid:13 ag, pointer, length  or by  F; C    cid:13 ag, character . Assume that the window length is W , and assume that the maximum match length is M .   a  How many bits are required to represent P ? To represent L ?   b  Assume that C , the representation of a character is 8 bits long. If the represen- tation of P plus L is longer than 8 bits, it would be better to represent a single character match as an uncompressed character rather than as a match within the dictionary. As a function of W and M , what is the shortest match that one should represent as a match rather than as uncompressed characters?   c  Let W = 4096 and M = 256 . What is the shortest match that one would  represent as a match rather than uncompressed characters?  Solution: Length of pointers in LZ77   a  Since P represents the position within the window, dlog We bits would su cid:14 ce to represent P . Since L represents the length of the match, which is at most M , dlog Me bits su cid:14 ce for L .   b  Ignoring the integer constraints, we can see that we would use the uncompressed character to represent a match of length 1 if log W + log M > 8 . Similarly, we would use single characters rather than the match representation for matches of length m if 1 + log W + log M > m 1 + 8  , since the representation as a sequence of single characters needs 9 bits per character.   c  If M = 256 , log M = 8 , W = 4096 ,  log W = 14 , and 1 + log W + log M = 23 and we would use the uncompressed representation if m = 1 or 2 . For m  cid:21  3 , the match representation is shorter.   a  Continue the Lempel-Ziv parsing of the sequence 0,00,001,00000011010111.   b  Give a sequence for which the number of phrases in the LZ parsing grows as fast   c  Give a sequence for which the number of phrases in the LZ parsing grows as slowly  9. Lempel-Ziv.  as possible.  as possible.  Solution: Lempel-Ziv.   Universal Source Coding  313   a  The Lempel-Ziv parsing is: 0; 00; 001; 000; 0001; 1; 01; 011; 1   b  The sequence is: 0; 1; 00; 01; 10; 11; 000; 001; : : : concatenating all binary strings of length 1,2,3, etc. This is the sequence where the phrases are as short as possible.   c  Clearly the constant sequence will do: 1; 11; 111; 1111; : : :  10. Two versions of  cid:12 xed-database Lempel-Ziv. Consider a source  A; P   . For simplicity assume that the alphabet is  cid:12 nite jAj = A < 1 , and the symbols are i.i.d.  cid:24  P . A  cid:12 xed database D is given, and is revealed to the decoder. The encoder parses the target sequence xn 1 into blocks of length l , and subsequently encodes them by giving the binary description of their last appearance in the database. If a match is not found, the entire block is sent uncompressed, requiring l log A bits. A  cid:13 ag is used to tell the decoder whether a match location is being described, or the sequence itself. Problems  a  and  b  give some preliminaries you will need in showing the optimality of  cid:12 xed-database LZ in  c .   a  Let xl be a  cid:14  -typical sequence of length l starting at 0, and let Rl xl  be the  corresponding recurrence index in the in cid:12 nite past : : : ; X cid:0 2; X cid:0 1 . Show that  EhRl X l jX l = xli  cid:20  2l H+ cid:14    where H is the entropy rate of the source.   b  Prove that for any  cid:15  > 0 , Pr cid:16 Rl X l  > 2l H+ cid:15   cid:17  ! 0 as l ! 1 .  Hint: Expand the probability by conditioning on strings xl , and break things up into typical and non-typical. Markov’s inequality and the AEP should prove handy as well.   c  Consider the following two  cid:12 xed databases  i  D1 is formed by taking all  cid:14  -typical l -vectors; and  ii  D2 formed by taking the most recent ~L = 2l H+ cid:14   symbols in the in cid:12 nite past  i.e., X  cid:0  ~L; : : : ; X cid:0 1  . Argue that the algorithm described above is asymptotically optimal, namely that the expected number of bits-per-symbol converges to the entropy rate, when used in conjunction with either database D1 or D2 .  Solution: Two versions of  cid:12 xed-database Lempel-Ziv   a  Since xl  is  cid:14  -typical, the AEP implies that p xl   cid:21  2 cid:0 l H+ cid:14   , and the result  follows from Kac’s lemma.  of sequences into the typical sequences and the non-typical sequences.   cid:14  be the  cid:14  -typical set for Al . We divide the set   b  Fix  cid:15  > 0 , and  cid:14  2  0;  cid:15   . Let A l  Pr Rl X l  > 2l H+ cid:15    = Xxl = Xxl2A l    cid:14   p xl  Pr Rl X l  > 2l H+ cid:15  jxl   p xl  Pr Rl X l  > 2l H+ cid:15  jxl    314  Universal Source Coding  p xl  Pr Rl X l  > 2l H+ cid:15  jxl   + Xxl =2A l  2l H+ cid:14   2l H+ cid:15   + Pr X l =2 A l   cid:14      cid:14    i   p xl    cid:20  Xxl2A l   cid:20  2 cid:0 l  cid:15  cid:0  cid:14   + Pr X l =2 A l   cid:14      cid:14   where  i  follows from Markov’s inequality and using the result of part  a . The proof now follows from the AEP, by sending l to in cid:12 nity.   c  For D1 the proof follows trivially from the analysis in x3.2 in Cover and Thomas. For D2 , let N = n=l be the number of blocks in the sequence, and let L Bi  denote the length of the encoding of the i -th block Bi . To simplify the notation, assume that N = n=l is an integer. We call a block ‘good’ if we can  cid:12 nd a match in D2 , and ‘bad’ otherwise. Let G be the set of good blocks. If Bi 2 G , we e ncode it using log jD2j bits, which by our choice of D2 is equal to H l +  cid:14   bits. If Bi =2 G then we encode it using l log A bits. We throw in one extra bit to distinguish between the two events. Then,  1 n  EL X1; X2; : : : ; Xn  =  L Bi   1 n  1 n  EXi EXi2G   1 + l H +  cid:14    +   1 + l log A   1 n  EXi =2G + PrfX l =2 D2g   1 l  + log A    1 + l H +  cid:14     l  =   i    cid:20   where step  i  follows from taking the  cid:12 rst summation over all N blocks, and  using N cid:0 1EPi =2G 1 = PrfGcg . Take ln to be a sequence of integers such that ln " 1 as n ! 1 . It now follows from part  b  that PrfX l =2 D2g ! 0 and thus, lim supn n cid:0 1ELn  cid:20  H + cid:14  and since  cid:14  is arbitrary, we have lim supn n cid:0 1ELn  cid:20  H . The proof is now complete since lim inf n n cid:0 1ELn  cid:21  H , by Shannon’s source coding theorem.  11. Tunstall Coding: The normal setting for source coding maps a symbol  or a block of symbols  from a  cid:12 nite alphabet onto a variable length string. An example of such a code is the Hu cid:11 man code, which is the optimal  minimal expected length  mapping from a set of symbols to a pre cid:12 x free set of codewords. Now consider the dual problem of variable- to- cid:12 xed length codes, where we map a variable length sequence of source symbols into a  cid:12 xed length binary  or D -ary  representation. A variable-to- cid:12 xed length code for an i.i.d. sequence of random variables X1; X2; : : : ; Xn; Xi  cid:24  p x ; x 2 X = f0; 1; : : : ; m cid:0 1g is de cid:12 ned by a pre cid:12 x-free set of phrases AD  cid:26  X  cid:3  , where X  cid:3  is the set of  cid:12 nite length strings of symbols of X , and jADj = D . Given any sequence X1; X2; : : : ; Xn , the string is parsed into phrases from AD  unique because of the pre cid:12 x free property of AD  , and represented by a sequence of symbols from a D -ary alphabet. De cid:12 ne the   Universal Source Coding  e cid:14 ciency of this coding scheme by  R AD  =  log D  EL AD   315   13.7   where EL AD  is the expected length of a phrase from AD .   a  Prove that R AD   cid:21  H X  .  b  The process of constructing AD can be considered as a process of constructing an m -ary tree whose leaves are the phrases in AD . Assume that D = 1 + k m  cid:0  1  for some integer k  cid:21  1 . Consider the following algorithm due to Tunstall: i. Start with A = f0; 1; : : : ; m  cid:0  1g with probabilities p0; p1; : : : ; pm cid:0 1 . This  corresponds to a complete m -ary tree of depth 1.  ii. Expand the node with the highest probability. For example, if p0 is the node with the highest probability, the new set is A = f00; 01; : : : ; 0 m  cid:0  1 ; 1; : : : ;  m  cid:0  1 g .  iii. Repeat step 2 until the number of leaves  number of phrases  reaches the  required value.  Show that the Tunstall algorithm is optimal, in the sense that it constructs a variable to  cid:12 xed code with the best R AD  for a given D , i.e., the largest value of EL AD  for a given D .   c  Show that there exists a D such that R A cid:3 D  < H X  + 1 .  Solution: Tunstall Coding:   a  We will argue that if R AD  < H X  , then it is possible to construct an uniquely decodable code with average length less than the entropy. Consider a long sequence of i.i.d. random variables X1; X2; : : : ; Xn  cid:24  p . We can parse this sequence into phrases using the pre cid:12 x-free set AD , and these phrases are independent and iden- tically distributed, with the distribution induced by p on the tree. Thus, using re- newal theory, since the expected phrase length is EL AD  , the number of phrases in the block of length n is approxn=EL AD  . These phrases can be described with log D bits each, so that the total description length is  cid:25  log D n=EL AD   . If R AD  < H , then the total description length is less than nH and we have a contradiction to the fundamental theorem of source coding. However, making the above argument precise raises issues for which we have two di cid:11 erent solutions:   cid:15  The algorithm above does not describe how to handle a sequence of random variables that is a pre cid:12 x of an element of AD . For example, after parsing a block of length n into phrases from AD , we might be left with a few symbols of X that are not long enough to make a phrase. We can imagine that these symbols are sent uncompressed, and the overhead is small  the set AD is  cid:12 nite, and so the maximal length of a phrase in AD is  cid:12 nite, and so the maximum length of the residue is bounded .   316  Universal Source Coding   cid:15  We could extend the fundamental theorem of source coding directly to the variable to variable case, i.e., we can prove that for any mapping F : X  cid:3  ! f0; 1g cid:3  , that EL AD H X   cid:20  EL C  , where EL C  is the average length of the binary codewords induced by the mapping. An example of such a mapping for X = fa; b; cg is the mapping aa ! 000; ab ! 001; ac ! 010; b ! 011; c ! 1 . It is easy to see that the above result is a \special" case of such a mapping, where we assume that we can  cid:12 nd a code length of log D for all the elements of AD . Let Y be the random variable whose values are the elements of AD , and whose distribution is induced by the distribution of X , i.e., if the phrase is ab , the probability Pr Y =0 ab0  = papb . It is easy to see that Y is well de cid:12 ned random variable with total probability 1. Now the code C is a code for the random variable Y , and by the standard source coding theorem, we have EL C   cid:21  H Y   . We will now prove that H Y   = EL AD H X  , which will complete the proof. There are many ways to prove this result, which is the Wald equation for entropies. We will prove it directly, using a summation over the leaves of a tree. Let L X1; X2; : : : ; Xn  be the stopping de cid:12 ned by set AD , i.e., L X1; X2; : : : ; Xn  = l if the sequence of length l at the beginning of X is in AD . Choose n larger than the maximal length in AD , and let Y = X L 1 be the  cid:12 rst phrase in the parsing of X1; X2; : : : ; Xn . Then  nH X  = H X1; X2; : : : ; Xn   = H X L = H X L  1 ; L; X n L+1  1   + H LjX L 1 , H LjX L  1   + H X n  L+1jL; X L 1    1   = 0 . Also, X n  L+1 is indpendent of  Now since L is  cid:12 xed given X L X L 1 given L , and we can write  H X n  L+1jL  =  Pr L = l H X n  l+1   n  n  Xl=1 Xl=1  =  Pr L = l  n  cid:0  l H X   =  n  cid:0  EL H X   Substituting this in the equation above, we get  nH X  = H X L  1   +  n  cid:0  EL H X   or  H Y   = H X L   13.15  To prove the required result of part  a , we only need to verify that H Y    cid:20  log D , which follows directly from the fact that the range of Y is limited to D values.  1   = ELH X    13.8    13.9    13.10    13.11    13.12    13.13    13.14    Universal Source Coding  317   b  We will prove the optimality of the Tunstall algorithm by induction in a fashion similar to the proof of Hu cid:11 man coding optimality. By the statement of the prob- lem, we restrict our attention to complete trees, i.e., trees for which every node is either a leaf  no children  or has m children. Clearly, the algorithm to minimize R AD  for a given D has to  cid:12 nd the set AD that maximizes EL AD  . We will need some notation for the analysis that follows: nodes in the tree are either leaf nodes  nodes that have no children  or internal nodes  nodes that have m children . The probability of a node is the product of the probability of the symbols that led up to the node. The probability of the root node is 1. We will assume that the algorithm constructs a tree that is optimal for Dk = 1 + k m  cid:0  1  . We will show that the algorithm than produces a tree that is optimal for Dk+1 = 1 +  k + 1  m  cid:0  1  . Any tree with Dk+1 nodes consists of tree with Dk nodes with one of the nodes expanded. Let Tk denote a tree with Dk nodes,  cid:27  denote a leaf of this tree with probability p cid:27  , and let Tk+1 denote the tree with Dk+1 nodes formed by expanding the node  cid:27  . Let N  T   denote the leaf nodes in T . Then  p i l i    13.16   m  Xj=1  p i l i  +  p  cid:27  p j  l  cid:27   + 1    13.17   p i l i  + p  cid:27   l  cid:27   + 1    13.18   EL Tk+1  = Xi2N  Tk+1  = Xi2N  Tk ;i6= cid:27  = Xi2N  Tk ;i6= cid:27  = Xi2N  Tk   p i l i  + p  cid:27    = EL Tk  + p  cid:27     13.19    13.20   Thus the expected length for any expanded tree is equal the expected length of the original tree plus the probability of the node that was expanded. This result provides the basic intuition that motivates the algorithm: to maximize EL Tk+1  given Tk , we should expand the node with the largest probability. Doing this repeatedly gives us the Tunstall algorithm. However, using this to prove the optimality of the Tunstall algorithm is surprisingly tricky. This is because there a di cid:11 erent sequences of node expansions that give rise to the same  cid:12 nal tree. Also, a suboptimal tree of size Dk might have a larger value of p  cid:27    the steps are not independent, and hence a greedy step early on might not be optimal later  , and thus we cannot directly use the above result for induction. Instead, we will use another property of the optimal tree constructed by the Tun- stall algorithm, that is, the probability of each of the internal nodes is higher than the probability of the leaves. We have the following statement: Lemma: Any optimal tree Tk+1  a tree maximizing EL   has the property that the probability of any of the internal nodes is greater than or equal to the probability   318  Universal Source Coding  of any of the leaves, i.e., if  cid:6 I is the set of internal nodes and  cid:6 L is the set of leaf nodes, then  8 cid:27  2  cid:6 I;8 cid:27 0 2  cid:6 L;  p  cid:27    cid:21  p  cid:27 0    13.21   If this were not true, then there exists an internal node with a lower Proof: probability than a leaf. Let  cid:27  be this internal node,  cid:27 l be the leaf, and since the probability of any descendant node is less than that of the parent, we can work down from  cid:27  until we  cid:12 nd an internal node  cid:27 0 just above the leaves that also satis cid:12 es this property, i.e., p  cid:27 0  < p  cid:27 l  . Now consider the tree Tk formed by deleting all the leaves coming out  cid:27 0 , and form a tree T 0k+1 by expanding node  cid:27 l in Tk . We have  EL Tk+1  = EL Tk  + p  cid:27 0  EL T 0k+1  = EL Tk  + p  cid:27 l    13.22    13.23   and since p  cid:27 0  < p  cid:27 l  , we have EL Tk+1  < EL T 0k+1  , contradicting the opti- mality of Tk+1 . Thus all optimal trees satisfy the propery above. We now prove the converse, i.e., that any tree that satis cid:12 es this property must be optimal. Again, we prove it by contradiction. Assume that there is a tree Tk satisfying this property that is not optimal, and therefore there is another tree T  cid:3 k which is optimal, i.e.,having larger expected length. By the previous result, this tree also satis cid:12 es  13.21 . Now consider the set of nodes that occur in T or T  cid:3  . These nodes can be classi cid:12 ed into 8 categories.  cid:15  S1 : nodes that are internal nodes in both T and T  cid:3  .  cid:15  S2 : nodes that are leaf nodes in both T and T  cid:3  .  cid:15  S3 : nodes that are internal nodes in T and leaf nodes in T  cid:3  .  cid:15  S4 : nodes that leaf nodes in T and internal nodes in T  cid:3  .  cid:15  S5 : nodes that are internal nodes in T that are not in T  cid:3   cid:15  S6 : nodes that are internal nodes in T  cid:3  that are not in T  cid:15  S7 : nodes that are leaf nodes in T that are not in T  cid:3   cid:15  S8 : nodes that are leaf nodes in T  cid:3  that are not in T By assumption, T 6= T  cid:3  , and therefore there are leaf nodes in T that are not in T  cid:3  . Some ancestor of this leaf node  cid:27  in T must be a leaf node of T  cid:3  , and therefore S3 is not empty. Similarly, if T 6= T  cid:3  , S4 must be non-empty. We now argue that all nodes in S3 and S4 have the same probability. Let  cid:27 3 2 S3 and  cid:27 4 2 S4 be two nodes in the two sets. By property  13.21  for T  cid:3  , it follows that p  cid:27 3   cid:20  p  cid:27 4  . By property  13.21  for T , we have p  cid:27 4   cid:20  p  cid:27 3  . Thus p  cid:27 3  = p  cid:27 4  . We now argue that S5 and S6 are empty sets. This follows from the fact that since any node in S5 has to be a descendant of a node in S3 , and hence p  cid:27 5  < p  cid:27 3  . But by the property  13.21  for T , p  cid:27 5   cid:21  p  cid:27 4  , and since p  cid:27 4  = p  cid:27 3  , we have a contradiction. Thus there can be no nodes in S5 or S6 .   Universal Source Coding  319  Thus the nodes in S7 are the children of nodes in S3 and the nodes in S8 are the children of nodes in S4 . Since T and T  cid:3  are equal except for these nodes in S7 and S8 and the average length of the trees depends only the probability of the internal nodes, it follows that T and T  cid:3  have the same average length. This  cid:12 nally proves the key result, which is that a tree is optimal if and only if it satis cid:12 es property  13.21 . It is now simple to show by induction that the Tunstall algorithm constructs a tree that satis cid:12 es  13.21 . Initially, the trivial tree of depth 1 satis cid:12 es  13.21 . Also, if we start with a tree that satis cid:12 es  13.21 , and expand the leaf with the highest probability, we still satisfy  13.21 , since the new internal node has a probability that is at least as high as any other leaf, and the new leaves have a lower probability that the original leaf node that was expanded to form the new internal node. Thus the new tree also satis cid:12 es  13.21 , and by induction, the tree constructed by the Tunstall algorithm satis cid:12 es  13.21 . Combining this with the previous result, we see that the tree constructed by the Tunstall algorithm has maximal average length, and therefore minimizes R AD  .   c  We will use the familiar Hu cid:11 man coding procedure and \invert" it to construct the variable to  cid:12 xed code which achieves a compression ratio within one bit of the entropy. First, we take a blocks of length 2 for the random variable, ie. X1X2 2 X 2 and construct an Hu cid:11 man code for this pair. By the standard results for Hu cid:11 man codes, we have  2H < EL2 < 2H + 1   13.24   Let lm be the maximal length of any of the Hu cid:11 man codewords. Now consider the set of binary sequences of length n , n >> lm . Parse each binary sequence to codewords from Hu cid:11 man code, and replace the codewords by the corresponding pair of symbols of X . This de cid:12 nes a set of sequences of X , which we will like to use to construct AD . This set of sequences might not correspond to a complete tree for X . We therefore add to this set by adding the X sequences that correspond to siblings of the X sequences already chosen. This augmented set will be the AD that we will use in our analysis. We now show that for an appropriate choice of n large enough, this choice of AD achieves an compression rate less than H + 1 . The number of elements in AD : It is not di cid:14 cult to see the code for any sequence in AD is less than n + lm , and thus the number of sequences in AD is less than 2n+lm . The average length of the sequences in AD : Using renewal theory, it follows that the expected number of Hu cid:11 man codewords in the parsing of a binary sequence of length n converges to n=L2 . Thus the average length of the X sequences corresponding to the parsed binary sequences converges to 2n=L2 , since each Hu cid:11 man codeword corresponds to a block of two symbols of X . The fact that we have added sequences to this set to form AD does not change   320  Universal Source Coding  this result, and we can prove this by considering the parsing of binary sequences of length n  cid:0  lm and n + lm . The details of this analysis are omitted. Thus we have the expected length of sequences of AD converges to 2n=L2 as n ! 1 . Thus the compression ratio,  R AD  =  log D  EL AD    13.25   is upper bounded by  n + lm = 2n=L2 +  cid:15   for n large enough. This converges to H + 1=2 as n ! 1 , and thus there exists an n such that we can achieve a compression ratio less than H + 1 . This proves the required result.   Chapter 14  Kolmogorov Complexity  1. Kolmogorov complexity of two sequences. Let x; y 2 f0; 1g cid:3 : Argue that K x; y   cid:20   K x  + K y  + c:  Solution: Suppose we are given two sequences, x and y , with Kolmogorov complexity K x  and K y  respectively. Then there must exist programs px and py , of length K x  and K y  respectively, which print out x and y . Form the following program:  Run the following two programs, not halting after the first;  Run the program px , interpreting the halt as a jump to the next step;  Run the program py .  This program, of length K x  + K y  + c , prints out x; y . Hence  K x; y   cid:20  K x  + K y  + c:   14.1   2. Complexity of the sum.   a  Argue that K n   cid:20  log n + 2 log log n + c:  b  Argue that K n1 + n2   cid:20  K n1  + K n2  + c:  c  Give an example in which n1 and n2 are complex but the sum is relatively simple.  Solution:   a  To describe an integer n , we will tell the computer the length of n , and then tell it the bits of n . Thus the program will be self delimiting. To represent the length of n , i.e., log n , we could use the simple code described in class: repeat each bit of log n twice, and end the description by 10. This representation requires 2 log log n + 2 bits. It requires log n bits to represent the bits of n , and hence the total length of the program is log n + 2 log log n + c , which is an upper bound on the complexity of n:  K n   cid:20  log n + 2 log log n + c:  321   14.2    322  Kolmogorov Complexity   b  Given two programs to print out n1 and n2 , we can modify them so that they write on the work tape, rather than the output tape. Then we can add an instruction to add the two numbers together and print them out. The length of this program is K n1  + K n2  + c , and hence  3. Images. Consider an n  cid:2  n array x of 0’s and 1’s . Thus x has n2 bits.  K n1 + n2   cid:20  K n1  + K n2  + c:   14.3   Find the Kolmogorov complexity K x j n   to  cid:12 rst order  if  a  x is a horizontal line.   c  x is the union of two lines, each line being vertical or horizontal.   b  x is a square.  Solution:   a  The program to print out an image of one horizontal line is of the form  For 1  cid:20  i  cid:20  n f Set pixels on row i to 0; g  Set pixels on row r to 1;  Print out image.  Since the computer already knows n , the length of this program is K rjn  + c , which is  cid:20  log n + c . Hence, the Kolmogorov complexity of a line image is  K linejn   cid:20  log n + c:   b  For a square, we have to tell the program the coordinates of the top left corner, and the length of the side of the square. This requires no more than 3 log n bits, and hence  K squarejn   cid:20  3 log n + c:  However, we can save some description length by  cid:12 rst describing the length of the side of the square and then the coordinates. Knowing the length of the side of the square reduces the range of possible values of the coordinates. Even better, we can count the total number of such squares. There is one n  cid:2  n square, four  n  cid:0  1   cid:2   n  cid:0  1  squares, nine  n  cid:0  2   cid:2   n  cid:0  2  squares, etc. The total number of squares is  12 + 22 + 32 +  cid:1  cid:1  cid:1  + n2 =  6  n n + 1  2n + 1   n3 3  :   cid:25    14.6    14.4    14.5    323   14.7    14.8   Kolmogorov Complexity  Since we can give the index of a square in a lexicographic ordering,   c  In this case, we have to tell the program the position of the horizontal line and  the position of the vertical line, requiring no more than 2 log n bits. Hence  K squarejn   cid:20  log  n3 3  + c:  K pair of linesjn   cid:20  2 log n + c:  In all the above cases, there are many images which are much simpler to describe. For example, in the case of the horizontal line image, the image of the  cid:12 rst line or the middle line is much easier to describe. However most of the images have description lengths close to the bounds derived above.  4. Do computers reduce entropy? Feed a random program P into an universal computer. What is the entropy of the corresponding output? Speci cid:12 cally, let X = U P   , where P is a Bernoulli 1 2  sequence. Here the binary sequence X is either unde cid:12 ned or is in f0; 1g cid:3  . Let H X  be the Shannon entropy of X . Argue that H X  = 1 . Thus although the computer turns nonsense into sense, the output entropy is still in cid:12 nite.  Solution: Do computers reduce entropy? The output probability distribution on strings x is PU  x  , the universal probability of the string x . Thus, by the arguments following equation  7.65 , the output distribution includes a mixture of all computable probability distributions.  Consider the following distribution on binary  cid:12 nite length stings:  1  0  An log2 n  if x = 111 : : : 1  P1 x  =8>< >: is chosen to ensure that Px P1 x  = 1 . Then P1 x  is a  otherwise   14.9   {z  n 1’s    }  0  computable probability distribution, and by problem 9 in Chapter 2, P1 x  has an in cid:12 nite entropy.  where A = P1n=1  1  n log2 n  By  7.65  in the text,  for some constant c1 that does not depend on x . Let  PU  x   cid:21  c1P1 x   P2 c  =  PU  x   cid:0  c1P1 x   :  1  cid:0  c1  It is easy to see that Px P2 x  = 1 , and therefore P2 x  is a probability distribution.  Also,  PU  x  = c1P1 x  +  1  cid:0  c1 P2 x :   14.10    14.11    14.12    324  Kolmogorov Complexity  By the results of Chapter 2,  cid:0 t log t is a concave function of t and therefore   cid:0 PU  x  log PU  x   cid:21   cid:0 c1P1 x  log P1 x   cid:0   1  cid:0  c1 P2 x  log P2 x    14.13   Summing this over all x , we obtain  H PU    cid:21  c1H P1  +  1  cid:0  c1 H P2  = 1   14.14   Thus the entropy at the output of a universal computer fed in Bernoulli 1 2  sequences is in cid:12 nite.  5. Monkeys on a computer. Suppose a random program is typed into a computer. Give  a rough estimate of the probability that the computer prints the following sequence:   a  0n followed by any arbitrary sequence.  b   cid:25 1 cid:25 2 : : :  cid:25 n followed by any arbitrary sequence, where  cid:25 i  is the i -th bit in the  expansion of  cid:25 :   c  0n1 followed by any arbitrary sequence.  d  !1!2 : : : !n followed by any arbitrary sequence.  e  A proof of the four color theorem.  Solution: The probability that a computer with a random input will print will print out the string x followed by any arbitrary sequence is the sum of the probabilities over all sequences starting with the string x .  pU  x : : :  =  Xy2f0;1g cid:3 [f0;1g1  pU  xy ; where pU  x  = Xp:U  p =x  2 cid:0 ‘ p :   14.15   This sum is lower bounded by the largest term, which corresponds to the simplest concatenated sequence.   a  The simplest program to print a sequence that starts with n 0’s is  This program has constant length c and hence the probability of strings starting with n zeroes is   b  Just as in part  a , there is a short program to print the bits of  cid:25  forever. Hence   c  A program to print out n 0’s followed by a 1 must in general specify n . Since most integers n have a complexity  cid:25  log cid:3  n , and given n , the program to print out 0n1 is simple, we have  Print 0’s forever.  pU  0n : : :   cid:25  2 cid:0 c:  pU   cid:25 1 cid:25 2 : : :  cid:25 n : : :   cid:25  2 cid:0 c:  pU  0n1 : : :   cid:25  2 cid:0  log cid:3  n cid:0 c;   14.16    14.17    14.18    Kolmogorov Complexity  325   d  We know that n bits of  cid:10  are essentially incompressible, i.e., their complexity  cid:21  n cid:0 c . Hence, the shortest program to print out n bits of  cid:10  followed by anything must have a length at least n  cid:0  c , and hence  pU  !1!2 : : : !n : : :   cid:25  2 cid:0  n cid:0 c :   14.19   6. Kolmogorov complexity and ternary programs.  Suppose that the input programs for a universal computer U are sequences in f0; 1; 2g cid:3   ternary inputs . Also, suppose U prints ternary outputs. Let K xjl x   = minU  p;l x  =x l p : Show that   a  K xnjn   cid:20  n + c:  b  jxn 2 f0; 1g cid:3  : K xnjn  < kj < 3k: Solution: Kolmogorov Complexity and Ternary Programs.   a  It is always possible to include a ternary representation of the string to be printed out in the program. This program has a length of n + c ternary digits, and therefore K xnjn   cid:20  n + c:   b  There are less than 3k ternary programs of length less than k and each of these programs can produce at most one output string and therefore the number of strings with Kolmogorov complexity less than k has to be less than 3k .  7. A law of large numbers. Using ternary inputs and outputs as in Problem 6, outline an argument demonstrating that if a sequence x is algorithmically random, i.e., if K xjl x    cid:25  l x ; then the proportion of 0’s, 1’s, and 2’s in x must each be near 1=3 . It may be helpful to use Stirling’s approximation n!  cid:25   n=e n: Solution: A Law of Large Numbers. The arguments parallel the arguments in the binary case in Theorem 7.5.2. We will only outline the main argument. Let  cid:18 0;  cid:18 1;  cid:18 2 be the proportions of 0’s, 1’s, and 2’s in the string xn . We can construct a two stage description of xn by  cid:12 rst describing  cid:18 0;  cid:18 1;  cid:18 2 , and then describing the string within the set of all strings with the same proportions of 0,1 and 2. The two stage description has a length bounded by nH3  cid:18 0;  cid:18 1;  cid:18 2  + 6 log n + c , where H3 denotes entropy to base 3. If K xnjn   cid:25  n , then  n  cid:0  cn  cid:21  K xnjn   cid:21  nH3  cid:18 0;  cid:18 1;  cid:18 2  + 6 log n + c;  and therefore  H3  cid:18 0;  cid:18 1;  cid:18 2   cid:21  1  cid:0   cid:14 n;  where  cid:14 n ! 0 . Thus  cid:18 0;  cid:18 1;  cid:18 2 must lie in a neighborhood of   1 3   . This can be seen by considering the behavior of the entropy functionit is close to 1 only in the neighborhood of the center of the three dimensional simplex. Therefore, the proportion of 0’s, 1’s and 2’s must be close to 1 3 for an incompressible ternary sequence.  3 ; 1  3 ; 1   14.20    14.21    326  8. Image complexity.  Consider two binary subsets A and B  of an n  cid:2  n grid . For example,  Kolmogorov Complexity  Find general upper and lower bounds, in terms of K Ajn  and K Bjn  , for  a  K Acjn :  b  K A [ Bjn :  c  K A \ Bjn : Solution: Image Complexity.   a  We can describe Ac by  cid:12 rst describing A , so  K Acjn  < K Ajn  + c   14.22    14.23    14.24    b  We can describe the union by describing each set separately and taking the union,  hence  K A [ Bjn   cid:20  K Ajn  + K Bjn  + c  c  The intersection can also be described similarly, and hence  K A \ Bjn   cid:20  K Ajn  + K Bjn  + c  9. Random program. Suppose that a random program  symbols i.i.d. uniform over the  symbol set  is fed into the nearest available computer. To our surprise the  cid:12 rst n bits of the binary expansion of 1=p2 are printed out. Roughly what would you say the probability is that the next output bit will agree with the corresponding bit in the expansion of 1=p2 ? Solution: Random program. The arguments parallel the argument in Section 7.10, and we will not repeat them. Thus the probability that the next bit printed out will  be the next bit of the binary expansion of p2 is  cid:25  1 cn+1 .  10. The face-vase illusion.   a  What is an upper bound on the complexity of a pattern on an m  cid:2  m grid that has mirror image symmetry about a vertical axis through the center of the grid and consists of horizontal line segments?   Kolmogorov Complexity  327   b  What is the complexity K if the image di cid:11 ers in one cell from the pattern described  above?  Solution: The face vase illusion.   a  An image with mirror image symmetry has only m2=2 independent pixels. We can describe only one half and ask the computer to construct the other. Therefore the Kolmogorov complexity of the image is less than m2=2 + c .  The fact that the image consists of horizontal line segments will not make a dif- ference unless we are given some further restrictions on the line segments. For example, in the image with the face-vase illusion, each half of any horizontal line consists of only two segments, one black and one white. In this case, we can describe the image by a sequence of boundary points between the black and the white. Thus the image will take m log  m 2   + c bits to describe the m bound- ary points in one half of the picture  the boundary points on the other half can be calculated from this half . Thus the image with the face-vase illusion has a Kolmogorov complexity less than m log m + c .   b  We can describe a picture that di cid:11 ers in one pixel from the image above by  cid:12 rst describing the above image, and then giving the location of the pixel that is di cid:11 erent. Therefore, the Kolmogorov complexity of the new image is less than m log m + 2 log m + c .  11. Kolmogorov complexity  Assume n very large and known. Let all rectangles be parallel to the frame.   a  What is the  maximal  Kolmogorov complexity of the union of two rectangles on  an n  cid:2  n grid?   b  What if the rectangles intersect at a corner?   328  Kolmogorov Complexity   c  What if they have the same  unknown  shape?   d  What if they have the same  unknown  area?   e  What is the minimum Kolmogorov complexity of the union of two rectangles?  That is, what is the simplest union?   f  What is the  maximal  Kolmogorov complexity over all images  not necessarily  rectangles  on an n  cid:2  n grid?  Solution: Kolmogorov complexity Note that K a single point on the screenjn   cid:25  2 log n+ c .   a  To specify two rectangles, we need to describe the coordinates of two corners  X; Y   and either length and width  L; W   or the opposite corner  X 0; Y 0  of the rectangle. Hence we will need to describe 4 numbers, each of which is  cid:20  n , and therefore we need 4 log n + c bits for each rectangle, for a total of 8 log n + c for two rectangles. With the upper-left corner and the lower-right corner, we can describe a rectangle. Hence, for two rectangles, K xjn   cid:25  K 4 pointsjn   cid:25  8 log n + c . We have not used the fact that the length and width of the rectangle are not independent of the position of the lower left corner{for example, if the lower left corner is near the NE corner of the square, the length and width of the rectangle have to be small. This will reduce the number of possible  X; Y; L;   combinations to be  n n + 1 =2 2 rather than n4 , but it does not change the key term.   b  Assuming two rectangles meet at a corner, we need to only describe 3 corners  instead of 4. Hence, K xjn   cid:25  K 3 pointsjn   cid:25  6 log n + c .   c  Assuming two rectangles of the same shape, we need to describe the upper-left and lower right corners of one rectangle and the one corner of the other. Hence, K xjn   cid:25  K 3 pointsjn   cid:25  6 log n + c .   d  If the rectangles have the same area, then describing one rectangle fully and the other rectangle by one corner and one side  the other side can be calculated . Thus K xjn   cid:25  K 3 pointsjn  + K 1 sidejn  = 7 log n + c .   Kolmogorov Complexity  329   e  An image is a speci cid:12 cation for each pixel whether it is black or white. Since there are n2 pixels in the image, 1 bit per pixel, the maximal Kolmogorov complexity is n2 + c bits.  12. Encrypted text  Suppose English text xn is encrypted into yn by a substitution cypher: a 1 to 1 reassignment of each of the 27 letters of the alphabet  A-Z including the space character  to itself. Suppose the Kolmogorov complexity of the text xn is K xn  = n 4 .  This is about right for English text. We’re now assuming a 27-symbol programming language, instead of a binary symbol-set for the programming language. So, the length of the shortest program, using a 27-ary programming language, that prints out a particular string of English text of length n, is approximately n 4.    a  What is the Kolmogorov complexity of the encryption map?  b  Estimate the Kolmogorov complexity of the encrypted text y n .  c  How high must n be before you would expect to be able to decode y n ?  Solution: Encrypted text   a  There are 27! encryption maps. To describe one of them requires in general log 27! symbols. Note that the question implicitly assumes that we are using a 27-symbol programming language, so the log here is to base 27.   b  The complexity of the encrypted text cannot be worse than the complexity of the English text plus the complexity of the encryption map  plus some small constant .   c  The idea here is that in order to be able to decode the encrypted text, the length of the encrypted string, n , must be greater than n=4 + log 27! . Why? Because short strings have short programs that print them out simply by writing including the text verbatim and saying \Print this". This does not take advantage of the structure of the text, but the text is so short that there isn’t really enough structure to take advantage of. Any random sequence of symbols of length n can always be printed out by a program of length n  + c , so if n is less than log 27! the overhead of expressing it as an encryption of English is higher than including it as verbatim data. It is only as the string grows to length appreciably greater than log27! that the overhead of expressing it as the encryption of English text becomes neglible. Now the structure starts to dominate. It should be pointed out that this is only the beginning of an idea about the rela- tionship between encrypted text and the ability to uniquely decipher it. Shannon studied the relationship between encryption and complexity in [12].  13. Kolmogorov complexity.  Consider the Kolmogorov complexity K n  over the integers n . If a speci cid:12 c integer n1 has a low Kolmogorov complexity K n1  , by how much can the Kolmogorov complexity K n1 + k  for the integer n1 + k vary from K n1  ?   330  Kolmogorov Complexity  Solution: Kolmogorov complexity.  Since we can describle n + k by describing n and then describing k and then adding them, K n + k   cid:20  K n  + log k + c , since the descriptive complexity of k is less the log k . Similarly, if the complexity of n + k is small, we cand describe n by describing n + k and k , and therefore K n  < K n + k  + log k + c . Thus we have jK n + k   cid:0  K n j  cid:20  log k + c .  14. Complexity of large numbers. Let A n  be the set of positive integers x for which a terminating program p of length less than or equal to n bits exists that outputs x . Let B n  be the complement of A n  , i.e., B n  is the set of integers x for which no program of length less than or equal to n outputs x . Let M  n  be the maximum of A n  and let S n  be the minimum of B n  . What is the Kolmogorov complexity K M  n    approximately ? What is K S n    approximately ? Which is larger   M  n  or S n   ? Give a reasonable lower bound on M  n  and a reasonable upper bound on S n  .  Solution: Complexity of large numbers.  Clearly since we can specify the program that printed out M  n  with length less than n , the Kolmogorov complexity of M  n  is less than n . The description \largest number that is printed out by a program of less than n bits" does not give rise to an e cid:11 ective program to compute M  n  , because even though we can simulate in parallel all programs of length less than n , we will never know when we have found M  n  . Thus a good bound on K M  n    cid:25  n . While S n  does not have a program of length less than n to compute it, and therefore K S n   > n , we know that since it is the smallest such number, S n   cid:0  1 has a short program of length less than n . Therefore we can describe S n  by describing S n  cid:0  1 and the di cid:11 erence, and the complexity K S n    cid:25  n . M  n  is likely to be much much larger than S n  since we can describe very very large numbers with short programs  e.g. iterated exponentials  S n  on the other hand is a boring small number.  M  n  could be very large, and a good lower bound is an iterated exponential, i.e., 222::: , where the iteration is done n times. S n  on the other hand cannot be less than 2n since all numbers less than 2n have description lengths less than n . However since there are not enough short programs, the numbers above 2n is likely to have complexity greater than n , and so S n   cid:25  2n .   Chapter 15  Network Information Theory  1. The cooperative capacity of a multiple access channel.  Figure 15.1    W1; W2    cid:8  cid:8  cid:8  cid:8  cid:8 * HHHHHj  X1  X2  -  -  p yjx1; x2   -  Y  -    ^W1; ^W2   Figure 15.1: Multiple access channel with cooperating senders.   a  Suppose X1 and X2 have access to both indices W1 2 f1; 2nRg; W2 2 f1; 2nR2g: Thus the codewords X1 W1; W2 ; X2 W1; W2  depend on both indices. Find the capacity region.   b  Evaluate this region for the binary erasure multiple access channel Y = X1 +  X2; Xi 2 f0; 1g: Compare to the non-cooperative region.  Solution: Cooperative capacity of multiple access channel   a  When both senders have access to the pair of messages to be transmitted, they can act in concert. The channel is then equivalent to a single user channel with  331   332  Network Information Theory  input alphabet X1  cid:2  X2 , and a larger message set W1  cid:2  W2 . The capacity of this single user channel is C = maxp x  I X; Y   = maxp x1;x2  I X1; X2; Y   . The two senders can send at any combination of rates with the total rate   b  The capacity for the binary erasure multiple access channel was evaluated in class.  When the two senders cooperate to send a common message, the capacity is  R1 + R2  cid:20  C   15.1   C = max p x1;x2   I X1; X2; Y   = max H Y   = log 3;   15.2   achieved by  for example  a uniform distribution on the pairs,  0,0 ,  0,1  and  1,1 . The cooperative and non-cooperative regions are illustrated in Figure 15.2.  R2  6  @  @  @  @  @  @  @  C2 = 1  1 2  0  @  @ @  @ @  @ @  @ @  @ @  @ @ @ @@  @  @  @  @  @  @  @  @  @  1 2  C1 = 1  Ccooper: = log 3  - R1  Figure 15.2: Cooperative and non-cooperative capacity for a binary erasure multiple access channel  2. Capacity of multiple access channels. Find the capacity region for each of the  following multiple access channels:   Network Information Theory  333  R2  6  C2 = 1  @  @  @  @  @  @  @  @  @  @  @  @  @  @  @ C1 = 1  - R1  0  Figure 15.3: Capacity region of additive modulo 2 MAC   a  Additive modulo 2 multiple access access channel. X1 2 f0; 1g; X2 2 f0; 1g; Y =  X1  cid:8  X2 .   b  Multiplicative multiple access channel. X1 2 f cid:0 1; 1g; X2 2 f cid:0 1; 1g; Y = X1  cid:1  X2: Solution: Examples of multiple access channels.   a  Additive modulo 2 MAC.  Y = X1 cid:8 X2 . Quite clearly we cannot send at a total rate of more than 1 bit, since H Y    cid:20  1 . We can achieve a rate of 1 bit from sender 1 by setting X2 = 0 , and similarly we can send 1 bit transmission from sender 2. By simple time sharing we can achieve the entire capacity region which is shown in Figure 15.3.   b  Multiplier channel.  X1; X2 2 f cid:0 1; 1g; Y = X1:X2 . This channel is equivalent to the previous channel with the mapping  cid:0 1 ! 1 and 1 ! 0 . Hence the capacity region is the same as the previous channel.  3. Cut-set interpretation of capacity region of multiple access channel. For the  multiple access channel we know that  R1; R2  is achievable if  R1 < I X1; Y j X2 ; R2 < I X2; Y j X1 ; R1 + R2 < I X1; X2; Y  ;   15.3    15.4    15.5    334  Network Information Theory  for X1; X2 independent. Show, for X1; X2 independent, that  I X1; Y j X2  = I X1; Y; X2 :  Interpret the information bounds as bounds on the rate of  cid:13 ow across cutsets S1; S2 and S3:  Solution: Cutset interpretation of the capacity region.  We can interpret I X1; Y; X2  as the maximum amount of information that could  cid:13 ow across the cutset S1 . This is an upper bound on the rate R1 . Similarly, we can interpret the other bounds.  4. Gaussian multiple access channel capacity. For the AWGN multiple access chan- nel, prove, using typical sequences, the achievability of any rate pairs  R1; R2  satisfying  R1 <  log 1 +  R2 <  log 1 +  R1 + R2 <  log 1 +  1 2 1 2 1 2   ;  P1 N P2 N P1 + P2   ;   :  N   15.6    15.7    15.8   The proof extends the proof for the discrete multiple access channel in the same way as the proof for the single user Gaussian channel extends the proof for the discrete single user channel.  Solution: Gaussian Multiple Access Channel Capacity.  The essence of the proof of the achievability of the capacity region for the Gaussian multiple access channel is the same as the discrete multiple access channel. The main di cid:11 erence is the introduction of the power constraint, and the modi cid:12 cations that have to   335   15.9    15.10    15.11    15.12    15.13    15.14    15.15    15.16   Network Information Theory  be made to ensure that the codewords satisfy the power constraint with high probability. We will brie cid:13 y outline the proof of achievability along the lines of the proof in the discrete cases, pausing only to emphasize the di cid:11 erences.  The channel is de cid:12 ned by  Y = X1 + X2 + Z; Z  cid:24  N  0; N    with power constraints P1 and P2 on the inputs. The achievable rates for this channel are  R1 < C cid:18  P1 N  cid:19  N  cid:19  R2 < C cid:18  P2 R1 + R2 < C cid:18  P1 + P2 N  cid:19  ;  C x  =  log 1 + x :  1 2   x1 i ; x2 j ; y  2 A n    cid:15   1 n  1 n  n  Xk=1 Xk=1  n  x2 1k i   cid:20  P1  x2 2k j   cid:20  P2  where  and  Codebook generation: Generate 2nR1 independent codewords X1 w1  , w1 2 f1; 2; : : : ; 2nR1g , of length n , generating each element i.i.d.  cid:24  N  0; P1  cid:0   cid:15   Similarly generate 2nR2 in- dependent codewords X2 w2  , w2 2 f1; 2; : : : ; 2nR2g , generating each element i.i.d.  cid:24  N  0; P2  cid:0   cid:15   . These codewords form the codebook. Encoding: To send index w1 , sender one sends the codeword X1 w1  . Similarly, to send w2 , sender 2 sends X2 w2  . Decoding: The receiver Y n chooses the pair  i; j  such that  if such a pair  i; j  exists and is unique; otherwise, an error is declared.  By the symmetry of the random code construction, the conditional probability of error does not depend on which pair of indices is sent. So, without loss of generality, we can assume that  w1; w2  =  1; 1  .  An error occurs in the decoding if   cid:15   x1 1 ; x2 1   =2 A n   cid:15   x1 i ; x2 j   2 A n    cid:15    cid:15   , for some i 6= 1 or j 6= 1 , or   336  Network Information Theory   cid:15  x1 1  or x2 1  do not satisfy the power constraint.  De cid:12 ne the events  and  For i 6= 0; j 6= 0 ,  E01 = f  X 2 1k 1  > P1g  E02 = f  X 2 2k 1  > P2g:  1 n  1 n  n  Xk=1 Xk=1  n  Eij = f X1 i ; X2 j ; Y  2 A n   cid:15  g:  Then by the union of events bound,  P  n   e  = P  cid:16 E01[ E02[ Ec  cid:20  P  E01  + P  E02  + P  Ec  11[ [ i;j 6= 1;1 Eij cid:17  11  + Xi6=1; j=1  P  Ei1  + Xi=1; j6=1  P  E1j  + Xi6=1; j6=1  where P is the probability given that  1; 1  was sent. Since we choose the codewords according to a normal distribution with mean Pi  cid:0   cid:15  , with very high probability the codeword power will be less than P . Hence, P  E01  ! 0 and P  E02  ! 0 . From the AEP, P  Ec  11  ! 0 . By the AEP, for i 6= 1 , we have  P  Ei1  = P   X1 i ; X2 1 ; Y  2 A n  f  x1 f  x2; y    cid:15       cid:15   = Z x1;x2;y 2A n   cid:20  2 cid:0 n h X1 +h X2;Y   cid:0 h X1;X2;Y   cid:0 3 cid:15   = 2 cid:0 n I X1;X2;Y   cid:0 3 cid:15   = 2 cid:0 n I X1;Y jX2  cid:0 3 cid:15   = 2 cid:0 n C  P1  N   cid:0 3 cid:15  ;  since X1 and X2 are independent, and therefore I X1; X2; Y   = I X1; X2 +I X1; Y jX2  = I X1; Y jX2  . Similarly, for j 6= 1 ,  and for i 6= 1; j 6= 1 ,  It follows that  P  E1j   cid:20  2 cid:0 n C  P2  N   cid:0 3 cid:15  ;  P  Eij   cid:20  2 cid:0 n C  P1 +P2  N   cid:0 4 cid:15  :  P  n   e   cid:20  P  E01  + P  E02  + P  Ec  11  + 2nR12 cid:0 n C  P1  +2nR22 cid:0 n C  P2  N   cid:0 3 cid:15   + 2n R1+R2 2 cid:0 n C  P1 +P2  N   cid:0 3 cid:15   N   cid:0 4 cid:15  :   15.29    15.17    15.18    15.19    15.20   P  Eij ;   15.21    15.22    15.23    15.24    15.25    15.26    15.27    15.28    Network Information Theory  337  Thus  cid:15  > 0 arbitrary and the conditions of the theorem cause each term to tend to 0 as n ! 1 . The above bound shows that the average probability of error, averaged over all choices of codebooks in the random code construction, is arbitrarily small. Hence there exists at least one code C cid:3  with arbitrarily small probability of error. The achievability of the capacity region is proved.  5. Converse for the Gaussian multiple access channel. Prove the converse for the Gaussian multiple access channel by extending the converse in the discrete case to take into account the power constraint on the codewords.  Solution: Converse for the Gaussian multiple access channel. The proof of the con- verse for the Gaussian case proceeds on very similar lines to the discrete case. However, for the Gaussian case, the two stages of proof that were required in the discrete case, namely, of  cid:12 nding a new expression for the capacity region and then proving a converse, can be combined into one single step.  By the code construction, it is possible to estimate  W1; W2  from the received sequence Y n with a low probability of error. Hence the conditional entropy of  W1; W2  given Y n must be small. By Fano’s inequality,  H W1; W2jY n   cid:20  n R1 + R2 P  n   e + H P  n   e    4= n cid:15 n:   15.30   It is clear that  cid:15 n ! 0 as P  n   e ! 0 .  Then we have  H W1jY n   cid:20  H W1; W2jY n   cid:20  n cid:15 n; H W2jY n   cid:20  H W1; W2jY n   cid:20  n cid:15 n:  We can now bound the rate R1 as  nR1 = H W1    a    b   = I W1; Y n  + H W1jY n   cid:20  I W1; Y n  + n cid:15 n  cid:20  I X n = H X n  c   1  W1 ; Y n  + n cid:15 n 1  W1    cid:0  H X n 1  W1 jX n 1  W1 ; Y njX n   cid:20  H X n = I X n = h Y njX n = h Y njX n = h Y njX n   e    d   1  W1 jY n  + n cid:15 n  2  W2    cid:0  H X n 2  W2   + n cid:15 n  1  W1 jY n; X n  2  W2   + n cid:15 n  2  W2    cid:0  h Y njX n 2  W2    cid:0  h Z njX n 2  W2    cid:0  h Z n  + n cid:15 n  1  W1 ; X n 1  W1 ; X n  2  W2   + n cid:15 n  2  W2   + n cid:15 n   15.31    15.32    15.33    15.34    15.35    15.36    15.37    15.38    15.39    15.40    15.41    15.42    338  Network Information Theory  h Zi  + n cid:15 n  h Zi  + n cid:15 n  h Zi  + n cid:15 n  h Zi  + n cid:15 n  h Zi  + n cid:15 n   f    = h Y njX n  n  n  n  h YijX n  2  W2    cid:0   h YijX2i   cid:0   Xi=1 Xi=1 2  W2    cid:0  Xi=1 h X1i + ZijX2i   cid:0  Xi=1  h X1i + Zi   cid:0  1 2  n  log 2 cid:25 e P1i + N    cid:0  log cid:18 1 + N  cid:19  + n cid:15 n  P1i  1 2  n  Xi=1  1 2  n  n  n  n  Xi=1 Xi=1 Xi=1 Xi=1 Xi=1 Xi=1  n  n   g    cid:20    h    cid:20    i  =   j  =   k    cid:20   =  log 2 cid:25 eN + n cid:15 n  1  W1  and X n  2  W2   = H X n  1  W1   , and H X n  1  W1 jX n 1 + X n  where  a  follows from Fano’s inequality,  b  from the data processing inequality,  c  from the fact that since W1 and W2 are independent, so are X n and hence it follows that H X n H X n 1  W1 jY n  by conditioning,  d  from the fact that Y n = X n  e  from the fact that Z n is independent of X n  f  from the fact that the noise is i.i.d.,  g  from the chain rule and removing conditioning,  h  from removing conditioning,  i  from the fact that Yi = X1i + X2i + Zi ,  j  from the fact that X1i and Zi are independent of X2i , and  k  from the entropy maximizing property of the normal  Theorem 9.6.5 , after de cid:12 ning P1i = EX 2 Hence, we have  2  W2  , 1  W1 jY n; X n  1 and X n 2 ,  2 + Z n ,  1i .  2  W2    cid:20   R1  cid:20   R2  cid:20   1 n  1 n  n  Xi=1  n  Xi=1  1 2  log cid:18 1 +  P1i  N  cid:19  +  cid:15 n:  1 2  log cid:18 1 +  P2i  N  cid:19  +  cid:15 n:  Similarly, we have  To bound the sum of the rates, we have  n R1 + R2  = H W1; W2    15.43    15.44    15.45    15.46    15.47    15.48    15.49    15.50    15.51    15.52    Network Information Theory  2  W2   + n cid:15 n   a    b   1  W1 ; X n  = I W1; W2; Y n  + H W1; W2jY n   cid:20  I W1; W2; Y n  + n cid:15 n  cid:20  I X n = h Y n   cid:0  h Y njX n = h Y n   cid:0  h Z n  + n cid:15 n = h Y n   cid:0   2  W2 ; Y n  + n cid:15 n  1  W1 ; X n  h Zi  + n cid:15 n  n   c    d   n  Xi=1 Xi=1 h Yi   cid:0  1 2  n  h Zi  + n cid:15 n  Xi=1 Xi=1 log 2 cid:25 e P1i + P2i + N    cid:0   cid:19  + n cid:15 n log cid:18 1 +  P1i + P2i  1 2  N   e    cid:20    f     cid:20   =  n  1 2  log 2 cid:25 eN + n cid:15 n   15.60   where  a  follows from Fano’s inequality,  b  from the data processing inequality,  c  from the fact that Y n = X n 1 and X n 2 ,  d  from the fact that Zi are i.i.d.,  e  follows from the chain rule and removing con- ditioning, and  f  from the entropy maximizing property of the normal, and the de cid:12 nitions of P1i and P2i .  2 + Z n , and Z n is independent of X n  1 + X n  Hence we have  R1 + R2  cid:20   1 n  n  Xi=1  1 2  log cid:18 1 +  P1i + P2i  N   cid:19  +  cid:15 n:  The power constraint on the codewords imply that  and  1 n  1 n  n  Xi=1  n  Xi=1  P1i  cid:20  P1;  P2i  cid:20  P2:  Now since log is concave function, we can apply Jensens inequality to the expressions in  15.50 ,  15.51  and  15.62 . Thus we obtain  R1  cid:20   log 1 +  1 2  1  nPn  i=1 P1i N  ! +  cid:15 n  339   15.53    15.54    15.55    15.56    15.57    15.58    15.59    15.61    15.62    15.63    15.64    15.65    340  Network Information Theory  which when combined with the power constraints, and taking the limit at n ! 1 , we obtain the desired converse, i.e.,  R2  cid:20   log 1 + log 1 +  1 2  1 2  1  nPn nPn  1  R1 + R2  cid:20   i=1 P2i N  ! +  cid:15 n  i=1 P1i + P2i  N  ! +  cid:15 n:  R1 <  log 1 +  R2 <  log 1 +  1 2 1 2 1 2   ;  P1 N P2 N P1 + P2   ;  N  R1 + R2 <  log 1 +   :   15.66    15.67    15.68    15.69    15.70   6. Unusual multiple access channel. Consider the following multiple access channel: X1 = X2 = Y = f0; 1g . If  X1; X2  =  0; 0  , then Y = 0 . If  X1; X2  =  0; 1  , then Y = 1 . If  X1; X2  =  1; 0  , then Y = 1 . If  X1; X2  =  1; 1  , then Y = 0 with probability 1  2 and Y = 1 with probability 1 2 .   a  Show that the rate pairs  1,0  and  0,1  are achievable.   b  Show that for any non-degenerate distribution p x1 p x2  , we have I X1; X2; Y   <  1 .   c  Argue that there are points in the capacity region of this multiple access channel that can only be achieved by timesharing, i.e., there exist achievable rate pairs  R1; R2  which lie in the capacity region for the channel but not in the region de cid:12 ned by  R1  cid:20  I X1; Y jX2 ; R2  cid:20  I X2; Y jX1 ; R1 + R2  cid:20  I X1; X2; Y     15.71    15.72    15.73   for any product distribution p x1 p x2  . Hence the operation of convexi cid:12 cation strictly enlarges the capacity region. This channel was introduced independently by Csisz cid:19 ar and K cid:127 orner[3] and Bierbaum and Wallmeier[2].  Solution:  Unusual multiple access channel.   a  It is easy to see how we could send 1 bit transmission from X1 to Y simply set X2 = 0 . Then Y = X1 , and we can send 1 bit transmission to from sender 1 to the receiver. Alternatively, if we evaluate the achievable region for the degenerate product distri- bution p x1 p x2  with p x1  =   1 2   , p x2  =  1; 0  , we have I X1; Y jX2  = 1 ,  2 ; 1   Network Information Theory  341  I X2; Y jX1  = 0 , and I X1; X2; Y   = 1 . Hence the point  1; 0  lies in the achievable region for the multiple access channel corresponding to this product distribution. By symmetry, the point  0; 1  also lies in the achievable region.   b  Consider any non-degenerate product distribution, and let p1 = p X1 = 1  , and let p2 = p X2 = 1  . By non-degenerate we mean that p1 6= 0 or 1, and p2 6= 0 or 1. In this case, Y = 0 when  X1; X2  =  0; 0  and half the time when  X1; X2  =  1; 1  , i.e., with a probability  1  cid:0  p1  1  cid:0  p2  + 1 2 p1p2 . Y1 = 1 for the other input pairs, i.e., with a probability p1 1  cid:0  p2  + p2 1  cid:0  p1  + 1 2 p1p2 . We can evaluate the achievable region of the multiple access channel for this product distribution. In particular,  R1+R2  cid:20  I X1; X2; Y   = H Y   cid:0 H Y jX1; X2  = H  1 cid:0 p1  1 cid:0 p2 + p1p2  cid:0 p1p2:  15.74  Now H  1  cid:0  p1  1  cid:0  p2  + 1 2 p1p2   cid:20  1  entropy of a binary random variable is at most 1  and p1p2 > 0 for a non-degenerate distribution. Hence R1 + R2 is strictly less than 1 for any non-degenerate distribution.  1 2   c  The degenerate distributions have either R1 or R2 equal to 0. Hence all the distri- butions that achieve rate pairs  R1; R2  with both rates positive have R1+R2 < 1 . For example the union of the achievable regions over all product distributions does not include the point   1 2   . But this point is clearly achievable by timesharing between the points  1; 0  and  0; 1  . Or equivalently, the point   1 2   lies in the convex hull of the union of the achievable regions, but not the union itself. So the operation of taking the convex hull has strictly increased the capacity region for this multiple access channel.  2 ; 1  2 ; 1  7. Convexity of capacity region of broadcast channel. Let C  cid:18  R2 be the capacity region of all achievable rate pairs R =  R1; R2  for the broadcast channel. Show that C is a convex set by using a timesharing argument. Speci cid:12 cally, show that if R 1  and R 2  are achievable, then  cid:21 R 1  +  1  cid:0   cid:21  R 2  achievable for 0  cid:20   cid:21   cid:20  1: Solution: Convexity of Capacity Regions. Let R 1  and R 2  be two achievable rate pairs. Then there exist a sequence of   2nR 1  1 ; 2nR 1  2  ; n  codes for the chan- nel with P  n   2  ! 0 . We will now construct a code of rate  cid:21 R 1  +  1  cid:0   cid:21  R 2  . For a code length n , use the concatenation of the codebook of length  cid:21 n and rate R 1  and the code of length  1  cid:0   cid:21  n and rate R 2  . The new codebook consists of all pairs of codewords and hence the number of X1 codewords is 2 cid:21 nR 1  1 2 1 cid:0  cid:21  nR 2  , and hence the rate is  cid:21 R 1  . Similarly the rate of the X2 codeword is  cid:21 R 1   2  ; n  codes and a sequence of   2nR 2   1  ! 0 and P  n   1 +  1  cid:0   cid:21  R 2   1 ; 2nR 2   is  1  .  e  e  1  2 +  1  cid:0   cid:21  R 2   2   342  Network Information Theory  R2  6  H Y    @  @  @  @  @  @  0  H XjY    @@ H X   - R1  Figure 15.4: Slepian Wolf rate region for Y = f  X  .  We will now show that the probability of error for this sequence of codes goes to zero. The decoding rule for the concatenated code is just the combination of the decoding rule for the parts of the code. Hence the probability of error for the combined codeword is less than the sum of the probabilities for each part. For the combined code,  e  cid:20  P   cid:21 n  P  n   e   1  + P   1 cid:0  cid:21  n    2   e   15.75   which goes to 0 as n ! 1 . Hence the overall probability of error goes to 0, which implies the  cid:21 R 1  +  1  cid:0   cid:21  R 2  is achievable.  8. Slepian-Wolf for deterministically related sources. Find and sketch the Slepian- Wolf rate region for the simultaneous data compression of  X; Y  ; where y = f  x  is some deterministic function of x:  Solution: Slepian Wolf for Y = f  X  . The quantities de cid:12 ning the Slepian Wolf rate region are H X; Y   = H X  , H Y jX  = 0 and H XjY    cid:21  0 . Hence the rate region is as shown in the Figure 15.4.  9. Slepian-Wolf. Let Xi be i.i.d. Bernoulli  p  . Let Zi be i.i.d.  cid:24  Bernoulli  r  , and let Z be independent of X: Finally, let Y = X cid:8  Z  mod 2 addition . Let X be described at rate R1 and Y be described at rate R2: What region of rates allows recovery of X; Y with probability of error tending to zero?  Solution: Slepian Wolf for binary sources.   Network Information Theory  343  R2  6  H Y    = H p  cid:3  r   H Y jX  = H r   @  @  @  @  @  @  @@  0  H XjY   = H p  +H r   cid:0  H p  cid:3  r   H X  = H p   Figure 15.5: Slepian Wolf region for binary sources  - R1  X  cid:24  Bern  p  . Y = X  cid:8  Z , Z  cid:24  Bern  r  . Then Y  cid:24  Bern  p  cid:3  r  , where p  cid:3  r = p 1 cid:0  r  + r 1 cid:0  p  . H X  = H p  . H Y   = H p cid:3  r  , H X; Y   = H X; Z  = H X  + H Z  = H p +H r  . Hence H Y jX  = H r  and H XjY   = H p +H r  cid:0 H p cid:3 r  . The Slepian Wolf region in this case is shown in Figure 15.5.  10. Broadcast capacity depends only on the conditional marginals. Consider the general broadcast channel  X; Y1  cid:2  Y2; p y1; y2 j x  : Show that the capacity region depends only on p y1 j x  and p y2 j x : To do this, for any given   2nR1 ; 2nR2 ; n  code, let  = Pf ^W1 Y1  6= W1g; = Pf ^W2 Y2  6= W2g;  P  n  1 P  n  2 P  n  = Pf  ^W1; ^W2  6=  W1; W2 g:  maxfP  n   1  ; P  n   2 g  cid:20  P  n   cid:20  P  n   1 + P  n   2  :   15.76    15.77    15.78   Then show  The result now follows by a simple argument.  Remark: The probability of error P  n  does depend on the conditional joint distribution p y1; y2 j x : But whether or not P  n  can be driven to zero  at rates   R1; R2    does not  except through the conditional marginals p y1 j x ; p y2 j x  :   344  Network Information Theory  Solution: Broadcast channel capacity depends only on conditional marginals  P  n  = P    ^W1 Y1 ; ^W2 Y2   6=  W1; W2    Then by the union of events bound, it is obvious that  P  n  1 = P   ^W1 Y1  6= W1  P  n  2 = P   ^W2 Y2  6= W2   P  n   cid:20  P  n   1 + P  n   2  :   15.79    15.80    15.81    15.82   Also since   ^W1 Y1  6= W1  or   ^W2 Y2  6= W2  implies    ^W1 Y1 ; ^W2 Y2   6=  W1; W2   , we have  15.83   P  n   cid:21  maxfP  n   1  ; P  n  2 g:  2 ! 0 .  1 ! 0 and P  n   Hence P  n  ! 0 i cid:11  P  n  The probability of error, P  n  , for a broadcast channel does depend on the joint con- ditional distribution. However, the individual probabilities of error P  n  and P  n  however depend only on the conditional marginal distributions p y1jx  and p y2jx  respectively. Hence if we have a sequence of codes for a particular broadcast channel with P  n  ! 0 , so that P  n  2 ! 0 , then using the same codes for another broadcast channel with the same conditional marginals will ensure that P  n  for that channel as well, and the corresponding rate pair is achievable for the second channel. Hence the capacity region for a broadcast channel depends only on the conditional marginals.  1 ! 0 and P  n   2  1  11. Converse for the degraded broadcast channel. The following chain of inequalities proves the converse for the degraded discrete memoryless broadcast channel. Provide reasons for each of the labeled inequalities.  Setup for converse for degraded broadcast channel capacity:   W1; W2 indep. ! X n W1; W2  ! Y n  1 ! Y n  2  Encoding fn : 2nR1  cid:2  2nR2 ! X n Decoding: gn : Y n Let Ui =  W2; Y i cid:0 1  1 ! 2nR1;   . Then  1  hn : Y n  2 ! 2nR2  nR2   cid:1  cid:20 Fano I W2; Y n 2    n   a  =  Xi=1 = Xi   b      2  I W2; Y2i j Y i cid:0 1  H Y2i j Y i cid:0 1  2     cid:0  H Y2i j W2; Y i cid:0 1  2       15.84    15.85    15.86    Network Information Theory  Continuation of converse. Give reasons for the labeled inequalities:   c    d    cid:20  Xi = Xi Xi=1   e  =  n   H Y2i   cid:0  H Y2i j W2; Y i cid:0 1  H Y2i   cid:0  H Y2i j W2; Y i cid:0 1  1  2      ; Y i cid:0 1  1      I Ui; Y2i :  nR1   f     cid:1  cid:20 Fano I W1; Y n 1   I W1; Y n  cid:20   cid:20   h  =  I W1; Y n n   g   1 ; W2   1 j W2   I W1; Y1i j Y i cid:0 1  1  ; W2    i    cid:20   I Xi; Y1i j Ui :  Xi cid:0 1 Xi=1  n  R1  cid:20  I XQ; Y1QjUQ; Q  R2  cid:20  I UQ; Y2QjQ ;  R1  cid:20  I X; Y1jU   R2  cid:20  I U ; Y2 ;  Now let Q be a time sharing random variable with Pr Q = i  = 1=n , i = 1; 2; : : : ; n . Justify the following:  for some distribution p q p ujq p xju; q p y1; y2jx  . By appropriately rede cid:12 ning U , argue that this region is equal to the convex closure of regions of the form  for some joint distribution p u p xju p y1; y2jx  . Solution: Converse for the degraded broadcast channel.  We also have  Let Ui =  W2; Y i cid:0 1  . By Fano’s inequality,   W1; W2  ! X W1; W2  ! Y ! Z   W1; W2  ! Xi W1; W2  ! Yi ! Zi:  H W2jZ n   cid:20  P  n   2 nR2 + H P  n   2    = n cid:15 n  345   15.87    15.88    15.89    15.90    15.91    15.92    15.93    15.94    15.95    15.96    15.97    15.98    15.99    15.100    15.101    346  Network Information Theory  where  cid:15 n ! 0 as P  n   2 ! 0 .  We then have the following chain of inequalities  nR2 = H W2    a    b   = I W2; Z n  + H W2jZ n   cid:20  I W2; Z n  + n cid:15 n = Xi = Xi  cid:20  Xi = Xi = Xi  I Ui; Zi  + n cid:15 n   c    e    d   I W2; ZijZ i cid:0 1  + n cid:15 n  H ZijZ i cid:0 1   cid:0  H ZijZ i cid:0 1; W2   + n cid:15 n   H Zi   cid:0  H ZijZ i cid:0 1; W2; Y i cid:0 1   + n cid:15 n  H Zi   cid:0  H ZijW2; Y i cid:0 1   + n cid:15 n  where  15.104  follows from Fano’s inequality,  a  from the chain rule,  b  from the de cid:12 nition of conditional mutual information,  c  from the fact that removing conditioning increases entropy and adding conditioning reduces it,  d  from the fact that since the broadcast channel is degraded, Z i cid:0 1 depends only on Y i cid:0 1 and is conditionally independent of everything else, hence Zi is conditionally independent of Z i cid:0 1 given Y i cid:0 1 ,  e  follows from the de cid:12 nition of Ui . Continuation of Converse.  Similarly by Fano’s inequality,  H W1jY n   cid:20  P  n   1 nR1 + H P  n   1    = n cid:15 n  and we have the chain of inequalities,  nR1 = H W1    f    = I W1; Y n  + H W1jY n   cid:20  I W1; Y n  + n cid:15 n  cid:20  I W1; W2; Y n  + n cid:15 n = I W1; Y njW2  + n cid:15 n  cid:20  I W1; YijW2; Y i cid:0 1  + n cid:15 n  cid:20  I W1; Xi; YijW2; Y i cid:0 1  + n cid:15 n   g    h    15.102    15.103    15.104    15.105    15.106    15.107    15.108    15.109    15.110    15.111    15.112    15.113    15.114    15.115    15.116    15.117    Network Information Theory   i    cid:20  I Xi; YijW2; Y i cid:0 1  + n cid:15 n = I Xi; YijUi  + n cid:15 n  347   15.118    15.119   where  15.113  follows from Fano’s inequality,  f  follows from the fact that the di cid:11 erence, I W1; W2jY n   cid:21  0 ,  g  follows from the chain rule for I and the fact that W1 and W2 are independent,  h  from the chain rule for mutual information, and  i  from the data processing inequality.  We can then use standard techniques like the introduction of a time-sharing random variable to complete the proof of the converse for the broadcast channel.  12. Capacity points.   a  For the degraded broadcast channel X ! Y1 ! Y2;  cid:12 nd the points a and b where  the capacity region hits the R1 and R2 axes  Figure 15.6 .  Figure 15.6: Capacity region of a broadcast channel   b  Show that b  cid:20  a: Solution: Capacity region of broadcast channel.   a  The capacity region of the degraded broadcast channel X ! Y1 ! Y2 is the  convex hull of regions of the form  R1  cid:20  I X; Y1jU   R2  cid:20  I U ; Y2    15.120    15.121   over all choices of auxiliary random variable U and joint distribution of the form p u p xju p y1; y2jx  . The region is of the form illustrated in Figure 15.7. The point a on the  cid:12 gure corresponds to the maximum achievable rate from the sender to receiver 2. From the expression for the capacity region, it is the maximum value of I U ; Y2  for all auxiliary random variables U .   348  Network Information Theory  For any random variable U and p u p xju  , U ! X ! Y2 forms a Markov chain, and hence I U ; Y2   cid:20  I X; Y2   cid:20  maxp x  I X; Y2  . The maximum can be achieved by setting U = X and choosing the distribution of X to be the one that maximizes I X; Y2  . Hence the point a corresponds to R2 = maxp x  I X; Y2 ; R1 = I X; Y1jU   = I X; Y1jX  = 0 . The point b has a similar interpretation. The point b corresponds to the maximum rate of transmission to receiver 1. From the expression for the capacity region,  R1  cid:20  I X; Y1jU   = H Y1jU    cid:0  H Y1jX; U   = H Y1jU    cid:0  H Y1jX ;   15.122   since U ! X ! Y1 forms a Markov chain. Since H Y1jU    cid:20  H Y1  , we have  R1  cid:20  H Y1   cid:0  H Y1jX  = I X; Y1   cid:20  max  p x   I X; Y1 ;   15.123   and the maximum is attained when we set U  cid:17  0 and choose p x  = p xju  to be the distribution that maximizes I X; Y1  . In this case, R2  cid:20  I U ; Y2  = 0 . Hence point b corresponds to the rates R1 = maxp x  I X; Y1 ; R2 = 0 . These results have a simple single user interpretation. If we not sending any information to receiver 1, then we can treat the channel to receiver 2 as a single user channel and send at capacity for this channel, i.e., max I X; Y2  . Similarly, if we are not sending any information to receiver 2, we can send at capacity to receiver 1, which is max I X; Y1  .   b  Since X ! Y1 ! Y2 forms a Markov chain for all distributions p x  , we have by  the data processing inequality  a = max p x   I X; Y2  = I X cid:3 ; Y2   cid:20  I X cid:3 ; Y1  = max p x   I X; Y1  = b;   15.124    15.125    15.126   where X cid:3  has the distribution that maximizes I X; Y2  .  13. Degraded broadcast channel. Find the capacity region for the degraded broadcast  channel in Figure 15.8.  Solution: Degraded broadcast channel. From the expression for the capacity region, it is clear that the only on trivial possibility for the auxiliary random variable U is that it be binary. From the symmetry of the problem, we see that the auxiliary random variable should be connected to X by a binary symmetric channel with parameter  cid:12  .  Hence we have the setup as shown in Figure 15.9.  We can now evaluate the capacity region for this choice of auxiliary random variable. By symmetry, the best distribution for U is the uniform. Hence  R2 = I U ; Y2    15.127    Network Information Theory  Also  2   cid:11   ;  cid:11 ;  = H Y2   cid:0  H Y2jU   = H cid:18   cid:11  = H  cid:11   +  cid:11 H cid:18  1 =  cid:11  1  cid:0  H  cid:12 p +  cid:12 p  :  2 cid:19   cid:0  H   cid:12 p +  cid:12 p  cid:11 ;  cid:11 ;   cid:12 p +  cid:12 p  cid:11    2 cid:19   cid:0  H  cid:11    cid:0   cid:11 H  cid:12 p +  cid:12 p   R1 = I X; Y1jU    = H Y1jU    cid:0  H Y1jU; X  = H  cid:12 p +  cid:12 p   cid:0  H p :  349   15.128    15.129    15.130    15.131    15.132    15.133    15.134   These two equations characterize the boundary of the capacity region as  cid:12  varies. When  cid:12  = 0 , then R1 = 0 and R2 =  cid:11  1  cid:0  H p   . When  cid:12  = 1 2 , we have R1 = 1  cid:0  H p  and R2 = 0 .  The capacity region is sketched in Figure 15.10.  14. Channels with unknown parameters. We are given a binary symmetric channel  with parameter p: The capacity is C = 1  cid:0  H p : Now we change the problem slightly. The receiver knows only that p 2 fp1; p2g , i.e., p = p1 or p = p2 , where p1 and p2 are given real numbers. The transmitter knows the actual value of p: Devise two codes for use by the transmitter, one to be used if p = p1; the other to be used if p = p2; such that transmission to the receiver can take place at rate  cid:25  C p1  if p = p1 and at rate  cid:25  C p2  if p = p2: Hint: Devise a method for revealing p to the receiver without a cid:11 ecting the asymptotic rate. Pre cid:12 xing the codeword by a sequence of 1’s of appropriate length should work.  Solution: Capacity of channels with unknown parameters.  We have two possiblities; the channel is a BSC with parameter p1 or a BSC with parameter p2 . If both sender and receiver know that state of channel, then we can achieve the capacity corresponding to which channel is in use, i.e., 1  cid:0  H p1  or 1  cid:0  H p2  .  If the receiver does not know the state of the channel, then he cannot know which codebook is being used by the transmitter. He cannot then decode optimally; hence he cannot achieve the rates corresponding to the capacities of the channels.  But the transmitter can inform the receiver of the state of the channel so that the receiver can decode optimally. To do this, the transmitter can precede the codewords by a sequence of 1’s or 0’s. Let us say we use a string of m 1’s to indicate that the channel was in state p1 and m 0’s to indicate state p2 . Then, if m = o n  and m ! 1 , where n is the block length of the code used, we have the probability of error in decoding the state of the channel going to zero. Since the receiver will then use the right code for the rest of the message, it will be decoded correctly with P  n  e ! 0 . The   350  Network Information Theory  e cid:11 ective rate for this code is log 2nC pi   n+m ! C pi  , since m = o n  . So we can achieve the same asymptotic rate as if both sender and receiver knew the state of the channel.  15. Two-way channel. Consider the two-way channel shown in Figure 15.6. The outputs  Y1 and Y2 depend only on the current inputs X1 and X2 .   a  By using independently generated codes for the two senders, show that the follow-  ing rate region is achievable:  R1 < I X1; Y2jX2 ; R2 < I X2; Y1jX1    15.135    15.136    15.137    15.138   for some product distribution p x1 p x2 p y1; y2jx1; x2  .   b  Show that the rates for any code for a two-way channel with arbitrarily small  probability of error must satisfy  R1  cid:20  I X1; Y2jX2 ; R2  cid:20  I X2; Y1jX1  for some joint distribution p x1; x2 p y1; y2jx1; x2  .  The inner and outer bounds on the capacity of the two-way channel are due to Shannon[14]. He also showed that the inner bound and the outer bound do not coincide in the case of the binary multiplying channel X1 = X2 = Y1 = Y2 = f0; 1g , Y1 = Y2 = X1X2 . The capacity of the two-way channel is still an open problem.  Solution: Two-way channel.   a  We will only outline the proof of achievability. It is quite straightforward compared  to the more complex channels considered in the text. Fix p x1 p x2 p y1; y2jx1; x2  . Code generation: Generate a code of size 2nR1 of codewords X1 w1  , where the x1i are generate i.i.d.  cid:24  p x1  . Similarly generate a codebook X2 w2  of size 2nR2 . Encoding: To send index w1 from sender 1, he sends X1 w1  . Similarly, sender 2 sends X2 w2  . Decoding: Receiver 1 looks for the unique w2 , such that  X1 w1 ; x2 w2 ; Y1  2 A n   X1; X2; Y1  . If there is no such w2 or more than one such, it declares an error. Similarly, receiver 2 looks for the unique w1 , such that  x1 w1 ; X2 w2 ; Y2  2 A n  Analysis of the probability of error: We will only analyze the error at receiver 1. The analysis for receiver 2 is similar. Without loss of generality, by the symmetry of the random code construction, we can assume that  1,1  was sent. We have an error at receiver 1 if   X1; X2; Y2  .   cid:15    cid:15    Network Information Theory  351   cid:15   X1 1 ; X2 1 ; Y1  =2 A n  the law of large numbers as n ! 1 .  cid:15  There exists an j 6= 1 , such that  X1 1 ; X2 j ; Y1  2 A n    cid:15    cid:15    X1; X2; Y1  . The probability of this goes to 0 by   X1; X2; Y1  .  De cid:12 ne the events  Then by the union of events bound,  Ej = f X1 1 ; X2 j ; Y1  2 A n   cid:15  g:  where P is the probability given that  1; 1  was sent. From the AEP, P  E c By Theorem 14.2.3, for j 6= 1 , we have  1  ! 0 .  P  n   e  = P  cid:16 Ec  cid:20  P  Ec  1[ [j6=1Ej cid:17  1  +Xj6=1  P  Ej ;  P  Ej  = P   X1 1 ; X2 j ; Y1  2 A n   cid:15  p x2 p x1; y1   =     X x1;x2;y1 2A n    cid:15    cid:15    cid:20  jA n  j2 cid:0 n H X2  cid:0  cid:15  2 cid:0 n H X1;Y   cid:0  cid:15    cid:20  2 cid:0 n H X2 +H X1;Y   cid:0 H X1;X2;Y   cid:0 3 cid:15   = 2 cid:0 n I X2;X1;Y   cid:0 3 cid:15   = 2 cid:0 n I X2;Y jX1  cid:0 3 cid:15  ;   15.139    15.140    15.141    15.142    15.143    15.144    15.145    15.146    15.147   since X1 and X2 are independent, and therefore I X1; X2; Y   = I X1; X2  + I X1; Y jX2  = I X1; Y jX2  . Therefore  e  cid:20  P  Ec P  n   1  + 2nR22 cid:0 n I X2;Y jX1  cid:0 3 cid:15  ;   15.148   Since  cid:15  > 0 is arbitrary, the conditions of the theorem imply that the probability of error tends to 0 as n ! 1 . Similarly, we can show that the probability of error at receiver two goes to 0, and thus we have proved the achievability of the region for the two way channel.   b  The converse is a simple application of the general Theorem 14.10.1 to this simple case. The sets S can be taken in turn to be each node. We will not go into the details.  16. Multiple-access channel  Let the output Y of a multiple-access channel be given by  where X1; X2 are both real and power limited,  Y = X1 + sgn X2    352  Network Information Theory  E[X 2 E[X 2  1 ]  cid:20  P1; 2 ]  cid:20  P2;  and sgn x  =  1; x > 0  cid:0 1; x  cid:20  0  :   a  Find the capacity region.  Note that there is interference but no noise in this channel.   b  Describe a coding scheme that achieves the capacity region.  Solution: Multiple-access channel   a  This is continuous noiseless multiple access channel, if we let U2 = sgn X2  , we  can consider a channel from X1 and U2 to Y  I X1; Y jX2  = h Y jX2   cid:0  h Y jX1; X2  = h X1jX2   cid:0    cid:0 1  = 1  15.149   since X1 and X2 are independent, and similarly  I X2; Y jX1  = I X2; U2; Y jX1  = I U2; Y jX1 +I X2; Y jX1; U2  = I U2; Y jX1  = H U2  cid:0 H U2jY; X1  = H U2   15.150  I X1; X2; Y   = 1 . Thus we can send at in cid:12 nite rate from X1 to Y and at a maximum rate of 1 bit transmission from X2 to Y .   b  We can send a 1 for X2 in the  cid:12 rst transmission, and knowing this, Y can recover X1 perfectly, recovering an in cid:12 nite number of bits. From then on, X1 can be 0 and we can send 1 bit per transmission using the sign of X2 .  17. Slepian Wolf  Let  X; Y   have the joint pmf p x; y   p x,y   1  2  3  1  2  3   cid:11   cid:12   cid:12    cid:12   cid:11   cid:12    cid:12   cid:12   cid:11   where  cid:12  = 1  6  cid:0   cid:11   2 .  Note: This is a joint, not a conditional, probability mass function.    a  Find the Slepian Wolf rate region for this source.  b  What is PrfX = Y g in terms of  cid:11  ?   Network Information Theory  353   c  What is the rate region if  cid:11  = 1  d  What is the rate region if  cid:11  = 1  3 ? 9 ?  Solution: Slepian Wolf   a  H X; Y   =  cid:0 P p x; y  log p x; y  =  cid:0 3 cid:11  log  cid:11   cid:0  6 cid:12  log  cid:12  . Since X and Y are  uniformly distributed  H X  = H Y   = log 3  and  Hence the Slepian Wolf rate region is  H XjY   = H Y jX  = H 3 cid:11 ; 3 cid:12 ; 3 cid:12    R1  cid:21  H XjY   = H 3 cid:11 ; 3 cid:12 ; 3 cid:12   R2  cid:21  H Y jX  = H 3 cid:11 ; 3 cid:12 ; 3 cid:12    R1 + R2  cid:21  H X; Y   = H 3 cid:11 ; 3 cid:12 ; 3 cid:12   + log 3   b  From the joint distribution, Pr X = Y   = 3 cid:11  .  c  If  cid:11  = 1  3 ,  cid:12  = 0 , and H XjY   = H Y jX  = 0 . The rate region then becomes   d  If  cid:11  = 1  9 ,  cid:12  = 1  9 , and H XjY   = H Y jX  = log 3 . X and Y are independent,  and the rate region then becomes   15.151    15.152    15.153    15.154    15.155    15.156    15.157    15.158    15.159    15.160    15.161   18. Square channel  What is the capacity of the following multiple access channel?  R1  cid:21  0 R2  cid:21  0  R1 + R2  cid:21  log 3  R1  cid:21  log 3 R2  cid:21  log 3 R1 + R2  cid:21  2 log 3  X1 2 f cid:0 1; 0; 1g X2 2 f cid:0 1; 0; 1g 1 + X 2 Y 2  = X 2   a  Find the capacity region.  b  Describe p cid:3  x1 ; p cid:3  x2  achieving a point on the boundary of the capacity region.  Solution: Square channel   354  Network Information Theory   a  If we let U1 = X 2  1 and U2 = X 2  2 , then the channel is equivalent to a sum multiple access channel Y = U1 + U2 . We could also get the same behaviour by using only two input symbols  0 and 1  for both X1 and X2 . Thus the capacity region  R1 < I X1; Y jX2  = H Y jX2  R2 < I X2; Y jX1  = H Y jX1   R1 + R2 < I X1; X2; Y   = H Y     15.162    15.163    15.164   By choosing p x1; x2  = 1=4 for  x1; x2  =  1; 0 ;  0; 0 ;  0; 1 ;  1; 1  and 0 other- wise, we obtain H Y jX1  = H Y jX2  = 1 , H Y   = 1:5 , and by the results for the binary erasure multiple access channel, the capacity of the channel is limited by  R1 < 1 R2 < 1  R1 + R2 < 1:5   15.165    15.166    15.167    b  One possible distribution that achieves points on the boundary of the rate region  is given by the distribution in part  a .  19. Slepian-Wolf: Two senders know random variables U1 and U2 respectively. Let the  random variables  U1; U2  have the following joint distribution:  U1nU2  0 1 2 ...  m  cid:0  1  0  cid:11   cid:13    cid:13   m cid:0 1 m cid:0 1 ... m cid:0 1   cid:13   1  cid:12  m cid:0 1 0 0 ... 0  2  cid:12  m cid:0 1 0 0 ... 0   cid:1  cid:1  cid:1  m  cid:0  1  cid:12   cid:1  cid:1  cid:1  m cid:0 1  cid:1  cid:1  cid:1  0  cid:1  cid:1  cid:1  0 ... . . .  cid:1  cid:1  cid:1  0  where  cid:11  +  cid:12  +  cid:13  = 1 . Find the region of rates  R1; R2  that would allow a common receiver to decode both random variables reliably.  Solution: Slepian-Wolf  For this joint distribution,  H U1  = H  cid:11  +  cid:12 ;  ; : : : ;    = H  cid:11  +  cid:12 ;  cid:13   +  cid:13  log m  cid:0  1    15.168   m  cid:0  1   cid:13    cid:12   m  cid:0  1   cid:13    cid:12   H U2  = H  cid:11  +  cid:13 ;  H U1; U2  = H cid:18  cid:11 ;  m  cid:0  1 ; : : : ;   cid:12   ; : : : ;    = H  cid:11  +  cid:13 ;  cid:12   +  cid:12  log m  cid:0  1  m  cid:0  1  cid:13  ;  ; : : : ;   cid:13    cid:12   m  cid:0  1  m  cid:0  1  m  cid:0  1  m  cid:0  1 cid:19  = H  cid:11 ;  cid:12 ;  cid:13  + cid:12  log m cid:0 1 + cid:13  log m cid:0 1    15.170    15.169    Network Information Theory  H U1jU2  = H  cid:11 ;  cid:12 ;  cid:13    cid:0  H  cid:11  +  cid:13 ;  cid:12   +  cid:13  log m  cid:0  1  H U2jU1  = H  cid:11 ;  cid:12 ;  cid:13    cid:0  H  cid:11  +  cid:12 ;  cid:13   +  cid:12  log m  cid:0  1   and hence the Slepian Wolf region is  R1  cid:21  H  cid:11 ;  cid:12 ;  cid:13    cid:0  H  cid:11  +  cid:13 ;  cid:12   +  cid:13  log m  cid:0  1  R2  cid:21  H  cid:11 ;  cid:12 ;  cid:13    cid:0  H  cid:11  +  cid:12 ;  cid:13   +  cid:12  log m  cid:0  1  R1 + R2  cid:21  H  cid:11 ;  cid:12 ;  cid:13   +  cid:12  log m  cid:0  1  +  cid:13  log m  cid:0  1   20. Multiple access.   a  Find the capacity region for the multiple access channel  355   15.171    15.172    15.173    15.174    15.175   Y = X X2  1  X1 cid:15 f2; 4g ; X2 cid:15 f1; 2g :  where  not?   b  Suppose the range of X1 is f1; 2g . Is the capacity region decreased? Why or why  Solution: Multiple access.   a  With X1 2 f2; 4g; X2 2 f1; 2g , the channel Y = X X2  1  behaves as:  We compute  R1  cid:20  I X1; Y jX2  = I X1; X X2 R2  cid:20  I X2; Y jX1  = I X2; X X2  1 jX2  = H X1  = 1 bit per trans 1 jX1  = H X2  = 1 bit per trans  R1 + R2  cid:20  I X1; X2; Y   = H Y    cid:0  H Y jX1; X2  = H Y   = where the bound on R1 + R2 is met at the corners in the picture below, where either sender 1 or 2 sends 1 bit per transmission and the other user treats the 2 = 1 channel as a binary erasure channel with capacity 1  cid:0  perasure = 1  cid:0  1 2 bits per use of the channel. Other points on the line are achieved by timesharing.  bits per trans;  3 2   b  With X1 2 f1; 2g; X2 2 f1; 2g , the channel Y = X X2  1  behaves as:  X1 X2 1 2 1 4 2 2 4 2  Y 2 4 4 16  X1 X2 Y 1 1 2 2 1 1 2 4  1 1 2 2   356  Network Information Theory  Note when X1 = 1 , X2 has no e cid:11 ect on Y and can not be recovered given X1 and Y . If X1  cid:24  Br  cid:11   and X2  cid:24  Br  cid:12   then: R1  cid:20  I X1; Y jX2  = I X1; X X2 R2  cid:20  I X2; Y jX1  = H Y jX1   cid:0  H Y jX1; X2  = H Y jX1   jX2  = H  cid:11    1  = p X1 = 1 H Y jX1 = 1  + p X1 = 2 H Y jX1 = 2  =  cid:11 H  cid:12    R1 + R2  cid:20  I X1; X2; Y   = H Y    cid:0  H Y jX1; X2  = H Y    = H  cid:11  cid:12 ;  cid:11  cid:12 ; 1  cid:0   cid:11  cid:12   cid:0   cid:11  cid:12   = H  cid:11   +  cid:11 H  cid:12    We may choose  cid:12  = 1  2 to maximize the above bounds, giving  R1  cid:20  H  cid:11   R2  cid:20   cid:11   R1 + R2  cid:20  H  cid:11   +  cid:11   Above, we plot the region for X1 2 f2; 4g  solid line  against that when X1 2 f1; 2g  dotted . What we  cid:12 nd is that, surprisingly, the rate region from the  cid:12 rst case is not reduced in the second. In fact, neither region contains the other, so for each version of this channel, there are achievable rate pairs which are not achievable in the other.  21. Broadcast Channel. Consider the following degraded broadcast channel.  0  -  1  cid:0   cid:11 1 HHHHHHj  cid:11 1  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8 * 1  cid:0   cid:11 1   cid:11 1  -  1 X  0  E  1 Y1  1  -  -  1  cid:0   cid:11 2 HHHHHHj  cid:11 2  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8 * 1  cid:0   cid:11 2   cid:11 2  -  0  E  1 Y2   a  What is the capacity of the channel from X to Y1 ?  b  From X to Y2 ?  c  What is the capacity region of all  R1; R2  achievable for this broadcast channel?  Simplify and sketch.  Solution: Broadcast Channel.   a  The channel from X to Y1 is a standard erasure channel with probability of  erasure =  cid:11 1 , and hence the capacity is 1  cid:0   cid:11 1   b  We can show that the e cid:11 ective channel from X to Y2 is a binary erasure channel with erasure probability  cid:11 1 +  cid:11 2  cid:0   cid:11 1 cid:11 2 , and hence the capacity is 1  cid:0   cid:11 1  cid:0   cid:11 2 +  cid:11 1 cid:11 2 =  1  cid:0   cid:11 1  1  cid:0   cid:11 2    Network Information Theory  357   c  As in Problem 15.13, the auxiliary random variable U in the capacity region of  the broadcast channel has to be binary. Hence we have the following picture  We can now evaluate the capacity region for this choice of auxiliary random variable. By symmetry, the best distribution for U is the uniform. Let  cid:11  =  cid:11 1 +  cid:11 2  cid:0   cid:11 1 cid:11 2 , and therefore 1  cid:0   cid:11  =  cid:11  =  cid:11 1 cid:11 2 . Hence  R2 = I U ; Y2   2   cid:11   ;  cid:11 ;  = H Y2   cid:0  H Y2jU   = H cid:18   cid:11  = H  cid:11   +  cid:11 H cid:18  1 =  cid:11  1  cid:0  H  cid:12   :  2 cid:19   cid:0  H   cid:12  cid:11 1 cid:11 2;  cid:11 1 +  cid:11 1 cid:11 2;  cid:12  cid:11 1 cid:11 2   2 cid:19   cid:0  H  cid:11    cid:0   cid:11 H  cid:12 ;  cid:12    Also  R1 = I X; Y1jU    = H Y1jU    cid:0  H Y1jU; X  = H  cid:12  cid:11 1;  cid:11 1;  cid:12  cid:11 1   cid:0  H  cid:11 1  =  cid:11 1H  cid:12   + H  cid:11 1   cid:0  H  cid:11 1  =  cid:11 1H  cid:12     15.176    15.177    15.178    15.179    15.180    15.181    15.182    15.183    15.184    15.185   These two equations characterize the boundary of the capacity region as  cid:12  varies. When  cid:12  = 0 , then R1 = 0 and R2 =  cid:11  . When  cid:12  = 1 2 , we have R1 =  cid:11 1 and R2 = 0 . The capacity region is sketched in Figure 15.13.  22. Stereo. The sum and the di cid:11 erence of the right and left ear signals are to be individu- ally compressed for a common receiver. Let Z1 be Bernoulli  p1  and Z2 be Bernoulli  p2  and suppose Z1 and Z2 are independent. Let X = Z1 + Z2 , and Y = Z1  cid:0  Z2 .   a  What is the Slepian Wolf rate region of achievable  RX; RY   ?  -  X  RX  -  Y  RY  -  -  Decoder -    ^X; ^Y     b  Is this larger or smaller than the rate region of  RZ1; RZ2  ? Why?   358  Network Information Theory  -  Z1  RZ1  -  Z2  RZ2  -  -  Decoder -    ^Z1; ^Z2   There is a simple way to do this part.  Solution: Stereo.  The joint distribution of X and Y is shown in following table  Z1 Z2 X Y 0 0 -1 0 1 1 0 1  0 1 0 1  0 1 1 2  probability   1  cid:0  p1  1  cid:0  p2    1  cid:0  p1 p2 p1 1  cid:0  p2   p1p2  and hence we can calculate  H X  = H p1p2; p1 + p2  cid:0  2p1p2;  1  cid:0  p1  1  cid:0  p2   H Y   = H p1p2 +  1  cid:0  p1  1  cid:0  p2 ; p1  cid:0  p1p2; p2  cid:0  p1p2   H X; Y   = H Z1; Z2  = H p1  + H p2    15.186    15.187    15.188   and  and therefore  H XjY   = H p1  + H p2   cid:0  H p1p2 +  1  cid:0  p1  1  cid:0  p2 ; p1  cid:0  p1p2; p2  cid:0  p1p2   15.189  H Y jX  = H p1  + H p2   cid:0  H p1p2; p1 + p2  cid:0  2p1p2;  1  cid:0  p1  1  cid:0  p2    15.190   The Slepian Wolf region in this case is  R1  cid:21  H XjY   = H p1  + H p2   cid:0  H p1p2 +  1  cid:0  p1  1  cid:0  p2 ; p1  cid:0  p1p2; p2  cid:0  p1p2   15.191  R2  cid:21  H Y jX  = H p1  + H p2   cid:0  H p1p2; p1 + p2  cid:0  2p1p2;  1  cid:0  p1  1  cid:0  p2   15.192   15.193   R1 + R2  cid:21  H p1  + H p2   23. The Slepian Wolf region for  Z1; Z2  is  R1  cid:21  H Z1jZ2  = H p1  R2  cid:21  H Z2jZ1  = H p2   R1 + R2  cid:21  H Z1; Z2  = H p1  + H p2    15.194    15.195    15.196   which is a rectangular region.  The minimum sum of rates is the same in both cases, since if we knew both X and Y , we could  cid:12 nd Z1 and Z2 and vice versa. However, the region in part  a  is usually pentagonal in shape, and is larger than the region in  b .   Network Information Theory  359  24. Multiplicative multiple access channel. Find and sketch the capacity region of  the multiplicative multiple access channel  X1  X2  HHHHHj  cid:8  cid:8  cid:8  cid:8  cid:8 *   cid:19  cid:16 t  cid:18  cid:17   - Y  with X1 2 f0; 1g , X2 2 f1; 2; 3g , and Y = X1X2: Solution: Multiplicative multiple access channel.  Since Y = X1X2 , if X1 = 0 , Y = 0 and we receive no information about X2 . When X1 = 1 , Y = X2 , and we can decode X2 perfectly, thus we can achieve a rate R1 = 0; R2 = log 3 .  Let  cid:11  be the probability that X1 = 1 . By symmetry, X2 should have an uniform distribution on f1; 2; 3g . The capacity region of the multiple access channel  I X1; Y jX2  = H X1jX2   cid:0  H X1jY; X2  = H X1  = H  cid:11   I X2; Y jX1  = H Y jX1  =  cid:11 H X2  =  cid:11  log 3 I X1; X2; Y   = H Y   = H 1  cid:0   cid:11 ;    = H  cid:11   +  cid:11  log 3   cid:11  3   cid:11  3   cid:11  3  ;  ;   15.197    15.198    15.199   Thus the rate region is characterized by the equations  R1  cid:20  H  cid:11   R2  cid:20   cid:11  log 3   15.200    15.201   where  cid:11  varies from 0 to 1  The maximum value for R1 occurs for  cid:11  = 1 rates occurs  by calculus  at  cid:11  = 3 4 .  2 . The maximum value for the sum of the  25. Distributed data compression. Let Z1; Z2; Z3 be independent Bernoulli  p  . Find  the Slepian-Wolf rate region for the description of  X1; X2; X3  where  X1 = Z1 X2 = Z1 + Z2 X3 = Z1 + Z2 + Z3 :   360  Network Information Theory  -  X1  -  X2  -  X3  -  -  -  -   ^X1; ^X2; ^X3   Solution: Distributed data compression.  To establish the rate region, appeal to Theorem 14.4.2 in the text, which generalizes the case with two encoders. The inequalities de cid:12 ning the rate region are given by  R S  > H X S jX Sc    for all S  cid:18  f1; 2; 3g , and R S  =Pi2S Ri . The rest is calculating entropies H X S jX S c   for each S . We have  H1 = H X1  = H Z1  = H p ; H2 = H X2  = H Z1 + Z2  = H p2; 2p 1  cid:0  p ;  1  cid:0  p 2 ; H3 = H X3  = H Z1 + Z2 + Z3   = H p3; 3p2 1  cid:0  p ; 3p 1  cid:0  p 2;  1  cid:0  p 3 ; H12 = H X1; X2  = H Z1; Z2  = 2H p ; H13 = H X1; X3  = H X1  + H X3jX1  = H X1  + H Z2 + Z3   = H p2; 2p 1  cid:0  p ;  1  cid:0  p 2  + H p ;  H23 = H X2; X3  = H X2  + H X3jX2  = H X2  + H Z3   = H p2; 2p 1  cid:0  p ;  1  cid:0  p 2  + H p ;  and  H123 = H X1; X2; X3  = H Z1; Z2; Z3  = 3H p :  Using the above identities and chain rule, we obtain the rate region as  R1 > H X1jX2; X3  = H123  cid:0  H23  = 2H p   cid:0  H p2; 2p 1  cid:0  p ;  1  cid:0  p 2  = 2p 1  cid:0  p ;  R2 > H X2jX1; X3  = H123  cid:0  H13 = 2p 1  cid:0  p ; R3 > H X3jX1; X2  = H123  cid:0  H12 = H p ;  R1 + R2 > H X1; X2jX3  = H123  cid:0  H3  = 3H p   cid:0  H p3; 3p2 1  cid:0  p ; 3p 1  cid:0  p 2;  1  cid:0  p 3  = 3p 1  cid:0  p  log 3 ;   Network Information Theory  361  R1 + R3 > H X1; X3jX2  = H123  cid:0  H2  = 3H p   cid:0  H p2; 2p 1  cid:0  p ;  1  cid:0  p 2  = H p  + 2p 1  cid:0  p ;  R2 + R3 > H X2; X3jX1  = H123  cid:0  H1 = 2H p ;  and  R1 + R2 + R3 > H123 = 3H p :  26. Noiseless multiple access channel Consider the following multiple access channel  with two binary inputs X1; X2 2 f0; 1g and output Y =  X1; X2  .   Simpli cid:12 cations are contributed by KBS.    a  Find the capacity region. Note that each sender can send at capacity.  b  Now consider the cooperative capacity region, R1  cid:21  0; R2  cid:21  0; R1+R2  cid:20  maxp x1;x2  I X1; X2; Y   .  Argue that the throughput R1 + R2 does not increase, but the capacity region increases.  Solution: Noiseless multiple access channel   a  Since Y =  X1; X2  , I X1; Y jX2  = H X1jX2  = H X1   cid:20  1 , and I X1; X2; Y   = H X1; X2   cid:20  2 , and hence the capacity region of the MAC becomes R1  cid:20  1 , R2  cid:20  1 , R1 + R2  cid:20  2 .   b  The cooperative capacity region is R1 + R2  cid:20  maxp x1;x2  I X1; X2; Y   = 2 . Thus, the cooperative capacity has the same sum of rates, but with cooperation, one of the senders could send 2 bits  while the other rate is 0 . Thus the capacity region increases from the square   R1  cid:20  1 , R2  cid:20  1   to the triangle R1 + R2  cid:20  2 .  27. In cid:12 nite bandwidth multiple access channel Find the capacity region for the Gaus- sian multiple access channel with in cid:12 nite bandwidth. Argue that all senders can send at their individual capacities, i.e., in cid:12 nite bandwidth eliminates interference.  Solution: In cid:12 nite bandwidth multiple access channel  The capacity of a Gaussian multiple access channel with bandwidth W is given by the following rate region  R1  cid:20  W log cid:18 1 + R2  cid:20  W log cid:18 1 + R1 + R2  cid:20  W log cid:18 1 +  P2  P1  N W cid:19  N W cid:19  N W  cid:19   P1 + P2   15.202    15.203    15.204   A hueristic argument to prove this follows from the single user Gaussian channel ca- pacity with bandwidth W combined with \onion-peeling" and timesharing.   362  Network Information Theory  As W ! 1 , these bounds reduce to  R1  cid:20  R2  cid:20  R1 + R2  cid:20   P1 N P2 N P1 + P2  N  which is a rectangular region corresponding to no interference between the two senders.  28. A multiple access identity.  Let C x  = 1 to noise ratio x . Show  2 log 1 + x  denote the channel capacity of a Gaussian channel with signal  C cid:18  P1  N cid:19  + C cid:18  P2  P1 + N cid:19  = C cid:18  P1 + P2 N  cid:19  :  This suggests that 2 independent users can send information as well as if they had pooled their power.  Solution: A multiple access identity.  P1 + P2  C   N    =  log 1 +  P1 + P2  N + P1 + P2        N  N  1 2 1 2 1 2 1 2  log   log   log   =  =  =  N + P1 + P2  N + P1  N + P1  N + P1 + P2  N + P1   cid:1    +  N 1 2     N + P1     log   N  = C     + C   P2  P1 + N  P N1     29. Frequency Division Multiple Access  FDMA . Maximize the throughput R1 + N  W cid:0 W1    over W1 to show that bandwidth  R2 = W1 log 1 + P1 N W1 should be proportional to transmitted power for FDMA.    +  W  cid:0  W1  log 1 +  P2  Solution: Frequency Division Multiple Access  FDMA . Allocating bandwidth W1 and W2 = W  cid:0  W1 to the two senders, we can achieve the following rates  R1 = W1 log cid:18 1 + R2 = W2 log cid:18 1 +  P1  N W1 cid:19  ; N W2 cid:19  :  P2   15.205    15.206    15.207    15.208    15.209    15.210    15.211    15.212    15.213    15.214    Network Information Theory  363  To maximimize the sum of the rates, we write  R = R1 + R2 = W1 log cid:18 1 +  P1  N W1 cid:19  +  W  cid:0  W1  log cid:18 1 +  P2  N  W  cid:0  W1  cid:19   15.215   and di cid:11 erentiating with respect to W1 , we obtain  log cid:18 1 +  P1  N W1 cid:19  +   cid:0 log cid:18 1 +  W1  N W1  cid:18  cid:0  N  W  cid:0  W1  cid:19  +  1 + P1 P2  P1  N W 2  1 cid:19  N  W cid:0 W1   cid:18  W  cid:0  W1  P2  1 +  P2  N  W  cid:0  W1 2 cid:19  = 0  15.216   Instead of solving this equation, we can verify that if we set  so that  W1 =  P1  W  P1 + P2  P1  P2  =  N W1  N W2  =  P1 + P2  N W   15.217    15.218   that  15.216  is satis cid:12 ed, and that using bandwidth proportional to the power optimizes the total rate for Frequency Division Multiple Access.  30. Trilingual speaker broadcast channel  A speaker of Dutch, Spanish and French wishes to communicate simultaneously to three people: D; S; and F . D knows only Dutch, but can distinguish when a Spanish word is being spoken as distinguished from a French word, similarly for the other two, who know only Spanish and French respectively, but can distinguish when a foreign word is spoken and which language is being spoken.  Suppose each language, Dutch, Spanish, and French, has M words: M words of Dutch, M words of French, and M words of Spanish.   a  What is the maximum rate at which the trilingual speaker can speak to D ?   b  If he speaks to D at the maximum rate, what is the maximum rate he can simul-  taneously speak to S ?   c  If he is speaking to D and S at the above joint rate, can he also speak to F at  some positive rate? If so, what is it? If not, why not?  Solution: Trilingual speaker broadcast channel   a  Speaking Dutch gives M words, and in addition two words for the distinguisha-  bility of French and Spanish from Dutch, thus log M + 2  bits.   b  Transmitting log M bits for a fraction of 1= M + 2  of the time gives R =   log M  = M + 2  .   c  Same reasoning as in  b  gives R =  log M  = M + 2  .   364  Network Information Theory  31. Parallel Gaussian channels from a mobile telephone  Assume that a sender X is sending to two  cid:12 xed base stations.  Assume that the sender sends a signal X that is constrained to have average power P . Assume that the two base stations receive signals Y1 and Y2 , where  Y1 =  cid:11 1X + Z1 Y2 =  cid:11 2X + Z2  where Zi  cid:24  N  0; N1  , Z2  cid:24  N  0; N2  , and Z1 and Z2 are independent. We will assume the  cid:11  ’s is constant over a transmitted block.   a  Assuming that both signals Y1 and Y2 are available at a common decoder Y =  Y1; Y2 ; what is the capacity of the channel from the sender to the common receiver?   b  If instead the two receivers Y1 and Y2 each independently decode their signals, this becomes a broadcast channel. Let R1 be the rate to base station 1 and R2 be the rate to base station 2. Find the capacity region of this channel.  Solution: Parallel Gaussian channels from a mobile telephone   a  Let Y = [Y1; Y2]T . Obviously,  thus it is clear that the maximizing distribution on X is Gaussian N  0; P   . There- fore we have  and consequently, by independence of the noises  I X; Y   = h Y1; Y2   cid:0  h Z1; Z2   h Y1; Y2  =  1 2  log 2 cid:25 ejKY j  C =  1 2  log jKY j N1N2  :  Plugging in jKY j =  1  cid:0   cid:11  2P N1 +  cid:11 2P N2 + N1N2 we have  C =  log 1 +  1 2   1  cid:0   cid:11  2P  +  N2   cid:11 2P  N1 !  :   b  The problem is equivalent to the degraded broadcast channel with  Y1 = X + Z1= cid:11  Y2 = X + Z2= 1  cid:0   cid:11    :  Thus, the noise is N  0; N1= cid:11 2  and N  0; N2= 1 cid:0  cid:11  2  . Without loss of generality assume that N2= 1  cid:0   cid:11  2 > N1= cid:11 2 . Then, referring to Example 14.6.6. in Cover   Network Information Theory  365  and Thomas, the rate region is  N1 ! R1 < C  cid:18  cid:11 2P R2 < C  1  cid:0   cid:18   1  cid:0   cid:11  2P  cid:18  1  cid:0   cid:11  2P + N2!  ; 0  cid:20   cid:18   cid:20  1 :  32. Gaussian multiple access.  A group of m users, each with power P , is using a Gaussian multiple access channel at capacity, so that  m  Xi=1  N  cid:19  ; Ri = C cid:18  mP   15.219   where C x  = 1  2 log 1 + x  and N is the receiver noise power.  A new user of power P0 wishes to join in.   a  At what rate can he send without disturbing the other users?   b  What should his power P0 be so that the new users rate is equal to the combined  communication rate C mP=N   of all the other users?  Solution: Gaussian multiple access.   a  If the new user can be decoded while treating all the other senders as part of the noise, then his signal can be subtracted out before decoding the other senders, and hence will not disturb the rates of the other senders. Therefore if  R0 <  1 2  log cid:18 1 +  P0  mP + N cid:19  ;  the new user will not disturb the other senders.   b  The new user will have a rate equal to the sum of the existing senders if  or  1 2  log cid:18 1 +  P0  mP + N cid:19  =  1 2  log cid:18 1 +  mP  N  cid:19   P0 =  mP + N    mP N  33. Converse for deterministic broadcast channel.  A deterministic broadcast channel is de cid:12 ned by an input X , two outputs, Y1 and Y2 which are functions of the input X . Thus Y1 = f1 X  and Y2 = f2 X  . Let R1 and R2 be the rates at which information can be sent to the two receivers. Prove that   15.220    15.221    15.222    15.223    15.224    15.225   R1  cid:20  H Y1  R2  cid:20  H Y2   R1 + R2  cid:20  H Y1; Y2    366  Network Information Theory  Solution: Converse for deterministic broadcast channel.  We can derive this bound from single user arguments. The maximum rate that the sender can send information to receiver 1 is less than  R1  cid:20  I X; Y1  = H Y1   cid:0  H Y1jX  = H Y1    15.226   since the channel is deterministic and therefore H Y1jX  = H Y2jX  = 0 . Similarly, R2  cid:20  H Y2  . Also, if the receivers cooperated with each other, the capacity  R1 + R2  cid:20  I X; Y1; Y2  = H Y1; Y2    15.227   since the sum of rates to the two receivers without cooperation cannot be greater than the single user capacity of a channel from X to  Y1; Y2  .  34. Multiple access channel  Consider the multiple access channel Y = X1+X2  mod 4 , where X1 2 f0; 1; 2; 3g; X2 2 f0; 1g .  a  Find the capacity region  R1; R2  .  b  What is the maximum throughput R1 + R2 ?  Solution: Multiple access channel   a  The MAC capacity region is given by the standard set of equations which reduce  as follows since there is no noise:  R1 < I X1; Y jX2  = H Y jX2   cid:0  H Y jX1; X2  = H Y jX2  = H X1  R2 < I X2; Y jX1  = H Y jX1   cid:0  H Y jX1; X2  = H Y jX1  = H X2   R1 + R2 < I X1; X2; Y   = H Y    cid:0  H Y jX1; X2  = H Y    Since entropy is maximized under a uniform distribution over the  cid:12 nite alphebet, R1 < H X1   cid:20  2 , R2 < H X2   cid:20  1 , and R1 + R2 < H Y    cid:20  2 . Further, if X1  cid:24  unif  0; 1; 2; 3  , and X2  cid:24  unif  0; 1  then Y  cid:24  unif  0; 1; 2; 3  , so the upper bounds are achieved. This gives the capacity region in Figure 15.14.   b  The throughput of R1 + R2  cid:20  2 by the third constraint above, and is achieved at many points including when R1 = 2 and R2 = 0 . So the maximum throughput is R1 + R2 = 2 .  35. Distributed source compression  Let  q;  0;  Z1 =  1; p Z2 =  1; p  0;  q;   Network Information Theory  367  and let U = Z1Z2; V = Z1 + Z2: Assume Z1 and Z2 are independent. This induces a joint distribution on  U; V   . Let  Ui; Vi  be iid according to this distribution. Sender 1 describes U n at rate R1 , and sender 2 describes V n at rate R2 .   a  Find the Slepian-Wolf rate region for recovering  U n; V n  at the receiver.   b  What is the residual uncertainty  conditional entropy  that the receiver has about   X n; Y n  .  Solution: Distributed source compression   a  Below is a table listing the possible results and their associated probabilities.  Z1 Z2 U V 0 0 1 0 1 1 1 2  0 0 0 1  0 1 0 1  Prob q2 pq pq p2  Evaluating the three standard inequalities for the Slepian-Wolf rate region gives the following:  R1 > H UjV   = 0 R2 > H V jU   = P r U = 0 H V jU = 0  =  1  cid:0  p2 H  q2 1  cid:0  p2! R1 + R2 > H U; V   = H V   + H UjV   = H V   = H q2; 2pq; p2   Where the  cid:12 rst equation comes because U is a deterministic function of V . The second equation comes from the de cid:12 nition of conditional entropy and noting that H V jU = 1  = 0 . The Slepian-Wolf rate region is depicted in  cid:12 gure 15.15.   b  The residual uncertainty is given by H Z n  2 jU n; V n  = nH Z1; Z2jU; V   be- cause everything is iid. Since there is only uncertainty in  Z1; Z2  when  U = 0; V = 1  , the residual uncertainty simpli cid:12 es to nP r U = 0; V = 1 H Z1; Z2jU = 0; V = 1  = n 2pq H cid:16  1  2 cid:17  = 2pqn .  1 ; Z n  36. MAC capacity with costs  The cost of using symbol x is r x  . The cost of a codeword xn is r xn  = 1 A  2nR; n  codebook satis cid:12 es cost constraint r if 1  i=1 r xi  . i=1 r xi w    cid:20  r; for all w 2 2nR .  a  Find an expression for the capacity C r  of a discrete memoryless channel with  nPn  nPn  cost constraint r .   b  Find an expression for the multiple access channel capacity region for  X1  cid:2  X2; p yjx1; x2 ;Y  if sender X1 has cost constraint r1 and sender X2 has cost constraint r2 .   368  Network Information Theory   c  Prove the converse for  b .  Solution: MAC capacity with costs   a  The capacity of a discrete memoryless channel with cost constraint r is given by  C r  =  I X; Y  :   15.228   max  p x :Px p x r x  cid:20 r  The achievability follows immediately from Shannon’s ‘average over random code- books’ method and joint typicality decoding.  See Section 9.1 for the power con- straint example.  For the converse, we need to establish following simple properties of the capacity- cost function C r  .  Theorem 15.0.4 The capacity cost function C r  given in  15:228  is a non- decreasing concave function of r .  Remark: These properties of the capacity cost function C r  exactly parallel those of the rate distortion function R D  .  See Lemma 10.4.1 of the text.  Proof: The monotonicity is a direct consequence of the de cid:12 nition of C r  . To prove the concavity, consider two points  C1; r1  and  C2; r2  which lie on the capacity cost curve. Let the distributions that achieve these pairs be p1 x  and p2 x  . Consider the distribution p cid:21  =  cid:21 p1 +  1  cid:0   cid:21  p2: Since the cost is a linear function of the distribution, we have r p cid:21   =  cid:21 r1 +  1 cid:0   cid:21  r2: Mutual information, on the other hand, is a concave function of the input distribution  Theorem 2.7.4  and hence  C  cid:21 r1 +  1  cid:0   cid:21  r2  = C r p cid:21     cid:21  Ip cid:21  X; Y    cid:21   cid:21 Ip1 X; Y   +  1  cid:0   cid:21  Ip2 X; Y   =  cid:21 C r1  +  1  cid:0   cid:21  C r2 ;   15.229    15.230    15.231    15.232   which proves that C r  is concave in r . 2 Now we are ready to prove the converse. Consider any  2nR; n  code that satis cid:12 es the cost constraint  r xi w    cid:20  r for w = 1; 2; : : : ; 2nR , which in turn implies that  E r Xi    cid:20  r;   15.233   where the expectation is with respect to the uniformly drawn message index W . As in the case without the cost constraint, we begin with Fano’s inequality to obtain the following chain of inequalities:  1 n  n  Xi=1  1 n  n  Xi=1   Network Information Theory  nR = H W     cid:20  I W ; Y n  + n cid:15 n  cid:20  I X n; Y n  + n cid:15 n  cid:20  H Y n   cid:0  H Y njX n  + n cid:15 n  cid:20   H Yi   cid:0  H YijX n; Y i cid:0 1  + n cid:15 n  n   a   n  n  n  =  =  Xi=1 Xi=1 Xi=1 Xi=1 Xi=1  cid:20  nC  1 Xi=1  cid:20  nC r  + n cid:15 n;  = n  1 n   cid:20   n  n  n   b    c   H Yi   cid:0  H YijXi  + n cid:15 n  I Xi; Yi  + n cid:15 n  C E r Xi    + n cid:15 n  C E r Xi    + n cid:15 n  E r Xi  ! + n cid:15 n  369   15.234    15.235    15.236    15.237    15.238    15.239    15.240    15.241    15.242    15.243    15.244   where  a  follows from the de cid:12 nition of the capacity cost function,  b  from the concavity of the capacity cost function and Jensen’s inequality, and  c  from Eq.  15.233  and the fact that C r  is non-decreasing in r . Note that we cannot jump from  15.240  to  15.244  since E r Xi   may be greater than r for some i .   b  The capacity region under cost constraints r1 and r2 is given by the closure of  the set of all  R1; R2  pairs satisfying  for some choice of the joint distribution p q p x1jq p x2jq p yjx1; x2  with  R1 < I X1; Y jX2; Q ; R2 < I X2; Y jX1; Q ; R1 + R2 < I X1; X2; Y jQ   Xx1 Xx2  p x1 r1 x1   cid:20  r1;  p x2 r2 x2   cid:20  r2;  and jQj  cid:20  4:   370  Network Information Theory   c  Again the achievability proof is an easy extension from the case without cost constraints. For the converse, consider any sequence of   2nR1 ; 2nR2 ; n  codes with P  n   e ! 0 satisfying  1  nXi nXi  1  r1 x1i w1i    cid:20  r1  r2 x2i w2i    cid:20  r2;  for all w1i = 1; 2; : : : ; 2nR1 ; w2i = 1; 2; : : : ; 2nR2 . By taking expectation with respect to the random message index pair  W1; W2  , we get  1  nXi  1  nXi  E r1 X1i    cid:20  r1  and  E r2 X2i    cid:20  r2:   15.245   By starting from Fano’s inequality and taking the exact same steps as in the converse proof for the MAC without constraints  see Section 14.3.4 of the text , we obtain  n  n  Xi=1 Xi=1 Xi=1  n  nR1  cid:20   I X1i; YijX2i  + n cid:15 1n = nI X1Q; YQjX2Q; Q  + n cid:15 1n;  nR2  cid:20   I X2i; YijX1i  + n cid:15 2n = nI X2Q; YQjX1Q; Q  + n cid:15 2n;  n R1 + R2   cid:20   I X1i; X2i; Yi  + n cid:15 n = nI X1Q; X2Q; YQjQ  + n cid:15 n;  where the random variable Q is uniform over f1; 2; : : : ; ng and independent of  X1i; X2i; Yi  for all i . Now de cid:12 ne X14=X1Q , X24=XQ , and Y 4=YQ . It is easy to check that  Q; X1; X2; Y   have a joint distribution of the form p q p x1jq p x2jq p yjx1; x2  . Moreover, from Eq.  15.245 ,  Xx1  Pr X1 = x1 r1 x1  = Xx1 = Xx1 = Xx1  Pr X1Q = x1 r1 x1   Pr X1Q = x1jQ = i  Pr Q = i r1 x1  1 n  Pr X1i = x1 r1 x1   n  n  n  Xi=1 Xi=1 Xi=1Xx1 Xi=1Xx1  n  =  =  1 n  1 n  Pr X1i = x1 r1 x1   Pr X1i = x1 r1 x1    Network Information Theory  371  Therefore, we have shown that any sequence of   2nR1 ; 2nR2  ; n  codes satisfying cost constraints with P  n   e ! 0 should have the rates satisfying  and similarly,  and  E r1 X1i    n  =  1 Xi=1 n  cid:20  r1; Xx2  Pr X2 = x2 r2 x2   cid:20  r2:  R1 < I X1; Y jX2; Q ; R2 < I X2; Y jX1; Q ; R1 + R2 < I X1; X2; Y jQ   Xx1 Xx2  p x1 r1 x1   cid:20  r1  p x2 r2 x2   cid:20  r2:  for some choice of the joint distribution p q p x1jq p x2jq p yjx1; x2  with  Finally, from Theorem 14.3.4, the region is unchanged if we limit the cardinality of Q to 4, which completes the proof of the converse. Note that, compared to the single user case in part  a , the converse for the MAC with cost constraints is rather straightforward. Here the time sharing random variable Q saves the trouble of dealing with costs at each time index i .   372  R 2  Network Information Theory  R 1  Figure 15.7: Capacity region of degraded broadcast channel  Q  Q  Q  1  cid:0  p  -  cid:17 3  -   cid:17    cid:17   Q  Q  Q   cid:17    cid:17    cid:17   p Q Q  cid:17   cid:17  p   cid:17  Q   cid:17    cid:17   Q  Q  X   cid:17    cid:17    cid:17    cid:17   Y1  Q  Q  Q  Qs - -  1  cid:0  p  -  1  cid:0   cid:11    cid:11   PPPPPPPPPPPPPPPq  cid:16  cid:16  cid:16  cid:16  cid:16  cid:16  cid:16  cid:16  cid:16  cid:16  cid:16  cid:16  cid:16  cid:16  cid:16 1  -   cid:11   Y2  1  cid:0   cid:11   Figure 15.8: Broadcast channel with a binary symmetric channel and an erasure channel   Network Information Theory  373  @  @  @  U   cid:0    cid:0    cid:0   @   cid:0   @  cid:0    cid:0  @   cid:0   @   cid:0    cid:0   @  @  X  Y1  Y2  -  cid:0  cid:18   -  @  -  1  cid:0  p  cid:0   -  cid:0  cid:18    cid:0   @  @   cid:0    cid:0   @   cid:0   p @  cid:0   cid:0  @ p   cid:0   @  @  @ 1  cid:0  p  @R -  -   cid:0   @R -  -  -  1  cid:0   cid:11    cid:11   HHHHHHHHHHj  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8 *  -   cid:11   1  cid:0   cid:11   Figure 15.9: Broadcast channel with auxiliary random variable  Figure 15.10: Capacity region of the broadcast channel  R 2  R 1   374  Network Information Theory  1 R  0.6  1  0.8  0.4  0.2  PSfrag replacements  0  0  0.2  0.4  0.8  1  1.2  0.6 R2  Figure 15.11: Rate regions for X1 2 f2; 4g and X1 2 f1; 2g  @  @  @  U   cid:0    cid:0    cid:0   @   cid:0   @  cid:0    cid:0  @   cid:0   @  -  -  cid:0  cid:18   X  - @R  -   cid:0    cid:0   @  @  @   cid:0   @  -  cid:0  cid:18   1  cid:0   cid:11 1  cid:0   @  cid:11 1 @  HHHHHHHHHHj  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8 *   cid:0   cid:11 1  cid:0    cid:0  @  @  cid:0    cid:0    cid:0   @  @   cid:0   - @R  @ 1  cid:0   cid:11 1  -  Y1  -  -  1  cid:0   cid:11 2   cid:11 2  HHHHHHHHHHj  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8  cid:8 *   cid:11 2  -  - Y2  1  cid:0   cid:11 2  Figure 15.12: Broadcast channel with auxiliary random variable   Network Information Theory  375  Figure 15.13: Capacity region of the broadcast channel  R 2  R 1   376  Network Information Theory  R2  6  1  0  @  @  @  @  @  @  @  @  @  1  @ 2  - R1  Figure 15.14: MAC Capacity Region  R2  6  @  @  @  @  @  @  @@   1  cid:0  p2 H cid:16  q2 1 cid:0 p2 cid:17   0  H cid:0 p2 cid:1   Figure 15.15: SW Rate Region  - R1   Chapter 16  Information Theory and Portfolio Theory  1. Growth rate. Let  X =   1; a ;  with probability 1=2  1; 1=a ; with probability 1=2  ;  where a > 1 . This vector X represents a stock market vector of cash vs. a hot stock. Let  W  b; F   = E log btX;  W  cid:3  = max  W  b; F    b  and  be the growth rate.   a  Find the log optimal portfolio b cid:3 :  b  Find the growth rate W  cid:3 :  c  Find the asymptotic behavior of  Sn =  btXi  n  Yi=1  for all b.  Solution: Doubling Rate.   a  Let the portfolio be  1  cid:0  b2; b2  . Then  W  b; F   =  ln 1  cid:0  b2 + ab2  +  ln 1  cid:0  b2 +  1 2  b2 a   :  1 2  377   16.1     16.2    16.3    16.4    16.5    16.6    16.7    16.8   378  Information Theory and Portfolio Theory  Di cid:11 erentiating to  cid:12 nd the maximum, we have  dW db2  =  1 2  a  cid:0  1  1  cid:0  b2 + ab2  cid:0   1  cid:0  1  a  1  cid:0  b2 + b2  a  = 0  Solving this equation, we get b cid:3 2 = 1  2 . Hence the log optimal portfolio b cid:3  is   1  2 ; 1  2   .   b  The optimal doubling rate W  cid:3  = W  b cid:3 ; F   is   c  The asymptotic behavior of an in cid:12 nite product of i.i.d. terms is essentially deter-  mined by the expected log of the individual terms.  W  cid:3  =  1 2 1 2  ln cid:18  1  2  ln cid:18  1  2  a  1 2  2 cid:19  +  cid:0  ln 2:  +  a   1 + a 2  =  ln  +  1  2a cid:19   Sn =  n  btXi  Yi=1 nPn = en 1 ! enE ln btX = enW  b;F  ;  i=1  ln btXi  where the convergence is with probability 1 by the strong law of large numbers. We can substitute for W  b; F   from  16.1 .  2. Side information. Suppose, in the previous problem, that  Y =  1;  0;  if  X1; X2   cid:21   1; 1 ; if  X1; X2   cid:20   1; 1 :  Let the portfolio b depend on Y: Find the new growth rate W  cid:3  cid:3  and verify that  cid:1 W = W  cid:3  cid:3   cid:0  W  cid:3  satis cid:12 es   cid:1 W  cid:20  I X; Y  :  Solution: Side Information.  In the previous problem, if we knew Y so that we knew which of the two possible stock vectors would occur, then the optimum strategy is clear. In the case when Y = 1 , we should put all out money in the second stock to maximize the conditional expected log return. Similarly, when Y = 0 , we should put all the money in the  cid:12 rst stock. The average expected log return is  W  cid:3  Y   =  ln a +  ln 1 =  ln a:  1 2  1 2  1 2   16.9    Information Theory and Portfolio Theory  The increase in doubling rate due to the side information is   cid:1 W = W  cid:3  Y    cid:0  W  cid:3  ln  =  ln a  cid:0   a  1 + a  1 2 = ln  cid:20  ln 2;   1 + a 2  a  1 2 + ln 2  + ln 2  since  a 1+a < 1 . Also in this case,  I X; Y   = H Y    cid:0  H Y jX  = H Y   = ln 2; since Y is a function of X and uniformly distributed on f0; 1g . We can hence verify that   cid:1 W  cid:20  I X; Y  : 3. Stock dominance. Consider a stock market vector  Suppose X1 = 2 with probability 1.  X =  X1; X2 :   a  Find necessary and su cid:14 cient conditions on the distribution of stock X2 such that the log optimal portfolio b cid:3  invests all the wealth in stock X2 , i.e., b cid:3  =  0; 1  .   b  Argue for any distribution on X2 that the growth rate satis cid:12 es W  cid:3   cid:21  1 . Solution: Stock Market We have a stock market vector   a  The Kuhn Tucker conditions for the portfolio b =  0; 1  to be optimal is that  with X1 = 2 :  and  X =  X1; X2   E  X2 X2  = 1  E  2 X2  cid:20  1:  The  cid:12 rst is trivial. So the second condition is the only condition on the distribution for the optimal portfolio to be  0,1 .   b  Since the optimal portfolio does better than the  1,0  portfolio  W  cid:3   cid:21  W  b  = W  1; 0  = log 2 = 1:   16.18   379   16.10    16.11    16.12    16.13    16.14    16.15    16.16    16.17    380  Information Theory and Portfolio Theory  4. Including experts and mutual funds. Let X  cid:24  F  x ; x 2 Rm  + be the vector of price relatives for a stock market. Suppose an \expert" suggests a portfolio b . This would result in a wealth factor btX . We add this to the stock alternatives to form ~X =  X1; X2; : : : ; Xm; btX  . Show that the new growth rate  is equal to the old growth rate  ~W  cid:3  =  b1;:::;bm;bm+1Z ln bt ~x dF  ~x   max  W  cid:3  = max  b1;:::;bmZ ln btx dF  x :   16.19    16.20   Solution: Including experts and mutual funds.  This problem asks you to show that the existence of a mutual fund does not fundamen- tally change the market; that is, it asks you to show that you can make as much money without the presence of the mutual fund as you can make with it. This should be obvi- ous, since, if you thought a particular mutual fund would be a good idea to hold, you could always invest in its constituent stocks directly in exactly the same proportions as the mutual fund did.   a  Outline of Proof  We are asked to compare two quantities, ^W  cid:3  and W  cid:3  . ^W  cid:3  is the maximum doubling rate of the \extended" market. That is, it is the maximimum achievable doubling-rate over the set of extended portfolios: those that include investment in the mutual fund. W  cid:3  is the maximum doubling rate of the \non-extended" market. That is, it is the maximum achievable doubling-rate over the set of non- extended portfolios: those without investment in the mutual fund. Our strategy will be to show that the set of achievable doubling rates in the extended market is precisely the same as the set of achievable doubling rates in the non-extended market, and hence that the maximum value on both sets must be the same. In particular, we need to show that for any extended portfolio ^b that achieves some particular doubling rate ^W on the extended market, there exists a correspond- ing non-extended portfolio b that achieves the same doubling rate W = ^W on the non-extended market, and, conversely, that for any non-extended portfolio b achieving some particular doubling-rate on the non-extended market, we can  cid:12 nd an equivalent extended portfolio ^b that achieves the same doubling-rate on the extended market.   b  Converse: W  cid:3   cid:20  ^W  cid:3   The converse is easy. Let b =  b1; b2; : : : ; bm  be any non-extended portfolio. Then clearly the extended portfolio ^b =  b; 0  =  b1; b2; : : : bm; 0  achieves the same doubling rate on the extended market. In particular, then, if b cid:3  achieves W  cid:3  on the non-extended market, then  b cid:3 ; 0  achieves W  cid:3  on the extended market, and so the maximum doubling rate on the extended market must be at least as big as W  cid:3  , that is: W  cid:3   cid:20  ^W  cid:3    Information Theory and Portfolio Theory  381   c  ^W  cid:3   cid:20  W  cid:3   First, some de cid:12 nitions. Let X =  X1; X2; : : : ; Xm  be the non-extended stock- market. Let c =  c1; c2; : : : ; cm  be the portfolio that generates the mutual fund, Xm+1 . Thus, Xm+1 = cT X . Let ^X =  X1; X2; : : : ; Xm; Xm+1  be the extended stock-market. Now consider any extended portfolio ^b =  ^b1; ^b2; : : : ; ^bm; ^bm+1  . The doubling rate ^W associated with the portfolio ^b is  ^W = E[log ^bT ^X ]  = E[log ^b1X1 + ^b2X2 + : : : + ^bmXm + ^bm+1Xm+1 ] = E[log ^b1X1 + ^b2X2 + : : : + ^bmXm + ^bm+1cT X ] = E[log ^b1X1 + ^b2X2 + : : : + ^bmXm + ^bm+1 c1X1 + c2X2 + : : : + cmXm  ] = E[log ^b1X1 + ^b2X2 + : : : + ^bmXm + ^bm+1 c1X1 + c2X2 + : : : + cmXm  ] = E[log  ^b1 + ^bm+1c1 X1 +  ^b2 + ^bm+1c2 X2 + : : : +  ^bm + ^bm+1cm Xm ]  But this last expression can be re-expressed as the doubling rate W associated with the non-extended portfolio b , where bi = ^bi + ^bm+1ci . In particular, then, when ^b = ^b cid:3  is the portfolio achieving the optimal doubling rate ^W  cid:3  , then there is an associated portfolio b, on the non-extended market, given by bi = ^b cid:3 i +^b cid:3 m+1ci that also achieves doubling-rate ^W  cid:3  . Hence, ^W  cid:3   cid:20  W  cid:3  .  Combining the above two inequalities, we must conclude that ^W  cid:3  = W  cid:3  .  5. Growth rate for symmetric distribution. Consider a stock vector X  cid:24  F  x ; X 2 Rm; X  cid:21  0 , where the component stocks are exchangeable. Thus F  x1; x2; : : : ; xm  = F  x cid:27  1 ; x cid:27  2 ; : : : ; x cid:27  m  ; for all permutations  cid:27  .  a  Find the portfolio b cid:3  optimizing the growth rate and establish its optimality.  Now assume that X has been normalized so that symmetric as before.  i=1 Xi = 1; and F is  1  mPm   b  Again assuming X to be normalized, show that all symmetric distributions F  have the same growth rate against b cid:3  .   c  Find this growth rate.  Solution: Growth rate for symmetric distribution.   a  By the assumption of exchangeability, putting an equal amount in each stock is  clearly the best strategy. In fact,  E  Xi bT  cid:3  X  = E  Xi  m cid:0 1P Xi  = 1  ;   382  Information Theory and Portfolio Theory  so b cid:3  satis cid:12 es the Kuhn-Tucker conditions.   b + c  Putting an equal amount in each stock we get  E log bT   cid:3  X = E log = E log 1  1 m  m  Xi=1  Xi  Thus the growth rate is 0.  6. Convexity. We are interested in the set of stock market densities that yield the same + for which b0  optimal porfolio. Let Pb0 be the set of all probability densities on Rm is optimal. Thus Pb0 = fp x  :R ln btx p x dx is maximized by b = b0g . Show that Pb0 is a convex set. It may be helpful to use Theorem 16.2.2.  Solution: Convexity.  Let f1 and f2 be two stock-market densities in the set Pb0 . Since both f1 and f2 are in this set, then, by de cid:12 nition, b0 is the optimal constant-rebalance portfolio when the stock market vector is drawn according to f1 , and it is also the optimal constant- rebalance portfolio when when stock market vector is drawn according to f2 . In order to show that the set Pb0 is convex, we need to show that any arbitrary mixture distribution, f =  cid:21 f1 +  cid:22  cid:21 f2 , is also in the set; that is, we must show that b0 is also the optimal portfolio for f .  We know that W  b; f   is linear in f . So  W  b; f   = W  b;  cid:21 f1 +  cid:22  cid:21 f2   =  cid:21 W  b; f1  +  cid:22  cid:21 W  b; f2   But by assumption each of the summands in the last expression is maximized when b = b0 , so the entire expression is also maximized when b = b0 . Hence, f is in Pb0 and the set is convex.  7. Short selling. Let  X =   1; 2 ; p   1; 1  2  ; 1  cid:0  p  Let B = f b1; b2  : b1 + b2 = 1g Thus this set of portfolios B does not include the constraint bi  cid:21  0 .  This allows short selling.    a  Find the log optimal portfolio b cid:3  p  .   Information Theory and Portfolio Theory  383   b  Relate the growth rate W  cid:3  p  to the entropy rate H p  .  Solution: Short selling.  First, some philosophy. What does it mean to allow negative components in our port- folio vector? Suppose at the beginning of a trading day our current wealth is S . We want to invest our wealth S according to the portfolio b . If bi is positive, then we want to own biS dollars worth of stock i . But if bi is negative, then we want to owe biS dollars worth of stock i . This is what selling-short means. It means we sell a stock we don’t own in exchange for cash, but then we end up owing our broker so many shares of the stock we sold. Instead of owing money, we owe stock. The di cid:11 erence is that if the stock goes down in price by the end of the trading day, then we owe less money! So selling short is equivalent to betting that the stock will go down.  So, this is all well and good, but it seems to me that there may be some problems. First of all, why do we still insist that the components sum to one? It made a lot of sense when we interpreted the components, all positive, as fractions of our wealth, but it makes less sense if we are allowed to borrow money by selling short. Why not have the components sum to zero instead?  Secondly, if you owe money, then it’s possible for your wealth to be negative. This is bad for our model because the log of a negative value is unde cid:12 ned. The reason we take logs in the  cid:12 rst place is to turn a product into a sum that converges almost surely. But we are only justi cid:12 ed in taking the logs in the  cid:12 rst place if the product is positive, which it may not be if we allow short-selling.  Now, having gotten all these annoying philosophical worries out of the way, we can solve the problem quite simply by viewing it just as an unconstrained calculus problem and not worrying about what it all means.   a  We’ll represent an arbitrary portfolio as b =  b; 1  cid:0  b  . The quantity we’re trying  to maximize is  W  b  = E[log bT X ]  = E[log bX1 +  1  cid:0  b X2 ] 1 = p log b + 2 1  cid:0  b   +  1  cid:0  p  log b + 2 1 2  cid:0  = p log b + 2  cid:0  2b  +  1  cid:0  p  log b + 1 = p log 2  cid:0  b  +  1  cid:0  p  log  b  2 1 = p log 2  cid:0  b  +  1  cid:0  p  log 2  1 2  +   1  cid:0  b   1 2  b   +  1  cid:0  p  log 1 + b   We solve for the maximum of W  b  by taking the derivative and solving for zero:   384  Information Theory and Portfolio Theory  dW db  1  cid:0  p 1 + b   cid:0 p 2  cid:0  b  +  = 0  =   b = 2  cid:0  3p   b =  2  cid:0  3p; 3p  cid:0  1    b  This questions asks us to relate the growth rate W  cid:3  to the entropy rate H p  of the market. Evidently there is some equality or inequality we should discover, as is the case with the horse race. Our intuition should tell us that low entropy rates correspond to high doubling rates and that high entropy rates correspond to low doubling rates. Quite simply, the more certain we are about what the market is going to do next  low entropy rate , the more money we should be able to make in it.  W  cid:3  = W  2  cid:0  3p    3p  cid:0  1    3 2  1 = p log  2  cid:0  3p  + 2 3p  cid:0  1   +  1  cid:0  p  log  2  cid:0  3p  + 2 1 = p log 2  cid:0  3p + 6p  cid:0  2  +  1  cid:0  p  log 2  cid:0  3p + p  cid:0  2 3 = p log 3p +  1  cid:0  p  log  2 = p log p + p log 3 +  1  cid:0  p  log =  cid:0 H p  + p log 3 +  1  cid:0  p  log 3  cid:0   1  cid:0  p  log 2 =  cid:0 H p  + log 3  cid:0   1  cid:0  p  log 2    +  1  cid:0  p  log 1  cid:0  p   3 2  cid:0   3 2  p      W  cid:3  + H p  = log 3  cid:0   1  cid:0  p    cid:20  log 3  Hence, we can conclude that W  cid:3  + H p   cid:20  log 3  8. Normalizing x . Suppose we de cid:12 ne the log optimal portfolio b cid:3  to be the portfolio  maximizing the relative growth rate  btx  Z ln  dF  x1; : : : ; xm :  1  i=1 xi  The virtue of the normalization 1 with a uniform portfolio, is that the relative growth rate is  cid:12 nite, even when the growth  mPm mP Xi , which can be viewed as the wealth associated rate R ln btxdF  x  is not. This matters, for example, if X has a St. Petersburg-like  distribution. Thus the log optimal portfolio b cid:3  is de cid:12 ned for all distributions F , even those with in cid:12 nite growth rates W  cid:3  F   .   Information Theory and Portfolio Theory  385   a  Show that if b maximizes R ln btx dF  x  , it also maximizes R ln btx  m ; : : : ; 1  u =   1  m ; 1  m   .  utx dF  x  , where   b  Find the log optimal portfolio b cid:3  for  X =   22k+1; 22k   ; 2 cid:0  k+1  ; 22k +1 ; 2 cid:0  k+1    22k  where k = 1; 2; : : :   c  Find EX and W  cid:3  .  d  Argue that b cid:3  is competitively better than any portfolio b in the sense that  PrfbtX > cb cid:3 tXg  cid:20  1 c . Solution: Normalizing x   a  E[ log bT X  uT X ] = E[log bT X  cid:0  log uT X] = E[log bT X]  cid:0  E[log uT X]  where the second quantity in this last expression is just a number that does not change as the portfolio b changes. So any portfolio that maximizes the  cid:12 rst quantity in the last expression maximizes the entire expression.   b  Well, you can grunge out all the math here, which is messy but not di cid:14 cult. But you can also notice that the symmetry of the values that X can take on demands that, if there is any optimum solution, it must be at b =   1 2   . For every value of the form  a; b  that X can take on, there is a value of the form  b; a  that X takes on with equal probability, so there is absolutely no bias in the market between allocating funds to stock 1 vs. stock 2. 2 22k Normalizing X by utx = 1  2  22k+1 + 22k  , we obtain    = 3  2 ; 1  ^X =    4  3 ; 2   2 3 ; 4  3  ; with probability 2 cid:0  k+1  3  ; with probability 2 cid:0  k+1   Since ^X only takes on two values, we can sum over k and obtain  3  ; with probability 1 3  ; with probability 1 The doubling rate for a portfolio on this distbribution is  3 ; 2   2 3 ; 4  ^X =    4  2  2  W  b  =  1 2  log cid:18  4  3  b1 +  2 3   1  cid:0  b1  cid:19  +  1 2  log cid:18  2  3  b1 +  Di cid:11 erentiating and setting to zero and solving gives b =   1  4 3   1  cid:0  b1  cid:19   2 ; 1  2   .   c  It is easy to calculate that  E[X] =  1Xk=1 cid:16 22k+1 + 22k cid:17  2 cid:0  k+1  1Xk=1 3  cid:1  22k cid:0 k cid:0 1  =  = 1   16.21    16.22    16.23    16.24    16.25    16.26    386  Information Theory and Portfolio Theory  and similarly that  W  cid:3  =  1Xk=1 cid:20 log cid:18  1  2  22k+1 +  1 2  22k cid:19  + log cid:18  1  2  22k  +  1 2  22k+1 cid:19  cid:21  2 cid:0  k+1    16.27   =  =  1Xk=1 1Xk=1  2 cid:0 k log cid:18 22k 3 2 cid:0 k cid:18 2k log 2 + log  2 cid:19  16.28  2 cid:19    16.29   3   16.30   = 1;  for the standard de cid:12 nition of W  cid:3  . If we use the new de cid:12 nition, then obviously W  cid:3  = 0 , since the maximizing distribution b cid:3  is the uniform distribution, which is the distribution by which we are normalizing.   d  The inequality can be shown by Markov’s inequality and Theorem 16.2.2 as follows  PrnbtX > cb cid:3 tXo = Pr  btX  b cid:3 tX  > c   E btX b cid:3 tX c  1 c   cid:20   cid:20    16.31    16.32    16.33   and therefore no portfolio exists that almost surely beats b cid:3  . Also the probability that any other portfolio is more than twice the return of b cid:3  is less than 1  2 , etc.  9. Universal portfolio. We examine the  cid:12 rst n = 2 steps of the implementation of the universal portfolio for m = 2 stocks. Let the stock vectors for days 1 and 2 be x1 =  1; 1  2   , and x2 =  1; 2 : Let b =  b; 1  cid:0  b  denote a portfolio.   a  Graph S2 b  =Q2   b  Calculate S cid:3 2 = maxb S2 b  .  c  Argue that log S2 b  is concave in b .  i=1 btxi; 0  cid:20  b  cid:20  1 .   e  Calculate the universal portfolio at times n = 1 and n = 2 :  0 S2 b db:   d  Calculate the  universal  wealth ^S2 =R 1 ^b1 =Z 1 bS1 b db=Z 1  ^b2 x1  =Z 1  bdb  0  0  0  S1 b db:   f  Which of S2 b ; S cid:3 2 ; ^S2; ^b2 are unchanged if we permute the order of appearance  of the stock vector outcomes, i.e., if the sequence is now  1; 2 ;  1; 1  2   ?   Information Theory and Portfolio Theory  387  Solution: Universal portfolio.  All integrals, unless otherwise stated are over [0; 1] .  a  S2 b  =  b=2 + 1=2  2  cid:0  b  = 1 + b=2  cid:0  b2=2:  b  Maximizing over S2 b  we have S cid:3 2 = S2 1=2  = 9=8:  c  S2 b  is concave and log  cid:1   is a monotonic increasing concave function so log S2 b   d  Using  a  we have ^S2 =R  1 + b=2 + b2=2 db = 13=12: ^b2 x1  = Z bS1 b db=Z S1 b db   e  Clearly ^b1 = 1=2 , and  is concave as well  check! .  = Z 0:5b b + 1 db=Z 0:5 b + 1 db  = 5=9:   f  Only ^b2 x1  changes.  10. Growth optimal. Let X1; X2  cid:21  0 , be price relatives of two independent stocks. Suppose EX1 > EX2 . Do you always want some of X1 in a growth rate optimal portfolio S b  = bX1 +  cid:22 bX2 ? Prove or provide a counterexample. Solution: Growth optimal.  X2  X2  cid:20  1 and E X2  Yes, we always want some of X1 . The following is a proof by contradiction. Assume that b cid:3  =  0; 1 t so that X1 is not active. Then the KKT conditions for this choice of b cid:3  imply that E X1 = 1 , because by assumption stock 1 is inactive and stock 2 is active. The second condition is obviously satis cid:12 ed, so only the  cid:12 rst condition needs to be checked. Since X1 and X2 are independent the expectation can be rewritten as EX1E 1 is convex over the region of X2 interest, so by Jensen’s inequality E 1 > 1 since EX1 > EX2 . But this contradicts the KKT condition, therefore the assumption that b cid:3  =  0; 1 t must be wrong, and so we must want some of X1 . Note that we never want to short sell X1 . For any b < 0 , we have  . Since X2 is nonnegative,  . This gives that E X1  X2  cid:21  EX1  X2  cid:21  1  1 X2  EX2  EX2  X1 E ln bX1 +  1  cid:0  b X2   cid:0  E log X2  cid:20  E ln  b X2 X1  cid:20  ln  bE X2 < ln 1 = 0:  +  1  cid:0  b   +  1  cid:0  b    Hence, the short selling on X1 is always worse than b =  0; 1  . Alternatively, we can prove the same result directly as follows. Let  cid:0 1 < b < 1: Consider the growth rate W  b  = E ln bX1  cid:0   1  cid:0  b X2  . Di cid:11 erentiating w.r.t. b , we get  W 0 b  = E  X1  cid:0  X2  bX1 +  1  cid:0  b X2  :   388  Information Theory and Portfolio Theory  Note that W  b  is concave in b . Thus W 0 b  is monotonically nonincreasing. Since W 0 b cid:3   = 0 and W 0 0  = E X1  X2  cid:0  1 > 0 , it is immediate that b cid:3  > 0 .  11. Cost of universality. In the discussion of  cid:12 nite horizon universal portfolios, it was  shown that the loss factor due to universality is  1 Vn  =  n  Xk=0 n  k! cid:18  k  n cid:19 k cid:18  n  cid:0  k  n  cid:19 n cid:0 k  :   16.34   Evaluate Vn for n = 1; 2; 3 .  Solution: Cost of universality.  Simple computation of the equation allows us to calculate  n 1 2 3 4 5 6 7 8 9 10  1 Vn 0 0.125 0.197530864197531 0.251953125 0.29696 0.336076817558299 0.371099019723317 0.403074979782104 0.43267543343584 0.460358496  Vn 1 8 5.0625 3.96899224806202 3.36745689655172 2.97551020408163 2.69469857599079 2.48092799146348 2.31120124398809 2.17222014731754  12. Convex families. This problem generalizes Theorem 16.2.2. We say that S is a convex family of random variables if S1; S2 2 S implies  cid:21 S1 +  1  cid:0   cid:21  S2 2 S . Let S be a closed convex family of random variables. Show that there is a random variable S cid:3  2 S such that  for all S 2 S if and only if  for all S 2 S . Solution: Convex families. De cid:12 ne S cid:3  as the random variable that maximizes E ln S over all S 2 S . Since this is a maximization of a concave function over a convex set, there is a global maximum. For this value of S cid:3  , we have  E ln cid:18  S E cid:18  S  S cid:3  cid:19   cid:20  0 S cid:3  cid:19   cid:20  1  E ln S  cid:20  E ln S cid:3   E ln  S S cid:3   cid:20  0   16.35    16.36    16.37    16.38   for all S 2 S , and therefore  for all S 2 S .   Information Theory and Portfolio Theory  We need to show that for this value of S cid:3  , that  E  S S cid:3   cid:20  1  for all S 2 S . Let T 2 S be de cid:12 ned as T =  cid:21 S +  1  cid:0   cid:21  S cid:3  = S cid:3  +  cid:21  S  cid:0  S cid:3   . Then as  cid:21  ! 0 , expanding the logarithm in a Taylor series and taking only the  cid:12 rst term, we have   cid:21  S  cid:0  S cid:3    S cid:3    cid:19   cid:0  E ln S cid:3   E ln T  cid:0  E ln S cid:3  = E ln S cid:3  cid:18 1 +  cid:21  S  cid:0  S cid:3   S cid:3   cid:0  1 cid:19   = E  S cid:3  S  =  cid:21  cid:18 E  cid:20  0  where the last inequality follows from the fact that S cid:3  maximizes the expected loga- rithm. Therefore if S cid:3  maximizes the expected logarithm over the convex set, then for every S in the set,  The other direction follows from Jensen’s inequality, since if ES=S cid:3   cid:20  1 for all S , then  E  S S cid:3   cid:20  1  E ln  S S cid:3   cid:20  ln E  S S cid:3   cid:20  ln 1 = 0:  389   16.39    16.40    16.41    16.42    16.43    16.44    16.45    390  Information Theory and Portfolio Theory   Chapter 17  Inequalities in Information Theory  1. Sum of positive de cid:12 nite matrices. For any two positive de cid:12 nite matrices, K1 and  K2 , show that jK1 + K2j  cid:21  jK1j . Solution: Sum of positive de cid:12 nite matrices Let X , Y be independent random vectors with X  cid:24   cid:30 K1 and Y  cid:24   cid:30 K2 . Then X+Y  cid:24   cid:30 K1+K2 and hence 1 2 ln 2 cid:25 e njK1j , by Lemma 17.2.1.  2 ln 2 cid:25 e njK1+K2j = h X+Y   cid:21  h X  = 1  2. Fan’s inequality[5] for ratios of determinants. For all 1  cid:20  p  cid:20  n , for a positive  de cid:12 nite K = K 1; 2; : : : ; n  , show that  jKj  jK p + 1; p + 2; : : : ; n j  cid:20   jK i; p + 1; p + 2; : : : ; n j jK p + 1; p + 2; : : : ; n j  :  p  Yi=1   17.1   Solution: Ky Fan’s inequality for the ratio of determinants. We use the same idea as in Theorem 17.9.2, except that we use the conditional form of Theorem 17.1.5.  1 2  ln 2 cid:25 e p  jKj  jK p + 1; p + 2; : : : ; n j  = h X1; X2; : : : ; XpjXp+1; Xp+2; : : : ; Xn   cid:20  X h XijXp+1; Xp+2; : : : ; Xn   p  ln 2 cid:25 ejK i; p + 1; p + 2; : : : ; n j jK p + 1; p + 2; : : : ; n j  1 2  Xi=1  =  : 17.2   3. Convexity of determinant ratios. For positive de cid:12 nite matrices K , K0 , show that  ln jK+K0j  jKj  is convex in K .  Solution: Convexity of determinant ratios  The form of the expression is related to the capacity of the Gaussian channel, and hence we can use results from the concavity of mutual information to prove this result.  391   392  Inequalities in Information Theory  Consider a colored noise Gaussian channel  where X1; X2; : : : ; Xn  cid:24  N  0; K0  and Z1; Z2; : : : ; Zn  cid:24  N  0; K  , and X and Z are independent  Yi = Xi + Zi;   17.3   Then  I X1; X2; : : : ; Xn; Y1; Y2; : : : ; Yn  = h Y1; Y2; : : : ; Yn   cid:0  h Y1; Y2; : : : ; YnjX1; X2; : : : ; Xn   17.4   17.5   = h Y1; Y2; : : : ; Yn   cid:0  h Z1; Z2; : : : ; Zn  log 2 cid:25 e njKj =  1 2  log 2 cid:25 e njK + K0j  cid:0  log jK0 + Kj  =  1 2 1 2  jKj   17.6    17.7   Now from Theorem 2.7.2, relative entropy is a convex function of the the distributions  The theorem should be extended to the continuous case by replacing probability mass functions by densities and summations by integrations.  Thus if f cid:21  x; y  =  cid:21 f1 x; y  +  1  cid:0   cid:21  f2 x; y  , g cid:21  x; y  =  cid:21 g1 x; y  +  1  cid:0   cid:21  g2 x; y  , we have  D f cid:21  x; y jjg cid:21  x; y    cid:20   cid:21 D f1 x; y jjg1 x; y   +  1  cid:0   cid:21  D f2 x; y jjg2 x; y     17.8  Let Z n  cid:24  N  0; K1  with probability  cid:21  and Z n  cid:24  N  0; K2  with probability 1  cid:0   cid:21  . Let f1 xn; yn  be the joint distribution corresponding to Y n = X n + Z n when Z n  cid:24  N  0; K1  , and g1 x; y  = f1 x f1 y  be the corresponding product distribution. Then log jK0 + K1j I X n jK1j  17.9   1   = D f1 xn; yn jjf1 xn f1 yn   = D f1 xn; yn jjg1 xn; yn   =  1 ; Y n  1 2  Similarly  I X n  2 ; Y n  2   = D f1 xn; yn jjf1 xn f1 yn   = D f1 xn; yn jjg1 xn; yn   =  log jK0 + K2j jK2j  17.10  However, the mixture distribution is not Guassian, and cannot write the same expres- sion in terms of determinants. Instead, using the fact that the Gaussian is the worst noise given the moment constraints, we have by convexity of relative entropy  1 2  log jK0 + K cid:21 j  1 2  jK cid:21 j   cid:21  ; Y n  cid:21      cid:20  I X n = D f cid:21  xn; yn jjf cid:21  xn f cid:21  yn    17.12   cid:20   cid:21 D f1 x; y jjg1 x; y   +  1  cid:0   cid:21  D f2 x; y jjg2 x; y   17.13  =  cid:21 I X n  17.14    17.11   1 ; Y n log jK0 + K1j  1   +  1  cid:0   cid:21  I X n +  1  cid:0   cid:21    1 2  2 ; Y n 2   log jK0 + K2j  1 2   17.15   =  cid:21   jK2j  jK1j  proving the convexity of the determinant ratio.   Inequalities in Information Theory  393  4. Data Processing Inequality: Let random variable X1; X2; X3 and X4 form a  Markov chain X1 ! X2 ! X3 ! X4 . Show that  I X1; X3  + I X2; X4   cid:20  I X1; X4  + I X2; X3 :   17.16   Solution: Data Processing Inequality:  repeat of Problem 4.33  X1 ! X2 ! X3 ! X4 I X1; X4   +I X2; X3   cid:0  I X1; X3   cid:0  I X2; X4   = H X1   cid:0  H X1jX4  + H X2   cid:0  H X2jX3   cid:0   H X1   cid:0  H X1jX3     cid:0  H X2   cid:0  H X2jX4    = H X1jX3   cid:0  H X1jX4  + H X2jX4   cid:0  H X2jX3   17.19  = H X1; X2jX3   cid:0  H X2jX1; X3   cid:0  H X1; X2jX4  + H X2jX1; X4  17.20  +H X1; X2jX4   cid:0  H X1jX2; X4   cid:0  H X1; X2jX3  + H X1jX2; X3    17.21   17.22   =  cid:0 H X2jX1; X3  + H X2jX1; X4   cid:0  H X2jX1; X4   cid:0  H X2jX1; X3; X4  = I X2; X3jX1; X4   cid:21  0   17.17    17.18    17.23    17.24    17.25   where H X1jX2; X3  = H X1jX2; X4  by the Markovity of the random variables.  5. Markov chains: Let random variables X; Y; Z and W form a Markov chain so that  X ! Y !  Z; W   , i.e., p x; y; z; w  = p x p yjx p z; wjy  . Show that  I X; Z  + I X; W    cid:20  I X; Y   + I Z; W     17.26   Solution: Markov chains:  repeat of Problem 4.34  X ! Y !  Z; W   , hence by the data processing inequality, I X; Y    cid:21  I X;  Z; W    , and hence  I X : Y    +I Z; W    cid:0  I X; Z   cid:0  I X; W     cid:21  I X : Z; W   + I Z; W    cid:0  I X; Z   cid:0  I X; W   = H Z; W   + H X   cid:0  H X; W; Z  + H W   + H Z   cid:0  H W; Z    cid:0 H Z   cid:0  H X  + H X; Z    cid:0  H W    cid:0  H X  + H W; X  17.29   17.30   =  cid:0 H X; W; Z  + H X; Z  + H X; W    cid:0  H X  = H WjX   cid:0  H WjX; Z  = I W ; ZjX   cid:21  0   17.27    17.28    17.31    17.32    17.33    394  Inequalities in Information Theory   Bibliography  [1] T. Berger. Multiterminal source coding. In G. Longo, editor, The Information Theory  Approach to Communications. Springer-Verlag, New York, 1977.  [2] M. Bierbaum and H.M. Wallmeier. A note on the capacity region of the multiple access  channel. IEEE Trans. Inform. Theory, IT-25:484, 1979.  [3] I. Csisz cid:19 ar and J. K cid:127 orner. Information Theory: Coding Theorems for Discrete Memoryless  Systems. Academic Press, 1981.  [4] Ky Fan. On a theorem of Weyl concerning the eigenvalues of linear transformations II.  Proc. National Acad. Sci. U.S., 36:31{35, 1950.  [5] Ky Fan. Some inequalities concerning positive-de cid:12 nite matrices. Proc. Cambridge Phil.  Soc., 51:414{421, 1955.  [6] R.G. Gallager.  Information Theory and Reliable Communication. Wiley, New York,  [7] R.G. Gallager. Variations on a theme by Hu cid:11 man. IEEE Trans. Inform. Theory, IT-  [8] L. Lovasz. On the Shannon capacity of a graph. IEEE Trans. Inform. Theory, IT-25:1{7,  1968.  1979.  24:668{674, 1978.  [9] J.T. Pinkston. An application of rate-distortion theory to a converse to the coding  theorem. IEEE Trans. Inform. Theory, IT-15:66{71, 1969.  [10] A R cid:19 enyi. Wahrscheinlichkeitsrechnung, mit einem Anhang  cid:127 uber Informationstheorie.  Veb Deutscher Verlag der Wissenschaften, Berlin, 1962.  [11] A.A. Sardinas and G.W. Patterson. A necessary and su cid:14 cient condition for the unique decomposition of coded messages. In IRE Convention Record, Part 8, pages 104{108, 1953.  [12] C.E. Shannon. Communication theory of secrecy systems. Bell Sys. Tech. Journal,  28:656{715, 1949.  [13] C.E. Shannon. Coding theorems for a discrete source with a  cid:12 delity criterion.  IRE  National Convention Record, Part 4, pages 142{163, 1959.  395   396  BIBLIOGRAPHY  [14] C.E. Shannon. Two-way communication channels. In Proc. 4th Berkeley Symp. Math.  Stat. Prob., volume 1, pages 611{644. Univ. California Press, 1961.  [15] J.A. Storer and T.G. Szymanski. Data compression via textual substitution. J. ACM,  29 4 :928{951, 1982.
